2014,Probabilistic low-rank matrix completion on finite alphabets,The task of reconstructing a matrix given a sample of observed entries is known as the \emph{matrix completion problem}. Such a consideration arises in a wide variety of problems  including recommender systems  collaborative filtering  dimensionality reduction  image processing  quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here  we investigate the case where the observations take a finite numbers of values  corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (non-necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely  we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice  we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.,Probabilistic low-rank matrix completion on ﬁnite

alphabets

Jean Lafond

Institut Mines-T´el´ecom

T´el´ecom ParisTech

CNRS LTCI

Olga Klopp

CREST et MODAL’X
Universit´e Paris Ouest
Olga.KLOPP@math.cnrs.fr

´Eric Moulines

Institut Mines-T´el´ecom

T´el´ecom ParisTech

CNRS LTCI

jean.lafond@telecom-paristech.fr

moulines@telecom-paristech.fr

Joseph Salmon

Institut Mines-T´el´ecom

T´el´ecom ParisTech

CNRS LTCI

joseph.salmon@telecom-paristech.fr

Abstract

The task of reconstructing a matrix given a sample of observed entries is known
as the matrix completion problem.
It arises in a wide range of problems  in-
cluding recommender systems  collaborative ﬁltering  dimensionality reduction 
image processing  quantum physics or multi-class classiﬁcation to name a few.
Most works have focused on recovering an unknown real-valued low-rank ma-
trix from randomly sub-sampling its entries. Here  we investigate the case where
the observations take a ﬁnite number of values  corresponding for examples to
ratings in recommender systems or labels in multi-class classiﬁcation. We also
consider a general sampling scheme (not necessarily uniform) over the matrix
entries. The performance of a nuclear-norm penalized estimator is analyzed the-
oretically. More precisely  we derive bounds for the Kullback-Leibler divergence
between the true and estimated distributions. In practice  we have also proposed
an efﬁcient algorithm based on lifted coordinate gradient descent in order to tackle
potentially high dimensional settings.

1

Introduction

Matrix completion has attracted a lot of contributions over the past decade. It consists in recovering
the entries of a potentially high dimensional matrix  based on their random and partial observations.
In the classical noisy matrix completion problem  the entries are assumed to be real valued and ob-
served in presence of additive (homoscedastic) noise. In this paper  it is assumed that the entries take
values in a ﬁnite alphabet that can model categorical data. Such a problem arises in analysis of vot-
ing patterns  recovery of incomplete survey data (typical survey responses are true/false  yes/no or
do not know  agree/disagree/indifferent)  quantum state tomography [13] (binary outcomes)  recom-
mender systems [18  2] (for instance in common movie rating datasets  e.g.  MovieLens or Neﬂix 
ratings range from 1 to 5) among many others. It is customary in this framework that rows represent
individuals while columns represent items e.g.  movies  survey responses  etc. Of course  the obser-
vations are typically incomplete  in the sense that a signiﬁcant proportion of the entries are missing.
Then  a crucial question to be answered is whether it is possible to predict the missing entries from
these partial observations.

1

Since the problem of matrix completion is ill-posed in general  it is necessary to impose a low-
dimensional structure on the matrix  one particularly popular example being a low rank constraint.
The classical noisy matrix completion problem (real valued observations and additive noise)  can be
solved provided that the unknown matrix is low rank  either exactly or approximately; see [7  15  17 
20  5  16] and the references therein. Most commonly used methods amount to solve a least square
program under a rank constraint or a convex relaxation of a rank constraint provided by the nuclear
(or trace norm) [10].
The problem of probabilistic low rank matrix completion over a ﬁnite alphabet has received much
less attention; see [22  8  6] among others. To the best of our knowledge  only the binary case
(also referred to as the 1-bit matrix completion problem) has been covered in depth. In [8]  the
authors proposed to model the entries as Bernoulli random variables whose success rate depend
upon the matrix to be recovered through a convex link function (logistic and probit functions being
natural examples). The estimated matrix is then obtained as a solution of a maximization of the
log-likelihood of the observations under an explicit low-rank constraint. Moreover  the sampling
model proposed in [8] assumes that the entries are sampled uniformly at random. Unfortunately 
this condition is not totally realistic in recommender system applications: in such a context some
users are more active than others and some popular items are rated more frequently. Theoretically 
an important issue is that the method from [8] requires the knowledge of an upper bound on the
nuclear norm or on the rank of the unknown matrix.
Variations on the 1-bit matrix completion was further considered in [6] where a max-norm (though
the name is similar  this is different from the sup-norm) constrained minimization is considered. The
method of [6] allows more general non-uniform samplings but still requires an upper bound on the
max-norm of the unknown matrix.
In the present paper we consider a penalized maximum log-likelihood method  in which the log-
likelihood of the observations is penalized by the nuclear norm (i.e.  we focus on the Lagrangian
version rather than on the constrained one). We ﬁrst establish an upper bound of the Kullback-
Leibler divergence between the true and the estimated distribution under general sampling distribu-
tions; see Section 2 for details. One should note that our method only requires the knowledge of
an upper bound on the maximum absolute value of the probabilities  and improves upon previous
results found in the literature.
Last but not least  we propose an efﬁcient implementation of our statistical procedure  which is
adapted from the lifted coordinate descent algorithm recently introduced in [9  14]. Unlike other
methods  this iterative algorithm is designed to solve the convex optimization and not (possibly non-
convex) approximated formulation as in [21]. It also has the beneﬁt that it does not need to perform
full/partial SVD (Singular Value Decomposition) at every iteration; see Section 3 for details.

Notation
Deﬁne m1 ∧ m2 := min(m1  m2) and m1 ∨ m2 := max(m1  m2). We equip the set of m1 × m2
matrices with real entries (denoted Rm1×m2) with the scalar product (cid:104)X|X(cid:48)(cid:105) := tr(X(cid:62)X(cid:48)). For a
given matrix X ∈ Rm1×m2 we write (cid:107)X(cid:107)∞ := maxi j |Xi j| and  for q ≥ 1  we denote its Schatten
q-norm by

(cid:33)1/q

(cid:32)m1∧m2(cid:88)

i=1

(cid:107)X(cid:107)σ q :=

σi(X)q

 

where σi(X) are the singular values of X ordered in decreasing order (see [1] for more details on
such norms). The operator norm of X is given by (cid:107)X(cid:107)σ ∞ := σ1(X). Consider two vectors of
p − 1 matrices (X j)p−1
k l ≥ 0 
j=1 X j
X

j=1 such that for any (k  l) ∈ [m1] × [m2] we have X j
(cid:48)j
k l ≥ 0. Their square Hellinger distance is

k l ≥ 0  1 −(cid:80)p−1

(cid:48)j

(cid:19)2

+

(cid:118)(cid:117)(cid:117)(cid:116)1 − p−1(cid:88)

(cid:48)j
k l

X

k l −
X j

(cid:118)(cid:117)(cid:117)(cid:116)1 − p−1(cid:88)

2

(cid:48)j
k l

X

j=1

j=1

H (X  X(cid:48)) :=
d2

1

m1m2

k l ≥ 0 and 1 −(cid:80)p−1
j=1 and (X(cid:48)j)p−1
p−1(cid:88)
(cid:18)(cid:113)
k l −(cid:113)
(cid:88)

j=1 X

X j

k∈[m1]
l∈[m2]

j=1

2

and their Kullback-Leibler divergence is

KL (X  X(cid:48)) :=

1

m1m2

X j

k l log

p−1(cid:88)

j=1

(cid:88)

k∈[m1]
l∈[m2]

+ (1 − p−1(cid:88)

j=1

X j
k l
(cid:48)j
k l

X

X j

k l) log

1 −(cid:80)p−1
1 −(cid:80)p−1

j=1 X j
k l
(cid:48)j
j=1 X
k l

 .

it satisﬁes f j(x) ≥ 0 for j ∈ [p − 1] and 1 −(cid:80)p−1

Given an integer p > 1  a function f : Rp−1 → Rp−1 is called a p-link function if for any x ∈ Rp−1
j=1 f j(x) ≥ 0. For any collection of p − 1 matrices
k l) for any

j=1  f (X) denotes the vector of matrices (f (X)j)p−1

j=1 such that f (X)j

k l = f (X j

(X j)p−1
(k  l) ∈ [m1] × [m2] and j ∈ [p − 1].

2 Main results

j=1 of Rm1×m2 and an index ω ∈ [m1] × [m2]  we denote by Xω the vector (X j

Let p denote the cardinality of our ﬁnite alphabet  that is the number of classes of the logistic model
(e.g.  ratings have p possible values or surveys p possible answers). For a vector of p − 1 matrices
X = (X j)p−1
ω)p−1
j=1.
We consider an i.i.d. sequence (ωi)1≤i≤n over [m1]× [m2]  with a probability distribution function
Π that controls the way the matrix entries are revealed.
It is customary to consider the simple
uniform sampling distribution over the set [m1] × [m2]  though more general sampling schemes
could be considered as well. We observe n independent random elements (Yi)1≤i≤n ∈ [p]n. The
observations (Y1  . . .   Yn) are assumed to be independent and to follow a multinomial distribution
with success probabilities given by

P(Yi = j) = f j( ¯X 1

ωi

  . . .   ¯X p−1

ωi

)

j ∈ [p − 1]

and P(Yi = p) = 1 − p−1(cid:88)

P(Yi = j)

j=1

j=1 is a p-link function and ¯X = ( ¯X j)p−1

where {f j}p−1
j=1 is the vector of true (unknown) parameters
we aim at recovering. For ease of notation  we often write ¯Xi instead of ¯Xωi. Let us denote by ΦY
the (normalized) negative log-likelihood of the observations:

n(cid:88)

p−1(cid:88)

i=1

j=1

ΦY(X) = − 1
n

1{Yi=j} log(cid:0)f j(Xi)(cid:1) + 1{Yi=p} log

  

f j(Xi)

(1)

For any γ > 0 our proposed estimator is the following:

1 − p−1(cid:88)
p−1(cid:88)

j=1

j=1

ˆX =

arg min

X∈(Rm1×m2 )p−1

maxj∈[p−1] (cid:107)X j(cid:107)∞≤γ

Φλ

Y (X)   where Φλ

Y (X) = ΦY(X) + λ

(cid:107)X j(cid:107)σ 1  

(2)

with λ > 0 being a regularization parameter controlling the rank of the estimator. In the rest of the
paper we assume that the negative log-likelihood ΦY is convex (this is the case for the multinomial
logit function  see for instance [3]).
In this section we present two results controlling the estimation error of ˆX in the binomial setting
(i.e.  when p = 2). Before doing so  let us introduce some additional notation and assumptions. The
score function (deﬁned as the gradient of the negative log-likelihood) taken at the true parameter ¯X 
is denoted by ¯Σ := ∇ ΦY( ¯X). We also need the following constants depending on the link function
f and γ > 0:

(cid:32)

Mγ = sup
|x|≤γ

Lγ = max

Kγ = inf
|x|≤γ

2| log(f (x))|  

sup
|x|≤γ

|f(cid:48)(x)|
f (x)
f(cid:48)(x)2

8f (x)(1 − f (x))

.

(cid:33)

 

|f(cid:48)(x)|
1 − f (x)

  sup
|x|≤γ

3

In our framework  we allow for a general distribution for observing the coefﬁcients. However  we
need to control deviations of the sampling mechanism from the uniform distribution and therefore
we consider the following assumptions.
H1. There exists a constant µ ≥ 1 such that for all indexes (k  l) ∈ [m1] × [m2]

l=1 πk l) for any l ∈ [m2] (resp. k ∈ [m1]) the

(πk l) ≥ 1/(µm1m2) .

min
k l

with πk l := Π(ω1 = (k  l)).

Let us deﬁne Cl := (cid:80)m1

k=1 πk l (resp. Rk := (cid:80)m2

probability of sampling a coefﬁcient in column l (resp. in row k).
H2. There exists a constant ν ≥ 1 such that

(Rk  Cl) ≤ ν/(m1 ∧ m2)  

max
k l

Assumption H1 ensures that each coefﬁcient has a non-zero probability of being sampled whereas
H2 requires that no column nor row is sampled with too high probability (see also [11  16] for more
details on this condition).
i=1 by
We deﬁne the sequence of matrices (Ei)n
Ei := eki(e(cid:48)
(e(cid:48)
l)m2
l=1) being the canonical ba-
sis of Rm1 (resp. Rm2). Furthermore  if (εi)1≤i≤n is a Rademacher sequence independent from
(ωi)n

)(cid:62) where (ki  li) = ωi and with (ek)m1

i=1 associated to the revealed coefﬁcient (ωi)n

i=1 and (Yi)1≤i≤n we deﬁne

k=1 (resp.

li

n(cid:88)

i=1

ΣR :=

1
n

εiEi .

We can now state our ﬁrst result. For completeness  the proofs can be found in the supplementary
material.
Theorem 1. Assume H1 holds  λ ≥ 2(cid:107) ¯Σ(cid:107)σ ∞ and (cid:107) ¯X(cid:107)∞ ≤ γ. Then  with probability at least
1 − 2/d the Kullback-Leibler divergence between the true and estimated distribution is bounded by

m1m2 rank( ¯X)(cid:0)λ2 + c∗L2

γ(E(cid:107)ΣR(cid:107)σ ∞)2(cid:1)   µeMγ

(cid:16)

KL

f ( ¯X)  f ( ˆX)

(cid:32)

(cid:17) ≤ 8 max

µ2
Kγ

(cid:112)log(d)

(cid:33)

 

n

where c∗ is a universal constant.
Note that (cid:107) ¯Σ(cid:107)σ ∞ is stochastic and that its expectation E(cid:107)ΣR(cid:107)σ ∞ is unknown. However  thanks to
Assumption H2 these quantities can be controlled.
To ease notation let us also deﬁne m := m1 ∧ m2  M := m1 ∨ m2 and d := m1 + m2.
Theorem 2. Assume H 1 and H 2 hold and that (cid:107) ¯X(cid:107)∞ ≤ γ. Assume in addition that n ≥
2m log(d)/(9ν). Taking λ = 6Lγ
folllowing holds
(cid:107) ¯X − ˆX(cid:107)2
m1m2

(cid:112)2ν log(d)/(mn)  then with probability at least 1 − 3/d the
(cid:33)
(cid:112)log(d)
(cid:17) ≤ max

M rank( ¯X) log(d)

f ( ¯X)  f ( ˆX)

≤ KL

  8µeMγ

νµ2L2
γ

(cid:32)

(cid:16)

Kγ

Kγ

σ 2

n

 

n

¯c

where ¯c is a universal constant.
Remark. Let us compare the rate of convergence of Theorem 2 with those obtained in previous
works on 1-bit matrix completion. In [8]  the parameter ¯X is estimated by minimizing the negative
log-likelihood under the constraints (cid:107)X(cid:107)∞ ≤ γ and (cid:107)X(cid:107)σ 1 ≤ γ
rm1m2 for some r > 0. Under
the assumption that rank( ¯X) ≤ r  they could prove that

√

(cid:114)

(cid:107) ¯X − ˆX(cid:107)2
m1m2

σ 2

≤ Cγ

rd
n

 

where Cγ is a constant depending on γ (see [8  Theorem 1]). This rate of convergence is slower
than the rate of convergence given by Theorem 2. [6] studied a max-norm constrained maximum
likelihood estimate and obtained a rate of convergence similar to [8].

4

3 Numerical Experiments

Implementation
For numerical experiments  data were simulated according to a multinomial
logit distribution. In this setting  an observation Yk l associated to row k and column l is distributed
as P(Yk l = j) = f j(X 1

k l  . . .   X p−1

k l ) where

f j(x1  . . .   xp−1) = exp(xj)

exp(xj)

for j ∈ [p − 1] .

 

(3)

1 +

p−1(cid:88)

j=1

−1

With this choice  ΦY is convex and problem (2) can be solved using convex optimization algo-
rithms. Moreover  following the advice of [8] we considered the unconstrained version of problem
(2) (i.e.  with no constraint on (cid:107)X(cid:107)∞)  which reduces signiﬁcantly the computation burden and has
no signiﬁcant impact on the solution in practice. To solve this problem  we have extended to the
multinomial case the coordinate gradient descent algorithm introduced by [9]. This type of algo-
rithm has the advantage  say over the Soft-Impute [19] or the SVT [4] algorithm  that it does not
require the computation of a full SVD at each step of the main loop of an iterative (proximal) algo-
rithm (bare in mind that the proximal operator associated to the nuclear norm is the soft-thresholding
operator of the singular values). The proposed version only computes the largest singular vectors
and singular values. This potentially decreases the computation by a factor close to the value of the
upper bound on the rank commonly used (see the aforementioned paper for more details).
Let us present the algorithm. Any vector of p− 1 matrices X = (X j)p−1
of the tensor product space Rm1×m2 ⊗ Rp−1 and denoted by:

j=1 is identiﬁed as an element

X =

X j ⊗ ej  

(4)

where again (ej)p−1
normalized rank-one matrices is denoted by

j=1 is the canonical basis on Rp−1 and ⊗ stands for the tensor product. The set of

M :=(cid:8)M ∈ Rm1×m2|M = uv(cid:62) | (cid:107)u(cid:107) = (cid:107)v(cid:107) = 1  u ∈ Rm1   v ∈ Rm2(cid:9) .
for a ﬁnite number of M ∈ M. This space is equipped with the (cid:96)1-norm (cid:107)θ(cid:107)1 =(cid:80)

Deﬁne Θ the linear space of real-valued functions on M with ﬁnite support  i.e.  θ(M ) = 0 except
M∈M |θ(M )|.
the cone of functions θ ∈ Θ such that θ(M ) ≥ 0 for all

Deﬁne by Θ+ the positive orthant  i.e. 
M ∈ M. Any tensor X can be associated with a vector θ = (θ1  . . .   θp−1) ∈ Θp−1

+   i.e. 

X =

θj(M )M ⊗ ej .

(5)

Such representations are not unique  and among them  the one associated to the SVD plays a key
role  as we will see below. For a given X represented by (4) and for any j ∈ {1  . . .   p − 1}  denote
by {σj
k=1 the associated singular
vectors. Then  X may be expressed as

k=1 the (non-zero) singular values of the matrix X j and {uj

k}nj

k}nj

k vj

p−1(cid:88)

j=1

p−1(cid:88)

(cid:88)

j=1

M∈M

p−1(cid:88)

nj(cid:88)

j=1

k=1

σj
kuj

k)(cid:62) ⊗ ej .
k(vj
k)(cid:62)  k ∈ [nj] and θj(M ) = 0 otherwise  one

(6)

X =

p−1(cid:88)

Deﬁning θj the function θj(M ) = σj
k if M = uj
obtains a representation of the type given in Eq. (5).
Conversely  for any θ = (θ1  . . .   θp−1) ∈ Θp−1  deﬁne the map

k(vj

W : θ → Wθ :=

and the auxiliary objective function

j=1

(cid:88)

M∈M

θj(M )M

θ :=

θ ⊗ ej with W j
W j
(cid:88)
p−1(cid:88)

j=1

M∈M

5

˜Φλ

Y (θ) = λ

θj(M ) + ΦY(Wθ) .

(7)

+

j=1

The map θ (cid:55)→ Wθ is a continuous linear map from (Θp−1 (cid:107) · (cid:107)1) to Rm1×m2 ⊗ Rp−1  where

(cid:107)θ(cid:107)1 =(cid:80)p−1
and one obtains (cid:107)θ(cid:107)1 =(cid:80)p−1

(cid:80)
M∈M |θj(M )|. In addition  for all θ ∈ Θp−1
θ (cid:107)σ 1 ≤ (cid:107)θ(cid:107)1  

p−1(cid:88)
θ (cid:107)σ 1 when θ is the representation associated to the SVD decom-
position. An important consequence  outlined in [9  Proposition 3.1]  is that the minimization of (7)
is actually equivalent to the minimization of (2); see [9  Theorem 3.2].
The proposed coordinate gradient descent algorithm updates at each step the nonnegative ﬁnite sup-
port function θ. For θ ∈ Θ we denote by supp(θ) the support of θ and for M ∈ M  by δM ∈ Θ the
Dirac function on M satisfying δM (M ) = 1 and δM (M(cid:48)) = 0 if M(cid:48) (cid:54)= M. In our experiments we
have set to zero the initial θ0.

j=1 (cid:107)W j

(cid:107)W j

j=1

Algorithm 1: Multinomial lifted coordinate gradient descent
Data: Observations: Y   tuning parameter λ
initial parameter: θ0 ∈ Θp−1
Result: θ ∈ Θp−1
Initialization: θ ← θ0  k ← 0
while k ≤ K do

+

for j = 0 to p − 1 do

Compute top singular vectors pair of (−∇ ΦY(Wθ))j: uj  vj

+ ; tolerance: ; maximum number of iterations: K

Let g = λ + minj=1 ... p−1(cid:104)∇ ΦY | uj(vj)(cid:62)(cid:105)
if g ≤ −/2 then

(cid:0)θ + (b0δu0(v0)(cid:62)   . . .   bp−1δup−1(vp−1)(cid:62) )(cid:1)

else

˜Φλ
Y

+

arg min

(b0 ... bp−1)∈Rp−1

(β0  . . .   βp−1) =
θ ← θ + (β0δu0(v0)(cid:62)  . . .   βp−1δup−1(vp−1)(cid:62) )
k ← k + 1
Let gmax = maxj∈[p−1] maxuj (vj )(cid:62)∈supp(θj ) |λ + (cid:104)∇ ΦY | uj(vj)(cid:62)(cid:105)|
if gmax ≤  then
else

break
θ ←
θ(cid:48)∈Θp−1
k ← k + 1

arg min

+  supp(θ(cid:48)j )⊂supp(θj ) j∈[p−1]

Y (θ(cid:48))
˜Φλ

A major interest of Algorithm 1 is that it requires to store the value of the parameter entries only
for the indexes which are actually observed. Since in practice the number of observations is much
smaller than the total number of coefﬁcients m1m2  this algorithm is both memory and computa-
tionally efﬁcient. Moreover  using an SVD algorithm such as Arnoldi iterations to compute the top
singular values and vector pairs (see [12  Section 10.5] for instance) allows us to take full advantage
of gradient sparse structure. Algorithm 1 was implemented in C and Table 1 gives a rough idea
of the execution time for the case of two classes on a 3.07Ghz w3550 Xeon CPU (RAM 1.66 Go 
Cache 8Mo).
Simulated experiments
To evaluate our procedure we have performed simulations for matri-
ces with p = 2 or 5. For each class matrix X j we sampled uniformly ﬁve unitary vector pairs
(uj

k=1. We have then generated matrices of rank equals to 5  such that

k  vj

k)5

√
X j = Γ

m1m2

αkuj

k(vj

k)(cid:62)  

√
with (α1  . . .   α5) = (2  1  0.5  0.25  0.1) and Γ is a scaling factor. The
that E[(cid:107)X j(cid:107)∞] does not depend on the sizes of the problem m1 and m2.

k=1

m1m2 factor  guarantees

5(cid:88)

6

Parameter Size
Observations
Execution Time (s.)

103 × 103

3 · 103 × 3 · 103

104 × 104

105
4.5

105
52

107
730

Table 1: Execution time of the proposed algorithm for the binary case.

We then sampled the entries uniformly and the observations according to a logit distribution given
by Eq. (3). We have then considered and compared the two following estimators both computed
using Algorithm 1:

• the logit version of our method (with the link function given by Eq. (3))
• the Gaussian completion method (denoted by ˆXN )  that consists in using the Gaussian
log-likelihood instead of the multinomial in (2)  i.e.  using a classical squared Frobenius
norm (the implementation being adapted mutatis mutandis). Moreover an estimation of the
standard deviation is obtained by the classical analysis of the residue.

Contrary to the logit version  the Gaussian matrix completion does not directly recover the proba-
bilities of observing a rating. However  we can estimate this probability by the following quantity:

P( ˆXN

k l = j) = FN (0 1)(pj+1) − FN (0 1)(pj) with pj =

j−0.5− ˆXN

k l

0

1

ˆσ

if j = 1  
if 0 < j < p
if j = p  

where FN (0 1) is the cdf of a zero-mean standard Gaussian random variable.
As we see on Figure 1  the logistic estimator outperforms the Gaussian for both cases p = 2 and
p = 5 in terms of the Kullback-Leibler divergence. This was expected because the Gaussian model
allows uniquely symmetric distributions with the same variance for all the ratings  which is not the
case for logistic distributions. The choice of the λ parameter has been set for both methods by
performing 5-fold cross-validation on a geometric grid of size 0.8 log(n).
Table 2 and Table 3 summarize the results obtained for a 900 × 1350 matrix respectively for p = 2
and p = 5. For both the binomial case p = 2 and the multinomial case p = 5  the logistic model
slightly outperforms the Gaussian model. This is partly due to the fact that in the multinomial case 
some ratings can have a multi-modal distribution. In such a case  the Gaussian model is unable
to predict these ratings  because its distribution is necessarily centered around a single value and
is not ﬂexible enough. For instance consider the case of a rating distribution with high probability
of seeing 1 or 5  low probability of getting 2  3 and 4  where we observed both 1’s and 5’s. The
estimator based on a Gaussian model will tend to center its distribution around 2.5 and therefore
misses the bimodal shape of the distribution.

Observations
Gaussian prediction error
Logistic prediction error

10 · 103
0.49
0.42

50 · 103
0.34
0.30

100 · 103

500 · 103

0.29
0.27

0.26
0.24

Table 2: Prediction errors for a binomial (2 classes) underlying model  for a 900 × 1350 matrix.

Observations
Gaussian prediction error
Logistic prediction error

10 · 103
0.78
0.75

50 · 103
0.76
0.54

100 · 103

500 · 103

0.73
0.47

0.69
0.43

Table 3: Prediction Error for a multinomial (5 classes) distribution against a 900 × 1350 matrix.

Real dataset We have also run the same estimators on the MovieLens 100k dataset. In the case
of real data we cannot calculate the Kullback-Leibler divergence since no ground truth is available.
Therefore  to compare the prediction errors  we randomly selected 20% of the entries as a test set 
and the remaining entries were split between a training set (80%) and a validation set (20%).

7

Figure 1: Kullback-Leibler divergence between the estimated and the true model for different
matrices sizes and sampling fraction  normalized by number of classes. Right ﬁgure: binomial
and Gaussian models ; left ﬁgure: multinomial with ﬁve classes and Gaussian model. Results are
averaged over ﬁve samples.

For this dataset  ratings range from 1 to 5. To consider the beneﬁt of a binomial model  we have
tested each rating against the others (e.g.  ratings 5 are set to 0 and all others are set to 1). Interest-
ingly we see that the Gaussian prediction error is signiﬁcantly better when choosing labels −1  1
instead of labels 0  1. This is another motivation for not using the Gaussian version: the sensibility to
the alphabet choice seems to be crucial for the Gaussian version  whereas the binomial/multinomial
ones are insensitive to it. These results are summarized in table 4.

Rating
Gaussian prediction error (labels −1 and 1)
Gaussian prediction error (labels 0 and 1)
Logistic prediction error

1

0.06
0.12
0.06

2

0.12
0.20
0.11

3

0.28
0.39
0.27

4

0.35
0.46
0.34

5

0.19
0.30
0.20

Table 4: Binomial prediction error when performing one versus the others procedure on the Movie-
Lens 100k dataset.

4 Conclusion and future work

We have proposed a new nuclear norm penalized maximum log-likelihood estimator and have pro-
vided strong theoretical guarantees on its estimation accuracy in the binary case. Compared to
previous works on 1-bit matrix completion  our method has some important advantages. First  it
works under quite mild assumptions on the sampling distribution. Second  it requires only an up-
per bound on the maximal absolute value of the unknown matrix. Finally  the rates of convergence
given by Theorem 2 are faster than the rates of convergence obtained in [8] and [6]. In future work 
we could consider the extension to more general data ﬁtting terms  and to possibly generalize the
results to tensor formulations  or to penalize directly the nuclear norm of the matrix probabilities
themselves.

Acknowledgments

Jean Lafond is grateful for fundings from the Direction G´en´erale de l’Armement (DGA) and to the
labex LMH through the grant no ANR-11-LABX-0056-LMH in the framework of the ”Programme
des Investissements d’Avenir”. Joseph Salmon acknowledges Chair Machine Learning for Big Data
for partial ﬁnancial support. The authors would also like to thank Alexandre Gramfort for helpful
discussions.

8

1000002000003000004000005000000.000.050.100.150.20size: 100x150size: 300x450size: 900x1350Normalized KL divergence for logistic (plain)  Gaussian (dashed)Mean KL divergence Number of observationsNormalized KL divergence for logistic (plain)  Gaussian (dashed)Mean KL divergence Number of observations1000002000003000004000005000000.000.050.100.150.20size: 100x150size: 300x450size: 900x1350References
[1] R. Bhatia. Matrix analysis  volume 169 of Graduate Texts in Mathematics. Springer-Verlag 

New York  1997.

[2] J. Bobadilla  F. Ortega  A. Hernando  and A. Guti´errez. Recommender systems survey.

Knowledge-Based Systems  46(0):109 – 132  2013.

[3] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press  Cambridge 

2004.

[4] J-F. Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix com-

pletion. SIAM Journal on Optimization  20(4):1956–1982  2010.

[5] T. T. Cai and W-X. Zhou. Matrix completion via max-norm constrained optimization. CoRR 

abs/1303.0341  2013.

[6] T. T. Cai and W-X. Zhou. A max-norm constrained minimization approach to 1-bit matrix

completion. J. Mach. Learn. Res.  14:3619–3647  2013.

[7] E. J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–

936  2010.

[8] M. A. Davenport  Y. Plan  E. van den Berg  and M. Wootters. 1-bit matrix completion. CoRR 

abs/1209.3672  2012.

[9] M. Dud´ık  Z. Harchaoui  and J. Malick. Lifted coordinate descent for learning with trace-norm

regularization. In AISTATS  2012.

[10] M. Fazel. Matrix rank minimization with applications. PhD thesis  Stanford University  2002.
[11] R. Foygel  R. Salakhutdinov  O. Shamir  and N. Srebro. Learning with the weighted trace-norm

under arbitrary sampling distributions. In NIPS  pages 2133–2141  2011.

[12] G. H. Golub and C. F. van Loan. Matrix computations.

Baltimore  MD  fourth edition  2013.

Johns Hopkins University Press 

[13] D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis.

Theory  IEEE Transactions on  57(3):1548–1566  2011.

Information

[14] Z. Harchaoui  A. Juditsky  and A. Nemirovski. Conditional gradient algorithms for norm-

regularized smooth convex optimization. Mathematical Programming  pages 1–38  2014.

[15] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. J. Mach.

Learn. Res.  11:2057–2078  2010.

[16] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli 

2(1):282–303  02 2014.

[17] V. Koltchinskii  A. B. Tsybakov  and K. Lounici. Nuclear-norm penalization and optimal rates

for noisy low-rank matrix completion. Ann. Statist.  39(5):2302–2329  2011.

[18] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems.

Computer  42(8):30–37  2009.

[19] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning

large incomplete matrices. J. Mach. Learn. Res.  11:2287–2322  2010.

[20] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix comple-

tion: optimal bounds with noise. J. Mach. Learn. Res.  13:1665–1697  2012.

[21] B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix completion.

Mathematical Programming Computation  5(2):201–226  2013.

[22] A. Todeschini  F. Caron  and M. Chavent. Probabilistic low-rank matrix completion with

adaptive spectral regularization algorithms. In NIPS  pages 845–853  2013.

9

,Jean Lafond
Olga Klopp
Eric Moulines
Joseph Salmon