2017,On Structured Prediction Theory with Calibrated Convex Surrogate Losses,We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss  we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work  we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence  we formalize the intuition that some task losses make learning harder than others  and that the classical 0-1 loss is ill-suited for structured prediction.,On Structured Prediction Theory with Calibrated

Convex Surrogate Losses

Anton Osokin

INRIA/ENS∗  Paris  France

HSE†  Moscow  Russia

Francis Bach

INRIA/ENS∗  Paris  France

Abstract

Simon Lacoste-Julien

MILA and DIRO

Université de Montréal  Canada

We provide novel theoretical insights on structured prediction in the context of
efﬁcient convex surrogate loss minimization with consistency guarantees. For any
task loss  we construct a convex surrogate that can be optimized via stochastic
gradient descent and we prove tight bounds on the so-called “calibration function”
relating the excess surrogate risk to the actual risk. In contrast to prior related
work  we carefully monitor the effect of the exponential number of classes in the
learning guarantees as well as on the optimization complexity. As an interesting
consequence  we formalize the intuition that some task losses make learning harder
than others  and that the classical 0-1 loss is ill-suited for structured prediction.

1

Introduction

Structured prediction is a subﬁeld of machine learning aiming at making multiple interrelated
predictions simultaneously. The desired outputs (labels) are typically organized in some structured
object such as a sequence  a graph  an image  etc. Tasks of this type appear in many practical domains
such as computer vision [34]  natural language processing [42] and bioinformatics [19].
The structured prediction setup has at least two typical properties differentiating it from the classical
binary classiﬁcation problems extensively studied in learning theory:
1. Exponential number of classes: this brings both additional computational and statistical challenges.
By exponential  we mean exponentially large in the size of the natural dimension of output  e.g.  the
number of all possible sequences is exponential w.r.t. the sequence length.
2. Cost-sensitive learning: in typical applications  prediction mistakes are not all equally costly.
The prediction error is usually measured with a highly-structured task-speciﬁc loss function  e.g. 
Hamming distance between sequences of multi-label variables or mean average precision for ranking.
Despite many algorithmic advances to tackle structured prediction problems [4  35]  there have been
relatively few papers devoted to its theoretical understanding. Notable recent exceptions that made
signiﬁcant progress include Cortes et al. [13] and London et al. [28] (see references therein) which
proposed data-dependent generalization error bounds in terms of popular empirical convex surrogate
losses such as the structured hinge loss [44  45  47]. A question not addressed by these works is
whether their algorithms are consistent: does minimizing their convex bounds with inﬁnite data lead
to the minimization of the task loss as well? Alternatively  the structured probit and ramp losses are
consistent [31  30]  but non-convex and thus it is hard to obtain computational guarantees for them.
In this paper  we aim at getting the property of consistency for surrogate losses that can be efﬁciently
minimized with guarantees  and thus we consider convex surrogate losses.
The consistency of convex surrogates is well understood in the case of binary classiﬁcation [50  5  43]
and there is signiﬁcant progress in the case of multi-class 0-1 loss [49  46] and general multi-

∗DI École normale supérieure  CNRS  PSL Research University
†National Research University Higher School of Economics

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

class loss functions [3  39  48]. A large body of work speciﬁcally focuses on the related tasks of
ranking [18  9  40] and ordinal regression [37].
Contributions. In this paper  we study consistent convex surrogate losses speciﬁcally in the context
of an exponential number of classes. We argue that even while being consistent  a convex surrogate
might not allow efﬁcient learning. As a concrete example  Ciliberto et al. [10] recently proposed a
consistent approach to structured prediction  but the constant in their generalization error bound can
be exponentially large as we explain in Section 5. There are two possible sources of difﬁculties from
the optimization perspective: to reach adequate accuracy on the task loss  one might need to optimize
a surrogate loss to exponentially small accuracy; or to reach adequate accuracy on the surrogate loss 
one might need an exponential number of algorithm steps because of exponentially large constants
in the convergence rate. We propose a theoretical framework that jointly tackles these two aspects
and allows to judge the feasibility of efﬁcient learning. In particular  we construct a calibration
function [43]  i.e.  a function setting the relationship between accuracy on the surrogate and task
losses  and normalize it by the means of convergence rate of an optimization algorithm.
Aiming for the simplest possible application of our framework  we propose a family of convex
surrogates that are consistent for any given task loss and can be optimized using stochastic gradient
descent. For a special case of our family (quadratic surrogate)  we provide a complete analysis
including general lower and upper bounds on the calibration function for any task loss  with exact
values for the 0-1  block 0-1 and Hamming losses. We observe that to have a tractable learning
algorithm  one needs both a structured loss (not the 0-1 loss) and appropriate constraints on the
predictor  e.g.  in the form of linear constraints for the score vector functions. Our framework also
indicates that in some cases it might be beneﬁcial to use non-consistent surrogates. In particular  a
non-consistent surrogate might allow optimization only up to speciﬁc accuracy  but exponentially
faster than a consistent one.
We introduce the structured prediction setting suitable for studying consistency in Sections 2 and 3.
We analyze the calibration function for the quadratic surrogate loss in Section 4. We review the
related works in Section 5 and conclude in Section 6.

2 Structured prediction setup
In structured prediction  the goal is to predict a structured output y ∈ Y (such as a sequence  a graph 
an image) given an input x ∈ X . The quality of prediction is measured by a task-dependent loss
function L( ˆy  y | x) ≥ 0 specifying the cost for predicting ˆy when the correct output is y. In this
paper  we consider the case when the number of possible predictions and the number of possible
labels are both ﬁnite. For simplicity 1 we also assume that the sets of possible predictions and correct
outputs always coincide and do not depend on x. We refer to this set as the set of labels Y  denote its
cardinality by k  and map its elements to 1  . . .   k. In this setting  assuming that the loss function
depends only on ˆy and y  but not on x directly  the loss is deﬁned by a loss matrix L ∈ Rk×k. We
assume that all the elements of the matrix L are non-negative and will use Lmax to denote the maximal
element. Compared to multi-class classiﬁcation  k is typically exponentially large in the size of the
natural dimension of y  e.g.  contains all possible sequences of symbols from a ﬁnite alphabet.
Following standard practices in structured prediction [12  44]  we deﬁne the prediction model by
a score function f : X → Rk specifying a score fy(x) for each possible output y ∈ Y. The ﬁnal
prediction is done by selecting a label with the maximal value of the score

pred(f(x)) := argmax

ˆy∈Y

f ˆy(x) 

(1)

with some ﬁxed strategy to resolve ties. To simplify the analysis  we assume that among the labels
with maximal scores  the predictor always picks the one with the smallest index.
The goal of prediction-based machine learning consists in ﬁnding a predictor that works well on
the unseen test set  i.e.  data points coming from the same distribution D as the one generating the
training data. One way to formalize this is to minimize the generalization error  often referred to as
the actual (or population) risk based on the loss L 

RL(f) := IE(x y)∼D L(cid:0)pred(f(x))  y(cid:1).

(2)
Minimizing the actual risk (2) is usually hard. The standard approach is to minimize a surrogate risk 
which is a different objective easier to optimize  e.g.  convex. We deﬁne a surrogate loss as a function
1Our analysis is generalizable to rectangular losses  e.g.  ranking losses studied by Ramaswamy et al. [40].

2

RΦ(f) := IE(x y)∼D Φ(f(x)  y) 

Φ : Rk × Y → R depending on a score vector f = f(x) ∈ Rk and a target label y ∈ Y as input
arguments. We denote the y-th component of f with fy. The surrogate risk (the Φ-risk) is deﬁned as
(3)
where the expectation is taken w.r.t. the data-generating distribution D. To make the minimization
of (3) well-deﬁned  we always assume that the surrogate loss Φ is bounded from below and continuous.
Examples of common surrogate losses include the structured hinge-loss [44  47] ΦSSVM(f   y) :=

max ˆy∈Y(cid:0)f ˆy + L( ˆy  y)(cid:1) − fy  the log loss (maximum likelihood learning) used  e.g.  in conditional
random ﬁelds [25]  Φlog(f   y) := log((cid:80)
In terms of task losses  we consider the unstructured 0-1 loss L01( ˆy  y) := [ ˆy (cid:54)= y] 2 and the
(cid:80)T
two following structured losses: block 0-1 loss with b equal blocks of labels L01 b( ˆy  y) :=
[ ˆy and y are not in the same block]; and (normalized) Hamming loss between tuples of T binary
t=1[ˆyt (cid:54)= yt]. To illustrate some aspects of our analysis  we also
variables yt: LHam T ( ˆy  y) := 1
T
look at the mixed loss L01 b η: a convex combination of the 0-1 and block 0-1 losses  deﬁned as
L01 b η := ηL01 + (1 − η)L01 b for some η ∈ [0  1].

ˆy∈Y exp f ˆy) − fy  and their hybrids [38  21  22  41].

3 Consistency for structured prediction

3.1 Calibration function
We now formalize the connection between the actual risk RL and the surrogate Φ-risk RΦ via the
so-called calibration function  see Deﬁnition 1 below [5  49  43  18  3]. As it is standard for this
kind of analysis  the setup is non-parametric  i.e. it does not take into account the dependency of
scores on input variables x. For now  we assume that a family of score functions FF consists of all
vector-valued Borel measurable functions f : X → F where F ⊆ Rk is a subspace of allowed score
vectors  which will play an important role in our analysis. This setting is equivalent to a pointwise
analysis  i.e  looking at the different input x independently. We bring the dependency on the input
back into the analysis in Section 3.3 where we assume a speciﬁc family of score functions.
Let DX represent the marginal distribution for D on x and IP(· | x) denote its conditional given x.
We can now rewrite the risk RL and Φ-risk RΦ as

RL(f) = IEx∼DX (cid:96)(f(x)  IP(· | x))  RΦ(f) = IEx∼DX φ(f(x)  IP(· | x)) 

where the conditional risk (cid:96) and the conditional Φ-risk φ depend on a vector of scores f and a
conditional distribution on the set of output labels q as

(cid:96)(f   q) :=

qcL(pred(f )  c)  φ(f   q) :=

qcΦ(f   c).

c=1

The calibration function HΦ L F between the surrogate loss Φ and the task loss L relates the excess
surrogate risk with the actual excess risk via the excess risk bound:

HΦ L F (δ(cid:96)(f   q)) ≤ δφ(f   q)  ∀f ∈ F  ∀q ∈ ∆k 

(4)
where δφ(f   q) = φ(f   q) − inf ˆf∈F φ( ˆf   q)  δ(cid:96)(f   q) = (cid:96)(f   q) − inf ˆf∈F (cid:96)( ˆf   q) are the excess
risks and ∆k denotes the probability simplex on k elements.
In other words  to ﬁnd a vector f that yields an excess risk smaller than ε  we need to optimize the
Φ-risk up to HΦ L F (ε) accuracy (in the worst case). We make this statement precise in Theorem 2
below  and now proceed to the formal deﬁnition of the calibration function.
Deﬁnition 1 (Calibration function). For a task loss L  a surrogate loss Φ  a set of feasible scores F 
the calibration function HΦ L F (ε) (deﬁned for ε ≥ 0) equals the inﬁmum excess of the conditional
surrogate risk when the excess of the conditional actual risk is at least ε:

(cid:88)k

c=1

(cid:88)k

HΦ L F (ε) := inf

f∈F   q∈∆k

δφ(f   q)
δ(cid:96)(f   q) ≥ ε.

s.t.

(5)

(6)

We set HΦ L F (ε) to +∞ when the feasible set is empty.
By construction  HΦ L F is non-decreasing on [0  +∞)  HΦ L F (ε) ≥ 0  the inequality (4) holds 
and HΦ L F (0) = 0. Note that HΦ L F can be non-convex and even non-continuous (see examples
in Figure 1). Also  note that large values of HΦ L F (ε) are better.

2Here we use the Iverson bracket notation  i.e.  [A] := 1 if a logical expression A is true  and zero otherwise.

3

(a): Hamming loss LHam T

(b): Mixed loss L01 b 0.4

Figure 1: Calibration functions for the quadratic surrogate Φquad (12) deﬁned in Section 4 and two
different task losses. (a) – the calibration functions for the Hamming loss LHam T when used without
constraints on the scores  F = Rk (in red)  and with the tight constraints implying consistency 
F = span(LHam T ) (in blue). The red curve can grow exponentially slower than the blue one. (b) –
the calibration functions for the mixed loss L01 b η with η = 0.4 (see Section 2 for the deﬁnition) when
used without constraints on the scores (red) and with tight constraints for the block 0-1 loss (blue).
The blue curve represents level-0.2 consistency. The calibration function equals zero for ε ≤ η/2 
but grows exponentially faster than the red curve representing a consistent approach and thus could
be better for small η. More details on the calibration functions in this ﬁgure are given in Section 4.

3.2 Notion of consistency

We use the calibration function HΦ L F to set a connection between optimizing the surrogate and
task losses by Theorem 2  which is similar to Theorem 3 of Zhang [49].
Theorem 2 (Calibration connection). Let HΦ L F be the calibration function between the surrogate
loss Φ and the task loss L with feasible set of scores F ⊆ Rk. Let ˇHΦ L F be a convex non-decreasing
lower bound of the calibration function. Assume that Φ is continuous and bounded from below. Then 
for any ε > 0 with ﬁnite ˇHΦ L F (ε) and any f ∈ FF   we have

RΦ(f) < R∗

Φ F + ˇHΦ L F (ε) ⇒ RL(f) < R∗

L F + ε 

where R∗

Φ F := inf f∈FF RΦ(f) and R∗

L F := inf f∈FF RL(f).

(7)

(8)

Proof. We take the expectation of (4) w.r.t. x  where the second argument of (cid:96) is set to the conditional
distribution IP(· | x). Then  we apply Jensen’s inequality (since ˇHΦ L F is convex) to get

ˇHΦ L F (RL(f) − R∗

L F ) ≤ RΦ(f) − R∗

Φ F < ˇHΦ L F (ε) 

which implies (7) by monotonicity of ˇHΦ L F .

A suitable convex non-decreasing lower bound ˇHΦ L F (ε) required by Theorem 2 always exists  e.g. 
the zero constant. However  in this case Theorem 2 is not informative  because the l.h.s. of (7) is
never true. Zhang [49  Proposition 25] claims that ˇHΦ L F deﬁned as the lower convex envelope of
the calibration function HΦ L F satisﬁes ˇHΦ L F (ε) > 0  ∀ε > 0  if HΦ L F (ε) > 0  ∀ε > 0  and 
e.g.  the set of labels is ﬁnite. This statement implies that an informative ˇHΦ L F always exists and
allows to characterize consistency through properties of the calibration function HΦ L F .
We now deﬁne a notion of level-η consistency  which is more general than consistency.
Deﬁnition 3 (level-η consistency). A surrogate loss Φ is consistent up to level η ≥ 0 w.r.t. a task
loss L and a set of scores F if and only if the calibration function satisﬁes HΦ L F (ε) > 0 for all
ε > η and there exists ˆε > η such that HΦ L F (ˆε) is ﬁnite.

Looking solely at (standard level-0) consistency vs. inconsistency might be too coarse to capture
practical properties related to optimization accuracy (see  e.g.  [29]). For example  if HΦ L F (ε) = 0
only for very small values of ε  then the method can still optimize the actual risk up to a certain
level which might be good enough in practice  especially if it means that it can be optimized faster.
Examples of calibration functions for consistent and inconsistent surrogate losses are shown in
Figure 1.
Other notions of consistency. Deﬁnition 3 with η = 0 and F = Rk results in the standard setting
often appearing in the literature. In particular  in this case Theorem 2 implies Fisher consistency as

4

00.51"H(")no constraintstight constraints00.20.4"H(")no constraintstight constraintsformulated  e.g.  by Pedregosa et al. [37] for general losses and Lin [27] for binary classiﬁcation.
This setting is also closely related to many deﬁnitions of consistency used in the literature. For
example  for a bounded from below and continuous surrogate  it is equivalent to inﬁnite-sample
consistency [49]  classiﬁcation calibration [46]  edge-consistency [18]  (L  Rk)-calibration [39] 
prediction calibration [48]. See [49  Appendix A] for the detailed discussion.
Role of F. Let the approximation error for the restricted set of scores F be deﬁned as R∗
L :=
inf f∈FF RL(f) − inf f RL(f). For any conditional distribution q  the score vector f := −Lq will
yield an optimal prediction. Thus the condition span(L) ⊆ F is sufﬁcient for F to have zero
approximation error for any distribution D  and for our 0-consistency condition to imply the standard
Fisher consistency with respect to L. In the following  we will see that a restricted F can both play a
role for computational efﬁciency as well as statistical efﬁciency (thus losses with smaller span(L)
might be easier to work with).

L F −R∗

3.3 Connection to optimization accuracy and statistical efﬁciency

The scale of a calibration function is not intrinsically well-deﬁned: we could multiply the surrogate
function by a scalar and it would multiply the calibration function by the same scalar  without
changing the optimization problem. Intuitively  we would like the surrogate loss to be of order 1. If
with this scale the calibration function is exponentially small (has a 1/k factor)  then we have strong
evidence that the stochastic optimization will be difﬁcult (and thus learning will be slow).
To formalize this intuition  we add to the picture the complexity of optimizing the surrogate loss with
a stochastic approximation algorithm. By using a scale-invariant convergence rate  we provide a
natural normalization of the calibration function. The following two observations are central to the
theoretical insights provided in our work:
1. Scale. For a properly scaled surrogate loss  the scale of the calibration function is a good indication
of whether a stochastic approximation algorithm will take a large number of iterations (in the worst
case) to obtain guarantees of small excess of the actual risk (and vice-versa  a large coefﬁcient
indicates a small number of iterations). The actual veriﬁcation requires computing the normalization
quantities given in Theorem 6 below.
2. Statistics. The bound on the number of iterations directly relates to the number of training
examples that would be needed to learn  if we see each iteration of the stochastic approximation
algorithm as using one training example to optimize the expected surrogate.
To analyze the statistical convergence of surrogate risk optimization  we have to specify the set of
score functions that we work with. We assume that the structure on input x ∈ X is deﬁned by a
positive deﬁnite kernel K : X × X → R. We denote the corresponding reproducing kernel Hilbert
space (RKHS) by H and its explicit feature map by ψ(x) ∈ H. By the reproducing property  we
have (cid:104)f  ψ(x)(cid:105)H = f (x) for all x ∈ X   f ∈ H  where (cid:104)· ·(cid:105)H is the inner product in the RKHS. We
deﬁne the subspace of allowed scores F ⊆ Rk via the span of the columns of a matrix F ∈ Rk×r.
The matrix F explicitly deﬁnes the structure of the score function. With this notation  we will assume
that the score function is of the form f(x) = F W ψ(x)  where W : H → Rr is a linear operator
to be learned (a matrix if H is of ﬁnite dimension) that represents a collection of r elements in H 
transforming ψ(x) to a vector in Rr by applying the RKHS inner product r times.3 Note that for
structured losses  we usually have r (cid:28) k. The set of all score functions is thus obtained by varying W
in this deﬁnition and is denoted by FF H. As a concrete example of a score family FF H for structured
prediction  consider the standard sequence model with unary and pairwise potentials. In this case  the
dimension r equals T s + (T − 1)s2  where T is the sequence length and s is the number of labels
of each variable. The columns of the matrix F consist of 2T − 1 groups (one for each unary and
pairwise potential). Each row of F has exactly one entry equal to one in each column group (with
zeros elsewhere).
In this setting  we use the online projected averaged stochastic subgradient descent ASGD4 (stochastic
w.r.t. data (x(n)  y(n)) ∼ D) to minimize the surrogate risk directly [6]. The n-th update consists in
(9)

(cid:2)W (n−1) − γ(n)F T∇Φψ(x(n))T(cid:3) 

W (n) := PD

3Note that if rank(F ) = r  our setup is equivalent to assuming a joint kernel [47] in the product form:

Kjoint((x  c)  (x(cid:48)  c(cid:48))) := K(x  x(cid:48))F (c  :)F (c(cid:48)  :)T  where F (c  :) is the row c for matrix F .

4See  e.g.  [36] for the formal setup of kernel ASGD.

5

where F T∇Φψ(x(n))T : H → Rr is the stochastic functional gradient  γ(n) is the step size
and PD is the projection on the ball of radius D w.r.t. the Hilbert–Schmidt norm5. The vector
∇Φ ∈ Rk is a regular gradient of the sampled surrogate Φ(f(x(n))  y(n)) w.r.t. the scores  ∇Φ =
∇f Φ(f   y(n))|f =f(x(n)). We wrote the above update using an explicit feature map ψ for notational
simplicity  but kernel ASGD can also be implemented without it by using the kernel trick. The
convergence properties of ASGD in RKHS are analogous to the ﬁnite-dimensional ASGD because
they rely on dimension-free quantities. To use a simple convergence analysis  we follow Ciliberto
et al. [10] and make the following simplifying assumption:
Assumption 4 (Well-speciﬁed optimization w.r.t. the function class FF H). The distribution D is
such that R∗
Assumption 4 simply means that each row of W ∗ deﬁning f∗ belongs to the RKHS H implying
a ﬁnite norm (cid:107)W ∗(cid:107)HS. Assumption 4 can be relaxed if the kernel K is universal  but then the
convergence analysis becomes much more complicated [36].
Theorem 5 (Convergence rate). Under Assumption 4 and assuming that (i) the functions Φ(f   y)
are bounded from below and convex w.r.t. f ∈ Rk for all y ∈ Y; (ii) the expected square of the norm
HS ≤ M 2 and (iii) (cid:107)W ∗(cid:107)HS ≤ D 
of the stochastic gradient is bounded  IE(x y)∼D(cid:107)F T∇Φψ(x)T(cid:107)2
√
for N steps admits the
then running the ASGD algorithm (9) with the constant step-size γ := 2D
(cid:88)N
N
following expected suboptimality for the averaged iterate ¯f(N ):

Φ F := inf f∈FF RΦ(f) has some global minimum f∗ that also belongs to FF H.

M

IE[RΦ(¯f(N ))] − R∗

Φ F ≤ 2DM√

N

where ¯f(N ) := 1
N

n=1
Theorem 5 is a straight-forward extension of classical results [33  36].
By combining the convergence rate of Theorem 5 with Theorem 2 that connects the surrogate and
actual risks  we get Theorem 6 which explicitly gives the number of iterations required to achieve
ε accuracy on the expected population risk (see App. A for the proof). Note that since ASGD is
applied in an online fashion  Theorem 6 also serves as the sample complexity bound  i.e.  says how
many samples are needed to achieve ε target accuracy (compared to the best prediction rule if F has
zero approximation error).
Theorem 6 (Learning complexity). Under the assumptions of Theorem 5  for any ε > 0  the random
(w.r.t. the observed training set) output ¯f(N ) ∈ FF H of the ASGD algorithm after

iterations has the expected excess risk bounded with ε  i.e.  IE[RL(¯f(N ))] < R∗

L F + ε.

N > N∗ := 4D2M 2
Φ L F (ε)

ˇH 2

(11)

4 Calibration function analysis for quadratic surrogate

F W (n)ψ(x(n))T.

(10)

A major challenge to applying Theorem 6 is the computation of the calibration function HΦ L F . In
App. C  we present a generalization to arbitrary multi-class losses of a surrogate loss class from Zhang
[49  Section 4.4.2] that is consistent for any task loss L. Here  we consider the simplest example of
this family  called the quadratic surrogate Φquad  which has the advantage that we can bound or even
compute exactly its calibration function. We deﬁne the quadratic surrogate as

k(cid:88)

Φquad(f   y) := 1

2k(cid:107)f + L(:  y)(cid:107)2

2 = 1
2k

(f 2

c + 2fcL(c  y) + L(c  y)2).

(12)

c=1

One simple sufﬁcient condition for the surrogate (12) to be consistent and also to have zero approxi-
mation error is that F fully contains span(L). To make the dependence on the score subspace explicit 
we parameterize it with a matrix F ∈ Rk×r with the number of columns r typically being much
smaller than the number of labels k. With this notation  we have F = span(F ) = {F θ | θ ∈ Rr} 
and the dimensionality of F equals the rank of F   which is at most r.6
5The Hilbert–Schmidt norm of a linear operator A is deﬁned as (cid:107)A(cid:107)HS =

√
trA‡A where A‡ is the adjoint
operator. In the case of ﬁnite dimension  the Hilbert–Schmidt norm coincides with the Frobenius matrix norm.
6Evaluating Φquad requires computing F TF and F TL(:  y) for which direct computation is intractable when
k is exponential  but which can be done in closed form for the structured losses we consider (the Hamming and
block 0-1 loss). More generally  these operations require suitable inference algorithms. See also App. F.

6

For the quadratic surrogate (12)  the excess of the expected surrogate takes a simple form:

2k(cid:107)F θ + Lq(cid:107)2
2.

δφquad(F θ  q) = 1

(13)
Equation (13) holds under the assumption that the subspace F contains the column space of the
loss matrix span(L)  which also means that the set F contains the optimal prediction for any q (see
Lemma 9 in App. B for the proof). Importantly  the function δφquad(F θ  q) is jointly convex in the
conditional probability q and parameters θ  which simpliﬁes its analysis.
Lower bound on the calibration function. We now present our main technical result: a lower
bound on the calibration function for the surrogate loss Φquad (12). This lower bound characterizes
the easiness of learning with this surrogate given the scaling intuition mentioned in Section 3.3. The
proof of Theorem 7 is given in App. D.1.
Theorem 7 (Lower bound on HΦquad). For any task loss L  its quadratic surrogate Φquad  and a score
subspace F containing the column space of L  the calibration function can be lower bounded:

HΦquad L F (ε) ≥

ε2

≥ ε2
4k  

2

4k   ε2

4b and ε2

2k maxi(cid:54)=j (cid:107)PF ∆ij(cid:107)2

(14)
where PF is the orthogonal projection on the subspace F and ∆ij = ei − ej ∈ Rk with ec being
the c-th basis vector of the standard basis in Rk.
Lower bound for speciﬁc losses. We now discuss the meaning of the bound (14) for some speciﬁc
losses (the detailed derivations are given in App. D.3). For the 0-1  block 0-1 and Hamming losses
(L01  L01 b and LHam T   respectively) with the smallest possible score subspaces F  the bound (14)
8T   respectively. All these bounds are tight (see App. E). However  if F = Rk
gives ε2
the bound (14) is not tight for the block 0-1 and mixed losses (see also App. E). In particular  the
bound (14) cannot detect level-η consistency for η > 0 (see Def. 3) and does not change when the
loss changes  but the score subspace stays the same.
Upper bound on the calibration function. Theorem 8 below gives an upper bound on the calibration
function holding for unconstrained scores  i.e  F = Rk (see the proof in App. D.2). This result shows
that without some appropriate constraints on the scores  efﬁcient learning is not guaranteed (in the
worst case) because of the 1/k scaling of the calibration function.
Theorem 8 (Upper bound on HΦquad). If a loss matrix L with Lmax > 0 deﬁnes a pseudometric7 on
labels and there are no constraints on the scores  i.e.  F = Rk  then the calibration function for the
quadratic surrogate Φquad can be upper bounded: HΦquad L Rk (ε) ≤ ε2
2k  
From our lower bound in Theorem 7 (which guarantees consistency)  the natural constraint on
the score is F = span(L)  with the dimension of this space giving an indication of the intrinsic
“difﬁculty” of a loss. Computations for the lower bounds in some speciﬁc cases (see App. D.3 for
details) show that the 0-1 loss is “hard” while the block 0-1 loss and the Hamming loss are “easy”.
Note that in all these cases the lower bound (14) is tight  see the discussion below.
Exact calibration functions. Note that the bounds proven in Theorems 7 and 8 imply that  in the
case of no constraints on the scores F = Rk  for the 0-1  block 0-1 and Hamming losses  we have
(15)
where L is the matrix deﬁning a loss. For completeness  in App. E  we compute the exact calibration
functions for the 0-1 and block 0-1 losses. Note that the calibration function for the 0-1 loss equals the
lower bound  illustrating the worst-case scenario. To get some intuition  an example of a conditional
distribution q that gives the (worst case) value to the calibration function (for several losses) is
qi = 1
In what follows  we provide the calibration functions in the cases with constraints on the scores. For
the block 0-1 loss with b equal blocks and under constraints that the scores within blocks are equal 
the calibration function equals (see Proposition 14 of App. E.2)

2 and qc = 0 for c (cid:54)∈ {i  j}. See the proof of Proposition 12 in App. E.1.

4k ≤ HΦquad L Rk (ε) ≤ ε2
2k  

0 ≤ ε ≤ Lmax.

2  qj = 1

ε2

2 + ε

2 − ε

(16)
7A pseudometric is a function d(a  b) satisfying the following axioms: d(x  y) ≥ 0  d(x  x) = 0 (but

HΦquad L01 b F01 b (ε) = ε2
4b  

possibly d(x  y) = 0 for some x (cid:54)= y)  d(x  y) = d(y  x)  d(x  z) ≤ d(x  y) + d(y  z).

0 ≤ ε ≤ 1.

7

For the Hamming loss deﬁned over T binary variables and under constraints implying separable
scores  the calibration function equals (see Proposition 15 in App. E.3)
8T   0 ≤ ε ≤ 1.

HΦquad LHam T  FHam T (ε) = ε2

(17)

b (ε − η

2   the calibration function is of the order 1

The calibration functions (16) and (17) depend on the quantities representing the actual complexities
of the loss (the number of blocks b and the length of the sequence T ) and can be exponentially larger
than the upper bound for the unconstrained case.
In the case of mixed 0-1 and block 0-1 loss  if the scores f are constrained to be equal inside the
blocks  i.e.  belong to the subspace F01 b = span(L01 b) (cid:40) Rk  then the calibration function is equal
to 0 for ε ≤ η
2   implying inconsistency (and also note that the approximation error can be as big as η
for F01 b). However  for ε > η
2 )2. See Figure 1b for
the illustration of this calibration function and Proposition 17 of App. E.4 for the exact formulation
and the proof. Note that while the calibration function for the constrained case is inconsistent  its
value can be exponentially larger than the one for the unconstrained case for ε big enough and when
the blocks are exponentially large (see Proposition 16 of App. E.4).
Computation of the SGD constants. Applying the learning complexity Theorem 6 requires to
compute the quantity DM where D bounds the norm of the optimal solution and M bounds the
expected square of the norm of the stochastic gradient. In App. F  we provide a way to bound this
quantity for our quadratic surrogate (12) under the simplifying assumption that each conditional qc(x)
(seen as function of x) belongs to the RKHS H (which implies Assumption 4). In particular  we get
(18)

object feature maps (cid:107)ψ(x)(cid:107)H. We deﬁne Qmax as an upper bound on(cid:80)k
the generalization of the inequality(cid:80)k

where κ(F ) is the condition number of the matrix F   R is an upper bound on the RKHS norm of
c=1 (cid:107)qc(cid:107)H (can be seen as
c=1 qc ≤ 1 for probabilities). The constants R and Qmax depend
on the data  the constant Lmax depends on the loss  r and κ(F ) depend on the choice of matrix F .
We compute the constant DM for the speciﬁc losses that we considered in App. F.1. For the 0-1  block
0-1 and Hamming losses  we have DM = O(k)  DM = O(b) and DM = O(log3
2 k)  respectively.
These computations indicate that the quadratic surrogate allows efﬁcient learning for structured block
0-1 and Hamming losses  but that the convergence could be slow in the worst case for the 0-1 loss.

√
maxξ(κ(F )

DM = L2

rRQmax) 

ξ(z) = z2 + z 

5 Related works

Consistency for multi-class problems. Building on signiﬁcant progress for the case of binary
classiﬁcation  see  e.g. [5]  there has been a lot of interest in the multi-class case. Zhang [49] and
Tewari & Bartlett [46] analyze the consistency of many existing surrogates for the 0-1 loss. Gao &
Zhou [20] focus on multi-label classiﬁcation. Narasimhan et al. [32] provide a consistent algorithm
for arbitrary multi-class loss deﬁned by a function of the confusion matrix. Recently  Ramaswamy &
Agarwal [39] introduce the notion of convex calibrated dimension  as the minimal dimensionality of
the score vector that is required for consistency. In particular  they showed that for the Hamming loss
on T binary variables  this dimension is at most T . In our analysis  we use scores of rank (T + 1) 
see (35) in App. D.3  yielding a similar result.
The task of ranking has attracted a lot of attention and [18  8  9  40] analyze different families of
surrogate and task losses proving their (in-)consistency. In this line of work  Ramaswamy et al.
[40] propose a quadratic surrogate for an arbitrary low rank loss which is related to our quadratic
surrogate (12). They also prove that several important ranking losses  i.e.  precision@q  expected
rank utility  mean average precision and pairwise disagreement  are of low-rank. We conjecture that
our approach is compatible with these losses and leave precise connections as future work.
Structured SVM (SSVM) and friends. SSVM [44  45  47] is one of the most used convex surrogates
for tasks with structured outputs  thus  its consistency has been a question of great interest. It is
known that Crammer-Singer multi-class SVM [15]  which SSVM is built on  is not consistent for
0-1 loss unless there is a majority class with probability at least 1
2 [49  31]. However  it is consistent
for the “abstain” and ordinal losses in the case of 3 classes [39]. Structured ramp loss and probit
surrogates are closely related to SSVM and are consistent [31  16  30  23]  but not convex.

8

minimizing(cid:80)n

Recently  Do˘gan et al. [17] categorized different versions of multi-class SVM and analyzed them
from Fisher and universal consistency point of views. In particular  they highlight differences between
Fisher and universal consistency and give examples of surrogates that are Fisher consistent  but not
universally consistent and vice versa. They also highlight that the Crammer-Singer SVM is neither
Fisher  not universally consistent even with a careful choice of regularizer.
Quadratic surrogates for structured prediction. Ciliberto et al. [10] and Brouard et al. [7] consider
i=1 (cid:107)g(xi) − ψo(yi)(cid:107)2H aiming to match the RKHS embedding of inputs g : X → H
to the feature maps of outputs ψo : Y → H. In their frameworks  the task loss is not considered at
the learning stage  but only at the prediction stage. Our quadratic surrogate (12) depends on the loss
directly. The empirical risk deﬁned by both their and our objectives can be minimized analytically with
the help of the kernel trick and  moreover  the resulting predictors are identical. However  performing
such computation in the case of large dataset can be intractable and the generalization properties have
to be taken care of  e.g.  by the means of regularization. In the large-scale scenario  it is more natural
to apply stochastic optimization (e.g.  kernel ASGD) that directly minimizes the population risk and
has better dependency on the dataset size. When combined with stochastic optimization  the two
approaches lead to different behavior. In our framework  we need to estimate r = rank(L) scalar
functions  but the alternative needs to estimate k functions (if  e.g.  ψo(y) = ey ∈ Rk)  which results
in signiﬁcant differences for low-rank losses  such as block 0-1 and Hamming.
Calibration functions. Bartlett et al. [5] and Steinwart [43] provide calibration functions for most
existing surrogates for binary classiﬁcation. All these functions differ in term of shape  but are
roughly similar in terms of constants. Pedregosa et al. [37] generalize these results to the case of
ordinal regression. However  their calibration functions have at best a 1/k factor if the surrogate is
normalized w.r.t. the number of classes. The task of ranking has been of signiﬁcant interest. However 
most of the literature [e.g.  11  14  24  1]  only focuses on calibration functions (in the form of regret
bounds) for bipartite ranking  which is more akin to cost-sensitive binary classiﬁcation.
Ávila Pires et al. [3] generalize the theoretical framework developed by Steinwart [43] and present
results for the multi-class SVM of Lee et al. [26] (the score vectors are constrained to sum to zero)
c∈Y L(c  y)a(fc)
c∈Y fc = 0 and a(f ) is some convex function with all subgradients at zero being positive.
The recent work by Ávila Pires & Szepesvári [2] reﬁnes the results  but speciﬁcally for the case of
0-1 loss. In this line of work  the surrogate is typically not normalized by k  and if normalized the
calibration functions have the constant 1/k appearing.
Finally  Ciliberto et al. [10] provide the calibration function for their quadratic surrogate. Assuming
that the loss can be represented as L( ˆy  y) = (cid:104)V ψo( ˆy)  ψo(y)(cid:105)HY   ˆy  y ∈ Y (this assumption can
always be satisﬁed in the case of a ﬁnite number of labels  by taking V as the loss matrix L and
ψo(y) := ey ∈ Rk where ey is the y-th vector of the standard basis in Rk). In their Theorem 2  they
provide an excess risk bound leading to a lower bound on the corresponding calibration function
HΦ L Rk (ε) ≥ ε2
where a constant c∆ = (cid:107)V (cid:107)2 maxy∈Y (cid:107)ψo(y)(cid:107) simply equals the spectral norm
of the loss matrix for the ﬁnite-dimensional construction provided above. However  the spectral
norm of the loss matrix is exponentially large even for highly structured losses such as the block 0-1
and Hamming losses  i.e.  (cid:107)L01 b(cid:107)2 = k − k
2 . This conclusion puts the objective
of Ciliberto et al. [10] in line with ours when no constraints are put on the scores.

that can be built for any task loss of interest. Their surrogate Φ is of the form(cid:80)
where(cid:80)

c2
∆

b   (cid:107)LHam T(cid:107)2 = k

6 Conclusion

In this paper  we studied the consistency of convex surrogate losses speciﬁcally in the context
of structured prediction. We analyzed calibration functions and proposed an optimization-based
normalization aiming to connect consistency with the existence of efﬁcient learning algorithms.
Finally  we instantiated all components of our framework for several losses by computing the
calibration functions and the constants coming from the normalization. By carefully monitoring
exponential constants  we highlighted the difference between tractable and intractable task losses.
These were ﬁrst steps in advancing our theoretical understanding of consistent structured prediction.
Further steps include analyzing more losses such as the low-rank ranking losses studied by Ra-
maswamy et al. [40] and  instead of considering constraints on the scores  one could instead put
constraints on the set of distributions to investigate the effect on the calibration function.

9

Acknowledgements

We would like to thank Pascal Germain for useful discussions. This work was partly supported
by the ERC grant Activia (no. 307574)  the NSERC Discovery Grant RGPIN-2017-06936 and the
MSR-INRIA Joint Center.

References
[1] Agarwal  Shivani. Surrogate regret bounds for bipartite ranking via strongly proper losses.

Journal of Machine Learning Research (JMLR)  15(1):1653–1674  2014.

[2] Ávila Pires  Bernardo and Szepesvári  Csaba. Multiclass classiﬁcation calibration functions.

arXiv  1609.06385v1  2016.

[3] Ávila Pires  Bernardo  Ghavamzadeh  Mohammad  and Szepesvári  Csaba. Cost-sensitive

multiclass classiﬁcation risk bounds. In ICML  2013.

[4] Bakir  Gökhan  Hofmann  Thomas  Schölkopf  Bernhard  Smola  Alexander J.  Taskar  Ben 

and Vishwanathan  S.V.N. Predicting Structured Data. MIT press  2007.

[5] Bartlett  Peter L.  Jordan  Michael I.  and McAuliffe  Jon D. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  101(473):138–156  2006.

[6] Bousquet  Olivier and Bottou  Léon. The tradeoffs of large scale learning. In NIPS  2008.

[7] Brouard  Céline  Szafranski  Marie  and d’Alché-Buc  Florence. Input output kernel regression:
Supervised and semi-supervised structured output prediction with operator-valued kernels.
Journal of Machine Learning Research (JMLR)  17(176):1–48  2016.

[8] Buffoni  David  Gallinari  Patrick  Usunier  Nicolas  and Calauzènes  Clément. Learning scoring

functions with order-preserving losses and standardized supervision. In ICML  2011.

[9] Calauzènes  Clément  Usunier  Nicolas  and Gallinari  Patrick. On the (non-)existence of convex 

calibrated surrogate losses for ranking. In NIPS  2012.

[10] Ciliberto  Carlo  Rudi  Alessandro  and Rosasco  Lorenzo. A consistent regularization approach

for structured prediction. In NIPS  2016.

[11] Clémençon  Stéphan  Lugosi  Gábor  and Vayatis  Nicolas. Ranking and empirical minimization

of U-statistics. The Annals of Statistics  pp. 844–874  2008.

[12] Collins  Michael. Discriminative training methods for hidden Markov models: Theory and

experiments with perceptron algorithms. In EMNLP  2002.

[13] Cortes  Corinna  Kuznetsov  Vitaly  Mohri  Mehryar  and Yang  Scott. Structured prediction

theory based on factor graph complexity. In NIPS  2016.

[14] Cossock  David and Zhang  Tong. Statistical analysis of bayes optimal subset ranking. IEEE

Transactions on Information Theory  54(11):5140–5154  2008.

[15] Crammer  Koby and Singer  Yoram. On the algorithmic implementation of multiclass kernel-

based vector machines. Journal of Machine Learning Research (JMLR)  2:265–292  2001.

[16] Do  Chuong B.  Le  Quoc  Teo  Choon Hui  Chapelle  Olivier  and Smola  Alex. Tighter bounds

for structured estimation. In NIPS  2009.

[17] Do˘gan  Ürün  Glasmachers  Tobias  and Igel  Christian. A uniﬁed view on multi-class support

vector classiﬁcation. Journal of Machine Learning Research (JMLR)  17(45):1–32  2016.

[18] Duchi  John C.  Mackey  Lester W.  and Jordan  Michael I. On the consistency of ranking

algorithms. In ICML  2010.

[19] Durbin  Richard  Eddy  Sean  Krogh  Anders  and Mitchison  Graeme. Biological sequence
analysis: probabilistic models of proteins and nucleic acids. Cambridge university press  1998.

10

[20] Gao  Wei and Zhou  Zhi-Hua. On the consistency of multi-label learning. In COLT  2011.

[21] Gimpel  Kevin and Smith  Noah A. Softmax-margin CRFs: Training loglinear models with cost

functions. In NAACL  2010.

[22] Hazan  Tamir and Urtasun  Raquel. A primal-dual message-passing algorithm for approximated

large scale structured prediction. In NIPS  2010.

[23] Keshet  Joseph. Optimizing the measure of performance in structured prediction. In Advanced

Structured Prediction. MIT Press  2014.

[24] Kotlowski  Wojciech  Dembczynski  Krzysztof  and Huellermeier  Eyke. Bipartite ranking

through minimization of univariate loss. In ICML  2011.

[25] Lafferty  John  McCallum  Andrew  and Pereira  Fernando. Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In ICML  2001.

[26] Lee  Yoonkyung  Lin  Yi  and Wahba  Grace. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance data. Journal of
the American Statistical Association  99(465):67–81  2004.

[27] Lin  Yi. A note on margin-based loss functions in classiﬁcation. Statistics & Probability Letters 

68(1):73–82  2004.

[28] London  Ben  Huang  Bert  and Getoor  Lise. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR)  17(222):1–52  2016.

[29] Long  Phil and Servedio  Rocco. Consistency versus realizable H-consistency for multiclass

classiﬁcation. In ICML  2013.

[30] McAllester  D. A. and Keshet  J. Generalization bounds and consistency for latent structural

probit and ramp loss. In NIPS  2011.

[31] McAllester  David. Generalization bounds and consistency for structured labeling. In Predicting

Structured Data. MIT Press  2007.

[32] Narasimhan  Harikrishna  Ramaswamy  Harish G.  Saha  Aadirupa  and Agarwal  Shivani.

Consistent multiclass algorithms for complex performance measures. In ICML  2015.

[33] Nemirovski  A.  Juditsky  A.  Lan  G.  and Shapiro  A. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[34] Nowozin  Sebastian and Lampert  Christoph H. Structured learning and prediction in computer

vision. Foundations and Trends in Computer Graphics and Vision  6(3–4):185–365  2011.

[35] Nowozin  Sebastian  Gehler  Peter V.  Jancsary  Jeremy  and Lampert  Christoph H. Advanced

Structured Prediction. MIT Press  2014.

[36] Orabona  Francesco. Simultaneous model selection and optimization through parameter-free

stochastic learning. In NIPS  2014.

[37] Pedregosa  Fabian  Bach  Francis  and Gramfort  Alexandre. On the consistency of ordinal

regression methods. Journal of Machine Learning Research (JMLR)  18(55):1–35  2017.

[38] Pletscher  Patrick  Ong  Cheng Soon  and Buhmann  Joachim M. Entropy and margin maxi-

mization for structured output learning. In ECML PKDD  2010.

[39] Ramaswamy  Harish G. and Agarwal  Shivani. Convex calibration dimension for multiclass

loss matrices. Journal of Machine Learning Research (JMLR)  17(14):1–45  2016.

[40] Ramaswamy  Harish G.  Agarwal  Shivani  and Tewari  Ambuj. Convex calibrated surrogates

for low-rank loss matrices with applications to subset ranking losses. In NIPS  2013.

[41] Shi  Qinfeng  Reid  Mark  Caetano  Tiberio  van den Hengel  Anton  and Wang  Zhenhua. A
hybrid loss for multiclass and structured prediction. IEEE transactions on pattern analysis and
machine intelligence (TPAMI)  37(1):2–12  2015.

11

[42] Smith  Noah A. Linguistic structure prediction. Synthesis lectures on human language tech-

nologies  4(2):1–274  2011.

[43] Steinwart  Ingo. How to compare different loss functions and their risks. Constructive Approxi-

mation  26(2):225–287  2007.

[44] Taskar  Ben  Guestrin  Carlos  and Koller  Daphne. Max-margin markov networks. In NIPS 

2003.

[45] Taskar  Ben  Chatalbashev  Vassil  Koller  Daphne  and Guestrin  Carlos. Learning structured

prediction models: a large margin approach. In ICML  2005.

[46] Tewari  Ambuj and Bartlett  Peter L. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research (JMLR)  8:1007–1025  2007.

[47] Tsochantaridis  I.  Joachims  T.  Hofmann  T.  and Altun  Y. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research (JMLR) 
6:1453–1484  2005.

[48] Williamson  Robert C.  Vernet  Elodie  and Reid  Mark D. Composite multiclass losses. Journal

of Machine Learning Research (JMLR)  17(223):1–52  2016.

[49] Zhang  Tong. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research (JMLR)  5:1225–1251  2004.

[50] Zhang  Tong. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics  32(1):56–134  2004.

12

,Stefan Wager
Sida Wang
Percy Liang
Anton Osokin
Francis Bach
Simon Lacoste-Julien