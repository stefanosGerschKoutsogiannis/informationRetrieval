2014,Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations,Various factors  such as identities  views (poses)  and illuminations  are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly  even without accessing 3D data  human not only can recognize face identity  but can also imagine face images of a person under different viewpoints given a single 2D image  making face perception in the brain robust to view changes. In this sense  human brain has learned and encoded 3D face models from 2D images. To take into account this instinct  this paper proposes a novel deep neural net  named multi-view perceptron (MVP)  which can untangle the identity and view features  and infer a full spectrum of multi-view images in the meanwhile  given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.,Multi-View Perceptron: a Deep Model for Learning

Face Identity and View Representations

Zhenyao Zhu1 3

Ping Luo3 1

Xiaogang Wang2 3

1Department of Information Engineering  The Chinese University of Hong Kong
2Department of Electronic Engineering  The Chinese University of Hong Kong
3Shenzhen Key Lab of CVPR  Shenzhen Institutes of Advanced Technology 

{zz012 lp011}@ie.cuhk.edu.hk xgwang@ee.cuhk.edu.hk xtang@ie.cuhk.edu.hk

Chinese Academy of Sciences  Shenzhen  China

Xiaoou Tang1 3

Abstract

Various factors  such as identity  view  and illumination  are coupled in face
images. Disentangling the identity and view representations is a major challenge
in face recognition. Existing face recognition systems either use handcrafted
features or learn features discriminatively to improve recognition accuracy. This
is different from the behavior of primate brain. Recent studies [5  19] discovered
that primate brain has a face-processing network  where view and identity are
processed by different neurons. Taking into account this instinct  this paper
proposes a novel deep neural net  named multi-view perceptron (MVP)  which can
untangle the identity and view features  and in the meanwhile infer a full spectrum
of multi-view images  given a single 2D face image. The identity features of MVP
achieve superior performance on the MultiPIE dataset. MVP is also capable to
interpolate and predict images under viewpoints that are unobserved in the training
data.

1

Introduction

The performance of face recognition systems depends heavily on facial representation  which is
naturally coupled with many types of face variations  such as view  illumination  and expression. As
face images are often observed in different views  a major challenge is to untangle the face identity
and view representations. Substantial efforts have been dedicated to extract identity features by
hand  such as LBP [1]  Gabor [14]  and SIFT [15]. The best practise of face recognition extracts
the above features on the landmarks of face images with multiple scales and concatenates them into
high dimensional feature vectors [4  21]. Deep learning methods  such as Boltzmann machine [9] 
sum product network [17]  and deep neural net [16  25  22  23  24  26] have been applied to face
recognition. For instance  Sun et al. [25  22] employed deep neural net to learn identity features
from raw pixels by predicting 10  000 identities.
Deep neural net is inspired by the understanding of hierarchical cortex in the primate brain and
mimicking some aspects of its activities. Recent studies [5  19] discovered that macaque monkeys
have a face-processing network that was made of six interconnected face-selective regions  where
neurons in some of these regions were view-speciﬁc  while some others were tuned to identity across
views  making face recognition in brain of primate robust to view variation. This intriguing function
of primate brain inspires us to develop a novel deep neural net  called multi-view perceptron (MVP) 
which can disentangle identity and view representations  and also reconstruct images under multiple
views. Speciﬁcally  given a single face image of an identity under an arbitrary view  it can generate
a sequence of output face images of the same identity  one at a time  under a full spectrum of
viewpoints. Examples of the input images and the generated multi-view outputs of two identities
are illustrated in Fig. 1. The images in the last two rows are from the same person. The extracted
features of MVP with respect to identity and view are plotted correspondingly in blue and orange.

1

Figure 1: The inputs (ﬁrst column) and the multi-view outputs (remaining columns) of two identities. The ﬁrst
input is from one identity and the last two inputs are from the other. Each reconstructed multi-view image (left)
has its ground truth (right) for comparison. The extracted identity features of the inputs (the second column) 
and the view features of both the inputs and outputs are plotted in blue and orange  respectively. The identity
features of the same identity are similar  even though the inputs are captured in diverse views  while the view
features of the same viewpoint are similar  although they are from different identities. The two persons look
similar in the frontal view  but can be better distinguished in other views.

We can observe that the identity features of the same identity are similar  even though the inputs are
captured in very different views  whilst the view features of images in the same view are similar 
although they are across different identities.
Unlike other deep networks that produce a deterministic output from an input  MVP employs the
deterministic hidden neurons to learn the identity features  whilst using the random hidden neurons
to capture the view representation. By sampling distinct values of the random neurons  output
images in distinct views are generated. Moreover  to yield images of different viewpoints  we
add regularization that images under similar viewpoints should have similar view representations
on the random neurons. The two types of neurons are modeled in a probabilistic way.
In the
training stage  the parameters of MVP are updated by back-propagation  where the gradient is
calculated by maximizing a variational lower bound of the complete data log-likelihood. With our
proposed learning algorithm  the EM updates on the probabilistic model are converted to forward
and backward propagation. In the testing stage  given an input image  MVP can extract its identity
and view features. In addition  if an order of viewpoints is also provided  MVP can sequentially
reconstruct multiple views of the input image by following this order.
This paper has several key contributions. (i) We propose a multi-view perceptron (MVP) and its
learning algorithm to factorize the identity and view representations with different sets of neurons 
making the learned features more discriminative and robust. (ii) MVP can reconstruct a full spectrum
of views given a single 2D image. The full spectrum of views can better distinguish identities  since
different identities may look similar in a particular view but differently in others as illustrated in Fig.
1. (iii) MVP can interpolate and predict images under viewpoints that are unobserved in the training
data  in some sense imitating the reasoning ability of human.
Related Works. In the literature of computer vision  existing methods that deal with view (pose)
variation can be divided into 2D- and 3D-based methods. For example  the 2D methods  such as [6] 
infer the deformation (e.g. thin plate splines) between 2D images across poses. The 3D methods 
such as [2  12]  capture 3D face models in different parametric forms. The above methods have
their inherent shortages. Extra cost and resources are necessitated to capture and process 3D data.
Because of lacking one degree of freedom  inferring 3D deformation from 2D transformation is
often ill-posed. More importantly  none of the existing approaches simulates how the primate brain
encodes view representations. In our approach  instead of employing any geometric models  view
information is encoded with a small number of neurons  which can recover the full spectrum of
views together with identity neurons. This representation of encoding identity and view information
into different neurons is closer to the face-processing system in the primate brain and new to the
deep learning literature. Our previous work [28] learned identity features by using CNN to recover a
single frontal view face image  which is a special case of MVP after removing the random neurons.
[28] did not learn the view representation as we do. Experimental results show that our approach not
only provides rich multi-view representation but also learns better identity features compared with

2

[28]. Fig. 1 shows examples that different persons may look similar in the front view  but are better
distinguished in other views. Thus it improves the performance of face recognition signiﬁcantly.
More recently  Reed et al. [20] untangled factors of image variation by using a high-order Boltzmann
machine  where all the neurons are stochastic and it is solved by gibbs sampling. MVP contains both
stochastic and deterministic neurons and thus can be efﬁciently solved by back-propagation.

2 Multi-View Perceptron

image pairs  I = {xij 
The training data is a set of
(yik  vik)}N M M
i=1 j=1 k=1  where xij is the input image of the i-
th identity under the j-th view  yik denotes the output image of
the same identity in the k-th view  and vik is the view label of
the output. vik is a M dimensional binary vector  with the k-th
element as 1 and the remaining zeros. MVP is learned from the
training data such that given an input x  it can output images y
of the same identity in different views and their view labels v.
Then  the output v and y are generated as1 

v = F (y  hv; Θ)  y = F (x  hid  hv  hr; Θ) +  

(1)

Network

Figure 2:
structure
of MVP  which has
six layers 
including three layers with only
the deterministic neurons (i.e.
the
layers parameterized by the weights
of U0  U1  U4)  and three layers
with both the deterministic and
random neurons (i.e. the weights of
U2  V2  W2  U3  V3  U5  W5).
This structure is used throughout
the experiments.

where F is a non-linear function and Θ is a set of weights and
biases to be learned. There are three types of hidden neurons 
hid  hv  and hr  which respectively extract identity features 
view features  and the features to reconstruct the output face
image.  signiﬁes a noise variable.
Fig. 2 shows the architecture2 of MVP  which is a directed
graphical model with six layers  where the nodes with and
without ﬁlling represent the observed and hidden variables  and
the nodes in green and blue indicate the deterministic and random
neurons  respectively. The generation process of y and v starts
from x  ﬂows through the neurons that extract identity feature
hid  which combines with the hidden view representation hv to
yield the feature hr for face recovery. Then  hr generates y.
Meanwhile  both hv and y are united to generate v. hid and hr are the deterministic binary hidden
neurons  while hv are random binary hidden neurons sampled from a distribution q(hv). Different
sampled hv generates different y  making the perception of multi-view possible. hv usually has a
low dimensionality  approximately ten  as ten binary neurons can ideally model 210 distinct views.
For clarity of derivation  we take an example of MVP that contains only one hidden layer of hid
and hv. More layers can be added and derived in a similar fashion. We consider a joint distribution 
which marginalizes out the random hidden neurons 
p(y  v  hv|hid; Θ) =

p(v |y  hv; Θ)p(y|hid  hv; Θ)p(hv) 

p(y  v |hid; Θ) =

(cid:88)

(cid:88)

(2)

hv

hv

where Θ = {U0  U1  V1  U2  V2}  the identity feature is extracted from the input image  hid =
f (U0x)  and f is the sigmoid activation function  f (x) = 1/(1 + exp(−x)). Other activation
functions  such as rectiﬁed linear function [18] and tangent [11]  can be used as well. To model
continuous values of the output  we assume y follows a conditional diagonal Gaussian distribution 
p(y|hid  hv; Θ) = N (y|U1hid + V1hv  σ2
y). The probability of y belonging to the j-th view
is modeled with the softmax function  p(vj = 1|y  hv; Θ) =
k∗hv)  where Uj∗
indicates the j-th row of the matrix.

exp(U2
k=1 exp(U2

j∗hv)
k∗y+V2

(cid:80)K

j∗y+V2

1The subscripts i  j  k are omitted for clearness.
2For clarity  the biases are omitted.

3

2.1 Learning Procedure

The weights and biases of MVP are learned by maximizing the data log-likelihood. The lower bound
of the log-likelihood can be written as 

p(y  v  hv|hid; Θ) ≥(cid:88)

q(hv) log

p(y  v  hv|hid; Θ)

.

(3)

log p(y  v |hid; Θ) = log

(cid:88)
+ (cid:80)

hv

S(cid:88)

s=1

(cid:88)

hv

hv

q(hv)

q(hv)

−(cid:80)

hv q(hv) log p(hv|y v;Θ)

hv q(hv) log p(y v hv|hid;Θ)

Eq.(3) is attained by decomposing the log-likelihood into two terms 

q(hv)
log p(y  v |hid; Θ) =
  which can be easily veriﬁed by
substituting the product  p(y  v  hv|hid) = p(y  v |hid)p(hv|y  v)  into the right hand side of the
decomposition. In particular  the ﬁrst term is the KL-divergence [10] between the true posterior
and the distribution q(hv). As KL-divergence is non-negative  the second term is regarded as the
variational lower bound on the log-likelihood.
The above lower bound can be maximized by using the Monte Carlo Expectation Maximization
(MCEM) algorithm recently introduced by [27]  which approximates the true posterior by using the
importance sampling with the conditional prior as the proposal distribution. With the Bayes’ rule  the
true posterior of MVP is p(hv|y  v) = p(y v |hv)p(hv)
  where p(y  v |hv) represents the multi-view
perception error  p(hv) is the prior distribution over hv  and p(y  v) is a normalization constant.
Since we do not assume any prior information on the view distribution  p(hv) is chosen as a uniform
distribution between zero and one. To estimate the true posterior  we let q(hv) = p(hv|y  v; Θold).
It is approximated by sampling hv from the uniform distribution  i.e. hv ∼ U(0  1)  weighted by the
importance weight p(y  v |hv; Θold). With the EM algorithm  the lower bound of the log-likelihood
turns into

p(y v)

L(Θ  Θold) =

p(hv|y  v; Θold) log p(y  v  hv|hid; Θ) (cid:39) 1
S

ws log p(y  v  hv

s|hid; Θ) 

(4)
where ws = p(y  v |hv; Θold) is the importance weight. The E-step samples the random hidden
neurons  i.e. hv

s ∼ U(0  1)  while the M-step calculates the gradient 
S(cid:88)

∂L(Θ  Θold)

S(cid:88)

{log p(v |y  hv

s) + log p(y|hid  hv

s)} 

(5)

∂L
∂Θ

(cid:39) 1
S

=

1
S

ws

∂
∂Θ

s=1

s=1

∂Θ

where the gradient is computed by averaging over all the gradients with respect to the importance
samples.
The two steps have to be iterated. When more samples are needed to estimate the posterior  the
space complexity will increase signiﬁcantly  because we need to store a batch of data  the proposed
samples  and their corresponding outputs at each layer of the deep network. When implementing the
algorithm with GPU  one needs to make a tradeoff between the size of the data and the accurateness
of the approximation  if the GPU memory is not sufﬁcient for large scale training data. Our empirical
study (Sec. 3.1) shows that the M-step of MVP can be computed by using only one sample  because
the uniform prior typically leads to sparse weights during training. Therefore  the EM process
develops into the conventional back-propagation.
In the forward pass  we sample a number of hv
s based on the current parameters Θ  such that only
the sample with the largest weight need to be stored. We demonstrate in the experiment (Sec. 3.1)
that a small number of times (e.g. < 20) are sufﬁcient to ﬁnd good proposal. In the backward pass 
we seek to update the parameters by the gradient 

∂L(Θ)
∂Θ

(cid:39) ∂
∂Θ

s)(cid:1)(cid:9) 

(cid:8)ws
(cid:0) log p(v |y  hv
s) + log p(y|hid  hv
s) = − log σy −(cid:107)(cid:98)y−(U1hid+V1hv
)  where(cid:98)y and(cid:98)v are the ground truth.

s )(cid:107)2

where hv
lowing two terms  log p(y|hid  hv

s is the sample that has the largest weight ws. We need to optimize the fol-
s) =
(cid:80)K

and log p(v |y  hv

(cid:80)
j(cid:98)vj log(

j∗y+V2

j∗hv
s )
k∗y+V2
k∗hv
s )

exp(U2
k=1 exp(U2

2σ2
y

2

(6)

4

• Continuous View In the previous discussion  v is assumed to be a binary vector. Note that v can
also be modeled as a continuous variable with a Gaussian distribution 
p(v |y  hv) = N (v |U2y + V2hv  σv) 

(7)

In this case  we can

where v is a scalar corresponding to different views from −90◦ to +90◦.
generate views not presented in the training data by interpolating v  as shown in Fig. 6.
• Difference with multi-task learning Our model  which only has a single task  is also different
from multi-task learning (MTL)  where reconstruction of each view could be treated as a different
task  although MTL has not been used for multi-view reconstruction in literature to the best of our
knowledge. In MTL  the number of views to be reconstructed is predeﬁned  equivalent to the number
of tasks  and it encounters problems when the training data of different views are unbalanced;
while our approach can sample views continuously and generate views not presented in the training
data by interpolating v as described above. Moreover  the model complexity of MTL increases
as the number of views and its training is more difﬁcult since different tasks may have difference
convergence rates.

2.2 Testing Procedure

Given the view label v  and the input x  we generate the face image y under the viewpoint of v in
s=1 ∼ U(0  1)  which corresponds to a set of
the testing stage. A set of hv are ﬁrst sampled  {hv
outputs {ys}S
s=1. For example  in a simple network with only one hidden layer  ys = U1hid+V1hv
s
and hid = f (U0x). Then  the desired face image in view v is the output ys that produces the
largest probability of p(v |ys  hv
s). A full spectrum of multi-view images are reconstructed for all
the possible view labels v.

s}S

2.3 View Estimation

Our model can also be used to estimate viewpoint of the input image x. First  given all possible
values of viewpoint v  we can generate a set of corresponding output images {yz}  where z
indicates the index of the values of view we generated (or interpolated). Then  to estimate
viewpoint  we assign the view label of the z-th output yz to x  such that yz is the most similar
image to x. The above procedure is formulated as below.
If v is discrete  the problem is 
z) − p(vj = 1|yz  hv
arg minj z (cid:107) p(vj = 1|x  hv
(cid:80)K
(cid:107)2
2. If v is continuous  the problem is deﬁned as  arg minz (cid:107) (U2x +
−
exp(U2
k=1 exp(U2
2 = arg minz (cid:107) x − yz (cid:107)2
z) (cid:107)2
z) − (U2yz + V2hv
2.
V2hv

2 = arg minj z (cid:107)

j∗hv
z )
k∗x+V2
k∗hv
z )

j∗yz+V2

j∗hv
z )
k∗yz+V2
k∗hv
z )

z) (cid:107)2

(cid:80)K

j∗x+V2

exp(U2
k=1 exp(U2

3 Experiments

Several experiments are designed for evaluation and comparison3. In Sec. 3.1  MVP is evaluated
on a large face recognition dataset to demonstrate the effectiveness of the identity representation.
Sec. 3.2 presents a quantitative evaluation  showing that the reconstructed face images are in good
quality and the multi-view spectrum has retained discriminative information for face recognition.
Sec. 3.3 shows that MVP can be used for view estimation and achieves comparable result as the
discriminative methods specially designed for this task. An interesting experiment in Sec. 3.4
shows that by modeling the view as a continuous variable  MVP can analyze and reconstruct views
not seen in the training data.

3.1 Multi-View Face Recognition

MVP on multi-view face recognition is evaluated on the MultiPIE dataset [7]  which contains
754  204 images of 337 identities. Each identity was captured under 15 viewpoints from −90◦
to +90◦ and 20 different illuminations. It is the largest and most challenging dataset for evaluating
face recognition under view and lighting variations. We conduct the following three experiments to
demonstrate the effectiveness of MVP.

3http://mmlab.ie.cuhk.edu.hk/projects/MVP.htm. For more technical details of this work 

please contact the corresponding author Ping Luo (pluo.lhi@gmail.com).

5

• Face recognition across views This setting follows the existing methods  e.g. [2  12  28]  which
employs the same subset of MultiPIE that covers images from −45◦ to +45◦ and with neutral
illumination. The ﬁrst 200 identities are used for training and the remaining 137 identities for test.
In the testing stage  the gallery is constructed by choosing one canonical view image (0◦) from each
testing identity. The remaining images of the testing identities from −45◦ to +45◦ are selected as
probes. The number of neurons in MVP can be expressed as 32 × 32 − 512 − 512(10) − 512(10) −
1024− 32× 32[7]  where the input and output images have the size of 32× 32  [7] denotes the length
of the view label vector (v)  and (10) represents that the third and forth layers have ten random
neurons.
We examine the performance of using the identity features  i.e. hid
)  and
compare it with seven state-of-the-art methods in Table 1. The ﬁrst three methods are based on
3D face models and the remaining ones are 2D feature extraction methods  including deep models 
such as FIP [28] and RL [28]  which employed the traditional convolutional network to recover
the frontal view face image. As the existing methods did  LDA is applied to all the 2D methods
to reduce the features’ dimension. The ﬁrst and the second best results are highlighted for each
viewpoint  as shown in Table 1. The two deep models (MVP and RL) outperform all the existing
methods  including the 3D face models. RL achieves the best results on three viewpoints  whilst
MVP is the best on four viewpoints. The extracted feature dimensions of MVP and RL are 512
and 9216  respectively. In summary  MVP obtains comparable averaged accuracy as RL under this
setting  while the learned feature representation is more compact.

2 (denoted as MVPhid

2

Table 1: Face recognition accuracies across views. The ﬁrst and the second best performances are in bold.

VAAM [2]
FA-EGFC [12]
SA-EGFC [12]
LE [3]+LDA
CRBM [9]+LDA
FIP [28]+LDA
RL [28]+LDA
MVPhid

+LDA

2

Avg.
86.9
92.7
97.2
93.2
87.6
95.6
98.3
98.1

−15◦ +15◦ −30◦ +30◦ −45◦ +45◦
74.8
95.7
85.2
99.3
93.6
99.7
81.8
99.9
94.9
75.2
100.0
89.8
97.8
100.0
100.0
95.6

95.7
99.0
99.7
99.7
96.4
98.5
99.3
100.0

89.5
92.9
98.3
95.5
88.3
96.4
98.5
100.0

91.0
95.0
98.7
95.5
90.5
95.6
98.5
99.3

74.1
84.7
93.0
86.9
80.3
93.4
95.6
93.4

Table 2: Face recognition accuracies across views and illuminations. The ﬁrst and the second best
performances are in bold.

Raw Pixels+LDA
LBP [1]+LDA
Landmark LBP [4]+LDA
CNN+LDA
FIP [28]+LDA
RL [28]+LDA
MTL+RL+LDA
MVPhid
+LDA
+LDA
MVPhid
2
+LDA
MVPhr
3
MVPhr
+LDA
4

1

Avg.
36.7
50.2
63.2
58.1
72.9
70.8
74.8
61.5
79.3
72.6
62.3

0◦
81.3
89.1
94.9
64.6
94.3
94.3
93.8
92.5
95.7
91.0
83.4

−15◦ +15◦ −30◦ +30◦ −45◦ +45◦ −60◦ +60◦
7.63
59.2
14.6
77.4
83.9
32.1
44.2
66.2
42.5
91.4
38.9
90.5
91.7
50.2
85.4
28.3
60.0
93.3
56.0
86.7
77.3
46.9

12.8
16.2
35.5
46.4
49.3
44.6
51.5
35.1
60.2
55.7
44.4

19.7
29.7
48.3
57.9
62.0
59.5
63.8
45.4
70.6
63.8
53.2

37.3
55.9
68.2
63.6
82.5
80.0
83.3
67.0
83.9
74.2
63.9

58.3
79.1
82.9
62.8
90.0
89.8
89.6
84.9
92.2
84.1
73.1

21.0
35.2
52.8
56.4
66.1
63.6
70.4
51.6
75.2
68.5
57.3

35.5
56.8
71.4
60.7
78.9
77.5
80.1
64.3
83.4
74.6
62.0

• Face recognition across views and illuminations To examine the robustness of different feature
representations under more challenging conditions  we extend the ﬁrst setting by employing a
larger subset of MultiPIE  which contains images from −60◦ to +60◦ and 20 illuminations. Other
experimental settings are the same as the above. In Table 2  feature representations of different
layers in MVP are compared with seven existing features  including raw pixels  LBP [1] on image
grid  LBP on facial landmarks [4]  CNN features  FIP [28]  RL [28]  and MTL+RL. LDA is applied
to all the feature representations. Note that the last four methods are built on the convolutional
neural networks. The only distinction is that they adopted different objective functions to learn
features. Speciﬁcally  CNN uses cross-entropy loss to classify face identity as in [26]. FIP and
RL utilized least-square loss to recover the frontal view image. MTL+RL is an extension of RL.
It employs multiple tasks  each of which is formulated as a least square loss  to recover multi-view
images  and all the tasks share feature layers. To achieve fair comparisons  CNN  FIP  and MTL+RL
adopt the same convolutional structure as RL [28]  since RL achieves competitive results in our ﬁrst
experiment.

6

2 > hr

3 and hr

4  because some randomly generated view factors (hv

3 > hid

1 > hr

4  which conforms our expectation. hid
2 performs better than hid

The ﬁrst and second best results are emphasized in bold in Table 2. The identity feature hid
2 of
MVP outperforms all the other methods on all the views with large margins. MTL+RL achieves
the second best results except on ±60◦. These results demonstrate the superior of modeling multi-
view perception. For the features at different layers of MVP  the performance can be summarized
as hid
2 performs the best because it is
1 because pose factors coupled in
the highest level of identity features. hid
1 to hid
the input image x have be further removed  after one more forward mapping from hid
2 . hid
2
also outperforms hr
2 and hv
3) have been
incorporated into these two layers during the construction of the full view spectrum. Please refer to
Fig. 2 for a better understanding.
• Effectiveness of the BP Procedure
Fig. 3 (a) compares the
convergence rates dur-
ing training  when using
different number of sam-
ples to estimate the true
posterior. We observe
that a few number of
samples  such as twenty 
can lead to reasonably
good convergence. Fig.
3 (b) empirically shows
that uniform prior leads
to sparse weights during
training. In other words 
if we seek to calculate the gradient of BP using only one sample  as did in Eq.(6). Fig. 3
(b) demonstrates that 20 samples are sufﬁcient  since only 6 percent of the samples’ weights
approximate one (all the others are zeros). Furthermore  as shown in Fig. 3 (c)  the convergence
rates of the one-sample gradient and the weighted summation are comparable.

Figure 3: Analysis of MVP on the MultiPIE dataset.
(a) Comparison of
convergence  using different number of samples to estimate the true posterior. (b)
Comparison of sparsity of the samples’ weights. (c) Comparison of convergence 
using the largest weighted sample and using the weighted average over all the
samples to compute gradient.

3.2 Reconstruction Quality

Another experiment is designed to quantitatively evaluate the multi-
view reconstruction result. The setting is the same as the ﬁrst
experiment in Sec. 3.1. The gallery images are all in the frontal view
(0◦). Differently  LDA is applied to the raw pixels of the original
images (OI) and the reconstructed images (RI) under the same view 
respectively. Fig. 4 plots the accuracies of face recognition with
respect to distinct viewpoints. Not surprisingly  under the viewpoints
of +30◦ and −45◦ the accuracies of RI are decreased compared to
OI. Nevertheless  this decrease is comparatively small (< 5%). It
implies that the reconstructed images are in reasonably good quality.
We notice that the reconstructed images in Fig. 1 lose some detailed
textures  while well preserving the shapes of proﬁle and the facial
components.

3.3 Viewpoint Estimation

Figure 4: Face recognition ac-
curacies. LDA is applied to the
raw pixels of the original im-
ages and the reconstructed im-
ages.

This experiment is conducted to evaluate the performance of
viewpoint estimation. MVP is compared to Linear Regression
(LR) and Support Vector Regression (SVR)  both of which have
been used in viewpoint estimation  e.g. [8  13]. Similarly  we
employ the ﬁrst setting as introduced in Sec. 3.1  implying that
we train the models using images of a set of identities  and then
estimate poses of the images of the remaining identities. For
training LR and SVR  the features are obtained by applying PCA on the raw image pixels. Fig. 5
reports the view estimation errors  which are measured by the differences between the pose degrees

Figure 5: Errors of view estima-
tion.

7

0246810120°+15°−15°+30°−30°+45°−45°ViewpointError of view estimation (in degree) LRSVRMVPFigure 6: We adopt the images in 0◦  30◦  and 60◦ for training  and test whether MVP can analyze and
reconstruct images under 15◦ and 45◦. The reconstructed images (left) and the ground truths (right) are shown
in (a). (b) visualizes the full spectrum of the reconstructed images  when the images in unobserved views are
used as inputs (ﬁrst column).

of ground truth and the predicted degrees. The averaged errors of MVP  LR  and SVR are 5.03◦ 
9.79◦  and 5.45◦  respectively. MVP achieves slightly better results compared to the discriminative
model  i.e. SVR  demonstrating that it is also capable for view estimation  even though it is not
designated for this task.

3.4 Viewpoint Interpolation

When the viewpoint is modeled as a continuous variable as described in Sec. 2.1  MVP implicitly
captures a 3D face model  such that it can analyze and reconstruct images under viewpoints that have
not been seen before  while this cannot be achieved with MTL. In order to verify such capability  we
conduct two tests. First  we adopt the images from MultiPIE in 0◦  30◦  and 60◦ for training  and test
whether MVP can generate images under 15◦ and 45◦. For each testing identity  the result is obtained
by using the image in 0◦ as input and reconstructing images in 15◦ and 45◦. Several synthesized
images (left) compared with the ground truth (right) are visualized in Fig. 6 (a). Although the
interpolated images have noise and blurring effect  they have similar views as the ground truth and
more importantly  the identity information is preserved. Second  under the same training setting as
above  we further examine  when the images of the testing identities in 15◦ and 45◦ are employed as
inputs  whether MVP can still generate a full spectrum of multi-view images and preserve identity
information in the meanwhile. The results are illustrated in Fig. 6 (b)  where the ﬁrst image is the
input and the remaining are the reconstructed images in 0◦  30◦  and 60◦.
These two experiments show that MVP essentially models a continuous space of multi-view images
such that ﬁrst  it can predict images in unobserved views  and second  given an image under an
unseen viewpoint  it can correctly extract identity information and then produce a full spectrum of
multi-view images. In some sense  it performs multi-view reasoning  which is an intriguing function
of human brain.
4 Conclusions

In this paper  we have presented a generative deep network  called Multi-View Perceptron (MVP)  to
mimic the ability of multi-view perception in primate brain. MVP can disentangle the identity and
view representations from an input image  and also can generate a full spectrum of views of the input
image. Experiments demonstrated that the identity features of MVP achieve better performance on
face recognition compared to state-of-the-art methods. We also showed that modeling the view
factor as a continuous variable enables MVP to interpolate and predict images under the viewpoints 
which are not observed in training data  imitating the reasoning capacity of human.
Acknowledgement This work is partly supported by Natural Science Foundation of China (91320101 
61472410)  Shenzhen Basic Research Program (JCYJ20120903092050890  JCYJ20120617114614438  J-
CYJ20130402113127496)  Guangdong Innovative Research Team Program (201001D0104648280).

References
[1] T. Ahonen  A. Hadid  and M. Pietikainen. Face description with local binary patterns:

Application to face recognition. TPAMI  28:2037–2041  2006.

[2] A. Asthana  T. K. Marks  M. J. Jones  K. H. Tieu  and M. Rohith. Fully automatic pose-

invariant face recognition via 3d pose normalization. In ICCV  2011.

8

[3] Z. Cao  Q. Yin  X. Tang  and J. Sun. Face recognition with learning-based descriptor. In CVPR 

2010.

[4] D. Chen  X. Cao  F. Wen  and J. Sun. Blessing of dimensionality: High-dimensional feature

and its efﬁcient compression for face veriﬁcation. In CVPR  2013.

[5] W. A. Freiwald and D. Y. Tsao. Functional compartmentalization and viewpoint generalization

within the macaque face-processing system. Science  330(6005):845–851  2010.

[6] D. Gonz´alez-Jim´enez and J. L. Alba-Castro. Toward pose-invariant 2-d face recognition
IEEE Transactions on Information

through point distribution models and facial symmetry.
Forensics and Security  2:413–429  2007.

[7] R. Gross  I. Matthews  J. F. Cohn  T. Kanade  and S. Baker. Multi-pie. In Image and Vision

Computing  2010.

[8] Y. Hu  L. Chen  Y. Zhou  and H. Zhang. Estimating face pose by facial asymmetry and

geometry. In AFGR  2004.

[9] G. B. Huang  H. Lee  and E. Learned-Miller. Learning hierarchical representations for face

veriﬁcation with convolutional deep belief networks. In CVPR  2012.

[10] S. Kullback and R. A. Leibler. On information and sufﬁciency. In Annals of Mathematical

Statistics  1951.

[11] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient based learning applied to document

recognition. Proceedings of IEEE  86(11):2278–2324  1998.

[12] S. Li  X. Liu  X. Chai  H. Zhang  S. Lao  and S. Shan. Morphable displacement ﬁeld based

image matching for face recognition across pose. In ECCV  2012.

[13] Y. Li  S. Gong  and H. Liddell. Support vector regression and classiﬁcation based multi-view

face detection and recognition. In AFGR  2000.

[14] C. Liu and H. Wechsler. Gabor feature based classiﬁcation using the enhanced ﬁsher linear

discriminant model for face recognition. TIP  11:467–476  2002.

[15] D. G. Lowe. Distinctive image features from scale-invariant keypoints.

2004.

IJCV  60:91–110 

[16] P. Luo  X. Wang  and X. Tang. Hierarchical face parsing via deep learning. In CVPR  2012.
[17] P. Luo  X. Wang  and X. Tang. A deep sum-product architecture for robust facial attributes

analysis. In ICCV  2013.

[18] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In

ICML  2010.

[19] S. Ohayon  W. A. Freiwald  and D. Y. Tsao. What makes a cell face selective? the importance

of contrast. Neuron  74:567–581  2013.

[20] S. Reed  K. Sohn  Y. Zhang  and H. Lee. Learning to disentangle factors of variation with

manifold interaction. In ICML  2014.

[21] K. Simonyan  O. M. Parkhi  A. Vedaldi  and A. Zisserman. Fisher vector faces in the wild. In

BMVC  2013.

[22] Y. Sun  X. Wang  and X. Tang. Hybrid deep learning for face veriﬁcation. In ICCV  2013.
[23] Y. Sun  X. Wang  and X. Tang. Deep convolutional network cascade for facial point detection.

In CVPR  2013.

[24] Y. Sun  Y. Chen  X. Wang  and X. Tang. Deep learning face representation by joint

identiﬁcation-veriﬁcation. In NIPS  2014.

[25] Y. Sun  X. Wang  and X. Tang. Deep learning face representation from predicting 10 000

classes. In CVPR  2014.

[26] Y. Taigman  M. Yang  M. A. Ranzato  and L. Wolf. Deepface: Closing the gap to human-level

performance in face veriﬁcation. In CVPR  2014.

[27] Y. Tang and R. Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS 

2013.

[28] Z. Zhu  P. Luo  X. Wang  and X. Tang. Deep learning identity preserving face space. In ICCV 

2013.

9

,Zhenyao Zhu
Ping Luo
Xiaogang Wang
Xiaoou Tang
Yu-Xiong Wang
Martial Hebert
Xueting Li
Sifei Liu
Shalini De Mello
Xiaolong Wang
Jan Kautz
Ming-Hsuan Yang