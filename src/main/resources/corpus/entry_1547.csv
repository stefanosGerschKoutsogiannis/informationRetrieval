2015,Discriminative Robust Transformation Learning,This paper proposes a framework for learning features that are robust to data variation  which is particularly important when only a limited number of trainingsamples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K  \epsilon)-robustness  thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization frameworkis used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets  such as labeled faces in the wild demonstrate the value of being able to balance discrimination and robustness.,Discriminative Robust Transformation Learning

Jiaji Huang

Qiang Qiu

Guillermo Sapiro

Robert Calderbank

Department of Electrical Engineering  Duke University

{jiaji.huang qiang.qiu guillermo.sapiro robert.calderbank}@duke.edu

Durham  NC 27708

Abstract

This paper proposes a framework for learning features that are robust to data vari-
ation  which is particularly important when only a limited number of training
samples are available. The framework makes it possible to tradeoff the discrim-
inative value of learned features against the generalization error of the learning
algorithm. Robustness is achieved by encouraging the transform that maps data
to features to be a local isometry. This geometric property is shown to improve
(K  )-robustness  thereby providing theoretical justiﬁcation for reductions in gen-
eralization error observed in experiments. The proposed optimization framework
is used to train standard learning algorithms such as deep neural networks. Exper-
imental results obtained on benchmark datasets  such as labeled faces in the wild 
demonstrate the value of being able to balance discrimination and robustness.

1

Introduction

Learning features that are able to discriminate is a classical problem in data analysis. The basic idea
is to reduce the variance within a class while increasing it between classes. One way to implement
this is by regularizing a certain measure of the variance  while assuming some prior knowledge
about the data. For example  Linear Discriminant Analysis (LDA) [4] measures sample covariance
and implicitly assumes that each class is Gaussian distributed. The Low Rank Transform (LRT) [10] 
instead uses nuclear norm to measure the variance and assumes that each class is near a low-rank
subspace. A different approach is to regularize the pairwise distances between data points. Examples
include the seminal work on metric learning [17] and its extensions [5  6  16].
While great attention has been paid to designing objectives to encourage discrimination  less effort
has been made in understanding and encouraging robustness to data variation  which is especially
important when a limited number of training samples are available. One exception is [19]  which
promotes robustness by regularizing the traditional metric learning objective using prior knowledge
from an auxiliary unlabeled dataset.
In this paper we develop a general framework for balancing discrimination and robustness. Robust-
ness is achieved by encouraging the learned data-to-features transform to be locally an isometry
within each class. We theoretically justify this approach using (K  )-robustness [1  18] and give an
example of the proposed formulation  incorporating it in deep neural networks. Experiments val-
idate the capability to trade-off discrimination against robustness. Our main contributions are the
following: 1) prove that locally near isometry leads to robustness; 2) propose a practical framework
that allows to robustify a wide class of learned transforms  both linear and nonlinear; 3) provide
an explicit realization of the proposed framework  achieving competitive results on difﬁcult face
veriﬁcation tasks.
The paper is organized as follows. Section 2 motivates the proposed study and proposes a general
formulation for learning a Discriminative Robust Transform (DRT). Section 3 provides a theoretical
justiﬁcation for the framework by making an explicit connection to robustness. Section 4 gives a

1

speciﬁc example of DRT  denoted as Euc-DRT. Section 5 provides experimental validation of Euc-
DRT  and section 6 presents conclusions. 1

2 Problem Formulation
Consider an L-way classiﬁcation problem. The training set is denoted by T = {(xi  yi)}  where
xi ∈ Rn is the data and yi ∈ {1  . . .   L} is the class label. We want to learn a feature transform
fα(·) such that a datum x becomes more discriminative when it is transformed to feature fα(x).
The transform fα is parametrized by a vector α  a framework that includes linear transforms and
neural networks where the entries of α are the learned network parameters.

2.1 Motivation

The transform fα promotes discriminability by reducing intra-class variance and enlarging inter-
class variance. This aim is expressed in the design of objective functions [5  10] or the structure
of the transform fα [7  11]. However the robustness of the learned transform is an important issue
that is often overlooked. When training samples are scarce  statistical learning theory [15] predicts
overﬁtting to the training data. The result of overﬁtting is that discrimination achieved on test data
will be signiﬁcantly worse than that on training data. Our aim in this paper is the design of robust
transforms fα for which the training-to-testing degradation is small [18].
We formally measure robustness of the learned transform fα in terms of (K  )-robustness [1].
Given a distance metric ρ  a learning algorithm is said to be (K  )-robust if the input data space
can be partitioned into K disjoint sets Sk  k = 1  ...  K  such that for all training sets T   the learned
parameter αT determines a loss for which the value on pairs of training samples taken from different
sets Sj and Sk is very close to the value of any pair of data samples taken from Sj and Sk.
(K  )-robustness is illustrated in Fig. 1  where S1 and S2 are both of diameter γ and
If the transform fα preserves all distances within S1 and S2  then |e− e(cid:48)| cannot deviate much from
|d − d(cid:48)| ≤ 2γ.

|e − e(cid:48)| = |ρ(fα(x1)  fα(x2)) − ρ(fα(x(cid:48)

1)  fα(x(cid:48)

2))|.

Figure 1: (K  )-robustness: Here d = ρ(x1  x2)  d(cid:48) = ρ(x(cid:48)
e(cid:48) = ρ(fα(x(cid:48)

2)). The difference |e − e(cid:48)| cannot deviate too much from |d − d(cid:48)|.

1)  fα(x(cid:48)

2)  e = ρ(fα(x1)  fα(x2))  and

1  x(cid:48)

2.2 Formulation and Discussion

(cid:26) 1

−1

1
|P|

(cid:88)

Motivated by the above reasoning  we now present our proposed framework. First we deﬁne a pair
if yi = yj
label (cid:96)i j (cid:44)
otherwise . Given a metric ρ  we use the following hinge loss to encourage
high inter-class distance and small intra-class distance.

max{0  (cid:96)i j [ρ (fα(xi)  fα(xj)) − t((cid:96)i j)]}  

(1)
Here P = {(i  j|i (cid:54)= j)} is the set of all data pairs. t((cid:96)i j) ≥ 0 is a function of (cid:96)i j and t(1) < t(−1).
Similar to metric learning [17]  this loss function connects pairwise distance to discrimination. How-
ever traditional metric learning typically assumes squared Euclidean distance and here the metric ρ
can be arbitrary.
For robustness  as discussed above  we may want fα(·) to be distance-preserving within each small
local region. In particular  we deﬁne the set of all local neighborhoods as

i j∈P

NB (cid:44) {(i  j)|(cid:96)i j = 1  ρ(xi  xj) ≤ γ} .

1A note on the notations: matrices (vectors) are denoted in upper (lower) case bold letters. Scalars are

denoted in plain letters.

2

(cid:88)

1
|NB|

(i j)∈NB

Therefore  we minimize the following objective function

|ρ(fα(xi)  fα(xj)) − ρ(xi  xj)| .

(2)

Note that we do not need to have the same metric in both the input and the feature space  they do not
even have in general the same dimension. With a slight abuse of notation we use the same symbol
to denote both metrics.
To achieve discrimination and robustness simultaneously  we formulate the objective function as a
weighted linear combination of the two extreme cases in (1) and (2)
λ
|P|

max{0  (cid:96)i j [ρ (fα(xi)  fα(xj)) − t((cid:96)i j)]}+

|ρ(fα(xi)  fα(xj)) − ρ(xi  xj)|

1 − λ
|NB|

(cid:88)

(cid:88)

i j∈P

(i j)∈NB

(3)
where λ ∈ [0  1]. The formulation (3) balances discrimination and robustness. When λ = 1 it seeks
discrimination  and as λ decreases it starts to encourage robustness. We shall refer to a transform
that is learned by solving (3) as a Discriminative Robust Transform (DRT). The DRT framework
provides opportunity to select both the distance measure and the transform family.

3 Theoretical Analysis

In this section  we provide a theoretical explanation for robustness. In particular  we show that if the
solution to (1) yields a transform fα that is locally a near isometry  then fα is robust.

3.1 Theoretical Framework
Let X denote the original data  let Y = {1  ...  L} denote the set of class labels  and let Z = X ×Y.
The training samples are pairs zi = (xi  yi)  i = 1  . . .   n drawn from some unknown distribution
D deﬁned on Z. The indicator function is deﬁned as (cid:96)i j = 1 if yi = yj and −1 otherwise. Let
fα be a transform that maps a low-level feature x to a more discriminative feature fα(x)  and let F
denote the space of transformed features.
For simplicity we consider an arbitrary metric ρ deﬁned on both X and F (the general case of
different metrics is a straightforward extension)  and a loss function g(ρ(fα(xi)  fα(xj))  (cid:96)i j) that
encourages ρ(fα(xi)  fα(xj)) to be small (big) if (cid:96)i j = 1 (−1). We shall require the Lipschtiz
constant of g(·  1) and g(· −1) to be upper bounded by A > 0. Note that the loss function in Eq. (1)
has a Lipschtiz constant of 1. We abbreviate

g(ρ(fα(xi)  fα(xj))  (cid:96)i j) (cid:44) hα(zi  zj).

The empirical loss on the training set is a function of α given by

Remp(α) (cid:44) 2
n(n−1)
and the expected loss on the test data is given by
R(α) (cid:44) Ez(cid:48)
1 z(cid:48)

(cid:80)n
2∼D [hα(z(cid:48)

i(cid:54)=j

i j=1

hα(zi  zj) 

1  z(cid:48)

2)] .

The algorithm operates on pairs of training samples and ﬁnds parameters

(6)
that minimize the empirical loss on the training set T . The difference Remp − R between expected
loss on the test data and empirical loss on the training data is the generalization error of the algorithm.

Remp(α) 

α

αT (cid:44) arg min

3.2

(K  )-robustness and Covering Number

We work with the following deﬁnition of (K  )-robustness [1].
Deﬁnition 1. A learning algorithm is (K  )-robust if Z = X ×Y can be partitioned into K disjoint
sets Zk  k = 1  . . .   K such that for all training sets T ∈ Z n  the learned parameter αT determines
a loss function where the value on pairs of training samples taken from sets Zp and Zq is “very
close” to the value of any pair of data samples taken from Zp and Zq. Formally 
assume zi  zj ∈ T   with zi ∈ Zp and zj ∈ Zq  if z(cid:48)

i ∈ Zp and z(cid:48)

j ∈ Zq  then

(cid:12)(cid:12)hαT (zi  zj) − hαT (z(cid:48)

j)(cid:12)(cid:12) ≤ .

i  z(cid:48)

3

(4)

(5)

j) in Zp × Zq is
Remark 1. (K  )-robustness means that the loss incurred by a testing pair (z(cid:48)
very close to the loss incurred by any training pair (zi  zj) in Zp × Zq. It is shown in [1] that the
generalization error of (K  )-robust algorithms is bounded as

i  z(cid:48)

R(αT ) − Remp(αT ) ≤  + O

.

(7)

(cid:32)(cid:114)

(cid:33)

K
n

Therefore the smaller   the smaller is the generalization error  and the more robust is the learning
algorithm.

Given a metric space  the covering number speciﬁes how many balls of a given radius are needed to
cover the space. The more complex the metric space  the more balls are needed to cover it. Covering
number is formally deﬁned as follows.
Deﬁnition 2 (Covering number). Given a metric space (S  ρ)  we say that a subset ˆS of S is a
γ-cover of S  if for every element s ∈ S  there exists ˆs ∈ ˆS such that ρ(s  ˆs) ≤ γ. The γ-covering
number of S is

Nγ(S  ρ) = min{| ˆS| : ˆS is a γ-cover of S}.

Remark 2. The covering number is a measure of the geometric complexity of (S  ρ). A set S with
covering number Nγ/2(S  ρ) can be partitioned into Nγ/2(S  ρ) disjoint subsets  such that any two
points within the same subset are separated by no more than γ.
Lemma 1. The metric space Z = X × Y can be partitioned into LNγ/2(X   ρ) subsets  denoted
as Z1  . . .  ZLNγ/2(X  ρ)  such that any two points z1 (cid:44) (x1  y1)  z2 (cid:44) (x2  y2) in the same subset
satisfy y1 = y2 and ρ(x1  x2) ≤ γ.
Proof. Assuming the metric space (X   ρ) is compact  we can partition X into Nγ/2(X   ρ) subsets 
each with diameter at most γ. Since Y is a ﬁnite set of size L  we can partition Z = X × Y into
LNγ/2(X   ρ) subsets with the property that two samples (x1  y1)  (x2  y2) in the same subset satisfy
y1 = y2 and ρ(x1  x2) ≤ γ.
It follows from Lemma 1 that we may partition X into subsets X1  . . .  XLNγ/2(X  ρ)  such that pairs
of points x1  x2 from the same subset have the same label and satisfy ρ(xi  xj) ≤ γ. Before we
connect local geometry to robustness we need one more deﬁnition. We say that a learned transform
fα is a δ-isometry if the metric is distorted by at most δ:
Deﬁnition 3 (δ-isometry). Let A B be metric spaces with metrics ρA and ρB. A map f : A (cid:55)→ B is
a δ-isometry if for any a1  a2 ∈ A  |ρA(f (a1)  f (a2)) − ρB(a1  a2)| ≤ δ.
Theorem 1. Let fα be a transform derived via Eq. (6) and let X1  . . .  XLNγ/2(X  ρ) be a cover of
X as described above. If fα is a δ-isometry  then it is (LNγ/2(X   ρ)  2A(γ + δ))-robust.
Proof sketch. Consider training samples zi  zj and testing samples z(cid:48)
i  z(cid:48)
j such that zi  z(cid:48)
j ∈ Zq for some p  q ∈ {1  . . .   LNγ/2(X   ρ)}. Then by Lemma 1 
zj  z(cid:48)

i ∈ Zp and

and xi  x(cid:48)

ρ(xi  x(cid:48)
i ∈ Xp and xj  x(cid:48)
|ρ(fαT (xi)  fαT (x(cid:48)
Rearranging the terms gives
ρ(fαT (xi)  fαT (x(cid:48)

j) ≤ γ 

i) ≤ γ and ρ(xj  x(cid:48)
j ∈ Xq. By deﬁnition of δ-isometry 
i)) − ρ(xi  x(cid:48)

i)| ≤ δ and |ρ(fαT (xj)  fαT (x(cid:48)

yi = y(cid:48)

i and yj = y(cid:48)
j 

j)) − ρ(xj  x(cid:48)

j)| ≤ δ.

i)) ≤ ρ(xi  x(cid:48)

i) + δ ≤ γ + δ and ρ(fαT (xj)  fαT (x(cid:48)

j)) ≤ ρ(xj  x(cid:48)

j) + δ ≤ γ + δ.

Figure 2: Proof without words.

4

i)  fαT (x(cid:48)

In order
to bound the generalization error  we need to bound the difference between
ρ(fαT (xi)  fαT (xj)) and ρ(fαT (x(cid:48)
j)). The details can be found in [9]; here we ap-
peal to the proof schematic in Fig. 2. We need to bound |e − e(cid:48)| and it cannot exceed twice the
diameter of a local region in the transformed domain.
Robustness of the learning algorithm depends on the granularity of the cover and the degree to
which the learned transform fα distorts distances between pairs of points in the same covering
subset. The subsets in the cover constitute regions where the local geometry makes it possible to
bound generalization error. It now follows from [1] that the generalization error satisﬁes R(αT ) −
Remp(αT ) ≤ 2A(γ + δ) + O
. The DRT proposed here is a particular example of a local
isometry  and Theorem 1 explains why the generalization error is smaller than that of pure metric
learning.
The transform described in [9] partitions the metric space X into exactly L subsets  one for each
class. The experiments reported in Section 5 demonstrate that the performance improvements de-
rived from working with a ﬁner partition can be worth the cost of learning ﬁner grained local regions.

(cid:16)(cid:113) K

(cid:17)

n

4 An Illustrative Realization of DRT

Having justiﬁed robustness  we now provide a realization of the proposed general DRT where the
metric ρ is Euclidean distance. We use Gaussian random variables to initialize α  then  on the
randomly transformed data  we set t(1) (t(−1)) to be the average intra-class (inter-class) pairwise
distance. In all our experiments  the solution satisﬁed the condition t(1) < t(−1) required in Eq. (1).
We calculate the diameter γ of the local regions NB indirectly  using the κ-nearest neighbors of each
training sample to deﬁne a local neighborhood. We leave the question of how best to initialize the
indicator t and the diameter γ for future research.
We denote this particular example as Euc-DRT and use gradient descent to solve for α. Denoting
the objective by J  we deﬁne yi (cid:44) fα(xi)  δi j (cid:44) fα(xi) − fα(xj)  and ρ0
(cid:44) (cid:107)xi − xj(cid:107). Then
(8)

(cid:88)

(cid:88)

1 − λ
|NB| · sgn((cid:107)δi j(cid:107) − ρ0

i j) · δi j(cid:107)δi j(cid:107) .

λ

|P| · (cid:96)i j · δi j(cid:107)δi j(cid:107) +

∂J
∂yi

=

i.j

(i j)∈NB

(i j)∈P

(cid:96)i j ((cid:107)δi j(cid:107)−t((cid:96)i j ))>0

In general  fα deﬁnes a D-layer neural network (when D = 1 it deﬁnes a linear transform). Let α(d)
be the linear weights at the d-th layer  and let x(d) be the output of the d-th layer  so that yi = x(D)
.
Then the gradients are computed as 

i

∂J

∂α(D)

=

∂J
∂yi

· ∂yi
∂α(D)

  and ∂J
∂α(d)

=

∂J

∂x(d+1)

i

i

i

· ∂x(d+1)
∂x(d)

i

· ∂x(d)
∂α(d) for 1 ≤ d ≤ D−1. (9)

i

Algorithm 1 provides a summary  and we note that the extension to stochastic training using min-
batches is straightforward.

5 Experimental Results

In this section we report on experiments that conﬁrm robustness of Euc-DRT. Recall that empirical
loss is given by Eq. (4) where α is learned as αT from the training set T   and |T | = N. The
generalization error is R − Remp where the expected loss R is estimated using a large test set.

5.1 Toy Example

This illustrative example is motivated by the discussion in Section 2.1. We ﬁrst generate a 2D
dataset consisting of two noisy half-moons  then use a random 100 × 2 matrix to embed the data
in a 100-dimensional space. We learn a linear transform fα that maps the 100 dimensional data to
2 dimensional features  and we use κ = 5 nearest neighbors to construct the set NB. We consider
λ = 1  0.5  0.25  representing the most discriminative  balanced  and more robust scenarios.
When λ = 1 the transformed training samples are rather discriminative (Fig. 3a)  but when the
transform is applied to testing data  the two classes are more mixed (Fig. 3d). When λ = 0.5  the

5

(cid:88)

i

(cid:88)

transform)  stepsize η  neighborhood size κ.

Algorithm 1 Gradient descent solver for Euc-DRT
Input: λ ∈ [0  1]  training pairs {(xi  xj  (cid:96)i j)}  a pre-deﬁned D-layer network (D = 1 as linear
Output: α
1: Randomly initialize α  compute yi = fα(xi).
2: On the yi  compute the average intra and inter-class pairwise distances  assign to t(1)  t(−1)
3: For each training datum  ﬁnd its κ nearest neighbor and deﬁne the set NB.
4: while stable objective not achieved do
5:
6:
7:
8:
9:
10:
end for
11:
12: end while

Compute yi = fα(xi) by a forward pass.
Compute objective J.
Compute ∂J
as Eq. (8).
∂yi
for l = D down to 1 do
Compute
α(d) ← α(d) − η ∂J

∂J

∂α(d) as Eq. (9).

∂α(d) .

(a) λ = 1 Transformed training
samples. (discriminative case)

(b) λ = 0.5 transformed training
samples. (balanced case)

(c) λ = 0.25 Transformed train-
ing samples. (robust case)

(d) λ = 1 Transformed testing
samples. (discriminative case)

(e) λ = 0.5 transformed testing
samples. (balanced case)

(f) λ = 0.25 Transformed testing
samples. (robust case)

Figure 3: Original and transformed training/testing samples embedded in 2-dimensional space with
different colors representing different classes.

transformed training data are more dispersed within each class (Fig. 3b)  hence less easily separated
than when λ = 1. However Fig. 3e shows that it is easier to separate the two classes on the test data.
When λ = 0.25  robustness is preferred to discriminative power as shown in Figs. 3c and 3f.
Tab. 1 quantiﬁes empirical loss Remp  generalization error  and classiﬁcation performance (by 1-nn)
for λ = 1  0.5 and 0.25. As λ decreases  Remp increases  indicating loss of discrimination on the
training set. However  generalization error decreases  implying more robustness. We conclude that
by varying λ  we can balance discrimination and robustness.

5.2 MNIST Classﬁcation Using a Very Small Training Set

The transform fα learned in the previous section was linear  and we now apply a more sophisticated
convolutional neural network to the MNIST dataset. The network structure is similar to LeNet  and is

6

-20020-30-20-100102030-20020-30-20-100102030-20020-30-20-100102030-20020-30-20-100102030-20020-30-20-100102030-20020-30-20-100102030Table 1: Varying λ on a toy dataset.

λ

Remp

generalization error

1-nn accuracy

(original data 93.35%)

1

0.5

0.25
1.9439
1.5983
8.8040
10.5855
92.20% 98.30% 91.55%

1.6025
9.5071

Table 2: Classiﬁcation error on MNIST.

Training/class
original pixels

LeNet
DML

Euc-DRT

30

50

70

100

81.91% 86.18% 86.86% 88.49%
87.51% 89.89% 91.24% 92.75%
92.32% 94.45% 95.67% 96.19%
94.14% 95.20% 96.05% 96.21%

Table 3: Implementation de-
tails of the neural network for
MNIST classiﬁcation.

name
conv1
pool1
conv2
pool2
conv3

parameters

size: 5 × 5 × 1 × 20

stride: 1  pad: 0

size: 2 × 2

size: 5 × 5 × 20 × 50

stride: 1  pad: 0

size: 2 × 2

size: 4 × 4 × 50 × 128

stride: 1  pad: 0

made up of alternating convolutional layers and pooling layers  with parameters detailed in Table 3.
We map the original 784-dimensional pixel values (28x28 image) to 128-dimensional features.
While state-of-art results often use the full training set (6 000 training samples per class)  here we are
interested in small training sets. We use only 30 training samples per class  and we use κ = 7 nearest
neighbors to deﬁne local regions in Euc-DRT. We vary λ and study empirical error  generalization
error  and classiﬁcation accuracy (1-nn). We observe in Fig. 4 that when λ decreases  the empirical
error also decreases  but that the generalization error actually increases. By balancing between these
two factors  a peak classiﬁcation accuracy is achieved at λ = 0.25. Next  we use 30  50  70  100

(a)

(b)

(c)

Figure 4: MNIST test: with only 30 training samples per class. We vary λ and assess (a) Remp; (b)
generalization error; and (c) 1-nn classiﬁcation accuracy. Peak accuracy is achieved at λ = 0.25.
training samples per class and compare the performance of Euc-DRT with LeNet and Deep Metric
Learning (DML) [7]. DML minimizes a hinge loss on the squared Euclidean distances. It shares the
same spirit with our Euc-DRT using λ = 1. All methods use the same network structure  Tab. 3  to
map to the features. For classiﬁcation  LeNet uses a linear softmax classiﬁer on top of the “conv3”
layer and minimizes the standard cross-entropy loss during training. DML and Euc-DRT both use
a 1-nn classiﬁer on the learned features. Classiﬁcation accuracies are reported in Tab. 2. In Tab. 2 
we see that all the learned features improve upon the original ones. DML is very discriminative
and achieves higher accuracy than LeNet. However  when the training set is very small  robustness
becomes more important and Euc-DRT signiﬁcantly outperforms DML.

5.3 Face Veriﬁcation on LFW

We now present face veriﬁcation on the more challenging Labeled Faces in the Wild (LFW) bench-
mark  where our experiments will show that there is an advantage to balancing disciminability and
robustness. Our goal is not to reproduce the success of deep learning in face veriﬁcation [7  14] 
but to stress the importance of robust training and to compare the proposed Euc-DRT objective
with popular alternatives. Note also that it is difﬁcult to compare with deep learning methods when
training sets are proprietary [12–14].

7

λ00.250.50.751Remp00.020.040.060.080.10.120.14λ00.250.50.751R-Remp1.522.533.544.5λ00.250.50.7511-nn accuracy(%)9292.59393.59494.5We adopt the experimental framework used in [2]  and train a deep network on the WDRef dataset 
where each face is described using a high dimensional LBP feature [3] (available at 2) that is re-
duced to a 5000-dimensional feature using PCA. The WDRef dataset is signiﬁcantly smaller than
the proprietary datasets typical of deep learning  such as the 4.4 million labeled faces from 4030
individuals in [14]  or the 202 599 labeled faces from 10 177 individuals in [12]. It contains 2 995
subjects with about 20 samples per subject.
We compare the Euc-DRT objective with DeepFace (DF) [14] and Deep Metric Learning (DML) [7] 
two state-of-the-art deep learning objectives. For a fair comparison  we employ the same network
structure and train on the same input data. DeepFace feeds the output of the last network layer to an
L-way soft-max to generate a probability distribution over L classes  then minimizes a cross entropy
loss. The Euc-DRT feature fα is implemented as a two-layer fully connected network with tanh as
the squash function. Weight decay (conventional Frobenius norm regularization) is employed in
both DF and DML  and results are only reported for the best weight decay factor. After a network
is trained on WDRef  it is tested on the LFW benchmark. Veriﬁcation simply consists of comparing
the cosine distance between a given pair of faces to a threshold.
Fig. 5 displays ROC curves and Table 4 reports area under the ROC curve (AUC) and veriﬁcation
accuracy. High-Dim LBP refers to veriﬁcation using the initial LBP features. DeepFace (DF) op-
timizes for a classiﬁcation objective by minimizing a softmax loss  and it successfully separates
samples from different classes. However the constraint that assigns similar representations to the
same class is weak  and this is reﬂected in the true positive rate displayed in Fig. 5. In Deep Metric
Learning (DML) this same constraint is strong  but robustness is a concern when the training set
is small. The proposed Euc-DRT improves upon both DF and DML by balancing disciminability
and robustness. It is less conservative than DF for better discriminability  and more responsive to
local geometry than DML for smaller generalization error. Face veriﬁcation accuracy for Euc-DRT
was obtained by varying the regularization parameter λ between 0.4 and 1 (as shown in Fig 6)  then
reporting the peak accuracy observed at λ = 0.9.

Table 4: Veriﬁcation accuracy and
AUCs on LFW

Method
HD-LBP
deepFace

DML

Euc-DRT

Accuracy

(%)
74.73
88.72
90.28
92.33

AUC
(×10−2)
82.22±1.00
95.50± 0.29
96.74±0.33
97.77± 0.25

Figure 5: Comparison of
ROCs for all methods

Figure 6: Veriﬁcation accu-
racy of Euc-DRT as λ varies

6 Conclusion

We have proposed an optimization framework within which it is possible to tradeoff the discrim-
inative value of learned features with robustness of the learning algorithm. Improvements to gen-
eralization error predicted by theory are observed in experiments on benchmark datasets. Future
work will investigate how to initialize and tune the optimization  also how the Euc-DRT algorithm
compares with other methods that reduce generalization error.

7 Acknowledgement

The work of Huang and Calderbank was supported by AFOSR under FA 9550-13-1-0076 and by
NGA under HM017713-1-0006. The work of Qiu and Sapiro is partially supported by NSF and
DoD.

2http://home.ustc.edu.cn/chendong/

8

00.510.50.60.70.80.91HD-LBPdeepFaceDMLEuc-DRTλ0.40.60.81verification accuracy (%)91.491.691.89292.292.4References
[1] A. Bellet and A. Habrard. Robustness and generalization for metric learning. Neurocomputing 

151(14):259–267  2015.

[2] D. Chen  X. Cao  L. Wang  F. Wen  and J. Sun. Bayesian face revisited: A joint formulation.

In European Conference on Computer Vision (ECCV)  2012.

[3] D. Chen  X. Cao  F. Wen  and J. Sun. Blessing of dimensionality: High-dimensional feature
and its efﬁcient compression for face veriﬁcation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)  2013.

[4] K. Fukunaga. Introduction to Statistical Pattern Recognition. San Diego: Academic Press 

1990.

[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural

Information Processing Systems (NIPS)  2005.

[6] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood components anal-

ysis. In Advances in Neural Information Processing Systems (NIPS)  2004.

[7] J. Hu  J. Lu  and Y. Tan. Discriminative deep metric learning for face veriﬁcation in the wild.

In Computer Vision and Pattern Recognition (CVPR)  pages 1875–1882  2014.

[8] G. B. Huang  M. Ramesh  T. Berg  and E. Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in unconstrained environments. Technical Report 07-
49  University of Massachusetts  Amherst  October 2007.

[9] J. Huang  Q. Qiu  R. Calderbank  and G. Sapiro. Geometry-aware deep transform. In Interna-

tional Conference on Computer Vision  2015.

[10] G. Sapiro Q. Qiu. Learning transformations for clustering and classiﬁcation. Journal of Ma-

chine Learning Research (JMLR)  pages 187–225  2015.

[11] C. Sumit  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively  with appli-
cation to face veriﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  volume 1  pages 539–546  2005.

[12] Y. Sun  Y. Chen  X. Wang  and X. Tang. Deep learning face representation by joint
In Advances in Neural Information Processing Systems (NIPS) 

identiﬁcation-veriﬁcation.
pages 1988–1996  2014.

[13] Y. Sun  X. Wang  and X. Tang. Deep learning face representation from predicting 10 000
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages

classes.
1891–1898  2014.

[14] Y. Taigman  M. Yang  M. A. Ranzato  and L. Wolf. Deepface: Closing the gap to human-
level performance in face veriﬁcation. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 1701–1708  2014.

[15] V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Net-

works  10(5):988–999  1999.

[16] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. Journal of Machine Learning Research  10:207–244  2009.

[17] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. Russell. Distance metric learning  with application
to clustering with side-information. In Advances in Neural Information Processing Systems
(NIPS)  2002.

[18] H. Xu and S. Mannor. Robustness and generalization. Machine Learning  86(3):391–423 

2012.

[19] Z. Zha  T. Mei  M. Wang  Z. Wang  and X. Hua. Robust distance metric learning with auxiliary

knowledge. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2009.

9

,Daniele Durante
Bruno Scarpa
David Dunson
Atsushi Nitanda
Jiaji Huang
Qiang Qiu
Robert Calderbank
Robert Gower
Filip Hanzely
Peter Richtarik
Sebastian Stich
Thang Vu
Hyunjun Jang
Trung Pham
Chang Yoo