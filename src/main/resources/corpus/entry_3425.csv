2019,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,Current deep neural networks(DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight  and then iterating between weight recalculating and classifier updating. Current approaches  however  need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue  we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer  constituting a universal approximator to almost any continuous functions  making the method able to fit a wide range of weighting function forms including those assumed in conventional research. Guided by a small amount of unbiased meta-data  the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases  fully complying with the common settings in traditional methods  and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.,Meta-Weight-Net: Learning an Explicit Mapping

For Sample Weighting

Jun Shu1  Qi Xie1  Lixuan Yi1  Qian Zhao1  Sanping Zhou1  Zongben Xu1  and Deyu Meng*2 1

1Xi’an Jiaotong University

2The Macau University of Science and Technology
*Corresponding author:dymeng@mail.xjtu.edu.cn

Abstract

Current deep neural networks (DNNs) can easily overﬁt to biased training data with
corrupted labels or class imbalance. Sample re-weighting strategy is commonly
used to alleviate this issue by designing a weighting function mapping from training
loss to sample weight  and then iterating between weight recalculating and classiﬁer
updating. Current approaches  however  need manually pre-specify the weighting
function as well as its additional hyper-parameters. It makes them fairly hard to be
generally applied in practice due to the signiﬁcant variation of proper weighting
schemes relying on the investigated problem and training data. To address this issue 
we propose a method capable of adaptively learning an explicit weighting function
directly from data. The weighting function is an MLP with one hidden layer 
constituting a universal approximator to almost any continuous functions  making
the method able to ﬁt a wide range of weighting functions including those assumed
in conventional research. Guided by a small amount of unbiased meta-data  the
parameters of the weighting function can be ﬁnely updated simultaneously with
the learning process of the classiﬁers. Synthetic and real experiments substantiate
the capability of our method for achieving proper weighting functions in class
imbalance and noisy label cases  fully complying with the common settings in tra-
ditional methods  and more complicated scenarios beyond conventional cases. This
naturally leads to its better accuracy than other state-of-the-art methods. Source
code is available at https://github.com/xjtushujun/meta-weight-net.

1

Introduction

DNNs have recently obtained impressive good performance on various applications due to their
powerful capacity for modeling complex input patterns. However  DNNs can easily overﬁt to biased
training data1  like those containing corrupted labels [2] or with class imbalance[3]  leading to
their poor performance in generalization in such cases. This robust deep learning issue has been
theoretically illustrated in multiple literatures [4  5  6  7  8  9].
In practice  however  such biased training data are commonly encountered. For instance  practically
collected training samples always contain corrupted labels [10  11  12  13  14  15  16  17]. A typical
example is a dataset roughly collected from a crowdsourcing system [18] or search engines [19  20] 
which would possibly yield a large amount of noisy labels. Another popular type of biased training
data is those with class imbalance. Real-world datasets are usually depicted as skewed distributions 
with a long-tailed conﬁguration. A few classes account for most of the data  while most classes are

1We call the training data biased when they are generated from a joint sample-label distribution deviating

from the distribution of evaluation/test set[1].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Weight function in focal loss

(b) Weight function in SPL

(c) Meta-Weight-Net architecture

(d) MW-Net function learned in
class imbalance case

(e) MW-Net function learned in
corrupter labels case

(f) MW-Net function learned in
real Clothing1M dataset

Figure 1: (a)-(b) weight functions set in focal loss and self-paced learning (SPL). (c) Meta-Weighting-
Net architecture. (d)-(f) Meta-Weighting-Net functions learned in class imbalance (imbalanced factor
100)  noisy label (40% uniform noise)  and real dataset  respectively  by our method.

under-represented. Effective learning with these biased training data  which is regarded to be biased
from evaluation/test ones  is thus an important while challenging issue in machine learning [1  21].
Sample reweighting approach is a commonly used strategy against this robust learning issue. The main
methodology is to design a weighting function mapping from training loss to sample weight (with
hyper-parameters)  and then iterates between calculating weights from current training loss values
and minimizing weighted training loss for classiﬁer updating. There exist two entirely contradictive
ideas for constructing such a loss-weight mapping. One makes the function monotonically increasing
as depicted in Fig. 1(a)  i.e.  enforce the learning to more emphasize samples with larger loss values
since they are more like to be uncertain hard samples located on the classiﬁcation boundary. Typical
methods of this category include AdaBoost [22  23]  hard negative mining [24] and focal loss [25].
This sample weighting manner is known to be necessary for class imbalance problems  since it can
prioritize the minority class with relatively higher training losses.
On the contrary  the other methodology sets the weighting function as monotonically decreasing  as
shown in Fig. 1(b)  to take samples with smaller loss values as more important ones. The rationality
lies on that these samples are more likely to be high-conﬁdent ones with clean labels. Typical methods
include self-paced learning(SPL) [26]  iterative reweighting [27  17] and multiple variants [28  29  30].
This weighting strategy has been especially used in noisy label cases  since it inclines to suppress the
effects of samples with extremely large loss values  possibly with corrupted incorrect labels.
Although these sample reweighting methods help improve the robustness of a learning algorithm on
biased training samples  they still have evident deﬁciencies in practice. On the one hand  current
methods need to manually set a speciﬁc form of weighting function based on certain assumptions on
training data. This  however  tends to be infeasible when we know little knowledge underlying data
or the label conditions are too complicated  like the case that the training set is both imbalanced and
noisy. On the other hand  even when we specify certain weighting schemes  like focal loss [25] or
SPL [26]  they inevitably involve hyper-parameters  like focusing parameter in the former and age
parameter in the latter  to be manually preset or tuned by cross-validation. This tends to further raise
their application difﬁculty and reduce their performance stability in real problems.
To alleviate the aforementioned issue  this paper presents an adaptive sample weighting strategy to
automatically learn an explicit weighting function from data. The main idea is to parameterize the
weighting function as an MLP (multilayer perceptron) network with only one hidden layer (as shown
in Fig. 1(c))  called Meta-Weight-Net  which is theoretically a universal approximator for almost
any continuous function [31]  and then use a small unbiased validation set (meta-data) to guide the
training of all its parameters. The explicit form of the weighting function can be ﬁnally attained
speciﬁcally suitable to the learning task.
In summary  this paper makes the following three-fold contributions:
1) We propose to automatically learn an explicit loss-weight function  parameterized by an MLP from
data in a meta-learning manner. Due to the universal approximation capability of this weight net  it
can ﬁnely ﬁt a wide range of weighting functions including those used in conventional research.

2

0246810Loss0.000.250.500.751.00WeightFocal Loss0246810Loss0.00.20.40.60.81.0WeightSelf-paced learning...LossWeight0246810Loss0.50.60.70.80.9Weightlong-tailed CIFAR-10long-tailed CIFAR-1000246810Loss0.00.20.4WeightCIFAR-10CIFAR-10005101520Loss0.450.500.55WeightClothing1M2) Experiments verify that the weighting functions learned by our method highly comply with
manually preset weighting manners used in tradition in different training data biases  like class
imbalance and noisy label cases as shown in Fig. 1(d) and 1(e))  respectively. This shows that the
weighting scheme learned by the proposed method inclines to help reveal deeper understanding for
data bias insights  especially in complicated bias cases where the extracted weighting function is with
complex tendencies (as shown in Fig. 1(f)).
3) The insights of why the proposed method works can be well interpreted. Particularly  the updating
equation for Meta-Weight-Net parameters can be explained by that the sample weights of those
samples better complying with the meta-data knowledge will be improved  while those violating such
meta-knowledge will be suppressed. This tallies with our common sense on the problem: we should
reduce the inﬂuence of those highly biased ones  while emphasize those unbiased ones.
The paper is organized as follows. Section 2 presents the proposed meta-learning method as well
as the detailed algorithm and analysis of its convergence property. Section 3 discusses related work.
Section 4 demonstrates experimental results and the conclusion is ﬁnally made.

2 The Proposed Meta-Weight-Net Learning Method

2.1 The Meta-learning Objective
Consider a classiﬁcation problem with the training set {xi  yi}N
i=1  where xi denotes the i-th sample 
yi ∈ {0  1}c is the label vector over c classes  and N is the number of the entire training data. f (x  w)
denotes the classiﬁer  and w denotes its parameters. In current applications  f (x  w) is always set as
a DNN. We thus also adopt DNN  and call it the classiﬁer network for convenience in the following.
Generally  the optimal classiﬁer parameter w∗ can be extracted by minimizing the loss 1
i=1
N
(cid:96)(yi  f (xi  w)) calculated on the training set. For notation convenience  we denote that Ltrain
(w) =
(cid:96)(yi  f (xi  w)). In the presence of biased training data  sample re-weighting methods enhance the
robustness of training by imposing weight V(Ltrain
(w); Θ) on the i-th sample loss  where V((cid:96); Θ)
denotes the weight net  and Θ represents the parameters contained in it. The optimal parameter w is
calculated by minimizing the following weighted loss:

(cid:80)N

i

i

w∗(Θ) = arg min

w

Ltrain(w; Θ) (cid:44) 1
N

V(Ltrain

i

(w); Θ)Ltrain

i

(w).

(1)

N(cid:88)

i=1

Meta-Weight-Net: Our method aims to automatically learn the hyper-parameters Θ in a meta-
learning manner. To this aim  we formulate V(Li(w); Θ) as a MLP network with only one hidden
layer containing 100 nodes  as shown in Fig. 1(c). We call this weight net as Meta-Weight-Net or
MW-Net for easy reference. Each hidden node is with ReLU activation function  and the output is
with the Sigmoid activation function  to guarantee the output located in the interval of [0  1]. Albeit
simple  this net is known as a universal approximator for almost any continuous function [31]  and
thus can ﬁt a wide range of weighting functions including those used in conventional research.
Meta learning process. The parameters contained in MW-Net can be optimized by using the meta
learning idea [32  33  34  35]. Speciﬁcally  assume that we have a small amount unbiased meta-data
}M
set (i.e.  with clean labels and balanced data distribution) {x(meta)
i=1  representing the
meta-knowledge of ground-truth sample-label distribution  where M is the number of meta-samples
and M (cid:28) N. The optimal parameter Θ∗ can be obtained by minimizing the following meta-loss:

  y(meta)

i

i

Θ∗ = arg min

Θ

Lmeta(w∗(Θ)) (cid:44) 1
M

Lmeta

i

(w∗(Θ)) 

(2)

where Lmeta

i

(w) = (cid:96)

y(meta)
i

  f (x(meta)

i

  w)

is calculated on meta-data.

2.2 The Meta-Weight-Net Learning Method
Calculating the optimal Θ∗ and w∗ require two nested loops of optimization. Here we adopt an
online strategy to update Θ and w through a single optimization loop  respectively  to guarantee the
efﬁciency of the algorithm.

3

(cid:16)

M(cid:88)

i=1

(cid:17)

Figure 2: Main ﬂowchart of the proposed MW-Net Learning algorithm (steps 5-7 in Algorithm 1).

Formulating learning manner of classiﬁer network. As general network training tricks  we employ
SGD to optimize the training loss (1). Speciﬁcally  in each iteration of training  a mini-batch of
training samples {(xi  yi)  1 ≤ i ≤ n} is sampled  where n is the mini-batch size. Then the updating
equation of the classiﬁer network parameter can be formulated by moving the current w(t) along the
descent direction of the objective loss in Eq. (1) on a mini-batch training data:

ˆw(t)(Θ) = w(t) − α

1
n

where α is the step size.

× n(cid:88)

i=1

V(Ltrain

i

(w(t)); Θ)∇wLtrain

i

(w)

 

(3)

(cid:12)(cid:12)(cid:12)w(t)

Algorithm 1 The MW-Net Learning Algorithm

Input: Training data D  meta-data set (cid:98)D  batch size n  m  max iterations T .

{x  y} ← SampleMiniBatch(D  n).

{x(meta)  y(meta)} ← SampleMiniBatch((cid:98)D  m).

Output: Classiﬁer network parameter w(T )
1: Initialize classiﬁer network parameter w(0) and Meta-Weight-Net parameter Θ(0).
2: for t = 0 to T − 1 do
3:
4:
5:
6:
7:
8: end for

Formulate the classiﬁer learning function ˆw(t)(Θ) by Eq. (3).
Update Θ(t+1) by Eq. (4).
Update w(t+1) by Eq. (5).

Updating parameters of Meta-Weight-Net: After receiving the feedback of the classiﬁer network
parameter updating formulation ˆw(t)(Θ) 2from the Eq .(3)  the parameter Θ of the Meta-Weight-Net
can then be readily updated guided by Eq. (2)  i.e.  moving the current parameter Θ(t) along the
objective gradient of Eq. (2) calculated on the meta-data:

Θ(t+1) = Θ(t) − β

1
m

∇ΘLmeta

i

( ˆw(t)(Θ))

 

(4)

m(cid:88)

i=1

(cid:12)(cid:12)(cid:12)Θ(t)

where β is the step size.
Updating parameters of classiﬁer network: Then  the updated Θ(t+1) is employed to ameliorate
the parameter w of the classiﬁer network  i.e. 

w(t+1) = w(t) − α

1
n

V(Ltrain

i

(w(t)); Θ(t+1))∇wLtrain

i

(w)

.

(5)

(cid:12)(cid:12)(cid:12)w(t)

× n(cid:88)

i=1

The MW-Net Learning algorithm can then be summarized in Algorithm 1  and Fig. 2 illustrates its
main implementation process (steps 5-7). All computations of gradients can be efﬁciently imple-
mented by automatic differentiation techniques and generalized to any deep learning architectures of
classiﬁer network. The algorithm can be easily implemented using popular deep learning frameworks
like PyTorch [36]. It is easy to see that both the classiﬁer network and the MW-Net gradually
ameliorate their parameters during the learning process based on their values calculated in the last
step  and the weights can thus be updated in a stable manner  as clearly shown in Fig. 6.

2Notice that Θ here is a variable instead of a quantity  which makes ˆwt(Θ) a function of Θ and the gradient

in Eq. (4) be able to be computed.

4

Step 5Step 6Step 7 Meta-Weight-Net Classifier network ...LossWeight(w(t)); Θ)

 

(6)

(cid:12)(cid:12)(cid:12)Θ(t)
(cid:80)m

2.3 Analysis on the Weighting Scheme of Meta-Weight-Net

The computation of Eq. (4) by backpropagation can be rewritten as3:
∂V(Ltrain
∂Θ

Θ(t+1) = Θ(t) +

αβ
n

1
m

Gij

m(cid:88)

j

i=1

(cid:33)

(cid:32)
n(cid:88)
(cid:12)(cid:12)(cid:12)w(t)

j=1

(cid:12)(cid:12)(cid:12)T

i

j

∂ ˆw

(w)

( ˆw)

∂Ltrain
∂w

ˆw(t)

(cid:80)m

. Neglecting the coefﬁcient 1
m

where Gij = ∂Lmeta
see that each term in the sum orients to the ascend gradient of the weight function V(Ltrain

i=1 Gij  it is easy to
(w(t)); Θ).
i=1 Gij  the coefﬁcient imposed on the j-th gradient term  represents the similarity between
1
m
the gradient of the j-th training sample computed on training loss and the average gradient of the
mini-batch meta data calculated on meta loss. That means if the learning gradient of a training sample
is similar to that of the meta samples  then it will be considered as beneﬁcial for getting right results
and its weight tends to be more possibly increased. Conversely  the weight of the sample inclines to
be suppressed. This understanding is consistent with why well-known MAML works [37  38  39].

j

2.4 Convergence of the MW-Net Learning algorithm

Our algorithm involves optimization of two-level objectives  and therefore we show theoretically that
our method converges to the critical points of both the meta and training loss function under some
mild conditions in Theorem 1 and 2  respectively. The proof is listed in the supplementary material.
Theorem 1. Suppose the loss function (cid:96) is Lipschitz smooth with constant L  and V(·) is differential
with a δ-bounded gradient and twice differential with its Hessian bounded by B  and the loss function
(cid:96) have ρ-bounded gradients with respect to training/meta data. Let the learning rate αt satisﬁes αt =
min{1  k
T < 1  and βt  1 ≤ t ≤ N is a monotone descent sequence 
βt = min{ 1
t ≤ ∞. Then
L  
the proposed algorithm can achieve E[(cid:107)∇G(Θ(t))(cid:107)2

c ≥ L and(cid:80)∞

t=1 βt ≤ ∞ (cid:80)∞

T }  for some k > 0  such that k

2] ≤  in O(1/2) steps. More speciﬁcally 

} for some c > 0  such that σ

t=1 β2

√

√
c

T

T

σ

E[(cid:107)∇Lmeta(Θ(t))(cid:107)2

2] ≤ O(

min
0≤t≤T

C√
T

) 

(7)

where C is some constant independent of the convergence process  and σ is the variance of drawing
uniformly mini-batch sample at random.
Theorem 2. The condions in Theorem 1 hold  then we have:

E[(cid:107)∇Ltrain(w(t); Θ(t+1))(cid:107)2

2] = 0.

lim
t→∞

(8)

3 Related Work

Sample Weighting Methods. The idea of reweighting examples can be dated back to dataset
resampling [40  41] or instance re-weight [42]  which pre-evaluates the sample weights as a pre-
processing step by using certain prior knowledge on the task or data. To make the sample weights ﬁt
data more ﬂexibly  more recent researchers focused on pre-designing a weighting function mapping
from training loss to sample weight  and dynamically ameliorate weights during training process
[43  44]. There are mainly two manners to design the weighting function. One is to make it
monotonically increasing  speciﬁcally effective in class imbalance case. Typical methods include the
boosting algorithm (like AdaBoost [22]) and multiple of its variations [45]  hard example mining
[24] and focal loss [25]  which impose larger weights to ones with larger loss values. On the contrary 
another series of methods specify the weighting function as monotonically decreasing  especially used
in noisy label cases. For example  SPL [26] and its extensions [28  29]  iterative reweighting [27  17]
and other recent work [46  30]  pay more focus on easy samples with smaller losses. The limitation
of these methods are that they all need to manually pre-specify the form of weighting function as
well as their hyper-parameters  raising their difﬁculty to be readily used in real applications.

3Derivation can be found in supplementary materials.

5

Meta Learning Methods. Inspired by meta-learning developments [47  48  49  37  50]  recently
some methods were proposed to learn an adaptive weighting scheme from data to make the learning
more automatic and reliable. Typical methods along this line include FWL [51]  learning to teach
[52  32] and MentorNet [21] methods  whose weight functions are designed as a Bayesian function
approximator  a DNN with attention mechanism  a bidirectional LSTM network  respectively. Instead
of only taking loss values as inputs as classical methods  the weighting functions they used (i.e. 
the meta-learner)  however  are with much more complex forms and required to input complicated
information (like sample features). This makes them not only hard to succeed good properties
possessed by traditional methods  but also to be easily reproduced by general users.
A closely related method  called L2RW [1]  adopts a similar meta-learning mechanism compared
with ours. The major difference is that the weights are implicitly learned there  without an explicit
weighting function. This  however  might lead to unstable weighting behavior during training and
unavailability for generalization. In contrast  with the explicit yet simple Meta-Weight-Net  our
method can learn the weight in a more stable way  as shown in Fig. 6  and can be easily generalized
from a certain task to related other ones (see in the supplementary material).
Other Methods for Class Imbalance. Other methods for handling data imbalance include: [53  54]
tries to transfer the knowledge learned from major classes to minor classes. The metric learning based
methods have also been developed to effectively exploit the tailed data to improve the generalization
ability  e.g.  triple-header loss [55] and range loss [56].
Other Methods for Corrupted Labels. For handling noisy label issue  multiple methods have been
designed by correcting noisy labels to their true ones via a supplemental clean label inference step
[11  14  57  13  21  1  15]. For example  GLC [15] proposed a loss correction approach to mitigate
the effects of label noise on DNN classiﬁers. Other methods along this line include the Reed [58] 
Co-training [16]  D2L [59] and S-Model [12].

4 Experimental Results

To evaluate the capability of the proposed algorithm  we implement experiments on data sets with
class imbalance and noisy label issues  and real-world dataset with more complicated data bias.

4.1 Class Imbalance Experiments

We use Long-Tailed CIFAR dataset [60]  that reduces the number of training samples per class
according to an exponential function n = niµi  where i is the class index  ni is the original number
of training images and µ ∈ (0  1). The imbalance factor of a dataset is deﬁned as the number of
training samples in the largest class divided by the smallest. We trained ResNet-32 [61] with softmax
cross-entropy loss by SGD with a momentum 0.9  a weight decay 5×10−4  an initial learning rate 0.1.
The learning rate of ResNet-32 is divided by 10 after 80 and 90 epoch (for a total 100 epochs)  and
the learning rate of WN-Net is ﬁxed as 10−5. We randomly selected 10 images per class in validation
set as the meta-data set. The compared methods include: 1) BaseModel  which uses a softmax
cross-entropy loss to train ResNet-32 on the training set; 2) Focal loss [25] and Class-Balanced
[60] represent the state-of-the-arts of the predeﬁned sample reweighting techniques; 3) Fine-tuning 
ﬁne-tune the result of BaseModel on the meta-data set; 4) L2RW [1]  which leverages an additional
meta-dataset to adaptively assign weights on training samples.
Table 1 shows the classiﬁcation accuracy of ResNet-32 on the test set and confusion matrices are
displayed in Fig. 3 (more details are listed in the supplementary material). It can be observed that:
1) Our algorithm evidently outperforms other competing methods on datasets with class imbalance 
showing its robustness in such data bias case; 2) When imbalance factor is 1  i.e.  all classes are
with same numbers of samples  ﬁne-tuning runs best  and our method still attains a comparable
performance; 3) When imbalance factor is 200 on long-tailed CIFAR-100  the smallest class has only
two samples. An extra ﬁne-tuning achieves performance gain  while our method still perform well in
such extreme data bias.
To understand the weighing scheme of MW-Net  we depict the tendency curve of weight with respect
to loss by the learned MW-Net in Fig. 1(d)  which complies with the classical optimal weighting
manner to such data bias. i.e.  larger weights should be imposed on samples with relatively large
losses  which are more likely to be minority class sample.

6

Table 1: Test accuracy (%) of ResNet-32 on long-tailed CIFAR-10 and CIFAR-100  and the best and
the second best results are highlighted in bold and italic bold  respectively.

Long-Tailed CIFAR-10

Long-Tailed CIFAR-100

Dataset Name

Imbalance
BaseModel
Focal Loss

Class-Balanced

Fine-tuning

L2RW
Ours

200
65.68
65.29
68.89
66.08
66.51
68.91

100
70.36
70.38
74.57
71.33
74.16
75.21

50

74.81
76.71
79.27
77.42
78.93
80.06

20

82.23
82.76
84.36
83.37
82.12
84.94

10

86.39
86.66
87.49
86.42
85.19
87.84

1

92.89
93.03
92.89
93.23
89.25
92.66

200
34.84
35.62
36.23
38.22
33.38
37.91

100
38.32
38.41
39.60
41.83
40.23
42.09

50

43.85
44.32
45.32
46.40
44.44
46.74

20

51.14
51.95
52.59
52.11
51.64
54.37

10

55.71
55.78
57.99
57.44
53.73
58.46

1

70.50
70.52
70.50
70.72
64.11
70.37

Table 2: Test accuracy comparison on CIFAR-10 and CIFAR-100 of WRN-28-10 with varying noise
rates under uniform noise. Mean accuracy (±std) over 5 repetitions are reported (‘—’ means the
method fails).

CIFAR-10

Datasets / Noise Rate
0%
40%
60%
0%
40%
60%

CIFAR-100

BaseModel
95.60±0.22
68.07±1.23
53.12±3.03
79.95±1.26
51.11±0.42
30.92±0.33

Reed-Hard
94.38±0.14
81.26±0.51
73.53±1.54
64.45±1.02
51.27±1.18
26.95±0.98

S-Model
83.79±0.11
79.58±0.33
52.86±0.99
42.12±0.99

—

—

Self-paced
90.81±0.34
86.41±0.29
53.10±1.78
59.79±0.46
46.31±2.45
19.08±0.57

Focal Loss
95.70±0.15
75.96±1.31
51.87±1.19
81.04±0.24
51.19±0.46
27.70±3.77

Co-teaching
88.67±0.25
74.81±0.34
73.06±0.25
61.80±0.25
46.20±0.15
35.67±1.25

D2L

94.64±0.33
85.60±0.13
68.02±0.41
66.17±1.42
52.10 ±0.97
41.11±0.30

Fine-tining
95.65±0.15
80.47±0.25
78.75±2.40
80.88±0.21
52.49±0.74
38.16±0.38

MentorNet
94.35±0.42
87.33±0.22
82.80±1.35
73.26±1.23
61.39±3.99
36.87±1.47

L2RW

92.38±0.10
86.92±0.19
82.24±0.36
72.99±0.58
60.79±0.91
48.15±0.34

GLC

94.30±0.19
88.28±0.03
83.49±0.24
73.75±0.51
61.31±0.22
50.81±1.00

Ours

94.52±0.25
89.27±0.28
84.07±0.33
78.76±0.24
67.73±0.26
58.75±0.11

4.2 Corrupted Label Experiment

We study two settings of corrupted labels on the training set: 1) Uniform noise. The label of each
sample is independently changed to a random class with probability p following the same setting
in [2]. 2) Flip noise. The label of each sample is independently ﬂipped to similar classes with total
probability p. In our experiments  we randomly select two classes as similar classes with equal
probability. Two benchmark datasets are employed: CIFAR-10 and CIFAR-100 [62]. Both are
popularly used for evaluation of noisy labels [59  16].1000 images with clean labels in validation set
are randomly selected as the meta-data set. We adopt a Wide ResNet-28-10 (WRN-28-10) [63] for
uniform noise and ResNet-32 [61] for ﬂip noise as our classiﬁer network models4.
The comparison methods include: BaseModel  referring to the similar classiﬁer network utilized in
our method  while directly trained on the biased training data; the robust learning methods Reed [58] 
S-Model [12]   SPL [26]  Focal Loss [25]  Co-teaching [16]  D2L [59]; Fine-tuning  ﬁne-tuning
the result of BaseModel on the meta-data with clean labels to further enhance its performance;
typical meta-learning methods MentorNet [21]  L2RW [1]  GLC [15]. We also trained the baseline
network only on 1000 meta-images. The performance are evidently worse than the proposed method
due to the neglecting of the knowledge underlying large amount of training samples. We thus have
not involved its results in comparison.
All the baseline networks were trained using SGD with a momentum 0.9  a weight decay 5 × 10−4
and an initial learning rate 0.1. The learning rate of classiﬁer network is divided by 10 after 36 epoch
and 38 epoch (for a total of 40 epoches) in uniform noise  and after 40 epoch and 50 epoch (for a
total of 60 epoches) in ﬂip noise. The learning rate of WN-Net is ﬁxed as 10−3. We repeated the
experiments 5 times with different random seeds for network initialization and label noise generation.
We report the accuracy averaged over 5 repetitions for each series of experiments and each competing
method in Tables 2 and 3. It can be observed that our method gets the best performance across
almost all datasets and all noise rates  except the second for 40% Flip noise. At 0% noise cases
(unbiased ones)  our method performs only slightly worse than the BaseModel. For other corrupted
label cases  the superiority of our method is evident. Besides  it can be seen that the performance
gaps between ours and all other competing methods increase as the noise rate is increased from 40%
to 60% under uniform noise. Even with 60% label noise  our method can still obtain a relatively
high classiﬁcation accuracy  and attains more than 15% accuracy gain compared with the second best
result for CIFAR100 dataset  which indicates the robustness of our methods in such cases.

4We have tried different classiﬁer network architectures as classiﬁer networks under each noise setting to
show our algorithm is suitable to different deep learning architectures. We show this effect in Fig.4  verifying
the consistently good performance of our method in two classiﬁer network settings.

7

Figure 3: Confusion matrices for the Basemodel
and ours on long-tailed CIFAR-10 with imbal-
ance factors 200.

Figure 4: Performance comparison for different
classiﬁer networks (WRN-28-10 and ResNet32)
under CIFAR ﬂip noise.

Table 3: Test accuracy comparison on CIFAR-10 and CIFAR-100 of ResNet-32 with varying noise
rates under ﬂip noise.

CIFAR-10

Datasets / Noise Rate
0%
20%
40%
0%
20%
40%

CIFAR-100

BaseModel
92.89±0.32
76.83±2.30
70.77±2.31
70.50±0.12
50.86±0.27
43.01±1.16

Reed-Hard
92.31±0.25
88.28±0.36
81.06±0.76
69.02±0.32
60.27±0.76
50.40±1.01

S-Model
83.61±0.13
79.25±0.30
75.73±0.32
51.46±0.20
45.45±0.25
43.81±0.15

Self-paced
88.52±0.21
87.03±0.34
81.63±0.52
67.55±0.27
63.63±0.30
53.51±0.53

Focal Loss
93.03±0.16
86.45±0.19
80.45±0.97
70.02±0.53
61.87±0.30
54.13±0.40

Co-teaching
89.87±0.10
82.83±0.85
75.41±0.21
63.31±0.05
54.13±0.55
44.85±0.81

D2L

92.02±0.14
87.66±0.40
83.89±0.46
68.11±0.26
63.48±0.53
51.83±0.33

Fine-tining
93.23±0.23
82.47±3.64
74.07±1.56
70.72±0.22
56.98±0.50
46.37±0.25

MentorNet
92.13±0.30
86.36±0.31
81.76±0.28
70.24±0.21
61.97±0.47
52.66±0.56

L2RW

89.25±0.37
87.86±0.36
85.66±0.51
64.11±1.09
57.47±1.16
50.98±1.55

GLC

91.02±0.20
89.68±0.33
88.92±0.24
65.42±0.23
63.07±0.53
62.22±0.62

Ours

92.04±0.15
90.33±0.61
87.54±0.23
70.11±0.33
64.22±0.28
58.64±0.47

Fig. 4 shows the performance comparison between WRN-28-10 and ResNet32 under ﬁxed ﬂip noise
setting. We can observe that the performance gains for our method and BaseModel between two
networks takes the almost same value. It implies that the performance improvement of our method is
not dependent on the selection of the classiﬁer network architectures.
As shown in Fig. 1(e)  the shape of the learned weight function depicts as monotonic decreasing 
complying with the traditional optimal setting to this bias condition  i.e.  imposing smaller weights on
samples with relatively large losses to suppress the effect of corrupted labels. Furthermore  we plot
the weight distribution of clean and noisy training samples in Fig. 5. It can be seen that almost all
large weights belongs to clean samples  and the noisy samples’s weights are smaller than that of clean
samples  which implies that the trained Meta-Weight-Net can distinguish clean and noisy images.
Fig. 6 plots the weight variation along with training epoches under 40% noise on CIFAR10 dataset
of our method and L2RW. y-axis denotes the differences of weights calculated between adjacent
epoches  and x-axis denotes the number of epoches. Ten noisy samples are randomly chosen to
compute their mean curve  surrounded by the region illustrating the standard deviations calculated on
these samples in the corresponding epoch. It is seen that the weight by our method is continuously
changed  gradually stable along iterations  and ﬁnally converges. As a comparison  the weight during
the learning process of L2RW ﬂuctuates relatively more wildly. This could explain the consistently
better performance of our method as compared with this competing method.

4.3 Experiments on Clothing1M

To verify the effectiveness of the proposed method on real-world data  we conduct experiments on
the Clothing1M dataset [64]  containing 1 million images of clothing obtained from online shopping
websites that are with 14 categories  e.g.  T-shirt  Shirt  Knitwear. The labels are generated by using
surrounding texts of the images provided by the sellers  and therefore contain many errors. We use
the 7k clean data as the meta dataset. Following the previous works [65  66]  we used ResNet-50
pre-trained on ImageNet. For preprocessing  we resize the image to 256 × 256  crop the middle
224 × 224 as input  and perform normalization. We used SGD with a momentum 0.9  a weight decay
10−3  and an initial learning rate 0.01  and batch size 32. The learning rate of ResNet-50 is divided
by 10 after 5 epoch (for a total 10 epoch)  and the learning rate of WN-Net is ﬁxed as 10−3.
The results are summarized in Table. 4. which shows that the proposed method achieves the best
performance. Fig. 1(f) plots the tendency curve of the learned MW-Net function  which reveals
abundant data insights. Speciﬁcally  when the loss is with relatively small values  the weighting
function inclines to increase with loss  meaning that it tends to more emphasize hard margin samples
with informative knowledge for classiﬁcation; while when the loss gradually changes large  the

8

0123456789Predicted label0123456789True label96.6%1.3%1.6%0.2%0.1%0.1%0.1%0.8%98.9%0.2%0.1%7.9%0.8%82.9%2.9%2.1%1.7%1.7%6.1%1.5%9.0%70.4%4.2%6.7%1.7%0.4%6.8%0.4%12.2%6.2%70.2%0.9%1.1%2.2%1.9%0.5%13.0%25.6%4.0%53.0%0.6%1.4%3.6%1.4%16.0%11.9%3.9%0.6%62.3%0.1%0.2%11.1%0.5%11.9%10.4%12.7%6.5%0.2%46.4%0.1%0.2%59.5%13.7%2.1%1.9%0.4%0.3%0.1%0.1%21.9%29.6%59.4%1.0%1.9%0.3%0.5%0.1%0.4%0.3%6.5%BaseModel02004006008000123456789Predicted label0123456789True label97.1%0.9%1.2%0.4%0.1%0.1%0.1%0.1%0.7%98.2%0.2%0.3%0.2%0.4%5.5%0.2%83.8%2.9%3.7%2.3%1.1%0.4%2.9%0.7%6.0%74.4%4.3%9.9%0.9%0.7%0.1%2.5%5.2%5.7%82.4%1.6%1.2%1.4%1.8%0.4%6.6%17.3%4.1%68.7%0.5%0.6%3.2%1.1%10.5%11.6%5.4%2.0%65.8%0.3%0.1%4.7%0.4%3.7%10.3%12.1%11.5%0.1%56.9%0.2%51.0%8.7%1.8%2.4%0.8%0.1%0.5%34.1%0.5%22.0%42.0%0.6%2.4%0.4%0.2%0.3%0.4%0.2%31.4%Ours02004006008000.00.20.4Noise Ratio(%)707580859095100Accuracy(%)CIFAR-10BaseModel(ResNet32)Ours(ResNet32)BaseModel(WRN-28-10)Ours(WRN-28-10 )0.00.20.4Noise Ratio(%)40455055606570758085Accuracy(%)CIFAR-100BaseModel(ResNet32)Ours(ResNet32)BaseModel(WRN-28-10)Ours(WRN-28-10)Figure 5: Sample weight distribution on training
data under 40% uniform noise experiments.

Figure 6: Weight variation curves under 40%
uniform noise experiment on CIFAR10 dataset.

Table 4: Classiﬁcation accuracy (%) of all competing methods on the Clothing1M test set.

#
1
2
3
4

Method

Accuracy

Cross Entropy

Bootstrapping [58]

Forward [65]

S-adaptation [12]

68.94
69.12
69.84
70.36

#
5
6
7
8

Method

Accuracy

Joint Optimization [66]

LCCN [67]
MLNT [68]

Ours

72.23
73.07
73.47
73.72

weighting function begins to monotonically decrease  implying that it tends to suppress noise labels
samples with relatively large loss values. Such complicated essence cannot be ﬁnely delivered by
conventional weight functions.

5 Conclusion
We have proposed a novel meta-learning method for adaptively extracting sample weights to guarantee
robust deep learning in the presence of training data bias. Compared with current reweighting methods
that require to manually set the form of weight functions  the new method is able to yield a rational
one directly from data. The working principle of our algorithm can be well explained and the
procedure of our method can be easily reproduced ( Appendix A provide the Pytorch implement
of our algorithm (less than 30 lines of codes))  and the completed training code is avriable at
https://github.com/xjtushujun/meta-weight-net.). Our empirical results show that the
propose method can perform superior in general data bias cases  like class imbalance  corrupted
labels  and more complicated real cases. Besides  such an adaptive weight learning approach is
hopeful to be employed to other weight setting problems in machine learning  like ensemble methods
and multi-view learning.

Acknowledgments

This research was supported by the China NSFC projects under contracts 61661166011  11690011 
61603292  61721002 U1811461. The authors would also like to thank anonymous reviewers for their
constructive suggestions on improving the paper  especially on the proofs and theoretical analysis of
our paper.

References

[1] Mengye Ren  Wenyuan Zeng  Bin Yang  and Raquel Urtasun. Learning to reweight examples

for robust deep learning. In ICML  2018.

[2] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. In ICLR  2017.

[3] Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on

Knowledge & Data Engineering  2008.

[4] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nati Srebro. Exploring

generalization in deep learning. In NeurIPS  2017.

9

0.3500.3750.4000.4250.4500.4750.5000.525Weight05000100001500020000NumbersCIFAR-10_40% noisenoiseclean0.100.150.200.250.300.350.40Weight0200040006000800010000CIFAR-100_40% noisenoiseclean05101520253035400.0060.0040.0020.0000.002WeightsOurs20406080100Epoches0.0500.0250.0000.0250.0500.075WeightsL2RW[5] Devansh Arpit  Stanisław Jastrz˛ebski  Nicolas Ballas  David Krueger  Emmanuel Bengio 
Maxinder S Kanwal  Tegan Maharaj  Asja Fischer  Aaron Courville  Yoshua Bengio  et al. A
closer look at memorization in deep networks. In ICML  2017.

[6] Kenji Kawaguchi  Leslie Pack Kaelbling  and Yoshua Bengio. Generalization in deep learning.

arXiv preprint arXiv:1710.05468  2017.

[7] Roman Novak  Yasaman Bahri  Daniel A Abolaﬁa  Jeffrey Pennington  and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. In ICLR 
2018.

[8] Mikel Galar  Alberto Fernandez  Edurne Barrenechea  Humberto Bustince  and Francisco
Herrera. A review on ensembles for the class imbalance problem: bagging-  boosting-  and
IEEE Transactions on Systems  Man  and Cybernetics  Part C
hybrid-based approaches.
(Applications and Reviews)  2012.

[9] Mateusz Buda  Atsuto Maki  and Maciej A Mazurowski. A systematic study of the class

imbalance problem in convolutional neural networks. Neural Networks  2018.

[10] Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks.

In ICLR workshop  2015.

[11] Samaneh Azadi  Jiashi Feng  Stefanie Jegelka  and Trevor Darrell. Auxiliary image regulariza-

tion for deep cnns with noisy labels. In ICLR  2016.

[12] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta-

tion layer. In ICLR  2017.

[13] Yuncheng Li  Jianchao Yang  Yale Song  Liangliang Cao  Jiebo Luo  and Li-Jia Li. Learning

from noisy labels with distillation. In ICCV  2017.

[14] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural

networks. In NeurIPS  2017.

[15] Dan Hendrycks  Mantas Mazeika  Duncan Wilson  and Kevin Gimpel. Using trusted data to

train deep networks on labels corrupted by severe noise. In NeurIPS  2018.

[16] Bo Han  Quanming Yao  Xingrui Yu  Gang Niu  Miao Xu  Weihua Hu  Ivor Tsang  and Masashi
Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In
NeurIPS  2018.

[17] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural

networks with noisy labels. In NeurIPS  2018.

[18] Wei Bi  Liwei Wang  James T Kwok  and Zhuowen Tu. Learning to predict from crowdsourced

data. In UAI  2014.

[19] Junwei Liang  Lu Jiang  Deyu Meng  and Alexander Hauptmann. Learning to detect concepts

from webly-labeled video data. In IJCAI  2016.

[20] Bohan Zhuang  Lingqiao Liu  Yao Li  Chunhua Shen  and Ian D Reid. Attend in groups: a

weakly-supervised deep learning framework for learning from web data. In CVPR  2017.

[21] Lu Jiang  Zhengyuan Zhou  Thomas Leung  Li-Jia Li  and Li Fei-Fei. Mentornet: Learning

data-driven curriculum for very deep neural networks on corrupted labels. In ICML  2018.

[22] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of computer and system sciences  55(1):119–139  1997.
[23] Yanmin Sun  Mohamed S Kamel  Andrew KC Wong  and Yang Wang. Cost-sensitive boosting

for classiﬁcation of imbalanced data. Pattern Recognition  40(12):3358–3378  2007.

[24] Tomasz Malisiewicz  Abhinav Gupta  and Alexei A Efros. Ensemble of exemplar-svms for

object detection and beyond. In ICCV  2011.

[25] Tsung-Yi Lin  Priyal Goyal  Ross Girshick  Kaiming He  and Piotr Dollár. Focal loss for dense

object detection. IEEE transactions on pattern analysis and machine intelligence  2018.

[26] M Pawan Kumar  Benjamin Packer  and Daphne Koller. Self-paced learning for latent variable

models. In NeurIPS  2010.

[27] De la Torre Fernando and J. Black Mkchael. A framework for robust subspace learning.

International Journal of Computer Vision  54(1):117–142  2003.

10

[28] Lu Jiang  Deyu Meng  Teruko Mitamura  and Alexander G Hauptmann. Easy samples ﬁrst:

Self-paced reranking for zero-example multimedia search. In ACM MM  2014.

[29] Lu Jiang  Deyu Meng  Shoou-I Yu  Zhenzhong Lan  Shiguang Shan  and Alexander Hauptmann.

Self-paced learning with diversity. In NeurIPS  2014.

[30] Yixin Wang  Alp Kucukelbir  and David M Blei. Robust probabilistic modeling with bayesian

data reweighting. In ICML  2017.

[31] Balázs Csanád Csáji. Approximation with artiﬁcial neural networks. Faculty of Sciences  Etvs

Lornd University  Hungary  24:48  2001.

[32] Lijun Wu  Fei Tian  Yingce Xia  Yang Fan  Tao Qin  Lai Jian-Huang  and Tie-Yan Liu. Learning

to teach with dynamic loss functions. In NeurIPS  2018.

[33] Marcin Andrychowicz  Misha Denil  Sergio Gomez  Matthew W Hoffman  David Pfau  Tom
Schaul  Brendan Shillingford  and Nando De Freitas. Learning to learn by gradient descent by
gradient descent. In NeurIPS  2016.

[34] Mostafa Dehghani  Aliaksei Severyn  Sascha Rothe  and Jaap Kamps. Learning to learn from

weak supervision by full supervision. In NeurIPS Workshop  2017.

[35] Luca Franceschi  Paolo Frasconi  Saverio Salzo  and Massimilano Pontil. Bilevel programming

for hyperparameter optimization and meta-learning. In ICML  2018.

[36] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. In NIPS Workshop  2017.

[37] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adapta-

tion of deep networks. In ICML  2017.

[38] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint

arXiv:1803.02999  2  2018.

[39] Amir Erfan Eshratifar  David Eigen  and Massoud Pedram. Gradient agreement as an optimiza-

tion objective for meta-learning. arXiv preprint arXiv:1810.08178  2018.

[40] Nitesh V Chawla  Kevin W Bowyer  Lawrence O Hall  and W Philip Kegelmeyer. Smote:
synthetic minority over-sampling technique. Journal of artiﬁcial intelligence research  16:321–
357  2002.

[41] Qi Dong  Shaogang Gong  and Xiatian Zhu. Class rectiﬁcation hard mining for imbalanced

deep learning. In ICCV  2017.

[42] Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In ICML 

2004.

[43] Charles Elkan. The foundations of cost-sensitive learning. In IJCAI  2001.
[44] Salman H Khan  Munawar Hayat  Mohammed Bennamoun  Ferdous A Sohel  and Roberto
Togneri. Cost-sensitive learning of deep feature representations from imbalanced data. IEEE
transactions on neural networks and learning systems  2018.

[45] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance.

Journal of Big Data  2019.

[46] Haw-Shiuan Chang  Erik Learned-Miller  and Andrew McCallum. Active bias: Training more

accurate neural networks by emphasizing high variance samples. In NeurIPS  2017.

[47] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332–1338  2015.

[48] Jun Shu  Zongben Xu  and Deyu Meng. Small sample learning in big data era. arXiv preprint

arXiv:1808.04572  2018.

[49] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR 

2017.

[50] Jake Snell  Kevin Swersky  and Richard Zemel. Prototypical networks for few-shot learning. In

NeurIPS  2017.

[51] Mostafa Dehghani  Arash Mehrjou  Stephan Gouws  Jaap Kamps  and Bernhard Schölkopf.

Fidelity-weighted learning. In ICLR  2018.

11

[52] Yang Fan  Fei Tian  Tao Qin  Xiang-Yang Li  and Tie-Yan Liu. Learning to teach. In ICLR 

2018.

[53] Yu-Xiong Wang  Deva Ramanan  and Martial Hebert. Learning to model the tail. In NeurIPS 

2017.

[54] Yin Cui  Yang Song  Chen Sun  Andrew Howard  and Serge Belongie. Large scale ﬁne-grained

categorization and domain-speciﬁc transfer learning. In CVPR  2018.

[55] Chen Huang  Yining Li  Chen Change Loy  and Xiaoou Tang. Learning deep representation for

imbalanced classiﬁcation. In CVPR  2016.

[56] Xiao Zhang  Zhiyuan Fang  Yandong Wen  Zhifeng Li  and Yu Qiao. Range loss for deep face

recognition with long-tailed training data. In ICCV  2017.

[57] Andreas Veit  Neil Alldrin  Gal Chechik  Ivan Krasin  Abhinav Gupta  and Serge J Belongie.

Learning from noisy large-scale datasets with minimal supervision. In CVPR  2017.

[58] Scott Reed  Honglak Lee  Dragomir Anguelov  Christian Szegedy  Dumitru Erhan  and Andrew
In ICLR

Rabinovich. Training deep neural networks on noisy labels with bootstrapping.
workshop  2015.

[59] Xingjun Ma  Yisen Wang  Michael E Houle  Shuo Zhou  Sarah M Erfani  Shu-Tao Xia  Sudanthi
Wijewickrema  and James Bailey. Dimensionality-driven learning with noisy labels. In ICML 
2018.

[60] Yin Cui  Menglin Jia  Tsung-Yi Lin  Yang Song  and Serge Belongie. Class-balanced loss based

on effective number of samples. In CVPR  2019.

[61] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In CVPR  2016.

[62] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report  2009.
[63] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMCV  2016.
[64] Tong Xiao  Tian Xia  Yi Yang  Chang Huang  and Xiaogang Wang. Learning from massive

noisy labeled data for image classiﬁcation. In CVPR  2015.

[65] Giorgio Patrini  Alessandro Rozza  Aditya Krishna Menon  Richard Nock  and Lizhen Qu.
Making deep neural networks robust to label noise: A loss correction approach. In CVPR  2017.
[66] Daiki Tanaka  Daiki Ikami  Toshihiko Yamasaki  and Kiyoharu Aizawa. Joint optimization

framework for learning with noisy labels. In CVPR  2018.

[67] Jiangchao Yao  Hao Wu  Ya Zhang  Ivor W Tsang  and Jun Sun. Safeguarded dynamic label

regression for noisy supervision. In AAAI  2019.

[68] Junnan Li  Yongkang Wong  Qi Zhao  and Mohan S. Kankanhalli. Learning to learn from noisy

labeled data. In CVPR  2019.

[69] Julien Mairal. Stochastic majorization-minimization algorithms for large-scale optimization. In

NeurIPS  2013.

12

,Miao Xu
Rong Jin
Zhi-Hua Zhou
Jun Shu
Qi Xie
Lixuan Yi
Qian Zhao
Sanping Zhou
Zongben Xu
Deyu Meng