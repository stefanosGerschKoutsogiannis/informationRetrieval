2013,Online Robust PCA via Stochastic Optimization,Robust PCA methods are typically based on batch optimization and have to load all the samples into memory. This prevents them from efficiently processing big data. In this paper  we develop  an Online Robust Principal Component Analysis (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the data size   significantly enhancing the computation and storage efficiency. The proposed method is based on stochastic optimization of an equivalent reformulation of the batch RPCA method. Indeed  we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust  to sparse corruption. Moreover  OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.,Online Robust PCA via Stochastic Optimization

Jiashi Feng

ECE Department

National University of Singapore

jiashi@nus.edu.sg

Huan Xu

ME Department

National University of Singapore

mpexuh@nus.edu.sg

Shuicheng Yan
ECE Department

National University of Singapore

eleyans@nus.edu.sg

Abstract

Robust PCA methods are typically based on batch optimization and have to load
all the samples into memory during optimization. This prevents them from ef-
ﬁciently processing big data. In this paper  we develop an Online Robust PCA
(OR-PCA) that processes one sample per time instance and hence its memory cost
is independent of the number of samples  signiﬁcantly enhancing the computation
and storage efﬁciency. The proposed OR-PCA is based on stochastic optimization
of an equivalent reformulation of the batch RPCA. Indeed  we show that OR-PCA
provides a sequence of subspace estimations converging to the optimum of its
batch counterpart and hence is provably robust to sparse corruption. Moreover 
OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive
simulations on subspace recovering and tracking demonstrate the robustness and
efﬁciency advantages of the OR-PCA over online PCA and batch RPCA methods.

1

Introduction

Principal Component Analysis (PCA) [19] is arguably the most widely used method for dimension-
ality reduction in data analysis. However  standard PCA is brittle in the presence of outliers and cor-
ruptions [11]. Thus many techniques have been developed towards robustifying it [12  4  24  25  7].
One prominent example is the Principal Component Pursuit (PCP) method proposed in [4] that ro-
bustly ﬁnds the low-dimensional subspace through decomposing the sample matrix into a low-rank
component and an overall sparse component. It is proved that both components can be recovered
exactly through minimizing a weighted combination of the nuclear norm of the ﬁrst term and (cid:96)1
norm of the second one. Thus the subspace estimation is robust to sparse corruptions.
However  PCP and other robust PCA methods are all implemented in a batch manner. They need
to access every sample in each iteration of the optimization. Thus  robust PCA methods require
memorizing all samples  in sharp contrast to standard PCA where only the covariance matrix is
needed. This pitfall severely limits their scalability to big data  which are becoming ubiquitous
now. Moreover  for an incremental samples set  when a new sample is added  the optimization
procedure has to be re-implemented on all available samples. This is quite inefﬁcient in dealing with
incremental sample sets such as network detection  video analysis and abnormal events tracking.
Another pitfall of batch robust PCA methods is that they cannot handle the case where the underlying
subspaces are changing gradually. For example  in the video background modeling  the background
is assumed to be static across different frames for applying robust PCA [4]. Such assumption is too
restrictive in practice. A more realistic situation is that the background is changed gradually along

1

with the camera moving  corresponding to a gradually changing subspace. Unfortunately  traditional
batch RPCA methods may fail in this case.
In order to efﬁciently and robustly estimate the subspace of a large-scale or dynamic samples set 
we propose an Online Robust PCA (OR-PCA) method. OR-PCA processes only one sample per
time instance and thus is able to efﬁciently handle big data and dynamic sample sets  saving the
memory cost and dynamically estimating the subspace of evolutional samples. We brieﬂy explain
our intuition here. The major difﬁculty of implementing the previous RPCA methods  such as PCP 
in an online fashion is that the adopted nuclear norm tightly couples the samples and thus the samples
have to be processed simultaneously. To tackle this  OR-PCA pursues the low-rank component in
a different manner: using an equivalent form of the nuclear norm  OR-PCA explicitly decomposes
the sample matrix into the multiplication of the subspace basis and coefﬁcients plus a sparse noise
component. Through such decomposition  the samples are decoupled in the optimization and can be
processed separately. In particular  the optimization consists of two iterative updating components.
The ﬁrst one is to project the sample onto the current basis and isolate the sparse noise (explaining
the outlier contamination)  and the second one is to update the basis given the new sample.
Our main technical contribution is to show the above mentioned iterative optimization sheme con-
verges to the global optimal solution of the original PCP formulation  thus we establish the validity
of our online method. Our proof is inspired by recent results from [16]  who proposed an online dic-
tionary learning method and provided the convergence guarantee of the proposed online dictionary
learning method. However  [16] can only guarantee that the solution converges to a stationary point
of the optimization problem.
Besides the nice behavior on single subspace recovering  OR-PCA can also be applied for tracking
time-variant subspace naturally  since it updates the subspace estimation timely after revealing one
new sample. We conduct comprehensive simulations to demonstrate the advantages of OR-PCA for
both subspace recovering and tracking in this work.

2 Related Work

The robust PCA algorithms based on nuclear norm minimization to recover low-rank matrices are
now standard  since the seminal works [21  6]. Recent works [4  5] have taken the nuclear norm
minimization approach to the decomposition of a low-rank matrix and an overall sparse matrix.
Different from the setting of samples being corrupted by sparse noise  [25  24] and [7] solve robust
PCA in the case that a few samples are completely corrupted. However  all of these RPCA methods
are implemented in batch manner and cannot be directly adapted to the online setup.
There are only a few pieces of work on online robust PCA [13  20  10]  which we discuss below.
In [13]  an incremental and robust subspace learning method is proposed. The method proposes
to integrate the M-estimation into the standard incremental PCA calculation. Speciﬁcally  each
newly coming data point is re-weighted by a pre-deﬁned inﬂuence function [11] of its residual
to the current estimated subspace. However  no performance guarantee is provided in this work.
In [20]  a compressive sensing based recursive robust PCA algorithm is proposed. The proposed
method essentially solves compressive sensing optimization over a small batch of data to update the
principal components estimation instead of using a single sample  and it is not clear how to extend
the method to the latter case. Recently  He et al. propose an incremental gradient descent method
on Grassmannian manifold for solving the robust PCA problem  named GRASTA [10]. In each
iteration  GRASTA uses the gradient of the updated augmented Lagrangian function after revealing
a new sample to perform the gradient descent. However  no theoretic guarantee of the algorithmic
convergence for GRASTA is provided in this work. Moreover  in the experiments in this work  we
show that our proposed method is more robust than GRASTA to the sparse corruption and achieves
higher breakdown point.
The most closely related work to ours in technique is [16]  which proposes an online learning method
for dictionary learning and sparse coding. Based on that work  [9] proposes an online nonnegative
matrix factorization method. Both works can be seen as solving online matrix factorization problems
with speciﬁc constraints (sparse or non-negative). Though OR-PCA can also be seen as a kind of
matrix factorization  it is essentially different from those two works. In OR-PCA  an additive sparse
noise matrix is considered along with the matrix factorization. Thus the optimization and analysis

2

are different from the ones in those works. In addition  beneﬁtting from explicitly considering the
noise  OR-PCA is robust to sparse contamination  which is absent in either the dictionary learning or
nonnegative matrix factorization works. Most importantly  in sharp contrast to [16  9] which shows
their methods converge to a stationary point  our method is solving essentially a re-formulation of a
convex optimization  and hence we can prove that the method converges to the global optimum.
After this paper was accepted  we found similar works which apply the same main idea of combining
the online learning framework in [16] with the factorization formulation of nuclear norm was pub-
lished in [17  18  23] before. However  in this work  we use different optimization from them. More
speciﬁcally  our proposed algorithm needs not determine the step size or solve a Lasso subproblem.

3 Problem Formulation

3.1 Notation
We use bold letters to denote vectors. In particular  x ∈ Rp denotes an authentic sample without
corruption  e ∈ Rp is for the noise  and z ∈ Rp is for the corrupted observation z = x + e. Here p
denotes the ambient dimension of the observed samples. Let r denote the intrinsic dimension of the
subspace underlying {xi}n
i=1. Let n denote the number of observed samples  t denote the index of
the sample/time instance. We use capital letters to denote matrices  e.g.  Z ∈ Rp×n is the matrix of
observed samples. Each column zi of Z corresponds to one sample. For an arbitrary real matrix E 
i j |Eij| denote the (cid:96)1-norm of E seen as a long
i σi(E) denote its nuclear norm  i.e.  the sum of its singular values.

Let (cid:107)E(cid:107)F denote its Frobenius norm  (cid:107)E(cid:107)(cid:96)1 =(cid:80)
vector in Rp×n  and (cid:107)E(cid:107)∗ =(cid:80)

3.2 Objective Function Formulation

Robust PCA (RPCA) aims to accurately estimate the subspace underlying the observed samples 
even though the samples are corrupted by gross but sparse noise. As one of the most popular RPCA
methods  the Principal Component Pursuit (PCP) method [4] proposes to solve RPCA by decompos-
ing the observed sample matrix Z into a low-rank component X accounting for the low-dimensional
subspace plus an overall sparse component E incorporating the sparse corruption. Under mild con-
ditions  PCP guarantees that the two components X and E can be exactly recovered through solving:

min
X E

1
2

(cid:107)Z − X − E(cid:107)2

F + λ1(cid:107)X(cid:107)∗ + λ2(cid:107)E(cid:107)1.

(1)

To solve the problem in (1)  iterative optimization methods such as Accelerated Proximal Gradient
(APG) [15] or Augmented Lagrangian Multiplier (ALM) [14] methods are often used. However 
these optimization methods are implemented in a batch manner. In each iteration of the optimization 
they need to access all samples to perform SVD. Hence a huge storage cost is incurred when solving
RPCA for big data (e.g.  web data  large image set).
In this paper  we consider online implementation of PCP. The main difﬁculty is that the nuclear norm
couples all the samples tightly and thus the samples cannot be considered separately as in typical
online optimization problems. To overcome this difﬁculty  we use an equivalent form of the nuclear
norm for the matrix X whose rank is upper bounded by r  as follows [21] 

(cid:26) 1

(cid:27)

.

(cid:107)X(cid:107)∗ =

inf

L∈Rp×r R∈Rn×r

(cid:107)L(cid:107)2

F +

1
2

(cid:107)R(cid:107)2

F : X = LRT

2

Namely  the nuclear norm is re-formulated as an explicit low-rank factorization of X. Such nuclear
norm factorization is developed in [3] and well established in recent works [22  21]. In this decom-
position  L ∈ Rp×r can be seen as the basis of the low-dimensional subspace and R ∈ Rn×r denotes
the coefﬁcients of the samples w.r.t. the basis. Thus  the RPCA problem (1) can be re-formulated as

min

X L∈Rp×r R∈Rn×r E

(cid:107)Z − X − E(cid:107)2

F +

1
2

λ1
2

((cid:107)L(cid:107)2

F + (cid:107)R(cid:107)2

F ) + λ2(cid:107)E(cid:107)1  s.t. X = LRT .

Substituting X by LRT and removing the constraint  the above problem is equivalent to:
F ) + λ2(cid:107)E(cid:107)1.

(cid:107)Z − LRT − E(cid:107)2

F + (cid:107)R(cid:107)2

((cid:107)L(cid:107)2

F +

min

L∈Rp×r R∈Rn×r E

1
2

λ1
2

3

(2)

(3)

(4)

(cid:18) 1

t(cid:88)

2

i=1

(cid:19)

Though the reformulated objective function is not jointly convex w.r.t. the variables L and R  we
prove below that the local minima of (2) are global optimal solutions to original problem in (1). The
details are given in the next section.
Given a ﬁnite set of samples Z = [z1  . . .   zn] ∈ Rp×n  solving problem (2) indeed minimizes the
following empirical cost function 

n(cid:88)

i=1

fn(L) (cid:44) 1
n

(cid:96)(zi  L) +

(cid:107)L(cid:107)2
F  

λ1
2n

where the loss function for each sample is deﬁned as

(cid:96)(zi  L) (cid:44) min

r e

1
2

(cid:107)zi − Lr − e(cid:107)2

2 +

(cid:107)r(cid:107)2

2 + λ2(cid:107)e(cid:107)1.

λ1
2

The loss function measures the representation error for the sample z on a ﬁxed basis L  where the
coefﬁcients on the basis r and the sparse noise e associated with each sample are optimized to
minimize the loss. In the stochastic optimization  one is usually interested in the minimization of
the expected cost overall all the samples [16] 

f (L) (cid:44) Ez[(cid:96)(z  L)] = lim

n→∞ fn(L) 

(5)

where the expectation is taken w.r.t. the distribution of the samples z. In this work  we ﬁrst establish
a surrogate function for this expected cost and then optimize the surrogate function for obtaining the
subspace estimation in an online fashion.

4 Stochastic Optimization Algorithm for OR-PCA

We now present our Online Robust PCA (OR-PCA) algorithm. The main idea is to develop a
stochastic optimization algorithm to minimize the empirical cost function (3)  which processes one
sample per time instance in an online manner. The coefﬁcients r  noise e and basis L are optimized
in an alternative manner. In the t-th time instance  we obtain the estimation of the basis Lt through
minimizing the cumulative loss w.r.t. the previously estimated coefﬁcients {ri}t
i=1 and sparse noise
{ei}t

i=1. The objective function for updating the basis Lt is deﬁned as 

gt(L) (cid:44) 1
t

(cid:107)zi − Lri − ei(cid:107)2

2 +

(cid:107)ri(cid:107)2

2 + λ2(cid:107)ei(cid:107)1

λ1
2

+

(cid:107)L(cid:107)2
F .

λ1
2t

(6)

This is a surrogate function of the empirical cost function ft(L) deﬁned in (3)  i.e.  it provides an
upper bound for ft(L): gt(L) ≥ ft(L).
The proposed algorithm is summarized in Algorithm 1. Here  the subproblem in (7) involves solving
a small-size convex optimization problem  which can be solved efﬁciently by the off-the-shelf solver
(see the supplementary material). To update the basis matrix L  we adopt the block-coordinate
descent with warm restarts [2]. In particular  each column of the basis L is updated individually
while ﬁxing the other columns.
The following theorem is the main theoretic result of the paper  which states that the solution from
Algorithm 1 will converge to the optimal solution of the batch optimization. Thus  the proposed
OR-PCA converges to the correct low-dimensional subspace even in the presence of sparse noise 
as long as the batch version – PCP – works.
Theorem 1. Assume the observations are always bounded. Given the rank of the optimal solution
to (5) is provided as r  and the solution Lt ∈ Rp×r provided by Algorithm 1 is full rank  then Lt
converges to the optimal solution of (5) asymptotically.

Note that the assumption that observations are bounded is quite natural for the realistic data (such as
images  videos). We ﬁnd in the experiments that the ﬁnal solution Lt is always full rank. A standard
stochastic gradient descent method may further enhance the computational efﬁciency  compared
with the used method here. We leave the investigation for future research.

4

Algorithm 1 Stochastic Optimization for OR-PCA

Input: {z1  . . .   zT} (observed data which are revealed sequentially)  λ1  λ2 ∈ R (regularization
parameters)  L0 ∈ Rp×r  r0 ∈ Rr  e0 ∈ Rp (initial solutions)  T (number of iterations).
for t = 1 to T do

1) Reveal the sample zt.
2) Project the new sample:

{rt  et} = arg min

(cid:107)zt − Lt−1r − e(cid:107)2

2 +

1
2

(cid:107)r(cid:107)2

2 + λ2(cid:107)e(cid:107)1.

λ1
2

3) At ← At−1 + rtrT
4) Compute Lt with Lt−1 as warm restart using Algorithm 2:

t   Bt ← Bt−1 + (zt − et)rT
t .

Tr(cid:2)LT (At + λ1I) L(cid:3) − Tr(LT Bt).

Lt (cid:44) arg min

1
2

end for
Return XT = LT RT

T (low-rank data matrix)  ET (sparse noise matrix).

Algorithm 2 The Basis Update

Input: L = [l1  . . .   lr] ∈ Rp×r  A = [a1  . . .   ar] ∈ Rr×r  and B = [b1  . . .   br] ∈ Rp×r.
˜A ← A + λ1I.
for j = 1 to r do

lj ← 1
˜Aj j

(bj − L˜aj) + lj.

end for
Return L.

5 Proof Sketch

(7)

(8)

(9)

In this section we sketch the proof of Theorem 1. The details are deferred to the supplementary
material due to space limit.
The proof of Theorem 1 proceeds in the following four steps: (I) we ﬁrst prove that the surrogate
function gt(Lt) converges almost surely; (II) we then prove that the solution difference behaves as
(cid:107)Lt − Lt−1(cid:107)F = O(1/t); (III) based on (II) we show that f (Lt) − gt(Lt) → 0 almost surely  and
the gradient of f vanishes at the solution Lt when t → ∞; (IV) ﬁnally we prove that Lt actually
converges to the optimum solution of the problem (5).
Theorem 2 (Convergence of the surrogate function gt). Let gt denote the surrogate function deﬁned
in (6). Then  gt(Lt) converges almost surely when the solution Lt is given by Algorithm 1.

We prove Theorem 2  i.e.  the convergence of the stochastic positive process gt(Lt) > 0  by showing
that it is a quasi-martingale. We ﬁrst show that the summation of the positive difference of gt(Lt) is
bounded utilizing the fact that gt(Lt) upper bounds the empirical cost ft(Lt) and the loss function
(cid:96)(zt  Lt) is Lipschitz. These imply that gt(Lt) is a quasi-martingale. Applying the lemma from [8]
about the convergence of quasi-martingale  we conclude that gt(Lt) converges.
Next  we show the difference of the two successive solutions converges to 0 as t goes to inﬁnity.
Theorem 3 (Difference of the solution Lt). For the two successive solutions obtained from Algo-
rithm 1  we have

(cid:107)Lt+1 − Lt(cid:107)F = O(1/t) a.s.

To prove the above result  we ﬁrst show that the function gt(L) is strictly convex. This holds since the
regularization component λ1(cid:107)L(cid:107)2
F naturally guarantees that the eigenvalues of the Hessian matrix
are bounded away from zero. Notice that this is essentially different from [16]  where one has to
assume that the smallest eigenvalue of the Hessian matrix is lower bounded. Then we further show

5

that variation of the function gt(L)  gt(Lt)− gt+1(Lt)  is Lipschitz if using the updating rule shown
in Algorithm 2. Combining these two properties establishes Theorem 3.
In the third step  we show that the expected cost function f (Lt) is a smooth one  and the difference
f (Lt)− gt(Lt) goes to zero when t → ∞. In order for showing the regularity of the function f (Lt) 
we ﬁrst provide the following optimality condition of the loss function (cid:96)(Lt).
Lemma 1 (Optimality conditions of Problem (4)). r(cid:63) ∈ Rr and e(cid:63) ∈ Rp is a solution of Problem (4)
if and only if

Λ) = λ2sign(e(cid:63)

CΛ(zΛ − e(cid:63)
|CΛc(zΛc − e(cid:63)
Λc)| ≤ λ2  otherwise 
r(cid:63) = (LT L + λ1I)−1LT (z − e(cid:63)) 

Λ) 

where C = I − L(LT L + λ1I)−1LT and CΛ denotes the columns of matrix C indexed by Λ =
{j|e(cid:63)[j] (cid:54)= 0} and Λc denotes the complementary set of Λ. Moreover  the optimal solution is unique.

Based on the above lemma  we can prove that the solution r(cid:63) and e(cid:63) are Lipschitz w.r.t. the basis L.
Then  we can obtain the following results about the regularity of the expected cost function f.
Lemma 2. Assume the observations z are always bounded. Deﬁne
(cid:107)r(cid:107)2

{r(cid:63)  e(cid:63)} = arg min

(cid:107)z − Lr − e(cid:107)2

2 + λ2(cid:107)e(cid:107)1.

2 +

λ1
2

1
2

r e

Then  1) the function (cid:96) deﬁned in (4) is continuously differentiable and

∇L(cid:96)(z  L) = (Lr(cid:63) + e(cid:63) − z)r(cid:63)T ;

2) ∇f (L) = Ez[∇L(cid:96)(z  L)]; and 3)∇f (L) is Lipschitz.
Equipped with the above regularities of the expected cost function f  we can prove the convergence
of f  as stated in the following theorem.
Theorem 4 (Convergence of f). Let gt denote the surrogate function deﬁned in (2). Then  1)
f (Lt) − gt(Lt) converges almost surely to 0; and 2) f (Lt) converges almost surely  when the
solution Lt is given by Algorithm 1.

Following the techniques developed in [16]  we can show the solution obtained from Algorithm 1 
L∞  satisﬁes the ﬁrst order optimality condition for minimizing the expected cost f (L). Thus the
OR-PCA algorithm provides a solution converging to a stationary point of the expected loss.
Theorem 5. The ﬁrst order optimal condition for minimizing the objective function in (5) is satisﬁed
by Lt  the solution provided by Algorithm 1  when t tends to inﬁnity.

Finally  to complete the proof  we establish the following result stating that any full-rank L that
satisﬁes the ﬁrst order condition is the global optimal solution.
Theorem 6. When the solution L satisﬁes the ﬁrst order condition for minimizing the objective
function in (5)   the obtained solution L is the optimal solution of the problem (5) if L is full rank.

Combining Theorem 5 and Theorem 6 directly yields Theorem 1 – the solution from Algorithm 1
converges to the optimal solution of Problem (5) asymptotically.

6 Empirical Evaluation

We report some numerical results in this section. Due to space constraints  more results  including
those of subspace tracking  are deferred in the supplementary material.

6.1 Medium-scale Robust PCA

We here evaluate the ability of the proposed OR-PCA of correctly recovering the subspace of cor-
rupted observations  under various settings of the intrinsic subspace dimension and error density. In
particular  we adopt the batch robust PCA method  Principal Component Pursuit [4]  as the batch

6

(a) Batch RPCA

(b) OR-PCA

(c) ρs = 0.1

(d) ρs = 0.3

Figure 1: (a) and (b): subspace recovery performance under different corruption fraction ρs (verti-
cal axis) and rank/n (horizontal axis). Brighter color means better performance; (c) and (d): the
performance comparison of the OR-PCA  Grasta  and online PCA methods against the number of
revealed samples under two different corruption levels ρs with PCP as reference.

counterpart of the proposed OR-PCA method for reference. PCP estimates the subspace in a batch
manner through solving the problem in (1) and outputs the low-rank data matrix. For fair compari-
son  we follow the data generation scheme of PCP as in [4]: we generate a set of n clean data points
as a product of X = U V T   where the sizes of U and V are p × r and n × r respectively. The
elements of both U and V are i.i.d. sampled from the N (0  1/n) distribution. Here U is the basis of
the subspace and the intrinsic dimension of the subspace spanned by U is r. The observations are
generated through Z = X + E  where E is a sparse matrix with a fraction of ρs non-zero elements.
The elements in E are from a uniform distribution over the interval of [−1000  1000]. Namely  the
matrix E contains gross but sparse errors.
We run the OR-PCA and the PCP algorithms 10 times under the following settings: the ambient
dimension and number of samples are set as p = 400 and n = 1  000; the intrinsic rank r of
√
the subspace varies from 4 to 200; the value of error fraction  ρs  varies from very sparse 0.01 to
relatively dense 0.5. The trade-off parameters of OR-PCA are ﬁxed as λ1 = λ2 = 1/
p. The
performance is evaluated by the similarity between the subspace obtained from the algorithms and
the groundtruth.
In particular  the similarity is measured by the Expressed Variance (E.V.) (see
deﬁnition in [24]). A larger value of E.V. means better subspace recovery.
We plot the averaged E.V. values of PCP and OR-PCA under different settings in a matrix form  as
shown in Figure 1(a) and Figure 1(b) respectively. The results demonstrate that under relatively low
intrinsic dimension (small rank/n) and sparse corruption (small ρs)  OR-PCA is able to recover
the subspace nearly perfectly (E.V.= 1). We also observe that the performance of OR-PCA is
close to that of the PCP. This demonstrates that the proposed OR-PCA method achieves comparable
performance with the batch method and veriﬁes our convergence guarantee on the OR-PCA. In the
relatively difﬁcult setting (high intrinsic dimension and dense error  shown in the top-right of the
matrix)  OR-PCA performs slightly worse than the PCP  possibly because the number of streaming
samples is not enough to achieve convergence.
To better demonstrate the robustness of OR-PCA to corruptions and illustrate how the performance
of OR-PCA is improved when more samples are revealed  we plot the performance curve of OR-
PCA against the number of samples in Figure 1(c)  under the setting of p = 400  n = 1  000 
ρs = 0.1  r = 80  and the results are averaged from 10 repetitions. We also apply GRASTA [10] to
solve this RPCA problem in an online fashion as a baseline. The parameters of GRASTA are set as
the values provided in the implementation package provided by the authors. We observe that when
more samples are revealed  both OR-PCA and GRASTA steadily improve the subspace recovery.
However  our proposed OR-PCA converges much faster than GRASTA  possibly because in each
iteration OR-PCA obtains the optimal closed-form solution to the basis updating subproblem while
GRASTA only takes one gradient descent step. Observe from the ﬁgure that after 200 samples are
revealed  the performance of OR-PCA is already satisfactory (E.V.> 0.8). However  for GRASTA 
it needs about 400 samples to achieve the same performance. To show the robustness of the pro-
posed OR-PCA  we also plot the performance of the standard online (or incremental) PCA [1] for
comparison. This work focuses on developing online robust PCA. The non-robustness of (online)
PCA is independent of used optimization method. Thus  we only compare with the basic online
PCA method [1]  which is enough for comparing robustness. The comparison results are given in
Figure 1(c). We observe that as expected  the online PCA cannot recover the subspace correctly
(E.V.≈ 0.1)  since standard PCA is fragile to gross corruptions. We then increase the corruption

7

Batch RPCArank/nρs0.10.20.30.40.50.50.40.30.20.1Online RPCArank/nρs0.10.20.30.40.50.50.40.30.20.10200400600800100000.20.40.60.81Number of SamplesE.V.  OR−PCAGrastaonline PCAbatch RPCA200400600800100000.20.40.60.81Number of SamplesE.V.  OR−PCAGrastaonline PCAbatch RPCAlevel to ρs = 0.3  and plot the performance curve of the above methods in Figure 1(d). From the
plot  it can be observed that the performance of GRASTA decreases severely (E.V.≈ 0.3) while
OR-PCA still achieves E.V. ≈ 0.8. The performance of PCP is around 0.88. This result clearly
demonstrates the robustness advantage of OR-PCA over GRASTA. In fact  from other simulation
results under different settings of intrinsic rank and corruption level (see supplementary material) 
we observe that the GRASTA breaks down at 25% corruption (the value of E.V. is zero). However 
OR-PCA achieves a performance of E.V.≈ 0.5  even in presence of 50% outlier corruption.

6.2 Large-scale Robust PCA

We now investigate the computational efﬁciency of OR-PCA and the performance for large scale
data. The samples are generated following the same model as explained in the above subsection.
The results are provided in Table 1. All of the experiments are implemented in a PC with 2.83GHz
Quad CPU and 8GB RAM. Note that batch RPCA cannot process these data due to out of memory.

Table 1: The comparison of OR-PCA and GRASTA under different settings of sample size (n) and
ambient dimensions (p). Here ρs = 0.3  r = 0.1p. The corresponding computational time (in ×103
seconds) is shown in the top row and the E.V. values are shown in the bottom row correspondingly.
The results are based on the average of 5 repetitions and the variance is shown in the parentheses.

p
n

1 × 106

1 × 103
1 × 108

1 × 1010

1 × 106

1 × 108

1 × 104

0.013(0.0004)

OR-PCA
0.99(0.01)
GRASTA 0.023(0.0008)
0.54(0.08)

1.312(0.082)

139.233(7.747)

0.633(0.047)

15.910(2.646)

0.99(0.00)

0.99(0.00)

0.82(0.09)

0.82(0.01)

2.137(0.016)

240.271(7.564)

2.514(0.011)

252.630(2.096)

0.55(0.02)

0.57(0.03)

0.45(0.02)

0.46(0.03)

From the above results  we observe that OR-PCA is much more efﬁcient and performs better than
GRASTA. In fact  the computational time of OR-PCA is linear in the sample size and nearly linear
in the ambient dimension. When the ambient dimension is large (p = 1× 104)  OR-PCA is more ef-
ﬁcient than GRASTA with an order magnitude efﬁciency enhancement. We then compare OR-PCA
with batch PCP. In each iteration  batch PCP needs to perform an SVD plus a thresholding operation 
whose complexity is O(np2). In contrast  for OR-PCA  in each iteration  the computational cost is
O(pr2)  which is independent of the sample size and linear in the ambient dimension. To see this 
note that in step 2) of Algorithm 1  the computation complexity is O(r2 + pr + r3). Here O(r3) is
for computing LT L. The complexity of step 3) is O(r2 + pr). For step 4) (i.e.  Algorithm 2)  the
cost is O(pr2) (updating each column of L requires O(pr) and there are r columns in total). Thus
the total complexity is O(r2 + pr + r3 + pr2). Since p (cid:29) r  the overall complexity is O(pr2).
The memory cost is signiﬁcantly reduced too. The memory required for OR-PCA is O(pr)  which
is independent of the sample size. This is much smaller than the memory cost of the batch PCP
algorithm (O(pn))  where n (cid:29) p for large scale dataset. This is quite important for processing big
data. The proposed OR-PCA algorithm can be easily parallelized to further enhance its efﬁciency.

7 Conclusions

In this work  we develop an online robust PCA (OR-PCA) method. Different from previous batch
based methods  the OR-PCA need not “remember” all the past samples and achieves much higher
storage efﬁciency. The main idea of OR-PCA is to reformulate the objective function of PCP (a
widely applied batch RPCA algorithm) by decomposing the nuclear norm to an explicit product of
two low-rank matrices  which can be solved by a stochastic optimization algorithm. We provide the
convergence analysis of the OR-PCA method and show that OR-PCA converges to the solution of
batch RPCA asymptotically. Comprehensive simulations demonstrate the effectiveness of OR-PCA.

Acknowledgments

J. Feng and S. Yan are supported by the Singapore National Research Foundation under its Inter-
national Research Centre @Singapore Funding Initiative and administered by the IDM Programme
Ofﬁce. H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier
Two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133.

8

References
[1] M. Artac  M. Jogan  and A. Leonardis. Incremental pca for on-line visual learning and recogni-
tion. In Pattern Recognition  2002. Proceedings. 16th International Conference on  volume 3 
pages 781–784. IEEE  2002.

[2] D.P. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[3] Samuel Burer and Renato Monteiro. A nonlinear programming algorithm for solving semidef-

inite programs via low-rank factorization. Math. Progam.  2003.

[4] E.J. Candes  X. Li  Y. Ma  and J. Wright.

ArXiv:0912.3599  2009.

Robust principal component analysis?

[5] V. Chandrasekaran  S. Sanghavi  P.A. Parrilo  and A.S. Willsky. Rank-sparsity incoherence for

matrix decomposition. SIAM Journal on Optimization  21(2):572–596  2011.

[6] M. Fazel. Matrix rank minimization with applications. PhD thesis  PhD thesis  Stanford

University  2002.

[7] J. Feng  H. Xu  and S. Yan. Robust PCA in high-dimension: A deterministic approach. In

ICML  2012.

[8] D.L. Fisk. Quasi-martingales. Transactions of the American Mathematical Society  1965.
[9] N. Guan  D. Tao  Z. Luo  and B. Yuan. Online nonnegative matrix factorization with robust
stochastic approximation. Neural Networks and Learning Systems  IEEE Transactions on 
23(7):1087–1099  2012.

[10] Jun He  Laura Balzano  and John Lui. Online robust subspace tracking from partial informa-

tion. arXiv preprint arXiv:1109.3827  2011.

[11] P.J. Huber  E. Ronchetti  and MyiLibrary. Robust statistics. John Wiley & Sons  New York 

1981.

[12] M. Hubert  P.J. Rousseeuw  and K.V. Branden. Robpca: a new approach to robust principal

component analysis. Technometrics  2005.

[13] Y. Li. On incremental and robust subspace learning. Pattern recognition  2004.
[14] Z. Lin  M. Chen  and Y. Ma. The augmented lagrange multiplier method for exact recovery of

corrupted low-rank matrices. arXiv preprint arXiv:1009.5055  2010.

[15] Z. Lin  A. Ganesh  J. Wright  L. Wu  M. Chen  and Y. Ma. Fast convex optimization algorithms
for exact recovery of a corrupted low-rank matrix. Computational Advances in Multi-Sensor
Adaptive Processing (CAMSAP)  2009.

[16] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse

coding. JMLR  2010.

[17] Morteza Mardani  Gonzalo Mateos  and G Giannakis. Dynamic anomalography: Tracking

network anomalies via sparsity and low rank. 2012.

[18] Morteza Mardani  Gonzalo Mateos  and Georgios B Giannakis. Rank minimization for sub-

space tracking from incomplete data. In ICASSP  2013.

[19] K. Pearson. On lines and planes of closest ﬁt to systems of points in space. Philosophical

Magazine  1901.

[20] C. Qiu  N. Vaswani  and L. Hogben. Recursive robust pca or recursive sparse recovery in large

but structured noise. arXiv preprint arXiv:1211.3754  2012.

[21] B. Recht  M. Fazel  and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix

equations via nuclear norm minimization. SIAM review  52(3):471–501  2010.

[22] Jasson Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative

prediction. In ICML  2005.

[23] Pablo Sprechmann  Alex M Bronstein  and Guillermo Sapiro. Learning efﬁcient sparse and

low rank models. arXiv preprint arXiv:1212.3631  2012.

[24] H. Xu  C. Caramanis  and S. Mannor. Principal component analysis with contaminated data:

The high dimensional case. In COLT  2010.

[25] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. Information Theory 

IEEE Transactions on  58(5):3047–3064  2012.

9

,Jiashi Feng
Huan Xu
Shuicheng Yan
Debarghya Ghoshdastidar
Ambedkar Dukkipati
ABHISEK KUNDU
Petros Drineas
Malik Magdon-Ismail
Ziang Yan
Yiwen Guo
Changshui Zhang