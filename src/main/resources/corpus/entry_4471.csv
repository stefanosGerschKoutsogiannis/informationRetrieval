2013,Provable Subspace Clustering: When LRR meets SSC,Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for {\em subspace clustering}. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of Self-Expressiveness''. The main difference is that SSC minimizes the vector $\ell_1$ norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank   we propose a new algorithm  termed Low-Rank Sparse Subspace Clustering (LRSSC)  by combining SSC and LRR  and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the "Self-Expressiveness Property'' and "Graph Connectivity'' at the same time.",Provable Subspace Clustering:

When LRR meets SSC

Yu-Xiang Wang

School of Computer Science
Carnegie Mellon University
Pittsburgh  PA 15213 USA
yuxiangw@cs.cmu.edu

Huan Xu

Dept. of Mech. Engineering
National Univ. of Singapore

Singapore  117576

Chenlei Leng

Department of Statistics
University of Warwick
Coventry  CV4 7AL  UK

mpexuh@nus.edu.sg

C.Leng@warwick.ac.uk

Abstract

Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both
considered as the state-of-the-art methods for subspace clustering. The two meth-
ods are fundamentally similar in that both are convex optimizations exploiting the
intuition of “Self-Expressiveness”. The main difference is that SSC minimizes the
vector (cid:96)1 norm of the representation matrix to induce sparsity while LRR mini-
mizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the
representation matrix is often simultaneously sparse and low-rank  we propose a
new algorithm  termed Low-Rank Sparse Subspace Clustering (LRSSC)  by com-
bining SSC and LRR  and develops theoretical guarantees of when the algorithm
succeeds. The results reveal interesting insights into the strength and weakness of
SSC and LRR and demonstrate how LRSSC can take the advantages of both meth-
ods in preserving the “Self-Expressiveness Property” and “Graph Connectivity” at
the same time.

1

Introduction

We live in the big data era – a world where an overwhelming amount of data is generated and collect-
ed every day  such that it is becoming increasingly impossible to process data in its raw form  even
though computers are getting exponentially faster over time. Hence  compact representations of data
such as low-rank approximation (e.g.  PCA [13]  Matrix Completion [4]) and sparse representation
[6] become crucial in understanding the data with minimal storage. The underlying assumption is
that high-dimensional data often lie in a low-dimensional subspace [4]). Yet  when such data points
are generated from different sources  they form a union of subspaces. Subspace Clustering deals
with exactly this structure by clustering data points according to their underlying subspaces. Appli-
cation include motion segmentation and face clustering in computer vision [16  8]  hybrid system
identiﬁcation in control [26  2]  community clustering in social networks [12]  to name a few.
Numerous algorithms have been proposed to tackle the problem. Recent examples include GP-
CA [25]  Spectral Curvature Clustering [5]  Sparse Subspace Clustering (SSC) [7  8]  Low Rank
Representation (LRR) [17  16] and its noisy variant LRSC [9] (for a more exhaustive survey of sub-
space clustering algorithms  we refer readers to the excellent survey paper [24] and the references
therein). Among these algorithms  LRR and SSC  based on minimizing the nuclear norm and (cid:96)1
norm of the representation matrix respectively  remain the top performers on the Hopkins155 mo-
tion segmentation benchmark dataset [23]. Moreover  they are among the few subspace clustering
algorithms supported with theoretic guarantees: Both algorithms are known to succeed when the
subspaces are independent [27  16]. Later  [8] showed that subspace being disjoint is sufﬁcient
for SSC to succeed1  and [22] further relaxed this condition to include some cases of overlapping
1 Disjoint subspaces only intersect at the origin. It is a less restrictive assumption comparing to independent

subspaces  e.g.  3 coplanar lines passing the origin are not independent  but disjoint.

1

subspaces. Robustness of the two algorithms has been studied too. Liu et al. [18] showed that a vari-
ant of LRR works even in the presence of some arbitrarily large outliers  while Wang and Xu [29]
provided both deterministic and randomized guarantees for SSC when data are noisy or corrupted.
Despite LRR and SSC’s success  there are questions unanswered. LRR has never been shown to
succeed other than under the very restrictive “independent subspace” assumption. SSC’s solution is
sometimes too sparse that the afﬁnity graph of data from a single subspace may not be a connected
body [19]. Moreover  as our experiment with Hopkins155 data shows  the instances where SSC fails
are often different from those that LRR fails. Hence  a natural question is whether combining the
two algorithms lead to a better method  in particular since the underlying representation matrix we
want to recover is both low-rank and sparse simultaneously.
In this paper  we propose Low-Rank Sparse Subspace Clustering (LRSSC)  which minimizes a
weighted sum of nuclear norm and vector 1-norm of the representation matrix. We show theoretical
guarantees for LRSSC that strengthen the results in [22]. The statement and proof also shed insight
on why LRR requires independence assumption. Furthermore  the results imply that there is a fun-
damental trade-off between the interclass separation and the intra-class connectivity. Indeed  our
experiment shows that LRSSC works well in cases where data distribution is skewed (graph connec-
tivity becomes an issue for SSC) and subspaces are not independent (LRR gives poor separation).
These insights would be useful when developing subspace clustering algorithms and applications.
We remark that in the general regression setup  the simultaneous nuclear norm and 1-norm regular-
ization has been studied before [21]. However  our focus is on the subspace clustering problem  and
hence the results and analysis are completely different.

2 Problem Setup
Notations: We denote the data matrix by X ∈ Rn×N   where each column of X (normalized to
unit vector) belongs to a union of L subspaces

S1 ∪ S2 ∪ ... ∪ SL.

Each subspace (cid:96) contains N(cid:96) data samples with N1 + N2 + ... + NL = N. We observe the noisy
data matrix X. Let X ((cid:96)) ∈ Rn×N(cid:96) denote the selection (as a set and a matrix) of columns in
X that belong to S(cid:96) ⊂ Rn  which is an d(cid:96)-dimensional subspace. Without loss of generality  let
X = [X (1)  X (2)  ...  X (L)] be ordered. In addition  we use (cid:107) · (cid:107) to represent Euclidean norm (for
vectors) or spectral norm (for matrices) throughout the paper.
Method: We solve the following convex optimization problem

(cid:107)C(cid:107)∗ + λ(cid:107)C(cid:107)1

min

C

LRSSC :

diag(C) = 0.

s.t. X = XC 

(1)
Spectral clustering techniques (e.g.  [20]) are then applied on the afﬁnity matrix W = |C| + |C|T
where C is the solution to (1) to obtain the ﬁnal clustering and |·| is the elementwise absolute value.
Criterion of success: In the subspace clustering task  as opposed to compressive sensing or matrix
completion  there is no “ground-truth” C to compare the solution against. Instead  the algorithm
succeeds if each sample is expressed as a linear combination of samples belonging to the same
subspace  i.e.  the output matrix C are block diagonal (up to appropriate permutation) with each
subspace cluster represented by a disjoint block. Formally  we have the following deﬁnition.
Deﬁnition 1 (Self-Expressiveness Property (SEP)). Given subspaces {S(cid:96)}L
(cid:96)=1 and data points X
from these subspaces  we say a matrix C obeys Self-Expressiveness Property  if the nonzero entries of
each ci (ith column of C) corresponds to only those columns of X sampled from the same subspace
as xi.

Note that the solution obeying SEP alone does not imply the clustering is correct  since each block
may not be fully connected. This is the so-called “graph connectivity” problem studied in [19].
On the other hand  failure to achieve SEP does not necessarily imply clustering error either  as the
spectral clustering step may give a (sometimes perfect) solution even when there are connections
between blocks. Nevertheless  SEP is the condition that veriﬁes the design intuition of SSC and
LRR. Notice that if C obeys SEP and each block is connected  we immediately get the correct
clustering.

2

3 Theoretic Guanratees

3.1 The Deterministic Setup

Before we state our theoretical results for the deterministic setup  we need to deﬁne a few quantities.
Deﬁnition 2 (Normalized dual matrix set). Let {Λ1(X)} be the set of optimal solutions to
(Λ3) = 0 

(cid:107)Λ2(cid:107)∞ ≤ λ  (cid:107)X T Λ1 − Λ2 − Λ3(cid:107) ≤ 1  diag

(cid:104)X  Λ1(cid:105)

max

s.t.

⊥

where (cid:107) · (cid:107)∞ is the vector (cid:96)∞ norm and diag
[ν∗
1   ...  ν∗
{Λ1(X)}  we deﬁne normalized dual matrix V for X as

N ] ∈ {Λ1(X)} obey ν∗

⊥ selects all the off-diagonal entries. Let Λ∗ =
i ∈ span(X) for every i = 1  ...  N.2 For every Λ = [ν1  ...  νN ] ∈

Λ1 Λ2 Λ3

(cid:20) ν1(cid:107)ν∗

V (X) (cid:44)

1(cid:107)   ... 

νN(cid:107)ν∗
N(cid:107)

(cid:21)

 

and the normalized dual matrix set {V (X)} as the collection of V (X) for all Λ ∈ {Λ1(X)}.
Deﬁnition 3 (Minimax subspace incoherence property). Compactly denote V ((cid:96)) = V (X ((cid:96))). We
say the vector set X ((cid:96)) is µ-incoherent to other points if

µ ≥ µ(X ((cid:96))) := min

V ((cid:96))∈{V ((cid:96))} max

x∈X\X ((cid:96))

(cid:107)V ((cid:96))T

x(cid:107)∞.

SL) =(cid:80)

The incoherence µ in the above deﬁnition measures how separable the sample points in S(cid:96) are a-
gainst sample points in other subspaces (small µ represents more separable data). Our deﬁnition
differs from Soltanokotabi and Candes’s deﬁnition of subspace incoherence [22] in that it is deﬁned
as a minimax over all possible dual directions. It is easy to see that µ-incoherence in [22  Deﬁni-
tion 2.4] implies µ-minimax-incoherence as their dual direction are contained in {V (X)}. In fact 
in several interesting cases  µ can be signiﬁcantly smaller under the new deﬁnition. We illustrate the
point with the two examples below and leave detailed discussions in the supplementary materials.
Example 1 (Independent Subspace). Suppose the subspaces are independent  i.e.  dim(S1 ⊕ ... ⊕
(cid:96)=1 ... L dim(S(cid:96))  then all X ((cid:96)) are 0-incoherent under our Deﬁnition 3. This is because
for each X ((cid:96)) one can always ﬁnd a dual matrix V ((cid:96)) ∈ {V ((cid:96))} whose column space is orthogonal to
the span of all other subspaces. To contrast  the incoherence parameter according to Deﬁnition 2.4
in [22] will be a positive value  potentially large if the angles between subspaces are small.
Example 2 (Random except 1 subspace). Suppose we have L disjoint 1-dimensional subspaces
in Rn (L > n). S1  ... SL−1 subspaces are randomly drawn. SL is chosen such that its angle
to one of the L − 1 subspace  say S1  is π/6. Then the incoherence parameter µ(X (L)) deﬁned
in [22] is at least cos(π/6). However under our new deﬁnition  it is not difﬁcult to show that
µ(X (L)) ≤ 2

(cid:113) 6 log(L)

with high probability3.

n

The result also depends on the smallest singular value of a rank-d matrix (denoted by σd) and the
inradius of a convex body as deﬁned below.
Deﬁnition 4 (inradius). The inradius of a convex body P  denoted by r(P)  is deﬁned as the radius
of the largest Euclidean ball inscribed in P.
The smallest singular value and inradius measure how well-represented each subspace is by its data
samples. Small inradius/singular value implies either insufﬁcient data  or skewed data distribution 
in other word  it means that the subspace is “poorly represented”. Now we may state our main result.
Theorem 1 (LRSSC). Self-expressiveness property holds for the solution of (1) on the data X
if there exists a weighting parameter λ such that for all (cid:96) = 1  ...  L  one of the following two
conditions holds:

(cid:112)

µ(X ((cid:96)))(1 + λ

N(cid:96)) < λ min

or

µ(X ((cid:96)))(1 + λ) < λ min

k

k

σd(cid:96) (X ((cid:96))−k) 
r(conv(±X ((cid:96))−k)) 

(2)

(3)

2If this is not unique  pick the one with least Frobenious norm.
3The full proof is given in the supplementary. Also it is easy to generalize this example to d-dimensional

subspaces and to “random except K subspaces”.

3

where X−k denotes X with its kth column removed and σd(cid:96) (X ((cid:96))−k) represents the dth
non-zero) singular value of the matrix X ((cid:96))−k.
We brieﬂy explain the intuition of the proof. The theorem is proven by duality. First we write out
the dual problem of (1) 

(smallest

(cid:96)

Dual LRSSC : max

Λ1 Λ2 Λ3

(cid:104)X  Λ1(cid:105)

s.t. (cid:107)Λ2(cid:107)∞ ≤ λ  (cid:107)X T Λ1 − Λ2 − Λ3(cid:107) ≤ 1  diag

⊥

(Λ3) = 0.

This leads to a set of optimality conditions  and leaves us to show the existence of a dual certiﬁcate
satisfying these conditions. We then construct two levels of ﬁctitious optimizations (which is the
main novelty of the proof) and construct a dual certiﬁcate from the dual solution of the ﬁctitious
optimization problems. Under condition (2) and (3)  we establish this dual certifacte meets all opti-
mality conditions  hence certifying that SEP holds. Due to space constraints  we defer the detailed
proof to the supplementary materials and focus on the discussions of the results in the main text.
Remark 1 (SSC). Theorem 1 can be considered a generalization of Theorem 2.5 of [22]. Indeed 
when λ → ∞  (3) reduces to the following

µ(X ((cid:96))) < min

k

r(conv(±X ((cid:96))−k)).

The readers may observe that this is exactly the same as Theorem 2.5 of [22]  with the only difference
being the deﬁnition of µ. Since our deﬁnition of µ(X ((cid:96))) is tighter (i.e.  smaller) than that in [22] 
our guarantee for SSC is indeed stronger. Theorem 1 also implies that the good properties of SSC
(such as overlapping subspaces  large dimension) shown in [22] are also valid for LRSSC for a range
of λ greater than a threshold.

To further illustrate the key difference from [22]  we describe the following scenario.
Example 3 (Correlated/Poorly Represented Subspaces). Suppose the subspaces are poorly repre-
sented  i.e.  the inradius r is small. If furthermore  the subspaces are highly correlated  i.e.  canonical
angles between subspaces are small  then the subspace incoherence µ(cid:48) deﬁned in [22] can be quite
large (close to 1). Thus  the succeed condition µ(cid:48) < r presented in [22] is violated. This is an
important scenario because real data such as those in Hopkins155 and Extended YaleB often suffer
from both problems  as illustrated in [8  Figure 9 & 10]. Using our new deﬁnition of incoherence
µ  as long as the subspaces are “sufﬁciently independent”4 (regardless of their correlation) µ will
assume very small values (e.g.  Example 2)  making SEP possible even if r is small  namely when
subspaces are poorly represented.
Remark 2 (LRR). The guarantee is the strongest when λ → ∞ and becomes superﬁcial when
λ → 0 unless subspaces are independent (see Example 1). This seems to imply that the “independent
subspace” assumption used in [16  18] to establish sufﬁcient conditions for LRR (and variants) to
work is unavoidable.5 On the other hand  for each problem instance  there is a λ∗ such that whenever
λ > λ∗  the result satisﬁes SEP  so we should expect phase transition phenomenon when tuning λ.
Remark 3 (A tractable condition). Condition (2) is based on singular values  hence is computa-
tionally tractable. In contrast  the veriﬁcation of (3) or the deterministic condition in [22] is NP-
Complete  as it involves computing the inradii of V-Polytopes [10]. When λ → ∞  Theorem 1
reduces to the ﬁrst computationally tractable guarantee for SSC that works for disjoint and poten-
tially overlapping subspaces.

3.2 Randomized Results

We now present results for the random design case  i.e.  data are generated under some random
models.
Deﬁnition 5 (Random data). “Random sampling” assumes that for each (cid:96)  data points in X ((cid:96))
are iid uniformly distributed on the unit sphere of S(cid:96). “Random subspace” assumes each S(cid:96) is
generated independently by spanning d(cid:96) iid uniformly distributed vectors on the unit sphere of Rn.

4Due to space constraint  the concept is formalized in supplementary materials.
5Our simulation in Section 6 also supports this conjecture.

4

(cid:32)(cid:114) N(cid:96)

d(cid:96)

(cid:114) log N(cid:96)

(cid:33)

d(cid:96)

Lemma 1 (Singular value bound). Assume random sampling. If d(cid:96) < N(cid:96) < n  then there exists an
absolute constant C1 such that with probability of at least 1 − N−10

 

(cid:96)

σd(cid:96) (X) ≥ 1
2

− 3 − C1

 

or simply

σd(cid:96)(X) ≥ 1
4

if we assume N(cid:96) ≥ C2d(cid:96)  for some constant C2.
Lemma 2 (Inradius bound [1  22]). Assume random sampling of N(cid:96) = κ(cid:96)d(cid:96) data points in each S(cid:96) 

then with probability larger than 1 −(cid:80)L

d(cid:96)N(cid:96)

(cid:114) N(cid:96)

 

d(cid:96)

(cid:115)
(cid:96)=1 N(cid:96)e−√
r(conv(±X ((cid:96))−k)) ≥ c(κ(cid:96))

log (κ(cid:96))

2d(cid:96)

for all pairs ((cid:96)  k).

Here  c(κ(cid:96)) is a constant depending on κ(cid:96). When κ(cid:96) is sufﬁciently large  we can take c(κ(cid:96)) = 1/

Combining Lemma 1 and Lemma 2  we get the following remark showing that conditions (2) and
(3) are complementary.
Remark 4. Under the random sampling assumption  when λ is smaller than a threshold  the singular
value condition (2) is better than the inradius condition (3). Speciﬁcally  σd(cid:96) (X) > 1
with
4
d(cid:96)
high probability  so for some constant C > 1  the singular value condition is strictly better if

√

8.

(cid:17)
(cid:16)√
N(cid:96) −(cid:112)log (N(cid:96)/d(cid:96))
(cid:17)  
(cid:16)
1 +(cid:112)log (N(cid:96)/d(cid:96))

N(cid:96)

C
√

λ <

or when N(cid:96) is large  λ <

(cid:113) N(cid:96)
1 +(cid:112)log (N(cid:96)/d(cid:96))

C

.

By further assuming random subspace  we provide an upper bound of the incoherence µ.
Lemma 3 (Subspace incoherence bound). Assume random subspace and random sampling. It holds
with probability greater than 1 − 2/N that for all (cid:96) 

(cid:114)

µ(X ((cid:96))) ≤

6 log N

n

.

Combining Lemma 1 and Lemma 3  we have the following theorem.
Theorem 2 (LRSSC for random data). Suppose L rank-d subspace are uniformly and independently
generated from Rn  and N/L data points are uniformly and independently sampled from the unit
sphere embedded in each subspace  furthermore N > CdL for some absolute constant C  then SEP
holds with probability larger than 1 − 2/N − 1/(Cd)10  if

d <

n

96 log N

 

for all λ >

(cid:113) N

L

(cid:16)(cid:113) n
(cid:17) .
96d log N − 1

1

(4)

The above condition is obtained from the singular value condition. Using the inradius guarantee 
combined with Lemma 2 and 3  we have a different succeed condition requiring d < n log(κ)
96 log N for all
. Ignoring constant terms  the condition on d is slightly better than (4) by a log
λ >

1(cid:113) n log κ
96d log N −1

factor but the range of valid λ is signiﬁcantly reduced.

4 Graph Connectivity Problem

The graph connectivity problem concerns when SEP is satisﬁed  whether each block of the solution
C to LRSSC represents a connected graph. The graph connectivity problem concerns whether each
disjoint block (since SEP holds true) of the solution C to LRSSC represents a connected graph. This
is equivalent to the connectivity of the solution of the following ﬁctitious optimization problem 
where each sample is constrained to be represented by the samples of the same subspace 

(cid:107)C ((cid:96))(cid:107)∗ + λ(cid:107)C ((cid:96))(cid:107)1

min
C((cid:96))

s.t. X ((cid:96)) = X ((cid:96))C ((cid:96)) 

diag(C ((cid:96))) = 0.

(5)

5

The graph connectivity for SSC is studied by [19] under deterministic conditions (to make the prob-
lem well-posed). They show by a negative example that even if the well-posed condition is satisﬁed 
the solution of SSC may not satisfy graph connectivity if the dimension of the subspace is greater
than 3. On the other hand  graph connectivity problem is not an issue for LRR: as the following
proposition suggests  the intra-class connections of LRR’s solution are inherently dense (fully con-
nected).
Proposition 1. When the subspaces are independent  X is not full-rank and the data points are
randomly sampled from a unit sphere in each subspace  then the solution to LRR  i.e. 

(cid:107)C(cid:107)∗

min

C

s.t. X = XC 

is class-wise dense  namely each diagonal block of the matrix C is all non-zero.

The proof makes use of the following lemma which states the closed-form solution of LRR.
Lemma 4 ([16]). Take skinny SVD of data matrix X = U ΣV T . The closed-form solution to LRR
is the shape interaction matrix C = V V T .

Proposition 1 then follows from the fact that each entry of V V T has a continuous distribution 
hence the probability that any is exactly zero is negligible (a complete argument is given in the
supplementary).
Readers may notice that when λ → 0  (5) is not exactly LRR  but with an additional constraint
that diagonal entries are zero. We suspect this constrained version also have dense solution. This is
demonstrated numerically in Section 6.

5 Practical issues

5.1 Data noise/sparse corruptions/outliers

The natural extension of LRSSC to handle noise is

min

C

1
2

(cid:107)X − XC(cid:107)2

F + β1(cid:107)C(cid:107)∗ + β2(cid:107)C(cid:107)1

s.t. diag(C) = 0.

(6)

We believe it is possible (but maybe tedious) to extend our guarantee to this noisy version following
the strategy of [29] which analyzed the noisy version of SSC. This is left for future research.
According to the noisy analysis of SSC  a rule of thumb of choosing the scale of β1 and β2 is

β1 =

σ( 1
1+λ )
√
2 log N

 

β2 =

σ( λ
1+λ )
√
2 log N

 

where λ is the tradeoff parameter used in noiseless case (1)  σ is the estimated noise level and N is
the total number of entries.
In case of sparse corruption  one may use (cid:96)1 norm penalty instead of the Frobenious norm. For
outliers  SSC is proven to be robust to them under mild assumptions [22]  and we suspect a similar
argument should hold for LRSSC too.

5.2 Fast Numerical Algorithm

As subspace clustering problem is usually large-scale  off-the-shelf SDP solvers are often too slow
to use. Instead  we derive alternating direction methods of multipliers (ADMM) [3]  known to be
scalable  to solve the problem numerically. The algorithm involves separating out the two objectives
and diagonal constraints with dummy variables C2 and J like

min

(cid:107)C1(cid:107)∗ + λ(cid:107)C2(cid:107)1

C1 C2 J
s.t. X = XJ 

J = C2 − diag(C2) 

J = C1 

(7)

and update J  C1  C2 and the three dual variables alternatively. Thanks to the change of variables 
all updates can be done in closed-form. To further speed up the convergence  we adopt the adap-
tive penalty mechanism of Lin et.al [15]  which in some way ameliorates the problem of tuning
numerical parameters in ADMM. Detailed derivations  update rules  convergence guarantee and the
corresponding ADMM algorithm for the noisy version of LRSSC are made available in the supple-
mentary materials.

6

GiniIndex (vec(CM)) = 1 − 2

|M|(cid:88)

k=1

ck(cid:107)(cid:126)c(cid:107)1

(cid:18)|M| − k + 1/2

(cid:19)

|M|

.

6 Numerical Experiments

To verify our theoretical results and illustrate the advantages of LRSSC  we design several numerical
experiments. Due to space constraints  we discuss only two of them in the paper and leave the rest to
the supplementary materials. In all our numerical experiments  we use the ADMM implementation
of LRSSC with ﬁxed set of numerical parameters. The results are given against an exponential grid
of λ values  so comparisons to only 1-norm (SSC) and only nuclear norm (LRR) are clear from two
ends of the plots.

6.1 Separation-Sparsity Tradeoff

We ﬁrst illustrate the tradeoff of the solution between obeying SEP and being connected (this is
measured using the intra-class sparsity of the solution). We randomly generate L subspaces of
dimension 10 from R50. Then  50 unit length random samples are drawn from each subspace and
we concatenate into a 50 × 50L data matrix. We use Relative Violation [29] to measure of the
violation of SEP and Gini Index [11] to measure the intra-class sparsity6. These quantities are
deﬁned below:

RelViolation (C M) =

(cid:80)
(cid:80)
(i j) /∈M |C|i j
(i j)∈M |C|i j

 

where M is the index set that contains all (i  j) such that xi  xj ∈ S(cid:96) for some (cid:96).
GiniIndex (C M) is obtained by ﬁrst sorting the absolute value of Cij∈M into a non-decreasing
sequence (cid:126)c = [c1  ...  c|M|]  then evaluate

Note that RelViolation takes the value of [0 ∞] and SEP is attained when RelViolation is zero.
Similarly  Gini index takes its value in [0  1] and it is larger when intra-class connections are sparser.
The results for L = 6 and L = 11 are shown in Figure 1. We observe phase transitions for both
metrics. When λ = 0 (corresponding to LRR)  the solution does not obey SEP even when the
independence assumption is only slightly violated (L = 6). When λ is greater than a threshold 
RelViolation goes to zero. These observations match Theorems 1 and 2. On the other hand  when λ
is large  intra-class sparsity is high  indicating possible disconnection within the class.
Moreover  we observe that there exists a range of λ where RelViolation reaches zero yet the sparsity
level does not reaches its maximum. This justiﬁes our claim that the solution of LRSSC  taking λ
within this range  can achieve SEP and at the same time keep the intra-class connections relatively
dense. Indeed  for the subspace clustering task  a good tradeoff between separation and intra-class
connection is important.

6.2 Skewed data distribution and model selection

In this experiment  we use the data for L = 6 and combine the ﬁrst two subspaces into one 20-
dimensional subspace and randomly sample 10 more points from the new subspace to “connect”
the 100 points from the original two subspaces together. This is to simulate the situation when data
distribution is skewed  i.e.  the data samples within one subspace has two dominating directions.
The skewed distribution creates trouble for model selection (judging the number of subspaces)  and
intuitively  the graph connectivity problem might occur.
We ﬁnd that model selection heuristics such as the spectral gap [28] and spectral gap ratio [14] of
the normalized Laplacian are good metrics to evaluate the quality of the solution of LRSSC. Here
the correct number of subspaces is 5  so the spectral gap is the difference between the 6th and 5th
smallest singular value and the spectral gap ratio is the ratio of adjacent spectral gaps. The larger
these quantities  the better the afﬁnity matrix reveals that the data contains 5 subspaces.

6We choose Gini Index over the typical (cid:96)0 to measure sparsity as the latter is vulnerable to numerical

inaccuracy.

7

Figure 1: Illustration of the separation-sparsity trade-off. Left: 6 subspaces. Right: 11 subspace.

Figure 2 demonstrates how singular values change when λ increases. When λ = 0 (corresponding
to LRR)  there is no signiﬁcant drop from the 6th to the 5th singular value  hence it is impossible for
either heuristic to identify the correct model. As λ increases  the last 5 singular values gets smaller
and become almost zero when λ is large. Then the 5-subspace model can be correctly identiﬁed
using spectral gap ratio. On the other hand  we note that the 6th singular value also shrinks as λ
increases  which makes the spectral gap very small on the SSC side and leaves little robust margin
for correct model selection against some violation of SEP. As is shown in Figure 3  the largest
spectral gap and spectral gap ratio appear at around λ = 0.1  where the solution is able to beneﬁt
from both the better separation induced by the 1-norm factor and the relatively denser connections
promoted by the nuclear norm factor.

Figure 2: Last 20 singular values of the normalized
Laplacian in the skewed data experiment.

Figure 3: Spectral Gap and Spectral Gap
Ratio in the skewed data experiment.

7 Conclusion and future works

In this paper  we proposed LRSSC for the subspace clustering problem and provided theoretical
analysis of the method. We demonstrated that LRSSC is able to achieve perfect SEP for a wider
range of problems than previously known for SSC and meanwhile maintains denser intra-class con-
nections than SSC (hence less likely to encounter the “graph connectivity” issue). Furthermore  the
results offer new understandings to SSC and LRR themselves as well as problems such as skewed
data distribution and model selection. An important future research question is to mathematically
deﬁne the concept of the graph connectivity  and establish conditions that perfect SEP and connec-
tivity indeed occur together for some non-empty range of λ for LRSSC.

Acknowledgments

H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier Two
grant R-265-000-443-112 and NUS startup grant R-265-000-384-133.

8

References
[1] D. Alonso-Guti´errez. On the isotropy constant of random convex sets. Proceedings of the American

Mathematical Society  136(9):3293–3300  2008.

[2] L. Bako. Identiﬁcation of switched linear systems via sparse optimization. Automatica  47(4):668–677 

2011.
[3] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning 
3(1):1–122  2011.

[4] E.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Computa-

tional mathematics  9(6):717–772  2009.

[5] G. Chen and G. Lerman. Spectral curvature clustering (SCC). International Journal of Computer Vision 

81(3):317–330  2009.

[6] M. Elad. Sparse and redundant representations. Springer  2010.
[7] E. Elhamifar and R. Vidal. Sparse subspace clustering. In Computer Vision and Pattern Recognition

(CVPR’09)  pages 2790–2797. IEEE  2009.

[8] E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm  theory  and applications. to appear in

IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  2013.

[9] P. Favaro  R. Vidal  and A. Ravichandran. A closed form solution to robust subspace estimation and

clustering. In Computer Vision and Pattern Recognition (CVPR’11)  pages 1801–1807. IEEE  2011.

[10] P. Gritzmann and V. Klee. Computational complexity of inner and outerj-radii of polytopes in ﬁnite-

dimensional normed spaces. Mathematical programming  59(1):163–213  1993.

[11] N. Hurley and S. Rickard. Comparing measures of sparsity. Information Theory  IEEE Transactions on 

55(10):4723–4741  2009.

[12] A. Jalali  Y. Chen  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex optimization.

In International Conference on Machine Learning (ICML’11)  pages 1001–1008  2011.

[13] I.T. Jolliffe. Principal component analysis  volume 487. Springer-Verlag New York  1986.
[14] F. Lauer and C. Schnorr. Spectral clustering of linear subspaces for motion segmentation. In International

Conference on Computer Vision (ICCV’09)  pages 678–685. IEEE  2009.

[15] Z. Lin  R. Liu  and Z. Su. Linearized alternating direction method with adaptive penalty for low-rank
representation. In Advances in Neural Information Processing Systems 24 (NIPS’11)  pages 612–620.
2011.

[16] G. Liu  Z. Lin  S. Yan  J. Sun  Y. Yu  and Y. Ma. Robust recovery of subspace structures by low-rank

representation. IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)  2012.

[17] G. Liu  Z. Lin  and Y. Yu. Robust subspace segmentation by low-rank representation. In International

Conference on Machine Learning (ICML’10)  pages 663–670  2010.

[18] G. Liu  H. Xu  and S. Yan. Exact subspace segmentation and outlier detection by low-rank representation.

In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’12)  2012.

[19] B. Nasihatkon and R. Hartley. Graph connectivity in sparse subspace clustering. In Computer Vision and

Pattern Recognition (CVPR’11)  pages 2137–2144. IEEE  2011.

[20] A.Y. Ng  M.I. Jordan  Y. Weiss  et al. On spectral clustering: Analysis and an algorithm. In Advances in

Neural Information Processing Systems 15 (NIPS’02)  volume 2  pages 849–856  2002.

[21] E. Richard  P. Savalle  and N. Vayatis. Estimation of simultaneously sparse and low rank matrices. In

International Conference on Machine learning (ICML’12)  2012.

[22] M. Soltanolkotabi and E.J. Candes. A geometric analysis of subspace clustering with outliers. The Annals

of Statistics  40(4):2195–2238  2012.

[23] R. Tron and R. Vidal. A benchmark for the comparison of 3-d motion segmentation algorithms.

Computer Vision and Pattern Recognition (CVPR’07)  pages 1–8. IEEE  2007.

In

[24] R. Vidal. Subspace clustering. Signal Processing Magazine  IEEE  28(2):52–68  2011.
[25] R. Vidal  Y. Ma  and S. Sastry. Generalized principal component analysis (gpca). IEEE Transactions on

Pattern Analysis and Machine Intelligence  27(12):1945–1959  2005.

[26] R. Vidal  S. Soatto  Y. Ma  and S. Sastry. An algebraic geometric approach to the identiﬁcation of a
class of linear hybrid systems. In Decision and Control  2003. Proceedings. 42nd IEEE Conference on 
volume 1  pages 167–172. IEEE  2003.

[27] R. Vidal  R. Tron  and R. Hartley. Multiframe motion segmentation with missing data using powerfac-

torization and gpca. International Journal of Computer Vision  79(1):85–105  2008.

[28] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing  17(4):395–416  2007.
[29] Y.X. Wang and H. Xu. Noisy sparse subspace clustering.

In International Conference on Machine

Learning (ICML’13)  volume 28  pages 100–108  2013.

9

,Yu-Xiang Wang
Huan Xu
Chenlei Leng