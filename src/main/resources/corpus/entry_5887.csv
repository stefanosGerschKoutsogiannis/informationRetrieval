2018,Video-to-Video Synthesis,We study the problem of video-to-video synthesis  whose goal is to learn a mapping function from an input source video (e.g.  a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart  the image-to-image translation problem  is a popular topic  the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics  directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper  we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators  coupled with a spatio-temporal adversarial objective  we achieve high-resolution  photorealistic  temporally coherent video results on a diverse set of input formats including segmentation masks  sketches  and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.  In particular  our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long  which significantly advances the state-of-the-art of video synthesis. Finally  we apply our method to future video prediction  outperforming several competing systems. Code  models  and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.),Video-to-Video Synthesis

Ting-Chun Wang1  Ming-Yu Liu1  Jun-Yan Zhu2  Guilin Liu1 

Andrew Tao1  Jan Kautz1  Bryan Catanzaro1

{tingchunw mingyul guilinl atao jkautz bcatanzaro}@nvidia.com 

1NVIDIA  2MIT CSAIL

junyanz@mit.edu

Abstract

We study the problem of video-to-video synthesis  whose goal is to learn a mapping
function from an input source video (e.g.  a sequence of semantic segmentation
masks) to an output photorealistic video that precisely depicts the content of the
source video. While its image counterpart  the image-to-image translation problem 
is a popular topic  the video-to-video synthesis problem is less explored in the
literature. Without modeling temporal dynamics  directly applying existing image
synthesis approaches to an input video often results in temporally incoherent videos
of low visual quality. In this paper  we propose a video-to-video synthesis approach
under the generative adversarial learning framework. Through carefully-designed
generators and discriminators  coupled with a spatio-temporal adversarial objective 
we achieve high-resolution  photorealistic  temporally coherent video results on
a diverse set of input formats including segmentation masks  sketches  and poses.
Experiments on multiple benchmarks show the advantage of our method compared
to strong baselines. In particular  our model is capable of synthesizing 2K resolution
videos of street scenes up to 30 seconds long  which signiﬁcantly advances the
state-of-the-art of video synthesis. Finally  we apply our method to future video
prediction  outperforming several competing systems. Code  models  and more
results are available at our website.

1

Introduction

The capability to model and recreate the dynamics of our visual world is essential to building
intelligent agents. Apart from purely scientiﬁc interests  learning to synthesize continuous visual
experiences has a wide range of applications in computer vision  robotics  and computer graphics.
For example  in model-based reinforcement learning [2  24]  a video synthesis model ﬁnds use in
approximating visual dynamics of the world for training the agent with less amount of real experience
data. Using a learned video synthesis model  one can generate realistic videos without explicitly
specifying scene geometry  materials  lighting  and dynamics  which would be cumbersome but
necessary when using a standard graphics rendering engine [35].
The video synthesis problem exists in various forms  including future video prediction [15  18  42  45 
50  64  67  70  76] and unconditional video synthesis [59  66  68]. In this paper  we study a new form:
video-to-video synthesis. At the core  we aim to learn a mapping function that can convert an input
video to an output video. To the best of our knowledge  a general-purpose solution to video-to-video
synthesis has not yet been explored by prior work  although its image counterpart  the image-to-image
translation problem  is a popular research topic [6  31  33  43  44  63  65  72  81  82]. Our method is
inspired by previous application-speciﬁc video synthesis methods [58  60  61  74].
We cast the video-to-video synthesis problem as a distribution matching problem  where the goal is
to train a model such that the conditional distribution of the synthesized videos given input videos
resembles that of real videos. To this end  we learn a conditional generative adversarial model [20]

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Generating a photorealistic video from an input segmentation map video on Cityscapes.
Top left: input. Top right: pix2pixHD. Bottom left: COVST. Bottom right: vid2vid (ours). The
ﬁgure is best viewed with Acrobat Reader. Click the image to play the video clip.

given paired input and output videos  With carefully-designed generators and discriminators  and a
new spatio-temporal learning objective  our method can learn to synthesize high-resolution  photore-
alistic  temporally coherent videos. Moreover  we extend our method to multimodal video synthesis.
Conditioning on the same input  our model can produce videos with diverse appearances.
We conduct extensive experiments on several datasets on the task of converting a sequence of
segmentation masks to photorealistic videos. Both quantitative and qualitative results indicate that
our synthesized footage looks more photorealistic than those from strong baselines. See Figure 1
for example. We further demonstrate that the proposed approach can generate photorealistic 2K
resolution videos  up to 30 seconds long. Our method also grants users ﬂexible high-level control
over the video generation results. For example  a user can easily replace all the buildings with trees in
a street view video. In addition  our method works for other input video formats such as face sketches
and body poses  enabling many applications from face swapping to human motion transfer. Finally 
we extend our approach to future prediction and show that our method can outperform existing
systems. Please visit our website for code  models  and more results.

2 Related Work

Generative Adversarial Networks (GANs). We build our model on GANs [20]. During GAN
training  a generator and a discriminator play a zero-sum game. The generator aims to produce
realistic synthetic data so that the discriminator cannot differentiate between real and the synthesized
data. In addition to noise distributions [14  20  55]  various forms of data can be used as input to the
generator  including images [33  43  81]  categorical labels [52  53]  and textual descriptions [56  79].
Such conditional models are called conditional GANs  and allow ﬂexible control over the output of
the model. Our method belongs to the category of conditional video generation with GANs. However 
instead of predicting future videos conditioning on the current observed frames [41  50  68]  our
method synthesizes photorealistic videos conditioning on manipulable semantic representations  such
as segmentation masks  sketches  and poses.
Image-to-image translation algorithms transfer an input image from one domain to a corresponding
image in another domain. There exists a large body of work for this problem [6  31  33  43  44  63  65 
72  81  82]. Our approach is their video counterpart. In addition to ensuring that each video frame
looks photorealistic  a video synthesis model also has to produce temporally coherent frames  which
is a challenging task  especially for a long duration video.
Unconditional video synthesis. Recent work [59  66  68] extends the GAN framework for un-
conditional video synthesis  which learns a generator for converting a random vector to a video.

2

VGAN [68] uses a spatio-temporal convolutional network. TGAN [59] projects a latent code to a
set of latent image codes and uses an image generator to convert those latent image codes to frames.
MoCoGAN [66] disentangles the latent space to motion and content subspaces and uses a recurrent
neural network to generate a sequence of motion codes. Due to the unconditional setting  these
methods often produce low-resolution and short-length videos.
Future video prediction. Conditioning on the observed frames  video prediction models are trained
to predict future frames [15  18  36  41  42  45  50  64  67  70  71  76]. Many of these models are trained
with image reconstruction losses  often producing blurry videos due to the classic regress-to-the-mean
problem. Also  they fail to generate long duration videos even with adversarial training [42  50]. The
video-to-video synthesis problem is substantially different because it does not attempt to predict
object motions or camera motions. Instead  our approach is conditional on an existing video and can
produce high-resolution and long-length videos in a different domain.
Video-to-video synthesis. While video super-resolution [61  62]  video matting and blending [3  12] 
and video inpainting [73] can be considered as special cases of the video-to-video synthesis problem 
existing approaches rely on problem-speciﬁc constraints and designs. Hence  these methods cannot
be easily applied to other applications. Video style transfer [10  22  28  58]  transferring the style of a
reference painting to a natural scene video  is also related. In Section 4   we show that our method
outperforms a strong baseline that combines a recent video style transfer with a state-of-the-art
image-to-image translation approach.

3 Video-to-Video Synthesis

1 to a sequence of output video frames  ˜xT

1 ≡ {s1  s2  ...  sT} be a sequence of source video frames. For example  it can be a sequence
Let sT
1 ≡ {x1  x2  ...  xT} be the sequence of
of semantic segmentation masks or edge maps. Let xT
corresponding real video frames. The goal of video-to-video synthesis is to learn a mapping function
1 ≡ {˜x1  ˜x2  ...  ˜xT}  so that the
that can convert sT
1 given sT
1 .
conditional distribution of ˜xT
(1)
Through matching the conditional video distributions  the model learns to generate photorealistic 
temporally coherent output sequences as if they were captured by a video camera.
We propose a conditional GAN framework for this conditional video distribution matching task. Let
G be a generator that maps an input source sequence to a corresponding output frame sequence:
xT
1 = G(sT

1 is identical to the conditional distribution of xT
p(˜xT

1 ). We train the generator by solving the minimax optimization problem given by

1 ) = p(xT

1 given sT

1 |sT
1 ).

1 |sT

max

D

min

G

E(xT

1 )[log D(xT

1   sT

1  sT

1 )] + EsT

1

[log(1 − D(G(sT

1 )  sT

1 ))] 

(2)

1 |sT

1 |sT

1 ) and p(xT

1 ) as shown by Goodfellow et al. [20].

where D is the discriminator. We note that as solving (2)  we minimize the Jensen-Shannon divergence
between p(˜xT
Solving the minimax optimization problem in (2) is a well-known  challenging task. Careful designs
of network architectures and objective functions are essential to achieve good performance as shown
in the literature [14  21  30  37  49  51  55  72  79]. We follow the same spirit and propose new network
designs and a spatio-temporal objective for video-to-video synthesis as detailed below.
Sequential generator. To simplify the video-to-video synthesis problem  we make a Markov
assumption where we factorize the conditional distribution p(˜xT

1 ) to a product form given by

1 |sT

T(cid:89)

p(˜xT

1 |sT

1 ) =

p(˜xt|˜xt−1

t−L  st

t−L).

(3)

t=1

In other words  we assume the video frames can be generated sequentially  and the generation of the
t-th frame ˜xt only depends on three factors: 1) current source frame st  2) past L source frames st−1
t−L 
and 3) past L generated frames ˜xt−1
t−L. We train a feed-forward network F to model the conditional
distribution p(˜xt|˜xt−1
1 by applying
the function F in a recursive manner. We found that a small L (e.g.  L = 1) causes training instability 
while a large L increases training time and GPU memory but with minimal quality improvement. In
our experiments  we set L = 2.

t−L). We obtain the ﬁnal output ˜xT

t−L) using ˜xt = F (˜xt−1

t−L  st

t−L  st

3

F (˜xt−1

Video signals contain a large amount of redundant information in consecutive frames. If the optical
ﬂow [46] between consecutive frames is known  we can estimate the next frame by warping the
current frame [54  69]. This estimation would be largely correct except for the occluded areas. Based
on this observation  we model F as

t−L) = (1 − ˜mt) (cid:12) ˜wt−1(˜xt−1) + ˜mt (cid:12) ˜ht 

(4)
where (cid:12) is the element-wise product operator and 1 is an image of all ones. The ﬁrst part corresponds
to pixels warped from the previous frame  while the second part hallucinates new pixels. The
deﬁnitions of the other terms in Equation 4 are given below.
• ˜wt−1 = W (˜xt−1

t−L) is the estimated optical ﬂow from ˜xt−1 to ˜xt  and W is the optical
t−L and

ﬂow prediction network. We estimate the optical ﬂow using both input source images st
previously synthesized images ˜xt−1

t−L. By ˜wt−1(˜xt−1)  we warp ˜xt−1 based on ˜wt−1.

t−L  st

t−L  st

• ˜ht = H(˜xt−1
• ˜mt = M (˜xt−1

t−L  st
t−L  st

t−L) is the hallucinated image  synthesized directly by the generator H.
t−L) is the occlusion mask with continuous values between 0 and 1. M denotes
the mask prediction network. Our occlusion mask is soft instead of binary to better handle the
“zoom in” scenario. For example  when an object is moving closer to our camera  the object will
become blurrier over time if we only warp previous frames. To increase the resolution of the
object  we need to synthesize new texture details. By using a soft mask  we can add details by
gradually blending the warped pixels and the newly synthesized pixels.

We use residual networks [26] for M  W   and H. To generate high-resolution videos  we adopt a
coarse-to-ﬁne generator design similar to the method of Wang et. al [72].
As using multiple discriminators can mitigate the mode collapse problem during GANs training [19 
66  72]  we also design two types of discriminators as detailed below.
Conditional image discriminator DI. The purpose of DI is to ensure that each output frame
resembles a real image given the same source image. This conditional discriminator should output 1
for a true pair (xt  st) and 0 for a fake one (˜xt  st).
Conditional video discriminator DV . The purpose of DV is to ensure that consecutive output
frames resemble the temporal dynamics of a real video given the same optical ﬂow. While DI
t−K be K − 1 optical ﬂow for the
conditions on the source image  DV conditions on the ﬂow. Let wt−2
K consecutive real images xt−1
t−K. This conditional discriminator DV should output 1 for a true pair
(xt−1
t−K  wt−2
We introduce two sampling operators to facilitate the discussion. First  let φI be a random image
sampling operator such that φI (xT
1 ) = (xi  si) where i is an integer uniformly sampled from 1 to
T . In other words  φI randomly samples a pair of images from (xT
1 ). Second  we deﬁne φV as a
sampling operator that randomly retrieve K consecutive frames. Speciﬁcally  φV (wT−1
1 ) =
(wi−2
i−K) where i is an integer uniformly sampled from K + 1 to T + 1. This operator
retrieves K consecutive frames and the corresponding K − 1 optical ﬂow images. With φI and φV  
we are ready to present our learning objective function.
Learning objective function. We train the sequential video synthesis function F by solving

t−K) and 0 for a fake one (˜xt−1

t−K  wt−2

i−K  xi−1

i−K  si−1

t−K).

1   sT

1   sT

1   sT

  xT

1

LV (F  DV )(cid:1) + λWLW (F ) 

LI (F  DI ) + max

F

DI

DV

min

(5)
where LI is the GAN loss on images deﬁned by the conditional image discriminator DI  LV is the
GAN loss on K consecutive frames deﬁned by DV   and LW is the ﬂow estimation loss. The weight
λW is set to 10 throughout the experiments based on a grid search. In addition to the loss terms
in Equation 5  we use the discriminator feature matching loss [40  72] and VGG feature matching
loss [16  34  72] as they improve the convergence speed and training stability [72]. Please see the
supplementary material for more details.
We further deﬁne the image-conditional GAN loss LI [33] using the operator φI
1 )[log(1 − DI (˜xi  si))].

1 )[log DI (xi  si)] + EφI (˜xT

EφI (xT

1  sT

1  sT

(6)

(cid:0) max

Similarly  the video GAN loss LV is given by
EφV (wT −1

1 )[log DV (xi−1

i−K  wi−2

1  sT

 xT

1

i−K)] + EφV (wT−1

1

1 )[log(1 − DV (˜xi−1

i−K  wi−2

i−K))].

 ˜xT

1  sT

(7)

4

Recall that we synthesize a video ˜xT
The ﬂow loss LW includes two terms. The ﬁrst is the endpoint error between the ground truth and
the estimated ﬂow  and the second is the warping loss when the ﬂow warps the previous frame to the
next frame. Let wt be the ground truth ﬂow from xt to xt+1. The ﬂow loss LW is given by

1 by recursively applying F .

(cid:0)|| ˜wt − wt(cid:107)1 + (cid:107) ˜wt(xt) − xt+1(cid:107)1

(cid:1).

T−1(cid:88)

t=1

LW =

1

T − 1

(8)

t−L  st

t−L) and a background model ˜hB t = HB(˜xt−1

t−L  st

F (˜xt−1

Foreground-background prior. When using semantic segmentation masks as the source video  we
can divide an image into foreground and background areas based on the semantics. For example 
buildings and roads belong to the background  while cars and pedestrians are considered as the
foreground. We leverage this strong foreground-background prior in the generator design to further
improve the synthesis performance of the proposed model.
In particular  we decompose the image hallucination network H into a foreground model ˜hF t =
t−L). We note that background motion
HF (st
can be modeled as a global transformation in general  where optical ﬂow can be estimated quite
accurately. As a result  the background region can be generated accurately via warping  and the
background hallucination network HB only needs to synthesize the occluded areas. On the other hand 
a foreground object often has a large motion and only occupies a small portion of the image  which
makes optical ﬂow estimation difﬁcult. The network HF has to synthesize most of the foreground
content from scratch. With this foreground–background prior  F is then given by

t−L) = (1 − ˜mt) (cid:12) ˜wt−1(˜xt−1) + ˜mt (cid:12)(cid:0)(1 − mB t) (cid:12) ˜hF t + mB t (cid:12) ˜hB t

(9)
where mB t is the background mask derived from the ground truth segmentation mask st. This prior
improves the visual quality by a large margin with the cost of minor ﬂickering artifacts. In Table 2 
our user study shows that most people prefer the results with foreground–background modeling. A
qualitative comparison is also included in the supplementary material.
Multimodal synthesis. The synthesis network F is a unimodal mapping function. Given an input
source video  it can only generate one output video. To achieve multimodal synthesis [19  72  82]  we
adopt a feature embedding scheme [72] for the source video that consists of instance-level semantic
segmentation masks. Speciﬁcally  at training time  we train an image encoder E to encode the ground
truth real image xt into a d-dimensional feature map (d = 3 in our experiments). We then apply
an instance-wise average pooling to the map so that all the pixels within the same object share the
same feature vectors. We then feed both the instance-wise averaged feature map zt and the input
semantic segmentation mask st to the generator F . Once training is done  we ﬁt a mixture of Gaussian
distribution to the feature vectors that belong to the same object class. At test time  we sample a
feature vector for each object instance using the estimated distribution of that object class. Given
different feature vectors  the generator F can synthesize videos with different visual appearances.
4 Experiments

(cid:1) 

Implementation details. We train our network in a spatio-temporally progressive manner.
In
particular  we start with generating low-resolution videos with few frames  and all the way up to
generating full resolution videos with 30 (or more) frames. Our coarse-to-ﬁne generator consists of
three scales: 512× 256  1024× 512  and 2048× 1024 resolutions  respectively. The mask prediction
network M and ﬂow prediction network W share all the weights except for the output layer. We
use the multi-scale PatchGAN discriminator architecture [33  72] for the image discriminator DI. In
addition to multi-scale in the spatial resolution  our multi-scale video discriminator DV also looks
at different frame rates of the video to ensure both short-term and long-term consistency. See the
supplementary material for more details.
We train our model for 40 epochs using the ADAM optimizer [39] with lr = 0.0002 and (β1  β2) =
(0.5  0.999) on an NVIDIA DGX1 machine. We use the LSGAN loss [49]. Due to the high image
resolution  even with one short video per batch  we have to use all the GPUs in DGX1 (8 V100 GPUs 
each with 16GB memory) for training. We distribute the generator computation task to 4 GPUs and
the discriminator task to the other 4 GPUs. Training takes ∼ 10 days for 2K resolution.
Datasets. We evaluate the proposed approach on several datasets.

5

Table 1: Comparison between competing video-to-video synthesis approaches on Cityscapes.
Fréchet Inception Dist.

Human Preference Score

pix2pixHD

COVST

vid2vid (ours)

I3D ResNeXt
5.57
5.55
4.66

0.18
0.18
0.15

vid2vid (ours)

/ pix2pixHD

vid2vid (ours)

/ COVST

short seq.
0.87 / 0.13
0.84 / 0.16

long seq.
0.83 / 0.17
0.80 / 0.20

Table 2: Ablation study. We compare the proposed approach to its three variants.

Human Preference Score

vid2vid (ours)

vid2vid (ours)

/ no background–foreground prior

/ no conditional video discriminator

vid2vid (ours)

/ no flow warping

0.80 / 0.20
0.84 / 0.16
0.67 / 0.33

Table 3: Comparison between future video prediction methods on Cityscapes.

Fréchet Inception Dist.

PredNet
MCNet

vid2vid (ours)

I3D
11.18
10.00
3.44

ResNeXt

Human Preference Score

0.59
0.43
0.18

vid2vid (ours)
vid2vid (ours)

/ PredNet
/ MCNet

0.92 / 0.08
0.98 / 0.02

• Cityscapes [13]. The dataset consists of 2048 × 1024 street scene videos captured in several
German cities. Only a subset of images in the videos contains ground truth semantic segmentation
masks. To obtain the input source videos  we use those images to train a DeepLabV3 semantic
segmentation network [11] and apply the trained network to segment all the videos. We use
the optical ﬂow extracted by FlowNet2 [32] as the ground truth ﬂow w. We treat the instance
segmentation masks computed by the Mask R-CNN [25] as our instance-level ground truth. In
summary  the training set contains 2975 videos  each with 30 frames. The validation set consists
of 500 videos  each with 30 frames. Finally  we test our method on three long sequences from
the Cityscapes demo videos  with 600  1100  and 1200 frames  respectively. We will show that
although trained on short videos  our model can synthesize long videos.

• Apolloscape [29] consists of 73 street scene videos captured in Beijing  whose video lengths vary
from 100 to 1000 frames. Similar to Cityscapes  Apolloscape is constructed for the image/video
semantic segmentation task. But we use it for synthesizing videos using the semantic segmentation
mask. We split the dataset into half for training and validation.

• Face video dataset [57]. We use the real videos in the FaceForensics dataset  which contains
854 videos of news brieﬁng from different reporters. We use this dataset for the sketch video to
face video synthesis task. To extract a sequence of sketches from a video  we ﬁrst apply a face
alignment algorithm [38] to localize facial landmarks in each frame. The facial landmarks are
then connected to create the face sketch. For background  we extract Canny edges outside the
face regions. We split the dataset into 704 videos for training and 150 videos for validation.

• Dance video dataset. We download YouTube dance videos for the pose to human motion
synthesis task. Each video is about 3 ∼ 4 minutes long at 1280 × 720 resolution  and we crop the
central 512 × 720 regions. We extract human poses with DensePose [23] and OpenPose [7]  and
directly concatenate the results together. Each training set includes a dance video from a single
dancer  while the test set contains videos of other dance motions or from other dancers.

approach to the video-to-video synthesis task  we process input videos frame-by-frame.

Baselines. We compare our approach to two baselines trained on the same data.
• pix2pixHD [72] is the state-of-the-art image-to-image translation approach. When applying the
• COVST is built on the coherent video style transfer [10] by replacing the stylization network with
pix2pixHD. The key idea in COVST is to warp high-level deep features using optical ﬂow for
achieving temporally coherent outputs. No additional adversarial training is applied. We feed in
ground truth optical ﬂow to COVST  which is impractical for real applications. In contrast  our
model estimates optical ﬂow from source videos.

Evaluation metrics. We use both subjective and objective metrics for evaluation.
• Human preference score. We perform a human subjective test for evaluating the visual quality
of synthesized videos. We use the Amazon Mechanical Turk (AMT) platform. During each

6

Figure 2: Apolloscape results. Left: pix2pixHD. Center: COVST. Right: proposed. The input
semantic segmentation mask video is shown in the left video. The ﬁgure is best viewed with Acrobat
Reader. Click the image to play the video clip.

Figure 3: Example multi-modal video synthesis results. These synthesized videos contain different
road surfaces. The ﬁgure is best viewed with Acrobat Reader. Click the image to play the video clip.

Figure 4: Example results of changing input semantic segmentation masks to generate diverse videos.
Left: tree→building. Right: building→tree. The original video is shown in Figure 3. The ﬁgure is
best viewed with Acrobat Reader. Click the image to play the video clip.

test  an AMT participant is ﬁrst shown two videos at a time (results synthesized by two different
algorithms) and then asked which one looks more like a video captured by a real camera. We
speciﬁcally ask the worker to check for both temporal coherence and image quality. A worker
must have a life-time task approval rate greater than 98% to participate in the evaluation. For each
question  we gather answers from 10 different workers. We evaluate the algorithm by the ratio
that the algorithm outputs are preferred.
• Fréchet Inception Distance (FID) [27] is a widely used metric for implicit generative models  as
it correlates well with the visual quality of generated samples. The FID was originally developed
for evaluating image generation. We propose a variant for video evaluation  which measures both
visual quality and temporal consistency. Speciﬁcally  we use a pre-trained video recognition CNN
as a feature extractor after removing the last few layers from the network. This feature extractor
will be our “inception” network. For each video  we extract a spatio-temporal feature map with
this CNN. We then compute the mean ˜µ and covariance matrix ˜Σ for the feature vectors from
all the synthesized videos. We also calculate the same quantities µ and Σ for the ground truth
videos. The FID is then calculated as (cid:107)µ − ˜µ(cid:107)2 + Tr
. We use two different
pre-trained video recognition CNNs in our evaluation: I3D [8] and ResNeXt [75].

Σ + ˜Σ − 2

(cid:112)

(cid:16)

(cid:17)

Σ ˜Σ

Main results. We compare the proposed approach to the baselines on the Cityscapes benchmark 
where we apply the learned models to synthesize 500 short video clips in the validation set. As shown
in Table 1  our results have a smaller FID and are often favored by the human subjects. We also
report the human preference scores on the three long test videos. Again  the videos rendered by our
approach are considered more realistic by the human subjects. The human preference scores for the
Apolloscape dataset are given in the supplementary material.

7

Figure 5: Example face→sketch→face results. Each set shows the original video  the extracted edges 
and our synthesized video. The ﬁgure is best viewed with Acrobat Reader. Click the image to play the
video clip.

Figure 6: Example dance→pose→dance results. Each set shows the original dancer  the extracted
poses  and the synthesized video. The ﬁgure is best viewed with Acrobat Reader. Click the image to
play the video clip.

Figures 1 and 2 show the video synthesis results. Although each frame rendered by pix2pixHD is
photorealistic  the resulting video lacks temporal coherence. The road lane markings and building
appearances are inconsistent across frames. While improving upon pix2pixHD  COVST still suffers
from temporal inconsistency. On the contrary  our approach produces a high-resolution  photorealistic 
temporally consistent video output. We can generate 30-second long videos  showing that our
approach synthesizes convincing videos with longer lengths.
We conduct an ablation study to analyze several design choices of our method. Speciﬁcally  we
create three variants. In one variant  we do not use the foreground-background prior  which is termed
no background–foreground prior. That is  instead of using Equation 9  we use Equation 4.
The second variant is no conditional video discriminator where we do not use the video
discriminator DV for training. In the last variant  we remove the optical ﬂow prediction network
W and the mask prediction network M from the generator F in Equation 4 and only use H for
synthesis. This variant is referred to as no flow warping. We use the human preference score on
Cityscapes for this ablation study. Table 2 shows that the visual quality of output videos degrades
signiﬁcantly without the ablated components. To evaluate the effectiveness of different components
in our network  we also experimented with directly using ground truth ﬂows instead of estimated
ﬂows by our network. An example can be found in the supplementary material. We found the results
visually similar  which suggests that our network is robust to the errors in the estimated ﬂows.
Multimodal results. Figure 3 shows example multimodal synthesis results. In this example  we
keep the sampled feature vectors of all the object instances in the video the same except for the road
instance. The ﬁgure shows temporally smooth videos with different road appearances.
Semantic manipulation. Our approach also allows the user to manipulate the semantics of source
videos. In Figure 4  we show an example of changing the semantic labels. In the left video  we
replace all trees with buildings in the original segmentation masks and synthesize a new video. On
the right  we show the result of replacing buildings with trees.
Sketch-to-video synthesis for face swapping. We train a sketch-to-face synthesis video model
using the real face videos in the FaceForensics dataset [57]. As shown in Figure 5  our model can
convert sequences of sketches to photorealistic output videos. This model can be used to change the
facial appearance of the original face videos [5].
Pose-to-video synthesis for human motion transfer. We also apply our method to the task of
converting sequences of human poses to photorealistic output videos. We note that the image
counterpart was studied in recent works [4  17  47  48]. As shown in Figure 6  our model learns to
synthesize high-resolution photorealistic output dance videos that contain unseen body shapes and

8

Figure 7: Future video prediction results. Top left: ground truth. Top right: PredNet [45]. Bottom
left: MCNet [67]. Bottom right: ours. The ﬁgure is best viewed with Acrobat Reader. Click the image
to play the video clip.

motions. Our method can change the clothing [78  80] for the same dancer (Figure 6 left) as well as
transfer the visual appearance to new dancers (Figure 6 right) as explored in concurrent work [1 9 77].
Future video prediction. We show an extension of our approach to the future video prediction
task: learning to predict the future video given a few observed frames. We decompose the task
into two sub-tasks: 1) synthesizing future semantic segmentation masks using the observed frames 
and 2) converting the synthesized segmentation masks into videos. In practice  after extracting the
segmentation masks from the observed frames  we train a generator to predict future semantic masks.
We then use the proposed video-to-video synthesis approach to convert the predicted segmentation
masks to a future video.
We conduct both quantitative and qualitative evaluations with comparisons to two start-of-the-art
approaches: PredNet [45] and MCNet [67]. We follow the prior work [41  69] and report the human
preference score. We also include the FID scores. As shown in Table 3  our model produces smaller
FIDs  and the human subjects favor our resulting videos. In Figure 7  we visualize the future video
synthesis results. While the image quality of the results from the competing algorithms degrades
signiﬁcantly over time  ours remains consistent.

5 Discussion
We present a general video-to-video synthesis framework based on conditional GANs. Through
carefully-designed generators and discriminators as well as a spatio-temporal adversarial objective 
we can synthesize high-resolution  photorealistic  and temporally consistent videos. Extensive
experiments demonstrate that our results are signiﬁcantly better than the results by state-of-the-art
methods. Our method also compares favorably against the competing video prediction methods.
Although our approach outperforms previous methods  our model still fails in a couple of situations.
For example  our model struggles in synthesizing turning cars due to insufﬁcient information in
label maps. This could be potentially addressed by adding additional 3D cues  such as depth maps.
Furthermore  our model still can not guarantee that an object has a consistent appearance across
the whole video. Occasionally  a car may change its color gradually. This issue might be alleviated
if object tracking information is used to enforce that the same object shares the same appearance
throughout the entire video. Finally  when we perform semantic manipulations such as turning trees
into buildings  visible artifacts occasionally appear as building and trees have different label shapes.
This might be resolved if we train our model with coarser semantic labels  as the trained model would
be less sensitive to label shapes.
Acknowledgements We thank Karan Sapra  Fitsum Reda  and Matthieu Le for generating the
segmentation maps for us. We also thank Lisa Rhee and Miss Ketsuki for allowing us to use their
dance videos for training. We thank William S. Peebles for proofreading the paper.

9

References

[1] K. Aberman  M. Shi  J. Liao  D. Lischinski  B. Chen  and D. Cohen-Or. Deep video-based

performance cloning. arXiv preprint arXiv:1808.06847  2018.

[2] K. Arulkumaran  M. P. Deisenroth  M. Brundage  and A. A. Bharath. Deep reinforcement

learning: A brief survey. IEEE Signal Processing Magazine  34(6):26–38  2017.

[3] X. Bai  J. Wang  D. Simons  and G. Sapiro. Video snapcut: robust video object cutout using

localized classiﬁers. ACM Transactions on Graphics (TOG)  28(3):70  2009.

[4] G. Balakrishnan  A. Zhao  A. V. Dalca  F. Durand  and J. Guttag. Synthesizing images of
humans in unseen poses. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2018.

[5] D. Bitouk  N. Kumar  S. Dhillon  P. Belhumeur  and S. K. Nayar. Face swapping: automatically

replacing faces in photographs. In ACM SIGGRAPH  2008.

[6] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan. Unsupervised pixel-level
domain adaptation with generative adversarial networks. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2017.

[7] Z. Cao  T. Simon  S.-E. Wei  and Y. Sheikh. Realtime multi-person 2D pose estimation using
part afﬁnity ﬁelds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017.

[8] J. Carreira and A. Zisserman. Quo vadis  action recognition? a new model and the kinetics

dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2017.

[9] C. Chan  S. Ginosar  T. Zhou  and A. A. Efros. Everybody dance now. In European Conference

on Computer Vision (ECCV) Workshop  2018.

[10] D. Chen  J. Liao  L. Yuan  N. Yu  and G. Hua. Coherent online video style transfer. In IEEE

International Conference on Computer Vision (ICCV)  2017.

[11] L.-C. Chen  G. Papandreou  F. Schroff  and H. Adam. Rethinking atrous convolution for

semantic image segmentation. arXiv preprint arXiv:1706.05587  2017.

[12] T. Chen  J.-Y. Zhu  A. Shamir  and S.-M. Hu. Motion-aware gradient domain video composition.

IEEE Trans. Image Processing  22(7):2532–2544  2013.

[13] M. Cordts  M. Omran  S. Ramos  T. Rehfeld  M. Enzweiler  R. Benenson  U. Franke  S. Roth 
and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[14] E. Denton  S. Chintala  A. Szlam  and R. Fergus. Deep generative image models using a
Laplacian pyramid of adversarial networks. In Advances in Neural Information Processing
Systems (NIPS)  2015.

[15] E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from

video. In Advances in Neural Information Processing Systems (NIPS)  2017.

[16] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on

deep networks. In Advances in Neural Information Processing Systems (NIPS)  2016.

[17] P. Esser  E. Sutter  and B. Ommer. A variational u-net for conditional appearance and shape
generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.
[18] C. Finn  I. Goodfellow  and S. Levine. Unsupervised learning for physical interaction through

video prediction. In Advances in Neural Information Processing Systems (NIPS)  2016.

[19] A. Ghosh  V. Kulharia  V. Namboodiri  P. H. Torr  and P. K. Dokania. Multi-agent diverse gen-
erative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2018.

[20] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial networks. In Advances in Neural Information Processing
Systems (NIPS)  2014.

[21] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. C. Courville. Improved training of

wasserstein GANs. In Advances in Neural Information Processing Systems (NIPS)  2017.

[22] A. Gupta  J. Johnson  A. Alahi  and L. Fei-Fei. Characterizing and improving stability in neural

style transfer. In IEEE International Conference on Computer Vision (ICCV)  2017.

[23] R. A. Güler  N. Neverova  and I. Kokkinos. Densepose: Dense human pose estimation in the

wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[24] D. Ha and J. Schmidhuber. World models. In Advances in Neural Information Processing

Systems (NIPS)  2018.

10

[25] K. He  G. Gkioxari  P. Dollár  and R. Girshick. Mask R-CNN. In IEEE International Conference

on Computer Vision (ICCV)  2017.

[26] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[27] M. Heusel  H. Ramsauer  T. Unterthiner  B. Nessler  and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information
Processing Systems (NIPS)  2017.

[28] H. Huang  H. Wang  W. Luo  L. Ma  W. Jiang  X. Zhu  Z. Li  and W. Liu. Real-time neural style
transfer for videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017.

[29] X. Huang  X. Cheng  Q. Geng  B. Cao  D. Zhou  P. Wang  Y. Lin  and R. Yang. The Apol-
loScape dataset for autonomous driving. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2018.

[30] X. Huang  Y. Li  O. Poursaeed  J. E. Hopcroft  and S. J. Belongie. Stacked generative adversarial

networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2017.

[31] X. Huang  M.-Y. Liu  S. Belongie  and J. Kautz. Multimodal unsupervised image-to-image

translation. In ECCV  2018.

[32] E. Ilg  N. Mayer  T. Saikia  M. Keuper  A. Dosovitskiy  and T. Brox. Flownet 2.0: Evolution
of optical ﬂow estimation with deep networks. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2017.

[33] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017.

[34] J. Johnson  A. Alahi  and L. Fei-Fei. Perceptual losses for real-time style transfer and super-

resolution. In European Conference on Computer Vision (ECCV)  2016.

[35] J. T. Kajiya. The rendering equation. In ACM SIGGRAPH  1986.
[36] N. Kalchbrenner  A. v. d. Oord  K. Simonyan  I. Danihelka  O. Vinyals  A. Graves  and

K. Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527  2016.

[37] T. Karras  T. Aila  S. Laine  and J. Lehtinen. Progressive growing of GANs for improved quality 
stability  and variation. In International Conference on Learning Representations (ICLR)  2018.
[38] D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research  2009.
[39] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference

on Learning Representations (ICLR)  2015.

[40] A. B. L. Larsen  S. K. Sønderby  H. Larochelle  and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning (ICML) 
2016.

[41] A. X. Lee  R. Zhang  F. Ebert  P. Abbeel  C. Finn  and S. Levine. Stochastic adversarial video

prediction. arXiv preprint arXiv:1804.01523  2018.

[42] X. Liang  L. Lee  W. Dai  and E. P. Xing. Dual motion GAN for future-ﬂow embedded video

prediction. In Advances in Neural Information Processing Systems (NIPS)  2017.

[43] M.-Y. Liu  T. Breuel  and J. Kautz. Unsupervised image-to-image translation networks. In

Advances in Neural Information Processing Systems (NIPS)  2017.

[44] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural

Information Processing Systems (NIPS)  2016.

[45] W. Lotter  G. Kreiman  and D. Cox. Deep predictive coding networks for video prediction and
unsupervised learning. In International Conference on Learning Representations (ICLR)  2017.
[46] B. D. Lucas  T. Kanade  et al. An iterative image registration technique with an application to

stereo vision. International Joint Conference on Artiﬁcial Intelligence (IJCAI)  1981.

[47] L. Ma  X. Jia  Q. Sun  B. Schiele  T. Tuytelaars  and L. Van Gool. Pose guided person image

generation. In Advances in Neural Information Processing Systems (NIPS)  2017.

[48] L. Ma  Q. Sun  S. Georgoulis  L. Van Gool  B. Schiele  and M. Fritz. Disentangled person
image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2018.

[49] X. Mao  Q. Li  H. Xie  R. Y. Lau  Z. Wang  and S. P. Smolley. Least squares generative

adversarial networks. In IEEE International Conference on Computer Vision (ICCV)  2017.

[50] M. Mathieu  C. Couprie  and Y. LeCun. Deep multi-scale video prediction beyond mean square

error. In International Conference on Learning Representations (ICLR)  2016.

11

[51] T. Miyato  T. Kataoka  M. Koyama  and Y. Yoshida. Spectral normalization for generative
adversarial networks. In International Conference on Learning Representations (ICLR)  2018.
[52] T. Miyato and M. Koyama. cGANs with projection discriminator. In International Conference

on Learning Representations (ICLR)  2018.

[53] A. Odena  C. Olah  and J. Shlens. Conditional image synthesis with auxiliary classiﬁer GANs.

In International Conference on Machine Learning (ICML)  2017.

[54] K. Ohnishi  S. Yamamoto  Y. Ushiku  and T. Harada. Hierarchical video generation from

orthogonal information: Optical ﬂow and texture. In AAAI  2018.

[55] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolu-
tional generative adversarial networks. In International Conference on Learning Representations
(ICLR)  2015.

[56] S. Reed  Z. Akata  X. Yan  L. Logeswaran  B. Schiele  and H. Lee. Generative adversarial text

to image synthesis. In International Conference on Machine Learning (ICML)  2016.

[57] A. Rössler  D. Cozzolino  L. Verdoliva  C. Riess  J. Thies  and M. Nießner. Faceforensics: A
large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179 
2018.

[58] M. Ruder  A. Dosovitskiy  and T. Brox. Artistic style transfer for videos. In German Conference

on Pattern Recognition  2016.

[59] M. Saito  E. Matsumoto  and S. Saito. Temporal generative adversarial nets with singular value

clipping. In IEEE International Conference on Computer Vision (ICCV)  2017.

[60] A. Schödl  R. Szeliski  D. H. Salesin  and I. Essa. Video textures. ACM Transactions on

Graphics (TOG)  2000.

[61] E. Shechtman  Y. Caspi  and M. Irani. Space-time super-resolution. IEEE Transactions on

Pattern Analysis and Machine Intelligence (TPAMI)  27(4):531–545  2005.

[62] W. Shi  J. Caballero  F. Huszár  J. Totz  A. P. Aitken  R. Bishop  D. Rueckert  and Z. Wang.
Real-time single image and video super-resolution using an efﬁcient sub-pixel convolutional
neural network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2016.

[63] A. Shrivastava  T. Pﬁster  O. Tuzel  J. Susskind  W. Wang  and R. Webb. Learning from
In IEEE Conference on

simulated and unsupervised images through adversarial training.
Computer Vision and Pattern Recognition (CVPR)  2017.

[64] N. Srivastava  E. Mansimov  and R. Salakhudinov. Unsupervised learning of video representa-

tions using lstms. In International Conference on Machine Learning (ICML)  2015.

[65] Y. Taigman  A. Polyak  and L. Wolf. Unsupervised cross-domain image generation.

International Conference on Learning Representations (ICLR)  2017.

In

[66] S. Tulyakov  M.-Y. Liu  X. Yang  and J. Kautz. MoCoGAN: Decomposing motion and content
for video generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2018.

[67] R. Villegas  J. Yang  S. Hong  X. Lin  and H. Lee. Decomposing motion and content for natural
video sequence prediction. In International Conference on Learning Representations (ICLR) 
2017.

[68] C. Vondrick  H. Pirsiavash  and A. Torralba. Generating videos with scene dynamics. In

Advances in Neural Information Processing Systems (NIPS)  2016.

[69] C. Vondrick and A. Torralba. Generating the future with adversarial transformers. In IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  2017.

[70] J. Walker  C. Doersch  A. Gupta  and M. Hebert. An uncertain future: Forecasting from static
images using variational autoencoders. In European Conference on Computer Vision (ECCV) 
2016.

[71] J. Walker  K. Marino  A. Gupta  and M. Hebert. The pose knows: Video forecasting by
generating pose futures. In IEEE International Conference on Computer Vision (ICCV)  2017.
[72] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  A. Tao  J. Kautz  and B. Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional GANs. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2018.

[73] Y. Wexler  E. Shechtman  and M. Irani. Space-time video completion. In IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  2004.

[74] Y. Wexler  E. Shechtman  and M. Irani. Space-time completion of video. IEEE Transactions on

Pattern Analysis and Machine Intelligence (TPAMI)  29(3)  2007.

12

[75] S. Xie  R. Girshick  P. Dollár  Z. Tu  and K. He. Aggregated residual transformations for deep
neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017.

[76] T. Xue  J. Wu  K. Bouman  and B. Freeman. Visual dynamics: Probabilistic future frame
synthesis via cross convolutional networks. In Advances in Neural Information Processing
Systems (NIPS)  2016.

[77] C. Yang  Z. Wang  X. Zhu  C. Huang  J. Shi  and D. Lin. Pose guided human video generation.

In European Conference on Computer Vision (ECCV)  2018.

[78] S. Yang  T. Ambert  Z. Pan  K. Wang  L. Yu  T. Berg  and M. C. Lin. Detailed garment recovery

from a single-view image. arXiv preprint arXiv:1608.01250  2016.

[79] H. Zhang  T. Xu  H. Li  S. Zhang  X. Huang  X. Wang  and D. Metaxas. StackGAN: Text
In IEEE

to photo-realistic image synthesis with stacked generative adversarial networks.
International Conference on Computer Vision (ICCV)  2017.

[80] Z.-H. Zheng  H.-T. Zhang  F.-L. Zhang  and T.-J. Mu. Image-based clothes changing system.

Computational Visual Media  3(4):337–347  2017.

[81] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In IEEE International Conference on Computer Vision (ICCV) 
2017.

[82] J.-Y. Zhu  R. Zhang  D. Pathak  T. Darrell  A. A. Efros  O. Wang  and E. Shechtman. Toward
multimodal image-to-image translation. In Advances in Neural Information Processing Systems
(NIPS)  2017.

13

,Yuhong Guo
Isabel Valera
Zoubin Ghahramani
Ting-Chun Wang
Ming-Yu Liu
Jun-Yan Zhu
Guilin Liu
Andrew Tao
Jan Kautz
Bryan Catanzaro