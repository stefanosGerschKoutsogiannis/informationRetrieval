2019,Planning in entropy-regularized Markov decision processes and games,We propose SmoothCruiser  a new planning algorithm for estimating the value function in entropy-regularized Markov decision processes and two-player games  given a generative model of the SmoothCruiser. SmoothCruiser makes use of the smoothness of the Bellman operator promoted by the regularization to achieve problem-independent sample complexity of order $\tilde{\mathcal{O}}(1/\epsilon^4)$ for a desired accuracy $\epsilon$  whereas for non-regularized settings there are no known algorithms with guaranteed polynomial sample complexity in the worst case.,Planning in entropy-regularized

Markov decision processes and games

Jean-Bastien Grill∗
DeepMind Paris

jbgrill@google.com

Omar D. Domingues∗
SequeL team  Inria Lille

omar.darwiche-domingues@inria.fr

Pierre Ménard

SequeL team  Inria Lille

pierre.menard@inria.fr

Rémi Munos
DeepMind Paris

munos@google.com

Michal Valko
DeepMind Paris

valkom@deepmind.com

Abstract

We propose SmoothCruiser  a new planning algorithm for estimating the value
function in entropy-regularized Markov decision processes and two-player games 
given a generative model of the environment. SmoothCruiser makes use of the
smoothness of the Bellman operator promoted by the regularization to achieve

problem-independent sample complexity of order (cid:101)O(1/ε4) for a desired accuracy ε 

whereas for non-regularized settings there are no known algorithms with guaranteed
polynomial sample complexity in the worst case.

1

Introduction

Planning with a generative model is thinking before acting. An agent thinks using a world model that
it has built from prior experience [Sutton  1991  Sutton and Barto  2018]. In the present paper  we
study planning in two types of environments  Markov decision processes (MDPs) and two-player
turn-based zero-sum games. In both settings  agents interact with an environment by taking actions
and receiving rewards. Each action changes the state of the environment and the agent aims to choose
actions to maximize the sum of rewards. We assume that we are given a generative model of the
environment  that takes as input a state and an action and returns a reward and a next state as output.
Such generative models  called oracles  are typically built from known data and involve simulations 
for example  a physics simulation. In many cases  simulations are costly. For example  simulations
may require the computation of approximate solutions of differential equations or the discretization
of continuous state spaces. Therefore  a smart algorithm makes only a small the number of oracles
calls required to estimate the value of a state. The total number of oracle calls made by an algorithm
is referred to as sample complexity.
The value of a state s  denoted by V (s)  is the maximum of the sum of discounted rewards that can
be obtained from that state. We want an algorithm that returns an estimate of precision ε of the V (s)
for any ﬁxed s and has a low sample complexity  which should naturally be a function of ε. An agent
can then use this algorithm to predict the value of the possible actions at any given state and choose
the best one. The main advantage in estimating the value of a single given state s at a time instead of
the complete value function2 s (cid:55)→ V (s) is that we can have algorithms whose sample complexity
does not depend on the size of the state space  which is important when our state space is very large
or continuous. On the other hand  the disadvantage is that the algorithm must be run each time a new
state is encountered.
∗equal contribution
2as done by approximate dynamic programming

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Our main contribution is an algorithm that estimates the value function in a given state in planning
problems that satisfy speciﬁc smoothness conditions  which is the case when the rewards are regular-
ized by adding an entropy term. We exploit this smoothness property to obtain a polynomial sample

complexity of order (cid:101)O(cid:0)1/ε4(cid:1) that is problem independent.
O(cid:0)(1/ε)log(1/ε)(cid:1)  which is non-polynomial in 1/ε. SSA is slow since its search is uniform  i.e.  it

Related work Kearns et al. [1999] came up with a sparse sampling algorithm (SSA) for planning
in MDPs with ﬁnite actions and arbitrary state spaces. SSA estimates the value of a state s by
building a sparse look-ahead tree starting from s. However  SSA achieves a sample complexity of

does not select actions adaptively. Walsh et al. [2010] gave an improved version of SSA with adaptive
action selection  but its sample complexity is still non-polynomial. The UCT algorithm [Kocsis
and Szepesvári  2006]  used for planning in MDPs and games  selects actions based on optimistic
estimates of their values and has good empirical performance in several applications. However  the
sample complexity of UCT can be worse than exponential in 1/ε for some environments  which is
mainly due to exploration issues [Coquelin and Munos  2007]. Algorithms with sample complexities

of order O(cid:0)1/εd(cid:1)  where d is a problem-dependent quantity  have been proposed for deterministic

dynamics [Hren and Munos  2008]  and in an open-loop3 setting [Bubeck and Munos  2010  Leurent
and Maillard  2019  Bartlett et al.  2019]  for bounded number of next states and a full MDP model
is known [Bu¸soniu and Munos  2012]  for bounded number of next states in a ﬁnite-horizon setting
[Feldman and Domshlak  2014]  for bounded number of next states [Szörényi et al.  2014]  and for
general MDPs [Grill et al.  2016]. In general  when the state space is inﬁnite and the transitions are
stochastic  the problem-dependent quantity d can make the sample complexity guarantees exponential.
For a related setting  when rewards are only obtained in the leaves of a ﬁxed tree  Kaufmann and
Koolen [2017] and Huang et al. [2017] present algorithms to identify the optimal action in a game
based on best-arm identiﬁcation tools.
Entropy regularization in MDPs and reinforcement learning have been employed in several commonly
used algorithms. In the context of policy gradient algorithms  common examples are the TRPO
algorithm [Schulman et al.  2015] which uses the Kullback-Leibler divergence between the current
and the updated policy to constrain the gradient step sizes  the A3C algorithm [Mnih et al.  2016]
that penalizes policies with low entropy to improve exploration  and the work of Neu et al. [2017]
presenting a theoretical framework for entropy regularization using the joint state-action distribution.
Formulations with entropy-augmented rewards  which is the case in our work  have been used to
learn multi-modal policies to improve exploration and robustness [Haarnoja et al.  2017  2018] and
can also be related to policy gradient methods [Schulman et al.  2017]. Furthermore  Geist et al.
[2019] propose a theory of regularized MDPs which includes entropy as a special case. Summing
up  reinforcement learning knows how to employ entropy regularization. In this work  we tasked
ourselves to give insights on why.

2 Setting and motivation
Both MDPs and two-player games can be formalized as a tuple (S A  P  R  γ)  where S is the set
of states  A is the set of actions  P (cid:44) {P (·|s  a)}s a∈S×A is a set of probability distributions over
S  R : S × A → [0  1] is a (possibly random) reward function and γ ∈ [0  1[ is the known discount
factor. In the MDP case  at each round t  an agent is at state s  chooses action a and observes a
reward R(s  a) and a transition to a next state z ∼ P (·|s  a). In the case of turn-based two-player
games  there are two agents and  at each round t  an agent chooses an action  observes a reward and a
transition; at round t + 1 it’s the other player’s turn. This is equivalent to an MDP with an augmented
state space S + (cid:44) S × {1  2} and transition probabilities such that P ((z  j)|(s  i)  a) = 0 if i = j.
We assume that the action space A is ﬁnite with cardinality K and the state space S has arbitrary
(possibly inﬁnite) cardinality.
Our objective is to ﬁnd an algorithm that outputs a good estimate of the value V (s) for any given
state s as quickly as possible. An agent can then use this algorithm to choose the best action in an
MDP or a game. More precisely  for a state s ∈ S and given ε > 0 and δ > 0  our goal is to compute

an estimate (cid:98)V (s) of V (s) such that P(cid:104)(cid:12)(cid:12)(cid:98)V (s) − V (s)(cid:12)(cid:12) > ε

(cid:105) ≤ δ with small number of oracle calls

3This means that the policy is seen as a function of time  not the states. The open-loop setting is particularly

adapted to environments with deterministic transitions.

2

(cid:110)E[R(s  a) + γV (z)] + λH(π(·|s))
(cid:111)
E[R(s  a) + γV (z)](cid:1)  z ∼ P (·|s  a) 
exp(cid:0) 1

λ

V (s) (cid:44) max

π(·|s)∈P(A)

= λ log

(cid:88)

a∈A

  a ∼ π(·|s)  z ∼ P (·|s  a)

(3)

required to compute this estimate. In our setting  we consider the case of entropy-regularized MDPs
and games  where the objective is augmented with an entropy term.

2.1 Value functions
Markov decision process The policy π of an agent is a function from S to P(A)  the set of
probability distributions over A. We denote by π(a|s) the probability of the agent choosing action
a at state s. In MDPs  the value function at a state s  V (s)  is deﬁned as the supremum over all
possible policies of the expected sum of discounted rewards obtained starting from s  which satisﬁes
the Bellman equations [Puterman  1994] 

∀s ∈ S  V (s) = max

E[R(s  a) + γV (z)]  a ∼ π(·|s)  z ∼ P (·|s  a).

(1)

π(·|s)∈P(A)

Two-player turn-based zero-sum games
In this case  there are two agents (1 and 2)  each one
with its own policy and different goals. If the policy of Agent 2 is ﬁxed  Agent 1 aims to ﬁnd a policy
that maximizes the sum of discounted rewards. Conversely  if the policy of Agent 1 is ﬁxed  Agent 2
aims to ﬁnd a policy that minimizes this sum. Optimal strategies for both agents can be shown to exist
and for any (s  i) ∈ S + (cid:44) S × {1  2}  the value function is deﬁned as [Hansen et al.  2013]

(cid:26)maxπ(·|s)∈P(A) E[R((s  i)  a) + γV (z  j)] 

(2)
with a ∼ π(·|s) and (z  j) ∼ P (·|(s  i)  a). In this case  the function s (cid:55)→ V (s  i) is the optimal
value function for Agent i when the other agent follows its optimal strategy.

minπ(·|s)∈P(A) E[R((s  i)  a) + γV (z  j)] 

if i = 1 
if i = 2 

V (s  i) (cid:44)

Entropy-regularized value functions Consider a regularization factor λ > 0. In the case of
MDPs  when rewards are augmented by an entropy term  the value function at state s is given by
[Haarnoja et al.  2017  Dai et al.  2018  Geist et al.  2019]

where H(π(·|s)) is the entropy of the probability distribution π(·|s) ∈ P(A).

The function LogSumExpλ : RK → R  deﬁned as LogSumExpλ(x) (cid:44) λ log(cid:80)K

i=1 exp(xi/λ)  is
a smooth approximation of the max function  since (cid:107)max−LogSumExpλ(cid:107)∞ ≤ λ log K. Similarly 
the function −LogSumExp−λ is a smooth approximation of the min function. This allows us to
deﬁne the regularized version of the value function for turn-based two player games  in which both
players have regularized rewards  by replacing the max and the min in Equation 2 by their smooth
approximations.
For any state s  let Fs (cid:44) LogSumExpλ or Fs (cid:44) −LogSumExp−λ depending on s. Both for MDPs
and games  we can write the entropy-regularized value functions as

V (s) = Fs(Qs)  with Qs(a) (cid:44) E[R(s  a) + γV (z)]  z ∼ P (·|s  a) 

(4)
where Qs (cid:44) (Qs(a))a∈A  the Q function at state s  is a vector in RK representing the value of each
action. The function Fs is the Bellman operator at state s  which becomes smooth due to the entropy
regularization.

Useful properties Our algorithm exploits the smoothness property of Fs deﬁned above. In particu-
lar  these functions are L-smooth  that is  for any Q  Q(cid:48) ∈ RK  we have
|Fs(Q) − Fs(Q(cid:48)) − (Q − Q(cid:48))T∇Fs(Q(cid:48))| ≤ L(cid:107)Q − Q(cid:48)(cid:107)2

(5)
Furthermore  the functions Fs have two important properties: ∇Fs(Q)4 (cid:23) 0 and (cid:107)∇Fs(Q)(cid:107)1 = 1
for all Q ∈ RK. This implies that the gradient ∇Fs(Q) deﬁnes a probability distribution.5

2  with L = 1/λ·

4∇Fs(Q) is the gradient of Fs(Q) with respect to Q.
5It is a Boltzmann distribution with temperature λ.

3

Assumptions We assume that S  A  λ  and γ are given to the learner. Moreover  we assume that we
can access a generative model  the oracle  from which we can get reward and transition samples from
arbitrary state-action pairs. Formally  when called with parameter (s  a) ∈ S × A  the oracle outputs
a new random variable (R  Z) independent from any other outputs received from the generative
model so far such that Z ∼ P (·|s  a) and R has same distribution as R(s  a). We denote a call to the
oracle as R  Z ← oracle(s  a).

2.2 Using regularization for the polynomial sample complexity

To pave the road for SmoothCruiser  we consider two extreme cases  based on the strength of the
regularization:

1. Strong regularization In this case  λ → ∞ and L = 0  that is  Fs is linear for all s:
2. No regularization In this case  λ = 0 and L → ∞  that is  Fs cannot be well approximated

sx  with (cid:107)ws(cid:107)1 = 1  ws ∈ Rk and ws (cid:23) 0 

Fs(x) = wT

by a linear function.6

In the strongly regularized case  we can approximate the value V (s) with (cid:101)O(1/ε2) oracle
E[(cid:80)∞
calls. This is due to the linearity of Fs  since the value function can be written as V (s) =
t=0 γtR(St  At) | S0 = s] where At is distributed according to the probability vector wSt.

As a result  V (s) can be estimated by Monte-Carlo sampling of trajectories.
With no regularization  we can apply a simple adaptation of the sparse sampling algorithm of
Kearns et al. [1999] that we brieﬂy describe. Assume that we have an subroutine that provides an
γ(s)  for any s. We can
approximation of the value function with precision ε/

call this subroutine several times as well as the oracle to get improved estimate (cid:98)V deﬁned as

√

√

γ  denoted by (cid:98)Vε/
(cid:104)
N(cid:88)
ri(s  a) + γ(cid:98)Vε/

(cid:105)

√

γ(zi)

 

(cid:98)V (s) = Fs

(cid:17)

(cid:16)(cid:98)Qs

(cid:98)Qs(a) ← 1

N

with

i=1

of V (s) with precision ε with high probability. By applying this idea recursively  we start with

where ri(s  a) and zi are rewards and next states sampled by calling the oracle with parameters

(s  a). By Hoeffding’s inequality  we can choose N = O(cid:0)1/ε2(cid:1) such that (cid:98)V (s) is an approximation
(cid:98)V = 0  which is an approximation of the value function with precision 1/(1 − γ)  and progressively
complexity of O(cid:0)(1/ε)log(1/ε)(cid:1): to estimate the value at a given recursion depth  we make O(cid:0)1/ε2(cid:1)

improve the estimates towards a desired precision ε  which can be reached at a recursion depth of
H = O(log(1/ε)). Following the same reasoning as Kearns et al. [1999]  this approach has a sample

recursive calls and stop once we reach the maximum depth  resulting in a sample complexity of

(cid:19)O(log( 1
ε ))·

(cid:18) 1

ε

=

1

(cid:125)
(cid:124)
ε2 × ··· × 1
ε2
O(log(1/ε)) times

(cid:123)(cid:122)

In the next section  we provide SmoothCruiser (Algorithm 1)  that uses the assumption that the
functions Fs are L-smooth with 0 < L < ∞ to interpolate between the two cases above and obtain a

sample complexity of (cid:101)O(cid:0)1/ε4(cid:1).

3 SmoothCruiser

We now describe our planning algorithm. Its building blocks are two procedures  sampleV (Algo-
rithm 2) and estimateQ (Algorithm 3) that recursively call each other. The procedure sampleV
returns a noisy estimate of V (s) with a bias bounded by ε. The procedure estimateQ averages the

outputs of several calls to sampleV to obtain an estimate (cid:98)Qs that is an approximation of Qs with
precision ε with high probability. Finally  SmoothCruiser calls estimateQ(s  ε) to obtain (cid:98)Qs and
outputs (cid:98)V (s) = Fs((cid:98)Qs). Using the assumption that Fs is 1-Lipschitz  we can show that (cid:98)V (s) is an

approximation of V (s) with precision ε. Figure 1 illustrates a call to SmoothCruiser.

6This is the case of the max and min functions.

4

Figure 1: Visualization of a call to SmoothCruiser(s0  ε0  δ(cid:48)).

3.1 Smooth sailing

The most important part of the algorithm is the pro-
cedure sampleV  that returns a low-bias estimate
of the value function. Having the estimate of the
value function  the procedure estimateQ averages
the outputs of sampleV to obtain a good estimate of
the Q function with high probability. The main idea
of sampleV is to ﬁrst compute an estimate of preci-
√
sion O(

ε) of the value of each action {(cid:98)Qs(a)}a∈A
to linearly approximate the function Fs around (cid:98)Qs.
The local approximation of Fs around (cid:98)Qs is subsequently used to estimate the value of s with a better

precision  of order O(ε)  which is possible due to the smoothness of Fs.

Algorithm 1 SmoothCruiser

Input: (s  ε  δ(cid:48)) ∈ S × R+× R+
Mλ ← sups∈S |Fs(0)| = λ log K
κ ← (1 − √
(cid:98)Qs ← estimateQ(s  ε)
Set δ(cid:48)  κ  and Mλ as global parameters
(cid:16)(cid:98)Qs

Output: Fs

γ)/(KL)

(cid:17)

Output: 0

Output: Fs

Algorithm 2 sampleV
1: Input: (s  ε) ∈ S × R+
2: if ε ≥ (1 + Mλ)/(1 − γ) then
3:
4: else if ε ≥ κ then
5:
6:
7: else if ε < κ then
8:
9:
10:
11:
12:
13:
14: end if

(cid:98)Qs ← estimateQ(s  ε)
(cid:17)
(cid:16)(cid:98)Qs
(cid:98)Qs ← estimateQ(s 
(cid:98)V ← sampleV(Z  ε/
(cid:17)− (cid:98)QT
(cid:16)(cid:98)Qs

√
κε)
A ← action drawn from ∇Fs
(R  Z) ← oracle(s  A)
√
γ)

(cid:16)(cid:98)Qs

Output:

s∇Fs

(cid:17)

Fs

(cid:17)

(cid:16)(cid:98)Qs
+ (R + γ(cid:98)V )

γ)2

(cid:24)

18(1+Mλ)2
(1−γ)4(1−√

(cid:25)

log(2K/δ(cid:48))

ε2

Algorithm 3 estimateQ
1: Input: (s  ε)
2: N (ε) ←
3: for a ∈ A do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for

qi ← 0 for i ∈ 1  ...  N (ε)
for i ∈ 1  ...  N (ε) do
(R  Z) ← oracle(s  a).
√

γ(cid:1)
(cid:98)V ← sampleV(cid:0)Z  ε/
qi ← R + γ(cid:98)V
(cid:98)Qs(a) ← mean(q1  . . .   qN )
// clip (cid:98)Qs(a) to [0  (1 + Mλ)/(1 − γ)]
(cid:98)Qs(a) ← max(0  (cid:98)Qs(a))
(cid:98)Qs(a) ← min((1 + Mλ)/(1− γ)  (cid:98)Qs(a))

end for

15: Output: (cid:98)Qs

For a target accuracy ε at state s  sampleV distinguishes three cases  based on a reference threshold
κ (cid:44) (1 − √
γ)/(KL)  which is the maximum value of ε for which we can compute a good estimate
of the value function using linear approximations of Fs.

• First  if ε ≥ (1 + λ log K)/(1 − γ)  then 0 is a valid output  since V (s) is bounded by
(1 + λ log K)/(1 − γ). This case furthermore ensures that our algorithm terminates  since
the recursive calls are made with increasing values of ε.

• Second  if κ ≤ ε ≤ (1 + λ log K)/(1− γ)  we run Fs(estimateQ(s  ε)) in which for each

action  both the oracle and sampleV are called O(cid:0)1/ε2(cid:1) times in order to return(cid:98)V (s) which

is with high probability an ε-approximation of V (s).

5

120<≥←( )ˆ000‾‾‾‾√←(  )ˆ11/0‾‾‾‾‾√∼∇()0ˆ0∼∇()1ˆ1345678of V (s) in a more efﬁcient way than calling the oracle and sampleV O(cid:0)1/ε2(cid:1) times. We

• Finally  if ε < κ  we take advantage of the smoothness of Fs to compute an ε-approximation
κε instead of ε  which requires O(1/ε)

achieve it by calling estimateQ with a precision
calls instead.

√

3.2 Smoothness guarantee an improved sample complexity

The term QT

QT

In this part  we describe the key ideas that allows us to exploit the smoothness of the Bellman operator
to obtain a better sample complexity. Notice that when ε < κ  the procedure estimateQ is called to

obtain an estimate (cid:98)Qs such that
The procedure sampleV then continues with computing a linear approximation of Fs(Qs) around (cid:98)Qs.

(cid:107)(cid:98)Qs − Qs(cid:107)2 = O(cid:16)(cid:112)ε/L

(cid:17)

.

Using the L-smoothness of Fs  we guarantee the ε-approximation 

We wish to output this linear approximation  but we need to handle the fact that the vector Qs (the true

Q-function at s) is unknown. Notice that the vector ∇Fs((cid:98)Qs) represents a probability distribution.

(cid:111)| ≤ L(cid:107)(cid:98)Qs − Qs(cid:107)2

Fs((cid:98)Qs) + (Qs − (cid:98)Qs)T∇Fs((cid:98)Qs)

|Fs(Qs) −(cid:110)
2 = O(ε).
s∇Fs((cid:98)Qs) in the linear approximation of Fs(Qs) above can be expressed as
(cid:12)(cid:12)(cid:12)(cid:98)Qs
(cid:105)
s∇Fs((cid:98)Qs) = E(cid:104)
  with A ∼ ∇Fs((cid:98)Qs).
s∇Fs((cid:98)Qs) from estimating only Qs(A):
• sample action A ∼ ∇Fs((cid:98)Qs)
• obtain an O(ε)-approximation of Qs(A): (cid:101)Q(A) = Rs A + γsampleV(cid:0)Zs A  ε/
• output (cid:98)V (s) = Fs((cid:98)Qs) − (cid:98)QT

s∇Fs((cid:98)Qs) + (cid:101)Q(A)

Qs(A)

• call the generative model to sample a reward and a next state Rs A  Zs A ← oracle(s  A)

Therefore  we can build a low-bias estimate of QT

We show that (cid:98)V (s) is an ε-approximation of the true value function V (s). The beneﬁt of such

ε) instead of O(ε)  which thanks to the
approach is that we can call estimateQ with a precision O(
smoothness of Fs  reduces the sample complexity. In particular  one call to sampleV(s  ε) will make
O(1/ε) recursive calls to sampleV(s O(
ε))  and the total number of calls to sampleV behaves as
× 1
× 1
ε1/4
ε1/2

Therefore  the number of sampleV calls made by SmoothCruiser is of order O(cid:0)1/ε2(cid:1)  which

× ··· ≤ 1
ε2·

√

√

1
ε

γ(cid:1)

√

implies that the total sample complexity is of O(1/ε4).

3.3 Comparison to Monte-Carlo tree search

Algorithm 4 genericMCTS

Several planning algorithms are based on Monte-
Carlo tree search (MCTS  Coulom  2007  Kocsis and
Szepesvári  2006). Algorithm 4 gives a template for
MCTS  which uses the procedure search that calls
selectAction and evaluateLeaf. Algorithm 5 
search  returns an estimate of the value function;
selectAction selects the action to be executed
(also called tree policy); and evaluateLeaf returns an estimate of the value of a leaf. We now
provide the analogies that make it possible to see SmoothCruiser as an MCTS algorithm:

Input: state s
repeat search(s  0)
until timeout
Output: estimate of best action or value.

• sampleV corresponds to the function search

6

• selectAction is implemented by calling estimateQ to compute (cid:98)Qs  followed by sam-
pling an action with probability proportional to ∇Fs((cid:98)Qs)

• evaluateLeaf is implemented using the sparse sampling strategy of Kearns et al. [1999] 

if we see leaves as the nodes reached when ε ≥ κ

4 Theoretical guarantees

Algorithm 5 search
Input: state s  depth d
if d > dmax then

In Theorem 1 we bound the sample complexity.
Note that SmoothCruiser is non-adaptive  hence
its sample complexity is deterministic and problem
independent. Indeed  since our algorithm is agnostic
to the output of the oracle  it performs the same num-
ber of oracle calls for any given ε and δ(cid:48)  regardless
of the random outcome of these calls.
Theorem 1. Let n(ε  δ(cid:48)) be the number of oracle
calls before SmoothCruiser terminates. For any state s ∈ S and ε  δ(cid:48) > 0 

Output: evaluateLeaf(s)

end if
a ← selectAction(s  d)
R  Z ← oracle(s  a)
Output: R + γsearch(Z  d + 1)

(cid:16) c4

(cid:17)(cid:105)log2(c5(log( c2

δ(cid:48) )))

= (cid:101)O

(cid:19)

 

(cid:18) 1

ε4

(cid:16) c2

(cid:17)(cid:104)

δ(cid:48)

n(ε  δ(cid:48)) ≤ c1

ε4 log

c3 log

ε

where c1  c2  c3  c4  and c5 are constants that depend only on K  L  and γ.

The proof of Theorem 1 with the exact constants is in the appendix. In Theorem 2  we provide our
consistency result  stating that the output of SmoothCruiser applied to a state s ∈ S is a good
approximation of V (s) with high probability.
Theorem 2. For any s ∈ S  ε > 0  and δ > 0  there exists a δ(cid:48) that depends on ε and δ such that the

P(cid:104)(cid:12)(cid:12)(cid:98)V (s) − V (s)(cid:12)(cid:12) > ε

output (cid:98)V (s) of SmoothCruiser(s  ε  δ(cid:48)) satisﬁes
and such that n(ε  δ(cid:48)) = O(cid:0)1/ε4+c(cid:1) for any c > 0.
P(cid:104)(cid:12)(cid:12)(cid:98)V (s) − V (s)(cid:12)(cid:12) > ε

More precisely  in the proof of Theorem 2  we establish that

(cid:105) ≤ δ.

(cid:105) ≤ δ(cid:48)n(ε  δ(cid:48)).

Therefore  for any parameter δ(cid:48) satisfying δ(cid:48)n(ε  δ(cid:48)) ≤ δ  SmoothCruiser with parameters ε and δ(cid:48)
provides an approximation of V (s) which is (ε  δ) correct.

Impact of regularization constant For a regularization constant λ  the smoothness constant is
L = 1/λ.
in Theorem 1 we did not make the dependence on L explicit to preserve simplicity.
However  it easy to analyze the sample complexity in the two limits:
strong regularization L → 0 and Fs is linear
no regularization L → ∞ and Fs is not smooth

As L → 0  the condition κ ≤ ε ≤ (1+λ log K)/(1−γ) will be met less and eventually the algorithm
the other hand  as L goes to ∞  the condition ε < κ will be met less and the algorithm eventually
runs a uniform sampling strategy of Kearns et al. [1999]  which results in a sample complexity of

will sample N = O(cid:0)1/ε2(cid:1) trajectories  which implies a sample complexity of order O(cid:0)1/ε2(cid:1). On
order O(cid:0)(1/ε)log(1/ε)(cid:1)  which is non-polynomial in 1/ε.

Let Vλ(s) be the entropy regularized value function and V0(s) be its non-regularized version. Since Fs
is 1-Lipschitz and (cid:107)LogSumExpλ − max(cid:107)∞ ≤ λ log K  we can prove that sups |Vλ(s) − V0(s)| ≤
λ log K/(1 − γ). Thus  we can interpret Vλ(s) as an approximate value function which we can
estimate faster.

7

complexity lower bound of Ω(cid:0)(1/ε)1/ log(1/γ)(cid:1)  which is polynomial in 1/ε  but its exponent grows

Comparison to lower bound For non-regularized problems  Kearns et al. [1999] prove a sample

as γ approaches 1. For regularized problems  Theorem 1 shows that the sample complexity is
polynomial with an exponent that is independent of γ. Hence  when γ is close to 1  regularization
gives us a better asymptotic behavior with respect to 1/ε than the lower bound for the non-regularized
case  although we are not estimating the same value.

5 Generalization of SmoothCruiser

Consider the general deﬁnition of value functions in Equation 4. Although we focused on the case
where Fs is the LogSumExp function  which arises as a consequence of entropy regularization 
our theoretical results hold for any set of functions {Fs}s∈S that for any s satisfy the following
conditions:

1. Fs is differentiable
2. ∀Q ∈ RK  0 < (cid:107)∇Fs(Q)(cid:107)1 ≤ 1
3. (nonnegative gradient) ∀Q ∈ RK ∇Fs(Q) (cid:23) 0
4. (L-smooth) there exists L ≥ 0 such that for any Q  Q(cid:48) ∈ RK

|Fs(Q) − Fs(Q(cid:48)) − (Q − Q(cid:48))T∇Fs(Q(cid:48))| ≤ L(cid:107)Q − Q(cid:48)(cid:107)2

2

For the more general deﬁnition above  we need to make two simple modiﬁcations of the procedure
sampleV. When ε < κ  the action A in sampleV is sampled according to

and its output is modiﬁed to

Fs((cid:98)Qs) − (cid:98)QT

A ∼ ∇Fs((cid:98)Qs)
(cid:107)∇Fs((cid:98)Qs)(cid:107)1
s∇Fs((cid:98)Qs) + (R + γ(cid:98)v)(cid:107)∇Fs((cid:98)Q)(cid:107)1.

In particular  SmoothCruiser can be used for more general regularization schemes  as long as the
Bellman operators satisfy the assumptions above. One such example is presented in Appendix E.

6 Conclusion

We provided SmoothCruiser  an algorithm that estimates the value function of MDPs and discounted
games deﬁned through smooth approximations of the optimal Bellman operator  which is the case
in entropy-regularized value functions. More generally  our algorithm can also be used when value
functions are deﬁned through any smooth Bellman operator with nonnegative gradients. We showed

that our algorithm has a polynomial sample complexity of (cid:101)O(1/ε4)  where ε is the desired precision.

This guarantee is problem independent and holds for state spaces of arbitrary cardinality.
One interesting interpretation of our results is that computing entropy-regularized value functions 
which are commonly employed for reinforcement learning  can be seen as a smooth relaxation of a
planning problem for which we can obtain a much better sample complexity in terms of the required
precision ε. Unsurprisingly  when the regularization tends to zero  we recover the well-known

non-polynomial bound O(cid:0)(1/ε)log(1/ε)(cid:1) of Kearns et al. [1999]. Hence  an interesting direction for

future work is to study adaptive regularization schemes in order to accelerate planning algorithms.
Although SmoothCruiser makes large amount of recursive calls  which makes it impractical in most
situations  we believe it might help us to understand how regularization speeds planning and inspire
more practical algorithms. This might be possible by exploiting its similarities to Monte-Carlo tree
search that we have outlined above.

Acknowledgments The research presented was supported by European CHIST-ERA project
DELTA  French Ministry of Higher Education and Research  Nord-Pas-de-Calais Regional Coun-
cil  Inria and Otto-von-Guericke-Universität Magdeburg associated-team north-European project
Allocate  and French National Research Agency project BoB (grant n.ANR-16-CE23-0003)  FMJH
Program PGMO with the support of this program from Criteo.

8

References
Peter L Bartlett  Victor Gabillon  Jennifer Healey  and Michal Valko. Scale-free adaptive planning for
deterministic dynamics & discounted rewards. In International Conference on Machine Learning 
2019.

Sébastien Bubeck and Rémi Munos. Open-loop optimistic planning. In Conference on Learning

Theory  2010.

Lucian Bu¸soniu and Rémi Munos. Optimistic planning for Markov decision processes. In Interna-

tional Conference on Artiﬁcial Intelligence and Statistics  2012.

Pierre-Arnaud Coquelin and Rémi Munos. Bandit algorithms for tree search. In Uncertainty in

Artiﬁcial Intelligence  2007.

Rémi Coulom. Efﬁcient selectivity and backup operators in Monte-Carlo tree search. Computers and

games  4630:72–83  2007.

Bo Dai  Albert Shaw  Lihong Li  Lin Xiao  Niao He  Zhen Liu  Jianshu Chen  and Le Song. SBEED:
In International

Convergent reinforcement learning with nonlinear function approximation.
Conference on Machine Learning  2018.

Zohar Feldman and Carmel Domshlak. Simple regret optimization in online planning for Markov

decision processes. Journal of Artiﬁcial Intelligence Research  2014.

Matthieu Geist  Bruno Scherrer  and Olivier Pietquin. A Theory of regularized Markov decision

processes. In International Conference on Machine Learning  pages 2160–2169  2019.

Jean-Bastien Grill  Michal Valko  and Rémi Munos. Blazing the trails before beating the path:

Sample-efﬁcient Monte-Carlo planning. In Neural Information Processing Systems  2016.

Tuomas Haarnoja  Haoran Tang  Pieter Abbeel  and Sergey Levine. Reinforcement learning with

deep energy-based policies. In International Conference on Machine Learning  2017.

Tuomas Haarnoja  Aurick Zhou  Pieter Abbeel  and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning  2018.

Thomas Dueholm Hansen  Peter Bro Miltersen  and Uri Zwick. Strategy iteration is strongly
polynomial for 2-player turn-based stochastic games with a constant discount factor. Journal of
the ACM  60  2013.

Jean-Francois Hren and Rémi Munos. Optimistic planning of deterministic systems. In European

Workshop on Reinforcement Learning  2008.

Ruitong Huang  Mohammad M. Ajallooeian  Csaba Szepesvári  and Martin Müller. Structured

best-arm identiﬁcation with ﬁxed conﬁdence. In Algorithmic Learning Theory  2017.

Emilie Kaufmann and Wouter M Koolen. Monte-carlo tree search by best-arm identiﬁcation. In

Neural Information Processing Systems  2017.

Michael Kearns  Yishay Mansour  and Andrew Y. Ng. A sparse sampling algorithm for near-optimal
planning in large Markov decision processes. In International Conference on Artiﬁcial Intelligence
and Statistics  1999.

Levente Kocsis and Csaba Szepesvári. Bandit-based Monte-Carlo planning. In European Conference

on Machine Learning  2006.

Edouard Leurent and Odalric-Ambrym Maillard. Practical open-loop pptimistic planning.

European Conference on Machine Learning  2019.

In

Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap  Tim
Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning  2016.

9

Gergely Neu  Anders Jonsson  and Vicenç Gómez. A uniﬁed view of entropy-regularized Markov

decision processes. In arXiv:1705.07798  2017.

Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons  New York  NY  1994.

John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region

policy optimization. In International Conference on Machine Learning  2015.

John Schulman  Xi Chen  and Pieter Abbeel. Equivalence between policy gradients and soft Q-

learning. In arXiv:1704.06440  2017.

Richard S. Sutton. Dyna  an integrated architecture for learning  planning  and reacting. SIGART

Bulletin  2(4):160–163  1991.

Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. The MIT Press 

second edition  2018.

Balázs Szörényi  Gunnar Kedenburg  and Rémi Munos. Optimistic planning in Markov decision

processes using a generative model. In Neural Information Processing Systems  2014.

Thomas J Walsh  Sergiu Goschin  and Michael L Littman. Integrating sample-based planning and

model-based reinforcement learning. AAAI Conference on Artiﬁcial Intelligence  2010.

10

,Jean-Bastien Grill
Omar Darwiche Domingues
Pierre Menard
Remi Munos
Michal Valko