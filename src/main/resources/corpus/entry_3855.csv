2018,Objective and efficient inference for couplings in neuronal networks,Inferring directional couplings from the spike data of networks is desired in various scientific fields such as neuroscience. Here  we apply a recently proposed objective procedure to the spike data obtained from the Hodgkin-Huxley type models and in vitro neuronal networks cultured in a circular structure. As a result  we succeed in reconstructing synaptic connections accurately from the evoked activity as well as the spontaneous one. To obtain the results  we invent an analytic formula approximately implementing a method of screening relevant couplings. This significantly reduces the computational cost of the screening method employed in the proposed objective procedure  making it possible to treat large-size systems as in this study.,Objective and efﬁcient inference for couplings in

neuronal networks

Yu Terada1;2  Tomoyuki Obuchi2  Takuya Isomura1  Yoshiyuki Kabashima2

1Laboratory for Neural Computation and Adaptation 

RIKEN Center for Brain Science 

2-1 Hirosawa  Wako  Saitama 351-0198  Japan

2Department of Mathematical and Computer Science

Tokyo Institute of Technology

Tokyo 152-8550  Japan

yu.terada@riken.jp  obuchi@c.titech.ac.jp 
takuya.isomura@riken.jp  kaba@c.titech.ac.jp

Abstract

Inferring directional couplings from the spike data of networks is desired in var-
ious scientiﬁc ﬁelds such as neuroscience. Here  we apply a recently proposed
objective procedure to the spike data obtained from the Hodgkin–Huxley type
models and in vitro neuronal networks cultured in a circular structure. As a result 
we succeed in reconstructing synaptic connections accurately from the evoked ac-
tivity as well as the spontaneous one. To obtain the results  we invent an analytic
formula approximately implementing a method of screening relevant couplings.
This signiﬁcantly reduces the computational cost of the screening method em-
ployed in the proposed objective procedure  making it possible to treat large-size
systems as in this study.

1

Introduction

Recent advances in experimental techniques make it possible to simultaneously record the activity
of multiple units. In neuroscience  multi-electrodes and optical imaging techniques capture large-
scale behaviors of neuronal networks  which facilitate a deeper understanding of the information
processing mechanism of nervous systems beyond the single neuron level [1-6]. This preferable
situation  however  involves technical issues in dealing with such datasets because they usually con-
sist of a large amount of high-dimensional data which are difﬁcult to be handled by naive usages of
conventional statistical methods.
A statistical-physics-based approach for tackling these issues was presented using the Ising model
[7]. Although the justiﬁcation to use the Ising model for analyzing neuronal systems is not com-
pletely clear [8 9 10]  its performance was empirically demonstrated [7]  which triggered further
applications [11-22]. An advantage of using the Ising model is that several analytical techniques for
inverse problems are available [23-29]  which allows us to infer couplings between neurons with a
feasible computational cost. Another advantage is that it is straightforward to introduce variants of
the model. Beyond the conventional data analysis  an important variant is the kinetic Ising model 
which is more suitable to take into account the correlations in time  since this extended model re-
moves the symmetric-coupling constraint of the Ising model. A useful mean-ﬁeld (MF) inverse
formula for the kinetic Ising model has been presented in [25 26].
Two problems arise when treating neuronal systems’ data in the framework of the Ising models.
The ﬁrst problem is how to determine an appropriate size of time bins when discretizing original
signals in time; the appropriate size differs from the intrinsic time-scale of the original neuronal sys-

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

tems because the Ising models are regarded as a coarse-grained description of the original systems.
Hence  the way of the transformation to the models of this type is nontrivial. The second problem
is extracting relevant couplings from the solution of the inverse problem; unavoidable noises in ex-
perimental data contaminate the inferred couplings  and hence  we need to screen the relevant ones
among them.
In a previous study [30]  an information-theoretic method and a computational-statistical technique
were proposed for resolving the aforementioned ﬁrst and second problems  respectively. Those
methods were validated in two cases: in a numerical simulation based on the Izhikevich models and
in analyzing in vitro neuronal networks. The result is surprisingly good: their synaptic connections
are reconstructed with fairly high accuracy. This ﬁnding motivates us to further examine the methods
proposed in [30].
Based on this motivation  this study applies these methods to the data from the Hodgkin–Huxley
model  which describes the ﬁring dynamics of a biological neuron more accurately than the Izhike-
vich model. Further  we examine the situation where responses of neuronal networks are evoked by
external stimuli. We implement this situation both in the Hodgkin–Huxley model and in a cultured
neuronal network of a previously described design [31]  and test the methods in both the cases.
Besides  based on the previously described MF formula of [25 26]  we derive an efﬁcient formula
implementing the previous method of screening relevant couplings within a signiﬁcantly smaller
computational cost. In practice  the naive implementation of the screening method is computation-
ally expensive  and can be a bottleneck when applied to large-scale networks. Hence  we exploit
the simplicity of the model  and use the advanced statistical processing with reasonable time in
this work. Below  we address those three points by employing the simple kinetic Ising model  to
efﬁciently infer synaptic couplings in neuronal networks.

Inference procedure

2
The kinetic Ising model consists of N units  fsigN
(cid:6)1. Its dynamics is governed by the so-called Glauber dynamics:
P (s(t + 1)js(t);fJij; (cid:18)i(t)g) =
∑

N∏

i=1

i=1  and each unit takes bipolar values as si(t) =

exp [si(t + 1)Hi(t;fJij; (cid:18)i(t)g)]

exp [Hi(t;fJij; (cid:18)i(t)g)] + exp [(cid:0)Hi(t;fJij; (cid:18)i(t)g)]

;

(1)

{

R∑

M∑

where Hi(t) is the effective ﬁeld  deﬁned as Hi(t) = (cid:18)i(t) +
j=1 Jijsj(t)  (cid:18)i(t) is the external
force  and Jij is the coupling strength from j to i. This model also corresponds to a generalized
McCulloch–Pitts model in theoretical neuroscience and logistic regression in statistics. When ap-
plying this to spike train data  we regard the state si(t) = 1 (-1) as the ﬁring (non-ﬁring) state.
The inference framework we adopt here is the standard maximum-likelihood (ML) framework. We
repeat R experiments and denote a ﬁring pattern fs
ir(t)gN
i=1 for t = 1; 2;(cid:1)(cid:1)(cid:1) ; M in an experiment
(cid:3)
r(= 1; 2;(cid:1)(cid:1)(cid:1) ; R). The ML framework requires us to solve the following maximization problem on
the variable set fJij; (cid:18)i(t)g:

N

}

r=1

t=1

:

1
R

f ^Jij; ^(cid:18)i(t)g = arg max
fJij ;(cid:18)i(t)g

r(t + 1)j s
r(t);fJij; (cid:18)i(t)g)
(cid:3)
(cid:3)
log P ( s

(2)
This cost function is concave with respect to fJij; (cid:18)i(t)g  and hence  a number of efﬁcient solvers
are available [32]. However  we do not directly maximize eq. (2) in this study but instead we
employ the MF formula proposed previously [25 26]. The MF formula is reasonable in terms of
the computational cost and sufﬁciently accurate when the dataset size R is large. Moreover  the
availability of an analytic formula enables us to construct an effective approximation to reduce the
computational cost in the post-processing step  as shown in Sec. 2.3.
Unfortunately  in many experimental settings  it is not easy to conduct a sufﬁcient number of inde-
pendent experiments [33 34]  as in the case of Sec. 4. Hence  below we assume the stationarity of
any statistics  and ignore the time dependence of (cid:18)(t). This allows us to identify the average over
time as the ensemble average  which signiﬁcantly improves statistics. We admit this assumption is
not always valid  particularly in the case where time-dependent external forces are present  although
we treat such cases in Sec. 3.2 and Sec. 4.2. Despite this limitation  we still stress that the present
approach can extract synaptic connections among neurons accurately  although the existence of the

2

time-dependent inputs may decrease its performance. Possible directions to overcome this limitation
are discussed in Sec. 5.

2.1 Pre-processing: Discretization of time and binarization of state

In the pre-processing step  we have to decide the duration of the interval that should be used to
transform the real time to the unit time ∆(cid:28) in the Ising scheme. We term ∆(cid:28) the bin size. Once the
bin size is determined  the whole real time interval [0;T ] is divided into the set of time bins that are
labelled as ftgM =T =∆(cid:28)
. Given this set of the time bins  we binarize the neuron states: if there is no
i (t) = (cid:0)1; otherwise s
(cid:3)
(cid:3)
spike train of the neuron i in the time bin with a label t  then s
i (t) = 1. This
is the whole pre-processing step we adopt  and is a commonly used approach [7].
Determination of the bin size ∆(cid:28) can be a crucial issue: different values of ∆(cid:28) may lead to dif-
ferent results. To determine it in an objective way  we employ an information-theory-based method
proposed previously [30]. Following this method  we determine the bin size as

t=1

∆(cid:28)opt = arg max

∆(cid:28)

^I∆(cid:28) (si(t + 1); sj(t))

(3)

where I∆(cid:28) (si(t + 1); sj(t)) denotes the mutual information between si(t + 1) and sj(t) in the
coarse-grained series with ∆(cid:28)  and ^I∆(cid:28) (si(t + 1); sj(t)) is its plug-in estimator. The explicit for-
mula is

^I∆(cid:28) (si(t + 1); sj(t)) =

r(cid:11)(cid:12)(i; t + 1; j; t) log

r(cid:11)(cid:12)(i; t + 1; j; t)
r(cid:11)(i; t + 1)r(cid:12)(j; t)

;

(4)

((cid:11);(cid:12))2f+;(cid:0)g2

where r++(i; t + 1; j; t) denotes the realized ratio of the pattern (si(t + 1); sj(t)) = (+1; +1) 
r++(i; t + 1; j; t) (cid:17) (1=(M (cid:0) 1))#f(si(t + 1); sj(t)) = (+1; +1)g  and the other double-subscript
quantities fr+(cid:0); r(cid:0)+; r(cid:0)(cid:0)g are deﬁned similarly. Single-subscript quantities are also the realized
ratios of the corresponding state  for example  r+(j; t) (cid:17) (1=M )#fsj(t) = +1g.
The meaning of eq. (3) is clear: the formula inside the brace brackets of the right-hand side  hereafter
termed gross mutual information  is merely the likelihood of a (null) hypothesis that si(t + 1) and
sj(t) are ﬁring without any correlation. The optimal value ∆(cid:28)opt is chosen to reject this hypothesis
most strongly. This can also be regarded as a generalization of the chi-square test.

2.2

Inference algorithm: The MF formula

( T

8<:

∆(cid:28)

)∑

i̸=j

(cid:0) 1

∑

9=; ;

The previously derived MF formula [25 26] is given by
(cid:0)1DC
(cid:0)1;

^J MF = A

where

8><>: (cid:22)i(t) = ⟨si(t)⟩ ;
(

1 (cid:0) (cid:22)2

)

(cid:14)ij;

i (t)

Aij(t) =
Cij(t) = ⟨si(t)sj(t)⟩ (cid:0) (cid:22)i(t)(cid:22)j(t);
Dij(t) = ⟨si(t + 1)sj(t)⟩ (cid:0) (cid:22)i(t + 1)(cid:22)j(t):
∑

^(cid:18)MF
i

(t) = tanh

(cid:0)1 ((cid:22)i(t + 1)) (cid:0)

^J MF
ij (cid:22)j(t);

j

Note that the estimate ^J MF seemingly depends on time  but it is known that the time dependence is
very weak and ignorable. Once given ^J MF  the MF estimate of the external ﬁeld is given as

although we focus on the couplings between neurons and do not estimate the external force in this
study. The literal meaning of the brackets is the ensemble average corresponding to (1=R)
r=1 in
eq. (2)  but here we identify it as the average over time. Here  we use the time-averaged statistics of
f(cid:22); C; D; (cid:18)g  as declared above.

R

(5)

(6)

(7)

∑

3

2.3 Post-processing: Screening relevant couplings and its fast approximation

The basic idea of our screening method is to compare the coupling estimated from the original
data with the one estimated from randomized data in which the time series of ﬁring patterns of
each neuron is randomly independently permuted. We do not explain the detailed procedures here
because similar methods have been described previously [7 30]. Instead  here we state the essential
point of the method and derive an approximate formula implementing the screening method in a
computationally efﬁcient manner.
The key of the method is to compute the probability distribution of ^Jij  P ( ^Jij)  when applying our
inference algorithm to the randomized data. Once we obtain the probability distribution  we can
judge how unlikely our original estimate is as compared to the estimates from the randomized data.
If the original estimate is sufﬁciently unlikely  we accept it as a relevant coupling; otherwise  we
reject it.
Evaluation of the above probability distribution is not easy in general  and hence  it is common to
have recourse to numerical sampling  which can be a computational burden. Here  we avoid this
problem by computing it in an analytical manner under a reasonable approximation.
For the randomized data  we may assume that two neurons si and sj ﬁre independently with ﬁxed
means (cid:22)i and (cid:22)j  respectively. Under this assumption  by the central limit theorem  each diagonal
component of C converges to Cii = 1 (cid:0) (cid:22)2
i = Aii  while its non-diagonal component becomes a
zero-mean Gaussian variable whose variance is proportional to 1=(M (cid:0) 1)  and is thus  small. All
the components of D behave similarly to the non-diagonal ones of C. This consideration leads to
the expression

(cid:0)1)jj =

1

(1 (cid:0) (cid:22)2

i )(1 (cid:0) (cid:22)2
j )

(8)
Dij:
i )(1 (cid:0) (cid:22)2
j )=(M (cid:0) 1).

^J ran
ij =

(A

∑

k

(

(cid:0)1)iiDik(C
(

(cid:0)1)kj (cid:25) (A
)

ij

j (cid:21) (cid:8)th
)

j ^J ran

ij

j (cid:21) (cid:8)th

P

(cid:25) 1 (cid:0) erf

(cid:0)1)iiDij(A
√

0@(cid:8)th
∫

By the independence between si and sj  the variance of Dij becomes (1 (cid:0) (cid:22)2
Hence the probability P

is obtained as

j ^J ran

(1 (cid:0) (cid:22)2

i )(1 (cid:0) (cid:22)2

j )(M (cid:0) 1)

where erf(x) is the error function deﬁned as
erf(x) (cid:17) 2p
(cid:25)
√

Inserting the absolute value of the original estimate of ^Jij in (cid:8)th  we obtain its likelihood  and can
judge whether it should be accepted. Below  we set the signiﬁcance level pth associated with ((cid:8)th)ij
as

2

(cid:0)y2

:

dy e

x

0

(9)

(10)

1A ;

((cid:8)th)ij =

2

(1 (cid:0) (cid:22)2

i )(1 (cid:0) (cid:22)2

j )(M (cid:0) 1)

and accept only ^Jij such that j ^Jijj > ((cid:8)th)ij.

(cid:0)1 (1 (cid:0) pth)

erf

(11)

3 Hodgkin–Huxley networks

We ﬁrst evaluate the accuracy of our methods using synthetic systems consisting of the Hodgkin–
Huxley neurons. The dynamics of the neurons are given by

C

dVi
d(cid:28)
dni
d(cid:28)
dmi
d(cid:28)
dhi
d(cid:28)

i hi (Vi (cid:0) ENa) (cid:0) (cid:22)gL (Vi (cid:0) EL) + Iex
i ;

= (cid:0)(cid:22)gKn4
i (Vi (cid:0) EK) (cid:0) (cid:22)gNam3
= (cid:11)n (Vi) (1 (cid:0) ni) (cid:0) (cid:12)n (Vi) ni;
= (cid:11)m (Vi) (1 (cid:0) mi) (cid:0) (cid:12)m (Vi) mi;
= (cid:11)h (Vi) (1 (cid:0) hi) (cid:0) (cid:12)h (Vi) hi;

(12)

(13)

(14)

(15)

4

where Vi is the membrane potential of ith neuron  ni is the activation variable that represents the
ratio of the open channels for K+ ion  and mi and hi are the activation and inactivation variables
for Na+ ion  respectively. All parameters  except the external input term Iex
i   are set as described in
[35]. The input forces are given by

Iex
i = ci((cid:28) ) +

KijVj(cid:2) (Vj (cid:0) Vth) + a

;

(16)

N∑

j=1

)

∑

(

(cid:28) (cid:0) (cid:28) k

i

(cid:14)

k

where ci(t) represents the environmental noise with a Poisson process  the second term represents
the couplings with the threshold voltage Vth = 30 mV and the Heaviside step function (cid:2)((cid:1))  and
the last term denotes the impulse stimulations with the delta function. Here  we consider no-delay
simple couplings  which we term the synaptic connections  and aim to reconstruct their structure
with the excitatory/inhibitory signs using our methods. We use N = 100 neuron networks  where
the 90 neurons are excitatory and have positive outgoing couplings while the others are inhibitory.
The rate and strength of the Poisson process are set as (cid:21) = 180 Hz and b = 2 mV  respectively 
for all neurons. We generate their time series  integrating (12)-(15) by the Euler method with d(cid:28) =
0:01 ms  where we suppose a neuron is ﬁring when its voltage exceeds Vth  and use the spike train
data with the whole period T = 106 ms for our inference.

3.1 Spontaneous activity case

√

(1 (cid:0) (cid:22)2

i )(1 (cid:0) (cid:22)2

At ﬁrst  we consider a system on a chain network in which each neuron has three synaptic connec-
tions to adjoint neurons in one direction. The connection strength Kij is drawn from the uniform
distributions in [0:015; 0:03] for the excitatory and in [(cid:0)0:06;(cid:0)0:03] for the inhibitory neurons  re-
spectively. Here  we set a = 0 mV to study the spontaneous activity. An example of the spike
trains generated during 3 seconds is shown in Fig. 1 (a)  where the spike times and correspond-
ing neuronal indices are plotted. Subsequently  using the whole spike train data  we calculate the
gross mutual information for different ∆(cid:28)  and the result is indicated by the red curve in Fig. 1
(b). The curve has the unimodal feature  which implies the existence of the optimal time bin size of
approximately ∆(cid:28) = 3 ms  although the original system does not have the delay. We suppose that
inputs must accumulate sufﬁciently to generate a spike  which costs some time scale  and this is a
possible reason for the emergence of the nontrivial time-scale. To validate our approximation (8) 
we randomize the coarse-grained series with ∆(cid:28) = 3 ms in the time direction independently  rescale
j )(M (cid:0) 1)  and compare the results of 1000 randomized data
^J ran
ij by multiplying
with the standard Gauss distribution in Fig. 1 (c)  which shows their good correspondence. Using
∆(cid:28) = 3 ms to make the spike trains coarse-grained  we apply the inverse formula to the series and
(cid:0)3  which leads to the estimated coupling matrix shown in
screen relevant couplings with pth = 10
Fig. 1 (e)  while the one used to generate the data is shown in Fig. 1 (d). The asymmetric network
structure is recovered sufﬁciently with the discrimination of the signs of the couplings. The con-
ditional ratios of the correctness are shown in Fig. 1 (f)  where the inference results obtained with
different values of ∆(cid:28) are also shown. This demonstrates the fairly accurate reconstruction result
obtained using our inference procedure. We also show the receiver operating characteristic (ROC)
curves obtained by gradually changing the value pth in Fig. 1 (g)  with the different values of ∆(cid:28).
We conclude that using non-optimal time bins drastically decreases the accuracy of the inference
results.
To illustrate the robustness of the optimality of the time bin  in Fig. 1 (i) we plot the means and
standard deviations of the gross mutual information through the 10 different simulations  showing
that the variance is small enough and the result is well robust.
To consider a more general situation  we also employ a Hodgkin–Huxley system on a random net-
work. The directional synaptic connection between every pair of neurons is generated with the
probability 0:1  and the excitatory and inhibitory couplings are drawn from the uniform distribu-
tions within [0:01; 0:02] and [(cid:0)0:04;(cid:0)0:02]  respectively. The corresponding inference results for
its spontaneous activity are shown by green curves in Figs. 1 (b) and (f). The ROC curves for the
three different three values of ∆(cid:28) are also shown in (h). We conﬁrm that the inference is sufﬁciently
effective in the random-network system as well as in the chain system.

5

Figure 1: Application of the proposed approach to the Hodgkin–Huxley models. (a) Spontaneous
spike trains during 3 seconds. (b) Gross mutual information v.s. time bin size ∆(cid:28). The red curve
shows the chain network while the green curve shows the random network. (c) Histogram of rescaled
^J ran
ij obtained by randomizing the original series  and the standard Gauss distribution. (d) An exam-
ple of the chain networks that we used  where the red and blue elements indicate the excitatory and
inhibitory couplings  respectively. (e) Corresponding inferred coupling network with ∆(cid:28) = 3 ms.
(f) Conditional correctness ratios for the existence  absence  excitatory coupling  and inhibitory cou-
pling  where the standard deviations of 10 different simulations are shown with the error bars. (g h)
Receiver operating characteristic curves for different coarse-grained series in the systems (g) on the
chain and (h) on the random network  where the error bars indicate the standard deviations of 10
(cid:0)3 used in (e) and (f). (i) The mean
different simulations. The marked points indicate pth = 10
and standard deviation of the gross mutual information for 10 independent simulations of the chain
systems. The result is shown to be robust.

3.2 Evoked activity case

We next investigate performance in systems where responses are evoked by impulse stimuli. The
model parameters  except for a  are the same as those in the chain model in Sec. 3.1. The strength of
the external force is set as a = 5:3 mV  and the stimulations are injected to all neurons with interval
1 s. In Fig. 2 (a) we show the spike trains  where we observe that most of the neurons ﬁre at the
injection times (cid:28) = 0:5; 1:5; 2:5 s. The gross mutual information against ∆(cid:28) is shown in Fig. 2 (b).
Although the curve feature is modiﬁed due to the existence of the impulse inputs  we observe that
its peak is located at a similar value of ∆(cid:28). Therefore  we use the same value ∆(cid:28) = 3 ms. Applying
(cid:0)3  we obtain the inferred couplings which
our inference procedure with ∆(cid:28) = 3 ms and pth = 10
are shown in Fig. 2 (c)  where the original network is in Fig. 1 (d). On comparing Fig. 2 (c) with
Fig. 1 (e)  while the inference detects the existence of the synaptic connections  we observe more
false couplings in the evoked case. The conditional ratios in Fig. 2 (d) indicate that the existence
of the external inputs may increase the false positive rate with the same pth. The ROC curves are
shown in Fig. 2 (f).

6

(a)(b)(c)(d)(e)(f)0100020003000102030405060708090100[ms]1011001021030[ms]24681014122040608010020406080100204060801002040608010010.50ExistenceAbsenceExcitatoryInhibitoryΔτ = 3 ms (chain)Δτ = 1 ms (chain)Δτ = 10 ms (chain)Δτ = 3 ms (random network)chainrandom network420-2-400.20.40.10.3StandardGaussdistributionRescaled(g)Δτ = 3 ms Δτ = 1 ms Δτ = 10 ms 00110.50.5(h)00110.50.5Δτ = 3 ms Δτ = 1 ms Δτ = 10 ms timeneuronΔτjijiFalse Positive RatioTrue Positive RatioFalse Positive RatioTrue Positive Ratio(i)101100102103[ms]Δτ02468101412Figure 2: Application of the proposed approach to the evoked activity in the Hodgkin–Huxley
models. (a) Evoked spike trains during 3 seconds  where the red line expresses the injection times
of the stimuli. (b) Gross mutual information v.s. time bin size. (c) Inferred coupling matrix with
the red excitatory and blue inhibitory elements using ∆(cid:28) = 3 ms  where the generative network is
the one shown in Fig. 1 (b). (d) Conditional correctness ratios. (e) Receiver operating characteristic
(cid:0)3 are marked. (f)
curves for different coarse-grained series  where the points denoting pth = 10
The mean and standard deviation of the gross mutual information for 10 independent simulations.
The result is shown to be robust.

4 Cultured neuronal networks

We apply our inference methods to the real neuronal systems introduced in a previous study [31] 
where rat cortical neurons were cultured in micro wells. The wells had a circular structure  and con-
sequently the synapses of the neurons were likely to form a physically asymmetric chain network 
which is similar to the situation in the Hodgkin–Huxley models we used in Sec. 3. The activity of
the neurons was recorded by the multi-electrode array with 40 (cid:22)s time resolution  and the Efﬁcient
Technology of Spike sorting method [36] was used to identify the spike events of individual neurons.
We study the spontaneous and evoked activities here.

4.1 Spontaneous activity case

We ﬁrst use the spontaneous activity data recorded during 120 s. The spike sorting identiﬁed 100
neurons which generated the spikes. The spike raster plot during 3 seconds is displayed in Fig.
3 (a). We calculate the gross mutual information as in case of the Hodgkin–Huxley models  and
the obtained optimal bin size is approximately ∆(cid:28) = 5 ms. We also conﬁrm that the inferred
couplings are similar to the results described previously [30]  and this supports the validity of our
novel approximation method introduced in Sec. 2.3. We show the inferred network in Figs. 3 (b-
(cid:0)9  where we locate the nodes denoting the neurons
d) with different values pth = 10
on a circle following the experimental design [31]. A more strict threshold provides us with clear
demonstration of the relevant couplings here.

(cid:0)6; 10

(cid:0)3; 10

4.2 Evoked activity case

We next study an evoked neuronal system  where an electrical pulse stimulation is injected from
an electrode after every 3 seconds  and the other experimental settings are similar to those of the
spontaneous case. In this case the activity of 149 neurons were identiﬁed by the spike sorting. The
example of the spike trains is shown in Fig. 4 (a). The gross mutual information is shown in Fig. 4
(cid:0)6 
(b)  where we can see the peak around ∆(cid:28) = 10 ms. Setting ∆(cid:28) = 10 ms and pth = 10
we obtain the estimated coupling matrices in Figs. 4 (c d). In these cases  we can also observe
the bold diagonal elements representing the asymmetric chain structure  although with the lower

(cid:0)3; 10

7

(a)(b)(c)(d)(e)0100020003000102030405060708090100[ms]1011001021030[ms]5102015204060801002040608010010.50ExistenceAbsenceExcitatoryInhibitory25Δτ = 3 msΔτ = 1 msΔτ = 10 ms00110.50.5False Positive RatioTrue Positive RatiotimeneuronΔτjiΔτ = 3 msΔτ = 1 msΔτ = 10 ms1011001021030[ms]510201525Δτ(f)Figure 3: Application of the proposed approach to a cultured-neuronal system. (a) Spike trains
during 3 seconds. (b-d) Inferred networks  where the nodes are located on the circle corresponding to
(cid:0)9.
the experimental design. The different signiﬁcant levels used are: (b) 10
The red and blue directional arrows represent the excitatory and inhibitory couplings  respectively.
(e) The gross mutual information for the 1st and 2nd halves of the data. The ﬁgure shows the
robustness of the result.

(cid:0)6  and (d) 10

(cid:0)3  (c) 10

signiﬁcant level some far-diagonal elements emerge due to the existence of the external inputs 
which is a situation similar to that in the Hodgkin–Huxley simulation in Sec. 3.2. The inferred
(cid:0)9 is displayed in Fig. 4 (e)  where some long-range
network with the strict threshold pth = 10
couplings are still estimated while physical connections corresponding to them do not exist because
of the experimental design.

5 Conclusion and discussion

We propose a systematic inference procedure for extracting couplings from point-process data. The
contribution of this study is three-fold: (i) invention of an analytic formula to screen relevant cou-
plings in a computationally efﬁcient manner; (ii) examination in the Hodgkin–Huxley model  with
and without impulse stimuli; (iii) examination in an evoked cultured neuronal network.
The applications to the synthetic data  with and without the impulse stimuli  demonstrate the fairly
accurate reconstructions of synaptic connections by our inference methods. The application to the
real data of the spontaneous activity in the cultured neuronal system also highlights the effectiveness
of the proposed methods in detecting the synaptic connections.
From the comparison between the analyses of the spontaneous and evoked activities  we found that
the inference accuracy becomes degraded by the external stimuli. One of the potential origins is
the breaking of our stationary assumption of the statistics f(cid:22); C; Dg because of the time-varying
external force (cid:18). To overcome this  certain techniques resolving the insufﬁciency of samples  such
as regularization  will be helpful. A promising approach might be the introduction of an ℓ1 regular-
ization into eq. (2)  which enables us to automatically screen out irrelevant couplings. Comparing it
with the present approach based on computational statistics will be an interesting future work.

Acknowledgments

This work was supported by MEXT KAKENHI Grant Numbers 17H00764 (YT  TO  and YK) and
18K11463 (TO)  and RIKEN Center for Brain Science (YT and TI).

8

(b)(c)(d)(d)(a)20406080100100020003000[ms]timeneuron1st half2nd half10110010210-1[ms]Δτ01234Figure 4: Application of the proposed approach to an evoked cultured-neuronal system. (a) Spike
trains during 3 seconds  where the red line indicates the injection time. (b) Gross mutual information
(cid:0)6. (e)
v.s. time bin size. (c d) Inferred coupling matrices for (c) pth = 10
(cid:0)9. (f) The gross mutual information for the 1st and 2nd halves of
Inferred network with pth = 10
the data. The ﬁgure shows the robustness of the result.

(cid:0)3 and (d) pth = 10

References

[1] Brown  E.N.  Kass  R.E. & Mitra  P.P. (2004) Multiple neural spike train data analysis: state-of-the-art and
future challenges. Nature Neuroscience 7 456-461.
[2] Buzsáki  G. (2004) Large-scale recording of neuronal ensembles. Nature Neuroscience 7 446-451.
[3] Yuste  R. (2015) From the neuron doctrine to neuronal networks. Nature Reviews Neuroscience 16 487-497.
[4] Roudi  Y.  Dunn  B. & Hertz  J. (2015) Multi-neuronal activity and functional connectivity. Current Opinion
in Neurobiology 32 38-44.
[5] Gao  P. & Ganguli  S. (2015) On simplicity and complexity in the brave new world of large-scale neuro-
science. Current Opinion in Neurobiology 32 148-155.
[6] Maass  W. (2016) Searching for principles of brain computation. Current Opinion in Behavioral Sciences
11 81-92.
[7] Schneidman  E.  Berry  M.J.  Segev  R. & Bialek  W. (2006) Weak pairwise correlations imply strongly
correlated network states in a neural population. Nature 440 1007-1012.
[8] Obuchi  T.  Cocco  S. & Monasson  S. (2015) Learning probabilities from random observables in high
dimensions: the maximum entropy distribution and others. Journal of Statistical Physics 161 598-632.
[9] Obuchi  T. & Monasson  R. (2015) Learning probability distributions from smooth observables and the
maximum entropy principle: some remarks. Journal of Physics: Conference Series 638 012018.
[10] Ferrari  U.  Obuchi  T. & Mora  T.  (2017) Random versus maximum entropy models of neural population
activity. Physical Review E 95 042321.
[11] Sessak  V. & Monasson  R. (2009) Small-correlation expansions for the inverse Ising problem. Journal of
Physics A: Mathematical and Theoretical 42 055001.
[12] Roudi  Y.  Tyrcha  J. & Hertz  J. (2009) Ising model for neural data: Model quality and approximate
methods for extracting functional connectivity. Physical Review E 79 051915.
[13] Shlens  J.  Field  G.D.  Gauthier  J.L.  Grivich  M.I.  Petrusca  D.  Sher  A.  Litke  A.M. & Chichilnisky 
E. (2006) The Structure of Multi-Neuron Firing Patterns in Primate Retina. The Journal of Neuroscience 26
8254-8266.

9

(a)(b)(d)(e)204060801002040608010010-1100101102070102060504030[ms]12014012014020406080100100020003000[ms]120140timeneuronΔτji(c)2040608010020406080100120140120140ji1st half2nd half(f)10110010210-1[ms]Δτ010203040(2015) Inferring Synaptic

[14] Tang  A.  Jackson  D.  Hobbs  J.  Chen  W.  Smith  J.L.  Patel  H.  Prieto  A.  Petrusca  D.  Grivich 
M.I.  Sher  A.  Hottowy  P.  Dabrowski  W.  Litke  A.M. & Beggs J.M. (2008) A Maximum Entropy Model
Applied to Spatial and Temporal Correlations from Cortical Networks In Vitro. The Journal of Neuroscience
28 505-518.
[15] Cocco  S.  Leibler  S. & Monasson  R. (2009) Neuronal couplings between retinal ganglion cells inferred
by efﬁcient inverse statistical physics methods. Proceedings of the National Academy of Sciences 106 14058-
14062.
[16] Marre  O.  Boustani  S.E.  Frégnac  Y.  & Destexhe  A. (2009) Prediction of Spatiotemporal Patterns of
Neural Activity from Pairwise Correlations. Physical Review Letters 102 138101.
[17] Ohiorhenuan  I.E.  Mechler  F.  Purpura  K.P.  Schmid  A.E.  Hu  Q. & Victor  J.D. (2010) Sparse coding
and high-order correlations in ﬁne-scale cortical networks. Nature 466 617-621.
[18] Ganmor  E.  Segev  R. & Schneidman  E. (2011) Sparse low-order interaction network underlies a highly
correlated and learnable neural population code. Proceedings of the National Academy of Sciences 108 9679-
9684.
[19] Tyrcha  J.  Roudi  Y.  Marsili  M. & Hertz  J. (2013) The effect of nonstationarity on models inferred from
neural data. Journal of Statistical Mechanics: Theory and Experiment P03005.
[20] Dunn  B.  Mørreaunet  M. & Roudi  Y. (2015) Correlations and Functional Connections in a Population
of Grid Cells. PLoS Computational Biology 11 e1004052.
[21] Capone  C.  Filosa  G.  Gigante  G.  Ricci-Tersenghi  F. & Del Giudice  P.
Structure in Presence of Neural Interaction Time Scales. PLoS ONE 10 e0118412.
[22] Posani  L.  Cocco  S. & Monasson  R. (2018) Integration and multiplexing of positional and contextual
information by the hippocampal network. bioRxiv 269340.
[23] Kappen  H.J. & Rodríguez  F.d.B. (1998) Efﬁcient Learning in Boltzmann Machines Using Linear Re-
sponse Theory. Neural Computation 10 1137-1156.
[24] Tanaka  T. (1998) Mean-ﬁeld theory of Boltzmann machine learning. Physical Review E 58 2302.
[25] Roudi  Y. & Hertz  J. (2011) Mean Field Theory for Nonequilibrium Network Reconstruction. Physical
Review Letters 106 048702.
[26] Mézard  M. & Sakellariou  J. (2011) Exact mean-ﬁeld inference in asymmetric kinetic Ising systems.
Journal of Statistical Mechanics: Theory and Experiment L07001.
[27] Zeng  H.L.  Aurell  E.  Alava  M. & Mahmoudi  H. (2011) Network inference using asynchronously
updated kinetic Ising model. Physical Review E 83 041135.
[28] Aurell  E. & Ekeberg  M. (2012) Inverse Ising Inference Using All the Data. Physical Review Letters 108
090201.
[29] Zeng  H.L.  Alava  M.  Aurell  E.  Hertz  J. & Roudi  Y. (2013) Mazimum Likelihood Reconstruction for
Ising Models with Asynchronous Updates. Physical Review Letters 110 210601.
[30] Terada  Y.  Obuchi  T.  Isomura  T. & Kabashima  Y. (2018) Objective Procedure for Reconstructing
Couplings in Complex Systems. arXiv 1803.04738.
[31] Isomura  T.  Shimba  K.  Takayama  Y.  Takeuchi  A.  Kotani  K. & Jimbo  Y. (2015) Signal transfer within
a cultured asymmetric cortical neuron circuit. Journal of Neural Engineering 12 066023.
[32] Hastie  T.  Tibshirani  R. & Friedman  J. (2016) The Elements of Statistical Learning: Data Mining 
Inference  and Prediction. 2nd ed. New York  Springer.
[33] Churchland  M.M.  Yu  B.M.  Sahani  M. & Shenoy  K.V. (2007) Techniques for extracting single-trial
activity patterns from large-scale neural recordings. Current Opinion in Neurobiology 17 609-618.
[34] Cunningham  J.P. & Yu  B.M. (2014) Dimensionality reduction for large-scale neural recordings. Nature
Neuroscience 17 1500-1509.
[35] Izhikevich  E.M. (2007) Dynamical systems in neuroscience. Cambridge  MIT Press.
[36] Takekawa  T.  Isomura  Y. & Fukai  T. (2010) Accurate spike sorting for multi-unit recordings. European
Journal of Neuroscience 31 263-272.

10

,Yu Terada
Tomoyuki Obuchi
Takuya Isomura
Yoshiyuki Kabashima