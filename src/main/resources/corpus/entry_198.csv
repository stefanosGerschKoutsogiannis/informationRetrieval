2017,A Screening Rule for l1-Regularized Ising Model Estimation,We discover a screening rule for l1-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exactly recovering the blockwise structure of a solution under any given regularization parameters. With enough sparsity  the screening rule can be combined with various optimization procedures to deliver solutions efficiently in practice. The screening rule is especially suitable for large-scale exploratory data analysis  where the number of variables in the dataset can be thousands while we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability. Experimental results on various datasets demonstrate the efficiency and insights gained from the introduction of the screening rule.,A Screening Rule for (cid:96)1-Regularized

Ising Model Estimation

Zhaobin Kuang1  Sinong Geng2  David Page3

University of Wisconsin

zkuang@wisc.edu1  sgeng2@wisc.edu2  page@biostat.wisc.edu3

Abstract

We discover a screening rule for (cid:96)1-regularized Ising model estimation. The simple
closed-form screening rule is a necessary and sufﬁcient condition for exactly
recovering the blockwise structure of a solution under any given regularization
parameters. With enough sparsity  the screening rule can be combined with various
optimization procedures to deliver solutions efﬁciently in practice. The screening
rule is especially suitable for large-scale exploratory data analysis  where the
number of variables in the dataset can be thousands while we are only interested
in the relationship among a handful of variables within moderate-size clusters for
interpretability. Experimental results on various datasets demonstrate the efﬁciency
and insights gained from the introduction of the screening rule.

1

Introduction

While the ﬁeld of statistical learning with sparsity [Hastie et al.  2015] has been steadily rising to
prominence ever since the introduction of the lasso (least absolute shrinkage and selection operator)
at the end of the last century [Tibshirani  1996]  it was not until the recent decade that various
screening rules debuted to further equip the ever-evolving optimization arsenals for some of the
most fundamental problems in sparse learning such as (cid:96)1-regularized generalized linear models
(GLMs  Friedman et al. 2010) and inverse covariance matrix estimation [Friedman et al.  2008].
Screening rules  usually in the form of an analytic formula or an optimization procedure that is
extremely fast to solve  can accelerate learning drastically by leveraging the inherent sparsity of many
high-dimensional problems. Generally speaking  screening rules can identify a signiﬁcant portion of
the zero components of an optimal solution beforehand at the cost of minimal computational overhead 
and hence substantially reduce the dimension of the parameterization  which makes possible efﬁcient
computation for large-scale sparse learning problems.
Pioneered by Ghaoui et al. 2010  various screening rules have emerged to speed up learning for
generative models (e.g. Gaussian graphical models) as well as for discriminative models (e.g. GLMs) 
and for continuous variables (e.g. lasso) as well as for discrete variables (e.g. logistic regression 
support vector machines). Table 1 summarizes some of the iconic work in the literature  where  to the
best of our knowledge  screening rules for generative models with discrete variables are still notably
absent.
Contrasted with this notable absence is the ever stronger craving in the big data era for scaling
up the learning of generative models with discrete variables  especially in a blockwise structure
identiﬁcation setting. For example  in gene mutation analysis [Wan et al.  2015  2016]  among tens of
thousands of sparse binary variables representing mutations of genes  we are interested in identifying
a handful of mutated genes that are connected into various blocks and exert synergistic effects on
the cancer. While a sparse Ising model is a desirable choice  for such an application the scalability
of the model could fail due to the innate NP-hardness [Karger and Srebro  2001] of inference  and
hence maximum likelihood learning  owing to the partition function. To date  even with modern

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Table 1: Screening rules in the literature at a glance

Discriminative Models

Generative Models

Continuous
Variables

Ghaoui et al. 2010  Tibshirani et al. 2012

Liu et al. 2013  Wang et al. 2013 

Fercoq et al. 2015  Xiang et al. 2016 

Lee et al. 2017

Discrete
Variables

Ghaoui et al. 2010  Tibshirani et al. 2012

Wang et al. 2014  Ndiaye et al. 2015

Banerjee et al. 2008 Honorio and Samaras 2010
Witten et al. 2011 Mazumder and Hastie 2012

Danaher et al. 2014  Luo et al. 2014

Yang et al. 2015

?

approximation techniques  a typical application with sparse discrete graphical models usually involves
only hundreds of variables [Viallon et al.  2014  Barber et al.  2015  Vuffray et al.  2016].
Between the need for the scalability of high-dimensional Ising models and the absence of screening
rules that are deemed crucial to accelerated and scalable learning  we have a technical gap to bridge:
can we identify screening rules that can speed up the learning of (cid:96)1-regularized Ising models? The
major contribution of this paper is to give an afﬁrmative answer to this question. Speciﬁcally  we
show the following.
• The screening rule is a simple closed-form formula that is a necessary and sufﬁcient condition for
exact blockwise structure recovery of the solution with a given regularization parameter. Upon the
identiﬁcation of blockwise structures  different blocks of variables can be considered as different
Ising models and can be solved separately. The various blocks can even be solved in parallel to
attain further efﬁciency. Empirical results on both simulated and real-world datasets demonstrate
the tremendous efﬁciency  scalability  and insights gained from the introduction of the screening
rule. Efﬁcient learning of (cid:96)1-regularized Ising models from thousands of variables on a single
machine is hence readily attainable.
• As an initial attempt to ﬁll in the vacancy illustrated in Table 1  our work is instructive to further
exploration of screening rules for other graphical models with discrete random variables  and
to combining screening rules with various optimization methods to facilitate better learning.
Furthermore  compared with its Gaussian counterpart  where screening rules are available (Table 1)
and learning is scalable [Hsieh et al.  2013]  the proposed screening rule is especially valuable and
desperately needed to address the more challenging learning problem of sparse Ising models.

We defer all the proofs in the paper to the supplement and focus on providing intuition and interpreta-
tion of the technical results in the paper.

2 Notation and Background

Ising Models

2.1
(cid:62)
Let X = [X1  X2 ···   Xp]
be a p × 1 binary random vector  with Xi ∈ {−1  1}  and i ∈
{1  2 ···   p} (cid:44) V . Let there be a dataset X with n independent and identically distributed samples

of X  denoted as X =(cid:8)x(1)  x(2) ···   x(n)(cid:9). Here  x(k) is a p×1 vector of assignments that realizes

X  where k ∈ {1  2 ···   n}. We further use x(k)
to denote the ith component of the kth sample in
the dataset. Let θ ∈ Θ be a p × p symmetric matrix whose diagonal entries are zeros. An Ising model
[Wan et al.  2016] with the parameterization θ is:

i

p−1(cid:88)

p(cid:88)

  

Pθ(x) =

1

Z(θ)

exp

θijxixj

i=1

j>i

(1)

(cid:80)

(cid:16)(cid:80)p−1

where θij represents the component of θ at the ith row and the jth column  and xi and xj represent
the ith and the jth components of x  respectively. Z(θ) is a normalization constant  partition
function  that ensures the probabilities sum up to one. The partition function is given as Z(θ) =
. Note that for ease of presentation  we consider Ising
models with only pairwise interaction/potential here. Generalization to Ising models with unary
potentials is given in Section 6.

x∈{−1 1}p exp

j>i θijxixj

(cid:80)p

(cid:17)

i=1

2

2.2 Graphical Interpretation

With the notion of the probability given by an Ising model in (1)  estimating an (cid:96)1-regularized Ising
model is deﬁned as ﬁnding ˆθ  the penalized maximum likelihood estimator (MLE) under the lasso
penalty:

ˆθ = arg max

θ

1
n

(cid:107)θ(cid:107)1

n(cid:88)
n(cid:88)

k=1

log Pθ

p−1(cid:88)

x(k)(cid:17) − λ
(cid:16)
p(cid:88)

2

(2)

θ

i=1

j>i

k=1

− 1
n

(cid:107)θ(cid:107)1.

θijx(k)

i x(k)

= arg min

j + A(θ) +

λ
Here  A(θ) = log Z(θ) is the log-partition function; (cid:107)θ(cid:107)1 =(cid:80)p
(cid:80)p
2
j=1|θij| is the lasso penalty
(cid:80)p
that encourages a sparse parameterization. λ ≥ 0 is a given regularization parameter. Using λ
2 is
j>i|θij|  which echoes the summations
suggestive of the symmetry of θ so that λ
in the negative log-likelihood function. Note that θ corresponds to the adjacency matrix constructed
by the p components of X as nodes  and θij (cid:54)= 0 indicates that there is an edge between Xi and
Xj. We further denote a partition of V into L blocks as {C1  C2 ···   CL}  where Cl  Cl(cid:48) ⊆ V  
l=1 Cl = V   l (cid:54)= l(cid:48)  and for all l  l(cid:48) ∈ {1  2 ···   L}. Without loss of generality  we
assume that the nodes in different blocks are ordered such that if i ∈ Cl  j ∈ Cl(cid:48)  and l < l(cid:48)  then
i < j.

2(cid:107)θ(cid:107)1 = λ(cid:80)p−1

Cl ∩ Cl(cid:48) = ∅ (cid:83)L

i=1

i=1

2.3 Blockwise Solutions

We introduce the deﬁnition of a blockwise parameterization:
Deﬁnition 1. We call θ blockwise with respect to the partition {C1  C2 ···   CL} if ∀l and l(cid:48) ∈
{1  2 ···   L}  where l (cid:54)= l(cid:48)  and ∀i ∈ Cl  ∀j ∈ Cl(cid:48)  we have θij = 0.
When θ is blockwise  we can represent θ in a block diagonal fashion:

θ = diag (θ1  θ2 ···   θL)  

(3)
where θ1  θ2  ···   and θL are symmetric matrices that correspond to C1  C2  ···   and CL  respectively.
Note that if we can identify the blockwise structure of ˆθ in advance  we can solve each block
independently (See A.1). Since the size of each block could be much smaller than the size of the
original problem  each block could be much easier to learn compared with the original problem.
Therefore  efﬁcient identiﬁcation of blockwise structure could lead to substantial speedup in learning.

3 The Screening Rule

3.1 Main Results

(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given. Deﬁne EXXiXj = 1

The preparation in Section 2 leads to the discovery of the following strikingly simple screening rule
presented in Theorem 1.
Theorem 1. Let a partition of V  {C1  C2 ···   CL}  be given.
i x(k)
condition for ˆθ to be blockwise with respect to the given partition is that

the dataset X =
. A necessary and sufﬁcient

(cid:80)n

k=1 x(k)

Let

n

j

|EXXiXj| ≤ λ 

(4)

for all l and l(cid:48) ∈ {1  2 ···   L}  where l (cid:54)= l(cid:48)  and for all i ∈ Cl  j ∈ Cl(cid:48).
In terms of exact blockwise structure identiﬁcation  Theorem 1 provides a foolproof (necessary and
sufﬁcient) and yet easily checkable result by comparing the absolute second empirical moments
|EXXiXj|’s with the regularization parameter λ. We also notice the remarkable similarity between
the proposed screening rule and the screening rule for Gaussian graphical model blockwise structure
identiﬁcation in Witten et al. 2011  Mazumder and Hastie 2012. In the Gaussian case  the screening
rule can be attained by simply replacing the second empirical moment matrix in (4) with the sample

3

Algorithm 1 Blockwise Minimization
1: Input: dataset X  regularization parameter λ.
2: Output: ˆθ.
3: ∀i  j ∈ V such that j > i  compute the second empirical moments EXXiXj’s .
4: Identify the partition {C1  C2 ···   CL} using the second empirical moments from the previous
5: ∀l ∈ L  perform blockwise optimization over Cl for ˆθl.
6: Ensemble ˆθl’s according to (3) for ˆθ.
7: Return ˆθ.

step and according to Witten et al. [2011]  Mazumder and Hastie [2012].

covariance matrix. While the exact solution in the Gaussian case can be computed in polynomial
time  estimating an Ising model via maximum likelihood in general is NP-hard . However  as a
consequence of applying the screening rule  the blockwise structure of an (cid:96)1-regularized Ising model
can be determined as easily as the blockwise structure of a Gaussian graphical model  despite the
fact that within each block  exact learning of a sparse Ising model could still be challenging.
Furthermore  the screening rule also provides us a principal approach to leverage sparsity for the gain
of efﬁciency: by increasing λ  the nodes of the Ising model will be shattered into smaller and smaller
blocks  according to the screening rule. Solving many Ising models with small blocks of variables is
amenable to both estimation algorithm and parallelism.

3.2 Regularization Parameters

The screening rule also leads to a signiﬁcant implication to the range of regularization parameters in
which ˆθ (cid:54)= 0. Speciﬁcally  we have the following theorem.

Theorem 2. Let the dataset X =(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given  and let λ = λmax represent the

smallest regularization parameter such that ˆθ = 0 in (2). Then λmax = maxi j∈V i(cid:54)=j|EXXiXj| ≤ 1.
With λmax  one can decide the range of regularization parameters  [0  λmax]  that generates graphs
with nonempty edge sets  which is an important ﬁrst step for pathwise optimization algorithms
(a.k.a. homotopy algorithms) that learn the solutions to the problem under a range of λ’s. Furthermore 
the fact that λmax ≤ 1 for any given dataset X suggests that comparison across different networks
generated by different datasets is comprehensible. Finally  in Section 4  λmax will also help to
establish the connection between the screening rule for exact learning and some of the popular inexact
(alternative) learning algorithms in the literature.

3.3 Fully Disconnected Nodes

Another consequence of the screening rule is the necessary and sufﬁcient condition that determines
the regularization parameter with which a node is fully disconnected from the remaining nodes:

Corollary 1. Let the dataset X =(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given. Xi is fully disconnected from

the remaining nodes in ˆθ  where i ∈ V (i.e.  ˆθij = ˆθji = 0  ∀j ∈ V \ {i})  if and only if
λ ≥ maxj∈V \{i}|EXXiXj|.
In high-dimensional exploratory data analysis  it is usually the case that most of the variables are
fully disconnected [Danaher et al.  2014  Wan et al.  2016]. In this scenario  Corollary 1 provides a
regularization parameter threshold with which we can identify exactly the subset of fully disconnected
nodes. Since we can choose a threshold large enough to make any nodes fully disconnected  we can
discard a signiﬁcant portion of the variables efﬁciently and ﬂexibly at will with exact optimization
guarantees due to Corollary 1. By discarding the large portion of fully disconnected variables  the
learning algorithm can focus on only a moderate number of connected variables  which potentially
results in a substantial efﬁciency gain.

3.4 Blockwise Minimization

We conclude this section by providing the blockwise minimization algorithm in Algorithm 1 due
to the screening rule. Note that both the second empirical moments and the partition of V in the

4

algorithm can be computed in O(p2) operations [Witten et al.  2011  Mazumder and Hastie  2012].
On the contrary  the complexity of the exact optimization of a block of variables grows exponentially
with respect to the maximal clique size of that block. Therefore  by encouraging enough sparsity 
the blockwise minimization due to the screening rule can provide remarkable speedup by not only
shrinking the size of the blocks in general but also potentially reducing the size of cliques within each
block via eliminating enough edges.

4 Applications to Inexact (Alternative) Methods

ij

(cid:54)= ˆθNW

ij = ˆθPL
ji ).

We now discuss the interplay between the screening rule and two popular inexact (alternative)
estimation methods: node-wise (NW) logistic regression [Wainwright et al.  2006  Ravikumar et al. 
2010] and the pseudolikelihood (PL) method [Höﬂing and Tibshirani  2009]. In what follows  we
use ˆθNW and ˆθPL to denote the solutions given by the node-wise logistic regression method and the
pseudolikelihood method  respectively. NW can be considered as an asymmetric pseudolikelihood
method (i.e.  ∃i j ∈ V such that i (cid:54)= j and ˆθNW
ji )  while PL is a pseudolikelihood method that
is similar to NW but imposes additional symmetric constraints on the parameterization (i.e.  ∀i j ∈ V
where i (cid:54)= j  we have ˆθPL
Our incorporation of the screening rule to the inexact methods is straightforward: after using the
screening rule to identify different blocks in the solution  we use inexact methods to solve each block
for the solution. As shown in Section 3  when combined with exact optimization  the screening
rule is foolproof for blockwise structure identiﬁcation. However  in general  when combined with
inexact methods  the proposed screening rule is not foolproof any more because the screening rule is
derived from the exact problem in (2) instead of the approximate problems such as NW and PL. We
provide a toy example in A.6 to illustrate mistakes made by the screening rule when combined with
inexact methods. Nonetheless  as we will show in this section  NW and PL are deeply connected to
the screening rule  and when given a large enough regularization parameter  the application of the
screening rule to NW and PL can be lossless in practice (see Section 5). Therefore  when applied to
NW and PL  the proposed screening rule can be considered as a strong rule (i.e.  a rule that is not
foolproof but barely makes mistakes) and an optimal solution can be safeguarded by adjusting the
screened solution to optimality based on the KKT conditions of the inexact problem [Tibshirani et al. 
2012].

4.1 Node-wise (NW) Logistic Regression and the Pseudolikelihood (PL) Method
In NW  for each i ∈ V   we consider the conditional probability of Xi upon X\i  where X\i =
{Xt | t ∈ V \ {i}}. This is equivalent to solving p (cid:96)1-regularized logistic regression problems
separately  i.e.  ∀i ∈ V :

(cid:16)

(cid:16)

(cid:17)(cid:17)(cid:105)

+ λ(cid:13)(cid:13)θ\i

(cid:13)(cid:13)1  

i η(k)\i + log

1 + exp

η(k)\i

(5)

(cid:104)−y(k)

n(cid:88)

k=1

ˆθNW\i = arg min
θ\i

1
n

where η(k)\i = θ(cid:62)
unsuccessful event x(k)

\i(2x(k)\i )  y(k)

i = 1 represents a successful event x(k)

i = 1  y(k)

i = 0 represents an

i = −1  and
θi2

θ\i =(cid:2)θi1
(cid:104)

x(k)\i =

x(k)
i1

···

θi(i−1)

θi(i+1)

···

θip

x(k)
i2

···

x(k)
i(i−1) x(k)

i(i+1)

···

x(k)
ip

(cid:3)(cid:62)

 

(cid:105)(cid:62)

.

Note that ˆθNW constructed from ˆθNW\i ’s is asymmetric  and ad hoc post processing techniques are used
to generate a symmetric estimation such as setting each pair of elements from ˆθNW in symmetric
positions to the one with a larger (or smaller) absolute value.
On the other hand  PL can be considered as solving all p (cid:96)1-regularized logistic regression problems
in (5) jointly with symmetric constraints over the parameterization [Geng et al.  2017]:
(cid:107)θ(cid:107)1  

(cid:104)−y(k)

(cid:17)(cid:17)(cid:105)

n(cid:88)

p(cid:88)

i + log

i ξ(k)

1 + exp

ξ(k)
i

(cid:16)

(cid:16)

(6)

+

ˆθPL = arg min
θ∈Θ

1
n

λ
2

k=1

i=1

5

i =(cid:80)

where ξ(k)
.That is to say  if i < j  then θmin{i j} max{i j} = θij;
if i > j  then θmin{i j} max{i j} = θji. Recall that Θ in (6) deﬁned in Section 2.1 represents a space
of symmetric matrices whose diagonal entries are zeros.

j∈V \{i} 2θmin{i j} max{i j}x(k)

j

4.2 Regularization Parameters in NW and PL

Since the blockwise structure of a solution is given by the screening rule under a ﬁxed regularization
parameter  the ranges of regularization parameters under which NW and PL can return nonzero
solutions need to be linked to the range [0  λmax] in the exact problem. Theorem 3 and Theorem 4
establish such relationships for NW and PL  respectively.

Theorem 3. Let the dataset X =(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given  and let λ = λNW
Theorem 4. Let the dataset X =(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given  and let λ = λPL

smallest regularization parameter such that ˆθNW\i = 0 in (5)  ∀i ∈ V . Then λNW

max represent the

smallest regularization parameter such that ˆθPL = 0 in (6)  then λPL

max = 2λmax.

max = λmax.

max represent the

Let λ be the regularization parameter used in the exact problem. A strategy is to set the corresponding
λNW = λ when using NW and λPL = 2λ when using PL  based on the range of regularization param-
eters given in Theorem 3 and Theorem 4 for NW and PL. Since the magnitude of the regularization
parameter is suggestive of the magnitude of the gradient of the unregulated objective  the proposed
strategy leverages that the magnitudes of the gradients of the unregulated objectives for NW and
PL are roughly the same as  and roughly twice as large as  that of the unregulated exact objective 
respectively.
This observation has been made in the literature of binary pairwise Markov networks [Höﬂing and
Tibshirani  2009  Viallon et al.  2014]. Here  by Theorem 3 and Theorem 4  we demonstrate that
this relationship is exactly true if the optimal parameterization is zero. Höﬂing and Tibshirani 2009
even further exploits this observation in PL for exact optimization. Their procedure can be viewed as
iteratively solving adjusted PL problems regularized by λPL = 2λ in order to obtain an exact solution
regularized by λ. The close quantitative correspondence between the derivatives of the inexact
objectives and that of the exact objective also provides insights into why combing the screening rule
with inexact methods does not lose much in practice.

4.3 Preservation for Fully Disconnectedness

Theorem 5. Let the dataset X =(cid:8)x(1)  x(2) ···   x(n)(cid:9) be given. Let ˆθNW

While the screening rule is not foolproof when combined with NW and PL  it turns out that in terms
of identifying fully disconnected nodes  the necessary and sufﬁcient condition in Corollary 1 can be
preserved when applying NW with caution  as shown in the following.
min ∈ Θ denote a symmetric
matrix derived from ˆθNW by setting each pair of elements from ˆθNW in symmetric positions to the
one with a smaller absolute value. A sufﬁcient condition for Xi to be fully disconnected from the
min  where i ∈ V   is that λNW ≥ maxj∈V \{i}|EXXiXj|. Furthermore  when
remaining nodes in ˆθNW
ˆθNW\i = 0  the sufﬁcient condition is also necessary.
In practice  the utility of Theorem 5 is to provide us a lower bound for λ above which we can fully
disconnect Xi (sufﬁciency). Moreover  if ˆθNW\i = 0 also happens to be true  which is easily veriﬁable 
we can conclude that such a lower bound is tight (necessity).

5 Experiments

Experiments are conducted on both synthetic data and real world data. We will focus on efﬁciency in
Section 5.1 and discuss support recovery performance in Section 5.2. We consider three synthetic
networks (Table 2) with 20  35  and 50 blocks of 20-node  35-node  and 50-node subnetworks 
respectively. To demonstrate the estimation of networks with unbalanced-size subnetworks  we also
consider a 46-block network with power law degree distributed subnetworks of sizes ranging from 5
to 50. Within each network  the subnetwork is generated according to a power law degree distribution 
which mimics the structure of a biological network and is believed to be more challenging to recover

6

(a) Network 1

(b) Network 2

(c) Network 3

(d) Network 4

Figure 1: Runtime of pathwise optimization on networks in Table 2. Runtime plotted is the median
runtime over ﬁve trials. The experiments of the baseline method PL without screening can not be
fully conducted on larger networks due to high memory cost. NW: Node-wise logistic regression
without screening; NW+screen: Node-wise logistic regression with screening; PL: Pseudolikelihood
without screening; PL+screen: Pseudolikelihood with screening.

compared with other less complicated structures [Chen and Sharp  2004  Peng et al.  2009  Danaher
et al.  2014]. Each edge of each network is associated with a weight ﬁrst sampled from a standard
normal distribution  and then increased or decreased by 0.2 to further deviate from zero. For each
network  1600 samples are generated via Gibbs sampling within each subnetwork. Experiments on
exact optimization are reported in B.2.

5.1 Pathwise Optimization

of the(cid:0)p

2

Pathwise optimization aims to compute solutions over a range of different λ’s. Formally  we denote
the set of λ’s used in (2) as Λ = {λ1  λ2 ···   λτ}  and without loss of generality  we assume that
λ1 < λ2 < ··· < λτ .
The introduction of the screening rule provides us insightful heuristics for the determination of Λ.
We start by choosing a λ1 that reﬂects the sparse blockwise structural assumption on the data. To
achieve sparsity and avoid densely connected structures  we assume that the number of edges in the
ground truth network is O(p). This assumption coincides with networks generated according to a
power law degree distribution and hence is a faithful representation of the prior knowledge stemming
from many biological problems. As a heuristic  we relax and apply the screening rule in (4) on each

(cid:1) second empirical moments and choose λ1 such that the number of the absolute second

empirical moments that are greater than λ1 is about p log p. Given a λ1 chosen this way  one can
check how many blocks ˆθ(λ1) has by the screening rule. To encourage blockwise structures  we
magnify λ1 via λ1 ← 1.05λ1 until the current ˆθ(λ1) has more than one block. We then choose λτ
such that the number of absolute second empirical moments that are greater than λτ is about p. In
our experiments  we use an evenly spaced Λ with τ = 25.
To estimate the networks in Table 2  we implement both NW and PL with and without screening
using glmnet [Friedman et al.  2010] in R as a building block for logistic regression according to
Ravikumar et al. 2010 and Geng et al. 2017. To generate a symmetric parameterization for NW  we
set each pair of elements from θNW in symmetric positions to the element with a larger absolute value.
Given Λ  we screen only at λ1 to identify various blocks. Each block is then solved separately in a
pathwise fashion under Λ without further screening. The rationale of performing only one screening
is that starting from a λ1 chosen in the aforementioned way has provided us a sparse blockwise
structure that sets a signiﬁcant portion of the parameterization to zeros; further screening over larger
λ’s hence does not necessarily offer more efﬁciency gain.
Figure 1 summarizes the runtime of pathwise optimization on the four synthetic networks in Table 2.
The experiments are conducted on a PowerEdge R720 server with two Intel(R) Xeon(R) E5-2620
CPUs and 128GB RAM. As many as 24 threads can be run in parallel. For robustness  each runtime
reported is the median runtime over ﬁve trials. When the sample size is less than 1600  each trial
uses a subset of samples (subsamples) that are randomly drawn from the original datasets without
replacement. As illustrated in Figure 1  the efﬁciency gain due to the screening rule is self-evident.
Both NW and PL beneﬁt substantially from the application of the screening rule. The speedup is
more apparent with the increase of sample size as well as the increase of the dimension of the data. In
our experiments  we observe that even with arguably the state-of-the-art implementation [Geng et al. 

7

5010040080012001600Sample SizeRuntime (s)MethodsPLNWPL+screenNW+screen0250500750100040080012001600Sample SizeRuntime (s)MethodsPLNWPL+screenNW+screen2505007501000125040080012001600Sample SizeRuntime (s)MethodsNWPL+screenNW+screen05001000150040080012001600Sample SizeRuntime (s)MethodsPLNWPL+screenNW+screenindx

1
2
3
4

#blk
20
35
50
46

#nd/blk

20
35
50
5-50

TL#nd
400
1225
2500
1265

Table 2: Summary of the four syn-
thetic networks used in the experi-
ments. indx represents the index
of each network. #blk represents
the number of blocks each net-
work has. #nd/blk represents the
number of nodes each block has.
TL#nd represents the total number
of nodes each network has.

(a) Edge recovery AUC

(b) Model selection runtime

Figure 2: Model selection performance. Mix: provide PL
+screen with the regularization parameter chosen by the model
selection of NW+screen. Other legend labels are the same as
in Figure 1.

2017]  PL without screening still has a signiﬁcantly larger memory footprint compared with that of
NW. Therefore  the experiments for PL without screening are not fully conducted in Figure 1b 1c 
and 1d for networks with thousands of nodes. On the contrary  PL with the screening rule has a
comparable memory footprint with that of NW. Furthermore  as shown in Figure 1  after applying the
screening rule  PL also has a similar runtime with NW. This phenomenon demonstrates the utility of
the screening rule for effectively reducing the memory footprint of PL  making PL readily available
for large-scale problems.

5.2 Model Selection

Our next experiment performs model selection by choosing an appropriate λ from the regularization
parameter set Λ. We leverage the Stability Approach to Regularization Selection (StARS  Liu et al.
2010) for this task. In a nutshell  StARS learns a set of various models  denoted as M  over Λ using
many subsamples that are drawn randomly from the original dataset without replacement. It then
picks a λ∗ ∈ Λ that strikes the best balance between network sparsity and edge selection stability
among the models in M. After the determination of λ∗  it is used on the entire original dataset to
learn a model with which we compare the ground truth model and calculate its support recovery Area
Under Curve (AUC). Implementation details of model selection are provided in B.1.
In Figure 2  we summarize the experimental results of model selection  where 24 subsamples are used
for pathwise optimization in parallel to construct M. In Figure 2a  NW with and without screening
achieve the same high AUC values over all four networks  while the application of the screening
rule to NW provides roughly a 2x speedup  according to Figure 2b. The same AUC value shared by
the two variants of NW is due to the same λ∗ chosen by the model selection procedure. Even more
importantly  it is also because that under the same λ∗  the screening rule is able to perfectly identify
the blockwise structure of the parameterization.
Due to high memory cost  the model selection for PL without screening (green bars in Figure 2)
is omitted in some networks. To control the memory footprint  the model selection for PL with
screening (golden bars in Figure 2) also needs to be carried out meticulously by avoiding small λ’s
in Λ that correspond to dense structures in M during estimation from subsamples. While avoiding
dense structures makes PL with screening the fastest among all (Figure 2b)  it comes at the cost
of delivering the least accurate (though still reasonably effective) support recovery performance
(Figure 2a). To improve the accuracy of this approach  we also leverage the connection between
NW and PL by substituting 2λ∗
NW for the resultant regularization parameter from model selection
of PL  where λ∗
NW is the regularization parameter selected for NW. This strategy results in better
performance in support recovery (purple bars in Figure 2a).

5.3 Real World Data

Our real world data experiment applies NW with and without screening to a real world gene mutation
dataset collected from 178 lung squamous cell carcinoma samples [Weinstein et al.  2013]. Each
sample contains 13 665 binary variables representing the mutation statuses of various genes. For ease

8

0.000.250.500.751.001234Network IndexAUCMethodsPLNWPL+screenNW+screenMix03006009001234Network IndexRuntime (s)MethodsPLNWPL+screenNW+screenMixALPK2

UNC13C

KIAA1109

STAB2

FN1

PLXNA4

USP34

CDH9

DYNC1H1

ASTN2

FBN2

ADAMTS20

MYH4

BAI3

VCAN

SYNE2

WDR17

PTPRT

COL12A1

PDE4DIP

ELTD1

HRNR

TMEM132D

ZNF804A

NRXN1

VPS13B

RIMS2

FAT1

COL6A6

SCN1A

ROS1

TPR

MAGEC1

ZNF676

ANKRD30A

UNC5D

THSD7B

CNTNAP2

MYH1

C20orf26

Figure 3: Connected components learned from lung squamous cell carcinoma mutation data. Genes in
red are (lung) cancer and other disease related genes [Uhlén et al.  2015]. Mutation data are extracted
via the TCGA2STAT package [Wan et al.  2015] in R and the ﬁgure is rendered by Cytoscape.

of interpretation  we keep genes whose mutation rates are at least 10% across all samples  yielding
a subset of 145 genes in total. We use the model selection procedure introduced in Section 5.2 to
determine a λ∗
NW with which we learn the gene mutation network whose connected components are
shown in Figure 3. For model selection  other than the conﬁguration in B.1  we choose τ = 25. 384
trials are run in parallel using all 24 threads. We also choose λ1 such that about 2p log(p) absolute
second empirical moments are greater than λ1. We choose λτ such that about 0.25p absolute second
empirical moments are greater than λτ .
In our experiment  NW with and without screening select the same λ∗
NW  and generate the same
network. Since the dataset in question has a lower dimension and a smaller sample size compared with
the synthetic data  NW without screening is adequately efﬁcient. Nonetheless  with screening NW is
still roughly 20% faster. This phenomenon once again indicates that in practice the screening rule
can perfectly identify the blockwise sparsity pattern in the parameterization and deliver a signiﬁcant
efﬁciency gain. The genes in red in Figure 3 represent (lung) cancer and other disease related genes 
which are scattered across the seven subnetworks discovered by the algorithm. In our experiment  we
also notice that all the weights on the edges are positive. This is consistent with the biological belief
that associated genes tend to mutate together to cause cancer.

6 Generalization

 p(cid:88)

p−1(cid:88)

p(cid:88)

− 1
n

ˆθ = arg min

where (cid:107)θ(cid:107)1 off = (cid:80)p

With unary potentials  the (cid:96)1-regularized MLE for the Ising model is deﬁned as:

n(cid:88)
(cid:80)p
j(cid:54)=i|θij|. Note that the unary potentials are not penalized  which is a
common practice [Wainwright et al.  2006  Höﬂing and Tibshirani  2009  Ravikumar et al.  2010 
Viallon et al.  2014] to ensure a hierarchical parameterization. The screening rule here is to replace
(4) in Theorem 3 with:

(cid:107)θ(cid:107)1 off 

θijx(k)

i x(k)

θiix(k)

i +

(7)

k=1

i=1

i=1

j>i

θ

i=1

 + A(θ) +

j

λ
2

|EXXiXj − EXXiEXXj| ≤ λ.

(8)

Exhaustive justiﬁcation  interpretation  and experiments are provided in Supplement C.
7 Conclusion
We have proposed a screening rule for (cid:96)1-regularized Ising model estimation. The simple closed-form
screening rule is a necessary and sufﬁcient condition for exact blockwise structural identiﬁcation.
Experimental results suggest that the proposed screening rule can provide drastic speedups for
learning when combined with various optimization algorithms. Future directions include deriving
screening rules for more general undirected graphical models [Liu et al.  2012  2014b a  Liu  2014 
Liu et al.  2016]  and deriving screening rules for other inexact optimization algorithms [Liu and
Page  2013]. Further theoretical justiﬁcations regarding the conditions upon which the screening rule
can be combined with inexact algorithms to recover block structures losslessly are also desirable.
Acknowledgment: The authors would like to gratefully acknowledge the NIH BD2K Initiative grant
U54 AI117924 and the NIGMS grant 2RO1 GM097618.

9

References
O. Banerjee  L. E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivariate gaussian or binary data. Journal of Machine Learning Research  9
(Mar):485–516  2008.

R. F. Barber  M. Drton  et al. High-dimensional ising model selection with bayesian information

criteria. Electronic Journal of Statistics  9(1):567–607  2015.

H. Chen and B. M. Sharp. Content-rich biological network constructed by mining pubmed abstracts.

BMC Bioinformatics  5(1):147  2004.

P. Danaher  P. Wang  and D. M. Witten. The joint graphical lasso for inverse covariance estimation
across multiple classes. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
76(2):373–397  2014.

O. Fercoq  A. Gramfort  and J. Salmon. Mind the duality gap: safer rules for the lasso. In Proceedings

of The 32nd International Conference on Machine Learning  pages 333–342  2015.

J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical

lasso. Biostatistics  9(3):432–441  2008.

J. Friedman  T. Hastie  and R. Tibshirani. Regularization paths for generalized linear models via

coordinate descent. Journal of Statistical Software  33(1):1  2010.

S. Geng  Z. Kuang  and D. Page. An efﬁcient pseudo-likelihood method for sparse binary pairwise

Markov network estimation. arXiv Preprint  2017.

L. E. Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination for the lasso and sparse supervised

learning problems. arXiv Preprint  2010.

T. Hastie  R. Tibshirani  and M. Wainwright. Statistical learning with sparsity: the lasso and

generalizations. CRC Press  2015.

H. Höﬂing and R. Tibshirani. Estimation of sparse binary pairwise Markov networks using pseudo-

likelihoods. Journal of Machine Learning Research  10(Apr):883–906  2009.

J. Honorio and D. Samaras. Multi-task learning of gaussian graphical models. In Proceedings of the

27th International Conference on Machine Learning (ICML-10)  pages 447–454  2010.

C.-J. Hsieh  M. A. Sustik  I. S. Dhillon  P. K. Ravikumar  and R. Poldrack. Big & quic: Sparse inverse
covariance estimation for a million variables. In Advances in Neural Information Processing
Systems  pages 3165–3173  2013.

D. Karger and N. Srebro. Learning Markov networks: Maximum bounded tree-width graphs. In
Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms  pages 392–401.
Society for Industrial and Applied Mathematics  2001.

D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press 

2009.

S. Lee  N. Gornitz  E. P. Xing  D. Heckerman  and C. Lippert. Ensembles of lasso screening rules.

IEEE Transactions on Pattern Analysis and Machine Intelligence  2017.

H. Liu  K. Roeder  and L. Wasserman. Stability approach to regularization selection (stars) for high
dimensional graphical models. In Advances in Neural Information Processing Systems  pages
1432–1440  2010.

J. Liu. Statistical Methods for Genome-wide Association Studies and Personalized Medicine. PhD

thesis  The University of Wisconsin-Madison  2014.

J. Liu and D. Page. Structure learning of undirected graphical models with contrastive divergence.
ICML 2013 Workshop on Structured Learning: Inferring Graphs from Structured and Unstructured
Inputs  2013.

10

J. Liu  P. Peissig  C. Zhang  E. Burnside  C. McCarty  and D. Page. Graphical-model based multiple
testing under dependence  with applications to genome-wide association studies. In Uncertainty in
Artiﬁcial Intelligence  volume 2012  page 511. NIH Public Access  2012.

J. Liu  Z. Zhao  J. Wang  and J. Ye. Safe screening with variational inequalities and its application to

lasso. arXiv Preprint arXiv:1307.7577  2013.

J. Liu  C. Zhang  E. Burnside  and D. Page. Learning heterogeneous hidden Markov random ﬁelds.

In Artiﬁcial Intelligence and Statistics  pages 576–584  2014a.

J. Liu  C. Zhang  E. Burnside  and D. Page. Multiple testing under dependence via semiparametric
graphical models. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14)  pages 955–963  2014b.

J. Liu  C. Zhang  D. Page  et al. Multiple testing under dependence via graphical models. The Annals

of Applied Statistics  10(3):1699–1724  2016.

P.-L. Loh  M. J. Wainwright  et al. Structure estimation for discrete graphical models: Generalized
covariance matrices and their inverses. In Advances in Neural Information Processing Systems 
pages 2096–2104  2012.

P.-L. Loh  M. J. Wainwright  et al. Structure estimation for discrete graphical models: Generalized

covariance matrices and their inverses. The Annals of Statistics  41(6):3022–3049  2013.

S. Luo  R. Song  and D. Witten. Sure screening for gaussian graphical models. arXiv Preprint

arXiv:1407.7819  2014.

R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale

graphical lasso. Journal of Machine Learning Research  13(Mar):781–794  2012.

E. Ndiaye  O. Fercoq  A. Gramfort  and J. Salmon. Gap safe screening rules for sparse multi-task and
multi-class models. In Advances in Neural Information Processing Systems  pages 811–819  2015.

J. Pena and R. Tibshirani. Lecture notes in machine learning 10-725/statistics 36-725-convex

optimization (fall 2016)  2016.

J. Peng  P. Wang  N. Zhou  and J. Zhu. Partial correlation estimation by joint sparse regression

models. Journal of the American Statistical Association  104(486):735–746  2009.

P. Ravikumar  M. J. Wainwright  J. D. Lafferty  et al. High-dimensional ising model selection using

l1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 267–288  1996.

R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. J. Tibshirani. Strong rules
for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society: Series B
(Statistical Methodology)  74(2):245–266  2012.

M. Uhlén  L. Fagerberg  B. M. Hallström  C. Lindskog  P. Oksvold  A. Mardinoglu  Å. Sivertsson 
C. Kampf  E. Sjöstedt  A. Asplund  et al. Tissue-based map of the human proteome. Science  347
(6220):1260419  2015.

V. Viallon  O. Banerjee  E. Jougla  G. Rey  and J. Coste. Empirical comparison study of approximate
methods for structure selection in binary graphical models. Biometrical Journal  56(2):307–331 
2014.

M. Vuffray  S. Misra  A. Lokhov  and M. Chertkov. Interaction screening: Efﬁcient and sample-
optimal learning of ising models. In Advances in Neural Information Processing Systems  pages
2595–2603  2016.

M. J. Wainwright  J. D. Lafferty  and P. K. Ravikumar. High-dimensional graphical model selection
using l1-regularized logistic regression. In Advances in Neural Information Processing Systems 
pages 1465–1472  2006.

11

Y.-W. Wan  G. I. Allen  and Z. Liu. Tcga2stat: simple tcga data access for integrated statistical

analysis in r. Bioinformatics  page btv677  2015.

Y.-W. Wan  G. I. Allen  Y. Baker  E. Yang  P. Ravikumar  M. Anderson  and Z. Liu. Xmrf: an r
package to ﬁt Markov networks to high-throughput genetics data. BMC Systems Biology  10(3):69 
2016.

J. Wang  J. Zhou  P. Wonka  and J. Ye. Lasso screening rules via dual polytope projection. In

Advances in Neural Information Processing Systems  pages 1070–1078  2013.

J. Wang  J. Zhou  J. Liu  P. Wonka  and J. Ye. A safe screening rule for sparse logistic regression. In

Advances in Neural Information Processing Systems  pages 1053–1061  2014.

J. N. Weinstein  E. A. Collisson  G. B. Mills  K. R. M. Shaw  B. A. Ozenberger  K. Ellrott  I. Shmule-
vich  C. Sander  J. M. Stuart  C. G. A. R. Network  et al. The cancer genome atlas pan-cancer
analysis project. Nature Genetics  45(10):1113–1120  2013.

D. M. Witten  J. H. Friedman  and N. Simon. New insights and faster computations for the graphical

lasso. Journal of Computational and Graphical Statistics  20(4):892–900  2011.

Z. J. Xiang  Y. Wang  and P. J. Ramadge. Screening tests for lasso problems. IEEE Transactions on
Pattern Analysis and Machine Intelligence  PP(99):1–1  2016. ISSN 0162-8828. doi: 10.1109/
TPAMI.2016.2568185.

S. Yang  Z. Lu  X. Shen  P. Wonka  and J. Ye. Fused multiple graphical lasso. SIAM Journal on

Optimization  25(2):916–943  2015.

12

,Wouter Koolen
Oluwasanmi Koyejo
Rajiv Khanna
Joydeep Ghosh
Russell Poldrack
Zhaobin Kuang
Sinong Geng
David Page
Jianlong Chang
xinbang zhang
Yiwen Guo
GAOFENG MENG
SHIMING XIANG
Chunhong Pan