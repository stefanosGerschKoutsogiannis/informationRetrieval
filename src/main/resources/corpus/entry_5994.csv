2019,iSplit LBI: Individualized Partial Ranking with Ties via Split LBI,Due to the inherent uncertainty of data  the problem of predicting partial ranking from pairwise comparison data with ties has attracted increasing interest in recent years. However  in real-world scenarios  different individuals often hold distinct preferences  thus might be misleading to merely look at a global partial ranking while ignoring personal diversity. In this paper  instead of learning a global ranking which is agreed with the consensus  we pursue the tie-aware partial ranking  from an individualized perspective. Particularly  we formulate a unified framework which not only can be used for individualized partial ranking prediction  but can also be helpful for abnormal users selection. This is realized by a variable splitting-based algorithm called iSplit LBI. Specifically  our algorithm  generates a sequence of estimations with a regularization path  where both the hyperparameters and model parameters are updated. At each step of the path  the parameters can be decomposed into three orthogonal parts  namely  abnormal signals  personalized signals and random noise. The abnormal signals can serve the purpose of abnormal user selection  while the  abnormal signals and personalized signals 
together are mainly responsible for user partial ranking prediction. Extensive experiments on simulated and real-world datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives.,iSplit LBI: Individualized Partial Ranking

with Ties via Split LBI

Qianqian Xu1
Xiaochun Cao3 4 7 Qingming Huang1 5 6 7 Yuan Yao8

Xinwei Sun2

Zhiyong Yang3 4

1Key Lab. of Intelligent Information Processing  Institute of Computing Technology  CAS

2Microsoft Research Asia

3State Key Laboratory of Information Security  Institute of Information Engineering  CAS

4School of Cyber Security  University of Chinese Academy of Sciences

5School of Computer Science and Tech.  University of Chinese Academy of Sciences

6Key Laboratory of Big Data Mining and Knowledge Management  CAS

7Peng Cheng Laboratory

8Department of Mathematics  Hong Kong University of Science and Technology

xuqianqian@ict.ac.cn  xinsun@microsoft.com  yangzhiyong@iie.ac.cn

caoxiaochun@iie.ac.cn  qmhuang@ucas.ac.cn  yuany@ust.hk

Abstract

Due to the inherent uncertainty of data  the problem of predicting partial ranking
from pairwise comparison data with ties has attracted increasing interest in recent
years. However  in real-world scenarios  different individuals often hold distinct
preferences.
It might be misleading to merely look at a global partial ranking
while ignoring personal diversity. In this paper  instead of learning a global rank-
ing which is agreed with the consensus  we pursue the tie-aware partial ranking
from an individualized perspective. Particularly  we formulate a uniﬁed frame-
work which not only can be used for individualized partial ranking prediction  but
also be helpful for abnormal user selection. This is realized by a variable splitting-
based algorithm called iSplitLBI. Speciﬁcally  our algorithm generates a se-
quence of estimations with a regularization path  where both the hyperparameters
and model parameters are updated. At each step of the path  the parameters can be
decomposed into three orthogonal parts  namely  abnormal signals  personalized
signals and random noise. The abnormal signals can serve the purpose of abnor-
mal user selection  while the abnormal signals and personalized signals together
are mainly responsible for individual partial ranking prediction. Extensive exper-
iments on simulated and real-world datasets demonstrate that our new approach
signiﬁcantly outperforms state-of-the-art alternatives.

1

Introduction

The ﬂourish of various online crowdsourcing services (e.g.  Amazon Mechanical Turk)  presents us
an effective way to distribute tasks to human workers around the world  on-demand and at scale.
Recently  there arises a plethora of pairwise comparison data in crowdsourcing experiments on the
Internet [16  2]  ranging from marketing and advertisements to competitions and election. Infor-
mation of this kind is all around us: which college a student selected  who won the chess match 
which movie a user watched  etc. How to aggregate the massive amount of personalized pairwise
comparison data to reveal the global preference function has been one important topic in the last
decades [4  11  26  20  2  22].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

But is the aggregated result necessarily more important than individual opinions? This is not al-
ways the case especially when our Internet is ﬂooded with personalized information in diversity.
The disagreement over the crowd could not be simply interpreted as a random perturbation of a
consensus that everybody should follow. For example  we often observe quite different preferences
on a college ranking or a favorite movie list. Hence the wave of personalized ranking arises in
recent years in search of better individualized models. One line of the related research assumes
that the ranking function is determined by a small number of underlying intrinsic functions such
that every individual’s personalized preference is a linear combination of these intrinsic functions
[30  19  12  6]. Another line of research attributes the personalized bias to user quality  where either
a single parameter or a general confusion matrix is adopted to model the users’ ability to provide a
correct label [7  13  15  23  25  33]. There is also a trend to explore personalized ranking effects in
terms of preference distributions [18  17]. Moreover  [28  27] take a wide spectrum by considering
both the social preference and individual variations simultaneously. Speciﬁcally  it designs a basic
linear mixed-effect model which not only can derive the common preference on population-level 
but also can estimate user’s preference/utility deviation in an individual-level.

Figure 1: An example of pairwise ranking
with ties.

All the work mentioned above either focuses on
instance-wise preference learning or assumes that
the candidates are comparable in a total order. For
pairwise preference learning  however  the answer
might go beyond a win/loss option in real-world sce-
narios. The following gives an example in crowd-
sourced college ranking.
Example.
In world college ranking with crowd-
sourcing platforms such as Allourideas  a partici-
pant is asked about “which university (of the follow-
ing two) would you rather attend?”. As is shown in
Fig.1  let G = (V  E) be a pairwise ranking graph
whose vertex set is V   the set of universities to be
ranked  and the edge set is E  the set of university pairs which receive some comparisons from
users. Here different colors indicate different users. If a voter thinks college V3 is better than col-
lege V6  a solid arrowed line from V3 to V6 occurs (i.e.  superiority). However  when a voter thinks
the two colleges (i.e.  V1 and V3) listed are incomparable and difﬁcult to judge  he may click the
button “I can’t decide”  then a dotted line connecting V1 and V3 happens (i.e.  tie).
Here for a pair (i  j)  if a voter believes i and j share a similar strength and neither one is superior to
the other  he may abstain from this decision and leave it with a tie. An abstention of this kind is an
obvious means to avoid unreliable predictions. Such kind of pairwise comparison data  together with
“I cannot decide” decision  provide us information about possible ties or equivalent classes of items
in partial orders. Though there is some work in the literature studying how to organize information
in partial orders of such tied subsets or equivalent classes (partitions  bucket orders) [5  14]  little
has been done on learning the individualized partial order models from such pairwise comparison
data with ties.
In this paper  we aim to learn the individualized partial ranking models for each user based on
such kind of pairwise ranking graph with ties. Based on the partial ranking  we could recommend
universities for a speciﬁc user. For example  recommending universities that are with the same
quality as college A; or  recommending universities that are slightly better than college B  etc.
Moreover  another challenge of personalized preference ranking comes from the fact that abnormal
users might exist in the crowd. They either bear an extremely different pattern with the majority of
the crowd or belong to malicious users trying to attack the learning system. To deal with abnormal
user detection in crowdsourced data  existing studies often take a majority voting strategy  which
often ignores the personalized effect.
Seeing the issues mentioned above  we propose a uniﬁed framework  called iSplitLBI  for per-
sonalized partial ranking  tie state recognition  and abnormal user detection. The merits of our
framework are of three-fold: 1) It decomposes the parameters into three orthogonal parts  namely 
abnormal signals  personalized signals  and random noise. The abnormal signals can serve the pur-
pose of abnormal user detection  while the abnormal signals and personalized signals together are
mainly responsible for user partial ranking prediction. 2) It provides a compatible framework be-

2

tween predict individual preferences (i.e.  model prediction) and identiﬁcation of abnormal users
(i.e.  model selection) by virtue of variable splitting scheme. 3) Exploiting the regularization path  it
simultaneously searches hyper-parameters and model parameters. Up to our knowledge  this is the
ﬁrst proposal of such a model in the literature on partial ranking.

2 Methodology

In crowdsourced pairwise comparison experiments  suppose there are n alternatives or items to be
ranked. Traditionally  the pairwise comparison labels collected from users can be naturally repre-
sented as a directed comparison graph G = (V  E). Let V = {1  2  . . .   n} be the vertex set of n
items and E = {(u  i  j) : i  j ∈ V  u ∈ U} be the set of edges  where U is the set of all users
who compared items. User u provides his/her preference between choice i and j  such that yu
ij = 1
means u prefers i to j and yu
However  in real-world applications  ties are ubiquitous. In this case  if a rater thinks neither of the
two items in a pair is superior to the other  he/she may abstain from this decision and instead declare
a tie  as is shown with the red dotted line in Fig.1. This inspires us to adopt a win/tie/lose user
feedback in the following sense:

ij = −1 otherwise.

(cid:40) 1 

−1 
0 

yu
ij =

if u prefers i to j 
if u prefers j to i 

otherwise.

(1)

Given the deﬁnition of the user feedback  in the rest of this section  we elaborate our proposed model
in the following order. First we propose a probability model to describe the generation process of
the comparison results yu
ij. Then we present a simple iterative algorithm called individualized Split
Linearized Bregman Iterations (i.e.  iSplitLBI) for individualized partial ranking. In the end 
we provide a decomposition property of iSplitLBI which dives deeper into the insights of our
proposed model.

2.1 Probabilistic Model of Partial Ranking with Ties

Now we describe our dataset at hand with the following notations. Suppose that we have U users
and for a speciﬁc user u  he/she annotates nu pairwise comparisons. For a speciﬁc comparison
(i  j)  the user provides a label yu
ij correspondingly following (1). We denote the set of all pairwise
comparisons available for user u as Ou  and deﬁne the label set Y u as:

Y u =(cid:8)yu

ij : (i  j) ∈ Ou(cid:9)

Then our dataset could be expressed as {Ou Y u}U
u=1. We assume that each user has a personalized
] ∀u 
score list for all items. We denote such true personalized score lists as su = [su
where nui is the number of items that are available for u. Furthermore  for any speciﬁc u  λu is a
personalized threshold value to be learned for decision. Then  for a speciﬁc user u  and a speciﬁc
observation (i  j)  we assume that yu
j with
the threshold λu. Meanwhile  to model the randomness of the sampling and the decision making
process  we model the uncertainty of su
ij which has a
c.d.f Φ(t). Then  in our model  user u would choose yu
ij = 1  if the observed personalized score
difference su
ij is smaller
i − su
than −λu  then user u would choose yu
ij = −1. Otherwise  su
ij has a smaller magnitude
than λu  in which case the user would claim a tie. Above all  yu
ij is obtained from the following rule:

j with an associated random noise u
i − su

ij is greater than the threshold λu. To the opposite  if su

ij is produced by comparing the score difference su

1  ···   su

i − su

i − su

j + u

i − su

j + u

j + u

nui

(2)

(3)

(4)

Furthermore  we deﬁne two variables ζ u+

yu
ij =

j + u
j + u

ij > λu;
ij ≤ −λu;

 1 

i − su
su
−1 
i − su
su
else.
0 
and ζ u−

k

as :
k
ij = λu − su
ζ u+
ij = −λu − su
ζ u−

i + su
j
i + su
j

3

ij is a random variable with a c.d.f Φ  we could then derive the probability to observe yu

Since u
(cid:1)
1  0 −1  respectively. Speciﬁcally  together with (3) and (4) we have:
ij ) − Φ(ζ u−
ij } = Φ(ζ u+
ij )

ij } = 1 − Φ(cid:0)ζ u+

P{yu
P{yu
P{yu

ij = 1} = P{u
ij = 0} = P{ζ u−
ij = −1} = P{u

ij > ζ u+
ij ≤ u
ij ≤ ζ u−

ij < ζ u+
ij } = Φ(ζ u−
ij ).

ij

ij =

Note that different Φ could lead to different models. In this paper  we simply consider the most
widely adopted Bradley-Terry model: Φ(t) = et
1+et   while leaving other models for future studies.

2.2

Individualized Split LBI

In our framework  we assume the majority of participants share a common preference interest and
behave rationally  while deviations from that exist but are sparse. To be speciﬁc  we consider the
following linear model for annotator’s individualized partial ranking:
ij = (csi + pu
si

s   λu = cλ + pu
λ 

) − (csj + pu

su = cs + pu

i − su
su

) + u
ij.

j + u

(5)

sj

s and pu

si and pu

λ represent the individualized bias pattern  in which pu

To make the notation clear  let P u
Φ(ζ u−

where (1) cs and cλ represent the consensus level pattern  in which cs is the common global ranking
score  cλ is the common λ  as a ﬁxed effect  csi and csj are the ith and jth element of cs  respectively;
(2) pu
s is the annotator’s preference
λ is the individualized bias with cλ  as a random
deviation from the common ranking score cs  pu
s   respectively; (3) u
sj are the ith and jth element of pu
effect  pu
1 ij = 1 − Φ(ζ u+
0 ij = Φ(ζ u+
ij )  P u
(cid:105)1{yu
ij =1}(cid:104)
(cid:105)1{yu
ij =0}(cid:104)
ij} as:
ij )  then we could represent P{yu
L(Ou Y u|su  λu) = − (cid:88)

ij is the random noise.
ij ) − Φ(ζ u−
(cid:105)1{yu

Given all above  for a speciﬁc user u  it is easy to write out the negative log-likelihood:

ij ) and P u−1 ij =

ij} =

P{yu

ij =−1}

P u−1 ij

(cid:104)

P u

P u

1 ij

0 ij

.

(i j)∈Ou

ij}.

log P{yu
λ  λu ≥ δ  cλ ≥ δ.

s.t. su = cs + pu

s   λu = cλ + pu

(6)

(7)

In the constraints we use λu ≥ δ  cλ ≥ δ  where δ > 0  as closed and convex approximations of the
positivity constraints λu > 0  cλ > 0. The beneﬁt to employ the relaxations are two-fold: 1) The
closed domain constraints induce closed-form solution; 2) The threshold δ improves the quality of
the solution to avoid ill-conditioned cases being too close to zero.
Obviously  the personalized bias could not grow arbitrarily large. More reasonably  only highly
personalized users have a signiﬁcant bias pu
λ  while the majority of the mass tends to have
smaller or even zero biases. If we denote P s = {pu
λ : u =
1 ···   U}   this means that (P s  P λ) satisﬁes group sparsity  then we add a group lasso penalty to
the loss function Jµ(P s  P λ)  which is in the form:

s : u = 1 ···   U} and P λ = {pu

s and pu

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)pu

s
pu
λ

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)   µ > 0 

(cid:88)

u

Jµ(P s  P λ) = µ

s and pu

where µ is a regularization parameter. Such a structural penalty (7) can identify abnormal users u
whose pu
λ are nonzero. These non-zero terms increase the penalty function. However the
corresponding reduction of loss function L(Ou Y u|su  λu) must dominate the increasing penalty
so as to minimize the overall objective function. In this sense  the abnormal users capture the strong
signals for individualized biases. However  it ignores the possibility that weak signals could also
induce individualized biases. Such signals help to decrease the loss  but the reduction of loss is not
strong enough to cover the penalty term. This motivates us to propose a variable splitting scheme to
simultaneously embrace strong and weak patterns. Speciﬁcally  we model the overall signal (pu
s   pu
λ)
as the sum of the strong signals (Γu
λ). The
group lasso penalty is exhibited on the strong signals. Moreover  we give the weak signals an
(cid:96)2 penalty in the form: Sν(Γ  P ) = 1

  to avoid it being arbitrarily large.

λ) and weak signals (∆u

λ) − (Γu

(cid:20)pu

λ) = (pu

(cid:80)

s   ∆u

s   Γu

s   Γu

s   pu

(cid:21)

−

2ν

u

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

s
pu
λ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)Γu

s
Γu
λ

4

Denote the parameter set as Θ = {P s  P λ  Γs  Γλ  cs  cλ}. Deﬁne Γs = {Γu
Γλ = {Γu

λ : u = 1 ···   U}  the loss function is deﬁned as:

s : u = 1 ···   U} and

min

L(Ou Y u|su  λu) + Sν(Γ  P ) + Jµ(Γs  Γλ)

(cid:88)

u

Θ
s.t. su = cs + Γu
λu ≥ δ  cλ ≥ δ.

s   λu = cλ + Γu
λ 

Instead of directly solving the above-mentioned problem  we adopt the Split Linearized Bregman
Iterations which we call individualized Split LBI (iSplitLBI)  which gives rise to a regularization
path where both the model parameters and hyper-parameters are simultaneously evolved. The (k +
1)-th iteration on such a path is given as:

(8)

(9a)

(9b)

(9c)

s
cu +1
λ

(cid:18) cu k+1
(cid:18) P k+1
(cid:18) Z k+1
(cid:18) Γk+1

P k+1

Z k+1

λ

λ

s

s

s

(cid:19)
(cid:19)
(cid:19)
(cid:19)

= ProxJc

= ProxJP

(cid:18) Z k

s
Z k
λ

=

(cid:19)
(cid:19)

− καk∇cL(Θk)

 ∀u ∈ U

− καk∇P L(Θk)

 

s
P k
λ

s
cu k
λ

(cid:18)(cid:18) cu k
(cid:19)
(cid:18)(cid:18) P k
(cid:19)
(cid:19)
(cid:18)(cid:18) Z k
(cid:19)(cid:19)
s = 0 ∈ Rp  P 0

− αk∇ΓL(Θk) 

s
Z k
λ

λ

k

s   ck

λ  P k

s   P k

s   ˜P

  µ = 1 

s = Z0

λ = Z0

s = 0 ∈ RU×p  P 0

λ) and sparse estimators ( ˜P

time at τk = (cid:80)k

= κ · ProxJµ
(9d)
Γk+1
λ = 0 ∈
λ = 1 ∈ R1  cu 0
where the initial choice cu 0
RU   parameters κ > 0  α > 0  ν > 0  and the proximal map associated with a convex function h
is deﬁned by Proxh(z) = arg minx (cid:107)z − x(cid:107)2/2 + h(x). The Jc(cλ) and JP (P λ) are denoted as
the indicator function for the set cλ ≥ δ and λu ≥ δ respectively (an indicator function of a set is
0 when the input variable is in the set  otherwise it is +∞). Hence  at each step  the ﬁrst two steps
give a projected gradient descent of cλ and P λ  which makes the variables feasible.
The iSplitLBI algorithm generates a regularized solution path of dense estimators
k
λ). These sparse estimators could be obtained by
(ck
projecting (P s  P λ) onto the support set of (Γs  Γλ)  respectively. Along the path  the stopping
i=1 αi in this algorithm plays the same role as the regularization parameter in the
lasso problem. In fact  Eq.(9a)-(9d) describes one iteration of the optimization process  which is
actually a discretization of a dynamical system shown in [10]. Such a dynamical system is known
as inverse scale spaces [1  21  9]  leveraging a regularization path consisting of sparse models at
different levels from the null to the full. At iteration k  the cumulative time τk can be regarded as
the inverse of the Lasso regularization parameter (here roughly τk ∼ 1/µ): the larger is τk  the
smaller is the regularization and hence the more nonzero parameters enter the model. Following
the dynamics  the model gradually grows from sparse to dense models with increasing complexity.
In particular as τk → ∞  the dynamics may reach some over-ﬁtting models when noise exists like
our case  equivalent to a full model in generalized Lasso of minimal regularization. To prevent such
over-ﬁtting models in noisy applications  we adopt an early stopping strategy to ﬁnd an optimal
stopping time by cross validation.
Moreover  the ν also plays an important role in the model. When ν → 0  only sparse strong signals
(features) are kept in models  then the iSplitLBI reduces to LBI algorithm  which is shown to
reach model selection consistency under nearly the same condition as LASSO for linear models [21].
Recently  it is shown in [10] that the model selection consistency can also hold even under non-linear
models. With a ﬁnite value of ν  it is shown in [8  9] that the sparse estimator enjoys improved model
selection consistency. Moreover  equipped with the variable splitting scheme  the ﬁnite value of ν
enables the overall signals (here P ) to capture features ignored by the strong (sparse) signals. It has
been shown in the literature (e.g. [24  32])  which coincides with our discussion  that such kinds of
features can improve prediction in various tasks. Now we note the following implementation details
for iSplitLBI. The hyper-parameter κ is a damping factor which determines the bias of the sparse
estimators  a bigger κ leading to less biased estimators (bias-free as κ → ∞). The hyper-parameter
αk is the step size which determines the precise of the path  with a large αk rapidly traversing a
coarse-grained path. However one has to keep αkκ small to avoid possible oscillations of the paths 
e.g. αkκ ≤
as a tradeoff between
performance and computation cost.

. The default choice in this paper is

κ(cid:107)∇2L(Θk)(cid:107)2

(cid:107)∇2L(Θk)(cid:107)2

2

1

2

2

5

2.3 Decomposition Property of iSplit LBI

By virtue of the variable splitting term 
the dense parameter P enjoys a speciﬁc
orthogonal decomposition property  as is
shown in Fig.2:

P = P abn ⊕ P per ⊕ P ran.

Figure 2: Decomposition of personalized parameters.

(1) P abn is simply ˜P   i.e.  the projection
of P on the support set of Γ.
In other
(cid:54)= 0  and
words  Pabnij = Pij if Γij
Pabnij = 0 otherwise. Users corresponding to the non-zero columns of P abn have signiﬁcant
biases toward the popular scores cs and the common threshold cλ. Thus the structure of P abn could
tell us who is an abnormal user in the crowd. In this sense  we refer to P abn as the abnormal sig-
nal. This corresponds to the strong signals in the last subsection. (2) Among the remainder of such
projection  P per stands for the elements having a signiﬁcant magnitude than random noise. This
component drives the dense parameter P further away from the sparse parameter ˜P . According to
the discussion in the previous subsection  this component takes into consideration the weak signals
that help to further reduce the loss function. In this sense  including P per brings better performance
to P . (3) The remaining entries in P are referred to as P ran  i.e.  the random noises  which are
inevitable due to the randomness of the data.
With all above  we present a compatible framework for both model prediction and model selection:
(1) The strong signal (P s  P λ) contains all the personalized biases which is a better choice for
model prediction; (2) (Γs  Γλ) and P abn exclude the weak and dense personalized signals in the
overall signals  which makes it a natural choice of abnormal user identiﬁcation using model selec-
tion. This motivates us to take advantage of the support set of ˜P to detect abnormal users  while
utilizing P for prediction.

3 Experiments

s is nonzero  we generate pu

3.1 Simulated Study
Settings. We validate our algorithm on simulated data with n = |V | = 20 items and U = 50
annotators. We ﬁrst generate the true common ranking scores cs ∼ N (0  52). Then each annotator
has a probability p1 = 0.2 to have a nonzero pu
s . Those nonzero pu
s s are drawn randomly from
λ ∼ cλ ∗ U(−0.5  0.5)  otherwise we simply set
N (0  52). If pu
λ as pu
λ = 0  where cλ = 1.5. At last  we draw N u samples for each user randomly following the
pu
Bradley-Terry model. The sample number N u uniformly spans [N1  N2] = [200  400]. Finally  we
obtain a multi-edge graph with ties annotated by 50 annotators.
Abnormal User Detection. In this part  we validate abnormal user detection ability of iSplitLBI
with visualization analysis. As we have stated  the support set of P (or ˜P equivalently) implies
the abnormal users. In this sense  we visualize the ˜P (the ground-truth parameters) and ˜P 0 (the
| ˜P|) and
estimated parameters) in Fig.4 (a)-(b)  whereas we visualize the magnitude of ˜P (i.e.
| ˜P 0|) in Fig.4 (c)-(d). Although the magnitude of ˜P 0 tends to be smaller than the true
˜P 0 (i.e.
parameter  the results in Fig.4 (a)-(b) clearly suggest a perfect detection of the abnormal users.
Furthermore  Fig.5 shows the L2-distance between each user’s individualized ranking (i.e.  su) and
the common ranking (i.e.  cs)  (cid:107)su − cs(cid:107). Clearly one can see the abnormal users we detected all
exhibit larger L2-distance with the common ranking compared with other users. This indicates that
these 13 abnormal users detected are those with large deviations from the population’s opinion.
Prediction Ability. After showing the successful detection of abnormal users  in the following  we
will exhibit the prediction ability of the proposed iSplit LBI method.
(1) Evaluation metrics: We measure the experimental results via two evaluation criteria  i.e.  Macro-
F1  and Micro-F1 over the three classes -1 0 1  which take both precision and recall into account.
Note that the larger the value of Micro-F1 and Macro-F1  the better the performance. For more
details  please refer to [31].

6

(a) Supp( ˜P )

(b) Supp( ˜P 0)

(c) Su ˜P Su

(d) Su ˜P 0Su

Figure 4: Visualization of the parameters.

Figure 5: Detected abnormal users.

(2) Competitors: We employ two competitors that share most of the problem settings with
iSplitLBI. i) the α-cut algorithm [3] is an early trial of common partial ranking. Since
α-cut is an ensemble-based algorithm  its performance depends on the choice of weak learners.
Consequently  we compare our proposed algorithm with the α-cut algorithm where different types
of such weak learners and regularization schemes are adopted. Regarding the parameter-tuning of
the weak learners in α-cut  we tune the coefﬁcients for Ridge/LASSO regularization from the range
{2−15  2−13 ···   2−5} and the best parameters are picked out through a 5-fold cross-validation
on the training set.
ii) a most recently developed margin-based MLE method [29] where
Uniform  Bradley-Terry  and Thurstone-Mosteller models are considered  re-
spectively.
(3) Qualitative Results: Tab.1 shows the
corresponding performance of our pro-
posed algorithms and the competitors. In
this table  the second column shows the
weak learners and regularization terms
employed in α-cut and three mod-
els proposed in MLE-based algorithm.
Speciﬁcally  LR represents for logistics re-
gression  SVM stands for the Support Vec-
tor Machine method  LS stands for the
method of least squares while SVR stands
for the Support Vector Regression method.
For regularization  we employ the Ridge
and LASSO regularization terms. Here we
split the data into a training set (80% of
each user’s pairwise comparisons) and a
testing set (the remaining 20%). To ensure the statistical stability  we repeat this procedure 20 times.
It is easy to see that iSplit LBI signiﬁcantly outperforms the other two competitors with an average
of 0.834 ± 0.005 in Micro-F1 and 0.761 ± 0.007 in Macro-F1 due to its individualized property.

algorithms min median max std
LRLasso .216 .345 .365 .033
LRRidge .319 .347 .380 .018
SVMlasso .318 .340 .367 .012
SVMRidge .294 .349 .367 .016
LSLasso .306 .344 .368 .016
LSRidge .320 .346 .368 .013
SVRlasso .329 .347 .377 .013
SVRRidge .312 .336 .378 .019
.599 .622 .660 .014
.772 .801 .839 .015
.767 .799 .820 .016
.825 .834 .841 .005

minmedian max std
.238 .323 .364 .036
.210 .338 .392 .061
.216 .305 .401 .051
.198 .334 .429 .077
.206 .347 .413 .044
.222 .325 .410 .051
.221 .357 .448 .057
.220 .346 .436 .050
.588 .605 .631 .011
.628 .660 .679 .015
.608 .637 .669 .016
.747 .761 .774 .007

Table 1: Experimental results on simulated dataset.

(a) Micro-F1

(b) Macro-F1

α-cut

types

MLE

Un
BT
TM

Ours

Ours

3.2 Human Age

Dataset. In this dataset  25 images from human age dataset FG-NET 1 are annotated by a group
of volunteers on ChinaCrowds platform. The annotator is presented with two images and given
a choice of which one is older (or difﬁcult to judge). Totally  we obtain 9589 feedbacks from 91
annotators.
Qualitative Results. Tab.2 shows the corresponding performance of our proposed algorithms and
the competitors. We can easily ﬁnd that our proposed algorithm signiﬁcantly outperforms the other
two competitors in terms of both Micro-F1 and Macro-F1. Moreover  Fig.6 (a) shows the L2-
distance between selected users’ (i.e.  the top 10% and bottom 10% in the regularization path) indi-
vidualized ranking and the common ranking. Clearly one can see that users jumped out earlier (i.e. 

1http://www.fgnet.rsunit.com/

7

the top 10% marked with pink) show larger L2-distance  thus are those with large deviation from
the population’s opinion and can be treated as abnormal users. On the contrary  users jumped out
later (i.e.  the bottom 10% marked with blue) tend to have smaller or even zero L2-distance.

3.3 WorldCollege Ranking

types

α-cut

(a) Micro-F1

(b) Macro-F1

MLE

Ours

Uni.
BT
TM

Ours

Table 2: Experimental results on Human Age dataset.

min median max std
.327 .358 .381 .016
.330 .364 .387 .015
.331 .355 .371 .013
.330 .351 .379 .014
.335 .361 .383 .014
.335 .365 .398 .014
.333 .364 .397 .014
.337 .367 .381 .012
.589 .606 .641 .012
.599 .628 .647 .012
.603 .623 .647 .012
.680 .694 .712 .010

algorithms min median max std
LRLasso .428 .443 .458 .008
LRRidge .422 .443 .457 .008
SVMlasso .422 .442 .463 .010
SVMRidge .424 .443 .457 .008
LSLasso .423 .441 .455 .008
LSRidge .426 .442 .464 .009
SVRlasso .418 .431 .449 .008
SVRRidge .422 .432 .450 .007
.692 .705 .738 .012
.731 .741 .755 .008
.728 .739 .756 .008
.765 .779 .791 .007

Dataset. We now apply the proposed
method to the world college ranking
dataset  which is composed of 261 col-
leges. Using the Allourideas crowdsourc-
ing platform  a total of 340 random an-
notators with different backgrounds from
various countries (e.g.  USA  Canada 
Spain  France  Japan  China  etc.)
are
shown randomly with pairs of these col-
leges and asked to decide which of the
two universities is more attractive to at-
tend.
If the voter thinks the two col-
leges are incomparable  he/she can choose
the third option by clicking “I cannot de-
cide”. Finally  we obtain a total of 11012
feedbacks  among which 9409 samples are
pairwise comparisons with clear opinions (i.e.  1/-1) and the remaining 1603 are samples records
with voter clicking “I cannot decide” (i.e.  0).
Qualitative Results.
Tab.3 shows the
comparable results on the college dataset.
It is easy to see that our proposed algo-
rithm again achieves better Micro-F1 and
Macro-F1 with a large margin than all the
α-cut and MLE-based variants. To in-
vestigate the reason behind this  we further
compare our proposed algorithm with the
MLE-based algorithms in terms of ﬁne-
grained precision  recall performances on
label {−1  0  1} in Fig.6 (c). For labels
-1 and 1  the performance improvement
is relatively small  whereas a sharp im-
provement is highlighted for label 0. This
suggests that the major contribution of the
overall improvements of our proposed al-
gorithm comes from its strength to recognize the incomparable pairs  which is exactly the main
pursuit of this paper. Moreover  similar to the human age dataset  we also plot the L2 distance
between the top/bottom 10% users’ individualized ranking and the common ranking and similar
phenomenon occurs on this dataset  as is shown in Fig.6 (b). Again  we see a signiﬁcant difference
between the recognized most individualized rankers and the least individualized rankers.

min median max std
.328 .349 .367 .011
.323 .348 .364 .010
.333 .345 .371 .009
.327 .345 .365 .010
.331 .342 .362 .010
.334 .345 .365 .009
.327 .346 .363 .010
.323 .350 .361 .012
.482 .496 .514 .009
.496 .513 .526 .010
.496 .511 .526 .010
.608 .645 .674 .016

algorithms min median max std
LRLasso .318 .350 .408 .026
LRRidge .325 .352 .408 .023
SVMlasso .325 .343 .404 .029
SVMRidge .327 .354 .402 .025
LSLasso .305 .320 .377 .016
LSRidge .331 .345 .403 .023
SVRlasso .306 .328 .378 .018
SVRRidge .323 .348 .402 .023
.521 .536 .550 .009
.539 .552 .565 .008
.539 .551 .565 .008
.637 .649 .663 .008

Uni.
BT
TM

Ours

Table 3: Experimental results on College dataset.

(b) Macro-F1

(a) Micro-F1

types

α-cut

MLE

Ours

4 Conclusions

In this paper  we propose a novel method called iSplitLBI which is capable of simultaneously
predicting personalized rankings with ties and detecting the abnormal users in the crowd. To tackle
the personalized deviations of the scores  a hierarchical decomposition of the model parameters is
designed where both the popular opinions and the individualized effects are taken into consideration.
In what follows  a speciﬁc variable splitting scheme is adopted to separate the functionality of model
prediction and abnormal user detection. Experiments on both simulated examples and real-world
applications together demonstrate the effectiveness of the proposed method.

8

(a) Age dataset

(b) College dataset

(c) Fine-grained comparison

Figure 6: (a)-(b) The L2 distance between individualized ranking scores and common ranking scores
of selected users on age and college datasets. (c) Fine-grained comparison on college dataset. P1 
P2  P3 represent the precision for class -1  0  1  respectively; while R1  R2  R3 represent the corre-
sponding recalls  respectively.

Acknowledgments

This work was supported in part by the National Key R&D Program of China (Grant No.
2016YFB0800403)  in part by National Natural Science Foundation of China: 61620106009 
U1636214  61836002  U1803264  U1736219  61672514 and 61976202  in part by National Basic
Research Program of China (973 Program): 2015CB351800  in part by Key Research Program of
Frontier Sciences  CAS: QYZDJ-SSW-SYS013  in part by the Strategic Priority Research Program
of Chinese Academy of Sciences  Grant No. XDB28000000  in part by Peng Cheng Laboratory
Project of Guangdong Province PCL2018KP004  in part by Beijing Natural Science Foundation
(4182079)  in part by Youth Innovation Promotion Association CAS  and in part by Hong Kong
Research Grant Council (HKRGC) grant 16303817.

References
[1] M. Burger  S. Osher  J. Xu  and G. Gilboa. Nonlinear inverse scale space methods for image restoration.
In International Workshop on Variational  Geometric  and Level Set Methods in Computer Vision  pages
25–36. Springer  2005.

[2] X. Chen  P. N. Bennett  K. Collins-Thompson  and E. Horvitz. Pairwise ranking aggregation in a crowd-

sourced setting. In International Conference on Web Search and Data Mining  pages 193–202  2013.

[3] W. Cheng  M. Rademaker  B. De Baets  and E. Hüllermeier. Predicting partial orders: ranking with
abstention. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases 
pages 215–230  2010.

[4] D. Cynthia  K. Ravi  N. Moni  and S. Dandapani. Rank aggregation methods for the web. In International

Conference on World Wide Web  pages 613–622  2001.

[5] A. Gionis  H. Mannila  K. Puolamäki  and A. Ukkonen. Algorithms for discovering bucket orders from
data. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 561–
566  2006.

[6] X. He  Z. He  X. Du  and T.-S. Chua. Adversarial personalized ranking for recommendation. In Inter-
national ACM SIGIR Conference on Research & Development in Information Retrieval  pages 355–364 
2018.

[7] H. Hu  Y. Zheng  Z. Bao  G. Li  J. Feng  and R. Cheng. Crowdsourced POI labelling: Location-aware
result inference and task assignment. In International Conference on Data Engineering  pages 61–72.
IEEE  2016.

[8] C. Huang  X. Sun  J. Xiong  and Y. Yao. Split LBI: An iterative regularization path with structural sparsity.

In Advances in Neural Information Processing Systems  pages 3369–3377  2016.

[9] C. Huang  X. Sun  J. Xiong  and Y. Yao. Boosting with structural sparsity: A differential inclusion

approach. Applied and Computational Harmonic Analysis  48(1):1–45  2020.

[10] C. Huang and Y. Yao. A uniﬁed dynamic approach to sparse model selection. In International Conference

on Artiﬁcial Intelligence and Statistics  pages 2047–2055  2018.

9

[11] X. Jiang  L.-H. Lim  Y. Yao  and Y. Ye. Statistical ranking and combinatorial Hodge theory. Mathematical

Programming  127(6):203–244  2011.

[12] Z. Jiang  H. Liu  B. Fu  Z. Wu  and T. Zhang. Recommendation in heterogeneous information networks
In ACM International

based on generalized random walk model and bayesian personalized ranking.
Conference on Web Search and Data Mining  pages 288–296  2018.

[13] E. Kamar  A. Kapoor  and E. Horvitz. Identifying and accounting for task-dependent bias in crowdsourc-

ing. In AAAI Conference on Human Computation and Crowdsourcing  2015.

[14] G. Lebanon and Y. Mao. Non-parametric modeling of partially ranked data. Journal of Machine Learning

Research  9:2401–2429  2008.

[15] G. Li  C. Chai  J. Fan  X. Weng  J. Li  Y. Zheng  Y. Li  X. Yu  X. Zhang  and H. Yuan. Cdb: optimizing
In ACM International Conference on Management of

queries with crowd-based selections and joins.
Data  pages 1463–1478. ACM  2017.

[16] T. Liu. Learning to Rank for Information Retrieval. Springer  2011.

[17] T. Lu and C. Boutilier. Learning mallows models with pairwise preferences. In International Conference

on Machine Learning  pages 145–152  2011.

[18] T. Lu and C. Boutilier. Effective sampling and learning for mallows models with pairwise-preference

data. The Journal of Machine Learning Research  15(1):3783–3829  2014.

[19] Y. Lu and S. N. Negahban. Individualized rank aggregation using nuclear norm regularization. In Allerton

Conference on Communication  Control  and Computing (Allerton)  pages 1473–1479  2015.

[20] S. Negahban  S. Oh  and D. Shah. Iterative ranking from pair-wise comparisons. In Advances in neural

information processing systems  pages 2474–2482  2012.

[21] S. Osher  F. Ruan  J. Xiong  Y. Yao  and W. Yin. Sparse recovery via differential inclusions. Applied and

Computational Harmonic Analysis  41(2):436–469  2016.

[22] B. Osting  C. Brune  and S. Osher. Enhanced statistical rankings via targeted data collection. In Interna-

tional Conference on Machine Learning  pages 489–497  2013.

[23] A. Sheshadri and M. Lease. Square: A benchmark for research on computing crowd consensus. In AAAI

conference on human computation and crowdsourcing  2013.

[24] X. Sun  L. Hu  Y. Yao  and Y. Wang. GSplit LBI: Taming the procedural bias in neuroimaging for
In International Conference on Medical Image Computing and Computer-Assisted

disease prediction.
Intervention  pages 107–115  2017.

[25] M. Venanzi  J. Guiver  G. Kazai  P. Kohli  and M. Shokouhi. Community-based bayesian aggregation
In International Conference on World Wide Web  pages 155–164. ACM 

models for crowdsourcing.
2014.

[26] Q. Xu  T. Jiang  Y. Yao  Q. Huang  B. Yan  and W. Lin. Random partial paired comparison for subjective
video quality assessment via HodgeRank. In ACM International Conference on Multimedia  pages 393–
402  2011.

[27] Q. Xu  J. Xiong  X. Cao  Q. Huang  and Y. Yao. From social to individuals: a parsimonious path of
multi-level models for crowdsourced preference aggregation. IEEE Transactions on Pattern Analysis and
Machine Intelligence  41(4):844–856  2019.

[28] Q. Xu  J. Xiong  X. Cao  and Y. Yao. Parsimonious mixed-effects HodgeRank for crowdsourced prefer-

ence aggregation. In ACM International Conference on Multimedia  pages 841–850  2016.

[29] Q. Xu  J. Xiong  X. Sun  Z. Yang  X. Cao  Q. Huang  and Y. Yao. A margin-based mle for crowdsourced

partial ranking. In ACM International Conference on Multimedia  pages 591–599  2018.

[30] J. Yi  R. Jin  S. Jain  and A. Jain. Inferring users’ preferences from crowdsourced pairwise comparisons:
A matrix completion approach. In AAAI Conference on Human Computation and Crowdsourcing  pages
207–215  2013.

[31] M.-L. Zhang and Z.-H. Zhou. A review on multi-label learning algorithms. IEEE Transactions on Knowl-

edge and Data Engineering  26(8):1819–1837  2014.

10

[32] B. Zhao  X. Sun  Y. Fu  Y. Yao  and Y. Wang. MSplit LBI: realizing feature selection and dense estimation
simultaneously in few-shot and zero-shot learning. In International Conference on Machine Learning 
pages 5907–5916  2018.

[33] Y. Zheng  J. Wang  G. Li  R. Cheng  and J. Feng. QASCA: A quality-aware task assignment system for
crowdsourcing applications. In ACM SIGMOD International Conference on Management of Data  pages
1031–1046  2015.

11

,Qianqian Xu
Xinwei Sun
Zhiyong Yang
Xiaochun Cao
Qingming Huang
Yuan Yao