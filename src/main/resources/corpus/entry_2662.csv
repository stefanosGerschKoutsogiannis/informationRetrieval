2013,Information-theoretic lower bounds for distributed statistical estimation with communication constraints,We establish minimax risk lower bounds for distributed statistical estimation given a budget $B$ of the total number of bits that may be communicated. Such lower bounds in turn reveal the minimum amount of communication required by any procedure to achieve the classical optimal rate for statistical estimation. We study two classes of protocols in which machines send messages either independently or interactively. The lower bounds are established for a variety of problems  from estimating the mean of a population to estimating parameters in linear regression or binary classification.,Information-theoretic lower bounds for distributed

statistical estimation with communication constraints

Yuchen Zhang1
John C. Duchi1 Michael I. Jordan1 2 Martin J. Wainwright1 2
1Department of Electrical Engineering and Computer Science and 2Department of Statistics

University of California  Berkeley

Berkeley  CA 94720

{yuczhang jduchi jordan wainwrig}@eecs.berkeley.edu

Abstract

We establish lower bounds on minimax risks for distributed statistical estima-
tion under a communication budget. Such lower bounds reveal the minimum
amount of communication required by any procedure to achieve the centralized
minimax-optimal rates for statistical estimation. We study two classes of proto-
cols: one in which machines send messages independently  and a second allowing
for interactive communication. We establish lower bounds for several problems 
including various types of location models  as well as for parameter estimation in
regression models.

1

Introduction

Rapid growth in the size and scale of datasets has fueled increasing interest in statistical estimation
in distributed settings [see  e.g.  5  23  7  9  17  2]. Modern data sets are often too large to be stored
on a single machine  so that it is natural to consider methods that involve multiple machines  each
assigned a smaller subset of the full dataset. An essential design parameter in such methods is the
amount of communication required between machines or chips. Bandwidth limitations on network
and inter-chip communication often impose signiﬁcant bottlenecks on algorithmic efﬁciency.

The focus of the current paper is the communication complexity of various classes of statistical es-
timation problems. More formally  suppose that we are interested in estimating the parameter θ of
some unknown distribution P   based on a dataset of N i.i.d. samples. In the classical setting  one
considers centralized estimators that have access to all N samples  and for a given estimation prob-
lem  the optimal performance over all centralized schemes can be characterized by the minimax rate.
By way of contrast  in the distributed setting  one is given m different machines  and each machine
is assigned a subset of samples of size n = ! N
m". Each machine may perform arbitrary operations
on its own subset of data  and it then communicates results of these intermediate computations to
the other processors or to a central fusion node. In this paper  we try to answer the following ques-
tion: what is the minimal number of bits that must be exchanged in order to achieve the optimal
estimation error achievable by centralized schemes?

There is a substantial literature on communication complexity in many settings  including function
computation in theoretical computer science (e.g.  [21  1  13])  decentralized detection and estima-
tion (e.g.  [18  16  15]) and information theory [11]. For instance  Luo [15] considers architectures in
which machines may send only a single bit to a centralized processor; for certain problems  he shows
that if each machine receives a single one-dimensional sample  it is possible to achieve the optimal
centralized rate up to constant factors. Among other contributions  Balcan et al. [2] study Probably
Approximately Correct (PAC) learning in the distributed setting; however  their stated lower bounds
do not involve the number of machines. In contrast  our work focuses on scaling issues  both in terms
of the number of machines as well as the dimensionality of the underlying data  and we formalize
the problem in terms of statistical minimax theory.

1

More precisely  we study the following problem: given a budget B of the total number of bits that
may be communicated from the m distributed datasets  what is the minimax risk of any estimator
based on the communicated messages? While there is a rich literature connecting information-
theoretic techniques with the risk of statistical estimators (e.g. [12  22  20  19])  little of it character-
izes the effects of limiting communication. In this paper  we present some minimax lower bounds for
distributed statistical estimation. By comparing our lower bounds with results in statistical estima-
tion  we can identify the minimal communication cost that a distributed estimator must pay to have
performance comparable to classical centralized estimators. Moreover  we show how to leverage
recent work [23] to achieve these fundamental limits.

2 Problem setting and notation

We begin with a formal description of the statistical estimation problems considered here. Let
P denote a family of distributions and let θ : P→ Θ ⊆ Rd denote a function deﬁned on P.
A canonical example throughout the paper is the problem of mean estimation  in which θ(P ) =
EP [X]. Suppose that  for some ﬁxed but unknown member P of P  there are m sets of data stored
on individual machines  where each subset X (i) is an i.i.d. sample of size n from the unknown
distribution P .1 Given this distributed collection of local data sets  our goal is to estimate θ(P )
based on the m samples X (1)  . . .   X (m)  but using limited communication.

We consider a class of distributed protocols Π  in which at each round t = 1  2  . . .  machine i sends a
message Yt i that is a measurable function of the local data X (i)  and potentially of past messages. It
is convenient to model this message as being sent to a central fusion center. Let Y t = {Yt i}i∈[m] de-
note the collection of all messages sent at round t. Given a total of T rounds  the protocol Π collects

the sequence (Y 1  . . .   Y T )  and constructs an estimator !θ :=!θ(Y 1  . . .   Y T ). The length Lt i of
message Yt i is the minimal number of bits required to encode it  and the total L ="T

i=1 Lt i
of all messages sent corresponds to the total communication cost of the protocol. Note that the com-
munication cost is a random variable  since the length of the messages may depend on the data  and
the protocol may introduce auxiliary randomness.

t=1"m

It is useful to distinguish two different classes  namely independent versus interactive protocols. An
independent protocol Π is based on a single round (T = 1) of communication  in which machine
i sends message Y1 i to the fusion center. Since there are no past messages  the message Y1 i can
depend only on the local sample X (i). Given a family P  the class of independent protocols with
budget B ≥ 0 is given by

(1)

(2)

Aind(B P) =# independent protocols Π such that

EP$ m%i=1

Li& ≤ B’.

sup
P∈P

(For simplicity  we use Yi to indicate the message sent from processor i and Li to denote its length
in the independent case.) It can be useful in some situations to have more granular control on the
amount of communication  in particular by enforcing budgets on a per-machine basis. In such cases 
we introduce the shorthand B1:m = (B1  . . .   Bm) and deﬁne

Aind(B1:m P) =#independent protocols Π such that

EP [Li] ≤ Bi for i ∈ [m]’ .

sup
P∈P

In contrast to independent protocols  the class of interactive protocols allows for interaction at dif-
ferent stages of the message passing process. In particular  suppose that machine i sends message
Yt i to the fusion center at time t  who then posts it on a “public blackboard ” where all machines can
read Yt i. We think of this as a global broadcast system  which may be natural in settings in which
processors have limited power or upstream capacity  but the centralized fusion center can send mes-
sages without limit. In the interactive setting  the message Yt i should be viewed as a measurable
function of the local data X (i)  and the past messages Y 1:t−1. The family of interactive protocols
with budget B ≥ 0 is given by

Ainter(B P) =#interactive protocols Π such that

EP [L] ≤ B’.

sup
P∈P

(3)

1 Although we assume in this paper that every machine has the same amount of data  our technique gener-

alizes easily to prove tight lower bounds for distinct data sizes on different machines.

2

We conclude this section by deﬁning the minimax framework used throughout this paper. We wish to

characterize the best achievable performance of estimators!θ that are functions of only the messages
(Y 1  . . .   Y T ). We measure the quality of a protocol and estimator!θ by the mean-squared error

EP Π((!θ(Y 1  . . .   Y T ) − θ(P )(2
2)  

where the expectation is taken with respect to the protocol Π and the m i.i.d. samples X (i) of size
n from distribution P . Given a class of distributions P  parameter θ : P→ Θ  and communication
budget B  the minimax risk for independent protocols is

Mind(θ  P  B) :=

inf

Π∈Aind(B P)

inf
!θ

sup
P∈P

EP Π$***!θ(Y1  . . .   Ym) − θ(P )***

2

2& .

(4)

Here  the inﬁmum is taken jointly over all independent procotols Π that satisfy the budget constraint

B  and over all estimators!θ that are measurable functions of the messages in the protocol. This min-

imax risk should also be understood to depend on both the number of machines m and the individual
sample size n. The minimax risk for interactive protocols  denoted by Minter  is deﬁned analogously 
where the inﬁmum is instead taken over the class of interactive protocols. These communication-
dependent minimax risks are the central objects in this paper: they provide a sharp characterization
of the optimal rate of statistical estimation as a function of the communication budget B.

3 Main results

With our setup in place  we now turn to the statement of our main results  along with some discussion
of their consequences.

3.1 Lower bound based on metric entropy

We begin with a general but relatively naive lower bound that depends only on the geometric struc-
ture of the parameter space  as captured by its metric entropy. In particular  given a subset Θ ⊂ Rd 
entropy of Θ as

we say {θ1  . . .  θ K} are δ-separated if**θi − θj**2 ≥ δ for i += j. We then deﬁne the packing

log MΘ(δ) := log2+max K ∈ N | {θ1  . . .  θ K}⊂ Θ are δ-separated-. .

(5)
The function θ  → log MΘ(δ) is left-continuous and non-increasing in δ  so we may deﬁne the
inverse function log M−1

Θ (B) := sup{δ | log MΘ(δ) ≥ B}.

Proposition 1 For any family of distributions P and parameter set Θ= θ(P)  the interactive
minimax risk is lower bounded as

Minter(θ  P  B) ≥/ 1

4

log M−1

Θ (2B + 2)02

.

(6)

Of course  the same lower bound also holds for Mind(θ  P  B)  since any independent protocol is
a special case of an interactive protocol. Although Proposition 1 is a relatively generic statement 
not exploiting any particular structure of the problem  it is in general unimprovable by more than
constant factors  as the following example illustrates.

Example: Bounded mean estimation. Suppose that our goal is to estimate the mean θ = θ(P )
of a class of distributions P supported on the interval [0  1]  so that Θ= θ(P) = [0  1]. Suppose
that a single machine (m = 1) receives n i.i.d. observations Xi according to P . Since the packing
entropy is lower bounded as log MΘ(δ) ≥ log(1/δ)  the lower bound (6) implies

Mind(θ  P  B) ≥ Minter(θ  P  B) ≥

e−2
4

e−2B.

2 log n yields the lower bound Mind(θ  P([0  1])  B) ≥ e−2
Thus  setting B = 1
4n . This lower bound
is sharp up to the constant pre-factor  since it can be achieved by a simple method. Given its n
n"n
observations  the single machine can compute the sample mean X n = 1
i=1 Xi. Since the
sample mean lies in the interval [0  1]  it can be quantized to accuracy 1/n using log(n) bits  and this
n  

quantized version!θ can be transmitted. A straightforward calculation shows that E[(!θ − θ)2] ≤ 2

so Proposition 1 yields an order-optimal bound in this case.

3

3.2 Multi-machine settings

We now turn to the more interesting multi-machine setting (m > 1). Let us study how the budget
B—meaning the of bits required to achieve the minimax rate—scales with the number of machines
m. We begin by considering the uniform location family U = {Pθ θ ∈ [−1  1]}  where Pθ is
the uniform distribution on the interval [θ − 1 θ + 1]. For this problem  a direct application of
Proposition 1 gives a nearly sharp result.

Corollary 1 Consider the uniform location family U with n i.i.d. observations per machine:

(a) Whenever the communication budget is upper bounded as B ≤ log(mn)  there is a univer-

sal constant c such that

Minter(θ  U  B) ≥

(mn)2 .

(b) Conversely  given a budget of B =+2 + 2 ln m. log(mn) bits  there is a universal constant

c# such that

c

c#

Minter(θ  U  B) ≤

(mn)2 .

If each of m machines receives n observations  we have a total sample size of mn  so the minimax
rate over all centralized procedures scales as 1/(mn)2 (for instance  see [14]). Consequently  Corol-
lary 1(b) shows that the number of bits required to achieve the centralized rate has only logarithmic
dependence on the number m of machines. Part (a) shows that this logarithmic dependence on m is
unavoidable.

It is natural to wonder whether such logarithmic dependence holds more generally. The following
result shows that it does not: for some problems  the dependence on m must be (nearly) linear. In
particular  we consider estimation in a normal location family model  where each machine receives
an i.i.d. sample of size n from a normal distribution N(θ  σ 2) with unknown mean θ.
Theorem 1 For the univariate normal family N = {N(θ  σ 2) | θ ∈ [−1  1]}  there is a universal
constant c such that

Minter(θ  N   B) ≥ c

σ2
mn

min# mn

σ2  

m

log m

 

m

(7)

B log m’ .
log m2 bits are required for a

mn ; consequently  the lower bound (7) shows that at least B =Ω1 m

The centralized minimax rate for estimating a univariate normal mean based on mn observations
is σ2
decentralized procedure to match the centralized rate in this case. This type of scaling is dramati-
cally different than the logarithmic scaling for the uniform family  showing that establishing sharp
communication-based lower bounds requires careful study of the underlying family of distributions.

3.3

Independent protocols in multi-machine settings

Departing from the interactive setting  in this section we focus on independent protocols  providing
somewhat more general results than those for interactive protocols. We ﬁrst provide lower bounds
for the problem of mean estimation in the parameter for a d-dimensional normal location family

Nd = {N(θ  σ 2Id×d) | θ ∈ Θ= [ −1  1]d} 

(8)

Theorem 2 For i = 1  . . .   m  assume that each machine has communication budget Bi  and re-
ceives an i.i.d. sample of size n from a distribution P ∈N d. There exists a universal (numerical)
constant c such that

Mind(θ  Nd  B1:m) ≥ c

σ2d
mn

min3 mn

σ2  

m

log m

 

m

1"m
i=1 min{1  Bi

d }2 log m4 .

(9)

Given centralized access to the full mn-sized sample  a reasonable procedure would be to compute
the sample mean  leading to an estimate with mean-squared error σ2d
mn   which is minimax optimal.

4

Consequently  Theorem 2 shows that to achieve an order-optimal mean-squared error  the total num-
ber of bits communicated must (nearly) scale with the product of the dimension d and number of
machines m  that is  as dm/ log m. If we ignore logarithmic factors  this lower bound is achievable
by a simple procedure: each machine computes the sample mean of its local data and quantizes each
coordinate to precision σ2/n using O(d log(n/σ2)) bits. These quantized sample averages are com-
municated to the fusion center using B = O(dm log(n/σ2)) total bits. The fusion center averages
them  obtaining an estimate with mean-squared error of optimal order σ2d/(mn) as required.

min3m 

d
m

m

d }4  

We ﬁnish this section by presenting a result that is sharp up to numerical constant prefactors. It is a
minimax lower bound for mean estimation over the family Pd = {P supported on [−1  1]d}.
Proposition 2 Assume that each of m machines receives a single sample (n = 1) from a distribution
in Pd. There exists a universal (numerical) constant c such that

(10)

Mind(θ  Pd  B1:m) ≥ c

where Bi is the budget for machine i.

The standard minimax rate for d-dimensional mean estimation scales as d/m. The lower bound (10)
d } ! m  showing that each

"m
i=1 min{1  Bi
shows that in order to achieve this scaling  we must have"m
i=1 min{1  Bi
machine must send Bi ! d bits.
Moreover  this lower bound is achievable by a simple scheme. Suppose that machine i receives
a d-dimensional vector Xi ∈ [−1  1]d. Based on Xi  it generates a Bernoulli random vector
Zi = (Zi1  . . .   Zid) with Zij ∈{ 0  1} taking the value 1 with probability (1 + Xij)/2  indepen-
dently across coordinates. Machine i uses d bits to send the vector Zi ∈{ 0  1}d to the fusion center.
The fusion center then computes the average!θ = 1
i=1(2Zi − 1). This average is unbiased  and
its expected squared error is bounded by d/m.

m"m

4 Consequences for regression

In this section  we turn to identifying the minimax rates for a pair of important estimation problems:
linear regression and probit regression.

4.1 Linear regression

We consider a distributed instantiation of linear regression with ﬁxed design matrices. Concretely 
suppose that each of m machines has stored a ﬁxed design matrix A(i) ∈ Rn×d and then observes a
response vector b(i) ∈ Rd from the standard linear regression model

(11)
where ε(i) ∼ N(0 σ 2In×n) is a noise vector. Our goal is to estimate unknown regression vector
θ ∈ Θ= [ −1  1]d  shared across all machines  in a distributed manner  To state our result  we
assume uniform upper and lower bounds on the eigenvalues of the rescaled design matrices  namely

b(i) = A(i)θ + ε(i) 

0 <λ min ≤ min

i∈{1 ... m}

ηmin(A(i))

√n

and

max

i∈{1 ... m}

ηmax(A(i))

√n

≤ λmax.

(12)

Corollary 2 Consider an instance of the linear regression model (11) under condition (12).

(a) Then there is a universal positive constant c such that

Mind(θ  P  B1:m) ≥ c

σ2d
mn

min3 mn

σ2  

m

λ2
max log m

 

m

i=1 min{1  Bi

λ2

max1"m

d }2 log m4 .

(b) Conversely  given budgets Bi ≥ d log(mn) for i = 1  . . .   m  there is a universal constant

c# such that

Mind(θ  P  B1:m) ≤

c#
λ2

min

σ2d
mn

.

5

It is a classical fact (e.g. [14]) that the minimax rate for d-dimensional linear regression scales as
dσ2/(nm). Part (a) of Corollary 2 shows this optimal rate is attainable only if the budget Bi at each
log m .
Part (b) of the corollary shows that the minimax rate is achievable with budgets that match the lower
bound up to logarithmic factors.

machine is of the order d/ log(m)  meaning that the total budget B ="m

i=1 Bi must grow as dm

Proof: The proof of part (b) follows from techniques of Zhang et al. [23]  who show that solving
each regression problem separately and then performing a form of approximate averaging  in which
each machine uses Bi = d log(mn) bits  achieves the minimax rate up to constant prefactors.

To prove part (a)  we show that solving an arbitrary Gaussian mean estimation problem can be
reduced to solving a specially constructed linear regression problem. This reduction allows us to
apply the lower bound from Theorem 2. Given θ ∈ Θ  consider the Gaussian mean model

X (i) = θ + w(i)  where w(i) ∼ N50 

σ2
λ2
maxn

Id×d6 .

Each machine i has its own design matrix A(i)  and we use it to construct a response vector
maxn A(i)(A(i))% is

b(i) ∈ Rn. Since ηmax(A(i)/√n) ≤ λmax  the matrix Σ(i) := σ2In×n − σ2

positive semideﬁnite. Consequently  we may form a response vector via

λ2

b(i) = A(i)X (i) + z(i) 

z(i) ∼ N10  Σ(i)2 is drawn independently of w(i).

(13)
The independence of w(i) and z(i) guarantees that b(i) ∼ N(A(i)θ  σ 2In×n)  so that the pair
(b(i)  A(i)) is faithful to the regression model (11).
Now consider any protocol Π ∈A ind(B P) that can solve any regression problem to within accu-
2] ≤ δ2. By the previously described reduction  the protocol Π can also
solve the mean estimation problem to accuracy δ  in particular via the pair (A(i)  b(i)) constructed
via expression (13). Combined with this reduction  the corollary thus follows from Theorem 2. "

racy δ  so that E[(!θ − θ(2

4.2 Probit regression

We now turn to the problem of binary classiﬁcation  in particular considering the probit re-
gression model. As in the previous section  each of m machines has a ﬁxed design matrix
A(i) ∈ Rn×d  where A(i k) denotes the kth row of A(i). Machine i receives n binary responses
Z(i) = (Z(i 1)  . . .   Z(i n))  drawn from the conditional distribution

P(Z(i k) = 1 | A(i k) θ ) =Φ( A(i k)θ)

(14)
where Φ(·) denotes the standard normal CDF. The log-likelihood of the probit model (14) is con-
cave [4  Exercise 3.54]. Under condition (12) on the design matrices  we have:

for some ﬁxed θ ∈ Θ= [ −1  1]d 

Corollary 3 Consider the probit model (14) under condition (12). Then

(a) There is a universal constant c such that

Mind(θ  P  B1:m) ≥ c

d
mn

min3mn 

m

λ2
max log m

 

m

i=1 min{1  Bi

λ2

max1"m

d }2 log m4 .

(b) Conversely  given budgets Bi ≥ d log(mn) for i = 1  . . .   m  there is a universal constant

c# such that

Mind(θ  P  B1:m) ≤

c#
λ2

min

d
mn

.

Proof: As in the previous case with linear regression  Zhang et al.’s study of distributed convex
optimization [23] gives part (b): each machine solves the local probit regression separately  after
which each machine sends Bi = d log(mn) bits to average its local solution.

To prove part (a)  we show that linear regression problems can be solved via estimation in a specially
constructed probit model. Consider an arbitrary θ ∈ Θ; assume we have a regression problem of the

6

form (11) with noise variance σ2 = 1. We construct the binary responses for our probit regression
(Z(i 1)  . . .   Z(i n)) by

Z(i k) =#1

0

if b(i k) ≥ 0 
otherwise.

(15)

By construction  we have P(Z(i k) = 1 | A(i) θ ) =Φ( A(i k)θ) as desired for our model (14). By
inspection  any protocol Π ∈A ind(B P) solving the probit regression problem provides an esti-
mator with the same error (risk) as the original linear regression problem via the construction (15).
"
Corollary 2 provides the desired lower bound.

5 Proof sketches for main results

We now give an outline of the proof of each of our main results (Theorems 1 and 2)  providing a
more detailed proof sketch for Proposition 2  since it displays techniques common to our arguments.

5.1 Broad outline

Most of our lower bounds follow the same basic strategy of reducing an estimation problem to
a testing problem. Following this reduction  we then develop inequalities relating the probability
of error in the test to the number of bits contained in the messages Yi sent from each machine.
Establishing these links is the most technically challenging aspect.

Our reduction from estimation to testing is somewhat more general than the classical reductions
(e.g.  [22  20])  since we do not map the original estimation problem to a strict test  but rather a
test that allows some errors. Let V denote an index set of ﬁnite cardinality  where ν ∈V indexes a
family of probability distributions {P (· | ν)}ν∈V . For each member of this family  associate with a
parameter θν := θ(P (· | ν)) ∈ Θ  where Θ denotes the parameter space. In our proofs applicable to
d-dimensional problems  we set V = {−1  1}d  and we index vectors θν by ν ∈V . Now  we sample
V uniformly at random from V. Conditional on V = ν  we then sample X from a distribution
PX(· | V = ν) satisfying θν := θ(PX(· | ν)) = δν  where δ> 0 is a ﬁxed quantity that we control.
We deﬁne dham(ν  ν#) to be the Hamming distance between ν  ν# ∈V . This construction gives

(θν − θν "(2 = 2δ7dham(ν  ν#).

Fixing t ∈ R  the following lemma reduces the problem of estimating θ to ﬁnding a point ν ∈V
within distance t of the random variable V . The result extends a result of Duchi and Wainwright
[8]; for completeness we provide a proof in Appendix H.

Lemma 1 Let V be uniformly sampled from V. For any estimator!θ and any t ∈ R  we have

2] ≥ δ2(!t" + 1) inf
!ν

sup
P∈P

E[(!θ − θ(P )(2

P (dham(!ν  V ) > t)  

where the inﬁmum ranges over all testing functions.

Lemma 1 shows that minimax lower lower bound can be derived by showing that  for some t > 0
to be chosen  it is difﬁcult to identify V within a radius of t. The following extension of Fano’s
inequality [8] can be used to control this type of error probability:

Lemma 2 Let V → X → !V be a Markov chain  where V is uniform on V. For any t ∈ R  we have

I(V ; X) + log 2

 

P(dham(!V   V ) > t) ≥ 1 −

log |V|Nt

where Nt := max

ν∈V |{ν# ∈V : dham(ν  ν#) ≤ t}| is the size of the largest t-neighborhood in V.

Lemma 2 allows ﬂexibility in the application of the minimax bounds from Lemma 1. If there is a
large set V for which it is easy to control I(V ; X)  whereas neighborhoods in V are relatively small
(i.e.  Nt is small)  then we can obtain sharp lower bounds.

7

In a distributed protocol  we have a Markov chain V → X → Y   where Y denotes the messages the

different machines send. Based on the messages Y   we consider an arbitrary estimator!θ(Y ). For
τ2 ≤ 21d
τ =01d
0 ≤ t ≤ /d/30  we have Nt ="t
t6 ≥ d log 2 −
Nt ≥ d log 2 − log 25d

t2 ≤ (de/t)t  for t ≤ d/6 we have

t2. Since1d

log(6e) − log 2 = d log

log |V|

21/d 6√6e

>

d
6

d
6

2

for d ≥ 12 (the case d < 12 can be checked directly). Thus  combining Lemma 1 and Lemma 2

(using the Markov chain V → X → Y →!θ)  we ﬁnd that for t = !d/6" 

2) ≥ δ2(!d/6" + 1)51 −

sup
P∈P

E((!θ(Y ) − θ(P )(2

d/6

I(Y ; V ) + log 2

With inequality (16) in hand  it then remains to upper bound the mutual information I(Y ; V )  which
is the main technical content of each of our results.

6 .

(16)

5.2 Proof sketch of Proposition 2
Following the general outline of the previous section  let V be uniform on V = {−1  1}d. Letting
0 <δ ≤ 1 be a positive number  for i ∈ [m] we independently sample X (i) ∈ Rd according to

P (X (i)

j = νj | V = ν) =

1 + δ

2

and P (X (i)

j = −νj | V = ν) =

1 − δ
2

.

(17)

Under this distribution  we can give a sharp characterization of the mutual information I(V ; Yi). In
particular  we show in Appendix B that under the sampling distribution (17)  there exists a numerical
constant c such that

I(V ; Yi) ≤ cδ2I(X (i); Yi).

(18)

Since the random variable X takes discrete values  we have

I(X (i); Yi) ≤ min{H(X (i))  H(Yi)}≤ min{d  H(Yi)}.

Since the expected length of message Yi is bounded by Bi  Shannon’s source coding theorem [6]
implies that H(Yi) ≤ Bi. In particular  inequality (18) establishes a link between the initial distri-
bution (17) and the number of bits used to transmit information  that is 

We can now apply the quantitative data processing inequality (19) in the bound (16). By the in-
i=1 I(V ; Yi)  and thus inequality (16)

I(V ; Yi) ≤ cδ2 min{d  Bi}.
dependence of the communication scheme  I(V ; Y1:m) ≤"m
cδ2"m

Mind(θ  P  B1:m) ≥ δ2(!d/6" + 1)51 −

simpliﬁes to

i=1 min{d  Bi} + log 2

d/6

6 .

(19)

Assuming d ≥ 9  so 1 − 6 log 2/d > 1/2  we see that choosing δ2 = min{1 
implies

24c "m

d

i=1 min{Bi d}}

Mind(θ  P  B1:m) ≥

δ2(!d/6" + 1)

4

= !d/6" + 1

4

min#1 

Rearranging slightly gives the statement of the proposition.

Acknowledgments

d

i=1 min{Bi  d}’ .

24c"m

We thank the anonymous reviewers for their helpful feedback and comments. JCD was supported
by a Facebook Graduate Fellowship. Our work was supported in part by the U.S. Army Research
Laboratory  U.S. Army Research Ofﬁce under grant number W911NF-11-1-0391  and Ofﬁce of
Naval Research MURI grant N00014-11-1-0688.

8

References

[1] H. Abelson. Lower bounds on information transfer in distributed computations. Journal of the

ACM  27(2):384–392  1980.

[2] M.-F. Balcan  A. Blum  S. Fine  and Y. Mansour. Distributed learning  communication com-
plexity and privacy. In Proceedings of the Twenty Fifth Annual Conference on Computational
Learning Theory  2012. URL http://arxiv.org/abs/1204.3514.

[3] K. Ball. An elementary introduction to modern convex geometry. In S. Levy  editor  Flavors

of Geometry  pages 1–58. MSRI Publications  1997.

[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[5] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Foundations and Trends in
Machine Learning  3(1)  2011.

[6] T. M. Cover and J. A. Thomas. Elements of Information Theory  Second Edition. Wiley  2006.

[7] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction

using mini-batches. Journal of Machine Learning Research  13:165–202  2012.

[8] J. C. Duchi and M. J. Wainwright. Distance-based and continuum fano inequalities with appli-

cations to statistical estimation. arXiv [cs.IT]  to appear  2013.

[9] J. C. Duchi  A. Agarwal  and M. J. Wainwright. Dual averaging for distributed optimization:
convergence analysis and network scaling. IEEE Transactions on Automatic Control  57(3):
592–606  2012.

[10] J. C. Duchi  M. I. Jordan  and M. J. Wainwright. Local privacy and statistical minimax rates.

arXiv:1302.3203 [math.ST]  2013. URL http://arXiv.org/abs/1302.3203.

[11] S. Han and S. Amari. Statistical inference under multiterminal data compression. IEEE Trans-

actions on Information Theory  44(6):2300–2324  1998.

[12] I. A. Ibragimov and R. Z. Has’minskii. Statistical Estimation: Asymptotic Theory. Springer-

Verlag  1981.

[13] E. Kushilevitz and N. Nisan. Communication Complexity. Cambridge University Press  1997.

[14] E. L. Lehmann and G. Casella. Theory of Point Estimation  Second Edition. Springer  1998.

[15] Z.-Q. Luo. Universal decentralized estimation in a bandwidth constrained sensor network.

IEEE Transactions on Information Theory  51(6):2210–2219  2005.

[16] Z.-Q. Luo and J. N. Tsitsiklis. Data fusion with minimal communication. IEEE Transactions

on Information Theory  40(5):1551–1563  1994.

[17] R. McDonald  K. Hall  and G. Mann. Distributed training strategies for the structured percep-
tron. In North American Chapter of the Association for Computational Linguistics (NAACL) 
2010.

[18] J. N. Tsitsiklis. Decentralized detection.

In Advances in Signal Processing  Vol. 2  pages

297–344. JAI Press  1993.

[19] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2009.

[20] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence.

Annals of Statistics  27(5):1564–1599  1999.

[21] A. C.-C. Yao. Some complexity questions related to distributive computing (preliminary re-
port). In Proceedings of the Eleventh Annual ACM Symposium on the Theory of Computing 
pages 209–213. ACM  1979.

[22] B. Yu. Assouad  Fano  and Le Cam.

In Festschrift for Lucien Le Cam  pages 423–435.

Springer-Verlag  1997.

[23] Y. Zhang  J. C. Duchi  and M. J. Wainwright. Communication-efﬁcient algorithms for statisti-

cal optimization. In Advances in Neural Information Processing Systems 26  2012.

9

,Yuchen Zhang
John Duchi
Michael Jordan
Martin Wainwright