2017,Regret Minimization in MDPs with Options without Prior Knowledge,The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e.  options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g.  RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless  the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees  while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option  which are hardly available in practice. In this paper  we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.,Regret Minimization in MDPs with Options

without Prior Knowledge

Ronan Fruit

Sequel Team - Inria Lille
ronan.fruit@inria.fr

Matteo Pirotta

Sequel Team - Inria Lille

matteo.pirotta@inria.fr

Alessandro Lazaric

Sequel Team - Inria Lille

alessandro.lazaric@inria.fr

Emma Brunskill
Stanford University

ebrun@cs.stanford.edu

Abstract

The option framework integrates temporal abstraction into the reinforcement learn-
ing model through the introduction of macro-actions (i.e.  options). Recent works
leveraged the mapping of Markov decision processes (MDPs) with options to
semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation
algorithms (e.g.  RMAX-SMDP and UCRL-SMDP) to analyze the impact of options
on the learning performance. Nonetheless  the PAC-SMDP sample complexity
of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical
guarantees  while the regret analysis of UCRL-SMDP requires prior knowledge of
the distributions of the cumulative reward and duration of each option  which are
hardly available in practice. In this paper  we remove this limitation by combining
the SMDP view together with the inner Markov structure of options into a novel
algorithm whose regret performance matches UCRL-SMDP’s up to an additive
regret term. We show scenarios where this term is negligible and the advantage
of temporal abstraction is preserved. We also report preliminary empirical results
supporting the theoretical ﬁndings.

1

Introduction

Tractable learning of how to make good decisions in complex domains over many time steps almost
deﬁnitely requires some form of hierarchical reasoning. One powerful and popular framework for
incorporating temporally-extended actions in the context of reinforcement learning is the options
framework [1]. Creating and leveraging options has been the subject of many papers over the last two
decades (see e.g.  [2  3  4  5  6  7  8]) and it has been of particular interest recently in combination
with deep reinforcement learning  with a number of impressive empirical successes (see e.g.  [9] for
an application to Minecraft). Intuitively (and empirically) temporal abstraction can help speed up
learning (reduce the amount of experience needed to learn a good policy) by shaping the actions
selected towards more promising sequences of actions [10]  and it can reduce planning computation
through reducing the need to evaluate over all possible actions (see e.g.  Mann and Mannor [11]).
However  incorporating options does not always improve learning efﬁciency as shown by Jong et al.
[12]. Intuitively  limiting action selection only to temporally-extended options might hamper the
exploration of the environment by restricting the policy space. Therefore  we argue that in addition to
the exciting work being done in heuristic and algorithmic approaches that leverage and/or dynamically
discover options  it is important to build a formal understanding of how and when options may help
or hurt reinforcement learning performance  and that such insights may also help inform empirically
motivated options-RL research.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

There has been fairly limited work on formal performance bounds of RL with options. Brunskill and
Li [13] derived sample complexity bounds for an RMAX-like exploration-exploitation algorithm for
semi-Markov decision processes (SMDPs). While MDPs with options can be mapped to SMDPs 
their analysis cannot be immediately translated into the PAC-MDP sample complexity of learning
with options  which makes it harder to evaluate their potential beneﬁt. Fruit and Lazaric [14] analyzed
an SMDP variant of UCRL [15] showing how its regret can be mapped to the regret of learning in the
original MDP with options. The resulting analysis explicitly showed how options can be beneﬁcial
whenever the navigability among the states in the original MDP is not compromised (i.e.  the MDP
diameter is not signiﬁcantly increased)  the level of temporal abstraction is high (i.e.  options have
long durations  thus reducing the number of decision steps)  and the optimal policy with options
performs as well as the optimal policy using primitive actions. While this result makes explicit the
impact of options on the learning performance  the proposed algorithm (UCRL-SMDP  or SUCRL
in short) needs prior knowledge on the parameters of the distributions of cumulative rewards and
durations of each option to construct conﬁdence intervals and compute optimistic solutions. In
practice this is often a strong requirement and any incorrect parametrization (e.g.  loose upper-bounds
on the true parameters) directly translates into a poorer regret performance. Furthermore  even if
a hand-designed set of options may come with accurate estimates of their parameters  this would
not be possible for automatically generated options  which are of increasing interest to the deep RL
community. Finally  this prior work views each option as a distinct and atomic macro-action  thus
losing the potential beneﬁt of considering the inner structure and the interaction between of options 
which could be used to signiﬁcantly improve sample efﬁciency.
In this paper we remove the limitations of prior theoretical analyses. In particular  we combine the
semi-Markov decision process view on options and the intrinsic MDP structure underlying their
execution to achieve temporal abstraction without relying on parameters that are typically unknown.
We introduce a transformation mapping each option to an associated irreducible Markov chain and
we show that optimistic policies can be computed using only the stationary distributions of the
irreducible chains and the SMDP dynamics (i.e.  state to state transition probabilities through options).
This approach does not need to explicitly estimate cumulative rewards and duration of options
and their conﬁdence intervals. We propose two alternative implementations of a general algorithm
(FREE-SUCRL  or FSUCRL in short) that differs in whether the stationary distribution of the options’
irreducible Markov chains and its conﬁdence intervals are computed explicitly or implicitly through
an ad-hoc extended value iteration algorithm. We derive regret bounds for FSUCRL that match the
regret of SUCRL up to an additional term accounting for the complexity of estimating the stationary
distribution of an irreducible Markov chain starting from its transition matrix. This additional regret
is the  possibly unavoidable  cost to pay for not having prior knowledge on options. We further the
theoretical ﬁndings with a series of simple grid-world experiments where we compare FSUCRL to
SUCRL and UCRL (i.e.  learning without options).

2 Preliminaries

Learning in MDPs with options. A ﬁnite MDP is a tuple M =(cid:8)S A  p  r(cid:9) where S is the set of
policy π : S → A maps states to actions. We deﬁne an option as a tuple o =(cid:8)so  βo  πo
(cid:9) where
decision processes (SMDP) MO =(cid:8)SO Os  pO  RO  τO(cid:9) where SO ⊆ S is the set of states where

states  A is the set of actions  p(s(cid:48)|s  a) is the probability of transition from state s to state s(cid:48) through
action a  r(s  a) is the random reward associated to (s  a) with expectation r(s  a). A deterministic
so ∈ S is the state where the option can be initiated1  πo : S → A is the associated stationary Markov
policy  and βo : S → [0  1] is the probability of termination. As proved by Sutton et al. [1]  when
primitive actions are replaced by a set of options O  the resulting decision process is a semi-Markov
options can start and end  Os is the set of options available at state s  pO(s(cid:48)|s  o) is the probability
of terminating in s(cid:48) when starting o from s  RO(s  o) is the (random) cumulative reward obtained
by executing option o from state s until interruption at s(cid:48) with expectation RO(s  o)  and τO(s  o) is
the duration (i.e.  number of actions executed to go from s to s(cid:48) by following πo) with expectation
τ (s  o).2 Throughout the rest of the paper  we assume that options are well deﬁned.

1Restricting the standard initial set to one state so is without loss of generality (see App. A).
2Notice that RO(s  o) (similarly for τO) is well deﬁned only when s = so  that is when o ∈ Os.

2

Assumption 1. The set of options O is admissible  that is 1) all options terminate in ﬁnite time with
probability 1  2)  in all possible terminal states there exists at least one option that can start  i.e. 
∪o∈O{s : βo(s) > 0} ⊆ ∪o∈O{so}  3) the resulting SMDP MO is communicating.
Lem. 3 in [14] shows that under Asm. 1 the family of SMDPs induced by using options in MDPs
is such that for any option o  the distributions of the cumulative reward and the duration are sub-
Exponential with bounded parameters (σr(o)  br(o)) and (στ (o)  bτ (o)) respectively. The maximal
expected duration is denoted by τmax = maxs o {τO(s  o)}. Let t denote primitive action steps and
let i index decision steps at option level. The number of decision steps up to (primitive) step t is
i=1 τi is the number of primitive steps executed over n
decision steps and τi is the (random) number of steps before the termination of the option chosen at
step i. Under Asm. 1 there exists a policy π∗ : S → O over options that achieves the largest gain
(per-step reward)

N (t) = max(cid:8)n : Tn ≤ t(cid:9)  where Tn =(cid:80)n

(cid:20)(cid:80)N (t)

i=1 Ri

t

(cid:21)

 

ρ∗
O def

= max

π

ρπO = max

π

t→+∞ Eπ
lim

(cid:26) RO(s  o)

τO(s  o)

(cid:18)(cid:88)

1

+

τO(s  o)

s(cid:48)∈S

(1)

(cid:19)(cid:27)

where Ri is the reward cumulated by the option executed at step i. The optimal gain also satisﬁes the
optimality equation of an equivalent MDP obtained by data-transformation (Lem. 2 in [16])  i.e. 

∀s ∈ S ρ∗

O = max
o∈Os

pO(s(cid:48)|s  o)u∗

O(s(cid:48)

) − u∗

O(s)

 

(2)

∆(A  n) =(cid:0)(cid:80)n

O is the optimal bias and Os is the set of options than can be started in s (i.e.  o ∈ Os ⇔
where u∗
so = s). In the following sections  we drop the dependency on the option set O from all previous
terms whenever clear from the context. Given the optimal average reward ρ∗
O  we evaluate the
performance of a learning algorithm A by its cumulative (SMDP) regret over n decision steps as
i=1 Ri. In [14] it is shown that ∆(A  n) is equal to the MDP regret
up to a linear “approximation” regret accounting for the difference between the optimal gains of M
on primitive actions and the associated SMDP MO.

(cid:1)ρ∗
O −(cid:80)n

i=1 τi

3 Parameter-free SUCRL for Learning with Options

Optimism in SUCRL. At each episode  SUCRL runs a variant of extended value iteration (EVI) [17]
to solve the “optimistic” version of the data-transformation optimality equation in Eq. 2  i.e. 

(cid:101)ρ∗

= max
o∈Os

(cid:40)

(cid:26)(cid:101)R(s  o)
(cid:101)τ (s  o)

max(cid:101)R (cid:101)τ

(cid:16)

1(cid:101)τ (s  o)

+

max(cid:101)p

(cid:110)(cid:88)

(cid:101)p(s(cid:48)|s  o)(cid:101)u∗

(s(cid:48)

)

(cid:111) −(cid:101)u∗

s(cid:48)∈S

where (cid:101)R and(cid:101)τ are the vectors of cumulative rewards and durations for all state-option pairs and they
Sect.3 in [14] for the exact expression). Similarly  conﬁdence intervals need to be computed for(cid:101)p 

belong to conﬁdence intervals constructed using parameters (σr(o)  br(o)) and (στ (o)  bτ (o)) (see

but this does not require any prior knowledge on the SMDP since the transition probabilities naturally
belong to the simplex over states. As a result  without any prior knowledge  such conﬁdence intervals
cannot be directly constructed and SUCRL cannot be run. In the following  we see how constructing
an irreducible Markov chain (MC) associated to each option avoids this problem.

(cid:17)(cid:27)(cid:41)

(s)

 

(3)

3.1

Irreducible Markov Chains Associated to Options

Options as absorbing Markov chains. A natural way to address SUCRL’s limitations is to avoid
considering options as atomic operations (as in SMDPs) but take into consideration their inner (MDP)
structure. Since options terminate in ﬁnite time (Asm. 1)  they can be seen as an absorbing Markov
reward process whose state space contains all states that are reachable by the option and where option
terminal states are absorbing states of the MC (see Fig. 1). More formally  for any option o the set
of inner states So includes the initial state so and all states s with βo(s) < 1 that are reachable by
executing πo from so (e.g.  So = {s0  s1} in Fig. 1)  while the set of absorbing states Sabs
includes
all states with βo(s) > 0 (e.g.  Sabs
o = {s0  s1  s2} in Fig. 1). The absorbing MC associated to o is

o

3

1 − p

1 − p

β2

a0

p

β1

a0

p

β0

s0

a1

. . .

s1

a1

. . .

(1−p)(1−β0)

(1−p)(1−β1)

p(s0|s0  o)

p(s2|s0  o)

a0

. . .

s2

a1

. . .

s0

. . .

o

p(s1|s0  o)

s1

. . .

so 0

(1−p)β0

(1−β1)p

β1p

So

so 1

p

(1−p)β1

s0

1

s1

1

s2

Sabs

o

1

p(cid:48)

so 0

(1−β1)p

p(cid:48)(cid:48)

. . .

s2

. . .

(1−p)(1−β1)

so 1

Figure 1: (upper-left) MDP with an option o starting from s0 and executing a0 in all states with termination
probabilities βo(s0) = β0  βo(s1) = β1 and βo(s2) = 1. (upper-right) SMDP dynamics associated to option o.
(lower-left) Absorbing MC associated to options o. (lower-right) Irreducible MC obtained by transforming the
associated absorbing MC with p(cid:48) = (1 − β0)(1 − p) + β0(1 − p) + pβ1 and p(cid:48)(cid:48) = β1(1 − p) + p.

characterized by a transition matrix Po of dimension (|So| + |Sabs

o |) × (|So| + |Sabs

o |) deﬁned as3

Po =

with

Qo(s  s(cid:48)) = (1 − βo(s(cid:48)))p(s(cid:48)|s  πo(s)) for any s  s(cid:48) ∈ So
Vo(s  s(cid:48)) = βo(s(cid:48))p(s(cid:48)|s  πo(s)) for any s ∈ So  s(cid:48) ∈ Sabs
o  

(cid:20)Qo Vo

(cid:21)

0

I

o

|So| × |So|)  Vo is the transition
where Qo is the transition matrix between inner states (dim.
o |)  and I is the identity matrix (dim.
matrix from inner states to absorbing states (dim. |So| × |Sabs
o |). As proved in Lem. 3 in [14]  the expected cumulative rewards R(s  o)  the duration
o | × |Sabs
|Sabs
τ (s  o)  and the sub-Exponential parameters (σr(o)  br(o)) and (στ (o)  bτ (o)) are directly related to
the transition matrices Qo and Vo of the associated absorbing chain Po. This suggests that  given an
estimate of Po  we could directly derive the corresponding estimates of R(s  o) and τ (s  o). Following
this idea  we could “propagate” conﬁdence intervals on the entries of Po to obtain conﬁdence intervals
on rewards and duration estimates without any prior knowledge on their parameters and thus solve
Eq. 3 without any prior knowledge. Nonetheless  intervals on Po do not necessarily translate into

compact bounds for R and τ. For example  if the value (cid:101)Vo = 0 belongs to the conﬁdence interval of
(cid:101)Po (no state in Sabs
can be reached)  the corresponding optimistic estimates (cid:101)R(s  o) and(cid:101)τ (s  o) are

unbounded and Eq. 3 is ill-deﬁned.
Options as irreducible Markov chains. We ﬁrst notice from Eq. 2 that computing the optimal
policy only requires computing the ratio R(s  o)/τ (s  o) and the inverse 1/τ (s  o). Starting from Po 
we can construct an irreducible MC whose stationary distribution is directly related to these terms.
We proceed as illustrated in Fig. 1: all terminal states are “merged” together and their transitions
are “redirected” to the initial state so. More formally  let 1 be the all-one vector of dimension
|Sabs
o |  then vo = Vo1 ∈ R|So| contains the cumulative probability to transition from an inner state
to any terminal state. Then the chain Po can be transformed into a MC with transition matrix
P (cid:48)
o = [vo Q(cid:48)
o is now an irreducible
MC as any state can be reached starting from any other state and thus it admits a unique stationary
distribution µo. In order to relate µo to the optimality equation in Eq. 2  we need an additional
assumption on the options.
Assumption 2. For any option o ∈ O  the starting state so is also a terminal state (i.e.  βo (so) = 1)
and any state s(cid:48) ∈ S with βo(s(cid:48)) < 1 is an inner state (i.e.  s(cid:48) ∈ So).

o contains all but the ﬁrst column of Qo. P (cid:48)

o] ∈ RSo×So  where Q(cid:48)

3In the following we only focus on the dynamics of the process; similar deﬁnitions apply for the rewards.

4

Input: Conﬁdence δ ∈]0  1[  rmax  S  A  O
For episodes k = 1  2  ... do
1. Set ik := i  t = tk and episode counters νk(s  a) = 0  νk(s  o) = 0

o k (cid:98)rk(s  a) and their conﬁdence intervals in Eq. 6

2. Compute estimates(cid:98)pk(s(cid:48)|s  o)  (cid:98)P (cid:48)
3. Compute an k-approximation of the optimal optimistic policy(cid:101)πk of Eq. 5
(a) Execute option oi =(cid:101)πk(si)  obtain primitive rewards r1
4. While ∀l ∈ [t + 1  t + τi]  νk(sl  al) < Nk(sl  al) do
(b) Set νk(si  oi) += 1  i += 1  t += τi and νk(s  πoi (s)) += 1 for all s ∈ {s1
5. Set Nk(s  o) += νk(s  o) and Nk(s  a) += νk(s  a)

i   ...  rτi

i and visited states s1
i   ...  sτi
i }
i   ...  sτi

i = si+1

Figure 2: The general structure of FSUCRL.

While the ﬁrst part has a very minor impact on the deﬁnition of O  the second part of the assumption
guarantees that options are “well designed” as it requires the termination condition to be coherent
with the true inner states of the option  so that if βo(s(cid:48)) < 1 then s(cid:48) should be indeed reachable by the
option. Further discussion about Asm. 2 is reported in App. A. We then obtain the following property.
Lemma 1. Under Asm. 2  let µo ∈ [0  1]So be the unique stationary distribution of the irreducible
MC P (cid:48)

o associated to option o  then 4
∀s ∈ S  ∀o ∈ Os 

1

= µo(s)

and

R(s  o)
τ (s  o)

=

τ (s  o)

r(s(cid:48)  πo(s(cid:48)

))µo(s(cid:48)

).

(4)

(cid:88)

s(cid:48)∈So

(cid:16)

s(cid:48)∈So

(cid:40)

= max
o∈Os

(s)

 

(5)

max(cid:101)bo

max(cid:101)µo (cid:101)ro

(cid:101)ro (s(cid:48)

)(cid:101)µo(s(cid:48)

(cid:17)(cid:27)(cid:41)

(cid:8)(cid:101)b
o(cid:101)u∗(cid:9) −(cid:101)u∗

(cid:124)

(cid:26) (cid:88)

This lemma illustrates the relationship between the stationary distribution of P (cid:48)
Eq. 2.5 As a result  we can apply Lem. 1 to Eq. 3 and obtain the optimistic optimality equation

o and the key terms in

) +(cid:101)µo(s)

∀s ∈ S (cid:101)ρ∗
where(cid:101)ro (s(cid:48)) =(cid:101)r (s(cid:48)  πo(s(cid:48))) and(cid:101)bo = ((cid:101)p(s(cid:48)|s  o))s(cid:48)∈S. Unlike in the absorbing MC case  where
compact conﬁdence sets for Po may lead to unbounded optimistic estimates for (cid:101)R and(cid:101)τ  in this for-

mulation µo(s) can be equal to 0 (i.e.  inﬁnite duration and cumulative reward) without compromising
the solution of Eq. 5. Furthermore  estimating µo implicitly leverages over the correlation between
cumulative reward and duration  which is ignored when estimating R(s  o) and τ (s  o) separately.
Finally  we prove the following result.

Lemma 2. Let(cid:101)ro ∈ R (cid:101)bo ∈ P  and(cid:101)µo ∈ M  with R  P  M compact sets containing the true
parameters ro  bo and µo  then the optimality equation in Eq. 5 always admits a unique solution(cid:101)ρ∗
and(cid:101)ρ∗ ≥ ρ∗ (i.e.  the solution of Eq. 5 is an optimistic gain).
Now  we need to provide an explicit algorithm to compute the optimistic optimal gain(cid:101)ρ∗ of Eq. 5 and

its associated optimistic policy. In the next section  we introduce two alternative algorithms that are
guaranteed to compute an -optimistic policy.

3.2

SUCRL with Irreducible Markov Chains

The structure of the UCRL-like algorithm for learning with options but with no prior knowledge on
distribution parameters (called FREE-SUCRL  or FSUCRL) is reported in Fig. 2. Unlike SUCRL we
do not directly estimate the expected cumulative reward and duration of options but we estimate the
SMDP transition probabilities p(s(cid:48)|s  o)  the irreducible MC P (cid:48)
o associated to each option  and the
state-action reward r(s  a). For all these terms we can compute conﬁdence intervals (Hoeffding and
empirical Bernstein) without any prior knowledge as

4Notice that since option o is deﬁned in s  then s = so. Furthermore r is the MDP expected reward.
5Lem. 4 in App. D extends this result by giving an interpretation of µo(s(cid:48))  ∀s(cid:48) ∈ So.

5

k(s  a) ∝ rmax

(cid:12)(cid:12)r(s  a) −(cid:98)rk(s  a)(cid:12)(cid:12) ≤ βr
(cid:12)(cid:12)p(s
(cid:48)|s  o)(cid:12)(cid:12) ≤ βp
(cid:48)|s  o) −(cid:98)pk(s
)(cid:12)(cid:12) ≤ βP
(cid:12)(cid:12)P
) − (cid:98)P

(cid:48)
o k(s  s

(cid:48)
o(s  s

(cid:48)

(cid:48)

(cid:48)
k(s  o  s

(cid:48)
k (s  o  s

(cid:115)

) ∝

) ∝

log(SAtk/δ)

(cid:115)
(cid:115)

 

Nk(s  a)

2(cid:98)pk(s(cid:48)|s  o)(cid:0)1 −(cid:98)pk(s(cid:48)|s  o))ctk δ
o k(s  s(cid:48))(cid:0)1 − (cid:98)P (cid:48)
2(cid:98)P (cid:48)

o k(s  s(cid:48)))dtk δ

Nk(s  o)

Nk(s  πo(s))

(6a)

(6b)

+

7ctk δ

3Nk(s  o)

 

+

7dtk δ

3Nk(s  πo(s))

 

(6c)

(cid:40)(cid:88)

(cid:40)
max(cid:101)µo

(cid:101)ro (s(cid:48)

)(cid:101)µo(s(cid:48)

) +(cid:101)µo(s)

(cid:110)(cid:101)b

(cid:124)
ouj

(cid:111) − uj(s)

(cid:19)(cid:41)(cid:41)

(cid:18)

max(cid:101)bo

where Nk(s  a) (resp. Nk(s  o)) is the number of samples collected at state-action s  a (resp. state-
option s  o) up to episode k  Eq. 6a coincides with the one used in UCRL  in Eq. 6b s = so
and s(cid:48) ∈ S  and in Eq. 6c s  s(cid:48) ∈ So. Finally  we set ctk δ = O (log (SOtk)/δ)) and dtk δ =
O (log (|So| log(tk)/δ)) [18  Eq. 31].
To obtain an actual implementation of the algorithm reported on Fig. 2 we need to deﬁne a procedure
to compute an approximation of Eq. 5 (step 3). Similar to UCRL and SUCRL  we deﬁne an EVI
algorithm starting from a function u0(s) = 0 and computing at each iteration j

+uj(s) 

(7)

uj+1(s) = max
o∈Os

(cid:124)

s(cid:48)∈So

where(cid:101)ro(s(cid:48)) is the optimistic reward (i.e.  estimate plus the conﬁdence bound of Eq. 6a) and the
optimistic transition probability vector(cid:101)bo is computed using the algorithm introduced in [19  App.

o(cid:98)P (cid:48)
o = (cid:98)µ

(cid:107)µo −(cid:98)µo(cid:107)1 ≤ βµ

o  let(cid:98)µo be the solution of (cid:98)µ

A] for Bernstein bound as in Eqs. 6b  6c or in [15  Fig. 2] for Hoeffding bound (see App. B).
Depending on whether conﬁdence intervals for µo are computed explicitly or implicitly we can deﬁne
two alternative implementations that we present below.

Explicit conﬁdence intervals. Given the estimate (cid:98)P (cid:48)
constraint(cid:98)µ

o under
o is computed after terminating
the option at least once and is thus irreducible. The perturbation analysis in [20] can be applied to
derive the conﬁdence interval

oe = e. Such a(cid:98)µo always exists and is unique since (cid:98)P (cid:48)
o − (cid:98)P (cid:48)

where (cid:107)·(cid:107)∞ 1 is the maximum of the (cid:96)1-norm of the rows of the transition matrix (cid:98)κo min is the
smallest condition number6 for the (cid:96)1-norm of µo. Let ζo ∈ R|So| be such that ζo(so) =(cid:101)ro(so) +
(cid:9) − uj(so) and ζo(s) = (cid:101)ro(s)  then the maximum over (cid:101)µo in Eq. 7 has the same
max(cid:101)bo
Alg. [15  Fig. 2] with parameters(cid:98)µo  βµ
form as the innermost maximum over bo (with Hoeffding bound) and thus we can directly apply
k (o)  and states So ordered descendingly according to ζo. The
resulting value is then directly plugged into Eq. 7 and uj+1 is computed. We refer to this algorithm
as FSUCRLV1.
Nested extended value iteration. An alternative approach builds on the observation that the maxi-
mum over µo in Eq. 7 can be seen as the optimization of the average reward (gain)

k (o) :=(cid:98)κo min(cid:107)P (cid:48)

o(cid:107)∞ 1 

(cid:8)(cid:101)b

(cid:124)
ouj

(8)

(cid:124)

(cid:124)

(cid:101)ρ∗
o(uj) = max(cid:101)µo

(cid:40)(cid:88)

s(cid:48)∈So

(cid:41)

)(cid:101)µo(s(cid:48)

ζo(s(cid:48)

)

 

(9)

(cid:40)

state space So  an action space composed of the option action (i.e.  πo(s))  and transitions (cid:101)P (cid:48)

where ζo is deﬁned as above. Eq. 9 is indeed the optimal gain of a bounded-parameter MDP with
o in the

conﬁdence intervals 7 of Eq. 6c  and thus we can write its optimality equation

(cid:101)ρ∗
o(uj) = max(cid:101)P (cid:48)
(cid:98)κo min = τ1((cid:98)Zo) = maxi j
2(cid:107)(cid:98)Zo(i  :)−(cid:98)Zo(j  :)(cid:107)1 where (cid:98)Zo(i  :) is the i-th row of (cid:98)Zo = (I −(cid:98)P (cid:48)
7The conﬁdence intervals on (cid:101)P (cid:48)

(cid:124)(cid:98)µo)−1.
6The provably smallest condition number (refer to [21  Th. 2.3]) is the one provided by Seneta [22]:
o can never exclude a non-zero transition between any two states of So. There-
o(uj) is state-independent.

fore  the corresponding bounded-parameter MDP is always communicating and ρ∗

(cid:101)P (cid:48)
o(s  s(cid:48)

)(cid:101)w∗
o(s(cid:48)

− (cid:101)w∗

(cid:88)

ζo(s) +

(cid:41)

o(s) 

(10)

o + 1

s(cid:48)

)

1

o

6

until the stopping condition lo
vanishing sequence. As wo

where (cid:101)w∗
bounded-parameter MDP  thus avoiding to explicitly construct the conﬁdence intervals of(cid:101)µo. As a

o is an optimal bias. For any input function v we can compute ρ∗

result  we obtain two nested EVI algorithms where  starting from an initial bias function v0(s) = 0 
8 at any iteration j we set the bias function of the inner EVI to wo
j 0(s) = 0 and we compute (see
App. C.3 for the general EVI for bounded-parameter MDPs and its guarantees)

o(v) by using EVI on the

(cid:124)

(cid:111)

(cid:110)
ζo(s) + (cid:101)Po(·|s(cid:48))
j l+1−wo
(cid:1)(cid:111)
o(vj) with l  the outer EVI becomes

wo
j l

 

j l+1(s(cid:48)
wo
j = inf{l ≥ 0 : sp{wo
j l converges to ρ∗

) = max(cid:101)Po
(cid:110)
g(cid:0)wo

j l+1 − wo

j l} ≤ εj} is met  where (εj)j≥0 is a

(11)

vj+1(s) = max
o∈Os

(12)
where g : v (cid:55)→ 1
2 (max{v} + min{v}). In App. C.4 we show that this nested scheme  that we
call FSUCRLV2  converges to the solution of Eq. 5. Furthermore  if the algorithm is stopped when

j +1 − wo
sp{vj+1 − vj} + εj ≤ ε then |(cid:101)ρ∗ − g(vj+1 − vj)| ≤ ε/2.

+ vj(s) 

j lo
j

j lo

One of the interesting features of this algorithm is its hierarchical structure. Nested EVI is operating
on two different time scales by iteratively considering every option as an independent optimistic
planning sub-problem (EVI of Eq. 11) and gathering all the results into a higher level planning
problem (EVI of Eq. 12). This idea is at the core of the hierarchical approach in RL  but it is not
always present in the algorithmic structure  while nested EVI naturally arises from decomposing
Eq. 7 in two value iteration algorithms. It is also worth to underline that the conﬁdence intervals

implicitly generated for(cid:101)µo are never worse than those in Eq. 8 and they are often much tighter. In

practice the bound of Eq. 8 may be actually worse because of the worst-case scenario considered in
the computation of the condition numbers (see Sec. 5 and App. F).

4 Theoretical Analysis

Before stating the guarantees for FSUCRL  we recall the deﬁnition of diameter of M and MO:

D = max

s s(cid:48)∈S min

)(cid:3)  DO = max

s s(cid:48)∈SO

π:S→O E(cid:2)τπ(s  s(cid:48)

min

)(cid:3) 

where τπ(s  s(cid:48)) is the (random) number of primitive actions to move from s to s(cid:48) following policy π.
We also deﬁne a pseudo-diameter characterizing the “complexity” of the inner dynamics of options:

π:S→A E(cid:2)τπ(s  s(cid:48)
(cid:101)DO =
(cid:9)   κ∞
(cid:8)κ1

o

r∗κ1∗ + τmaxκ∞
∗

√

µ∗

(cid:26)

(cid:27)

= min
o∈O

min
s∈So

µo(s)

where we deﬁne:

r∗

= max

o∈O {sp(ro)}   κ1∗ = max
o∈O

∗ = max

o∈O {κ∞

o }   and µ∗

o and κ∞

with κ1
o the condition numbers of the irreducible MC associated to options o (for the (cid:96)1 and
(cid:96)∞-norm respectively [20]) and sp(ro) the span of the reward of the option. In App. D we prove the
following regret bound.
Theorem 1. Let M be a communicating MDP with reward bounded between 0 and rmax = 1 and
let O be a set of options satisfying Asm. 1 and 2 such that σr(s  o) ≤ σr  στ (s  o) ≤ στ   and
τ (s  o) ≤ τmax. We also deﬁne BO = maxs o supp(p(·|s  o)) (resp. B = maxs a supp(p(·|s  a)) as
the largest support of the SMDP (resp. MDP) dynamics. Let Tn be the number of primitive steps
executed when running FSUCRLV2 over n decision steps  then its regret is bounded as

√

(cid:123)(cid:122)

∆p

(cid:125)

(cid:124)

(cid:123)(cid:122)

∆R τ

√
n

(cid:125)

SATn + (cid:101)DO
(cid:123)(cid:122)

√

(cid:124)

∆µ

√

DO

SBOOn

+ (σr + στ )

+

SBOTn

(13)

(cid:19)

(cid:125)

∆(FSUCRL  n) = (cid:101)O

(cid:18)

(cid:124)

8We use vj instead of uj since the error in the inner EVI directly affects the value of the function at the outer

EVI  which thus generates a sequence of functions different from (uj).

7

Comparison to SUCRL. Using the conﬁdence intervals of Eq. 6b and a slightly tighter analysis than
the one by Fruit and Lazaric [14] (Bernstein bounds and higher accuracy for EVI) leads to a regret
bound for SUCRL as

∆(SUCRL  n) = (cid:101)O

(cid:16)
∆p + ∆R τ +(cid:0)σ+
(cid:124)

(cid:1)√

(cid:123)(cid:122)

r + σ+
τ
∆(cid:48)

R τ

(cid:17)

(cid:125)

SAn

 

(14)

√

r and σ+

where σ+
τ are upper-bounds on σr and στ that are used in deﬁning the conﬁdence intervals for
τ and R that are actually used in SUCRL. The term ∆p is the regret induced by errors in estimating
the SMDP dynamics p(s(cid:48)|s  o)  while ∆R τ summarizes the randomness in the cumulative reward
and duration of options. Both these terms scale as
n  thus taking advantage of the temporal
abstraction (i.e.  the ratio between the number of primitive steps Tn and the decision steps n). The
main difference between the two bounds is then in the last term  which accounts for the regret due to
the optimistic estimation of the behavior of the options. In SUCRL this regret is linked to the upper
bounds on the parameters of R and τ. As shown in Thm.2 in [14]  when σ+
τ = στ   the
bound of SUCRL is nearly-optimal as it almost matches the lower-bound  thus showing that ∆(cid:48)
R τ
is unavoidable. In FSUCRL however  the additional regret ∆µ comes from the estimation errors of
the per-time-step rewards ro and the dynamic P (cid:48)
o. Similar to ∆p  these errors are ampliﬁed by the

pseudo-diameter (cid:101)DO. While ∆µ may actually be the unavoidable cost to pay for removing the prior
knowledge about options  it is interesting to analyze how (cid:101)DO changes with the structure of the options

r = σr and σ+

(see App. E for a concrete example). The probability µo(s) decreases as the probability of visiting
an inner state s ∈ So using the option policy. In this case  the probability of collecting samples on
the inner transitions is low and this leads to large estimation errors for P (cid:48)
o. These errors are then
propagated to the stationary distribution µo through the condition numbers κ (e.g.  κ1
o directly follows
from an non-empirical version of Eq. 8). Furthermore  we notice that 1/µo(s) ≥ τo(s) ≥ |So| 
suggesting that “long” or “big” options are indeed more difﬁcult to estimate. On the other hand  ∆µ
becomes smaller whenever the transition probabilities under policy πo are supported over a few states
(B small) and the rewards are similar within the option (sp(ro) small). While in the worst case ∆µ
may actually be much bigger than ∆(cid:48)
R τ when the parameters of R and τ are accurately known (i.e. 
r ≈ σr)  in Sect. 5 we show scenarios in which the actual performance of FSUCRL
τ ≈ στ and σ+
σ+
is close or better than SUCRL and the advantage of learning with options is preserved.
To explain why FSUCRL can perform better than SUCRL we point out that FSUCRL’s bound is
somewhat worst-case w.r.t. the correlation between options. In fact  in Eq. 6c the error in estimating
P (cid:48)
o in a state s does not scale with the number of samples obtained while executing option o but
those collected by taking the primitive action prescribed by πo. This means that even if o has a low
probability of reaching s starting from so (i.e.  µo(s) is very small)  the true error may still be small
as soon as another option o(cid:48) executes the same action (i.e.  πo(s) = πo(cid:48)(s)). In this case the regret
bound is loose and the actual performance of FSUCRL is much better. Therefore  although it is
not apparent in the regret analysis  not only is FSUCRL leveraging on the correlation between the
cumulative reward and duration of a single option  but it is also leveraging on the correlation between
different options that share inner state-action pairs.
Comparison to UCRL. We recall that the regret of UCRL is bounded as O(D
SBATn)  where
Tn is to the total number of steps. As discussed by [14]  the major advantage of options is in terms
of temporal abstraction (i.e.  Tn (cid:29) n) and reduction of the state-action space (i.e.  SO < S and
O < A). Eq.(13) also reveals that options can also improve the learning speed by reducing the size of
the support BO of the dynamics of the environment w.r.t. primitive actions. This can lead to a huge
improvement e.g.  when options are designed so as to reach a speciﬁc goal. This potential advantage
is new compared to [14] and matches the intuition on “good” options often presented in the literature
(see e.g.  the concept of “funnel” actions introduced by Dietterich [23]).

Bound for FSUCRLV1. Bounding the regret of FSUCRLV1 requires bounding the empirical(cid:98)κ in
Eq. (8) with the true condition number κ. Since(cid:98)κ tends to κ as the number of samples of the option
(cid:1)
(cid:0)κ1

increases  the overall regret would only be increased by a lower order term. In practice however 
FSUCRLV2 is preferable to FSUCRLV1. The latter will suffer from the true condition numbers
o∈O since they are used to compute the conﬁdence bounds on the stationary distributions
(µo)o∈O  while for FSUCRLV2 they appear only in the analysis. As much as the dependency on the
diameter in the analysis of UCRL  the condition numbers may also be loose in practice  although
tight from a theoretical perspective. See App.D.6 and experiments for further insights.

√

o

8

Figure 3: (Left) Regret after 1.2 · 108 steps normalized w.r.t. UCRL for different option durations in a 20x20
grid-world. (Right) Evolution of the regret as Tn increases for a 14x14 four-rooms maze.

5 Numerical Simulations

In this section we compare the regret of FSUCRL to SUCRL and UCRL to empirically verify
the impact of removing prior knowledge about options and estimating their structure through the
irreducible MC transformation. We consider the toy domain presented in [14] that was speciﬁcally
designed to show the advantage of temporal abstraction and the classical 4-rooms maze [1]. To be
able to reproduce the results of [14]  we run our algorithm with Hoeffding conﬁdence bounds for the
(cid:96)1-deviation of the empirical distribution (implying that BO has no impact). We consider settings
where ∆R τ is the dominating term of the regret (refer to App. F for details).
When comparing the two versions of FSUCRL to UCRL on the grid domain (see Fig. 3 (left))  we
empirically observe that the advantage of temporal abstraction is indeed preserved when removing
the knowledge of the parameters of the option. This shows that the beneﬁt of temporal abstraction is
not just a mere artifact of prior knowledge on the options. Although the theoretical bound in Thm. 1
is always worse than its SMDP counterpart (14)  we see that FSUCRL performs much better than
SUCRL in our examples. This can be explained by the fact that the options we use greatly overlap.
Even if our regret bound does not make explicit the fact that FSUCRL exploits the correlation between
options  this can actually signiﬁcantly impact the result in practice. The two versions of SUCRL
differ in the amount of prior knowledge given to the algorithm to construct the parameters σ+
r and
τ that are used in building the conﬁdence intervals.In v3 we provide a tight upper-bound rmax
σ+
on the rewards and distinct option-dependent parameters for the duration (τo and στ (o))  in v2 we
only provide a global (option-independent) upper bound on τo and σo. Unlike FSUCRL which is
“parameter-free”  SUCRL is highly sensitive to the prior knowledge about options and can perform
even worse than UCRL. A similar behaviour is observed in Fig. 3 (right) where both the versions
of SUCRL fail to beat UCRL but FSUCRLV2 has nearly half the regret of UCRL. On the contrary 
FSUCRLV1 suffers a linear regret due to a loose dependency on the condition numbers (see App. F.2).
This shows that the condition numbers appearing in the bound of FSUCRLV2 are actually loose. In
both experiments  UCRL and FSUCRL had similar running times meaning that the improvement in
cumulative regret is not at the expense of the computational complexity.

6 Conclusions

We introduced FSUCRL  a parameter-free algorithm to learn in MDPs with options by combining
the SMDP view to estimate the transition probabilities at the level of options (p(s(cid:48)|s  o)) and the
MDP structure of options to estimate the stationary distribution of an associated irreducible MC
which allows to compute the optimistic policy at each episode. The resulting regret matches SUCRL
bound up to an additive term. While in general  this additional regret may be large  we show both
theoretically and empirically that FSUCRL is actually competitive with SUCRL and it retains the
advantage of temporal abstraction w.r.t. learning without options. Since FSUCRL does not require
strong prior knowledge about options and its regret bound is partially computable  we believe the
results of this paper could be used as a basis to construct more principled option discovery algorithms
that explicitly optimize the exploration-exploitation performance of the learning algorithm.

9

246810120.60.70.80.911.1MaximaldurationofoptionsTmaxRatioofregretsRUCRLFSUCRLv1FSUCRLv2SUCRLv2SUCRLv30246810·1080123·106DurationTnCumulativeRegret∆(Tn)UCRLFSUCRLv1FSUCRLv2SUCRLv2SUCRLv3Acknowledgments

This research was supported in part by French Ministry of Higher Education and Research  Nord-Pas-
de-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn
(n.ANR-14-CE24-0010-01).

References
[1] Richard S. Sutton  Doina Precup  and Satinder Singh. Between mdps and semi-mdps: A
framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence  112(1):
181 – 211  1999.

[2] Amy McGovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement
learning using diverse density. In Proceedings of the Eighteenth International Conference on
Machine Learning  pages 361–368  2001.

[3] Ishai Menache  Shie Mannor  and Nahum Shimkin. Q-cut—dynamic discovery of sub-goals in
reinforcement learning. In Proceedings of the 13th European Conference on Machine Learning 
Helsinki  Finland  August 19–23  2002  pages 295–306. Springer Berlin Heidelberg  2002.

[4] Özgür ¸Sim¸sek and Andrew G. Barto. Using relative novelty to identify useful temporal abstrac-
tions in reinforcement learning. In Proceedings of the Twenty-ﬁrst International Conference on
Machine Learning  ICML ’04  2004.

[5] Pablo Samuel Castro and Doina Precup. Automatic construction of temporally extended actions
for mdps using bisimulation metrics. In Proceedings of the 9th European Conference on Recent
Advances in Reinforcement Learning  EWRL’11  pages 140–152  Berlin  Heidelberg  2012.
Springer-Verlag.

[6] Kﬁr Y. Levy and Nahum Shimkin. Uniﬁed inter and intra options learning using policy gradient
In EWRL  volume 7188 of Lecture Notes in Computer Science  pages 153–164.

methods.
Springer  2011.

[7] Munu Sairamesh and Balaraman Ravindran. Options with exceptions. In Proceedings of the
9th European Conference on Recent Advances in Reinforcement Learning  EWRL’11  pages
165–176  Berlin  Heidelberg  2012. Springer-Verlag.

[8] Timothy Arthur Mann  Daniel J. Mankowitz  and Shie Mannor. Time-regularized interrupting
options (TRIO). In Proceedings of the 31th International Conference on Machine Learning 
ICML 2014  volume 32 of JMLR Workshop and Conference Proceedings  pages 1350–1358.
JMLR.org  2014.

[9] Chen Tessler  Shahar Givony  Tom Zahavy  Daniel J. Mankowitz  and Shie Mannor. A deep
hierarchical approach to lifelong learning in minecraft. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence  February 4-9  2017  San Francisco  California  USA. 
pages 1553–1561. AAAI Press  2017.

[10] Martin Stolle and Doina Precup. Learning options in reinforcement learning. In SARA  volume

2371 of Lecture Notes in Computer Science  pages 212–223. Springer  2002.

[11] Timothy A. Mann and Shie Mannor. Scaling up approximate value iteration with options: Better
policies with fewer iterations. In Proceedings of the 31th International Conference on Machine
Learning  ICML 2014  volume 32 of JMLR Workshop and Conference Proceedings  pages
127–135. JMLR.org  2014.

[12] Nicholas K. Jong  Todd Hester  and Peter Stone. The utility of temporal abstraction in rein-
forcement learning. In The Seventh International Joint Conference on Autonomous Agents and
Multiagent Systems  May 2008.

[13] Emma Brunskill and Lihong Li. PAC-inspired Option Discovery in Lifelong Reinforcement
Learning. In Proceedings of the 31st International Conference on Machine Learning  ICML
2014  volume 32 of JMLR Proceedings  pages 316–324. JMLR.org  2014.

[14] Ronan Fruit and Alessandro Lazaric. Exploration–exploitation in mdps with options.

In
Proceedings of Machine Learning Research  volume 54: Artiﬁcial Intelligence and Statistics 
20-22 April 2017  Fort Lauderdale  FL  USA  pages 576–584  2017.

[15] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

10

[16] A. Federgruen  P.J. Schweitzer  and H.C. Tijms. Denumerable undiscounted semi-markov
decision processes with unbounded rewards. Mathematics of Operations Research  8(2):298–
313  1983.

[17] Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation
for markov decision processes. Journal of Computer and System Sciences  74(8):1309–1331 
December 2008.

[18] Daniel J. Hsu  Aryeh Kontorovich  and Csaba Szepesvári. Mixing time estimation in reversible
markov chains from a single sample path. In Proceedings of the 28th International Conference
on Neural Information Processing Systems  NIPS 15  pages 1459–1467. MIT Press  2015.

[19] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-
ment learning. In Proceedings of the 28th International Conference on Neural Information
Processing Systems  NIPS 15  pages 2818–2826. MIT Press  2015.

[20] Grace E. Cho and Carl D. Meyer. Comparison of perturbation bounds for the stationary
distribution of a markov chain. Linear Algebra and its Applications  335(1):137 – 150  2001.
[21] Stephen J. Kirkland  Michael Neumann  and Nung-Sing Sze. On optimal condition numbers for

markov chains. Numerische Mathematik  110(4):521–537  Oct 2008.

[22] E. Seneta. Sensitivity of ﬁnite markov chains under perturbation. Statistics & Probability

Letters  17(2):163–168  May 1993.

[23] Thomas G. Dietterich. Hierarchical reinforcement learning with the maxq value function

decomposition. Journal of Artiﬁcial Intelligence Research  13:227–303  2000.

[24] Ronald Ortner. Optimism in the face of uncertainty should be refutable. Minds and Machines 

18(4):521–526  2008.

[25] Pierre Bremaud. Applied Probability Models with Optimization Applications  chapter 3: Recur-

rence and Ergodicity. Springer-Verlag Inc  Berlin; New York  1999.

[26] Pierre Bremaud. Applied Probability Models with Optimization Applications  chapter 2:

Discrete-Time Markov Models. Springer-Verlag Inc  Berlin; New York  1999.

[27] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  New York  NY  USA  1st edition  1994.

[28] Peter L. Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  UAI ’09  pages 35–42. AUAI Press  2009.

[29] Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral

methods. Electronic Journal of Probability  20  2015.

[30] Martin Wainwright. Course on Mathematical Statistics  chapter 2: Basic tail and concentration

bounds. University of California at Berkeley  Department of Statistics  2015.

11

,Ronan Fruit
Matteo Pirotta
Alessandro Lazaric
Emma Brunskill