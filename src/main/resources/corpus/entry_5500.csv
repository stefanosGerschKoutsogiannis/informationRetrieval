2019,The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection,Modern photo editing tools allow creating realistic manipulated images easily. While fake images can be quickly generated  learning models for their detection is challenging due to the high variety of tampering artifacts and the lack of large labeled datasets of manipulated images. In this paper  we propose a new framework for training of discriminative segmentation model via an adversarial process. We simultaneously train four models: a generative retouching model G_R that translates manipulated image to the real image domain  a generative annotation model G_A that estimates the pixel-wise probability of image patch being either real or fake  and two discriminators D_R and D_A that qualify the output of G_R and G_A. The aim of model G_R is to maximize the probability of model G_A making a mistake. Our method extends the generative adversarial networks framework with two main contributions: (1) training of a generative model G_R against a deep semantic segmentation network G_A that learns rich scene semantics for manipulated region detection  (2) proposing per class semantic loss that facilitates semantically consistent image retouching by the G_R. We collected large-scale manipulated image dataset to train our model. The dataset includes 16k real and fake images with pixel-level annotations of manipulated areas. The dataset also provides ground truth pixel-level object annotations. We validate our approach on several modern manipulated image datasets  where quantitative results and ablations demonstrate that our method achieves and surpasses the state-of-the-art in manipulated image detection. We made our code and dataset publicly available.,The Point Where Reality Meets Fantasy: Mixed
Adversarial Generators for Image Splice Detection

Vladimir V. Kniaz1 2  Vladimir A. Knyaz1 2

1State Res. Institute of Aviation Systems (GosNIIAS)

125319  7  Victorenko str.  Moscow  Russia

{vl.kniaz  knyaz}@gosniias.ru

2Moscow Institute of Physics and Technology (MIPT)

141701  9 Institutskiy per.  Dolgoprudny  Russia

Fabio Remondino

Fondazione Bruno Kessler (FBK)
Via Sommarive 18  Trento  Italy

remondino@fbk.eu

Abstract

Modern photo editing tools allow creating realistic manipulated images easily.
While fake images can be quickly generated  learning models for their detection
is challenging due to the high variety of tampering artifacts and the lack of large
labeled datasets of manipulated images. In this paper  we propose a new framework
for training of discriminative segmentation model via an adversarial process. We
simultaneously train four models: a generative retouching model GR that translates
manipulated image to the real image domain  a generative annotation model GA that
estimates the pixel-wise probability of image patch being either real or fake  and two
discriminators DR and DA that qualify the output of GR and GA. The aim of model
GR is to maximize the probability of model GA making a mistake. Our method
extends the generative adversarial networks framework with two main contributions:
(1) training of a generative model GR against a deep semantic segmentation
network GA that learns rich scene semantics for manipulated region detection  (2)
proposing per class semantic loss that facilitates semantically consistent image
retouching by the GR. We collected large-scale manipulated image dataset to
train our model. The dataset includes 16k real and fake images with pixel-level
annotations of manipulated areas. The dataset also provides ground truth pixel-
level object annotations. We validate our approach on several modern manipulated
image datasets  where quantitative results and ablations demonstrate that our
method achieves and surpasses the state-of-the-art in manipulated image detection.
We made our code and dataset publicly available 1.

1

Introduction

While every image captured by the human eye is real  digital photos can be easily manipulated to
present scenes that never existed in reality. Such manipulated image can be easily generated by
copying the part of one image into another. This image manipulation is called an image splice and
can be used maliciously to create fake news or change historical photos [1]. Recent research [1  2]
suggests that training a model for splice localization is more challenging than other types of object
detection problems as the domain of manipulated images is extensive and diverse. Therefore  the
collection of the representative training dataset is difﬁcult. Moreover  the forger can adapt to the
detection algorithm by changing the manipulation technique. This principle is used in Generative
Adversarial Networks (GANs) to train a generator network to synthesize images from noise [3]  text
descriptions [4]  scene graphs [5] or by image-to-image translation [6  7  8  9]. Fake images produced
by the generator are evaluated against real images by an adversarial discriminator network that learns

1http://zefirus.org/MAG

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Manipulated Image Ground Truth

CFA [10]

LSC [1]

Ours

Figure 1: Comparison against two state-of-the-art methods on our FantasticReality dataset (Sec-
tion 3.3). Our results are shown in the last column. Zoom in for details.

to classify them as ‘real’ or ‘fake.’ Even though no ‘fake’ images exist in the training dataset  the
discriminator successfully learns to detect them during the training process. We hypothesize that
adversarial training of an image-to-image translation generator against a splice localization generator
can improve the splice localization accuracy.
In this paper  we propose a Mixed Adversarial Generators (MAG) framework  in which we simultane-
ously train four models: a generative retoucher GR  an adversarial generative annotator GA  and two
discriminators DR and DA that qualify the output of GR and GA. The aim of our retoucher GR is
suppressing image tampering artifacts in the input image splices from the training dataset. We train
our adversarial annotator GA to predict splice localization masks in the ‘retouched’ images generated
by the retoucher GR. The adversarial loss provided by the annotator GA forces the retoucher GR
to mask those particular tampering artifacts that allow GA to detect the image splice. Unlike other
splice detection models  our annotator GA learns to adapt to changing tampering techniques of the
retoucher GR. Therefore  our annotator GA receives a new sample from the manipulated image
domain every iteration. Moreover  with the increasing epoch samples are becoming more complex.
To further increase the splice localization rate  we train our annotator GA to predict object classes for
the input image. Resulting semantic labeling is used to provide a semantic consistency loss for the
retoucher GR. The semantic loss forces the output of the retoucher GR to present objects of the same
semantic classes as the input image.
Our adversarial generators extend the GAN framework with two key contributions: (1) training of a
generative model GR against a deep semantic segmentation network GA that learns rich scene seman-
tics for manipulated region detection  (2) proposing per class semantic loss that facilitates semantically
consistent image retouching by the GR. Unlike the recently proposed Sem-GAN model [11]  we do
not use the pertained segmentation model but train it adversarially. We perform a comprehensive
evaluation of our MAG framework  where quantitative results and ablations demonstrate that our
annotator GA achieves and surpasses the state-of-the-art in splice localization on several challenging
image splice datasets (see Figure 1 and 3).
We evaluate our retoucher GR on image-to-image translation tasks to demonstrate that our MAG
framework is not limited to the splice localization task. Semantic loss function allows us to train
challenging image-to-image translation tasks that are unfeasible for baselines. We also introduce a
new FantasticReality dataset that includes 16k image splices with pixel-level ground truth annotations

2

of manipulated areas  and instance and class labels for ten object categories. We made our code and
the dataset publicly available.

2 Related Work

Splice detection. Modern splice detection methods fall into three categories: tampering artifacts-
based approaches leverage local discrepancies in image noise [12  13  14  15  16  17  18]  compression
artifacts [19  20  21  22]  or camera’s color ﬁlter array inconsistencies [10  23  24  25  26  27  28  29 
30  31] to detect tampered image regions; consistency-based methods [32  1] compare pairs of local
image patches to localize image areas  where predicted camera model [33  32] or image metadata [1]
are inconsistent with the rest of the image; deep learning-based methods [34  35  1  2  36] detect
image splice regions either by comparison of local patches in a siamese network [1] or using fully
convolutional networks [2] to predict labeling of the tampered regions. While many digital image
forensic datasets were introduced recently [37  38  39  40  41]  they usually include only several
hundreds of photos and do not provide enough of training data for modern methods. Related to our
multi-task annotation prediction  Salloum et al. [2] have proposed multi-task training to localize
tampered regions and their edges.
Image-to-image translation. Modern methods for image generation conditioned by an input image
are trained in either supervised [6  42  43  11  44]  unsupervised [7  8  9  45  46  47] or mixed [48]
setting. Unsupervised approaches are trained on an unpaired dataset leveraging the latent space
assumption [8]  the cycle consistency loss [7] or other criteria to learn a mapping from source
to target domain. Recent research demonstrates exciting progress in multimodal image-to-image
translation [49  42]. Related to our semantic consistency loss function are the loss functions proposed
in Sem-GAN [11] and InstaGAN [50] models. Unlike our MAG framework Sem-GAN model leverages a
pretrained segmentation model to provide semantic loss. Unlike InstaGAN [50] model  our retoucher
generator GR does not require instance masks as an input. Closely related to our retoucher generator
GR  Mejjati et al. [9] propose to use attention guided training to perform translation only for the
target object.
Most of the modern approaches in the image-to-image translation are based on Generative Adversarial
Networks [3]  which can capture the sample distribution in the target domain using an adversarial game
of two players. Recent research demonstrates that GANs can solve more challenging tasks than image-
to-image translation. They can learn complex transforms between physically different domains such as
image-to-thermal translation [51  52  53  54  55]  image-to-voxel model transformation [56  57]  and
image synthesis from audio data [58]. In our MAG framework  we replace the discriminator network
with an adversarial annotator generator GA. While the discriminator predicts a scalar probability of
an input image being either real or fake  our annotator generator GA predicts a pixel-level probability
map of an image patch being either authentic image or splice.

3 Mixed Adversarial Generators

Our goal is training two generator networks adversarially: a splice retoucher GR and a splice
localization annotator GA. We consider three domains: the input domain A ∈ RW×H×3 of potentially
manipulated images  the authentic domain B ∈ RW×H×3 of untampered images  and the output
domain C ∈ [0  1]W×H×(2+K) of splice localization and class segmentation masks  where K is the
number of predicted object classes. While an image A ∈ A may be either authentic or tampered 
all images B ∈ B are authentic  B ⊂ A. We use assumptions made by Salloum et al. [2] as the
starting point for our generator GA. Speciﬁcally  we train our generator GA for multi-task prediction
of splice segmentation mask Cm ∈ [0  1]W×H  splice edge mask Ce ∈ [0  1]W×H  and object class
segmentation Cs ∈ [0  1]W×H×K. Therefore  we learn a mapping GA : (A) → C  where A ∈ A
is an input potentially manipulated image  C ∈ C is an output tensor obtained by concatenation
of Cm  Ce  Cs. The goal of our retoucher generator GR is learning a mapping from manipulated
image domain A to the authentic domain B. To this end  the aim of adversarial training of the
GR is maximizing the probability of an annotator GA making a mistake in splice detection of the
retouched image ˆB. We believe that the retoucher GR in the training loop facilitates our annotator
GA to learn complicated splice retouching approaches. We use attention-guided learning assumption
made by Mejjati et al. [9] as the starting point for our retoucher GR. We observe the similarity
between the attention map proposed by Mejjati et al. [9] and the alpha channel used for the splice

3

Figure 2: Our proposed pipeline: We want our annotator GA to predict annotations correctly for
three kinds of images: retouched spliced images ˆB = GR(A) (1)  original spliced images A ∈ A
from the training dataset (2)  and authentic images B ∈ B (3). Our retoucher GR learns to hide a
wide range of tampering artifacts such as modern-to-retro photo translation  blurring of tampering
edges  and compensating light source inconsistencies. During the training  we feed manipulated
images A  retouched images ˆB  and authentic images B to our splice localization annotator GA.

generation. We hypothesize that attention-guided learning of our retoucher GR allows us to model
splice generation with layers in photo-editing applications  e.g.  GIMP or Photoshop. We learn a
mapping GR : (A) → ( ˆBrgb  ˆBα)  where ˆBrgb ∈ RW×H×3 is an image with the retouched splice
area  and ˆBα ∈ [0  1]W×H is the attention map. We obtain the target retouched splice image ˆB
similarly to [9] by

ˆB = ˆBα (cid:12) ˆBrgb + (1 − ˆBα) (cid:12) A 

(1)

where (cid:12) is an element-wise product. Our proposed pipeline is presented in Figure 2. We train two
discriminator networks DR  DA to provide adversarial losses for the output of our generators GR
and GA. The architecture and the loss function of the retoucher GR are presented in Section 3.1 
whereas the structured loss function of the annotator GA is described in Section 3.2.

3.1 Retoucher Generator GR

Architecture. We use the U-Net generator architecture [59] as the starting point for our retoucher
GR. While skip connections of the U-Net generator facilitate robust learning of tampering techniques
by our retoucher GR  deconvolutional layers often introduce checkerboard artifacts in output images.
Our annotator GA quickly learns checkerboard features to detect images produced by our retoucher
GR. To avoid such a scenario  we replaced deconvolutional layers with an upsample layer followed by
a convolutional layer  inspired by the architecture proposed in [60]. We term the resulting architecture
that is free from the checkerboard artifacts as U-Net-UC (see supplementary material Table 1).
Loss function. Three loss functions govern the training process for our retoucher GR: LGA
and LDR

sem LGA
adv 
adv  where a superscript indicates the network providing the loss. The aim of our semantic

4

 ÂB̂Cc GR∑ GÂCm̂CêBrgb̂Bα1−̂BαCcLbal+LDAadvCmCeLsem+LGAadv+LDRadvB1.2. GA GA3.B ĈBCACBAInput potentially manipulated imageOutput class segmentationRetouched image color channelsÂBrgb̂Bα̂Cm̂CêCcAuthentic image for the loss function and for step 3BRetouched image attention map̂BRetouched manipulated imageOutput edges of detected manipulated areasOutput detected manipulated areasGround truth outputLossDeep networkwhere 0W H is the a splice localization ﬁlled with zeros. Finally  we use a discriminator’s DR
adversarial loss function to make our image realistic globally

LGA
adv(Cm  ˆCm) = EB∼p(B)

LDR
adv( ˆB) = EB∼p(B)

(cid:21)

 

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)0W H − ˆCm
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1
(cid:105)

log(1 − DR( ˆB))

.

(cid:104)

(2)

(3)

(4)

(5)

(6)

consistency loss function LGA
by our annotator GA

sem is to make the classes of objects in the output image ˆB recognizable

LGA
sem(Cs  ˆCs) = EB∼p(B)

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Cs − ˆCs

(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1

 

where ˆCs = GA( ˆB)s is the class segmentation produced by our annotator GA  Cs is the ground
truth class segmentation. Our adversarial annotator loss LGA
adv stimulates our retoucher GR to mask
tampering artifacts in the input sliced images. In other words  we want to maximize the probability of
our annotator GA making a mistake in splice localization ˆCm = GA( ˆB)m

We obtain the ﬁnal energy to be optimized by combining all losses

LR(Cs  ˆCs  Cm  ˆCm  ˆB) = λGA

sem + λGA

sem · LGA
sem = 10  λGA

adv · LGA
adv = 10  λDR

adv + λDR

adv · LDR
adv 

adv = 0.25 in our experiments.

where we use the loss hyper-parameters λGA

3.2 Annotator Generator GA
Loss function. We train our annotator GA utilizing a combination of our balanced L1 loss function
Lbal and an adversarial loss LDA
adv. We observe that training our annotator GA using the L1 distance
||C − ˆC|| between the ground-truth and predicted annotations results in a large number of false
negatives in splice localizations. We hypothesize that making the penalty for false negatives and false
positives equal for each image can improve the overall splice localization score. We implement this
hypothesis in our balanced loss function based on the Dice loss [61]

Lbal(C  ˆC) =

2+K(cid:88)
(cid:124)

i=1

|Ci ∩ (1 − ˆCi)|

|Ci|

(cid:123)(cid:122)

2+K(cid:88)
(cid:124)

i=1

+

(cid:125)

|(1 − Ci) ∩ ˆCi|

|1 − Ci|

(cid:123)(cid:122)

 

(cid:125)

False negatives

False positives

where i is the index of an annotation channel. Channel C1 provides a splice mask annotation Cm 
channel C2 provides a splice edges annotation Ce. The predicted class labels are given by Ci for
i ∈ {3  4  . . .   2 + K}  where K is the number of classes (K = 10 in our experiments). The area of
predicted annotations (white area) in the channel Ci is given by |Ci|  the background area (black
area) in the channel Ci is given by |1 − Ci|.
We want our annotator GA to predict annotations correctly for three kinds of images: original spliced
images A ∈ A from the training dataset  retouched spliced images ˆB = GR(A)  and authentic
images B ∈ B. Therefore  for each iteration  we evaluate the loss Lbal on three pairs of ground
truth and predicted annotations: (CA(cid:48)
  ˆC ˆB)  (CB  ˆCB). We use the superscript to
denote the corresponding color image for the annotation. Please  note that both original spliced
image A and the retouched spliced image ˆB have the same annotation CA(cid:48)
with an adversarial class
segmentation mask C A
m). We want to train our annotator GA to predict class
s
segmentation adversarially: it must generate the correct class annotations only for authentic image
areas and predict empty class annotations for manipulated regions. Speciﬁcally  we multiply our
ground truth semantic segmentation C A
m). The

s by an inverted splice localization mask (1 − C A

s (cid:12) (1 − C A

  ˆCA)  (CA(cid:48)

= C A

(cid:48)

5

multiplication by the inverted mask leaves authentic areas untouched and removes annotations for
manipulated regions.
The aim of our adversarial loss LDA
provided by a conditional discriminator DA with PatchGAN architecture [6]

adv is to avoid blurry output splice localization masks [6]. It is

LDA
adv( ˆB  ˆC

ˆB) = EB∼p(B)

log(1 − DA( ˆB  ˆC

ˆB))

.

We obtain the resulting energy to optimize by combining four loss functions

(cid:16)Lbal(CA(cid:48)

LA = λbal

(cid:104)

(cid:105)

(cid:17)

(7)

ˆB)  (8)

  ˆCA) + Lbal(CA(cid:48)

  ˆC

ˆB) + Lbal(CB  ˆCB)

+ λDA

adv · LDA

adv( ˆB  ˆC

where we use the loss hyper-parameters λbal = 1  λDA

adv = 1 in our experiments.

3.3 FantasticReality Dataset

We collected large-scale image tampering dataset with 16k authentic and 16k tampered images to
perform extensive training and evaluation of our MAG model. Compared to previous datasets [37  38 
39  40]  our FantasticReality dataset is more extensive in terms of scene variety and image count.
To the best of our knowledge  it is the ﬁrst tampering dataset that provides both tampering masks
and instance and class labels for each image. For each authentic and tampered image  we manually
generated instance and class segmentation for ten object classes: person  car  truck  van  bus  building 
cat  dog  tram  boat. Examples from the dataset are presented in Figure 1 in the supplementary
material.

4 Experiments

We perform extensive experiments to evaluate our MAG model on splice localization. We compare
our model to three modern state-of-the-art deep learning splice detection frameworks: ManTra [62] 
LSC [1]  MFCN [2]. We provide a comparison to non-deep learning methods to be consistent with LSC:
NOI [18]  CFA [10]  DCT [19]. ManTra-Net (ManTra) [62] is a self-supervised model that learns
to classify 385 image manipulation types. Learned Self-Consistency (LSC) [1] is a self-supervised
model. Multi-Task Fully Convolutional Network (MFCN) [2] leverages a deep two-stream architecture
to predict splice mask and splice edge mask. Noise Variance (NOI) [18] leverages wavelet analysis
to detect inconsistency in noise patterns. Color Filter Array (CFA) [10] searches for inconsistencies
in artifacts of demosaicking algorithm to detect tampered regions. JPEG DCT [19] leverages incon-
sistencies of JPEG blocking artifacts to detect tampered image regions. For the LSC algorithm  we
use a pertained model provided by authors. We implemented the MFCN model and train it on the
training split of our FantasticReality dataset. We train our MAG model on the ‘Rough’ split of our
FantasticReality dataset. We use a batch size of one and an Adam solver with initial learning rate of
2 · 10−4. We trained our MAG model for 400 epochs.
We perform evaluation on ﬁve manipulated image datasets CASIA v2.0 [37]  Carvalho [38] 
Columbia [39]  Realistic Tampering [40] and our FantasticReality dataset. For the fair evalua-
tion  we downscale all images to match the input size 512 × 512 of our annotator generator GR. We
use the downscaled images to evaluate all baselines and our framework. If two images are used for
splice generation  the choice of ‘authentic’ and ‘tampered’ regions is ambiguous. To avoid ambiguity 
we follow the method proposed in [1]. Namely  we compare the areas of the ‘background’ image
and the ‘pasted’ images. We deﬁne the smaller region as the tampered. If the regions are equal  we
calculate the mAP score for the original tampering mask and an inverted mask. We use the higher
score and term it permuted mAP (p-mAP) similar to [1]. For additional details on the evaluation
protocol  please  refer to the supplementary material. Furthermore  we perform ablation studies to
demonstrate the inﬂuence of each component of our framework on the resulting performance.

4.1 Annotator Generator GA Evaluation
Splice Localization. We evaluate our model and baselines on the task of splice localization using
ground-truth masks of spliced regions. Speciﬁcally  we want our model to predict a per-pixel

6

CASIA v2.0 [37]

Carv. [38]

Realistic Tampering [40]

t
u
p
n
I

T
G

]
1
[
C
S
L

]
8
1
[
I
O
N

]
9
1
[
T
C
D

s
r
u
O

Figure 3: Comparison against the state-of-the-art methods on image splices from CASIAv2  Carvalho
and Realistic Tampering datasets. Our results are shown in the last row. Zoom in for details.

RT

[40] Carvalho

CASIA v2.0 [37] Columbia [39]
[38] FantasticReality
Dataset
mAP p-mAP cIOU mAP p-mAP cIOU mAP p-mAP cIOU mAP p-mAP cIOU mAP p-mAP cIOU
Metric
0.36
0.32 0.41
LSC
0.48
0.37 0.40
CFA
0.29 0.45
0.29
NOI
0.41
0.35 0.39
LSC
0.46
0.36 0.41
MFCN
0.73
0.40 0.40
ManTra
0.41 0.35
0.36
No GR
0.21
Single-task 0.12 0.15
0.74 0.74
0.76
Ours

0.41 0.33 0.47
0.44 0.45 0.48
0.40 0.48 0.45
0.43 0.37 0.49
0.42 0.41 0.51
0.58 0.50 0.50
0.29 0.28 0.35
0.21 0.17 0.16
0.77 0.50 0.51

0.52 0.15 0.33
0.49 0.32 0.32
0.50 0.21 0.31
0.48 0.16 0.37
0.36 0.42 0.36
0.54 0.33 0.33
0.18 0.20 0.37
0.14 0.19 0.15
0.55 0.48 0.48

0.24 0.17 0.45
0.33 0.45 0.50
0.21 0.18 0.49
0.25 0.26 0.51
0.37 0.40 0.51
0.38 0.57 0.57
0.29 0.27 0.45
0.18 0.12 0.17
0.56 0.61 0.61

0.47 0.25 0.44
0.42 0.39 0.44
0.46 0.26 0.43
0.35 0.22 0.42
0.48 0.27 0.45
0.45 0.48 0.48
0.12 0.34 0.32
0.24 0.11 0.19
0.76 0.69 0.69

Table 1: Splice Localization: We evaluate our model on 5 datasets using mean average precision
(mAP  permuted-mAP) over pixels and per class IOU (cIOU).

probability of an image patch being tampered. We present results in terms of mAP  permuted
mAP [1]  and per class Intersection over Union (cIOU) in Table 1 and in Figure 3. Our MAG model
achieves state-of-the-art in splice localization on all datasets. The LSC model fails to detect splices
when authentic and spliced regions originate from the same camera model and share similar camera
metadata.
Ablation Study. We evaluate the necessity of all components of our MAG framework by comparing the
splice localization accuracy of several ablated versions of our model presented in Table 1 and Figure 4.
Firstly  we evaluate the performance of annotator GA trained without retoucher GR (No GR). Both
qualitative and quantitative results demonstrate that the competition of two generators is the critical

7

Manipulated Image Ground Truth

No GR

Single-task

Original

Figure 4: Qualitative results for ablated versions of our MAG framework evaluated on Realistic
Tampering dataset.

component of our MAG framework. Secondly  we evaluate our framework trained for the single task
of predicting splice area annotations. The results prove that multi-task training outperforms the
single-task version of our model (Single-task).

4.2 Semantic-guided Retoucher Generator GR Evaluation

Examples in Figures 5 and 6 demonstrate how our retoucher GR gradually removes the tampering
artifacts in the input splice A with an increasing epoch. While other deep learning splice detection
methods receive both realistic and rough splices from the ﬁrst training epoch  our annotator GA
sees only rough splices at the ﬁrst epoch. With an increasing epoch  retoucher GR produces more
complicated splices  which allows GA to focus attention on the sophisticated tampering techniques
that could appear in real splices. We believe that this is the main reason why our MAG framework
achieves state-of-the-art results and outperforms other deep learning methods.

A

ˆB = GR(A)

GT

ˆC = GA(ˆB)

ˆB

ˆC

Epoch 100

Epoch 300

ˆB

ˆC

h
g
u
o
R

c
i
t
s
i
l
a
e
R

Figure 5: Performance for retoucher GR on rough
and realistic splices.

Figure 6: Adaptation of annotator GA over time.

5 Conclusion

We showed how adversarial training based on a learning retoucher generator in the loop could help
a splice localization model to learn a wide range of image manipulations. Our mixed adversarial
generators extend the generative adversarial networks framework by replacing a scalar value fake
prediction discriminator with a pixel-level fake region annotator. The proposed retoucher generator
is trained simultaneously with an annotator generator trying to maximize the probability of the
annotator to make a mistake. Such adversarial training improves the annotator splice localization rate
as it observes changing image manipulation techniques through the training process. Furthermore 
the competition of two generators allows the retoucher generator to achieve the state-of-the-art
performance in image-to-image translation tasks. Our main observation is that semantic-guided
training allows our splice localization annotator to reason explicitly about splices and their semantic
consistency  and achieve and surpass the state-of-the-art methods in splice localization on several
challenging datasets.

8

Acknowledgments

The reported study was funded by the Russian Science Foundation (RSF) according to the research
project No 19-11-11008 and the Russian Foundation for Basic Research (RFBR) according to the
research project No 17-29-04509 . We want to thank Belgian Surrealist artist René Magritte for
teaching us through his art how to ﬁnd the point where fantasy meets reality.

References

[1] Minyoung Huh  Andrew Liu  Andrew Owens  and Alexei A Efros. Fighting Fake News: Image Splice
Detection via Learned Self-Consistency. In The European Conference on Computer Vision (ECCV) 
September 2018.

[2] Ronald Salloum  Yuzhuo Ren  and C C Jay Kuo. Image Splicing Localization using a Multi-task Fully
Convolutional Network (MFCN). Journal of Visual Communication and Image Representation  51:201–
209  February 2018.

[3] Ian J Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron C

Courville  and Yoshua Bengio. Generative Adversarial Networks. CoRR  2014.

[4] Han Zhang  Tao Xu  Hongsheng Li  Shaoting Zhang  Xiaogang Wang  Xiaolei Huang  and Dimitris N.
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In The IEEE International Conference on Computer Vision (ICCV)  Oct 2017.

[5] Justin Johnson  Agrim Gupta  and Li Fei-Fei. Image Generation From Scene Graphs. In The IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  June 2018.

[6] Phillip Isola  Jun-Yan Zhu  Tinghui Zhou  and Alexei A Efros. Image-to-Image Translation with Conditional
Adversarial Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 5967–5976. IEEE  2017.

[7] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networkss. In Computer Vision (ICCV)  2017 IEEE International Conference
on  2017.

[8] Ming-Yu Liu  Thomas Breuel  and Jan Kautz. Unsupervised image-to-image translation networks. In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors 
Advances in Neural Information Processing Systems 30  pages 700–708. Curran Associates  Inc.  2017.

[9] Youssef Alami Mejjati  Christian Richardt  James Tompkin  Darren Cosker  and Kwang In Kim. Unsuper-
vised attention-guided image-to-image translation. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages
3693–3703. Curran Associates  Inc.  2018.

[10] P. Ferrara  T. Bianchi  A. De Rosa  and A. Piva. Image forgery localization via ﬁne-grained analysis of cfa

artifacts. IEEE Transactions on Information Forensics and Security  7(5):1566–1577  Oct 2012.

[11] Anoop Cherian and Alan Sullivan. Sem-gan: Semantically-consistent image-to-image translation. In IEEE
Winter Conference on Applications of Computer Vision  WACV 2019  Waikoloa Village  HI  USA  January
7-11  2019  pages 1797–1806  2019.

[12] Bo Liu and Chi-Man Pun. Splicing Forgery Exposure in Digital Image by Detecting Noise Discrepancies.

International Journal of Computer and Communication Engineering  4(1):33–38  2015.

[13] Bo Liu  Chi-Man Pun  and Xiao-Chen Yuan. Digital Image Forgery Detection Using JPEG Features and

Local Noise Discrepancies. The Scientiﬁc World Journal  2014(6):1–12  2014.

[14] Chi-Man Pun  Bo Liu  and Xiao-Chen Yuan. Multi-scale Noise Estimation for Image Splicing Forgery

Detection. Journal of Visual Communication and Image Representation  38(C):195–206  July 2016.

[15] Thibaut Julliand  Vincent Nozick  and Hugues Talbot. Image Noise and Digital Image Forensics. In
Yun-Qing Shi  Hyoung Joong Kim  Fernando Pérez-González  and Isao Echizen  editors  Digital-Forensics
and Watermarking  pages 3–17  Cham  2016. Springer International Publishing.

[16] Wu-Chih Hu  Wei-Hao Chen  Deng-Yuan Huang  and Ching-Yu Yang. Novel Detection of Image Forgery
for Exchanged Foreground and Background Using Image Watermarking Based on Alpha Matte. In 2012
Sixth International Conference on Genetic and Evolutionary Computing  ICGEC 2012  Kitakyushu  Japan 
August 25-28  2012  pages 245–248  2012.

[17] Miroslav Goljan  Jessica J. Fridrich  and Rémi Cogranne. Rich model for steganalysis of color images. In
2014 IEEE International Workshop on Information Forensics and Security  WIFS 2014  Atlanta  GA  USA 
December 3-5  2014  pages 185–190  2014.

9

[18] Babak Mahdian and Stanislav Saic. Using noise inconsistencies for blind image forensics. Image and
Vision Computing  27(10):1497 – 1503  2009. Special Section: Computer Vision Methods for Ambient
Intelligence.

[19] S. Ye  Q. Sun  and E. Chang. Detecting digital image forgeries by measuring inconsistencies of blocking

artifact. In 2007 IEEE International Conference on Multimedia and Expo  pages 12–15  July 2007.

[20] Y Su  J Zhang  and J Liu. Exposing Digital Video Forgery by Detecting Motion-Compensated Edge
Artifact. In 2009 International Conference on Computational Intelligence and Software Engineering  pages
1–4  December 2009.

[21] Wu-Chih Hu and Wei-Hao Chen. Effective forgery detection using DCT+SVD-based watermarking for

region of interest in key frames of vision-based surveillance. IJCSE  8(4):297–305  2013.

[22] Ashima Gupta  Nisheeth Saxena  and Sunil Kumar. Detecting Copy move Forgery using DCT. International

Journal of Scientiﬁc and Research Publications  3:2250–3153  May 2013.

[23] Harpreet Kaur  Jyoti Saxena  and Sukhjinder Singh. Simulative comparison of copy-move forgery detection
methods for digital images. International Journal of Electronics  Electrical and Computational System 
4:62–66  2015.

[24] Bo Liu and Chi-Man Pun. HSV Based Image Forgery Detection for Copy-Move Attack. In Mechatronics
Engineering  Computing and Information Technology  pages 2825–2828. Trans Tech Publications Ltd  July
2014.

[25] Feng Zeng  Wei Wang  Min Tang  and Zhanghua Cao. Exposing Blurred Image Forgeries through Blind
Image Restoration. In 10th International Conference on P2P  Parallel  Grid  Cloud and Internet Computing 
3PGCIC 2015  Krakow  Poland  November 4-6  2015  pages 466–469  2015.

[26] Wu-Chih Hu  Wei-Hao Chen  Deng-Yuan Huang  and Ching-Yu Yang. Effective image forgery detection
of tampered foreground or background image based on image watermarking and alpha mattes. Multimedia
Tools and Applications  75(6):3495–3516  March 2016.

[27] Xin Wang  Bo Xuan  and Si-long Peng. Digital Image Forgery Detection Based on the Consistency of
Defocus Blur. In 2008 Fourth International Conference on Intelligent Information Hiding and Multimedia
Signal Processing (IIH-MSP)  pages 192–195. IEEE  July 2008.

[28] Aniket Roy  Rahul Dixit  Ruchira Naskar  and Rajat Subhra Chakraborty. Copy-Move Forgery Detection
with Similar But Genuine Objects. In Digital Image Forensics: Theory and Implementation  pages 65–77.
Springer Singapore  Singapore  2020.

[29] Irene Amerini  Lamberto Ballan  Roberto Caldelli  Alberto Del Bimbo  Luca Del Tongo  and Giuseppe
Serra. Copy-move forgery detection and localization by means of robust clustering with J-Linkage. Signal
Processing : Image Communication  28(6):659–669  July 2013.

[30] Longyin Wen  Honggang Qi  and Siwei Lyu. Contrast enhancement estimation for digital image forensics.
ACM Transactions on Multimedia Computing  Communications and Applications  14(2):1–21  May 2018.
[31] I-Cheng Chang  J Cloud Yu  and Chih-Chuan Chang. A forgery detection algorithm for exemplar-based
inpainting images using multi-region relation. Image and Vision Computing  31(1):57–71  January 2013.
[32] L. Bondi  S. Lameri  D. Güera  P. Bestagini  E. J. Delp  and S. Tubaro. Tampering detection and localization
through clustering of camera-based cnn features. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW)  pages 1855–1864  July 2017.

[33] L. Bondi  L. Barofﬁo  D. Güera  P. Bestagini  E. J. Delp  and S. Tubaro. First steps toward camera model
identiﬁcation with convolutional neural networks. IEEE Signal Processing Letters  24(3):259–263  March
2017.

[34] J. H. Bappy  A. K. Roy-Chowdhury  J. Bunk  L. Nataraj  and B. S. Manjunath. Exploiting spatial structure
for localizing manipulated image regions. In 2017 IEEE International Conference on Computer Vision
(ICCV)  pages 4980–4989  Oct 2017.

[35] Belhassen Bayar and Matthew C. Stamm. A deep learning approach to universal image manipulation
detection using a new convolutional layer. In Proceedings of the 4th ACM Workshop on Information Hiding
and Multimedia Security  IH&MMSec 2016  Vigo  Galicia  Spain  June 20-22  2016  pages 5–10  2016.

[36] Peng Zhou  Xintong Han  Vlad I Morariu  and Larry S Davis. Learning Rich Features for Image
Manipulation Detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
June 2018.

[37] Jing Dong  Wei Wang  and Tieniu Tan. CASIA image tampering detection evaluation database. In 2013
IEEE China Summit and International Conference on Signal and Information Processing  ChinaSIP 2013 
Beijing  China  July 6-10  2013  pages 422–426  2013.

[38] T. J. d. Carvalho  C. Riess  E. Angelopoulou  H. Pedrini  and A. d. R. Rocha. Exposing digital image
forgeries by illumination color classiﬁcation. IEEE Transactions on Information Forensics and Security 
8(7):1182–1194  July 2013.

10

[39] Tian-Tsong Ng and Shih-Fu Chang. A data set of authentic and spliced image blocks. Technical report 

Columbia University  June 2004.

[40] P. Korus and J. Huang. Multi-scale analysis strategies in prnu-based tampering localization. IEEE Trans.

on Information Forensics & Security  2017.

[41] B Wen  Y Zhu  R Subramanian  T Ng  X Shen  and S Winkler. COVERAGE — A novel database for
copy-move forgery detection. In 2016 IEEE International Conference on Image Processing (ICIP)  pages
161–165. IEEE  September 2016.

[42] Jun-Yan Zhu  Richard Zhang  Deepak Pathak  Trevor Darrell  Alexei A Efros  Oliver Wang  and Eli Shecht-
man. Toward multimodal image-to-image translation. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach 
R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural Information Processing Systems
30  pages 465–476. Curran Associates  Inc.  2017.

[43] Yunjey Choi  Minje Choi  Munyoung Kim  Jung-Woo Ha  Sunghun Kim  and Jaegul Choo. Stargan:
In The IEEE

Uniﬁed generative adversarial networks for multi-domain image-to-image translation.
Conference on Computer Vision and Pattern Recognition (CVPR)  June 2018.

[44] Patsorn Sangkloy  Jingwan Lu  Chen Fang  Fisher Yu  and James Hays. Scribbler: Controlling deep image
synthesis with sketch and color. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  July 2017.

[45] Amelie Royer  Konstantinos Bousmalis  Stephan Gouws  Fred Bertsch  Inbar Mosseri  Forrester Cole  and

Kevin Murphy. XGAN: Unsupervised image-to-image translation for many-to-many mappings  2018.

[46] Matthew Amodio and Smita Krishnaswamy. Travelgan: Image-to-image translation by transformation

vector learning. CoRR  abs/1902.09631  2019.

[47] Wayne Wu  Kaidi Cao  Cheng Li  Chen Qian  and Chen Change Loy. Transgaga: Geometry-aware

unsupervised image-to-image translation. CoRR  abs/1904.09571  2019.

[48] Soumya Tripathy  Juho Kannala  and Esa Rahtu. Learning image-to-image translation using paired and

unpaired training samples. arXiv preprint arXiv:1805.03189  2018.

[49] Xun Huang  Ming-Yu Liu  Serge Belongie  and Jan Kautz. Multimodal unsupervised image-to-image

translation. In ECCV  2018.

[50] Sangwoo Mo  Minsu Cho  and Jinwoo Shin. Instagan: Instance-aware image-to-image translation. In

International Conference on Learning Representations  2019.

[51] He Zhang  Vishal M Patel  Benjamin S Riggan  and Shuowen Hu. Generative adversarial network-based
synthesis of visible faces from polarimetrie thermal faces. In 2017 IEEE International Joint Conference on
Biometrics (IJCB)  pages 100–107. IEEE  2017.

[52] Teng Zhang  Arnold Wiliem  Siqi Yang  and Brian C Lovell. TV-GAN: Generative Adversarial Network

Based Thermal to Visible Face Recognition. December 2017.

[53] Vladimir V. Kniaz  Vladimir A. Knyaz  Jiˇrí Hlad˚uvka  Walter G. Kropatsch  and Vladimir Mizginov.
Thermalgan: Multimodal color-to-thermal image translation for person re-identiﬁcation in multispectral
dataset. In Laura Leal-Taixé and Stefan Roth  editors  Computer Vision – ECCV 2018 Workshops  pages
606–624  Cham  2019. Springer International Publishing.

[54] V. V. Kniaz and A. N. Bordodymov. Long wave infrared image colorization for person re-identiﬁcation.
ISPRS - International Archives of the Photogrammetry  Remote Sensing and Spatial Information Sciences 
XLII-2/W12:111–116  2019.

[55] Vladimir V. Kniaz and Vladimir A. Knyaz. Chapter 6 - multispectral person re-identiﬁcation using gan for
color-to-thermal image translation. In Michael Ying Yang  Bodo Rosenhahn  and Vittorio Murino  editors 
Multimodal Scene Understanding  pages 135 – 158. Academic Press  2019.

[56] Vladimir A. Knyaz  Vladimir V. Kniaz  and Fabio Remondino. Image-to-voxel model translation with
conditional adversarial networks. In Laura Leal-Taixé and Stefan Roth  editors  Computer Vision – ECCV
2018 Workshops  pages 601–618  Cham  2019. Springer International Publishing.

[57] V. V. Kniaz  F. Remondino  and V. A. Knyaz. Generative adversarial networks for single photo 3d
ISPRS - International Archives of the Photogrammetry  Remote Sensing and Spatial

reconstruction.
Information Sciences  XLII-2/W9:403–408  2019.

[58] Chia-Hung Wan  Shun-Po Chuang  and Hung-yi Lee. Towards audio to scene image synthesis using
In IEEE International Conference on Acoustics  Speech and Signal

generative adversarial network.
Processing  ICASSP 2019  Brighton  United Kingdom  May 12-17  2019  pages 496–500  2019.

[59] Olaf Ronneberger  Philipp Fischer  and Thomas Brox. U-Net: Convolutional Networks for Biomedical

Image Segmentation. Springer International Publishing  Cham  2015.

11

[60] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for improved
quality  stability  and variation. In 6th International Conference on Learning Representations  ICLR 2018 
Vancouver  BC  Canada  April 30 - May 3  2018  Conference Track Proceedings  2018.

[61] W. R. Crum  O. Camara  and D. L. G. Hill. Generalized overlap measures for evaluation and validation in

medical image analysis. IEEE Transactions on Medical Imaging  25(11):1451–1461  Nov 2006.

[62] Wael AbdAlmageed Yue Wu and Premkumar Natarajan. Mantra-net: Manipulation tracing network
for detection and localization of image forgerieswith anomalous features. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2019.

12

,Yao-Liang Yu
Chris Oates
Steven Niederer
Angela Lee
François-Xavier Briol
Mark Girolami
Vladimir Kniaz
Vladimir Knyaz
Fabio Remondino