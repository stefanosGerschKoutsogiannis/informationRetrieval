2017,Affinity Clustering: Hierarchical Clustering at Scale,Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular  identifying a good hierarchical structure is at the same time a fundamental and challenging problem for several applications. The amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data.  The main focus of this paper is on minimum spanning tree (MST) based clusterings. In particular  we propose affinity  a novel hierarchical clustering based on Boruvka's MST algorithm. We prove certain theoretical guarantees for affinity (as well as some other classic algorithms) and show that in practice it is superior to several other state-of-the-art clustering algorithms.   Furthermore  we present two MapReduce implementations for affinity. The first one works for the case where the input graph is dense and takes constant rounds. It is based on a Massively Parallel MST algorithm for dense graphs that improves upon the state-of-the-art algorithm of Lattanzi et al. (SPAA 2011). Our second algorithm has no assumption on the density of the input graph and finds the affinity clustering in $O(\log n)$ rounds using Distributed Hash Tables (DHTs). We show experimentally that our algorithms are scalable for huge data sets  e.g.  for graphs with trillions of edges.,Afﬁnity Clustering: Hierarchical Clustering at Scale

MohammadHossein Bateni

Google Research

bateni@google.com

Soheil Behnezhad∗
University of Maryland
soheil@cs.umd.edu

Mahsa Derakhshan∗
University of Maryland
mahsaa@cs.umd.edu

MohammadTaghi Hajiaghayi∗

University of Maryland
hajiagha@cs.umd.edu

Raimondas Kiveris

Google Research

rkiveris@google.com

Silvio Lattanzi
Google Research

silviol@google.com

Vahab Mirrokni
Google Research

mirrokni@google.com

Abstract

Graph clustering is a fundamental task in many data-mining and machine-learning
pipelines. In particular  identifying a good hierarchical structure is at the same time
a fundamental and challenging problem for several applications. The amount of
data to analyze is increasing at an astonishing rate each day. Hence there is a need
for new solutions to efﬁciently compute effective hierarchical clusterings on such
huge data.
The main focus of this paper is on minimum spanning tree (MST) based clusterings.
In particular  we propose afﬁnity  a novel hierarchical clustering based on Bor˚uvka’s
MST algorithm. We prove certain theoretical guarantees for afﬁnity (as well as
some other classic algorithms) and show that in practice it is superior to several
other state-of-the-art clustering algorithms.
Furthermore  we present two MapReduce implementations for afﬁnity. The ﬁrst
one works for the case where the input graph is dense and takes constant rounds. It
is based on a Massively Parallel MST algorithm for dense graphs that improves
upon the state-of-the-art algorithm of Lattanzi et al. [34]. Our second algorithm has
no assumption on the density of the input graph and ﬁnds the afﬁnity clustering in
O(log n) rounds using Distributed Hash Tables (DHTs). We show experimentally
that our algorithms are scalable for huge data sets  e.g.  for graphs with trillions of
edges.

1

Introduction

Clustering is a classic unsupervised learning problem with many applications in information retrieval 
data mining  and machine learning. In hierarchical clustering the goal is to detect a nested hierarchy
of clusters that unveils the full clustering structure of the input data set. In this work we study the
hierarchical clustering problem on real-world graphs. This problem has received a lot of attention
in recent years [13  16  41] and new elegant formulations and algorithms have been introduced.
Nevertheless many of the newly proposed techniques are sequential  hence difﬁcult to apply on large
data sets.

∗Supported in part by NSF CAREER award CCF-1053605  NSF BIGDATA grant IIS-1546108  NSF
AF:Medium grant CCF-1161365  DARPA GRAPHS/AFOSR grant FA9550-12-1-0423  and another DARPA
SIMPLEX grant.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

With the constant increase in the size of data sets to analyze  it is crucial to design efﬁcient large-scale
solutions that can be easily implemented in distributed computing platforms (such as Spark [45]
and Hadoop [43] as well as MapReduce and its extension Flume [17])  and cloud services (such as
Amazon Cloud or Google Cloud). For this reason in the past decade several papers proposed new
distributed algorithms for classic computer science and machine learning problems [3  4  7  14  15  19].
Despite these efforts not much is known about distributed algorithms for hierarchical clustering. There
are only two works analyzing these problems [27  28]  and neither gives any theoretical guarantees
on the quality of their algorithms or on the round complexity of their solutions.
In this work we propose new parallel algorithms in the MapReduce model to compute hierarchical
clustering and we analyze them from both theoretical and experimental perspectives. The main idea
behind our algorithms is to adapt clustering techniques based on classic minimum spanning tree
algorithms such as Bor˚uvka’s algorithm [11] and Kruskal’s algorithm [33] to run efﬁciently in parallel.
Furthermore we also provide a new theoretical framework to compare different clustering algorithms
based on the concept of a “certiﬁcate” and show new interesting properties of our algorithms.
We can summarize our contribution in four main points.
First  we focus on the distributed implementations of two important clustering techniques based
on classic minimum spanning tree algorithms. In particular we consider linkage-based clusterings
inspired by Kruskal’s algorithm and a novel clustering called afﬁnity clustering based on Bor˚uvka’s
algorithm. We provide new theoretical frameworks to compare different clustering algorithms based
on the concept of a “certiﬁcate” as a proof of having a good clustering and show new interesting
properties of both afﬁnity and single-linkage clustering algorithms.
Then  using a connection between linkage-based clustering  afﬁnity clustering and the minimum
spanning tree problem  we present new efﬁcient distributed algorithms for the hierarchical clustering
problem in a MapReduce model. In our analysis we consider the most restrictive model for distributed
computing  called Massively Parallel Communication  among previously studied MapReduce-like
models [10  23  30]. Along the way  we obtain a constant round MapReduce algorithm for minimum
spanning tree (MST) of dense graphs (in Section 5). Our algorithm for graphs with Θ(n1+c) edges
and for any given  with 0 <  < c < 1  ﬁnds the MST in (cid:100)log(c/)(cid:101)+1 rounds using ˜O(n1+) space
per machine and O(nc−) machines (i.e.  optimal total space). This improves the round complexity of
the state-of-the-art MST algorithm of Lattanzi et al. [34] for dense graphs which requires up to (cid:100)c/(cid:101)
rounds using the same number of machines and space. Prior to our work  no hierarchical clustering
algorithm was known in this model.
Then we turn our attention to real world applications and we introduce efﬁcient implementations of
afﬁnity clustering as well as classic single-linkage clustering that leverage Distributed Hash Tables
(DHTs) [12  31] to speed up computation for huge data sets.
Last but not least  we present an experimental study where we analyze the scalability and effectiveness
of our newly introduced algorithms and we observe that  in most cases  afﬁnity clustering outperforms
all state-of-the-art algorithms from both quality and scalability standpoints.2

2 Related Work

Clustering and  in particular  hierarchical clustering techniques have been studied by hundreds of
researchers [16  20  22  32]. In social networks  detecting the hierarchical clustering structure is a
basic primitive for studying the interaction between nodes [36  39]. Other relevant applications of
hierarchical clustering can be found in bioinformatics  image analysis and text classiﬁcation.
Our paper is closely related to two main lines of research. The ﬁrst one focuses on studying
theoretical properties of clustering approaches based on minimum spanning trees (MSTs). Linkage-
based clusterings (often based on Kruskal’s algorithm) have been extensively studied as basic
techniques for clustering datasets. The most common linkage-based clustering algorithms are single-
linkage  average-linkage and complete-linkage algorithms. In [44]  Zadeh and Ben-David gave a
characterization of the single-linkage algorithm. Their result has been then generalized to linkage-
based algorithms in [1]. Furthermore single-linkage algorithms are known to provably recover a
ground truth clustering if the similarity function has some stability properties [6]. In this paper we

2Implementations are available at https://github.com/MahsaDerakhshan/AffinityClustering.

2

introduce a new technique to compare clustering algorithms based on “certiﬁcates.” Furthermore we
introduce and analyze a new algorithm—afﬁnity—based on Bor˚uvka’s well-known algorithm. We
show that afﬁnity is not only scalable for huge data sets but also its performance is superior to several
state-of-the-art clustering algorithms. To the best of our knowledge though Bor˚uvka’s algorithm is a
well-known and classic algorithm  not many clustering algorithms have been considered based on
Bor˚uvka’s.
The second line of work is closely related to distributed algorithms for clustering problems. Several
models of MapReduce computation have been introduced in the past few years [10  23  30]. The ﬁrst
paper that studied clustering problems in these models is by Ene et al. [18]  where the authors prove
that any α approximation algorithm for the k-center or k-median problems can produce 4α + 2 and
10α + 3 approximation factors  respectively  for the k-center or k-median problems in the MapReduce
model. Subsequently several papers [5  7  8] studied similar problems in the MapReduce model. A
lot of efforts also went into studying efﬁcient algorithms on graphs [3  4  7  15  14  19]. However the
problem of hierarchical clustering did not receive a lot of attention. To the best of our knowledge
there are only two papers [27  28] on this topic  and neither analyzes the problem formally or proves
any guarantee in any MapReduce model.

3 Minimum Spanning Tree-Based Clusterings

We begin by going over two famous algorithms for minimum spanning tree and deﬁne the corre-
sponding algorithms for clustering.
Bor˚uvka’s algorithm and afﬁnity clustering: Bor˚uvka’s algorithm [11]  ﬁrst published in 1926  is
an algorithm for ﬁnding a minimum spanning tree (MST)3. The algorithm was rediscovered a few
times  in particular by Sollin [42] in 1965 in the parallel computing literature. Initially each vertex
forms a group (cluster) by itself. The algorithm begins by picking the cheapest edge going out of
each cluster  in each round (in parallel) joins these clusters to form larger clusters and continues
joining in a similar manner until a tree spanning all vertices is formed. Since the size of the smallest
cluster at least doubles each time  the number of rounds is at most O(log n). In afﬁnity clustering  we
stop Bor˚uvka’s algorithm after r > 0 rounds when for the ﬁrst time we have at most k clusters for a
desired number k > 0. In case the number of clusters is strictly less than k  we delete the edges that
we added in the last round in a non-increasing order (i.e.  we delete the edge with the highest weight
ﬁrst) to obtain exactly k clusters. To the best of our knowledge  although Bor˚uvka’s algorithm is a
well-known and classic algorithm  clustering algorithms based on it have not been considered much.
A natural hierarchy of nodes can be obtained by continuing Bor˚uvka’s algorithm: each cluster here
will be a subset of future clusters. We call this hierarchical afﬁnity clustering.
We present distributed implementations of Bor˚uvka/afﬁnity in Section 5 and show its scalability even
for huge graphs. We also show afﬁnity clustering  in most cases  works much better than several
well-known clustering algorithms in Section 6.
Kruskal’s algorithm and single-linkage clustering: Kruskal’s algorithm [33] ﬁrst introduced in
1956 is another famous algorithm for ﬁnding MST. The algorithm is highly sequential and iteratively
picks an edge of the least possible weight that connects any two trees (clusters) in the forest.4 Though
the number of iterations in Kruskal’s algorithm is n − 1 (the number of edges of any tree on n nodes) 
the algorithm can be implemented in O(m log n) time with simple data structures (m is the number
of edges) and in O(ma(n)) time using a more sophisticated disjoint-set data structure  where a(.) is
the extremely slowly growing inverse of the single-valued Ackermann function.
In single-linkage clustering  we stop Kruskal’s algorithm when we have at least k clusters (trees) for
a desired number k > 0. Again if we desire to obtain a corresponding hierarchical single-linkage
clustering  by adding further edges which will be added in Kruskal’s algorithm later  we can obtain a
natural hierarchical clustering (each cluster here will be a subset of future clusters).
As mentioned above  Kruskal’s Algorithm and single-linkage clustering are highly sequential  however
as we show in Section 5 thinking backward once we have an efﬁcient implementation of Bor˚uvka’s

3More precisely the algorithm works when there is a unique MST  in particular  when all edge weights are
distinct; however this can be easily achieved by either perturbing the edge weights by an  > 0 amount or have a
tie-breaking ordering for edges with the same weights

4Unlike Bor˚uvka’s method  this greedy algorithm has no limitations on the distinctness of edge weights.

3

(or any MST algorithm) in Map-Reduce and using Distributed Hash Tables (DHTs)  we can achieve
an efﬁcient parallel implementation of single-linkage clustering as well. We show scalability of this
implementation even for huge graphs in Section 5 and its performance in experiments in Section 6.

4 Guaranteed Properties of Clustering Algorithms

An important property of afﬁnity clustering is that it produces clusters that are roughly of the same
size. This is intuitively correct since at each round of the algorithm  each cluster is merged to at
least one other cluster and as a result  the size of even the smallest cluster is at least doubled. In fact
linkage based algorithms (and specially single linkage) are often criticized for producing uneven
clusters; therefore it is tempting to give a theoretical guarantee for the size ratio of the clusters that
afﬁnity produces. Unfortunately  as it is illustrated in Figure 1  we cannot give any worst case bounds
since even in one round we may end up having a cluster of size Ω(n) and another cluster of size
O(1). As the ﬁrst property  we show that at least in the ﬁrst round  this does not happen when the
observations are randomly distributed. Our empirical results on real world data sets in Section 6.1 
further conﬁrm this property for all rounds  and on real data sets.

Figure 1: An example of how afﬁnity may produce a large component in one round.

We start by deﬁning the nearest neighbor graph.
Deﬁnition 1 (Nearest Neighbor Graph). Let S be a set of points in a metric space. The nearest
neighbor graph of S  denoted by GS  has |S| vertices  each corresponding to an element in S and if
a ∈ S is the nearest element to b ∈ S in S  graph GS contains an edge between the corresponding
vertices of a and b.

At each round of afﬁnity clustering  all the vertices that are in the same connected component of the
nearest neighbor graph will be merged together5. Thus  it sufﬁces to bound the connected components’
size.
For a random model of points  consider a Poisson point process X in Rd (d ≥ 1) with density 1.
It has two main properties. First  the number of points in any ﬁnite region of volume V is Poisson
distributed with mean V . Second  the number of points in any two disjoint regions are independent
of each other.
Theorem 1 (Häggström et al. [38]). For any d ≥ 2  consider the (Euclidean distance) nearest
neighbor graph G of a realization of a Poisson point process in Rd with density 1. All connected
components of G are ﬁnite almost surely.

Theorem 1 implies that the size of the maximum connected component of the points within any ﬁnite
region in Rd is bounded by almost a constant number. This is a very surprising result compared to
the worst case scenario of having a connected component that contains all the points.
Note that although the aforementioned bound holds for the ﬁrst round of afﬁnity  after the connected
components are contracted  we cannot necessarily assume that the new points are Poisson distributed
and the same argument cannot be used for the rest of the rounds.
Next we present further properties of afﬁnity clustering. Let us begin by introducing the concept of
“cost” for a clustering solution to be able to compare clustering algorithms.
Deﬁnition 2. The cost of a cluster is the sum of edge lengths (weights) of a minimum Steiner tree
connecting all vertices inside the cluster. The cost of a clustering is the sum of the costs of its clusters.
Finally a non-singleton clustering of a graph is a partition of its vertices into clusters of size at least
two.

Even one round of afﬁnity clustering often produces good solutions for several applications. Now we
are ready to present the following extra property of the result of the ﬁrst round of afﬁnity clustering.

5Depending on the variant of afﬁnity that we use  the distance function will be updated.

4

Theorem 2. The cost of any non-singleton clustering is at least half of that of the clustering obtained
after the ﬁrst round of afﬁnity clustering.

Before presenting the proof of Theorem 2  we need to demonstrate the concept of disc painting
introduced previously in [29  2  21  9  25]. In this setting  we consider a topological structure of
a graph metric in which each edge is a curve connecting its endpoints whose length is equal to its
weight. We assume each vertex has its own color. A disc painting is simply a set of disjoint disks
centered at terminals (with the same colors of the center vertices). A disk of radius r centered at
vertex v paints all edges (or portions) of them which are at distance r from vertex v with the color of
v. Thus we paint (portions of) edges by different disks each corresponding to a vertex and each edge
can be painted by at most two disks. With this deﬁnition of disk painting  we now demonstrate the
proof of Theorem 2.
Next we turn our focus to obtain structural properties for single-linkage clustering. We denote by Fk
the set of edges added after k iterations of Kruskal  i.e.  when we have n− k clusters in single-linkage
clustering. Note that Fk is a forest  i.e.  a set of edges with no cycle. First we start with an important
observation whose proof comes directly from the description of the single-linkage algorithm.
Proposition 3. Suppose we run single-linkage clustering until we have n − k clusters. Let doutside
be the minimum distance between any two clusters and dinside be the maximum distance of any edge
added to forest Fk. Then doutside ≥ dinside.
We note that Proposition 3 demonstrates the following important property of single-linkage clustering:
Each vertex of a cluster at any time has a neighbor inside to which is closer than any other vertex
outside of its clusters.
Next we deﬁne another criterion for desirability of a clustering algorithm. This generalizes Proposi-
tion 3.
Deﬁnition 3. An α-certiﬁcate for a clustering algorithm  where α ≥ 1  is an assignment of shares to
each vertex of the graph with the following two properties: (1) The cost of each cluster is at most
α times the sum of shares of vertices inside the cluster; (2) For any set S of vertices containing at
most one from each cluster in our solution  the imaginary cluster S costs at least the sum of shares of
vertices in S.

Note that intuitively the ﬁrst property guarantees that vertices inside each cluster can pay the cost of
their corresponding cluster and that there is no free-rider. The second property intuitively implies we
cannot ﬁnd any better clustering by combining vertices from different clusters in our solution.
Next we show that there always exists a 2-certiﬁcate for single-linkage clustering guaranteeing its
worst-case performance.
Theorem 4. Single-linkage always produces a clustering solution that has a 2-certiﬁcate.

5 Distributed Algorithms

5.1 Constant Round Algorithm For Dense Graphs

Unsurprisingly  ﬁnding the afﬁnity clustering of a given graph G is closely related to the problem of
ﬁnding its Minimum Spanning Tree (MST). In fact  we show the data that is encoded in the MST of G
is sufﬁcient for ﬁnding its afﬁnity clustering (Theorem 9). This property is also known to be true for
single linkage [24]. For MapReduce algorithms this is particularly useful because the MST requires a
substantially smaller space than the original graph and can be stored in one machine. Therefore  once
we have the MST  we can obtain afﬁnity or single linkage in one round.
The main contribution of this section is an algorithm for ﬁnding the MST (and therefore the afﬁnity
clustering) of dense graphs in constant rounds of MapReduce which improves upon prior known
dense graph MST algorithms of Karloff et al. [30] and Lattanzi et al. [34].
Theoretical Model. Let N denote the input size. There are a total number of M machines and each
of them has a space of size S. Both S and M must be substantially sublinear in N. In each round 
the machines can run an arbitrary polynomial time algorithm on their local data. No communication
is allowed during the rounds but any two machines can communicate with each other between the
rounds as long as the total communication size of each machine does not exceed its memory size.

5

(cid:46) Since G is assumed to be dense we know c > 0.

Algorithm 1 MST of Dense Graphs
Input: A weighted graph G
Output: The minimum spanning tree of G
1: function MST(G = (V  E)  )
c ← logn (m/n)
2:
while |E| > O(n1+) do
3:
REDUCEEDGES(G  c)
4:
c ← (c − )/2
5:
6:
7: function REDUCEEDGES(G = (V  E)  c)
8:
9:
10:
11:

Move all the edges to one machine and ﬁnd MST of G in there.
k ← n(c−)/2
Independently and u.a.r. partition V into k subsets {V1  . . .   Vk}.
Independently and u.a.r. partition V into k subsets {U1  . . .   Uk}.
Let Gi j be a subgraph of G with vertex set Vi ∪ Uj containing any edge (v  u) ∈ E(G)
for any i  j ∈ {1  . . .   k} do

where v ∈ Vi and u ∈ Uj.

12:
13:
14:

Send all the edges of Gi j to the same machine and ﬁnd its MST in there.
Remove an edge e from E(G)   if e ∈ Gi j and it is not in MST of Gi j.

This model is called Massively Parallel Communication (MPC) in the literature and is “arguably the
most popular one” [26] among MapReduce like models.
Theorem 5. Let G = (V  E) be a graph with n vertices and n1+c edges for any constant c > 0 and
let w : E (cid:55)→ R+ be its edge weights. For any given  such that 0 <  < c  there exists a randomized
algorithm for ﬁnding the MST of G that runs in at most (cid:100)log (c/)(cid:101) + 1 rounds of MPC where
every machine uses a space of size ˜O(n1+) with high probability and the total number of required
machines is O(nc−).

Our algorithm  therefore  uses only enough total space ( ˜O(n1+c)) on all machines to store the input.
The following observation is mainly used by Algorithm 1 to iteratively remove the edges that are not
part of the ﬁnal MST.
Lemma 6. Let G(cid:48) = (V (cid:48)  E(cid:48)) be a (not necessarily connected) subgraph of the input graph G. If an
edge e ∈ E(cid:48) is not in the MST of G(cid:48)  then it is not in the MST of G either.
To be more speciﬁc  we iteratively divide G into its subgraphs  such that each edge of G is at least in
one subgraph. Then  we handle each subgraph in one machine and throw away the edges that are not
in their MST. We repeat this until there are only O(n1+) edges left in G. Then we can handle all
these edges in one machine and ﬁnd the MST of G. Algorithm 1 formalizes this process.
Lemma 7. Algorithm 1 correctly ﬁnds the MST of the input graph in (cid:100)log (c/)(cid:101) + 1 rounds.
By Lemma 6 we know any edge that is removed from is not part of the MST therefore it sufﬁces to
prove the while loop in Algorithm 1 takes (cid:100)log (c/)(cid:101) + 1 iterations.
Lemma 8. In Algorithm 1  every machine uses a space of size ˜O(n1+) with high probability.
The combination of Lemma 7 and Lemma 8 implies that Algorithm 1 is indeed in MPC and
Theorem 5 holds. See supplementary material for omitted proofs.
The next step is to prove all the information that is required for afﬁnity clustering is indeed contained
in the MST.
Theorem 9. Let G = (V  E) denote an arbitrary graph  and let G(cid:48) = (V  E(cid:48)) denote the minimum
spanning tree of G. Running afﬁnity clustering algorithm on G gives the same clustering of V as
running this algorithm on G(cid:48).
By combining the MST algorithm given for Theorem 5 and the sufﬁciency of MST for computing
afﬁnity clustering (Theorem 9) and single linkage ([24]) we get the following corollary.
Corollary 10. Let G = (V  E) be a graph with n vertices and n1+c edges for any constant c > 0
and let w : E (cid:55)→ R+ be its edge weights. For any given  such that 0 <  < c  there exists a

6

randomized algorithm for afﬁnity clustering and single linkage that runs in (cid:100)log (c/)(cid:101) + 1 rounds of
MPC where every machine uses a space of size ˜O(n1+) with high probability and the total number
of required machines is O(nc−).

5.2 Logarithmic Round Algorithm For Sparse Graphs
Consider a graph G(V  E) on n = |V | vertices  with edge weights w : E (cid:55)→ R. We assume that the
edge weights denote distances. (The discussion applies mutatis mutandis to the case where edge
weights signify similarity.)
The algorithm works for a ﬁxed number of synchronous rounds  or until no further progress is made 
say  by reaching a single cluster of all vertices. Each round consists of two steps: First  every vertex
picks its best edge (i.e.  that of the minimum weight) at each round; and then the graph is contracted
along the selected edges. (See Algorithm 2 in the appendix.)
For a connected graph  the algorithm continues until a single cluster of all vertices is obtained. The
supernodes at different rounds can be thought of as a hierarchical clustering of the vertices.
While the ﬁrst step of each round has a trivial implementation in MapReduce  the latter might
take Ω(log n) MapReduce rounds to implement  as it is an instance of the connected components
problem. Using a DHT was shown to signiﬁcantly improve the running time here  by implementing
the operation in one round of MapReduce [31]. Basically we have a read-only random-access table
mapping each vertex to its best neighbor. Repeated lookups in the table allows each vertex to follow
the chain of best neighbors until a loop (of length two) is encountered. This assigns a unique name for
each connected component; then all the vertices in the same component are reduced into a supernode.
Theorem 11. The afﬁnity clustering algorithm runs in O(log n) rounds of MapReduce when we have
access to a distributed hash table (DHT). Without the DHT  the algorithm takes O(log2 n) rounds.

6 Experiments

6.1 Quality Analysis

In this section  we compare well known hierarchical and ﬂat clustering algorithms  such as k-means 
single linkage  complete linkage and average linkage with different variants of afﬁnity clustering 
such as single afﬁnity  complete afﬁnity and average afﬁnity. We run our experiments on several data
sets from the UCI database [37] and use Euclidean distance6.
To evaluate the outputs of these algorithms we use Rand index which is deﬁned as follows.
Deﬁnition 4 (Rand index [40]). Given a set V = {v1  . . .   vn} of n points and two clusterings
X = {X1  . . .   Xr} and Y = {Y1  . . .   Ys} of V . Deﬁne the following.

• a: the number of pairs in V that are in the same cluster in X and in the same cluster in Y .
• b: the number of pairs in V that are in different clusters in X and in different clusters in Y .

the Rand index r(X  Y ) is deﬁned to be (a + b)/(cid:0)n

(cid:1). By having the ground truth clustering T of a

data set  we deﬁne the Rand index score of a clustering X  to be r(X  T ).

2

The Rand index based scores are in range [0  1] and a higher number implies a better clustering.
For a hierarchical clustering  the level of its corresponding tree with the highest score is used for
evaluations.
Figure 2 (a) compares the Rand index score of different clustering algorithms for different data sets.
We observe that single afﬁnity generally performs really well and is among the top two algorithms
for most of the datasets (all except Glass). Average afﬁnity also seems to perform well and in some
cases (e.g.  for Soybean data set) it produces a very high quality clustering compared to others. To
summarize  linkage based algorithms do not seem to be as good as afﬁnity based algorithms but in
some cases k-means could be close.

6We consider Iris  Wine  Soybean  Digits and Glass data sets.

7

(a)

(b)

Figure 2: Comparison of clustering algorithms based on their Rand index score (a) and clusters size
ratio (b).

Table 1: Statistics about datasets used. (Numbers for ImageGraph are approximate.) The ﬁfth column
shows the relative running time of afﬁnity clustering  and the last column is the speedup obtained by
a ten-fold increase in parallelism.

Dataset
LiveJournal
Orkut
Friendster
ImageGraph

# nodes
4 846 609
3 072 441
65 608 366
2 × 1010

7 861 383 690
42 687 055 644
1 092 793 541 014
1012

# edges max degree
444 522
893 056
2 151 462
14000

running time
1.0
2.4
54
142

speedup
4.3
9.2
5.9
4.1

Another property of the algorithms that we measure is the clusters’ size ratio. Let X = {X1  . . .   Xr}
be a clustering. We deﬁne the size ratio of X to be mini j∈[r] |Xi|/|Xj|. As it is visualized in Figure 2
(b)  afﬁnity based algorithms have a much higher size ratio (i.e.  the clusters are more balanced)
compared to linkage based algorithms. This conﬁrms the property that we proved for Poisson
distributions in Section 4 for real world data sets. Hence we believe afﬁnity clustering is superior
to (or at least as good as) the other methods when the dataset under consideration is not extremely
unbalanced.

6.2 Scalability

Here we demonstrate the scalability of our implementation of afﬁnity clustering. A collection of
public and private graphs of varying sizes are studied. These graphs have between 4 million and 20
billion vertices and from 4 billion to one trillion edges. The ﬁrst three graphs in Table 1 are based on
public graphs [35]. As most public graphs are unweighted  we use the number of common neighbors
between a pair of nodes as the weight of the edge between them. (This is computed for all pairs 
whether they form a pair in the original graph or not  and then new edges of weight zero are removed.)
The last graph is based on (a subset of) an internal corpus of public images found on the web and
their similarities.
We note that we use the “maximum” spanning tree variant of afﬁnity clustering; here edge weights
denote similarity rather than distance.
While we cannot reveal the exact running times and number of machines used in the experiments  we
report these quantities in “normalized form.” We only run one round of afﬁnity clustering (consisting
of a “Find Best Neighbors” and a “Contract Graph” step). Two settings are used in the experiments.
We once use W MapReduce workers and D machines for the DHT  and compare this to the case with
10W MapReduce workers and D machines for the DHT. This ten-fold increase in the number of
MapReduce workers leads to four- to ten-fold decrease in the total running time for different datasets.
Each running time is itself the average over three runs to reduce the effect of external network events.
Table 1 also shows how the running time changes with the size of the graph. With a modest number
of MapReduce workers  afﬁnity clustering runs in less than an hour for all the graphs.

8

DatasetsRand Index ScoreAlgorithmSingle AffinityAverage AffinityComplete AffinityComplete LinkageAverage LinkageSingle Linkagek-Means0.40.60.81.0IrisSoybeanWineGlassDigits0.00.20.40.60.8IrisSoybeanWineGlassDigitsDatasetsClusters' Size RatioAlgorithmSingle AffinityAverage AffinityComplete AffinityComplete LinkageAverage LinkageSingle Linkagek-MeansReferences
[1] Margareta Ackerman  Shai Ben-David  and David Loker. Characterization of linkage-based clustering. In
COLT 2010 - The 23rd Conference on Learning Theory  Haifa  Israel  June 27-29  2010  pages 270–281 
2010.

[2] Ajit Agrawal  Philip N. Klein  and R. Ravi. When trees collide: An approximation algorithm for the

generalized steiner problem on networks. SIAM J. Comput.  24(3):440–456  1995.

[3] Kook Jin Ahn  Sudipto Guha  and Andrew McGregor. Analyzing graph structure via linear measurements.
In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms  pages 459–467 
2012.

[4] Alexandr Andoni  Aleksandar Nikolov  Krzysztof Onak  and Grigory Yaroslavtsev. Parallel algorithms for
geometric graph problems. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing 
pages 574–583. ACM  2014.

[5] Bahman Bahmani  Benjamin Moseley  Andrea Vattani  Ravi Kumar  and Sergei Vassilvitskii. Scalable

k-means++. PVLDB  5(7):622–633  2012.

[6] Maria-Florina Balcan  Avrim Blum  and Santosh Vempala. A discriminative framework for clustering
via similarity functions. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing 
Victoria  British Columbia  Canada  May 17-20  2008  pages 671–680  2008.

[7] Maria-Florina Balcan  Steven Ehrlich  and Yingyu Liang. Distributed k-means and k-median clustering on
general communication topologies. In Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8 
2013  Lake Tahoe  Nevada  United States.  pages 1995–2003  2013.

[8] MohammadHossein Bateni  Aditya Bhaskara  Silvio Lattanzi  and Vahab S. Mirrokni. Distributed balanced
clustering via mapping coresets. In Advances in Neural Information Processing Systems 27: Annual
Conference on Neural Information Processing Systems 2014  December 8-13 2014  Montreal  Quebec 
Canada  pages 2591–2599  2014.

[9] MohammadHossein Bateni  Mohammad Taghi Hajiaghayi  and Dániel Marx. Approximation schemes for

steiner forest on planar graphs and graphs of bounded treewidth. J. ACM  58(5):21:1–21:37  2011.

[10] Paul Beame  Paraschos Koutris  and Dan Suciu. Communication steps for parallel query processing. In
Proceedings of the 32nd ACM SIGMOD-SIGACT-SIGAI symposium on Principles of database systems 
pages 273–284. ACM  2013.

[11] Oktar Boruvka. O jistém problému minimálním. Práce Moravské pˇrírodovˇedecké spoleˇcnosti. Mor.

pˇrírodovˇedecká spoleˇcnost  1926.

[12] Fay Chang  Jeffrey Dean  Sanjay Ghemawat  Wilson C. Hsieh  Deborah A. Wallach  Michael Burrows 
Tushar Chandra  Andrew Fikes  and Robert E. Gruber. Bigtable: A distributed storage system for structured
data. ACM Trans. Comput. Syst.  26(2):4:1–4:26  2008.

[13] Moses Charikar and Vaggos Chatziafratis. Approximate hierarchical clustering via sparsest cut and
spreading metrics. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA 2017  Barcelona  Spain  Hotel Porta Fira  January 16-19  pages 841–854  2017.

[14] Rajesh Chitnis  Graham Cormode  Hossein Esfandiari  MohammadTaghi Hajiaghayi  Andrew McGregor 
Morteza Monemizadeh  and Sofya Vorotnikova. Kernelization via sampling with applications to ﬁnding
matchings and related problems in dynamic graph streams. In Proceedings of the Twenty-Seventh Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 1326–1344  2016.

[15] Rajesh Hemant Chitnis  Graham Cormode  Mohammad Taghi Hajiaghayi  and Morteza Monemizadeh.
Parameterized streaming: Maximal matching and vertex cover. In Proceedings of the Twenty-Sixth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 1234–1251  2015.

[16] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In Proceedings of the 48th
Annual ACM SIGACT Symposium on Theory of Computing  STOC 2016  Cambridge  MA  USA  June 18-21 
2016  pages 118–127  2016.

[17] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simpliﬁed data processing on large clusters. Communi-

cations of the ACM  51(1):107–113  2008.

9

[18] Alina Ene  Sungjin Im  and Benjamin Moseley. Fast clustering using mapreduce. In Proceedings of the
17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  San Diego  CA 
USA  August 21-24  2011  pages 681–689  2011.

[19] Hossein Esfandiari  Mohammad Taghi Hajiaghayi  Vahid Liaghat  Morteza Monemizadeh  and Krzysztof
Onak. Streaming algorithms for estimating the matching size in planar graphs and beyond. In Proceedings
of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms  pages 1217–1233  2015.

[20] Assaf Glazer  Omer Weissbrod  Michael Lindenbaum  and Shaul Markovitch. Approximating hierarchical
mv-sets for hierarchical clustering. In Advances in Neural Information Processing Systems 27: Annual
Conference on Neural Information Processing Systems 2014  December 8-13 2014  Montreal  Quebec 
Canada  pages 999–1007  2014.

[21] Michel X. Goemans and David P. Williamson. A general approximation technique for constrained forest

problems. SIAM J. Comput.  24(2):296–317  1995.

[22] Jacob Goldberger and Sam T. Roweis. Hierarchical clustering of a mixture model. In Advances in Neural
Information Processing Systems 17 [Neural Information Processing Systems  NIPS 2004  December 13-18 
2004  Vancouver  British Columbia  Canada]  pages 505–512  2004.

[23] Michael T Goodrich  Nodari Sitchinava  and Qin Zhang. Sorting  searching  and simulation in the
mapreduce framework. In International Symposium on Algorithms and Computation  pages 374–383.
Springer  2011.

[24] John C Gower and GJS Ross. Minimum spanning trees and single linkage cluster analysis. Applied

statistics  pages 54–64  1969.

[25] Mohammad Taghi Hajiaghayi  Vahid Liaghat  and Debmalya Panigrahi. Online node-weighted steiner
forest and extensions via disk paintings. In 54th Annual IEEE Symposium on Foundations of Computer
Science  FOCS 2013  26-29 October  2013  Berkeley  CA  USA  pages 558–567  2013.

[26] Sungjin Im  Benjamin Moseley  and Xiaorui Sun. Efﬁcient massively parallel methods for dynamic
programming. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing. ACM  2017.

[27] Chen Jin  Ruoqian Liu  Zhengzhang Chen  William Hendrix  Ankit Agrawal  and Alok N. Choudhary. A
scalable hierarchical clustering algorithm using spark. In First IEEE International Conference on Big Data
Computing Service and Applications  BigDataService 2015  Redwood City  CA  USA  March 30 - April 2 
2015  pages 418–426  2015.

[28] Chen Jin  Md Mostofa Ali Patwary  Ankit Agrawal  William Hendrix  Wei-keng Liao  and Alok Choudhary.
Disc: A distributed single-linkage hierarchical clustering algorithm using mapreduce. In Proceedings of
the 4th International SC Workshop on Data Intensive Computing in the Clouds  2013.

[29] Michael Jünger and William R. Pulleyblank. New primal and dual matching heuristics. Algorithmica 

13(4):357–386  1995.

[30] Howard Karloff  Siddharth Suri  and Sergei Vassilvitskii. A model of computation for mapreduce. In
Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms  pages 938–948.
Society for Industrial and Applied Mathematics  2010.

[31] Raimondas Kiveris  Silvio Lattanzi  Vahab S. Mirrokni  Vibhor Rastogi  and Sergei Vassilvitskii. Connected
components in MapReduce and beyond. In Proceedings of the ACM Symposium on Cloud Computing 
Seattle  WA  USA  November 03 - 05  2014  pages 18:1–18:13  2014.

[32] Akshay Krishnamurthy  Sivaraman Balakrishnan  Min Xu  and Aarti Singh. Efﬁcient active algorithms for
hierarchical clustering. In Proceedings of the 29th International Conference on Machine Learning  ICML
2012  Edinburgh  Scotland  UK  June 26 - July 1  2012  2012.

[33] Joseph B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. In

Proceedings of the American Mathematical Society  volume 7  pages 48–50  1956.

[34] Silvio Lattanzi  Benjamin Moseley  Siddharth Suri  and Sergei Vassilvitskii. Filtering: a method for solving
graph problems in mapreduce. In Proceedings of the twenty-third annual ACM symposium on Parallelism
in algorithms and architectures  pages 85–94. ACM  2011.

[35] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:

//snap.stanford.edu/data  June 2014.

10

[36] Jure Leskovec  Anand Rajaraman  and Jeffrey D. Ullman. Mining of Massive Datasets  2nd Ed. Cambridge

University Press  2014.

[37] Moshe Lichman. UCI machine learning repository  2013.

[38] Ronald Meester et al. Nearest neighbor and hard sphere models in continuum percolation. Random

structures and algorithms  9(3):295–315  1996.

[39] Mark Newman. Networks: An Introduction. Oxford University Press  Inc.  New York  NY  USA  2010.

[40] William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American

Statistical association  66(336):846–850  1971.

[41] Aurko Roy and Sebastian Pokutta. Hierarchical clustering via spreading metrics. In Advances in Neural
Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016 
December 5-10  2016  Barcelona  Spain  pages 2316–2324  2016.

[42] M. Sollin. Le tracé de canalisation. Programming  Games  and Transportation Networks (in French)  1965.

[43] Tom White. Hadoop: The Deﬁnitive Guide. O’Reilly Media  Inc.  2012.

[44] Reza Zadeh and Shai Ben-David. A uniqueness theorem for clustering. In UAI 2009  Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  Montreal  QC  Canada  June 18-21 
2009  pages 639–646  2009.

[45] Matei Zaharia  Mosharaf Chowdhury  Michael J. Franklin  Scott Shenker  and Ion Stoica. Spark: Cluster
computing with working sets. In Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud
Computing  pages 10–10  2010.

11

,Mohammadhossein Bateni
Soheil Behnezhad
Mahsa Derakhshan
MohammadTaghi Hajiaghayi
Raimondas Kiveris
Silvio Lattanzi
Vahab Mirrokni