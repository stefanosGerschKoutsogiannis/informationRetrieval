2017,Lower bounds on the robustness to adversarial perturbations,The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific  hardly perceptible perturbations to the input  called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them. A proven explanation remains elusive  however. In this work  we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The bounds are experimentally verified on the MNIST and CIFAR-10 data sets.,Lower bounds on the robustness to adversarial

perturbations

Jonathan Peck1 2  Joris Roels2 3  Bart Goossens3  and Yvan Saeys1 2

1Department of Applied Mathematics  Computer Science and Statistics  Ghent University  Ghent  9000  Belgium

2Data Mining and Modeling for Biomedicine  VIB Inﬂammation Research Center  Ghent  9052  Belgium
3Department of Telecommunications and Information Processing  Ghent University  Ghent  9000  Belgium

Abstract

The input-output mappings learned by state-of-the-art neural networks are sig-
niﬁcantly discontinuous. It is possible to cause a neural network used for image
recognition to misclassify its input by applying very speciﬁc  hardly perceptible
perturbations to the input  called adversarial perturbations. Many hypotheses have
been proposed to explain the existence of these peculiar samples as well as several
methods to mitigate them  but a proven explanation remains elusive. In this work 
we take steps towards a formal characterization of adversarial perturbations by
deriving lower bounds on the magnitudes of perturbations necessary to change the
classiﬁcation of neural networks. The proposed bounds can be computed efﬁciently 
requiring time at most linear in the number of parameters and hyperparameters
of the model for any given sample. This makes them suitable for use in model
selection  when one wishes to ﬁnd out which of several proposed classiﬁers is
most robust to adversarial perturbations. They may also be used as a basis for
developing techniques to increase the robustness of classiﬁers  since they enjoy the
theoretical guarantee that no adversarial perturbation could possibly be any smaller
than the quantities provided by the bounds. We experimentally verify the bounds
on the MNIST and CIFAR-10 data sets and ﬁnd no violations. Additionally  the
experimental results suggest that very small adversarial perturbations may occur
with non-zero probability on natural samples.

1

Introduction

Despite their big successes in various AI tasks  neural networks are basically black boxes: there is no
clear fundamental explanation how they are able to outperform the more classical approaches. This
has led to the identiﬁcation of several unexpected and counter-intuitive properties of neural networks.
In particular  Szegedy et al. [2014] discovered that the input-output mappings learned by state-of-the-
art neural networks are signiﬁcantly discontinuous. It is possible to cause a neural network used for
image recognition to misclassify its input by applying a very speciﬁc  hardly perceptible perturbation
to the input. Szegedy et al. [2014] call these perturbations adversarial perturbations  and the inputs
resulting from applying them to natural samples are called adversarial examples.
In this paper  we hope to shed more light on the nature and cause of adversarial examples by
deriving lower bounds on the magnitudes of perturbations necessary to change the classiﬁcation of
neural network classiﬁers. Such lower bounds are indispensable for developing rigorous methods
that increase the robustness of classiﬁers without sacriﬁcing accuracy. Since the bounds enjoy the
theoretical guarantee that no adversarial perturbation could ever be any smaller  a method which
increases these lower bounds potentially makes the classiﬁer more robust. They may also aid model
selection: if the bounds can be computed efﬁciently  then one can use them to compare different

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

models with respect to their robustness to adversarial perturbations and select the model that scores
the highest in this regard without the need for extensive empirical tests.
The rest of the paper is organized as follows. Section 2 discusses related work that has been done
on the phenomenon of adversarial perturbations; Section 3 details the theoretical framework used
to prove the lower bounds; Section 4 proves lower bounds on the robustness of different families of
classiﬁers to adversarial perturbations; Section 5 empirically veriﬁes that the bounds are not violated;
Section 6 concludes the paper and provides avenues for future work.

2 Related work

Since the puzzling discovery of adversarial perturbations  several hypotheses have been proposed to
explain why they exist  as well as a number of methods to make classiﬁers more robust to them.

2.1 Hypotheses

The leading hypothesis explaining the cause of adversarial perturbations is the linearity hypothesis by
Goodfellow et al. [2015]. According this view  neural network classiﬁers tend to act very linearly on
their input data despite the presence of non-linear transformations within their layers. Since the input
data on which modern classiﬁers operate is often very high in dimensionality  such linear behavior
can cause minute perturbations to the input to have a large impact on the output. In this vein  Lou
et al. [2016] propose a variant of the linearity hypothesis which claims that neural network classiﬁers
operate highly linearly on certain regions of their inputs  but non-linearly in other regions. Rozsa et al.
[2016] conjecture that adversarial examples exist because of evolutionary stalling: during training 
the gradients of samples that are classiﬁed correctly diminish  so the learning algorithm “stalls” and
does not create signiﬁcantly ﬂat regions around the training samples. As such  most of the training
samples will lie close to some decision boundary  and only a small perturbation is required to push
them into a different class.

2.2 Proposed solutions

Gu and Rigazio [2014] propose the Deep Contractive Network  which includes a smoothness penalty
in the training procedure inspired by the Contractive Autoencoder. This penalty encourages the
Jacobian of the network to have small components  thus making the network robust to small changes
in the input. Based on their linearity hypothesis  Goodfellow et al. [2015] propose the fast gradient
sign method for efﬁciently generating adversarial examples. They then use this method as a regularizer
during training in an attempt to make networks more robust. Lou et al. [2016] use their “local linearity
hypothesis” as the basis for training neural network classiﬁers using foveations  i.e. a transformation
which selects certain regions from the input and discards all other information. Rozsa et al. [2016]
introduce Batch-Adjusted Network Gradients (BANG) based on their idea of evolutionary stalling.
BANG normalizes the gradients on a per-minibatch basis so that even correctly classiﬁed samples
retain signiﬁcant gradients and the learning algorithm does not stall.
The solutions proposed above provide attractive intuitive explanations for the cause of adversarial
examples  and empirical results seem to suggest that they are effective at eliminating them. However 
none of the hypotheses on which these methods are based have been formally proven. Hence  even
with the protections discussed above  it may still be possible to generate adversarial examples for
classiﬁers using techniques which defy the proposed hypotheses. As such  there is a need to formally
characterize the nature of adversarial examples. Fawzi et al. [2016] take a step in this direction by
deriving precise bounds on the norms of adversarial perturbations of arbitrary classiﬁers in terms of
the curvature of the decision boundary. Their analysis encourages to impose geometric constraints
on this curvature in order to improve robustness. However  it is not obvious how such constraints
relate to the parameters of the models and hence how one would best implement such constraints
in practice. In this work  we derive lower bounds on the robustness of neural networks directly in
terms of their model parameters. We consider only feedforward networks comprised of convolutional
layers  pooling layers  fully-connected layers and softmax layers.

2

3 Theoretical framework

The theoretical framework used in this paper draws heavily from Fawzi et al. [2016] and Papernot
et al. [2016]. In the following  (cid:107)·(cid:107) denotes the Euclidean norm and (cid:107)·(cid:107)F denotes the Frobenius norm.
We assume we want to train a classiﬁer f : Rd → {1  . . .   C} to correctly assign one of C different
classes to input vectors x from a d-dimensional Euclidean space. Let µ denote the probability measure
on Rd and let f (cid:63) be an oracle that always returns the correct label for any input. The distribution µ
is assumed to be of bounded support  i.e. Px∼µ(x ∈ X ) = 1 with X = {x ∈ Rd | (cid:107)x(cid:107) ≤ M} for
some M > 0.
Formally  adversarial perturbations are deﬁned relative to a classiﬁer f and an input x. A perturbation
η is called an adversarial perturbation of x for f if f (x + η) (cid:54)= f (x) while f (cid:63)(x + η) = f (cid:63)(x).
An adversarial perturbation η is called minimal if no other adversarial perturbation ξ for x and f
satisﬁes (cid:107)ξ(cid:107) < (cid:107)η(cid:107). In this work  we will focus on minimal adversarial perturbations.
The robustness of a classiﬁer f is deﬁned as the expected norm of the smallest perturbation necessary
to change the classiﬁcation of an arbitrary input x sampled from µ:

where

ρadv(f ) = Ex∼µ[∆adv(x; f )] 

∆adv(x; f ) = min
η∈Rd

{(cid:107)η(cid:107) | f (x + η) (cid:54)= f (x)}.

A multi-index is a tuple of non-negative integers  generally denoted by Greek letters such as α and β.
For a multi-index α = (α1  . . .   αm) and a function f we deﬁne

|α| = α1 + ··· + αn 

∂αf =

∂|α|f
1 . . . ∂xαn

n

∂xα1

.

The Jacobian matrix of a function f : Rn → Rm : x (cid:55)→ [f1(x)  . . .   fm(x)]T is deﬁned as

 ∂f1

∂x1

...

∂fm
∂x1

 .

. . .
...
. . .

∂f1
∂xn

...

∂fm
∂xn

∂
∂x

f =

3.1 Families of classiﬁers

The derivation of the lower bounds will be built up incrementally. We will start with the family
of linear classiﬁers  which are among the simplest. Then  we extend the analysis to Multi-Layer
Perceptrons  which are the oldest neural network architectures. Finally  we analyze Convolutional
Neural Networks. In this section  we introduce each of these families of classiﬁers in turn.
A linear classiﬁer is a classiﬁer f of the form

f (x) = arg max

i=1 ... C

wi · x + bi.

The vectors wi are called weights and the scalars bi are called biases.
A Multi-Layer Perceptron (MLP) is a classiﬁer given by

f (x) = arg max

i=1 ... C

softmax(hL(x))i 

hL(x) = gL(VLhL−1(x) + bL) 

...

h1(x) = g1(V1x + b1).

An MLP is nothing more than a series of linear transformations Vlhl−1(x) + bl followed by non-
linear activation functions gl (e.g. a ReLU [Glorot et al.  2011]). Here  softmax is the softmax
function:

softmax(y)i =

(cid:80)
exp(wi · y + bi)
j exp(wj · y + bj)

.

3

c(cid:88)

q(cid:88)

q(cid:88)

This function is a popular choice as the ﬁnal layer for an MLP used for classiﬁcation  but it is by no
means the only possibility. Note that having a softmax as the ﬁnal layer essentially turns the network
into a linear classiﬁer of the output of its penultimate layer  hL(x).
A Convolutional Neural Network (CNN) is a neural network that uses at least one convolution
operation. For an input tensor X ∈ Rc×d×d and a kernel tensor W ∈ Rk×c×q×q  the discrete
convolution of X and W is given by

(X (cid:63) W)ijk =

wi n m lxn m+s(q−1) l+s(q−1).

n=1

m=1

l=1

Here  s is the stride of the convolution. The output of such a layer is a 3D tensor of size k × t × t
s + 1. After the convolution operation  usually a bias b ∈ Rk is added to each of the
where t = d−q
feature maps. The different components (W (cid:63) X)i constitute the feature maps of this convolutional
layer. In a slight abuse of notation  we will write W (cid:63) X + b to signify the tensor W (cid:63) X where each
of the k feature maps has its respective bias added in:

(W (cid:63) X + b)ijk = (W (cid:63) X)ijk + bi.

CNNs also often employ pooling layers  which perform a sort of dimensionality reduction. If we
write the output of a pooling layer as Z(X)  then we have

zijk(X) = p({xi n+s(j−1) m+s(k−1) | 1 ≤ n  m ≤ q}).

Here  p is the pooling operation  s is the stride and q is a parameter. The output tensor Z(X) has
dimensions c × t × t. For ease of notation  we assume each pooling operation has an associated
function I such that

zijk(X) = p({xinm | (n  m) ∈ I(j  k)}).

In the literature  the set I(j  k) is referred to as the receptive ﬁeld of the pooling layer. Each receptive
ﬁeld corresponds to some q × q region in the input X. Common pooling operations include taking
the maximum of all inputs  averaging the inputs and taking an Lp norm of the inputs.

4 Lower bounds on classiﬁer robustness

Comparing the architectures of several practical CNNs such as LeNet [Lecun et al.  1998]  AlexNet
[Krizhevsky et al.  2012]  VGGNet [Simonyan and Zisserman  2015]  GoogLeNet [Szegedy et al. 
2015] and ResNet [He et al.  2016]  it would seem the only useful approach is a “modular” one. If we
succeed in lower-bounding the robustness of some layer given the robustness of the next layer  we can
work our way backwards through the network  starting at the output layer and going backwards until
we reach the input layer. That way  our approach can be applied to any feedforward neural network
as long as the robustness bounds of the different layer types have been established. To be precise  if a
given layer computes a function h of its input y and if the following layer has a robustness bound of
κ in the sense that any adversarial perturbation to this layer has a Euclidean norm of at least κ  then
we want to ﬁnd a perturbation r such that

(cid:107)h(y + r)(cid:107) = (cid:107)h(y)(cid:107) + κ.

This is clearly a necessary condition for any adversarial perturbation to the given layer. Hence  any
adversarial perturbation q to this layer will satisfy (cid:107)q(cid:107) ≥ (cid:107)r(cid:107). Of course  the output layer of the
network will require special treatment. For softmax output layers  κ is the norm of the smallest
perturbation necessary to change the maximal component of the classiﬁcation vector.
The obvious downside of this idea is that we most likely introduce cumulative approximation errors
which increase as the number of layers of the network increases. In turn  however  we get a ﬂexible
and efﬁcient framework which can handle any feedforward architecture composed of known layer
types.

4.1 Softmax output layers
We now want to ﬁnd the smallest perturbation r to the input x of a softmax layer such that f (x+r) (cid:54)=
f (x). It can be proven (Theorem A.3) that any such perturbation satisﬁes

(cid:107)r(cid:107) ≥ min
c(cid:48)(cid:54)=c

|(wc(cid:48) − wc) · x + bc(cid:48) − bc|

(cid:107)wc(cid:48) − wc(cid:107)

 

where f (x) = c. Moreover  there exist classiﬁers for which this bound is tight (Theorem A.4).

4

4.2 Fully-connected layers

To analyze the robustness of fully-connected layers to adversarial perturbations  we assume the next
layer has a robustness of κ (this will usually be the softmax output layer  however there exist CNNs
which employ fully-connected layers in other locations than just at the end [Lin et al.  2014]). We
then want to ﬁnd a perturbation r such that

(cid:107)hL(x + r)(cid:107) = (cid:107)hL(x)(cid:107) + κ.

We ﬁnd
Theorem 4.1. Let hL : Rd → Rn be twice differentiable with second-order derivatives bounded by
M. Then for any x ∈ Rd 

(cid:113)(cid:107)J (x)(cid:107)2 + 2M

(cid:107)r(cid:107) ≥

√
√

n

M

nκ − (cid:107)J (x)(cid:107)

 

(1)

where J (x) is the Jacobian matrix of hL at x.

The proof can be found in Appendix A. In Theorem A.5 it is proved that the assumptions on hL
are usually satisﬁed in practice. The proof of this theorem also yields an efﬁcient algorithm for
approximating M  a task which otherwise might involve a prohibitively expensive optimization
problem.

4.3 Convolutional layers

The next layer of the network is assumed to have a robustness bound of κ  in the sense that any
adversarial perturbation Q to X must satisfy (cid:107)Q(cid:107)F ≥ κ. We can now attempt to bound the norm of a
perturbation R to X such that

(cid:107)ReLU(W (cid:63) (X + R) + b)(cid:107)F = (cid:107)ReLU(W (cid:63) X + b)(cid:107)F + κ.

We ﬁnd
Theorem 4.2. Consider a convolutional layer with ﬁlter tensor W ∈ Rk×c×q×q and stride s whose
input consists of a 3D tensor X ∈ Rc×d×d. Suppose the next layer has a robustness bound of κ  then
any adversarial perturbation to the input of this layer must satisfy

(cid:107)R(cid:107)F ≥ κ
(cid:107)W(cid:107)F
The proof of Theorem 4.2 can be found in Appendix A.

.

4.4 Pooling layers

(2)

To facilitate the analysis of the pooling layers  we make the following assumption which is satisﬁed
by the most common pooling operations (see Appendix B):
Assumption 4.3. The pooling operation satisﬁes

zijk(X + R) ≤ zijk(X) + zijk(R).

We have
Theorem 4.4. Consider a pooling layer whose operation satisﬁes Assumption 4.3. Let the input be of
size c× d× d and the receptive ﬁeld of size q × q. Let the output be of size c× t× t. If the robustness
bound of the next layer is κ  then the following bounds hold for any adversarial perturbation R:

• MAX or average pooling:

• Lp pooling:

Proof can be found in Appendix A.

(cid:107)R(cid:107)F ≥ κ
t

.

(cid:107)R(cid:107)F ≥ κ
tq2/p

.

5

(3)

(4)

Figure 1: Illustration of LeNet architecture. Image taken from Lecun et al. [1998].

Table 1: Normalized summary of norms of adversarial perturbations found by FGS on MNIST and
CIFAR-10 test sets
Data set
MNIST
CIFAR-10

Median
0.884287
0.0091399

0.4655439
0.06103627

0.000023
0.0000012

3.306903
1.6975207

0.933448
0.0218984

Mean

Min

Max

Std

5 Experimental results

We tested the theoretical bounds on the MNIST and CIFAR-10 test sets using the Caffe [Jia et al. 
2014] implementation of LeNet [Lecun et al.  1998]. The MNIST data set [LeCun et al.  1998]
consists of 70 000 28 × 28 images of handwritten digits; the CIFAR-10 data set [Krizhevsky and
Hinton  2009] consists of 60 000 32 × 32 RGB images of various natural scenes  each belonging to
one of ten possible classes. The architecture of LeNet is depicted in Figure 1. The kernels of the two
convolutional layers will be written as W1 and W2  respectively. The output sizes of the two pooling
layers will be written as t1 and t2. The function computed by the ﬁrst fully-connected layer will be
denoted by h with Jacobian J. The last fully-connected layer has a weight matrix V and bias vector
b. For an input sample x  the theoretical lower bound on the adversarial robustness of the network
with respect to x is given by κ1  where

(cid:113)(cid:107)J (x)(cid:107)2 + 2M

√
√

M

500κ6 − (cid:107)J (x)(cid:107)
500

 

|(vc(cid:48) − vc) · x + bc(cid:48) − bc|

(cid:107)vc(cid:48) − vc(cid:107)

κ4 =

κ6 = min
c(cid:48)(cid:54)=c
κ5
t2
κ3
t1

κ2 =

 

 

 

κ5 =

κ3 =

κ1 =

κ4
(cid:107)W2(cid:107)F
κ2
(cid:107)W1(cid:107)F

 

.

Because our method only computes norms and does not provide a way to generate actual adversarial
perturbations  we used the fast gradient sign method (FGS) [Goodfellow et al.  2015] to adversarially
perturb each sample in the test sets in order to assess the tightness of our theoretical bounds. FGS
linearizes the cost function of the network to obtain an estimated perturbation

η = εsign∇xL(x  θ).

Here  ε > 0 is a parameter of the algorithm  L is the loss function and θ is the set of parameters of
the network. The magnitudes of the perturbations found by FGS depend on the choice of ε  so we
had to minimize this value in order to obtain the smallest perturbations the FGS method could supply.
This was accomplished using a simple binary search for the smallest value of ε which still resulted in
misclassiﬁcation. As the MNIST and CIFAR-10 samples have pixel values within the range [0  255] 
we upper-bounded ε by 100.
No violations of the bounds were detected in our experiments. Figure 2 shows histograms of the
norms of adversarial perturbations found by FGS and Table 1 summarizes their statistics. Histograms
of the theoretical bounds of all samples in the test set are shown in Figure 3; their statistics are
summarized in Table 2. Note that the statistics of Tables 1 and 2 have been normalized by dividing
them by the dimensionality of their respective data sets (i.e. 28 × 28 for MNIST and 3 × 32 × 32
for CIFAR-10) to allow for a meaningful comparison between the two networks. Figure 4 provides
histograms of the per-sample log-ratio between the norms of the adversarial perturbations and their
corresponding theoretical lower bounds.

6

(a) MNIST

(b) CIFAR-10

Figure 2: Histograms of norms of adversarial perturbations found by FGS on MNIST and CIFAR-10
test sets

(a) MNIST

(b) CIFAR-10

Figure 3: Histograms of theoretical bounds on MNIST and CIFAR-10 test sets

Although the theoretical bounds on average deviate considerably from the perturbations found by
FGS  one has to take into consideration that the theoretical bounds were constructed to provide a
worst-case estimate for the norms of adversarial perturbations. These estimates may not hold for
all (or even most) input samples. Furthermore  the smallest perturbations we were able to generate
on the two data sets have norms that are much closer to the theoretical bound than their averages
(0.0179 for MNIST and 0.0000012 for CIFAR-10). This indicates that the theoretical bound is
not necessarily very loose  but rather that very small adversarial perturbations occur with non-zero
probability on natural samples. Note also that the FGS method does not necessarily generate minimal
perturbations even with the smallest choice of ε: the method depends on the linearity hypothesis and
uses a ﬁrst-order Taylor approximation of the loss function. Higher-order methods may ﬁnd much
smaller perturbations by exploiting non-linearities in the network  but these are generally much less
efﬁcient than FGS.
There is a striking difference in magnitude between MNIST and CIFAR-10 of both the empirical
and theoretical perturbations: the perturbations on MNIST are much larger than the ones found for

Table 2: Normalized summary of theoretical bounds on MNIST and CIFAR-10 test sets

Data set
MNIST
CIFAR-10

Mean
7.274e−8
4.812e−13

Median
6.547e−8
4.445e−13

Std

4.229566e−8
2.605381e−13

Min

4.073e−10
7.563e−15

Max

2.932e−7
2.098e−12

7

(a) MNIST

(b) CIFAR-10

Figure 4: Histograms of the per-sample log-ratio between adversarial perturbation and lower bound
for MNIST and CIFAR-10 test sets. A higher ratio indicates a bigger deviation of the theoretical
bound from the empirical norm.

CIFAR-10. This result can be explained by the linearity hypothesis of Goodfellow et al. [2015].
The input samples of CIFAR-10 are much larger in dimensionality than MNIST samples  so the
linearity hypothesis correctly predicts that networks trained on CIFAR-10 are more susceptible to
adversarial perturbations due to the highly linear behavior these classiﬁers are conjectured to exhibit.
However  these differences may also be related to the fact that LeNet achieves much lower accuracy
on the CIFAR-10 data set than it does on MNIST (over 99% on MNIST compared to about 60% on
CIFAR-10).

6 Conclusion and future work

Despite attracting a signiﬁcant amount of research interest  a precise characterization of adversarial
examples remains elusive. In this paper  we derived lower bounds on the norms of adversarial
perturbations in terms of the model parameters of feedforward neural network classiﬁers consisting
of convolutional layers  pooling layers  fully-connected layers and softmax layers. The bounds can be
computed efﬁciently and thus may serve as an aid in model selection or the development of methods
to increase the robustness of classiﬁers. They enable one to assess the robustness of a classiﬁer
without running extensive tests  so they can be used to compare different models and quickly select
the one with highest robustness. Furthermore  the bounds enjoy a theoretical guarantee that no
adversarial perturbation could ever be smaller  so methods which increase these bounds may make
classiﬁers more robust. We tested the validity of our bounds on MNIST and CIFAR-10 and found no
violations. Comparisons with adversarial perturbations generated using the fast gradient sign method
suggest that these bounds can be close to the actual norms in the worst case.
We have only derived lower bounds for feedforward networks consisting of fully-connected layers 
convolutional layers and pooling layers. Extending this analysis to recurrent networks and other types
of layers such as Batch Normalization [Ioffe and Szegedy  2015] and Local Response Normalization
[Krizhevsky et al.  2012] is an obvious avenue for future work.
It would also be interesting to quantify just how tight the above bounds really are. In the absence
of a precise characterization of adversarial examples  the only way to do this would be to generate
adversarial perturbations using optimization techniques that make no assumptions on their underlying
cause. Szegedy et al. [2014] use a box-constrained L-BFGS approach to generate adversarial examples
without any assumptions  so using this method for comparison could provide a more accurate picture
of how tight the theoretical bounds are. It is much less efﬁcient than the FGS method  however.
The analysis presented here is a “modular” one: we consider each layer in isolation  and derive bounds
on their robustness in terms of the robustness of the next layer. However  it may also be insightful to
study the relationship between the number of layers  the breadth of each layer and the robustness of
the network. Providing estimates on the approximation errors incurred by this layer-wise approach
could also be useful.

8

Finally  there is currently no known precise characterization of the trade-off between classiﬁer robust-
ness and accuracy. Intuitively  one might expect that as the robustness of the classiﬁer increases  its
accuracy will also increase up to a point since it is becoming more robust to adversarial perturbations.
Once the robustness exceeds a certain threshold  however  we expect the accuracy to drop because the
decision surfaces are becoming too ﬂat and the classiﬁer becomes too insensitive to changes. Having
a precise characterization of this relationship between robustness and accuracy may aid methods
designed to protect classiﬁers against adversarial examples while also maintaining state-of-the-art
accuracy.

References
A. Fawzi  S.-M. Moosavi-Dezfooli  and P. Frossard. Robustness of classiﬁers: from adversarial to
random noise. In Proceedings of Advances in Neural Information Processing Systems 29  pages
1632–1640. Curran Associates  Inc.  2016.

X. Glorot  A. Bordes  and Y. Bengio. Deep sparse rectiﬁer neural networks. In Proceedings of
the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  volume 15 of
Proceedings of Machine Learning Research  pages 315–323  Fort Lauderdale  FL  USA  11–13
Apr 2011. PMLR.

I. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. In
Proceedings of the Third International Conference on Learning Representations  volume 3 of
Proceedings of the International Conference on Learning Representations  San Diego  CA  USA 
7–9 May 2015. ICLR.

S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples.

NIPS Workshop on Deep Learning and Representation Learning  2014.

K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778  Las Vegas 
NV  USA  26 Jun – 1 Jul 2016. CVPR.

S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning 
volume 37 of Proceedings of the International Conference on Machine Learning  pages 448–456 
Lille  6–11 Jul 2015. JMLR.

Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and T. Darrell.
Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM
International Conference on Multimedia  pages 675–678. ACM  2014.

A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical

report  University of Toronto  2009.

A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. In Proceedings of the 25th International Conference on Advances in Neural Information
Processing Systems  volume 25 of Advances in Neural Information Processing Systems  pages
1097–1105  Lake Tahoe  USA  3–8 Dec 2012. NIPS.

Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  Nov 1998.

Y. LeCun  C. Cortes  and C. J. Burges. The MNIST database of handwritten digits. http://yann.

lecun.com/exdb/mnist/  1998. Accessed 2017-04-17.

M. Lin  Q. Chen  and S. Yan. Network in network. Proceedings of International Conference on

Learning Representations  2014.

Y. Lou  X. Boix  G. Roig  T. Poggio  and Q. Zhao. Foveation-based mechanisms alleviate adversarial

examples. arXiv preprint arXiv:1511.06292  2016.

N. Papernot  P. McDaniel  X. Wu  S. Jha  and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy
(SP)  pages 582–597  May 2016.

9

A. Rozsa  M. Gunther  and T. E. Boult. Towards robust deep neural networks with BANG. arXiv

preprint arXiv:1612.00138  2016.

K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
In Proceedings of the Third International Conference on Learning Representations  volume 3 of
Proceedings of the International Conference on Learning Representations  San Diego  CA  USA 
7–9 May 2015. ICLR.

C. Szegedy  W. Zaremba  and I. Sutskever. Intriguing properties of neural networks. In Proceedings
of the Second International Conference on Learning Representations  volume 2 of Proceedings
of the International Conference on Learning Representations  Banff  Canada  14–16 Apr 2014.
ICLR.

C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  and
A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  Boston  MA  USA  7-12 Jun 2015. CVPR.

10

,Jonathan Peck
Joris Roels
Bart Goossens
Yvan Saeys