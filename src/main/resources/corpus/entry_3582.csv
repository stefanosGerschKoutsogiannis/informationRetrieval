2018,Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects  we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to generate high-resolution objects with more efficient scaling than methods which work directly in 3D. We decompose the problem of 2D depth super-resolution into silhouette and depth prediction to capture both structure and fine detail. This allows our method to generate sharp edges more easily than an individual network. We evaluate our work on multiple experiments concerning high-resolution 3D objects  and show our system is capable of accurately predicting novel objects at resolutions as large as 512x512x512 -- the highest resolution reported for this task. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset  and further demonstrate the first effective 3D super-resolution method.,Multi-View Silhouette and Depth Decomposition for

High Resolution 3D Object Representation

Edward Smith
McGill University

edward.smith@mail.mcgill.ca

Scott Fujimoto
McGill University

scott.fujimoto@mail.mcgill.ca

David Meger

McGill University

dmeger@cim.mcgill.ca

Abstract

We consider the problem of scaling deep generative shape models to high-resolution.
Drawing motivation from the canonical view representation of objects  we introduce
a novel method for the fast up-sampling of 3D objects in voxel space through
networks that perform super-resolution on the six orthographic depth projections.
This allows us to generate high-resolution objects with more efﬁcient scaling than
methods which work directly in 3D. We decompose the problem of 2D depth
super-resolution into silhouette and depth prediction to capture both structure and
ﬁne detail. This allows our method to generate sharp edges more easily than an
individual network. We evaluate our work on multiple experiments concerning
high-resolution 3D objects  and show our system is capable of accurately predicting
novel objects at resolutions as large as 512×512×512 – the highest resolution
reported for this task. We achieve state-of-the-art performance on 3D object
reconstruction from RGB images on the ShapeNet dataset  and further demonstrate
the ﬁrst effective 3D super-resolution method.

1

Introduction

The 3D shape of an object is a combination of countless physical elements that range in scale
from gross structure and topology to minute textures endowed by the material of each surface.
Intelligent systems require representations capable of modeling this complex shape efﬁciently  in
order to perceive and interact with the physical world in detail (e.g.  object grasping  3D perception 
motion prediction and path planning). Deep generative models have recently achieved strong
performance in hallucinating diverse 3D object shapes  capturing their overall structure and rough
texture [3  37  47]. The ﬁrst generation of these models utilized voxel representations which scale
cubically with resolution  limiting training to only 643 shapes on typical hardware. Numerous recent
papers have begun to propose high resolution 3D shape representations with better scaling  such
as those based on meshes  point clouds or octrees but these often require more difﬁcult training
procedures and customized network architectures.
Our 3D shape model is motivated by a foundational concept in 3D perception: that of canonical
views. The shape of a 3D object can be completely captured by a set of 2D images from multiple
viewpoints (see [21  4] for an analysis of selecting the location and number of viewpoints). Deep
learning approaches for 2D image recognition and generation [40  10  8  13] scale easily to high
resolutions. This motivates the primary question in this paper: can a multi-view representation be
used efﬁciently with modern deep learning methods?

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Scene created from objects reconstructed by our method from RGB images at 2563 resolution. See the
supplementary video for better viewing https://sites.google.com/site/mvdnips2018.

We propose a novel approach for deep shape interpretation which captures the structure of an object
via modeling of its canonical views in 2D as depth maps  in a framework we refer to as Multi-
View Decomposition (MVD). By utilizing many 2D orthographic projections to capture shape 
a model represented in this fashion can be up-scaled to high resolution by performing semantic
super-resolution in 2D space  which leverages efﬁcient  well-studied network structures and training
procedures. The higher resolution depth maps are ﬁnally merged into a detailed 3D object using
model carving.
Our method has several key components that allow effective and efﬁcient training. We leverage
two synergistic deep networks that decompose the task of representing an object’s depth: one that
outputs the silhouette – capturing the gross structure; and a second that produces the local variations
in depth – capturing the ﬁne detail. This decomposition addresses the blurred images that often occur
when minimizing reconstruction error by allowing the silhouette prediction to form sharp edges. Our
method utilizes the low-resolution input shape as a rough template which simply needs carving and
reﬁnement to form the high resolution product. Learning the residual errors between this template
and the desired high resolution shape simpliﬁes the generation task and allows for constrained output
scaling  which leads to signiﬁcant performance improvements.
We evaluate our method’s ability to perform 3D object reconstruction on the the ShapeNet dataset [1].
This standard evaluation task requires generating high resolution 3D objects from single 2D RGB
images. Furthermore  due to the nature of our pipeline we present the ﬁrst results for 3D object
super-resolution – generating high resolution 3D objects directly from low resolution 3D objects. Our
method achieves state-of-the-art quantitative performance  when compared to a variety of other 3D
representations such as octrees  mesh-models and point clouds. Furthermore  our system is the ﬁrst
to produce 3D objects at 5123 resolution. We demonstrate these objects are visually impressive in
isolation  and when compared to the ground truth objects. We additionally demonstrate that objects
reconstructed from images can be placed in scenes to create realistic environments  as shown in ﬁgure
1. In order to ensure reproducible experimental comparison  code for our system has been made
publicly available on a GitHub repository1. Given the efﬁciency of our method  each experiment was
run on a single NVIDIA Titan X GPU in the order of hours.

2 Related Work

Deep Learning with 3D Data
Recent advances with 3D data have leveraged deep learning 
beginning with architectures such as 3D convolutions for object classiﬁcation [25  19]. When adapted
to 3D generation  these methods typically use an autoencoder network  with a decoder composed of
3D deconvolutional layers [3  47]. This decoder receives a latent representation of the 3D shape and
produces a probability for occupancy at each discrete position in 3D voxel space. This approach has
been combined with generative adversarial approaches [8] to generate novel 3D objects [47  41  20] 
but only at a limited resolution.

1https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-

Resolution-3D-Object-Representation

2

Figure 2: The complete pipeline for 3D object reconstruction and super-resolution outlined in this paper. Our
method accepts either a single RGB image for low resolution reconstruction or a low resolution object for 3D
super-resolution. ODM up-scaling is deﬁned in section 3.1 and model carving in section 3.2

2D Super-Resolution
Super-resolution of 2D images is a well-studied problem [29]. Traditionally 
image super-resolution has used dictionary-style methods [7  49]  matching patches of images to
higher-resolution counterparts. This research also extends to depth map super-resolution [22  28  11].
Modern approaches to super-resolution are built on deep convolutional networks [5  46  27] as well
as generative adversarial networks [18  13] which use an adversarial loss to imagine high-resolution
details in RGB images.
Multi-View Representation Our work connects to multi-view representations which capture the
characteristics of a 3D object from multiple viewpoints in 2D [17  26  43  32  12  39  34]  such as
decomposing image silhouettes [23  42]  Light Field Descriptors [2]  and 2D panoramic mapping [38].
Other representations aim to use orientation [36]  rotational invariance [15] or 3D-SURF features [16].
While many of these representations are effective for 3D classiﬁcation  they have not previously been
utilized to recover 3D shape in high resolution.
Efﬁcient 3D Representations Given that naïve representations of 3D data require cubic computa-
tional costs with respect to resolution  many alternate representations have been proposed. Octree
methods [44  9] use non-uniform discretization of the voxel space to efﬁciently capture 3D objects by
adapting the discretization level locally based on shape. Hierarchical surface prediction (HSP) [9]
is an octree-style method which divides the voxel space into free  occupied and boundary space.
The object is generated at different scales of resolution  where occupied space is generated at a very
coarse resolution and the boundary space is generated at a very ﬁne resolution. Octree generating
networks (OGN) [44] use a convolutional network that operates directly on octrees  rather than in
voxel space. These methods have only shown novel generation results up to 2563 resolution. Our
method achieves higher accuracy at this resolution and can efﬁciently produce novel objects as large
as 5123.
A recent trend is the use of unstructured representations such as mesh models [31  14  45] and
point clouds [33  6] which represent the data by an unordered set with a ﬁxed number of points.
MarrNet [48]  which resembles our work  models 3D objects through the use of 2.5D sketches 
which capture depth maps from a single viewpoint. This approach requires working in voxel space
when translating 2.5D sketches to high resolution  while our method can work directly in 2D space 
leveraging 2D super-resolution technology within the 3D pipeline.

3 Method

In this section we describe our methodology for representing high resolution 3D objects. Our
algorithm is a novel approach which uses the six axis-aligned orthographic depth maps (ODM)  to
efﬁciently scale 3D objects to high resolution without directly interacting with the voxels. To achieve
this  a pair of networks is used for each view  decomposing the super-resolution task into predicting
the silhouette and relative depth from the low resolution ODM. This approach is able to recover ﬁne
object details and scales better to higher resolutions than previous methods  due to the simpliﬁed
learning problem faced by each network  and scalable computations that occur primarily in 2D image
space.

3

䰀漀眀 刀攀猀漀氀甀琀椀漀渀 刀攀挀漀渀猀琀爀甀挀琀椀漀渀                               ㌀䐀 伀戀樀攀挀琀 匀甀瀀攀爀ⴀ刀攀猀漀氀甀琀椀漀渀䔀渀挀漀搀攀爀                   䐀攀挀漀搀攀爀䤀洀愀最攀                 倀爀攀搀椀挀琀攀搀 伀戀樀攀挀琀䔀砀琀爀愀挀琀攀搀 伀䐀䴀猀䰀漀眀 刀攀猀漀氀甀琀椀漀渀 伀戀樀攀挀琀䔀砀愀挀琀氀礀 唀瀀ⴀ匀挀愀氀攀搀 伀戀樀攀挀琀一攀愀爀攀猀琀 一攀椀最栀戀漀爀 唀瀀ⴀ猀挀愀氀椀渀最䘀椀渀愀氀 倀爀攀搀椀挀琀椀漀渀䠀椀最栀 刀攀猀漀氀甀琀椀漀渀 伀䐀䴀猀伀䐀䴀 唀瀀ⴀ匀挀愀氀椀渀最䴀漀搀攀氀 䌀愀爀瘀椀渀最Figure 3: Our Multi-View Decomposition framework (MVD). Each ODM prediction task can be decomposed
into a silhouette and detail prediction. We further simplify the detail prediction task by encoding only the residual
details (change from the low resolution input)  masked by the ground truth silhouette.

3.1 Orthographic Depth Map Super-Resolution

Our method begins by obtaining the orthographic depth maps of the six primary views of the low-
resolution 3D object. In an ODM  each pixel holds a value equal to the surface depth of the object
along the viewing direction at the corresponding coordinate. This projection can be computed quickly
and easily from an axis-aligned 3D array via z-clipping. Super-resolution is then performed directly
on these ODMs  before being mapped onto the low resolution object to produce a high resolution
object.
Representing an object by a set of depth maps however  introduces a challenging learning problem 
which requires both local and global consistency in depth. Furthermore  minimizing the mean squared
error results in blurry images without sharp edges [24  30]. This is particularly problematic as a depth
map is required to be bimodal  with large variations in depth to create structure and small variations
in depth to create texture and ﬁne detail. To address this concern  we propose decomposing the
learning problem into predicting the silhouette and depth map separately. Separating the challenge
of predicting gross shape from ﬁne detail regularizes and reduces the complexity of the learning
problem  leading to improved results when compared with directly estimating new surface depths.
Our Multi-View Decomposition framework (MVD) uses a set of twin of deep convolutional models
fSIL and f∆D  to separately predict silhouette and variations in depth of the higher resolution ODM.
We depict our system in ﬁgure 3. The deep convolutional network for predicting the high-resolution
silhouette  fSIL with parameters θ  is passed the low resolution ODM DL  extracted from the input
3D object. The network outputs a probability that each pixel is occupied. It is trained by minimizing
the mean squared error between the predicted and true silhouette of the high resolution ODM DH:

N(cid:88)

i=1

L(θ) =

(cid:107)fSIL(D(i)

L ; θ) − 1

H (cid:54)=0(D(i)

D(i)

H )(cid:107)2 

(1)

where 1

H (cid:54)=0 is an indicator function for each coordinate in the image.
D(i)

The same low-resolution ODM DL is passed through the second deep convolution neural network 
denoted f∆D with parameters φ  whose ﬁnal output is passed through a sigmoid  to produce an
estimate for the variation of the ODM within a ﬁxed range r. This output is added to the low-
resolution depth map to produce our prediction for a constrained high-resolution depth map CH:

CH = rσ(f∆D(DL; φ)) + g(DL) 

(2)

where g(·) denotes up-sampling using nearest neighbor interpolation.
We train our network f∆D by minimizing the mean squared error between our prediction and the
ground truth high-resolution depth map DH. During training only  we mask the output with the ground
truth silhouette to allow effective focus on ﬁne detail for f∆D. We further add a smoothing regularizer

(cid:112)(xi+1 j − xi j)2 + (xi j+1 − xi j)2 [35] within

which penalizes the total variation V (x) =(cid:80)

i j

4

the predicted ODM. Our loss function is a simple combination of these terms:

L(φ) =

(cid:107)(C (i)

H ◦ 1

H (j k)(cid:54)=0(D(i)

D(i)

H )) − D(i)

H (cid:107)2 + λV (C (i)
H ) 

(3)

N(cid:88)

i=1

where ◦ is the Hadamard product. The total variation penalty is used as an edge-preserving denoising
which smooths out irregularities in the output.
The output of the constrained depth map and silhouette networks are then combined to produce a
complete prediction for the high-resolution ODM. This accomplished by masking the constrained
high-resolution depth map by the predicted silhouette:

ˆDH = CH ◦ fSIL(DL; θ).

(4)
ˆDH denotes our predicted high resolution ODM which can then be mapped back onto the original low
resolution object by model carving to produce a high resolution object. Each of the 6 high resolution
ODMS are predicted using the same 2 network models  with the side information for each passed
using a forth channel in the corresponding low resolution ODM passed to the networks.

3.2

3D Model Carving

To complete our super-resolution procedure  the six ODMs are combined with the low-resolution
object to form a high-resolution object. This begins by further smoothing the up-sampled ODM with
an adaptive averaging ﬁlter  which only consider neighboring pixels within a small radius. To preserve
edges  only neighboring pixels within a threshold of the value of the center pixel are included. This
smoothing  along with the total variation regularization in the our loss function  are added to enforce
smooth changes in local depth regions.
Model carving begins by ﬁrst up-sampling the low-resolution model to the desired resolution  using
nearest neighbor interpolation. We then use the predicted ODMs ˆDH = CH ◦ fSIL(DL; θ) to
determine the surface of the new object. The carving procedure is separated into (1) structure carving 
corresponding to the silhouette prediction fSIL(DL; θ)  and (2) detail carving  corresponding to the
constrained depth prediction CH.
For the structure carving  for each predicted ODM fSIL(DL; θ)  if a coordinate is predicted unoccu-
pied  then all voxels perpendicular to the coordinate are highlighted to be removed. The removal
only occurs if there is agreement of at least two ODMs for the removal of a voxel. As there is a
large amount of overlap in the surface area that the six ODMs observe  this silhouette agreement is
enforced to maintain the structure of the object.
This same process occurs for detail carving with CH. However  we do not require agreement within
the constrained depth map predictions. This is because  unlike the silhouettes  a depth map can cause
or deepen concavities in the surface of the object which may not be visible from any other face.
Requiring agreement among depth maps would eliminate their ability to inﬂuence these concavities.
Thus  performing detail carving simply involves removing all voxels perpendicular to each coordinate
of each ODM  up to the predicted depth.

4 Experiments

In this section we present our results for our method  Multi-View Decomposition Networks (MVD) 
for both 3D object super-resolution and 3D object reconstruction from single RGB images. Our
results are evaluated across 13 classes of the ShapeNet [1] dataset. 3D super-resolution is the task
of generating a high resolution 3D object conditioned on a low resolution input  while 3D object
reconstruction is the task of re-creating high resolution 3D objects from a single RGB image of the
object.

4.1

3D Object Super-Resolution

Dataset
The dataset consists of 323 low resolution voxelized objects and their 2563 high resolution
counterparts. These objects were produced by converting CAD models found in the ShapeNetCore
dataset [1] into voxel format  in a canonical view. We work with the three commonly used object

5

(a)

(b)

Figure 4: Super-resolution rendering results. Each set shows  from left to right  the low resolution input and the
results of MVD at 5123. Sets in (b) additionally show the ground-truth 5123 objects on the far right.

Figure 5: Super-resolution rendering results. Each pair shows the low resolution input (left) and the results of
MVD at 2563 resolution (right).

classes from this dataset: Car  Chair and Plane  with around 8000  7000  4000 objects respectively.
For training  we pre-process this dataset  to extract the six ODMs from each object at high and
low-resolution. CAD models converted at this resolution do not remain watertight in many cases 
making it difﬁcult to ﬁll the inner volume of the object. We describe an efﬁcient method for obtaining
high resolution voxelized objects in the supplementary material. Data is split into training  validation 
and test set using a ratio of 70:10:20 respectively.
Evaluation We evaluate our method quantitatively using the intersection over union metric (IoU)
against a simple baseline and the prediction of the individual networks on the test set. The baseline
method corresponds to the ground truth at 323 resolution  by up-scaling to the high resolution using
nearest neighbor up-sampling. While our full method  MVD  uses a combination of networks  we
present an ablation study to evaluate the contribution of each separate network.
Implementation
The super-resolution task requires a pair of networks  f∆D and fSIL  which share
the same architecture. This architecture is derived from the generator of SRGAN [18]  a state of the
art 2D super-resolution network. Exact network architectures and training regime are provided in the
supplementary material.
Results
The super-resolution IoU scores are presented in table 1. Our method greatly outperforms
the naïve nearest neighbor up-sampling baseline in every class. While we ﬁnd that the silhouette
prediction contributes far more to the IoU score  the addition of the depth variation network further
increases the IoU score. This is due to the silhouette capturing the gross structure of the object from
multiple viewpoints  while the depth variation captures the ﬁne-grained details  which contributes
less to the total IoU score. To qualitatively demonstrate the results of our super-resolution system we
render objects from the test set at both 2563 resolution in ﬁgure 5 and 5123 resolution in ﬁgure 4.
The predicted high-resolution objects are all of high quality and accurately mimic the shapes of the
ground truth objects. Additional 5123 renderings as well as multiple objects from each class at 2563
resolution can be found in our supplementary material.

4.2

3D Object Reconstruction from RGB Images

Dataset
To match the datasets used by prior work  two datasets are used for 3D object reconstruc-
tion  both derived from the ShapeNet dataset. The ﬁrst  which we refer to as DataHSP   consists of

6

Category Baseline Depth Variation (f∆D)
Car
Chair
Plane

73.2
54.9
39.9

80.6
58.5
50.5

Silhouette (fSIL) MVD (Both)

86.9
67.3
70.2

89.9
68.5
71.1

Table 1: Super-Resolution IoU Results against nearest neighbor baseline and an ablation over individual networks
at 2563 from 323 input.

Figure 6: 3D object reconstruction 2563 rendering results from our method  MVD (bottom)  of the 13 classes
from ShapeNet  by interpreting 2D image input (top).

only the Car  Chair and Plane classes from the Shapenet dataset  and we re-use the 323 and 2563
voxel objects produced for these classes in the previous section. The CAD models for each of these
object were rendered into 1282 RGB images capturing random viewpoints of the objects at elevations
between (−20◦  30◦) and all possible azimuth rotations. The voxelized objects and corresponding
images were split into a training  validation and test set  with a ratio of 70:10:20 respectively.
The second dataset  which we refer to as Data3D−R2N 2  is that provided by Choy et al. [3]. It
consists of images and objects produced from the 3 classes in the ShapeNet dataset used in the
previous section  as well as 10 additional classes  for a total of around 50000 objects. From each
object 1372 RGB images are rendered at random viewpoints  and we again compute their 323 and
2563 resolution voxelized models and ODMs. The data is split into a training  validation and test set
with a ratio of 70:10:20.
Evaluation We evaluate our method quantitatively with two evaluation schemes. In the ﬁrst  we
use IoU scores when reconstructing objects at 2563 resolution. We compare against HSP [9] using
the ﬁrst dataset DataHSP   and against OGN [44] using the second dataset Data3D−R2N 2. To study
the effectiveness of our super-resolution pipeline  we also compute the IoU scores using the low
resolution objects predicted by our autoencoder (AE) with nearest neighbor up-sampling to produce
predictions at 2563 resolution.
Our second evaluation is performed only on the second dataset  Data3D−R2N 2  by comparing the
accuracy of the surfaces of predicted objects to those of the ground truth meshes. Following the
evaluation procedure deﬁned by Wang et al. [45]  we ﬁrst convert the 2563 voxel models into meshes
by deﬁning squared polygons on all exposed faces on the surface of the voxel models. We then
uniformly sample points from the two mesh surfaces and compute F1 scores. Precision and recall are
calculated using the percentage of points found with a nearest neighbor in the ground truth sampling
set less than a squared distance threshold of 0.0001. We compare to state of the art mesh model
methods  N3MR [14] and Pixel2Mesh [45]  a point cloud method  PSG [6]  and a voxel baseline 
3D-R2N2 [3]  using the values reported by Wang et al. [45].
Implementation
For 3D object reconstruction  we ﬁrst trained a standard autoencoder  similar to
prior work [3  41]  to produce objects at 323 resolution. These low resolution objects are then used
with our 3D super-resolution method  to generate 3D object reconstructions at a high 2563 resolution.
This process is described in ﬁgure 2. The exact network architecture and training regime are provided
in the supplementary material.

7

Category AE HSP [9] MVD (Ours)
Car
Chair
Plane

72.7
40.1
56.4

70.1
37.8
56.1

55.2
36.4
28.9

Category AE OGN [44] MVD (Ours)
Car
Chair
Plane

80.7
43.3
58.9

68.1
37.6
34.6

78.2

-
-

(a) DataHSP

(b) Data3D−R2N 2

Table 2: 3D Object Reconstruction IoU at 2563. Cells with a dash - indicate that the corresponding result was
not reported by the original author.

Category
Plane
Bench
Cabinet
Car
Chair
Monitor
Lamp
Speaker
Firearm
Couch
Table
Cellphone
Watercraft
Mean

3D-R2N2 [3]

41.46
34.09
49.88
37.80
40.22
34.38
32.35
45.30
28.34
40.01
43.79
42.31
37.10
39.01

PSG [6] N3MR [14]
68.20
49.29
39.93
50.70
41.60
40.53
41.40
32.61
69.96
36.59
53.44
55.95
51.28
48.58

62.10
35.84
21.04
36.66
30.25
28.77
27.97
19.46
52.22
25.04
28.40
27.96
43.71
33.80

Pixel2Mesh [45] MVD (Ours)

71.12
57.57
60.39
67.86
54.38
51.39
48.15
48.84
73.20
51.90
66.30
70.24
55.12
59.72

87.34
69.92
65.87
67.69
62.57
57.48
48.37
53.88
78.12
53.66
68.06
86.00
64.07
66.39

Table 3: 3D object reconstruction surface sampling F1 scores.

Results
The results of our IoU evaluation compared to the octree methods [44  9] can be seen in
table 2. We achieve state-of-the-art performance on every object class in both datasets. Our surface
accuracy results can be seen in table 3 compared to [45  6  14  3]. Our method achieves state of the art
results on all 13 classes. We show signiﬁcant improvements for many object classes and demonstrate
a large improvement on the mean over all classes when compared against the methods presented. To
qualitatively evaluate our performance  we rendered our reconstructions for each class  which can be
seen in ﬁgure 6. Additional renderings can be found in the supplementary material.

5 Conclusion

In this paper we argue for the application of multi-view representations when predicting the structure
of objects at high resolution. We outline our Multi-View Decomposition framework  a novel system
for learning to represent 3D objects and demonstrate its afﬁnity for capturing category-speciﬁc shape
details at a high resolution by operating over the six orthographic projections of the object.
In the task of super-resolution  our method outperforms baseline methods by a large margin  and
we show its ability to produce objects as large as 5123  with a 16 times increase in size from the
input objects. The results produced are visually impressive  even when compared against the ground-
truth. When applied to the reconstruction of high-resolution 3D objects from single RGB images 
we outperform several state of the art methods with a variety of representation types  across two
evaluation metrics.
All of our visualizations demonstrate the effectiveness of our method at capturing ﬁne-grained detail 
which is not present in the low resolution input but must be captured in our network’s weights during
learning. Furthermore  given that the deep aspect of our method works entirely in 2D space  our
method scales naturally to high resolutions. This paper demonstrates that multi-view representations
along with 2D super-resolution through decomposed networks is indeed capable of modeling complex
shapes.

8

References
[1] Angel X Chang  Thomas Funkhouser  Leonidas Guibas  Pat Hanrahan  Qixing Huang  Zimo Li 
Silvio Savarese  Manolis Savva  Shuran Song  Hao Su  et al. Shapenet: An information-rich 3d
model repository. arXiv preprint arXiv:1512.03012  2015.

[2] Ding-Yun Chen  Xiao-Pei Tian  Yu-Te Shen  and Ming Ouhyoung. On visual similarity based
3d model retrieval. In Computer graphics forum  volume 22  pages 223–232. Wiley Online
Library  2003.

[3] Christopher B Choy  Danfei Xu  JunYoung Gwak  Kevin Chen  and Silvio Savarese. 3d-r2n2:
A uniﬁed approach for single and multi-view 3d object reconstruction. In European Conference
on Computer Vision  pages 628–644. Springer  2016.

[4] Trip Denton  M Fatih Demirci  Jeff Abrahamson  Ali Shokoufandeh  and Sven Dickinson.
Selecting canonical views for view-based 3-d object recognition. In Pattern Recognition  2004.
ICPR 2004. Proceedings of the 17th International Conference on  volume 2  pages 273–276.
IEEE  2004.

[5] Chao Dong  Chen Change Loy  Kaiming He  and Xiaoou Tang. Image super-resolution using
deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence 
38(2):295–307  2016.

[6] Haoqiang Fan  Hao Su  and Leonidas Guibas. A point set generation network for 3d object
reconstruction from a single image. In Conference on Computer Vision and Pattern Recognition
(CVPR)  volume 38  2017.

[7] William T Freeman  Thouis R Jones  and Egon C Pasztor. Example-based super-resolution.

IEEE Computer graphics and Applications  22(2):56–65  2002.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680. 2014.

[9] Christian Häne  Shubham Tulsiani  and Jitendra Malik. Hierarchical surface prediction for 3d

object reconstruction. arXiv preprint arXiv:1704.00710  2017.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[11] Tak-Wai Hui  Chen Change Loy  and Xiaoou Tang. Depth map super-resolution by deep

multi-scale guidance. pages 353–369  2016.

[12] Abhishek Kar  Christian Häne  and Jitendra Malik. Learning a multi-view stereo machine. In

Advances in Neural Information Processing Systems  pages 364–375  2017.

[13] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for
improved quality  stability  and variation. International Conference on Learning Representations 
2018.

[14] Hiroharu Kato  Yoshitaka Ushiku  and Tatsuya Harada. Neural 3d mesh renderer. arXiv preprint

arXiv:1711.07566  2017.

[15] Michael Kazhdan  Thomas Funkhouser  and Szymon Rusinkiewicz. Rotation invariant spherical
In Symposium on geometry processing 

harmonic representation of 3 d shape descriptors.
volume 6  pages 156–164  2003.

[16] Jan Knopp  Mukta Prasad  Geert Willems  Radu Timofte  and Luc Van Gool. Hough transform
and 3d surf for robust three dimensional classiﬁcation. In European Conference on Computer
Vision  pages 589–602. Springer  2010.

[17] Jan J Koenderink and Andrea J Van Doorn. The singularities of the visual mapping. Biological

cybernetics  24(1):51–59  1976.

9

[18] Christian Ledig  Lucas Theis  Ferenc Huszár  Jose Caballero  Andrew Cunningham  Ale-
jandro Acosta  Andrew Aitken  Alykhan Tejani  Johannes Totz  Zehan Wang  et al. Photo-
realistic single image super-resolution using a generative adversarial network. arXiv preprint
arXiv:1609.04802  2016.

[19] Yangyan Li  Soeren Pirk  Hao Su  Charles R Qi  and Leonidas J Guibas. Fpnn: Field probing
neural networks for 3d data. In Advances in Neural Information Processing Systems  pages
307–315  2016.

[20] Jerry Liu  Fisher Yu  and Thomas Funkhouser. Interactive 3d modeling with a generative

adversarial network. arXiv preprint arXiv:1706.05170  2017.

[21] Q-T Luong and Thierry Viéville. Canonical representations for the geometries of multiple

projective views. Computer vision and image understanding  64(2):193–229  1996.

[22] Oisin Mac Aodha  Neill DF Campbell  Arun Nair  and Gabriel J Brostow. Patch based synthesis
for single depth image super-resolution. In European Conference on Computer Vision  pages
71–84. Springer  2012.

[23] Diego Macrini  Ali Shokoufandeh  Sven Dickinson  Kaleem Siddiqi  and Steven Zucker. View-
based 3-d object recognition using shock graphs. In Pattern Recognition  2002. Proceedings.
16th International Conference on  volume 3  pages 24–28. IEEE  2002.

[24] Michael Mathieu  Camille Couprie  and Yann LeCun. Deep multi-scale video prediction beyond

mean square error. arXiv preprint arXiv:1511.05440  2015.

[25] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-
time object recognition. In Intelligent Robots and Systems (IROS)  2015 IEEE/RSJ International
Conference on  pages 922–928. IEEE  2015.

[26] Hiroshi Murase and Shree K Nayar. Visual learning and recognition of 3-d objects from

appearance. International journal of computer vision  14(1):5–24  1995.

[27] Christian Osendorfer  Hubert Soyer  and Patrick Van Der Smagt.

Image super-resolution
with fast approximate convolutional sparse coding. In International Conference on Neural
Information Processing  pages 250–257. Springer  2014.

[28] Jaesik Park  Hyeongwoo Kim  Yu-Wing Tai  Michael S Brown  and Inso Kweon. High quality
depth map upsampling for 3d-tof cameras. In Computer Vision (ICCV)  2011 IEEE International
Conference on  pages 1623–1630. IEEE  2011.

[29] Sung Cheol Park  Min Kyu Park  and Moon Gi Kang. Super-resolution image reconstruction: a

technical overview. IEEE signal processing magazine  20(3):21–36  2003.

[30] Deepak Pathak  Philipp Krahenbuhl  Jeff Donahue  Trevor Darrell  and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 2536–2544  2016.

[31] Jhony K Pontes  Chen Kong  Sridha Sridharan  Simon Lucey  Anders Eriksson  and Clinton
Fookes. Image2mesh: A learning framework for single image 3d reconstruction. arXiv preprint
arXiv:1711.10669  2017.

[32] Charles R Qi  Hao Su  Matthias Nießner  Angela Dai  Mengyuan Yan  and Leonidas J Guibas.
Volumetric and multi-view cnns for object classiﬁcation on 3d data. In Proceedings of the IEEE
conference on computer vision and pattern recognition  pages 5648–5656  2016.

[33] Charles R Qi  Hao Su  Kaichun Mo  and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classiﬁcation and segmentation. Proc. Computer Vision and Pattern Recognition
(CVPR)  IEEE  1(2):4  2017.

[34] Gernot Riegler  Ali Osman Ulusoy  Horst Bischof  and Andreas Geiger. Octnetfusion: Learning

depth fusion from data. In Proceedings of the International Conference on 3D Vision  2017.

[35] Leonid I Rudin  Stanley Osher  and Emad Fatemi. Nonlinear total variation based noise removal

algorithms. Physica D: nonlinear phenomena  60(1-4):259–268  1992.

10

[36] Ashutosh Saxena  Min Sun  and Andrew Y Ng. Make3d: Learning 3d scene structure from
a single still image. IEEE transactions on pattern analysis and machine intelligence  31(5):
824–840  2009.

[37] Abhishek Sharma  Oliver Grau  and Mario Fritz. Vconv-dae: Deep volumetric shape learning
without object labels. In European Conference on Computer Vision  pages 236–250. Springer 
2016.

[38] Baoguang Shi  Song Bai  Zhichao Zhou  and Xiang Bai. Deeppano: Deep panoramic rep-
resentation for 3-d shape recognition. IEEE Signal Processing Letters  22(12):2339–2343 
2015.

[39] Daeyun Shin  Charless Fowlkes  and Derek Hoiem. Pixels  voxels  and views: A study of shape
representations for single view 3d object shape prediction. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2018.

[40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[41] Edward J Smith and David Meger. Improved adversarial systems for 3d object generation and

reconstruction. In Conference on Robot Learning  pages 87–96  2017.

[42] Amir Arsalan Soltani  Haibin Huang  Jiajun Wu  Tejas D Kulkarni  and Joshua B Tenen-
baum. Synthesizing 3d shapes via modeling multi-view depth maps and silhouettes with deep
generative networks.

[43] Hang Su  Subhransu Maji  Evangelos Kalogerakis  and Erik Learned-Miller. Multi-view convo-
lutional neural networks for 3d shape recognition. In Proceedings of the IEEE international
conference on computer vision  pages 945–953  2015.

[44] Maxim Tatarchenko  Alexey Dosovitskiy  and Thomas Brox. Octree generating networks:
Efﬁcient convolutional architectures for high-resolution 3d outputs. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 2088–2096  2017.

[45] Nanyang Wang  Yinda Zhang  Zhuwen Li  Yanwei Fu  Wei Liu  and Yu-Gang Jiang. Pixel2mesh:
Generating 3d mesh models from single rgb images. arXiv preprint arXiv:1804.01654  2018.

[46] Zhaowen Wang  Ding Liu  Jianchao Yang  Wei Han  and Thomas Huang. Deep networks for
image super-resolution with sparse prior. In Proceedings of the IEEE International Conference
on Computer Vision  pages 370–378  2015.

[47] Jiajun Wu  Chengkai Zhang  Tianfan Xue  William T Freeman  and Joshua B Tenenbaum.
Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling.
In Advances in Neural Information Processing Systems  pages 82–90  2016.

[48] Jiajun Wu  Yifan Wang  Tianfan Xue  Xingyuan Sun  Bill Freeman  and Josh Tenenbaum.
In Advances In Neural Information

Marrnet: 3d shape reconstruction via 2.5 d sketches.
Processing Systems  pages 540–550  2017.

[49] Jianchao Yang  John Wright  Thomas S Huang  and Yi Ma. Image super-resolution via sparse

representation. IEEE transactions on image processing  19(11):2861–2873  2010.

11

,Amélie Heliou
Johanne Cohen
Panayotis Mertikopoulos
Edward Smith
Scott Fujimoto
David Meger
Kai Zheng
Haipeng Luo
Ilias Diakonikolas
Liwei Wang