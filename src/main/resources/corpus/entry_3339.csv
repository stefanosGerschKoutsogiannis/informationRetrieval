2019,Margin-Based Generalization Lower Bounds for Boosted Classifiers,Boosting is one of the most successful ideas in machine learning. 
The most well-accepted explanations for the low generalization error 
of boosting algorithms such as AdaBoost stem from
margin theory. The study of margins in the context of boosting
algorithms was initiated by Schapire  Freund  Bartlett and Lee (1998)  
and has inspired numerous boosting algorithms and generalization bounds. 
To date  the strongest known generalization (upper bound) is the $k$th margin 
bound of Gao and Zhou (2013). 
Despite the numerous generalization upper bounds that have been proved 
over the last two decades  nothing is known about the tightness of these bounds. 
In this paper  we give the first margin-based lower bounds on the generalization
error of boosted classifiers. Our lower bounds nearly match the
$k$th margin bound and thus almost settle the generalization performance
of boosted classifiers in terms of margins.,Margin-Based Generalization Lower Bounds for

Boosted Classiﬁers

Allan Grønlund ‡§

Lior Kamma § Kasper Green Larsen § Alexander Mathiasen §

Jelani Nelson ∗

Abstract

Boosting is one of the most successful ideas in machine learning. The most well-
accepted explanations for the low generalization error of boosting algorithms such
as AdaBoost stem from margin theory. The study of margins in the context of
boosting algorithms was initiated by Schapire  Freund  Bartlett and Lee (1998) and
has inspired numerous boosting algorithms and generalization bounds. To date 
the strongest known generalization (upper bound) is the kth margin bound of Gao
and Zhou (2013). Despite the numerous generalization upper bounds that have
been proved over the last two decades  nothing is known about the tightness of
these bounds. In this paper  we give the ﬁrst margin-based lower bounds on the
generalization error of boosted classiﬁers. Our lower bounds nearly match the kth
margin bound and thus almost settle the generalization performance of boosted
classiﬁers in terms of margins.

1

Introduction

classiﬁer is then added to f. The ﬁnal classiﬁer is obtained by taking the sign of f (x) =(cid:80)

Boosting algorithms produce highly accurate classiﬁers by combining several less accurate classiﬁers
and are amongst the most popular learning algorithms  obtaining state-of-the-art performance on
several benchmark machine learning tasks [KMF+17  CG16]. The most famous of these boosting
algorithm is arguably AdaBoost [FS97]. For binary classiﬁcation  AdaBoost takes a training set
S = (cid:104)(x1  y1)  . . .   (xm  ym)(cid:105) of m labeled samples as input  with xi ∈ X and labels yi ∈ {−1  1}.
It then produces a classiﬁer f in iterations: in the jth iteration  a base classiﬁer hj : X → {−1  1}
is trained on a reweighed version of S that emphasizes data points that f struggles with and this
j αjhj(x) 
where the αj’s are non-negative coefﬁcients carefully chosen by AdaBoost. The base classiﬁers hj all
come from a hypothesis set H  e.g. H could be a set of small decision trees or similar. As AdaBoost’s
training progresses  more and more base classiﬁers are added to f  which in turn causes the training
error of f to decrease. If H is rich enough  AdaBoost will eventually classify all the data points in
the training set correctly [FS97].
Early experiments with AdaBoost report a surprising generalization phenomenon [SFBL98]. Even
after perfectly classifying the entire training set  further iterations keeps improving the test accuracy.
This is contrary to what one would expect  as f gets more complicated with more iterations  and thus
prone to overﬁtting. The most prominent explanation for this phenomena is margin theory  introduced
by Schapire et al. [SFBL98]. The margin of a training point (xi  yi) is a number in [−1  1]  which
can be interpreted  loosely speaking  as the classiﬁer’s conﬁdence on that point. Formally  we say that
j αjhj(x) is a voting classiﬁer if αj ≥ 0 for all j. Note that one can additionally assume

f (x) =(cid:80)

‡All authors contributed equally  and are presented in alphabetical order.
§Department of Computer Science  Aarhus University  {jallan lior.kamma larsen alexmath}@cs.au.dk
∗Department of EECS  UC Berkeley  minilek@berkeley.edu

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

without loss of generality that(cid:80)

j αj = 1 since normalizing each αi by(cid:80)

j αj leaves the sign of
f (xi) unchanged. The margin of a point (xi  yi) with respect to a voting classiﬁer f is then deﬁned
as

margin(xi) := yif (xi) = yi

αjhj(xi) .

(cid:88)

j

Thus margin(xi) ∈ [−1  1]  and if margin(xi) > 0  then taking the sign of f (xi) correctly classiﬁes
(xi  yi). Informally speaking  margin theory guarantees that voting classiﬁers with large (positive)
margins have a smaller generalization error. Experimentally AdaBoost has been found to continue
to improve the margins even when training past the point of perfectly classifying the training set.
Margin theory may therefore explain the surprising generalization phenomena of AdaBoost. Indeed 
the original paper by Schapire et al. [SFBL98] that introduced margin theory  proved the following
margin-based generalization bound. Let D be an unknown distribution over X ×{−1  1} and assume
that the training data S is obtained by drawing m i.i.d. samples from D. Then with high probability
over S it holds that for every margin θ ∈ (0  1]  every voting classiﬁer f satisﬁes
ln|H| ln m

(cid:32)(cid:114)

(cid:33)

(x y)∼D[yf (x) ≤ 0] ≤ Pr
(x y)∼S

Pr

[yf (x) < θ] + O

θ2m

.

(1)

The left-hand side of the equation is the out-of-sample error of f (since sign(f (x)) (cid:54)= y precisely
when yf (x) < 0). On the right-hand side  we use (x  y) ∼ S to denote a uniform random point from
S. Hence Pr(x y)∼S[yf (x) < θ] is the fraction of training points with margin less than θ. The last
term is increasing in |H| and decreasing in θ and m. Here it is assumed H is ﬁnite. A similar bound
can be proved for inﬁnite H by replacing |H| by d lg m  where d is the VC-dimension of H. This
holds for all the generalization bounds below as well. The generalization bound thus shows that f
has low out-of-sample error if it attains large margins on most training points. This ﬁts well with the
observed behaviour of AdaBoost in practice.
The generalization bound above holds for every voting classiﬁer f  i.e. regardless of how f was
obtained. Hence a natural goal is to design boosting algorithms that produce voting classiﬁers with
large margins on many points. This has been the focus of a long line of research and has resulted
in numerous algorithms with various margin guarantees  see e.g. [GS98  Bre99  BDST00  RW02 
RW05  GLM19]. One of the most well-known of these is Breimann’s ArcGV [Bre99]. ArcGV
produces a voting classiﬁer maximizing the minimal margin  i.e. it produces a classiﬁer f for which
min(x y)∈S yf (x) is as large as possible. Breimann complemented the algorithm with a generalization
bound stating that with high probability over the sample S  it holds that every voting classiﬁer f
satisﬁes:

(x y)∼D[yf (x) ≤ 0] ≤ O

Pr

 

(2)

(cid:18) ln|H| ln m

(cid:19)

ˆθ2m

where ˆθ = min(x y)∈S yf (x) is the minimal margin over all training examples. Notice that if one
chooses θ as the minimal margin in the generalization bound (1) of Schapire et al. [SFBL98]  then
the term Pr(x y)∼S[yf (x) < θ] becomes 0 and one obtains the bound
ln|H| ln m

(x y)∼D[yf (x) ≤ 0] ≤ O

Pr

ˆθ2m

(cid:115)

  

which is weaker than Breimann’s bound and motivated his focus on maximizing the minimal margin.
Minimal margin is however quite sensitive to outliers and work by Gao and Zhou [GZ13] proved a
generalization bound which provides an interpolation between (1) and (2). Their bound is known
as the kth margin bound  and states that with high probability over the sample S  it holds for every
margin θ ∈ (0  1] and every voting classiﬁer f that:
(x y)∼D[yf (x) < 0] ≤ Pr
(x y)∼S
The kth margin bound remains the strongest margin-based generalization bound to date (see Sec-
tion 1.2 for further details). The kth margin bound recovers Breimann’s minimal margin bound by
choosing θ as the minimal margin (making Pr(x y)∼S[yf (x) < θ] = 0)  and it is always at most the

ln|H| ln m

ln|H| ln m

[yf (x) < θ]+O

[yf (x) < θ]

(cid:115)

(x y)∼S

(cid:32)

(cid:33)

θ2m

+

Pr

θ2m

Pr

.

2

same as the bound (1) by Schapire et al. As with previous generalization bounds  it suggests that
boosting algorithms should focus on obtaining a large margin on as large a fraction of training points
as possible.
Despite the decades of progress on generalization upper bounds  we still do not know how tight these
bounds are. That is  we do not have any margin-based generalization lower bounds. Generalization
lower bounds are not only interesting from a theoretical point of view  but also from an algorithmic
point of view: If one has a provably tight generalization bound  then a natural goal is to design
a boosting algorithm minimizing a loss function that is equal to this generalization bound. This
approach makes most sense with a matching lower bound as the algorithm might otherwise minimize
a sub-optimal loss function. Furthermore  a lower bound may also inspire researchers to look for other
parameters than margins when explaining the generalization performance of voting classiﬁers. Such
new parameters may even prove useful in designing new algorithms  with even better generalization
performance in practice.

1.1 Our Results

In this paper we prove the ﬁrst margin-based generalization lower bounds for voting classiﬁers. Our
lower bounds almost match the kth margin bound and thus essentially settles the generalization
performance of voting classiﬁers in terms of margins.
f : X → [−1  1] that can be written as f (x) = (cid:80)
To present our main theorems  we ﬁrst introduce some notation. For a ground set X and hypothesis
(cid:80)
set H  let C(H) denote the family of all voting classiﬁers over H  i.e. C(H) contains all functions
h∈H αhh(x) such that αh ≥ 0 for all h and
h αh = 1. For a (randomized) learning algorithm A and a sample S of m points  let fA S denote
the (possibly random) voting classiﬁer produced by A when given the sample S as input. With this
notation  our ﬁrst main theorem is the following:
Theorem 1. For every large enough integer N  every θ ∈ (1/N  1/40) and every τ ∈ [0  49/100]
there exist a set X and a hypothesis set H over X   such that ln|H| = Θ(ln N ) and for every
over X × {−1  1} and a voting classiﬁer f ∈ C(H) such that with probability at least 1/100 over
the choice of samples S ∼ Dm and the random choices of A

m = Ω(cid:0)θ−2 ln|H|(cid:1) and for every (randomized) learning algorithm A  there exist a distribution D

1.

2.

[yf (x) < θ] ≤ τ; and

Pr

(x y)∼S
(x y)∼D[yfA S(x) < 0] ≥ τ + Ω

Pr

(cid:18)

(cid:113)

τ · ln |H|

mθ2

(cid:19)

.

ln |H|
mθ2 +

Theorem 1 states that for any algorithm A  there is a distribution D for which the out-of-sample error
of the voting classiﬁer produced by A is at least that in the second point of the theorem. At the same
time  one can ﬁnd a voting classiﬁer f obtaining a margin of at least θ on at least a 1 − τ fraction
of the sample points. Our proof of Theorem 1 not only shows that such a classiﬁer exists  but also
provides an algorithm that constructs such a classiﬁer. Loosely speaking  the ﬁrst part of the theorem
reﬂects on the nature of the distribution D and the hypothesis set H. Intuitively it means that the
distribution is not too hard and the hypothesis set is rich enough  so that it is possible to construct a
voting classiﬁer with good empirical margins. Clearly  we cannot hope to prove that the algorithm
A constructs a voting classiﬁer that has a margin of at least θ on a 1 − τ fraction of the sample set 
since we make no assumptions on the algorithm. For example  if the constant hypothesis h1 that
always outputs 1 is in H  then A could be the algorithm that simply outputs h1. The interpretation is
thus: D and H allow for an algorithm A to produce a voting classiﬁer f with margin at least θ on a
1 − τ fraction of samples. The second part of the theorem thus guarantees that regardless of which
voting classiﬁer A produces  it still has large out-of-sample error. This implies that every algorithm
that constructs a voting classiﬁer by minimizing the empirical risk  must have a large error. Formally 
Theorem 1 implies that if Pr(x y)∼S[yfA S(x) > θ] ≤ τ then

(x y)∼D[yfA S(x) < 0] ≥ Pr
(x y)∼S

Pr

[yfA S(x) > θ] + Ω

ln|H|
mθ2 +

τ · ln|H|

mθ2

(cid:32)

(cid:114)

(cid:33)

.

The ﬁrst part of the theorem ensures that the condition is not void. That is  there exists an algorithm
A for which Pr(x y)∼S[yfA S(x) < θ] ≤ τ. Comparing Theorem 1 to the kth margin bound  we

3

√

see that the parameter τ corresponds to Pr(x y)∼S[yf (x) < θ]. The magnitude of the out-of-sample
error in the second point in the theorem thus matches that of the kth margin bound  except for a
factor ln m in the ﬁrst term inside the Ω(·) and a
ln m factor in the second term. If we consider
the range of parameters θ  τ  ln|H| and m for which the lower bound applies  then these ranges are
almost as tight as possible. For τ  note that the theorem cannot generally be true for τ > 1/2  as the
algorithm A that outputs a uniform random choice of hypothesis among h1 and h−1 (the constant
hypothesis outputting −1)  gives a (random) voting classiﬁer fA S with an expected out-of-sample
error of 1/2. This is less than the second point of the theorem would state if it was true for τ > 1/2.
For ln|H|  observe that our theorem holds for arbitrarily large values of |H|. That is  the integer N
can be as large as desired  making ln|H| = Θ(ln N ) as large as desired. Finally  for the constraint
on m  notice again that the theorem simply cannot be true for smaller values of m as then the term
ln|H|/(mθ2) exceeds 1.
Our second main result gets even closer to the kth margin bound:
Theorem 2. For every large enough integer N  every θ ∈ (1/N  1/40)  τ ∈ [0  49/100] and every

m =(cid:0)θ−2 ln N(cid:1)1+Ω(1)  there exist a set X   a hypothesis set H over X and a distribution D over

X × {−1  1} such that ln|H| = Θ(ln N ) and with probability at least 1/100 over the choice of
samples S ∼ Dm there exists a voting classiﬁer fS ∈ C(H) such that

1.

2.

Pr

[yfS(x) < θ] ≤ τ; and

(x y)∼S
(x y)∼D[yfS(x) < 0] ≥ τ + Ω

Pr

(cid:18)

(cid:113)

τ · ln |H|

mθ2

(cid:19)

.

ln |H| ln m

mθ2 +

√

Observe that the second point of Theorem 2 has an additional ln m factor on the ﬁrst term in Ω(·)
compared to Theorem 1. It is thus only off from the kth margin bound by a
ln m factor in the
second term and hence completely matches the kth margin bound for small values of τ. To obtain
this strengthening  we replaced the guarantee in Theorem 1 saying that all algorithms A have such a
large out-of-sample error. Instead  Theorem 2 demonstrates only the existence of a voting classiﬁer
fS (that is chosen as a function of the sample S) that simultaneously achieves a margin of at least θ
on a 1 − τ fraction of the sample points  and yet has out-of-sample error at least that in point 2. Since
the kth margin bound holds with high probability for all voting classiﬁers  Theorem 2 rules out any
strengthening of the kth margin bound  except for possibly a
ln m factor on the second additive
term. Again  our lower bound holds for almost the full range of parameters of interest. As for the

bound on m  our proof assumes m =(cid:0)θ−2 ln N(cid:1)1+1/8  however the theorem holds for any constant

greater than 1 in the exponent.
Finally  we mention that both our lower bounds are proved for a ﬁnite hypothesis set H. This only
makes the lower bounds stronger than if we proved it for an inﬁnite H with bounded VC-dimension 
since the VC-dimension of a ﬁnite H  is no more than lg |H|.

√

1.2 Related Work

We mentioned above that the kth margin bound is the strongest margin-based generalization bound
to date. Technically speaking  it is incomparable to the so-called emargin bound by Wang et al.
[WSJ+11]. The kth margin bound by Gao and Zhou [GZ13]  the minimum margin bound by
Breimann [Bre99] and the bound by Schapire et al. [SFBL98] all have the form Pr(x y)∼D[yf (x) <
0] ≤ Pr(x y)∼S[yf (x) < θ]+Γ(θ  m |H|  Pr(x y)∼S[yf (x) < θ]) for some function Γ. The emargin
bound has a different (and quite involved) form  making it harder to interpret and compute. We
will not discuss it in further detail here and just remark that our results show that for generalization
bounds of the form studied in most previous work [SFBL98  Bre99  GZ13]  one cannot hope for
much stronger upper bounds than the kth margin bound.

2 Proof Overview

The main argument that lies in the heart of both proofs is a probabilistic method argument. With every
labeling (cid:96) ∈ {−1  1}u we associate a distribution D(cid:96) over X × {−1  1}. We then show that with
some positive probability if we sample (cid:96) ∈ {−1  1}u  D(cid:96) satisﬁes the requirements of Theorem 1

4

(cid:16) ln |H|

(cid:17)

θ2m

10m  then the expected generalization error (over the choice of (cid:96)) is still Ω(cid:0) 1

10m|X|(cid:1).

(respectively Theorem 2). We thus conclude the existence of a suitable distribution. We next give a
more detailed high-level description of the proof for Theorem 1. The proof of Theorem 2 follows
similar lines.
Constructing a Family of Distributions. We start by ﬁrst describing the construction of D(cid:96) for
(cid:96) ∈ {−1  1}u. Our construction combines previously studied distribution patterns in a subtle manner.
Ehrenfeucht et al. [EHKV89] observed that if a distribution D assigns each point in X a ﬁxed (yet
unknown) label  then  loosely speaking  every classiﬁer f  that is constructed using only information
supplied by a sample S  cannot do better than random guessing the labels for the points in X \ S.
Intuitively  consider a uniform distribution D(cid:96) over X . If we assume  for example  that |X| ≥ 10m 
then with very high probability over a sample S of m points  many elements of X are not in S.
Moreover  assume that D(cid:96) associates every x ∈ X with a unique “correct” label (cid:96)(x). Consider some
(perhaps random) learning algorithm A  and let fA S be the classiﬁer it produces given a sample S as
input. If (cid:96) is chosen randomly  then  loosely speaking  for every point x not in the sample  fA S(x)
and (cid:96)(x) are independent  and thus A returns the wrong label with probability 1/2. In turn  this
implies that there exists a labeling (cid:96) such that A is wrong on a constant fraction of X when receiving
a sample S ∼ Dm
(cid:96) . While the argument above can in fact be used to prove an arbitrarily large
generalization error  it requires |X| to be large  and speciﬁcally to increase with m. This conﬂicts
with the ﬁrst point in Theorem 1  that is  we have to argue that a voting classiﬁer f with good margins
exist for the sample S. If S consists of m distinct points  and each point in X can have an arbitrary
label  then intuitively H needs to be very large to ensure the existence of f. In order to overcome
this difﬁculty  we set D(cid:96) to assign very high probability to one designated point in X   and the rest of
the probability mass is then equally distributed between all other points. The argument above still
applies for the subset of small-probability points. More precisely  if D(cid:96) assigns all but one point in
X probability 1
It remains to determine how large can we set |X|. In the notations of the theorem  in order for a
hypothesis set H to satisfy ln|H| = Θ(ln N )  and at the same time  have an f ∈ C(H) obtaining
margins of θ on most points in a sample  our proof (and speciﬁcally Lemma 3  described hereafter)
requires X to be not signiﬁcantly larger than ln N
θ2   and therefore the generalization error we get is
. This accounts for the ﬁrst term inside the Ω-notation in the second point of Theorem 1.
Ω
Anthony and Bartlett [AB09  Chapter 5] additionally observed that for a distribution D that assigns
each point in X a random label  if S does not sample a point x enough times  any classiﬁer f  that is
constructed using only information supplied by S  cannot determine with good probability the Bayes
label of x  that is  the label of x that minimizes the error probability. Intuitively  consider once more
a distribution D(cid:96) that is uniform over X . However  instead of associating every point x ∈ X with
one correct label (cid:96)(x)  D(cid:96) is now only slightly biased towards (cid:96). That is  given that x is sampled  the
label in the sample point is (cid:96)(x) with probability that is a little larger than 1/2  say (1 + α)/2 for
some small α ∈ (0  1). Note that every classiﬁer f has an error probability of at least (1 − α)/2 on
every given point in X . Consider once again a learning algorithm A and the voting classiﬁer fA S it
constructs. Loosely speaking  if S does not sample a point x enough times  then with good probability
fA S(x) (cid:54)= (cid:96)(x). More formally  in order to correctly assign the Bayes label of x  an algorithm
probability the algorithm does not see a constant fraction of X enough times to correctly assign their
expectation is over the choice of (cid:96). By once again letting |X| = ln N
θ2 we conclude that there exists a
labeling (cid:96) such that for S ∼ Dm
.
This expression is almost the second term inside the Ω-notation in the theorem statement  though
slightly larger. We note  however  for large values of m  the in-sample error is arbitrarily close to
1/2. One challenge is therefore to reduce the in-sample-error  and moreover guarantee that we can
ﬁnd a voting classiﬁer f where the (mτ )’th smallest margin for f is at least θ  where τ  θ are the
parameters provided by the theorem statement.
To this end  our proof subtly weaves the two ideas described above and constructs a family of
distributions {D(cid:96)}(cid:96)∈{−1 1}u. Informally  we partition X into two disjoint sets  and conditioned on
the sample point x ∈ X belonging to each of the subsets  D(cid:96) is deﬁned similarly to be one of the two
distribution patterns deﬁned above. The main difﬁculty lies in delicately balancing all ingredients and

must see Ω(α−2) samples of x. Therefore if we set the bias α to be(cid:112)|X|/(10m)  then with high
label. In turn  this implies an expected generalization error of (1 − α)/2 + Ω((cid:112)|X|/m)  where the
(cid:19)
(cid:18)(cid:113) ln |H|

(cid:96)   the expected generalization error of fA S is 1−α

2 + Ω

θ2m

5

ensuring that we can ﬁnd an f with margins of at least θ on all but τ m of the sample points  while
still enforcing a large generalization error. Our proof reﬁnes the proof given by Ehrenfeucht et al.
and Anthony and Bartlett and shows that not only does there exists a labeling (cid:96) such that fA S has
large generalization error with respect to D(cid:96) (with probability at least 1/100 over the randomness of
A  S)  but rather that a large (constant) fraction of labelings (cid:96) share this property. This distinction
becomes crucial in the proof.

Small yet Rich Hypothesis Sets. The technical crux in our proofs is the construction of an ap-
propriate hypothesis set. Loosely speaking  the size of H has to be small  and most importantly 
independent of the size m of the sample set. On the other hand  the set of voting classiﬁers C(H)
is required to be rich enough to  intuitively  contain a classiﬁer that with good probability has good
in-sample margins for a sample S ∼ Dm
(cid:96) with a large fraction of labelings (cid:96) ∈ {−1  1}u. Our main
technical lemma presents a distribution µ over small hypothesis sets H ⊂ X → {−1  1} such that
for every sparse (cid:96) ∈ {−1  1}u  that is (cid:96)i = −1 for a small number of entries i ∈ [u]  with high
probability over H ∼ µ  there exists some voting classiﬁer f ∈ C(H) that has minimum margin θ
with (cid:96) over the entire set X . In fact  the size of the hypothesis set does not depend on the size of X  
but only on the sparsity parameter d. More formally  we show the following.
Lemma 3. For every θ ∈ (0  1/40)  δ ∈ (0  1) and integers d ≤ u  there exists a distribution
µ = µ(u  d  θ  δ) over hypothesis sets H ⊂ X → {−1  1}  where X is a set of size u  such that the
following holds for N = Θ

θ−2 ln d ln(θ−2dδ−1)eΘ(θ2d)(cid:17)

(cid:16)

.

1. For all H ∈ supp(µ)  we have |H| = N; and
2. For every labeling (cid:96) ∈ {−1  +1}u  if no more than d points x ∈ X satisfy (cid:96)(x) = −1  then

[∃f ∈ C(H) : ∀x ∈ X . (cid:96)(x)f (x) ≥ θ] ≥ 1 − δ  

PrH∼µ

In fact  we prove that if H is a random hypothesis set that also contains the hypothesis mapping all
points to 1  then with good probability H satisﬁes the second requirement in the theorem.
To show the existence of a good voting classiﬁer in C(H) our proof actually employs a slight variant
of the celebrated AdaBoost algorithm  and shows that with high probability (over the choice of the
random hypothesis set H)  the voting classiﬁer constructed by this algorithm attains minimum margin
at least θ over the entire set X .

Existential Lower Bound. The difference between the generalization lower bound (second point)
in Theorem 1 and 2 is a ln m factor in the ﬁrst term inside the Ω(·) notation. This term originated
from having ln|H|/θ2 points with a probability mass of 1/10m in D(cid:96) and one point having the
remaining probability mass. In the proof of Theorem 2  we ﬁrst exploit that we are proving an
existential lower bound by assigning all points the same label 1. Since we are not proving a lower
bound for every algorithm  this will not cause problems. We then change |X| to about m/ ln m and
assign each point the same probability mass ln m/m in the distribution D. The key observation is
that on a random sample S of m points  by a coupon-collector argument  there will still be mΩ(1)
points from X that were not sampled. From Lemma 3  we can now ﬁnd a voting classiﬁer f  such
that sign(f (x)) is 1 on all points in x ∈ S  and −1 on a set of d = ln|H|/θ2 points in X \ S. This
means that f has out-of-sample error Ω(d ln m/m) = Ω( ln |H| ln m
θ2m ) under distribution D and obtains
a margin of θ on all points in the sample S.

3 Proof of Algorithmic Lower Bound

In this section we prove Theorem 1 assuming Lemma 3. The proof of Lemma 3  as well as the proof
of Theorem 2 are deferred to the full version of the paper [GKL+19]. To prove Theorem 1  ﬁx some
integer N  and ﬁx θ ∈ (1/N  1/40). In order to ensure that the hypothesis set constructed using
Lemma 3 is small enough  and speciﬁcally has size N O(1)  we need the sparsity parameter to be
θ2 . As described in Section 2  the family of distributions we present will be
not much larger than ln N
deﬁned separately over two subsets of X . To this end  and for ease of notations  we let the size u
of X be 2 ln N
2 be the size of each half. Finally  denote X = {ξ1  . . .   ξu}.

  and let d = ln N

θ2 = u

θ2

6

We start by constructing the family {D(cid:96)}(cid:96)∈{−1 1}u of distributions over X × {−1  1}. Fixing a
labeling (cid:96) ∈ {−1  1}u  we deﬁne D(cid:96) separately for the ﬁrst u/2 points and the last u/2 points of
X . Intuitively  every point in {ξi}i∈[u/2] has a ﬁxed label determined by (cid:96)  however all points but
one have a very small probability of being sampled according to D(cid:96). Every point in {ξi}i∈[u/2+1 u] 
on the other hand  has an equal probability of being sampled  however its label is not ﬁxed by (cid:96) 
but instead slightly biased towards (cid:96). Formally  let α  β  ε ∈ [0  1] be constants to be ﬁxed later.
We construct D(cid:96) using the ideas described earlier in Section 2  by sewing them together over two
parts of the set X . We assign probability 1 − β to {ξi}i∈[u/2] and β to {ξi}i∈[u/2+1 u]. That is  for
(x  y) ∼ D(cid:96)  the probability that x ∈ {ξi}i∈[u/2] is 1 − β. Next  conditioned on x ∈ {ξi}i∈[u/2] 
(ξ1  (cid:96)1) is assigned high probability (1 − ε) and the rest of the measure is distributed uniformly over
{(ξi  (cid:96)i)}i∈[2 u/2]. That is

[(ξ1  (cid:96)1)] = (1 − β)(1 − ε)   and ∀j ∈ [2  u/2]. PrD(cid:96)
PrD(cid:96)

[(ξj  (cid:96)j)] =

Finally  conditioned on x ∈ {ξi}i∈[u/2+1 u]  x distributes uniformly over {ξi}i∈[u/2+1 u]  and
conditioned on x = ξi  we have y = (cid:96)i with probability 1+α

2 . That is

∀j ∈ [u/2 + 1  u]. PrD(cid:96)

[(ξj  (cid:96)j)] =

(1 + α)β

2d

[(ξj −(cid:96)j)] =

  and PrD(cid:96)

(1 − β)ε
u/2 − 1

.

(1 − α)β

.

2d

(cid:88)

Ψ1((cid:96)  f ) =

(1 − ε)β
u/2 − 1

(cid:88)

i∈[2 u/2]

In order to give a lower bound on the out-of-sample error for an arbitrary voting classiﬁer f  we
deﬁne a new random variable that is dominated by Pr(x y)∼D(cid:96)[yf (x) < 0]  and give a lower bound
on that random variable. To this end  ﬁx some (cid:96) ∈ {−1  1}u and f : X → R  and denote

1(cid:96)if (ξi)<0

; Ψ2((cid:96)  f ) =

αβ
d

1(cid:96)if (ξi)<0 .

(3)

i∈[u/2+1 u]

2 + Ψ1 + Ψ2.

When f  (cid:96) are clear from the context we shall simply denote Ψ1  Ψ2. In this notation  we show the
following.
Claim 4. Pr(x y)∼D(cid:96)[yf (x) < 0] ≥ β(1−α)
While the proof of the claim is deferred to the full version of the paper [GKL+19]  we explain why we
focus on Ψ1 + Ψ2  rather than bounding the out-of-sample error directly. The reason lies in the fact
that we need a lower bound to hold with constant probability over the choice of (cid:96) and S (and in the
case of Theorem 1 also the random choices made by the algorithm) and not only in expectation. While
lower bounding E[Pr(x y)∼D(cid:96)[yf (x) < 0]] is clearly not harder than lower bounding E[Ψ1 + Ψ2] 
showing that a lower bound holds with some constant probability is slightly more delicate. Our proof
uses the fact that with probability 1  Ψ1 + Ψ2 is not larger than a constant from its expectation  and
therefore we can use Markov’s inequality to lower bound Ψ1 + Ψ2 with constant probability.
We next show that there exists a small enough (with respect to N) hypothesis set ˆH that is rich
enough. That is  with high probability over (cid:96) ∈ {−1  1}u  there exists a weighted average f ∈ C( ˆH)
that attains margin at least θ over the entire set X . The following claim follows from Lemma 3 and
Yao’s minimax principle. Its proof is deferred to the full version of the paper [GKL+19].
Claim 5. There exists a hypothesis set ˆH such that ln| ˆH| = Θ (ln N ) and

Pr

(cid:96)∈R{−1 1}u

[∃f ∈ C( ˆH) : ∀i ∈ [u]. (cid:96)if (ξi) ≥ θ] ≥ 19/20 .

We next show that there exist some distribution D ∈ {D(cid:96)}(cid:96)∈{−1 1}u and some classiﬁer ˆf ∈ C( ˆH)
such that for every algorithm A  with constant probability over S ∼ D(cid:96)  ˆf has large margins on
points in S  yet fA S has large out-of-sample error. To this end ﬁx A to be a (perhaps randomized)
learning algorithm. For every m-point sample S  recall that fA S denotes the (random) classiﬁer
returned by A when running on sample S.
The main challenge is to show that there exists a labeling ˆ(cid:96) ∈ {−1  1}u such that C( ˆH) contains
a good voting classiﬁer for ˆ(cid:96) and  in addition  fA S has a out-of-sample error with respect to Dˆ(cid:96).
We will show that if α is small enough  then indeed such a labeling exists. Formally  we show the
following.

7

Lemma 6. If α ≤(cid:113) u

40βm   then there exists ˆ(cid:96) ∈ {−1  1}u such that
1. There exists ˆf = ˆfˆ(cid:96) ∈ C( ˆH) such that for every i ∈ [u]  ˆ(cid:96)i
2. with probability at least 1/25 over S ∼ Dm
ˆ(cid:96)

ˆf (ξi) ≥ θ ; and

and the randomness of A we have

Ψ1(ˆ(cid:96)  fA S) + Ψ2(ˆ(cid:96)  fA S) ≥ (1 − β)ε

+

αβ
24

.

24

The proof of the lemma is quite involved technically  and is therefore also deferred to the full version
of the paper [GKL+19]. We will next show that the lemma implies Theorem 1.
Proof of Theorem 1. Fix some τ ∈ [0  49/100]. Assume ﬁrst that τ ≤ u
300m   and let ε = u
10m and
β = α = 0. Let ˆ(cid:96)  ˆf be as in Lemma 6  then for every sample S ∼ Dm
  Pr(x y)∼S[y ˆf (x) < θ] =
ˆ(cid:96)
0 ≤ τ  and moreover with probability at least 1/25 over S and the randomness of A

(cid:16) u

(cid:17)

m

= τ + Ω

(cid:115)

 ln| ˆH|

mθ2 +

τ ln| ˆH|
mθ2

 .

[yfA S(x) < 0] ≥ (1 − β)ε

≥ τ + Ω

24

Pr

(x y)∼Dˆ(cid:96)

300m   and let ε = u

therefore α = (cid:112) u

2560τ m ≤ (cid:113) u

10m  α =(cid:112) u

where the ﬁrst inequality follows from Claim 4 and the second point of Lemma 6  and the last
transition is due to the fact that u = 2θ−2 ln N = Θ(θ−2 lg | ˆH|) and τ = O(u/m).
32−31α. Since τ ≥ u
Otherwise  assume τ > u
300m 
then α ∈ [0  1]. Moreover  if m > Cu for large enough but universal constant C > 0  then
100 ≥ 64τ  and hence β ∈ [0  1]. Moreover  since α ≤ 1 then β ≤ 64τ  and
32 − 31α ≥ 64 · 49
40βm. Let therefore ˆ(cid:96)  ˆf be a labeling and a classiﬁer in C( ˆH)
be a sample of m
yj ˆf (xj )<θ] = (1−α)β
.

whose existence is guaranteed in Lemma 6. Let (cid:104)(x1  y1)  . . .   (xm  ym)(cid:105) ∼ Dm
points drawn independently according to Dˆ(cid:96). For every j ∈ [m]  we have E[1
Therefore by Chernoff we get that for large enough N 

2560τ m and β = 64τ

ˆ(cid:96)

2

(cid:20)

(cid:104)

(cid:21)

(cid:105) ≥ τ

Pr

S∼Dm
ˆ(cid:96)

Pr

(x y)∼S

y ˆf (x) < θ

= Pr

S∼Dm
ˆ(cid:96)

 1

m

(cid:88)

j∈[m]

yj ˆf (xj )<θ ≥ (1 − 31α/32)β

2

1



≤ e−Θ(α2βm) ≤ e−Θ(u) ≤ 10−3  

2560τ = Ω(u)  since β ≥ 2τ.
where the second-to-last inequality is due to the fact that α2βm = uβ
Moreover  from Claim 4 and the second point of Lemma 6 we get that with probability at least 1/25
over S and A we have

[yfA S(x) < 0] ≥ (1 − α)β

Pr

(x y)∼Dˆ(cid:96)

(1 − 31α/32)β

2

=

(cid:115)

αβ
32

+

 ln| ˆH|

mθ2 +

2
τ ln| ˆH|
mθ2

  

≥ τ + Ω

+

αβ
64

= τ + Ω

(cid:18)(cid:114) τ u

(cid:19)

m

where the last transition is due to the fact that τ = Ω(u/m).

4 Existence of a Small Hypotheses Set

This section is devoted to the proof of Lemma 3. That is  we present a distribution µ over ﬁxed-size
hypothesis sets and show that for every ﬁxed labeling (cid:96) with not too many negative labels  with high
probability over H ∼ µ  C(H) contains a voting classiﬁer f that attains good margins with respect to
(cid:96). In fact  our proof not only shows existence of such a voting classiﬁer  but also presents a procedure
for constructing one. The presented algorithm is an adaptation of the AdaBoost algorithm.

8

δ

(cid:17)

More formally  ﬁx some θ ∈ (0  1/40)  δ ∈ (0  1) and an integer d ≤ u. Let γ = 4θ ∈ (0  1/10) and
let N = 2γ−2 ln d· ln γ−2 ln d
· eO(θ2d). We deﬁne the distribution µ via the following procedure  that
samples a hypothesis set H ∼ µ. Let ˆh be deﬁned by ˆh(x) = 1 for all x ∈ X . Sample independently
and uniformly at random N hypotheses h1  . . .   hN   and deﬁne H := {ˆh} ∪ {hj}j∈[N ].
Clearly every H ∈ supp(µ) satisﬁes |H| = N + 1. We therefore turn to prove the second property.
To this end  let k = γ−2 ln d. In order to show existence of a voting classiﬁer  we conceptually change
the procedure deﬁning µ  and think of the random hypotheses as being sampled in k equally sized
“batches”  each of size N/k  and adding ˆh to each of them. Denote the batches by H1 H2  . . .  Hk.
We consider next the following procedure to construct a voting classiﬁer f ∈ C(H) given H ∼ µ. We
will use the main ideas from the AdaBoost algorithm. Recall that AdaBoost creates a voting classiﬁer
using a sample S = ((x1  y1)  . . .   (xu  yu)) in iterations. Staring with f0 = 0  in iteration j  it
computes a new voting classiﬁer fj = fj−1 + αjhj for some hypothesis hj ∈ H and weight αj. The
heart of the algorithm lies in choosing hj. In each iteration  AdaBoost computes a distribution Dj over
S and chooses a hypothesis hj minimizing the empirical error probability εj = Pri∼Dj [hj(xi) (cid:54)= yi]
with respect to Dj and then reweighs the sample points to construct Dj+1. The weight it then assigns
to hj is αj = (1/2) ln((1 − εj)/εj) The ﬁrst distribution D1 is the uniform distribution.
We alter the above slightly assigning uniform weights on the hypotheses  and setting αj = 1
for all iterations j. The algorithm is formally described as Algorithm 1.
Input: (H1  . . .  Hk) ∼ µ
Output: f ∈ C
j∈[k] Hj
1: let α = 1
2: let f (x) = 0 for all x ∈ X
3: let D1(i) = 1
4: for j = 1 to k do
5:

2 ln 1+2γ
1−2γ
u for all i ∈ [u].

i∈[u] Dj(i)1yi(cid:54)=hj (xi) ≤ 1

(cid:16)(cid:83)

2 ln 1+2γ
1−2γ

2 − γ.

Algorithm 1: Construct a Voting Classiﬁer

(cid:80)
j∈[k] hj ∈ C(H)
First note that if f is the classiﬁer returned by the algorithm  then clearly f = 1
k
is a voting classiﬁer. The following claim implies Lemma 3. Its proof is quite technical  and deferred
to the full version of the paper [GKL+19].
Claim 7. With probability at least 1 − δ Algorithm 1 does not fail  and moreover  in that case  for
every i ∈ [y]  yif (xu) ≥ θ.

5 Conclusions

In this work  we showed almost tight margin-based generalization lower bounds for voting classiﬁers.
These new bounds essentially complete the theory of generalization for voting classifers based on
margins alone. Closing the remaining gap between the upper and lower bounds is an intriguing open
problem and we hope our techniques might inspire further improvements. Our results come in the
form of two theorems  one showing generalization lower bounds for any algorithm producing a voting
classiﬁer  and a slightly stronger lower bound showing the existence of a voting classiﬁer with poor
generalization. This raises the important question of whether speciﬁc boosting algorithms can produce
voting classiﬁers that avoid the lg m factor in the second lower bound via a careful analysis tailored
to the algorithm. As a ﬁnal important direction for future work  we suggest investigating whether
natural parameters other than margins may be used to better explain the practical generalization error
of voting classiﬁers. At least  we now have an almost tight understanding  if no further parameters
are taken into consideration.

9

Find a hypothesis hj ∈ Hj satisfying(cid:80)
Zj ←(cid:80)

If there is no such hypothesis  return fail.
fj ← fj−1 + hj.
for every i ∈ [u] let Dj+1(i) = 1

i∈[u] Dj(i) exp(−αyihj(xi)).

Zj

6:
7:
8:
9: return 1

k fk.

Dj(i) exp(−αyihj(xi)).

Acknowledgments

This work was supported by a Villum Young Investigator Grant and an AUFF Starting Grant.
Jelani Nelson is supported by NSF CAREER award CCF-1350670  NSF grant IIS-1447471  ONR
grant N00014-18-1-2562  ONR DORECG award N00014-17-1-2127  an Alfred P. Sloan Research
Fellowship  and a Google Faculty Research Award

References
[AB09]

M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations.
Cambridge University Press  New York  NY  USA  1st edition  2009.

[BDST00] K. P. Bennett  A. Demiriz  and J. Shawe-Taylor. A column generation algorithm for

[Bre99]

[CG16]

boosting. In ICML  pages 65–72  2000.
L. Breiman. Prediction games and arcing algorithms. Neural computation  11(7):1493–
1517  1999.
T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of
the 22nd acm sigkdd international conference on knowledge discovery and data mining 
pages 785–794. ACM  2016.

[EHKV89] A. Ehrenfeucht  D. Haussler  M. Kearns  and L. Valiant. A general lower bound on the
number of examples needed for learning. Information and Computation  82(3):247 –
261  1989.
Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of computer and system sciences  55(1):119–139 
1997.

[FS97]

[GKL+19] A. Grønlund  L. Kamma  K. G. Larsen  A. Mathiasen  and J. Nelson. Margin-based

generalization lower bounds for boosted classiﬁers  2019. arXiv:1909.12518.

[GLM19] A. Grønlund  K. G. Larsen  and A. Mathiasen. Optimal minimal margin maximization
with boosting. In Proceedings of the 36th International Conference on Machine Learning 
pages 4392–4401. PMLR  2019.
A. J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned
ensembles. In AAAI/IAAI  pages 692–699  1998.
W. Gao and Z.-H. Zhou. On the doubt about margin explanation of boosting. Artiﬁcial
Intelligence  203:1–18  2013.

[GZ13]

[GS98]

[RW02]

[KMF+17] G. Ke  Q. Meng  T. Finley  T. Wang  W. Chen  W. Ma  Q. Ye  and T.-Y. Liu. Lightgbm:
A highly efﬁcient gradient boosting decision tree. In Advances in Neural Information
Processing Systems  pages 3146–3154  2017.
G. Rätsch and M. K. Warmuth. Maximizing the margin with boosting. In COLT  volume
2375  pages 334–350. Springer  2002.
G. Rätsch and M. K. Warmuth. Efﬁcient margin maximizing with boosting. Journal of
Machine Learning Research  6(Dec):2131–2152  2005.

[RW05]

[SFBL98] R. E. Schapire  Y. Freund  P. Bartlett  and W. S. Lee. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics  26(5):1651–
1686  1998.

[WSJ+11] L. Wang  M. Sugiyama  Z. Jing  C. Yang  Z.-H. Zhou  and J. Feng. A reﬁned margin
analysis for boosting algorithms via equilibrium margin. Journal of Machine Learning
Research  12(Jun):1835–1863  2011.

10

,Elahe Ghalebi
Baharan Mirzasoleiman
Radu Grosu
Jure Leskovec
Allan Grønlund
Lior Kamma
Kasper Green Larsen
Alexander Mathiasen
Jelani Nelson