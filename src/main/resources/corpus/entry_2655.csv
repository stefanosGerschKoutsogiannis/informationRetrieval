2019,Information-Theoretic Confidence Bounds for Reinforcement Learning,We integrate information-theoretic concepts into the design and analysis of optimistic algorithms and Thompson sampling. By making a connection between information-theoretic quantities and confidence bounds  we obtain results that relate the per-period performance of the agent with its information gain about the environment  thus explicitly characterizing the exploration-exploitation tradeoff. The resulting cumulative regret bound depends on the agent's uncertainty over the environment and quantifies the value of prior information. We show applicability of this approach to several environments  including linear bandits  tabular MDPs  and factored MDPs. These examples demonstrate the potential of a general information-theoretic approach for the design and analysis of reinforcement learning algorithms.,Information-Theoretic Conﬁdence Bounds for

Reinforcement Learning

Xiuyuan Lu

Stanford University
lxy@stanford.edu

Benjamin Van Roy
Stanford University
bvr@stanford.edu

Abstract

We integrate information-theoretic concepts into the design and analysis of op-
timistic algorithms and Thompson sampling. By making a connection between
information-theoretic quantities and conﬁdence bounds  we obtain results that
relate the per-period performance of the agent with its information gain about the
environment  thus explicitly characterizing the exploration-exploitation tradeoff.
The resulting cumulative regret bound depends on the agent’s uncertainty over
the environment and quantiﬁes the value of prior information. We show applica-
bility of this approach to several environments  including linear bandits  tabular
MDPs  and factored MDPs. These examples demonstrate the potential of a gen-
eral information-theoretic approach for the design and analysis of reinforcement
learning algorithms.

1

Introduction

We consider an online decision problem where an agent repeatedly interacts with an uncertain
environment and observes outcomes. The agent has a reward function that speciﬁes its preferences
over outcomes. The objective of the agent is to sequentially select actions so as to maximize the
long-term expected cumulative reward. One classic example is the multi-armed bandit problem 
where the agent observes only the reward of the action selected during each period. Another example
is episodic reinforcement learning  where the agent selects a policy at the beginning of each episode 
and observes a trajectory of states and rewards realized over the episode.
The agent’s uncertainty about the environment gives rise to a need to trade off between exploration
and exploitation. Exploring parts of the environment that are poorly understood could lead to better
performance in the future  while exploiting current knowledge of the environment could lead to
higher reward in the short term. Thompson sampling [18] and optimistic algorithms [9  12] are
two classes of algorithms that effectively balance the exploration-exploitation tradeoff and achieve
near-optimal performance in many stylized online decision problems. However  most analyses of such
algorithms focus on establishing performance guarantees that only exploit the parametric structure of
the model [1  2  3  5  7  9  10  11]. There has not been much focus on how prior information as well
as information gain during the learning process affect performance  with few exceptions [15  16].
In our work  we leverage concepts from information theory to quantify the information gain of
the agent and address the exploration-exploitation tradeoff for Thompson sampling and optimistic
algorithms. By connecting information-theoretic quantities with conﬁdence bounds  we are able to
relate the agent’s per-period performance with its information gain about the environment during the
period. The relation explicitly characterizes how an algorithm balances exploration and exploitation
on a single-period basis. The information gain is represented succinctly using mutual information 
which abstracts away from the speciﬁc parametric form of the model. The level of abstraction
offered by information theory shows promise of these information-theoretic conﬁdence bounds
being generalizable to a broad class of problems. Moreover  the resulting cumulative regret bound

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

explicitly depends on the agent’s uncertainty over the environment  which naturally exhibits the value
of prior information. We present applications of information-theoretic conﬁdence bounds on three
environments  linear bandits  tabular MDPs  and factored MDPs.
One paper that is closely related to our work is [16]  which proposes an upper conﬁdence bound
(UCB) algorithm for bandit learning with a Gaussian process prior and derives a regret bound that
depends on maximal information gain. Some of their results parallel what we establish in the context
of the linear bandit  though our analysis extends to Thompson sampling as well. More importantly  our
work generalizes the information-theoretic conﬁdence bound approach to problems with signiﬁcantly
more complicated information structure  such as MDPs.
Another closely related paper is [15]  which provides an information-theoretic analysis of Thompson
sampling for bandit problems with partial feedback. The paper introduces the notion of an information
ratio  which relates the one-period regret of Thompson sampling with one-period information gain
towards the optimal action. Using this concept  the authors are able to derive a series of regret bounds
that depend on the information entropy of the optimal action. While this is an elegant result  it is
unclear how to extend the approach to MDPs  as information gain about the optimal policy is hard to
quantify. In our paper  we consider information gain about the underlying environment rather than
the optimal action or policy  which may be seen as a relaxation of their method. The relaxation allows
us to leverage conﬁdence bounds and obtain information-theoretic regret bounds for MDPs.

2 Problem formulation

We consider an online decision problem where an agent repeatedly interacts with an uncertain environ-
ment and observes outcomes. All random variables are deﬁned on a probability space (Ω F  P). The
environment is described by an unknown model parameter θ which governs the outcome distribution.
The agent’s uncertainty over the environment is represented as a prior distribution over θ. Thus  θ
will be treated as a random variable in the agent’s mind. During each time period (cid:96)  the agent selects
an action A(cid:96) ∈ A and observes an outcome Y(cid:96) A(cid:96) ∈ Y. We assume that the space of outcomes Y is a
subset of a ﬁnite dimensional Euclidean space. Conditioned on the model index θ  outcomes Y(cid:96) are
i.i.d. for (cid:96) = 1  2  . . . . The agent has a reward function r : Y → (cid:60) that encodes its preferences over
outcomes. We make a simplifying assumption that rewards are bounded.
Assumption 1. There exists B ≥ 0 such that supy∈Y r(y) − inf y∈Y r(y) ≤ B.
The objective of the agent is to maximize its long-term expected cumulative reward. Let
H(cid:96) = (A1  Y1 A1  . . .   A(cid:96)−1  Y(cid:96)−1 A(cid:96)−1) denote the history up to time (cid:96). An algorithm π is a
sequence of functions {π(cid:96)}(cid:96)≥1 that map histories to distributions over actions. For any a ∈ A 
let rθ(a) = E[r(Y1 a)|θ] denote the expected reward of selecting action a under model θ. Let
A∗ ∈ arg maxa∈A rθ(a) denote the optimal action under model θ. We deﬁne the Bayesian regret of
an algorithm π over L periods

L(cid:88)

E[Regret(L  π)] =

E[rθ(A∗) − rθ(A(cid:96))] 

(cid:96)=1

where the expectation is taken over the randomness in outcomes  algorithm π  as well as the prior
distribution over θ.
Note that episodic reinforcement learning also falls in the above formulation by considering policies
as actions and trajectories as observations.

3 Preliminaries

3.1 Basic quantities in information theory

For two probability measures P and Q such that P is absolutely continuous with respect to Q  the
Kullback-Leibler divergence between them is
D(P||Q) =

(cid:18) dP

(cid:19)

(cid:90)

log

dP 

dQ

2

dQ is the Radon-Nikodym derivative of P with respect to Q.

where dP
Let P (X) ≡ P(X ∈ ·) denote the probability distribution of a random variable X  and let P (X|Y ) ≡
P(X ∈ ·|Y ) denote the conditional probability distribution of X conditioned on Y .
The mutual information between two random variables X and Y

I(X; Y ) = D(P (X  Y )|| P (X) P (Y ))

is the Kullback-Leibler divergence between their joint distribution and product distribution [6].
I(X; Y ) is always nonnegative  and I(X; Y ) = 0 if and only if X and Y are independent. In our
analysis  we will use I(θ; A  YA) to measure the agent’s information gain of θ from selecting an
action and observing an outcome.
The conditional mutual information between two random variables X and Y   conditioned on a third
random variable Z  is

I(X; Y |Z) = E[D(P (X  Y |Z)|| P (X|Z)P (Y |Z))] 

where the expectation is taken over Z. An elegant property of mutual information is that the mutual
information between a random variable X and a collection of random variables Y1  . . .   Yn can be
decomposed into a sum of conditional mutual information using the chain rule.
Lemma 1. (Chain rule of mutual information)

I(X; Y1  Y2  . . .   Yn) =

I(X; Yi|Y1  . . .   Yi−1).

n(cid:88)

i=1

3.2 Notation under posterior distributions
We will use subscript (cid:96) on P and E to indicate quantities conditioned on H(cid:96)  i.e.  P(cid:96)(·) ≡ P(·|H(cid:96)) =
P(·|A1  Y1 A1   . . .   A(cid:96)−1  Y(cid:96)−1 A(cid:96)−1)  and similarly for E(cid:96)[·]. Let P(cid:96)(X) ≡ P(cid:96)(X ∈ ·) denote the
conditional distribution of a random variable X conditioned on H(cid:96). We deﬁne ﬁltered mutual
information
which is a random variable of H(cid:96). Note that by the deﬁnition of conditional mutual information 

I(cid:96)(X; Y ) = D(P(cid:96)(X  Y )|| P(cid:96)(X)P(cid:96)(Y )) 

E[I(cid:96)(X; Y )] = I(X; Y |H(cid:96)) = I(X; Y |A1  Y1 A1   . . .   A(cid:96)−1  Y(cid:96)−1 A(cid:96)−1).

3.3 Algorithms

Thompson sampling is a simple yet effective heuristic for trading off between exploration and
exploitation. Conceptually  it samples each action according to the probability that it is optimal. The
algorithm typically operates by starting with a prior distribution over θ. During each time period 
it samples from the posterior distribution over θ  and selects an action that maximizes the expected
reward under the sampled model. It then updates the posterior distribution with the observed outcome.
Another widely studied class of algorithms that effectively trade off between exploration and exploita-
tion are upper conﬁdence bound (UCB) algorithms  which apply the principle of optimism in the face
of uncertainty. For each time period  they typically construct an upper conﬁdence bound for the mean
reward of each action based on past observations  and then select the action with the highest upper
conﬁdence bound.

Algorithm 1 Thompson Sampling
1: Input: prior p
2: for (cid:96) = 1  2  . . .   L do
3:
4:
5: Observe: Y(cid:96) A(cid:96)
6:
7: end for

Sample: ˆθ(cid:96) ∼ p
Act: A(cid:96) = arg max
a∈A

r ˆθ(cid:96)

(a)

Update: p ← P(θ ∈ ·|p  A(cid:96)  Y(cid:96) A(cid:96))

Algorithm 2 Upper Conﬁdence Bound Algorithm
1: Input: upper conﬁdence functions {U(cid:96)}L
2: for (cid:96) = 1  2  . . .   L do
Act: A(cid:96) = arg max
3:
4: Observe: Y(cid:96) A(cid:96)
5:
6: end for

Update: H(cid:96)+1 ← H(cid:96) ∪ {A(cid:96)  Y(cid:96) A(cid:96)}

U(cid:96)(H(cid:96)  a)

a∈A

(cid:96)=1

3

4

Information-theoretic conﬁdence bounds

(cid:113)

Information-theoretic conﬁdence bounds are deﬁned with the intention of capturing the exploration-
exploitation tradeoff for Thompson sampling and optimistic algorithms – if the regret is large  the
agent must have learned a lot about the environment. Let ∆(cid:96) = rθ(A∗) − rθ(A(cid:96)) denote the regret
over the (cid:96)th period. We aim to obtain per-period regret bound of the form

E(cid:96)[∆(cid:96)] ≤ Γ(cid:96)

I(cid:96)(θ; A(cid:96)  Y(cid:96) A(cid:96)) + (cid:96) 

(1)
where I(cid:96)(θ; A(cid:96)  Y(cid:96) A(cid:96)) is the ﬁltered mutual information between θ and the action-outcome pair
during the (cid:96)th period  Γ(cid:96) is the rate at which regret scales with information gain  and we also allow
for a small error term (cid:96). If (1) is satisﬁed with reasonable values for Γ(cid:96) and (cid:96)  a large expected
regret on the left-hand side would imply that the right-hand side must be large as well  meaning that
the agent should gain a lot of information about the environment.
If Γ(cid:96) can be uniformly bounded over (cid:96) = 1  . . .   L  we obtain an information-theoretic regret bound
for any algorithm that satisﬁes (1).
Proposition 2. If (1) holds with Γ(cid:96) ≤ Γ for all (cid:96) = 1  . . .   L  then

E[Regret(L  π)] ≤ Γ

L I(θ; A1  Y1 A1  . . .   A(cid:96)  Y(cid:96) A(cid:96)) + E

(cid:96).

(cid:113)

L(cid:88)

(cid:96)=1

The proof follows from Jensen’s and the Cauchy-Schwarz inequalities  and the chain rule of mutual
information. All complete proofs can be found in the appendix.
The mutual information term on the right-hand side shows how much the agent expects to learn about
θ over L periods. If θ already concentrates around some value  there is not much to learn  and the
result would suggest that the expected regret should be small. In general  the mutual information
term can be bounded by the maximal information gain under any algorithm over L periods  though a
more careful analysis specialized to the algorithm of interest might lead to a better bound.
One way to obtain a per-period regret bound of the form in Equation (1) is through construction of
information-theoretic conﬁdence sets for mean rewards. For each action a  the width of the conﬁdence
set is designed to depend on the information gain about θ from observing outcome Y(cid:96) a.
Lemma 3. Under Assumption 1  if

|rθ(a) − E(cid:96) [rθ(a)]| ≤ Γ(cid:96)
2

I(cid:96)(θ; Y(cid:96) a) ∀a ∈ A

≥ 1 − δ
2

 

(cid:18)
(cid:112)I(cid:96)(θ; Y(cid:96) a) satisﬁes

P(cid:96)

(cid:113)

(cid:113)

(cid:19)

(cid:19)

(cid:19)

then the per-period regret of Thompson sampling and UCB with upper conﬁdence function U(cid:96)(a) =
E(cid:96) [rθ(a)] + Γ(cid:96)

2

E(cid:96) [∆(cid:96)] ≤ Γ(cid:96)

I(cid:96)(θ; A(cid:96)  Y(cid:96) A(cid:96) ) + δB.

The proof follows from the probability matching property of Thompson sampling  optimism of UCB 
and properties of mutual information.
When the reward function r(y) is Lipschitz continuous  we may alternatively construct conﬁdence
sets on outcomes. Further  if the observation noise is additive  we may construct conﬁdence sets on
the mean outcomes.
Lemma 4. If r(·) is K-Lipschitz continuous with respect to some norm (cid:107) · (cid:107) on Y  and if

(cid:107)Y(cid:96) a − E(cid:96) [Y(cid:96) a](cid:107) ≤ Γ(cid:96)
2
then the per-period regret of Thompson sampling

P(cid:96)

I(cid:96)(θ; Y(cid:96) a) ∀a ∈ A

≥ 1 − δ
2

 

E(cid:96) [∆(cid:96)] ≤ K Γ(cid:96)

I(cid:96)(θ; A(cid:96)  Y(cid:96) A(cid:96)) + δB.

Moreover  if there exists a function y : Θ × A → Y such that Y(cid:96) a − y(θ  a) is independent of θ for
all a ∈ A  then it is sufﬁcient to have

P(cid:96)

(cid:107)y(θ  a) − E(cid:96) [y(θ  a)](cid:107) ≤ Γ(cid:96)
2

I(cid:96)(θ; Y(cid:96) a) ∀a ∈ A

≥ 1 − δ
2

(cid:113)
(cid:113)

(cid:113)

(cid:18)

(cid:18)

for the one-period regret bound to hold.

4

for

Γ(cid:96) = 4

(cid:19)

I(cid:96)(θ; Y(cid:96) a) ∀a ∈ A

≥ 1 − δ
2

 

4|A|
δ

  where σ2

(cid:96) max = max

a∈A a(cid:62)Σ(cid:96)a.

(cid:113)

P(cid:96)

2

(cid:18)(cid:12)(cid:12)Y a − E(cid:96)Y a
(cid:12)(cid:12) ≤ Γ(cid:96)
(cid:118)(cid:117)(cid:117)(cid:116)
(cid:16)
(cid:17) log
(cid:112)I(cid:96)(θ; Y(cid:96) a) satisﬁes

(cid:96) max
σ2

(cid:96) max
σ2
w

1 +

log

σ2

(cid:113)

An analogous result would hold for a UCB algorithm if outcomes Y(cid:96) a are scalar valued and the
reward function is nondecreasing. We will push further details to the appendix.

5 Examples

In this section  we show applications of information-theoretic conﬁdence bounds on linear ban-
dits  tabular MDPs  and factored MDPs. The per-period regret bounds highlight the single-period
exploration-exploitation tradeoff for Thompson sampling and the corresponding optimistic algorithms 
while the cumulative regret bounds show how the prior distribution over θ affects regret.

5.1 Linear bandits
Let A ⊂ (cid:60)d be a ﬁnite action set  and assume that supa∈A (cid:107)a(cid:107)2 ≤ 1. We assume a N (µ1  Σ1) prior
over the model parameter θ ∈ (cid:60)d. At time (cid:96)  an action A(cid:96) ∈ A is selected  and Y(cid:96) A(cid:96) = θ(cid:62)A(cid:96) + w(cid:96)
w). Conditioned on H(cid:96)  the posterior distribution of θ is again
is observed  where w(cid:96) are i.i.d. N (0  σ2
normal. Let µ(cid:96) and Σ(cid:96) denote the posterior mean and covariance matrix conditioned on H(cid:96). We
assume that r(·) is bounded under Assumption 1  and is nondecreasing and 1-Lipschitz.
By Lemma 4  since noise is additive  it is sufﬁcient to construct conﬁdence sets on Y a = θ(cid:62)a.
Lemma 5. Under the assumptions stated in Section 5.1 

Thus  it follows from Lemma 4 that the one-period regret of Thompson sampling and UCB with
U(cid:96)(a) = E(cid:96)Y a + Γ(cid:96)

2

E(cid:96)[∆(cid:96)] ≤ Γ(cid:96)

I(cid:96)(θ; A(cid:96)  Y(cid:96) A(cid:96)) + δB.

By Proposition 2  we have the following Bayesian regret bound for Thompson sampling and UCB by
choosing δ = 1
L.
Proposition 6. Under the assumptions stated in Section 5.1  the Bayesian regret of Thompson
sampling and UCB over L periods is

E[Regret(L  π)] ≤ Γ

L I(θ; A1  Y1 A1  . . .   AL  YL AL) + B

(cid:113)

(cid:118)(cid:117)(cid:117)(cid:116)

Γ = 4

(cid:16)

log

σ2

1 max
σ2

1 +

1 max
σ2
w

(cid:17) log(4|A|L).

where

The following lemma bounds the maximal information gain over L time periods.
Lemma 7. For any H(cid:96)-adapted action sequence 

I(θ; A1  Y1 A1  . . .   AL  YL AL) ≤ 1
2

d log

1 +

λmaxL

σ2
w

(cid:18)

(cid:19)

 

where λmax is the largest eigenvalue of Σ1.

O((cid:112)dL log |A| log L)  which matches the result in [16]. For large action sets  it is possible to apply

It follows from Lemma 7 that the Bayesian regret of Thompson sampling and UCB is bounded by

√
a discretization argument and obtain a regret bound of order O(d

L log L).

5

5.2 Tabular MDPs
We consider the problem of learning to optimize a random ﬁnite-horizon MDP M = (S A  R  P  τ  ρ)
in repeated episodes. S is the state space  A is the action space  and we assume that both S and A
are ﬁnite. Assume that for each s  a  the reward distribution is Bernoulli with mean R(s  a)  where
1 s a ∈ (cid:60)2. We further assume that for
R(s  a) follows an independent Beta prior with parameter αR
each s  a  the transition distribution P (s  a ·) follows an independent Dirichlet prior with parameter
1 s a ∈ (cid:60)|S|. τ is a ﬁxed time horizon  and ρ is the initial state distribution. We make the following
αP
simplifying assumption on the prior parameters.
Assumption 2. For all s ∈ S and a ∈ A  αR
1 s a(j) ≥ 2|S| for all
j ∈ {1  . . .  |S|}.
A (deterministic) policy µ is a sequence of functions (µ0  . . .   µτ−1) that map states to actions.
During each episode (cid:96)  the agent selects a policy µ(cid:96)  and observes a trajectory

1 s a(i) ≥ 1 for i ∈ {1  2}  and αP

Y(cid:96) µ(cid:96) = (s(cid:96) 0  a(cid:96) 0  r(cid:96) 1  s(cid:96) 1  . . .   s(cid:96) τ−1  a(cid:96) τ−1  r(cid:96) τ ).

We deﬁne the value function of a policy µ under an MDP ˜M

µ k(s) := E
V ˜M

R(st  at)

(cid:35)
(cid:12)(cid:12)(cid:12) M = ˜M   µ  sk = s

.

Deﬁne the expected value of a policy µ under an MDP ˜M

(cid:34)τ−1(cid:88)
µ = E(cid:104)

t=k

˜M

V

V ˜M

(cid:105)
µ 0(s0)(cid:12)(cid:12) M = ˜M   µ
L(cid:88)

E(cid:104)

µ∗ − V

V

M

.

(cid:105)

M
µ(cid:96)

 

(cid:96)=1

E[Regret(L  π)] =

Let µ∗ denote an optimal policy for the true environment M. The Bayesian regret of an algorithm π
over L episodes is

where the expectation is taken over the randomness in observations  algorithm π  as well as the prior
distribution over M.
We will construct conﬁdence bounds on value functions in the spirit of Lemma 3. As we will see
in the following two lemmas  the structure of MDPs allows us to break down the deviation of value
functions and the information gain at the level of state-action pairs. Thus  it would be sufﬁcient to
construct conﬁdence sets for the reward and transition functions for individual state-action pairs.
The following lemma decomposes the planning error to a sum of on-policy Bellman errors. The proof
can be found in Section 5.1 of [13].
Lemma 8. For any MDP ˆM and policy µ 

(cid:20)(cid:16) ˆR(st  at) − R(st  at)
(cid:17)

E

(cid:16) ˆP (st  at) − P (st  at)

(cid:17)(cid:62)

+

ˆM

µ − V

V

M
µ =

(cid:12)(cid:12)(cid:12)(cid:12) ˆM   M  µ
(cid:21)

 

V ˆM
µ t+1

τ−1(cid:88)

t=0

where ˆR  ˆP are the reward and transition functions under ˆM.
Let H(cid:96)t = (µ(cid:96)  s(cid:96) 0  a(cid:96) 0  . . .   r(cid:96) t  s(cid:96) t) denote the history of episode (cid:96) up to time t and the policy
selected for episode (cid:96). The chain rule of mutual information gives us the following lemma.
Lemma 9. (information decomposition)

I(cid:96) (M ; µ(cid:96)  Y(cid:96) µ(cid:96)) =

I(cid:96) (M ; (s(cid:96) t  a(cid:96) t  r(cid:96) t+1  s(cid:96) t+1) | H(cid:96)t) .

t=0

By Lemmas 8 and 9  we will construct information-theoretic conﬁdence sets for reward and transition
distributions individually for each state-action pair.

6

τ−1(cid:88)

Lemma 10. Let r(cid:96) t s a ∼ Bernoulli(R(s  a))|H(cid:96) H(cid:96)t and s(cid:48)

(cid:96) t s a ∼ P (s  a ·)|H(cid:96) H(cid:96)t. Then 

P(cid:96)

(cid:32)
|R(s  a) − E(cid:96)R(s  a)| ≤ ΓR(cid:114)
(cid:12)(cid:12)(cid:12) ≤ ΓP(cid:114)

(cid:32)(cid:12)(cid:12)(cid:12)(cid:0)P (s  a) − E(cid:96)P (s  a)(cid:1)(cid:62)

V M
µ∗ t+1

min
˜t h˜t

and

P(cid:96)

I(cid:96)(R; s  a  r(cid:96) ˜t s a |H(cid:96)˜t = h˜t)

≥ 1 − δ

(2)

(cid:33)
|H(cid:96)˜t = h˜t)

≥ 1 − δ (3)

I(cid:96)(P ; s  a  s(cid:48)

(cid:96) ˜t s a

min
˜t h˜t

(cid:33)

(cid:19)

for all t and all s  a such that 1(cid:62)αR

(cid:113)
(cid:96) s a ≥ τ − 1 and 1(cid:62)αP

(cid:113)
(cid:96) s a ≥ τ − 1  respectively  where

ΓR =

24 log 2
δ

and ΓP = τ
I(cid:96)(R; s  a  r(cid:96) ˜t s a |H(cid:96)˜t = h˜t) and min˜t h˜t

24 log 2
δ .
I(cid:96)(P ; s  a  s(cid:48)

(cid:96) ˜t s a |H(cid:96)˜t = h˜t) measure
The terms min˜t h˜t
the minimum per-step information gain that the agent can obtain about the reward and transition
functions of a state-action pair during the (cid:96)th episode  conditioned on H(cid:96)  where the minimum is
taken over all possible values of the time step 0 ≤ ˜t ≤ τ − 1 and all possible realizations of the
trajectory H(cid:96)˜t.
Lemma 10 allows us to construct a high probability conﬁdence set M(cid:96) over M  which is dis-
cussed more in details in the appendix. The corresponding UCB algorithm will select µ(cid:96) =
ˆM
µ . Combining with Lemmas 8 and 9  we are able to obtain a per-period
arg maxµ max ˆM∈M(cid:96)
regret bound for Thompson sampling and UCB in the form of (1)  with Γ(cid:96) = ˜O(τ
τ ). This leads to
the following proposition.
Proposition 11. The Bayesian regret of Thompson sampling and UCB over L episodes is

√

V

(cid:18)

(cid:113)

E[Regret(L  π)] = ˜O

τ

τ L I(M ; µ1  Y1 µ1  . . .   µL  YL µL)

.

We show in the appendix that I(M ; µ1  Y1 µ1   . . .   µL  YL µL) = ˜O(S2A). Though we conjecture
that a bound of ˜O(SA) may be attainable under appropriate conditions. The conjecture is supported
by our simulations  as discussed in the appendix.

5.3 Factored MDPs

Factored MDPs [4] are a class of structured MDPs where transitions are represented by a dynamic
Bayesian network [8]  and can typically be encoded in a compact parametric form. Our information-
theoretic approach will lead to a regret bound for Thompson sampling and UCB that depends on the
prior of the model parameter  which typically has dimension exponentially smaller than the state
space and action space.
We start with some deﬁnitions common in the literature [17].
Deﬁnition 1. Let X = X1 × ··· × Xd be a factored set. For any subset of indices Z ⊆ {1  2  . . .   d} 
we deﬁne the scope set X [Z] := ⊗i∈ZXi. Further  for any x ∈ X   deﬁne the scope variable
x[Z] ∈ X [Z] to be the value of the variables xi ∈ Xi with indices i ∈ Z. If Z is a singleton  we will
write x[i] for x[{i}].
Let P(X  Y) denote the set of functions that map x ∈ X to a probability distribution on Y.
Deﬁnition 2. The reward function class R ⊂ P(X  (cid:60)) is factored over S × A = X = X1 ×
··· × Xd with scopes Z1  . . .   Zm if and only if  for all R ∈ R  x ∈ X   there exist functions
{Ri ∈ P(X [Zi] (cid:60))}m

i=1 such that

where r ∼ R(x) is equal to(cid:80)m

E[r] =

E[ri]

i=1 ri with each ri ∼ Ri(x[Zi]) individually observed.

m(cid:88)

i=1

7

Deﬁnition 3. The transition function class P ⊂ P(X  S) is factored over S×A = X = X1×···×Xd
and S = S1 × ··· × Sn with scopes Z1  . . .   Zn if and only if  for all P ∈ P  x ∈ X   s ∈ S  there
exist functions {Pj ∈ P(X [Zj] Sj)}n

j=1 such that

n(cid:89)

j=1

P (s|x) =

Pj(s[j] | x[Zj]).

j }n

M =(cid:0){Xi}d

A factored MDP is an MDP with factored rewards and transitions. Let X = S × A. A factored MDP
is fully characterized by

j=1; τ ; ρ(cid:1)  
(cid:12)(cid:12)X [Z P
j ](cid:12)(cid:12) be the sum of

i }m

i }m

j=1; {Z P

i=1; {Sj}n

j=1; {Pj}n

i=1; {Ri}m

function has size at most K ζ. Let DR =(cid:80)m

i=1; {Z R
j }n
i=1 and {Z P
where {Z R
j=1 are the scopes for the reward and transition functions  which we
assume are known to the agent  τ is a ﬁxed time horizon  and ρ is the initial state distribution.
We assume that |Z R
j | ≤ ζ (cid:28) d and |Xi| ≤ K  so the domain of any reward and transition
i | |Z P
the cardinality of the domains  and let D = DR + DP (cid:28) |S||A|. To simplify exposition  let us
assume that |Sj| ≤ K for all j = 1  . . .   n.
Let xR
factorized rewards are Bernoulli. With a slight abuse of notation  we let Ri(xR
reward  and we assume that Ri(xR
)  αP
Pj(xP
component of αR

j ]. We assume that the
i ) denote the mean
∈ (cid:60)2. We further assume that
∈ (cid:60)|Sj|. Similar to the tabular case  we assume that each

(cid:12)(cid:12)X [Z R
i ](cid:12)(cid:12) and DP =(cid:80)n

i denote an element in X [Z R

j denote an element in X [Z P

i ]  and xP
i ) ∼ Beta(αR

is at least 1  and each component of αP

j  ·) ∼ Dirichlet(αP

)  where αR

is at least

1 i xR
i

1 i xR
i

1 j xP
j

1 j xP
j

j=1

i=1

2|Sj| for all i and j.

1 i xR
i

1 j xP
j

i

∼ Bernoulli(R(xR

i ))|H(cid:96) H(cid:96)t and s(cid:48)

Lemmas 8 and 9 still hold. Since the reward and transition functions can be factorized  we will
construct information-theoretic conﬁdence bounds on these factor functions.
(cid:33)
j  ·)|H(cid:96) H(cid:96)t. Then 
Lemma 12. Let r(cid:96) t i xR
(cid:33)
| H(cid:96)˜t = h˜t)

i )(cid:12)(cid:12) ≤ ΓR(cid:114)
(cid:114)

(cid:32)(cid:12)(cid:12)Ri(xR
(cid:32)

i ) − E(cid:96)Ri(xR

j ) − E(cid:96)Pj(xP

| H(cid:96)˜t = h˜t)

∼ Pj(xP

≥ 1 − δ

≥ 1 − δ

(cid:107)Pj(xP

i   r(cid:96) ˜t i xR

I(cid:96)(Ri; xR

I(cid:96)(Pj; xP

min
˜t h˜t

(cid:96) t j xP
j

j   s(cid:48)

and

P(cid:96)

P(cid:96)

(4)

(5)

i

(cid:96) ˜t j xP
j

for all s  a such that 1(cid:62)αR

(cid:96) i xR
i

≥ τ − 1  respectively  with

j

min
˜t h˜t
≥ τ − 1 and 1(cid:62)αP

j )(cid:107)1 ≤ ΓP
(cid:113)

(cid:96) j xP
j

(cid:113)

ΓR = 2

6 log 2
δ

and ΓP

j = 4

6|Sj| log 2
δ .

The lemma allows us to obtain a per-period regret bound in the form of (1) for Thompson sampling
and a UCB algorithm that uses (4) and (5) to construct conﬁdence sets. As shown in the appendix 

the scaling factor Γ(cid:96) = ˜O(mτ(cid:112)(m + n)Kτ )  which leads to the following proposition.
(cid:19)

Proposition 13. The Bayesian regret of Thompson sampling and UCB over L episodes is

(cid:18)

(cid:113)

mτ

(m + n)Kτ L I (M ; µ1  Y1 µ1  . . .   µL  YL µL)

.

E[Regret(L  π)] = ˜O

Similar to the tabular case  we show that I (M ; µ1  Y1 µ1   . . .   µL  YL µL ) is ˜O(KD) for any algo-
rithm  while we conjecture that this may be ˜O(D) under appropriate conditions. The resulting regret
bound matches the one in [14] if the conjecture proves to be true. Again  our bound in Proposition 13
reveals an explicit dependence on the prior uncertainty  which is not captured by previous work.

8

6 Conclusion

We introduce information-theoretic conﬁdence bounds for analyzing Thompson sampling and deriving
optimistic algorithms. We show that the information-theoretic approach allows us to formally
quantify the agent’s information gain of the unknown environment  and to explicitly characterize
the exploration-exploitation tradeoff for linear bandits  tabular MDPs  and factored MDPs. This
work opens up multiple directions for future research. It would be interesting to extend information-
theoretic conﬁdence bounds to a broader range of problems and see whether a general information-
theoretic framework is plausible for addressing online decision problems. It would also be interesting
to think about whether an information-theoretic perspective could lead to tighter regret bounds for
Thompson sampling and optimistic algorithms. One may also consider the practical implications of
these conﬁdence bounds and how they can be used to design better reinforcement learning algorithms.

Acknowledgments

This work was generously supported by the Charles and Katherine Lin Graduate Fellowship  the
Dantzig-Lieberman Operations Research Fellowship  and a research grant from Boeing. We would
also like to thank Ayfer Özgür and Daniel Russo for helpful discussions.

References
[1] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem.

In Proceedings of the 21st Annual Conference on Learning Theory (COLT)  2012.

[2] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In
Proceedings of The 30th International Conference on Machine Learning  pages 127–135  2013.

[3] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2):235–256  2002.

[4] Craig Boutilier  Richard Dearden  and Moisés Goldszmidt. Stochastic dynamic programming

with factored representations. Artiﬁcial Intelligence  121(1):49 – 107  2000.

[5] O. Cappé  A. Garivier  O.-A. Maillard  R. Munos  and G. Stoltz. Kullback-Leibler upper
conﬁdence bounds for optimal sequential allocation. Annals of Statistics  41(3):1516–1541 
2013.

[6] T.M. Cover and J.A. Thomas. Elements of information theory. John Wiley & Sons  2012.

[7] V. Dani  T.P. Hayes  and S.M. Kakade. Stochastic linear optimization under bandit feedback. In
Proceedings of the 21st Annual Conference on Learning Theory (COLT)  pages 355–366  2008.

[8] Zoubin Ghahramani. Learning dynamic Bayesian networks  pages 168–197. Springer Berlin

Heidelberg  Berlin  Heidelberg  1998.

[9] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research  11:1563–1600  2010.

[10] E. Kauffmann  N. Korda  and R. Munos. Thompson sampling: an asymptotically optimal ﬁnite

time analysis. In International Conference on Algorithmic Learning Theory  2012.

[11] E. Kaufmann  O. Cappé  and A. Garivier. On Bayesian upper conﬁdence bounds for bandit

problems. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[12] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied

mathematics  6(1):4–22  1985.

[13] Ian Osband  Daniel Russo  and Benjamin Van Roy. (More) efﬁcient reinforcement learning
via posterior sampling. In Advances in Neural Information Processing Systems 26. Curran
Associates  Inc.  2013.

9

[14] Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps.
In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors 
Advances in Neural Information Processing Systems 27  pages 604–612. Curran Associates 
Inc.  2014.

[15] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal

of Machine Learning Research  17(68):1–30  2016.

[16] N. Srinivas  A. Krause  S.M. Kakade  and M. Seeger. Information-theoretic regret bounds for
Gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory 
58(5):3250 –3265  May 2012.

[17] István Szita and András L˝orincz. Optimistic initialization and greediness lead to polynomial
time learning in factored mdps. In Proceedings of the 26th Annual International Conference on
Machine Learning  ICML ’09  pages 1001–1008. ACM  2009.

[18] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

10

,Ming Yu
Mladen Kolar
Varun Gupta
Marek Śmieja
Łukasz Struski
Jacek Tabor
Bartosz Zieliński
Przemysław Spurek
Xiuyuan Lu
Benjamin Van Roy