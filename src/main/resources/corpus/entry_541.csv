2014,Analog Memories in a Balanced Rate-Based Network of E-I Neurons,The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However  despite decades of theoretical work on the subject  the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically  some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory)  while some others restrict the representation of memories to a binary format  or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime  are robust to corruptions of the memory cue as well as to ongoing noise  and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.,Analog Memories in a Balanced Rate-Based

Network of E-I Neurons

Dylan Festa

df325@cam.ac.uk

Guillaume Hennequin
gjeh2@cam.ac.uk

M´at´e Lengyel

m.lengyel@eng.cam.ac.uk

Computational & Biological Learning Lab  Department of Engineering

University of Cambridge  UK

Abstract

The persistent and graded activity often observed in cortical circuits is some-
times seen as a signature of autoassociative retrieval of memories stored earlier
in synaptic efﬁcacies. However  despite decades of theoretical work on the sub-
ject  the mechanisms that support the storage and retrieval of memories remain
unclear. Previous proposals concerning the dynamics of memory networks have
fallen short of incorporating some key physiological constraints in a uniﬁed way.
Speciﬁcally  some models violate Dale’s law (i.e. allow neurons to be both excita-
tory and inhibitory)  while some others restrict the representation of memories to
a binary format  or induce recall states in which some neurons ﬁre at rates close
to saturation. We propose a novel control-theoretic framework to build function-
ing attractor networks that satisfy a set of relevant physiological constraints. We
directly optimize networks of excitatory and inhibitory neurons to force sets of
arbitrary analog patterns to become stable ﬁxed points of the dynamics. The re-
sulting networks operate in the balanced regime  are robust to corruptions of the
memory cue as well as to ongoing noise  and incidentally explain the reduction
of trial-to-trial variability following stimulus onset that is ubiquitously observed
in sensory and motor cortices. Our results constitute a step forward in our under-
standing of the neural substrate of memory.

1

Introduction

Memories are thought to be encoded in the joint  persistent activity of groups of neurons. According
to this view  memories are embedded via long-lasting modiﬁcations of the synaptic connections
between neurons (storage) such that partial or noisy initialization of the network activity drives
the collective dynamics of the neurons into the corresponding memory state (recall) [1]. Models of
memory circuits following these principles abound in the theoretical neuroscience literature  but few
respect some of the most fundamental properties of brain networks  including: i) the separation of
neurons into distinct classes of excitatory (E) and inhibitory (I) cells – known as Dale’s law –  ii) the
presence of recurrent and sparse synaptic connections  iii) the possibility for each neuron to sustain
graded levels of activity in different memories  iv) the ﬁring of action potentials at reasonably low
rates  and v) a dynamic balance of E and I inputs.
In the original Hopﬁeld network [1]  connectivity must be symmetrical  which violates Dale’s law.
Moreover  just as in much of the work following it up  memories are encoded in binary neuronal
responses and so converge towards effectively binary recall states even if the recall dynamics for-
mally uses graded activities [2]. Subsequent work considered non-binary pattern distributions [3  4] 
and derived high theoretical capacity limits for them  but those capacities proved difﬁcult – if not
impossible – to realise in practice [5  6]  and the network dynamics therein did not explicitly model
inhibitory neurons thus implicitly assuming instantaneous inhibitory feedback. More recent work

1

Figure 1: (a) Examples of analog patterns of excitatory neuronal activities  drawn from a log-normal
distribution. In all our training experiments  network parameters were optimized to stabilize a set
of such analog patterns and the baseline  uniform activity state (top row). For ease of visualization 
only 30 of the 100 excitatory neurons are shown. (b) Optimized values of the inhibitory (auxiliary)
neuronal ﬁring rates for 5 of 30 learned memories (corresponding to those in panel a). Only 30 of
the 50 auxiliary neurons are shown. (c) Empirical distributions of ﬁring rates across neurons and
memory patterns  for each population.

incorporated Dale’s law  and described neurons using the more realistic  leaky integrate-and-ﬁre
(LIF) neuron model [7]. However  the stability of the recall states still relied critically on the satu-
rating behavior of the LIF input-output transfer function at high rates. Although it was later shown
that dynamic feedback inhibition can stabilize relatively low ﬁring rates in subpopulations of more
tightly connected neurons [8  9]  inhibitory feedback in these models is global  and calibrated for a
single stereotypical level of excitation for all memories  implying effectively binary memories again.
Finally  spatially connected networks are able to sustain graded activity patterns (spatial “bumps”) 
but make strong assumptions about the spatial structure of both the connectivity and the memory
patterns  and are sensitive to ongoing noise (e.g. [10  11]). Ref. [12] provides a rare example of
spike timing-based graded memory network  but it again did not contain inhibitory units.
Here we propose a general control-theoretic framework that overcomes all of the above limitations
with minimal additional assumptions. We formalize memory storage as implying two conditions:
that the desired activity states be ﬁxed points of the dynamics  and that the dynamics be stable
around those ﬁxed points. We directly optimize the network parameters  including the synaptic
connectivity  to satisfy both conditions for a collection of arbitrary  graded memory patterns (Fig. 1).
The ﬁxed point condition is achieved by minimizing the time derivative of the neural activity  such
that ideally it reaches zero  at each of the desired attractor states. Stability  however  is more difﬁcult
to achieve because the ﬁxed-point constraints tend to create strong positive feedback loops in the
recurrent circuitry  and direct measures of dynamical stability (eg. the spectral abscissa) do not admit
efﬁcient  gradient-based optimization. Thus  we use recently developed methods from robust control
theory  namely the minimization of the Smoothed Spectral Abscissa (SSA  [13  14]) to perform
robust stability optimization. To satisfy biological constraints  we parametrize the networks that we
optimize such that they have realistic ﬁring rate dynamics and their connectivities obey Dale’s law.
We show that despite these constraints the resulting networks perform memory recall that is robust
to noise in both the recall cue and the ongoing dynamics  and is stabilized through a tight dynamic
balance of excitation and inhibition. This novel way of constructing structurally realistic memory
networks should open new routes to the understanding of memory and its neural substrate.

2 Methods

We study a network of n = nE (excitatory) +nI (inhibitory) neurons. The activity of neuron i is
represented by a single scalar potential vi  which is converted into a ﬁring rate ri via a threshold-
quadratic gain function (e.g. [15]):

ri = g(vi)

i

if
vi > 0
otherwise

(1)

:= (cid:26)γv2

0

2

5Hzmemoriesabc20Hzexc.(prescribeddistribution)0102030inh.(optimizeddistribution)exc.neuronsinh.neuronsﬁringrate[Hz]We set γ to 0.04  such that g(vi) spans a few tens of Hz when vi spans a few tens of mV  as
experimentally observed in cortical areas (e.g. cat’s V1 [16]). The instantaneous state of the system
can be expressed as a vector v(t) := (v1(t)  . . .   vn(t)). We denote the activity of the excitatory or
inhibitory subpopulation by vexc and vinh  respectively. The recurrent interactions between neurons
are governed by a synaptic weight matrix W  in which the sign of each element Wij depends on
the nature (excitatory or inhibitory) of the presynaptic neuron j. We enforce Dale’s law via a re-
parameterization of the synaptic weights:

Wij = sj log(1 + exp βij) with sj =(cid:26)+1

−1

if j ≤ nE
otherwise

(2)

where the βij’s are free  unconstrained parameters. (We do not allow for autapses  i.e. we ﬁx Wii =
0). The network dynamics are thus given by:

τi

dvi
dt

= −vi +

n(cid:88)j=1

Wij g(vj) + hi

 

(3)

where τi is the membrane time constant  and hi is a constant external input  independent of the
memory we wish to recall.
It is worth noting that  since the gain function g(vi) deﬁned in Eq (1) has no upper saturation 
recurrent interactions can easily result in runaway excitation and ﬁring rates growing unbounded.
However  our optimization algorithm will naturally seek stable solutions  in which ﬁring rates are
kept within a limited range due to a ﬁne dynamic balance of excitation and inhibition [14].

Optimizing network parameters to embed attractor memories

We are going to build and study networks that have a desired set of analog activity patterns as stable
ﬁxed points of their dynamics. Let {vµ
exc}µ=1 ... m be a set of m target analog patterns (Fig. 1) 
deﬁned in the space of excitatory neuronal activity (potentials). For a given pattern µ  the inhibitory
neurons will be free to adjust their steady state ﬁring rates vµ
inh to whatever pattern proves to be
optimal to maintain stability.
In other words  we think of the activity of inhibitory neurons as
“auxiliary” variables.
A given activity pattern vµ ≡ (vµ(cid:62)
only if  it satisﬁes the following two conditions:

inh)(cid:62) is a stable ﬁxed point of the network dynamics if  and

exc  vµ(cid:62)

= 0

and

α (Jµ) < 0

(4)

dv

dt(cid:12)(cid:12)(cid:12)(cid:12)v=vµ

ij := Wij g(cid:48)(vµ

where Jµ is the Jacobian matrix of the dynamics in Eq. 3  i.e. J µ
j ) − δij (Kronecker’s
delta)  and α(Jµ) denotes the spectral abscissa (SA)  deﬁned as the largest real part in the eigenvalue
spectrum of Jµ. The ﬁrst condition makes vµ a ﬁxed point of the dynamics  while the second
condition makes that ﬁxed point asymptotically stable with respect to small local perturbations.
Note that the width of the basin of attraction is not captured by the SA.
The two conditions in Eq. 4 depend on a set of network parameters that we will allow ourselves
to optimize. These are all the synaptic weight parameters (βij  i (cid:54)= j)  as well as the values of the
inhibitory neurons’ ﬁring rates in each attractor (vµ
inh  µ = 1  . . .   m). Thus  we may adjust a total
of n(n − 1) + nI m parameters.
Using Eq. 3  the ﬁrst condition in Eq. 4 can be rewritten as vµ
j ) − hi = 0.
Despite this equation being linear in the synaptic weights  the re-parameterization of Eq. 2 makes
it nonlinear in β  and it is in any case nonlinear in vµ
inh. We will therefore seek to satisfy this
condition by minimizing (cid:107) dv/dt|v=vµ (cid:107)2  which quantiﬁes how fast the potentials drift away when
initialized in the desired attractor state vµ. When it is zero  vµ is a ﬁxed point of the dynamics. Our
optimization procedure (see below) may not be able to set this term to exactly zero  especially as we
try to store a large number of memories  but in practice we ﬁnd it becomes small enough that the
Jacobian-based stability criterion remains valid.
Meeting the stability condition (second condition in Eq. 4) turns out to be more involved. The SA
is  in general  a non-smooth function of the matrix elements and is therefore difﬁcult to minimize.

i −(cid:80)n

j=1 Wijg(vµ

3

ij}  which in turn depend both on the connectivity parameters {βij} and on vµ

A more suitable stability measure has been introduced recently in the context of robust control
theory [13  14]  called the Smoothed Spectral Abscissa (SSA)  which we will use here and denote
by ˜αε(Jµ). The SSA  deﬁned for some smoothness parameter ε > 0  is a differentiable relaxation of
the SA  with the properties α(Jµ) < ˜αε(Jµ) and limε→0 ˜αε(Jµ) = α(Jµ). Therefore  the criterion
˜αε(Jµ) ≤ 0 implies α(Jµ) < 0  and can therefore be used as an indication of local stability.
Both the SSA and its gradient are straightforward to evaluate numerically  making it amenable to
minimization through gradient descent. Note that the SSA depends on the Jacobian matrix elements
{J µ
inh. Note also that
the parameter ε > 0 controls how tightly the SSA hugs the SA. Small values make it a tight upper
bound  with increasingly ill-behaved gradients. Large values imply more smoothness  but may no
longer guarantee that the SSA has a negative minimum even though the SA might have one. In our
system of n = 150 neurons we found ε = 0.01 to yield a good compromise. In the general case the
distance between SA and SSA grows linearly with the number of dimensions.To keep it invariant  ε
should be scaled accordingly. We therefore used the following heuristic rule ε = 0.01 · 150/n.
We summarize the above objective into a global cost function by lumping together the ﬁxed point
and stability conditions  summing over the entire set of m target memory patterns  and adding an L2
penalty term on the synaptic weights to regularize:

ψ ({βij} {vµ

inh}) :=

1
m

m(cid:88)µ=1(cid:32) 1
n(cid:13)(cid:13)(cid:13)(cid:13)

dv

dt(cid:13)(cid:13)(cid:13)(cid:13)

2

v=vµ

+ ηs ˜αε (Jµ)(cid:33) +

ηF
n2 (cid:107)W(cid:107)2

F .

(5)

F is the squared Frobenius norm of W  i.e. the sum of its squared elements  and the
where (cid:107)W(cid:107)2
parameters ηs and ηF control the relative importance of each component of the objective function.
We set them heuristically (Table 1). We used a variant of the low-storage BFGS algorithm included
in the open source library NLopt [17] to minimize ψ.

Choice of initial parameters and attractors

The synaptic weights are initially drawn randomly from a Gamma distribution with a shape factor of
2 and a mean that depends only on the type of pre- and post-synaptic population. The mean synaptic
weights of the four synapse types were computed using a mean-ﬁeld reduction of the full network
in which all
to meet the condition that the network initially exhibits a stable baseline state vµ=1
exc
excitatory ﬁring rates equal rbaseline = 5 Hz (Table 1  and Supplementary Material). This base-
line state was included in every set of m target attractors that we used and was thus stable from
exc}µ=2 ... m were generated
the beginning  by construction. For the remaining target patterns  {vµ
by inverting (using g−1) ﬁring rates that were sampled from a log-normal distribution with a mean
matching the baseline ﬁring rate  rbaseline (Fig. 1a) and a variance of 5 Hz. This log-normal distri-
bution was chosen to roughly capture the skewed and heavy-tailed nature of ﬁring rate distributions
observed in vivo (see e.g. for a review [18]). The inhibitory potentials in the memory states  {vµ
inh} 
were initialized to the baseline  g−1(5 Hz)  and were subsequently used as free parameters by the
learning algorithm (cf. above; see also Fig. 1b).

3 Results

Example of successful storage

Figure 2 shows an example of stability optimization: in this speciﬁc run we used 150 neurons to em-
bed 30 graded attractors (examples of which where shown in Fig. 1)  yielding a storage capacity of
0.2. Other parameters are listed in Table 1. Gradient descent gradually reduces each of the attractor-
speciﬁc sub-objectives in Eq. 5  namely the SSA  the SA  and the potential velocities (cid:107)dv/dt(cid:107)2 in
each target state (Fig. 2). After convergence  the SSA has become negative for all desired states 
indicating stability. Note  however  that (cid:107)dv/dt(cid:107) after convergence is small but non-zero in each
of the target memories. Thus  strictly speaking  the target patterns haven’t become ﬁxed points of
the dynamics  but only slow points from which the system will eventually drift away. In practice
though  we found that stability was robust enough that an exact  stable ﬁxed point had in fact been
created very near each target pattern. This is detailed below.

4

Figure 2: (a) Decrease of the SA (solid line) and of the SSA (dotted line) during learning in systems
with 30 (purple) and 50 attractors (orange). Thick lines show averages across attractors  ﬂanking
lines show the corresponding standard deviations. The x-axis marks the actual duration of the run of
the learning algorithm. (b) Euclidean norm of the velocity at the ﬁxed point during learning. Lines
and colors as in a. Note the logarithmic y-axis.

Table 1: Parameter settings

100
nE
50
nI
m 30

τE
τI
rbaseline

20 ms
10 ms
5 Hz

ηs
ηF

0.02
0.001

Memory recall performance and robustness

For recall  we initialize neuronal activities at a noisy version of one of the target patterns  and study
the subsequent evolution of the network state. The network performs well if its dynamics clean up
the noise and home in on the target pattern (autoassociative behavior) and if it achieves this robustly
even in the face of large amounts of noise.
Initial cues are chosen to be linear combinations of the form r(t = 0) = σ ˜r + (1− σ) rµ  where rµ
is the memory we intend to recall and ˜r is an independent random vector with the same lognormal
statistics used to generate the memory patterns themselves. The parameter σ regulates the noise
level: σ = 0 sets the network activity directly in the desired attractor  while σ = 1 initializes it with
completely random values.
The deviation of the momentary network state r(t) ≡ g(v(t)) from the target pattern rµ ≡ g(vµ)
is measured in terms of the squared Euclidean distance  further normalized by the expected squared
distance between rµ and a random pattern drawn from the same distribution (log-normal in our
case). Formally:

.

(6)

dµ(t) := (cid:107)rexc(t) − rµ
(cid:104)(cid:107)˜rexc − rµ

exc(cid:107)2
exc(cid:107)2(cid:105)˜r

Figure 3a shows the temporal evolution of dµ(t) on a few sample recall trials  for two different noise
levels σ. For σ = 0.5  recalls are always successful  as the network state converges to the right target
pattern on each trial. For σ = 0.75  the network activity occasionally settles in another  well distinct
attractor.
We used the convention that a trial is deemed successful if the distance dµ(t) falls below 0.001. (A
∼ 3 Hz deviation from the target in only one of the 100 exc. neurons  with all other 99 neurons
behaving perfectly  would be sufﬁcient to cross this threshold and fail the test.) We further measure
performance as the probability of successful recall  which we estimated from many independent
trials with different realizations of the noise ˜r in the initial condition (Figure 3b). The network
performance is also compared to an “ideal observer” [6] that has direct access to all the stored
memories (rather than just their reﬂection in the synaptic weights) and simply returns that pattern
in the training set {rµ} to which the initial cue is closest (Fig. 3b). Thus  as an upper bound on
performance  the ideal observer only produces a wrong recall when the added noise brings the
initial state closer to an attractor that is different from the target. Remarkably  our network dynamics

5

0204060−1−0.50time(hours)SA/SSAh˜αε(Jµ)iµhα(Jµ)iµm=30m=50a020406010−410−2time(hours)D(cid:13)(cid:13)˙v(µ)(cid:13)(cid:13)2Eµb(a) Example recall trials for a single memory rµ  which is presented to the network at
Figure 3:
time t = 0 in a corrupted version that is different on every trial  for two different values of the
noise level σ (colors). Shown here is the temporal evolution of the momentary distance between the
vector of excitatory ﬁring rates rexc(t) and the memory pattern rµ
exc. Different lines correspond to
different trials. (b) Fraction of trials that converged onto the correct attractor (ﬁnal distance dµ(t =
∞) < 0.001  cf. text) as a function of the normalized distance between the initial condition and the
desired attractor  dµ(t = 0). Thick lines show medians across attractors  ﬂanking thin lines show
the 25th and 75th percentiles. The performance of the baseline state is shown separately (orange).
The dashed lines show the performance of an “ideal observer”  always selecting the memory closest
to the initial condition  for the same trials.

(continuous lines) and the ideal observer (dashed lines) have comparable performances. When trying
to recall the uniform pattern of baseline activity  the performance appears much better (orange line)
both for the ideal observer and the network. This is simply because the random vectors used to
perturb the system have a high probability of lying closer to the mean of the log normal distribution
(that is  the baseline state) than to any other memory pattern. Moreover  the network was initialized
prior to learning with the baseline as the single global attractor  and this might account for the
additional tendency of the network (solid orange line) to fall on such state  as compared to the ideal
observer (dotted orange line).

Only a few strong synaptic weights contribute to memory recall

Synaptic weights after learning (Fig. 4a) are sparse: their distribution shows the characteristic peak
near zero and the long tail observed in real cortical circuits [19  20] (Fig. 4b). This sparseness cannot
be accounted for by the L2 norm regularizer in the cost function (Eq. 5) as it does not promote
sparsity as an L1 term would. Thus  the observed sparsity in the trained network must be a genuine
consequence of having optimized the connectivity for robust stability.
If we assume that weights |Wij| ≤ 0.01 correspond to functionally silent synapses  then the trained
network contains 52% of silent excitatory synapses and 46% of silent inhibitory ones (Fig. 4c). We
wondered if those weak  “silent” synapses are necessary for stability of memory recall  or could be
removed altogether without affecting performance. To test that  we clipped those synapses {|Wij| <
0.01} to zero  and computed recall performance again (Fig. 4d). This clipping turns out to slightly
shift the position of the attractors in state space  so we increased the distance threshold that deﬁnes
a successful recall trial to 0.08. The test reveals that one of the attractors loses stability  reducing
the average performance. However the remaining 29 attractors are robust to this removal of weak
synapses and show near-equal recall performance as above. This demonstrates that small weights 
though numerous  are not necessary for competent recall performance.

Balanced state

As a result of the connection weight distributions and robust stability  the trained network produces
a regime in which excitation and inhibition balance each other  precisely tuning each neuron to
its target frequency in each attractor. Excitatory and inhibitory inputs are deﬁned as hexc
(t) =
(t)

j=1(cid:98)− Wij(cid:99)+ rj(t) so that the difference hexc

corresponds to the total recurrent input  i.e. the second term on the r.h.s. of Eq. 3.

(cid:80)n
j=1(cid:98)Wij(cid:99)+ rj(t) and hinh

i

(t) =(cid:80)n

i

i

(t) − hinh

i

6

00.10.200.511.52(a)t(s)dµ(t)σ=0.50σ=0.75a00.20.40.60.8100.20.40.60.81bdµ(t0)probabilityofsuccessnetworkidealmemoriesbaselineFigure 4: (a) Synaptic weight matrix after learning. Note the logarithmic color scale. (b) Distri-
bution of the excitatory (red) and inhibitory (blue) weights. (c) Cumulative weight distribution of
absolute weight values. Gray line marks the 0.01 threshold we use to deﬁned “silent” synapses. (d)
Performance of the network after clipping the weights below 0.01 to zero (black  median with 25th
and 75th percentiles)  compared to the performance of the unperturbed network redrawn from Fig. 3
(purple).

Figure 5: (a) Dynamics of the excitatory and inhibitory inputs during a memory recall trial  for
three sample neurons. (b) Scatter plot of steady-state excitatory versus inhibitory inputs. Each dot
corresponds to a different memory pattern  and several neurons are shown in different colors. (c)
Histogram of E and I input correlations across all memories for each neuron (for example  one value
binned in this histogram would be the correlation between all green dots in b).

i

i

(t) and hinh

Figure 5a shows the evolution of hexc
(t) during a recall trial for one of the stored random
attractors  for 3 different neurons. Neuron 3 has rate target of 9Hz  well above average  therefore its
excitation is much higher than inhibition. Neuron 72 has a steady state ﬁring rate of 2 Hz  below
average: its inhibitory input is greater than the excitatory one  and ﬁring is driven by the external
current. Finally  neuron 101 is inhibitory and has a target rate 0  and indeed its inhibitory input
is large enough to overwhelm the combined effects of the external and recurrent excitatory inputs.
Notably  in all these cases  both E and I input currents are fairly large but cancel each other to leave
something smaller  either positive or negative.
Figure 5b shows the E vs. I inputs at steady-state across all the embedded attractors  for various
neurons plotted in different colors. These E and I inputs tend to be correlated across attractors for
every single neuron (dots in Fig. 5 tend to hug the identity line)  with relative differences ﬁne-tuned
to yield the desired ﬁring rates. These across-attractors E/I correlations are summarized in Fig. 5c
as a histogram over neurons.

Robustness to ongoing noise and reduction of across-trial variability following recall onset

Finally  to probe the system under more realistic dynamics  we added time-varying  Gaussian white
noise such that  in an excitatory neuron free from network interactions  the potential would ﬂuctuate

7

11501150postsynapticpresynapticexc.inh.-15-5-1-0.100.11515Wij−10−50510weightexc.inh.ba00.5100.250.50.751startingdistancefromattr.successrateclippedfullcd10−410−210000.51weight020406002040t(ms)hexck(t) hinhk(t)k=3k=72k=101a02040600204060hexck(t∞)hinhk(t∞)k=3k=72k=101b00.51correlationk=15cFigure 6: (a) Normalized distance calculated according to Eq. 6 between the network activity and
each of the attractors (targeted attractor: green line; others: orange lines) during a noisy recall
episode. (b) Trial-to-trial variability  expressed as the standard deviation of a neuron’s activity across
multiple repetitions with random initial conditions. At time t = 0.5 s the network receives a pulse
in the direction of one target attractor (µ = 2). Gray lines are for single neurons; the black line is
an average over the population.

with standard deviation 0.33. Figure 6a shows the momentary distance dµ(t) of the network state
from the attractor closest to the initial cue (green)  and for all other attractors (orange)  during a
recall trial. It is clear that the system revolves around the desired attractor  performing successful
recall despite the ongoing noise. In a second experiment  we ran many trials in which the initial-
ization at time t = 0 was random  while the same spatially patterned stimulation – aligned onto a
chosen attractor – is given to the network in each trial at time t = 0.5 sec. Figure 6b shows the stan-
dard deviation of the internal state of a neuron across trials  averaged across the neural population.
Following stimulus onset  neurons are always pushed towards the target attractor  and this greatly
reduces trial-by-trial variability  compared to the initial spontaneous regime in which the neurons
would ﬂuctuate around any of the activity levels corresponding to its assigned attractors. Interest-
ingly  such stimulus-induced variability reduction has been observed very broadly across sensory
and motor cortical areas [21]. This extends previous work  e.g. [22] and [23]  showing variability
reduction in a multiple-attractor scenario with effectively binary patterns  to the case of patterns with
graded activities.

4 Discussion

We have provided a proof of concept that a model cortical networks of E and I neurons can embed
multiple analog memories as stable ﬁxed-points of their dynamics. Memories are stable in the face
of ongoing noise and corruption of the recall cues. Neuronal activities do not saturate  and indeed 
our single-neuron model did not explicitly incorporate an upper saturation mechanism: dynamic
feedback inhibition  precisely matched to the level of excitation incurred by each attractor  ensures
that each neuron can ﬁre at a relatively low rate during recall. As a result  excitation and inhibition
are tightly balanced.
We have used a rate-based formulation of the circuit dynamics  which raises the question of the
applicability of our method to understanding spiking memory networks. Once the connectivity
in the rate model is generated and optimized  it could still be used in a spiking model  provided
the gain function we have used here matches that of the single spiking neurons. In this respect 
the gain function we have used here is likely an appropriate choice: in physiological conditions 
cortical neurons have input-output gain functions that are well approximated by a rectiﬁed power-
law function over their entire dynamic range [24  25  26].
An important question for future research is how local synaptic learning rules can achieve the stabi-
lization objective that we have approached here from an optimal  algorithmic viewpoint. Inhibitory
synaptic plasticity is a promising candidate  as it has already been shown to enable self-regulation of
the spontaneous  baseline activity regime  and also to promote the stable storage of binary memory
patterns [27]. More work is required in this direction.
Acknowledgements. This work was supported by the Wellcome Trust (GH  ML)  the European
Union Seventh Framework Programme (FP7/20072013) under grant agreement no. 269921 (Brain-
ScaleS) (DF  ML)  and the Swiss National Science Foundation (GH).

8

00.20.40.60.81012at(s)dµ(t)nearestothers00.20.40.60.810123bt(s)hstd[vi(t)]iiReferences
[1] Hopﬁeld J. Neural networks and physical systems with emergent collective computational abilities  Pro-

ceedings of the national academy of sciences 79:2554  1982.

[2] Hopﬁeld J. Neurons with graded response have collective computational properties like those of two-state

neurons  Proceedings of the national academy of sciences 81:3088  1984.

[3] Treves A. Graded-response neurons and information encodings in autoassociative memories  Phys. Rev.

A 42:2418  1990.

[4] Treves A  Rolls ET. What determines the capacity of autoassociative memories in the brain?  Network:

Computation in Neural Systems 2:371  1991.

[5] Battaglia FP  Treves A. Stable and rapid recurrent processing in realistic autoassociative memories  Neural

Comput 10:431  1998.

[6] Lengyel M  Dayan P. Rate- and phase-coded autoassociative memory  In Advances in Neural Information

Processing Systems 17  769  Cambridge  MA  2005. MIT Press.

[7] Amit D  Brunel N. Dynamics of a recurrent network of spiking neurons before and following learning 

Network: Computation in Neural Systems 8:373  1997.

[8] Latham P  Nirenberg S. Computing and stability in cortical networks  Neural computation 16:1385  2004.
[9] Roudi Y  Latham PE. A balanced memory network  PLoS Computational Biology 3:e141  2007.
[10] Ben-Yishai R  et al. Theory of orientation tuning in visual cortex  Proc. Natl. Acad. Sci. USA 92:3844 

1995.

[11] Goldberg JA  et al. Patterns of ongoing activity and the functional architecture of the primary visual

cortex  Neuron 42:489  2004.

[12] Lengyel M  et al. Matching storage and recall: hippocampal spike timing–dependent plasticity and phase

response curves  Nature Neuroscience 8:1677  2005.

[13] Vanbiervliet J  et al. The smoothed spectral abscissa for robust stability optimization  SIAM Journal on

Optimization 20:156  2009.

[14] Hennequin G  et al. Optimal control of transient dynamics in balanced networks supports generation of

complex movements  Neuron 82:1394  2014.

[15] Ahmadian Y  et al. Analysis of the stabilized supralinear network  Neural Comput. 25:1994  2013.
[16] Anderson JS  et al. The contribution of noise to contrast invariance of orientation tuning in cat visual

cortex  Science 290:1968  2000.

[17] Johnson SG. The NLopt nonlinear-optimization package  http://ab-initio.mit.edu/nlopt .
[18] Roxin A  et al. On the distribution of ﬁring rates in networks of cortical neurons  The Journal of Neuro-

science 31:16217  2011.

[19] Song S  et al. Highly nonrandom features of synaptic connectivity in local cortical circuits  PLoS Biol 3:

e68  2005.

[20] Lefort S  et al. The excitatory neuronal network of the C2 barrel column in mouse primary somatosensory

cortex  Neuron 61:301   2009.

[21] Churchland MM  et al. Stimulus onset quenches neural variability: a widespread cortical phenomenon 

Nat Neurosci 13:369  2010.

[22] Litwin-Kumar A  Doiron B. Slow dynamics and high variability in balanced cortical networks with clus-

tered connections  Nat Neurosci 15:1498  2012.

[23] Deco G  Hugues E. Neural network mechanisms underlying stimulus driven variability reduction  PLoS

computational biology 8:e1002395  2012.

[24] Priebe NJ  Ferster D. Direction selectivity of excitation and inhibition in simple cells of the cat primary

visual cortex  Neuron 45:133  2005.

[25] Priebe NJ  Ferster D. Mechanisms underlying cross-orientation suppression in cat visual cortex  Nat Neu-

rosci 9:552  2006.

[26] Finn IM  et al. The emergence of contrast-invariant orientation tuning in simple cells of cat visual cortex 

Neuron 54:137  2007.

[27] Vogels TP  et al. Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory

networks  Science 334:1569  2011.

9

,Dylan Festa
Guillaume Hennequin
Mate Lengyel
Laetitia Papaxanthos
Felipe Llinares-López
Dean Bodenham
Karsten Borgwardt
Wei-Ning Hsu
Yu Zhang
James Glass