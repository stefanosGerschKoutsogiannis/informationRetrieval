2012,Generalization Bounds for Domain Adaptation,In this paper  we provide a new framework to study the generalization bound of the learning process for domain adaptation. Without loss of generality  we consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data. In particular  we introduce two quantities that capture the inherent characteristics of domains. For either kind of domain adaptation  based on the two quantities  we then develop the specific Hoeffding-type deviation inequality and symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. By using the resultant generalization bound  we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile  we discuss the factors that affect the asymptotic behavior of the learning process. The numerical experiments support our results.,Generalization Bounds for Domain Adaptation

Chao Zhang1  Lei Zhang2 

Jieping Ye1 3

1Center for Evolutionary Medicine and Informatics  The Biodesign Institute 

and 3Computer Science and Engineering  Arizona State University  Tempe  USA

{czhan117 jieping.ye}@asu.edu
2School of Computer Science and Technology 

Nanjing University of Science and Technology  Nanjing  P.R. China

zhanglei.njust@yahoo.com.cn

Abstract

In this paper  we provide a new framework to study the generalization bound of
the learning process for domain adaptation. We consider two kinds of representa-
tive domain adaptation settings: one is domain adaptation with multiple sources
and the other is domain adaptation combining source and target data. In particu-
lar  we use the integral probability metric to measure the difference between two
domains. Then  we develop the speciﬁc Hoeffding-type deviation inequality and
symmetrization inequality for either kind of domain adaptation to achieve the cor-
responding generalization bound based on the uniform entropy number. By using
the resultant generalization bound  we analyze the asymptotic convergence and the
rate of convergence of the learning process for domain adaptation. Meanwhile  we
discuss the factors that affect the asymptotic behavior of the learning process. The
numerical experiments support our results.

1

Introduction

In statistical learning theory  one of the major concerns is to obtain the generalization bound of a
learning process  which measures the probability that a function  chosen from a function class by an
algorithm  has a sufﬁciently small error (cf. [1 2]). Generalization bounds have been widely used to
study the consistency of the learning process [3]  the asymptotic convergence of empirical process
[4] and the learnability of learning models [5]. Generally  there are three essential aspects to obtain
generalization bounds of a speciﬁc learning process: complexity measures of function classes  devi-
ation (or concentration) inequalities and symmetrization inequalities related to the learning process
(cf. [3  4  6  7]).
It is noteworthy that the aforementioned results of statistical learning theory are all built under the
assumption that training and test data are drawn from the same distribution (or brieﬂy called the
assumption of same distribution). This assumption may not be valid in many practical applications
such as speech recognition [8] and natural language processing [9] in which training and test data
may have different distributions. Domain adaptation has recently been proposed to handle this
situation and it is aimed to apply a learning model  trained by using the samples drawn from a
certain domain (source domain)  to the samples drawn from another domain (target domain) with a
different distribution (cf. [10  11  12  13]).
This paper is mainly concerned with two variants of domain adaptation. In the ﬁrst variant  the
learner receives training data from several source domains  known as domain adaptation with multi-
ple sources (cf. [14  15  16  17]). In the second variant  the learner minimizes a convex combination

1

of empirical source and target risk  termed as domain adaptation combining source and target data
(cf. [13  18])1.

1.1 Overview of Main Results

In this paper  we present a new framework to study generalization bounds of the learning processes
for domain adaptation with multiple sources and domain adaptation combining source and target
data  respectively. Based on the resultant bounds  we then study the asymptotic behavior of the
learning process for the two kinds of domain adaptation  respectively. There are three major aspects
in the framework: the quantity that measures the difference between two domains  the deviation
inequalities and the symmetrization inequalities that are both designed for the situation of domain
adaptation2.
Generally  in order to obtain the generalization bounds of a learning process  it is necessary to obtain
the corresponding deviation (or concentration) inequalities. For either kind of domain adaptation 
we use a martingale method to develop the related Hoeffding-type deviation inequality. Moreover 
in the situation of domain adaptation  since the source domain differs from the target domain  the
desired symmetrization inequality for domain adaptation should incorporate some quantity to re-
ﬂect the difference. We then obtain the related symmetrization inequality incorporating the integral
probability metric that measures the difference between the distributions of the source and the target
domains.
Next  we present the generalization bounds based on the uniform entropy number for both kinds of
domain adaptation. Finally  based on the resultant bounds  we give a rigorous theoretic analysis to
the asymptotic convergence and the rate of convergence of the learning processes for both types of
domain adaptation. Meanwhile  we give a comparison with the related results under the assumption
of same distribution. We also present numerical experiments to support our results.

1.2 Organization of the Paper

The rest of this paper is organized as follows. Section 2 introduces the problems studied in this
paper. Section 3 introduces the integral probability metric that measures the difference between the
distributions of two domains. We introduce the uniform entropy number for the situation of multiple
sources in Section 4. In Section 5  we present the generalization bounds for domain adaptation with
multiple sources  and then analyze the asymptotic behavior of the learning process for this type of
domain adaptation. The last section concludes the paper. In the supplement (part A)  we discuss the
relationship between the integral probability metric DF (S  T ) and the other quantities proposed in
the existing works including the H-divergence and the discrepancy distance. Proofs of main results
of this paper are provided in the supplement (part B). We study domain adaptation combining source
and target data in the supplement (part C) and then give a comparison with the existing works on the
theoretical analysis of domain adaptation in the supplement (part D).

2 Problem Setup
We denote Z (Sk) := X (Sk)×Y (Sk) ⊂ RI×RJ (1 ≤ k ≤ K) and Z (T ) := X (T )×Y (T ) ⊂ RI×RJ
as the k-th source domain and the target domain  respectively. Set L = I + J. Let D(Sk) and
D(T ) stand for the distributions of the input spaces X (Sk) (1 ≤ k ≤ K) and X (T )  respectively.
Denote g(Sk)
: X (T ) → Y (T ) as the labeling functions of Z (Sk)
(1 ≤ k ≤ K) and Z (T )  respectively. In the situation of domain adaptation with multiple sources 
the distributions D(Sk) (1 ≤ k ≤ K) and D(T ) differ from each other  or g(Sk)
(1 ≤ k ≤ K) and
g(T )∗
differ from each other  or both of the cases occur. There are sufﬁcient amounts of i.i.d. samples

: X (Sk) → Y (Sk) and g(T )∗

∗

∗

1Due to the page limitation  the discussion on domain adaptation combining source and target data is pro-

vided in the supplement (part C).

2Due to the page limitation  we only present the generalization bounds for domain adaptation with multiple
sources and the discussions of the corresponding deviation inequalities and symmetrization inequalities are
provided in the supplement (part B) along with the proofs of main results.

2

n=1 drawn from each source domain Z (Sk) (1 ≤ k ≤ K) but little or no labeled

n }Nk

1 = {z(k)
ZNk
samples drawn from the target domain Z (T ).

Given w = (w1 ···   wK) ∈ [0  1]K with(cid:80)K

the empirical risk

E(S)
w ((cid:96) ◦ g) =

wkE(Sk)
Nk

(cid:90)
over G with respect to sample sets {ZNk
target expected risk:

1 }K

K(cid:88)

k=1

Nk(cid:88)

K(cid:88)

wk
Nk

k=1 wk = 1  let gw ∈ G be the function that minimizes

((cid:96) ◦ g) =
k=1  and it is expected that gw will perform well on the

n )  y(k)
n )

(cid:96)(g(x(k)

n=1

k=1

(1)

(cid:12)(cid:12)E(T )f − E(S)
w f(cid:12)(cid:12).

sup
f∈F

3

E(T )((cid:96) ◦ g) :=
i.e.  gw approximates the labeling g(T )∗
In the learning process of domain adaptation with multiple sources  we are mainly interested in the
following two types of quantities:

(cid:96)(g(x(T ))  y(T ))dP(z(T ))  g ∈ G 
as precisely as possible.

(2)

Recalling (1) and (2)  since

• E(T )((cid:96) ◦ gw) − E(S)
w ((cid:96) ◦ gw)  which corresponds to the estimation of the expected risk in
the target domain Z (T ) from a weighted combination of the empirical risks in the multiple
sources {Z (Sk)}K
k=1;
domain adaptation with multiple sources 

• E(T )((cid:96) ◦ gw) − E(T )((cid:96) ◦(cid:101)g∗)  which corresponds to the performance of the algorithm for

where(cid:101)g∗ ∈ G is the function that minimizes the expected risk E(T )((cid:96) ◦ g) over G.

E(S)

w ((cid:96) ◦ gw) ≥ 0 

w ((cid:96) ◦(cid:101)g∗) − E(S)
w ((cid:96) ◦(cid:101)g∗) − E(S)
w ((cid:96) ◦ gw) + E(T )((cid:96) ◦ gw) − ET ((cid:96) ◦(cid:101)g∗) + ET ((cid:96) ◦(cid:101)g∗)
(cid:12)(cid:12)E(T )((cid:96) ◦ g) − E(S)

E(T )((cid:96) ◦ gw) =E(T )((cid:96) ◦ gw) − E(T )((cid:96) ◦(cid:101)g∗) + E(T )((cid:96) ◦(cid:101)g∗)
(cid:12)(cid:12) + E(T )((cid:96) ◦(cid:101)g∗) 
(cid:12)(cid:12)E(T )((cid:96) ◦ g) − E(S)

w ((cid:96) ◦ g)(cid:12)(cid:12).

≤E(S)
≤2 sup
g∈G

w ((cid:96) ◦ g)

0 ≤ E(T )((cid:96) ◦ gw) − E(T )((cid:96) ◦(cid:101)g∗) ≤ 2 sup
(cid:12)(cid:12)E(T )((cid:96) ◦ g) − E(S)

g∈G

sup
g∈G

This shows that the asymptotic behaviors of the aforementioned two quantities  when the sample
numbers N1 ···   NK go to inﬁnity  can both be described by the supremum

we have

and thus

(cid:12)(cid:12) 

w ((cid:96) ◦ g)

which is the so-called generalization bound of the learning process for domain adaptation with
multiple sources.
For convenience  we deﬁne the loss function class

F := {z (cid:55)→ (cid:96)(g(x)  y) : g ∈ G} 

and call F as the function class in the rest of this paper. By (1) and (2)  given sample sets {ZNk
drawn from the multiple sources {Z (Sk)}K
k=1 respectively  we brieﬂy denote for any f ∈ F 

(cid:90)

E(T )f :=

f (z(T ))dP(z(T )) ; E(S)

w f :=

f (z(k)

n ).

(6)

Nk(cid:88)

K(cid:88)

wk
Nk

k=1

n=1

Thus  we can equivalently rewrite the generalization bound (4) for domain adaptation with multiple
sources as

(3)

(4)

(7)

(5)
1 }K

k=1

3

Integral Probability Metric

and g(T )∗

.

differs from g(T )∗

As shown in some prior works (e.g.
[13  16  17  18  19  20])  one of major challenges in the
theoretical analysis of domain adaptation is how to measure the distance between the source domain
Z (S) and the target domain Z (T ). Recall that  if Z (S) differs from Z (T )  there are three possibilities:
D(S) differs from D(T )  or g(S)∗
  or both of them occur. Therefore  it is necessary
to consider two kinds of distances: the distance between D(S) and D(T ) and the distance between
g(S)∗
In [13  18]  the H-divergence was introduced to derive the generalization bounds based on the VC
dimension under the condition of “λ-close”. Mansour et al. [20] obtained the generalization bounds
based on the Rademacher complexity by using the discrepancy distance. Both quantities are aimed
to measure the difference between two distributions D(S) and D(T ). Moreover  Mansour et al. [17]
used the R´enyi divergence to measure the distance between two distributions. In this paper  we
use the following quantity to measure the difference of the distributions between the source and the
target domains:
Deﬁnition 3.1 Given two domains Z (S) Z (T ) ⊂ RL  let z(S) and z(T ) be the random variables
taking value from Z (S) and Z (T )  respectively. Let F ⊂ RZ be a function class. We deﬁne
(8)

DF (S  T ) := sup

f∈F |E(S)f − E(T )f| 

where the expectations E(S) and E(T ) are taken with respect to the distributions Z (S) and Z (T ) 
respectively.

The quantity DF (S  T ) is termed as the integral probability metric that plays an important role in
probability theory for measuring the difference between the two probability distributions (cf. [23 
24  25  26]). Recently  Sriperumbudur et al. [27] gave the further investigation and proposed the
empirical methods to compute the integral probability metric in practice. As mentioned by M¨uller
[page 432  25]  the quantity DF (S  T ) is a semimetric and it is a metric if and only if the function
class F separates the set of all signed measures with µ(Z) = 0. Namely  according to Deﬁnition
3.1  given a non-trivial function class F  the quantity DF (S  T ) is equal to zero if the domains Z (S)
and Z (T ) have the same distribution.
In the supplement (part A)  we discuss the relationship between the quantity DF (S  T ) and other
quantities proposed in the previous works  and then show that the quantity DF (S  T ) can be bounded
by the summation of the discrepancy distance and another quantity  which measure the difference
between the input-space distributions D(S) and D(T ) and the difference between the labeling func-
tions g(S)∗

  respectively.

and g(T )∗

4 The Uniform Entropy Number

Generally  the generalization bound of a certain learning process is achieved by incorporating the
complexity measure of the function class  e.g.  the covering number  the VC dimension and the
Rademacher complexity. The results of this paper are based on the uniform entropy number that is
derived from the concept of the covering number and we refer to [22] for more details.
The covering number of a function class F is deﬁned as follows:
Deﬁnition 4.1 Let F be a function class and d is a metric on F. For any ξ > 0  the covering
number of F at radius ξ with respect to the metric d  denoted by N (F  ξ  d) is the minimum size of
a cover of radius ξ.

In some classical results of statistical learning theory  the covering number is applied by letting
d be the distribution-dependent metric. For example  as shown in Theorem 2.3 of [22]  one can
learning process
set d as the norm (cid:96)1(ZN
1 ) and then derive the generalization bound of the i.i.d.
by incorporating the expectation of the covering number  i.e.  EN (F  ξ  (cid:96)1(ZN
1 )). However  in
the situation of domain adaptation  we only know the information of source domain  while the
expectation EN (F  ξ  (cid:96)1(ZN
1 )) is dependent on both distributions of source and target domains

4

n }Nk

:= {z(k)

because z = (x  y). Therefore  the covering number is no longer applicable to our framework to
obtain the generalization bounds for domain adaptation. By contrast  uniform entropy number is
distribution-free and thus we choose it as the complexity measure of function class to derive the
generalization bounds for domain adaptation.
For clarity of presentation  we give a useful notation for the following discussion. For any 1 ≤ k ≤
n=1 drawn from Z (Sk)  we denote Z(cid:48)Nk
K  given a sample set ZNk
n=1 as
1
the ghost-sample set drawn from Z (Sk) such that the ghost sample z(cid:48)(k)
n has the same distribution as
1   Z(cid:48)Nk
z(k)
n for any 1 ≤ n ≤ Nk and any 1 ≤ k ≤ K. Denoting Z2Nk
1 }. Moreover  given
:= {ZNk
(cid:17)
k=1 wk = 1  we introduce a variant of the
(cid:96)1 norm:

any f ∈ F and any w = (w1 ···   wK) ∈ [0  1]K with(cid:80)K
(cid:16)
Nk(cid:88)

k=1) :=
}K
It is noteworthy that the variant (cid:96)w
1 of the (cid:96)1 norm is still a norm on the functional space  which
can be easily veriﬁed by using the deﬁnition of norm  so we omit it here. In the situation of domain
k=1)  we then deﬁne the uniform
adaptation with multiple sources  setting the metric d as (cid:96)w
1 ({Z2Nk
}K
(cid:16)
entropy number of F with respect to the metric (cid:96)w
k=1) as
1 ({Z2Nk
}K
F  ξ  (cid:96)w

(cid:1) := sup

n )| + |f (z(cid:48)(k)
n )|

:= {z(cid:48)(k)

1 ({Z2Nk

|f (z(k)

K(cid:88)

K(cid:88)

n }Nk

lnN w

F  ξ  2

}K
k=1)

(cid:107)f(cid:107)(cid:96)w

lnN

wk
Nk

1 ({Z

2Nk
1

(cid:17)

(10)

(cid:0)

(9)

Nk

k=1

n=1

1

1

.

1

.

1

1

1

k=1

{Z

2Nk
1

}K

k=1

5 Domain Adaptation with Multiple Sources

In this section  we present the generalization bound for domain adaptation with multiple sources.
Based on the resultant bound  we then analyze the asymptotic convergence and the rate of conver-
gence of the learning process for such kind of domain adaptation.

5.1 Generalization Bounds for Domain Adaptation with Multiple Sources

Based on the aforementioned uniform entropy number  the generalization bound for domain adap-
tation with multiple sources is presented in the following theorem:
Theorem 5.1 Assume that F is a function class consisting of the bounded functions with the range
k=1 wk = 1. Then  given an arbitrary ξ >
(ξ(cid:48))2 and any  > 0  with probability at least 1 −  

[a  b]. Let w = (w1 ···   wK) ∈ [0  1]K with (cid:80)K
D(w)F (S  T )  we have for any(cid:0)(cid:81)K
≥ 8(b−a)2
(cid:16)
(cid:0)
F  ξ(cid:48)/8  2(cid:80)K
(cid:0)(cid:81)K
32(b−a)2(cid:0)(cid:80)K

(cid:1)
(cid:12)(cid:12)E(S)
w f − E(T )f(cid:12)(cid:12) ≤ D(w)F (S  T ) +

1
2

 

(11)

− ln(/8)





lnN w

k=1 Nk

k=1 Nk

sup
f∈F

(cid:17)

(cid:1)

(cid:1)

(cid:1)
k((cid:81)

k=1 Nk
k=1 w2

1

i(cid:54)=k Ni)

where ξ(cid:48) = ξ − D(w)F (S  T ) and

D(w)F (S  T ) :=

K(cid:88)

k=1

wkDF (Sk  T ).

(12)

In the above theorem  we show that the generalization bound supf∈F |E(T )f − E(S)
w f| can be
bounded by the right-hand side of (11). Compared to the classical result under the assumption
of same distribution (cf. Theorem 2.3 and Deﬁnition 2.5 of [22]): with probability at least 1 −  

(cid:12)(cid:12)EN f − Ef

(cid:12)(cid:12) ≤ O

sup
f∈F

(cid:32)

(cid:0)

F  ξ  N(cid:1)

N

2
(cid:33) 1

− ln(/8)

(13)

lnN1

5

with EN f being the empirical risk with respect to the sample set ZN
1   there is a discrepancy quantity
D(w)F (S  T ) that is determined by the two factors: the choice of w and the quantities DF (Sk  T )
(1 ≤ k ≤ K). The two results will coincide if any source domain and the target domain match  i.e. 
DF (Sk  T ) = 0 holds for any 1 ≤ k ≤ K.
In order to prove this result  we develop the related Hoeffding-type deviation inequality and the
symmetrization inequality for domain adaptation with multiple sources  respectively. The detailed
proof is provided in the supplement (part B). By using the resultant bound (11)  we can analyze the
asymptotic behavior of the learning process for domain adaptation with multiple sources.

5.2 Asymptotic Convergence

In statistical learning theory  it is well-known that the complexity of the function class is the main
factor to the asymptotic convergence of the learning process under the assumption of same distribu-
tion (cf. [3  4  22]).
Theorem 5.1 can directly lead to the concerning theorem showing that the asymptotic convergence
of the learning process for domain adaptation with multiple sources:
Theorem 5.2 Assume that F is a function class consisting of bounded functions with the range

k=1 wk = 1. If the following condition holds:

1

lim

N1 ···  NK→+∞

[a  b]. Let w = (w1 ···   wK) ∈ [0  1]K with(cid:80)K
(cid:0)
F  ξ(cid:48)/8  2(cid:80)K
(cid:0)(cid:81)K
(cid:1)
32(b−a)2(cid:0)(cid:80)K
lnN w
k((cid:81)
i(cid:54)=k Ni)
(cid:110)
w f(cid:12)(cid:12) > ξ
(cid:12)(cid:12)E(T )f − E(S)
with ξ(cid:48) = ξ − D(w)F (S  T )  then we have for any ξ > D(w)F (S  T ) 
(cid:1) satisfy the condition (14) with(cid:80)K

N1 ···  NK→+∞ Pr

F  ξ(cid:48)/8  2(cid:80)K

k=1 Nk
k=1 w2

k=1 Nk

sup
f∈F

(cid:0)

lim

(cid:1)
(cid:1) < +∞
(cid:111)

= 0.

1

if the choice of w ∈ [0  1]K and the uniform entropy number
k=1 wk = 1  the probability of the

As shown in Theorem 5.2 
lnN w
k=1 Nk
event that “supf∈F
the sample numbers N1 ···   NK of multiple sources go to inﬁnity  respectively. This is partially in
accordance with the classical result of the asymptotic convergence of the learning process under the
assumption of same distribution (cf. Theorem 2.3 and Deﬁnition 2.5 of [22]): the probability of the
event that “supf∈F
number lnN1 (F  ξ  N ) satisﬁes the following:

(cid:12)(cid:12) > ξ” will converge to zero for any ξ > D(w)F (S  T )  when
(cid:12)(cid:12)E(T )f − E(S)
(cid:12)(cid:12)Ef − EN f(cid:12)(cid:12) > ξ” will converge to zero for any ξ > 0  if the uniform entropy

w f

(14)

(15)

lim

N→+∞

lnN1 (F  ξ  N )

N

< +∞.

(16)

Note that in the learning process of domain adaptation with multiple sources  the uniform conver-
gence of the empirical risk on the source domains to the expected risk on the target domain may not
hold  because the limit (15) does not hold for any ξ > 0 but for any ξ > D(w)F (S  T ). By contrast 
the limit (15) holds for all ξ > 0 in the learning process under the assumption of same distribution 
if the condition (16) is satisﬁed.
By Cauchy-Schwarz inequality  setting wk = Nk(cid:80)K

(1 ≤ k ≤ K) minimizes the second term of

k=1 Nk

the right-hand side of (11) and then we arrive at

and our numerical experiments presented in the next section also support this point (cf. Fig. 1).

k=1 Nk

(1 ≤ k ≤ K) can result in the fastest rate of convergence

6

(cid:12)(cid:12)E(S)
w f − E(T )f(cid:12)(cid:12)
(cid:80)K

sup
f∈F

(cid:80)K

≤

k=1 NkDF (Sk  T )
which implies that setting wk = Nk(cid:80)K

k=1 Nk

+

 (lnN w

1 (F  ξ(cid:48)/8  2(cid:80)K
(cid:0)(cid:80)K

(cid:1)

k=1 Nk
32(b−a)2

k=1 Nk) − ln(/8)

 1

2

 

(17)

6 Numerical Experiments

n }NT

We have performed the numerical experiments to verify the theoretic analysis of the asymptotic
convergence of the learning process for domain adaptation with multiple sources. Without loss of
generality  we only consider the case of K = 2  i.e.  there are two source domains and one target
domain. The experiment data are generated in the following way:
For the target domain Z (T ) = X (T )×Y (T ) ⊂ R100×R  we consider X (T ) as a Gaussian distribution
n=1 (NT = 4000) from X (T ) randomly and independently. Let β ∈ R100
N (0  1) and draw {x(T )
be a random vector of a Gaussian distribution N (1  5)  and let the random vector R ∈ R100 be a
noise term with R ∼ N (0  0.5). For any 1 ≤ n ≤ NT   we randomly draw β and R from N (1  5)
and N (0  0.01) respectively  and then generate y(T )
n = (cid:104)x(T )
y(T )

n ∈ Y as follows:
n   β(cid:105) + R.

(18)
n=1 (NT = 4000) are the samples of the target domain Z (T ) and will be
n=1 (N1 = 2000) of the source domain

The derived {(x(T )
used as the test data.
In the similar way  we derive the sample set {(x(1)
n   y(1)
Z (S1) = X (1) × Y (1) ⊂ R100 × R: for any 1 ≤ n ≤ N1 

n )}NT

n )}N1

n   y(T )

y(1)
n = (cid:104)x(1)

n   β(cid:105) + R 
n ∼ N (0.5  1)  β ∼ N (1  5) and R ∼ N (0  0.5).

where x(1)
For the source domain Z (S2) = X (2) × Y (2) ⊂ R100 × R  the samples {(x(2)
2000) are generated in the following way: for any 1 ≤ n ≤ N2 

n   y(2)

n )}N2

n=1 (N2 =

(19)

(20)

n = (cid:104)x(2)
y(2)

n   β(cid:105) + R 

N2(cid:88)

n=1

N1(cid:88)

n=1

where x(2)
In this experiment  we use the method of Least Square Regression to minimize the empirical risk

n ∼ N (2  5)  β ∼ N (1  5) and R ∼ N (0  0.5).

E(S)
w ((cid:96) ◦ g) =

w
N1

(cid:96)(g(x(1)

n )  y(1)

n ) +

(1 − w)

N2

(cid:96)(g(x(2)

n )  y(2)
n )

(21)

NT

w f − E(T )

for different combination coefﬁcients w ∈ {0.1  0.3  0.5  0.9} and then compute the discrepancy
|E(S)
f| for each N1 + N2. The initial N1 and N2 both equal to 200. Each test is repeated
30 times and the ﬁnal result is the average of the 30 results. After each test  we increment both N1
and N2 by 200 until N1 = N2 = 2000. The experiment results are shown in Fig. 1.
w f − E(T )
From Fig. 1  we can observe that for any choice of w  the curve of |E(S)
f| is decreasing
when N1 + N2 increases  which is in accordance with the results presented in Theorems 5.1 & 5.2.
Moreover  when w = 0.5  the discrepancy |E(S)
f| has the fastest rate of convergence  and
the rate becomes slower as w is further away from 0.5. In this experiment  we set N1 = N2 that
implies that N2/(N1 + N2) = 0.5. Recalling (17)  we have shown that w = N2/(N1 + N2) will
provide the fastest rate of convergence and this proposition is supported by the experiment results
shown in Fig. 1.

w f − E(T )

NT

NT

7 Conclusion

In this paper  we present a new framework to study the generalization bounds of the learning process
for domain adaptation. We use the integral probability metric to measure the difference between the
distributions of two domains. Then  we use a martingale method to develop the speciﬁc deviation
inequality and the symmetrization inequality incorporating the integral probability metric. Next  we
utilize the resultant deviation inequality and symmetrization inequality to derive the generalization
bound based on the uniform entropy number. By using the resultant generalization bound  we an-
alyze the asymptotic convergence and the rate of convergence of the learning process for domain
adaptation.

7

Figure 1: Domain Adaptation with Multiple Sources

We point out that the asymptotic convergence of the learning process is determined by the complex-
ity of the function class F measured by the uniform entropy number. This is partially in accordance
with the classical result of the asymptotic convergence of the learning process under the assumption
of same distribution (cf. Theorem 2.3 and Deﬁnition 2.5 of [22]). Moreover  the rate of convergence
of this learning process is equal to that of the learning process under the assumption of same dis-
tribution. The numerical experiments support our results. Finally  we give a comparison with the
previous works [13  14  15  16  17  18  20] (cf. supplement  part D).
It is noteworthy that by Theorem 2.18 of [22]  the generalization bound (11) can lead to the result
based on the fat-shattering dimension. According to Theorem 2.6.4 of [4]  the bound based on the
VC dimension can also be obtained from the result (11). In our future work  we will attempt to
ﬁnd a new distance between two distributions and develop the generalization bounds based on other
complexity measures  e.g.  Rademacher complexities  and analyze other theoretical properties of
domain adaptation.

Acknowledgments

This research is sponsored in part by NSF (IIS-0953662  CCF-1025177)  NIH (LM010730)  and
ONR (N00014-1-1-0108).

References

Introduction to Statistical Learning Theory.

[1] V.N. Vapnik (1999). An overview of statistical learning theory. IEEE Transactions on Neural Networks
10(5):988-999.
[2] O. Bousquet  S. Boucheron  and G. Lugosi (2004).
Bousquet et al. (ed.)  Advanced Lectures on Machine Learning  169-207.
[3] V.N. Vapnik (1998). Statistical Learning Theory. New York: John Wiley and Sons.
[4] A. Blumer  A. Ehrenfeucht  D. Haussler  and M. K. Warmuth (1989). Learnability and the Vapnik-
Chervonenkis dimension. Journal of the ACM 36(4):929-965.
[5] A. van der Vaart  and J. Wellner (2000). Weak Convergence and Empirical Processes With Applications to
Statistics (Hardcover). Springer.
[6] P.L. Bartlett  O. Bousquet  and S. Mendelson (2005). Local Rademacher Complexities. Annals of Statistics
33:1497-1537.
[7] Z. Hussain  and J. Shawe-Taylor (2011). Improved Loss Bounds for Multiple Kernel Learning. Journal of
Machine Learning Research - Proceedings Track 15:370-377.

In O.

8

50010001500200025003000350040000.250.30.350.40.450.5N1+N2|E(S)wf−E(T)NTf| w=0.1w=0.3w=0.5w=0.9[8] J. Jiang  and C. Zhai (2007). Instance Weighting for Domain Adaptation in NLP. Proceedings of the 45th
Annual Meeting of the Association of Computational Linguistics (ACL)  264-271.
[9] J. Blitzer  M. Dredze  and F. Pereira (2007). Biographies  bollywood  boomboxes and blenders: Domain
adaptation for sentiment classiﬁcation. Proceedings of the 45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL)  440-447.
[10] S. Bickel  M. Br¨uckner  and T. Scheffer (2007). Discriminative learning for differing training and test
distributions. Proceedings of the 24th international conference on Machine learning (ICML)  81-88.
[11] P. Wu  and T.G. Dietterich (2004). Improving SVM accuracy by training on auxiliary data sources. Pro-
ceedings of the twenty-ﬁrst international conference on Machine learning (ICML)  871-878.
[12] J. Blitzer  R. McDonald  and F. Pereira (2006). Domain adaptation with structural correspondence learning.
Conference on Empirical Methods in Natural Language Processing (EMNLP)  120-128.
[13] S. Ben-David  J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman (2010). A Theory of
Learning from Different Domains. Machine Learning 79:151-175.
[14] K. Crammer  M. Kearns  and J. Wortman (2006). Learning from Multiple Sources. Advances in Neural
Information Processing Systems (NIPS).
[15] K. Crammer  M. Kearns  and J. Wortman (2008). Learning from Multiple Sources. Journal of Machine
Learning Research 9:1757-1774.
[16] Y. Mansour  M. Mohri  and A. Rostamizadeh (2008). Domain adaptation with multiple sources. Advances
in Neural Information Processing Systems (NIPS)  1041-1048.
[17] Y. Mansour  M. Mohri  and A. Rostamizadeh (2009). Multiple Source Adaptation and The R´enyi Diver-
gence. Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
[18] J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman (2007). Learning Bounds for Domain
Adaptation. Advances in Neural Information Processing Systems (NIPS).
[19] S. Ben-David  J. Blitzer  K. Crammer  and F. Pereira  F (2006). Analysis of Representations for Domain
Adaptation. Advances in Neural Information Processing Systems (NIPS)  137-144.
[20] Y. Mansour  M. Mohri  and A. Rostamizadeh (2009). Domain Adaptation: Learning Bounds and Algo-
rithms. Conference on Learning Theory (COLT).
[21] W. Hoeffding (1963). Probability Inequalities for Sums of Bounded Random Variables. Journal of the
American Statistical Association 58(301):13-30.
[22] S. Mendelson (2003). A Few Notes on Statistical Learning Theory. Lecture Notes in Computer Science
2600:1-40.
[23] V.M. Zolotarev (1984). Probability Metrics. Theory of Probability and its Application 28(1):278-302.
[24] S.T. Rachev (1991). Probability Metrics and the Stability of Stochastic Models. John Wiley and Sons.
[25] A. M¨uller (1997). Integral Probability Metrics and Their Generating Classes of Functions. Advances in
Applied Probability 29(2):429-443.
[26] M.D. Reid and R.C. Williamson (2011). Information  Divergence and Risk for Binary Experiments. Jour-
nal of Machine Learning Research 12:731-817.
[27] B.K. Sriperumbudur  A. Gretton  K. Fukumizu  G.R.G. Lanckriet and B. Sch¨olkopf (2009). A Note on
Integral Probability Metrics and φ-Divergences. CoRR abs/0901.2698.

9

,Shahin Jabbari
Ryan Rogers
Aaron Roth
Steven Wu