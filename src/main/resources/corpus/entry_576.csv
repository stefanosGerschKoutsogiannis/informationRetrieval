2016,Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods,The well known maximum-entropy principle due to Jaynes  which states that given mean parameters  the maximum entropy distribution matching them is in an exponential family has been very popular in machine learning due to its “Occam’s razor” interpretation. Unfortunately  calculating the potentials in the maximum entropy distribution is intractable [BGS14]. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments  while having entropy which is comparable to the maximum entropy distribution matching those moments.  We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely  we show that we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit  which is the setting of optimization of quadratic forms over the hypercube. ([AN06]),Approximate maximum entropy principles via

Goemans-Williamson with applications to provable

variational methods

Yuanzhi Li

Princeton University
Princeton  NJ  08450

Andrej Risteski

Princeton University
Princeton  NJ  08450

Department of Computer Science

Department of Computer Science

yuanzhil@cs.princeton.edu

risteski@cs.princeton.edu

Abstract

The well known maximum-entropy principle due to Jaynes  which states that
given mean parameters  the maximum entropy distribution matching them is in an
exponential family has been very popular in machine learning due to its “Occam’s
razor” interpretation. Unfortunately  calculating the potentials in the maximum-
entropy distribution is intractable [BGS14]. We provide computationally efﬁcient
versions of this principle when the mean parameters are pairwise moments: we
design distributions that approximately match given pairwise moments  while
having entropy which is comparable to the maximum entropy distribution matching
those moments.
We additionally provide surprising applications of the approximate maximum
entropy principle to designing provable variational methods for partition function
calculations for Ising models without any assumptions on the potentials of the
model. More precisely  we show that we can get approximation guarantees for the
log-partition function comparable to those in the low-temperature limit  which is
the setting of optimization of quadratic forms over the hypercube. ([AN06])

1

Introduction

i.e. µ(x) ∝ exp((cid:80)T

Maximum entropy principle The maximum entropy principle [Jay57] states that given mean pa-
rameters  i.e. Eµ[φt(x)] for a family of functionals φt(x)  t ∈ [1  T ]  where µ is distribution over the
hypercube {−1  1}n  the entropy-maximizing distribution µ is an exponential family distribution 
t=1 Jtφt(x)) for some potentials Jt  t ∈ [1  T ]. 1 This principle has been one
of the reasons for the popularity of graphical models in machine learning: the “maximum entropy”
assumption is interpreted as “minimal assumptions” on the distribution other than what is known
about it.
However  this principle is problematic from a computational point of view. Due to results of
[BGS14  SV14]  the potentials Jt of the Ising model  in many cases  are impossible to estimate well
in polynomial time  unless NP = RP – so merely getting the description of the maximum entropy
distribution is already hard. Moreover  in order to extract useful information about this distribution 
usually we would also like to at least be able to sample efﬁciently from this distribution – which is
typically NP-hard or even #P-hard.

1There is a more general way to state this principle over an arbitrary domain  not just the hypercube  but for

clarity in this paper we will focus on the hypercube only.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In this paper we address this problem in certain cases. We provide a “bi-criteria” approximation
for the special case where the functionals φt(x) are φi j(x) = xixj  i.e. pairwise moments: we
produce a efﬁciently sampleable distribution over the hypercube which matches these moments up
to multiplicative constant factors  and has entropy at most a constant factor smaller from from the
entropy of the maximum entropy distribution. 2
Furthermore  the distribution which achieves this is very natural: the sign of a multivariate normal
variable. This provides theoretical explanation for the phenomenon observed by the computational
neuroscience community [BB07] that this distribution (there named dichotomized Gaussian there)
has near-maximum entropy.
Variational methods The above results also allow us to get results for a seemingly unrelated problem
t=1 Jtφt(x)) of a member of an

– approximating the partition function Z = (cid:80)

x∈{−1 1}n exp((cid:80)T

exponential family. The reason this task is important is that it is tied to calculating marginals.
One of the ways this task is solved is variational methods: namely  expressing log Z as an optimization
problem. While there is a plethora of work on variational methods  of many ﬂavors (mean ﬁeld 
Bethe/Kikuchi relaxations  TRBP  etc. for a survey  see [WJ08])  they typically come either with no
guarantees  or with guarantees in very constrained cases (e.g. loopless graphs; graphs with large girth 
etc. [WJW03  WJW05]). While this is a rich area of research  the following extremely basic research
question has not been answered:
What is the best approximation guarantee on the partition function in the worst case (with no
additional assumptions on the potentials)?

In the low-temperature limit  i.e. when |Jt| → ∞  log Z → maxx∈{−1 1}n(cid:80)T

t=1 Jtφt(x) - i.e. the
question reduces to purely optimization. In this regime  this question has very satisfying answers
for many families φt(x). One classical example is when the functionals are φi j(x) = xixj. In the
graphical model community  these are known as Ising models  and in the optimization community this
is the problem of optimizing quadratic forms and has been studied by [CW04  AN06  AMMN06].
In the optimization version  the previous papers showed that in the worst case  one can get O(log n)
factor multiplicative factor approximation of it  and that unless P = NP  one cannot get better than
constant factor approximations of it.
In the ﬁnite-temperature version  it is known that it is NP-hard to achieve a 1 +  factor approximation
to the partition function (i.e. construct a FPRAS) [SS12]  but nothing is known about coarser
approximations. We prove in this paper  informally  that one can get comparable multiplicative
guarantees on the log-partition function in the ﬁnite temperature case as well – using the tools and
insights we develop on the maximum entropy principles.
Our methods are extremely generic  and likely to apply to many other exponential families  where
algorithms based on linear/semideﬁnite programming relaxations are known to give good guarantees
in the optimization regime.

2 Statements of results and prior work

Approximate maximum entropy The main theorem in this section is the following one.
Theorem 2.1. For any covariance matrix Σ of a centered distribution µ : {−1  1}n → R  i.e.
Eµ[xixj] = Σi j  Eµ[xi] = 0  there is an efﬁciently sampleable distribution ˜µ  which can be sampled
as sign(g)  where g ∼ N (0  Σ + βI) and satisﬁes
Σi j and has
entropy H(˜µ) ≥ n

Σi j ≤ E˜µ[XiXj] ≤ 1
1 + β

G
1 + β

(31/4√
√

β−1)2
3β

  for any β ≥ 1

31/2 .

25

There are two prior works on computational issues relating to maximum entropy principles  both
proving hardness results.
[BGS14] considers the “hard-core” model where the functionals φt are such that the distribution µ(x)
puts zero mass on conﬁgurations x which are not independent sets with respect to some graph G.

2In fact  we produce a distribution with entropy Ω(n)  which implies the latter claim since the maximum

entropy of any distribution of over {−1  1}n is at most n

2

They show that unless NP = RP  there is no FPRAS for calculating the potentials Jt  given the mean
parameters Eµ[φt(x)].
[SV14] prove an equivalence between calculating the mean parameters and calculating partition
functions. More precisely  they show that given an oracle that can calculate the mean parameters up
to a (1 + ) multiplicative factor in time O(poly(1/))  one can calculate the partition function of the
same exponential family up to (1 + O(poly())) multiplicative factor  in time O(poly(1/)). Note 
the  in this work potentially needs to be polynomially small in n (i.e. an oracle that can calculate the
mean parameters to a ﬁxed multiplicative constant cannot be used.)
Both results prove hardness for ﬁne-grained approximations to the maximum entropy principle  and
ask for outputting approximations to the mean parameters. Our result circumvents these hardness
results by providing a distribution which is not in the maximum-entropy exponential family  and is
allowed to only approximately match the moments as well. To the best of our knowledge  such an
approximation  while very natural  has not been considered in the literature.
Provable variational methods The main theorems in this section will concern the approximation
factor that can be achieved by degree-2 pseudo-moment relaxations of the standard variational
principle due to Gibbs. ([Ell12]) As outlined before  we will be concerned with a particularly popular
exponential family: Ising models. We will prove the following three results:
Theorem 2.2 (Ferromagnetic Ising  informal). There is a convex programming relaxation based on
degree-2 pseudo-moments that calculates up to multiplicative approximation factor 50 the value of
log Z where Z is the partition function of the exponential distribution µ(x) ∝ exp(
Ji jxixj) for

(cid:88)

Ji j > 0.
Theorem 2.3 (Ising model  informal). There is a convex programming relaxation based on degree-2
pseudo-moments that calculates up to multiplicative approximation factor O(log n) the value of
log Z where Z is the partition function of the exponential distribution µ(x) ∝ exp(
Ji jxixj).

i j

(cid:88)
(cid:88)

i j

i j∈E(G)

Theorem 2.4 (Ising model  informal). There is a convex programming relaxation based on degree-2
pseudo-moments that calculates up to multiplicative approximation factor O(log χ(G)) the value of
log Z where Z is the partition function of the exponential distribution µ(x) ∝ exp(
Ji jxixj)

where G = (V (G)  E(G)) is a graph with chromatic number χ(G). 3

While a lot of work is done on variational methods in general (see the survey by [WJ08] for a detailed
overview)  to the best of our knowledge nothing is known about the worst-case guarantee that we
are interested in here. Moreover  other than a recent paper by [Ris16]  no other work has provided
provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof.4
[Ris16] provides guarantees in the case of Ising models that are also based on pseudo-moment
relaxations of the variational principle  albeit only in the special case when the graph is “dense” in a
suitably deﬁned sense. 5 The results there are very speciﬁc to the density assumption and can not be
adapted to our worst-case setting.
Finally  we mention that in the special case of the ferromagnetic Ising models  an algorithm based on
MCMC was provided by [JS93]  which can give an approximation factor of (1 + ) to the partition
function and runs in time O(n11poly(1/)). In spite of this  the focus of this part of our paper is
to provide understanding of variational methods in certain cases  as they continue to be popular in
practice for their faster running time compared to MCMC-based methods but are theoretically much
more poorly studied.

3Theorem 2.4 is strictly more general than Theorem 2.3  however the proof of Theorem 2.3 uses less heavy

machinery and is illuminating enough that we feel merits being presented as a separate theorem.

4In some sense  it is possible to give provable bounds for Bethe-entropy based relaxations  via analyzing
(cid:80)
belief propagation directly  which has been done in cases where there is correlation decay and the graph is locally
tree-like. [WJ08] has a detailed overview of such results.
i j |Ji j|  one can get an additive

5More precisely  they prove that in the case when ∀i  j  ∆|Ji j| ≤ ∆
i j Ji j) approximation to log Z in time nO( ∆
2 ).

((cid:80)

n2

3

3 Approximate maximum entropy principles

Let us recall what the problem we want to solve:
Approximate maximum entropy principles We are given a positive-semideﬁnite matrix Σ ∈ Rn×n
with Σi i = 1 ∀i ∈ [n]  which is the covariance matrix of a centered distribution over {−1  1}n 
i.e. Eµ[xixj] = Σi j  Eµ[xi] = 0  for a distribution µ : {−1  1}n → R. We wish to produce a
distribution ˜µ : {−1  1}n → R with pairwise covariances that match the given ones up to constant
factors  and entropy within a constant factor of the maximum entropy distribution with covariance Σ.
6

(cid:8) 2
π arcsin(t)/t(cid:9) ≈ 0.64.

Before stating the result formally  it will be useful to deﬁne the following constant:
Deﬁnition 3.1. Deﬁne the constant G = mint∈[−1 1]
We will prove the following main theorem:
Theorem 3.1 (Main  approximate entropy principle). For any positive-semideﬁnite matrix Σ with
Σi i = 1 ∀i  there is an efﬁciently sampleable distribution ˜µ : {−1  1}n → R  which can be sampled
as sign(g)  where g ∼ N (0  Σ + βI)  and satisﬁes G
1+β Σi j and has entropy
H(˜µ) ≥ n

1+β Σi j ≤ E˜µ[xixj] ≤ 1

  where β ≥ 1

(31/4√
√

β−1)2
3β

25

31/2 .

Note ˜µ is in fact very close to the the one which is classically used to round semideﬁnite relaxations
for solving the MAX-CUT problem. [GW95] We will prove Theorem 3.1 in two parts – by ﬁrst lower
bounding the entropy of ˜µ  and then by bounding the moments of ˜µ.
Theorem 3.2. The entropy of the distribution ˜µ satisﬁes H(˜µ) ≥ n

31/2 .
Proof. A sample g from N (0  ˜Σ) can be produced by sampling g1 ∼ N (0  Σ)  g2 ∼ N (0  βI) and
setting g = g1 + g2. The sum of two multivariate normals is again a multivariate normal. Furthermore 
the mean of g is 0  and since g1  g2 are independent  the covariance of g is Σ + βI = ˜Σ.
Let’s denote the random variable Y = sign(g1 + g2) which is distributed according to ˜µ. We wish
to lower bound the entropy of Y. Toward that goal  denote the random variable S := {i ∈ [n] :
|(g1)i| ≤ cD} for c  D to be chosen. Then  we have: for γ = c−1
c  

when β ≥ 1

(31/4√
√

β−1)2
3β

25

(cid:88)

Pr[S = S]H(Y|S = S) ≥ (cid:88)

Pr[S = S]H(Y|S = S)

H(Y) ≥ H(Y|S) =

S⊆[n]

S⊆[n] |S|≥γn

S⊆[n] |S|≥γn
= Pr [|S| ≥ γn]

H(Y|S = S)

where the ﬁrst inequality follows since conditioning doesn’t decrease entropy  and the latter by the
non-negativity of entropy. Continue the calculation we can get:

(cid:88)

Pr[S = S]H(Y|S = S) ≥ (cid:88)

Pr[S = S]

min

S⊆[n] |S|≥γn

H(Y|S = S)

S⊆[n] |S|≥γn

We will lower bound Pr[|S| ≥ γn] ﬁrst. Notice that E[(cid:80)n
. On the other hand  if(cid:80)n

(cid:35)
i ≥ Dn

(cid:34) n(cid:88)

(g1)2

i=1(g1)2

min

S⊆[n] |S|≥γn

i=1

≤ 1
D
c   which means that |{i : (g1)2

inequality  Pr
cD}| ≤ n
this means Pr [|S| ≥ γn] ≥ 1 − 1
D
It remains to lower bound minS⊆[n] |S|≥γn H(Y|S = S). For every S ⊆ [n] |S| ≥ γn  denote by
YS the coordinates of Y restricted to S  we get

c = γn. Putting things together 

i ≤ cD}| ≥ n − n

c = (c−1)n

i=1(g1)2

.

i ] = n  therefore by Markov’s
i ≥

i ≤ Dn  then |{i : (g1)2

H(Y|S = S) ≥ H(YS|S = S) ≥ H∞(YS|S = S) = − log(max

Pr[YS = yS|S = S])

6Note for a distribution over {−1  1}n  the maximal entropy a distribution can have is n  which is achieved

by the uniform distribution.

4

yS

(where H∞ is the min-entropy) so we only need to bound maxyS Pr[YS = yS|S = S]
We will now  for any yS  upper bound Pr[YS = yS|S = S]. Recall that the event S = S implies that
∀i ∈ S  |(g1)i| ≤ cD. Since g2 is independent of g1  we know that for every ﬁxed g ∈ Rn:

Pr[YS = yS|S = S  g1 = g] = Πi∈S Pr[sign([g]i + [g2]i) = yi]

For a ﬁxed i ∈ [S]  consider the term Pr[sign([g]i + [g2]i) = yi]. Without loss of generality  let’s
assume [g]i > 0 (the proof is completely symmetric in the other case). Then  since [g]i is positive
and g2 has mean 0  we have Pr[[g]i + (g2)i < 0] ≤ 1
2
Moreover 

.

Pr [[g]i + [g2]i > 0] = Pr[[g2]i > 0] Pr [[g]i + [g2]i > 0 | [g2]i > 0]

+ Pr[[g2]i < 0] Pr [[g]i + [g2]i > 0 | [g2]i < 0]

The ﬁrst term is upper bounded by 1
standard Gaussian tail bounds:

2 since Pr[[g2]i > 0] ≤ 1

2. The second term we will bound using

Pr [[g]i + [g2]i > 0 | [g2]i < 0] ≤ Pr [|[g2]i| ≤ |[g]i| | [g2]i < 0]

= Pr[|[g2]i| ≤ |[g]i|] ≤ Pr[([g2]i)2 ≤ cD]
= 1 − Pr[([g2]i)2 > cD]
≤ 1 − 2√
2π

exp (−cD/2β)

−

(cid:32)(cid:114)

β
cD

β
cD

(cid:114)
(cid:32)(cid:114)

(cid:114)
(cid:114)

β
cD

(cid:32)(cid:114)

−

(cid:33)3
(cid:33)3(cid:33)(cid:33)

β
cD

(cid:32)(cid:114)

−

β
cD

β
cD

(cid:32)(cid:114)
(cid:33)3
(cid:33)3γn
(cid:33)3
(cid:32)(cid:114)

β
cD

β
cD

which implies

Pr[[g2]i < 0] Pr[[g]i + [g2]i > 0 | [g2]i < 0] ≤ 1
2

Putting together  we have

(cid:32)

1 − 2√
2π

exp (−cD/2β)

Pr[sign((g1)i + (g2)i) = yi] ≤ 1 − 1√
2π

exp (−cD/2β)

−

β
cD

Together with the fact that |S| ≥ γn we get

Pr[YS = yS|S = s  g1 = g] ≤

exp (−cD/2β)

1 − 1√
1 − 1√

log

2π

(cid:114)

−

β
cD

exp (−cD/2β)

2π

(cid:18)

(cid:19) (c − 1)n

c

which implies that

H(Y) ≥ −

1 − 1
D

By setting c = D = 31/4√
H(Y) ≥ n

(31/4√
√

β−1)2
3β

25

  as we need.

β and a straightforward (albeit unpleasant) calculation  we can check that

We next show that the moments of the distribution are preserved up to a constant G
Lemma 3.1. The distribution ˜µ has G

1+β Σi j ≤ E˜µ[XiXj] ≤ 1

1+β Σi j

1+β .

5

Proof. Consider the Gram decomposition of ˜Σi j = (cid:104)vi  vj(cid:105). Then  N (0  ˜Σ) is in distribution equal
to (sign((cid:104)v1  s(cid:105))  . . .   sign((cid:104)vn  s(cid:105))) where s ∼ N (0  I). Similarly as in the analysis of Goemans-
Williamson [GW95]  if ¯vi = 1(cid:107)vi(cid:107) vi  we have G(cid:104)¯vi  ¯vj(cid:105) ≤ E˜µ[XiXj] =
arcsin((cid:104)¯vi  ¯vj(cid:105)) ≤
(cid:113)
(cid:104)¯vi  ¯vj(cid:105). However  since (cid:104)¯vi  ¯vj(cid:105) =
(cid:107)vi(cid:107)(cid:107)vj(cid:107) Σi j and (cid:107)vi(cid:107) =

˜Σi i =(cid:112)1 + β ∀i ∈ [1  n]  we get that

(cid:107)vi(cid:107)(cid:107)vj(cid:107) ˜Σi j =
Σi j ≤ E˜µ[XiXj] ≤ 1
1 + β

1

(cid:107)vi(cid:107)(cid:107)vj(cid:107)(cid:104)vi  vj(cid:105) =

Σi j as we want.

G
1 + β

2
π
1

1

Lemma 3.2 and 3.1 together imply Theorem 3.1.

4 Provable bounds for variational methods

We will in this section consider applications of the approximate maximum entropy principles we
developed for calculating partition functions of Ising models. Before we dive into the results  we give
brief preliminaries on variational methods and pseudo-moment convex relaxations.
Preliminaries on variational methods and pseudo-moment convex relaxations Recall  varia-
tional methods are based on the following simple lemma  which characterizes log Z as the solution
of an optimization problem. It essentially dates back to Gibbs [Ell12]  who used it in the context of
statistical mechanics  though it has been rediscovered by machine learning researchers [WJ08]:
Lemma 4.1 (Variational characterization of log Z). Let us denote by M the polytope of distributions
over {−1  1}n. Then 

log Z = max
µ∈M

JtEµ[φt(x)] + H(µ)

(1)

(cid:40)(cid:88)

t

(cid:41)

(cid:41)

While the above lemma reduces calculating log Z to an optimization problem  optimizing over
the polytope M is impossible in polynomial time. We will proceed in a way which is natural for
optimization problems – by instead optimizing over a relaxation M(cid:48) of that polytope.
The relaxation will be associated with the degree-2 Lasserre hierarchy. Intuitively  M(cid:48) has as
variables tentative pairwise moments of a distribution of {−1  1}n  and it imposes all constraints on
the moments that hold for distributions over {−1  1}n. To deﬁne M(cid:48) more precisely we will need
the following notion: (for a more in-depth review of moment-based convex hierarchies  the reader
can consult [BKS14])
Deﬁnition 4.1. A degree-2 pseudo-moment 7 ˜Eν[·] is a linear operator mapping polynomials of
degree 2 to R  such that ˜Eν[x2
We will be optimizing over the polytope M(cid:48) of all degree-2 pseudo-moments  i.e. we will consider
solving

i ] = 1  and ˜Eν[p(x)2] ≥ 0 for any polynomial p(x) of degree 1.

(cid:40)(cid:88)

t

max

˜Eν [·]∈M(cid:48)

˜Eν[φt(x)] + ˜H(˜Eν[·])

Jt

where ˜H will be a proxy for the entropy we will have to deﬁne (since entropy is a global property
that depends on all moments  and ˜Eν only contains information about second order moments).
To see this optimization problem is convex  we show that it can easily be written as a semideﬁnite
program. Namely  note that the pseudo-moment operators are linear  so it sufﬁces to deﬁne them over
monomials only. Hence  the variables will simply be ˜Eν(xS) for all monomials xS of degree at most
2. The constraints ˜Eν[x2
i ] = 1 then are clearly linear  as is the “energy part” of the objective function.
So we only need to worry about the constraint ˜Eν[p(x)2] ≥ 0 and the entropy functional.
We claim the constraint ˜Eν[p(x)2] ≥ 0 can be written as a PSD constraint: namely if we deﬁne the
matrix Q  which is indexed by all the monomials of degree at most 1  and it satisﬁes Q(xS  xT ) =
˜Eν[xSxT ]. It is easy to see that ˜Eν[p(x)2] ≥ 0 ≡ Q (cid:23) 0.

7The reason ˜Eν [·] is called a pseudo-moment  is that it behaves like the moments of a distribution ν :

{−1  1}n → [0  1]  albeit only over polynomials of degree at most 2.

6

Hence  the ﬁnal concern is how to write an expression for the entropy in terms of the low-order
moments  since entropy is a global property that depends on all moments. There are many candidates
for this in machine learning are like Bethe/Kikuchi entropy  tree-reweighted Bethe entropy  log-
determinant etc. However  in the worst case – none of them come with any guarantees. We will in
fact show that the entropy functional is not an issue – we will relax the entropy trivially to n.
Given all of this  the ﬁnal relaxation we will consider is:

(cid:40)(cid:88)

t

(cid:41)

max

˜Eν [·]∈M(cid:48)

˜Eν[φt(x)] + n

Jt

(2)

From the prior setup it is clear that the solution to 2 is an upper bound to log Z. To prove a claim like
Theorem 2.3 or Theorem 2.4  we will then provide a rounding of the solution. In this instance  this
t JtE˜µ[φt(x)] + H(˜µ) comparable to
the value of the solution. Note this is slightly different than the usual requirement in optimization 
where one cares only about producing a single x ∈ {−1  1}n with comparable value to the solution.
Our distribution ˜µ will have entropy Ω(n)  and preserves the “energy” portion of the objective

will mean producing a distribution ˜µ which has the value of(cid:80)
(cid:80)

t JtEµ[φt(x)] up to a comparable factor to what is achievable in the optimization setting.

Warmup: exponential family analogue of MAX-CUT As a warmup  to illustrate the basic ideas
behind the above rounding strategy  before we consider Ising models we consider the exponential
family analogue of MAX-CUT. It is deﬁned by the functionals φi j(x) = (xi − xj)2. Concretely 
we wish to approximate the partition function of the distribution µ(x) ∝ exp

Ji j(xi − xj)2

(cid:88)

.

We will prove the following simple observation:
Observation 4.1. The relaxation 2 provides a factor 2 approximation of log Z.

i j

Proof. We proceed as outlined in the previous section  by providing a rounding of 2. We point out
again  unlike the standard case in optimization  where typically one needs to produce an assignment of
the variables  because of the entropy term here it is crucial that the rounding produces a distribution.
The distribution ˜µ we produce here will be especially simple: we will round each xi independently
2. Then  clearly H(˜µ) = n. On the other hand  we similarly have Pr˜µ[(xi − xj)2 =
with probability 1
2. Altogether  this
1] = 1
as we needed.

(cid:17)
2  since xi and xj are rounded independently. Hence  E˜µ[(xi − xj)2] ≥ 1

i j Ji jE˜µ[(xi − xj)2] + H(˜µ) ≥ 1

i j Ji jEν[(xi − xj)2] + n

implies(cid:80)

(cid:16)(cid:80)

2

4.1

Ising models

We proceed with the main results of this section on Ising models  which is the case where φi j(x) =
xixj. We will split into the ferromagnetic and general case separately  as outlined in Section 2.
To be concrete  we will be given potentials Ji j  and we wish to calculate the partition function of the

Ising model µ(x) ∝ exp((cid:80)

i j Ji jxixj).

Ferromagnetic case
Recall  in the ferromagnetic case of Ising model  we have the conditions that the potentials Ji j > 0.
We will provide a convex relaxation which has a constant factor approximation in this case. First  recall
the famous ﬁrst Grifﬁths inequality due to Grifﬁths [Gri67] which states that in the ferromagnetic
case  Eµ[xixj] ≥ 0 ∀i  j.
Using this inequality  we will look at the following natural strenghtening of the relaxation 2:

(cid:40)(cid:88)

t

(cid:41)

max

˜Eν [·]∈M(cid:48);˜Eν [xixj ]≥0 ∀i j

˜Eν[φt(x)] + n

Jt

(3)

We will prove the following theorem  as a straightforward implication of our claims from Section 3:

7

Theorem 4.1. The relaxation 3 provides a factor 50 approximation of log Z.

Proof. Notice  due to Grifﬁths’ inequality  3 is in fact a relaxation of the Gibbs variational principle
and hence an upper bound)of log Z. Same as before  we will provide a rounding of 3. We will use the
distribution ˜µ we designed in Section 3 the sign of a Gaussian with covariance matrix Σ + βI  for a
β which we will specify. By Lemma 3.2  we then have H(˜µ) ≥ n
whenever β ≥ 1
31/2 .
By Lemma 3.1  on the other hand  we can prove that E˜µ[xixj] ≥ G

(31/4√
√
˜Eν[xixj]

β−1)2
3β

25

1 + β

By setting β = 21.8202  we get n
25

(31/4√
√

β−1)2
3β

≥ 0.02 and G

1+β ≥ 0.02  which implies that

(cid:88)

Ji jE˜µ[xixj] + H(˜µ) ≥ 0.02

˜Eν[xixj] + n

Ji j

(cid:88)



i j

i j

which is what we need.

preserve the sum(cid:80)

Note that the above proof does not work in the general Ising model case: when ˜Eν[xixj] can be
either positive or negative  even if we preserved each ˜Eν[xixj] up to a constant factor  this may not

˜Eν[xixj] due to cancellations in that expression.

i j Ji j
General Ising models case
Finally  we will tackle the general Ising model case. As noted in the previous section  the straightfor-
ward application of the results proven in Section 3 doesn’t work  so we have to consider a different
rounding – again inspired by roundings used in optimization.
The intuition is the same as in the ferromagnetic case: we wish to design a rounding which preserves
the “energy” portion of the objective  while having a high entropy. In the previous section  this
was achieved by modifying the Goemans-Williamson rounding so that it produces a high-entropy
distribution. We will do a similar thing here  by modifying rounding due to [CW04] and [AMMN06].
The convex relaxation we will consider will just be the basic one: 2 and we will prove the following
two theorems:
Theorem 4.2. The relaxation 2 provides a factor O(log n) approximation to log Z when φi j(x) =
xixj.
Theorem 4.3. The relaxation 2 provides a factor O(log(χ(G))) approximation to log Z when
φi j(x) = xixj for i  j ∈ E(G) of some graph G = (V (G)  E(G))  and χ(G) is the chromatic
number of G.

Since the chromatic number of a graph is bounded by n  the second theorem is in fact strictly stronger
than the ﬁrst  however the proof of the ﬁrst theorem uses less heavy machinery  and is illuminating
enough to be presented on its own.
Due to space constraints  the proofs of these theorems are forwarded to the appendix.

5 Conclusion

In summary  we presented computationally efﬁcient approximate versions of the classical max-
entropy principle by [Jay57]: efﬁciently sampleable distributions which preserve given pairwise
moments up to a multiplicative constant factor  while having entropy within a constant factor of the
maximum entropy distribution matching those moments. Additionally  we applied our insights to
designing provable variational methods for Ising models which provide comparable guarantees for
approximating the log-partition function to those in the optimization setting. Our methods are based
on convex relaxations of the standard variational principle due to Gibbs  and are extremely generic
and we hope they will ﬁnd applications for other exponential families.

8

References
[AMMN06] Noga Alon  Konstantin Makarychev  Yury Makarychev  and Assaf Naor. Quadratic

forms on graphs. Inventiones mathematicae  163(3):499–522  2006.

[AN06] Noga Alon and Assaf Naor. Approximating the cut-norm via grothendieck’s inequality.

SIAM Journal on Computing  35(4):787–803  2006.

[BB07] Matthias Bethge and Philipp Berens. Near-maximum entropy models for binary neural

representations of natural images. 2007.

[BGS14] Guy Bresler  David Gamarnik  and Devavrat Shah. Hardness of parameter estimation
in graphical models. In Advances in Neural Information Processing Systems  pages
1062–1070  2014.

[BKS14] Boaz Barak  Jonathan A Kelner  and David Steurer. Rounding sum-of-squares relax-
ations. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing 
pages 31–40. ACM  2014.

[CW04] Moses Charikar and Anthony Wirth. Maximizing quadratic programs: extending
grothendieck’s inequality. In Foundations of Computer Science  2004. Proceedings.
45th Annual IEEE Symposium on  pages 54–60. IEEE  2004.

[Ell12] Richard S Ellis. Entropy  large deviations  and statistical mechanics  volume 271.

Springer Science & Business Media  2012.

[EN78] Richard S Ellis and Charles M Newman. The statistics of curie-weiss models. Journal

of Statistical Physics  19(2):149–161  1978.

[Gri67] Robert B Grifﬁths. Correlations in ising ferromagnets. i. Journal of Mathematical

Physics  8(3):478–483  1967.

[GW95] Michel X Goemans and David P Williamson. Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming. Journal of
the ACM (JACM)  42(6):1115–1145  1995.

[Jay57] Edwin T Jaynes.
106(4):620  1957.

Information theory and statistical mechanics. Physical review 

[JS93] Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the

ising model. SIAM Journal on computing  22(5):1087–1116  1993.

[Ris16] Andrej Risteski. How to compute partition functions using convex programming
hierarchies: provable bounds for variational methods. In Proceedings of the Conference
on Learning Theory (COLT)  2016.

[SS12] Allan Sly and Nike Sun. The computational hardness of counting in two-spin models
on d-regular graphs. In Foundations of Computer Science (FOCS)  2012 IEEE 53rd
Annual Symposium on  pages 361–369. IEEE  2012.

[SV14] Mohit Singh and Nisheeth K Vishnoi. Entropy  optimization and counting. In Proceed-
ings of the 46th Annual ACM Symposium on Theory of Computing  pages 50–59. ACM 
2014.

[WJ08] Martin J Wainwright and Michael I Jordan. Graphical models  exponential families 
and variational inference. Foundations and Trends in Machine Learning  1(1-2):1–305 
2008.

[WJW03] Martin J Wainwright  Tommi S Jaakkola  and Alan S Willsky. Tree-reweighted belief
propagation algorithms and approximate ml estimation by pseudo-moment matching.
2003.

[WJW05] Martin J Wainwright  Tommi S Jaakkola  and Alan S Willsky. A new class of upper
Information Theory  IEEE Transactions on 

bounds on the log partition function.
51(7):2313–2335  2005.

9

,Andrej Risteski
Yuanzhi Li