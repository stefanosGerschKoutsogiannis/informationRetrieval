2014,Exponential Concentration of a Density Functional Estimator,We analyse a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy  divergence  mutual information  and their conditional versions. For densities on the d-dimensional unit cube [0 1]^d that lie in a beta-Holder smoothness class  we prove our estimator converges at the rate O(n^(1/(beta+d))). Furthermore  we prove that the estimator obeys an exponential concentration inequality about its mean  whereas most previous related results have bounded only expected error of estimators. Finally  we demonstrate our bounds to the case of conditional Renyi mutual information.,Exponential Concentration of a Density Functional

Estimator

Shashank Singh

Statistics & Machine Learning Departments

Carnegie Mellon University

Pittsburgh  PA 15213

sss1@andrew.cmu.edu

Barnab´as P´oczos

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA 15213

bapoczos@cs.cmu.edu

Abstract

We analyze a plug-in estimator for a large class of integral functionals of one
or more continuous probability densities. This class includes important families
of entropy  divergence  mutual information  and their conditional versions. For
densities on the d-dimensional unit cube [0  1]d that lie in a β-H¨older smoothness
class  we prove our estimator converges at the rate O
. Furthermore  we
prove the estimator is exponentially concentrated about its mean  whereas most
previous related results have proven only expected error bounds on estimators.

(cid:16)

(cid:17)

− β

β+d

n

1

Introduction

Many important quantities in machine learning and statistics can be viewed as integral functionals
of one of more continuous probability densities; that is  quanitities of the form

(cid:90)

F (p1 ···   pk) =

X1×···×Xk

f (p1(x1)  . . .   pk(xk)) d(x1  . . .   xk) 

where p1 ···   pk are probability densities of random variables taking values in X1 ···   Xk  re-
spectively  and f : Rk → R is some measurable function. For simplicity  we refer to such integral
functionals of densities as ‘density functionals’. In this paper  we study the problem of estimating
density functionals. In our framework  we assume that the underlying distributions are not given
explicitly. Only samples of n independent and identically distributed (i.i.d.) points from each of the
unknown  continuous  nonparametric distributions p1 ···   pk are given.

1.1 Motivations and Goals

One density functional of interest is Conditional Mutual Information (CMI)  a measure of con-
ditional dependence of random variables  which comes in several varieties including R´enyi-α and
Tsallis-α CMI (of which Shannon CMI is the α → 1 limit case). Estimating conditional dependence
in a consistent manner is a crucial problem in machine learning and statistics; for many applications 
it is important to determine how the relationship between two variables changes when we observe
additional variables. For example  upon observing a third variable  two correlated variables may be-
come independent  and  similarly  two independent variables may become dependent. Hence  CMI
estimators can be used in many scientiﬁc areas to detect confounding variables and avoid infering
causation from apparent correlation [20  16]. Conditional dependencies are also central to Bayesian
network learning [7  35]  where CMI estimation can be used to verify compatibility of a particular
Bayes net with observed data under a local Markov assumption.
Other important density functionals are divergences between probability distributions  including
R´enyi-α [25] and Tsallis-α [32] divergences (of which Kullback-Leibler (KL) divergence [9] is the

1

α → 1 limit case)  and Lp divergence. Divergence estimators can be used to extend machine
learning algorithms for regression  classiﬁcation  and clustering from the standard setting where in-
puts are ﬁnite-dimensional feature vectors to settings where inputs are sets or distributions [23  19].
Entropy and mutual information (MI) can be estimated as special cases of divergences. Entropy
estimators are used in goodness-of-ﬁt testing [5]  parameter estimation in semi-parametric models
[34]  and texture classiﬁcation [6]  and MI estimators are used in feature selection [21]  clustering
[1]  optimal experimental design [13]  and boosting and facial expression recognition [26]. Both en-
tropy and mutual information estimators are used in independent component and subspace analysis
[10  30] and image registration [6]. Further applications of divergence estimation are in [11].
Despite the practical utility of density functional estimators  little is known about their statistical
performance  especially for functionals of more than one density. In particular  few density func-
tional estimators have known convergence rates  and  to the best of our knowledge  no ﬁnite sample
exponential concentration bounds have been derived for general density functional estimators. One
consequence of this exponential bound is that  using a union bound  we can guarantee accuracy of
multiple estimates simultaneously. For example  [14] shows how this can be applied to optimally
analyze forest density estimation algorithms. Because the CMI of variables X and Y given a third
variable Z is zero if and only X and Y are conditionally independent given Z  by estimating CMI
with a conﬁdence interval  we can test for conditional independence with bounded type I error prob-
abilty.
Our main contribution is to derive convergence rates and an exponential concentration inequality
for a particular  consistent  nonparametric estimator for large class of density functionals  including
conditional density functionals. We also apply our concentration inequality to the important case of
R´enyi-α CMI.

1.2 Related Work

density cases of L2  R´enyi-α  and Tsallis-α divergences and gave plug-in estimators which achieve

optimally estimating the density and then applying a correction to the plug-in estimate. In contrast 

Although lower bounds are not known for estimation of general density functionals (of arbitrarily
many densities)  [2] lower bounded the convergence rate for estimators of functionals of a single

density (e.g.  entropy functionals) by O(cid:0)n−4β/(4β+d)(cid:1). [8] extended this lower bound to the two-
this rate. These estimators enjoy the parametric rate of O(cid:0)n−1/2(cid:1) when β > d/4  and work by
our estimator undersmooths the density  and converges at a slower rate of O(cid:0)n−β/(β+d)(cid:1) when
β < d (and the parametric rate O(cid:0)n−1/2(cid:1) when β ≥ d)  but obeys an exponential concentration

inequality  which is not known for the estimators of [8].
Another exception for f-divergences is provided by [18]  using empirical risk minimization. This
approach involves solving an ∞-dimensional convex minimization problem which be reduced to an
n-dimensional problem for certain function classes deﬁned by reproducing kernel Hilbert spaces (n
is the sample size). When n is large  these optimization problems can still be very demanding. They
studied the estimator’s convergence rate  but did not derive concentration bounds.
A number of papers have studied k-nearest-neighbors estimators  primarily for R´enyiα density func-
tionals including entropy [12]  divergence [33] and conditional divergence and MI [22]. These esti-
mators work directly  without the intermediate density estimation step  and generally have proofs of
consistency  but their convergence rates and dependence on k  α  and the dimension are unknown.
One recent exception is a k-nearest-neighbors based estimator that converges at the parametric rate
when β > d  using an optimally weighted ensemble of weak estimators [28  17]. These estimators
appear to perform well in higher dimensions  but rates for these estimators require that k → ∞ as
n → ∞  causing computational difﬁculties for large samples.
Although the literature on dependence measures is huge  few estimators have been generalized to the
conditional case [4  24]. There is some work on testing conditional dependence [29  3]  but  unlike
CMI estimation  these tests are intended to simply accept or reject the hypothesis that variables
are conditionally independent  rather than to measure conditional dependence. Our exponential
concentration inequality also suggests a new test for conditional independence.
This paper continues a line of work begun in [14] and continued in [27]. [14] proved an exponential
concentration inequality for an estimator of Shannon entropy and MI in the 2-dimensional case.

2

[27] used similar techniques to derive an exponential concentration inequality for an estimator of
R´enyi-α divergence in d dimensions  for a larger family of densities. Both used plug-in estimators
based on a mirrored kernel density estimator (KDE) on [0  1]d. Our work generalizes these results to
a much larger class of density functionals  as well as to conditional density functionals (see Section
6).
In particular  we use a plug-in estimator for general density functionals based on the same
mirrored KDE  and also use some lemmas regarding this KDE proven in [27]. By considering the
more general density functional case  we are also able to signiﬁcantly simplify the proofs of the
convergence rate and exponential concentration inequality.

Organization

In Section 2  we establish the theoretical context of our work  including notation  the precise prob-
lem statement  and our estimator. In Section 3  we outline our main theoretical results and state
some consequences. Sections 4 and 5 give precise statements and proofs of the results in Section 3.
Finally  in Section 6  we extend our results to conditional density functionals  and state the conse-
quences in the particular case of R´enyi-α CMI.

2 Density Functional Estimator

2.1 Notation
For an integer k  [k] = {1 ···   k} denotes the set of positive integers at most k. Using the notation
of multi-indices common in multivariable calculus  Nd denotes the set of d-tuples of non-negative
integers  which we denote with a vector symbol(cid:126)·  and  for(cid:126)i ∈ Nd 
|(cid:126)i| =

d(cid:88)

D(cid:126)i :=

∂|(cid:126)i|

and

ik.

For ﬁxed β  L > 0  r ≥ 1  and a positive integer d  we will work with densities in the following
bounded subset of a β-H¨older space:

k=1

C β

L r([0  1]d) :=

|D(cid:126)ip(x) − D(cid:126)ip(y)|

(cid:107)x − y(cid:107)(β−(cid:96))

(1)

  

where (cid:96) = (cid:98)β(cid:99) is the greatest integer strictly less than β  and (cid:107) · (cid:107)r : Rd → R is the usual r-norm.
To correct for boundary bias  we will require the densities to be nearly constant near the boundary
of [0  1]d  in that their derivatives vanish at the boundary. Hence  we work with densities in
|D(cid:126)ip(x)| → 0 as dist(x  ∂[0  1]d) → 0

Σ(β  L  r  d) :=

p ∈ C β

(cid:40)

(cid:41)

(2)

 

L r([0  1]d)

where ∂[0  1]d = {x ∈ [0  1]d : xj ∈ {0  1} for some j ∈ [d]}.

2.2 Problem Statement
For each i ∈ [k] let Xi be a di-dimensional random vector taking values in Xi := [0  1]di  distributed
according to a density pi : X → R. For an appropriately smooth function f : Rk → R  we are
interested in a using random sample of n i.i.d. points from the distribution of each Xi to estimate

f (p1(x1)  . . .   pk(xk)) d(x1  . . .   xk).

(3)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) sup

x(cid:54)=y∈D
|(cid:126)i|=(cid:96)

∂i1x1 ··· ∂id xd

p : [0  1]d → R
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) max

1≤|(cid:126)i|≤(cid:96)

(cid:90)

(cid:90)

3

F (p1  . . .   pk) :=

2.3 Estimator

X1×···×Xk

For a ﬁxed bandwidth h  we ﬁrst use the mirrored kernel density estimator (KDE) ˆpi described in
[27] to estimate each density pi. We then use a plug-in estimate of F (p1  . . .   pk).

F (ˆp1  . . .   ˆpk) :=

f (ˆp1(x1)  . . .   ˆpk(xk)) d(x1  . . .   xk).

X1×···×Xk

Our main results generalize those of [27] to a broader class of density functionals.

3 Main Results

In this section  we outline our main theoretical results  proven in Sections 4 and 5  and also discuss
some important corollaries.
We decompose the estimatator’s error into bias and a variance-like terms via the triangle inequality:

|F (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ≤ |F (ˆp1  . . .   ˆpk) − EF (ˆp1  . . .   ˆpk)|

(cid:124)
(cid:124)

variance-like term

+ |EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)|

(cid:125)
(cid:125)

.

(4)

(5)

(cid:19)
(cid:19)

 

(cid:123)(cid:122)
(cid:123)(cid:122)

bias term

(cid:18)

(cid:18)

hβ + h2β +

1
nhd

(cid:16)

(cid:17)

We will prove the “variance” bound

P (|F (ˆp1  . . .   ˆpk) − EF (ˆp1  . . .   ˆpk)| > ε) ≤ 2 exp

− 2ε2n
C 2
V

for all ε > 0 and the bias bound

|EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ≤ CB

where d := maxi di  and CV and CB are constant in the sample size n and bandwidth h for exact
values. To the best of our knowledge  this is the ﬁrst time an exponential inequality like (4) has been
established for general density functional estimation. This variance bound does not depend on h and
the bias bound is minimized by h (cid:16) n

β+d   we have the convergence rate

− 1

|EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ∈ O

− β

β+d

n

.

It is interesting to note that  in optimizing the bandwidth for our density functional estimate  we use
a smaller bandwidth than is optimal for minimizing the bias of the KDE. Intuitively  this reﬂects the
fact that the plug-in estimator  as an integral functional  performs some additional smoothing.
We can use our exponential concentration bound to obtain a bound on the true variance of
F (ˆp1  . . .   ˆpk). If G : [0 ∞) → R denotes the cumulative distribution function of the squared
deviation of F (ˆp1  . . .   ˆpk) from its mean  then

(cid:18)

− 2εn
C 2
V

(cid:19)

.

(F (ˆp1  . . .   ˆpk) − EF (ˆp1  . . .   ˆpk))2 > ε

(cid:17) ≤ 2 exp
1 − G(ε) = P(cid:16)
(F (ˆp1  . . .   ˆpk) − EF (ˆp1  . . .   ˆpk))2(cid:105)
V[F (ˆp1  . . .   ˆpk)] = E(cid:104)
(cid:19)
(cid:90) ∞

(cid:18)

(cid:90) ∞
(F (ˆp1  . . .   ˆpk) − F (p1  . . .   pk))2(cid:105) ∈ O

1 − G(ε) dε ≤ 2

− 2εn
C 2
V

n−1 + n

E(cid:104)

(cid:16)

exp

=

0

0

− 2β

β+d

= C 2

V n−1.

(cid:17)

 

We then have a mean squared error of

Thus 

− 2β

which is in O(n−1) if β ≥ d and O
It should be noted that the constants in both the bias bound and the variance bound depend expo-
nentially on the dimension d. Lower bounds in terms of d are unknown for estimating most density
functionals of interest  and an important open problem is whether this dependence can be made
asymptotically better than exponential.

otherwise.

β+d

n

4 Bias Bound

In this section  we precisely state and prove the bound on the bias of our density functional estimator 
as introduced in Section 3.

4

(cid:16)

(cid:17)

Assume each pi ∈ Σ(β  L  r  d) (for i ∈ [k])  assume f : Rk → R is twice continuously differen-
tiable  with ﬁrst and second derivatives all bounded in magnitude by some Cf ∈ R  1 and assume
the kernel K : R → R has bounded support [−1  1] and satisﬁes

(cid:90) 1

(cid:90) 1

K(u) du = 1

and

ujK(u) du = 0

for all j ∈ {1  . . .   (cid:96)}.

−1

−1
Then  there exists a constant CB ∈ R such that

|EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ≤ CB

(cid:18)

hβ + h2β +

(cid:19)

.

1
nhd

4.1 Proof of Bias Bound
By Taylor’s Theorem  ∀x = (x1  . . .   xk) ∈ X1 × ··· × Xk  for some ξ ∈ Rk on the line segment
between ˆp(x) := (ˆp1(x1)  . . .   ˆpk(xk)) and p(x) := (p1(x1)  . . .   pk(xk))  letting Hf denote the
Hessian of f

|Ef (ˆp(x)) − f (p(x))| =

≤ Cf

(ˆp(x) − p(x))T Hf (ξ)(ˆp(x) − p(x))

|Bpi(xi)Bpj (xj)| +

E[ˆpi(xi) − pi(xi)]2

(cid:12)(cid:12)(cid:12)(cid:12)E(∇f )(p(x)) · (ˆp(x) − p(x)) +
 k(cid:88)

(cid:88)

1
2

i=1

i<j≤k

|Bpi (xi)| +
(cid:90)

X1×···×Xk

(cid:90)

(cid:88)

i<j≤k

k(cid:88)

i=1

(cid:90)

where we used that ˆpi and ˆpj are independent for i (cid:54)= j. Applying H¨older’s Inequality 
|EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ≤

|Ef (ˆp(x)) − f (p(x))| dx

 k(cid:88)
(cid:90)
(cid:115)(cid:90)
(cid:32) k(cid:88)

i=1

≤ Cf

≤ Cf

(cid:12)(cid:12)(cid:12)(cid:12)




(cid:33)

(cid:90)

Xi

(cid:90)

[0 1]d

|Bpi(xi)| + E[ˆpi(xi) − pi(xi)]2 dxi +
Xi

|Bpi(xi)| dxi
Xi

|Bpj (xj)| dxj
Xj

B2
pi

(xi) dxi +

i=1

Xi

E[ˆpi(xi) − pi(xi)]2 dxi

(cid:115)(cid:90)

(cid:88)

+

i<j≤k

Xi

(cid:90)

Xj

B2
pi

(xi) dxi

B2
pj

(xj) dxj

.

We now make use of the so-called Bias Lemma proven by [27]  which bounds the integrated squared
bias of the mirrored KDE ˆp on [0  1]d for an arbitrary p ∈ Σ(β  L  r  d). Writing the bias of ˆp at
x ∈ [0  1]d as Bp(x) = Eˆp(x) − p(x)  [27] showed that there exists C > 0 constant in n and h such
that

p(x) dx ≤ Ch2β.
B2

(6)

Applying the Bias Lemma and certain standard results in kernel density estimation (see  for example 
Propositions 1.1 and 1.2 of [31]) gives

|EF (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| ≤ C(cid:0)k2hβ + kh2β(cid:1) +

(cid:107)K(cid:107)d
nhd ≤ CB

1

hβ + h2β +

1
nhd

(cid:19)

 

(cid:18)

where (cid:107)K(cid:107)1 denotes the 1-norm of the kernel. (cid:4)

1If p1(X1) × ··· × pk(Xk) is known to lie within some cube [κ1  κ2]k  then it sufﬁces for f to be twice
continuously differentiable on [κ1  κ2]k (and the boundedness condition follows immediately). This will be
important for our application to R´enyi-α Conditional Mutual Information.

5

5 Variance Bound

In this section  we precisely state and prove the exponential concentration inequality for our density
functional estimator  as introduced in Section 3. Assume that f is Lipschitz continuous with constant
Cf in the 1-norm on p1(X1) × ··· × pk(Xk) (i.e. 
|xi − yi| 

∀x  y ∈ p1(X1) × ··· × pk(Xk)).

|f (x) − f (y)| ≤ Cf

∞(cid:88)

(7)

k=1

and assume the kernel K ∈ L1(R) (i.e.  it has ﬁnite 1-norm). Then  there exists a constant CV ∈ R
such that ∀ε > 0 

P (|F (ˆp1  . . .   ˆpk) − EF (ˆp1  . . .   ˆpk)|) ≤ 2 exp

(cid:18)

− 2ε2n
C 2
V

(cid:19)

.

Note that  while we require no assumptions on the densities here  in certain speciﬁc applications 
such us for some R´enyi-α quantities  where f = log  assumptions such as lower bounds on the
density may be needed to ensure f is Lipschitz on its domain.

5.1 Proof of Variance Bound

Consider i.i.d. samples (x1
p = p1×··· pk. In anticipation of using McDiarmid’s Inequality [15]  let ˆp(cid:48)
KDE when the sample xi
(7) on f 

k ) ∈ X1 × ··· × Xk drawn according to the product distribution
j denote the jth mirrored
j)(cid:48). Then  applying the Lipschitz condition

j is replaced by new sample (xi

1  . . .   xn

|F (ˆp1  . . .   ˆpk) − F (ˆp1  . . .   ˆp(cid:48)

|pj(x) − p(cid:48)

since most terms of the sum in (7) are zero. Expanding the deﬁnition of the kernel density estimates
ˆpj and ˆp(cid:48)
|F (ˆp1  . . .   ˆpk) − F (ˆp1  . . .   ˆp(cid:48)

j and noting that most terms of the mirrored KDEs ˆpj and ˆp(cid:48)
x − xi

j are identical gives
x − (xi
j)(cid:48)
h

j  . . .   ˆpk)| =

− Kdj

Cf
nhdj

j

(cid:33)

where Kdj denotes the dj-dimensional mirrored product kernel based on K. Performing a change
of variables to remove h and applying the triangle inequality followed by the bound on the integral
of the mirrored kernel proven in [27] 

(cid:90)
j  . . .   ˆpk)| ≤ Cf
(cid:32)

(cid:90)

j(x)| dx 
(cid:32)

Xj

h

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Kdj
(cid:12)(cid:12)Kdj (x − xi

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dx
j)(cid:48))(cid:12)(cid:12) dx

CV
n

 

(8)

|F (ˆp1  . . .   ˆpk) − F (ˆp1  . . .   ˆp(cid:48)

j  . . .   ˆpk)| ≤ Cf
n
≤ 2Cf
n

j) − Kdj (x − (xi
(cid:107)K(cid:107)dj

1 =

|Kdj (x)| dx ≤ 2Cf
n

[−1 1]dj

for CV = 2Cf maxj (cid:107)K(cid:107)dj
armid’s Inequality then gives  for any ε > 0 

P (|F (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| > ε) ≤ 2 exp

(cid:19)

(cid:18)

(cid:19)

− 2ε2
knC 2

V /n2

= 2 exp

− 2ε2n
kC 2
V

. (cid:4)

1 . Since F (ˆp1  . . .   ˆpk) depends on kn independent variables  McDi-

6 Extension to Conditional Density Functionals

Our convergence result and concentration bound can be fairly easily adapted to to KDE-based plug-
in estimators for many functionals of interest  including R´enyi-α and Tsallis-α entropy  divergence 
and MI  and Lp norms and distances  which have either the same or analytically similar forms as as
the functional (3). As long as the density of the variable being conditioned on is lower bounded on
its domain  our results also extend to conditional density functionals of the form 2

(cid:19)

(cid:19)

 

P (x2  z)

P (z)

  . . .  

P (xk  z)

P (z)

d(x1  . . .   xk)

dz (9)

(cid:18)(cid:90)

(cid:90)

Z

F (P ) =

P (z)f

g

X1×···×Xk

(cid:18) P (x1  z)

P (z)

2We abuse notation slightly and also use P to denote all of its marginal densities.

6

Xj

(cid:90)
(cid:90)

hXj

(cid:18)

including  for example  R´enyi-α conditional entropy  divergence  and mutual information  where f
is the function x (cid:55)→ 1
1−α log(x). The proof of this extension for general k is essentially the same as
for the case k = 1  and so  for notational simplicity  we demonstrate the latter.

6.1 Problem Statement  Assumptions  and Estimator
For given dimensions dx  dz ≥ 1  consider random vectors X and Z distributed on unit cubes
X := [0  1]dx and Z := [0  1]dz according to a joint density P : X × Z → R. We use a random
sample of 2n i.i.d. points from P to estimate a conditional density functional F (P )  where F has
the form (9).
Suppose that P is in the H¨older class Σ(β  L  r  dx + dz)  noting that this implies an analogous
condition on each marginal of P   and suppose that P bounded below and above  i.e.  0 < κ1 :=
inf x∈X  z∈Z P (z) and ∞ > κ2 := inf x∈X  z∈Z P (x  z). Suppose also that f and g are continuously
differentiable  with

where

Cf := sup

x∈[cg Cg]

(cid:18)(cid:20)

cg := inf g

0 

|f (x)|

(cid:21)(cid:19)

κ2
κ1

and Cf(cid:48) := sup

|f(cid:48)(x)| 

x∈[cg Cg]

(cid:18)(cid:20)

0 

κ2
κ1

(cid:21)(cid:19)

(10)

.

and Cg := sup g

After estimating the densities P (z) and P (x  z) by their mirrored KDEs  using n independent data
samples for each  we clip the estimates of P (x  z) and P (z) below by κ1 and above by κ2 and
denote the resulting density estimates by ˆP . Our estimate F ( ˆP ) for F (P ) is simply the result of
plugging ˆP into equation (9).

6.2 Proof of Bounds for Conditional Density Functionals

We bound the error of F ( ˆP ) in terms of the error of estimating the corresponding unconditional
density functional using our previous estimator  and then apply our previous results.
Suppose P1 is either the true density P or a plug-in estimate of P computed as described above 
and P2 is a plug-in estimate of P computed in the same manner but using a different data sample.
Applying the triangle inequality twice 
|F (P1) − F (P2)| ≤

− P2(z)f

(cid:18)(cid:90)

dx

dx

g

g

X

X

+

≤

+ P2(z)

dx

dx

dx

g

X

Z

Z

g

X

g

X

P1(z)

P1(z)

P1(z)

P2(z)

(cid:19)

− P2(z)f

|P1(z) − P2(z)|

(cid:18) P1(x  z)
(cid:18) P1(x  z)
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)f
(cid:18)(cid:90)
(cid:18) P1(x  z)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)P1(z)f
(cid:18) P1(x  z)
(cid:19)
(cid:90)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) dz
(cid:12)(cid:12)(cid:12)(cid:12)P2(z)f
(cid:18)(cid:90)
(cid:19)
(cid:18) P2(x  z)
(cid:90)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) dz
(cid:12)(cid:12)(cid:12)(cid:12)f
(cid:19)
(cid:18)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) dz
(cid:19)
(cid:18) P2(x  z)
Cf|P1(z) − P2(z)| + κ2Cf(cid:48)(cid:12)(cid:12)GP1(z)(P1(·  z)) − GP2(z)(P2(·  z))(cid:12)(cid:12) dz 

(cid:19)
(cid:19)
(cid:18)(cid:90)
(cid:19)
(cid:18)(cid:90)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18) P1(x  z)
(cid:19)
(cid:18)(cid:90)
(cid:19)
(cid:18) P2(x  z)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:19)
(cid:18) P1(x  z)
(cid:18) Q(x)

P1(z)
− f

(cid:19)

P1(z)

P2(z)

P2(z)

P1(z)

− g

g

X

g

X

(cid:90)
(cid:90)

(cid:90)

dx

dx

dx

=

Z

Applying the Mean Value Theorem and the bounds in (10) gives
|F (P1) − F (P2)| ≤

Cf|P1(z) − P2(z)| + κ2Cf(cid:48)
Z

X

g

where Gz is the density functional

GP (z)(Q) =

g

X

P (z)

dx.

Note that  since the data are split to estimate P (z) and P (x  z)  G ˆP (z)( ˆP (·  z)) depends on each
data point through only one of these KDEs. In the case that P1 is the true density P   taking the

7

expectation and using Fubini’s Theorem gives

E|F (P ) − F ( ˆP )| ≤

(cid:90)

Z
≤Cf

CfE|P (z) − ˆP (z)| + κ2Cf(cid:48)E(cid:12)(cid:12)(cid:12)GP (z)(P (·  z)) − G ˆP (z)( ˆP (·  z))
(cid:12)(cid:12)(cid:12) dz 
(cid:115)(cid:90)
(cid:19)

(cid:18)

E(P (z) − ˆP (z))2dz + 2κ2Cf(cid:48)CB
Z

hβ + h2β +

1
nhd

(cid:18)

(cid:19)

≤ (2κ2Cf(cid:48)CB + Cf C)

hβ + h2β +

1
nhd

applying H¨older’s Inequality and our bias bound (5)  followed by the bias lemma (6). This extends
our bias bound to conditional density functionals. For the variance bound  consider the case where
P1 and P2 are each mirrored KDE estimates of P   but with one data point resampled (as in the proof
of the variance bound  setting up to use McDiarmid’s Inequality). By the same sequence of steps
used to show (8) 

(cid:90)
|P1(z) − P2(z)| dz ≤ 2(cid:107)K(cid:107)dz
(cid:12)(cid:12)(cid:12)GP (z)(P (·  z)) − G ˆP (z)( ˆP (·  z))

Z

n

1

and

(cid:90)

Z

 

(cid:12)(cid:12)(cid:12) dz ≤ CV
(cid:18)

n

.

(cid:19)

(by casing on whether the resampled data point was used to estimate P (x  z) or P (z))  for an
appropriate CV depending on supx∈[κ1/κ2 κ2/κ1] |g(cid:48)(x)|. Then  by McDiarmid’s Inequality 

P (|F (ˆp1  . . .   ˆpk) − F (p1  . . .   pk)| > ε) = 2 exp

− ε2n
4C 2
V

. (cid:4)

6.3 Application to R´enyi-α Conditional Mutual Information

As an example  we demonstrate our concentration inequality to the R´enyi-α Conditional Mutual
Information (CMI). Consider random vectors X  Y   and Z on X = [0  1]dx  Y = [0  1]dy  Z =
[0  1]dz  respectively. α ∈ (0  1) ∪ (1 ∞)  the R´enyi-α CMI of X and Y given Z is
I(X; Y |Z) =

(cid:19)α(cid:18) P (x  z)P (y  z)

(cid:18) P (x  y  z)

(cid:19)1−α

d(x  y) dz. (11)

P (z) log

(cid:90)

(cid:90)

1

X×Y

P (z)

P (z)2

1 − α

Z

In this case  the estimator which plugs mirrored KDEs for P (x  y  z)  P (x  z)  P (y  z)  and P (z)
into (11) obeys the concentration inequality (4) with CV = κ∗(cid:107)K(cid:107)dx+dy+dz
  where κ∗ depends
only on α  κ1  and κ2.

1

References
[1] M. Aghagolzadeh  H. Soltanian-Zadeh  B. Araabi  and A. Aghagolzadeh. A hierarchical clus-
tering based on mutual information maximization. In in Proc. of IEEE International Confer-
ence on Image Processing  pages 277–280  2007.

[2] L. Birge and P. Massart. Estimation of integral functions of a density. A. Statistics  23:11–29 

1995.

[3] T. Bouezmarni  J. Rombouts  and A. Taamouti. A nonparametric copula based test for condi-
tional independence with applications to granger causality  2009. Technical report  Universidad
Carlos III  Departamento de Economia.

[4] K. Fukumizu  A. Gretton  X. Sun  and B. Schoelkopf. Kernel measures of conditional depen-

dence. In Neural Information Processing Systems (NIPS)  2008.

[5] M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. L. Novi Inverardi. A new class of random
vector entropy estimators and its applications in testing statistical hypotheses. J. Nonparamet-
ric Statistics  17:277–297  2005.

[6] A. O. Hero  B. Ma  O. J. J. Michel  and J. Gorman. Applications of entropic spanning graphs.

IEEE Signal Processing Magazine  19(5):85–95  2002.

[7] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT

Press  Cambridge  MA  2009.

8

[8] A. Krishnamurthy  K. Kandasamy  B. Poczos  and L. Wasserman. Nonparametric estimation
of renyi divergence and friends. In International Conference on Machine Learning (ICML) 
2014.

[9] S. Kullback and R.A. Leibler. On information and sufﬁciency. Annals of Mathematical Statis-

tics  22:79–86  1951.

[10] E. G. Learned-Miller and J. W. Fisher. ICA using spacings estimates of entropy. J. Machine

Learning Research  4:1271–1295  2003.

[11] N. Leonenko  L. Pronzato  and V. Savani. A class of R´enyi information estimators for multidi-

mensional densities. Annals of Statistics  36(5):2153–2182  2008.

[12] N. Leonenko  L. Pronzato  and V. Savani. Estimation of entropies and divergences via nearest

neighbours. Tatra Mt. Mathematical Publications  39  2008.

[13] J. Lewi  R. Butera  and L. Paninski. Real-time adaptive information-theoretic optimization of
neurophysiology experiments. In Advances in Neural Information Processing Systems  vol-
ume 19  2007.

[14] H. Liu  J. Lafferty  and L. Wasserman. Exponential concentration inequality for mutual infor-

mation estimation. In Neural Information Processing Systems (NIPS)  2012.

[15] C. McDiarmid. On the method of bounded differences. Surveys in Combinatorics  141:148–

188  1989.

[16] D. Montgomery. Design and Analysis of Experiments. John Wiley and Sons  2005.
[17] K.R. Moon and A.O. Hero. Ensemble estimation of multivariate f-divergence. In Information

Theory (ISIT)  2014 IEEE International Symposium on  pages 356–360  June 2014.

[18] X. Nguyen  M.J. Wainwright  and M.I. Jordan. Estimating divergence functionals and the

likelihood ratio by convex risk minimization. IEEE Trans. on Information Theory.  2010.

[19] J. Oliva  B. Poczos  and J. Schneider. Distribution to distribution regression. In International

Conference on Machine Learning (ICML)  2013.

[20] J. Pearl. Why there is no statistical test for confounding  why many think there is  and why

they are almost right  1998. UCLA Computer Science Department Technical Report R-256.

[21] H. Peng and C. Dind. Feature selection based on mutual information: Criteria of max-
dependency  max-relevance  and min-redundancy. IEEE Trans On Pattern Analysis and Ma-
chine Intelligence  27  2005.

[22] B. Poczos and J. Schneider. Nonparametric estimation of conditional information and diver-
In International Conference on AI and Statistics (AISTATS)  volume 20 of JMLR

gences.
Workshop and Conference Proceedings  2012.

[23] B. Poczos  L. Xiong  D. Sutherland  and J. Schneider. Nonparametric kernel estimators for
image classiﬁcation. In 25th IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2012.

[24] S. J. Reddi and B. Poczos. Scale invariant conditional dependence measures. In International

Conference on Machine Learning (ICML)  2013.

[25] A. R´enyi. Probability Theory. North-Holland Publishing Company  Amsterdam  1970.
[26] C. Shan  S. Gong  and P. W. Mcowan. Conditional mutual information based boosting for

facial expression recognition. In British Machine Vision Conference (BMVC)  2005.

[27] S. Singh and B. Poczos. Generalized exponential concentration inequality for r´enyi divergence

estimation. In International Conference on Machine Learning (ICML)  2014.

[28] K. Sricharan  D. Wei  and A. Hero. Ensemble estimators for multivariate entropy estimation 

2013.

[29] L. Su and H. White. A nonparametric Hellinger metric test for conditional independence.

[30] Z. Szab´o  B. P´oczos  and A. L˝orincz. Undercomplete blind subspace deconvolution. J. Ma-

Econometric Theory  24:829–864  2008.

chine Learning Research  8:1063–1095  2007.

[31] A.B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company 

Incorporated  1st edition  2008.

[32] T. Villmann and S. Haase. Mathematical aspects of divergence based vector quantization using

Frechet-derivatives  2010. University of Applied Sciences Mittweida.

[33] Q. Wang  S.R. Kulkarni  and S. Verd´u. Divergence estimation for multidimensional densities

via k-nearest-neighbor distances. IEEE Transactions on Information Theory  55(5)  2009.

[34] E. Wolsztynski  E. Thierry  and L. Pronzato. Minimum-entropy estimation in semi-parametric

models. Signal Process.  85(5):937–949  2005.

[35] K. Zhang  J. Peters  D. Janzing  and B. Scholkopf. Kernel-based conditional independence test

and application in causal discovery. In Uncertainty in Artiﬁcial Intelligence (UAI)  2011.

9

,Shashank Singh
Barnabas Poczos