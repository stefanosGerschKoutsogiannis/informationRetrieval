2013,Estimation  Optimization  and Parallelism when Data is Sparse,We study stochastic optimization problems when the \emph{data} is sparse   which is in a sense dual to the current understanding of high-dimensional statistical learning and optimization. We highlight both the difficulties---in terms of increased sample complexity that sparse data necessitates---and the potential benefits  in terms of allowing parallelism and asynchrony in the design of algorithms. Concretely  we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data  and we exhibit algorithms achieving these rates. Our algorithms are adaptive: they achieve the best possible rate for the data observed. We also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms  providing experimental evidence complementing our theoretical results on medium to large-scale learning tasks.,Estimation  Optimization  and Parallelism when

Data is Sparse

John C. Duchi1 2 Michael I. Jordan1

University of California  Berkeley1

Berkeley  CA 94720

{jduchi jordan}@eecs.berkeley.edu

H. Brendan McMahan2

Google  Inc.2

Seattle  WA 98103

mcmahan@google.com

Abstract

We study stochastic optimization problems when the data is sparse  which is in
a sense dual to current perspectives on high-dimensional statistical learning and
optimization. We highlight both the difﬁculties—in terms of increased sample
complexity that sparse data necessitates—and the potential beneﬁts  in terms of
allowing parallelism and asynchrony in the design of algorithms. Concretely  we
derive matching upper and lower bounds on the minimax rate for optimization
and learning with sparse data  and we exhibit algorithms achieving these rates.
We also show how leveraging sparsity leads to (still minimax optimal) parallel
and asynchronous algorithms  providing experimental evidence complementing
our theoretical results on several medium to large-scale learning tasks.

1

Introduction and problem setting

In this paper  we investigate stochastic optimization problems in which the data is sparse. Formally 
let {F (·; ξ) ξ ∈ Ξ} be a collection of real-valued convex functions  each of whose domains con-
tains the convex set X⊂ Rd. For a probability distribution P on Ξ  we consider the following
optimization problem:

minimize

x∈X

f (x) := E[F (x; ξ)] =!Ξ

F (x; ξ)dP (ξ).

(1)

By data sparsity  we mean the samples ξ are sparse: assuming that samples ξ lie in Rd  and deﬁning
the support supp(x) of a vector x to the set of indices of its non-zero components  we assume

supp∇F (x; ξ) ⊂ supp ξ.

(2)

The sparsity condition (2) means that F (x; ξ) does not “depend” on the values of xj for indices j
such that ξj = 0.1 This type of data sparsity is prevalent in statistical optimization problems and
machine learning applications; in spite of its prevalence  study of such problems has been limited.
As a motivating example  consider a text classiﬁcation problem: data ξ ∈ Rd represents words
appearing in a document  and we wish to minimize a logistic loss F (x; ξ) = log(1 + exp($ξ  x%))
on the data (we encode the label implicitly with the sign of ξ). Such generalized linear models satisfy
the sparsity condition (2)  and while instances are of very high dimension  in any given instance  very
few entries of ξ are non-zero [8]. From a modelling perspective  it thus makes sense to allow a dense
predictor x: any non-zero entry of ξ is potentially relevant and important. In a sense  this is dual to
the standard approaches to high-dimensional problems; one usually assumes that the data ξ may be
dense  but there are only a few relevant features  and thus a parsimonious model x is desirous [2]. So

1Formally  if πξ denotes the coordinate projection zeroing all indices j of its argument where ξj = 0  then

F (πξ(x); ξ) = F (x; ξ) for all x  ξ. This follows from the ﬁrst-order conditions for convexity [6].

1

while such sparse data problems are prevalent—natural language processing  information retrieval 
and other large data settings all have signiﬁcant data sparsity—they do not appear to have attracted
as much study as their high-dimensional “duals” of dense data and sparse predictors.

In this paper  we investigate algorithms and their inherent limitations for solving problem (1) under
natural conditions on the data generating distribution. Recent work in the optimization and machine
learning communities has shown that data sparsity can be leveraged to develop parallel (and even
asynchronous [12]) optimization algorithms [13  14]  but this work does not consider the statistical
effects of data sparsity. In another line of research  Duchi et al. [4] and McMahan and Streeter [9]
develop “adaptive” stochastic gradient algorithms to address problems in sparse data regimes (2).
These algorithms exhibit excellent practical performance and have theoretical guarantees on their
convergence  but it is not clear if they are optimal—in that no algorithm can attain better statistical
performance—or whether they can leverage parallel computing as in the papers [12  14].

In this paper  we take a two-pronged approach. First  we investigate the fundamental limits of
optimization and learning algorithms in sparse data regimes. In doing so  we derive lower bounds
on the optimization error of any algorithm for problems of the form (1) with sparsity condition (2).
These results have two main implications. They show that in some scenarios  learning with sparse
data is quite difﬁcult  as essentially each coordinate j ∈ [d] can be relevant and must be optimized
for. In spite of this seemingly negative result  we are also able to show that the ADAGRAD algorithms
of [4  9] are optimal  and we show examples in which their dependence on the dimension d can be
made exponentially better than standard gradient methods.

As the second facet of our two-pronged approach  we study how sparsity may be leveraged in parallel
computing frameworks to give substantially faster algorithms that still achieve optimal sample com-
plexity in terms of the number of samples ξ used. We develop two new algorithms  asynchronous
dual averaging (ASYNCDA) and asynchronous ADAGRAD (ASYNCADAGRAD)  which allow asyn-
chronous parallel solution of the problem (1) for general convex f and X . Combining insights of
Niu et al.’s HOGWILD! [12] with a new analysis  we prove our algorithms achieve linear speedup in
the number of processors while maintaining optimal statistical guarantees. We also give experiments
on text-classiﬁcation and web-advertising tasks to illustrate the beneﬁts of the new algorithms.

2 Minimax rates for sparse optimization

We begin our study of sparse optimization problems by establishing their fundamental statistical and
optimization-theoretic properties. To do this  we derive bounds on the minimax convergence rate

from the distribution P as

of any algorithm for such problems. Formally  let"x denote any estimator for a minimizer of the
objective (1). We deﬁne the optimality gap N for the estimator"x based on N samples ξ1  . . .  ξ N
This quantity is a random variable  since"x is a random variable (it is a function of ξ1  . . .  ξ N ).

To deﬁne the minimax error  we thus take expectations of the quantity N   though we require a bit
more than simply E[N ]. We let P denote a collection of probability distributions  and we consider
a collection of loss functions F speciﬁed by a collection F of convex losses F : X× ξ → R. We
can then deﬁne the minimax error for the family of losses F and distributions P as

N ("x  F X   P ) := f ("x) − inf

f (x) = EP [F ("x; ξ)] − inf

x∈X

EP [F (x; ξ)] .

x∈X

(3)

∗
N (X  P F) := inf

!x

sup
P ∈P

sup
F ∈F

EP [N ("x(ξ1:N )  F X   P )] 

where the inﬁmum is taken over all possible estimators"x (an estimator is an optimization scheme 
or a measurable mapping"x :Ξ N →X ) .

2.1 Minimax lower bounds

Let us now give a more precise characterization of the (natural) set of sparse optimization problems
we consider to provide the lower bound. For the next proposition  we let P consist of distributions
supported on Ξ= {−1  0  1}d  and we let pj := P (ξj )= 0) be the marginal probability of ap-
pearance of feature j ∈{ 1  . . .   d}. For our class of functions  we set F to consist of functions F
satisfying the sparsity condition (2) and with the additional constraint that for g ∈ ∂xF (x; ξ)  we
have that the jth coordinate |gj|≤ Mj for a constant Mj < ∞. We obtain

2

Proposition 1. Let the conditions of the preceding paragraph hold. Let R be a constant such that
X⊃ [−R  R]d. Then

∗
N (X  P F) ≥

1
8

R

Mj min$pj 

√pj√N log 3% .

d#j=1

We provide the proof of Proposition 1 in the supplement A.1 in the full version of the paper  provid-
ing a few remarks here. We begin by giving a corollary to Proposition 1 that follows when the data
ξ obeys a type of power law: let p0 ∈ [0  1]  and assume that P (ξj )= 0) = p0j−α. We have
Corollary 2. Let α ≥ 0. Let the conditions of Proposition 1 hold with Mj ≡ M for all j  and
assume the power law condition P (ξj )= 0) = p0j−α on coordinate appearance probabilities. Then
(1) If d > (p0N )1/α 

∗
N (X  P F) ≥

(2) If d ≤ (p0N )1/α 

M R

8 & 2

2 − α’ p0

N ((p0N )
8 ’ p0
N +

2−α

2α − 1) +

p0

1 − α(d1−α − (p0N )

1

1 − α/2

d1− α

2 −

1

1 − α/2  .

M R

1−α

α )* .

∗
N (X  P F) ≥

Expanding Corollary 2 slightly  for simplicity assume the number of samples is large enough that
d ≤ (p0N )1/α. Then we ﬁnd that the lower bound on optimization error is of order
log d when α → 2  and M R’ p0
M R’ p0

2 when α< 2  M R’ p0

when α> 2. (4)

These results beg the question of tightness: are they improvable? As we see presently  they are not.

d1− α

N

N

N

2.2 Algorithms for attaining the minimax rate

To show that the lower bounds of Proposition 1 and its subsequent specializations are sharp  we
review a few stochastic gradient algorithms. We begin with stochastic gradient descent (SGD): SGD
repeatedly samples ξ ∼ P   computes g ∈ ∂xF (x; ξ)  then performs the update x ← ΠX (x − ηg) 
where η is a stepsize parameter and ΠX denotes Euclidean projection onto X . Standard analyses of

stochastic gradient descent [10] show that after N samples ξi  the SGD estimator"x(N ) satisﬁes

j=1 pj)

(5)

1
2

 

f (x) ≤O (1)

R2M (-d
√N

where R2 denotes the &2-radius of X . Dual averaging  due to Nesterov [11] (sometimes called
“follow the regularized leader” [5]) is a more recent algorithm. In dual averaging  one again samples
g ∈ ∂xF (x; ξ)  but instead of updating the parameter vector x one updates a dual vector z by
z ← z + g  then computes

E[f ("x(N ))] − inf

x∈X

x ← argmin

x∈X $$z  x% +

1
η

ψ(x)%  

where ψ(x) is a strongly convex function deﬁned over X (often one takes ψ(x) = 1
2). As
we discuss presently  the dual averaging algorithm is somewhat more natural in asynchronous and
parallel computing environments  and it enjoys the same type of convergence guarantees (5) as SGD.

2 2x22

The ADAGRAD algorithm [4  9] is an extension of the preceding stochastic gradient methods. It
maintains a diagonal matrix S  where upon receiving a new sample ξ  ADAGRAD performs the
following: it computes g ∈ ∂xF (x; ξ)  then updates
Sj ← Sj + g2
x"∈X $$z  x$% +

The dual averaging variant of ADAGRAD updates the usual dual vector z ← z + g; the update to x
is based on S and a stepsize η and computes

j for j ∈ [d].

x ← argmin

2η.x$  S

2 x$/% .

1

1

3

After N samples ξ  the averaged parameter"x(N ) returned by ADAGRAD satisﬁes

f (x) ≤O (1)

R∞M
√N

E[f ("x(N ))] − inf

x∈X

√pj 

d#j=1

(6)

where R∞ denotes the &∞-radius of X (cf. [4  Section 1.3 and Theorem 5]). By inspection  the
ADAGRAD rate (6) matches the lower bound in Proposition 1 and is thus optimal. It is interesting
to note  though  that in the power law setting of Corollary 2 (recall the error order (4))  a calculation
shows that the multiplier for the SGD guarantee (5) becomes R∞√d max{d(1−α)/2  1}  while ADA-
GRAD attains rate at worst R∞ max{d1−α/2  log d}. For α> 1  the ADAGRAD rate is no worse 
and for α ≥ 2  is more than √d/ log d better—an exponential improvement in the dimension.

3 Parallel and asynchronous optimization with sparsity

As we note in the introduction  recent works [12  14] have suggested that sparsity can yield beneﬁts
in our ability to parallelize stochastic gradient-type algorithms. Given the optimality of ADAGRAD-
type algorithms  it is natural to focus on their parallelization in the hope that we can leverage their
ability to “adapt” to sparsity in the data. To provide the setting for our further algorithms  we
ﬁrst revisit Niu et al.’s HOGWILD! [12]. HOGWILD! is an asynchronous (parallelized) stochastic
gradient algorithm for optimization over product-space domains  meaning that X in problem (1)
decomposes as X = X1 ×···×X d  where Xj ⊂ R. Fix a stepsize η> 0. A pool of independently
running processors then performs the following updates asynchronously to a centralized vector x:

1. Sample ξ ∼ P
2. Read x and compute g ∈ ∂xF (x; ξ)
3. For each j s.t. gj )= 0  update xj ← ΠXj (xj − ηgj).
Here ΠXj denotes projection onto the jth coordinate of the domain X . The key of HOGWILD! is that
in step 2  the parameter x is allowed to be inconsistent—it may have received partial gradient updates
from many processors—and for appropriate problems  this inconsistency is negligible. Indeed  Niu
et al. [12] show linear speedup in optimization time as the number of processors grow; they show
this empirically in many scenarios  providing a proof under the somewhat restrictive assumptions
that there is at most one non-zero entry in any gradient g and that f has Lipschitz gradients.

3.1 Asynchronous dual averaging

A weakness of HOGWILD! is that it appears only applicable to problems for which the domain X
is a product space  and its analysis assumes 2g20 = 1 for all gradients g. In effort to alleviate these
difﬁculties  we now develop and present our asynchronous dual averaging algorithm  ASYNCDA.
ASYNCDA maintains and upates a centralized dual vector z instead of a parameter x  and a pool of
processors perform asynchronous updates to z  where each processor independently iterates:

1. Read z and compute x := argminx∈X0$z  x% + 1

counter t and let x(t) = x

η ψ(x)1

// Implicitly increment “time”

2. Sample ξ ∼ P and let g ∈ ∂xF (x; ξ) // Let g(t) = g.
3. For j ∈ [d] such that gj )= 0  update zj ← zj + gj.
Because the actual computation of the vector x in ASYNCDA is performed locally on each processor
in step 1 of the algorithm  the algorithm can be executed with any proximal function ψ and domain
X . The only communication point between any of the processors is the addition operation in step 3.
Since addition is commutative and associative  forcing all asynchrony to this point of the algorithm
is a natural strategy for avoiding synchronization problems.

In our analysis of ASYNCDA  and in our subsequent analysis of the adaptive methods  we require a
measurement of time elapsed. With that in mind  we let t denote a time index that exists (roughly)
behind-the-scenes. We let x(t) denote the vector x ∈X computed in the tth step 1 of the ASYNCDA

4

algorithm  that is  whichever is the tth x actually computed by any of the processors. This quantity
τ =1 x(τ ).

exists and is recoverable from the algorithm  and it is possible to track the running sum-t

Additionally  we state two assumptions encapsulating the conditions underlying our analysis.
Assumption A. There is an upper bound m on the delay of any processor. In addition  for each
j ∈ [d] there is a constant pj ∈ [0  1] such that P (ξj )= 0) ≤ pj.
We also require certain continuity (Lipschitzian) properties of the loss functions; these amount to a
second moment constraint on the instantaneous ∂F and a rough measure of gradient sparsity.
Assumption B. There exist constants M and (Mj)d
x ∈X : E[2∂xF (x; ξ)22

2] ≤ M2 and for each j ∈ [d] we have E[|∂xj F (x; ξ)|] ≤ pjMj.

j=1 such that the following bounds hold for all

With these deﬁnitions  we have the following theorem  which captures the convergence behavior of
ASYNCDA under the assumption that X is a Cartesian product  meaning that X = X1 ×···×X d 
where Xj ⊂ R  and that ψ(x) = 1
2. Note the algorithm itself can still be efﬁciently parallelized
for more general convex X   even if the theorem does not apply.
Theorem 3. Let Assumptions A and B and the conditions in the preceding paragraph hold. Then

2 2x22

E& T#t=1

F (x(t); ξt) − F (x∗; ξt)* ≤

1
2η 2x∗22

2 +

η
2

T M2 + ηT m

j M 2
p2
j .

d#j=1

We now provide a few remarks to explain and simplify the result. Under the more stringent condition
j . Thus  for the
j   which upper bounds the Lipschitz continuity

that |∂xj F (x; ξ)|≤ Mj  Assumption A implies E[2∂xF (x; ξ)22
remainder of this section we take M2 =-d
constant of the objective function f . We then obtain the following corollary.
T -T
Corollary 4. Deﬁne"x(T ) = 1

2] ≤ -d
t=1 x(t)  and set η = 2x∗22 /M√T . Then
j M 2
p2
j .

j=1 pjM 2

j=1 pjM 2

M2x∗22√T

+ m 2x∗22
2M√T

d#j=1

E[f ("x(T )) − f (x∗)] ≤

Corollary 4 is nearly immediate: since ξt is independent of x(t)  we have E[F (x(t); ξt) | x(t)] =
f (x(t)); applying Jensen’s inequality to f ("x(T )) and performing an algebraic manipulation give the
result. If the data is suitably sparse  meaning that pj ≤ 1/m  the bound in Corollary 4 simpliﬁes to

3
2

M2x∗22√T

=

3

22-d

j=1 pjM 2

j 2x∗22

√T

 

(7)

which is the convergence rate of stochastic gradient descent even in centralized settings (5). The
convergence guarantee (7) shows that after T timesteps  the error scales as 1/√T ; however  if we
have k processors  updates occur roughly k times as quickly  as they are asynchronous  and in time
scaling as N/k  we can evaluate N gradient samples: a linear speedup.

E[f ("x(T )) − f (x∗)] ≤

3.2 Asynchronous AdaGrad

We now turn to extending ADAGRAD to asynchronous settings  developing ASYNCADAGRAD
(asynchronous ADAGRAD). As in the ASYNCDA algorithm  ASYNCADAGRAD maintains a shared
dual vector z (the sum of gradients) and the shared matrix S  which is the diagonal sum of squares
of gradient entries (recall Section 2.2). The matrix S is initialized as diag(δ2)  where δj ≥ 0 is an
initial value. Each processor asynchronously performs the following iterations:

1. Read S and z and set G = S

1

2 . Compute x := argminx∈X{$z  x% + 1

2η $x  Gx%}

// Implicitly

increment “time” counter t and let x(t) = x  S(t) = S

2. Sample ξ ∼ P and let g ∈ ∂F (x; ξ)
3. For j ∈ [d] such that gj )= 0  update Sj ← Sj + g2

j and zj ← zj + gj.

5

As in the description of ASYNCDA  we note that x(t) is the vector x ∈X computed in the tth “step”
of the algorithm (step 1)  and similarly associate ξt with x(t).

To analyze ASYNCADAGRAD  we make a somewhat stronger assumption on the sparsity properties
of the losses F than Assumption B.
Assumption C. There exist constants (Mj)d
x ∈X .

j=1 such that E[(∂xj F (x; ξ))2 | ξj )= 0] ≤ M 2

j m and X⊂ [−R∞  R∞]d. Then

j shows that Assumption C implies Assumption B with speciﬁc

Indeed  taking M2 = -j pjM 2
constants. We then have the following convergence result.
Theorem 5. In addition to the conditions of Theorem 3  let Assumption C hold. Assume that for all
j we have δ2 ≥ M 2
E3F (x(t); ξt) − F (x∗; ξt)4
∞E5+δ2 +
d#j=1

26 (1 + pjm)  MjR∞pjT%.

26 + ηE5+ T#t=1

gj(t)2  1

gj(t)2  1

min$ 1

T#t=1

T#t=1

j for all

R2

≤

η

It is possible to relax the condition on the initial constant diagonal term; we defer this to the full
version of the paper.

It is natural to ask in which situations the bound provided by Theorem 5 is optimal. We note that  as

T#t=1

26 ≤+δ2 +

t=1 x(t). By Jensen’s inequality  we have for any δ that

in the case with Theorem 3  we may obtain a convergence rate for f ("x(T ))− f (x∗) using convexity 
T -T
where"x(T ) = 1
E5+δ2 +
gj(t)2  1
E[f ("x(T )) − f (x∗)] ≤O (1)& 1

E[gj(t)2]  1
≤2δ2 + T pjM 2
Mj min78log(T )/T + pj

For interpretation  let us now make a few assumptions on the probabilities pj. If we assume that
pj ≤ c/m for a universal (numerical) constant c  then Theorem 5 guarantees that

which is the convergence rate of ADAGRAD except for a small factor of min{√log T /T  pj} in
addition to the usual8pj/T rate. In particular  optimizing by choosing η = R∞  and assuming
pj ! 1

T#t=1
∞ + η* d#j=1

T log T   we have convergence guarantee

  pj9  

√T

R2

(8)

j .

η

2

which is minimax optimal by Proposition 1.

E[f ("x(T )) − f (x∗)] ≤O (1)R∞

Mj min$√pj√T

  pj%  

d#j=1

In fact  however  the bounds of Theorem 5 are somewhat stronger: they provide bounds using the
expectation of the squared gradients gj(t) rather than the maximal value Mj  though the bounds are
perhaps clearer in the form (8). We note also that our analysis applies to more adversarial settings
than stochastic optimization (e.g.  to online convex optimization [5]). Speciﬁcally  an adversary may
choose an arbitrary sequence of functions subject to the random data sparsity constraint (2)  and our
results provide an expected regret bound  which is strictly stronger than the stochastic convergence
guarantees provided (and guarantees high-probability convergence in stochastic settings [3]). More-
over  our comments in Section 2 about the relative optimality of ADAGRAD versus standard gradient
methods apply. When the data is sparse  we indeed should use asynchronous algorithms  but using
adaptive methods yields even more improvement than simple gradient-based methods.

4 Experiments

In this section  we give experimental validation of our theoretical results on ASYNCADAGRAD and
ASYNCDA  giving results on two datasets selected for their high-dimensional sparsity.2

2In our experiments  ASYNCDA and HOGWILD! had effectively identical performance.

6

8

7

6

5

4

3

2

p
u
d
e
e
p
S

1

 

2

4

 

s
s
o
l

g
n
i
n
i
a
r
T

0.07

0.065

0.06

0.055

0.05

0.045

0.04

0.035

0.03

0.025

0.02

2

4

r
o
r
r
e

t
s
e
T

0.025

0.024

0.023

0.022

0.021

0.02

0.019

0.018

0.017

16

2

4

8

6

Number of workers

10

12

14

16

6

8

Number of workers

10

12

14

A-ADAGRAD
ASYNCDA

6

8

Number of workers

10

12

14

16

Figure 1. Experiments with URL data. Left: speedup relative to one processor. Middle: training
dataset loss versus number of processors. Right: test set error rate versus number of processors. A-
ADAGRAD abbreviates ASYNCADAGRAD.

Fixed stepsizes  training data  L2=0

Fixed stepsizes  test data  L2=0

Impact of L2 regularizaton on test error

A-AdaGrad η = 0.002
A-AdaGrad η = 0.002
A-AdaGrad η = 0.004
A-AdaGrad η = 0.004
A-AdaGrad η = 0.008
A-AdaGrad η = 0.008
A-AdaGrad η = 0.016
A-AdaGrad η = 0.016
A-DA η = 0.800
A-DA η = 1.600
A-DA η = 3.200

s
s
o
l
-
g
o
l
 
e
v
i
t
a
e
r

l

8

.

1

6
1

.

4

.

1

2

.

1

0

.

1

4
0

.

1

3
0
1

.

2
0

.

1

1
0

.

1

0
0

.

1

A-AdaGrad  η = 0.008 L2 = 0
A-AdaGrad  η = 0.008 L2 = 80
A-DA  η = 0.8 L2 = 0
A-DA  η = 0.8 L2 = 80

4
0

.

1

3
0
1

.

2
0

.

1

1
0

.

1

0
0

.

1

1

2

4

8 16

64

256

1

2

4

8

16

32

64

128

256

1

2

4

8

16

32

64

128

256

number of passes

number of passes

number of passes

Figure 2: Relative accuracy for various stepsize choices on an click-through rate prediction dataset.

4.1 Malicious URL detection

For our ﬁrst set of experiments  we consider the speedup attainable by applying ASYNCADAGRAD
and ASYNCDA  investigating the performance of each algorithm on a malicious URL prediction
task [7]. The dataset in this case consists of an anonymized collection of URLs labeled as malicious
(e.g.  spam  phishing  etc.) or benign over a span of 120 days. The data in this case consists of
2.4· 106 examples with dimension d = 3.2· 106 (sparse) features. We perform several experiments 
randomly dividing the dataset into 1.2 · 106 training and test samples for each experiment.
In Figure 1 we compare the performance of ASYNCADAGRAD and ASYNCDA after doing after
single pass through the training dataset. (For each algorithm  we choose the stepsize η for optimal
training set performance.) We perform the experiments on a single machine running Ubuntu Linux
with six cores (with two-way hyperthreading) and 32Gb of RAM. From the left-most plot in Fig. 1 
we see that up to six processors  both ASYNCDA and ASYNCADAGRAD enjoy the expected linear
speedup  and from 6 to 12  they continue to enjoy a speedup that is linear in the number of processors
though at a lesser slope (this is the effect of hyperthreading). For more than 12 processors  there is
no further beneﬁt to parallelism on this machine.

The two right plots in Figure 1 plot performance of the different methods (with standard errors)
versus the number of worker threads used. Both are essentially ﬂat; increasing the amount of par-
allelism does nothing to the average training loss or the test error rate for either method. It is clear 
however  that for this dataset  the adaptive ASYNCADAGRAD algorithm provides substantial per-
formance beneﬁts over ASYNCDA.

4.2 Click-through-rate prediction experiments

We also experiment on a proprietary datasets consisting of search ad impressions. Each example
corresponds to showing a search-engine user a particular text ad in response to a query string. From
this  we construct a very sparse feature vector based on the text of the ad displayed and the query
string (no user-speciﬁc data is used). The target label is 1 if the user clicked the ad and -1 otherwise.

7

(A) Optimized stepsize for each number of passes

(B) A-AdaGrad speedup

(D) Impact of training data ordering

5
1
0

.

1

0
1
0

.

1

5
0
0

.

1

0
0
0

.

1

A-DA  L2=80
A-AdaGrad  L2=80

p
u
d
e
e
p
s

i

e
z
s
p
e

t
s
 

e
v
i
t

l

a
e
r

2
1

8

4

0

2

1

0

1.001

1.002

1.003

1.004

1.005

1.006

1.007

1.008

target relative log-loss

(C) Optimal stepsize scaling

A-DA base η = 1.600
A-AdaGrad base η = 0.023

s
s
o
l
-
g
o

l
 

e
v
i
t

l

a
e
r

A-DA η = 1.600
A-AdaGrad η = 0.016

5
1
0

.

1

0
1
0

.

1

5
0
0

.

1

0
0
0

.

1

s
s
o
l
-
g
o

l
 

e
v
i
t

l

a
e
r

1

2

4

8

16

32

64

128

256

1

2

4

8

16

32

64

128

256

1

2

4

8

16

32

64

128

256

number of passes

number of passes

number of passes

Figure 3.
(A) Relative test-set log-loss for ASYNCDA and ASYNCADAGRAD  choosing the best
stepsize (within a factor of about 1.4×) individually for each number of passes. (B) Effective speedup
for ASYNCADAGRAD. (C) The best stepsize η  expressed as a scaling factor on the stepsize used for
one pass. (D) Five runs with different random seeds for each algorithm (with $2 penalty 80).

We ﬁt logistic regression models using both ASYNCDA and ASYNCADAGRAD. We run extensive
experiments on a moderate-sized dataset (about 107 examples  split between training and testing) 
which allows thorough investigation of the impact of the stepsize η  the number of training passes 3
and &2-regularization on accuracy. For these experiments we used 32 threads on 16 core machines
for each run  as ASYNCADAGRAD and ASYNCDA achieve similar speedups from parallelization.

On this dataset  ASYNCADAGRAD typically achieves an effective additional speedup over
ASYNCDA of 4× or more. That is  to reach a given level of accuracy  ASYNCDA generally needs
four times as many effective passes over the dataset. We measure accuracy with log-loss (the lo-
gistic loss) averaged over ﬁve runs using different random seeds (which control the order in which
the algorithms sample examples during training). We report relative values in Figures 2 and 3  that
is  the ratio of the mean loss for the given datapoint to the lowest (best) mean loss obtained. Our
results are not particularly sensitive to the choice of relative log-loss as the metric of interest; we
also considered AUC (the area under the ROC curve) and observed similar results.

Figure 2 shows relative log-loss as a function of the number of training passes for various stepsizes.
Without regularization  ASYNCADAGRAD is prone to overﬁtting: it achieves signiﬁcantly higher
accuracy on the training data (Fig. 2 (left))  but unless the stepsize is tuned carefully to the number
of passes  it will overﬁt (Fig. 2 (middle)). Fortunately  the addition of &2 regularization largely solves
this problem. Indeed  Figure 2 (right) shows that while adding an &2 penalty of 80 has very little
impact on ASYNCDA  it effectively prevents the overﬁtting of ASYNCADAGRAD.4

Fixing &2 regularization multiplier to 80  we varied the stepsize η over a multiplicative grid with res-
olution √2 for each number of passes and for each algorithm. Figure 3 reports the results obtained by
selecting the best stepsize in terms of test set log-loss for each number of passes. Figure 3(A) shows
relative log-loss of the best stepsize for each algorithm; 3(B) shows the relative time ASYNCDA
requires with respect to ASYNCADAGRAD to achieve a given loss. Speciﬁcally  Fig. 3(B) shows the
ratio of the number of passes the algorithms require to achieve a ﬁxed loss  which gives a broader
estimate of the speedup obtained by using ASYNCADAGRAD; speedups range from 3.6× to 12×.
Figure 3(C) shows the optimal stepsizes as a function of the best setting for one pass. The optimal
stepsize decreases moderately for ASYNCADAGRAD  but are somewhat noisy for ASYNCDA.

It is interesting to note that ASYNCADAGRAD’s accuracy is largely independent of the ordering of
the training data  while ASYNCDA shows signiﬁcant variability. This can be seen both in the error
bars on Figure 3(A)  and explicitly in Figure 3(D)  where we plot one line for each of the ﬁve random
seeds used. Thus  while on the one hand ASYNCDA requires somewhat less tuning of the stepsize
and &2 parameter  tuning ASYNCADAGRAD is much easier because of its predictable response.

3Here “number of passes” more precisely means the expected number of times each example in the dataset
is trained on. That is  each worker thread randomly selects a training example from the dataset for each update 
and we continued making updates until (dataset size) × (number of passes) updates have been processed.

4For both algorithms  this is accomplished by adding the term η80 "x"2

2 to the ψ function. We can achieve

slightly better results for ASYNCADAGRAD by varying the $2 penalty with the number of passes.

8

References

[1] P. Auer and C. Gentile. Adaptive and self-conﬁdent online learning algorithms. In Proceedings

of the Thirteenth Annual Conference on Computational Learning Theory  2000.

[2] P. B¨uhlmann and S. van de Geer. Statistics for High-Dimensional Data: Methods  Theory and

Applications. Springer  2011.

[3] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  September 2004.

[4] J. C. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[5] E. Hazan. The convex optimization approach to regret minimization.

In Optimization for

Machine Learning  chapter 10. MIT Press  2012.

[6] J. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms I & II.

Springer  New York  1996.

[7] J. Ma  L. K. Saul  S. Savage  and G. M. Voelker. Identifying malicious urls: An application of
large-scale online learning. In Proceedings of the 26th International Conference on Machine
Learning  2009.

[8] C. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. MIT

Press  1999.

[9] B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization.
In Proceedings of the Twenty Third Annual Conference on Computational Learning Theory 
2010.

[10] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[11] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Program-

ming  120(1):261–283  2009.

[12] F. Niu  B. Recht  C. R´e  and S. Wright. Hogwild: a lock-free approach to parallelizing stochas-

tic gradient descent. In Advances in Neural Information Processing Systems 24  2011.

[13] P. Richt´arik and M. Tak´aˇc. Parallel coordinate descent methods for big data optimization.

arXiv:1212.0873 [math.OC]  2012. URL http://arxiv.org/abs/1212.0873.

[14] M. Tak´aˇc  A. Bijral  P. Richt´arik  and N. Srebro. Mini-batch primal and dual methods for

SVMs. In Proceedings of the 30th International Conference on Machine Learning  2013.

9

,John Duchi
Michael Jordan
Brendan McMahan