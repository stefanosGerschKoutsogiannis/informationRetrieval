2018,Deep Homogeneous Mixture Models: Representation  Separation  and Approximation,At their core  many unsupervised learning models provide a compact representation of homogeneous density mixtures  but their similarities and differences are not always clearly understood. In this work  we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models)  hierarchical tensor formats and sum-product networks. Based on this connection  we then give a unified treatment of exponential separation in \emph{exact} representation size between deep mixture architectures and shallow ones. In contrast  for \emph{approximate} representation  we show that the conditional gradient algorithm can approximate any homogeneous mixture within $\epsilon$ accuracy by combining $O(1/\epsilon^2)$ ``shallow'' architectures  where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation.,Deep Homogeneous Mixture Models:

Representation  Separation  and Approximation

Department of Computer Science & Waterloo AI Institute

Priyank Jaini

University of Waterloo
pjaini@uwaterloo.ca

Pascal Poupart

University of Waterloo  Vector Institute & Waterloo AI Institute

ppoupart@uwaterloo.ca

Department of Computer Science & Waterloo AI Institute

Yaoliang Yu

University of Waterloo

yaoliang.yu@uwaterloo.ca

Abstract

At their core  many unsupervised learning models provide a compact representation
of homogeneous density mixtures  but their similarities and differences are not
always clearly understood. In this work  we formally establish the relationships
among latent tree graphical models (including special cases such as hidden Markov
models and tensorial mixture models)  hierarchical tensor formats and sum-product
networks. Based on this connection  we then give a uniﬁed treatment of expo-
nential separation in exact representation size between deep mixture architectures
and shallow ones. In contrast  for approximate representation  we show that the
conditional gradient algorithm can approximate any homogeneous mixture within
 accuracy by combining O(1/2) “shallow” architectures  where the hidden con-
stant may decrease (exponentially) with respect to the depth. Our experiments on
both synthetic and real datasets conﬁrm the beneﬁts of depth in density estimation.

Introduction

1
Multivariate density estimation  a widely studied problem in statistics and machine learning [28] 
is becoming even more relevant nowadays due to the availability of huge amounts of unlabeled
data in various applications. Many unsupervised and semi-supervised learning algorithms either
implicitly (e.g. generative adversarial networks) or explicitly estimate (some functional of) the
underlying density function. In this work  we study the problem of density estimation with an explicit
representation through ﬁnite mixture models (FMMs) [19]  which have endured thorough scientiﬁc
scrutiny over decades. The popularity of FMMs is largely due to their simplicity  interpretability 
and universality  in the sense that  given sufﬁciently many components (satisfying mild conditions) 
FMMs can approximate any distribution to an arbitrary level of accuracy [22].
Many familiar unsupervised models in machine learning  at their core  provide a compact represen-
tation of homogeneous density mixtures. This list includes (but is not limited to) hidden Markov
models (HMM)  the recently proposed tensorial mixture models (TMM) [26]  latent tree graphi-
cal models (LTM)[21]  hierarchical tensor formats (HTF) [13]  and sum-product networks (SPN)
[9; 24]. However  despite all being a certain form of FMM  the precise relationships among these
models are not always well-understood. Our ﬁrst contribution ﬁlls this gap: we prove (roughly) that

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

{HM M  T M M} ⊆ LT M ⊆ HT F ⊆ SP N. Moreover  converting from a lower to an upper class
can be achieved in linear time and without any increase in size. Our results not only clarify the
similarities and subtle differences between these widely-used models  but also pave the way for a
uniﬁed treatment of many properties of such models  using tools from linear algebra.
We next investigate the consequence of converting a deep mixture model into a shallow one. We ﬁrst
prove that the (nonnegative) tensor rank exactly characterizes the minimum size of a shallow SPN
(or LTM or HTF due to equivalence) that represents a given homogeneous mixture. Then  we show
that a generic “deep” SPN (with depth at least 2) can be exactly represented by a shallow SPN only
when the latter contains exponentially many product nodes. Our result extends signiﬁcantly those in
[7; 26; 10; 18; 8] in various aspects  but most saliently from the restrictive full binary tree [7; 26] to
any rooted tree. As a consequence  our results imply that a generic HMM (whose underlying tree
is “completely” unbalanced) cannot be exactly represented by any polynomially-sized shallow SPN 
which  to our best knowledge  has not been shown before.
From a practical point of view  exact representations are an overkill: it sufﬁces to approximate a
given density mixture with reasonable accuracy. Our third contribution demonstrates that under the
(cid:96)∞ metric  we can approximate any homogeneous density mixture within  accuracy by combining
O(1/2) shallow SPNs. However  our proof requires the knowledge of the target density hence is not
practical. Instead  borrowing a classic idea from [17] we show that minimizing the KL divergence
using the conditional gradient algorithm can also approximate any homogeneous mixture within 
accuracy by combining O(1/2) base SPNs  where the hidden constant decreases exponentially wrt
the depth of the base SPNs. Each iteration of the conditional gradient algorithm amounts to learning a
base SPN hence can be efﬁciently implemented. We conduct thorough experiments on both synthetic
and real datasets and conﬁrm the beneﬁts of depth in density estimation.
We proceed as follows: In §2 we introduce homogeneous density mixtures. In §3 we articulate the
relationships among various popular mixture models. §4 examines the exponential separation in exact
representation size between deep and shallow models while §5 turns into approximate representations.
We report our experiments in §6 and ﬁnally we conclude in §7. All proofs are deferred to Appendix C.

2 Density Estimation using Mixture Models
In this section  we introduce our main problem: how to estimate a multivariate density through an
explicit  ﬁnite homogeneous mixture. To set up the stage  let x = (x1  . . .   xd)  with xi ∈ Xi where
each Xi is a Borel (measurable) subset of the Euclidean space Ei. We equip a Borel measure µi
on Xi. All our subsequent measure-theoretic deﬁnitions are w.r.t. the Borel σ-ﬁeld of Xi and the
measure µi. Let X = X1 × ··· × Xd and µ = µ1 × ··· × µd be the product space and product
measure  respectively. For each i ∈ [d] := {1  . . .   d}  let Fi be a class of density functions (w.r.t.
µi) of the variable xi  and let Gi = conv(Fi) be its convex hull. The function class Fi is essentially
our basis of densities for the variable xi. Our setting here follows that in [18] and includes both
continuous and discrete distributions.
We are interested in constructing a ﬁnite density mixture [19]  using component densities from the

basis class F =(cid:83)d
i=1 Fi. We assume that our ﬁnite mixture f is “homogeneous ” i.e.
k1(cid:88)
k2(cid:88)

(xi) = (cid:104)W  (cid:126)f 1(x1) ⊗ ··· ⊗ (cid:126)f d(xd)(cid:105) 

··· kd(cid:88)

f (x) =

(1)

d(cid:89)
+ (cid:39) Rk1×···×kd
Rki

f i
ji

i=1

Wj1 j2 ... jd

  W ∈(cid:78)

j1=1

j2=1

jd=1

) ∈ F ki

i

i

+

1  . . .   f i
ki

where (cid:126)f i := (f i
is a d-order density tensor (nonnega-
tive and sum to 1)  and (cid:104)· ·(cid:105) is the standard inner product on the tensor product space. We refer to the
excellent book [13] and Appendix A for some basic deﬁnitions about tensors. By dropping linearly
dependent densities in each Fi we can assume w.l.o.g. the tensor representation W is unique.
There are a number of reasons for restricting to homogeneous mixtures: Firstly  this is the most
common choice for estimating a multivariate density function [28]. Secondly  we can always apply the
usual “homogenization” trick  i.e.  by enlarging the function class Fi and appending the (improper)
density 1 to each Fi. Thirdly  homogeneous densities are “universal” if each class Fi is  c.f. Appendix
A of [26]. In other words  any joint density can be approximated arbitrarily well by a homogeneous
density  provided that each marginal class Fi can approximate any marginal density arbitrarily well
and the size (i.e. ki) tends to ∞. See Appendix F.1 for some empirical veriﬁcations  where we
show that convex combinations of relatively few isotropic Gaussians can approximate mixtures of

2

Gaussians of full covariance matrices surprisingly well. Lastly  as we argue below  many known
models in machine learning are simply compact representations of homogeneous mixtures.

weight wuv. The value Tv at a product node v is the product of the values of its children (cid:81)
The value Tu at a sum node u is the weighted sum of the values of its children (cid:80)

3 Compact Representation of Homogeneous Mixtures
We now recall a few unsupervised learning models in machine learning and show that they have a
compact representation of homogeneous mixtures at their core. We prove the precise relationship
amongst them. Our results clarify the similarity and difference of these recent developments  and pave
the way for a uniﬁed treatment of depth separation (Section 4) and model approximation (Section 5).
Sum-Product Networks (SPN) [9; 24; 18] An SPN T is a rooted tree whose leaves are density
functions f i
j (xi) over each of the variables x1  . . .   xd and whose internal nodes are either a sum node
or a product node. Each edge (u  v) emanating from a sum node u has an associated nonnegative
u∈ch(v) Tu.
v∈ch(u) wuvTv.
The value of an SPN T is the expression evaluated at the root node  which we denote as T(x). The
scope of a node v in an SPN is the set of all variables that appear in the leaves of the sub-SPN rooted
at v. We only consider decomposable and complete SPNs  i.e.  the children of each sum node must
have the same scope and the children of each product node must have disjoint scopes. The main
advantage of a decomposable and complete SPN over a generic graphical model is that joint  marginal
and conditional queries can be answered by two network evaluations and hence  exact inference takes
linear time with respect to the size of the network [9; 24; 18]. In comparison  inference in Bayesian
Networks and Markov Networks may take exponential time in terms of the size of the network.
W.l.o.g. we can rearrange an SPN to have alternating sum and product layers (see Theorem C.1).
The latent variable semantics [23] as well as SPNs representing a mixture model over its leaf densities
[24] is well-known. It is also informally known that many tractable graphical models can be treated as
SPNs  but precise characterizations are scarce (see [29] which relates SPNs with Bayesian Networks).

Self-similar SPNs (S3PN) We call an SPN self-similar  if at every sum node  the sub-tree rooted at
each of its (product node) children is the same  except the weights at corresponding sum nodes and
the densities (but not the variables) at corresponding leaf nodes may differ. This special class of SPNs
is exactly equivalent to some recently proposed unsupervised learning models  as we show below.

(cid:81)d

j1 ... jk

j1 ... jk

(cid:80)

j1

···(cid:80)

wv γ

j1 ... jk

jk

Hierarchical Tensor Format (HTF) [13] We showed in (1) that a homogeneous mixture can be
identiﬁed with a tensor W  whose explicit storage can  however  be quite challenging since its size is
i=1 ki. HTF [13] aims at representing tensors compactly  hence can also be used for representing
homogeneous mixtures. An HTF consists of a dimension-partition rooted tree (DPT) T  d vector
spaces Vi with bases1 Fi at the d leaf nodes  and at most d − 1 internal nodes which are certain
subspaces of the tensor product of vector spaces at disjoint children nodes. Note that the dimension of
the tensor product U ⊗ V is the product of the dimensions of U and V. The key in HTF is to truncate
each tensor product with a (much smaller) subspace  hence keeping the total storage manageable.
Moreover  at each internal node v with k children nodes {vi}  instead of storing its r bases directly 
: γ ∈ [r]} such that  recursively  the γ-th basis at node v is
we store r coefﬁcient tensors {wv γ
vj1 ⊗ ··· ⊗ vjk  where {vji} consists of the bases at the i-th child node vi. To
our best knowledge  HTFs have not been recognized as SPNs previously  although they have been
used in a spectral method for latent variable models [27].
To turn an HTF into an SPN  more precisely an S3PN  we start from the root of the dimension-partition
tree T. For each internal node v with say r bases and say k children nodes {vi}  each of which has
ri bases themselves  we create three layers in the corresponding S3PN: in the ﬁrst layer we have r
sum nodes {Sv
  to the second
i=1 ri sum nodes
{Svi
}. Note that the weights
ji
wv γ
need not be positive or sum to 1 in HTF  although for representing a homogeneous mixture
we can make this choice and we call this subclass HTF+. Clearly  our construction is reversible hence
we can turn an S3PN into an equivalent HTF+ as well. The construction takes linear time and there is

}  and ﬁnally the third layer consists of(cid:80)k

γ}  each of which is (fully) connected  with respective weights wv γ

i=1 ri product nodes {Pv

j1 ... jk

j1 ... jk is connected to k sum nodes {Sv1

j1

layer of(cid:81)k

}. The product node Pv

j1 ... jk

  . . .   Svk
jk

1More generally frames  in particular  the elements need not be linearly independent.

3

H 2

+

Pr(H = 1)

Pr(H = 2)

×

×

{1  2  3  4}1

X1 X2 X3 X4

f 1
1

f 2
1

f 3
1

f 4
1

f 1
2

f 2
2

f 3
2

f 4
2

{1}2 {2}2 {3}2 {4}2

Figure 1: Left: A simple latent class model (special case of LTM). The superscript 2 indicates the
number of values the hidden variable H can take. Middle: The equivalent S3PN  where f i
j (xi) =
p(Xi = xi|H = j) is from the density class Fi. Right: The dimension-partition tree in an equivalent
HTF+. The superscript indicates the number of bases  which should be the same for sibling nodes.

f (x1  . . .   xd) =(cid:80)

···(cid:80)

W(h1  . . .   ht)(cid:81)d

j1 ... jk

j1 ... jk

no increase of representation size. See Figs.1 5 for simple illustrations2. In summary  HTF is exactly
S3PN with arbitrary weights.
Diagonal HTF (dHTF) [13] For later reference  let us call the subclass of HTFs whose coefﬁcient
tensors wv γ
(that deﬁne bases recursively at internal nodes of the DPT  see above) are diagonal
for all v and γ as dHTF  i.e.  siblings in the DPT must have the same number of bases (ri ≡ r)
(cid:54)= 0 only when j1 = . . . = jk. In neural network terminology  dHTFs are “locally
and wv γ
connected.” Compared to the fully connected HTF  dHTFs signiﬁcantly reduce the representation
i=1 ri = rk product nodes

size (at the expense of expressiveness  see Figure 7). For instance  the(cid:81)k

in the above conversion from HTF to S3PN are reduced to merely r product nodes.
Latent Tree Models (LTM) [21; 27; 5] An LTM is a rooted tree graphical model with observed
variables Xi on the leaves and hidden variables Hj on the internal nodes. Note that we allow observed
variables Xi to be either continuous or discrete but the hidden variables Hj can take only ﬁnitely
many values. Using conditional independence  the joint density of observed variables is given as

ht

h1

hπi

(xi) 

i=1 f i

(2)
where Hπi is the parent of Xi. From (2) it is clear that an LTM is a homogeneous density mixture 
whose tensor representation is given by the joint density W of the hidden variables. What is less
known3 is that LTMs are a special subclass of self-similar SPNs. It may appear that the size of
S3PN is larger than that of an equivalent LTM  but this is because S3PN also encodes the conditional
probability tables (CPT) into its structure whereas LTMs require other means to store CPTs. Note
also that to evaluate an LTM  one usually needs to run a separately designed algorithm (such as
message passing)  while in S3PN we evaluate the leaf densities and propagate in linear time to the
root. In summary  LTM is a subclass of S3PN with CPTs encoded as edge weights and with inference
simpliﬁed as network propagation. More precisely  LTM is exactly dHTF+  since conditioned on the
parent  all children nodes must depend on the same realization. An algorithm for converting LTMs
into equivalent S3PNs  along with more examples (Figs. 1-6)  can be found in Appendix B.1.
Tensorial Mixture Models (TMM) [26; 7; 6] TMM [26] is a recently proposed subclass of dHTF+
where nodes on the same level of the dimension-partition tree must have the same number of bases.
Clearly  TMM is a strict subclass of LTM since the latter only requires sibling nodes in the DPT to
have the same number of bases. We note that TMM  as deﬁned in [26]  also assumes the DPT to be
binary and balanced  i.e. each internal node has exactly two children  although this condition can be
easily relaxed. See Figure 2 and its reduced form in Appendix B.3 for a simple example. Further  in
Appendix B.4  we give an example of an LTM that is not a TMM.
Hidden Markov Models (HMM) [3; 25] HMM is a strict subclass of LTM. [14] recently observed
that HMM is equivalent to the tensor-train format  a special subclass of dHTF+ where the DPT is
binary and completely “imbalanced.” See Appendix B.5 for a simple example. In some sense  TMM
and HMM are the two opposite extremes within dHTF+ (or equivalently LTM).
Further  in Appendix B.6 we give an example of an S3PN that is not an LTM  and in Appendix B.7 
we give an example of an SPN that is not an S3PN  leading to the following summary:
Theorem 3.1. {TMM  HMM} ⊆ LTM = dHTF+ ⊆ HTF+ = S3PN ⊆ SPN  in the sense that we can
convert in linear time from a lower representation class to an upper one  without any increase in size.

2All of our illustrations of S3PN in the main text are drawn with some redundant leaves  for the sake of

making the self-similar property apparent. See Appendix B for the reduced (but equivalent) counterparts.

3As an evidence  we note that the recent survey [21] on LTMs did not mention SPNs at all.

4

{1  2  3  4}1

{1  2}3

{3  4}3

×

+

×

H 3
1

×

+

+

+

+

+

+

H 2
2

H 2
3

×

×

×

×

×

×

×

×

×

×

×

×

{1}2 {2}2

{3}2 {4}2

f 1
1 f 2
1

f 1
2 f 2
2

f 3
1 f 4
1

f 3
2 f 4
2

f 1
1 f 2
1

f 1
2 f 2
2

f 3
1 f 4
1

f 3
2 f 4
2

f 1
1 f 2
1

f 1
2 f 2
2

f 3
1 f 4
1

f 3
2 f 4
2

X1 X2

X3 X4

Figure 2: Left: A dimension-partition tree in HTF. The superscripts indicate the number of bases 
which should remain constant on each level. Middle: The equivalent S3PN. The leaf f i
j is the j-th
basis of vector space Vi. Right: An equivalent TMM. The superscripts indicate the number of values
each hidden variable can take (again  remaining constant on each level).

It is important to point out one subtlety here: any (complete and decomposable) SPN  if expanded at
the root  is a homogeneous mixture (c.f. (1)). Hence  any SPN is even equivalent to an LCM (i.e. an
LTM with one hidden variable taking many values  like in Figure 1)  at the expense of potentially
increasing the size (signiﬁcantly). Thus  the containment in Theorem 3.1 should be understood under
the premise of not increasing the representation size. It would be interesting to understand if the
containment is strict if only polynomial increase in size is allowed. We provide more comparing
examples in Appendix B for different models  and in the next section we discuss the (huge) size
consequence from converting a certain upper representation class to some lower one.

4 Depth Separation
In the previous section  we established relationships among different representation schemes for
homogeneous density mixtures. In this section  we prove an exponential separation in size when
converting one representation to another and extend the results in [10; 18; 7; 26]. The key is to exploit
the equivalence to HTF  which allows us to bound the model size using linear algebra.
We call a (complete and decomposable) SPN shallow if it has only one sum node  followed by
a layer of product nodes. Using the equivalence in Section 3  we know a shallow SPN (trivially
self-similar) is equivalent to an LCM (a latent tree model with one hidden node taking as many values
as the number of product nodes)  or an HTF+ whose DPT has depth 1 (c.f. Figure 1). Recall that
rank+(W) denotes the nonnegative rank of a tensor and nnz(W) is the number of nonzeros (c.f.
Appendix A). The leaf nodes in SPN (LTM) or the leaf bases in HTF are either from F (union of
linearly independent component densities) or G (the convex hull)  see the deﬁnitions in Section 2.
Our ﬁrst result characterizes the model capacity of shallow SPNs (LCMs):
Theorem 4.1. If a shallow SPN T  with leaf (input) nodes from G  represents the density mixture W 
then T has at least rank+(W) many product nodes. Conversely  there always exists a shallow SPN
that represents W using rank+(W) product nodes and 1 sum node.
In other words  the nonnegative rank characterizes the smallest size of shallow SPNs (LCMs) that
represent the density mixture W. Similarly  we can prove the following result when the leaf nodes
are from F instead of the convex hull G.
Theorem 4.2. If a shallow SPN T  with leaf nodes from F  represents the density mixture W  then
either T has at least nnz(W) product nodes or rank+(W) = 1. Conversely  there always exists a
shallow SPN that represents W using nnz(W) product nodes and 1 sum node.
Note that we always have rank(W) ≤ rank+(W) ≤ nnz(W)  thus the lower bound in Theorem 4.2
is stronger than that in Theorem 4.1. This is not surprising  because an SPN with leaf nodes from G
is the same as an SPN with leaf nodes from F and with an additional layer of sum nodes appended at
the bottom (to perform the convex hull operation). This difference already indicates that an additional
layer of sum nodes at the bottom can strictly increase the expressive power of SPNs. This distinction
between leaf nodes from F or from G  to our best knowledge  has not been noted before.
The signiﬁcance of Theorem 4.1 and Theorem 4.2 is that they give exact characterizations of the
model size of shallow SPNs  and they pave the way for comparing more interesting models. For
convenience  we state our next result in terms of LTMs  but the consequence for dHTFs or SPNs
should be clear  thanks to the equivalence in Theorem 3.1.

5

m(cid:89)

i=1

Theorem 4.3. Let an LTM T have d observed variables X = {X1  . . .   Xd} with parents Hi taking
ri values respectively. Assuming the CPTs of T are sampled from a continuous distribution  then
almost surely  the tensor representation W for T has rank at least

max

1≤m≤d/2

max

{S1 ... Sm  ¯S1 ...  ¯Sm}⊆X

min{ri  ¯ri  ki  ¯ki} 

(3)

where ki (¯ki) is the number of (linearly independent) component densities that Si ( ¯Si) has  and Si
( ¯Si) are non-siblings.
Corollary 4.4. In addition to the setting in Theorem 4.3  if each observed variable Xi has b sibling
observed variables and ri ≡ r ≤ k ≡ ki  then the tensor representation W has rank at least r(cid:98)d/b(cid:99).
Corollary 4.5. In addition to the setting in Theorem 4.3  if each observed variable Xi has no sibling
observed variables and ri ≡ r ≤ k ≡ ki  then the tensor representation W has rank at least r(cid:98)d/2(cid:99).
Combining Corollary 4.4 with Theorem 4.2 we conclude that an LTM T with d observed variables
Xi where every b of them share the same hidden parent node is equivalent to an LCM T(cid:48) where the
hidden node must take at least r(cid:98)d/b(cid:99) many values. Note that T has Θ(d/b) hidden variables  each
of them taking r values  thus the total size of the CPTs of T is Θ(rd/b) while the total size of that
of T(cid:48) is r(cid:98)d/b(cid:99)  an exponential blow-up. By combining Corollary 4.5 with Theorem 4.2 a similar
conclusion can be made for converting an HMM into a LCM. Of course  interpretation using SPNs is
also readily available: Almost all depth-L S3PNs (L ≥ 2) with weights sampled from a continuous
distribution can be written as a shallow SPN with necessarily exponentially many product nodes.
To our best knowledge  [10] was the ﬁrst to construct a polynomial that  while representable by a
polynomially-sized depth-log d SPN  would require exponentially many product nodes if represented
by a shallow SPN. However  the deep SPN given in [10  Figure 1] is not complete. Recently  [7]
proved that the existence result of [10] is in fact generic. However  the results of [7] and subsequent
work [26] are limited to full binary trees. In contrast  our general Theorem 4.3 holds for any tree  and
we allow non-sibling nodes to take different number of values. As a result  we are able to handle
HMMs  the opposite extreme of TMM. Another important point we want to emphasize is that the
exponential separation from a shallow (i.e. depth-1) tree can be achieved by increasing the depth by
merely 1  as opposed to the depth-log d constructions in [10; 26].
We end this section by making another observation about Theorem 4.3: It also allows us to compare
the model size of LTMs T1 and T2 where say T1  after removing its root R  is a subtree of T2. Indeed 
in this case we need only deﬁne the children nodes of R as “observed” variables. Then  T1 becomes
an LCM and T2 serves as T in Theorem 4.3  with observed variables as the children nodes of R. This
essentially extends [7  Theorem 3] from a full binary tree to any tree and allowing non-sibling nodes
to take different number of values.

5 Approximate Representation
In the previous section  we proved that homogeneous mixtures representable by “deep” architectures
(such as SPN or LTM) of polynomial size cannot be exactly represented by a shallow one with
sub-exponential size. In this section  we address a more intricate and relevant question: What if we
are only interested in an approximate representation?
To formulate the problem  let g and h be two homogeneous mixtures with tensor representation W
and Z  respectively. We consider the distance dist(g  h) := (cid:107)W − Z(cid:107) for some norm (cid:107) · (cid:107) speciﬁed
later. Using the characterization in Theorem 4.1 we formulate our approximation problem as follows.
Let ∆ be a perturbation tensor with (cid:107)∆(cid:107) ≤ . What is the minimum value for rank+(W + ∆)  i.e.
the size of a shallow SPN? This motivates the following deﬁnition adapted from [1]:

-rank+(W) = min

rank+(W + ∆) : (cid:107)∆(cid:107) ≤ 

= min

rank+(Z) : (cid:107)Z − W(cid:107) ≤ 

.

(4)

(cid:111)

(cid:110)

(cid:110)

(cid:111)

In other words  -rank+ is precisely the minimum size of a shallow SPN (LCM) that approximates
a speciﬁed mixture W with accuracy . We can similarly deﬁne -rank  where we replace the
nonnegative rank with the usual rank in (4). Note that the notion of -rank depends on the norm (cid:107) · (cid:107).
(cid:96)∞-norm Let the norm in the deﬁnition (4) be the usual (cid:96)∞ norm  and we signify this choice with
∞. In this setting  we can prove the following nearly-tight bound on the -rank.
the notation -rank

6

Theorem 5.1. Fix  > 0 and tensor W ∈ Rk1×···×kd. Then  for some (small) constant c > 0 

∞

(W) ≤ c(cid:107)W(cid:107)tr

 

2

-rank

(5)
where (cid:107)W(cid:107)tr is the tensor trace norm. A similar result holds for -rank
+ (W). The dependence on 
∞
is tight up to a log factor.
Note that the representative tensor W for a homogeneous density mixture f is nonnegative and sums
to 1  in which case (cid:107)W(cid:107)tr ≤ (cid:107)W(cid:107)1 = 1. Thus  very surprisingly  Theorem 5.1 conﬁrms that any
deep SPN (or any LTM or HTF+) can be approximated by some shallow SPN with accuracy  under
the (cid:96)∞ metric and with at most c/2 many product nodes. Of course  this does not contradict with
the impossibility results in [7] and [18]  because the accuracy  there is exponentially small.
Theorem 5.1 remains mostly of theoretical interest  though  because (i) a straightforward application
of Theorem 5.1 leads to a disappointing bound on the total variational distance between the two
i ki; (ii) in practical applications
we do not have access to W so the constructive algorithm in our proof does not apply.
KL divergence
In contrast to the above (cid:96)∞ approximation  we now give an efﬁcient algorithm to
approximate a homogeneous density mixture h  using a classic idea of [17]. We propose to estimate
h by minimizing the KL divergence over the convex hull4 of a hypothesis class H:

homogeneous mixtures f and g  due to scaling by the big constant(cid:81)

where KL(h(cid:107)g) := (cid:82) h(x) log h(x)

(6)
g(x) dµ(x)  and Wg is the representative tensor for the mixture g.
Following [17]  we apply the conditional gradient algorithm [12] to solve (6): Given gt−1  we ﬁnd

Wg∈conv(H)

min

KL(h(cid:107)g) 

(ηt  ft) ← arg

min

η∈[0 1] Wf∈H

KL(h(cid:107)(1 − η)gt−1 + ηf ) 

gt ← (1 − ηt)gt−1 + ηtft.

(7)

One can also simply set ηt = 2
solved based on an iid sample x1  . . .   xn hence is practical:

2+t  as is common in practice. Note that (7) can be approximately

(cid:88)n

max

η∈[0 1] Wf∈H

i=1

log[(1 − η)gt−1(xi) + ηf (xi)].

(8)

Using basically the same argument as in [17]  the above algorithm enjoys the following guarantee:

where δ = sup{log

KL(h(cid:107)gt) ≤ chδ/t 
ch = min{p ≥ 0 : Wh =(cid:80)p
(cid:104)W  (cid:126)f1⊗···⊗ (cid:126)fd(cid:105)
(cid:104)Z  (cid:126)f1⊗···⊗ (cid:126)fd(cid:105) : W Z ∈ H  x ∈ X}  and

i=1 λiWi Wi ∈ H  λ ≥ 0  1(cid:62)λ = 1}

(9)

(10)

larger than(cid:81)

is essentially the rank of the mixture h (with tensor representation Wh) w.r.t. the class H.
The important conclusion we draw from the above bound (9) is as follows: First  the constant ch is no
i ki if H is any of the classes in Theorem 3.1 (since we only consider ﬁnite homogeneous
mixtures h). Second  if the target density h is a small number of combinations of densities in H  then
ch is small and we can approximate h using the algorithm (7) efﬁciently. Third  ch can be vastly
different for different hypothesis classes H  as shown in Section 4. For instance  if h is a generic
TMM and H is the shallow class LCM  then ch is exponential in d  whereas if H is the class TMM 
then ch can be as small as 1. There is a trade-off though  since solving (8) for a simpler class (such as
LCM) is easier than a deeper one (such as TMM). We will verify this trade-off in our experiments.

6 Experiments
We perform experiments on both synthetic and real world data to reinforce our theoretical ﬁndings.
Firstly  we present experiments on synthetic data to demonstrate the expressive power of an SPN and
the algorithm proposed in (7)-(8) which we call SPN-CG. Next  we present two sets of experiments
on real world datasets and present results for image classiﬁcation under missing data.

7

Figure 3: Depth efﬁciency and performance of SPN-CG

Synthetic data Firstly  in appendix F.1 we conﬁrm that a Gaussian mixture model (GMM) with
full covariance matrices can be well approximated by a homogeneous mixture model represented by
an SPN learned using SPN-CG. Secondly  we generate 20 000 samples from a 16 dimensional GMM
under three different settings - (i) 8 component GMM with full covariance matrices  (ii) 8 component
GMM with diagonal covariance matrices and  (iii) GMMs represented by a deep SPN with 4 layers
- and estimate each using SPN-CG. We consider layers  L ∈ {1  2  3  4} where L = 1 corresponds
to a shallow network and L = 4 corresponds to a network in TMM (a full binary tree). For each
L  at every iteration of SPN-CG we add a network with L layers. In Figure 3  we plot the number
of iterations and the total running time until convergence w.r.t. the depth for each setting described
above. We make the following observations: As the depth (layer) increases  the number of iterations
decreases sharply  since adding a deeper network effectively is the same as adding exponentially
many shallower networks (conﬁrming Section 4). Moreover  although learning a deeper network in
each iteration is more expensive than learning a shallower network  the sharp decrease in iterations
full compensates this overhead and leads to a much reduced total running time. The advantage in
using deeper networks is more pronounced when the data is indeed generated from a deep model.

Image Classiﬁcation under Missing Data by Marginalization A natural setting to test the ef-
fectiveness of generative models like deep SPNs is for classiﬁcation in the regime of missing data.
Generative models can cope with missing data naturally through marginalizing the missing values 
effectively learning all possible completions for classiﬁcation. As stated earlier  SPNs are attractive
because inference  marginalization and evaluating conditionals is tractable and amounts to one pass
through the network. This is in stark contrast with discriminative models that often rely on either data
imputation techniques (which result in sub-optimal classiﬁcation) or by assuming the distribution of
missing values is same during train and test time; an assumption that is often not valid in practice.
We perform experiments on MNIST [15] for digit classiﬁcation and small NORB [16] for 3D object
recognition under the MAR (missing at random) regime as described in [26] (Section 3). We
experiment with two missing distributions- (i) an i.i.d. mask with a ﬁxed probability of missing each
pixel  and (ii) a mask obtained by the union of rectangles of a certain size  each positioned uniformly
at random in the image. Concretely  let P (X  Y) be the joint distribution over the images (X ∈ Rd)
and labels Y ∈ [M ]. Further  let Z be a random binary vector conditioned on X = x with distribution
Q(Z|X = x). To generate images with missing pixels  we sample z ∈ {0  1}d and consider the
vector x (cid:12) z. A pixel xi  i ∈ [d] is considered missing if zi = 0 in which case the corresponding
coordinate in x (cid:12) z holds ∗ and it holds xi if zi = 1. In the MAR setting that we consider for our
experiments  Q(Z = z|X = x) is a function of both z and x but is independent of changes to xi
if zi = 0 i.e. Z is independent of missing pixels. As described in [26]  the optimal classiﬁcation
rule in the MAR regime is h∗(x (cid:12) z) = P (Y = y|w(x  z)) where w(x  z) is the realization when X
coincides with x on coordinates i for which zi = 1.
Our major goal with these experiments is to test our algorithm SPN-CG for high-dimensional real
world settings and show the efﬁcacy of learning SPNs by increasing their expressiveness iteratively.
Therefore  we directly adapt the experiments as presented in [26]. Speciﬁcally  we adapt the code
of HT-TMM for our SPN-CG by following the details in [26]. In each iteration of our algorithm 
we add an SPN structure exactly similar to HT-TMM. Therefore  the ﬁrst iteration of our algorithm
(i.e. SPN-CG1) amounts to a structure similar to HT-TMM while additional iterations increase the
network capacity. For each iteration  we train the network using an AdamSGD variant with a base
learning rate of 0.03 and momentum parameters β1 = β2 = 0.9. For each added network structure 
we train the model for 22 000 iterations for MNIST and 40 000 for NORB.

4This is similar in spirit to [20; 2] which learn mixture of trees  but the algorithms are quite different.

8

12341020304050IterationsGMM : 16 dims  8 components  full covariance1234Layers600700800900Running time (s)123402468IterationsGMM : 16 dims  8 components  diagonal covariance1234Layers50607080Running time (s)123010203040Iterations8 dim GMM from a 4 layered deep SPN123Layers200300400Running time (s)Figure 4: Performance of SPN-CG for missing data on MNIST and NORB

Due to space limit Figure 4 only presents results comparing our model with (i) data imputation
techniques that complete missing pixels with zeros or NICE [11]  a generative model suited for
inpainting  and ﬁnally using a ConvNet for prediction  (ii) an SPN with structure learned using data
as proposed in [24] augmented with a class variable to maximize joint probability  and (iii) shallow
networks to demonstrate the beneﬁts of depth. A more comprehensive ﬁgure showing comparisons
with several other algorithms is given in appendix F.2  along with details.
SPN-CG1 and SPN-CG3 in Figure 4 stand for one and three iterations of our algorithm respectively.
The results show that SPN-CG performs well in all regimes of missing data for both MNIST
and NORB. Furthermore  other generative models including SPN with structure learning perform
comparably only when a few pixels are missing but perform very poorly as compared to SPN-CG
when larger amounts of data is missing. Our results here complement those in [26] where these
experiments were ﬁrst reported with state of the art results.

7 Conclusion
We have formally established the relationships among some popular unsupervised learning models 
such as latent tree graphical models  hierarchical tensor formats and sum-product networks  based
on which we further provided a uniﬁed treatment of exponential separation in exact representation
size between deep architectures and shallow ones. Surprisingly  for approximate representation  the
conditional gradient algorithm can approximate any homogeneous mixture within accuracy  by
combining O(1/2) shallow models  where the hidden constant may decrease exponentially wrt the
depth. Experiments on both synthetic and real datasets conﬁrmed our theoretical ﬁndings.

Acknowledgement

The authors gratefully acknowledge support from the NSERC discovery program.

References
[1] Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics 

Probability and Computing  18(1-2):3–15  2009.

[2] Animashree Anandkumar  Daniel Hsu  Furong Huang  and Sham M. Kakade. Learning mixtures

of tree graphical models. In Advances in Neural Information Processing Systems  2012.

[3] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of ﬁnite state

markov chains. The annals of mathematical statistics  37(6):1554–1563  1966.

[4] Richard Caron and Tim Traynor. The zero set of a polynomial. Technical report  2005.

[5] Myung Jin Choi  Vincent Y. F. Tan  Animashree Anandkumar  and Alan S.Willsky. Learning

latent tree graphical models. Journal of Machine Learning Research  12:1771–1812  2011.

[6] Nadav Cohen  Or Sharir  Yoav Levine  Ronen Tamari  David Yakira  and Amnon Shashua.
Analysis and design of convolutional networks via hierarchical tensor decompositions  2017.
arXiv:1705.02302v4.

[7] Nadav Cohen  Or Sharir  and Amnon Shashua. On the expressive power of deep learning: A

tensor analysis. In Conference on Learning Theory  pages 698–728  2016.

9

0.00.250.50.750.90.950.99Probability of missing pixels10%20%30%40%50%60%70%80%90%100%AccuracyMNIST (i.i.d. corruption)ZeroNICESPNShallow_NetSPN-CG1SPN-CG3(1 7)(2 7)(3 7)(1 11)(2 11)(3 11)(1 15)(2 15)(3 15)size of missing rectangles20%30%40%50%60%70%80%90%100%accuracyMNIST (missing rectangles)ZeroNICESPNShallow_NetSPN-CG1SPN-CG30.00.250.50.750.90.950.99Probability of missing pixels15%25%35%45%55%65%75%85%95%AccuracyNORB (i.i.d. corruption)ZeroNICESPNShallow_netSPN-CG1SPN-CG3(1 7)(2 7)(3 7)(1 11)(2 11)(3 11)(1 15)(2 15)(3 15)size of missing rectangles15%25%35%45%55%65%75%85%95%accuracyNORB (missing rectangles)ZeroNICEShallow_NetSPN-CG1SPN-CG3[8] Nadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor

decompositions. In ICML  2016.

[9] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the

ACM (JACM)  50(3):280–305  2003.

[10] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in

Neural Information Processing Systems  pages 666–674  2011.

[11] Laurent Dinh  David Krueger  and Yoshua Bengio. Nice: Non-linear independent components

estimation. arXiv preprint arXiv:1410.8516  2014.

[12] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research

Logistics Quarterly  3(1-2):95–110  1956.

[13] Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor Calculus. Springer  2012.
[14] M. Ishteva. Tensors and latent variable models. In The 12th International Conference on Latent

Variable Analysis and Signal Separation (LVA/ICA)  pages 49–â ˘A¸S55  2015.

[15] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[16] Yann LeCun  Fu Jie Huang  and Leon Bottou. Learning methods for generic object recognition
with invariance to pose and lighting. In Computer Vision and Pattern Recognition  2004. CVPR
2004. Proceedings of the 2004 IEEE Computer Society Conference on  volume 2  pages II–104.
IEEE  2004.

[17] Jonathan Q Li and Andrew R Barron. Mixture density estimation. In Advances in neural

information processing systems  pages 279–285  2000.

[18] James Martens and Venkatesh Medabalimi. On the expressive efﬁciency of sum product

networks. arXiv preprint arXiv:1411.7717  2014.

[19] Geoffrey McLachlan and David Peel. Finite mixture models. John Wiley & Sons  2004.
[20] Marina Meila and Michael I. Jordan. Learning with mixtures of trees. Journal of Machine

Learning Research  1:1–48  2000.

[21] Raphaël Mourad  Christine Sinoquet  Nevin L. Zhang  Tengfei Liu  and Philippe Leray. A
survey on latent tree models and applications. Journal of Artiﬁcial Intelligence Research 
47:157–203  2013.

[22] Hien D Nguyen and Geoffrey J McLachlan. On approximations via convolution-deﬁned mixture

models. arXiv preprint arXiv:1611.03974  2016.

[23] Robert Peharz  Robert Gens  Franz Pernkopf  and Pedro Domingos. On the latent variable
interpretation in sum-product networks. IEEE transactions on pattern analysis and machine
intelligence  39(10):2030–2044  2017.

[24] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In

Uncertainty in Artiﬁcial Intelligence. UAI  2011.

[25] Lawrence R Rabiner. A tutorial on hidden markov models and selected applications in speech

recognition. Proceedings of the IEEE  77(2):257–286  1989.

[26] Or Sharir  Ronen Tamari  Nadav Cohen  and Amnon Shashua. Tensorial mixture models  2018.

arXiv:1610.04167v5.

[27] Le Song  Haesun Park  Mariya Ishteva  Ankur Parikh  and Eric Xing. Hierarchical tensor

decomposition of latent tree graphical models. In ICML  2013.

[28] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2009.
[29] Han Zhao  Mazen Melibari  and Pascal Poupart. On the relationship between sum-product
networks and bayesian networks. In International Conference on Machine Learning  pages
116–124  2015.

10

,Priyank Jaini
Pascal Poupart
Yaoliang Yu