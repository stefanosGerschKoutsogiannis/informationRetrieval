2016,Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables,We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences.  Yet  being a global property of the graph  it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task  scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.,Learning Treewidth-Bounded Bayesian Networks

with Thousands of Variables

Mauro Scanagatta
IDSIA∗  SUPSI†   USI‡
Lugano  Switzerland
mauro@idsia.ch

Giorgio Corani

IDSIA∗  SUPSI†   USI‡
Lugano  Switzerland

giorgio@idsia.ch

Cassio P. de Campos

Queen’s University Belfast

Northern Ireland  UK

c.decampos@qub.ac.uk

Marco Zaffalon

IDSIA∗

Lugano  Switzerland

zaffalon@idsia.ch

Abstract

We present a method for learning treewidth-bounded Bayesian networks from
data sets containing thousands of variables. Bounding the treewidth of a Bayesian
network greatly reduces the complexity of inferences. Yet  being a global property
of the graph  it considerably increases the difﬁculty of the learning process. Our
novel algorithm accomplishes this task  scaling both to large domains and to large
treewidths. Our novel approach consistently outperforms the state of the art on
experiments with up to thousands of variables.

Introduction

1
We consider the problem of structural learning of Bayesian networks with bounded treewidth 
adopting a score-based approach. Learning the structure of a bounded treewidth Bayesian network is
an NP-hard problem (Korhonen and Parviainen  2013). Yet learning Bayesian networks with bounded
treewidth is necessary to allow exact tractable inference  since the worst-case inference complexity is
exponential in the treewidth k (under the exponential time hypothesis) (Kwisthout et al.  2010).
A pioneering approach  polynomial in both the number of variables and the treewidth bound  has
been proposed in Elidan and Gould (2009). It incrementally builds the network; at each arc addition
it provides an upper-bound on the treewidth of the learned structure. The limit of this approach is that 
as the number of variables increases  the gap between the bound and the actual treewidth becomes
large  leading to sparse networks. An exact method has been proposed in Korhonen and Parviainen
(2013)  which ﬁnds the highest-scoring network with the desired treewidth. However  its complexity
increases exponentially with the number of variables n. Thus it has been applied in experiments with
15 variables at most. Parviainen et al. (2014) adopted an anytime integer linear programming (ILP)
approach  called TWILP. If the algorithm is given enough time  it ﬁnds the highest-scoring network
with bounded treewidth. Otherwise it returns a sub-optimal DAG with bounded treewidth. The ILP
problem has an exponential number of constraints in the number of variables; this limits its scalability 
even if the constraints can be generated online. Berg et al. (2014) casted the problem of structural
learning with limited treewidth as a problem of weighted partial Maximum Satisﬁability. They solved
the problem exactly through a MaxSAT solver and performed experiments with 30 variables at most.
Nie et al. (2014) proposed an efﬁcient anytime ILP approach with a polynomial number of constraints

∗Istituto Dalle Molle di studi sull’Intelligenza Artiﬁciale (IDSIA)
†Scuola universitaria professionale della Svizzera italiana (SUPSI)
‡Università della Svizzera italiana (USI)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

in the number of variables. Yet they report that the quality of the solutions quickly degrades as the
number of variables exceeds a few dozens and that no satisfactory solutions are found with data sets
containing more than 50 variables. Approximate approaches are therefore needed to scale to larger
domains.
Nie et al. (2015) proposed the method S2. It exploits the notion of k-tree  which is an undirected
maximal graph with treewidth k. A Bayesian network whose moral graph is a subgraph of a k-tree
has treewidth bounded by k. S2 is an iterative algorithm. Each iteration consists of two steps: a)
sampling uniformly a k-tree from the space of k-trees and b) recovering a DAG whose moral graph
is a sub-graph of the most promising sampled k-tree. The goodness of the k-tree is assessed via a
so-called informative score. Nie et al. (2016) further reﬁne this idea  obtaining via A* the k-tree
which maximizes the informative score. This algorithm is called S2+.
Recent structural learning algorithms with unbounded treewidth (Scanagatta et al.  2015) can cope
with thousands of variables. Yet the unbounded treewidth provides no guarantee about the tractability
of the inferences of the learned models. We aim at ﬁlling this gap  learning treewidth-bounded
Bayesian network models in domains with thousands of variables.
We propose two novel methods for learning Bayesian networks with bounded treewidth. They exploit
the fact that any k-tree can be constructed by an iterative procedure that adds one variable at a time.
We propose an iterative procedure that  given an order on the variables  builds a DAG G adding one
variable at a time. The moral graph of G is ensured to be subgraph of a k-tree. The k-tree is designed
as to maximize the score of the resulting DAG. This is a major difference with respect to previous
works (Nie et al.  2015  2016) in which the k-trees were randomly sampled. We propose both an
exact and an approximated variant of our algorithm; the latter is necessary to scale to thousands of
variables.
We discuss that the search space of the presented algorithms does not span the whole space of
bounded-treewidth DAGs. Yet our algorithms consistently outperform the state-of-the-art competitors
for structural learning with bounded treewidth. For the ﬁrst time we present experimental results for
structural learning with bounded treewidth for domains involving up to ten thousand variables.
Software and supplementary material are available from http://blip.idsia.ch.
2 Structural learning
Consider the problem of learning the structure of a Bayesian network from a complete data set of N
instances D = {D1  ...  DN}. The set of n categorical random variables is X = {X1  ...  Xn}. The
goal is to ﬁnd the best DAG G = (V  E)  where V is the collection of nodes and E is the collection
of arcs. E can be represented by the set of parents Π1  ...  Πn of each variable.
Different scores can be used to assess the ﬁt of a DAG; we adopt the Bayesian information criterion
(or simply BIC). The BIC score is decomposable  being constituted by the sum of the scores of the
individual variables:

BIC(G) =

BIC(Xi  Πi) =

(LL(Xi|Πi) + Pen(Xi  Πi)) =

i=1

π∈|Πi| x∈|Xi| Nx π

ˆθx|π − log N
2

(|Xi| − 1)(|Πi|))

n(cid:88)

n(cid:88)
n(cid:88)
(cid:88)

i=1

=

(

i=1

where ˆθx|π is the maximum likelihood estimate of the conditional probability P (Xi = x|Πi = π) 
Nx π represents the number of times (X = x ∧ Πi = π) appears in the data set  and | · | indicates the
size of the Cartesian product space of the variables given as argument. Thus |Xi| is the number of
states of Xi and |Πi| is the product of the number of states of the parents of Xi.
Exploiting decomposability  we ﬁrst identify independently for each variable a list of candidate
parent sets (parent set identiﬁcation). Later  we select for each node the parent set that yields the
highest-scoring treewidth-bounded DAG (structure optimization).

2

both v1 and v2;

2.1 Treewidth and k-trees
We illustrate the concept of treewidth following the notation of Elidan and Gould (2009). We
denote an undirected graph as H = (V  E) where V is the vertex set and E is the edge set. A tree
decomposition of H is a pair (C T ) where C = {C1  C2  ...  Cm} is a collection of subsets of V and
T is a tree on C  so that:
i=1 Ci = V ;

• ∪m
• for every edge which connects the vertices v1 and v2  there is a subset Ci which contains
• for all i  j  k in {1  2  ..m} if Cj is on the path between Ci and Ck in T then Ci ∩ Ck ⊆ Cj.
The width of a tree decomposition is max(|Ci|) − 1 where |Ci| is the number of vertices in Ci. The
treewidth of H is the minimum width among all possible tree decompositions of G.
The treewidth can be equivalently deﬁned in terms of a triangulation of H. A triangulated graph is an
undirected graph in which every cycle of length greater than three contains a chord. The treewidth of
a triangulated graph is the size of the maximal clique of the graph minus one. The treewidth of H is
the minimum treewidth over all the possible triangulations of H.
The treewidth of a Bayesian network is characterized with respect to all possible triangulations of its
moral graph. The moral graph M of a DAG is an undirected graph that includes an edge i − j for
every edge i → j in the DAG and an edge p − q for every pair of edges p → i  q → i in the DAG.
The treewidth of a DAG is the minimum treewidth over all the possible triangulations of its moral
graph M. Thus the maximal clique of any moralized triangulation of G is an upper bound on the
treewidth of the model.

Incremental treewidth-bounded structure learning

k-trees An undirected graph Tk = (V  E) is a k-tree if it is a maximal graph of tree-width k: any
edge added to Tk increases its treewidth. A k-tree is inductively deﬁned as follows (Patil  1986).
Consider a (k + 1)-clique  namely a complete graph with k + 1 nodes. A (k + 1)-clique is a k-tree. A
(k + 1)-clique can be decomposed into multiple k-cliques. Let us denote by z a node not yet included
in the list of vertices V . Then the graph obtained by connecting z to every node of a k-clique of Tk
is also a k-tree. The treewidth of any subgraph of a k-tree (partial k-tree) is bounded by k. Thus a
DAG whose moral graph is subgraph of a k-tree has treewidth bounded by k.
3
Our approach for the structure optimization task proceeds by repeatedly sampling an order ≺ over
the variables and then identifying the highest-scoring DAG with bounded treewidth consistent with
the order. An effective approach for structural learning based on order sampling has been introduced
by Teyssier and Koller (2012); however it does not enforce any treewidth constraint.
The size of the search space of orders is n!; this is smaller than the search space of the k-trees 
O(enlog(nk)). Once the order ≺ is sampled  we incrementally learn the DAG. At each iteration the
moralization of the DAG is by design a subgraph of a k-tree. The treewidth of the DAG eventually
obtained is thus bounded by k. The algorithm proceeds as follows.
Initialization The initial k-tree Kk+1 is constituted by the complete clique over the ﬁrst k + 1
variables in the order. The initial DAG Gk+1 is learned over the same k + 1 variables. Since (k + 1)
is a tractable number of variables  we exactly learn Gk+1 adopting the method of Cussens (2011).
The moral graph of Gk+1 is a subgraph of Kk+1 and thus Gk+1 has bounded treewidth.
Addition of the subsequent nodes We then iteratively add each remaining variable in the order.
Consider the next variable in the order  X≺i  where i ∈ {k + 2  ...  n}. Let us denote by Gi−1
and Ki−1 the DAG and the k-tree which have to be updated by adding X≺i. We add X≺i to Gi−1 
constraining its parent set Π≺i to be a k-clique (or a subset of) in Ki−1. This yields the updated DAG
Gi. We then update the k-tree  connecting X≺i to such k-clique. This yields the k-tree Ki; it contains
an additional k + 1-clique compared to Ki−1. By construction  Ki is also a k-tree. The moral graph
of Gi cannot add arc outside this (k + 1)-clique; thus it is a subgraph of Ki.
Pruning orders The initial k-tree Kk+1 and the initial DAG Gk+1 depend on which are the ﬁrst
k + 1 variables in the order  but not on their relative positions. Thus all the orders which differ only

3

i(cid:88)

n(cid:88)

best(X≺j) .

as for the relative position of the ﬁrst k + 1 elements are equivalent for our algorithm: they yield the
same Kk+1 and Gk+1. Thus once we sample an order and perform structural learning  we prune the
(k + 1)! − 1 orders which are equivalent to the current one.
In order to choose the parent set to be assigned to each variable added to the graph we propose two
algorithms: k-A* and k-G.
3.1 k-A*
We formulate the problem as a shortest path ﬁnding problem. We deﬁne each state as a step towards
the completion of the structure  where a new variable is added to the DAG G. Given X≺i the variable
assigned in the state S  we deﬁne a successor state of S for each k-clique to which we can link
X≺i+1. The approach to solve the problem is based on a path-ﬁnding A* search  with cost function
for state S deﬁned as f (S) = g(S) + h(S). The goal is to ﬁnd the state which minimizes f (S) once
all variables have been assigned.
We deﬁne g(S) and h(S) as:

g(S) =

score(X≺j Π≺j)  

h(S) =

j=0

j=i+1

g(S) is the cost from the initial state to S; it corresponds to the sum of scores of the already assigned
parent sets.
h(S) is the estimated cost from S to the goal. It is the sum of the best assignable parent sets for the
remaining variables. Variable Xa can have Xb as parent only if Xb ≺ Xa.
The A* approach requires the h function to be admissible. The function h is admissible if the
estimated cost is never greater than the true cost to the goal state. Our approach satisﬁes this property
since the true cost of each step (score of chosen parent set for X≺i+1) is always equal to or greater
than the estimated one (the score of the best selectable parent set for X≺i+1).
The previous discussion implies that h is consistent  meaning that for any state S and its successor
T   h(S) ≤ h(T ) + c(S  T )  where c(S  T ) is the cost of the edges added in T . The function f is
monotonically non-decreasing on any path  and the algorithm is guaranteed to ﬁnd the optimal path
as long as the goal state is reachable. Additionally there is no need to process a node more than once 
as no node will be explored a second time with a lower cost.
3.2 k-G
A very high number of variables might prevent the use of k-A*. For those cases we propose k-G as a
greedy alternative approach  which chooses at each step the best local parent set. Given the set of
existing k-clique in K as KC  we choose as parent set for X≺i:
score(π) .

ΠX≺i = argmax
π⊂c c∈KC

3.3 Space of learnable DAGs
A reverse topological order is an order {v1  ...vn} over the vertexes V of a DAG in which each vi
appears before its parents Πi. The search space of our algorithms is restricted to the DAGs whose
reverse topological order  when used as variable elimination order  has treewidth k. This prevents
recovering DAGs which have bounded treewidth but lack this property.
We start by proving by induction that the reverse topological order has treewidth k in the DAGs
recovered by our algorithms. Consider the incremental construction of the DAG previously discussed.
The initial DAG Gk+1 is induced over k + 1 variables; thus every elimination ordering has treewidth
bounded by k.
For the inductive case  assume that Gi−1 satisﬁes the property. Consider the next variable in the
order  X≺i  where i ∈ {k + 2  ...  n}. Its parent set Π≺i is a subset of a k-clique in Ki−1. The
only neighbors of X≺i in the updated DAG Gi are its parents Π≺i. Consider performing variable
elimination on the moral graph of Gi  using a reverse topological order. Then X≺i will be eliminated
before Π≺i  without introducing ﬁll-in edges. Thus the treewidth associated to any reverse topological
order is bounded by k. This property inductively applies to the addition also of the following nodes
up to X≺n.

4

Inverted trees An example of DAG non recoverable by our algorithms is the speciﬁc class of
polytrees that we call inverted trees  that is  DAGs with out-degree equal to one. An inverted tree
with m levels and treewidth k can be built as follows. Take the root node (level one) and connect it to
k child nodes (level two). Connect each node of level two to k child nodes (level three). Proceed in
this way up to the m-th level and then invert the direction of all the arcs.
Figure 1 shows an inverted tree with k=2 and m=3. It has treewidth two  since its moral graph
is constituted by the cliques {A B E}  {C D F}  {E F G}. The treewidth associated to the reverse
topological order is instead three  using the order G  F  D  C  E  A  B.

A

B

C

D

E

F

G

Figure 1: Example of inverted tree.

If we run our algorithms with bounded treewidth k=2  it will be unable to recover the actual inverted
tree. It will instead identify a high-scoring DAG whose reverse topological order has treewidth 2.
4 Experiments
We compare k-A*  k-G  S2  S2+ and TWILP in various experiments. We compare them through
an indicator which we call W-score: the percentage of worsening of the BIC score of the selected
treewidth-bounded method compared to the score of the Gobnilp solver Cussens (2011). Gobnilp
achieves higher scores than the treewidth-bounded methods since it has no limits on the treewidth.
Let us denote by G the BIC score achieved by Gobnilp and by T the BIC score obtained by the given
treewidth-bounded method. Notice that both G and T are negative. The W-score is W = G−T
G . W
stands for worsening and thus lower values of W are better. The lowest value of W is zero  while
there is no upper bound.
We adopt BIC as a scoring function. The reason is that an algorithm for approximate exploration of
the parent sets (Scanagatta et al.  2015) allowing high in-degree even on large domains exists at the
moment only for BIC.
4.1 Parent set score exploration
Before performing structural learning it is necessary to compute the scores of the candidate parent sets
for each node (parent set exploration). The different structural learning methods are then provided
with the same score of the parent sets.
A treewidth k implies that one should explore all the parent sets up to size k; thus the complexity of
parent set exploration increases exponentially with the treewidth. To let the parent set exploration
scale efﬁciently with large treewidths and large number of variables we apply the approach of
Scanagatta et al. (2015). It guides the exploration towards the most promising parent sets (with size
up to k) without scoring them all. This is done on the basis of an approximated score function that is
computed in constant time. The actual score of the most promising parent sets is eventually computed.
We allow 60 seconds of time for the computation of the scores of the parent set of each variable  in
each data set.
4.2 Our implementation of S2 and S2+
Here we provide some details of our implementation of S2 and S2+. The second phase of both S2
and S2+ looks for a DAG whose moralization is a subgraph of a chosen k-tree. For this task Nie et al.
(2014) adopt an approximate approach based on partial order sampling (Algorithm 2). We found that
using Gobnilp for this task yields consistently slightly higher scores; thus we adopt this approach in
our implementation. We believe that it is due to the fact that constraining the structure optimization
to a subjacent graph of a k-tree results in a small number of allowed arcs for the DAG. Thus our
implementation of S2 and S2+ ﬁnds the highest-scoring DAG whose moral graph is a subgraph of
the provided k-tree.

5

4.3 Learning inverted trees
As already discussed our approach cannot learn an inverted tree with k parents per node if the given
bounded treewidth is k. In this section we study this worst-case scenario.
We start with treewidth k = 2. We consider the number of variables n ∈ {21  41  61  81  101}. For
each value of n we generate 5 different inverted trees. To generate as inverted tree we ﬁrst select a
root variable X and add k parents to it as ΠX; then we continue by randomly choosing a leaf of the
graph (at a generic iteration  there are leaves at different distance from X) and adding k parents to it 
until the the graph contains n variables.
All variables are binary and we sample their conditional probability tables from a Beta(1 1). We
sample 10 000 instances from each generated inverted tree.
We then perform structural learning with k-A*  k-G  S2  S2+ and TWILP setting k = 2 as limit on
the treewidth. We allow each method to run for ten minutes. S2  S2+ and TWILP could in principle
recover the true structure  which is prevented to our algorithms. The results are shown in Fig.2.
Qualitatively similar results are obtained repeating the experiments with k = 4.

0.1

0.05

e
r
o
c
s
-

W

0
20

30

40

50

60

70

80

90

100

Number of variables

S2

TWILP

S2+
k-G
k-A*

Figure 2: Structural learning results when the actual DAGs are inverted trees (k=2). Each point
represent the mean W-score over 5 experiments. Lower values of the W -score are better.

Despite the unfavorable setting  both k-G and k-A* yield DAGs with higher score than S2  S2+ and
TWILP consistently for each value of n. For n = 20 they found a close approximation to the optimal
graph. S2  S2+ and TWILP found different structures  with close score.
Thus the limitation of the space of learnable DAGs does not hurt the performance of k-G and k-A*.
In fact S2 could theoretically recover the actual DAG  but this is not feasible in practice as it requires
a prohibitive number of samples from the space of k-trees. The exact solver TWILP was unable to
ﬁnd the exact solution within the time limit; thus it returned a best solution achieved in the time limit.

Iterations
Median

Max

S2

803150
-273600
-271484

S2+
3
-267921
-266593

k-G
7176
-261648
-258601

k-A*
66
-263250
-261474

Table 1: Statistics of the solutions yielded by different methods on an inverted tree (n = 100  k = 4).

We further investigate the differences between methods in Table 1. Iterations is the number of
proposed solutions; for S2 and S2+ it corresponds to the number of explored k-trees  while for k-G
and k-A* it corresponds to the number of explored orders. During the execution  S2 samples almost
one million k-trees. Yet it yields the lowest-scoring DAGs among the different methods. This can be
explained considering that a randomly sampled k-tree has a low chance to cover a high-scoring DAG.
S2+ recovers only a few k-trees  but their scores are higher than those of S2. Thus the informative
score is effective at driving the search for good k-trees; yet it does not scale on large data sets as we
will see later. As for our methods  k-G samples a larger number of orders than k-A* does and this
allows it to achieve higher scores  even if it sub-optimally deals with each single order. Such statistics
show a similar pattern also in the next experiments.

6

DATASET VAR. GOBNILP

S2

S2+

TWILP

k-A*
k-G
−72159
-72159 −72159 −72159 −72159 −72159
−2698
−2698
-2698
−3203
-3185
-3206
-200431 −200363
-200142
-183369 −183241
-181748
−613
-608
−55785
-53104
−7088
-6919
−2185
-2173
−82003
-77555
−1279
-1277

−2698
-3252
-201235
-189539
-620
-68670
-7213
-2283
-107252
-1641

−2698
-3247
-200926
-186815
-619
-64769
-7209
-2208
-88350
-1427

−2698
-3213
-200340
-190086
-620
-68298
-7190
-2277

-615
-57021
-7109
-2201
-82633
-1284

nursery
breast
housing
adult
letter
zoo

mushroom

wdbc
audio

hill

community

9
10
14
15
17
17
22
31
62
100
100

Table 2: Comparison of the BIC scores yielded by different algorithms on the data sets analyzed by
Nie et al. (2016). The highest-scoring solution with limited treewidth is boldfaced. In the ﬁrst column
we report the score obtained by Gobnilp without bound on the treewidth.

4.4 Small data sets
We now present experiments on the data sets considered by Nie et al. (2016). They involve up to 100
variables. We set the bounded treewidth to k = 4. We allow each method to run for ten minutes. We
perform 10 experiments on each data set and we report the median scores in Table 2.
On the smallest data sets all methods (including Gobnilp) achieve the same score. As the data sets
becomes larger  both k-A* and k-G achieve higher scores than S2  S2+ and TWILP (which does not
achieve the exact solution). Between our two novel algorithms  k-A* has a slight advantage over k-G.
4.5 Large data sets
We now consider 10 large data sets (100 ≤ n ≤ 400) listed in Table 3. We no longer run TWILP  as
it is unable to handle this number of variables.

Data set
Audio
Jester

Data set
Netﬂix

n
100
100 Accidents
Table 3: Large data sets sorted according to the number of variables.

n Data set
100
Retail
111 Kosarek

n Data set
135
Andes
190 MSWeb

n
223
294

DNA

Data set

Pumsb-star

n
163
180

k-A*
29/20/24

S2
30/30/29
29/27/20

S2+
30/30/30
29/27/21
12/13/30

k-G
k-A*
S2

Table 4: Result on the 30 experiments on large data sets. Each cell report how many times the row
algorithm yields a higher score than the column algorithm for treewidth 2/5/8. For instance k-G wins
on all the 30 data sets against S2+ for each considered treewidth.

We consider the following treewidths: k ∈ {2  5  8}. We split each data set randomly into three
subsets. Thus for each treewidth we run 10·3=30 structural learning experiments.
We let each method run for one hour. For S2+  we adopt a more favorable approach  allowing it to
run for one hour; if after one hour the ﬁrst k-tree was not yet solved  we allow it to run until it has
solved the ﬁrst k-tree.
In Table 4 we report how many times each method wins against another for each treewidth  out
of 30 experiments. The entries are boldfaced when the number of victories of an algorithm over
another is statistically signiﬁcant (p-value <0.05) according to the sign-test. Consistently for any
chosen treewidth  k-G is signiﬁcantly better than any competitor  including k-A*; moreover  k-A* is
signiﬁcantly better than both S2 and S2+.

7

This can be explained by considering that k-G explores more orders than k-A*  as for a given order it
only ﬁnds an approximate solution. The results suggest that it is more important to explore many
orders instead of obtaining the optimal DAG given an order.
4.6 Very large data sets
Eventually we consider 14 very large data sets  containing between 400 and 10000 variables. We
split each algorithm in three subsets. We thus perform 14·3=42 structural learning experiments with
each algorithm.
We include three randomly-generated synthetic data sets containing 2000  4000 and 10000 variables
respectively. These networks have been generated using the software BNGenerator 4. Each variable
has a number of states randomly drawn from 2 to 4 and a number of parents randomly drawn from 0
to 6.

Data set
Diabets

Pigs
Book

n

Data set
500 Reuters-52
724
839

n

Data set

n Data set

n Data set

413 EachMovie
441
500
Table 5: Very large data sets sorted according to the number n of variables.

C20NG
Munin

BBC
Ad
R2

889
910
1041

Link

WebKB

1058
1556
2000

R4
R10

n
4000
10000

We let each method run for one hour. The only two algorithms able to cope with these data sets are
k-G and S2. For all the experiments  both k-A* and S2+ fail to ﬁnd even a single solution in the
allowed time limit; we veriﬁed this is not due to memory issues. Among them  k-G wins 42 times out
of 42; this dominance is clearly signiﬁcant. This result is consistently found under each choice of
treewidth (k =2  5  8). On average  the improvement of k-G over S2 ﬁlls about 60% of the gap which
separates S2 from the unbounded solver.

e
r
o
c
s
-

W

102

101

100
10−1

k-G(2)

k-G(5)

k-G(8)

S2(2)

S2(5)

S2(8)

Figure 3: Boxplots of the W-scores  summarizing the results over 14·3=42 structural learning
experiments on very large data sets. Lower W-scores are better. The y-axis is shown in logarithmic
scale. In the label of the x-axis we also report the adopted treewidth for each method: 2  5 or 8.

The W-scores of such 42 structural learning experiments are summarized in Figure 3. For both S2 and
k-G  a larger treewidth allows to recover a higher-scoring graph. In turn this decreases the W-score.
However k-G scales better than S2 with respect to the treewidth; its W-score decreases more sharply
with the treewidth. For S2  the difference between the treewidth seems negligible from the ﬁgure.
This is due to the fact that the graph learned are actually sparse.
Further experimental documentation is available  including how the score achieved by the algorithms
evolve with time  are available from http://blip.idsia.ch.
5 Conclusions
Our novel approaches for treewidth-bounded structure learning scale effectively with both in the
number of variables and the treewidth  outperforming the competitors.
Acknowledgments
Work partially supported by the Swiss NSF grants 200021_146606 / 1 and IZKSZ2_162188.

4http://sites.poli.usp.br/pmr/ltd/Software/BNGenerator/

8

References
Berg J.  Järvisalo M.  and Malone B. Learning optimal bounded treewidth Bayesian networks via
maximum satisﬁability. In AISTATS-14: Proceedings of the 17th International Conference on
Artiﬁcial Intelligence and Statistics  2014.

Cussens J. Bayesian network learning with cutting planes. In UAI-11: Proceedings of the 27th
Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence  pages 153–160. AUAI
Press  2011.

Elidan G. and Gould S. Learning bounded treewidth Bayesian networks. In Advances in Neural

Information Processing Systems 21  pages 417–424. Curran Associates  Inc.  2009.

Korhonen J. H. and Parviainen P. Exact learning of bounded tree-width Bayesian networks. In Proc.

16th Int. Conf. on AI and Stat.  page 370–378. JMLR W&CP 31  2013.

Kwisthout J. H. P.  Bodlaender H. L.  and van der Gaag L. C. The necessity of bounded treewidth
for efﬁcient inference in Bayesian networks. In ECAI-10: Proceedings of the 19th European
Conference on Artiﬁcial Intelligence  2010.

Nie S.  Mauá D. D.  de Campos C. P.  and Ji Q. Advances in learning Bayesian networks of bounded

treewidth. In Advances in Neural Information Processing Systems  pages 2285–2293  2014.

Nie S.  de Campos C. P.  and Ji Q. Learning Bounded Tree-Width Bayesian Networks via Sampling.
In ECSQARU-15: Proceedings of the 13th European Conference on Symbol and Quantitative
Approaches to Reasoning with Uncertainty  pages 387–396  2015.

Nie S.  de Campos C. P.  and Ji Q. Learning Bayesian networks with bounded treewidth via guided

search. In AAAI-16: Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence  2016.

Parviainen P.  Farahani H. S.  and Lagergren J. Learning bounded tree-width Bayesian networks using
integer linear programming. In Proceedings of the 17th International Conference on Artiﬁcial
Intelligence and Statistics  2014.

Patil H. P. On the structure of k-trees. Journal of Combinatorics  Information and System Sciences 

pages 57–64  1986.

Scanagatta M.  de Campos C. P.  Corani G.  and Zaffalon M. Learning Bayesian Networks with
Thousands of Variables. In NIPS-15: Advances in Neural Information Processing Systems 28 
pages 1855–1863  2015.

Teyssier M. and Koller D. Ordering-based search: A simple and effective algorithm for learning

Bayesian networks. CoRR  abs/1207.1429  2012.

9

,Mohammad Emtiyaz Khan
Mauro Scanagatta
Giorgio Corani
Cassio de Campos
Marco Zaffalon