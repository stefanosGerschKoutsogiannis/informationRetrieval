2018,Credit Assignment For Collective Multiagent RL With Global Rewards,Scaling decision theoretic planning to large multiagent systems is challenging due to uncertainty and partial observability in the environment. We focus on a multiagent planning model subclass  relevant to urban settings  where agent interactions are dependent on their ``collective influence'' on each other  rather than their identities. Unlike previous work  we address a general setting where system reward is not decomposable among agents. We develop collective actor-critic RL approaches for this setting  and address the problem of multiagent credit assignment  and computing low variance policy gradient estimates that result in faster convergence to high quality solutions. We also develop difference rewards based credit assignment methods for the collective setting. Empirically our new approaches provide significantly better solutions than previous methods in the presence of global rewards on two real world problems modeling taxi fleet optimization and multiagent patrolling  and a synthetic grid navigation domain.,Credit Assignment For Collective Multiagent RL

With Global Rewards

Duc Thien Nguyen Akshat Kumar Hoong Chuin Lau

{dtnguyen.2014 akshatkumar hclau}@smu.edu.sg

School of Information Systems

Singapore Management University
80 Stamford Road  Singapore 178902

Abstract

Scaling decision theoretic planning to large multiagent systems is challenging due
to uncertainty and partial observability in the environment. We focus on a multia-
gent planning model subclass  relevant to urban settings  where agent interactions
are dependent on their “collective inﬂuence” on each other  rather than their iden-
tities. Unlike previous work  we address a general setting where system reward
is not decomposable among agents. We develop collective actor-critic RL ap-
proaches for this setting  and address the problem of multiagent credit assignment 
and computing low variance policy gradient estimates that result in faster conver-
gence to high quality solutions. We also develop difference rewards based credit
assignment methods for the collective setting. Empirically our new approaches
provide signiﬁcantly better solutions than previous methods in the presence of
global rewards on two real world problems modeling taxi ﬂeet optimization and
multiagent patrolling  and a synthetic grid navigation domain.

1

Introduction

Sequential multiagent decision making allows multiple agents operating in an uncertain  partially
observable environment to take coordinated decision towards a long term goal [15]. Decentralized
partially observable MDPs (Dec-POMDPs) have emerged as a rich framework for cooperative multi-
agent planning [8]  and are applicable to several domains such as multiagent robotics [4]  multiagent
patrolling [19] and vehicle ﬂeet optimization [42]. Scalability remains challenging due to NEXP-
Hard complexity even for two agent systems [8]. To address the complexity  various models are
explored where agent interactions are limited by design by enforcing various conditional and con-
textual independencies such as transition and observation independence among agents [7  24] where
agents are coupled primarily via joint-rewards  event driven interactions [6]  and weakly coupled
agents [34  44]. However  their impact remains limited due to narrow application scope.
Recent multiagent planning research has focused on models where agent interactions are primar-
ily dependent on agents’ “collective inﬂuence” on each other rather than their identities [42  33 
30  25  26]. Such models are widely applicable in urban system optimization problems due to the
insight that urban systems are often composed of a large number of nearly identical agents  such
as taxis in transportation  and vessels in a maritime trafﬁc setting [2]. In our work  we focus on
the collective Dec-POMDP model (CDec-POMDP) that formalizes such collective multiagent plan-
ning [25]  and also generalizes “anonymity planning” models [42]. The CDec-POMDP model is
based on the idea of partial exchangeability [13  27]  and collective graphical models [31  35]. Par-
tial exchangeability in probabilistic inference is complementary to the notion of conditional and
contextual independence  and combining all of them leads to a larger class of tractable models and
inference algorithms [27].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

When only access to a domain simulator is available without exact model deﬁnition  several multi-
agent RL (MARL) approaches are developed such as independent Q-learning [38]  counterfactual
multiagent policy gradients and actor-critic methods [16  23]  multiagent Q-learning [29]  SARSA-
based MARL for Dec-POMDPs [14]  and MARL with limited communication [47  48]. However 
most of these approaches are limited to tens of agents in contrast to the collective setting with thou-
sands of agents  which is the setting we target. Closely related to the collective setting we address 
special MARL sub-classes are proposed to model and control population-level behaviour of agents
such as mean ﬁeld RL (MFRL) [46] and mean ﬁeld games (MFGs) [45  17]. MFGs are used learn
the behaviour of a population of agents in an inverse RL setting. The MFRL framework does not
explicitly address credit assignment  and also requires agents to maintain individual state-action tra-
jectories  which may not be scalable with thousands of agents  as is the case in our tested domains.
We focus on the problem of learning agent policies in a MARL setting for CDec-POMDPs. We
address the crucial challenge of multiagent credit assignment in the collective setting when joint ac-
tions generate a team reward that may not be decomposable among agents. The joint team rewards
make it difﬁcult for agent to deduce their individual contribution to the team’s success. Such team
reward settings have been recognised as particularly challenging in the MARL literature [9  16]  and
are common in disaster rescue domains (ambulance dispatch  police patrolling) where the penalty of
not attending to a victim is awarded to the whole team  team games such as StarCraft [16]  and trafﬁc
control [39]. Previous work in CDec-POMDPs develops an actor-critic RL approach when the joint
reward is additively decomposable among agents [25]  and is unable to address non-decomposable
team rewards. Therefore  we develop multiple actor-critic approaches for the general team reward
setting where some (or all) joint-reward component may be non-decomposable among agents. We
address two crucial issues—multiagent credit assignment  and computing low variance policy gra-
dient estimates for faster convergence to high quality solutions even with thousands of agents. As
a baseline approach  we ﬁrst extend the notion of difference rewards [41  16]  which are a popular
way to perform credit assignment  to the collective setting. Difference rewards (DRs) provide a con-
ceptual framework for credit assignment; there is no general computational technique to compute
DRs in different settings. Naive extension of the previous DR methods in deep multiagent RL set-
ting [16] is infeasible for large domains. Therefore  we develop novel approximation schemes that
can compute DRs in the collective case even with thousands of agents.
We show empirically that DRs can result in high variance policy gradient estimates  and are un-
able to provide high quality solutions when the agent population is small. We therefore develop a
new approach called mean collective actor critic (MCAC) that works signiﬁcantly better than DRs
and MFRL across a range of agent population sizes from 5 to 8000 agents. The MCAC analyti-
cally marginalizes out the actions of agents by using an approximation of the critic. This results in
low variance gradient estimates  allows credit assignment at the level of gradients  and empirically
performs better than DR-based approaches.
We test our approaches on two real world problems motivated by supply-demand taxi matching
problem (with 8000 taxis or agents)  and police patrolling for incident response in the city. We use
real world data for both these problem for constructing our models. We also test on a synthetic
grid navigation domain. Thanks to the techniques for credit assignment and low variance policy
gradients  our approches converge to high quality solutions signiﬁcantly faster than the standard
policy gradient method and the previous best approach [26]. For the police patrolling domain  our
approach provides better quality than a strong baseline static allocation approach that is computed
using a math program [10].

2 Collective Decentralized POMDP Model
We describe the CDec-POMDP model [25] . The model extends the statistical notion of partial
exchangeability to multiagent planning [13  27]. Previous works have mostly explored only condi-
tional and contextual independences in multiagent models [24  44]. CDec-POMDPs combine both
conditional independences and partial exchangeability to solve much larger instances of multiagent
decision making.
Deﬁnition 1 ([27]). Let X = {X1  . . .   Xn} be a set of random variables  and x denote an assign-
ment to X. Let Di denote the domain of the variable Xi  and let T : ×n
i=1Di → S be a statistic of
X  where S is a ﬁnite set. The set of random variables X is partially exchangeable w.r.t. the statistic
T if and only if T (x) = T (x(cid:48)) implies Pr(x) = Pr(x(cid:48)).

2

In the CDec-POMDP model  agent identities do not matter; different model components are only
affected by agent’s local state-action  and a statistic of other agents’ states-actions. There are M
agents in the environment. An agent m can be in one of the states i ∈ S. We also assume a global
state component d ∈ D. The joint state space is ×M
m=1S × D. The component d typically models
variables common to all the agents such as demand in the supply-demand matching case or location
of incidents in the emergency response. Let st  at denote the joint state-action of agents at time t.
The joint-state transition probability is:

P (st+1  dt+1|st  dt  at) = Pg(dt+1|dt T (st  at))

Pl

t+1|sm

t   am

t  T (st  at)  dt

(cid:1)

(cid:0)sm

M(cid:89)

m=1

t   am

o(d)∀d.

m rl(sm

t   am

is the same for all

the reward function rl

t denote agent m’s local state  action components  and T is a statistic of the corre-
where sm
sponding random variables (deﬁned later). We assume that the local state transition function is the
same for all the agents. Such an expression conveys that only the statistic T of the joint state-action 
and an agent’s local state-action are sufﬁcient to predict the agent’s next state.
Observation function: We assume a decentralized and partially observable setting in which each
agent receives only a partial observation about the environment. Let the current joint-state be
(st  dt) after the last join-action  then the observation for agent m is given using the function
In the taxi supply-demand case  the observation for a taxi in location z can
ot(sm
be the local demand in zone z  and the counts of other taxis in z and neighbouring zones of z. No
agent has a complete view of the system.

t   dt T (st)).
The reward function is r(st  dt  at) =(cid:80)
t   dt T (st  at)) + rg(dt T (st  at)) where rl is
(cid:80)
the local reward for individual agents  and rg is the non-decomposable global reward. Given
that
the agents  we can further simplify it as
i j n(i  j)rl(i  j  dt T (st  at)) + rg(dt T (st  at))  where n(i  j) is the number of agents in state i
and taking action j given the joint state-action (st  at). We assume that the initial state distribution 
bo(i)∀i∈ S  is the same for all the agents; initial distribution over global states is bg
The above deﬁned model components can also differentiate among agents by using the notion of
agent types  which can be included in an agent’s state-space S  and each agent can receive its type
as part of its observation. In the extreme case  each agent would be of a different type representing a
fairly general multiagent planning problem. However  the main beneﬁt of the model lies in settings
when agent types are much smaller than the number of agents.
We consider a ﬁnite-horizon problem with H time steps. Each agent has a non-stationary reactive
policy that takes as input agent’s current state i and the observation o  and outputs the probability
t (j|i  o). Such a policy is analogous to ﬁnite-state controllers in POMDPs
of the next action j as πm
and Dec-POMDPs [28  3]. Let π = (cid:104)π1  . . .   πM(cid:105) denote the joint-policy. The goal is to optimize
E[rt|bo  bd
o].
Global rewards:
The key difference from previous
works [25  26] is that in our model we have a global reward
signal rg that is not decomposable among individual agents 
which is crucial to model real world applications. Consider
a real world multiagent patrolling problem in ﬁgure 1. A set
of homogeneous police patrol cars (or agents) are stationed
in predeﬁned geographic regions to respond to incidents that
may arise over a shift (say 7AM to 7PM). When an inci-
dent comes  the central command unit dispatches the closet
patrol car to the incident location. The dispatched car be-
comes unavailable for some amount of time (including travel
and incident service time). To cover for the engaged car  other
available patrol cars from nearby zones may need to reallocate
themselves so that no zones are left vulnerable. The reward in
this system depends on the response time to incidents (e.g.  threshold to attend to urgent incidents is
10 min  non-urgent in 20 min). The goal is to compute a reallocation policy for agents to minimize
the number of unsatisﬁed incidents where the response time was more than the speciﬁed threshold.
To model this objective  we award penalty -10 whenever the response time requirement of an in-
cident is not met and 0 otherwise. In this domain  the delay penalty is non-decomposable among
patrol cars. It is not reasonable to attribute penalty in an incident to its assigned agent because delay

the value V (π) =(cid:80)H

Figure 1: Solid black lines deﬁne 24
patrolling zones of a city district

t=1

3

T (st  at) = (nt(i  j)∀i∈ S  j∈ A) where each entry nt(i  j) =(cid:80)

is due to the intrinsic system-wide supply-demand mismatch. Furthermore  individual agent penal-
ties may even discourage agents to go to nearby critical sectors  which is undesirable (we observed
it empirically). Indeed  in this domain  all rewards are global  therefore  previous approaches that
require local rewards for agents are not applicable. This is precisely the gap our work targets  and
signiﬁcantly increases the applicability of multiagent decision making to real world applications.
Statistic for Planning: We now describe the statistic T which increases scalability and the
generalization ability of solution approaches. For a given joint state-action (st  at)  we deﬁne
t ) = (i  j)} counts the
number of agents that are in state i and take action j. We can similarly deﬁne T (st) = (nt(i)∀i∈ S)
that counts the number of agents in each state i. For clarity  we denote T (st  at) or state-action count
table as nsa
t. Given a transition (st  at  st+1)  we deﬁne the count
(i  j  i(cid:48))∀i  i(cid:48) ∈ S  j ∈ A) which counts the number of agents in state i that took
table nsas
action j and transitioned to next state i(cid:48). Complete count table is denoted as nt = (ns
In collective planning settings  the agent population size is typically very large (≈ 8000 for our real
world experiments). Given such a large population  it is infeasible to compute unique policy for
each agent. Therefore  similar to previous works [43  26]  our goal is to compute a homogenous
stochastic policy πt(j|i  ot(i  dt  ns
t )) that outputs the probability of each action j given an agent in
state i receiving local observation ot depending on the state variable dt and state-counts ns
t at time
t. As the policy π is dependent on count based observations  it represents an expressive class of
policies. Let n1:H be the combined vector of count tables over all the time steps. Let Ω1:H be the
space of consistent count tables satisfying constraints:

t   and the state count table as ns

m I{(sm

t = (nsas

t   nsas

t   am

t  nsa

).

t

t

∀t :

(cid:88)
(cid:88)
(cid:88)

i(cid:48)∈S

i∈S

i∈S j∈A

(cid:88)

ns
t(i) = M ;
(cid:48)
t (i  j  i

nsas

(cid:48)
nsas
t (i  j  i

t(i) ∀i ∈ S

j∈A
) = nsa

nsa
t (i  j) = ns
t (i  j)∀i ∈ S  j ∈ A
(cid:48) ∈ S ;

) ∀i

t+1(i

(cid:48)

) = ns

(1)

(2)

Count tables n1:H are the sufﬁcient statistic for planning for CDec-POMDPs.
Theorem 1. [25] The joint-value function of a policy π over horizon H given by the expectation of

joint reward r  V (π) =(cid:80)H

t=1

E[rt]  can be computed by the expectation over counts:

(cid:88)

(cid:0)nsa

(cid:1)(cid:21)

(cid:20) H(cid:88)

t=1

(cid:0)nsa

t   dt

where rt

(cid:1) =(cid:80)

V (π) =

P (n1:H   d1:H ; π)

rt

t   dt

n1:H∈Ω1:H  d1:H

i j nsa

t (i  j)rl(i  j  dt  nsa

t )+rg(dt  nsa
t )

We show in appendix how to sample from the distribution P (n1:H   d1:H ; π) directly without sam-
pling individual state-action trajectories.
Scalability to large agent population: Since sampling individual agent trajectories is not required 
the count-based computation can scale to thousands of agents. Such beneﬁts also extends to com-
puting policy gradients which also depend only on counts n. Furthermore  different data structures
and function approximators such as the policy π and action-value function depend only on counts
n. Such a setting is computationally tractable because if we change only the agent population  the
dimensions of the count table still remains ﬁxed  only the counts of agents in different buckets (e.g.
n(i)  n(i  j)) changes. Such count-based formulations also extend the generalization ability of RL
approaches as multiple joint state-actions (st  at  dt) can give rise to the same statistic n. Our goal
is to compute the optimal policy π that maximizes V (π).
Learning framework: We follow a centralized learning and decentralized execution RL frame-
work. Such centralized learning is possible in the presence of domain simulators [16  23]. We
assume access only to a domain simulator that provides count samples n and the team reward r.
During centralized training  we have access to all the count-based information  which helps deﬁne
a centralized action-value function resulting in faster convergance to good solutions. During pol-
icy execution  agents execute individual policies without accessing centralized functions . In single
agent RL  agent experiences the tuple (st  at  st+1  rt) by interacting with the environment. In the
collective case  given that the sufﬁcient statistic is counts  we simulate and learn at the abstraction

4

  dt+1  rt). The current
t  dt); observations are generated for agents from this statistic and fed into
t . As a result of this joint action  agents transition to
. Given the constraint set Ω in (1) 
; dt+1 is the next global state. The joint-reward is

of counts. The experience tuple for the centralized learner is (ns
joint-state statistic is (ns
policies. The output is state-action counts nsa
new states  and their joint transitions are recorded in the table nsas
the next count table ns
rt(nsa
Actor Critic based MARL: We follow an actor-critic (AC) based policy gradient approach [21].
The policy π is parameterized using θ. The parameters are adjusted to maximize the objective

t   dt). Appendix shows how simulator generates such count-based samples.

t+1 is computed from nsas

t  dt  nsa

t   nsas

t

t

t

J(θ) = E[(cid:80)H

t=1 rt] by taking steps in the direction of ∇θJ(θ)  which is shown in [26] as:
(cid:19)(cid:21)

(cid:18) (cid:88)

∇θJ(θ) =

H(cid:88)

(cid:20)
t is the expected return E(cid:2)(cid:80)H

t |bo bd

dt nsa

Qπ

o  π

t=1

E

t (nsa

t   dt)

i∈S j∈A

t (i  j)∇θ log πt(j|i  o(i  dt  ns
nsa
t))

(cid:3). The above expression can be evaluated by

(3)

T =t rT|dt  nsa

t

where Qπ
In the AC approach  the policy π is termed as actor. We can estimate Qπ
sampling counts n.
t
using empirical returns  but it has high variance. To remedy this  AC methods often use a function
approximator for Qπ (say Qw)  which is termed as the critic. We consider the critic Qw to be
a continuous function (e.g.  a deep neural network) instead of a function deﬁned only for integer
inputs. This allows us to compute the derivative of Qw with respect to all the input variables  which
will be useful later. The critic can be learned from empirical returns using temporal-difference
learning. We next show several techniques to estimate the collective policy gradient ∇θJ(θ) that
help in the credit assignment problem and provide low variance gradient estimates even for very
large number of agents.

3 Difference Rewards Based Credit Assignment

Difference rewards provide a powerful way to perform credit assignment when there are several
agents  and have been explored extensively in the MARL literature [41  1  39  40  12]. Difference
rewards (DR) are shaped rewards that help individual agents ﬁlter out the noise from the global
reward signal (which includes effects from other agents’ actions)  and assess their individual con-
tribution to the global reward. As such  there is no general technique to compute DRs for different
problems. We therefore develop novel methods to approximately compute two popular types of
DRs—wonderful life utility (WLU) and aristocratic utility (AU) [41] for the collective case.
Wonderful Life Utility (WLU): Let s  a denote the joint state-action; r(s  a) be the system reward.
The WLU based DR for an agent m is rm = r(s  a) − r(s  a−m) where a−m is the joint-action
without the agent m. The WLU DR compares the global reward to the reward received when agent
m is not in the system. Agent m can use this shaped reward rm for its individual learning. However
extracting such shaped rewards from the simulator is very challenging and not feasible for large
number of agents. Therefore  we apply this reasoning to the critic (or action-value function approxi-
mator) Qw(nsa  d). Similar to WLU  we deﬁne WLQ (wonderful life Q-function) for an agent m as
Qm = Qw(nsa  d)− Qw(nsa−m  d) where nsa−m is the state-action count table without the agent m.
For a given (nsa  d)  we show how to estimate Qm. Assume that the agent m is in some state
i ∈ S and performing action j ∈ A. As agents do not have identities  we use Qij to denote the
WLQ for any agent in state-action (i  j). Let eij be a vector with the same dimension as nsa;
all entries in eij are zero except value 1 at the index corresponding to state-action (i  j). We have
Qij = Qw(nsa  d)−Qw(nsa −eij  d). Typically  critic Qw is represented using a neural network; we
normalize all count inputs to the network (denoted as ˜nsa = nsa/M) using the total agent population
M. We now estimate WLQ assuming that M is large:
Qij ≈ lim
M→∞
= −1 ·
lim
= −1 ∗ (−∆)
∂Qw

(cid:0)(nsa −eij )/M  d(cid:1)(cid:3) = lim
(cid:0)˜nsa  d(cid:1)(cid:3)

(cid:0) nsa/M  d(cid:1)−Qw
(cid:2)Qw

(cid:0)˜nsa − ∆ · eij  d(cid:1) − Qw

(cid:0)˜nsa − ∆ · eij  d(cid:1)(cid:3)

(cid:0)˜nsa  d(cid:1) − Qw

(by deﬁnition of total differential)

(cid:2)Qw

(cid:2)Qw

∆=1/M→0

∆=1/M→0

(˜nsa  d)

∂Qw

(4)

Qij ≈ 1
M

∂˜nsa(i  j)

∂˜nsa(i  j)
(˜nsa  d)

(5)

5

t

E

t   nsas

t  dt  nsa

(cid:20) (cid:88)

  dt+1  rt)  global reward rt

is used to
Thus  upon experiencing the tuple (ns
train the global critic Qw. An agent m in state-action (i  j) accumulates the gradient term
Qij∇θ log πt(j|i  o(i  dt  ns
t)) as per the standard policy gradient result [37](notice that policy π is
the same for all the agents). Given that there are nsa
t (i  j) agents performing action j in state i  the
total accumulated gradient based on WLQ updates (5) by all the agents for all time steps is given as:

H(cid:88)
∇wlq
θ J(θ) =
We can estimate ∇wlq
Aristrocratic Utility (AU): For a given joint state-action (s  a)  the AU based DR for an agent m
am πm(am|om(s))r(s  a−m ∪ am) where a−m ∪ am is the joint-
action where agent m’s action in a is replaced with am; om is the observation of the agent; πm is
the probability of action am. The AU marginalizes over all the actions of agent m keeping other
agents’ actions ﬁxed. We next deﬁne AU-based reasoning to the critic Qw. For a given (nsa  d)  we
deﬁne Aij as the counterfactual advantage function for the agent in state i and taking action j as:

is deﬁned as rm = r(s  a) −(cid:80)

θ J(θ) by sampling counts and the state dt for all the time steps.

t   dt)∇θ log πt(j|i  o(i  dt  ns
t))

nsa
t (i  j)Qij

t |bo bd

i∈S j∈A

t (nsa

(cid:21)

dt nsa

(6)

t=1

o

π(cid:0)j

(cid:48)|i  o(i  d  ns)(cid:1)Qw(nsa −eij + eij(cid:48)

Aij = Qw(nsa  d) −(cid:88)

  d)

(7)

j(cid:48)

where vectors eik are deﬁned as for WLQ. Such advantages have been used by [16]. However in
our setting  computing them naively is prohibitively expensive as the number of agents is large (in
thousands). Therefore  we use similar technique as for WLQ by normalizing counts  and computing
differentials lim∆=1/M→0
t   dt) ≈ 1
M

(cid:2)Qw(˜nsa  d)−Qw(˜nsa+∆·(eij(cid:48)−eij)  d)(cid:3)  ﬁnal estimate is (proof in appendix):
(cid:104)

t   dt) −(cid:88)

(cid:48)|i  o(i  dt  ns
t))

∂˜nsa(i  j(cid:48))

π(cid:0)j

∂˜nsa(i  j)

t (nsa

t   dt)

∂Qw

∂Qw

(nsa

(nsa

(cid:105)

Aij

(8)

j(cid:48)

Crucially  the above computation is independent of agent population M  and is thus highly scalable.
Using the same reasoning as WLQ  the gradient ∇au
t replaced by
advantages Aij
in (8). Empirically  we observed that using advantages Aij resulted in better quality
t

is exactly the same as (6) with Qij

because the additional term(cid:80)

j(cid:48) in Aij acts as a baseline and reduces variance.

θ

θ

θ   ∇wlq

4 Mean Collective Actor Critc—Credit assignment  low variance gradients
Notice that computing gradients ∇au
for DRs requires taking expectation over state-action
counts nsa (see (6))  which can have high variance. Furthermore  the DR approximation is accu-
rate only when the agent population M is large; for smaller populations we empirically observed a
drop in the solution quality using DRs. We next show how to address these limitations by develop-
ing a new approach called mean collective actor critic (MCAC) which is robust across a range of
population sizes  and empirically works better than DRs in several problems.
• We develop an alternative formulation of the policy gradient (3) that allows to analytically
t . By analytically computing the expectation over counts 
• We show that a factored critic structure is particularly suited for credit assignment  and also
allows analytical gradient computation by using results from collective graphical models [22].
• However  factored critic is not effectively learnable with global rewards. Our key insight is
that we learn a global critic which is not factorizable among agents.
Instead of computing
gradients from this critic  we estimate gradients from its ﬁrst-order Taylor approximation  which
fortunately is factored among agents  and ﬁts well within our previous two results above.

marginalize out state-action counts nsa
variance in the gradient estimates can be reduced  as also shown for MDPs in [11  5].

Variance reduction of gradient using expectation: Before reformulating the gradient expres-
sion (3)  we ﬁrst deﬁne P π(nsa
t  dt) as the collective distribution of the action counts given
t
the action probabilities π and state counts:

P π(nsa
t

| ns

t  dt) =

The above is a multinomial distribution—for each state i  we perform ns
for each of ns

t(i) agents). Each trial’s outcome is an action j∈ A with probability π(cid:0)j|i  o(i  dt  ns
t)(cid:1).

t(i) trials independently (one

t (i  j)!

j∈A

i∈S

(9)

| ns
(cid:89)

(cid:104)

(cid:81)

ns
t(i)!
j∈A nsa

(cid:89)

t (i j)(cid:105)
t)(cid:1)nsa
π(cid:0)j|i  o(i  dt  ns

6

Proposition 1. The collective policy gradient in (3) can be reformulated as:

H(cid:88)

t=1

(cid:104)(cid:88)

nsa

(cid:105)

∇θJ(θ) =

E
t dt|bo bd
ns

o

t (nsa  dt)∇θP π(nsa | ns
Qπ

t  dt)

(10)

t   dt) and analytically marginal-
Proof is provided in appendix. In the above expression we sample (ns
ize out state-action counts nsa
t   which will result in lower variance than using (3) directly to estimate
gradients. In the AC approach  we use a critic to approximate Qπ. However  not all types of critics
will enable analytical marginalization over state-action counts.
Critic design for multiagent credit assignment: We now investigate the special structure required
for the critic Qw that enables the analytical computation required in (10)  and also helps in the mul-
tiagent credit assignment. One solution studied in several previous works is a linear decomposition

of the critic among agents [36  18  20]: Qw(st  dt  at) =(cid:80)M

(cid:0)sm

t)(cid:1).

t   dt  ns

t   o(sm

t   am

m=1 f m
w

t   dt) =(cid:80)

Such a factored critic structure is particularly suited for credit assignment as we are explicitly learn-
ing f m
w as an agent m’s contribution to the global critic value. Crucially  we also show that the
policy gradient computed using such a critic also gets factored among agents  which is essentially
credit assignment at the level of gradients among agents. In the collective setting  counts are the suf-
ﬁcient statistic for planning  and we assume a homogenous stochastic policy. Therefore  the critic
simpliﬁes as: Qw(nsa
deﬁnition of fw that may depend on entire state counts ns
Gaussian approximation of collective graphical models [22].
Theorem 2. A linear critic  Qw(nsa
t (i  j)fw
b only depends on (dt  ns

t)(cid:1). The next result uses a more general
(cid:0)i  j  dt  ns

t)  has the expected policy gradient under the policy πθ as:

(cid:0)i  j  o(i  dt  ns

t. Proof (in appendix) uses results from

(cid:1)+b(dt  ns

t) where function

t (i  j)fw

i j nsa

i j nsa

t

Qw(nsa  dt)∇θP π(nsa | ns

t(i)∇θπθ
ns

t (j|i  o(i  dt  ns

t))fw

(cid:0)i  j  dt  ns

t

(cid:1)

(11)

t   dt) =(cid:80)
(cid:88)

t  dt) =

i∈S j∈A

(cid:88)

nsa

Learning the critic from global rewards: The factored critic used in theorem 2 has two major
disadvantages. First  learning the factored critic from global returns is not effective as crediting
empirical returns into contributions from different agents is difﬁcult to learn without requiring too
many samples. Second  the critic components fw are based on an agent’s local state  action while
ignoring other agents’ policy and actions which increases the inaccuracy as both local and global
rewards are affected by other agents’ actions.
Our key insight is that instead of learning a decomposable critic  we learn a global critic which is
not factorized among agents. This addresses the problem of learning from global rewards; as the
critic is deﬁned over the input from all the agents (count tables n in our case). However  instead
of computing policy gradients directly from the global critic  we compute gradients from a linear
approximation to the global critic using ﬁrst-order Taylor approximation. Actor update using linear
approximation of the critic is studied previously for MDPs in [11  32]. Given a small step size  the
linear approximation is sufﬁcient to estimate the direction of the policy gradient to move towards
a higher value. For our case  linear critic addresses both the credit assignment problem and low
t   dt)  we consider its ﬁrst order Taylor
variance gradient estimates. Consider the global critic Qw(nsa
t))∀i  j(cid:105) with
| ns
t  dt] =(cid:104)ns
expansion at the mean value of action counts n(cid:63) sa
π as the current policy:
Qw(nsa

(∇nsa Qw(nsa  dt)|nsa=n(cid:63) sa

t(i)π(j|i  o(i  dt  ns

t   dt) ≈ Qw(n(cid:63) sa

t =E[nsa

t − n(cid:63) sa

  dt) + (nsa

(12)

(cid:124)

)

)

t

t

t

t

Upon re-arranging the above  it ﬁts the critic structure in theorem 2:

nsa
t (i  j)

∂Qw

∂ nsa(i  j)

(n(cid:63) sa

t

  dt)+

Qw(n(cid:63) sa

t

(cid:124)

)

(∇nsa Qw(nsa  dt)|nsa=n(cid:63) sa

t

(cid:104)

  dt)−(cid:0) n(cid:63) sa

t

t   dt)≈(cid:88)

i j

Qw(nsa

(cid:1)(cid:105)

Using theorem 2 and proposition 1  we have (proof in appendix):
Corollary 1. Using the ﬁrst-order Taylor approximation of the critic at the expected state-action
counts n(cid:63) sa

t  dt; π]  the collective policy gradient is:

t =E[nsa

| ns

E

t dt|bo bd
ns

o

t(i)∇θπt(j|i  ot(i  dt  ns
ns
t))

∂Qw

∂ nsa(i  j)

(n(cid:63) sa

t

  dt)

(13)

t

∇θJ(θ)≈ H(cid:88)

t=1

(cid:104) (cid:88)

i∈S j∈A

(cid:105)

7

(a) Objective value

(b) Objective value

(c) Avg. proﬁt/taxi/day (d) Avg. unserved demand

Figure 2: Different metrics on the taxi problem with different penalty weights w.

Intuitively  terms ∂Qw/∂n(i  j) facilitate credit assignment  which also occur in DR based formula-
tions (section 3). When this term has a high value  it implies that a higher count of agents in state i
and taking action j would increase the overall critic value Q. This will encourage more agents to take
action j in state i. Each term ∂Qw/∂n(i  j) is evaluated at the overall state-action counts n(cid:63) sa
t which
in turn depend on the policy and actions of other agents. Thus  it overcomes the second limitation
of the factored critic in theorem 2 where terms fw ignore policy and actions of other agents.

5 Experiments

We test the aristocratic utility based approach (called ‘CCAC’ or collective counterfactual AC) that
uses gradient estimates (8)  and the mean collective AC (‘MCAC’) that uses (13). We test against
(a) the standard AC approach which ﬁts the critic using global rewards and computes gradients from
the global critic; (b) the factored actor critic (‘fAfC’) approach of [26]  the previous best approach
for CDec-POMDPs with decomposable rewards; (c) the average ﬂow based solver (‘AverageFlow’)
of [42]. In some domains (speciﬁcally the taxi problem)  we have both local and global rewards.
The local rewards are incorporated in ‘fAfC’ as before; for global rewards  we change the training
procedure of the critic in ‘fAfC’ (different AC updates are shown in appendix). We test on two real
world domains—taxi supply-demand matching [43]  and the police patrolling problem [10].
Taxi Supply-Demand Matching: The dataset consists of taxi demands (GPS traces of the taxi
movement and their hired/unhired status) in an Asian city over 1 year. The ﬂeet contains 8000
taxis (or agents) with the city divided in 81 zones. Environment dynamics are similar to [43]. The
environment is uncertain (due to stochastic demand)  and partially observable as each taxi observes
the count of other taxis and the demand in the current zone and geographically connected neighbor
zones  and decides its next action (stay or move to a neighboring zone). Over the plan horizon of 48
half hour intervals  the goal is to compute policies that enable strategic movement of taxis to optimize
the total ﬂeet proﬁt. Individual rewards model the revenue each taxi receives. Global rewards model
quality-of-service (QoS) by giving a high positive reward when the ratio of available taxis and the
current demand in a zone is greater than some threshold  and negative reward when the ratio is below
the set QoS. We selected the topmost 15 busiest zones for such global rewards. To enforce QoS level

α = 95% for each zone i and time t  we add penalty terms min(0  w×(cid:0) ˆdt(i)− αdt(i)(cid:1)) where w is the

penalty weight  ˆdt(i) is the total served demand at time t  and dt(i) is the total demand at time t. We
test the effect of QoS penalty by using weights w∈ [0  10.0]. We normalize all trip payments between
(0  1) which implies that the penalty for missing a customer over the QoS threshold is roughly w
times the negative of the maximum reward for serving a customer.
Figure 2(a) shows the quality comparisons (higher is better) between MCAC (CCAC is almost iden-
tical to MCAC) and fAfC with varying penalty w. It shows that with increasing w  fAfC becomes
signiﬁcantly worse than MCAC. We next investigate the reason. Figure 2(b) summarizes quality
comparisons among all approaches for three settings of w. Results conﬁrm that both MCAC and
CCAC provide similar quality  and are the best performing among the rest. ‘AverageFlow’ and ‘AC’
are much worse off due to presence of global rewards. As the weight w increases from 0 to 10  the
difference between CCAC/MCAC and fAfC increases signiﬁcantly. This is because higher w puts
more emphasis on optimizing global rewards. Figure 2(d) shows unserved demand below the QoS
threshold or (α · dt(i) − ˆdt(i)) averaged over all 15 zones and all the time steps (AC  AverageFlow
are omitted as their high numbers distort the ﬁgure). When penalty increased from w = 0 to 1 in

8

0110Penalty Weight-50050100150200Ind. RevenueACMCACCCACAverageFlowfAfC0123456710Penalty Weight-150000-100000-50000050000100000150000Objective value0110Penalty Weight−200.0E+3−100.0E+30.0E+0100.0E+3200.0E+3Objective value0110Penalty Weight-50050100150200Individual Profit0110Penalty Weight01020304050Unserved Demands(a) Algorithm convergence

(b) Objective value

(c) Unsatisﬁed Percentage

(d) M = 5; grid 5x5 (e) M = 20; grid 5x5 (f) M = 50; grid 5x5 (g) M = 20; grid 9x9 (h) M = 50; grid 9x9
Figure 3: (a)-(c) Police patrolling problem; (d)-(h) synthetic grid patrolling with varying population M  grids

ﬁgure 2(c)  MCAC/CCAC still maintain similar individual proﬁts  but their unserved demand de-
creased signiﬁcantly (by 32%) as shown in ﬁgure 2(d). Thus  CCAC/MCAC maintain individual
proﬁts while still reducing global penalty  and are therefore effective with global rewards. In con-
trast  the unserved demand by fAfC does not decrease much from w = 0 to w = 1  10; because the
QoS penalty constitutes global rewards whereas ‘fAfC’ is optimized for decomposable rewards.
Police Patrolling: The problem is introduced in section 2. There are 24 city zones  and 16 patrol
cars (or agents). We have access to real world data about all incidents for 31 days in 24 zones.
Roughly 50-60 incidents happen per day (7AM-7PM shift). The goal is to compute reallocation pol-
icy for agents such that number of incidents with response time more than the threshold is minimized
(further details in appendix). This domain has only global rewards. Therefore  we compare MCAC 
CCAC and AC (fAfC  AverageFlow are unable to model this domain). As a baseline  we compare
against a static allocation of patrol cars that is optimized using a stochastic math program [10] 
denoted as ‘MIP’. Figure 3(a) shows the convergence results. MCAC performs much better than
CCAC. This is because this problem is sparse with sparse tables nsa  resulting in higher gradient
variance for CCAC; MCAC marginalizes out nsa  thus has lower variance. Figure 3(b) shows over-
all objective comparisons (higher is better) among all three approaches. It conﬁrms that MCAC is
the best approach. MCAC has 7.8% incidents where response time was more than the threshold ver-
sus 9.32% for MIP (ﬁgure 3(c)). Notice that even this improvement is signiﬁcant as it allows ≈25
more incidents to be served within the threshold over 31 days (assuming 55 avg. incidents/day). In
emergency scenarios  improving response time even by few minutes is potentially life saving.
To further compare CCAC and MCAC  we created a synthetic grid patrolling problem also inspired
by police patrolling  where we vary grid sizes and agent population (domain details in appendix).
Figure 3(d-h) show convergence plots. In these problems  CCAC performs much worse (even worse
than AC) as these problems are sparse with sparse state-action counts nsa. This makes its gradient
variance higher than MCAC  which again performs best. To summarize  when the population size
is large and state-action counts are dense (as in the taxi problem with M = 8000)  both CCAC and
MCAC give similar quality; but for small population size (as in grid patrolling with M = 5)  MCAC
is more robust than CCAC and AC.

6 Summary

We developed several approaches for credit assignment in collective multiagent RL. We extended
the notion of difference rewards to the collective setting and showed how to compute them efﬁciently
even for very large agent population. To further reduce the gradient variance  we developed a num-
ber of results that analytically marginalize out agents’ actions from the gradient expression. This
approach  called MCAC  was more robust than difference rewards based approach across a number
of problems settings and consistently provided better quality over varying agent population sizes.

9

025005000750010000125001500017500Iteration0102030Objective valueACMCACCCAC05000100001500020000Iteration1201008060Objective value−140−120−100−80−60−40−200Objective valueMIPMCACCCACAC05101520Unsatisfied Incidents (%)MIPMCACCCACAC025005000750010000125001500017500Iteration0102030Objective valueACMCACCCAC010000Iteration050Quality010000Iteration0100Quality010000Iteration50100Quality010000Iteration050Quality010000Iteration050Quality7 Acknowledgments

This research project is supported by National Research Foundation Singapore under its Corp Lab
@ University scheme and Fujitsu Limited. First author is also supported by A(cid:63)STAR graduate
scholarship.

References
[1] Adrian K Agogino and Kagan Tumer. Unifying temporal and structural credit assignment problems. In

International Joint Conference on Autonomous Agents and Multiagent Systems  pages 980–987  2004.

[2] Lucas Agussurja  Akshat Kumar  and Hoong Chuin Lau. Resource-constrained scheduling for maritime

trafﬁc management. In AAAI Conference on Artiﬁcial Intelligence  pages 6086–6093  2018.

[3] Christopher Amato  Daniel S. Bernstein  and Shlomo Zilberstein. Optimizing ﬁxed-size stochastic con-
trollers for pomdps and decentralized pomdps. Autonomous Agents and Multi-Agent Systems  21(3):293–
320  2010.

[4] Christopher Amato  George Konidaris  Gabriel Cruz  Christopher A. Maynor  Jonathan P. How  and
Leslie Pack Kaelbling. Planning for decentralized control of multiple robots under uncertainty. In IEEE
International Conference on Robotics and Automation  ICRA  pages 1241–1248  2015.

[5] Kavosh Asadi  Cameron Allen  Melrose Roderick  Abdel-Rahman Mohamed  George Konidaris  and

Michael Littman. Mean Actor Critic. In eprint arXiv:1709.00503  September 2017.

[6] Raphen Becker  Shlomo Zilberstein  and Victor Lesser. Decentralized Markov decision processes with
event-driven interactions. In International Conference on Autonomous Agents and Multiagent Systems 
pages 302–309  2004.

[7] Raphen Becker  Shlomo Zilberstein  Victor Lesser  and Claudia V. Goldman. Solving transition indepen-
dent decentralized Markov decision processes. Journal of Artiﬁcial Intelligence Research  22:423–455 
2004.

[8] Daniel S. Bernstein  Rob Givan  Neil Immerman  and Shlomo Zilberstein. The complexity of decentral-

ized control of Markov decision processes. Mathematics of Operations Research  27:819–840  2002.

[9] Yu-Han Chang  Tracey Ho  and Leslie P Kaelbling. All learning is local: Multi-agent learning in global

reward games. In Advances in neural information processing systems  pages 807–814  2004.

[10] J. Chase  J. Du  N. Fu  T. V. Le  and H. C. Lau. Law enforcement resource optimization with response

time guarantees. In IEEE Symposium Series on Computational Intelligence  pages 1–7  2017.

[11] Kamil Ciosek and Shimon Whiteson. Expected policy gradients. In AAAI Conference on Artiﬁcial Intel-

ligence  2018.

[12] Sam Devlin  Logan Yliniemi  Daniel Kudenko  and Kagan Tumer. Potential-based difference rewards for
multiagent reinforcement learning. In International conference on Autonomous agents and multi-agent
systems  pages 165–172  2014.

[13] P Diaconis and Diaconis Freedman. De Finetti’s generalizations of exchangeability. Studies in Inductive

Logic and Probability  2:233–249  1980.

[14] Jilles Steeve Dibangoye and Olivier Buffet. Learning to act in decentralized partially observable MDPs.

In International Conference on Machine Learning  pages 1241–1250  2018.

[15] Ed Durfee and Shlomo Zilberstein. Multiagent planning  control  and execution. In Gerhard Weiss  editor 

Multiagent Systems  chapter 11  pages 485–546. MIT Press  Cambridge  MA  USA  2013.

[16] Jakob Foerster  Gregory Farquhar  Triantafyllos Afouras  Nantas Nardelli  and Shimon Whiteson. Coun-

terfactual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence  2018.

[17] Diogo A Gomes  Joana Mohr  and Rafael Rigao Souza. Discrete time  ﬁnite state space mean ﬁeld games.

Journal de mathématiques pures et appliquées  93(3):308–328  2010.

[18] Carlos Guestrin  Daphne Koller  and Ronald Parr. Multiagent planning with factored MDPs. In Advances

in Neural Information Processing Systems  pages 1523–1530  2002.

10

[19] Tarun Gupta  Akshat Kumar  and Praveen Paruchuri. Planning and learning for decentralized MDPs with

event driven rewards. In AAAI Conference on Artiﬁcial Intelligence  pages 6186–6194  2018.

[20] Jelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propagation.

Journal of Machine Learning Research  7(Sep):1789–1828  2006.

[21] Vijay R. Konda and John N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Opti-

mization  42(4):1143–1166  2003.

[22] Liping Liu  Daniel Sheldon  and Thomas Dietterich. Gaussian approximation of collective graphical

models. In International Conference on Machine Learning  pages 1602–1610  2014.

[23] Ryan Lowe  Yi Wu  Aviv Tamar  Jean Harb  Pieter Abbeel  and Igor Mordatch. Multi-agent actor-critic
for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 
pages 6382–6393  2017.

[24] R. Nair  P. Varakantham  M. Tambe  and M. Yokoo. Networked distributed POMDPs: A synthesis of
distributed constraint optimization and POMDPs. In AAAI Conference on Artiﬁcial Intelligence  pages
133–139  2005.

[25] Duc Thien Nguyen  Akshat Kumar  and Hoong Chuin Lau. Collective multiagent sequential decision

making under uncertainty. In AAAI Conference on Artiﬁcial Intelligence  pages 3036–3043  2017.

[26] Duc Thien Nguyen  Akshat Kumar  and Hoong Chuin Lau. Policy gradient with value function approxi-
mation for collective multiagent planning. In Advances in Neural Information Processing Systems  pages
4322–4332  2017.

[27] Mathias Niepert and Guy Van den Broeck. Tractability through exchangeability: A new perspective on
efﬁcient probabilistic inference. In AAAI Conference on Artiﬁcial Intelligence  pages 2467–2475  July
2014.

[28] Pascal Poupart and Craig Boutilier. Bounded ﬁnite state controllers. In Neural Information Processing

Systems  pages 823–830  2003.

[29] Tabish Rashid  Mikayel Samvelyan  Christian Schröder de Witt  Gregory Farquhar  Jakob N. Foerster 
and Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning  pages 4292–4301  2018.

[30] Philipp Robbel  Frans A Oliehoek  and Mykel J Kochenderfer. Exploiting anonymity in approximate
linear programming: Scaling to large multiagent MDPs. In AAAI Conference on Artiﬁcial Intelligence 
pages 2537–2543  2016.

[31] Daniel R Sheldon and Thomas G Dietterich. Collective graphical models. In Advances in Neural Infor-

mation Processing Systems  pages 1161–1169  2011.

[32] David Silver  Guy Lever  Nicolas Heess  Thomas Degris  Daan Wierstra  and Martin Riedmiller. Deter-
ministic policy gradient algorithms. In International Conference on Machine Learning  pages 387–395 
2014.

[33] Ekhlas Sonu  Yingke Chen  and Prashant Doshi.

Individual planning in agent populations: Exploit-
ing anonymity and frame-action hypergraphs. In International Conference on Automated Planning and
Scheduling  pages 202–210  2015.

[34] Matthijs T. J. Spaan and Francisco S. Melo. Interaction-driven Markov games for decentralized multiagent
planning under uncertainty. In International COnference on Autonomous Agents and Multi Agent Systems 
pages 525–532  2008.

[35] Tao Sun  Daniel Sheldon  and Akshat Kumar. Message passing for collective graphical models.

International Conference on Machine Learning  pages 853–861  2015.

In

[36] Peter Sunehag  Guy Lever  Audrunas Gruslys  Wojciech Marian Czarnecki  Vinicius Zambaldi  Max
Jaderberg  Marc Lanctot  Nicolas Sonnerat  Joel Z Leibo  Karl Tuyls  et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296  2017.

[37] Richard S. Sutton  David McAllester  Satinder Singh  and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In International Conference on Neural Information
Processing Systems  pages 1057–1063  1999.

11

[38] Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of

the tenth international conference on machine learning  pages 330–337  1993.

[39] Kagan Tumer and Adrian Agogino. Distributed agent-based air trafﬁc ﬂow management. In International

Joint Conference on Autonomous Agents and Multiagent Systems  pages 255:1–255:8  2007.

[40] Kagan Tumer and Adrian K. Agogino. Multiagent learning for black box system reward functions. Ad-

vances in Complex Systems  12(4-5):475–492  2009.

[41] Kagan Tumer  Adrian K Agogino  and David H Wolpert. Learning sequences of actions in collectives
of autonomous agents. In International joint conference on Autonomous agents and multiagent systems 
pages 378–385  2002.

[42] Pradeep Varakantham  Yossiri Adulyasak  and Patrick Jaillet. Decentralized stochastic planning with

anonymity in interactions. In AAAI Conference on Artiﬁcial Intelligence  pages 2505–2512  2014.

[43] Pradeep Reddy Varakantham  Shih-Fen Cheng  Geoff Gordon  and Asrar Ahmed. Decision support for
agent populations in uncertain and congested environments. In AAAI Conference on Artiﬁcial Intelligence 
pages 1471–1477  2012.

[44] Stefan J. Witwicki and Edmund H. Durfee. Inﬂuence-based policy abstraction for weakly-coupled Dec-

POMDPs. In International Conference on Automated Planning and Scheduling  pages 185–192  2010.

[45] Jiachen Yang  Xiaojing Ye  Rakshit Trivedi  Huan Xu  and Hongyuan Zha. Deep mean ﬁeld games for
learning optimal behavior policy of large populations. In International Conference on Learning Repre-
sentations  2018.

[46] Yaodong Yang  Rui Luo  Minne Li  Ming Zhou  Weinan Zhang  and Jun Wang. Mean ﬁeld multi-agent

reinforcement learning. In International Conference on Machine Learning  pages 5567–5576  2018.

[47] Chongjie Zhang and Victor R. Lesser. Coordinated multi-agent reinforcement learning in networked

distributed POMDPs. In AAAI Conference on Artiﬁcial Intelligence  2011.

[48] Chongjie Zhang and Victor R. Lesser. Coordinating multi-agent reinforcement learning with limited
In International conference on Autonomous Agents and Multi-Agent Systems  pages

communication.
1101–1108  2013.

12

,Duc Thien Nguyen
Akshat Kumar
Hoong Chuin Lau