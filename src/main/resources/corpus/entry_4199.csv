2014,PEWA: Patch-based Exponentially Weighted Aggregation for image denoising,Patch-based methods have been widely used for noise reduction in recent years. In this paper  we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms. We show that weakly denoised versions of the input image obtained with standard methods  can serve to compute an efficient patch-based aggregated estimd aggregation (EWA) estimator. The resulting approach (PEWA) is based on a MCMC sampling and has a nice statistical foundation while producing denoising results that are comparable to the current state-of-the-art. We demonstrate the performance of the denoising algorithm on real images and we compare the results to several competitive methods.,PEWA: Patch-based Exponentially Weighted

Aggregation for image denoising

Charles Kervrann

Inria Rennes - Bretagne Atlantique

Serpico Project-Team

Campus Universitaire de Beaulieu  35 042 Rennes Cedex  France

charles.kervrann@inria.fr

Abstract

Patch-based methods have been widely used for noise reduction in recent years.
In this paper  we propose a general statistical aggregation method which combines
image patches denoised with several commonly-used algorithms. We show that
weakly denoised versions of the input image obtained with standard methods  can
serve to compute an efﬁcient patch-based aggregated estimator. In our approach 
we evaluate the Stein’s Unbiased Risk Estimator (SURE) of each denoised can-
didate image patch and use this information to compute the exponential weighted
aggregation (EWA) estimator. The aggregation method is ﬂexible enough to com-
bine any standard denoising algorithm and has an interpretation with Gibbs distri-
bution. The denoising algorithm (PEWA) is based on a MCMC sampling and is
able to produce results that are comparable to the current state-of-the-art.

1

Introduction

Several methods have been proposed to solve the image denoising problem including anisotropic
diffusion [15]  frequency-based methods [26]  Bayesian and Markov Random Fields methods [20] 
locally adaptive kernel-based methods [17] and sparse representation [10]. The objective is to esti-
mate a clean image generally assumed to be corrupted with additive white Gaussian (AWG) noise.
In recent years  state-of-the-art results have been considerably improved and the theoretical lim-
its of denoising algorithms are currently discussed in the literature [4  14]. The most competitive
methods are mostly patch-based methods  such as BM3D [6]  LSSC [16]  EPLL [28]  NL-Bayes
[12]  inspired from the N(on)L(ocal)-means [2]. In the NL-means method  each patch is replaced
by a weighted mean of the most similar patches found in the noisy input image. BM3D combines
clustering of noisy patches  DCT-based transform and shrinkage operation to achieve the current
state-of-the-art results [6]. PLOW [5]  S-PLE [24] and NL-Bayes [12]  falling in the same cate-
gory of the so-called internal methods  are able to produce very comparable results. Unlike BM3D 
covariances matrices of clustered noisy patches are empirically estimated to compute a Maximum
A Posteriori (MAP) or a Minimum-Mean-Squared-Error (MMSE) estimate. The aforementioned
algorithms need two iterations [6  12  18] and the performances are surprisingly very close to the
state-of-the-art in average while the motivation and the modeling frameworks are quite different. In
this paper  the proposed Patch-based Exponential Weighted Aggregation (PEWA) algorithm  requir-
ing no patch clustering  achieves also the state-of-the-art results.
A second category of patch-based external methods (e.g. FoE [20]  EPLL [28]  MLP [3]) has been
also investigated. The principle is to approximate the noisy patches using a set of patches of an
external learned dictionary. The statistics of a noise-free training set of image patches  serve as
priors for denoising. EPLL computes a prior from a mixture of Gaussians trained with a database
of clean image patches [28]; denoising is then performed by maximizing the so-called Expected
Patch Log Likelihood (EPLL) criteria using an optimization algorithm. In this line of work  a multi-

1

layer perceptron (MLP) procedure exploiting a training set of noisy and noise-free patches was able
to achieve the state-of-the-art performance [3]. Nevertheless  the training procedure is dedicated
to handle a ﬁxed noise level and the denoising method is not ﬂexible enough  especially for real
applications when the signal-to-noise ratio is not known.
Recently  the similarity of patch pairs extracted from the input noisy image and from clean patch
dataset has been studied in [27]. The authors observed that more repetitions are found in the same
noisy image than in a clean image patch database of natural images; also  it is not necessary to
examine patches far from the current patch to ﬁnd good matching. While the external methods
are attractive  computation is not always feasible since a very large collection of clean patches are
required to denoise all patches in the input image. Other authors have previously proposed to learn
a dictionary on the noisy image [10] or to combine internal and external information (LSSC) [16].
In this paper  we focus on internal methods since they are more ﬂexible for real applications than
external methods. They are less computationally demanding and remain the most competitive.
Our approach consists in estimating an image patch from “weakly” denoised image patches in the
input image. We consider the general problem of combining multiple basic estimators to achieve
an estimation accuracy not much worse than that of the “best” single estimator in some sense. This
problem is important for practical applications because single estimators often do not perform as
well as their combinations. The most important and widely studied aggregation method that achieves
the optimal average risk is the Exponential Weighted Aggregation (EWA) algorithm [13  7  19].
Salmon & Le Pennec have already interpreted the NL-means as a special case of the EWA procedure
but the results of the extended version described in [21] were similar to [2].
Our estimator combination is then achieved through a two-step procedure  where multiple estimators
are ﬁrst computed and are then combined in a second separate computing step. We shall see that
the proposed method can be thought as a boosting procedure [22] since the performance of the pre-
computed estimators involved in the ﬁrst step are rather poor  both visually and in terms of peak
signal-to-noise ratio (PSNR). Our contributions are the following ones:

1. We show that “weak” denoised versions of the input noisy images can be combined to get

a boosted estimator.

2. A spatial Bayesian prior and a Gibbs energy enable to select good candidate patches.
3. We propose a dedicated Monte Carlo Markov Chain (MCMC) sampling procedure to com-

pute efﬁciently the PEWA estimator.

The experimental results are comparable to BM3D [6] and the method is implemented efﬁciently
since all patches can be processed independently.

2 Patch-based image representation and SURE estimation
Formally  we represent a n-dimensional image patch at location x ∈ X ⊂ R2 as a vector f (x) ∈ Rn.
We deﬁne the observation patch v(x) ∈ Rn as: v(x) = f (x) + ε(x) where ε(x) ∼ N (0  σ2In×n)

represents the errors. We are interested in an estimator (cid:98)f (x) of f (x) assumed to be independent of
in the Mean Square Error sense such that E[R((cid:98)f (x))] = E[(cid:107)f (x) − (cid:98)f (x)(cid:107)2

n] (E denotes the math-
ematical expectation). SURE has been already investigated for image denoising using NL-means
[23  9  22  24] and for image deconvolution [25].

v(x) that achieves a small L2 risk. We consider the Stein’s Unbiased Risk Estimator

R((cid:98)f (x)) = (cid:107)v(x) − (cid:98)f (x)(cid:107)2

n − nσ2

3 Aggregation by exponential weights
Assume a family {fλ(x)  λ ∈ Λ} of functions such that the mapping λ → fλ(x) is measurable
and Λ = {1 ···   M}. Functions fλ(x) can be viewed as some pre-computed estimators of f (x) or
“weak” denoisers independent of observations v(x)  and considered as frozen in the following. The
set of M estimators is assumed to be very large  that is composed of several hundreds of thousands

2

M(cid:88)

(cid:18) wλ(x)

(cid:19)

πλ(x)

(cid:41)

M(cid:88)

(cid:98)f (x) =

of candidates. In this paper  we consider aggregates that are weighted averages of the functions in
the set {fλ(x)  λ ∈ Λ} with some data-dependent weights:

wλ(x) fλ(x) such that wλ(x) ≥ 0 and

wλ(x) = 1.

(1)

λ=1

λ=1

As suggested in [19]  we can associate two probability measures w(x) = {w1(x) ···   wM (x)} and
π(x) = {π1(x) ···   πM (x)} on {1 ···   M} and we deﬁne the Kullback-Leibler divergence as:

DKL(w(x)  π(x)) =

wλ(x) log

.

(2)

M(cid:88)

λ=1

The exponential weights are obtained as the solution of the following optimization problem:

wλ(x)φ(R(fλ(x))) + β DKL(w(x)  π(x))

subject to (1)

(3)

where β > 0 and φ : R → R (e.g. φ(z) = z (see [19])). From the Karush-Kuhn-Tucker conditions 
the unique closed-form solution is

(cid:98)w(x) = arg min

w(x)∈RM

(cid:40) M(cid:88)

λ=1

(cid:98)wλ(x) =

and β can be interpreted as a “temperature” parameter. This estimator satisﬁes oracle inequalities of
the following form [7]:

(cid:80)M
exp(−φ(R(fλ(x)))/β) πλ(x)
λ(cid:48)=1 exp(−φ(R(fλ(cid:48)(x)))/β) πλ(cid:48)(x)
(cid:40) M(cid:88)

(cid:41)

wλ(x)φ(R(fλ(x))) + β DKL(w(x)  π(x))

.

(5)

 

(4)

E[R((cid:98)f (x))] ≤ min

w(x)∈RM

λ=1

The role of the distribution π is to put a prior weight on the functions in the set. When there is no
preference  the uniform prior is a common choice but other choices are possible (see [7]).
In the proposed approach  we deﬁne the set of estimators as the set of patches taken in denoised
versions of the input image v. The next question is to develop a method to efﬁciently compute the
sum in (1) since the collection can be very large. For a typical image of N = 512 × 512 pixels 
we could potentially consider M = L × N pre-computed estimators if we apply L denoisers to the
input image v.

4 PEWA: Patch-based EWA estimator

Suppose that we are given a large collection of M competing estimators. These basic estimators
can be chosen arbitrarily among the researchers favorite denoising algorithm: Gaussian  Bilateral 
Wiener  Discrete Cosine Transform or other transform-based ﬁlterings. Let us emphasize here that
the number of estimators M is not expected to grow and is typically very large (M is chosen on the
order of several hundreds of thousands). In addition  the essential idea is that these estimators only
slightly improve the PSNR values of a few dBs.
Let us consider u(cid:96)  (cid:96) = 1 ···   L denoised versions of v. A given pre-computed patch estimator
fλ(x) is then a n-dimensional patch taken in the denoised image u(cid:96) at any location y ∈ X   in the
spirit of the NL-means algorithm which considers only the noisy input patches for denoising. The
proposed estimator is then more general since a set of denoised patches at a given location are used.
Our estimator is then of the following form if we choose φ(z) = |z|:

(cid:98)f (x) =

1

Z(x)

L(cid:88)

(cid:88)

y∈X

(cid:96)=1

L(cid:88)

(cid:88)

(cid:96)(cid:48)=1

y(cid:48)∈X

e−|R(u(cid:96)(y))|/β π(cid:96)(y) u(cid:96)(y)  Z(x) =

e−|R(u(cid:96)(cid:48) (y(cid:48)))|/β π(cid:96)(cid:48)(y)

(6)

where Z(x) is a normalization constant.
Instead of considering a uniform prior over the set of
denoised patches taken in the whole image  it is appropriate to encourage patches located in the

3

spatial neighborhood of x [27]. This can be achieved by introducing a spatial Gaussian prior
Gτ (z) ∝ e−z2/(2τ 2) in the deﬁnition as

e−|R(u(cid:96)(y))|/β Gτ (x − y) u(cid:96)(y).

(7)

(cid:98)fPEWA(x) =

L(cid:88)

(cid:88)

y∈X

(cid:96)=1

1

Z(x)

The Gaussian prior has a signiﬁcant impact on the performance of the EWA estimator. Moreover  the
practical performance of the estimator strongly relies on an appropriate choice of β. This important
question has been thoroughly discussed in [13] and β = 4σ2 is motivated by the authors. Finally 
our patch-based EWA (PEWA) estimator can be written in terms of energies and Gibbs distributions
as:

(cid:98)fPEWA(x) =

E(u(cid:96)(y)) =

L(cid:88)

(cid:88)

1

e−E(u(cid:96)(y)) u(cid:96)(y) 
n − nσ2|

+

Z(x)
|(cid:107)v(x) − u(cid:96)(y)(cid:107)2

y∈X

(cid:96)=1

4σ2

(cid:107)x − y(cid:107)2

2

.

2τ 2

L(cid:88)

(cid:88)

(cid:96)(cid:48)=1

y(cid:48)∈X

Z(x) =

e−E(u(cid:96)(cid:48) (y(cid:48))) 

(8)

The sums in (8) cannot be computed  especially when we consider a large collection of estimators.
In that sense  it differs from the NL-means methods [2  11  23  9] which exploits patches generally
taken in a neighborhood of ﬁxed size.
Instead  we propose a Monte-Carlo sampling method to
approximately compute such an EWA when the number of aggregated estimators is large [1  19].

4.1 Monte-Carlo simulations and computational issues

Because of the high dimensionality of the problem  we need efﬁcient computational algorithms 
and therefore we suggest a stochastic approach to compute the PEWA estimator. Let us con-
sider a random process (Fn(x))n≥0 consisting in an initial noisy patch F0(x) = v(x). The pro-
posed Monte-Carlo procedure recommended to compute the estimator is based on the following
Metropolis-Hastings algorithm:
Draw a patch by considering a two-stage drawing procedure:

• draw uniformly a value (cid:96) in the set {1  2 ···   L}.
• draw a pixel y = yc + γ  y ∈ X   with γ ∼ N (0  I2×2τ 2) and yc is the position of the

current patch. At the initialization yc = x.

Deﬁne Fn+1(x) as: Fn+1(x) =

if α ≤ e−∆E(u(cid:96)(y)) Fn(x))
otherwise

(9)

(cid:26) u(cid:96)(y)

Fn(x)

where α is a random variable: α ∼ U [0  1] and ∆E(u(cid:96)(y)  Fn(x))
If we assume the Markov chain is ergodic  homogeneous  reductible  reversible and stationary  for
any F0(x)  we have almost surely

(cid:52)
= E(u(cid:96)(y)) − E(Fn(x)).

lim

T→+∞

1

T − Tb

T(cid:88)

n=Tb

Fn(x) ≈ (cid:98)fPEWA(x)

(10)

where T is the maximum number of samples of the Monte-Carlo procedure. It is also recommended
to introduce a burn-in phase to get a more satisfying estimator. Hence  the ﬁrst Tb samples are
discarded in the average The Metropolis-Hastings rule allows reversibility and then stationarity of
the Markov chain. The chain is irreducible since it is possible to reach any patch in the set of possible
considered patches. The convergence is ensured when T tends to inﬁnity. In practice  T is assumed

to be high to get a reasonable approximation of (cid:98)fPEWA(x). In our implementation  we set T ≈ 1000

and Tb = 250 to produce fast and satisfying results. To improve convergence speed  we can use
several chains instead of only one [21].
In the Metropolis-Hastings dynamics  some patches are more frequently selected than others at a
given location. The number of occurrences of a particular candidate patch can be then evaluated. In
constant image areas  there is probably no preference for any one patch over any other and a low
number of candidate patches is expected along image contours and discontinuities.

4

4.2 Patch overlapping and iterations

The next step is to extend the PEWA procedure at every position of the entire image. To avoid
block effects at the patch boundaries  we overlap the patches. As a result  for the pixels lying
in the overlapping regions  we obtain multiple EWA estimates. These competing estimates must
be fused or aggregated into the single ﬁnal estimate. The ﬁnal aggregation can be performed by a
weighted average of the multiple EWA estimates as suggested in [21  5  22]. The simplest method of
aggregating such multiple estimates is to average them using equal weights. Such uniform averaging
provided the best results in our experiments and amounts to fusing n independent Markov chains.
The proposed implementation proceeds in two identical iterations. At the ﬁrst iteration  the esti-
mation is performed using several denoised versions of the noisy image. At the second iteration 
the ﬁrst estimator is used as an additional denoised image in the procedure to improve locally the
estimation as in [6  12]. The second iteration improves the PSNR values in the range of 0.2 to 0.5
dB as demonstrated by the experiments presented in the next section. Note that the ﬁrst iteration is
able to produce very satisfying results for low and medium levels of noise. In practical imaging  we
use the method described in [11] to estimate the noise variance σ2 for real-world noisy images.

5 Experimental results

We evaluated the PEWA algorithm on 25 natural images showing natural  man-made  indoor and
outdoor scenes (see Fig. 1). Each original image was corrupted with white Gaussian noise with zero
mean and variance σ2. In our experiments  the best results are obtained with n = 7 × 7 patches
and L = 4 images ul denoised with DCT-based transform [26] ; we consider three different DCT
shrinkage thresholds: 1.25σ  1.5σ and 1.75σ to improve the PSNR of 1 to 6 db at most  depending
on σ and images (see Figs. 2-3). The fourth image is the noisy input image itself. We evaluated
the algorithm with a larger number L of denoised images and the quality drops by 0.1 db to 0.3 db 
which is visually imperceptible. Increasing L suggest also to considering more than 1000 samples
since the space of candidate patches is larger. The prior neighborhood size corresponds to a disk of
radius τ = 7 pixels but it can be smaller.
Performances of PEWA and other methods are quantiﬁed in terms of PSNR values for several noise
levels (see Tables 1-3). Table 1 reports the results obtained with PEWA on each individual image for
different values of standard deviation of noise. Table 2 compares the average PSNR values on these
25 images obtained by PEWA (after 1 and 2 iterations) and two state-of-the-art denoising methods
[6  12]. We used the implementations provided by the authors: BM3D (http://www.cs.tut.ﬁ/˜foi/GCF-
BM3D/) and NL-Bayes (www.ipol.im). The best PSNR values are in bold and the results are quan-
titatively quite comparable except for very high levels of noise. We compared PEWA to the baseline
NL-means [2] and DCT [26] (using the implementation of www.ipol.im) since they form the core of
PEWA. The PSNR values increases of 1.5 db and 1.35 db on average over NL-means and DCT re-
spectively. Finally  we compared the results to the recent S-PLE method which uses SURE to guide
the probabilistic patch-based ﬁltering described in [24]. Figure 2 shows the denoising results on the
noisy Valdemossa (σ = 15)  Man (σ = 20) and Castle (σ = 25) images denoised with BM3D 
NL-Bayes and PEWA. Visual quality of methods is comparable.
Table 3 presents the denoising results with PEWA if the pre-computed estimators are obtained with
a Wiener ﬁltering (spatial domain1) and DCT-based transform [26]. The results of PEWA with 5× 5
or 7 × 7 patches are also given in Table 3  for one and two iterations. Note that NL-means can be
considered as a special case of the proposed method in which the original noisy patches constitute
the set of “weak” estimators. The MCMC-based procedure can be then considered as an alternative
procedure to the usual implementation of NL-means to accelerate summation. Accordingly  in Table
3 we added a fair comparison (7×7 patches) with the implementation of NL-means algorithm (IPOL
(ipol.im)) which restricts the search of similar patches in a neighborhood of 21 × 21 pixels. In these
experiments  “PEWA basic” (1 iteration) produced better results especially for σ ≥ 10. Finally we
compared these results with the most popular and competitive methods on the same images. The
PSNR values are selected from publications cited in the literature. LSSC and BM3D are the most

× (v(x) − mean(v(x)))  where (cid:96) = {1  2  3} and

(cid:18)

0 

1 u(cid:96)(x) = mean(v(x)) + max
a1 = 0.15  a2 = 0.20  a3 = 0.25.

(cid:19)

var(v(x)) − a(cid:96)σ2

var(v(x))

5

cameraman
(256 × 256)

peppers

(256 × 256)

house

(256 × 256)

Lena

(512 × 512)

barbara

(512 × 512)

boat

(512 × 512)

man

(512 × 512)

couple

(512 × 512)

hill

(512 × 512)

maya

(313 × 473)

asia

(313 × 473)

aircraft

(473 × 313)

panther

(473 × 313)

alley

(192 × 128)

computer

(704 × 469)

dice

(704 × 469)

ﬂowers

(704 × 469)

girl

(704 × 469)

trafﬁc

(704 × 469)

trees

(192 × 128)

valldemossa
(769 × 338)

castle

(313 × 473)

young man
(313 × 473)

tiger

(473 × 313)

man on wall picture

(473 × 313)

Figure 1: Set of 25 tested images. Top left: images from the BM3D website (cs.tut.ﬁ/˜foi/GCF-
BM3D/) ; Bottom left: images from IPOL (ipol.im); Right: images from the Berkeley segmentation
database (eecs.berkeley.edu/Research/Projects/CS/ vision/bsds/).

performant but PEWA is able to produce better results on several piecewise smooth images while
BM3D is more appropriate for textured images.
In terms of computational complexity  denoising a 512 × 512 grayscale image with an unoptimized
implementation of our method in C++ take about 2 mins (Intel Core i7 64-bit CPU 2.4 Ghz). Re-
cently  PEWA has been implemented in parallel since every patch can be processed independently
and the computational times become a few seconds.

6 Conclusion

We presented a new general two-step denoising algorithm based on non-local image statistics and
patch repetition  that combines ideas from the popular NL-means [6] and BM3D algorithms [6] and
theoretical results from the statistical literature on Exponentially Weighted Aggregation [7  21]. The
ﬁrst step of PEWA involves the computation of denoised images obtained with a separate collec-
tion of multiple denoisers (Wiener  DCT... ) applied to the input image. In the second step  the
set of denoised image patches are selectively exploited to compute an aggregated estimator. We
showed that the estimator can be computed in reasonable time using a Monte-Carlo Markov Chain
(MCMC) sampling procedure. If we consider DCT-based transform [6] in the ﬁrst step  the results
are comparable in average to the state-of-the-art results. The PEWA method generalizes the NL-
means algorithm in some sense but share also common features with BM3D (e.g. DCT transform 
two-stage collaborative ﬁltering). tches  contrary to NL-Bayes and BM3D. For future work  wavelet-
based transform  multiple image patch sizes  robust statistics and sparse priors will be investigated
to improve the results of the ﬂexible PEWA method.

6

noisy (PSNR = 24.61)

PEWA (PSNR = 29.25)

BM3D [6] (PSNR = 29.19)

NL-Bayes [12] (PSNR = 29.22)

Figure 2: Comparison of algorithms. Valldemossa image corrupted with white Gaussian noise (σ =
15). The PSNR values of the three images denoised with DCT-based transform [26] and combined
with PEWA are 27.78  27.04 and 26.26.

noisy

(PSNR = 20.18)

PEWA

(PSNR = 29.49)

BM3D [6]

(PSNR = 29.36)

NL-Bayes [12]
(PSNR = 29.48)

noisy

(PSNR = 22.11)

PEWA

(PSNR = 30.50)

BM3D [6]

(PSNR = 30.59)

NL-Bayes [12]
(PSNR = 30.60)

Figure 3: Comparison of algorithms. First row: Castle image corrupted with white Gaussian noise
(σ = 25). The PSNR values of the three images denoised with DCT-based transform [26] and
combined with PEWA are 25.77  24.26 and 22.85. Second row: Man image corrupted with white
Gaussian noise (σ = 20). The PSNR values of the three images denoised with DCT-based transform
[26] and combined with PEWA are 27.42  26.00 and 24.67.

7

Cameraman

Peppers
House
Lena
Barbara

Boat
Man
Couple

Hill
Alley

Computer

Dice

Flowers

Girl
Trafﬁc
Trees

Valldemossa

Man Picture

Aircraft

Asia
Castle

Maya
Panther
Tiger

Young man
Average

σ = 5
38.20
38.00
39.56
38.57
38.09
37.12
37.68
37.35
37.01
36.29
39.04
46.82
43.48
43.95
37.85
34.88
36.65
37.59
38.67
38.06
37.78
34.72
38.53
36.92
40.79
38.54

σ = 10
34.23
34.68
36.40
35.78
34.73
33.75
33.93
33.91
33.52
32.20
35.13
43.87
39.67
41.22
33.54
29.93
31.79
34.62
34.46
34.13
33.58
29.64
33.91
32.85
37.36
34.75

σ = 15
31.98
32.75
34.86
34.12
32.86
31.94
31.93
31.98
31.69
29.98
32.81
42.05
37.47
39.52
31.13
27.49
29.25
33.00
32.25
32.02
31.27
27.17
31.56
30.63
35.58
32.67

σ = 20
30.60
31.40
33.72
32.90
31.43
30.64
30.50
30.57
30.50
28.54
31.23
40.58
35.90
38.27
29.58
25.86
27.59
31.75
30.73
30.56
29.73
25.42
30.02
29.13
34.30
31.26

σ = 25
29.48
30.30
32.77
31.89
30.28
29.65
29.50
29.48
29.56
27.46
30.01
39.36
34.55
37.33
28.48
24.69
26.37
30.72
29.60
29.49
28.44
24.28
28.83
27.99
33.25
30.15

σ = 50
26.25
26.69
29.29
28.83
26.58
26.64
26.67
26.02
26.92
24.13
26.38
35.33
30.81
34.14
25.50
21.78
23.18
27.68
26.63
26.15
24.65
22.85
25.59
24.63
29.59
26.95

σ = 100
22.81
22.84
25.35
25.65
22.95
23.63
24.15
23.27
24.49
21.37
23.27
30.82
27.53
30.50
22.90
20.03
20.71
24.99
24.32
23.09
21.50
18.17
22.75
21.90
25.20
23.76

Table 1: Denoising results on the 25 tested images for several values of σ. The PSNR values are
averaged over 3 experiments corresponding to 3 different noise realizations.

PEWA 1
PEWA 2
BM3D [6]

NL-Bayes [12]

S-PLE [24]
NL-means [2]

DCT [26]

σ = 5
38.27
38.54
38.64
38.60
38.17
37.44
37.81

σ = 10
34.39
34.75
34.78
34.75
34.38
33.35
33.57

σ = 15
32.26
32.67
32.68
32.48
32.35
31.00
31.87

σ = 20
30.76
31.26
31.25
31.22
30.67
30.16
29.95

σ = 25
29.62
30.15
30.19
30.12
29.77
28.96
28.97

σ = 50
26.00
26.95
26.97
26.90
26.46
25.53
25.91

σ = 100
22.35
23.76
24.08
23.65
23.21
22.29
23.08

Table 2: Average of denoising results over the 25 tested images for several values of σ. The exper-
iments with NL-Bayes [12]  S-PLE [24]  NL-means [2] and DCT [26] have been performed using
the implementations of IPOL (ipol.im). The best PSNR values are in bold.

Image

σ

Peppers

(256 × 256)

House

(256 × 256)

Lena

(512 × 512)

Barbara

(512 × 512)

5.00 15.00 25.00 50.00

5.00 15.00 25.00 50.00

5.00 15.00 25.00 50.00

5.00 15.00 25.00 50.00

PEWA 1 (W) (5×5)
PEWA 2 (W) (5×5)
PEWA 1 (W) (7 ×7)
PEWA 2 (W) (7 ×7)
PEWA 1 (D) (5 ×5)
PEWA 2 (D) (5 ×5)
PEWA 1 (D) (7 ×7)
PEWA 2 (D) (7 ×7)
PEWA Basic (7×7)
NL-means [2] (7×7)

BM3D [6]

NL-Bayes [12]
ND-SAFIR [11]

K-SVD [10]
LSSC [16]
PLOW [5]
SOP [18]

36.69 30.58 27.50 22.85
37.45 32.20 29.72 26.09
36.72 30.60 27.60 22.82
37.34 32.34 30.11 26.53
37.70 32.45 29.83 26.01
37.95 32.80 30.20 26.66
37.71 32.43 29.87 26.00
38.00 32.75 30.30 26.69
36.88 31.34 29.47 26.02
36.77 30.93 28.76 24.24
38.12 32.70 30.16 26.68
38.09 32.26 29.79 26.10
37.34 32.13 29.73 25.29
37.80 32.23 29.81 26.24
38.18 32.82 30.21 26.62
37.69 31.82 29.53 26.32
37.63 32.40 30.01 26.75

37.89 31.88 28.55 23.49
38.98 34.27 32.13 28.35
37.90 31.90 28.59 23.52
39.00 34.57 32.51 29.04
39.28 34.23 31.79 27.72
39.46 34.74 31.67 29.15
39.27 34.26 31.79 27.71
39.56 34.83 32.77 29.29
37.88 34.13 32.14 28.25
37.75 32.36 31.11 27.54
39.83 34.94 32.86 29.69
39.39 33.77 31.36 27.62
37.62 34.08 32.22 28.67
39.33 34.19 31.97 28.01
39.93 35.35 33.15 30.04
39.52 34.72 32.70 29.08
38.76 34.35 32.54 29.64

37.27 31.43 28.30 23.45
38.05 33.40 31.11 27.80
37.26 31.45 28.33 23.45
38.00 33.65 31.56 28.40
38.46 33.72 31.33 27.59
38.57 33.96 31.81 28.43
38.45 33.72 31.25 27.62
38.58 34.12 31.89 28.83
37.39 33.26 31.20 27.92
36.65 32.00 30.45 27.32
38.72 34.27 32.08 29.05
38.75 33.51 31.16 27.62
37.91 33.70 31.73 28.38
38.63 33.76 31.35 27.85
38.69 34.15 31.87 28.87
38.66 33.90 31.92 28.32
38.31 33.84 31.80 28.96

36.39 30.18 29.31 22.71
37.13 31.94 29.47 25.58
36.40 30.18 27.32 22.71
37.00 32.10 30.00 26.20
37.71 32.20 29.55 25.58
38.03 32.70 30.03 26.01
37.70 32.30 29.84 26.20
38.09 32.86 30.28 26.58
36.80 31.89 29.76 25.83
36.79 30.65 28.99 25.63
38.31 33.11 30.72 27.23
38.38 32.47 30.02 26.45
37.12 31.80 29.24 24.09
38.08 32.33 29.54 25.43
38.48 33.00 30.47 27.06
37.98 21.17 30.20 26.29
37.74 32.65 30.37 27.35

Table 3: Comparison of several versions of PEWA (W (Wiener)  D (DCT)  Basic) and competitive
methods on a few standard images corrupted with white Gaussian noise. The best PSNR values are
in bold (PSNR values from publications cited in the literature).

8

References
[1] Alquier  P.  Lounici  K. (2011) PAC-Bayesian bounds for sparse regression estimation with exponential

weights. Electronic Journal of Statistics 5:127-145.

[2] Buades A.  Coll  B. & Morel  J.-M. (2005) A review of image denoising algorithms  with a new one. SIAM

J. Multiscale Modeling & Simulation  4(2):490-530.

[3] Burger  H.  Schuler  C. & Harmeling  S. (2012) Image denoising: can plain neural networks compete with
BM3D ? In IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR’12)  pp. 2392-2399  Providence  Rhodes Island.
[4] Chatterjee  P. & Milanfar  P. (2010) Is denoising dead?  IEEE Transactions on Image Processing 

19(4):895-911.

[5] Chatterjee  P. & Milanfar  P. (2012) Patch-based near-optimal image denoising  IEEE Transactions on

Image Processing  21(4):1635-1649.

[6] Dabov  K.  Foi  A.  Katkovnik  V. & Egiazarian  K. (2007) Image denoising by sparse 3D transform-domain

collaborative ﬁltering  IEEE Transactions on Image Processing  16(8):2080-2095.

[7] Dalayan  A.S. & Tsybakov  A. B. (2008) Aggregation by exponential weighting  sharp PAC-Bayesian

bounds and sparsity. Machine Learning 72:39-61.

[8] Dalayan  A.S. & Tsybakov  A. B. (2009) Sparse regression learning by aggregation and Langevin Monte

Carlo. Available at ArXiv:0903.1223

[9] Deledalle  C.-A.  Duval  V.  Salmon  J. (2012) Non-local methods with shape-adaptive patches (NLM-

SAP). J. Mathematical Imaging and Vision  43:103-120.

[10] Elad  M. & Aharon  M. (2006) Image denoising via sparse and redundant representations over learned

dictionaries. IEEE Transactions on Image Processing  15(12):3736-3745.

[11] Kervrann  C. & Boulanger  J. (2006) Optimal spatial adaptation for patch-based image denoising. IEEE

Transcations on Image Processing  15(10):2866-2878.

[12] Lebrun  M.  Buades  A. & Morel  J.-M. (2013) Implementation of the “Non-Local Bayes” (NL-Bayes)

image denoising algorithm  Image Processing On Line  3:1-42. http://dx.doi.org/10.5201/ipol.2013.16

[13] Leung  G. & Barron  A. R. (2006) Information theory and mixing least-squares regressions. IEEE Trans-

actions on Information Theory 52:3396-3410.

[14] Levin  A.  Nadler  B.  Durand  F. & Freeman  W.T. (2012) Patch complexity  ﬁnite pixel correlations and

optimal denoising. Europ. Conf. Comp. Vis. (ECCV’12)  pp. 73-86  Firenze  Italy.

[15] Louchet  C. & Moisan  L. (2011) Total Variation as a local ﬁlter. SIAM J. Imaging Sciences 4(2):651-694.
[16] Mairal  J.  Bach  F.  Ponce  J.  Sapiro  G. &. Zisserman  A. (2009) Non-local sparse models for image

restoration. In IEEE Int. Conf. Comp. Vis. (ICCV’09)  pp. 2272-2279  Tokyo  Japan.

[17] Milanfar  P. (2013) A Tour of modern image ﬁltering. IEEE Signal Processing Magazine  30(1):106-128.
[18] Ram  I.  Elad  M. & Cohen  I. (2013) Image processing using smooth ordering of its patches. IEEE

Transactions on Image Processing  22(7):2764–2774

[19] Rigollet  P. & Tsybakov  A. B. (2012) Sparse estimation by exponential weighting. Statistical Science

27(4):558-575.

[20] Roth  S. & Black  M.J. (2005) Fields of experts: A framework for learning image priors. In IEEE Conf.

Comp. Vis. Patt. Recogn. (CVPR’05)  vol. 2  pp. 860-867  San Diego  CA.

[21] Salmon  J. & Le Pennec  E. (2009) NL-Means and aggregation procedures. In IEEE Int. Conf. Image

Process. (ICIP’09)  pp. 2977-2980  Cairo  Egypt.

[22] Talebi  H.  Xhu  X. & Milanfar  P. (2013) How to SAIF-ly boost denoising performance. In IEEE Trans-

actions on Image Processing 22(4):1470-1485.

[23] Van De Ville  D. & Kocher  M. (2009) SURE based non-local means. IEEE Signal Processing Letters 

16(11):973-976  2009.

[24] Wang  Y.-Q. & Morel  J.-M. (2013). SURE guided Gaussian mixture image denoising. SIAM J. Imaging

Sciences  6(2):999-1034.

[25] Xue  F.  Luisier  F. & Blu T. (2013) Multi-wiener SURE-LET deconvolution. IEEE Transactions on Image

Processing  22(5):1954-1968.

[26] Yu G. & Sapiro G. (2011) . DCT image denoising: a simple and effective image denoising algorithm.

Image Processing On Line (http://dx.doi.org/10.5201/ipol.2011.ys-dct).

[27] Zontak  M. & Irani  M. (2011) Internal statistics of a single natural image. In IEEE Comp. Vis. Patt.

Recogn. (CVPR’11)  pp. 977-984  Colorado Springs  CO.

[28] Zoran  D. & Weiss  Y. (2011) From learning models of natural image patches to whole image restoration.

In IEEE Int. Conf. Comp. Vis. (ICCV’11)  pp. 479-486  Barcelona  Spain.

9

,Nataliya Shapovalova
Michalis Raptis
Leonid Sigal
Greg Mori
Charles Kervrann
Satyen Kale
Chansoo Lee
David Pal