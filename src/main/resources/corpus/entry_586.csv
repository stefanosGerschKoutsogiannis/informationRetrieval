2017,Approximation Bounds for Hierarchical Clustering: Average Linkage  Bisecting K-means  and Local Search,Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use  the method has an underdeveloped analytical foundation. Having a well understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta. The main result is that one of the most popular algorithms used in practice  average linkage agglomerative clustering  has a small constant approximation ratio for this objective. Furthermore  this paper establishes that using bisecting k-means divisive clustering has a very poor lower bound on its approximation ratio for the same objective.  However  we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function.  This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well.,Approximation Bounds for Hierarchical Clustering:

Average Linkage  Bisecting K-means  and Local

Search

Benjamin Moseley∗

Carnegie Mellon University
Pittsburgh  PA 15213  USA

moseleyb@andrew.cmu.edu

Joshua R. Wang†

Department of Computer Science Stanford University

353 Serra Mall  Stanford  CA 94305  USA
joshua.wang@cs.stanford.edu

Abstract

Hierarchical clustering is a data analysis method that has been used for decades.
Despite its widespread use  the method has an underdeveloped analytical foun-
dation. Having a well understood foundation would both support the currently
used methods and help guide future improvements. The goal of this paper is to
give an analytic framework to better understand observations seen in practice.
This paper considers the dual of a problem framework for hierarchical clustering
introduced by Dasgupta [Das16]. The main result is that one of the most popular
algorithms used in practice  average linkage agglomerative clustering  has a small
constant approximation ratio for this objective. Furthermore  this paper establishes
that using bisecting k-means divisive clustering has a very poor lower bound on
its approximation ratio for the same objective. However  we show that there are
divisive algorithms that perform well with respect to this objective by giving two
constant approximation algorithms. This paper is some of the ﬁrst work to establish
guarantees on widely used hierarchical algorithms for a natural objective function.
This objective and analysis give insight into what these popular algorithms are
optimizing and when they will perform well.

1

Introduction

Hierarchical clustering is a widely used method to analyze data. See [MC12  KBXS12  HG05] for an
overview and pointers to relevant work. In a typical hierarchical clustering problem  one is given a set
of n data points and a notion of similarity between the points. The output is a hierarchy of clusters of
the input. Speciﬁcally  a dendrogram (tree) is constructed where the leaves correspond to the n input
data points and the root corresponds to a cluster containing all data points. Each internal node of the
tree corresponds to a cluster of the data points in its subtree. The clusters (internal nodes) become
more reﬁned as the nodes are lower in the tree. The goal is to construct the tree so that the clusters
deeper in the tree contain points that are relatively more similar.
There are many reasons for the popularity of hierarchical clustering  including that the number of
clusters is not predetermined and that the clusters produced induce taxonomies that give meaningful
ways to interpret data.
Methods used to perform hierarchical clustering are divided into two classes: agglomerative and
divisive. Agglomerative algorithms are a bottom-up approach and are more commonly used than
∗Benjamin Moseley was supported in part by a Google Research Award  a Yahoo Research Award and NSF
Grants CCF-1617724  CCF-1733873 and CCF-1725661. This work was partially done while the author was
working at Washington University in St. Louis.

†Joshua R. Wang was supported in part by NSF Grant CCF-1524062.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

divisive approaches [HTF09]. In an agglomerative method  each of the n input data points starts
as a cluster. Then iteratively  pairs of similar clusters are merged according to some appropriate
metric of similarity. Perhaps the most popular metric to deﬁne similarity is average linkage where
the similarity between two clusters is deﬁned as the average similarity between all pairs of data points
in the two clusters. In average linkage agglomerative clustering the two clusters with the highest
average similarity are merged at each step. Other metrics are also popular. Related examples include:
single linkage  where the similarity between two clusters is the maximum similarity between any
two single data points in each cluster  and complete linkage  where the distance is the minimum
similarity between any two single data points in each cluster.
Divisive algorithms are a top-down approach where initially all data points belong to a single cluster.
Splits are recursively performed  dividing a cluster into two clusters that will be further divided. The
process continues until each cluster consists of a single data point. In each step of the algorithm  the
data points are partitioned such that points in each cluster are more similar than points across clusters.
There are several approaches to perform divisive clustering. One example is bisecting k-means where
k-means is used at each step with k = 2. For details on bisecting k-means  see [Jai10].

Motivation: Hierarchical clustering has been used and studied for decades. There has been some
work on theoretically quantifying the quality of the solutions produced by algorithms  such as
[ABBL12  AB16  ZB09  BA08  Das16]. Much of this work focuses on deriving the structure
of solutions created by algorithms or analytically describing desirable properties of a clustering
algorithm. Though the area has been well-studied  there is no widely accepted formal problem
framework. Hierarchical clustering describes a class of algorithmic methods rather than a problem
with an objective function. Studying a formal objective for the problem could lead to the ability to
objectively compare different methods; there is a desire for the community to investigate potential
objectives. This would further support the use of current methods and guide the development of
improvements.
This paper is concerned with investigating objectives for hierarchical clustering. The main goal and
result of this paper is giving a natural objective that results in a theoretical guarantee for the most
commonly used hierarchical clustering algorithm  average linkage agglomerative clustering. This
guarantee gives support for why the algorithm is popular in practice and the objective gives insight
into what the algorithm optimizes. This paper also proves a bad lower bound on bisecting k-means
with respect to the same natural objective. This objective can therefore be used as a litmus test for the
applicability of particular algorithms. This paper further gives top-down approaches that do have
strong theoretical guarantees for the objective.

Problem Formulation: Towards this paper’s goal  ﬁrst a formal problem framework for hierarchical
clustering needs to be established. Recently  Dasgupta [Das16] introduced a new problem framework
for hierarchical clustering. This work justiﬁed their objective by establishing that for several sample
problem instances  the resulting solution corresponds to what one might expect out of a desirable
solution. This work has spurred considerable interest and there have been several follow up papers
[CC17  Das16  RP16].
In the problem introduced by Dasgupta [Das16] there is a set of n data points as input and for two
points i and j there is a weight wi j denoting their similarity. The higher the weight  the larger the
similarity. This is represented as a weighted complete graph G. In the problem the output is a (full)
binary tree where the leaves of the tree correspond to the input data points. For each pair of points i
and j  let T [i∨ j] denote the subtree rooted at i and j’s least common ancestor. Let leaves(T [i∨ j])
denote the set of leaves in the tree T [i ∨ j]. The goal is to construct the tree such that the cost
i j∈[n] wij|leaves(T [i ∨ j])| is minimized. Intuitively  this objective enforces that
more similar points i and j should have a lower common ancestor in the tree because the weight wi j
is large and having a smaller least common ancestor ensures that |leaves(T [i ∨ j])| is smaller. In
particular  more similar points should be separated at lower levels of the hierarchical clustering.
√
For this objective  several approximation algorithms have been given [CC17  Das16  RP16]. It is
known that there is a divisive clustering algorithm with an approximation ratio of O(
log n) [CC17].
In particular  the algorithm gives a O(αn)-approximation where αn is the approximation ratio of
√
the sparsest cut subroutine [CC17]. Furthermore  assuming the Small-Set Expansion Hypothesis 
every algorithm is a ω(1)-approximation [CC17]. The current best known bound on αn is O(
log n)
[ARV09]. Unfortunately  this conclusion misses one of our key goals in trying to establish an

costG(T ) :=(cid:80)

2

objective function. While the algorithms and analysis are ingenious  none of the algorithms with
theoretical guarantees are from the class of algorithms used in practice. Due to the complexity of the
proposed algorithms  it will also be difﬁcult to put them into practice.
Hence the question still looms: are there strong theoretical guarantees for practical algorithms? Is
the objective from [Das16] the ideal objective for our goals? Is there a natural objective that admits
solutions that are provably close to optimal?

on constructing a binary tree T to minimize the cost costG(T ) :=(cid:80)
(cid:80)
i j∈[n] wij|non-leaves(T [i ∨ j])| = (n(cid:80)

Results: In this paper  we consider an objective function motivated by the objective introduced by Das-
gupta in [Das16]. For a given tree T let |non-leaves(T [i ∨ j])| be the total number of leaves that
are not in the subtree rooted at the least common ancestor of i and j. The objective in [Das16] focuses
i j∈[n] wij|leaves(T [i ∨ j])|.
This paper considers the dual problem where T is constructed to maximize the revenue revG(T ) :=
i j∈[n] wi j) − costG(T ). It is important to observe
that the optimal clustering is the same for both objectives. Due to this  all the examples given in
[Das16] motivating their objective by showing desirable structural properties of the optimal solution
also apply to the objective considered in this paper. Our objective can be interpreted similarly to that
in [Das16]. In particular  similar points i and j should be located lower in the tree as to maximize
|non-leaves(T [i ∨ j])|  the points that get separated at high levels of the hierarchical clustering.
This paper gives a thorough investigation of this new problem framework by analyzing several
algorithms for the objective. The main result is establishing that average linkage clustering is a 1
3-
approximation. This result gives theoretical justiﬁcation for the use of average linkage clustering and 
additionally  this shows that the objective considered is tractable since it admits Ω(1)-approximations.
This suggests that the objective captures a component of what average linkage is optimizing for.
This paper then seeks to understand what other algorithms are good for this objective. In particular 
is there a divisive algorithm with strong theoretical guarantees? What can be said about practical
divisive algorithms? We establish that bisecting k-means is no better than a O( 1√
n ) approximation.
This establishes that this method is very poor for the objective considered. This suggests that bisecting
k-means is optimizing for something different than what average linkage optimizes for.
Given this negative result  we question whether there are divisive algorithms that optimize for our
objective. We answer this question afﬁrmatively by giving a local search strategy that obtains a
3-approximation as well as showing that randomly partitioning is a tight 1
3-approximation. The
1
randomized algorithm can be found in the supplementary material.

Other Related Work: Very recently a contemporaneous paper [CKMM17] done independently has
been published on ArXiv. This paper considers another class of objectives motivated by the work of
[Das16]. For their objective  they also derive positive results for average linkage clustering.

2 Preliminaries

In this section  we give preliminaries including a formal deﬁnition of the problem considered and
basic building blocks for later algorithm analysis.
In the Revenue Hierarchical Clustering Problem there are n input data points given as a set V .
There is a weight wi j ≥ 0 between each pair of points i and j denoting their similarity  represented
as a complete graph G. The output of the problem is a rooted tree T where the leaves correspond to
the data points and the internal nodes of the tree correspond to clusters of the points in the subtree. We
will use the indices 1  2  . . . n to denote the leaves of the tree. For two leaves i and j  let T [i∨j] denote
the subtree rooted at the least common ancestor of i and j and let the set non-leaves(T [i ∨ j])
denote the number of leaves in T that are not in T [i ∨ j]. The objective is to construct T to maximize

(cid:80)
j(cid:54)=i∈[n] wi j|non-leaves(T [i ∨ j])|.

the revenue revG(T ) =(cid:80)
leaves(T [i ∨ j]) be the set of leaves in T [i ∨ j] and costG(T ) :=(cid:80)
revG(T ) = n(cid:80)

We make no assumptions on the structure of the optimal tree T ; however  one optimal tree is a
binary tree  so we may restrict the solution to binary trees without loss of generality. To see this  let
i j wij|leaves(T [i ∨ j])|.
The objective considered in [Das16] focuses on minimizing costG(T ). We note than costG(T ) +
i j wi j  so the optimal solution to minimizing costG(T ) is the same as the optimal

i∈[n]

3

solution to maximizing revG(T ). In [Das16] it was shown that the optimal solution for any input is a
binary tree.
As mentioned  there are two common types of algorithms for hierarchical clustering: agglomerative
(bottom-up) algorithms and divisive (top-down) algorithms. In an agglomerative algorithm  each
vertex v ∈ V begins in separate cluster  and each iteration of the algorithm chooses two clusters to
merge into one. In a divisive algorithm  all vertices v ∈ V begin in a single cluster  and each iteration
of the algorithm selects a cluster with more than one vertex and partitions it into two small clusters.
In this section  we present some basic techniques which we later use to analyze the effect each
iteration has on the revenue. It will be convenient for us to think of the weight function as taking in
two vertices instead of an edge  i.e. w : V × V → R≥0. This is without loss of generality  because
we can always set the weight of any nonedge to zero (e.g. wvv = 0 ∀v ∈ V ).
To bound the performance of an algorithm it sufﬁces to bound revG(T ) and costG(T ) since revG(T )+
i j wi j. Further  let T ∗ denote the optimal hierarchical clustering. Then its revenue
ij wij. This is because any edge ij can have at most (n − 2)

costG(T ) = n(cid:80)
is at most revG(T ∗) ≤ (n − 2)(cid:80)

non-leaves for its subtree T [i ∨ j]; i and j are always leaves.

2.1 Analyzing Agglomerative Algorithms

a∈A b∈B wab.

merges A  B merge-revG(A  B).

Notice that the ﬁnal revenue revG(T ) is exactly the sum over iterations of the revenue gains  since
each edge is counted exactly once: when its endpoints are merged into a single cluster. Hence 

In this section  we discuss a method for bounding the performance of an agglomerative algorithm.
When an agglomerative algorithm merges two clusters A  B  this determines the least common
ancestor for any pair of nodes i  j where i ∈ A and j ∈ B. Given this  we deﬁne the revenue gain

due to merging A and B as  merge-revG(A  B) := (n − |A| − |B|)(cid:80)
revG(T ) =(cid:80)
possible. Deﬁne  merge-costG(A  B) := |B|(cid:80)
an additional 2(cid:80)
all pairs i  j this is the following  costG(T ) =(cid:80)
(cid:80)

We next deﬁne the cost of merging A and B as the following. This is the potential revenue lost by
merging A and B; revenue that can no longer be gained after A and B are merged  but was initially
b∈B c∈[n]\(A∪B) wbc.
The total cost of the tree T   costG(T )  is exactly the sum over iterations of the cost increases  plus
ij wij term that accounts for each edge being counted towards its own endpoints.
We can see why this is true if we consider a pair of vertices i  j ∈ [n] in the ﬁnal hierarchical
clustering T . If at some point a cluster containing i is merged with a third cluster before it gets
merged with the cluster containing j  then the number of leaves in T [i ∨ j] goes up by the size of the
third cluster. This is exactly the quantity captured by our cost increase deﬁnition. Aggregated over
i j∈[n] wij +

i j∈[n] wij|leaves(T [i ∨ j])| = 2(cid:80)

a∈A c∈[n]\(A∪B) wac + |A|(cid:80)

merges A  B merge-costG(A  B).

2.2 Analyzing Divisive Algorithms

Similar reasoning can be used for divisive algorithms. The following are revenue gain and
cost increase deﬁnitions for when a divisive algorithm partitions a cluster into two clusters
b b(cid:48)∈B wbb(cid:48) and split-costG(A  B) :=

A  B. Deﬁne  split-revG(A  B) := |B|(cid:80)
(|A| + |B|)(cid:80)

a a(cid:48)∈A waa(cid:48) + |A|(cid:80)

a∈A b∈B wab.

Consider the revenue gain. For a  a(cid:48) ∈ A we are now guaranteed that when the nodes in B are split
from A then every node in B will not be a leaf in T [a ∨ a(cid:48)] (and a symmetric term for when they
are both in B). On the cost side  the term counts the cost of any pairs a ∈ A and b ∈ B that are now
separated since we now know their subtree T [i ∨ j] has exactly the nodes in A ∪ B as leaves.

3 A Theoretical Guarantee for Average Linkage Agglomerative Clustering

In this section  we present the main result  a theoretical guarantee on average linkage clustering. We
additionally give a bad example lower bounding the best performance of the algorithm. See [MC12]
for details and background on this widely used algorithm. The formal deﬁnition of the algorithm

4

is given in the following pseudocode. The main idea is that initially all n input points are in their
own cluster and the algorithm recursively merges clusters until there is one cluster. In each step  the
algorithm mergers the clusters A and B such that the pair maximizes the average distances of points
between the two clusters 

(cid:80)

a∈A b∈B wab.

1|A||B|

Data: Vertices V   weights w : E → R≥0
Initialize clusters C ← ∪v∈V {v};
while |C| ≥ 2 do

Choose A  B ∈ C to maximize ¯w(A  B) := 1|A||B|
Set C ← C ∪ {A ∪ B} \ {A  B};

(cid:80)

a∈A b∈B wab;

end

Algorithm 1: Average Linkage

The following theorem establishes that this algorithm is only a small constant factor away from
optimal.
Theorem 3.1. Consider a graph G = (V  E) with nonnegative edge weights w : E → R≥0. Let the
hierarchical clustering T ∗ be a optimal solution maximizing of revG(·) and let T be the hierarchical
clustering returned by Algorithm 1. Then  revG(T ) ≥ 1
Proof. Consider an iteration of Algorithm 1. Let the current clusters be in the set C  and the algorithm
chooses to merge clusters A and B from C. When doing so  the algorithm attains a revenue gain
of the following. Let ¯w(A  B) = 1|A||B|
a∈A b∈B wab be the average weight of an edge between
points in A and B.

(cid:80)
merge-revG(A  B) = (n − |A| − |B|)

|C| (cid:88)

3 revG(T ∗).

(cid:88)

(cid:88)

wab =

wab

a∈A b∈B

C∈C\{A B}

a∈A b∈B

=

C∈C\{A B}

(cid:88)
merge-costG(A  B) = |B| (cid:88)
= |B| (cid:88)
(cid:88)
≤ (cid:88)

C∈C\{A B}

=

|C||A||B| ¯w(A  B)

a∈A c∈[n]\(A∪B)

b∈B c∈[n]\(A∪B)

C∈C\{A B}

a∈A c∈C

C∈C\{A B}

|B||A||C| ¯w(A  C) +

wbc

b∈B c∈C
|A||B||C| ¯w(B  C)

wbc

(cid:88)

wac + |A| (cid:88)
(cid:88)
wac + |A| (cid:88)
(cid:88)
(cid:88)

C∈C\{A B}

C∈C\{A B}

|B||A||C| ¯w(A  B) +

|A||B||C| ¯w(A  B)

while at the same time incurring a cost increase of:

C∈C\{A B}

= 2 · merge-revG(A  B)
(cid:88)

Intuitively  every time this algorithm loses two units of potential it cements the gain of one unit of
potential  which is why it is a 1
costG(T ) = 2

3-approximation. Formally:
merge-costG(A  B) ≤ 2

wij + 2 · (cid:88)

merge-revG(A  B)

(cid:88)

wij +

(cid:88)
(cid:88)

i j

≤ 2

merges A  B

wij + 2 · revG(T )

i j

(cid:88)
revG(T ) ≥ n
revG(T ) ≥ n − 2

ij

3

(cid:88)

ij

Now the revenue can be bounded as follows.

wij − costG(T ) ≥ n

wij ≥ 1
3

revG(T ∗)

5

i j

merges A  B

(cid:88)

ij

wij − 2

(cid:88)

i j

wij − 2 · revG(T )

where the last step follows from the fact that it is impossible to have more than n − 2 non-leaves.

u

1 + δ

v

···

···

n/2 nodes

n/2 nodes

Figure 1: Hard graph for Average Linkage (k = 2 case).

In the following  we establish that the algorithm is at best a 1/2 approximation. The proof can be
found in Section 1 of the supplementary material.
Lemma 3.2. Let  > 0 be any ﬁxed constant. There exists a graph G = (V  E) with nonnegative edge
weights w : E → R≥0  such that if the hierarchical clustering T ∗ is an optimal solution of revG(·)

and T is the hierarchical clustering returned by Average Linkage  revG(T ) ≤(cid:0) 1

2 + (cid:1) revG(T ∗).

4 A Lower Bound on Bisecting k-means

i=1

i=1

1|Si|

x∈Si

x y∈Si

(cid:80)

(cid:80)

(cid:80)

points and their cluster center: min(cid:80)k
rewritten as a sum over intra-cluster distances: min(cid:80)k

In this section  we consider the divisive algorithm which uses the k-means objective (with k = 2)
when choosing how to split clusters. Normally  the k-means objective concerns the distances between
||x − µi||2. However  it is known that this can be
(cid:80)
||x−y||2 [ABC+15]. In other
a a(cid:48)∈A ||a −
words  when splitting a cluster into two sets A and B  the algorithm minimizes 1|A|
b b(cid:48)∈B ||b − b(cid:48)||2. At ﬁrst glance  this appears to almost capture split-revG(A  B); the
a(cid:48)||2 + 1
key difference is that the summation has been scaled down by a factor of |A||B|. Of course  it also
(cid:80)
involves minimization over squared distances instead of maximization over similarity weights. We
show that the divisive algorithm which splits clusters by the natural k-means similarity objective 
namely max 1|A|
b b(cid:48)∈B wbb(cid:48)  is not a good approximation to the optimal
hierarchical clustering.
Lemma 4.1. There exists a graph G = (V  E) with nonnegative edge weights w : E → R≥0 
such that if the hierarchical clustering T ∗ is a maximizer of revG(·) and T is the hierarchical
clustering returned by the divisive algorithm which  splits clusters by the k-means similarity objective 
revG(T ) ≤ 1
√

a a(cid:48)∈A waa(cid:48) + 1|B|

(cid:80)

n) revG(T ∗).

B

Ω(

√

n. There are still unit weight edges between u and n

Proof. The plan is to exploit the fact that k-means is optimizing an objective function which differs
from the actual split revenue by a factor of |A||B|.
We use almost the same group as in the lower bound against Average Linkage  except that the weight
2 − 1 other
of the edge between u and v is
2 − 1 nodes. See Figure 1 for the structure
nodes and unit weight edges between v and the remaining n
of this graph. The key claim is that Divisive k-means will begin by separating u and v from all other
nodes.
It is easy to see that this split scores a value of 1
n under our alternate k-means objective function.
2
Why does no other split score better? Well  any other split can either keep u and v together or
separate them. If the split keeps the two together along with k other nodes  then it scores at most
√
n > 6. If the split separates the two  then it
1
k+2 [
scores at most 2  since at best each side can be a tree of weight one edges and hence has fewer edges
than nodes.
Now that we have established our key claim  it is easy to see that Divisive k-means is done scoring
on this graph  since it must next cut the edge uv and the other larger cluster has no edges in it. Hence
Divisive k-means will score

k+2 + 1  which is less than 1

n(n − 2) on this graph.

n + k] ≤ √

n if

√

√

√

√

n

2

6

As before  the optimal clustering may merge u with its other neighbors ﬁrst and v with its other
neighbors ﬁrst  scoring a revenue gain of 2 [(n − 2) + (n − 3) + ··· + (n/2)] = 3
4 n2− O(n). There
√
is a Ω(

n) gap between these revenues  completing the proof.

5 Divisive Local-Search

j∈A j(cid:54)=i wi j (resp.(cid:80)

In this section  we develop a simple local search algorithm and bound its approximation ratio. The
local search algorithm takes as input a cluster C and divides it into two clusters A and B to optimize
a local objective: the split revenue. In particular  initially A = B = ∅. Each node in C is added to A
or B uniformly at random.
Local search is run by moving individual nodes between A and B.

i ∈ A (resp. B) is added to B (resp. A) if (cid:80)
(cid:80)
j l∈B wj l + |B|(cid:80)
|A|(cid:80)

In a step  any point
j∈B wi j >
j l∈A wj l +
j∈B j(cid:54)=i wi j). This states that a point is moved to another set if the objective increases. The
algorithm performs these local moves until there is no node that can be moved to improve the
objective.
Data: Vertices V   weights w : E → R≥0
Initialize clusters C ← {V };
while some cluster C ∈ C has more than one vertex do
Let A  B be a uniformly random 2-partition of C;

j l∈A;j l(cid:54)=i wj l + (|A| − 1)(cid:80)
j∈A wi j >(cid:80)

j l∈B;j l(cid:54)=i wj l + (|B| − 1)(cid:80)

Run local search on A  B to maximize |B|(cid:80)

a a(cid:48)∈A waa(cid:48) + |A|(cid:80)

b b(cid:48)∈B wbb(cid:48)  considering just

moving a single node;
Set C ← C ∪ {A  B} \ {C};

end

Algorithm 2: Divisive Local-Search

1

1

3 revG(T ∗).

3 approximation.

ij wij  it sufﬁces to show that revG(T ) ≥

ij wij. We do this by considering possible local moves at every step.

In the following theorem  we show that the algorithm is arbitrarily close to a 1
Theorem 5.1. Consider a graph G = (V  E) with nonnegative edge weights w : E → R≥0. Let the
hierarchical clustering T ∗ be the optimal solution of revG(·) and let T be the hierarchical clustering
returned by Algorithm 2. Then  revG(T ) ≥ (n−6)
(n−2)

Consider any step of the algorithm and suppose the algorithm decides to partition a cluster into
a a(cid:48)∈A waa(cid:48) +
b b(cid:48)∈B wbb(cid:48). Assume without loss of generality that |B| ≥ |A|  and consider the expected local
search objective OBJ(cid:48) value for moving a random node from B to A. Note that the new local search
objective value is at most what the algorithm obtained  i.e. OBJ(cid:48) ≤ OBJ:

Proof. Since we know that revG(T ∗) ≤ (n − 2)(cid:80)
3 (n − 2)(cid:80)
A  B. As stated in the algorithm  its local search objective value is OBJ = |B|(cid:80)
|A|(cid:80)
(cid:0)|B|−1
(cid:1)
(cid:1) (cid:88)
(cid:0)|B|
|B| − 2
(cid:88)
(1 − 2
(cid:88)
(cid:88)

 (cid:88)
 (cid:88)
(cid:88)
= OBJ − (cid:88)

 + (|A| + 1)
 + (|A| + 1)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

wab + (|A| + 1)
wab + (− 2|A|

E[OBJ(cid:48)] = (|B| − 1)

= (|B| − 1)

1
|B|

1
|B|

= (|B| − 1)

|B| )

b b(cid:48)∈B

wbb(cid:48)

wab

wab




2

2

wbb(cid:48)

b b(cid:48)∈B

a∈A b∈B

a∈A b∈B

|B|

wbb(cid:48)

b b(cid:48)∈B

waa(cid:48) +

a a(cid:48)∈A

waa(cid:48) +

a a(cid:48)∈A

a∈A b∈B

waa(cid:48) +

a a(cid:48)∈A



|B| − 1
|B|
|B| − 1
|B|

waa(cid:48) +

a a(cid:48)∈A

a∈A b∈B

|B| + 1 − 2
|B| )

wbb(cid:48)

b b(cid:48)∈B

7

But since there are no improving moves we know the following.

0 ≥ E[OBJ(cid:48)] − OBJ = − (cid:88)

(cid:88)

waa(cid:48) +

|B| − 1
|B|

a a(cid:48)∈A

a∈A b∈B

wab − 2|A| − |B| + 2

|B|

(cid:88)

b b(cid:48)∈B

wbb(cid:48)

Rearranging terms and multiplying by |B| yields the following.

(cid:88)

wab ≤ |B| (cid:88)

a∈A b∈B

a a(cid:48)∈A

(|B| − 1)

waa(cid:48) + (2|A| − |B| + 2)

(cid:88)

b b(cid:48)∈B

wbb(cid:48)

We now consider three cases. Either (i) |B| ≥ |A| + 2  (ii) |B| = |A| + 1  or (iii) |B| = |A|. Case (i)
is straightforward:

(cid:19)

(cid:18) |B| − 1

|A| + |B|

split-costG(A  B) ≤ split-revG(A  B)
split-costG(A  B) ≤ split-revG(A  B)

1
2

In case (ii)  we use the fact that (x + 2)(x − 2) ≤ (x + 1)(x − 1) to simplify:

|A| + |B|

(cid:18) |B| − 1
(cid:18) |B| − 1
(cid:19)(cid:18) |B| − 1
(cid:18)|B| + 1
(cid:18) |B| − 2
(cid:18) 1

|B| + 2

1.5

|A| + |B|

|A| + |B|

|A| + |B|

(cid:19)
(cid:19)
(cid:19)
(cid:19)
(cid:19)

−

2

|A| + |B|

(cid:18)|A| + 1
(cid:18)|B| + 2

|A|

(cid:19)
(cid:19)

|B| + 1

split-costG(A  B) ≤
split-costG(A  B) ≤
split-costG(A  B) ≤ split-revG(A  B)
split-costG(A  B) ≤ split-revG(A  B)
split-costG(A  B) ≤ split-revG(A  B)

split-revG(A  B)

split-revG(A  B)

Case (iii) proceeds similarly; we now use the fact that (x + 2)(x − 3) ≤ (x)(x − 1) to simplify:

|A| + |B|

(cid:18) |B| − 1
(cid:18) |B| − 1
(cid:18) |B|
(cid:19)(cid:18) |B| − 1
(cid:18) |B| − 3
(cid:18) 1

|B| + 2

|A| + |B|

|A| + |B|

|A| + |B|

3

(cid:19)
(cid:19)
(cid:19)
(cid:19)
(cid:19)

−

2

|A| + |B|

(cid:18)|A| + 2
(cid:18)|B| + 2

|A|

(cid:19)
(cid:19)

|B|

split-costG(A  B) ≤
split-costG(A  B) ≤
split-costG(A  B) ≤ split-revG(A  B)
split-costG(A  B) ≤ split-revG(A  B)
split-costG(A  B) ≤ split-revG(A  B)

split-revG(A  B)

split-revG(A  B)

8

Hence we have shown that for each step of our algorithm  the split revenue is at least ( 1
times the split cost. We rewrite this inequality and then sum over all iterations:

2 − 3

|A|+|B| )

split-costG(A  B) − 3
(cid:88)

costG(T ) − 3

wij

i j∈[n]

split-revG(A  B) ≥ 1
2
revG(T ) ≥ 1
2

=

1
2

n

wij − revG(T )

wab

a∈A b∈B

(cid:88)
 − 3

(cid:88)

i j∈[n]

wij

i j∈[n]

(cid:88)
(cid:88)
(cid:88)

wij

wij

i j∈[n]

i j∈[n]

3
2

revG(T ) ≥ n − 6
revG(T ) ≥ n − 6

2

3

This is what we wanted to prove.
We note that it is possible to improve the loss in terms of n to n−4
b b(cid:48)∈B wbb(cid:48).

search objective (|B| − 1)(cid:80)

a a(cid:48)∈A waa(cid:48) + (|A| − 1)(cid:80)

n−2 by instead considering the local

6 Conclusion

One purpose of developing an analytic framework for problems is that it can help clarify and explain
our observations from practice. In this case  we have shown that average linkage is a 1
3-approximation
to a particular objective function  and the analysis that does so helps explain what average linkage
is optimizing. There is much more to explore in this direction. Are there other objective functions
which characterize other hierarchical clustering algorithms? For example  what are bisecting k-means 
single-linkage  and complete-linkage optimizing for?
An analytic framework can also serve to guide development of new algorithms. How well can this
dual objective be approximated? For example  we suspect that average linkage is actually a constant
approximation strictly better than 1
2 threshold? Perhaps the 1
2
threshold is due to a family of graphs which we do not expect to see in practice. Is there a natural
input restriction that would allow for better guarantees?

3. Could a smarter algorithm break the 1

References

[AB16]

Margareta Ackerman and Shai Ben-David. A characterization of linkage-based hierar-
chical clustering. Journal of Machine Learning Research  17:232:1–232:17  2016.

[ABBL12] Margareta Ackerman  Shai Ben-David  Simina Brânzei  and David Loker. Weighted
clustering. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelli-
gence  July 22-26  2012  Toronto  Ontario  Canada.  2012.

[ARV09]

[ABC+15] Pranjal Awasthi  Afonso S Bandeira  Moses Charikar  Ravishankar Krishnaswamy 
Soledad Villar  and Rachel Ward. Relax  no need to round: Integrality of clustering
formulations. In Proceedings of the 2015 Conference on Innovations in Theoretical
Computer Science  pages 191–200. ACM  2015.
Sanjeev Arora  Satish Rao  and Umesh V. Vazirani. Expander ﬂows  geometric embed-
dings and graph partitioning. J. ACM  56(2):5:1–5:37  2009.
Shai Ben-David and Margareta Ackerman. Measures of clustering quality: A working
set of axioms for clustering. In Advances in Neural Information Processing Systems 21 
Proceedings of the Twenty-Second Annual Conference on Neural Information Processing
Systems  Vancouver  British Columbia  Canada  December 8-11  2008  pages 121–128 
2008.

[BA08]

9

[CC17]

Moses Charikar and Vaggos Chatziafratis. Approximate hierarchical clustering via
sparsest cut and spreading metrics. In Proceedings of the Twenty-Eighth Annual ACM-
SIAM Symposium on Discrete Algorithms  SODA 2017  Barcelona  Spain  Hotel Porta
Fira  January 16-19  pages 841–854  2017.

[Das16]

[CKMM17] Vincent Cohen-Addad  Varun Kanade  Frederik Mallmann-Trenn  and Claire Mathieu.
Hierarchical clustering: Objective functions and algorithms. CoRR  abs/1704.02147 
2017.
Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering.
In
Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing 
STOC 2016  Cambridge  MA  USA  June 18-21  2016  pages 118–127  2016.
Katherine A. Heller and Zoubin Ghahramani. Bayesian hierarchical clustering. In
Machine Learning  Proceedings of the Twenty-Second International Conference (ICML
2005)  Bonn  Germany  August 7-11  2005  pages 297–304  2005.
Trevor Hastie  Robert Tibshirani  and Jerome Friedman. Unsupervised Learning  pages
485–585. Springer New York  New York  NY  2009.
Anil K. Jain. Data clustering: 50 years beyond k-means. Pattern Recognition Letters 
31(8):651 – 666  2010.

[HTF09]

[HG05]

[Jai10]

[MC12]

[KBXS12] Akshay Krishnamurthy  Sivaraman Balakrishnan  Min Xu  and Aarti Singh. Efﬁcient
active algorithms for hierarchical clustering. In Proceedings of the 29th International
Conference on Machine Learning  ICML 2012  Edinburgh  Scotland  UK  June 26 - July
1  2012  2012.
Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: an overview.
Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery  2(1):86–97  2012.
Aurko Roy and Sebastian Pokutta. Hierarchical clustering via spreading metrics. In
Advances in Neural Information Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages
2316–2324  2016.
Reza Zadeh and Shai Ben-David. A uniqueness theorem for clustering. In UAI 2009 
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence 
Montreal  QC  Canada  June 18-21  2009  pages 639–646  2009.

[ZB09]

[RP16]

10

,Joshua Wang