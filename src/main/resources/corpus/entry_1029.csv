2018,Compact Generalized Non-local Network,The non-local module is designed for capturing long-range spatio-temporal dependencies in images and videos. Although having shown excellent performance  it lacks the mechanism to model the interactions between positions across channels  which are of vital importance in recognizing fine-grained objects and actions. To address this limitation  we generalize the non-local module and take the correlations between the positions of any two channels into account. This extension utilizes the compact representation for multiple kernel functions with Taylor expansion that makes the generalized non-local module in a fast and low-complexity computation flow. Moreover  we implement our generalized non-local method within channel groups to ease the optimization. Experimental results illustrate the clear-cut improvements and practical applicability of the generalized non-local module on both fine-grained object recognition and video classification. Code is available at: https://github.com/KaiyuYue/cgnl-network.pytorch.,Compact Generalized Non-local Network

Kaiyu Yue† § Ming Sun† Yuchen Yuan† Feng Zhou‡ Errui Ding† Fuxin Xu§

†Baidu VIS ‡Baidu Research §Central South University

{yuekaiyu  sunming05  yuanyuchen02  zhoufeng09  dingerrui}@baidu.com

fxxu@csu.edu.cn

Abstract

The non-local module [27] is designed for capturing long-range spatio-temporal
dependencies in images and videos. Although having shown excellent performance 
it lacks the mechanism to model the interactions between positions across channels 
which are of vital importance in recognizing ﬁne-grained objects and actions. To
address this limitation  we generalize the non-local module and take the correlations
between the positions of any two channels into account. This extension utilizes the
compact representation for multiple kernel functions with Taylor expansion that
makes the generalized non-local module in a fast and low-complexity computation
ﬂow. Moreover  we implement our generalized non-local method within chan-
nel groups to ease the optimization. Experimental results illustrate the clear-cut
improvements and practical applicability of the generalized non-local module on
both ﬁne-grained object recognition and video classiﬁcation. Code is available at:
https://github.com/KaiyuYue/cgnl-network.pytorch.

1

Introduction

Figure 1: Comparison between non-local (NL) and compact generalized non-local (CGNL) networks on
recognizing an action video of kicking the ball. Given the reference patch (green rectangle) in the ﬁrst frame  we
visualize for each method the highly related responses in the other frames by thresholding the feature space.
CGNL network out-performs the original NL network in capturing the ball that is not only in long-range distance
from the reference patch but also corresponds to different channels in the feature map.

Capturing spatio-temporal dependencies between spatial pixels or temporal frames plays a key role in
the tasks of ﬁne-grained object and action classiﬁcation. Modeling such interactions among images
and videos is the major topic of various feature extraction techniques  including SIFT  LBP  Dense
Trajectory [26]  etc. In the past few years  deep neural network automates the feature designing
pipeline by stacking multiple end-to-end convolutional or recurrent modules  where each of them
processes correlation within spatial or temporal local regions. In general  capturing the long-range
dependencies among images or videos still requires multiple stacking of these modules  which greatly
hinders the learning and inference efﬁciency. A recent work [16] also suggests that stacking more
layers cannot always increase the effective receptive ﬁelds to capture enough local relations.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Inspired by the classical non-local means for image ﬁltering  the recently proposed non-local neural
network [27] addresses this challenge by directly modeling the correlation between any two positions
in the feature maps in a single module. Without bells and whistles  the non-local method can greatly
improve the performances of existing networks on many video classiﬁcation benchmarks. Despite
its great performances  the original non-local network only considers the global spatio-temporal
correlation by merging channels  and it might miss the subtle but important cross-channel clues for
discriminating ﬁne-grained objects or actions. For instance  the body  the ball and their interaction
are all necessary for describing the action of kicking the ball in Fig. 1  while the original non-local
operation learns to focus on the body part relations but neglect the body-ball interactions that usually
correspond to different channels of the input features.
To improve the effectiveness in ﬁne-grained object and action recognition tasks  this work extends
the non-local module by learning explicit correlations among all of the elements across the channels.
First  this extension scale-ups the representation power of the non-local operation to attend the
interaction between subtle object parts (e.g.  the body and ball in Fig. 1). Second  we propose its
compact representation for various kernel functions to address the high computation burden issue. We
show that as a self-contained module  the compact generalized non-local (CGNL) module provides
steady improvements in classiﬁcation tasks. Third  we also investigate the grouped CGNL blocks 
which model the correlations across channels within each group.
We evaluate the proposed CGNL method on the task of ﬁne-grained classiﬁcation and action recog-
nition. Extensive experimental results show that: 1) The CGNL network are easy to optimize as
the original non-local network; 2) Compared with the non-local module  CGNL module enjoys
capturing richer features and dense clues for prediction  as shown in Figure 1  which leads to results
substantially better than those of the original non-local module. Moreover  in the appendix of exten-
sional experiments  the CGNL network can also promise a higher accuracy than the baseline on the
large-scale ImageNet dataset [20].

2 Related Works

Channel Correlations: The mechanism of sharing the same conv kernel among channels of a
layer in a ConvNet [12] can be seen as a basic way to capture correlations among channels  which
aggregates the channels of feature maps by the operation of sum pooling. The SENet [10] may be the
ﬁrst work that explicitly models the interdependencies between the channels of its spatial features.
It aims to select the useful feature maps and suppress the others  and only considers the global
information of each channel. Inspired by [27]  we present the generalized non-local (GNL) module 
which generalizes the non-local (NL) module to learn the correlations between any two positions
across the channels. Compared to the SENet  we model the interdependencies among channels in an
explicit and dense manner.
Compact Representation: After further investigation  we ﬁnd that the non-local module contains a
second-order feature space (Sect.3.1)  which is used widely in previous computer vision tasks  e.g. 
SIFT [15]  Fisher encoding [17]  Bilinear model [14] [5] and segmentation task [2]. However  such
second-order feature space involves high dimensions and heavy computational burdens. In the area of
kernel learning [21]  there are many prior works such as compact bilinear pooling (CBP) [5] that uses
the Tensor Sketching [18] to address this problem. But this type of method is not perfect yet. Because
the it cannot produce a light computation to the various size of sketching vectors. Fortunately  in
mathematics  the whole non-local operation can be viewed as a trilinear formation. It can be fast
computed with the associative law of matrix production. To the other types of pairwise function  such
as Embedded Gaussian or RBF [19]  we propose a tight approximation for them by using the Taylor
expansion.

3 Approach

In this section  we introduce a general formulation of the proposed general non-local operation. We
then show that the original non-local and the bilinear pooling are special cases of this formulation.
After that  we illustrate that the general non-local operation can be seen as a modality in the trilinear
matrix production and show how to implement our generalized non-local (GNL) module in a compact
representations.

2

3.1 Review of Non-local Operation

We begin by brieﬂy reviewing the original non-local operation [27] in matrix form. Suppose that an
image or video is given to the network and let X ∈ RN×C denote (see notation1) the input feature
map of the non-local module  where C is the number of channels. For the sake of notation clarity 
we collapse all the spatial (width W and height H) and temporal (video length T ) positions in one
dimension  i.e.  N = HW or N = HW T . To capture long-range dependencies across the whole
feature map  the original non-local operation computes the response Y ∈ RN×C as the weighted
sum of the features at all positions 

(1)
where θ(·)  φ(·)  g(·) are learnable transformations on the input. In [27]  the authors suggest using
1 × 1 or 1 × 1 × 1 convolution for simplicity  i.e.  the transformations can be written as

Y = f(cid:0)θ(X)  φ(X)(cid:1)g(X) 

θ(X) = XWθ ∈ RN×C  φ(X) = XWφ ∈ RN×C 

(2)
parameterized by the weight matrices Wθ  Wφ  Wg ∈ RC×C respectively. The pairwise function
f (· ·) : RN×C × RN×C → RN×N computes the afﬁnity between all positions (space or space-time).
There are multiple choices for f  among which dot-product is perhaps the simplest one  i.e. 

g(X) = XWg ∈ RN×C 

f(cid:0)θ(X)  φ(X)(cid:1) = θ(X)φ(X)(cid:62).

(3)

Plugging Eq. 2 and Eq. 3 into Eq. 1 yields a trilinear interpretation of the non-local operation 

Y = XWθW(cid:62)
(4)
φ X(cid:62) ∈ RN×N encodes the similarity between any locations of
where the pairwise matrix XWθW(cid:62)
the input feature. The effect of non-local operation can be related to the self-attention module [1]
based on the fact that each position (row) in the result Y is a linear combination of all the positions
(rows) of XWg weighted by the corresponding row of the pairwise matrix.

φ X(cid:62)XWg 

3.2 Review of Bilinear Pooling

Analogous to the conventional kernel trick [21]  the idea of bilinear pooling [14] has recently been
adopted in ConvNets for enhancing the feature representation in various tasks  such as ﬁne-grained
classiﬁcation  person re-id  action recognition. At a glance  bilinear pooling models pairwise feature
interactions using explicit outer product at the ﬁnal classiﬁcation layer:

Z = X(cid:62)X ∈ RC×C 

of the ﬁnal descriptor zc1c2 =(cid:80)

(5)
where X ∈ RN×C is the input feature map generated by the last convolutional layer. Each element
n xnc1xnc2 sum-pools at each location n = 1 ···   N the bilinear
product xnc1 xnc2 of the corresponding channel pair c1  c2 = 1 ···   C.
Despite the distinct design motivation  it is interesting to see that bilinear pooling (Eq. 5) can be
viewed as a special case of the second-order term (Eq. 3) in the non-local operation if we consider 

θ(X) = X(cid:62) ∈ RC×N   φ(X) = X(cid:62) ∈ RC×N .

(6)

3.3 Generalized Non-local Operation

The original non-local operation aims to directly capture long-range dependencies between any two
positions in one layer. However  such dependencies are encoded in a joint location-wise matrix
f (θ(X)  φ(X)) by aggregating all channel information together. On the other hand  channel-wise
correlation has been recently explored in both discriminative [14] and generative [24] models through
the covariance analysis across channels. Inspired by these works  we generalize the original non-local
operation to model long-range dependencies between any positions of any channels.

1Bold capital letters denote a matrix X  bold lower-case letters a column vector x. xi represents the ith
column of the matrix X. xij denotes the scalar in the ith row and jth column of the matrix X. All non-bold
letters represent scalars. 1m ∈ Rm is a vector of ones. In ∈ Rn×n is an identity matrix. vec(X) denotes the
vectorization of matrix X. X ◦ Y and X ⊗ Y are the Hadamard and Kronecker products of matrices.

3

We ﬁrst reshape the output of the transformations (Eq. 2) on X by merging channel into position:

θ(X) = vec(XWθ) ∈ RN C  φ(X) = vec(XWφ) ∈ RN C  g(X) = vec(XWg) ∈ RN C.

(7)

By lifting the row space of the underlying transformations  our generalized non-local (GNL) operation
pursues the same goal of Eq. 1 that computes the response Y ∈ RN×C as:

vec(Y) = f(cid:0) vec(XWθ)  vec(XWφ)(cid:1) vec(XWg).

(8)

Compared to the original non-local operation (Eq. 4)  GNL utilizes a more general pairwise function
f (· ·) : RN C × RN C → RN C×N C that can differentiate between pairs of same location but at
different channels. This richer similarity greatly augments the non-local operation in discriminating
ﬁne-grained object parts or action snippets that usually correspond to channels of the input feature.
Compared to the bilinear pooling (Eq. 5) that can only be used after the last convolutional layer  GNL
maintains the input size and can thus be ﬂexibly plugged between any network blocks. In addition 
bilinear pooling neglects the spatial correlation which  however  is preserved in GNL.
Recently  the idea of dividing channels into groups has been established as a very effective technique
in increasing the capacity of ConvNets. Well-known examples include Xception [3]  MobileNet [9] 
ShufﬂeNet [31]  ResNeXt [29] and Group Normalization [28]. Given its simplicity and independence 
we also realize the channel grouping idea in GNL by grouping all C channels into G groups  each
of which contains C(cid:48) = C/G channels of the input feature. We then perform GNL operation
independently for each group to compute Y(cid:48) and concatenate the results along the channel dimension
to restore the full response Y.

3.4 Compact Representation

A straightforward implementation of GNL (Eq. 8) is prohibitive as the quadratic increase with respect
to the channel number C in the presence of the N C × N C pairwise matrix. Although the channel
grouping technique can reduce the channel number from C to C/G  the overall computational
complexity is still much higher than the original non-local operation. To mitigate this problem  this
section proposes a compact representation that leads to an affordable approximation for GNL.
Let us denote θ = vec(XWθ)  φ = vec(XWφ) and g = vec(XWg)  each of which is a N C-D
vector column. Without loss of generality  we assume f is a general kernel function (e.g.  RBF 
bilinear  etc.) that computes a N C × N C matrix composed by the elements 

(cid:2)f (θ  φ)(cid:3)

ij ≈ P(cid:88)

p=0

α2
p(θiφj)p 

(9)

which can be approximated by Taylor series up to certain order P . The coefﬁcient αp can be computed
in closed form once the kernel function is known. Taking RBF kernel for example 

[f (θ  φ)]ij = exp(−γ(cid:107)θi − φj(cid:107)2) ≈ P(cid:88)
and β = exp(cid:0) − γ((cid:107)θ(cid:107)2 + (cid:107)φ(cid:107)2)(cid:1) is a constant and β = exp(−2γ) if the input

(θiφj)p 

(2γ)p

(10)

p=0

β

p!

p = β (2γ)p

where α2
vectors θ and φ are (cid:96)2-normalized. By introducing two matrices 

p!

Θ = [α0θ0 ···   αP θP ] ∈ RN C×(P +1)  Φ = [α0φ0 ···   αP φP ] ∈ RN C×(P +1)

(11)

our compact generalized non-local (CGNL) operation approximates Eq. 8 via a trilinear equation 

vec(Y) ≈ ΘΦ(cid:62)g.

(12)

At ﬁrst glance  the above approximation still involves the computation of a large pairwise matrix
ΘΦ(cid:62) ∈ RN C×N C. Fortunately  the order of Taylor series is usually relatively small P (cid:28) N C.
According to the associative law  we could alternatively compute the vector z = Φ(cid:62)g ∈ RP +1 ﬁrst
and then calculate Θz in a much smaller complexity of O(N C(P + 1)). In another view  the process
that this bilinear form Φ(cid:62)g is squeezed into scalars can be treated as a related concept of the SE
module [10].

4

Complexity analysis: Table 1 compares the com-
putational complexity of CGNL network with the
GNL ones. We cannot afford for directly comput-
ing GNL operation because of its huge complexity of
O(2(N C)2) in both time and space. Instead  our com-
pact method dramatically eases the heavy calculation
to O(N C(P + 1)).

3.5

Implementation Details

Strategy
Time
Space

Table 1: Complexity comparison of GNL and
CGNL operations  where N and C indicate the
number of positions and channels respectively.

General NL Method

f(cid:0)ΘΦ(cid:62)(cid:1)g

O(2(N C)2)
O(2(N C)2)

CGNL Method

ΘΦ(cid:62)g

O(N C(P + 1))
O(N C(P + 1))

Figure 2: Grouped compact generalized non-local (CGNL) module. The feature maps are shown with the
shape of their tensors  e.g.  [C  N ]  where N = T HW or N = HW . The feature maps will be divided along
channels into multiple groups after three conv layers whose kernel size and stride both equals 1 (k = 1  s = 1).
The channels dimension is grouped into C(cid:48) = C/G  where G is a group number. The compact representations
for generalized non-local module are build within each group. P indicates the order of Taylor expansion for
kernel functions.

Fig. 2 illustrates the workﬂow of how CGNL module processes a feature map X of the size N × C 
where N = H × W or N = T × H × W . X is ﬁrst fed into three 1 × 1 × 1 convolutional layers
that are described by the weights Wθ  Wφ  Wg respectively in Eq. 7. To improve the capacity of
neural networks  the channel grouping idea [29  28] is then applied to divide the transformed feature
along the channel dimension into G groups. As shown in Fig. 2  we approximate for each group
the GNL operation (Eq. 8) using the Taylor series according to Eq. 12. To achieve generality and
compatibility with existing neural network blocks  the CGNL block is implemented by wrapping
Eq. 8 in an identity mapping of the input as in residual learning [8]:
Z = concat(BN (Y(cid:48)Wz)) + X 

(13)
where Wz ∈ RC×C denotes a 1 × 1 or 1 × 1 × 1 convolution layer followed by a Batch Normaliza-
tion [11] in each group.

4 Experiments

4.1 Datasets

We evaluate the CGNL network on multiple tasks  including ﬁne-grained classiﬁcation and action
recognition. For ﬁne-grained classiﬁcation  we experiment on the Birds-200-2011 (CUB) dataset [25] 
which contains 11788 images of 200 bird categories. For action recognition  we experiment on
two challenging datasets  Mini-Kinetics [30] and UCF101 [22]. The Mini-Kinetics dataset contains
200 action categories. Due to some video links are unavaliable to download  we use 78265 videos
for training and 4986 videos for validation. The UCF101 dataset contains 101 actions  which are
separated into 25 groups with 4-7 videos of each action in a group.

4.2 Baselines

Given the steady performance and efﬁciency  the ResNet [8] series (ResNet-50 and ResNet-101) are
adopted as our baselines. For video tasks  we keep the same architecture conﬁguration with [27] 
where the temporal dimension is trivially addressed by max pooling. Following [27] the convolutional
layers in the baselines are implemented as 1 × k × k kernels  and we insert our CGNL blocks into

5

Ө**[ NC′  P+1 ][ P+1  NC′ ][ NC′  1 ]=*[ NC′  P+1 ]=[ P+1  1 ]YXInput[ N  C ]conv_θ conv_Ø conv_g k=1  s=1groups groups groups +Identity Mapping[ NC′  1 ]concatenate groupsconv_z+ BN ZOutput[ N  C ]Φ ㄒΦㄒk=1  s=1ӨTable 2: Ablations. Top1 and top5 accuracy (%) on various datasets.

(a) Results of adding 1 CGNL
block on CUB. The kernel of dot
production achieves the best result.
The accuracies of others are at the
edge of baselines.

model

R-50.

Dot Production
Gaussian RBF
Embedded Gaussian

top1

84.05

85.14
84.10
84.01

top5

96.00

96.88
95.78
96.08

(b) Results of comparison on UCF-
101. Note that CGNL network is not
grouped in channel.

model

R-50.

+ 1 NL block
+ 1 CGNL block

top1

81.62

82.88
83.38

top5

94.62

95.74
95.42

(c) Results of channel grouped CGNL networks on CUB. A few groups
can boost the performance. But more groups tend to prevent the CGNL
block from capturing the correlations between positions across channels.

model

groups

R-101

+ 1 CGNL
block

-

1
4
8
16
32

top1

85.05

86.17
86.24
86.35
86.13
86.04

top5

96.70

97.82
97.05
97.86
96.75
96.69

model

groups

R-101

+ 5 CGNL
block

-

1
4
8
16
32

top1

85.05

86.01
86.19
86.24
86.43
86.10

top5

96.70

95.97
96.07
97.23
98.89
97.13

(d) Results of grouped CGNL networks on Mini-Kinetics. More groups
help the CGNL networks improve top1 accuracy obveriously.

model

R-50

+ 1 CGNL
block

gorups

-

1
4
8

top1

75.54

77.16
77.56
77.76

top5

92.16

93.56
93.00
93.18

model

R-101

+ 1 CGNL
block

gorups

-

1
4
8

top1

77.44

78.79
79.06
79.54

top5

93.18

93.64
93.54
93.84

the network to turn them into compact generalized non-local (CGNL) networks. We investigate the
conﬁgurations of adding 1 and 5 blocks. [27] suggests that adding 1 block on the res4 is slightly
better than the others. So our experiments of adding 1 block all target the res4 of ResNet. The
experiments of adding 5 blocks  on the other hand  are conﬁgured by inserting 2 blocks on the res3 
and 3 blocks on the res4  to every other residual block in ResNet-50 and ResNet-101.
Training: We use the models pretrained on ImageNet [20] to initialize the weights. The frames of a
video are extracted in a dense manner. Following [27]  we generate 32-frames input clips for models 
ﬁrst randomly crop out 64 consecutive frames from the full-length video and then drop every other
frame. The way to choose these 32-frames input clips can be viewed as a temporal augmentation.
The crop size for each clip is distributed evenly between 0.08 and 1.25 of the original image and its
aspect ratio is chosen randomly between 3/4 and 4/3. Finally we resize it to 224. We use a weight
decay of 0.0001 and momentum of 0.9 in default. The strategy of gradual warmup is used in the
ﬁrst ten epochs. The dropout [23] with ratio 0.5 is inserted between average pooling layer and last
fully-connected layer. To keep same with [27]  we use zero to initialize the weight and bias of the
BatchNorm (BN) layer in both CGNL and NL blocks [6]. To train the networks on CUB dataset  we
follow the same training strategy above but the ﬁnal crop size of 448.
Inference: The models are tested immediately after training is ﬁnished. In [27]  spatially fully-
convolutional inference 2 is used for NL networks. For these video clips  the shorter side is resized to
256 pixels and use 3 crops to cover the entire spatial size along the longer side. The ﬁnal prediction
is the averaged softmax scores of all clips. For ﬁne-grined classiﬁcation  we do 1 center-crop testing
in size of 448.

4.3 Ablation Experiments

Kernel Functions: We use three popular kernel functions  namely dot production  embedded
Gaussian and Gaussian RBF  in our ablation studies. For dot production  Eq. 12 will be held for
direct computation. For embedded Gaussian  the α2
p! in Eq. 9. And for Gaussian RBF 
the corresponding formula is deﬁned as Eq. 10. We expend the Taylor series with third order and
the hyperparameter γ for RBF is set by 1e-4 [4]. Table 2a suggests that dot production is the best
kernel functions for CGNL networks. Such experimental observations are consistent with [27]. The
other kernel functions we used  Embedded Gaussion and Gaussian RBF  has a little improvements
for performance. Therefore  we choose the dot production as our main experimental conﬁguration for
other tasks.

p will be 1

2https://github.com/facebookresearch/video-nonlocal-net

6

Grouping: The grouping strategy is another important technique. On Mini-Kinetics  Table 2d shows
that grouping can bring higher accuracy. The improvements brought in by adding groups are larger
than those by reducing the channel reduction ratio. The best top1 accuracy is achieved by splitting
into 8 groups for CGNL networks. On the other hand  however  it is worthwhile to see if more groups
can always improve the results  and Table 2c gives the answer that more groups will hamper the
performance improvements. This is actually expected  as the afﬁnity in CGNL block considers the
points across channels. When we split the channels into a few groups  it can facilitate the restricted
optimization and ease the training. However  if too many groups are adopted  it hinder the afﬁnity to
capture the rich correlations between elements across the channels.

Table 3: Results comparison of
the CGNL block to the simple
residual block on CUB dataset.

Figure 3: The workﬂow of our
CGNL block. The corresponding
formula is shown below in a blue
tinted box.

Figure 4: The workﬂow of the
simple residual block for compar-
ison. The corresponding formula is
shown below in a blue tinted box.

model

R-50

+ 1 Residual Block
+ 1 CGNL block

top1

84.05

84.11
85.14

top5

96.00

96.23
96.88

√

∗ γ + β = Θ−E(Θ)

√

V ar(Θ)

√

V ar(sΘ)

∗ γ + β = sΘ−sE(Θ)

Comparison of CGNL Block to Simple Residual Block: There is a confusion about the efﬁciency
caused by the possibility that the scalars from Φ(cid:62)g in Eq. 12 could be wiped out by the BN layer.
Because according to Algorithm 1 in [11]  the output of input Θ weighted by the scalars s = Φ(cid:62)g
can be approximated to O = sΘ−E(sΘ)
∗ γ + β. At
ﬁrst glance  the scalars s is totally erased by BN in this mathmatical process. However  the de facto
operation of a convolutional module has a process order to aggregate the features. Before passing into
the BN layer  the scalars s has already saturated in the input features Θ and then been transformed
into a different feature space by a learnable parameter Wz. In other words  it is Wz that "protects" s
from being erased by BN via the convolutional operation. To eliminate this confusion  we further
compare adding 1 CGNL block (with the kernel of dot production) in Fig 3 and adding 1 simple
residual block in Fig 4 on CUB dataset in Table 3. The top1 accuracy 84.11% of adding a simple
residual block is slightly better than 84.05% of the baseline  but still worse than 85.14% of adding a
linear kerenlized CGNL module. We think that the marginal improvement (84.06% → 84.11%) is
due to the more parameters from the added simple residual block.

s2V ar(Θ)

Figure 5: Result analysis of the NL block and our CGNL block on CUB. Column 1: the input images with
a small reference patch (green rectangle)  which is used to ﬁnd the highly related patches (white rectangle).
Column 2: the highly related clues for prediction in each feature map found by the NL network. The dimension
of self-attention space in NL block is N × N  where N = HW . So its visualization only has one column.
Columns 3 to 7: the most related patches computed by our compact generalized non-local module. We ﬁrst
pick a reference position in the space of g  then we use the corresponding vectors in Θ and Φ to compute the
attention maps with a threshold (here we use 0.7). Last column: the ground truth of body parts. The highly
related areas of CGNL network can easily cover all of the standard parts that provide the prediction clues.

7

Figure 6: Visualization with feature heatmaps. We select a reference patch (green rectangle) in one frame  then
visualize the high related ares by heatmaps. The CGNL network enjoys capturing dense relationships in feature
space than NL networks.

Table 4: Main results. Top1 and top5 accuracy (%) on various datasets.

(a) Main validation results on
Mini-Kinetics. The CGNL net-
works is build within 8 groups.

model

R-50

+ 1 NL block
+ 1 CGNL block

+ 5 NL block
+ 5 CGNL block

R-101

+ 1 NL block
+ 1 CGNL block

+ 5 NL block
+ 5 CGNL block

top1

75.54

76.53
77.76

77.53
78.79

77.44

78.02
79.54

79.21
79.88

top5

92.16

92.90
93.18

94.00
94.37

93.18

93.86
93.84

93.21
93.37

(b) Results on CUB. The CGNL networks are set by 8 channel groups.

model

R-50

+ 1 NL block
+ 1 CGNL block

+ 5 NL block
+ 5 CGNL block

top1

84.05

84.79
85.14

85.10
85.68

top5

96.00

96.76
96.88

96.18
96.69

model

R-101

+ 1 NL block
+ 1 CGNL block

+ 5 NL block
+ 5 CGNL block

top1

85.05

85.49
86.35

86.10
86.24

top5

96.70

97.04
97.86

96.35
97.23

(c) Results on COCO. 1 NL or 1 CGNL block is added in Mask R-CNN.

model

Baseline

+ 1 NL block
+ 1 CGNL block

APbox
34.47

35.02
35.70

APbox
50
54.87

55.79
56.07

APbox
75
36.58

37.54
38.69

APmask
30.44

30.23
31.22

APmask
50
51.55

52.40
52.44

APmask
75
31.95

32.77
32.67

4.4 Main Results

Table 4a shows that although adding 5 NL and CGNL blocks in the baseline networks can both
improve the accuracy  the improvement of CGNL network is larger. The same applies to Table 2b
and Table 4b. In experiments on UCF101 and CUB dataset  the similar results are also observed that
adding 5 CGNL blocks provides the optimal results both for R-50 and R-101.
Table 4a shows the main results on Mini-Kinetics dataset. Compared to the baseline R-50 whose top1
is 75.54%  adding 1 NL block brings improvement by about 1.0%. Similar results can be found in
the experiments based on R-101  where adding 1 CGNL provides about more than 2% improvement 
which is larger than that of adding 1NL block. Table 2b shows the main results on the UCF101
dataset  where adding 1CGNL block achieves higher accuracy than adding 1NL block. And Table 4b
shows the main results on the CUB dataset. To understand the effects brought by CGNL network  we
show the visualization analysis as shown in Fig 5 and Fig 6. Additionly  to investigate the capacity
and the generalization ability of our CGNL network. We test them on the task of object detection and

8

instance segmentation. We add 1 NL and 1 CGNL block in the R-50 backbone for Mask-RCNN [7].
Table 4c shows the main results on COCO2017 dataset [13] by adopting our 1 CGNL block in the
backbone of Mask-RCNN [7]. It shows that the performance of adding 1 CGNL block is still better
than that of adding 1 NL block.
We observe that adding CGNL block can always obtain better results than adding the NL block with
the same blocks number. These experiments suggest that considering the correlations between any
two positions across the channels can signiﬁcantly improve the performance than that of original
non-local methods.

5 Conclusion

We have introduced a simple approximated formulation of compact generalized non-local operation 
and have validated it on the task of ﬁne-grained classiﬁcation and action recognition from RGB images.
Our formulation allows for explicit modeling of rich interdependencies between any positions across
channels in the feature space. To ease the heavy computation of generalized non-local operation  we
propose a compact representation with the simple matrix production by using Taylor expansion for
multiple kernel functions. It is easy to implement and requires little additional parameters  making it
an attractive alternative to the original non-local block  which only considers the correlations between
two positions along the speciﬁc channel. Our model produces competitive or state-of-the-art results
on various benchmarked datasets.

Appendix: Experiments on ImageNet

As a general method  the CGNL block is compatible with complementary techniques developed for
the image task of ﬁne-grained classiﬁcation  temporal feature needed task of action recognition and
the basic task of object detection.

Table 5: Results on ImageNet. Best top1 and top5 accuracy (%).

model

R-50

+ 1 CGNL block
+ 1 CGNLx block

R-152

+ 1 CGNL block
+ 1 CGNLx block

top1

76.15

77.69
77.32

78.31

79.53
79.37

top5

92.87

93.64
93.46

94.06

94.59
94.47

In this appendix  we further report the results of our spatial CGNL network on the large-scale
ImageNet [20] dataset  which has 1.2 million training images and 50000 images for validation in
1000 object categories. The training strategy and conﬁgurations of our CGNL networks is kept same
as those in Sec 4  only except the crop size here used for input is 224. For a better demonstration
of the generality of our CGNL network  we investigate both adding 1 dot production CGNL block
and 1 Gaussian RBF CGNL block (identiﬁed by CGNLx) in Table 5. We compare these models with
two strong baselines  R-50 and R-152. In Table 5  all the best top1 and top5 accuracies are reported
under the single center crop testing. The CGNL networks beat the basemodels by larger than 1 point
no matter whichever the dot production or Gaussian RBF plays as the kernel function in the CGNL
module.

9

References
[1] N. P. J. U. L. J. A. N. G. L. K. Ashish Vaswani  Noam Shazeer and I. Polosukhin. Attention is all you need.

In NIPS  2017.

[2] J. Carreira  R. Caseiro  J. Batista  and C. Sminchisescu. Semantic segmentation with second-order pooling.

In European Conference on Computer Vision  pages 430–443. Springer  2012.

[3] F. Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint  2016.
[4] Y. Cui  F. Zhou  J. Wang  X. Liu  Y. Lin  and S. Belongie. Kernel pooling for convolutional neural networks.

In Computer Vision and Pattern Recognition (CVPR)  2017.

[5] Y. Gao  O. Beijbom  N. Zhang  and T. Darrell. Compact bilinear pooling. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition  pages 317–326  2016.

[6] P. Goyal  P. Dollár  R. Girshick  P. Noordhuis  L. Wesolowski  A. Kyrola  A. Tulloch  Y. Jia  and K. He.

Accurate  large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

[7] K. He  G. Gkioxari  P. Dollár  and R. Girshick. Mask r-cnn. In Computer Vision (ICCV)  2017 IEEE

International Conference on  pages 2980–2988. IEEE  2017.

[8] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

[9] A. G. Howard  M. Zhu  B. Chen  D. Kalenichenko  W. Wang  T. Weyand  M. Andreetto  and H. Adam.
Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861  2017.

[10] J. Hu  L. Shen  and G. Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507  2017.
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. arXiv preprint arXiv:1502.03167  2015.

[12] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[13] T.-Y. Lin  M. Maire  S. Belongie  J. Hays  P. Perona  D. Ramanan  P. Dollár  and C. L. Zitnick. Microsoft
coco: Common objects in context. In European conference on computer vision  pages 740–755. Springer 
2014.

[14] T.-Y. Lin  A. RoyChowdhury  and S. Maji. Bilinear cnn models for ﬁne-grained visual recognition. In

Proceedings of the IEEE International Conference on Computer Vision  pages 1449–1457  2015.

[15] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer

vision  60(2):91–110  2004.

[16] W. Luo  Y. Li  R. Urtasun  and R. Zemel. Understanding the effective receptive ﬁeld in deep convolutional

neural networks. In Advances in Neural Information Processing Systems  pages 4898–4906  2016.

[17] F. Perronnin  J. Sánchez  and T. Mensink. Improving the ﬁsher kernel for large-scale image classiﬁcation.

In European conference on computer vision  pages 143–156. Springer  2010.

[18] N. Pham and R. Pagh. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of
the 19th ACM SIGKDD international conference on Knowledge discovery and data mining  pages 239–247.
ACM  2013.

[19] T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of the IEEE  78(9):1481–

1497  1990.

[20] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy  A. Khosla 
M. Bernstein  et al. Imagenet large scale visual recognition challenge. International Journal of Computer
Vision  115(3):211–252  2015.

[21] B. Scholkopf and A. J. Smola. Learning with kernels: support vector machines  regularization  optimization 

and beyond. MIT press  2001.

[22] K. Soomro  A. R. Zamir  and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the

wild. arXiv preprint arXiv:1212.0402  2012.

[23] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A simple way to
prevent neural networks from overﬁtting. The Journal of Machine Learning Research  15(1):1929–1958 
2014.

[24] I. Ustyuzhaninov  W. Brendel  L. A. Gatys  and M. Bethge. What does it take to generate natural textures?

[25] C. Wah  S. Branson  P. Welinder  P. Perona  and S. Belongie. The Caltech-UCSD birds-200-2011 dataset.

Technical Report CNS-TR-2011-001  California Institute of Technology  2011.

[26] H. Wang  A. Kläser  C. Schmid  and C.-L. Liu. Action recognition by dense trajectories. In Computer

Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on  pages 3169–3176. IEEE  2011.

[27] X. Wang  R. Girshick  A. Gupta  and K. He. Non-local neural networks. arXiv preprint arXiv:1711.07971 

In ICLR  2017.

2017.

[28] Y. Wu and K. He. Group normalization. arXiv preprint arXiv:1803.08494  2018.
[29] S. Xie  R. Girshick  P. Dollár  Z. Tu  and K. He. Aggregated residual transformations for deep neural
In Computer Vision and Pattern Recognition (CVPR)  2017 IEEE Conference on  pages

networks.
5987–5995. IEEE  2017.

[30] S. Xie  C. Sun  J. Huang  Z. Tu  and K. Murphy. Rethinking spatiotemporal feature learning for video

understanding. arXiv preprint arXiv:1712.04851  2017.

[31] X. Zhang  X. Zhou  M. Lin  and J. Sun. Shufﬂenet: An extremely efﬁcient convolutional neural network

for mobile devices. arXiv preprint arXiv:1707.01083  2017.

10

,Asish Ghoshal
Jean Honorio
Kaiyu Yue
Ming Sun
Yuchen Yuan
Feng Zhou
Errui Ding
Fuxin Xu