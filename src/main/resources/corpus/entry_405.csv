2013,Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising,Stacked sparse denoising auto-encoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However  like most denoising techniques  the SSDA is not robust to variation in noise types beyond what it has seen during training. We present the multi-column stacked sparse denoising autoencoder  a novel technique of combining multiple SSDAs into a multi-column SSDA (MC-SSDA) by combining the outputs of each SSDA. We eliminate the need to determine the type of noise  let alone its statistics  at test time. We show that good denoising performance can be achieved with a single system on a variety of different noise types  including ones not seen in the training set. Additionally  we experimentally demonstrate the efficacy of MC-SSDA denoising by achieving MNIST digit error rates on denoised images at close to that of the uncorrupted images.,Adaptive Multi-Column Deep Neural Networks

with Application to Robust Image Denoising

Forest Agostinelli

Michael R. Anderson

Honglak Lee

Division of Computer Science and Engineering

University of Michigan

{agostifo mrander honglak}@umich.edu

Ann Arbor  MI 48109  USA

Abstract

Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be
successful at removing noise from corrupted images. However  like most denois-
ing techniques  the SSDA is not robust to variation in noise types beyond what
it has seen during training. To address this limitation  we present the adaptive
multi-column stacked sparse denoising autoencoder (AMC-SSDA)  a novel tech-
nique of combining multiple SSDAs by (1) computing optimal column weights
via solving a nonlinear optimization program and (2) training a separate network
to predict the optimal weights. We eliminate the need to determine the type of
noise  let alone its statistics  at test time and even show that the system can be
robust to noise not seen in the training set. We show that state-of-the-art denois-
ing performance can be achieved with a single system on a variety of different
noise types. Additionally  we demonstrate the efﬁcacy of AMC-SSDA as a pre-
processing (denoising) algorithm by achieving strong classiﬁcation performance
on corrupted MNIST digits.

Introduction

1
Digital images are often corrupted with noise during acquisition and transmission  degrading perfor-
mance in later tasks such as: image recognition and medical diagnosis. Many denoising algorithms
have been proposed to improve the accuracy of these tasks when corrupted images must be used.
However  most of these methods are carefully designed only for a certain type of noise or require
assumptions about the statistical properties of the corrupting noise.
For instance  the Wiener ﬁlter [30] is an optimal linear ﬁlter in the sense of minimum mean-square
error and performs very well at removing speckle and Gaussian noise  but the input signal and noise
are assumed to be wide-sense stationary processes  and known autocorrelation functions of the input
are required [7]. Median ﬁltering outperforms linear ﬁltering for suppressing noise in images with
edges and gives good output for salt & pepper noise [2]  but it is not as effective for the removal
of additive Gaussian noise [1]. Periodic noise such as scan-line noise is difﬁcult to eliminate using
spatial ﬁltering but is relatively easy to remove using Fourier domain band-stop ﬁlters once the
period of the noise is known [6].
Much of this research has taken place in the ﬁeld of medical imaging  most recently because of a
drive to reduce patient radiation exposure. As radiation dose is decreased  noise levels in medical
images increases [12  16]  so noise reduction techniques have been key to maintaining image quality
while improving patient safety [27]. In this application  assumptions must also be made or statistical
properties must also be determined for these techniques to perform well [26].
Recently  various types of neural networks have been evaluated for their denoising efﬁcacy. Xie
et al. [31] had success at removing noise from corrupted images with the stacked sparse denoising

1

autoencoder (SSDA). The SSDA is trained on images corrupted with a particular noise type  so it
too has a dependence on a priori knowledge about the general nature of the noise.
In this paper  we present the adaptive multi-column sparse stacked denoising autoencoder (AMC-
SSDA)  a method to improve the SSDA’s robustness to various noise types. In the AMC-SSDA 
columns of single-noise SSDAs are run in parallel and their outputs are linearly combined to pro-
duce the ﬁnal denoised image. Taking advantage of the sparse autoencoder’s capability for learning
features  the features encoded by the hidden layers of each SSDA are supplied to an additional
network to determine the optimal weighting for each column in the ﬁnal linear combination.
We demonstrate that a single AMC-SSDA network provides better denoising results for both noise
types present in the training set and for noise types not seen by the denoiser during training. A given
instance of noise corruption might have features in common with one or more of the training set noise
types  allowing the best combination of denoisers to be chosen based on that image’s speciﬁc noise
characteristics. With our method  we eliminate the need to determine the type of noise  let alone its
statistics  at test time. Additionally  we demonstrate the efﬁcacy of AMC-SSDA as a preprocessing
(denoising) algorithm by achieving strong classiﬁcation performance on corrupted MNIST digits.

2 Related work

Numerous approaches have been proposed for image denoising using signal processing techniques
(e.g.  see [23  8] for a survey). Some methods transfer the image signal to an alternative domain
where noise can be easily separated from the signal [25  21]. Portilla et al. [25] proposed a wavelet-
based Bayes Least Squares with a Gaussian Scale-Mixture (BLS-GSM) method. More recent ap-
proaches exploit the “non-local” statistics of images: different patches in the same image are often
similar in appearance  and thus they can be used together in denoising [11  22  8]. This class of
algorithms—BM3D [11] in particular—represents the current state-of-the-art in natural image de-
noising; however  it is targeted primarily toward Gaussian noise.
In our preliminary evaluation 
BM3D did not perform well on many of the variety of noise types.
While BM3D is a well-engineered algorithm  Burger et al. [9] showed that it is possible to achieve
state-of-the-art denoising performance with a plain multi-layer perceptron (MLP) that maps noisy
patches onto noise-free ones  once the capacity of the MLP  the patch size  and the training set are
large enough. Therefore  neural networks indeed have a great potential for image denoising.
Vincent et al. [29] introduced the stacked denoising autoencoders as a way of providing a good initial
representation of the data in deep networks for classiﬁcation tasks. Our proposed AMC-SSDA builds
upon this work by using the denoising autoencoder’s internal representation to determine the optimal
column weighting for robust denoising.
Cires¸an et al. [10] presented a multi-column approach for image classiﬁcation  averaging the output
of several deep neural networks (or columns) trained on inputs preprocessed in different ways. How-
ever  based on our experiments  this approach (i.e.  simply averaging the output of each column) is
not robust in denoising since each column has been trained on a different type of noise. To address
this problem  we propose an adaptive weighting scheme that can handle a variety of noise types.
Jain et al. [18] used deep convolutional neural networks for image denoising. Rather than using
a convolutional approach  our proposed method applies multiple sparse autoencoder networks in
combination to the denoising task. Tang et al. [28] applied deep learning techniques (e.g.  extensions
of the deep belief network with local receptive ﬁelds) to denoising and classifying MNIST digits. In
comparison  we achieve favorable classiﬁcation performance on corrupted MNIST digits.

3 Algorithm

In this section  we ﬁrst describe the SSDA [31]. Then we will present the AMC-SSDA and describe
the process of ﬁnding optimal column weights and predicting column weights for test images.

3.1 Stacked sparse denoising autoencoders
A denoising autoencoder (DA) [29] is typically used as a way to pre-train layers in a deep neural
network  avoiding the difﬁculty in training such a network as a whole from scratch by performing
greedy layer-wise training (e.g.  [4  5  14]). As Xie et al. [31] showed  a denoising autoencoder is

2

also a natural ﬁt for performing denoising tasks  due to its behavior of taking a noisy signal as input
and reconstructing the original  clean signal as output.
Commonly  a series of DAs are connected to form a stacked denoising autoencoder (SDA)—a deep
network formed by feeding the hidden layer’s activations of one DA into the input of the next DA.
Typically  SDAs are pre-trained in an unsupervised fashion where each DA layer is trained by gen-
erating new noise [29]. We follow Xie et al.’s method of SDA training by calculating the ﬁrst layer
activations for both the clean input and noisy input to use as training data for the second layer. As
they showed  this modiﬁcation to the training process allows the SDA to better learn the features for
denoising the original corrupting noise.
More formally  let y ∈ RD be an instance of uncorrupted data and x ∈ RD be the corrupted version
of y. We can deﬁne the feedforward functions of the DA with K hidden units as follows:

1

h(x) = f (Wx + b)
ˆy(x) = g(W(cid:48)h + b(cid:48))

(1)
(2)
where f () and g() are respectively encoding and decoding functions (for which sigmoid function
1+exp(−s) is often used) 1 W ∈ RK×D and b ∈ RK are encoding weights and biases 
σ(s) =
and W(cid:48) ∈ RD×K and b(cid:48) ∈ RD are the decoding weights and biases. h(x) ∈ RK is the hidden
layer’s activation  and ˆy(x) ∈ RD is the reconstruction of the input (i.e.  the DA’s output). Given
training data D = {(x1  y1)  ...  (xN   yN )} with N training examples  the DA is trained by back-
propagation to minimize the sparsity regularized reconstruction loss given by
((cid:107)W(cid:107)2

(3)
where Θ = {W  b  W(cid:48)  b(cid:48)} are the parameters of the model  and the sparsity-inducing term
KL(ρ(cid:107)ˆρj) is the Kullback-Leibler divergence between ρ (target activation) and ˆρj (empirical av-
erage activation of the j-th hidden unit) [20  13]:
+ (1 − ρ) log

LDA(D; Θ) =

(cid:107)yi − ˆy(xi)(cid:107)2

2 + β

F + (cid:107)W(cid:48)(cid:107)2
F)

KL(ˆρj(cid:107)ρ) = ρ log

KL(ρ(cid:107)ˆρj) +

N(cid:88)

i=1

K(cid:88)

j=1

N(cid:88)

where

ˆρj =

hj(xi)

(4)

1
N

λ
2

ρ
ˆρj

(1 − ρ)
1 − ˆρj

1
N

i=1
and λ  β  and ρ are scalar-valued hyperparameters determined by cross validation.
In this work  two DAs are stacked as shown in Figure 1a  where the activation of the ﬁrst DA’s
hidden layer provides the input to the second DA  which in turn provides the input to the output
layer of the ﬁrst DA. This entire network—the SSDA—is trained again by back-propagation in a
ﬁne tuning stage  minimizing the loss given by

LSSDA(D; Θ) =

1
N

(cid:107)yi − ˆy(xi)(cid:107)2

2 +

λ
2

(cid:107)W(l)(cid:107)2

F

(5)

N(cid:88)

i=1

2L(cid:88)

l=1

where L is the number of stacked DAs (we used L = 2 in our experiments)  and W(l) denotes
weights for the l-th layer in the stacked deep network.2 The sparsity-inducing term is not needed
for this step because the sparsity was already incorporated in the pre-trained DAs. Our experiments
show that there is not a signiﬁcant change in performance when sparsity is included.
3.2 Adaptive Multi-Column SSDA
The adaptive multi-column SSDA is the linear combination of several SSDAs  or columns  each
trained on a single type of noise using optimized weights determined by the features of each given
input image. Taking advantage of the SSDA’s capability of feature learning  we use the features gen-
erated by the activation of the SSDA’s hidden layers as inputs to a neural network-based regression
component  referred to here as the weight prediction module. As shown in Figure 1b  this module
then uses these features to compute the optimal weights used to linearly combine the column outputs
into a weighted average.

1In particular  the sigmoid function is often used for decoding the input data when their values are bounded
between 0 and 1. For general cases  other types of functions (such as tanh  rectiﬁed linear  or linear functions)
can be used.

2After pre-training  we initialized W(1) and W(4) from the encoding and decoding weights of the ﬁrst-layer

DA  and W(2) and W(3) from the encoding and decoding weights of the second-layer DA  respectively.

3

(a) SSDA

(b) AMC-SSDA

Figure 1: Illustration of the AMC-SSDA. We concatenate the activations of the ﬁrst-layer hidden
units of the SSDA in each column (i.e.  fc denotes the concatenated hidden unit vectors h(1)(x)
and h(2)(x) of the SSDA corresponding to c-th column) as input features to the weight prediction
module for determining the optimal weight for each column of the AMC-SSDA. See text for details.

3.2.1 Training the AMC-SSDA
The AMC-SSDA has three training phases: training the SSDAs  determining optimal weights for a
set of training images  and then training the weight prediction module. The SSDAs are trained as
discussed in Section 3.1  with each SSDA provided a noisy training set  corrupted by a single noise
type along with the original versions of those images as a target set. Each SSDA column c then
produces an output ˆyc ∈ RD for an input x ∈ RD  which is the noisy version of original image y.
(We omit index i to remove clutter.)
3.2.2 Finding optimal column weights via quadratic program
Once the SSDAs are trained  we construct a new training set that pairs features extracted from the
hidden layers of the SSDAs with optimal column weights. Speciﬁcally  for each image  a vector
φ = [f1; ...; fC] is built from the features extracted from the hidden layers of each SSDA  where C is
the number of columns. That is  for SSDA column c  the activations of hidden layers h(1) and h(2)
(as shown in Figure 1a) are concatenated into a vector fc  and then f1  f2  . . .   fC are concatenated to
form the whole feature vector φ.
Additionally  the output of each column for each image is collected into a matrix ˆY = [y1  ...  yC] ∈
RD×C  with each column being the output of one of the SSDA columns  ˆyc. To determine the ideal
linear weighting of the SSDA columns for that given image  we perform the following non-linear
minimization (quadratic program) as follows:3

minimize{sc}
subject to

(cid:107) ˆYs − y(cid:107)2
1
2
0 ≤ sc ≤ 1 ∀c

1 − δ ≤ C(cid:88)

sc ≤ 1 + δ

(6)
(7)

(8)

Here s ∈ RC is the vector of weights sc corresponding to each SSDA column c. Constraining the
weights between 0 and 1 was shown to allow for better weight predictions by reducing overﬁtting.
The constraint in Eq. (8) helps to avoid degenerate cases where weights for very bright or dark spots

c=1

3In addition to the L2 error shown in Equation (6)  we also tested minimizing the L1 distance as the error
function  which is a standard method in the related ﬁeld of image registration [3]. The version using the L1
error performed slightly better in our noisy digit classiﬁcation task  suggesting that the loss function might need
to be tuned to the task and images at hand.

4

...............yˆxW(4)h(1)W(3)W(2)W(1)h(2)h(3)...Noisy ImageSSDA2+Denoised ImageWeightPredictionModule...f1f2fC...f1f2fCWeightsFeaturesSSDACSSDA1s1s2sCs1s2sCNoise Type
Gaussian
Speckle
Salt & Pepper

Parameter

Parameter value

σ2
ρ
ρ

0.02  0.06  0.10  0.14  0.18  0.22  0.26
0.05  0.10  0.15  0.20  0.25  0.30  0.35
0.05  0.10  0.15  0.20  0.25  0.30  0.35

Table 1: SSDA training noises in the 21-column AMC-SSDA. ρ is the noise density.

would otherwise be very high or low. Although making the weights sum exactly to one is more
intuitive  we found that the performance slightly improved when given some ﬂexibility  as shown in
Eq. (8). For our experiments  δ = 0.05 is used.
3.2.3 Learning to predict optimal column weights via RBF networks
The ﬁnal training phase is to train the weight prediction module. A radial basis function (RBF)
network is trained to take the feature vector φ as input and produce a weight vector s  using the
optimal weight training set described in Section 3.2.2. An RBF network was chosen for our exper-
iments because of its known performance in function approximation [24]. However  other function
approximation techniques could be used in this step.
3.2.4 Denoising with the AMC-SSDA
Once training has been completed  the AMC-SSDA is ready for use. A noisy image x is supplied
as input to each of the columns  which together produce the output matrix ˆY  each column of
which is the output of a particular column of the AMC-SSDA. The feature vector φ is created
from the activation of the hidden layers of each SSDA (as described in Section 3.2.2) and fed into
the weight prediction module (as described in Section 3.2.3)  which then computes the predicted
column weights  s∗. The ﬁnal denoised image ˆy is produced by linearly combining the columns
using these weights: ˆy = ˆYs∗.4

4 Experiments
We performed a number of denoising tasks by corrupting and denoising images of computed to-
mography (CT) scans of the head from the Cancer Imaging Archive [17] (Section 4.1). Quan-
titative evaluation of denoising results was performed using peak signal-to-noise ratio (PSNR) 
a standard method used for evaluating denoising performance. PSNR is deﬁned as PSNR =
e is the mean-square
10 log10(p2
error between the noisy and original images. We also tested the AMC-SSDA as pre-processing step
in an image classiﬁcation task by corrupting MNIST database of handwritten digits [19] with various
types of noise and then denoising and classifying the digits with a classiﬁer trained on the original
images (Section 4.2).
Our code is available at: http://sites.google.com/site/nips2013amcssda/.

e )  where pmax is the maximum possible pixel value and σ2

max/σ2

Image denoising

4.1
To evaluate general denoising performance  images of CT scans of the head were corrupted with
seven variations of Gaussian  salt-and-pepper  and speckle noise  resulting in the 21 noise types
shown in Table 1. Twenty-one individual SSDAs were trained on randomly selected 8-by-8 pixel
patches from the corrupted images; each SSDA was trained on a single type of noise. These twenty-
one SSDAs were used as columns to create an AMC-SSDA.5 The testing noise is given in Table 2.
The noise was produced using Matlab’s imnoise function with the exception of uniform noise 
which was produced with our own implementation. For Poisson noise  the image is divided by λ
prior to applying the noise; the result is then multiplied by λ.
To train the weight predictor for the AMC-SSDA  a set of images disjoint from the training set of
the individual SSDAs were used. The training images for the AMC-SSDA were corrupted with the
same noise types used to train its columns. The AMC-SSDA was tested on another set of images

4We have tried alternatives to this approach. Some of these involved using a single uniﬁed network to
combine the columns  such as joint training. In our preliminary experiments  these approaches did not yield
signiﬁcant improvements.

5We also evaluated AMC-SSDAs with smaller number of columns. In general  we achieved better perfor-

mance with more columns. We discuss its statistical signiﬁcance later in this section.

5

Noise Type
Gaussian
Speckle
Salt & Pepper
Poisson
Uniform [-0.5  0.5]

1
σ2 = 0.01
ρ = 0.1
ρ = 0.1
log(λ) = 24.4
30%

2
σ2 = 0.07
ρ = 0.15
ρ = 0.15
log(λ) = 25.3
50%

3
σ2 = 0.1
ρ = 0.3
ρ = 0.3
log(λ) = 26.0
70%

4
σ2 = 0.25
ρ = 0.4
ρ = 0.4
log(λ) = 26.4
100%

Table 2: Parameters of noise types used for testing. The Poisson and uniform noise types are not
seen in the training set. The percentage for uniform noise denotes how many pixels are affected. ρ
is the noise density.

(a) Original

(b) Noisy

(c) Mixed-SSDA

(d) AMC-SSDA

Figure 2: Visualization of the denoising performance of the Mixed-SSDA and AMC-SSDA. Top:
Gaussian noise. Bottom: speckle noise.

disjoint from both the individual SSDA and AMC-SSDA training sets. The AMC-SSDA was trained
on 128-by-128 pixel patches. When testing  64-by-64 pixel patches are denoised with a stride of 48.
During testing  we found that smaller strides yielded a very small increase in PSNR; however  having
a small stride was not feasible due to memory constraints. Since our SSDAs denoise 8-by-8 patches 
features for  say  a 64-by-64 patch are the average of the features extracted for each 8-by-8 patch in
the 64-by-64 patch. We ﬁnd that this allows for more consistent and predictable weights. The AMC-
SSDA is ﬁrst tested on noise types that have been seen (i.e.  noise types that were in the training set)
but have different statistics. It is then tested on noise not seen in the training examples  referred to
as “unseen” noise.
To compare with the experiments of Xie et al. [31]  one SSDA was trained on only the Gaussian noise
types  one on only salt & pepper  one on only speckle  and one on all the noise types from Table 1.
We refer to these as gaussian SSDA  s&p SSDA  speckle SSDA  and mixed SSDA  respectively. These
SSDAs were then tested on the same types of noise that the AMC-SSDA was tested on. The results
for both seen and unseen noise can be found in Tables 3 and 4. On average  for all cases  the AMC-
SSDA produced superior PSNR values when compared to these SSDAs. Some example results are
shown in Figure 2. In addition  we test the case where all the weights are equal and sum to one. We
call this the MC-SSDA; note that there is no adaptive element to it. We found that AMC-SSDA also
outperformed MC-SSDA.

Statistical signiﬁcance We statistically evaluated the difference between our AMC-SSDA and the
mixed SSDA (the best performing SSDA baseline) for the results shown in Table 3  using the one-
tailed paired t-test. The AMC-SSDA was signiﬁcantly better than the mixed-SSDA  with a p-value
of 3.3×10−5 for the null hypothesis. We also found that even for a smaller number of columns (such
as 9 columns)  the AMC-SSDA still was superior to the mixed-SSDA with statistical signiﬁcance.
In this paper  we report results from the 21-column AMC-SSDA.
We also performed additional control experiments in which we gave the SSDA an unfair advantage.
Speciﬁcally  each test image corrupted with seen noise was denoised with an SSDA that had been
trained on the exact type of noise and statistics that the test image has been corrupted with; we call
this the “informed-SSDA.” We saw that the AMC-SSDA performed slightly better on the Gaussian

6

Speckle Mixed MC-SSDA AMC-SSDA

S&P

Noise Noisy Gaussian
Type
G 1
G 2
G 3
G 4
SP 1
SP 2
SP 3
SP 4
S 1
S 2
S 3
S 4
Avg

Image
22.10
13.92
12.52
9.30
13.50
11.76
8.75
7.50
19.93
18.22
15.35
14.24
13.92

SSDA SSDA SSDA SSDA
27.15
26.64
25.52
25.83
25.09
25.50
22.72
23.11
25.86
26.32
25.77
25.40
24.32
23.95
22.95
22.46
26.97
26.41
26.44
25.92
23.54
24.42
22.93
21.80
24.70
25.05

26.84
19.76
18.35
14.88
22.27
20.07
15.88
13.86
28.22
27.75
25.79
24.41
21.51

26.69
23.07
22.17
20.17
26.26
25.77
23.96
22.20
26.37
25.80
23.36
21.69
23.96

27.37
23.34
22.00
17.97
25.84
24.54
20.42
17.76
27.43
26.71
23.91
22.20
23.29

29.60
26.85
26.10
23.66
27.72
26.77
24.65
23.01
28.59
27.68
25.72
24.35
26.23

Speckle Mixed MC-SSDA AMC-SSDA

(a) PSNRs for previously seen noise  best values in bold.

(b) Average PNSRs for speciﬁc noise types
Figure 3: Average PSNR values for denoised images of various previously seen noise types (G:
Gaussian  S: Speckle  SP: Salt & Pepper).
Noise Noisy Gaussian
SSDA
Type
26.27
P 1
P 2
25.77
24.61
P 3
23.36
P 4
23.40
U 1
26.21
U 2
23.24
U 3
U 4
16.54
23.67
Avg

S&P
SSDA SSDA SSDA
26.80
26.48
25.92
26.01
24.43
24.54
23.01
23.07
23.74
23.68
26.28
25.86
22.89
21.36
15.45
16.04
23.65
23.29

Image
19.90
16.90
13.89
12.11
17.20
16.04
12.98
8.78
14.72

28.83
27.64
25.50
23.43
24.50
28.06
23.70
16.78
24.80

27.35
26.78
25.11
23.28
24.71
26.13
21.07
14.11
23.57

27.99
26.94
24.65
22.64
25.05
23.21
17.83
12.01
22.54

(a) PSNR for unseen noise  best values in bold.

(b) Average results for noise types.

Figure 4: Average PSNR values for denoised images of various previously unseen noise types (P:
Poisson noise; U: Uniform noise).

and salt & pepper noise and slightly worse on speckle noise. Overall  the informed-SSDA had 
on average  a PSNR that was only 0.076dB better than the AMC-SSDA. The p-value obtained was
0.4708  indicating little difference between the two methods. This suggests that the AMC-SSDA can
perform as well as using an ”ideally” trained network for speciﬁc noise type (i.e.  training and testing
an SSDA for the same speciﬁc noise type). This is achieved through its adaptive functionality.

4.2 Digit recognition from denoised images
Since the results of denoising images from a visual standpoint can be more qualitative than quan-
titative  we have tested using denoising as a preprocessing step done before a classiﬁcation task.
Speciﬁcally  we used the MNIST database of handwritten digits [19] as benchmark to evaluate the
efﬁcacy of our denoising procedures.
First  we trained a deep neural network digit classiﬁer from the MNIST training digits  following
[15]. The digit classiﬁer achieved a baseline error rate of 1.09% when tested on the uncorrupted
MNIST test set.
The MNIST digits are corrupted with Gaussian  salt & pepper  speckle  block  and border noise.
Examples of this are shown in Figure 5. The block and border noises are similar to that of Tang

Figure 5: Example MNIST digits. Noisy images are shown on top and the corresponding denoised
images by the AMC-SSDA are shown below. Noise types from left: Gaussian  speckle  salt &
pepper  block  border.

7

Gaussian AvgSalt & Pepper AvgSpeckle Avg051015202530Average PSNRPSNR for Seen Noise NoisyGaussianS&PSpeckleMixedMC−SSDAAMC−SSDAPoisson AvgUniform Avg051015202530Average PSNRPSNR for Unseen Noise NoisyGaussianS&PSpeckleMixedMC−SSDAAMC−SSDAet al. [28]. An SSDA was trained on each type of noise. An AMC-SSDA was also trained using
these types of noise. The goal of this experiment is to show that the potential cumbersome and
time-consuming process of determining the type of noise that an image is corrupted with at test time
is not needed to achieve good classiﬁcation results.
As the results show in Table 3  the denoising performance was strongly correlated to the type of noise
upon which the denoiser was trained. The bold-faced values show the best performing denoiser for a
given noise type. Since a classiﬁcation difference of 0.1% or larger is considered statistically signif-
icant [5]  we bold all values within 0.1% of the best error rate. The AMC-SSDA either outperforms 
or comes close to (within 0.06%)  the SSDA that was trained with the same type of noise as in the
test data. In terms of average error across all types of noises  the AMC-SSDA is signiﬁcantly better
than any single denoising algorithms we compared. The results suggest that the AMC-SSDA con-
sistently achieves strong classiﬁcation performance without having to determine the type of noise
during test time.
These results are also comparable to the results of Tang et al. [28]. We show that we get better
classiﬁcation accuracy for the block and border noise types. In addition  we note that Tang et al.
uses a 7-by-7 local receptive ﬁeld  while ours uses 28-by-28 patches. As suggested by Tang et al. 
we expect that using a local ﬁeld in our architecture could further improve our results.

Method / Noise Type
No denoising
Gaussian SSDA
Salt & Pepper SSDA
Speckle SSDA
Block SSDA
Border SSDA
AMC-SSDA
Tang et al. [28]*

Clean
1.09%
2.13%
1.94%
1.58%
1.67%
8.42%
1.50%
1.24%

Gaussian
29.17%
1.52%
1.71%
5.86%
5.92%
19.87%
1.47%

-

Border

Speckle
8.11%
5.10%
4.78%
2.03%
7.64%

Block
Average
S & P
25.72% 90.05% 28.80%
18.63%
6.65%
20.03% 8.69%
2.44%
19.71% 2.16%
5.45%
2.38%
7.26%
19.95% 7.36%
6.80%
5.15%
6.25%
15.29%
1.81%
19.45% 13.89% 31.38% 1.12%
15.69%
5.18%
2.22%
1.15%
2.27%
19.09% 1.29%

-

2.09%

-

-

Table 3: MNIST test classiﬁcation error of denoised images. Rows denote the performance of
different denoising methods  including: “no denoising ” SSDA trained on a speciﬁc noise type  and
AMC-SSDA. Columns represent images corrupted with the given noise type. Percentage values are
classiﬁcation error rates for a set of test images corrupted with the given noise type and denoised
prior to classiﬁcation. Bold-faced values represent the best performance for images corrupted by a
given noise type. *Note: we compare the numbers reported from Tang et al. [28] (“7x7+denoised”).

5 Conclusion

In this paper  we proposed the adaptive multi-column SSDA  a novel technique of combining mul-
tiple SSDAs by predicting optimal column weights adaptively. We have demonstrated that AMC-
SSDA can robustly denoise images corrupted by multiple different types of noise without knowledge
of the noise type at testing time. It has also been shown to perform well on types of noise that were
not in the training set. Overall  the AMC-SSDA has signiﬁcantly outperformed the SSDA in denois-
ing. The good classiﬁcation results of denoised MNIST digits also support the hypothesis that the
AMC-SSDA eliminates the need to know about the type of noise during test time.

Acknowledgments

This work was funded in part by Google Faculty Research Award  ONR N00014-13-1-0762  and
NSF IIS 1247414. F. Agostinelli was supported by GEM Fellowship  and M. Anderson was sup-
ported in part by NSF IGERT Open Data Fellowship (#0903629). We also thank Roni Mittelman 
Yong Peng  Scott Reed  and Yuting Zhang for their helpful comments.

References

[1] G. R. Arce. Nonlinear signal processing: A statistical approach. Wiley-Interscience  2005.
[2] E. Arias-Castro and D. L. Donoho. Does median ﬁltering truly preserve edges better than linear ﬁltering?

The Annals of Statistics  37(3):1172–1206  2009.

8

[3] D. I. Barnea and H. F. Silverman. A class of algorithms for fast digital image registration. IEEE Transac-

tions on Computers  100(2):179–186  1972.

[4] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning  2(1):1–127 

2009.

[5] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep networks. In

NIPS  2007.

[6] R. Bourne. Image ﬁlters. In Fundamentals of Digital Imaging in Medicine  pages 137–172. Springer

London  2010.

[7] R. G. Brown and P. Y. Hwang. Introduction to random signals and applied Kalman ﬁltering  volume 1.

John Wiley & Sons New York  1992.

[8] A. Buades  B. Coll  and J.-M. Morel. A review of image denoising algorithms  with a new one. Multiscale

Modeling & Simulation  4(2):490–530  2005.

[9] H. C. Burger  C. J. Schuler  and S. Harmeling. Image denoising: Can plain neural networks compete with

BM3D? In CVPR  2012.

[10] D. Cires¸an  U. Meier  and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.

In CVPR  2012.

[11] K. Dabov  R. Foi  V. Katkovnik  and K. Egiazarian. Image denoising by sparse 3D transform-domain

collaborative ﬁltering. IEEE Transactions on Image Processing  16(8):2080–2095  2007.

[12] L. W. Goldman. Principles of CT: Radiation dose and image quality. Journal of Nuclear Medicine

Technology  35(4):213–225  2007.

[13] G. Hinton. A practical guide to training restricted boltzmann machines. Technical report  University of

Toronto  2010.

[14] G. E. Hinton  S. Osindero  and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation  18(7):1527–1554  2006.

[15] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504–507  2006.

[16] W. Huda. Dose and image quality in CT. Pediatric Radiology  32(10):709–713  2002.
[17] N. C. Institute. The Cancer Imaging Archive. http://www.cancerimagingarchive.net  2013.
[18] V. Jain and H. S. Seung. Natural image denoising with convolutional networks. In NIPS  2008.
[19] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[20] H. Lee  C. Ekanadham  and A. Y. Ng. Sparse deep belief net model for visual area V2. In NIPS. 2008.
[21] F. Luisier  T. Blu  and M. Unser. A new SURE approach to image denoising: Interscale orthonormal

wavelet thresholding. IEEE Transactions on Image Processing  16(3):593–606  2007.

[22] J. Mairal  F. Bach  J. Ponce  G. Sapiro  and A. Zisserman. Non-local sparse models for image restoration.

In ICCV  2009.

[23] M. C. Motwani  M. C. Gadiya  R. C. Motwani  and F. C. Harris. Survey of image denoising techniques.

In GSPX  2004.

[24] J. Park and I. W. Sandberg. Universal approximation using radial-basis-function networks. Neural Com-

putation  3(2):246–257  1991.

[25] J. Portilla  V. Strela  M. J. Wainwright  and E. P. Simoncelli. Image denoising using scale mixtures of

Gaussians in the wavelet domain. IEEE Transactions on Image Processing  12(11):1338–1351  2003.

[26] M. G. Rathor  M. A. Kaushik  and M. V. Gupta. Medical images denoising techniques review. Interna-

tional Journal of Electronics Communication and Microelectronics Designing  1(1):33–36  2012.

[27] R. Siemund  A. L¨ove  D. van Westen  L. Stenberg  C. Petersen  and I. Bj¨orkman-Burtscher. Radiation
dose reduction in CT of the brain: Can advanced noise ﬁltering compensate for loss of image quality?
Acta Radiologica  53(4):468–472  2012.

[28] Y. Tang and C. Eliasmith. Deep networks for robust visual recognition. In ICML  2010.
[29] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine
Learning Research  11:3371–3408  2010.

[30] N. Wiener. Extrapolation  interpolation  and smoothing of stationary time series: with engineering ap-

plications. Technology Press of the Massachusetts Institute of Technology  1950.

[31] J. Xie  L. Xu  and E. Chen. Image denoising and inpainting with deep neural networks. In NIPS  2012.

9

,Forest Agostinelli
Michael Anderson
Honglak Lee
Weizhu Chen
Zhenghao Wang
Jingren Zhou