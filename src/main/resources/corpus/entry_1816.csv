2016,End-to-End Kernel Learning with Supervised Convolutional Kernel Networks,In this paper  we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task  we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then  we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network  where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ``deep learning'' datasets such as CIFAR-10 and SVHN  and also for image super-resolution  demonstrating the applicability of our approach to a large variety of image-related tasks.,End-to-End Kernel Learning with

Supervised Convolutional Kernel Networks

Julien Mairal

Inria∗

julien.mairal@inria.fr

Abstract

In this paper  we introduce a new image representation based on a multilayer kernel
machine. Unlike traditional kernel methods where data representation is decoupled
from the prediction task  we learn how to shape the kernel with supervision. We
proceed by ﬁrst proposing improvements of the recently-introduced convolutional
kernel networks (CKNs) in the context of unsupervised learning; then  we derive
backpropagation rules to take advantage of labeled training data. The resulting
model is a new type of convolutional neural network  where optimizing the ﬁlters
at each layer is equivalent to learning a linear subspace in a reproducing kernel
Hilbert space (RKHS). We show that our method achieves reasonably competitive
performance for image classiﬁcation on some standard “deep learning” datasets
such as CIFAR-10 and SVHN  and also for image super-resolution  demonstrating
the applicability of our approach to a large variety of image-related tasks.

1

Introduction

In the past years  deep neural networks such as convolutional or recurrent ones have become highly
popular for solving various prediction problems  notably in computer vision and natural language
processing. Conceptually close to approaches that were developed several decades ago (see  [13]) 
they greatly beneﬁt from the large amounts of labeled data that have been made available recently 
allowing to learn huge numbers of model parameters without worrying too much about overﬁtting.
Among other reasons explaining their success  the engineering effort of the deep learning community
and various methodological improvements have made it possible to learn in a day on a GPU complex
models that would have required weeks of computations on a traditional CPU (see  e.g.  [10  12  23]).
Before the resurgence of neural networks  non-parametric models based on positive deﬁnite kernels
were one of the most dominant topics in machine learning [22]. These approaches are still widely
used today because of several attractive features. Kernel methods are indeed versatile; as long as a
positive deﬁnite kernel is speciﬁed for the type of data considered—e.g.  vectors  sequences  graphs 
or sets—a large class of machine learning algorithms originally deﬁned for linear models may be
used. This family include supervised formulations such as support vector machines and unsupervised
ones such as principal or canonical component analysis  or K-means and spectral clustering. The
problem of data representation is thus decoupled from that of learning theory and algorithms. Kernel
methods also admit natural mechanisms to control the learning capacity and reduce overﬁtting [22].
On the other hand  traditional kernel methods suffer from several drawbacks. The ﬁrst one is their
computational complexity  which grows quadratically with the sample size due to the computation of
the Gram matrix. Fortunately  signiﬁcant progress has been achieved to solve the scalability issue 
either by exploiting low-rank approximations of the kernel matrix [28  31]  or with random sampling
techniques for shift-invariant kernels [21]. The second disadvantage is more critical; by decoupling

∗Thoth team  Inria Grenoble  Laboratoire Jean Kuntzmann  CNRS  Univ. Grenoble Alpes  France.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

learning and data representation  kernel methods seem by nature incompatible with end-to-end
learning—that is  the representation of data adapted to the task at hand  which is the cornerstone of
deep neural networks and one of the main reason of their success. The main objective of this paper is
precisely to tackle this issue in the context of image modeling.
Speciﬁcally  our approach is based on convolutional kernel networks  which have been recently
introduced in [18]. Similar to hierarchical kernel descriptors [3]  local image neighborhoods are
mapped to points in a reproducing kernel Hilbert space via the kernel trick. Then  hierarchical
representations are built via kernel compositions  producing a sequence of “feature maps” akin to
convolutional neural networks  but of inﬁnite dimension. To make the image model computationally
tractable  convolutional kernel networks provide an approximation scheme that can be interpreted as
a particular type of convolutional neural network learned without supervision.
To perform end-to-end learning given labeled data  we use a simple but effective principle consisting
of learning discriminative subspaces in RKHSs  where we project data. We implement this idea
in the context of convolutional kernel networks  where linear subspaces  one per layer  are jointly
optimized by minimizing a supervised loss function. The formulation turns out to be a new type of
convolutional neural network with a non-standard parametrization. The network also admits simple
principles to learn without supervision: learning the subspaces may be indeed achieved efﬁciently
with classical kernel approximation techniques [28  31].
To demonstrate the effectiveness of our approach in various contexts  we consider image classiﬁcation
benchmarks such as CIFAR-10 [12] and SVHN [19]  which are often used to evaluate deep neural
networks; then  we adapt our model to perform image super-resolution  which is a challenging inverse
problem. On the SVHN and CIFAR-10 datasets  we obtain a competitive accuracy  with about 2% and
10% error rates  respectively  without model averaging or data augmentation. For image up-scaling 
we outperform recent approaches based on classical convolutional neural networks [7  8].
We believe that these results are highly promising. Our image model achieves competitive perfor-
mance in two different contexts  paving the way to many other applications. Moreover  our results are
also subject to improvements. In particular  we did not use GPUs yet  which has limited our ability
to exhaustively explore model hyper-parameters and evaluate the accuracy of large networks. We
also did not investigate classical regularization/optimization techniques such as Dropout [12]  batch
normalization [11]  or recent advances allowing to train very deep networks [10  23]. To gain more
scalability and start exploring these directions  we are currently working on a GPU implementation 
which we plan to publicly release along with our current CPU implementation.

Related Deep and Shallow Kernel Machines. One of our goals is to make a bridge between kernel
methods and deep networks  and ideally reach the best of both worlds. Given the potentially attractive
features of such a combination  several attempts have been made in the past to unify these two schools
of thought. A ﬁrst proof of concept was introduced in [5] with the arc-cosine kernel  which admits an
integral representation that can be interpreted as a one-layer neural network with random weights
and inﬁnite number of rectiﬁed linear units. Besides  a multilayer kernel may be obtained by kernel
compositions [5]. Then  hierarchical kernel descriptors [3] and convolutional kernel networks [18]
extend a similar idea in the context of images leading to unsupervised representations [18].
Multiple kernel learning [24] is also related to our work since is it is a notable attempt to introduce
supervision in the kernel design. It provides techniques to select a combination of kernels from a pre-
deﬁned collection  and typically requires to have already “good” kernels in the collection to perform
well. More related to our work  the backpropagation algorithm for the Fisher kernel introduced in [25]
learns the parameters of a Gaussian mixture model with supervision. In comparison  our approach
does not require a probabilistic model and learns parameters at several layers. Finally  we note that a
concurrent effort to ours is conducted in the Bayesian community with deep Gaussian processes [6] 
complementing the Frequentist approach that we follow in our paper.

2 Learning Hierarchies of Subspaces with Convolutional Kernel Networks

In this section  we present the principles of convolutional kernel networks and a few generalizations
and improvements of the original approach of [18]. Essentially  the model builds upon four ideas that
are detailed below and that are illustrated in Figure 1 for a model with a single layer.

2

Idea 1: use the kernel trick to represent local image neighborhoods in a RKHS.
Given a set X   a positive deﬁnite kernel K : X × X → R implicitly deﬁnes a Hilbert space H  called
reproducing kernel Hilbert space (RKHS)  along with a mapping ϕ : X → H. This embedding is
such that the kernel value K(x  x(cid:48)) corresponds to the inner product (cid:104)ϕ(x)  ϕ(x(cid:48))(cid:105)H. Called “kernel
trick”  this approach can be used to obtain nonlinear representations of local image patches [3  18].
More precisely  consider an image I0 : Ω0 → Rp0  where p0 is the number of channels  e.g.  p0 = 3
for RGB  and Ω0 ⊂ [0  1]2 is a set of pixel coordinates  typically a two-dimensional grid. Given two
image patches x  x(cid:48) of size e0 × e0  represented as vectors in Rp0e2

0  we deﬁne a kernel K1 as

(cid:16)(cid:68) x

  x(cid:48)
(cid:107)x(cid:48)(cid:107)

(cid:107)x(cid:107)

(cid:69)(cid:17)

(cid:105)−1) = e− α1

2 (cid:107)y−y(cid:48)

(cid:107)2
2.

K1(x  x(cid:48)) = (cid:107)x(cid:107)(cid:107)x(cid:48)(cid:107) κ1

κ1((cid:104)y  y(cid:48)(cid:105)) = eα1((cid:104)y y(cid:48)

if x  x(cid:48) (cid:54)= 0 and 0 otherwise 

(1)
where (cid:107).(cid:107) and (cid:104).  .(cid:105) denote the usual Euclidean norm and inner-product  respectively  and κ1((cid:104).  .(cid:105)) is
a dot-product kernel on the sphere. Speciﬁcally  κ1 should be smooth and its Taylor expansion have
non-negative coefﬁcients to ensure positive deﬁniteness [22]. For example  the arc-cosine [5] or the
Gaussian (RBF) kernels may be used: given two vectors y  y(cid:48) with unit (cid:96)2-norm  choose for instance
(2)
0 → H1.

Then  we have implicitly deﬁned the RKHS H1 associated to K1 and a mapping ϕ1 : Rp0e2
Idea 2: project onto a ﬁnite-dimensional subspace of the RKHS with convolution layers.
The representation of patches in a RKHS requires ﬁnite-dimensional approximations to be computa-
tionally manageable. The original model of [18] does that by exploiting an integral form of the RBF
kernel. Speciﬁcally  given two patches x and x(cid:48)  convolutional kernel networks provide two vectors
ψ1(x)  ψ1(x(cid:48)) in Rp1 such that the kernel value (cid:104)ϕ1(x)  ϕ1(x(cid:48))(cid:105)H1 is close to the Euclidean inner
product (cid:104)ψ1(x)  ψ1(x(cid:48))(cid:105). After applying this transformation to all overlapping patches of the input
image I0  a spatial map M1 : Ω0 → Rp1 may be obtained such that for all z in Ω0  M1(z) = ψ1(xz) 
where xz is the e0 × e0 patch from I0 centered at pixel location z.2 With the approximation scheme
of [18]  M1 can be interpreted as the output feature map of a one-layer convolutional neural network.
A conceptual drawback of [18] is that data points ϕ1(x1)  ϕ1(x2)  . . . are approximated by vectors
that do not live in the RKHS H1. This issue can be solved by using variants of the Nyström
method [28]  which consists of projecting data onto a subspace of H1 with ﬁnite dimension p1.
For this task  we have adapted the approach of [31]: we build a database of n patches x1  . . .   xn
randomly extracted from various images and normalized to have unit (cid:96)2-norm  and perform a spherical
K-means algorithm to obtain p1 centroids z1  . . .   zp1 with unit (cid:96)2-norm. Then  a new patch x is
approximated by its projection onto the p1-dimensional subspace F1 =Span(ϕ(z1)  . . .   ϕ(zp1)).
The projection of ϕ1(x) onto F1 admits a natural parametrization ψ1(x) in Rp1 . The explicit formula
is classical (see [28  31] and Appendix A)  leading to
Z(cid:62) x
(cid:107)x(cid:107)

ψ1(x) := (cid:107)x(cid:107)κ1(Z(cid:62)Z)−1/2κ1

if x (cid:54)= 0 and 0 otherwise 

where we have introduced the matrix Z = [z1  . . .   zp1 ]  and  by an abuse of notation  the function κ1
is applied pointwise to its arguments. Then  the spatial map M1 : Ω0 → Rp1 introduced above can
be obtained by (i) computing the quantities Z(cid:62)x for all patches x of the image I (spatial convolution
after mirroring the ﬁlters zj); (ii) contrast-normalization involving the norm (cid:107)x(cid:107); (iii) applying the
pointwise non-linear function κ1; (iv) applying the linear transform κ1(Z(cid:62)Z)−1/2 at every pixel
location (which may be seen as 1×1 spatial convolution); (v) multiplying by the norm (cid:107)x(cid:107) making ψ1
homogeneous. In other words  we obtain a particular convolutional neural network  with non-standard
parametrization. Note that learning requires only performing a K-means algorithm and computing
the inverse square-root matrix κ1(Z(cid:62)Z)−1/2; therefore  the training procedure is very fast.
Then  it is worth noting that the encoding function ψ1 with kernel (2) is reminiscent of radial basis
function networks (RBFNs) [4]  whose hidden layer resembles (3) without the matrix κ1(Z(cid:62)Z)−1/2
and with no normalization. The difference between RBFNs and our model is nevertheless signiﬁcant.
The RKHS mapping  which is absent from RBFNs  is indeed a key to the multilayer construction
that will be presented shortly: a network layer takes points from the RKHS’s previous layer as input
and use the corresponding RKHS inner-product. To the best of our knowledge  there is no similar
multilayer and/or convolutional construction in the radial basis function network literature.

(cid:18)

(cid:19)

(3)

2To simplify  we use zero-padding when patches are close to the image boundaries  but this is optional.

3

Figure 1: Our variant of convolutional kernel networks  illustrated between layers 0 and 1. Local
patches (receptive ﬁelds) are mapped to the RKHS H1 via the kernel trick and then projected to
the ﬁnite-dimensional subspace F1 =Span(ϕ(z1)  . . .   ϕ(zp1)). The small blue crosses on the right
represent the points ϕ(z1)  . . .   ϕ(zp1 ). With no supervision  optimizing F1 consists of minimizing
projection residuals. With supervision  the subspace is optimized via back-propagation. Going from
layer k to layer k + 1 is achieved by stacking the model described here and shifting indices.

Idea 3: linear pooling in F1 is equivalent to linear pooling on the ﬁnite-dimensional map M1.
The previous steps transform an image I0 : Ω0 → Rp0 into a map M1 : Ω0 → Rp1  where each
vector M1(z) in Rp1 encodes a point in F1 representing information of a local image neighborhood
centered at location z. Then  convolutional kernel networks involve a pooling step to gain invariance
to small shifts  leading to another ﬁnite-dimensional map I1 : Ω1 → Rp1 with smaller resolution:
(4)

M1(z(cid:48))e−β1(cid:107)z(cid:48)

I1(z) =

−z(cid:107)2
2 .

(cid:88)

z(cid:48)∈Ω0

The Gaussian weights act as an anti-aliasing ﬁlter for downsampling the map M1 and β1 is set
according to the desired subsampling factor (see [18])  which does not need to be integer. Then  every
point I1(z) in Rp1 may be interpreted as a linear combination of points in F1  which is itself in F1
since F1 is a linear subspace. Note that the linear pooling step was originally motivated in [18] as an
approximation scheme for a match kernel  but this point of view is not critically important here.

Idea 4: build a multilayer image representation by stacking and composing kernels.
By following the ﬁrst three principles described above  the input image I0 : Ω0 → Rp0 is transformed
into another one I1 : Ω1 → Rp1. It is then straightforward to apply again the same procedure to
obtain another map I2 : Ω2 → Rp2  then I3 : Ω3 → Rp3  etc. By going up in the hierarchy  the
vectors Ik(z) in Rpk represent larger and larger image neighborhoods (aka. receptive ﬁelds) with
more invariance gained by the pooling layers  akin to classical convolutional neural networks.
The multilayer scheme produces a sequence of maps (Ik)k≥0  where each vector Ik(z) encodes
a point—say fk(z)—in the linear subspace Fk of Hk. Thus  we implicitly represent an image at
layer k as a spatial map fk : Ωk → Hk such that (cid:104)Ik(z)  I(cid:48)k(z(cid:48))(cid:105) = (cid:104)fk(z)  f(cid:48)k(z(cid:48))(cid:105)Hk for all z  z(cid:48).
As mentioned previously  the mapping to the RKHS is a key to the multilayer construction. Given Ik 
larger image neighborhoods are represented by patches of size ek × ek that can be mapped to a
point in the Cartesian product space Hek×ek
endowed with its natural inner-product; ﬁnally  the
kernel Kk+1 deﬁned on these patches can be seen as a kernel on larger image neighborhoods than Kk.

k

3 End-to-End Kernel Learning with Supervised CKNs

In the previous section  we have described a variant of convolutional kernel networks where linear
subspaces are learned at every layer. This is achieved without supervision by a K-means algorithm
leading to small projection residuals. It is thus natural to introduce also a discriminative approach.

4

I0xx0kerneltrickprojectiononF1M1ψ1(x)ψ1(x0)I1linearpoolingHilbertspaceH1F1ϕ1(x)ϕ1(x0)3.1 Backpropagation Rules for Convolutional Kernel Networks

λ
2(cid:107)f(cid:107)2
H 
(cid:88)

z∈Ωk

1
n

min
f∈H

i=1

(cid:88)

z∈Ωk

We now consider a prediction task  where we are given a training set of images I 1
0   . . .   I n
0
with respective scalar labels y1  . . .   yn living either in {−1; +1} for binary classiﬁcation and R
for regression. For simplicity  we only present these two settings here  but extensions to multiclass
classiﬁcation and multivariate regression are straightforward. We also assume that we are given a
smooth convex loss function L : R × R → R that measures the ﬁt of a prediction to the true label y.
Given a positive deﬁnite kernel K on images  the classical empirical risk minimization formulation
consists of ﬁnding a prediction function in the RKHS H associated to K by minimizing the objective
(5)

n(cid:88)

0   I 2

L(yi  f (I i

0)) +

n(cid:88)

i=1

where the parameter λ controls the smoothness of the prediction function f with respect to the
geometry induced by the kernel  hence regularizing and reducing overﬁtting [22]. After training a
convolutional kernel network with k layers  such a positive deﬁnite kernel may be deﬁned as

KZ (I0  I(cid:48)0) =

(cid:104)fk(z)  f(cid:48)k(z)(cid:105)Hk =

(cid:104)Ik(z)  I(cid:48)k(z)(cid:105) 

(6)

where Ik  I(cid:48)k are the k-th ﬁnite-dimensional feature maps of I0  I(cid:48)0  respectively  and fk  f(cid:48)k the
corresponding maps in Ωk → Hk  which have been deﬁned in the previous section. The kernel is
also indexed by Z  which represents the network parameters—that is  the subspaces F1  . . .  Fk  or
equivalently the set of ﬁlters Z1  . . .   Zk from Eq. (3). Then  formulation (5) becomes equivalent to

min

W∈Rpk×|Ωk|

1
n

L(yi (cid:104)W  I i

k(cid:105)) +

λ
2(cid:107)W(cid:107)2
F 

(7)

where (cid:107).(cid:107)F is the Frobenius norm that extends the Euclidean norm to matrices  and  with an abuse of
k are seen as matrices in Rpk×|Ωk|. Then  the supervised convolutional kernel
notation  the maps I i
network formulation consists of jointly minimizing (7) with respect to W in Rpk×|Ωk| and with respect
to the set of ﬁlters Z1  . . .   Zk  whose columns are constrained to be on the Euclidean sphere.

Computing the derivative with respect to the ﬁlters Z1  . . .   Zk.
Since we consider a smooth loss function L  e.g.  logistic  squared hinge  or square loss  optimizing (7)
with respect to W can be achieved with any gradient-based method. Moreover  when L is convex 
we may also use fast dedicated solvers  (see  e.g.  [16]  and references therein). Optimizing with
respect to the ﬁlters Zj  j = 1  . . .   k is more involved because of the lack of convexity. Yet  the
objective function is differentiable  and there is hope to ﬁnd a “good” stationary point by using
classical stochastic optimization techniques that have been successful for training deep networks.
For that  we need to compute the gradient by using the chain rule—also called “backpropagation” [13].
We instantiate this rule in the next lemma  which we have found useful to simplify the calculation.
Lemma 1 (Perturbation view of backpropagration.)
Consider an image I0 represented here as a matrix in Rp0×|Ω0|  associated to a label y in R and
call IZk the k-th feature map obtained by encoding I0 with the network parameters Z. Then  consider
a perturbation E = {ε1  . . .   εk} of the set of ﬁlters Z. Assume that we have for all j ≥ 0 

where (cid:107)E(cid:107) is equal to(cid:80)k

of the same size 

IZ+E
j

= IZj + ∆IZ Ej + o((cid:107)E(cid:107)) 

(8)
is a matrix in Rpj×|Ωj| such that for all matrices U

l=1 (cid:107)εl(cid:107)F  and ∆IZ Ej
(cid:104)∆IZ Ej

  U(cid:105) = (cid:104)εj  gj(U)(cid:105) + (cid:104)∆IZ Ej−1  hj(U)(cid:105) 

where the inner-product is the Frobenius’s one and gj  hj are linear functions. Then 

∇Zj L(y (cid:104)W  IZk (cid:105)) = L(cid:48)(y (cid:104)W  IZk (cid:105)) gj(hj+1(. . . hk(W)) 

where L(cid:48) denote the derivative of the smooth function L with respect to its second argument.

The proof of this lemma is straightforward and follows from the deﬁnition of the Fréchet derivative.
Nevertheless  it is useful to derive the closed form of the gradient in the next proposition.

5

(9)

(10)

Proposition 1 (Gradient of the loss with respect to the the ﬁlters Z1  . . .   Zk.)
Consider the quantities introduced in Lemma 1  but denote IZj by Ij for simplicity. By construction 
we have for all j ≥ 1 
(11)
where Ij is seen as a matrix in Rpj×|Ωj|; Ej is the linear operator that extracts all overlapping
ej−1 × ej−1 patches from a map such that Ej(Ij−1) is a matrix of size pj−1e2
j−1 × |Ωj−1|; Sj is a
diagonal matrix whose diagonal entries carry the (cid:96)2-norm of the columns of Ej(Ij−1); Aj is short
for κj(Z(cid:62)j Zj)−1/2; and Pj is a matrix of size |Ωj−1|×|Ωj| performing the linear pooling operation.
Then  the gradient of the loss with respect to the ﬁlters Zj  j = 1  . . .   k is given by (10) with

Ij = Ajκj(Z(cid:62)j Ej(Ij−1)S−1

j )SjPj 

gj(U) = Ej(Ij−1)B(cid:62)j −
hj(U) = E(cid:63)
j

(cid:0)κ(cid:48)j(Z(cid:62)j Zj) (cid:12) (Cj + C(cid:62)j )(cid:1)
(cid:0)ZjBj + Ej(Ij−1)(cid:0)S−2
(cid:0)M(cid:62)j UP(cid:62)j − Ej(Ij−1)(cid:62)ZjBj
(cid:0)Z(cid:62)j Ej(Ij−1)S−1
(cid:1) and Cj = A1/2
(cid:0)AjUP(cid:62)j

j (cid:12)

where U is any matrix of the same size as Ij  Mj = Ajκj(Z(cid:62)j Ej(Ij−1)S−1
map before the pooling step  (cid:12) is the Hadamart (elementwise) product  E(cid:63)

j )Sj is the j-th feature
j is the adjoint of Ej  and
(13)

IjU(cid:62)A3/2

.

(cid:1)(cid:1)(cid:1) 

Bj = κ(cid:48)j

1
2

Zj

j

j

(cid:1)

j

(cid:12)

(12)

j

The proof is presented in Appendix B. Most quantities that appear above admit physical interpretations:
multiplication by Pj performs downsampling; multiplication by P(cid:62)j performs upsampling; multipli-
cation of Ej(Ij−1) on the right by S−1
performs (cid:96)2-normalization of the columns; Z(cid:62)j Ej(Ij−1) can
be seen as a spatial convolution of the map Ij−1 by the ﬁlters Zj; ﬁnally  E(cid:63)
j “combines” a set of
patches into a spatial map by adding to each pixel location the respective patch contributions.
Computing the gradient requires a forward pass to obtain the maps Ij through (11) and a backward
pass that composes the functions gj  hj as in (10). The complexity of the forward step is dominated
by the convolutions Z(cid:62)j Ej(Ij−1)  as in convolutional neural networks. The cost of the backward
pass is the same as the forward one up to a constant factor. Assuming pj ≤|Ωj−1|  which is typical
for lower layers that require more computation than upper ones  the most expensive cost is due to
Ej(Ij−1)B(cid:62)j and ZjBj which is the same as Z(cid:62)j Ej(Ij−1). We also pre-compute A1/2
and A3/2
by eigenvalue decompositions  whose cost is reasonable when performed only once per minibatch.
Off-diagonal elements of M(cid:62)j UP(cid:62)j − Ej(Ij−1)(cid:62)ZjBj are also not computed since they are set to
zero after elementwise multiplication with a diagonal matrix. In practice  we also replace Aj by
(κj(Z(cid:62)j Zj) + εI)−1/2 with ε = 0.001  which corresponds to performing a regularized projection
onto Fj (see Appendix A). Finally  a small offset of 0.00001 is added to the diagonal entries of Sj.
Optimizing hyper-parameters for RBF kernels. When using the kernel (2)  the objective is
differentiable with respect to the hyper-parameters αj. When large amounts of training data are
available and overﬁtting is not a issue  optimizing the training loss by taking gradient steps with
respect to these parameters seems appropriate instead of using a canonical parameter value. Otherwise 
more involved techniques may be needed; we plan to investigate other strategies in future work.

j

j

3.2 Optimization and Practical Heuristics

The backpropagation rules of the previous section have set up the stage for using a stochastic gradient
descent method (SGD). We now present a few strategies to accelerate it in our context.
Hybrid convex/non-convex optimization. Recently  many incremental optimization techniques
have been proposed for solving convex optimization problems of the form (7) when n is large but
ﬁnite (see [16] and references therein). These methods usually provide a great speed-up over the
stochastic gradient descent algorithm without suffering from the burden of choosing a learning rate.
The price to pay is that they rely on convexity  and they require storing into memory the full training
set. For solving (7) with ﬁxed network parameters Z  it means storing the n maps I i
k  which is often
reasonable if we do not use data augmentation. To partially leverage these fast algorithms for our
non-convex problem  we have adopted a minimization scheme that alternates between two steps: (i)
ﬁx Z  then make a forward pass on the data to compute the n maps I i
k and minimize the convex
problem (7) with respect to W using the accelerated MISO algorithm [16]; (ii) ﬁx W  then make one
pass of a projected stochastic gradient algorithm to update the k set of ﬁlters Zj. The set of network
parameters Z is initialized with the unsupervised learning method described in Section 2.

6

Preconditioning on the sphere. The kernels κj are deﬁned on the sphere; therefore  it is natural
to constrain the ﬁlters—that is  the columns of the matrices Zj—to have unit (cid:96)2-norm. As a result 
a classical stochastic gradient descent algorithm updates at iteration t each ﬁlter z as follows z ←
Proj
(cid:107).(cid:107)2=1[z−ηt∇zLt]  where ∇zLt is an estimate of the gradient computed on a minibatch and ηt is
a learning rate. In practice  we found that convergence could be accelerated by preconditioning  which
consists of optimizing after a change of variable to reduce the correlation of gradient entries. For
unconstrained optimization  this heuristic involves choosing a symmetric positive deﬁnite matrix Q
and replacing the update direction ∇zLt by Q∇zLt  or  equivalently  performing the change of
variable z = Q1/2z(cid:48) and optimizing over z(cid:48). When constraints are present  the case is not as simple
since Q∇zLt may not be a descent direction. Fortunately  it is possible to exploit the manifold
structure of the constraint set (here  the sphere) to perform an appropriate update [1]. Concretely  (i)
we choose a matrix Q per layer that is equal to the inverse covariance matrix of the patches from the
same layer computed after the initialization of the network parameters. (ii) We perform stochastic
gradient descent steps on the sphere manifold after the change of variable z = Q1/2z(cid:48)  leading to the
(cid:107).(cid:107)2=1[z − ηt(I − (1/z(cid:62)Qz)Qzz(cid:62))Q∇zLt]. Because this heuristic is not a critical
update z ← Proj
component  but simply an improvement of SGD  we relegate mathematical details in Appendix C.

Automatic learning rate tuning. Choosing the right learning rate in stochastic optimization is
still an important issue despite the large amount of work existing on the topic  see  e.g.  [13] and
references therein. In our paper  we use the following basic heuristic: the initial learning rate ηt
is chosen “large enough”; then  the training loss is evaluated after each update of the weights W.
When the training loss increases between two epochs  we simply divide the learning rate by two  and
perform “back-tracking” by replacing the current network parameters by the previous ones.

Active-set heuristic. For classiﬁcation tasks  “easy” samples have often negligible contribution to
the gradient (see  e.g.  [13]). For instance  for the squared hinge loss L(y  ˆy) = max(0  1 − y ˆy)2  the
gradient vanishes when the margin y ˆy is greater than one. This motivates the following heuristic: we
consider a set of active samples  initially all of them  and remove a sample from the active set as soon
as we obtain zero when computing its gradient. In the subsequent optimization steps  only active
samples are considered  and after each epoch  we randomly reactivate 10% of the inactive ones.

4 Experiments

We now present experiments on image classiﬁcation and super-resolution. All experiments were
conducted on 8-core and 10-core 2.4GHz Intel CPUs using C++ and Matlab.

Image Classiﬁcation on “Deep Learning” Benchmarks

4.1
We consider the datasets CIFAR-10 [12] and SVHN [19]  which contain 32 × 32 images from 10
classes. CIFAR-10 is medium-sized with 50 000 training samples and 10 000 test ones. SVHN is
larger with 604 388 training examples and 26 032 test ones. We evaluate the performance of a 9-layer
network  designed with few hyper-parameters: for each layer  we learn 512 ﬁlters and choose the RBF
kernels κj deﬁned in (2) with initial parameters αj = 1/(0.52). Layers 1  3  5  7  9 use 3×3 patches
and a subsampling pooling factor of √2 except for layer 9 where the factor is 3; Layers 2  4  6  8 use
simply 1 × 1 patches and no subsampling. For CIFAR-10  the parameters αj are kept ﬁxed during
training  and for SVHN  they are updated in the same way as the ﬁlters. We use the squared hinge
loss in a one vs all setting to perform multi-class classiﬁcation (with shared ﬁlters Z between classes).
The input of the network is pre-processed with the local whitening procedure described in [20]. We
use the optimization heuristics from the previous section  notably the automatic learning rate scheme 
and a gradient momentum with parameter 0.9  following [12]. The regularization parameter λ and
the number of epochs are set by ﬁrst running the algorithm on a 80/20 validation split of the training
set. λ is chosen near the canonical parameter λ = 1/n  in the range 2i/n  with i = −4  . . .   4  and
the number of epochs is at most 100. The initial learning rate is 10 with a minibatch size of 128.
We present our results in Table 1 along with the performance achieved by a few recent methods
without data augmentation or model voting/averaging. In this context  the best published results are
obtained by the generalized pooling scheme of [14]. We achieve about 2% test error on SVHN and
about 10% on CIFAR-10  which positions our method as a reasonably “competitive” one  in the same
ballpark as the deeply supervised nets of [15] or network in network of [17].

7

Table 1: Test error in percents reported by a few recent publications on the CIFAR-10 and SVHN
datasets without data augmentation or model voting/averaging.

Stoch P. [29] MaxOut [9]

CIFAR-10

SVHN

15.13
2.80

11.68
2.47

NiN [17]

10.41
2.35

DSN [15]

9.69
1.92

Gen P. [14]

7.62
1.69

SCKN (Ours)

10.20
2.04

Due to lack of space  the results reported here only include a single supervised model. Preliminary
experiments with no supervision show also that one may obtain competitive accuracy with wide
shallow architectures. For instance  a two-layer network with (1024-16384) ﬁlters achieves 14.2%
error on CIFAR-10. Note also that our unsupervised model outperforms original CKNs [18]. The best
single model from [18] gives indeed 21.7%. Training the same architecture with our approach is two
orders of magnitude faster and gives 19.3%. Another aspect we did not study is model complexity.
Here as well  preliminary experiments are encouraging. Reducing the number of ﬁlters to 128 per
layer yields indeed 11.95% error on CIFAR-10 and 2.15% on SVHN. A more precise comparison
with no supervision and with various network complexities will be presented in another venue.

4.2

Image Super-Resolution from a Single Image

Image up-scaling is a challenging problem  where convolutional neural networks have obtained
signiﬁcant success [7  8  27]. Here  we follow [8] and replace traditional convolutional neural
networks by our supervised kernel machine. Speciﬁcally  RGB images are converted to the YCbCr
color space and the upscaling method is applied to the luminance channel only to make the comparison
possible with previous work. Then  the problem is formulated as a multivariate regression one. We
build a database of 200 000 patches of size 32 × 32 randomly extracted from the BSD500 dataset [2]
after removing image 302003.jpg  which overlaps with one of the test images. 16× 16 versions of the
patches are build using the Matlab function imresize  and upscaled back to 32 × 32 by using bicubic
interpolation; then  the goal is to predict high-resolution images from blurry bicubic interpolations.
The blurry estimates are processed by a 9-layer network  with 3 × 3 patches and 128 ﬁlters at every
layer without linear pooling and zero-padding. Pixel values are predicted with a linear model applied
to the 128-dimensional vectors present at every pixel location of the last layer  and we use the square
loss to measure the ﬁt. The optimization procedure and the kernels κj are identical to the ones used
for processing the SVHN dataset in the classiﬁcation task. The pipeline also includes a pre-processing
step  where we remove from input images a local mean component obtained by convolving the images
with a 5 × 5 averaging box ﬁlter; the mean component is added back after up-scaling.
For the evaluation  we consider three datasets: Set5 and Set14 are standard for super-resolution;
Kodim is the Kodak Image database  available at http://r0k.us/graphics/kodak/  which con-
tains high-quality images with no compression or demoisaicing artefacts. The evaluation procedure
follows [7  8  26  27] by using the code from the author’s web page. We present quantitative results
in Table 2. For x3 upscaling  we simply used twice our model learned for x2 upscaling  followed by a
3/4 downsampling. This is clearly suboptimal since our model is not trained to up-scale by a factor 3 
but this naive approach still outperforms other baselines [7  8  27] that are trained end-to-end. Note
that [27] also proposes a data augmentation scheme at test time that slightly improves their results. In
Appendix D  we also present a visual comparison between our approach and [8]  whose pipeline is
the closest to ours  up to the use of a supervised kernel machine instead of CNNs.

Table 2: Reconstruction accuracy for super-resolution in PSNR (the higher  the better). All CNN
approaches are without data augmentation at test time. See Appendix D for the SSIM quality measure.
Fact. Dataset Bicubic SC [30] ANR [26] A+[26] CNN1 [7] CNN2 [8] CSCN [27]

x2

x3

Set5
Set14
Kodim
Set5
Set14
Kodim

33.66
30.23
30.84
30.39
27.54
28.43

35.78
31.80
32.19
31.90
28.67
29.21

35.83
31.79
32.23
31.92
28.65
29.21

36.54
32.28
32.71
32.58
29.13
29.57

36.34
32.18
32.62
32.39
29.00
29.42

36.66
32.45
32.80
32.75
29.29
29.64

36.93
32.56
32.94
33.10
29.41
29.76

SCKN
37.07
32.76
33.21
33.08
29.50
29.88

Acknowledgments

This work was supported by ANR (MACARON project ANR-14-CE23-0003-01).

8

References
[1] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton

University Press  2009.

[2] P. Arbelaez  M. Maire  C. Fowlkes  and J. Malik. Contour detection and hierarchical image segmentation.

IEEE T. Pattern Anal.  33(5):898–916  2011.

[3] L. Bo  K. Lai  X. Ren  and D. Fox. Object recognition with hierarchical kernel descriptors. In CVPR  2011.
[4] D. S. Broomhead and D. Lowe. Radial basis functions  multi-variable functional interpolation and adaptive

networks. Technical report  DTIC Document  1988.

[5] Y. Cho and L. K. Saul. Kernel methods for deep learning. In Adv. NIPS  2009.
[6] A. Damianou and N. Lawrence. Deep Gaussian processes. In Proc. AISTATS  2013.
[7] C. Dong  C. C. Loy  K. He  and X. Tang. Learning a deep convolutional network for image super-resolution.

In Proc. ECCV. 2014.

[8] C. Dong  C. C. Loy  K. He  and X. Tang. Image super-resolution using deep convolutional networks. IEEE

T. Pattern Anal.  38(2):295–307  2016.

[9] I. J. Goodfellow  D. Warde-Farley  M. Mirza  A. Courville  and Y. Bengio. Maxout networks. In Proc.

ICML  2013.

[10] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proc. CVPR  2016.
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In Proc. ICML  2015.

[12] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Adv. NIPS  2012.

[13] Y. Le Cun  L. Bottou  G. B. Orr  and K.-R. Müller. Efﬁcient backprop. In Neural Networks  Tricks of the

Trade  Lecture Notes in Computer Science LNCS 1524. 1998.

[14] C.-Y. Lee  P. W. Gallagher  and Z. Tu. Generalizing pooling functions in convolutional neural networks:

Mixed  gated  and tree. In Proc. AISTATS  2016.

[15] C.-Y. Lee  S. Xie  P. W. Gallagher  Z. Zhang  and Z. Tu. Deeply-supervised nets. In Proc. AISTATS  2015.
[16] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Adv. NIPS  2015.
[17] M. Lin  Q. Chen  and S. Yan. Network in network. In Proc. ICLR  2013.
[18] J. Mairal  P. Koniusz  Z. Harchaoui  and C. Schmid. Convolutional kernel networks. In Adv. NIPS  2014.
[19] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural images with

unsupervised feature learning. In NIPS workshop on deep learning  2011.

[20] M. Paulin  M. Douze  Z. Harchaoui  J. Mairal  F. Perronin  and C. Schmid. Local convolutional features

with unsupervised training for image retrieval. In Proc. ICCV  2015.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Adv. NIPS  2007.
[22] B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines  regularization  optimization 

and beyond. MIT press  2002.

[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

Proc. ICLR  2015.

[24] S. Sonnenburg  G. Rätsch  C. Schäfer  and B. Schölkopf. Large scale multiple kernel learning. J. Mach.

Learn. Res.  7:1531–1565  2006.

[25] V. Sydorov  M. Sakurada  and C. Lampert. Deep Fisher kernels — end to end learning of the Fisher kernel

GMM parameters. In Proc. CVPR  2014.

[26] R. Timofte  V. Smet  and L. van Gool. Anchored neighborhood regression for fast example-based super-

resolution. In Proc. ICCV  2013.

[27] Z. Wang  D. Liu  J. Yang  W. Han  and T. Huang. Deep networks for image super-resolution with sparse

prior. In Proc. ICCV  2015.

[28] C. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. In Adv. NIPS  2001.
[29] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks.

In Proc. ICLR  2013.

[30] R. Zeyde  M. Elad  and M. Protter. On single image scale-up using sparse-representations. In Curves and

Surfaces  pages 711–730. 2010.

[31] K. Zhang  I. W. Tsang  and J. T. Kwok. Improved Nyström low-rank approximation and error analysis. In

Proc. ICML  2008.

9

,Leonid Boytsov
Bilegsaikhan Naidan
Julien Mairal