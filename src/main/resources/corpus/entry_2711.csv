2013,Inverse Density as an Inverse Problem: the Fredholm Equation Approach,We address the problem of estimating the ratio $\frac{q}{p}$ where $p$ is a density function and $q$ is another density  or  more generally an arbitrary function.  Knowing or approximating this ratio is needed in various problems of inference and integration  in particular  when one needs to average a function with respect to one probability distribution  given a sample from another. It is often referred as {\it importance sampling} in statistical inference and is  also closely related to the problem of {\it covariate shift} in transfer learning as well as to various MCMC methods. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel  and thus reducing it to an integral equation  known as the Fredholm problem of the first kind.   This formulation  combined with the techniques of regularization and kernel methods  leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically.  The resulting family of algorithms (FIRE  for Fredholm Inverse Regularized Estimator) is flexible   simple and  easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities defined on $\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difficult problem. Interestingly  it turns out that in the density ratio estimation setting  when samples from both distributions are available  there are simple completely unsupervised methods for choosing parameters. We  call this model selection mechanism CD-CV for Cross-Density Cross-Validation. Finally  we show encouraging experimental results including applications to classification  within the covariate shift framework.,Inverse Density as an Inverse Problem:

the Fredholm Equation Approach

Qichao Que  Mikhail Belkin

Department of Computer Science and Engineering
{que mbelkin}@cse.ohio-state.edu

The Ohio State University

Abstract

We address the problem of estimating the ratio q
p where p is a density function
and q is another density  or  more generally an arbitrary function. Knowing or ap-
proximating this ratio is needed in various problems of inference and integration
often referred to as importance sampling in statistical inference. It is also closely
related to the problem of covariate shift in transfer learning. Our approach is
based on reformulating the problem of estimating the ratio as an inverse problem
in terms of an integral operator corresponding to a kernel  known as the Fredholm
problem of the ﬁrst kind. This formulation  combined with the techniques of reg-
ularization leads to a principled framework for constructing algorithms and for
analyzing them theoretically. The resulting family of algorithms (FIRE  for Fred-
holm Inverse Regularized Estimator) is ﬂexible  simple and easy to implement.
We provide detailed theoretical analysis including concentration bounds and con-
vergence rates for the Gaussian kernel for densities deﬁned on Rd and smooth
d-dimensional sub-manifolds of the Euclidean space.
Model selection for unsupervised or semi-supervised inference is generally a difﬁ-
cult problem. It turns out that in the density ratio estimation setting  when samples
from both distributions are available  simple completely unsupervised model se-
lection methods are available. We call this mechanism CD-CV for Cross-Density
Cross-Validation. We show encouraging experimental results including applica-
tions to classiﬁcation within the covariate shift framework.

Introduction

1
In this paper we address the problem of estimating the ratio of two functions  q(x)
p(x) where p is given
by a sample and q(x) is either a known function or another probability density function given by a
sample. This estimation problem arises naturally when one attempts to integrate a function with re-
spect to one density  given its values on a sample obtained from another distribution. Recently there
have been a signiﬁcant amount of work on estimating the density ratio (also known as the impor-
tance function) from sampled data  e.g.  [6  10  9  22  2]. Many of these papers consider this problem
in the context of covariate shift assumption [19] or the so-called selection bias [27]. The approach
taken in our paper is based on reformulating the density ratio estimation as an integral equation 
known as the Fredholm equation of the ﬁrst kind  and solving it using the tools of regularization
and Reproducing Kernel Hilbert Spaces. That allows us to develop simple and ﬂexible algorithms
for density ratio estimation within the popular kernel learning framework. The connection to the
classical operator theory setting makes it easier to apply the standard tools of spectral and Fourier
analysis to obtain theoretical results.
We start with the following simple equality underlying the importance sampling method:

Z

Z

(cid:18)

h(x) q(x)
p(x)

(cid:19)

(1)

Eq(h(x)) =

h(x)q(x)dx =

h(x) q(x)

p(x) p(x)dx = Ep

1

Z

Z

By replacing the function h(x) with a kernel k(x  y)  we obtain

Kp

q
p

(x) :=

k(x  y) q(y)

p(y) p(y)dy =

k(x  y)q(y)dy := Kq1(x).

(2)

Thinking of the function q(x)
p(x) as an unknown quantity and assuming that the right hand side is known
this becomes a Fredholm integral equation. Note that the right-hand side can be estimated given a
sample from q while the operator on the left can be estimated using a sample from p.
To push this idea further  suppose kt(x  y) is a “local” kernel  (e.g.  the Gaussian  kt(x  y) =
(2πt)d/2 e− kx−yk2
Rd kt(x  y)dx = 1. When we use δ-kernels  like Gaussian  and f
Rd kt(x  y)f(x)dx = f(y) + O(t) (see [24]  Ch.

satisﬁes some smoothness conditions  we haveR

) such thatR

2t

1

1). Thus we get another (approximate) integral equality:
kt(x  y) q(x)

(y) :=

Kt p

q
p

p(x) p(x)dx ≈ q(y).

Z

Rd

(3)

It becomes an integral equation for q(x)

p(x)  assuming that q is known or can be approximated.

n

L2 p

L2 p

≈ 1

n

L2 p

≈ arg min

≈ arg min

f∈H kKpf−Kq1(x)k2

We address these inverse problems by formulating them within the classical framework of Tiknonov-
Philips regularization with the penalty term corresponding to the norm of the function in the Repro-
ducing Kernel Hilbert Space H with kernel kH used in many machine learning algorithms.
+λkfk2H
[Type I]: q
p
Importantly  given a sample x1  . . .   xn from p  the integral operator Kpf applied to a function f
can be approximated by the corresponding discrete sum Kpf(x) ≈ 1
i f(xi)K(xi  x)  while
L2 p norm is approximated by an average: kfk2
i f(xi)2. Of course  the same holds for
a sample from q. We see that the Type I formulation is useful when q is a density and samples from
both p and q are available  while the Type II is useful  when the values of q (which does not have to
be a density function at all1) are known at the data points sampled from p.
Since all of these involve only function evaluations at the sample points  an application of the usual
representer theorem for Reproducing Kernel Hilbert Spaces  leads to simple  explicit and easily
implementable algorithms  representing the solution of the optimization problem as linear combi-
i αikH(xi  x) (see Section 2). We call the

nations of the kernels over the points of the sampleP

+λkfk2H [II]: q
p
P

f∈H kKt pf−qk2
P

resulting algorithms FIRE for Fredholm Inverse Regularized Estimator.
Remark: Other norms and loss functions. Norms and loss functions other that L2 p can also be
used in our setting as long as they can be approximated from a sample using function evaluations.
1. Perhaps  the most interesting is L2 q norm available in the Type I setting  when a sample from
the probability distribution q is available. In fact  given a sample from both p and q we can use the
combined empirical norm γk · kL2 p + (1 − γ)k · kL2 q. Optimization using those norms leads to
some interesting kernel algorithms described in Section 2. We note that the solution is still a linear
combination of kernel functions centered on the sample from p and can still be written explicitly.
2. In Type I formulation  if the kernels k(x  y) and kH(x  y) coincide  it is possible to use the RKHS
norm k · kH instead of L2 p. This formulation (see Section 2) also yields an explicit formula and is
related to the Kernel Mean Matching [9]   although with a different optimization procedure.
Since we are dealing with a classical inverse problem for integral operators  our formulation allows
for theoretical analysis using the methods of spectral theory. In Section 3 we present concentration
and error bounds as well as convergence rates for our algorithms when data are sampled from a
distribution deﬁned in Rd  a domain in Rd with boundary or a compact d-dimensional sub-manifold
of a Euclidean space RN for the case of the Gaussian kernel.
In Section 4 we introduce a unsupervised method  referred as CD-CV (for cross-density cross-
validation) for model selection and discuss the experimental results on several data sets comparing
our method FIRE with the available alternatives  Kernel Mean Matching (KMM) [9] and LSIF [10]
as well as the base-line thresholded inverse kernel density estimator2 (TIKDE) and importance sam-
pling (when available).

1This could be useful in sampling procedures  when the normalizing coefﬁcients are hard to estimate.
2The standard kernel density estimator for q divided by a thresholded kernel density estimator for p.

2

We summarize the contributions of the paper as follows:
1. We provide a formulation of estimating the density ratio (importance function) as a classical
inverse problem  known as the Fredholm equation  establishing a connections to the methods of
classical analysis. The underlying idea is to “linearize” the properties of the density by studying an
associated integral operator.
2. To solve the resulting inverse problems we apply regularization with an RKHS norm penalty. This
provides a ﬂexible and principled framework  with a variety of different norms and regularization
techniques available. It separates the underlying inverse problem from the necessary regularization
and leads to a family of very simple and direct algorithms within the kernel learning framework in
machine learning.
3. Using the techniques of spectral analysis and concentration  we provide a detailed theoretical
analysis for the case of the Gaussian kernel  for Euclidean case as well as for distributions supported
on a sub-manifold. We prove error bounds and as well as the convergence rates.
4. We also propose a completely unsupervised technique  CD-CV  for cross-validating the parame-
ters of our algorithm and demonstrate its usefulness  thus addressing in our setting one of the most
thorny issues in unsupervised/semi-supervised learning. We evaluate and compare our methods on
several different data sets and in various settings and demonstrate strong performance and better
computational efﬁciency compared to the alternatives.
Related work. Recently the problem of density ratio estimation has received signiﬁcant attention
due in part to the increased interest in transfer learning [15] and  in particular to the form of transfer
learning known as covariate shift [19]. To give a brief summary  given the feature space X and the
label space Y   two probability distributions p and q on X × Y satisfy the covariate assumption if
for all x  y  p(y|x) = q(y|x). It is easy to see that training a classiﬁer to minimize the error for q 
given a sample from p requires estimating the ratio of the marginal distributions qX (x)
pX (x). The work on
covariate shift  density ratio estimation and related settings includes [27  2  6  10  22  9  23  14  7].
The algorithm most closely related to ours is Kernel Mean Matching [9]. It is based on the equation:
pΦ(x))  where Φ is the feature map corresponding to an RKHS H. It is rewritten
Eq(Φ(x)) = Ep( q
p(x) ≈ arg minβ∈L2 β(x)>0 Ep(β)=1 kEq(Φ(x)) − Ep(β(x)Φ(x))kH.
as an optimization problem q(x)
The quantity on the right can be estimated given a sample from p and a sample from q and the
minimization becomes a quadratic optimization problem over the values of β at the points sampled
from p. Writing down the feature map explicitly  i.e.  recalling that Φ(x) = KH(x ·)  we see that
the equality Eq(Φ(x)) = Ep( q
pΦ(x)) is equivalent to the integral equation Eq. 2 considered as an
identity in the Hilbert space H. Thus the problem of KMM can be viewed within our setting Type I
(see the Remark 2 in the introduction)  with a RKHS norm but a different optimization algorithm.
However  while the KMM optimization problem uses the RKHS norm  the weight function β itself
is not in the RKHS. Thus  unlike most other algorithms in the RKHS framework (in particular 
FIRE)  the empirical optimization problem does not have a natural out-of-sample extension. Also 
since there is no regularizing term  the problem is less stable (see Section 4 for some experimental
comparisons) and the theoretical analysis is harder (however  see [6] and the recent paper [26] for
some nice theoretical analysis of KMM in certain settings).
Another related recent algorithm is Least Squares Importance Sampling (LSIF) [10]  which attempts
to estimate the density ratio by choosing a parametric linear family of functions and choosing a
function from this family to minimize the L2 p distance to the density ratio. A similar setting with
the Kullback-Leibler distance (KLIEP) was proposed in [23]. This has an advantage of a natural
out-of-sample extension property. We note that our method for unsupervised parameter selection in
Section 4 is related to their ideas. However  in our case the set of test functions does not need to
form a good basis since no approximation is required.
We note that our methods are closely related to a large body of work on kernel methods in machine
learning and statistical estimation (e.g.  [21  17  16]). Many of these algorithms can be interpreted
as inverse problems  e.g.  [3  20] in the Tikhonov regularization or other regularization frameworks.
In particular  we note interesting methods for density estimation proposed in [12] and estimating the
support of density through spectral regularization in [4]  as well as robust density estimation using
RKHS formulations [11] and conditional density [8]. We also note the connections of the methods
in this paper to properties of density-dependent operators in classiﬁcation and clustering [25  18  1].
Among those works that provide theoretical analysis of algorithms for estimating density ratios 

3

P

[14] establishes minimax rates for likelihood ratio estimation. Another recent theoretical analysis of
KMM in [26] contains bounds for the output of the corresponding integral operators.
2 Settings and Algorithms
Settings and objects. We start by introducing objects and function spaces important for our de-
velopment. As usual  the norm in space of square-integrable functions with respect to a measure

Ω |f(x)|2dρ < ∞(cid:9) . This is a Hilbert space with the inner

ρ  is deﬁned as follows: L2 ρ = (cid:8)f :R
product deﬁned in the usual way by hf  gi2 ρ =R
the operator Kρ: Kρf(y) :=R

Ω f(x)g(x)dρ. Given a kernel k(x  y) we deﬁne
Ω k(x  y)f(x)dρ(x). We will use the notation Kt ρ to explicitly refer
to the parameter of the kernel function kt(x  y)  when it is a δ-family. If the function k(x  y) is
symmetric and positive deﬁnite  then there is a corresponding Reproducing Kernel Hilbert space
(RKHS) H. We recall the key property of the kernel kH: for any f ∈ H  hf  kH(x ·)iH = f(x).
The Representer Theorem allows us to write solutions to various optimization problems over H in
P
terms of linear combinations of kernels supported on sample points (see [21] for an in-depth discus-
sion or the RKHS theory and the issues related to learning). Given a sample x1  . . .   xn from p  one
can approximate the L2 p norm of a sufﬁciently smooth functionf by kfk2
i |f(xi)|2  and
similarly  the integral operator Kpf(x) ≈ 1
i k(xi  x)f(xi). These approximate equalities can
be made precise by using appropriate concentration inequalities.
The FIRE Algorithms. As discussed in the introduction  the starting point for our development is
the two integral equalities 
p(y) dp(y) = q(·) + o(1)
[I]: Kp

p(y) dp(y) = Kq1(·) [II]:Kt p

kt(·  y) q(y)

k(·  y) q(y)

q   known as Fredholm equations of the ﬁrst kind. To estimate p

(4)
Notice that in the Type I setting  the kernel does not have to be in a δ-family. For example  a linear
kernel is admissible. Type II setting comes from the fact Kt qf(x) ≈ f(x)p(x) + O(t) for a “δ-
function-like” kernel and we keep t in the notation in that case. Assuming that either Kq1 or q are
(approximately) known (Type I and II settings  respectively) equalities in Eqs. 4 become integral
equations for p
q   we need to obtain
an approximation to the solution which (a) can be obtained computationally from sampled data  (b)
is stable with respect to sampling and other perturbation of the input function  (c) can be analyzed
using the standard machinery of functional analysis.
To provide a framework for solving these inverse problems  we apply the classical techniques of
regularization combined with the RKHS norm popular in machine learning. In particular a simple
formulation of Type I using Tikhonov regularization  ([5]  Ch. 5)  with the L2 p norm is as follows:
(5)

[Type I]:

2 p ≈ 1

(·) =

(·) =

2 p + λkfk2H

f I
λ = arg min

f∈HkKpf − Kq1k2

Here H is an appropriate Reproducing Kernel Hilbert Space. Similarly Type II can be solved by

Z

Z

q
p

q
p

n

n

[Type II]:

f II
λ = arg min

f∈H kKt pf − qk2

2 p + λkfk2H

(6)

We will now discuss the empirical versions of these equations and the resulting algorithms.
Type I setting. Algorithm for L2 p norm. Given an iid sample from p  zp = {xi}n
an iid sample from q  zq = {x0
P
integral operators Kp and Kq by Kzpf(x) = 1

i=1 and
j}m
j=1 (z for the combined sample)  we can approximate the
and Kzq f(x) =
X

i). Thus the empirical version of Eq. 5 becomes

k(xi  x)f(xi)

P

i∈zq
x0

k(x0

xi∈zp

((Kzpf)(xi) − (Kzq1)(xi))2 + λkfk2H

i  x)f(x0
f I
λ z = arg min
f∈H

(7)

1
m

n

1
n

xi∈zp

The ﬁrst term of the optimization problem involves only evaluations of the function f at the points
of the sample. From Representer Theorem and matrix manipulation  we obtain the following:

λ z(x) = X

f I

kH(xi  x)vi and v =(cid:0)K 2

p pKH + nλI(cid:1)−1

Kp pKp q1zq .

(8)

xi∈zp

where the kernel matrices are deﬁned as follows: (Kp p)ij = 1
xi  xj ∈ zp and Kp q is deﬁned as (Kp q)ij = 1

m k(xi  x0

j) for xi ∈ zp and x0

j ∈ zq.

n k(xi  xj)  (KH)ij = kH(xi  xj) for

4

(cid:0)K 3
p p + λI(cid:1)−1

X

Kp pKp q1zq.

(9)
m} we

2  . . .   x0

f *
λ = arg min

arg min
f∈H

γ
n

λ z(x) =
1 − γ
m

2 p + (1 − γ)kKpf − Kq1k2

X
(cid:18) γ

xi∈zp

If KH and Kp p are the same kernel we simply have: v = 1
n
Algorithms for γL2 p +(1− γ)L2 q norm. Depending on the setting  we may want to minimize the
error of the estimate over the probability distribution p  q or over some linear combination of these.
A signiﬁcant potential beneﬁt of using a linear combination is that both samples can be used at the
same time in the loss function. First we state the continuous version of the problem:
2 q + λkfk2H
1  x0

f∈H γkKpf − Kq1k2
Given a sample from p  zp = {x1  x2  . . .   xn} and a sample from q  zq = {x0
i )(cid:1)2 +
(cid:0)Kzpf(xi) − Kzq1(xp
obtain an empirical version of the Eq. 9: f∗
λ z(x) =P
(cid:19)

i)(cid:1)2 + λkfk2
(cid:19)
v = (K + nλI)−1 K11zq
m k(xi  x0
j)
j ∈ zq. Despite the loss function combining both samples 

n k(xi  xj)  (KH)ij = kH(xi  xj) for xi  xj ∈ zp  and (Kp q)ij = 1
j  xi) for xi ∈ zp x0

where (Kp p)ij = 1
n k(x0
and (Kq p)ji = 1
the solution is still a summation of kernels over the points in the sample from p.
Algorithms for the RKHS norm. In addition to using the RKHS norm for regularization norm  we
λ = arg minf∈H kKpf − Kq1k2H0 + λkfk2H Here the Hilbert
can also use it as a loss function: f *
space H0 must correspond to the kernel k and can potentially be different from the space H used for
regularization. Note that this formulation is only applicable in the Type I setting since it requires
the function q to belong to the RKHS H0. Given two samples zp  zq  it is easy to write down the
empirical version of this problem  leading to the following formula:

(cid:0)(Kzpf)(x0
(cid:18) γ

From the Representer Theorem f∗

i∈zq
x0
vikH(xi  x)

K T

q pKq p

KH and K1 =

i) − (Kzq1)(x0

K =

(Kp p)2 +

Kp pKp q +

n

1 − γ
m

1 − γ
m

K T

q pKq q

xi∈zp

n

H

vikH(xi  x)

v = (Kp pKH + nλI)−1 Kp q1zq .

(10)

λ z(x) = X

f∗

xi∈zp

The result is somewhat similar to our Type I formulation with the L2 p norm. We note the connection
between this formulation of using the RKHS norm as a loss function and the KMM algorithm [9].
When the kernels K and KH are the same  Eq. 10 can be viewed as a regularized version of KMM
(with a different optimization procedure).
Type II setting. In Type II setting we assume that we have a sample z = {xi}n
i=1 drawn from p
and that we know the function values q(xi) at the points of the sample. Replacing the norm and the
integral operator with their empirical versions  we obtain the following optimization problem:

As before  using the Representer Theorem we obtain an analytical formula for the solution:

f II
λ z = arg min
f∈H

λ z(x) = X

f II

xi∈z

(Kt zpf(xi) − q(xi))2 + λkfk2H

kH(xi  x)vi where v =(cid:0)K 2KH + nλI(cid:1)−1

Kq.

(11)

X

xi∈z

1
n

n kt(xi  xj)  (KH)ij = kH(xi  xj) and qi = q(xi).

where the kernel matrix K is deﬁned by Kij = 1
Comparison of type I and type II settings.
1. In Type II setting q does not have to be a density function (i.e.  non-negative and integrate to one).
2. Eq. 7 of the Type I setting cannot be easily solved in the absence of a sample zq from q  since esti-
mating Kq requires either sampling from q (if it is a density) or estimating the integral in some other
way  which may be difﬁcult in high dimension but perhaps of interest in certain low-dimensional
application domains.
3. There are a number of problems (e.g.  many problems involving MCMC) where q(x) is known
explicitly (possibly up to a multiplicative constant)  while sampling from q is expensive or even
impossible computationally [13].
4. Unlike Eq. 5  Eq. 6 has an error term depending on the kernel. For example  in the important case
of the Gaussian kernel  the error is of the order O(t)  where t is the variance of Gaussian.
5. Several norms are available in the Type I setting  but only the L2 p norm is available for Type II.

5

3 Theoretical analysis: bounds and convergence rates for Gaussian Kernels
In this section  we state our main results on bounds and convergence rates for our algorithm based
on Tikhonov regularization with a Gaussian kernel. We consider both Type I and Type II settings
for the Euclidean and manifold cases and make a remark on the Euclidean domains with boundary.
To simplify the theoretical development  the integral operator and the RKHS H will correspond to
the same Gaussian kernel kt(x  y). The proofs will be found in the supplemental material.
Assumptions: The set Ω  where the density function p is deﬁned  could be one of the following: (1)
the whole Rd; (2) a compact smooth Riemannian sub-manifold M of d-dimension in Rn. We also
need p(x) < Γ  q(x) < Γ for any x ∈ Ω and that q
Theorem 1. ( Type I setting.) Let p and q be two density functions on Ω. Given n points  zp =
m}  i.i.d. sampled from
{x1  x2  . . .   xn}  i.i.d. sampled from p and m points  zq = {x0
q  and for small enough t  for the solution to the optimization problem in (7)  with conﬁdence at
least 1 − 2e−τ   we have
(cid:19)
(1) If the domain Ω is Rd  for some constants C1  C2  C3 independent of t  λ.
1
√
λ1/6

p2 are in Sobolev space W 2

≤ C1t + C2λ

λ z − q
p

2  . . .   x0

τ
λtd/2

2 + C3

2 (Ω).

1  x0

p   q

(12)

√

+

n

1

(2) If the domain Ω is a compact sub-manifold without boundary of d dimension  for some 0 < ε < 1 
C1  C2  C3 independent of t  λ.

(cid:13)(cid:13)(cid:13)(cid:13)f I
(cid:13)(cid:13)(cid:13)(cid:13)f I
(cid:13)(cid:13)(cid:13)(cid:13)2

2 p

(cid:13)(cid:13)(cid:13)(cid:13)2 p
(cid:13)(cid:13)(cid:13)(cid:13)2 p
(cid:16)√

τ n

m

(cid:18) 1√
(cid:18) 1√
(cid:16)√

m

= O

√

τ
λtd/2

(cid:13)(cid:13)(cid:13)(cid:13)2

2 p

(cid:19)

+

1
√
λ1/6

n

Corollary 2. ( Type I setting.) Assuming m > λ1/3n  with conﬁdence at least 1 − 2e−τ   when
(1) Ω = Rd  (2) Ω is a d-dimensional sub-manifold of a Euclidean space  we have

= O

− 1

3.5+d/2

(2)

−

τ n

1

3.5(1−ε)+d/2

(13)

(cid:17)∀ε ∈ (0  1)

λ z − q
p

≤ C1t1−ε + C2λ

1

2 + C3

(cid:13)(cid:13)(cid:13)(cid:13)f I

(1)

λ z − q
p

(cid:17)

(cid:13)(cid:13)(cid:13)(cid:13)f I

λ z − q
p

(cid:13)(cid:13)(cid:13)(cid:13)2 p
(cid:13)(cid:13)(cid:13)(cid:13)2 p

Theorem 3. ( Type II setting.) Let p be a density function on Ω and q be a function satisfying the
assumptions. Given n points z = {x1  x2  . . .   xn} sampled i.i.d. from p  and for sufﬁciently small
t  for the solution to the optimization problem in (11)  with conﬁdence at least 1 − 2e−τ   we have
(1) If the domain Ω is Rd 

√

(cid:13)(cid:13)(cid:13)(cid:13)f II
(cid:13)(cid:13)(cid:13)(cid:13)f II
(cid:13)(cid:13)(cid:13)(cid:13)f II

λ z − q
p

λ z − q
p

≤C1t + C2λ

1

2 + C3λ− 1

3 kKt q1 − qk2 p + C4

τ
λ3/2td/2

√

 

n

(14)

where C1  C2  C3  C4 are constants independent of t  λ. Moreover  kKt q1 − qk2 p = O(t).
(2) If Ω is a d-dimensional sub-manifold of a Euclidean space  for any 0 < ε < 1
√

λ z − q
p

≤C1t1−ε + C2λ

1

2 + C3λ− 1

3 kKt q1 − qk2 p + C4

τ
λ3/2td/2

√

 

n

(15)

where C1  C2  C3  C4 are independent of t  λ. Moreover  kKt q1 − qk2 p = O(t1−η) ∀η > 0.
Corollary 4. ( Type II setting.) With conﬁdence at least 1 − 2e−τ   when
(1) Ω = Rd  (2) Ω is a d-dimensional sub-manifold of a Euclidean space  we have
− 1−η
4−4η+ 5

(cid:18)√

(cid:18)√

− 1
4+ 5

(cid:19)

(cid:19)

= O

τ n

6 d

(2)

= O

τ n

(1)

6 d

∀η ∈ (0  1)

(cid:13)(cid:13)(cid:13)(cid:13)f II

λ z − q
p

(cid:13)(cid:13)(cid:13)(cid:13)2

2 p

(cid:13)(cid:13)(cid:13)(cid:13)2

2 p

4 Model Selection and Experiments
We describe an unsupervised technique for parameter selection  Cross-Density Cross-Validation
(CD-CV) based on a performance measure unique to our setting. We proceed to evaluate our method.
The setting. In our experiments  we have X p = {xp
m}. The
goal is to estimate q
p  assuming that X p  X q are i.i.d. sampled from p  q respectively. Note that

n} and X q = {xq

1  . . .   xp

1  . . .   xq

6

p is unsupervised and our algorithms typically have two parameters: the kernel width t and

learning q
regularization parameter λ.
(cid:16)
Performance Measures and CD-CV Model Selection. We describe a set of performance measures
used for parameter selection. For a given function u  we have the following importance sampling
equality (Eq. 1): Ep(u(x)) = Eq
p  and
X p  X q are samples from p  q respectively  we will have the following approximation to the previous
i ) ≈ 1
j). So after obtaining an estimate f of the ratio  we
equation: 1
n
can validate it using the following performance measure:

. If f is an approximation of the true ratio q

Pm

Pn

u(x) p(x)
q(x)

j=1 u(xq

m

i=1 u(xp

i )f(xp

(cid:17)
 nX

FX

i ) − mX

2

JCD(f; X p  X q  U) =

1
F

ul(xp

i )f(xp

ul(xq
j)

(16)

l=1

i=1

j=1

cv and X q

train and X q

train for training and X p

where U = {u1  . . .   uF} is a collection of test functions. Using this performance measure allows
various cross-validation procedures to be used for parameter selection. We note that this way to
measure error is related to the LSIF [10] and KLIEP [23] algorithms. However  there a similar
measure is used to construct an approximation to the ratio q
p using functions u1  . . .   uF as a basis.
In our setting  we can use test functions (e.g.  linear functions) which are poorly suited as a basis for
approximating the density ratio.
We will use the following two families of test functions for parameter selection: (1) Sets of ran-
dom linear functions ui(x) = βT x where β ∼ N(0  Id); (2) Sets of random half-space indicator
functions  ui(x) = 1βT x>0.
Procedures for parameter selection. The performance is optimized using ﬁve-fold cross-validation
by splitting the data set into two parts X p
cv for validation.
The range we use for kernel width t is (t0  2t0  . . .   29t0)  where t0 is the average distance of the 10
nearest neighbors. The range for regularization parameter λ is (1e − 5  1e − 6  . . .   1e − 10).
Data sets and Resampling We use two datasets  CPUsmall and Kin8nm  for regression; and USPS
handwritten digits for classiﬁcation. And we draw the ﬁrst 500 or 1000 points from the original
data set as X p. To obtain X q  the following two ways of resampling  using the features or the label
information  are used (along the lines of those in [6]).
Given a set of data with labels {(x1  y1)  (x2  y2)  . . .   (xn  yn)} and denoting Pi the probability of
i’th instance being chosen  we resample as follows:
(1) Resampling using features (labels yi are not used). Pi = e(ahxi e1i−b)/σv
1+e(ahxi e1i−b)/σv   where a  b are the
resampling parameters  e1 is the ﬁrst principal component  and σv is the standard deviation of the
projections to e1. This resampling method will be denoted by PCA(a  b).
where yi ∈ L = {1  2  . . .   k} and Lq is a
(2) Resampling using labels. Pi =
subset of the whole label set L. It only applies to binary problems obtained by aggregating different
classes in multi-class setting.
Testing the FIRE algorithm. In the ﬁrst experiment  we test our method for selecting parameters
by focusing on the error JCD(f; X p  X q  U) in Eq. 16 for different function classes U. Parameters
are chosen using a family of functions U1  while the performance of the parameter is measured using
an independent function family U2. This measure is important because in practice the functions we
are interested in may not be the ones chosen for validation.
We use the USPS data sets for this experiment. As a basis for comparison we use TIKDE (Thresh-
olded Inverse Kernel Density Estimator). TIKDE estimates ˆp and ˆq respectively using Kernel Den-
sity Estimation (KDE)  and assigns ˆp(x) = α to any x satisfying ˆp(x) < α. TIKDE then outputs
ˆq/ˆp. We note that chosen threshold α is key to reasonable performance. One issue of this heuristic
is that it could underestimate at the region with high density ratio  due to the uniform thresholding.
We also compare our methods to LSIF [10]. In these experiments we do not compare with KMM as
out-of-sample extension is necessary for fair comparison.
Table 1 shows the average errors of various methods  deﬁned in Eq. 16 on held-out set Xerr over 5
trials. We use different validation functions f cv(Columns) and error-measuring functions f err(Row).
N is the number of random functions used for validation. The error-measuring function families U2
are as follows: (1) Linear(L.): random linear functions f(x) = βT x where β ∼ N(0  Id); (2) Half-

(cid:26)1 yi ∈ Lq

0 Otherwise.

7

space(H.S.): Sets of random half-space indicator functions  f(x) = 1βT x; (3) Kernel(K.): random
linear combinations of kernel functions centered at training data  f(x) = γT K where γ ∼ N(0  Id)
and Kij = k(xi  xj) for xi from training set; (4) Kernel indicator(K.I.) functions f(x) = 1g(x)>0 
where g is as in (3).
Table 1: USPS data set with resampling using PCA(5  σv) with |X p| = 500  |X q| = 1371. Around
400 in X p and 700 in X q are used in 5-fold CV  the rest are held-out for computing the error.

N
TIKDE
LSIF
FIREp
FIREp q
FIREq
TIKDE
LSIF
FIREp
FIREp q
FIREq

L.

H.S.

Linear
50
10.9
14.1
3.6
4.7
5.9
2.6
3.9
1.0
0.9
1.2

200
10.9
14.1
3.7
4.7
6.2
2.6
3.9
0.9
1.0
1.4

Half-Spaces
50
200
10.9
10.9
28.2
26.8
6.3
5.5
6.8
7.4
9.3
9.3
2.6
2.6
3.9
3.7
1.2
1.0
1.4
1.1
1.6
1.6

N
TIKDE
LSIF
FIREp
FIREp q
FIREq
TIKDE
LSIF
FIREp
FIREp q
FIREq

K.

K.I.

Linear
50
4.7
16.1
1.2
2.1
5.2
4.2
4.4
0.9
0.6
1.2

200
4.7
16.1
1.1
2.0
4.3
4.2
4.4
0.7
0.6
0.9

Half-Spaces
50
200
4.7
4.7
13.8
15.6
3.6
2.8
2.6
4.2
6.1
6.1
4.2
4.2
4.4
5.3
1.1
1.2
1.9
1.1
2.2
2.2

Supervised Learning: Regression and Classiﬁcation. We compare our FIRE algorithm with sev-
eral other methods in regression and classiﬁcation tasks. We consider the situation where part of
the data set X p are labeled and all of X q are unlabeled. We use weighted ordinary least-square for
regression and weighted linear SVM for classiﬁcation.
Regression. Square loss function is used for regression. The performance is measured using nor-
(ˆyi−yi)2
Var(ˆy−y) . X q is resampled using PCA resampler  described before. L is
for Linear  and HS is for Half-Space function families for parameter selection.
Table 2: Mean normalized square loss on the CPUsmall and Kin8nm. |X p| = 1000  |X q| = 2000.

malized square loss Pn

i=1

No. of Labeled

Weights

OLS
TIKDE
KMM
LSIF
FIREp
FIREp q
FIREq

CPUsmall  resampled by PCA(5  σv)

Kin8nm  resampled by PCA(1  σv)

100

200

500

100

200

500

HS

L

L

.74

HS

.50

.38
1.86
.39
.33
.33
.32

.36
1.86
.39
.33
.33
.33

.30
1.9
.31
.29
.29
.28

.29
1.9
.31
.29
.29
.29

L

.28
2.5
.33
.27
.27
.27

HS

.83

.28
2.5
.33
.27
.27
.27

L

.57
.58
.57
.57
.56
.56

HS

.59

.57
.58
.56
.56
.56
.56

L

.55
.55
.54
.55
.55
.55

HS

.55

.55
.55
.54
.54
.54
.54

L

.53
.52
.52
.52
.52
.52

HS

0.54

.53
.52
.52
.52
.52
.52

Classiﬁcation. Weighted linear SVM. Percentage of incorrectly labeled test set instances.
Table 3: Average error on USPS with +1 class= {0 − 4}  −1 class= {5 − 9} and |X p| = 1000
and |X q| = 2000. Left half of the table uses resampling PCA(5  σv)  where σv. Right half shows
resampling based on Label information.
L = {{0 − 4} {5 − 9}} L0 = {0  1  5  6}
PCA(5  σv)

No. of Labeled

100

200

500

100

200

500

Weights
SVM
TIKDE
KMM
LSIF
FIREp
FIREp q
FIREq

L

HS

10.2

9.4
8.1
9.5
8.9
7.0
5.5

9.4
8.1
10.2
6.8
7.0
7.3

L

7.2
5.9
7.3
5.3
5.1
4.8

HS

8.1

7.2
5.9
8.1
5.0
5.1
5.4

L

4.9
4.7
5.0
4.1
4.1
4.1

HS

5.7

L

HS

18.6

L

HS

16.4

4.9
4.7
5.7
4.1
4.1
4.4

18.5
17.5
18.5
17.9
18.0
18.3

18.5
17.5
18.5
18.4
18.5
18.4

16.4
13.5
16.2
16.1
16.1
16.0

16.4
13.5
16.3
16.1
16.2
16.2

L

12.4
10.3
12.2
11.5
11.6
11.8

HS

12.9

12.4
10.3
12.2
12.0
12.0
12.0

Acknowledgements. The work was partially supported by NSF Grants IIS 0643916  IIS 1117707.

8

References
[1] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: A geometric framework for

learning from labeled and unlabeled examples. JMLR  7:2399–2434  2006.

[2] S. Bickel  M. Br¨uckner  and T. Scheffer. Discriminative learning for differing training and test

distributions. In ICML  2007.

[3] E. De Vito  L. Rosasco  A. Caponnetto  U. De Giovannini  and F. Odone. Learning from

examples as an inverse problem. JMLR  6:883  2006.

[4] E. De Vito  L. Rosasco  and A. Toigo. Spectral regularization for support estimation. In NIPS 

pages 487–495  2010.

[5] H. W. Engl  M. Hanke  and A. Neubauer. Regularization of inverse problems. Springer  1996.
[6] A. Gretton  A. Smola  J. Huang  M. Schmittfull  K. Borgwardt  and B. Sch¨olkopf. Covariate

shift by kernel mean matching. Dataset shift in machine learning  pages 131–160  2009.

[7] S. Gr¨unew¨alder  A. Gretton  and J. Shawe-Taylor. Smooth operators. In ICML  2013.
[8] S. Gr¨unew¨alder  G. Lever  L. Baldassarre  S. Patterson  A. Gretton  and M. Pontil. Conditional

mean embeddings as regressors. In ICML  2012.

[9] J. Huang  A. Gretton  K. M. Borgwardt  B. Sch¨olkopf  and A. Smola. Correcting sample

selection bias by unlabeled data. In NIPS  pages 601–608  2006.

[10] T. Kanamori  S. Hido  and M. Sugiyama. A least-squares approach to direct importance esti-

mation. JMLR  10:1391–1445  2009.

[11] J. S. Kim and C. Scott. Robust kernel density estimation. In ICASSP  pages 3381–3384  2008.
[12] S. Mukherjee and V. Vapnik. Support vector method for multivariate density estimation. In
Center for Biological and Computational Learning. Department of Brain and Cognitive Sci-
ences  MIT. CBCL  volume 170  1999.

[13] R. M. Neal. Annealed importance sampling. Statistics and Computing  11(2):125–139  2001.
[14] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Estimating divergence functionals and the

likelihood ratio by penalized convex risk minimization. NIPS  20:1089–1096  2008.

[15] S. J. Pan and Q. Yang. A survey on transfer learning. Knowledge and Data Engineering  IEEE

Transactions on  22(10):1345–1359  2010.

[16] B. Sch¨olkopf and A. J. Smola. Learning with kernels: Support vector machines  regularization 

optimization  and beyond. MIT press  2001.

[17] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge university

press  2004.

[18] T. Shi  M. Belkin  and B. Yu. Data spectroscopy: Eigenspaces of convolution operators and

clustering. The Annals of Statistics  37(6B):3960–3984  2009.

[19] H. Shimodaira.

Improving predictive inference under covariate shift by weighting the log-

likelihood function. Journal of Statistical Planning and Inference  90(2):227–244  2000.

[20] A. J. Smola and B. Sch¨olkopf. On a kernel-based method for pattern recognition  regression 

approximation  and operator inversion. Algorithmica  22(1):211–231  1998.
[21] I. Steinwart and A. Christmann. Support vector machines. Springer  2008.
[22] M. Sugiyama  M. Krauledat  and K. M¨uller. Covariate shift adaptation by importance weighted

cross validation. JMLR  8:985–1005  2007.

[23] Masashi Sugiyama  Shinichi Nakajima  Hisashi Kashima  Paul Von Buenau  and Motoaki
Kawanabe. Direct importance estimation with model selection and its application to covariate
shift adaptation. NIPS  20:1433–1440  2008.

[24] A. Tsybakov. Introduction to nonparametric estimation. Springer  2009.
[25] C. Williams and M. Seeger. The effect of the input density distribution on kernel-based classi-

ﬁers. In ICML  2000.

[26] Y. Yu and C. Szepesv´ari. Analysis of kernel mean matching under covariate shift. In ICML 

2012.

[27] B. Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In ICML  2004.

9

,Qichao Que
Mikhail Belkin