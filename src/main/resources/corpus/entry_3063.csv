2012,Learning from Distributions via Support Measure Machines,This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples  our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS)  we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this  we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights  we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.,Learning from Distributions via Support Measure

Machines

Krikamol Muandet

MPI for Intelligent Systems  T¨ubingen
krikamol@tuebingen.mpg.de

Kenji Fukumizu

The Institute of Statistical Mathematics  Tokyo

fukumizu@ism.ac.jp

Francesco Dinuzzo

MPI for Intelligent Systems  T¨ubingen
fdinuzzo@tuebingen.mpg.de

Bernhard Sch¨olkopf

MPI for Intelligent Systems  T¨ubingen

bs@tuebingen.mpg.de

Abstract

This paper presents a kernel-based discriminative learning framework on prob-
ability measures. Rather than relying on large collections of vectorial training
examples  our framework learns using a collection of probability distributions
that have been constructed to meaningfully represent training data. By represent-
ing these probability distributions as mean embeddings in the reproducing kernel
Hilbert space (RKHS)  we are able to apply many standard kernel-based learning
techniques in straightforward fashion. To accomplish this  we construct a gener-
alization of the support vector machine (SVM) called a support measure machine
(SMM). Our analyses of SMMs provides several insights into their relationship
to traditional SVMs. Based on such insights  we propose a ﬂexible SVM (Flex-
SVM) that places different kernel functions on each training example. Experi-
mental results on both synthetic and real-world data demonstrate the effectiveness
of our proposed framework.

1

Introduction

Discriminative learning algorithms are typically trained from large collections of vectorial training
examples. In many classical learning problems  however  it is arguably more appropriate to represent
training data not as individual data points  but as probability distributions. There are  in fact  multiple
reasons why probability distributions may be preferable.

Firstly  uncertain or missing data naturally arises in many applications. For example  gene expres-
sion data obtained from the microarray experiments are known to be very noisy due to various
sources of variabilities [1]. In order to reduce uncertainty  and to allow for estimates of conﬁdence
levels  experiments are often replicated. Unfortunately  the feasibility of replicating the microarray
experiments is often inhibited by cost constraints  as well as the amount of available mRNA. To cope
with experimental uncertainty given a limited amount of data  it is natural to represent each array as
a probability distribution that has been designed to approximate the variability of gene expressions
across slides.

Probability distributions may be equally appropriate given an abundance of training data. In data-
rich disciplines such as neuroinformatics  climate informatics  and astronomy  a high throughput
experiment can easily generate a huge amount of data  leading to signiﬁcant computational chal-
lenges in both time and space. Instead of scaling up one’s learning algorithms  one can scale down
one’s dataset by constructing a smaller collection of distributions which represents groups of similar
samples. Besides computational efﬁciency  aggregate statistics can potentially incorporate higher-
level information that represents the collective behavior of multiple data points.

1

Previous attempts have been made to learn from distributions by creating positive deﬁnite (p.d.)
kernels on probability measures. In [2]  the probability product kernel (PPK) was proposed as a
generalized inner product between two input objects  which is in fact closely related to well-known
kernels such as the Bhattacharyya kernel [3] and the exponential symmetrized Kullback-Leibler
(KL) divergence [4]. In [5]  an extension of a two-parameter family of Hilbertian metrics of Topsøe
was used to deﬁne Hilbertian kernels on probability measures. In [6]  the semi-group kernels were
designed for objects with additive semi-group structure such as positive measures. Recently  [7] in-
troduced nonextensive information theoretic kernels on probability measures based on new Jensen-
Shannon-type divergences. Although these kernels have proven successful in many applications 
they are designed speciﬁcally for certain properties of distributions and application domains. More-
over  there has been no attempt in making a connection to the kernels on corresponding input spaces.

The contributions of this paper can be summarized as follows. First  we prove the representer the-
orem for a regularization framework over the space of probability distributions  which is a gener-
alization of regularization over the input space on which the distributions are deﬁned (Section 2).
Second  a family of positive deﬁnite kernels on distributions is introduced (Section 3). Based on
such kernels  a learning algorithm on probability measures called support measure machine (SMM)
is proposed. An SVM on the input space is provably a special case of the SMM. Third  the paper
presents the relations between sample-based and distribution-based methods (Section 4). If the dis-
tributions depend only on the locations in the input space  the SMM particularly reduces to a more
ﬂexible SVM that places different kernels on each data point.

2 Regularization on probability distributions

Given a non-empty set X   let P denote the set of all probability measures P on a measurable
space (X   A)  where A is a σ-algebra of subsets of X . The goal of this work is to learn a function
h : P → Y given a set of example pairs {(Pi  yi)}m
i=1  where Pi ∈ P and yi ∈ Y. In other words 
we consider a supervised setting in which input training examples are probability distributions. In
this paper  we focus on the binary classiﬁcation problem  i.e.  Y = {+1  −1}.
In order to learn from distributions  we employ a compact representation that not only preserves
necessary information of individual distributions  but also permits efﬁcient computations. That is 
we adopt a Hilbert space embedding to represent the distribution as a mean function in an RKHS
[8  9]. Formally  let H denote an RKHS of functions f : X → R  endowed with a reproducing
kernel k : X × X → R. The mean map from P into H is deﬁned as

µ : P → H  P 7−→ZX

k(x  ·) dP(x) .

(1)

We assume that k(x  ·) is bounded for any x ∈ X . It can be shown that  if k is characteristic  the map
(1) is injective  i.e.  all the information about the distribution is preserved [10]. For any P  letting
µP = µ(P)  we have the reproducing property

(2)
That is  we can see the mean embedding µP as a feature map associated with the kernel K :
P × P → R  deﬁned as K(P  Q) = hµP  µQiH. Since supx kk(x  ·)kH < ∞  it also follows

EP[f ] = hµP  f iH  ∀f ∈ H .

that K(P  Q) = RR hk(x  ·)  k(z  ·)iH dP(x) dQ(z) = RR k(x  z) dP(x) dQ(z)  where the second

equality follows from the reproducing property of H. It is immediate that K is a p.d. kernel on P.
The following theorem shows that optimal solutions of a suitable class of regularization problems
involving distributions can be expressed as a ﬁnite linear combination of mean embeddings.
Theorem 1. Given training examples (Pi  yi) ∈ P × R  i = 1  . . .   m  a strictly monotonically
increasing function Ω : [0  +∞) → R  and a loss function ℓ : (P × R2)m → R ∪ {+∞}  any
f ∈ H minimizing the regularized risk functional

ℓ (P1  y1  EP1 [f ]  . . .   Pm  ym  EPm [f ]) + Ω (kf kH)

(3)

admits a representation of the form f =Pm

i=1 αiµPi for some αi ∈ R  i = 1  . . .   m.

Theorem 1 clearly indicates how each distribution contributes to the minimizer of (3). Roughly
speaking  the coefﬁcients αi controls the contribution of the distributions through the mean em-
beddings µPi. Furthermore  if we restrict P to a class of Dirac measures δx on X and consider

2

recovered as a particular case (see also [12] for more general results on representer theorem).

i=1  the functional (3) reduces to the usual regularization functional [11]
i=1 αik(xi  ·). Therefore  the standard representer theorem is

the training set {(δxi   yi)}m

and the solution reduces to f = Pm

Note that  on the one hand  the minimization problem (3) is different from minimizing the functional
EP1 . . . EPmℓ(x1  y1  f (x1)  . . .   xm  ym  f (xm))+Ω(kf kH) for the special case of the additive loss
ℓ. Therefore  the solution of our regularization problem is different from what one would get in the
limit by training on an inﬁnitely many points sampled from P1  . . .   Pm. On the other hand  it is
also different from minimizing the functional ℓ(M1  y1  f (M1)  . . .   Mm  ym  f (Mm)) + Ω(kf kH)
where Mi = Ex∼Pi [x]. In a sense  our framework is something in between.

3 Kernels on probability distributions

As the map (1) is linear in P  optimizing the functional (3) amounts to ﬁnding a function in H that

approximate well functions from P to R in the function class F   {P → RX g dP | P ∈ P  g ∈

C(X )} where C(X ) is a class of bounded continuous functions on X . Since δx ∈ P for any x ∈ X  
it follows that C(X ) ⊂ F ⊂ C(P) where C(P) is a class of bounded continuous functions on P
endowed with the topology of weak convergence and the associated Borel σ-algebra. The following
lemma states the relation between the RKHS H induced by the kernel k and the function class F.
Lemma 2. Assuming that X is compact  the RKHS H induced by a kernel k is dense in F if k
is universal  i.e.  for every function F ∈ F and every ε > 0 there exists a function g ∈ H with

supP∈P|F (P) −R g dP| ≤ ε.

Proof. Assume that k is universal. Then  for every function f ∈ C(X ) and every ε > 0 there exists a
function g ∈ H induced by k with supx∈X |f (x)−g(x)| ≤ ε [13]. Hence  by linearity of F  for every

F ∈ F and every ε > 0 there exists a function h ∈ H such that supP∈P|F (P) −R h dP| ≤ ε. (cid:4)

Nonlinear kernels on P can be deﬁned in an analogous way to nonlinear kernels on X   by treating
mean embeddings µP of P ∈ P as its feature representation. First  assume that the map (1) is
injective and let h·  ·iP be an inner product on P. By linearity  we have hP  QiP = hµP  µQiH (cf.
[8] for more details). Then  the nonlinear kernels on P can be deﬁned as K(P  Q) = κ(µP  µQ) =
hψ(µP)  ψ(µQ)iHκ where κ is a p.d. kernel. As a result  many standard nonlinear kernels on X can
be used to deﬁne nonlinear kernels on P as long as the kernel evaluation depends entirely on the in-
ner product hµP  µQiH  e.g.  K(P  Q) = (hµP  µQiH + c)d. Although requiring more computational
effort  their practical use is simple and ﬂexible. Speciﬁcally  the notion of p.d. kernels on distri-
butions proposed in this work is so generic that standard kernel functions can be reused to derive
kernels on distributions that are different from many other kernel functions proposed speciﬁcally for
certain distributions.
It has been recently proved that the Gaussian RBF kernel given by K(P  Q) = exp(− γ
2 kµP −
H)  ∀P  Q ∈ P is universal w.r.t C(P) given that X is compact and the map µ is injective
µQk2
[14]. Despite its success in real-world applications  the theory of kernel-based classiﬁers beyond
the input space X ⊂ Rd  as also mentioned by [14]  is still incomplete. It is therefore of theoretical
interest to consider more general classes of universal kernels on probability distributions.

3.1 Support measure machines

This subsection extends SVMs to deal with probability distributions  leading to support measure
machines (SMMs). In its general form  an SMM amounts to solving an SVM problem with the
expected kernel K(P  Q) = Ex∼P z∼Q[k(x  z)]. This kernel can be computed in closed-form for
certain classes of distributions and kernels k. Examples are given in Table 1.
Alternatively  one can approximate the kernel K(P  Q) by the empirical estimate:

Kemp(bPn bQm) =

1

n · m

nXi=1

mXj=1

k(xi  zj)

(4)

where bPn and bQm are empirical distributions of P and Q given random samples {xi}n

i=1 and
j=1  respectively. A ﬁnite sample of size m from a distribution P sufﬁces (with high probability)

{zj}m

3

Table 1: the analytic forms of expected kernels for different choices of kernels and distributions.

Distributions

Arbitrary P(m; Σ)
Gaussian N (m; Σ) Gaussian RBF exp(− γ

Embedding kernel k(x  y)
Linear hx  yi

Gaussian N (m; Σ)
Gaussian N (m; Σ)

Polynomial degree 2 (hx  yi + 1)2
Polynomial degree 3 (hx  yi + 1)3

2 kx − yk2)

2 (mi − mj)T(Σi + Σj + γ−1I)−1(mi − mj))

1

i mj + δijtr Σi

K(Pi  Pj) = hµPi   µPj iH
mT
exp(− 1
/|γΣi + γΣj + I|
(hmi  mji + 1)2 + tr ΣiΣj + mT
(hmi  mji + 1)3 + 6mT
i ΣiΣjmj
+3(hmi  mji + 1)(tr ΣiΣj + mT

2

i Σjmi + mT

j Σimj

i Σjmi + mT

j Σimj)

to compute an approximation within an error of O(m− 1
2 ). Instead  if the sample set is sufﬁciently
large  one may choose to approximate the true distribution by simpler probabilistic models  e.g.  a
mixture of Gaussians model  and choose a kernel k whose expected value admits an analytic form.
Storing only the parameters of probabilistic models may save some space compared to storing all
data points.
Note that the standard SVM feature map φ(x) is usually nonlinear in x  whereas µP is linear in P.
Thus  for an SMM  the ﬁrst level kernel k is used to obtain a vectorial representation of the measures 
and the second level kernel K allows for a nonlinear algorithm on distributions. For clarity  we will
refer to k and K as the embedding kernel and the level-2 kernel  respectively

4 Theoretical analyses

This section presents key theoretical aspects of the proposed framework  which reveal important
connection between kernel-based learning algorithms on the space of distributions and on the input
space on which they are deﬁned.

4.1 Risk deviation bound

1

i=1 drawn i.i.d.

ℓ(yi  f (xij)) based

from some unknown probability distribu-
Given a training sample {(Pi  yi)}m
tion P on P × Y  a loss function ℓ : R × R → R  and a function class Λ  the goal of
statistical learning is to ﬁnd the function f ∈ Λ that minimizes the expected risk functional

R(f ) = RPRX ℓ(y  f (x)) dP(x) dP(P  y). Since P is unknown  the empirical risk Remp(f ) =
i=1RX ℓ(yi  f (x)) dPi(x) based on the training sample is considered instead. Furthermore 
mPm

the risk functional can be simpliﬁed further by considering 1
on n samples xij drawn from each Pi.
Our framework  on the other hand  alleviates the problem by minimizing the risk functional

i=1Pxij ∼Pi

m·nPm

emp(f ) = 1

mPm

Rµ(f ) = RP ℓ(y  EP[f (x)]) dP(P  y) for f ∈ H with corresponding empirical risk functional

Rµ
i=1 ℓ(yi  EPi [f (x)]) (cf. the discussion at the end of Section 2). It is often easier
to optimize Rµ
emp(f ) as the expectation can be computed exactly for certain choices of Pi and H.
Moreover  for universal H  this simpliﬁcation preserves all information of the distributions. Never-
theless  there is still a loss of information due to the loss function ℓ.
Due to the i.i.d. assumption  the analysis of the difference between R and Rµ can be simpliﬁed
w.l.o.g. to the analysis of the difference between EP[ℓ(y  f (x))] and ℓ(y  EP[f (x)]) for a particular
distribution P ∈ P. The theorem below provides a bound on the difference between EP[ℓ(y  f (x))]
and ℓ(y  EP[f (x)]).
Theorem 3. Given an arbitrary probability distribution P with variance σ2  a Lipschitz continu-
ous function f : R → R with constant Cf   an arbitrary loss function ℓ : R × R → R that is
Lipschitz continuous in the second argument with constant Cℓ  it follows that |Ex∼P[ℓ(y  f (x))] −
ℓ(y  Ex∼P[f (x)])| ≤ 2CℓCf σ for any y ∈ R.

Theorem 3 indicates that if the random variable x is concentrated around its mean and the func-
tion f and ℓ are well-behaved  i.e.  Lipschitz continuous  then the loss deviation |EP[ℓ(y  f (x))] −
ℓ(y  EP[f (x)])| will be small. As a result  if this holds for any distribution Pi in the training set
{(Pi  yi)}m

i=1  the true risk deviation |R − Rµ| is also expected to be small.

4

4.2 Flexible support vector machines

It turns out that  for certain choices of distributions P  the linear SMM trained using {(Pi  yi)}m
i=1
is equivalent to an SVM trained using some samples {(xi  yi)}m
i=1 with an appropriate choice of
kernel function.

Lemma 4. Let k(x  z) be a bounded p.d. kernel on a measure space such thatRR k(x  z)2 dx dz <
∞  and g(x  ˜x) be a square integrable function such that R g(x  ˜x) d˜x < ∞ for all x. Given
RR k(˜x  ˜z)g(x  ˜x)g(z  ˜z) d˜x d˜z.

i=1 where each Pi is assumed to have a density given by g(xi  x)  the lin-
i=1 with kernel Kg(x  z) =

a sample {(Pi  yi)}m
ear SMM is equivalent to the SVM on the training sample {(xi  yi)}m

Note that the important assumption for this equivalence is that the distributions Pi differ only in their
location in the parameter space. This need not be the case in all possible applications of SMMs.

Furthermore  we have Kg(x  z) = (cid:10)R k(˜x  ·)g(x  ˜x) d˜x R k(˜z  ·)g(z  ˜z) d˜z(cid:11)H. Thus  it is clear that

the feature map of x depends not only on the kernel k  but also on the density g(x  ˜x). Consequently 
by virtue of Lemma 4  the kernel Kg allows the SVM to place different kernels at each data point.
We call this algorithm a ﬂexible SVM (Flex-SVM).
Consider for example the linear SMM with Gaussian distributions N (x1; σ2
m · I)
and Gaussian RBF kernel kσ2 with bandwidth parameter σ. The convolution theorem of Gaussian
distributions implies that this SMM is equivalent to a ﬂexible SVM that places a data-dependent
kernel kσ2+2σ2

(xi  ·) on training example xi  i.e.  a Gaussian RBF kernel with larger bandwidth.

1 · I)  . . .   N (xm; σ2

i

5 Related works

The kernel K(P  Q) = hµP  µQiH is in fact a special case of the Hilbertian metric [5]  with the
associated kernel K(P  Q) = EP Q[k(x  ˜x)]  and a generative mean map kernel (GMMK) proposed
by [15]. In the GMMK  the kernel between two objects x and y is deﬁned via ˆpx and ˆpy  which are
estimated probabilistic models of x and y  respectively. That is  a probabilistic model ˆpx is learned
for each example and used as a surrogate to construct the kernel between those examples. The idea
of surrogate kernels has also been adopted by the Probability Product Kernel (PPK) [2]. In this case 

we have Kρ(p  p′) = RX p(x)ρp′(x)ρ dx  which has been shown to be a special case of GMMK

when ρ = 1 [15]. Consequently  GMMK  PPK with ρ = 1  and our linear kernels are equivalent
when the embedding kernel is k(x  x′) = δ(x − x′). More recently  the empirical kernel (4) was
employed in an unsupervised way for multi-task learning to generalize to a previously unseen task
[16].
the regularized
functional (3)) and the kernel is not restricted to only the empirical kernel.

In contrast  we treat the probability distributions in a supervised way (cf.

The use of expected kernels in dealing with the uncertainty in the input data has a connection to
robust SVMs. For instance  a generalized form of the SVM in [17] incorporates the probabilistic
uncertainty into the maximization of the margin. This results in a second-order cone programming
(SOCP) that generalizes the standard SVM. In SOCP  one needs to specify the parameter τi that
reﬂects the probability of correctly classifying the ith training example. The parameter τi is therefore
closely related to the parameter σi  which speciﬁes the variance of the distribution centered at the
ith example. [18] showed the equivalence between SVMs using expected kernels and SOCP when
τi = 0. When τi > 0  the mean and covariance of missing kernel entries have to be estimated
explicitly  making the SOCP more involved for nonlinear kernels. Although achieving comparable
performance to the standard SVM with expected kernels  the SOCP requires a more computationally
extensive SOCP solver  as opposed to simple quadratic programming (QP).

6 Experimental results

In the experiments  we primarily consider three different learning algorithms: i) SVM is considered
as a baseline algorithm. ii) Augmented SVM (ASVM) is an SVM trained on augmented samples
drawn according to the distributions {Pi}m
i=1. The same number of examples are drawn from each
distribution. iii) SMM is distribution-based method that can be applied directly on the distributions1.

1We used the LIBSVM implementation.

5

 100

)

%
(
y
c
a
r
u
c
c
A

 80

 60

 40

 20

 0

 

 

1
F
B
R
g
n
d
d
e
b
m
E

i

Embedding RBF 1
Level-2 RBF
Embedding RBF 2
Level-2 Poly

 

 

2
F
B
R
g
n
d
d
e
b
m
E

i

 0 1 2 3 4 5 6 7 8

Parameters

Level-2 POLY

Level-2 RBF

 100
 90
 80
 70
 60
 50
 40

(a) decision boundaries.

(b) sensitivity of kernel parameters

Figure 1: (a) the decision boundaries of SVM  ASVM  and SMM. (b) the heatmap plots of average
accuracies of SMM over 30 experiments using POLY-RBF (center) and RBF-RBF (right) kernel
combinations with the plots of average accuracies at different parameter values (left).

Table 2: accuracies (%) of SMM on synthetic data with different combinations of embedding and
level-2 kernels.

LIN

POLY2

Embedding kernels

POLY3

RBF

URBF

2
-
l
e
v
e
L

l
e
n
r
e
k

s LIN

85.20±2.20
POLY 83.95±2.11
RBF
87.80±1.96

81.04±3.11
81.34±1.21
73.12±3.29

81.10±2.76
82.66±1.75
78.28±2.19

87.74±2.19
88.06±1.73
89.65±1.37

85.39±2.56
86.84±1.51
86.86±1.88

6.1 Synthetic data

Firstly  we conducted a basic experiment that illustrates a fundamental difference between SVM 
ASVM  and SMM. A binary classiﬁcation problem of 7 Gaussian distributions with different means
and covariances was considered. We trained the SVM using only the means of the distributions 
ASVM with 30 virtual examples generated from each distribution  and SMM using distributions as
training examples. A Gaussian RBF kernel with γ = 0.25 was used for all algorithms.
Figure 1a shows the resulting decision boundaries. Having been trained only on means of the dis-
tributions  the SVM classiﬁer tends to overemphasize the regions with high densities and underrep-
resent the lower density regions. In contrast  the ASVM is more expensive and sensitive to outliers 
especially when learning on heavy-tailed distributions. The SMM treats each distribution as a train-
ing example and implicitly incorporates properties of the distributions  i.e.  means and covariances 
into the classiﬁer. Note that the SVM can be trained to achieve a similar result to the SMM by
choosing an appropriate value for γ (cf. Lemma 4). Nevertheless  this becomes more difﬁcult if the
training distributions are  for example  nonisotropic and have different covariance matrices.

Secondly  we evaluate the performance of the SMM for different combinations of embedding and
level-2 kernels. Two classes of synthetic Gaussian distributions on R10 were generated. The mean
parameters of the positive and negative distributions are normally distributed with means m+ =
(1  . . .   1) and m− = (2  . . .   2) and identical covariance matrix Σ = 0.5 · I10  respectively. The
covariance matrix for each distribution is generated according to two Wishart distributions with
covariance matrices given by Σ+ = 0.6 · I10 and Σ− = 1.2 · I10 with 10 degrees of freedom.
The training set consists of 500 distributions from the positive class and 500 distributions from the
negative class. The test set consists of 200 distributions with the same class proportion.

The kernels used in the experiment include linear kernel (LIN)  polynomial kernel of degree 2
(POLY2)  polynomial kernel of degree 3 (POLY3)  unnormalized Gaussian RBF kernel (RBF)  and
normalized Gaussian RBF kernel (NRBF). To ﬁx parameter values of both kernel functions and
SMM  10-fold cross-validation (10-CV) is performed on a parameter grid  C ∈ {2−3  2−2  . . .   27}
for SMM  bandwidth parameter γ ∈ {10−3  10−2  . . .   102} for Gaussian RBF kernels  and degree
parameter d ∈ {2  3  4  5  6} for polynomial kernels. The average accuracy and ±1 standard de-
viation for all kernel combinations over 30 repetitions are reported in Table 2. Moreover  we also
investigate the sensitivity of kernel parameters for two kernel combinations: RBF-RBF and POLY-
RBF. In this case  we consider the bandwidth parameter γ = {10−3  10−2  . . .   103} for Gaussian

6

1 vs 8

3 vs 4

3 vs 8

6 vs 9

 100

 95

 90
 100

 95

 100

)

%

(
 
y
c
a
r
u
c
c
A

 100

 95

 100

 95

 90
 100

 95

 95

 10

 20

 30

 10

 100
 95
 90
 85
 100
 95
 90
 85

 100
 95
 90
 85

 20
Number of virtual examples

 20

 30

 10

 100

 95

 100

 95

 90
 100
 90
 80
 70

g
n

i
l

a
c
S

n
o

i
t

l

a
s
n
a
r
T

n
o

i
t

t

a
o
R

 30

 10

 20

 30

Figure 2: the performance of SVM  ASVM  and SMM
algorithms on handwritten digits constructed using three
basic transformations.

t
s
o
c
 
.

p
m
o
c
 

e
v
i
t

l

a
e
R

103
102
101
100
10-1

SMM

ASVM

2000

4000

6000

Number of virtual examples

Figure 3: relative computational cost of
ASVM and SMM (baseline: SMM with
2000 virtual examples).

 70

)

 65

 60

 55

%

(
 
y
c
a
r
u
c
c
A

 50

pLSA

SVM LSMM NLSMM

Figure 4: accuracies of four different
techniques for natural scene categoriza-
tion.

RBF kernels and degree parameter d = {2  3  . . .   8} for polynomial kernels. Figure 1b depicts the
accuracy values and average accuracies for considered kernel functions.

Table 2 indicates that both embedding and level-2 kernels are important for the performance of the
classiﬁer. The embedding kernels tend to have more impact on the predictive performance compared
to the level-2 kernels. This conclusion also coincides with the results depicted in Figure 1b.

6.2 Handwritten digit recognition

In this section  the proposed framework is applied to distributions over equivalence classes of images
that are invariant to basic transformations  namely  scaling  translation  and rotation. We consider
the handwritten digits obtained from the USPS dataset. For each 16 × 16 image  the distribution
over the equivalence class of the transformations is determined by a prior on parameters associated
with such transformations. Scaling and translation are parametrized by the scale factors (sx  sy) and
displacements (tx  ty) along the x and y axes  respectively. The rotation is parametrized by an angle
θ. We adopt Gaussian distributions as prior distributions  including N ([1  1]  0.1·I2)  N ([0  0]  5·I2) 
and N (0; π). For each image  the virtual examples are obtained by sampling parameter values from
the distribution and applying the transformation accordingly.

Experiments are categorized into simple and difﬁcult binary classiﬁcation tasks. The former consists
of classifying digit 1 against digit 8 and digit 3 against digit 4. The latter considers classifying digit 3
against digit 8 and digit 6 against digit 9. The initial dataset for each task is constructed by randomly
selecting 100 examples from each class. Then  for each example in the initial dataset  we generate
10  20  and 30 virtual examples using the aforementioned transformations to construct virtual data
sets consisting of 2 000  4 000  and 6 000 examples  respectively. One third of examples in the
initial dataset are used as a test set. The original examples are excluded from the virtual datasets.
The virtual examples are normalized such that their feature values are in [0  1]. Then  to reduce
computational cost  principle component analysis (PCA) is performed to reduce the dimensionality
to 16. We compare the SVM on the initial dataset  the ASVM on the virtual datasets  and the SMM.
For SVM and ASVM  the Gaussian RBF kernel is used. For SMM  we employ the empirical kernel
(4) with Gaussian RBF kernel as a base kernel. The parameters of the algorithms are ﬁxed by 10-CV
over parameters C ∈ {2−3  2−2  . . .   27} and γ ∈ {0.01  0.1  1}.
The results depicted in Figure 2 clearly demonstrate the beneﬁts of learning directly from the equiv-
alence classes of digits under basic transformations2. In most cases  the SMM outperforms both the
SVM and the ASVM as the number of virtual examples increases. Moreover  Figure 3 shows the
beneﬁt of the SMM over the ASVM in term of computational cost3.

2While the reported results were obtained using virtual examples with Gaussian parameter distributions

(Sec. 6.2)  we got similar results using uniform distributions.

3The evaluation was made on a 64-bit desktop computer with Intel R(cid:13) CoreTM 2 Duo CPU E8400 at

3.00GHz×2 and 4GB of memory.

7

6.3 Natural scene categorization

This section illustrates beneﬁts of the nonlinear kernels between distributions for learning natural
scene categories in which the bag-of-word (BoW) representation is used to represent images in the
dataset. Each image is represented as a collection of local patches  each being a codeword from a
large vocabulary of codewords called codebook. Standard BoW representations encode each image
as a histogram that enumerates the occurrence probability of local patches detected in the image w.r.t.
those in the codebook. On the other hand  our setting represents each image as a distribution over
these codewords. Thus  images of different scenes tends to generate distinct set of patches. Based
on this representation  both the histogram and the local patches can be used in our framework.

We use the dataset presented in [19]. According to their results  most errors occurs among the four
indoor categories (830 images)  namely  bedroom (174 images)  living room (289 images)  kitchen
(151 images)  and ofﬁce (216 images). Therefore  we will focus on these four categories. For each
category  we split the dataset randomly into two separate sets of images  100 for training and the rest
for testing.

A codebook is formed from the training images of all categories. Firstly  interesting keypoints in the
image are randomly detected. Local patches are then generated accordingly. After patch detection 
each patch is transformed into a 128-dim SIFT vector [20]. Given the collection of detected patches 
K-means clustering is performed over all local patches. Codewords are then deﬁned as the centers
of the learned clusters. Then  each patch in an image is mapped to a codeword and the image can
be represented by the histogram of the codewords. In addition  we also have an M × 128 matrix of
SIFT vectors where M is the number of codewords.
We compare the performance of a Probabilistic Latent Semantic Analysis (pLSA) with the stan-
dard BoW representation  SVM  linear SMM (LSMM)  and nonlinear SMM (NLSMM). For
SMM  we use the empirical embedding kernel with Gaussian RBF base kernel k: K(hi  hj) =
s=1 hi(cr)hj(cs)k(cr  cs) where hi is the histogram of the ith image and cr is the rth
SIFT vector. A Gaussian RBF kernel is also used as the level-2 kernel for nonlinear SMM. For
the SVM  we adopt a Gaussian RBF kernel with χ2-distance between the histograms [21]  i.e. 
. The parameters of
the algorithms are ﬁxed by 10-CV over parameters C ∈ {2−3  2−2  . . .   27} and γ ∈ {0.01  0.1  1}.
For NLSMM  we use the best γ of LSMM in the base kernel and perform 10-CV to choose γ param-
eter only for the level-2 kernel. To deal with multiple categories  we adopt the pairwise approach
and voting scheme to categorize test images. The results in Figure 4 illustrate the beneﬁt of the
distribution-based framework. Understanding the context of a complex scene is challenging. Em-
ploying distribution-based methods provides an elegant way of utilizing higher-order statistics in
natural images that could not be captured by traditional sample-based methods.

PM
r=1PM
K(hi  hj) = exp(cid:0)−γχ2(hi  hj)(cid:1) where χ2(hi  hj) = PM

(hi(cr)−hj (cr))2

r=1

hi(cr)+hj (cr)

7 Conclusions

This paper proposes a method for kernel-based discriminative learning on probability distributions.
The trick is to embed distributions into an RKHS  resulting in a simple and efﬁcient learning al-
gorithm on distributions. A family of linear and nonlinear kernels on distributions allows one to
ﬂexibly choose the kernel function that is suitable for the problems at hand. Our analyses provide
insights into the relations between distribution-based methods and traditional sample-based meth-
ods  particularly the ﬂexible SVM that allows the SVM to place different kernels on each training
example. The experimental results illustrate the beneﬁts of learning from a pool of distributions 
compared to a pool of examples  both on synthetic and real-world data.

Acknowledgments

KM would like to thank Zoubin Gharamani  Arthur Gretton  Christian Walder  and Philipp Hennig
for a fruitful discussion. We also thank all three insightful reviewers for their invaluable comments.

8

References

[1] Y. H. Yang and T. Speed. Design issues for cDNA microarray experiments. Nat. Rev. Genet. 

3(8):579–588  2002.

[2] T. Jebara  R. Kondor  A. Howard  K. Bennett  and N. Cesa-bianchi. Probability product kernels.

Journal of Machine Learning Research  5:819–844  2004.

[3] A. Bhattacharyya. On a measure of divergence between two statistical populations deﬁned by

their probability distributions. Bull. Calcutta Math Soc.  1943.

[4] P. J. Moreno  P. P. Ho  and N. Vasconcelos. A Kullback-Leibler divergence based kernel
In Proceedings of Advances in Neural

for SVM classiﬁcation in multimedia applications.
Information Processing Systems. MIT Press  2004.

[5] M. Hein and O. Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability.
In Proceedings of The 12th International Conference on Artiﬁcial Intelligence and Statistics 
pages 136–143  2005.

[6] M. Cuturi  K. Fukumizu  and J-P. Vert. Semigroup kernels on measures. Journal of Machine

Learning Research  6:1169–1198  2005.

[7] Andr´e F. T. Martins  Noah A. Smith  Eric P. Xing  Pedro M. Q. Aguiar  and M´ario A. T.
Figueiredo. Nonextensive information theoretic kernels on measures. Journal of Machine
Learning Research  10:935–975  2009.

[8] A. Berlinet and Thomas C. Agnan. Reproducing kernel Hilbert spaces in probability and

statistics. Kluwer Academic Publishers  2004.

[9] A. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A hilbert space embedding for distributions.
In Proceedings of the 18th International Conference on Algorithmic Learning Theory  pages
13–31. Springer-Verlag  2007.

[10] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and Gert R. G. Lanckriet.
Hilbert space embeddings and metrics on probability measures. Journal of Machine Learn-
ing Research  99:1517–1561  2010.

[11] B. Sch¨olkopf  R. Herbrich  and A. J. Smola. A generalized representer theorem. In COLT

’01/EuroCOLT ’01  pages 416–426. Springer-Verlag  2001.

[12] F. Dinuzzo and B. Sch¨olkopf. The representer theorem for Hilbert spaces: a necessary and
sufﬁcient condition. In Advances in Neural Information Processing Systems 25  pages 189–
196. 2012.

[13] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines.

Journal of Machine Learning Research  2:67–93  2001.

[14] A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In Proceed-

ings of Advances in Neural Information Processing Systems  pages 406–414. 2010.

[15] N. A. Mehta and A. G. Gray. Generative and latent mean map kernels. CoRR  abs/1005.0188 

2010.

[16] G. Blanchard  G. Lee  and C. Scott. Generalizing from several related classiﬁcation tasks to
a new unlabeled sample. In Advances in Neural Information Processing Systems 24  pages
2178–2186. 2011.

[17] P. K. Shivaswamy  C. Bhattacharyya  and A. J. Smola. Second order cone programming ap-
proaches for handling missing and uncertain data. Journal of Machine Learning Research 
7:1283–1314  2006.

[18] H.S. Anderson and M.R. Gupta. Expected kernel for missing features in support vector ma-

chines. In Statistical Signal Processing Workshop  pages 285–288  2011.

[19] L. Fei-fei. A bayesian hierarchical model for learning natural scene categories. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 524–531 
2005.

[20] D. G. Lowe. Object recognition from local scale-invariant features. In Proceedings of the In-
ternational Conference on Computer Vision  pages 1150–1157  Washington  DC  USA  1999.
[21] A. Vedaldi  V. Gulshan  M. Varma  and A. Zisserman. Multiple kernels for object detection.

In Proceedings of the International Conference on Computer Vision  pages 606–613  2009.

9

,Richard Nock