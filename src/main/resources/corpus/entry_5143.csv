2010,Spatial and anatomical regularization of SVM for brain image analysis,Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover  when the kernel is linear  SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However  the features' spatial distribution is not taken into account. As a consequence  the optimal margin hyperplane is often scattered and lacks spatial coherence  making its anatomical interpretation difficult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classification of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer's disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.,Spatial and anatomical regularization of SVM

for brain image analysis

R´emi Cuingnet

CRICM (UPMC/Inserm/CNRS)  Paris  France

Inserm - LIF (UMR S 678)  Paris  France
remi.cuingnet@imed.jussieu.fr

Marie Chupin

CRICM  Paris  France

marie.chupin@upmc.fr

Habib Benali

Inserm - LIF  Paris  France

habib.benali@imed.jussieu.fr

Olivier Colliot

CRICM  Paris  France

olivier.colliot@upmc.fr

Abstract

Support vector machines (SVM) are increasingly used in brain image analyses
since they allow capturing complex multivariate relationships in the data. More-
over  when the kernel is linear  SVMs can be used to localize spatial patterns
of discrimination between two groups of subjects. However  the features’ spa-
tial distribution is not taken into account. As a consequence  the optimal margin
hyperplane is often scattered and lacks spatial coherence  making its anatomical
interpretation difﬁcult. This paper introduces a framework to spatially regularize
SVM for brain image analysis. We show that Laplacian regularization provides a
ﬂexible framework to integrate various types of constraints and can be applied to
both cortical surfaces and 3D brain images. The proposed framework is applied
to the classiﬁcation of MR images based on gray matter concentration maps and
cortical thickness measures from 30 patients with Alzheimer’s disease and 30 el-
derly controls. The results demonstrate that the proposed method enables natural
spatial and anatomical regularization of the classiﬁer.

1

Introduction

Brain image analyses have widely relied on univariate voxel-wise analyses  such as voxel-based
morphometry (VBM) for structural MRI [1]. In such analyses  brain images are ﬁrst spatially reg-
istered to a common stereotaxic space  and then mass univariate statistical tests are performed in
each voxel to detect signiﬁcant group differences. However  the sensitivity of theses approaches
is limited when the differences are spatially complex and involve a combination of different vox-
els or brain structures [2]. Recently  there has been a growing interest in support vector machines
(SVM) methods [3  4] to overcome the limits of these univariate analyses. Theses approaches allow
capturing complex multivariate relationships in the data and have been successfully applied to the
individual classiﬁcation of a variety of neurological conditions [5  6  7  8]. Moreover  the output of
the SVM can also be analyzed to localize spatial patterns of discrimination  for example by drawing
the coefﬁcients of the optimal margin hyperplane (OMH) – which  in the case of a linear SVM  live
in the same space as the MRI data [7  8]. However  one of the problems with analyzing directly
the OMH coefﬁcients is that the corresponding maps are scattered and lack spatial coherence. This
makes it difﬁcult to give a meaningful interpretation of the maps  for example to localize the brain
regions altered by a given pathology.
In this paper  we address this issue by proposing a framework to introduce spatial consistency into
SVMs by using regularization operators. Section 2 provides some background information on SVMs

1

and regularization operators. We then show that the regularization operator framework provides a
ﬂexible approach to model different types of proximity (section 3). Section 4 presents the ﬁrst type
of regularization  which models spatial proximity  i.e.
two features are close if they are spatially
close. We then present in section 5 a more complex type of constraint  called anatomical proxim-
ity. In the latter case  two features are considered close if they belong to the same brain network;
for instance two voxels are close if they belong to the same anatomical or functional region or if
they are anatomically or functionally connected (based on fMRI networks or white matter tracts).
Finally  in section 6  the proposed framework is illustrated on the analysis of MR images using gray
matter concentration maps and cortical thickness measures from 30 patients with AD and 30 elderly
controls from the ADNI database (www.adni-info.org).

2 Priors in SVM

In this section  we ﬁrst describe the neuroimaging data that we consider in this paper. Then  after
some background on SVMs and on how to add prior knowledge in SVMs  we describe the frame-
work of regularization operators.

2.1 Brain imaging data

In this contribution  we consider any feature computed either at each voxel of a 3D brain image
or at any vertex of the cortical surface. Typically  for anatomical studies  the features could be
tissue concentration maps such as gray matter (GM) or white matter (WM) for the 3D case or
cortical thickness maps for the surface case. The proposed methods are also applicable to functional
or diffusion weighted MRI. We further assume that 3D images or cortical surfaces were spatially
normalized to a common stereotaxic space (e.g.
[9]) as in many group studies or classiﬁcation
methods [5  6  7  8  10].
Let V be the domain of the 3D images or surfaces. v will denote an element of V (i.e. a voxel or a
vertex). Thus  X = RV  together with the canonical dot product will be the input space.
Let xs ∈ X be the data of a given subject s. In the case of 3D images  xs can be considered in two
different ways: (i) as an element of Rd where d denotes the number of voxels  (ii) as a real-valued
function deﬁned on a compact subset of R3. Both ﬁnite and continuous viewpoints will be studied in
this paper because they allow different types of regularization. Similarly  in the surface case  xs can
be viewed either as an element of Rd where d denotes the number of vertices or as a real-valued
function on a 2-dimensional compact Riemannian manifold.
We consider a group of N subjects with their corresponding data (xs)s∈[1 N ] ∈ X N . Each subject
is associated with a group (ys)s∈[1 N ] ∈ {−1  1}N (typically his diagnosis  i.e. diseased or healthy).

2.2 Linear SVM

The linear SVM solves the following optimization problem [3  4  11]:

(cid:0)wopt  bopt(cid:1) = arg min

w∈X  b∈R

NX

s=1

lhinge (ys [hw  xsi + b]) + λ k w k2

(1)

where λ ∈ R+ is the regularization parameter and lhinge the hinge loss function deﬁned as:
lhinge : u ∈ R 7→ max(0  1 − u).
With a linear SVM  the feature space is the same as the input space. Thus  when the input features
are the voxels of a 3D image  each element of wopt = (wopt
)v∈V also corresponds to a voxel.
Similarly  for the surface-based methods  the elements of wopt can be represented on the vertices
of the cortical surface. To be anatomically consistent  if v(1) ∈ V and v(2) ∈ V are close according
to the topology of V  their weights in the SVM classiﬁer  wopt
v(2) respectively  should be
similar. In other words  if v(1) and v(2) correpond to two neighboring regions  they should have a
similar role in the classiﬁer function. However  this is not guaranteed with the standard linear SVM
(as for example in [7]) because the regularization term is not a spatial regularization. The aim of
the present paper is to propose methods to ensure that wopt is spatially regularized.

v(1) and wopt

v

2

2.3 How to include priors in SVM

To spatially regularize the SVM  one has to include some prior knowledge on the proximity of
features. In the literature  three main ways have been considered in order to include priors in SVMs.
In an SVM  all the information used for classiﬁcation is encoded in the kernel. Hence  the ﬁrst way
to include prior is to directly design the kernel function [4]. But this implies knowing a metric on
the input space X consistent with the prior knowledge.
Another way is to force the classiﬁer function to be locally invariant to some transformations. This
can be done: (i) by directly engineering a kernel which leads to locally invariant SVM  (ii) by
generating artiﬁcially transformed examples from the training set to create virtual support vectors
(virtual SV)  (iii) by using a combination of both these approaches called kernel jittering [12  13 
14]. But the main difﬁculty here is how to deﬁne the transformations to which we would like the
kernel to be invariant.
The last way is to consider SVM from the regularization viewpoint [15  4]. The idea is to force the
classiﬁer function to be smooth with respect to some criteria. This is the viewpoint which is adopted
in this paper.

2.4 Regularization operators

Our aim is to introduce a spatial regularization on the classiﬁer function of the SVM which can be
written as sgn (f(xs) + b) where f ∈ RX . This is done through the deﬁnition of a regularization
operator P on f. Following [15  4]  P is deﬁned as a linear map from a space F ⊂ RX into a dot
product space (D h· ·iD).
G : X × X → R is a Green’s function of a regularization operator P iff:
∀f ∈ F  ∀x ∈ X   f(x) = hP (G(x ·)  P (f)iD

(2)
If P admits at least a Green’s function called G  then G is a positive semi-deﬁnite kernel and the
minimization problem:

lhinge (ys [f(xs) + b]) + λ k P (f) k2D

(3)

(cid:0)f opt  bopt(cid:1) = arg min

f∈F  b∈R

NX

s=1

is equivalent to the SVM minimization problem with kernel G.
Since in linear SVM  the feature space is the input space  f lies in the input space. Therefore  the
optimisation problem (3) is very convenient to include spatial regularization on f via the deﬁnition of
P . Note that  usually  F is a Reproducing Kernel Hilbert Space (RKHS) with kernel K and D = F.
Hence  if P is bounded  injective and compact  P admits a Green’s function G = (P †P )−1K where
P † denotes the adjoint of P .
One has to deﬁne the regularization operator P so as to obtain the suitable regularization for the
problem.

3 Laplacian regularization
Spatial regularization requires the notion of proximity between elements of V. This can be done
through the deﬁnition of a graph in the discrete case or a metric in the continuous case. In this sec-
tion  we propose spatial regularizations based on the Laplacian for both of these proximity models.
This penalizes the high-frequency components with respect to the topology of V.

3.1 Graphs
When V is ﬁnite  weighted graphs are a natural framework to take spatial information into consid-
eration. Voxels of a brain image can be considered as nodes of a graph which models the voxels’
proximity. This graph can be the voxel connectivity (6  18 or 26) or a more sophisticated graph.
We chose the following regularization operator:

P : w∗ ∈ F = L(RV   R) 7→(cid:16)

1
2 βLw

e

(cid:17)∗ ∈ F

(4)

3

where L denotes the graph Laplacian [16] and w∗ the dual vector of w. β controls the size of the
regularization. The optimization problem then becomes:

(wopt  bopt) = arg min
w∈X  b∈R

lhinge (ys [hw  xsi + b]) + λ k e

1

2 βLw k2

(5)

NX

s=1

Such a regularization exponentially penalizes the high-frequency components and thus forces the
classiﬁer to consider as similar voxels highly connected according to the graph adjacency matrix.
According to the previous section  this new minimization problem (5) is equivalent to an SVM
optimization problem. The new kernel Kβ is given by:
Kβ(x1  x2) = xT

(6)
This is a heat or diffusion kernel on a graph. Our approach differs from the diffusion kernels intro-
duced by Kondor et al. [17] because the nodes of the graph are the features  here the voxels  whereas
in [17]  the nodes were the objects to classify. Laplacian regularization was also used in satellite
imaging [18] but  again  the nodes were the objects to classify. Our approach can also be considered
as a spectral regularization on the graph [19]. To our knowledge  such spectral regularization has
not been applied to brain images but only to the classiﬁcation of microarray data [20].

1 e−βLx2

3.2 Compact Riemannian manifolds
In this paper  when V is continuous  it can be considered as a 2-dimensional (e.g. surfaces) or a
3-dimensional (e.g. 3D Euclidean or more complex) compact Riemannian manifold. The metric
then models the notion of proximity. On such spaces  the heat kernel exists [21  22]. Therefore  the
Laplacian regularization presented in the previous paragraph can be extended to compact Rieman-
nian manifolds [22]. Similarly to the graphs  we chose the following regularization operator:

P : w∗ ∈ F = L(RV   R) 7→(cid:16)

1
2 β∆w

(7)
where ∆ denotes the Laplace-Beltramin operator. The optimization problem is also equivalent to
1 e−β∆x2. Note the difference between
an SVM optimization problem with kernel Kβ(x1  x2) = xT
our approach and that of Laferty and Lebanon [22]. In our case  the points of the manifolds are the
features  whereas in [22]  they were the objects to classify.
In sections 4 and 5  we present different types of proximity models which correspond to different
types of graphs or distances.

e

(cid:17)∗ ∈ F

4 Spatial proximity

In this section  we consider the case of regularization based on spatial proximity  i.e. two voxels (or
vertices) are close if they are spatially close.

√

β [17].

4.1 The 3D case
When V are the image voxels (discrete case)  the simplest option to encode the spatial proximity
is to use the image connectivity (e.g. 6-connectivity) as a regularization graph. Similarly  when V
is a compact subset of R3 (continuous case)  the proximity is encoded by a Euclidean distance. In
both cases  this is equivalent to pre-process the data with a Gaussian smoothing kernel with standard
deviation σ =
However  smoothing the data with a Gaussian kernel would mix gray matter (GM)  white mat-
ter (WM) and cerebrospinal ﬂuid (CSF). Instead  we propose a graph which takes into considera-
tion both the spatial localization and the tissue types. Based on tissue probability maps  in each
voxel v  we have the set of probabilities pv that this voxel belongs to GM  WM or CSF. We con-
sidered the following graph. Two voxels are connected if and only if they are neighbors in the
image (6-connectivity). The weight au v of the edge between two connected voxels u and v is
au v = e−dχ2 (pu pv)2/(2σ2)  where dχ2 is the χ2-distance between two distributions. We chose
beforehand σ equal to the standard deviation of dχ2(pu  pv).
To compute the kernel  we computed e−βLxs for each subject s in the training set by scaling the
Laplacian and using the Taylor series expansion.

4

4.2 The surface case

The connectivity graph is not directly applicable to surfaces. Indeed  the regularization would then
strongly depend on the mesh used to discretize the surface. This shortcoming can be overcome
by reweighing the graph with conformal weights. In this paper  we chose a different approach by
adopting the continuous viewpoint: we consider the cortical surface as a 2-dimensional Riemannian
manifold and use the regularization operator deﬁned by equation (7). Indeed  the Laplacian is an
intrinsic operator and does not depend on the chosen surface parameterization. The heat kernel has
already been used for cortical smoothing for example in [23  24  25  26]. We will therefore not detail
this part. We used the implementation described in [26].

5 Anatomical proximity

In this section  we consider a different type of proximity  which we call anatomical proximity. Two
voxels are considered close if they belong to the same brain network. For example  two voxels
can be close if they belong to the same anatomical or functional region (deﬁned for example by
a probabilistic atlas). This can be seen as a “short-range” connectivity. Another example is that
of “long-range” proximity which models the fact that distant voxels can be anatomically (through
white matter tracts) or functionally connected (based on fMRI networks).
We ﬁrst focus on the discrete case. The presented framework can be used either for 3D images or
surfaces and computed very efﬁciently. However  such an efﬁcient implementation was obtained at
the cost of the spatial proximity. Therefore  we then show a continuous formulation which enables
to consider both spatial and anatomical proximity.

5.1 On graphs: atlas and connectivity
Let (A1 ···  AR) be the R regions of interest (ROI) of an atlas and p(v ∈ Ar) the probabil-
ity that the voxel v belongs to region Ar. Then the probability that two voxels v(i) and v(j)

r=1 p(cid:0)(cid:0)v(i)  v(j)(cid:1) ∈ A2
(cid:1). We assume that if v(i) 6= v(j) then:
(cid:1) p(cid:0)v(j) ∈ Ar
(cid:1). Let E ∈ Rd×R be the right stochastic matrix
(cid:1). Then  for i 6= j  the (i  j)-th entry of the adjacency matrix EEt

belong to the same region is: PR
p(cid:0)(cid:0)v(i)  v(j)(cid:1) ∈ A2
(cid:1) = p(cid:0)v(i) ∈ Ar
deﬁned by: Ei r = p(cid:0)v(i) ∈ Ar

r

r

is the probability that the voxels v(i) and v(j) belong to the same regions.
For “long-range“ connections (structural or functional)  one can consider an R-by-R matrix C with
the (r1  r2)-th entry being the probability that Ar1 and Ar2 are connected. Then the adjacency
matrix becomes: ECEt. We considered the normalized Laplacian ˜L [16]  to be sure that the two
terms commute:

where D is a diagonal matrix. Hence  if CEtD−1E is not singular  we have:

˜L = Id − D− 1

2 ECEtD− 1

2

i

(8)

(9)

e−β ˜L = e−βh

Id + D− 1

2 E(eβCEtD−1E − IR)(CEtD−1E)−1CEtD− 1

2

The computation requires only the computation of D− 1
2   which is done efﬁciently since D is a
diagonal matrix  and the computation of inverse and the matrix exponential of an R-by-R matrix 
which is also efﬁcient since R ∼ 102.
This method can be directly applied to both 3D images and cortical surfaces. Unfortunately  the
efﬁcient implementation was obtained at the cost of the spatial proximity. The next section presents
a combination of anatomical and spatial proximity using the continuous viewpoint.

5.2 On statistical manifolds

In this section  the goal is to take into account various prior informations such as tissue information 
atlas information and spatial proximity. We ﬁrst show that this can be done by considering the
images or surfaces as statistical manifolds together with the Fisher metric. We then give some
details about the computation of the kernel.

5

Fisher metric We assume that we are given an anatomical or a functional atlas A composed of
R regions: {Ar}r=1···R. Similarly  T = {TGM TWM TCSF} denotes the set of brain tissues. In
each point v ∈ V  we have a probability distribution patlas(·|v) ∈ RT ×A which informs about the
tissue type and the atlas region in v. Without any loss of generality  one can assume that the tis-
sue information is encoded in the atlas. Therefore  we consider the probability patlas(·|v) ∈ RA.
We also consider a probability distribution ploc(·|v) ∈ RV which encodes the spatial proxim-
ity. A simple example is ploc(·|v) ∼ N (v  σ2
loc). Therefore  we consider the probability family:

M = (cid:8)p(·|v) ∈ RA×V(cid:9)

v∈V where p(·|v) = patlas(·|v)ploc(·|v).

A natural way to encode proximity on M is to use the Fisher metric as in [22]. With some smooth-
ness assumption about p  M together with this metric is a compact Riemannian manifold [27]. For
clarity  we present this framework only for 3D images but it could be applied to cortical surfaces
with minor changes. The metric tensor g is then given for all v ∈ V by:

gij(v) = Ev

∂vj
If we further assume that ploc(·|v) is isotropic we have:

∂vi

(cid:20) ∂ log p(·|v)
Z

∂ log p(·|v)

  1 ≤ i  j ≤ 3

(cid:21)
(cid:18) ∂ log ploc(u|v)

(cid:19)2

(10)

(v) + δij

gij(v) = gatlas

(11)
where δij is the Kronecker delta and gatlas is the metric tensor when p(·|v) = patlas(·|v). When
ploc(·|v) ∼ N (v  σ2

locI3)  we have: gij(v) = gatlas

u∈V

∂vi

du

ij

ij

ploc(u|v)

(v) + δij
σ2

loc

.

Computing the kernel Once the notion of proximity is deﬁned  one has to compute the kernel
matrix. The computation of the kernel matrix requires the computation of e−β∆xs for all the subjects
of the training set. The eigendecomposition of the Laplace-Beltrami operator is intractable since the
number of voxels in a brain images is about 106. Hence e−β∆xs is considered as the solution at
t = β of the heat equation with the Dirichlet homogeneous boundary conditions:

3X

j=1

∂
∂vj

  3X

pdet g

hij

(12)

!

∂u
∂vi

i=1

The Laplace-Beltrami operator is given by [21]: ∆u =

1√
det g

where h is the inverse tensor of g.
To solve equation (12)  one can use a variational approach [28]. We used the rectangular ﬁnite
elements in space and the explicit ﬁnite difference scheme for the time discretization. ∆x and ∆t
denote the space step and the time step respectively. ∆x is ﬁxed by the MRI spatial resolution. ∆t
is then chosen so as to respect the Courant-Friedrichs-Lewy (CFL) condition  which can be written
in this case as: ∆t ≤ 2(max λi)−1  where λi are the eigenvalues of the general eigenproblem:
KU = λMU with K the stiffness matrix and M the mass matrix. To compute the optimal time
step ∆t  we estimated the largest eigenvalue with the power iteration method.

(cid:26) ∂u
∂t − ∆u = 0
u(t = 0) = xs

6 Experiments and results

6.1 Material

Subjects and MRI acquisition Data were obtained from the Alzheimer’s Disease Neuroimaging
Initiative (ADNI) database 1. The Principal Investigator of this initiative is Michael W. Weiner 
M.D.  VA Medical Center and University of California - San Francisco.For up-to-date information
see www.adni-info.org. We studied 30 patients with probable AD (age± standard-deviation (SD) =
74±4  range = 60-80 years  mini-mental score (MMS) = 23±2) and 30 elderly controls (age± SD =
73±4  range = 60-80  MMS = 29±1) which were selected from the ADNI database according to the

1www.loni.ucla.edu/ADNI

6

following criteria. Subjects were excluded if their scan revealed major artifacts or gross structural
abnormalities of the white matter  for it makes the tissue segmentation step fail. 80-year-old subjects
or older were also excluded. The MR scans are T1-weighted MR images. MRI acquisition was done
according to the ADNI acquisition protocol in [29].

Features extraction For the 3D image analyses  all T1-weighted MR images were segmented into
gray matter (GM)  white matter (WM) and cerebrospinal ﬂuid (CSF) using the SPM5 (Statistical
Parametric Mapping  London  UK) uniﬁed segmentation routine [30] and spatially normalized with
DARTEL [9]. The features are the GM probability maps in the MNI space. For the surface-based
analyses  the features are the cortical thickness values at each vertex of the cortical surface. Cortical
thickness measures were performed with Freesurfer (Massachusetts General Hospital  Boston  MA).

6.2 Proposed experiments

As an illustration of the method  we present the results of the AD versus controls analysis. We
present the maps associated to the optimal margin hyperplane (OMH). The classiﬁcation function
obtained with a linear SVM is the sign of the inner product of the features with wopt  a vector
orthogonal to the OMH [3  4]. Therefore  if the absolute value of the ith component of wopt  |wopt
| 
is small compared to the other components (|wopt
|)j6=i  the ith feature will have a small inﬂuence
on the classiﬁcation. Conversely  if |wopt
| is relatively large  the ith feature will play an important
role in the classiﬁer. Thus the optimal weights wopt allow us to evaluate the anatomical consistency
of the classiﬁer. In all experiments  the C parameter of the SVM was ﬁxed to one (λ = 1
2N C [4]).

j

i

i

6.3 Results: spatial proximity

In this section  we present the results for the spatial proximity in the 3D case (method presented in
section 4.1). Due to space limitations  the surface case is not presented. Fig. 1(a) presents the OMH
when no spatial regularization is performed. Fig. 1(b) shows the results with spatial proximity but
without tissue probability maps. w becomes smoother and spatially consistent. However it mixes
tissues and does not respect the topology of the cortex. For instance  it mixes tissues of the temporal
lobe with tissues of the frontal and parietal lobes. The results with both spatial proximity and tissue
maps are shown on Fig. 1(c). The OMH is much more consistent with the brain anatomy. β controls
the size of the spatial regularization and was chosen to be equivalent to a 4mm-FWHM of the
Gaussian smoothing. The classiﬁcation accuracy was estimated by a leave-one-out cross validation.
The classiﬁers were able to distinguish AD from CN with similar accuracies (83% with no spatial
priors and 85% with spatial priors).

6.4 Results: anatomical proximity

In this section  we present the results for the anatomical proximity. We ﬁrst present the discrete
surface case. The discrete 3D case leads to comparable results but is omitted here due to space
limitations. We then present the continuous 3D case. Extension to surfaces is left for future work.

Discrete case For the discrete case  we used ”short-range“ proximity  deﬁned by the cortical atlas
of Desikan et al. [31] with binary probabilities. We tested different values for β = 0  1 ···   5.
The accuracies ranged between 80% and 85%. The highest accuracy was reached for β = 3. The
optimal SVM weights w are shown on Fig. 2. When no regularization has been carried out  they
are noisy and scattered (Fig. 2 (a)). When the amount of regularization is increased  voxels of a
same region tend to be considered as similar by the classiﬁer (Fig. 2(b-d)). Note how the anatomical
coherence of the OMH varies with β.

Continuous case We then present the results of the 3D continuous case (section 5.2). The atlas
information used was only the tissue types. We chose σloc = 10mm for the spatial conﬁdency.
β was chosen to be equivalent to a 4mm-FWHM of the Gaussian smoothing. The classiﬁer reached
87% accuracy. The optimal SVM weights w are shown on Fig. 1(d). The tissue knowledge enables
the classiﬁer to be more consistent with the anatomy. For instance  note the difference with the
Gaussian smoothing (Fig. 1(b)) and how the proposed method avoids mixing the temporal lobe with
the parietal and frontal lobes.

7

-0.5

-0.05 +0.05

+0.5

(a)

(b)

(c)

(d)

Figure 1: Normalized w coefﬁcients: (a) no spatial prior  (b) spatial proximity: FWHM=4mm  (c) spatial
proximity and tissues: FWHM∼4mm  (d) Fisher metric using tissue maps.

-0.5

(b)

0 0

+0.5

(c)

(d)

(a)

Figure 2: Normalized w of the left hemisphere when the SVM is regularized with a cortical atlas [31]:
(a) β = 0 (no prior)  (b) β = 1  (c) β = 2  (d) β = 3.

7 Discussion

In this contribution  we proposed to use regularization operators to add spatial consistency to SVMs
for brain image analysis. We show that this provides a ﬂexible approach to model different types of
proximity between the features. We proposed derivations for both 3D image features  such as tissue
maps  or surface characteristics  such as cortical thickness. We considered two different types of
formulations: a discrete viewpoint in which the proximity is encoded via a graph  and a continuous
viewpoint in which the data lies on a Riemannian manifold. In particular  the latter viewpoint is
useful for surface cases because it overcomes problems due to surface parameterization. This paper
introduced two different types of proximity. We ﬁrst considered the case of regularization based on
spatial proximity  which results in spatially consistent OMH making their anatomical interpretation
more meaningful. We then considered a different type of proximity which allows modeling higher-
level knowledge  which we call anatomical proximity. In this model  two voxels are considered close
if they belong to the same brain network. For example  two voxels can be close if they belong to the
same anatomical region. This can be seen as a “short-range” connectivity. Another example is that
of “long-range” proximity which models the fact that distant voxels can be anatomically connected 
through white matter tracts  or functionally connected  based on fMRI networks.
Preliminary evaluation was performed on 30 patients with AD and 30 age-matched controls. The
results demonstrate that the proposed approaches allow obtaining spatially and anatomically coher-
ent discrimination patterns. In particular  the obtained hyperplanes are largely consistent with the
neuropathology of AD  with highly discriminant features in the medial temporal lobe  as well as
lateral temporal  parietal associative and frontal areas. As for the classiﬁcation results  they were
comparable to those reported in the literature for AD classiﬁcation (e.g. [5  8  7]). The use of regu-
larization did not substantially improve the accuracy. However  the most important point is that the
proposed approach makes the results more consistent with the anatomy  making their interpretation
more meaningful.
Finally  it should be noted that the proposed approach is not speciﬁc to structural MRI  and can be
applied to other pathologies and other types of data (e.g. functional or diffusion-weighted MRI).

Acknowledgments

This work was supported by ANR (project HM-TC  number ANR-09-EMER-006).
Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging
Initiative (ADNI; Principal Investigator: Michael Weiner; NIH grant U01 AG024904). ADNI data
are disseminated by the Laboratory of Neuro Imaging at the University of California  Los Angeles.

8

References
[1] J. Ashburner and K.J. Friston. Voxel-based morphometry–the methods. NeuroImage  11(6):805–21  2000.
[2] C. Davatzikos. Why voxel-based morphometric analysis should be used with great caution when charac-

terizing group differences. NeuroImage  23(1):17–20  2004.

[3] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag  1995.
[4] B. Sch¨olkopf and A.J. Smola. Learning with Kernels. MIT Press  2001.
[5] Z. Lao et al. Morphological classiﬁcation of brains via high-dimensional shape transformations and

machine learning methods. NeuroImage  21(1):46–57  2004.

[6] Y. Fan et al. COMPARE: classiﬁcation of morphological patterns using adaptive regional elements. IEEE

TMI  26(1):93–105  2007.

[7] S. Kl¨oppel et al. Automatic classiﬁcation of MR scans in Alzheimer’s disease. Brain  131(3):681–9 

2008.

[8] P. Vemuri et al. Alzheimer’s disease diagnosis in individual subjects using structural MR images: valida-

tion studies. NeuroImage  39(3):1186–97  2008.

[9] J. Ashburner et al. A fast diffeomorphic image registration algorithm. NeuroImage  38(1):95–113  2007.
[10] O. Querbes et al. Early diagnosis of Alzheimer’s disease using cortical thickness: impact of cognitive

reserve. Brain  132(8):2036  2009.

[11] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge Univ Pr  2004.
[12] D. Decoste and B. Sch¨olkopf. Training invariant support vector machines. Machine Learning  46(1):161–

90  2002.

[13] B. Sch¨olkopf et al. Incorporating invariances in support vector learning machines. In Proc. ICANN 1996 

page 47. Springer Verlag  1996.

[14] B. Sch¨olkopf et al. Prior knowledge in support vector kernels. In Proc. conference on Advances in neural

information processing systems’97  pages 640–46. MIT Press  1998.

[15] A.J. Smola and B. Sch¨olkopf. On a kernel-based method for pattern recognition  regression  approxima-

tion  and operator inversion. Algorithmica  22(1/2):211–31  1998.
[16] F.R.K. Chung. Spectral Graph Theory. Number 92. AMS  1992.
[17] R. I. Kondor and J.D. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In Proc.

International Conference on Machine Learning  pages 315–22  2002.

[18] L. G´omez-Chova et al. Semi-supervised image classiﬁcation with Laplacian support vector machines.

IEEE Geo Rem Sens Let  5(3):336–40  2008.

[19] A.J. Smola and R. Kondor. Kernels and regularization on graphs. In Proc. COLT  page 144. Springer

Verlag  2003.

[20] F. Rapaport et al. Classiﬁcation of microarray data using gene networks. BMC bioinformatics  8(1):35 

2007.

[21] J. Jost. Riemannian geometry and geometric analysis. Springer Verlag  2008.
[22] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. JMLR  6:129–63  2005.
[23] A. Andrade et al. Detection of fMRI activation using cortical surface mapping. Hum Brain Mapp 

12(2):79–93  2001.

[24] A. Cachia et al. A primal sketch of the cortex mean curvature: a morphogenesis based approach to study

the variability of the folding patterns. IEEE TMI  22(6):754–765  2003.

[25] M.K. Chung. Heat kernel smoothing and its application to cortical manifolds. Technical report  1090.

Department of Statistics  Univ of Wisconsin  Madison  2004.

[26] M.K. Chung et al. Cortical thickness analysis in autism with heat kernel smoothing. NeuroImage 

25(4):1256–65  2005.

[27] S.-I. Amari et al. Differential Geometry in Statistical Inference  volume 10. Institute of Mathematical

Statistics  1987.

[28] O. Druet et al. Blow-up theory for elliptic PDEs in Riemannian geometry. Princeton Univ Pr  2004.
[29] C.R.Jr Jack et al. The Alzheimer’s Disease Neuroimaging Initiative (ADNI): MRI methods. J Magn

Reson Imaging  27(4):685–91  2008.

[30] J. Ashburner and K.J. Friston. Uniﬁed segmentation. NeuroImage  26(3):839–51  2005.
[31] R. S. Desikan et al. An automated labeling system for subdividing the human cerebral cortex on MRI

scans into gyral based regions of interest. Neuroimage  31(3):968–980  2006.

9

,Celestine Dünner
Thomas Parnell
Dimitrios Sarigiannis
Nikolas Ioannou
Andreea Anghel
Gummadi Ravi
Madhusudanan Kandasamy
Haralampos Pozidis