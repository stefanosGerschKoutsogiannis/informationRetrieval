2019,Comparing distributions: $\ell_1$ geometry improves kernel two-sample testing,Are two sets of observations drawn from the same distribution? This
problem is a two-sample test. 
Kernel methods lead to many appealing properties. Indeed state-of-the-art
approaches use the $L^2$ distance between kernel-based
distribution representatives to derive their test statistics. Here  we show that
$L^p$ distances (with $p\geq 1$) between these
distribution representatives give metrics on the space of distributions that are
well-behaved to detect differences between distributions as they
metrize the weak convergence. Moreover  for analytic kernels 
we show that the $L^1$ geometry gives improved testing power for
scalable computational procedures. Specifically  we derive a finite
dimensional approximation of the metric given as the $\ell_1$ norm of a vector which captures differences of expectations of analytic functions evaluated at spatial locations or frequencies (i.e  features). The features can be chosen to
maximize the differences of the distributions and give interpretable
indications of how they differs. Using an $\ell_1$ norm gives better detection
because differences between representatives are dense
as we use analytic kernels (non-zero almost everywhere). The tests are consistent  while
much faster than state-of-the-art quadratic-time kernel-based tests. Experiments
on artificial
and real-world problems demonstrate
improved power/time tradeoff than the state of the art  based on
$\ell_2$ norms  and in some cases  better outright power than even the most
expensive quadratic-time tests. This performance gain is retained even in high dimensions.,Comparing distributions: `1 geometry improves

kernel two-sample testing

CREST  ENSAE & Inria  Université Paris-Saclay

Meyer Scetbon

Gaël Varoquaux

Inria  Université Paris-Saclay

Abstract

Are two sets of observations drawn from the same distribution? This problem is
a two-sample test. Kernel methods lead to many appealing properties. Indeed
state-of-the-art approaches use the L2 distance between kernel-based distribution
representatives to derive their test statistics. Here  we show that Lp distances
(with p  1) between these distribution representatives give metrics on the space
of distributions that are well-behaved to detect differences between distributions
as they metrize the weak convergence. Moreover  for analytic kernels  we show
that the L1 geometry gives improved testing power for scalable computational
procedures. Speciﬁcally  we derive a ﬁnite dimensional approximation of the
metric given as the `1 norm of a vector which captures differences of expectations
of analytic functions evaluated at spatial locations or frequencies (i.e  features).
The features can be chosen to maximize the differences of the distributions and
give interpretable indications of how they differs. Using an `1 norm gives better
detection because differences between representatives are dense as we use analytic
kernels (non-zero almost everywhere). The tests are consistent  while much faster
than state-of-the-art quadratic-time kernel-based tests. Experiments on artiﬁcial
and real-world problems demonstrate improved power/time tradeoff than the state
of the art  based on `2 norms  and in some cases  better outright power than even
the most expensive quadratic-time tests.

We consider two sample tests: testing whether two random variables are identically distributed without
assumption on their distributions. This problem has many applications such as data integration [4] or
automated model checking [22]. Distances between distributions underlie progress in unsupervised
learning with generative adversarial networks [20  1]. A kernel on the sample space can be used to
build the Maximum Mean Discrepancy (MMD) [11  12  13  26]  a metric on distribution which has the
strong propriety of metrizing the weak convergence of probability measures. It leads to non-parametric
two-sample tests using the reproducing kernel Hilbert space (RKHS) distance [15  9]  or energy
distance [32  3]. The MMD has a quadratic computational cost  which may force to use of subsampled
estimates [33  14]. [5] approximate the L2 distance between distribution representatives in the RKHS 
to compute in linear time a pseudo metric over the space of distributions. Such approximations are
related to random (Fourier) features  used in kernels algorithms [24  19]. Distribution representatives
can be mean embeddings [29  30] or smooth characteristic functions [5  17].
We ﬁrst introduce the state of the art on Kernel-based two-sample testing built from the L2 distance
between mean embeddings in the RKHS. In fact  a wider family of distance is well suited for the
two-sample problem: we show that for any p  1  the Lp distance between these distribution repre-
sentatives is a metric on the space of Borel probability measures that metrizes their weak convergence.
We then deﬁne our `1-based statistic derived from the L1 geometry and study its asymptotic behavior.
We consider the general case where the number of samples of the two distributions may differ. We
show that using the `1 norm provides a better testing power. Indeed  test statistics approximate such
metrics and are deﬁned as the norm of a J-dimensional vector which is the difference between the
two distribution representatives at J locations. Under the alternative hypothesis H1: P 6= Q  the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

analyticity of the kernel ensures that all the features of this vector are non zero almost surely. We
show that the `1 norm captures this dense difference better than the `2 norm and leads to better tests.
We show also that improvements of Kernel two-sample tests established with the `2 norm [17] hold in
the `1 case: optimizing features and the choice of kernel. We adapt the construction in the frequency
domain as in [5]. Finally  we show that on 4 synthetic and 3 real-life problems  our new `1-based
tests outperform the state of the art.

1 Prior art: kernel embeddings for two-sample tests
Given two samples X := {xi}n
i=1 ⇢X independently and identically distributed
(i.i.d.) according to two probability measures P and Q on a metric space (X   d) respectively  the goal
of a two-sample test is to decide whether P is different from Q on the basis of the samples. Kernel
methods arise naturally in two-sample testing as they provide Euclidean norms over the space of
probability measures that metrize the convergence in law. To deﬁne such a metric  we need ﬁrst to
introduce the notion of Integral Probability Metric (IPM):

i=1  Y := {yi}n

IPM[F  P  Q] := sup

f2F⇣Ex⇠P [f (x)]  Ey⇠Q [f (y)]⌘

where F is an arbitrary class of functions. When F is the unit ball Bk in the RKHS Hk associated
with a positive deﬁnite bounded kernel k : X⇥X! R  the IPM is known as the Maximum Mean
Discrepancy (MMD) [11]  and it can be shown that the MMD is equal to the RKHS distance between
so called mean embeddings [13] 

MMD[P  Q] = kµP  µQkHk
where µP is an embedding of the probability measure P to Hk 
µP (t) :=ZRd

k(x  t)dP (x)

and k.kHk denotes the norm in the RKHS Hk. Moreover for kernels said to be characteristic [10] 
eg Gaussian kernels  MMD[P  Q] = 0 if and only if P = Q [11]. In addition  when the kernel
is bounded  and X is a compact Hausdorff space  [28] show that the MMD metrizes the weak
convergence. Tests between distributions can be designed using an empirical estimation of the MMD.
A drawback of the MMD is the computation cost of empirical estimates  these being the sum of two
U-statistics and an empirical average  with a quadratic cost in the sample size.
[5] study a related expression deﬁned as the L2 distance between mean embeddings of Borel
probability measures:

where  is a Borel probability measure. They estimate the integral (4) with the random variable 

(1)

(2)

(3)

(4)

(5)

2

d(t)

d2

L2 µ(P  Q) :=Zt2RdµP (t)  µQ(t)
JXj=1µP (Tj)  µQ(Tj)

d2
`2 µ J (P  Q) :=

1
J

2

j=1 are sampled i.i.d. from the distribution . This expression still has desirable metric-

where {Tj}J
like properties  provided that the kernel is analytic:
Deﬁnition 1.1 (Analytic kernel). A positive deﬁnite kernel k : Rd ⇥ Rd ! R is analytic on its
domain if for all x 2 Rd  the feature map k(x  .) is an analytic function on Rd.
Indeed  for k a deﬁnite positive  characteristic  analytic  and bounded kernel on Rd  [5] show that
d`2 µ J is a random metric1 from which consistent two-sample test can be derived. By denoting µX
and µY respectively the empirical mean embeddings of P and Q 

µX(T ) :=

1
n

k(xi  T ) 

µY (T ) :=

1
n

k(yi  T )

nXi=1

1A random metric is a random process which satisﬁes all the conditions for a metric ‘almost surely’ [5].

nXi=1

2

[5] show that for {Tj}J
as n ! 1  the following test statistic:

j=1 sampled from the distribution   under the null hypothesis H0 : P = Q 

2

(6)

`2 µ J [X  Y ] := n

JXj=1µX(Tj)  µY (Tj)

bd2
converges in distribution to a sum of correlated chi-squared variables. Moreover  under the alternative
hypothesis H1 : P 6= Q  bd2
`2 µ J [X  Y ] can be arbitrarly large as n ! 1  allowing the test to
correctly reject H0. For a ﬁxed level ↵  the test rejects H0 if bd2
`2 µ J [X  Y ] exceeds a predetermined
test threshold  which is given by the (1  ↵)-quantile of its asymptotic null distribution. As it is
very computationally costly to obtain quantiles of this distribution  [5] normalize the differences
between mean embeddings  and consider instead the test statistic ME[X Y]:=kpn⌃1/2
2 where
Sn := 1
j=1 2
RJ. Under the null hypothesis H0  asymptotically the ME statistic follows 2(J)  a chi-squared
distribution with J degrees of freedom. Moreover  for k a translation-invariant kernel  [5] derive
another statistical test  called the SCF test (for Smooth Characteristic Function)  where its statistic
SCF[X  Y ] is of the same form as the ME test statistic with a modiﬁed zi := [f (xi) sin(xT
i Tj) 
j=1 2 R2J where f is the inverse Fourier
f (yi) sin(yT
transform of k  and show that under H0  SCF[X  Y ] follows asymptotically 2(2J).

Snk2
i=1(zi  Sn)(zi  Sn)T   and zi := (k(xi  Tj)  k(yi  Tj))J

i Tj)  f (yi) cos(yT

n1Pn

i Tj)  f (xi) cos(xT

i=1 zi  ⌃n := 1

nPn

n

i Tj)]J

2 A family of metrics that metrize of the weak convergence

+(Rd):

+(Rd) ⇥M 1

[5] build their ME statistic by estimating the L2 distance between mean embeddings. This metric can
be generalized using any Lp distance with p  1. These metrics are well suited for the two-sample
problem as they metrize the weak convergence (see proof in supp. mat. A.1):
Theorem 2.1. Given p  1  k a deﬁnite positive  characteristic  continuous  and bounded kernel on
Rd  µP and µQ the mean embeddings of the Borel probability measures P and Q respectively  the
function deﬁned on M1

dLp µ(P  Q) :=✓Zt2RdµP (t)  µQ(t)

d(t)◆1/p
is a metric on the space of Borel probability measures  for  a Borel probability measure absolutely
continuous with respect to Lebesgue measure. Moreover a sequence (↵n)n0 of Borel probability
measures converges weakly towards ↵ if and only if dLp µ(↵n ↵ ) ! 0.
Therefore  as the MMD  these metrics take into account the geometry of the underlying space and
metrize the convergence in law. If we assume in addition that the kernel is analytic  we will show that
deriving test statistics from the L1 distance instead of the L2 distance improves the test power for
two-sample testing.

(7)

p

3 Two-sample testing using the `1 norm

3.1 A test statistic with simple asymptotic distribution
From now  we assume that k is a positive deﬁnite  characteristic  analytic  and bounded kernel.
The statistic presented in eq. 6 is based on the `2 norm of a vector that capture differences between
distributions in the RKHS at J locations. We will show that using an `1 norm instead of an `2 norm
improves the test power (Proposition 3.1). It captures better the geometry of the problem. Indeed 
when P 6= Q  the differences between distributions are dense which allow the `1 norm to reject better
the null hypothesis H0: P = Q.
We now build a consistent statistical test based on an empirical estimation of the L1 metric introduced
in eq. 7:

bd`1 µ J [X  Y ] := pn

JXj=1µX(Tj)  µY (Tj)

3

(8)

j=1 are sampled from the distribution . We show that under H0 bd`1 µ J [X  Y ] converges
where {Tj}J
in distribution to a sum of correlated Nakagami variables2 and under H1  bd`1 µ J [X  Y ] can be
arbitrary large as n ! 1 (see supp. mat. C.1). For a ﬁxed level ↵  the test rejects H0 ifbd`1 µ J [X  Y ]
exceeds the (1  ↵)-quantile of its asymptotic null distribution. We now compare the power of the
statistics based respectively on the `2 norm (eq. 6) and the `1 norm (eq. 8) at the same level ↵> 0
and we show that the power of the test using the `1 norm is better with high probability (see supp.
mat. C.2):
Proposition 3.1. Let ↵ 2]0  1[  > 0 and J  2. Let {Tj}J
and let X := {xi}n
i=1 and Y := {yi}n
the (1  ↵)-quantile of the asymptotic null distribution of bd`1 µ J [X  Y ] and  the (1  ↵)-quantile
of the asymptotic null distribution of bd2
there exists N  1 such that for all n  N  with a probability of at least 1   we have:

j=1 sampled i.i.d. from the distribution 
i=1 i.i.d. samples from P and Q respectively. Let us denote 

`2 µ J [X  Y ]. Under the alternative hypothesis  almost surely 

bd2
`2 µ J [X  Y ] > ) bd`1 µ J [X  Y ] >

Therefore  for a ﬁxed level ↵  under the alternative hypothesis  when the number of samples is large
enough  with high probability  the `1-based test rejects better the null hypothesis. However  even
for ﬁxed {Tj}J
j=1  computing the quantiles of these distributions requires a computationally-costly
bootstrap or permutation procedure. Thus we follow a different approach where we allow the number
of samples to differ. Let X := {xi}N1
i=1 i.i.d according to respectively P and Q.
We deﬁne for any sequence of {Tj}J

i=1 and Y := {yi}N2
j=1 in Rd:

SN1 N2 :=⇣µX(T1)  µY (T1)  ...  µX(TJ )  µY (TJ )⌘

X := (k(xi  T1)  ...  k(xi  TJ )) 2 RJ
Zi

Zj
Y := (k(yj  T1)  ...  k(yj  TJ )) 2 RJ

(9)

(10)

And by denoting:

⌃N1 :=

1

N1  1

N1Xi=1

We can deﬁne our new statistic as:

(Zi

X  ZX)(Zi

X  ZX)T

⌃N2 :=

1

N2  1

⌃N1 N2 :=

⌃N1
⇢

+

⌃N2
1  ⇢

L1-ME[X  Y ] :=

pt⌃ 1

2

N1 N2

SN1 N21

N2Xj=1

(Zj

Y  ZY )(Zj

Y  ZY )T

(11)

t ! ⇢ and therefore N2

We assume that the number of samples of the distributions P and Q are of the same order  i.e: let
t ! 1  ⇢ with ⇢ 2]0  1[. The computation of
t = N1 + N2  we have: N1
the statistic requires inverting a J ⇥ J matrix ⌃N1 N2  but this is fast and numerically stable: J is
typically be small  eg less than 10. The next proposition demonstrates the use of this statistic as a
consistent two-sample test (see supp. mat. C.3 for the proof).
Proposition 3.2. Let {Tj}J
{yi}N2
surely asymptotically distributed as Naka( 1
Nakagami distribution of parameter m = 1
can be arbitrarily large as t ! 1  enabling the test to correctly reject H0.
Statistical test of level ↵: Compute kpt⌃ 1
the (1  ↵)-quantile of Naka( 1
is larger than .

i=1 and Y :=
i=1 be i.i.d. samples from P and Q respectively. Under H0  the statistic L1-ME[X  Y ] is almost
2   1  J)  a sum of J random variables i.i.d which follow a
2 and ! = 1. Finally under H1  almost surely the statistic

SN1 N2k1  choose the threshold  corresponding to
SN1 N2k1

2   1  J)  and reject the null hypothesis whenever kpt⌃ 1

j=1 sampled i.i.d. from the distribution  and X := {xi}N1

N1 N2

N1 N2

2

2

2the pdf of the Nakagami distribution of parameters m  1

2 and !> 0 is 8x  0 

(m)!m x2m1 exp( m

! x2) where  is the Gamma function.

f (x  m  !) = 2mm

4

sup

2 SN1 N2k1 (see proof in supp. mat. D.1).

3.2 Optimizing test locations to improve power
As in [17]  we can optimize the test locations V and kernel parameters (jointly referred to as ✓)
by maximizing a lower bound on the test power which offers a simple objective function for fast
parameter tuning. We make the same regularization as in [17] of the test statistic for stability of the
matrix inverse  by adding a regularization parameter N1 N2 > 0 which goes to 0 as t goes to inﬁnity 
giving L1-ME[X  Y ] := kpt(⌃N1 N2 + N1 N2I) 1
Proposition 3.3. Let K be a uniformly bounded family of k : Rd ⇥ Rd ! R measurable kernels (i.e. 
(x y)2(Rd)2|k(x  y)| K). Let V be a collection in which each element is a
9 K < 1 such that sup
k2K
V 2V k2Kk⌃1/2k< 1. Then the test power P⇣bt  ⌘ of
set of J test locations. Assume that c := sup
(N1 + N2)2!
J 2 + J◆2 N1 N2N1N2
(J 2+J)pt  J 3K2
t
(N1 + N2) max⇣ 8

the L1-ME test satisﬁes P⇣bt  ⌘  L(t) where:
exp ✓ t  
JXk=1
exp0B@2⇣ N1 N2
JXk q=1

(1⇢)N2⌘2 1CA
pN1 N2  J 4K1⌘2
2 Sk1 is the population counterpart ofbt := kpt(⌃N1 N2 + N1 N2I) 1

and K1  K2  K3 and K  are positive constants depending on only K  J and c. The parameter
t := kpt⌃ 1
2 SN1 N2k1
where S = Ex y(SN1 N2) and ⌃ = Ex y(⌃N1 N2). Moreover for large t  L(t) is increasing in t.
Proposition 3.3 suggests that it is sufﬁcient to maximize t to maximize a lower bound on the L1-ME
test power. The statistic t for this test depends on a set of test locations V and a kernel parameter .
We set ✓⇤ := {V  } = arg max
2 Sk1. As proposed in [14]  we can maximize
a proxy test power to optimize ✓: it does not affect H0 and H1 as long as the data used for parameter
tuning and for testing are disjoint.

✓ kpt ⌃ 1

L(t) =1  2

t = arg max

 2

K3J 2

K2

⇢N1

✓

 

8

3.3 Using smooth characteristic functions (SCF)

As the ME statistic  the SCF statistic estimates the L2 distance between well chosen distribution
representatives. Here  the representatives of the distributions are the convolution of their characteristic
functions and the kernel k  assumed translation-invariant. [5] use them to detect differences between
distributions in the frequency domain. We show that the L1 version (denoted dL1 ) is a metric on the
space of Borel probability measures with integrable characteristic functions such that if ↵n converge
weakly towards ↵  then dL1 (↵n ↵ ) ! 0 (see supp. mat. A.2). Let us introduce the test statistics in
the frequency domain respectively based on the `2 norm and on the `1 norm which lead to consistent
tests:

2

and

(12)

i=1 zi  zi

:= [f (xi) sin(xT

i Tj)  f (yi) sin(yT

bd`1  J [X  Y ] := kpnSnk1

`2  J [X  Y ] := kpnSnk2
bd2
nPn
where Sn := 1
i Tj) 
j=1 2 R2J and f is the inverse Fourier transform of k. We show that  at the
f (yi) cos(yT
i Tj)]J
same level ↵  using the `1 norm in the frequency domain provides a better power with high probabil-
ity (see supp. mat. E.1):
Proposition 3.4. Let ↵ 2]0  1[  > 0 and J  2. Let {Tj}J
and let X := {xi}n
i=1 and Y := {yi}n
the (1  ↵)-quantile of the asymptotic null distribution of bd`1  J [X  Y ] and  the (1  ↵)-quantile
of the asymptotic null distribution of bd2
there exists N  1 such that for all n  N  with a probability of at least 1   we have:

j=1 sampled i.i.d. from the distribution 
i=1 i.i.d. samples from P and Q respectively. Let us denote 

`2  J [X  Y ]. Under the alternative hypothesis  almost surely 

i Tj)  f (xi) cos(xT

bd2
`2  J [X  Y ] > ) bd`1  J [X  Y ] >

5

(13)

2

L1-SCF[X  Y ] := kpt ⌃ 1
1 xi f (xi)  ...  sinT T

X =cosT T

N1 N2

Zi

SN1 N2k1

J xi f (xi) 2 R2J

(14)
Y ):

We now adapt the construction of the L1-ME test to the frequency domain to avoid computational
issues of the quantiles of the asymptotic null distribution:

with ⌃N1 N2  and SN1 N2 deﬁned as in the L1-ME statistic with new expression for Zi

X (and Zj

From this statistic  we build a consistent test. Indeed  an analogue proof of the Proposition 3.2 gives
that under H0  L1-SCF[X  Y ] is a.s. asymptotically distributed as Naka( 1
2   1  2J)  and under H1  the
test statistic can be arbitrarily large as t goes to inﬁnity. Finally an analogue proof of Proposition 3.3
shows that we can optimize the test locations and the kernel parameter to improve the power as well.

4 Experimental study

We now run empirical comparisons of our `1-based tests to their `2 counterparts  state-of-the-
art Kernel-based two-sample tests. We study both toy and real problems. We use the isotropic
Gaussian kernel class Kg. We call L1-opt-ME and L1-opt-SCF the tests based respectively on
mean embeddings and smooth characteristic functions proposed in this paper when optimizing test
locations and the Gaussian width  on a separate training set of the same size as the test set. We
denote also L1-grid-ME and L1-grid-SCF where only the Gaussian width is optimized by a grid
search  and locations are randomly drawn from a multivariate normal distribution. We write ME-full
and SCF-full for the tests of [17]  also fully optimized according to their criteria. MMD-quad
(quadratic-time) and MMD-lin (linear-time) refer to the MMD-based tests of [11]  where  to ensure
a fair comparison  the kernel width is also set to maximize the test power following [14]. For MMD-
quad  as its null distribution is an inﬁnite sum of weighted chi-squared variables (no closed-form
quantiles)  we approximate the null distribution with 200 random permutations in each trial.
In all the following experiments  we repeat each problem 500 times. For synthetic problems  we
generate new samples from the speciﬁed P   Q distributions in each trial. For the ﬁrst real problem
(Higgs dataset)  as the dataset is big enough we use new samples from the two distributions for each
trial. For the second and third real problem (Fast food and text datasets)  samples are split randomly
into train and test sets in each trial. In all the simulations we report an empirical estimate of the
Type-I error when H0 hold and of the Type-II error when H1 hold. We set ↵ = 0.01. The code is
available at https://github.com/meyerscetbon/l1_two_sample_test.
How to realize `1-based tests ? The asymptotic distributions of the statistics is a sum of i.i.d.
Nakagami distribution. [8] give a closed form for the probability density function. As the formula is
not simple  we can also derive an estimate of the CDF (see supp. mat. F.1).
Optimization For a fair comparison between our tests and those of [17]  we use the same initialization
of the test locations3. For the ME-based tests  we initialize the test locations with realizations from
two multivariate normal distributions ﬁtted to samples from P and Q and for the for initialization of
the SCF-based tests  we use the standard normal distribution. The regularization parameter is set to
N1 N2 = 105. The computation costs for our proposed tests are the same as that of [17]: with t
samples  optimization is O(J 3 + dJt) per gradient ascent iteration and testing O(J 3 + Jt + dJt)
(see supp. mat. Table 3).
The experiments on synthetic problems mirror those of [17] to make a fair comparison between the
prior art and the proposed methods.
Test power vs. sample size We consider four syn-
thetic problems: Same Gaussian (SG  dim= 50) 
Gaussian mean difference (GMD  dim= 100)  Gaus-
sian variance difference (GVD  dim= 30)  and Blobs.
Table 1 summarizes the speciﬁcations of P and Q. In
the Blobs problem  P and Q are a mixture of Gaus-
sian distributions on a 4⇥ 4 grid in R2. This problem
is challenging as the difference of P and Q is en-
coded at a much smaller length scale compared to the
global structure as explained in [14]. We set J = 5 in this experiment.

Data
SG
N (0  Id)
GMD N (0  Id)
N ((1  0  ..  0)T   Id)
GVD N (0  Id)
N (0  diag(2  1  ..  1))
Blobs Mixture of 16 Gaussians in R2 as [17]

Table 1: Synthetic problems.
H0 holds only in SG.

N (0  Id)

Q

P

3[17]: github.com/wittawatj/interpretable-test

6

Figure 1: Plots of type-I/type-II errors against the test sample size nte in the four synthetic problems.

Figure 1 shows type-I error (for SG problem)  and test power (for GMD  GVD and Blobs problem)
as a function of test sample size. In the SG problem  the type-I error roughly stays at the speciﬁed
level ↵ for all tests except the L1-ME tests  which reject the null at a rate below the speciﬁed level ↵.
Therefore  here these tests are more conservative.
GMD with 100 dimensions is an easy problem for L1-opt-ME  L1-opt-SCF  ME-full MMD-quad 
while the SCF-full test requires many samples to achieve optimal test power. In the GMD  GVD and
Blobs cases  L1-opt-ME and L1-opt-SCF achieve substantially higher test power than L1-grid-ME
and L1-grid-SCF  respectively: optimizing the test locations brings a clear beneﬁt. Remarkably
L1-opt-SCF consistently outperforms the quadratic-time MMD-quad up to 2 500 samples in the
GVD case. SCF variants perform signiﬁcantly better than ME variants on the Blobs problem  as the
difference in P and Q is localized in the frequency domain. For the same reason  L1-opt-SCF does
much better than the quadratic-time MMD up to 3 000 samples  as the latter represents a weighted
distance between characteristic functions integrated across the frequency domain as explained in [29].
We also perform a more difﬁcult GMD problem to distinguish the power of the proposed tests with
the ME-full as all reach maximal power. L1-opt-ME then performs better than ME-full  its `2
counterpart  as it needs less data to achieve good control (see mat. supp. F.3).
Test power vs. dimension d On ﬁg 2  we study how the dimension of the problem affects type-I error
and test power of our tests. We consider the same synthetic problems: SG  GMD and GVD  we ﬁx the
test sample size to 10000  set J = 5  and vary the dimension. Given that these experiments explore
large dimensions and a large number of samples  computing the MMD-quad was too expensive.
In the SG problem  we observe the L1-ME tests are more conservative as dimension increases  and
the others tests can maintain type-I error at roughly the speciﬁed signiﬁcance level ↵ = 0.01. In the
GMD problem  we note that the tests proposed achieve the maximum test power without making
error of type-II whatever the dimension is  while the SCF-full loses power as dimension increases.
However  this is true only with optimization of the test locations as it is shown by the test power of
L1-grid-ME and L1-grid-SCF which drops as dimension increases. Moreover the performance of
MMD-lin degrades quickly with increasing dimension  as expected from [25]. Finally in the GVD
problem  all tests failed to keep a good test power as the dimension increases  except the L1-opt-SCF 
which has a very low type-II for all dimensions. These results echo those obtained by [34]. Indeed
[34] study a class of two sample test statistics based on inter-point distances and they show beneﬁts
of using the `1 norm over the Euclidean distance and the Maximum Mean Discrepancy (MMD) when
the dimensionality goes to inﬁnity. For this class of test statistics  they characterize asymptotic power

Figure 2: Plots of type-
I/type-II error against
the dimension in three
synthetic
problems:
SG (Same Gaussian) 
GMD (Gaussian Mean
Difference)  and GVD
(Gaussian
Variance
Difference).

7

that the objective functionbtr

loss w.r.t the dimension and show that the `1 norm is beneﬁcial compared to the `2 norm provided
that the summation of discrepancies between marginal univariate distributions is large enough.
Informative features Figure 3 we replicate the ex-
periment of [17]  showing that the selected locations
capture multiple modes in the `1 case  as in the `2
case. (details in supp. mat. F.4). The ﬁgure shows
t (T1  T2) used to posi-
tion the second test location T2 has a maximum far
from the chosen position for the ﬁrst test location T1.
Real Data 1  Higgs: The ﬁrst real problem is the
Higgs dataset [21] described in [2]: distinguishing
signatures of Higgs bosons from the background. We
use a two-sample test on 4 derived features as in [5].
We compare for various sample sizes the performance
of the proposed tests with those of [17]. We do not
study the MMD-quad test as its computation is too
expensive with 10 000 samples. To make the problem
harder  we only consider J = 3 locations. Fig. 4
shows a clear beneﬁt of the optimized `1-based tests 
in particular for SCF (L1-opt-SCF) compared to its
`2 counterpart (SCF-full). Optimizing the location
is important  as L1-opt-SCF and L1-opt-ME per-
form much better than their grid versions (which are
comparable to the tests of [5]).
Real Data 2  Fastfood: We use a Kaggle dataset listing locations of over 10 000 fast food restaurants
across America4. We consider the 6 most frequent brands in mainland USA: Mc Donald’s  Burger
King  Taco Bell  Wendy’s  Arby’s and KFC. We benchmark the various two-sample tests to test
whether the spatial distribution (in R2) of restaurants differs across brand. This is a non trivial
question  as it depends on marketing strategy of the brand. We compare the distribution of Mc
Donald’s restaurants with others. We also compare the distribution of Mc Donald’s restaurants with
itself to evaluate the level of the tests (see supp. mat. Table 5). The number of samples differ across
the distributions; hence to perform the tests from [17]  we randomly subsample the largest distribution.
We use J = 3 as the number of locations.

Illustrating interpretable fea-
Figure 3:
tures  replicating in the `1 case the ﬁgure
t (T1  T2) as a
function of T2  when J = 2  and T1 is ﬁxed.
The red and black dots represent the samples
from the P and Q distributions  and the big
black triangle the position of T1 –complete
ﬁgure in supp. mat. F.4.

of [17]. A contour plot ofbtr

4www.kaggle.com/dataﬁniti/fast-food-restaurants

Figure 4: Higgs dataset: Plots of type-II errors against
the test sample size nte.

E

L 1-o pt- M

E

L 1-grid- M

F

C

L 1-o pt-S

F

C

L 1-grid-S

Problem
McDo vs Burger King (1141)
McDo vs Taco Bell (877)
McDo vs Wendy’s (733)
McDo vs Arby’s (517)
McDo vs KFC (429)
Table 2: Fast food dataset: Type-II errors for distinguishing the distribution of fast food restaurants.
↵ = 0.01. J = 3. The number in brackets denotes the sample size of the distribution on the right.
We consider MMD-quad as the gold standard.

0.428
0.710
0.752
0.006
1.00

0.960
0.834
0.942
0.468
0.998

0.112
0.554
0.156
0.000
0.912

0.426
0.624
0.246
0.004
0.990

E -full
M
0.170
0.684
0.416
0.004
0.996

F -full
C
S
0.094
0.638
0.624
0.012
0.856

D -q u a d

M

M
0.184
0.666
0.208
0.004
0.980

8

Table 2 summarizes type-II errors of the tests. Note that it is not clear that distributions must differ  as
two brands sometimes compete directly  and target similar locations. We consider the MMD-quad
as the gold standard to decide whether distributions differ or not. The three cases for which there
seems to be a difference are Mc Donald’s vs Burger King  Mc Donald’s vs Wendy’s  and Mc Donalds
vs Arby’s. Overall  we ﬁnd that the optimized L1-opt-ME agrees best with this gold standard. The
Mc Donald’s vs Arby’s problem seems to be an easy problem  as all tests reach a maximal power 
except for the L1-grid-SCF test which shows the gain of power brought by the optimization. In
the Mc Donald’s vs Wendy’s problem the L1-opt-ME test outperforms the `2 tests and even the
quadratic-time MMD. Finally  all the tests fail to discriminate Mc Donald’s vs KFC. The data provide
no evidence that these brands pursue different strategies to chose locations.
In the Mc Donald’s vs Burger King and Mc Donald’s vs Wendy’s problems  the optimized version of
the test proposed based on mean embedding outperform the grid version. This success implies that
the locations learned are each informative  and we plot them (see supp. mat. Figure 8)  to investigate
the interpretability of the L1-opt-ME test. The ﬁgure shows that the procedure narrows on speciﬁc
regions of the USA to ﬁnd differences between distributions of restaurants.
Real Data 3  text: For a high-dimension problem  we consider the problem of distinguishing the
newsgroups text dataset [18] (details in supp. Mat. F.5). Compared to their `2 counterpart  `1-
optimized tests bring clear beneﬁts and separate all topics of articles based on their word distribution.
Discussion: Our theoretical results suggest it is always beneﬁcial for statistical power to build tests on
`1 norms rather than `2 norm of differences between kernel distribution representatives (Propositions
3.1  3.4). In practice  however  optimizing test locations with `1-norm tests leads to non-smooth
objective functions that are harder to optimize. Our experiments conﬁrm the theoretical beneﬁt of the
`1-based framework. The beneﬁt is particularly pronounced for a large number J of test locations
–as the difference between `1 and `2 norms increases with dimension (see in supp. mat. Lemmas
8  12)– as well as for large dimension of the native space (Figure 2). The beneﬁt of `1 distances
for two-sample testing in high dimension has also been reported by [34]  though their framework
does not link to kernel embeddings or to the convergence of probability measures. Further work
should consider extending these results to goodness-of-ﬁt testing  where the L1 geometry was shown
empirically to provide excellent performance [16].

5 Conclusion

In this paper  we show that statistics derived from the Lp distances between well-chosen distribution
representatives are well suited for the two-sample problem as these distances metrize the weak conver-
gence (Theorem 2.1). We then compare the power of tests introduced in [5] and their `1 counterparts
and we show that `1-based statistics have better power with high probability (Propositions 3.1  3.4).
As with state-of-the-art Euclidean approaches  the framework leads to tractable computations and
learns interpretable locations of where the distributions differ. Empirically  on all 4 synthetic and 3
real problems investigated  the `1 geometry gives clear beneﬁts compared to the Euclidean geometry.
The L1 distance is known to be well suited for densities  to control differences or estimation [7]. It is
also beneﬁcial for kernel embeddings of distributions.

Acknowledgments This work was funded by the DirtyDATA ANR grant (ANR-17-CE23-0018).
We also would like to thank Zoltán Szabó from École Polytechnique for crucial suggestions  and
acknowledge hardware donations from NVIDIA Corporation.

References
[1] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein generative adversarial networks. In

International Conference on Machine Learning  pages 214–223  2017.

[2] P. Baldi  P. Sadowski  and D. Whiteson. Searching for exotic particles in high-energy physics

with deep learning. Nature communications  5:4308  2014.

[3] L. Baringhaus and C. Franz. On a new multivariate two-sample test. Journal of multivariate

analysis  88(1):190–206  2004.

9

[4] K. M. Borgwardt  A. Gretton  M. J. Rasch  H.-P. Kriegel  B. Schölkopf  and A. J. Smola.
Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics 
22(14):e49–e57  2006.

[5] K. P. Chwialkowski  A. Ramdas  D. Sejdinovic  and A. Gretton. Fast two-sample testing with
analytic representations of probability measures. In Advances in Neural Information Processing
Systems  pages 1981–1989  2015.

[6] F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American

mathematical society  39(1):1–49  2002.

[7] L. Devroye and L. Györﬁ. Nonparametric density estimation: The l1 view  1985.

[8] P. Dharmawansa  N. Rajatheva  and K. Ahmed. On the distribution of the sum of nakagami-m

random variables. IEEE transactions on communications  55(7):1407–1416  2007.

[9] M. Fromont  M. Lerasle  P. Reynaud-Bouret  et al. Kernels based tests with non-asymptotic
bootstrap approaches for two-sample problems. In Conference on Learning Theory  page 23 
2012.

[10] K. Fukumizu  A. Gretton  X. Sun  and B. Schölkopf. Kernel measures of conditional dependence.

In Advances in neural information processing systems  pages 489–496  2008.

[11] A. Gretton  K. M. Borgwardt  M. Rasch  B. Schölkopf  and A. J. Smola. A kernel method
for the two-sample-problem. In Advances in neural information processing systems  pages
513–520  2007.

[12] A. Gretton  K. Fukumizu  Z. Harchaoui  and B. K. Sriperumbudur. A fast  consistent kernel
two-sample test. In Advances in neural information processing systems  pages 673–681  2009.

[13] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. Smola. A kernel two-sample

test. Journal of Machine Learning Research  13(Mar):723–773  2012.

[14] A. Gretton  D. Sejdinovic  H. Strathmann  S. Balakrishnan  M. Pontil  K. Fukumizu  and B. K.
Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in neural
information processing systems  page 1205  2012.

[15] Z. Harchaoui  E. Moulines  and F. R. Bach. Testing for homogeneity with kernel Fisher
discriminant analysis. In Advances in Neural Information Processing Systems  page 609  2008.

[16] J. Huggins and L. Mackey. Random feature stein discrepancies.

Information Processing Systems  pages 1899–1909  2018.

In Advances in Neural

[17] W. Jitkrittum  Z. Szabó  K. P. Chwialkowski  and A. Gretton. Interpretable distribution features
with maximum testing power. In Advances in Neural Information Processing Systems  pages
181–189  2016.

[18] K. Lang. Newsweeder: Learning to ﬁlter netnews. In Proceedings of the Twelfth International

Conference on Machine Learning  pages 331–339  1995.

[19] Q. Le  T. Sarlós  and A. Smola. Fastfood-computing hilbert space expansions in loglinear time.

In International Conference on Machine Learning  pages 244–252  2013.

[20] C.-L. Li  W.-C. Chang  Y. Cheng  Y. Yang  and B. Póczos. Mmd gan: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing
Systems  pages 2203–2213  2017.

[21] M. Lichman et al. UCI machine learning repository  2013.

[22] J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In

Advances in Neural Information Processing Systems  pages 829–837  2015.

[23] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 

L. Antiga  and A. Lerer. Automatic differentiation in pytorch. 2017.

10

[24] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems  pages 1177–1184  2008.

[25] A. Ramdas  S. J. Reddi  B. Póczos  A. Singh  and L. A. Wasserman. On the decreasing power
of kernel and distance based nonparametric hypothesis tests in high dimensions. In AAAI  pages
3571–3577  2015.

[26] D. Sejdinovic  B. Sriperumbudur  A. Gretton  and K. Fukumizu. Equivalence of distance-based
and rkhs-based statistics in hypothesis testing. The Annals of Statistics  pages 2263–2291  2013.

[27] B. Simon. Trace ideals and their applications. Number 120. Am. Math. Soc.  2005.
[28] C.-J. Simon-Gabriel and B. Schölkopf. Kernel distribution embeddings: Universal kernels 
characteristic kernels and kernel metrics on distributions. arXiv preprint arXiv:1604.05251 
2016.

[29] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. R. Lanckriet. Hilbert
space embeddings and metrics on probability measures. Journal of Machine Learning Research 
11:1517  2010.

[30] B. K. Sriperumbudur  K. Fukumizu  and G. R. Lanckriet. Universality  characteristic kernels

and rkhs embedding of measures. Journal of Machine Learning Research  12:2389  2011.

[31] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines.

Journal of machine learning research  2(Nov):67–93  2001.

[32] G. J. Székely and M. L. Rizzo. Testing for equal distributions in high dimension. InterStat  5

(16.10):1249–1272  2004.

[33] W. Zaremba  A. Gretton  and M. Blaschko. B-test: A non-parametric  low variance kernel
two-sample test. In Advances in neural information processing systems  pages 755–763  2013.
[34] C. Zhu and X. Shao. Interpoint distance based two sample tests in high dimension. arXiv

preprint arXiv:1902.07279  2019.

11

,meyer scetbon
Gael Varoquaux