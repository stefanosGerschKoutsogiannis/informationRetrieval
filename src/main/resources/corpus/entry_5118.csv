2008,Sparse Online Learning via Truncated Gradient,We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First  the degree of sparsity is continuous---a parameter controls the rate of sparsification from no sparsification to total sparsification. Second  the approach is theoretically motivated  and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees. Finally  the approach works well empirically. We apply it to several datasets and find that for datasets with large numbers of features  substantial sparsity is discoverable.,Sparse Online Learning via Truncated Gradient

John Langford
Yahoo! Research

jl@yahoo-inc.com

Lihong Li

Department of Computer Science

Rutgers University

Tong Zhang

Department of Statistics

Rutgers University

lihong@cs.rutgers.edu

tongz@rci.rutgers.edu

Abstract

We propose a general method called truncated gradient to induce sparsity in the
weights of online-learning algorithms with convex loss. This method has several
essential properties. First  the degree of sparsity is continuous(cid:151)a parameter con-
trols the rate of sparsi(cid:2)cation from no sparsi(cid:2)cation to total sparsi(cid:2)cation. Second 
the approach is theoretically motivated  and an instance of it can be regarded as
an online counterpart of the popular L1-regularization method in the batch set-
ting. We prove small rates of sparsi(cid:2)cation result in only small additional regret
with respect to typical online-learning guarantees. Finally  the approach works
well empirically. We apply it to several datasets and (cid:2)nd for datasets with large
numbers of features  substantial sparsity is discoverable.

1

Introduction

We are concerned with machine learning over large datasets. As an example  the largest dataset
we use in this paper has over 107 sparse examples and 109 features using about 1011 bytes. In this
setting  many common approaches fail  simply because they cannot load the dataset into memory
or they are not suf(cid:2)ciently ef(cid:2)cient. There are roughly two approaches which can work: one is
to parallelize a batch learning algorithm over many machines (e.g.  [3])  the other is to stream the
examples to an online-learning algorithm (e.g.  [2  6]). This paper focuses on the second approach.
Typical online-learning algorithms have at least one weight for every feature  which is too expensive
in some applications for a couple reasons. The (cid:2)rst is space constraints: if the state of the online-
learning algorithm over(cid:3)ows RAM it can not ef(cid:2)ciently run. A similar problem occurs if the state
over(cid:3)ows the L2 cache. The second is test-time constraints: reducing the number of features can
signi(cid:2)cantly reduce the computational time to evaluate a new sample.
This paper addresses the problem of inducing sparsity in learned weights while using an online-
learning algorithm. Natural solutions do not work for our problem. For example  either simply
adding L1 regularization to the gradient of an online weight update or simply rounding small weights
to zero are problematic. However  these two ideas are closely related to the algorithm we propose
and more detailed discussions are found in section 3. A third solution is black-box wrapper ap-
proaches which eliminate features and test the impact of the elimination. These approaches typically
run an algorithm many times which is particularly undesirable with large datasets.
Similar problems have been considered in various settings before. The Lasso algorithm [12] is
commonly used to achieve L1 regularization for linear regression. This algorithm does not work
automatically in an online fashion. There are two formulations of L1 regularization. Consider a
loss function L(w; zi) which is convex in w  where zi = (xi; yi) is an input(cid:150)output pair. One is the
convex-constraint formulation

^w = arg min

w

n

Xi=1

L(w; zi)

subject to kwk1 (cid:20) s;

(1)

where s is a tunable parameter. The other is soft-regularization with a tunable parameter g:

n

^w = arg min

w

L(w; zi) + gkwk1:

(2)

Xi=1

With appropriately chosen g  the two formulations are equivalent. The convex-constraint formu-
lation has a simple online version using the projection idea in [14]. It requires the projection of
weight w into an L1 ball at every online step. This operation is dif(cid:2)cult to implement ef(cid:2)ciently
for large-scale data with many features even if all features are sparse  although important progress
was made recently so the complexity is logarithmic in the number of features [5]. In contrast  the
soft-regularization formulation (2) is ef(cid:2)cient for a batch setting[8] so we pursue it here in an online
setting where it has complexity independent of the number of features. In addition to the L1 regu-
larization formulation (2)  the family of online-learning algorithms we consider also includes some
non-convex sparsi(cid:2)cation techniques.
The Forgetron [4] is an online-learning algorithm that manages memory use. It operates by decay-
ing the weights on previous examples and then rounding these weights to zero when they become
small. The Forgetron is stated for kernelized online algorithms  while we are concerned with the
simple linear setting. When applied to a linear kernel  the Forgetron is not computationally or space
competitive with approaches operating directly on feature weights.
At a high level  our approach is weight decay to a default value. This simple method enjoys strong
performance guarantees (section 3). For instance  the algorithm never performs much worse than a
standard online-learning algorithm  and the additional loss due to sparsi(cid:2)cation is controlled contin-
uously by a single real-valued parameter. The theory gives a family of algorithms with convex loss
functions for inducing sparsity(cid:151)one per online-learning algorithm. We instantiate this for square
loss in section 4 and show how this algorithm can be implemented ef(cid:2)ciently in large-scale prob-
lems with sparse features. For such problems  truncated gradient enjoys the following properties:
(i) It is computationally ef(cid:2)cient: the number of operations per online step is linear in the number
of nonzero features  and independent of the total number of features; (ii) It is memory ef(cid:2)cient: it
maintains a list of active features  and can insert (when the corresponding weight becomes nonzero)
and delete (when the corresponding weight becomes zero) features dynamically.
Theoretical results stating how much sparsity is achieved using this method generally require addi-
tional assumptions which may or may not be met in practice. Consequently  we rely on experiments
in section 5 to show truncated gradient achieves good sparsity in practice. We compare truncated
gradient to a few others on small datasets  including the Lasso  online rounding of coef(cid:2)cients to
zero  and L1-regularized subgradient descent. Details of these algorithms are given in section 3.

2 Online Learning with Stochastic Gradient Descent

We are interested in the standard sequential prediction problems where for i = 1; 2; : : ::

1. An unlabeled example xi arrives.
2. We make a prediction ^yi based on the current weights wi = [w1
3. We observe yi  let zi = (xi; yi)  and incur some known loss L(wi; zi) convex in wi.
4. We update weights according to some rule: wi+1   f (wi).
We want an update rule f allows us to bound the sum of losses Pt

i=1 L(wi; zi)  as well as achieving
sparsity. For this purpose  we start with the standard stochastic gradient descent (SGD) rule  which
is of the form:

i ; : : : ; wd

i ] 2 Rd.

f (wi) = wi (cid:0) (cid:17)r1L(wi; zi);

(3)
where r1L(a; b) is a subgradient of L(a; b) with respect to the (cid:2)rst variable a. The parameter (cid:17) > 0
is often referred to as the learning rate. In the analysis  we only consider constant learning rate for
simplicity. In theory  it might be desirable to have a decaying learning rate (cid:17)i which becomes smaller
when i increases to get the so-called no-regret bound without knowing T in advance. However  if
T is known in advance  one can select a constant (cid:17) accordingly so the regret vanishes as T ! 1.
Since the focus of the present paper is on weight sparsity  rather than choosing the learning rate  we
use a constant learning rate in the analysis because it leads to simpler bounds.

The above method has been widely used in online learning (e.g.  [2  6]). Moreover  it is argued
to be ef(cid:2)cient even for solving batch problems where we repeatedly run the online algorithm over
training data multiple times. For example  the idea has been successfully applied to solve large-scale
standard SVM formulations [10  13]. In the scenario outlined in the introduction  online-learning
methods are more suitable than some traditional batch learning methods. However  the learning rule
(3) itself does not achieve sparsity in the weights  which we address in this paper. Note that variants
of SGD exist in the literature  such as exponentiated gradient descent (EG) [6]. Since our focus is
sparsity  not SGD vs. EG  we shall only consider modi(cid:2)cations of (3) for simplicity.

3 Sparse Online Learning

In this section  we (cid:2)rst examine three methods for achieving sparsity in online learning  including
a novel algorithm called truncated gradient. As we shall see  all these ideas are closely related.
Then  we provide theoretical justi(cid:2)cations for this algorithm  including a general regret bound and
a fundamental connection to the Lasso.

3.1 Simple Coef(cid:2)cient Rounding

In order to achieve sparsity  the most natural method is to round small coef(cid:2)cients (whose magni-
tudes are below a threshold (cid:18) > 0) to zero after every K online steps. That is  if i=K is not an
integer  we use the standard SGD rule (3); if i=K is an integer  we modify the rule as:

f (wi) = T0(wi (cid:0) (cid:17)r1L(wi; zi); (cid:18));

(4)
where: (cid:18) (cid:21) 0 is a threshold  T0(v; (cid:18)) = [T0(v1; (cid:18)); : : : ; T0(vd; (cid:18))] for vector v = [v1; : : : ; vd] 2 Rd 
T0(vj; (cid:18)) = vjI(jvjj < (cid:18))  and I((cid:1)) is the set-indicator function. In other words  we (cid:2)rst perform a
standard stochastic gradient descent  and then round the coef(cid:2)cients to zero. The effect is to remove
nonzero and small weights. In general  we should not take K = 1  especially when (cid:17) is small  since
in each step wi is modi(cid:2)ed by only a small amount. If a coef(cid:2)cient is zero  it remains small after
one online update  and the rounding operation pulls it back to zero. Consequently  rounding can be
done only after every K steps (with a reasonably large K); in this case  nonzero coef(cid:2)cients have
suf(cid:2)cient time to go above the threshold (cid:18). However  if K is too large  then in the training stage  we
must keep many more nonzero features in the intermediate steps before they are rounded to zero. In
the extreme case  we may simply round the coef(cid:2)cients in the end  which does not solve the storage
problem in the training phase at all. The sensitivity in choosing appropriate K is a main drawback of
this method; another drawback is the lack of theoretical guarantee for its online performance. These
issues motivate us to consider more principled solutions.

3.2 L1-Regularized Subgradient

In the experiments  we also combined rounding-in-the-end-of-training with a simple online subgra-
dient method for L1 regularization with a regularization parameter g > 0:
f (wi) = wi (cid:0) (cid:17)r1L(wi; zi) (cid:0) (cid:17)g sgn(wi);

(5)

where for a vector v = [v1; : : : ; vd]  sgn(v) = [sgn(v1); : : : ; sgn(vd)]  and sgn(vj) = 1 if vj > 0 
sgn(vj) = (cid:0)1 if vj < 0  and sgn(vj) = 0 if vj = 0. In the experiments  the online method (5)
with rounding in the end is used as a simple baseline. Notice this method does not produce sparse
weights online simply because only in very rare cases do two (cid:3)oats add up to 0. Therefore  it is not
feasible in large-scale problems for which we cannot keep all features in memory.

3.3 Truncated Gradient

In order to obtain an online version of the simple rounding rule in (4)  we observe that direct round-
ing to zero is too aggressive. A less aggressive version is to shrink the coef(cid:2)cient to zero by a
smaller amount. We call this idea truncated gradient  where the amount of shrinkage is controlled
by a gravity parameter gi > 0:

f (wi) = T1(wi (cid:0) (cid:17)r1L(wi; zi); (cid:17)gi; (cid:18));

(6)

where for a vector v = [v1; : : : ; vd] 2 Rd  and a scalar g (cid:21) 0  T1(v; (cid:11); (cid:18)) =
[T1(v1; (cid:11); (cid:18)); : : : ; T1(vd; (cid:11); (cid:18))]  with

T1(vj; (cid:11); (cid:18)) =8<
:

max(0; vj (cid:0) (cid:11))
min(0; vj + (cid:11))
vj

if vj 2 [0; (cid:18)]
if vj 2 [(cid:0)(cid:18); 0]
otherwise

:

Again  the truncation can be performed every K online steps. That is  if i=K is not an integer  we
let gi = 0; if i=K is an integer  we let gi = Kg for a gravity parameter g > 0. The reason for
doing so (instead of a constant g) is that we can perform a more aggressive truncation with gravity
parameter Kg after each K steps. This can potentially lead to better sparsity. We also note that when
(cid:17)Kg (cid:21) (cid:18)  truncate gradient coincides with (4). But in practice  as is also veri(cid:2)ed by the theory  one
should adopt a small g; hence  the new learning rule (6) is expected to differ from (4).
In general  the larger the parameters g and (cid:18) are  the more sparsity is expected. Due to the extra
truncation T1  this method can lead to sparse solutions  which is con(cid:2)rmed empirically in section 5.
A special case  which we use in the experiment  is to let g = (cid:18) in (6). In this case  we can use only
one parameter g to control sparsity. Since (cid:17)Kg (cid:28) (cid:18) when (cid:17)K is small  the truncation operation
is less aggressive than the rounding in (4). At (cid:2)rst sight  the procedure appears to be an ad-hoc
way to (cid:2)x (4). However  we can establish a regret bound (in the next subsection) for this method 
showing it is theoretically sound. Therefore  it can be regarded as a principled variant of rounding.
Another important special case of (6) is setting (cid:18) = 1  in which all weight components shrink in
every online step. The method is a modi(cid:2)cation of the L1-regularized subgradient descent rule (5).
The parameter gi (cid:21) 0 controls the sparsity achieved with the algorithm  and setting gi = 0 gives
exactly the standard SGD rule (3). As we show in section 3.5  this special case of truncated gradient
can be regarded as an online counterpart of L1 regularization since it approximately solves an L1
regularization problem in the limit of (cid:17) ! 0. We also show the prediction performance of trun-
cated gradient  measured by total loss  is comparable to standard stochastic gradient descent while
introducing sparse weight vectors.

3.4 Regret Analysis
Throughout the paper  we use k (cid:1) k1 for 1-norm  and k (cid:1) k for 2-norm. For reference  we make the
following assumption regarding the loss function:
Assumption 3.1 We assume L(w; z) is convex in w  and there exist non-negative constants A and
B such that (r1L(w; z))2 (cid:20) AL(w; z) + B for all w 2 Rd and z 2 Rd+1.
For linear prediction problems  we have a general loss function of the form L(w; z) = (cid:30)(wT x; y).
The following are some common loss functions (cid:30)((cid:1);(cid:1)) with corresponding choices of parameters A
and B (which are not unique)  under the assumption supx kxk (cid:20) C. All of them can be used for
binary classi(cid:2)cation where y 2 (cid:6)1  but the last one is more often used in regression where y 2 R:
Logistic: (cid:30)(p; y) = ln(1 + exp((cid:0)py))  with A = 0 and B = C 2; SVM (hinge loss): (cid:30)(p; y) =
max(0; 1 (cid:0) py)  with A = 0 and B = C 2; Least squares (square loss): (cid:30)(p; y) = (p (cid:0) y)2  with
A = 4C 2 and B = 0.
The main result is Theorem 3.1 which is parameterized by A and B. The proof will be provided in
a longer paper.

Theorem 3.1 (Sparse Online Regret) Consider sparse online update rule (6) with w1 = [0; : : : ; 0]
and (cid:17) > 0. If Assumption 3.1 holds  then for all (cid:22)w 2 Rd we have

T

1 (cid:0) 0:5A(cid:17)

T

(cid:17)
2

(cid:20)

gi

1 (cid:0) 0:5A(cid:17)kwi+1 (cid:1) I(wi+1 (cid:20) (cid:18))k1(cid:21)
[L( (cid:22)w; zi) + gik (cid:22)w (cid:1) I(wi+1 (cid:20) (cid:18))k1];

Xi=1(cid:20)L(wi; zi) +
B + k (cid:22)wk2
Xi=1
2(cid:17)T
jj (cid:20) (cid:18)) for vectors v = [v1; : : : ; vd] and v0 = [v0

j=1 jvjjI(jv0

+

1
T

T

where kv(cid:1)I(jv0j (cid:20) (cid:18))k1 =Pd

1; : : : ; v0

d].

The theorem is stated with a constant learning rate (cid:17). As mentioned earlier  it is possible to obtain
a result with variable learning rate where (cid:17) = (cid:17)i decays as i increases. Although this may lead to a
no-regret bound without knowing T in advance  it introduces extra complexity to the presentation of
the main idea. Since the focus is on sparsity rather than optimizing learning rate  we do not include
such a result for clarity. If T is known in advance  then in the above bound  one can simply take
(cid:17) = O(1=pT ) and the regret is of order O(1=pT ).
In the above theorem  the right-hand side involves a term gik (cid:22)w (cid:1) I(wi+1 (cid:20) (cid:18))k1 that depends on
wi+1 which is not easily estimated. To remove this dependency  a trivial upper bound of (cid:18) = 1
can be used  leading to L1 penalty gik (cid:22)wk1. In the general case of (cid:18) < 1  we cannot remove the
wi+1 dependency because the effective regularization condition (as shown on the left-hand side)
is the non-convex penalty gikw (cid:1) I(jwj (cid:20) (cid:18))k1. Solving such a non-convex formulation is hard
both in the online and batch settings. In general  we only know how to ef(cid:2)ciently discover a local
minimum which is dif(cid:2)cult to characterize. Without a good characterization of the local minimum 
it is not possible for us to replace gik (cid:22)w (cid:1) I(wi+1 (cid:20) (cid:18))k1 on the right-hand side by gik (cid:22)w (cid:1) I( (cid:22)w (cid:20)
(cid:18))k1 because such a formulation implies we could ef(cid:2)ciently solve a non-convex problem with a
simple online update rule. Still  when (cid:18) < 1  one naturally expects the right-hand side penalty
gik (cid:22)w (cid:1) I(wi+1 (cid:20) (cid:18))k1 is much smaller than the corresponding L1 penalty gik (cid:22)wk1  especially when
wj has many components close to 0. Therefore the situation with (cid:18) < 1 can potentially yield better
performance on some data.
Theorem 3.1 also implies a tradeoff between sparsity and regret performance. We may simply
consider the case where gi = g is a constant. When g is small  we have less sparsity but the regret
term gk (cid:22)w (cid:1) I(wi+1 (cid:20) (cid:18))k1 (cid:20) gk (cid:22)wk1 on the right-hand side is also small. When g is large  we are
able to achieve more sparsity but the regret gk (cid:22)w(cid:1)I(wi+1 (cid:20) (cid:18))k1 on the right-hand side also becomes
large. Such a tradeoff between sparsity and prediction accuracy is empirically studied in section 5 
where we achieve signi(cid:2)cant sparsity with only a small g (and thus small decrease of performance).
Now consider the case (cid:18) = 1 and gi = g. When T ! 1  if we let (cid:17) ! 0 and (cid:17)T ! 1  then

1
T

T

Xi=1

[L(wi; zi) + gkwik1] (cid:20) inf

(cid:22)w2Rd" 1

T

T

Xi=1

L( (cid:22)w; zi) + 2gk (cid:22)wk1# + o(1):

In other words  if we let L0(w; z) = L(w; z) + gkwk1 be the L1-
follows from Theorem 3.1.
regularized loss  then the L1-regularized regret is small when (cid:17) ! 0 and T ! 1. This implies
truncated gradient can be regarded as the online counterpart of L1-regularization methods. In the
stochastic setting where the examples are drawn iid from some underlying distribution  the sparse
online gradient method proposed in this paper solves the L1 regularization problem.

3.5 Stochastic Setting

SGD-based online-learning methods can be used to solve large-scale batch optimization problems.
In this setting  we can go through training examples one-by-one in an online fashion  and repeat
multiple times over the training data. To simplify the analysis  instead of assuming we go through
example one by one  we assume each additional example is drawn from the training data randomly
with equal probability. This corresponds to the standard stochastic optimization setting  in which
observed samples are iid from some underlying distributions. The following result is a simple conse-
quence of Theorem 3.1. For simplicity  we only consider the case with (cid:18) = 1 and constant gravity
gi = g. The expectation E is taken over sequences of indices i1; : : : ; iT .
Theorem 3.2 (Stochastic Setting) Consider a set of training data zi = (xi; yi) for 1 (cid:20) i (cid:20) n. Let

n

1
n

R(w; g) =

Xi=1
wt+1 = T (wt (cid:0) (cid:17)r1(wt; zit ); g(cid:17));

be the L1-regularized loss over training data. Let ^w1 = w1 = 0  and de(cid:2)ne recursively for t (cid:21) 1:
where each it is drawn from f1; : : : ; ng uniformly at random. If Assumption 3.1 holds  then for all
T and (cid:22)w 2 Rd:
E(cid:20)(1 (cid:0) 0:5A(cid:17))R( ^wT ;

^wt+1 = ^wt + (wt+1 (cid:0) ^wt)=(t + 1);

)(cid:21) (cid:20) E" 1 (cid:0) 0:5A(cid:17)

)# (cid:20)

k (cid:22)wk2
2(cid:17)T

1 (cid:0) 0:5A(cid:17)

1 (cid:0) 0:5A(cid:17)

+R( (cid:22)w; g):

(cid:17)
2

B +

R(wi;

T

g

g

T

Xi=1

L(w; zi) + gkwk1

Observe that if we let (cid:17) ! 0 and (cid:17)T ! 1  the bound in Theorem 3.2 becomes E [R( ^wT ; g)] (cid:20)
t=1 R(wt; g)i (cid:20) inf (cid:22)w R( (cid:22)w; g) + o(1): In other words  on average ^wT approximately solves
Eh 1
T PT
the batch L1-regularization problem inf w(cid:2) 1

If we
choose a random stopping time T   then the above inequalities say that on average wT also solves
this L1-regularization problem approximately. Thus  we use the last solution wT instead of the ag-
gregated solution ^wT in experiments. Since L1 regularization is often used to achieve sparsity in the
batch learning setting  the connection of truncated gradient to L1 regularization can be regarded as
an alternative justi(cid:2)cation for the sparsity ability of this algorithm.

nPn

i=1 L(w; zi) + gkwk1(cid:3) when T is large.

4 Ef(cid:2)cient Implementation of Truncated Gradient for Square Loss

leading to f (wi) = T1(wi + 2(cid:17)(yi (cid:0) ^yi)xi; (cid:17)gi; (cid:18))  where the prediction is given by ^yi =Pj wj

The truncated descent update rule (6) can be applied to least-squares regression using square loss 
i xj
i .
We altered an ef(cid:2)cient SGD implementation  Vowpal Wabbit [7]  for least-squares regression
according to truncated gradient. The program operates in an entirely online fashion. Features are
hashed instead of being stored explicitly  and weights can be easily inserted into or deleted from the
table dynamically. So the memory footprint is essentially just the number of nonzero weights  even
when the total numbers of data and features are astronomically large.
In many online-learning situations such as web applications  only a small subset of the features
have nonzero values for any example x. It is thus desirable to deal with sparsity only in this small
subset rather than in all features  while simultaneously inducing sparsity on all feature weights. The
approach we take is to store a time-stamp (cid:28)j for each feature j. The time-stamp is initialized to the
index of the example where feature j becomes nonzero for the (cid:2)rst time. During online learning  at
each step i  we only go through the nonzero features j of example i  and calculate the un-performed
shrinkage of wj between (cid:28)j and the current time i. These weights are then updated  and their time
stamps are reset to i. This lazy-update idea of delaying the shrinkage calculation until needed is
the key to ef(cid:2)cient implementation of truncated gradient. The implementation satis(cid:2)es ef(cid:2)ciency
requirements outlined at the end of the introduction section. A similar time-stamp trick can be
applied to the other two algorithms given in section 3.

5 Empirical Results

We applied the algorithm  with the ef(cid:2)ciently implemented sparsify option  as described in the
previous section  to a selection of datasets  including eleven datasets from the UCI repository [1] 
the much larger dataset rcv1 [9]  and a private large-scale dataset Big_Ads related to ad interest
prediction. While UCI datasets are useful for benchmark purposes  rcv1 and Big_Ads are more
interesting since they embody real-world datasets with large numbers of features  many of which
are less informative for making predictions than others. The UCI datasets we used do not have
many features  and it seems a large fraction of these features are useful for making predictions. For
comparison purposes and to better demonstrate the behavior of our algorithm  we also added 1000
random binary features to those datasets. Each feature has value 1 with prob. 0:05 and 0 otherwise.
In the (cid:2)rst set of experiments  we are interested in how much reduction in the number of features is
possible without affecting learning performance signi(cid:2)cantly; speci(cid:2)cally  we require the accuracy
be reduced by no more than 1% for classi(cid:2)cation tasks  and the total square loss be increased by no
more than 1% for regression tasks. As common practice  we allowed the algorithm to run on the
training data set for multiple passes with decaying learning rate. For each dataset  we performed 10-
fold cross validation over the training set to identify the best set of parameters  including the learning
rate (cid:17)  the gravity g  number of passes of the training set  and the decay of learning rate across these
passes. This set of parameters was then used on the whole training set. Finally  the learned classi-
(cid:2)er/regressor was evaluated on the test set. We (cid:2)xed K = 1 and (cid:18) = 1 in this set of experiments.
The effects of K and (cid:18) are included in an extended version of this paper. Figure 1 shows the fraction
of reduced features after sparsi(cid:2)cation is applied to each dataset. For UCI datasets with randomly
added features  truncated gradient was able to reduce the number of features by a fraction of more
than 90%  except for the ad dataset in which only 71% reduction was observed. This less satisfying
result might be improved by a more extensive parameter search in cross validation. However  if

Base data
1000 extra
 1

t
f
e
L
 
n
o
i
t
c
a
r
F

 0.8

 0.6

 0.4

 0.2

 0

d
a

x
r
c

p
k
s
v
r
k

i

g
n
s
u
o
h

i

4
0
c
g
a
m

m
o
o
r
h
s

m
a
p
s

c
b
w

c
b
d
w

c
b
p
w

o
o
z

1
v
c
r

Dataset

s
d
A
_
g
B

i

Fraction of Features Left

Base data
1000 extra

Ratio of AUC

o
i
t
a
R

 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0

d
a

x
r
c

p
k
s
v
r
k

i

4
0
c
g
a
m

m
o
o
r
h
s

m
a
p
s

c
b
w

c
b
d
w

c
b
p
w

1
v
c
r

Dataset

Figure 1: Left: the amount of features left after sparsi(cid:2)cation for each dataset without 1% perfor-
mance loss. Right: the ratio of AUC with and without sparsi(cid:2)cation.
we tolerated 1:3% decrease in accuracy (instead of 1%) during cross validation  truncated gradient
was able to achieve 91:4% reduction  indicating a large reduction is still possible at the tiny addi-
tional accuracy loss of 0:3%. Even for the original UCI datasets without arti(cid:2)cially added features 
some of the less useful features were removed while the same level of performance was maintained.
For classi(cid:2)cation tasks  we also studied how truncated gradient affects AUC (Area Under the ROC
Curve)  a standard metric for classi(cid:2)cation. We use AUC here because it is insensitive to threshold 
unlike accuracy. Using the same sets of parameters from 10-fold cross validation described above 
we found the criterion was not affected signi(cid:2)cantly by sparsi(cid:2)cation and in some cases  it was ac-
tually improved  due to removal of some irrelevant features. The ratios of the AUC with and without
sparsi(cid:2)cation for all classi(cid:2)cation tasks are plotted in Figure 1. Often these ratios are above 98%.
The previous results do not exercise the full power of the approach presented here because they are
applied to datasets where the standard Lasso is computationally viable. We have also applied this
approach to a large non-public dataset Big_Ads where the goal is predicting which of two ads was
clicked on given context information (the content of ads and query information). Here  accepting a
0:9% increase in classi(cid:2)cation error allows us to reduce the number of features from about 3 (cid:2) 109
to about 24 (cid:2) 106(cid:151)a factor of 125 decrease in the number of features.
The next set of experiments compares truncated gradient to other algorithms regarding their abilities
to tradeoff feature sparsi(cid:2)cation and performance. Again  we focus on the AUC metric in UCI
classi(cid:2)cation tasks. The algorithms for comparison include: (i) the truncated gradient algorithm with
K = 10 and (cid:18) = 1; (ii) the truncated gradient algorithm with K = 10 and (cid:18) = g; (iii) the rounding
algorithm with K = 10; (iv) the L1-regularized subgradient algorithm with K = 10; and (v) the
Lasso [12] for batch L1 regularization (a publicly available implementation [11] was used). We have
chosen K = 10 since it worked better than K = 1  and this choice was especially important for the
coef(cid:2)cient rounding algorithm. All unspeci(cid:2)ed parameters were identi(cid:2)ed using cross validation.
Note that we do not attempt to compare these algorithms on rcv1 and Big_Ads simply because their
sizes are too large for the Lasso and subgradient descent. Figure 2 gives the results on datasets
ad and spambase. Results on other datasets were qualitatively similar. On all datasets  truncated
gradient (with (cid:18) = 1) is consistently competitive with the other online algorithms and signi(cid:2)cantly
outperformed them in some problems  implying truncated gradient is generally effective. Moreover 
truncated gradient with (cid:18) = g behaves similarly to rounding (and sometimes better). This was
expected as truncated gradient with (cid:18) = g can be regarded as a principled variant of rounding with
valid theoretical justi(cid:2)cation. It is also interesting to observe the qualitative behavior of truncated
gradient was often similar to LASSO  especially when very sparse weight vectors were allowed
(the left sides in the graphs). This is consistent with theorem 3.2 showing the relation between
these two algorithms. However  LASSO usually performed worse when the allowed number of
nonzero weights was large (the right side of the graphs). In this case  LASSO seemed to over(cid:2)t
while truncated gradient was more robust to over(cid:2)tting. The robustness of online learning is often
attributed to early stopping  which has been extensively studied (e.g.  in [13]).
Finally  it is worth emphasizing that these comparison experiments shed some light on the relative
strengths of these algorithms in terms of feature sparsi(cid:2)cation  without considering which one can
be ef(cid:2)ciently implemented. For large datasets with sparse features  only truncated gradient and the
ad hoc coef(cid:2)cient rounding algorithm are applicable.

C
U
A

1

0.9

0.8

0.7

0.6

0.5

0.4

 

0.3
100

ad

 

Trunc. Grad. (q=¥)
Trunc. Grad. (q=g)
Rounding
Sub−gradient
Lasso
102

103

101
Number of Features

C
U
A

1

0.9

0.8

0.7

0.6

0.5

0.4

 

0.3
100

spambase

 

Trunc. Grad. (q=¥)
Trunc. Grad. (q=g)
Rounding
Sub−gradient
Lasso
102

103

101
Number of Features

Figure 2: Comparison of the (cid:2)ve algorithms in two sample UCI datasets.

6 Conclusion

This paper covers the (cid:2)rst ef(cid:2)cient sparsi(cid:2)cation technique for large-scale online learning with
strong theoretical guarantees. The algorithm  truncated gradient  is the natural extension of Lasso-
style regression to the online-learning setting. Theorem 3.1 proves the technique is sound: it never
harms performance much compared to standard stochastic gradient descent in adversarial situations.
Furthermore  we show the asymptotic solution of one instance of the algorithm is essentially equiv-
alent to Lasso regression  thus justifying the algorithm’s ability to produce sparse weight vectors
when the number of features is intractably large. The theorem is veri(cid:2)ed experimentally in a num-
ber of problems. In some cases  especially for problems with many irrelevant features  this approach
achieves a one or two orders of magnitude reduction in the number of features.

References
[1] A. Asuncion and D.J. Newman. UCI machine learning repository  2007. UC Irvine.
[2] N. Cesa-Bianchi  P.M. Long  and M. Warmuth. Worst-case quadratic loss bounds for prediction using

linear functions and gradient descent. IEEE Transactions on Neural Networks  7(3):604(cid:150)619  1996.

[3] C.-T. Chu  S.K. Kim  Y.-A. Lin  Y. Yu  G. Bradski  A.Y. Ng  and K. Olukotun. Map-reduce for machine
learning on multicore. In Advances in Neural Information Processing Systems 20  pages 281(cid:150)288  2008.
[4] O. Dekel  S. Shalev-Schwartz  and Y. Singer. The Forgetron: A kernel-based perceptron on a (cid:2)xed budget.

In Advances in Neural Information Processing Systems 18  pages 259(cid:150)266  2006.

[5] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Ef(cid:2)cient projections onto the ‘1-ball for learning

in high dimensions. In Proceedings of ICML-08  pages 272(cid:150)279  2008.

[6] J. Kivinen and M.K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.

Information and Computation  132(1):1(cid:150)63  1997.

[7] J. Langford  L. Li  and A.L. Strehl. Vowpal Wabbit (fast online learning)  2007. http://hunch.net/(cid:24)vw/.
[8] Honglak Lee  Alexis Batle  Rajat Raina  and Andrew Y. Ng. Ef(cid:2)cient sparse coding algorithms.
In

Advances in Neural Information Processing Systems 19 (NIPS-07)  2007.

[9] D.D. Lewis  Y. Yang  T.G. Rose  and F. Li. RCV1: A new benchmark collection for text categorization

research. Journal of Machine Learning Research  5:361(cid:150)397  2004.

[10] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.

In Proceedings of ICML-07  pages 807(cid:150)814  2007.

[11] K. Sj(cid:246)strand. Matlab implementation of LASSO  LARS  the elastic net and SPCA  June 2005. Version

2.0  http://www2.imm.dtu.dk/pubdb/p.php?3897.

[12] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

B.  58(1):267(cid:150)288  1996.

[13] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In

Proceedings of ICML-04  pages 919(cid:150)926  2004.

[14] M. Zinkevich. Online convex programming and generalized in(cid:2)nitesimal gradient ascent. In Proceedings

of ICML-03  pages 928(cid:150)936  2003.

,Mikhail Figurnov
Shakir Mohamed
Andriy Mnih