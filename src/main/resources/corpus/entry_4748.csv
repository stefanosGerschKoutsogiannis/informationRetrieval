2010,Robust Clustering as Ensembles of Affinity Relations,In this paper  we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations. The average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efficient procedure to solve this optimization problem  and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters  leaving other points un-grouped; thus it is inherently robust to large numbers of outliers  which has seriously limited the applicability of classical methods. Our method also provides a unified solution to clustering from k-ary affinity relations with k ≥ 2  that is  it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem  especially when there exists a large number of outliers.,Robust Clustering as Ensembles of Afﬁnity Relations

Hairong Liu1  Longin Jan Latecki2  Shuicheng Yan1

1Department of Electrical and Computer Engineering  National University of Singapore  Singapore

2Department of Computer and Information Sciences  Temple University  Philadelphia  USA

lhrbss@gmail.com latecki@temple.edu eleyans@nus.edu.sg

Abstract

In this paper  we regard clustering as ensembles of k-ary afﬁnity relations and
clusters correspond to subsets of objects with maximal average afﬁnity relations.
The average afﬁnity relation of a cluster is relaxed and well approximated by a
constrained homogenous function. We present an efﬁcient procedure to solve this
optimization problem  and show that the underlying clusters can be robustly re-
vealed by using priors systematically constructed from the data. Our method can
automatically select some points to form clusters  leaving other points un-grouped;
thus it is inherently robust to large numbers of outliers  which has seriously limited
the applicability of classical methods. Our method also provides a uniﬁed solu-
tion to clustering from k-ary afﬁnity relations with k ≥ 2  that is  it applies to both
graph-based and hypergraph-based clustering problems. Both theoretical analysis
and experimental results show the superiority of our method over classical solu-
tions to the clustering problem  especially when there exists a large number of
outliers.

1 Introduction
Data clustering is a fundamental problem in many ﬁelds  such as machine learning  data mining and
computer vision [1]. Unfortunately  there is no universally accepted deﬁnition of a cluster  probably
because of the diverse forms of clusters in real applications. But it is generally agreed that the objects
belonging to a cluster satisfy certain internal coherence condition  while the objects not belonging
to a cluster usually do not satisfy this condition.
Most of existing clustering methods are partition-based  such as k-means [2]  spectral clustering
[3  4  5] and afﬁnity propagation [6]. These methods implicitly share an assumption: every data
point must belong to a cluster. This assumption greatly simpliﬁes the problem  since we do not
need to judge whether a data point is an outlier or not  which is very challenging. However  this
assumption also results in bad performance of these methods when there exists a large number of
outliers  as frequently met in many real-world applications.
The criteria to judge whether several objects belong to the same cluster or not are typically expressed
by pairwise relations  which is encoded as the weights of an afﬁnity graph. However  in many
applications  high order relations are more appropriate  and may even be the only choice  which
naturally results in hyperedges in hypergraphs. For example  when clustering a given set of points
into lines  pairwise relations are not meaningful  since every pair of data points trivially deﬁnes
a line. However  for every three data points  whether they are near collinear or not conveys very
important information.
As graph-based clustering problem has been well studied  many researchers tried to deal with
hypergraph-based clustering by using existing graph-based clustering methods. One direction is
to transform a hypergraph into a graph  whose edge-weights are mapped from the weights of the
original hypergraph. Zien et. al. [7] proposed two approaches called “clique expansion” and “star
expansion”  respectively  for such a purpose. Rodriguez [8] showed the relationship between the

1

spectral properties of the Laplacian matrix of the resulting graph and the minimum cut of the orig-
inal hypergraph. Agarwal et al. [9] proposed the “clique averaging” method and reported better
results than “clique expansion” method. Another direction is to generalize graph-based clustering
method to hypergraphs. Zhou et al. [10] generalized the well-known “normalized cut” method [5]
and deﬁned a hypergraph normalized cut criterion for a k-partition of the vertices. Shashua et al.
[11] cast the clustering problem with high order relations into a nonnegative factorization problem
of the closest hyper-stochastic version of the input afﬁnity tensor.
Based on game theory  Bulo and Pelillo [12] proposed to consider the hypergraph-based clustering
problem as a multi-player non-cooperative “clustering game” and solve it by replicator equation 
which is in fact a generalization of their previous work [13]. This new formulation has a solid
theoretical foundation  possesses several appealing properties  and achieved state-of-art results. This
method is in fact a speciﬁc case of our proposed method  and we will discuss this point in Section 2.
In this paper  we propose a uniﬁed method for clustering from k-ary afﬁnity relations  which is
applicable to both graph-based and hypergraph-based clustering problems. Our method is motivated
by an intuitive observation: for a cluster with m objects  there may exist (m
k ) possible k-ary afﬁnity
relations  and most of these (sometimes even all) k-ary afﬁnity relations should agree with each
other on the same criterion. For example  in the line clustering problem  for m points on the same
line  there are (m
3 ) possible triplets  and all these triplets should satisfy the criterion that they lie on
a line. The ensemble of such large number of afﬁnity relations is hardly produced by outliers and is
also very robust to noises  thus yielding a robust mechanism for clustering.

2 Formulation
Clustering from k-ary afﬁnity relations can be intuitively described as clustering on a special kind
of edge-weighted hypergraph  k-graph. Formally  a k-graph is a triplet G = (V  E  w)  where
V = {1 ···   n} is a ﬁnite set of vertices  with each vertex representing an object  E ⊆ V k is the
set of hyperedges  with each hyperedge representing a k-ary afﬁnity relation  and w : E → R is a
weighting function which associates a real value (can be negative) with each hyperedge  with larger
weights representing stronger afﬁnity relations. We only consider the k-ary afﬁnity relations with no
duplicate objects  that is  the hyperedges among k different vertices. For hyperedges with duplicated
vertices  we simply set their weights to zeros.
}|
z
Each hyperedge e ∈ E involves k vertices  thus can be represented as k-tuple {v1 ···   vk}. The
n × n × ··· × n super-symmetry array  denoted by M 
w({v1 ···   vk})

{
if {v1 ···   vk} ∈ E 
Note that each edge {v1 ···   vk} ∈ E has k! duplicate entries in the array M.
For a subset U ⊆ V with m vertices  its edge set is denoted as EU . If U is really a cluster  then
most of hyperedges in EU should have large weights. The simplest measure to reﬂect such ensemble
phenomenon is the sum of all entries in M whose corresponding hyperedges contain only vertices
in U  which can be expressed as:

weighted adjacency array of graph G is an
and deﬁned as

M (v1 ···   vk) =

{

else 

(1)

k

0

∑

S(U ) =

v1;···;vk∈U

M (v1 ···   vk).

(2)

(3)

∑

i yi = m and

Suppose y is an n × 1 indicator vector of the subset U  such that yvi = 1 if vi ∈ U and zero
otherwise  then S(U ) can be expressed as:

∑

z

{
}|
··· yvk .

k

M (v1 ···   vk)

yv1

S(U ) = S(y) =

v1;···;vk∈V

Obviously  S(U ) usually increases as the number of vertices in U increases. Since
there are mk summands in S(U )  the average of these entries can be expressed as:

Sav(U ) =

1
mk S(y)

2

∑

1
mk

v1;···;vk∈V

∑
∑

v1;···;vk∈V

k

yv1

}|
{
z
··· yvk
{
}|
{
}|
··· xvk  

··· yvk
m

k

k

M (v1 ···   vk)
z
z

yv1
m

M (v1 ···   vk)

M (v1 ···   vk)

xv1

=

=

=

∑

The reward at vertex i  denoted by ri(x)  is deﬁned as follows:

ri(x) =

v1;···;vk−1∈V
Since M is a super-symmetry array  then @f (x)
@xi
of f (x) at x.

= kri(x)  i.e.  ri(x) is proportional to the gradient

3

∑

(4)

v1;···;vk∈V
i xi = 1 is a natural constraint over x.

{

∏

∑

k
i=1 xvi  

i yi = m 

where x = y/m. As
Intuitively  when U is a true cluster  Sav(U ) should be relatively large. Thus  the clustering problem
corresponds to the problem of maximizing Sav(U ). In essence  this is a combinatorial optimization
problem  since we know neither m nor which m objects to select. As this problem is NP-hard  to
reduce its complexity  we relax x to be within a continuous range [0  ε]  where ε ≤ 1 is a constant 
while keeping the constraint

i xi = 1. Then the problem becomes:

subject to x ∈ ∆n and xi ∈ [0  ε]

∑
v1;···;vk∈V M (v1 ···   vk)
∑
(5)
i xi = 1} is the standard simplex in Rn. Note that Sav(x) is

max f (x) =
where ∆n = {x ∈ Rn : x ≥ 0 and
abbreviated by f (x) to simplify the formula.
The adoption of ℓ1-norm in (5) not only let xi have an intuitive probabilistic meaning  that is  xi
represents the probability for the cluster contain the i-th object  but also makes the solution sparse 
which means to automatically select some objects to form a cluster  while ignoring other objects.
Relation to Clustering Game. In [12]  Bulo and Pelillo proposed to cast the hypergraph-based
clustering problem into a clustering game  which leads to a similar formulation as (5). In fact  their
formulation is a special case of (5) when ε = 1. Setting ε < 1 means that the probability of choosing
each strategy (from game theory perspective) or choosing each object (from our perspective) has an
known upper bound  which is in fact a prior  while ε = 1 represents a noninformative prior. This
point is very essential in many applications  it avoids the phenomenon where some components of
x dominate. For example  if the weight of a hyperedge is extremely large  then the cluster may only
select the vertices associated with this hyperedge  which is usually not desirable. In fact  ε offers us
a tool to control the least number of objects in cluster. Since each component does not exceed ε  the
cluster contains at least [ 1
" ] objects  where [z] represents the smallest integer larger than or equal to
z. Because of the constraint xi ∈ [0  ε]  the solution is also totally different from [12].
3 Algorithm
Formulation (5) usually has many local maxima. Large maxima correspond to true clusters and
small maxima usually form meaningless subsets. In this section  we ﬁrst analyze the properties
∗  which are critical in algorithm design  and then introduce our algorithm to
of the maximizer x
∗.
calculate x
Since the formulation (5) is a constrained optimization problem  by adding Lagrangian multipliers
λ  µ1 ···   µn and β1 ···   βn  µi ≥ 0 and βi ≥ 0 for all i = 1 ···   n  we can obtain its Lagrangian
function:

xi − 1) +

µixi +

βi(ε − xi).

n∑
L(x  λ  µ  β) = f (x) − λ(
∑

i=1

n∑

i=1

n∑

i=1

k−1∏

xvt

t=1

M (v1 ···   vk−1  i)

(6)

(7)

Any local maximizer x
order necessary conditions for local optimality. That is 

∗ must satisfy the Karush-Kuhn-Tucker (KKT) condition [14]  i.e.  the ﬁrst-

∗

) − λ + µi − βi = 0  i = 1 ···   n 
∗
i µi = 0 

 kri(x
∑
∑
n
i=1 x
i=1(ε − x
∗
n
i )βi = 0.
∑
i=1(ε − x
∗
i )βi = 0 is equivalent to saying that if x

∑

n
i=1 x

∗
i µi = 0 is equivalent to saying that if
∗
i < ε  then βi = 0.

∗
i   µi and βi are all nonnegative for all i’s 

Since x
∗
i > 0  then µi = 0  and
x
Hence  the KKT conditions can be rewritten as:

n

{ ≤ λ/k 

∗
ri(x

)

∗
i = 0 
x
∗
∗
i > 0 and x
= λ/k  x
i < ε 
≥ λ/k 
∗
i = ε.
x

∗

∗
) are equal to η; and 3) the rewards at all vertices belonging to V3(x

According to x  the vertices set V can be divided into three disjoint subsets  V1(x) = {i|xi = 0} 
V2(x) = {i|xi ∈ (0  ε)} and V3(x) = {i|xi = ε}. The Equation (9) characterizes the properties of
the solution of (5)  which are further summarized in the following theorem.
∗ is the solution of (5)  then there exists a constant η (= λ/k) such that 1) the
Theorem 1. If x
) are not larger than η; 2) the rewards at all vertices
rewards at all vertices belonging to V1(x
∗
belonging to V2(x
) are not
smaller than η.
∗ must satisfy
Proof: Since KKT condition is a necessary condition  according to (9)  the solution x
1)  2) and 3).
The set of non-zero components is Vd(x) = V2(x) ∪ V3(x) and the set of the components which are
smaller than ε is Vu(x) = V1(x)∪V2(x). For any x  if we want to update it to increase f (x)  then the
values of some components belonging to Vd(x) must decrease and the values of some components
belonging to Vu(x) must increase. According to Theorem 1  if x is the solution of (5)  then ri(x) ≤
rj(x) ∀i ∈ Vu(x) ∀j ∈ Vd(x). On the contrary  if ∃i ∈ Vu(x) ∃j ∈ Vd(x)  ri(x) > rj(x)  then x
is not the solution of (5). In fact  in such case  we can increase xi and decrease xj to increase f (x).
That is  let

′
l =

x

xl 
xl + α 
xl − α 

l ̸= i  l ̸= j;

l = i;
l = j.

{
∑

(8)

(9)

(10)

(11)

(12)

and deﬁne

Then

rij(x) =

v1;···;vk−2

M (v1 ···   vk−2  i  j)

′
f (x

) − f (x) = −k(k − 1)rij(x)α2 + k(ri(x) − rj(x))α

k−2∏

t=1

xvt

2(k−1)rij (x) )  the increase of f (x) reaches maximum.

Since ri(x) > rj(x)  we can always select a proper α > 0 to increase f (x). According to formula
(10) and the constraint over xi  α ≤ min(xj  ε − xi). Since ri(x) > rj(x)  if rij(x) ≤ 0  then
when α = min(xj  ε − xi)  the increase of f (x) reaches maximum; if rij > 0  then when α =
min(xj  ε − xi  ri(x)−rj (x)
According to the above analysis  if ∃i ∈ Vu(x) ∃j ∈ Vd(x)  ri(x) > rj(x)  then we can update
x to increase f (x). Such procedure iterates until ri(x) ≤ rj(x) ∀i ∈ Vu(x) ∀j ∈ Vd(x). From
a prior (initialization) x(0)  the algorithm to compute the local maximizer of (5) is summarized in
Algorithm 1  which successively chooses the “best” vertex and the “worst” vertex and then update
their corresponding components of x.
Since signiﬁcant maxima of formulation (5) usually correspond to true clusters  we need multiple
initializations (priors) to obtain them  with at least one initialization at the basin of attraction of
every signiﬁcant maximum. Such informative priors in fact can be easily and efﬁciently constructed
from the neighborhood of every vertex (vertices with hyperedges connecting to this vertex)  because
the neighbors of a vertex generally have much higher probabilities to belong to the same cluster.

4

∗ from a prior x(0)

Algorithm 1 Compute a local maximizer x
1: Input: Weighted adjacency array M  prior x(0);
2: repeat
3:
4:
5:

Compute the reward ri(x) for each vertex i;
Compute V1(x(t))  V2(x(t))  V3(x(t))  Vd(x(t))  and Vu(x(t));
Find the vertex i in Vu(x(t)) with the largest reward and the vertex j in Vd(x(t)) with the
smallest reward;
Compute α and update x(t) by formula (10) to obtain x(t + 1);

6:
7: until x is a local maximizer
∗.
8: Output: The local maximizer x

Algorithm 2 Construct a prior x(0) containing vertex v
1: Input: Hyperedge set E(v) and ε;
2: Sort the hyperedges in E(v) in descending order according to their weights;
3: for i = 1 ···  |E(v)| do
Add all vertices associated with the i-th hyperedge to L. If |L| ≥ [ 1
4:
5: end for
6: For each vertex vj ∈ L  set the corresponding component xvj (0) = 1|L|;
7: Output: a prior x(0).

" ]  then break;

For a vertex v  the set of hyperedges connected to v is denoted by E(v). We can construct a prior
containing v from E(v)  which is described in Algorithm 2.
Because of the constraint xi ≤ ε  the initializations need to contain at least [ 1
" ] nonzero compo-
nents. To cover basin of attractions of more maxima  we expect these initializations to locate more
uniformly in the space {x|x ∈ ∆n  xi ≤ ε}.
Since from every vertex  we can construct such a prior  thus  we can construct n priors in total. From
these n priors  according to Algorithm 1  we can obtain n maxima. The signiﬁcant maxima of (5)
are usually among these n maxima  and a signiﬁcant maximum may appear multiple times. In this
way  we can robustly obtain multiple clusters simultaneously  and these clusters may overlap  both
of which are desirable properties in many applications. Note that the clustering game approach [12]
utilizes a noninformative prior  that is  all vertices have equal probability. Thus  it cannot obtain
multiple clusters simultaneously. In clustering game approach [12]  if xi(t) = 0  then xi(t + 1) = 0 
which means that it can only drop points and if a point is initially not included  then it cannot be
selected. However  our method can automatically add or drop points  which is another key difference
to the clustering game approach.
In each iteration of Algorithm 1  we only need to consider two components of x  which makes
both the update of rewards and the update of x(t) very efﬁcient. As f (x(t)) increases  the sizes of
Vu(x(t)) and Vd(x(t)) both decrease quickly  thus f (x) converges to local maximum quickly. Sup-
pose the maximal number of hyperedges containing a certain vertex is h  then the time complexity
of Algorithm 1 is O(thk)  where t is the number of iterations. The total time complexity of our
method is then O(nthk)  since we need to ran Algorithm 1 from n initializations.
4 Experiments
We evaluate our method on three types of experiments. The ﬁrst one addresses the problem of line
clustering  the second addresses the problem of illumination-invariant face clustering  and the third
addresses the problem of afﬁne-invariant point set matching. We compare our method with clique
averaging [9] algorithm and matching game approach [12]. In all experiments  the clique averaging
approach needs to know the number of clusters in advance; however  both clustering game approach
and our method can automatically reveal the number of clusters  which yields the advantages of the
latter two in many applications.
4.1 Line Clustering
In this experiment  we consider the problem of clustering lines in 2D point sets. Pairwise similarity
measures are useless in this case  and at least three points are needed for characterizing such a

5

property. The dissimilarity measure on triplets of points is given by their mean distance to the best
ﬁtting line. If d(i  j  k) is the dissimilarity measure of points {i  j  k}  then the similarity function is
given by s({i  j  k}) = exp(−d(i  j  k)2/σ2
d)  where σd is a scaling parameter  which controls the
sensitivity of the similarity measure to deformation.
We randomly generate three lines within the region [−0.5  0.5]2  each line contains 30 points  and all
these points have been perturbed by Gaussian noise N (0  σ). We also randomly add outliers into the
point set. Fig. 1(a) illustrates such a point set with three lines shown in red  blue and green colors 
respectively  and the outliers are shown in magenta color. To evaluate the performance  we ran all
algorithms on the same data set over 30 trials with varying parameter values  and the performance
is measured by F-measure.
We ﬁrst ﬁx the number of outliers to be 60  vary the scaling parameter σd from 0.01 to 0.14  and
the result is shown in Fig. 1(b). For our method  we set ε = 1/30. Obviously  our method is nearly
not affected by the scaling parameter σd  while the clustering game approach is very sensitive to σd.
Note that σd in fact controls the weights of the hyperedge graph and many graph-based algorithms
are notoriously sensitive to the weights of the graph. Instead  by setting a proper ε  our method
overcomes this problem. From Fig. 1(b)  we observe that when σd = 4σ  the clustering game
approach will get the best performance. Thus  we ﬁx σd = 4σ  and change the noise parameter
σ from 0.01 to 0.1  the results of clustering game approach  clique averaging algorithm and our
method are shown in blue  green and red colors in Fig. 1(c)  respectively. As the ﬁgure shows  when
the noise is small  matching game approach outperforms clique averaging algorithm  and when the
noise becomes large  the clique averaging algorithm outperforms matching game approach. This is
because matching game approach is more robust to outliers  while the clique averaging algorithm
seems more robust to noises. Our method always gets the best result  since it can not only select
coherent clusters as matching game approach  but also control the size of clusters  thus avoiding the
problem of too few points selected into clusters.
In Fig. 1(d) and Fig. 1(e)  we vary the number of outliers from 10 to 100  the results clearly demon-
strate that our method and clustering game approach are robust to outliers  while clique averaging
algorithm is very sensitive to outliers  since it is a partition-based method and every point must be
assigned to a cluster. To illustrate the inﬂuence of ε  we ﬁx σd = σ = 0.02  and test the perfor-
mance of our method under different ε  the result is shown in Fig. 1(f)  note that x axis is 1/ε. As we
stressed in Section 2  clustering game approach is in fact a special case of our method when ε = 1 
thus  the result at ε = 1 is nearly the same as the result of clustering game approach in Fig. 1(b)
under the same conditions. Obviously  as 1/ε approaches the real number of points in the cluster 
the result become much better. Note that the best result appears when 1/ε > 30  which is due to the
fact that some outliers fall into the line clusters  as can be seen in Fig. 1(a).

s2
4

4

Illumination-invariant face clustering

4.2
It has been shown that the variability of images of a Labmertian surface in ﬁxed pose  but under
variable lighting conditions where no surface point is shadowed  constitutes a three dimensional
linear subspace [15]. This leads to a natural measure of dissimilarity over four images  which can
be used for clustering. In fact  this is a generalization of the k-lines problem into the k-subspaces
problem. If we assume that the four images under consideration form the columns of a matrix  and
normalize each column by ℓ2 norm  then d =
serves as a natural measure of dissimilarity 
1+···+s2
s2
where si is the ith singular value of this matrix.
In our experiments we use the Yale Face Database B and its extended version [16]  which contains 38
individuals  each under 64 different illumination conditions. Since in some lighting conditions  the
images are severely shadowed  we delete these images and do the experiments on a subset (about
35 images for each individual). We considered cases where we have faces from 4 and 5 random
individuals (randomly choose 10 faces for each individual)  with and without outliers. The case with
outliers consists 10 additional faces each from a different individual. For each of those combinations 
we ran 10 trials to obtain the average F-measures (mean and standard deviation)  and the result is
reported in Table 1. Note that for each algorithm  we individually tune the parameters to obtain
the best results. The results clearly show that partition-based clustering method (clique averaging)
is very sensitive to outliers  but performs better when there are no outliers. The clustering game
approach and our method both perform well  especially when there are outliers  and our method
performs a little better.

6

Figure 1: Results on clustering three lines with noises and outliers. The performance of clique
averaging algorithm [9]  matching game approach [12] and our method is shown as green dashed 
blue dotted and read solid curves  respectively. This ﬁgure is best viewed in color.

Table 1: Experiments on illuminant-invariant face clustering

Classes
Outliers

Clique Averaging
Clustering Game

Our Method

4

0

0.95 ± 0.05
0.92 ± 0.04
0.93 ± 0.04

10

0.84 ± 0.08
0.90 ± 0.04
0.92 ± 0.05

0

0.93 ± 0.05
0.91 ± 0.06
0.92 ± 0.07

5

10

0.83 ± 0.07
0.90 ± 0.07
0.91 ± 0.04

4.3 Afﬁne-invariant Point Set Matching
An important problem in the object recognition is the fact that an object can be seen from different
viewpoints  resulting in differently deformed images. Consequently  the invariance to viewpoints
is a desirable property for many vision tasks. It is well-known that a near-planar object seen from
different viewpoint can be modeled by afﬁne transformations. In this subsection  we will show that
matching planar point sets under different viewpoints can be formulated into a hypergraph clustering
problem and our algorithm is very suitable for such tasks.
Suppose the two point sets are P and Q  with nP and nQ points  respectively. For each point
in P   it may match to any point in Q  thus there are nP nQ candidate matches. Under the afﬁne
Si′ j′ k′ = |det(A)|  where Sijk is
transformation A  for three correct matches  mii′  mjj′ and mkk′  Sijk
the area of the triangle formed by points i  j and k in P   Si′j′k′ is the area of the triangle formed
′ in Q  and det(A) is the determinant of A. If we regard each candidate match
by points i
as a point  then s = exp(− (Sijk−Si′j′k′|det(A)|)2
) serves as a natural similarity measure for three
points (candidate matches)  mii′  mjj′ and mkk′  σd is a scaling parameter  and the correct matching
conﬁguration then naturally form a cluster. Note that in this problem  most of the candidate matches
are incorrect matches  and can be considered to be outliers.
We did the experiments on 8 shapes from MPEG-7 shape database [17]. For each shape  we uni-
formly sample its contour into 20 points. Both the shapes and sampled point sets are demonstrated
in Fig. 2. We regard original contour point sets as P s  then randomly add Gaussian noise N (0  σ) 
and transform them by randomly generated afﬁne matrices As to form corresponding Qs. Fig. 3
(a) shows such a pair of P and Q in red and blue  respectively. Since most of points (candidate
matches) should not belong to any cluster  partition-based clustering method  such as clique aver-

′ and k

′  j

(cid:27)2
d

7

aging method  cannot be used. Thus  we only compare our method with matching game approach
and measure the performance of these two methods by counting how many matches agree with the
ground truths. Since |det(A)| is unknown  we estimate its range and sample several possible values
in this range  and conduct the experiment for each possible |det(A)|. In Fig. 3(b)  we ﬁx noise
parameter σ = 0.05  and test the robustness of both methods under varying scaling parameter σd.
Obviously  our method is very robust to σd  while the matching game approach is very sensitive to
it. In Fig. 3(c)  we increase σ from 0.04 to 0.16  and for each σ  we adjust σd to reach the best
performances for both methods. As expected  our method is more robust to noise by beneﬁting from
the parameter ε  which is set to 0.05 in both Fig. 3(b) and Fig. 3(c). In Fig. 3(d)  we ﬁx σ = 0.05
and σd = 0.15  and test the performance of our method under different ε. The result again veriﬁes
the importance of the parameter ε.

Figure 2: The shapes and corresponding contour point sets used in our experiment.

Figure 3: Performance curves on afﬁne-invariant point set matching problem. The red solid curves
demonstrate the performance of our method  while the blue dotted curve illustrates the performance
of matching game approach.

5 Discussion
In this paper  we characterized clustering as an ensemble of all associated afﬁnity relations and relax
the clustering problem into optimizing a constrained homogenous function. We showed that the
clustering game approach turns out to be a special case of our method. We also proposed an efﬁcient
algorithm to automatically reveal the clusters in a data set  even under severe noises and a large num-
ber of outliers. The experimental results demonstrated the superiority of our approach with respect
to the state-of-the-art counterparts. Especially  our method is not sensitive to the scaling parameter
which affects the weights of the graph  and this is a very desirable property in many applications. A
key issue with hypergraph-based clustering is the high computational cost of the construction of a
hypergraph  and we are currently studying how to efﬁciently construct an approximate hypergraph
and then perform clustering on the incomplete hypergraph.
6 Acknowledgement
This research is done for CSIDM Project No. CSIDM-200803 partially funded by a grant from the
National Research Foundation (NRF) administered by the Media Development Authority (MDA) of
Singapore  and this work has also been partially supported by the NSF Grants IIS-0812118  BCS-
0924164 and the AFOSR Grant FA9550-09-1-0207.

8

References
[1] A. Jain  M. Murty  and P. Flynn  “Data clustering: a review ” ACM Computing Surveys  vol. 31 

no. 3  pp. 264–323  1999.

[2] T. Kanungo  D. Mount  N. Netanyahu  C. Piatko  R. Silverman  and A. Wu  “An efﬁcient
k-means clustering algorithm: Analysis and implementation ” IEEE Transactions on Pattern
Analysis and Machine Intelligence  vol. 24  no. 7  pp. 881–892  2002.

[3] A. Ng  M. Jordan  and Y. Weiss  “On spectral clustering: Analysis and an algorithm ” in Ad-

vances in Neural Information Processing Systems  vol. 2  2002  pp. 849–856.

[4] I. Dhillon  Y. Guan  and B. Kulis  “Kernel k-means: spectral clustering and normalized cuts ”
in Proceedings of the tenth ACM International Conference on Knowledge Discovery and Data
Mining  2004  pp. 551–556.

[5] J. Shi and J. Malik  “Normalized cuts and image segmentation ” IEEE Transactions on Pattern

Analysis and Machine Intelligence  vol. 22  no. 8  pp. 888–905  2000.

[6] B. Frey and D. Dueck  “Clustering by passing messages between data points ” Science  vol.

315  no. 5814  pp. 972–976  2007.

[7] J. Zien  M. Schlag  and P. Chan  “Multilevel spectral hypergraph partitioning with arbitrary
vertex sizes ” IEEE Transactions on Computer-aided Design of Integrated Circuits and Sys-
tems  vol. 18  no. 9  pp. 1389–1399  1999.

[8] J. Rodriguez  “On the Laplacian spectrum and walk-regular hypergraphs ” Linear and Multi-

linear Algebra  vol. 51  no. 3  pp. 285–297  2003.

[9] S. Agarwal  J. Lim  L. Zelnik-Manor  P. Perona  D. Kriegman  and S. Belongie  “Beyond
pairwise clustering ” in IEEE Computer Society Conference on Computer Vision and Pattern
Recognition  vol. 2  2005  pp. 838–845.

[10] D. Zhou  J. Huang  and B. Scholkopf  “Learning with hypergraphs: Clustering  classiﬁcation 
and embedding ” in Advances in Neural Information Processing Systems  vol. 19  2007  pp.
1601–1608.

[11] A. Shashua  R. Zass  and T. Hazan  “Multi-way clustering using super-symmetric non-negative

tensor factorization ” in European Conference on Computer Vision  2006  pp. 595–608.

[12] S. Bulo and M. Pelillo  “A game-theoretic approach to hypergraph clustering ” in Advances in

Neural Information Processing Systems  2009.

[13] M. Pavan and M. Pelillo  “Dominant sets and pairwise clustering ” IEEE Transactions on Pat-

tern Analysis and Machine Intelligence  vol. 29  no. 1  pp. 167–172  2007.

[14] H. Kuhn and A. Tucker  “Nonlinear programming ” ACM SIGMAP Bulletin  pp. 6–18  1982.
[15] P. Belhumeur and D. Kriegman  “What is the set of images of an object under all possible
illumination conditions?” International Journal of Computer Vision  vol. 28  no. 3  pp. 245–
260  1998.

[16] K. Lee  J. Ho  and D. Kriegman  “Acquiring linear subspaces for face recognition under vari-
able lighting ” IEEE Transactions on Pattern Analysis and Machine Intelligence  vol. 27  no. 5 
pp. 684–698  2005.

[17] L. Latecki  R. Lakamper  and T. Eckhardt  “Shape descriptors for non-rigid shapes with a single
closed contour ” in IEEE Conference on Computer Vision and Pattern Recognition  vol. 1 
2000  pp. 65–72.

9

,Xihui Liu
Guojun Yin
Jing Shao
Xiaogang Wang
hongsheng Li