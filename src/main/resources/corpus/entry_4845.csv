2017,Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions,We provide new results for noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity  and includes many important additional distributions  e.g.  the Pareto distribution and $t$ distribution. This class has been studied in the context of efficient sampling  integration  and optimization  but much remains unknown about the geometry of this class of distributions and their applications in the context of learning. The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions)  this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work  we introduce new convex geometry tools to study the properties of $s$-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces  disagreement outside a band  and the disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning  disagreement-based active learning  and passive learning of intersections of halfspaces. Our analysis of geometric properties of $s$-concave distributions might be of independent interest to optimization more broadly.,Sample and Computationally Efﬁcient Learning

Algorithms under S-Concave Distributions

Maria-Florina Balcan

Machine Learning Department

Carnegie Mellon University  USA

ninamf@cs.cmu.edu

Hongyang Zhang∗

Machine Learning Department

Carnegie Mellon University  USA

hongyanz@cs.cmu.edu

Abstract

We provide new results for noise-tolerant and sample-efﬁcient learning algorithms
under s-concave distributions. The new class of s-concave distributions is a broad
and natural generalization of log-concavity  and includes many important additional
distributions  e.g.  the Pareto distribution and t-distribution. This class has been
studied in the context of efﬁcient sampling  integration  and optimization  but
much remains unknown about the geometry of this class of distributions and their
applications in the context of learning. The challenge is that unlike the commonly
used distributions in learning (uniform or more generally log-concave distributions) 
this broader class is not closed under the marginalization operator and many such
distributions are fat-tailed.
In this work  we introduce new convex geometry
tools to study the properties of s-concave distributions and use these properties
to provide bounds on quantities of interest to learning including the probability
of disagreement between two halfspaces  disagreement outside a band  and the
disagreement coefﬁcient. We use these results to signiﬁcantly generalize prior
results for margin-based active learning  disagreement-based active learning  and
passive learning of intersections of halfspaces. Our analysis of geometric properties
of s-concave distributions might be of independent interest to optimization more
broadly.

1

Introduction

Developing provable learning algorithms is one of the central challenges in learning theory. The study
of such algorithms has led to signiﬁcant advances in both the theory and practice of passive and active
learning. In the passive learning model  the learning algorithm has access to a set of labeled examples
sampled i.i.d. from some unknown distribution over the instance space and labeled according to
some underlying target function. In the active learning model  however  the algorithm can access
unlabeled examples and request labels of its own choice  and the goal is to learn the target function
with signiﬁcantly fewer labels. In this work  we study both learning models in the case where the
underlying distribution belongs to the class of s-concave distributions.
Prior work on noise-tolerant and sample-efﬁcient algorithms mostly relies on the assumption that
the distribution over the instance space is log-concave [1  12  7  30]. A distribution is log-concave
if the logarithm of its density is a concave function. The assumption of log-concavity has been
made for a few purposes: for computational efﬁciency reasons and for sample efﬁciency reasons.
For computational efﬁciency reasons  it was made to obtain a noise-tolerant algorithm even for
seemingly simple decision surfaces like linear separators. These simple algorithms exist for noiseless
scenarios  e.g.  via linear programming [28]  but they are notoriously hard once we have noise [15 
25  19]; This is why progress on noise-tolerant algorithms has focused on uniform [22  26] and

∗Corresponding author.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

log-concave distributions [4]. Other concept spaces  like intersections of halfspaces  even have no
computationally efﬁcient algorithm in the noise-free settings that works under general distributions 
but there has been nice progress under uniform and log-concave distributions [27]. For sample
efﬁciency reasons  in the context of active learning  we need distributional assumptions in order to
obtain label complexity improvements [16]. The most concrete and general class for which prior
work obtains such improvements is when the marginal distribution over instance space satisﬁes
log-concavity [32  7]. In this work  we provide a broad generalization of all above results  showing
how they extend to s-concave distributions (s < 0). A distribution with density f (x) is s-concave
if f (x)s is a concave function. We identify key properties of these distributions that allow us to
simultaneously extend all above results.

How general and important is the class of s-concave distributions? The class of s-concave
distributions is very broad and contains many well-known (classes of) distributions as special cases.
For example  when s → 0  s-concave distributions reduce to log-concave distributions. Furthermore 
the s-concave class contains inﬁnitely many fat-tailed distributions that do not belong to the class of
log-concave distributions  e.g.  Cauchy  Pareto  and t distributions  which have been widely applied in
the context of theoretical physics and economics  but much remains unknown about how the provable
learning algorithms  such as active learning of halfspaces  perform under these realistic distributions.
We also compare s-concave distributions with nearly-log-concave distributions  a slightly broader
class of distributions than log-concavity. A distribution with density f (x) is nearly-log-concave if
for any λ ∈ [0  1]  x1  x2 ∈ Rn  we have f (λx1 + (1 − λ)x2) ≥ e−0.0154f (x1)λf (x2)1−λ [7]. The
class of s-concave distributions includes many important extra distributions which do not belong to
the nearly-log-concave distributions: a nearly-log-concave distribution must have sub-exponential
tails (see Theorem 11  [7])  while the tail probability of an s-concave distribution might decay much
slower (see Theorem 1 (6)). We also note that efﬁcient sampling  integration and optimization
algorithms for s-concave distributions have been well understood [13  23]. Our analysis of s-concave
distributions bridges these algorithms to the strong guarantees of noise-tolerant and sample-efﬁcient
learning algorithms.

1.1 Our Contributions

Structural Results. We study various geometric properties of s-concave distributions. These proper-
ties serve as the structural results for many provable learning algorithms  e.g.  margin-based active
learning [7]  disagreement-based active learning [29  21]  learning intersections of halfspaces [27] 
etc. When s → 0  our results exactly reduce to those for log-concave distributions [7  2  4]. Below 
we state our structural results informally:
Theorem 1 (Informal). Let D be an isotropic s-concave distribution in Rn. Then there exist closed-
form functions γ(s  m)  f1(s  n)  f2(s  n)  f3(s  n)  f4(s  n)  and f5(s  n) such that
1. (Weakly Closed under Marginal) The marginal of D over m arguments (or cumulative distribution

function  CDF) is isotropic γ(s  m)-concave. (Theorems 3  4)
2. (Lower Bound on Hyperplane Disagreement) For any two unit vectors u and v in Rn 
f1(s  n)θ(u  v) ≤ Prx∼D[sign(u · x) (cid:54)= sign(v · x)]  where θ(u  v) is the angle between u
and v. (Theorem 12)
3. (Probability of Band) There is a function d(s  n) such that for any unit vector w ∈ Rn and any
0 < t ≤ d(s  n)  we have f2(s  n)t < Prx∼D[|w · x| ≤ t] ≤ f3(s  n)t. (Theorem 11)
4. (Disagreement outside Margin) For any absolute constant c1 > 0 and any function f (s  n) 
there exists a function f4(s  n) > 0 such that Prx∼D[sign(u · x) (cid:54)= sign(v · x) and |v · x| ≥
f4(s  n)θ(u  v)] ≤ c1f (s  n)θ(u  v). (Theorem 13)
5. (Variance in 1-D Direction) There is a function d(s  n) such that for any unit vectors u and a in Rn
such that (cid:107)u−a(cid:107) ≤ r and for any 0 < t ≤ d(s  n)  we have Ex∼Du t [(a·x)2] ≤ f5(s  n)(r2 +t2) 
where Du t is the conditional distribution of D over the set {x : |u · x| ≤ t}. (Theorem 14)

6. (Tail Probability) We have Pr[(cid:107)x(cid:107) >
If s → 0 (i.e.  the distribution is log-concave)  then γ(s  m) → 0 and the functions f (s  n)  f1(s  n) 
f2(s  n)  f3(s  n)  f4(s  n)  f5(s  n)  and d(s  n) are all absolute constants.

. (Theorem 5)

1 − cst

1+ns

nt] ≤(cid:104)

√

(cid:105)(1+ns)/s

2

Table 1: Comparisons with prior distributions for margin-based active learning  disagreement-based
active learning  and Baum’s algorithm.

Margin (Efﬁcient  Noise)

Disagreement

Baum’s

uniform [3]
uniform [20]
symmetric [9]

Prior Work

log-concave [4]

nearly-log-concave [7]

log-concave [27]

Ours

s-concave
s-concave
s-concave

To prove Theorem 1  we introduce multiple new techniques  e.g.  extension of Prekopa-Leindler
theorem and reduction to baseline function (see the supplementary material for our techniques) 
which might be of independent interest to optimization more broadly.
Margin Based Active Learning: We apply our structural results to margin-based active learning of
a halfspace w∗ under any isotropic s-concave distribution for both realizable and adversarial noise
models. In the realizable case  the instance X is drawn from an isotropic s-concave distribution and
the label Y = sign(w∗ · X). In the adversarial noise model  an adversary can corrupt any η (≤ O())
fraction of labels. For both cases  we show that there exists a computationally efﬁcient algorithm that
outputs a linear separator wT such that Prx∼D[sign(wT · x) (cid:54)= sign(w∗ · x)] ≤  (see Theorems 15
and 16). The label complexity w.r.t. 1/ improves exponentially over the passive learning scenario
under s-concave distributions  though the underlying distribution might be fat-tailed. To the best
of our knowledge  this is the ﬁrst result concerning the computationally-efﬁcient  noise-tolerant
margin-based active learning under the broader class of s-concave distributions. Our work solves
an open problem proposed by Awasthi et al. [4] about exploring wider classes of distributions for
provable active learning algorithms.
Disagreement Based Active Learning: We apply our results to agnostic disagreement-based active
learning under s-concave distributions. The key to the analysis is estimating the disagreement
coefﬁcient  a distribution-dependent measure of complexity that is used to analyze certain types of
active learning algorithms  e.g.  the CAL [14] and A2 algorithm [5]. We work out the disagreement
coefﬁcient under isotropic s-concave distribution (see Theorem 17). By composing it with the
existing work on active learning [17]  we obtain a bound on label complexity under the class of
s-concave distributions. As far as we are aware  this is the ﬁrst result concerning disagreement-
based active learning that goes beyond log-concave distributions. Our bounds on the disagreement
coefﬁcient match the best known results for the much less general case of log-concave distributions [7];
Furthermore  they apply to the s-concave case where we allow an arbitrary number of discontinuities 
a case not captured by [18].
Learning Intersections of Halfspaces: Baum’s algorithm is one of the most famous algorithms
for learning the intersections of halfspaces. The algorithm was ﬁrst proposed by Baum [9] under
symmetric distribution  and later extended to log-concave distribution by Klivans et al. [27] as these
distributions are almost symmetric. In this paper  we show that approximate symmetry also holds for
the case of s-concave distributions. With this  we work out the label complexity of Baum’s algorithm
under the broader class of s-concave distributions (see Theorem 18)  and advance the state-of-the-art
results (see Table 1).
We provide lower bounds to partially show the tightness of our analysis. Our results can be potentially
applied to other provable learning algorithms as well [24  31  10  30  8]  which might be of inde-
pendent interest. We discuss our techniques and other related papers in the supplementary material.

denote by B(α  β) =(cid:82) 1

2 Preliminary
Before proceeding  we deﬁne some notations and clarify our problem setup in this section.
Notations: We will use capital or lower-case letters to represent random variables  D to represent
an s-concave distribution  and Du t to represent the conditional distribution of D over the set
|u · x| ≤ t}. We deﬁne the sign function as sign(x) = +1 if x ≥ 0 and −1 otherwise. We
{x :
tα−1e−tdt the gamma
function. We will consider a single norm for the vectors in Rn  namely  the 2-norm denoted by
(cid:107)x(cid:107). We will frequently use µ (or µf   µD) to represent the measure of the probability distribution
D with density function f. The notation ball(w∗  t) represents the set {w ∈ Rn : (cid:107)w − w∗(cid:107) ≤ t}.
For convenience  the symbol ⊕ slightly differs from the ordinary addition +: For f = 0 or g = 0 
{f s ⊕ gs}1/s = 0; Otherwise  ⊕ and + are the same. For u  v ∈ Rn  we deﬁne the angle between
them as θ(u  v).

0 tα−1(1− t)β−1dt the beta function  and Γ(α) =(cid:82) ∞

0

3

2.1 From Log-Concavity to S-Concavity
We begin with the deﬁnition of s-concavity. There are slight differences among the deﬁnitions of
s-concave density  s-concave distribution  and s-concave measure.
Deﬁnition 1 (S-Concave (Density) Function  Distribution  Measure). A function f: Rn → R+ is
s-concave  for −∞ ≤ s ≤ 1  if f (λx + (1 − λ)y) ≥ (λf (x)s + (1 − λ)f (y)s)1/s for all λ ∈ [0  1] 
∀x  y ∈ Rn.2 A probability distribution D is s-concave  if its density function is s-concave. A
probability measure µ is s-concave if µ(λA + (1 − λ)B) ≥ [λµ(A)s + (1 − λ)µ(B)s]1/s for any
sets A  B ⊆ Rn and λ ∈ [0  1].
Special classes of s-concave functions include concavity (s = 1)  harmonic-concavity (s = −1) 
quasi-concavity (s = −∞)  etc. The conditions in Deﬁnition 1 are progressively weaker as s becomes
smaller: s1-concave densities (distributions  measures) are s2-concave if s1 ≥ s2. Thus one can
verify [13]: concave (s = 1) (cid:40) log-concave (s = 0) (cid:40) s-concave (s < 0) (cid:40) quasi-concave (s =
−∞).

3 Structural Results of S-Concave Distributions: A Toolkit

In this section  we develop geometric properties of s-concave distribution. The challenge is that unlike
the commonly used distributions in learning (uniform or more generally log-concave distributions) 
this broader class is not closed under the marginalization operator and many such distributions are fat-
tailed. To address this issue  we introduce several new techniques. We ﬁrst introduce the extension of
the Prekopa-Leindler inequality so as to reduce the high-dimensional problem to the one-dimensional
case. We then reduce the resulting one-dimensional s-concave function to a well-deﬁned baseline
function  and explore the geometric properties of that baseline function. We summarize our high-level
proof ideas brieﬂy by the following ﬁgure.

3.1 Marginal Distribution and Cumulative Distribution Function
We begin with the analysis of the marginal distribution  which forms the basis of other geometric
properties of s-concave distributions (s ≤ 0). Unlike the (nearly) log-concave distribution where the
marginal remains (nearly) log-concave  the class of s-concave distributions is not closed under the
marginalization operator. To study the marginal  our primary tool is the theory of convex geometry.
Speciﬁcally  we will use an extension of the Prékopa-Leindler inequality developed by Brascamp and
Lieb [11]  which allows for a characterization of the integral of s-concave functions.
Theorem 2 ([11]  Thm 3.3). Let 0 < λ < 1  and Hs  G1  and G2 be non-negative integrable
functions on Rm such that Hs(λx + (1− λ)y) ≥ [λG1(x)s⊕ (1− λ)G2(y)s]1/s for every x  y ∈ Rm.

Rm G2(x)dx(cid:1)γ(cid:3)1/γ for s ≥ −1/m  with

+ (1 − λ)(cid:0)(cid:82)

Then(cid:82)

Rm Hs(x)dx ≥ (cid:2)λ(cid:0)(cid:82)

Rm G1(x)dx(cid:1)γ

γ = s/(1 + ms).

Building on this  the following theorem plays a key role in our analysis of the marginal distribution.
Theorem 3 (Marginal). Let f (x  y) be an s-concave density on a convex set K ⊆ Rn+m with
m . Denote by K|Rn = {x ∈ Rn : ∃y ∈ Rm s.t. (x  y) ∈ K}. For every x in K|Rn  consider
s ≥ − 1
K(x) f (x  y)dy is

the section K(x) (cid:44) {y ∈ Rm : (x  y) ∈ K}. Then the marginal density g(x) (cid:44)(cid:82)

γ-concave on K|Rn  where γ = s

1+ms . Moreover  if f (x  y) is isotropic  then g(x) is isotropic.

Similar to the marginal  the CDF of an s-concave distribution might not remain in the same class.
This is in sharp contrast to log-concave distributions. The following theorem studies the CDF of an
s-concave distribution.
1+ns and s ≥ − 1
Theorem 4. The CDF of s-concave distribution in Rn is γ-concave  where γ = s
n .
2When s → 0  we note that lims→0(λf (x)s + (1 − λ)f (y)s)1/s = exp(λ log f (x) + (1 − λ) log f (y)).

In this case  f (x) is known to be log-concave.

4

				n-D	s-concave																									1-D	!-concave																									1-D	ℎ#=%(1+)#)+/-	Extension	of	Prekopa-Leindler			Baseline	Function	Theorem 3 and 4 serve as the bridge that connects high-dimensional s-concave distributions to
one-dimensional γ-concave distributions. With them  we are able to reduce the high-dimensional
problem to the one-dimensional one.
3.2 Fat-Tailed Density
Tail probability is one of the most distinct characteristics of s-concave distributions compared to
(nearly) log-concave distributions. While it can be shown that the (nearly) log-concave distribution
has an exponentially small tail (Theorem 11  [7])  the tail of an s-concave distribution is fat  as proved
by the following theorem.
Theorem 5 (Tail Probability). Let x come from an isotropic distribution over Rn with an s-concave
density. Then for every t ≥ 16  we have Pr[(cid:107)x(cid:107) >
  where c is an
absolute constant.
Theorem 5 is almost tight for s < 0. To see this  consider X that is drawn from a one-dimensional
Pareto distribution with density f (x) = (−1 − 1
s (x ≥ s+1−s ). It can be easily seen that
for t ≥ s+1−s   which matches Theorem 5 up to an absolute constant factor.

nt] ≤ (cid:104)

(cid:105)(1+ns)/s

(cid:104) −s

(cid:105) s+1

s

s )− 1

s x 1

1 − cst

1+ns

√

Pr[X > t] =

s+1 t

3.3 Geometry of S-Concave Distributions
We now investigate the geometry of s-concave distributions. We ﬁrst consider one-dimensional s-
concave distributions: We provide bounds on the density of centroid-centered halfspaces (Lemma 6)
and range of the density function (Lemma 7). Building upon these  we develop geometric properties
of high-dimensional s-concave distributions by reducing the distributions to the one-dimensional
case based on marginalization (Theorem 3).
3.3.1 One-Dimensional Case
We begin with the analysis of one-dimensional halfspaces. To bound the probability  a normal
technique is to bound the centroid region and the tail region separately. However  the challenge is that
the s-concave distribution is fat-tailed (Theorem 5). So while the probability of a one-dimensional
halfspace is bounded below by an absolute constant for log-concave distributions  such a probability
for s-concave distributions decays as s (≤ 0) becomes smaller. The following lemma captures such
an intuition.
Lemma 6 (Density of Centroid-Centered Halfspaces). Let X be drawn from a one-dimensional
distribution with s-concave density for −1/2 ≤ s ≤ 0. Then Pr(X ≥ EX) ≥ (1 + γ)−1/γ for
γ = s/(1 + s).

We also study the image of a one-dimensional s-concave density. The following condition for
s > −1/3 is for the existence of second-order moment.
Lemma 7. Let g : R → R+ be an isotropic s-concave density function and s > −1/3. (a) For all x 
g(x) ≤ 1+s

1+3s ; (b) We have g(0) ≥(cid:113) 1

3(1+γ)3/γ   where γ = s

s+1 .

3.3.2 High-Dimensional Case
2n+3 ≤
We now move on to the high-dimensional case (n ≥ 2). In the following  we will assume − 1
s ≤ 0. Though this working range of s vanishes as n becomes larger  it is almost the broadest range
of s that we can hopefully achieve: Chandrasekaran et al. [13] showed a lower bound of s ≥ − 1
n−1
if one require the s-concave distribution to have good geometric properties. In addition  we can
see from Theorem 3 that if s < − 1
n−1  the marginal of an s-concave distribution might even not
exist; Such a case does happen for certain s-concave distributions with s < − 1
n−1  e.g.  the Cauchy
distribution. So our range of s is almost tight up to a 1/2 factor.
We start our analysis with the density of centroid-centered halfspaces in high-dimensional spaces.
Lemma 8 (Density of Centroid-Centered Halfspaces). Let f : Rn → R+ be an s-concave density
H f (x)dx ≥ (1 + γ)−1/γ for

function  and let H be any halfspace containing its centroid. Then(cid:82)
s/(1+(n−1)s)-concave. Then by Lemma 6 (cid:82)

γ = s/(1 + ns).
Proof. W.L.O.G.  we assume H is orthogonal to the ﬁrst axis. By Theorem 3  the ﬁrst marginal of f is

H f (x)dx ≥ (1+γ)−1/γ  where γ = s/(1+ns).

5

The following theorem is an extension of Lemma 7 to high-dimensional spaces. The proofs basically
reduce the n-dimensional density to its ﬁrst marginal by Theorem 3  and apply Lemma 7 to bound
the image.
Theorem 9 (Bounds on Density). Let f : Rn → R+ be an isotropic s-concave density. Then
(a) Let d(s  n) = (1 + γ)−1/γ 1+3β

1+(n−1)s and γ = β

3+3β   where β =

(cid:17)1/s
1+β . For any u ∈ Rn such that

(cid:107)u(cid:107) ≤ d(s  n)  we have f (u) ≥(cid:16)(cid:107)u(cid:107)

s

1+3β

f (0).

d ((2 − 2−(n+1)s)−1 − 1) + 1

(b) f (x) ≤ f (0)
(c) There exists an x ∈ Rn such that f (x) > (4eπ)−n/2.

(cid:104)(cid:16) 1+β
(d) (4eπ)−n/2(cid:104)(cid:16) 1+β
(f) For any line (cid:96) through the origin (cid:82)

(cid:112)3(1 + γ)3/γ2n−1+1/s(cid:17)s − 1
(cid:105)1/s
(cid:17)s − 1
(cid:105)− 1
(cid:112)3(1 + γ)3/γ2n−1+ 1
(cid:112)3(1 + γ)3/γ2n−1+1/s(cid:17)s − 1
(cid:104)(cid:16) 1+β
(cid:96) f ≤ (2 − 2−ns)1/s (n−1)Γ((n−1)/2)
2π(n−1)/2dn−1 .

(e) f (x) ≤ (2 − 2−(n+1)s)1/s nΓ(n/2)

for every x.

2πn/2dn

1+3β

1+3β

s

s

< f (0) ≤ (2 − 2−(n+1)s)1/s nΓ(n/2)
2πn/2dn .

(cid:105)1/s

for every x.

Theorem 9 provides uniform bounds on the density function. To obtain more reﬁned upper bound on
the image of s-concave densities  we have the following lemma. The proof is built upon Theorem 9.
Lemma 10 (More Reﬁned Upper Bound on Densities). Let f : Rn → R+ be an isotropic s-concave
density. Then f (x) ≤ β1(n  s)(1 − sβ2(n  s)(cid:107)x(cid:107))1/s for every x ∈ Rn  where

(1 − s)−1/snΓ(n/2)

(cid:20)(cid:18) 1 + β
(cid:113)
(cid:112)3(1 + γ)3/γ2n−1+1/s(cid:17)s − 1
(cid:105)−1

(2 − 2−ns)− 1

1 + 3β

s

3(1 + γ)3/γ2n−1+1/s

[(a(n  s) + (1 − s)β1(n  s)s)1+ 1

s − a(n  s)1+ 1

s ]s

β1(n  s)s(1 + s)(1 − s)
1+β   β =

  γ = β

s

1+(n−1)s   and

(cid:21)1/s

(cid:19)s − 1

 

 

β1(n  s) =

(2 − 2−(n+1)s) 1

s

2πn/2dn
2π(n−1)/2dn−1

β2(n  s) =

a(n  s) = (4eπ)−ns/2(cid:104)(cid:16) 1+β

(n − 1)Γ((n − 1)/2)

1+3β

d = (1 + γ)

− 1

γ 1+3β
3+3β .

γ

1+γ

1+3γ

(cid:17)− 1+γ

3+3γ where γ =

Moreover  if t ≤ d(s  n) (cid:44)(cid:16) 1+2γ

We also give an absolute bound on the measure of band.
Theorem 11 (Probability inside Band). Let D be an isotropic s-concave distribution in Rn. Denote
by f3(s  n) = 2(1 + ns)/(1 + (n + 2)s). Then for any unit vector w  Prx∼D[|w· x| ≤ t] ≤ f3(s  n)t.
(cid:33)−1/γ
(cid:18)
1+(n−1)s   then Prx∼D[|w · x| ≤ t] >

f2(s  n)t  where f2(s  n) = 2(2 − 2−2γ)−1/γ(4eπ)−1/2
To analyze the problem of learning linear separators  we are interested in studying the disagreement
between the hypothesis of the output and the hypothesis of the target. The following theorem captures
such a characteristic under s-concave distributions.
Theorem 12 (Probability of Disagreement). Assume D is an isotropic s-concave distribution in Rn.
Then for any two unit vectors u and v in Rn  we have dD(u  v) = Prx∼D[sign(u· x) (cid:54)= sign(v· x)] ≥
f1(s  n)θ(u  v)  where f1(s  n) = c(2 − 2−3α)− 1
(1 +

(cid:112)3(1 + γ)3/γ21+1/α(cid:17)α− 1
(cid:105)− 1

2γ (cid:19)γ − 1
(cid:17) 3+3γ

(cid:104)(cid:16) 1+β

(cid:16) 1+2γ

(cid:32)

1+γ
1+3γ

√

1+γ

2

3

.

s

α

α

1+3β

  c is an absolute constant  α =

s

1+(n−2)s   β =

s

1+(n−1)s   γ = s

1+ns .

γ)−2/γ(cid:16) 1+3β

3+3β

(cid:17)2

Due to space constraints  all missing proofs are deferred to the supplementary material.

4 Applications: Provable Algorithms under S-Concave Distributions

In this section  we show that many algorithms that work under log-concave distributions behave well
under s-concave distributions by applying the above-mentioned geometric properties. For simplicity 
we will frequently use the notations in Theorem 1.

6

4.1 Margin Based Active Learning
We ﬁrst investigate margin-based active learning under isotropic s-concave distributions in both
realizable and adversarial noise models. The algorithm (see Algorithm 1) follows a localization
technique: It proceeds in rounds  aiming to cut the error down by half in each round in the margin [6].

Algorithm 1 Margin Based Active Learning under S-Concave Distributions

Input: Parameters bk  τk  rk  mk  κ  and T as in Theorem 16.
1: Draw m1 examples from D  label them and put them into W .
2: For k = 1  2  ...  T
3:

Find vk ∈ ball(wk−1  rk) to approximately minimize the hinge loss over W s.t. (cid:107)vk(cid:107) ≤ 1:
(cid:96)τk ≤ minw∈ball(wk−1 rk)∩ball(0 1) (cid:96)τk (w  W ) + κ/8.

4: Normalize vk  yielding wk = vk(cid:107)vk(cid:107) ; Clear the working set W .
5: While mk+1 additional data points are not labeled
6:
7:
Output: Hypothesis wT .

Draw sample x from D.
If |wk · x| ≥ bk  reject x; else ask for label of x and put into W .

4.1.1 Relevant Properties of S-Concave Distributions
The analysis requires more reﬁned geometric properties as below. Theorem 13 basically claims that
the error mostly concentrates in a band  and Theorem 14 guarantees that the variance in any 1-D
direction cannot be too large. We defer the detailed proofs to the supplementary material.
Theorem 13 (Disagreement outside Band). Let u and v be two vectors in Rn and assume that
θ(u  v) = θ < π/2. Let D be an isotropic s-concave distribution. Then for any absolute constant
c1 > 0 and any function f1(s  n) > 0  there exists a function f4(s  n) > 0 such that Prx∼D[sign(u ·
x) (cid:54)= sign(v · x) and |v · x| ≥ f4(s  n)θ] ≤ c1f1(s  n)θ  where f4(s  n) = 4β1(2 α)B(−1/α−3 3)
−c1f1(s n)α3β2(2 α)3  
B(· ·) is the beta function  α = s/(1 + (n − 2)s)  β1(2  α) and β2(2  α) are given by Lemma 10.
Theorem 14 (1-D Variance). Assume that D is isotropic s-concave. For d given by Theorem 9
(a)  there is an absolute C0 such that for all 0 < t ≤ d and for all a such that (cid:107)u − a(cid:107) ≤ r and
8β1(2 η)B(−1/η−3 2)
(cid:107)a(cid:107) ≤ 1  Ex∼Du t [(a · x)2] ≤ f5(s  n)(r2 + t2)  where f5(s  n) = 16 + C0
f2(s n)β2(2 η)3(η+1)η2  
(β1(2  η)  β2(2  η)) and f2(s  n) are given by Lemma 10 and Theorem 11  and η =
4.1.2 Realizable Case
We show that margin-based active learning works under s-concave distributions in the realizable case.
let D be an isotropic s-concave distribution in Rn.
Theorem 15.
Then for 0 <  < 1/4  δ > 0  and absolute constants c 
there is an algorith-
c(cid:101) iterations  requires mk =
m (see the supplementary material) that runs in T = (cid:100)log 1
labels in the k-th round  and outputs
O
a linear separator of error at most  with probability at least 1 − δ. In particular  when s → 0 (a.k.a.

(cid:16) f3 min{2−kf4f
log-concave)  we have mk = O(cid:0)n + log( 1+s−k

n log f3 min{2−kf4f

In the realizable case 

+log 1+s−k

s

1+(n−2)s .

−1
1  d}

2−k

−1
1  d}

2−k

)(cid:1).

δ

In the adversarial noise model  an adversary can choose any distribution (cid:101)P over Rn ×{+1 −1} such

By Theorem 15  we see that the algorithm of margin-based active learning under s-concave dis-
tributions works almost as well as the log-concave distributions in the resizable case  improving
exponentially w.r.t. the variable 1/ over passive learning algorithms.
4.1.3 Efﬁcient Learning with Adversarial Noise
that the marginal D over Rn is s-concave but an η fraction of labels can be ﬂipped adversarially. The
analysis builds upon an induction technique where in each round we do hinge loss minimization in
the band and cut down the 0/1 loss by half. The algorithm was previously analyzed in [3  4] for the
special class of log-concave distributions. In this paper  we analyze it for the much more general
class of s-concave distributions.
Theorem 16. Let D be an isotropic s-concave distribution in Rn over x and the label y
obey the adversarial noise model.
If the rate η of adversarial noise satisﬁes η < c0 for
some absolute constant c0 
then for 0 <  < 1/4  δ > 0  and an absolute constan-
t c  Algorithm 1 runs in T = (cid:100)log 1
c(cid:101) iterations  outputs a linear separator wT such that
Prx∼D[sign(wT · x) (cid:54)= sign(w∗ · x)] ≤  with probability at least 1 − δ. The label complexity

(cid:16)

(cid:17)(cid:17)

δ

7

(cid:16) [bk−1s+τk(1+ns)[1−(δ/(

(cid:111)

f5

τk

f2

(cid:16)

(cid:17)(cid:17)

 

(cid:16)
5 2−(k−1)(cid:17)
δ ))(cid:1).

n

(cid:16)

√

(cid:16)

n3/2

(cid:17)

s

r

(cid:16)

  and bk =

of (cid:101)O

n + log k+k2

δ

√
κ2τ 2
  τk = Θ

√
√
f2 min{bk−1 d}   bk−1

n(k+k2)))s/(1+ns)]+τks]2
k s2
f−2
1 f

−1/2
2

f3f 2

4 f 1/2
δ )(n + log( k

s(1+(n+2)s)f (s) (1 − s/(1+ns))

(1+ns)2

n(1+ns)2

s(1+(n+2)s)f1(s n) (1 − 
n log(1/)).

(cid:110)
1 )  d}. In particular  if s → 0  mk = O(cid:0)n log( n

in the k-th round is mk = O
f3τk
where κ = max
min{Θ(2−kf4f−1
By Theorem 16  the label complexity of margin-based active learning improves exponentially over
that of passive learning w.r.t. 1/ even under fat-tailed s-concave distributions and challenging
adversarial noise model.
4.2 Disagreement Based Active Learning
We apply our results to the analysis of disagreement-based active learning under s-concave dis-
tributions. The key is estimating the disagreement coefﬁcient  a measure of complexity of active
learning problems that can be used to bound the label complexity [20]. Recall the deﬁnition of the
disagreement coefﬁcient w.r.t. classiﬁer w∗  precision   and distribution D as follows. For any r > 0 
deﬁne ballD(w  r) = {u ∈ H : dD(u  w) ≤ r} where dD(u  w) = Prx∼D[(u · x)(w · x) < 0].
Deﬁne the disagreement region as DIS(H) = {x : ∃u  v ∈ H s.t. (u · x)(v · x) < 0}. Let the
Alexander capacity capw∗ D = PrD(DIS(ballD(w∗ r)))
. The disagreement coefﬁcient is deﬁned as
Θw∗ D() = supr≥[capw∗ D(r)]. Below  we state our results on the disagreement coefﬁcient under
isotropic s-concave distributions.
Theorem 17 (Disagreement Coefﬁcient). Let D be an isotropic s-concave distribution over Rn. For
any w∗ and r > 0  the disagreement coefﬁcient is Θw∗ D() = O
.
1+ns )
√
In particular  when s → 0 (a.k.a. log-concave)  Θw∗ D() = O(
Our bounds on the disagreement coefﬁcient match the best known results for the much less general
case of log-concave distributions [7]; Furthermore  they apply to the s-concave case where we allow
arbitrary number of discontinuities  a case not captured by [18]. The result immediately implies
concrete bounds on the label complexity of disagreement-based active learning algorithms  e.g. 
CAL [14] and A2 [5]. For instance  by composing it with the result from [17]  we obtain a bound
for agnostic active learning under an
isotropic s-concave distribution D. Namely  it sufﬁces to output a halfspace with error at most
OP T +   where OP T = minw errD(w).
4.3 Learning Intersections of Halfspaces
Baum [9] provided a polynomial-time algorithm for learning the intersections of halfspaces w.r.t.
symmetric distributions. Later  Klivans [27] extended the result by showing that the algorithm works
under any distribution D as long as µD(E) ≈ µD(−E) for any set E. In this section  we show that it
is possible to learn intersections of halfspaces under the broader class of s-concave distributions.
Theorem 18. In the PAC realizable case  there is an algorithm (see the supplementary material) that
outputs a hypothesis h of error at most  with probability at least 1 − δ under isotropic s-concave
The label complexity is M (/2  δ/4  n2) + max{2m2/  (2/2) log(4/δ)} 
distributions.
where M (  δ  m)
=
M (max{δ/(4eKm1)  /2}  δ/4  n)  K = β1(3  κ) B(−1/κ−3 3)
h(κ)d3+1/κ   d = (1 + γ)−1/γ 1+3β
3+3β  
(−κβ2(3 κ))3
1+2κ  
  β = κ
1+(n−3)s . In particular  if s → 0 (a.k.a. log-concave)  K is an absolute constant.

(cid:1)  m2
(cid:105)−1/κ
(cid:17)κ−1

1+κ   and κ =

γ = κ
5 Lower Bounds
In this section  we give information-theoretic lower bounds on the label complexity of passive and
active learning of homogeneous halfspaces under s-concave distributions.
Theorem 19. For a ﬁxed value − 1
2n+3 ≤ s ≤ 0 we have: (a) For any s-concave distribution D in
Rn whose covariance matrix is of full rank  the sample complexity of learning origin-centered linear
separators under D in the passive learning scenario is Ω (nf1(s  n)/); (b) The label complexity of
active learning of linear separators under s-concave distributions is Ω (n log (f1(s  n)/)).
If the covariance matrix of D is not of full rank  then the intrinsic dimension is less than d. So
our lower bounds essentially apply to all s-concave distributions. According to Theorem 19  it is
possible to have an exponential improvement of label complexity w.r.t. 1/ over passive learning by
active sampling  even though the underlying distribution is a fat-tailed s-concave distribution. This
observation is captured by Theorems 15 and 16.

d ((2 − 2−4κ)−1 − 1) + 1(cid:1) 1

(cid:112)3(1+γ)3/γ22+ 1

h(κ) = (cid:0) 1

(cid:104)(cid:16) 1+β

 log 1

 + 1

 log 1

δ

O(cid:0) n

log2 1

 + OP T 2

2

by M (  δ  n)

κ (4eπ)− 3

2

1+3β

κ

=

3+1/κ

is

deﬁned

(cid:17)(cid:17)

s

8

6 Conclusions

In this paper  we study the geometric properties of s-concave distributions. Our work advances the
state-of-the-art results on the margin-based active learning  disagreement-based active learning  and
learning intersections of halfspaces w.r.t. the distributions over the instance space. When s → 0  our
results reduce to the best-known results for log-concave distributions. The geometric properties of
s-concave distributions can be potentially applied to other learning algorithms  which might be of
independent interest more broadly.
Acknowledgements. This work was supported in part by grants NSF-CCF 1535967  NSF CCF-
1422910  NSF CCF-1451177  a Sloan Fellowship  and a Microsoft Research Fellowship.

References
[1] D. Applegate and R. Kannan. Sampling and integration of near log-concave functions. In ACM

Symposium on Theory of Computing  pages 156–163  1991.

[2] P. Awasthi  M.-F. Balcan  N. Haghtalab  and H. Zhang. Learning and 1-bit compressed sensing

under asymmetric noise. In Annual Conference on Learning Theory  pages 152–192  2016.

[3] P. Awasthi  M.-F. Balcan  and P. M. Long. The power of localization for efﬁciently learning
linear separators with noise. In ACM Symposium on Theory of Computing  pages 449–458 
2014.

[4] P. Awasthi  M.-F. Balcan  and P. M. Long. The power of localization for efﬁciently learning

linear separators with noise. Journal of the ACM  63(6):50  2017.

[5] M.-F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. Journal of Computer

and System Sciences  75(1):78–89  2009.

[6] M.-F. Balcan  A. Broder  and T. Zhang. Margin based active learning. In Annual Conference on

Learning Theory  pages 35–50  2007.

[7] M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave

distributions. In Annual Conference on Learning Theory  pages 288–316  2013.

[8] M.-F. Balcan and H. Zhang. Noise-tolerant life-long matrix completion via adaptive sampling.

In Advances in Neural Information Processing Systems  pages 2955–2963  2016.

[9] E. B. Baum. A polynomial time algorithm that learns two hidden unit nets. Neural Computation 

2(4):510–522  1990.

[10] A. Beygelzimer  S. Dasgupta  and J. Langford.

Importance weighted active learning.

International Conference on Machine Learning  pages 49–56  2009.

In

[11] H. J. Brascamp and E. H. Lieb. On extensions of the Brunn-Minkowski and Prékopa-Leindler
theorems  including inequalities for log concave functions  and with an application to the
diffusion equation. Journal of Functional Analysis  22(4):366–389  1976.

[12] C. Caramanis and S. Mannor. An inequality for nearly log-concave distributions with applica-

tions to learning. IEEE Transactions on Information Theory  53(3):1043–1057  2007.

[13] K. Chandrasekaran  A. Deshpande  and S. Vempala. Sampling s-concave functions: The
limit of convexity based isoperimetry. In Approximation  Randomization  and Combinatorial
Optimization. Algorithms and Techniques  pages 420–433. 2009.

[14] D. Cohn  L. Atlas  and R. Ladner. Improving generalization with active learning. Machine

Learning  15(2):201–221  1994.

[15] A. Daniely. Complexity theoretic limitations on learning halfspaces. In ACM Symposium on

Theory of computing  pages 105–117  2016.

[16] S. Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Information

Processing Systems  volume 17  pages 337–344  2004.

9

[17] S. Dasgupta  D. J. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In

Advances in Neural Information Processing Systems  pages 353–360  2007.

[18] E. Friedman. Active learning for smooth problems. In Annual Conference on Learning Theory 

2009.

[19] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. SIAM Journal

on Computing  39(2):742–765  2009.

[20] S. Hanneke. A bound on the label complexity of agnostic active learning. In International

Conference on Machine Learning  pages 353–360  2007.

[21] S. Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning  7(2-3):131–309  2014.

[22] A. T. Kalai  A. R. Klivans  Y. Mansour  and R. A. Servedio. Agnostically learning halfspaces.

SIAM Journal on Computing  37(6):1777–1805  2008.

[23] A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of

Operations Research  31(2):253–266  2006.

[24] D. M. Kane  S. Lovett  S. Moran  and J. Zhang. Active classiﬁcation with comparison queries.

In IEEE Symposium on Foundations of Computer Science  pages 355–366  2017.

[25] A. Klivans and P. Kothari. Embedding hard learning problems into gaussian space. International
Workshop on Approximation Algorithms for Combinatorial Optimization Problems  28:793–809 
2014.

[26] A. R. Klivans  P. M. Long  and R. A. Servedio. Learning halfspaces with malicious noise.

Journal of Machine Learning Research  10:2715–2740  2009.

[27] A. R. Klivans  P. M. Long  and A. K. Tang. Baum’s algorithm learns intersections of halfspaces
with respect to log-concave distributions. In Approximation  Randomization  and Combinatorial
Optimization. Algorithms and Techniques  pages 588–600. 2009.

[28] R. A. Servedio. Efﬁcient algorithms in computational learning theory. PhD thesis  Harvard

University  2001.

[29] L. Wang. Smoothness  disagreement coefﬁcient  and the label complexity of agnostic active

learning. Journal of Machine Learning Research  12(Jul):2269–2292  2011.

[30] Y. Xu  H. Zhang  A. Singh  A. Dubrawski  and K. Miller. Noise-tolerant interactive learning
using pairwise comparisons. In Advances in Neural Information Processing Systems  pages
2428–2437  2017.

[31] S. Yan and C. Zhang. Revisiting perceptron: Efﬁcient and label-optimal active learning of

halfspaces. arXiv preprint arXiv:1702.05581  2017.

[32] C. Zhang and K. Chaudhuri. Beyond disagreement-based agnostic active learning. In Advances

in Neural Information Processing Systems  pages 442–450  2014.

10

,Maria-Florina Balcan
Hongyang Zhang
Yonathan Efroni
Gal Dalal
Bruno Scherrer
Shie Mannor