2010,Layered image motion with explicit occlusions  temporal consistency  and depth ordering,Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation  such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular  we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally  a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.,Layered Image Motion with Explicit Occlusions 

Temporal Consistency  and Depth Ordering

Deqing Sun  Erik B. Sudderth  and Michael J. Black
Department of Computer Science  Brown University
{dqsun sudderth black}@cs.brown.edu

Abstract

Layered models are a powerful way of describing natural scenes containing
smooth surfaces that may overlap and occlude each other. For image motion es-
timation  such models have a long history but have not achieved the wide use or
accuracy of non-layered methods. We present a new probabilistic model of optical
ﬂow in layers that addresses many of the shortcomings of previous approaches. In
particular  we deﬁne a probabilistic graphical model that explicitly captures: 1)
occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal con-
sistency of the layer segmentation. Additionally the optical ﬂow in each layer is
modeled by a combination of a parametric model and a smooth deviation based
on an MRF with a robust spatial prior; the resulting model allows roughness in
layers. Finally  a key contribution is the formulation of the layers using an image-
dependent hidden ﬁeld prior based on recent models for static scene segmentation.
The method achieves state-of-the-art results on the Middlebury benchmark and
produces meaningful scene segmentations as well as detected occlusion regions.

1 Introduction

Layered models of scenes offer signiﬁcant beneﬁts for optical ﬂow estimation [8  11  25]. Splitting
the scene into layers enables the motion in each layer to be deﬁned more simply  and the estimation
of motion boundaries to be separated from the problem of smooth ﬂow estimation. Layered models
also make reasoning about occlusion relationships easier. In practice  however  none of the current
top performing optical ﬂow methods use a layered approach [2]. The most accurate approaches
are single-layered  and instead use some form of robust smoothness assumption to cope with ﬂow
discontinuities [5]. This paper formulates a new probabilistic  layered motion model that addresses
the key problems of previous layered approaches. At the time of writing  it achieves the lowest
average error of all tested approaches on the Middlebury optical ﬂow benchmark [2]. In particular 
the accuracy at occlusion boundaries is signiﬁcantly better than previous methods. By segmenting
the observed scene  our model also identiﬁes occluded and disoccluded regions.

Layered models provide a segmentation of the scene and this segmentation  because it corresponds
to scene structure  should persist over time. However  this persistence is not a beneﬁt if one is only
computing ﬂow between two frames; this is one reason that multi-layer models have not surpassed
their single-layer competitors on two-frame benchmarks. Without loss of generality  here we use
three-frame sequences to illustrate our method. In practice  these three frames can be constructed
from an image pair by computing both the forward and backward ﬂow. The key is that this gives
two segmentations of the scene  one at each time instant  both of which must be consistent with the
ﬂow. We formulate this temporal layer consistency probabilistically. Note that the assumption of
temporal layer consistency is much more realistic than previous assumptions of temporal motion
consistency [4]; while the scene motion can change rapidly  scene structure persists.

1

One of the main motivations for layered models is that  conditioned on the segmentation into layers 
each layer can employ afﬁne  planar  or other strong models of optical ﬂow. By applying a single
smooth motion across the entire layer  these models combine information over long distances and
interpolate behind occlusions. Such rigid parametric assumptions  however  are too restrictive for
real scenes. Instead one can model the ﬂow within each layer as smoothly varying [26]. While
the resulting model is more ﬂexible than traditional parametric models  we ﬁnd that it is still not as
accurate as robust single-layer models. Consequently  we formulate a hybrid model that combines a
base afﬁne motion with a robust Markov random ﬁeld (MRF) model of deformations from afﬁne [6].
This roughness in layers model  which is similar in spirit to work on plane+parallax [10  14  19] 
encourages smooth ﬂow within layers but allows signiﬁcant local deviations.

Because layers are temporally persistent  it is also possible to reason about their relative depth or-
dering. In general  reliable recovery of depth order requires three or more frames. Our probabilistic
formulation explicitly orders layers by depth  and we show that the correct order typically produces
more probable (lower energy) solutions. This also allows explicit reasoning about occlusions  which
our model predicts at locations where the layer segmentations for consecutive frames disagree.

Many previous layered approaches are not truly “layered”: while they segment the image into mul-
tiple regions with distinct motions  they do not model what is in front of what. For example  widely
used MRF models [27] encourage neighboring pixels to occupy the same region  but do not capture
relationships between regions. In contrast  building on recent state-of-the-art results in static scene
segmentation [21]  our model determines layer support via an ordered sequence of occluding binary
masks. These binary masks are generated by thresholding a series of random  continuous functions.
This approach uses image-dependent Gaussian random ﬁeld priors and favors partitions which ac-
curately match the statistics of real scenes [21]. Moreover  the continuous layer support functions
play a key role in accurately modeling temporal layer consistency. The resulting model produces
accurate layer segmentations that improve ﬂow accuracy at occlusion boundaries  and recover mean-
ingful scene structure.

As summarized in Figure 1  our method is based on a principled  probabilistic generative model
for image sequences. By combining recent advances in dense ﬂow estimation and natural image
segmentation  we develop an algorithm that simultaneously estimates accurate ﬂow ﬁelds  detects
occlusions and disocclusions  and recovers the layered structure of realistic scenes.

2 Previous Work

Layered approaches to motion estimation have long been seen as elegant and promising  since spatial
smoothness is separated from the modeling of discontinuities and occlusions. Darrell and Pentland
[7  8] provide the ﬁrst full approach that incorporates a Bayesian model  “support maps” for seg-
mentation  and robust statistics. Wang and Adelson [25] clearly motivate layered models of image
sequences  while Jepson and Black [11] formalize the problem using probabilistic mixture models.
A full review of more recent methods is beyond our scope [1  3  12  13  16  17  20  24  27  29].

Early methods  which use simple parametric models of image motion within layers  are not highly
accurate. Observing that rigid parametric models are too restrictive for real scenes  Weiss [26] uses a
more ﬂexible Gaussian process to describe the motion within each layer. Even using modern imple-
mentation methods [22] this approach does not achieve state-of-the-art results. Allocating a separate
layer for every small surface discontinuity is impractical and fails to capture important global scene
structure. Our approach  which allows “roughness” within layers rather than “smoothness ” provides
a compromise that captures coarse scene structure as well as ﬁne within-layer details.

One key advantage of layered models is their ability to realistically model occlusion boundaries.
To do this properly  however  one must know the relative depth order of the surfaces. Performing
inference over the combinatorial range of possible occlusion relationships is challenging and  con-
sequently  only a few layered ﬂow models explicitly encode relative depth [12  30]. Recent work
revisits the layered model to handle occlusions [9]  but does not explicitly model the layer ordering
or achieve state-of-the-art performance on the Middlebury benchmark. While most current optical
ﬂow methods are “two-frame ” layered methods naturally extend to longer sequences [12  29  30].

Layered models all have some way of making either a hard or soft assignment of pixels to layers.
Weiss and Adelson [27] introduce spatial coherence to these layer assignments using a spatial MRF

2

tkg

tks

K

g

t+1 k

K−1

s

t+1 k

K

tI

t+1I

tku

tkv

K

Figure 1: Left: Graphical representation for the proposed layered model. Right: Illustration of variables from
the graphical model for the “Schefﬂera” sequence. Labeled sub-images correspond to nodes in the graph. The
left column shows the ﬂow ﬁelds for three layers  color coded as in [2]. The g and s images illustrate the
reasoning about layer ownership (see text). The composite ﬂow ﬁeld (u  v) and layer labels (k) are also shown.

model. However  the Ising/Potts MRF they employ assigns low probability to typical segmentations
of natural scenes [15]. Adapting recent work on static image segmentation by Sudderth and Jor-
dan [21]  we instead generate spatially coherent  ordered layers by thresholding a series of random
continuous functions. As in the single-image case  this approach realistically models the size and
shape properties of real scenes. For motion estimation there are additional advantages: it allows
accurate reasoning about occlusion relationships and modeling of temporal layer consistency.

3 A Layered Motion Model

Building on this long sequence of prior work  our generative model of layered image motion is
summarized in Figure 1. Below we describe how the generative model captures piecewise smooth
deviation of the layer motion from parametric models (Sec. 3.1)  depth ordering and temporal con-
sistency of layers (Sec. 3.2)  and regions of occlusion and disocclusion (Sec. 3.3).

3.1 Roughness in Layers

Our approach is inspired by Weiss’s model of smoothness in layers [26]. Given a sequence of
images It  1 ≤ t ≤ T   we model the evolution from the current frame It  to the subsequent frame
It+1  via K locally smooth  but potentially globally complex  ﬂow ﬁelds. Let utk and vtk denote
the horizontal and vertical ﬂow ﬁelds  respectively  for layer k at time t. The corresponding ﬂow
vector for pixel (i  j) is then denoted by (uij
Each layer’s ﬂow ﬁeld is drawn from a distribution chosen to encourage piecewise smooth motion.
For example  a pairwise Markov random ﬁeld (MRF) would model the horizontal ﬂow ﬁeld as

tk  vij

tk).

p(utk) ∝ exp{−Emrf(utk)} = exp(cid:26) −

1

2 X(i j) X(i′ j ′)∈Γ(i j)

ρs(uij

tk − ui′j ′

tk )(cid:27).

(1)

Here  Γ(i  j) is the set of neighbors of pixel (i  j)  often its four nearest neighbors. The potential
ρs(·) is some robust function [5] that encourages smoothness  but allows occasional signiﬁcant de-
viations from it. The vertical ﬂow ﬁeld vtk can then be modeled via an independent MRF prior as
in Eq. (1)  as justiﬁed by the statistics of natural ﬂow ﬁelds [18].

While such MRF priors are ﬂexible  they capture very little dependence between pixels separated by
even moderate image distances. In contrast  real scenes exhibit coherent motion over large scales 
due to the motion of (partially) rigid objects in the world. To capture this  we associate an afﬁne (or
planar) motion model  with parameters θtk  to each layer k. We then use an MRF to allow piecewise
smooth deformations from the globally rigid assumptions of afﬁne motion:

Eaff(utk  θtk) =

1

2 X(i j) X(i′ j ′)∈Γ(i j)

tk − ¯uij

θtk

ρs(cid:16)(uij

) − (ui′j ′

tk − ¯ui′j ′

θtk

)(cid:17).

(2)

3

Here  ¯uij
denotes the horizontal motion predicted for pixel (i  j) by an afﬁne model with pa-
θtk
rameters θtk. Unlike classical models that assume layers are globally well ﬁt by a single afﬁne
motion [6  25]  this prior allows signiﬁcant  locally smooth deviations from rigidity. Unlike the ba-
sic smoothness prior of Eq. (1)  this semiparametric construction allows effective global reasoning
about non-contiguous segments of partially occluded objects. More sophisticated ﬂow deformation
priors may also be used  such as those based on robust non-local terms [22  28].

3.2 Layer Support and Spatial Contiguity

The support for whether or not a pixel belongs to a given layer k is deﬁned using a hidden random
ﬁeld gk. We associate each of the ﬁrst K − 1 layers at time t with a random continuous function
gtk  deﬁned over the same domain as the image. This hidden support ﬁeld is illustrated in Figure 1.
We assume a single  unique layer is observable at each location and that the observed motion of
that pixel is determined by its assigned layer. Analogous to level set representations  the discrete
support of each layer is determined by thresholding gtk: pixel (i  j) is considered visible when
gtk(i  j) ≥ 0. Let stk(i  j) equal one if layer k is visible at pixel (i  j)  and zero otherwise; note that

Pk stk(i  j) = 1. For pixels (i  j) for which gtk(i  j) < 0  we necessarily have stk(i  j) = 0.

We deﬁne the layers to be ordered with respect to the camera  so that layer k occludes layers k′ > k.
Given the full set of support functions gtk  the unique layer kij

(i  j) = 1 is then

t∗ for which stkij

t∗

kij
t∗ = min ({k | 1 ≤ k ≤ K − 1  gtk(i  j) ≥ 0} ∪ {K}) .

(3)

Note that layer K is essentially a background layer that captures all pixels not assigned to the ﬁrst
K − 1 layers. For this reason  only K − 1 hidden ﬁelds gtk are needed (see Figure 1).
Our use of thresholded  random continuous functions to deﬁne layer support is partially motivated
by known shortcomings of discrete Ising/Potts MRF models for image partitions [15]. They also
provide a convenient framework for modeling the temporal and spatial coherence observed in real
motion sequences. Spatial coherence is captured via a Gaussian conditional random ﬁeld in which
edge weights are modulated by local differences in Lab color vectors  Ic

t (i  j):

(4)

(5)

Espace(gtk) =

1

2 X(i j) X(i′ j ′)∈Γ(i j)
i′j ′ = max(cid:26) expn −

1
2σ2
c

wij

wij

i′j ′ (gtk(i  j) − gtk(i′  j ′))2 

||Ic

t (i  j) − Ic

t (i′  j ′)||2o  δc(cid:27).

The threshold δc > 0 adds robustness to large color changes in internal object texture. Temporal
coherence of surfaces is then encouraged via a corresponding Gaussian MRF:

Etime(gtk  gt+1 k  utk  vtk) = X(i j)

(gtk(i  j) − gt+1 k(i + uij

tk  j + vij

tk))2.

(6)

Critically  this energy function uses the corresponding ﬂow ﬁeld to non-rigidly align the layers at
subsequent frames. By allowing smooth deformation of the support functions gtk  we allow layer
support to evolve over time  as opposed to transforming a single rigid template [12].

Our model of layer coherence is inspired by a recent method for image segmentation  based on
spatially dependent Pitman-Yor processes [21]. That work makes connections between layered oc-
clusion processes and stick breaking representations of nonparametric Bayesian models. By as-
signing appropriate stochastic priors to layer thresholds  the Pitman-Yor model captures the power
law statistics of natural scene partitions and infers an appropriate number of segments for each
image. Existing optical ﬂow benchmarks employ artiﬁcially constructed scenes that may have dif-
ferent layer-level statistics. Consequently our experiments in this paper employ a ﬁxed number of
layers K.

3.3 Depth Ordering and Occlusion Reasoning

The preceding generative process deﬁnes a set of K ordered layers  with corresponding ﬂow
ﬁelds utk  vtk and segmentation masks stk. Recall that the layer assignment masks s are a

4

deterministic function (threshold) of the underlying continuous layer support functions g (see
Eq. (3)). To consistently reason about occlusions  we examine the layer assignments stk(i  j) and
st+1 k(i + uij
tk) at locations corresponded by the underlying ﬂow ﬁelds. This leads to a far
richer occlusion model than standard spatially independent outlier processes: geometric consistency
is enforced via the layered sequence of ﬂow ﬁelds.

tk  j + vij

t (i  j) denote an observed image feature for pixel (i  j); we work with a ﬁltered
Let Is
If
version of the intensity images to provide some invariance to illumination changes.
stk(i  j) = st+1 k(i + uij
tk) = 1  the visible layer for pixel (i  j) at time t remains unoc-
cluded at time t + 1  and the image observations are modeled using a standard brightness (or  here 
feature) constancy assumption. Otherwise  that pixel has become occluded  and is instead generated
from a uniform distribution. The image likelihood model can then be written as

tk  j + vij

p(Is

t | Is

t+1  ut  vt  gt  gt+1) ∝ exp{−Edata(ut  vt  gt  gt+1)}

t (i  j) − Is

t+1(i + uij

tk  j + vij

tk))stk(i  j)st+1 k(i + uij

tk  j + vij
tk)

= expn −Xk X(i j)(cid:16)ρd(Is

where ρd(·) is a robust potential function and the constant λd arises from the difference of the log
normalization constants for the robust and uniform distributions. With algebraic simpliﬁcations  the
data error term can be written as

+ λdstk(i  j)(1 − st+1 k(i + uij

tk  j + vij

tk))(cid:17)o

Edata(ut  vt  gt  gt+1) =

Xk X(i j)(cid:16)ρd(Is

t (i  j) − Is

t+1(i + uij

tk  j + vij

tk)) − λd(cid:17)stk(i  j)st+1 k(i + uij

tk  j + vij
tk)

(7)

up to an additive  constant multiple of λd. The shifted potential function (ρd(·) − λd) represents the
change in energy when a pixel transitions from an occluded to an unoccluded conﬁguration. Note
that occlusions have higher likelihood only for sufﬁciently large discrepancies in matched image
features and can only occur via a corresponding change in layer visibility.

4 Posterior Inference from Image Sequences

Considering the full generative model deﬁned in Sec. 3  maximum a posteriori (MAP) estimation
for a T frame image sequence is equivalent to minimization of the following energy function:

E(u  v  g  θ) =

λa(Eaff(utk  θtk) + Eaff(vtk  θtk))

T −1

K

Xt=1 (cid:26)Edata(ut  vt  gt  gt+1) +
λbEspace(gtk) + λcEtime(gtk  gt+1 k  utk  vtk)(cid:27) +
Xk=1

Xk=1

K−1

+

K−1

Xk=1

Here λa  λb  and λc are weights controlling the relative importance of the afﬁne  spatial  and tempo-
ral terms respectively. Simultaneously inferring ﬂow ﬁelds  layer support maps  and depth ordering
is a challenging process; our approach is summarized below.

λbEspace(gT k).

(8)

4.1 Relaxation of the Layer Assignment Process

Due to the non-differentiability of the threshold process that determines assignments of regions to
layers  direct minimization of Eq. (8) is challenging. For a related approach to image segmentation 
a mean ﬁeld variational method has been proposed [21]. However  that segmentation model is based
on a much simpler  spatially factorized likelihood model for color and texture histogram features.
Generalization to the richer ﬂow likelihoods considered here raises signiﬁcant complications.

Instead  we relax the hard threshold assignment process using the logistic function σ(g) = 1/(1 +
exp(−g)). Applied to Eq. (3)  this induces the following soft layer assignments:

˜stk(i  j) =(σ(λegtk(i  j))Qk−1

k′=1 σ(−λegtk′ (i  j)) 

k′=1 σ(−λegtk′ (i  j)) 

QK−1

5

1 ≤ k < K 

k = K.

(9)

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 2: Results on the “Venus” sequence with 4 layers. The two background layers move faster than the two
foreground layers  and the solution with the correct depth ordering has lower energy and smaller error. (a) First
786 × 106. Left to right: motion segmentation 
frame. (b-d) Fast-to-slow ordering: EPE 0
estimated ﬂow ﬁeld  and absolute error of estimated ﬂow ﬁeld. (f-g) Slow-to-fast ordering: EPE 0
195 and
energy −1

808 × 106. Darker indicates larger ﬂow ﬁeld errors in (d) and (g).

.

.

252 and energy −1

.

.

Note that σ(−g) = 1 − σ(g)  andPK

Substituting these soft assignments ˜stk(i  j) for stk(i  j) in Eq. (7)  we obtain a differentiable energy
function that can be optimized via gradient-based methods. A related relaxation underlies the classic
backpropagation algorithm for neural network training.

k=1 ˜stk(i  j) = 1 for any gtk and constant λe > 0.

4.2 Gradient-Based Energy Minimization

We estimate the hidden ﬁelds for all the frames together  while ﬁxing the ﬂow ﬁelds  by optimizing
an objective involving the relevant Edata(·)  Espace(·)  and Etime(·) terms. We then estimate the ﬂow
ﬁelds ut  vt for each frame  while ﬁxing those of neighboring frames and the hidden ﬁelds  via the
Edata(·)  Eaff(·)  and Etime(·) terms. For ﬂow estimation  we use a standard coarse-to-ﬁne  warping-
based technique as described in [22]. For hidden ﬁeld estimation  we use an implementation of
conjugate gradient descent with backtracking and line search. See Supplemental Material for details.

5 Experimental Results

We apply the proposed model to two-frame sequences and compute both the forward and backward
ﬂow ﬁelds. This enables the use of the temporal consistency term by treating one frame as both
the previous and the next frame of the other1. We obtain an initial ﬂow ﬁeld using the Classic+NL
method [22]  cluster the ﬂow vectors into K groups (layers)  and convert the initial segmentation
into the corresponding hidden ﬁelds. We then use a two-level Gaussian pyramid (downsampling
factor 0.8) and perform a fairly standard incremental estimation of the ﬂow ﬁelds for each layer. At
each level  we perform 20 incremental warping steps and during each step alternately solve for the
hidden ﬁelds and the ﬂow estimates. In the end  we threshold the hidden ﬁelds to compute a hard
segmentation  and obtain the ﬁnal ﬂow ﬁeld by selecting the ﬂow ﬁeld from the appropriate layers.

Occluded regions are determined by inconsistencies between the hard segmentations at subsequent
frames  as matched by the ﬁnal ﬂow ﬁeld. We would ideally like to compare layer initializations
based on all permutations of the initial ﬂow vector clusters  but this would be computationally inten-
sive for large K. Instead we compare two orders: a fast-to-slow order appropriate for rigid scenes 
and an opposite slow-to-fast order (for variety and robustness). We illustrate automatic selection of
the preferred order for the “Venus” sequence in Figure 2.
The parameters for all experiments are set to λa = 3  λb = 30  λc = 4  λd = 9  λe = 2 
σi = 12  and δc = 0.004. A generalized Charbonnier function is used for ρS(·) and ρd(·) (see
Supplemental Material). Optimization takes about 5 hours for the two-frame “Urban” sequence
using our MATLAB implementation.

5.1 Results on the Middlebury Benchmark

Training Set As a baseline  we implement the smoothness in layers model [26] using modern
techniques  and obtain an average training end-point error (EPE) of 0.487. This is reasonable but
not competitive with state-of-the-art methods. The proposed model with 1 to 4 layers produces
average EPEs of 0.248  0.212  0.200  and 0.194  respectively (see Table 1). The one-layer model is
similar to the Classic+NL method  but has a less sophisticated (more local) model of the ﬂow within

1Our model works for longer sequences. We use two frames here for fair comparison with other methods.

6

Table 1: Average end-point error (EPE) on the Middlebury optical ﬂow benchmark training set.

Venus
0.510
0.271
0.238
0.243
0.219
0.212
0.197
0.211
0.212

Dimetrodon

Hydrangea

RubberWhale

0.179
0.128
0.131
0.144
0.147
0.149
0.148
0.150
0.151

0.249
0.153
0.152
0.175
0.169
0.173
0.159
0.161
0.161

0.236
0.081
0.073
0.095
0.081
0.073
0.068
0.067
0.066

Grove2
0.221
0.139
0.103
0.125
0.098
0.090
0.088
0.086
0.087

Grove3
0.608
0.614
0.468
0.504
0.376
0.343
0.359
0.331
0.339

Urban2
0.614
0.336
0.220
0.279
0.236
0.220
0.230
0.210
0.210

Urban3
1.276
0.555
0.384
0.422
0.370
0.338
0.300
0.345
0.396

Avg. EPE

Weiss [26]
Classic++
Classic+NL
1layer
2layers
3layers
4layers
3layers w/ WMF
3layers w/ WMF C++Init

0.487
0.285
0.221
0.248
0.212
0.200
0.194
0.195
0.203

Table 2: Average end-point error (EPE) on the Middlebury optical ﬂow benchmark test set.

Rank

Average

Army

Mequon

Schefﬂera

Wooden

Grove

Urban

Yosemite

Teddy

EPE
Layers++
Classic+NL
EPE in boundary regions
Layers++
Classic+NL

4.3
6.5

0.270
0.319

0.560
0.689

0.08
0.08

0.21
0.23

0.19
0.22

0.56
0.74

0.20
0.29

0.40
0.65

0.13
0.15

0.58
0.73

0.48
0.64

0.70
0.93

0.47
0.52

1.01
1.12

0.15
0.16

0.14
0.13

0.46
0.49

0.88
0.98

that layer. It thus performs worse than the Classic+NL initialization; the performance improvements
allowed by additional layers demonstrate the beneﬁts of a layered model.

Accuracy is improved by applying a 15 × 15 weighted median ﬁlter (WMF) [22] to the ﬂow ﬁelds of
each layer during the iterative warping step (EPE for 1 to 4 layers: 0.231  0.204  0.195  and 0.193).
Weighted median ﬁltering can be interpreted as a non-local spatial smoothness term in the energy
function that integrates ﬂow ﬁeld information over a larger spatial neighborhood.

The “correct” number of layers for a real scene is not well deﬁned (consider the “Grove3” sequence 
for example). We use a restricted number of layers  and model the remaining complexity of the ﬂow
within each layer via the roughness-in-layers spatial term and the WMF. As the number of layers
increases  the complexity of the ﬂow within each layer decreases  and consequently the need for
WMF also decreases; note that the difference in EPE for the 4-layer model with and without WMF
is insigniﬁcant. For the remaining experiments we use the version with WMF.

To test the sensitivity of the result to the initialization  we also initialized with Classic++ (“C++Init”
in Table 1)  a good  but not top  non-layered method [22]. The average EPE for 1 to 4 layers increases
to 0.248  0.206  0.203  and 0.198  respectively. While the one-layer method gets stuck in poor local
minima on the “Grove3” and “Urban3” sequences  models with additional layers are more robust to
the initialization. For more details and full EPE results  see the Supplemental Material.

Test Set For evaluation  we focus on a model with 3 layers (denoted “Layers++” in the Middlebury
public table). On the Middlebury test set it has an average EPE of 0.270 and average angular error
(AAE) of 2.556; this is the lowest among all tested methods [2] at the time of writing (Oct. 2010).
Table 2 summarizes the results for individual test sequences. The layered model is particularly
accurate at motion boundaries  probably due to the use of layer-speciﬁc motion models  and the
explicit modeling of occlusion in Edata (Eq. (7)). For more extensive results  see the Supplemental
Material.

Visual Comparison Figure 3 shows results for the 3-layer model on several training and test
sequences. Notice that the layered model produces a motion segmentation that captures the major
structure of the scene  and the layer boundaries correspond well to static image edges. It detects most
occlusion regions and interpolates their motion reasonably well. Several sequences show signiﬁcant
improvement due to the global reasoning provided by the layered model. On the training “Grove3”
sequence  the proposed method correctly identiﬁes many holes between the branches and leaves as
background. It also associates the branch at the bottom right corner with branches in the center.
As the branch moves beyond the image boundary  the layered model interpolates its motion using
long-range correlation with the branches in the center.
In contrast  the single-layered approach
incorrectly interpolates from local background regions. The “Schefﬂera” result illustrates how the
layered method can separate foreground objects from the background (e.g.  the leaves in the top
right corner)  and thereby reduce errors made by single-layer approaches such as Classic+NL.

7

Figure 3: Results on some Middlebury training (rows 1 to 3) and test (rows 4 to 6) sequences. Top to bottom:
“RubberWhale”  “Grove3”  “Urban3”  “Mequon”  “Schefﬂera”  and “Grove”. Left to right: First image frame 
initial ﬂow ﬁeld from “Classic+NL”  ﬁnal ﬂow ﬁeld  motion segmentation (green front  blue middle  red back) 
and detected occlusions. Best viewed in color and enlarged to allow comparison of detailed motions.

6 Conclusion and Discussion

We have described a new probabilistic formulation for layered image motion that explicitly models
occlusion and disocclusion  depth ordering of layers  and the temporal consistency of the layer seg-
mentation. The approach allows the ﬂow ﬁeld in each layer to have piecewise smooth deformation
from a parametric motion model. Layer support is modeled using an image-dependent hidden ﬁeld
prior that supports a model of temporal layer continuity over time. The image data error term takes
into account layer occlusion relationships  resulting in increased ﬂow accuracy near motion bound-
aries. Our method achieves state-of-the-art results on the Middlebury optical ﬂow benchmark while
producing meaningful segmentation and occlusion detection results.

Future work will address better inference methods  especially a better scheme to infer the layer or-
der  and the automatic estimation of the number of layers. Computational efﬁciency has not been
addressed  but will be important for inference on long sequences. Currently our method does not
capture transparency  but this could be supported using a soft layer assignment and a different gen-
erative model. Additionally  the parameters of the model could be learned [23]  but this may require
more extensive and representative training sets. Finally  the parameters of the model  especially the
number of layers  should adapt to the motions in a given sequence.

Acknowledgments DS and MJB were supported in part by the NSF Collaborative Research in Computa-
tional Neuroscience Program (IIS–0904875) and a gift from Intel Corp.

8

References
[1] S. Ayer and H. S. Sawhney. Layered representation of motion video using robust maximum-likelihood

estimation of mixture models and MDL encoding. In ICCV  pages 777–784  Jun 1995.

[2] S. Baker  D. Scharstein  J. P. Lewis  S. Roth  M. J. Black  and R. Szeliski. A database and evaluation

methodology for optical ﬂow. IJCV  to appear.

[3] S. Birchﬁeld and C. Tomasi. Multiway cut for stereo and motion with slanted surfaces. In ICCV  pages

489–495  1999.

[4] M. J. Black and P. Anandan. Robust dynamic motion estimation over time. In CVPR  pages 296–302 

1991.

[5] M. J. Black and P. Anandan. The robust estimation of multiple motions: Parametric and piecewise-smooth

ﬂow ﬁelds. CVIU  63:75–104  1996.

[6] M. J. Black and A. D. Jepson. Estimating optical-ﬂow in segmented images using variable-order para-

metric models with local deformations. PAMI  18(10):972–986  October 1996.

[7] T. Darrell and A. Pentland. Robust estimation of a multi-layered motion representation. In Workshop on

Visual Motion  pages 173–178  1991.

[8] T. Darrell and A. Pentland. Cooperative robust estimation using layers of support. PAMI  17(5):474–487 

1995.

[9] B. Glocker  T. H. Heibel  N. Navab  P. Kohli  and C. Rother. Triangleﬂow: Optical ﬂow with triangulation-

based higher-order likelihoods. In ECCV  pages 272–285  2010.

[10] M. Irani  P. Anandan  and D. Weinshall. From reference frames to reference planes: Multi-view parallax

geometry and applications. In ECCV  1998.

[11] A. Jepson and M. J. Black. Mixture models for optical ﬂow computation. In CVPR  1993.
[12] N. Jojic and B. Frey. Learning ﬂexible sprites in video layers. In CVPR  pages I:199–206  2001.
[13] A. Kannan  B. Frey  and N. Jojic. A generative model of dense optical ﬂow in layers. Technical Report

TR PSI-2001-11  University of Toronto  Aug. 2001.

[14] R. Kumar  P. Anandan  and K. Hanna. Shape recovery from multiple views: A parallax based approach.

In Proc 12th ICPR  1994.

[15] R. D. Morris  X. Descombes  and J. Zerubia. The Ising/Potts model is not well suited to segmentation

tasks. In Proceedings of the IEEE Digital Signal Processing Workshop  1996.

[16] M. Nicolescu and G. Medioni. Motion segmentation with accurate boundaries - a tensor voting approach.

In CVPR  pages 382–389  2003.

[17] M. P. Kumar  P. H. Torr  and A. Zisserman. Learning layered motion segmentations of video.

76(3):301–319  2008.

IJCV 

[18] S. Roth and M. J. Black. On the spatial statistics of optical ﬂow. IJCV  74(1):33–50  August 2007.
[19] H. S. Sawhney. 3D geometry from planar parallax. In CVPR  pages 929–934  1994.
[20] T. Schoenemann and D. Cremers. High resolution motion layer decomposition using dual-space graph

cuts. In CVPR  pages 1–7  June 2008.

[21] E. Sudderth and M. Jordan. Shared segmentation of natural scenes using dependent Pitman-Yor processes.

In NIPS  pages 1585–1592  2009.

[22] D. Sun  S. Roth  and M. J. Black. Secrets of optical ﬂow estimation and their principles. In CVPR  2010.
[23] D. Sun  S. Roth  J. P. Lewis  and M. J. Black. Learning optical ﬂow. In ECCV  pages 83–97  2008.
[24] P. Torr  R. Szeliski  and P. Anandan. An integrated Bayesian approach to layer extraction from image

sequences. PAMI  23(3):297–303  Mar 2001.

[25] J. Y. A. Wang and E. H. Adelson. Representing moving images with layers. IEEE Transactions on Image

Processing  3(5):625–638  Sept. 1994.

[26] Y. Weiss. Smoothness in layers: Motion segmentation using nonparametric mixture estimation. In CVPR 

pages 520–526  Jun 1997.

[27] Y. Weiss and E. Adelson. A uniﬁed mixture framework for motion segmentation: Incorporating spatial

coherence and estimating the number of models. In CVPR  pages 321–326  Jun 1996.

[28] M. Werlberger  T. Pock  and H. Bischof. Motion estimation with non-local total variation regularization.

In CVPR  2010.

[29] H. Yalcin  M. J. Black  and R. Fablet. The dense estimation of motion and appearance in layers. In IEEE

Workshop on Image and Video Registration  pages 777–784  Jun 2004.

[30] Y. Zhou and H. Tao. Background layer model for object tracking through occlusion. In ICCV  volume 2 

pages 1079–1085  2003.

9

,Minh Ha Quang
Marco San Biagio
Vittorio Murino
Huan Li
Zhouchen Lin