2016,Hardness of Online Sleeping Combinatorial Optimization Problems,We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically  we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions  a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths  Online Minimum Spanning Tree  Online k-Subsets  Online k-Truncated Permutations  Online Minimum Cut  and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 [Koolen et al.  2015].,Hardness of Online Sleeping Combinatorial

Optimization Problems

Satyen Kale∗†
Yahoo Research

satyen@satyenkale.com

Chansoo Lee†

Univ. of Michigan  Ann Arbor

chansool@umich.edu

D´avid P´al

Yahoo Research

dpal@yahoo-inc.com

Abstract

We show that several online combinatorial optimization problems that admit ef-
ﬁcient no-regret algorithms become computationally hard in the sleeping setting
where a subset of actions becomes unavailable in each round. Speciﬁcally  we
show that the sleeping versions of these problems are at least as hard as PAC learn-
ing DNF expressions  a long standing open problem. We show hardness for the
sleeping versions of ONLINE SHORTEST PATHS  ONLINE MINIMUM SPANNING
TREE  ONLINE k-SUBSETS  ONLINE k-TRUNCATED PERMUTATIONS  ONLINE
MINIMUM CUT  and ONLINE BIPARTITE MATCHING. The hardness result for
the sleeping version of the Online Shortest Paths problem resolves an open prob-
lem presented at COLT 2015 [Koolen et al.  2015].

1

Introduction

Online learning is a sequential decision-making problem where learner repeatedly chooses an action
in response to adversarially chosen losses for the available actions. The goal of the learner is to
minimize the regret  deﬁned as the difference between the total loss of the algorithm and the loss of
the best ﬁxed action in hindsight. In online combinatorial optimization  the actions are subsets of
a ground set of elements (also called components) with some combinatorial structure. The loss of
an action is the sum of the losses of its elements. A particular well-studied instance is the ONLINE
SHORTEST PATH problem [Takimoto and Warmuth  2003] on a graph  in which the actions are the
paths between two ﬁxed vertices and the elements are the edges.
We study a sleeping variant of online combinatorial optimization where the adversary not only
chooses losses but availability of the elements every round. The unavailable elements are called
sleeping or sabotaged. In ONLINE SABOTAGED SHORTEST PATH problem  for example  the ad-
versary speciﬁes unavailable edges every round  and consequently the learner cannot choose any
path using those edges. A straightforward application of the sleeping experts algorithm proposed
by Freund et al. [1997] gives a no-regret learner  but it takes exponential time (in the input graph
size) every round. The design of a computationally efﬁcient no-regret algorithm for ONLINE SAB-
OTAGED SHORTEST PATH problem was presented as an open problem at COLT 2015 by Koolen
et al. [2015].
In this paper  we resolve this open problem and prove that ONLINE SABOTAGED SHORTEST PATH
problem is computationally hard. Speciﬁcally  we show that a polynomial-time low-regret algorithm
for this problem implies a polynomial-time algorithm for PAC learning DNF expressions  which is
a long-standing open problem. The best known algorithm for PAC learning DNF expressions on n

variables has time complexity 2(cid:101)O(n1/3) [Klivans and Servedio  2001].
∗Current afﬁliation: Google Research.
†This work was done while the authors were at Yahoo Research.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Our reduction framework (Section 4) in fact shows a general result that any online sleeping com-
binatorial optimization problem with two simple structural properties is as hard as PAC learning
DNF expressions. Leveraging this result  we obtain hardness results for the sleeping variant of well-
studied online combinatorial optimization problems for which a polynomial-time no-regret algo-
rithm exists: ONLINE MINIMUM SPANNING TREE  ONLINE k-SUBSETS  ONLINE k-TRUNCATED
PERMUTATIONS  ONLINE MINIMUM CUT  and ONLINE BIPARTITE MATCHING (Section 5).
Our hardness result applies to the worst-case adversary as well as a stochastic adversary  who draws
an i.i.d. sample every round from a ﬁxed (but unknown to the learner) joint distribution over avail-
abilities and losses. This implies that no-regret algorithms would require even stronger restrictions
on the adversary.

1.1 Related Work

√
Online Combinatorial Optimization. The standard problem of online linear optimization with
d actions (Experts setting) admits algorithms with O(d) running time per round and O(
T log d)
regret after T rounds [Littlestone and Warmuth  1994  Freund and Schapire  1997]  which is mini-
max optimal [Cesa-Bianchi and Lugosi  2006  Chapter 2]. A naive application of such algorithms
√
to online combinatorial optimization problem (precise deﬁnitions to be given momentarily) over a
ground set of d elements will result in exp(O(d)) running time per round and O(
√
Despite this  many online combinatorial optimization problems  such as the ones considered in this
paper  admit algorithms with3 poly(d) running time per round and O(poly(d)
T ) regret [Takimoto
and Warmuth  2003  Kalai and Vempala  2005  Koolen et al.  2010  Audibert et al.  2013]. In fact 
Kalai and Vempala [2005] shows that the existence of a polynomial-time algorithm for an ofﬂine
√
combinatorial problem implies the existence of an algorithm for the corresponding online optimiza-
tion problem with the same per-round running time and O(poly(d)

T d) regret.

T ) regret.

Online Sleeping Optimization.
In studying online sleeping optimization  three different notions
of regret have been used: (a) policy regret  (b) ranking regret  and (c) per-action regret  in decreasing
order of computational hardness to achieve no-regret. Policy regret is the total difference between
the loss of the algorithm and the loss of the best policy  which maps a set of available actions and
the observed loss sequence to an available action [Neu and Valko  2014]. Ranking regret is the
total difference between the loss of the algorithm and the loss of the best ranking of actions  which
corresponds to a policy that chooses in each round the highest-ranked available action [Kleinberg
et al.  2010  Kanade and Steinke  2014  Kanade et al.  2009]. Per-action regret is the difference
between the loss of the algorithm and the loss of an action  summed over only the rounds in which
the action is available [Freund et al.  1997  Koolen et al.  2015]. Note that policy regret upper bounds
ranking regret  and while ranking regret and per-action regret are generally incomparable  per-action
regret is usually the smallest of the three notions.
The sleeping Experts (also known as Specialists) setting has been extensively studied in the literature
[Freund et al.  1997  Kanade and Steinke  2014]. In this paper we focus on the more general online
sleeping combinatorial optimization problem  and in particular  the per-action notion of regret.
A summary of known results for online sleeping optimization problems is given in Figure 1. Note
in particular that an efﬁcient algorithm was known for minimizing per-action regret in the sleeping
Experts problem [Freund et al.  1997]. We show in this paper that a similar efﬁcient algorithm for
minimizing per-action regret in online sleeping combinatorial optimization problems cannot exist 
unless there is an efﬁcient algorithm for learning DNFs. Our reduction technique is closely related to
that of Kanade and Steinke [2014]  who reduced agnostic learning of disjunctions to ranking regret
minimization in the sleeping Experts setting.

2 Preliminaries

An instance of online combinatorial optimization is deﬁned by a ground set U of d elements  and
a decision set D of actions  each of which is a subset of U. In each round t  the online learner is
required to choose an action Vt ∈ D  while simultaneously an adversary chooses a loss function
3In this paper  we use the poly(·) notation to indicate a polynomially bounded function of the arguments.

2

Regret notion

Policy

Ranking

Per-action

Sleeping Experts

Bound
√
Upper O(

T log d) 

under

ILA

[Kanade et al.  2009]

Lower

Lower Ω(poly(d)T 1−δ)  under SLA
[Kanade and Steinke  2014]

√
Upper O(

T log d)  adversarial setting

[Freund et al.  1997]

Lower

T ) 

under

Sleeping Combinatorial Opt.
√
O(poly(d)
ILA
[Neu and Valko  2014  Abbasi-
Yadkori et al.  2013]
Ω(poly(d)T 1−δ)  under SLA
[Abbasi-Yadkori et al.  2013]
T )  under SLA
Ω(exp(Ω(d))
[Easy construction  omitted]

√

Ω(poly(d)T 1−δ)  under SLA
[This paper]

Figure 1:
Summary of known results. Stochastic Losses and Availabilities (SLA) assumption is where
adversary chooses a joint distribution over loss and availability before the ﬁrst round  and takes an i.i.d. sample
every round. Independent Losses and Availabilities (ILA) assumption is where adversary chooses losses and
availabilities independently of each other (one of the two may be adversarially chosen; the other one is then
chosen i.i.d in each round). Policy regret upper bounds ranking regret which in turn upper bounds per-action
regret for the problems of interest; hence some bounds shown in some cells of the table carry over to other
cells by implication and are not shown for clarity. The lower bound on ranking regret in online sleeping
combinatorial optimization is unconditional and holds for any algorithm  efﬁcient or not. All other lower
bounds are computational  i.e. for polynomial time algorithms  assuming intractability of certain well-studied
learning problems  such as learning DNFs or learning noisy parities.

(cid:96)t : U → [−1  1]. The loss of any V ∈ D is given by (with some abuse of notation)

(cid:96)t(V ) := (cid:80)
RegretT (V ) := (cid:80)T

e∈V (cid:96)t(e).

t=1 (cid:96)t(Vt) − (cid:96)t(V ).

The learner suffers loss (cid:96)t(Vt) and obtains (cid:96)t as feedback. The regret of the learner with respect to
an action V ∈ D is deﬁned to be

We say that an online optimization algorithm has a regret bound of f (d  T ) if RegretT (V ) ≤ f (d  T )
for all V ∈ D. We say that the algorithm has no regret if f (d  T ) = poly(d)T 1−δ for some
δ ∈ (0  1)  and it is computationally efﬁcient if it has a per-round running time of order poly(d  T ).
We now deﬁne an instance of the online sleeping combinatorial optimization. In this setting  at the
start of each round t  the adversary selects a set of sleeping elements St ⊆ U and reveals it to the
learner. Deﬁne At = {V ∈ D | V ∩ St = ∅}  the set of awake actions at round t; the remaining
actions in D  called sleeping actions  are unavailable to the learner for that round. If At is empty 
i.e.  there are no awake actions  then the learner is not required to do anything for that round and the
round is discarded from computation of the regret.
For the rest of the paper  unless noted otherwise  we use per-action regret as our performance mea-
sure. Per-action regret with respect to V ∈ D is deﬁned as:

RegretT (V ) :=

(cid:96)t(Vt) − (cid:96)t(V ).

(1)

(cid:88)

t: V ∈At

In other words  our notion of regret considers only the rounds in which V is awake.
For clarity  we deﬁne an online combinatorial optimization problem as a family of instances of online
combinatorial optimization (and correspondingly for online sleeping combinatorial optimization).
For example  ONLINE SHORTEST PATH problem is the family of all instances of all graphs with
designated source and sink vertices  where the decision set D is a set of paths from the source to
sink  and the elements are edges of the graph.
Our main result is that many natural online sleeping combinatorial optimization problems are un-
likely to admit a computationally efﬁcient no-regret algorithm  although their non-sleeping versions
(i.e.  At = D for all t) do. More precisely  we show that these online sleeping combinatorial op-
timization problems are at least as hard as PAC learning DNF expressions  a long-standing open
problem.

3

Our goal is to design an algorithm that is competitive with any disjunction  i.e. for any disjunction
φ over n variables  the regret is bounded by poly(n) · T 1−δ for some δ ∈ (0  1). Recall that a
disjunction over n variables is a boolean function φ : {0  1}n → {0  1} that on an input x =
(x(1)  x(2)  . . .   x(n)) outputs

RegretT (φ) =(cid:80)T
t=1 1[(cid:98)yt (cid:54)= yt] − 1[φ(xt) (cid:54)= yt].
(cid:32)(cid:95)

(cid:32)(cid:95)

(cid:33)

(cid:33)

φ(x) =

x(i)

∨

i∈P

x(i)

i∈N

3 Online Agnostic Learning of Disjunctions

Instead of directly reducing PAC learning DNF expressions to no-regret learning for online sleep-
ing combinatorial optimization problems  we use an intermediate problem  online agnostic learning
of disjunctions. By a standard online-to-batch conversion argument [Kanade and Steinke  2014] 
online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of dis-
junctions [Kearns et al.  1994]  which in turn is at least as hard as PAC-learning of DNF expressions
[Kalai et al.  2012]. The online-to-batch conversion argument allows us to assume the stochastic
adversary (i.i.d. input sequence) for online agnostic learning of disjunctions  which in turn implies
that our reduction applies to online sleeping combinatorial optimization with a stochastic adversary.
Online agnostic learning of disjunctions is a repeated game between the adversary and a learning
algorithm. Let n denote the number of variables in the disjunction. In each round t  the adversary

chooses a vector xt ∈ {0  1}n  the algorithm predicts a label(cid:98)yt ∈ {0  1} and then the adversary
reveals the correct label yt ∈ {0  1}. If(cid:98)yt (cid:54)= yt  we say that algorithm makes an error.

For any predictor φ : {0  1}n → {0  1}  we deﬁne the regret with respect to φ after T rounds as

where P and N are disjoint subsets of {1  2  . . .   n}. We allow either P or N to be empty  and the
empty disjunction is interpreted as the constant 0 function. For any index i ∈ {1  2  . . .   n}  we call
it a relevant index for φ if i ∈ P ∪ N and irrelevant index for φ otherwise. For any relevant index i 
we call it positive if i ∈ P and negative if i ∈ N.

4 General Hardness Result

In this section  we identify two combinatorial properties of online sleeping combinatorial optimiza-
tion problems that are computationally hard.
Deﬁnition 1. Let n be a positive integer. Consider an instance of online sleeping combinatorial
optimization where the ground set U has d elements with 3n + 2 ≤ d ≤ poly(n). This instance
is called a hard instance with parameter n  if there exists a subset Us ⊆ U of size 3n + 2 and a
bijection between Us and the set (i.e.  labeling of elements in Us by the set)

n(cid:91)

{(i  0)  (i  1)  (i  (cid:63))} ∪ {0  1} 

such that the decision set D satisﬁes the following properties:

i=1

1. (Heaviness) Any action V ∈ D has at least n + 1 elements in Us.
2. (Richness) For

{(1  s1)  (2  s2)  . . .   (n  sn)  sn+1} ∈ Us is in D.

(s1  . . .   sn+1)

all

∈

{0  1  (cid:63)}n × {0  1} 

the

action

We now show how to use the above deﬁnition of hard instances to prove the hardness of an online
sleeping combinatorial optimization (OSCO) problem by reducing from the online agnostic learning
of disjunction (OALD) problem. At a high level  the reduction works as follows. Given an instance
of the OALD problem  we construct a speciﬁc instance of the the OSCO and a sequence of losses
and availabilities based on the input to the OALD problem. This reduction has the property that
for any disjunction  there is a special set of actions of size n + 1 such that (a) exactly one action
is available in any round and (b) the loss of this action exactly equals the loss of the disjunction on
the current input example. Furthermore  the action chosen by the OSCO can be converted into a
prediction in the OALD problem with only lesser or equal loss. These two facts imply that the regret
of the OALD algorithm is at most n + 1 times the per-action regret of the OSCO algorithm.

4

mization problem  and run Algosco on it.

input size n for the disjunction learning problem.

Algorithm 1 ALGORITHM ALGDISJ FOR LEARNING DISJUNCTIONS
Require: An algorithm Algosco for the online sleeping combinatorial optimization problem  and the
1: Construct a hard instance (U D) with parameter n of the online sleeping combinatorial opti-
2: for t = 1  2  . . .   T do
3:
4:
5:
6:
7:
8:

Receive xt ∈ {0  1}n.
Set the set of sleeping elements for Algosco to be St = {(i  1 − xt(i)) | i = 1  2  . . .   n}.
Set(cid:98)yt = 1[0 /∈ Vt].
Obtain an action Vt ∈ D by running Algosco such that Vt ∩ St = ∅.
Predict(cid:98)yt  and receive true label yt.
In algorithm Algosco  set the loss of the awake elements e ∈ U \ St as follows:

(cid:40) 1−yt

n+1

yt − n(1−yt)

n+1

if e (cid:54)= 0
if e = 0.

(cid:96)t(e) =

9: end for

Theorem 1. Consider an online sleeping combinatorial optimization problem such that for any
positive integer n  there is a hard instance with parameter n of the problem. Suppose there is an
algorithm Algosco that for any instance of the problem with ground set U of size d  runs in time
poly(T  d) and has regret bounded by poly(d) · T 1−δ for some δ ∈ (0  1). Then  there exists an
algorithm Algdisj for online agnostic learning of disjunctions over n variables with running time
poly(T  n) and regret poly(n) · T 1−δ.

Proof. Algdisj is given in Algorithm 1. First  we note that in each round t  we have

(2)
We prove this separately for two different cases; in both cases  the inequality follows from the
heaviness property  i.e.  the fact that |Vt| ≥ n + 1.

(cid:96)t(Vt) ≥ 1[yt (cid:54)=(cid:98)yt].

1. If 0 /∈ Vt  then the prediction of Algdisj is(cid:98)yt = 1  and thus
2. If 0 ∈ Vt  then the prediction of Algdisj is(cid:98)yt = 0  and thus
yt − n(1 − yt)

(cid:96)t(Vt) = |Vt| · 1 − yt
(cid:18)

(cid:96)t(Vt) = (|Vt| − 1) · 1 − yt

n + 1

+

(cid:19)

≥ 1 − yt = 1[yt (cid:54)=(cid:98)yt].

≥ yt = 1[yt (cid:54)=(cid:98)yt].

n + 1

n + 1

Note that if Vt satisﬁes the equality |Vt| = n + 1  then we have an equality (cid:96)t(Vt) = 1[yt (cid:54)=(cid:98)yt]; this

property will be useful later.
Next  let φ be an arbitrary disjunction  and let i1 < i2 < ··· < im be its relevant indices sorted
in increasing order. Deﬁne fφ : {1  2  . . .   m} → {0  1} as fφ(j) := 1[ij is a positive index for φ] 
and deﬁne the set of elements Wφ := {(i  (cid:63)) | i is an irrelevant index for φ}. Finally  let Dφ =
{V 1

} be the set of m + 1 actions where for j = 1  2  . . .   m  we deﬁne

φ   . . .   V m+1
φ   V 2
φ := {(i(cid:96)  1 − fφ((cid:96))) | 1 ≤ (cid:96) < j} ∪ {(ij  fφ(j))} ∪ {(i(cid:96)  (cid:63)) | j < (cid:96) ≤ m} ∪ Wφ ∪ {1} 
V j

φ

V m+1
φ

and
:= {(i(cid:96)  1 − fφ((cid:96))) | 1 ≤ (cid:96) ≤ m} ∪ Wφ ∪ {0}.
The actions in Dφ are indeed in the decision set D due to the richness property.
We claim that Dφ contains exactly one awake action in every round and the awake action contains
the element 1 if and only if φ(xt) = 1. First  we prove uniqueness: if V j
φ (where j < k)
are both awake in the same round  then (ij  fφ(j)) ∈ V j
φ are both awake
elements  contradicting our choice of St. To prove the rest of the claim  we consider two cases:

φ and V k
φ and (ij  1 − fφ(j)) ∈ V k

5

1. If φ(xt) = 1  then there is at least one j ∈ {1  2  . . .   m} such that xt(ij) = fφ(j). Let j(cid:48)
(cid:48)
φ is awake at time t  and 1 ∈ V j
φ  

be the smallest such j. Then  by construction  the set V j
as required.

(cid:48)

2. If φ(xt) = 0  then for all j ∈ {1  2  . . .   m} we must have xt(ij) = 1 − fφ(j). Then  by

construction  the set V m+1

φ

is awake at time t  and 0 ∈ V m+1

φ

  as required.

Since every action in Dφ has exactly n + 1 elements  and if V is awake action in Dφ at time t  we
just showed that 1 ∈ V if and only if φ(xt) = 1  exactly the same argument as in the beginning of
this proof implies that
(3)
Furthermore  since exactly one action in Dφ is awake every round  we have

(cid:96)t(V ) = 1[yt (cid:54)= φ(xt)].

T(cid:88)

(cid:88)

(cid:88)

1[yt (cid:54)= φ(xt)] =

t=1

V ∈Dφ

t: V ∈At

(cid:96)t(V ).

(4)

Finally  we can bound the regret of algorithm Algdisj (denoted Regretdisj
algorithm Algosco (denoted Regretosco

T

T ) in terms of the regret of

) as follows:

1[(cid:98)yt (cid:54)= yt] − 1[φ(xt) (cid:54)= yt] ≤ (cid:88)
T(cid:88)
(cid:88)

t=1

(cid:88)

Regretosco

T

(V ) ≤ |Dφ| · poly(d) · T 1−δ = poly(n) · T 1−δ 

V ∈Dφ

t: V ∈At

(cid:96)t(Vt) − (cid:96)t(V )

Regretdisj

T (φ) =

=

The ﬁrst inequality follows by (2) and (4)  and the last equation since |Dφ| ≤ n + 1 and d ≤
poly(n).

V ∈Dφ

4.1 Hardness results for Policy Regret and Ranking Regret

It is easy to see that our technique for proving hardness easily extends to ranking regret (and there-
fore  policy regret). The reduction simply uses any algorithm for minimizing ranking regret in
exactly one action Vt ∈ Dφ is awake in any round t  and (cid:96)t(Vt) = 1[yt (cid:54)=(cid:98)yt]. Thus  if we consider
Algorithm 1 as Algosco. This is because in the proof of Theorem 1  the set Dφ has the property that
a ranking where the actions in Dφ are ranked at the top positions (in arbitrary order)  the loss of this
ranking exactly equals the number of errors made by the disjunction φ on the input sequence. The
same arguments as in the proof of Theorem 1 then imply that the regret of Algdisj is bounded by that
of Algosco  implying the hardness result.

5 Hard Instances for Speciﬁc Problems

Now we apply Theorem 1 to prove that many online sleeping combinatorial optimization problems
are as hard as PAC learning DNF expressions by constructing hard instances for them. Note that all
these problems admit efﬁcient no-regret algorithms in the non-sleeping setting.

5.1 Online Shortest Path Problem

In the ONLINE SHORTEST PATH problem  the learner is given a directed graph G = (V  E) and
designated source and sink vertices s and t. The ground set is the set of edges  i.e. U = E 
and the decision set D is the set of all paths from s to t. The sleeping version of this problem
has been called the ONLINE SABOTAGED SHORTEST PATH problem by Koolen et al. [2015]  who
posed the open question of whether it admits an efﬁcient no-regret algorithm. For any n ∈ N  a
hard instance is the graph G(n) shown in Figure 2. It has 3n + 2 edges that are labeled by the
i=1{(i  0)  (i  1)  (i  (cid:63))} ∪ {0  1}  as required. Now note that any
s-t path in this graph has length exactly n + 1  so D satisﬁes the heaviness property. Furthermore 
the richness property is clearly satisﬁed  since for any s ∈ {0  1  (cid:63)}n × {0  1}  the set of edges
{(1  s1)  (2  s2)  . . .   (n  sn)  sn+1} is an s-t path and therefore in D.

elements of ground set U = (cid:83)n

6

Figure 2: Graph G(n).

Figure 3: Graph P (n). This is a complete bipartite graph as described in the text  but only the
special labeled edges shown for clarity.

5.2 Online Minimum Spanning Tree Problem

In the ONLINE MINIMUM SPANNING TREE problem  the learner is given a ﬁxed graph G = (V  E).
The ground set here is the set of edges  i.e. U = E  and the decision set D is the set of spanning
trees in the graph. For any n ∈ N  a hard instance is the same graph G(n) shown in Figure 2  except
that the edges are undirected. Note that the spanning trees in G(n) are exactly the paths from s to
t. The hardness of this problem immediately follows from the hardness of the ONLINE SHORTEST
PATHS problem.

5.3 Online k-Subsets Problem

In the ONLINE k-SUBSETS problem  the learner is given a ﬁxed ground set of elements U. The
decision set D is the set of subsets of U of size k. For any n ∈ N  we construct a hard instance with
parameter n of the ONLINE k-SUBSETS problem with k = n + 1 and d = 3n + 2. The set D of all
subsets of size k = n + 1 of a ground set U of size d = 3n + 2 clearly satisﬁes both the heaviness
and richness properties.

5.4 Online k-Truncated Permutations Problem

In the ONLINE k-TRUNCATED PERMUTATIONS problem (also called the ONLINE k-RANKING
problem)  the learner is given a complete bipartite graph with k nodes on one side and m ≥ k nodes
on the other  and the ground set U is the set of all edges; thus d = km. The decision set D is the
set of all maximal matchings  which can be interpreted as truncated permutations of k out of m ob-
jects. For any n ∈ N  we construct a hard instance with parameter n of the ONLINE k-TRUNCATED
PERMUTATIONS problem with k = n + 1  m = 3n + 2 and d = km = (n + 1)(3n + 2). Let
L = {u1  u2  . . .   un+1} be the nodes on the left side of the bipartite graph  and since m = 3n + 2 
let R = {vi 0  vi 1  vi (cid:63) | i = 1  2  . . .   n} ∪ {v0  v1} denote the nodes on the right side of the
graph. The ground set U consists of all d = km = (n + 1)(3n + 2) edges joining nodes in L to
nodes in R. We now specify the special 3n + 2 elements of the ground set U: for i = 1  2  . . .   n 
label the edges (ui  vi 0)  (ui  vi 1)  (ui  vi (cid:63)) by (i  0)  (i  1)  (i  (cid:63)) respectively. Finally  label the
edges (un+1  v0)  (un+1  v1) by 0 and 1 respectively. The resulting bipartite graph P (n) is shown in
Figure 3  where only the special labeled edges are shown for clarity.
Now note that any maximal matching in this graph has exactly n+1 edges  so the heaviness condition
is satisﬁed. Furthermore  the richness property is satisﬁed  since for any s ∈ {0  1  (cid:63)}n ×{0  1}  the
set of edges {(1  s1)  (2  s2)  . . .   (n  sn)  sn+1} is a maximal matching and therefore in D.

7

(2 1)(2 0)v2sv1vn−1vnt(1 0)(n 0)(1 1)(n 1)(1 ?)(2 ?)(n ?)10u1v1 1(1 0)(1 1)(1 ?)v1 ?v1 0u2v2 1(2 0)(2 1)(2 ?)v2 ?v2 0unvn 1(n 0)(n 1)(n ?)vn ?vn 010un+110Figure 4: Graph M (n) for the ONLINE BIPARTITE MATCHING problem.

Figure 5: Graph C (n) for the ONLINE MINIMUM CUT problem.

5.5 Online Bipartite Matching Problem

set U = (cid:83)n

In the ONLINE BIPARTITE MATCHING problem  the learner is given a ﬁxed bipartite graph
G = (V  E). The ground set here is the set of edges  i.e. U = E  and the decision set D is
the set of maximal matchings in G. For any n ∈ N  a hard instance with parameter n is the
graph M (n) shown in Figure 4. It has 3n + 2 edges that are labeled by the elements of ground
i=1{(i  0)  (i  1)  (i  (cid:63))} ∪ {0  1}  as required. Now note that any maximal match-
ing in this graph has size exactly n + 1  so D satisﬁes the heaviness property. Furthermore 
the richness property is clearly satisﬁed  since for any s ∈ {0  1  (cid:63)}n × {0  1}  the set of edges
{(1  s1)  (2  s2)  . . .   (n  sn)  sn+1} is a maximal matching and therefore in D.

5.6 Online Minimum Cut Problem

It has 3n + 2 edges that are labeled by the elements of ground set U =(cid:83)n

In the ONLINE MINIMUM CUT problem the learner is given a ﬁxed graph G = (V  E) with a
designated pair of vertices s and t. The ground set here is the set of edges  i.e. U = E  and the
decision set D is the set of cuts separating s and t: a cut here is a set of edges that when removed from
the graph disconnects s from t. For any n ∈ N  a hard instance is the graph C (n) shown in Figure 5.
i=1{(i  0)  (i  1)  (i  (cid:63))} ∪
{0  1}  as required. Now note that any cut in this graph has size at least n + 1  so D satisﬁes
the heaviness property. Furthermore  the richness property is clearly satisﬁed  since for any s ∈
{0  1  (cid:63)}n × {0  1}  the set of edges {(1  s1)  (2  s2)  . . .   (n  sn)  sn+1} is a cut and therefore in D.

6 Conclusion

In this paper we showed that obtaining an efﬁcient no-regret algorithm for sleeping versions of sev-
eral natural online combinatorial optimization problems is as hard as efﬁciently PAC learning DNF
expressions  a long-standing open problem. Our reduction technique requires only very modest con-
ditions for hard instances of the problem of interest  and in fact is considerably more ﬂexible than
the speciﬁc form presented in this paper. We believe that almost any natural combinatorial optimiza-
tion problem that includes instances with exponentially many solutions will be a hard problem in
its online sleeping variant. Furthermore  our hardness result is via stochastic i.i.d. availabilities and
losses  a rather benign form of adversary. This suggests that obtaining sublinear per-action regret
is perhaps a rather hard objective  and suggests that to obtain efﬁcient algorithms we might need to
either (a) make suitable simpliﬁcations of the regret criterion or (b) restrict the adversary’s power.

8

u1v1un+1vn+1(1 0)(1 1)(1 ?)10u2v2(2 0)(2 1)(2 ?)unvn(n 0)(n 1)(n ?)u1v1(1 0)(1 1)(1 ?)10u2v2(2 0)(2 1)(2 ?)unvn(n 0)(n 1)(n ?)stwReferences
Yasin Abbasi-Yadkori  Peter L. Bartlett  Varun Kanade  Yevgeny Seldin  and Csaba Szepesv´ari.
Online learning in markov decision processes with adversarially chosen transition probability
distributions. In Advances in Neural Information Processing Systems (NIPS)  pages 2508–2516 
2013.

Jean-Yves Audibert  Bubeck S´ebastien  and G´abor Lugosi. Regret in online combinatorial optimiza-

tion. Mathematics of Operations Research  39(1):31–45  2013.

Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  Learning and Games. Cambridge University

Press  New York  NY  2006.

Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

Yoav Freund  Robert E. Schapire  Yoram Singer  and Warmuth K. Manfred. Using and combining
In Proceedings of the 29th Annual ACM symposium on Theory of

predictors that specialize.
Computing  pages 334–343. ACM  1997.

Adam Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems. Journal of

Computer and System Sciences  71(3):291–307  2005.

Adam Tauman Kalai  Varun Kanade  and Yishay Mansour. Reliable agnostic learning. Journal of

Computer and System Sciences  78(5):1481–1495  2012.

Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. ACM Transactions on

Computation Theory (TOCT)  6(3):11  2014.

Varun Kanade  H. Brendan McMahan  and Brent Bryan. Sleeping experts and bandits with stochastic
action availability and adversarial rewards. In Proceedings of the 12th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 272–279  2009.

Michael J. Kearns  Robert E. Schapire  and Linda M. Sellie. Toward efﬁcient agnostic learning.

Machine Learning  17(2–3):115–141  1994.

Robert Kleinberg  Alexandru Niculescu-Mizil  and Yogeshwer Sharma. Regret bounds for sleeping

experts and bandits. Machine learning  80(2-3):245–272  2010.

Adam R. Klivans and Rocco Servedio. Learning DNF in Time 2 ˜O(n1/3). In Proceedings of the 33rd

Annual ACM Symposium on Theory of Computing (STOC)  pages 258–265. ACM  2001.

Wouter M. Koolen  Manfred K. Warmuth  and Jyrki Kivinen. Hedging structured concepts.

In
Adam Tauman Kalai and Mehryar Mohri  editors  Proceedings of the 23th Conference on Learn-
ing Theory (COLT)  pages 93–105  2010.

Wouter M. Koolen  Manfred K. Warmuth  and Dmitry Adamskiy. Open problem: Online sabotaged

shortest path. In Proceedings of the 28th Conference on Learning Theory (COLT)  2015.

Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.

computation  108(2):212–261  1994.

Information and

Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets
and adversarial losses. In Advances in Neural Information Processing Systems  pages 2780–2788 
2014.

Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. The Journal of

Machine Learning Research  4:773–818  2003.

9

,Nataliya Shapovalova
Michalis Raptis
Leonid Sigal
Greg Mori
Charles Kervrann
Satyen Kale
Chansoo Lee
David Pal