2013,Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards,We consider an infinite-armed bandit problem with Bernoulli rewards. The mean rewards are independent    uniformly distributed over $[0 1]$. Rewards 0 and  1 are referred to as a success and a failure  respectively. We propose a novel algorithm where the decision to exploit  any arm is based on two successive  targets  namely  the total number of successes until the first failure and the first $m$ failures  respectively  where $m$ is a fixed parameter. This two-target algorithm  achieves a long-term average regret in $\sqrt{2n}$ for  a large parameter $m$ and a known time horizon $n$. This regret is optimal and  strictly less than the regret  achieved by the best known algorithms  which is in $2\sqrt{n}$. The results are extended to any mean-reward distribution whose support contains 1 and to unknown  time horizons. Numerical experiments show the performance of the algorithm for finite time horizons.,Two-Target Algorithms for Inﬁnite-Armed Bandits

with Bernoulli Rewards

Thomas Bonald∗

Telecom ParisTech

Paris  France

thomas.bonald@telecom-paristech.fr

Department of Networking and Computer Science

Automatic Control Department

Alexandre Prouti`ere∗†

KTH

Stockholm  Sweden
alepro@kth.se

Abstract

We consider an inﬁnite-armed bandit problem with Bernoulli rewards. The mean
rewards are independent  uniformly distributed over [0  1]. Rewards 0 and 1 are
referred to as a success and a failure  respectively. We propose a novel algorithm
where the decision to exploit any arm is based on two successive targets  namely 
the total number of successes until the ﬁrst failure and until the ﬁrst m failures 
respectively  where m is a ﬁxed parameter. This two-target algorithm achieves a
long-term average regret in
2n for a large parameter m and a known time hori-
√
zon n. This regret is optimal and strictly less than the regret achieved by the best
known algorithms  which is in 2
n. The results are extended to any mean-reward
distribution whose support contains 1 and to unknown time horizons. Numerical
experiments show the performance of the algorithm for ﬁnite time horizons.

√

1

Introduction

Motivation. While classical multi-armed bandit problems assume a ﬁnite number of arms [9] 
many practical situations involve a large  possibly inﬁnite set of options for the player. This is the
case for instance of on-line advertisement and content recommandation  where the objective is to
propose the most appropriate categories of items to each user in a very large catalogue. In such
situations  it is usually not possible to explore all options  a constraint that is best represented by a
bandit problem with an inﬁnite number of arms. Moreover  even when the set of options is limited 
the time horizon may be too short in practice to enable the full exploration of these options. Unlike
classical algorithms like UCB [10  1]  which rely on a initial phase where all arms are sampled
once  algorithms for inﬁnite-armed bandits have an intrinsic stopping rule in the number of arms to
explore. We believe that this provides useful insights into the design of efﬁcient algorithms for usual
ﬁnite-armed bandits when the time horizon is relatively short.

Overview of the results. We consider a stochastic inﬁnite-armed bandit with Bernoulli rewards 
the mean reward of each arm having a uniform distribution over [0  1]. This model is representative
of a number of practical situations  such as content recommandation systems with like/dislike feed-
back and without any prior information on the user preferences. We propose a two-target algorithm
2n for large m and a
based on some ﬁxed parameter m that achieves a long-term average regret in
√
large known time horizon n. We prove that this regret is optimal. The anytime version of this algo-
rithm achieves a long-term average regret in 2
n for unknown time horizon n  which we conjecture
to be also optimal. The results are extended to any mean-reward distribution whose support contains
1. Speciﬁcally  if the probability that the mean reward exceeds u is equivalent to α(1 − u)β when

√

∗The authors are members of the LINCS  Paris  France. See www.lincs.fr.
†Alexandre Prouti`ere is also afﬁliated to INRIA  Paris-Rocquencourt  France.

1

u → 1−  the two-target algorithm achieves a long-term average regret in C(α  β)n
β+1   with some
explicit constant C(α  β) that depends on whether the time horizon is known or not. This regret is
provably optimal when the time horizon is known. The precise statements and proofs of these more
general results are given in the appendix.

β

√

√

√

2n  which invalidates the conjecture. We also provide a proof of the lower bound in

Related work. The stochastic inﬁnite-armed bandit problem has ﬁrst been considered in a general
setting by Mallows and Robbins [12] and then in the particular case of Bernoulli rewards by Her-
schkorn  Pek¨oz and Ross [6]. The proposed algorithms are ﬁrst-order optimal in the sense that they
minimize the ratio Rn/n for large n  where Rn is the regret after n time steps. In the considered
setting of Bernoulli rewards with mean rewards uniformly distributed over [0  1]  this means that the
ratio Rn/n tends to 0 almost surely. We are interested in second-order optimality  namely  in min-
√
imizing the equivalent of Rn for large n. This issue is addressed by Berry et. al. [2]  who propose
various algorithms achieving a long-term average regret in 2
n  conjecture that this regret is opti-
√
2n. Our algorithm achieves a regret that is arbitrarily close to
mal and provide a lower bound in
2n since that
given in [2  Theorem 3] relies on the incorrect argument that the number of explored arms and the
mean rewards of these arms are independent random variables1; the extension to any mean-reward
distribution [2  Theorem 11] is based on the same erroneous argument2.
The algorithms proposed by Berry et. al. [2] and applied in [11  4  5  7] to various mean-reward
distributions are variants of the 1-failure strategy where each arm is played until the ﬁrst failure 
√
called a run. For instance  the non-recalling
n-run policy consists in exploiting the ﬁrst arm giving
√
n. For a uniform mean-reward distribution over [0  1]  the average number of
a run larger than
√
n and the selected arm is exploited for the equivalent of n time steps with an
explored arms is
expected failure rate of 1/
n. We introduce a second target to improve
the expected failure rate of the selected arm  at the expense of a slightly more expensive exploration

phase. Speciﬁcally  we show that it is optimal to explore(cid:112)n/2 arms on average  resulting in the

2n of the exploited arm  for the equivalent of n time steps  hence the
2n. For unknown horizon times  anytime versions of the algorithms of Berry et. al. [2]
n). We
n  which we

√
expected failure rate 1/
√
regret of
√
are proposed by Teytaud  Gelly and Sebag in [13] and proved to achieve a regret in O(
show that the anytime version of our algorithm achieves a regret arbitrarily close to 2
conjecture to be optimal.
Our results extend to any mean-reward distribution whose support contains 1  the regret depending
on the characteristics of this distribution around 1. This problem has been considered in the more
general setting of bounded rewards by Wang  Audibert and Munos [15]. When the time horizon
is known  their algorithms consist in exploring a pre-deﬁned set of K arms  which depends on
the parameter β mentioned above  using variants of the UCB policy [10  1]. In the present case
of Bernoulli rewards and mean-reward distributions whose support contains 1  the corresponding
β+1   up to logarithmic terms coming from the exploration of the K arms  as in usual
regret is in n
ﬁnite-armed bandits algorithms [9]. The nature of our algorithm is very different in that it is based
on a stopping rule in the exploration phase that depends on the observed rewards. This does not only
remove the logarithmic terms in the regret but also achieves the optimal constant.

n  yielding the regret of 2

√

√

β

2 Model

We consider a stochastic multi-armed bandit with an inﬁnite number of arms. For any k = 1  2  . . . 
the rewards of arm k are Bernoulli with unknown parameter θk. We refer to rewards 0 and 1 as a
failure and a success  respectively  and to a run as a consecutive sequence of successes followed by
a failure. The mean rewards θ1  θ2  . . . are themselves random  uniformly distributed over [0  1].

1Speciﬁcally  it is assumed that for any policy  the mean rewards of the explored arms have a uniform
distribution over [0  1]  independently of the number of explored arms. This is incorrect. For the 1-failure
policy for instance  given that only one arm has been explored until time n  the mean reward of this arm has a
beta distribution with parameters 1  n.

2This lower bound is 4(cid:112)n/3 for a beta distribution with parameters 1/2  1  see [11]  while our algorithm

n in this case  since C(α  β) = 2 for α = 1/2 and β = 1  see the

√
achieves a regret arbitrarily close to 2
appendix. Thus the statement of [2  Theorem 11] is false.

2

At any time t = 1  2  . . .  we select some arm It and receive the corresponding reward Xt  which
is a Bernoulli random variable with parameter θIt. We take I1 = 1 by convention. At any
time t = 2  3  . . .  the arm selection only depends on previous arm selections and rewards; for-
mally  the random variable It is Ft−1-mesurable  where Ft denotes the σ-ﬁeld generated by the
set {I1  X1  . . .   It  Xt}. Let Kt be the number of arms selected until time t. Without any loss of
generality  we assume that {I1  . . .   It} = {1  2  . . .   Kt} for all t = 1  2  . . .  i.e.  new arms are
selected sequentially. We also assume that It+1 = It whenever Xt = 1: if the selection of arm It
gives a success at time t  the same arm is selected at time t + 1.
The objective is to maximize the cumulative reward or  equivalently  to minimize the regret deﬁned
t=1 Xt. Speciﬁcally  we focus on the average regret E(Rn)  where expectation
is taken over all random variables  including the sequence of mean rewards θ1  θ2  . . .. The time
horizon n may be known or unknown.

by Rn = n −(cid:80)n

3 Known time horizon

3.1 Two-target algorithm

The two-target algorithm consists in exploring new arms until two successive targets (cid:96)1 and (cid:96)2 are
reached  in which case the current arm is exploited until the time horizon n. The ﬁrst target aims
at discarding “bad” arms while the second aims at selecting a “good” arm. Speciﬁcally  using the
names of the variables indicated in the pseudo-code below  if the length L of the ﬁrst run of the
current arm I is less than (cid:96)1  this arm is discarded and a new arm is selected; otherwise  arm I is
pulled for m − 1 additional runs and exploited until time n if the total length L of the m runs is at
least (cid:96)2  where m ≥ 2 is a ﬁxed parameter of the algorithm. We prove in Proposition 1 below that 

for large m  the target values3 (cid:96)1 = (cid:98) 3(cid:112) n

2(cid:99) and (cid:96)2 = (cid:98)m(cid:112) n

2(cid:99) provide a regret in

2n.

√

(cid:96)1 = (cid:98) 3(cid:112) n

2(cid:99)  (cid:96)2 = (cid:98)m(cid:112) n

Algorithm 1: Two-target algorithm with known time horizon n.
Parameters: m  n
Function:
Explore
I ← I + 1  L ← 0  M ← 0
Algorithm:
2(cid:99)
I ← 0
Explore
Exploit ← false
forall the t = 1  2  . . .   n do
Get reward X from arm I
if not Exploit then
if X = 1 then
L ← L + 1
M ← M + 1
if M = 1 then

else

if L < (cid:96)1 then

Explore

else if M = m then
if L < (cid:96)2 then

else

Explore
Exploit ← true

3The ﬁrst target could actually be any function (cid:96)1 of the time horizon n such that (cid:96)1 → +∞ and (cid:96)1/

when n → +∞. Only the second target is critical.

√
n → 0

3

3.2 Regret analysis

Proposition 1 The two-target algorithm with targets (cid:96)1 = (cid:98) 3(cid:112) n
(cid:18) (cid:96)2 − m + 2

(cid:96)2 + 1

2(cid:99) and (cid:96)2 = (cid:98)m(cid:112) n
(cid:19)m(cid:18)

∀n ≥ m2
2

  E(Rn) ≤ m +

m

(cid:96)2 − (cid:96)1 − m + 2

(cid:19)
2(cid:99) satisﬁes:

m + 1
(cid:96)1 + 1

.

2 +

+ 2

1
m

In particular 

lim sup
n→+∞

E(Rn)√
n

≤

√

2 +

√
1
m

2

.

Proof. Note that Let U1 = 1 if arm 1 is used until time n and U1 = 0 otherwise. Denote by M1 the
total number of 0’s received from arm 1. We have:

E(Rn) ≤ P (U1 = 0)(E(M1|U1 = 0) + E(Rn)) + P (U1 = 1)(m + nE(1 − θ1|U1 = 1)) 

so that:

E(Rn) ≤ E(M1|U1 = 0)

P (U1 = 1)

+ m + nE(1 − θ1|U1 = 1).

(1)

Let Nt be the number of 0’s received from arm 1 until time t when this arm is played until time t.
Note that n ≥ m2
2 implies n ≥ (cid:96)2. Since P (N(cid:96)1 = 0|θ1 = u) = u(cid:96)1  the probability that the ﬁrst
target is achieved by arm 1 is given by:

(cid:90) 1
m−1(cid:88)

0

1

.

(cid:96)1 + 1

u(cid:96)1du =

(cid:18)(cid:96)2 − (cid:96)1

(cid:19)

P (N(cid:96)1 = 0) =

Similarly 

so that the probability that arm 1 is used until time n is given by:

P (U1 = 1) =

P (N(cid:96)2−(cid:96)1 < m|θ1 = u) =

u(cid:96)2−(cid:96)1−j(1 − u)j 

j

0

j=0

(cid:90) 1
P (N(cid:96)1 = 0|θ1 = u)P (N(cid:96)2−(cid:96)1 < m|θ1 = u)du 
m−1(cid:88)
(cid:18) (cid:96)2 − (cid:96)1 − m + 2

(cid:19)m ≤ P (U1 = 1) ≤ m

((cid:96)2 − (cid:96)1)!
((cid:96)2 − (cid:96)1 − j)!

((cid:96)2 − j)!
((cid:96)2 + 1)!

j=0

.

.

=

(cid:96)2 − m + 2

(cid:96)2 + 1

We deduce:

m

(cid:96)2 + 1

Moreover 
E(M1|U1 = 0) = 1 + (m − 1)P (N(cid:96)1 = 0|U1 = 0) ≤ 1 + (m − 1)
where the last inequality follows from (2) and the fact that (cid:96)2 ≥ m2
2 .
It remains to calculate E(1 − θ1|U1 = 1). Since:

(2)

P (N(cid:96)1 = 0)
P (U1 = 0)

≤ 1 + 2

m + 1
(cid:96)1 + 1

 

j

j=0

(cid:18)(cid:96)2 − (cid:96)1
(cid:19)
m−1(cid:88)
(cid:18)(cid:96)2 − (cid:96)1
(cid:90) 1
m−1(cid:88)
m−1(cid:88)

j=0

j

0

(cid:19)

u(cid:96)2−j(1 − u)j 

u(cid:96)2−j(1 − u)j+1du 

((cid:96)2 − (cid:96)1)!
((cid:96)2 − j)!
((cid:96)2 − (cid:96)1 − j)!
((cid:96)2 + 2)!
≤
m(m + 1)

1

j=0

2((cid:96)2 + 1)((cid:96)2 + 2)

P (U1 = 1)

(j + 1) 

(cid:18)

1 +

1
m

(cid:19)

.

(cid:3)

P (U1 = 1|θ1 = u) =

we deduce:

E(1 − θ1|U1 = 1) =

=

≤

1

P (U1 = 1)

1

P (U1 = 1)

1

P (U1 = 1)

The proof then follows from (1) and (2).

4

3.3 Lower bound

The following result shows that the two-target algorithm is asymptotically optimal (for large m).

Theorem 1 For any algorithm with known time horizon n 
√

lim inf
n→+∞

E(Rn)√
n

≥

2.

1

(cid:113) 2

Proof. We present the main ideas of the proof. The details are given in the appendix. Assume an
oracle reveals the parameter of each arm after the ﬁrst failure of this arm. With this information 
the optimal policy explores a random number of arms  each until the ﬁrst failure  then plays only
one of these arms until time n. Let µ be the parameter of the best known arm at time t. Since the
probability that any new arm is better than this arm is 1 − µ  the mean cost of exploration to ﬁnd
a better arm is
1−µ. The corresponding mean reward has a uniform distribution over [µ  1] so that
the mean gain of exploitation is less than (n − t) 1−µ
(it is not equal to this quantity due to the time
spent in exploration). Thus if 1 − µ <
n−t  it is preferable not to explore new arms and to play
the best known arm  with mean reward µ  until time n. A fortiori  the best known arm is played
n. We denote by An the ﬁrst arm whose
n. We have Kn ≤ An (the optimal policy cannot explore more than

until time n whenever its parameter is larger than 1 −(cid:113) 2
parameter is larger than 1 −(cid:113) 2
(cid:114) n
The parameter θAn of arm An is uniformly distributed over [1 −(cid:113) 2
(cid:114) 1

n   1]  so that

An arms) and

E(An) =

2

2

.

E(θAn ) = 1 −

.

2n

For all k = 1  2  . . .  let L1(k) be the length of the ﬁrst run of arm k. We have:

(cid:114) 2

n

(cid:113) 2
1 −(cid:113) 2

− ln(

n

n )

−1)

(cid:114) n

2

.

(3)

  (4)

(5)

E(L1(1)+. . .+L1(An−1)) = (E(An)−1)E(L1(1)|θ1 ≤ 1−

) = (

using the fact that:

E(L1(1)|θ1 ≤ 1 −

(cid:114) 2

n

) =

(cid:90) 1−

√

2
n

0

1

1 − u

1 −(cid:113) 2

du

n

In particular 

and

lim

n→+∞

1
n

E(L1(1) + . . . + L1(An − 1)) → 0

lim

n→+∞

1
n

P (L1(1) + . . . + L1(An − 1) ≤ n

4

5 ) → 1.

To conclude  we write:

E(Rn) ≥ E(Kn) + E((n − L1(1) − . . . − L1(An − 1)))(1 − θAn )).

n denotes the ﬁrst arm whose parameter is larger than 1 −(cid:113) 2

5}  the number of explored arms satisﬁes
. Since

Observe that  on the event {L1(1) + . . . + L1(An− 1) ≤ n 4
Kn ≥ A(cid:48)
P (L1(1) + . . . + L1(An − 1) ≤ n 4

n where A(cid:48)

4
5

n−n
2

  we deduce that:

(cid:113)

n−n

4
5

5 ) → 1 and E(A(cid:48)
E(Kn)√
n

lim inf
n→+∞

n) =
≥ 1√
2

.

5

By the independence of θAn and L1(1)  . . .   L1(An − 1) 

1√
n

E((n − L1(1) − . . . − L1(An − 1)))(1 − θAn ))

=

1√
n

(n − E(L1(1) + . . . + L1(An − 1)))(1 − E(θAn )) 

which tends to 1√

2

in view of (3) and (5). The announced bound follows.

(cid:3)

4 Unknown time horizon

4.1 Anytime version of the algorithm

When the time horizon is unknown  the targets depend on the current time t  say (cid:96)1(t) and (cid:96)2(t).
Now any arm that is exploited may be eventually discarded  in the sense that a new arm is explored.
This happens whenever either L1 < (cid:96)1(t) or L2 < (cid:96)2(t)  where L1 and L2 are the respective lengths
of the ﬁrst run and the ﬁrst m runs of this arm. Thus  unlike the previous version of the algorithm
which consists in an exploration phase followed by an exploitation phase  the anytime version of the
√
algorithm continuously switches between exploration and exploitation. We prove in Proposition 2
below that  for large m  the target values (cid:96)1(t) = (cid:98) 3
t(cid:99) given in the pseudo-code
√
achieve an asymptotic regret in 2

√
t(cid:99) and (cid:96)2(t) = (cid:98)m

n.

Algorithm 2: Two-target algorithm with unknown time horizon.
Parameter: m
Function:
Explore
I ← I + 1  L ← 0  M ← 0
Algorithm:
I ← 0
Explore
Exploit ← false
forall the t = 1  2  . . . do
√
t(cid:99)  (cid:96)2 = (cid:98)m

√
Get reward X from arm I
(cid:96)1 = (cid:98) 3
t(cid:99)
if Exploit then

if L1 < (cid:96)1 or L2 < (cid:96)2 then

Explore
Exploit ← false

else

if X = 1 then
L ← L + 1
M ← M + 1
if M = 1 then

else

if L < (cid:96)1 then

else

Explore
L1 ← L

else if M = m then
if L < (cid:96)2 then

else

Explore
L2 ← L
Exploit← true

6

4.2 Regret analysis
√
Proposition 2 The two-target algorithm with time-dependent targets (cid:96)1(t) = (cid:98) 3
√
(cid:98)m

t(cid:99) satisﬁes:

t(cid:99) and (cid:96)2(t) =

lim sup
n→+∞

E(Rn)√
n

≤ 2 +

1
m

.

Proof. For all k = 1  2  . . .  denote by L1(k) and L2(k) the respective lengths of the ﬁrst run and
of the ﬁrst m runs of arm k when this arm is played continuously. Since arm k cannot be selected
before time k  the regret at time n satisﬁes:

Rn ≤ Kn + m

1{L1(k)>(cid:96)1(k)} +

k=1

t=1

(1 − Xt)1{L2(It)>(cid:96)2(t)}.

First observe that  since the target functions (cid:96)1(t) and (cid:96)2(t) are non-decreasing  Kn is less than or
equal to K(cid:48)
n  the number of arms selected by a two-target policy with known time horizon n and
ﬁxed targets (cid:96)1(n) and (cid:96)2(n). In this scheme  let U(cid:48)
1 = 0
otherwise. It then follows from (2) that P (U(cid:48)
n when
n → +∞.
Now 

1 = 1 if arm 1 is used until time n and U(cid:48)
n) ∼ √

n and E(Kn) ≤ E(K(cid:48)

1 = 1) ∼ 1√

n(cid:88)

Kn(cid:88)

k=1

∞(cid:88)
∞(cid:88)
∞(cid:88)

k=1

k=1

(cid:32) Kn(cid:88)

k=1

E

(cid:33)

1{L1(k)>(cid:96)1(k)}

=

=

≤

P (L1(k) > (cid:96)1(k)  Kn ≥ k) 

P (L1(k) > (cid:96)1(k))P (Kn ≥ k|L1(k) > (cid:96)1(k)) 

E(Kn)(cid:88)

k=1

P (L1(k) > (cid:96)1(k))P (Kn ≥ k) ≤

P (L1(k) > (cid:96)1(k)) 

where the ﬁrst inequality follows from the fact that for any arm k and all u ∈ [0  1] 

P (θk ≥ u|L1(k) > (cid:96)1(k)) ≥ P (θk ≥ u)

and the second inequality follows from the fact that the random variables L1(1)  L1(2)  . . . are
i.i.d. and the sequence (cid:96)1(1)  (cid:96)1(2)  . . . is non-decreasing. Since E(Kn) ≤ E(K(cid:48)
n when
n → +∞ and P (L1(k) > (cid:96)1(k)) ∼ 1
3√

when k → +∞  we deduce:

and P (Kn ≥ k|θk ≥ u) ≤ P (Kn ≥ k) 
n) ∼ √

k

(cid:32) Kn(cid:88)

k=1

(cid:33)

1{L1(k)>(cid:96)1(k)}

= 0.

lim

n→+∞

1√
n

E

Finally 

E((1 − Xt)1{L2(It)>(cid:96)2(t)}) ≤ E(1 − Xt|L2(It) > (cid:96)2(t)) ∼ m + 1
m

√
1
2

so that

lim sup
n→+∞

1√
n

n(cid:88)

t=1

E((1 − Xt)1{L2(It)>(cid:96)2(t)}) ≤ m + 1
m

lim

n→+∞

(cid:90) 1

1
n
√
1
2

u

m + 1

m

0

Combining the previous results yields:

=

when t → +∞ 

(cid:114) n

 

t

1
2

t

n(cid:88)

t=1

du =

m + 1

m

.

(cid:3)

lim sup
n→+∞

E(Rn)√
n

≤ 2 +

1
m

.

7

4.3 Lower bound

√
We believe that if E(Rn)/
n tends to some limit  then this limit is at least 2. To support this
conjecture  consider an oracle that reveals the parameter of each arm after the ﬁrst failure of this arm 
as in the proof of Theorem 1. With this information  an optimal policy exploits an arm whenever its
parameter is larger than some increasing function ¯θt of time t. Assume that 1 − ¯θt ∼ 1
√
for some
c > 0 when t → +∞. Then proceeding as in the proof of Theorem 1  we get:

c

t

lim inf
n→+∞

E(Rn)√
n

≥ c + lim
n→+∞

1
n

5 Numerical results

(cid:114) n

n(cid:88)

t=1

(cid:90) 1

0

1
2c

= c +

1
c

t

√
du
2
u

= c +

≥ 2.

1
c

Figure 1 gives the expected failure rate E(Rn)/n with respect to the time horizon n  that is supposed
to be known. The results are derived from the simulation of 105 independent samples and shown
with 95% conﬁdence intervals. The mean rewards have (a) a uniform distribution or (b) a Beta(1 2)
distribution  corresponding to the probability density function u (cid:55)→ 2(1 − u). The single-target
algorithm corresponds to the run policy of Berry et. al. [2] with the asymptotically optimal target
values
2n  respectively. For the two-target algorithm  we take m = 3 and the target
values given in Proposition 1 and Proposition 3 (in the appendix). The results are compared with

the respective asymptotic lower bounds(cid:112)2/n and 3(cid:112)3/n. The performance gains of the two-target

√
n and 3

algorithm turn out to be negligible for the uniform distribution but substantial for the Beta(1 2)
distribution  where “good” arms are less frequent.

√

(a) Uniform mean-reward distribution

(b) Beta(1 2) mean-reward distribution

Figure 1: Expected failure rate E(Rn)/n with respect to the time horizon n.

6 Conclusion

√

√
2n and 2

The proposed algorithm uses two levels of sampling in the exploration phase: the ﬁrst eliminates
“bad” arms while the second selects “good” arms. To our knowledge  this is the ﬁrst algorithm that
achieves the optimal regrets in
n for known and unknown horizon times  respectively.
Future work will be devoted to the proof of the lower bound in the case of unknown horizon time. We
also plan to study various extensions of the present work  including mean-reward distributions whose
support does not contain 1 and distribution-free algorithms. Finally  we would like to compare the
performance of our algorithm for ﬁnite-armed bandits with those of the best known algorithms like
KL-UCB [10  3] and Thompson sampling [14  8] over short time horizons where the full exploration
of the arms is generally not optimal.

Acknowledgments

The authors acknowledge the support of the European Research Council  of the French ANR (GAP
project)  of the Swedish Research Council and of the Swedish SSF.

8

 0 0.1 0.2 0.3 0.4 0.5 10 100 1000 10000Expected failure rateTime horizonAsymptotic lower boundSingle-target algorithmTwo-target algorithm 0 0.1 0.2 0.3 0.4 0.5 0.6 10 100 1000 10000Expected failure rateTime horizonAsymptotic lower boundSingle-target algorithmTwo-target algorithmReferences
[1] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Mach. Learn.  47(2-3):235–256  May 2002.

[2] Donald A. Berry  Robert W. Chen  Alan Zame  David C. Heath  and Larry A. Shepp. Bandit

problems with inﬁnitely many arms. Annals of Statistics  25(5):2103–2116  1997.

[3] Olivier Capp´e  Aur´elien Garivier  Odalric-Ambrym Maillard  R´emi Munos  and Gilles Stoltz.
Kullback-leibler upper conﬁdence bounds for optimal sequential allocation. To appear in An-
nals of Statistics  2013.

[4] Kung-Yu Chen and Chien-Tai Lin. A note on strategies for bandit problems with inﬁnitely

many arms. Metrika  59(2):193–203  2004.

[5] Kung-Yu Chen and Chien-Tai Lin. A note on inﬁnite-armed bernoulli bandit problems with

generalized beta prior distributions. Statistical Papers  46(1):129–140  2005.

[6] Stephen J Herschkorn  Erol Pekoez  and Sheldon M Ross. Policies without memory for the
inﬁnite-armed bernoulli bandit under the average-reward criterion. Probability in the Engi-
neering and Informational Sciences  10:21–28  1996.

[7] Ying-Chao Hung. Optimal bayesian strategies for the inﬁnite-armed bernoulli bandit. Journal

of Statistical Planning and Inference  142(1):86–94  2012.

[8] Emilie Kaufmann  Nathaniel Korda  and R´emi Munos. Thompson sampling: An asymptoti-
cally optimal ﬁnite-time analysis. In Algorithmic Learning Theory  pages 199–213. Springer 
2012.

[9] Tze L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances

in Applied Mathematics  6(1):4–22  1985.

[10] Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals

of Statistics  pages 1091–1114  1987.

[11] Chien-Tai Lin and CJ Shiau. Some optimal strategies for bandit problems with beta prior

distributions. Annals of the Institute of Statistical Mathematics  52(2):397–405  2000.

[12] C.L Mallows and Herbert Robbins. Some problems of optimal sampling strategy. Journal of

Mathematical Analysis and Applications  8(1):90 – 103  1964.

[13] Olivier Teytaud  Sylvain Gelly  and Mich`ele Sebag. Anytime many-armed bandits. In CAP07 

2007.

[14] W. R. Thompson. On the Likelihood that one Unknown Probability Exceeds Another in View

of the Evidence of Two Samples. Biometrika  25:285–294  1933.

[15] Yizao Wang  Jean-Yves Audibert  and R´emi Munos. Algorithms for inﬁnitely many-armed

bandits. In NIPS  2008.

9

,Thomas Bonald
Alexandre Proutiere
Johannes Friedrich
Liam Paninski
Filip Hanzely
Konstantin Mishchenko
Peter Richtarik
Alex Lu
Amy Lu
Wiebke Schormann
Marzyeh Ghassemi
David Andrews
Alan Moses