2013,A memory frontier for complex synapses,An incredible gulf separates theoretical models of synapses  often described solely by a single scalar value denoting the size of a postsynaptic potential  from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory  it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover  theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question  how does synaptic complexity give rise to memory? To address this  we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover  in proving such theorems  we uncover a framework  based on first passage time theory  to impose an order on the internal states of complex synaptic models  thereby simplifying the relationship between synaptic structure and function.,A memory frontier for complex synapses

Subhaneil Lahiri and Surya Ganguli

Department of Applied Physics  Stanford University  Stanford CA
sulahiri@stanford.edu  sganguli@stanford.edu

Abstract

An incredible gulf separates theoretical models of synapses  often described solely
by a single scalar value denoting the size of a postsynaptic potential  from the
immense complexity of molecular signaling pathways underlying real synapses.
To understand the functional contribution of such molecular complexity to learn-
ing and memory  it is essential to expand our theoretical conception of a synapse
from a single scalar to an entire dynamical system with many internal molecular
functional states. Moreover  theoretical considerations alone demand such an ex-
pansion; network models with scalar synapses assuming ﬁnite numbers of distin-
guishable synaptic strengths have strikingly limited memory capacity. This raises
the fundamental question  how does synaptic complexity give rise to memory? To
address this  we develop new mathematical theorems elucidating the relationship
between the structural organization and memory properties of complex synapses
that are themselves molecular networks. Moreover  in proving such theorems  we
uncover a framework  based on ﬁrst passage time theory  to impose an order on
the internal states of complex synaptic models  thereby simplifying the relation-
ship between synaptic structure and function.

1

Introduction

It is widely thought that our very ability to remember the past over long time scales depends crucially
on our ability to modify synapses in our brain in an experience dependent manner. Classical models
of synaptic plasticity model synaptic efﬁcacy as an analog scalar value  denoting the size of a post-
synaptic potential injected into one neuron from another. Theoretical work has shown that such
models have a reasonable  extensive memory capacity  in which the number of long term associations
that can be stored by a neuron is proportional its number of afferent synapses [1–3]. However 
recent experimental work has shown that many synapses are more digital than analog; they cannot
robustly assume an inﬁnite continuum of analog values  but rather can only take on a ﬁnite number
of distinguishable strengths  a number than can be as small as two [4–6] (though see [7]). This
one simple modiﬁcation leads to a catastrophe in memory capacity: classical models with digital
synapses  when operating in a palimpset mode in which the ongoing storage of new memories can
overwrite previous memories  have a memory capacity proportional to the logarithm of the number
of synapses [8  9]. Intuitively  when synapses are digital  the storage of a new memory can ﬂip
a population of synaptic switches  thereby rapidly erasing previous memories stored in the same
synaptic population. This result indicates that the dominant theoretical basis for the storage of long
term memories in modiﬁable synaptic switches is ﬂawed.
Recent work [10–12] has suggested that a way out of this logarithmic catastrophe is to expand our
theoretical conception of a synapse from a single scalar value to an entire stochastic dynamical sys-
tem in its own right. This conceptual expansion is further necessitated by the experimental reality
that synapses contain within them immensely complex molecular signaling pathways  with many in-
ternal molecular functional states (e.g. see [4  13  14]). While externally  synaptic efﬁcacy could be
digital  candidate patterns of electrical activity leading to potentiation or depression could yield tran-
sitions between these internal molecular states without necessarily inducing an associated change in

1

synaptic efﬁcacy. This form of synaptic change  known as metaplasticity [15  16]  can allow the
probability of synaptic potentiation or depression to acquire a rich dependence on the history of
prior changes in efﬁcacy  thereby potentially improving memory capacity.
Theoretical studies of complex  metaplastic synapses have focused on analyzing the memory per-
formance of a limited number of very speciﬁc molecular dynamical systems  characterized by a
number of internal states in which potentiation and depression each induce a speciﬁc set of allow-
able transitions between states (e.g. see Figure 1 below). While these models can vastly outperform
simple binary synaptic switches  these analyses leave open several deep and important questions.
For example  how does the structure of a synaptic dynamical system determine its memory perfor-
mance? What are the fundamental limits of memory performance over the space of all possible
synaptic dynamical systems? What is the structural organization of synaptic dynamical systems that
achieve these limits? Moreover  from an experimental perspective  it is unlikely that all synapses
can be described by a single canonical synaptic model; just like the case of neurons  there is an
incredible diversity of molecular networks underlying synapses both across species and across brain
regions within a single organism [17]. In order to elucidate the functional contribution of this di-
verse molecular complexity to learning and memory  it is essential to move beyond the analysis of
speciﬁc models and instead develop a general theory of learning and memory for complex synapses.
Moreover  such a general theory of complex synapses could aid in development of novel artiﬁcial
memory storage devices.
Here we initiate such a general theory by proving upper bounds on the memory curve associated with
any synaptic dynamical system  within the well established ideal observer framework of [10  11  18].
Along the way we develop principles based on ﬁrst passage time theory to order the structure of
synaptic dynamical systems and relate this structure to memory performance. We summarize our
main results in the discussion section.

2 Overall framework: synaptic models and their memory curves

In this section  we describe the class of models of synaptic plasticity that we are studying and how
we quantify their memory performance. In the subsequent sections  we will ﬁnd upper bounds on
this performance.
We use a well established formalism for the study of learning and memory with complex synapses
(see [10  11  18]). In this approach  electrical patterns of activity corresponding to candidate po-
tentiating and depressing plasticity events occur randomly and independently at all synapses at a
Poisson rate r. These events reﬂect possible synaptic changes due to either spontaneous network
activity  or the storage of new memories. We let f pot and f dep denote the fraction of these events that
are candidate potentiating or depressing events respectively. Furthermore  we assume our synaptic
model has M internal molecular functional states  and that a candidate potentiating (depotentiat-
ing) event induces a stochastic transition in the internal state described by an M × M discrete time
Markov transition matrix Mpot (Mdep). In this framework  the states of different synapses will be
independent  and the entire synaptic population can be fully described by the probability distribution
across these states  which we will indicate with the row-vector p(t). Thus the i’th component of
p(t) denotes the fraction of the synaptic population in state i. Furthermore  each state i has its own
synaptic weight  wi  which we take  in the worst case scenario  to be restricted to two values. After
shifting and scaling these two values  we can assume they are ±1  without loss of generality.
We also employ an “ideal observer” approach to the memory readout  where the synaptic weights
are read directly. This provides an upper bound on the quality of any readout using neural activity.
For any single memory  stored at time t = 0  we assume there will be an ideal pattern of synaptic
weights across a population of N synapses  the N-element vector (cid:126)wideal  that is +1 at all synapses
that experience a candidate potentiation event  and −1 at all synapses that experience a candidate
depression event at the time of memory storage. We assume that any pattern of synaptic weights
close to (cid:126)wideal is sufﬁcient to recall the memory. However  the actual pattern of synaptic weights at
some later time  t  will change to (cid:126)w(t) due to further modiﬁcations from the storage of subsequent
memories. We can use the overlap between these  (cid:126)wideal· (cid:126)w(t)  as a measure of the quality of the
memory. As t → ∞  the system will return to its steady state distribution which will be uncorrelated

2

(a)

(b)

(c)

Figure 1: Models of complex synapses. (a) The cascade model of [10]  showing transitions between
states of high/low synaptic weight (red/blue circles) due to potentiation/depression (solid red/dashed
blue arrows). (b) The serial model of [12]. (c) The memory curves of these two models  showing
the decay of the signal-to-noise ratio (to be deﬁned in §2) as subsequent memories are stored.

with the memory stored at t = 0. The probability distribution of the quantity (cid:126)wideal· (cid:126)w(∞) can be
used as a “null model” for comparison.
The extent to which the memory has been stored is described by a signal-to-noise ratio (SNR)
[10  11]:

SNR(t) =

.

(1)

(cid:104) (cid:126)wideal· (cid:126)w(t)(cid:105) − (cid:104) (cid:126)wideal· (cid:126)w(∞)(cid:105)

(cid:112)Var( (cid:126)wideal· (cid:126)w(∞))

√

The noise in the denominator is essentially
N. There is a correction when potentiation and de-
pression are imbalanced  but this will not affect the upper bounds that we will discuss below and
will be ignored in the subsequent formulae.
A simple average memory curve can be derived as follows. All of the preceding plasticity events 
prior to t = 0  will put the population of synapses in its steady-state distribution  p∞. The mem-
ory we are tracking at t = 0 will change the internal state distribution to p∞Mpot (or p∞Mdep)
in those synapses that experience a candidate potentiation (or depression) event. As the potentiat-
ing/depressing nature of the subsequent memories is independent of (cid:126)wideal  we can average over all
sequences  resulting in the evolution of the probability distribution:

dp(t)

dt

= rp(t)WF 

where WF = f potMpot + f depMdep − I.

(2)

w.

SNR(t) =

Here WF is a continuous time transition matrix that models the process of forgetting the memory
stored at time t = 0 due to random candidate potentiation/depression events occurring at each
synapse due to the storage of subsequent memories. Its stationary distribution is p∞.
This results in the following SNR
√

N(cid:0)2f potf dep(cid:1) p∞(cid:0)Mpot − Mdep(cid:1) ertWF

(3)
A detailed derivation of this formula can be found in the supplementary material. We will frequently
refer to this function as the memory curve. It can be thought of as the excess fraction of synapses
(relative to equilibrium) that maintain their ideal synaptic strength at time t  as dictated by the stored
memory at time t = 0.
Much of the previous work on these types of complex synaptic models has focused on understanding
the memory curves of speciﬁc models  or choices of Mpot/dep. Two examples of these models are
shown in Figure 1. We see that they have different memory properties. The serial model performs
relatively well at one particular timescale  but it performs poorly at other times. The cascade model
does not perform quite as well at that time  but it maintains its performance over a wider range of
timescales.
In this work  rather than analyzing speciﬁc models  we take a different approach  in order to obtain
a more general theory. We consider the entire space of these models and ﬁnd upper bounds on the
memory capacity of any of them. The space of models with a ﬁxed number of internal states M is
parameterized by the pair of M × M discrete time stochastic transition matrices Mpot and Mdep  in
addition to f pot/dep. The parameters must satisfy the following constraints:
p∞WF = 0 

f pot/dep ∈ [0  1] 

wi = ±1 

f pot + f dep = 1 

p∞
i = 1.

(4)

(cid:88)

ij

Mpot/dep
Mpot/dep

ij

∈ [0  1] 
= 1 

(cid:88)

j

i

3

Cascade modelSerial model10−110010110210310−310−210−1TimeSNR  CascadeSerialThe upper bounds on Mpot/dep
The critical question is: what do these constraints imply about the space of achievable memory
curves in (3)? To answer this question  especially for limits on achievable memory at ﬁnite times  it
will be useful to employ the eigenmode decomposition:

and f pot/dep follow automatically from the other constraints.

ij

WF =

−qauava  vaub = δab  WFua = −qaua  vaWF = −qava.

(5)

(cid:88)

a

Here qa are the negative of the eigenvalues of the forgetting process WF  ua are the right (column)
eigenvectors and va are the left (row) eigenvectors. This decomposition allows us to write the
memory curve as a sum of exponentials 

SNR(t) =

N

(6)
where Ia = (2f potf dep)p∞(Mpot − Mdep)uavaw and τa = 1/qa. We can then ask the question:
what are the constraints on these quantities  namely eigenmode initial SNR’s  Ia  and time constants 
τa  implied by the constraints in (4)? We will derive some of these constraints in the next section.

a

(cid:88)

√

Iae−rt/τa  

3 Upper bounds on achievable memory capacity

In the previous section  in (3) we have described an analytic expression for a memory curve as a
function of the structure of a synaptic dynamical system  described by the pair of stochastic transition
matrices Mpot/dep. Since the performance measure for memory is an entire memory curve  and not
just a single number  there is no universal scalar notion of optimal memory in the space of synaptic
dynamical systems. Instead there are tradeoffs between storing proximal and distal memories; often
attempts to increase memory at late (early) times by changing Mpot/dep  incurs a performance loss
in memory at early (late) times in speciﬁc models considered so far [10–12]. Thus our end goal 
achieved in §4  is to derive an envelope memory curve in the SNR-time plane  or a curve that forms
an upper-bound on the entire memory curve of any model. In order to achieve this goal  in this
section  we must ﬁrst derive upper bounds  over the space of all possible synaptic models  on two
different scalar functions of the memory curve: its initial SNR  and the area under the memory curve.
In the process of upper-bounding the area  we will develop an essential framework to organize the
structure of synaptic dynamical systems based on ﬁrst passage time theory.

3.1 Bounding initial SNR

We now give an upper bound on the initial SNR 

N(cid:0)2f potf dep(cid:1) p∞(cid:0)Mpot − Mdep(cid:1) w 

√

SNR(0) =

(7)

over all possible models and also ﬁnd the class of models that saturate this bound. A useful quantity
is the equilibrium probability ﬂux between two disjoint sets of states  A and B:

ΦAB =

rp∞

i WF
ij.

(8)

The initial SNR is closely related to the ﬂux from the states with wi = −1 to those with wj = +1
(see supplementary material):

SNR(0) ≤ 4

N Φ−+

r

.

(9)

This inequality becomes an equality if potentiation never decreases the synaptic weight and depres-
sion never increases it  which should be a property of any sensible model.
To maximize this ﬂux  potentiation from a weak state must be guaranteed to end in a strong state 
and depression must do the reverse. An example of such a model is shown in Figure 2(a b). These
models have a property known as “lumpability” (see [19  §6.3] for the discrete time version and
[20  21] for continuous time). They are completely equivalent (i.e. have the same memory curve) as
a two state model with transition probabilities equal to 1  as shown in Figure 2(c).

4

(cid:88)

(cid:88)

i∈A

j∈B

√

(a)

(b)

(c)

Figure 2: Synaptic models that maximize initial SNR. (a) For potentiation  all transitions starting
from a weak state lead to a strong state  and the probabilities for all transitions leaving a given weak
state sum to 1. (a) Depression is similar to potentiation  but with strong and weak interchanged.
(c) The equivalent two state model  with transition probabilities under potentiation and depression
equal to one.

This two state model has the equilibrium distribution p∞ = (f dep  f pot) and its ﬂux is given by
Φ−+ = rf potf dep. This is maximized when f pot = f dep = 1

2  leading to the upper bound:

√

SNR(0) ≤

N .

(10)

We note that while this model has high initial SNR  it also has very fast memory decay – with a
timescale τ ∼ 1
r . As the synapse is very plastic  the initial memory is encoded very easily  but
the subsequent memories also overwrite it rapidly. This is one example of the tradeoff between
optimizing memory at early versus late times.

3.2

Imposing order on internal states through ﬁrst passage times

Our goal of understanding the relationship between structure and function in the space of all possible
synaptic models is complicated by the fact that this space contains many different possible network
topologies  encoded in the nonzero matrix elements of Mpot/dep. To systematically analyze this
entire space  we develop an important organizing principle using the theory of ﬁrst passage times
in the stochastic process of forgetting  described by WF. The mean ﬁrst passage time matrix  Tij 
is deﬁned as the average time it takes to reach state j for the ﬁrst time  starting from state i. The
diagonal elements are deﬁned to be zero.
A remarkable theorem we will exploit is that the quantity
Tijp∞
j  

η ≡(cid:88)

(11)

j

known as Kemeny’s constant (see [19  §4.4])  is independent of the starting state i. Intuitively  (11)
states that the average time it takes to reach any state  weighted by its equilibrium probability  is
independent of the starting state  implying a hidden constancy inherent in any stochastic process.
In the context of complex synapses  we can deﬁne the partial sums

(cid:88)

j∈+

(cid:88)

j∈−

η+
i =

Tijp∞
j  

η−
i =

Tijp∞
j .

(12)

i . Because η+

i or increasing η−

These can be thought of as the average time it takes to reach the strong/weak states respectively.
Using these deﬁnitions  we can then impose an order on the states by arranging them in order of
decreasing η+
i = η is independent of i  the two orderings are
the same. In this order  which depends sensitively on the structure of Mpot/dep  states later (to the
right in ﬁgures below) can be considered to be more potentiated than states earlier (to the left in
ﬁgures below)  despite the fact that they have the same synaptic efﬁcacy. In essence  in this order  a
state is considered to be more potentiated if the average time it takes to reach all the strong efﬁcacy
states is shorter. We will see that synaptic models that optimize various measures of memory have
an exceedingly simple structure when  and only when  their states are arranged in this order.1

i + η−

1Note that we do not need to worry about the order of the η

±
i changing during the optimization: necessary
conditions for a maximum only require that there is no inﬁnitesimal perturbation that increases the area. There-
fore we need only consider an inﬁnitesimal neighborhood of the model  in which the order will not change.

5

11(a)

(b)

(c)

(a) Perturbations that increase elements of Mpot
Figure 3: Perturbations that increase the area.
above the diagonal and decrease the corresponding elements of Mdep. It can no longer be used
when Mdep is lower triangular  i.e. depression must move synapses to “more depressed” states. (b)
Perturbations that decrease elements of Mpot below the diagonal and increase the corresponding
elements of Mdep. It can no longer be used when Mpot is upper triangular  i.e. potentiation must
move synapses to “more potentiated” states. (c) Perturbation that decreases “shortcut” transitions
and increases the bypassed “direct” transitions. It can no longer be used when there are only nearest-
neighbor “direct” transitions.

3.3 Bounding area

Now consider the area under the memory curve:

A =

dt SNR(t).

(cid:90) ∞

0

(cid:88)
(cid:88)

ij

ij

We will ﬁnd an upper bound on this quantity as well as the model that saturates this bound.
First passage time theory introduced in the previous section becomes useful because the area has a
simple expression in terms of quantities introduced in (12) (see supplementary material):

√

A =

N (4f potf dep)

√

=

N (4f potf dep)

(cid:16)
(cid:16)

p∞

i

p∞

i

Mpot

ij − Mdep

ij

Mpot

ij − Mdep

ij

(cid:17)(cid:0)η+
(cid:17)(cid:0)η−

i − η+

j

j − η−

i

(cid:1)
(cid:1) .

(13)

(14)

ij and decreasing Mdep

With the states in the order described above  we can ﬁnd perturbations of Mpot/dep that will always
increase the area  whilst leaving the equilibrium distribution  p∞  unchanged. Some of these pertur-
bations are shown in Figure 3  see supplementary material for details. For example  in Figure 3(a) 
for two states i on the left and j on the right  with j being more “potentiated” than i (i.e. η+
i > η+
j ) 
we have proven that increasing Mpot
leads to an increase in area. The only
thing that can prevent these perturbations from increasing the area is when they require the decrease
of a matrix element that has already been set to 0. This determines the topology (non-zero transition
probabilities) of the model with maximal area. It is of the form shown in Figure 4(c) with potenti-
ation moving one step to the right and depression moving one step to the left. Any other topology
would allow some class of perturbations (e.g. in Figure 3) to further increase the area.
As these perturbations do not change the equilibrium distribution  this means that the area of any
model is bounded by that of a linear chain with the same equilibrium distribution. The area of
a linear chain model can be expressed directly in terms of its equilibrium state distribution  p∞ 
yielding the following upper bound on the area of any model with the same p∞ (see supplementary
material):

ij

k −(cid:88)
where we chose wk = sgn[k −(cid:80)

√
A ≤ 2

(cid:88)

N
r

k

j

j jp∞

 p∞

jp∞

j

k wk =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k −(cid:88)

j

(cid:88)

k

√

2

N
r

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) p∞

k  

jp∞

j

j ]. We can then maximize this by pushing all of the equilib-
rium distribution symmetrically to the two end states. This can be done by reducing the transition
probabilities out of these states  as in Figure 4(c). This makes it very difﬁcult to exit these states
once they have been entered. The resulting area is

(15)

√

A ≤

N (M − 1)

r

.

(16)

This analytical result is similar to a numerical result found in [18] under a slightly different infor-
mation theoretic measure of memory performance.

6

The “sticky” end states result in very slow decay of memory  but they also make it difﬁcult to encode
the memory in the ﬁrst place  since a small fraction of synapses are able to change synaptic efﬁcacy
during the storage of a new memory. Thus models that maximize area optimize memory at late
times  at the expense of early times.

4 Memory curve envelope

(cid:88)
(cid:88)

a

a

Now we will look at the implications of the upper bounds found in the previous section for the SNR
at ﬁnite times. As argued in (6)  the memory curve can be written in the form

√

SNR(t) =

N

Iae−rt/τa .

(17)

(cid:88)

a

Ia ≤ 1 

Iaτa ≤ M − 1.

The upper bounds on the initial SNR  (10)  and the area  (16)  imply the following constraints on the
parameters {Ia  τa}:

(18)
We are not claiming that these are a complete set of constraints: not every set {Ia  τa} that satisﬁes
these inequalities will actually be achievable by a synaptic model. However  any set that violates
either inequality will deﬁnitely not be achievable.
Now we can pick some ﬁxed time  t0  and maximize the SNR at that time wrt. the parameters
{Ia  τa}  subject to the constraints above. This always results in a single nonzero Ia; in essence 
optimizing memory at a single time requires a single exponential. The resulting optimal memory
curve  along with the achieved memory at the chosen time  depends on t0 as follows:
t0 ≤ M − 1
t0 ≥ M − 1

√
N e−rt/(M−1)
√
N (M − 1)e−t/t0

√
N e−rt0/(M−1) 
√
N (M − 1)

=⇒ SNR(t0) =
=⇒ SNR(t0) =

=⇒ SNR(t) =

=⇒ SNR(t) =

(19)

.

r

r

rt0

ert0

Both the initial SNR bound and the area bound are saturated at early times. At late times  only
the area bound is saturated. The function SNR(t0)  the green curve in Figure 4(a)  above forms a
memory curve envelope with late-time power-law decay ∼ t−1
0 . No synaptic model can have an
SNR that is greater than this at any time. We can use this to ﬁnd an upper bound on the memory
lifetime  τ ()  by ﬁnding the point at which the envelope crosses :

τ () ≤

√

N (M − 1)

er

 

(20)

where we assume N > (e)2. Intriguingly  both the lifetime and memory envelope expand linearly
with the number of internal states M  and increase as the square root of the number of synapses N.
This leaves the question of whether this bound is achievable. At any time  can we ﬁnd a model
whose memory curve touches the envelope? The red curves in Figure 4(a) show the closest we
have come to the envelope with actual models  by repeated numerical optimization of SNR(t0) over
Mpot/dep with random initialization and by hand designed models.
We see that at early  but not late times  there is a gap between the upper bound that we can prove
and what we can achieve with actual models. There may be other models we haven’t found that
could beat the ones we have  and come closer to our proven envelope. However  we suspect that the
area constraint is not the bottleneck for optimizing memory at times less than O( M
r ). We believe
there is some other constraint that prevents models from approaching the envelope  and currently are
exploring several mathematical conjectures for the precise form of this constraint in order to obtain
a potentially tighter envelope. Nevertheless  we have proven rigorously that no model’s memory
curve can ever exceed this envelope  and that it is at least tight for late times  longer than O( M
r ) 
where models of the form in Figure 4(c)can come close to the envelope.

5 Discussion

We have initiated the development of a general theory of learning and memory with complex
synapses  allowing for an exploration of the entire space of complex synaptic models  rather than

7

(a)

(b)

(c)

Figure 4: The memory curve envelope for N = 100  M = 12. (a) An upper bound on the SNR
at any time is shown in green. The red dashed curve shows the result of numerical optimization of
synaptic models with random initialization. The solid red curve shows the highest SNR we have
found with hand designed models. At early times these models are of the form shown in (b) with
different numbers of states  and all transition probabilities equal to 1. At late times they are of the
form shown in (c) with different values of ε. The model shown in (c) also saturates the area bound
(16) in the limit ε → 0.

√

analyzing individual models one at a time. In doing so  we have obtained several new mathemati-
cal results delineating the functional limits of memory achievable by synaptic complexity  and the
structural characterization of synaptic dynamical systems that achieve these limits. In particular 
operating within the ideal observer framework of [10  11  18]  we have shown that for a population
of N synapses with M internal states  (a) the initial SNR of any synaptic model cannot exceed
N 
and any model that achieves this bound is equivalent to a binary synapse  (b) the area under the
√
memory curve of any model cannot exceed that of a linear chain model with the same equilibrium
distribution  (c) both the area and memory lifetime of any model cannot exceed O(
N M )  and the
model that achieves this limit has a linear chain topology with only nearest neighbor transitions  (d)
we have derived an envelope memory curve in the SNR-time plane that cannot be exceeded by the
memory curve of any model  and models that approach this envelope for times greater than O( M
√
r )
are linear chain models  and (e) this late-time envelope is a power-law proportional to O(
N M /rt) 
indicating that synaptic complexity can strongly enhance the limits of achievable memory.
This theoretical study opens up several avenues for further inquiry. In particular  the tightness of our
envelope for early times  less than O( M
r )  remains an open question  and we are currently pursuing
several conjectures. We have also derived memory constrained envelopes  by asking in the space
of models that achieve a given SNR at a given time  what is the maximal SNR achievable at other
times. If these two times are beyond a threshold separation  optimal constrained models require
two exponentials. It would be interesting to systematically analyze the space of models that achieve
good memory at multiple times  and understand their structural organization  and how they give rise
to multiple exponentials  leading to power law memory decays.
Finally  it would be interesting to design physiological experiments in order to perform optimal
systems identiﬁcation of potential Markovian dynamical systems hiding within biological synapses 
given measurements of pre and post-synaptic spike trains along with changes in post-synaptic po-
tentials. Then given our theory  we could match this measured synaptic model to optimal models to
understand for which timescales of memory  if any  biological synaptic dynamics may be tuned.
In summary  we hope that a deeper theoretical understanding of the functional role of synaptic
complexity  initiated here  will help advance our understanding of the neurobiology of learning and
memory  aid in the design of engineered memory circuits  and lead to new mathematical theorems
about stochastic processes.

Acknowledgements

We thank Sloan  Genenetech  Burroughs-Wellcome  and Swartz foundations for support. We thank
Larry Abbott  Marcus Benna  Stefano Fusi  Jascha Sohl-Dickstein and David Sussillo for useful
discussions.

8

10−110010110210310−210−1100101TimeSNRenvelopenumerical searchhand designedInitial SNR bound activeArea bound activeεεReferences
[1] J. J. Hopﬁeld  “Neural networks and physical systems with emergent collective computational

abilities ” Proc. Natl. Acad. Sci. U.S.A. 79 (1982) no. 8  2554–2558.

[2] D. J. Amit  H. Gutfreund  and H. Sompolinsky  “Spin-glass models of neural networks ” Phys.

Rev. A 32 (Aug  1985) 1007–1018.

[3] E. Gardner  “The space of interactions in neural network models ” Journal of Physics A:

Mathematical and General 21 (1988) no. 1  257.

[4] T. V. P. Bliss and G. L. Collingridge  “A synaptic model of memory: long-term potentiation in

the hippocampus ” Nature 361 (Jan  1993) 31–39.

[5] C. C. H. Petersen  R. C. Malenka  R. A. Nicoll  and J. J. Hopﬁeld  “All-or-none potentiation at

CA3-CA1 synapses ” Proc. Natl. Acad. Sci. U.S.A. 95 (1998) no. 8  4732–4737.

[6] D. H. O’Connor  G. M. Wittenberg  and S. S.-H. Wang  “Graded bidirectional synaptic

plasticity is composed of switch-like unitary events ” Proc. Natl. Acad. Sci. U.S.A. 102 (2005)
no. 27  9679–9684.

[7] R. Enoki  Y. ling Hu  D. Hamilton  and A. Fine  “Expression of Long-Term Plasticity at
Individual Synapses in Hippocampus Is Graded  Bidirectional  and Mainly Presynaptic:
Optical Quantal Analysis ” Neuron 62 (2009) no. 2  242 – 253.

[8] D. J. Amit and S. Fusi  “Constraints on learning in dynamic synapses ” Network:

Computation in Neural Systems 3 (1992) no. 4  443–464.

[9] D. J. Amit and S. Fusi  “Learning in neural networks with material synapses ” Neural

Computation 6 (1994) no. 5  957–982.

[10] S. Fusi  P. J. Drew  and L. F. Abbott  “Cascade models of synaptically stored memories ”

Neuron 45 (Feb  2005) 599–611.

[11] S. Fusi and L. F. Abbott  “Limits on the memory storage capacity of bounded synapses ” Nat.

Neurosci. 10 (Apr  2007) 485–493.

[12] C. Leibold and R. Kempter  “Sparseness Constrains the Prolongation of Memory Lifetime via

Synaptic Metaplasticity ” Cerebral Cortex 18 (2008) no. 1  67–77.

[13] D. S. Bredt and R. A. Nicoll  “AMPA Receptor Trafﬁcking at Excitatory Synapses ” Neuron

40 (2003) no. 2  361 – 379.

[14] M. P. Coba  A. J. Pocklington  M. O. Collins  M. V. Kopanitsa  R. T. Uren  S. Swamy  M. D.
Croning  J. S. Choudhary  and S. G. Grant  “Neurotransmitters drive combinatorial multistate
postsynaptic density networks ” Sci Signal 2 (2009) no. 68  ra19.

[15] W. C. Abraham and M. F. Bear  “Metaplasticity: the plasticity of synaptic plasticity ” Trends

in Neurosciences 19 (1996) no. 4  126 – 130.

[16] J. M. Montgomery and D. V. Madison  “State-Dependent Heterogeneity in Synaptic

Depression between Pyramidal Cell Pairs ” Neuron 33 (2002) no. 5  765 – 777.

[17] R. D. Emes and S. G. Grant  “Evolution of Synapse Complexity and Diversity ” Annual

Review of Neuroscience 35 (2012) no. 1  111–131.

[18] A. B. Barrett and M. C. van Rossum  “Optimal learning rules for discrete synapses ” PLoS

Comput. Biol. 4 (Nov  2008) e1000230.

[19] J. Kemeny and J. Snell  Finite markov chains. Springer  1960.
[20] C. Burke and M. Rosenblatt  “A Markovian function of a Markov chain ” The Annals of

Mathematical Statistics 29 (1958) no. 4  1112–1122.

[21] F. Ball and G. F. Yeo  “Lumpability and Marginalisability for Continuous-Time Markov

Chains ” Journal of Applied Probability 30 (1993) no. 3  518–528.

9

,Subhaneil Lahiri
Surya Ganguli