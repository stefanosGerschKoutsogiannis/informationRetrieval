2013,Sign Cauchy Projections and Chi-Square Kernel,The method of Cauchy random projections is popular  for computing the $l_1$ distance in high dimension. In this paper  we propose to use only the signs of the projected data and show that the  probability of collision (i.e.  when the two signs differ) can be accurately approximated as a function of the chi-square ($\chi^2$) similarity  which is a popular  measure for nonnegative data (e.g.  when features are generated from histograms as common in text and vision applications). Our experiments   confirm that this method of sign Cauchy random projections is promising for large-scale  learning applications. Furthermore  we extend the idea to sign $\alpha$-stable random projections and derive a bound of the collision probability.,Sign Cauchy Projections and Chi-Square Kernel

Ping Li

Dept of Statistics & Biostat.
Dept of Computer Science

Rutgers University

pingli@stat.rutgers.edu

Gennady Samorodnitsky

ORIE and Dept of Stat. Science

Cornell University
Ithaca  NY 14853
gs18@cornell.edu

John Hopcroft

Dept of Computer Science

Cornell University
Ithaca  NY 14853
jeh@cs.cornell.edu

Abstract

The method of stable random projections is useful for efﬁciently approximating
the l(cid:11) distance (0 < (cid:11) ≤ 2) in high dimension and it is naturally suitable for data
streams. In this paper  we propose to use only the signs of the projected data and
we analyze the probability of collision (i.e.  when the two signs differ). Interest-
ingly  when (cid:11) = 1 (i.e.  Cauchy random projections)  we show that the probability
of collision can be accurately approximated as functions of the chi-square ((cid:31)2)
similarity. In text and vision applications  the (cid:31)2 similarity is a popular measure
when the features are generated from histograms (which are a typical example of
data streams). Experiments conﬁrm that the proposed method is promising for
large-scale learning applications. The full paper is available at arXiv:1308.1009.
There are many future research problems. For example  when (cid:11) → 0  the collision
probability is a function of the resemblance (of the binary-quantized data). This
provides an effective mechanism for resemblance estimation in data streams.

1 Introduction
High-dimensional representations have become very popular in modern applications of machine
learning  computer vision  and information retrieval. For example  Winner of 2009 PASCAL image
classiﬁcation challenge used millions of features [29]. [1  30] described applications with billion or
trillion features. The use of high-dimensional data often achieves good accuracies at the cost of a
signiﬁcant increase in computations  storage  and energy consumptions.
Consider two data vectors (e.g.  two images) u; v ∈ RD. A basic task is to compute their distance
or similarity. For example  the correlation ((cid:26)2) and l(cid:11) distance (d(cid:11)) are commonly used:

∑
√∑

∑

D
i=1 uivi
D
i=1 v2
i

D
i=1 u2
i

(cid:26)2(u; v) =

;

d(cid:11)(u; v) =

In this study  we are particularly interested in the (cid:31)2 similarity  denoted by (cid:26)(cid:31)2:

D∑

i=1

|ui − vi|(cid:11)
D∑

D∑

(1)

(2)

(3)

D∑
D∑

i=1

The chi-square similarity is closely related to the chi-square distance d(cid:31)2:

(cid:26)(cid:31)2 =

2uivi
ui + vi

;

(ui − vi)2
ui + vi

d(cid:31)2 =

i=1

where ui ≥ 0; vi ≥ 0;
D∑
(ui + vi) − D∑

=

i=1

i=1

ui =

vi = 1

i=1

i=1

4uivi
ui + vi

= 2 − 2(cid:26)(cid:31)2

The chi-square similarity is an instance of the Hilbertian metrics  which are deﬁned over probability
space [10] and suitable for data generated from histograms. Histogram-based features (e.g.  bag-
of-word or bag-of-visual-word models) are extremely popular in computer vision  natural language
processing (NLP)  and information retrieval. Empirical studies have demonstrated the superiority of
the (cid:31)2 distance over l2 or l1 distances for image and text classiﬁcation tasks [4  10  13  2  28  27  26].
The method of normal random projections (i.e.  (cid:11)-stable projections with (cid:11) = 2) has become
popular in machine learning (e.g.  [7]) for reducing the data dimensions and data sizes  to facilitate

1

efﬁcient computations of the l2 distances and correlations. More generally  the method of stable
random projections [11  17] provides an efﬁcient algorithm to compute the l(cid:11) distances (0 < (cid:11) ≤ 2).
In this paper  we propose to use only the signs of the projected data after applying stable projections.
1.1 Stable Random Projections and Sign (1-Bit) Stable Random Projections
Consider two high-dimensional data vectors u; v ∈ RD. The basic idea of stable random projections
is to multiply u and v by a random matrix R ∈ RD(cid:2)k: x = uR ∈ Rk  y = vR ∈ Rk  where entries
of R are i.i.d. samples from a symmetric (cid:11)-stable distribution with unit scale. By properties of
stable distributions  xj − yj follows a symmetric (cid:11)-stable distribution with scale d(cid:11). Hence  the
task of computing d(cid:11) boils down to estimating the scale d(cid:11) from k i.i.d. samples. In this paper  we
propose to store only the signs of projected data and we study the probability of collision:

(4)
Using only the signs (i.e.  1 bit) has signiﬁcant advantages for applications in search and learning.
When (cid:11) = 2  this probability can be analytically evaluated [9] (or via a simple geometric argument):

P(cid:11) = Pr (sign(xj) ̸= sign(yj))

P2 = Pr (sign(xj) ̸= sign(yj)) =

(cid:0)1 (cid:26)2

cos

1
(cid:25)

(5)

which is an important result known as sim-hash [5]. For (cid:11) < 2  the collision probability is an
open problem. When the data are nonnegative  this paper (Theorem 1) will prove a bound of P(cid:11)
for general 0 < (cid:11) ≤ 2. The bound is exact at (cid:11) = 2 and becomes less sharp as (cid:11) moves away
from 2. Furthermore  for (cid:11) = 1 and nonnegative data  we have the interesting observation that the
probability P1 can be well approximated as functions of the (cid:31)2 similarity (cid:26)(cid:31)2.
1.2 The Advantages of Sign Stable Random Projections

1. There is a signiﬁcant saving in storage space by using only 1 bit instead of (e.g. ) 64 bits.
2. This scheme leads to an efﬁcient linear algorithm (e.g.  linear SVM). For example  a nega-
tive sign can be coded as “01” and a positive sign as “10” (i.e.  a vector of length 2). With
k projections  we concatenate k short vectors to form a vector of length 2k. This idea is
inspired by b-bit minwise hashing [20]  which was designed for binary sparse data.

3. This scheme also leads to an efﬁcient near neighbor search algorithm [8  12]. We can code
a negative sign by “0” and positive sign by “1” and concatenate k such bits to form a hash
table of 2k buckets. In the query phase  one only searches for similar vectors in one bucket.

1.3 Data Stream Computations
Stable random projections are naturally suitable for data streams. In modern applications  massive
datasets are often generated in a streaming fashion  which are difﬁcult to transmit and store [22]  as
the processing is done on the ﬂy in one-pass of the data. In the standard turnstile model [22]  a data
stream can be viewed as high-dimensional vector with the entry values changing over time.

= u(t(cid:0)1)

Here  we denote a stream at time t by u(t)
  i = 1 to D. At time t  a stream element (it; It)
i
arrives and updates the it-th coordinate as u(t)
+ It. Clearly  the turnstile data stream
it
model is particularly suitable for describing histograms and it is also a standard model for network
trafﬁc summarization and monitoring [31]. Because this stream model is linear  methods based on
linear projections (i.e.  matrix-vector multiplications) can naturally handle streaming data of this
sort. Basically  entries of the projection matrix R ∈ RD(cid:2)k are (re)generated as needed using
pseudo-random number techniques [23]. As (it; It) arrives  only the entries in the it-th row  i.e. 
+ It × ritj.
rit;j  j = 1 to k  are (re)generated and the projected data are updated as x(t)
Recall that  in the deﬁnition of (cid:31)2 similarity  the data are assumed to be normalized (summing to
1). For nonnegative streams  the sum can be computed error-free by using merely one counter:
s=1 Is. Thus we can still use  without loss of generality  the sum-to-one assump-
tion  even in the streaming environment. This fact was recently exploited by another data stream
algorithm named Compressed Counting (CC) [18] for estimating the Shannon entropy of streams.
Because the use of the (cid:31)2 similarity is popular in (e.g. ) computer vision  recently there are other
proposals for estimating the (cid:31)2 similarity. For example  [15] proposed a nice technique to approxi-
mate (cid:26)(cid:31)2 by ﬁrst expanding the data from D dimensions to (e.g. ) 5 ∼ 10 × D dimensions through
a nonlinear transformation and then applying normal random projections on the expanded data. The
nonlinear transformation makes their method not applicable to data streams  unlike our proposal.

j = x(t(cid:0)1)

∑

∑

D

i=1 u(t)

i =

t

j

it

2

For notational simplicity  we will drop the superscript (t) for the rest of the paper.
2 An Experimental Study of Chi-Square Kernels
We provide an experimental study to validate the use of (cid:31)2 similarity. Here  the “(cid:31)2-kernel” is
deﬁned as K(u; v) = (cid:26)(cid:31)2 and the “acos-(cid:31)2-kernel” as K(u; v) = 1 − 1
(cid:0)1 (cid:26)(cid:31)2. With a slight
(cid:25) cos
abuse of terminology  we call both “(cid:31)2 kernel” when it is clear in the context.
We use the “precomputed kernel” functionality in LIBSVM on two datasets: (i) UCI-PEMS  with
267 training examples and 173 testing examples in 138 672 dimensions; (ii) MNIST-small  a subset
of the popular MNIST dataset  with 10 000 training examples and 10 000 testing examples.
The results are shown in Figure 1. To compare these two types of (cid:31)2 kernels with “linear” kernel 
we also test the same data using LIBLINEAR [6] after normalizing the data to have unit Euclidian
norm  i.e.  we basically use (cid:26)2. For both LIBSVM and LIBLINEAR  we use l2-regularization with
a regularization parameter C and we report the test errors for a wide range of C values.

Figure 1: Classiﬁcation accuracies. C is the l2-regularization parameter. We use LIBLINEAR
for “linear” (i.e.  (cid:26)2) kernel and LIBSVM “precomputed kernel” for two types of (cid:31)2 kernels (“(cid:31)2-
kernel” and “acos-(cid:31)2-kernel”). For UCI-PEMS  the (cid:31)2-kernel has better performance than the linear
kernel and acos-(cid:31)2-kernel. For MNIST-Small  both (cid:31)2 kernels noticeably outperform linear kernel.
Note that MNIST-small used the original MNIST test set and merely 1/6 of the original training set.

(

)

∑

Here  we should state that it is not the intention of this paper to use these two small examples
to conclude the advantage of (cid:31)2 kernels over linear kernel. We simply use them to validate our
proposed method  which is general-purpose and is not limited to data generated from histograms.
3 Sign Stable Random Projections and the Collision Probability Bound
(
We apply stable random projections on two vectors u; v ∈ RD: x =
i=1 viri 
ri ∼ S((cid:11); 1)  i.i.d. Here Z ∼ S((cid:11); (cid:13)) denotes a symmetric (cid:11)-stable distribution with scale (cid:13) 
)
(cid:0)(cid:13)jtj(cid:11). By properties of stable distributions 
whose characteristic function [24] is E
|ui − vi|(cid:11)
we know x−y ∼ S
. Applications including linear learning and near neighbor
search will beneﬁt from sign (cid:11)-stable random projections. When (cid:11) = 2 (i.e. normal)  the collision
probability Pr (sign(x) ̸= sign(y)) is known [5  9]. For (cid:11) < 2  it is a difﬁcult probability problem.
This section provides a bound of Pr (sign(x) ̸= sign(y))  which is fairly accurate for (cid:11) close to 2.
3.1 Collision Probability Bound
In this paper  we focus on nonnegative data (as common in practice). We present our ﬁrst theorem.
Theorem 1 When the data are nonnegative  i.e.  ui ≥ 0; vi ≥ 0  we have

i=1 uiri  y =

p(cid:0)1Zt

∑

∑

(cid:11);

D
i=1

e

= e

D

D

Pr (sign(x) ̸= sign(y)) ≤ 1
(cid:25)

(cid:0)1 (cid:26)(cid:11); where (cid:26)(cid:11) =

cos

 ∑
√∑

∑

D

i=1 u(cid:11)=2
i
D
i=1 u(cid:11)
i

v(cid:11)=2
i
D
i=1 v(cid:11)
i

2=(cid:11)

(cid:3) (6)

For (cid:11) = 2  this bound is exact [5  9]. In fact the result for (cid:11) = 2 leads to the following Lemma:
Lemma 1 The kernel deﬁned as K(u; v) = 1 − 1
Proof: The indicator function 1{sign(x) = sign(y)} can be written as an inner product (hence PD)
and Pr (sign(x) = sign(y)) = E (1{sign(x) = sign(y)}) = 1 − 1
(cid:3)

(cid:0)1 (cid:26)2 is positive deﬁnite (PD).

(cid:25) cos

(cid:0)1 (cid:26)2.

(cid:25) cos

3

10−210−1100101102103020406080100CClassification Acc (%) PEMSlinearχ2acos χ210−210−110010110260708090100CClassification Acc (%) MNIST−Smalllinearχ2acos χ23.2 A Simulation Study to Verify the Bound of the Collision Probability
We generate the original data u and v by sampling from a bivariate t-distribution  which has two
parameters: the correlation and the number of degrees of freedom (which is taken to be 1 in our
experiments). We use a full range of the correlation parameter from 0 to 1 (spaced at 0.01). To
generate positive data  we simply take the absolute values of the generated data. Then we ﬁx the
data as our original data (like u and v)  apply sign stable random projections  and report the empirical
collision probabilities (after 105 repetitions).
Figure 2 presents the simulated collision probability Pr (sign(x) ̸= sign(y)) for D = 100 and (cid:11) ∈
{1:5; 1:2; 1:0; 0:5}. In each panel  the dashed curve is the theoretical upper bound 1
(cid:0)1 (cid:26)(cid:11)  and
the solid curve is the simulated collision probability. Note that it is expected that the simulated data
can not cover the entire range of (cid:26)(cid:11) values  especially as (cid:11) → 0.

(cid:25) cos

Figure 2: Dense Data and D = 100. Simulated collision probability Pr (sign(x) ̸= sign(y)) for
sign stable random projections. In each panel  the dashed curve is the upper bound 1

(cid:0)1 (cid:26)(cid:11).

(cid:25) cos

(cid:25) cos

(cid:0)1 (cid:26)(cid:11). When (cid:11) ≥ 1:5  this upper bound is fairly
Figure 2 veriﬁes the theoretical upper bound 1
sharp. However  when (cid:11) ≤ 1  the bound is not tight  especially for small (cid:11). Also  the curves of the
empirical collision probabilities are not smooth (in terms of (cid:26)(cid:11)).
Real-world high-dimensional datasets are often sparse. To verify the theoretical upper bound of
the collision probability on sparse data  we also simulate sparse data by randomly making 50% of
the generated data as used in Figure 2 be zero. With sparse data  it is even more obvious that the
theoretical upper bound 1

(cid:0)1 (cid:26)(cid:11) is not sharp when (cid:11) ≤ 1  as shown in Figure 3.

(cid:25) cos

Figure 3: Sparse Data and D = 100. Simulated collision probability Pr (sign(x) ̸= sign(y)) for
sign stable random projection. The upper bound is not tight especially when (cid:11) ≤ 1.
In summary  the collision probability bound: Pr (sign(x) ̸= sign(y)) ≤ 1
when (cid:11) is close to 2 (e.g.  (cid:11) ≥ 1:5). However  for (cid:11) ≤ 1  a better approximation is needed.
4 (cid:11) = 1 and Chi-Square ((cid:31)2) Similarity
In this section  we focus on nonnegative data (ui ≥ 0; vi ≥ 0) and (cid:11) = 1. This case is important in
practice. For example  we can view the data (ui  vi) as empirical probabilities  which are common
when data are generated from histograms (as popular in NLP and vision) [4  10  13  2  28  27  26].

(cid:0)1 (cid:26)(cid:11) is fairly sharp

(cid:25) cos

∑

∑

In this context  we always normalize the data  i.e. 

D
i=1 ui =

D

i=1 vi = 1. Theorem 1 implies

Pr (sign(x) ̸= sign(y)) ≤ 1
(cid:25)

(cid:0)1 (cid:26)1; where (cid:26)1 =

cos

u1=2
i

v1=2
i

(7)

)2

(

D∑

i=1

While the bound is not tight  interestingly  the collision probability can be related to the (cid:31)2 similarity.

Recall the deﬁnitions of the chi-square distance d(cid:31)2 =
(cid:26)(cid:31)2 = 1 − 1

. In this context  we should view 0

D
i=1

2 d(cid:31)2 =

D
i=1

2uivi
ui+vi

0 = 0.

(ui(cid:0)vi)2
ui+vi

and the chi-square similarity

∑

∑

4

0.20.40.60.8100.10.20.30.40.5α = 1.5  D = 100ραCollision probability0.40.60.8100.10.20.30.40.5α = 1.2  D = 100ραCollision probability0.40.60.8100.10.20.30.40.5α = 1  D = 100ραCollision probability0.70.80.9100.10.20.30.40.5α = 0.5  D = 100ραCollision probability00.20.40.60.8100.10.20.30.40.5α = 1.5  D = 100  SparseραCollision probability00.20.40.60.8100.10.20.30.40.5α = 1.2  D = 100  SparseραCollision probability00.20.40.60.8100.10.20.30.40.5α = 1  D = 100  SparseραCollision probability00.10.20.30.400.10.20.30.40.5α = 0.5  D = 100  SparseραCollision probability∑
D∑
Lemma 2 Assume ui ≥ 0; vi ≥ 0 

D

i=1 ui = 1 

D

i=1 vi = 1. Then

)2

∑
≥ (cid:26)1 =

(
D∑

i=1

(cid:26)(cid:31)2 =

i=1

2uivi
ui + vi

u1=2
i

v1=2
i

(cid:3)

(8)

It is known that the (cid:31)2-kernel is PD [10]. Consequently  we know the acos-(cid:31)2-kernel is also PD.
Lemma 3 The kernel deﬁned as K(u; v) = 1 − 1
The remaining question is how to connect Cauchy random projections with the (cid:31)2 similarity.
5 Two Approximations of Collision Probability for Sign Cauchy Projections
It is a difﬁcult problem to derive the collision probability of sign Cauchy projections if we would
like to express the probability only in terms of certain summary statistics (e.g.  some distance). Our
ﬁrst observation is that the collision probability can be well approximated using the (cid:31)2 similarity:

(cid:0)1 (cid:26)(cid:31)2 is positive deﬁnite (PD).

(cid:25) cos

(cid:3)

Pr (sign(x) ̸= sign(y)) ≈ P(cid:31)2(1) =
(

(9)
(cid:0)1 ((cid:26)1). Particularly  in sparse data  the
is very accurate (except when (cid:26)(cid:31)2 is close to 1)  while the bound

(cid:25) cos

)

(cid:26)(cid:31)2

(cid:0)1

1
(cid:25)

cos

Figure 4 shows this approximation is better than 1
approximation 1
1
(cid:25) cos

(cid:0)1 ((cid:26)1) is not sharp (and the curve is not smooth in (cid:26)1).

(cid:25) cos

(cid:26)(cid:31)2

(cid:0)1

(

)

(cid:0)1 ((cid:26))  where (cid:26) can be (cid:26)1 or (cid:26)(cid:31)2 depending on the context. In
Figure 4: The dashed curve is 1
each panel  the two solid curves are the empirical collision probabilities in terms of (cid:26)1 (labeled by
(cid:0)1 (cid:26)(cid:31)2 in (9) is more
“1”) or (cid:26)(cid:31)2 (labeled by “(cid:31)2). It is clear that the proposed approximation 1
tight than the upper bound 1

(cid:0)1 (cid:26)1  especially so in sparse data.

(cid:25) cos

(cid:25) cos

(cid:25) cos

Our second (and less obvious) approximation is the following integral:

∫

(

)

Pr (sign(x) ̸= sign(y)) ≈ P(cid:31)2(2) =

1
2

− 2
(cid:25)2

(cid:25)=2

(cid:0)1

tan

0

(cid:26)(cid:31)2

2 − 2(cid:26)(cid:31)2

tan t

dt

(10)

Figure 5 illustrates that  for dense data  the second approximation (10) is more accurate than the
ﬁrst (9). The second approximation (10) is also accurate for sparse data. Both approximations 
P(cid:31)2(1) and P(cid:31)2(2)  are monotone functions of (cid:26)(cid:31)2. In practice  we often do not need the (cid:26)(cid:31)2 values
explicitly because it often sufﬁces if the collision probability is a monotone function of the similarity.
5.1 Binary Data
Interestingly  when the data are binary (before normalization)  we can compute the collision prob-
ability exactly  which allows us to analytically assess the accuracy of the approximations. In fact 
this case inspired us to propose the second approximation (10)  which is otherwise not intuitive.
For convenience  we deﬁne a = |Ia|; b = |Ib|; c = |Ic|  where
Ib = {i|vi > 0; ui = 0};

Ia = {i|ui > 0; vi = 0};

Ic = {i|ui > 0; vi > 0};

(11)

Assume binary data (before normalization  i.e.  sum to one). That is 

ui =

1

|Ia| + |Ic| =

1

a + c

; ∀i ∈ Ia ∪ Ic;

The chi-square similarity (cid:26)(cid:31)2 becomes (cid:26)(cid:31)2 =

1

|Ib| + |Ic| =
= 2c

vi =

2uivi
ui+vi

; ∀i ∈ Ib ∪ Ic

(12)

1

b + c

a+b+2c and hence

(cid:26)(cid:31)2
2(cid:0)2(cid:26)(cid:31)2

= c

a+b.

∑

D
i=1

5

0.40.60.8100.10.20.30.40.5ρχ2  ρ1Collision probability1χ2α = 1  D = 10000.20.40.60.8100.10.20.30.40.5ρχ2  ρ1Collision probability1χ2α = 1  D = 100  Sparse)}
Theorem 2 Assume binary data. When (cid:11) = 1  the exact collision probability is
|R|
(

where R is a standard Cauchy random variable.

Pr (sign(x) ̸= sign(y)) =

− 2

(cid:25)2 E

|R|

{

)

(

tan

tan

(cid:0)1

(cid:0)1

1
2

c
b

(

(cid:0)1

{
(

(

(cid:0)1

|R|)
)}

When a = 0 or b = 0  we have E
c
b
observation inspires us to propose the approximation (10):
− 2
(cid:25)2

− 1
(cid:25)

P(cid:31)2(2) =

a + b

{

tan

tan

tan

(cid:0)1

1
2

1
2

=

E

c
a

c

c
a

(
|R|)}
∫

{
(

= (cid:25)

2 E

(cid:0)1

tan

c

a+b

(cid:25)=2

(cid:0)1

tan

0

c

a + b

tan t

dt

(13)
(cid:3)

. This

)}
|R|
)

Figure 5: Comparison of two approximations: (cid:31)2(1) based on (9) and (cid:31)2(2) based on (10). The
solid curves (empirical probabilities expressed in terms of (cid:26)(cid:31)2) are the same solid curves labeled
“(cid:31)2” in Figure 4. The left panel shows that the second approximation (10) is more accurate in dense
data. The right panel illustrate that both approximations are accurate in sparse data. (9) is slightly
more accurate at small (cid:26)(cid:31)2 and (10) is more accurate at (cid:26)(cid:31)2 close to 1.

|R|
(

1
b=c

To validate this approximation for binary data  we study the difference between (13) and (10)  i.e. 

{

{
Z(a=c; b=c) = Err = Pr (sign(x) ̸= sign(y)) − P(cid:31)2(2)
= − 2

)}

)

(

|R|

|R|

tan

tan

(cid:0)1

(cid:0)1

+

E

(

(cid:25)2 E

1
a=c

1
(cid:25)

(cid:0)1

tan

1

a=c + b=c

|R|

(14)

)}

(14) can be easily computed by simulations. Figure 6 conﬁrms that the errors are larger than zero
and very small . The maximum error is smaller than 0.0192  as proved in Lemma 4.

Figure 6: Left panel: contour plot for the error Z(a=c; b=c) in (14). The maximum error (which is
< 0:0192) occurs along the diagonal line. Right panel: the diagonal curve of Z(a=c; b=c).

Lemma 4 The error deﬁned in (14) ranges between 0 and Z(t
0 ≤ Z(a=c; b=c) ≤ Z(t

tan

) =

(cid:0)1

+

(cid:3)

∫ 1

{

(

− 2
(cid:25)2

))2

(

r
t(cid:3)

(cid:3)

):

(cid:0)1

tan

1
(cid:25)

)}

(

r
2t(cid:3)

(cid:3)

where t

= 2:77935 is the solution to

0

1

t2(cid:0)1 log 2t

1+t = log(2t)

(cid:3)
(2t)2(cid:0)1 . Numerically  Z(t

6

1

2
1 + r2 dr (15)
(cid:25)
) = 0:01919. (cid:3)

0.40.60.8100.10.20.30.40.5ρχ2Collision probabilityα = 1  D = 100 χ2 (1)χ2 (2)Empirical00.20.40.60.8100.10.20.30.40.5ρχ2Collision probabilityα = 1  D = 100  Sparse χ2 (1)χ2 (2)Empiricala/cb/c0.0010.010.01910−210−110010110210−210−110010110210−210−110010110210300.0050.010.0150.02tZ(t)5.2 An Experiment Based on 3.6 Million English Word Pairs
To further validate the two (cid:31)2 approximations (in non-binary data)  we experiment with a word
occurrences dataset (which is an example of histogram data) from a chunk of D = 216 web crawl
documents. There are in total 2 702 words  i.e.  2 702 vectors and 3 649 051 word pairs. The entries
of a vector are the occurrences of the word. This is a typical sparse  non-binary dataset. Interestingly 
the errors of the collision probabilities based on two (cid:31)2 approximations are still very small. To report
the results  we apply sign Cauchy random projections 107 times to evaluate the approximation errors
(cid:0)1 (cid:26)1
of (9) and (10). The results  as presented in Figure 7  again conﬁrm that the upper bound 1
is not tight and both (cid:31)2 approximations  P(cid:31)2(1) and P(cid:31)2(2)  are accurate.

(cid:25) cos

Figure 7: Empirical collision probabilities for 3.6 million English word pairs. In the left panel 
we plot the empirical collision probabilities against (cid:26)1 (lower  green if color is available) and (cid:26)(cid:31)2
(cid:0)1 (cid:26)1 is not tight (and the curve is not smooth).
(higher  red). The curves conﬁrm that the bound 1
We plot the two (cid:31)2 approximations as dashed curves which largely match the empirical probabilities
plotted against (cid:26)(cid:31)2  conﬁrming that the (cid:31)2 approximations are good. For smaller (cid:26)(cid:31)2 values  the
ﬁrst approximation P(cid:31)2(1) is slightly more accurate. For larger (cid:26)(cid:31)2 values  the second approximation
P(cid:31)2(2) is more accurate. In the right panel  we plot the errors for both P(cid:31)2(1) and P(cid:31)2(2).

(cid:25) cos

6 Sign Cauchy Random Projections for Classiﬁcation
Our method provides an effective strategy for classiﬁcation. For each (high-dimensional) data vec-
tor  using k sign Cauchy projections  we encode a negative sign as “01” and a positive as “10” (i.e. 
a vector of length 2) and concatenate k short vectors to form a new feature vector of length 2k. We
then feed the new data into a linear classiﬁer (e.g.  LIBLINEAR). Interestingly  this linear classiﬁer
approximates a nonlinear kernel classiﬁer based on acos-(cid:31)2-kernel: K(u; v) = 1− 1
(cid:0)1 (cid:26)(cid:31)2. See
Figure 8 for the experiments on the same two datasets in Figure 1: UCI-PEMS and MNIST-Small.

(cid:25) cos

Figure 8: The two dashed (red if color is available) curves are the classiﬁcation results obtained
using “acos-(cid:31)2-kernel” via the “precomputed kernel” functionality in LIBSVM. The solid (black)
curves are the accuracies using k sign Cauchy projections and LIBLINEAR. The results conﬁrm
that the linear kernel from sign Cauchy projections can approximate the nonlinear acos-(cid:31)2-kernel.

Figure 1 has already shown that  for the UCI-PEMS dataset  the (cid:31)2-kernel ((cid:26)(cid:31)2) can produce notice-
ably better classiﬁcation results than the acos-(cid:31)2-kernel (1 − 1
(cid:0)1 (cid:26)(cid:31)2). Although our method
does not directly approximate (cid:26)(cid:31)2  we can still estimate (cid:26)(cid:31)2 by assuming the collision probability
is exactly Pr (sign(x) ̸= sign(y)) = 1
(cid:0)1 (cid:26)(cid:31)2 and then we can feed the estimated (cid:26)(cid:31)2 values
into LIBSVM “precomputed kernel” for classiﬁcation. Figure 9 veriﬁes that this method can also
approximate the (cid:31)2 kernel with enough projections.

(cid:25) cos

(cid:25) cos

7

00.20.40.60.8100.10.20.30.40.5ρχ2 or ρ1Collision probabilityχ2(1)χ2(2)00.20.40.60.81−0.03−0.02−0.0100.010.020.03ρχ2Errorχ2(1)χ2(2)10−210−1100101102103020406080100k = 32k = 64k = 128k = 2565121024k = 2048 4096 8192PEMS: SVMCClassification Acc (%)10−210−110010110260708090100k = 64k = 128k = 256k = 51210242048k = 4096  8192CClassification Acc (%)MNIST−Small: SVMFigure 9: Nonlinear kernels. The dashed curves are the classiﬁcation results obtained using (cid:31)2-
kernel and LIBSVM “precomputed kernel” functionality. We apply k sign Cauchy projections and
(cid:0)1 (cid:26)(cid:31)2 and then feed the estimated
estimate (cid:26)(cid:31)2 assuming the collision probability is exactly 1
(cid:26)(cid:31)2 into LIBSVM again using the “precomputed kernel” functionality.

(cid:25) cos

7 Conclusion
The use of (cid:31)2 similarity is widespread in machine learning  especially when features are generated
from histograms  as common in natural language processing and computer vision. Many prior stud-
ies [4  10  13  2  28  27  26] have shown the advantage of using (cid:31)2 similarity compared to other
measures such as l2 distance. However  for large-scale applications with ultra-high-dimensional
datasets  using (cid:31)2 similarity becomes challenging for practical reasons. Simply storing (and maneu-
vering) all the high-dimensional features would be difﬁcult if there are a large number of observa-
tions. Computing all pairwise (cid:31)2 similarities can be time-consuming and in fact we usually can not
materialize an all-pairwise similarity matrix even if there are merely 106 data points. Furthermore 
the (cid:31)2 similarity is nonlinear  making it difﬁcult to take advantage of modern linear algorithms
which are known to be very efﬁcient  e.g.  [14  25  6  3]. When data are generated in a streaming
fashion  computing (cid:31)2 similarities without storing the original data will be even more challenging.
The method of (cid:11)-stable random projections (0 < (cid:11) ≤ 2) [11  17] is popular for efﬁciently com-
puting the l(cid:11) distances in massive (streaming) data. We propose sign stable random projections by
storing only the signs (i.e.  1-bit) of the projected data. Obviously  the saving in storage would be
a signiﬁcant advantage. Also  these bits offer the indexing capability which allows efﬁcient search.
For example  we can build hash tables using the bits to achieve sublinear time near neighbor search
(although this paper does not focus on near neighbor search). We can also build efﬁcient linear
classiﬁers using these bits  for large-scale high-dimensional machine learning applications.
A crucial task in analyzing sign stable random projections is to study the probability of collision (i.e. 
when the two signs differ). We derive a theoretical bound of the collision probability which is exact
when (cid:11) = 2. The bound is fairly sharp for (cid:11) close to 2. For (cid:11) = 1 (i.e.  Cauchy random projec-
tions)  we ﬁnd the (cid:31)2 approximation is signiﬁcantly more accurate. In addition  for binary data  we
analytically show that the errors from using the (cid:31)2 approximation are less than 0.0192. Experiments
on real and simulated data conﬁrm that our proposed (cid:31)2 approximations are very accurate.
We are enthusiastic about the practicality of sign stable projections in learning and search applica-
tions. The previous idea of using the signs from normal random projections has been widely adopted
in practice  for approximating correlations. Given the widespread use of the (cid:31)2 similarity and the
simplicity of our method  we expect the proposed method will be adopted by practitioners.
Future research Many interesting future research topics can be studied. (i) The processing cost
of conducting stable random projections can be dramatically reduced by very sparse stable random
projections [16]. This will make our proposed method even more practical. (ii) We can try to utilize
more than just 1-bit of the projected data  i.e.  we can study the general coding problem [19]. (iii)
Another interesting research would be to study the use of sign stable projections for sparse signal
(iv) When (cid:11) → 0  the collision
recovery (Compressed Sensing) with stable distributions [21].
probability becomes Pr (sign(x) ̸= sign(y)) = 1
2 Resemblance  which provides an elegant
mechanism for computing resemblance (of the binary-quantized data) in sparse data streams.
Acknowledgement
The work of Ping Li is supported by NSF-III-1360971  NSF-Bigdata-
1419210  ONR-N00014-13-1-0764  and AFOSR-FA9550-13-1-0137. The work of Gennady
Samorodnitsky is supported by ARO-W911NF-12-10385.

− 1

2

8

10−210−1100101102103020406080100k = 32k = 64k = 128k = 256k = 512k = 1024k = 2048k = 4096k = 8192PEMS: χ2 kernel SVMCClassification Acc (%)10−210−110010110260708090100k = 64k = 128k = 256k = 512k = 1024204840968192CClassification Acc (%)MNIST−Small: χ2 kernel SVMReferences
[1] http://googleresearch.blogspot.com/2010/04/ lessons-learned-developing-practical.html.
[2] Bogdan Alexe  Thomas Deselaers  and Vittorio Ferrari. What is an object? In CVPR  pages 73–80  2010.
[3] Leon Bottou. http://leon.bottou.org/projects/sgd.
[4] Olivier Chapelle  Patrick Haffner  and Vladimir N. Vapnik. Support vector machines for histogram-based

image classiﬁcation. IEEE Trans. Neural Networks  10(5):1055–1064  1999.

[5] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  2002.
[6] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. Liblinear: A library

for large linear classiﬁcation. Journal of Machine Learning Research  9:1871–1874  2008.

[7] Yoav Freund  Sanjoy Dasgupta  Mayank Kabra  and Nakul Verma. Learning the structure of manifolds

using random projections. In NIPS  Vancouver  BC  Canada  2008.

[8] Jerome H. Friedman  F. Baskett  and L. Shustek. An algorithm for ﬁnding nearest neighbors.

Transactions on Computers  24:1000–1006  1975.

IEEE

[9] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and

satisﬁability problems using semideﬁnite programming. Journal of ACM  42(6):1115–1145  1995.

[10] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability mea-

sures. In AISTATS  pages 136–143  Barbados  2005.

[11] Piotr Indyk. Stable distributions  pseudorandom generators  embeddings  and data stream computation.

Journal of ACM  53(3):307–323  2006.

[12] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimen-

sionality. In STOC  pages 604–613  Dallas  TX  1998.

[13] Yugang Jiang  Chongwah Ngo  and Jun Yang. Towards optimal bag-of-features for object categorization

and semantic video retrieval. In CIVR  pages 494–501  Amsterdam  Netherlands  2007.

metric convergence. Technical report  arXiv:1206.4074  2013.

[14] Thorsten Joachims. Training linear svms in linear time. In KDD  pages 217–226  Pittsburgh  PA  2006.
[15] Fuxin Li  Guy Lebanon  and Cristian Sminchisescu. A linear approximation to the (cid:31)2 kernel with geo-
[16] Ping Li. Very sparse stable random projections for dimension reduction in l(cid:11) (0 < (cid:11) (cid:20) 2) norm. In
[17] Ping Li. Estimators and tail bounds for dimension reduction in l(cid:11) (0 < (cid:11) (cid:20) 2) using stable random

KDD  San Jose  CA  2007.

projections. In SODA  pages 10 – 19  San Francisco  CA  2008.

[18] Ping Li. Improving compressed counting. In UAI  Montreal  CA  2009.
[19] Ping Li  Michael Mitzenmacher  and Anshumali Shrivastava. Coding for random projections. 2013.
[20] Ping Li  Art B Owen  and Cun-Hui Zhang. One permutation hashing. In NIPS  Lake Tahoe  NV  2012.
[21] Ping Li  Cun-Hui Zhang  and Tong Zhang. Compressed counting meets compressed sensing. 2013.
[22] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical

Computer Science  1:117–236  2 2005.

[23] Noam Nisan. Pseudorandom generators for space-bounded computations. In STOC  1990.
[24] Gennady Samorodnitsky and Murad S. Taqqu. Stable Non-Gaussian Random Processes. Chapman &

Hall  New York  1994.

[25] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver

for svm. In ICML  pages 807–814  Corvalis  Oregon  2007.

[26] Andrea Vedaldi and Andrew Zisserman. Efﬁcient additive kernels via explicit feature maps. IEEE Trans.

Pattern Anal. Mach. Intell.  34(3):480–492  2012.

[27] Sreekanth Vempati  Andrea Vedaldi  Andrew Zisserman  and C. V. Jawahar. Generalized rbf feature maps

for efﬁcient detection. In BMVC  pages 1–11  Aberystwyth  UK  2010.

[28] Gang Wang  Derek Hoiem  and David A. Forsyth. Building text features for object image classiﬁcation.

In CVPR  pages 1367–1374  Miami  Florida  2009.

[29] Jinjun Wang  Jianchao Yang  Kai Yu  Fengjun Lv  Thomas S. Huang  and Yihong Gong. Locality-
constrained linear coding for image classiﬁcation. In CVPR  pages 3360–3367  San Francisco  CA  2010.
[30] Kilian Weinberger  Anirban Dasgupta  John Langford  Alex Smola  and Josh Attenberg. Feature hashing

for large scale multitask learning. In ICML  pages 1113–1120  2009.

[31] Haiquan (Chuck) Zhao  Nan Hua  Ashwin Lall  Ping Li  Jia Wang  and Jun Xu. Towards a universal sketch
for origin-destination network measurements. In Network and Parallel Computing  pages 201–213  2011.

9

,Ping Li
Gennady Samorodnitsk
John Hopcroft
Niladri Chatterji
Peter Bartlett