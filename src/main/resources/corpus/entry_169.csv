2019,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models,Generative models produce realistic objects in many domains  including text  image  video  and audio synthesis. Most popular models—Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)—usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions—Tensor Ring Induced Prior (TRIP)—that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Fréchet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether  we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures.,A Prior of a Googol Gaussians: a Tensor Ring

Induced Prior for Generative Models

Maksim Kuznetsov1 ∗ Daniil Polykovskiy1 ∗

Dmitry Vetrov2

Alexander Zhebrak1

1Insilico Medicine 2National Research University Higher School of Economics

{kuznetsov daniil zhebrak}@insilico.com

vetrovd@yandex.ru

Abstract

Generative models produce realistic objects in many domains  including text  im-
age  video  and audio synthesis. Most popular models—Generative Adversarial
Networks (GANs) and Variational Autoencoders (VAEs)—usually employ a stan-
dard Gaussian distribution as a prior. Previous works show that the richer family
of prior distributions may help to avoid the mode collapse problem in GANs and
to improve the evidence lower bound in VAEs. We propose a new family of prior
distributions—Tensor Ring Induced Prior (TRIP)—that packs an exponential num-
ber of Gaussians into a high-dimensional lattice with a relatively small number
of parameters. We show that these priors improve Fréchet Inception Distance for
GANs and Evidence Lower Bound for VAEs. We also study generative models
with TRIP in the conditional generation setup with missing conditions. Altogether 
we propose a novel plug-and-play framework for generative models that can be
utilized in any GAN and VAE-like architectures.

1

Introduction

Modern generative models are widely applied to the generation of realistic and diverse images  text 
and audio ﬁles [1–5]. Generative Adversarial Networks (GAN) [6]  Variational Autoencoders (VAE)
[7]  and their variations are the most commonly used neural generative models. Both architectures
learn a mapping from some prior distribution p(z)—usually a standard Gaussian—to the data
distribution p(x). Previous works showed that richer prior distributions might improve the generative
models—reduce mode collapse for GANs [8  9] and obtain a tighter Evidence Lower Bound (ELBO)
for VAEs [10].
If the prior p(z) lies in a parametric family  we can learn the most suitable distribution for it during
training.
In this work  we investigate Gaussian Mixture Models as prior distributions with an
exponential number of Gaussians in nodes of a multidimensional lattice. In our experiments  we
used a prior with more than a googol (10100) Gaussians. To handle such complex distributions 
we represent p(z) using a Tensor Ring decomposition [11]—a method for approximating high-
dimensional tensors with a relatively small number of parameters. We call this family of distributions
a Tensor Ring Induced Prior (TRIP). For this distribution  we can compute marginal and conditional
probabilities and sample from them efﬁciently.
We also extend TRIP to conditional generation  where a generative model p(x | y) produces new
objects x with speciﬁed attributes y. With TRIP  we can produce new objects conditioned only on a
subset of attributes  leaving some labels unspeciﬁed during both training and inference.
Our main contributions are summarized as follows:

∗equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) 2D Tensor Ring Induced Prior.

(b) An example Tensor Ring decomposition.

Figure 1: (a) The TRIP distribution is a multidimensional Gaussian Mixture Model with an exponen-

tially large number of modes located on the lattice nodes. (b) To compute the value (cid:98)P [0  2  1]  one
should multiply the highlighted matrices and compute the trace (cid:98)P [0  2  1] = Tr(Q1[0]· Q2[2]· Q3[1]).

improves quality on sparsely labeled datasets.

use it as a prior for generative models—VAE  GAN  and its variations.

• We introduce a family of distributions that we call a Tensor Ring Induced Prior (TRIP) and
• We investigate an application of TRIP to conditional generation and show that this prior
• We evaluate TRIP models on the generation of CelebA faces for both conditional and
unconditional setups. For GANs  we show improvement in Fréchet Inception Distance (FID)
and improved ELBO for VAEs. For the conditional generation  we show lower rates of
condition violation compared to standard conditional models.

2 Tensor Ring Induced Prior

In this section  we introduce a Tensor Ring-induced distribution for both discrete and continuous
variables. We also deﬁne a Tensor Ring Induced Prior (TRIP) family of distributions.

2.1 Tensor Ring decomposition

Tensor Ring decomposition [11] represents large high-dimensional tensors (such as discrete distribu-
tions) with a relatively small number of parameters. Consider a joint distribution p(r1  r2  . . . rd) of
d discrete random variables rk taking values from {0  1  . . . Nk − 1}. We write these probabilities as
elements of a d-dimensional tensor P [r1  r2  . . . rd] = p(r1  r2  . . . rd). For the brevity of notation 
we use r1:d for (r1  . . .   rd). The number of elements in this tensor grows exponentially with the
number of dimensions d  and for only 50 binary variables the tensor contains 250 ≈ 1015 real
numbers. Tensor Ring decomposition reduces the number of parameters by approximating tensor P
with low-rank non-negative tensors cores Qk ∈ RNk×mk×mk+1
  where m1  . . .   md+1 are core sizes 
and md+1 = m1:

+

p(r1:d) ∝ (cid:98)P [r1:d] = Tr

(cid:16) d(cid:89)

(cid:17)

Qj[rj]

(1)

j=1

+

To compute P [r1:d]  for each random variable rk  we slice a tensor Qk along the ﬁrst dimension
and obtain a matrix Qk[rk] ∈ Rmk×mk+1
. We multiply these matrices for all random variables and
compute the trace of the resulting matrix to get a scalar (see Figure 1(b) for an example). In Tensor
Ring decomposition  the number of parameters grows linearly with the number of dimensions. With
larger core sizes mk  Tensor Ring decomposition can approximate more complex distributions. Note
that the order of the variables matters: Tensor Ring decomposition better captures dependencies
between closer variables than between the distant ones.
With Tensor Ring decomposition  we can compute marginal distributions without computing the

whole tensor (cid:98)P [r1:d]. To marginalize out the random variable rk  we replace cores Qk in Eq 1 with

2

matrix (cid:101)Qk =(cid:80)Nk−1

rk=0 Qk[rk]:

p(r1:k−1  rk+1:d) ∝ (cid:98)P [r1:k−1  rk+1:d] = Tr

(cid:32) k−1(cid:89)

j=1

Qj[rj] · (cid:101)Qk ·

d(cid:89)

j=k+1

(cid:33)

Qj[rj]

(2)

In Supplementary Materials  we show an Algorithm for computing marginal distributions. We
can also compute conditionals as a ratio between the joint and marginal probabilities p(A | B) =
p(A  B)/p(B); we sample from conditional or marginal distributions using the chain rule.

2.2 Continuous Distributions parameterized with Tensor Ring Decomposition

In this section  we apply the Tensor Ring decomposition to continuous distributions over vectors
z = [z1  . . .   zd]. In our Learnable Prior model  we assume that each component of zk is a Gaussian
Mixture Model with Nk fully factorized components. The joint distribution p(z) is a multidimensional
Gaussian Mixture Model with modes placed in the nodes of a multidimensional lattice (Figure 1(a)).
The latent discrete variables s1  . . .   sd indicate the index of mixture component for each dimension
(sk corresponds to the k-th dimension of the latent code zk):

(cid:88)

p(s1:d)p(z1:d | s1:d) ∝(cid:88)

(cid:98)P [s1:d]

d(cid:89)

p(z1:d) =

N (zj | µsj

j   σsj
j )

(3)

Here  p(s) is a discrete distribution of prior probabilities of mixture components  which we store as a

tensor (cid:98)P [s] in a Tensor Ring decomposition. Note that p(s) is not a factorized distribution  and the

s1:d

s1:d

j=1

(cid:110)

Q1  . . .   Qd  µ0

learnable prior p(z) may learn complex weightings of the mixture components. We call the family of
distributions parameterized in this form a Tensor Ring Induced Prior (TRIP) and denote its learnable
parameters (cores  means  and standard deviations) as ψ:
1  . . .   µNd−1

(4)
To highlight that the prior distribution is learnable  we further write it as pψ(z). As we show later  we
can optimize ψ directly using gradient descent for VAE models and REINFORCE [12] for GANs.
An important property of the proposed TRIP family is that we can derive its one-dimensional
conditional distributions in a closed form. For example  to sample using a chain rule  we need
distributions pψ(zk | z1:k−1):
pψ(zk | z1:k−1) =

pψ(sk | z1:k−1)pψ(zk | sk  z1:k−1)

1  . . .   σNd−1

Nk−1(cid:88)

ψ =

(cid:111)

.

  σ0

d

d

Nk−1(cid:88)

=

sk=0

pψ(sk | z1:k−1)pψ(zk | sk) =

Nk−1(cid:88)

pψ(sk | z1:k−1)N (zk | µsk

k   σsk
k )

(5)

sk=0

sk=0

From Equation 5 we notice that one-dimensional conditional distributions are Gaussian Mixture
Models with the same means and variances as priors  but with different weights pψ(sk | z1:k−1) (see
Supplementary Materials).
Computations for marginal probabilities in the general case are shown in Algorithm 1; conditional
probabilities can be computed as a ratio between the joint and marginal probabilities. Note that we
compute a normalizing constant on-the-ﬂy.

3 Generative Models With Tensor Ring Induced Prior

In this section  we describe how popular generative models—Variational Autoencoders (VAEs) and
Generative Adversarial Networks (GANs)—can beneﬁt from using Tensor Ring Induced Prior.

3.1 Variational Autoencoder

Variational Autoencoder (VAE) [7  13] is an autoencoder-based generative model that maps data
points x onto a latent space with a probabilistic encoder qφ(z | x) and reconstructs objects with a
probabilistic decoder pθ(x | z). We used a Gaussian encoder with the reparameterization trick:

qφ(z | x) = N (z | µφ(x)  σφ(x)) = N ( | 0  I) · σφ(x) + µφ(x).

(6)

3

Algorithm 1 Calculation of marginal probabilities in TRIP

Input: A set M of variable indices for which we compute the probability  and values of these
latent codes zi for i ∈ M
Output: Joint probability log p(zM )  where zM = {zi ∀i ∈ M}
Initialize Qbuff = I ∈ Rm1×m1  Qnorm = I ∈ Rm1×m1
for j = 1 to d do

if j is marginalized out (j /∈ M) then

Qbuff = Qbuff ·(cid:16)(cid:80)Nj−1
Qbuff = Qbuff ·(cid:16)(cid:80)Nj−1
Qnorm = Qnorm ·(cid:16)(cid:80)Nj−1

end if

else

(cid:17)
(cid:17)

k=0 Qj[k]

k=0 Qj[k] · N(cid:0)zk | µsj

j   σsj

j

(cid:1)(cid:17)

k=0 Qj[k]

end for
log p(zM ) = log Tr (Qbuff) − log Tr (Qnorm)

The most common choice for a prior distribution pψ(z) in the latent space is a standard Gaussian
distribution N (0  I). VAEs are trained by maximizing the lower bound of the log marginal likelihood
log p(x)  also known as the Evidence Lower Bound (ELBO):

L(θ  φ  ψ) = Eqφ(z|x)log pθ(x | z) − KL(cid:0)qφ(z | x) || pψ(z)(cid:1) 

(7)
where KL is a Kullback-Leibler divergence. We get an unbiased estimate of L(θ  φ  ψ) by sampling
i ∼ N (0  I) and computing a Monte Carlo estimate

l(cid:88)

i=1

(cid:18) pθ(x | zi)pψ(zi)

(cid:19)

qφ(zi | x)

L(θ  φ  ψ) ≈ 1
l

log

zi = i · σφ(x) + µφ(x)

 

(8)

When pψ(z) is a standard Gaussian  the KL term can be computed analytically  reducing the
estimation variance.
For VAEs  ﬂexible priors give tighter evidence lower bound [10  14] and can help with a problem
of the decoder ignoring the latent codes [14  15]. In this work  we parameterize the learnable prior
pψ(z) as a Tensor Ring Induced Prior model and train its parameters ψ jointly with encoder and
decoder (Figure 2). We call this model a Variational Autoencoder with Tensor Ring Induced Prior
(VAE-TRIP). We initialize the means and the variances by ﬁtting 1D Gaussian Mixture models for
each component using samples from the latent codes and initialize cores with a Gaussian noise. We
then re-initialize means  variances and cores after the ﬁrst epoch  and repeat such procedure every 5
epochs.

Figure 2: A Variational Autoencoder with a Tensor Ring Induced Prior (VAE-TRIP).

3.2 Generative Adversarial Networks

Generative Adversarial Networks (GANs) [6] consist of two networks: a generator G(z) and a
discriminator D(x). The discriminator is trying to distinguish real objects from objects produced
by a generator. The generator  on the other hand  is trying to produce objects that the discriminator
considers real. The optimization setup for all models from the GAN family is a min-max problem.
For the standard GAN  the learning procedure alternates between optimizing the generator and the

4

Figure 3: A Generative Adversarial Network with a Tensor Ring Induced Prior (GAN-TRIP).

discriminator networks with a gradient descent/ascent:

min
G ψ

max

D

LGAN = Ex∼p(x) log D(x) + Ez∼pψ(z) log

(cid:16)

1 − D(cid:0)G(z)(cid:1)(cid:17)

(9)

Similar to VAE  the prior distribution pψ(z) is usually a standard Gaussian  although Gaussian Mixture
Models were also previously studied [16]. In this work  we use a TRIP family of distributions to
parameterize a multimodal prior of GANs (Figure 3). We expect that having multiple modes as the
prior improves the overall quality of generation and helps to avoid anomalies during sampling  such
as partially present eyeglasses.
During training  we sample multiple latent codes from the prior pψ(z) and use REINFORCE [12]
to propagate the gradient through the parameters ψ. We reduce the variance by using average
discriminator output as a baseline:

l(cid:88)

i=1

∇ψ log pψ(zi)

di − 1

l

  

dj

l(cid:88)

j=1

(10)

∇ψLGAN ≈ 1
l

(cid:16)

1− D(cid:0)G(z)(cid:1)(cid:17)

is the discriminator’s output and zi are samples from the prior pψ(z).
where di = log
We call this model a Generative Adversarial Network with Tensor Ring Induced Prior (GAN-TRIP).
We initialize means uniformly in a range [−1  1] and standard deviations as 1/Nk.

4 Conditional Generation

(cid:88)

(cid:101)P [s1:d  y]

d(cid:89)

p(z  y) =

In conditional generation problem  data objects x (for example  face images) are coupled with
properties y describing the objects (for example  sex and hair color). The goal of this model is to
learn a distribution p(x | y) that produces objects with speciﬁed attributes. Some of the attributes
y for a given x may be unknown (yun)  and the model should learn solely from observed attributes
(yob): p(x | yob).
For VAE-TRIP  we train a joint model pψ(z  y) on all attributes y and latent codes z parameterized
with a Tensor Ring. For discrete conditions  the joint distribution is:

N (zd | µsd

d   σsd

d ) 

(11)

where tensor (cid:101)P [s1:d  y] is represented in a Tensor Ring decomposition. In this work  we focus on

s1:d

j=1

discrete attributes  although we can extend the model to continuous attributes with Gaussian Mixture
Models as we did for the latent codes.
With the proposed parameterization  we can marginalize out missing attributes and compute condi-
tional probabilities. We can efﬁciently compute both probabilities similar to Algorithm 1.
For conditional VAE model  the lower bound on log p(x  yob) is:

(cid:101)L(θ  φ  ψ) = Eqφ(z|x yob) log pθ(x  yob | z) − KL(cid:0)qφ(z | x  yob) || pψ(z)(cid:1).

(12)

5

Figure 4: Visualization of the ﬁrst two dimensions of the learned prior pψ(z1  z2). Left: VAE-TRIP 
Right: WGAN-GP-TRIP.

We simplify the lower bound by making two restrictions. First  we assume that the conditions y are
fully deﬁned by the object x  which implies qφ(z | x  yob) = qφ(z | x). For example  an image with
a person wearing a hat deﬁnes the presence of a hat. The second restriction is that we can reconstruct
an object directly from its latent code: pθ(x | z  yob) = pθ(x | z). This restriction also gives:

pθ(x  yob | z) = pθ(x | z  yob)pψ(yob | z) = pθ(x | z)pψ(yob | z).

(13)

The resulting Evidence Lower Bound is

(cid:101)L(θ  φ  ψ) = Eqφ(z|x)

(cid:2) log pθ(x | z) + log pψ(yob | z)(cid:3) − KL(cid:0)qφ(z | x) || pψ(z)(cid:1).

(14)
In the proposed model  an autoencoder learns to map objects onto a latent manifolds  while TRIP
prior log pψ(z | yob) ﬁnds areas on the manifold corresponding to objects with the speciﬁed attributes.
The quality of the model depends on the order of the latent codes and the conditions in pψ(z  y)  since
the Tensor Ring poorly captures dependence between variables that are far apart. In our experiments 
we found that randomly permuting latent codes and conditions gives good results.
We can train the proposed model on partially labeled datasets and use it to draw conditional samples
with partially speciﬁed constraints. For example  we can ask the model to generate images of men in
hats  not specifying hair color or the presence of glasses.

5 Related Work

The most common generative models are based on Generative Adversarial Networks [6] or Variational
Autoencoders [7]. Both GAN and VAE models usually use continuous unimodal distributions (like a
standard Gaussian) as a prior. A space of natural images  however  is multimodal: a person either
wears glasses or not—there are no intermediate states. Although generative models are ﬂexible
enough to transform unimodal distributions to multimodal  they tend to ignore some modes (mode
collapse) or produce images with artifacts (half-present glasses).
A few models with learnable prior distributions were proposed. Tomczak and Welling [10] used a
Gaussian mixture model based on encoder proposals as a prior on the latent space of VAE. Chen et al.
[14] and Rezende and Mohamed [17] applied normalizing ﬂows [18–20] to transform a standard
normal prior into a more complex latent distribution. [14  15] applied auto-regressive models to learn
better prior distribution over the latent variables. [21] proposed to update a prior distribution of a
trained VAE to avoid samples that have low marginal posterior  but high prior probability.
Similar to Tensor Ring decomposition  a Tensor-Train decomposition [22] is used in machine learning
and numerical methods to represent tensors with a small number of parameters. Tensor-Train was
applied to the compression of fully connected [23]  convolutional [24] and recurrent [25] layers. In
our models  we can use a Tensor-Train decomposition instead of Tensor Ring  but it requires larger
cores to achieve comparable results  as ﬁrst and last dimensions are farther apart.
Most conditional models work with missing values by imputing them with a predictive model or
setting them to a special value. With this approach  we cannot sample objects specifying conditions
partially. VAE TELBO model [26] proposes to train a Product of Experts-based model  where the
pψ(z | yi)  requiring to train

posterior on the latent codes is approximated as pψ(z | yob) =(cid:81)

yi∈yob

6

432101234z1432101234z21.51.00.50.00.51.01.5z11.51.00.50.00.51.01.5z2a separate posterior model for each condition. JMVAE model [27] contains three encoders that take
both image and condition  only a condition  or only an image.

6 Experiments

We conducted experiments on CelebFaces Attributes Dataset (CelebA) [28] of approximately 400 000
photos with a random train-test split. For conditional generation  we selected 14 binary image
attributes  including sex  hair color  presence mustache  and beard. We compared both GAN and
VAE models with and without TRIP. We also compared our best model with known approaches on
CIFAR-10 [29] dataset with a standard split. Model architecture and training details are provided in
Supplementary Materials.

6.1 Generating Objects With VAE-TRIP and GAN-TRIP

Table 1: FID for GAN and VAE-based architectures trained on CelebA dataset  and ELBO for VAE.
F = Fixed  L = Learnable. We also report ELBO for importance-weighted autoencoder with k = 100
points [30]

METRIC

MODEL

FID

VAE
WGAN
WGAN-GP

ELBO
IWAE ELBO (k = 100)

VAE

N (0  I)

86.72
63.46
54.71
-194.16
-185.09

GMM

TRIP

F

85.64
67.10
57.82
-201.60
-191.99

L

84.48
61.82
62.10
-193.88
-184.73

F

85.31
62.48
63.06
-202.04
-190.09

L

83.54
57.6
52.86
-193.32
-184.43

We evaluate GAN-based models with and without Tensor Ring Learnable Prior by measuring a
Fréchet Inception Distance (FID). For the baseline models  we used Wasserstein GAN (WGAN)
[31] and Wasserstein GAN with Gradient Penalty (WGAN-GP) [32] on CelebA dataset. We also
compared learnable priors with ﬁxed randomly initialized parameters ψ. The results in Table 1
(CelebA) and Table 2 (CIFAR-10) suggest that with a TRIP prior the quality improves compared
to standard models and models with GMM priors. In some experiments  the GMM-based model
performed worse than a standard Gaussian  since KL had to be estimated with Monte-Carlo sampling 
resulting in higher gradient variance.

Table 2: FID for CIFAR-10 GAN-based models

Model
SN-GANs [33]
WGAN-GP + Two Time-Scale [34]
WGAN-GP [32]
WGAN-GP-TRIP (ours)

FID
21.7
24.8
29.3
16.72

6.2 Visualization of TRIP

In Figure 4  we visualize ﬁrst two dimensions of the learned prior pψ(z1  z2) in VAE-TRIP and
WGAN-GP-TRIP models. For both models  prior uses most of the components to produce a complex
distribution. Also  notice that the components learned different non-uniform weights.

6.3 Generated Images

Here  we visualize the correspondence of modes and generated images by a procedure that we call
mode hopping. We start by randomly sampling a latent code and producing the ﬁrst image. After that 
we randomly select ﬁve dimensions and sample them conditioned on the remaining dimensions. We

7

Figure 5: Mode hopping in WGAN-GP-TRIP. We start with a random sample from the prior and
conditionally sample ﬁve random dimensions on each iteration. Each line shows a single trajectory.

repeat this procedure multiple times and obtain a sequence of sampled images shown in Figure 5.
With these results  we see that similar images are localized in the learned prior space  and changes in
a few dimensions change only a few ﬁne-grained features.

6.4 Generated Conditional Images

In this experiment  we generate images given a subset of attributes to estimate the diversity of
generated images. For example  if we specify ‘Young man ’ we would expect different images to
have different hair colors  presence and absence of glasses or hat. Generated images shown in Figure
3 indicate that the model learned to produce diverse images with multiple varying attributes.

Table 3: Generated images with VAE-TRIP for different attributes.

Young man

Smiling woman in eyeglasses

Smiling woman with a hat

Blond woman with eyeglasses

7 Discussion

We designed our prior utilizing Tensor Ring decomposition due to its higher representation capacity
compared to other decompositions. For example  a Tensor Ring with core size m has the same
capacity as a Tensor-Train with core size m2 [35]. Although the prior contains an exponential
number of modes  in our experiments  its learnable parameters accounted for less than 10% of total
weights  which did not cause overﬁtting. The results can be improved by increasing the core size m;
however  the computational complexity has a cubic growth with the core size. We also implemented
a conditional GAN but found the REINFORCE-based training of this model very unstable. Further
research with variance reduction techniques might improve this approach.

8

8 Acknowledgements

Image generation for Section 6.3 was supported by the Russian Science Foundation grant no. 17-71-
20072.

References
[1] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive Growing of GANs for
Improved Quality  Stability  and Variation. International Conference on Learning Representa-
tions  2018.

[2] Wei Ping  Kainan Peng  Andrew Gibiansky  Sercan Ömer Arik  Ajay Kannan  Sharan Narang 
Jonathan Raiman  and John Miller. Deep Voice 3: Scaling Text-to-Speech with Convolutional
Sequence Learning. International Conference on Learning Representations  2018.

[3] Aaron van den Oord  Yazhe Li  Igor Babuschkin  Karen Simonyan  Oriol Vinyals  Koray
Kavukcuoglu  George van den Driessche  Edward Lockhart  Luis Cobo  Florian Stimberg  Nor-
man Casagrande  Dominik Grewe  Seb Noury  Sander Dieleman  Erich Elsen  Nal Kalchbrenner 
Heiga Zen  Alex Graves  Helen King  Tom Walters  Dan Belov  and Demis Hassabis. Parallel
WaveNet: Fast High-Fidelity Speech Synthesis. 2018.

[4] Daniil Polykovskiy  Alexander Zhebrak  Dmitry Vetrov  Yan Ivanenkov  Vladimir Aladinskiy 
Polina Mamoshina  Marine Bozdaganyan  Alexander Aliper  Alex Zhavoronkov  and Artur
Kadurin. Entangled conditional adversarial autoencoder for de novo drug discovery. Mol.
Pharm.  September 2018.

[5] Alex Zhavoronkov  Yan A Ivanenkov  Alex Aliper  Mark S Veselov  Vladimir A Aladinskiy 
Anastasiya V Aladinskaya  Victor A Terentiev  Daniil A Polykovskiy  Maksim D Kuznetsov 
Arip Asadulaev  et al. Deep learning enables rapid identiﬁcation of potent ddr1 kinase inhibitors.
Nature biotechnology  pages 1–4  2019.

[6] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative Adversarial Nets. Advances in neural
information processing systems  pages 2672–2680  2014.

[7] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes.

Conference on Learning Representations  2013.

International

[8] Matan Ben-Yosef and Daphna Weinshall. Gaussian Mixture Generative Adversarial Net-
works for Diverse Datasets  and the Unsupervised Clustering of Images. arXiv preprint
arXiv:1808.10356  2018.

[9] Lili Pan  Shen Cheng  Jian Liu  Yazhou Ren  and Zenglin Xu. Latent dirichlet allocation in

generative adversarial networks. arXiv preprint arXiv:1812.06571  2018.

[10] Jakub M Tomczak and Max Welling. VAE with a VampPrior. International Conference on

Artiﬁcial Intelligence and Statistics  2018.

[11] Qibin Zhao  Guoxu Zhou  Shengli Xie  Liqing Zhang  and Andrzej Cichocki. Tensor Ring

Decomposition. arXiv preprint arXiv:1606.05535  2016.

[12] Ronald J Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Rein-

forcement Learning. Machine learning  8(3-4):229–256  1992.

[13] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic Backpropagation
and Approximate Inference in Deep Generative Models. International Conference on Machine
Learning  2014.

[14] Xi Chen  Diederik P Kingma  Tim Salimans  Yan Duan  Prafulla Dhariwal  John Schulman 
Ilya Sutskever  and Pieter Abbeel. Variational Lossy Autoencoder. International Conference on
Learning Representations  2017.

9

[15] Aaron van den Oord  Oriol Vinyals  and koray kavukcuoglu. Neural Discrete Representation

Learning. Advances in Neural Information Processing Systems  2017.

[16] Swaminathan Gurumurthy  Ravi Kiran Sarvadevabhatla  and R Venkatesh Babu. DeLiGAN:
Generative Adversarial Networks for Diverse and Limited Data. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 166–174  2017.

[17] Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows.

International Conference on Machine Learning  2015.

[18] Laurent Dinh  David Krueger  and Yoshua Bengio. NICE: Non-linear Independent Components

Estimation. International Conference on Learning Representations Workshop  2015.

[19] Diederik P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max
Welling. Improved Variational Inference with Inverse Autoregressive Flow. In Advances in
Neural Information Processing Systems  pages 4743–4751  2016.

[20] Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density Estimation Using Real NVP.

International Conference on Learning Representations  2017.

[21] Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. 89:66–75 

16–18 Apr 2019. URL http://proceedings.mlr.press/v89/bauer19a.html.

[22] Ivan V Oseledets. Tensor-Train Decomposition. SIAM Journal on Scientiﬁc Computing  33(5):

2295–2317  2011.

[23] Alexander Novikov  Dmitry Podoprikhin  Anton Osokin  and Dmitry Vetrov. Tensorizing

Neural Networks. Advances in Neural Information Processing Systems  2015.

[24] Timur Garipov  Dmitry Podoprikhin  Alexander Novikov  and Dmitry Vetrov. Ultimate Ten-
sorization: Compressing Convolutional and FC Layers Alike. Advances in Neural Information
Processing Systems  2016.

[25] Andros Tjandra  Sakriani Sakti  and Satoshi Nakamura. Compressing Recurrent Neural Network

with Tensor Train. International Joint Conference on Neural Networks  2017.

[26] Ramakrishna Vedantam  Ian Fischer  Jonathan Huang  and Kevin Murphy. Generative Models
of Visually Grounded Imagination. International Conference on Learning Representations 
2018.

[27] Masahiro Suzuki  Kotaro Nakayama  and Yutaka Matsuo. Joint Multimodal Learning with Deep
Generative Models. International Conference on Learning Representations Workshop  2017.

[28] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep Learning Face Attributes in the

Wild. In Proceedings of International Conference on Computer Vision (ICCV)  12 2015.

[29] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. Cifar-10 (canadian institute for advanced

research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

[30] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Importance weighted autoencoders.

arXiv preprint arXiv:1509.00519  2015.

[31] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein Generative Adversarial

Networks. International Conference on Machine Learning  2017.

[32] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved Training of Wasserstein Gans. In Advances in Neural Information Processing Systems 
pages 5767–5777  2017.

[33] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. International Conference on Learning Representations 
2018.

10

[34] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems  pages 6629–6640  2017.

[35] Yoav Levine  David Yakira  Nadav Cohen  and Amnon Shashua. Deep learning and quantum
entanglement: Fundamental connections with implications to network design. International
Conference on Learning Representations  2018.

11

,Yuchen Pu
Weiyao Wang
Ricardo Henao
Liqun Chen
Zhe Gan
Chunyuan Li
Lawrence Carin
Maxim Kuznetsov
Daniil Polykovskiy
Dmitry Vetrov
Alex Zhebrak