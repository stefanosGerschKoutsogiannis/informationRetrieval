2011,Efficient Offline Communication Policies for Factored Multiagent POMDPs,Factored Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) form a powerful framework for multiagent planning under uncertainty  but optimal solutions require a rigid history-based policy representation. In this paper we allow inter-agent communication which turns the problem in a centralized Multiagent POMDP (MPOMDP). We map belief distributions over state factors to an agent's local actions by exploiting structure in the joint MPOMDP policy.  The key point is that when sparse dependencies between the agents' decisions exist  often the belief over its local state factors is sufficient for an agent to unequivocally identify the optimal action  and communication can be avoided. We formalize these notions by casting the problem into convex optimization form  and present experimental results illustrating the savings in communication that we can obtain.,Efﬁcient Ofﬂine Communication Policies for

Factored Multiagent POMDPs

Jo˜ao V. Messias

Institute for Systems and Robotics

Instituto Superior T´ecnico

Lisbon  Portugal

Matthijs T.J. Spaan

Delft University of Technology

Delft  The Netherlands

m.t.j.spaan@tudelft.nl

jmessias@isr.ist.utl.pt

Pedro U. Lima

Institute for Systems and Robotics

Instituto Superior T´ecnico

Lisbon  Portugal

pal@isr.ist.utl.pt

Abstract

Factored Decentralized Partially Observable Markov Decision Processes (Dec-
POMDPs) form a powerful framework for multiagent planning under uncertainty 
but optimal solutions require a rigid history-based policy representation. In this
paper we allow inter-agent communication which turns the problem in a central-
ized Multiagent POMDP (MPOMDP). We map belief distributions over state fac-
tors to an agent’s local actions by exploiting structure in the joint MPOMDP pol-
icy. The key point is that when sparse dependencies between the agents’ decisions
exist  often the belief over its local state factors is sufﬁcient for an agent to un-
equivocally identify the optimal action  and communication can be avoided. We
formalize these notions by casting the problem into convex optimization form  and
present experimental results illustrating the savings in communication that we can
obtain.

1 Introduction

Intelligent decision making in real-world scenarios requires an agent to take into account its limita-
tions in sensing and actuation. These limitations lead to uncertainty about the state of environment 
as well as how the environment will respond to performing a certain action. When multiple agents
interact and cooperate in the same environment  the optimal decision-making problem is particularly
challenging. For an agent in isolation  planning under uncertainty has been studied using decision-
theoretic models like Partially Observable Markov Decision Processes (POMDPs) [4]. Our focus
is on multiagent techniques  building on the factored Multiagent POMDP model. In this paper  we
propose a novel method that exploits sparse dependencies in such a model in order to reduce the
amount of inter-agent communication.

The major source of intractability for optimal Dec-POMDP solvers is that they typically reason over
all possible histories of observations other agents can receive. In this work  we consider factored
Dec-POMDPs in which communication between agents is possible  which has already been explored
for non-factored models [10  11  15  13] as well as for factored Dec-MDPs [12]. When agents share
their observations at each time step  the decentralized problem reduces to a centralized one  known
as a Multiagent POMDP (MPOMDP) [10]. In this work  we develop individual policies which map
beliefs over state factors to actions or communication decisions.

1

Maintaining an exact  factorized belief state is typically not possible in cooperative problems. While
bounded approximations are possible for probabilistic inference [2]  these results do not carry over
directly to decision-making settings (but see [5]). Intuitively  even a small difference in belief can
lead to a different action being taken. However  when sparse dependencies between the agents’
decisions exist  often the belief over its local state factors is sufﬁcient for an agent to identify the
action that it should take  and communication can be avoided. We formalize these notions as con-
vex optimization problems  extracting those situations in which communication is superﬂuous. We
present experimental results showing the savings in communication that we can obtain  and the
overall impact on decision quality.

The rest of the paper is organized as follows. First  Section 2 presents the necessary background
material. Section 3 presents the formalization of our method to associate belief points over state
factors to actions. Next  Section 4 illustrates the concepts with experimental results  and Section 5
provides conclusions and discusses future work.

2 Background

In this section we provide background on factored Dec-POMDPs and Multiagent POMDPs.

A factored Dec-POMDP is deﬁned as the following tuple [8]:

D = {1  ...  n} is the set of agents. Di will be used to refer to agent i;
S = ×iXi  i = 1  . . .   nf is the state space  decomposable into nf factors Xi ∈ {1  ...  mi} which
lie inside a ﬁnite range of integer values. X = {X1  . . .   Xnf } is the set of all state factors;
A = ×iAi  i = 1  ...  n is the joint action space. At each step  every agent i takes an individual

action ai ∈ Ai  resulting in the joint action a = ha1  ...  ani ∈ A;

O = ×iOi  i = 1  ...  n is the space of joint observations o = ho1  ...  oni  where oi ∈ Oi are the

individual observations. An agent receives only its own observation;

T : S × S × A → [0  1] speciﬁes the transition probabilities Pr (s′|s  a);
O : O × S × A → [0  1] speciﬁes the joint observation probabilities Pr (o|s′  a);
R : S × A → R speciﬁes the reward for performing action a ∈ A in state s ∈ S;
b0 ∈ B is the initial state distribution. The set B is the space of all possible distributions over S;
h is the planning horizon.

The main advantage of factored (Dec-)POMDP models over their standard formulation lies in their
more efﬁcient representation. Existing methods for factored Dec-POMDPs can partition the decision
problem across local subsets of agents  due to the possible independence between their actions and
observations [8]. A natural state-space decomposition is to perform an agent-wise factorization  in
which a state in the environment corresponds to a unique assignment over the states of individual
agents. Note that this does not preclude the existence of state factors which are common to multiple
agents.

The possibility of exchanging information between agents greatly inﬂuences the overall complexity
of solving a Dec-POMDP. In a fully communicative Dec-POMDP  the decentralized model can be
reduced to a centralized one  the so-called Multiagent POMDP (MPOMDP) [10]. An MPOMDP is
a regular single-agent POMDP but deﬁned over the joint models of all agents. In a Dec-POMDP 
at each t an agent i knows only ai and oi  while in an MPOMDP  it is assumed to know a and o.
In the latter case  inter-agent communication is necessary to share the local observations. Solving
an MPOMDP is of a lower complexity class than solving a Dec-POMDP (PSPACE-Complete vs.
NEXP-Complete) [1].

It is well-known that  for a given decision step t  the value function V t of a POMDP is a piecewise
linear  convex function [4]  which can be represented as

V t(bt) = max
α∈Γt

αT · bt

 

(1)

where Γt is a set of vectors (traditionally referred to as α-vectors). Every α ∈ Γt has a particular
joint action a associated to it  which we will denote as φ(α). The transpose operator is here denoted
as (·)T. In this work  we assume that a value function is given for the Multiagent POMDP. However 

2

this value function need not be optimal  nor stationary. Our techniques preserve the quality of the
supplied value function  even if it is an approximation.

A joint belief state is a probability distribution over the set of states S  and encodes all of the
information gathered by all agents in the Dec-POMDP up to a given time t:

bt(s) = Pr(st|ot−1  at−1  ot−2  at−2  . . .   o1  a1  b0)

= Pr(X t

1   . . .   X t

nf |·)

(2)

A factored belief state is a representation of this very same joint belief as the product of nF assumed
independent belief states over the state factors Xi  which we will refer to as belief factors:

Every factor bt
Fi

is deﬁned over a subset Fi ⊆ X of state factors  so that:

bt = ×nF

i=1bt
Fi

bt(s) ≃ Pr(F t

1|·)Pr(F t

2|·) · · · Pr(F t

nF |·)

(3)

(4)

With Fi ∩ Fj = ∅   ∀i 6= j. A belief point over factors L which are locally available to the agent
will be denoted bL.
The marginalization of b onto bF is:

F (F t) = Pr(cid:0)F t|a1 ···  t−1  o1 ···  t−1(cid:1)
bt
2   · · ·   X t

1   X t

Pr(cid:16)X t

= X

X t\F t

nf |·(cid:17) = X

X t\F t

which can be viewed as a projection of b onto the smaller subspace BF :

bF = M X
F b

bt(st) 

(5)

(6)

F is a matrix where M X

F (u  v) = 1 if the assignments to all state factors contained in
where M X
state u ∈ F are the same as in state v ∈ X   and 0 otherwise. This intuitively carries out the
marginalization of points in B onto BF .

3 Exploiting Sparse Dependencies in Multiagent POMDPs

In the implementation of Multiagent POMDPs  an important practical issue is raised: since the joint
policy arising from the value function maps joint beliefs to joint actions  all agents must maintain
and update the joint belief equivalently for their decisions to remain consistent. The amount of
communication required to make this possible can then become problematically large. Here  we
will deal with a fully-communicative team of agents  but we will be interested in minimizing the
necessary amount of communication. Even if agents can communicate with each other freely  they
might not need to always do so in order to act independently  or even cooperatively.

The problem of when and what to communicate has been studied before for Dec-MDPs [12]  where
factors can be directly observed with no associated uncertainty  by reasoning over the possible local
alternative actions to a particular assignment of observable state features. For MPOMDPs  this
had been approximated at runtime  but implied keeping track and reasoning over a rapidly-growing
number of possible joint belief points [11].

We will describe a method to map a belief factor (or several factors) directly to a local action  or to a
communication decision  when applicable. Our approach is the ﬁrst to exploit  ofﬂine  the structure
of the value function itself in order to identify regions of belief space where an agent may act inde-
pendently. This raises the possibility of developing more ﬂexible forms for joint policies which can
be efﬁciently decoupled whenever this is advantageous in terms of communication. Furthermore 
since our method runs ofﬂine  it is not mutually exclusive with online communication-reduction
techniques: it can be used as a basis for further computations at runtime  thereby increasing their
efﬁciency.

3

3.1 Decision-making with factored beliefs

Note that  as fully described in [2]  the factorization (4) typically results in an approximation of the
true joint belief  since it is seldom possible to decouple the dynamics of a MDP into strictly inde-
pendent subprocesses. The dependencies between factors  induced by the transition and observation
model of the joint process  quickly develop correlations when the horizon of the decision problem
is increased  even if these dependencies are sparse. Still  it was proven in [2] that  if some of these
dependencies are broken  the resulting error (measured as the KL-divergence) of the factored belief
state  with respect to the true joint belief  is bounded. Unfortunately  even a small error in the belief
state can lead to different actions being selected  which may signiﬁcantly affect the decision quality
of the multiagent team in some settings [5  9]. However  in rapidly-mixing processes (i.e.  models
with transition functions which quickly propagate uncertainty)  the overall negative effect of using
this approximation is minimized.

Each belief factor’s dynamics can be described using a two-stage Dynamic Bayesian Network
(DBN). For an agent to maintain  at each time step  a set of belief factors  it must have access
to the state factors contained in a particular time slice of the respective DBNs. This can be ac-
complished either through direct observation  when possible  or by requesting this information from
other agents. In the latter case  it may be necessary to perform additional communication in order
to keep belief factors consistent. The amount of data to be communicated in this case  as well as its
frequency  depends largely on the factorization scheme which is selected for a particular problem.
We will not be here concerned with the problem of obtaining a suitable partition scheme of the joint
belief onto its factors. Such a partitioning is typically simple to identify for multi-agent teams which
exhibit sparsity of interaction. Instead we will focus on the amount of communication which is
necessary for the joint decision-making of the multi-agent team.

3.2 Formal model

We will hereafter focus on the value function  and its associated quantities  at a given decision step t 
and  for simplicity  we shall omit this dependency. However  we restate that the value function does
not need to be stationary – for a ﬁnite-horizon problem  the following methods can simply be applied
for every t = 1  . . .   h.

3.2.1 Value Bounds Over Local Belief Space

Recall that  for a given α-vector  Vα(b) = α · b represents the expected reward for selecting the
action associated with α. Ideally  if this quantity could be mapped from a local belief point bL  then
it would be possible to select the best action for an agent based only on its local information. This
is typically not possible since the projection (6) is non-invertible. However  as we will show  it is
possible to obtain bounds on the achievable value of any given vector  in local belief space.
The available information regarding Vα(b) in local space can be expressed in the linear forms:

Vα(b) = α · b

1T

nb = 1
L b = bL

M X

(7)

T

Vα(bL) = (cid:0)β + γ(cid:1) · bL + δ

.

4

. . . 1 ]

where 1n = [ 1 1
∈ Rn. Let m be size of the local belief factor which contains bL.
Reducing this system  we can associate Vα(b) with b and bL  having at least n − m free variables in
the leading row  induced by the locally unavailable dimensions of b. The resulting equation can be
rewritten as:

Vα(b) = β · b + γ · bL + δ

(8)
with β ∈ Rn  γ ∈ Rm and δ ∈ R. By maximizing (or minimizing) the terms associated with the
potentially free variables  we can use this form to establish the maximum (and minimum) value that
can be attained at bL.
Theorem 1. Let Iu = (cid:8)v : M X
L (u  v) = 1(cid:9)  β ∈ Rm : βi = maxj∈Ii βj  i = 1  . . .   m and
β ∈ Rm : βi = minj∈Ii βj  i = 1  . . .   m. The maximum achievable value for a local belief
point  bL  according to α  is:

 

(9)

Analogously  the minimum achievable value is

Vα(bL) = (cid:0)β + γ(cid:1) · bL + δ

 

(10)

Proof. First  we shall establish that Vα(bL) is an upper bound on Vα(b). The set Ii contains the
indexes of the elements of b which marginalize onto (bL)i. From the deﬁnition of β it follows that 
∀b ∈ B:

X

j∈Ii

βibj ≥ X

j∈Ii

βjbj

  i = 1  . . .   m ⇔

⇔ βi(bL)i ≥ X

j∈Ii

βjbj

  i = 1  . . .   m  

where we used the fact that Pj∈Ii
Using (8) and (9) 

bj = (bL)i. Summing over all i  this implies that β · bL ≥ β · b.

β · bL + γ · bL + δ ≥ β · b + γ · bL + δ ⇔ Vα(bL) ≥ Vα(b)

Next  we need to show that ∃b ∈ B : Vα(bL) = Vα(b). Since 1T
convex combination of the elements in β. Consequently 
β · M X

L b = max

βi

max
b∈B

β · b = max
b∈B

i

nb = 1 and bi ≥ 0 ∀i  β · b is a

Therefore  for bm = arg max
b∈B

β · b  we have that Vα(M X

L bm) = Vα(bm).

The proof for the minimum achievable value Vα(bL) is analogous.

By obtaining the bounds (9) and (10)  we have taken a step towards identifying the correct action
for an agent to take  based on the local information contained in bL. From their evaluation  the
following remarks can be made: if α and α′ are such that Vα′ (bL) ≤ Vα(bL)  then α′ is surely
not the maximizing vector at b; if this property holds for all α′ such that (φ(α′))i 6= (φ(α))i  then
by following the action associated with α  agent i will accrue at least as much value as with any
other vector for all possible b subject to (6). That action can be safely selected without needing to
communicate.

The complexity of obtaining the local value bounds for a given value function is basically that of
reducing the system (7) for each vector. This is typically achieved through Gaussian Elimination 
with an associated complexity of O(n(m + 2)2) [3]. Note that the dominant term corresponds to
the size of the local belief factor  which is usually exponentially smaller than n. This is repeated
for all vectors  and if pruning is then done over the resulting set (the respective cost is O(|Γ|2))  the
total complexity is O(|Γ|n(m + 2)2 + |Γ|2). The pruning process used here is the same as what is
typically done by POMDP solvers [14].

3.2.2 Dealing With Locally Ambiguous Actions

The deﬁnition of the value bounds (9) and (10) only allows an agent to act in atypical situations in
which an action is clearly dominant in terms of expected value. However  this is often not the case 
particularly when considering a large decision horizon  since the present effects of any given action
on the overall expected reward are typically not pronounced enough for these considerations to be
practical. In a situation where multiple value bounds are conﬂicting (i.e. Vα(bL) > Vα′ (bL) and
Vα(bL) < Vα′ (bL))  an agent is forced to further reason about which of those actions is best.
In order to tackle this problem  let us assume that two actions a and a′ have conﬂicting bounds at
bL. Given Γa = {α ∈ Γ : (φ(α))i = a} and similarly deﬁned Γa′  we will deﬁne the matrices
|. Then  the vectors v = Ab
A = [Γa
and v′ = A′b (in Rk and Rk′ respectively) contain all possible values attainable at b through the
vectors in Γa and Γa′. Naturally  we will be interested in the maximum of these values for each
action. In particular  we want to determine if maxi vi is greater than maxj v′
j for all possible b such
that bL = M X
L b. If this is the case  then a should be selected as the best action  since it is guaranteed
to provide a higher value at bL than a′.

i = 1  . . .   |Γa| and A′ = [Γa′

i = 1  . . .   |Γa′

i ]k′×n 

i ]k×n 

5

The problem of determining the minimum value of v − v′ at bL can be expressed as the following
set of Linear Programs (LPs) [6]. Note that x (cid:23) y is here assumed to mean that xi ≥ yi ∀i:

∀i = 1  . . .   |Γa′

| maximize Γa′

i b − s
subject to Ab (cid:22) 1ks

b (cid:23) 0n

(11)

M X

L b = bL 1T

n b = 1

If the solution bopt to each of these LPs is such that maxi(Abopt)i ≥ maxj(A′bopt)j  then action a
can be safely selected based on bL. If this is not the case for any of the solutions  then it is not
possible to map the agent’s best action solely through bL. In order to disambiguate every possible
action  this optimization needs to be carried out for all conﬂicting pairs of actions. However  a less
computationally expensive alternative is to approximate the optimization (11) by a single LP (refer
to [6] for more details):

maximize 1T
subject to Ab (cid:22) 1ks

k′ ξ

A′b = 1k′ s + ξ 1T

b (cid:23) 0n M X
n b = 1

L b = bL

(12)

3.2.3 Mapping Local Belief Points to Communication Decisions

For an environment with only two belief factors  the method described so far could already incor-
porate an explicit communication policy: given the local belief bL of an agent  if it is possible to
unequivocally identify any action as being maximal  then that action can be safely executed without
any loss of expected value. Otherwise  the remaining belief factor should be requested from other
agents  in order to reconstruct b through (4)  and map that agent’s action through the joint policy.
However  in most scenarios  it is not sufﬁcient to know whether or not to communicate: equally
important are the issues of what to communicate  and with whom.
Let us consider the general problem with nF belief factors contained in the set F. In this case there
are 2|F |−1 combinations of non-local factors which the agent can request. Our goal is to identify one
such combination which contains enough information to disambiguate the agent’s actions. Central
to this process is the ability to quickly determine  for a given set of belief factors G ⊆ F  if there are
no points in bG with non-decidable actions. The exact solution to this problem would require  in the
worst case  the solution of |Γa| × |Γa′
| LPs of the form (11) for every pair of actions with conﬂicting
value bounds. However  a modiﬁcation of the approximate LP (12) allows us to tackle this problem
efﬁciently:

maximize 1T
k′ ξ′ + 1T
kξ
subject to Ab (cid:22) 1ks

A′b′ (cid:22) 1k′ s′
b (cid:23) 0n

A′b = 1k′ s + ξ
Ab′ = 1ks′ + ξ′
b′ (cid:23) 0n

M X
M X
M X

L b = bL
L b′ = bL
G b = M X

G b′

(13)

The rationale behind this formulation is that any solution to the LP  in which maxi ξi > 0 and
j > 0 simultaneously  identiﬁes two different points b and b′ which map to the same point
maxj ξ′
bG in G  but share different maximizing actions a′ and a respectively. This implies that  in order to
select an action unambiguously from the belief over G  no such solution may be possible.
Equipped with this result  we can now formulate a general procedure that  for a set of belief points
in local space  returns the corresponding belief factors which must be communicated in order for an
agent to act unambiguously. We refer to this as obtaining the communication map for the problem.
This procedure is as follows (a more detailed version is included in [6]): we begin by computing the
value bounds of V over local factors L  and sampling N reachable local belief points bL; for each
of these points  if the value bounds of the best action are not conﬂicting (see Section 3.2.1)  or any
conﬂicting bounds are resolved by LP (12)  we can mark bL as safe  add it to the communication
map  and continue on to the next point; otherwise  using LP (13)  we search for the minimum set of
non-local factors G which resolves all conﬂicts; we then associate bL with G and add it to the map.
During execution  an agent updates its local information bL  ﬁnds the nearest neighbor point in the
communication map  and requests the corresponding factors from the other agents. The agent then
selects the action which exhibits the highest maximum value bound given the resulting information.

6

L1

L2

D
1

R1

D

2

R2

(a) Relay-Small.

(b) Relay-Large.

)
1
)
1
X
b
(
(
p
a
M

1

0

0

0.2

0.4

0.6

0.8

1

(bX1)1

(c) Communication Map.

Figure 1: (a) Layout of the Relay-Small problem.
Communication map for the Relay-Small problem.

(b) Layout of the Relay-Large problem.

(c)

4 Experiments

We now analyze the results of applying the aforementioned ofﬂine communication mapping process
to three different MPOMDP environments  each with a different degrees of interdependency between
agents. The ﬁrst and smallest of the test problems  shown in Figure 1a  is named the Relay-Small
problem  and is mainly used for explanatory purposes. In this world each agent is conﬁned to a
two-state area. One of the agents possesses a package which it must hand over to the other agent 
through the non-traversable opening between the rooms L1 and R1. Each agent can move randomly
inside its own room (a Shufﬂe action)  Exchange the package with the other agent  or Sense its
environment in order to ﬁnd the opening. An Exchange is only successful if both agents are in
the correct position (L1  R1) and if both agents perform this action at the same time  which makes
it the only available cooperative action. The fact that  in this problem  each belief factor is two-
dimensional (each factor spans one of the rooms) allows us to visualize the results of our method. In
Figure 2  we see that some of the agent’s expected behavior is already contained in the value bounds
over its local factor: if an agent is certain of being in room R1 (i.e. (bX1)1 = 0)  then the action
with the highest-valued bound is Shufﬂe. Likewise  an Exchange should only be carried out when
the agent is certain of being in L1  but it is an ambiguous action since the agent needs to be sure that
its teammate can cooperate. In Figure 1c we represent the communication map which was obtained
ofﬂine through the proposed algorithm. Since there are only two factors  the agent only needs to
make a binary decision of whether or not to communicate for a given local belief point. The belief
points considered safe are marked as 0  and those associated with a communication decision are
marked as 1. In terms of quantitative results  we see that ∼ 30 − 40% of communication episodes
are avoided in this simple example  without a signiﬁcant loss of collected reward.

Another test scenario is the OneDoor environment of [7]  which is further described in [6]. In this
49-state world  two agents lie inside opposite rooms  akin to the Relay-Small problem  but each
agent has the goal of moving to the other room. There is only one common passage between both
rooms  where the agents may collide. For shorter-horizon solutions  agents may not be able to reach
their goal  and they communicate so as to minimize negative reward (collisions). For the inﬁnite-
horizon case  however  typically only one of the agents communicates  while waiting for its partner
to clear the passage. Note that this relationship between the problem’s horizon and the amount of
communication savings does not hold for all of the problems. The proposed method exploits the
invariance of local policies over subsets of the joint belief space  and this may arbitrarily change
with the problem’s horizon.

A larger example is displayed in Figure 1b. This is an adaptation of the Relay-Small problem (aptly
named Relay-Large) to a setting in which each room has four different states  and each agent may be
carrying a package at a given time. Agent D1 may retrieve new packages from position L1  and D2

Relay-Small

OneDoor

Relay-Large

Red. Comm.

Full Comm.

h.
Red. Comm.
Full Comm.
Full Comm.
15.4  100% 14.8  56.9% 0.35  100% 0.30  89.0% 27.4  100%
25.8  44.1%
6
39.8  100% 38.7  68.2% 1.47  100% 1.38  76.2% -19.7  100% -21.6  62 5%
10
∞ 77.5  100% 73.9  46.1% 2.31  100% 2.02  61.3% 134.0  100% 129.7  58.9%

Red. Comm.

Table 1: Results of the proposed method for various environments. For settings assuming full and
reduced communication  we show empirical control quality  online communication usage.

7

h

Perseus

Comm. Map

Relay-Small
6
1.1
5.9

10 ∞ 6
7.3
4.3
21.4
12.4

0.1
7.4

OneDoor

Relay-Large

10 ∞
5.3
33.3
57.7
5.9

6

239.5
368.7

10

643.0
859.5

∞
31.5
138.1

Table 2: Running time (in seconds) of the proposed method in comparison to the Perseus point-based
POMDP solver.

V

160

140

120

100

80

60

40

20

Value Bounds (Relay)

Pruned Value Bounds (Relay)

Shuffle

Exchange

Sense

V

160

140

120

100

80

60

40

20

0

0.2

0.4
0.6
(bX1)1

0.8

1

0

0.2

0.4
0.6
(bX1)1

0.8

1

Figure 2: Value bounds for the Relay-Small problem. The dashed lines indicate the minimum value
bounds  and the ﬁlled lines represent the maximum value bounds  for each action.

can deliver them to L2  receiving for that a positive reward. There are a total of 64 possible states for
the environment. Here  since the agents can act independently for a longer time  the communication
savings are more pronounced  as shown in Table 1.

Finally  we argue that the running time of the proposed algorithm is comparable to that of general
POMDP solvers for these same environments. Even though both the solver and the mapper algo-
rithms must be executed in sequence  the results in Table 2 show that they are typically both in the
same order of magnitude.

5 Conclusions and Future Work

Traditional multiagent planning on partially observable environments mostly deals with fully-
communicative or non-communicative situations. For a more realistic scenario where communi-
cation should be used only when necessary  state-of-the-art methods are only capable of approxi-
mating the optimal policy at run-time [11  15]. Here  we have analyzed the properties of MPOMDP
models which can be exploited in order to increase the efﬁciency of communication between agents.
We have shown that these properties hold  for various MPOMDP scenarios  and that the decision
quality can be maintained while signiﬁcantly reducing the amount of communication  as long as the
dependencies within the model are sparse.

Although one of the main features of these techniques is that they may be applied to any given
MPOMDP value function  in some situations this value function may be costly to obtain. As future
work  we will investigate methods for obtaining MPOMDP value functions that are easy to partition
using our techniques.

Acknowledgments

This work was funded in part by Fundac¸˜ao para a Ciˆencia e a Tecnologia (ISR/IST pluriannual fund-
ing) through the PIDDAC Program funds and was supported by project CMU-PT/SIA/0023/2009
under the Carnegie Mellon-Portugal Program. J.M. was supported by a PhD Student Scholarship 
SFRH/BD/44661/2008  from the Portuguese FCT POCTI programme. M.S. is funded by the FP7
Marie Curie Actions Individual Fellowship #275217 (FP7-PEOPLE-2010-IEF).

8

References
[1] Daniel S. Bernstein  Robert Givan  Neil Immerman  and Shlomo Zilberstein. The complexity
of decentralized control of Markov decision processes. Mathematics of Operations Research 
27(4):819–840  2002.

[2] Xavier Boyen and Daphne Koller. Tractable inference for complex stochastic processes.

Proc. of Uncertainty in Artiﬁcial Intelligence  1998.

In

[3] X.G. Fang and G. Havas. On the worst-case complexity of integer gaussian elimination. In
Proceedings of the 1997 international symposium on Symbolic and algebraic computation  pages
28–31. ACM  1997.

[4] L. P. Kaelbling  M. L. Littman  and A. R. Cassandra. Planning and acting in partially observable

stochastic domains. Artiﬁcial Intelligence  101:99–134  1998.

[5] David A. McAllester and Satinder Singh. Approximate planning for factored POMDPs using

belief state simpliﬁcation. In Proc. of Uncertainty in Artiﬁcial Intelligence  1999.

[6] J.V. Messias  M.T.J. Spaan  and P. U. Lima. Supplementary material for “Efﬁcient Ofﬂine

Communication Policies for Factored Multiagent POMDPs”. ISR/IST  2011.

[7] Frans A. Oliehoek  Matthijs T. J. Spaan  and Nikos Vlassis. Dec-POMDPs with delayed com-
munication. In Multi-agent Sequential Decision Making in Uncertain Domains  2007. Workshop
at AAMAS07.

[8] Frans A. Oliehoek  Matthijs T. J. Spaan  Shimon Whiteson  and Nikos Vlassis. Exploiting
In Proc. of Int. Conference on Autonomous

locality of interaction in factored Dec-POMDPs.
Agents and Multi Agent Systems  2008.

[9] P. Poupart and C. Boutilier. Value-directed belief state approximation for POMDPs. In Proc. of

Uncertainty in Artiﬁcial Intelligence  volume 130  2000.

[10] David V. Pynadath and Milind Tambe. The communicative multiagent team decision problem:
Analyzing teamwork theories and models. Journal of Artiﬁcial Intelligence Research  16:389–
423  2002.

[11] M. Roth  R. Simmons  and M. Veloso. Decentralized communication strategies for coordinated
multi-agent policies. In Multi-Robot Systems: From Swarms to Intelligent Automata  volume IV.
Kluwer Academic Publishers  2005.

[12] Maayan Roth  Reid Simmons  and Manuela Veloso. Exploiting factored representations for
decentralized execution in multi-agent teams. In Proc. of Int. Conference on Autonomous Agents
and Multi Agent Systems  2007.

[13] Matthijs T. J. Spaan  Frans A. Oliehoek  and Nikos Vlassis. Multiagent planning under uncer-
tainty with stochastic communication delays. In Proc. of Int. Conf. on Automated Planning and
Scheduling  pages 338–345  2008.

[14] Chelsea C. White. Partially observed Markov decision processes: a survey. Annals of Opera-

tions Research  32  1991.

[15] Feng Wu  Shlomo Zilberstein  and Xiaoping Chen. Multi-agent online planning with commu-

nication. In Int. Conf. on Automated Planning and Scheduling  2009.

9

,Nils Napp
Ryan Adams
Yi Sun
Yuheng Chen
Xiaogang Wang
Xiaoou Tang