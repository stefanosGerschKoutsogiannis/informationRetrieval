2014,Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers,We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach  refereed as Inference by Learning or IbyL  relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems  where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line.,Inference by Learning: Speeding-up Graphical

Model Optimization via a Coarse-to-Fine Cascade of

Pruning Classiﬁers

Bruno Conejo∗

GPS Division  California Institute of Technology  Pasadena  CA  USA
Universite Paris-Est  Ecole des Ponts ParisTech  Marne-la-Vallee  France

bconejo@caltech.edu

Universite Paris-Est  Ecole des Ponts ParisTech  Marne-la-Vallee  France

nikos.komodakis@enpc.fr

Nikos Komodakis

Sebastien Leprince & Jean Philippe Avouac

GPS Division  California Institute of Technology  Pasadena  CA  USA

leprincs@caltech.edu avouac@gps.caltech.edu

Abstract

We propose a general and versatile framework that signiﬁcantly speeds-up graph-
ical model optimization while maintaining an excellent solution accuracy. The
proposed approach  refereed as Inference by Learning or in short as IbyL  relies
on a multi-scale pruning scheme that progressively reduces the solution space by
use of a coarse-to-ﬁne cascade of learnt classiﬁers. We thoroughly experiment
with classic computer vision related MRF problems  where our novel framework
constantly yields a signiﬁcant time speed-up (with respect to the most efﬁcient
inference methods) and obtains a more accurate solution than directly optimizing
the MRF. We make our code available on-line [4].

Introduction

1
Graphical models in computer vision Optimization of undirected graphical models such as
Markov Random Fields  MRF  or Conditional Random Fields  CRF  is of fundamental importance in
computer vision. Currently  a wide spectrum of problems including stereo matching [25  13]  opti-
cal ﬂow estimation [27  16]  image segmentation [23  14]  image completion and denoising [10]  or 
object recognition [8  2] rely on ﬁnding the mode of the distribution associated to the random ﬁeld 
i.e.  the Maximum A Posteriori (MAP) solution. The MAP estimation  often referred as the labeling
problem  is posed as an energy minimization task. While this task is NP-Hard  strong optimum solu-
tions or even the optimal solutions can be obtained [3]. Over the past 20 years  tremendous progress
has been made in term of computational cost  and  many different techniques have been developed
such as move making approaches [3  19  22  21  28]  and message passing methods [9  32  18  20].
A review of their effectiveness has been published in [31  12]. Nevertheless  the ever increasing
dimensionality of the problems and the need for larger solution space greatly challenge these tech-

∗This work was supported by USGS through the Measurements of surface ruptures produced by continental
earthquakes from optical imagery and LiDAR project (USGS Award G13AP00037)  the Terrestrial Hazard
Observation and Reporting Center of Caltech  and the Moore foundation through the Advanced Earth Surface
Observation Project (AESOP Grant 2808).

1

niques as even the best ones have a highly super-linear computational cost and memory requirement
relatively to the dimensionality of the problem.
Our goal in this work is to develop a general MRF optimization framework that can provide a
signiﬁcant speed-up for such methods while maintaining the accuracy of the estimated solutions.
Our strategy for accomplishing this goal will be to gradually reduce (by a signiﬁcant amount) the
size of the discrete state space via exploiting the fact that an optimal labeling is typically far from
being random. Indeed  most MRF optimization problems favor solutions that are piecewise smooth.
In fact  this spatial structure of the MAP solution has already been exploited in prior work to reduce
the dimensionality of the solution space.

Related work A ﬁrst set of methods of this type  referred here for short as the super-pixel approach
[30]  deﬁnes a grouping heuristic to merge many random variables together in super-pixels. The
grouping heuristic can be energy-aware if it is based on the energy to minimize as in [15]  or  energy-
agnostic otherwise as in [7  30]. All random variables belonging to the same super-pixel are forced
to take the same label. This restricts the solution space and results in an optimization speed-up as
a smaller number of variables needs to be optimized. The super-pixel approach has been applied
with segmentation  stereo and object recognition [15]. However  if the grouping heuristic merges
variables that should have a different label in the MAP solution  only an approximate labeling is
computed. In practice  deﬁning general yet efﬁcient grouping heuristics is difﬁcult. This represents
the key limitation of super-pixel approaches.
One way to overcome this limitation is to mimic the multi-scale scheme used in continuous opti-
mization by building a coarse to ﬁne representation of the graphical model. Similarly to the super-
pixel approach  such a multi-scale method  relies again on a grouping of variables for building the
required coarse to ﬁne representation [17  24  26]. However  contrary to the super-pixel approach 
if the grouping merges variables that should have a different label in the MAP solution  there al-
ways exists a scale at which these variables are not grouped. This property thus ensures that the
MAP solution can still be recovered. Nevertheless  in order to manage a signiﬁcant speed-up of
the optimization  the multi-scale approach also needs to progressively reduce the number of labels
per random variable (i.e.  the solution space). Typically  this is achieved by use  for instance  of a
heuristic that keeps only a small ﬁxed number of labels around the optimal label of each node found
at the current scale  while pruning all other labels  which are therefore not considered thereafter [5].
This strategy  however  may not be optimal or even valid for all types of problems. Furthermore 
such a pruning heuristic is totally inappropriate (and can thus lead to errors) for nodes located along
discontinuity boundaries of an optimal solution  where such boundaries are always expected to exist
in practice. An alternative strategy followed by some other methods relies on selecting a subset of
the MRF nodes at each scale (based on some criterion) and then ﬁxing their labels according to the
optimal solution estimated at the current scale (essentially  such methods contract the entire label
set of a node to a single label). However  such a ﬁxing strategy may be too aggressive and can also
easily lead to eliminating good labels.

Proposed approach Our method simultaneously makes use of the following two strategies for
speeding-up the MRF optimization process:

(i) it solves the problem through a multi-scale approach that gradually reﬁnes the MAP esti-

mation based on a coarse-to-ﬁne representation of the graphical model 

(ii) and  at the same time  it progressively reduces the label space of each variable by cleverly

utilizing the information computed during the above coarse-to-ﬁne process.

To achieve that  we propose to signiﬁcantly revisit the way that the pruning of the solution space
takes place. More speciﬁcally:

(i) we make use of and incorporate into the above process a ﬁne-grained pruning scheme that
allows an arbitrary subset of labels to be discarded  where this subset can be different for
each node 

(ii) additionally  and most importantly  instead of trying to manually come up with some criteria
for deciding what labels to prune or keep  we introduce the idea of relying entirely on
a sequence of trained classiﬁers for taking such decisions  where different classiﬁers per
scale are used.

2

We name such an approach Inference by Learning  and show that it is particularly efﬁcient and effec-
tive in reducing the label space while omitting very few correct labels. Furthermore  we demonstrate
that the training of these classiﬁers can be done based on features that are not application speciﬁc
but depend solely on the energy function  which thus makes our approach generic and applicable
to any MRF problem. The end result of this process is to obtain both an important speed-up and
a signiﬁcant decrease in memory consumption as the solution space is progressively reduced. Fur-
thermore  as each scale reﬁnes the MAP estimation  a further speed-up is obtained as a result of a
warm-start initialization that can be used when transitioning between different scales.
Before proceeding  it is worth also noting that there exists a body of prior work [29] that focuses on
ﬁxing the labels of a subset of nodes of the graphical model by searching for a partial labeling with
the so-called persistency property (which means that this labeling is provably guaranteed to be part
of an optimal solution). However  ﬁnding such a set of persistent variables is typically very time
consuming. Furthermore  in many cases only a limited number of these variables can be detected.
As a result  the focus of these works is entirely different from ours  since the main motivation in our
case is how to obtain a signiﬁcant speed-up for the optimization.
Hereafter  we assume without loss of generality that the graphical model is a discrete pairwise
CRF/MRF. However  one can straightforwardly apply our approach to higher order models.

Outline of the paper We brieﬂy review the optimization problem related to a discrete pairwise
MRF and introduce the necessary notations in section 2. We describe our general multi-scale pruning
framework in section 3. We explain how classiﬁers are trained in section 4. Experimental results
and their analysis are presented in 5. Finally  we conclude the paper in section 6.
2 Notation and preliminaries
To represent a discrete MRF model M  we use the following notation

M =(cid:0)V E L {φi}i∈V  {φij}(i j)∈E(cid:1) .

(1)
Here V and E represent respectively the nodes and edges of a graph  and L represents a discrete
label set. Furthermore  for every i ∈ V and (i  j) ∈ E  the functions φi : L → R and φij : L2 → R
represent respectively unary and pairwise costs (that are also known connectively as MRF potentials
vertex i  taking values in the label set L  and the total cost (energy) E(x|M) of such a solution is

φ =(cid:8){φi}i∈V  {φij}(i j)∈E(cid:9)). A solution x = (xi)i∈V of this model consists of one variable per

E(x|M) =

φi(xi) +

φij(xi  xj) .

(cid:88)

i∈V

(cid:88)

(i j)∈E

The goal of MAP estimation is to ﬁnd a solution that has minimum energy  i.e.  computes

xMAP = arg min
x∈L|V|

E(x|M) .

The above minimization takes place over the full solution space of model M  which is L|V|. Here
we will also make use of a pruned solution space S(M  A)  which is deﬁned based on a binary
function A : V × L → {0  1} (referred to as the pruning matrix hereafter) that speciﬁes the status
(active or pruned) of a label for a given vertex  i.e. 

A(i  l) =

if label l is active at vertex i
if label l is pruned at vertex i

(2)

(cid:26) 1
(cid:110)

0

During optimization  active labels are retained while pruned labels are discarded. Based on a given
A  the corresponding pruned solution space of model M is deﬁned as
x ∈ L|V| | (∀i)  A(i  xi) = 1

S(M  A) =

(cid:111)

.

3 Multiscale Inference by Learning
In this section we describe the overall structure of our MAP estimation framework  beginning by
explaining how to construct the coarse-to-ﬁne representation of the input graphical model.

3

(cid:0)V(cid:48) E(cid:48) L {φ(cid:48)

ij}(i j)∈E(cid:48)(cid:1). Intuitively  we want to partition the nodes of M into groups 

3.1 Model coarsening
Given a model M (deﬁned as in (1))  we wish to create a “coarser” version of this model M(cid:48) =
i}i∈V(cid:48) {φ(cid:48)
and treat each group as a single node of the coarser model M(cid:48) (the implicit assumption is that nodes
of M that are grouped together are assigned the same label). To that end  we will make use of a
grouping function g : V → N . The nodes and edges of the coarser model are then deﬁned as follows

V(cid:48) ={i(cid:48) | ∃i ∈ V  i(cid:48) = g(i)}  
E(cid:48) ={(i(cid:48)  j(cid:48)) | ∃(i  j) ∈ E  i(cid:48) = g(i)  j(cid:48) = g(j)  i(cid:48) (cid:54)= j(cid:48)} .
Furthermore  the unary and pairwise potentials of M(cid:48) are given by

(3)
(4)

  (5)

(∀i(cid:48) ∈ V(cid:48))  φ(cid:48)

i(cid:48)(l) =(cid:80)
+(cid:80)
(cid:88)

i∈V|i(cid:48)=g(i)
(i j)∈E|i(cid:48)=g(i)=g(j) φij(l  l)

φi(l)

i(cid:48)j(cid:48)(l0  l1) =

φij(l0  l1) . (6)

(i j)∈E|i(cid:48)=g(i) j(cid:48)=g(j)

(∀(i(cid:48)  j(cid:48)) ∈ E(cid:48))  φ(cid:48)

Figure 1: Black circles:
V  Black lines: E  Red
squares: V(cid:48)  Blue lines:
E(cid:48).
With a slight abuse of notation  we will hereafter use g(M) to denote the coarser model resulting
from M when using the grouping function g  i.e.  we deﬁne g(M) = M(cid:48). Also  given a solution x(cid:48)
of M(cid:48)  we can “upsample” it into a solution x of M by setting xi = x(cid:48)
g(i) for each i ∈ V. We will
use the following notation in this case: g−1(x(cid:48)) = x. We provide a toy example in supplementary
materials.
3.2 Coarse-to-ﬁne optimization and label pruning
To estimate the MAP of an input model M  we ﬁrst construct a series of N +1 progressively coarser
models (M(s))0≤s≤N by use of a sequence of N grouping functions (g(s))0≤s<N   where

M(0) = M and

(∀s)  M(s+1) = g(s)(M(s)) .

This provides a multiscale (coarse-to-ﬁne) representation of the original model.  where the elements
of the resulting models are denoted as follows:

(cid:16)V (s) E (s) L {φ(s)

M(s) =

i }i∈V (s)  {φ(s)

ij }(i j)∈E (s)

(cid:17)

In our framework  MAP estimation proceeds from the coarsest to the ﬁnest scale (i.e.  from model
M(N ) to M(0)). During this process  a pruning matrix A(s) is computed at each scale s  which is
used for deﬁning a restricted solution space S(M(s)  A(s)). The elements of the matrix A(N ) at the
coarsest scale are all set equal to 1 (i.e.  no label pruning is used in this case)  whereas in all other
scales A(s) is computed by use of a trained classiﬁer f (s).
More speciﬁcally  at any given scale s  the following steps take place:

i. We approximately minimize (via any existing MRF optimization method) the energy of the

model M(s) over the restricted solution space S(M(s)  A(s))  i.e.  we compute

x(s) ≈ arg minx∈S(M(s) A(s)) E(x|M(s)) .

ii. Given the estimated solution x(s)  a feature map z(s) : V (s) × L → RK is computed at
the current scale  and a trained classiﬁer f (s) : RK → {0  1} uses this feature map z(s) to
construct the pruning matrix A(s−1) for the next scale as follows

(∀i ∈ V (s−1)  ∀l ∈ L)  A(s−1)(i  l) = f (s)(z(s)(g(s−1)(i)  l)) .

iii. Solution x(s) is “upsampled” into x(s−1) = [g(s−1)]−1(x(s)) and used as the initializa-
tion for the optimization at the next scale s − 1. Note that  due to (5) and (6)  it holds
E(x(s−1)|M(s−1)) = E(x(s)|M(s)). Therefore  this initialization ensures that energy will
continually decrease if the same is true for the optimization applied per scale. Furthermore 
it can allow for a warm-starting strategy when transitioning between scales.

The pseudocode of the resulting algorithm appears in Algo. 1.

4

Algorithm 1: Inference by learning framework
Data: Model M  grouping functions (g(s))0≤s<N   classiﬁers (f (s))0<s≤N
Result: x(0)
Compute the coarse to ﬁne sequence of MRFs:
M(0) ← M
for s = [0 . . . N − 1] do

M(s+1) ← g(s)(M(s))

Optimize the coarse to ﬁne sequence of MRFs over pruned solution spaces:
(∀i ∈ V (N ) ∀l ∈ L)  A(N )(i  l) ← 1
Initialize x(N )
for s = [N...0] do
Update x(s) by iterative minimization: x(s) ≈ arg minx∈S(M(s) A(s)) E(x|M(s))
if s (cid:54)= 0 then

Compute feature map z(s)
Update pruning matrix for next ﬁner scale: A(s−1)(i  l) = f (s)(z(s)(g(s−1)(i)  l))
Upsample x(s) for initializing solution x(s−1) at next scale: x(s−1) ← [g(s−1)]−1(x(s))

4 Features and classiﬁer for label pruning
For each scale s  we explain how the set of features comprising the feature map z(s) is computed
and how we train (off-line) the classiﬁer f (s). This is a crucial step for our approach. Indeed  if the
classiﬁer wrongly prunes labels that belong to the MAP solution  then  only an approximate labeling
might be found at the ﬁnest scale. Moreover  keeping too many active labels will result in a poor
speed-up for MAP estimation.
4.1 Features
The feature map z(s) : V (s) × L → RK is formed by stacking K individual real-valued features
deﬁned on V (s) × L. We propose to compute features that are not application speciﬁc but depend
solely on the energy function and the current solution x(s). This makes our approach generic and
applicable to any MRF problem. However  as we establish a general framework  speciﬁc application
features can be straightforwardly added in future work.

Presence of strong discontinuity This binary feature  PSD(s)  accounts for the existence of dis-
continuity in solution x(s) when a strong link (i.e.  φij(x(s)
  x(s)
j ) > ρ) exists between neighbors.
Its deﬁnition follows for any vertex i ∈ V (s) and any label l ∈ L :
∃(i  j) ∈ E (s)| φij(x(s)
otherwise

PSD(s)(i  l) =

j ) > ρ

(cid:26)

  x(s)

i

(7)

1
0

i

Local energy variation This feature represents the local variation of the energy around the current
solution x(s). It accounts for both the unary and pairwise terms associated to a vertex and a label.
As in [11]  we remove the local energy of the current solution as it leads to a higher discriminative
power. The local energy variation feature  LEV(s)  is deﬁned for any i ∈ V (s) and l ∈ L as follows:

(cid:88)

LEV(s)(i  l) =

i (l) − φ(s)
φ(s)
N (s)V (i)

i (x(s)
i )

+

j:(i j)∈E (s)

φ(s)
ij (l  x(s)

j ) − φ(s)
N (s)E (i)

ij (x(s)

i

  x(s)
j )

(8)

with N (s)V (i) = card{i(cid:48) ∈ V (s−1) : g(s−1)(i(cid:48)) = i} and N (s)E (i) = card{(i(cid:48)  j(cid:48)) ∈ E (s−1) :
g(s−1)(i(cid:48)) = i  g(s−1)(j(cid:48)) = j}.

Unary “coarsening” This feature  UC(s)  aims to estimate an approximation of the coarsening
induced in the MRF unary terms when going from model M(s−1) to model M(s)  i.e.  as a result of

5

applying the grouping function g(s−1). It is deﬁned for any i ∈ V (s) and l ∈ L as follows

(cid:88)

UC(s)(i  l) =

i(cid:48)∈V (s−1)|g(s−1)(i(cid:48))=i

|φ(s−1)
i(cid:48)

i

(l)
N (s)V (i)

(l) − φ(s)
N (s)V (i)

|

(9)

Feature normalization The features are by design insensitive to any additive term applied on all
the unary and pairwise terms. However  we still need to apply a normalization to the LEV(s) and
UC(s) features to make them insensitive to any positive global scaling factor applied on both the
unary and pairwise terms (such scaling variations are commonly used in computer vision). Hence 
we simply divide group of features  LEV(s) and UC(s) by their respective mean value.
4.2 Classiﬁer
To train the classiﬁers  we are given as input a set of MRF instances (all of the same class  e.g. 
stereo-matching) along with the ground truth MAP solutions. We extract a subset of MRFs for off-
line learning and a subset for on-line testing. For each MRF instance in the training set  we apply
the algorithm 1 without any pruning (i.e.  A(s) ≡ 1) and  at each scale  we keep track of the features
z(s) and also compute the binary function X (s)

(cid:26)1 

MAP : V (s) × L → {0  1} deﬁned as follows:
if l is the ground truth label for node i
otherwise

0 

X (s−1)
MAP (i(cid:48)  l)  

MAP(i  l) =

(∀i ∈ V ∀l ∈ L)  X (0)

(cid:95)
where(cid:87) denotes the binary OR operator. The values 0 and 1 in X (s)

(∀s > 0)(∀i ∈ V (s) ∀l ∈ L)  X (s)

MAP(i  l) =

i(cid:48)∈V (s−1):g(s)(i(cid:48))=i

0

and z(s)

1   where z(s)

MAP deﬁne respectively the two
classes c0 and c1 when training the classiﬁer f (s)  where c0 means that the label can be pruned and
c1 that the label should not be pruned.
To treat separately the nodes that are on the border of a strong discontinuity  we split the feature map
z(s) into two groups z(s)
contains only features where PSD(s) = 0 and z(s)
1
0
contains only features where PSD(s) = 1 (strong discontinuity). For each group  we train a standard
linear C-SVM classiﬁer with l2-norm regularization (regularization parameter was set to C = 10).
The linear classiﬁers give good enough accuracy during training while also being fast to evaluate at
test time
During training (and for each group)  we also introduce weights to balance the different number of
elements in each class (c0 is much larger than c1)  and to also strongly penalize misclassiﬁcation in
c1 (as such misclassiﬁcation can have a more drastic impact on the accuracy of MAP estimation). To
accomplish that  we set the weight for class c0 to 1  and the weight for class c1 to λ card(c0)
card(c1)  where
card(·) counts the number of training samples in each class. Parameter λ is a positive scalar (com-
mon to both groups) used for tuning the penalization of misclassiﬁcation in c1 (it will be referred
to as the pruning aggressiveness factor hereafter as it affects the amount of labels that get pruned).
During on-line testing  depending on the value of the PSD feature  f (s) applies the linear classiﬁer
learned on group z(s)
0
5 Experimental results
We evaluate our framework on pairwise MRFs from stereo-matching  image restoration  and  optical
ﬂow estimation problems. The corresponding MRF graphs consist of regular 4-connected grids in
this case. At each scale  the grouping function merges together vertices of 2 × 2 subgrids. We leave
more advanced grouping functions [15] for future work. As MRF optimization subroutine  we use
the Fast-PD algorithm [21]. We make our code available on-line [4].

if PSD(s) = 0  or the linear classiﬁer learned on group z(s)
1

if PSD(s) = 1.

Experimental setup For the stereo matching problem  we estimate the disparity map from images
IR and IL where each label encodes a potential disparity d (discretized at quarter of a pixel preci-
sion)  with MRF potentials φp(d) = ||IL(yp  xp)−IR(yp  xp−d)||1 and φpq(d0  d1) = wpq|d0−d1| 
with the weight wpq varying based on the image gradient (parameters are adjusted for each se-
quence). We train the classiﬁer on the well-known Tsukuba stereo-pair (61 labels)  and use all other

6

(a) Speed-up

(b) Active label ratio

(c) Energy ratio

(d) Label agreement

Figure 2: Performance of our Inference by Learning framework: (Top row) stereo matching  (Middle row)
optical ﬂow  (Bottom row) image restoration. For stereo matching  the Average Middlebury curve represents
the average value of the statistic for the entire Middlebury dataset [6] (2001  2003  2005 and 2006) (37 stereo-
pairs).

stereo-pairs of [6] (2001  2003  2005 and 2006) for testing. For image restoration  we estimate the
pixel intensity of a noisy and incomplete image I with MRF potentials φp(l) = ||I(yp  xp) − l||2
and φ(l0  l1) = 25 min(||l0 − l1||2
2
2  200). We train the classiﬁer on the Penguin image stereo-pair
(256 labels)  and use House (256 labels) for testing (dataset [31]). For the optical ﬂow estimation 
we estimate a subpixel-accurate 2D displacement ﬁeld between two frames by extending the stereo
matching formulation to 2D. Using the dataset of [1]  we train the classiﬁer on Army (1116 labels) 
and test on RubberWhale (625 labels) and Dimetrodon (483 labels). For all experiments  we use 5
scales and set in (7) ρ = 5 ¯wpq with ¯wpq being the mean value of edge weights.

Evaluations We evaluate three optimization strategies: the direct optimization (i.e.  optimizing
the full MRF at the ﬁnest scale)  the multi-scale optimization (λ = 0  i.e.  our framework without
any pruning)  and our Inference by Learning optimization  where we experiment with different error
ratios λ that range between 0.001 and 1.
We assess the performance by computing the energy ratio  i.e.  the ratio between the current energy
and the energy computed by the direct optimization  the best label agreement  i.e.  the proportion
of labels that coincides with the labels of the lowest computed energy  the speed-up factor  i.e.  the
ratio of computation time between the direct optimization and the current optimization strategy  and 
the active label ratio  i.e.  the percentage of active labels at the ﬁnest scale.

Results and discussion For all problems  we present in Fig. 2 the performance of our Inference
by Learning approach for all tested aggressiveness factors and show in Fig. 3 estimated results for
λ = 0.01. We present additional results in the supplementary material.
For every problem and aggressiveness factors until λ = 0.1  our pruning-based optimization obtains
a lower energy (column (c) of Fig. 2) in less computation time  achieving a speed-up factor (column
(a) of Fig. 2) close to 5 for Stereo-matching  above 10 for Optical-ﬂow and up to 3 for image
restoration. (note that these speed-up factors are with respect to an algorithm  FastPD  that was the
most efﬁcient one in recent comparisons [12]). The percentage of active labels (Fig. 2 column (b))
strongly correlates with the speed-up factor. The best labeling agreement (Fig. 2 column (d)) is
never worse than 97% (except for the image restoration problems because of the in-painted area)

7

a
b
u
k
u
s
T

s
u
n
e
V

y
d
d
e
T

y
m
A

r

.
t
e
m
D

i

e
s
u
o
H

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: Results of our Inference by Learning framework for λ = 0.1. Each row is a different MRF problem.
(a) original image  (b) ground truth  (c) solution of the pruning framework  (d e f) percentage of active labels
per vertex for scale 0  1 and 2 (black 0%  white 100%).

and is always above 99% for λ (cid:54) 0.1. As expected  less pruning happens near label discontinuities
as illustrated in column (d e f) of Fig. 3 justifying the use of a dedicated linear classiﬁer. Moreover 
large homogeneously labeled regions are pruned earlier in the coarse to ﬁne scale.
6 Conclusion and future work
Our Inference by Learning approach consistently speeds-up the graphical model optimization by
a signiﬁcant amount while maintaining an excellent accuracy of the labeling estimation. On most
experiments  it even obtains a lower energy than direct optimization.
In future work  we plan to experiment with problems that require general pairwise potentials where
message-passing techniques can be more effective than graph-cut based methods but are at the same
time much slower. Our framework is guaranteed to provide an even more dramatic speedup in this
case since the computational complexity of message-passing methods is quadratic with respect to
the number of labels while being linear for graph-cut based methods used in our experiments. We
also intend to explore the use of application speciﬁc features  learn the grouping functions used in
the coarse-to-ﬁne scheme  jointly train the cascade of classiﬁers  and apply our framework to high
order graphical models.
References
[1] S. Baker  S. Roth  D. Scharstein  M.J. Black  J. P. Lewis  and R. Szeliski. A database and evaluation

methodology for optical ﬂow. In ICCV 2007.  2007.

[2] Martin Bergtholdt  J¨org Kappes  Stefan Schmidt  and Christoph Schn¨orr. A study of parts-based object

class detection using complete graphs. IJCV  2010.

[3] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI  2001.
[4] B. Conejo. http://imagine.enpc.fr/˜conejob/ibyl/.

8

[5] B. Conejo  S. Leprince  F. Ayoub  and J. P. Avouac. Fast global stereo matching via energy pyramid

minimization. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci.  2014.

[6] Middlebury Stereo Datasets.
[7] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efﬁcient graph-based image segmentation.

2004.

IJCV 

[8] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Pictorial structures for object recognition. IJCV  2005.
[9] P.F. Felzenszwalb and D.P. Huttenlocher. Efﬁcient belief propagation for early vision. In CVPR  2004.
[10] W.T. Freeman and E.C. Pasztor. Learning low-level vision. In ICCV  1999.
[11] Xiaoyan Hu and P. Mordohai. A quantitative evaluation of conﬁdence measures for stereo vision. PAMI. 

2012.

[12] J.H. Kappes  B. Andres  F.A. Hamprecht  C. Schnorr  S. Nowozin  D. Batra  Sungwoong Kim  B.X.
Kausler  J. Lellmann  N. Komodakis  and C. Rother. A comparative study of modern inference techniques
for discrete energy minimization problems. In CVPR  2013.

[13] Junhwan Kim  V. Kolmogorov  and R. Zabih. Visual correspondence using energy minimization and

mutual information. In ICCV  2003.

[14] S. Kim  C. Yoo  S. Nowozin  and P. Kohli. Image segmentation using higher-order correlation clustering 

2014.

[15] Taesup Kim  S. Nowozin  P. Kohli  and C.D. Yoo. Variable grouping for energy minimization. In CVPR 

2011.

[16] T. Kohlberger  C. Schnorr  A. Bruhn  and J. Weickert. Domain decomposition for variational optical-ﬂow

computation. IEEE Transactions on Information Theory/Image Processing  2005.

[17] Pushmeet Kohli  Victor S. Lempitsky  and Carsten Rother. Uncertainty driven multi-scale optimization.

In DAGM-Symposium  2010.

[18] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. PAMI  2006.
[19] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? PAMI  2004.
[20] N. Komodakis  N. Paragios  and G. Tziritas. Mrf optimization via dual decomposition: Message-passing

revisited. In CVPR  2007.

[21] N. Komodakis  G. Tziritas  and N. Paragios. Fast  approximately optimal solutions for single and dynamic

mrfs. In CVPR  2007.

[22] M. Pawan Kumar and Daphne Koller. Map estimation of semi-metric mrfs via hierarchical graph cuts. In

UAI  2009.

[23] M.P. Kumar  P.H.S. Ton  and A. Zisserman. Obj cut. In CVPR  2005.
[24] H. Lombaert  Yiyong Sun  L. Grady  and Chenyang Xu. A multilevel banded graph cuts method for fast

image segmentation. In ICCV 2005.  2005.

[25] T. Meltzer  C. Yanover  and Y. Weiss. Globally optimal solutions for energy minimization in stereo vision

using reweighted belief propagation. In ICCV  2005.

[26] P. Perez and F. Heitz. Restriction of a markov random ﬁeld on a graph and multiresolution statistical

image modeling. IEEE Transactions on Information Theory/Image Processing  1996.

[27] S. Roth and M.J. Black. Fields of experts: a framework for learning image priors. In CVPR  2005.
[28] C. Rother  V. Kolmogorov  V. Lempitsky  and M. Szummer. Optimizing binary mrfs via extended roof

duality. In CVPR  2007.

[29] Alexander Shekhovtsov. Maximum persistency in energy minimization. In CVPR  2014.
[30] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. PAMI.  2000.
[31] R. Szeliski  R. Zabih  D. Scharstein  O. Veksler  V. Kolmogorov  Aseem Agarwala  M. Tappen  and
C. Rother. A comparative study of energy minimization methods for markov random ﬁelds with
smoothness-based priors. PAMI  2008.

[32] M.J. Wainwright  T.S. Jaakkola  and A.S. Willsky. Map estimation via agreement on trees: message-

passing and linear programming. IEEE Transactions on Information Theory/Image Processing  2005.

9

,Bruno Conejo
Nikos Komodakis
Sebastien Leprince
Jean Philippe Avouac
Jakob Foerster
Ioannis Alexandros Assael
Nando de Freitas
Shimon Whiteson