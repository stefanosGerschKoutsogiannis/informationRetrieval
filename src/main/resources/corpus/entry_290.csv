2016,A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification,We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes  while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges  we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method  and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.,A scalable end-to-end Gaussian process adapter for

irregularly sampled time series classiﬁcation

Steven Cheng-Xian Li
Benjamin Marlin
College of Information and Computer Sciences

University of Massachusetts Amherst

Amherst  MA 01003

{cxl marlin}@cs.umass.edu

Abstract

We present a general framework for classiﬁcation of sparse and irregularly-sampled
time series. The properties of such time series can result in substantial uncertainty
about the values of the underlying temporal processes  while making the data
difﬁcult to deal with using standard classiﬁcation methods that assume ﬁxed-
dimensional feature spaces. To address these challenges  we propose an uncertainty-
aware classiﬁcation framework based on a special computational layer we refer to
as the Gaussian process adapter that can connect irregularly sampled time series
data to any black-box classiﬁer learnable using gradient descent. We show how
to scale up the required computations based on combining the structured kernel
interpolation framework and the Lanczos approximation method  and how to
discriminatively train the Gaussian process adapter in combination with a number
of classiﬁers end-to-end using backpropagation.

1

Introduction

In this paper  we propose a general framework for classiﬁcation of sparse and irregularly-sampled
time series. An irregularly-sampled time series is a sequence of samples with irregular intervals
between their observation times. These intervals can be large when the time series are also sparsely
sampled. Such time series data are studied in various areas including climate science [22]  ecology
[4]  biology [18]  medicine [15] and astronomy [21]. Classiﬁcation in this setting is challenging both
because the data cases are not naturally deﬁned in a ﬁxed-dimensional feature space due to irregular
sampling and variable numbers of samples  and because there can be substantial uncertainty about
the underlying temporal processes due to the sparsity of observations.
Recently  Li and Marlin [13] introduced the mixture of expected Gaussian kernels (MEG) framework 
an uncertainty-aware kernel for classifying sparse and irregularly sampled time series. Classiﬁcation
with MEG kernels is shown to outperform models that ignore uncertainty due to sparse and irregular
sampling. On the other hand  various deep learning models including convolutional neural networks
[12] have been successfully applied to ﬁelds such as computer vision and natural language processing 
and have been shown to achieve state-of-the-art results on various tasks. Some of these models
have desirable properties for time series classiﬁcation  but cannot be directly applied to sparse and
irregularly sampled time series.
Inspired by the MEG kernel  we propose an uncertainty-aware classiﬁcation framework that enables
learning black-box classiﬁcation models from sparse and irregularly sampled time series data. This
framework is based on the use of a computational layer that we refer to as the Gaussian process
(GP) adapter. The GP adapter uses Gaussian process regression to transform the irregular time series
data into a uniform representation  allowing sparse and irregularly sampled data to be fed into any
black-box classiﬁer learnable using gradient descent while preserving uncertainty. However  the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

O(n3) time and O(n2) space of exact GP regression makes the GP adapter prohibitively expensive
when scaling up to large time series.
To address this problem  we show how to speed up the key computation of sampling from a GP
posterior based on combining the structured kernel interpolation (SKI) framework that was recently
proposed by Wilson and Nickisch [25] with Lanczos methods for approximating matrix functions [3].
Using the proposed sampling algorithm  the GP adapter can run in linear time and space in terms of
the length of the time series  and O(m log m) time when m inducing points are used.
We also show that GP adapter can be trained end-to-end together with the parameters of the chosen
classiﬁer by backpropagation through the iterative Lanczos method. We present results using logistic
regression  fully-connected feedforward networks  convolutional neural networks and the MEG kernel.
We show that end-to-end discriminative training of the GP adapter outperforms a variety of baselines
in terms of classiﬁcation performance  including models based only on GP mean interpolation  or
with GP regression trained separately using marginal likelihood.

2 Gaussian processes for sparse and irregularly-sampled time series

Our focus in this paper is on time series classiﬁcation in the presence of sparse and irregular sampling.
In this problem  the data D contain N independent tuples consisting of a time series Si and a label
yi. Thus  D = {(S1  y1)  . . .   (SN   yN )}. Each time series Si is represented as a list of time points
ti = [ti1  . . .   ti|Si|](cid:62)  and a list of corresponding values vi = [vi1  . . .   vi|Si|](cid:62). We assume that
each time series is observed over a common time interval [0  T ]. However  different time series
are not necessarily observed at the same time points (i.e. ti (cid:54)= tj in general). This implies that the
number of observations in different time series is not necessary the same (i.e. |Si| (cid:54)= |Sj| in general).
Furthermore  the time intervals between observation within a single time series are not assumed to be
uniform.
Learning in this setting is challenging because the data cases are not naturally deﬁned in a ﬁxed-
dimensional feature space due to the irregular sampling. This means that commonly used classiﬁers
that take ﬁxed-length feature vectors as input are not applicable. In addition  there can be substantial
uncertainty about the underlying temporal processes due to the sparsity of observations.
To address these challenges  we build on ideas from the MEG kernel [13] by using GP regression
[17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series. We
ﬁx a set of reference time points x = [x1  . . .   xd](cid:62) and represent a time series S = (t  v) in terms
of its posterior marginal distribution at these time points. We use GP regression with a zero-mean
GP prior and a covariance function k(· ·) parameterized by kernel hyperparameters η. Let σ2 be the
independent noise variance of the GP regression model. The GP parameters are θ = (η  σ2).
Under this model  the marginal posterior GP at x is Gaussian distributed with the mean and covariance
given by

µ = Kx t(Kt t + σ2I)−1v 
Σ = Kx x − Kx t(Kt t + σ2I)−1Kt x

(1)
(2)
where Kx t denotes the covariance matrix with [Kx t]ij = k(xi  tj). We note that it takes O(n3 +nd)
time to exactly compute the posterior mean µ  and O(n3 + n2d + nd2) time to exactly compute the
full posterior covariance matrix Σ  where n = |t| and d = |x|.

3 The GP adapter and uncertainty-aware time series classiﬁcation

In this section we describe our framework for time series classiﬁcation in the presence of sparse
and irregular sampling. Our framework enables any black-box classiﬁer learnable by gradient-based
methods to be applied to the problem of classifying sparse and irregularly sampled time series.

3.1 Classiﬁcation frameworks and the Gaussian process adapter

In Section 2 we described how we can represent a time series through the marginal posterior it induces
under a Gaussian process regression model at any set of reference time points x. By ﬁxing a common

2

set of reference time points x for all time series in a data set  every time series can be transformed
into a common representation in the form of a multivariate Gaussian N (z|µ  Σ; θ) with z being the
random vector distributed according to the posterior GP marginalized over the time points x.1 Here
we assume that the GP parameters θ are shared across the entire data set.
If the z values were observed  we could simply apply a black-box classiﬁer. A classiﬁer can be
generally deﬁned by a mapping function f (z; w) parameterized by w  associated with a loss function
(cid:96)(f (z; w)  y) where y is a label value from the output space Y. However  in our case z is a Gaussian
random variable  which means (cid:96)(f (z; w)  y) is now itself a random variable given a label y. Therefore 
we use the expectation Ez∼N (µ Σ;θ)
series S given its Gaussian representation N (µ  Σ; θ). The learning problem becomes minimizing
the expected loss over the entire data set:

(cid:2)(cid:96)(f (z; w)  y)(cid:3) as the overall loss between the label y and a time
N(cid:88)

(cid:2)(cid:96)(f (zi; w)  yi)(cid:3).

w∗  θ

∗

= argmin

Ezi∼N (µi Σi;θ)

(3)

w θ

i=1

∗  we can make predictions on unseen data. In
Once we have the optimal parameters w∗ and θ
general  given an unseen time series S and its Gaussian representation N (µ  Σ; θ
)  we can predict
its label using (4)  although in many cases this can be simpliﬁed into a function of f (z; w∗) with the
expectation taken on or inside of f (z; w∗).

∗

y∗ = argmin
y∈Y

Ez∼N (µ Σ;θ∗)

(cid:2)(cid:96)(f (z; w∗)  y)(cid:3)

(4)

We name the above approach the Uncertainty-Aware Classiﬁcation (UAC) framework. Importantly 
this framework propagates the uncertainty in the GP posterior induced by each time series all the way
through to the loss function. Besides  we call the transformation S (cid:55)→ (µ  Σ) the Gaussian process
adapter  since it provides a uniform representation to connect the raw irregularly sampled time series
data to a black-box classiﬁer.
Variations of the UAC framework can be derived by taking the expectation at various position of
f (z; w) where z ∼ N (µ  Σ; θ). Taking the expectation at an earlier stage simpliﬁes the computation 
but the uncertainty information will be integrated out earlier as well.2 In the extreme case  if the
expectation is computed immediately followed by the GP adapter transformation  it is equivalent to
using a plug-in estimate µ for z in the loss function  (cid:96)(f (Ez∼N (µ Σ;θ)[z]; w)  y) = (cid:96)(f (µ; w)  y).
We refer to this as the IMPutation (IMP) framework. The IMP framework discards the uncertainty
information completely  which further simpliﬁes the computation. This simpliﬁed variation may be
useful when the time series are more densely sampled  where the uncertainty is less of a concern.
In practice  we can train the model using the UAC objective (3) and predict instead by IMP. In that
case  the predictions would be deterministic and can be computed efﬁciently without drawing samples
from the posterior GP as described later in Section 4.

3.2 Learning with the GP adapter

In the previous section  we showed that the UAC framework can be trained using (3). In this paper 
we use stochastic gradient descent to scalably optimize (3) by updating the model using a single time
series at a time  although it can be easily modiﬁed for batch or mini-batch updates. From now on 
we will focus on the optimization problem minw θ Ez∼N (µ Σ;θ)
output of the GP adapter given a time series S = (t  v) and its label y. For many classiﬁers  the
expected loss Ez∼N (µ Σ;θ)
the Monte Carlo average to approximate the expected loss:

(cid:2)(cid:96)(f (z; w)  y)(cid:3) where µ  Σ are the
(cid:2)(cid:96)(f (z; w)  y)(cid:3) cannot be analytically computed. In such cases  we use

Ez∼N (µ Σ;θ)

(cid:2)(cid:96)(f (z; w)  y)(cid:3) ≈ 1

S(cid:88)

s=1

S

(cid:96)(f (zs; w)  y)  where zs ∼ N (µ  Σ; θ).

(5)

To learn the parameters of both the classiﬁer w and the Gaussian process regression model θ jointly
under the expected loss  we need to be able to compute the gradient of the expectation given in (5).
1 The notation N (µ  Σ; θ) explicitly expresses that both µ and Σ are functions of the GP parameters θ.
2 For example  the loss of the expected output of the classiﬁer (cid:96)(Ez∼N (µ Σ;θ)[f (z; w)]  y).

Besides  they are also functions of S = (t  v) as shown in (1) and (2).

3

To achieve this  we reparameterize the Gaussian random variable using the identity z = µ + Rξ
where ξ ∼ N (0  I) and R satisﬁes Σ = RR(cid:62) [11]. The gradients under this reparameterization
are given below  both of which can be approximated using Monte Carlo sampling as in (5). We will
focus on efﬁciently computing the gradient shown in (7) since we assume that the gradient of the
base classiﬁer f (z; w) can be computed efﬁciently.

(cid:2)(cid:96)(f (z; w)  y)(cid:3) = Eξ∼N (0 I)
(cid:2)(cid:96)(f (z; w)  y)(cid:3) = Eξ∼N (0 I)

(cid:20) ∂
(cid:34)(cid:88)

∂w

∂
∂w

∂
∂θ

Ez∼N (µ Σ;θ)

Ez∼N (µ Σ;θ)

(cid:21)

(cid:96)(f (z; w)  y)

∂(cid:96)(f (z; w)  y)

i

∂zi

(cid:35)

∂zi
∂θ

(6)

(7)

There are several choices for R that satisfy Σ = RR(cid:62). One common choice of R is the Cholesky
factor  a lower triangular matrix  which can be computed using Cholesky decomposition in O(d3) for
a d × d covariance matrix Σ [7]. We instead use the symmetric matrix square root R = Σ1/2. We
will show that this particular choice of R leads to an efﬁcient and scalable approximation algorithm
in Section 4.2.

4 Fast sampling from posterior Gaussian processes

The computation required by the GP adapter is dominated by the time needed to draw samples from
the marginal GP posterior using z = µ + Σ1/2ξ. In Section 2 we noted that the time complexity of
exactly computing the posterior mean µ and covariance Σ is O(n3 + nd) and O(n3 + n2d + nd2) 
respectively. Once we have both µ and Σ we still need to compute the square root of Σ  which
requires an additional O(d3) time to compute exactly. In this section  we show how to efﬁciently
generate samples of z.

Ka b ≈ (cid:101)Ka b = WaKu uW(cid:62)

4.1 Structured kernel interpolation for approximating GP posterior means

and Nickisch [25] is to approximate a stationary kernel matrix Ka b by the approximate kernel (cid:101)Ka b

The main idea of the structured kernel interpolation (SKI) framework recently proposed by Wilson
deﬁned below where u = [u1  . . .   um](cid:62) is a collection of evenly-spaced inducing points.

b .

(8)
Letting p = |a| and q = |b|  Wa ∈ Rp×m is a sparse interpolation matrix where each row
contains only a small number of non-zero entries. We use local cubic convolution interpolation
(cubic interpolation for short) [10] as suggested in Wilson and Nickisch [25]. Each row of the
interpolation matrices Wa  Wb has at most four non-zero entries. Wilson and Nickisch [25] showed
that when the kernel is locally smooth (under the resolution of u)  cubic interpolation results in
accurate approximation. This can be justiﬁed as follows: with cubic interpolation  the SKI kernel is
essentially the two-dimensional cubic interpolation of Ka b using the exact regularly spaced samples

stored in Ku u  which corresponds to classical bicubic convolution. In fact  we can show that (cid:101)Ka b

asymptotically converges to Ka b as m increases by following the derivation in Keys [10].
Plugging the SKI kernel into (1)  the posterior GP mean evaluated at x can be approximated by

(cid:0)WtK−1

u uW(cid:62)

t + σ2I(cid:1)−1

v.

(9)

(cid:0)Kt t + σ2I(cid:1)−1

µ = Kx t

v ≈ WxKu uW(cid:62)

t

The inducing points u are chosen to be evenly-spaced because Ku u forms a symmetric Toeplitz
matrix under a stationary covariance function. A symmetric Toeplitz matrix can be embedded into a
circulant matrix to perform matrix vector multiplication using fast Fourier transforms [7].
t +σ2I)−1v which only
Further  one can use the conjugate gradient method to solve for (WtK−1
involves computing the matrix-vector product (WtK−1
t + σ2I)v. In practice  the conjugate
gradient method converges within only a few iterations. Therefore  approximating the posterior mean
µ using SKI takes only O(n + d + m log m) time to compute. In addition  since a symmetric Toeplitz
matrix Ku u can be uniquely characterized by its ﬁrst column  and Wt can be stored as a sparse
matrix  approximating µ requires only O(n + d + m) space.

u uW(cid:62)

u uW(cid:62)

4

d = Σdj − βjdj−1
αj = d(cid:62)
j d
d = d − αjdj
βj+1 = (cid:107)d(cid:107)
dj+1 = d/βj+1

H = tridiagonal(β  α  β) =



α1 β2
β2 α2 β3
β3 α3
...



...
... βk
βk αk

Algorithm 1: Lanczos method for approximating Σ1/2ξ
Input: covariance matrix Σ  dimension of the Krylov subspace k  random vector ξ
β1 = 0 and d0 = 0
d1 = ξ/(cid:107)ξ(cid:107)
for j = 1 to k do

D = [d1  . . .   dk]
H = tridiagonal(β  α  β)
return (cid:107)ξ(cid:107)DH1/2e1

4.2 The Lanczos method for covariance square root-vector products

// e1 = [1  0  . . .   0](cid:62)

With the SKI techniques  although we can efﬁciently approximate the posterior mean µ  computing
Σ1/2ξ is still challenging. If computed exactly  it takes O(n3 + n2d + nd2) time to compute Σ and
O(d3) time to take the square root. To overcome the bottleneck  we apply the SKI kernel to the
Lanczos method  one of the Krylov subspace approximation methods  to speed up the computation
of Σ1/2ξ as shown in Algorithm 1. The advantage of the Lanczos method is that neither Σ nor Σ1/2
needs to be computed explicitly. Like the conjugate gradient method  another example of the Krylov
subspace method  it only requires the computation of matrix-vector products with Σ as the matrix.
The idea of the Lanczos method is to approximate Σ1/2ξ in the Krylov subspace Kk(Σ  ξ) =
span{ξ  Σξ  . . .   Σk−1ξ}. The iteration in Algorithm 1  usually referred to the Lanczos process 
essentially performs the Gram-Schmidt process to transform the basis {ξ  Σξ  . . .   Σk−1ξ} into an
orthonormal basis {d1  . . .   dk} for the subspace Kk(Σ  ξ).
The optimal approximation of Σ1/2ξ in the Krylov subspace Kk(Σ  ξ) that minimizes the (cid:96)2-norm
of the error is the orthogonal projection of Σ1/2ξ onto Kk(Σ  ξ) as y∗ = DD(cid:62)Σ1/2ξ. Since we
choose d1 = ξ/(cid:107)ξ(cid:107)  the optimal projection can be written as y∗ = (cid:107)ξ(cid:107)DD(cid:62)Σ1/2De1 where
e1 = [1  0  . . .   0](cid:62) is the ﬁrst column of the identify matrix.
One can show that the tridiagonal matrix H deﬁned in Algorithm 1 satisﬁes D(cid:62)ΣD = H [20]. Also 
we have D(cid:62)Σ1/2D ≈ (D(cid:62)ΣD)1/2 since the eigenvalues of H approximate the extremal eigenvalues
of Σ [19]. Therefore we have y∗ = (cid:107)ξ(cid:107)DD(cid:62)Σ1/2De1 ≈ (cid:107)ξ(cid:107)DH1/2e1.
The error bound of the Lanczos method is analyzed in Ili´c et al. [9]. Alternatively one can show that
the Lanczos approximation converges superlinearly [16]. In practice  for a d × d covariance matrix
Σ  the approximation is sufﬁcient for our sampling purpose with k (cid:28) d. As H is now a k × k matrix 
we can use any standard method to compute its square root in O(k3) time [2]  which is considered
O(1) when k is chosen to be a small constant. Now the computation of the Lanczos method for
approximating Σ1/2ξ is dominated by the matrix-vector product Σd during the Lanczos process.
Here we apply the SKI kernel trick again to efﬁciently approximate Σd by

Σd ≈ WxKu uW(cid:62)

x d − WxKu uW(cid:62)

t

(10)
Similar to the posterior mean  Σd can be approximated in O(n + d + m log m) time and linear space.
Therefore  for k = O(1) basis vectors  the entire Algorithm 1 takes O(n + d + m log m) time and
O(n + d + m) space  which is also the complexity to draw a sample from the posterior GP.
To reduce the variance when estimating the expected loss (5)  we can draw multiple samples from the
posterior GP: {Σ1/2ξs}s=1 ... S where ξs ∼ N (0  I). Since all of the samples are associated with the
same covariance matrix Σ  we can use the block Lanczos process [8]  an extension to the single-vector
Lanczos method presented in Algorithm 1  to simultaneously approximate Σ1/2Ξ for all S random

x d.

(cid:0)WtKu uW(cid:62)

t + σ2I(cid:1)−1

WtKu uW(cid:62)

5

vectors Ξ = [ξ1  . . .   ξS]. Similarly  during the block Lanczos process  we use the block conjugate
gradient method [6  5] to simultaneously solve the linear equation (WtKu uW(cid:62)
t + σ2I)−1α for
multiple α.

5 End-to-end learning with the GP adapter

The most common way to train GP parameters is through maximizing the marginal likelihood [17]

v(cid:62)(cid:0)Kt t + σ2I(cid:1)−1

log p(v|t  θ) = − 1
2

log(cid:12)(cid:12)Kt t + σ2I(cid:12)(cid:12) − n

2

v − 1
2

log 2π.

(11)

If we follow this criterion  training the UAC framework becomes a two-stage procedure: ﬁrst we
learn GP parameters by maximizing the marginal likelihood. We then compute µ and Σ given each
time series S and the learned GP parameters θ
∗. Both µ and Σ are then ﬁxed and used to train the
classiﬁer using (6).
In this section  we describe how to instead train the GP parameters discriminatively end-to-end using
backpropagation. As mentioned in Section 3  we train the UAC framework by jointly optimizing the
GP parameters θ and the parameters of the classiﬁer w according to (6) and (7).
The most challenging part in (7) is to compute ∂z = ∂µ + ∂(Σ1/2ξ).3 For ∂µ  we can derive the
gradient of the approximating posterior mean (9) as given in Appendix A. Note that the gradient ∂µ
can be approximated efﬁciently by repeatedly applying fast Fourier transforms and the conjugate
gradient method in the same time and space complexity as computing (9).
On the other hand  ∂(Σ1/2ξ) can be approximated by backpropagating through the Lanczos method
described in Algorithm 1. To carry out backpropagation  all operations in the Lanczos method must
be differentiable. For the approximation of Σd during the Lanczos process  we can similarly compute
the gradient of (10) efﬁciently using the SKI techniques as in computing ∂µ (see Appendix A).
The gradient ∂H1/2 for the last step of Algorithm 1 can be derived as follows. From H = H1/2H1/2 
we have ∂H = (∂H1/2)H1/2 + H1/2(∂H1/2). This is known as the Sylvester equation  which has
the form of AX + XB = C where A  B  C are matrices and X is the unknown matrix to solve
for. We can compute the gradient ∂H1/2 by solving the Sylvester equation using the Bartels-Stewart
algorithm [1] in O(k3) time for a k × k matrix H  which is considered O(1) for a small constant k.
Overall  training the GP adapter using stochastic optimization with the aforementioned approach
takes O(n + d + m log m) time and O(n + d + m) space for m inducing points  n observations in
the time series  and d features generated by the GP adapter.

6 Related work

The recently proposed mixtures of expected Gaussian kernels (MEG) [13] for classiﬁcation of
irregular time series is probably the closest work to ours. The random feature representation of the

(cid:2)cos(w(cid:62)

i z + bi)(cid:3)  which the algorithm described

MEG kernel is in the form of(cid:112)2/m Ez∼N (µ Σ)
(cid:112)2/m exp(−w(cid:62)

i Σwi and w(cid:62)

i Σwi/2) cos(w(cid:62)

in Section 4 can be applied to directly. However  by exploiting the spectral property of Gaussian
kernels  the expected random feature of the MEG kernel is shown to be analytically computable by
i µ + bi). With the SKI techniques  we can efﬁciently approximate
both w(cid:62)
i µ in the same time and space complexity as the GP adapter. Moreover  the
random features of the MEG kernel can be viewed as a stochastic layer in the classiﬁcation network 
with no trainable parameters. All {wi  bi}i=1 ... m are randomly initialized once in the beginning and
associated with the output of the GP adapter in a nonlinear way described above.
Moreover  the MEG kernel classiﬁcation is originally a two-stage method: one ﬁrst estimates the
GP parameters by maximizing the marginal likelihood and then uses the optimized GP parameters
to compute the MEG kernel for classiﬁcation. Since the random feature is differentiable  with the
approximation of ∂µ and ∂(Σd) described in Section 5  we can form a similar classiﬁcation network
that can be efﬁciently trained end-to-end using the GP adapter. In Section 7.2  we will show that
training the MEG kernel end-to-end leads to better classiﬁcation performance.

3 For brevity  we drop 1/∂θ from the gradient notation in this section.

6

Figure 1: Left: Sample approximation error versus the number of inducing points. Middle: Sample
approximation error versus the number of Lanczos iterations. Right: Running time comparisons (in
seconds). BP denotes computing the gradient of the sample using backpropagation.

7 Experiments

In this section  we present experiments and results exploring several facets of the GP adapter
framework including the quality of the approximations and the classiﬁcation performance of the
framework when combined with different base classiﬁers.

7.1 Quality of GP sampling approximations

The key to scalable learning with the GP adapter relies on both fast and accurate approximation
for drawing samples from the posterior GP. To assess the approximation quality  we ﬁrst generate
a synthetic sparse and irregularly-sampled time series S by sampling from a zero-mean Gaussian
process at random time points. We use the squared exponential kernel k(ti  tj) = a exp(−b(ti− tj)2)
denote our approximation of z = µ + Σ1/2ξ. In this experiment  we set the output size z to be |S| 

with randomly chosen hyperparameters. We then infer µ and Σ at some reference x given S. Let(cid:101)z
that is  d = n. We evaluate the approximation quality by assessing the error (cid:107)(cid:101)z − z(cid:107) computed with

a ﬁxed random vector ξ.
The leftmost plot in Figure 1 shows the approximation error under different numbers of inducing
points m with k = 10 Lanczos iterations. The middle plot compares the approximation error as the
number of Lanczos iterations k varies  with m = 256 inducing points. These two plots show that the
approximation error drops as more inducing points and Lanczos iterations are used. In both plots 
the three lines correspond to different sizes for z: 1000 (bottom line)  2000 (middle line)  3000 (top
line). The separation between the curves is due to the fact that the errors are compared under the
same number of inducing points. Longer time series leads to lower resolution of the inducing points
and hence the higher approximation error.
Note that the approximation error comes from both the cubic interpolation and the Lanczos method.
Therefore  to achieve a certain normalized approximation error across different data sizes  we should
simultaneously use more inducing points and Lanczos iterations as the data grows. In practice  we
ﬁnd that k ≥ 3 is sufﬁcient for estimating the expected loss for classiﬁcation.
The rightmost plot in Figure 1 compares the time to draw a sample using exact computation versus
the approximation method described in Section 4 (exact and Lanczos in the ﬁgure). We also compare
the time to compute the gradient with respect to the GP parameters by both the exact method and
the proposed approximation (exact BP and Lanczos BP in the ﬁgure) because this is the actual
computation carried out during training. In this part of the experiment  we use k = 10 and m = 256.
The plot shows that Lanczos approximation with the SKI kernel yields speed-ups of between 1 and
3 orders of magnitude. Interestingly  for the exact approach  the time for computing the gradient
roughly doubles the time of drawing samples. (Note that time is plotted in log scale.) This is because
computing gradients requires both forward and backward propagation  whereas drawing samples
corresponds to only the forward pass. Both the forward and backward passes take roughly the same
computation in the exact case. However  the gap is relatively larger for the approximation approach
due to the recursive relationship of the variables in the Lanczos process. In particular  dj is deﬁned
recursively in terms of all of d1  . . .   dj−1  which makes the backpropagation computation more
complicated than the forward pass.

7

345678910log2(#inducingpoints)10−310−210−1100101error05101520#Lanczositerations10−310−210−1100errorlengthoftimeseries:10002000300051015202530lengthoftimeseries(×100)100101102103time(logscale)exactexactBPLanczosLanczosBPTable 1: Comparison of classiﬁcation accuracy (in percent). IMP and UAC refer to the loss functions
for training described in Section 3.1  and we use IMP predictions throughout. Although not belonging
to the UAC framework  we put the MEG kernel in UAC since it is also uncertainty-aware.

Marginal likelihood

End-to-end

IMP
UAC
IMP
UAC

LogReg
77.90
78.23
79.12
79.24

MLP
85.49
87.05
86.49
87.95

ConvNet

MEG kernel

87.61
88.17
89.84
91.41

–

84.82

–

86.61

7.2 Classiﬁcation with GP adapter

In this section  we evaluate the performance of classifying sparse and irregularly-sampled time series
using the UAC framework. We test the framework on the uWave data set 4 a collection of gesture
samples categorized into eight gesture patterns [14]. The data set has been split into 3582 training
instances and 896 test instances. Each time series contains 945 fully observed samples. Following
the data preparation procedure in the MEG kernel work [13]  we randomly sample 10% of the
observations from each time series to simulate the sparse and irregular sampling scenario. In this
experiment  we use the squared exponential covariance function k(ti  tj) = a exp(−b(ti − tj)2) for
a  b > 0. Together with the independent noise parameter σ2 > 0  the GP parameters are {a  b  σ2}.
To bypass the positive constraints on the GP parameters  we reparameterize them by {α  β  γ} such
that a = eα  b = eβ  and σ2 = eγ.
To demonstrate that the GP adapter is capable of working with various classiﬁers  we use the UAC
framework to train three different classiﬁers: a multi-class logistic regression (LogReg)  a fully-
connected feedforward network (MLP)  and a convolutional neural network (ConvNet). The detailed
architecture of each model is described in Appendix C.
We use m = 256 inducing points  d = 254 features output by the GP adapter  k = 5 Lanczos
iterations  and S = 10 samples. We split the training set into two partitions: 70% for training and
30% for validation. We jointly train the classiﬁer with the GP adapter using stochastic gradient
descent with Nesterov momentum. We apply early stopping based on the validation set. We also
compare to classiﬁcation with the MEG kernel implemented using our GP adapter as described in
Section 6. We use 1000 random features trained with multi-class logistic regression.
Table 1 shows that among all three classiﬁers  training GP parameters discriminatively always leads
to better accuracy than maximizing the marginal likelihood. This claim also holds for the results
using the MEG kernel. Further  taking the uncertainty into account by sampling from the posterior
GP always outperforms training using only the posterior means. Finally  we can also see that the
classiﬁcation accuracy improves as the model gets deeper.

8 Conclusions and future work

We have presented a general framework for classifying sparse and irregularly-sampled time series
and have shown how to scale up the required computations using a new approach to generating
approximate samples. We have validated the approximation quality  the computational speed-ups 
and the beneﬁt of the proposed approach relative to existing baselines.
There are many promising directions for future work including investigating more complicated
covariance functions like the spectral mixture kernel [24]  different classiﬁers including the encoder
LSTM [23]  and extending the framework to multi-dimensional time series and GPs with multi-
dimensional index sets (e.g.  for spatial data). Lastly  the GP adapter can also be applied to other
problems such as dimensionality reduction by combining it with an autoencoder.

Acknowledgements

This work was supported by the National Science Foundation under Grant No. 1350522.

4 The data set UWaveGestureLibraryAll is available at http://timeseriesclassification.com.

8

References
[1] Richard H. Bartels and GW Stewart. Solution of the matrix equation AX + XB = C. Communications

of the ACM  15(9):820–826  1972.

[2] Åke Björck and Sven Hammarling. A Schur method for the square root of a matrix. Linear algebra and its

applications  52:127–140  1983.

[3] Edmond Chow and Yousef Saad. Preconditioned krylov subspace methods for sampling multivariate

gaussian distributions. SIAM Journal on Scientiﬁc Computing  36(2):A588–A608  2014.

[4] J.S. Clark and O.N. Bjørnstad. Population time series: process variability  observation errors  missing

values  lags  and hidden states. Ecology  85(11):3140–3150  2004.

[5] Augustin A Dubrulle. Retooling the method of block conjugate gradients. Electronic Transactions on

Numerical Analysis  12:216–233  2001.

[6] YT Feng  DRJ Owen  and D Peri´c. A block conjugate gradient method applied to linear systems with

multiple right-hand sides. Computer methods in applied mechanics and engineering  1995.
[7] Gene H Golub and Charles F Van Loan. Matrix computations  volume 3. JHU Press  2012.
[8] Gene Howard Golub and Richard Underwood. The block Lanczos method for computing eigenvalues.

Mathematical software  3:361–377  1977.

[9] M Ili´c  Ian W Turner  and Daniel P Simpson. A restarted Lanczos approximation to functions of a

symmetric matrix. IMA journal of numerical analysis  page drp003  2009.

[10] Robert G Keys. Cubic convolution interpolation for digital image processing. Acoustics  Speech and Signal

Processing  IEEE Transactions on  29(6):1153–1160  1981.

[11] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Proceedings of the 2nd Interna-

tional Conference on Learning Representations (ICLR)  2014.

[12] Yann LeCun  Fu Jie Huang  and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of Computer Vision and Pattern Recognition (CVPR)  2004.
[13] Steven Cheng-Xian Li and Benjmain M. Marlin. Classiﬁcation of sparse and irregularly sampled time
series with mixtures of expected Gaussian kernels and random features. In 31st Conference on Uncertainty
in Artiﬁcial Intelligence  2015.

[14] Jiayang Liu  Lin Zhong  Jehan Wickramasuriya  and Venu Vasudevan. uwave: Accelerometer-based

personalized gesture recognition and its applications. Pervasive and Mobile Computing  2009.

[15] Benjamin M. Marlin  David C. Kale  Robinder G. Khemani  and Randall C. Wetzel. Unsupervised pattern
discovery in electronic health care data using probabilistic clustering models. In Proceedings of the 2nd
ACM SIGHIT International Health Informatics Symposium  pages 389–398  2012.
[16] Beresford N Parlett. The symmetric eigenvalue problem  volume 7. SIAM  1980.
[17] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006.

[18] T. Ruf. The lomb-scargle periodogram in biological rhythm research: analysis of incomplete and unequally

spaced time-series. Biological Rhythm Research  30(2):178–201  1999.

[19] Yousef Saad. On the rates of convergence of the Lanczos and the block-Lanczos methods. SIAM Journal

on Numerical Analysis  17(5):687–706  1980.

[20] Yousef Saad. Iterative methods for sparse linear systems. Siam  2003.
[21] Jeffrey D Scargle. Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of

unevenly spaced data. The Astrophysical Journal  263:835–853  1982.

[22] M. Schulz and K. Stattegger. Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series.

Computers & Geosciences  23(9):929–945  1997.

[23] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural networks. In

Advances in neural information processing systems  pages 3104–3112  2014.

[24] Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern discovery and

extrapolation. In Proceedings of the 30th International Conference on Machine Learning  2013.

[25] Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian
processes (KISS-GP). In Proceedings of the 32nd International Conference on Machine Learning  2015.

9

,Jerry Zhu
Damien Garreau
Rémi Lajugie
Sylvain Arlot
Francis Bach
Steven Cheng-Xian Li
Benjamin Marlin