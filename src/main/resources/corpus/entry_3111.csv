2016,Learning to Communicate with Deep Multi-Agent Reinforcement Learning,We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments  agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks  we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning  while the latter exploits the fact that  during learning  agents can backpropagate error derivatives through (noisy) communication channels. Hence  this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.,Learning to Communicate with

Deep Multi-Agent Reinforcement Learning

Jakob N. Foerster1 †

jakob.foerster@cs.ox.ac.uk

Yannis M. Assael1 †

yannis.assael@cs.ox.ac.uk

Nando de Freitas1 2 3

nandodefreitas@google.com

Shimon Whiteson1

shimon.whiteson@cs.ox.ac.uk

1University of Oxford  United Kingdom

2Canadian Institute for Advanced Research  CIFAR NCAP Program

3Google DeepMind

Abstract

We consider the problem of multiple agents sensing and acting in environments
with the goal of maximising their shared utility. In these environments  agents must
learn communication protocols in order to share information that is needed to solve
the tasks. By embracing deep neural networks  we are able to demonstrate end-
to-end learning of protocols in complex environments inspired by communication
riddles and multi-agent computer vision problems with partial observability. We
propose two approaches for learning in these domains: Reinforced Inter-Agent
Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses
deep Q-learning  while the latter exploits the fact that  during learning  agents can
backpropagate error derivatives through (noisy) communication channels. Hence 
this approach uses centralised learning but decentralised execution. Our experi-
ments introduce new environments for studying the learning of communication
protocols and present a set of engineering innovations that are essential for success
in these domains.

1

Introduction

How language and communication emerge among intelligent agents has long been a topic of intense
debate. Among the many unresolved questions are: Why does language use discrete structures?
What role does the environment play? What is innate and what is learned? And so on. Some of the
debates on these questions have been so ﬁery that in 1866 the French Academy of Sciences banned
publications about the origin of human language.
The rapid progress in recent years of machine learning  and deep learning in particular  opens the
door to a new perspective on this debate. How can agents use machine learning to automatically
discover the communication protocols they need to coordinate their behaviour? What  if anything 
can deep learning offer to such agents? What insights can we glean from the success or failure of
agents that learn to communicate?
In this paper  we take the ﬁrst steps towards answering these questions. Our approach is programmatic:
ﬁrst  we propose a set of multi-agent benchmark tasks that require communication; then  we formulate
several learning algorithms for these tasks; ﬁnally  we analyse how these algorithms learn  or fail to
learn  communication protocols for the agents.

†These authors contributed equally to this work.
30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The tasks that we consider are fully cooperative  partially observable  sequential multi-agent decision
making problems. All the agents share the goal of maximising the same discounted sum of rewards.
While no agent can observe the underlying Markov state  each agent receives a private observation
correlated with that state. In addition to taking actions that affect the environment  each agent can
also communicate with its fellow agents via a discrete limited-bandwidth channel. Due to the partial
observability and limited channel capacity  the agents must discover a communication protocol that
enables them to coordinate their behaviour and solve the task.
We focus on settings with centralised learning but decentralised execution. In other words  com-
munication between agents is not restricted during learning  which is performed by a centralised
algorithm; however  during execution of the learned policies  the agents can communicate only via the
limited-bandwidth channel. While not all real-world problems can be solved in this way  a great many
can  e.g.  when training a group of robots on a simulator. Centralised planning and decentralised
execution is also a standard paradigm for multi-agent planning [1  2].
To address this setting  we formulate two approaches. The ﬁrst  reinforced inter-agent learning
(RIAL)  uses deep Q-learning [3] with a recurrent network to address partial observability. In one
variant of this approach  which we refer to as independent Q-learning  the agents each learn their
own network parameters  treating the other agents as part of the environment. Another variant trains
a single network whose parameters are shared among all agents. Execution remains decentralised  at
which point they receive different observations leading to different behaviour.
The second approach  differentiable inter-agent learning (DIAL)  is based on the insight that cen-
tralised learning affords more opportunities to improve learning than just parameter sharing. In
particular  while RIAL is end-to-end trainable within an agent  it is not end-to-end trainable across
agents  i.e.  no gradients are passed between agents. The second approach allows real-valued mes-
sages to pass between agents during centralised learning  thereby treating communication actions as
bottleneck connections between agents. As a result  gradients can be pushed through the communica-
tion channel  yielding a system that is end-to-end trainable even across agents. During decentralised
execution  real-valued messages are discretised and mapped to the discrete set of communication
actions allowed by the task. Because DIAL passes gradients from agent to agent  it is an inherently
deep learning approach.
Experiments on two benchmark tasks  based on the MNIST dataset and a well known riddle  show 
not only can these methods solve these tasks  they often discover elegant communication protocols
along the way. To our knowledge  this is the ﬁrst time that either differentiable communication or
reinforcement learning with deep neural networks has succeeded in learning communication protocols
in complex environments involving sequences and raw images. The results also show that deep
learning  by better exploiting the opportunities of centralised learning  is a uniquely powerful tool
for learning such protocols. Finally  this study advances several engineering innovations that are
essential for learning communication protocols in our proposed benchmarks.

2 Related Work

Research on communication spans many ﬁelds  e.g. linguistics  psychology  evolution and AI. In AI 
it is split along a few axes: a) predeﬁned or learned communication protocols  b) planning or learning
methods  c) evolution or RL  and d) cooperative or competitive settings.
Given the topic of our paper  we focus on related work that deals with the cooperative learning of
communication protocols. Out of the plethora of work on multi-agent RL with communication 
e.g.  [4–7]  only a few fall into this category. Most assume a pre-deﬁned communication protocol 
rather than trying to learn protocols. One exception is the work of Kasai et al. [7]  in which
tabular Q-learning agents have to learn the content of a message to solve a predator-prey task with
communication. Another example of open-ended communication learning in a multi-agent task is
given in [8]. Here evolutionary methods are used for learning the protocols which are evaluated
on a similar predator-prey task. Their approach uses a ﬁtness function that is carefully designed to
accelerate learning. In general  heuristics and handcrafted rules have prevailed widely in this line of
research. Moreover  typical tasks have been necessarily small so that global optimisation methods 
such as evolutionary algorithms  can be applied. The use of deep representations and gradient-based
optimisation as advocated in this paper is an important departure  essential for scalability and further

2

progress. A similar rationale is provided in [9]  another example of making an RL problem end-to-end
differentiable.
Unlike the recent work in [10]  we consider discrete communication channels. One of the key
components of our methods is the signal binarisation during the decentralised execution. This is
related to recent research on ﬁtting neural networks in low-powered devices with memory and
computational limitations using binary weights  e.g. [11]  and previous works on discovering binary
codes for documents [12].

3 Background

i ). Here  θ−

i

i

− Q(s  u; θi))2]  at each iteration i  with target yDQN

Deep Q-Networks (DQN). In a single-agent  fully-observable  RL setting [13]  an agent observes the
current state st ∈ S at each discrete time step t  chooses an action ut ∈ U according to a potentially
stochastic policy π  observes a reward signal rt  and transitions to a new state st+1. Its objective
is to maximise an expectation over the discounted return  Rt = rt + γrt+1 + γ2rt+2 + ···   where
rt is the reward received at time t and γ ∈ [0  1] is a discount factor. The Q-function of a policy π
is Qπ(s  u) = E [Rt|st = s  ut = u]. The optimal action-value function Q∗(s  u) = maxπ Qπ(s  u)
obeys the Bellman optimality equation Q∗(s  u) = Es(cid:48) [r + γ maxu(cid:48) Q∗(s(cid:48)  u(cid:48)) | s  u]. Deep Q-
learning [3] uses neural networks parameterised by θ to represent Q(s  u; θ). DQNs are optimised
by minimising: Li(θi) = Es u r s(cid:48)[(yDQN
=
r + γ maxu(cid:48) Q(s(cid:48)  u(cid:48); θ−
i are the parameters of a target network that is frozen for a number
of iterations while updating the online network Q(s  u; θi). The action u is chosen from Q(s  u; θi)
by an action selector  which typically implements an -greedy policy that selects the action that
maximises the Q-value with a probability of 1 −  and chooses randomly with a probability of .
DQN also uses experience replay: during learning  the agent builds a dataset of episodic experiences
and is then trained by sampling mini-batches of experiences.
Independent DQN. DQN has been extended to cooperative multi-agent settings  in which each agent
a observes the global st  selects an individual action ua
t   and receives a team reward  rt  shared
among all agents. Tampuu et al. [14] address this setting with a framework that combines DQN
with independent Q-learning  in which each agent a independently and simultaneously learns its
own Q-function Qa(s  ua; θa
i ). While independent Q-learning can in principle lead to convergence
problems (since one agent’s learning makes the environment appear non-stationary to other agents) 
it has a strong empirical track record [15  16]  and was successfully applied to two-player pong.
Deep Recurrent Q-Networks. Both DQN and independent DQN assume full observability  i.e.  the
agent receives st as input. By contrast  in partially observable environments  st is hidden and the
agent receives only an observation ot that is correlated with st  but in general does not disambiguate
it. Hausknecht and Stone [17] propose deep recurrent Q-networks (DRQN) to address single-agent 
partially observable settings. Instead of approximating Q(s  u) with a feed-forward network  they
approximate Q(o  u) with a recurrent neural network that can maintain an internal state and aggregate
observations over time. This can be modelled by adding an extra input ht−1 that represents the hidden
state of the network  yielding Q(ot  ht−1  u). For notational simplicity  we omit the dependence of Q
on θ.

4 Setting

In this work  we consider RL problems with both multiple agents and partial observability. All the
agents share the goal of maximising the same discounted sum of rewards Rt. While no agent can
observe the underlying Markov state st  each agent a receives a private observation oa
t correlated with
t ∈ U that affects the environment 
st. In every time-step t  each agent selects an environment action ua
t ∈ M that is observed by other agents but has no direct impact on the
and a communication action ma
environment or reward. We are interested in such settings because it is only when multiple agents and
partial observability coexist that agents have the incentive to communicate. As no communication
protocol is given a priori  the agents must develop and agree upon such a protocol to solve the task.
Since protocols are mappings from action-observation histories to sequences of messages  the space
of protocols is extremely high-dimensional. Automatically discovering effective protocols in this
space remains an elusive challenge. In particular  the difﬁculty of exploring this space of protocols
is exacerbated by the need for agents to coordinate the sending and interpreting of messages. For

3

example  if one agent sends a useful message to another agent  it will only receive a positive reward
if the receiving agent correctly interprets and acts upon that message. If it does not  the sender will be
discouraged from sending that message again. Hence  positive rewards are sparse  arising only when
sending and interpreting are properly coordinated  which is hard to discover via random exploration.
We focus on settings where communication between agents is not restricted during centralised
learning  but during the decentralised execution of the learned policies  the agents can communicate
only via a limited-bandwidth channel.

5 Methods

In this section  we present two approaches for learning communication protocols.

5.1 Reinforced Inter-Agent Learning

t−1  ha

u and Qa

t   ma(cid:48)
t as well as messages from other agents ma(cid:48)

The most straightforward approach  which we call reinforced inter-agent learning (RIAL)  is to
combine DRQN with independent Q-learning for action and communication selection. Each agent’s
Q-network represents Qa(oa
t−1  ua)  which conditions on that agent’s individual hidden
state ha
t−1 and observation oa
t−1.
To avoid needing a network with |U||M| outputs  we split the network into Qa
m  the Q-values
for the environment and communication actions  respectively. Similarly to [18]  the action selector
t and ma
separately picks ua
t from Qu and Qm  using an -greedy policy. Hence  the network requires
only |U| + |M| outputs and action selection requires maximising over U and then over M  but not
maximising over U × M.
Both Qu and Qm are trained using DQN with the following two modiﬁcations  which were found to be
essential for performance. First  we disable experience replay to account for the non-stationarity that
occurs when multiple agents learn concurrently  as it can render experience obsolete and misleading.
Second  to account for partial observability  we feed in the actions u and m taken by each agent
as inputs on the next time-step. Figure 1(a) shows how information ﬂows between agents and the
environment  and how Q-values are processed by the action selector in order to produce the action 
t   and message ma
t . Since this approach treats agents as independent networks  the learning phase is
ua
not centralised  even though our problem setting allows it to be. Consequently  the agents are treated
exactly the same way during decentralised execution as during learning.

(a) RIAL - RL based communication

(b) DIAL - Differentiable communication

Figure 1: The bottom and top rows represent the communication ﬂow for agent a1 and agent a2 
respectively. In RIAL (a)  all Q-values are fed to the action selector  which selects both environment
and communication actions. Gradients  shown in red  are computed using DQN for the selected
action and ﬂow only through the Q-network of a single agent. In DIAL (b)  the message ma
t bypasses
the action selector and instead is processed by the DRU (Section 5.2) and passed as a continuous
value to the next C-network. Hence  gradients ﬂow across agents  from the recipient to the sender.
For simplicity  at each time step only one agent is highlighted  while the other agent is greyed out.

Parameter Sharing. RIAL can be extended to take advantage of the opportunity for centralised
learning by sharing parameters among the agents. This variation learns only one network  which is
used by all agents. However  the agents can still behave differently because they receive different

4

ot1ut+12Q-Netut1Q-NetActionSelectmt1mt+12Agent 1Agent 2ot+12ActionSelectmt-12EnvironmentQ-NetActionSelectQ-NetActionSelectt+1tot1ut+12C-Netut1C-NetActionSelectDRUmt1mt+12Agent 1Agent 2ot+12ActionSelectEnvironmentC-NetActionSelectC-NetActionSelectDRUt+1tobservations and thus evolve different hidden states. In addition  each agent receives its own index
a as input  allowing it to specialise. The rich representations in deep Q-networks can facilitate
the learning of a common policy while also allowing for specialisation. Parameter sharing also
dramatically reduces the number of parameters that must be learned  thereby speeding learning.
Under parameter sharing  the agents learn two Q-functions Qu(oa
t−1  a  ua
t )
and Qm(oa
t ). During decentralised execution  each agent uses its
own copy of the learned network  evolving its own hidden state  selecting its own actions  and
communicating with other agents only through the communication channel.

t−1  a  ua

t   ma(cid:48)

t   ma(cid:48)

t−1  ha

t−1  ua

t−1  ma

t−1  ha

t−1  ua

t−1  ma

5.2 Differentiable Inter-Agent Learning

While RIAL can share parameters among agents  it still does not take full advantage of centralised
learning. In particular  the agents do not give each other feedback about their communication actions.
Contrast this with human communication  which is rich with tight feedback loops. For example 
during face-to-face interaction  listeners send fast nonverbal queues to the speaker indicating the level
of understanding and interest. RIAL lacks this feedback mechanism  which is intuitively important
for learning communication protocols.
To address this limitation  we propose differentiable inter-agent learning (DIAL). The main insight
behind DIAL is that the combination of centralised learning and Q-networks makes it possible  not
only to share parameters but to push gradients from one agent to another through the communication
channel. Thus  while RIAL is end-to-end trainable within each agent  DIAL is end-to-end trainable
across agents. Letting gradients ﬂow from one agent to another gives them richer feedback  reducing
the required amount of learning by trial and error  and easing the discovery of effective protocols.
DIAL works as follows: during centralised learning  communication actions are replaced with direct
connections between the output of one agent’s network and the input of another’s. Thus  while
the task restricts communication to discrete messages  during learning the agents are free to send
real-valued messages to each other. Since these messages function as any other network activation 
gradients can be passed back along the channel  allowing end-to-end backpropagation across agents.
In particular  the network  which we call a C-Net  outputs two distinct types of values  as shown in
Figure 1(b)  a) Q(·)  the Q-values for the environment actions  which are fed to the action selector 
and b) ma
t   the real-valued vector message to other agents  which bypasses the action selector and
is instead processed by the discretise/regularise unit (DRU(ma
t )). The DRU regularises it during
centralised learning  DRU(ma
t   σ))  where σ is the standard deviation of the noise
t > 0}.
added to the channel  and discretises it during decentralised execution  DRU(ma
Figure 1 shows how gradients ﬂow differently in RIAL and DIAL. The gradient chains for Qu  in
RIAL and Q  in DIAL  are based on the DQN loss. However  in DIAL the gradient term for m is the
backpropagated error from the recipient of the message to the sender. Using this inter-agent gradient
for training provides a richer training signal than the DQN loss for Qm in RIAL. While the DQN
error is nonzero only for the selected message  the incoming gradient is a |m|-dimensional vector
that can contain more information. It also allows the network to directly adjust messages in order to
minimise the downstream DQN loss  reducing the need for trial and error learning of good protocols.
While we limit our analysis to discrete messages  DIAL naturally handles continuous message spaces 
as they are used anyway during centralised learning. At the same time  DIAL can also scale to large
discrete message spaces  since it learns binary encodings instead of the one-hot encoding in RIAL 
|m| = O(log(|M|). Further algorithmic details and pseudocode are in the supplementary material.

t ) = Logistic(N (ma

t ) = 1{ma

6 Experiments

In this section  we evaluate RIAL and DIAL with and without parameter sharing in two multi-agent
problems and compare it with a no-communication shared-parameter baseline (NoComm). Results
presented are the average performance across several runs  where those without parameter sharing (-
NS)  are represented by dashed lines. Across plots  rewards are normalised by the highest average
reward achievable given access to the true state (Oracle). In our experiments  we use an -greedy
policy with  = 0.05  the discount factor is γ = 1  and the target network is reset every 100 episodes.
To stabilise learning  we execute parallel episodes in batches of 32. The parameters are optimised
using RMSProp [19] with a learning rate of 5 × 10−4. The architecture uses rectiﬁed linear units

5

(ReLU)  and gated recurrent units (GRU) [20]  which have similar performance to long short-term
memory [21] (LSTM) [22]. Unless stated otherwise  we set the standard deviation of noise added to
the channel to σ = 2  which was found to be essential for good performance.1

6.1 Model Architecture

RIAL and DIAL share the same individual model archi-
tecture. For brevity  we describe only the DIAL model
here. As illustrated in Figure 2  each agent consists of a re-
current neural network (RNN)  unrolled for T time-steps 
that maintains an internal state h  an input network for
producing a task embedding z  and an output network for
the Q-values and the messages m. The input for agent a is
deﬁned as a tuple of (oa
t−1  a). The inputs a and
t−1 are passed through lookup tables  and ma(cid:48)
t−1 through
ua
a 1-layer MLP  both producing embeddings of size 128.
t is processed through a task-speciﬁc network that pro-
oa
duces an additional embedding of the same size. The state
embedding is produced by element-wise summation of
these embeddings  za
We found that performance and stability improved when a batch normalisation layer [23]
was used to preprocess mt−1. za
1 t =
t
1 t−1)  which is used to approximate the agent’s action-observation history.
GRU[128  128](za
t   ha
Finally  the output ha
2 t of the top GRU layer  is passed through a 2-layer MLP Qa
t =
MLP[128  128  (|U| + |M|)](ha

t ) + MLP[|M|  128](mt−1) + Lookup(ua
is processed through a 2-layer RNN with GRUs  ha

t−1) + Lookup(a)(cid:1).

t =(cid:0)TaskMLP(oa

Figure 2: DIAL architecture.

t   ma

t   ma(cid:48)

t−1  ua

2 t).

6.2 Switch Riddle

Figure 3: Switch: Every day one pris-
oner gets sent to the interrogation room
where he sees the switch and chooses
from “On”  “Off”  “Tell” and “None”.

The ﬁrst task is inspired by a well-known riddle described
as follows: “One hundred prisoners have been newly
ushered into prison. The warden tells them that starting
tomorrow  each of them will be placed in an isolated cell 
unable to communicate amongst each other. Each day 
the warden will choose one of the prisoners uniformly
at random with replacement  and place him in a central
interrogation room containing only a light bulb with a
toggle switch. The prisoner will be able to observe the
current state of the light bulb. If he wishes  he can toggle
the light bulb. He also has the option of announcing that he believes all prisoners have visited the
interrogation room at some point in time. If this announcement is true  then all prisoners are set free 
but if it is false  all prisoners are executed[...]” [24].
t ∈ {0  1}  which indicates if
Architecture. In our formalisation  at time-step t  agent a observes oa
the agent is in the interrogation room. Since the switch has two positions  it can be modelled as a
t ∈ {“None” “Tell”};
1-bit message  ma
otherwise the only action is “None”. The episode ends when an agent chooses “Tell” or when the
maximum time-step  T   is reached. The reward rt is 0 unless an agent chooses “Tell”  in which
case it is 1 if all agents have been to the interrogation room and −1 otherwise. Following the riddle
deﬁnition  in this experiment ma
t−1 is available only to the agent a in the interrogation room. Finally 
we set the time horizon T = 4n − 6 in order to keep the experiments computationally tractable.
Complexity. The switch riddle poses signiﬁcant protocol learning challenges. At any time-step t 
there are |o|t possible observation histories for a given agent  with |o| = 3: the agent either is not
in the interrogation room or receives one of two messages when it is. For each of these histories 
an agent can chose between 4 = |U||M| different options  so at time-step t  the single-agent policy
space is (|U||M|)
= 43t. The product of all policies for all time-steps deﬁnes the total policy
= 4(3T +1−3)/2  where T is the ﬁnal time-step. The size of the multi-agent

space for an agent:(cid:81) 43t

t . If agent a is in the interrogation room  then its actions are ua

|o|t

1Source code is available at: https://github.com/iassael/learning-to-communicate

6

………………h21az1az2az3azTah11ah12ah13ah1T -1ah22ah23ah2Tah11ah12ah13ah1Tah21ah22ah23ah2T -1aQ1am1a )(Q3am3a )(QTa)(……)m0a (o1a0u a)m2a (o3a2u a)mT-1a (oTaT-1u a   Day 13231OffOnOffOnOffOnDay 2Day 3Day 4Switch:Action:OnNoneNoneTellOffOnPrisoner in IR:(a) Evaluation of n = 3

(b) Evaluation of n = 4

(c) Protocol of n = 3

Figure 4: Switch: (a-b) Performance of DIAL and RIAL  with and without ( -NS) parameter sharing 
and NoComm-baseline  for n = 3 and n = 4 agents. (c) The decision tree extracted for n = 3 to
interpret the communication protocol discovered by DIAL.
policy space grows exponentially in n  the number of agents: 4n(3T +1−3)/2. We consider a setting
where T is proportional to the number of agents  so the total policy space is 4n3O(n). For n = 4  the
size is 4354288. Our approach using DIAL is to model the switch as a continuous message  which is
binarised during decentralised execution.
Experimental results. Figure 4(a) shows our results for n = 3 agents. All four methods learn an
optimal policy in 5k episodes  substantially outperforming the NoComm baseline. DIAL with param-
eter sharing reaches optimal performance substantially faster than RIAL. Furthermore  parameter
sharing speeds both methods. Figure 4(b) shows results for n = 4 agents. DIAL with parameter
sharing again outperforms all other methods. In this setting  RIAL without parameter sharing was
unable to beat the NoComm baseline. These results illustrate how difﬁcult it is for agents to learn the
same protocol independently. Hence  parameter sharing can be crucial for learning to communicate.
DIAL-NS performs similarly to RIAL  indicating that the gradient provides a richer and more robust
source of information. We also analysed the communication protocol discovered by DIAL for n = 3
by sampling 1K episodes  for which Figure 4(c) shows a decision tree corresponding to an optimal
strategy. When a prisoner visits the interrogation room after day two  there are only two options:
either one or two prisoners may have visited the room before. If three prisoners had been  the third
prisoner would have ﬁnished the game. The other options can be encoded via the “On” and “Off”
positions respectively.

6.3 MNIST Games

In this section  we consider two tasks based on the well known MNIST digit classiﬁcation dataset [25].
Colour-Digit MNIST is a two-player
game in which each agent observes the
pixel values of a random MNIST digit in
red or green  while the colour label and
digit value are hidden. The reward consists
of two components that are antisymmetric
in the action  colour  and parity of the dig-
its. As only one bit of information can be
sent  agents must agree to encode/decode
either colour or parity  with parity yielding
greater rewards. The game has two steps;
in the ﬁrst step  both agents send a 1-bit message  in the second step they select a binary action.
Multi-Step MNIST is a grayscale variant that requires agents to develop a communication protocol
that integrates information across 5 time-steps in order to guess each others’ digits. At each step 
the agents exchange a 1-bit message and at he ﬁnal step  t = 5  they are awarded r = 0.5 for each
correctly guessed digit. Further details on both tasks are provided in the supplementary material.
Architecture. The input processing network is a 2-layer MLP TaskMLP[(|c|×28×28)  128  128](oa
t ).
Figure 5 depicts the generalised setting for both games. Our experimental evaluation showed improved
training time using batch normalisation after the ﬁrst layer.

Figure 5: MNIST games architectures.

7

1k2k3k4k5k# Epochs0.50.60.70.80.91.0Norm. R (Optimal)DIALDIAL-NSRIALRIAL-NSNoCommOracle10k20k30k40k# Epochs0.50.60.70.80.91.0Norm. R (Optimal)DIALDIAL-PSRIALRIAL-NSNoCommOracleOffHas Been?OnYesNoNoneHas Been?YesNoSwitch?OnOnOffTellOnDay123+u12m1m2m3m4u11u51u52………………………Agent 1Agent 2m1……u12u11u22u21Agent 1Agent 2(a) Evaluation of Multi-Step

(c) Protocol of Multi-Step
Figure 6: MNIST Games: (a b) Performance of DIAL and RIAL  with and without (-NS) parameter
sharing  and NoComm  for both MNIST games. (c) Extracted coding scheme for multi-step MNIST.

(b) Evaluation of Colour-Digit

Experimental results. Figures 6(a) and 6(b) show that DIAL substantially outperforms the other
methods on both games. Furthermore  parameter sharing is crucial for reaching the optimal protocol.
In multi-step MNIST  results were obtained with σ = 0.5. In this task  RIAL fails to learn  while in
colour-digit MNIST it ﬂuctuates around local minima in the protocol space; the NoComm baseline
is stagnant at zero. DIAL’s performance can be attributed to directly optimising the messages in
order to reduce the global DQN error while RIAL must rely on trial and error. DIAL can also
optimise the message content with respect to rewards taking place many time-steps later  due to the
gradient passing between agents  leading to optimal performance in multi-step MNIST. To analyse
the protocol that DIAL learned  we sampled 1K episodes. Figure 6(c) illustrates the communication
bit sent at time-step t by agent 1  as a function of its input digit. Thus  each agent has learned a binary
encoding and decoding of the digits. These results illustrate that differentiable communication in
DIAL is essential to fully exploiting the power of centralised learning and thus is an important tool
for studying the learning of communication protocols.

6.4 Effect of Channel Noise

The question of why language evolved to be discrete has
been studied for centuries  see e.g.  the overview in [26].
Since DIAL learns to communicate in a continuous channel 
our results offer an illuminating perspective on this topic. In
particular  Figure 7 shows that  in the switch riddle  DIAL
without noise in the communication channel learns centred
activations. By contrast  the presence of noise forces mes-
sages into two different modes during learning. Similar
observations have been made in relation to adding noise
when training document models [12] and performing clas-
siﬁcation [11]. In our work  we found that adding noise
was essential for successful training. More analysis on this
appears in the supplementary material.

7 Conclusions

Figure 7: DIAL’s learned activations
with and without noise in DRU.

This paper advanced novel environments and successful techniques for learning communication
protocols. It presented a detailed comparative analysis covering important factors involved in the
learning of communication protocols with deep networks  including differentiable communication 
neural network architecture design  channel noise  tied parameters  and other methodological aspects.
This paper should be seen as a ﬁrst attempt at learning communication and language with deep
learning approaches. The gargantuan task of understanding communication and language in their
full splendour  covering compositionality  concept lifting  conversational agents  and many other
important problems still lies ahead. We are however optimistic that the approaches proposed in this
paper can help tackle these challenges.

8

10k20k30k40k50k# Epochs0.00.20.40.60.81.0Norm. R (Optimal)DIALDIAL-NSRIALRIAL-NSNoCommOracle5k10k15k20k# Epochs0.00.20.40.60.81.0Norm. R (Optimal)DIALDIAL-NSRIALRIAL-NSNoCommOracle1234Step0123456789True Digit-10010Activation0.00.51.0Probability¾=0-10010Activation¾=2:0Epoch 1kEpoch 3kEpoch 5kReferences
[1] F. A. Oliehoek  M. T. J. Spaan  and N. Vlassis. Optimal and approximate Q-value functions for decentralized

POMDPs. JAIR  32:289–353  2008.

[2] L. Kraemer and B. Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning.

Neurocomputing  190:82–94  2016.

[3] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. Riedmiller 
A. K. Fidjeland  G. Ostrovski  S. Petersen  C. Beattie  A. Sadik  I. Antonoglou  H. King  D. Kumaran 
D. Wierstra  S. Legg  and D. Hassabis. Human-level control through deep reinforcement learning. Nature 
518(7540):529–533  2015.

[4] M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML  1993.
[5] F. S. Melo  M. Spaan  and S. J. Witwicki. QueryPOMDP: POMDP-based communication in multiagent

systems. In Multi-Agent Systems  pages 189–204. 2011.

[6] L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous Agents and

Multi-Agent Systems  11(3):387–434  2005.

[7] T. Kasai  H. Tenmoto  and A. Kamiya. Learning of communication codes in multi-agent reinforcement

learning problem. In IEEE Soft Computing in Industrial Applications  pages 1–6  2008.

[8] C. L. Giles and K. C. Jim. Learning communication for multi-agent systems. In Innovative Concepts for

Agent-Based Systems  pages 377–390. Springer  2002.

[9] K. Gregor  I. Danihelka  A. Graves  and D. Wierstra. Draw: A recurrent neural network for image

generation. arXiv preprint arXiv:1502.04623  2015.

[10] S. Sukhbaatar  A. Szlam  and R. Fergus. Learning multiagent communication with backpropagation. arXiv

preprint arXiv:1605.07736  2016.

[11] M. Courbariaux and Y. Bengio. BinaryNet: Training deep neural networks with weights and activations

constrained to +1 or -1. arXiv preprint arXiv:1602.02830  2016.

[12] G. Hinton and R. Salakhutdinov. Discovering binary codes for documents by learning deep generative

models. Topics in Cognitive Science  3(1):74–91  2011.

[13] R. S. Sutton and A. G. Barto. Introduction to reinforcement learning. MIT Press  1998.
[14] A. Tampuu  T. Matiisen  D. Kodelja  I. Kuzovkin  K. Korjus  J. Aru  J. Aru  and R. Vicente. Multiagent
cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1511.08779  2015.
[15] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic  Game-Theoretic  and Logical

Foundations. Cambridge University Press  New York  2009.

[16] E. Zawadzki  A. Lipson  and K. Leyton-Brown. Empirically evaluating multiagent learning algorithms.

arXiv preprint 1401.8074  2014.

[17] M. Hausknecht and P. Stone. Deep recurrent Q-learning for partially observable MDPs. arXiv preprint

arXiv:1507.06527  2015.

[18] K. Narasimhan  T. Kulkarni  and R. Barzilay. Language understanding for text-based games using deep

reinforcement learning. arXiv preprint arXiv:1506.08941  2015.

[19] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent

magnitude. COURSERA: Neural Networks for Machine Learning  4(2)  2012.

[20] K. Cho  B. van Merriënboer  D. Bahdanau  and Y. Bengio. On the properties of neural machine translation:

Encoder-decoder approaches. arXiv preprint arXiv:1409.1259  2014.

[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780  1997.
[22] J. Chung  C. Gulcehre  K. Cho  and Y. Bengio. Empirical evaluation of gated recurrent neural networks on

sequence modeling. arXiv preprint arXiv:1412.3555  2014.

[23] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In ICML  pages 448–456  2015.

[24] W. Wu. 100 prisoners and a lightbulb. Technical report  OCF  UC Berkeley  2002.
[25] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[26] M. Studdert-Kennedy. How did language go discrete?

In M. Tallerman  editor  Language Origins:

Perspectives on Evolution  chapter 3. Oxford University Press  2005.

9

,Bruno Conejo
Nikos Komodakis
Sebastien Leprince
Jean Philippe Avouac
Jakob Foerster
Ioannis Alexandros Assael
Nando de Freitas
Shimon Whiteson