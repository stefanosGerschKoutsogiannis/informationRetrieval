2008,Regularized Learning with Networks of Features,For many supervised learning problems  we possess prior knowledge about which features yield similar information about the target variable.  In predicting the topic of a document  we might know that two words are synonyms  or when performing image recognition  we know which pixels are adjacent.  Such synonymous or neighboring features are near-duplicates and should therefore be expected to have similar weights in a good model.  Here we present a framework for regularized learning in settings where one has prior knowledge about which features are expected to have similar and dissimilar weights.  This prior knowledge is encoded as a graph whose vertices represent features and whose edges represent similarities and dissimilarities between them.  During learning  each feature's weight is penalized by the amount it differs from the average weight of its neighbors.  For text classification  regularization using graphs of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods.  For sentiment analysis  feature graphs constructed from declarative human knowledge  as well as from auxiliary task learning  significantly improve prediction accuracy.,Regularized Learning with Networks of Features

Ted Sandler  Partha Pratim Talukdar  and Lyle H. Ungar

Department of Computer & Information Science  University of Pennsylvania

{tsandler partha ungar}@cis.upenn.edu

Department of Computer Science  U.C. Berkeley

John Blitzer

blitzer@cs.berkeley.edu

Abstract

For many supervised learning problems  we possess prior knowledge about which
features yield similar information about the target variable. In predicting the topic
of a document  we might know that two words are synonyms  and when perform-
ing image recognition  we know which pixels are adjacent. Such synonymous or
neighboring features are near-duplicates and should be expected to have similar
weights in an accurate model. Here we present a framework for regularized learn-
ing when one has prior knowledge about which features are expected to have sim-
ilar and dissimilar weights. The prior knowledge is encoded as a network whose
vertices are features and whose edges represent similarities and dissimilarities be-
tween them. During learning  each feature’s weight is penalized by the amount
it differs from the average weight of its neighbors. For text classiﬁcation  reg-
ularization using networks of word co-occurrences outperforms manifold learn-
ing and compares favorably to other recently proposed semi-supervised learning
methods. For sentiment analysis  feature networks constructed from declarative
human knowledge signiﬁcantly improve prediction accuracy.

1 Introduction

For many important problems in machine learning  we have a limited amount of labeled training
data and a very high-dimensional feature space. A common approach to alleviating the difﬁculty
of learning in these settings is to regularize a model by penalizing a norm of its parameter vector.
The most commonly used norms in classiﬁcation  L1 and L2  assume independence among model
parameters [1]. However  we often have access to information about dependencies between param-
eters. For example  with spatio-temporal data  we usually know which measurements were taken at
points nearby in space and time. And in natural language processing  digital lexicons such as Word-
Net can indicate which words are synonyms or antonyms [2]. For the biomedical domain  databases
such as KEGG and DIP list putative protein interactions [3  4]. And in the case of semi-supervised
learning  dependencies can be inferred from unlabeled data [5  6]. Consequently  we should be able
to learn models more effectively if we can incorporate dependency structure directly into the norm
used for regularization.

Here we introduce regularized learning with networks of features  a framework for constructing cus-
tomized norms on the parameters of a model when we have prior knowledge about which parameters
are likely to have similar values. Since our focus is on classiﬁcation  the parameters we consider are
feature weights in a linear classiﬁer. The prior knowledge is encoded as a network or graph whose
nodes represent features and whose edges represent similarities between the features in terms of how
likely they are to have similar weights. During learning  each feature’s weight is penalized by the
amount it differs from the average weight of its neighbors. This regularization objective is closely

connected to the unsupervised dimensionality reduction method  locally linear embedding (LLE) 
proposed by Roweis and Saul [7]. In LLE  each data instance is assumed to be a linear combina-
tion of its nearest neighbors on a low dimensional manifold. In this work  each feature’s weight is
preferred (though not required) to be a linear combination of the weights of its neighbors.

Similar to other recent methods for incorporating prior knowledge in learning  our framework can
be viewed as constructing a Gaussian prior with non-diagonal covariance matrix on the model pa-
rameters [6  8]. However  instead of constructing the covariance matrix directly  it is induced from
a network. The network is typically sparse in that each feature has only a small number of neigh-
bors. However  the induced covariance matrix is generally dense. Consequently  we can implicitly
construct rich and dense covariance matrices over large feature spaces without incurring the space
and computational blow-ups that would be incurred if we attempted to construct these matrices
explicitly.

Regularization using networks of features is especially appropriate for high-dimensional feature
spaces such as are encountered in text processing where the local distances required by tradi-
tional manifold classiﬁcation methods [9  10] may be difﬁcult to estimate accurately  even with
large amounts of unlabeled data. We show that regularization with feature-networks derived from
word co-occurrence statistics outperforms manifold regularization and another  more recent  semi-
supervised learning approach [5] on the task of text classiﬁcation. Feature network based regu-
larization also supports extensions which provide ﬂexibility in modeling parameter dependencies 
allowing for feature dissimilarities and the introduction of feature classes whose weights have com-
mon but unknown means. We demonstrate that these extensions improve classiﬁcation accuracy
on the task of classifying product reviews in terms of how favorable they are to the products in
question [11]. Finally  we contrast our approach with related regularization methods.

2 Regularized Learning with Networks of Features

We assume a standard supervised learning framework in which we are given a training set of in-
stances T = {(xi  yi)}n
i=1 with xi ∈ Rd and associated labels yi ∈ Y. We wish to learn a linear
classiﬁer parameterized by weight vector w ∈ Rd by minimizing a convex loss function l(x  y ; w)
over the training instances  (xi  yi). For many problems  the dimension  d  is much larger than the
number of labeled instances  n. Therefore  it is important to impose some constraints on w. Here
we do this using a directed network or graph  G  whose vertices  V = {1  ...  d}  correspond to the
features of our model and whose edges link features whose weights are believed to be similar. The
edges of G are non-negative with larger weights indicating greater similarity. Conversely  a weight
of zero means that two features are not believed a priori to be similar. As has been shown elsewhere
[5  6  8]  such similarities can be inferred from prior domain knowledge  auxiliary task learning  and
statistics computed on unlabeled data. For the time being we assume that G is given and defer its
construction until section 4  experimental work.
The weights of G are encoded by a matrix  P   where Pij ≥ 0 gives the weight of the directed edge
from vertex i to vertex j. We constrain the out-degree of each vertex to sum to one  Pj Pij = 1  so
that no feature “dominates” the graph. Because the semantics of the graph are that linked features
should have similar weights  we penalize each feature’s weight by the squared amount it differs from
the weighted average of its neighbors. This gives us the following criterion to optimize in learning:

loss(w) =

n

X

i=1

l(xi  yi ; w) + α

d

X

j=1

¡wj − X

k

Pjk wk¢2

+ β kwk2
2 

(1)

where we have added a ridge term to make the loss strictly convex. The hyperparameters α and β
specify the amount of network and ridge regularization respectively. The regularization penalty can
be rewritten as w⊤M w where M = α (I − P )⊤(I − P ) + β I. The matrix M is symmetric positive
deﬁnite  and therefore our criterion possesses a Bayesian interpretation in which the weight vector 
w  is a priori normally distributed with mean zero and covariance matrix 2M −1.
Minimizing equation (1) is equivalent to ﬁnding the MAP estimate for w. The gradient of (1) with
respect to w is ∇w loss = Pn
i=1 ∇w l(xi  yi ; w) + 2M w and therefore requires only an additional
If P is sparse  as it is in
matrix multiply on top of computing the loss over the training data.
our experiments—i.e.  it has only kd entries for k ≪ d—then the matrix multiply is O(d). Thus

equation (1) can be minimized very quickly. Additionally  the induced covariance matrix M −1
will typically be dense even though P is sparse  showing that we can construct dense covariance
structures over w without incurring storage and computation costs.

2.1 Relationship to Locally Linear Embedding

Locally linear embedding (LLE) is an unsupervised learning method for embedding high dimen-
sional data in a low dimensional vector space. The data { ~Xi}n
i=1 is assumed to lie on a low dimen-
sional manifold of dimension c within a high dimensional vector space of dimension d with c ≪ d.
Since the data lies on a manifold  each point is approximately a convex combination of its nearest
neighbors on the manifold. That is  ~Xi ≈ Pj∼i Pij ~Xj  where j ∼ i denotes the samples  j  which
lie close to i on the manifold. As above  the matrix P has non-negative entries and its rows sum to
one. The set of low dimensional coordinates  {~Yi}n
i=1  ~Yi ∈ Rc  are found by minimizing the sum
of squares cost:
k~Yi − X
(2)

cost({~Yi}) = X

Pij ~Yjk2
2 

i

j

subject to the constraint that the {~Yi} have unit variance in each of the c dimensions. The solution
to equation (2) is found by performing eigen-decomposition on the matrix (I − P )⊤(I − P ) =
U ΛU ⊤ where U is the matrix of eigenvectors and Λ is the diagonal matrix of eigenvalues. The
LLE coordinates are obtained from the eigenvectors  u1  ...  uc whose eigenvalues  λ1  ...  λc  are
smallest1 by setting ~Yi = (u1i  ...  uci)⊤. Looking at equation (1) and ignoring the ridge term  it is
clear that our feature network regularization penalty is identical to LLE except that the embedding
is found for the feature weights rather than data instances. However  there is a deeper connection.
If we let L(Y  Xw) denote the unregularized loss over the training set where X is the n × d matrix
of instances and Y is the n-vector of class labels  we can express equation (1) in matrix form as

w

∗ = argmin

L(Y  Xw) + w

w

⊤¡ α (I − P )⊤(I − P ) + β I ¢ w.

(3)

Deﬁning ˜X to be XU (αΛ + β I)−1/2 where U and Λ are from the eigen-decomposition above  it is
not hard to show that equation (3) is equivalent to the alternative ridge regularized learning problem

˜w

∗ = argmin

L(Y  ˜X ˜w) + ˜w

⊤ ˜w.

˜w

(4)

That is  the two minimizers  w and ˜w  yield the same predictions: ˆY = Xw = ˜X ˜w. Consequently 
we can view feature network regularization as: 1) ﬁnding an embedding for the features using LLE
in which all of the eigenvectors are used and scaled by the inverse square-roots of their eigenvalues
(plus a smoothing term  βI  that makes the inverse well-deﬁned); 2) projecting the data instances
onto these coordinates; and 3) learning a ridge-penalized model for the new representation. In using
all of the eigenvectors  the dimensionality of the feature embedding is not reduced. However  in
scaling the eigenvectors by the inverse square-roots of their eigenvalues  the directions of least cost
in the network regularized problem become the directions of maximum variance in the associated
ridge regularized problem  and hence are the directions of least cost in the ridge problem. As a result 
the effective dimensionality of the learning problem is reduced to the extent that the distribution
of inverted eigenvalues is sharply peaked. When the best representation for classiﬁcation has high
dimension  it is faster to solve (3) than to compute a large eigenvector basis and solve (4). In the high
dimensional problems of section 4  we ﬁnd that regularization with feature networks outperforms
LLE-based regression.

3 Extensions to Feature Network Regularization

In this section  we pose a number of extensions and alternatives to feature network regularization as
formulated in section 2  including the modeling of classes of features whose weights are believed
to share the same unknown means  the incorporation of feature dissimilarities  and two alternative
regularization criteria based on the graph Laplacian.

1More precisely  eigenvectors u2  ...  uc+1 are used so that the {~Yi} are centered.

3.1 Regularizing with Classes of Features

In machine learning  features can often be grouped into classes  such that all the weights of the
features in a given class are drawn from the same underlying distribution. For example  words can
be grouped by part of speech  by meaning (as in WordNet’s synsets)  or by clustering based on the
words they co-occur with or the documents they occur in. Using an appropriately constructed feature
graph  we can model the case in which the underlying distributions are believed to be Gaussians with
known  identical variances but with unknown means. That is  the case in which there are k disjoint
i=1 whose weights are drawn i.i.d. N (µi  σ2) with µi unknown but σ2
classes of features {Ci}k
known and shared across all classes.

The straight-forward approach to modeling this scenario might seem to be to link all the features
within a class to each other  forming a clique  but this does not lead to the desired interpretation.
Additionally  the number of edges in this construction scales quadratically in the clique sizes  result-
ing in feature graphs that are not sparse. Our approach is therefore to create k additional “virtual”
features  f1  ...  fk  that do not appear in any of the data instances but whose weights ˆµ1  ...  ˆµk serve
as the estimates for the true but unknown means  µ1  ...  µk. In creating the feature graph  we link
each feature to the virtual feature for its class with an edge of weight one. The virtual features 
themselves  do not possess any out-going links.

Denoting the class of feature i as c(i)  and setting the hyperparameters α and β in equation (1) to
1/(2σ2) and 0  respectively  yields a network regularization cost of 1
i=1(wi− ˆµc(i))2. Since
the virtual features do not appear in any instances  i.e. their values are zero in every data instance 
their weights are free to take on whatever values minimize the network regularization cost in (1) 
in particular the estimates of the class means  µ1  ...  µk. Consequently  minimizing the network
regularization penalty maximizes the log-likelihood for the intended scenario. We can extend this
construction to model the case in which the feature weights are drawn from a mixture of Gaussians
by connecting each feature to a number of virtual features with edge weights that sum to one.

2 σ−2 Pd

3.2

Incorporating Feature Dissimilarities

Feature network regularization can also be extended to induce features to have opposing weights.
Such feature “dissimilarities” can be useful in tasks such as sentiment prediction where we would
like weights for words such as “great” or “fantastic” to have opposite signs from their negated bigram
counterparts “not great” and “not fantastic ” and from their antonyms. To model dissimilarities  we
construct a separate graph whose edges represent anti-correlations between features. Regularizing
over this graph enforces each feature’s weight to be equal to the negative of the average of the neigh-
boring weights. To do this  we encode the dissimilarity graph using a matrix Q  deﬁned analogously
to the matrix P   and add the term Pi¡wi + Pj Qij wj¢2 to the network regularization criterion 
which can be written as w⊤(I +Q)⊤(I +Q)w. The matrix (I +Q)⊤(I +Q) is positive semideﬁnite
like its similarity graph counterpart. Goldberg et al. [12] use a similar construction with the graph
Laplacian in order to incorporate dissimilarities between instances in manifold learning.

3.3 Regularizing Features with the Graph Laplacian

A natural alternative to the network regularization criterion given in section (2) is to regularize the
feature weights using a penalty derived from the graph Laplacian [13]. Here  the feature graph’s edge
weights are given by a symmetric matrix  W   whose entries  Wij ≥ 0  give the weight of the edge
between features i and j. The Laplacian penalty is 1
2 Pi j Wij(wi − wj)2 which can be written as
w⊤(D−W ) w  where D = diag(W 1) is the vertex degree matrix. The main difference between the
Laplacian penalty and the network penalty in equation (1) is that the Laplacian penalizes each edge
equally (modulo the edge weights) whereas the network penalty penalizes each feature equally. In
graphs where there are large differences in vertex degree  the Laplacian penalty will therefore focus
most of the regularization cost on features with many neighbors. Experiments in section 4 show
that the criterion in (1) outperforms the Laplacian penalty as well as a related penalty derived from
the normalized graph Laplacian  1

2 Pi j Wij(wi/√Dii − wj/pDjj)2. The normalized Laplacian
penalty assumes that pDjj wi ≈ √Diiwj  which is different from assuming that linked features

should have similar weights.

y
c
a
r
u
c
c
A

0
8

0
7

0
6

0
5

0
4

0
3

FNR
LLE Regression
PCR
Norm. Laplacian
Laplacian
Ridge Penalty

60

200

100
1000
Number of Training Instances

500

y
c
a
r
u
c
c
A

0
8

0
7

0
6

0
5

0
4

0
3

2000

FNR
Manifold (Loc/Glob)
ASO Top
ASO Bottom

100

200

500

1000

Number of Training Instances

Figure 1: Left: Accuracy of feature network regularization (FNR) and ﬁve baselines on “20 newsgroups” data.
Right: Accuracy of FNR compared to reported accuracies of three other semi-supervised learning methods.

4 Experiments

We evaluated logistic regression augmented with feature network regularization on two natural lan-
guage processing tasks. The ﬁrst was document classiﬁcation on the 20 Newsgroups dataset  a
well-known document classiﬁcation benchmark. The second was sentiment classiﬁcation of prod-
uct reviews  the task of classifying user-written reviews according to whether they are favorable or
unfavorable to the product under review based on the review text [11]. Feature graphs for the two
tasks were constructed using different information. For document classiﬁcation  the feature graph
was constructed using feature co-occurrence statistics gleaned from unlabeled data. In sentiment
prediction  both co-occurrence statistics and prior domain knowledge were used.

4.1 Experiments on 20 Newsgroups

We evaluated feature network based regularization on the 20 newsgroups classiﬁcation task using
all twenty classes. The feature set was restricted to the 11 376 words which occurred in at least 20
documents  not counting stop-words. Word counts were transformed by adding one and taking logs.
To construct the feature graph  each feature (word) was represented by a binary vector denoting its
presence/absence in each of the 20 000 documents of the dataset. To measure similarity between
features  we computed cosines between these binary vectors. Each feature was linked to the 25
other features with highest cosine scores  provided that the scores were above a minimum threshold
of 0.10. The edge weights of the graph were set to these cosine scores and the matrix P was
constructed by normalizing each vertex’s out-degree to sum to one.

Figure 1 (left) shows feature network regularization compared against ﬁve other baselines: logis-
tic regression with an L2 (ridge) penalty; principal components logistic regression (PCR) in which
each instance was projected onto the largest 200 right singular vectors of the n× d matrix  X; LLE-
logistic regression in which each instance was projected onto the smallest 200 eigenvectors of the
matrix (I−P )⊤(I−P ) described in section 2; and logistic regression regularized by the normalized
and unnormalized graph Laplacians described in section 3.3. Results at each training set size are
averages of ﬁve trials with training sets sampled to contain an equal number of documents per class.
For ridge  the amount of L2 regularization was chosen using cross validation on the training set.
Similarly  for feature network regularization and the Laplacian regularizers  the hyperparameters α
and β were chosen through cross validation on the training set using a simple grid search. The ratio
of α to β tended to be around 100:1. For PCR and LLE-logistic regression  the number of eigenvec-
tors used was chosen to give good performance on the test set at both large and small training set
sizes. All models were trained using L-BFGS with a maximum of 200 iterations. Learning a sin-
gle model took between between 30 seconds and two minutes  with convergence typically achieved
before the full 200 iterations.

Books

DVDs

Electronics

Kitchen Appliances

sim
sim+dissim
ridge

sim
sim+dissim
ridge

sim
sim+dissim
ridge

sim
sim+dissim
ridge

0
8

0
7

0
6

0
5

2

10

50

250 1000

2

10

50

250

1000

2

10

50

250

1000

2

10

50

250

1000

Training Instances

Training Instances

Training Instances

Training Instances

Figure 2: Accuracy of feature network regularization on the sentiment datasets using feature classes and
dissimilarity edges to regularize the small sent of SentiWordNet features.

The results in ﬁgure 1 show that feature network regularization with a graph constructed from unla-
beled data outperforms all baselines and increases accuracy by 4%-17% over the plain ridge penalty 
an error reduction of 17%-30%. Additionally  it outperforms the related LLE regression. We conjec-
ture this is because in tuning the hyperparameters  we can adaptively tune the dimensionality of the
underlying data representation. Additionally  by scaling the eigenvectors by their eigenvalues  fea-
ture network regularization keeps more information about the directions of least cost in weight space
than does LLE regression  which does not rescale the eigenvectors but simply keeps or discards them
(i.e. scales them by 1 or 0).

Figure 1 (right) compares feature network regularization against two external approaches that lever-
age unlabeled data: a multi-task learning approach called alternating structure optimization (ASO) 
and our reimplementation of a manifold learning method which we refer to as “local/global consis-
tency” [5  10]. To make a fair comparison against the reported results for ASO  training sets were
sampled so as not to necessarily contain an equal number of documents per class. Accuracies are
given for the highest and lowest performing variants of ASO reported in [5]. Our reimplementation
of local/global consistency used the same document preprocessing described in [10]. However  the
graph was constructed so that each document had only K = 10 neighbors (the authors in [10] use
a fully connected graph which does not ﬁt in memory for the entire 20 newsgroups dataset). Clas-
siﬁcation accuracy of local/global consistency did not vary much with K and up to 500 neighbors
were tried for each document. Here we see that feature network regularization is competitive with
the other semi-supervised methods and performs best at all but the smallest training set size.

4.2 Sentiment Classiﬁcation

For sentiment prediction  we obtained the product review datasets used in [11]. Each dataset con-
sists of reviews downloaded from Amazon.com for one of four different product domains: books 
DVDs  electronics  and kitchen appliances. The reviews have an associated number of “stars ” rang-
ing from 0 to 5  rating the quality of a product. The goal of the task is to predict whether a review
has more than (positive) or less than (negative) 3 stars associated with it based only on the text in the
review. We performed two sets of experiments in which prior domain knowledge was incorporated
using feature networks. In both  we used a list of sentimentally-charged words obtained from the
SentiWordNet database [14]  a database which associates positive and negative sentiment scores to
each word in WordNet. In the ﬁrst experiment  we constructed a set of feature classes in the manner
described in section 3.1 to see if such classes could be used to boot-strap weight polarities for groups
of features. In the second  we computed similarities between words in terms of the similarity of their
co-occurrence’s with the sentimentally charged words.

From SentiWordNet we extracted a list of roughly 200 words with high positive and negative sen-
timent scores that also occurred in the product reviews at least 100 times. Words to which Senti-
WordNet gave a high ‘positive’ score were placed in a “positive words” cluster and words given
a high ‘negative’ score were placed in a “negative words” cluster. As described in section 3.1  all
words in the positive cluster were attached to a virtual feature representing the mean feature weight
of the positive cluster words  and all words in the negative cluster were attached to a virtual weight
representing the mean weight of the negative cluster words. We also added a dissimilarity edge (de-
scribed in section 3.2) between the positive and negative clusters’ virtual features to induce the two

0
9

5
8

0
8

5
7

0
7

5
6

0
6

Books

DVDs

Electronics

Kitchen Appliances

FNR
Ridge Penalty

FNR
Ridge Penalty

FNR
Ridge Penalty

FNR
Ridge Penalty

50

100

250

500 1000

50

100

250

500

1000

50

100

250

500

1000

50

100

250

500

1000

Training Instances

Training Instances

Training Instances

Training Instances

Figure 3: Accuracy of feature network and ridge regularization on four sentiment classiﬁcation datasets.

classes of features to have opposite means. As shown in ﬁgure 2  imposing feature clusters on the
two classes of words improves performance noticeably while the addition of the feature dissimilarity
edge does not yield much beneﬁt. When it helps  it is only for the smallest training set sizes.

This simple set of experiments demonstrated the applicability of feature classes for inducing groups
of features to have similar means  and that the words extracted from SentiWordNet were relatively
helpful in determining the sentiment of a review. However  the number of features used in these
experiments was too small to yield reasonable performance in an applied setting. Thus we extended
the feature sets to include all unigram and bigram word-features which occurred in ten or more
reviews. The total number of reviews and size of the feature sets is given in table 1.

Instances
13 161
13 005
8 922
7 760

Features
29 404
31 475
15 104
11 658

Table 1: Sentiment Data Statistics

Edges
470 034
419 178
343 890
305 926

Dataset
books
DVDs
electronics
kitchen

The method used to construct the feature graph
in the 20 newsgroups experiments was not well
suited for sentiment prediction since plain feature
co-occurrence statistics tended to ﬁnd groups of
words that showed up in reviews for products of the
same type  e.g.  digital cameras or laptops. While
such similarities are useful in predicting what type
of product is being reviewed  they are of little help
in determining whether a review is favorable or un-
favorable. Thus  to align features along dimensions
of ‘sentiment ’ we computed the correlations of all features with the SentiWordNet features so that
each word was represented as a 200 dimensional vector of correlations with these highly charged
sentiment words. Distances between these correlation vectors were computed in order to determine
which features should be linked. We next computed each feature’s 100 nearest neighbors. Two fea-
tures were linked if both were in the other’s set of nearest 100 neighbors. For simplicity  the edge
weights were set to one and the graph weight matrix was then row-normalized in order to construct
the matrix P . The number of edges in each feature graph is given in table 1.
The ‘kitchen’ dataset was used as a development dataset in order to arrive at the method for con-
structing the feature graph and for choosing the hyperparameter values: α = 9.9 and β = 0.1.
Figure 3 gives accuracy results for all four sentiment datasets at training sets of 50 to 1000 in-
stances. The results show that linking features which are similarly correlated with sentiment-loaded
words yields improvements on every dataset and at every training set size.

5 Related Work

Most similar to the work presented here is that of the fused lasso (Tibshirani et al. [15]) which can
be interpreted as using the graph Laplacian regularizer but with an L1 norm instead of L2 on the
residuals of weight differences: Pi Pj∼i |wi − wj| and all edge weights set to one. As the authors
discuss  an L1 penalty prefers that weights of linked features be exactly equal so that the residual
vector of weight differences is sparse. L1 is appropriate if the true weights are believed to be exactly
equal  but in many settings  features are near copies of one another whose weights should be similar
rather than identical. Thus in these settings  penalizing squared differences rather than absolute
ones is more appropriate. Optimizing L1 feature weight differences also leads to a much harder
optimization problem  making it less applicable in large scale learning. Li and Li [13] regularize

feature weights using the normalized graph Laplacian in their work on biomedical prediction tasks.
As shown  this criterion does not work as well on the text prediction problems considered here.

Krupka and Tishby [8] proposed a method for inducing feature-weight covariance matrices using
distances in a “meta-feature” space. Under their framework  two features positively covary if they
are close in this space and approach independence as they grow distant. The authors represent each
feature i as a vector of meta-features  ui  and compute the entries of the feature weight covariance
matrix  Cij = exp(− 1
2σ2kui − ujk2). Obviously  the choice of which is more appropriate  a feature
graph or metric space  is application dependent. However  it is less obvious how to incorporate
feature dissimilarities in a metric space. A second difference is that our work deﬁnes the regularizer
in terms of C −1 ≈ (I−P )⊤(I−P ) rather than C itself. While C −1 is constructed to be sparse with
a nearest neighbors graph  the induced covariance matrix  C  need not be sparse. Thus  working with
C −1 allows for construct dense covariance matrices without having to explicitly store them. Finally 
Raina et al. [6] learn a feature-weight covariance matrix via auxiliary task learning. Interestingly  the
entries of this covariance matrix are learned jointly with a regression model for predicting feature
weight covariances as a function of meta-features. However  since their approach explicitly predicts
each entry of the covariance matrix  they are restricted to learning smaller models  consisting of
hundreds rather than tens of thousands of features.

6 Conclusion

We have presented regularized learning with networks of features  a simple and ﬂexible framework
for incorporating expectations about feature weight similarities in learning. Feature similarities
are modeled using a feature graph and the weight of each feature is preferred to be close to the
average of its neighbors. On the task of document classiﬁcation  feature network regularization
is superior to several related criteria  as well as to a manifold learning approach where the graph
models similarities between instances rather than between features. Extensions for modeling feature
classes  as well as feature dissimilarities  yielded beneﬁts on the problem of sentiment prediction.

References
[1] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer New York  2001.
[2] C. Fellbaum. WordNet: an electronic lexical database. MIT Press  1998.
[3] H. Ogata  S. Goto  K. Sato  W. Fujibuchi  H. Bono  and M. Kanehisa. KEGG: Kyoto Encyclopedia of

Genes and Genomes. Nucleic Acids Research  27(1):29–34  1999.

[4] I. Xenarios  D.W. Rice  L. Salwinski  M.K. Baron  E.M. Marcotte  and D. Eisenberg. DIP: The Database

of Interacting Proteins. Nucleic Acids Research  28(1):289–291  2000.

[5] R.K. Ando and T. Zhang. A Framework for Learning Predictive Structures from Multiple Tasks and

Unlabeled Data. JMLR  6:1817–1853  2005.

[6] R. Raina  A.Y. Ng  and D. Koller. Constructing informative priors using transfer learning. In ICML  2006.
[7] S.T. Roweis and L.K. Saul. Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science 

290(5500):2323–2326  2000.

[8] E. Krupka and N. Tishby. Incorporating Prior Knowledge on Features into Learning. In AISTATS  2007.
[9] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: a geometric framework for lerning

from lableed and unlabeled examples. JMLR  7:2399–2434  2006.

[10] D. Zhou  O. Bousquet  T.N. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global consistency.

In NIPS  2004.

[11] J. Blitzer  M. Dredze  and F. Pereira. Biographies  Bollywood  Boom-boxes and Blenders: Domain

Adaptation for Sentiment Classiﬁcation. In ACL  2007.

[12] A.B. Goldberg  X. Zhu  and S. Wright. Dissimilarity in Graph-Based Semi-Supervised Classiﬁcation. In

AISTATS  2007.

[13] C. Li and H. Li. Network-constrained regularization and variable selection for analysis of genomic data.

Bioinformatics  24(9):1175–1182  2008.

[14] A. Esuli and F. Sebastiani. SentiWordNet: A Publicly Available Lexical Resource For Opinion Mining.

In LREC  2006.

[15] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and Smoothness via the Fused

Lasso. Journal of the Royal Statistical Society Series B  67(1):91–108  2005.

,Jonathan Huggins
Lester Mackey