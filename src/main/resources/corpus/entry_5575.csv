2014,Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time,We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently  though its global solution attains the optimal statistical rate of convergence  such solution is computationally intractable to obtain. Meanwhile  although its convex relaxations are tractable to compute  they yield estimators with suboptimal statistical rates of convergence. On the other hand  existing nonconvex optimization procedures  such as greedy methods  lack statistical guarantees. In this paper  we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit  which iteratively solves the underlying nonconvex problem. However  our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region  namely the basin of attraction. To obtain the desired initial estimator that falls into this region  we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework  we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally  our procedure converges at the rate of $1/\sqrt{t}$ within the initialization stage  and at a geometric rate within the main stage. Statistically  the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$  dimension $d$ and sample size $n$. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.,Tighten after Relax: Minimax-Optimal Sparse PCA

in Polynomial Time

Zhaoran Wang

Huanran Lu

Han Liu

Department of Operations Research and Financial Engineering

Princeton University
Princeton  NJ 08540

{zhaoran huanranl hanliu}@princeton.edu

Abstract

We provide statistical and computational analysis of sparse Principal Component
Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex
in nature. Consequently  though its global solution attains the optimal statistical rate
of convergence  such solution is computationally intractable to obtain. Meanwhile 
although its convex relaxations are tractable to compute  they yield estimators with
suboptimal statistical rates of convergence. On the other hand  existing nonconvex
optimization procedures  such as greedy methods  lack statistical guarantees.
In this paper  we propose a two-stage sparse PCA procedure that attains the optimal
principal subspace estimator in polynomial time. The main stage employs a novel
algorithm named sparse orthogonal iteration pursuit  which iteratively solves the
underlying nonconvex problem. However  our analysis shows that this algorithm
only has desired computational and statistical guarantees within a restricted region 
namely the basin of attraction. To obtain the desired initial estimator that falls into
this region  we solve a convex formulation of sparse PCA with early stopping.
Under an integrated analytic framework  we simultaneously characterize the com-
√
putational and statistical performance of this two-stage procedure. Computationally 
our procedure converges at the rate of 1/
t within the initialization stage  and at
a geometric rate within the main stage. Statistically  the ﬁnal principal subspace
estimator achieves the minimax-optimal statistical rate of convergence with respect
to the sparsity level s∗  dimension d and sample size n. Our procedure motivates a
general paradigm of tackling nonconvex statistical learning problems with provable
statistical guarantees.

1  . . .   u∗

Introduction

1
We denote by x1  . . .   xn the n realizations of a random vector X ∈ Rd with population covariance
matrix Σ ∈ Rd×d. The goal of Principal Component Analysis (PCA) is to recover the top k leading
k of Σ. In high dimensional settings with d (cid:29) n  [1–3] showed that classical
eigenvectors u∗
PCA can be inconsistent. Additional assumptions are needed to avoid such a curse of dimensionality.
For example  when the ﬁrst leading eigenvector is of primary interest  one common assumption is that
u∗
1 is sparse — the number of nonzero entries of u∗
1  denoted by s∗  is smaller than n. Under such
an assumption of sparsity  signiﬁcant progress has been made on the methodological development
[4–13] as well as theoretical understanding [1  3  14–21] of sparse PCA.
However  there remains a signiﬁcant gap between the computational and statistical aspects of sparse
PCA: No tractable algorithm is known to attain the statistical optimal sparse PCA estimator provably
without relying on the spiked covariance assumption. This gap arises from the nonconvexity of sparse

1

PCA. In detail  the sparse PCA estimator for the ﬁrst leading eigenvector u∗

1 is

(cid:98)u1 = argmin

(cid:107)v(cid:107)2=1

−vT(cid:98)Σv 

subject to (cid:107)v(cid:107)0 = s∗ 

(1)

To address this computational issue  [5] proposed a convex relaxation approach  named DSPCA  for
estimating the ﬁrst leading eigenvector. [13] generalized DSPCA to estimate the principal subspace
spanned by the top k leading eigenvectors. Nevertheless  [13] proved the obtained estimator only

nonzero coordinates  and s∗ is the sparsity level of u∗
1. Although this estimator has been proven to
attain the optimal statistical rate of convergence [15  17]  its computation is intractable because it
requires minimizing a concave function over cardinality constraints [22]. Estimating the top k leading

where (cid:98)Σ is the sample covariance estimator  (cid:107) · (cid:107)2 is the Euclidean norm  (cid:107) · (cid:107)0 gives the number of
eigenvectors is even more challenging because of the extra orthogonality constraint on(cid:98)u1  . . .  (cid:98)u2.
attains the suboptimal s∗(cid:112)log d/n statistical rate. Meanwhile  several methods have been proposed
proposed the truncated power method  which attains the optimal(cid:112)s∗ log d/n rate for estimating u∗
However  it hinges on the assumption that the initial estimator u(0) satisﬁes(cid:12)(cid:12)sin ∠(u(0)  u∗)(cid:12)(cid:12)≤ 1−C 

to directly address the underlying nonconvex problem (1)  e.g.  variants of power methods or iterative
thresholding methods [10–12]  greedy method [8]  as well as regression-type methods [4  6  7  18].
However  most of these methods lack statistical guarantees. There are several exceptions: (1) [11]
1.
where C ∈ (0  1) is a constant. Suppose u(0) is chosen uniformly at random on the (cid:96)2 sphere  this
assumption holds with probability decreasing to zero when d → ∞ [23]. (2) [12] proposed an iterative
thresholding method  which attains a near optimal statistical rate when estimating several individual
leading eigenvectors. [18] proposed a regression-type method  which attains the optimal principal
subspace estimator. However  these two methods hinge on the spiked covariance assumption  and
require the data to be exactly Gaussian (sub-Gaussian not included). For them  the spiked covariance
assumption is crucial  because they use diagonal thresholding method [1] to obtain the initialization 
which would fail when the assumption of spiked covariance doesn’t hold  or each coordinate of X
has the same variance. Besides  except [12] and [18]  all the computational procedures only recover
the ﬁrst leading eigenvector  and leverage the deﬂation method [24] to recover the rest  which leads
to identiﬁability and orthogonality issues when the top k eigenvalues of Σ are not distinct.
To close the gap between computational and statistical aspects of sparse PCA  we propose a two-stage
procedure for estimating the k-dimensional principal subspace U∗ spanned by the top k leading
eigenvectors u∗
k. The details of the two stages are as follows: (1) For the main stage  we
propose a novel algorithm  named sparse orthogonal iteration pursuit  to directly estimate the principal
subspace of Σ. Our analysis shows  when its initialization falls into a restricted region  namely the
basin of attraction  this algorithm enjoys fast optimization rate of convergence  and attains the optimal
principal subspace estimator. (2) To obtain the desired initialization  we compute a convex relaxation
of sparse PCA. Unlike [5  13]  which calculate the exact minimizers  we early stop the corresponding
optimization algorithm as soon as the iterative sequence enters the basin of attraction for the main
stage. The rationale is  this convex optimization algorithm converges at a slow sublinear rate towards
a suboptimal estimator  and incurs relatively high computational overhead within each iteration.
Under a uniﬁed analytic framework  we provide simultaneous statistical and computational guarantees
for this two-stage procedure. Given the sample size n is sufﬁciently large  and the eigengap between
the k-th and (k + 1)-th eigenvalues of the population covariance matrix Σ is nonzero  we prove: (1)

The ﬁnal subspace estimator (cid:98)U attained by our two-stage procedure achieves the minimax-optimal
(cid:112)s∗ log d/n statistical rate of convergence. (2) Within the initialization stage  the iterative sequence
of subspace estimators(cid:8)U (t)(cid:9)T

t=0 (at the T -th iteration we early stop the initialization stage) satisﬁes

1  . . .   u∗

D(cid:0)U∗ U (t)(cid:1) ≤ δ1(Σ) · s∗(cid:112)log d/n
(cid:125)

(cid:123)(cid:122)

(cid:124)

(cid:124)

√
+ δ2(k  s∗  d  n) · 1/

(cid:123)(cid:122)

(cid:125)

t

Statistical Error

Optimization Error

(2)

with high probability. Here D(· ·) is the subspace distance  while s∗ is the sparsity level of U∗  both
of which will be deﬁned in §2. Here δ1(Σ) is a quantity which depends on the population covariance
matrix Σ  while δ2(k  s∗  d  n) depends on k  s∗  d and n (see §4 for details). (3) Within the main

t=T +1 (where (cid:101)T denotes the total number of iterations of sparse

stage  the iterative sequence(cid:8)U (t)(cid:9)T +(cid:101)T

2

orthogonal iteration pursuit) satisﬁes

D(cid:0)U∗ U (t)(cid:1) ≤ δ3(Σ  k) ·

(cid:124)

Optimal Rate

(cid:125)(cid:124)

(cid:123)
(cid:122)
(cid:112)s∗ log d/n
(cid:123)(cid:122)
(cid:125)

Statistical Error

+ γ(Σ)(t−T−1)/4 · D(cid:0)U∗ U (T +1)(cid:1)
(cid:124)
(cid:125)

(cid:123)(cid:122)

Optimization Error

with high probability  where δ3(Σ  k) is a quantity that only depends on Σ and k  and

γ(Σ) = [3λk+1(Σ) + λk(Σ)]/[λk+1(Σ) + 3λk(Σ)] < 1.

Here λk(Σ) and λk+1(Σ) are the k-th and (k + 1)-th eigenvalues of Σ. See §4 for more details.
Unlike previous works  our theory and method don’t depend on the spiked covariance assumption  or
require the data distribution to be Gaussian.

(3)

(4)

Figure 1: An illustration of our two-stage procedure.

Our analysis shows  at the initialization stage  the optimization error decays to zero at the rate of 1/

However  the upper bound of D(cid:0)U∗ U (t)(cid:1) in (2) can’t be smaller than the suboptimal s∗(cid:112)log d/n
as the optimization error term in (3) decreases to zero geometrically  the upper bound of D(cid:0)U∗ U (t)(cid:1)
decreases towards the(cid:112)s∗ log d/n statistical rate of convergence  which is minimax-optimal with

rate of convergence  even with inﬁnite number of iterations. This phenomenon  which is illustrated in
Figure 1  reveals the limit of the convex relaxation approaches for sparse PCA. Within the main stage 

respect to the sparsity level s∗  dimension d and sample size n [17]. Moreover  in Theorem 2 we will
show that  the basin of attraction for the proposed sparse orthogonal iteration pursuit algorithm can
be characterized as

√

t.

U : D(cid:0)U∗ U(cid:1) ≤ R = min

(cid:110)(cid:113)
(cid:111)
kγ(Σ)(cid:2)1 − γ(Σ)1/2(cid:3)/2 (cid:112)2γ(Σ)/4

.

(5)

Here γ(Σ) is deﬁned in (4) and R denotes its radius.
The contribution of this paper is three-fold: (1) We propose the ﬁrst tractable procedure that provably
attains the subspace estimator with minimax-optimal statistical rate of convergence with respect to the
sparsity level s∗  dimension d and sample size n  without relying on the restrictive spiked covariance
assumption or the Gaussian assumption. (2) We propose a novel algorithm named sparse orthogonal
iteration pursuit  which converges to the optimal estimator at a fast geometric rate. The computation
within each iteration is highly efﬁcient compared with convex relaxation approaches. (3) We build a
joint analytic framework that simultaneously captures the computational and statistical properties of
sparse PCA. Under this framework  we characterize the phenomenon of basin of attraction for the
proposed sparse orthogonal iteration pursuit algorithm. In comparison with our previous work on
nonconvex M-estimators [25]  our analysis provides a more general paradigm of solving nonconvex
learning problems with provable guarantees. One byproduct of our analysis is novel techniques for
analyzing the statistical properties of the intermediate solutions of the Alternating Direction Method
of Multipliers [26].
Notation: Let A = [Ai j] ∈ Rd×d and v = (v1  . . .   vd)T ∈ Rd. The (cid:96)q norm (q ≥ 1) of v is (cid:107)v(cid:107)q.
Speciﬁcally  (cid:107)v(cid:107)0 gives the number of nonzero entries of v. For matrix A  the i-th largest eigenvalue
and singular value are λi(A) and σi(A). For q ≥ 1  (cid:107)A(cid:107)q is the matrix operator q-norm  e.g.  we
have (cid:107)A(cid:107)2 = σ1(A). The Frobenius norm is denoted as (cid:107)A(cid:107)F . For A1 and A2  their inner product
is (cid:104)A1  A2(cid:105) = tr(AT
1 A2). For a set S  |S| denotes its cardinality. The d × d identity matrix is Id.

3

UUinitU(t)SuboptimalRateOptimalRateBasinofAttractionConvexRelaxationSparseOrthogonalIterationPursuitFor index sets I J ⊆ {1  . . .   d}  we deﬁne AI J ∈ Rd×d to be the matrix whose (i  j)-th entry is
Ai j if i ∈ I and j ∈ J   and zero otherwise. When I = J   we abbreviate it as AI. If I or J is
{1  . . .   d}  we replace it with a dot  e.g.  AI ·. We denote by Ai ∗ ∈ Rd the i-th row vector of A. A
matrix is orthonormal if its columns are unit length orthogonal vectors. The (p  q)-norm of a matrix 
denoted as (cid:107)A(cid:107)p q  is obtained by ﬁrst taking the (cid:96)p norm of each row  and then taking (cid:96)q norm of
these row norms. We denote diag(A) to be the vector consisting of the diagonal entries of A. With a
little abuse of notation  we denote by diag(v) the the diagonal matrix with v1  . . .   vd on its diagonal.
Hereafter  we use generic numerical constants C  C(cid:48)  C(cid:48)(cid:48)  . . .  whose values change from line to line.

2 Background

In the following  we introduce the distance between subspaces and the notion of sparsity for subspace.
Subspace Distance: Let U and U(cid:48) be two k-dimensional subspaces of Rd. We denote the projection
matrix onto them by Π and Π(cid:48) respectively. One deﬁnition of the distance between U and U(cid:48) is

D(U U(cid:48)) = (cid:107)Π − Π(cid:48)(cid:107)F .

(6)

This deﬁnition is invariant to the rotations of the orthonormal basis.
Subspace Sparsity: For the k-dimensional principal subspace U∗ of Σ  the deﬁnition of its sparsity
should be invariant to the choice of basis  because Σ’s top k eigenvalues might be not distinct. Here
we deﬁne the sparsity level s∗ of U∗ to be the number of nonzero coefﬁcients on the diagonal of its
projection matrix Π∗. One can verify that (see [17] for details)

s∗ =(cid:12)(cid:12)supp[diag(Π∗)](cid:12)(cid:12) = (cid:107)U∗(cid:107)2 0 

(7)
where (cid:107) · (cid:107)2 0 gives the row-sparsity level  i.e.  the number of nonzero rows. Here the columns of U∗
can be any orthonormal basis of U∗. This deﬁnition reduces to the sparsity of u∗
Subspace Estimation: For the k-dimensional s∗-sparse principal subspace U∗ of Σ  [17] considered
the following estimator for the orthonormal matrix U∗ consisting of the basis of U∗ 

1 when k = 1.

subject to U orthonormal  and (cid:107)U(cid:107)2 0 ≤ s∗ 

U∈Rd×k

where (cid:98)Σ is an estimator of Σ. Let (cid:98)U be the column space of (cid:98)U. [17] proved that  assuming (cid:98)Σ is
the sample covariance estimator  and the data are independent sub-Gaussian  (cid:98)U attains the optimal

statistical rate. However  direct computation of this estimator is NP-hard even for k = 1 [22].

(8)

(cid:98)U = argmin

−(cid:10)(cid:98)Σ  UUT(cid:11) 

3 A Two-stage Procedure for Sparse PCA

In this following  we present the two-stage procedure for sparse PCA. We will ﬁrst introduce sparse
orthogonal iteration pursuit for the main stage and then present the convex relaxation for initialization.

Algorithm 1 Main stage: Sparse orthogonal iteration pursuit. Here T denotes the total number of
iterations of the initialization stage. To unify the later analysis  let t start from T + 1.

1: Function: (cid:98)U ← Sparse Orthogonal Iteration Pursuit(cid:0)(cid:98)Σ  Uinit(cid:1)
2: Input: Covariance Matrix Estimator (cid:98)Σ  Initialization Uinit
3: Parameter: Sparsity Parameter(cid:98)s  Maximum Number of Iterations (cid:101)T
4: Initialization: (cid:101)U(T +1) ← Truncate(cid:0)Uinit (cid:98)s(cid:1)  U(T +1)  R(T +1)
5: For t = T + 1  . . .   T + (cid:101)T − 1
(cid:101)V(t+1) ← (cid:98)Σ · U(t) 
(cid:101)U(t+1) ← Truncate(cid:0)V(t+1) (cid:98)s(cid:1) 
9: Output: (cid:98)U ← U(T +(cid:101)T )

6:
7:
8: End For

V(t+1)  R(t+1)
U(t+1)  R(t+1)

1 ← Thin QR(cid:0)(cid:101)V(t+1)(cid:1)
2 ← Thin QR(cid:0)(cid:101)U(t+1)(cid:1)

2

← Thin QR(cid:0)(cid:101)U(T +1)(cid:1)

4

= (cid:101)U(t+1)  where R(t+1)

(cid:101)V(t+1)  and U(t+1) · R(t+1)

Sparse Orthogonal Iteration Pursuit: For the main stage  we propose sparse orthogonal iteration
pursuit (Algorithm 1) to solve (8). In Algorithm 1  Truncate(· ·) (Line 7) is deﬁned in Algorithm
2. In Lines 6 and 7  Thin QR(·) denotes the thin QR decomposition (see [27] for details). In detail 
V(t+1) ∈ Rd×k and U(t+1) ∈ Rd×k are orthonormal matrices  and they satisfy V(t+1) · R(t+1)
=
∈ Rk×k. This decomposition can be
accomplished with O(k2d) operations using Householder algorithm [27]. Here remind that k is the
rank of the principal subspace of interest  which is much smaller than the dimension d.
Algorithm 1 consists of two steps: (1) Line 6 performs a matrix multiplication and a renormalization
using QR decomposition. This step is named orthogonal iteration in numerical analysis [27]. When
the ﬁrst leading eigenvector (k = 1) is of interest  it reduces to the well-known power iteration. The
intuition behind this step can be understood as follows. We consider the minimization problem in (8)

without the row-sparsity constraint. Note that the gradient of the objective function is −2(cid:98)Σ · U(t).

  R(t+1)

2

1

1

2

Hence  the gradient descent update scheme for this problem is

(cid:101)V(t+1) ← Porth

(cid:0)U(t) + η · 2(cid:98)Σ · U(t)(cid:1) 
(cid:0)U(t)+η·2(cid:98)Σ·U(t)(cid:1)=Porth

(9)
where η is the step size  and Porth(·) denotes the renormalization step. [28] showed that the optimal
step size η is inﬁnity. Thus we have Porth
which implies that (9) is equivalent to Line 6. (2) In Line 7  we take a truncation step to enforce the

(cid:0)(cid:98)Σ·U(t)(cid:1) 
row-sparsity constraint in (8). In detail  we greedily select the(cid:98)s most important rows. To enforce
(cid:101)U(t+1) is row-sparse by truncation  and QR decomposition preserves its row-sparsity. By iteratively

the orthonormality constraint in (8)  we perform another renormalization step after the truncation.
Note that the QR decomposition in Line 7 gives a both orthonormal and row-sparse U(t+1)  because

(cid:0)η·2(cid:98)Σ·U(t)(cid:1)=Porth

performing these two steps  we are approximately solving the nonconvex problem in (8). Although
it is not clear whether this procedure achieves the global minimum of (8)  we will prove that  the
obtained estimator enjoys the same optimal statistical rate of convergence as the global minimum.

i ∗

 

i ∗

(cid:13)(cid:13)2’s

for all i ∈ {1  . . .   d}

i ∗ ← 1(cid:0)i ∈ I(cid:98)s

Algorithm 3 Initialization stage: Solving convex relaxation (10) using ADMM. In Lines 6 and 7 

Algorithm 2 Main stage: The Truncate(· ·) function used in Line 7 of Algorithm 1.

This projection can be computed using Algorithm 4 in [29]. The second can be solved by entry-wise
soft-thresholding shown in Algorithm 5 in [29]. We defer these two algorithms and their derivations
to the extended version [29] of this paper.

1: Function: (cid:101)U(t+1) ← Truncate(cid:0)V(t+1) (cid:98)s(cid:1)
2: Row Sorting: I(cid:98)s ← The set of row index i(cid:48)s with the top(cid:98)s largest(cid:13)(cid:13)V(t+1)
(cid:1) · V(t+1)
3: Truncation: (cid:101)U(t+1)
4: Output: (cid:101)U(t+1)
we need to solve two subproblems. The ﬁrst one is equivalent to projecting Φ(t)−Θ(t)+(cid:98)Σ/ρ to A.
1: Function: Uinit ← ADMM(cid:0)(cid:98)Σ(cid:1)
2: Input: Covariance Matrix Estimator (cid:98)Σ
6: Π(t+1)← argmin(cid:8)L(cid:0)Π  Φ(t)  Θ(t)(cid:1) + β/2 ·(cid:13)(cid:13)Π − Φ(t)(cid:13)(cid:13)2
7: Φ(t+1)← argmin(cid:8)L(cid:0)Π(t+1)  Φ  Θ(t)(cid:1) + β/2 ·(cid:13)(cid:13)Π(t+1) − Φ(cid:13)(cid:13)2
8: Θ(t+1)←Θ(t) − β(cid:0)Π(t+1) − Φ(t+1)(cid:1)
10: Π(T ) ← 1/T ·(cid:80)T

3: Parameter: Regularization Parameter ρ > 0 in (10)  Penalty Parameter β > 0 of the Augmented
4: Π(0) ← 0  Φ(0) ← 0  Θ(0) ← 0
5: For t = 0  . . .   T − 1

(cid:12)(cid:12) Π ∈ A(cid:9)
(cid:12)(cid:12) Φ ∈ B(cid:9)

t=0 Π(t)  let the columns of Uinit be the top k leading eigenvectors of Π(T )

Lagrangian  Maximum Number of Iterations T

F

F

9: End For
11: Output: Uinit ∈ Rd×k

5

Convex Relaxation for Initialization: To obtain a good initialization for sparse orthogonal iteration
pursuit  we consider the following convex minimization problem proposed by [5  13]

(cid:110)−(cid:10)(cid:98)Σ  Π(cid:11) + ρ(cid:107)Π(cid:107)1 1

(cid:12)(cid:12) tr(Π) = k  0 (cid:22) Π (cid:22) Id

(cid:111)

 

(10)

minimize

which relaxes the combinatorial optimization problem in (8). The intuition behind this relaxation can
be understood as follows: (1) Π is a reparametrization for UUT in (8)  which is a projection matrix
with k nonzero eigenvalues of 1. In (10)  this constraint is relaxed to tr(Π) = k and 0 (cid:22) Π (cid:22) Id 
which indicates that the eigenvalues of Π should be in [0  1] while the sum of them is k. (2) For the
row-sparsity constraint in (8)  [13] proved that (cid:107)Π∗(cid:107)0 0 ≤ |supp[diag(Π∗)]|2 = (cid:107)U∗(cid:107)2
2 0 = (s∗)2.
Correspondingly  the row-sparsity constraint in (8) translates to (cid:107)Π(cid:107)0 0 ≤ (s∗)2  which is relaxed to
the regularization term (cid:107)Π(cid:107)1 1 in (10). For notational simplicity  we deﬁne

(11)
Note (10) has both nonsmooth regularization term and nontrivial constraint A. We use the Alternating
Direction Method of Multipliers (ADMM  Algorithm 3). It considers the equivalent form of (10)

A =(cid:8)Π : Π ∈ Rd×d  tr(Π) = k  0 (cid:22) Π (cid:22) Id
(cid:12)(cid:12) Π = Φ  Π ∈ A  Φ ∈ B(cid:111)

(cid:110)−(cid:10)(cid:98)Σ  Π(cid:11) + ρ(cid:107)Φ(cid:107)1 1

and iteratively minimizes the augmented Lagrangian L(Π  Φ  Θ) + β/2 · (cid:107)Π − Φ(cid:107)2

L(Π  Φ  Θ) = −(cid:10)(cid:98)Σ  Π(cid:11) + ρ(cid:107)Φ(cid:107)1 1 − (cid:104)Θ  Π − Φ(cid:105)  Π ∈ A  Φ ∈ B  Θ ∈ Rd×d

(13)
is the Lagrangian corresponding to (12)  Θ ∈ Rd×d is the Lagrange multiplier associated with the
equality constraint Π = Φ  and β > 0 is a penalty parameter that enforces such an equality constraint.
Note that other variants of ADMM  e.g.  Peaceman-Rachford Splitting Method [30] is also applicable 
which would yield similar theoretical guarantees along with improved practical performance.

F   where

  where B = Rd×d 

minimize

(cid:9) .

(12)

4 Theoretical Results
To describe our results  we deﬁne the model class Md(Σ  k  s∗) as follows 

X = Σ1/2Z  where Z ∈ Rd is sub-Gaussian with mean zero 

variance proxy less than 1  and covariance matrix Id;

Md(Σ  k  s∗) :

The k-dimensional principal subspace U∗ of Σ is s∗-sparse; λk(Σ)−λk+1(Σ)>0.
where Σ1/2 satisﬁes Σ1/2·Σ1/2 = Σ. Here remind the sparsity of U∗ is deﬁned in (7) and λj(Σ) is
the j-th eigenvalue of Σ. For notational simplicity  hereafter we abbreviate λj(Σ) as λj. This model
class doesn’t restrict Σ to spiked covariance matrices  where the (k + 1)  . . .   d-th eigenvalues of
Σ can only be identical. Moreover  we don’t require X to be exactly Gaussian  which is a crucial
requirement in several previous works  e.g.  [12  18].
We ﬁrst introduce some notation. Remind D(· ·) is the subspace distance deﬁned in (6). Note that
γ(Σ) < 1 is deﬁned in (4) and will be abbreviated as γ hereafter. We deﬁne

1 

(cid:111)2 · (λk − λk+1)2/λ2
(cid:105) ·(cid:0)k · s∗ · d2 log d/n(cid:1)1/4
(cid:105) ·(cid:112)s∗·(k + log d)/n 

(14)

 

(15)

(16)

which denotes the required sample complexity. We also deﬁne

nmin = C · (s∗)2 log d · min

(cid:110)(cid:113)
ζ1 = [Cλ1/(λk−λk+1)] · s∗(cid:112)log d/n 

k · γ(1 − γ1/2)/2 (cid:112)2γ/4
4/(cid:112)λk−λk+1
k · [λk/(λk − λk+1)]2 ·(cid:104)(cid:112)λ1λk+1/(λk − λk+1)
Tmin =(cid:6)ζ 2

2 /(R − ζ1)2(cid:7)  

which will be used in the analysis of the ﬁrst stage  and

ξ1 = C

ζ2 =

(cid:104)

√

which will be used in the analysis of the main stage. Meanwhile  remind the radius of the basin of
attraction for sparse orthogonal iteration pursuit is deﬁned in (5). We deﬁne

(17)
as the required minimum numbers of iterations of the two stages respectively. The following results
will be proved in the extended version [29] of this paper accordingly.
Main Result: Recall that U (t) denotes the subspace spanned by the columns of U(t) in Algorithm 1.

(cid:101)Tmin = 4(cid:100)log(R/ξ1)/log(1/γ)(cid:101)

6

√

Theorem 1. Let x1  . . .   xn be independent realizations of X ∈ Md(Σ  k  s∗) with n ≥ nmin  and

a sufﬁciently large C > 0 in (10) and the penalty parameter β of ADMM (Line 3 of Algorithm 3)
is β = d · ρ/

(cid:112)log d/n for
(cid:98)Σ be the sample covariance matrix. Suppose the regularization parameter ρ = Cλ1
k. Also  suppose the sparsity parameter(cid:98)s in Algorithm 1 (Line 3) is chosen such
that(cid:98)s = C max(cid:8)(cid:6)4k/(γ−1/2 − 1)2(cid:7)   1(cid:9) · s∗  where C ≥ 1 is an integer constant. After T ≥ Tmin
iterations of Algorithm 3 and then (cid:101)T ≥ (cid:101)Tmin iterations of Algorithm 1  we obtain (cid:98)U = U (T +(cid:101)T ) and
(cid:105) ·(cid:112)s∗·(k + log d)/n
D(cid:0)U∗ (cid:98)U(cid:1) ≤ Cξ1 = C(cid:48)√
(cid:102)Md(Σ  k  s∗  κ)  which is the same as Md(Σ  k  s∗) except the eigengap of Σ satisﬁes λk − λk+1 >
λk − λk+1 ≥ κλ1  which is more restrictive because λ1 ≥ λk. Within (cid:102)M  we assume that the rank k

k · [λk/(λk − λk+1)]2 ·(cid:104)(cid:112)λ1λk+1/(λk − λk+1)

Minimax-Optimality: To establish the optimality of Theorem 1  we consider a smaller model class

κλk for some constant κ > 0. This condition is mild compared to previous works  e.g.  [12] assumes

with high probability. Here the equality follows from the deﬁnition of ξ1 in (16).

sup

of the principal subspace is ﬁxed. This assumption is reasonable  e.g.  in applications like population
genetics [31]  the rank k of principal subspaces represents the number of population groups  which
doesn’t increase when the sparsity level s∗  dimension d and sample size n are growing.
Theorem 3.1 of [17] implies the following minimax lower bound

E D(cid:0)(cid:101)U U∗(cid:1)2 ≥ Cλ1λk+1/(λk−λk+1)2 · (s∗−k) ·(cid:8)k + log[(d−k)/(s∗−k)](cid:9)/n 
inf(cid:101)U
X∈(cid:102)Md(Σ k s∗)
where (cid:101)U denotes any principal subspace estimator. Suppose s∗ and d are sufﬁciently large (to avoid
By Lemma 2.1 in [29]  we have D(cid:0)U∗ (cid:98)U(cid:1) ≤ √
above within (cid:102)Md(Σ  k  s∗  κ)  up to the 1/4 constant in front of log d and a total constant of k · κ−4.
Theorem 2. Under the same condition as in Theorem 1  and provided that D(cid:0)U∗ U init(cid:1) ≤ R  the

trivial cases)  the right-hand side is lower bounded by C(cid:48)λ1λk+1/(λk−λk+1)2·s∗(k+1/4·log d)/n.
2k. For n  d and s∗ sufﬁciently large  it is easy to
derive the same upper bound in expectation from in Theorem 1. It attains the minimax lower bound

Analysis of the Main Stage: Remind that U (t) is the subspace spanned by the columns of U(t) in
Algorithm 1  and the initialization is Uinit while its column space is U init.

iterative sequence U (T +1) U (T +2)  . . .  U (t)  . . . satisﬁes

D(cid:0)U∗ U (t)(cid:1) ≤

Cξ1(cid:124)(cid:123)(cid:122)(cid:125)

+

γ(t−T−1)/4 · γ−1/2R

(cid:123)(cid:122)

(cid:125)

(cid:124)

Statistical Error

Optimization Error

(18)

with high probability  where ξ1 is deﬁned in (16)  R is deﬁned in (5)  and γ is deﬁned in (4).
Theorem 2 shows that  as long as U init falls into its basin of attraction  sparse orthogonal iteration
pursuit converges at a geometric rate of convergence in optimization error since γ < 1. According to
the deﬁnition of γ in (4)  when λk is close to λk+1  γ is close to 1  then the optimization error term
decays at a slower rate. Here the optimization error doesn’t increase with dimension d  which makes
this algorithm suitable to solve ultra high dimensional problems. In (18)  when t is sufﬁciently large

such that γ(t−T−1)/4·γ−1/2R≤ ξ1  D(cid:0)U∗ U (t)(cid:1) is upper bounded by 2Cξ1  which gives the optimal
statistical rate. Solving t in this inequality  we obtain that t = (cid:101)T ≥ (cid:101)Tmin  which is deﬁned in (17).
Analysis of the Initialization Stage: Let Π(t) = 1/t·(cid:80)t

3. Let U (t) be the k-dimensional subspace spanned by the top k leading eigenvectors of Π(t).
Theorem 3. Under the same condition as in Theorem 1  the iterative sequence of k-dimensional
subspaces U (0) U (1)  . . .  U (t)  . . . satisﬁes

i=1 Π(i) where Π(i) is deﬁned in Algorithm

D(cid:0)U∗ U (t)(cid:1) ≤

ζ1(cid:124)(cid:123)(cid:122)(cid:125)

+

ζ2 · 1/

(cid:123)(cid:122)

√

(cid:125)

t

(cid:124)

Statistical Error

Optimization Error

(19)

with high probability. Here ζ1 and ζ2 are deﬁned in (15).

7

√

In Theorem 3 the optimization error term decays to zero at the rate of 1/
t. Note that ζ2 increases
d · (log d)1/4. That is to say  computationally convex relaxation is less efﬁcient
with d at the rate of
than sparse orthogonal iteration pursuit  which justiﬁes the early stopping of ADMM. To ensure U (T )
√
T ≤ R. Solving T gives T ≥ Tmin where Tmin is
enters the basin of attraction  we need ζ1 + ζ2/
deﬁned in (17). The proof of Theorem 3 is a nontrivial combination of optimization and statistical
analysis under the variational inequality framework  which is provided in the extended version [29]
of this paper with detail.

√

(a)

(b)

(c)

(d)

(e)

Figure 2: An Illustration of main results. See §5 for detailed experiment settings and the interpretation.

Table 1: A comparison of subspace estimation error with existing sparse PCA procedures. The error

is measured by D(U∗ (cid:98)U) deﬁned in (6). Standard deviations are provided in the parentheses.
D(U∗ (cid:98)U) for Setting (i) D(U∗ (cid:98)U) for Setting (ii)

Procedure

Our Procedure

Convex Relaxation [13]

TPower [11] + Deﬂation Method [24]
GPower [10] + Deﬂation Method [24]
PathSPCA [8] + Deﬂation Method [24]

0.32 (0.0067)
1.62 (0.0398)
1.15 (0.1336)
1.84 (0.0226)
2.12 (0.0226)

0.064 (0.00016)

0.57 (0.021)
0.01 (0.00042)
1.75 (0.029)
2.10 (0.018)

(i): d = 200  s = 10  k = 5  n = 50  Σ’s eigenvalues are {100  100  100  100  4  1  . . .   1};
(ii): The same as (i) except n = 100  Σ’s eigenvalues are {300  240  180  120  60  1  . . .   1}.

5 Numerical Results
Figure 2 illustrates the main theoretical results. For (a)-(c)  we set d=200  s∗=10  k=5  n=100  and
Σ’s eigenvalues are {100  100  100  100  10  1  . . .   1}. In detail  (a) illustrates the 1/
t decay of
optimization error at the initialization stage; (b) illustrates the decay of the total estimation error (in
log-scale) at the main stage; (c) illustrates the basin of attraction phenomenon  as well as the geometric
decay of optimization error (in log-scale) of sparse orthogonal iteration pursuit as characterized in §4.
For (d) and (e)  the eigenstructure is the same  while d  n and s∗ take multiple values. They show that

the theoretical(cid:112)s∗ log d/n statistical rate of our estimator is tight in practice.

√

In Table 1  we compare the subspace error of our procedure with existing methods  where all except
our procedure and convex relaxation [13] leverage the deﬂation method [24] for subspace estimation
with k > 1. We consider two settings: Setting (i) is more challenging than setting (ii)  since the top k
eigenvalues of Σ are not distinct  the eigengap is small and the sample size is smaller. Our procedure
signiﬁcantly outperforms other existing methods on subspace recovery in both settings.

Acknowledgement:
IIS1332109  NIH R01MH102339  NIH R01GM083084  and NIH R01HG06841.

This research is partially supported by the grants NSF IIS1408910  NSF

References
[1] I. Johnstone  A. Lu. On consistency and sparsity for principal components analysis in high dimensions 

Journal of the American Statistical Association 2009;104:682–693.

8

1020301.522.53tD(U∗ U(t))InitialStage510152010−1100tD(U∗ U(t))MainStage102030100tD(U∗ U(t))−D(U∗ U(T+eT)) InitialStageMainStage11.520.20.40.60.81ps∗logd/nD(U∗ bU)n=60 d=128d=192d=2560.60.811.21.41.61.80.20.40.6ps∗logd/nD(U∗ bU)n=100 d=128d=192d=256[2] D. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance model  Statistica

Sinica 2007;17:1617.

[3] B. Nadler. Finite sample approximation results for principal component analysis: A matrix perturbation

approach  The Annals of Statistics 2008:2791–2817.

[4] I. Jolliffe  N. Trendaﬁlov  M. Uddin. A modiﬁed principal component technique based on the Lasso 

Journal of Computational and Graphical Statistics 2003;12:531–547.

[5] A. d’Aspremont  L. E. Ghaoui  M. I. Jordan  G. R. Lanckriet. A Direct Formulation for Sparse PCA Using

Semideﬁnite Programming  SIAM Review 2007:434–448.

[6] H. Zou  T. Hastie  R. Tibshirani. Sparse principal component analysis  Journal of computational and

graphical statistics 2006;15:265–286.

[7] H. Shen  J. Huang. Sparse principal component analysis via regularized low rank matrix approximation 

Journal of Multivariate Analysis 2008;99:1015–1034.

[8] A. d’Aspremont  F. Bach  L. Ghaoui. Optimal solutions for sparse principal component analysis  The

Journal of Machine Learning Research 2008;9:1269–1294.

[9] D. Witten  R. Tibshirani  T. Hastie. A penalized matrix decomposition  with applications to sparse principal

components and canonical correlation analysis  Biostatistics 2009;10:515–534.

[10] M. Journ´ee  Y. Nesterov  P. Richt´arik  R. Sepulchre. Generalized power method for sparse principal

component analysis  The Journal of Machine Learning Research 2010;11:517–553.

[11] X.-T. Yuan  T. Zhang. Truncated power method for sparse eigenvalue problems  The Journal of Machine

Learning Research 2013;14:899–925.

[12] Z. Ma. Sparse principal component analysis and iterative thresholding  The Annals of Statistics 2013;41.
[13] V. Q. Vu  J. Cho  J. Lei  K. Rohe. Fantope projection and selection: A near-optimal convex relaxation of

sparse PCA  in Advances in Neural Information Processing Systems:2670–2678 2013.

[14] A. Amini  M. Wainwright. High-dimensional analysis of semideﬁnite relaxations for sparse principal

components  The Annals of Statistics 2009;37:2877–2921.

[15] V. Q. Vu  J. Lei. Minimax Rates of Estimation for Sparse PCA in High Dimensions  in International

Conference on Artiﬁcial Intelligence and Statistics:1278–1286 2012.

[16] A. Birnbaum  I. M. Johnstone  B. Nadler  D. Paul  others. Minimax bounds for sparse PCA with noisy

high-dimensional data  The Annals of Statistics 2013;41:1055–1084.

[17] V. Q. Vu  J. Lei. Minimax sparse principal subspace estimation in high dimensions  The Annals of Statistics

2013;41:2905–2947.

[18] T. T. Cai  Z. Ma  Y. Wu  others. Sparse PCA: Optimal rates and adaptive estimation  The Annals of Statistics

2013;41:3074–3110.

[19] Q. Berthet  P. Rigollet. Optimal detection of sparse principal components in high dimension  The Annals of

Statistics 2013;41:1780–1815.

[20] Q. Berthet  P. Rigollet. Complexity Theoretic Lower Bounds for Sparse Principal Component Detection  in

COLT:1046-1066 2013.

[21] J. Lei  V. Q. Vu. Sparsistency and Agnostic Inference in Sparse PCA  arXiv:1401.6978 2014.
[22] B. Moghaddam  Y. Weiss  S. Avidan. Spectral bounds for sparse PCA: Exact and greedy algorithms 

Advances in neural information processing systems 2006;18:915.

[23] K. Ball. An elementary introduction to modern convex geometry  Flavors of geometry 1997;31:1–58.
[24] L. Mackey. Deﬂation methods for sparse PCA  Advances in neural information processing systems

2009;21:1017–1024.

[25] Z. Wang  H. Liu  T. Zhang. Optimal computational and statistical rates of convergence for sparse nonconvex

learning problems  The Annals of Statistics 2014;42:2164–2201.
[26] S. Boyd  N. Parikh  E. Chu  B. Peleato  J. Eckstein. Distributed optimization and statistical learning via the
alternating direction method of multipliers  Foundations and Trends R(cid:13) in Machine Learning 2011;3:1–122.

[27] G. H. Golub  C. F. Van Loan. Matrix computations. Johns Hopkins University Press 2012.
[28] R. Arora  A. Cotter  K. Livescu  N. Srebro. Stochastic optimization for PCA and PLS  in Communication 

Control  and Computing (Allerton)  2012 50th Annual Allerton Conference on:861–868IEEE 2012.

[29] Z. Wang  H. Lu  H. Liu. Nonconvex statistical optimization: Minimax-optimal Sparse PCA in polynomial

time  arXiv:1408.5352 2014.

[30] B. He  H. Liu  Z. Wang  X. Yuan. A Strictly Contractive Peaceman–Rachford Splitting Method for Convex

Programming  SIAM Journal on Optimization 2014;24:1011–1040.

[31] B. E. Engelhardt  M. Stephens. Analysis of population structure: a unifying framework and novel methods

based on sparse factor analysis  PLoS genetics 2010;6:e1001117.

9

,Zhaoran Wang
Huanran Lu
Han Liu
Roland Kwitt
Stefan Huber
Marc Niethammer
Weili Lin
Ulrich Bauer
Behnam Neyshabur
Yuhuai Wu
Russ Salakhutdinov
Nati Srebro
Francesco Locatello
Gideon Dresdner
Rajiv Khanna
Isabel Valera
Gunnar Raetsch
Zhiqing Sun
Zhuohan Li
Haoqing Wang
Di He
Zi Lin
Zhihong Deng