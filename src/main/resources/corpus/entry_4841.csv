2010,Sample Complexity of Testing the Manifold Hypothesis,The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper  we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error.   Given upper bounds on the dimension  volume  and curvature  we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is {\it independent} of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature  exponentially on the intrinsic dimension  and linearly on the intrinsic volume. For constant error  we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension  volume and curvature is unavoidable. Whether the known lower bound of $O(\frac{k}{\eps^2} + \frac{\log \frac{1}{\de}}{\eps^2})$ for the sample complexity of Empirical Risk minimization on $k-$means applied to data in a unit ball of arbitrary dimension is tight  has been an open question since 1997 \cite{bart2}. Here $\eps$ is the desired bound on the error and $\de$ is a bound on the probability of failure. We improve the best currently known upper bound \cite{pontil} of $O(\frac{k^2}{\eps^2} + \frac{\log \frac{1}{\de}}{\eps^2})$ to $O\left(\frac{k}{\eps^2}\left(\min\left(k  \frac{\log^4 \frac{k}{\eps}}{\eps^2}\right)\right) + \frac{\log \frac{1}{\de}}{\eps^2}\right)$. Based on these results  we devise a simple algorithm for $k-$means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to  high dimensional data  where the sample complexity is independent of the ambient dimension.,Sample complexity of testing the manifold hypothesis

Hariharan Narayanan∗

Laboratory for Information and Decision Systems

Laboratory for Information and Decision Systems

EECS  MIT

Cambridge  MA 02139

har@mit.edu
Sanjoy Mitter

EECS  MIT

Cambridge  MA 02139
mitter@mit.edu

Abstract

The hypothesis that high dimensional data tends to lie in the vicinity of a low di-
mensional manifold is the basis of a collection of methodologies termed Manifold
Learning. In this paper  we study statistical aspects of the question of ﬁtting a
manifold with a nearly optimal least squared error. Given upper bounds on the
dimension  volume  and curvature  we show that Empirical Risk Minimization
can produce a nearly optimal manifold using a number of random samples that is
independent of the ambient dimension of the space in which data lie. We obtain
an upper bound on the required number of samples that depends polynomially on
the curvature  exponentially on the intrinsic dimension  and linearly on the intrin-
sic volume. For constant error  we prove a matching minimax lower bound on the
sample complexity that shows that this dependence on intrinsic dimension  volume
2 + log 1
and curvature is unavoidable. Whether the known lower bound of O( k
2 )
for the sample complexity of Empirical Risk minimization on k−means applied
to data in a unit ball of arbitrary dimension is tight  has been an open question
since 1997 [3]. Here  is the desired bound on the error and δ is a bound on the
probability of failure. We improve the best currently known upper bound [14] of
O( k2
. Based on these results  we
devise a simple algorithm for k−means and another that uses a family of convex
programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional
data  where the sample complexity is independent of the ambient dimension.

2 + log 1

k  log4 k
2

δ

2 ) to O

+ log 1
2

δ

(cid:17)(cid:17)

(cid:16)

(cid:16) k

2

min



(cid:17)

(cid:16)

δ

1

Introduction

We are increasingly confronted with very high dimensional data in speech signals  images  gene-
expression data  and other sources. Manifold Learning can be loosely deﬁned to be a collection of
methodologies that are motivated by the belief that this hypothesis (henceforth called the manifold
hypothesis) is true. It includes a number of extensively used algorithms such as Locally Linear
Embedding [17]  ISOMAP [19]  Laplacian Eigenmaps [4] and Hessian Eigenmaps [8]. The sample
complexity of classiﬁcation is known to be independent of the ambient dimension [15] under the
manifold hypothesis  (assuming the decision boundary is a manifold as well ) thus obviating the
curse of dimensionality. A recent empirical study [6] of a large number of 3× 3 images  represented
as points in R9 revealed that they approximately lie on a two dimensional manifold known as the

∗Research supported by grant CCF-0836720

1

Figure 1: Fitting a torus to data.

Klein bottle. On the other hand  knowledge that the manifold hypothesis is false with regard to
certain data would give us reason to exercise caution in applying algorithms from manifold learning
and would provide an incentive for further study.
It is thus of considerable interest to know whether given data lie in the vicinity of a low dimensional
manifold. Our primary technical results are the following.

1. We obtain uniform bounds relating the empirical squared loss and the true squared loss
over a class F consisting of manifolds whose dimensions  volumes and curvatures are
bounded in Theorems 1 and 2. These bounds imply upper bounds on the sample complexity
of Empirical Risk Minimization (ERM) that are independent of the ambient dimension 
exponential in the intrinsic dimension  polynomial in the curvature and almost linear in the
volume.

2. We obtain a minimax lower bound on the sample complexity of any rule for learning a
manifold from F in Theorem 6 showing that for a ﬁxed error  the the dependence of the
sample complexity on intrinsic dimension  curvature and volume must be at least exponen-
tial  polynomial  and linear  respectively.

3. We improve the best currently known upper bound [14] on the sample complexity of Em-
pirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimen-
sion from O( k2
2 ) to O
. Whether the known lower
2 ) is tight  has been an open question since 1997 [3]. Here  is the

bound of O( k
desired bound on the error and δ is a bound on the probability of failure.

2 + log 1
2 + log 1

+ log 1
2

δ

k  log4 k
2



min

δ

δ

(cid:16)

(cid:16) k

2

(cid:16)

(cid:17)(cid:17)

(cid:17)

One technical contribution of this paper is the use of dimensionality reduction via random projec-
tions in the proof of Theorem 5 to bound the Fat-Shattering dimension of a function class  elements
of which roughly correspond to the squared distance to a low dimensional manifold. The application
of the probabilistic method involves a projection onto a low dimensional random subspace. This is
then followed by arguments of a combinatorial nature involving the VC dimension of halfspaces 
and the Sauer-Shelah Lemma applied with respect to the low dimensional subspace. While random
projections have frequently been used in machine learning algorithms  for example in [2  7]  to our
knowledge  they have not been used as a tool to bound the complexity of a function class. We il-
lustrate the algorithmic utility of our uniform bound by devising an algorithm for k−means and a
convex programming algorithm for ﬁtting a piecewise linear curve of bounded length. For a ﬁxed
error threshold and length  the dependence on the ambient dimension is linear  which is optimal
since this is the complexity of reading the input.

2 Connections and Related work

In the context of curves  [10] proposed “Principal Curves”  where it was suggested that a natural
curve that may be ﬁt to a probability distribution is one where every point on the curve is the center
of mass of all those points to which it is the nearest point. A different deﬁnition of a principal curve
was proposed by [12]  where they attempted to ﬁnd piecewise linear curves of bounded length which
minimize the expected squared distance to a random point from a distribution. This paper studies
the decay of the error rate as the number of samples tends to inﬁnity  but does not analyze the
dependence of the error rate on the ambient dimension and the bound on the length. We address this
in a more general setup in Theorem 4  and obtain sample complexity bounds that are independent of

2

δ

2 + log 1

the ambient dimension  and depend linearly on the bound on the length. There is a signiﬁcant amount
of recent research aimed at understanding topological aspects of data  such its homology [20  16].
It has been an open question since 1997 [3]  whether the known lower bound of O( k
2 ) for
the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball
of arbitrary dimension is tight. Here  is the desired bound on the error and δ is a bound on the
probability of failure. The best currently known upper bound is O( k2
2 ) and is based on
Rademacher complexities. We improve this bound to O
  using an
argument that bounds the Fat-Shattering dimension of the appropriate function class using random
projections and the Sauer-Shelah Lemma. Generalizations of principal curves to parameterized
principal manifolds in certain regularized settings have been studied in [18]. There  the sample
complexity was related to the decay of eigenvalues of a Mercer kernel associated with the regularizer.
When the manifold to be ﬁt is a set of k points (k−means)  we obtain a bound on the sample
complexity s that is independent of m and depends at most linearly on k  which also leads to an
approximation algorithm with additive error  based on sub-sampling. If one allows a multiplicative
error of 4 in addition to an additive error of   a statement of this nature has been proven by Ben-
David (Theorem 7  [5]).

2 + log 1
k  log4 k
2

(cid:16) k

+ log 1
2

(cid:17)(cid:17)

(cid:17)

(cid:16)

(cid:16)

min

2

δ



δ

3 Upper bounds on the sample complexity of Empirical Risk Minimization

unit ball B in Rm  let L(M P) :=(cid:82) d(M  x)2dP(x). Given a set of i.i.d points x = {x1  . . .   xs}
manifold in Merm(x) ∈ F such that(cid:80)s

In the remainder of the paper  C will always denote a universal constant which may differ across
the paper. For any submanifold M contained in  and probability distribution P supported on the
from P  a tolerance  and a class of manifolds F  Empirical Risk Minimization (ERM) outputs a
i=1 d(xi Merm)2 ≤ /2+infN∈F d(xi N )2. Given error
parameters   δ  and a rule A that outputs a manifold in F when provided with a set of samples  we
deﬁne the sample complexity s = s(  δ A) to be the least number such that for any probability
distribution P in the unit ball  if the result of A applied to a set of at least s i.i.d random samples
from P is N   then P [L(N  P) < infM∈F L(M P) + ] > 1 − δ.

3.1 Bounded intrinsic curvature
Let M be a Riemannian manifold and let p ∈ M. Let ζ be a geodesic starting at p.
Deﬁnition 1. The ﬁrst point on ζ where ζ ceases to minimize distance is called the cut point of p
along M. The cut locus of p is the set of cut points of M. The injectivity radius is the minimum
taken over all points of the distance between the point and its cut locus. M is complete if it is
complete as a metric space.
Let Gi = Gi(d  V  λ  ι) be the family of all isometrically embedded  complete Riemannian sub-
manifolds of B having dimension less or equal to d  induced d−dimensional volume less or
equal to V   sectional curvature less or equal to λ and injectivity radius greater or equal to ι. Let
Uint( 1
Theorem 1. Let  and δ be error parameters. If

  which for brevity  we denote Uint.

   d  V  λ  ι) := V

min( ι λ−1/2)

(cid:16)

C

d

(cid:17)(cid:17)d
(cid:18) Uint

(cid:19)

(cid:16)
(cid:18)

(cid:18)(cid:18) 1

(cid:19)

s ≥ C

min

(cid:19)

 

log4

  Uint

2 +

1
2 log

1
δ

2



and x = {x1  . . .   xs} is a set of i.i.d points from P then 

(cid:20)
L(Merm(x) P) − infM∈Gi
The proof of this theorem is deferred to Section 4.

P

L(M P) < 

> 1 − δ.

(cid:19) Uint
(cid:21)

3.2 Bounded extrinsic curvature

We will consider submanifolds of B that have the property that around each of them  for any radius
r < τ  the boundary of the set of all points within a distance r is smooth. This class of submanifolds

3

has appeared in the context of manifold learning [16  15]. The condition number is deﬁned as
follows.
Deﬁnition 2 (Condition Number). Let M be a smooth d−dimensional submanifold of Rm. We
deﬁne the condition number c(M) to be 1
τ   where τ is the largest number to have the property that
for any r < τ no two normals of length r that are incident on M have a common point unless it is
on M.
Let Ge = Ge(d  V  τ ) be the family of Riemannian submanifolds of B having dimension ≤ d 
volume ≤ V and condition number ≤ 1
   d  τ ) :=
V
min( τ )
Theorem 2. If

τ . Let  and δ be error parameters. Let Uext( 1

  which for brevity  we denote by Uext.

(cid:17)(cid:17)d

(cid:16)

(cid:16)

C

d

s ≥ C

min

(cid:19)

(cid:19)

(cid:18)(cid:18) 1

(cid:18) Uext

(cid:18)
(cid:19) Uext
P(cid:104)L(Merm(x) P) − infM L(M P) < 
(cid:105)

  Uext

log4

2



and x = {x1  . . .   xs} is a set of i.i.d points from P then 

4 Relating bounded curvature to covering number

(cid:19)

2 +

1
2 log

1
δ

> 1 − δ.

 

(1)

In this subsection  we note that that bounds on the dimension  volume  sectional curvature and
injectivity radius sufﬁce to ensure that they can be covered by relatively few −balls. Let V M
be
the volume of a ball of radius M centered around a point p. See ([9]  page 51) for a proof of the
following theorem.
Theorem 3 (Bishop-G¨unther Inequality). Let M be a complete Riemannian manifold and assume
that r is not greater than the injectivity radius ι. Let KM denote the sectional curvature of M and
let λ > 0 be a constant. Then  KM ≤ λ implies V M

(cid:16) sin(t

(cid:17)n−1

(cid:82) r

λ)√

dt.

√

p

p (r) ≥ 2π
n
2
Γ( n
2 )

0

λ

Cd

)  then  V M

− 1
Thus  if  < min(ι  πλ
2
2

Proof of Theorem 1. As a consequence of Theorem 3  we obtain an upper bound of V (cid:0) Cd
(cid:19)d
balls (cid:83)

the number of disjoint sets of the form M ∩ B/32(p) that can be packed in M.
If {M ∩
B/32(p1)  . . .  M∩B/32(pk)} is a maximal family of disjoint sets of the form M∩B/32(p)  then
there is no point p ∈ M such that min
(cid:107)p − pi(cid:107) > /16. Therefore  M is contained in the union of

B/16(pi). Therefore  we may apply Theorem 4 with U(cid:0) 1

(cid:1) ≤ V

(cid:1)d on

(cid:18)

Cd

.



i



− 1

min( λ

2  ι)

i

p () >(cid:0) 

(cid:1)d.

The proof of Theorem 2 is along the lines of that of Theorem 1  so it has been deferred to the journal
version.

5 Class of manifolds with a bounded covering number

In this section  we show that uniform bounds relating the empirical squares loss and the expected
squared loss can be obtained for a class of manifolds whose covering number at a different scale 
has a speciﬁed upper bound. Let U : R+ → Z+ be any integer valued function. Let G be any family
of subsets of B such that for all r > 0 every element M ∈ G can be covered using open Euclidean
balls of radius r centered around U ( 1
r ) points; let this set be ΛM(r). Note that if the subsets consist
of k−tuples of points  U (1/r) can be taken to be the constant function equal to k and we recover
the k−means question. A priori  it is unclear if

(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

sup
M∈G

(cid:12)(cid:12)(cid:12)(cid:12) 

i=1 d(xi M)2

− EP d(x M)2

s

4

(2)

(cid:18)

2 min

(cid:18) U (16/)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s
(cid:20)
d2(x  g) ≤ (cid:16) 

sup
M∈G

s

i=1 d(xi M)2

is a random variable  since the supremum of a set of random variables is not always a random
variable (although if the set is countable this is true). However (2) is equal to
− EP d(x  ΛM(1/n))2

i=1 d(xi  ΛM(1/n))2

(3)

n→∞ sup
lim
M∈G

s

(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

(cid:12)(cid:12)(cid:12)(cid:12) 

and for each n  the supremum in the limits is over a set parameterized by U (n) points  which without
loss of generality we may take to be countable (due to the density and countability of rational points).
Thus  for a ﬁxed n  the quantity in the limits is a random variable. Since the limit as n → ∞ of a
sequence of bounded random variables is a random variable as well  (2) is a random variable too.
Theorem 4. Let  and δ be error parameters. If

(cid:18) 1

(cid:19)

2

U (16/) 

log4

(cid:19)

 

+

1
2 log

1
δ

(cid:19)(cid:19)

(cid:18) U (16/)
(cid:12)(cid:12)(cid:12)(cid:12) <
(cid:21)


2



− EP d(x M)2

> 1 − δ.

(4)

Then 

s ≥ C

P

(cid:17)2

Proof. For every g ∈ G  let c(g  ) = {c1  . . .   ck} be a set of k := U (16/) points in g ⊆ B  such
that g is covered by the union of balls of radius /16 centered at these points. Thus  for any point
x ∈ B 

+ d(x  c(g  ))

 mini (cid:107)x − ci(cid:107)

+ d(x  c(g  ))2.

(5)

(6)

16
≤ 2
256

+

8

Since mini (cid:107)x − ci(cid:107) is less or equal to 2  the last expression is less than 
2 + d(x  c(g  ))2.
Our proof uses the “kernel trick” in conjunction with Theorem 5. Let Φ : (x1  . . .   xm)T (cid:55)→
2−1/2(x1  . . .   xm  1)T map a point x ∈ Rm to one in Rm+1. For each i  let ci := (ci1  . . .   cim)T  
and ˜ci := 2−1/2(−ci1  . . .  −cim 
)T . The factor of 2−1/2 is necessitated by the fact that we
wish the image of a point in the unit ball to also belong to the unit ball. Given a collection of points
c := {c1  . . .   ck} and a point x ∈ B  let fc(x) := d(x  c(g  ))2. Then 

(cid:107)ci(cid:107)2

2

fc(x) = (cid:107)x(cid:107)2 + 4 min(Φ(x) · ˜c1  . . .   Φ(x) · ˜ck).

For any set of s samples x1  . . .   xs 
− EP fc(x)

i=1 fc(xi)

sup
fc∈G

s

(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

(cid:12)(cid:12)(cid:12)(cid:12) ≤

(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

(cid:12)(cid:12)(cid:12)(cid:12)

− EP(cid:107)x(cid:107)2

Φ(xi) · ˜ci

i=1 min
s

i

s

i=1 (cid:107)xi(cid:107)2
(cid:80)s
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) >


4

(cid:21)

+ 4 sup
fc∈G

i=1 (cid:107)xi(cid:107)2

s

− EP(cid:107)x(cid:107)2

< 2e−( 1

8 )s2

 

− EP min

i

Φ(x) · ˜ci

(7)

(8)

(cid:12)(cid:12)(cid:12)(cid:12).

(9)

P

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s
(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

By Hoeffding’s inequality 

(cid:34)

2.
which is less than δ

By Theorem 5  P

(cid:34)

sup
fc∈G

(cid:12)(cid:12)(cid:12)(cid:12)(cid:80)s

Therefore  P

sup
fc∈G

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12) > 

16

< δ
2 .

Φ(xi)·˜ci

i=1 min
s

i

− EP min

Φ(x) · ˜ci

i=1 fc(xi)

s

− EP fc(x)

≥ 1 − δ.

i

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 

2

5

Figure 2: Random projections are likely to preserve linear separations.

6 Bounding the Fat-Shattering dimension using random projections

The core of the uniform bounds in Theorems 1 and 2 is the following uniform bound on the minimum
of k linear functions on a ball in Rm.
Theorem 5. Let F be the set of all functions f from B := {x ∈ Rm : (cid:107)x(cid:107) ≤ 1} to R  such that for
some k vectors v1  . . .   vk ∈ B 

f (x) := min

(vi · x).

(cid:18) 1

i

(cid:18) k

(cid:19)

2 min

2 log4



(cid:19)

 

1
2 log

1
δ

Independent of m  if

then

(cid:18) k
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:80)s

s ≥ C

(cid:34)

P

+

  k

(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) < 
(cid:35)
(cid:19)
(cid:19)

(cid:19)

It has been open since 1997 [3]  whether the known lower bound of C(cid:0) k

s

sup
F∈F

i=1 F (xi)

− EP F (x)

> 1 − δ.

(10)

(cid:1) on the sample

2 + 1

2 log 1

δ

complexity s is tight. Theorem 5 in [14]  uses Rademacher complexities to obtain an upper bound
of

(11)
(The scenarios in [3  14] are that of k−means  but the argument in Theorem 4 reduces k−means to
our setting.) Theorem 5 improves this to

2 +

.

1
2 log

1
δ

(cid:19)

C

2 min

2 log4

  k

+

1
2 log

1
δ

by putting together (11) with a bound of

(cid:18) k

C

(cid:18) k2
(cid:18) 1
(cid:18) k

4 log4

C

(cid:19)

(cid:18) k
(cid:19)
(cid:18) k





+

1
2 log

1
δ

(12)

(13)

obtained using the Fat-Shattering dimension. Due to constraints on space  the details of the proof of
Theorem 5 will appear in the journal version  but the essential ideas are summarized here.
24 ) and x1  . . .   xu be a set of vectors that is γ−shattered by F . We would like to use
Let u := fatF ( 
VC theory to bound u  but doing so directly leads to a linear dependence on the ambient dimension
  we consider a g−dimensional random
m. In order to circumvent this difﬁculty  for g := C log(u+k)
linear subspace and the image under an appropriately scaled orthogonal projection R of the points
2−shatter coefﬁcient of {Rx1  . . .   Rxu}
x1  . . .   xu onto it. We show that the expected value of the γ
is at least 2u−1 using the Johnson-Lindenstrauss Lemma [11] and the fact that {x1  . . .   xu} is
γ−shattered. Using Vapnik-Chervonenkis theory and the Sauer-Shelah Lemma  we then show that
(cid:1)
2−shatter coefﬁcient cannot be more than uk(g+2). This implies that 2u−1 ≤ uk(g+2)  allowing us
to conclude that fatF ( 
on fatF ( 

(cid:1) . By a well-known theorem of [1]  a bound of Ck

24 ) implies the bound in (13) on the sample complexity  which implies Theorem 5.

2 log2(cid:0) k

2 log2(cid:0) k

24 ) ≤ Ck

2

γ





6

γx1x2x3x4Rx1Rx2Rx3Rx4γ2RandommapR7 Minimax lower bounds on the sample complexity

is the translation of W by x. Note that each Si has radius τ. Let (cid:96) =(cid:0)K2dk

Let K be a universal constant whose value will be ﬁxed throughout this section. In this section  we
will state lower bounds on the number of samples needed for the minimax decision rule for learning
from high dimensional data  with high probability  a manifold with a squared loss that is within 
of the optimal. We will construct a carefully chosen prior on the space of probability distributions
and use an argument that can either be viewed as an application of the probabilistic method or of the
fact that the Minimax risk is at least the risk of a Bayes optimal manifold computed with respect to
this prior. Let U be a K 2dk−dimensional vector space containing the origin  spanned by the basis
{e1  . . .   eK2dk} and S be the surface of the ball of radius 1 in Rm. We assume that m be greater or
equal to K 2dk +d. Let W be the d−dimensional vector space spanned by {eK2dk+1  . . .   eK2dk+d}.
√
Let S1  . . .   SK2dk denote spheres  such that for each i  Si := S ∩ (
1 − τ 2ei + W )  where x + W
consist of all K dk−element subsets of {S1  . . .   SK2dk}. Let ωd be the volume of the unit ball in
Rd. The following theorem shows that no algorithm can produce a nearly optimal manifold with
high probability unless it uses a number of samples that depends linearly on volume  exponentially
on intrinsic dimension and polynomially on the curvature.
Theorem 6. Let F be equal to either Ge(d  V  τ ) or Gi(d  V  1
(cid:99). Let A
be an arbitrary algorithm that takes as input a set of data points x = {x1  . . .   xk} and outputs a
manifold MA(x) in F. If  + 2δ < 1

(cid:1) and {M1  . . .  M(cid:96)}

τ 2   πτ ). Let k = (cid:98)

(cid:16) 1

V
dωd(K

(cid:17)2

5
4 τ )d

Kdk

2

3

√

− τ

(cid:20)
(cid:21)
L(MA(x) P) − infM∈F L(M P) < 

then 

2

P

infP

< 1 − δ 

where P ranges over all distributions supported on B and x1  . . .   xk are i.i.d draws from P.
Proof. Observe from Lemma ?? and Theorem 3 that F is a class of a manifolds such that
each manifold in F is contained in the union of K 3d
2 k m−dimensional balls of radius τ  and
{M1  . . .  M(cid:96)} ⊆ F.
(The reason why we have K 3d
4 as in the statement of
the theorem is that the parameters of Gi(d  V  τ ) are intrinsic  and to transfer to the extrinsic setting
of the last sentence  one needs some leeway.) Let P1  . . .  P(cid:96) be probability distributions that are
uniform on {M1  . . .  M(cid:96)} with respect to the induced Riemannian measure. Suppose A is an al-
gorithm that takes as input a set of data points x = {x1  . . .   xt} and outputs a manifold MA(x).
Let r be chosen uniformly at random from {1  . . .   (cid:96)}. Then 

rather than K 5d

2

(cid:20)(cid:12)(cid:12)L(MA(x) P) − infM∈F L(M P)(cid:12)(cid:12) < 
(cid:21)

P

infP

≤ EPr

= ExPPr
= ExPPr
We ﬁrst prove a lower bound on inf x Er [L(MA(x) Pr)|x].
We see that

(cid:2)L(MA(x) Pr)(cid:12)(cid:12)x(cid:3) = Er xk+1

Er

(cid:21)

Px

(cid:20)(cid:12)(cid:12)L(MA(x) Pr) − infM∈F L(M Pr)(cid:12)(cid:12) < 
(cid:21)
(cid:20)(cid:12)(cid:12)L(MA(x) Pr) − infM∈F L(M Pr)(cid:12)(cid:12) < (cid:12)(cid:12)x
(cid:2)L(MA(x) Pr) < (cid:12)(cid:12)x(cid:3) .
(cid:2)d(MA(x)  xk+1)2(cid:12)(cid:12)x(cid:3) .

(14)

Conditioned on x  the probability of the event (say Edif ) that xk+1 does not belong to the same
2.
sphere as one of the x1  . . .   xk is at least 1
Conditioned on Edif and x1  . . .   xk  the probability that xk+1 lies on a given sphere Sj is equal to
K2k−k(cid:48) otherwise  where k(cid:48) ≤ k is the number of spheres in
0 if one of x1  . . .   xk lies on Sj and
{Si} which contain at least one point among x1  . . .   xk.
By construction  A(x1  . . .   xk) can be covered by K 3d
y1  . . .   y

2 k balls of radius τ; let their centers be

.

1

3d
2 k

K

7

However  it is easy to check that for any dimension m  the cardinality of the set Sy of all Si that
have a nonempty intersection with the balls of radius
  is at most
K 3d

2 k. Therefore  P(cid:104)

centered around y1  . . .   y

√
1
2

(cid:105)

3d
2 k

K

(cid:20)

d(MA(x)  xk+1) ≥ 1
√
2
√
}  xk+1) ≥ 1
2

3d
2 k

K

2

2

P

d({y1  . . .   y

P [Edif ] P [xk+1 (cid:54)∈ Sy|Edif ]
K 2dk − k(cid:48) − K 3d
2 k

K 2dk − k(cid:48)

− τ(cid:12)(cid:12)x
(cid:21)
(cid:12)(cid:12)x

2
is at least
≥

1
2

≥
≥ 1
3 .
− τ

(cid:17)2

(cid:2)d(MA(x)  xk+1)2(cid:12)(cid:12)x(cid:3) ≥ 1
(cid:2)L(MA(x) Pr) < (cid:12)(cid:12)x(cid:3) to be more than 1 − δ if inf x PPr

Therefore  Er xk+1
sible for ExPPr
 + 2δ  because L(MA(x) Pr) is bounded above by 2.

√

2

2

3

(cid:16) 1

. Finally  we observe that it is not pos-

(cid:2)L(MA(x) Pr)(cid:12)(cid:12)x(cid:3) >

8 Algorithmic implications
8.1 k−means
Applying Theorem 4 to the case when P is a distribution supported equally on n speciﬁc points (that
are part of an input) in a unit ball of Rm  we see that in order to obtain an additive  approximation
for the k−means problem with probability 1 − δ  it sufﬁces to sample
1
δ

log4(cid:0) k

1
2 log

s ≥ C

(cid:33)

(cid:33)

(cid:32)

(cid:32)

k
2

(cid:1)

  k

2

+



points uniformly at random (which would have a cost of O(s log n) if the cost of one random bit
is O(1)) and exhaustively solve k−means on the resulting subset. Supposing that a dot product
between two vectors xi  xj can be computed using ˜m operations  the total cost of sampling and
then exhaustively solving k−means on the sample is O( ˜msks log n). In contrast  if one asks for a
multiplicative (1 + ) approximation  the best running time known depends linearly on n [13]. If
P is an unknown probability distribution  the above algorithm improves upon the best results in a
natural statistical framework for clustering [5].

8.2 Fitting piecewise linear curves

In this subsection  we illustrate the algorithmic utility of the uniform bound in Theorem 4 by ob-
taining an algorithm for ﬁtting a curve of length no more than L  to data drawn from an unknown
probability distribution P supported in B  whose sample complexity is independent of the ambient
dimension. This curve  with probability 1 − δ  achieves a mean squared error of less than  more
than the optimum. The proof of its correctness and analysis of its run-time have been deferred to the
journal version. The algorithm is as follows:

(cid:18)

(cid:18) log4( k

(cid:19)

(cid:19)

1. Let k := (cid:100) L

 (cid:101) and s ≥ C

 )
  k
from P for s =  and set J := span({xi}s
i=1).

2. For every permutation σ of [s]  minimize the convex objective function(cid:80)n
over the convex set of all s−tuples of points (y1  . . .   ys) in J  such that(cid:80)s−1

2 log 1

+ 1

k
2

2

δ

. Sample points x1  . . .   xs i.i.d

i=1 d(xσ(i)  yi)2
i=1 (cid:107)yi+1 −

yi(cid:107) ≤ L.

3. If the minimum over all (y1  . . .   ys) (and σ) is achieved for (z1  . . .   zs)  output the curve

obtained by joining zi to zi+1 for each i by a straight line segment.

9 Acknowledgements

We are grateful to Stephen Boyd for several helpful conversations.

8

References
[1] Noga Alon  Shai Ben-David  Nicol`o Cesa-Bianchi  and David Haussler. Scale-sensitive di-

mensions  uniform convergence  and learnability. J. ACM  44(4):615–631  1997.

[2] Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and

random projection. In FOCS  pages 616–623  1999.

[3] Peter Bartlett. The minimax distortion redundancy in empirical quantizer design. IEEE Trans-

actions on Information Theory  44:1802–1813  1997.

[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data

representation. Neural Comput.  15(6):1373–1396  2003.

[5] Shai Ben-David. A framework for statistical clustering with constant time approximation al-

gorithms for k-median and k-means clustering. Mach. Learn.  66(2-3):243–257  2007.

[6] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society  46:255–

308  January 2009.

[7] Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS  pages 634–644  1999.
[8] David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding
techniques for high-dimensional data. Proceedings of the National Academy of Sciences 
100(10):5591–5596  May 2003.

[9] A. Gray. Tubes. Addison-Wesley  1990.
[10] Trevor J. Hastie and Werner Stuetzle. Principal curves. Journal of the American Statistical

Association  84:502–516  1989.

[11] William Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert

space. Contemporary Mathematics  26:419–441  1984.

[12] Bal´azs K´egl  Adam Krzyzak  Tam´as Linder  and Kenneth Zeger. Learning and design of prin-
cipal curves. IEEE Transactions on Pattern Analysis and Machine Intelligence  22:281–297 
2000.
[13] Amit Kumar  Yogish Sabharwal  and Sandeep Sen. A simple linear time (1+)−approximation

algorithm for k-means clustering in any dimensions. In FOCS  pages 454–462  2004.

[14] Andreas Maurer and Massimiliano Pontil. Generalization bounds for k-dimensional coding

schemes in hilbert spaces. In ALT  pages 79–91  2008.

[15] H. Narayanan and P. Niyogi. On the sample complexity of learning smooth cuts on a manifold.

In Proc. of the 22nd Annual Conference on Learning Theory (COLT)  June 2009.

[16] Partha Niyogi  Stephen Smale  and Shmuel Weinberger. Finding the homology of submani-
folds with high conﬁdence from random samples. Discrete & Computational Geometry  39(1-
3):419–441  2008.

[17] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear

embedding. SCIENCE  290:2323–2326  2000.

[18] Alexander J. Smola  Sebastian Mika  Bernhard Sch¨olkopf  and Robert C. Williamson. Regu-

larized principal manifolds. J. Mach. Learn. Res.  1:179–209  2001.

[19] J. B. Tenenbaum  V. Silva  and J. C. Langford. A Global Geometric Framework for Nonlinear

Dimensionality Reduction. Science  290(5500):2319–2323  2000.

[20] Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Compu-

tational Geometry  33(2):249–274  2005.

9

,Daniel Levine
Jonathan How