2019,A Condition Number for Joint Optimization of Cycle-Consistent Networks,A recent trend in optimizing maps such as dense correspondences between objects or neural networks between pairs of domains is to optimize them jointly. In this context  there is a natural \textsl{cycle-consistency} constraint  which regularizes composite maps associated with cycles  i.e.  they are forced to be identity maps. However  as there is an exponential number of cycles in a graph  how to sample a subset of cycles becomes critical for efficient and effective enforcement of the cycle-consistency constraint. This paper presents an algorithm that select a subset of weighted cycles to minimize a condition number of the induced joint optimization problem. Experimental results on benchmark datasets justify the effectiveness of our approach for optimizing dense correspondences between 3D shapes and neural networks for predicting dense image flows.,A Condition Number for Joint Optimization of

Cycle-Consistent Networks

Leonidas Guibas1  Qixing Huang2  and Zhenxiao Liang2

1Stanford University

2The University of Texas at Austin

Abstract

A recent trend in optimizing maps such as dense correspondences between objects
or neural networks between pairs of domains is to optimize them jointly. In this
context  there is a natural cycle-consistency constraint  which regularizes composite
maps associated with cycles  i.e.  they are forced to be identity maps. However 
as there is an exponential number of cycles in a graph  how to sample a subset
of cycles becomes critical for efﬁcient and effective enforcement of the cycle-
consistency constraint. This paper presents an algorithm that select a subset of
weighted cycles to minimize a condition number of the induced joint optimization
problem. Experimental results on benchmark datasets justify the effectiveness of
our approach for optimizing dense correspondences between 3D shapes and neural
networks for predicting dense image ﬂows.

1

Introduction

Maps between sets are important mathematical quantities. Depending on the deﬁnition of sets  maps
can take different forms. Examples include dense correspondences between image pixels [28  23] 
sparse correspondences between feature points [27]  vertex correspondences between social or
biological networks [13]  and rigid transformations between archaeological pieces [16]. In the
deep learning era  the concept of maps naturally extends to neural networks between different
domains. A fundamental challenge in map computation is that there is only limited information
between pairs of objects/domains for map computation  and the resulting maps are often noisy and
incorrect  particularly between relevant but dissimilar objects. A recent trend in map computation
seeks to address this issue by performing map synchronization  which jointly optimizes maps among
a collection of related objects/domains to improve the maps between pairs of objects/domains in
isolation [29  24  15  14  39  9  17  11  19  45  34]. In this context  there is a natural constraint called
cycle-consistency  which states that composite maps along cycles should be the identity map. When
maps admit matrix representations  state-of-the-art techniques [14  41  9  41  36  36  5  4] formulate
map synchronization as recovering a low-rank matrix  where the pairwise maps computed between
pairs of objects are noisy measurements of the blocks of this matrix. This paradigm enjoys tight exact
recovery conditions as well as empirical success (e.g. [14  41  9  41  36  36  5  4]).
In this paper  we focus on the case where maps between objects/domains do not admit matrix
representations (c.f. [45])  which is a popular setting for neural networks between domains. To jointly
optimize neural networks in this context  one has to enforce the original cycle-consistency constraint.
The technical challenge  though  is that the number of cycles in a graph may be exponential in the
number of vertices. In other words  we have to develop a strategy to effectively sample a subset
of cycles to enforce the cycle-consistency constraint. The goal for cycle selection is three-fold:
completeness  conciseness  and stability. Completeness stands for the fact that enforcing the cycle-
consistency constraint on the selected cycles induces the cycle-consistency property among all cycles
in the graph. Conciseness means both the size of the cycle-set and the length of each cycle should
be small. Stability concerns the convergence behavior when solving the induce joint optimization
problem  e.g.  joint learning of a network of neural networks. In particular  stability is crucial for

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

cycle-set selection as many cycle sets satisfy the ﬁrst two objectives  yet the numerical behavior of the
induced optimization problem turns out drastically different. To date  cycle selection is mostly done
manually or uses approaches that only consider the ﬁrst two objectives. In contrast  we introduce an
automatic approach that takes all three objectives into account for cycle selection.
Our cycle selection approach ﬁrst establishes a stability score based on a condition number of the
Hessian matrix of the induced optimization problem. This condition number dictates the local
convergence rate as well as the convergence radius of the induced optimization problem. Our
approach then combines semideﬁnite programming and importance sampling to select a small subset
of cycles from an over-complete set of cycles to minimize the condition number.
We have evaluated our approach on a variety of settings of optimizing cycle-consistent maps  including
dense correspondences across hundreds of shapes and neural networks for predicting dense ﬂows
across natural images. Example results show that cycle selection not only improves the convergence
rate of the induced optimization problem but also leads to maps with improved map quality.

2 Related Works

Optimizing maps among a collection of objects/domains is a fundamental problem across many
different scientiﬁc domains. In the following  we review works that focus on various optimization
techniques  which are most relevant to the context of this paper.
Cycle-consistency constraint. The foundation for joint map computation is the so-called cycle-
consistency constraint [20  14]  which states that the composition of correct maps along cycles should
be the identity map. There are two widely used formulations of the cycle-consistency constraint.
Low-rank based techniques utilize matrix representations of maps and the equivalence between cycle-
consistent maps and the fact that the matrix that stores pair-wise maps in blocks is low-rank and/or
semideﬁnite (c.f.[14]). This equivalence leads to a simple formulation of joint map computation via
low-rank matrix recovery [41  24  15  14  17  10  48  36  26  18  3  33  32  1  6  2]. Such techniques
enjoy both empirical success and exact recovery conditions. However  a fundamental limitation of
such techniques is that there must exist matrix map representations  and such assumptions are not
always true  e.g.  when neural networks encode maps.
Another category of methods utilizes spanning trees [20  16]. In a modern context of joint map
computation  i.e.  recovering accurate maps from maps computed between pairs of objects in isolation 
one can seek to recover the correct maps by computing the minimum spanning tree where the induced
maps agree with the input maps as much as possible. Most recent combinatorial optimization tech-
niques are based on sampling inconsistent cycles [44  29  47]. They formulate map synchronization
as removing maps so that each inconsistent cycle contains at least one removed map. However 
both techniques are most suited for the task of pruning incorrect maps. They are not suitable for
optimizing maps continuously. In contrast  the approach described in this paper combines the strength
of both formulations. Precisely  we still formulate map synchronization by minimizing an objective
function that combines an observation term and a prior term. The observation term evaluates the
quality of each map based on the training data. The difference is in the regularization term  where
we directly enforce the consistency of maps along cycles. This approach is suitable for diverse map
representations. However  a fundamental challenge is to obtain a concise and effective cycle-basis 
which is the main focus of this paper.
Cycle-basis. Cycle-basis is a well studied topic on undirected graphs (c.f.[22]). In the standard
setting  a cycle-basis consists of a minimum set of cycles of a graph  where all other cycles in this
graph are linear combinations of the cycles in this graph. In this paper  we extend this notion to cycle-
consistency bases. The goal is to compute a minimum set of cycles  where enforcing consistency
along these cycles induces consistency along all cycles of the input graph. Although cycle-consistency
bases are equivalent  enforcing them for map computation exhibits different behavior. The primary
goal of this paper is to properly deﬁne the condition number of cycle-consistency basis and develop
efﬁcient ways to optimize in the space of cycle-consistency bases to minimize the condition number.

3 Cycle-Consistency Bases

In this section  we deﬁne map graphs and cycle-consistency bases. In Section 4  we discuss how to
optimize cycle-consistency bases for joint map optimization. Note that due to the space constraint 
we defer the proofs to the supplemental material.

2

We ﬁrst deﬁne the notion of a map graph along an undirected graph G = (V E). Since parametric
maps (e.g.  neural networks) are oriented  we let the edges in E be oriented. We say G is undirected if
and only if ∀(i  j) ∈ E  (j  i) ∈ E.
Deﬁnition 1 We deﬁne a map graph F as an attributed undirected graph G = (V E) where V =
{v1  . . .   v|V|}. Each vertex vi ∈ V is associated with a domain Di. Each edge e = (i  j) ∈ E is
associated with a map fij : Di → Dj. In this paper  we assume G is connected. Note that when
deﬁning cycle-consistency bases later  we always assume fij is an isomorphism between Di and Dj 
and fji = f−1
ij .
To deﬁne cycle-consistency bases on G  we introduce composite maps along cycles of G.
Deﬁnition 2 Consider a cycle c = (i1 ···   iki1) along G. We deﬁne the composite map along c
induced from a map graph F as
(1)

fc = fiki1 ◦ ··· ◦ fi1i2 .

In this paper  we are talking about a cycle. We always assume it has no repeating edges.
Deﬁnition 3 Given a map graph F associated with a graph G  let C be a cycle set of G. We say F is
cycle-consistent on C  if
(2)
Here IdX refers to the identity mapping from X to itself. Let C collects all cycles of G. We say F is
cycle-consistent  if it is cycle-consistent on C.
Remark 1 Note that due to the bi-directional consistency in Def. 1  (2) is independent of the starting
vertex i1. In fact  it induces

∀c = (i1 ··· iki1) ∈ C.

fc = IdDi1

 

fil···iki1···il = IdDil

 

1 ≤ l ≤ k.

Since C contains an exponential number of cycles  a natural question is whether we can choose a
small subset of cycles C so that for every map graph F that is cycle-consistent on C  it induces the
cycle-consistency of F. To this end  we need to deﬁne the notion of induction:
Deﬁnition 4 Consider a cycle set C and a cycle c /∈ C. We say C induces c if there exists an ordered
cycle set c1 ···   cK ∈ C and intermediate simple cycles c(k)  1 ≤ k ≤ K so that (1) c(1) = c1  (2)
c(K) = c  and (3) c(k) = c(k−1) ⊕ ck  i.e.  c(k) is generated by adding new edges in ck to c(k−1)
while removing their common edges.

An immediate consequence of Def. 4 is the following:
Fact 1 Given a map graph F  a cycle set C and another cycle c. If (1) F is cycle-consistent on C 
and (2) C induces c. Then F is cycle-consistent on {c}.
Remark 2 It is necessary for Def. 4 to require that c(k)  1 ≤ k ≤ K are simple cycles. We provide a
counter example in the supplemental material.

The following proposition shows that Def. 4 is complete.
Proposition 1 Suppose a map graph F is cycle-consistent on a cycle set C. If a cycle c can not be
induced from C using the procedure described in Def. 4  then F may not be cycle-consistent on {c}.
Now we deﬁne the notion of cycle-consistency basis:
Deﬁnition 5 We say a cycle set C is a cycle-consistency basis if it induces all other cycles of C.
The following proposition characterizes the minimum size of cycle-consistency bases and a procedure
for constructing a category of cycle-consistency bases with minimum size.
Proposition 2 The minimum size of a cycle-consistency basis on a connected graph G is |E|−|V| + 1.
Moreover  we can construct a minimal cycle-consistency basis from a spanning tree T ⊂ E of G  i.e. 
by creating a cycle ce for each edge pair e = (i  j) ∈ E \ T   where ce = (i  j) ∼ pji  where pji is
the unique path from j to i on T .

3

Remark 3 A difference between cycle-consistency bases on undirected graphs and path-invariance
bases on directed graphs is that the minimum size of cycle-consistency bases is known and is upper
bounded by the number of edges. In contrast  computing the minimum size of path-invariance bases
of a given directed graph remains an open problem (c.f.[45]).

Connections to cycle bases [22]. When talking about cycles of an undirected graph  there exists a
related notion of cycle bases [22]. The difference between cycle-consistency bases and cycle bases
lies in the induction procedure. Speciﬁcally  cycle bases utilize a vector that collects edge indicators 
i.e.  each edge has an orientation  and each cycle corresponds to a sparse vector whose elements are
in {1  0}  which represent edges in c and the other edges. The induction procedure takes the form of
linear combinations of indicator vectors. Depending on the weights for linear combination  cycle
bases fall into zero-one cycle bases  integral cycle bases and general cycle-bases  which correspond
to {−1  0  1}  integer and real weights  respectively. Please refer to [22] for more details.
It is easy to see that one can use linear combinations of vectors with binary weights to encode the
induction procedure of cycle-consistency bases. On the other hand  the reverse is not true.
Proposition 3 A cycle-consistency basis is a zero-one cycle basis. A zero-one cycle basis may not be
a cycle-consistency basis.

Remark 4 Unlike the situation that we can verify a zero-one cycle basis by checking independence
within polynomial time (c.f. [22])  we conjecture that verifying whether a cycle set forms a cycle-
consistency basis is NP-hard. In light of this  when optimizing a cycle set we enforce its cycle-
consistency property by adding cycles to a minimum cycle-consistency basis.

4 Cycle Consistency Basis Optimization for Joint Map Optimization

In this section  we present an approach that optimizes a cycle-consistency basis for a given joint map
optimization task. Speciﬁcally  we assume each edge (i  j) ∈ E is associated with a parametric map
: Di → Dj  where θij denotes the parameters of this map. We also assume we have a subset
f θij
ij
of edges E0 ⊆ E. For each edge e ∈ E0  we have a loss term denoted as lij(θij) (e.g.  we may only
have training data among a subset of edges). We assume G0 = (V E0) forms a connected graph.
Otherwise  we have insufﬁcient constraints to determine all parametric maps.
Our main idea is to pre-compute a super set of cycles Csup (See Section 4.2 for details) and formulate
cycle-consistency basis optimization as determining a cycle set C  where Cmin ⊆ C ⊆ Csup  and a
weight wc > 0 of each cycle c ∈ C  for solving the following joint map optimization problem:

(cid:88)

(cid:88)

lij(θij) +

(i j)∈E0

c=(i1···iki1)∈C

wcli1(f Θ

c   IdDi1

)

(3)

θik i1
iki1

c = f

◦ ··· ◦ f θ

i1i2 is the composite map along c and li(· ·) denotes a loss-term for
Here f Θ
comparing self-maps on Di. For example  li(f  f(cid:48)) := Ex∼pd2Di
(f (x)  f(cid:48)(x))  where dDi(· ·) is
a distance metric of Di  and where p is an empirical distribution on Di. Note that (3) essentially
enforces the cycle-consistency constraint along C.
In the reminder of this section  Section 4.1 introduces a condition number of (3); Section 4.2 describes
how to minimize this condition number by selecting and weighting cycles.

4.1 A Condition Number for Cycle-Consistency Map Optimization

We begin with a simple setting of translation synchronization [18]  where pairwise parametric maps
are given by translations. We then discuss how to generalize this deﬁnition to neural networks. Note
that for the particular task of translation synchronization  there exist many other formulations  e.g. 
[21  18]. Our goal here is simply to motivate the deﬁnition of the condition number. Speciﬁcally 
ij ∈ R for each edge e = (i  j) ∈ E0. Our goal is to recover
consider a pre-computed translation t0
translations tij  (i  j) ∈ E by solving the following quadratic minimization problem:
tilii+1)2.

(tij − t0

(cid:88)

(cid:88)

(cid:88)

ij)2 +

wc(

for edge e. With vc =(cid:80)k

where we set lk+1 := l1. Given an ordering of the edge set E  let ve ∈ {0  1}|E| be the indicator vector
l=1 v(il il+1) we denote the indicator vector for cycle c = (i1 ··· iki1). Let

c=(i1···iki1)∈C

(i j)∈E0

min

{tij  (i j)∈E}

(4)

4

t0 =(cid:80)

ij and t =(cid:80)

(i j)∈E 0 v(i j)t0

(i j)∈E v(i j)tij. We can rewrite (4) in the matrix form as

tT Ht − 2tT t0 + (cid:107)t0(cid:107)2 

H :=

min

t

vevT

e +

wcvcvT
c

(5)

(cid:88)

e∈E 0

(cid:88)

c∈C

When solving (5) using gradient-based techniques (which is the case for neural networks)  their
convergence rates are generally relevant to the condition number κ(H) := λmax(H)/λmin(H). For
example  steepest descent with exact line search admits a linear convergence rate of (κ(H) − 1)/
(κ(H) + 1)  and this argument also applies to the local convergence behavior of non-linear objective
functions (c.f. Sec.1.3 of [7]). In addition  the deviation between the optimal solution t(cid:63) and the
ground truth solution tgt is (cid:107)t(cid:63) − tgt(cid:107) = (cid:107)H−1e(cid:107) ≤
λmin(H)(cid:107)e(cid:107)  where e = t0 − tgt
0 is the vector
that encodes the error in the input. Minimizing the κ(H) usually leads to an increased value of λmin 
which reduces the error in the output. Thus  we proceed with the following deﬁnition:
Deﬁnition 6 We deﬁne the condition number of (3) as the condition number κ(H).

1

Def. 6 also generalizes to other parametric maps:
Theorem 4.1 (Informal) Under mild conditions  the condition number of the Hessian matrix at a
local optimal of (3) is O(sκ(H))  where s depends on quantities of individual f θij
ij .
Another factor related to an efﬁcient optimization of (3) is the size of C. As there is some ﬁxed
overhead for implementing the constraint associated with each cycle  we favor that the size of C
is small. In summary  our goal for cycle-consistency basis optimization is to reduce the condition
number of H while simultaneously to minimize the size of the resulting cycle-consistency basis.

4.2 Algorithm for Cycle-Consistency Basis Construction

Our approach for cycle-consistency basis optimization proceeds in three steps. The ﬁrst step generates
the super cycle set Csup. The second step solves a semi-deﬁnite program to optimize weights
wc  c ∈ Csup to minimize the condition number of H. The ﬁnal setup controls the size of the resulting
cycle-consistency basis via importance sampling.
Csup generation. We construct Csup by computing the breadth-ﬁrst spanning tree T (vi) rooted at
each vertex vi ∈ V. For each spanning-tree T (vi)  we use the procedure described in Prop. 2 to
construct a minimum cycle-consistency basis C(vi). We deﬁne Csup := ∪vi∈VC(vi). We set Cmin as
the cycle-consistency basis with minimum depth.
The resulting Csup has two desired properties. First  the cycles in Csup are kept as short as possible.
For example  if G is a clique  then Csup only contains the desired 3-cycles. Second  if G is sparse 
then Csup contains a mixture of short and long cycles. These long cycles can address the issue of
accumulated errors if we only enforce the cycle-consistency constraint along short cycles.
Weight optimization. As the condition number of H is minimized if it is a scalar multiple of the
identity matrix  we formulate the following semideﬁnite program for optimizing cycle weights:

min

wc≥0 s1 s2

αs2 − s1

subject to (C1) : s1I (cid:22) (cid:88)

vevT

e +

e∈E 0
|vc|2wc = λ 

(cid:88)

c∈Csup

(C2) :

(cid:88)

c∈Csup

wcvcvT

c (cid:22) s2I

(C3) : wc ≥ δ ∀c ∈ Cmin

(6)

Here λ characterizes the trade-off between the loss terms and the regularization terms and α is a super
parameter. To ﬁgure out the motivation of introducing such a parameter  recall that the deﬁnition of
condition number is λmax
where λmax  λmin are maximal and minimal eigenvalues of the weighted
λmin
matrix sum respectively. But optimizing the ratio of two eigenvalues is non-convex in terms of
variables {wc}  thus we in turn try to reduce the gap between λmax and λmin. Different α’s would
provide different outcomes of eigen-ratio. Hence we test over a set of α’s and look for the one with
the objective function α∗s2 − s1
optimal eigen-ratio. In fact it is easy to see that as α∗ = max λmin
would have minimum 0 at the point which minimizes λmax
as well. In most cases where the optimal
λmin
ratio is in the order of magnitude near 1  so that the setting α = 1 would provide a good enough
approximation.

λmax

5

Moreover  (C3) ensures that edges in the minimum cycle consistency basis Cmin are also selected. In
our experiments  we set λ = 8|E0|/|E| and δ = λ/(8|E|). We solve (6) using alternating direction
method of multipliers (See the supplemental material for details)
As the cycle indicators  which are sparse vectors  tend to be orthogonal to each other. The solution to
(6) typically turns a matrix with small condition number.

1 and s(cid:63)

2 − s(cid:63)

2 be the optimal solution to (6). Then maxc wc ≤ s(cid:63)

2  and both
1 are upper bounded by quantities that are related to an uniform score of the spherical

Theorem 4.2 (Informal) Let s(cid:63)
2 and s(cid:63)
s(cid:63)
Voronoi diagram of copies of {ve  e ∈ E0} ∪ { vc(cid:107)vc(cid:107)   c ∈ Csup}.
Importance sampling. Although the semideﬁnite program described in (6) controls the condition
number of H  it does not control the size of the cycle sets with positive weights. Thus we seek to
select a subset of cycles Csample ⊂ Cactive := Csup \ Cmin and compute new weights wc  c ∈ Csample  so
that
(7)

c ≈ (cid:88)

We achieve this goal through sampling. Speciﬁcally  consider a desired size L for Csample. Let
(wc/wmax)α. We deﬁne an
wmax = max
c∈Cactive
independent random variable xc and a modiﬁed weight wc for each cycle c ∈ Cactive:

wc. Choose the maximum α ≤ 1 so that L ≤ (cid:80)
(cid:26) 1

(cid:88)

(cid:88)

wcvcvT
c .

wcvcvT

c∈Csample

c∈Cactive

c∈Cactive

wc = wc/pc 

pc := L · wα
c /

xc =

with probability pc
0 with probability 1 − pc

c∈Cactive

To generate Csample  we simply sample Cactive according xc. It is easy to check that

E[|Csample|] = L 

wcvcvT

c ] =

wcvcvT
c .

(cid:88)

E[

c∈Csample

(cid:88)

c∈Cactive

wα
c .

(8)

(9)
(10)

In the following  we provide concentration inequalities on both quantities:

Theorem 4.3 Given the sampling procedure described in (8) with standard deviation

σ1 :=

and condition

we have with probability at least 1 − O(1/poly(n)) 

(cid:115)(cid:88)

pc(1 − pc) 

c

σ2
1 = Ω(1) 

(cid:115)(cid:88)
wc)2(cid:1) 
2 = Ω(cid:0)(max

σ2 :=

c

c

σ2

pc(1 − pc)w2

c

(cid:107) (cid:88)

c∈Csample

c − (cid:88)

c∈Cactive

wcvcvT

||Csample| − L| ≤ O(log n)σ1
c (cid:107) ≤ O(log n)σ2

wcvcvT

Note that (10)  which utilizes rank(vcvT
c ) = 1  is sharper than general concentration bounds [38].
To be more precise  the known bound contains an extra multiplicative term O(log d) where d is the
dimension of matrices involved. In our case d = O(|E|).

5 Experimental Evaluation

In this section  we present an experimental evaluation of our joint map optimization approach in two
application settings: consistent shape maps (c.f. [24  14  17  11]) and dense image correspondence
using neural networks (c.f. [46  45]).

5.1 Consistent Shape Correspondences
Experimental setup. Similar to [39  40  17]  we encode the map from one shape Si and another
shape Sj as a function map Xij : F(Si) → F(Sj) [31]. The same as [17]  we choose each functional
space F(Si) as the linear space spanned by smallest m = 30 eigenvectors of the co-tangent mesh

6

(cid:88)

(i j)∈E

(cid:88)

c=(i1···i|c|i1)∈C

(a)

(b)

(c)

Figure 1: The top and bottom rows show quantitative evaluations on the ShapeCoSeg [42] and the PAS-
CAL3D [43]  respectively. (a) Cumulative distributions of geodesic errors (ShapeCoSeg) and Euclidean errors
(PASCAL3D) of predicted feature correspondences of our approach and baseline approaches. (b) Distributions
of cycle weights. (c) Distributions of cycle weights per cycle-length
Laplacian. A functional map Xij ∈ Rm×m essentially encodes a linear map from F(Si) to F(Sj).
We refer to [31] for more details about functional maps.
The evaluation considers two shape collections from ShapeCoSeg [42]: Alien (200 shapes) and Vase
(300 shapes). For each shape collection  we construct G by connecting every shape with k = 25
randomly chosen shapes. The edge set E0 collects for each shape the k0 = 9 closest shapes among
the neighbors speciﬁed by E via the GPS descriptor [35]. For each edge e = (i  j) ∈ E0  we compute
dense correspondences from Si to Sj using Blended Intrinsic Maps [25]. We then convert this map
into the corresponding functional map X 0
For joint map computation  we obtain improved functional maps Xij  (i  j) ∈ E by minimizing

ij using [31].

1
|E|

(cid:107)Xij − X in

ij (cid:107)2F + λ

wc(cid:107)Xi|c|i1 ··· Xi1i2 − Im(cid:107)2F

(11)

For numerical optimization  we start from the identity map Xij = Im  (i  j) ∈ E and apply steepest
descent with exact line search [30]. We run 3000 iteration on each dataset. After optimization  we
generate maps between all pairs of shapes by composing maps along shortest paths on G.
Analysis of results. To evaluate the quality of shape maps  we report the cumulative distribution (or
CD) of normalized geodesic error egeo of predicted feature correspondences (c.f [25]. We compare our
approach with three state-of-the-art joint shape matching approaches: Huang14 [17]  Cosmo17 [11]
and Zhang19 [45]. As shown in Figure 1(a)  our approach leads to noticeable performance gains
from Huang14 and Cosmo17 that leverage low-rank relaxations of the cycle-consistency constraint
(c.f. [14]). An explanation is that when the observations are sparse  these low-rank relaxations become
loose. In contrast  enforcing the cycle-consistency constraint exactly offers strong regularization.
Our approach also outperforms [45] (i.e.  by 4.9% when egeo = 0.1)  which employs a relevant
path-invariance constraint by treating G as a directed graph. As we will discuss immediately  the
improvement comes from weighted cycles.
Analysis of cycle weighting. As show in Figure 1(a)  weighting the cycles has a signiﬁcant impact on
the quality of the optimized maps. When solving (11) with equal weight λ/|Csup|  the CD percentage
drops by 5.2% (when egeo = 0.15). Note that using more iterations does not close the gap  as there is
still a 2.4% difference even after 30000 iterations. This gap also justiﬁes the argument that reducing
the condition number alleviates the ampliﬁed errors in the solution of (11) that are caused by X 0
ij.
Figure 1(b) plots the distribution of cycle weights returned by the SDP formulation. We can see that
the SDP formulation leads to sparse and relatively uniform cycle weights  indicating the effectiveness

7

0.030.060.090.120.150.18Geodesic Error20406080100%CorrespondencesShapeCoSeg-Baseline-EvalOursNoWeightZhang19Cosmo17Huang1420406080100Percentile %0.20.40.60.81.0Weight ValueWeight Dist. (ShapeCoSeg)AlienVase0.20.40.60.81.0Percentile2345678Cycle LengthWeight vs Cycle-length (ShapeCoSeg)AlienVase0.030.060.090.120.150.18Euclidean Error15304560%CorrespondencesPASCAL3D-Baseline-EvalOursNoWeightIdenticalNetZhang19Zhou16Dosovitsky15Zhou1520406080100Percentile %0.20.40.60.81.0Weight ValueWeight Dist. (PASCAL3D)Grid0.20.40.60.81.0Percentile2345678910Cycle LengthWeight vs Cycle-length (PASCAL3D)Gridof our approach for selecting important cycles. Moreover  most cycles with positive weights are short
(See Figure 1(c)). This behavior coincides with the intuition that G is a dense graph  and utilizing
short cycles for optimizing (11) is sufﬁcient.

5.2 Consistent Neural Networks among Multiple Domains

Figure 2: Map graph for image ﬂow.

Experimental setup. In this setting  we consider the task of predicting dense ﬂows between image
objects using a neural network (c.f. [12  46]. We perform experimental evaluation on 12 rigid
categories from PASCAL3D [43]. For each category  we construct a map graph G = (V E)  where
each vertex v ∈ V represents image objects viewed from similar camera poses  and where each edge
represents a dense ﬂow neural network between some adjacent vertex pairs (to be discussed shortly).
In our experiments  we generate V by ﬁrst picking the dom-
inant view of each category [43] and then sampling a grid
of 5 × 5 camera poses. This grid is centered at the domi-
nant view  its two axes align with the latitude and longitude 
and its spacing is 22.5◦. Similar to ([46])  we consider both
real images from PASCAL3D [43] and synthetic images from
ShapeNet [8] for training. For each training image  we allo-
cate it to four closest vertices in terms of camera poses. We
connect an edge between two vertices if the angular distance
between their camera poses is less than 35◦. All edges use the
same network architecture [46]. However  we allow them to
take different weights to learn speciﬁc features associated with
each camera pose pair. Moreover  we set E = E0.
We apply (3) to jointly learn the neural networks associated
with each edge. Inspired by [12]  we use synthetic images
to deﬁne the loss term associated with each edge. In contrast 
the cycle-consistency constraint is enforced on real images. To initialize the neural networks  we
ﬁrst pre-train a single network using synthetic images. We then pass the pre-trained weights for all
networks. The same as joint shape matching  we generate dense image ﬂows between all pairs of
images by composing neural networks along shortest paths on G.
During testing time  we use [37] to predict a camera pose for each image and associate it with
the closest vertex of G. Given two images  we extract the corresponding network to predict dense
correspondences.
Analysis of results. For experimental evaluation  we report cumulative distributions of normalized
Euclidean error eeuc (with respect to the max(width  height)) of predicted feature correspondences
(c.f [47]. We compare our approach with four state-of-the-art data-driven dense image ﬂow ap-
proaches: Zhou15 [47]  Dosovitskiy15 [12] Zhou16 [46]  Zhang19 [45]. As shown in Figure 1(a) 
our approach leads to noticeable performance gains from Zhou15  which only utilizes real images.
Likewise  our approach also signiﬁcantly outperforms Dosovitskiy15 trained on synthetic data alone
(i.e.  by 8.1% when eeuc = 0.1. This encouraging result shows the potential of leveraging the
self-supervision constraint on real images. In addition  our approach is also superior to [46] and [45].
Such improvements are attributed to allowing network parameters to vary across different edges —
enforcing identical network parameters results in a 3.2% drop in the CD percentage.
Analysis of cycle weighting. Similar to the case of joint shape matching  using uniform weights to
solve (3) leads to a 4.3% drop in the CD percentage  which again shows the advantages of weighing
the cycles for both the convergence behavior and the robustness of the solution. Moreover  the
distribution of cycle weights is similar to that of joint shape matching (See Figure 1(b))  where the
solution of SDP returns sparse and uniform cycle weights.
A unique characteristic of this application is that with a relatively sparse graph  the selected cycles
contain a few long cycles (See Figure 1(c)). One explanation is that if all selected cycles are short  then
the composite networks along long cycles may suffer from accumulated errors. As a consequence 
the composite networks between non-adjacent vertices may drift.
Acknowledgement. Qixing Huang would like to acknowledge support from NSF DMS-1700234  a
Gift from Snap Research  and a hardware Donation from NVIDIA. Leonidas Guibas would like to
acknowledge NSF grant DMS-1546206  a grant from the Stanford-Toyota AI center  and a Vannevar
Bush Faculty Fellowship.

8

References
[1] Federica Arrigoni  Andrea Fusiello  and Beatrice Rossi. Camera motion from group synchro-
nization. In 3D Vision (3DV)  2016 Fourth International Conference on  pages 546–555. IEEE 
2016.

[2] Federica Arrigoni  Luca Magri  Beatrice Rossi  Pasqualina Fragneto  and Andrea Fusiello.
Robust absolute rotation estimation via low-rank and sparse matrix decomposition. In 3D Vision
(3DV)  2014 2nd International Conference on  volume 1  pages 491–498. IEEE  2014.

[3] Federica Arrigoni  Beatrice Rossi  and Andrea Fusiello. Spectral synchronization of multiple

views in se (3). SIAM Journal on Imaging Sciences  9(4):1963–1990  2016.

[4] Chandrajit Bajaj  Tingran Gao  Zihang He  Qixing Huang  and Zhenxiao Liang. SMAC: Simul-
taneous mapping and clustering using spectral decompositions. In Jennifer Dy and Andreas
Krause  editors  Proceedings of the 35th International Conference on Machine Learning  vol-
ume 80 of Proceedings of Machine Learning Research  pages 324–333  Stockholmsmässan 
Stockholm Sweden  10–15 Jul 2018. PMLR.

[5] Afonso S. Bandeira  Nicolas Boumal  and Amit Singer. Tightness of the maximum likelihood
semideﬁnite relaxation for angular synchronization. Math. Program.  163(1-2):145–167  May
2017.

[6] Florian Bernard  Johan Thunberg  Peter Gemmar  Frank Hertel  Andreas Husch  and Jorge
Goncalves. A solution for multi-alignment by transformation synchronisation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition  pages 2161–2169  2015.

[7] D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  1999.

[8] Angel X. Chang  Thomas A. Funkhouser  Leonidas J. Guibas  Pat Hanrahan  Qi-Xing Huang 
Zimo Li  Silvio Savarese  Manolis Savva  Shuran Song  Hao Su  Jianxiong Xiao  Li Yi  and
Fisher Yu. Shapenet: An information-rich 3d model repository. CoRR  abs/1512.03012  2015.

[9] Yuxin Chen  Leonidas J. Guibas  and Qi-Xing Huang. Near-optimal joint object matching via
convex relaxation. In Proceedings of the 31th International Conference on Machine Learning 
ICML 2014  Beijing  China  21-26 June 2014  pages 100–108  Beijing  China  2014. JMLR 
Inc.

[10] Yuxin Chen  Leonidas J. Guibas  and Qi-Xing Huang. Near-optimal joint object matching via

convex relaxation. In ICML  pages 100–108  2014.

[11] Luca Cosmo  Emanuele Rodolà  Andrea Albarelli  Facundo Mémoli  and Daniel Cremers.
Consistent partial matching of shape collections via sparse modeling. Comput. Graph. Forum 
36(1):209–221  2017.

[12] Alexey Dosovitskiy  Philipp Fischer  Eddy Ilg  Philip Häusser  Caner Hazirbas  Vladimir Golkov 
Patrick van der Smagt  Daniel Cremers  and Thomas Brox. Flownet: Learning optical ﬂow with
convolutional networks. In 2015 IEEE International Conference on Computer Vision  ICCV
2015  Santiago  Chile  December 7-13  2015  pages 2758–2766  2015.

[13] Somaye Hashemifar  Qixing Huang  and Jinbo Xu. Joint alignment of multiple protein-protein
interaction networks via convex optimization. Journal of Computational Biology  23(11):903–
911  2016.

[14] Qi-Xing Huang and Leonidas Guibas. Consistent shape maps via semideﬁnite programming.
In Proceedings of the Eleventh Eurographics/ACMSIGGRAPH Symposium on Geometry Pro-
cessing  SGP ’13  pages 177–186  Aire-la-Ville  Switzerland  Switzerland  2013. Eurographics
Association.

[15] Qi-Xing Huang  Guo-Xin Zhang  Lin Gao  Shi-Min Hu  Adrian Butscher  and Leonidas Guibas.
An optimization approach for extracting and encoding consistent maps in a shape collection.
ACM Trans. Graph.  31(6):167:1–167:11  November 2012.

[16] Qixing Huang  Simon Flöry  Natasha Gelfand  Michael Hofer  and Helmut Pottmann. Re-
assembling fractured objects by geometric matching. ACM Trans. Graph.  25(3):569–578  July
2006.

9

[17] Qixing Huang  Fan Wang  and Leonidas Guibas. Functional map networks for analyzing and
exploring large shape collections. ACM Transactions on Graphics  33(4):36:1–36:11  July 2014.

[18] Xiangru Huang  Zhenxiao Liang  Chandrajit Bajaj  and Qixing Huang. Translation synchroniza-

tion via truncated least squares. In NIPS  2017.

[19] Xiangru Huang  Zhenxiao Liang  Xiaowei Zhou  Yao Xie  Leonidas J. Guibas  and Qixing

Huang. Learning transformation synchronization. CoRR  abs/1901.09458  2019.

[20] Daniel Huber. Automatic Three-dimensional Modeling from Reality. PhD thesis  Carnegie

Mellon University  Pittsburgh  PA  December 2002.

[21] Xiaoye Jiang  Lek-Heng Lim  Yuan Yao  and Yinyu Ye. Statistical ranking and combinatorial

hodge theory. Math. Program.  127(1):203–244  March 2011.

[22] Telikepalli Kavitha  Christian Liebchen  Kurt Mehlhorn  Dimitrios Michail  Romeo Rizzi 
Torsten Ueckerdt  and Katharina A. Zweig. Survey: Cycle bases in graphs characterization 
algorithms  complexity  and applications. Comput. Sci. Rev.  3(4):199–243  November 2009.

[23] Jaechul Kim  Ce Liu  Fei Sha  and Kristen Grauman. Deformable spatial pyramid matching for

fast dense correspondences. In CVPR  pages 2307–2314. IEEE Computer Society  2013.

[24] Vladimir Kim  Wilmot Li  Niloy Mitra  Stephen DiVerdi  and Thomas Funkhouser. Exploring
collections of 3d models using fuzzy correspondences. ACM Trans. Graph.  31(4):54:1–54:11 
July 2012.

[25] Vladimir G. Kim  Yaron Lipman  and Thomas Funkhouser. Blended intrinsic maps. ACM Trans.

Graph.  30(4):79:1–79:12  July 2011.

[26] Spyridon Leonardos  Xiaowei Zhou  and Kostas Daniilidis. Distributed consistent data associa-

tion via permutation synchronization. In ICRA  pages 2645–2652. IEEE  2017.

[27] Marius Leordeanu and Martial Hebert. A spectral technique for correspondence problems using
pairwise constraints. In Proceedings of the Tenth IEEE International Conference on Computer
Vision - Volume 2  ICCV ’05  pages 1482–1489  Washington  DC  USA  2005. IEEE Computer
Society.

[28] Ce Liu  Jenny Yuen  and Antonio Torralba. Sift ﬂow: Dense correspondence across scenes and

its applications. IEEE Trans. Pattern Anal. Mach. Intell.  33(5):978–994  May 2011.

[29] Andy Nguyen  Mirela Ben-Chen  Katarzyna Welnicka  Yinyu Ye  and Leonidas Guibas. An
Optimization Approach to Improving Collections of Shape Maps. Computer Graphics Forum 
30:1481–1491  2011.

[30] Jorge Nocedal and Stephen J. Wright. Numerical optimization. Springer series in operations

research and ﬁnancial engineering. Springer  New York  NY  2. ed. edition  2006.

[31] Maks Ovsjanikov  Mirela Ben-Chen  Justin Solomon  Adrian Butscher  and Leonidas J. Guibas.
Functional maps: a ﬂexible representation of maps between shapes. ACM Trans. Graph. 
31(4):30:1–30:11  2012.

[32] Deepti Pachauri  Risi Kondor  Gautam Sargur  and Vikas Singh. Permutation diffusion maps
(PDM) with application to the image association problem in computer vision. In NIPS  pages
541–549  2014.

[33] Deepti Pachauri  Risi Kondor  and Vikas Singh. Solving the multi-way matching problem by

permutation synchronization. In NIPS  pages 1860–1868  2013.

[34] David M. Rosen  Luca Carlone  Afonso S. Bandeira  and John J. Leonard. Se-sync: A certiﬁably
correct algorithm for synchronization over the special euclidean group. I. J. Robotics Res. 
38(2-3)  2019.

[35] Raif M. Rustamov. Laplace-beltrami eigenfunctions for deformation invariant shape representa-
tion. In Proceedings of the Fifth Eurographics Symposium on Geometry Processing  SGP ’07 
pages 225–233  Aire-la-Ville  Switzerland  Switzerland  2007. Eurographics Association.

10

[36] Yanyao Shen  Qixing Huang  Nati Srebro  and Sujay Sanghavi. Normalized spectral map
synchronization. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors 
Advances in Neural Information Processing Systems 29  pages 4925–4933. Curran Associates 
Inc.  Barcelona  Spain  2016.

[37] Hao Su  Charles Ruizhongtai Qi  Yangyan Li  and Leonidas J. Guibas. Render for CNN:
viewpoint estimation in images using cnns trained with rendered 3d model views. In ICCV 
pages 2686–2694. IEEE Computer Society  2015.

[38] Joel A. Tropp. An introduction to matrix concentration inequalities. Found. Trends Mach.

Learn.  8(1-2):1–230  May 2015.

[39] Fan Wang  Qixing Huang  and Leonidas J. Guibas. Image co-segmentation via consistent
functional maps. In Proceedings of the 2013 IEEE International Conference on Computer
Vision  ICCV ’13  pages 849–856  Washington  DC  USA  2013. IEEE Computer Society.

[40] Fan Wang  Qixing Huang  Maks Ovsjanikov  and Leonidas J. Guibas. Unsupervised multi-class

joint image segmentation. In CVPR  pages 3142–3149. IEEE Computer Society  2014.

[41] Lanhui Wang and Amit Singer. Exact and stable recovery of rotations for robust synchronization.

Information and Inference: A Journal of the IMA  2:145–193  December 2013.

[42] Yunhai Wang  Shmulik Asaﬁ  Oliver van Kaick  Hao Zhang  Daniel Cohen-Or  and Baoquan
Chen. Active co-analysis of a set of shapes. ACM Trans. Graph.  31(6):165:1–165:10  November
2012.

[43] Yu Xiang  Roozbeh Mottaghi  and Silvio Savarese. Beyond PASCAL: A benchmark for 3d

object detection in the wild. In WACV  pages 75–82. IEEE Computer Society  2014.

[44] Christopher Zach  Manfred Klopschitz  and Marc Pollefeys. Disambiguating visual relations

using loop constraints. In CVPR  pages 1426–1433. IEEE Computer Society  2010.

[45] Zaiwei Zhang  Zhenxiao Liang  Lemeng Wu  Xiaowei Zhou  and Qixing Huang. Path-invariant

map networks. CoRR  abs/1812.11647  2018.

[46] Tinghui Zhou  Philipp Krähenbühl  Mathieu Aubry  Qi-Xing Huang  and Alexei A. Efros.
Learning dense correspondence via 3d-guided cycle consistency. In CVPR  pages 117–126 
2016.

[47] Tinghui Zhou  Yong Jae Lee  Stella X. Yu  and Alexei A. Efros. Flowweb: Joint image set
alignment by weaving consistent  pixel-wise correspondences. In CVPR  pages 1191–1200.
IEEE Computer Society  2015.

[48] Xiaowei Zhou  Menglong Zhu  and Kostas Daniilidis. Multi-image matching via fast alternating
minimization. In Proceedings of the IEEE International Conference on Computer Vision  pages
4032–4040  2015.

11

,Shenlong Wang
Alex Schwing
Raquel Urtasun
Leonidas Guibas
Qixing Huang
Zhenxiao Liang