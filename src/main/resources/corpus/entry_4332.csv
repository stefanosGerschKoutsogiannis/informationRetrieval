2017,Position-based Multiple-play Bandit Problem with Unknown Position Bias,Motivated by online advertising  we study a multiple-play multi-armed bandit problem with position bias that involves several slots and the latter slots yield fewer rewards. We characterize the hardness of the problem by deriving an asymptotic regret bound. We propose the Permutation Minimum Empirical Divergence (PMED) algorithm and derive its asymptotically optimal regret bound. Because of the uncertainty of the position bias  the optimal algorithm for such a problem requires non-convex optimizations that are different from usual partial monitoring and semi-bandit problems. We propose a cutting-plane method and related bi-convex relaxation for these optimizations by using auxiliary variables.,Position-based Multiple-play Bandit Problem with

Unknown Position Bias

Junpei Komiyama

The University of Tokyo
junpei@komiyama.info

Junya Honda

The University of Tokyo / RIKEN
honda@stat.t.u-tokyo.ac.jp

Akiko Takeda

The Institute of Statistical Mathematics / RIKEN

atakeda@ism.ac.jp

Abstract

Motivated by online advertising  we study a multiple-play multi-armed bandit
problem with position bias that involves several slots and the latter slots yield
fewer rewards. We characterize the hardness of the problem by deriving an asymp-
totic regret bound. We propose the Permutation Minimum Empirical Divergence
(PMED) algorithm and derive its asymptotically optimal regret bound. Because
of the uncertainty of the position bias  the optimal algorithm for such a problem
requires non-convex optimizations that are different from usual partial monitor-
ing and semi-bandit problems. We propose a cutting-plane method and related
bi-convex relaxation for these optimizations by using auxiliary variables.

1

Introduction

One of the most important industries related to computer science is online advertising. In the United
States  72.5 billion dollars was spent on online advertising [19] in 2016. Most online advertising is
viewed on web pages during Internet browsing. A web-site owner has a set of possible advertisements
(ads): some of them are more attractive than others  and the owner would like to maximize the
attention of visiting users. One of the observable metrics of the user attention is the number of
clicks on the ads. By considering each ad (resp. click) to be an arm (resp. reward) and assuming
only one slot is available for advertisements  the maximization of clicks boils down to the so-called
multi-armed bandit problem  where the arm with the largest expected reward is sought.
When two or more ad slots are available on the web page  the problem boils down to a multiple-play
multi-armed bandit problem. Several variants of the multiple play bandit problem and its extension
called semi-bandit problem have been considered in the literature. Arguably  the simplest is one
assuming that an ad receives equal clicks regardless of its position [2  24]. In practice  ads receive
less clicks when they are placed at bottom slots; this is so-called position bias.
A well-known model that explains position bias is the cascade model [23]  which assumes that the
users’ attention goes from top to bottom until they lose interest. While this model explains position
bias in early positions well [10]  a drawback to the cascade model when it is applied to the bandit
setting [26] is that the order of the allocated ads does not affect the reward  which is not very natural.
To resolve this issue  Combes et al. [8] introduced a weight for each slot that corresponds to the
reward obtained by clicking on that slot. However  no principled way of deﬁning the weight has been
described.
An extension of the cascade model  called the dependent click model (DCM) [14]  addresses these
issues by admitting multiple clicks of a user. In DCM  each slot is associated with a probability that

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the user loses interest in the following ads if the current ad is interesting. While the algorithm in
Katariya et al. [21] cleverly exploits this structure  it still depends on the cascade assumption  and
as a result it discards some of the feedback on the latter slots  which reduces the efﬁciency of the
algorithm. Moreover  the reward in DCM does not exactly correspond to the number of clicks.
Lagrée et al. [27] has studied a position-based model (PBM) where each slot has its own discount
factor on the number of clicks. PBM takes the order of the shown ads into consideration. However 
the algorithms proposed in Lagrée et al. [27] are “half-online” in the sense that the value of an ad is
adaptively estimated  whereas the values of the slots are estimated by using an off-line dataset. Such
an off-line computation is not very handy since the click trend varies depending on the day and hour
[1]. Moreover  a signiﬁcant portion of online advertisements is sold via ad networks [34]. As a result 
advertisers have to deal with thousands of web pages to show their ads. Taking these aspects into
consideration  pre-computing position bias for each web page limits the use of these algorithms.
To address this issue  we provide a way to allocate advertisements in a fully online manner by
considering “PBM under Uncertainty of position bias” (PBMU). One of the challenges when the
uncertainty of a position-based factor is taken into account is that  when some ad appears to have a
small click through rate (CTR  the probability of click) in some slot  we cannot directly attribute it to
either the arm or the slot. In this sense  several combinations of ads and slots need to be examined to
estimate both the ad-based and position-based model parameters.
Note also that an extension of the non-stochastic bandit approach [3] to multiple-play  such as
the ordered slate model [20]  is general enough to deal with PBMU. However  algorithms based
on the non-stochastic approach do not always perform well in compensation for its generality.
Another extension of multi-armed bandit problems is the partial monitoring problem [31  4] that
admits the case in which the parameters are not directly observable. However  partial monitoring is
inefﬁcient at solving bandit problems: a K-armed bandit problem with binary rewards corresponds to
a partial monitoring problem with 2K possible outcomes. As a result  the existing partial monitoring
algorithms  such as the ones in [33  25]  are not practical even for a moderate number of arms.
Besides  the computation of a feasible solution in PBMU requires non-convex optimizations as we
will see in Section 5. This implies that PBMU cannot directly be converted into the partial monitoring
where such a non-convex optimization does not appear [25].
The contributions of this paper are as follows: First  we study the position-based bandit model with
uncertainty (PBMU) and derive a regret lower bound (Section 3). Second  we propose an algorithm
that efﬁciently utilizes feedback (Section 4). One of the challenges in the multiple-play bandit
problem is that there is an exponentially large number of possible sequences of arms to allocate at
each round. We reduce the number of candidates by using a bipartite matching algorithm that runs in
a polynomial time to the number of arms. The performance of the proposed algorithm is veriﬁed in
Section 6. Third  a slightly modiﬁed version of the algorithm is analyzed in Section 7. This algorithm
has a regret upper bound that matches the lower bound. Finally  we reveal that the lower bound is
related to a linear optimization problem with an inﬁnite number of constraints. Such an optimization
problem appears in many versions of the bandit problem [9  25  12]. We propose an optimization
method that reduces it to a ﬁnite-constraint linear optimization based on a version of the cutting-plane
method (Section 5). Related non-convex optimizations that are characteristic to PBMU are solved by
using bi-convex relaxation. Such optimization methods are of interest in solving even larger classes
of bandit problems.

2 Problem Setup

Let K be the number of arms (ads) and L < K be the number of slots. Each arm i ∈ [K] =
{1  2  . . .   K} is associated with a distinct parameter θ∗
i ∈ (0  1)  and each slot l ∈ [L] is associated
l ∈ (0  1]. At each round t = 1  2  . . .   T   the system selects L arms I(t) =
with a parameter κ∗
(I1(t)  . . .   IL(t)) and receives a corresponding binary reward (click or non-click) for each slot. The
reward of the l-th slot is i.i.d. drawn from a Bernoulli distribution Ber(µ∗
i κ∗
l .
Although the slot-based parameters are unknown  it is natural that the ads receives more clicks when
they are placed at early slots: we assume κ∗
Note that this model is redundant: a model with µ∗
µ∗
i l = (θ∗

is equivalent to the model with
l κ1). Therefore  without loss of generality  we assume κ1 = 1. In summary 

2 > ··· > κ∗
i l = θ∗

i /κ1)(κ∗

i l = θ∗

Il(t) l)  where µ∗
L > 0 and this order is known.
i κ∗

l

1 > κ∗

2

i }i∈[K] and {κ∗

t-th round at which arm i was in slot l (i.e.  Ni l(t) =(cid:80)t−1

this model involves K + L parameters {θ∗
l }l∈[L]  and the number of rounds T . The
parameters except for κ1 = 1 are unknown to the system. Let Ni l(t) be the number of rounds before
t(cid:48)=1 1{i = Il(t(cid:48))}  where 1{E} is 1 if E
holds and 0 otherwise). In the following  we abbreviate arm i in slot l to “pair (i  l)”. Let ˆµi l(t) be
the empirical mean of the reward of pair (i  l) after the ﬁrst t − 1 rounds.
The goal of the system is to maximize the cumulative rewards by using some sophisticated algorithm.
Without loss of generality  we can assume θ∗
K. The algorithm cannot exploit
this ordering. In this model  allocating arms of larger expected rewards on earlier slots increases
expected rewards: As a result  allocating arms 1  2  . . .   L to slots 1  2  . . .   L maximizes the expected
 
l . Re-
(i l)∈[K]×[L] ∆i lNi l(T ). The regret increases

reward. A quantity called (pseudo) regret is deﬁned as: Reg(T ) =(cid:80)T
gret can be alternatively represented as Reg(T ) =(cid:80)

i − θ∗
i∈[L](θ∗
l − θ∗
and E[Reg(T )] is used for evaluating the performance of an algorithm. Let ∆i l = θ∗
l κ∗

Ii(t))κ∗
i κ∗

3 > ··· > θ∗

(cid:16)(cid:80)

2 > θ∗

1 > θ∗

(cid:17)

t=1

i

unless I(t) = (1  2  . . .   L).

1  . . .   θ(cid:48)

K) ∈ (0  1)K} and Kall = {(κ(cid:48)

3 Regret Lower Bound
Here  we derive an asymptotic regret lower bound when T → ∞. In the context of the standard multi-
armed bandit problem  Lai and Robbins [28] derived a regret lower bound for strongly consistent
algorithms  and it is followed by many extensions  such as the one for multi-parameter distributions
[6] and the ones for Markov decision processes [13  7]. Intuitively  a strongly consistent algorithm is
“uniformly good” in the sense that it works well with any set of model parameters. Their result was
extended to the multiple-play [2] and PBM [27] cases. We further extend it to the case of PBMU.
L > 0} be
Let Tall = {(θ(cid:48)
the sets of all possible values on the parameters of the arms and slots  respectively. Let (1)  . . .   (K)
be a permutation of 1  . . .   K and T(1) ... (L) be the subset of Tall such that the i-th best arm is (i).
(cid:111)
Namely 
T(1) ... (L) =
 
(1) ... (L) = Tall \T(1) ... (L). An algorithm is strongly consistent if E[Reg(T )] = o(T a) for any
and T c
a > 0 given any instance of the bandit problem with its parameters {θ(cid:48)
l} ∈ Kall.
The following lemma  whose proof is in Appendix F  lower-bounds the number of draws on the pairs
of arms and slots.
Lemma 1. (Lower bound on the number of draws) The following inequality holds for Ni l(T ) of the
strongly consistent algorithm:

(L) ∀i /∈{(1) ... (L)}(θ(cid:48)

i}i∈[K] ∈ Tall  {κ(cid:48)

K) ∈ (0  1)K : θ(cid:48)

(2) > ··· > θ(cid:48)

2 > ··· > κ(cid:48)

L) : 1 = κ(cid:48)

(θ(cid:48)
1  . . .   θ(cid:48)

1  . . .   κ(cid:48)

(1) > θ(cid:48)

1 > κ(cid:48)

i < θ(cid:48)

(cid:110)

(L))

∀{θ(cid:48)

i}∈T c

1 ... L {κ(cid:48)

l}∈Kall

E[Ni l(T )]dKL(θ∗

i κ∗

l) ≥ log T − o(log T ) 
iκ(cid:48)
l   θ(cid:48)

(cid:88)

(i l)∈[K]×[L]

where dKL(p  q) = p log(p/q) + (1 − p) log((1 − p)/(1 − q)) is the KL divergence between two
Bernoulli distributions.

Such a divergence-based bound appears in many stochastic bandit problems. However  unlike other
bandit problems  the argument inside the KL divergence is a product of parameters θ(cid:48)
iκ(cid:48)
l: While
i} {κ(cid:48)
l}. Therefore  ﬁnding
dKL(·  θ(cid:48)
l  it is not convex to the parameter space {θ(cid:48)
iκ(cid:48)
l) is convex to θ(cid:48)
iκ(cid:48)
i l dKL(µi l  θ(cid:48)
iκ(cid:48)
l) is non-convex  which makes PBMU difﬁcult.
(cid:88)

a set of parameters that minimizes(cid:80)
{qi l} ∈ [0 ∞)[K]×[K] : ∀i∈[K−1]

Furthermore  we can formalize the regret lower bound in what follows. Let

qi+1 l ∀l∈[K−1]

(cid:88)

(cid:88)

(cid:88)

Q =

qi l =

qi l =

qi l+1

 .

i∈[K]

l∈[K]

l∈[K]

i∈[K]

Intuitively  {qi l} for l ≤ L corresponds to the draw of arm i in slot l  and {qi l} for l > L corresponds
to the non-draw of arm i  as we will see later. The following quantities characterizes the minimum

3

(1) ... (L)({µi l} {θi} {κl}) =
C∗
(cid:26)

the set of optimal solutions of which is denoted by

{qi l}∈R(1) ... (L)({µi l} {θi} {κl})

inf

∆i lqi l  

(i l)∈[K]×[L]

amount of exploration for consistency:

R(1) ... (L)({µi l} {θi} {κl}) =

{qi l} ∈ Q :

(cid:26)

{θ(cid:48)

i}∈T c

(1) ... (L) {κ(cid:48)

(cid:88)

(i l)∈[K]×[L]:i(cid:54)=(l)

inf
l}∈Kall:∀i∈[L]θ(cid:48)
iκ(cid:48)
l) ≥ 1
iκ(cid:48)
qi ldKL (µi l  θ(cid:48)

i=θiκi

(cid:27)

.

(1)

i} {κ(cid:48)

Equality (1) states that drawing each pair (i  l) for Ni l = qi l log T times sufﬁces to reduce the risk
that the true parameter is {θ(cid:48)
(1) ... (L) and
l = θiκi for any i ∈ [L]. Note that the constraint θ(cid:48)
θ(cid:48)
iκ(cid:48)
iκ(cid:48)
i = θiκi corresponds to the fact that drawing
an optimal list of arms does not increase the regret: Intuitively  this corresponds to the fact that the
true parameter of the best arm is obtained for free in the regret lower bound of the standard bandit
problem1. Moreover  let

l} for any parameters {θ(cid:48)

l} such that θ(cid:48)

i ∈ T c

i} {κ(cid:48)

(cid:88)

R∗
(1) ... (L)({µi l} {θi} {κl}) =

{qi l} ∈ R(1) ... (L)({µi l} {θi} {κl}) :

(cid:88)

∆i lqi l = C∗

(i l)∈[K]×[L]

(cid:27)
(1) ... (L)({µi l} {θi} {κl})

.

(2)

i } {κ∗

l } from any {θ(cid:48)

1 ... L log T is the possible minimum regret such that the minimum divergence of
l} is larger than log T . Using Lemma 1 yields the following regret

The value C∗
{θ∗
lower bound  whose proof is also in the Appendix F.
Theorem 2. The regret of a strongly consistent algorithm is lower bounded as follows:

i} {κ(cid:48)

E[Reg(T )] ≥ C∗

1 ... L({µ∗

i l} {θ∗

i } {κ∗

l }) log T − o(log T ).

i ) for j = min(i − 1  L) satisﬁes the conditions in
Remark 3. Ni l = (log T )/dKL(θ∗
Lemma 1  which means that regret lower bound in Theorem 2 is O(K log T /∆) = O(K log T ) 
where ∆ = mini(cid:54)=j l(cid:54)=m |θ∗
j||κ∗

i   θ∗
i κ∗
j κ∗
m|.
l − κ∗

i − θ∗

4 Algorithm

Our algorithm  called Permutation Minimum Empirical Divergence (PMED)  is closely related to the
optimization we discussed in Section 3.

4.1 PMED Algorithm

We denote a list of L arms that are drawn at each round as L-allocation. For example  (3  2  1  5)
is a 4-allocation  which corresponds to allocating arms 3  2  1  5 to slots 1  2  3  4  respectively.
Like the Deterministic Minimum Empirical Divergence (DMED) algorithm [17] for the single-play
multi-armed bandit problem  Algorithm 1 selects arms by using a loop. LC = LC(t) is the set of
L-allocations in the current loop  and LN = LN (t) is the set of L-allocations that are to be drawn in
the next loop. Note that  |LN| ≥ 1 always holds at the end of each loop so that at least one element is
i (cid:54)= θiκi into consideration. However  such parameters can
be removed without increasing regret  and thus the inﬁmum over θ(cid:48)
i = θiκi sufﬁces. This can be under-
stood because the regret bound of the standard K-armed bandit problem with expectation of each arm µi
i=2(log T )/dKL(µi  µ1): Arm 1 is drawn without increasing regret  and thus estimation of µ1 can be
arbitrary accurate. In our case placing arms 1  ...  L into slots 1  ...  L does not increase the regret  and thus the
estimation of the product parameter θiκi for each i ∈ [L] is very accurate.

1The inﬁmum should take parameters θ(cid:48)

is(cid:80)K

iκ(cid:48)

iκ(cid:48)

4

Algorithm 1 PMED and PMED-Hinge Algorithms
√
1: Input: α > 0  β > 0 (for PMED-Hinge)  f (n) = γ/
2: LN ← ∅. LC ← {vmod
3: while t ≤ T do
for each vmod
4:
5:
6:
7:

If there exists some pair (i  l) ∈ vmod
i=1  {ˆκl(t)}L

end for
Compute the MLE {ˆθi(t)}K

m : m ∈ [K] do

K }.
  . . .   vmod

l=1

1

n with γ > 0 (for PMED-Hinge).

m such that Ni l(t) < α

√

log t  then put vmod

m into LN .

=

(cid:40)min{θi κl}(cid:80)
min{θi κl}(cid:80)
If (cid:83)

(i l)∈[K]×[L] Ni l(t)dKL(ˆµi l(t)  θiκl)
(i l)∈[K]×[L] Ni l(t) (dKL(ˆµi l(t)  θiκl) − f (Ni l(t)))+ .

(PMED)

(PMED-Hinge)

if Algorithm is PMED-Hinge then
If |ˆθi(t) − ˆθj(t)| < β/(log log t) for some i (cid:54)= j or |ˆκl(t) − ˆκm(t)| < β/(log log t) for
some l (cid:54)= m  then put all of vmod

  . . .   vmod

K to LN .

(i l)∈[K]×[L]{dKL(ˆµi l(t)  ˆθi(t)ˆκl(t)) > f (Ni l(t))} holds 
  . . .   vmod

1

then put all of

({ˆµi l(t)} {ˆθi(t)} {ˆκl(t)})
({ˆµi l(t)} {ˆθi(t)} {ˆκl(t)} {f (Ni l(t))}). (PMED-Hinge)

(PMED)

K into LN .

vmod
1
end if
Compute {qi l}∈
˜Ni l ← qi l log t for each (i  l) ∈ [K] × [K].

R∗
Decompose ˜Ni l =(cid:80)

ˆ1(t) ...  ˆL(t)
R∗ H
ˆ1(t) ...  ˆL(t)

v creq

8:
9:

10:

11:

12:

13:
14:

15:
16:
17:
18:

v ev where ev for each v is a permutation matrix and creq

v > 0 by

(cid:16)

using Algorithm 2.
ri l ← Ni l(t).
for each permutation matrix ev do
v ← min
caﬀ
Let (v1  . . .   vL) be the L-allocation corresponding to ev. If caﬀ
pair (vl  l) that is in none of the L-allocations in LN   then put (v1  . . .   vL) into LN .
ri l ← ri l − caﬀ

(cid:8)c > 0 : min(i l)∈[K]×[L](ri l − c ev i l) ≥ 0(cid:9)(cid:17)

.
v < creq

creq
v   maxc

v ev i l.

v

and there exists a

19:
20:
21:
22:
23:
24: end while

end for
Select I(t) ∈ LC in an arbitrary ﬁxed order. LC → LC \ {I(t)}.
Put (ˆ1(t)  . . .   ˆL(t)) into LN .
If LC = ∅ then LC ← LN   LN ← ∅.

  . . .   vmod

put into LC. There are three lines where L-allocations are put into LN without duplication: Lines 5 
18  and 22. We explain each of these lines below.
Line 5 is a uniform exploration over all pairs (i  l). For m ∈ [K]  let vmod
m be an L-allocation
(1 + modK(m)  1 + modK(1 + m)  . . .   1 + modK(L + m− 1))  where modK(x) is the minimum
non-negative integer among {x−cK : c ∈ N}. From the deﬁnition of vmod
m   any pair (i  l) ∈ [K]×[L]
√
belongs to exactly one of vmod
log t times  a
K . If some pair (i  l) is not allocated α
corresponding L-allocation is put into LN . This exploration stabilizes the estimators.
Line 18 and related routines are based on the optimal amount of explorations. { ˜Ni l}i∈[K] l∈[K] is
calculated by plugging in the maximum likelihood estimator (MLE) ({ˆθi}i∈[K] {ˆκl}l∈[L]) into the
optimization problem of Inequality (2). As { ˜Ni l} is a set of K × K variables2  the algorithm needs
to convert it into a set of L-allocations to put them into LN . This is done by decomposing it into a set
of permutation matrices  which we will explain in Section 4.2.
Line 22 is for exploitation: If no pair is put to LN by Line 5 or Line 18 and LC is empty  then Line
22 puts arms (ˆ1(t)  . . .   ˆL(t)) of the top-L largest {ˆθi(t)} (with ties broken arbitrarily) into LN .

1

2K × K is not a typo of K × L: {qi l} and { ˜Ni l} are sets of K 2 variables.

5

Algorithm 2 Permutation Matrix Decomposition
1: Input: Ni l.
2: ¯Ni l ← Ni l.
3: while ¯Ni l > 0 for some (i  l) ∈ [K] × [K] do
4:
5:
6:
7: end while
8: Output {creq

v   ev}

(cid:8)c > 0 : min(i l)∈[K]×[K]( ¯Ni l − cev i l) ≥ 0(cid:9).

Find a permutation matrix ev such that  for any i  l such that ev i l = 1 ⇒ ¯Ni l > 0.
Let creq
v = maxc
¯Ni l ← ¯Ni l − creq

v ev i l for each (i  l) ∈ [K] × [K].

Figure 1: A permutation matrix with K = 4  where (i  l) = 1 for (i  l) ∈ (1  1)  (2  3)  (3  2)  (4  4).
If L = 2  this matrix corresponds to allocating arm 1 in slot 1 and arm 3 in slot 2.

4.2 Permutation Matrix and Allocation Strategy
In this section  we discuss the way to convert { ˜Ni l} = {qi l log t}  the estimated optimal amount of
exploration  into L-allocations. A permutation matrix is a square matrix that has exactly one entry of
1 at each row and each column and 0s elsewhere (Figure 1  left). There are K! permutation matrices
since they corresponds to ordering K elements. Therefore  even though {qi l} can be obviously
decomposed into a linear combination of permutation matrices  it is not clear how to compute them
without computing the set of all permutation matrices that are exponentially large in K. Algorithm
2 solves this problem: Let ¯Ni l be a temporal variable that is initialized by ˜Ni l at the beginning.
In each iteration  it subtracts a scalar multiplication of a permutation matrix ev whose (i  l) entry
ev i l of value 1 corresponds to ¯Ni l > 0. (Line 6 in Algorithm 2). This boils down to ﬁnding a
perfect matching in a bipartite graph where the left (resp. right) nodes correspond to rows (resp.
columns) and edges between nodes i and l are spanned if ¯Ni l > 0. Although a naive greedy fails
in such a matching problem (c.f.  Appendix A)  a maximal matching in a bipartite graph can be
computed by the Hopcroft–Karp algorithm [18] in O(K 2.5) times  and Theorem 4 below ensures
that the maximum matching is always perfect:
¯Ni l > 0}
Theorem 4. (Existence of a perfect matching) For any { ¯Ni l ∈ [K]× [K] : ¯Ni l ≥ 0 ∃(i l)
such that the sums of each row and column are equal  there exists a permutation matrix ev such that
∀(i l)∈[K]×[K]:ev i l=1
The proof of Theorem 4 is in Appendix E. Each subtraction increases the number of 0 entries in
¯Ni l (Line 5 in Algorithm 2); Algorithm 2 runs in O(K 4.5) times by computing at most O(K 2)
perfect matching sub-problems  and as a result it decomposes ˜Ni l into a positive linear combination
of permutation matrices. The main algorithm checks whether each the entries of the permutation
matrices are sufﬁciently explored (Line 18 in Algorithm 1)  and draws an L-allocation corresponding
to a permutation matrix (Figure 1  right) if under-explored.

¯Ni l > 0.

5 Optimizations

This section discusses two optimizations that appear in Algorithm 1  namely  the MLE computation
(Line 7)  and the computation of the optimal solution (Line 12).
MLE (Line 7) is the solution of a bi-convex optimization: the optimization of {θi} (resp. {κl}) is
convex when we view {κl} (resp. {θi}) as a constant. Therefore  off-the-shelf tools for optimizing
convex functions (e.g.  Newton’s method) are applicable to alternately optimizing {θi} and {κl}.
Assuming that each convex optimization yields an optimal value  such an alternate optimization

6

111100000000000011000000Algorithm 3 Cutting-plane method for obtaining {qi l} on Line 12 of Algorithm 1
1: Input: the number of iterations S  nominal constraint {θ(0)
2: for s = 1  2  . . .   S do
3:

(i l)∈[K]×[L] ∆i lqi l such that

i } ∈ T c

Find q(s)

ˆ1(t) ...  ˆL(t)

.

(cid:32)

(cid:33)

qi ldKL

ˆµi l(t)  θ(cid:48)

i

ˆθl(t)ˆκl(t)

θ(cid:48)

l

≥ 1

(i l)∈[K]×[L]:i(cid:54)=ˆl(t)

i }  . . .  {θ(s−1)
}.
i l dKL(ˆµi l(t)  θ(cid:48)
(i l)∈[K]×[L] q(s)

i

i

ˆθl(t)ˆκl(t)

θ(cid:48)

l

).

i l ← min{qi l}∈Q(cid:80)
(cid:88)
i}(cid:80)

i} ∈ {θ(0)
i } ← min{θ(cid:48)

i } {θ(1)

for all {θ(cid:48)
Find {θ(s)

4:
5: end for

ˆθl(t)ˆκl(t)

i

θ(cid:48)

l

ˆ1(t) ...  ˆL(t)

(cid:80)

i} and {κ(cid:48)

(Convergence of

there exists a constant C and that

) is Lipchitz continuous as |f ({θ(1)

monotonically decreases the objective function and thus converges. Note that a local minimum
obtained by bi-convex optimizations is not always a global minimum due to its non-convex nature.
Although the computation of the optimal solution (Line 12) involves {θ(cid:48)
l}  the constraint
i = ˆθi(t)ˆκi(t)/θ(cid:48)
eliminates latter variables as κ(cid:48)
i. This optimization is a linear semi-inﬁnite pro-
gramming (LSIP) on {qi l}  which is a linear programming (LP) with an inﬁnite set of linear
constraints parameterized by {θ(cid:48)
i}. Algorithm 3 is the cutting-plane method with pessimistic oracle
[29] that boils the LSIP down to ﬁnite constraint LPs. At each iteration s  it adds a new constraint
{θ(s)
i } ∈ T c
that is “hardest” in a sense that it minimizes the sum of divergences (Line 4 in
Algorithm 3). The following theorem guarantees the convergence of the algorithm when the exactly
hardest constraint is found.
Theorem 5.
tion 5.2]) Assume that

the cutting-plane method  Mutapcic and Boyd [29  Sec-
i}) =
i })| ≤
i l dKL(ˆµi l(t)  θ(cid:48)
i }||  where the norm ||·|| is any Lp norm. Then  Algorithm 3 converges to its optimal

(i l)∈[K]×[L] q(s)
i }−{θ(2)
C||{θ(1)
solution as S → ∞.
Although the Lipchitz continuity assumption does not hold as dKL(p  q) approaches inﬁnity when q
is close to 0 or 1  by restricting q to some region [  1 − ]  Lipchitz continuity can be guaranteed for
some C = C(). Theorem 5 assumes the availability of an exact solution to the hardest constraint 
which is generally hard since this objective is non-convex in its nature. Still  we can obtain a fair
solution with the following reasons: First  although the space T c
is not convex  it suf-
i} ∈ (0  1)K : θ(cid:48)
ﬁces to consider each of the convex subspaces
ˆl(t)
where X = min(L  l − 1)  for each l ∈ [K] \ {1} separately because the hardest constraint
is always in one of these subspaces (which follows from the convexity of the objective func-
tion). Second  the following bi-convex relaxation can be used: Let η(cid:48)
(cid:80)
L be auxiliary
1  . . .   1/θ(cid:48)
variables that correspond to 1/θ(cid:48)
L. Namely  we optimize a relaxed objective function
i − 1)2  where φ > 0 is a penalty
i∈[L](θ(cid:48)
iη(cid:48)
i l dKL(ˆµi l(t)  θ(cid:48)
iη(cid:48)
q(s)
(i l)∈[K]×[L]
i} and
l}  and thus an alternate optimization is effective. Setting φ → ∞ induces a solution in which η(cid:48)
i is
i ([30  Theorem 17.1]). Our algorithm starts with a small value of φ; then it gradually

parameter. Convexity of KL divergence implies that this objective is a bi-convex function of {θ(cid:48)
{η(cid:48)
equal to 1/θ(cid:48)
increases φ.

the constraint f ({θ(cid:48)
i }) − f ({θ(2)

(cid:110){θ(cid:48)
(cid:17)

+ φ(cid:80)

≥ ··· ≥ θ(cid:48)

1  . . .   η(cid:48)

ˆθl(t)ˆκl(t))

ˆ1(t) ...  ˆL(t)

ˆL(t)

ˆX(t)

= θ(cid:48)

(cid:111)

(cid:16)

  θ(cid:48)

ˆ1(t)

l

6 Experiment

To evaluate the empirical performance of the proposed algorithms  we conducted computer simula-
tions with synthetic and real-world datasets. The compared algorithms are MP-TS [24]  dcmKL-UCB
[21]  PBM-PIE [27]  and PMED (proposed in this paper). MP-TS is an algorithm based on Thompson
sampling [32] that ignores position bias: it draws the top-L arms on the basis of posterior sampling 
and the posterior is calculated without considering position bias. DcmKL-UCB is a KL-UCB [11]

7

(a) Synthetic

(b) Real-world (Tencent)

Figure 2: Regret-round log-log plots of algorithms.

based algorithm that works under the DCM assumption. PBM-PIE is an algorithm that allocates
top-(L − 1) slots greedily and allocates L-th arm based on the KL-UCB bound. Note that PBM-PIE
requires an estimation of {κ∗
l }; here  a bi-convex optimization is used to estimate it3. We did not test
PBM-TS [27]  which is another algorithm for PBM  mainly because that its regret bound has not
been derived yet. However  its regret appears to be asymptotically optimal when {κ∗
l } are known
(Figure 1(a) in Lagrée et al.[27])  and thus it does not explore sufﬁciently when there is uncertainty in
the position bias. We set α = 10 for PMED. We used the Gurobi LP solver4 for solving the LPs. To
speed up the computation  we skipped the bi-convex and LP optimizations in most rounds with large
t and used the result of the last computation. We used the Newton’s method (resp. a gradient method)
for computing the MLE (resp. the hardest constraint) in Algorithm 3.
Synthetic data: This simulation was designed to check the consistency of the algorithms  and it
involved 5 arms with (θ1  . . .   θ5) = (0.95  0.8  0.65  0.5  0.35)  and 2 slots with (κ1  κ2) = (1  0.6).
The experimental results are shown on the left of Figure 2. The results are averaged over 100 runs. LB
is the simulated value of the regret lower bound in Section 3. While the regret of PMED converges 
the other algorithms suffer a 100 times or larger regret than LB at T = 107  which implies that these
algorithms are not consistent under our model.
Real-world data: Following the existing work [24  27]  we used the KDD Cup 2012 track 2 dataset
[22] that involves session logs of soso.com  a search engine owned by Tencent. Each of the 150M
lines from the log contains the user ID  the query  an ad  and a slot in {1  2  3} at which the ad was
displayed and a binary reward indicated (click/no-click). Following Lagrée et al. [27]  we obtained
major 8 queries. Using the click logs of the queries  the CTRs and position bias were estimated in
order to maximize the likelihood by using bi-convex optimization in Section 4. Note that  the number
of arms and parameters are slightly different from the ones reported previously [27]. For the sake
of completeness  we show the parameters in Appendix C. We conducted 100 runs for each queries 
and the right ﬁgure in Figure 2 shows the averaged regret over 8 queries. Although the gap between
PMED and existing algorithms are not drastic compared with synthetic parameters  the existing
algorithms suffer larger regret than PMED.

7 Analysis

Although the authors conjecture that PMED is optimal  it is hard to analyze it directly. The technically
hardest part arises from the case in which the divergence of each action is small but not yet fully
converged. To circumvent these difﬁculty  we devised a modiﬁed algorithm called PMED-Hinge
(Algorithm 1) that involves extra exploration. In particular  we modify the optimization problem as

3The bi-convex optimization is identical to the one used for obtaining the MLE in PMED.
4http://www.gurobi.com

8

follows: Let

(1) ... (L)({µi l} {θi} {κl} {δi l}) =
RH

(cid:26)

{qi l} ∈ Q :

(cid:88)

(i l)∈[K]×[L]:i(cid:54)=(l)

where (x)+ = max(x  0). Moreover  let
∗ H
(1) ... (L)({µi l} {θi} {κl} {δi l}) =

C

the optimal solution of which is

R∗ H
(1) ... (L)({µi l} {θi} {κl} {δi l}) =
(cid:88)

(cid:26)

(l)κ(cid:48)

l)≤δi l

{θ(cid:48)

inf

(1) ... (L) {κ(cid:48)

l}∈Kall:∀l∈[L]dKL(µ(l) l θ(cid:48)

(cid:27)
i}∈T c
l) − δi l)+ ≥ 1
qi l (dKL(µi l  θ(cid:48)
iκ(cid:48)
(cid:88)

 

{qi l}∈RH

(1) ... (L)({µi l} {θi} {κl} {δi l})

inf

∆i lqi l  

(i l)∈[K]×[L]

{qi l} ∈ RH

(1) ... (L)({µi l} {θi} {κl} {δi l}) :
∗ H
(1) ... (L)({µi l} {θi} {κl} {δi l})

(cid:27)

.

∆i lqi l = C

(i l)∈[K]×[L]

The necessity of additional terms in PMED-Hinge are discussed in Appendix B. The following
theorem  whose proof is in Appendix G  derives a regret upper bound that matches the lower bound
in Theorem 2.
Theorem 6. (Asymptotic optimality of PMED-Hinge) Let the solution of the optimal exploration
R∗ H
l } {0}). For any
1 ... L({µi l} {θi} {κl} {δi l}) restricted to l ≤ L is unique at ({µ∗
α > 0  β > 0  and γ > 0  the regret of PMED-Hinge is bounded as:

i l} {θ∗

i } {κ∗

E[Reg(T )] ≤ C∗

1 ... L({µ∗

i l} {θ∗

i } {κ∗

l }) log T + o(log T ) .

Note that  the assumption on the uniqueness of the solution in Theorem 6 is required to achieve
an optimal coefﬁcient on the log T factor. It is not very difﬁcult to derive an O(log T ) regret even
though the uniqueness condition is not satisﬁed. Although our regret bound is not ﬁnite-time  the
only asymptotic analysis comes from the optimal constant on the top of log T term (Lemma 11 in
Appendix) and it is not very hard to derive an O(log T ) ﬁnite-time regret bound.

8 Conclusion

By providing a regret lower bound and an algorithm with a matching regret bound  we gave the ﬁrst
complete characterization of a position-based multiple-play multi-armed bandit problem where the
quality of the arms and the discount factor of the slots are unknown. We provided a way to compute
the optimization problems related to the algorithm  which is of its own interest and is potentially
applicable to other bandit problems.

9

Acknowledgements

The authors gratefully acknowledge Kohei Komiyama for discussion on a permutation matrix and
sincerely thank the anonymous reviewers for their useful comments. This work was supported in
part by JSPS KAKENHI Grant Number 17K12736  16H00881  15K00031  and Inamori Foundation
Research Grant.

References
[1] D. Agarwal  B.-C. Chen  and P. Elango. Spatio-temporal models for estimating click-through

rate. In WWW  pages 21–30  2009.

[2] V. Anantharam  P. Varaiya  and J. Walrand. Asymptotically efﬁcient allocation rules for the
multiarmed bandit problem with multiple plays-part i: I.i.d. rewards. Automatic Control  IEEE
Transactions on  32(11):968–976  1987.

[3] P. Auer  Y. Freund  and R. E. Schapire. The non-stochastic multi-armed bandit problem. Siam

Journal on Computing  2002.

[4] G. Bartók  D. P. Foster  D. Pál  A. Rakhlin  and C. Szepesvári. Partial monitoring - classiﬁcation 

regret bounds  and algorithms. Math. Oper. Res.  39(4):967–997  2014.

[5] S. Bubeck. Bandits Games and Clustering Foundations. Theses  Université des Sciences et

Technologie de Lille - Lille I  June 2010.

[6] A. Burnetas and M. Katehakis. Optimal adaptive policies for sequential allocation problems.

Advances in Applied Mathematics  17(2):122–142  1996.

[7] A. Burnetas and M. Katehakis. Optimal adaptive policies for markov decision processes. Math.

Oper. Res.  22(1):222–255  Feb. 1997.

[8] R. Combes  S. Magureanu  A. Proutière  and C. Laroche. Learning to rank: Regret lower bounds
and efﬁcient algorithms. In Proceedings of the 2015 ACM SIGMETRICS  pages 231–244  2015.

[9] R. Combes  M. S. Talebi  A. Proutière  and M. Lelarge. Combinatorial bandits revisited. In

NIPS  pages 2116–2124  2015.

[10] N. Craswell  O. Zoeter  M. J. Taylor  and B. Ramsey. An experimental comparison of click

position-bias models. In WSDM  pages 87–94  2008.

[11] A. Garivier and O. Cappé. The KL-UCB algorithm for bounded stochastic bandits and beyond.

In COLT  pages 359–376  2011.

[12] A. Garivier and E. Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence. In COLT 

pages 998–1027  2016.

[13] T. L. Graves and T. L. Lai. Asymptotically efﬁcient adaptive choice of control laws in controlled

markov chains. SIAM Journal on Control and Optimization  35(3):715–743  1997.

[14] F. Guo  C. Liu  and Y. M. Wang. Efﬁcient multiple-click models in web search. In WSDM 

pages 124–131  2009.

[15] P. Hall. On representatives of subsets. Journal of the London Mathematical Society  s1-10(1):26–

30  1935.

[16] W. W. Hogan. Point-to-set maps in mathematical programming. SIAM Review  15(3):591–603 

1973.

[17] J. Honda and A. Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded Support

Models. In COLT  pages 67–79  2010.

[18] J. E. Hopcroft and R. M. Karp. An n5/2 algorithm for maximum matchings in bipartite graphs.

SIAM Journal on Computing  2(4):225–231  1973.

10

[19] Interactive Advertising Bureau. IAB internet advertising revenue report - 2016 full year results. 

2017.

[20] S. Kale  L. Reyzin  and R. E. Schapire. Non-stochastic bandit slate problems. In NIPS  pages

1054–1062  2010.

[21] S. Katariya  B. Kveton  C. Szepesvári  and Z. Wen. DCM bandits: Learning to rank with

multiple clicks. In ICML  pages 1215–1224  2016.

[22] KDD cup 2012 track 2  2012.

[23] D. Kempe and M. Mahdian. A cascade model for externalities in sponsored search. In WINE 

pages 585–596  2008.

[24] J. Komiyama  J. Honda  and H. Nakagawa. Optimal regret analysis of thompson sampling in
stochastic multi-armed bandit problem with multiple plays. In ICML  pages 1152–1161  2015.

[25] J. Komiyama  J. Honda  and H. Nakagawa. Regret lower bound and optimal algorithm in ﬁnite

stochastic partial monitoring. In NIPS  pages 1792–1800  2015.

[26] B. Kveton  C. Szepesvári  Z. Wen  and A. Ashkan. Cascading bandits: Learning to rank in the

cascade model. In ICML  pages 767–776  2015.

[27] P. Lagrée  C. Vernade  and O. Cappé. Multiple-play bandits in the position-based model. In

NIPS  pages 1597–1605  2016.

[28] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[29] A. Mutapcic and S. P. Boyd. Cutting-set methods for robust convex optimization with pessimiz-

ing oracles. Optimization Methods and Software  24(3):381–406  2009.

[30] J. Nocedal and S. Wright. Numerical Optimization. Springer Series in Operations Research and

Financial Engineering. Springer New York  2nd edition  2006.

[31] A. Piccolboni and C. Schindelhauer. Discrete prediction games with arbitrary feedback and

loss. In COLT 2001 and EuroCOLT 2001  pages 208–223  2001.

[32] W. R. Thompson. On The Likelihood That One Unknown Probability Exceeds Another In View

Of The Evidence Of Two Samples. Biometrika  25:285–294  1933.

[33] H. P. Vanchinathan  G. Bartók  and A. Krause. Efﬁcient partial monitoring with prior information.

In NIPS  pages 1691–1699  2014.

[34] S. Yuan  J. Wang  and X. Zhao. Real-time bidding for online advertising: Measurement and
analysis. In Proceedings of the Seventh International Workshop on Data Mining for Online
Advertising  ADKDD ’13  pages 3:1–3:8. ACM  2013.

11

,Junpei Komiyama
Junya Honda
Akiko Takeda