2017,Adaptive Classification for Prediction Under a Budget,We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile  IoT  health  security and other applications  where constraints in the form of computation  communication  latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.,Adaptive Classiﬁcation for Prediction Under a Budget

Feng Nan

Systems Engineering

Boston University
Boston  MA 02215

fnan@bu.edu

Venkatesh Saligrama
Electrical Engineering

Boston University
Boston  MA 02215

srv@bu.edu

Abstract

We propose a novel adaptive approximation approach for test-time resource-
constrained prediction motivated by Mobile  IoT  health  security and other ap-
plications  where constraints in the form of computation  communication  latency
and feature acquisition costs arise. We learn an adaptive low-cost system by train-
ing a gating and prediction model that limits utilization of a high-cost model to
hard input instances and gates easy-to-handle input instances to a low-cost model.
Our method is based on adaptively approximating the high-cost model in regions
where low-cost models sufﬁce for making highly accurate predictions. We pose an
empirical loss minimization problem with cost constraints to jointly train gating
and prediction models. On a number of benchmark datasets our method outper-
forms state-of-the-art achieving higher accuracy for the same cost.

1

Introduction

Resource costs arise during test-time prediction in a number of machine learning applications. Fea-
ture costs in Internet  Healthcare  and Surveillance applications arise due to to feature extraction
time [23]  and feature/sensor acquisition [19]. In addition to feature acquisition costs  communica-
tion and latency costs pose a key challenge in the design of mobile computing  or the Internet-of-
Things(IoT) applications  where a large number of sensors/camera/watches/phones (known as edge
devices) are connected to a cloud.
Adaptive System: Rather than having the edge devices constantly transmit measurements/images
to the cloud where a centralized model makes prediction  a more efﬁcient approach is to allow
the edge devices make predictions locally [12]  whenever possible  saving the high communication
cost and reducing latency. Due to the memory  computing and battery constraints  the prediction
models on the edge devices are limited to low complexity. Consequently  to maintain high-accuracy 
adaptive systems are desirable. Such systems identify easy-to-handle input instances where local
edge models sufﬁce  thus limiting the utilization cloud services for only hard instances. We propose
to learn an adaptive system by training on fully annotated training data. Our objective is to maintain
high accuracy while meeting average resource constraints during prediction-time.
There have been a number of promising approaches that focus on methods for reducing costs while
improving overall accuracy [9  24  19  20  13  15]. These methods are adaptive in that  at test-
time  resources (features  computation etc) are allocated adaptively depending on the difﬁculty of
the input. Many of these methods train models in a top-down manner  namely  attempt to build out
the model by selectively adding the most cost-effective features to improve accuracy.
In contrast we propose a novel bottom-up approach. We train adaptive models on annotated training
data by selectively identifying parts of the input space for which high accuracy can be maintained at
a lower cost. The principle advantage of our method is twofold. First  our approach can be readily
applied to cases where it is desirable to reduce costs of an existing high-cost legacy system. Second 
training top-down models in case of feature costs leads to fundamental combinatorial issues in multi-

stage search over all feature subsets (see Sec. 2). In contrast  we bypass many of these issues by
posing a natural adaptive approximation objective to partition the input space into easy and hard
cases.
In particular  when no legacy system is available  our
method consists of ﬁrst learning a high-accuracy model
that minimizes the empirical loss regardless of costs. The
resulting high prediction-cost model (HPC) can be read-
ily trained using any of the existing methods. For ex-
ample  this could be a large neural network in the cloud
that achieves the state-of-the-art accuracy. Next  we
jointly learn a low-cost gating function as well as a low
prediction-cost (LPC) model so as to adaptively approx-
imate the high-accuracy model by identifying regions of
input space where a low-cost gating and LPC model are
adequate to achieve high-accuracy. In IoT applications 
such low-complexity models can be deployed on the edge
devices to perform gating and prediction. At test-time  for
each input instance  the gating function decides whether
or not the LPC model is adequate for accurate classiﬁca-
tion. Intuitively  “easy” examples can be correctly clas-
siﬁed using only an LPC model while “hard” examples
require HPC model. By identifying which of the input
instances can be classiﬁed accurately with LPCs we by-
pass the utilization of HPC model  thus reducing average
prediction cost. The upper part of Figure 1 is a schematic
of our approach  where x is feature vector and y is the
predicted label; we aim to learn g and an LPC model to
adaptively approximate the HPC. The key observation as
depicted in the lower ﬁgure is that the probability of cor-
rect classiﬁcation given x for a HPC model is in general
a highly complex function with higher values than that of
a LPC model. Yet there exists regions of the input space
where the LPC has competitive accuracy (as shown to the
right of the gating threshold). Sending examples in such
regions (according to the gating function) to the LPC re-
sults in no loss of prediction accuracy while reducing pre-
diction costs.
The problem would be simpler if our task were to pri-
marily partition the input space into regions where LPC
models would sufﬁce. The difﬁculty is that we must also
learn a low-cost gating function capable of identifying in-
put instances for which LPC sufﬁces. Since both prediction and gating account for cost  we favor
design strategies that lead to shared features and decision architectures between the gating function
and the LPC model. We pose the problem as a discriminative empirical risk minimization problem
that jointly optimizes for gating and prediction models in terms of a joint margin-based objective
function. The resulting objective is separately convex in gating and prediction functions. We propose
an alternating minimization scheme that is guaranteed to converge since with appropriate choice of
loss-functions (for instance  logistic loss)  each optimization step amounts to a probabilistic approx-
imation/projection (I-projection/M-projection) onto a probability space. While our method can be
recursively applied in multiple stages to successively approximate the adaptive system obtained in
the previous stage  thereby reﬁning accuracy-cost trade-off  we observe that on benchmark datasets
even a single stage of our method outperforms state-of-art in accuracy-cost performance.

Figure 1: Upper: single stage schematic
of our approach. We learn low-cost gating
g and a LPC model to adaptively approx-
imate a HPC model. Lower: Key insight
for adaptive approximation. x-axis repre-
sents feature space; y-axis represents condi-
tional probability of correct prediction; LPC
can match HPC’s prediction in the input re-
gion corresponding to the right of the gat-
ing threshold but performs poorly otherwise.
Our goal is to learn a low-cost gating func-
tion that attempts to send examples on the
right to LPC and the left to HPC.

2 Related Work

Learning decision rules to minimize error subject to a budget constraint during prediction-time is an
area of active interest[9  17  24  19  22  20  21  13  16]. Pre-trained Models: In one instantiation

2

of these methods it is assumed that there exists a collection of prediction models with amortized
costs [22  19  1] so that a natural ordering of prediction models can be imposed. In other instances 
the feature dimension is assumed to be sufﬁciently low so as to admit an exhaustive enumeration of
all the combinatorial possibilities [20  21]. These methods then learn a policy to choose amongst
the ordered prediction models. In contrast we do not impose any of these restrictions. Top-Down
Methods: For high-dimensional spaces  many existing approaches focus on learning complex adap-
tive decision functions top-down [9  24  13  21]. Conceptually  during training  top-down methods
acquire new features based on their utility value. This requires exploration of partitions of the input
space together with different combinatorial low-cost feature subsets that would result in higher accu-
racy. These methods are based on multi-stage exploration leading to combinatorially hard problems.
Different novel relaxations and greedy heuristics have been developed in this context. Bottom-up
Methods: Our work is somewhat related to [16]  who propose to prune a fully trained random forests
(RF) to reduce costs. Nevertheless  in contrast to our adaptive system  their perspective is to com-
press the original model and utilize the pruned forest as a stand-alone model for test-time prediction.
Furthermore  their method is speciﬁcally tailored to random forests.
Another set of related work includes classiﬁer cascade [5] and decision DAG [3]  both of which aim
to re-weight/re-order a set of pre-trained base learners to reduce prediction budget. Our method 
on the other hand  only requires to pre-train a high-accuracy model and jointly learns the low-cost
models to approximate it; therefore ours can be viewed as complementary to the existing work.
The teacher-student framework [14] is also related to our bottom-up approach; a low-cost student
model learns to approximate the teacher model so as to meet test-time budget. However  the goal
there is to learn a better stand-alone student model.
In contrast  we make use of both the low-
cost (student) and high-accuracy (teacher) model during prediction via a gating function  which
learns the limitation of the low-cost (student) model and consult the high-accuracy (teacher) model if
necessary  thereby avoiding accuracy loss. Our composite system is also related to HME [10]  which
learns the composite system based on max-likelihood estimation of models. A major difference
is that HME does not address budget constraints. A fundamental aspect of budget constraints is
the resulting asymmetry  whereby  we start with an HPC model and sequentially approximate with
LPCs. This asymmetry leads us to propose a bottom-up strategy where the high-accuracy predictor
can be separately estimated and is critical to posing a direct empirical loss minimization problem.

3 Problem Setup

We consider the standard learning scenario of resource constrained prediction with feature costs. A
training sample S = {(x(i)  y(i)) : i = 1  . . .   N} is generated i.i.d. from an unknown distribution 
where x(i) ∈ (cid:60)K is the feature vector with an acquisition cost cα ≥ 0 assigned to each of the features
α = 1  . . .   K and y(i) is the label for the ith example. In the case of multi-class classiﬁcation y ∈
{1  . . .   M}  where M is the number of classes. Let us consider a single stage of our training method
in order to formalize our setup. The model  f0  is a high prediction-cost (HPC) model  which is either
a priori known  or which we train to high-accuracy regardless of cost considerations. We would like
to learn an alternative low prediction-cost (LPC) model f1. Given an example x  at test-time  we
have the option of selecting which model  f0 or f1  to utilize to make a prediction. The accuracy of
a prediction model fz is modeled by a loss function (cid:96)(fz(x)  y)  z ∈ {0  1}. We exclusively employ
the logistic loss function in binary classiﬁcation: (cid:96)(fz(x)  y) = log(1 + exp(−yfz(x))  although
our framework allows other loss models. For a given x  we assume that once it pays the cost to
acquire a feature  its value can be efﬁciently cached; its subsequent use does not incur additional
cost. Thus  the cost of utilizing a particular prediction model  denoted by c(fz  x)  is computed as
the sum of the acquisition cost of unique features required by fz.
Oracle Gating: Consider a general gating likelihood function q(z|x) with z ∈ {0  1}  that outputs
the likelihood of sending the input x to a prediction model  fz. The overall empirical loss is:

ESnEq(z|x)[(cid:96)(fz(x)  y)] = ESn[(cid:96)(f0(x)  y)] + ESn

3

(cid:2)q(1|x) ((cid:96)(f1(x)  y) − (cid:96)(f0(x)  y))(cid:3)
(cid:125)

(cid:123)(cid:122)

(cid:124)

Excess Loss

The ﬁrst term only depends on f0  and from our perspective a constant. Similar to average loss we
can write the average cost as (assuming gating cost is negligible for now):

(cid:125)
ESnEq(z|x)[c(fz  x)] = ESn [c(f0  x)] − ESn [q(1|x) (c(f0  x) − c(f1  x))

(cid:123)(cid:122)

(cid:124)

] 

Cost Reduction

where the ﬁrst term is again constant. We can characterize the optimal gating function (see [19])
that minimizes the overall average loss subject to average cost constraint:

Excess loss

(cid:96)(f1  x) − (cid:96)(f0  x)

Cost reduction

(c(f0  x) − c(f1  x))

η

(cid:125)(cid:124)

(cid:123)

(cid:122)

(cid:125)(cid:124)

(cid:122)

(cid:123)

q(1|x)=0

><

q(1|x)=1

DKL(q(·|x)(cid:107)g(x)) =(cid:80)

for a suitable choice η ∈ R. This characterization encodes the important principle that if the marginal
cost reduction is smaller than the excess loss  we opt for the HPC model. Nevertheless  this charac-
terization is generally infeasible. Note that the LHS depends on knowing how well HPC performs
on the input instance. Since this information is unavailable  this target can be unreachable with
low-cost gating.
Gating Approximation: Rather than directly enforcing a low-cost structure on q  we decouple
the constraint and introduce a parameterized family of gating functions g ∈ G that attempts to
mimic (or approximate) q. To ensure such approximation  we can minimize some distance mea-
sure D(q(·|x)  g(x)). A natural choice for an approximation metric is the Kullback-Leibler (KL)
divergence although other choices are possible. The KL divergence between q and g is given by
z q(z|x) log(q(z|x)/σ(sgn(0.5 − z)g(x)))  where σ(s) = 1/(1 + e−s) is
the sigmoid function. Besides KL divergence  we have also proposed another symmetrized metric
ﬁtting g directly to the log odds ratio of q. See Suppl. Material for details.
Budget Constraint: With the gating function g  the cost of predicting x depends on whether the
example is sent to f0 or f1. Let c(f0  g  x) denote the feature cost of passing x to f0 through g.
As discussed  this is equal to the sum of the acquisition cost of unique features required by f0 and
g for x. Similarly c(f1  g  x) denotes the cost if x is sent to f1 through g. In many cases the cost
c(fz  g  x) is independent of the example x and depends primarily on the model being used. This
is true for linear models where each x must be processed through the same collection of features.
For these cases c(fz  g  x) (cid:44) c(fz  g). The total budget simpliﬁes to: ESn [q(0|x)]c(f0  g) + (1 −
ESn [q(0|x)])c(f1  g) = c(f1  g) + ESn [q(0|x)](c(f0  g) − c(f1  g)). The budget thus depends on 3
quantities: ESn [q(0|x)]  c(f1  g) and c(f0  g). Often f0 is a high-cost model that requires most  if
not all  of features so c(f0  g) can be considered a large constant.
Thus  to meet the budget constraint  we would like to have (a) low-cost g and f1 (small c(f1  g));
and (b) small fraction of examples being sent to the high-accuracy model (small ESn [q(0|x)]). We
can therefore split the budget constraint into two separate objectives: (a) ensure low-cost through
α cα(cid:107)Vα + Wα(cid:107)0  where γ is a tradeoff parameter and the indicator vari-
ables Vα  Wα ∈ {0  1} denote whether or not the feature α is required by f1 and g  respectively.
Depending on the model parameterization  we can approximate Ω(f1  g) using a group-sparse norm
or in a stage-wise manner as we will see in Algorithms 1 and 2. (b) Ensure only Pfull fraction of
examples are sent to f0 via the constraint ESn[q(0|x)] ≤ Pfull.
Putting Together: We are now ready to pose our general optimization problem:
(cid:122)

penalty Ω(f1  g) = γ(cid:80)

(cid:122) (cid:125)(cid:124) (cid:123)

Gating Approx

(cid:125)(cid:124)

(cid:125)(cid:124)
f1∈F  g∈G q
subject to: ESn [q(0|x)] ≤ Pfull. (F raction to f0)

[q(z|x)(cid:96)(fz(x)  y)] +

ESn

Losses

D(q(·|x)  g(x)) +

(cid:122)
(cid:88)

Ω(f1  g)

(OPT)

(cid:123)

(cid:123)

min

Costs

z

The objective function penalizes excess loss and ensures through the second term that this excess
loss can be enforced through admissible gating functions. The third term penalizes the feature cost
usage of f1 and g. The budget constraint limits the fraction of examples sent to the costly model f0.
Remark 1: Directly parameterizing q leads to non-convexity. Average loss is q-weighted sum
of losses from HPC and LPC; while the space of probability distributions is convex  a ﬁnite-
dimensional parameterization is generally non-convex (e.g. sigmoid). What we have done is to
keep q in non-parametric form to avoid non-convexity and only parameterize g  connecting both via

4

a KL term. Thus  (OPT) is now convex with respect to the f1 and g for a ﬁxed q. It is again convex
in q for a ﬁxed f1 and g. Otherwise it would introduce non-convexity as in prior work. For instance 
in [5] a non-convex problem is solved in each inner loop iteration (line 7 of their Algorithm 1).
Remark 2: We presented the case for a single stage approximation system. However  it is straightfor-
ward to recursively continue this process. We can then view the composite system f0 (cid:44) (g  f1  f0)
as a black-box predictor and train a new pair of gating and prediction models to approximate the
composite system.
Remark 3: To limit the scope of our paper  we focus on reducing feature acquisition cost during
prediction as it is a more challenging (combinatorial) problem. However  other prediction-time costs
such as computation cost can be encoded in the choice of functional classes F and G in (OPT).
Surrogate Upper Bound of Composite System: We can get better insight for the ﬁrst two terms
of the objective in (OPT) if we view z ∈ {0  1} as a latent variable and consider the composite
z Pr(z|x; g) Pr(y|x  fz). A standard application of Jensen’s inequality reveals
that  − log(Pr(y|x)) ≤ Eq(z|x)(cid:96)(fz(x)  y) + DKL(q(z|x)(cid:107) Pr(z|x; g)). Therefore  the conditional-
entropy of the composite system is bounded by the expected value of our loss function (we overload
notation and represent random-variables in lower-case format):

system Pr(y|x) =(cid:80)

H(y | x) (cid:44) E[− log(Pr(y|x))] ≤ Ex×y[Eq(z|x)(cid:96)(fz(x)  y) + DKL(q(z|x)(cid:107) Pr(z|x; g))].

This implies that the ﬁrst two terms of our objective attempt to bound the loss of the composite
system; the third term in the objective together with the constraint serve to enforce budget limits on
the composite system.
Group Sparsity: Since the cost for feature re-use is zero we encourage feature re-use among gat-
ing and prediction models. So the fundamental question here is: How to choose a common  sparse
(low-cost) subset of features on which both g and f1 operate  such that g can effective gate exam-
ples between f1 and f0 for accurate prediction? This is a hard combinatorial problem. The main
contribution of our paper is to address it using the general optimization framework of (OPT).

4 Algorithms

Algorithm 1 ADAPT-LIN

Solve (OPT1) for q given g  f1.
Solve (OPT2) for g  f1 given q.

Input: (x(i)  y(i))  Pfull  γ
Train f0. Initialize g  f1.
repeat

To be concrete  we instantiate our general framework (OPT) into two algorithms via different param-
eterizations of g  f1: ADAPT-LIN for the linear class and ADAPT-GBRT for the non-parametric class.
Both of them use the KL-divergence as distance
measure. We also provide a third algorithm
ADAPT-LSTSQ that uses the symmetrized dis-
tance in the Suppl. Material. All of the al-
gorithms perform alternating minimization of
(OPT) over q  g  f1. Note that convergence
of alternating minimization follows as in [8].
Common to all of our algorithms  we use two
parameters to control cost: Pfull and γ. In prac-
tice they are swept to generate various cost-
accuracy tradeoffs and we choose the best one
satisfying the budget B using validation data.
ADAPT-LIN: Let g(x) = gT x and f1(x) =
1 x be linear classiﬁers. A feature is used if the
f T
corresponding component is non-zero: Vα = 1
if f1 α (cid:54)= 0  and Wα = 1 if gα (cid:54)= 0. The mini-
(cid:80)N
mization for q solves the following problem:
(cid:80)N
i=1 [(1 − qi)Ai + qiBi − H(qi)]
i=1 qi ≤ Pfull 

Find f t
1.
f1 = f1 + f t
For each feature α used  set uα = 0.
Find gt using CART to minimize (2).
g = g + gt.
For each feature α used  set uα = 0.

Algorithm 2 ADAPT-GBRT
Input: (x(i)  y(i))  Pfull  γ
Train f0. Initialize g  f1.
repeat

Solve (OPT1) for q given g  f1.
for t = 1 to T do

1 using CART to minimize (1).

min

q
s.t.

1
N

1
N

until convergence

(OPT1)
where we have used shorthand notations qi =
q(z = 0|x(i))  H(qi) = −qi log(qi) − (1 −
qi) log(1 − qi)  Ai = log(1 + e−y(i)f T
) +

1 x(i)

end for

until convergence

5

) and Bi = − log p(y(i)|z(i) = 0; f0) + log(1 + e−gT x(i)

log(1 + egT x(i)
). This optimization has
a closed form solution: qi = 1/(1 + eBi−Ai+β) for some non-negative constant β such that the
constraint is satisﬁed. This optimization is also known as I-Projection in information geometry
because of the entropy term [8]. Having optimized q  we hold it constant and minimize with respect
α cα(cid:107)Vα +
Wα(cid:107)0 into a L2 1 norm for group sparsity and a tradeoff parameter γ to make sure the feature budget
is satisﬁed. Once we solve for g  f1  we can hold them constant and minimize with respect to q again.
ADAPT-LIN is summarized in Algorithm 1.

to g  f1 by solving the problem (OPT2)  where we have relaxed the non-convex cost(cid:80)
(cid:113)
(cid:88)

N(cid:88)

(cid:17)

(cid:16)

(cid:104)

(cid:105)

(1 − qi)

log(1 + e−y(i)f T

1 x(i)

) + log(1 + egT x(i)

)

+ qi log(1 + e−gT x(i)

)

+ γ

min
g f1

1
N

i=1

boosted trees [7]: g(x) = (cid:80)T

t=1 gt(x) and f1(x) = (cid:80)T

ADAPT-GBRT: We can also consider the non-parametric family of classiﬁers such as gradient
1 are limited-
depth regression trees. Since the trees are limited to low depth  we assume that the feature utility
of each tree is example-independent: Vα t(x) (cid:117) Vα t  Wα t(x) (cid:117) Wα t ∀x. Vα t = 1 if fea-
ture α appears in f t
1  otherwise Vα t = 0  similarly for Wα t. The optimization over q still solves
(OPT1). We modify Ai = log(1 + e−y(i)f1(x(i))) + log(1 + eg(x(i))) and Bi = − log p(y(i)|z(i) =
0; f0) + log(1 + e−g(x(i))). Next  to minimize over g  f1  denote loss:

1(x)  where gt and f t

t=1 f t

α + f 2
g2

1 α.

α

(OPT2)

(cid:34)
(1 − qi) ·(cid:16)

N(cid:88)

i=1

(cid:96)(f1  g) =

1
N

log(1 + e−y(i)f1(x(i))) + log(1 + eg(x(i)))

+ qi log(1 + e−g(x(i)))

 

(cid:17)

which is essentially the same as the ﬁrst part of the objective in (OPT2). Thus  we need to minimize
(cid:96)(f1  g) + Ω(f1  g) with respect to f1 and g. Since both f1 and g are gradient boosted trees  we
naturally adopt a stage-wise approximation for the objective. In particular  we deﬁne an impurity
function which on the one hand approximates the negative gradient of (cid:96)(f1  g) with the squared
loss  and on the other hand penalizes the initial acquisition of features by their cost cα. To capture
the initial acquisition penalty  we let uα ∈ {0  1} indicates if feature α has already been used in
previous trees (uα = 0)  or not (uα = 1). uα is updated after adding each tree. Thus we arrive at
the following impurity for f1 and g  respectively:

(cid:35)

(1)

(2)

N(cid:88)
N(cid:88)

i=1

i=1

1
2

1
2

(− ∂(cid:96)(f1  g)
∂f1(x(i))

− f t

1(x(i)))2 + γ

uαcαVα t 

(− ∂(cid:96)(f1  g)
∂g(x(i))

− gt(x(i)))2 + γ

uαcαWα t.

(cid:88)
(cid:88)

α

α

Minimizing such impurity functions balances the need to minimize loss and re-using the already
acquired features. Classiﬁcation and Regression Tree (CART) [2] can be used to construct deci-
sion trees with such an impurity function. ADAPT-GBRT is summarized in Algorithm 2. Note
that a similar impurity is used in GREEDYMISER [24]. Interestingly  if Pfull is set to 0  all the ex-
amples are forced to f1  then ADAPT-GBRT exactly recovers the GREEDYMISER. In this sense 
GREEDYMISER is a special case of our algorithm. As we will see in the next section  thanks to the
bottom-up approach  ADAPT-GBRT beneﬁts from high-accuracy initialization and is able to perform
accuracy-cost tradeoff in accuracy levels beyond what is possible for GREEDYMISER.

5 Experiments

BASELINE ALGORITHMS: We consider the following simple L1 baseline approach for learning
f1 and g: ﬁrst perform a L1-regularized logistic regression on all data to identify a relevant  sparse
subset of features; then learn f1 using training data restricted to the identiﬁed feature(s); ﬁnally 
learn g based on the correctness of f1 predictions as pseudo labels (i.e. assign pseudo label 1 to
example x if f1(x) agrees with the true label y and 0 otherwise). We also compare with two state-
of-the-art feature-budgeted algorithms: GREEDYMISER[24] - a top-down method that builds out an

6

ensemble of gradient boosted trees with feature cost budget; and BUDGETPRUNE[16] - a bottom-up
method that prunes a random forest with feature cost budget. A number of other methods such as
ASTC [13] and CSTC [23] are omitted as they have been shown to under-perform GREEDYMISER
on the same set of datasets [15]. Detailed experiment setups can be found in the Suppl. Material.
We ﬁrst visualize/verify the adaptive approximation ability of ADAPT-LIN and ADAPT-GBRT on the
Synthetic-1 dataset without feature costs. Next  we illustrate the key difference between ADAPT-LIN
and the L1 baseline approach on the Synthetic-2 as well as the Letters datasets. Finally  we compare
ADAPT-GBRT with state-of-the-art methods on several resource constraint benchmark datasets.

(a) Input Data

(b) Lin Initialization

(c) Lin after 10 iterations

(d) RBF Contour

(e) Gbrt Initialization

(f) Gbrt after 10 iterations

Figure 2: Synthetic-1 experiment without feature cost. (a): input data. (d): decision contour of
RBF-SVM as f0. (b) and (c): decision boundaries of linear g and f1 at initialization and after 10
iterations of ADAPT-LIN. (e) and (f): decision boundaries of boosted tree g and f1 at initialization
and after 10 iterations of ADAPT-GBRT. Examples in the beige areas are sent to f0 by the g.

POWER OF ADAPTATION: We construct a 2D binary classiﬁcation dataset (Synthetic-1)
as shown in (a) of Figure 2. We learn an RBF-SVM as the high-accuracy classiﬁer f0
To better visualize the adaptive approximation process in 2D  we turn off the
as in (d).
feature costs (i.e.
set Ω(f1  g) to 0 in (OPT)) and run ADAPT-LIN and ADAPT-GBRT.
The initializations of g and f1 in (b) results in wrong pre-
dictions for many red points in the blue region. After 10
iterations of ADAPT-LIN  f1 adapts much better to the lo-
cal region assigned by g while g sends about 60% (Pfull)
of examples to f0. Similarly  the initialization in (e) re-
sults in wrong predictions in the blue region. ADAPT-
GBRT is able to identify the ambiguous region in the cen-
ter and send those examples to f0 via g. Both of our algo-
rithms maintain the same level of prediction accuracy as
f0 yet are able to classify large fractions of examples via
much simpler models.
POWER OF JOINT OPTIMIZATION: We return to the
problem of prediction under feature budget constrains.
We illustrate why a simple L1 baseline approach for
learning f1 and g would not work using a 2D dataset
(Synthetic-2) as shown in Figure 3 (left). The data points
are distributed in four clusters  with black triangles and
red circles representing two class labels. Let both feature
1 and 2 carry unit acquisition cost. A complex classiﬁer
f0 that acquires both features can achieve full accuracy
at the cost of 2. In our synthetic example  clusters 1 and 2 are given more data points so that the
L1-regularized logistic regression would produce the vertical red dashed line  separating cluster 1
from the others. So feature 1 is acquired for both g and f1. The best such an adaptive system can

Figure 3: A 2-D synthetic example for
adaptive feature acquisition. On the left:
data distributed in four clusters. The
two features correspond to x and y co-
ordinates  respectively. On the right:
accuracy-cost tradeoff curves. Our al-
gorithm can recover the optimal adap-
tive system whereas a L1-based ap-
proach cannot.

7

do is to send cluster 1 to f1 and the other three clusters to the complex classiﬁer f0  incurring an
average cost of 1.75  which is sub-optimal. ADAPT-LIN  on the other hand  optimizing between
q  g  f1 in an alternating manner  is able to recover the horizontal lines in Figure 3 (left) for g and
f1. g sends the ﬁrst two clusters to the full classiﬁer and the last two clusters to f1. f1 correctly
classiﬁes clusters 3 and 4. So all of the examples are correctly classiﬁed by the adaptive system; yet
only feature 2 needs to be acquired for cluster 3 and 4 so the overall average feature cost is 1.5  as
shown by the solid curve in the accuracy-cost tradeoff plot on the right of Figure 3. This example
shows that the L1 baseline approach is sub-optimal as it doesnot optimize the selection of feature
subsets jointly for g and f1.

(a) MiniBooNE

(b) Forest Covertype

(c) Yahoo! Rank

(d) CIFAR10

Figure 4: Comparison of ADAPT-GBRT against GREEDYMISER and BUDGETPRUNE on four
benchmark datasets. RF is used as f0 for ADAPT-GBRT in (a-c) while an RBF-SVM is used as
f0 in (d). ADAPT-GBRT achieves better accuracy-cost tradeoff than other methods. The gap is sig-
niﬁcant in (b) (c) and (d). Note the accuracy of GREEDYMISER in (b) never exceeds 0.86 and its
precision in (c) slowly rises to 0.138 at cost of 658. We limit the cost range for a clearer comparison.

#Validation

4000
19510
15688
8468
146769

16
50
54
400
519

Feature Costs

Dataset
Letters

MiniBooNE

Forest

CIFAR10
Yahoo!

#Test
4000
65031
58101
10000
184968

#Train
12000
45523
36603
19761
141397

Uniform
Uniform
Uniform
Uniform
CPU units

Table 1: Dataset Statistics
#Features

REAL DATASETS: We test various aspects
of our algorithms and compare with state-
of-the-art feature-budgeted algorithms on ﬁve
real world benchmark datasets: Letters  Mini-
BooNE Particle Identiﬁcation  Forest Cover-
type datasets from the UCI repository [6] 
CIFAR-10 [11] and Yahoo!
Learning to
Rank[4]. Yahoo! is a ranking dataset where each example is associated with features of a query-
document pair together with the relevance rank of the document to the query. There are 519 such
features in total; each is associated with an acquisition cost in the set {1 5 20 50 100 150 200} 
which represents the units of CPU time required to extract the feature and is provided by a Yahoo!
employee. The labels are binarized into relevant or not relevant. The task is to learn a model that
takes a new query and its associated documents and produce a relevance ranking so that the relevant
documents come on top  and to do this using as little feature cost as possible. The performance
metric is Average Precision @ 5 following [16]. The other datasets have unknown feature costs so
we assign costs to be 1 for all features; the aim is to show ADAPT-GBRT successfully selects sparse
subset of “usefull” features for f1 and g. We summarize the statistics of these datasets in Table 1.
Next  we highlight the key insights from the real dataset experiments.
Generality of Approximation: Our framework allows approximation of powerful classiﬁers such
as RBF-SVM and Random Forests as shown in Figure 5 as red and black curves  respectively.
In particular  ADAPT-GBRT can well maintain high accuracy while reducing cost. This is a key
advantage for our algorithms because we can choose to approximate the f0 that achieves the best
accuracy. ADAPT-LIN Vs L1: Figure 5 shows that ADAPT-LIN outperforms L1 baseline method
on real dataset as well. Again  this conﬁrms the intuition we have in the Synthetic-2 example as
ADAPT-LIN is able to iteratively select the common subset of features jointly for g and f1. ADAPT-
GBRT Vs ADAPT-LIN: ADAPT-GBRT leads to signiﬁcantly better performance than ADAPT-LIN in
approximating both RBF-SVM and RF as shown in Figure 5. This is expected as the non-parametric
non-linear classiﬁers are much more powerful than linear ones.
ADAPT-GBRT Vs BUDGETPRUNE: Both are bottom-up approaches that beneﬁt from good initial-
izations. In (a)  (b) and (c) of Figure 4 we let f0 in ADAPT-GBRT be the same RF that BUDGET-
PRUNE starts with. ADAPT-GBRT is able to maintain high accuracy longer as the budget decreases.

8

1520253035404550Average Feature Cost0.9200.9250.9300.935Test AccuracyAdapt_GbrtGreedyMiser(Xu et al. 2012)BudgetPrune (Nan et al. 2016)1015202530Average Feature Cost0.840.860.880.900.92Test Accuracy406080100120140160180Average Feature Cost0.1280.1300.1320.1340.1360.138Average Precision@5050100150200250300350400Average Feature Cost0.650.700.750.80Test AccuracyThus  ADAPT-GBRT improves state-of-the-art bottom-up method. Notice in (c) of Figure 4 around
the cost of 100  BUDGETPRUNE has a spike in precision. We believe this is because the initial
pruning improved the generalization performance of RF.
But in the cost region of 40-80  ADAPT-GBRT maintains much better accuracy than BUDGET-
PRUNE. Furthermore  ADAPT-GBRT has the freedom to approximate the best f0 given the problem.
So in (d) of Figure 4 we see that with f0 being RBF-SVM  ADAPT-GBRT can achieve much higher
accuracy than BUDGETPRUNE.
ADAPT-GBRT Vs GREEDYMISER: ADAPT-GBRT out-
performs GREEDYMISER on all the datasets. The gaps in
Figure 5  (b) (c) and (d) of Figure 4 are especially signif-
icant.
Signiﬁcant Cost Reduction: Without sacriﬁcing top ac-
curacies (within 1%)  ADAPT-GBRT reduces average fea-
ture costs during test-time by around 63%  32%  58% 
12% and 31% on MiniBooNE  Forest  Yahoo  Cifar10
and Letters datasets  respectively.

6 Conclusions

Figure 5: Compare the L1 baseline ap-
proach  ADAPT-LIN and ADAPT-GBRT
based on RBF-SVM and RF as f0’s on
the Letters dataset.

We presented an adaptive approximation approach to ac-
count for prediction costs that arise in various applica-
tions. At test-time our method uses a gating function to
identify a prediction model among a collection of models
that is adapted to the input. The overall goal is to reduce
costs without sacriﬁcing accuracy. We learn gating and
prediction models by means of a bottom-up strategy that trains low prediction-cost models to ap-
proximate high prediction-cost models in regions where low-cost models sufﬁce. On a number of
benchmark datasets our method leads to an average of 40% cost reduction without sacriﬁcing test
accuracy (within 1%). It outperforms state-of-the-art top-down and bottom-up budgeted learning
algorithms  with a signiﬁcant margin in several cases.

Acknowledgments

Feng Nan would like to thank Dr Ofer Dekel for ideas and discussions on resource constrained
machine learning during an internship in Microsoft Research in summer 2016. Familiarity and
intuition gained during the internship contributed to the motivation and formulation in this paper.
We also thank Dr Joseph Wang and Tolga Bolukbasi for discussions and helps in experiments. This
material is based upon work supported in part by NSF Grants CCF: 1320566  CNS: 1330008  CCF:
1527618  DHS 2013-ST-061-ED0001  NGA Grant HM1582-09-1-0037 and ONR Grant N00014-
13-C-0288.

References
[1] Tolga Bolukbasi  Joseph Wang  Ofer Dekel  and Venkatesh Saligrama. Adaptive neural net-
works for efﬁcient inference. In Proceedings of the 34th International Conference on Machine
Learning  volume 70 of Proceedings of Machine Learning Research  pages 527–536  Interna-
tional Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[2] Leo Breiman  Jerome Friedman  Charles J Stone  and Richard A Olshen. Classiﬁcation and

regression trees. CRC press  1984.

[3] Róbert Busa-Fekete  Djalel Benbouzid  and Balázs Kégl. Fast classiﬁcation using sparse deci-
sion dags. In Proceedings of the 29th International Conference on Machine Learning  ICML
2012  Edinburgh  Scotland  UK  June 26 - July 1  2012  2012.

[4] O Chapelle  Y Chang  and T Liu  editors. Proceedings of the Yahoo! Learning to Rank Chal-

lenge  held at ICML 2010  Haifa  Israel  June 25  2010  2011.

9

111213141516Average Feature Cost0.880.900.920.940.960.98Test AccuracyAdapt_Gbrt RFAdapt_Lin RFL1 RFAdapt_Gbrt RBFAdapt_Lin RBFL1 RBFGreedyMiser(Xu et al 2012)[5] Minmin Chen  Zhixiang Eddie Xu  Kilian Q. Weinberger  Olivier Chapelle  and Dor Kedem.
In Proceedings of the Fifteenth
Classiﬁer cascade for minimizing feature evaluation cost.
International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2012  La Palma 
Canary Islands  April 21-23  2012  pages 218–226  2012.

[6] A. Frank and A. Asuncion. UCI machine learning repository  2010.

[7] Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals

of Statistics  29:1189–1232  2000.

[8] Kuzman Ganchev  Ben Taskar  and Jo ao Gama. Expectation maximization and posterior
constraints. In J. C. Platt  D. Koller  Y. Singer  and S. T. Roweis  editors  Advances in Neural
Information Processing Systems 20  pages 569–576. Curran Associates  Inc.  2008.

[9] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In Advances in Neural

Information Processing Systems (NIPS 2011)  2011.

[10] Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the em algorithm.

Neural Comput.  6(2):181–214  March 1994.

[11] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis 

2009.

[12] Ashish Kumar  Saurabh Goyal  and Manik Varma. Resource-efﬁcient machine learning in 2
KB RAM for the internet of things. In Proceedings of the 34th International Conference on
Machine Learning  volume 70 of Proceedings of Machine Learning Research  pages 1935–
1944  International Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[13] M Kusner  W Chen  Q Zhou  E Zhixiang  K Weinberger  and Y Chen. Feature-cost sensitive
learning with submodular trees of classiﬁers. In AAAI Conference on Artiﬁcial Intelligence 
2014.

[14] D. Lopez-Paz  B. Schölkopf  L. Bottou  and V. Vapnik. Unifying distillation and privileged

information. In International Conference on Learning Representations  2016.

[15] Feng Nan  Joseph Wang  and Venkatesh Saligrama. Feature-budgeted random forest. In David
Blei and Francis Bach  editors  Proceedings of the 32nd International Conference on Machine
Learning (ICML-15)  pages 1983–1991. JMLR Workshop and Conference Proceedings  2015.

[16] Feng Nan  Joseph Wang  and Venkatesh Saligrama. Pruning random forests for prediction on a
budget. In Advances in Neural Information Processing Systems 29  pages 2334–2342. Curran
Associates  Inc.  2016.

[17] Feng Nan  Joseph Wang  Kirill Trapeznikov  and Venkatesh Saligrama. Fast margin-based
cost-sensitive classiﬁcation. In IEEE International Conference on Acoustics  Speech and Sig-
nal Processing  ICASSP 2014  Florence  Italy  May 4-9  2014  2014.

[18] Daniel P. Robinson and Suchi Saria. Trading-off cost of deployment versus accuracy in learn-
ing predictive models. In Proceedings of the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence  IJCAI’16  pages 1974–1982. AAAI Press  2016.

[19] K Trapeznikov and V Saligrama. Supervised sequential classiﬁcation under budget constraints.

In International Conference on Artiﬁcial Intelligence and Statistics  pages 581–589  2013.

[20] Joseph Wang  Tolga Bolukbasi  Kirill Trapeznikov  and Venkatesh Saligrama. Model Selection

by Linear Programming  pages 647–662. Springer International Publishing  Cham  2014.

[21] Joseph Wang  Kirill Trapeznikov  and Venkatesh Saligrama. Efﬁcient learning by directed
acyclic graph for resource constrained prediction. In Advances in Neural Information Process-
ing Systems 28  pages 2143–2151. Curran Associates  Inc.  2015.

[22] D. Weiss  B. Sapp  and B. Taskar. Dynamic structured model selection. In 2013 IEEE Inter-

national Conference on Computer Vision  pages 2656–2663  Dec 2013.

10

[23] Z Xu  M Kusner  M Chen  and K. Q Weinberger. Cost-sensitive tree of classiﬁers. In Proceed-

ings of the 30th International Conference on Machine Learning  2013.

[24] Zhixiang Eddie Xu  Kilian Q. Weinberger  and Olivier Chapelle. The greedy miser: Learning
In Proceedings of the 29th International Conference on Machine

under test-time budgets.
Learning  ICML  2012.

11

,Scott Wisdom
Thomas Powers
John Hershey
Jonathan Le Roux
Les Atlas
Feng Nan
Venkatesh Saligrama
Zhihui Zhu
Xiao Li
Kai Liu
Qiuwei Li