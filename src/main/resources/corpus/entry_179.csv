2009,A Rate Distortion Approach for Semi-Supervised Conditional Random Fields,We propose a novel information theoretic approach for semi-supervised learning of conditional random fields. Our approach defines a training objective that combines the conditional likelihood on labeled data and the mutual information on unlabeled data. Different from previous minimum conditional entropy semi-supervised discriminative learning methods  our approach can be naturally cast into the rate distortion theory framework in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations. Our experimental results show that the rate distortion approach outperforms standard $l_2$ regularization and minimum conditional entropy regularization on both multi-class classification and sequence labeling problems.,A Rate Distortion Approach for Semi-Supervised

Conditional Random Fields

Yang Wang†∗

Gholamreza Haffari†∗
†School of Computing Science

Simon Fraser University

Burnaby  BC V5A 1S6  Canada

{ywang12 ghaffar1 mori}@cs.sfu.ca

Shaojun Wang‡

Greg Mori†

‡Kno.e.sis Center

Wright State University
Dayton  OH 45435  USA
shaojun.wang@wright.edu

Abstract

We propose a novel information theoretic approach for semi-supervised learning
of conditional random ﬁelds that deﬁnes a training objective to combine the con-
ditional likelihood on labeled data and the mutual information on unlabeled data.
In contrast to previous minimum conditional entropy semi-supervised discrimi-
native learning methods  our approach is grounded on a more solid foundation 
the rate distortion theory in information theory. We analyze the tractability of the
framework for structured prediction and present a convergent variational train-
ing algorithm to defy the combinatorial explosion of terms in the sum over label
conﬁgurations. Our experimental results show the rate distortion approach outper-
forms standard l2 regularization  minimum conditional entropy regularization as
well as maximum conditional entropy regularization on both multi-class classiﬁ-
cation and sequence labeling problems.

Introduction

1
In most real-world machine learning problems (e.g.  for text  image  audio  biological sequence
data)  unannotated data is abundant and can be collected at almost no cost. However  supervised
machine learning techniques require large quantities of data be manually labeled so that automatic
learning algorithms can build sophisticated models. Unfortunately  manual annotation of a large
quantity of data is both expensive and time-consuming. The challenge is to ﬁnd ways to exploit the
large quantity of unlabeled data and turn it into a resource that can improve the performance of su-
pervised machine learning algorithms. Meeting this challenge requires research at the cutting edge
of automatic learning techniques  useful in many ﬁelds such as language and speech technology  im-
age processing and computer vision  robot control and bioinformatics. A surge of semi-supervised
learning research activities has occurred in recent years to devise various effective semi-supervised
training schemes. Most of these semi-supervised learning algorithms are applicable only to multi-
class classiﬁcation problems [1  10  32]  with very few exceptions that develop discriminative mod-
els suitable for structured prediction [2  9  16  20  21  22].

In this paper  we propose an information theoretic approach for semi-supervised learning of condi-
tional random ﬁelds (CRFs) [19]  where we use the mutual information between the empirical distri-
bution of unlabeled data and the discriminative model as a data-dependent regularized prior. Grand-
valet and Bengio [15] and Jiao et al. [16] have proposed a similar information theoretic approach that
used the conditional entropy of their discriminative models on unlabeled data as a data-dependent
regularization term to obtain very encouraging results. Minimum entropy approach can be explained
from data-smoothness assumption and is motivated from semi-supervised classiﬁcation  using unla-
beled data to enhance classiﬁcation; however  its degeneracy is even more problematic and arguable
by noting minimum entropy 0 can be achieved by putting all mass on one label and zeros for the
rest of labels. As far as we know  there is no formal principled explanation for the validity of this
minimum conditional entropy approach. Instead  our approach can be naturally cast into the rate

∗These authors contributed equally to this work.

1

distortion theory framework which is well-known in information theory [14]. The closest work to
ours is the one by Corduneanu et al. [11  12  13  28]. Both works are discriminative models and
do indeed use mutual information concepts. There are two major distinctions between our work
and theirs. First  their approach is essentially motivated from semi-supervised classiﬁcation point
of view and formulated as a communication game  while our approach is based on a completely
different motivation  semi-supervised clustering that uses labeled data to enhance clustering and is
formulated as a data compression scheme  thus leads to a formulation distinctive from Corduneanu
et al. Second  their model is non-parametric  whereas ours is parametric. As a result  their model can
be trained by optimizing a convex objective function through a variant of Blahut-Arimoto alternating
minimization algorithm  whereas our model is more complex and the objective function becomes
non-convex. In particular  training a simple chain structured CRF model [19] in our framework turns
out to be intractable even if using Blahut-Arimoto’s type of alternating minimization algorithm. We
develop a convergent variational approach to approximately solve this problem. Another relevant
work is the information bottleneck (IB) method introduced by Tishby et al [30]. IB method is an
information-theoretic framework for extracting relevant components of an input random variable
X  with respect to an output random variable Y . Instead of directly compressing X to its repre-
sentation Y subject to an expected distortion through a parametric probabilistic mapping like our
proposed approach  IB method is performed by ﬁnding a third  compressed  non-parametric and
model-independent representation T of X that is most informative about Y . Formally speaking  the
notion of compression is quantiﬁed by the mutual information between T and X while the informa-
tiveness is quantiﬁed by the mutual information between T and Y . The solutions are characterized
by the bottleneck equations and can be found by a convergent re-estimation method that general-
izes the Blahut-Arimoto algorithm. Finally in contrast to our approach which minimizes both the
negative conditional likelihood on labeled data and the mutual information between the hidden vari-
ables and the observations on unlabeled data for a discriminative model  Oliver and Garg [24] have
proposed maximum mutual information hidden Markov models (MMIHMM) of semi-supervised
training for chain structured graph. The objective is to maximize both the joint likelihood on labeled
data and the mutual information between the hidden variables and the observations on unlabeled data
for a generative model. It is equivalent to minimizing conditional entropy of a generative HMM for
the part of unlabeled data. The maximum mutual information of a generative HMM was originally
proposed by Bahl et al. [4] and popularized in speech recognition community [23]  but it is differ-
ent from Oliver and Garg’s approach in that an individual HMM is learned for each possible class
(e.g.  one HMM for each word string)  and the point-wise mutual information between the choice
of HMM and the observation sequence is maximized. It is equivalent to maximizing the conditional
likelihood of a word string given observation sequence to improve the discrimination across differ-
ent models [18]. Thus in essence  Bahl et al. [4] proposed a discriminative learning algorithm for
generative HMMs of training utterances in speech recognition.

In the following  we ﬁrst motivate our rate distortion approach for semi-supervised CRFs as a data
compression scheme and formulate the semi-supervised learning paradigm as a classic rate distortion
problem. We then analyze the tractability of the framework for structured prediction and present a
convergent variational learning algorithm to defy the combinatorial explosion of terms in the sum
over label conﬁgurations. Finally we demonstrate encouraging results with two real-world problems
to show the effectiveness of the proposed approach: text categorization as a multi-class classiﬁcation
problem and hand-written character recognition as a sequence labeling problem. Similar ideas have
been successfully applied to semi-supervised boosting [31].

2 Rate distortion formulation
Let X be a random variable over data sequences to be labeled  and Y be a random variable
over corresponding label sequences. All components  Yi  of Y are assumed to range over a ﬁ-
nite label alphabet Y. Given a set of labeled examples  Dl = n(x
(N ))o 
(M )o  we would like to build a CRF model
and unlabeled examples  Du = nx
Zθ(x) exp(cid:16)hθ  f (x  y)i(cid:17) over sequential input data x  where θ = (θ1  · · ·   θK )⊤ 
pθ(y|x) = 1
f (x  y) = (f1(x  y)  · · ·   fK(x  y))⊤  and Zθ(x) = Py exp(cid:16)hθ  f (x  y)i(cid:17). Our goal is to learn
such a model from the combined set of labeled and unlabeled examples  Dl ∪ Du. For notational
convenience  we assume that there are no identical examples in Dl and Du.

(N +1)  · · ·   x

(1)  y

(1))  · · ·   (x

(N )  y

2

The standard supervised training procedure for CRFs is based on minimizing the negative log con-
ditional likelihood of the labeled examples in Dl

N

CL(θ) = −

log pθ(y

(i)|x

(i)) + λU (θ)

(1)

where U (θ) can be any standard regularizer on θ  e.g. U (θ) = kθk2/2 and λ is a parameter that
controls the inﬂuence of U (θ). Regularization can alleviate over-ﬁtting on rare features and avoid
degeneracy in the case of correlated features.
Obviously  Eq. (1) ignores the unlabeled examples in Du. To make full use of the available training
data  Grandvalet and Bengio [15] and Jiao et al. [16] proposed a semi-supervised learning algo-
rithm that exploits a form of minimum conditional entropy regularization on the unlabeled data.
Speciﬁcally  they proposed to minimize the following objective

RLminCE(θ) = −

log pθ(y

(i)|x

(i)) + λU (θ) − γ

pθ(y|x

(j)) log pθ(y|x

(j))

(2)

N

M

Xi=1

Xi=1

Xj=N +1Xy

where the ﬁrst term is the negative log conditional likelihood of the labeled data  and the third term
is the conditional entropy of the CRF model on the unlabeled data. The tradeoff parameters λ and γ
control the inﬂuences of U (θ) and the unlabeled data  respectively.
This is equivalent to minimizing the following objective (with different values of λ and γ)

(3)

˜pl(x y)

RLminCE(θ) = D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ) + γ Xx∈Du
˜pu(x)H“pθ(y|x)”
where D(cid:16)˜pl(x  y)  ˜pl(x)pθ(y|x)(cid:17) = P(x y)∈Dl ˜pl(x  y) log
˜pl(x)pθ(y|x)   H(cid:16)pθ(y|x)(cid:17) =
Py pθ(y|x) log pθ(y|x). Here we use ˜pl(x  y) to denote the empirical distribution of both X and
Y on labeled data Dl  ˜pl(x) to denote the empirical distribution of X on labeled data Dl  and ˜pu(x)
to denote the empirical distribution of X on unlabeled data Du.
In this paper  we propose an alternative approach for semi-supervised CRFs. Rather than using
minimum conditional entropy as a regularization term on unlabeled data  we use minimum mutual
information on unlabeled data. This approach has a nice and strong information theoretic interpre-
tation by rate distortion theory.
We deﬁne the marginal distribution pθ(y) of our discriminative model on unlabeled data Du to be
pθ(y) = Px∈Du ˜pu(x)pθ(y|x) over the input data x. Then the mutual information between the
empirical distribution ˜p(x) and the discriminative model is
˜pu(x)H“pθ(y|x)”
I“˜pu(x)  pθ(y|x)” = Xx∈DuXy
where H(cid:16)pθ(y)(cid:17) = −Py Px∈Du ˜pu(x)pθ(y|x) log (cid:16)Px∈Du ˜pu(x)pθ(y|x)(cid:17) is the entropy of
the label Y on unlabeled data. Thus in rate distortion terminology  the empirical distribution of
unlabeled data ˜pu(x) corresponds to input distribution  the model pθ(y|x) corresponds to the prob-
abilistic mapping from X to Y   and pθ(y) corresponds to the output distribution of Y .
Our proposed rate distortion approach for semi-supervised CRFs optimizes the following con-
strained optimization problem 

˜pu(x)pθ(y) ” = H“pθ(y)” −Xx∈Du

˜pu(x)pθ(y|x) log“ ˜pu(x)pθ(y|x)

min

θ

I“˜pu(x)  pθ(y|x)” s.t. D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ) ≤ d

The rationale for this formulation can be seen from an information-theoretic perspective using the
rate distortion theory [14]. Assume we have a source X with a source distribution p(x) and its com-
pressed representation Y through a probabilistic mapping pθ(y|x). If there is a large set of features
(inﬁnite in the extreme case)  this probabilistic mapping might be too redundant. We’d better look
for its minimum description. What determines the quality of the compression is the information
rate  i.e. the average number of bits per message needed to specify an element in the representation
without confusion. According to the standard asymptotic arguments [14]  this quantity is bounded
below by the mutual information I(cid:16)p(x)  pθ(y|x)(cid:17) since the average cardinality of the partition-
ing of X is given by the ratio of the volume of X to the average volume of the elements of X

(4)

3

that are mapped to the same representation Y through pθ(y|x)  2H(X)/2H(X|Y ) = 2I(X Y ). Thus
mutual information is the minimum information rate and is used as a good metric for clustering
[26  27]. True distribution of X should be used to compute the mutual information. Since it is
unknown  we use its empirical distribution on unlabeled data set Du and the mutual information
I(cid:16)˜pu(x)  pθ(y|x)(cid:17) instead. However  information rate alone is not enough to characterize good
representation since the rate can always be reduced by throwing away many features in the prob-
abilistic mapping. This makes the mapping likely too simple and leads to distortion. Therefore
we need an additional constraint provided through a distortion function which is presumed to be
small for good representations. Apparently there is a tradeoff between minimum representation
and maximum distortion. Since joint distribution gives the distribution for the pair of X and its
representation Y   we choose the log likelihood ratio  log
p(x)pθ(y|x)   plus a regularized complexity
term of θ  λU (θ)  as the distortion function. Thus the expected distortion is the non-negative term
D(cid:16)p(x  y)  p(x)pθ(y|x)(cid:17) + λU (θ). Again true distributions p(x  y) and p(x) should be used here 
but they are unknown. In semi-supervised setting  we have labeled data available which provides
valuable information to measure the distortion: we use the empirical distributions on labeled data set
Dl and the expected distortion D(cid:16)˜pl(x  y)  ˜pl(x)pθ(y|x)(cid:17) + λU (θ) instead to encode the informa-
tion provided by labeled data  and add a distortion constraint we should respect for data compression
to help the clustering. There is a monotonic tradeoff between the rate of the compression and the
expected distortion: the larger the rate  the smaller is the achievable distortion. Given a distortion
measure between X and Y on the labeled data set Dl  what is the minimum rate description re-
quired to achieve a particular distortion on the unlabeled data set Du? The answer can be obtained
by solving (4).
Following standard procedure  we convert the constrained optimization problem (4) into an uncon-
strained optimization problem which minimizes the following objective:

p(x y)

κ )1:

(5)

(6)

where κ > 0  which again is equivalent to minimizing the following objective (with γ = 1

RLMI(θ) = I“˜pu(x)  pθ(y|x)” + κ“D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ)”
RLMI(θ) = D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ) + γI“˜pu(x)  pθ(y|x)”

If (4) is a convex optimization problem  then for every solution θ to Eq. (4) found using some
particular value of d  there is some corresponding value of γ in the optimization problem (6) that
will give the same θ. Thus  these are two equivalent re-parameterizations of the same problem. The
equivalence between the two problems can be veriﬁed using convex analysis [8] by noting that the
Lagrangian for the constrained optimization (4) is exactly the objective in the optimization (5) (plus
a constant that does not depend on θ)  where κ is the Lagrange multiplier. Thus  (4) can be solved
by solving either (5) or (6) for an appropriate κ or γ. Unfortunately (4) is not a convex optimization
problem  because its objective I(cid:16)˜pu(x)  pθ(y|x)(cid:17) is not convex. This can be veriﬁed using the same
argument as in the minimum conditional entropy regularization case [15  16]. There may be some
minima of (4) that do not minimize (5) or (6) whatever the value of κ or γ may be. This is however
not essential to motivate the optimization criterion. Moreover there are generally local minima in
(5) or (6) due to the non-convexity of its mutual information regularization term.
Another training method for semi-supervised CRFs is the maximum entropy approach  maximizing
conditional entropy (minimizing negative conditional entropy) over unlabeled data Du subject to the
constraint on labeled data Dl 

min

θ

“ − Xx∈Du

˜pu(x)H“pθ(y|x)”” s.t. D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ) ≤ d

again following standard procedure  we convert the constrained optimization problem (7) into an
unconstrained optimization problem which minimizes the following objective:

(7)

RLmaxCE(θ) = D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ) − γ Xx∈Du

˜pu(x)H“pθ(y|x)”

(8)

1For

the part of unlabeled data 

information 
I(˜pu(x)  pθ(x|y))  of a generative model pθ(x|y) instead  which is equivalent to minimizing conditional en-
tropy of a generative model pθ(x|y)  since I(˜pu(x)  pθ(x|y)) = H(˜pu(x)) − H(pθ(x|y)) and H(˜pu(x)) is
a constant.

the MMIHMM algorithm [24] maximizes mutual

4

Again minimizing (8) is not exactly equivalent to (7); however  it is not essential to motivate the
optimization criterion. When comparing maximum entropy approach with minimum conditional
entropy approach  there is only a sign change on conditional entropy term.

For non-parametric models  using the analysis developed in [5  6  7  25]  it can be shown that maxi-
mum conditional entropy approach is equivalent to rate distortion approach when we compress code
vectors in a mass constrained scheme [25]. But for parametric models such as CRFs  these three
approaches are completely distinct.
The difference between our rate distortion approach for semi-supervised CRFs (6) and the minimum
conditional entropy regularized semi-supervised CRFs (2) is not only on the different sign of condi-
tional entropy on unlabeled data but also the additional term – entropy of pθ(y) on unlabeled data.
It is this term that makes direct computation of the derivative of the objective for the rate distortion
approach for semi-supervised CRFs intractable. To see why  we take derivative of this term with
respect to θ  we have:

∂

∂θ“ − H(pθ(y))” = Xx∈Du
− Xx∈Du

˜pu(x)Xy
˜pu(x)Xy

pθ(y|x)f (x  y) log“ Xx∈Du
pθ(y|x) log“ Xx∈Du

˜pu(x)pθ(y|x)”
˜pu(x)pθ(y|x)”Xy

′

pθ(y

′|x)f (x  y

′)

In the case of structured prediction  the number of sums over Y is exponential  and there is a sum
inside the log. These make the computation of the derivative intractable even for a simple chain
structured CRF.

An alternative way to solve (6) is to use the famous algorithm for the computation of the rate distor-
tion function established by Blahut [6] and Arimoto [3]. Corduneanu and Jaakkola [12  13] proposed
a distributed propagation algorithm  a variant of Blahut-Arimoto algorithm  to solve their problem.
However as illustrated in the following  this approach is still intractable for structured prediction in
our case.
By extending a lemma for computing rate distortion in [14] to parametric models  we can rewrite
the minimization problem (5) of mutual information regularized semi-supervised CRFs as a double
minimization 

min

θ

min
r(y)

g(θ  r(y)) where

g(θ  r(y)) = Xx∈DuXy

˜pu(x)pθ(y|x) log

pθ(y|x)

r(y)

+ κ“D“˜pl(x  y)  ˜pl(x)pθ(y|x)” + λU (θ)”

We can use an alternating minimization algorithm to ﬁnd a local minimum of RLM I (θ). First  we
assign the initial CRF model to be the optimal solution of the supervised CRF on labeled data and
denote it as pθ(0)(y|x). Then we deﬁne r(0)(y) and in general r(t)(y) for t ≥ 1 by

r(t)(y) = Xx∈Du

˜pu(x)pθ(t) (y|x)

(9)

In order to deﬁne pθ(1)(y|x) and in general pθ(t)(y|x)  we need to ﬁnd the pθ(y|x) which minimizes
g for a given r(y). The gradient of g(θ  r(y)) with respect to θ is

∂
∂θ

g(θ  r(y)) =

˜pu(x

M

Xi=N +1
+Xy
Xi=1

−κ

N

pθ(y|x

(i))“covpθ (y|x(i))hf (x
(i)) log r(y)Xy
(i)) f (x

(i)  y

′

˜pl(x

(i)  y)iθ −Xy

pθ(y

′|x

(i))f (x

(i)  y

pθ(y|x

(i))f (x

(i)) −Xy

pθ(y|x

(i))f (x

(i)  y) log r(y) (10)

′)”
(i)  y)! + κλ

(11)

∂
∂θ

U (θ)

(12)

Even though the ﬁrst term in Eq. (10) and (12) can be efﬁciently computed via recursive formu-
las [16]  we run into the same intractable problem to compute the second term Eq. (10) and Eq. 11)
since the number of sums over Y is exponential and implicitly there is a sum inside the log due
to r(y). This makes the computation of the derivative in the alternating minimization algorithm
intractable.

5

3 A variational training procedure
In this section  we derive a convergent variational algorithm to train rate distortion based semi-
supervised CRFs for sequence labeling. The basic idea of convexity-based variational inference is
to make use of Jensen’s inequality to obtain an adjustable upper bound on the objective function
[17]. Essentially  one considers a family of upper bounds indexed by a set of variational parameters.
The variational parameters are chosen by an optimization procedure that attempts to ﬁnd the tightest
possible upper bound.
Following Jordan et al.
H(pθ(y)) using Jensen’s inequality as the following 

[17]  we begin by introducing a variational distribution q(x) to bound

H(pθ(y)) = −Xy Xx∈Du
Xj=N +1

≤ −Xy

M

˜pu(x)pθ(y|x) log  Xx∈Du
(j))" M
Xl=N +1

(j))pθ(y|x

˜pu(x

˜pu(x)pθ(y|x)

q(x)

q(x)!
(l)) log„ ˜pu(x

q(x

(l))pθ(y|x
q(x(l))

(l))

«#

Thus the desideratum of ﬁnding a tight upper bound of RLMI(θ) in Eq. (6) translates directly into
the following alternative optimization problem:

U(θ  q) =

N

(θ∗  q∗) = min

θ q

U(θ  q)

where

M

M

(i)) log pθ(y

(i)|x

(i)) + λU (θ) − γ

(j))q(x

pθ(y|x

(j)) log pθ(y|x

(l)) (13)

˜pu(x

(j))

M

Xl=N +1

q(x

(l)) log

(l))
˜pu(x
q(x(l))

+ γ

Minimizing U with respect to q has a closed form solution 

(l))Xy

Xj=N +1

˜pu(x

Xl=N +1
Xj=N +1Xy

M

˜pu(x

(j))pθ(y|x

(j)) log pθ(y|x

(j))

(14)

−

−γ

˜pl(x

Xi=1
Xj=N +1

M

q(x

(l)) =

It can be shown that

˜pu(x

(l)) exp“PM
k=1 ˜pu(x(k)) exp“PM
PM
U(θ  q) ≥ RLMI(θ) +Xy Xx∈Du

(j))pθ(y|x

(j)) log pθ(y|x

(l))”
j=N +1Py ˜pu(x
j=N +1Py ˜pu(x(j))pθ(y|x(j)) log pθ(y|x(k))”

∀ x

(l) ∈ Du

(15)

˜pu(x)pθ(y|x) Xx∈Du

D“q(x)  qθ(x|y)” ≥ 0

(16)

˜pu(x)pθ(y|x)

Px∈Du ˜pu(x)pθ(y|x) ∀ x ∈ Du. Thus U is bounded below  the alternative mini-

where qθ(x|y) =
mization algorithm monotonically decreases U and converges.
In order to calculate the derivative of U with respect to θ  we just need to notice that the ﬁrst term
in Eq. (13) is the log-likelihood in CRF  and the ﬁrst term in Eq. (14) is a constant and second term
in Eq. (14) is the conditional entropy in [16]. They all can be efﬁciently computed [16  21]. In the
following  we show how to compute the derivative of the last term in Eq.(13) using an idea similar
to that proposed in [21]. Without loss of generality  we assume all the unlabeled data are of equal
lengths in the sequence labeling case. We will describe how to handle the case of unequal lengths in
Sec. 4.
If we deﬁne A(y  x
(l)) in (13) for a ﬁxed (j  l) pair 
(l) form two linear-chain graphs of equal lengths  we can calcu-
where we assume x
late the derivative of A(y  x
(l)) with respect to the k-th parameter θk  where all the terms
can be computed through standard dynamic programming techniques in CRFs except one term
(j)  y). Nevertheless similar to [21]  we compute this term as
Py pθ(y|x
follows [21]: we ﬁrst deﬁne pairwise subsequence constrained entropy on (x
(l)) (as suppose
to the subsequence constrained entropy deﬁned in [21]) as:

(l)) = Py pθ(y|x

(j)  x
(j) and x

(j)) log pθ(y|x

(j)) log pθ(y|x

(l))fk(x

(j)  x

(j)  x

H σ

jl(y−(a..b)|ya..b  x

(j)  x

(l)) = Xy−(a..b)

pθ(y−(a..b)|ya..b  x(j)) log pθ(y−(a..b)|ya..b  x(l))

6

where y−(a..b) is the label sequence with its subsequence ya..b ﬁxed. If we have H σ
then the term Py pθ(y|x
dence property of linear-chain CRF  we have the following:

jl for all (a  b) 
(j)  y) can be easily computed. Using the indepen-

(j)) log pθ(y|x

(l))fk(x

pθ(y−(a..b)  ya..b|x

(j)) log pθ(y−(a..b)  ya..b|x

(l))

Xy−(a..b)

= pθ(ya..b|x

(j)) log pθ(ya..b|x

(j))H α

jl(y1..(a−1)|ya  x

(j)  x

(l))

+pθ(ya..b|x

(j))H β

jl(y(b+1)..n|yb  x

(l)) + pθ(ya..b|x
(l))

(j)  x

Given H α
H α

jl(·) and H β

jl(·)  any sequence entropy can be computed in constant time [21]. Computing

jl(·) can be done using the following dynamic programming [21]:

H α

jl(y1..i|yi+1  x

(j)  x

pθ(yi|yi+1  x

(j)) log pθ(yi|yi+1  x

(l))

pθ(yi|yi+1  x

(j))H α

jl(y1..(i−1)|yi  x

(j)  x

(l))

(l)) = Xyi
+Xyi

The base case for the dynamic programming is H α
pθ(yi|yi+1  x
be similarly computed using dynamic programming.

j)) needed in the above formula can be obtained using belief propagation. H β

jl(∅|y1  x

(j)  x

(l)) = 0. All the probabilities (i.e. 
jl(·) can

4 Experiments

We compare our rate distortion approach for semi-supervised learning with one of the state-of-the-art
semi-supervised learning algorithms  minimum conditional entropy approach and maximum condi-
tional entropy approach on two real-world problems: text categorization and hand-written character
recognition. The purpose of the ﬁrst task is to show the effectiveness of rate distortion approach
over minimum and maximum conditional entropy approaches when no approximation is needed in
training. In the second task  a variational method has to be used to train semi-supervised chain
structured CRFs. We demonstrate the effectiveness of the rate distortion approach over minimum
and maximum conditional entropy approaches even when an approximation is used during training.

4.1 Text categorization
We select different class pairs from the 20 newsgroup dataset 2 to construct our binary classiﬁcation
problems. The chosen classes are similar to each other and thus hard for classiﬁcation algorithms.
We use Porter stemmer to reduce the morphological word forms. For each label  we rank words
based on their mutual information with that label (whether it predicts label 1 or 0). Then we choose
the top 100 words as our features. For each problem  we select 15% of the training data  almost 150
instances  as the labeled training data and select the unlabeled data from the remaining data. The
validation set (for setting the free parameters  e.g. λ and γ) contains 100 instances. The test set
contains about 700 instances. We vary the ratio between the amount of unlabeled and labeled data 
repeat the experiments ten times with different randomly selected labeled and unlabeled training
data  and report the mean and standard deviation over different trials. For each run  we initialize the
model parameter for mutual information (MI) regularization and maximum/minimum conditional
entropy (CE) regularization using the parameter learned from a l2-regularized logistic regression
classiﬁer. Figure 1 shows the classiﬁcation accuracies of these four regularization methods versus
the ratio between the amount of unlabeled and labeled data on different classiﬁcation problems. We
can see that mutual information regularization outperforms the other three regularization schemes.
In most cases  maximum CE regularization outperforms minimum CE regularization and the base-
line (logistic regression with l2 regularization) which uses only the labeled data. Although the
randomly selected labeled instances are different for different experiments  we should not see a sig-
niﬁcant difference in the performance of the learned models based on the baseline; since for each
particular ratio of labeled and unlabeled data  the performance is averaged over ten runs. We suspect
the reason for the performance differences of the baselines models in Figure 1 is due to our feature
selection phase.

2http://people.csail.mit.edu/jrennie/20Newsgroups.

7

0.882

0.88

0.878

0.876

0.874

0.872

0.87

0.868

0.866

0.864

y
c
a
r
u
c
c
a

0.862
 
0

1

MI
minCE
maxCE
L2

 

0.87

0.865

0.86

0.855

0.85

0.845

0.84

y
c
a
r
u
c
c
a

MI
minCE
maxCE
L2

 

 

y
c
a
r
u
c
c
a

0.845

0.84

0.835

0.83

0.825

0.82

MI
minCE
maxCE
L2

2

3

4

ratio unlabel/label

5

6

0.835
 
0

1

2

3

4

ratio unlabel/label

5

6

0.815
 
0

1

2

3

4

5

6

ratio unlabel/label

MI
minCE
maxCE
L2

0.73

0.72

0.71

0.7

0.69

0.68

0.67

y
c
a
r
u
c
c
a

MI
minCE
maxCE
L2

 

0.8

0.795

0.79

0.785

0.78

0.775

0.77

y
c
a
r
u
c
c
a

0.66
 
0

1

2

3

4

ratio unlabel/label

5

6

0.765
 
0

1

2

3

4

ratio unlabel/label

 

5

6

Figure 1: Results on ﬁve different binary classiﬁcation problems in text categorization (left to right):
comp.os.ms-windows.misc vs comp.sys.mac.hardware; rec.autos vs rec.motorcycles; rec.sport.baseball vs
rec.sport.hockey; talk.politics.guns vs talk.politics.misc; sci.electronics vs sci.med.

0.825

0.82

0.815

0.81

0.805

0.8

0.795

0.79

y
c
a
r
u
c
c
a

0.785
 
0

1

MI

minCE

maxCE

L2

 

MI

minCE

maxCE

L2

 

0.78

0.76

0.74

0.72

0.7

0.68

0.66

y
c
a
r
u
c
c
a

2

3

4

ratio unlabel/label

5

6

0.64
 
0

1

2

3

4

5

6

ratio unlabel/label

Figure 2: Results on hand-written character recognition: (left) sequence labeling; (right) multi-class classiﬁ-
cation.

4.2 Hand-written character recognition
Our dataset for hand-written character recognition contains ∼6000 handwritten words with average
length of ∼8 characters. Each word was divided into characters  each character is resized to a 16 × 8
binary image. We choose ∼600 words as labeled data  ∼600 words as validation data  ∼2000 words
as test data. Similar to text categorization  we vary the ratio between the amount of unlabeled and
labeled data  and report the mean and standard deviation of classiﬁcation accuracies over several
trials.

We use a chain structured graph to model hand-written character recognition as a sequence labeling
problem  similar to [29]. Since the unlabeled data may have different lengths  we modify the mu-
tual information as I = Pℓ Iℓ  where Iℓ is the mutual information computed on all the unlabeled
data with length ℓ. We compare our approach (MI) with other regularizations (maximum/minimum
conditional entropy  l2). The results are shown in Fig. 2 (left). As a sanity check  we have also
tried solving hand-written character recognition as a multi-class classiﬁcation problem  i.e. without
considering the correlation between adjacent characters in a word. The results are shown in Fig. 2
(right). We can see that MI regularization outperforms maxCE  minCE and l2 regularizations in
both multi-class and sequence labeling cases. There are signiﬁcant gains in the structured learning
compared with the standard multi-class classiﬁcation setting.
5 Conclusion and future work

We have presented a new semi-supervised discriminative learning algorithm to train CRFs. The
proposed approach is motivated by the rate distortion framework in information theory and utilizes
the mutual information on the unlabeled data as a regularization term  to be more precise a data
dependent prior. Even though a variational approximation has to be used during training process for
even a simple chain structured graph  our experimental results show that our proposed rate distortion
approach outperforms supervised CRFs with l2 regularization and a state-of-the-art semi-supervised
minimum conditional entropy approach as well as semi-supervised maximum conditional entropy
approach in both multi-class classiﬁcation and sequence labeling problems. As future work  we
would like to apply this approach to other graph structures  develop more efﬁcient learning algo-
rithms and illuminate how reducing the information rate helps generalization.

8

References

[1] S. Abney. Semi-Supervised Learning for Computational Linguistics. Chapman & Hall/CRC  2007.
[2] Y. Altun  D. McAllester and M. Belkin. Maximum margin semi-supervised learning for structured vari-

ables. NIPS 18:33-40  2005.

[3] S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. IEEE

Transactions on Information Theory  18:1814-1820  1972.

[4] L. Bahl  P. Brown  P. de Souza and R. Mercer. Maximum mutual information estimation of hidden Markov

model parameters for speech recognition. ICASSP  11:49-52  1986.

[5] T. Berger and J. Gibson. Lossy source coding. IEEE Transactions on Information Theory  44(6):2693-

2723  1998.

[6] R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on Informa-

tion Theory  18:460-473  1972.

[7] R. Blahut. Principles and Practice of Information Theory  Addison-Wesley  1987.
[8] S. Boyd and L. Vandenberghe. Convex Optimization  Cambridge University Press  2004.
[9] U. Brefeld and T. Scheffer. Semi-supervised learning for structured output variables.

2006.

ICML  145-152 

[10] O. Chapelle  B. Scholk¨opf and A. Zien. Semi-Supervised Learning  MIT Press  2006.
[11] A. Corduneanu and T. Jaakkola. On information regularization. UAI  151-158  2003.
[12] A. Corduneanu and T. Jaakkola. Distributed information regularization on graphs. NIPS  17:297-304 

2004.

[13] A. Corduneanu and T. Jaakkola. Data dependent regularization.

In Semi-Supervised Learning  O.

Chapelle  B. Scholk¨opf and A. Zien  (Editors)  163-182  MIT Press  2006.

[14] T. Cover and J. Thomas. Elements of Information Theory  Wiley  1991.
[15] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. NIPS  17:529-536 

2004.

[16] F. Jiao  S. Wang  C. Lee  R. Greiner and D. Schuurmans. Semi-supervised conditional random ﬁelds for

improved sequence segmentation and labeling. COLING/ACL  209-216  2006.

[17] M. Jordan  Z. Ghahramani  T. Jaakkola and L. Saul. Introduction to variational methods for graphical

models. Machine Learning  37:183-233  1999.

[18] D. Jurafsky and J. Martin. Speech and Language Processing  2nd Edition  Prentice Hall  2008.
[19] J. Lafferty  A. McCallum and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. ICML  282-289  2001.

[20] C. Lee  S. Wang  F. Jiao  D. Schuurmans and R. Greiner. Learning to model spatial dependency: Semi-

supervised discriminative random ﬁelds. NIPS  19:793-800  2006.

[21] G. Mann and A. McCallum. Efﬁcient computation of entropy gradient for semi-supervised conditional

random ﬁelds. NAACL/HLT  109-112  2007.

[22] G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional

random ﬁelds. ACL  870-878  2008.

[23] Y. Normandin. Maximum mutual information estimation of hidden Markov models. In Automatic Speech
and Speaker Recognition: Advanced Topics  C. Lee  F. Soong and K. Paliwal (Editors)  57-81  Springer 
1996.

[24] N. Oliver and A. Garg. MMIHMM: maximum mutual information hidden Markov models. ICML  466-

473  2002.

[25] K. Rose. Deterministic annealing for clustering  compression  classiﬁcation  regression  and related opti-

mization problems. Proceedings of the IEEE  80:2210-2239  1998.

[26] N. Slonim  G. Atwal  G. Tkacik and W. Bialek. Information based clustering. Proceedings of National

Academy of Science (PNAS)  102:18297-18302  2005.

[27] S. Still and W. Bialek. How many clusters? An information theoretic perspective. Neural Computation 

16:2483-2506  2004.

[28] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. NIPS  1025-1032 

2002.

[29] B. Taskar  C. Guestrain and D. Koller. Max-margin Markov networks. NIPS  16:25-32  2003.
[30] N. Tishby  F. Pereira  and W. Bialek. The information bottleneck method. The 37th Annual Allerton

Conference on Communication  Control  and Computing  368-377  1999.

[31] L. Zheng  S. Wang  Y. Liu and C. Lee. Information theoretic regularization for semi-supervised boosting.

KDD  1017-1026  2009.

[32] X. Zhu. Semi-supervised learning literature survey. Computer Sciences TR 1530  University of Wisconsin

Madison  2007.

9

,Hajin Shim
Sung Ju Hwang
Eunho Yang