2017,Collaborative PAC Learning,We introduce a collaborative PAC learning model  in which k players attempt to learn the same underlying concept. We ask how much more information is required to learn an accurate classifier for all players simultaneously. We refer to the ratio between the sample complexity of collaborative PAC learning and its non-collaborative (single-player) counterpart as the overhead. We design learning algorithms with O(ln(k)) and O(ln^2(k)) overhead in the personalized and centralized variants our model. This gives an exponential improvement upon the naive algorithm that does not share information among players. We complement our upper bounds with an Omega(ln(k)) overhead lower bound  showing that our results are tight up to a logarithmic factor.,Collaborative PAC Learning

Toyota Technological Institute at Chicago

Avrim Blum

Chicago  IL 60637
avrim@ttic.edu

Nika Haghtalab

Computer Science Department
Carnegie Mellon University

Pittsburgh  PA 15213

nhaghtal@cs.cmu.edu

Ariel D. Procaccia

Computer Science Department
Carnegie Mellon University

Pittsburgh  PA 15213

arielpro@cs.cmu.edu

Institute for Interdisciplinary Information Sciences

Mingda Qiao

Tsinghua University
Beijing  China 100084

qmd14@mails.tsinghua.edu.cn

Abstract

We consider a collaborative PAC learning model  in which k players attempt to
learn the same underlying concept. We ask how much more information is re-
quired to learn an accurate classiﬁer for all players simultaneously. We refer to
the ratio between the sample complexity of collaborative PAC learning and its
non-collaborative (single-player) counterpart as the overhead. We design learning
algorithms with O(ln(k)) and O(ln2(k)) overhead in the personalized and central-
ized variants our model. This gives an exponential improvement upon the naïve
algorithm that does not share information among players. We complement our
upper bounds with an Ω(ln(k)) overhead lower bound  showing that our results are
tight up to a logarithmic factor.

Introduction

1
According to Wikipedia  collaborative learning is a “situation in which two or more people learn ...
something together ” e.g.  by “capitalizing on one another’s resources” and “asking one another for
information.” Indeed  it seems self-evident that collaboration  and the sharing of information  can
make learning more efﬁcient. Our goal is to formalize this intuition and study its implications.
As an example  suppose k branches of a department store  which have sales data for different items in
different locations  wish to collaborate on learning which items should be sold at each location. In
this case  we would like to use the sales information across different branches to learn a good policy
for each branch. Another example is given by k hospitals with different patient demographics  e.g. 
in terms of racial or socio-economic factors  which want to predict occurrence of a disease in patients.
In addition to requiring a classiﬁer that performs well on the population served by each hospital  it is
natural to assume that all hospitals deploy a common classiﬁer.
Motivated by these examples  we consider a model of collaborative PAC learning  in which k players
attempt to learn the same underlying concept. We then ask how much information is needed for all
players to simultaneously succeed in learning a desirable classiﬁer. Speciﬁcally  we focus on the
classic probably approximately correct (PAC) setting of Valiant [14]  where there is an unknown target
function f∗ ∈ F. We consider k players with distributions D1  . . .   Dk that are labeled according to
f∗. Our goal is to learn f∗ up to an error of  on each and every player distribution while requiring
only a small number of samples overall.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

A natural but naïve algorithm that forgoes collaboration between players can achieve our objective
by taking  from each player distribution  a number of samples that is sufﬁcient for learning the
individual task  and then training a classiﬁer over all samples. Such an algorithm uses k times as
many samples as needed for learning an individual task — we say that this algorithm incurs O(k)
overhead in sample complexity. By contrast  we are interested in algorithms that take advantage
of the collaborative environment  learn k tasks by sharing information  and incur o(k) overhead in
sample complexity.
We study two variants of the aforementioned model: personalized and centralized. In the personalized
setting (as in the department store example)  we allow the learning algorithm to return different
functions for different players. That is  our goal is to return classiﬁers f1  . . .   fk that have error of at
most  on player distributions D1  . . .   Dk  respectively. In the centralized setting (as in the hospital
example)  the learning algorithm is required to return a single classiﬁer f that has an error of at most
 on all player distributions D1  . . .   Dk. Our results provide upper and lower bounds on the sample
complexity overhead required for learning in both settings.
1.1 Overview of Results
In Section 3  we provide algorithms for personalized and centralized collaborative learning that obtain
exponential improvements over the sample complexity of the naïve approach. In Theorem 3.1  we
introduce an algorithm for the personalized setting that has O(ln(k)) overhead in sample complexity.
For the centralized setting  in Theorem 3.2  we develop an algorithm that has O(ln2(k)) overhead in
sample complexity. At a high level  the latter algorithm ﬁrst learns a series of functions on adaptively
chosen mixtures of player distributions. These mixtures are chosen such that for any player a large
majority of the functions perform well. This allows us to combine all functions into one classiﬁer
that performs well on every player distribution. Our algorithm is an improper learning algorithm  as
the combination of these functions may not belong to F.
In Section 4  we present lower bounds on the sample complexity of collaborative PAC learning for
the personalized and centralized variants. In particular  in Theorem 4.1 we show that any algorithm
that learns in the collaborative setting requires Ω(ln(k)) overhead in sample complexity. This shows
that our upper bound for the personalized setting  as stated in Theorem 3.1  is tight. Furthermore  in
Theorem 4.5  we show that obtaining uniform convergence across F over all k player distributions
requires Ω(k) overhead in sample complexity. Interestingly  our centralized algorithm (Theorem 3.2)
bypasses this lower bound by using arguments that do not depend on uniform convergence. Indeed 
this can be seen from the fact that it is an improper learning algorithm.
In Appendix D  we discuss the extension of our results to the non-realizable setting. Speciﬁcally  we
consider a setting where there is a “good” but not “perfect” target function f∗ ∈ F that has a small
error with respect to every player distribution  and prove that our upper bounds carry over.
1.2 Related Work
Related work in computational and statistical learning has examined some aspects of the general
problem of learning multiple related tasks simultaneously. Below we discuss papers on multi-task
learning [4  3  7  5  10  13]  domain adaptation [11  12  6]  and distributed learning [2  8  15]  which
are most closely related.
Multi-task learning considers the problem of learning multiple tasks in series or in parallel. In this
space  Baxter [4] studied the problem of model selection for learning multiple related tasks. In their
work  each learning task is itself randomly drawn from a distribution over related tasks  and the
learner’s goal is to ﬁnd a hypothesis space that is appropriate for learning all tasks. Ben-David and
Schuller [5] also studied the sample complexity of learning multiple related tasks. However  in their
work similarity between two tasks is represented by existence of “transfer” functions though which
underlying distributions are related.
Mansour et al. [11  12] consider a multi-source domain adaptation problem  where the learner is given
k distributions and k corresponding predictors that have error at most  on individual distributions.
The goal of the learner is to combine these predictors to obtain error of k on any unknown mixture of
player distributions. Our work is incomparable to this line of work  as our goal is to learn classiﬁers 
rather than combining existing ones  and our benchmark is to obtain error  on each individual
distribution. Indeed  in our setting one can learn a hypothesis that has error k on any mixture of
players with no overhead in sample complexity.

2

well-known that sampling a set S of size m δ = O(cid:0) 1

Distributed learning [2  8  15] also considers the problem of learning from k different distributions
simultaneously. However  the main objective in this space is to learn with limited communication
between the players  rather than with low sample complexity.
2 Model
Let X be an instance space and Y = {0  1} be the set of labels. A hypothesis is a function
f : X → Y that maps any instance x ∈ X to a label y ∈ Y. We consider a hypothesis class F
with VC dimension d. Given a distribution D over X × Y  the error of a hypothesis f is deﬁned as
errD(f ) = Pr(x y)∼D [f (x) (cid:54)= y].
In the collaborative learning setting  we consider k players with distributions D1  . . .   Dk over
X × Y. We focus on the realizable setting  where all players’ distributions are labeled according to
a common target function f∗ ∈ F  i.e.  errDi(f∗) = 0 for all i ∈ [k] (but see Appendix D for an
extension to the non-realizable setting). We represent an instance of the collaborative PAC learning
setting with the 3-tuple (F  f∗ {D}i∈[k]).
Our goal is to learn a good classiﬁer with respect to every player distribution. We call this (  δ)-
learning in the collaborative PAC setting  and study two variants: the personalized setting  and the
centralized setting. In the personalized setting  our goal is to learn functions f1  . . .   fk  such that
with probability 1 − δ  errDi(fi) ≤  for all i ∈ [k]. In the centralized setting  we require all the
output functions to be identical. Put another way  our goal is to return a single f  such that with
probability 1 − δ  errDi(f ) ≤  for all i ∈ [k]. In both settings  we allow our algorithm to be
improper  that is  the learned functions need not belong to F.
We compare the sample complexity of our algorithms to their PAC counterparts in the realizable
setting. In the traditional realizable PAC setting  m δ denotes the number of samples needed for
(  δ)-learning F. That is  m δ is the total number of samples drawn from a realizable distribution D 
such that  with probability 1 − δ  any classiﬁer f ∈ F that is consistent with the sample set satisﬁes
errD(f ) ≤ . We denote by OF (·) the function that  for any set S of labeled samples  returns a
function f ∈ F that is consistent with S if such a function exists (and outputs “none” otherwise). It is
sufﬁcient for (  δ)-learning a hypothesis class F of VC dimension d [1]. We refer to the ratio of the
sample complexity of an algorithm in the collaborative PAC setting to that of the (non-collaborative)
realizable PAC setting as the overhead. For ease of exposition  we only consider the dependence of
the overhead on parameters k  d  and .
3 Sample Complexity Upper Bounds
In this section  we prove upper bounds on the sample complexity of (  δ)-learning in the collaborative
PAC setting. We begin by providing a simple algorithm with O(ln(k)) overhead (in terms of sample
complexity  see Section 2) for the personalized setting. We then design and analyze an algorithm for
the centralized setting with O(ln2(k)) overhead  following a discussion of additional challenges that
arise in this setting.
3.1 Personalized Setting
The idea underlying the algorithm for the personalized setting is quite intuitive: If we were to learn a
classiﬁer that is on average good for the players  then we have learned a classiﬁer that is good for a
large fraction of the players. Therefore  a large fraction of the players can be simultaneously satisﬁed
by a single good global classiﬁer. This process can be repeated until each player receives a good
classiﬁer.
(cid:80)
In more detail  let us consider an algorithm that pools together a sample set of total size m/4 δ from
i∈[k] Di over individual player distributions  and ﬁnds f ∈ F that
the uniform mixture D = 1
k
is consistent with this set. Clearly  with probability 1 − δ  f has a small error of /4 with respect
to distribution D. However  we would like to understand how well f performs on each individual
player’s distribution.
Since errD(f ) ≤ /4 is also the average error of f on player distributions  with probability 1 − δ  f
must have error of at most /2 on at least half of the players. Indeed  one can identify such players
by taking additional ˜O( 1
 ) samples from each player and asking whether the empirical error of f
on these sample sets is at most 3/4. Using a variant of the VC theorem  it is not hard to see that

(cid:1)(cid:1)(cid:1)  and applying OF (S)  is



(cid:0)d ln(cid:0) 1

(cid:1) + ln(cid:0) 1

δ



3

for any player i such that errDi(f ) ≤ /2  the empirical error of f is at most 3/4  and no player
with empirical error at most 3/4 has true error that is worst than . Once players with empirical
error 3/4 are identiﬁed  one can output fi = f for any such player  and repeat the procedure for
the remaining players. After log(k) rounds  this process terminates with all players having received
functions with error of at most  on their respective distributions  with probability 1 − log(k)δ.
We formalize the above discussion via Algorithm 1 and Theorem 3.1. For completeness  a more
rigorous proof of the theorem is given in Appendix A.

Algorithm 1 PERSONALIZED LEARNING
N1 ← [k]; δ(cid:48) ← δ/2 log(k);
for r = 1  . . .  (cid:100)log(k)(cid:101) do
Di;

(cid:80)

i∈Nr

˜Dr ← 1|Nr|
Let S be a sample of size m/4 δ(cid:48) drawn from ˜Dr  and f (r) ← OF (S);
Let Gr ← TEST(f (r)  Nr    δ(cid:48));
Nr+1 ← Nr \ Gr;
for i ∈ Gr do fi ← f (r);

end
return f1  . . .   fk
TEST(f  N    δ):
for i ∈ N do take sample set Ti of size O
return {i | errTi(f ) ≤ 3
Theorem 3.1. For any   δ > 0  and hypothesis class F of VC dimension d  Algorithm 1 (  δ)-learns
F in the personalized collaborative PAC setting using m samples  where

(cid:16)|N|

from Di ;

(cid:17)(cid:17)

(cid:16) 1

4 }

 ln

δ

m = O

(d + k) ln

+ k ln

.

(cid:18) ln(k)

(cid:18)



(cid:18) 1

(cid:19)



(cid:18) k

(cid:19)(cid:19)(cid:19)

δ

Note that Algorithm 1 has O(ln(k)) overhead when k = O(d).
3.2 Centralized Setting

We next present a learning algorithm with O(ln2(k)) overhead in the centralized setting. Recall that
our goal is to learn a single function f that has an error of  on every player distribution  as opposed
to the personalized setting where players can receive different functions.
A natural ﬁrst attempt at learning in the centralized setting is to combine the classiﬁers f1  . . .   fk
that we learned in the personalized setting (Algorithm 1)  say  through a weighted majority vote.
One challenge with this approach is that  in general  it is possible that many of the functions fj
perform poorly on the distribution of a different player i. The reason is that when Algorithm 1 ﬁnds a
suitable f (r) for players in Gr  it completely removes them from consideration for future rounds;
subsequent functions may perform poorly with respect to the distributions associated with those
players. Therefore  this approach may lead to a global classiﬁer with large error on some player
distributions.
To overcome this problem  we instead design an algorithm that continues to take additional samples
from players for whom we have already found suitable classiﬁers. The key idea behind the centralized
learning algorithm is to group the players at every round based on how many functions learned so
far have large error rates on those players’ distributions  and to learn from data sampled from all
the groups simultaneously. This ensures that the function learned in each round performs well on a
large fraction of the players in each group  thereby reducing the likelihood that in later stages of this
process a player appears in a group for which a large fraction of the functions perform poorly.
In more detail  our algorithm learns t = Θ(ln(k)) classiﬁers f (1)  f (2)  . . .   f (t)  such that for any
player i ∈ [k]  at least 0.6t functions among them achieve an error below (cid:48) = /6 on Di. The
algorithm then returns the classiﬁer maj({f (r)}t
r=1)  where  for a set of hypotheses F   maj(F )
denotes the classiﬁer that  given x ∈ X   returns the label that the majority of hypotheses in F assign
to x. Note that any instance that is mislabeled by this classiﬁer must be mislabeled by at least 0.1t

4

i

r=1)

functions among the 0.6t good functions  i.e.  1/6 of the good functions. Hence  maj({f (r)}t
has an error of at most 6(cid:48) =  on each distribution Di.
for any round r ∈ [t] and player i ∈ [k] 
Throughout the algorithm  we keep track of counters α(r)
which  roughly speaking  record the number of classiﬁers among f (1)  f (2)  . . .   f (r) that have an
error of more than (cid:48) on distribution Di. To learn f (r+1)  we ﬁrst group distributions D1  . . .   Dk
based on the values of α(r)
  draw about m(cid:48) δ samples from the mixture of the distributions in each
group  and return a function f (r+1) that is consistent with all of the samples. Similarly to Section 3.1 
one can show that f (r+1) achieves O((cid:48)) error with respect to a large fraction of player distributions
in each group. Consequently  the counters are increased  i.e.  α(r+1)
  only for a small fraction
i ≤ 0.4t for any player i ∈ [k]  i.e.  on
of players. Finally  we show that with high probability  α(t)
each distribution Di  at least 0.6t functions achieve error of at most (cid:48).
The algorithm is formally described in Algorithm 2. The next theorem states our sample complexity
upper bound for the centralized setting.

> α(r)

i

i

i

(cid:109)

t ←(cid:108) 5

Algorithm 2 CENTRALIZED LEARNING
i ← 0 for each i ∈ [k];
α(0)

; (cid:48) ← /6;

2 log8/7(k)
0 ← [k]; N (0)
N (0)
for r = 1  2  . . .   t do
for c = 0  1  . . .   t − 1 do

c ← ∅ for each c ∈ [t];

if N (r−1)

(cid:54)= ∅ then

c
Draw a sample set S(r)

c

(cid:17)

else S(r)

c ← ∅ ;

(cid:16)(cid:83)t−1

c=0 S(r)

end
f (r) ← OF
Gr ← TEST(f (r)  [k]  (cid:48)  δ/(2t));
i ← α(r−1)
for i = 1  . . .   k do α(r)
c ← {i ∈ [k] : α(r)
for c = 0  . . .   t do N (r)

;

c

i

+ I [i /∈ Gr];
i = c};

end
return maj({f (r)}t

r=1);

of size m(cid:48)/16 δ/(2t2) from (cid:101)D(r−1)

c

=

1

|N (r−1)

c

|

(cid:80)

i∈N (r−1)

c

Di;

Theorem 3.2. For any   δ > 0  and hypothesis class F of VC dimension d  Algorithm 2 (  δ)-learns
F in the centralized collaborative PAC setting using m samples  where

(cid:18) ln2(k)

(cid:18)



(cid:18) 1

(cid:19)



(cid:18) 1

(cid:19)(cid:19)(cid:19)

δ

m = O

(d + k) ln

+ k ln

.

In particular  Algorithm 2 has O(ln2(k)) overhead when k = O(d).
Turning to the theorem’s proof  note that in Algorithm 2  N (r−1)

whom c out of the r − 1 functions learned so far have a large error  and (cid:101)D(r−1)
each function f (r) has a small error on (cid:101)D(r−1)
(cid:108) 5

represents the set of players for
represents the mixture
of distribution of players in N (r−1)
. Moreover  Gr is the set of players for whom f (r) has a small
error. The following lemma  whose proof appears in Appendix B.1  shows that with high probability
for all c. Here and in the following  t stands for

(cid:109)

c

c

c

c

as in Algorithm 2.

2 log8/7(k)
Lemma 3.3. With probability 1 − δ  the following two properties hold for all r ∈ [t]:
1. For any c ∈ {0  . . .   t − 1} such that N (r−1)
2. For any i ∈ Gr  errDi(f (r)) ≤ (cid:48)  and for any i /∈ Gr  errDi(f (r)) > (cid:48)/2.

is non-empty  err(cid:101)D(r−1)

(f (r)) ≤ (cid:48)/16.

c

c

5

The next lemma gives an upper bound on |N (r)
learned functions have a large error.
Lemma 3.4. With probability 1 − δ  for any r  c ∈ {0  . . .   t}  we have |N (r)

c

c

| — the number of players for whom c out of the r

| ≤(cid:0)r

(cid:1) · k

c

8c .

c

| = |{i ∈ [k] : α(r)

Proof. Let nr c = |N (r)
i = c}| be the number of players for whom c functions in
f (1)  . . .   f (r) do not have a small error. We note that n0 0 = k and n0 c = 0 for c ∈ {1  . . .   t}. The
next technical claim  whose proof appears in Appendix B.2  asserts that to prove this lemma  it is
sufﬁcient to show that for any r ∈ {1  . . .   t} and c ∈ {0  . . .   t}  nr c ≤ nr−1 c + 1
8 nr−1 c−1. Here
we assume that nr−1 −1 = 0.
Claim 3.5. Suppose that n0 0 = k  n0 c = 0 for c ∈ {1  . . .   t}  and nr c ≤ nr−1 c + 1

holds for any r ∈ {1  . . .   t} and c ∈ {0  . . .   t}. Then for any r  c ∈ {0  . . .   t}  nr c ≤(cid:0)r

8 nr−1 c−1

i

i

c

c−1

nr c =

\ Gr

(cid:12)(cid:12)(cid:12) .

c   N (r)

=nr−1 c +

  and nr c  we have

By deﬁnition of α(r)

= c}(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12){i ∈ [k] : α(r−1)
8 nr−1 c−1. Recall that (cid:101)D(r−1)

i = c}(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12){i ∈ [k] : α(r−1)
(cid:12)(cid:12)(cid:12){i ∈ [k] : α(r)
(cid:12)(cid:12)(cid:12)N (r−1)
. By Lemma 3.3  with probability 1− δ  err(cid:101)D(r−1)
8|N (r−1)
c−1
(cid:12)(cid:12)(cid:12)N (r−1)
: errDi(f (r)) > (cid:48)/2}(cid:12)(cid:12)(cid:12) ≤ 1
(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12){i ∈ N (r−1)

It remains to show that |N (r−1)
c−1
distributions in N (r−1)
c−1
errDi (f (r)) < (cid:48)

is the mixture of all
c−1
(f (r)) < (cid:48)/16. Put another
| players i ∈ N (r−1)
can have
c−1
errDi(f (r)) > (cid:48)/2. Moreover  by Lemma 3.3  for any i /∈ Gr  we have that errDi(f (r)) > (cid:48)/2.

way (cid:80)
Therefore (cid:12)(cid:12)(cid:12)N (r−1)

|. Thus  at most 1

16 · |N (r−1)
c−1

\ Gr| ≤ 1

(cid:12)(cid:12)(cid:12) =

i∈N (r−1)

c−1

\ Gr

c−1

c−1

c−1

1
8

nr−1 c−1.

c−1

8

c

8c .

(cid:1) · k
= c − 1 ∧ i /∈ Gr}(cid:12)(cid:12)(cid:12)

This completes the proof.

We now prove Theorem 3.2 using Lemma 3.4.

Proof of Theorem 3.2. We ﬁrst show that  with high probability  for any i ∈ [k]  at most 0.4t functions
i < 0.4t for all i ∈ [k]. Note that by our
among f (1)  . . .   f (t) have error greater than (cid:48)  i.e.  α(t)
  we have (8/7)0.4t ≥ k. By Lemma 3.4 and an upper bound on binomial
choice of t =
coefﬁcients  with probability 1 − δ  for any integer c ∈ [0.4t  t] 

2 log8/7(k)

(cid:108) 5

(cid:109)

(cid:18)t

(cid:19)

c

(cid:18) et

(cid:19)c · k

c

|N (t)

c

| ≤

· k
8c <

8c <

k

(8/7)c ≤ 1 

i < 0.4t for all i ∈ [k].

c = ∅. Therefore  with probability 1 − δ  α(t)

which implies that N (t)
Next  we prove that f = maj({f (r)}t
r=1) has error at most  on every player distribution. Consider
distribution Di of player i. By deﬁnition  t − α(t)
functions have error at most (cid:48) on Di. We
refer to these functions as “good” functions. Note that for any instance x that is mislabeled by
f  at least 0.5t − α(t)
i )(cid:48) ≥
(0.5t − α(t)

i good functions must make a wrong prediction. Therefore  (t − α(t)
i < 0.4t for all i ∈ [k]. Hence 
(cid:48) ≤  

i ) · errDi(f ). Moreover  with probability 1 − δ  α(t)
(cid:48) ≤ 0.6t
0.1t

errDi(f ) ≤ t − α(t)
0.5t − α(t)

i

i

i

with probability 1 − δ. This proves that Algorithm 2 (  δ)-learns F in the centralized collaborative
PAC setting.

6

Finally  we bound the sample complexity of Algorithm 2. Recall that t = Θ(ln(k)) and (cid:48) = /6. At
each iteration of Algorithm 2  we draw total of t · m(cid:48)/16 δ/(4t2) samples from t mixtures. Therefore 
over t time steps  we draw a total of
t2 · m(cid:48)/16 δ/(4t2) = O

(cid:18) ln2(k)

(cid:18) 1

(cid:18) 1

(cid:19)(cid:19)

+ ln ln(k)

(cid:18)

(cid:19)

(cid:19)

+ ln

d ln

samples for learning f (1)  . . .   f (t). Moreover  the total number samples requested for subroutine
TEST(f (r)  [k]  (cid:48)  δ/(4t)) for r = 1 . . .   t is

(cid:18) tk



O

(cid:18) k

δ

· ln

k ln

+ k ln

(cid:19)

+

ln2(k)



k

.



·

(cid:18)

·

= O

(cid:19)(cid:19)
(cid:18) ln2(k)

(cid:18) ln(k)
(cid:18)





(cid:19)

(cid:18) 1
(cid:19)
(cid:18) 1





δ

(cid:18) 1
(cid:19)(cid:19)
(cid:19)(cid:19)(cid:19)
(cid:18) 1

δ

.

δ

O



(d + k) ln

+ k ln

We conclude that the total sample complexity is

We remark that Algorithm 2 is inspired by the classic boosting scheme. Indeed  an algorithm that
is directly adapted from boosting attains a similar performance guarantee as in Theorem 3.2. The
algorithm assigns a uniform weight to each player  and learns a classiﬁer with O() error on the
mixture distribution. Then  depending on whether the function achieves an O() error on each
distribution  the algorithm updates the players’ weights  and learns the next classiﬁer from the
weighted mixture of all distributions. An analysis similar to that of AdaBoost [9] shows that the
majority vote of all the classiﬁers learned over Θ(ln(k)) iterations of the above procedure achieves
a small error on every distribution. Similar to Algorithm 2  this algorithm achieves an O(ln2(k))
overhead for the centralized setting.
4 Sample Complexity Lower Bounds
In this section  we present lower bounds on the sample complexity of collaborative PAC learning. In
Section 4.1  we show that any learning algorithm for the collaborative PAC setting incurs Ω(log(k))
overhead in terms of sample complexity. In Section 4.2  we consider the sample complexity required
for obtaining uniform convergence across F in the collaborative PAC setting. We show that Ω(k)
overhead is necessary to obtain such results.
4.1 Tight Lower Bound for the Personalized Setting
We now turn to establishing the Ω(log(k)) lower bound mentioned above. This lower bound implies
the tightness of the O(log(k)) overhead upper bound obtained by Theorem 3.1 in the personalized
setting. Moreover  the O(log2(k)) overhead obtained by Theorem 3.2 in the centralized setting is
nearly tight  up to a log(k) multiplicative factor. Formally  we prove the following theorem.
Theorem 4.1. For any k ∈ N    δ ∈ (0  0.1)  and (  δ)-learning algorithm A in the collaborative
PAC setting  there exist an instance with k players  and a hypothesis class of VC-dimension k  on
which A requires at least 3k ln[9k/(10δ)]/(20) samples in expectation.
Hard instance distribution. We show that for any k ∈ N and   δ ∈ (0  0.1)  there is a distribution
Dk  of “hard” instances  each with k players and a hypothesis class with VC-dimension k  such that
any (  δ)-learning algorithm A requires Ω(k log(k)/) samples in expectation on a random instance
drawn from the distribution  even in the personalized setting. This directly implies Theorem 4.1 
since A must take Ω(k log(k)/) samples on some instance in the support of Dk . We deﬁne Dk 
as follows:
• Instance space: Xk = {1  2  . . .   k ⊥}.
• Hypothesis class: Fk is the collection of all binary functions on Xk that map ⊥ to 0.
• Target function: f∗ is chosen from Fk uniformly at random.
• Players’ distributions: The distribution Di of player i is either a degenerate distribution that assigns
probability 1 to ⊥  or a Bernoulli distribution on {i ⊥} with Di(i) = 2 and Di(⊥) = 1 − 2.
Di is chosen from these two distributions independently and uniformly at random.

7

Note that the VC-dimension of Fk is k. Moreover  on any instance in the support of Dk   learning
in the personalized setting is equivalent to learning in the centralized setting. This is due to the fact
that given functions f1  f2  . . .   fk for the personalized setting  where fi is the function assigned to
player i  we can combine these functions into a single function f ∈ Fk for the centralized setting
by deﬁning f (⊥) = 0 and f (i) = fi(i) for all i ∈ [k]. Then  errDi(f ) ≤ errDi(fi) for all i ∈ [k].1
Therefore  without loss of generality we focus below on the centralized setting.

Lower bound for k = 1. As a building block in our proof of Theorem 4.1  we establish a lower
bound for the special case of k = 1. For brevity  let D denote the instance distribution D1 . We say
that A is an (  δ)-learning algorithm for the instance distribution D if and only if on any instance
in the support of D  with probability at least 1 − δ  A outputs a function f with error below .
The following lemma  proved in Appendix C  states that any (  δ)-learning algorithm for D takes
Ω(log(1/δ)/) samples on a random instance drawn from D.2
Lemma 4.2. For any   δ ∈ (0  0.1) and (  δ)-learning algorithm A for D  A takes at least
ln(1/δ)/(6) samples in expectation on a random instance drawn from D. Here the expecta-
tion is taken over both the randomness in the samples and the randomness in drawing the instance
from D.
Now we prove Theorem 4.1 by Lemma 4.2 and a reduction from a random instance sampled from D
to instances sampled from Dk . Intuitively  a random instance drawn from Dk  is equivalent to k
independent instances from D. We show that any learning algorithm A that simultaneously solves k
tasks (i.e.  an instance from Dk ) with probability 1 − δ can be transformed into an algorithm A(cid:48) that
solves a single task (i.e.  an instance from D) with probability 1 − O(δ/k). Moreover  the expected
sample complexity of A(cid:48) is only an O(1/k) fraction of the complexity of A. This transformation 
together with Lemma 4.2  gives a lower bound on the sample complexity of A.
Proof Sketch of Theorem 4.1. We construct an algorithm A(cid:48) for the instance distribution D from an
algorithm A that (  δ)-learns in the centralized setting. Recall that on an instance drawn from D 
A(cid:48) has access to a distribution D  i.e.  the single player’s distribution.
• A(cid:48) generates an instance (Fk  f∗ {Di}i∈[k]) from the distribution Dk  (speciﬁcally  A(cid:48) knows
the target function f∗ and the distributions)  and then chooses l ∈ [k] uniformly at random.
• A(cid:48) simulates A on instance (Fk  f∗ {Di}i∈[k])  with Dl replaced by the distribution D. Specif-
ically  every time A draws a sample from Dj for some j (cid:54)= l  A(cid:48) samples Dj and forwards the
sample to A. When A asks for a sample from Dl  A(cid:48) samples the distribution D instead and
replies to A accordingly  i.e.  A(cid:48) returns l  together with the label  if the sample is 1 (recall that
X1 = {1 ⊥})  and returns ⊥ otherwise.
• Finally  when A terminates and returns a function f on Xk  A(cid:48) checks whether errDj (f ) < 
for every j (cid:54)= l. If so  A(cid:48) returns the function f(cid:48) deﬁned as f(cid:48)(1) = f (l) and f(cid:48)(⊥) = f (⊥).
Otherwise  A(cid:48) repeats the simulation process on a new instance drawn from Dk .

Let mi be the expected number of samples drawn from the i-th distribution when A runs on an instance
drawn from Dk . We have the following two claims  whose proofs are relegated to Appendix C.
Claim 4.3. A(cid:48) is an (  10δ/(9k))-learning algorithm for D.

Claim 4.4. A(cid:48) takes at most 10/(9k)(cid:80)k
Applying Lemma 4.2 to A(cid:48) gives(cid:80)k

i=1 mi samples in expectation.

i=1 mi ≥ 3k ln[9k/(10δ)]

20

  which proves Theorem 4.1.

4.2 Lower Bound for Uniform Convergence
We next examine the sample complexity required for obtaining uniform convergence across the
hypothesis class F in the centralized collaborative PAC setting  and establish an overhead lower
bound of Ω(k). Interestingly  our centralized learning algorithm (Algorithm 2) achieves O(log2(k))
overhead — it circumvents the lower bound by not relying on uniform convergence.

1In fact  when fi ∈ Fk  errDi (f ) = errDi (fi) for all i ∈ [k].
2 Here we only assume that A is correct for instances in the support of D  rather than being correct on every

instance.

8

To be more formal  we ﬁrst need to deﬁne uniform convergence in the cooperative PAC learning
setting. We say that a hypothesis class F has the uniform convergence property with sample size
 δ if for any k distributions D1  . . .   Dk  there exist integers m1  . . .   mk that sum up to m(k)
m(k)
 δ  
such that when mi samples are drawn from Di for each i ∈ [k]  with probability 1 − δ  any function
in F that is consistent with all the m(k)
 δ samples achieves error at most  on every distribution Di.
Note that the foregoing deﬁnition is a relatively weak adaptation of uniform convergence to the
cooperative setting  as the integers mi are allowed to depend on the distributions Di. But this
observation only strengthens our lower bound  which holds despite the weak requirement.
Theorem 4.5. For any k  d ∈ N and (  δ) ∈ (0  0.1)  there exists a hypothesis class F of VC-
dimension d  such that m(k)

 δ ≥ dk(1 − δ)/(4).

Proof Sketch of Theorem 4.5. Fix k  d ∈ N and   δ ∈ (0  0.1). We deﬁne instance (F  f∗ {Di}k
i=1)
as follows. The instance space is X = ([k]×[d])∪{⊥}  and the hypothesis class F contains all binary
functions on X that map ⊥ to 0 and take value 1 on at most d points. The target function f∗ maps
every element in X to 0. Finally  the distribution of each player i ∈ [k] is given by Di((i  j)) = 2/d
for any j ∈ [d] and Di(⊥) = 1 − 2.
Note that if a sample set contains strictly less than d/2 elements in {(i∗  1)  (i∗  2)  . . .   (i∗  d)} for
some i∗  there is a consistent function in F with error strictly greater than  on Di∗  namely  the
function that maps (i  j) to 1 if and only if i = i∗ and (i∗  j) is not in the sample set.
Therefore  to achieve uniform convergence  at least d/2 elements from X \ {⊥} must be drawn from
each distribution. Since the probability that each sample is different from ⊥ is 2  drawing d/2 such
samples from k distribution requires Ω(dk/) samples.

A complete proof of Theorem 4.5 appears in Appendix C.
Acknowledgments
We thank the anonymous reviewers for their helpful remarks and suggesting an alternative boosting-
based approach for the centralized setting. This work was partially supported by the NSF grants
CCF-1525971  CCF-1536967  CCF-1331175  IIS-1350598  IIS-1714140  CCF-1525932  and CCF-
1733556  Ofﬁce of Naval Research grants N00014-16-1-3075 and N00014-17-1-2428  a Sloan
Research Fellowship  and a Microsoft Research Ph.D. fellowship. This work was done while Avrim
Blum was working at Carnegie Mellon University.
References
[1] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  1999.

[2] Maria Florina Balcan  Avrim Blum  Shai Fine  and Yishay Mansour. Distributed learning  com-
munication complexity and privacy. In Proceedings of the 25th Conference on Computational
Learning Theory (COLT)  pages 26.1–26.22  2012.

[3] Jonathan Baxter. A Bayesian/information theoretic model of learning to learn via multiple task

sampling. Machine learning  28(1):7–39  1997.

[4] Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research 

12:149–198  2000.

[5] Shai Ben-David and Reba Schuller. Exploiting task relatedness for mulitple task learning.
In Proceedings of the 16th Conference on Computational Learning Theory (COLT)  pages
567–580  2003.

[6] Shai Ben-David  John Blitzer  Koby Crammer  Alex Kulesza  Fernando Pereira  and Jen-
nifer Wortman Vaughan. A theory of learning from different domains. Machine learning  79(1):
151–175  2010.

[7] Rich Caruana. Multitask learning. Machine Learning  28(1):41–75  1997.

9

[8] Ofer Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao. Optimal distributed online
prediction. In Proceedings of the 28th International Conference on Machine Learning (ICML) 
pages 713–720  2011.

[9] Yoav Freund and Robert E Schapire. A desicion-theoretic generalization of on-line learning and
an application to boosting. In Proceedings of the 2nd European Conference on Computational
Learning Theory (EuroCOLT)  pages 23–37  1995.

[10] Abhishek Kumar and Hal Daumé III. Learning task grouping and overlap in multi-task learning.
In Proceedings of the 29th International Conference on Machine Learning (ICML)  pages
1103–1110  2012.

[11] Yishay Mansour  Mehryar Mohri  and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. In Proceedings of the 22nd Conference on Computational Learning
Theory (COLT)  pages 19–30  2009.

[12] Yishay Mansour  Mehryar Mohri  and Afshin Rostamizadeh. Domain adaptation with multiple
sources. In Proceedings of the 23rd Annual Conference on Neural Information Processing
Systems (NIPS)  pages 1041–1048  2009.

[13] Massimiliano Pontil and Andreas Maurer. Excess risk bounds for multitask learning with trace
norm regularization. In Proceedings of the 26th Conference on Computational Learning Theory
(COLT)  pages 55–76  2013.

[14] Leslie G. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142 

1984.

[15] Jialei Wang  Mladen Kolar  and Nathan Srerbo. Distributed multi-task learning. In Proceedings
of the 19th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
751–760  2016.

10

,Jean-Baptiste SCHIRATTI
Stéphanie ALLASSONNIERE
Olivier Colliot
Stanley DURRLEMAN
Ping Li
Michael Mitzenmacher
Martin Slawski
Avrim Blum
Nika Haghtalab
Ariel Procaccia
Mingda Qiao