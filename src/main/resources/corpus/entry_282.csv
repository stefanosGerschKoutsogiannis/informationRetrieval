2016,A posteriori error bounds for joint matrix decomposition problems,Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices  with applications in signal processing and machine learning. We consider the problem of approximate joint matrix triangularization when the matrices in M are jointly diagonalizable and real  but we only observe a set M' of noise perturbed versions of the matrices in M. Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M' and any exact joint triangularizer of the matrices in M. The bound depends only on the observable matrices in M' and the noise level. In particular  it does not depend on optimization specific properties of the triangularizer  such as its proximity to critical points  that are typical of existing bounds in the literature. To our knowledge  this is the first a posteriori bound for joint matrix decomposition. We demonstrate the bound on synthetic data for which the ground truth is known.,A Posteriori Error Bounds for Joint Matrix

Decomposition Problems

Nicolò Colombo

nicolo.colombo@ucl.ac.uk

Department of Statistical Science

University College London

Nikos Vlassis
Adobe Research

San Jose  CA

vlassis@adobe.com

Abstract

Joint matrix triangularization is often used for estimating the joint eigenstructure
of a set M of matrices  with applications in signal processing and machine learning.
We consider the problem of approximate joint matrix triangularization when the
matrices in M are jointly diagonalizable and real  but we only observe a set M’
of noise perturbed versions of the matrices in M. Our main result is a ﬁrst-order
upper bound on the distance between any approximate joint triangularizer of the
matrices in M’ and any exact joint triangularizer of the matrices in M. The bound
depends only on the observable matrices in M’ and the noise level. In particular  it
does not depend on optimization speciﬁc properties of the triangularizer  such as its
proximity to critical points  that are typical of existing bounds in the literature. To
our knowledge  this is the ﬁrst a posteriori bound for joint matrix decomposition.
We demonstrate the bound on synthetic data for which the ground truth is known.

1

Introduction

Joint matrix decomposition problems appear frequently in signal processing and machine learning 
with notable applications in independent component analysis [7]  canonical correlation analysis [20] 
and latent variable model estimation [5  4]. Most of these applications reduce to some instance of
a tensor decomposition problem  and the growing interest in joint matrix decomposition is largely
motivated by such reductions. In particular  in the past decade several ‘matricization’ methods have
been proposed for factorizing tensors by computing the joint decomposition of sets of matrices
extracted from slices of the tensor (see  e.g.  [10  22  17  8]).
In this work we address a standard joint matrix decomposition problem  in which we assume a set of
jointly diagonalizable ground-truth (unobserved) matrices

M◦ = {Mn = V diag([Λn1  . . .   Λnd])V −1  V ∈ Rd×d  Λ ∈ RN×d}N

n=1  

(1)

which have been corrupted by noise and we observe their noisy versions:

Mσ = { ˆMn = Mn + σRn  Mn ∈ M◦  Rn ∈ Rd×d  (cid:107)Rn(cid:107) ≤ 1}N

(2)
The matrices ˆMn ∈ Mσ are the only observed quantities. The scalar σ > 0 is the noise level  and
the matrices Rn are arbitrary noise matrices with Frobenius norm (cid:107)Rn(cid:107) ≤ 1. The key problem
is to estimate from the observed matrices in Mσ the joint eigenstructure V  Λ of the ground-truth
matrices in M◦. One way to address this estimation problem is by trying to approximately jointly
diagonalize the observed matrices in Mσ  for instance by directly searching for an invertible matrix
that approximates V in (1). This approach is known as nonorthogonal joint diagonalization [23  15 
18]  and is often motivated by applications that reduce to nonsymmetric CP tensor decomposition
(see  e.g.  [20]).

n=1 .

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

An alternative approach to the above estimation problem (in the general case of nonorthogonal V ) is
via joint triangularization  also known as joint or simultaneous Schur decomposition [1  13  11  12 
22  8]. Under mild conditions [14]  the ground-truth matrices in M◦ can be jointly triangularized 
that is  there exists an orthogonal matrix U◦ that simultaneously renders all matrices U(cid:62)
◦ MnU◦ upper
triangular:

(3)
where low(A) is the strictly lower triangular part of A  i.e.  [low(A)]ij = Aij if i > j and 0 otherwise.
On the other hand  when σ > 0 the observed matrices in Mσ can only be approximately jointly
triangularized  for instance by solving the following optimization problem

for all n = 1  . . . N 

◦ MnU◦) = 0 

low(U(cid:62)

min
U∈O(d)

L(U )  where L(U ) =

1
N

(cid:107)low(U(cid:62) ˆMnU )(cid:107)2  

(4)

where (cid:107)·(cid:107) denotes Frobenius norm and optimization is over the manifold O(d) of orthogonal matrices.
The optimization problem can be addressed by Jacobi-like methods [13]  or Newton-like methods that
optimize directly on the O(d) manifold [8]. For any feasible U in (4)  the joint eigenvalues Λ in (1)
can be estimated from the diagonals of U(cid:62) ˆMnU. This approach has been used in nonsymmetric CP
tensor decomposition [22  8] and other applications [9  13].
We also note two related problems. In the special case that the ground-truth matrices Mn in M◦ are
symmetric  the matrix V in (1) is orthogonal  and the estimation problem is known as orthogonal joint
diagonalization [7]. Our results apply to this special case too. Another problem is joint diagonalization
by congruence [6  3  17]  in which the matrix V −1 in (1) is replaced by V T . In that case the matrix Λ
in (1) does not contain the joint eigenvalues  and our results do not apply directly.

Contributions We are addressing the joint matrix triangularization problem deﬁned via (4)  under
the model assumptions (1)  (2)  and (3). The optimization problem (4) is nonconvex  and hence it
is expected to be hard to solve to global optimality in general. Therefore  error bounds are needed
that can assess the quality of a solution produced by some algorithm that tries to solve (4). Our
main result (Theorem 1) is an error bound that allows to directly assess a posteriori the quality of
any feasible triangularizer U in (4)  in terms of its proximity to the (unknown) exact trangularizer
of the ground-truth matrices in M◦  regardless of the algorithm used for optimization. The bound
depends only on observable quantities and the noise parameter σ in (2). The parameter σ can often be
bounded by a function of the sample size  as in problems involving empirical moment matching [4].
Our approach draws on the perturbation analysis of the Schur decomposition of a single matrix [16].
To our knowledge  our bound in Theorem 1 is the ﬁrst a posteriori error bound for joint matrix
decomposition problems. Existing bounds in the literature have a dependence on the ground-truth
(and hence unobserved) matrices [11  17]  the proximity of a feasible U to critical points of the
objective function [6]  or the amount of collinearity between the columns of the matrix Λ in (1) [3].
Our error bound is free of such dependencies. Outside the context of joint matrix decomposition  a
posteriori error bounds have found practical uses in nonconvex optimization [19] and the design of
algorithms [21].

Notation All matrices  vectors  and numbers are real. When the context is clear we use 1 to
denote the identity matrix. We use (cid:107) · (cid:107) for matrix Frobenius norm and vector l2 norm. O(d) is the
manifold of orthogonal matrices U such that U(cid:62)U = 1. The matrix commutator [A  B] is deﬁned
by [A  B] = AB − BA. We use ⊗ for Kronecker product. For a matrix A  we denote by λi(A) its
ith eigenvalue  λmin(A) its smallest eigenvalue  κ(A) its condition number  vec(A) its columnwise
vectorization  and low(A) and up(A) its strictly lower triangular and strictly upper triangular parts 
respectively. Low is a binary diagonal matrix deﬁned by vec(low(A)) = Low vec(A). Skew is a
skew-symmetric projector deﬁned by Skew vec(A) = vec(A − A(cid:62)). PLow is a d(d − 1)/2 × d2
binary matrix with orthogonal rows  which projects to the subspace of vectorized strictly lower
triangular matrices  such that PLowP (cid:62)
LowPLow = Low. For example  for d = 3  one
has Low = diag([0  1  1  0  0  1  0  0  0]) and

Low = 1 and P (cid:62)

N(cid:88)

n=1

(cid:32) 0

Plow =

0
0

(cid:33)

.

1
0
0

0
1
0

0
0
0

0
0
0

0
0
1

0
0
0

0
0
0

0
0
0

2

2 Perturbation of joint triangularizers

The objective function (4) is continuous in the parameter σ. This implies that  for σ small enough 
the approximate joint triangularizers of the observed matrices ˆMn ∈ Mσ can be expected to be
perturbations of the exact triangularizers of the ground-truth matrices Mn ∈ M◦. To formalize this 
we express each feasible triangularizer U in (4) as a function of some exact triangularizer U◦ of the
ground-truth matrices  as follows:

U = U◦eαX   where X = −X(cid:62) 

(cid:107)X(cid:107) = 1  α > 0 

(5)

where e denotes matrix exponential and X is a skew-symmetric matrix. Such an expansion holds for
any pair U  U◦ of orthogonal matrices with det(U ) = det(U◦) (see  e.g.  [2]). The scalar α in (5) can
be interpreted as the ‘distance’ between the matrices U and U◦. Our main result is an upper bound on
this distance:
Theorem 1. Let M◦ and Mσ be the sets of matrices deﬁned in (1) and (2)  respectively. Let U be a
feasible solution of the optimization problem (4)  with corresponding value L(U ). Then there exists
an orthogonal matrix U◦ that is an exact joint triangularizer of M◦  such that U can be expressed as
a perturbation of U◦ according to (5)  with α obeying

N(cid:88)

n=1

ˆτ =

1
2N

+ O(α2)  

where

(cid:0)1 ⊗ (U(cid:62) ˆMnU ) − (U(cid:62) ˆM(cid:62)

n U ) ⊗ 1(cid:1) Skew P (cid:62)

low.

(6)

(7)

ˆT (cid:62)

n

ˆTn 

ˆTn = Plow

(cid:112)L(U ) + σ
(cid:112)λmin(ˆτ )

α ≤

Proof. Let U◦ be the exact joint triangularizer of M◦ that is the nearest to U and det U = det U◦.
Then U = U◦eαX for some unit-norm skew-symmetric matrix X and scalar α > 0. Using the
expansion eαX = I + αX + O(α2) and the fact that X is skew-symmetric  we can write  for any
n = 1  . . .   N 

U(cid:62) ˆMnU = U(cid:62)

◦ ˆMnU◦ + α[U(cid:62)

(8)
where [· ·] denotes matrix commutator. Applying the low(·) operator and using the facts that
αU(cid:62)
◦ MnU◦) = 0  for any n = 1  . . .   N  we can write
(9)

α low([U(cid:62) ˆMnU  X]) = low(U(cid:62) ˆMnU ) − σ low(U(cid:62)

◦ ˆMnU◦ = αU(cid:62) ˆMnU + O(α2) and low(U(cid:62)

◦ ˆMnU◦  X] + O(α2) 

◦ RnU◦) + O(α2).

Stacking (9) over n  then taking Frobenius norm  and applying the triangle inequality together with
the fact (cid:107)low(U(cid:62)

◦ RnU◦(cid:107) = (cid:107)Rn(cid:107) ≤ 1 for all n = 1  . . .   N  we get

◦ RnU◦)(cid:107) ≤ (cid:107)U(cid:62)

(cid:19) 1
2 ≤(cid:112)NL(U ) + σ

√

N + O(α2) 

(10)

α

(cid:107)low([U(cid:62) ˆMnU  X])(cid:107)2

(cid:18) N(cid:88)

n=1

where we used the deﬁnition of L(U ) from (4). The rest of the proof involves computing a lower
bound of the left-hand side of (10) that holds for all X. Since (cid:107)low(A)(cid:107) = (cid:107)Plowvec(A)(cid:107)  we can
rewrite the argument of each norm in the left-hand side of (10) as

low([U(cid:62) ˆMnU  X]) = Plow vec([U(cid:62) ˆMnU  X])

(cid:0)1 ⊗ (U(cid:62) ˆMnU ) − (U(cid:62) ˆM(cid:62)

n U ) ⊗ 1(cid:1) vec(X) 

= Plow

and  due to the skew-symmetry of X  we can write

vec(X) = vec(low(X) + up(X)) = vec(low(X) − low(X)(cid:62))

= Skew Low vec(X) = Skew P (cid:62)

low Plow vec(X) .

Hence  for all n = 1  . . .   N  we can write (cid:107)low([U(cid:62) ˆMnU  X])(cid:107)2 = (cid:107) ˆTnx(cid:107)2 = x(cid:62) ˆT (cid:62)
ˆTnx  where
x = Plow vec(X) and ˆTn is deﬁned in (7). The inequality in (6) then follows by using the inequality
x(cid:62)Ax ≥ (cid:107)x(cid:107)2 λmin(A)  which holds for any symmetric matrix A  and noting that (cid:107)x(cid:107)2 = 1
2 (since
x contains the lower triangular part of X and (cid:107)X(cid:107)2 = 1).

n

3

(11)
(12)

(13)
(14)

(cid:112)λmin(ˆτ ) ≥

√
κ(V )2  

Γ

(15)

For general Mσ  an analytical expression of λmin (ˆτ ) in (6) is not available. However  it is straight-
forward to compute λmin (ˆτ ) numerically since all quantities in (7) are observable. Moreover  it
is possible to show (see Theorem 2) that in the limit σ → 0 and under certain conditions on the
ground-truth matrices in M◦  the operator τ = limσ→0 ˆτ is nonsingular  i.e.  λmin(τ ) > 0. Since
both ˆτ and L are continuous in σ for σ → 0  the boundedness of the right-hand side of (6) is
guaranteed  for σ small enough  by eigenvalue perturbation theorems.
Theorem 2. The operator ˆτ deﬁned in (7) obeys

lim
σ→0

(cid:80)N
n=1(λi(Mn) − λj(Mn))2  and the matrix V is deﬁned in (1).

1
2N

where Γ = mini>j
The proof is given in the Appendix. The quantity Γ can be interpreted as a ‘joint eigengap’ of M◦ (see
also [17] for a similar deﬁnition in the context of joint diagonalization by congruence). Theorem 2
implies that limσ→0 λmin(ˆτ ) > 0 if Γ > 0  and the latter is guaranteed under the following condition:
Condition 1. For every i (cid:54)= j  i  j = 1  . . .   d  there exists at least n ∈ {1  . . .   N} such that

λi(Mn) (cid:54)= λj(Mn)  where Mn ∈ M◦ .

(16)

3 Experiments

To assess the tightness of the inequality in (6)  we created a set of synthetic problems in which
the ground truth is known  and we evaluated the bounds obtained from (6) against the true values.
Each problem involved the approximate triangularization of a set of randomly generated nearly joint
diagonalizable matrices of the form ˆMn = V ΛnV −1 + σRn  with Λn diagonal and (cid:107)Rn(cid:107) = 1  for
n = 1  . . .   N. For each set Mσ = { ˆMn}N
n=1  two approximate joint triangularizers were computed
by optimizing (4) using two different iterative algorithms  the Gauss-Newton algorithm [8]  and the
Jacobi algorithm [13] (our implementation)  initialized with the same random orthogonal matrix. The
obtained solutions U (which may not be the global optima) were then used to compute the empirical
bound α from (6)  as well as the actual distance parameter αtrue = (cid:107) log U(cid:62)U◦(cid:107)  with U◦ being the
global optimum of the unperturbed problem (σ = 0) that is closest to U and has the same determinant.
Locating the closest U◦ to the given U required checking all 2dd! possible exact triangularizers of M◦ 
thus we restricted our empirical evaluation to the case d = 5. We considered two settings  N = 5
and N = 100  and several different noise levels obtained by varying the perturbation parameter σ.
The ﬁrst two graphs in Figure 1 show the value of the noise level σ against the values of αtrue = αtrue(U )
and the corresponding empirical bounds α = α(U ) from (6)  where U are the solutions found by the
Gauss-Newton algorithm. (Very similar results were obtained using the Jacobi algorithm.) All values
are obtained by averaging over 10 equivalent experiments  and the errorbars show the corresponding
standard deviations. For the same set of solutions U  the third graph in Figure 1 shows the ratios α
αtrue .
The experiments show that  at least for small N  the bound (6) produces a reasonable estimate of
the true perturbation parameter αtrue. However  our bound does not fully capture the concentration
that is expected (and observed in practice) for large sets of nearly jointly decomposable matrices
(note  for instance  the average value of αtrue in Figure 1  for N = 5 vs N = 100). This is most likely
◦ RnU◦)(cid:107) ≤ 1 and the use of the triangle inequality in
due to the introduced approximation (cid:107)low(U(cid:62)
(10) (see proof of Theorem 1)  which are needed to separate the observable terms U(cid:62) ˆMnU from the
unobservable terms U(cid:62)
0 RnU0 in the right-hand side of (9). Extra assumptions on the distribution of
the random matrices Rn can possibly allow obtaining tighter bounds in a probabilistic setting.

4 Conclusions

We addressed a joint matrix triangularization problem that involves ﬁnding an orthogonal matrix that
approximately triangularizes a set of noise-perturbed jointly diagonalizable matrices. The setting can
have many applications in statistics and signal processing  in particular in problems that reduce to a
nonsymmetric CP tensor decomposition [4  8  20]. The joint matrix triangularization problem can be
cast as a nonconvex optimization problem over the manifold of orthogonal matrices  and it can be

4

N(cid:88)

n=1

τ =

1
2N

(cid:0)1 ⊗ (U(cid:62)

Figure 1: The empirical bound α from (6) vs the true distance αtrue  on synthetic experiments.

solved numerically but with no success guarantees. We have derived a posteriori upper bounds on
the distance between any approximate triangularizer (obtained by any algorithm) and the (unknown)
solution of the underlying unperturbed problem. The bounds depend only on empirical quantities
and hence they can be used to asses the quality of any feasible solution  even when the ground truth
is not known. We established that  under certain conditions  the bounds are well deﬁned when the
noise is small. Synthetic experiments suggest that the obtained bounds are tight enough to be useful
in practice.
In future work  we want to apply our analysis to related problems  such as nonnegative tensor
decomposition and simultaneous generalized Schur decomposition [11]  and to empirically validate
the obtained bounds in machine learning applications [4].

A Proof of Theorem 2
The proof consists of two steps. The ﬁrst step consists of showing that in the limit σ → 0 the operator
ˆτ deﬁned in (7) tends to a simpler operator  τ  which depends on ground-truth quantities only. The
second step is to derive a lower bound on the smallest eigenvalue of the operator τ.
Let τ be deﬁned by

n U◦) ⊗ 1(cid:1)Plow 
(cid:12)(cid:12)(cid:12)σ=0

= Tn .

T (cid:62)
n Tn 

Tn = Plow

◦ MnU◦) − (U(cid:62)

◦ M(cid:62)

(17)

(cid:12)(cid:12)(cid:12)σ=0

where Mn ∈ M◦  and U◦ is the exact joint triangularizer of M◦ that is closest to  and has the same
determinant as  U  the approximate joint triangularizer that is used to deﬁne ˆτ. Proving that ˆτ → τ
as σ → 0 is equivalent to showing that

(cid:0)1 ⊗ (U(cid:62) ˆMnU ) − (U(cid:62) ˆM(cid:62)

n U ) ⊗ 1(cid:1) Skew P (cid:62)

ˆTn

low

= Plow

(18)
Since for all n = 1  . . .   N  one has ˆMn → Mn when σ → 0  we need to prove that U → U◦ and
that we can remove the Skew operator on the right.
We ﬁrst show that U = U◦eαX → U◦  that is  α → 0 as σ → 0. Assume that the descent algorithm
used to obtain U is initialized with Uinit obtained from the Schur decomposition of ˆM∗ ∈ Mσ. Let
U◦ be the exact triangularizer of M◦ closest to Uinit and Uopt be the local optimum of the joint
triangularization objective closest to Uinit. Then  as σ → 0 one has Uopt → U◦  by continuity of the
objective in σ  and also Uinit → U◦ due to the perturbation properties of the Schur decomposition.
This implies U → U◦  and hence α → 0.
Then  it is easy to prove that Plow(1 ⊗ (U(cid:62)
low = Plow(1 ⊗
n U◦) ⊗ 1)P (cid:62)
◦ MnU◦) − (U(cid:62)
(U(cid:62)
low by considering the action of the two operators on x =
Plowvec(X)  with X = −X(cid:62). One has

n U◦) ⊗ 1) Skew P (cid:62)

◦ MnU◦) − (U(cid:62)

◦ M(cid:62)

◦ M(cid:62)

Plow(1 ⊗ (U(cid:62)
= P (cid:62)

lowvec(cid:0)low[U(cid:62)

◦ MnU◦  low(X)](cid:1) = Plow(1 ⊗ (U(cid:62)

n U◦) ⊗ 1) Skew P (cid:62)

◦ M(cid:62)

◦ MnU◦) − (U(cid:62)

low = P (cid:62)

where in the second line we used the fact that U(cid:62)
as σ → 0.

◦ M(cid:62)

lowvec(cid:0)low[U(cid:62)

◦ MnU◦  X](cid:1)

◦ MnU◦) − (U(cid:62)

low (19)
n U◦ is upper triangular. This shows that ˆτ → τ

n U◦) ⊗ 1)P (cid:62)

◦ M(cid:62)

5

-8-6-4-20log(<)-10-8-6-4-202log( )N=5 true -8-6-4-20log(<)-10-8-6-4-202log( )N=100 true -8-7-6-5-4-3-2-10log(<)020406080100 / true N=5 N=100The second part of the proof consists of bounding the smallest eigenvalue of τ. We will make use of
the following identity that holds when A and C are upper triangular:

low(ABC) = low(A low(B) C)  

(20)

from which we get the following identity when A and C are upper triangular:

Low vec(ABC) = Low (C(cid:62) ⊗ A) Low vec(B) .
lowTnx = Lowvec([U(cid:62)

◦ MnU◦) and it can be shown that1

◦ MnU◦  low(X)]) = Lowvec(U(cid:62)

In particular  one has P (cid:62)
low(X)U(cid:62)
Low vec( ˜V Λn ˜V −1low(X)) = Low ( ˜V −T ⊗ ˜V ) Low (I ⊗ Λn) Low ( ˜V (cid:62) ⊗ ˜V −1) Low vec(X) (29)
and
Low vec(low(X) ˜V Λn ˜V −1) = Low ( ˜V −T ⊗ ˜V ) Low (Λn ⊗ I) Low ( ˜V (cid:62) ⊗ ˜V −1) Low vec(X) (30)
◦ MnU◦ = ˜V Λn ˜V −1. Now  since
where ˜V and ˜V −1 are upper triangular matrices deﬁned by U(cid:62)
˜V = U(cid:62)
◦ V   where V is deﬁned via the spectral decomposition Mn = V ΛnV −1  we can rewrite the
operator Tn as

(21)
◦ MnU◦low(X) −

Tn = Plow(U(cid:62)

◦ V −T ⊗ U(cid:62)

◦ V )Low(1 ⊗ Λn − Λn ⊗ 1)Low(V T U◦ ⊗ V −1U◦)P (cid:62)

low   (31)

and use the following inequality for the smallest eigenvalue of τ = 1
2N

(cid:80)N
n=1 T (cid:62)

n Tn:

where

λmin(τ ) ≥

1
2N

λmin(A) λmin(B) λmin(C) 

A = Plow(U(cid:62)

(cid:32) N(cid:88)

◦ V ⊗ U(cid:62)

◦ V −T )P (cid:62)
low 
(1 ⊗ Λn − Λn ⊗ 1)2

(cid:33)

P (cid:62)
low 

(32)

(33)

(34)

Now  it is easy to show that λmin(A) = λmin(C) ≥ 1
A and C are obtained by deleting certain rows and columns of U(cid:62)
respectively. The matrix B is a diagonal matrix with entries given by

B = Plow
C = Plow(V −1U◦ ⊗ V T U◦)P (cid:62)
low.

n=1

N(cid:88)

([Λn]ii − [Λn]jj)2 

(35)
κ(V )2 since the d(d−1)/2×d(d−1)/2 matrices
◦ V ⊗ U(cid:62)
◦ V −T and V −1U◦⊗ V T U◦
i−1(cid:88)
(cid:80)N
n=1(λi(Mn) − λj(Mn))2 and
(cid:80)N
(37)
n=1(λi(Mn)− λj(Mn))2 is a ‘joint eigengap’ of the ground-truth matrices
(cid:3)

λmin(ˆτ ) ≥ Γ

k = j − i +

(d − a) 

κ(V )4  

lim
σ→0

(36)

n=1

a=1

[B]kk =

with 0 < i < j and j = 1  . . . d. This implies λmin(B) = mini<j

where Γ = mini>j
Mn ∈ M◦.

1
2N

1For any matrix Y one has

−1Y ) =
−1Y ˜V ˜V

Low vec( ˜V Λn ˜V
Low vec( ˜V Λn ˜V
Low ( ˜V
Low ( ˜V
Low ( ˜V

−1) =
−T ⊗ ˜V ) Low vec(Λn ˜V
−1Y ˜V ) =
−T ⊗ ˜V ) Low (I ⊗ Λn) Low vec( ˜V
−T ⊗ ˜V ) Low (I ⊗ Λn) Low ( ˜V

−1Y ˜V ) =

(cid:62) ⊗ ˜V

−1) Low vec(Y )

and similarly

Low vec(Y ˜V Λn ˜V
Low ( ˜V

−1) =

−T ⊗ ˜V ) Low (Λn ⊗ I) Low ( ˜V

(cid:62) ⊗ ˜V

−1) Low vec(Y )

6

(22)
(23)
(24)
(25)
(26)

(27)
(28)

References
[1] K. Abed-Meraim and Y. Hua. A least-squares approach to joint Schur decomposition. In
Acoustics  Speech and Signal Processing  1998. Proceedings of the 1998 IEEE International
Conference on  volume 4  pages 2541–2544. IEEE  1998.

[2] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press  2009.

[3] B. Afsari. Sensitivity analysis for the problem of matrix joint diagonalization. SIAM Journal on

Matrix Analysis and Applications  30(3):1148–1171  2008.

[4] A. Anandkumar  R. Ge  D. Hsu  S. Kakade  and M. Telgarsky. Tensor decompositions for
learning latent variable models. Journal of Machine Learning Research  15:2773–2832  2014.

[5] B. Balle  A. Quattoni  and X. Carreras. A spectral learning algorithm for ﬁnite state transducers.
In Machine Learning and Knowledge Discovery in Databases  pages 156–171. Springer  2011.

[6] J.-F. Cardoso. Perturbation of joint diagonalizers. Telecom Paris  Signal Department  Technical

Report 94D023  1994.

[7] J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM journal

on matrix analysis and applications  17(1):161–164  1996.

[8] N. Colombo and N. Vlassis. Tensor decomposition via joint matrix Schur decomposition. In

Proc. 33rd International Conference on Machine Learning  2016.

[9] R. M. Corless  P. M. Gianni  and B. M. Trager. A reordered Schur factorization method for zero-
dimensional polynomial systems with multiple roots. In Proceedings of the 1997 international
symposium on Symbolic and algebraic computation  pages 133–140. ACM  1997.

[10] L. De Lathauwer. A link between the canonical decomposition in multilinear algebra and
simultaneous matrix diagonalization. SIAM Journal on Matrix Analysis and Applications 
28(3):642–666  2006.

[11] L. De Lathauwer  B. De Moor  and J. Vandewalle. Computation of the canonical decomposition
by means of a simultaneous generalized Schur decomposition. SIAM Journal on Matrix Analysis
and Applications  26(2):295–327  2004.

[12] T. Fu  S. Jin  and X. Gao. Balanced simultaneous Schur decomposition for joint eigenvalue esti-
mation. In Communications  Circuits and Systems Proceedings  2006 International Conference
on  volume 1  pages 356–360. IEEE  2006.

[13] M. Haardt and J. A. Nossek. Simultaneous Schur decomposition of several nonsymmetric
matrices to achieve automatic pairing in multidimensional harmonic retrieval problems. Signal
Processing  IEEE Transactions on  46(1):161–169  1998.

[14] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press  2nd edition  2012.

[15] R. Iferroudjene  K. A. Meraim  and A. Belouchrani. A new Jacobi-like method for joint
diagonalization of arbitrary non-defective matrices. Applied Mathematics and Computation 
211(2):363–373  2009.

[16] M. Konstantinov  P. H. Petkov  and N. Christov. Nonlocal perturbation analysis of the Schur
system of a matrix. SIAM Journal on Matrix Analysis and Applications  15(2):383–392  1994.

[17] V. Kuleshov  A. Chaganty  and P. Liang. Tensor factorization via matrix factorization. In 18th

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

[18] X. Luciani and L. Albera. Joint eigenvalue decomposition using polar matrix factorization. In
International Conference on Latent Variable Analysis and Signal Separation  pages 555–562.
Springer  2010.

[19] J.-S. Pang. A posteriori error bounds for the linearly-constrained variational inequality problem.

Mathematics of Operations Research  12(3):474–484  1987.

7

[20] A. Podosinnikova  F. Bach  and S. Lacoste-Julien. Beyond CCA: Moment matching for multi-

view models. In Proc. 33rd International Conference on Machine Learning  2016.

[21] S. Prudhomme  J. T. Oden  T. Westermann  J. Bass  and M. E. Botkin. Practical methods for a
posteriori error estimation in engineering applications. International Journal for Numerical
Methods in Engineering  56(8):1193–1224  2003.

[22] S. H. Sardouie  L. Albera  M. B. Shamsollahi  and I. Merlet. Canonical polyadic decomposition
of complex-valued multi-way arrays based on simultaneous Schur decomposition. In Acoustics 
Speech and Signal Processing (ICASSP)  2013 IEEE International Conference on  pages 4178–
4182. IEEE  2013.

[23] A. Souloumiac. Nonorthogonal joint diagonalization by combining Givens and hyperbolic

rotations. IEEE Transactions on Signal Processing  57(6):2222–2231  2009.

8

,Nicolo Colombo
Nikos Vlassis
Liping Liu
Susan Athey
David Blei