2017,Dynamic Importance Sampling for Anytime Bounds of the Partition Function,Computing the partition function is a key inference task in many graphical models. In this paper  we propose a dynamic importance sampling scheme that provides  anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies  heuristic search  variational bounds  and Monte Carlo methods  blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search and probabilistic bounds of the partition function. By using an intelligently chosen weighted average over the samples  we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling and the long-term benefits of an improved proposal from search. This gives significantly improved anytime behavior  and more flexible trade-offs between memory  time  and solution quality. We demonstrate the effectiveness of our approach empirically  on real-world problem instances taken from recent UAI competitions.,Dynamic Importance Sampling for Anytime Bounds

of the Partition Function

Qi Lou

Computer Science

Univ. of California  Irvine
Irvine  CA 92697  USA
qlou@ics.uci.edu

Rina Dechter

Computer Science

Univ. of California  Irvine
Irvine  CA 92697  USA
dechter@ics.uci.edu

Alexander Ihler
Computer Science

Univ. of California  Irvine
Irvine  CA 92697  USA
ihler@ics.uci.edu

Abstract

Computing the partition function is a key inference task in many graphical models.
In this paper  we propose a dynamic importance sampling scheme that provides
anytime ﬁnite-sample bounds for the partition function. Our algorithm balances
the advantages of the three major inference strategies  heuristic search  variational
bounds  and Monte Carlo methods  blending sampling with search to reﬁne a
variationally deﬁned proposal. Our algorithm combines and generalizes recent
work on anytime search [16] and probabilistic bounds [15] of the partition function.
By using an intelligently chosen weighted average over the samples  we construct
an unbiased estimator of the partition function with strong ﬁnite-sample conﬁdence
intervals that inherit both the rapid early improvement rate of sampling and the
long-term beneﬁts of an improved proposal from search. This gives signiﬁcantly
improved anytime behavior  and more ﬂexible trade-offs between memory  time 
and solution quality. We demonstrate the effectiveness of our approach empirically
on real-world problem instances taken from recent UAI competitions.

1

Introduction

Probabilistic graphical models  including Bayesian networks and Markov random ﬁelds  provide a
framework for representing and reasoning with probabilistic and deterministic information [5  6  8].
Reasoning in a graphical model often requires computing the partition function  or normalizing
constant of the underlying distribution. Exact computation of the partition function is known to be
#P-hard [19] in general  leading to the development of many approximate schemes. Two important
properties for a good approximation are that (1) it provides bounds or conﬁdence guarantees on the
result  so that the degree of approximation can be measured; and that (2) it can be improved in an
anytime manner  so that the approximation becomes better as more computation is available.
In general  there are three major paradigms for approximate inference: variational bounds  heuristic
search  and Monte Carlo sampling. Each method has advantages and disadvantages. Variational
bounds [21]  and closely related approximate elimination methods [7  14] provide deterministic
guarantees on the partition function. However  these bounds are not anytime; their quality often
depends on the amount of memory available  and do not improve without additional memory. Search
algorithms [12  20  16] explicitly enumerate over the space of conﬁgurations and eventually provide
an exact answer; however  while some problems are well-suited to search  others only improve their
quality very slowly with more computation. Importance sampling [e.g.  4  15] gives probabilistic
bounds that improve with more samples at a predictable rate; in practice this means bounds that
improve rapidly at ﬁrst  but are slow to become very tight. Several algorithms combine two strategies:
approximate hash-based counting combines sampling (of hash functions) with CSP-based search [e.g. 
3  2] or other MAP queries [e.g.  9  10]  although these are not typically formulated to provide anytime

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

behavior. Most closely related to this work are [16] and [15]  which perform search and sampling 
respectively  guided by variational bounds.
In this work  we propose a dynamic importance sampling algorithm that provides anytime probabilis-
tic bounds (i.e.  they hold with probability 1 − δ for some conﬁdence parameter δ). Our algorithm
interleaves importance sampling with best ﬁrst search [16]  which is used to reﬁne the proposal
distribution of successive samples. In practice  our algorithm enjoys both the rapid bound improve-
ment characteristic of importance sampling [15]  while also beneﬁting signiﬁcantly from search
on problems where search is relatively effective  or when given enough computational resources 
even when these points are not known in advance. Since our samples are drawn from a sequence of
different  improving proposals  we devise a weighted average estimator that upweights higher-quality
samples  giving excellent anytime behavior.

Motivating example. We illustrate the focus and contribu-
tions of our work on an example problem instance (Fig. 1).
Search [16] provides strict bounds (gray) but may not improve
rapidly  particularly once memory is exhausted; on the other
hand  importance sampling [15] provides probabilistic bounds
(green) that improve at a predictable rate  but require more and
more samples to become tight. We ﬁrst describe a “two stage”
sampling process that uses a search tree to improve the baseline
bound from which importance sampling starts (blue)  greatly
improving its long-term performance  then present our dynamic
importance sampling (DIS) algorithm  which interleaves the
search and sampling processes (sampling from a sequence of
proposal distributions) to give bounds that are strong in an
anytime sense.

2 Background

[16]

[15]

Figure 1: Example: bounds on
logZ for protein instance 1bgc.

Let X = (X1  . . .   XM ) be a vector of random variables  where each Xi takes values in a discrete
domain Xi; we use lower case letters  e.g. xi ∈ Xi  to indicate a value of Xi  and x to indicate an
assignment of X. A graphical model over X consists of a set of factors F = {fα(Xα) | α ∈ I} 
where each factor fα is deﬁned on a subset Xα = {Xi | i ∈ α} of X  called its scope.
We associate an undirected graph G = (V  E) with F  where each node i ∈ V corresponds to
a variable Xi and we connect two nodes  (i  j) ∈ E  iff {i  j} ⊆ α for some α. The set I then
corresponds to cliques of G. We can interpret F as an unnormalized probability measure  so that

(cid:88)

(cid:89)

α∈I

x

fα(xα)

(cid:89)

α∈I

f (x) =

fα(xα) 

Z =

Z is called the partition function  and normalizes f (x). Computing Z is often a key task in evaluating
the probability of observed data  model selection  or computing predictive probabilities.

2.1 AND/OR search trees

We ﬁrst require some notations from search. AND/OR search trees are able to exploit the conditional
independence properties of the model  as expressed by a pseudo tree:
Deﬁnition 1 (pseudo tree). A pseudo tree of an undirected graph G = (V  E) is a directed tree
T = (V  E(cid:48)) sharing the same set of nodes as G. The tree edges E(cid:48) form a subset of E  and we
require that each edge (i  j) ∈ E \ E(cid:48) be a “back edge”  i.e.  the path from the root of T to j passes
through i (denoted i ≤ j). G is called the primal graph of T.
Fig. 2(a)-(b) show an example primal graph and pseudo tree. Guided by the pseudo tree  we can
construct an AND/OR search tree T consisting of alternating levels of OR and AND nodes. Each OR
node s is associated with a variable  which we slightly abuse notation to denote Xs; the children of
s  ch(s)  are AND nodes corresponding to the possible values of Xs. The root ∅ of the AND/OR
search tree corresponds to the root of the pseudo tree. Let pa(c) = s indicate the parent of c  and
an(c) = {n | n ≤ c} be the ancestors of c (including itself) in the tree.

2

102104−78−76−74−72−70−68−66−64time (sec)upper bound searchsamplingtwo-stageDIS(a)

(b)

(c)

Figure 2: (a) A primal graph of a graphical model over 7 variables. (b) A pseudo tree for the primal
graph consistent with elimination order G  F  E  D  C  B  A. (c) AND/OR search tree guided by the
pseudo tree. One full solution tree is marked red and one partial solution tree is marked blue.

As the pseudo tree deﬁnes a partial ordering on the variables Xi  the AND/OR tree extends this to one
over partial conﬁgurations of X. Speciﬁcally  any AND node c corresponds to a partial conﬁguration
x≤c of X  deﬁned by its assignment and that of its ancestors: x≤c = x≤p ∪ {Xs = xc}  where
s = pa(c)  p = pa(s). For completeness  we also deﬁne x≤s for any OR node s  which is the same
as that of its AND parent  i.e.  x≤s = x≤pa(s). For any node n  the corresponding variables of
x≤n is denoted as X≤n. Let de(Xn) be the set of variables below Xn in the pseudo tree; we deﬁne
X>n = de(Xn) if n is an AND node; X>n = de(Xn) ∪ {Xn} if n is an OR node.
The notion of a partial solution tree captures partial conﬁgurations of X respecting the search order:
Deﬁnition 2 (partial solution tree). A partial solution tree T of an AND/OR search tree T is a subtree
satisfying three conditions: (1) T contains the root of T ; (2) if an OR node is in T   at most one of its
children is in T ; (3) if an AND node is in T   all of its children or none of its children are in T .

Any partial solution tree T deﬁnes a partial conﬁguration xT of X; if xT is a complete conﬁguration
of X  we call T a full solution tree  and use Tx to denote the corresponding solution tree of a complete
assignment x. Fig. 2(c) illustrates these concepts.
We also associate a weight wc with each AND node  deﬁned to be the product of all factors fα that
are instantiated at c but not before:

(cid:89)

wc =

fα(xα) 

Ic = {α | Xc ∈ Xα ⊆ X≤c}

α∈Ic

product of weights on a path to the root  gn =(cid:81)

For completeness  deﬁne ws = 1 for any OR node s. It is then easy to see that  for any node n  the
a≤n wa (termed the cost of the path)  equals the value
of the factors whose scope is fully instantiated at n  i.e.  fully instantiated by x≤n. We can extend this
cost notion to any partial solution tree T by deﬁning g(T ) as the product of all factors fully instantiated
by xT ; we will slightly abuse notation by using g(T ) and g(xT ) interchangeably. Particularly  the
cost of any full solution tree equals the value of its corresponding complete conﬁguration. We use
g(x>n|x≤n) (termed the conditional cost) to denote the quotient g([x≤n  x>n])/g(x≤n)  where x>n
is any assignment of X>n  the variables below n in the search tree.
We give a “value” vn to each node n equal to the total conditional cost of all conﬁgurations below n:
(1)

g(x>n|x≤n).

vn =

(cid:88)

x>n

The value of the root is simply the partition function  v∅ = Z. Equivalently  vn can be deﬁned
recursively: if n is an AND node corresponding to a leaf of the pseudo tree  let vn = 1; otherwise 

vn =

c∈ch(n) vc 
c∈ch(n) wcvc 

if AND node n
if OR node n

(2)

(cid:40)(cid:81)
(cid:80)

2.2 AND/OR best-ﬁrst search for bounding the partition function

AND/OR best-ﬁrst search (AOBFS) can be used to bound the partition function in an anytime fashion
by expanding and updating bounds deﬁned on the search tree [16]. Beginning with only the root

3

A"B"C"D"E"F"G"ABCFGDEABB010101F01G01G01F01G01G01C01E01D01E01D01C01E01D01E01D01C01E01D01E01D01C01E01D01E01D01F01G01G01F01G01G01∅  AOBFS expands the search tree in a best-ﬁrst manner. More precisely  it maintains an explicit
AND/OR search tree of visited nodes  denoted S. For each node n in the AND/OR search tree 
AOBFS maintains un  an upper bound on vn  initialized via a pre-compiled heuristic vn ≤ h+
n   and
subsequently updated during search using information propagated from the frontier:

c∈ch(n) uc 
c∈ch(n) wcuc 

if AND node n
if OR node n

(3)

(cid:40)(cid:81)
(cid:80)

un =

Thus  the upper bound at the root  u∅  is an anytime deterministic upper bound of Z. Note that this
upper bound depends on the current search tree S  so we write US = u∅.
If all nodes below n have been visited  then un = vn; we call n solved and can remove the subtree
below n from memory. Hence we can partition the frontier nodes into two sets: solved frontier nodes 
SOLVED(S)  and unsolved ones  OPEN(S). AOBFS assigns a priority to each node and expands a
top-priority (unsolved) frontier node at each iteration. We use the “upper priority” from [16] 

Un = gnun

us

(4)

(cid:89)

s∈branch(n)

where branch(n) are the OR nodes that are siblings of some node ≤ n. Un quantiﬁes n’s contribution
to the global bound US  so this priority attempts to reduce the upper bound on Z as quickly as possible.
We can also interpret our bound US as a sum of bounds on each of the partial conﬁgurations covered
by S. Concretely  let TS be the set of projections of full solution trees on S (in other words  TS are
partial solution trees whose leaves are frontier nodes of S); then 

UT

where

UT = g(T )

us

(5)

(cid:88)

T∈TS

US =

(cid:89)

s∈leaf (T )

and leaf (T ) are the leaf nodes of the partial solution tree T .

2.3 Weighted mini-bucket for heuristics and sampling

To construct a heuristic function for search  we can use a class of variational bounds called weighted
mini-bucket (WMB  [14]). WMB corresponds to a relaxed variable elimination procedure  respecting
the search pseudo tree order  that can be tightened using reparameterization (or “cost-shifting”) opera-
tions. Importantly for this work  this same relaxation can also be used to deﬁne a proposal distribution
for importance sampling that yields ﬁnite-sample bounds [15]. We describe both properties here.
Let n be any node in the search tree; then  one can show that WMB yields the following reparametriza-
tion of the conditional cost below n [13]:

g(x>n|x≤n) = h+

n

bkj(xk|xanj (k))ρkj   Xk ∈ X>n

(6)

(cid:89)

(cid:89)

k

j

where Xanj (k) are the ancestors of Xk in the pseudo tree that are included in the j-th mini-bucket
of Xk. The size of Xanj (k) is controlled by a user-speciﬁed parameter called the ibound. The

bkj(xk|xanj (k)) are conditional beliefs  and the non-negative weights ρkj satisfy(cid:80)

j ρkj = 1.

Suppose that we deﬁne a conditional distribution q(x>n|x≤n) by replacing the geometric mean over
the bkj in (6) with their arithmetic mean:

q(x>n|x≤n) =

ρkjbkj(xk|xanj (k))

(7)

Applying the arithmetic-geometric mean inequality  we see that g(x>n|x≤n)/h+
Summing over x>n shows that h+

n is a valid upper bound heuristic for vn:

n ≤ q(x>n|x≤n).

vn =

g(x>n|x≤n) ≤ h+

n

The mixture distribution q can be also used as a proposal for importance sampling  by drawing
samples from q and averaging the importance weights  g/q. For any node n  we have that

g(x>n|x≤n)/q(x>n|x≤n)

= vn

(8)

(cid:105)

g(x>n|x≤n)/q(x>n|x≤n) ≤ h+
n  

4

(cid:89)

(cid:88)

k

j

(cid:88)

x>n

E(cid:104)

i.e.  the importance weight g(x>n|x≤n)/q(x>n|x≤n) is an unbiased and bounded estimator of vn.
In [15]  this property was used to give ﬁnite-sample bounds on Z which depended on the WMB
bound  h+∅ . To be more speciﬁc  note that g(x>n|x≤n) = f (x) when n is the root ∅  and thus
f (x)/q(x) ≤ h+∅ ; the boundedness of f (x)/q(x) results in the following ﬁnite-sample upper bound
on Z that holds with probability at least 1 − δ:

Z ≤ 1
N

f (xi)
q(xi)

+

i=1 are i.i.d. samples drawn from q(x)  and (cid:100)Var({f (xi)/q(xi)}N

where {xi}N
i=1) is the unbiased
empirical variance. This probabilistic upper bound usually becomes tighter than h+∅ very quickly. A
corresponding ﬁnite-sample lower bound on Z exists as well [15].

N

i=1) ln(2/δ)

+

7 ln(2/δ)h+∅
3(N − 1)

(9)

(cid:115)

2(cid:100)Var({f (xi)/q(xi)}N

N(cid:88)

i=1

3 Two-step sampling

The ﬁnite-sample bound (9) suggests that improvements to the upper bound on Z may be translatable
into improvements in the probabilistic  sampling bound. In particular  if we deﬁne a proposal that
uses the search tree S and its bound US  we can improve our sample-based bound as well. This
motivates us to design a two-step sampling scheme that exploits the reﬁned upper bound from search;
it is a top-down procedure starting from the root:

Step 1 For an internal node n: if it is an AND node  all its children are selected; if n is an OR node 
Step 2 When a frontier node n is reached  if it is unsolved  draw a sample of X>n from q(x>n|x≤n);

one child c ∈ ch(n) is randomly selected with probability wcuc/un.

if it is solved  quit.

The behavior of Step 1 can be understood by the following proposition:
Proposition 1. Step 1 returns a partial solution tree T ∈ TS with probability UT /US (see (5)). Any
frontier node of S will be reached with probability proportional to its upper priority deﬁned in (4).
Note that at Step 2  although the sampling process terminates when a solved node n is reached  we
associate every conﬁguration x>n of X>n with probability g(x>n|x≤n)/vn which is appropriate in
lieu of (1). Thus  we can show that this two-step sampling scheme induces a proposal distribution 
denoted qS (x)  which can be expressed as:

qS (x) =

wnun/upa(n)

q(x>n(cid:48)|x≤n(cid:48))

g(x>n(cid:48)(cid:48)|x≤n(cid:48)(cid:48))/vn(cid:48)(cid:48)

n∈AND(Tx∩S)

n(cid:48)∈OPEN(S)∩Tx

n(cid:48)(cid:48)∈SOLVED(S)∩Tx

where AND(Tx ∩ S) is the set of all AND nodes of the partial solution tree Tx ∩ S. By applying (3) 
and noticing that the upper bound is the initial heuristic for any node in OPEN(S) and is exact at any
solved node  we re-write qS (x) as

(cid:89)

(cid:89)

(cid:89)

qS (x) =

g(Tx ∩ S)

US

n(cid:48) q(x>n(cid:48)|x≤n(cid:48))
h+

g(x>n(cid:48)(cid:48)|x≤n(cid:48)(cid:48))

(10)

n(cid:48)∈OPEN(S)∩Tx

n(cid:48)(cid:48)∈SOLVED(S)∩Tx

(cid:89)

qS (x) actually provides bounded importance weights that can use the reﬁned upper bound US:
Proposition 2. Importance weights from qS (x) are bounded by the upper bound of S  and are
unbiased estimators of Z  i.e. 

f (x)/qS (x) ≤ US  

f (x)/qS (x)

= Z

(11)

E(cid:104)

(cid:105)

(cid:89)

(cid:89)

Proof. Note that f (x) can be written as

f (x) = g(Tx ∩ S)

g(x>n(cid:48)|x≤n(cid:48))

n(cid:48)∈OPEN(S)∩Tx

(cid:89)

Noticing that for any n(cid:48) ∈ OPEN(S)  g(x>n(cid:48)|x≤n(cid:48)) ≤ h+
with (10)  we see f (x)/qS (x) is bounded by US. Its unbiasedness is trivial.

g(x>n(cid:48)(cid:48)|x≤n(cid:48)(cid:48))

(12)

n(cid:48)(cid:48)∈SOLVED(S)∩Tx
n(cid:48) q(x>n(cid:48)|x≤n(cid:48)) by (8)  and comparing

5

Expand Nd nodes via AOBFS (Alg. 1 of [16]) with the upper priority deﬁned in (4).

// update S and its associated upper bound US

if within the memory budget

i=1)  (cid:98)Z  ∆.

Ensure: N  HM(U ) (cid:100)Var({(cid:98)Zi/Ui}N

end if
Draw Nl samples via TWOSTEPSAMPLING(S).
After drawing each sample:

Algorithm 1 Dynamic importance sampling (DIS)
Require: Control parameters Nd  Nl; memory budget  time budget.
1: Initialize S ← {∅} with the root ∅.
2: while within the time budget
3:
4:
5:
6:
7:
8:
9:
10: end while
11: function TWOSTEPSAMPLING(S)
12:
13:
14:
15:
16: end function

Update N  HM(U ) (cid:100)Var({(cid:98)Zi/Ui}N
Update (cid:98)Z  ∆ via (13)  (14).

Start from the root of the search tree S:

i=1).

For an internal node n: select all its children if it is an AND node; select exactly
one child c ∈ ch(n) with probability wcuc/un if it is an OR node.
At any unsolved frontier node n  draw one sample from q(x>n|x≤n) in (7).

Thus  importance weights resulting from our two-step sampling can enjoy the same type of bounds
described in (9). Moreover  note that at any solved node  our sampling procedure incorporates the
“exact” value of that node into the importance weights  which serves as Rao-Blackwellisation and can
potentially reduce variance.
We can see that if S = ∅ (before search)  qS (x) is the proposal distribution of [15]; as search
proceeds  the quality of the proposal distribution improves (gradually approaching the underlying
distribution f (x)/Z as S approaches the complete search tree). If we perform search ﬁrst  up to some
memory limit  and then sample  which we refer to as two-stage sampling  our probabilistic bounds
will proceed from an improved baseline  giving better bounds at moderate to long computation times.
However  doing so sacriﬁces the quick improvement early on given by basic importance sampling. In
the next section  we describe our dynamic importance sampling procedure  which balances these two
properties.

4 Dynamic importance sampling

To provide good anytime behavior  we would like to do both sampling and search  so that early
samples can improve the bound quickly  while later samples obtain the beneﬁts of the search tree’s
improved proposal. To do so  we deﬁne a dynamic importance sampling (DIS) scheme  presented in
Alg. 1  which interleaves drawing samples and expanding the search tree.
One complication of such an approach is that each sample comes from a different proposal distribution 
and thus has a different bound value entering into the concentration inequality. Moreover  each sample
is of a different quality – later samples should have lower variance  since they come from an improved
proposal. To this end  we construct an estimator of Z that upweights higher-quality samples. Let
{xi}N
i=1 the corresponding
importance weights  and {Ui = USi}N
i=1 the corresponding upper bounds on the importance weights

i=1 be a series of samples drawn via Alg. 1  with {(cid:98)Zi = f (xi)/qSi (xi)}N
N(cid:88)

respectively. We introduce an estimator (cid:98)Z of Z:

(cid:105)−1

N(cid:88)

(cid:104) 1

HM(U )

(cid:98)Zi

N

where HM(U ) is the harmonic mean of the upper bounds Ui. (cid:98)Z is an unbiased estimator of Z
(cid:98)Z/ HM(U )  and (cid:98)Zi/Ui are all within the interval [0  1]  we can apply an empirical Bernstein

(since it is a weighted average of independent  unbiased estimators). Additionally  since Z/ HM(U ) 

Ui

i=1

1
Ui

N

i=1

 

HM(U ) =

(13)

(cid:98)Z =

bound [17] to derive ﬁnite-sample bounds:

6

Theorem 1. Deﬁne the deviation term

(cid:16)(cid:115)

2(cid:100)Var({(cid:98)Zi/Ui}N

(cid:17)

(14)

i=1) ln(2/δ)

+

7 ln(2/δ)
3(N − 1)

∆ = HM(U )

N

i=1) is the unbiased empirical variance of {(cid:98)Zi/Ui}N

where(cid:100)Var({(cid:98)Zi/Ui}N
i=1. Then (cid:98)Z + ∆ and (cid:98)Z − ∆
are upper and lower bounds of Z with probability at least 1 − δ  respectively  i.e.  Pr[Z ≤ (cid:98)Z + ∆] ≥
1 − δ and Pr[Z ≥ (cid:98)Z − ∆] ≥ 1 − δ.
It is possible that (cid:98)Z − ∆ < 0 at ﬁrst; if so  we may replace (cid:98)Z − ∆ with any non-trivial lower bound
of Z. In the experiments  we use (cid:98)Zδ  a (1 − δ) probabilistic bound by the Markov inequality [11].
We can also replace (cid:98)Z + ∆ with the current deterministic upper bound if the latter is tighter.

Intuitively  our DIS algorithm is similar to Monte Carlo tree search (MCTS) [1]  which also grows
an explicit search tree while sampling. However  in MCTS  the sampling procedure is used to grow
the tree  while DIS uses a classic search priority. This ensures that the DIS samples are independent 
since samples do not inﬂuence the proposal distribution of later samples. This also distinguishes DIS
from methods such as adaptive importance sampling (AIS) [18].

5 Empirical evaluation

We evaluate our approach (DIS) against AOBFS (search  [16]) and WMB-IS (sampling  [15]) on
several benchmarks of real-world problem instances from recent UAI competitions. Our benchmarks
include pedigree  22 genetic linkage instances from the UAI’08 inference challenge1; protein  50
randomly selected instances made from the “small” protein side-chains of [22]; and BN  50 randomly
selected Bayesian networks from the UAI’06 competition2. These three sets are selected to illustrate
different problem characteristics; for example protein instances are relatively small (M = 100
variables on average  and average induced width 11.2) but high cardinality (average max|Xi| = 77.9) 
while pedigree and BN have more variables and higher induced width (average M 917.1 and 838.6 
average width 25.5 and 32.8)  but lower cardinality (average max|Xi| 5.6 and 12.4).
We alloted 1GB memory to all methods  ﬁrst computing the largest ibound that ﬁts the memory budget 
and using the remaining memory for search. All the algorithms used the same upper bound heuristics 
which also means DIS and AOBFS had the same amount of memory available for search. For AOBFS 
we use the memory-limited version (Alg. 2 of [16]) with “upper” priority  which continues improving
its bounds past the memory limit. Additionally  we let AOBFS access a lower bound heuristic for no
cost  to facilitate comparison between DIS and AOBFS. We show DIS for two settings  (Nl=1  Nd=1)
and (Nl=1  Nd=10)  balancing the effort between search and sampling. Note that WMB-IS can be
viewed as DIS with (Nl=Inf  Nd=0)  i.e.  it runs pure sampling without any search  and two-stage
sampling viewed as DIS with (Nl=1  Nd=Inf)  i.e.  it searches to the memory limit then samples. We
set δ = 0.025 and ran each algorithm for 1 hour. All implementations are in C/C++.
Anytime bounds for individual instances. Fig. 3 shows the anytime behavior of all methods on
two instances from each benchmark. We observe that compared to WMB-IS  DIS provides better
upper and lower bounds on all instances. In 3(d)–(f)  WMB-IS is not able to produce tight bounds
within 1 hour  but DIS quickly closes the gap. Compared to AOBFS  in 3(a)–(c) (e)  DIS improves
much faster  and in (d) (f) it remains nearly as fast as search. Note that four of these examples are
sufﬁciently hard to be unsolved by a variable elimination-based exact solver  even with several orders
of magnitude more computational resources (200GB memory  24 hour time limit).
Thus  DIS provides excellent anytime behavior; in particular  (Nl=1  Nd=10) seems to work well 
perhaps because expanding the search tree is slightly faster than drawing a sample (since the tree
depth is less than the number of variables). On the other hand  two-stage sampling gives weaker early
bounds  but is often excellent at longer time settings.
Aggregated results across the benchmarks. To quantify anytime performance of the methods in
each benchmark  we introduce a measure based on the area between the upper and lower bound of

1http://graphmod.ics.uci.edu/uai08/Evaluation/Report/Benchmarks/
2http://melodi.ee.washington.edu/~bilmes/uai06InferenceEvaluation/

7

(a) pedigree/pedigree33

(b) protein/1co6

(c) BN/BN_30

(d) pedigree/pedigree37

(e) protein/1bgc

(f) BN/BN_129

Figure 3: Anytime bounds on logZ for two instances per benchmark. Dotted line sections on some
curves indicate Markov lower bounds. In examples where search is very effective (d f)  or where
sampling is very effective (a)  DIS is equal or nearly so  while in (b c e) DIS is better than either.

Table 1: Mean area between upper and lower bounds of logZ  normalized by WMB-IS  for each
benchmark. Smaller numbers indicate better anytime bounds. The best for each benchmark is bolded.

pedigree
protein

BN

AOBFS WMB-IS DIS (Nl=1  Nd=1) DIS (Nl=1  Nd=10)
16.638
1.576
0.233

0.585
0.095
0.162

1
1
1

0.711
0.110
0.340

two-stage

1.321
2.511
0.865

logZ. For each instance and method  we compute the area of the interval between the upper and
lower bound of logZ for that instance and method. To avoid vacuous lower bounds  we provide each
algorithm with an initial lower bound on logZ from WMB. To facilitate comparison  we normalize
the area of each method by that of WMB-IS on each instance  then report the geometric mean of the
normalized areas across each benchmark in Table 1. This shows the average relative quality compared
to WMB-IS; smaller values indicate tighter anytime bounds. We see that on average  search is more
effective than sampling on the BN instances  but much less effective on pedigree. Across all three
benchmarks  DIS (Nl=1  Nd=10) produces the best result by a signiﬁcant margin  while DIS (Nl=1 
Nd=1) is also very competitive  and two-stage sampling does somewhat less well.

6 Conclusion

We propose a dynamic importance sampling algorithm that embraces the merits of best-ﬁrst search
and importance sampling to provide anytime ﬁnite-sample bounds for the partition function. The
AOBFS search process improves the proposal distribution over time  while our particular weighted
average of importance weights gives the resulting estimator quickly decaying ﬁnite-sample bounds 
as illustrated on several UAI problem benchmarks. Our work also opens up several avenues for future
research  including investigating different weighting schemes for the samples  more ﬂexible balances
between search and sampling (for example  changing over time)  and more closely integrating the
variational optimization process into the anytime behavior.

8

101102103104−130−125−120time (sec)logZ ( −124.979 ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stage100102104−105−100−95−90−85time (sec)logZ ( unknown ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stage100102104−35−30−25−20time (sec)logZ ( unknown ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stage101102103104−280−275−270−265−260time (sec)logZ ( −268.435 ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stage100102104−95−90−85−80−75−70−65time (sec)logZ ( unknown ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stage101102103104−160−150−140−130−120time (sec)logZ ( unknown ) AOBFSWMB-ISDIS(Nl=1 Nd=1)DIS(Nl=1 Nd=10)two-stageAcknowledgements

We thank William Lam  Wei Ping  and all the reviewers for their helpful feedback.
This work is sponsored in part by NSF grants IIS-1526842  IIS-1254071  and by the United States
Air Force under Contract No. FA8750-14-C-0011 and FA9453-16-C-0508.

References
[1] C. B. Browne  E. Powley  D. Whitehouse  S. M. Lucas  P. I. Cowling  P. Rohlfshagen  S. Tavener  D. Perez 
S. Samothrakis  and S. Colton. A survey of Monte Carlo tree search methods. IEEE Transactions on
Computational Intelligence and AI in games  4(1):1–43  2012.

[2] S. Chakraborty  K. S. Meel  and M. Y. Vardi. Algorithmic improvements in approximate counting for

probabilistic inference: From linear to logarithmic SAT calls. IJCAI’16.

[3] S. Chakraborty  D. J. Fremont  K. S. Meel  S. A. Seshia  and M. Y. Vardi. Distribution-aware sampling and

weighted model counting for SAT. AAAI’14  pages 1722–1730. AAAI Press  2014.

[4] P. Dagum and M. Luby. An optimal approximation algorithm for Bayesian inference. Artiﬁcial Intelligence 

93(1-2):1–27  1997.

[5] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press  2009.
[6] R. Dechter. Reasoning with probabilistic and deterministic graphical models: Exact algorithms. Synthesis

Lectures on Artiﬁcial Intelligence and Machine Learning  7(3):1–191  2013.

[7] R. Dechter and I. Rish. Mini-buckets: A general scheme of approximating inference. Journal of ACM  50

(2):107–153  2003.

[8] R. Dechter  H. Geffner  and J. Y. Halpern. Heuristics  Probability and Causality. A Tribute to Judea Pearl.

College Publications  2010.

[9] S. Ermon  C. Gomes  A. Sabharwal  and B. Selman. Taming the curse of dimensionality: Discrete
In International Conference on Machine Learning  pages

integration by hashing and optimization.
334–342  2013.

[10] S. Ermon  C. Gomes  A. Sabharwal  and B. Selman. Low-density parity constraints for hashing-based

discrete integration. In International Conference on Machine Learning  pages 271–279  2014.

[11] V. Gogate and R. Dechter. Sampling-based lower bounds for counting queries. Intelligenza Artiﬁciale  5

(2):171–188  2011.

[12] M. Henrion. Search-based methods to bound diagnostic probabilities in very large belief nets.

Proceedings of the 7th conference on Uncertainty in Artiﬁcial Intelligence  pages 142–150  1991.

In

[13] Q. Liu. Reasoning and Decisions in Probabilistic Graphical Models–A Uniﬁed Framework. PhD thesis 

University of California  Irvine  2014.

[14] Q. Liu and A. Ihler. Bounding the partition function using Hölder’s inequality. In Proceedings of the 28th

International Conference on Machine Learning (ICML)  New York  NY  USA  2011.

[15] Q. Liu  J. W. Fisher  III  and A. T. Ihler. Probabilistic variational bounds for graphical models. In Advances

in Neural Information Processing Systems  pages 1432–1440  2015.

[16] Q. Lou  R. Dechter  and A. Ihler. Anytime anyspace AND/OR search for bounding the partition function.

In Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence  2017.

[17] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In COLT  2009.
[18] M.-S. Oh and J. O. Berger. Adaptive importance sampling in Monte Carlo integration. Journal of Statistical

Computation and Simulation  41(3-4):143–168  1992.

[19] L. Valiant. The complexity of computing the permanent. Theoretical Computer Science  8(2):189 – 201 

1979.

[20] C. Viricel  D. Simoncini  S. Barbe  and T. Schiex. Guaranteed weighted counting for afﬁnity computation:
Beyond determinism and structure. In International Conference on Principles and Practice of Constraint
Programming  pages 733–750. Springer  2016.

[21] M. Wainwright and M. Jordan. Graphical models  exponential families  and variational inference. Founda-

tions and Trends R(cid:13) in Machine Learning  1(1-2):1–305  2008.

[22] C. Yanover and Y. Weiss. Approximate inference and protein-folding. In Advances in Neural Information

Processing Systems  pages 1457–1464  2002.

9

,Dinesh Jayaraman
Kristen Grauman
Qi Lou
Rina Dechter
Alexander Ihler
Irene Chen
Fredrik Johansson
David Sontag