2017,Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon,How to develop slim and accurate deep neural networks has become crucial for real- world applications  especially for those employed in embedded systems. Though previous work along this research line has shown some promising results  most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper  we propose a new layer-wise pruning method for deep neural networks. In our proposed method  parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. By controlling layer-wise errors properly  one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. Codes of our work are released at: https://github.com/csyhhu/L-OBS.,Learning to Prune Deep Neural Networks via

Layer-wise Optimal Brain Surgeon

Xin Dong

Shangyu Chen

Nanyang Technological University  Singapore

Nanyang Technological University  Singapore

n1503521a@e.ntu.edu.sg

schen025@e.ntu.edu.sg

Sinno Jialin Pan

Nanyang Technological University  Singapore

sinnopan@ntu.edu.sg

Abstract

How to develop slim and accurate deep neural networks has become crucial for real-
world applications  especially for those employed in embedded systems. Though
previous work along this research line has shown some promising results  most
existing methods either fail to signiﬁcantly compress a well-trained deep network
or require a heavy retraining process for the pruned deep network to re-boost its
prediction performance. In this paper  we propose a new layer-wise pruning method
for deep neural networks. In our proposed method  parameters of each individual
layer are pruned independently based on second order derivatives of a layer-wise
error function with respect to the corresponding parameters. We prove that the
ﬁnal prediction performance drop after pruning is bounded by a linear combination
of the reconstructed errors caused at each layer. By controlling layer-wise errors
properly  one only needs to perform a light retraining process on the pruned network
to resume its original prediction performance. We conduct extensive experiments
on benchmark datasets to demonstrate the effectiveness of our pruning method
compared with several state-of-the-art baseline methods. Codes of our work are
released at: https://github.com/csyhhu/L-OBS.

1

Introduction

Intuitively  deep neural networks [1] can approximate predictive functions of arbitrary complexity
well when they are of a huge amount of parameters  i.e.  a lot of layers and neurons. In practice  the
size of deep neural networks has been being tremendously increased  from LeNet-5 with less than
1M parameters [2] to VGG-16 with 133M parameters [3]. Such a large number of parameters not
only make deep models memory intensive and computationally expensive  but also urge researchers
to dig into redundancy of deep neural networks. On one hand  in neuroscience  recent studies point
out that there are signiﬁcant redundant neurons in human brain  and memory may have relation with
vanishment of speciﬁc synapses [4]. On the other hand  in machine learning  both theoretical analysis
and empirical experiments have shown the evidence of redundancy in several deep models [5  6].
Therefore  it is possible to compress deep neural networks without or with little loss in prediction by
pruning parameters with carefully designed criteria.
However  ﬁnding an optimal pruning solution is NP-hard because the search space for pruning
is exponential in terms of parameter size. Recent work mainly focuses on developing efﬁcient
algorithms to obtain a near-optimal pruning solution [7  8  9  10  11]. A common idea behind most
exiting approaches is to select parameters for pruning based on certain criteria  such as increase in
training error  magnitude of the parameter values  etc. As most of the existing pruning criteria are

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

designed heuristically  there is no guarantee that prediction performance of a deep neural network
can be preserved after pruning. Therefore  a time-consuming retraining process is usually needed to
boost the performance of the trimmed neural network.
Instead of consuming efforts on a whole deep network  a layer-wise pruning method  Net-Trim  was
proposed to learn sparse parameters by minimizing reconstructed error for each individual layer [6].
A theoretical analysis is provided that the overall performance drop of the deep network is bounded by
the sum of reconstructed errors for each layer. In this way  the pruned deep network has a theoretical
guarantee on its error. However  as Net-Trim adopts (cid:96)1-norm to induce sparsity for pruning  it fails to
obtain high compression ratio compared with other methods [9  11].
In this paper  we propose a new layer-wise pruning method for deep neural networks  aiming to
achieve the following three goals: 1) For each layer  parameters can be highly compressed after
pruning  while the reconstructed error is small. 2) There is a theoretical guarantee on the overall
prediction performance of the pruned deep neural network in terms of reconstructed errors for each
layer. 3) After the deep network is pruned  only a light retraining process is required to resume its
original prediction performance.
To achieve our ﬁrst goal  we borrow an idea from some classic pruning approaches for shallow neural
networks  such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13]. These
classic methods approximate a change in the error function via functional Taylor Series  and identify
unimportant weights based on second order derivatives. Though these approaches have proven to
be effective for shallow neural networks  it remains challenging to extend them for deep neural
networks because of the high computational cost on computing second order derivatives  i.e.  the
inverse of the Hessian matrix over all the parameters. In this work  as we restrict the computation on
second order derivatives w.r.t. the parameters of each individual layer only  i.e.  the Hessian matrix is
only over parameters for a speciﬁc layer  the computation becomes tractable. Moreover  we utilize
characteristics of back-propagation for fully-connected layers in well-trained deep networks to further
reduce computational complexity of the inverse operation of the Hessian matrix.
To achieve our second goal  based on the theoretical results in [6]  we provide a proof on the bound
of performance drop before and after pruning in terms of the reconstructed errors for each layer.
With such a layer-wise pruning framework using second-order derivatives for trimming parameters
for each layer  we empirically show that after signiﬁcantly pruning parameters  there is only a little
drop of prediction performance compared with that before pruning. Therefore  only a light retraining
process is needed to resume the performance  which achieves our third goal.
The contributions of this paper are summarized as follows. 1) We propose a new layer-wise pruning
method for deep neural networks  which is able to signiﬁcantly trim networks and preserve the
prediction performance of networks after pruning with a theoretical guarantee. In addition  with the
proposed method  a time-consuming retraining process for re-boosting the performance of the pruned
network is waived. 2) We conduct extensive experiments to verify the effectiveness of our proposed
method compared with several state-of-the-art approaches.

2 Related Works and Preliminary

Pruning methods have been widely used for model compression in early neural networks [7] and
modern deep neural networks [6  8  9  10  11]. In the past  with relatively small size of training data 
pruning is crucial to avoid overﬁtting. Classical methods include OBD and OBS. These methods
aim to prune parameters with the least increase of error approximated by second order derivatives.
However  computation of the Hessian inverse over all the parameters is expensive. In OBD  the
Hessian matrix is restricted to be a diagonal matrix to make it computationally tractable. However 
this approach implicitly assumes parameters have no interactions  which may hurt the pruning
performance. Different from OBD  OBS makes use of the full Hessian matrix for pruning. It obtains
better performance while is much more computationally expensive even using Woodbury matrix
identity [14]  which is an iterative method to compute the Hessian inverse. For example  using OBS
on VGG-16 naturally requires to compute inverse of the Hessian matrix with a size of 133M × 133M.
Regarding pruning for modern deep models  Han et al. [9] proposed to delete unimportant parameters
based on magnitude of their absolute values  and retrain the remaining ones to recover the original
prediction performance. This method achieves considerable compression ratio in practice. However 

2

as pointed out by pioneer research work [12  13]  parameters with low magnitude of their absolute
values can be necessary for low error. Therefore  magnitude-based approaches may eliminate
wrong parameters  resulting in a big prediction performance drop right after pruning  and poor
robustness before retraining [15]. Though some variants have tried to ﬁnd better magnitude-based
criteria [16  17]  the signiﬁcant drop of prediction performance after pruning still remains. To avoid
pruning wrong parameters  Guo et al. [11] introduced a mask matrix to indicate the state of network
connection for dynamically pruning after each gradient decent step. Jin et al. [18] proposed an
iterative hard thresholding approach to re-activate the pruned parameters after each pruning phase.
Besides Net-trim  which is a layer-wise pruning method discussed in the previous section  there
is some other work proposed to induce sparsity or low-rank approximation on certain layers for
pruning [19  20]. However  as the (cid:96)0-norm or the (cid:96)1-norm sparsity-induced regularization term
increases difﬁculty in optimization  the pruned deep neural networks using these methods either
obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining
of the whole network to prevent accumulation of errors [10].
Optimal Brain Surgeon As our proposed layer-wise pruning method is an extension of OBS on
δE =(cid:0) ∂E
deep neural networks  we brieﬂy review the basic of OBS here. Consider a network in terms of
parameters w trained to a local minimum in error. The functional Taylor series of the error w.r.t. w is:
variable  H ≡ ∂2E/∂w2 ∈ Rm×m is the Hessian matrix  where m is the number of parameters  and
O((cid:107)δΘl(cid:107)3) is the third and all higher order terms. For a network trained to a local minimum in error 
the ﬁrst term vanishes  and the term O((cid:107)δΘl(cid:107)3) can be ignored. In OBS  the goal is to set one of the
parameters to zero  denoted by wq (scalar)  to minimize δE in each pruning iteration. The resultant
optimization problem is written as follows 

(cid:107)δw(cid:107)3(cid:1)  where δ denotes a perturbation of a corresponding

2 δw(cid:62)Hδw + O(cid:0)

(cid:1)(cid:62)

δw + 1

∂w

min

q

1
2

δw(cid:62)Hδw  s.t. e(cid:62)

q δw + wq = 0 

(1)

where eq is the unit selecting vector whose q-th element is 1 and otherwise 0. As shown in [21]  the
optimization problem (1) can be solved by the Lagrange multipliers method. Note that a computation
bottleneck of OBS is to calculate and store the non-diagonal Hesssian matrix and its inverse  which
makes it impractical on pruning deep models which are usually of a huge number of parameters.

3 Layer-wise Optimal Brain Surgeon

3.1 Problem Statement

1

  ...  yl−1

Given a training set of n instances  {(xj  yj)}n
j=1  and a well-trained deep neural network of L layers
(excluding the input layer)1. Denote the input and the output of the whole deep neural network by
X = [x1  ...  xn]∈Rd×n and Y∈Rn×1  respectively. For a layer l  we denote the input and output of
the layer by Yl−1 = [yl−1
n]∈Rml×n  respectively  where
n ]∈Rml−1×n and Yl = [yl
1  ...  yl
i can be considered as a representation of xi in layer l  and Y0 = X  YL = Y  and m0 = d. Using
yl
(cid:62)Yl−1 with Wl∈Rml−1×ml being the
one forward-pass step  we have Yl = σ(Zl)  where Zl =Wl
matrix of parameters for layer l  and σ(·) is the activation function. For convenience in presentation
and proof  we deﬁne the activation function σ(·) as the rectiﬁed linear unit (ReLU) [22]. We further
denote by Θl∈Rml−1ml×1 the vectorization of Wl. For a well-trained neural network  Yl  Zl and
Θ∗
l are all ﬁxed matrixes and contain most information of the neural network. The goal of pruning is
to set the values of some elements in Θl to be zero.

3.2 Layer-Wise Error
During layer-wise pruning in layer l  the input Yl−1 is ﬁxed as the same as the well-trained network.
Suppose we set the q-th element of Θl  denoted by Θl[q]  to be zero  and get a new parameter vector 
denoted by ˆΘl. With Yl−1  we obtain a new output for layer l  denoted by ˆYl. Consider the root of

1For simplicity in presentation  we suppose the neural network is a feed-forward (fully-connected) network.

In Section 3.4  we will show how to extend our method to ﬁlter layers in Convolutional Neural Networks.

3

mean square error between ˆYl and Yl over the whole training data as the layer-wise error:

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

(cid:0)(ˆyl

n(cid:88)

j=1

εl =

j)(cid:1) =

j − yl

j)(cid:62)(ˆyl

j − yl

1

√n(cid:107) ˆYl − Yl(cid:107)F  

(2)

where (cid:107) · (cid:107)F is the Frobenius Norm. Note that for any single parameter pruning  one can compute its
q  where 1 ≤ q ≤ ml−1ml  and use it as a pruning criterion. This idea has been adopted by
error εl
some existing methods [15]. However  in this way  for each parameter at each layer  one has to pass
the whole training data once to compute its error measure  which is very computationally expensive.
A more efﬁcient approach is to make use of the second order derivatives of the error function to help
identify importance of each parameter.
We ﬁrst deﬁne an error function E(·) as

El = E(ˆZl) =

 

(3)

(cid:13)(cid:13)(cid:13)ˆZl − Zl(cid:13)(cid:13)(cid:13)2

F

1
n

where Zl is outcome of the weighted sum operation right before performing the activation function
σ(·) at layer l of the well-trained neural network  and ˆZl is outcome of the weighted sum operation
after pruning at layer l . Note that Zl is considered as the desired output of layer l before activation.
The following lemma shows that the layer-wise error is bounded by the error deﬁned in (3).
Lemma 3.1. With the error function (3) and Yl = σ(Zl)  the following holds: εl ≤
Therefore  to ﬁnd parameters whose deletion (set to be zero) minimizes (2) can be translated to ﬁnd
parameters those deletion minimizes the error function (3). Following [12  13]  the error function can
be approximated by functional Taylor series as follows 

E(ˆZl).

(cid:113)

(cid:62)HlδΘl + O(cid:0)

(cid:107)δΘl(cid:107)3(cid:1)  

(4)

(cid:18) ∂El

(cid:19)(cid:62)

∂Θl

δΘl +

δΘl

1
2

(cid:12)(cid:12)(cid:12)Θl=Θ∗

E(ˆZl) − E(Zl) = δEl =

2 is the Hessian matrix
where δ denotes a perturbation of a corresponding variable  Hl ≡ ∂2El/∂Θl
w.r.t. Θl  and O((cid:107)δΘl(cid:107)3) is the third and all higher order terms. It can be proven that with the error
function deﬁned in (3)  the ﬁrst (linear) term ∂El
∂Θl

l and O((cid:107)δΘl(cid:107)3) are equal to 0.

Suppose every time one aims to ﬁnd a parameter Θl[q] to set to be zero such that the change δEl is
minimal. Similar to OBS  we can formulate it as the following optimization problem:

min

q

1
2

δΘl

(cid:62)HlδΘl  s.t. e(cid:62)

q δΘl + Θl[q] = 0 

(5)

where eq is the unit selecting vector whose q-th element is 1 and otherwise 0. By using the Lagrange
multipliers method as suggested in [21]  we obtain the closed-form solutions of the optimal parameter
pruning and the resultant minimal change in the error function as follows 
(Θl[q] )2
[H−1
]qq

l eq  and Lq = δEl =

Θl[q]
[H−1

δΘl = −

H−1

(6)

1
2

]qq

.

l

l

Here Lq is referred to as the sensitivity of parameter Θl[q]. Then we select parameters to prune based
on their sensitivity scores instead of their magnitudes. As mentioned in section 2  magnitude-based
criteria which merely consider the numerator in (6) is a poor estimation of sensitivity of parameters.
Moreover  in (6)  as the inverse Hessian matrix over the training data is involved  it is able to capture
data distribution when measuring sensitivities of parameters.
After pruning the parameter  Θl[q]  with the smallest sensitivity  the parameter vector is updated via
ˆΘl = Θl +δΘl. With Lemma 3.1 and (6)  we have that the layer-wise error for layer l is bounded by

(cid:113)

(cid:113)

εl
q ≤

E(ˆZl) =

E(ˆZl) − E(Zl) = √δEl = |Θl[q]|
2[H−1

l

.

]qq

(7)

(cid:113)

Note that ﬁrst equality is obtained because of the fact that E(Zl) = 0. It is worth to mention
that though we merely focus on layer l  the Hessian matrix is still a square matrix with size of
for

ml−1ml × ml−1ml. However  we will show how to signiﬁcantly reduce the computation of H−1

l

each layer in Section 3.4.

4

3.3 Layer-Wise Error Propagation and Accumulation

So far  we have shown how to prune parameters for each layer  and estimate their introduced errors
independently. However  our aim is to control the consistence of the network’s ﬁnal output YL before
and after pruning. To do this  in the following  we show how the layer-wise errors propagate to ﬁnal
output layer  and the accumulated error over multiple layers will not explode.
Theorem 3.2. Given a pruned deep network via layer-wise pruning introduced in Section 3.2  each
layer has its own layer-wise error εl for 1 ≤ l ≤ L  then the accumulated error of ultimate network
output ˜εL = 1√

L−1(cid:88)
n(cid:107) ˜YL − YL(cid:107)F obeys:

(cid:32) L(cid:89)

(cid:33)

(cid:107) ˆΘl(cid:107)F√δEk

+ √δEL 

l

k=1

l=k+1

˜εL ≤

1 X).

(8)
˜Yl−1)  for 2 ≤ l ≤ L denotes ‘accumulated pruned output’ of layer l  and

where ˜Yl = σ( ˆW(cid:62)
˜Y1 = σ( ˆW(cid:62)
Theorem 3.2 shows that: 1) Layer-wise error for a layer l will be scaled by continued multiplication
of parameters’ Frobenius Norm over the following layers when it propagates to ﬁnal output  i.e. 
the L−l layers after the l-th layer; 2) The ﬁnal error of ultimate network output is bounded by the
weighted sum of layer-wise errors. The proof of Theorem 3.2 can be found in Appendix.
Consider a general case with (6) and (8): parameter Θl[q] who has the smallest sensitivity in layer l

is pruned by the i-th pruning operation  and this ﬁnally adds(cid:81)L
by a quite large product factor  Sl =(cid:81)L

k=l+1 (cid:107) ˆΘk(cid:107)F√δEl to the ultimate
network output error. It is worth to mention that although it seems that the layer-wise error is scaled
k=l+1 (cid:107) ˆΘk(cid:107)F when it propagates to the ﬁnal layer  this scaling
is still tractable in practice because ultimate network output is also scaled by the same product factor
compared with the output of layer l. For example  we can easily estimate the norm of ultimate network
output via  (cid:107)YL(cid:107)F ≈ S1(cid:107)Y1(cid:107)F . If one pruning operation in the 1st layer causes the layer-wise error
√δE1  then the relative ultimate output error is

r = (cid:107) ˜YL − YL(cid:107)F
ξL

(cid:107)YL(cid:107)F

√δE1
(cid:107) 1
n Y1(cid:107)F

.

≈

√δE1/(cid:107) 1

Thus  we can see that even S1 may be quite large  the relative ultimate output error would still be about
n Y1(cid:107)F which is controllable in practice especially when most of modern deep networks
adopt maxout layer [23] as ultimate output. Actually  S0 is called as network gain representing the
ratio of the magnitude of the network output to the magnitude of the network input.

3.4 The Proposed Algorithm

3.4.1 Pruning on Fully-Connected Layers

To selectively prune parameters  our approach needs to compute the inverse Hessian matrix at each
layer to measure the sensitivities of each parameter of the layer  which is still computationally
expensive though tractable. In this section  we present an efﬁcient algorithm that can reduce the size
of the Hessian matrix and thus speed up computation on its inverse.
For each layer l  according to the deﬁnition of the error function used in Lemma 3.1  the ﬁrst
derivative of the error function with respect to ˆΘl is ∂El
j and
∂Θl
j are the j-th columns of the matrices ˆZl and Zl  respectively  and the Hessian matrix is deﬁned as:
zl
j)(cid:62)
Hl ≡ ∂2El
close to zl
j. Even in the late-stage of pruning when this
difference is not small  we can still ignore the corresponding term [13]. For layer l that has ml output
units  zl

∂2zl
∂(Θl)2 (ˆzl
j
j  we simply ignore the term containing ˆzl

mlj]  the Hessian matrix can be calculated via

. Note that for most cases ˆzl

(cid:33)
= − 1

(cid:18) ∂zl

∂(Θl)2 = 1

j)  where ˆzl

(ˆzl
j − zl

(cid:80)n

j = [zl

1j  . . .   zl

(cid:80)n

(cid:19)(cid:62)

j is quite

(cid:32)

∂zl
j
∂Θl

∂zl
j
∂Θl

j
∂Θl

n

j=1

n

j=1

−

n(cid:88)

j=1

Hl =

1
n

Hj

l =

1
n

(cid:32)

(cid:33)(cid:62)

∂zl
ij
∂Θl

∂zl
ij
∂Θl

 

(9)

j−zl
j−zl
ml(cid:88)
n(cid:88)

j=1

i=1

5

Figure 1: Illustration of shape of Hessian. For feed-forward neural networks  unit z1 gets its
activation via forward propagation: z = W(cid:62)y  where W ∈ R4×3  y = [y1  y2  y3  y4](cid:62)
∈ R4×1 
∈ R3×1. Then the Hessian matrix of z1 w.r.t. all parameters is denoted by
and z = [z1  z2  z3](cid:62)
H[z1]. As illustrated in the ﬁgure  H[z1]’s elements are zero except for those corresponding to W∗1
(the 1st column of W)  which is denoted by H11. H[z2] and H[z3] are similar. More importantly 
H−1 = diag(H−1
22   H−1
33 )  and H11 = H22 = H33. As a result  one only needs to compute
H−1
11 to obtain H−1 which signiﬁcantly reduces computational complexity.

11   H−1

(cid:21)

∂zl
1j
∂wml

(cid:20) ∂zl

where the Hessian matrix for a single instance j at layer l  Hj
of the size ml−1× ml. Speciﬁcally  the gradient of the ﬁrst output unit zl

l   is a block diagonal square matrix
=

1j w.s.t. Θl is ∂zl

1j
∂Θl

j

1j
∂w1

ij
∂wk

  . . .  

= yl−1

if k = i  otherwise ∂zl

  where wi is the i-th column of Wl. As zl

1j is the layer output before activation
function  its gradient is simply to calculate  and more importantly all output units’s gradients are
equal to the layer input: ∂zl
= 0. An illustrated example is shown in
Figure 1  where we ignore the scripts j and l for simplicity in presentation.
It can be shown that the block diagonal square matrix Hj
where 1 ≤ i ≤ ml  are all equal to ψj
block diagonal square matrix with its diagonal blocks being ( 1
n
Ψl = 1
n
matrix identity [13]:

lii ∈ Rml−1×ml−1 
is also a
l )−1. In addition  normally
l is degenerate and its pseudo-inverse can be calculated recursively via Woodbury

  and the inverse Hessian matrix H−1

l ’s diagonal blocks Hj

l = yl−1

(cid:80)n

j=1 ψj

j=1 ψj

(yl−1

ij
∂wk

(cid:62)

)

j

j

l

(cid:80)n
(cid:0)yl−1
(cid:1)(cid:62)
(cid:1)(cid:62)

(Ψl
j)

−1

yl−1

n +(cid:0)yl−1

−1

 

(Ψl
j)
−1
yl−1
−1

−1

(Ψl

j+1)

−1

= (Ψl
j)

(cid:80)t

j

−

j
(Ψl
j)
= αI  α ∈ [104  108]  and (Ψl)

j+1

−1

t = 1
t

(cid:1).

j=1 ψj

l with (Ψl
0)

is O(cid:0)nm2

j+1
−1. The size of Ψl
where Ψl
= (Ψl
n)
is then reduced to ml−1  and the computational complexity of calculating H−1
To make the estimated minimal change of the error function optimal in (6)  the layer-wise Hessian
matrices need to be exact. Since the layer-wise Hessian matrices only depend on the corresponding
layer inputs  they are always able to be exact even after several pruning operations. The only parameter
we need to control is the layer-wise error εl. Note that there may be a “pruning inﬂection point” after
which layer-wise error would drop dramatically. In practice  user can incrementally increase the size
of pruned parameters based on the sensitivity Lq  and make a trade-off between the pruning ratio and
the performance drop to set a proper tolerable error threshold or pruning ratio.
The procedure of our pruning algorithm for a fully-connected layer l is summarized as follows.
Step 1: Get layer input yl−1 from a well-trained deep network.
Step 2: Calculate the Hessian matrix Hlii  for i = 1  ...  ml  and its pseudo-inverse over the dataset 

l−1

l

and get the whole pseudo-inverse of the Hessian matrix.

Step 3: Compute optimal parameter change δΘl and the sensitivity Lq for each parameter at layer l.

Set tolerable error threshold .

6

H11H22H33W11W21W31W41y1y2y3y4z1z2z3H∈R12×12H11 H22 H33∈R4×4Step 4: Pick up parameters Θl[q]’s with the smallest sensitivity scores.

Step 5: If(cid:112)Lq ≤   prune the parameter Θl[q]’s and get new parameter values via ˆΘl = Θl + δΘl 

then repeat Step 4; otherwise stop pruning.

3.4.2 Pruning on Convolutional Layers

It is straightforward to generalize our method to a convolutional layer and its variants if we vectorize
ﬁlters of each channel and consider them as special fully-connected layers that have multiple inputs
(patches) from a single instance. Consider a vectorized ﬁlter wi of channel i  1 ≤ i ≤ ml  it
acts similarly to parameters which are connected to the same output unit in a fully-connected layer.
However  the difference is that for a single input instance j  every ﬁlter step of a sliding window across
of it will extract a patch Cjn from the input volume. Similarly  each pixel zl
ijn in the 2-dimensional
activation map that gives the response to each patch corresponds to one output unit in a fully-connected
layer. Hence  for convolutional layers  (9) is generalized as Hl = 1
∂[w1 ... wml ] 
n
where Hl is a block diagonal square matrix whose diagonal blocks are all the same. Then  we can
slightly revise the computation of the Hessian matrix  and extend the algorithm for fully-connected
layers to convolutional layers.
Note that the accumulated error of ultimate network output can be linearly bounded by layer-wise
error as long as the model is feed-forward. Thus  L-OBS is a general pruning method and friendly
with most of feed-forward neural networks whose layer-wise Hessian can be computed expediently
with slight modiﬁcations. However  if models have sizable layers like ResNet-101  L-OBS may not
be economical because of computational cost of Hessian  which will be studied in our future work.

(cid:80)ml

(cid:80)n

(cid:80)

∂zl

ijn

j=1

i=1

jn

4 Experiments

In this section  we verify the effectiveness of our proposed Layer-wise OBS (L-OBS) using various
architectures of deep neural networks in terms of compression ratio (CR)  error rate before retraining 
and the number of iterations required for retraining to resume satisfactory performance. CR is deﬁned
as the ratio of the number of preserved parameters to that of original parameters  lower is better.
We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly
pruning  2) OBD [12]  3) LWC [9]  4) DNS [11]  and 5) Net-Trim [6]. The deep architectures used for
experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset  CIFAR-Net2 [24]
on the CIFAR-10 dataset  AlexNet [25] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset. For
experiments  we ﬁrst well-train the networks  and apply various pruning approaches on networks to
evaluate their performance. The retraining batch size  crop method and other hyper-parameters are
under the same setting as used in LWC. Note that to make comparisons fair  we do not adopt any
other pruning related methods like Dropout or sparse regularizers on MNIST. In practice  L-OBS can
work well along with these techniques as shown on CIFAR-10 and ImageNet.

4.1 Overall Comparison Results

The overall comparison results are shown in Table 1. In the ﬁrst set of experiments  we prune each
layer of the well-trained LeNet-300-100 with compression ratios: 6.7%  20% and 65%  achieving
slightly better overall compression ratio (7%) than LWC (8%). Under comparable compression
ratio  L-OBS has quite less drop of performance (before retraining) and lighter retraining compared
with LWC whose performance is almost ruined by pruning. Classic pruning approach OBD is
also compared though we observe that Hessian matrices of most modern deep models are strongly
non-diagonal in practice. Besides relative heavy cost to obtain the second derivatives via the chain
rule  OBD suffers from drastic drop of performance when it is directly applied to modern deep
models.
To properly prune each layer of LeNet-5  we increase tolerable error threshold  from relative small
initial value to incrementally prune more parameters  monitor model performance  stop pruning and
set  until encounter the “pruning inﬂection point” mentioned in Section 3.4. In practice  we prune
each layer of LeNet-5 with compression ratio: 54%  43%  6% and 25% and retrain pruned model with

2A revised AlexNet for CIFAR-10 containing three convolutional layers and two fully connected layers.

7

Table 1: Overall comparison results. (For iterative L-OBS  err. after pruning regards the last pruning stage.)

Networks

Original error

CR

Err. after pruning

Re-Error

Method

Random
OBD
LWC
DNS
L-OBS
L-OBS (iterative)

OBD
LWC
DNS
L-OBS
L-OBS (iterative)

LWC
L-OBS

DNS
LWC
L-OBS

LeNet-300-100
LeNet-300-100
LeNet-300-100
LeNet-300-100
LeNet-300-100
LeNet-300-100

LeNet-5
LeNet-5
LeNet-5
LeNet-5
LeNet-5

CIFAR-Net
CIFAR-Net

AlexNet (Top-1 / Top-5 err.)
AlexNet (Top-1 / Top-5 err.)
AlexNet (Top-1 / Top-5 err.)

DNS
LWC
L-OBS (iterative)

VGG-16 (Top-1 / Top-5 err.)
VGG-16 (Top-1 / Top-5 err.)
VGG-16 (Top-1 / Top-5 err.)

1.76%
1.76%
1.76%
1.76%
1.76%
1.76%

1.27%
1.27%
1.27%
1.27%
1.27%

18.57%
18.57%

43.30 / 20.08%
43.30 / 20.08%
43.30 / 20.08%

31.66 / 10.12%
31.66 / 10.12%
31.66 / 10.12%

8%
8%
8%
1.8%
7%
1.5%

8%
8%
0.9%
7%
0.9%

9%
9%
5.7%
11%
11%

7.5%
7.5%
7.5%

85.72%
86.72%
81.32%

-

3.10%
2.43%

86.72%
89.55%

-

3.21%
2.04%

87.65%
21.32%

2.25%
1.96%
1.95%
1.99%
1.82%
1.96%

2.65%
1.36%
1.36%
1.27%
1.66%

19.36%
18.76%

#Re-Iters.
3.50 × 105
8.10 × 104
1.40 × 105
3.40 × 104

510
643

2.90 × 105
9.60 × 104
4.70 × 104

740
841

1.62 × 105

1020

-

43.91 / 20.72%
76.14 / 57.68%
44.06 / 20.64%
50.04 / 26.87% 43.11 / 20.01%

7.30 × 105
5.04 × 106
1.81 × 104
63.38% / 38.69% 1.07 × 106
2.35 × 107
73.61 / 52.64%
32.43 / 11.12%
8.63 × 104
37.32 / 14.82% 32.02 / 10.97%

-

much fewer iterations compared with other methods (around 1 : 1000). As DNS retrains the pruned
network after every pruning operation  we are not able to report its error rate of the pruned network
before retraining. However  as can be seen  similar to LWC  the total number of iterations used by
DNS for rebooting the network is very large compared with L-OBS. Results of retraining iterations
of DNS are reported from [11] and the other experiments are implemented based on TensorFlow [26].
In addition  in the scenario of requiring high pruning ratio  L-OBS can be quite ﬂexibly adopted to an
iterative version  which performs pruning and light retraining alternatively to obtain higher pruning
ratio with relative higher cost of pruning. With two iterations of pruning and retraining  L-OBS is
able to achieve as the same pruning ratio as DNS with much lighter total retraining: 643 iterations on
LeNet-300-100 and 841 iterations on LeNet-5.
Regarding comparison experiments on CIFAR-Net  we ﬁrst well-train it to achieve a testing error of
18.57% with Dropout and Batch-Normalization. We then prune the well-trained network with LWC
and L-OBS  and get the similar results as those on other network architectures. We also observe that
LWC and other retraining-required methods always require much smaller learning rate in retraining.
This is because representation capability of the pruned networks which have much fewer parameters
is damaged during pruning based on a principle that number of parameters is an important factor for
representation capability. However  L-OBS can still adopt original learning rate to retrain the pruned
networks. Under this consideration  L-OBS not only ensures a warm-start for retraining  but also
ﬁnds important connections (parameters) and preserve capability of representation for the pruned
network instead of ruining model with pruning.
Regarding AlexNet  L-OBS achieves an overall compression ratio of 11% without loss of accuracy
with 2.9 hours on 48 Intel Xeon(R) CPU E5-1650 to compute Hessians and 3.1 hours on NVIDIA
Tian X GPU to retrain pruned model (i.e. 18.1K iterations). The computation cost of the Hessian
inverse in L-OBS is negligible compared with that on heavy retraining in other methods. This
claim can also be supported by the analysis of time complexity. As mentioned in Section 3.4  the
time complexity of calculating H−1
SGD  then the approximate time complexity of retraining is O (IdM )  where d is the size of the
mini-batch  M and I are the total numbers of parameters and iterations  respectively. By considering
that M ≈
(Id (cid:29) n) as shown in experiments  complexity of calculating the Hessian (inverse) in L-OBS is quite
economic. More interestingly  there is a trade-off between compression ratio and pruning (including
retraining) cost. Compared with other methods  L-OBS is able to provide fast-compression: prune
AlexNet to 16% of its original size without substantively impacting accuracy (pruned top-5 error
20.98%) even without any retraining. We further apply L-OBS to VGG-16 that has 138M parameters.
To achieve more promising compression ratio  we perform pruning and retraining alteratively twice.
As can be seen from the table  L-OBS achieves an overall compression ratio of 7.5% without loss

(cid:1). Assume that neural networks are retrained via
(cid:1)  and retraining in other methods always requires millions of iterations

is O(cid:0)nm2

(cid:80)l=L

l−1

l

(cid:0)m2

l−1

l=1

8

(a) Top-5 test accuracy of L-OBS on ResNet-50
under different compression ratios.

(b) Memory Comparion between L-OBS and Net-
Trim on MNIST.

Table 2: Comparison of Net-Trim and Layer-wise OBS on the second layer of LeNet-300-100.

Method

Net-Trim
L-OBS
L-OBS

ξ2
r
0.13
0.70
0.71

Pruned Error

CR Method

13.24%
11.34%
10.83%

19%
3.4%
3.8%

Net-Trim
L-OBS
Net-Trim

ξ2
r
0.62
0.37
0.71

Pruned Error

28.45%
4.56%
47.69%

CR

7.4%
7.4%
4.2%

of accuracy taking 10.2 hours in total on 48 Intel Xeon(R) CPU E5-1650 to compute the Hessian
inverses and 86.3K iterations to retrain the pruned model.
We also apply L-OBS on ResNet-50 [27]. From our best knowledge  this is the ﬁrst work to perform
pruning on ResNet. We perform pruning on all the layers: All layers share a same compression ratio 
and we change this compression ratio in each experiments. The results are shown in Figure 2(a). As
we can see  L-OBS is able to maintain ResNet’s accuracy (above 85%) when the compression ratio is
larger than or equal to 45%.

4.2 Comparison between L-OBS and Net-Trim

As our proposed L-OBS is inspired by Net-Trim  which adopts (cid:96)1-norm to induce sparsity  we
conduct comparison experiments between these two methods. In Net-Trim  networks are pruned by
l Yl−1) − Yl(cid:107)F ≤ ξl 
formulating layer-wise pruning as a optimization: minWl (cid:107)Wl(cid:107)1 s.t. (cid:107)σ(W(cid:62)
where ξl corresponds to ξl
r(cid:107)Yl(cid:107)F in L-OBS. Due to memory limitation of Net-Trim  we only prune
the middle layer of LeNet-300-100 with L-OBS and Net-Trim under the same setting. As shown in
Table 2  under the same pruned error rate  CR of L-OBS outnumbers that of the Net-Trim by about
(cid:1). For example  Q requires
quadratic constraints used in Net-Trim for optimization is O(cid:0)2nm2
six times. In addition  Net-Trim encounters explosion of memory and time on large-scale datasets
and large-size parameters. Speciﬁcally  space complexity of the positive semideﬁnite matrix Q in

about 65.7Gb for 1 000 samples on MNIST as illustrated in Figure 2(b). Moreover  Net-Trim is
designed for multi-layer perceptrons and not clear how to deploy it on convolutional layers.
5 Conclusion

l ml−1

We have proposed a novel L-OBS pruning framework to prune parameters based on second order
derivatives information of the layer-wise error function and provided a theoretical guarantee on the
overall error in terms of the reconstructed errors for each layer. Our proposed L-OBS can prune
considerable number of parameters with tiny drop of performance and reduce or even omit retraining.
More importantly  it identiﬁes and preserves the real important part of networks when pruning
compared with previous methods  which may help to dive into nature of neural networks.
Acknowledgements

This work is supported by NTU Singapore Nanyang Assistant Professorship (NAP) grant
M4081532.020  Singapore MOE AcRF Tier-2 grant MOE2016-T2-2-060  and Singapore MOE
AcRF Tier-1 grant 2016-T1-001-159.

9

0.30.40.50.60.70.80.91.0CompressionRate0.600.650.700.750.800.850.900.951.00Accuracy(Top-5)100101102Numberofdatasample0.00.20.40.60.81.01.21.4Memoryused(Byte)×108Net-TrimOurMethodReferences
[1] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[2] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[3] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[4] Luisa de Vivo  Michele Bellesi  William Marshall  Eric A Bushong  Mark H Ellisman  Giulio
Tononi  and Chiara Cirelli. Ultrastructural evidence for synaptic scaling across the wake/sleep
cycle. Science  355(6324):507–510  2017.

[5] Misha Denil  Babak Shakibi  Laurent Dinh  Nando de Freitas  et al. Predicting parameters in
deep learning. In Advances in Neural Information Processing Systems  pages 2148–2156  2013.

[6] Nguyen N. Aghasi  A. and J. Romberg. Net-trim: A layer-wise convex pruning of deep neural

networks. Journal of Machine Learning Research  2016.

[7] Russell Reed. Pruning algorithms-a survey. IEEE transactions on Neural Networks  4(5):740–

747  1993.

[8] Yunchao Gong  Liu Liu  Ming Yang  and Lubomir Bourdev. Compressing deep convolutional

networks using vector quantization. arXiv preprint arXiv:1412.6115  2014.

[9] Song Han  Jeff Pool  John Tran  and William Dally. Learning both weights and connections
for efﬁcient neural network. In Advances in Neural Information Processing Systems  pages
1135–1143  2015.

[10] Yi Sun  Xiaogang Wang  and Xiaoou Tang. Sparsifying neural network connections for
face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 4856–4864  2016.

[11] Yiwen Guo  Anbang Yao  and Yurong Chen. Dynamic network surgery for efﬁcient dnns. In

Advances In Neural Information Processing Systems  pages 1379–1387  2016.

[12] Yann LeCun  John S Denker  Sara A Solla  Richard E Howard  and Lawrence D Jackel. Optimal

brain damage. In NIPs  volume 2  pages 598–605  1989.

[13] Babak Hassibi  David G Stork  et al. Second order derivatives for network pruning: Optimal

brain surgeon. Advances in neural information processing systems  pages 164–164  1993.

[14] Thomas Kailath. Linear systems  volume 156. Prentice-Hall Englewood Cliffs  NJ  1980.

[15] Nikolas Wolfe  Aditya Sharma  Lukas Drude  and Bhiksha Raj. The incredible shrinking neural
network: New perspectives on learning representations through the lens of pruning. arXiv
preprint arXiv:1701.04465  2017.

[16] Hengyuan Hu  Rui Peng  Yu-Wing Tai  and Chi-Keung Tang. Network trimming: A data-driven
neuron pruning approach towards efﬁcient deep architectures. arXiv preprint arXiv:1607.03250 
2016.

[17] Hao Li  Asim Kadav  Igor Durdanovic  Hanan Samet  and Hans Peter Graf. Pruning ﬁlters for

efﬁcient convnets. arXiv preprint arXiv:1608.08710  2016.

[18] Xiaojie Jin  Xiaotong Yuan  Jiashi Feng  and Shuicheng Yan. Training skinny deep neural

networks with iterative hard thresholding methods. arXiv preprint arXiv:1607.05423  2016.

[19] Cheng Tai  Tong Xiao  Yi Zhang  Xiaogang Wang  et al. Convolutional neural networks with

low-rank regularization. arXiv preprint arXiv:1511.06067  2015.

[20] Baoyuan Liu  Min Wang  Hassan Foroosh  Marshall Tappen  and Marianna Pensky. Sparse
convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 806–814  2015.

10

[21] R Tyrrell Rockafellar. Convex analysis. princeton landmarks in mathematics  1997.

[22] Xavier Glorot  Antoine Bordes  and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In

Aistats  volume 15  page 275  2011.

[23] Ian J Goodfellow  David Warde-Farley  Mehdi Mirza  Aaron C Courville  and Yoshua Bengio.

Maxout networks. ICML (3)  28:1319–1327  2013.

[24] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[25] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[26] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 
2016.

[27] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

11

,Xin Dong
Shangyu Chen
Sinno Pan