2013,Annealing between distributions by averaging moments,Many powerful Monte Carlo techniques for estimating partition functions  such as annealed importance sampling (AIS)  are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions  but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families: averaging the moments of the initial and target distributions. We derive an asymptotically optimal piecewise linear schedule for the moments path and show that it performs at least as well as geometric averages with a linear schedule.  Moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs)  which form the building blocks of many deep learning models  including Deep Belief Networks and Deep Boltzmann Machines.,Annealing Between Distributions by

Averaging Moments

Roger Grosse

Comp. Sci. & AI Lab

MIT

Cambridge  MA 02139

Chris J. Maddison

Ruslan Salakhutdinov

Dept. of Computer Science

Depts. of Statistics and Comp. Sci. 

University of Toronto
Toronto  ON M5S 3G4

University of Toronto

Toronto  ON M5S 3G4  Canada

Abstract

Many powerful Monte Carlo techniques for estimating partition functions  such
as annealed importance sampling (AIS)  are based on sampling from a sequence
of intermediate distributions which interpolate between a tractable initial distribu-
tion and the intractable target distribution. The near-universal practice is to use
geometric averages of the initial and target distributions  but alternative paths can
perform substantially better. We present a novel sequence of intermediate distribu-
tions for exponential families deﬁned by averaging the moments of the initial and
target distributions. We analyze the asymptotic performance of both the geomet-
ric and moment averages paths and derive an asymptotically optimal piecewise
linear schedule. AIS with moment averaging performs well empirically at esti-
mating partition functions of restricted Boltzmann machines (RBMs)  which form
the building blocks of many deep learning models.

Introduction

1
Many generative models are deﬁned in terms of an unnormalized probability distribution  and com-
puting the probability of a state requires computing the (usually intractable) partition function. This
is problematic for model selection  since one often wishes to compute the probability assigned to
held-out test data. While partition function estimation is intractable in general  there has been ex-
tensive research on variational [1  2  3] and sampling-based [4  5  6] approximations. In the context
of model comparison  annealed importance sampling (AIS) [4] is especially widely used because
given enough computing time  it can provide high-accuracy estimates. AIS has enabled precise
quantitative comparisons of powerful generative models in image statistics [7  8] and deep learning
[9  10  11]. Unfortunately  applying AIS in practice can be computationally expensive and require
laborious hand-tuning of annealing schedules. Because of this  many generative models still have
not been quantitatively compared in terms of held-out likelihood.
AIS requires deﬁning a sequence of intermediate distributions which interpolate between a tractable
initial distribution and the intractable target distribution. Typically  one uses geometric averages of
the initial and target distributions. Tantalizingly  [12] derived the optimal paths for some toy mod-
els in the context of path sampling and showed that they vastly outperformed geometric averages.
However  as choosing an optimal path is generally intractable  geometric averages still predominate.
In this paper  we present a theoretical framework for evaluating alternative paths. We propose a novel
path deﬁned by averaging moments of the initial and target distributions. We show that geometric
averages and moment averages optimize different variational objectives  derive an asymptotically
optimal piecewise linear schedule  and analyze the asymptotic performance of both paths. Our
proposed path often outperforms geometric averages at estimating partition functions of restricted
Boltzmann machines (RBMs).

1

2 Estimating Partition Functions
Suppose we have a probability distribution pb(x) = fb(x)/Zb deﬁned on a space X   where fb(x)
can be computed efﬁciently for a given x ∈ X   and we are interested in estimating the partition
function Zb. Annealed importance sampling (AIS) is an algorithm which estimates Zb by gradu-
ally changing  or “annealing ” a distribution. In particular  one must specify a sequence of K + 1
intermediate distributions pk(x) = fk(x)/Zk for k = 0  . . . K  where pa(x) = p0(x) is a tractable
initial distribution  and pb(x) = pK(x) is the intractable target distribution. For simplicity  assume
all distributions are strictly positive on X . For each pk  one must also specify an MCMC transi-
tion operator Tk (e.g. Gibbs sampling) which leaves pk invariant. AIS alternates between MCMC
transitions and importance sampling updates  as shown in Alg 1.
The output of AIS is an unbiased estimate ˆZb
of Zb. Remarkably  unbiasedness holds even in
the context of non-equilibrium samples along
the chain [4  13]. However  unless the interme-
diate distributions and transition operators are
carefully chosen  ˆZb may have high variance
and be far from Zb with high probability.
The mathematical formulation of AIS leaves
much ﬂexibility for choosing intermediate dis-
tributions. However  one typically deﬁnes a
[0  1] → P through some family P
path γ :
of distributions. The intermediate distributions
pk are chosen to be points along this path corresponding to a schedule 0 = β0 < β1 < . . . < βK = 1.
One typically uses the geometric path γGA  deﬁned in terms of geometric averages of pa and pb:

w(i) ← w(i) fk(xk−1)
fk−1(xk−1)
xk ← sample from Tk (x| xk−1)

x0 ← sample from p0(x)
w(i) ← Za
for k = 1 to K do

return ˆZb =(cid:80)M

Algorithm 1 Annealed Importance Sampling

for i = 1 to M do

end for

end for

i=1 w(i)/M

pβ(x) = fβ(x)/Z(β) = fa(x)1−βfb(x)β/Z(β).

(1)
Commonly  fa is the uniform distribution  and (1) reduces to pβ(x) = fb(x)β/Z(β). This motivates
the term “annealing ” and β resembles an inverse temperature parameter. As in simulated annealing 
the “hotter” distributions often allow faster mixing between modes which are isolated in pb.
AIS is closely related to a broader family of techniques for posterior inference and partition function
estimation  all based on the following identity from statistical physics:

log Zb − log Za =

Ex∼pβ

log fβ(x)

dβ.

(2)

(cid:90) 1

0

(cid:20) d

dβ

(cid:21)

Thermodynamic integration [14] estimates (2) using numerical quadrature  and path sampling [12]
does so with Monte Carlo integration. The weight update in AIS can be seen as a ﬁnite difference
approximation. Tempered transitions [15] is a Metropolis-Hastings proposal operator which heats
up and cools down the distribution  and computes an acceptance ratio by approximating (2).
The choices of a path and a schedule are central to all of these methods. Most work on adapting paths
has focused on tuning schedules along a geometric path [15  16  17]. [15] showed that the geometric
schedule was optimal for annealing the scale parameter of a Gaussian  and [16] extended this result
more broadly. The aim of this paper is to propose  analyze  and evaluate a novel alternative to γGA
based on averaging moments of the initial and target distributions.

3 Analyzing AIS Paths
When analyzing AIS  it is common to assume perfect transitions  i.e. that each transition opera-
tor Tk returns an independent and exact sample from the distribution pk [4]. This corresponds to
the (somewhat idealized) situation where the Markov chains mix quickly. As Neal [4] pointed out 
assuming perfect transitions  the Central Limit Theorem shows that the samples w(i) are approx-
imately log-normally distributed. In this case  the variances var(w(i)) and var(log w(i)) are both
monotonically related to E[log w(i)]. Therefore  our analysis focuses on E[log w(i)].
Assuming perfect transitions  the expected log weights are given by:
E[log w(i)] = log Za +

Epk [log fk+1(x) − log fk(x)] = log Zb − K−1(cid:88)

DKL(pk(cid:107)pk+1).

K−1(cid:88)

(3)

k=0

k=0

2

In other words  each log w(i) can be seen as a biased estimator of log Zb  where the bias δ =

log Zb − E[log w(i)] is given by the sum of KL divergences(cid:80)K−1

k=0 DKL(pk(cid:107)pk+1).

Suppose P is a family of probability distributions parameterized by θ ∈ Θ  and the K + 1 distribu-
tions p0  . . .   pK are chosen to be linearly spaced along a path γ : [0  1] → P. Let θ(β) represent
the parameters of the distribution γ(β). As K is increased  the bias δ decays like 1/K  and the
asymptotic behavior is determined by a functional F(γ).
Theorem 1. Suppose K + 1 distributions pk are linearly spaced along a path γ. Assuming per-
fect transitions  if θ(β) and the Fisher information matrix Gθ(β) = covx∼pθ (∇θ log pθ(x)) are
continuous and piecewise smooth  then as K → ∞ the bias δ behaves as follows:

Kδ = K

DKL(pk(cid:107)pk+1) → F(γ) ≡ 1
2

˙θ(β)T Gθ(β) ˙θ(β)dβ 

(4)

where ˙θ(β) represents the derivative of θ with respect to β. [See supplementary material for proof.]

This result reveals a relationship with path sampling  as [12] showed that the variance of the path
sampling estimator is proportional to the same functional. One useful result from their analysis is
a derivation of the optimal schedule along a given path. In particular  the value of F(γ) using the
optimal schedule is given by (cid:96)(γ)2/2  where (cid:96) is the Riemannian path length deﬁned by

K−1(cid:88)

k=0

(cid:90) 1

0

(cid:90) 1

(cid:113)

(cid:96)(γ) =

˙θ(β)T Gθ(β) ˙θ(β)dβ.

(5)

0

Intuitively  the optimal schedule allocates more distributions to regions where pβ changes quickly.
While [12] derived the optimal paths and schedules for some simple examples  they observed that
this is intractable in most cases and recommended using geometric paths in practice.
The above analysis assumes perfect transitions  which can be unrealistic in practice because many
distributions have separated modes between which mixing is difﬁcult. As Neal [4] observed  in
such cases  AIS can be viewed as having two sources of variance: that caused by variability within
a mode  and that caused by misallocation of samples between modes. The former source of vari-
ance is well modeled by the perfect transitions analysis and can be made small by adding more
intermediate distributions. The latter  however  can persist even with large numbers of intermediate
distributions. While our theoretical analysis assumes perfect transitions  our proposed method often
gave substantial improvement empirically in situations with poor mixing.

4 Moment Averaging
As discussed in Section 2  the typical choice of intermediate distributions for AIS is the geometric
averages path γGA given by (1). In this section  we propose an alternative path for exponential
family models. An exponential family model is deﬁned as

h(x) exp(cid:0)ηT g(x)(cid:1)  

p(x) =

1
Z(η)

(6)

where η are the natural parameters and g are the sufﬁcient statistics. Exponential families include a
wide variety of statistical models  including Markov random ﬁelds.
In exponential families  geometric averages correspond to averaging the natural parameters:

η(β) = (1 − β)η(0) + βη(1).

(7)
Exponential families can also be parameterized in terms of their moments s = E[g(x)]. For any
minimal exponential family (i.e. one whose sufﬁcient statistics are linearly independent)  there is a
one-to-one mapping between moments and natural parameters [18  p. 64]. We propose an alternative
to γGA called the moment averages path  denoted γM A  and deﬁned by averaging the moments of
the initial and target distributions:

(8)
This path exists for any exponential family model  since the set of realizable moments is convex
[18]. It is unique  since g is unique up to afﬁne transformation.

s(β) = (1 − β)s(0) + βs(1).

3

E[xxT ] = − 1

As an illustrative example  consider a multivariate Gaussian distribution parameterized by the mean
µ and covariance Σ. The moments are E[x] = µ and − 1
2 (Σ + µµT ). By plugging
these into (8)  we ﬁnd that γM A is given by:
µ(β) = (1 − β)µ(0) + βµ(1)
Σ(β) = (1 − β)Σ(0) + βΣ(1) + β(1 − β)(µ(1) − µ(0))(µ(1) − µ(0))T .

(9)
(10)
In other words  the means are linearly interpolated  and the covariances are linearly interpolated
and stretched in the direction connecting the two means.
Intuitively  this stretching is a useful
property  because it increases the overlap between successive distributions with different means. A
comparison of the two paths is shown in Figure 1.

2

Next consider the example of a restricted Boltzmann machine (RBM) 
a widely used model in deep learning. A binary RBM is a Markov
random ﬁeld over binary vectors v (the visible units) and h (the hidden
units)  and which has the distribution

p(v  h) ∝ exp(cid:0)aT v + bT h + vT Wh(cid:1) .

(11)
The parameters of the model are the visible biases a  the hidden biases
b  and the weights W. Since these parameters are also the natural
parameters in the exponential family representation  γGA reduces to
linearly averaging the biases and the weights. The sufﬁcient statistics
of the model are the visible activations v  the hidden activations h  and
the products vhT . Therefore  γM A is deﬁned by:
E[v]β = (1 − β)E[v]0 + βE[v]1
E[h]β = (1 − β)E[h]0 + βE[h]1

E[vhT ]β = (1 − β)E[vhT ]0 + βE[vhT ]1

(12)
(13)
(14)

Figure 1: Comparison of
γGA and γM A for multivari-
ate Gaussians:
intermediate
distribution for β = 0.5 
and µ(β) for β evenly spaced
from 0 to 1.

For many models of interest  including RBMs  it is infeasible to de-
termine γM A exactly  as it requires solving two often intractable prob-
lems: (1) computing the moments of pb  and (2) solving for model
parameters which match the averaged moments s(β). However  much work has been devoted to
practical approximations [19  20]  some of which we use in our experiments with intractable mod-
els. Since it would be infeasible to moment match every βk even approximately  we introduce the
moment averages spline (MAS) path  denoted γM AS. We choose a set of R values β1  . . .   βR called
knots  and solve for the natural parameters η(βj) to match the moments s(βj) for each knot. We
then interpolate between the knots using geometric averages. The analysis of Section 4.2 shows that 
under the assumption of perfect transitions  using γM AS in place of γM A does not affect the cost
functional F deﬁned in Theorem 1.
4.1 Variational Interpretation
By interpreting γGA and γM A as optimizing different variational objectives  we gain additional
insight into their behavior. For geometric averages  the intermediate distribution γGA(β) minimizes
a weighted sum of KL divergences to the initial and target distributions:

p(GA)
β

= arg min

q

(1 − β)DKL(q(cid:107)pa) + βDKL(q(cid:107)pb).

(15)

On the other hand  γM A minimizes the weighted sum of KL divergences in the reverse direction:

p(M A)
β

= arg min

q

(1 − β)DKL(pa(cid:107)q) + βDKL(pb(cid:107)q).

(16)

See the supplementary material for the derivations. The objective function (15) is minimized by a
distribution which puts signiﬁcant mass only in the “intersection” of pa and pb  i.e. those regions
which are likely under both distributions. By contrast  (16) encourages the distribution to be spread
out in order to capture all high probability regions of both pa and pb. This interpretation helps
explain why the intermediate distributions in the Gaussian example of Figure 1 take the shape that
they do. In our experiments  we found that γM A often gave more accurate results than γGA because
the intermediate distributions captured regions of the target distribution which were missed by γGA.

4

4.2 Asymptotics under Perfect Transitions
In general  we found that γGA and γM A can look very different. Intriguingly  both paths always
result in the same value of the cost functional F(γ) of Theorem 1 for any exponential family model.
Furthermore  nothing is lost by using the spline approximation γM AS in place of γM A:
Theorem 2. For any exponential family model with natural parameters η and moments s  all three
paths share the same value of the cost functional:
F(γGA) = F(γM A) = F(γM AS) =

(η(1) − η(0))T (s(1) − s(0)).

(17)

Proof. The two parameterizations of exponential families satisfy the relationship Gη ˙η = ˙s [21 
sec. 3.3]. Therefore  F(γ) can be rewritten as 1
0 ˙η(β)T ˙s(β) dβ. Because γGA and γM A linearly
interpolate the natural parameters and moments respectively 

2

1
2

(cid:82) 1

(cid:90) 1
(cid:90) 1

0

F(γGA) =

1
2

(η(1) − η(0))T

˙s(β) dβ =

1
2

F(γM A) =

(s(1) − s(0))T

(19)
Finally  to show that F(γM AS) = F(γM A)  observe that γM AS uses the geometric path between
each pair of knots γ(βj) and γ(βj+1)  while γM A uses the moments path. The above analysis shows
the costs must be equal for each segment  and therefore equal for the entire path.

˙η(β) dβ =

0

(η(1) − η(0))T (s(1) − s(0))

1
2
(s(1) − s(0))T (η(1) − η(0)).

1
2

(18)

This analysis shows that all three paths result in the same expected log weights asymptotically 
assuming perfect transitions. There are several caveats  however. First  we have noticed experimen-
tally that γM A often yields substantially more accurate estimates of Zb than γGA even when the
average log weights are comparable. Second  the two paths can have very different mixing prop-
erties  which can strongly affect the results. Third  Theorem 2 assumes linear schedules  and in
principle there is room for improvement if one is allowed to tune the schedule.
For instance  consider annealing between two Gaussians pa = N (µa  σ) and pb = N (µb  σ). The
optimal schedule for γGA is a linear schedule with cost F(γGA) = O(d2)  where d = |µb − µa|/σ.
Using a linear schedule  the moment path also has cost O(d2)  consistent with Theorem 2. However 
most of the cost of the path results from instability near the endpoints  where the variance changes
suddenly. Using an optimal schedule  which allocates more distributions near the endpoints  the cost
functional falls to O((log d)2)  which is within a constant factor of the optimal path derived by [12].
(See the supplementary material for the derivations.) In other words  while F(γGA) = F(γM A) 
they achieve this value for different reasons: γGA follows an optimal schedule along a bad path 
while γM A follows a bad schedule along a near-optimal path. We speculate that  combined with the
procedure of Section 4.3 for choosing a schedule  moment averages may result in large reductions
in the cost functional for some models.
4.3 Optimal Binned Schedules
In general  it is hard to choose a good schedule for a given path. However  consider the set of binned
schedules  where the path is divided into segments  some number Kj of intermediate distributions
are allocated to each segment  and the distributions are spaced linearly within each segment. Under
the assumption of perfect transitions  there is a simple formula for an asymptotically optimal binned
schedule which requires only the parameters obtained through moment averaging:
Theorem 3. Let γ be any path for an exponential family model deﬁned by a set of knots βj  each with
natural parameters ηj and moments sj  connected by segments of either γGA or γM A paths. Then 
under the assumption of perfect transitions  an asymptotically optimal allocation of intermediate
distributions to segments is given by:

(20)
2 (ηj+1 − ηj)T (sj+1 − sj). Hence 
Proof. By Theorem 2  the cost functional for segment j is Fj = 1
with Kj distributions allocated to it  it contributes Fj/Kj to the total cost. The values of Kj which

(ηj+1 − ηj)T (sj+1 − sj).

Kj ∝(cid:113)
j Kj = K and Kj ≥ 0 are given by Kj ∝(cid:112)Fj.

j Fj/Kj subject to(cid:80)

minimize(cid:80)

5

Figure 2: Estimates of log Zb for a normalized Gaussian as K  the number of intermediate distributions  is
varied. True value: log Zb = 0. Error bars show bootstrap 95% conﬁdence intervals. (Best viewed in color.)

1 −0.85

−0.85

1

0

(cid:1) (cid:0)

(cid:1)(cid:1) and N(cid:0)(cid:0) 10

0

(cid:1) (cid:0) 1 0.85

0.85 1

(cid:1)(cid:1). As both distributions

whose parameters are N(cid:0)(cid:0) −10

5 Experimental Results
In order to compare our proposed path with geometric averages  we ran AIS using each path to es-
timate partition functions of several probability distributions. For all of our experiments  we report
two sets of results. First  we show the estimates of log Z as a function of K  the number of interme-
diate distributions  in order to visualize the amount of computation necessary to obtain reasonable
accuracy. Second  as recommended by [4]  we report the effective sample size (ESS) of the weights
for a large K. This statistic roughly measures how many independent samples one obtains using
AIS.1 All results are based on 5 000 independent AIS runs  so the maximum possible ESS is 5 000.
5.1 Annealing Between Two Distant Gaussians
In our ﬁrst experiment  the initial and target distributions were the two Gaussians shown in Fig. 1 
are normalized  Za = Zb = 1. We compared γGA and γM A both under perfect transitions  and
using the Gibbs transition operator. We also compared linear schedules with the optimal binned
schedules of Section 4.3  using 10 segments evenly spaced from 0 to 1.
Figure 2 shows the estimates of log Zb for K ranging from 10 to 1 000. Observe that with 1 000
intermediate distributions  all paths yielded accurate estimates of log Zb. However  γM A needed
fewer intermediate distributions to achieve accurate estimates. For example  with K = 25  γM A
resulted in an estimate within one nat of log Zb  while the estimate based on γGA was off by 27 nats.
This result may seem surprising in light of Theorem 2  which implies that F(γGA) = F(γM A) for
linear schedules. In fact  the average log weights for γGA and γM A were similar for all values of K 
as the theorem would suggest; e.g.  with K = 25  the average was -27.15 for γM A and -28.04 for
γGA. However  because the γM A intermediate distributions were broader  enough samples landed
in high probability regions to yield reasonable estimates of log Zb.
5.2 Partition Function Estimation for RBMs
Our next set of experiments focused on restricted Boltzmann machines (RBMs)  a building block of
many deep learning models (see Section 4). We considered RBMs trained with three different meth-
ods: contrastive divergence (CD) [19] with one step (CD1)  CD with 25 steps (CD25)  and persistent
contrastive divergence (PCD) [20]. All of the RBMs were trained on the MNIST handwritten digits
dataset [22]  which has long served as a benchmark for deep learning algorithms. We experimented
both with small  tractable RBMs and with full-size  intractable RBMs.
Since it is hard to compute γM A exactly for RBMs  we used the moments spline path γM AS of
Section 4 with the 9 knot locations 0.1  0.2  . . .   0.9. We considered the two initial distributions
discussed by [9]: (1) the uniform distribution  equivalent to an RBM where all the weights and
biases are set to 0  and (2) the base rate RBM  where the weights and hidden biases are set to 0  and
the visible biases are set to match the average pixel values over the MNIST training set.
Small  Tractable RBMs: To better understand the behavior of γGA and γM AS  we ﬁrst evaluated
the paths on RBMs with only 20 hidden units. In this setting  it is feasible to exactly compute the

1The ESS is deﬁned as M/(1 + s2(w(i)∗ )) where s2(w(i)∗ ) is the sample variance of the normalized weights
[4]. In general  one should regard ESS estimates cautiously  as they can give misleading results in cases where
an algorithm completely misses an important mode of the distribution. However  as we report the ESS in cases
where the estimated partition functions are close to the true value (when known) or agree closely with each
other  we believe the statistic is meaningful in our comparisons.

6

Figure 3: Estimates of log Zb for the tractable PCD(20) RBM as K  the number of intermediate distributions 
is varied. Error bars indicate bootstrap 95% conﬁdence intervals. (Best viewed in color.)

path & schedule

pa(v)
uniform GA linear
uniform GA optimal binned
uniform MAS linear
uniform MAS optimal binned

log Zb
178.06

PCD(20)
log ˆZb
177.99
177.92
178.09
178.08

ESS
204
142
289
934

log Zb
279.59

CD1(20)
log ˆZb
279.60
279.51
279.59
279.60

ESS
248
124
2686
2619

Table 1: Comparing estimates of log Zb and effective sample size (ESS) for tractable RBMs. Results are shown
for K = 100 000 intermediate distributions  with 5 000 chains and Gibbs transitions. Bolded values indicate
ESS estimates that are not signiﬁcantly different from the largest value (bootstrap hypothesis test with 1 000
samples at α = 0.05 signiﬁcance level). The maximum possible ESS is 5 000.

Figure 4: Visible activations for samples from the PCD(500) RBM. (left) base rate RBM  β = 0 (top) geometric
path (bottom) MAS path (right) target RBM  β = 1.

partition function and moments and to generate exact samples by exhaustively summing over all
220 hidden conﬁgurations. The moments of the target RBMs were computed exactly  and moment
matching was performed with conjugate gradient using the exact gradients.
The results are shown in Figure 3 and Table 1. Under perfect transitions  γGA and γM AS were both
able to accurately estimate log Zb using as few as 100 intermediate distributions. However  using
the Gibbs transition operator  γM AS gave accurate estimates using fewer intermediate distributions
and achieved a higher ESS at K = 100 000. To check that the improved performance didn’t rely on
accurate moments of pb  we repeated the experiment with highly biased moments.2 The differences
in log ˆZb and ESS compared to the exact moments condition were not statistically signiﬁcant.
Full-size  Intractable RBMs: For intractable RBMs  moment averaging required approximately
solving two intractable problems: moment estimation for the target RBM  and moment matching.
We estimated the moments from 1 000 independent Gibbs chains  using 10 000 Gibbs steps with
1 000 steps of burn-in. The moment averaged RBMs were trained using PCD. (We used 50 000 up-
dates with a ﬁxed learning rate of 0.01 and no momentum.) In addition  we ran a cheap  inaccurate
moment matching scheme (denoted MAS cheap) where visible moments were estimated from the
empirical MNIST base rate and the hidden moments from the conditional distributions of the hidden
units given the MNIST digits. Intermediate RBMs were ﬁt using 1 000 PCD updates and 100 par-
ticles  for a total computational cost far smaller than that of AIS itself. Results of both methods are

2In particular  we computed the biased moments from the conditional distributions of the hidden units given

the MNIST training examples  where each example of digit class i was counted i + 1 times.

7

Figure 5: Estimates of log Zb for intractable RBMs. Error bars indicate bootstrap 95% conﬁdence intervals.
(Best viewed in color.)

path
GA linear

pa(v)
uniform
uniform MAS linear
uniform MAS cheap linear
base rate GA linear
base rate MAS linear
base rate MAS cheap linear

CD1(500)

PCD(500)

log ˆZb
341.53
359.09
359.09
359.10
359.07
359.09

ESS
4
3076
3773
4924
2203
2465

log ˆZb
417.91
418.27
418.33
418.20
418.26
418.25

ESS
169
620
5
159
1460
359

CD25(500)
log ˆZb
451.34
449.22
450.90
451.27
451.31
451.14

ESS
13
12
30
2888
304
244

Table 2: Comparing estimates of log Zb and effective sample size (ESS) for intractable RBMs. Results are
shown for K = 100 000 intermediate distributions  with 5 000 chains and Gibbs transitions. Bolded values
indicate ESS estimates that are not signiﬁcantly different from the largest value (bootstrap hypothesis test with
1 000 samples at α = 0.05 signiﬁcance level). The maximum possible ESS is 5 000.

shown in Figure 5 and Table 2. Overall  the MAS results compare favorably with those of GA on
both of our metrics. Performance was comparable under MAS cheap  suggesting that γM AS can be
approximated cheaply and effectively. As with the tractable RBMs  we found that optimal binned
schedules made little difference in performance  so we focus here on linear schedules.
The most serious failure was γGA for CD1(500) with uniform initialization  which underestimated
our best estimates of the log partition function (and hence overestimated held-out likelihood) by
nearly 20 nats. The geometric path from uniform to PCD(500) and the moments path from uni-
form to CD1(500) also resulted in underestimates  though less drastic. The rest of the paths agreed
closely with each other on their partition function estimates  although some methods achieved sub-
stantially higher ESS values on some RBMs. One conclusion is that it’s worth exploring multiple
initializations and paths for a given RBM in order to ensure accurate results.
Figure 4 compares samples along γGA and γM AS for the PCD(500) RBM using the base rate ini-
tialization. For a wide range of β values  the γGA RBMs assigned most of their probability mass
to blank images. As discussed in Section 4.1  γGA prefers conﬁgurations which are probable under
both the initial and target distributions. In this case  the hidden activations were closer to uniform
conditioned on a blank image than on a digit  so γGA preferred blank images. By contrast  γM AS
yielded diverse  blurry digits which gradually coalesced into crisper ones.

6 Conclusion

We presented a theoretical analysis of the performance of AIS paths and proposed a novel path
for exponential families based on averaging moments. We gave a variational interpretation of this
path and derived an asymptotically optimal piecewise linear schedule. Moment averages performed
well empirically at estimating partition functions of RBMs. We hope moment averaging can also
improve other path-based sampling algorithms which typically use geometric averages  such as path
sampling [12]  parallel tempering [23]  and tempered transitions [15].

Acknowledgments

This research was supported by NSERC and Quanta Computer. We thank Geoffrey Hinton for
helpful discussions. We also thank the anonymous reviewers for thorough and helpful feedback.

8

References

[1] J. S. Yedidia  W. T. Freeman  and Y. Weiss. Constructing free-energy approximations and gen-

eralized belief propagation algorithms. IEEE Trans. on Inf. Theory  51(7):2282–2312  2005.

[2] Martin J. Wainwright  Tommi Jaakkola  and Alan S. Willsky. A new class of upper bounds on
the log partition function. IEEE Transactions on Information Theory  51(7):2313–2335  2005.
[3] Amir Globerson and Tommi Jaakkola. Approximate Inference Using Conditional Entropy
Decompositions. In 11th International Workshop on AI and Statistics (AISTATS’2007)  2007.
[4] Radford Neal. Annealed importance sampling. Statistics and Computing  11:125–139  2001.
[5] John Skilling. Nested sampling for general Bayesian computation. Bayesian Analysis 

1(4):833–859  2006.

[6] Pierre Del Moral  Arnaud Doucet  and Ajay Jasra. Sequential Monte Carlo samplers. Journal

of the Royal Statistical Society: Series B (Methodology)  68(3):411–436  2006.

[7] Jascha Sohl-Dickstein and Benjamin J. Culpepper. Hamiltonian annealed importance sampling

for partition function estimation. Technical report  Redwood Center  UC Berkeley  2012.

[8] Lucas Theis  Sebastian Gerwinn  Fabian Sinz  and Matthias Bethge. In all likelihood  deep

belief is not enough. Journal of Machine Learning Research  12:3071–3096  2011.

[9] Ruslan Salakhutdinov and Ian Murray. On the quantitative analysis of deep belief networks.

In Int’l Conf. on Machine Learning  pages 6424–6429  2008.

[10] Guillaume Desjardins  Aaron Courville  and Yoshua Bengio. On tracking the partition func-

tion. In NIPS 24. MIT Press  2011.

[11] Graham Taylor and Geoffrey Hinton. Products of hidden Markov models: It takes N > 1 to

tango. In Uncertainty in Artiﬁcial Intelligence  2009.

[12] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: From importance

sampling to bridge sampling to path sampling. Statistical Science  13(2):163–186  1998.

[13] Christopher Jarzynski. Equilibrium free-energy differences from nonequilibrium measure-

ments: A master-equation approach. Physical Review E  56:5018–5035  1997.

[14] Daan Frenkel and Berend Smit. Understanding Molecular Simulation: From Algorithms to

Applications. Academic Press  2 edition  2002.

[15] Radford Neal. Sampling from multimodal distributions using tempered transitions. Statistics

and Computing  6:353–366  1996.

[16] Gundula Behrens  Nial Friel  and Merrilee Hurn. Tuning tempered transitions. Statistics and

Computing  22:65–78  2012.

[17] Ben Calderhead and Mark Girolami. Estimating Bayes factors via thermodynamic integration
and population MCMC. Computational Statistics and Data Analysis  53(12):4028–4045  2009.
[18] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[19] Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation  14(8):1771–1800  2002.

[20] Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likeli-

hood gradient. In Intl. Conf. on Machine Learning  2008.

[21] Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry. Oxford University

Press  2000.

[22] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[23] Y. Iba. Extended ensemble Monte Carlo.

12(5):623–656  2001.

International Journal of Modern Physics C 

9

,Roger Grosse
Chris Maddison
Russ Salakhutdinov
Zhongwen Xu
Joseph Modayil
Hado van Hasselt
Andre Barreto
David Silver
Tom Schaul