2013,Contrastive Learning Using Spectral Methods,In many natural settings  the analysis goal is not to characterize a single data set in isolation  but rather to understand the difference between one set of observations and another. For example  given a background corpus of news articles together with writings of a particular author  one may want a topic model that explains word patterns and themes specific to the author. Another example comes from genomics  in which biological signals may be collected from different regions of a genome  and one wants a model that captures the differential statistics observed in these regions. This paper formalizes this notion of contrastive learning for  mixture models  and develops spectral algorithms for inferring mixture components specific to a foreground data set when contrasted with a background data set. The method builds on recent moment-based estimators and tensor decompositions for latent variable models  and has the intuitive feature of using background data statistics to appropriately modify moments estimated from foreground data. A key advantage of the method is that the background data need only be coarsely modeled  which is important when the background is too complex  noisy  or not of interest. The method is demonstrated on applications in contrastive topic modeling and genomic sequence analysis.,Contrastive Learning Using Spectral Methods

James Zou

Harvard University

Daniel Hsu

Columbia University

David Parkes

Harvard University

Ryan Adams

Harvard University

Abstract

In many natural settings  the analysis goal is not to characterize a single data set in
isolation  but rather to understand the difference between one set of observations
and another. For example  given a background corpus of news articles together
with writings of a particular author  one may want a topic model that explains
word patterns and themes speciﬁc to the author. Another example comes from
genomics  in which biological signals may be collected from different regions
of a genome  and one wants a model that captures the differential statistics ob-
served in these regions. This paper formalizes this notion of contrastive learning
for mixture models  and develops spectral algorithms for inferring mixture com-
ponents speciﬁc to a foreground data set when contrasted with a background data
set. The method builds on recent moment-based estimators and tensor decomposi-
tions for latent variable models  and has the intuitive feature of using background
data statistics to appropriately modify moments estimated from foreground data.
A key advantage of the method is that the background data need only be coarsely
modeled  which is important when the background is too complex  noisy  or not
of interest. The method is demonstrated on applications in contrastive topic mod-
eling and genomic sequence analysis.

1

Introduction

Generative latent variable models offer an intuitive way to explain data in terms of hidden structure 
and are a cornerstone of exploratory data analysis. Popular examples of generative latent variable
models include Latent Dirichlet Allocation (LDA) [1] and Hidden Markov Models (HMMs) [2] 
although the modularity of the generative approach has led to a wide range of variations. One of
the challenges of using latent variable models for exploratory data analysis  however  is developing
models and learning techniques that accurately reﬂect the intuitions of the modeler. In particular 
when analyzing multiple specialized data sets  it is often the case that the most salient statistical
structure—that most easily found by ﬁtting latent variable models—is shared across all the data and
does not reﬂect interesting speciﬁc local structure. For example  if we apply a topic model to a set
of English-language scientiﬁc papers on computer science  we might hope to identify different co-
occurring words within subﬁelds such as theory  systems  graphics  etc. Instead  such a model will
simply learn about English syntactic structure and invent topics that reﬂect uninteresting statistical
correlations between stop words [3].
Intuitively  what we would like from such an exploratory
analysis is to answer the question: What makes these data different from other sets of data in the
same broad category?
To answer this question  we develop a new set of techniques that we refer to as contrastive learning
methods. These methods differentiate between foreground and background data and seek to learn
a latent variable model that captures statistical relationships that appear in the foreground but do
not appear in the background. Revisiting the previous scientiﬁc topics example  contrastive learning
could treat computer science papers as a foreground corpus and (say) English-language news articles
as a background corpus. As both corpora share the same broad syntactic structure  a contrastive
foreground topic model would be more likely to discover semantic relationships between words that
are speciﬁc to computer science. This intuition has broad applicability in other models and domains

1

Background
Foreground

 

 

(a) PCA

(b) Linear contrastive analysis

Figure 1: These ﬁgures show foreground and background data from Gaussian distributions. The foreground
data has greater variance in its minor direction  but the same variance in its major direction. The means are
slightly different. Different projection lines are shown for different methods  to illustrate the difference be-
tween (a) the purely unsupervised variance-preserving linear projection of principal component analysis  (b)
the contrastive foreground projection that captures variance that is not present in the background.

as well. For example  in genomics one might use a contrastive hidden Markov model to amplify the
signal of a particular class of sequences  relative to the broader genome.
Note that the objective of contrastive learning is not to discriminate between foreground and back-
ground data  but to learn an interpretable generative model that captures the differential statistics
between the two data sets. To clarify this difference  consider the difference between principal com-
ponent analysis and contrastive analysis. Principal component analysis ﬁnds the linear projection
that maximally preserves variance without regard to foreground versus background. A contrastive
approach  however  would try to ﬁnd a linear projection that maximally preserves the foreground
variance that is not explained by the background. Figure 1 illustrates the differences between these.
Novelty detection [4] is also related  but it does not directly learn a generative model of the novelty.

Our contributions. We formalize the concept of contrastive learning for mixture models and
present new spectral contrast algorithms. We prove that by appropriately “subtracting” background
moments from the foreground moments  our algorithms recover the model for the foreground-
speciﬁc data. To achieve this  we extend recent developments in learning latent variable models
with moment matching and tensor decompositions. We demonstrate the effectiveness  robustness 
and scalability of our method in contrastive topic modeling and contrastive genomics.

2 Contrastive learning in mixture models

Many data can be naturally described by a mixture model. The general mixture model has the form

p({xn}N

n=1;{(µj  wj)}J

j=1) =

N￿n=1￿ J￿j=1

wjf (xn|µj)￿

(1)

where {µj} are the parameters of the mixture components  {wj} are the mixture weights 
and f (·|µj) is the density of the j-th mixture component. Each µj is a vector in some parame-
ter space  and a common estimation task is to infer the component parameters {(µj  wj)} given the
observed data {xn}.
In many applications  we have two sets of observations {xf
n}  which we call the foreground
data and the background data  respectively. The foreground and background are generated by two
possibly overlapping sets of mixture components. More concretely  let {µj}j∈A  {µj}j∈B  and
{µj}j∈C be three disjoint sets of parameters  with A  B  and C being three disjoint index sets. The
foreground {xf
j)}j∈A∪B  and the background {xb
n}
is generated from {(µj  wb
The goal of contrastive learning is to infer the parameters {(µj  wf
j)}j∈A  which we call the
j)}j∈A∪B just from {xf
n} 
foreground-speciﬁc model. The direct approach would be to infer {(µj  wf
n}  and then pick out the components speciﬁc to
and in parallel infer {(µj  wb
the foreground. However  this involves explicitly learning a model for the background data  which

n} is generated from the mixture model {(µj  wf

j )}j∈B∪C just from {xb

n} and {xb

j )}j∈B∪C.

2

n} is too noisy  or if we do not want to de-
is undesirable if the background is too complex  if {xb
vote computational power to learn the background. In many applications  we are only interested in
learning a generative model for the difference between the foreground and background  because that
contrast is the interesting signal.
In this paper  we introduce an efﬁcient and general approach to learn the foreground-speciﬁc model
without having to learn an accurate model of the background. Our approach is based on a method-of-
moments that uses higher-order tensor decompositions for estimation [5]; we generalize the tensor
decomposition technique to deal with our task of contrastive learning. Many other recent spectral
learning algorithms for latent variable models are also based on the method-of-moments (e.g.  [6–
13])  but their parameter estimation can not account for the asymmetry between foreground and
background.
We demonstrate spectral contrastive learning through two concrete applications: contrastive topic
modeling and contrastive genomics. In contrastive topic modeling we are given a foreground cor-
pus of documents and a background corpus. We want to learn a fully generative topic model that
explains the foreground-speciﬁc documents (the contrast). We show that even when the background
is extremely sparse—too noisy to learn a good background topic model—our spectral contrast algo-
rithm still recovers foreground-speciﬁc topics. In contrastive genomics  sequence data is modeled by
HMMs. The foreground data is generated by a mixture of two HMMs; one is foreground-speciﬁc 
and the other captures some background process. The background data is generated by this sec-
ond HMM. Contrastive learning ampliﬁes the foreground-speciﬁc signal  which have meaningful
biological interpretations.

3 Contrastive topic modeling

To illustrate contrastive analysis and introduce tensor methods  we consider a simple topic model
where each document is generated by exactly one topic. In LDA [1]  this corresponds to setting the
Dirichlet prior hyper-parameter α → 0. The techniques here can be extended to the general α> 0
case using the moment transformations given in [10]. The generative topic model for a document is
as follows.

• A word x is represented by an indicator vector ex ∈ RD which is 1 in its x-th entry and 0
elsewhere. D is the size of the vocabulary. A document is a bag-of-words and is represented
by a vector c ∈ RD with non-negative integer word counts.
the probability vector w ∈ RK.
the distribution speciﬁed by the probability vector µt ∈ RD.

• A topic is ﬁrst chosen according to the distribution on [K] := {1  2  . . .   K} speciﬁed by
• Given that the chosen topic is t  the words in the document are drawn independently from

Following previous work (e.g.  [10]) we assume that µ1  µ2  . . .   µK are linearly independent prob-
ability vectors in RD. Let the foreground corpus of documents be generated by the mixture
of |A| + |B| topics {(µt  wf
t)}t∈B  and the background topics be generated by the
mixture of |B| + |C| topics {(µt  wb
t )}t∈C (here  we assume (A  B  C) is a non-
trivial partition of [K]  and that wf
t  wb

t )}t∈B ∪{ (µt  wb
t > 0 for all t). Our goal is to learn {(µt  wf

t)}t∈A ∪{ (µt  wf

t)}t∈A.

3.1 Moment decompositions
We use the symbol ⊗ to denote the tensor product of vectors  so a⊗b is the matrix whose (i  j)-th en-
try is aibj  and a⊗b⊗c is the third-order tensor whose (i  j  k)-th entry is aibjck. Given a third-order
tensor T ∈ Rd1×d2×d3 and vectors a ∈ Rd1  b ∈ Rd2  and c ∈ Rd3  we let T (I  b  c) ∈ Rd1 denote
the vector whose i-th entry is￿j k Ti j kbjck  and T (a  b  c) denote the scalar￿i j k Ti j kaibjck.
We review the moments of the word observations in this model (see  e.g.  [10]). Let x1  x2  x3 ∈ [D]
be three random words sampled from a random document generated by the foreground model
(the discussion here also applies to the background model). The second-order (cross) moment
matrix M f
2 := E[ex1 ⊗ ex2] is the matrix whose (i  j)-th entry is the probability that x1 = i
and x2 = j. Similarly  the third-order (cross) moment tensor M f
3 := E[ex1 ⊗ ex2 ⊗ ex3] is the

3

Algorithm 1 Contrastive Topic Model estimator
input Foreground and background documents {cf
output Foreground-speciﬁc topics Topicsf.
1: Let ˆM f

2 and ˆM b
n} ({cb
2: Run Algorithm 2 with input ˆM2  ˆM3  K  and N to obtain {(ˆat  ˆλt) : t ∈ [K]}.
3: Topicsf := {(ˆat/￿ˆat￿1  1/ˆλ2

n}; parameter γ> 0; number of topics K.
3 ) be the foreground (background) second- and third-order moment
n})  and let ˆM2 := ˆM f
t ) : t ∈ [K]  ˆλt > 0}.

3 ( ˆM b
estimates based on {cf

2 and ˆM3 := ˆM f

3 − γ ˆM b
3 .

2 − γ ˆM b

n}  {cb

2 and ˆM f

t implies that the second- and third-order moments are

third-order tensor whose (i  j  k)-th entry is the probability that x1 = i  x2 = j  x3 = k. Ob-
serve that for any t ∈ A ∪ B 
the i-th entry of E[ex1|topic = t] is precisely the probability
that x1 = i given topic = t  which is i-th entry of µt. Therefore  E[ex1|topic = t] = µt. Since
the words are independent given the topic  the (i  j)-th entry of E[ex1 ⊗ ex2|topic = t] is the
Similarly 
product of the i-th and j-th entry of µt 
E[ex1 ⊗ ex2 ⊗ ex3|topic = t] = µt ⊗ µt ⊗ µt. Averaging over the choices of t ∈ A ∪ B with the
weights wf
2 = E[ex1 ⊗ ex2] = ￿t∈A∪B

i.e.  E[ex1 ⊗ ex2|topic = t] = µt ⊗ µt.
3 = E[ex1 ⊗ ex2 ⊗ ex3] = ￿t∈A∪B
wf
t µt ⊗ µt ⊗ µt.
(We discuss how to efﬁciently use documents of length > 3 in Section 5.2.) We can similarly
decompose the background moments M b
3 in terms of tensors products of {µt}t∈B∪C. These
equations imply the following proposition (proved in Appendix A).
Proposition 1. Let M f
3 be the second- and third-order moments from the fore-
ground and background data  respectively. Deﬁne

wf
t µt ⊗ µt

3 and M b

2 and M b

and M f

2   M b

2  M f

M f

M2 := M f

2 − γM b

2

and M3 := M f

3 − γM b
3 .

If γ ≥ maxj∈B wf

j/wb

j   then

M2 =

K￿t=1

ωt µt ⊗ µt

and M3 =

ωt µt ⊗ µt ⊗ µt

(2)

K￿t=1

where ωt = wf

t > 0 for t ∈ A (foreground-speciﬁc topic)  and ωt ≤ 0 for t ∈ B ∪ C.

2 and M b

Using tensor decompositions. Proposition 1 implies that the modiﬁed moments M2 and M3 have
low-rank decompositions in which the components t with positive multipliers ωt correspond to the
t)}t∈A. A main technical innovation of this paper is a general-
foreground-speciﬁc topics {(µt  wf
ized tensor power method  described in Section 5  which takes as input (estimates of) second- and
third-order tensors of the form in (2)  and approximately recovers the individual components. We
argue that under some natural conditions  the generalized power method is robust to large pertur-
bations in M b
3   which suggests that foreground-speciﬁc topics can be learned even when it
is not possible to accurately model the background. We use the generalized tensor power method
to estimate the foreground-speciﬁc topics in our Contrastive Topic Model estimator (Algorithm 1).
j gives good
Proposition 1 gives the lower bound on γ; we empirically ﬁnd that γ ≈ maxj∈B wf
results. When γ is too large  the convergence of the tensor power worsens. Where possible in prac-
tice  we recommend using prior belief about foreground and background compositions to estimate
maxj∈B wf
3.2 Experiments with contrastive topic modeling
We test our contrastive topic models on the RCV1 dataset  which consists of ≈ 800000 news ar-
ticles. Each document comes with multiple category labels (e.g.  economics  entertainment) and
region labels (e.g.  USA  Europe  China). The corpus spans a large set of complex and overlapping
categories  making this a good dataset to validate our contrastive learning algorithm.
In one set of experiments  we take documents associated with one region as the foreground corpus 
and documents associated with a general theme  such as economics  as the background. The goal
of the contrast is to ﬁnd the region-speciﬁc topics which are not relevant to the background theme.
The top half of Table 1 shows the example where we take USA-related documents as the foreground

j   and then vary γ as part of the exploratory analysis.

j/wb

j/wb

4

USA foreground

million

percent lbs
usda
week
rate
hog
market gilt
wheat

bond
municipal week
sale
index
year
export
total
barrow trade
China foreground

stock
price
close
trade
index

USA foreground  Economics background

play
round
golf
open
hole

research result
science hockey
nation
cancer
cell
cap
ny
study

basketball game
game
run
nation
hit
la
win
association inn
China foreground  Economics background

share
market

billion
reserve

china
ton
percent percent bank
import million balance
alumin trade

trade

shanghai yuan
yuan
year
bank
ﬁrm
china
foreign storm xinhua
exchange invest

panda
china
china
east
typhoon year

ﬂood

zoo

earthquake china
china
ofﬁce
court
ofﬁce
smuggle
richt
scale
ship

interest
bond
million
cost
moody

Table 1: Top words from representative topics: foreground alone (left); foreground/background contrast (right).
Each column corresponds to one topic.

 

e
r
o
c
s
n
o
i
t
a
c

i
f
i

s
s
a

l

c

1.6

1.4

1.2

1

0.8

0.6

 

 

 N=10000
 N=1000
 N=100
 N=50

1

2

0

0.5

γ
(a)

!"#$%#"&'(

2

-

.

/

0

1

3
)4)!
!"!# !"$$ !"!% !"!& !"!! !"!! !"!!
5.3+6
!"(# !"(# !"$$ !"(' !")# !"!# !"((
5.37$/ ("!! !"!% !"!! !"!# !"!& !"!! !"!!
5/27$/ !"!' !"!# !"!& !"!# !"!$ !"#+ !"(&
507$-
!"') !"+' !"+% !")* !"$$ !"(! !"'*
507$.
!")! !"++ ("!! !")$ !"%+ !"!& !"(+
507$/
!"!# !"') !"*! !"&# !"!$ !"!+ !"!*
58+6
!"!! !"($ !"$+ !"() !"+% !"!& !"!*
5.97$- !"(! !"!' !"!& !"!# !"!+ !"!* !"(%
:);
!"!& !"!' !"!! !"!! !"!! !"!( !"!(
<$=%>*
!"!( !"!# !"(! !"(+ !"(& !"'% !"+'
(b)

)"'*#+ *

0

/

.

-

1

2

3
!"!' ("!! !"!# !"!& !"!! !"!! !"!!
!"!% !"() !"$* !"(+ !")& !"!% !"'(
!"$! !"!+ !"!! !"!) !"!( !"!! !"!*
!"!! !"!% !"!+ !"!# !"!* !"#+ !"+(
!"'& !"+( !"+% !")$ !"*! !"!) !"&&
!"&# !"+( ("!! !")* !"%+ !"!+ !"'%
!"(( !"') !"*! !"&) !"!* !"!( !"(#
!"!! !"($ !"$+ !"($ !"+& !"!+ !"(*
!"'+ !"!+ !"!+ !"!& !"!+ !"!% !"%&
!"!! !"!! !"!! !"!! !"!( !"!! !"!(
!"!+ !"!) !"(+ !"() !"($ !"+# !"!#

Figure 2: (a) Relative AUC as function of γ (Sec. 3.2). (b) Emission probabilities of HMM states (Sec. 4).

and Economics as the background theme. We ﬁrst set the contrast parameter γ = 0 in Algorithm 1;
this learns the topics from the foreground dataset alone. Due to the composition of the corpus  the
foreground topics for USA is dominated by topics relevant to stock markets and trade; representative
topics and keywords are shown on the left of Table 1. Then we increase γ to observe the effects
of contrast. In the right half of Table 1  we show the heavily weighted topics and keywords for
when γ = 2. The topics involving market and trade are also present in the background corpus  so
their weights are reduced through contrast. Topics which are very USA-speciﬁc and distinct from
economics rise to the top: basketball  baseball  scientiﬁc research  etc. A similar experiment with
China-related articles as foreground  and the same economics themed background is shown in the
bottom of Table 1.
These examples illustrate that Algorithm 1 learns topics which are unique to the foreground. To
quantify this effect  we devised a speciﬁcity test. Using the RCV1 labels  we partition the foreground
USA documents into two disjoint groups: documents with any economics-related labels (group 0)
and the rest (group 1). Because Algorithm 1 learns the full probabilistic model  we use the inferred
topic parameters to compute the marginal likelihood for each foreground document given the model.
We then use the likelihood value to classify each foreground document as belonging to group 0 or 1.
The performance of the classiﬁer is summarized by the AUC score.
We ﬁrst set γ = 0 and compute the AUC score  which corresponds to how well a topic model
learned from only the foreground can distinguish between the two groups. We use this score as
the baseline and normalize so it is equal to 1. The hope is that by using the background data  the
contrastive model can better identify the documents that are generated by foreground-speciﬁc topics.
Indeed  as γ increases  the AUC score improves signiﬁcantly over the benchmark (dark blue bars in
Figure 2(a)). For γ> 2 we ﬁnd that the foreground speciﬁc topics do not change qualitatively.
A major advantage of our approach is that we do not need to learn a very accurate background
model to learn the contrast. To validate this  we down sample the background corpus to 1000  100 

5

and 50 documents. This simulates settings where the background is very sparsely sampled  so it
is not possible to learn a background model very accurately. Qualitatively  we observe that even
with only 50 randomly sampled background documents  Algorithm 1 still recovers topics speciﬁc to
USA and not related to Economics. At γ = 2  it learns sports and NASA/space as the most promi-
nent foreground-speciﬁc topics. This is supported by the speciﬁcity test  where contrastive topic
models with sparse background better identify foreground-speciﬁc documents relative to the γ = 0
(foreground data-only) model.

4 Contrastive Hidden Markov Models

Hidden Markov Models (HMMs) are commonly used to model sequence and time series data. For
example  a biologist may collect several sequences from an experiment; some of the sequences are
generated by a biological process of interest (modeled by an HMM)  while others are generated by
a different “background” process—e.g.  noise or a process that is not of primary interest.
Consider a simple generative process where foreground data are generated by a mixture of two
HMMs: (1 − γ) HMMA +γ HMMB  and background data are generated by HMMB. The goal
is to learn the parameters of HMMA  which models the biological process of interest. As we did
for topic models  we can estimate a contrastive HMM by taking appropriate combinations of ob-
3  . . . be a random emission sequence in RD generated by the
servable moments. Let xf
foreground model (1− γ) HMMA +γ HMMB  and xb
3  . . . be the sequence generated by the
background model HMMB. Following [5]  we estimate the following cross moment matrices and
tensors: M f
3] 
3]  M f
2 ⊗ xf
as well as the corresponding moments for the background model. This is similar to the estimation
the word pair and triple frequencies in LDA. Here we only use the ﬁrst three observations in the
sequence  but it is also justiﬁable to average over all consecutive observation triplets [14]. Then 
analogous to Proposition 1  we deﬁne the contrastive moments as Mu v := M f
u v (for
1 2 3. In the Appendix (Sec. D and Algorithm 3)  we
{u  v}⊂{ 1  2  3}) and M1 2 3 := M f
describe how to recover the foreground-speciﬁc model HMMA. The key technical difference from
contrastive LDA lies in the asymmetric generalization of the Tensor Power Method of Algorithm 2.

1 2 3− γM b

u v − γM b

1 2 3 := E[xf

2]  M f

1 3 := E[xf

2 3 := E[xf

2 ⊗ xf

3]  M f

1  xb

2  xb

1  xf

2  xf

1 2 := E[xf

1 ⊗ xf

1 ⊗ xf

1 ⊗ xf

Application to contrastive genomics. For many biological problems  it is important to understand
how signals in certain data are enriched relative to some related background data. For instance  we
may want to contrast foreground data composed of gene expressions (or mutation rates  protein
levels  etc) from one population against background data taken from (say) a control experiment  a
different cell type  or a different time point. The contrastive analysis methods developed here can be
a powerful exploratory tool for biology.
As a concrete illustration  we use spectral contrast to reﬁne the characterization of chromatin states.
The human genome consists of ≈ 3 billion DNA bases  and has recently been shown that these bases
can be naturally segmented into a handful of chromatin states [15  16]. Each state describes a set of
genomic properties: several states describe different active and regulatory features  while other states
describe repressive features. The chromatin state varies across the genome  remaining constant for
relatively short regions (say  several thousand bases). Learning the nature of the chromatin states
is of great interest in genomics. The state-of-the-art approach for modeling chromatin states uses
an HMM [16]. The observable data are  at every 200 bases  a binary feature vector in {0  1}10.
Each feature indicates the presence/absence of a speciﬁc chemical feature at that site (assumed
independent given the chromatin state). This correspond to ≈ 15 million observations across the
genome  which are used to learn the parameters of an HMM. Each chromatin state corresponds to a
latent state  characterized by a vector of 10 emission probabilities.
We take as foreground data the observations from exons  introns and promoters  which account for
about 30% of the genome; as background data  we take observations from intergenic regions. Be-
cause exons and introns are transcribed  we expect the foreground to be a mixture of functional
chromatin states and spurious states due to noise  and expect more of the background observations
to be due to non-functional process. The contrastive HMM should capture biologically meaningful
signals in the foreground data. In Figure 2(b)  we show the emission matrix for the foreground HMM
and for the contrastive HMM. We learn K = 7 latent states  corresponding to 7 chromatin states.

6

Algorithm 2 Generalized Tensor Power Method
input ˆM2 ∈ RD×D; ˆM3 ∈ RD×D×D; target rank K; number of iterations N.
output Estimates {(ˆat  ˆλt) : t ∈ [K]}.
1: Let ˆM†2 := Moore-Penrose pseudoinverse of rank K approximation to ˆM2; initialize T := ˆM3.
2: for t = 1 to K do
3:
4:
5:
6: end for

Randomly draw u(0) ∈ RD from any distribution with full support in the range of ˆM2.
Repeat power iteration update N times: u(i+1) := T (I  ˆM†2 u(i)  ˆM†2 u(i)).
ˆat := u(N )/|￿u(N )  ˆM†2 u(N )￿|1/2; ˆλt := T ( ˆM†2 ˆat  ˆM†2 ˆat  ˆM†2 ˆat); T := T −| ˆλt|ˆat ⊗ ˆat ⊗ ˆat.

Each row is a chemical feature of the genome. The foreground states recover the known biologi-
cal chromatin states from literature [16]. For example  state 6  with high emission for K36me3  is
transcribed genes; state 5 is active enhancers; state 4 is poised enhancers. In the contrastive HMM 
most of the states are the same as before. Interestingly  state 7  which is associated with feature
K20me1  drops from the largest component of the foreground to a very small component of the con-
trast. This ﬁnding suggests that state 7 and K20me1 are less speciﬁc to gene bodies than previously
thought [17]  and raises more questions regarding its function  which is relatively unknown.

5 Generalized tensor power method

and

i=1 σiai ⊗ ai and M3 :=￿K

We now describe our general approach for tensor decomposition used in Algorithm 1. Let
Let
a1  a2  . . .   aK ∈ RD be linearly independent vectors  and set A := [a1|a2|···|aK].
M2 :=￿K
i=1 λiai ⊗ ai ⊗ ai  where σi = sign(λi) ∈ {±1}. The goal
is to recover {(at λ t) : t ∈ [K]} from (estimates of) M2 and M3.
The following proposition shows that one of the vectors ai (and its associated λi) can be obtained
from M2 and M3 using a simple power method similar to that from [5  18] (note that which of the
K components is obtained depends on the initialization of the procedure). Note that the error ε is
exponentially small in 2t after t iterations  so the number of iterations required to converge is very
small. Below  we use (·)† to denote the Moore-Penrose pseudoinverse.
Proposition 2 (Informal statement). Consider the sequence u(0)  u(1)  . . . in RD determined by
u(i+1) := M3(I  M†2 u(i)  M†2 u(i)) . Then for any ε ∈ (0  1) and almost all u(0) ∈ range(A) 
there exists t∗ ∈ [K]  c1  c2 > 0 (all depending on u(0) and {(µt λ t) : t ∈ [K]}) such that
|˜λ −| λt∗|| ≤ |λt∗|ε + maxt￿=t∗ |λt|ε3/2 for ε := c1 exp(−c22i)  where
￿˜u(i) − at∗￿2 ≤ ε
˜u(i) := σt∗u(i)/￿A†u(i)￿  and ˜λ := M3(M†2 ˜u(i)  M†2 ˜u(i)  M†2 ˜u(i)).
See Appendix B for the formal statement and proof which give explicit dependencies. We use the
iterations from Proposition 2 in our main decomposition algorithm (Algorithm 2)  which is a variant
of the main algorithm from [5]. The main difference is that we do not require M2 to be positive semi-
deﬁnite  which is essential for our application  but requires subtle modiﬁcations. For simplicity  we
assume we run Algorithm 2 with exact moments M2 and M3 — a detailed perturbation analysis
would be similar to that in [5] but is beyond the scope of this paper. Proposition 2 shows that a single
component can be accurately recovered  and we use deﬂation to recover subsequent components
(normalization and deﬂation is further discussed in Appendix B). As noted in [5]  errors introduced
in this deﬂation step have only a lower-order effect  and therefore it can be used reliably to recover
all K components. For increased robustness  we actually repeat steps 3–5 in Algorithm 2 several
times  and use the results of the trial in which |ˆλt| takes the median value.
5.1 Robustness to sparse background sampling
Algorithm 1 can recover the foreground-speciﬁc {µt}t∈A even with relatively small numbers
of background data. We can illustrate this robustness under the assumption that the support
of the foreground-speciﬁc topics S0 := ∪t∈A supp(µt) is disjoint from that of the other topics
S1 := ∪t∈B∪C supp(µt) (similar to Brown clusters [19]). Suppose that M f
2 is estimated accurately
using a large sample of foreground documents. Then because S0 and S1 are disjoint  Algorithm 1

7

(using sufﬁciently large γ) will accurately recover the topics {(µt  wf
t) : t ∈ A} in Topicsf. The
remaining concern is that sampling errors will cause Algorithm 1 to mistakenly return additional
topics in Topicsf  namely the topics t ∈ B ∪ C. It thus sufﬁces to guarantee that the signs of the ˆλt
returned by Algorithm 2 are correct. The sample size requirement for this is independent of the de-
sired accuracy level for the foreground-speciﬁc topics—it depends only on γ and ﬁxed properties of
the background model.1 As reported in Section 3.2  this robustness is borne out in our experiments.

5.2 Scalability

Our algorithms are scalable to large datasets when implemented to exploit sparsity and low-rank
structure (each experiment we report runs on a standard laptop in a few minutes). Two important
details are (i) how the moments M2 and M3 are represented  and (ii) how to execute the power
iteration update in Algorithm 2. These issues are only brieﬂy mentioned in [5] and without proof 
so in this section  we address these issues in detail.

2 and M f

2 is ˆM f

2]i j.

2]i i and E[cn(i)cn(j)/(￿n(￿n − 1))] = [M f
2 := N−1￿N

Efﬁcient moment estimates for topic models. We ﬁrst discuss how to represent empirical esti-
mates of the second- and third-order moments M f
3 for the foreground documents (the same
will hold for the background documents). Let document n ∈ [N ] have length ￿n  and let cn ∈ ND
be its word count vector (its i-th entry cn(i) is the number of times word i appears in document n).
Proposition 3 (Estimator for M f
2). Assume ￿n ≥ 2. For any distinct i  j ∈ [D]  E[(cn(i)2 −
cn(i))/(￿n(￿n − 1))] = [M f
By Proposition 3  an unbiased estimator of M f
n=1(￿n(￿n − 1))−1(cn ⊗ cn −
diag(cn)). Since ˆM f
2 is sum of sparse matrices  it can be represented efﬁciently  and we may use
sparsity-aware methods for computing its low-rank spectral decompositions. It is similarly easy to
obtain such a decomposition for ˆM f
2   from which one can compute its pseudoinverse and
represent it in factored form as P Q￿ for some P  Q ∈ RD×K.
3). Assume ￿n ≥ 3. For any distinct i  j  k ∈ [D]  E[(cn(i)3 −
Proposition 4 (Estimator for M f
3]i i i  E[(cn(i)2cn(j) − cn(i)cn(j))/(￿n(￿n −
3cn(i)2 + 2cn(i))/(￿n(￿n − 1)(￿n − 2))] = [M f
1)(￿n − 2))] = [M f
By Proposition 4  an unbiased estimator of M f
3(I  v  v) :=
N−1￿N
n=1(￿n(￿n−1)(￿n−2))−1(￿cn  v￿2cn−2￿cn  v￿(cn◦v)−￿cn  v◦v￿cn +2cn◦v◦v) (where
◦ denotes component-wise product of vectors). Let nnz(cn) be the number of non-zero entries in
cn  then each term in the sum takes only O(nnz(cn)) operations to compute. So the time to compute
ˆM f
3(I  v  v) is proportional to the number of non-zero entries of the term-document matrix  using
just a single pass over the document corpus.

3]i j k.
3(I  v  v) for any vector v ∈ RD is ˆM f

3]i i j  and E[(cn(i)cn(j)cn(k))/(￿n(￿n − 1)(￿n − 2))] = [M f

2 − γ ˆM b

Power iteration computation. Each power iteration update in Algorithm 2 just requires the eval-
3 (I  v  v) (one-pass linear time  as shown above) for v := ˆM†2 u(i)  and
uating ˆM f
ˆλτ￿ˆaτ   v￿2ˆaτ (O(DK) time). Since ˆM†2 is kept in rank-K factored

3(I  v  v) − γ ˆM b
computing the deﬂation￿τ<t

form  v can also be computed in O(DK) time.

6 Discussion

In this paper  we formalize a model of contrastive learning and introduce efﬁcient spectral methods to
learn the model parameters speciﬁc to the foreground. Experiments with contrastive topic modeling
show that Algorithm 1 can learn foreground-speciﬁc topics even when the background data is noisy.
Our application in contrastive genomics illustrates the utility of this method in exploratory analysis
of biological data. The contrast identiﬁes an intriguing change associated with K20me1  which
can be followed up with biological experiments. While we have focused in this work on a natural
contrast model for mixture models  we also discuss an alternative approach in Appendix E.
Acknowledgement This work was partially supported by DARPA Young Faculty Award DARPA
N66001-12-1-4219.

1For instance  if the background model consists only of one topic µ  then the analyses from [5  10] can be

adapted to bound the sample size requirement by O(1/￿µ￿6).

8

References

[1] David M. Blei  Andrew Ng  and Michael Jordan. Latent Dirichlet allocation. JMLR  3:993–

1022  2003.

[2] Leonard E. Baum and J. A. Eagon. An inequality with applications to statistical estimation
for probabilistic functions of Markov processes and to a model for ecology. Bull. Amer. Math.
Soc.  73(3):360–363  1967.

[3] J. Zou and R. Adams. Priors for diversity in generative latent variable models. In Advances in

Neural Information Processing Systems 25  2012.

[4] B. Scholkopf  R. Williamson  A. Smola  J. Shawe-Taylor  and J. Platt. Support vector method

for novelty detection. In Advances in Neural Information Processing Systems 25  2000.

[5] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and T. Telgarsky. Tensor decompositions for

learning latent variable models  2012. arXiv:1210.7559.

[6] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden Markov models.

Journal of Computer and System Sciences  78(5):1460–1480  2012.

[7] S. Siddiqi  B. Boots  and G. Gordon. Reduced rank hidden markov models. In Proceedings of

the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics  2010.

[8] B. Balle  A. Quattoni  and X. Carreras. Local loss optimization in operator models: A new
insight into spectral learning. In Twenty-Ninth International Conference on Machine Learning 
2012.

[9] S. B. Cohen  K. Stratos  M. Collins  D. P. Foster  and L. Ungar. Spectral learning of latent

variable PCFGs. In Proceedings of Association of Computational Linguistics  2012.

[10] A. Anandkumar  D. P. Foster  D. Hsu  S. M. Kakade  and Y. K. Liu. A spectral algorithm for

latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25  2012.

[11] D. Hsu  S. M. Kakade  and P. Liang.

Identiﬁability and unmixing of latent parse trees.

Advances in Neural Information Processing Systems 25  2012.

In

[12] S. B. Cohen  K. Stratos  M. Collins  D. P. Foster  and L. Ungar. Experiments with spectral
learning of latent-variable PCFGs. In Proceedings of Conference of the North American Chap-
ter of the Association for Computational Linguistics  2013.

[13] A. T. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. In

Thirtieth International Conference on Machine Learning  2013.

[14] A. Kontorovich  B. Nadler  and R. Weiss. On learning parametric-output HMMs. In Thirtieth

International Conference on Machine Learning  2013.

[15] J. Zhu et al. Genome-wide chromatin state transitions associated with developmental and

environmental cues. Cell  152(3):642–54  2013.

[16] J. Ernst et al. Mapping and analysis of chromatin state dynamics in nine human cell types.

Nature  473(7345):43–49  2011.

[17] D. Beck et al. Signal analysis for genome wide maps of histone modiﬁcations measured by

chip-seq. Bioinformatics  28(8):1062–9  2012.

[18] L. De Lathauwer  B. De Moor  and J. Vandewalle.

On the best rank-1 and rank-
(R1  R2  ...  Rn) approximation and applications of higher-order tensors. SIAM J. Matrix Anal.
Appl.  21(4):1324–1342  2000.

[19] Peter F. Brown  Peter V. deSouza  Robert L. Mercer  Vincent J. Della Pietra  and Jenifer C. Lai.

Class-based n-gram models of natural language. Comput. Linguist.  18(4):467–479  1992.

9

,James Zou
Daniel Hsu
David Parkes
Ryan Adams