2014,Quantized Estimation of Gaussian Sequence Models in Euclidean Balls,A central result in statistical theory is Pinsker's theorem  which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper  we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular  we place limits on the number of bits used to encode an estimator  and analyze the excess risk in terms of this constraint  the signal size  and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball  which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.,QuantizedEstimationofGaussianSequenceModelsinEuclideanBallsYuanchengZhuJohnLaffertyDepartmentofStatisticsUniversityofChicagoAbstractAcentralresultinstatisticaltheoryisPinsker’stheorem whichcharacterizestheminimaxrateinthenormalmeansmodelofnonparametricestimation.Inthispaper wepresentanextensiontoPinsker’stheoremwhereestimationiscarriedoutunderstorageorcommunicationconstraints.Inparticular weplacelimitsonthenumberofbitsusedtoencodeanestimator andanalyzetheexcessriskintermsofthisconstraint thesignalsize andthenoiselevel.WegivesharpupperandlowerboundsforthecaseofaEuclideanball whichestablishesthePareto-optimalminimaxtradeoffbetweenstorageandriskinthissetting.1IntroductionClassicalstatisticaltheorystudiestherateatwhichtheerrorinanestimationproblemdecreasesasthesamplesizeincreases.Methodologyforaparticularproblemisdevelopedtomakeestimationefﬁcient andlowerboundsestablishhowquicklytheerrorcandecreaseinprinciple.AsymptoticallymatchingupperandlowerboundstogetheryieldtheminimaxrateofconvergenceRn(F)=infbfsupf∈FR(bf f).Thisistheworst-caseerrorinestimatinganelementofamodelclassF whereR(bf f)istheriskorexpectedloss andbfisanestimatorconstructedonadatasampleofsizen.Thecorrespondingsamplecomplexityoftheestimationproblemisn( F)=min{n:Rn(F)<}.Intheclassicalsetting theinﬁmumisoverallestimators.Incontemporarysettings itisincreasinglyofinteresttounderstandhowerrordependsoncomputation.Forinstance whenthedataarehighdimensionalandthesamplesizeislarge constructingtheestimatorusingstandardmethodsmaybecomputationallyprohibitive.Theuseofheuristicsandapproximationalgorithmsmaymakecomputationmoreefﬁcient butitisimportanttounderstandthelossinstatisticalefﬁciencythatthisincurs.Intheminimaxframework thiscanbeformulatedbyplacingcomputationalconstraintsontheestimator:Rn(F Bn)=infbf:C(bf)≤Bnsupf∈FR(bf f).HereC(bf)≤BnindicatesthatthecomputationC(bf)usedtoconstructbfisrequiredtofallwithina“computationalbudget”Bn.Minimaxlowerboundsontheriskasafunctionofthecomputa-tionalbudgetthusdetermineafeasibleregionforcomputation-constrainedestimation andaPareto-optimaltradeoffforerrorversuscomputation.Oneimportantmeasureofcomputationisthenumberofﬂoatingpointoperations ortherunningtimeofanalgorithm.ChandrasekaranandJordan[3]havestudiedupperboundsforstatisticalestimationwithcomputationalconstraintsofthisforminthenormalmeansmodel.However usefullowerboundsareelusive.Thisisduetothedifﬁcultnatureofestablishingtightlowerboundsfor1thismodelofcomputationinthepolynomialhierarchy apartfromanystatisticalconcerns.Anotherimportantmeasureofcomputationisstorage orthespaceusedbyaprocedure.Inparticular wemaywishtolimitthenumberofbitsusedtorepresentourestimatorbf.Thequestionthenbecomes howdoestheexcessriskdependonthebudgetBnimposedonthenumberofbitsC(bf)usedtoencodetheestimator?Thisproblemisnaturallymotivatedbycertainapplications.Forinstance theKeplertelescopecollectsﬂuxdataforapproximately150 000stars[6].Thecentralstatisticaltaskistoestimatethelightcurveofeachstarnonparametrically inordertodenoiseanddetectplanettransits.Ifthisestimationisdoneonboardthetelescope theestimatedfunctionvaluesmayneedtobesentbacktoearthforfurtheranalysis.Tolimitcommunicationcosts theestimatescanbequantized.Thefundamentalquestionis whatislostintermsofstatisticalriskinquantizingtheestimates?Or inacloudcomputingenvironment(suchasAmazonEC2) alargenumberofnonparametricestimatesmightbeconstructedoveraclusterofcomputenodesandthenstored(forexampleinAmazonS3)forlateranalysis.Tolimitthestoragecosts whichcoulddominatethecomputecostsinmanyscenarios itisofinteresttoquantizetheestimates.Howmuchislostintermsofrisk inprinciple byusingdifferentlevelsofquantization?Withsuchapplicationsasmotivation weaddressinthispapertheproblemofrisk-storagetradeoffsinthenormalmeansmodelofnonparametricestimation.Thenormalmeansmodelisacenterpieceofnonparametricestimation.Itarisesnaturallywhenrepresentinganestimatorintermsofanor-thogonalbasis[8 11].OurmainresultisasharpcharacterizationofthePareto-optimaltradeoffcurveforquantizedestimationofanormalmeansvector intheminimaxsense.WeconsiderthecaseofaEuclideanballofunknownradiusinRn.Thiscaseexhibitsmanyofthekeytechnicalchal-lengesthatariseinnonparametricestimationoverricherspaces includingtheSteinphenomenonandtheproblemofadaptivity.Aswillbeapparenttothereader theproblemweconsiderisintimatelyrelatedtoclassicalratedistortiontheory[7].Indeed ourresultsrequireamarriageofminimaxtheoryandratedistortionideas.WethusbuildonthefundamentalconnectionbetweenfunctionestimationandlossysourcecodingthatwaselucidatedinDonoho’s1998WaldLectures[4].Thisconnectioncanalsobeusedtoadvantageforpracticalestimationschemes.Aswediscussfurtherbelow recentadvancesoncomputationallyefﬁcient near-optimallossycompressionusingsparseregressionalgorithms[12]canperhapsbeleveragedforquantizednonparametricestimation.Inthefollowingsection wepresentrelevantbackgroundandgiveadetailedstatementofourresults.InSection3wesketchaproofofourmainresultontheexcessriskfortheEuclideanballcase.Section4presentssimulationstoillustrateourtheoreticalanalyses.Section5discussesrelatedwork andoutlinesfuturedirectionsthatourresultssuggest.2BackgroundandproblemformulationInthissectionwebrieﬂyreviewtheessentialelementsofrate-distortiontheoryandminimaxtheory toestablishnotation.Wethenstateourmainresult whichbridgestheseclassicaltheories.Intherate-distortionsettingwehaveasourcethatproducesasequenceXn=(X1 X2 ... Xn) eachcomponentofwhichisindependentandidenticallydistributedasN(0 σ2).Thegoalistotransmitarealizationfromthissequenceofrandomvariablesusingaﬁxednumberofbits insuchawaythatresultsintheminimalexpecteddistortionwithrespecttotheoriginaldataXn.SupposethatweareallowedtouseatotalbudgetofnBbits sothattheaveragenumberofbitspervariableisB whichisreferredtoastherate.Totransmitorstorethedata theencoderdescribesthesourcesequenceXnbyanindexφn(Xn) whereφn:Rn→{1 2 ... 2nB}≡C(B)istheencodingfunction.ThenB-bitindexisthentransmittedorstoredwithoutloss.Adecoder whenreceivingorretrievingthedata representsXnbyanestimateˇXnbasedontheindexusingadecodingfunctionψn:{1 2 ... 2nB}→Rn.Theimageofthedecodingfunctionψniscalledthecodebook whichisadiscretesetinRnwithcardinalitynolargerthan2nB.TheprocessisillustratedinFigure1 andvariouslyreferredtoas2XnEncoderφnDecoderψnˇXn=ψn(φn(Xn))φn(Xn)∈C(B)θnXnEncoderφnDecoderψnˇθn=ψn(φn(Xn))φn(Xn)∈C(B)Figure1:Encodinganddecodingprocessforlossycompression(top)andquantizedestimation(bottom).Forquantizedestimation themodel(meanvector)θnisdeterministic notrandom.sourcecoding lossycompression orquantization.Wecallthepairofencodinganddecodingfunc-tionsQn=(φn ψn)an(n B)-ratedistortioncode.WewillalsouseQntodenotethecompositionofthetwofunctions i.e. Qn(·)=ψn(φn(·)).Adistortionmeasure oralossfunction d:R×R→R+isusedtoevaluatetheperformanceoftheabovecodingandtransmissionprocess.Inthispaper wewillusethesquaredlossd(Xi ˇXi)=(Xi−ˇXi)2.ThedistortionbetweentwosequencesXnandˇXnisthendeﬁnedbydn(Xn ˇXn)=1nPni=1(Xi−ˇXi)2 theaverageoftheperobservationdistortions.Wedropthesubscriptnindwhenitisclearfromthecontext.Thedistortion orrisk fora(n B)-ratedistortioncodeQnisdeﬁnedastheexpectedlossEd(Xn Qn(Xn)).DenotingbyQn Bthesetofall(n B)-ratedistortioncodes thedistortionratefunctionisdeﬁnedasR(B σ)=liminfn→∞infQn∈Qn BEd(Xn Qn(Xn)).ThisdistortionratefunctiondependsontherateBaswellasthesourcedistribution.Forthei.i.d.N(0 σ2)source accordingtothewell-knownratedistortiontheorem[7] R(B σ)=σ22−2B.WhenBiszero meaningnoinformationgetsencodedatall thisboundbecomesσ2 whichistheexpectedlosswheneachrandomvariableisrepresentedbyitsmean.AsBapproachesinﬁnity thedistortiongoestozero.ThepreviousdiscussionassumesthesourcerandomvariablesareindependentandfollowacommondistributionN(0 σ2).ThegoalistominimizetheexpecteddistortioninthereconstructionofXnaftertransmittingorstoringthedataunderacommunicationconstraint.NowsupposethatXiind.∼N(θi σ2)fori=1 2 ... n.Weassumethevarianceσ2isknownandthemeansθn=(θ1 ... θn)areunknown.Suppose fur-thermore thatinsteadoftryingtominimizetherecoverydistortiond(Xn ˇXn) wewanttoestimatethemeanswithariskassmallaspossible butagainusingabudgetofBbitsperindex.Withoutthecommunicationconstraint thisproblemhasbeenverywellstudied[10 9].Letbθ(Xn)≡bθn=(bθ1 ... bθn)denoteanestimatorofthetruemeanθn.ForaparameterspaceΘn⊂Rn theminimaxriskoverΘnisdeﬁnedasinfbθnsupθn∈ΘnEd(θn bθn)=infbθnsupθn∈ΘnE1nnXi=1(θi−bθi)2.FortheL2ballofradiusc Θn(c)=n(θ1 ... θn):1nnXi=1θ2i≤c2o (1)Pinsker’stheoremgivestheexact limitingformoftheminimaxriskliminfn→∞infbθnsupθn∈Θn(c)Ed(θn bθn)=σ2c2σ2+c2.Toimposeacommunicationconstraint weincorporateavariantofthesourcecodingschemede-scribedaboveintothisminimaxframeworkofestimation.Deﬁnea(n B)-rateestimationcode3246012345Bits per symbol BRisk RFigure2.OurresultestablishesthePareto-optimaltradeoffinthenonparametricnormalmeansprob-lemforriskversusnumberofbits:R(σ2 c2 B)=c2σ2σ2+c2+c42−2Bσ2+c2Curvesforﬁvesignalsizesareshown c2=2 3 4 5 6.Thenoiselevelisσ2=1.Withzerobits therateisc2 thehighestpointontheriskcurve.TherateforlargeBapproachesthePinskerboundσ2c2/(σ2+c2).Mn=(φn ψn) asapairofencodinganddecodingfunctions asbefore.Theencodingfunctionφn:Rn→{1 2 ... 2nB}isamappingfromobservationsXntoanindexset.Thedecodingfunctionisamappingfromindicestomodelsˇθn∈Rn.WewritethecompositionoftheencoderanddecoderasMn(Xn)=ψn(φn(Xn))=ˇθn whichwecallaquantizedestimator.DenotingbyMn Bthesetofall(n B)-rateestimationcodes wethendeﬁnethequantizedminimaxriskasRn(B σ Θn)=infMn∈Mn Bsupθn∈ΘnEd(θn Mn(Xn)).WewillfocusonthecasewhereourparameterspaceistheL2balldeﬁnedin(1) andwriteRn(B σ c)=Rn(B σ Θn(c)).Inthissetting weletngotoinﬁnityanddeﬁnetheasymptoticquantizedminimaxriskasR(B σ c)=liminfn→∞Rn(B σ c)=liminfn→∞infMn∈Mn Bsupθn∈Θn(c)Ed(θn Mn(Xn)).(2)NotethatwecouldestimateθnbasedonthequantizeddataˇXn=Qn(Xn).OnceagaindenotingbyQn Bthesetofall(n B)-ratedistortioncodes suchanestimatoriswrittenˇθn=ˇθn(Qn(Xn)).Clearly ifthedecodingfunctionsψnofQnareinjective thenthisformulationisequivalent.ThequantizedminimaxriskisthenexpressedasRn(B σ Θn)=infˇθninfQn∈Qn Bsupθn∈ΘnEd(θn ˇθn).Themanynormalmeansproblemexhibitsmuchofthecomplexityandsubtletyofgeneralnonpara-metricregressionanddensityestimationproblems.Itarisesnaturallyintheestimationofafunctionexpressedintermsofanorthogonalfunctionbasis[8 13].OurmainresultsharplycharacterizestheexcessriskthatcommunicationconstraintsimposeonminimaxestimationforΘ(c).3MainresultsOurﬁrstresultgivesalowerboundontheexactquantizedasymptoticriskintermsofB σ andc.Theorem1.ForB≥0 σ>0andc>0 theasymptoticminimaxriskdeﬁnedin(2)satisﬁesR(B σ c)≥σ2c2σ2+c2+c4σ2+c22−2B.(3)Thislowerboundonthelimitingminimaxriskcanbeviewedastheusualminimaxriskwithoutquantization plusanexcessrisktermduetoquantization.IfwetakeBtobezero theriskbecomesc2 whichisobtainedbyestimatingallofthemeanssimplybyzero.Ontheotherhand lettingB→∞ werecovertheminimaxriskinPinsker’stheorem.ThistradeoffisillustratedinFigure2.Theproofofthetheoremistechnicalandwedeferittothesupplementarymaterial.Herewesketchthebasicideaoftheproof.Supposeweareabletoﬁndapriordistributionπnonθnandarandom4vectoreθnsuchthatforany(n B)-rateestimationcodeMnthefollowingholds:σ2c2σ2+c2+c4σ2+c22−2B(I)=ZEXnd(θn eθn)dπn(θn)(II)≤ZEXnd(θn Mn(Xn))dπn(θn)(III)≤supθn∈Θn(c)EXnd(θn Mn(Xn)).ThentakinganinﬁmumoverMn∈Mn Bgivesusthedesiredresult.Infact wecantakeπn theprioronθn tobeN(0 c2In) andthemodelbecomesθi∼N(0 c2)andXi|θi∼N(θi σ2).ThenaccordingtoLemma1 inequality(II)holdswitheθnbeingtheminimizertotheoptimizationproblemminp(eθn|Xn θn)Ed(θn eθn)subjecttoI(Xn;eθn)≤nB p(eθn|Xn θn)=p(eθn|Xn).Theequality(I)holdsduetoLemma2.Theinequality(III)canbeshownbyalimitingconcentrationargumentonthepriordistribution whichisincludedinthesupplementarymaterial.Lemma1.SupposethatX1 ... Xnareindependentandgeneratedbyθi∼π(θi)andXi|θi∼p(xi|θi).SupposeMnisan(n B)-rateestimationcodewithriskEd(θn Mn(Xn))≤D.ThentherateBislowerboundedbythesolutiontothefollowingproblem:minp(eθn|Xn θn)I(Xn;eθn)subjecttoEd(θn eθn)≤D (4)p(eθn|Xn θn)=p(eθn|Xn).Thenextlemmagivesthesolutiontoproblem(4)whenwehaveθi∼N(0 c2)andXi|θi∼N(θi σ2)Lemma2.Supposeθi∼N(0 c2)andXi|θi∼N(θi σ2)fori=1 ... n.ForanyrandomvectoreθnsatisfyingEd(θn eθn)≤Dandp(eθn|Xn θn)=p(eθn|Xn)wehaveI(Xn;eθn)≥n2logc4(σ2+c2)(D−σ2c2σ2+c2).Combiningtheabovetwolemmas weobtainalowerboundoftheriskassumingthatθnfollowsthepriordistributionπn:Corollary1.SupposeMnisa(n B)-rateestimationcodeforthesourceθi∼N(0 c2)andXi|θi∼N(θi σ2) thenEd(θn Mn(Xn))≥σ2c2σ2+c2+c4σ2+c22−2B.(5)3.1AnadaptivesourcecodingmethodWenowpresentasourcecodingmethod whichwewillshowattainstheminimaxlowerboundasymptoticallywithhighprobability.Supposethattheencoderisgivenasequenceofobservations(X1 ... Xn) andboththeencoderandthedecoderknowtheradiuscoftheL2ballinwhichthemeanvectorlies.Thestepsofthesourcecodingmethodareoutlinedbelow:Step1.Generatingcodebooks.Thecodebooksaredistributedtoboththeencoderandthedecoder.5(a)GeneratecodebookB={1/√n 2/√n ... dc2√ne/√n}.(b)GeneratecodebookXwhichconsistsof2nBi.i.d.randomvectorsfromtheuniformdistributiononthen-dimensionalunitsphereSn−1.Step2.Encoding.(a)Encodebb2=1nkXk2−σ2byˇb2=argmin{|b2−bb2|:b2∈B}.(b)EncodeXnbyˇXn=argmax{hXn xni:xn∈X}.Step3.Transmitorstore(ˇb2 ˇXn)bytheircorrespondingindicesusinglogc2+12logn+nBbits.Step4.Decoding.(a)Recover(ˇb2 ˇXn)bythetransmittedorstoredindices.(b)Estimateθbyˇθn=snˇb4(1−2−2B)ˇb2+σ2·ˇXn.Wemakeseveralremarksonthisquantizedestimationmethod.Remark1.TherateofthiscodingmethodisB+logc2n+logn2n whichisasymptoticallyBbits.Remark2.Themethodisprobabilistic;therandomnesscomesfromtheconstructionofthecode-bookX.DenotingbyM∗n B σ ctheensembleofsuchrandomquantizers thereisthenanaturalone-to-onemappingbetweenM∗n B σ cand(Sn−1)2nBandweattachprobabilitymeasuretoM∗n B σ ccorrespondingtotheproductuniformdistributionon(Sn−1)2nB.Remark3.Themainideabehindthiscodingschemeistoencodethemagnitudeandthedirectionoftheobservationvectorseparately insuchawaythattheprocedureadaptstosourceswithdifferentnormsofthemeanvectors.Remark4.Thecomputationalcomplexityofthissourcecodingmethodisexponentialinn.There-fore liketheShannonrandomcodebook thisisademonstrationoftheasymptoticachievabilityofthelowerbound(3) ratherthanapracticalschemetobeimplemented.WediscusspossiblecomputationallyefﬁcientalgorithmsinSection5.Thefollowingshowsthatwithhighprobabilitythisprocedurewillattainthedesiredlowerboundasymptotically.Theorem2.Forasequenceofvectors{θn}∞n=1satisfyingθn∈Rnandkθnk2/n=b2≤c2 asn→∞P d(θn Mn(Xn))>σ2b2σ2+b2+b4σ2+b22−2B+Crlognn!−→0(6)forsomeconstantCthatdoesnotdependonn(butcouldpossiblydependonb σandB).TheprobabilitymeasureiswithrespecttobothMn∈M∗n B σ candXn∈Rn.ThistheoremshowsthatthesourcecodingmethodnotonlyachievesthedesiredminimaxlowerboundfortheL2ballwithhighprobabilitywithrespecttotherandomcodebookandsourcedistri-bution butalsoadaptstothetruemagnitudeofthemeanvectorθn.ItagreeswiththeintuitionthatthehardestmeanvectortoestimateliesontheboundaryoftheL2ball.BasedonTheorem2wecanobtainauniformhighprobabilityboundformeanvectorsintheL2ball.Corollary2.Foranysequenceofvectors{θn}∞n=1satisfyingθn∈Rnandkθnk2/n≤c2 asn→∞P d(θn Mn(Xn))>σ2c2σ2+c2+c4σ2+c22−2B+C0rlognn!−→0forsomeconstantC0thatdoesnotdependonn.WeincludethedetailsoftheproofofTheorem2inthesupplementarymaterial whichcarefullyanalyzesthethreetermsinthefollowingdecompositionofthelossfunction:6−4−2024IndexEstimateB=0.1B=0.2B=0.5B=1James−SteinFigure3:ComparisonofthequantizedestimateswithdifferentratesB theJames-Steinestimator andthetruemeanvector.Theheightsofthebarsaretheaveragedestimatesbasedon100replicates.Eachlargebackgroundrectangleindicatestheoriginalmeancomponentθj.d(θn ˇθn)=1n(cid:13)(cid:13)ˇθn−θn(cid:13)(cid:13)2=1n(cid:13)(cid:13)ˇθn−bγXn+bγXn−θn(cid:13)(cid:13)2=1n(cid:13)(cid:13)ˇθn−bγXn(cid:13)(cid:13)2|{z}A1+1nkbγXn−θnk2|{z}A2+2nhˇθn−bγXn bγXn−θni|{z}A3wherebγ=bb2bb2+σ2withbb2=kXnk2/n−σ2.TermA1characterizesthequantizationerror.TermA2doesnotinvolvetherandomcodebook andisthelossofatypeofJames-Steinestimator.ThecrosstermA3vanishesasn→∞.4SimulationsInthissectionwepresentasetofsimulationresultsshowingtheempiricalperformanceoftheproposedquantizedestimationmethod.Throughoutthesimulation weﬁxthenoiselevelσ2=1 whilevaryingtheotherparameterscandB.FirstweshowinFigure3theeffectofquantizedestimationandcompareitwiththeJames-Steinestimator.Settingn=15andc=2 werandomlygenerateameanvectorθn∈Rnwithkθk2/n=c2.ArandomvectorXisthendrawnfromN(θn In)andquantizedestimateswithratesB∈{0.1 0.2 0.5 1}arecalculated;forcomparisonwealsocomputetheJames-Steinestimator givenbybθnJS=(cid:16)1−(n−2)σ2kXnk2(cid:17)Xn.Werepeatthissamplingandestimationprocedure100timesandreporttheaveragedriskestimatesinFigure3.Weseethatthequantizedestimatoressentiallyshrinkstherandomvectortowardszero.Withsmallrates theshrinkageisstrong withalltheestimatesclosetozero.EstimateswithlargerratesapproachtheJames-Steinestimator.Inoursecondsetofsimulations wechoosecfrom{0.1 0.5 1 5 10}toreﬂectdifferentsignal-to-noiseratios andchooseBfrom{0.1 0.2 0.5 1}.ForeachcombinationofthevaluesofcandB wevaryn thedimensionofthemeanvector whichisalsothenumberofobservations.Givenasetofparametersc Bandn ameanvectorθnisgenerateduniformlyonthespherekθnk2/n=c2anddataXnaregeneratedfollowingthedistributionN(θn σ2In).Wequantizethedatausingthesourcecodingmethod andcomputethemeansquarederrorbetweentheestimatorandthetruemeanvector.Theprocedureisrepeated100timesforeachoftheparametercombinations andtheaverageandstandarddeviationofthemeansquarederrorsarerecorded.TheresultsareshowninFigure4.Weseethatasnincreases theaverageerrordecreasesandapproachesthetheoreticlowerboundinTheorem1.Moreover thestandarddeviationofthemeansquarederrorsalsodecreases conﬁrmingtheresultofTheorem2thattheconvergenceiswithhighprobability.5DiscussionandfutureworkInthispaper weestablishasharplowerboundontheasymptoticminimaxriskforquantizedesti-matorsofnonparametricnormalmeansforthecaseofaEuclideanball.Similartechniquescanbe7llllllllllllllllllllllllllllllllllllllllllllllllllll11004812nMSEB=0.1llllllllllllllllllllllllllllllllllllllllllllllllllll11004812nB=0.2llllllllllllllllllllllllllllllllllllllllllllllllllll11004812nB=0.5llllllllllllllllllllllllllllllllllllllllllllllllllll11004812nB=1llllc=0.5c=1c=5c=10Figure4:Meansquarederrorsandstandarddeviationsofthequantizedestimatorversusnfordifferentvaluesof(B c).Thehorizontaldashedlinesindicatethelowerbounds.appliedtothesettingwheretheparameterspaceisanellipsoidΘ={θ:P∞j=1a2jθ2j≤c2}.AprincipalcaseofinterestistheSobolevellipsoidofordermwherea2j∼(πj)2masj→∞.TheSobolevellipsoidarisesnaturallyinnonparametricfunctionestimationandisthusofgreatimpor-tance.Weleavethistofuturework.DonohodiscussestheparallelbetweenratedistortiontheoryandPinsker’sworkinhisWaldLec-tures[4].FocusingonthecaseoftheSobolevspaceoforderm whichwedenotebyFm itisshownthattheKolmogoroventropyH(Fm)andtheratedistortionfunctionR(D X)satisfyH(Fm)(cid:16)sup{R(2 X):P(X∈Fm)=1}as→0.Thisconnectstheworst-caseminimaxanalysisandleast-favorableratedistortionfunctionforthefunctionclass.Anotherinformation-theoreticformulationofminimaxratesliesintheso-called“leCamequation”H(F)=n2[14 15].However botharedifferentfromthedirectionwepursueinthispaper whichistoim-posecommunicationconstraintsinminimaxanalysis.Inotherrelatedwork researchersincommunicationstheoryhavestudiedestimationproblemsinsensornetworksundercommunicationconstraints.DraperandWornell[5]obtainaresultontheso-called“one-stepproblem”forthequadratic-Gaussiancase whichisessentiallythesameasthestatementinourCorollary1.Infact theyconsiderasimilarsetting buttreatthemeanvectorasrandomandgeneratedindependentlyfromaknownnormaldistribution.Incontrast weassumeaﬁxedbutunknownmeanvectorandestablishaminimaxlowerboundaswellasanadaptivesourcecodingmethodthatadaptstotheﬁxedmeanvectorwithintheparameterspace.Zhangetal.[16]alsoconsiderminimaxboundswithcommunicationconstraints.However theanalysisin[16]isfocusedondistributedparametricestimation wherethedataaredistributedbetweenseveralmachines.Informationissharedbetweenthemachinesinordertoconstructaparameterestimate andconstraintsareplacedontheamountofcommunicationthatisallowed.Inadditiontotreatingmoregeneralellipsoids animportantdirectionforfutureworkistodesigncomputationallyefﬁcientquantizednonparametricestimators.Onepossiblemethodistodividethevariablesintosmallerblocksandquantizethemseparately.AmoreinterestingandpromisingapproachistoadapttherecentworkofVenkataramananetal.[12]thatusessparseregressionforlossycompression.Weanticipatethatwithappropriatemodiﬁcations thisschemecanbeappliedtoquantizednonparametricestimationtoyieldpracticalalgorithms tradingoffaworseerrorexponentintheconvergenceratetotheoptimalquantizedminimaxriskforreducedcomplexityencodersanddecoders.AcknowledgementsResearchsupportedinpartbyNSFgrantIIS-1116730 AFOSRgrantFA9550-09-1-0373 ONRgrantN000141210762 andanAmazonAWSinEducationMachineLearningResearchgrant.TheauthorsthankAndrewBarron JohnDuchi andAlfredHeroforvaluablecommentsonthiswork.8References[1]T.TonyCai JianqingFan andTiefengJiang.Distributionsofanglesinrandompackingonspheres.TheJournalofMachineLearningResearch 14(1):1837–1864 2013.[2]T.TonyCaiandTiefengJiang.Phasetransitioninlimitingdistributionsofcoherenceofhigh-dimensionalrandommatrices.JournalofMultivariateAnalysis 107:24–39 2012.[3]VenkatChandrasekaranaandMichaelI.Jordan.Computationalandstatisticaltradeoffsviaconvexrelaxation.PNAS 110(13):E1181–E1190 March2013.[4]DavidL.Donoho.WaldlectureI:CountingbitswithKolmogorovandShannon.2000.[5]StarkC.DraperandGregoryW.Wornell.Sideinformationawarecodingstrategiesforsensornetworks.SelectedAreasinCommunications IEEEJournalon 22(6):966–976 2004.[6]JonM.Jenkinsetal.OverviewoftheKeplerscienceprocessingpipeline.TheAstrophysicalJournalLetters 713(2):L87 2010.[7]RobertG.Gallager.InformationTheoryandReliableCommunication.JohnWiley&Sons 1968.[8]IainM.Johnstone.FunctionestimationandGaussiansequencemodels.2002.Unpublishedmanuscript.[9]MichaelNussbaum.Minimaxrisk:Pinskerbound.EncyclopediaofStatisticalSciences 3:451–460 1999.[10]MarkSemenovichPinsker.Optimalﬁlteringofsquare-integrablesignalsinGaussiannoise.ProblemyPeredachiInformatsii 16(2):52–68 1980.[11]AlexandreB.Tsybakov.IntroductiontoNonparametricEstimation.SpringerSeriesinStatis-tics 1stedition 2008.[12]RamjiVenkataramanan TuhinSarkar andSekharTatikonda.Lossycompressionviasparselinearregression:Computationallyefﬁcientencodinganddecoding.InIEEEInternationalSymposiumonInformationTheory(ISIT) pages1182–1186.IEEE 2013.[13]LarryWasserman.AllofNonparametricStatistics.Springer-Verlag 2006.[14]WingHungWongandXiaotongShen.Probabilityinequalitiesforlikelihoodratiosandcon-vergenceratesofsieveMLEs.TheAnnalsofStatistics 23:339–362 1995.[15]YuhongYangandAndrewBarron.Information-theoreticdeterminationofminimaxratesofconvergence.TheAnnalsofStatistics 27(5):1564–1599 1999.[16]YuchenZhang JohnDuchi MichaelJordan andMartinJ.Wainwright.Information-theoreticlowerboundsfordistributedstatisticalestimationwithcommunicationconstraints.InAd-vancesinNeuralInformationProcessingSystems pages2328–2336 2013.9,Yuancheng Zhu
John Lafferty
Srinadh Bhojanapalli
Behnam Neyshabur
Nati Srebro
David Durfee
Ryan Rogers