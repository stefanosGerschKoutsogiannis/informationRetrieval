2016,Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back,In stochastic convex optimization the goal is to minimize a convex function $F(x) \doteq \E_{f\sim D}[f(x)]$ over a convex set $\K \subset \R^d$ where $D$ is some unknown distribution and each $f(\cdot)$ in the support of $D$ is convex over $\K$. The optimization is based on i.i.d.~samples $f^1 f^2 \ldots f^n$ from $D$. A common approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \doteq \frac{1}{n}\sum_{i\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\K$. We demonstrate that in the standard $\ell_p/\ell_q$ setting of Lipschitz-bounded functions over a $\K$ of bounded radius  ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\Omega(\log d)$ dependence proved for $\ell_2/\ell_2$ setting in (Shalev-Shwartz et al.  2009). In stark contrast  these problems can be solved using dimension-independent number of samples for $\ell_2/\ell_2$ setting and $\log d$ dependence for $\ell_1/\ell_\infty$ setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.,Generalization of ERM in Stochastic Convex

Optimization:

The Dimension Strikes Back∗

Vitaly Feldman

IBM Research – Almaden

Abstract

.
= 1
n

(cid:80)

In stochastic convex optimization the goal is to minimize a convex function
= Ef∼D[f (x)] over a convex set K ⊂ Rd where D is some unknown
.
F (x)
distribution and each f (·) in the support of D is convex over K. The optimi-
zation is commonly based on i.i.d. samples f 1  f 2  . . .   f n from D. A standard
approach to such problems is empirical risk minimization (ERM) that optimizes
i≤n f i(x). Here we consider the question of how many samples
FS(x)
are necessary for ERM to succeed and the closely related question of uniform
convergence of FS to F over K. We demonstrate that in the standard (cid:96)p/(cid:96)q setting
of Lipschitz-bounded functions over a K of bounded radius  ERM requires sample
size that scales linearly with the dimension d. This nearly matches standard upper
bounds and improves on Ω(log d) dependence proved for (cid:96)2/(cid:96)2 setting in [18]. In
stark contrast  these problems can be solved using dimension-independent number
of samples for (cid:96)2/(cid:96)2 setting and log d dependence for (cid:96)1/(cid:96)∞ setting using other
approaches.
We further show that our lower bound applies even if the functions in the support
of D are smooth and efﬁciently computable and even if an (cid:96)1 regularization term is
added. Finally  we demonstrate that for a more general class of bounded-range (but
not Lipschitz-bounded) stochastic convex programs an inﬁnite gap appears already
in dimension 2.

1

Introduction

Numerous central problems in machine learning  statistics and operations research are special cases of
stochastic optimization from i.i.d. data samples. In this problem the goal is to optimize the value of the
= Ef∼D[f (x)] over some set K given i.i.d. samples f 1  f 2  . . .   f n
.
expected objective function F (x)
of f. For example  in supervised learning the set K consists of hypothesis functions from Z to Y
and each sample is an example described by a pair (z  y) ∈ (Z  Y ). For some ﬁxed loss function
L : Y × Y → R  an example (z  y) deﬁnes a function from K to R given by f(z y)(h) = L(h(z)  y).
The goal is to ﬁnd a hypothesis h that (approximately) minimizes the expected loss relative to some
distribution P over examples: E(z y)∼P [L(h(z)  y)] = E(z y)∼P [f(z y)(h)].
Here we are interested in stochastic convex optimization (SCO) problems in which K is some convex
subset of Rd and each function in the support of D is convex over K. The importance of this
setting stems from the fact that such problems can be solved efﬁciently via a large variety of known
techniques. Therefore in many applications even if the original optimization problem is not convex  it
is replaced by a convex relaxation.
A classic and widely-used approach to solving stochastic optimization problems is empirical risk
minimization (ERM) also referred to as stochastic average approximation (SAA) in the optimization

∗See [9] for the full version of this work.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

(cid:80)

.
= 1
n

literature. In this approach  given a set of samples S = (f 1  f 2  . . .   f n) the empirical objective
function: FS(x)
i≤n f i(x) is optimized (sometimes with an additional regularization term
such as λ(cid:107)x(cid:107)2 for some λ > 0). The question we address here is the number of samples required
for this approach to work distribution-independently. More speciﬁcally  for some ﬁxed convex
body K and ﬁxed set of convex functions F over K  what is the smallest number of samples
n such that for every probability distribution D supported on F  any algorithm that minimizes
FS given n i.i.d. samples from D will produce an -optimal solution ˆx to the problem (namely 
F (ˆx) ≤ minx∈K F (x) + ) with probability at least 1− δ? We will refer to this number as the sample
complexity of ERM for -optimizing F over K (we will ﬁx δ = 1/2 for now).
The sample complexity of ERM for -optimizing F over K is lower bounded by the sample complexity
of -optimizing F over K  that is the number of samples that is necessary to ﬁnd an -optimal
solution for any algorithm. On the other hand  it is upper bounded by the number of samples that
ensures uniform convergence of FS to F . Namely  if with probability ≥ 1 − δ  for all x ∈ K 
|FS(x) − F (x)| ≤ /2 then  clearly  any algorithm based on ERM will succeed. As a result  ERM
and uniform convergence are the primary tool for analysis of the sample complexity of learning
problems and are the key subject of study in statistical learning theory. Fundamental results in VC
theory imply that in some settings  such as binary classiﬁcation and least-squares regression  uniform
convergence is also a necessary condition for learnability (e.g. [23  17]) and therefore the three
measures of sample complexity mentioned above nearly coincide.
In the context of stochastic convex optimization the study of sample complexity of ERM and
uniform convergence was initiated in a groundbreaking work of Shalev-Shwartz  Shamir  Srebro and
Sridharan [18]. They demonstrated that the relationships between these notions of sample complexity
are substantially more delicate even in the most well-studied settings of SCO. Speciﬁcally  let K
be a unit (cid:96)2 ball and F be the set of all convex sub-differentiable functions with Lipschitz constant
relative to (cid:96)2 bounded by 1 or  equivalently  (cid:107)∇f (x)(cid:107)2 ≤ 1 for all x ∈ K. Then  known algorithm
√
for SCO imply that sample complexity of this problem is O(1/2) and often expressed as 1/
n
rate of convergence (e.g. [14  17]). On the other hand  Shalev-Shwartz et al.[18] show2 that the
sample complexity of ERM for solving this problem with  = 1/2 is Ω(log d). The only known
upper bound for sample complexity of ERM is ˜O(d/2) and relies only on the uniform convergence
of Lipschitz-bounded functions [21  18].
As can seen from this discussion  the work of Shalev-Shwartz et al.[18] still leaves a major gap
between known bounds on sample complexity of ERM (and also uniform convergence) for this basic
Lipschitz-bounded (cid:96)2/(cid:96)2 setup. Another natural question is whether the gap is present in the popular
(cid:96)1/(cid:96)∞ setup. In this setup K is a unit (cid:96)1 ball (or in some cases a simplex) and (cid:107)∇f (x)(cid:107)∞ ≤ 1 for all
x ∈ K. The sample complexity of SCO in this setup is θ(log d/2) (e.g. [14  17]) and therefore  even
an appropriately modiﬁed lower bound in [18]  does not imply any gap. More generally  the choice
of norm can have a major impact on the relationship between these sample complexities and hence
needs to be treated carefully. For example  for (the reversed) (cid:96)∞/(cid:96)1 setting the sample complexity
of the problem is θ(d/2) (e.g. [10]) and nearly coincides with the number of samples sufﬁcient for
uniform convergence.

1.1 Overview of Results

In this work we substantially strengthen the lower bound in [18] proving that a linear dependence on
the dimension d is necessary for ERM (and  consequently  uniform convergence). We then extend
the lower bound to all (cid:96)p/(cid:96)q setups and examine several related questions. Finally  we examine a
more general setting of bounded-range SCO (that is |f (x)| ≤ 1 for all x ∈ K). While the sample
complexity of this setting is still low (for example ˜O(1/2) when K is an (cid:96)2 ball) and efﬁcient
algorithms are known  we show that ERM might require an inﬁnite number of samples already for
d = 2.
Our work implies that in SCO  even optimization algorithms that exactly minimize the empirical
objective function can produce solutions with generalization error that is much larger than the generali-
zation error of solutions obtained via some standard approaches. Another  somewhat counterintuitive 
conclusion from our lower bounds is that  from the point of view of generalization of ERM and
uniform convergence  convexity does not reduce the sample complexity in the worst case.

2The dependence on d is not stated explicitly but follows immediately from their analysis.

2

Basic construction: Our basic construction is fairly simple and its analysis is inspired by the
technique in [18]. It is based on functions of the form max{1/2  maxv∈V (cid:104)v  x(cid:105)}. Note that the
maximum operator preserves both convexity and Lipschitz bound (relative to any norm). See Figure
1 for an illustration of such function for d = 2.

Figure 1: Basic construction for d = 2.

The distribution over the sets V that deﬁne such functions is uniform over all subsets of some set
of vectors W of size 2d/6 such that for any two district u  v ∈ W   (cid:104)u  v(cid:105) ≤ 1/2. Equivalently  each
element of W is included in V with probability 1/2 independently of other elements in W . This
implies that if the number of samples is less than d/6 then  with probability > 1/2  at least one of
the vectors in W (say w) will not be observed in any of the samples. This implies that FS can be
minimized while maximizing (cid:104)w  x(cid:105) (the maximum over the unit (cid:96)2 ball is w). Note that a function
randomly chosen from our distribution includes the term (cid:104)w  x(cid:105) in the maximum operator with
probability 1/2. Therefore the value of the expected function F at w is 3/4 whereas the minimum of
F is 1/2. In particular  there exists an ERM algorithm with generalization error of at least 1/4. The
details of the construction appear in Sec. 3.1 and Thm. 3.3 gives the formal statement of the lower
bound. We also show that  by scaling the construction appropriately  we can obtain the same lower
bound for any (cid:96)p/(cid:96)q setup with 1/p + 1/q = 1 (see Thm. 3.4).

Low complexity construction: The basic construction relies on functions that require 2d/6 bits to
describe and exponential time to compute. Most application of SCO use efﬁciently computable
functions and therefore it is natural to ask whether the lower bound still holds for such functions.
To answer this question we describe a construction based on a set of functions where each function
requires just log d bits to describe (there are at most d/2 functions in the support of the distribution)
and each function can be computed in O(d) time. To achieve this we will use W that consists of
(scaled) codewords of an asymptotically good and efﬁciently computable binary error-correcting
code [12  22]. The functions are deﬁned in a similar way but the additional structure of the code
allows to use at most d/2 subsets of W to deﬁne the functions. Further details of the construction
appear in Section 4.

Smoothness: The use of maximum operator results in functions that are highly non-smooth (that
is  their gradient is not Lipschitz-bounded) whereas the construction in [18] uses smooth functions.
Smoothness plays a crucial role in many algorithms for convex optimization (see [5] for examples).
It reduces the sample complexity of SCO in (cid:96)2/(cid:96)2 setup to O(1/) when the smoothness parameter
is a constant (e.g. [14  17]). Therefore it is natural to ask whether our strong lower bound holds
for smooth functions as well. We describe a modiﬁcation of our construction that proves a similar
lower bound in the smooth case (with generalization error of 1/128). The main idea is to replace
each linear function (cid:104)v  x(cid:105) with some smooth function ν((cid:104)v  x(cid:105)) guaranteing that for different vectors
v1  v2 ∈ W and every x ∈ K  only one of ν((cid:104)v1  x(cid:105)) and ν((cid:104)v2  x(cid:105)) can be non-zero. This allows to
easily control the smoothness of maxv∈V ν((cid:104)v  x(cid:105)). See Figure 2 for an illustration of a function on
which the construction is based (for d = 2). The details of this construction appear in Sec. 3.2 and
the formal statement in Thm. 3.6.

3

Figure 2: Construction using 1-smooth functions for d = 2.

(cid:96)1-regularization: Another important contribution in [18] is the demonstration of the important role
that strong convexity plays for generalization in SCO: Minimization of FS(x) + λR(x) ensures that
ERM will have low generalization error whenever R(x) is strongly convex (for a sufﬁciently large
λ). This result is based on the proof that ERM of a strongly convex Lipschitz function is uniform
replace-one stable and the connection between such stability and generalization showed in [4] (see
also [19] for a detailed treatment of the relationship between generalization and stability). It is
natural to ask whether other approaches to regularization will ensure generalization. We demonstrate
that for the commonly used (cid:96)1 regularization the answer is negative. We prove this using a simple
modiﬁcation of our lower bound construction: We shift the functions to the positive orthant where
the regularization terms λ(cid:107)x(cid:107)1 is just a linear function. We then subtract this linear function from
each function in our construction  thereby balancing the regularization (while maintaining convexity
and Lipschitz-boundedness). The details of this construction appear in Sec. 3.3 (see Thm. 3.7).

Dependence on accuracy: For simplicity and convenience we have ignored the dependence on the
accuracy   Lipschitz bound L and radius R of K in our lower bounds. It is easy to see  that this more
general setting can be reduced to the case we consider here (Lipschitz bound and radius are equal to
1) with accuracy parameter (cid:48) = /(LR). We generalize our lower bound to this setting and prove
that Ω(d/(cid:48)2) samples are necessary for uniform convergence and Ω(d/(cid:48)) samples are necessary
for generalization of ERM. Note that the upper bound on the sample complexity of these settings is
˜O(d/(cid:48)2) and therefore the dependence on (cid:48) in our lower bound does not match the upper bound for
ERM. Resolving this gap or even proving any ω(d/(cid:48) + 1/(cid:48)2) lower bound is an interesting open
problem. Additional details can be found in the full version.

Bounded-range SCO: Finally  we consider a more general class of bounded-range convex functions
Note that the Lipschitz bound of 1 and the bound of 1 on the radius of K imply a bound of 1 on the
range (up to a constant shift which does not affect the optimization problem). While this setting is not
as well-studied  efﬁcient algorithms for it are known. For example  the online algorithm in a recent
work of Rakhlin and Sridharan [16] together with standard online-to-batch conversion arguments
[6]  imply that the sample complexity of this problem is ˜O(1/2) for any K that is an (cid:96)2 ball (of any
radius). For general convex bodies K  the problems can be solved via random walk-based approaches
[3  10] or an adaptation of the center-of-gravity method given in [10]. Here we show that for this
setting ERM might completely fail already for K being the unit 2-dimensional ball. The construction
is based on ideas similar to those we used in the smooth case and is formally described in in the full
version.

2 Preliminaries
= {1  . . .   n}. Random variables are denoted by bold letters  e.g.  f.
For an integer n ≥ 1 let [n]
.
p(R)  and the unit ball by Bd
Given p ∈ [1 ∞] we denote the ball of radius R > 0 in (cid:96)p norm by Bd
p.
For a convex body (i.e.  compact convex set with nonempty interior) K ⊆ Rd  we consider problems
(cid:27)
of the form

(cid:26)

.
= E
f∼D

[f (x)]

 

minK (FD)

.
= min
x∈K

FD(x)

4

.
= 1
n

(cid:80)

i∈[n] f i.

where f is a random variable deﬁned over some set of convex  sub-differentiable functions F on K
and distributed according to some unknown probability distribution D. We denote F ∗ = minK(FD).
For an approximation parameter  > 0 the goal is to ﬁnd x ∈ K such that FD(x) ≤ F ∗ +  and we
call any such x an -optimal solution. For an n-tuple of functions S = (f 1  . . .   f n) we denote by
FS
We say that a point ˆx is an empirical risk minimum for an n-tuple S of functions over K  if
FS(ˆx) = minK(FS). In some cases there are many points that minimize FS and in this case we refer
to a speciﬁc algorithm that selects one of the minimums of FS as an empirical risk minimizer. To
make this explicit we refer to the output of such a minimizer by ˆx(S) .
Given x ∈ K  and a convex function f we denote by ∇f (x) ∈ ∂f (x) an arbitrary selection of
a subgradient. Let us make a brief reminder of some important classes of convex functions. Let
p ∈ [1 ∞] and q = p∗ .
= 1/(1 − 1/p). We say that a subdifferentiable convex function f : K → R
is in the class

• F(K  B) of B-bounded-range functions if for all x ∈ K  |f (x)| ≤ B.
• F 0

p (K  L) of L-Lipschitz continuous functions w.r.t. (cid:96)p  if for all x  y ∈ K  |f (x) − f (y)| ≤
L(cid:107)x − y(cid:107)p;
p (K  σ) of functions with σ-Lipschitz continuous gradient w.r.t. (cid:96)p  if for all x  y ∈ K 
(cid:107)∇f (x) − ∇f (y)(cid:107)q ≤ σ(cid:107)x − y(cid:107)p.

• F 1

We will omit p from the notation when p = 2. Omitted proofs can be found in the full version [9].

3 Lower Bounds for Lipschitz-Bounded SCO

In this section we present our main lower bounds for SCO of Lipschitz-bounded convex functions.
For comparison purposes we start by formally stating some known bounds on sample complexity of
solving such problems. The following uniform convergence bounds can be easily derived from the
standard covering number argument (e.g. [21  18])
Theorem 3.1. For p ∈ [1 ∞]  let K ⊆ Bd
p(R) and let D be any distribution supported on functions
L-Lipschitz on K relative to (cid:96)p (not necessarily convex). Then  for every   δ > 0 and n ≥ n1 =
O

(cid:16) d·(LR)2·log(dLR/(δ))

(cid:17)

2

[∃x ∈ K  |FD(x) − FS(x)| ≥ ] ≤ δ.

Pr
S∼Dn

The following upper bounds on sample complexity of Lipschitz-bounded SCO can be obtained from
several known algorithms [14  18] (see [17] for a textbook exposition for p = 2).
Theorem 3.2. For p ∈ [1  2]  let K ⊆ Bd
p(R). Then  there is an algorithm Ap that given   δ > 0 and
p (K  L)  outputs an -
n = np(d  R  L    δ) i.i.d. samples from any distribution D supported on F 0
optimal solution to FD over K with probability ≥ 1− δ. For p ∈ (1  2]  np = O((LR/)2 · log(1/δ))
and for p = 1  np = O((LR/)2 · log d · log(1/δ)).
Stronger results are known under additional assumptions on smoothness and/or strong convexity
(e.g. [14  15  20  1]).

3.1 Non-smooth construction

We will start with a simpler lower bound for non-smooth functions. For simplicity  we will also
restrict R = L = 1. Lower bounds for the general setting can be easily obtained from this case by
scaling the domain and desired accuracy.
We will need a set of vectors W ⊆ {−1  1}d with the following property: for any distinct w1  w2 ∈
W   (cid:104)w1  w2(cid:105) ≤ d/2. The Chernoff bound together with a standard packing argument imply that
there exists a set W with this property of size ≥ ed/8 ≥ 2d/6.
For any subset V of W we deﬁne a function

gV (x)

= max{1/2  max
.
w∈V

(cid:104) ¯w  x(cid:105)} 

(1)

5

√
= w/(cid:107)w(cid:107) = w/
.

d. See Figure 1 for an illustration. We ﬁrst observe that gV is convex and
where ¯w
1-Lipschitz (relative to (cid:96)2). This immediately follows from (cid:104) ¯w  x(cid:105) being convex and 1-Lipschitz for
every w and gV being the maximum of convex and 1-Lipschitz functions.
Theorem 3.3. Let K = Bd
= {gV | V ⊆ W} for gV deﬁned in eq. (1). Let D be
.
the uniform distribution over H2. Then for n ≤ d/6 and every set of samples S there exists an ERM
ˆx(S) such that

2 and we deﬁne H2

[FD(ˆx(S)) − F ∗ ≥ 1/4] > 1/2.

Pr
S∼Dn

.

Proof. We start by observing that the uniform distribution over H2 is equivalent to picking the
function gV where V is obtained by including every element of W with probability 1/2 randomly
and independently of all other elements. Further  by the properties of W   for every w ∈ W   and
V ⊆ W   gV ( ¯w) = 1 if w ∈ V and gV ( ¯w) = 1/2 otherwise. For gV chosen randomly with respect
to D  we have that w ∈ V with probability exactly 1/2. This implies that FD( ¯w) = 3/4.
Let S = (gV1  . . .   gVn ) be the random samples. Observe that minK(FS) = 1/2 and F ∗ =
i∈[n] Vi (cid:54)= W then let
i∈[n] Vi. Otherwise ˆx(S) is deﬁned to be the origin ¯0. Then by the
ˆx(S)
property of H2 mentioned above  we have that for all i  gVi(ˆx(S)) = 1/2 and hence FS(ˆx(S)) = 1/2.
This means that ˆx(S) is a minimizer of FS.
i∈[n] Vi (cid:54)= W then there exists an ERM ˆx(S) such that
FS(ˆx(S)) = minK(FS) and FD(ˆx(S))− F ∗ = 1/4. Therefore to prove the claim it sufﬁces to show
that for n ≤ d/6 we have that

= ¯w for any w ∈ W \(cid:83)

minK(FD) = 1/2 (the minimum is achieved at the origin ¯0). Now  if(cid:83)
Combining these statements  we get that  if(cid:83)
(cid:91)
w ∈ (cid:91)
 =(cid:0)1 − 2−n(cid:1)|W| ≤ e−2−n·2d/6 ≤ e−1 <

 >
 = 1 − 2−n
and this event is independent from the inclusion of other elements in(cid:83)

This easily follows from observing that for the uniform distribution over subsets of W   for every
w ∈ W  

(cid:91)

i∈[n] Vi. Therefore

Vi (cid:54)= W

Vi = W

Vi

i∈[n]

Pr
S∼Dn

i∈[n]

Pr
S∼Dn

1
2

.

1
2

.

Pr
S∼Dn

i∈[n]

Other (cid:96)p norms: We now observe that exactly the same approach can be used to extend this lower
bound to (cid:96)p/(cid:96)q setting. Speciﬁcally  for p ∈ [1 ∞] and q = p∗ we deﬁne

gp V (x)

.
= max

2
It is easy to see that for every V ⊆ W   gq V ∈ F 0
p  1). We can now use the same argument
as before with the appropriate normalization factor for points in Bd
p. Namely  instead of ¯w for
w ∈ W we consider the values of the minimized functions at w/d1/p ∈ Bd
p. This gives the following
generalization of Thm. 3.3.
Theorem 3.4. For every p ∈ [1 ∞] let K = Bd
= {gp V | V ⊆ W} and let D be
.
the uniform distribution over Hp. Then for n ≤ d/6 and every set of samples S there exists an ERM
ˆx(S) such that

p and we deﬁne Hp

(cid:27)

.

(cid:104)w  x(cid:105)
d1/q

(cid:26) 1

  max
w∈V
p (Bd

[FD(ˆx(S)) − F ∗ ≥ 1/4] > 1/2.

Pr
S∼Dn

6

3.2 Smoothness does not help

We now extend the lower bound to smooth functions. We will for simplicity restrict our attention to
(cid:96)2 but analogous modiﬁcations can be made for other (cid:96)p norms. The functions gV that we used in the
construction use two maximum operators each of which introduces non-smoothness. To deal with
maximum with 1/2 we simply replace the function max{1/2 (cid:104) ¯w  x(cid:105)} with a quadratically smoothed
version (in the same way as hinge loss is sometimes replaced with modiﬁed Huber loss). To deal
with the maximum over all w ∈ V   we show that it is possible to ensure that individual components
do not “interact". That is  at every point x  the value  gradient and Hessian of at most one component
function are non-zero (value  vector and matrix  respectively). This ensures that maximum becomes
addition and Lipschitz/smoothness constants can be upper-bounded easily.
Formally  we deﬁne

Now  for V ⊆ W   we deﬁne

ν(a)

.
=

hV (x)

.
=

if a ≤ 0
otherwise.

a2

(cid:26) 0
(cid:88)

w∈V

ν((cid:104) ¯w  x(cid:105) − 7/8).

(2)

See Figure 2 for an illustration. We ﬁrst prove that hV is 1/4-Lipschitz and 1-smooth.
Lemma 3.5. For every V ⊆ W and hV deﬁned in eq. (2) we have hV ∈ F 0
From here we can use the proof approach from Thm. 3.3 but with hV in place of gV .
Theorem 3.6. Let K = Bd
= {hV | V ⊆ W} for hV deﬁned in eq. (2). Let D be
the uniform distribution over H. Then for n ≤ d/6 and every set of samples S there exists an ERM
ˆx(S) such that

2 and we deﬁne H .

2  1/4) ∩ F 1

2 (Bd

2 (Bd

2  1).

[FD(ˆx(S)) − F ∗ ≥ 1/128] > 1/2.

Pr
S∼Dn

(cid:96)1 Regularization does not help

√
d. (Note that if λ > 1/

3.3
Next we show that the lower bound holds even with an additional (cid:96)1 regularization term λ(cid:107)x(cid:107) for
√
positive λ ≤ 1/
d then the resulting program is no longer 1-Lipschitz
relative to (cid:96)2. Any constant λ can be allowed for (cid:96)1/(cid:96)∞ setup). To achieve this we shift the
construction to the positive orthant (that is x such that xi ≥ 0 for all i ∈ [d]). In this orthant the
subgradient of the regularization term is simply λ¯1 where ¯1 is the all 1’s vector. We can add a linear
term to each function in our distribution that balances this term thereby reducing the analysis to
non-regularized case. More formally  we deﬁne the following family of functions. For V ⊆ W  

√

hλ
V (x)

= hV (x − ¯1/
.

d) − λ(cid:104)¯1  x(cid:105).
V (x) is L-Lipschitz for L ≤ 2(2 − 7/8) + λ
2(2) and for a given λ ∈ (0  1/

Note that over Bd
2(2)  hλ
prove this formally.
V | V ⊆ W} for
Theorem 3.7. Let K = Bd
V deﬁned in eq. (3). Let D be the uniform distribution over Hλ. Then for n ≤ d/6 and every set of
hλ
samples S there exists ˆx(S) such that

(3)
d ≤ 9/4. We now state and

d]  we deﬁne Hλ .

= {hλ

√

√

• FS(ˆx(S)) = minx∈K(FS(x) + λ(cid:107)x(cid:107)1);
• PrS∼Dn [FD(ˆx(S)) − F ∗ ≥ 1/128] > 1/2.

4 Lower Bound for Low-Complexity Functions

We will now demonstrate that our lower bounds hold even if one restricts the attention to functions
that can be computed efﬁciently (in time polynomial in d). For this purpose we will rely on known
constructions of binary linear error-correcting codes. We describe the construction for non-smooth
(cid:96)2/(cid:96)2 setting but analogous versions of other constructions can be obtained in the same way.

7

(cid:26)

(cid:27)

1 − r
2d

  max
w∈Wj

(cid:104) ¯w  x(cid:105)

 

We start by brieﬂy providing the necessary background about binary codes. For two vectors w1  w2 ∈
{±1}d let #(cid:54)=(w1  w2) denote the Hamming distance between the two vectors. We say that a
mapping G : {±1}k → {±1}d is a [d  k  r  T ] binary error-correcting code if G has distance at least
2r + 1  G can be computed in time T and there exists an algorithm that for every w ∈ {±1}d such
that for some z ∈ {±1}k  #(cid:54)=(w  G(z)) ≤ r ﬁnds such z in time T (note that such z is unique).
Given [d  k  r  T ] code G  for every j ∈ [k]  we deﬁne a function

gj(x)

.
= max

(4)
= {G(z) | z ∈ {±1}k  zj = 1}. As before  we note that gj is convex and 1-Lipschitz
.

where Wj
(relative to (cid:96)2).
We can now use any existing constructions of efﬁcient binary error-correcting codes to obtain a lower
bound that uses only a small set of efﬁciently computable convex functions. Getting a lower bound
that has asymptotically optimal dependence on d requires that k = Ω(d) and r = Ω(d) (referred
to as being asymptotically good). The existence of efﬁciently computable and asymptotically good
binary error-correcting codes was ﬁrst shown by Justesen [12]. More recent work of Spielman [22]
shows existence of asymptotically good codes that can be encoded and decoded in O(d) time. In
particular  for some constant ρ > 0  there exists a [d  d/2  ρ · d  O(d)] binary error-correcting code.
As a corollary we obtain the following lower bound.
Corollary 4.1. Let G be an asymptotically-good [d  d/2  ρ · d  O(d)] error-correcting code for a
constant ρ > 0. Let K = Bd
= {gj | j ∈ [d/2]} for gj deﬁned in eq. (4). Let D
.
be the uniform distribution over HG. Then for every x ∈ K  gj(x) can be computed in time O(d).
Further  for n ≤ d/4 and every set of samples S ∈ Hn

2 and we deﬁne HG

G there exists an ERM ˆx(S) such that

FD(ˆx(S)) − F ∗ ≥ ρ/4.

5 Discussion

Our work points out to substantial limitations of the classic approach to understanding and analysis
of generalization in the context of general SCO. Further  it implies that in order to understand
how well solutions produced by an optimization algorithm generalize  it is necessary to examine
the optimization algorithm itself. This is a challenging task that we still have relatively few tools
to address. Yet such understanding is also crucial for developing theory to guide the design of
optimization algorithms that are used in machine learning applications.
One way to bypass our lower bounds is to use additional structural assumptions. For example  for
generalized linear regression problems uniform convergence gives nearly optimal bounds on sample
complexity [13]. One natural question is whether there exist more general classes of functions that
capture most of the practically relevant SCO problems and enjoy dimension-independent (or  scaling
as log d) uniform convergence bounds.
An alternative approach is to bypass uniform convergence (and possibly also ERM) altogether.
Among a large number of techniques that have been developed for ensuring generalization  the most
general ones are based on notions of stability [4  19]. However  known analyses based on stability
often do not provide the strongest known generalization guarantees (e.g. high probability bounds
require very strong assumptions). Another issue is that we lack general algorithmic tools for ensuring
stability of the output. Therefore many open problems remain and signiﬁcant progress is required to
obtain a more comprehensive understanding of this approach. Some encouraging new developments
in this area are the use of notions of stability derived from differential privacy [7  8  2] and the use of
techniques for analysis of convergence of convex optimization algorithms for proving stability [11].

Acknowledgements

I am grateful to Ken Clarkson  Sasha Rakhlin and Thomas Steinke for discussions and insightful
comments related to this work.

8

References
[1] F. R. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate

o(1/n). In NIPS  pages 773–781  2013.

[2] R. Bassily  K. Nissim  A. D. Smith  T. Steinke  U. Stemmer  and J. Ullman. Algorithmic stability for

adaptive data analysis. In STOC  pages 1046–1059  2016.

[3] A. Belloni  T. Liang  H. Narayanan  and A. Rakhlin. Escaping the local minima via simulated annealing:

Optimization of approximately convex functions. In COLT  pages 240–265  2015.

[4] O. Bousquet and A. Elisseeff. Stability and generalization. JMLR  2:499–526  2002.

[5] S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine

Learning  8(3-4):231–357  2015.

[6] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[7] C. Dwork  V. Feldman  M. Hardt  T. Pitassi  O. Reingold  and A. Roth. Preserving statistical validity in

adaptive data analysis. CoRR  abs/1411.2664  2014. Extended abstract in STOC 2015.

[8] C. Dwork  V. Feldman  M. Hardt  T. Pitassi  O. Reingold  and A. Roth. Generalization in adaptive data

analysis and holdout reuse. CoRR  abs/1506  2015. Extended abstract in NIPS 2015.

[9] V. Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes back. CoRR 

abs/1608.04414  2016. Extended abstract in NIPS 2016.

[10] V. Feldman  C. Guzman  and S. Vempala. Statistical query algorithms for mean vector estimation and

stochastic convex optimization. CoRR  abs/1512.09170  2015. Extended abstract in SODA 2017.

[11] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient descent.

In ICML  pages 1225–1234  2016.

[12] J. Justesen. Class of constructive asymptotically good algebraic codes. IEEE Trans. Inf. Theor.  18(5):652 –

656  1972.

[13] S. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk bounds  margin

bounds  and regularization. In NIPS  pages 793–800  2008.

[14] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM J. Optim.  19(4):1574–1609  2009.

[15] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex stochastic

optimization. In ICML  2012.

[16] A. Rakhlin and K. Sridharan. Sequential probability assignment with binary alphabets and large classes of

experts. CoRR  abs/1501.07340  2015.

[17] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press  2014.

[18] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Stochastic convex optimization. In COLT 

2009.

[19] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Learnability  stability and uniform convergence.

The Journal of Machine Learning Research  11:2635–2670  2010.

[20] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results

and optimal averaging schemes. In ICML  pages 71–79  2013.

[21] A. Shapiro and A. Nemirovski. On complexity of stochastic programming problems. In V. Jeyakumar and
A. M. Rubinov  editors  Continuous Optimization: Current Trends and Applications 144. Springer  2005.

[22] D. Spielman. Linear-time encodable and decodable error-correcting codes.

Information Theory  42(6):1723–1731  1996.

IEEE Transactions on

[23] V. Vapnik. Statistical Learning Theory. Wiley-Interscience  New York  1998.

9

,Vitaly Feldman