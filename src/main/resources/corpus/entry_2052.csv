2019,Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes,We show for the first time that it is possible to  reconcile in online learning in zero-sum games two seemingly contradictory objectives: vanishing time-average regret and non-vanishing step sizes. This phenomenon  that we coin   ``fast and furious" learning in games  sets a new benchmark about what is possible both in  max-min optimization as well as in multi-agent systems. Our analysis does not depend on introducing a carefully tailored  dynamic. Instead we focus on the most well studied online dynamic  gradient descent. Similarly  we focus on the simplest textbook class of games  two-agent two-strategy zero-sum games  such as Matching Pennies. Even for this simplest of benchmarks the best known bound for total regret  prior to our work  was the trivial one of $O(T)$  which is immediately applicable even to a non-learning agent.  Based on a tight understanding of the geometry of the non-equilibrating trajectories in the dual space we prove a regret bound of $\Theta(\sqrt{T})$ matching the well known optimal bound for adaptive step sizes in the online setting. This guarantee holds for all fixed step-sizes without having to know the time horizon in advance and adapt the fixed step-size accordingly.As a corollary  we establish that even with fixed learning rates the time-average of mixed strategies  utilities converge to their exact Nash equilibrium values. We also provide experimental evidence suggesting the stronger regret bound holds for all zero-sum games.,Fast and Furious Learning in Zero-Sum Games:
Vanishing Regret with Non-Vanishing Step Sizes

James P. Bailey

Texas A&M University

jamespbailey@tamu.edu

Georgios Piliouras

Singapore University of Technology and Design

georgios@sutd.edu.sg

Abstract

We show for the ﬁrst time that it is possible to reconcile in online learning in
zero-sum games two seemingly contradictory objectives: vanishing time-average
regret and non-vanishing step sizes. This phenomenon  that we coin “fast and
furious" learning in games  sets a new benchmark about what is possible both
in max-min optimization as well as in multi-agent systems. Our analysis does
not depend on introducing a carefully tailored dynamic. Instead we focus on the
most well studied online dynamic  gradient descent. Similarly  we focus on the
simplest textbook class of games  two-agent two-strategy zero-sum games  such as
Matching Pennies. Even for this simplest of benchmarks the best known bound for
total regret  prior to our work  was the trivial one of O(T )  which is immediately
applicable even to a non-learning agent. Based on a tight understanding of the
√
geometry of the non-equilibrating trajectories in the dual space we prove a regret
bound of Θ(
T ) matching the well known optimal bound for adaptive step sizes
in the online setting. This guarantee holds for all ﬁxed step-sizes without having
to know the time horizon in advance and adapt the ﬁxed step-size accordingly.As
a corollary  we establish that even with ﬁxed learning rates the time-average of
mixed strategies  utilities converge to their exact Nash equilibrium values. We also
provide experimental evidence suggesting the stronger regret bound holds for all
zero-sum games.

(a) Player Strategies

(b) Player 1 Regret

(c) Player 1 Regret Squared

Figure 1: 5000 Iterations of Gradient Descent on Matching Pennies with η = .15.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Heads HeadsTails HeadsHeads TailsTails Tails100020003000400050001020304050Heads HeadsRegretvsIteration10002000300040005000Regret2vsIteration50010001500200025001

Introduction

The performance of online learning algorithms such as online gradient descent in adversarial  adaptive
settings is a classic staple of optimization and game theory  e.g  Cesa-Bianchi and Lugoisi [2006] 
Fudenberg and Levine [1998]  Young [2004]. Arguably  the most well known results in this space are
the following:

√

i) Sublinear regret of O(

T ) is achievable in adversarial settings but only after employing a
carefully chosen sequence of shrinking step-sizes or if the time horizon is ﬁnite and known
in advance and the ﬁxed learning rate is selected accordingly.

ii) Sublinear regret algorithms “converge" to Nash equilibria in zero-sum games.

Despite the well established nature of these results recent work has revealed some surprising insights
that come to challenge the traditional ways of thinking in this area. Speciﬁcally  in the case of
zero-sum games what is referred to as “convergence" to equilibrium  is the fact that when both agent
apply regret-minimizing algorithms  both the time-average of the mixed strategy proﬁles as well
as the utilities of the agents converge approximately to their Nash equilibrium values  where the
approximation error can become arbitrarily close to zero by choosing a sufﬁciently small step-size.
Naturally  this statement does not imply that the day-to-day behavior converges to equilibria. In fact 
the actual realized behavior is antithetical to convergence to equilibrium. Bailey and Piliouras [2018]
showed that Nash equilibria are repelling in zero-sum games for all follow-the-regularized-leader
dynamics. As seen in Figure 1 the dynamics spiral outwards away from the equilibrium.
These novel insights about the geometry of learning dynamics in zero-sum games suggest a much
richer and not well understood landscape of coupled strategic behaviors. They also raise the tantalizing
possibility that we may be able to leverage this knowledge to prove tighter regret bounds in games.
In fact  a series of recent papers has focused on beating the “black-box" regret bounds using a
combination of tailored dynamics and adaptive step-sizes  e.g  Daskalakis et al. [2011]  Rakhlin and
Sridharan [2013]  Syrgkanis et al. [2015]  Foster et al. [2016] but so far no new bounds have been
proven for the classic setting of ﬁxed learning rates. Interestingly  Foster et al. [2016] explicitly
examine the case of ﬁxed learning rates η to show that learning achieves sublinear “approximate
regret" where the algorithm compares itself against (1 − η) times the performance of the best action
with hindsight. In contrast  our aim is to show sublinear regret for ﬁxed η using the standard notion
of regret.
Intuitively  non-equilibration and more generally this emergent behavioral complexity seem like
harbingers of bad news in terms of system performance as well as of signiﬁcant analytical obstacles.
This pessimism seems especially justiﬁed given recent results about the behavior of online dynamics
with ﬁxed step-sizes in other small games (e.g. two-by-two coordination/congestion games)  where
their behavior can be shown to become provably chaotic (Palaiopanos et al. [2017]  Chotibut et al.
[2018]). Nevertheless  we show that we can leverage this geometric information to provide the ﬁrst to
our knowledge sublinear regret guarantees for online gradient descent with ﬁxed step-size in games.
Instability of Nash equilibria is not an obstacle  but in fact may be leveraged as a tool  for proving
low regret.

Our theoretical results. We study the dynamics of gradient descent with ﬁxed step size in two-
strategy  two-player games. We leverage a deep understanding of the geometry of its orbits to
prove the ﬁrst sublinear regret bounds despite the constant learning rate. We show that the player
strategies are repelled away from the Nash equilibrium. More speciﬁcally  regardless of the choice
of the initial condition there are only a ﬁnite number of iterations where both players select mixed
strategies (Theorem 1). We prove a worst-case regret bound of O(
T ) for arbitrarily learning
without prior knowledge of T (Theorem 3) matching the well known optimal bound for adaptive
learning rates. An immediate corollary of our results is that time-average of the mixed strategy
√
proﬁles as well as the utilities of the agents converge to their exact Nash equilibrium values (and
not to approximations thereof) (Corollary 4). Finally  we present a matching lower bound of Ω(
T )
(Theorem 5) establishing that our regret analysis is tight.
To obtain the upper bound  we establish a tight understanding of the geometry of the trajectories in the
dual space  i.e.  the trajectories of the payoff vectors. We show there exists a linear transformation of

√

2

the payoff vectors that rotate around the Nash equilibrium. Moreover  the distance between the Nash
equilibrium and these transformed utility vectors increases by a constant in each rotation (Lemma
8). In addition  the time to complete a rotation is proportional to the distance between the Nash
equilibrium and the transformed payoff vectors (Lemma 9). Together  these results imply a quadratic
√
relationship between the number of iterations and the number of rotations completed establishing the
T ) regret bound. We establish the lower bound by exactly tracking the strategies and regret for a
O(
single game.
Our experimental results. Many of the proof techniques we develop extend to higher dimensions
suggesting sublinear regret in general zero-sum games. To test this  we conducted experiments to
√
measure regret in higher dimension. Our simulations for 5x5  10x10  and 50x50 games suggest that
regret is sublinear and close to Θ(
T ) for larger games. A summary of our simulations are given in
Table 1 and the fully details appear in Appendix I.

Table 1: Regression Summary for 10 000 Iterations of Gradient Descent in 30 Random Games
|support of x∗|
strategies Regret1(T ) ≈ b · T a

% of variability explained

p-value
a ∈ [0.4492  0.5248] < .000001
a ∈ [0.3662  0.5504] < .000001
a ∈ [0.4653  0.5563] < .000001
a ∈ [0.5260  0.5776] < .000001

2
5
10
50

93.53403 – 99.83818
97.04427 – 99.91377
98.79963 – 99.87485
99.40158 – 99.86970

2
2-5
3-7
21-30

2 Preliminaries
A two-player game consists of two players {1  2} where each player has ni strategies to select from.
(cid:80)
Player i can either select a pure strategy j ∈ [ni] or a mixed strategy xi ∈ Xi = {xi ∈ Rni≥0 :
j∈[ni] xij = 1}. A strategy is fully mixed if xi ∈ Rni
>0.

The most commonly studied class of games is zero-sum games. In a zero-sum game  there is a payoff
matrix A ∈ Rn1×n2 where player 1 receives utility x1 · Ax2 and player 2 receives utility −x1 · Ax2
resulting in the following optimization problem:

max
x1∈X1

min
x2∈X2

x1 · Ax2

(Two-Player Zero-Sum Game)

The solution to this saddle problem is the Nash equilibrium xN E. If player 1 selects her Nash
equilibria xN E
independent of what
strategy player 2 selects. xN E

· AxN E
is referred to as the value of the game.

  then she guarantees her utility is xN E

· Ax2 ≥ xN E

· AxN E

1

1

2

1

1

2

2.1 Online Learning in Continuous Time

In many applications of game theory  players know neither the payoff matrix nor the Nash equilibria.
In such settings  players select their strategies adaptively. The most common way to do this in
continuous time is by using a follow-the-regularized-leader (FTRL) algorithm. Given a strongly
convex regularizer  a learning rate η  and an initial payoff vector yi(0)  players select their strategies
at time T according to

y1(T ) = y1(0) +

Ax2(t)dt

(Player 1 Payoff Vector)

(cid:90) T
(cid:90) T

0

0

y2(T ) = y2(0) −
xi≥0:(cid:80)

xi(T ) =

arg max

j∈[ni] xij =1

(cid:124)

A

x1(t)dt

(cid:26)

yi(T ) · xi − hi(xi)

η

3

(cid:27)

(Player 2 Payoff Vector)

(Continuous FTRL)

In this paper  we are primarily interested in the regularizer hi(xi) = ||xi||2
Descent algorithm:

2/2 resulting in the Gradient

(cid:26)

(cid:27)

xi(t) =

arg max

xi≥0:(cid:80)

j∈[ni] xij =1

yi(t) · xi − ||xi||2

2

2η

(Continuous Gradient Descent)

Continuous time FTRL learning in games has an interesting number of properties including time-
average converge to the set of coarse correlated equilibria at a rate of O(1/T ) in general games
(Mertikopoulos et al. [2018]) and thus to Nash equilibria in zero-sum games. These systems can also
exhibit interesting recurrent behavior e.g. periodicity (Piliouras and Schulman [2018]  Nagarajan et al.
[2018])  Poincaré recurrence (Mertikopoulos et al. [2018]  Piliouras and Shamma [2014]  Piliouras
et al. [2014]) and limit cycles (Kleinberg et al. [2011]). These systems have formal connections to
Hamiltonian dynamics (i.e. energy preserving systems) (Bailey and Piliouras [2019]). All of these
types of recurrent behavior are special cases of chain recurrence (Papadimitriou and Piliouras [2018] 
Omidshaﬁei et al. [2019]).

2.2 Online Learning in Discrete Time

In most settings  players update their strategies iteratively in discrete time steps. The most common
class of online learning algorithms is again the family of follow-the-regularized-leader algorithms.

t=1

1 +

T−1(cid:88)
2 − T−1(cid:88)
xi≥0:(cid:80)
xi≥0:(cid:80)

t=1

yT
1 = y0

yT
2 = y0

xt
i =

xt
i =

Axt
2

(cid:124)

A

xt
1

arg max

j∈[ni] xij =1

arg max

j∈[ni] xij =1

(cid:26)
(cid:26)

(cid:27)
i · xi − hi(xi)
(cid:27)
yt
i · xi − ||xi||2

yt

η

2

2η

(Player 1 Payoff Vector)

(Player 2 Payoff Vector)

(FTRL)

(Gradient Descent)

where η corresponds to the learning rate. In Lemma 6 of Appendix B  we show (FTRL) is the ﬁrst
order approximation of (Continuous FTRL).
These algorithms again have interesting properties in zero-sum games. The time-average strategy
converges to a O(η)-approximate Nash equilibrium (Cesa-Bianchi and Lugoisi [2006]). On the
contrary  Bailey and Piliouras [2018] show that the day-to-day behavior diverges away from interior
Nash equilibria. For notational simplicity we do not introduce different learning rates η1  η2 but all of
our proofs immediately carry over to this setting.

2.3 Regret in Online Learning

The most common way of analyzing an online learning algorithm is by examining its regret. The
regret at time/iteration T is the difference between the accumulated utility gained by the algorithm
and the total utility of the best ﬁxed action with hindsight. Formally for player 1 

x1 · Ax2(t)dt

−

x1(t) · Ax2(t)dt

(cid:40)(cid:90) T
(cid:40) T(cid:88)

0

(cid:41)
− T(cid:88)

(cid:41)

(cid:90) T

0

1 · Axt
xt

2

x1 · Axt

2

t=0

t=0

Regret1(T ) = max
x1∈X1

Regret1(T ) = max
x1∈X1

(1)

(2)

for continuous and discrete time respectively.
In the case of (Continuous FTRL) it is possible to show rather strong regret guarantees. Speciﬁcally 
Mertikopoulos et al. [2018] establish that Regret1(T ) ∈ O(1) even for non-zero-sum games. In
contrast  (FTRL) only guarantees Regret1(T ) ∈ O(η · T ) for a ﬁxed learning rate. In this paper  we
√
utilize the geometry of (Gradient Descent) to show Regret1(T ) ∈ O(
T ) in 2x2 zero-sum games
(n1 = n2 = 2).

4

3 The Geometry of Gradient Descent

Theorem 1. Let A be a 2x2 game that has a unique fully mixed Nash equilibrium where strategies
are updated according to (Gradient Descent). For any non-equilibrium initial strategies and any
ﬁxed learning rate η  there exists a B such that xt is on the boundary for all t ≥ B.
Theorem 1 strengthens the result for (Gradient Descent) in 2x2 games from Bailey and Piliouras
[2018]. Speciﬁcally  Bailey and Piliouras [2018] show that strategies come arbitrarily close to the
boundary inﬁnitely often when updated with any version of (FTRL). This is accomplished by closely
studying the geometry of the player strategies. We strengthen this result for (Gradient Descent) in
2x2 games by focusing on the geometry of the payoff vectors. The proof of Theorem 1 relies on
many of the tools developed in Section 4 for Theorem 3 and is deferred to Appendix G. The ﬁrst step
to understanding the trajectories of the dynamics of (Gradient Descent)  is characterizing the solution
to (Gradient Descent). The exact solution of (Gradient Descent) is described by Lemma 2 below.
Lemma 2. The solution to (Gradient Descent) is given by

(cid:40)0

(cid:16)

η

yt

ij −(cid:80)

xt
ij =

(cid:17)

for j /∈ Si
for j ∈ Si

.

+ 1|Si|

(3)

k∈Si

yt
ik|Si|

where Si is found using Algorithm 1.

Si ← [ni]
(cid:16)
(cid:17)
ik}
Select j ∈ arg mink∈Si{yt
if η
+ 1|Si| < 0

Algorithm 1 Finding Optimal Set Si
1: procedure FIND Si
2:
3: Search:
4:
5:
6:
7:
8:
9:

yt
k∈Si
Si ← Si \ {j}
goto Search

ij −(cid:80)

return Si

yt
ik|Si|

else

We defer the proof of Lemma 2 to Appendix C.

3.1 Convex Conjugate of the Regularizer

Our analysis primarily takes place in the space of payoff vectors. The payoff vector yt
dual of the strategy xt

i obtained via

i is a formal

(cid:26)

(cid:27)

h∗(yt

i ) =

xi≥0:(cid:80)

max
j∈[ni] xij =1

i · xi − hi(xi)
yt

η

(4)

i (yt

i=1 h∗

“energy” r =(cid:80)2
order approximation of (Continuous FTRL). The energy {y : r ≤ (cid:80)2

which is known as the convex conjugate or Fenchel Coupling of hi and is closely related to the
Bregman Divergence. Mertikopoulos et al. [2018] and Bailey and Piliouras [2019] show that the
i ) is conserved in (Continuous FTRL). By Lemma 6  (FTRL) is the ﬁrst
i (yi)} is convex  and
therefore the energy will be non-decreasing in (FTRL). Bailey and Piliouras [2018] capitalized on
this non-decreasing energy to show that strategies come arbitrarily close to the boundary inﬁnitely
often in (FTRL).
In a similar fashion  we precisely compute h∗(yt
i ) to better understand the dynamics of (Gradient
Descent). We deviate slightly from traditional analysis of (FTRL) and embed the learning rate η into
i||2
the regularizer hi(xt
2/(2η). Through the maximizing argument
(Kakade et al. [2009])  we have
i||2
i − ||xt
2η

i). Formally  deﬁne hi(xt

i) = ||xt

i=1 h∗

h∗
i (yt

i ) = yt

i · xt

(5)

2

.

5

From Lemma 2 

h∗
i (yt

i ) = yt

i · xt

i||2
i − ||xt
2η

2

(cid:88)

j∈Si

=

η
2

(yt

ij)2 +

(cid:88)

j∈Si

yt

ij|Si| − η

2

(cid:17)2

(cid:16)(cid:80)

yt
j∈Si
ij
|Si|

− 1
2η

1
|Si| .

(6)

2/(2η) is a strongly smooth function in the simplex  we expect for h∗

3.2 Selecting the Right Dual Space in 2x2 Games
Since hi(xi) = ||xi||2
i (yi) to be
strongly convex (Kakade et al. [2009]) – at least when it’s corresponding dual variable xi is positive.
However  (6) is not strongly convex for all yt
cannot appear anywhere
j=1 xij = 1}.
in Rni. Rather  yt+1
There are many non-intersecting dual spaces for the payoff vectors that yield strategies {xt
i}∞
t=1.
Mertikopoulos et al. [2018] informally deﬁne a dual space when they focus the analysis on the vector
yi(t) − yini(t)1. Similarly  we deﬁne a dual space that will be convenient for showing our results in
2x2 zero-sum games. Consider the payoff matrix

i dual to the domain {xi ∈ Rni≥0 :(cid:80)ni

i ∈ Rni. This is because yt+1

is contained to a space X ∗

i

i

(cid:21)

(cid:20) a b

c

d

A =

Without loss of generality  we may assume a > min{0  b  c}  d > min{0  b  c}  and A is singular 
i.e.  ad − bc = 0 (see Appendix D for details). Denote ∆yt

1 as

∆yt

1 = yt+1
= Axt
2

1 − yt

(cid:20) (a − b)xt

1

(c − d)xt

=

21 + b
21 + d

(cid:21)

(8)
(9)

(10)

Therefore

[d − c  a − b] · ∆yt
11 increases by a− b  yt

1 = ad − bc = 0
(11)
12 increases by c− d. Thus  the vector [a− b  c− d]
1 . Moreover  (FTRL) is invariant to constant shifts in the

since A is singular. When yt
describes the span of the dual space X ∗
payoff vector yt

[d − c  a − b] · yt

1 and therefore we may assume [d − c  a − b] · y0
1 = [d − c  a − b] · (yt−1
= [d − c  a − b] · yt−1
12 in terms of yt

11 

1 = 0. By induction 
1 + ∆yt−1
1 = 0

)

1

This conveniently allows us to express yt

(7)

(12)
(13)

(14)

(15)

(16)

(17)

yt
12 =

c − d
a − b

yt
11.

Symmetrically 

b − d
a − c
Combining these relationships with Lemma 2 yields

yt
22 =

yt
21.




xt
11 =

xt
21 =

(cid:16)

(cid:16)

0

1

η

0

1

η

1 − c−d
a−b

1 − b−d
a−c

(cid:17) yt
(cid:17) yt

11

2 + 1

2

21

2 + 1

2

6

(cid:17) yt
(cid:17) yt
(cid:17) yt
(cid:17) yt

(cid:16)
(cid:16)
(cid:16)
(cid:16)

1 − c−d
a−b
1 − c−d
a−b

if η
if η
otherwise

1 − b−d
a−c
1 − b−d
a−c

if η
if η
otherwise

11

2 + 1
2 + 1

11

2 ≤ 0
2 ≥ 1

21

2 + 1
2 + 1

21

2 ≤ 0
2 ≥ 1

The selection of this dual space also allows us to employ a convenient variable substitution to plot xt
and yt on the same graph.

zt
1 = η

zt
2 = η

The strategy xt can now be expressed as

Moreover  (6) can be rewritten as

h∗
i (yt

1) = ¯h∗

1(zt

1) =

h∗
i (yt

2) = ¯h∗

2(zt

2) =

(cid:19) yt
(cid:19) yt

11
2

21
2

+

+

1
2
1
2

1 − c − d
a − b
1 − b − d
a − c

xt
i1 =

i ≤ 0
if zt
i ≥ 1
if zt
otherwise

(cid:18)
(cid:18)

1
zt
i

0
α10zt
α20zt

α11zt
γ1(zt

α21zt
γ2(zt

1 − β10
1 − β11
1)2 + α1zt
2 − β20
2 − β21
2)2 + α2zt

1 − β1

2 − β2

(18)

(19)

(20)

(21)

(22)

1 ≤ 0
if zt
1 ≥ 1
if zt
otherwise
2 ≤ 0
if zt
2 ≥ 1
if zt
otherwise

where αi0 < 0  αi1 > 0  and γi > 0. Both of these expressions are obviously strongly convex when
the corresponding player strategy is in (0  1). The full details of these reduction can be found in
Appendix E. With this notation  (xt
21) is simply the projection of zt onto the unit square as
shown in Figure 2.

11  xt

z2

Strategies xt
Transformed Payoff Vector zt

z2

z1

z1

(a) Iterations 1-95

(b) Iterations 95-140

Figure 2: Strategies and Transformed Payoff Vectors Rotating Clockwise and Outwards in Matching
Pennies with η = .15 and (y0

11) = (.2 −.3).

11  y0

√
T ) Regret in 2x2 Zero-Sum Games
4 Θ(

(cid:16)√

(cid:17)

Theorem 3. Let A be a 2x2 game that has a unique fully mixed Nash equilibrium. When xt is
updated according to (Gradient Descent) with any ﬁxed learning rate η  Regret1(T ) ∈ O
.

T

It is well known that if an algorithm admits sublinear regret in zero-sum games  then the time-average
play converges to a Nash equilibirum. Thus  Theorem 3 immediately results in the following corollary.
Corollary 4. Let A be a 2x2 game that has a unique fully mixed Nash equilibrium. When xt
is updated according to (Gradient Descent) with any ﬁxed learning rate η  the average strategy

xt

T converges to xN E as T → ∞.

t=1

¯xT =(cid:80)T

Proof of Theorem 3. The result is simple if x1 = xN E. Neither player strategy will ever change.
Since player 1’s opponent is playing the fully mixed xN E
  player 1’s utility is constant independent
of what strategy is selected and therefore the regret is always 0. Now consider x1 (cid:54)= xN E.

2

7

z2

Strategies xt
Transformed Payoff Vector zt
Energy rj increases by Θ(1) per iteration.
There are Θ(1) iterations per rotation.
Energy rj does not change per iteration.
There are Θ(rj) iterations per rotation.

z1

Figure 3: Partitioning of Payoff Vectors for the Proof of Theorem 3.

The main details of the proof are captured in Figure 3. Speciﬁcally in Appendix F.1  we establish
break points t0 < t1 < ... < tk = T + 1 and analyze the impact strategies xtj   xtj +1  ...  xtj+1−1
have on the regret. The strategies xtj   xtj +1  ...  xtj+1−1 are contained in adjacent red and green
sections as shown in Figure 3.
In Appendix F.2  we show that there exists Θ(1) iterations where xt (cid:54)= xt+1 for each partitioning 
{tj  tj + 1  ...  tj+1 − 1}. Speciﬁcally  we show that Θ(1) consecutive payoff vectors appear in a red
section of Figure 3. The remaining points all appear in a green section and the corresponding player
strategies are equivalent. This implies

tj+1−1(cid:88)

1 − xt
(xt+1

1) · Axt

2 =

t=tj

∈

(cid:88)
(cid:88)

t∈[tj  tj+1−1]:xt+1

1

t∈[tj  tj+1−1]:xt+1

1

(cid:54)=xt

1

(cid:54)=xt

1

1 − xt
(xt+1

1) · Axt

2

O(1) ∈ O(1)

(23)

(24)

Denote rj =(cid:80)2
(cid:17)

¯h∗
i (ztj

i=1

i ) as the total energy of the system in iteration tj. In Appendix F.3  we
show this energy increases linearly in each partition  i.e.  rj+1 − rj ∈ Θ(1). In Appendix F.4  we
also show that the size of each partition is proportional to the energy in the system at the beginning of
that partition  i.e.  tj+1 − tj ∈ Θ(rj). Combining these two  tj ∈ Θ(j2). Therefore T ∈ Θ(k2) and
k ∈ Θ
where k is the total number of partitions. Finally  it is well known (Cesa-Bianchi and
Lugoisi [2006]) that the regret of player 1 in zero-sum games through T iterations is bounded by

(cid:16)√

T

t=0

T(cid:88)
t0−1(cid:88)
k(cid:88)

t=0

Regret1(T ) ≤ O(1) +

≤ O(1) +

∈ O(1) +

1 − xt
(xt+1

1) · Axt

2

2 +

1) · Axt
1 − xt
(xt+1
(cid:16)√
(cid:17)

O(1) ∈ O

T

k(cid:88)

ti−1(cid:88)

i=1

t=ti−1

1 − xt
(xt+1

1) · Axt

2

(25)

(26)

(27)

completing the proof of the theorem.

i=1

√
Next  we provide a game and initial conditions that has regret Θ(
Theorem 3 is tight.
Theorem 5. Consider the game Matching Pennies with ﬁxed learning rate η = 1 and initial
conditions y0
T ) when strategies are updated with
1 = y0
(Gradient Descent).

√
2 = (1  0). Then player 1’s regret is Θ(

T ) establishing that the bound in

The proof follows similarly to the proof of Theorem 3 by exactly computing the regret in every
iteration of (Gradient Descent). The full details appear in Appendix H.

8

5 Higher Dimensions and Other Regularizers

Many of the techniques introduced in this paper extend both to higher dimensions for Gradient
Descent and to other variants of FTRL. Our proof consists mainly of three parts:

1. the “step-size” in the dual space is bounded; i.e.  ||yt
2. a proof of divergence in the dual space where the divergence grows linearly when at least
one agent is not playing a pure strategy and negligibly when both agents are playing a pure
strategy.

|| ≤ b for some constant b.

i − yt−1

i

3. a proof of recurrence where the “cycle” length (in the primal/strategy space) is bounded

√

The ﬁrst two components immediately extend to higher dimensions using the current analysis. In
regards to the last step  recent advancements in understanding the geometry of learning dynamics in
larger games (e.g.  Mertikopoulos et al. [2018]  Bailey and Piliouras [2019]) suggest that  although
non-trivial  this last step can also be eventually rigorously established. However  new ideas are most
likely needed to for the last step. In Appendix I  we provide more evidence for sublinear regret in
higher dimensions including experiments suggesting that regret grows at approximately O(
T ) even
when the number of strategies is large.
It is also likely that sublinear regret extends to other variants of FTRL using a similar analysis. In
two-by-two zero-sum games  both steps (1) and (3) trivially extend for other variants of FTRL. As
we discuss further in Appendix I  the proof for (2) relies primarily on the strict convexity of the
regularizer h – a property shared by all variants of FTRL. For Gradient Descent  we make use of this
property by showing divergence increases as the strategies move from one pure strategy to another.
However  strategies will never reach the boundary for some variants of FTRL. For example  the
multiplicative weights update algorithm always selects fully-mixed strategies and (2) does not hold
exactly as written. Instead  for any   after a ﬁnite number of iterations all strategies will appear
within  of the boundary. This proof follows identically to the proof for Gradient Descent in Appendix
G. Moreover  the ﬁrst part of (2) extends to this settings; when one agent is more than  away from
the boundary  the divergence grows linearly. However  to prove that the divergence grows negligibly
when both players are within  of the boundary  we will have to carefully evolve  over time. This is
because for an algorithm like multiplicative weights  the convex conjugate h∗ is never linear; rather it
becomes arbitrarily close to a linear function as both agents come closer to playing a pure strategy.
Alternatively  (2) will more readily follow upon establishing a tighter understanding of the geometry
of learning dynamics.
For both higher dimensions and other variants of FTRL  this work provides evidence that regret grows
sublinearly when both agents are using ﬁxed step-size. More importantly  it establishes an outline
on the proof that relies on further developments in understanding the trajectories of online learning
dynamics.

6 Conclusion

We present the ﬁrst proof of sublinear regret for the most classic FTRL dynamic  online gradient
descent  in two-by-two zero-sum games. Our proof techniques leverage geometric information and
hinge upon the fact that FTRL dynamics  although are typically referred to as “converging" to Nash
equilibria in zero-sum games  diverge away from them. Our simulations further suggest that sublinear
regret bounds carry over to larger zero-sum games.

7 Acknowledgements

James P. Bailey and Georgios Piliouras acknowledge MOE AcRF Tier 2 Grant 2016-T2-1-170  grant
PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF-NRFF2018-07.

References
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In ACM

Conference on Economics and Computation  2018.

9

James P. Bailey and Georgios Piliouras. Multi-Agent Learning in Network Zero-Sum Games is a

Hamiltonian System. In AAMAS  2019.

James P. Bailey  Gauthier Gidel  and Georgios Piliouras. Finite regret and cycles with ﬁxed step-size

via alternating gradient descent-ascent  2019.

D. Balduzzi  S. Racaniere  J. Martens  J. Foerster  K. Tuyls  and T. Graepel. The Mechanics of

n-Player Differentiable Games. In ICML  2018.

David Balduzzi  Karl Tuyls  Julien Pérolat  and Thore Graepel. Re-evaluating evaluation. In NIPS 

2018.

David Balduzzi  Marta Garnelo  Yoram Bachrach  Wojciech M Czarnecki  Julien Perolat  Max
Jaderberg  and Thore Graepel. Open-ended learning in symmetric zero-sum games. arXiv preprint
arXiv:1901.08106  2019.

Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont  1999.

Nikolo Cesa-Bianchi and Gabor Lugoisi. Prediction  Learning  and Games. Cambridge University

Press  2006.

Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:

Chaos and butterﬂy effects of online learning in zero-sum games. In COLT  2019.

Thiparat Chotibut  Fryderyk Falniowski  Michal Misiurewicz  and Georgios Piliouras. Family of

chaotic maps from game theory. arXiv preprint arXiv:1807.06831  2018.

Thiparat Chotibut  Fryderyk Falniowski  Michał Misiurewicz  and Georgios Piliouras. The route
to chaos in routing games: Population increase drives period-doubling instability  chaos &amp;
inefﬁciency with Price of Anarchy equal to one. arXiv e-prints  art. arXiv:1906.02486  Jun 2019.

Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and con-
strained min-max optimization. In 10th Innovations in Theoretical Computer Science Conference 
ITCS 2019  January 10-12  2019  San Diego  California  USA  pages 27:1–27:18  2019. doi:
10.4230/LIPIcs.ITCS.2019.27. URL https://doi.org/10.4230/LIPIcs.ITCS.2019.27.

Constantinos Daskalakis  Alan Deckelbaum  and Anthony Kim. Near-optimal no-regret algorithms for
zero-sum games. In Proceedings of the Twenty-second Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA ’11  pages 235–254  Philadelphia  PA  USA  2011. Society for Industrial and
Applied Mathematics. URL http://dl.acm.org/citation.cfm?id=2133036.2133057.

Constantinos Daskalakis  Andrew Ilyas  Vasilis Syrgkanis  and Haoyang Zeng. Training gans with

optimism. In ICLR  2018.

Dylan J Foster  Thodoris Lykouris  Karthik Sridharan  and Eva Tardos. Learning in games: Robustness
of fast convergence. In Advances in Neural Information Processing Systems  pages 4727–4735 
2016.

Drew Fudenberg and David K. Levine. The Theory of Learning in Games. MIT Press Books. The

MIT Press  1998.

Gauthier Gidel  Hugo Berard  Gaetan Vignoud  Pascal Vincent  and Simon Lacoste-Julien. A

variational inequality perspective on generative adversarial networks. In ICLR  2019.

Sham M. Kakade  Shai Shalev-shwartz  and Ambuj Tewari. On the duality of strong convexity and

strong smoothness: Learning applications and matrix regularization  2009.

R. Kleinberg  K. Ligett  G. Piliouras  and É. Tardos. Beyond the Nash equilibrium barrier. In

Symposium on Innovations in Computer Science (ICS)  2011.

P. Mertikopoulos  H. Zenati  B. Lecouat  C.-S. Foo  V. Chandrasekhar  and G. Piliouras. Mirror

descent in saddle-point problems: Going the extra (gradient) mile. In ICLR  2019.

Panayotis Mertikopoulos  Christos Papadimitriou  and Georgios Piliouras. Cycles in adversarial

regularized learning. In ACM-SIAM Symposium on Discrete Algorithms  2018.

10

Sai Ganesh Nagarajan  Sameh Mohamed  and Georgios Piliouras. Three body problems in evo-
In Proceedings of the
lutionary game dynamics: Convergence  periodicity and limit cycles.
17th International Conference on Autonomous Agents and MultiAgent Systems  pages 685–693.
International Foundation for Autonomous Agents and Multi-agent Systems  2018.

Shayegan Omidshaﬁei  Christos Papadimitriou  Georgios Piliouras  Karl Tuyls  Mark Rowland 
Jean-Baptiste Lespiau  Wojciech M Czarnecki  Marc Lanctot  Julien Perolat  and Remi Munos.
α-rank: Multi-agent evaluation by evolution. arXiv preprint arXiv:1903.01373  2019.

Gerasimos Palaiopanos  Ioannis Panageas  and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence  limit cycles and chaos. In Advances in
Neural Information Processing Systems  pages 5872–5882  2017.

Marco Pangallo  James Sanders  Tobias Galla  and Doyne Farmer. A taxonomy of learning dynamics

in 2 x 2 games  2017.

Christos Papadimitriou and Georgios Piliouras. From nash equilibria to chain recurrent sets: An
algorithmic solution concept for game theory. Entropy  20(10)  2018. ISSN 1099-4300. doi:
10.3390/e20100782. URL http://www.mdpi.com/1099-4300/20/10/782.

Georgios Piliouras and Leonard J. Schulman. Learning dynamics and the co-evolution of competing

sexual species. In ITCS  2018.

Georgios Piliouras and Jeff S Shamma. Optimization despite chaos: Convex relaxations to complex
limit sets via poincaré recurrence. In Proceedings of the twenty-ﬁfth annual ACM-SIAM symposium
on Discrete algorithms  pages 861–873. SIAM  2014.

Georgios Piliouras  Carlos Nieto-Granda  Henrik I. Christensen  and Jeff S. Shamma. Persistent
patterns: Multi-agent learning beyond equilibrium and utility. In AAMAS  pages 181–188  2014.

Sasha Rakhlin and Karthik Sridharan. Optimization  learning  and games with predictable sequences.

In Advances in Neural Information Processing Systems  pages 3066–3074  2013.

Vasilis Syrgkanis  Alekh Agarwal  Haipeng Luo  and Robert E. Schapire. Fast convergence of
regularized learning in games. In Proceedings of the 28th International Conference on Neural
Information Processing Systems  NIPS’15  pages 2989–2997  Cambridge  MA  USA  2015. MIT
Press. URL http://dl.acm.org/citation.cfm?id=2969442.2969573.

Y. Yazıcı  C.-S. Foo  S. Winkler  K.-H. Yap  G. Piliouras  and V. Chandrasekhar. The Unusual

Effectiveness of Averaging in GAN Training. In ICLR  2019.

H Peyton Young. Strategic learning and its limits. Oxford Univ. Press  2004.

11

,James Bailey
Georgios Piliouras