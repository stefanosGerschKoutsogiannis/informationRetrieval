2018,Estimators for Multivariate Information Measures in General Probability Spaces,Information theoretic quantities play an important role in various settings in machine learning  including causality testing  structure inference in graphical models  time-series problems  feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof  including conditional mutual information  multivariate mutual information  total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces  existing estimators employ a $\Sigma H$ method  which can only work in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime.
In this paper  we define a general graph divergence measure ($\mathbb{GDM}$)  generalizing the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail  thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous \textit{mixtures} (3) the data is real-valued but does not have a joint density on the entire space  rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets.,Estimators for Multivariate Information Measures

in General Probability Spaces

Arman Rahimzamani
Department of ECE

University of Washington

armanrz@uw.edu

Himanshu Asnani
Department of ECE

University of Washington

asnani@uw.edu

Pramod Viswanath
Department of ECE

University of Illinois at Urbana-Champaign

pramodv@illinois.edu

Sreeram Kannan
Department of ECE

University of Washington

ksreeram@uw.edu

Abstract

Information theoretic quantities play an important role in various settings in ma-
chine learning  including causality testing  structure inference in graphical models 
time-series problems  feature selection as well as in providing privacy guarantees.
A key quantity of interest is the mutual information and generalizations thereof 
including conditional mutual information  multivariate mutual information  to-
tal correlation and directed information. While the aforementioned information
quantities are well deﬁned in arbitrary probability spaces  existing estimators add
or subtract entropies (we term them ΣH methods). These methods work only
in purely discrete space or purely continuous case since entropy (or differential
entropy) is well deﬁned only in that regime.
In this paper  we deﬁne a general graph divergence measure (GDM) as a measure
of incompatibility between the observed distribution and a given graphical model
structure. This generalizes the aforementioned information measures and we con-
struct a novel estimator via a coupling trick that directly estimates these multivariate
information measures using the Radon-Nikodym derivative. These estimators are
proven to be consistent in a general setting which includes several cases where the
existing estimators fail  thus providing the only known estimators for the following
settings: (1) the data has some discrete and some continuous valued components
(2) some (or all) of the components themselves are discrete-continuous mixtures (3)
the data is real-valued but does not have a joint density on the entire space  rather is
supported on a low-dimensional manifold. We show that our proposed estimators
signiﬁcantly outperform known estimators on synthetic and real datasets.

1

Introduction

Information theoretic quantities  such as mutual information and its generalizations  play an important
role in various settings in machine learning and statistical estimation and inference. Here we
summarize brieﬂy the role of some generalizations of mutual information in learning (cf. Sec. 2.1 for
a mathematical deﬁnition of these quantities).

1. Conditional mutual information measures the amount of information between two variables X
and Y given a third variable Z and is zero iff X is independent of Y given Z. CMI ﬁnds a wide

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

range of applications in learning including causality testing [1  2]  structure inference in graphical
models [3]  feature selection [4] as well as in providing privacy guarantees [5].

2. Total correlation measures the degree to which a set of N variables are independent of each other 
and appears as a natural metric of interest in several machine learning problems  for example  in
independent component analysis  the objective is to maximize the independence of the variables
quantiﬁed through total correlation [6]. In feature selection  ensuring the independence of selected
features is one goal  pursued using total correlation in [7  8].

3. Multivariate mutual information measures the amount of information shared between multiple

variables [9  10] and is useful in feature selection [11  12] and clustering [13].

4. Directed information measures the amount of information between two random processes [14 15]

and is shown as the correct metric in identifying time-series graphical models [16–21].

Estimation of these information-theoretic quantities from observed samples is a non-trivial problem
that needs to be solved in order to utilize these quantities in the aforementioned applications. While
there has been long history in estimation of entropy [22–25]  and renewed recent interest [26–28] 
much less effort has been spent on the multivariate versions. A standard approach to estimating
general information theoretic quantities is to write them out as a sum or difference of entropy (denoted
H usually) terms which are then directly estimated; we term such a paradigm as ΣH paradigm.
However  the ΣH paradigm is applicable only when the variables involved are all discrete or there
is a joint density on the space of all variables (in which case  differential entropy h can be utilized).
The underlying information measures themselves are well deﬁned in very general probability spaces 
for example  involving mixtures of discrete and continuous variables; however  no known estimators
exist.
We motivate the requirement of estimators in general probability spaces by some examples in
contemporary machine learning and statistical inference.

1. It is common place in machine learning to have data-sets where some variables are discrete 
and some are continuous. For example  in recent work on utilizing information bottleneck to
understand deep learning [29]  an important step is to quantify the mutual information between the
training samples (which are discrete) and the layer output (which is continuous). The employed
methodology was to quantize the continuous variables; this is common practice  even though
highly sub-optimal.

2. Some variables involved in the calculation may be mixtures of discrete and continuous vari-
ables. For example  the output of ReLU neuron will not have a density even when the input data
has a density. Instead  the neuron will have a discrete mass at 0 (or wherever the ReLU breakpoint
is) but will have a continuous distribution on the positive values. This is also the case in gene
expression data  where a gene may have a discrete mass at expression 0 due to an effect called
drop-out [30] but have a continuous distribution elsewhere.

3. The variables involved may have a joint density only on a low dimensional manifold. For
example  when calculating mutual information between input and output of a neural network 
some of the neurons are deterministic functions of the input variables and hence they will have a
joint density supported on a low-dimensional manifold rather than the entire space.

In the aforementioned cases  no existing estimators are known to work. It is not merely a matter of
having provable guarantees either. When we plug in estimators that assume a joint density into data
that does not  the estimated information measure can be strongly negative.
We summarize our main contributions below:

1. General paradigm (Section 2): We deﬁne a general paradigm of graph divergence measures
which captures the aforementioned generalizations of mutual information as special cases. Given a
directed acyclic graph (DAG) between n variables  the graph divergence is deﬁned as the Kullback-
Leibler (KL) divergence between the true data distribution PX and a restricted distribution PX
deﬁned on the Bayesian network and can be thought of as a measure of incompatibility with the
given graphical model structure. These graph divergence measures are deﬁned using the Radon
Nikodym derivatives which are well-deﬁned for general probability spaces.

2. Novel estimators (Section 3): We propose novel estimators for these graph divergence measures 
which directly estimate the corresponding Radon-Nikodym derivatives. To the best of our knowl-

2

(a)

(b)

(c)

Figure 1: (a) An example of Bayesian Network G with PX as the induced distribution PX1
PX4|X1 X2
(c) A Bayesian Network G with PX as the induced distribution PX1

PX6|X4. (b) A Bayesian Network G inducing a Markov chain PX3

PX2 ··· PXd.

PX5|X4

PX2
PX1|X3

PX3|X1
PX2|X3.

edge  these are the ﬁrst family of estimators that are well deﬁned for general probability spaces
(breaking the ΣH paradigm).

3. Consistency proofs (Section 4): We prove that the proposed estimators converge to the true value
of the corresponding graph divergence measures as the number of observed samples increases in a
general setting which includes several cases: (1) the data has some discrete and some continuous
valued components (2) some (or all) of the components themselves are discrete-continuous
mixtures (3) the data is real-valued but does not have a joint density on the entire space but is
supported on a low-dimensional manifold.

4. Numerical results (Section 5): Extensive numerical results demonstrate that (1) existing algo-
rithms have severe failure modes in general probability spaces (strongly negative values  for
example)  and (2) our proposed estimator achieves consistency as well as signiﬁcantly better
ﬁnite-sample performance.

2 Graph Divergence Measure

In this section  we deﬁne the family of graph divergence measures. To begin with  we ﬁrst deﬁne
some notational preliminaries. We denote any random variable by an uppercase letter such as X.
The sample space of the variable X is denoted by X and any value in X is denoted by the lowercase
letter x. For any subset A ⊆ X   the probability of A for a given distribution function PX (.) over X
is denoted by PX (A). We note that the random variable X can be a d-dimensional vector of random
variables  i.e. X ≡ (X1  . . .   Xd). The N observed samples drawn from the distribution PX are
denoted by x(1)  x(2)  . . .   x(N )  i.e. x(i) is the ith observed sample.
Sometimes we might be interested in a subset of components of a random variable  S ⊆
{X1  . . .   Xd} instead of the entire vector X. Accordingly  the sample space of the variable S
is denoted by S. For instance  X = (X1  X2  X3  X4) and S = (X1  X2). Throughout the entire
paper  unless otherwise stated  there is a one-to-one correspondence between the notations of X
and any S. For example for any value x ∈ X   the corresponding value in S is simply denoted by s.
Further  s(i) ∈ S represents the lower-dimensional sample corresponding to the ith observed sample
x(i) ∈ X . Furthermore  any marginal distribution deﬁned over S with respect to PX is denoted by
PS.
Consider a directed acyclic graph (DAG) G deﬁned over d nodes (corresponding to the d components
of the random variable X). A probability measure Q over X is said to be compatible with the graph
G if it is a Bayesian network on G. Given a graph G and a distribution PX  there is a natural measure
PX (.) which is compatible with the graph and is deﬁned as follows:

d(cid:89)

PX =

PXl|pa(Xl)

(1)

where pa(Xl) ⊂ X is the set of the parent nodes of the random variable Xl  with the sample space
denoted by Xpa(l)  and the sample values xpa(l) corresponding to x. The distribution PXl|pa(Xl) is
(cid:17)
the conditional distribution of Xl given pa(Xl). Throughout the paper  whenever mentioning the
.

variable Xl with its own parents pa(Xl) we indicate it by pa+(Xl)  i.e. pa+(Xl) ≡(cid:16)

Xl  pa(Xl)

l=1

An example is shown in Fig. 1a.

3

We note the fact that PS|X\S is well deﬁned for any subset of variables S ⊂ X. Therefore if we let
S = X \ pa(Xl)  then PX\pa(Xl)|pa(Xl) is well deﬁned for any l ∈ {1  . . .   d}. By marginalizing over
X \ pa+(Xl) we see that PXl|pa(Xl) and thus the distribution PX is well deﬁned.
The graph divergence measure is now deﬁned as a function of the probability measure PX and the
graph G. In this work we will focus only on the KL Divergence as being the distance metric  hence
unless otherwise stated D(· (cid:107) ·) = DKL(· (cid:107) ·). Let’s ﬁrst consider the case where PX is absolutely
continuous with respect to PX and hence the Radon-Nikodym derivative dPX /dPX exists. Therefore
for a given set of random variables X and a Bayesian Network G  we deﬁne Graph Divergence
Measure (GDM) as :

Here we implicitly assume that log(cid:0)dPX /dPX

GDM(X G) = D(PX(cid:107)PX ) =

(cid:1) is measurable and integrable with respect to the

measure PX. The GDM is set to inﬁnity wherever Radon-Nikodym derivative does not exist. It is
clear that GDM(X G) = 0 if and only if the data distribution is compatible with the graphical model 
thus the GDM can be thought of as a measure of incompatibility with the given graphical model
structure.
We now have relevant variational characterization as below on our graph divergence measure  which
can be harnessed to compute upper and lower bounds (More details in supplementary material):
Proposition 2.1. For a random variable X  a DAG G  let Π(G) be the set of measures QX deﬁned
on the Bayesian Network G  then GDM(X G) = infQX∈Π(G) D(PX(cid:107)QX ).
Furthermore  let C denote the set of functions h : X → R such that EQX [exp(h(X))] < ∞. If
GDM(X G) < ∞  then for every h ∈ C  EPX [h(X)] exists and:

dPX
dPX

log

X

dPX

(cid:90)

(2)

GDM(X G) = sup
h∈C

EPX [h(X)] − log EQX [exp(h(X))] .

(3)

2.1 Special cases
For speciﬁc choices of X and Bayesian Network  G  the Equation 2 is reduced to the well-known
information measures. Some examples of these measures are as follows:
Mutual Information (MI): X = (X1  X2) and G has no directed edge between X1 and X2. Thus
PX = PX1 .PX2  and we get  GDM(X G) = I(X1; X2) = D(PX1X2(cid:107)PX1
Conditional Mutual Information (CMI): We recover the conditional mutual information of X1
and X2 given X3 by constraining G to be the one in Fig. 1b  since PX = PX3.PX2|X3.PX1|X3  i.e. 
GDM(X G) = I(X1; X2|X3) = D(PX1X2X3(cid:107)PX1|X3
Total Correlation (TC): When X = (X1 ···   Xd)  and G is the graph with no edges (as in Fig. 1c 
we recover the total correlation of the random variables X1  . . .   Xd since PX = PX1 . . . PXd  i.e. 
GDM(X Gdc) = T C(X1  . . .   Xd) = D(PX1...Xd(cid:107)PX1 . . . PXd )

PX2|X3

PX2 ).

PX3).

Multivariate Mutual Information (MMI) : While the MMI deﬁned by [9] is not positive in gen-
eral there is a different deﬁnition by [10] which is both non-negative and has an operational interpre-
tation. Since MMI can be deﬁned as the optimal total correlation after clustering  we can utilize our
deﬁnition to deﬁne MMI (cf. supplementary material).
Directed Information : Suppose there are two stationary random processes X and Y   the directed
information rate from X to Y as ﬁrst introduced by Massey [31] is deﬁned as:

I(X → Y ) =

I(X → Y ) = GDM(cid:16)

T(cid:88)

t=1

1
T

(cid:12)(cid:12)Y t−1(cid:1)
I(cid:0)X t; Yt
(cid:17) − GDM(cid:16)

(cid:17)

It can be seen that the directed information can be written as:

(X T   Y T ) GI

(X T   Y T ) GC

where the graphical model GI correponds to the independent distribution between X T and Y T   and
GC corresponds to the causal distribution from X to Y (more details provided in supplementary
material).

4

3 Estimators

3.1 Prior Art

Estimators for entropy date back to Shannon  who guesstimated the entropy rate of English [32]. Dis-
crete entropy estimation is a well-studied topic and minimax rate of this problem is well-understood as
a function of the alphabet size [33–35]. The estimation of differential entropy is considerably harder
and also studied extensively in literature [23 25 26 36–39] and can be broadly divided into two groups;
based on either Kernel density estimates [40 41] or based on k-nearest-neighbor estimation [27 42 43].
In a remarkable work  Kozachenko and Leonenko suggested a nearest neighbor method for entropy
estimation [22] which was then generalized to a kth nearest neighbor approach [44]. In this method 
the distance to the kth nearest neighbor (KNN) is measured for each data-point  and based on this the
probability density around each data point is estimated and substituted into the entropy expression.
When k is ﬁxed  each density estimate is noisy and the estimator accrues a bias and a remarkable
result is that the bias is distribution-independent and can be subtracted out [45].
While the entropy estimation problem is well-studied  mutual information and its generalizations
are typically estimated using a sum of signed entropy (H) terms  which are estimated ﬁrst; we term
such estimators as ΣH estimators. In the discrete alphabet case  this principle has been shown to be
worst-case optimal [28]. In the case of distributions with a joint density  an estimator that breaks the
ΣH principle is the KSG estimator [46]  which builds on the KNN estimation paradigm but couples
the estimates in order to reduce the bias. This estimator is widely used and has excellent practical
performance. The original paper did not have any consistency guarantees and its convergence rates
were recently established [47]. There have been some extensions to the KSG estimator for other
information measures such as conditional mutual information [48  49]  directed information [50] but
none of them show theoretical guarantees on consistency of the estimators  furthermore they fail
completely in mixture distributions.
When the data distribution is neither discrete nor admits a joint density  the ΣH approach is no longer
feasible as the individual entropy terms are not well deﬁned. This is the regime of interest in our
paper. Recently  Gao et al [51] proposed a mutual-information estimator based on KNN principle 
which can handle such continuous-discrete mixture cases  and the consistency was demonstrated.
However it is not clear how it generalizes to even Conditional Mutual Information (CMI) estimation 
let alone other generalizations of mutual information. In this paper  we build on that estimator in
order to design an estimator for general graph divergence measures and establish its consistency for
generic probability spaces.

3.2 Proposed Estimator
The proposed estimator is given in Algorithm 1 where ψ(·) is the digamma function and 1{·} is the
indicator function. The process is schematically shown in Fig. 3 (cf. supplementary material). We
used the (cid:96)∞-norm everywhere in our algorithm and proofs.
The estimator intuitively estimates the GDM by the resubstitution estimate 1
i=1 log ˆf (x(i)) in
which ˆf (x(i)) is the estimation of Radon-Nikodym derivative at each sample x(i). If x(i) lies in a
region where there is a density  the RN derivative is equal to gX (x(i))/¯gX (x(i)) in which gX (.) and
¯gX (.) are density functions corresponding to PX and PX respectively. On the other hand  if x(i) lies
on a point where there is a discrete mass  the RN derivative will be equal to hX (x(i))/¯hX (x(i)) in
which hX (.) and ¯hX (.) are mass functions corresponding to PX and PX respectively.

(cid:80)N

N

The density function ¯gX (x(i)) can be written as (cid:81)d
(cid:81)d

(i))(cid:1)
(i))(cid:1). Thus we need to estimate the density functions g(.)

(cid:0)gpa+(Xl)(xpa+(l)

the mass function ¯hX (x(i)) can be written as

(cid:0)hpa+(Xl)(xpa+(l)

(i))/hpa(Xl)(xpa(l)

for continuous components.

Equivalently 

l=1

(i))/gpa(Xl)(xpa(l)

l=1

and the mass functions h(.) according to the type of x(i). The existing kth nearest neighbor algorithms
will suffer while estimating the mass functions h(.)  since ρnS  i (the distance to the nS-th nearest
neighbor in subspace S) for such points will be equal to zero for large N. Our algorithm  however  is
designed in a way that it’s capable of approximating both g(.) functions as ≈ nS
(ρnS  i)dS and h(.)
functions as ≈ nS
N dynamically for any subset S ⊆ X. This is achieved by setting ρnS  i terms such
that all of them cancel out  yielding the estimator as in Eq. (4).

N

1

5

.
Input: Parameter: k ∈ Z+  Samples: x(1)  x(2)  . . .   x(N )  Bayesian Network: G on Variables:

X = (X1  X2 ···   Xd)
Output: (cid:92)GDM(N )
(X G)
1: for i = 1 to N do
2:
3:
4:
5:
6:

7:
8:
9:
10: end for
11: Final Estimator:

Query:
ρk i = (cid:96)∞-distance to the kth nearest neighbor of x(i) in the space X
Inquire:
˜ki = # points within the ρk i-neighborhood of x(i) in the space X
pa(Xl) = # points within the ρk i-neighborhood of x(i) in the space Xpa(l)
n(i)
pa+(Xl) = # points within the ρk i-neighborhood of x(i) in the space Xpa+(l)
(cid:17)(cid:17)
ζi = ψ(˜ki) +(cid:80)d
n(i)
Compute:

(cid:17) − log

1{pa(Xl)(cid:54)=∅} log

n(i)
pa+(Xl) + 1

n(i)
pa(Xl) + 1

(cid:16)

(cid:16)

l=1

(cid:16)
N(cid:88)

(cid:92)GDM(N )

(X G) =

1
N

ζi +

i=1

l=1

(cid:32) d(cid:88)

(cid:33)
1{pa(Xl)=∅} − 1

log N

(4)

Algorithm 1: Estimating Graph Divergence Measure GDM(X G)

4 Proof of Consistency

The proof of consistency for our estimator consists of two steps: First we prove that the expected
value of the estimator in Eq. (4) converges to the true value as N → ∞   and second we prove that the
variance of the estimator converges to zero as N → ∞. Let’s begin with the deﬁnition of PX (x  r):

(cid:8)a ∈ X : (cid:107)a − x(cid:107)∞ ≤ r(cid:9) = PX

(cid:110)

Br(x)

(cid:111)

PX (x  r) = PX

(5)

Thus PX (x  r) is the probability of a hypercube with the edge length of 2r centered at the point x.
We then state the following assmuptions:
Assumption 1. We make the following assumptions to prove the consistency of our method:

1. k is set such that limN→∞ k = ∞ and limN→∞ k log N
2. The set of discrete points {x : PX (x  0) > 0} is ﬁnite.

(cid:12)(cid:12) log f (x)(cid:12)(cid:12)dPX < +∞  where f ≡ dPX /dPX is Radon-Nikodym derivative.

3. (cid:82)

N = 0.

X

The Assumption 1.1 with 1.2 controls the boundary effect between the continuous and the discrete
regions; with this assumption we make sure that all the k nearest neighbors of each point belong
to the same region almost surely (i.e. all of them are either continuous or discrete). Assumption
1.3 is the log-integrability of the Radon-Nikodym derivative. These assumptions are satisﬁed under
mild technical conditions whenever the distribution PX over the set X is (1) ﬁnitely discrete; (2)
continuous; (3) ﬁnitely discrete over some dimensions and continuous over some others; (4) a mixture
of the previous cases; (5) has a joint density supported over a lower dimensional manifold. These
cases represent almost all the real world data.
As an example of a case not conforming to the aforementioned cases  we can consider singular
distributions  among which the Cantor distribution is a signiﬁcant example whose cumulative
distribution function is the Cantor function. This distribution has neither a probability density
function nor a probability mass function  although its cumulative distribution function is a continuous
function. It is thus neither a discrete nor an absolutely continuous probability distribution  nor is it a
mixture of these.
The Theorem 1 formally states the mean-convergence of the estimator while Theorem 2 formally
states that convergence of the variance to zero.

6

Theorem 1. Under the Assumptions 1  we have limN→∞ E
Theorem 2. In addition to the Assumptions 1  assume that we have (kN log N )2/N → 0 as N goes
to inﬁnity. Then we have limN→∞ Var

(X G)

= GDM(X G).

(X G)

(cid:21)

= 0.

(cid:20)(cid:92)GDM(N )

(cid:21)

(cid:20)(cid:92)GDM(N )
we need to upper-bound the term(cid:12)(cid:12)E(cid:2)(cid:92)GDM(N )

The Theorems 1 and 2 combined yield the consistency of the estimator 4.
The proof of the Theorem 1 starts with writing the Radon-Nikodym derivative explicitly. Then

(X G)(cid:3) − GDM(X G)(cid:12)(cid:12). To achieve this goal  we

segregate the domain of X into three parts as X = Ω1 ∪ Ω2 ∪ Ω3 where Ω1 = {x : f (x) = 0} 
Ω2 = {x : f (x) > 0  PX (x  0) > 0} and Ω3 = {x : f (x) > 0  PX (x  0) = 0}. We will show that
PX (Ω1) = 0. The sets Ω2 and Ω3 correspond to the discrete and continuous regions respectively.
Then for each of the two regions  we introduce an upperbound which goes to zero as N grows
boundlessly. Thus equivalently we show the mean of the estimate ζ1 is close to log f (x) for any x.
The proof of the Theorem 2 is based on the Efron-Stein inequality  which upperbounds any estimator
for any quantity from the observed samples x(1)  . . .   x(N ). For any sample x(i)  we then upperbound

the term(cid:12)(cid:12)ζi(X) − ζi(X\j)(cid:12)(cid:12) by segregating the samples into various cases  and examining each case

separately. ζi(X) is the estimate using all the samples x(1)  . . .   x(N ) and ζi(X\j) is the estimate
when the jth sample is removed. Summing up over all the i’s  we obtain an upper-bound which will
converge to 0 as N goes to inﬁnity.

5 Empirical Results

In this section  we evaluate the performance of our proposed estimator in comparison with other
estimators via numerical experiments. The estimators evaluated here are our estimator referred to as
GDM  the plain KSG-based estimators for continuous distributions to which we refer by KSG  the
binning estimators and the noise-induced ΣH estimators. A more detailed discussion can be found in
Section G.
Experiment 1: Markov chain model with continuous-discrete mixture. For the ﬁrst experiment 
we simulated an X-Z-Y Markov chain model in which the random variable X is a uniform random
variable U(0  1) clipped at a threshold 0 < α1 < 1 from above. Then Z = min (X  α2) and
Y = min (Z  α3) in which 0 < α3 < α2 < α1. We simulated this system for various numbers of
samples  setting α1 = 0.9  α2 = 0.8 and α3 = 0.7. For each set of samples we estimated I(X; Y |Z)
via different methods. The theory value for I(X; Y |Z) is 0. The results are shown in Figure 2a. We
can see that in this regime  only the GDM estimator can correctly converge. The KSG estimator and
the ΣH estimator show high negative biases and the binning estimator shows a positive bias.
Experiment 2: Mixture of AWGN and BSC channels with variable error probability. For the
second scheme of our experiments  we considered an Additive White Gaussian Noise (AWGN)
Channel in parallel with a Binary Symmetric Channel (BSC) where only one of the two can be
activated at a time. The random variable Z = min(α  ˜Z) where ˜Z ∼ U (0  1) controls which channel
is activated; i.e. if Z is lower than the threshold β  activate the AWGN channel  otherwise initiate
the BSC channel where Z also determines the error probability at each time point. We set α = 0.3 
β = 0.2  BSC channel input as X ∼ Bern(0.5)  and AWGN input and noise deviation as σX = 1
and σN = 0.1 respectively  and obtained the estimates of I(X; Y |Z  Z 2  Z 3) for various estimators.
While the theory value is equal to I(X; Y |Z) = 0.53241  yet it’s conditioned over a low-dimensional
manifold in a high-dimensional space. The results are shown in Figure 2b. Similar to the previous
experiment  the GDM estimator can correctly converge to the true value. The ΣH and binning
estimators show a negative bias  and the KSG estimator gets totally lost.
Experiment 3: Total Correlation for independent mixtures. In this experiment  we estimate
the total correlation of three independent variables X  Y and Z. The samples for the variable X
are generated in the following fashion: First toss a fair coin  if heads appears we ﬁx X at αX 
otherwise we draw X from a uniform distribution between 0 and 1. samples from Y and Z are also
generated in the same way independently with parameters αY and αZ respectively. For this setup 
T C(X  Y  Z) = 0. We set αX = 1  αY = 1/2 and αZ = 1/4  and generated various datasets with
different lengths. Then estimated total correlation values are shown in the Figure 2c.

7

Experiment 4: Total Correlation for independent uniforms with correlated zero-inﬂation. Here
we ﬁrst consider four auxiliary uniform variables ˜X1  ˜X2  ˜X3 and ˜X4 which are taken from
U(0.5  1.5). Then each sample is erased with a Bernoulli probability; i.e. X1 = α1 ˜X1  X2 = α1 ˜X2
and X3 = α2 ˜X3  X4 = α2 ˜X4 in which α1 ∼ Bern(p1) and α2 ∼ Bern(p2). As we see  after
zero-inﬂation X1 and X2 become correlated  so do X3 and X4 while still (X1  X2)|=(X3  X4). In
the experiment  we set p1 = p2 = 0.6. The results of running different algorithms over the data can
be seen in Figure 2d. For the total correlation experiments 3 and 4  similar to that of conditional
mutual information in experiments 1 and 2  only the GDM estimator can best estimate the true value.
The estimator ΣH was removed from the ﬁgures due to its high inaccuracy.
Experiment 5: Gene Regulatory Networks. In this experiment we use different estimators to
do Gene Regulatory Network inference based on the conditional Restricted Directed Information
(cRDI) [20]. We do our test on the simulated neuron cells’ development process  based on a model
from [52]. In this model  the time series vector X consists of 13 random variables each of which
corresponding to a single gene in the development process. We simulated the development process for
various lengths of time-series in which the noise N ∼ N (0  .03) is added for all the genes  and every
single sample is then subject to erasure (i.e. be replaced by 0s) with a probability of 0.5. Then we
applied the cRDI method utilizing various CMI estimators and then calculated the Area-Under-ROC
curve (AUROC). The results are shown in Figure 2e. It’s seen that the cRDI method implemented
with the GDM estimator outperform the other estimators by at least %10 in terms of AUROC. In the
tests  cRDI for each (Xi  Xj) is conditioned over the node k (cid:54)= i with the highest RDI value to j. We
notice that the causal signals are highly destroyed due to the zero-inﬂation  so we won’t expect high
performance of the causal inference over the data. We did not include the ΣH estimator results due
to its very low performance.
Experiment 6: Feature Selection by Conditional Mutual Information Maximization. Feature
selection is an important pre-processing step in many learning tasks. The application of information
theoretic measures in feature selection is well studied in the literature [7]. Among the well-known
methods is the conditional mutual information maximization (CMIM) ﬁrst introduced by Flueret [4] 
a variation of which was later introduced called CMIM-2 [53]. Both methods use conditional mutual
information as their core measure to select the features. Hence the performance of the estimators can
signiﬁcantly inﬂuence the performance of the methods. In our experiment  we generated a vector
X = (X1  . . .   X15) of 15 random variables in which all the random variables are taken from N (0  1)
and then each random variable Xi is clipped from above at αi which is initially taken randomly
from U(0.25  0.3) and then kept constant during the sample generation. Then Y is generated as

(cid:1). Then we did the CMIM-2 algorithm with various CMI estimators to evaluate

the performance of the estimators in extracting the relevant features X1  . . .   X5. The AUROC values
for each algorithm versus the number of samples generated are shown in Figure 2f. The feature
selection methods implemented with the GDM estimator outperform the other estimators.

Y = cos(cid:0)(cid:80)5

i=1 Xi

6 Discussion and Future Work

A general paradigm of graph divergence measures and novel estimators thereof  for general probability
spaces are proposed  which estimate several generalizations of mutual information. In future  we
would like to derive further efﬁcient estimators for high dimensional data. In the current work 
estimators are shown to be consistent with inﬁnite scaling of parameter k. In future  we would like to
understand the ﬁnite k performance of the estimators as well as guarantees on sample complexity and
rates of convergence. Another potential direction to follow is to study the variational characterization
of the graph divergence measure to design estimators. Improving the computational efﬁciency of
the estimator is another direction of future work. Recent literature including [54] provide a new
methodology to estimate mutual information in a computationally efﬁcient manner and leveraging
these ideas for the generalized measures and general proabability distributions can be a promising
direction ahead.

7 Acknowledgement

This work was partially supported by NSF grants 1651236  1703403 and NIH grant 5R01HG008164.
The authors also would like to thank Yihan Jiang for presenting our work at the NeurIPS conference.

8

(a)

(c)

(e)

(b)

(d)

(f)

Figure 2: The results for the experiments versus the number of samples: 2a: The estimated CMI for
the X-Z-Y Markov chain. 2b: CMI for the AWGN+BSC channels with low-dimensional Z manifold.
2c: The estimated TC values for three independent variables. 2d: The estimated TC for zero-inﬂated
variables. 2e: The AUROC values for gene regulatory network inference. The error bars show the
standard deviation scaled down by 0.2. 2f: The AUROC values for feature selection accuracy. The
error bars show the standard deviations scaled down by 0.2.

9

01000020000300004000050000Number of samples1.00.80.60.40.20.0CMI valuesGDMKSG-continuousSigmaHBinningTheory02000400060008000100001200014000Number of samples321012CMI valuesGDMKSG-continuousSigmaHBinningTheory0200040006000800010000Number of samples0.80.60.40.20.0TC valuesGDMKSG-continuousBinningTheory0200040006000800010000Number of samples0.40.60.81.01.21.4TC valuesGDMKSG-continuousBinningTheory2004006008001000Number of Samples0.500.550.600.650.700.750.80AUC for different methodscRDI - GDMcRDI - KSGcRDI - Binning100200300400500Number of Samples0.60.70.80.91.0AUC for different methodsCMIM2 - GDMCMIM2 - KSGCMIM2 - BinningCMIM2 - SigmaHReferences

[1] A. P. Dawid  “Conditional independence in statistical theory ” Journal of the Royal Statistical

Society. Series B (Methodological)  pp. 1–31  1979.

[2] K. Zhang  J. Peters  D. Janzing  and B. Schölkopf  “Kernel-based conditional independence test

and application in causal discovery ” arXiv preprint arXiv:1202.3775  2012.

[3] J. Whittaker  Graphical models in applied multivariate statistics. Wiley Publishing  2009.
[4] F. Fleuret  “Fast binary feature selection with conditional mutual information ” Journal of

Machine Learning Research  vol. 5  no. Nov  pp. 1531–1555  2004.

[5] P. Cuff and L. Yu  “Differential privacy as a mutual information constraint ” in Proceedings of
the 2016 ACM SIGSAC Conference on Computer and Communications Security  pp. 43–54 
ACM  2016.

[6] A. Hyvärinen and E. Oja  “Independent component analysis: algorithms and applications ”

Neural networks  vol. 13  no. 4-5  pp. 411–430  2000.

[7] J. R. Vergara and P. A. Estévez  “A review of feature selection methods based on mutual

information ” Neural computing and applications  vol. 24  no. 1  pp. 175–186  2014.

[8] P. E. Meyer  C. Schretter  and G. Bontempi  “Information-theoretic feature selection in microar-
ray data using variable complementarity ” IEEE Journal of Selected Topics in Signal Processing 
vol. 2  no. 3  pp. 261–274  2008.

[9] W. McGill  “Multivariate information transmission ” Transactions of the IRE Professional

Group on Information Theory  vol. 4  no. 4  pp. 93–111  1954.

[10] C. Chan  A. Al-Bashabsheh  J. B. Ebrahimi  T. Kaced  and T. Liu  “Multivariate mutual
information inspired by secret-key agreement ” Proceedings of the IEEE  vol. 103  no. 10 
pp. 1883–1913  2015.

[11] J. Lee and D.-W. Kim  “Feature selection for multi-label classiﬁcation using multivariate mutual

information ” Pattern Recognition Letters  vol. 34  no. 3  pp. 349–357  2013.

[12] G. Brown  “A new perspective for information theoretic feature selection ” in Artiﬁcial Intelli-

gence and Statistics  pp. 49–56  2009.

[13] C. Chan  A. Al-Bashabsheh  Q. Zhou  T. Kaced  and T. Liu  “Info-clustering: A mathematical
theory for data clustering ” IEEE Transactions on Molecular  Biological and Multi-Scale
Communications  vol. 2  no. 1  pp. 64–91  2016.

[14] S. Watanabe  “Information theoretical analysis of multivariate correlation ” IBM Journal of

research and development  vol. 4  no. 1  pp. 66–82  1960.

[15] H. H. Permuter  Y.-H. Kim  and T. Weissman  “Interpretations of directed information in
portfolio theory  data compression  and hypothesis testing ” IEEE Transactions on Information
Theory  vol. 57  no. 6  pp. 3248–3259  2011.

[16] C. J. Quinn  N. Kiyavash  and T. P. Coleman  “Directed information graphs ” IEEE Transactions

on information theory  vol. 61  no. 12  pp. 6887–6909  2015.

[17] J. Sun  D. Taylor  and E. M. Bollt  “Causal network inference by optimal causation entropy ”

SIAM Journal on Applied Dynamical Systems  vol. 14  no. 1  pp. 73–106  2015.

[18] K. Hlaváˇcková-Schindler  M. Paluš  M. Vejmelka  and J. Bhattacharya  “Causality detection
based on information-theoretic approaches in time series analysis ” Physics Reports  vol. 441 
no. 1  pp. 1–46  2007.

[19] P.-O. Amblard and O. J. Michel  “On directed information theory and granger causality graphs ”

Journal of computational neuroscience  vol. 30  no. 1  pp. 7–16  2011.

[20] A. Rahimzamani and S. Kannan  “Network inference using directed information: The determin-
istic limit ” in Communication  Control  and Computing (Allerton)  2016 54th Annual Allerton
Conference on  pp. 156–163  IEEE  2016.

[21] A. Rahimzamani and S. Kannan  “Potential conditional mutual information: Estimators and
properties ” in Communication  Control  and Computing (Allerton)  2017 55th Annual Allerton
Conference on  pp. 1228–1235  IEEE  2017.

10

[22] L. Kozachenko and N. N. Leonenko  “Sample estimate of the entropy of a random vector ”

Problemy Peredachi Informatsii  vol. 23  no. 2  pp. 9–16  1987.

[23] J. Beirlant  E. J. Dudewicz  L. Györﬁ  and E. C. Van der Meulen  “Nonparametric entropy
estimation: An overview ” International Journal of Mathematical and Statistical Sciences 
vol. 6  no. 1  pp. 17–39  1997.

[24] R. Wieczorkowski and P. Grzegorzewski  “Entropy estimators-improvements and comparisons ”
Communications in Statistics-Simulation and Computation  vol. 28  no. 2  pp. 541–567  1999.
[25] E. G. Miller  “A new class of entropy estimators for multi-dimensional densities ” in Acoustics 
Speech  and Signal Processing  2003. Proceedings.(ICASSP’03). 2003 IEEE International
Conference on  vol. 3  pp. III–297  IEEE  2003.

[26] I. Lee  “Sample-spacings-based density and entropy estimators for spherically invariant multidi-

mensional data ” Neural Computation  vol. 22  no. 8  pp. 2208–2227  2010.

[27] K. Sricharan  D. Wei  and A. O. Hero  “Ensemble estimators for multivariate entropy estimation ”

IEEE transactions on information theory  vol. 59  no. 7  pp. 4374–4388  2013.

[28] Y. Han  J. Jiao  and T. Weissman  “Adaptive estimation of shannon entropy ” in Information

Theory (ISIT)  2015 IEEE International Symposium on  pp. 1372–1376  IEEE  2015.

[29] N. Tishby and N. Zaslavsky  “Deep learning and the information bottleneck principle ” in

Information Theory Workshop (ITW)  2015 IEEE  pp. 1–5  IEEE  2015.

[30] S. Liu and C. Trapnell  “Single-cell transcriptome sequencing: recent advances and remaining

challenges ” F1000Research  vol. 5  2016.

[31] J. Massey  “Causality  feedback and directed information ” in Proc. Int. Symp. Inf. Theory

Applic.(ISITA-90)  pp. 303–305  1990.

[32] C. E. Shannon  “Prediction and entropy of printed english ” Bell Labs Technical Journal  vol. 30 

no. 1  pp. 50–64  1951.

[33] L. Paninski  “Estimation of entropy and mutual information ” Neural computation  vol. 15 

no. 6  pp. 1191–1253  2003.

[34] J. Jiao  K. Venkat  Y. Han  and T. Weissman  “Minimax estimation of functionals of discrete
distributions ” IEEE Transactions on Information Theory  vol. 61  no. 5  pp. 2835–2885  2015.
[35] Y. Wu and P. Yang  “Minimax rates of entropy estimation on large alphabets via best polynomial
approximation ” IEEE Transactions on Information Theory  vol. 62  no. 6  pp. 3702–3720 
2016.

[36] I. Nemenman  F. Shafee  and W. Bialek  “Entropy and inference  revisited ” in Advances in

neural information processing systems  pp. 471–478  2002.

[37] M. Le´sniewicz  “Expected entropy as a measure and criterion of randomness of binary se-

quences ” Przegl ˛ad Elektrotechniczny  vol. 90  no. 1  pp. 42–46  2014.

[38] K. Sricharan  R. Raich  and A. O. Hero  “Estimation of nonlinear functionals of densities with
conﬁdence ” IEEE Transactions on Information Theory  vol. 58  no. 7  pp. 4135–4159  2012.
[39] S. Singh and B. Póczos  “Exponential concentration of a density functional estimator ” in

Advances in Neural Information Processing Systems  pp. 3032–3040  2014.

[40] K. Kandasamy  A. Krishnamurthy  B. Poczos  L. Wasserman  et al.  “Nonparametric von
mises estimators for entropies  divergences and mutual informations ” in Advances in Neural
Information Processing Systems  pp. 397–405  2015.

[41] W. Gao  S. Oh  and P. Viswanath  “Breaking the bandwidth barrier: Geometrical adaptive
entropy estimation ” in Advances in Neural Information Processing Systems  pp. 2460–2468 
2016.

[42] J. Jiao  W. Gao  and Y. Han  “The nearest neighbor information estimator is adaptively near

minimax rate-optimal ” arXiv preprint arXiv:1711.08824  2017.

[43] D. Pál  B. Póczos  and C. Szepesvári  “Estimation of rényi entropy and mutual information
based on generalized nearest-neighbor graphs ” in Advances in Neural Information Processing
Systems  pp. 1849–1857  2010.

11

[44] H. Singh  N. Misra  V. Hnizdo  A. Fedorowicz  and E. Demchuk  “Nearest neighbor estimates
of entropy ” American journal of mathematical and management sciences  vol. 23  no. 3-4 
pp. 301–321  2003.

[45] S. Singh and B. Póczos  “Finite-sample analysis of ﬁxed-k nearest neighbor density functional

estimators ” in Advances in Neural Information Processing Systems  pp. 1217–1225  2016.

[46] A. Kraskov  H. Stögbauer  and P. Grassberger  “Estimating mutual information ” Physical review

E  vol. 69  no. 6  p. 066138  2004.

[47] W. Gao  S. Oh  and P. Viswanath  “Demystifying ﬁxed k-nearest neighbor information estima-

tors ” IEEE Transactions on Information Theory  pp. 1–1  2018.

[48] J. Runge  “Conditional independence testing based on a nearest-neighbor estimator of condi-

tional mutual information ” arXiv preprint arXiv:1709.01447  2017.

[49] S. Frenzel and B. Pompe  “Partial mutual information for coupling analysis of multivariate time

series ” Physical review letters  vol. 99  no. 20  p. 204101  2007.

[50] M. Vejmelka and M. Paluš  “Inferring the directionality of coupling with conditional mutual

information ” Physical Review E  vol. 77  no. 2  p. 026214  2008.

[51] W. Gao  S. Kannan  S. Oh  and P. Viswanath  “Estimating mutual information for discrete-
continuous mixtures ” in Advances in Neural Information Processing Systems  pp. 5988–5999 
2017.

[52] X. Qiu  S. Ding  and T. Shi  “From understanding the development landscape of the canonical
fate-switch pair to constructing a dynamic landscape for two-step neural differentiation ” PloS
one  vol. 7  no. 12  p. e49271  2012.

[53] J. R. Vergara and P. A. Estévez  “Cmim-2: an enhanced conditional mutual information maxi-
mization criterion for feature selection ” Journal of Applied Computer Science Methods  vol. 2 
2010.

[54] M. Noshad and A. O. Hero III  “Scalable hash-based estimation of divergence measures ” arXiv

preprint arXiv:1801.00398  2018.

[55] Y. Wu  “Lecture notes in information theory ” www.stat.yale.edu/ yw562/teaching/itlectures.pdf.
[56] J. M. Bernardo  “Algorithm as 103: Psi (digamma) function ” Journal of the Royal Statistical

Society. Series C (Applied Statistics)  vol. 25  no. 3  pp. 315–317  1976.

[57] L. Evans  Measure theory and ﬁne properties of functions. Routledge  2018.

12

,Arman Rahimzamani
Himanshu Asnani
Pramod Viswanath
Sreeram Kannan