2011,A Model for Temporal Dependencies in Event Streams,We introduce the Piecewise-Constant Conditional Intensity Model  a model for learning temporal dependencies in event streams.  We describe a closed-form Bayesian approach to learning these models  and describe an importance sampling algorithm for forecasting future events using these models  using a proposal distribution based on Poisson superposition.  We then use synthetic data  supercomputer event logs  and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies  and that our importance sampling algorithm can effectively forecast future events.,A Model for Temporal Dependencies

in Event Streams

Asela Gunawardana
Microsoft Research
One Microsoft Way
Redmond  WA 98052

aselag@microsoft.com

Christopher Meek
Microsoft Research
One Microsoft Way
Redmond  WA 98052

meek@microsoft.com

Puyang Xu

ECE Dept. & CLSP

Johns Hopkins University

Baltimore  MD 21218
puyangxu@jhu.edu

Abstract

We introduce the Piecewise-Constant Conditional Intensity Model  a model for
learning temporal dependencies in event streams. We describe a closed-form
Bayesian approach to learning these models  and describe an importance sampling
algorithm for forecasting future events using these models  using a proposal distri-
bution based on Poisson superposition. We then use synthetic data  supercomputer
event logs  and web search query logs to illustrate that our learning algorithm can
efﬁciently learn nonlinear temporal dependencies  and that our importance sam-
pling algorithm can effectively forecast future events.

1

Introduction

The problem of modeling temporal dependencies in temporal streams of discrete events arises in
a wide variety of applications. For example  system error logs [14]  web search query logs  the
ﬁring patterns of neurons [18] and gene expression data [8]  can all be viewed as streams of events
over time. Events carry both information about their timing and their type (e.g.  the web query
issued or the type of error logged)  and the dependencies between events can be due to both their
timing and their types. Modeling these dependencies is valuable for forecasting future events in
applications such as system failure prediction for preemptive maintenance or forecasting web users’
future interests for targeted advertising.
We introduce the Piecewise-Constant Conditional Intensity Model (PCIM)  which is a class of
marked point processes [4] that can model the types and timing of events. This model captures
the dependencies of each type of event on events in the past through a set of piecewise-constant
conditional intensity functions. We use decision trees to represent these dependencies and give a
conjugate prior for this model  allowing for closed-form computation of the marginal likelihood and
parameter posteriors. Model selection then becomes a problem of choosing a decision tree. Decision
tree induction can be done efﬁciently because of the closed form for the marginal likelihood. Fore-
casting can be carried out using forward sampling for arbitrary ﬁnite duration queries. For episodic
sequence queries  that is  queries that specify particular sequences of events in given future time
intervals  we develop a novel approach for estimating the probability of rare queries  which we call
the Poisson Superposition Importance Sampler (PSIS).
We validate our learning and inference procedures empirically. Using synthetic data we show that
PCIMs can correctly learn the underlying dependency structure of event streams  and that the PSIS
leads to effective forecasting. We then use real supercomputer event log data to show that PCIMs
can be learned more than an order of magnitude faster than Poisson Networks [15  18]  and that they
have better test set likelihood. Finally  we show that PCIMs and the PSIS are useful in forecasting
future interests of real web search users.

1

2 Related Work

While graphical models such as Bayesian networks [2] and dependency networks [10] are widely
used to model the dependencies between variables  they do not model temporal dependencies (see
e.g.  [8]). Dynamic Bayesian Networks (DBN) [5  9] allow modeling of temporal dependencies in
discrete time. It is not clear how timestamps in our data should be discretized in order to apply the
DBN approach. At a minimum  too slow a sampling rate results in poor representation of the data 
and too fast a sampling rate increases the number of samples making learning and inference more
costly. In addition  allowing long term dependencies requires conditioning on multiple steps into
the past  and choosing too fast a sampling rate increases the number of such steps that need to be
conditioned on.
Recent progress in modeling continuous time processes include Continuous Time Bayesian Net-
works (CTBNs) [12  13]  Continuous Time Noisy-Or (CT-NOR) [16]  Poisson Cascades [17]  and
Poisson Networks [15  18]. CTBNs are homogeneous Markov models of the joint trajectories of
discrete ﬁnite variables  rather than models of event streams in continuous time [15]. In contrast 
CT-NOR and Poisson Cascades model event streams  but require the modeler to choose a parametric
form for temporal dependencies. Simma et al [16  17] describe how this choice signiﬁcantly impacts
model performance  and depends strongly on the domain. In particular  the problem of model selec-
tion for CT-NOR and Poisson Cascades is unaddressed. PCIMs  in contrast to CT-NOR and Poisson
Cascades  perform structure learning to learn how different events in the past affect future events.
Poisson Networks  described in more detail below  are closely related to PCIMs  but PCIMs are over
an order of magnitude faster to learn and can model nonlinear temporal dependencies.

3 Conditional Intensity Models

In this section  we deﬁne Conditional Intensity Models  introduce the class of Piecewise-Constant
Conditional Intensity Models  and describe Poisson Networks. We assume that events of dif-
ferent types are distinguished by labels l drawn from a ﬁnite set L. An event is then com-
posed of a non-negative time-stamp t and a label l. An event sequence x = {(ti  li)}n
where 0 < t1 < ··· < tn. The history at time t of event sequence x is the sub-sequence
h(t  x) = {(ti  li) | (ti  li) ∈ x  ti ≤ t}. We write hi for h(ti−1  x) when it is clear from context
which x is meant. By convention t0 = 0. We deﬁne the ending time t(x) of an event sequence x as
the time of the last event in x: t(x) = max ({t : (t  l) ∈ x}) so that t(hi) = ti−1.
A Conditional Intensity Model (CIM) is a set of non-negative conditional intensity functions indexed
by label {λl(t|x; θ)}l∈L. The data likelihood for this model is

i=1

λl(ti|hi  θ)1l(li)e−Λl(ti|hi;θ)

(1)

l∈L

i=1

−∞ λl(τ|x; θ)dτ for each event sequence x and the indicator function 1l(l0) is
one if l0 = l and zero otherwise. The conditional intensities are assumed to satisfy λl(t|x; θ) = 0 for
t ≤ t(x) to ensure that ti > ti−1 = t(hi). These modeling assumptions are quite weak. In fact  any
distribution for x in which the timestamps are continuous random variables can be written in this
form. For more details see [4  6]. Despite the fact that the modeling assumptions are weak  these
models offer a powerful approach for decomposing the dependencies of different event types on the
past. In particular  this per-label conditional factorization allows one to model detailed label-speciﬁc
dependence on past events.

p(x|θ) =Y

nY

where Λl(t|x; θ) =R t

3.1 Piecewise-Constant Conditional Intensity Models

Piecewise-Constant Conditional Intensity Models (PCIMs) are Conditional Intensity Models where
the conditional intensity functions are assumed to be piecewise-constant. As described below  this
assumption allows efﬁcient learning and inference. PCIMs are deﬁned in terms of local structures
Sl for each label l  which specify regions in time where the corresponding conditional intensity
function is constant  and local parameters θl for each label which specify the values taken in those
regions. Piecewise-Constant Conditional Intensity Models (PCIMs) are deﬁned by local structures
Sl = (Σl  σl(t  x)) and local parameters θl = {λls}s∈Σl  where Σl denotes a set discrete states  λls

2

are non-negative constants  and σl denotes a state function that maps a time and an event sequence
to Σl and is piecewise constant in time for every event sequence. The conditional intensity functions
are deﬁned as λl(t|x) = λls with s = σl(t  x)  and thus are piecewise constant. The resulting data
likelihood can be written as

p(x|S  θ) =Y

Y

λcls(x)
ls

e−λlsdls(x)

where S = {Sl}l∈L   θ = {θl}l∈L   cls(x) is the number of times label l occurs in x when the state
i 1l (li) 1s (σl(ti  hi)))  and dls(x) is the total duration during

function for l maps to state s (i.e. P
which the state function for l maps to state s in the data x (i.e. R t(x)

1s (σ (τ  h (τ  x))) dτ).

s∈Σl

l∈L

0

(2)

3.2 Poisson Networks
Poisson networks[15  18] are closely related to PCIMs. Given a basis set B of piecewise-constant
real-valued feature functions f(t  x)  a feature vector σl(t  x) is deﬁned for each l by selecting
component feature functions from B. The resulting σl(t  x) are piecewise-constant in time. The
conditional intensity for l is given by the regression λl(t|x  θ) = ewl·σl(t x) with parameter wl. By
convention  the component σl 0(t  x) = 1 so that wl 0 is a bias parameter.
The resulting likelihood does not have a conjugate prior  and in our experiments we use iterative
(cid:16)
MAP parameter estimates under a Gaussian prior  and use a Laplace approximation of the marginal
likelihood for structure learning (i.e.  feature selection) [15]. In our experiments  each f ∈ B is spec-
iﬁed by a label l and a pair of time offsets 0 ≤ d1 < d2  and takes on the value log
1 + cl d1 d2 (t x)
where cl d1 d2(t  x) is the number of times l occurs in x in the interval [t − d2  t − d1).

d2−d1

(cid:17)

4 Learning PCIMs

In this section  we present an efﬁcient learning algorithm for PCIMs. We give a conjugate prior for
the parameters θ which yields closed form formulas for the parameter posteriors and the marginal
likelihood of the data given a structure S. We then give a decision tree based learning algorithm that
uses the closed-form marginal likelihood formula to learn the local structure Sl for each label.

4.1 Closed-Form Parameter Posterior and Marginal Likelihood

In general  computing parameter posteriors for likelihoods of the form of equation (1) is compli-
cated. However  in the case of PCIMs  the Gamma distribution is a conjugate prior for λls  despite
the fact that the data likelihood of equation (2) is not a product of exponential densities (i.e.  when
cls(x) 6= 1). The corresponding prior and posterior densities are given by
p(λls|αls  βls) = βls
Assuming the prior over θ is a product of such p(λls|αls  βls)  the marginal likelihood is

p(λls|αls  βls  x) = p(λls|αls + cls(x)  βls + dls(x))

e−βlsλls;

αls

In our experiments  we use the point estimate ˆλls = αls+cls(x)

γls(x);

γls(x) = βls

(βls + dls(x))αls+cls(x)

Γ(αls + cls(x))

αls
Γ(αls)
βls+dls(x) which is E [λls | x].

ls

Γ(αls) λαls−1
p(x|S) =Y
Y

l∈L

s∈Σl

4.2 Structure Learning with Decision Trees

In this section  we specify the set of possible structures in terms of a set of basis state functions  a
set of decision trees built from them  and a greedy Bayesian model selection procedure for learning
a structure. Finally  we describe the particular set of basis state functions we use in our experiments.
We use B to denote the set of basis state functions f(t  x)  each taking values in a basis state set
Σf . Given B  we specify Sl through a decision tree whose interior nodes each have an associated
f ∈ B and a child corresponding to each value in Σf . The per-label state set Σl is then the set of

3

s∈Σl

Q

l∈LQ

leaves in the tree. The state function σl(t  x) is computed by recursively applying the basis state
functions in the tree until a leaf is reached. Note that the resulting mapping is a valid state function
by construction.
In order to carry out Bayesian model selection  we use a factored structural prior p(S) ∝
κls. Since the prior and the marginal likelihood both factor over l  the local struc-
tures Sl can be chosen independently. We search for each Sl as follows. We begin with Sl being the
trivial decision tree that maps all event sequences and times to the root. In this case  λl(t|x) = λl.
l speciﬁed by choosing a leaf s ∈ Σl and a basis state function
Given the current Sl  we consider S0
f ∈ B  and assigning f to s to get a set of new child leaves {s1 ···   sm} where m = |Σf|. Be-
cause the marginal likelihood factors over states  the gain in the posterior of the structure due to this
split is p(S0
l is chosen by selecting the s and f
with the largest gain. The search terminates if there is no gain larger than one. We note that the
local structure representation and search can be extended from decision trees to decision graphs in a
manner analogous to [3].
In our experiments  we wish to learn how events depend on the timing and type of prior events. We
therefore use a set of time and label speciﬁc basis state functions. In particular  we use binary basis
state functions fl0 d1 d2 τ indexed by a label l0 ∈ L  two time offsets 0 ≤ d1 < d2 and a threshold
τ > 0. Such a f encodes whether or not the event sequence x contains at least τ events with label l0
with timestamps in the window [t − d2  t − d1). Examples of decision trees that use such basis state
functions are shown in Figure 1.

p(Sl|x) = κls1 γls1 (x)···κlsm γlsm (x)

. The next structure S0

κlsγls(x)

l|x)

5 Forecasting

j   b∗

j   b∗

j   [a∗

j   tij ∈ [a∗

j )(cid:1)(cid:9)k

episodic sequence and denote it by e = (cid:8)(cid:0)l∗

In this section  we describe how to use PCIMs to forecast whether a sequence of target labels will
occur in a given order and in given time intervals. For example  we may wish to know the probability
that a computer system will experience a system failure in the next week and again in the following
week  or that an internet user will be shown a particular display ad and then visit the advertising
merchant’s website in the next month. We call such a sequence and set of associated intervals an
j )) the jth episode.
We say that the episodic sequence e occurs in an event sequence x if ∃i1 < ··· < ik : (tij   lij ) ∈
x  lij = l∗
Given an event sequence h and a time t∗ ≥ t(h)  we term any event sequence x whose history up
to t∗ agrees with h (i.e.  h(t∗  x) = h) an extension of h from t∗. Our forecasting problem is  given
at observed sequence h at time t∗ ≥ t(h)  to compute the probability that e occurs in extensions of
h from t∗. This probability is p (X ∈ Xe | h(t∗  X) = h) and will be denoted using the shorthand
p(Xe|h  t∗). Computing p(Xe|h  t∗) is hard in general because the probability of episodes of interest
can depend on arbitrary numbers of intervening events. We therefore give Monte Carlo estimates for
p(Xe|h  t∗)  ﬁrst describing a forward sampling procedure for forecasting episodic sequences (also
applicable to other forecasting problems)  and then introducing an importance sampling scheme
speciﬁcally designed for forecasting episodic sequences.

j ). The set of event sequences x in which e occurs is denoted Xe.

. We call (l∗

j   [a∗

j   b∗

j=1

5.1 Forward Sampling

1
M

PM
The probability of an episodic sequence can be estimated using a forward sampling approach by
sampling M extensions {x(m)}M
m=1 of h from t∗ and using the estimate ˆpFwd(Xe|h  t∗; M) =
m=1 1Xe(x(m)). By Hoeffding’s inequality  P (|ˆpFwd(Xe|h  t∗; M) − p(Xe|h  t∗)| > ) ≤
It is important to note that
k  and thus we need only sample ﬁnite extensions x such that

(cid:1).
k from p(cid:0)x | h(t∗  x) = h  t|x|+1 ≥ b∗
an arbitrary event sequence x can be written asQn

2e−22M . Thus  the error in ˆpFwd(Xe|h  t∗; M) falls as O(1/
1Xe(x) only depends on x up to b∗
t(x) < b∗
The forward sampling algorithm for Poisson Networks [15] can be easily adapted for PCIMs. Here
we outline how to forward sample an extension x of h from t∗ to b∗
k given a general CIM. Forward
sampling consists of iteratively obtaining a sample sequence xi of length i by sampling (ti  li) and
appending to a prior sampled sequence xi−1 of length i − 1. The CIM likelihood (Equation 1) of
i=1 p(ti  li|hi; θ). Thus  we begin with x|h| = h 

M).

√

k

4

note that p(ti  li|hi; θ) = λli(ti|hi  θ)e−Λli (ti|hi;θ)Q

and iteratively sample (ti  li) from p(ti  li|hi = xi−1; θ) and append to xi−1 to obtain xi. Note
that one needs to use rejection sampling during the ﬁrst iteration to ensure t|h|+1 > t∗. The ﬁnite
k is obtained by terminating when ti > b∗
extension up to b∗
k and rejecting ti. To sample (ti  li) we
e−Λl(ti|hi;θ) has a competing risks form
l6=li
[1  11]  so that we can sample |L| candidate times tl
i independently from the non-homogeneous
i|hi;θ) and then let ti be the smallest of these candidate times
exponential densities λl(tl
i from a piecewise constant
and li be the corresponding l. A more detailed description of sampling tl
conditional intensities is given in [15]. Finally  we note that the basic sampling procedure can be
made more efﬁcient using the techniques described in [15] and [7].

i|hi  θ)e−Λl(tl

5.2

Importance Sampling

When using a forward sampling approach to forecast unlikely episodic sequences  the episodes
of interest will not occur in most of the sampled extensions and our estimate of p(Xe|h  t∗) will
be noisy. In fact  due to the fact that absolute error in ˆpFwd falls as the square root of the num-
ber of sequences sampled  we would need O(1/p(Xe|h  t∗)2) sample sequences to get non-trivial
lower bounds on p(Xe|h  t∗) using a forward sampling approach. To mitigate this problem we
develop an importance sampling approach  where sequences are drawn from a proposal distribu-
tion q(·) that has an increased likelihood of generating extensions in which Xe occurs  and then
uses a weighted empirical estimate. In particular  we will sample extensions x(m) of h from t∗
mate p(Xe|h  t∗) through

from q(cid:0)x | h(t∗  x) = h  t|x|+1 ≥ b∗

(cid:1)  and will esti-

k

k

(cid:1) instead of p(cid:0)x | h(t∗  x) = h  t|x|+1 ≥ b∗
1PM
p(cid:0)x | h(t∗  x) = h  t|x|+1 ≥ b∗
q(cid:0)x | h(t∗  x) = h  t|x|+1 ≥ b∗

w(x(m))1Xe(x(m)) 

m=1 w(x(m))

MX

(cid:1)
(cid:1)

m=1

k

k

ˆpImp(Xe|h  t∗; M) =

w(x) =

The Poisson Superposition Importance Sampler (PSIS) is an importance sampler whose proposal
distribution q is based on Poisson superposition. This proposal distribution is deﬁned to be a CIM
l (t|x) where λl(t|x; θ) is the con-
whose conditional intensity functions are given by λl(t|x; θ) + λ∗
ditional intensity function of l under the model and λ∗

l (t|x) is given by
j(x)  t ∈ [aj(x)(x)  b∗

j(x))  and j(x) 6= 0.

(

l (t|x) =
λ∗

1

j(x)−aj(x)(x)
b∗
0

for l = l∗
otherwise 

where the active episode j(x) is 0 if t(x) ≥ bj(x)  j = 1 ···   k and is min ({j : bj(x) > t(x)})
otherwise. The time bj(x) when the jth episode ceases to be active is the time at which the jth
episode occurs in x  or b∗
j ) do not overlap 
aj(x) = a∗

j if it does not occur. If the episodic intervals [a∗

j . In general aj(x) and bj(x) are given by the recursion

j   b∗

aj(x) = max(cid:0){a∗
bj(x) = min(cid:0){b∗

j   bj−1(x)}(cid:1)

j} ∪ {(ti  li) ∈ x : li = l∗

j   ti ∈ [aj(x)  b∗

j )}(cid:1) .

This choice of q makes it likely that the jth episode will occur after the j − 1th episode.
As the proposal distribution is also a CIM  importance sampling can be done using the forward
sampling procedure above. If the model is a PCIM  the proposal distribution is also a PCIM  since
l (t|x) are piecewise constant in t. In practice the computation of j(x)  aj(x)  and bj(x) can be
λ∗
done during forward sampling.
The importance weight corresponding to our proposal distribution is

 

kY

j=1

w(x) =

exp

bj(x) − aj(x)
j − aj(x)
b∗

! Y

(ti li)∈x:

ti=bj (x) li=l∗

j

λl∗

j

5

λl∗

j

(ti|xi) +

(ti|xi)
j −aj (x)
b∗

1

.

In many problems  the importance weight w(x) of a sequence x of length n is a product of n small
terms. When n large  this can cause the importance weights to become degenerate  and this problem
is often solved using particle ﬁltering [7]. Note that the second product in w(x) above has at most
one term for each j so that w(x) has k terms corresponding to the k episodes  which is independent
of n. Thus  we do not experience the problem of degenerate weights when k is small  regardless of
the number of events sampled.

6 Experimental Results

We ﬁrst validate that PCIMs can learn temporal dependencies and that the PSIS gives faster fore-
casting than forward sampling using a synthetic data set. We then show that PCIMs are more than
an order of magnitude faster to train than Poisson Networks  and better model unseen test data using
real supercomputer log data. Finally we show that PCIMs and the PSIS allow the forecasting future
interests of web search users using real log data from a major commercial search engine.

6.1 Validation on Synthetic Data

In order to evaluate the ability of PCIMs to learn nonlinear temporal dependencies we sampled data
from a known model and veriﬁed that the dependencies learned were correct. Data was sampled
from a PCIM with L = {A B C}. The known model is shown in Figure 1.

(a) Event type A

(b) Event type B

(c) Event type C

Figure 1: Decision trees representing S and θ for events of type A  B and C.

We sampled 100 time units of data  observing 97 instances of A  58 instances of B  and 71 instances
of C. We then learn a PCIM from the sampled data. We used basis state functions that tested for the
presence of each label in windows with boundaries at t − 0  1  2 ···   10  and +∞ time units. We
used a common prior with a mean rate of 0.1 and a equivalent sample size of one time unit for all
λls  and the structural prior described above with κls = 0.1 for all s.
The learned PCIM perfectly recovered the correct model structure. We repeated the experiment by
sampling data from a model with ﬁfteen labels  consisting of ﬁve independent copies of the model
above. That is  L = {A1  B1  C1 ···   A5  B5  C5} with each triple Ai  Bi  Ci independent of other
labels  and dependent on each other as speciﬁed by Figure 1. Once again  the model structure was
recovered perfectly.
We evaluated the PSIS in forecasting event sequences with the model shown in Figure 1. The
convergence of importance sampling is compared with that of forward sampling in Figure 2. We
give results for forecasting three different episodic sequences  consisting of the label sequences
{C}  {C  B}  and {C  B  A}  all in the interval [0  1]  given an empty history. The three queries are
given in order of decreasing probability  so that inference becomes harder. We show how estimates
of the probabilities of given episodic sequences vary as a function of the number of sequences
sampled  giving the mean and variance of the trajectories of the estimates computed over ten runs.
For all three queries  importance sampling converges faster and has lower variance. Since exact
inference is infeasible for this model  we forward sample 4 000 000 event sequences and display
this estimate. Note that despite the large sample size the Hoeffding bound gives a 95% conﬁdence

6

A in[t-1 t)A in [t-2 t-1)λ=10.0λ=0.0yesyesnoλ=0.1noB in [t-1 t)B in [t-2 t-1)λ=10.0λ=0.0yesyesnoA in [t-5 t)λ=0.002λ=0.2yesnonoC in [t-1 t)C in [t-2 t-1)λ=10.0λ=0.0yesyesnoB in [t-5 t)λ=0.002λ=0.2yesnono(a) Label C in [0  1]

(b) Labels C  B in [0  1]

(c) Labels C  B  A in [0  1]

Figure 2: Trajectories of ˆpImp and ˆpFwd vs.
the number of sequences sampled for three different
queries. The dashed and dotted lines show the empirical mean and standard deviation over ten runs
of ˆpImp and ˆpFwd. The solid line shows ˆpFwd based on 4 million event sequences.

interval of ±0.0006 for this estimate  which is large relative to the probabilities estimated. This
further suggests the need for importance sampling for rare label sequences.

6.2 Modeling Supercomputer Event Logs

We compared PCIM and Poisson Nets on the task of modeling system event logs from the Blue-
Gene/L supercomputer at Lawrence Livermore National Laboratory [14]  available at the USENIX
Computer Failure Data Repository. We ﬁltered out informational (non-alert) messages from the logs 
and randomly split the events by node into a training set with 311 060 alerts from 21 962 nodes  and
a test set with 68 502 alerts from 9 412 nodes. We learned dependencies between the 38 alert types
in the data. We treat the events from each node as separate sequences  and use a product of the
per-sequence likelihoods given in equation (1).
For both models  we used window boundaries at t − 1/60  1  60  3600  and ∞ seconds. The PCIM
used count threshold basis state functions with thresholds of 1  4  16 and 64 while the Poisson Net
used log count feature vectors as described above. Both models used priors with a mean rate of an
event every 100 days  no dependencies  and an equivalent sample size of one second. Both used a
structural prior with κls = 0.1. Table 1 shows the test set likelihood and the run time for the two
approaches. PCIM achieves better test set likelihood and is more than an order of magnitude faster.

PCIM
Poisson Net

Test Log Likelihood Training Time
11 min
3 hr 33 min

-85.3
-88.8

Table 1: A comparison of the PCIM and Poisson Net in modeling supercomputer event logs. The
test set log likelihood reported has been divided by the number of test nodes (9 412). The training
time for the PCIM and Poisson Net are also shown.

6.3 Forecasting Future Interests of Web Search Users

We used the query logs of a major internet search engine to investigate the use of PCIMs in forecast-
ing the future interests of web search users. All queries are mapped to one of 36 different interest
categories using an automatic classiﬁer. Thus  L contains 36 labels  such as “Travel” or “Health &
Wellness.” Our training set contains event sequences for approximately 23k users consisting of about
385k timestamped labels recorded over a two month period. The test set contains event sequences
for approximately 11k users of about 160k timestamped labels recorded over the next month.
We trained a PCIM on the training data using window boundaries at t − 1 hour  t − 1 day  and t − 1
week  and basis state functions that tested for the presence of one or more instance of each label in
each window  treating users as i.i.d. The prior had a mean rate of an event every year  an equivalent
sample size of one day. The structural prior had κls = 0.1. The model took 1 day and 18 hours to
train on 3 GHz workstation. We did not compare to a Poisson network on this data since  as shown
above  Poisson networks take an order of magnitude longer to learn.

7

Figure 3: Precision-recall curves for forecasting future Health & Wellness queries using a full PCIM 
a restricted PCIM that conditions only on past Health & Wellness queries  a baseline that takes into
account only past Health & Wellness queries and not their timing  and random guessing.

Given the ﬁrst week of each test user’s event sequence  we forecasted whether they would issue a
query in a chosen target category in the second week. We used the PSIS with 100 sample sequences
for forecasting. Figure 3 shows the precision recall curve for one target category label. Also shown
is the result for restricted PCIMs that only model dependencies on prior occurrences of the target
category. This is compared to a baseline where the conditional intensity depends only on whether the
target label appeared in the history. This shows that modeling the temporal aspect of dependencies
does provide a large improvement. Modeling dependencies on past occurrences of other labels also
provides an improvement in the right-hand region of the precision-recall curve.
To better understand the performance of PCIMs we also examined the problem of predicting the ﬁrst
occurrence of the target label. As Figure 3 suggests (but doesn’t show)  the PCIM can model cross-
label dependencies to forecast the ﬁrst occurrence of the target label. Forecasting new interests is
valuable in a variety of applications including advertising and the fact that PCIMs are able to forecast
ﬁrst occurrences is promising. Results similar to Figure 3 were obtained for other target labels.

7 Discussion

We presented the Piecewise-Constant Conditional Intensity Model  which is a model of temporal
dependencies in continuous time event streams. We gave a conjugate prior and a greedy tree build-
ing procedure that allow for efﬁcient learning of these models. Dependencies on the history are
represented through automatically learned combinations of a given set of basis state functions. One
of the key beneﬁts of PCIMs is that they allow domain knowledge to be encoded in these basis
state functions. This domain knowledge is incorporated into the model during structure search in
situations where it is supported by the data. The fact that we use decision trees allows us to easily
interpret the learned dependencies.
In this paper  we focused on basis state functions indexed by a ﬁxed set of time windows and labels.
Exploring alternative types of basis state functions is an area for future research. For example  basis
state functions could encode the most recent events that have occurred in the history rather than the
events that occurred in windows of interest. The capacity of the resulting model class depends on
the set of basis state functions chosen. Understanding how to choose the basis state functions and
how to adapt our learning procedure to control the resulting capacity is another open topic. We also
presented the Poisson Superposition Importance Sampler for forecasting episodic sequences with
PCIMs. Developing forecasting algorithms for more general queries is of interest.
Finally  we demonstrated the value of PCIMs in modeling the temporal behavior of web search users
and of supercomputer nodes. In many applications  we have access to richer event streams such as
spatio-temporal event streams and event streams with structured labels. It would be interesting to
extend PCIMs to handle such rich event streams.

8

References
[1] Simeon M. Berman. Note on extreme values  competing risks and semi-Markov processes.

Ann. Math. Stat.  34(3):1104–1106  1963.

[2] W. Buntine. Theory reﬁnement on Bayesian networks. In UAI  1991.
[3] David Maxwell Chickering  David Heckerman  and Christopher Meek. A Bayesian approach

to learning Bayesian networks with local structure. In UAI  1997.

[4] D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes: Elementary

Theory and Methods  volume I. Springer  2 edition  2003.

[5] Thomas Dean and Keiji Kanazawa. Probabilistic temporal reasoning. In AAAI  1988.
[6] Vanessa Didelez. Graphical models for marked point processes based on local independence.

J. Roy. Stat. Soc.  Ser. B  70(1):245–264  2008.

[7] Yu Fan and Christian R. Shelton. Sampling for approximate inference in continuous time

Bayesian networks. In AI & M  2008.

[8] N. Friedman  I. Nachman  and D. Pe´er. Using Bayesian networks to analyze expression data.

J. Comp. Bio.  7:601–620  2000.

[9] Nir Friedman  Kevin Murphy  and Stuart Russell. Learning the structure of dynamic proba-

bilistic networks. In UAI  1998.

[10] David Heckerman  David Maxwell Chickering  Christopher Meek  Robert Rounthwaite  and
Carl Kadie. Dependency networks for inference  collaborative ﬁltering  and data visualization.
JMLR  1:49–75  October 2000.

[11] A. A. J. Marley and Hans Colonius. The “horse race” random utility model for choice prob-
abilities and reaction times  and its competing risks interpretation. J. Math. Psych.  36:1–20 
1992.

[12] Uri Nodelman  Christian R. Shelton  and Daphne Koller. Continuous time Bayesian networks.

In UAI  2002.

[13] Uri Nodelman  Christian R. Shelton  and Daphne Koller. Expectation Maximization and com-

plex duration distributions for continuous time Bayesian networks. In UAI  2005.

[14] Adam Oliner and Jon Stearley. What supercomputers say - an analysis of ﬁve system logs. In

IEEE/IFIP Conf. Dep. Sys. Net.  2007.

[15] Shyamsundar Rajaram  Thore Graepel  and Ralf Herbrich. Poisson-networks: A model for

structured point processes. In AIStats  2005.

[16] Aleksandr Simma  Moises Goldszmidt  John MacCormick  Paul Barham  Richard Brock  Re-
becca Isaacs  and Reichard Mortier. CT-NOR: Representing and reasoning about events in
continuous time. In UAI  2008.

[17] Aleksandr Simma and Michael I. Jordan. Modeling events with cascades of Poisson processes.

In UAI  2010.

[18] Wilson Truccolo  Uri T. Eden  Matthew R. Gellows  John P. Donoghue  and Emery N. Brown.
A point process framework relating neural spiking activity to spiking history  neural ensemble 
and extrinsic covariate effects. J. Neurophysiol.  93:1074–1089  2005.

9

,Helena Peic Tukuljac
Antoine Deleforge
Remi Gribonval