2019,Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness,Many recent works have shown that adversarial examples that fool classifiers can be found by minimally perturbing a normal input. Recent theoretical results  starting with Gilmer et al. (2018b)  show that if the inputs are drawn from a concentrated metric probability space  then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with Ω(1) (e.g. 1/100) measure  according to the imposed distribution  has small distance to almost all (e.g.   99/100) of the points in the space. It is not clear   however   whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to and L_2 and L_infinity perturbations of several image classification benchmarks. Code for our experiments is available at https://github.com/xiaozhanguva/Measure-Concentration.,Empirically Measuring Concentration:

Fundamental Limits on Intrinsic Robustness

Saeed Mahloujifar∗  Xiao Zhang∗  Mohammad Mahmoody  and David Evans

University of Virginia

[saeed  shawn  mohammad  evans]@virginia.edu

Abstract

Many recent works have shown that adversarial examples that fool classiﬁers can be
found by minimally perturbing a normal input. Recent theoretical results  starting
with Gilmer et al. (2018b)  show that if the inputs are drawn from a concentrated
metric probability space  then adversarial examples with small perturbation are
inevitable. A concentrated space has the property that any subset with Ω(1) (e.g. 
1/100) measure  according to the imposed distribution  has small distance to almost
all (e.g.  99/100) of the points in the space. It is not clear  however  whether
these theoretical results apply to actual distributions such as images. This paper
presents a method for empirically measuring and bounding the concentration of a
concrete dataset which is proven to converge to the actual concentration. We use
it to empirically estimate the intrinsic robustness to (cid:96)∞ and (cid:96)2 perturbations of
several image classiﬁcation benchmarks. Code for our experiments is available at
https://github.com/xiaozhanguva/Measure-Concentration.

1

Introduction

Despite achieving exceptionally high accuracy on natural inputs  state-of-the-art machine learning
models have been shown to be vulnerable to adversaries who use small perturbations to fool the
classiﬁer (Szegedy et al.  2014; Goodfellow et al.  2015). This phenomenon  known as adversarial
examples  has motivated numerous studies (Papernot et al.  2016; Madry et al.  2018; Biggio & Roli 
2018; Gilmer et al.  2018a) to develop heuristic defenses that aim to improve classiﬁer robustness.
However  most defense mechanisms have been quickly broken by adaptive attacks (Carlini & Wagner 
2017; Athalye et al.  2018). Although certiﬁcation methods (Raghunathan et al.  2018; Wong &
Kolter  2018; Sinha et al.  2018; Wong et al.  2018; Gowal et al.  2019; Wang et al.  2018; Zhang
et al.  2019) have been proposed aiming to end such arms race and continuous efforts have been
made to develop better robust models  both the robustness guarantees and efﬁciency achieved by
state-of-the-art robust classiﬁers are far from satisfying.
This motivates a fundamental information-theoretic question: what are the inherent limitations
of developing robust classiﬁers? Several recent works (Gilmer et al.  2018b; Fawzi et al.  2018;
Mahloujifar et al.  2019; Shafahi et al.  2019; Bhagoji et al.  2019) have shown that under certain
assumptions regarding the data distribution and the perturbation metric  adversarial examples are
theoretically inevitable. As a result  for a broad set of theoretically natural metric probability spaces
of inputs  there is no classiﬁer for the data distribution that achieves adversarial robustness. For
example  Gilmer et al. (2018b) assumed that the input data are sampled uniformly from n-spheres and
proved a model-independent theoretical bound connecting the risk to the average Euclidean distance
to the “caps” (i.e.  round regions on a sphere). Mahloujifar et al. (2019) generalized this result to any
concentrated metric probability space of inputs and showed  for example  that if the inputs come from

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

any Normal Lévy family (Lévy  1951)  any classiﬁer with a noticable test error will be vulnerable to
small (i.e.  sublinear in the typical norm of the inputs) perturbations.
Although such theoretical ﬁndings seem discouraging to the goal of developing robust classiﬁers 
all these impossibility results depend on assumptions about data distributions that might not hold
for cases of interest. Our work develops a general method for testing properties of concrete datasets
against these theoretical assumptions.

Contributions. Our work shrinks the gap between theoretical analyses of robustness of classiﬁcation
for theoretical data distributions and understanding the intrinsic robustness of actual datasets. Indeed 
quantitative estimates of the intrinsic robustness2 of benchmark image datasets such as MNIST and
CIFAR-10 can provide us with a better understanding of the threat of adversarial examples for natural
image distributions and may suggest promising directions for further improving classiﬁer robustness.
Our main technical contribution is a general method to evaluate the concentration of a given input
distribution µ based on a set of data samples. We prove that by simultaneously increasing the sample
size m and a complexity parameter T   the concentration of the empirical measure converges to the
actual concentration of µ (Section 3). Using this method  we perform experiments to demonstrate
the existence of robust error regions for benchmark datasets under both (cid:96)∞ and (cid:96)2 perturbations
(Section 4). Compared with state-of-the-art robustly trained models  our estimated intrinsic robustness
shows that  for most settings  there exists a large gap between the robust error achieved by the best
current models and the theoretical limits implied by concentration. This suggests the concentration
of measure is not the only reason behind the vulnerability of existing classiﬁers to adversarial
perturbations. Thus  either there is room for improving the robustness of image classiﬁers (even with
non-zero classiﬁcation error) or a need for deeper understanding of the reasons for the gap between
intrinsic robustness and the actual robustness achieved by robust models  at least for the datasets like
the image classiﬁcation benchmarks used in our experiments.

Related Work. We are aware of only one previous work that attempts to heuristically estimate these
properties. To extend their theoretical impossibility result to the practical distributions  Gilmer et al.
(2018b) studied MNIST dataset to ﬁnd a region that is somewhat robust in terms of the expected
(cid:96)2 distance of other images from the region. In their setting  they showed the existence of a set of
measure 0.01 with average (cid:96)2 distance 6.59 to all points. In comparison  our work is the ﬁrst to
provide a general methodology to empirically estimate the concentration of measure with provable
guarantees. Moreover  we are able to deal with (cid:96)∞  and worst-case bounded perturbations for
modeling adversarial risk  which is the most popular setting for research in adversarial examples.
In addition  another related concurrent work (Bhagoji et al.  2019) studied lower bounds on the
adversarial risk using optimal transport on the metric probability space of instances. They also
measure the optimal transport on the empirical distributions but do not characterize the relationship
between the optimal transport of empirical datasets and the actual one of the underlying distributions.
Another related line of work estimated lower bounds on the concentration of measure of the underlying
distribution through simulating distributions by generative models. Fawzi et al. (2018) proved a lower
bound on the concentration of the generated image distribution  assuming the underlying generative
model has Gaussian latent space and small Lipschitz constant. Krusinga et al. (2019) estimated
an upper bound on the density function of the distribution using generative model  then proved
concentration inequalities based on upper bounds on the density function. Our work is distinct from
these works  because we directly learn the concentration function instead of a lower bound  and we
use the actual data samples instead of samples generated from some trained generative model.
The work of Tsipras et al. (2019) studied the trade-off between robustness and accuracy. They show
that for some speciﬁc learning problems  achieving robustness and accuracy together is not possible.
At ﬁrst glance  it might seem that this trade-off contradicts the existing lower bounds that come from
concentration of measure. However  there is no contradiction and what is proved there is with regard
to a different deﬁnition of adversarial examples. The deﬁnition of adversarial examples used there
could diverge from our deﬁnition in some learning problems (see Diochnos et al. (2018))  but they
coincide in the cases that the ground truth function is robust to small perturbations.

2See Deﬁnition 2.2 for the formal deﬁnition of intrinsic robustness. The term robustness has been used with
different meanings in previous works (e.g.  in Diochnos et al. (2018)  it refers to the average distances to the
error region). However  all such uses refer to a desirable property of the classiﬁer in being resilient to adversarial
perturbations  which is the case here as well. See Diochnos et al. (2018) for a taxonomy of different deﬁnitions.

2

of x are deﬁned as (cid:107)x(cid:107)∞ = maxi∈[n] |xi| and (cid:107)x(cid:107)2 = ((cid:80)
measure with respect to a set S sampled from µ as ˆµS (A) = (cid:80)

Notation. Lowercase boldface letters such as x are used to denote vectors  and [n] is used to
represent {1  2  . . .   n}. For any set A  let Pow(A)  |A| and 1A(·) be the set of measurable subsets of
A  cardinality and indicator function of A  respectively. For any x ∈ Rn  the (cid:96)∞-norm and (cid:96)2-norm
i )1/2 respectively. Let (X   µ) be a
probability space and d : X × X → R be some distance metric deﬁned on X . Deﬁne the empirical
x∈S 1A(x)/|S| ∀A ⊆ X . Let
Ball(x  ) = {x(cid:48) ∈ X : d(x(cid:48)  x) ≤ } be the ball around x with radius . For any subset A ⊆ X  
deﬁne the -expansion A = {x ∈ X : ∃ x(cid:48) ∈ Ball(x  ) ∩ A}. The collection of the -expansions
for members of any G ⊆ Pow(X ) is deﬁned and denoted as G = {A : A ∈ G}.

i∈[n] x2

2 Robustness and Concentration of Measure

In this paper  we work with the following deﬁnition of adversarial risk:
Deﬁnition 2.1 (Adversarial Risk). Let (X   µ) be the probability space of instances and f∗ be the
underlying ground-truth. The adversarial risk of a classiﬁer f in metric d with strength  is deﬁned as

(cid:2)∃ x(cid:48) ∈ Ball(x  ) s.t. f (x(cid:48)) (cid:54)= f∗(x(cid:48))(cid:3).3

AdvRisk(f  f∗) = Pr
x←µ

For  = 0  which allows no perturbation  the notion of adversarial risk coincides with traditional risk.
Deﬁnition 2.2 (Intrinsic Robustness). Consider the same setting as in Deﬁnition 2.1. Let F be some
family of classiﬁers  then the intrinsic robustness is deﬁned as the maximum adversarial robustness
that can be achieved within F  namely

Rob(F  f∗) = 1 − inf
f∈F

(cid:8)AdvRisk(f  f∗)(cid:9).

In this work  we specify F as the family of imperfect classiﬁers that have risk at least α ∈ (0  1).
Previous work shows a connection between concentration of measure and the intrinsic robustness
with respect to some families of classiﬁers (Gilmer et al. (2018b); Fawzi et al. (2018); Mahloujifar
et al. (2019); Shafahi et al. (2019)). The concentration of measure on a metric probability space is
deﬁned by a concentration function as follows.
Deﬁnition 2.3 (Concentration Function). Consider a metric probability space (X   µ  d). Suppose
 > 0 and α ∈ (0  1) are given parameters  then the concentration function of the probability measure
µ with respect to   α is deﬁned as

h(µ  α  ) = inf

E∈Pow(X )

{µ(E) : µ(E) ≥ α} .

Note that the standard notion of concentration function (e.g.  see Talagrand (1995)) is related to a
special case of Deﬁnition 2.3 by ﬁxing α = 1/2.
Generalizing the result of Gilmer et al. (2018b) about instances drawn from spheres  Mahloujifar
et al. (2019) showed that  in general  if the metric probability space of instances is concentrated  then
any classiﬁer with 1% risk incurs large adversarial risk for small amount of perturbations.
Theorem 2.4 (Mahloujifar et al. (2019)). Let (X   µ) be the probability space of instances and f∗ be
the underlying ground-truth. For any classiﬁer f  we have

AdvRisk(f  f∗) ≥ h(µ  Risk(f  f∗)  ).

In order for this theorem to be useful  we need to know the concentration function. The behavior of
this function is studied extensively for certain theoretical metric probability spaces (Ledoux  2001;
Milman & Schechtman  1986). However  it is not known how to measure the concentration function
for arbitrary metric probability spaces. In this work  we provide a framework to (algorithmically)
bound the concentration function from i.i.d. samples from a distribution. Namely  we want to solve
the following optimization task using our i.i.d. samples:

minimize
E∈Pow(X )

µ(E)

subject to µ(E) ≥ α.

(1)

3Note that bounding lp norm might be restrictive for the adversary (Gilmer et al.  2018a) and this deﬁnition

only covers a subset of possible adversaries.

3

We aim to estimate the minimum possible adversarial risk  which captures the intrinsic robustness
for classiﬁcation in terms of the underlying distribution µ  conditioned on the fact that the original
risk is at least α. Note that solving this optimization problem only shows the possibility of existence
of an error region E with certain (small) expansion. This means that there could potentially exist a
classiﬁer with risk at least α and adversarial risk equal to the solution of the optimization problem
of (1). Actually ﬁnding such an optimally robust classiﬁer (with error α) using a learning algorithm
might be a much more difﬁcult task or even infeasible. We do not consider that problem in this work.

3 Method for Measuring Concentration

In this section  we present a method to measure the concentration of measure on a metric probability
space using i.i.d. samples. To measure concentration  there are two main challenges:

1. Measuring concentration appears to require knowledge of the density function of the distri-

bution  but we only have a data set sampled from the distribution.

2. Even with the density function  we have to ﬁnd the best possible subset among all the subsets

of the space  which seems infeasible.

We show how to overcome these challenges and ﬁnd the actual concentration in the limit by ﬁrst
empirically simulating the distribution and then narrowing down our search space to a speciﬁc
collection of subsets. Our results show that for a carefully chosen family of sets  the set with
minimum expansion can be approximated using polynomially many samples. On the other hand  the
minimum expansion convergence to the actual concentration (without the limits on the sets) as the
complexity of the collection goes to inﬁnity.
Before stating our main theorems  we introduce two useful deﬁnitions. The following deﬁnition
captures the concentration function for a speciﬁc collection of subsets.
Deﬁnition 3.1 (Concentration Function for a Collection of Subsets). Consider a metric probability
space (X   µ  d). Let  > 0 and α ∈ (0  1) be given parameters  then the concentration function of
the probability measure µ with respect to   α and a collection of subsets G ⊆ Pow(X ) is deﬁned as

h(µ  α   G) = infE∈G {µ(E) : µ(E) ≥ α} .

When G = Pow(X )  we write h(µ  α  ) for simplicity.
We also need to deﬁne the notion of complexity penalty for a collection of subsets. The complexity
penalty for a collection of subsets captures the rate of the uniform convergence for the subsets in
that collection. One can get such uniform convergence rates using the VC dimension or Rademacher
complexity of the collection.
Deﬁnition 3.2 (Complexity Penalty). Let G ⊆ Pow(X ) be a collection of subsets of X . A function
φ : N × R → [0  1] is a complexity penalty for G iff for any probability measure µ supported on X
and any δ ∈ [0  1]  we have
Pr

[∃ E ∈ G s.t. |µ(E) − ˆµS(E)| ≥ δ] ≤ φ(m  δ).

S←µm

Theorem 3.3 shows how to overcome the challenge of measuring concentration from ﬁnite samples 
when the concentration is deﬁned with respect to speciﬁc families of subsets. Namely  it shows that
the empirical concentration is close to the true concentration  if the underlying collection of subsets
is not too complex. The proof of Theorem 3.3 is provided in Appendix A.1.
Theorem 3.3 (Generalization of Concentration). Let (X   µ  d) be a metric probability space and
G ⊆ Pow(X ). For any δ  α   ∈ [0  1]  we have

[h(µ  α− δ   G)− δ ≤ h(ˆµS  α   G) ≤ h(µ  α + δ   G) + δ] ≥ 1− 2(cid:0)φ(m  δ) + φ(m  δ)(cid:1)

Pr
S←µm
where φ and φ are complexity penalties for G and G respectively.
Remark 3.4. Theorem 3.3 shows that if we narrow down our search to a collection of subsets G
such that both G and G have small complexity penalty  then we can use the empirical distribution to
measure concentration of measure for that speciﬁc collection. Note that the generalization bound of

4

Theorem 3.3 depends on complexity penalties for both G and G. Therefore  in order for this theorem
to be useful  the collection G must be chosen in a careful way. For example  if G has bounded VC
dimension  then G might still have a very large VC dimension. Alternatively  G might denote the
collection of subsets that are decidable by a neural network of a certain size. In that case  even though
there are well known complexity penalties for such collections (see Neyshabur et al. (2017))  the
complexity of their expansions is unknown. In fact  relating the complexity penalty for expansion of a
collection to that of the original collection is tightly related to generalization bounds in the adversarial
settings  which has also been the subject of several recent works (Cullina et al.  2018; Attias et al. 
2019; Montasser et al.  2019; Yin et al.  2019; Raghunathan et al.  2019).

(cid:9)

(cid:8)φT(cid:9)

T∈N and(cid:8)φT

The following theorem  proved in Appendix A.2  states that if we gradually increase the complexity of
the collection and the number of samples together  the empirical estimate of concentration converges
to actual concentration  as long as several conditions hold. Theorem 3.5 and the techniques used in
its proof are inspired by the work of Scott & Nowak (2006) on learning minimum volume sets.
Theorem 3.5. Let {G(T )}T∈N be a family of subset collections deﬁned over a space X . Let
 are
complexity penalties for G(T ) and G(T ) respectively  for some  ∈ [0  1]. Let {m(T )}T∈N and
{δ(T )}T∈N be two sequences such that m(T ) ∈ N and δ(T ) ∈ [0  1].
Consider a sequence of datasets {ST}T∈N  where ST consists of m(T ) i.i.d. samples from a measure
µ supported on X . Also let α ∈ [0  1] be such that h is locally continuous w.r.t the second parameter
at point (µ  α    Pow(X )). If all the following hold 

T∈N be two families of complexity penalty functions such that φT and φT



1. (cid:80)∞
2. (cid:80)∞

T =1 φT (m(T )  δ(T )) < ∞
 (m(T )  δ(T )) < ∞
T =1 φT

3. limT→∞ δ(T ) = 0
4. limT→∞ h(µ  α   G(T )) = h(µ  α  )

then with probability 1  we have limT→∞ h(ˆµST   α   G(T )) = h(µ  α  ).
Remark 3.6. In Theorem 3.5  the ﬁrst two conditions restrict the growth rate for the complexity of
the collections. Namely  we need the complexity penalties φT (m(T )  δ(T )) and φT
 (m(T )  δ(T )) to
rapidly approach 0 as T → ∞  which means the complexity of G(T ) and G(T ) should grow at a
slow rate. The third condition requires that our generalization error goes to zero as we increase T .
Note that the complexity penalty is a decreasing function with respect to δ  which means condition 3
makes achieving the ﬁrst two conditions harder. However  since the complexity penalty is a function
of both δ and sample size  we can still increase the sample size with a faster rate to satisfy the ﬁrst two
conditions. Finally  the fourth condition requires our approximation error goes to 0 as we increase T .
Note that this condition holds for any family of collections of subsets that is a universal approximator
(e.g.  decision trees or neural networks). However  in order for our theorem to hold  we also need all
the other conditions. In particular  we cannot use decision trees or neural networks as our collection
of subsets  because we do not know if there is a complexity penalty for them that satisﬁes condition 2.

3.1 Special Case of (cid:96)∞

(cid:110)Rn \ ∪T

In this subsection  we show how to instantiate Theorem 3.5 for the case of (cid:96)∞. Below  we introduce
a special collection of subsets characterized by the complement of a union of hyperrectangles:
Deﬁnition 3.7 (Complement of union of hyperrectangles). For any positive integer T   the collection
of subsets speciﬁed by the complement of a union of T n-dimensional hyperrectangles is deﬁned as

CR(T  n) =

where Rect(u  r) =(cid:8)x ∈ X : ∀j ∈ [n] |xj − uj| ≤ rj/2(cid:9) denotes the hyperrectangle centered at

t=1Rect(u(t)  r(t)) : ∀t ∈ [T ]  (u(t)  r(t)) ∈ Rn × Rn≥0

u with r representing the edge size vector. When n is free of context  we simply write CR(T ).
Recall that our goal is to ﬁnd a subset E ∈ Rn such that E has measure at least α and the ∞-expansion
of E under (cid:96)∞ has the minimum measure. To achieve this goal  we approximate the distribution µ

 

(cid:111)

5

(cid:110) ∪T

(cid:111)

.

with an empirical distribution ˆµS  and limit our search to the special collection CR(T ) (though our
goal is to ﬁnd the minimum concentration around arbitrary subsets). Namely  what we ﬁnd is still an
upper bound on the concentration function  and it is an upper bound that we know it converges the
actual value in the limit. Our problem thus becomes the following optimization task:

minimize
E∈CR(T )

ˆµS (E∞ )

subject to ˆµS (E) ≥ α.

(2)

The following theorem provides the key to our empirical method by providing a convergence
guarantee. It states that if we increase the number of rectangles and the number of samples together
in a careful way  the solution to the problem using restricted sets converges to the true concentration.
Theorem 3.8. Consider a nice metric probability space (Rn  µ  (cid:96)∞). Let {ST}T∈N be a family
of datasets such that for all T ∈ N  ST contains at least T 4 i.i.d. samples from µ. For any ∞
and α ∈ [0  1]  if h is locally continuous w.r.t the second parameter at point (µ  α  ∞)  then with
probability 1 we get

T→∞ h(ˆµST   α  ∞ CR(T )) = h(µ  α  ∞).

lim

Note that the size of ST is selected as T 4 to guarantee conditions 1 and 2 are satisﬁed in Theorem
3.5. In fact  we can tune the parameters more carefully to get T 2  instead of T 4  but the convergence
will be slower. See Appendix A.3 for the proof.

3.2 Special Case of (cid:96)2

This subsection demonstrates how to apply Theorem 3.5 to the case of (cid:96)2. The following deﬁnition
introduces the collection of subsets characterized by a union of balls:
Deﬁnition 3.9 (Union of Balls). For any positive integer T   the collection of subsets speciﬁed by a
union of T n-dimensional balls is deﬁned as

B(T  n) =

t=1 Ball(u(t)  r(t)) : ∀t ∈ [T ]  (u(t)  r(t)) ∈ Rn × Rn≥0

When n is free of context  we simply write B(T ).
By restricting our search to the collection of a union of balls B(T ) and replacing the underlying
distribution µ with the empirical one ˆµS  our problem becomes the following optimization task

minimize
E∈B(T )

ˆµS (E2 )

subject to ˆµS (E) ≥ α.

(3)

Theorem 3.10  proven in Appendix A.4  guarantees that if we increase the number of balls and
samples together in a careful way  the solution to the empirical problem (3) converges to the true
concentration.
Theorem 3.10. Consider a nice metric probability space (Rn  µ  (cid:96)2). Let {ST}T∈N be a family
of datasets such that for all T ∈ N  ST contains at least T 4 i.i.d. samples from µ. For any 2
and α ∈ [0  1]  if h is locally continuous w.r.t the second parameter at point (µ  α  2)  then with
probability 1 we get

T→∞ h(ˆµST   α  2 B(T )) = h(µ  α  2).

lim

4 Experiments

In this section  we provide heuristic methods to ﬁnd the best possible error region  which covers at
least α fraction of the samples and its expansion covers the least number of points  for both (cid:96)∞ and (cid:96)2
settings. Speciﬁcally  we ﬁrst introduce our algorithm  then evaluate our approach on two benchmark
image datasets: MNIST (LeCun et al.  2010) and CIFAR-10 (Krizhevsky & Hinton  2009). Note that
in our experiments we exactly use the collection of subsets as suggested by our theoretical results in
the previous section. However  that is not necessary and one might work with any subset collection to
run experiments  as long as they can estimate the measure of the sets and their expansion. We tried
working with other collection of subsets that we do not have theoretical support for (e.g. sets deﬁned
by a neural network) and observed a large generalization gap. This observation shows the importance
of working with subset collections that we can theoretically control their generalization penalty.

6

(a) varying q

(b) varying T

Figure 1: (a) Plots of risk and adversarial risk w.r.t. the resulted error region using our method as q
varies (CIFAR-10  ∞ = 8/255  T = 30); (b) Plots of adversarial risk w.r.t. the resulted error region
using our method (best q) as T varies on MNIST (∞ = 0.3) and CIFAR-10 (∞ = 8/255).

4.1 Experiments for (cid:96)∞
Theorem 3.8 shows that the empirical concentration function h(ˆµS   α  ∞ CR(T )) converges to the
actual concentration h(µ  α  ∞) asymptotically  when T and |S| go to inﬁnity with |S| ≥ T 4. Thus 
to measure the concentration of µ  it remains to solve the optimization problem (2).

Method. Although the collection of subsets is speciﬁed using simple topology  solving (2) exactly is
still difﬁcult  as the problem itself is combinatorial in nature. Borrowing techniques from clustering 
we propose an empirical method to search for desirable error region within CR(T ). Any error region
E could be used to deﬁne fE  i.e.  fE (x) = f∗(x)  if x /∈ E; fE (x) (cid:54)= f∗(x)  if x ∈ E. However 
ﬁnding a classiﬁer corresponding to fE using a learning algorithm might be a very difﬁcult task. Here 
we ﬁnd the optimally robust error region  not the corresponding classiﬁer. A desirable error region
should have small adversarial risk4  compared with all subsets in CR(T ) that have measure at least α.
The high-level intuition is that images from different classes are likely to be concentrated in separable
regions  since it is generally believed that small perturbations preserve the ground-truth class at the
sampled images. Therefore  if we cluster all the images into different clusters  a desired region with
low adversarial risk should exclude any image from the dense clusters  otherwise the expansion of
such a region will quickly cover the whole cluster. In other words  a desirable subset within CR(T )
should be ∞ away (in (cid:96)∞ norm) from all the dense image clusters  which motivates our method to
cover the dense image clusters using hyperrectangles and treat the complement of them as error set.
More speciﬁcally  our algorithm (for pseudocode  see Algorithm 1 in Appendix B) starts by sorting
all the training images in an ascending order based on the (cid:96)1-norm distance to the k-th nearest
neighbour with k = 50  and then obtains T hyperrectangular image clusters by performing k-means
clustering (Hartigan & Wong  1979) on the top-q densest images  where the metric is chosen as (cid:96)1
and the maximum iterations is set as 30. Finally  we perform a binary search over q ∈ [0  1]  where
we set δbin = 0.005 as the stopping criteria  to obtain the best robust subset (lowest adversarial risk)
in CR(T ) with empirical measure at least α.
Results. We choose α to reﬂect the best accuracy achieved by state-of-the-art classiﬁers  using
α = 0.01 and ∞ ∈ {0.1  0.2  0.3  0.4} for MNIST and selecting appropriate values to represent the
best typical results on the other datasets (see Table 1). Given the number of hyperrectangles  T   we
obtain the resulting error region using the proposed algorithm on the training dataset  and tune T for
the minimum adversarial risk on the testing dataset.
Figure 1 shows the learning curves regarding risk and adversarial risk for two speciﬁc experimental
settings (similar results are obtained under other experimental settings  see Appendix C.3). Figure
1(a) suggests that as we increase the initial covered percentage q  both risk and adversarial risk of the
corresponding error region decrease. This supports our use of binary search on q in Algorithm 1. On

4The adversarial risk of an error region E simply refers to the adversarial risk of fE.

7

0%20%40%60%80%100%initial covered percentage q0%20%40%60%80%100%risk / adversarial riskrisk (training)risk (testing)adversarial risk (training)adversarial risk (testing)020406080number of hyperrectangles T0%10%20%30%adversarial riskmnist trainingmnist testingcifar trainingcifar testingTable 1: Summary of the main results using our method for different settings with (cid:96)∞ perturbations.

Dataset

α

MNIST

0.01

CIFAR-10

0.05

∞

0.1
0.2
0.3
0.4

2/255
4/255
8/255
16/255

T

5
10
10
10

10
20
40
75

Best q

0.662
0.660
0.629
0.598

0.680
0.688
0.734
0.719

1.22 ± 0.11
1.12 ± 0.13
1.12 ± 0.12
1.15 ± 0.09
5.32 ± 0.21
5.59 ± 0.25
5.55 ± 0.21
5.16 ± 0.25

1.23 ± 0.12
1.11 ± 0.10
1.15 ± 0.13
1.21 ± 0.09
5.72 ± 0.25
6.05 ± 0.40
5.94 ± 0.34
5.28 ± 0.23

Empirical Risk (%)
training
testing

Empirical AdvRisk (%)
training
testing

3.65 ± 0.29
5.76 ± 0.38
7.34 ± 0.38
9.89 ± 0.57
7.29 ± 0.20
11.43 ± 0.24
13.69 ± 0.19
19.77 ± 0.22

3.64 ± 0.30
5.89 ± 0.44
7.24 ± 0.38
9.92 ± 0.60
8.13 ± 0.26
13.66 ± 0.33
18.13 ± 0.30
28.83 ± 0.46

Table 2: Comparisons between our method and the existing adversarially trained robust classiﬁers
under different settings. We use the Risk and AdvRisk for robust training methods to denote the
standard test error and attack success rate reported in literature. The AdvRisk reported for our method
can be seen as an estimated lower bound of adversarial risk for existing classiﬁers.

Dataset

Strength (metric)

Method

Empirical Risk

Empirical AdvRisk

MNIST

MNIST

∞ = 0.3

2 = 1.5

CIFAR-10

∞ = 8/255

Madry et al. (2018)

Ours (T = 10  α = 0.012)

1.20%

1.35% ± 0.08%

10.70%

8.28% ± 0.22%

Schott et al. (2019)

Ours (T = 20  α = 0.01)

Madry et al. (2018)

Ours (T = 40  α = 0.127)

1.00%
1.08%

20.00%
2.12%

12.70%

14.22% ± 0.46%

52.96%

29.21% ± 0.35%

the other hand  as can be seen from Figure 1(b)  overﬁtting with respect to adversarial risk becomes
signiﬁcant as we increase the number of hyperrectangles. According to the adversarial risk curve for
testing data  the optimal value of T is selected as T = 10 for MNIST (∞ = 0.3) and T = 40 for
CIFAR-10 (∞ = 8/255).
Table 1 summarizes the optimal parameters  the empirical risk and adversarial risk of the correspond-
ing error region on both training and testing datasets for each experimental setting (see Appendix C.1
for similar results on Fashion-MNIST and SVHN). Since the k-means algorithm does not guarantee
global optimum  we repeat our method for 10 runs with random restarts in terms of the best param-
eters  then report both the mean and the standard deviation. Our experiments provide examples of
rather robust error regions for real image datasets. For instance  in Table 1 we have a case where
the measure of the resulting error region increases from 5.94% to 18.13% after expansion with
∞ = 8/255 on CIFAR-10 dataset. This means that there could potentially be a classiﬁer with 5.94%
risk and 18.13% adversarial risk  but the-state-of-the-art robust classiﬁer has empirically-measured
adversarial risk 52.96% (Madry et al.  2018).
Noticing that the risk lower threshold α = 0.05 is much lower than the empirical risk 12.70% of the
adversarially-trained robust model reported in Madry et al. (2018)  we further measure the empirical
concentration on MNIST and CIFAR-10 using our method with α set to be the same as the reported
standard test error in Madry et al. (2018)  which is demonstrated in Table 2. In particular  we show
that the gap between the attack success rate of Madry et al.’s classiﬁer (10.70%) and our estimated
best-achievable adversarial risk (8.28%) is quite small on MNIST  suggesting that the robustness of
Madry et al.’s classiﬁer is actually close to the intrinsic robustness. In sharp contrast  the gap becomes
signiﬁcantly larger on CIFAR-10: 29.21% for our estimate  while 52.96% for the reported attack
success rate in Madry et al. (2018). Regardless of the difference  this gap cannot be explained by
the concentration of measure phenomenon  suggesting there may still be room for developing more
robust classiﬁers  or that other inherent reasons impede learning a more robust classiﬁer.

8

Table 3: Comparisons between different methods for ﬁnding robust error region with (cid:96)2 perturbations.

Dataset

α

2

MNIST

0.01

1.58
3.16
4.74

Gilmer et al. (2018b)
Risk
1.18%
1.18%
1.18%

AdvRisk
3.92%
9.73%
23.40%

CIFAR-10

0.05

0.2453
0.4905
0.9810

5.27%
5.27%
5.27%

5.58%
5.93%
6.47%

Our Method
AdvRisk
Risk
2.19%
1.07%
1.02%
4.15%
1.07% 10.09%

5.16%
5.14%
5.12%

5.53%
5.83%
6.56%

T

20
20
20

5
5
5

4.2 Experiments for (cid:96)2

For (cid:96)2 adversaries  Theorem 3.10 guarantees the asymptotic convergence of the empirical concentra-
tion function characterized by union of balls B(T ) towards the actual concentration. Thus  it remains
to solve the corresponding optimization problem (3). Similar to (cid:96)∞  we propose an empirical method
to search for desirable robust error regions under (cid:96)2 perturbations. From a high level  our algorithm
(for pseudocode  see Algorithm 2 in Appendix B) places T balls in a sequential manner  and searches
for the best possible placement using a greedy approach at each time. Since enumerating all the
possible ball centers is infeasible  we restrict the choice of the center to be the set of training data
points. Our method keeps two sets of indices: one for the initial coverage and one for the coverage
after expansion  and updates them when we ﬁnd the optimal placement  i.e. the ball centered at some
training data point that has the minimum expansion with respect to both sets.
We compare our empirical method for ﬁnding robust error regions characterized by a union of balls
with the hyperplane-based approach (Gilmer et al.  2018b) on MNIST and CIFAR-10. In particular 
the risk threshold α is set to be the same as the case of (cid:96)∞  and the adversarial strength 2 is chosen
such that the volume of an (cid:96)2 ball with radius 2 is roughly the same as the (cid:96)∞ ball with radius ∞ 

using the conversion rule 2 =(cid:112)n/π · ∞ as in Wong et al. (2018). Table 3 summarizes the optimal

parameters  the testing risk and adversarial risk (see Appendix C.2 for more detailed results  including
for other datasets) of the trained error regions using different methods  where we tune the number of
balls T for our method.
Our results show that there exist rather robust (cid:96)2 error regions for real image datasets. For example 
the measure of the resulting error region using our method only increases by 0.69% (from 5.14% to
5.83%) after expansion with 2 = 0.4905 on CIFAR-10. Compared with Gilmer et al. (2018b)  our
method is able to ﬁnd regions with signiﬁcantly smaller adversarial risk (around half the adversarial
risk of regions found by their method) on MNIST  while attaining comparable error region robustness
on CIFAR-10. Nevertheless  the adversarial risk attained by state-of-the-art robust classiﬁers against
(cid:96)2 perturbations is much higher than these reported rates (see Table 2 for a comparison with the best
robust classiﬁer against (cid:96)2 perturbations proposed in Schott et al. (2019)).

5 Conclusion

To understand whether theoretical results showing limits of intrinsic robustness for natural distribu-
tions apply to concrete datasets  we developed a general framework to measure the concentration
of an unknown distribution through its i.i.d. samples and a carefully-selected collection of subsets.
Our experimental results suggest that the concentration of measure phenomenon is not the sole
reason behind vulnerability of the existing classiﬁers to adversarial examples. In other words  recent
impossibility results (Gilmer et al.  2018b; Fawzi et al.  2018; Mahloujifar et al.  2019; Shafahi et al. 
2019) should not cause us to lose hope in the possibility of ﬁnding more robust classiﬁers.

Acknowledgements. This work was partially funded by an award from the National Science Founda-
tion SaTC program (Center for Trustworth Machine Learning  #1804603)  an NSF CAREER award
(CCF-1350939)  and support from Baidu  Intel  and Amazon.

9

References
Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning  2018.

Idan Attias  Aryeh Kontorovich  and Yishay Mansour. Improved generalization bounds for robust

learning. In Algorithmic Learning Theory  2019.

Arjun Nitin Bhagoji  Daniel Cullina  and Prateek Mittal. Lower bounds on adversarial robustness

from optimal transport. In Advances in Neural Information Processing Systems  2019.

Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.

Pattern Recognition  84:317–331  2018.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy  2017.

Daniel Cullina  Arjun Nitin Bhagoji  and Prateek Mittal. PAC-learning in the presence of adversaries.

In Advances in Neural Information Processing Systems  2018.

Luc Devroye  László Györﬁ  and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition.

Springer Science & Business Media  2013.

Dimitrios Diochnos  Saeed Mahloujifar  and Mohammad Mahmoody. Adversarial risk and robust-
ness: General deﬁnitions and implications for the uniform distribution. In Advances in Neural
Information Processing Systems  2018.

David Eisenstat and Dana Angluin. The VC dimension of k-fold union. Information Processing

Letters  101(5):181–184  2007.

Alhussein Fawzi  Hamza Fawzi  and Omar Fawzi. Adversarial vulnerability for any classiﬁer. In

Advances in Neural Information Processing Systems  2018.

Justin Gilmer  Ryan P Adams  Ian Goodfellow  David Andersen  and George E Dahl. Motivating the

rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732  2018a.

Justin Gilmer  Luke Metz  Fartash Faghri  Samuel S Schoenholz  Maithra Raghu  Martin Wattenberg 

and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774  2018b.

Ian Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations  2015.

Sven Gowal  Krishnamurthy Dvijotham  Robert Stanforth  Rudy Bunel  Chongli Qin  Jonathan
Uesato  Relja Arandjelovic  Timothy Mann  and Pushmeet Kohli. Scalable veriﬁed training for
provably robust image classiﬁcation. In IEEE International Conference on Computer Vision
(ICCV)  2019.

John A Hartigan and Manchek A Wong. A K-means clustering algorithm. Journal of the Royal

Statistical Society. Series C (Applied Statistics)  28(1):100–108  1979.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  University of Toronto  2009.

Ryen Krusinga  Sohil Shah  Matthias Zwicker  Tom Goldstein  and David Jacobs. Understanding
the (un)interpretability of natural image distributions using generative models. arXiv preprint
arXiv:1901.01499  2019.

Yann LeCun  Corinna Cortes  and CJ Burges. MNIST handwritten digit database. http://yann.lecun.

com/exdb/mnist  2010.

Michel Ledoux. The Concentration of Measure Phenomenon. Number 89 in Mathematical Surveys and

Monographs. American Mathematical Society  2001.

Paul Lévy. Problèmes concrets d’analyse fonctionnelle  volume 6. Gauthier-Villars Paris  1951.

10

Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In International Conference on Learning Representations 
2018.

Saeed Mahloujifar  Dimitrios I Diochnos  and Mohammad Mahmoody. The curse of concentration in robust
learning: Evasion and poisoning attacks from concentration of measure. In AAAI Conference on Artiﬁcial
Intelligence  2019.

Vitali D Milman and Gideon Schechtman. Asymptotic theory of ﬁnite dimensional normed spaces. Springer-

Verlag  1986.

Omar Montasser  Steve Hanneke  and Nathan Srebro. VC classes are adversarially robustly learnable  but only

improperly. Proceedings of Machine Learning Research  99:1–19  2019.

Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. In NeurIPS Workshop on Deep Learning and Unsupervised
Feature Learning  2011.

Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nati Srebro. Exploring generalization in deep

learning. In Advances in Neural Information Processing Systems  2017.

Stephen M Omohundro. Five balltree construction algorithms.

Berkeley  1989.

International Computer Science Institute

Nicolas Papernot  Patrick McDaniel  Xi Wu  Somesh Jha  and Ananthram Swami. Distillation as a defense to
adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy  2016.

Fabian Pedregosa  Gaël Varoquaux  Alexandre Gramfort  Vincent Michel  Bertrand Thirion  Olivier Grisel 
Mathieu Blondel  Andreas Müller  Joel Nothman  Gilles Louppe  Peter Prettenhofer  Ron Weiss  Vincent
Dubourg  Jake Vanderplas  Alexandre Passos  David Cournapeau  Matthieu Brucher  Matthieu Perrot  and
Édouard Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 
October 2011.

Aditi Raghunathan  Jacob Steinhardt  and Percy Liang. Certiﬁed defenses against adversarial examples. In

International Conference on Learning Representations  2018.

Aditi Raghunathan  Sang Michael Xie  Fanny Yang  John C Duchi  and Percy Liang. Adversarial training can

hurt generalization. arXiv preprint arXiv:1906.06032  2019.

Lukas Schott  Jonas Rauber  Matthias Bethge  and Wieland Brendel. Towards the ﬁrst adversarially robust neural

network model on MNIST. In International Conference on Learning Representations  2019.

Clayton D Scott and Robert D Nowak. Learning minimum volume sets. Journal of Machine Learning Research 

7(Apr):665–704  2006.

Ali Shafahi  W. Ronny Huang  Christoph Studer  Soheil Feizi  and Tom Goldstein. Are adversarial examples

inevitable? In International Conference on Learning Representations  2019.

Aman Sinha  Hongseok Namkoong  and John Duchi. Certiﬁable distributional robustness with principled

adversarial training. In International Conference on Learning Representations  2018.

Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow  and Rob
Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations 
2014.

Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications

Mathématiques de l’Institut des Hautes Etudes Scientiﬁques  81(1):73–205  1995.

Dimitris Tsipras  Shibani Santurkar  Logan Engstrom  Alexander Turner  and Aleksander Madry. Robustness

may be at odds with accuracy. In International Conference on Learning Representations  2019.

Shiqi Wang  Yizheng Chen  Ahmed Abdou  and Suman Jana. MixTrain: Scalable training of formally robust

neural networks. arXiv preprint arXiv:1811.02625  2018.

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial

polytope. In International Conference on Machine Learning  2018.

Eric Wong  Frank R Schmidt  Jan Hendrik Metzen  and Zico Kolter. Scaling provable adversarial defenses. In

Advances in Neural Information Processing Systems  2018.

11

Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine

learning algorithms. arXiv preprint arXiv:1708.07747  2017.

Dong Yin  Ramchandran Kannan  and Peter Bartlett. Rademacher complexity for adversarially robust general-

ization. In International Conference on Machine Learning  2019.

Huan Zhang  Hongge Chen  Chaowei Xiao  Bo Li  Duane Boning  and Cho-Jui Hsieh. Towards stable and

efﬁcient training of veriﬁably robust neural networks. arXiv preprint arXiv:1906.06316  2019.

12

,Saeed Mahloujifar
Xiao Zhang
Mohammad Mahmoody
David Evans