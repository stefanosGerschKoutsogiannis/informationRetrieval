2017,Inverse Filtering for Hidden Markov Models,This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular  given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations  ii) estimate the observation likelihoods  and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations  and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally  the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.,Inverse Filtering for Hidden Markov Models

Robert Mattila

Department of Automatic Control
KTH Royal Institute of Technology

rmattila@kth.se

Vikram Krishnamurthy

Cornell Tech

Cornell University

vikramk@cornell.edu

Cristian R. Rojas

Department of Automatic Control
KTH Royal Institute of Technology

crro@kth.se

Bo Wahlberg

Department of Automatic Control
KTH Royal Institute of Technology

bo@kth.se

Abstract

This paper considers a number of related inverse ﬁltering problems for hidden
Markov models (HMMs). In particular  given a sequence of state posteriors and
the system dynamics; i) estimate the corresponding sequence of observations 
ii) estimate the observation likelihoods  and iii) jointly estimate the observation
likelihoods and the observation sequence. We show how to avoid a computation-
ally expensive mixed integer linear program (MILP) by exploiting the algebraic
structure of the HMM ﬁlter using simple linear algebra operations  and provide
conditions for when the quantities can be uniquely reconstructed. We also propose a
solution to the more general case where the posteriors are noisily observed. Finally 
the proposed inverse ﬁltering algorithms are evaluated on real-world polysomno-
graphic data used for automatic sleep segmentation.

1

Introduction

The hidden Markov model (HMM) is a cornerstone of statistical modeling [1–4]. In it  a latent (i.e. 
hidden) state evolves according to Markovian dynamics. The state of the system is only indirectly
observed via a sensor that provides noisy observations. The observations are sampled independently 
conditioned on the state of the system  according to observation likelihood probabilities. Of paramount
importance in many applications of HMMs is the classical stochastic ﬁltering problem  namely:

Given observations from an HMM with known dynamics and observation likelihood
probabilities  compute the posterior distribution of the latent state.

Throughout the paper  we restrict our attention to discrete-time ﬁnite observation-alphabet HMMs.
For such HMMs  the solution to the ﬁltering problem is a recursive algorithm known as the HMM
ﬁlter [1  4].
In this paper  we consider the inverse of the above problem. In particular  our aim is to provide
solutions to the following inverse ﬁltering problems:

Given a sequence of posteriors (or  more generally  noisily observed posteriors)
from an HMM with known dynamics  compute (estimate) the observation likelihood
probabilities and/or the observations that generated the posteriors.

To motivate these problems  we give several possible applications of our results below.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Applications The underlying idea of inverse ﬁltering problems (“inform me about your state
estimate and I will know your sensor characteristics  including your measurements”) has potential
applications in  e.g.  autonomous calibration of sensors  fault diagnosis  and detecting Bayesian
behavior in agents. In model-based fault-detection [5  6]  sensor information together with solutions
to related inverse ﬁltering problems are used to detect abnormal behavior. (As trivial examples; i)
if the true sequence of observations is known from a redundant sensor  it can be compared to the
reconstructed sequence; if there is a miss-match  something is wrong  or ii) if multiple data batches
are available  then change detection can be performed on the sequence of reconstructed observation
likelihoods.) They are also of relevance in a revealed preference context in microeconomics where
the aim is to detect expected utility maximization behavior of an agent; estimating the posterior given
the agent’s actions is a crucial step  see  e.g.  [7].
Recent advances in wearables and smart-sensor technology have led to consumer grade products
(smart watches with motion and heart-beat monitoring  sleep trackers  etc.) that produce vast amounts
of personal data by performing state estimation. This information can serve as an indicator of health 
ﬁtness and stress. It may be very difﬁcult  or even impossible  to access the raw sensor data since the
sensor and state estimator usually are tightly integrated and encapsulated in intelligent sensor systems.
Inverse ﬁltering provides a framework for reverse engineering and performing fault detection of such
sensors. In Section 5  we demonstrate our proposed solutions on a system that performs automatic
sequencing of sleep stages based on electroencephalogram (EEG) data – the outputs of such an
automatic system are exactly posteriors over the different sleep stages [8].
Another important application of the inverse ﬁltering problem arises in electronic warfare and cyber-
physical security. How can one determine how accurate an enemy’s sensors are? In such problems 
the state of the underlying Markov chain is usually known (a probing sequence)  and one observes
actions taken by the enemy which are based on ﬁltered posterior distributions. The aim is to estimate
the observation likelihood probabilities of the enemy  i.e.  determine how accurate its sensors are.

Our contributions
It is possible to obtain a solution to the inverse ﬁltering problem for HMMs by
employing a brute-force approach (see Section 2.3) – essentially by testing observations from the
alphabet  and at the same time ﬁnding system parameters consistent with the data. However  this
leads to a computationally expensive combinatorial optimization problem. Instead  we demonstrate
in this paper an efﬁcient solution based on linear algebra by exploiting the inherent structure of the
problem and the HMM ﬁlter. In particular  the contributions of this paper are three-fold:

1. We propose analytical solutions to three inverse ﬁltering problems for HMMs that avoid
computationally expensive mixed integer linear program (MILP) formulations. Moreover 
we establish theorems guaranteeing unique identiﬁability.

2. We consider the setting where the output of the HMM ﬁlter is corrupted by noise  and

propose an inverse ﬁltering algorithm based on clustering.

3. We evaluate the algorithm on real-world data for automatic segmentation of the sleep cycle.

Related work There are only two known cases where the optimal ﬁlter allows a ﬁnite dimensional
characterization: the HMM ﬁlter for (discrete) HMMs  and the Kalman ﬁlter [9  10] for linear
Gaussian state-space models. Inverse ﬁltering problems for the Kalman ﬁlter have been considered
in  e.g.  [5  6  10]  however  inverse ﬁltering for HMMs has  to the best knowledge of the authors 
received much less attention.
The inverse ﬁltering problem has connections to a number of other inverse problems in various ﬁelds.
For example  in control theory  the fundamental inverse optimal control problem  whose formulation
dates back to 1964 [11]  studies the question: given a system and a policy  for what cost criteria is the
policy optimal? In microeconomic theory  the related problem of revealed preferences [12] asks the
question: given a set of decisions made by an agent  is it possible to determine if a utility is being
maximized  and if so  which?
In machine learning  there are clear connections to  e.g.  apprenticeship learning  imitation learning
and inverse reinforcement learning  see  e.g.  [13–17]  which recently have received much attention.
In these  the reward function of a Markov decision process (MDP) is learned by observing an expert
demonstrating the task that an agent wants to learn to perform.
The key difference between these works and our work is the set of system parameters we aim to learn.

2

2 Preliminaries

In this section  we formulate the inverse ﬁltering problems  discuss how these can be solved using
combinatorial optimization  and state our assumptions formally. With regards to notation  all vectors
† denotes the
are column vectors  unless transposed. The vector 1 is the vector of all ones.
Moore–Penrose pseudoinverse.

2.1 Hidden Markov models (HMMs) and the HMM ﬁlter
We consider a discrete-time ﬁnite observation-alphabet HMM. Denote its state at time k as xk ∈
{1  . . .   X} and the corresponding observation yk ∈ {1  . . .   Y }. The underlying Markov chain xk
evolves according to the row-stochastic transition probability matrix P ∈ RX×X  where [P ]ij =
Pr[xk+1 = j|xk = i]. The initial state x0 is sampled from the probability distribution π0 ∈ RX 
where [π0]i = Pr[x0 = i]. The noisy observations of the underlying Markov chain are obtained from
the row-stochastic observation likelihood matrix B ∈ RX×Y   where [B]ij = Pr[yk = j|xk = i] are
the observation likelihood probabilities. We denote the columns of the observation likelihood matrix
as {bi}Y
In the classical stochastic ﬁltering problem  the aim is to compute the posterior distribution πk ∈ RX
of the latent state (Markov chain  in our case) at time k  given observations from the system up to
time k. The HMM ﬁlter [1  4] computes these posteriors via the following recursive update:

i=1  i.e.  B = [b1 . . . bY ].

(1)
initialized by π0  where [πk]i = Pr[xk = i|y1  . . .   yk] is the posterior distribution at time k 
Byk = diag(byk ) ∈ RX×X  and {yk}N

k=1 is a set of observations.

πk =

 

Byk P T πk−1
1T Byk P T πk−1

2.2

Inverse HMM ﬁltering problem formulations

k=0

k=0

k=1.

(cid:8)P  B {πk}N
(cid:8)P {yk}N
data D = (cid:8)P {πk}N

The inverse ﬁltering problem for HMMs is not a single problem – multiple variants can be formulated
depending on what information is available a priori. We pose and consider a number of variations of
increasing levels of generality depending on what data we can extract from the sensor system. To
restrict the scope of the paper  we assume throughout that the transition matrix P is known  and is the
same in both the system and the HMM ﬁlter (i.e  we do not consider miss-matched HMM ﬁltering
problems). Formally  the inverse ﬁltering problems considered in this paper are as follows:
Problem 1 (Inverse ﬁltering problem with unknown observations). Consider the known data D =
the observations {yk}N
Problem 2 (Inverse ﬁltering problem with unknown sensor). Consider the known data D =

(cid:9)  where the posteriors have been generated by an HMM-ﬁlter sensor. Reconstruct
(cid:9)  where the posteriors have been generated by an HMM-ﬁlter sensor. Recon-
(cid:9)  where the posteriors have been generated by an HMM-ﬁlter sensor.

Combining these two formulations yields the general problem:
Problem 3 (Inverse ﬁltering problem with unknown sensor and observations). Consider the known
Reconstruct the observations {yk}N
Finally  we consider the more general setting where the posteriors we obtain are corrupted by noise
(due to  e.g.  quantization  measurement or model uncertainties). In particular  we consider the case
where the following sequence of noisy posteriors is obtained over time:

k=1 and the observation likelihood matrix B.

struct the observation likelihood matrix B.

k=1 {πk}N

k=0

˜πk = πk + noise 

Consider the data D =(cid:8)P {˜πk}N

(2)
from the sensor system. We state directly the generalization of Problem 3 (the corresponding
generalizations of Problems 1 and 2 follow as special-cases):
Problem 4 (Noise-corrupted inverse ﬁltering problem with unknown sensor and observations).
ﬁlter sensor  but we obtain noise-corrupted measurements ˜πk. Estimate the observations {yk}N
and the observation likelihood matrix B.

(cid:9)  where the posteriors πk have been generated by an HMM-

k=0

k=1

3

2.3

Inverse ﬁltering as an optimization problem

It is possible to formulate Problems 1-4 as optimization problems of increasing levels of generality.
As a ﬁrst step  rewrite the HMM ﬁlter equation (1) as:1

(1) ⇐⇒ bT

yk

P T πk−1πk = diag(byk )P T πk−1.

(3)

In Problem 3 we need to ﬁnd what observation occurred at each time instant (a combinatorial
problem)  and at the same time reconstruct an observation likelihood matrix consistent with the
data. To be consistent with the data  equation (3) has to be satisﬁed. This feasibility problem can be
formulated as the following mixed-integer linear program (MILP):

N(cid:88)

{yk}N

min
k=1 {bi}Y
s.t.

i=1

(cid:107)bT

yk

P T πk−1πk − diag(byk )P T πk−1(cid:107)∞

k=1

yk ∈ {1  . . .   Y } 
bi ≥ 0 
[b1 . . . bY ]1 = 1 

for k = 1  . . .   N 
for i = 1  . . .   Y 

(4)

where the choice of norm is arbitrary since for noise-free data it is possible to exactly ﬁt observations
and an observation likelihood matrix. In Problem 1  the bi:s are dropped as optimization variables and
the problem reduces to an integer program (IP). In Problem 2  where the sequence of observations is
known  the problem reduces to a linear program (LP) .
Despite the ease of formulation  the down-side of this approach is that  even though Problems 1 and 2
are computationally tractable  the MILP-formulation of Problem 3 can become computationally very
expensive for larger data sets. In the following sections  we will outline how the problems can be
solved efﬁciently by exploiting the structure of the HMM ﬁlter.

2.4 Assumptions

Before providing solutions to Problems 1-4  we state the assumptions that the HMMs in this paper
need to satisfy to guarantee unique solutions. The ﬁrst assumption serves as a proxy for ergodicity of
the HMM and the HMM ﬁlter – it is a common assumption in statistical inference for HMMs [18  4].
Assumption 1 (Ergodicity). The transition matrix P and the observation matrix B are elementwise
(strictly) positive.

The second assumption is a natural rank assumption on the observation likelihoods. The assumption
says that the conditional distribution of any observation is not a linear combination of the conditional
distributions of any other observations.
Assumption 2 (Distinguishable observation likelihoods). The observation likelihood matrix B is full
column rank.

We will see that this assumption can be relaxed to the following assumption in problems where only
the sequence of observations is to be reconstructed:
Assumption 3 (Non-parallel observation likelihoods). No pair of columns of the observation likeli-
hood matrix B is colinear  i.e.  bi (cid:54)= κbj for any real number κ and any i (cid:54)= j.
Without Assumption 3  it is impossible to distinguish between observation i and observation j. Note
also that Assumption 2 implies Assumption 3.

3 Solution to the inverse ﬁltering problem for HMMs in absence of noise

In this section  we detail our solutions to Problems 1-3. We ﬁrst provide the following two useful
lemmas that will be key to the solutions for Problems 1-4. They give an alternative characterization
of the HMM-ﬁlter update equation. (Note that all proofs are in the supplementary material.)

1Multiplication by the denominator is allowed under Assumption 1 – see below.

4

Lemma 1. The HMM-ﬁlter update equation (3) can equivalently be written

πk(P T πk−1)T − diag(P T πk−1)

byk = 0.

(cid:16)

(cid:17)

The second lemma characterizes the solutions to (5).
Lemma 2. Under Assumption 1  the nullspace of the X × X matrix

πk(P T πk−1)T − diag(P T πk−1)

is of dimension one for k > 1.

3.1 Solution to the inverse ﬁltering problem with unknown observations

(5)

(6)

In the formulation of Problem 1  we assumed that the observation likelihoods B were known  and
aimed to reconstruct the sequence of observations from the posterior data. Equation (5) constrains
which columns of the observation matrix B that are consistent with the update of the posterior vector
at each time instant. Formally  any sequence

ˆyk ∈(cid:8)y ∈ {1  . . .   Y } :(cid:0)πk(P T πk−1)T − diag(P T πk−1)(cid:1) by = 0(cid:9) 

(7)

for k = 1  . . .   N  is consistent with the HMM ﬁlter posterior updates. (Recall that by denotes column
y of the observation matrix B.) Since the problems (7) are decoupled in time k  they can trivially be
solved in parallel.
Theorem 1. Under Assumptions 1 and 3  the set in the right-hand side of equation (7) is a singleton 
and is equal to the true observation  i.e. 

ˆyk = yk 

(8)

for k > 1.

3.2 Solution to the inverse ﬁltering problem with unknown sensor

The second inverse ﬁltering problem we consider is when the sequence of observations is known  but
the observation likelihoods B are unknown (Problem 2). This problem can be solved by exploiting
Lemmas 1 and 2.
Computing a basis for the nullspace of the coefﬁcient matrix in formulation (5) of the HMM ﬁlter
recovers  according to Lemmas 1 and 2  the direction of one column of B. In particular  the direction
of the column corresponding to observation yk  i.e.  byk. From such basis vectors  we can construct a
matrix C ∈ RX×Y where the yth column is aligned with by. Note that to be able to fully construct
this matrix  every observation from the set {1  . . .   Y } needs to have been observed at least once.
Due to being basis vectors for nullspaces  the columns of C are only determined up to scalings  so
we need to exploit the structure of the observation matrix B to properly normalize them. To form an
estimate ˆB from C  we employ that the observation likelihood matrix is row-stochastic. This means
that we should rescale each column:

ˆB = C diag(α)

(9)

for some α ∈ RY   such that ˆB1 = 1. Details are provided in the following theorem.
Theorem 2. If Assumption 1 holds  and every possible observation has been observed (i.e.  that
{1  . . .   Y } ⊂ {yk}N

k=1)  then:

i) there exists α ∈ RY such that ˆB = B 
ii) if Assumption 2 holds  then the choice of α is unique  and ˆB is equal to B. In particular 

α = C†1.

5

3.3 Solution to the inverse ﬁltering problem with unknown sensor and observations

Finally  we turn to the general formulation in which we consider the combination of the previous
two problems: both the sequence of observations and the observation likelihoods are unknown
(Problem 3). Again  the solution follows from Lemmas 1 and 2. Note that there will be a degree of
freedom since we can arbitrarily relabel each observation and correspondingly permute the columns
of the observation likelihood matrix.
As in the solution to Problem 2  computing a basis vector  say ¯ck  for the nullspace of the coefﬁcient
matrix in equation (5) recovers the direction of one column of the B matrix. However  since the
sequence of observations is unknown  we do not know which column. To circumvent this  we
concatenate such basis vectors in a matrix2

¯C = [¯c2 . . . ¯cN ] ∈ RX×(N−1).

(10)

For sufﬁciently large N – essentially when every possible observation has been processed by the
HMM ﬁlter – the matrix ¯C in (10) will contain Y columns out of which no pair is colinear (due
to Assumption 3). All the columns that are parallel correspond to one particular observation. Let
{σ1  . . .   σY } be the indices of Y such columns  and construct

using the selection matrix

C = ¯CΣ

Σ = [eσ1 . . . eσY ] ∈ R(N−1)×Y  

(11)

(12)

where ei is the ith Cartesian basis vector.
Lemma 3. Under Assumption 1 and Assumption 3  the expected number of samples needed to be
able to construct the selection matrix Σ is upper-bounded by

β−1 (1 + 1/2 + ··· + 1/Y )  

(13)

where B ≥ β > 0 elementwise.

With C constructed in (11)  we have obtained the direction of each column of the observation matrix.
However  as before  they need to be properly normalized. For this  we exploit the sum-to-one property
of the observation matrix as in the previous section. Let

ˆB = C diag(α) 

(14)

for α ∈ RY   such that ˆB1 = 1. Details on how to ﬁnd α are provided in the theorem below.
This solves the ﬁrst part of the problem  i.e.  reconstructing the observation matrix. Secondly  to
recover the sequence of observations  take

(cid:111)

ˆyk ∈(cid:110)

y ∈ {1  . . .   Y } : ˆby = κ¯ck for some real number κ

 

(15)

for k > 1. In words; check which columns of ˆB that the nullspace of the HMM ﬁlter coefﬁcient-
matrix (6) is colinear with at each time instant.
Theorem 3. If Assumptions 1 and 3 hold  and the number of samples N is sufﬁciently large – see
Lemma 3 – then:

i) there exists α ∈ RY in equation (14) such that ˆB = BP  where P is a permutation matrix.
ii) the set on the right-hand side of equation (15) is a singleton. Moreover  the reconstructed
observations ˆyk are  up to relabellings corresponding to P  equal to the true observations
yk.

iii) if Assumption 2 holds  then the choice of α is unique  and ˆB = BP. In particular  α = C†1.

2We start with ¯c2  since we make no assumption on the positivity of π0 – see the proof of Lemma 2.

6

4 Solution to the inverse ﬁltering problem for HMMs in presence of noise

In this section  we discuss the more general setting where the posteriors obtained from the sensor
system are corrupted by noise. We will see that this problem naturally ﬁts in a clustering framework
since every posterior update will provide us with a noisy estimate of the direction of one column of
the observation likelihood matrix. We consider an additive noise model of the following form:
Assumption 4 (Noise model). The posteriors are corrupted by additive noise wk:

˜πk = πk + wk 

(16)

such that 1T ˜πk = 1 and ˜πk > 0.

This noise model is valid  for example  when each observed posterior vector has been subsequently
renormalized after noise that originates from quantization or measurement errors has been added.
In the solution proposed in Section 3.3 for the noise-free case  the matrix ¯C in equation (10) was
constructed by concatenating basis vectors for the nullspaces of the coefﬁcient matrix in equation (5).
With perturbed posterior vectors  the corresponding system of equations becomes

˜πk(P T ˜πk−1)T − diag(P T ˜πk−1)

(17)
where ˜ck is now a perturbed (and scaled) version of byk. That this equation is valid is guaranteed by
the generalization of Lemma 2:
Lemma 4. Under Assumptions 1 and 4  the nullspace of the matrix
˜πk(P T ˜πk−1)T − diag(P T ˜πk−1)

˜ck = 0 

(18)

(cid:16)

(cid:17)

is of dimension one for k > 1.
Remark 1. In case Assumption 4 does not hold  the problem can instead be interpreted as a perturbed
eigenvector problem. The vector ˜ck should then be taken as the eigenvector corresponding to the
smallest eigenvalue.

Lemma 4 says that we can construct a matrix ˜C (analogous to ¯C in Section 3.3) by concatenating the
basis vectors from the one-dimensional nullspaces in (17). Due to the perturbations  every solution to
equation (17) will be a perturbed version of the solution to the corresponding noise-free version of the
equation. This means that it will not be possible to construct a selection matrix Σ as was done for ¯C
in equation (12). However  because there are only Y unique solutions to the noise-free equations (5) 
it is natural to circumvent this (assuming that the perturbations are small) by clustering the columns
of ˜C into Y clusters. As the columns of ˜C are only unique up to scaling  the clustering has to be
performed with respect to their angular separations (using  e.g.  the spherical k-means algorithm
[19]).
Let C ∈ RX×Y be the matrix of the Y centroids resulting from running a clustering algorithm on the
columns of ˜C. Each centroid can be interpreted as a noisy estimate of one column of the observation
likelihood matrix. To obtain a properly normalized estimate of the observation likelihood matrix  we
take
(19)
where A ∈ RY ×Y . Note that  since C now contains noisy estimates of the directions of the columns
of the observation likelihood matrix  we are not certain to be able to properly normalize it by purely
rescaling each column (i.e.  taking A to be a diagonal matrix as was done in Sections 3.2 and 3.3). A
logical choice is the solution to the following LP 

ˆB = CA 

(20)
which tries to minimize the off-diagonal elements of A. The resulting rescaling matrix A guarantees
that ˆB = CA is a proper stochastic matrix (non-negative and has row-sum equal to one)  as well as
that the discrepancy between the directions of the columns of C and ˆB are minimized.
The second part of the problem – reconstructing the sequence of observations – follows naturally
from the clustering algorithm; an estimate of the sequence is obtained by checking to what cluster the
solution ˜ck of equation (17) belongs in for each time instant.

(cid:12)(cid:12)[A]ij

(cid:12)(cid:12)

max
i(cid:54)=j
CA ≥ 0 
CA1 = 1 

min

A∈RY ×Y

s.t.

7

5 Experimental results for sleep segmentation

In this section  we illustrate the inverse ﬁltering problem on real-world data.

Background Roughly one third of a person’s life is spent sleeping. Sleep disorders are becoming
more prevalent and  as public awareness has increased  the usage of sleep trackers is becoming
wide-spread. The example below illustrates how the inverse ﬁltering formulation and associated
algorithms can be used as a step in real-time diagnosis of failure of sleep-tracking medical equipment.
During the course of sleep  a human transitions through ﬁve different sleep stages [20]: wake  S1 
S2  slow wave sleep (SWS) and rapid eye movement (REM). An important part of sleep analysis is
obtaining a patient’s evolution over these sleep stages. Manual sequencing from all-night polysomno-
graphic (PSG) recordings (including  e.g.  electroencephalogram (EEG) readings) can be performed
according to the Rechtschaffen and Kales (R&K) rules by well-trained experts [8  20]. However 
this is costly and laborious  so several works  e.g.  [8  20  21]  propose automatic sequencing based
on HMMs. These systems usually output a posterior distribution over the sleep stages  or provide a
Viterbi path.
A malfunction of such an automatic system could have problematic consequences since medical
decisions would be based on faulty information. The inverse ﬁltering problem arises naturally for
such reasons of fault-detection. Joint knowledge of the transition matrix can be assumed  since it is
possible to obtain  from public sources  manually labeled data from which an estimate of P can be
computed.

Setup A version of the automatic sleep-staging system in [8  20] was implemented. The mean
frequency over the 0-30 Hz band of the EEG (over C3-A2 or C4-A1  according to the international
10-20 system) was used as observations. These readings were encoded to ﬁve symbols using a vector-
quantization based codebook. The model was trained on data from nine patients in the PhysioNet
CAP Sleep Database [22  23]. The model was then evaluated on another patient – see Fig. 1 – over
one full-night of sleep. The manually labeled stages according to K&R-rules are dashed-marked in
the ﬁgure. To summarize the resulting posterior distributions over the sleep stages  we plot the mean
state estimate when equidistant numbers have been assigned to each state.
For the inverse ﬁltering  the full posterior vectors were elementwise corrupted by Gaussian noise of
standard deviation σ  and projected back to the simplex (to ensure a valid posterior probability vector)
– simulating a noisy reading from the automatic system. A total of one hundred noise realizations
were simulated. The noise can be a manifestation of measurement or quantization noise in the sensor
system  or noise related to model uncertainties (in this case  an error in the transition probability
matrix P ).

Results After permuting the labels of the observations  the error in the reconstructed observation
likelihood matrix  as well as the fraction of correctly reconstructed observations  were computed. This
is illustrated in Fig. 2. For the 1030 quantized EEG samples from the patient  the entire procedure
takes less than one second on a 2.0 Ghz Intel Core 2 Duo processor system.

REM
SWS
S2
S1
WAKE

0

1

2

3

4

5

6

7

8

hours since bedtime

Figure 1: One night of sleep in which polysomnographic (PSG) observation data has been manually
processed by an expert sleep analyst according to the R&K rules to obtain the sleep stages (
).
The posterior distribution over the sleep stages  resulting from an automatic sleep-staging system  has
been summarized to a mean state estimate (

).

8

n
o
i
t
c
a
r
f

1

0.5
0.2

Correctly recovered observations

10−8

10−6

10−4
noise σ

10−2

100

F
(cid:107)
P
B
−
ˆB
(cid:107)
n
P
m

i

100
10−2
10−4

Error in B

10−8

10−6

10−4
noise σ

10−2

100

Figure 2: Result of inverse ﬁltering for various noise standard deviations σ. The vector of posterior
probabilities is perturbed elementwise with Gaussian noise. Right: Error in the recovered observation
likelihood matrix after permuting the columns to ﬁnd the best match to the true matrix. Left: Fraction
of correctly reconstructed observations. As the signal-to-noise ratio increases  the inverse ﬁltering
algorithm successfully reconstructs the sequence of observations and estimates the observation
likelihoods.

From Fig. 2  we can see that as the variance of the noise decreases  the left hand side of equation
(17) converges to that of equation (5) and the true quantities are recovered. On the other extreme 
as the signal-to-noise ratio becomes small  the estimated sequence of observations tends to that of
a uniform distribution at 1/Y = 0.2. This is because the clusters in ˜C become heavily intertwined.
The discontinuous nature of the solution of the clustering algorithm is apparent by the plateau-like
behaviour in the middle of the scale – a few observations linger on the edge of being assigned to the
correct clusters.
In conclusion  the results show that it is possible to estimate the observation sequence processed by
the automatic sleep-staging system  as well as  its sensor’s speciﬁcations. This is an important step in
performing fault detection for such a device: for example  using several nights of data  it is possible
to perform change detection on the observation likelihoods to detect if the sleep monitoring device
has failed.

6 Conclusions

In this paper  we have considered several inverse ﬁltering problems for HMMs. Given posteriors
from an HMM ﬁlter (or more generally  noisily observed posteriors)  the aim was to reconstruct the
observation likelihoods and also the sample path of observations. It was shown that a computationally
expensive solution based on combinatorial optimization can be avoided by exploiting the algebraic
structure of the HMM ﬁlter. We provided solutions to the inverse ﬁltering problems  as well as
theorems guaranteeing unique identiﬁability. The more general case of noise-corrupted posteriors
was also considered. A solution based on clustering was proposed and evaluated on real-world data
based on a system for automatic sleep-staging from EEG readings.
In the future  it would be interesting to consider other variations and generalizations of inverse
ﬁltering. For example  the case where the system dynamics are unknown and need to be estimated  or
when only actions based on the ﬁltered distribution can be observed.

Acknowledgments

This work was partially supported by the Swedish Research Council under contract 2016-06079 
the U.S. Army Research Ofﬁce under grant 12346080 and the National Science Foundation under
grant 1714180. The authors would like to thank Alexandre Proutiere for helpful comments during the
preparation of this work.

References
[1] V. Krishnamurthy  Partially Observed Markov Decision Processes. Cambridge  UK: Cambridge

University Press  2016.

9

[2] L. Rabiner  “A tutorial on hidden Markov models and selected applications in speech recogni-

tion ” Proceedings of the IEEE  vol. 77  pp. 257–286  Feb. 1989.

[3] R. J. Elliott  J. B. Moore  and L. Aggoun  Hidden Markov Models: Estimation and Control.

New York  NY: Springer  1995.

[4] O. Cappé  E. Moulines  and T. Rydén  Inference in Hidden Markov Models. New York  NY:

Springer  2005.

[5] F. Gustafsson  Adaptive ﬁltering and change detection. New York: Wiley  2000.
[6] J. Chen and R. J. Patton  Robust Model-Based Fault Diagnosis for Dynamic Systems. Boston 

MA: Springer  1999.

[7] A. Caplin and M. Dean  “Revealed preference  rational inattention  and costly information

acquisition ” The American Economic Review  vol. 105  no. 7  pp. 2183–2203  2015.

[8] A. Flexerand  G. Dorffner  P. Sykacekand  and I. Rezek  “An automatic  continuous and
probabilistic sleep stager based on a hidden Markov model ” Applied Artiﬁcial Intelligence 
vol. 16  pp. 199–207  Mar. 2002.

[9] D. Koller and N. Friedman  Probabilistic graphical models: principles and techniques. Cam-

bridge  MA: MIT Press  2009.

[10] B. Anderson and J. Moore  Optimal Filtering. Englewood Cliffs  NJ: Prentice-Hall  1979.
[11] R. E. Kalman  “When is a linear control system optimal ” Journal of Basic Engineering  vol. 86 

no. 1  pp. 51–60  1964.

[12] H. R. Varian  Microeconomic analysis. New York: Norton  3rd ed.  1992.
[13] D. Hadﬁeld-Menell  S. J. Russell  P. Abbeel  and A. Dragan  “Cooperative inverse reinforcement

learning ” in Advances in Neural Information Processing Systems  2016.

[14] J. Choi and K.-E. Kim  “Nonparametric Bayesian inverse reinforcement learning for multiple

reward functions ” in Advances in Neural Information Processing Systems  2012.

[15] E. Klein  M. Geist  B. Piot  and O. Pietquin  “Inverse Reinforcement Learning through Structured

Classiﬁcation ” in Advances in Neural Information Processing Systems  2012.

[16] S. Levine  Z. Popovic  and V. Koltun  “Nonlinear inverse reinforcement learning with gaussian

processes ” in Advances in Neural Information Processing Systems  2011.

[17] A. Ng  “Algorithms for inverse reinforcement learning ” in Proceedings of the 17th International

Conference on Machine Learning (ICML’00)  pp. 663–670  2000.

[18] L. E. Baum and T. Petrie  “Statistical inference for probabilistic functions of ﬁnite state Markov

chains ” The annals of mathematical statistics  vol. 37  no. 6  pp. 1554–1563  1966.

[19] C. Buchta  M. Kober  I. Feinerer  and K. Hornik  “Spherical k-means clustering ” Journal of

Statistical Software  vol. 50  no. 10  pp. 1–22  2012.

[20] S.-T. Pan  C.-E. Kuo  J.-H. Zeng  and S.-F. Liang  “A transition-constrained discrete hidden
Markov model for automatic sleep staging ” BioMedical Engineering OnLine  vol. 11  no. 1 
p. 52  2012.

[21] Y. Chen  X. Zhu  and W. Chen  “Automatic sleep staging based on ECG signals using hidden
Markov models ” in Proceedings of the 37th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC)  pp. 530–533  2015.

[22] A. L. Goldberger  L. A. Amaral  L. Glass  J. M. Hausdorff  P. C. Ivanov  R. G. Mark  J. E.
Mietus  G. B. Moody  C.-K. Peng  and H. E. Stanley  “Physiobank  physiotoolkit  and physionet ”
Circulation  vol. 101  no. 23  pp. e215–e220  2000.

[23] M. G. Terzano  L. Parrino  A. Sherieri  R. Chervin  S. Chokroverty  C. Guilleminault  M. Hir-
shkowitz  M. Mahowald  H. Moldofsky  A. Rosa  and others  “Atlas  rules  and recording
techniques for the scoring of cyclic alternating pattern (CAP) in human sleep ” Sleep medicine 
vol. 2  no. 6  pp. 537–553  2001.

10

,Robert Mattila
Cristian Rojas
Vikram Krishnamurthy
Bo Wahlberg
Ruibing Hou
Hong Chang
Bingpeng MA
Shiguang Shan
Xilin Chen