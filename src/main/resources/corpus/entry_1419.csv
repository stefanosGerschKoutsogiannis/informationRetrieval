2019,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks.
Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training  which typically suffer from issues such as label leaking as noted in recent works.
Differently  the proposed approach generates adversarial images for training through feature scattering in the latent space  which is unsupervised in nature and avoids label leaking. More importantly  this new approach generates perturbed images in a collaborative fashion  taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach  through extensively experiments on different datasets compared with state-of-the-art approaches.,Defense Against Adversarial Attacks Using

Feature Scattering-based Adversarial Training

Haichao Zhang∗
Horizon Robotics
hczhang1@gmail.com

Jianyu Wang
Baidu Research
wjyouch@gmail.com

Abstract

We introduce a feature scattering-based adversarial training approach for improving
model robustness against adversarial attacks. Conventional adversarial training
approaches leverage a supervised scheme (either targeted or non-targeted) in gener-
ating attacks for training  which typically suffer from issues such as label leaking
as noted in recent works. Differently  the proposed approach generates adversarial
images for training through feature scattering in the latent space  which is unsu-
pervised in nature and avoids label leaking. More importantly  this new approach
generates perturbed images in a collaborative fashion  taking the inter-sample
relationships into consideration. We conduct analysis on model robustness and
demonstrate the effectiveness of the proposed approach through extensively exper-
iments on different datasets compared with state-of-the-art approaches. Code is
available: https://github.com/Haichao-Zhang/FeatureScatter.

1

Introduction

While breakthroughs have been made in many ﬁelds such as image classiﬁcation leveraging deep
neural networks  these models could be easily fooled by the so call adversarial examples [55  4].
In terms of the image classiﬁcation  an adversarial example for a natural image is a modiﬁed
version which is visually indistinguishable from the original but causes the classiﬁer to produce a
different label prediction [4  55  24]. Adversarial examples have been shown to be ubiquitous beyond
classiﬁcation  ranging from object detection [64  18] to speech recognition [11  9].
Many encouraging progresses been made towards improving model robustness against adversarial
examples under different scenarios [58  36  33  67  72  16  71]. Among them  adversarial train-
ing [24  36] is one of the most popular technique [2]  which conducts model training using the
adversarially perturbed images in place of the original ones. However  several challenges remain to
be addressed. Firstly  some adverse effects such as label leaking is still an issue hindering adversarial
training [32]. Currently available remedies either increase the number of iterations for generating the
attacks [36] or use classes other than the ground-truth for attack generation [32  65  61]. Increasing
the attack iterations will increase the training time proportionally while using non-ground-truth
targeted approach cannot fully eliminate label leaking. Secondly  previous approaches for both
standard and adversarial training treat each training sample individually and in isolation w.r.t.other
samples. Manipulating each sample individually this way neglects the inter-sample relationships and
does not fully leverage the potential for attacking and defending  thus limiting the performance.
Manifold and neighborhood structure have been proven to be effective in capturing the inter-sample
relationships [51  22]. Natural images live on a low-dimensional manifold  with the training and
testing images as samples from it [26  51  44  56]. Modern classiﬁers are over-complete in terms of
parameterizations and different local minima have been shown to be equally effective under the clean
image setting [14]. However  different solution points might leverage different set of features for

∗Work done while with Baidu Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

prediction. For learning a well-performing classiﬁer on natural images  it sufﬁces to simply adjust
the classiﬁcation boundary to intersect with this manifold at locations with good separation between
classes on training data  as the test data will largely reside on the same manifold [28]. However 
the classiﬁcation boundary that extends beyond the manifold is less constrained  contributing to
the existence of adversarial examples [56  59]. For examples  it has been pointed out that some
clean trained models focus on some discriminative but less robust features  thus are vulnerable to
adversarial attacks [28  29]. Therefore  the conventional supervised attack that tries to move feature
points towards this decision boundary is likely to disregard the original data manifold structure.
When the decision boundary lies close to the manifold for its out of manifold part  adversarial
perturbations lead to a tilting effect on the data manifold [56]; at places where the classiﬁcation
boundary is far from the manifold for its out of manifold part  the adversarial perturbations will move
the points towards the decision boundary  effectively shrinking the data manifold. As the adversarial
examples reside in a large  contiguous region and a signiﬁcant portion of the adversarial subspaces
is shared [24  19  59  40]  pure label-guided adversarial examples will clutter as least in the shared
adversarial subspace. In summary  while these effects encourage the model to focus more around the
current decision boundary  they also make the effective data manifold for training deviate from the
original one  potentially hindering the performance.
Motived by these observations  we propose to shift the previous focus on the decision boundary
to the inter-sample structure. The proposed approach can be intuitively understood as generating
adversarial examples by perturbing the local neighborhood structure in an unsupervised fashion and
then performing model training with the generated adversarial images. The overall framework is
shown in Figure 1. The contributions of this work are summarized as follows:
• we propose a novel feature-scattering approach for generating adversarial images for adversarial
• we present an adversarial training formulation which deviates from the conventional minimax
• we analyze the proposed approach and compare it with several state-of-the-art techniques  with

formulation and falls into a broader category of bilevel optimization;

extensive experiments on a number of standard benchmarks  verifying its effectiveness.

training in a collaborative and unsupervised fashion;

2 Background

2.1 Adversarial Attack  Defense and Adversarial Training

Adversarial examples  initially demonstrated in [4  55]  have attracted great attention recently [4 
24  58  36  2  5]. Szegedy et al. pointed out that CNNs are vulnerable to adversarial examples
and proposed an L-BFGS-based algorithm for generating them [55]. A fast gradient sign method
(FGSM) for adversarial attack generation is developed and used in adversarial training in [24]. Many
variants of attacks have been developed later [41  8  54  62  7  6]. In the mean time  many efforts
have been devoted to defending against adversarial examples [38  37  63  25  33  50  53  46  35].
Recently  [2] showed that many existing defence methods suffer from a false sense of robustness
against adversarial attacks due to gradient masking  and adversarial training [24  32  58  36] is one of
the effective defense method against adversarial attacks. It improves model robustness by solving a
minimax problem as [24  36]:

(1)

min

θ

(cid:2) max
x(cid:48)∈Sx L(x(cid:48)  y; θ)(cid:3)
(cid:0)xt + α · sign(cid:0)

∇xL(xt  y; θ)(cid:1)(cid:1) 

where the inner maximization essentially generates attacks while the outer minimization corresponds
to minimizing the “adversarial loss” induced by the inner attacks [36]. The inner maximization can
be solved approximately  using for example a one-step approach such as FGSM [24]  or a multi-step
projected gradient descent (PGD) method [36]

xt+1 = PSx

(2)
where PSx(·) is a projection operator projecting the input into the feasible region Sx. In the PGD
approach  the original image x is randomly perturbed to some point x0 within B(x  )  the -cube
around x  and then goes through several PGD steps with a step size of α as shown in Eqn.(2).
Label leaking [32] and gradient masking [43  58  2] are some well-known issues that hinder the
adversarial training [32]. Label leaking occurs when the additive perturbation is highly correlated with
the ground-truth label. Therefore  when it is added to the image  the network can directly tell the class
label by decoding the additive perturbation without relying on the real content of the image  leading

2

clean
batch

pert.
batch

label

Figure 1: Feature Scattering-based Adversarial Training Pipeline. The adversarial perturbations
are generated collectively by feature scattering  i.e.  maximizing the feature matching distance
between the clean samples {xi} and the perturbed samples {x(cid:48)j}. The model parameters are updated
by minimizing the cross-entropy loss using the perturbed images {x(cid:48)j} as the training samples.
to higher adversarial accuracy than the clean image during training. Gradient masking [43  58  2]
refers to the effect that the adversarially trained model learns to “improve” robustness by generating
less useful gradients for adversarial attacks  which could be by-passed with a substitute model for
generating attacks  thus giving a false sense of robustness [2].

2.2 Different Distances for Feature and Distribution Matching

Euclidean distance is arguably one of the most commonly used metric for measuring the distance
between a pair of points. When it comes to two sets of points  it is natural to accumulate the individual
pairwise distance as a measure of distance between the two sets  given the proper correspondence.
Alternatively  we can view each set as an empirical distribution and measure the distance between
them using Kullback-Leibler (KL) or Jensen-Shannon (JS) divergence. The challenge for learning
with KL or JS divergence is that no useful gradient is provided when the two empirical distributions
have disjoint supports or have a non-empty intersection contained in a set of measure zero [1  49].
The optimal transport (OT) distance is an alternative measure of the distance between distributions
with advantages over KL and JS in the scenarios mentioned earlier. The OT distance between two
probability measures µ and ν is deﬁned as:

D(µ  ν) = inf

γ∈Π(µ ν)

E(x y)∼γ c(x  y)  

(3)

where Π(µ  ν) denotes the set of all joint distributions γ(x  y) with marginals µ(x) and ν(y)  and
c(x  y) is the cost function (Euclidean or cosine distance). Intuitively  D(µ  ν) is the minimum cost
that γ has to transport from µ to ν. It provides a weaker topology than many other measures  which
is important for applications where the data typically resides on a low dimensional manifold of the
input embedding space [1  49]  which is the case for natural images. It has been widely applied
to many tasks  such as generative modeling [21  1  49  20  10]  auto-encoding [57] and dictionary
learning [47]. For comprehensive historical and computational perspective of OT  we refer to [60  45].

3 Feature Scattering-based Adversarial Training
3.1 Feature Matching and Feature Scattering
Feature Matching. Conventional training treats training data as i.i.d samples from a data distribution 
overlooking the connections between samples. The same assumption is used when generating
adversarial examples for training  with the direction for perturbing a sample purely based on the
direction from the current data point to the decision boundary  regardless of other samples. While
effective  it disregards the inter-relationship between different feature points  as the adversarial
perturbation is computed individually for each sample  neglecting any collective distributional
property. Furthermore  the supervised generation of the attacks makes the generated perturbations
highly biases towards the decision boundary  as shown in Figure 2. This is less desirable as it might
neglect other directions that are crucial for learning robust models [28  17] and leads to label leaking
due to high correlation between the perturbation and the decision boundary.

3

 	 	CT{xi}f✓(xi){x0j}f✓(x0j)CijOTsolver{yj}horse	{y0j}{yj}min✓max{x0j}{fi}{f0j}g(f0j)DL(a)

(b)

(c)

Figure 2: Illustration Example of Different Perturbation Schemes. (a) Original data. Perturbed
data using (b) supervised adversarial generation method and (c) the proposed feature scattering 
which is an unsupervised method. The overlaid boundary is from the model trained on clean data.

The idea of leveraging inter-sample relationship for learning dates back to the seminal work of [51  22 
48]. This type of local structure is also exploited in this work  but for adversarial perturbation. The
quest of local structure utilization and seamless integration with the end-to-end-training framework
naturally motivates an OT-based soft matching scheme  using the OT-distance as in Eqn.(3). We
written as µ =(cid:80)n
consider OT between discrete distributions hereafter as we mainly focus on applying the OT distance
on image features. Speciﬁcally  consider two discrete distributions µ  ν ∈ P(X)  which can be
(cid:80)
i ui=(cid:80)
  with δx the Dirac function centered on x.2 The
i=1∈∆n belong to the n-dimensional simplex  i.e. 
weight vectors µ={ui}n
i vi=1  as both µ and ν are probability distributions. Under such a setting  computing the
OT distance as deﬁned in Eqn.(3) is equivalent to solving the following network-ﬂow problem

i=1 uiδxi and ν =(cid:80)n
n(cid:88)

i=1 viδx(cid:48)i
i=1∈∆n and ν ={vi}n
n(cid:88)

(cid:104)T  C(cid:105)

(4)

Tij · c(xi  x
(cid:48)
j) = min

T∈Π(u v)

D(µ  ν) = min

T∈Π(u v)

i=1

j=1

where Π(u  v) = {T ∈ Rn×n
+ |T1n = u  T(cid:62)1n = v}. 1n is an n-dimensional all-one vector. (cid:104)· ·(cid:105)
represents the Frobenius dot-product. C is the transport cost matrix such that Cij = c(xi  x(cid:48)j). In
this work  the transport cost is deﬁned as the cosine distance between image features:

c(xi  x

(cid:48)

j) = 1 − fθ(xi)(cid:62)fθ(x(cid:48)
j)
j)(cid:107)2
(cid:107)fθ(xi)(cid:107)2(cid:107)fθ(x(cid:48)

= 1 −

i f(cid:48)
f(cid:62)
(cid:107)fi(cid:107)2(cid:107)f(cid:48)

j

j(cid:107)2

(5)

where fθ(·) denotes the feature extractor with parameter θ. We implement fθ(·) as the deep neural
network upto the softmax layer. We can now formally deﬁne the feature matching distance as follows.
Deﬁnition 1. (Feature Matching Distance) The feature matching distance between two set of images
is deﬁned as D(µ  ν)  the OT distance between empirical distributions µ and ν for the two sets.
Note that the feature-matching distance is also a function of θ (i.e. Dθ) when fθ(·) is used for
extracting the features in the computation of the ground distance as in Eqn.(5). We will simply use
the notation D in the following when there is no danger of confusion to minimize notional clutter .
Feature Scattering. Based on the feature matching distance deﬁned above  we can formulate
proposed feature scattering method as follows:

n(cid:88)

n(cid:88)

ˆν = arg max

ν∈Sµ D(µ  ν)  µ =

i=1

i=1

uiδxi   ν =

viδx(cid:48)i

.

(6)

This can be intuitively interpreted as maximizing the feature matching distance between the original
and perturbed empirical distributions with respect to the inputs subject to domain constraints Sµ

Sµ = {

viδzi | zi ∈ B(xi  ) ∩ [0  255]d} 

empirical distribution as µ =(cid:80)
producing a perturbed empirical distribution ν =(cid:80)

where B(x  ) = {z|(cid:107)z − x(cid:107)∞ ≤ } denotes the (cid:96)∞-cube with center x and radius . Formally  we
present the notion of feature scattering as follows.
Deﬁnition 2. (Feature Scattering) Given a set of clean data {xi}  which can be represented as an
i ui = 1  the feature scattering procedure is deﬁned as
i vi = 1 by maximizing D(µ  ν) 
2The two discrete distributions could be of different dimensions; here we present the exposition assuming the

the feature matching distance between µ and ν  subject to domain and budget constraints.

with(cid:80)

i viδx(cid:48)i

same dimensionality to avoid notion clutter.

i

(cid:88)
i uiδxi with(cid:80)

4

Remark 1. As the feature scattering is performed on a batch of samples leveraging inter-sample
structure  it is more effective as adversarial attacks compared to structure-agnostic random perturba-
tion while is less constrained than supervisedly generated perturbation which is decision boundary
oriented and suffers from label leaking. Empirical comparisons will be provided in Section 5.

3.2 Adversarial Training with Feature Scattering

We leverage feature scattering for adversarial training  with the mathmatical formulation as follows

n(cid:88)

i=1

minθ

1
n

Lθ(x(cid:48)i  yi)

s.t. ν∗ (cid:44) n(cid:88)

viδx(cid:48)i

= max

ν∈Sµ D(µ  ν).

(7)

i=1

The proposed formulation deviates from the conventional minimax formulation for adversarial
training [24  36]. More speciﬁcally  it can be regarded as an instance of the more general bilevel
optimization problem [13  3]. Feature scattering is effective for adversarial training scenario as there
is a requirements of more data [52]. Feature scattering promotes data diversity without drastically
altering the structure of the data manifold as in the conventional supervised approach  with label
leaking as one manifesting phenomenon. Secondly  the feature matching distance couples the samples
within the batch together  therefore the generated adversarial attacks are produced collaboratively by
taking the inter-sample relationship into consideration. Thirdly  feature scattering implicitly induces
a coupled regularization (detailed below) on model training  leveraging the inter-sample structure for
joint regularization.
The proposed approach is equivalent
i=1 Lθ(xi  yi) +
λRθ(x1 ···   xn)  consisting of the conventional loss Lθ(xi  yi) on the original data  and a regu-
inputs  i.e.  Rθ(x1 ···   xn)(cid:54)=(cid:80)
larization term Rθ coupled over the inputs. It ﬁrst highlights the unique property of the proposed
feature scattering approach that it induces an effective regularization term that is coupled over all
i R(cid:48)θ(xi). This implies that the model leverages information from
all inputs in a joint fashion for learning  offering the opportunity of collaborative regularization
leveraging inter-sample relationships. Second  the usage of a function (Dθ) different from Lθ for
inducing Rθ offers more ﬂexibilities in the effective regularization; moreover  no label information is
(cid:80)
incorporated in Dθ  thus avoiding potential label leaking as in the conventional case when ∂Lθ (xi yi)
is highly correlated with yi. Finally  in the case when Dθ is separable over inputs and takes the form
of a supervised loss  e.g.  Dθ ≡
i Lθ(xi  yi)  the proposed approach reduces to the conventional
adversarial training setup [24  36]. The overall procedure for the proposed approach is in Algorithm 1.

to the minimization of a loss 

(cid:80)n

∂xi

1
n

Algorithm 1 Feature Scattering-based Adversarial Training

i=1 ∼S do
i uiδxi 

Input: dataset S  training epochs K  batch size n  learning rate γ  budget   attack iterations T
for k = 1 to K do
for random batch {xi  yi}n
feature scattering (maximizing feature matching distance D w.r.t. ν):
for t = 1 to T do
· x(cid:48)i ← PSx

initialization: µ =(cid:80)
(cid:0)x(cid:48)i +  · sign(cid:0)
(cid:80)n
i=1 ∇θL(x(cid:48)i  yi; θ)

end for
adversarial training (updating model parameters):
· θ ← θ − γ · 1

ν =(cid:80)
∇x(cid:48)iD(µ  ν)(cid:1)(cid:1)

  x(cid:48)i ∼ B(xi  )

∀i = 1 ···   n  ν =(cid:80)

i viδx(cid:48)i

i viδx(cid:48)i

n

end for

end for
Output: model parameter θ.

4 Discussions
Manifold-based Defense [34  37  15  27]. [34  37  27] proposed to defend by projecting the perturbed
image onto a proper manifold. [15] used a similar idea of manifold projection but approximated
this step with a nearest neighbor search against a web-scale database. Differently  we leverage the
manifold in the form of inter-sample relationship for the generation of the perturbations  which
induces an implicit regularization of the model when used in the adversarial training framework.
While defense in [34  37  15  27] is achieved by shrinking the perturbed inputs towards the manifold 
we expand the manifold using feature scattering to generate perturbed inputs for adversarial training.

5

Inter-sample Regularization [70  30  39]. Mixup [70] generates training examples by linear inter-
polation between pairs of natural examples  thus introducing an linear inductive bias in the vicinity of
training samples. Therefore  the model is expected to reduce the amount of undesirable oscillations
for off-manifold samples. Logit pairing [30] augments the original training loss with a “pairing”
loss  which measures the difference between the logits of clean and adversarial images. The idea
is to suppress spurious logits responses using the natural logits as a reference. Similarly  virtual
adversarial training [39] proposed a regularization term based on the KL divergence of the prediction
probability of original and adversarially perturbed images. In our model  the inter-sample relationship
is leveraged for generating the adversarial perturbations  which induces an implicit regularization
term in the objective function that is coupled over all input samples.
Wasserstein GAN and OT-GAN [1  49  10]. Generative Adversarial Networks (GAN) is a family
of techniques that learn to capture the data distribution implicitly by generating samples directly [23].
It originally suffers from the issues of instability of training and mode collapsing [23  1]. OT-
related distances [1  12] have been used for overcoming the difﬁculties encountered in the original
GAN training [1  49]. This technique has been further extended to generating discrete data such as
texts [10]. Different from GANs  which maximizes a discrimination criteria w.r.t.the parameters of
the discriminator for better capturing the data distribution  we maximize a feature matching distance
w.r.t.the perturbed inputs for generating proper training data to improve model robustness.

5 Experiments
Baselines and Implementation Details. Our implementation is based on PyTorch and the code as
well as other related resources are available on the project page.3 We conduct extensive experiments
across several benchmark datasets including CIFAR10 [31]  CIFAR100 [31] and SVHN [42]. We
use Wide ResNet (WRN-28-10) [68] as the network structure following [36]. We compare the
performance of the proposed method with a number of baseline methods  including: i) the model
trained with standard approach using clean images (Standard) [31]  ii) PGD-based approach from
Madry et al. (Madry) [36]  which is one of the most effective defense method [2]  iii) another
recent method performs adversarial training with both image and label adversarial perturbations
(Bilateral) [61]. For training  the initial learning rate γ is 0.1 for CIFAR and 0.01 for SVHN.
We set the number of epochs the Standard and Madry methods as 100 with transition epochs as
{60  90} as we empirically observed the performance of the trained model stabilized before 100
epochs. The training scheduling of 200 epochs similar to [61] with the same transition epochs used as
we empirically observed it helps with the model performance  possibly due to the increased variations
of data via feature scattering. We performed standard data augmentation including random crops
with 4 pixels of padding and random horizontal ﬂips [31] during training. The perturbation budget
of  = 8 is used in training following literature [36]. Label smoothing of 0.5  attack iteration T=1
and Sinkhorn algorithm [12] with regularization of 0.01 is used. For testing  model robustness is
evaluated by approximately computing an upper bound of robustness on the test set  by measuring the
accuracy of the model under different adversarial attacks  including white-box FGSM [24]  PGD [36] 
CW [8] (CW-loss [8] within the PGD framework) attacks and variants of black-box attacks.
5.1 Visual Classiﬁcation Performance Under White-box Attacks
CIFAR10. We conduct experiments on CIFAR10 [31]  which is a popular dataset that is widely
use in adversarial training literature [36  61] with 10 classes  5K training images per class and 10K
test images. We report the accuracy on the original test images (Clean) and under PGD and CW
attack with T iterations (PGDT and CWT ) [36  8]. The evaluation results are summarized in Table 1.
It is observed Standard model fails drastically under different white-box attacks. Madry method
improves the model robustness signiﬁcantly over the Standard model. Under the standard PGD20
attack  it achieves 44.9% accuracy. The Bilateral approach further boosts the performance to
57.5%. The proposed approach outperforms both methods by a large margin  improving over Madry
by 25.6%  and is 13.0% better than Bilateral  achieving 70.5% accuracy under the standard 20
steps PGD attack. Similar patten has been observed for CW metric.
We further evaluate model robustness against PGD attacker under different attack budgets with a
ﬁxed attack step of 20  with the results shown in Figure 3 (a). It is observed that the performance
of Standard model drops quickly as the attack budget increases. The Madry model [36] improves
the model robustness signiﬁcantly across a wide range of attack budgets. The Proposed approach

3https://sites.google.com/site/hczhang1/projects/feature_scattering

6

(a)

(b)

(c)

Figure 3: Model performance under PGD attack with different (a) attack budgets (b) attack iterations.
Madry and Proposed models are trained with the attack iteration of 7 and 1 respectively.

Models

Standard

Madry

Bilateral
Proposed

Clean
95.6
85.7
91.2
90.0

FGSM PGD10
36.9
54.9
70.7
78.4

0.0
45.1

–
70.9

PGD20

0.0
44.9
57.5
70.5

Accuracy under White-box Attack ( = 8)

PGD40

PGD100

0.0
44.8

–
70.3

0.0
44.8
55.2
68.6

CW10 CW20
0.0
0.0
45.7
45.9
56.2
62.4

–
62.6

CW40 CW100
0.0
45.6

0.0
45.4
53.8
60.6

–
62.1

Table 1: Accuracy comparison of the Proposed approach with Standard  Madry [36] and
Bilateral [61] methods on CIFAR10 under different threat models.

further boosts the performance over the Madry model [36] by a large margin under different attack
budgets. We also conduct experiments using PGD attacker with different attack iterations with a
ﬁxed attack budget of 8  with the results shown in Figure 3 (b-c) and also Table 1. It is observed that
both Madry [36] and Proposed can maintain a fairly stable performance when the number of attack
iterations is increased. Notably  the proposed approach consistently outperforms the Madry [36]
model across a wide range of attack iterations. From Table 1  it is also observed that the Proposed
approach also outperforms Bilateral [61] under all variants of PGD and CW attacks. We will use
a PGD/CW attackers with =8 and attack step 20 and 100 in the sequel as part of the threat models.

Models Clean

White-box Attack ( = 8)

FGSM PGD20 PGD100 CW20 CW100

Madry

Standard 97.2 53.0
93.9 68.4
Bilateral 94.1 69.8
Proposed 96.2 83.5

0.3
47.9
53.9
62.9

0.1
46.0
50.3
52.0

0.3
48.7

–
61.3

0.1
47.3
48.9
50.8

Models Clean

White-box Attack ( = 8)

FGSM PGD20 PGD100 CW20 CW100

Madry

Standard 79.0 10.0
59.9 28.5
Bilateral 68.2 60.8
Proposed 73.9 61.0

0.0
22.6
26.7
47.2

0.0
22.3
25.3
46.2

0.0
23.2

–
34.6

0.0
23.0
22.1
30.6

Table 2: Accuracy comparison on (a) SVHN and (b) CIFAR100.

SVHN. We further report results on the SVHN dataset [42]. SVHN is a 10-way house number
classiﬁcation dataset  with 73257 training images and 26032 test images. The additional training
images are not used in experiment. The results are summarized in Table 2(a). Experimental results
show that the proposed method achieves the best clean accuracy among all three robust models and
outperforms other method with a clear margin under both PGD and CW attacks with different number
of attack iterations  demonstrating the effectiveness of the proposed approach.
CIFAR100. We also conduct experiments on CIFAR100 dataset  with 100 classes  50K training
and 10K test images [31]. Note that this dataset is more challenging than CIFAR10 as the number
of training images per class is ten times smaller than that of CIFAR10. As shown by the results in
Table 2(b)  the proposed approach outperforms all baseline methods signiﬁcantly  which is about
20% better than Madry [36] and Bilateral [61] under PGD attack and about 10% better under CW
attack. The superior performance of the proposed approach on this data set further demonstrates the
importance of leveraging inter-sample structure for learning [69].
5.2 Ablation Studies
We investigate the impacts of algorithmic components and more results are in the supplementary ﬁle.
The Importance of Feature Scattering. We empirically verify the effectiveness of feature scat-
tering  by comparing the performances of models trained using different perturbation schemes:
i) Random: a natural baseline approach that randomly perturb each sample within the epsilon neigh-
borhood; ii) Supervised: perturbation generated using ground-truth label in a supervised fashion;
iii) FeaScatter: perturbation generated using the proposed feature scattering method. All other
hyper-parameters are kept exactly the same other than the perturbation scheme used. The results are
summarized in Table 3(a). It is evident that the proposed feature scattering (FeaScatter) approach

7

Attack Budget05101520Accuracy (%)020406080100NaturalMadryProposedAttack Iteration0510Accuracy (%)020406080100Attack Iteration050100Accuracy (%)020406080100dr

dr

dr

s
s
o

l

s
s
o

l

da

s
s
o

l

da

da

da

dr
(b)

dr
(a)
Figure 4: Loss surface visualization in the vicinity of a natural image along adversarial direction
(da) and direction of a Rademacher vector (dr) for (a) Standard (b) Madry (c) Proposed models.
outperforms both Random and Supervised methods  demonstrating its effectiveness. Furthermore 
as it is the major component that is difference from the conventional adversarial training pipeline  this
result suggests that feature scattering is the main contributor to the improved adversarial robustness.

dr
(c)

da

da

White-box Attack ( = 8)

Perturb
Random

Clean
95.3 75.7
Supervised 86.9 64.4
FeaScatter 90.0 78.4

FGSM PGD20 PGD100 CW20 CW100
26.2
50.3
60.6

29.9
56.0
70.5

18.3
54.5
68.6

34.7
51.2
62.4

White-box Attack ( = 8)

Match Clean

Uniform 90.0 71.0
Identity 87.4 66.3
90.0 78.4

OT

FGSM PGD20 PGD100 CW20 CW100
51.4
50.6
60.6

53.2
52.4
62.4

57.1
57.5
70.5

54.7
56.0
68.6

Table 3: (a) Importance of feature-scattering. (b) Impacts of different matching schemes.

The Role of Matching. We further investigate the role of matching schemes within the feature
scattering component by comparing several different schemes: i) Uniform matching  which matches
each clean sample uniformly with all perturbed samples in the batch; ii) Identity matching 
which matches each clean sample to its perturbed sample only; iii) OT-matching: the proposed
approach that assigns soft matches between the clean samples and perturbed samples according to the
optimization criteria. The results are summarized in Table 3(b). It is observed all variants of matching
schemes lead to performances that are on par or better than state-of-the-art methods  implying that
the proposed framework is effective in general. Notably  OT-matching leads to the best results 
suggesting the importance of the proper matching for feature scattering.
The Impact of OT-Solvers. Exact minimization of Eqn.(4) over T is intractable in general [1  49  21 
12]. Here we compare two practical solvers  the Sinkhorn algorithm [12] and the Inexact Proximal
point method for Optimal Transport (IPOT) algorithm [66]. More details on them can be found in the
supplementary ﬁle and [12  66  45]. The results are summarized in Table 4. It is shown that different
instantiations of the proposed approach with different OT-solvers lead to comparable performances 
implying that the proposed approach is effective in general regardless of the choice of OT-solvers.

SVHN

CIFAR100

CIFAR10

70.5
69.9

68.6
67.3

62.4
59.6

46.2
46.3

34.6
32.0

47.2
47.5

50.8
48.4

61.3
57.8

52.0
49.3

62.9
60.0

60.6
56.9

OT-solver
Sinkhorn 90.0 78.4
89.9 77.9

B-Attack

73.9 61.0
74.2 67.3

96.2 83.5
96.0 82.6

Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100
30.6
29.3
IPOT
Table 4: Impacts of OT-solvers. The proposed approach performs well with different OT-solvers.
5.3 Performance under Black-box Attack
To further verify if a degenerate minimum is obtained 
we evaluate the robustness of the model trained with the
proposed approach w.r.t.black-box attacks (B-Attack) fol-
lowing [58]. Two different models are used for generating
test time attacks: i) Undefended: undefended model trained using Standard approach  ii) Siamese:
a robust model from another training session using the proposed approach. As demonstrated by
the results in the table on the right  the model trained with the proposed approach is robust against
different types of black-box attacks  verifying that a non-degenerate solution is learned [58].
Finally  we visualize in Figure 4 the loss surfaces of different models as another level of comparison.
6 Conclusion
We present a feature scattering-based adversarial training method in this paper. The proposed
approach distinguish itself from others by using an unsupervised feature-scattering approach for
generating adversarial training images  which leverages the inter-sample relationship for collaborative
perturbation generation. We show that a coupled regularization term is induced from feature scattering
for adversarial training and empirically demonstrate the effectiveness of the proposed approach
through extensive experiments on benchmark datasets.

PGD20 PGD100 CW20 CW100
88.8
79.8

Undefended 89.0
81.6

88.7
81.0

88.9
80.3

Siamese

8

References
[1] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein generative adversarial networks. In Proceedings of

the 34th International Conference on Machine Learning  2017.

[2] A. Athalye  N. Carlini  and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing

defenses to adversarial examples. In International Conference on Machine learning  2018.

[3] J. F. Bard. Practical Bilevel Optimization: Algorithms and Applications. Springer Publishing Company 

Incorporated  1st edition  2010.

[4] B. Biggio  I. Corona  D. Maiorca  B. Nelson  N. Srndic  P. Laskov  G. Giacinto  and F. Roli. Evasion
attacks against machine learning at test time. In European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases  2013.

[5] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. CoRR 

abs/1712.03141  2017.

[6] W. Brendel  J. Rauber  and M. Bethge. Decision-based adversarial attacks: Reliable attacks against

black-box machine learning models. In International Conference on Learning Representations  2018.

[7] T. B. Brown  D. Mané  A. Roy  M. Abadi  and J. Gilmer. Adversarial patch. CoRR  abs/1712.09665  2017.

[8] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on

Security and Privacy  2017.

[9] N. Carlini and D. A. Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In IEEE

Symposium on Security and Privacy Workshops  2018.

[10] L. Chen  S. Dai  C. Tao  H. Zhang  Z. Gan  D. Shen  Y. Zhang  G. Wang  R. Zhang  and L. Carin.
Adversarial text generation via feature-mover’s distance. In Advances in Neural Information Processing
Systems  2018.

[11] M. Cisse  Y. Adi  N. Neverova  and J. Keshet. Houdini: Fooling deep structured prediction models. In

Advances in Neural Information Processing Systems  2017.

[12] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems  2013.

[13] S. Dempe  V. Kalashnikov  G. A. Prez-Valds  and N. Kalashnykova. Bilevel Programming Problems:
Theory  Algorithms and Applications to Energy Networks. Springer Publishing Company  Incorporated 
2015.

[14] F. Draxler  K. Veschgini  M. Salmhofer  and F. Hamprecht. Essentially no barriers in neural network energy

landscape. In International Conference on Machine Learning  2018.

[15] A. Dubey  L. van der Maaten  Z. Yalniz  Y. Li  and D. Mahajan. Defense against adversarial images using

web-scale nearest-neighbor search. CoRR  abs/1903.01612  2019.

[16] L. Engstrom  B. Tran  D. Tsipras  L. Schmidt  and A. Madry. Exploring the landscape of spatial robustness.

In International Conference on Machine Learning  2019.

[17] C. Etmann  S. Lunz  P. Maass  and C.-B. Schönlieb. On the connection between adversarial robustness and

saliency map interpretability. In International Conference on Machine Learning  2019.

[18] K. Eykholt  I. Evtimov  E. Fernandes  B. Li  A. Rahmati  F. Tramèr  A. Prakash  T. Kohno  and D. Song.

Physical adversarial examples for object detectors. CoRR  abs/1807.07769  2018.

[19] A. Fawzi  S. Moosavi-Dezfooli  P. Frossard  and S. Soatto. Empirical study of the topology and geometry

of deep networks. In IEEE Conference on Computer Vision and Pattern Recognition  2018.

[20] A. Genevay  G. Peyre  and M. Cuturi. GAN and VAE from an optimal transport point of view.

arXiv:1706.01807  2017.

[21] A. Genevay  G. Peyre  and M. Cuturi. Learning generative models with sinkhorn divergences.

International Conference on Artiﬁcial Intelligence and Statistics  2018.

In

In

[22] J. Goldberger  G. E. Hinton  S. T. Roweis  and R. R. Salakhutdinov. Neighbourhood components analysis.

In Advances in Neural Information Processing Systems  2005.

9

[23] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In Advances in Neural Information Processing Systems  2014.

[24] I. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. In International

Conference on Learning Representations  2015.

[25] C. Guo  M. Rana  M. Cissé  and L. van der Maaten. Countering adversarial images using input transforma-

tions. In International Conference on Learning Representations  2018.

[26] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504–507  2006.

[27] A. Ilyas  A. Jalal  E. Asteri  C. Daskalakis  and A. G. Dimakis. The robust manifold defense: Adversarial

training using generative models. CoRR  abs/1712.09196  2017.

[28] A. Ilyas  S. Santurkar  D. Tsipras  L. Engstrom  B. Tran  and A. Madry. Adversarial examples are not bugs 

they are features. In International Conference on Learning Representations  2019.

[29] J.-H. Jacobsen  J. Behrmann  R. Zemel  and M. Bethge. Excessive invariance causes adversarial vulnerabil-

ity. In International Conference on Learning Representations  2019.

[30] H. Kannan  A. Kurakin  and I. J. Goodfellow. Adversarial logit pairing. CoRR  abs/1803.06373  2018.

[31] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report  2009.

[32] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial machine learning at scale.

Conference on Learning Representations  2017.

In International

[33] F. Liao  M. Liang  Y. Dong  and T. Pang. Defense against adversarial attacks using high-level representation

guided denoiser. In Computer Vision and Pattern Recognition  2018.

[34] B. Lindqvist  S. Sugrim  and R. Izmailov. AutoGAN: Robust classiﬁer against adversarial attacks. CoRR 

abs/1812.03405  2018.

[35] X. Liu  M. Cheng  H. Zhang  and C.-J. Hsieh. Towards robust neural networks via random self-ensemble.

In European Conference on Computer Vision  2018.

[36] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models resistant to

adversarial attacks. In International Conference on Learning Representations  2018.

[37] D. Meng and H. Chen. MagNet: a two-pronged defense against adversarial examples. In Proceedings of

the 2017 ACM SIGSAC Conference on Computer and Communications Security  2017.

[38] J. H. Metzen  T. Genewein  V. Fischer  and B. Bischoff. On detecting adversarial perturbations.

International Conference on Learning Representations  2017.

In

[39] T. Miyato  S. Maeda  M. Koyama  and S. Ishii. Virtual adversarial training: a regularization method for

supervised and semi-supervised learning. CoRR  abs/1704.03976  2017.

[40] S. Moosavi-Dezfooli  A. Fawzi  O. Fawzi  and P. Frossard. Universal adversarial perturbations. In CVPR 

2017.

[41] S.-M. Moosavi-Dezfooli  A. Fawzi  and P. Frossard. Deepfool: a simple and accurate method to fool deep

neural networks. In IEEE Conference on Computer Vision and Pattern Recognition  2016.

[42] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural images with
unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 
2011.

[43] N. Papernot  P. D. McDaniel  S. Jha  M. Fredrikson  Z. B. Celik  and A. Swami. The limitations of deep

learning in adversarial settings. CoRR  abs/1511.07528  2015.

[44] S. Park and M. Thorpe. Representing and learning high dimensional data with the optimal transport map
from a probabilistic viewpoint. In IEEE Conference on Computer Vision and Pattern Recognition  2018.

[45] G. Peyré and M. Cuturi. Computational optimal transport. to appear in Foundations and Trends in Machine

Learning  2018.

[46] A. Prakash  N. Moran  S. Garber  A. DiLillo  and J. Storer. Deﬂecting adversarial attacks with pixel

deﬂection. In IEEE Conference on Computer Vision and Pattern Recognition  2018.

10

[47] A. Rolet  M. Cuturi  and G. Peyré. Fast dictionary learning with a smoothed Wasserstein loss.

International Conference on Artiﬁcial Intelligence and Statistics  2016.

In

[48] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neighbourhood

structure. In International Conference on Artiﬁcial Intelligence and Statistics  2007.

[49] T. Salimans  H. Zhang  A. Radford  and D. Metaxas. Improving GANs using optimal transport. In

International Conference on Learning Representations  2018.

[50] P. Samangouei  M. Kabkab  and R. Chellappa. Defense-GAN: Protecting classiﬁers against adversarial

attacks using generative models. In International Conference on Learning Representations  2018.

[51] L. K. Saul  S. T. Roweis  and Y. Singer. Think globally  ﬁt locally: Unsupervised learning of low

dimensional manifolds. Journal of Machine Learning Research  4:119–155  2003.

[52] L. Schmidt  S. Santurkar  D. Tsipras  K. Talwar  and A. Madry. Adversarially robust generalization requires

more data. arXiv preprint arXiv:1804.11285  2018.

[53] Y. Song  T. Kim  S. Nowozin  S. Ermon  and N. Kushman. Pixeldefend: Leveraging generative models to

understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766  2017.

[54] J. Su  D. V. Vargas  and K. Sakurai. One pixel attack for fooling deep neural networks. CoRR 

abs/1710.08864  2017.

[55] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus. Intriguing

properties of neural networks. In International Conference on Learning Representations  2014.

[56] T. Tanay and L. D. Grifﬁn. A boundary tilting persepective on the phenomenon of adversarial examples.

CoRR  abs/1608.07690  2016.

[57] I. Tolstikhin  O. Bousquet  S. Gelly  and B. Scholkopf. Wasserstein auto-encoders. In International

Conference on Learning Representations  2018.

[58] F. Tramèr  A. Kurakin  N. Papernot  D. Boneh  and P. McDaniel. Ensemble adversarial training: Attacks

and defenses. In International Conference on Learning Representations  2018.

[59] F. Tramèr  N. Papernot  I. J. Goodfellow  D. Boneh  and P. D. McDaniel. The space of transferable

adversarial examples. CoRR  abs/1704.03453  2017.

[60] C. Villani. Optimal transport  old and new. Springer  2008.
[61] J. Wang and H. Zhang. Bilateral adversarial training: Towards fast training of more robust models against

adversarial attacks. In IEEE International Conference on Computer Vision  2019.

[62] C. Xiao  B. Li  J. yan Zhu  W. He  M. Liu  and D. Song. Generating adversarial examples with adversarial

networks. In International Joint Conference on Artiﬁcial Intelligence.

[63] C. Xie  J. Wang  Z. Zhang  Z. Ren  and A. Yuille. Mitigating adversarial effects through randomization. In

International Conference on Learning Representations  2018.

[64] C. Xie  J. Wang  Z. Zhang  Y. Zhou  L. Xie  and A. Yuille. Adversarial examples for semantic segmentation

and object detection. In International Conference on Computer Vision  2017.

[65] C. Xie  Y. Wu  L. van der Maaten  A. Yuille  and K. He. Feature denoising for improving adversarial

robustness. arXiv preprint arXiv:1812.03411  2018.

[66] Y. Xie  X. Wang  R. Wang  and H. Zha. A fast proximal point method for Wasserstein distance. In

arXiv:1802.04307  2018.

[67] Z. Yan  Y. Guo  and C. Zhang. Deep defense: Training dnns with improved adversarial robustness. In

Advances in Neural Information Processing Systems  2018.

[68] S. Zagoruyko and N. Komodakis. Wide residual networks. In British Machine Vision Conference  2016.
[69] H. Zhang  H. Chen  Z. Song  D. Boning  inderjit dhillon  and C.-J. Hsieh. The limitations of adversarial

training and the blind-spot attack. In International Conference on Learning Representations  2019.

[70] H. Zhang  M. Cisse  Y. N. Dauphin  and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In

International Conference on Learning Representations  2018.

[71] H. Zhang and J. Wang. Joint adversarial training: Incorporating both spatial and pixel attacks. CoRR 

abs/1907.10737  2019.

[72] H. Zhang and J. Wang. Towards adversarially robust object detection. In IEEE International Conference

on Computer Vision  2019.

11

,Haichao Zhang
Jianyu Wang