2012,Learning Probability Measures with respect to Optimal Transport Metrics,We study the problem of estimating  in the sense of optimal transport metrics  a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics  optimal quantization  and learning theory  we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means)  when used to produce a probability measure derived from the data. In the course of the analysis  we arrive at new lower bounds  as well as probabilistic bounds on the convergence rate of the empirical law of large numbers  which  unlike existing bounds  are applicable to a wide class of measures.,Learning Probability Measures with Respect to

Optimal Transport Metrics

Guillermo D. Canas(cid:63) †
Lorenzo A. Rosasco(cid:63) †
(cid:63) Laboratory for Computational and Statistical Learning - MIT-IIT
† CBCL  McGovern Institute - Massachusetts Institute of Technology

{guilledc lrosasco}@mit.edu

Abstract

We study the problem of estimating  in the sense of optimal transport metrics  a
measure which is assumed supported on a manifold embedded in a Hilbert space.
By establishing a precise connection between optimal transport metrics  optimal
quantization  and learning theory  we derive new probabilistic bounds for the per-
formance of a classic algorithm in unsupervised learning (k-means)  when used to
produce a probability measure derived from the data. In the course of the analysis 
we arrive at new lower bounds  as well as probabilistic upper bounds on the con-
vergence rate of empirical to population measures  which  unlike existing bounds 
are applicable to a wide class of measures.

1

Introduction and Motivation

In this paper we study the problem of learning from random samples a probability distribution
supported on a manifold  when the learning error is measured using transportation metrics.
The problem of learning a probability distribution is classic in statistics  and is typically analyzed
for distributions in X = Rd that have a density with respect to the Lebesgue measure  with total
variation  and L2 among the common distances used to measure closeness of two densities (see for
instance [10  32] and references therein.) The setting in which the data distribution is supported on
a low dimensional manifold embedded in a high dimensional space has only been considered more
recently. In particular  kernel density estimators on manifolds have been described in [36]  and their
pointwise consistency  as well as convergence rates  have been studied in [25  23  18]. A discussion
on several topics related to statistics on a Riemannian manifold can be found in [26].
Interestingly  the problem of approximating measures with respect to transportation distances has
deep connections with the ﬁelds of optimal quantization [14  16]  optimal transport [35] and  as
we point out in this work  with unsupervised learning (see Sec. 4.)
In fact  as described in the
sequel  some of the most widely-used algorithms for unsupervised learning  such as k-means (but
also others such as PCA and k-ﬂats)  can be shown to be performing exactly the task of estimating
the data-generating measure in the sense of the 2-Wasserstein distance. This close relation between
learning theory  and optimal transport and quantization seems novel and of interest in its own right.
Indeed  in this work  techniques from the above three ﬁelds are used to derive the new probabilistic
bounds described below.
Our technical contribution can be summarized as follows:

(a) we prove uniform lower bounds for the distance between a measure and estimates based on
discrete sets (such as the empirical measure or measures derived from algorithms such as k-
means);

(b) we provide new probabilistic bounds for the rate of convergence of empirical to population

measures which  unlike existing probabilistic bounds  hold for a very large class of measures;

1

(c) we provide probabilistic bounds for the rate of convergence of measures derived from k-means

to the data measure.

The structure of the paper is described at the end of Section 2  where we discuss the exact formula-
tion of the problem as well as related previous works.

2 Setup and Previous work
Consider the problem of learning a probability measure ρ supported on a space M  from an i.i.d.
sample Xn = (x1  . . .   xn) ∼ ρn of size n. We assume M to be a compact  smooth d-dimensional
manifold of bounded curvature  with C1 metric and volume measure λM  embedded in the unit ball
of a separable Hilbert space X with inner product (cid:104)· ·(cid:105)  induced norm (cid:107) · (cid:107)  and distance d (for
instance M = Bd
2 (1) the unit ball in X = Rd.) Following [35  p. 94]  let Pp(M) denote the
Wasserstein space of order 1 ≤ p < ∞:

Pp(M) :=

ρ ∈ P (M) :

(cid:107)x(cid:107)pdρ(x) < ∞

(cid:90)

M

(cid:27)

(cid:26)

(cid:110)

of probability measures P (M) supported on M  with ﬁnite p-th moment. The p-Wasserstein dis-
tance

[E(cid:107)X − Y (cid:107)p]1/p : Law(X) = ρ  Law(Y ) = µ

(1)

(cid:111)

Wp(ρ  µ) = inf
X Y

where the random variables X and Y are distributed according to ρ and µ respectively  is the optimal
expected cost of transporting points generated from ρ to those generated from µ  and is guaranteed to
be ﬁnite in Pp(M) [35  p. 95]. The space Pp(M) with the Wp metric is itself a complete separable
metric space [35]. We consider here the problem of learning probability measures ρ ∈ P2(M) 
where the performance is measured by the distance W2.
There are many possible choices of distances between probability measures [13]. Among them 
Wp metrizes weak convergence (see [35] theorem 6.9)  that is  in Pp(M)  a sequence (µi)i∈N of
measures converges weakly to µ iff Wp(µi  µ) → 0 and their p-th order moments converge to that of
µ. There are other distances  such as the L´evy-Prokhorov  or the weak-* distance  that also metrize
weak convergence. However  as pointed out by Villani in his excellent monograph [35  p. 98] 

1. “Wasserstein distances are rather strong  [...]a deﬁnite advantage over the weak-* distance”.
2. “It is not so difﬁcult to combine information on convergence in Wasserstein distance with

some smoothness bound  in order to get convergence in stronger distances.”

Wasserstein distances have been used to study the mixing and convergence of Markov chains [22]  as
well as concentration of measure phenomena [20]. To this list we would add the important fact that
existing and widely-used algorithms for unsupervised learning can be easily extended (see Sec. 4)
to compute a measure ρ(cid:48) that minimizes the distance W2(ˆρn  ρ(cid:48)) to the empirical measure

n(cid:88)

i=1

ˆρn :=

1
n

δxi 

a fact that will allow us to prove  in Sec. 5  bounds on the convergence of a measure induced by
k-means to the population measure ρ.
The most useful versions of Wasserstein distance are p = 1  2  with p = 1 being the weaker of the
two (by H¨older’s inequality  p ≤ q ⇒ Wp ≤ Wq.) In particular  “results in W2 distance are usually
stronger  and more difﬁcult to establish than results in W1 distance” [35  p. 95]. A discussion of
p = ∞ would take us out of topic  since its behavior is markedly different.

2.1 Closeness of Empirical and Population Measures

By the strong law of large numbers  the empirical measure converges almost surely to the population
measure: ˆρn → ρ in the sense of the weak topology [34]. Since weak convergence and convergence
in Wp plus convergence of p-th moments are equivalent in Pp(M)  this means that  in the Wp sense 
the empirical measure ˆρn converges to ρ  as n → ∞. A fundamental question is therefore how fast
the rate of convergence of ˆρn → ρ is.

2

2.1.1 Convergence in expectation
The rate of convergence of ˆρn → ρ in expectation has been widely studied in the past  result-
ing in upper bounds of order EW2(ρ  ˆρn) = O(n−1/(d+2)) [19  8]  and lower bounds of order
EW2(ρ  ˆρn) = Ω(n−1/d) [29] (both assuming that the absolutely continuous part of ρ is ρA (cid:54)= 0 
with possibly better rates otherwise).
More recently  an upper bound of order EWp(ρ  ˆρn) = O(n−1/d) has been proposed [2] by proving
a bound for the Optimal Bipartite Matching (OBM) problem [1]  and relating this problem to the
expected distance EWp(ρ  ˆρn).
In particular  given two independent samples Xn  Yn  the OBM

problem is that of ﬁnding a permutation σ that minimizes the matching cost n−1(cid:80)(cid:107)xi−yσ(i)(cid:107)p [24 

30]. It is not hard to show that the optimal matching cost is Wp(ˆρXn   ˆρYn )p  where ˆρXn   ˆρYn are
the empirical measures associated to Xn  Yn. By Jensen’s inequality  the triangle inequality  and
(a + b)p ≤ 2p−1(ap + bp)  it holds

EWp(ρ  ˆρn)p ≤ EWp(ˆρXn   ˆρYn )p ≤ 2p−1EWp(ρ  ˆρn)p 

and therefore a bound of order O(n−p/d) for the OBM problem [2] implies a bound EWp(ρ  ˆρn) =
O(n−1/d). The matching lower bound is only known for a special case: ρA constant over a bounded
set of non-null measure [2] (e.g. ρA uniform.) Similar results  with matching lower bounds are found
for W1 in [11].

2.1.2 Convergence in probability

Results for convergence in probability  one of the main results of this work  appear to be considerably
harder to obtain. One fruitful avenue of analysis has been the use of so-called transportation  or
Talagrand inequalities Tp  which can be used to prove concentration inequalities on Wp [20]. In
particular  we say that ρ satisﬁes a Tp(C) inequality with C > 0 iff Wp(ρ  µ)2 ≤ CH(µ|ρ) ∀µ ∈
Pp(M)  where H(·|·) is the relative entropy [20]. As shown in [6  5]  it is possible to obtain
probabilistic upper bounds on Wp(ρ  ˆρn)  with p = 1  2  if ρ is known to satisfy a Tp inequality
of the same order  thereby reducing the problem of bounding Wp(ρ  ˆρn) to that of obtaining a Tp
inequality. Note that  by Jensen’s inequality  and as expected from the behavior of Wp  the inequality
T2 is stronger than T1 [20].
While it has been shown that ρ satisﬁes a T1 inequality iff it has a ﬁnite square-exponential moment
(E[eα(cid:107)x(cid:107)2
] ﬁnite for some α > 0) [4  7]  no such general conditions have been found for T2. As
an example  consider that  if M is compact with diameter D then  by theorem 6.15 of [35]  and the
celebrated Csisz´ar-Kullback-Pinsker inequality [27]  for all ρ  µ ∈ Pp(M)  it is
TV ≤ 22p−1D2pH(µ|ρ) 

Wp(ρ  µ)2p ≤ (2D)2p(cid:107)ρ − µ(cid:107)2

where (cid:107) · (cid:107)TV is the total variation norm. Clearly  this implies a Tp=1 inequality  but for p ≥ 2 it
does not.
The T2 inequality has been shown by Talagrand to be satisﬁed by the Gaussian distribution [31]  and
then slightly more generally by strictly log-concave measures (see [20  p. 123]  and [3].) However  as
noted in [6]  “contrary to the T1 case  there is no hope to obtain T2 inequalities from just integrability
or decay estimates.”
Structure of this paper.
In this work we obtain bounds in probability (learning rates) for the
problem of learning a probability measure in the sense of W2. We begin by establishing (lower)
bounds for the convergence of empirical to population measures  which serve to set up the problem
and introduce the connection between quantization and measure learning (sec. 3.) We then describe
how existing unsupervised learning algorithms that compute a set (k-means  k-ﬂats  PCA . . . ) can
be easily extended to produce a measure (sec. 4.) Due to its simplicity and widespread use  we focus
here on k-means. Since the two measure estimates that we consider are the empirical measure  and
the measure induced by k-means  we next set out to prove upper bounds on their convergence to
the data-generating measure (sec. 5.) We arrive at these bounds by means of intermediate measures 
which are related to the problem of optimal quantization. The bounds apply in a very broad setting
(unlike existing bounds based on transportation inequalities  they are not restricted to log-concave
measures [20  3].)

3

3 Learning probability measures  optimal transport and quantization

We address the problem of learning a probability measure ρ when the only observations we have at
our disposal are n i.i.d. samples Xn = (x1  . . .   xn). We begin by establishing some notation and
useful intermediate results.
Given a closed set S ⊆ X   let {Vq : q ∈ S} be a Borel Voronoi partition of X composed of sets
Vq closest to each q ∈ S  that is  such that each Vq ⊆ {x ∈ X : (cid:107)x − q(cid:107) = minr∈S (cid:107)x − r(cid:107)} is
measurable (see for instance [15].) Consider the projection function πS : X → S mapping each
x ∈ Vq to q. By virtue of {Vq}q∈S being a Borel Voronoi partition  the map πS is measurable [15] 
and it is d (x  πS (x)) = minq∈S (cid:107)x − q(cid:107) for all x ∈ X .
For any ρ ∈ Pp(M)  let πS ρ be the pushforward  or image measure of ρ under the mapping πS  
−1
which is deﬁned to be (πS ρ)(A) := ρ(π
S (A)) for all Borel measurable sets A. From its deﬁnition 
it is clear that πS ρ is supported on S.
We now establish a connection between the expected distance to a set S  and the distance between ρ
and the set’s induced pushforward measure. Notice that  for discrete sets S  the expected Lp distance
to S is exactly the expected quantization error

Ep ρ(S) := Ex∼ρd(x  S)p = Ex∼ρ(cid:107)x − πS (x)(cid:107)p

incurred when encoding points x drawn from ρ by their closest point πS (x) in S [14]. This close
connection between optimal quantization and Wasserstein distance has been pointed out in the past
in the statistics [28]  optimal quantization [14  p. 33]  and approximation theory [16] literatures.
The following two lemmas are key tools in the reminder of the paper. The ﬁrst highlights the close
link between quantization and optimal transport.
Lemma 3.1. For closed S ⊆ X   ρ ∈ Pp(M)  1 ≤ p < ∞  it holds Ex∼ρd(x  S)p = Wp(ρ  πS ρ)p.
Note that the key element in the above lemma is that the two measures in the expression Wp(ρ  πS ρ)
must match. When there is a mismatch  the distance can only increase. That is  Wp(ρ  πS µ) ≥
Wp(ρ  πS ρ) for all µ ∈ Pp(M). In fact  the following lemma shows that  among all the measures
with support in S  πS ρ is closest to ρ.
Lemma 3.2. For closed S ⊆ X   and all µ ∈ Pp(M) with supp(µ) ⊆ S  1 ≤ p < ∞  it holds
Wp(ρ  µ) ≥ Wp(ρ  πS ρ).
When combined  lemmas 3.1 and 3.2 indicate that the behavior of the measure learning problem is
limited by the performance of the optimal quantization problem. For instance  Wp(ρ  ˆρn) can only
be  in the best-case  as low as the optimal quantization cost with codebook of size n. The following
section makes this claim precise.

can be written in this case as πX4 ρ = (cid:80)4
double-counting would imply(cid:80)

3.1 Lower bounds
Consider the situation depicted in ﬁg. 1  in which a sample X4 = {x1  x2  x3  x4} is drawn from
a distribution ρ which we assume here to be absolutely continuous on its support. As shown  the
projection map πX4
sends points x to their closest point in X4. The resulting Voronoi decomposition
of supp(ρ) is drawn in shades of blue. By lemma 5.2 of [9]  the pairwise intersections of Voronoi
regions have null ambient measure  and since ρ is absolutely continuous  the pushforward measure
j=1 ρ(Vxj )δxj   where Vxj is the Voronoi region of xj.
Note that  even for ﬁnite sets S  this particular decomposition is not always possible if the {Vq}q∈S
form a Borel Voronoi tiling  instead of a Borel Voronoi partition. If  for instance  ρ has an atom
falling on two Voronoi regions in a tiling  then both regions would count the atom as theirs  and
q ρ(Vq) > 1. The technicalities required to correctly deﬁne a Borel
Voronoi partition are such that  in general  it is simpler to write πSρ  even though (if S is discrete)
this measure can clearly be written as a sum of deltas with appropriate masses.
By lemma 3.1  the distance Wp(ρ  πX4 ρ)p is the (expected) quantization cost of ρ when using X4
as codebook. Clearly  this cost can never be lower than the optimal quantization cost of size 4. This
reasoning leads to the following lower bound between empirical and population measures.

4

Theorem 3.3. For ρ ∈ Pp(M) with absolutely continuous part ρA (cid:54)= 0  and 1 ≤ p < ∞  it holds
Wp(ρ  ˆρn) = Ω(n−1/d) uniformly over ˆρn  where the constants depend on d and ρA only.
Proof: Let Vn p(ρ) := inf S⊂M |S|=n Ex∼ρd(x  S)p be the optimal quantization cost of ρ of order
p with n centers. Since ρA (cid:54)= 0  and since ρ has a ﬁnite (p + δ)-th order moment  for some δ > 0
(since it is supported on the unit ball)  then it is Vn p(ρ) = Θ(n−p/d)  with constants depending on
d and ρA (see [14  p. 78] and [16].) Since supp(ˆρn) = Xn  it follows that

Wp(ρ  ˆρn)p ≥

lemma 3.2

Wp(ρ  πXn ρ)p =

lemma 3.1

Ex∼ρd(x  Xn)p ≥ Vn p(ρ) = Θ(n

−p/d)

Note that the bound of theorem 3.3 holds for ˆρn derived from any sample Xn  and is therefore
stronger than the existing lower bounds on the convergence rates of EWp(ρ  ˆρn) → 0. In particular 
it trivially induces the known lower bound Ω(n−1/d) on the rate of convergence in expectation.

4 Unsupervised learning algorithms for learning a probability measure

etc. Performance is measured by the empirical quantity n−1(cid:80)n

As described in [21]  several of the most widely used unsupervised learning algorithms can be
interpreted to take as input a sample Xn and output a set ˆSk  where k is typically a free parameter
of the algorithm  such as the number of means in k-means1  the dimension of afﬁne spaces in PCA 
i=1 d(xi  ˆSk)2  which is minimized
among all sets in some class (e.g. sets of size k  afﬁne spaces of dimension k . . . ) This formulation is
general enough to encompass k-means and PCA  but also k-ﬂats  non-negative matrix factorization 
and sparse coding (see [21] and references therein.)
Using the discussion of Sec. 3  we can establish a clear connection between unsupervised learning
and the problem of learning probability measures with respect to W2. Consider as a running example
the k-means problem  though the argument is general. Given an input Xn  the k-means problem is
to ﬁnd a set | ˆSk| = k minimizing its average distance from points in Xn. By associating to ˆSk the
pushforward measure π ˆSk

ˆρn  we ﬁnd that

n(cid:88)

i=1

1
n

d(xi  ˆSk)2 = Ex∼ ˆρn d(x  ˆSk)2 =

lemma 3.1

W2(ˆρn  π ˆSk

ˆρn)2.

(2)

Since k-means minimizes equation 2  it also ﬁnds the measure that is closest to ˆρn  among those
with support of size k. This connection between k-means and W2 measure approximation was  to
the best of the authors’ knowledge  ﬁrst suggested by Pollard [28] though  as mentioned earlier  the
argument carries over to many other unsupervised learning algorithms.

Unsupervised measure learning algorithms. We brieﬂy clarify the steps involved in using an
existing unsupervised learning algorithm for probability measure learning. Let Uk be a parametrized
algorithm (e.g. k-means) that takes a sample Xn and outputs a set Uk(Xn). The measure learning
algorithm Ak : Mn → Pp(M) corresponding to Uk is deﬁned as follows:

1. Ak takes a sample Xn and outputs the measure π ˆSk
2. since ˆρn is discrete  then so must π ˆSk
3. in practice  we can simply store an n-vector

(cid:104)

ˆρn be  and thus Ak(Xn) = 1

ˆρn  supported on ˆSk = Uk(Xn);
(xi);

n

π ˆSk

(x1)  . . .   π ˆSk

(xn)

(cid:80)n
(cid:105)

i=1 δπ ˆSk
  from which Ak(Xn)

can be reconstructed by placing atoms of mass 1/n at each point.

In the case that Uk is the k-means algorithm  only k points and k masses need to be stored.
Note that any algorithm A(cid:48) that attempts to output a measure A(cid:48)(Xn) close to ˆρn can be cast in the
above framework. Indeed  if S(cid:48) is the support of A(cid:48)(Xn) then  by lemma 3.2  πS(cid:48) ˆρn is the measure
closest to ˆρn with support in S(cid:48). This effectively reduces the problem of learning a measure to that of
1In a slight abuse of notation  we refer to the k-means algorithm here as an ideal algorithm that solves the

k-means problem  even though in practice an approximation algorithm may be used.

5

ﬁnding a set  and is akin to how the fact that every optimal quantizer is a nearest-neighbor quantizer
(see [15]  [12  p. 350]  and [14  p. 37–38]) reduces the problem of ﬁnding an optimal quantizer to
that of ﬁnding an optimal quantizing set.
Clearly  the minimum of equation 2 over sets of size k (the output of k-means) is monotonically
ˆρn = ˆρn  it is Ex∼ ˆρn d(x  ˆSn)2 =
non-increasing with k. In particular  since ˆSn = Xn and π ˆSn
ˆρn)2 = 0. That is  we can always make the learned measure arbitrarily close to ˆρn
W2(ˆρn  π ˆSn
by increasing k. However  as pointed out in Sec. 2  the problem of measure learning is concerned
with minimizing the 2-Wasserstein distance W2(ρ  π ˆSk
ˆρn) to the data-generating measure. The
actual performance of k-means is thus not necessarily guaranteed to behave in the same way as the
empirical one  and the question of characterizing its behavior as a function of k and n naturally
arises.
Finally  we note that  while it is Ex∼ ˆρnd(x  ˆSk)2 = W2(ˆρn  π ˆSk
ˆρn)2 (the empirical performances
are the same in the optimal quantization  and measure learning problem formulations)  the actual
performances satisfy

Ex∼ρd(x  ˆSk)2 =

lemma 3.1

W2(ρ  π ˆSk

ρ)2 ≤

lemma 3.2

W2(ρ  π ˆSk

ˆρn)2 

1 ≤ k ≤ n.

Consequently  with the identiﬁcation between sets S and measures πS ˆρn  the measure learning
problem is  in general  harder than the set-approximation problem (for example  if M = Rd and ρ
is absolutely continuous over a set of non-null volume  it is not hard to show that the inequality is
almost surely strict: Ex∼ρd(x  ˆSk)2 < W2(ρ  π ˆSk
In the remainder  we characterize the performance of k-means on the measure learning problem  for
varying k  n. Although other unsupervised learning algorithms could have been chosen as basis for
our analysis  k-means is one of the oldest and most widely used  and the one for which the deep
connection between optimal quantization and measure approximation is most clearly manifested.
Note that  by setting k = n  our analysis includes the problem of characterizing the behavior of
the distance W2(ρ  ˆρn) between empirical and population measures which  as indicated in Sec. 2.1 
is a fundamental question in statistics (i.e. the speed of convergence of empirical to population
measures.)

ˆρn)2 for 1 < k < n.)

5 Learning rates

In order to analyze the performance of k-means as a measure learning algorithm  and the conver-
gence of empirical to population measures  we propose the decomposition shown in ﬁg. 2. The
diagram includes all the measures considered in the paper  and shows the two decompositions used
to prove upper bounds. The upper arrow (green)  illustrates the decomposition used to bound the dis-
tance W2(ρ  ˆρn). This decomposition uses the measures πSk ρ and πSk ˆρn as intermediates to arrive
at ˆρn  where Sk is a k-point optimal quantizer of ρ  that is  a set Sk minimizing Ex∼ρd(x  S)2 over
all sets of size |S| = k. The lower arrow (blue) corresponds to the decomposition of W2(ρ  π ˆSk
ˆρn)
(the performance of k-means)  whereas the labelled black arrows correspond to individual terms in
the bounds. We begin with the (slightly) simpler of the two results.

5.1 Convergence rates for the empirical to population measures

Let Sk be the optimal k-point quantizer of ρ of order two [14  p. 31]. By the triangle inequality and
the identity (a + b + c)2 ≤ 3(a2 + b2 + c2)  it follows that

W2(ρ  ˆρn)2 ≤ 3(cid:2)W2(ρ  πSk ρ)2 + W2(πSk ρ  πSk ˆρn)2 + W2(πSk ˆρn  ˆρn)2(cid:3) .

(3)

This is the decomposition depicted in the upper arrow of ﬁg. 2.
By lemma 3.1  the ﬁrst term in the sum of equation 3 is the optimal k-point quantization error of
ρ over a d-manifold M which  using recent techniques from [16] (see also [17  p. 491])  is shown
in the proof of theorem 5.1 (part a) to be of order Θ(k−2/d). The remaining terms  b) and c)  are
slightly more technical and are bounded in the proof of theorem 5.1.
Since equation 3 holds for all 1 ≤ k ≤ n  the best bound on W2(ρ  ˆρn) can be obtained by optimiz-
ing the right-hand side over all possible values of k  resulting in the following probabilistic bound
for the rate of convergence of the empirical to population measures.

6

Figure 1: A sample {x1  x2  x3  x4} is
drawn from a distribution ρ with support in
supp ρ. The projection map π{x1 x2 x3 x4}
sends points x to their closest one in the sam-
ple. The induced Voronoi tiling is shown in
shades of blue.

Figure 2: The measures considered in this paper
are linked by arrows for which upper bounds for
their distance are derived. Bounds for the quan-
tities of interest W2(ρ  ˆρn)2  and W2(ρ  π ˆSk
ˆρn)2 
are decomposed by following the top and bottom
colored arrows.

Theorem 5.1. Given ρ ∈ Pp(M) with absolutely continuous part ρA (cid:54)= 0  sufﬁciently large n  and
τ > 0  it holds

W2(ρ  ˆρn) ≤ C · m(ρA) · n

−1/(2d+4) · τ 

with probability 1 − e

−τ 2

.

where m(ρA) :=(cid:82)

M ρA(x)d/(d+2)dλM(x)  and C depends only on d.

5.2 Learning rates of k-means

The key element in the proof of theorem 5.1 is that the distance between population and empirical
measures can be bounded by choosing an intermediate optimal quantizing measure of an appropriate
size k. In the analysis  the best bounds are obtained for k smaller than n. If the output of k-means
is close to an optimal quantizer (for instance if sufﬁcient data is available)  then we would similarly
expect that the best bounds for k-means correspond to a choice of k < n.
The decomposition of the bottom (blue) arrow in ﬁgure 2 leads to the following bound in probability.
Theorem 5.2. Given ρ ∈ Pp(M) with absolutely continuous part ρA (cid:54)= 0  and τ > 0  then for all
sufﬁciently large n  and letting

k = C · m(ρA) · nd/(2d+4) 

it holds

where m(ρA) :=(cid:82)

W2(ρ  π ˆSk

ˆρn) ≤ C · m(ρA) · n

−1/(2d+4) · τ 

with probability 1 − e

−τ 2

.

M ρA(x)d/(d+2)dλM(x)  and C depends only on d.

Note that the upper bounds in theorem 5.1 and 5.2 are exactly the same. Although this may appear
surprising  it stems from the following fact. Since S = ˆSk is a minimizer of W2(πS ˆρn  ˆρn)2  the
bound d) of ﬁgure 2 satisﬁes:

W2(π ˆSk

ˆρn  ˆρn)2 ≤ W2(πSk ˆρn  ˆρn)2

and therefore (by the deﬁnition of c)  the term d) is of the same order as c). It follows then that
adding term d) to the bound only affects the constants  but otherwise leaves it unchanged. Since
d) is the term that takes the output measure of k-means to the empirical measure  this implies that
the rate of convergence of k-means (for suitably chosen k) cannot be worse than that of ˆρn → ρ.
Conversely  bounds for ˆρn → ρ are obtained from best rates of convergence of optimal quantizers 
whose convergence to ρ cannot be slower than that of k-means (since the quantizers that k-means
produces are suboptimal.)

7

xsuppρπ{x1 x2 x3 x4}x1x2x3x4ρˆρnπSkˆρnπˆSkˆρnπSkρa)b)c)d)W2(ρ ˆρn)W2(ρ πˆSkˆρn)Since the bounds obtained for the convergence of ˆρn → ρ are the same as those for k-means with
k of order k = Θ(nd/(2d+4))  this suggests that estimates of ρ that are as accurate as those derived
from an n point-mass measure ˆρn can be derived from k point-mass measures with k (cid:28) n.
Finally  we note that the introduced bounds are currently limited by the statistical bound

|W2(πS ˆρn  ˆρn)2 − W2(πSρ  ρ)2| =

lemma 3.1

sup
|S|=k

sup
|S|=k

|Ex∼ ˆρn d(x  S)2 − Ex∼ρd(x  S)2|

(4)

(see for instance [21])  for which non-matching lower bounds are known. This means that  if better
upper bounds can be obtained for equation 4  then both bounds in theorems 5.1 and 5.2 would
automatically improve (would become closer to the lower bound.)

References
[1] M. Ajtai  J. Komls  and G. Tusndy. On optimal matchings. Combinatorica  4:259–264  1984.
[2] Franck Barthe and Charles Bordenave. Combinatorial optimization over two random point sets. Technical

Report arXiv:1103.2734  Mar 2011.

[3] Gordon Blower. The Gaussian isoperimetric inequality and transportation. Positivity  7:203–224  2003.
[4] S. G. Bobkov and F. G¨otze. Exponential integrability and transportation cost related to logarithmic

Sobolev inequalities. Journal of Functional Analysis  163(1):1–28  April 1999.

[5] Emmanuel Boissard. Simple bounds for the convergence of empirical and occupation measures in 1-

wasserstein distance. Electron. J. Probab.  16(83):2296–2333  2011.

[6] F. Bolley  A. Guillin  and C. Villani. Quantitative concentration inequalities for empirical measures on

non-compact spaces. Probability Theory and Related Fields  137(3):541–593  2007.

[7] F. Bolley and C. Villani. Weighted Csisz´ar-Kullback-Pinsker inequalities and applications to transporta-

tion inequalities. Annales de la Faculte des Sciences de Toulouse  14(3):331–352  2005.

[8] Claire Caillerie  Fr´ed´eric Chazal  J´erˆome Dedecker  and Bertrand Michel. Deconvolution for the Wasser-

stein metric and geometric inference. Rapport de recherche RR-7678  INRIA  July 2011.

[9] Kenneth L. Clarkson. Building triangulations using -nets. In Proceedings of the thirty-eighth annual
ACM symposium on Theory of computing  STOC ’06  pages 326–335  New York  NY  USA  2006. ACM.
[10] Luc Devroye and G´abor Lugosi. Combinatorial methods in density estimation. Springer Series in Statis-

tics. Springer-Verlag  New York  2001.

[11] V. Dobri and J. Yukich. Asymptotics for transportation cost in high dimensions. Journal of Theoretical

Probability  8:97–118  1995.

[12] A. Gersho and R.M. Gray. Vector Quantization and Signal Compression. Kluwer International Series in

Engineering and Computer Science. Kluwer Academic Publishers  1992.

[13] Alison L. Gibbs and Francis E. Su. On choosing and bounding probability metrics. International Statis-

tical Review  70:419–435  2002.

[14] Siegfried Graf and Harald Luschgy. Foundations of quantization for probability distributions. Springer-

Verlag New York  Inc.  Secaucus  NJ  USA  2000.

[15] Siegfried Graf  Harald Luschgy  and Gilles Page`s. Distortion mismatch in the quantization of probability

measures. Esaim: Probability and Statistics  12:127–153  2008.

[16] Peter M. Gruber. Optimum quantization and its applications. Adv. Math  186:2004  2002.
[17] P.M. Gruber. Convex and discrete geometry. Grundlehren der mathematischen Wissenschaften. Springer 

2007.

[18] Guillermo Henry and Daniela Rodriguez. Kernel density estimation on riemannian manifolds: Asymp-

totic results. J. Math. Imaging Vis.  34(3):235–239  July 2009.

[19] Joseph Horowitz and Rajeeva L. Karandikar. Mean rates of convergence of empirical measures in the

Wasserstein metric. J. Comput. Appl. Math.  55(3):261–273  November 1994.

[20] M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. Amer-

ican Mathematical Society  2001.

[21] A. Maurer and M. Pontil. K–dimensional coding schemes in Hilbert spaces.

Information Theory  56(11):5839 –5846  nov. 2010.

IEEE Transactions on

[22] Yann Ollivier. Ricci curvature of markov chains on metric spaces. J. Funct. Anal.  256(3):810–864  2009.

8

[23] Arkadas Ozakin and Alexander Gray. Submanifold density estimation. In Y. Bengio  D. Schuurmans 
J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural Information Processing Systems
22  pages 1375–1382. 2009.

[24] C. Papadimitriou. The probabilistic analysis of matching heuristics. In Proc. of the 15th Allerton Conf.

on Communication  Control and Computing  pages 368–378  1978.

[25] Bruno Pelletier. Kernel density estimation on Riemannian manifolds. Statist. Probab. Lett.  73(3):297–

304  2005.

[26] Xavier Pennec. Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements. J.

Math. Imaging Vis.  25(1):127–154  July 2006.

[27] M. S. Pinsker. Information and information stability of random variables and processes. San Francisco:

Holden-Day  1964.

[28] David Pollard. Quantization and the method of k-means.

28(2):199–204  1982.

IEEE Transactions on Information Theory 

[29] S.T. Rachev. Probability metrics and the stability of stochastic models. Wiley series in probability and

mathematical statistics: Applied probability and statistics. Wiley  1991.

[30] J.M. Steele. Probability Theory and Combinatorial Optimization. Cbms-Nsf Regional Conference Series

in Applied Mathematics. Society for Industrial and Applied Mathematics  1997.

[31] M. Talagrand. Transportation cost for Gaussian and other product measures. Geometric And Functional

Analysis  6:587–600  1996.

[32] Alexandre B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. Springer 

New York  2009. Revised and extended from the 2004 French original  Translated by Vladimir Zaiats.

[33] A.W. van der Vaart and J.A. Wellner. Weak Convergence and Empirical Processes. Springer Series in

Statistics. Springer  1996.

[34] V. S. Varadarajan. On the convergence of sample probability distributions. Sankhy¯a: The Indian Journal

of Statistics  19(1/2):23–26  Feb. 1958.

[35] C. Villani. Optimal Transport: Old and New. Grundlehren der Mathematischen Wissenschaften. Springer 

2009.

[36] P. Vincent and Y. Bengio. Manifold Parzen Windows. In Advances in Neural Information Processing

Systems 22  pages 849–856. 2003.

9

,Noam Brown
Tuomas Sandholm
Sergey Ioffe