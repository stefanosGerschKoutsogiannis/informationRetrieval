2014,Beta-Negative Binomial Process and Exchangeable ￼Random Partitions for Mixed-Membership Modeling,The beta-negative binomial process (BNBP)  an integer-valued stochastic process  is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed  current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions  which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP  leading to a novel nonparametric Bayesian topic model that is distinct from existing ones  with simple implementation  fast convergence  good mixing  and state-of-the-art predictive performance.,Beta-Negative Binomial Process and Exchangeable
Random Partitions for Mixed-Membership Modeling

IROM Department  McCombs School of Business

The University of Texas at Austin  Austin  TX 78712  USA

Mingyuan Zhou

mingyuan.zhou@mccombs.utexas.edu

Abstract

The beta-negative binomial process (BNBP)  an integer-valued stochastic process 
is employed to partition a count vector into a latent random count matrix. As the
marginal probability distribution of the BNBP that governs the exchangeable ran-
dom partitions of grouped data has not yet been developed  current inference for
the BNBP has to truncate the number of atoms of the beta process. This paper
introduces an exchangeable partition probability function to explicitly describe
how the BNBP clusters the data points of each group into a random number of
exchangeable partitions  which are shared across all the groups. A fully col-
lapsed Gibbs sampler is developed for the BNBP  leading to a novel nonparametric
Bayesian topic model that is distinct from existing ones  with simple implementa-
tion  fast convergence  good mixing  and state-of-the-art predictive performance.

1

Introduction

For mixture modeling  there is a wide selection of nonparametric Bayesian priors  such as the Dirich-
let process [1] and the more general family of normalized random measures with independent incre-
ments (NRMIs) [2  3]. Although a draw from an NRMI usually consists of countably inﬁnite atoms
that are impossible to instantiate in practice  one may transform the inﬁnite-dimensional problem
into a ﬁnite one by marginalizing out the NRMI. For instance  it is well known that the marginal-
ization of the Dirichlet process random probability measure under multinomial sampling leads to
the Chinese restaurant process [4  5]. The general structure of the Chinese restaurant process is
broadened by [5] to the so called exchangeable partition probability function (EPPF) model  leading
to fully collapsed inference and providing a uniﬁed view of the characteristics of various nonpara-
metric Bayesian mixture-modeling priors. Despite signiﬁcant progress on EPPF models in the past
decade  their use in mixture modeling (clustering) is usually limited to a single set of data points.
Moving beyond mixture modeling of a single set  there has been signiﬁcant recent interest in mixed-
membership modeling  i.e.  mixture modeling of grouped data x1  . . .   xJ  where each group xj =
{xji}i=1 mj consists of mj data points that are exchangeable within the group. To cluster the mj
data points in each group into a random  potentially unbounded number of partitions  which are
exchangeable and shared across all the groups  is a much more challenging statistical problem.
While the hierarchical Dirichlet process (HDP) [6] is a popular choice  it is shown in [7] that a wide
variety of integer-valued stochastic processes  including the gamma-Poisson process [8  9]  beta-
negative binomial process (BNBP) [10  11]  and gamma-negative binomial process (GNBP)  can all
be applied to mixed-membership modeling. However  none of these stochastic processes are able
to describe their marginal distributions that govern the exchangeable random partitions of grouped
data. Without these marginal distributions  the HDP exploits an alternative representation known as
the Chinese restaurant franchise [6] to derive collapsed inference  while fully collapsed inference is
available for neither the BNBP nor the GNBP.

1

The EPPF provides a uniﬁed treatment to mixture modeling  but there is hardly a uniﬁed treatment
to mixed-membership modeling. As the ﬁrst step to ﬁll that gap  this paper thoroughly investigates
the law of the BNBP that governs its exchangeable random partitions of grouped data. As directly
deriving the BNBP’s EPPF for mixed-membership modeling is difﬁcult  we ﬁrst randomize the
group sizes {mj}j and derive the joint distribution of {mj}j and their random partitions on a shared
list of exchangeable clusters; we then derive the marginal distribution of the group-size count vector
m = (m1  . . .   mJ )T   and use Bayes’ rule to further arrive at the BNBP’s EPPF that describes the
prior distribution of a latent column-exchangeable random count matrix  whose jth row sums to mj.
The general method to arrive at an EPPF for mixed-membership modeling using an integer-valued
stochastic process is an important contribution. We make several additional contributions: 1) We
derive a prediction rule for the BNBP to simulate exchangeable random partitions of grouped data
governed by its EPPF. 2) We construct a BNBP topic model  derive a fully collapsed Gibbs sam-
pler that analytically marginalizes out not only the topics and topic weights  but also the inﬁnite-
dimensional beta process  and provide closed-form update equations for model parameters. 3) The
straightforward to implement BNBP topic model sampling algorithm converges fast  mixes well 
and produces state-of-the-art predictive performance with a compact representation of the corpus.

1.1 Exchangeable Partition Probability Function
Let Πm = {A1  . . .   Al} denote a random partition of the set [m] = {1  2  . . .   m}  where there
are l partitions and each element i ∈ [m] belongs to one and only one set Ak from Πm. If P (Πm =
{A1  . . .   Al}|m) depends only on the number and sizes of the Ak’s  regardless of their order  then
it is called an exchangeable partition probability function (EPPF) of Πm. An EPPF of Πm is an
EPPF of Π := (Π1  Π2  . . .) if P (Πm|n) = P (Πm|m) does not depend on n  where P (Πm|n)
denotes the marginal partition probability for [m] when it is known the sample size is n. Such a
constraint can also be expressed as an addition rule for the EPPF [5]. In this paper  the addition rule
is not required and the proposed EPPF is allowed to be dependent on the group sizes (or sample
size if the number of groups is one). Detailed discussions about sample size dependent EPPFs
can be found in [12]. We generalize the work of [12] to model the partition of a count vector into a
latent column-exchangeable random count matrix. A marginal sampler for σ-stable Poisson-Kigman
mixture models (but not mixed-membership models) is proposed in [13]  encompassing a large class
of random probability measures and their corresponding EPPFs of Π. Note that the BNBP is not
within that class and both the BNBP’s EPPF and prediction rule are dependent on the group sizes.

1.2 Beta Process
The beta process B ∼ BP(c  B0) is a completely random measure deﬁned on the product space
[0  1] × Ω  with a concentration parameter c > 0 and a ﬁnite and continuous base measure B0 over
a complete separable metric space Ω [14  15] . We deﬁne the L´evy measure of the beta process as

ν(dpdω) = p−1(1 − p)c−1dpB0(dω).

(cid:80)∞
(1)
A draw from B ∼ BP(c  B0) can be represented as a countably inﬁnite sum as B =
divisible  and its measure on a Borel set A ⊂ Ω  expressed as B(A) = (cid:80)
k=1 pkδωk   ωk ∼ g0  where γ0 = B0(Ω) is the mass parameter and g0(dω) = B0(dω)/γ0
is the base distribution. The beta process is unique in that the beta distribution is not inﬁnitely
Q(A) = −(cid:80)
k:ωk∈A pk  could be
In this paper we will work with
k:ωk∈A ln(1 − pk)  deﬁned as a logbeta random variable  to analyze model properties
and derive closed-form Gibbs sampling update equations. We provide these details in the Appendix.

larger than one and hence clearly not a beta random variable.

2 Exchangeable Cluster/Partition Probability Functions for the BNBP

The integer-valued beta-negative binomial process (BNBP) is deﬁned as

Xj|B ∼ NBP(rj  B)  B ∼ BP(c  B0) 

is a negative binomial process such that Xj(A) =(cid:80)

(2)
where for the jth group rj is the negative binomial dispersion parameter and Xj|B ∼ NBP(rj  B)
k:ωk∈A njk  njk ∼ NB(rj  pk) for each Borel
set A ⊂ Ω. The negative binomial distribution n ∼ NB(r  p) has probability mass function (PMF)
n!Γ(r) pn(1 − p)r for n ∈ Z  where Z = {0  1  . . .}. Our deﬁnition of the BNBP follows
fN (n) = Γ(n+r)

2

those of [10  7  11]  where for inference [10  7] used ﬁnite truncation and [11] used slice sampling.
There are two recent papers [16  17] that both marginalize out the beta process from the negative
binomial process  with the predictive structures of the BNBP described as the negative binomial
Indian buffet process (IBP) [16] and “ice cream” buffet process [17]  respectively. Both processes
are also related to the “multi-scoop” IBP of [10]  and they all generalize the binary-valued IBP [18].
Different from these two papers on inﬁnite random count matrices  this paper focuses on generating
a latent column-exchangeable random count matrix  each row of which sums to a ﬁxed observed
integer. This paper generalizes the techniques developed in [17  12] to deﬁne an EPPF for mixed-
membership modeling and derive truncation-free fully collapsed inference.
The BNBP by nature is an integer-valued stochastic process as Xj(A) is a random count for each
Borel set A ⊂ Ω. As the negative binomial process is also a gamma-Poisson mixture process  we
can augment (2) as a beta-gamma-Poisson process as

Xj|Θj ∼ PP(Θj)  Θj|rj  B ∼ ΓP[rj  B/(1 − B)]  B ∼ BP(c  B0) 

ΓP[rj  B/(1−B)] is a gamma process such that Θj(A) =(cid:80)

where Xj|Θj ∼ PP(Θj) is a Poisson process such that Xj(A) ∼ Pois[Θj(A)]  and Θj|B ∼
k:ωk∈A θjk  θjk ∼ Gamma[rj  pk/(1−
pk)]  for each Borel set A ⊂ Ω. The mixed-membership-modeling potentials of the BNBP become
clear under this augmented representation. The Poisson process provides a bridge to link count mod-
eling and mixture modeling [7]  since Xj ∼ PP(Θj) can be equivalently generated by ﬁrst drawing
a total random count mj := Xj(Ω) ∼ Pois[Θj(Ω)] and then assigning this random count to disjoint
Borel sets of Ω using a multinomial distribution.

2.1 Exchangeable Cluster Probability Function

In mixed-membership modeling  the size of each group is observed rather being random  thus al-
though the BNBP’s augmented representation is instructive  it is still unclear how exactly the integer-
valued stochastic process leads to a prior distribution on exchangeable random partitions of grouped
data. The ﬁrst step for us to arrive at such a prior distribution is to build a group size dependent
model that treats the number of data points to be clustered (partitioned) in each group as random.
Below we will ﬁrst derive an exchangeable cluster probability function (ECPF) governed by the
BNBP to describe the joint distribution of the random group sizes and their random partitions over a
random  potentially unbounded number of exchangeable clusters shared across all the groups. Later
we will show how to derive the EPPF from the ECPF using Bayes’ rule.

Lemma 1. Denote δk(zji) as a unit point mass at zji = k  njk = (cid:80)mj
(cid:80)

i=1 δk(zji)  and Xj(A) =
k:ωk∈A njk as the number of data points in group j assigned to the atoms within the Borel set

A ⊂ Ω. The Xj’s generated via the group size dependent mixed-membership model as

θjk

k=1

Θj (Ω) δk  mj ∼ Pois[Θj(Ω)] 
Θj ∼ ΓP[rj  B/(1 − B)]  B ∼ BP(c  B0)
is equivalent in distribution to the Xj’s generated from a BNBP as in (2).

k=1 pkδωk and Θj =(cid:80)∞

Proof. With B =(cid:80)∞
zj = (zj1  . . .   zjmj ) given Θj and mj can be expressed as p(zj|Θj  mj) = (cid:81)mj
(cid:81)∞
((cid:80)∞

k=1 θjkδωk  the joint distribution of the cluster indices
θjzji(cid:80)∞
k(cid:48)=1 θjk(cid:48) =
jk   which is not in a fully factorized form. As mj is linked to the total random

k=1θnjk

(3)

i=1

1

mass Θj(Ω) with a Poisson distribution  we have the joint likelihood of zj and mj given Θj as

k(cid:48)=1 θjk(cid:48) )mj

zji ∼(cid:80)∞

(cid:81)∞

f (zj  mj|Θj) = f (zj|Θj  mj)Pois[mj; Θj(Ω)] = 1

(4)
which is fully factorized and hence amenable to marginalization. Since θjk ∼ Gamma[rj  pk/(1 −
pk)]  we can marginalize θjk out analytically as f (zj  mj|rj  B) = EΘj [f (zj  mj|Θj)]  leading to
(5)

f (zj  mj|rj  B) = 1

(1 − pk)rj .

jk e−θjk  

(cid:81)∞

k=1 θnjk

Γ(njk+rj )

mj !

pnjk
k

mj !

k=1

Γ(rj )

Multiplying the above equation with a multinomial coefﬁcient transforms the prior distribution
for the categorical random variables zj to the prior distribution for a random count vector as
f (nj1  . . .   nj∞|rj  B) =
k=1 NB(njk; rj  pk). Thus in the prior 

k=1 njk! f (zj  mj|rj  B) = (cid:81)∞
mj !(cid:81)∞

3

data points independently at each atom. With Xj :=(cid:80)∞
such that Xj(A) =(cid:80)

for each group  the group size dependent model in (3) draws njk ∼ NB(rj  pk) random number of
k=1 njkδωk  we have Xj|B ∼ NBP(rj  B)

k:ωk∈A njk  njk ∼ NB(rj  pk).

The Lemma below provides a ﬁnite-dimensional distribution obtained by marginalizing out the
inﬁnite-dimensional beta process from the BNBP. The proof is provided in the Appendix.
Lemma 2 (ECPF). The exchangeable cluster probability function (ECPF) of the BNBP  which de-
scribes the joint distribution of the random count vector m := (m1  . . .   mJ )T and its exchangeable
random partitions z := (z11  . . .   zJmJ )  can be expressed as

f (z  m|r  γ0  c) = γKJ

0

r· :=(cid:80)J

j=1 rj  n·k :=(cid:80)J

(cid:81)J

(cid:81)KJ

e−γ0[ψ(c+r· )−ψ(c)]

(cid:104) Γ(n·k)Γ(c+r·)

(cid:81)J
j=1 njk  and mj ∈ Z is the random size of group j.

Γ(c+n·k+r·)

j=1 mj !

k=1

j=1

where KJ is the number of observed points of discontinuity for which n·k > 0  r := (r1  . . .   rJ )T  

Γ(njk+rj )

Γ(rj )

 

(6)

(cid:105)

2.2 Exchangeable Partition Probability Function and Prediction Rule

(cid:81)J

Having the ECPF does not directly lead to the EPPF for the BNBP  as an EPPF describes the distri-
bution of the exchangeable random partitions of the data groups whose sizes are observed rather than
being random. To arrive at the EPPF  ﬁrst we organize z into a random count matrix NJ ∈ ZJ×KJ  
whose jth row represents the partition of the mj data points into the KJ shared exchangeable clus-
ters and whose order of these KJ nonzero columns is chosen uniformly at random from one of the
KJ ! possible permutations  then we obtain a prior distribution on a BNBP random count matrix as

f (NJ|r  γ0  c) = 1
= γKJ

KJ !

0

f (z  m|r  γ0  c)

mj !(cid:81)KJ

j=1

k=1 njk!
e−γ0[ψ(c+r·)−ψ(c)]

KJ !

(cid:81)KJ

k=1

(cid:81)J

1

is

for

(cid:81)J

Γ(r·)

Γ(njk+rj )

Γ(n·k)Γ(c+r·)
Γ(c+n·k+r·)

Γ(njk+rj )
njk!Γ(rj ) .

j=1

Γ(n·k+r·)

j=1

Γ(rj )

Γ(r+n)Γ(c+r)

(cid:81)J

n·k!
j=1 njk!

the count vector

:= (n1k  . . .   nJk)T

(7)
As described in detail in [17]  although the matrix prior does not appear to be simple  direct calcula-
tion shows that this random count matrix has KJ ∼ Pois{γ0 [ψ(c + r·) − ψ(c)]} independent and
identically distributed (i.i.d.) columns that can be generated via

n:k ∼ DirMult(n·k  r1  . . .   rJ )  n·k ∼ Digam(r·  c) 

where n:k
the Dirichlet-multinomial

(8)
the kth column (cluster) 
(DirMult) distribution [19] has PMF DirMult(n:k|n·k  r) =
  and the digamma distribution [20] has PMF Digam(n|r  c) =
nΓ(c+n+r)Γ(r)   where n = 1  2  . . .. Thus in the prior  the BNBP generates a Poisson
ψ(c+r)−ψ(c)
random number of clusters  the size of each cluster follows a digamma distribution  and each cluster
is further partitioned into the J groups using a Dirichlet-multinomial distribution [17].
Lemma 3 (EPPF). Let (cid:80)(cid:80)K
With both the ECPF and random count matrix prior governed by the BNBP  we are ready to derive
both the EPPF and prediction rule  given in the following two Lemmas  with proofs in the Appendix.
(cid:80)K
k=1 n:k = m  where m· = (cid:80)J
k=1 n:k=m denote a summation over all sets of count vectors with
j=1 mj and n·k ≥ 1. The group-size dependent exchangeable
(cid:81)KJ
0(cid:81)J
(cid:80)(cid:80)K(cid:48)

partition probability function (EPPF) governed by the BNBP can be expressed as
Γ(njk+rj )

(cid:104) Γ(n·k)Γ(c+r·)
(cid:81)K(cid:48)

f (z|m  r  γ0  c) =

(cid:80)m·

γKJ
j=1 mj !

(cid:81)J

(cid:81)J

Γ(c+n·k+r·)

which is a function of the cluster sizes {njk}k=1 KJ   regardless of the orders of the indices k’s.
Although the EPPF is fairly complicated  one may derive a simple prediction rule  as shown below 
to simulate exchangeable random partitions of grouped data governed by this EPPF.
Lemma 4 (Prediction Rule). With y−ji representing the variable y excluding the contribution of
xji  the prediction rule of the BNBP group size dependent mixed-membership model in (3) is

Γ(n·k(cid:48) )Γ(c+r·)
Γ(c+n·k(cid:48) +r·)

Γ(njk(cid:48) +rj )
njk(cid:48) !Γ(rj )

k(cid:48)=1 n:k(cid:48) =m

j=1

Γ(rj )

γK(cid:48)
K(cid:48)!

0

K(cid:48)=1

 

(9)

k(cid:48)=1

(cid:105)

k=1

j=1

 n

c+n

γ0
c+r· rj 

−ji
·k
−ji
·k +r·

4

P (zji = k|z−ji  m  r  γ0  c) ∝

−ji
jk + rj) 

(n

for k = 1  . . .   K

−ji
J

;

if k = K

−ji
J + 1.

(10)

ψ(c+(cid:80)

Figure 1:
Random draws from the EPPF that governs the BNBP’s exchangeable random partitions of
10 groups (rows)  each of which has 50 data points. The parameters of the EPPF are set as c = 2 
j rj )−ψ(c)   and (a) rj = 1  (b) rj = 10  or (c) rj = 100 for all the 10 groups. The jth row
γ0 =
of each matrix  which sums to 50  represents the partition of the mj = 50 data points of the jth group over
a random number of exchangeable clusters  and the kth column of each matrix represents the kth nonempty
cluster in order of appearance in Gibbs sampling (the empty clusters are deleted).

12

2.3 Simulating Exchangeable Random Partitions of Grouped Data

While the EPPF (9) is not simple  the prediction rule (10) clearly shows that the probability of
selecting k is proportional to the product of two terms: one is related to the kth cluster’s overall
popularity across groups  and the other is only related to the kth cluster’s popularity at that group
and that group’s dispersion parameter; and the probability of creating a new cluster is related to γ0 
c  r· and rj. The BNBP’s exchangeable random partitions of the group-size count vector m  whose
prior distribution is governed by (9)  can be easily simulated via Gibbs sampling using (10).
Running Gibbs sampling using (10) for 2500 iterations and displaying the last sample  we show in
Figure 1 (a)-(c) three distinct exchangeable random partitions of the same group-size count vector 
It is clear that the dispersion parameters {rj}j play a
under three different parameter settings.
critical role in controlling how overdispersed the counts are: the smaller the {rj}j are  the more
overdispersed the counts in each row and those in each column are. This is unsurprising as in the
BNBP’s prior  we have njk ∼ NB(rj  pk) and n:k ∼ DirMult(n·k  r1  . . .   rJ ). Figure 1 suggests
that it is important to infer rj rather than setting them in a heuristic manner or ﬁxing them.

3 Beta-Negative Binomial Process Topic Model

θjk

Θj (Ω) δk  mj ∼ Pois[Θj(Ω)] 

With the BNBP’s EPPF derived  it becomes evident that the integer-valued BNBP also governs a
prior distribution for exchangeable random partitions of grouped data. To demonstrate the use of
the BNBP  we apply it to topic modeling [21] of a document corpus  a special case of mixture
modeling of grouped data  where the words of the jth document xj1  . . .   xjmj constitute a group
xj (mj words in document j)  each word xji is an exchangeable group member indexed by vji in
a vocabulary with V unique terms. We choose the base distribution as a V dimensional Dirichlet
distribution as g0(φ) = Dir(φ; η  . . .   η)  and choose a multinomial distribution to link a word to a
topic (atom). We express the hierarchical construction of the BNBP topic model as

(cid:1)  rj ∼ Gamma(a0  1/b0)  B ∼ BP(c  B0)  γ0 ∼ Gamma(e0  1/f0). (11)
(cid:81)V

xji ∼ Mult(φzji)  φk ∼ Dir(η  . . .   η)  zji ∼(cid:80)∞
Θj ∼ ΓP(cid:0)rj  B
Let nvjk := (cid:80)mj
1−B
(cid:81)∞
(cid:81)V
i=1 δv(xji)δk(zji). Multiplying (4) and the data likelihood f (xj|zj  Φ) =
(cid:81)∞
(cid:81)V
(cid:81)∞
k=1(φvk)nvjk   where Φ = (φ1  . . .   φ∞)  we have f (xj  zj  mj|Φ  Θj) =
(mvj)v=1:V  j=1:J is factorized under the Poisson likelihood as mvj = (cid:80)∞
v=1 Pois(nvjk; φvkθjk). Thus the BNBP topic model can also be con-
sidered as an inﬁnite Poisson factor model [10]  where the term-document word count matrix
k=1 nvjk  nvjk ∼
Pois(φvkθjk)  whose likelihood f ({nvjk}v k|Φ  Θj) is different from f (xj  zj  mj|Φ  Θj) up to
The full conditional likelihood f (x  z  m|Φ  Θ) = (cid:81)J
a multinomial coefﬁcient.
(cid:26)(cid:81)∞
(cid:111) ·
j=1 f (xj  zj  mj|Φ  Θj) can be further
expressed as f (x  z  m|Φ  Θ) =
  where the
marginalization of Φ from the ﬁrst right-hand-side term is the product of Dirichlet-multinomial dis-
tributions and the second right-hand-side term leads to the ECPF. Thus we have a fully marginalized

(cid:110)(cid:81)∞

v=1 φnv·k

vk

njk
jk e

j=1 θ
j=1 mj !

(cid:81)V

(cid:81)J
(cid:81)J

v=1 nvjk!

mj !

k=1

(cid:27)

−θjk

v=1

k=1

k=1

k=1

k=1

5

274948 4 1 14949492345484950 1 1 1 2 1 1 1PartitionGroup(a) ri = 1123456782468102537391128203334333318 2 5181426 8 912 1 1 1 1 3 2 3 3 3 113 5 311 4 2 7 2 3 1 2 1 1 1 1 1 1 1 1 3 2 1 1 1 1 1 2 1 1 1PartitionGroup(b) ri = 10246810121424681026191521161310172217 9241414182931172118 9 213 912 7 3 7 613 1 2 1 1 1 1 2 2 1 2 2 1 1 4 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1PartitionGroup(c) ri = 10051015246810likelihood as f (x  z  m|γ0  c  r) = f (z  m|γ0  c  r)(cid:81)KJ

. Directly
applying Bayes’ rule to this fully marginalized likelihood  we construct a nonparametric Bayesian
fully collapsed Gibbs sampler for the BNBP topic model as
· (n

for k = 1  . . .   K

Γ(n·k+V η)

Γ(η)

k=1

v=1

·

−ji
jk + rj) 

−ji
J

;

P (zji = k|x  z

−ji  γ0  m  c  r)∝

−ji
n
·k
−ji
·k +r·
c+n

(12)

Γ(V η)

Γ(nv·k+η)

(cid:81)V

(cid:105)

(cid:104)

 η+n

−ji
vji·k
−ji
·k
V · γ0

V η+n

1

c+r· · rj 

if k = K

−ji
J + 1.

In the Appendix we include all the other closed-form Gibbs sampling update equations.

3.1 Comparison with Other Collapsed Gibbs Samplers

One may compare the collapsed Gibbs sampler of the BNBP topic model with that of latent Dirichlet
allocation (LDA) [22]  which  in our notation  can be expressed as

P (zji = k|x  z−ji  m  α  K) ∝ η+n

−ji
vji·k
−ji
·k

V η+n

· (n

−ji
jk + α

K ) 

for k = 1  . . .   K 

(13)

where the number of topics K and the topic proportion Dirichlet smoothing parameter α are both
tuning parameters. The BNBP topic model is a nonparametric Bayesian algorithm that removes the
need to tune these parameters. One may also compare the BNBP topic model with the HDP-LDA
[6  23]  whose direct assignment sampler in our notation can be expressed as

 η+n

1

V η+n

−ji
vji·k
−ji
·k
V · (α˜r∗) 

P (zji = k|x  z−ji  m  α  ˜r) ∝

· (n

−ji
jk + α˜rk) 

for k = 1  . . .   K

−ji
J

;

(14)

where α is the concentration parameter for the group-speciﬁc Dirichlet processes (cid:101)Θj ∼ DP(α (cid:101)G) 
and ˜rk = (cid:101)G(ωk) and ˜r∗ = (cid:101)G(Ω\DJ ) are the measures of the globally shared Dirichlet process (cid:101)G ∼
DP(γ0 (cid:101)G0) over the observed points of discontinuity and absolutely continuous space  respectively.

if k = K

−ji
J + 1;

Comparison between (14) and (12) shows that distinct from the HDP-LDA that combines a topic’s
−ji
jk + α˜rk)  the BNBP topic model com-
global and local popularities in an additive manner as (n
· (n
−ji
bines them in a multiplicative manner as
jk + rj). This term can also be rewritten

−ji
n
·k
−ji
·k +r·
c+n

−ji
·k

and n

−ji
jk +rj
−ji
·k +r·

c+n

  the latter of which represents how much the jth document
as the product of n
contributes to the overall popularity of the kth topic. Therefore  the BNBP and HDP-LDA have dis-
tinct mechanisms to automatically shrink the number of topics. Note that while the BNBP sampler
in (12) is fully collapsed  the direct assignment sampler of the HDP-LDA in (14) is only partially

collapsed as neither the globally shared Dirichlet process (cid:101)G nor the concentration parameter α are
marginalized out. To derive a collapsed sampler for the HDP-LDA that marginalizes out (cid:101)G (but still

not α)  one has to use the Chinese restaurant franchise [6]  which has cumbersome book-keeping as
each word is indirectly linked to its topic via a latent table index.

4 Example Results

We consider the JACM1  PsyReview2  and NIPS123 corpora  restricting the vocabulary to terms that
occur in ﬁve or more documents. The JACM corpus includes 536 documents  with V = 1539 unique
terms and 68 055 total word counts. The PsyReview corpus includes 1281 documents  with V =
2566 and 71 279 total word counts. The NIPS12 corpus includes 1740 documents  with V = 13  649
and 2 301 375 total word counts. To evaluate the BNBP topic model4 and its performance relative to
that of the HDP-LDA  which are both nonparametric Bayesian algorithms  we randomly choose 50%

1http://www.cs.princeton.edu/∼blei/downloads/
2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
3http://www.cs.nyu.edu/∼roweis/data.html
4Matlab code available in http://mingyuanzhou.github.io/

6

Figure 2: The inferred number of topics KJ for the ﬁrst 1500 Gibbs sampling iterations for the (a) HDP-LDA
and (b) BNBP topic model on JACM. (c)-(d) and (e)-(f) are analogous plots to (a)-(c) for the PsyReview and
NIPS12 corpora  respectively. From bottom to top in each plot  the red  blue  magenta  black  green  yellow 
and cyan curves correspond to the results for η = 0.50  0.25  0.10  0.05  0.02  0.01  and 0.005  respectively.

mtest··

v

j mtest

v

(cid:17)

s

s

v

j mtest

(cid:80)

k φ(s)

vk θ(s)
k φ(s)

vk θ(s)

jk

jk

(cid:80)

(cid:80)

(cid:16) − 1

(cid:80)
(cid:80)
(cid:80)
(cid:80)
(cid:80)

mtest·· = (cid:80)

of the words in each document as training  and use the remaining ones to calculate per-word heldout
perplexity. We set the hyperparameters as a0 = b0 = e0 = f0 = 0.01. We consider 2500 Gibbs
sampling iterations and collect the last 1500 samples. In each iteration  we randomize the ordering
of the words. For each collected sample  we draw the topics (φk|−) ∼ Dir(η + n1·k  . . .   η +
nJ·k)  and the topics weights (θjk|−) ∼ Gamma(njk + rj  pk) for the BNBP and topic proportions
(θk|−) ∼ Dir(nj1 + α˜r1  . . .   njKJ + α˜rKJ ) for the HDP  with which the per-word perplexity is
  where s ∈ {1  . . .   S} is the index
computed as exp
vj ln
of a collected MCMC sample  mtest
is the number of test words at term v in document j  and
vj
vj . The ﬁnal results are averaged over ﬁve random training/testing partitions.
The evaluation method is similar to those used in [24  25  26  10]. Similar to [26  10]  we set the
topic Dirichlet smoothing parameter as η = 0.01  0.02  0.05  0.10  0.25  or 0.50. To test how the
algorithms perform in more extreme settings  we also consider η = 0.001  0.002  and 0.005. All
algorithms are implemented with unoptimized Matlab code. On a 3.4 GHz CPU  the fully collapsed
Gibbs sampler of the BNBP topic model takes about 2.5 seconds per iteration on the NIPS12 corpus
when the inferred number of topics is around 180. The direct assignment sampler of the HDP-LDA
has comparable computational complexity when the inferred number of topics is similar. Note that
when the inferred number of topics KJ is large  the sparse computation technique for LDA [27  28]
may also be used to considerably speed up the sampling algorithm of the BNBP topic model.
We ﬁrst diagnose the convergence and mixing of the collapsed Gibbs samplers for the HDP-LDA
and BNBP topic model via the trace plots of their samples. The three plots in the left column of
Figures 2 show that the HDP-LDA travels relatively slowly to the target distributions of the number
of topics  often reaching them in more than 300 iterations  whereas the three plots in the right column
show that the BNBP topic model travels quickly to the target distributions  usually reaching them
in less than 100 iterations. Moreover  Figures 2 shows that the chains of the HDP-LDA are taking
in small steps and do not traverse their distributions quickly  whereas the chains of the BNBP topic
models mix very well locally and traverse their distributions relatively quickly.
A smaller topic Dirichlet smoothing parameter η generally supports a larger number of topics  as
shown in the left column of Figure 3  and hence often leads to lower perplexities  as shown in
the middle column of Figure 3; however  an η that is as small as 0.001 (not commonly used in
practice) may lead to more than a thousand topics and consequently overﬁt the corpus  which is
particularly evident for the HDP-LDA on both the JACM and PsyReview corpora. Similar trends
are also likely to be observed on the larger NIPS2012 corpus if we allow the values of η to be
even smaller than 0.001. As shown in the middle column of Figure 3  for the same η  the BNBP
topic model  usually representing the corpus with a smaller number of topics  often have higher
perplexities than those of the HDP-LDA  which is unsurprising as the BNBP topic model has a
multiplicative control mechanism to more strongly shrink the number of topics  whereas the HDP
has a softer additive shrinkage mechanism. Similar performance differences have also been observed

7

050010001500101001000(d) BNBP Topic Model  PsyReview050010001500101001000(c) HDP−LDA  PsyReviewNumber of topics050010001500101001000(b) BNBP Topic Model  JACM050010001500101001000(a) HDP−LDA  JACMNumber of topics050010001500101001000(f) BNBP Topic Model  NIPS12Gibbs sampling iteration050010001500101001000(e) HDP−LDA  NIPS12Gibbs sampling iterationNumber of topicsFigure 3: Comparison between the HDP-LDA and BNBP topic model with the topic Dirichlet smoothing pa-
rameter η ∈ {0.001  0.002  0.005  0.01  0.02  0.05  0.10  0.25  0.50}. For the JACM corpus: (a) the posterior
mean of the inferred number of topics KJ and (b) per-word heldout perplexity  both as a function of η  and (c)
per-word heldout perplexity as a function of the inferred number of topics KJ; the topic Dirichlet smoothing
parameter η and the number of topics KJ are displayed in the logarithmic scale. (d)-(f) Analogous plots to
(a)-(c) for the PsyReview corpus. (g)-(i) Analogous plots to (a)-(c) for the NIPS12 corpus  where the results of
η = 0.002 and 0.001 for the HDP-LDA are omitted.

in [7]  where the HDP and BNBP are inferred under ﬁnite approximations with truncated block
Gibbs sampling. However  it does not necessarily mean that the HDP-LDA has better predictive
performance than the BNBP topic model. In fact  as shown in the right column of Figure 3  the
BNBP topic model’s perplexity tends to be lower than that of the HDP-LDA if their inferred number
of topics are comparable and the η is not overly small  which implies that the BNBP topic model is
able to achieve the same predictive power as the HDP-LDA  but with a more compact representation
of the corpus under common experimental settings. While it is interesting to understand the ultimate
potentials of the HDP-LDA and BNBP topic model for out-of-sample prediction by setting the
η to be very small  a moderate η that supports a moderate number of topics is usually preferred
in practice  for which the BNBP topic model could be a preferred choice over the HDP-LDA  as
our experimental results on three different corpora all suggest that the BNBP topic model could
achieve lower-perplexity using the same number of topics. To further understand why the BNBP
topic model and HDP-LDA have distinct characteristics  one may view them from a count-modeling
perspective [7] and examine how they differently control the relationship between the variances and
means of the latent topic usage count vectors {(n1k  . . .   nJk)}k.
We also ﬁnd that the BNBP collapsed Gibbs sampler clearly outperforms the blocked Gibbs sampler
of [7] in terms of convergence speed  computational complexity and memory requirement. But a
blocked Gibbs sampler based on ﬁnite truncation [7] or adaptive truncation [11] does have a clear
advantage that it is easy to parallelize. The heuristics used to parallelize an HDP collapsed sampler
[24] may also be modiﬁed to parallelize the proposed BNBP collapsed sampler.

5 Conclusions

A group size dependent exchangeable partition probability function (EPPF) for mixed-membership
modeling is developed using the integer-valued beta-negative binomial process (BNBP). The ex-
changeable random partitions of grouped data  governed by the EPPF of the BNBP  are strongly in-
ﬂuenced by the group-speciﬁc dispersion parameters. We construct a BNBP nonparametric Bayesian
topic model that is distinct from existing ones  intuitive to interpret  and straightforward to imple-
ment. The fully collapsed Gibbs sampler converges fast  mixes well  and has state-of-the-art predic-
tive performance when a compact representation of the corpus is desired. The method to derive the
EPPF for the BNBP via a group size dependent model is unique  and it is of interest to further inves-
tigate whether this method can be generalized to derive new EPPFs for mixed-membership modeling
that could be introduced by other integer-valued stochastic processes  including the gamma-Poisson
and gamma-negative binomial processes.

8

0.010.10.5100102104Topic Dirichlet parameter ηNumber of topics K(a)0.010.10.5240260280300320Topic Dirichlet parameter ηHeldout perplexity(b)101001000240260280300320Number of topics KHeldout perplexity(c)BNBP Topic ModelHDP−LDA0.010.10.5100102104Topic Dirichlet parameter ηNumber of topics K(d) 0.010.10.5800900100011001200Topic Dirichlet parameter ηHeldout perplexity(e)1010010002000800900100011001200Number of topics KHeldout perplexity(f)BNBP Topic ModelHDP−LDA0.010.10.5100102104Topic Dirichlet parameter ηNumber of topics K(g) 0.010.10.51000120014001600180020002200Topic Dirichlet parameter ηHeldout perplexity(h)1010010001000120014001600180020002200Number of topics KHeldout perplexity(i)BNBP Topic ModelHDP−LDAReferences
[1] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Ann. Statist.  1973.
[2] E. Regazzini  A. Lijoi  and I. Pr¨unster. Distributional results for means of normalized random

measures with independent increments. Annals of Statistics  2003.

[3] A. Lijoi and I. Pr¨unster. Models beyond the Dirichlet process. In N. L. Hjort  C. Holmes 
P. M¨uller  and S. G. Walker  editors  Bayesian nonparametrics. Cambridge Univ. Press  2010.
[4] D. Blackwell and J. MacQueen. Ferguson distributions via P´olya urn schemes. The Annals of

Statistics  1973.

[5] J. Pitman. Combinatorial stochastic processes. Lecture Notes in Mathematics. Springer-

Verlag  2006.

[6] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. JASA 

2006.

[7] M. Zhou and L. Carin. Negative binomial process count and mixture modeling. To appear in

IEEE Trans. Pattern Anal. Mach. Intelligence  2014.

[8] A. Y. Lo. Bayesian nonparametric statistical inference for Poisson point processes. Zeitschrift

fur  pages 55–66  1982.

[9] M. K. Titsias. The inﬁnite gamma-Poisson feature model. In NIPS  2008.
[10] M. Zhou  L. Hannah  D. Dunson  and L. Carin. Beta-negative binomial process and Poisson

factor analysis. In AISTATS  2012.

[11] T. Broderick  L. Mackey  J. Paisley  and M. I. Jordan. Combinatorial clustering and the beta
negative binomial process. To appear in IEEE Trans. Pattern Anal. Mach. Intelligence  2014.
[12] M. Zhou and S. G. Walker. Sample size dependent species models. arXiv:1410.3155  2014.
[13] M. Lomel´ı  S. Favaro  and Y. W. Teh. A marginal sampler for σ-Stable Poisson-Kingman

mixture models. arXiv preprint arXiv:1407.4211  2014.

[14] N. L. Hjort. Nonparametric Bayes estimators based on beta processes in models for life history

data. Ann. Statist.  1990.

[15] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In

AISTATS  2007.

[16] C. Heaukulani and D. M. Roy. The combinatorial structure of beta negative binomial processes.

arXiv:1401.0062  2013.

[17] M. Zhou  O.-H. Madrid-Padilla  and J. G. Scott. Priors for random count matrices derived from

a family of negative binomial processes. arXiv:1404.3331v2  2014.

[18] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In NIPS  2005.

[19] R. E. Madsen  D. Kauchak  and C. Elkan. Modeling word burstiness using the Dirichlet distri-

bution. In ICML  2005.

[20] M. Sibuya. Generalized hypergeometric  digamma and trigamma distributions. Annals of the

Institute of Statistical Mathematics  pages 373–390  1979.

[21] D. Blei  A. Ng  and M. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res.  2003.
[22] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS  2004.
[23] C. Wang  J. Paisley  and D. M. Blei. Online variational inference for the hierarchical Dirichlet

process. In AISTATS  2011.

[24] D. Newman  A. Asuncion  P. Smyth  and M. Welling. Distributed algorithms for topic models.

JMLR  2009.

[25] H. M. Wallach  I. Murray  R. Salakhutdinov  and D. Mimno. Evaluation methods for topic

models. In ICML  2009.

[26] J. Paisley  C. Wang  and D. Blei. The discrete inﬁnite logistic normal distribution for mixed-

membership modeling. In AISTATS  2011.

[27] I. Porteous  D. Newman  A. Ihler  A. Asuncion  P. Smyth  and M. Welling. Fast collapsed

Gibbs sampling for latent Dirichlet allocation. In SIGKDD  2008.

[28] D. Mimno  M. Hoffman  and D. Blei. Sparse stochastic inference for latent Dirichlet allocation.

In ICML  2012.

9

,Mingyuan Zhou
Jack Rae
Jonathan Hunt
Ivo Danihelka
Timothy Harley
Andrew Senior
Gregory Wayne
Alex Graves
Timothy Lillicrap
Alireza Aghasi
Afshin Abdi
Nam Nguyen
Justin Romberg
David Zoltowski
Jonathan Pillow