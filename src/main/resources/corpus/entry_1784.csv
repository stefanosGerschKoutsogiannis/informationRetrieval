2019,Efficiently Learning Fourier Sparse Set Functions,Learning set functions is a key challenge arising in many domains  ranging from sketching graphs to black-box optimization with discrete parameters. In this paper we consider the problem of efficiently learning set functions that are defined over a ground set of size $n$ and that are sparse (say $k$-sparse) in the Fourier domain. This is a wide class  that includes graph and hypergraph cut functions  decision trees and more. Our central contribution is the first algorithm that allows learning functions whose Fourier support only contains low degree (say degree $d=o(n)$) polynomials using $O(k d \log n)$ sample complexity and runtime $O( kn \log^2 k \log n \log d)$. This implies that sparse graphs with $k$ edges can  for the first time  be learned from $O(k \log n)$ observations of cut values and in linear time in the number of vertices. 
 Our algorithm can also efficiently learn (sums of) decision trees of small depth.
 The algorithm exploits techniques from the sparse Fourier transform literature and is easily implementable. Lastly  we also develop an efficient robust version of our algorithm and prove $\ell_2/\ell_2$ approximation guarantees without any statistical assumptions on the noise.,Efﬁciently Learning Fourier Sparse Set Functions

Andisheh Amrollahi ∗

ETH Zurich

Zurich  Switzerland
amrollaa@ethz.ch

Amir Zandieh ∗

EPFL

Lausanne  Switzerland

amir.zandieh@epfl.ch

Michael Kapralov†

EPFL

Lausanne  Switzerland

michael.kapralov@epfl.ch

Andreas Krause

ETH Zurich

Zurich  Switzerland
krausea@ethz.ch

Abstract

Learning set functions is a key challenge arising in many domains  ranging from
sketching graphs to black-box optimization with discrete parameters. In this paper
we consider the problem of efﬁciently learning set functions that are deﬁned over a
ground set of size n and that are sparse (say k-sparse) in the Fourier domain. This
is a wide class  that includes graph and hypergraph cut functions  decision trees and
more. Our central contribution is the ﬁrst algorithm that allows learning functions
whose Fourier support only contains low degree (say degree d = o(n)) polynomials
using O(kd log n) sample complexity and runtime O(kn log2 k log n log d). This
implies that sparse graphs with k edges can  for the ﬁrst time  be learned from
O(k log n) observations of cut values and in linear time in the number of vertices.
Our algorithm can also efﬁciently learn (sums of) decision trees of small depth.
The algorithm exploits techniques from the sparse Fourier transform literature and
is easily implementable. Lastly  we also develop an efﬁcient robust version of
our algorithm and prove (cid:96)2/(cid:96)2 approximation guarantees without any statistical
assumptions on the noise.

1

Introduction

How can we learn the structure of a graph by observing the values of a small number of cuts? Can we
learn a decision tree efﬁciently by observing its evaluation on a few samples? Both of these important
applications are instances of the more general problem of learning set functions.
Consider a set function which maps subsets of a ground set V of size n to real numbers  x : 2V → R.
Set functions that arise in applications often exhibit structure  which can be effectively captured in
the Fourier (also called Walsh-Hadamard) basis. One common studied structure for set functions
is Fourier sparsity [2]. A k-Fourier-sparse set function contains no more than k nonzero Fourier
coefﬁcients. A natural example for k-Fourier-sparse set functions are cut functions of graphs with
k edges or evaluations of a decision tree of depth d [7  8  12]. The cut function of a graph only
contains polynomials of degree at most two in the Fourier basis and in the general case  the cut
function of a hypergraph of degree d only contains polynomials of degree at most d in the Fourier
basis [12]. Intuitively this means that these set functions can be written as sums of terms where each
term depends on at most d elements in the ground set. Also a decision tree of depth d only contains
polynomials of degree at most d in the Fourier basis [7][8]. Learning such functions has recently

∗The ﬁrst two authors contributed equally
†Supported by ERC Starting Grant SUBLINEAR.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

found applications in neural network hyper-parameter optimization [5]. Therefore  the family of
Fourier sparse set functions whose Fourier support only contains low order terms is a natural and
important class of functions to consider.

Related work One approach for learning Fourier sparse functions uses Compressive Sensing

(CS) methods [12]. Suppose we know that the Fourier transform of our function(cid:98)x is k-sparse i.e.
|supp((cid:98)x)| ≤ k  and supp((cid:98)x) ⊆ P for some known set P of size p. In [12] it is shown that recovery
of(cid:98)x is possible (with high probability) by observing the value of x on O(k log4 p) subsets chosen
(cid:1) = O(n2)
a matrix satisfying the RIP which is required for recovery. For the case of graphs p =(cid:0)n

independently and uniformly at random. They utilize results from [10  13] which prove that picking
O(k log4 p) rows of the Walsh-Hadamard matrix independently and uniformly at random results in

2

and one can essentially learn the underlying graph with O(k log4 n) samples. In fact this result can be
further improved  and O(k log2 k log n) samples sufﬁce [4]. Computationally  for the CS approach 
one may use matching pursuit which takes Ω(kp) time and thus results in runtime of Ω(knd) for
k Fourier sparse functions of order d. This equals Ω(kn2) for graphs  where d = 2. In [12] 
proximal methods are used to optimize the Lagrangian form of the (cid:96)1 norm minimization problem.
Optimization is performed on p variables which results in Ω(n2) runtime for graphs and Ω(nd) time
for the general order d sparse recovery case. Hence  these algorithms scale exponentially with d and
have at least quadratic dependence on n even in the simple case of learning graph cut functions.
There is another line of work on this problem in the sparse Fourier transform literature. [11] provides a
non-robust version of the sparse Walsh Hadamard Transform (WHT). This algorithm makes restrictive
assumptions on the signal  namely that the k non-zero Fourier coefﬁcients are chosen uniformly
at random from the Fourier domain. This is a strong assumption that does not hold for the case
of cut functions or decision trees. This work is extended in [4] to a robust sparse WHT called
SPRIGHT. In addition to the the random uniform support assumption  [4] further presumes that the
Fourier coefﬁcients are ﬁnite valued and the noise is Gaussian. Furthermore  all existing sparse WHT
algorithms are unable to exploit low-degree Fourier structure.

Our results We build on techniques from the sparse Fourier transform literature [3  6  2] and
develop an algorithm to compute the Walsh-Hadamard transform (WHT) of a k-Fourier-sparse signal
whose Fourier support is constrained to low degree frequencies (low degree polynomials). For
recovering frequencies with low degree  we utilize ideas that are related to compressive sensing over

ﬁnite ﬁelds [1]. We show that if the frequencies present in the support of(cid:98)x are of low order then there

exists an algorithm that computes WHT in O(kn log2 k log n log d) time using O(kd log n) samples.
As opposed to [11]  we avoid distributional assumptions on the support using hashing schemes. Our
approach is the ﬁrst one to achieve the sampling complexity of O(kd log n). Moreover its running
time scales linearly in n and there is no exponential dependence on d. For the important special case
of graphs  where d = 2  our sampling complexity is near optimally O(k log n) and our runtime is
O(kn log2 k log n) which is strictly better than CS methods which take at least quadratic time in n.
This allows us to learn sparse graphs which have in the range of 800 vertices in ≈ 2 seconds whereas
the previous methods [12] were constrained to the range of 100 for similar runtimes.

For the case where (cid:98)x is not exactly k-sparse  we provide novel robust algorithms that recover
O(cid:0)nk log3 k + nk log2 k log n log(d log n) log d(cid:1).

the k dominant Fourier coefﬁcients with provable (cid:96)2/(cid:96)2 approximation guarantees. We provide
a robust algorithm using appropriate hashing schemes and a novel analysis. We further de-
velop a robust recovery algorithm that uses O(kd log n log(d log n)) samples and runs in time

2 Problem Statement

Here we deﬁne the problem of learning set functions. Consider a set function which maps subsets of
a ground set V (cid:44) {1  . . .   n} = [n] of size n to real numbers  x : 2V → R. We assume oracle access
to this function  that is  we can observe the function value x(A) for any subset A that we desire. The
goal is to learn the function  that is to be able to evaluate it for all subsets B ⊆ V . A problem which
has received considerable interest is learning cut functions of sparse (in terms of edges) graphs [12].
Given a weighted undirected graph G = (V  E  w)  the cut function associated to G is deﬁned as

s∈A t∈V \A w(s  t)  for every A ⊆ V .

x(A) =(cid:80)

2

Note that we can equivalently represent each subset A ⊆ V by a vector t ∈ Fn
2 which is the indicator
of set A. Here F2 = {0  1} denotes the ﬁnite ﬁeld with 2 elements. Hence the set function can be
2 → R. It
viewed as x : Fn
is deﬁned as:

2 → R. We denote the Walsh-Hadamard transform of x : Fn

2 → R by(cid:98)x : Fn

(cid:98)xf =

1√
N

(cid:88)

t∈Fn

2

xt · (−1)(cid:104)f t(cid:105)

  f ∈ Fn
2 .

The inner product (cid:104)f  t(cid:105) throughout the paper is performed modulo 2.

The Fourier transform of the graph cut function(cid:98)x is the following 

(cid:80)

 1

2

−w(s  t)/2
0

(cid:98)xf =

s t∈V w(s  t)

if f = (0  . . .   0)
if fs = ft = 1 and fi = 0 ∀i (cid:54)= s  t
otherwise

.

It is clear that the Fourier support of the cut function for graph G contains only |E| + 1 nonzero
elements (and hence it is sparse). Furthermore  the nonzero Fourier coefﬁcients correspond to
frequencies with hamming weights at most 2.
One of the classes of set functions that we consider is that of exactly low order Fourier sparse
functions. Under this model we address the following problem:

Input: oracle access to x : Fn

such that (cid:107)(cid:98)x(cid:107)0 ≤ k and |f| ≤ d for all f ∈ support((cid:98)x)

Output: nonzero coefﬁcients of(cid:98)x and their corresponding frequencies

2 → R

(1)

where |f| denotes the Hamming weight of f.
We also consider the robust version of problem (1) where we only have access to noisy measurements
of the input set function. We make no assumption about the noise  which can be chosen adversarially.
Equivalently one can think of a general set function whose spectrum is well approximated by a low
such that the frequency has low Hamming weight |f| ≤ d. We refer to the noise spectrum as tail.

order sparse function which we refer to as head. Head of(cid:98)x is just the top k Fourier coefﬁcients(cid:98)xf
Deﬁnition 1 (Head and Tail norm). For all integers n  d  and k we deﬁne the head of(cid:98)x : Fn
2 → R as 

(cid:98)xhead := arg

(cid:107)(cid:98)x − y(cid:107)2.

min
2 →R
y:Fn
(cid:107)y(cid:107)0≤k

The tail norm of(cid:98)x is deﬁned as  Err((cid:98)x  k  d) := (cid:107)(cid:98)x −(cid:98)xhead(cid:107)2

|j|≤d for all j∈supp(y)

2.

Since the set function to be learned is only approximately in the low order Fourier sparse model 
it makes sense to consider the approximate version of problem (1). We use the well known (cid:96)2/(cid:96)2
approximation to formally deﬁne the robust version of problem (1) as follows 

function(cid:98)χ : Fn
2 → R
Input: oracle access to x : Fn
2 → R
such that (cid:107)(cid:98)χ −(cid:98)x(cid:107)2
2 ≤ (1 + δ)Err((cid:98)x  k  d) 
Output:
|f| ≤ d for all f ∈ support((cid:98)χ)

(2)

Note that no assumptions are made about the function x and it can be any general set function.

3 Algorithm and Analysis

In this section we present our algorithm and analysis. We use techniques from the sparse FFT
literature [3  6  2]. Our main technical novelty is a new primitive for estimating a low order frequency 
i.e.  |f| ≤ d  efﬁciently using an optimal number of samples O(d log n) given in Section 3.1. This

primitive relies heavily on the fact that a low order frequency is constrained on a subset of size(cid:0)n

(cid:1) as

opposed to the whole universe of size 2n. We show that problem (1) can be solved quickly and using
a few samples from the function x by proving the following theorem 

d

3

probability 9/10. Moreover the runtime of this algorithm is O(cid:0)kn log2 k log n log d(cid:1) and the sample

Theorem 2. For any integers n  k  and d  the procedure EXACTSHT solves problem (1) with

complexity of this procedure is O (kd log n).

We also show that problem (2) can be solved efﬁciently by proving the following theorem in the full
version of this paper 
Theorem 3. For any
integers n 
problem (2) with probability 9/10.

O(cid:0)nk log3 k + nk log2 k log n log(d log n) log d(cid:1) and the sample complexity of the procedure is

the procedure ROBUSTSHT solves
the runtime of
this procedure is

and d 
Moreover

k 

O (kd log n log(d log n)).

Remark: This theorem proves that for any arbitrary input signal  we are able to achieve the (cid:96)2/(cid:96)2
guarantee using O (kd · log n · log(d log n)) samples. Using the techniques of [9] one can prove that
the sample complexity is optimal up to log(d log n) factor. Note that it is impossible to achieve this
sample complexity without exploiting the low degree structure of the Fourier support.

3.1 Low order frequency recovery
In this section we provide a novel method for recovering a frequency f ∈ Fn
2 with bounded Hamming
weight |f| ≤ d  from measurements (cid:104)mi  f(cid:105) i ∈ [s] for some s = O(d log n). The goal of this
section is to design a measurement matrix M ∈ Fs×n
2 with
|f| ≤ d the following system of constraints  with constant probability  has a unique solution j = f
and has an efﬁcient solver 

2 with small s  such that for any f ∈ Fn

(cid:26)M j = M f

|j| ≤ d

.

j ∈ Fn

2 such that

l=0

(cid:107)

(cid:106) (j mod 2l)

To design an efﬁcient solver for the above problem with optimal s  we ﬁrst need an optimal algorithm
for recovering frequencies with weight one |f| ≤ 1. In this case  we can locate the index of the
nonzero coordinate of f optimally via binary search using O(log n) measurements and runtime.
Deﬁnition 4 (Binary search vectors). For any integer n  the ensemble of vectors {vl}(cid:100)log2 n(cid:101)
⊆ Fn
corresponding to binary search on n elements is deﬁned as follows. Let v0 = {1}n (the all ones
vector). For every l ∈ {1 ···  (cid:100)log2 n(cid:101)} and every j ∈ [n]  vl
Lemma 5. There exists a set of measurements {mi}s
algorithm such that for every f ∈ Fn
(cid:104)f  mi(cid:105) in time O(log2 n).
To recover a frequency f with Hamming weight d  we hash the coordinates of f randomly into O(d)
buckets. In expectation  a constant fraction of nonzero elements of f get isolated in buckets  and
hence the problem reduces to the weight one recovery. We know how to solve this using binary search
as shown in Lemma 5 in time O(log n) and with sample complexity O(log n). We recover a constant
fraction of the nonzero indices of f and then we subtract those from f and recurse on the residual.
The pseudocode of the recovery procedure is presented in Algorithm 1.
Lemma 6. For any integers n and d   any power of two integer D ≥ 128d  and any frequency
f ∈ Fn
2 with |f| ≤ d  the procedure RECOVERFREQUENCY given in Algorithm 1 outputs f with
probability at least 7/8  if we have access to the following 

i=1 for s = (cid:100)log2 n(cid:101) + 1 together with an
2 with |f| ≤ 1 the algorithm can recover f from the measurements

j =

2l−1

.

2

1. For every r = 0  1 ···   log4 D  a hash function hr : [n] → [D/2r] which is an instance

from a pairwise independent hash family.

2. For every l = 0  1 ···  (cid:100)log2 n(cid:101) and every r = 0  1 ···   log4 D  the measurements φl

r(i)

that are equal to φl

r (i) fj · vl

−1

j for every i ∈ [D/2r].

j∈h

r(i) =(cid:80)

Moreover  the runtime of this procedure is O(D log D log n) and the number of measurements is
O(D log n).
Proof. The proof is by induction on the iteration number r = 0  1 ···   T . We denote by Er the event
|f − ˜f (r)| ≤ d
4r   that is the sparsity goes down by a factor of 4 in every iteration up to rth iteration.
The inductive hypothesis is Pr[Er+1|Er] ≥ 1 − 1

16·2r .

4

2

Algorithm 1 RECOVERFREQUENCY
input: power of two integer D  hash functions hr : [n] → [D/2r] for every r ∈ {0  1 ···   log4 D} 
r ∈ FD/2r
for every l = 0  1 ···(cid:100)log2 n(cid:101) and every r = 0  1 ···   log4 D.
measurement vectors φl
output: recovered frequency ˜f.
1: {vl}(cid:100)log2 n(cid:101)
l=0 ← binary search vectors on n elements (Deﬁnition 4)  T ← log4 D  ˜f (0) ← {0}n
2: for r = 0 to T do
w ← {0}n.
3:
for i = 1 to D/2r do
4:
5:
−1
j∈h
r (i)
6:
7:
8:
9:
10:
11:
12: return ˜f (T +1).

· v0
˜f (r)
j
index ← {0}(cid:100)log2 n(cid:101)  a (cid:100)log2 n(cid:101) bits pointer.
for l = 1 to (cid:100)log2 n(cid:101) do

w(index) ← 1  set the coordinate of w positioned at index to 1.

[index]l ← 1  set lth bit of index to 1.

r(i) −(cid:80)

r(i) −(cid:80)

˜f (r+1) ← ˜f (r) + w.

j = 1 then

j = 1 then

˜f (r)
j

if φ0

−1
r (i)

if φl

· vl

j∈h

4r . For every i ∈ [D/2r] and every l ∈
Conditioning on Er we have that |f − ˜f (r)| ≤ d
(cid:88)
{0  1 ···  (cid:100)log2 n(cid:101)} it follows from the deﬁnition of φl
r that 

(cid:16)

r(i) − (cid:88)

φl

˜f (r)
j

· vl

j =

fj − ˜f (r)

j

j∈h

−1
r (i)

j∈h

−1
r (i)

Let us denote by S the support of vector f − ˜f (r)  namely let S = supp
From the pairwise independence of the hash function hr the following holds for every a ∈ S 

.

Pr[hr(a) ∈ hr(S \ {a})] ≤ 2r · |S|

≤ 2r ·
This shows that for every a ∈ S  with probability 1 − 1
element of S. Because the vector f − ˜f (r) restricted to the elements in bucket h−1
Hamming weight one  for every a ∈ S 

128·2r   the bucket hr(a) contains no other
r (hr(a)) has

128 · 4r ≤

128 · 2r .

D

1

1

(cid:17) · vl
(cid:16)
f − ˜f (r)(cid:17)

j.

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)(cid:16)

f − ˜f (r−1)(cid:17)

Pr

(cid:12)(cid:12)(cid:12)(cid:12) = 1
(cid:21)

−1
r (hr(a))

h

≥ 1 −

1

128 · 2r .

If the above condition holds  then it is possible to ﬁnd the index of the nonzero element via binary
search as in Lemma 5. The for loop in line 7 of Algorithm 1 implements this. Therefore with
probability 1 − 1
16·2r by Markov’s inequality a 1 − 1/8 fraction of the support elements  S  gets
recovered correctly and at most 1/8 fraction of elements remain unrecovered and possibly result
in false positive. Since the algorithm recovers at most one element per bucket  the total number
of falsely recovered indices is no more than the number of non-isolated buckets which is at most
1/8 · |S|. Therefore with probability 1 − 1
16·2r   the residual at the end of rth iteration has sparsity
1/8 · |S| + 1/8 · |S| = 1/4 · |S|  i.e.
It follows from the event ET for T = log4 D that ˜f (T ) = f  where ˜f (T ) is the output of Algorithm

1. The inductive hypothesis along with union bound implies that Pr(cid:2) ¯ET
Pr(cid:2) ¯E0
therefore the time to calculate(cid:80)

Runtime:
the algorithm has three nested loops and the total number of repetitions of all loops
together is O(D log n). The recovered frequency ˜f (r) always has at most O(D) nonzero entries
j for a ﬁxed r and a ﬁxed l and all i ∈ [D/2r] is

(cid:12)(cid:12)(cid:12)f − ˜f (r+1)(cid:12)(cid:12)(cid:12) ≤ |S|

r=1 Pr(cid:2) ¯Er|Er−1

4r+1 . This proves the inductive step.

(cid:3) ≤(cid:80)T

(cid:3) ≤(cid:80)T

16·2r ≤ 1/8.

4 ≤ d

˜f (r−1)

(cid:3) +

· vl

r=0

1

j

j∈h

−1
r (i)

O(D). Therefore the total runtime is O(D log D log n).

5

Number of measurements:
vectors φl

r which is O(D log n).

the number of measurements is the total size of the measurement

3.2 Signal reduction

2

σ : Fb

2   and every σ ∈ Fn×b
σ(t) =

We now develop the main tool for estimating the frequencies of a sparse signal  namely the
HASH2BINS primitive. If we hash the frequencies of a k-sparse signal into O(k) buckets  we
expect most buckets to contain at most one of the elements of the support of our signal. The next
(cid:113) 2n
deﬁnition shows how we compute the hashing of a signal in the time domain.
Deﬁnition 7. For every n  b ∈ N  every a ∈ Fn
2 → R  we
deﬁne the hashing of(cid:98)x as ua
and every x : Fn
2 → R  where ua
2b · xσt+a  for every t ∈ Fb
2.
σ corresponds to hashing(cid:98)x into B buckets.
σ(j) =(cid:80)
2 (cid:98)ua
2 :σ(cid:62)f =j(cid:98)xf · (−1)(cid:104)a f(cid:105).
2 (cid:98)ua
σ is the sum of(cid:98)xf · (−1)(cid:104)a f(cid:105) for all frequencies f ∈ Fn

We denote by B (cid:44) 2b the number of buckets of the hash function. In the next claim we show that the
Fourier transform of ua
Claim 8. For every j ∈ Fb
Let h(f ) (cid:44) σ(cid:62)f. For every j ∈ Fb
2 such
that h(f ) = j  hence h(f ) can be thought of as the bucket that f is hashed into. If the matrix σ is
chosen uniformly at random then the hash function h(·) is pairwise independent.
Claim 9. For any n  b ∈ N  if the hash function h : Fn
2 is deﬁned as h(·) = σ(cid:62)(·)  where
σ ∈ Fn×b
is a random matrix whose entries are distributed independently and uniformly at random
on F2  then for any f (cid:54)= f(cid:48) ∈ Fn
B   where the probability is over
picking n · b random bits of σ.

2 it holds that Pr[h(f ) = h(f(cid:48))] = 1

2 → Fb

f∈Fn

2

σ.

2

2.

(cid:17)

.

f∈Fn

σ.
σ = FHT

Algorithm 2 HASH2BINS

  shift vector a ∈ Fn
2 .

(cid:46) FHT is the fast Hadamard transform algorithm

(cid:16)(cid:113) 2n
2b · xσ(·)+a
2 :σ(cid:62)f =j(cid:98)χf · (−1)(cid:104)a f(cid:105) for every j ∈ Fb

input: signal x ∈ R2n  signal(cid:98)χ ∈ R2n  integer b  binary matrix σ ∈ Fn×b
output: hashed signal(cid:98)ua
1: Compute(cid:98)ua
σ(j) −(cid:80)
σ(j) ←(cid:98)ua
2: (cid:98)ua
3: return(cid:98)ua
each of the buckets. We denote by(cid:98)χ the estimate of(cid:98)x in each iteration. As we will see in Section 3.3 
the recovery algorithm is iterative in the sense that we iterate over(cid:98)x −(cid:98)χ (the residue) whose sparsity
Claim 10. For any signal x (cid:98)χ : Fn
the procedure HASH2BINS(x (cid:98)χ  b  σ  a) given in Algorithm 2 computes the following using O(B)
samples from x in time O(Bn log B + (cid:107)(cid:98)χ(cid:107)0 · n log B)

The HASH2BINS primitive computes the Fourier coefﬁcients of the residue signal that are hashed to

2 → R  integer b  matrix σ ∈ Fn×b
(cid:88)

is guaranteed to decrease by a constant factor in each step.

  and vector a ∈ Fn

2

2

(cid:98)ua

σ(j) =

((cid:92)x − χ)f · (−1)(cid:104)a f(cid:105).

f∈Fn

2 :σ(cid:62)f =j

3.3 Exact Fourier recovery

deﬁned in (1) and prove Theorem 2. Let S (cid:44) supp((cid:98)x). Problem (1) assumes that |S| ≤ k and also

In this section  we present our algorithm for solving the exact low order Fourier sparse problem
for every f ∈ S  |f| ≤ d. The recovery algorithm hashes the frequencies into B = 2b buckets using
Algorithm 2. Every frequency in the support f ∈ S is recoverable  with constant probability  if no
other frequency from the support collides with it in the hashed signal. The collision event is formally
deﬁned below 
Deﬁnition 11 (Collision). For any frequency f ∈ Fn
h(f ) ∈ h(S \ {f}).

2 and every sparse signal (cid:98)x with support
S = supp((cid:98)x)  the collision event Ecoll(f ) corresponding to the hash function h(f ) = σ(cid:62)f holds iff

6

2

2 → Fb

2   if the hash function h : Fn

Claim 12 (Probability of collision). For every f ∈ Fn
as h(·) = σ(cid:62)(·)  where σ ∈ Fn×b
and uniformly at random on F2 then Pr[Ecoll(f )] ≤ k
the randomness of matrix σ.
If the hash function h(·) = σ(cid:62)(·) is such that the collision event Ecoll(f ) does not occur for a
frequency f  then it follows from Claim 8 and Deﬁnition 11 that for every a ∈ Fn
2  

2 is deﬁned
is a random matrix whose entries are distributed independently
B (see Deﬁnition 11). The probability is over

(cid:98)ua
σ(h(f )) =(cid:98)xf · (−1)(cid:104)a f(cid:105).
σ(h(f )) =(cid:98)xf . Hence for any m ∈ Fn
(cid:98)ua
If a = {0}n then 
sign of(cid:98)um
σ (h(f )) = (cid:98)xf · (−1)(cid:104)m f(cid:105) and(cid:98)ua
2   one can learn the inner product (cid:104)m  f(cid:105) by comparing the
σ(h(f )). If the signs are the same then (−1)(cid:104)m f(cid:105) = 1
meaning that (cid:104)m  f(cid:105) = 0 and if the signs are different then (cid:104)m  f(cid:105) = 1. In Section 3.1 we gave
an algorithm for learning a low order frequency |f| ≤ d from measurements of the form (cid:104)m  f(cid:105).
So putting these together gives the inner subroutine for our sparse fast Hadamard transform  which
performs one round of hashing  presented in Algorithm 3.

Therefore  under this condition  the problem reduces to d-sparse recovery.

l=0

Algorithm 3 SHTINNER

input: signal x ∈ R2n  signal(cid:98)χ ∈ R2n  failure probability p  integer b  integer d.
output: recovered signal(cid:98)χ(cid:48).
1: Let {vl}(cid:100)log2 n(cid:101)
be binary search vectors on n elements (Deﬁnition 4).
2: D ← smallest power of two integer s.t. D ≥ 128d  R ← (cid:100)2 log2(1/p)(cid:101).
r : [n] → [D/2r] be an independent
3: For every r ∈ {0  1 ···   log4 D} and every s ∈ [R]  let hs
4: For every r ∈ {0  1 ···   log4 D}  every s ∈ [R]  and every j ∈ [D/2r] let wj
2 be the
binary indicator vector of the set hs
5: For every s ∈ [R]  every r ∈ {0  1 ···   log4 D} and every l ∈ {0  1 ···  (cid:100)log2 n(cid:101)} and every
r s · vl to set As.
j ∈ [D/2r]  add wj
7: For every a ∈ ∪s∈[R]As compute(cid:98)ua
6: Let σ ∈ Fn×b

σ = HASH2BINS(x (cid:98)χ  b  σ  a).

be a random matrix. Each entry is independent and uniform on F2.

copy of a pairwise independent hash function.

r s ∈ Fn

r(j)−1.

2

8: for j = 1 to B do
Let L be an empty multi-set.
9:
for s ∈ [R] do
10:
11:
12:

if(cid:98)uc
for every r ∈ {0 ···   log4 D}  every i ∈ [D/2r]  and every l ∈ {0 ···  (cid:100)log2 n(cid:101)} do
σ(j) (cid:54)= 0  where c = {0}n then
if(cid:98)uc
σ(j) and(cid:98)u

(j) have same sign then φl

r(i) ← 1.

(cid:19)
(cid:111)(cid:100)log2 n(cid:101)
r(i) ← 0. else φl

(cid:18)

D {hs

r}log2 D

r=0

 

r}log4 D

r=0

(cid:110){φl

r s·vl
wi
σ
˜f ← RECOVERFREQUENCY
Append ˜f to multi-set L.

.

l=0

13:

14:

15:
16:
17:

f ← majority(L)

(cid:98)χ(cid:48)
f ←(cid:98)uc(j)  where c = {0}n.
18: return(cid:98)χ(cid:48).
Lemma 13. For all integers b and d  every signals x (cid:98)χ ∈ R2n such that |ξ| ≤ d for every
ξ ∈ supp((cid:92)x − χ)  and any parameter p > 0  Algorithm 3 outputs a signal (cid:98)χ(cid:48) ∈ R2n such that
|supp((cid:98)χ(cid:48))| ≤ |supp((cid:92)x − χ)| and also for every frequency f ∈ supp((cid:92)x − χ)  if the collision event

Ecoll(f ) does not happen then 

(cid:16)

(cid:17)
Moreover the sample complexity of this procedure is O(Bd log n log 1
.
is O
x (cid:98)χ ∈ R2n such that (cid:107)(cid:92)x − χ(cid:107)0 ≤ k and |ξ| ≤ d for every ξ ∈ supp((cid:92)x − χ)  the output of
Lemma 14. For any parameter p > 0  all integers k  d  and b ≥ log2(k/p)  every signal

p ) and also its time complexity

p ) + nB log n log d log 1

B log B(n + d log n log 1

p )

(cid:105) ≥ 1 − p.
p + (cid:107)(cid:98)χ(cid:107)0 · n(log B + log n log d log 1

(cid:104)(cid:98)χ(cid:48)

Pr

f = ((cid:92)x − χ)f

7

SHTINNER(x (cid:98)χ  p  b  d) (cid:98)χ(cid:48) satisﬁes the following with probability at least 1 − 32p 

(cid:107)(cid:98)x −(cid:98)χ − (cid:98)χ(cid:48)(cid:107)0 ≤ k/8.

Our sparse Hadamard transform algorithm iteratively calls the primitive SHTINNER to reduce the
sparsity of the residual signal by a constant factor in every iteration. Hence  it terminates in O(log k)
iterations. See Algorithm 4.

Algorithm 4 EXACTSHT
output: estimate(cid:98)χ ∈ R2n.
input: signal x ∈ R2n  failure probability q  sparsity k  integer d.
1: p(1) ← q/32  b(1) ← (cid:100)log2
q (cid:101)  w(0) ← {0}2n
(cid:101)χ ← SHTINNER(x  w(r−1)  p(r)  b(r)  d)
2: for r = 1 to T do
w(r) ← w(r−1) +(cid:101)χ.
3:
4:
5:

p(r+1) ← p(r)/2  b(r+1) ← b(r) − 2.

  T ← (cid:100)log8 k(cid:101).

64k

6: (cid:98)χ ← w(T ).
7: return(cid:98)χ.
(cid:107)(cid:98)x − w(r)(cid:107)0 ≤ k
Er−1 we have that (cid:107)(cid:98)x − w(r−1)(cid:107)0 ≤ k
(cid:107)(cid:98)x − w(r)(cid:107)0 ≤ k
to(cid:98)χ  hence we can assume that (cid:107)(cid:98)χ(cid:107)0 ≤ 128k
O(cid:0)kn log2 k log n log d(cid:1).
The sample complexity of iteration r is O(cid:0) kd

Runtime and Sample complexity:
2b(r)

q·4r and the error probability p(r) = q

= 64k

8r . This proves the inductive step.

Proof of Theorem 2: The proof is by induction. We denote by Er the event corresponding to
8r . The inductive hypothesis is Pr[Er|Er−1] ≥ 1 − 16p(r). Conditioned on
8r−1 . The number of buckets in iteration r of the algorithm
4r−1·q . Hence  it follows from Lemma 14  that with probability 1 − 32p(r) 

is B(r) = 2br ≥ 64k

In iteration r ∈ [(cid:100)log8 k(cid:101)]  the size of the bucket B(r) =
r B(r) elements are added
. From Lemma 13 it follows that the total runtime is

32·2r . Moreover at most(cid:80)
2r log n log 2r(cid:1) hence the total sample complexity is

q

dominated by the sample complexity of the ﬁrst iteration which is equal to O (kd log n).

4 Experiments

We test our EXACTSHT algorithm for graph sketching on a real world data set. We utilize the
autonomous systems dataset from the SNAP data collection.3 In order to compare our methods with
[12] we reproduce their experimental setup. The dataset consists of 9 snapshots of an autonomous
system in Oregon on 9 different dates. The goal is detect which edges are added and removed when
comparing the system on two different dates. As a pre-processing step  we ﬁnd the common vertices
that exist on all dates and look at the induced subgraphs on these vertices. We take the symmetric
differences (over the edges) of dates 7 and 9. Results for other date combinations can be found in the
supplementary material. This results in a sparse graph (sparse in the number of edges). Recall that
the running time of our algorithm is O(kn log2 k log n log d) which reduces to O(nk log2 k log n)
for the case of cut functions where d = 2.

4.1 Sample and time complexities as number of vertices varies

In the ﬁrst experiment depicted in Figure 1b we order the vertices of the graph by their degree and
look at the induced subgraph on the n largest vertices in terms of degree where n varies. For each n
we pick e = 50 edges uniformly at random. The goal is to learn the underlying graph by observing
the values of cuts. We choose parameters of our algorithm such that the probability of success is at
least 0.9. The parameters tuned in our algorithm to reach this error probability are the initial number

3snap.stanford.edu/data/

8

(a) Avg. time vs. no. edges

(b) Avg. time vs. no. vertices

Table 1: Sampling and computational complexity
Our method

CS method

No. of vertices

Runtime Samples Runtime Samples

70
90
110
130
150
170
190
210
230
250
300
400
500
600
700
800

1.14
1.88
3.00
4.31
5.34
6.13
7.36
8.24
∗
∗
∗
∗
∗
∗
∗
∗

767
812
850
880
905
927
947
965
∗
∗
∗
∗
∗
∗
∗
∗

0.85
0.92
0.82
1.01
1.16
1.22
1.18
1.28
1.38
1.38
1.66
2.06
2.42
3.10
3.35
3.60

6428
6490
6491
7549
7942
7942
7271
7271
7942
7271
8051
8794
8794
9646
9646
9646

of buckets the frequencies are hashed to and the ratio at which they reduce in each iteration. We plot
running times as n varies. We compare our algorithm with that of [12] which utilizes a CS approach.
We ﬁne-tune their algorithm by tuning the sampling complexity. Both algorithms are run in a way
such that each sample (each observation of a cut value) takes the same time. As one can see our
algorithm scales linearly with n (up to log factors) whereas the CS approach scales quadratically.
Our algorithm continues to work in a reasonable amount of time for vertex sizes as much as 900 in
under 2 seconds. Error bars depict standard deviations.
In Table 1 we include both sampling complexities (number of observed cuts) and running times
as n varies. Our sampling complexity is equal to O(k log n). In practice they perform around a
constant factor of 10 worse than the compressive sensing method  which are not provably optimal (see
Section 1) but perform well in practice. In terms of computational cost  however  the CS approach
quickly becomes intractable  taking large amounts of time on instance sizes around 200 and larger
[12]. Asterisks in Table 1 refer to experiments that have taken too long to be feasible to run.

4.2 Time complexities as number of edges varies

Here we ﬁx the number of vertices to n = 100 and consider the induced subgraph on these vertices.
We randomly pick e edges to include in the graph. We plot computational complexities. Our running
time provably scales linearly in the number of edges as can be seen in Figure 1a.

9

References
[1] Abhik Kumar Das and Sriram Vishwanath. On ﬁnite alphabet compressive sensing. In 2013
IEEE International Conference on Acoustics  Speech and Signal Processing  pages 5890–5894.
IEEE  2013.

[2] Oded Goldreich and Leonid A Levin. A hard-core predicate for all one-way functions. In
Proceedings of the twenty-ﬁrst annual ACM symposium on Theory of computing  pages 25–32.
ACM  1989.

[3] Haitham Hassanieh  Piotr Indyk  Dina Katabi  and Eric Price. Nearly optimal sparse fourier
transform. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing 
pages 563–578. ACM  2012.

[4] Ishay Haviv and Oded Regev. The restricted isometry property of subsampled fourier matrices.

In Geometric Aspects of Functional Analysis  pages 163–179. Springer  2017.

[5] Elad Hazan  Adam Klivans  and Yang Yuan. Hyperparameter optimization: a spectral approach.

In International Conference on Learning Representations  2018.

[6] Piotr Indyk  Michael Kapralov  and Eric Price. (nearly) sample-optimal sparse fourier transform.
In Proceedings of the twenty-ﬁfth annual ACM-SIAM symposium on Discrete algorithms  pages
480–499. Society for Industrial and Applied Mathematics  2014.

[7] Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the fourier spectrum. SIAM

Journal on Computing  22(6):1331–1348  1993.

[8] Yishay Mansour. Learning boolean functions via the fourier transform. In Theoretical advances

in neural computation and learning  pages 391–424. Springer  1994.

[9] Eric Price and David P. Woodruff. (1 + eps)-approximate sparse recovery. In Proceedings of the
2011 IEEE 52Nd Annual Symposium on Foundations of Computer Science  FOCS ’11  pages
295–304  Washington  DC  USA  2011. IEEE Computer Society.

[10] Mark Rudelson and Roman Vershynin. On sparse reconstruction from fourier and gaussian
measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences  61(8):1025–1045  2008.

[11] Robin Scheibler  Saeid Haghighatshoar  and Martin Vetterli. A fast hadamard transform for
signals with sublinear sparsity in the transform domain. IEEE Transactions on Information
Theory  61(4):2115–2132  2015.

[12] Peter Stobbe and Andreas Krause. Learning fourier sparse set functions. In Artiﬁcial Intelligence

and Statistics  pages 1125–1133  2012.

[13] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

e-prints  page arXiv:1011.3027  Nov 2010.

10

,Andisheh Amrollahi
Amir Zandieh
Michael Kapralov
Andreas Krause