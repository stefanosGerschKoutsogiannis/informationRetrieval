2018,Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks,We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step  it guarantees that the best fitness in the population will never degrade. In addition  individuals in the population optimized with various SGD-based optimizers using distinct hyper-parameters in the SGD step are considered as competing species in a coevolution setting such that the complementarity of the optimizers is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition  image recognition and language modeling  using networks with a variety of deep architectures.,Evolutionary Stochastic Gradient Descent for

Optimization of Deep Neural Networks

Xiaodong Cui  Wei Zhang  Zoltán Tüske and Michael Picheny

{cuix  weiz  picheny}@us.ibm.com  {Zoltan.Tuske}@ibm.com

IBM Research AI

IBM T. J. Watson Research Center
Yorktown Heights  NY 10598  USA

Abstract

We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD)
framework for optimizing deep neural networks. ESGD combines SGD and
gradient-free evolutionary algorithms as complementary algorithms in one frame-
work in which the optimization alternates between the SGD step and evolution
step to improve the average ﬁtness of the population. With a back-off strategy in
the SGD step and an elitist strategy in the evolution step  it guarantees that the
best ﬁtness in the population will never degrade. In addition  individuals in the
population optimized with various SGD-based optimizers using distinct hyper-
parameters in the SGD step are considered as competing species in a coevolution
setting such that the complementarity of the optimizers is also taken into account.
The effectiveness of ESGD is demonstrated across multiple applications including
speech recognition  image recognition and language modeling  using networks
with a variety of deep architectures.

1

Introduction

Stochastic gradient descent (SGD) is the dominant technique in deep neural network optimization
[1]. Over the years  a wide variety of SGD-based algorithms have been developed [2  3  4  5].
SGD algorithms have proved to be effective in optimization of large-scale deep learning models.
Meanwhile  gradient-free evolutionary algorithms (EA) [6  7  8  9  10] also have been used in various
applications. They represent another family of so-called black-box optimization techniques which
are well suited for some non-linear  non-convex or non-smooth optimization problems. Biologically
inspired  population-based EA make no assumptions on the optimization landscape. The population
evolves based on genetic variation and selection towards better solutions of the problems of interest.
In deep learning applications  EA such as genetic algorithms (GA)  evolutionary strategies (ES) and
neuroevolution have been used for optimizing neural network architectures [11  12  13  14  15] and
tuning hyper-parameters [16  17]. Applying EA to the direct optimization of deep neural networks
is less common. In [18]  a simple EA is shown to be competitive to SGD when optimizing a small
neural network (around 1 000 parameters). However  competitive performance on a state-of-the-art
deep neural network with complex architectures and more parameters is yet to be seen.
The complementarity between SGD and EA is worth investigating. While SGD optimizes objective
functions based on their gradient or curvature information  gradient-free EA sometimes are advan-
tageous when dealing with complex and poorly-understood optimization landscape. Furthermore 
EA are population-based so computation is intrinsically parallel. Hence  implementation is very
suitable for large-scale distributed optimization. In this paper we propose Evolutionary Stochastic
Gradient Descent (ESGD) – a framework to combine the merits of SGD and EA by treating them as
complementary optimization techniques.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Given an optimization problem  ESGD works with a population of candidate solutions as individ-
uals. Each individual represents a set of model parameters to be optimized by an optimizer (e.g.
conventional SGD  Nesterov accelerated SGD or ADAM) with a distinct set of hyper-parameters (e.g.
learning rate and momentum). Optimization is carried out by alternating SGD and EA in a stage-wise
manner in each generation of the evolution. Following the EA terminology [6  19  20]  consider each
individual in the population as a “species". Over the course of any single generation  each species
evolves independently in the SGD step  and then interacts with each other in the EA step. This has the
effect of producing more promising candidate solutions for the next generation  which is coevolution
in a broad sense. Therefore  ESGD not only integrates EA and SGD as complementary optimization
strategies but also makes use of complementary optimizers under this coevolution mechanism. We
evaluated ESGD in a variety of tasks. Experimental results showed the effectiveness of ESGD across
all of these tasks in improving performance.

2 Related Work

The proposed ESGD is pertinent to neuroevolution [21  22] which consists of a broad family of
techniques to evolve neural networks based on EA. A large amount of work in this domain is devoted
to optimizing the networks with respect to their architectures and hyper-parameters [11  12  15  16 
23  24]. Recently remarkable progress has been made in reinforcement learning (RL) using ES
[22  25  26  27  28]. In the reported work  EA is utilized as an alternative approach to SGD and is
able to compete with state-of-the-art SGD-based performance in RL with deep architectures. It shows
that EA works surprisingly well in RL where only imperfect gradient is available with respect to
the ﬁnal performance. In our work  rather than treating EA as an alternative optimization paradigm
to replace SGD  the proposed ESGD attempts to integrate the two as complementary paradigms to
optimize the parameters of networks.
The ESGD proposed in this paper carries out population-based optimization which deals with a
set of models simultaneously. Many of the neuroevolution approaches also belong to this category.
Recently population-based techniques have also been applied to optimize neural networks with deep
architectures  most notably population-based training (PBT) in [17]. Although both ESGD and PBT
are population-based optimization strategies whose motivations are similar in spirit  there are clear
differences between the two. While evolution is only used for optimizing the hyper-parameters in
PBT  ESGD treats EA and SGD as complementary optimizers to directly optimize model parameters
and only indirectly optimize hyper-parameters. We investigate ESGD in the conventional setting
of supervised learning of deep neural networks with a ﬁxed architecture without explicit tuning of
hyper-parameters. More importantly  ESGD uses a model back-off and elitist strategy to give a
theoretical guarantee that the best model in the population will never degrade.
The idea of coevolution is used in the design of ESGD where candidates under different optimizers
can be considered as competing species. Coevolution has been widely employed for improved
neuroevolution [19  20  29  30] but in cooperative coevolution schemes species typically represent a
subcomponent of a solution in order to decompose difﬁcult high-dimensional problems. In ESGD 
the coevolution is carried out on competing optimizers to take advantage of their complementarity.

3 Evolutionary SGD

3.1 Problem Formulation
Consider the supervised learning problem. Suppose X ⊆ Rdx is the input space and Y ⊆ Rdy is the
output (label) space. The goal of learning is to estimate a function h that maps from the input to the
output

h(x; θ) : X → Y

(1)
where x ∈ X and h comes from a family of functions parameterized by θ ∈ Rd. A loss function
(cid:96)(h(x; θ)  y) is deﬁned on X ×Y to measure the closeness between the prediction h(x; θ) and the
label y∈Y. A risk function R(θ) for a given θ is deﬁned as the expected loss over the underlying
joint distribution p(x  y):

R(θ) = E(x y)[(cid:96)(h(x; θ)  y)]

(2)

2

We want to ﬁnd a function h(x; θ∗) that minimizes this expected risk. In practice  we only have
i=1∈X×Y which are independently drawn from p(x  y).
access to a set of training samples {(xi  yi)}n
Accordingly  we minimize the following empirical risk with respect to n samples

n(cid:88)

i=1

n(cid:88)

i=1

Rn(θ) =

1
n

(cid:96)(h(xi; θ)  yi) (cid:44) 1
n

li(θ)

(3)

where li(θ) (cid:44) (cid:96)(h(xi; θ)  yi). Under stochastic programming  Eq.3 can be cast as

(4)
where ω ∼ Uniform{1 ···   n}. In the conventional SGD setting  at iteration k  a sample (xik   yik ) 
ik ∈ {1 ···   n}  is drawn at random and the stochastic gradient ∇lik is then used to update θ with
an appropriate stepsize αk > 0:

Rn(θ) = Eω[lω(θ)]

θk+1 = θk − αk∇lik (θk).

(5)

In conventional SGD optimization of Eq.3 or Eq.4  there is only one parameter vector θ under
consideration. We further assume θ follows some distribution p(θ) and consider the expected
empirical risk over p(θ)

In practice  a population of µ candidate solutions  {θj}µ
average empirical risk of the population

J = Eθ[Rn(θ)] = Eθ[Eω[lω(θ)]]

(6)
j=1  is drawn and we deal with the following

µ(cid:88)

j=1

Jµ =

1
µ

Rn(θj) =

1
µ

(cid:32)

µ(cid:88)

j=1

n(cid:88)

i=1

1
n

(cid:33)

li(θj)

(7)

Eq.6 and Eq.7 formulate the objective function of the proposed ESGD algorithm. Following the EA
terminology  we interpret the empirical risk Rn(θ) given parameter θ as the ﬁtness function of θ
which we want to minimize. 1 We want to choose a population of parameter θ  {θj}µ
j=1  such that
the whole population or its selected subset has the best average ﬁtness values.
Deﬁnition 1 (m-elitist average ﬁtness). Let Ψµ = {θ1 ···   θµ} be a population with µ individuals θj
and let f be a ﬁtness function associated with each individual in the population. Rank the individuals
in the ascending order

(8)
where θk:µ denotes the k-th best individual of the population [9]. The m-elitist average ﬁtness of
Ψµ is deﬁned to be the average of ﬁtness of the ﬁrst m-best individuals (1≤ m≤ µ)

f (θ1:µ) ≤ f (θ2:µ) ≤ ··· ≤ f (θµ:µ)

m(cid:88)

k=1

J ¯m:µ =

1
m

f (θk:µ)

(9)

Note that  when m = µ  J ¯m:µ amounts to the average ﬁtness of the whole population. When m = 1 
J ¯m:µ = f (θ1:µ)  the ﬁtness of the single best individual of the population.

3.2 Algorithm

ESGD iteratively optimizes the m-elitist average ﬁtness of the population deﬁned in Eq.9. The
evolution inside each ESGD generation for improving J ¯m:µ alternates between the SGD step  where
each individual θj is updated using the stochastic gradient of the ﬁtness function Rn(θj)  and the
evolution step  where the gradient-free EA is applied using certain transformation and selection
operators. The overall procedure is given in Algorithm 1.
To initialize ESGD  a parent population Ψµ with µ individuals is ﬁrst created. This population evolves
in generations. Each generation consists of an SGD step followed by an evolution step. In the SGD

1Conventionally one wants to increase the ﬁtness. But to keep the notation uncluttered we will deﬁne the

ﬁtness function here as the risk function which we want to minimize.

3

Algorithm 1: Evolutionary Stochastic Gradient Descent (ESGD)
Input: generations K  SGD steps Ks  evolution steps Kv  parent population size µ  offspring
population size λ and elitist level m.
Initialize population Ψ(0)
// K generations
for k = 1 : K do

1  ···   θ(0)
µ };

µ ← {θ(0)

Update population Ψ(k)

µ ← Ψ(k−1)

µ

;

// in parallel
for j = 1 : µ do

for individual θ(k)

;

j

j

Pick an optimizer π(k)
Select hyper-parameters of π(k)
// Ks SGD steps
for s = 1 : Ks do

j

and set a learning schedule;

SGD update of individual θ(k)
If the ﬁtness degrades  the individual backs off to the previous step s−1.

using π(k)

;

j

j

end

end
// Kv evolution steps
for v = 1 : Kv do

λ ← {θ(k)

1  ···   θ(k)
λ };

Generate offspring population Ψ(k)
µ+λ ← Ψ(k)
Sort the ﬁtness of the parent and offspring population Ψ(k)
Select the top m (m ≤ µ) individuals with the best ﬁtness (m-elitist);
Update population Ψ(k)
non-m-elitist candidates;

λ ;
µ by combining m-elitist and randomly selected µ−m

µ

(cid:83) Ψ(k)

end

end

step  an SGD-based optimizer πj with certain hyper-parameters and learning schedule is selected
for each individual θj which is updated by Ks epochs. In this step  there is no interaction between
the optimizers. From the EA perspective  their gene isolation as a species is preserved. After each
epoch  if the individual has a degraded ﬁtness  θj will back off to the previous epoch. After the SGD
step  the gradient-free evolution step follows. In this step  individuals in the parent population Ψµ
start interacting via model combination and mutation to produce an offspring population Ψλ with
λ offsprings. An m-elitist strategy is applied to the combined population Ψµ+λ = Ψµ
the m (m≤ µ) individuals with the best ﬁtness are selected  together with the rest µ−m randomly
selected individuals to form the new parent population Ψµ for the next generation.
The following theorem shows that the proposed ESGD given in Algorithm 1 guarantees that the
m-elitist average ﬁtness will never degrade.
Theorem 1. Let Ψµ be a population with µ individuals {θj}µ
j=1. Suppose Ψµ evolves according to
the ESGD algorithm given in Algorithm 1 with back-off and m-elitist. Then for each generation k 

(cid:83) Ψλ where

¯m:µ ≤ J (k−1)
J (k)
¯m:µ  

k ≥ 1

The proof of the theorem is given in the supplementary material. From the theorem  we also have the
following corollary regarding the m-elitist average ﬁtness.
Corollary 1. ∀m(cid:48)  1 ≤ m(cid:48) ≤ m  we have

¯m(cid:48):µ ≤ J (k−1)
J (k)
¯m(cid:48):µ  

for k ≥ 1.

Particularly  for m(cid:48) = 1  we have

f (k)(θ1:µ) ≤ f (k−1)(θ1:µ) 

for k ≥ 1.

The ﬁtness of the best individual in the population never degrades.

(10)

(11)

4

3.3

Implementation

In this section  we give the implementation details of ESGD. The initial population is created either
by randomized initialization of the weights of networks or by perturbing some existing networks.
In the SGD step of each generation  a family of SGD-based optimizers (e.g. conventional SGD
and ADAM) is considered. For each selected optimizer  a set of hyper-parameters (e.g. learning
rate  momentum  Nesterov acceleration and dropout rate) is chosen and a learning schedule is set.
The hyper-parameters are randomly selected from a pre-deﬁned range. In particular  an annealing
schedule is applied to the range of the learning rate over generations.
In the evolution step there are a wide variety of evolutionary algorithms that can be considered.
Despite following similar biological principles  these algorithms have diverse evolving diagrams.
In this work  we use the (µ/ρ+λ)-ES [6]. Speciﬁcally  we have the following transformation and
selection schemes:

1. Encoding: Parameters are vectorized into a real-valued vector in the continuous space.
2. Selection  recombination and mutation:

In generation k  ρ individuals are selected from the
parent population Ψ(k)
µ using roulette wheel selection where the probability of selection is
proportional to the ﬁtness of an individual [7]. An individual with better ﬁtness has a higher
probability to be selected. λ offsprings are generated to form the offspring population Ψ(k)
λ
by intermediate recombination followed by a perturbation with the zero-mean Gaussian
noise  which is given in Eq.12.

ρ(cid:88)

j=1

θ(k)
i =

1
ρ

θ(k)
j + (k)

i

(12)

k). An annealing schedule may be applied

λ   θj ∈ Ψ(k)
where θi ∈ Ψ(k)
to the mutation strength σ2

i ∼ N (0  σ2

µ and (k)
k over generations.

(cid:83) Ψ(k)

λ is evaluated.

3. Fitness evaluation: After the offspring population is generated  the ﬁtness value for each

individual in Ψ(k)

µ+λ = Ψ(k)

µ

4. m-elitist: m (1≤ m≤ µ) individuals with the best ﬁtness are ﬁrst selected from Ψ(k)

µ+λ. The
rest µ−m individuals are then randomly selected from the other µ+λ−m candidates in
µ+λ to form the parent population Ψ(k+1)
Ψ(k)

of the next generation.

µ

After ESGD training is ﬁnished  the candidate with the best ﬁtness in the population θ1:µ is used as
the ﬁnal model for classiﬁcation or regression tasks. All the SGD updates and ﬁtness evaluation are
carried out in parallel on a set of GPUs.

4 Experiments

We evaluate the performance of the proposed ESGD on large vocabulary continuous speech recog-
nition (LVCSR)  image recognition and language modeling. We compare ESGD with two baseline
systems. The ﬁrst baseline system  denoted “single baseline” when reporting experimental results  is
a well-established single model system with respect to the application under investigation  trained
using certain SGD-based optimizer with appropriately selected hyper-parameters following certain
training schedule. The second baseline system  denoted “population baseline”  is a population-based
system with the same size of population as ESGD. Optimizers being considered are SGD and ADAM
except in image recognition where only SGD variants are considered. The optimizers together with
their hyper-parameters are randomly decided at the beginning and then ﬁxed for the rest of the
training with a pre-determined training schedule. This baseline system is used to mimic the typical
hyper-parameter tuning process when training deep neural network models. We also conducted
ablation experiments where the evolution step is removed from ESGD to investigate the impact of
evolution. The m-elitist strategy is applied to 60% of the parent population.

4.1 Speech Recognition

BN50 The 50-hour Broadcast News is a widely used dataset for speech recognition [31]. The
50-hour data consists of 45-hour training set and a 5-hour validation set. The test set comprises 3

5

hours of audio from 6 broadcasts. The acoustic models we used in the experiments are fully-connected
feed-forward network with 6 hidden layers and one softmax output layer with 5 000 states. There
are 1 024 units in the ﬁrst 5 hidden layers and 512 units in the last hidden layer. Sigmoid activation
functions are used for all hidden units except the bottom 3 hidden layers in which ReLU functions
are used. The fundamental acoustic features are 13-dimensional Perceptual Linear Predictive (PLP)
[32] coefﬁcients. The input to the network is 9 consecutive 40-dimensional speaker-adapted PLP
features after linear discriminative analysis (LDA) projection from adjacent frames.
SWB300 The 300-hour Switchboard dataset is another widely used dataset in speech recognition
[31]. The test set is the Hub5 2000 evaluation set composed of two parts: 2.1 hours of switchboard
(SWB) data from 40 speakers and 1.6 hours of call-home (CH) data from 40 different speakers.
Acoustic models are bi-directional long short-term memory (LSTM [33]) networks with 4 LSTM
layers. Each layer contains 1 024 cells with 512 in each direction. On top of the LSTM layers  there is
a linear bottleneck layer with 256 hidden units followed by a softmax output layer with 32 000 units.
The LSTMs are unrolled 21 frames. The input dimensionality is 140 which comprises 40-dimensional
speaker-adapted PLP features after LDA projection and 100-dimensional speaker embedding vectors
(i-vectors [34]).
The networks are optimized under the cross-entropy criterion. The single baseline is trained using
SGD with a batch size 128 without momentum for 20 epochs. The initial learning rate is 0.001 for
BN50 and 0.025 for SWB300. The learning rate is annealed by 2x every time the loss on the validation
set of the current epoch is worse than the previous epoch and meanwhile the model is backed off
to the previous epoch. The population sizes for both baseline and ESGD are 100. The offspring
population of ESGD consists of 400 individuals. In ESGD  after 15 generations (Ks = 1)  a 5-epoch
ﬁne-tuning is applied to each individual with a small learning rate. The details of experimental
conﬁguration are given in the supplementary material.
Table 1 shows the results of two baselines and ESGD on BN50 and SWB300  respectively. Both
the validation losses and word error rates (WERs) are presented for the best individual and the
top 15 individuals of the population. For the top 15 individuals  a range of losses and WERs are
presented. From the tables  it can be observed that the best individual of the population in ESGD
signiﬁcantly improves the losses and also improves the WERs over both the single baseline as well
as the population baseline. Note that the model with the best loss may not give the best WER in
some cases  although typically they correlate well. The ablation experiment shows that the interaction
between individuals in the evolution step of ESGD is helpful  and removing the evolution step hurts
the performance in both cases.

4.2

Image Recognition

The CIFAR10 [35] dataset is a widely used image recognition benchmark. It contains a 50K image
training-set and a 10K image test-set. Each image is a 32x32 3-channel color image. The model
used in this paper is a depth-20 ResNet model [36] with a 64x10 linear layer in the end. The ResNet
is trained under the cross-entropy criterion with batch-normalization. Note that CIFAR10 does not
include a validation set. To be consistent with the training set used in the literature  we do not split a
validation-set from the training-set. Instead  we evaluate training ﬁtness over the entire training-set.
For the single-run baseline  we follow the recipes proposed in [37]  in which the initial learning
rate is 0.1 and gets annealed by 10x after 81 epochs and then annealed by another 10x at epoch 122.
Training ﬁnishes in 160 epochs. The model is trained by SGD using Nesterov acceleration with a
momentum 0.9. The classiﬁcation error rate of the single baseline is 8.34%. In practice  we found
for this workload  ESGD works best when only considering SGD optimizer with randomized hyper-
parameters (e.g.  learning rate and momentum). We record the detailed experimental conﬁguration in
the supplementary material. The CIFAR10 results in Table 1 indicate that ESGD clearly outperforms
the two baselines in both training ﬁtness and classiﬁcation error rates.

4.3 Language Modeling

The evaluation of the ESGD algorithm is also carried out on the standard language modeling task
Penn Treebank (PTB) dateset [38]. Hyper-parameters have been massively optimized over the
previous years. The current state-of-the-art results are reported in [39] and [40]. Hence  our focus is
to investigate the effect of ESGD on top of a state-of-the-art 1-layer LSTM LM training recipe [40].

6

The results are summarized in Table 1. Starting from scratch  the single baseline model converged
after 574 epochs and achieved a perplexity of 67.3 and 64.6 on the validation and evaluation sets. The
population baseline models are initialized by cloning the single baseline and generating offsprings by
mutation. Then optimizers (SGD and ADAM) are randomly picked and models are trained for 300
epochs. Randomizing the optimizer includes additional hyper-parameters like the various dropout
ratios/models ([41  42  43  40  44  45])  batch size  etc. For comparison  the warm restart of the
single baseline gives 0.2 perplexity improvement on the test set [46]. Using ESGD  we also relax the
back-off to the initial model by probability of pbackoff = 0.7 in each generation. The single baseline
model is always added to each generation without any update  which guarantees that the population
can not perform worse than the single baseline. The detailed parameter settings are provided in the
supplementary material. ESGD without the evolutionary step clearly shows difﬁculties  and the best
model’s “gene” can not become prevalent in the successive generations according to the proportion
of its ﬁtness value. In summary  we observe small but consistent gain by ﬁne-tuning existing  highly
optimized model with ESGD.
Note that the above implementation for PTB experiments can be viewed as another variant of ESGD:
Suppose we have a well-trained model (e.g. a competitive baseline model) which is always inserted
into the population in each generation of evolution. The m-elitist strategy will guarantee that the best
model in the population is not worse than this well-trained model even if we relax the back-off in
SGD with probability.
In Fig.1 we show the ﬁtness as a function of ESGD generations in the four investigated tasks.

Table 1: Performance of single baseline  population baseline and ESGD on BN50  SWB300  CIFAR10
and PTB. For ESGD  the tables show the losses and classiﬁcation error rates of the best individual as
well as the top 15 individuals in the population for the ﬁrst three tasks. In PTB  the perplexities (ppl) 
which is the exponent of loss  of the validation set and test set are presented. The tables also present
the results of the ablation experiments where the evolution step is removed from ESGD.

BN50

single baseline
population baseline
ESGD w/o evolution
ESGD

loss
θ1:µ ↔ θ15:µ

–

[2.029  2.062]
[2.036  2.075]
[1.916  1.920]

WER
θ1:µ ↔ θ15:µ

–

[16.9  17.6]
[17.1  17.7]
[16.2  16.4]

θ1:µ
17.4
17.1
17.4
16.4

θ1:µ
2.082
2.029
2.036
1.916

SWB300

single baseline
population baseline
ESGD w/o evolution
ESGD

loss
θ1:µ ↔ θ15:µ

–

[1.645  1.666]
[1.626  1.641]
[1.551  1.557]

SWB WER

θ1:µ ↔ θ15:µ

–

[10.3  10.7]
[10.3  10.7]
[10.0  10.1]

θ1:µ
10.4
10.4
10.3
10.0

CH WER
θ1:µ ↔ θ15:µ

–

[18.2  18.8]
[18.0  18.6]
[18.0  18.3]

θ1:µ
18.5
18.2
18.3
18.2

θ1:µ
1.648
1.645
1.626
1.551

CIFAR10

single baseline
population baseline
ESGD w/o evolution
ESGD

θ1:µ
0.0176
0.0151
0.0147
0.0142

loss
θ1:µ ↔ θ15:µ

–

[0.0151  0.0164]
[0.0147  0.0166]
[0.0142  0.0159]

error rate
θ1:µ ↔ θ15:µ

–

[7.90  8.69]
[7.86  8.53]
[7.43  8.10]

θ1:µ
8.34
8.24
8.49
7.52

PTB

single baseline
population baseline
ESGD w/o evolution
ESGD

θ1:µ
67.27
66.58
67.27
66.29

validation ppl

θ1:µ ↔ θ15:µ

–

[66.58  68.04]
[67.27  79.25]
[66.29  66.30]

7

test ppl
θ1:µ ↔ θ15:µ

–

[63.96  64.58]
[64.58  76.64]
[63.72  63.74]

θ1:µ
64.58
63.96
64.58
63.73

Figure 1: Fitness as a function of ESGD generations for BN50  SWB300  CIFAR10 and PTB. The
three curves represent the single baseline (red)  top 15 individuals of population baseline (orange)
and ESGD (green). The latter two are illustrated as bands. The lower bounds of the ESGD curve
bands indicate the best ﬁtness values in the populations which are always non-increasing. Note that in
the PTB case  this monotonicity is violated since the back-off strategy was relaxed with probability 
which explains the increase of perplexity in some generations.

Figure 2: Percentage of offsprings selected in the 60% m-elitist over generations of ESGD in BN50 
SWB300  CIFAR10 and PTB.

4.4 Discussion

Population diversity
It is important to maintain a good population diversity in EA to avoid
premature convergence due to the homogeneous ﬁtness among individuals. In experiments  we ﬁnd
that the m-elitist strategy applied to the whole population  although has a better overall average
ﬁtness  can give rise to premature convergence in the early stage. Therefore  we set the percentage of
m-elitist to 60% of the population and the remaining 40% of the population is generated by random
selection. This m-elitist strategy is helpful in practice.
Population evolvement The SGD step of ESGD mimics the coevolution mechanism between com-
peting species (individuals under different optimizers) where distinct species evolute independently.
The evolution step of ESGD allows the species to interact with each other to hopefully produce
promising candidate solutions for the next generation. Fig.2 shows the percentage of offsprings
selected in the 60% m-elitist for the next generation. From the table  in the early stage the population

8

0369121518211.82.02.22.42.62.83.0BN50/DNN0369121518211.41.61.82.02.22.42.6SWB300/LSTMSinglePopulationESGD123456789100.050.100.150.200.250.300.350.400.45CIFAR10/ResNet-2002468101214164.204.214.224.234.244.254.26PTB/LSTM56780.010.020.030.040.05GenerationFitness0246810121416020406080100BN50/DNN0246810121416SWB300/LSTM0123456789CIFAR10/ResNet-200246810121416PTB/LSTMGeneration% offsprings selectedevolves dominantly based on SGD since the offsprings are worse than almost all the parents. How-
ever  in late generations the number of elite offsprings increases. The interaction between distinct
optimizers starts to play an important role in selecting better candidate solutions.
Complementary optimizers In each generation of ESGD  an individual selects an optimizer from a
pool of optimizers with certain hyper-parameters. In most of the experiments  the pool of optimizers
consists of SGD variants and ADAM. It is often observed that ADAM tends to be aggressive in the
early stage but plateaus quickly. SGD  however  starts slow but can get to better local optima. ESGD
can automatically choose optimizers and their appropriate hyper-parameters based on the ﬁtness value
during the evolution process so that the merits of both SGD and ADAM can be combined to seek
a better local optimal solution to the problem of interest. In the supplementary material examples
are given where we show the optimizers with their training hyper-parameters selected by the best
individuals in ESGD in each generation. It indicates that over generations different optimizers are
automatically chosen by ESGD giving rise to a better ﬁtness value.
Parallel computation In the experiments of this paper  all SGD updates and EA ﬁtness evaluations
are carried out in parallel using multiple GPUs. The SGD updates dominate the ESGD computation.
The EA updates and ﬁtness evaluations have a fairly small computational cost compared to the SGD
updates. Given sufﬁcient computing resource (e.g. µ GPUs)  ESGD should take about the same
amount of time as one end-to-end vanilla SGD run. Practically  trade-off has to be made between
the training time and performance under the constraint of computational budget. In general  parallel
computation is suitable and preferred for population-based optimization.

5 Conclusion

We have presented the population-based ESGD as an optimization framework to combine SGD and
gradient-free evolutionary algorithm to explore their complementarity. ESGD alternately optimizes
the m-elitist average ﬁtness of the population between an SGD step and an evolution step. The SGD
step can be interpreted as a coevolution mechanism where individuals under distinct optimizers evolve
independently and then interact with each other in the evolution step to hopefully create promising
candidate solutions for the next generation. With an appropriate decision strategy  the ﬁtness of the
best individual in the population is guaranteed to be non-degrading. Extensive experiments have
been carried out in three applications using various neural networks with deep architectures. The
experimental results have demonstrated the effectiveness of ESGD.

References
[1] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning.

arXiv preprint arXiv:1606.04838  2016.

[2] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[3] D. P. Kingma and J. L. Ba. ADAM: a method for stochastic optimization. In International

Conference on Learning Representations (ICLR)  2015.

[4] T. Tieleman and G. Hinton. Lecture 6.e. RMSProp: divide the gradient by a running average of

its recent magnitude. COURSERA: Neural Networks for Machine Learning  2012.

[5] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of

convergence o(1/k2). Soviet Math Docl  269:543–547  1983.

[6] H.-G. Beyer and H.-P. Schwefel. Evolution strategies: a comprehensive introduction. Natural

computing  1(1):3–52  2002.

[7] D. E. Goldberg. Genetic algorithm in search  optimization and machine learning. Addison-

Wesley Publishing Co.  1989.

[8] C. Igel. Neuroevolution for reinforcement learning using evolution strategies. In IEEE Congress

of Evolutionary Computation (CEC)  page 2588–2595  2003.

[9] N. Hansen. The CMA evolution strategy: a tutorial. arXiv preprint arXiv:1604.00772  2016.

9

[10] I. Loshchilov. LM-CMA: an alternative to L-BFGS for large scale black-box optimization.

Evolutionary Computation  25(1):143–171  2017.

[11] E. Real  S. Moore  A. Selle  S. Saxena  Y. L. Suematsu  J. Tan  Q. V. Le  and A. Kurakin.
Large-scale evolution of image classiﬁers. In International Conference on Machine Learning
(ICML)  pages 2902–2911  2017.

[12] E. Real  A. Aggarwal  Y. Huang  and Q. V. Le. Regularized evolution for image classiﬁer

architecture search. arXiv preprint arXiv:1802.01548  2018.

[13] J. Liang  E. Meyerson  and R. Miikkulainen. Evolutionary architecture search for deep multitask

networks. arXiv preprint arXiv:1803.03745  2018.

[14] S. Ebrahimi  A. Rohrbach  and T. Darrell. Gradient-free policy architecture search and adapta-

tion. In Conference on Robot Learning (CoRL)  2017.

[15] R. Miikkulainen  J. Liang  E. Meyerson  A. Rawal  D. Fink  O. Francon  B. Raju  H. Shahrzad 
A. Navruzyan  N. Nuffy  and B. Hodjat. Evolving deep neural networks. arXiv preprint
arXiv:1703.00548  2017.

[16] I. Loshchilov and F. Hutter. CMA-ES for hyperparameter optimization of deep neural networks.

In International Conference on Learning Representations (ICLR)  workshop track  2016.

[17] M. Jaderberg  V. Dalibard  S. Osindero  W. M. Czarnecki  J. Donahue  A. Razavi  O. Vinyals 
T. Green  I. Dunning  K. Simonyan  C. Fernando  and K. Kavukcuoglu. Population based
training of neural networks. arXiv preprint arXiv:1711.09846  2017.

[18] G. Morse and K. O. Stanley. Simple evolutionary optimization can rival stochastic gradient de-
scent in neural networks. In The Genetic and Evolutionary Computation Conference (GECCO) 
pages 477–484  2016.

[19] Z. Yang  K. Tang  and X. Yao. Large scale evolutionary optimization using cooperative

coevolution. Information Sciences  178(15):2985–2999  2008.

[20] N. Garcia-Pedrajas  C. Hervas-Martinez  and J. Munoz-Perez. COVNET: a cooperative co-
evolutionary model for evolving artiﬁcial neural networks. IEEE Trans. on Neural Networks 
14(3):575–595  2003.

[21] X. Yao. Evolving artiﬁcial neural networks. Proceedings of the IEEE  87(9):1423–1447  1999.

[22] F. P. Such  V. Madhavan  E. Conti  J. Lehman  K. O. Stanley  and J. Clune. Deep neuroevo-
lution: genetic algorithms are a competitive alternative for training deep neural networks for
reinforcement learning. arXiv preprint arXiv:1712.06567  2017.

[23] M. Suganuma  S. Shirakawa  and T. Nagao. A genetic programming approach to designing
convolutional neural network architectures. In The Genetic and Evolutionary Computation
Conference (GECCO)  pages 497–504  2017.

[24] C. Liu  B. Zoph  J. Shlens  W. Hua  L.-J. Li  F.-F. Li  A. Yuille  J. Huang  and K. Murphy.

Progressive neural architecture search. arXiv preprint arXiv:1712.00559  2017.

[25] P. Chrabaszcz  I. Loshchilov  and F. Hutter. Back to basics: benchmarking canonical evolution

strategies for playing Atari. arXiv preprint arXiv:1802.08842  2018.

[26] T. Salimans  J. Ho  X. Chen  S. Sidor  and I. Sutskever. Evolution strategies as a scalable

alternative to reinforcement learning. arXiv preprint arXiv:1703.03864  2017.

[27] X. Zhang  J. Clune  and K. O. Stanley. On the relationship between the OpenAI evolution

strategy and stochastic gradient descent. arXiv preprint arXiv:1712.06564  2017.

[28] J. Lehman  J. Chen  J. Clune  and K. O. Stanley. ES is more than just a traditional ﬁnite-

difference approximator. arXiv preprint arXiv:1712.06568  2017.

[29] F. Gomez  J. Schmidhuber  and R. Miikkulainen. Accelerated neural evolution through coopera-

tively coevolved synapses. Journal of Machine Learning Research  9:937–965  2008.

10

[30] N. Garcia-Pedrajas  C. Hervas-Martinez  and D. Ortiz-Boyer. Cooperative coevolution of
IEEE Trans. on Evolutionary

artiﬁcial neural network ensembles for pattern recognition.
Computation  9(3):271–302  2005.

[31] G. Hinton  L. Deng  D. Yu  G. Dahl  A. Mohamed  N. Jaitly  A. Senior  V. Vanhoucke  P. Nguyen 
T. N. Sainath  and B. Kingsbury. Deep neural networks for acoustic modeling in speech
recognition. IEEE Signal Processing Maganize  pages 82–97  November 2012.

[32] H. Hermansky. Perceptual linear predictive (PLP) analysis of speech. Journal of Acoustical

Society America  87(4):1738–1752  1990.

[33] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):1735–

1780  1997.

[34] N. Dehak  P. Kenny  R. Dehak  P. Dumouchel  and P. Ouellet. Front-end factor analysis
for speaker veriﬁcation. IEEE Transactions on Audio  Speech  and Language Processing  
19(4):788–798  2011.

[35] A. Krizhevsky. Learning multiple layers of features from tiny images. In Technical Report 

2009.

[36] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. Conference

on Computer Vision and Pattern Recognition (CVPR’15)  2015.

[37] https://github.com/facebook/fb.resnet.torch.

[38] T. Mikolov  M. Karaﬁát  L. Burget  J. Cernocky  and S. Khudanpur. Recurrent neural network

based language model. In Interspeech  pages 1045–1048  2010.

[39] S. Merity  N. Keskar  and R. Socher. Regularizing and optimizing LSTM language models. In

International Conference on Learning Representations (ICLR)  2018.

[40] K. Zolna  D. Arpit  D. Suhubdy  and Y. Bengio. Fraternal dropout. In International Conference

on Learning Representations (ICLR)  2018.

[41] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research 
15:1929–1958  2014.

[42] L. Wan  M. Zeiler  S. Zhang  Y. LeCun  and R. Fergus. Regularization of neural networks using
dropconnect. In International Conference on Machine Learning (ICML)  volume 28  pages
1058–1066  2013.

[43] Y. Gal and Z. Ghahramani. A theoretically grounded application of dropout in recurrent neural
networks. In International Conference on Neural Information Processing Systems (NIPS) 
NIPS’16  pages 1027–1035  2016.

[44] X. Ma  Y. Gao  Z. Hu  Y. Yu  Y. Deng  and E. H. Hovy. Dropout with expectation-linear

regularization. In International Conference on Learning Representations (ICLR)  2017.

[45] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International

Conference on Learning Representations (ICLR)  2017.

[46] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts.

International Conference on Learning Representations (ICLR)  2017.

In

11

,Xiaodong Cui
Wei Zhang
Zoltán Tüske
Michael Picheny