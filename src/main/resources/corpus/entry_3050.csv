2016,Learning brain regions via large-scale online structured sparse dictionary learning,We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an $\ell_1$-norm constraint. By "structured"  we mean that the atoms are piece-wise smooth and compact  thus making up blobs  as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties  we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning  work of Mairal et al. (2010)  at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method  the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium  and large-scale regimes alike  compared to state-of-the-art models.,Learning brain regions via large-scale online

structured sparse dictionary-learning

Elvis Dohmatob  Arthur Mensch  Gael Varoquaux  Bertrand Thirion

Parietal Team  INRIA / CEA  Neurospin  Université Paris-Saclay  France

firstname.lastname@inria.fr

Abstract

We propose a multivariate online dictionary-learning method for obtaining de-
compositions of brain images with structured and sparse components (aka atoms).
Sparsity is to be understood in the usual sense: the dictionary atoms are constrained
to contain mostly zeros. This is imposed via an (cid:96)1-norm constraint. By "struc-
tured"  we mean that the atoms are piece-wise smooth and compact  thus making up
blobs  as opposed to scattered patterns of activation. We propose to use a Sobolev
(Laplacian) penalty to impose this type of structure. Combining the two penalties 
we obtain decompositions that properly delineate brain structures from functional
images. This non-trivially extends the online dictionary-learning work of Mairal et
al. (2010)  at the price of only a factor of 2 or 3 on the overall running time. Just
like the Mairal et al. (2010) reference method  the online nature of our proposed
algorithm allows it to scale to arbitrarily sized datasets. Preliminary xperiments
on brain data show that our proposed method extracts structured and denoised
dictionaries that are more intepretable and better capture inter-subject variability in
small medium  and large-scale regimes alike  compared to state-of-the-art models.

1

Introduction

In neuro-imaging  inter-subject variability is often handled as a statistical residual and discarded. Yet
there is evidence that it displays structure and contains important information. Univariate models are
ineﬀective both computationally and statistically due to the large number of voxels compared to the
number of subjects. Likewise  statistical analysis of weak eﬀects on medical images often relies on
deﬁning regions of interests (ROIs). For instance  pharmacology with Positron Emission Tomography
(PET) often studies metabolic processes in speciﬁc organ sub-parts that are deﬁned from anatomy.
Population-level tests of tissue properties  such as diﬀusion  or simply their density  are performed on
ROIs adapted to the spatial impact of the pathology of interest. Also  in functional brain imaging 
e.g function magnetic resonance imaging (fMRI)  ROIs must be adapted to the cognitive process
under study  and are often deﬁned by the very activation elicited by a closely related process [18].
ROIs can boost statistical power by reducing multiple comparisons that plague image-based statistical
testing. If they are deﬁned to match spatially the diﬀerences to detect  they can also improve the
signal-to-noise ratio by averaging related signals. However  the crux of the problem is how to deﬁne
these ROIs in a principled way. Indeed  standard approaches to region deﬁnition imply a segmentation
step. Segmenting structures in individual statistical maps  as in fMRI  typically yields meaningful
units  but is limited by the noise inherent to these maps. Relying on a diﬀerent imaging modality hits
cross-modality correspondence problems.

Sketch of our contributions.
In this manuscript  we propose to use the variability of the statistical
maps across the population to deﬁne regions. This idea is reminiscent of clustering approaches  that
have been employed to deﬁne spatial units for quantitative analysis of information as diverse as brain
ﬁber tracking  brain activity  brain structure  or even imaging-genetics. See [21  14] and references
therein. The key idea is to group together features –voxels of an image  vertices on a mesh  ﬁber tracts–
30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

based on the quantity of interest  to create regions –or ﬁber bundles– for statistical analysis. However 
unlike clustering that models each observation as an instance of a cluster  we use a model closer to
the signal  where each observation is a linear mixture of several signals. The model is closer to mode
ﬁnding  as in a principal component analysis (PCA)  or an independent component analysis (ICA) 
often used in brain imaging to extract functional units [5]. Yet  an important constraint is that the
modes should be sparse and spatially-localized. For this purpose  the problem can be reformulated as
a linear decomposition problem like ICA/PCA  with appropriate spatial and sparse penalties [25  1].
We propose a multivariate online dictionary-learning method for obtaining decompositions with
structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the
atoms contain mostly zeros. This is imposed via an (cid:96)1 penalty on the atoms. By "structured"  we mean
that the atoms are piece-wise smooth and compact  thus making up blobs  as opposed to scattered
patterns of activation. We impose this type of structure via a Laplacian penalty on the dictionary atoms.
Combining the two penalties  we therefore obtain decompositions that are closer to known functional
organization of the brain. This non-trivially extends the online dictionary-learning work [16]  with
only a factor of 2 or 3 on the running time. By means of experiments on a large public dataset  we
show the improvements brought by the spatial regularization with respect to traditional (cid:96)1-regularized
dictionary learning. We also provide a concise study of the impact of hyper-parameter selection on
this problem and describe the optimality regime  based on relevant criteria (reproducibility  captured
variability  explanatory power in prediction problems).

2 Smooth Sparse Online Dictionary-Learning (Smooth-SODL)
Consider a stack X ∈ Rn×p of n subject-level brain images X1  X2  . . .   Xn each of shape n1 ×
n2 × n3  seen as p-dimensional row vectors –with p = n1 × n2 × n3  the number of voxels. These
could be images of fMRI activity patterns like statistical parametric maps of brain activation  raw
pre-registered (into a common coordinate space) fMRI time-series  PET images  etc. We would
like to decompose these images as a mixture of k ≤ min(n  p) component maps (aka latent factors
or dictionary atoms) V1  . . .   Vk ∈ Rp×1 and modulation coeﬃcients U1  . . .   Un ∈ Rk×1 called
codes (one k-dimensional code per sample point)  i.e
(1)
where V := [V1| . . .|Vk] ∈ Rp×k  an unknown dictionary to be estimated. Typically  p ∼ 105 –
106 (in full-brain high-resolution fMRI) and n ∼ 102 – 105 (for example  in considering all the 500
subjects and all the about functional tasks of the Human Connectome Project dataset [20]). Our
work handles the extreme case where both n and p are large (massive-data setting). It is reasonable
then to only consider under-complete dictionaries: k ≤ min(n  p). Typically  we use k ∼ 50 or 100
components. It should be noted that online optimization is not only crucial in the case where n/p is
big; it is relevant whenever n is large  leading to prohibitive memory issues irrespective of how big or
small p is.
As explained in section 1  we want the component maps (aka dictionary atoms) Vj to be sparse and
spatially smooth. A principled way to achieve such a goal is to impose a boundedness constraint on
(cid:96)1-like norms of these maps to achieve sparsity and simultaneously impose smoothness by penalizing
their Laplacian. Thus  we propose the following penalized dictionary-learning model

Xi ≈ VUi  for i = 1  2  . . .   n

1
2(cid:107)Xi − VUi(cid:107)2

2 +

1
2

α(cid:107)Ui(cid:107)2

2

+ γ

ΩLap(Vj).

(2)

(cid:33)

k(cid:88)

j=1

(cid:32)

n(cid:88)

i=1

1
n

min

lim
n→∞

min
V∈Rp×k
Ui∈Rk
subject to V1  . . .   Vk ∈ C

The ingredients in the model can be broken down as follows:

1
2(cid:107)Xi − VUi(cid:107)2

2 measures how well the current dictionary V
• Each of the terms maxUi∈Rk
2 on the codes
explains data Xi from subject i. The Ridge penalty term φ(Ui) ≡ 1
amounts to assuming that the energy of the decomposition is spread across the diﬀerent
samples. In the context of a speciﬁc neuro-imaging problem  if there are good grounds to
assume that each sample / subject should be sparsely encoded across only a few atoms of
the dictionary  then we can use the (cid:96)1 penalty φ(Ui) := α(cid:107)Ui(cid:107)1 as in [16]. We note that in

2 α(cid:107)Ui(cid:107)2

2

contrast to the (cid:96)1 penalty  the Ridge leads to stable codes. The parameter α > 0 controls the
amount of penalization on the codes.
• The constraint set C is a sparsity-inducing compact simple (mainly in the sense that the
Euclidean projection onto C should be easy to comput) convex subset of Rp like an (cid:96)1-ball
Bp (cid:96)1(τ ) or a simplex Sp(τ )  deﬁned respectively as
(3)
Other choices (e.g ElasticNet ball) are of course possible. The radius parameter τ > 0
controls the amount of sparsity: smaller values lead to sparser atoms.
• Finally  ΩLap is the 3D Laplacian regularization functional deﬁned by

Bp (cid:96)1(τ ) := {v ∈ Rp s.t |v1| + . . . + |vp| ≤ τ}   and Sp(τ ) := Bp (cid:96)1(τ ) ∩ Rp
+.

p(cid:88)

k=1

ΩLap(v) :=

1
2

(∇xv)2

k + (∇yv)2

k + (∇zv)2

k =

1
2

vT ∆v ≥ 0  ∀v ∈ Rp 

(4)

∇x being the discrete spatial gradient operator along the x-axis (a p-by-p matrix)  ∇y along
the y-axis  etc.  and ∆ := ∇T∇ is the p-by-p matrix representing the discrete Laplacian
operator. This penalty is meant to impose blobs. The regularization parameter γ ≥ 0 controls
how much regularization we impose on the atoms  compared to the reconstruction error.

The above formulation  which we dub Smooth Sparse Online Dictionary-Learning (Smooth-SODL)
is inspired by  and generalizes the standard online dictionary-learning framework of [16] –henceforth
referred to as Sparse Online Dictionary-Learning (SODL)– with corresponds to the special case
γ = 0.

3 Estimating the model
3.1 Algorithms
The objective function in problem (2) is separately convex and block-separable w.r.t each of U and V
but is not jointly convex in (U  V). Also  it is continuously diﬀerentiable on the constraint set  which
is compact and convex. Thus by classical results (e.g Bertsekas [6])  the problem can be solved via
Block-Coordinate Descent (BCD) [16]. Reasoning along the lines of [15]  we derive that the BCD
iterates are as given in Alg. 1 in which  for each incoming sample point Xt  the loading vector Ut is
computing by solving a ridge regression problem (5) with the current dictionary Vt held ﬁxed  and
the dictionary atoms are then updated sequentially via Alg. 2. A crucial advantage of using a BCD
scheme is that it is parameter free: there is not step size to tune. The resulting algorithm Alg. 1  is
adapted from [16]. It relies on Alg. 2 for performing the structured dictionary updates  the details of
which are discussed below.
Algorithm 1 Online algorithm for the dictionary-learning problem (2)
Require: Regularization parameters α  γ > 0; initial dictionary V ∈ Rp×k  number of passes /
1: A0 ← 0 ∈ Rk×k  B0 ← 0 ∈ Rp×k (historical “suﬃcient statistics”)
2: for t = 1 to T do
3:
4:

Empirically draw a sample point Xt at random.
Code update: Ridge-regression (via SVD of current dictionary V)

iterations T on the data.

Ut ← argminu∈Rk
Rank-1 updates: At ← At−1 + UtUT
BCD dictionary update: Compute update for dictionary V using Alg. 2.

t   Bt ← Bt−1 + XtUT

2 +

α(cid:107)u(cid:107)2
2.

t

1
2(cid:107)Xt − Vu(cid:107)2

1
2

5:
6:
7: end for

Update of the codes: Ridge-coding. The Ridge sub-problem for updating the codes

Ut = (VT V + αI)−1VT Xt

3

(5)

(6)

(cid:80)t

i=1 XiUT

i=1 UiUT

Vj = argminv∈C V=[V1|...|v|...|Vk]

i ∈ Rk×k and Bt :=(cid:80)t

is computed via an SVD of the current dictionary V. For α ≈ 0  Ut reduces to the orthogonal
projection of Xt onto the image of the current dictionary V. As in [16]  we speed up the overall
algorithm by sampling mini-batches of η samples Xt  . . .   Xη and compute the corresponding codes
U1  U2  ...  Uη at once. We typically use we use mini-batches of size η = 20.
BCD dictionary update for the dictionary atoms. Let us deﬁne time-varying matrices At :=
i ∈ Rp×k  where t = 1  2  . . . denotes time. We ﬁx
(cid:32) t(cid:88)
the matrix of codes U  and for each j  consider the update of the jth dictionary atom  with all the
other atoms Vk(cid:54)=j kept ﬁxed. The update for the atom Vj can then be written as
(cid:124)
where F˜γ(v  a) ≡ 1
2(cid:107)v − a(cid:107)2
Algorithm 2 BCD dictionary update with Laplacian prior
Require: V = [V1| . . .|Vk] ∈ Rp×k (input dictionary) 
1: A = [A1| . . .|Ak] ∈ Rk×k  Bt = [B1
2: while stopping criteria not met  do
3:
4:

1
2(cid:107)Xi − VUi(cid:107)2
(cid:123)(cid:122)
= argminv∈C Fγ(At[j j]/t)−1(v  Vj + At[j  j]−1(Bj

Fix the code U and all atoms k (cid:54)= j of the dictionary V and then update Vj as follows
Vj ← argminv∈C Fγ(At[j j]/t)−1(v  Vj + At[j  j]−1(Bj
(8)
(See below for details on the derivation and the resolution of this problem)

t − VAj))

refer to [16] for the details
2 ˜γvT ∆v.

2 + 1

t| . . .|Bk

t ] ∈ Rp×k (history)

for j = 1 to r do

t − VAj
t )

) 

2(cid:107)v − a(cid:107)2

2 + ˜γΩLap(v) = 1

+ γtΩLap(v)

2

(cid:33)

(cid:125)

(7)

i=1

end for
5:
6: end while

Problem (7) is the compactly-constrained minimization of the 1-strongly-convex quadratic functions
F˜γ(.  a) : Rp → R deﬁned above. This problem can further be identiﬁed with a denoising instance
(i.e in which the design matrix / deconvolution operator is the identity operator) of the GraphNet
model [11  13]. Fast ﬁrst-order methods like FISTA [4] with optimal rates O(L/√) are available1
for solving such problems to arbitrary precision  > 0. One computes the Lipschitz constant to be
LF˜γ (. a) ≡ 1 + ˜γLΩLap = 1 + 4D˜γ  where as before  D is the number of spatial dimensions (D = 3
for volumic images). One should also mention that under certain circumstances  it is possible to
perform the dictionary updates in the Fourier domain  via FFT. This alternative approach is detailed
in the supplementary materials.
Finally  one notes that  since constraints in problem (2) are separable in the dictionary atoms Vj 
the BCD dictionary-update algorithm Alg. 2 is guaranteed to converge to a global optimum  at each
iteration [6  16].

How diﬃcult is the dictionary update for our proposed model ? A favorable property of the
vanilla dictionary-learning [16] is that the BCD dictionary updates amount to Euclidean projections
onto the constraint set C  which can be easily computed for a variety of choices (simplexes  closed
convex balls  etc.). One may then ask: do we retain a comparable algorithmic simplicity even with the
additional Laplacian terms ΩLap(Vj) ? YES!: empirically  we found that 1 or 2 iterations of FISTA
[4] are suﬃcient to reach an accuracy of 10−6 in problem (7)  which is suﬃcient to obtain a good
decomposition in the overall algorithm.
However  choosing γ “too large” will provably cause the dictionary updates to eventually take forever
to run. Indeed  the Lipschitz constant in problem (7) is Lt = 1 + 4Dγ(At[j  j]/t)−1  which will
blow-up (leading to arbitrarily small step-sizes) unless γ is chosen so that

(cid:18)

(cid:19)

(cid:32)

γ = γt = O

max
1≤j≤k

At[j  j]

= O

max
1≤j≤k

(cid:107)Uj(cid:107)2
2/t

= O((cid:107)At(cid:107)∞ ∞/t).

(9)

(cid:33)

t(cid:88)

i=1

1For example  see [8  24]  implemented as part of the Nilearn open-source library Python library [2].

4

Finally  the Euclidean projections onto the (cid:96)1 ball C can be computed exactly in linear-time O(p) (see
for example [7  9]). The dictionary atoms j are repeatedly cycled and problem (7) solved. All in all 
in practice we observe that a single iteration is suﬃcient for the dictionary update sub-routine in Alg.
2 to converge to a qualitatively good dictionary.

Convergence of the overall algorithm. The Convergence of our algorithm (to a local optimum) is
guaranteed since all hypotheses of [16] are satisﬁed. For example  assumption (A) is satisﬁed because
fMRI data are naturally compactly supported. Assumption (C) is satisﬁed since the ridge-regression
problem (5) has a unique solution. More details are provided in the supplementary materials.

3.2 Practical considerations
Hyper-parameter
tuning. Parameter-
selection in dictionary-learning is known to
be a diﬃcult unsolved problem [16  15]  and
our proposed model (2) is not an exception
to this rule. We did an extensive study of the
quality of estimated dictionary varies with the
model hyper-parameters (α  γ  τ ). The data
experimental setup is described in Section 5.
The results are presented in Fig. 1. We make
the following observations: Taking the sparsity
parameter τ in (2) too large leads to dense
atoms that perfectly explain the data but are not
very intepretable. Taking it too small leads to
overly sparse maps that barely explain the data.
This normalized sparsity metric (small is better 
ceteris paribus) is deﬁned as the mean ratio
(cid:107)Vj(cid:107)1/(cid:107)Vj(cid:107)2 over the dictionary atoms.
Concerning the α parameter  inspired by [26]  we have found the following time-varying data-adaptive
choice for the α parameter to work very well in practice:
α = αt ∼ t−1/2.

Figure 1: Inﬂuence of model parameters. In the
experiments  α was chosen according to (10). Left:
Percentage explained variance of the decomposi-
tion  measured on left-out data split. Right: Aver-
age normalized sparsity of the dictionary atoms.

(10)

Likewise  care must be taken in selecting the Laplacian regularization parameter γ. Indeed taking it
too small amounts to doing vanilla dictionary-learning model [16]. Taking it too large can lead to
degenerate maps  as the spatial regularization then dominates the reconstruction error (data ﬁdelity)
term. We ﬁnd that there is a safe range of the parameter pair (γ  τ ) in which a good compromise
between the sparsity of the dictionary (thus its intepretability) and its explanation power of the data
can be reached. See Fig. 1. K-fold cross-validation with explained variance metric was retained as a
good strategy for setting the Laplacian regularization γ parameter and the sparsity parameter τ.

Initialization of the dictionary. Problem (2) is non-convex jointly in (U  V)  and so initialization
might be a crucial issue. However  in our experiments  we have observed that even randomly initialized
dictionaries eventually produce sensible results that do not jitter much across diﬀerent runs of the
same experiment.

4 Related works

While there exist algorithms for online sparse dictionary-learning that are very eﬃcient in large-scale
settings (for example [16]  or more recently [17]) imposing spatial structure introduces couplings
in the corresponding optimization problem [8]. So far  spatially-structured decompositions have
been solved by very slow alternated optimization [25  1]. Notably  structured priors such as TV-(cid:96)1
[3] minimization  were used by [1] to extract data-driven state-of-the-art atlases of brain function.
However  alternated minimization is very slow  and large-scale medical imaging has shifted to online
solvers for dictionary-learning like [16] and [17]. These do not readily integrate structured penalties.
As a result  the use of structured decompositions has been limited so far  by the computational cost of
the resulting algorithms. Our approach instead uses a Laplacian penalty to impose spatial structure at

5

0102103104105106107108γ2−32−22−12021222324τ6%12%18%24%30%36%42%48%54%explainedvariance0102103104105106107108γ020406080100120140normalizedsparsitya very minor cost and adapts the online-learning dictionary-learning framework [16]  resulting in a
fast and scalable structured decomposition. Second  the approach in [1] though very novel  is mostly
heuristic. In contrast  our method enjoys the same convergence guarantees and comparable numerical
complexity as the basic unstructured online dictionary-learning [16].
Finally  one should also mention [23] that introduced an online group-level functional brain mapping
strategy for diﬀerentiating regions reﬂecting the variety of brain network conﬁgurations observed in a
the population  by learning a sparse-representation of these in the spirit of [16].

5 Experiments
Setup. Our experiments were done on task fMRI data from 500 subjects from the HCP –Human
Connectome Project– dataset [20]. These task fMRI data were acquired in an attempt to assess
major domains that are thought to sample the diversity of neural systems of interest in functional
connectomics. We studied the activation maps related to a task that involves language (story under-
standing) and mathematics (mental computation). This particular task is expected to outline number 
attentional and language networks  but the variability modes observed in the population cover even
wider cognitive systems. For the experiments  mass-univariate General Linear Models (GLMs) [10]
for n = 500 subjects were estimated for the Math vs Story contrast (language protocol)  and the
corresponding full-brain Z-score maps each containing p = 2.6 × 105 voxels  were used as the input
data X ∈ Rn×p  and we sought a decomposition into a dictionary of k = 40 atoms (components).
The input data X were shuﬄed and then split into two groups of the same size.
Models compared and metrics. We compared our proposed Smooth-SODL model (2) against
both the Canonical ICA –CanICA [22]  a single-batch multi-subject PCA/ICA-based method  and
the standard SODL (sparse online dictionary-learning) [16]. While the CanICA model accounts for
subject-to-subject diﬀerences  one of its major limitations is that it does not model spatial variability
across subjects. Thus we estimated the CanICA components on smoothed data: isotropic FWHM of
6mm  a necessary preprocessing step for such methods. In contrast  we did not perform pre-smoothing
for the SODL of Smooth-SODL models. The diﬀerent models were compared across a variety of
qualitative and quantitative metrics: visual quality of the dictionaries obtained  explained variance 
stability of the dictionary atoms  their reproducibility  performance of the dictionaries in predicting
behavioral scores (IQ  picture vocabulary  reading proﬁciency  etc.) shipped with the HCP data [20].
For both SODL [16] and our proposed Smooth-SODL model  the constraint set for the dictionary
atoms was taken to be a simplex C := Sp(τ ) (see section 2 for deﬁnition). The results of these
experiments are presented in Fig. 2 and Tab. 1.

6 Results
Running time. On the computational side  the vanilla dictionary-learning SODL algorithm [16]
with a batch size of η = 20 took about 110s (≈ 1.7 minutes) to run  whilst with the same batch size 
our proposed Smooth-SODL model (2) implemented in Alg. 1 took 340s (≈ 5.6 minutes)  which
is slightly less than 3 times slower than SODL. Finally  CanICA [22] for this experiment took 530s
(≈ 8.8 minutes) to run  which is about 5 times slower than the SODL model and 1.6 times slower
than our proposed Smooth-SODL (2) model. All experiments were run on a single CPU of laptop.

Qualitative assessment of dictionaries. As can be seen in Fig. 2(a)  all methods recover dictionary
atoms that represent known functional brain organization; notably the dictionaries all contain the
well-known executive control and attention networks  at least in part. Vanilla dictionary-learning
leverages the denoising properties of the (cid:96)1 sparsity constraint  but the voxel clusters are not very
structured. For  example most blobs are surrounded with a thick ring of very small nonzero values. In
contrast  our proposed regularization model leverages both sparse and structured dictionary atoms 
that are more spatially structured and less noisy.
In contrast to both SODL and Smooth-SODL  CanICA [22] is an ICA-based method that enforces no
notion of sparsity whatsoever. The result are therefore dense and noisy dictionary atoms that explain
the data very well (Fig. 2(b) but which are completely unintepretable. In a futile attempt to remedy
the situation  in practice such PCA/ICA-based methods (including FSL’s MELODIC tool [19]) are
hard-thresholded in order to see information. For CanICA  the hard-thresholded version has been

6

(a) Qualitative comparison of the estimated dictionaries. Each column represents an atom of the estimated
dictionary  where atoms from the diﬀerent models (the rows of the plots) have been matched via a Hungarian
algorithm. Here  we only show a limited number of the most “intepretable” atoms. Notice how the major
structures in each atom are reproducible across the diﬀerent models. Maps corresponding to hard-thresholded
CanICA [22] components have also been included  and have been called tCanICA. In contrast  the maps from the
SODL [16] and our proposed Smooth-SODL (2) have not been thresholded.

(b) Mean explained variance of the
diﬀerent models on both training data
and test (left-out) data. N.B.: Bold
bars represent performance on test
set while faint bars in the background
represent performance on train set.
Figure 2: Main results. Benchmarking our proposed Smooth-SODL (2) model against competing
state-of-the-art methods like SODL (sparse online dictionary-learning) [16] and CanICA [22].

(c) Predicting behavioral variables of the HCP [20] dataset using
subject-level Z-maps. N.B.: Bold bars represent performance on
test set while faint bars in the background represent performance
on train set.

named tCanICA in Fig. 2. That notwithstanding  notice how the major structures (parietal lobes 
sulci  etc.) in each atom are reproducible across the diﬀerent models.

Stability-ﬁdelity trade-oﬀs. PCA/ICA-based methods like CanICA [22] and MELODIC [19] are
the optimal linear decomposition method to maximize explained variance on a dataset. On the training
set  CanICA [22] out-performs all others algorithms with about 66% (resp. 50% for SODL [16]
and 58% for Smooth-SODL) of explained variance on the training set  and 60% (resp. 49% for
SODL and 55% for Smooth-SODL) on left-out (test) data. See Fig. 2(b). However  as noted in the
above paragraph  such methods lead to dictionaries that are hardly intepretable and thus the user
must recourse to some kind of post-processing hard-thresholding step  which destroys the estimated
model. More so  assessing the stability of the dictionaries  measured by mean correlation between
corresponding atoms  across diﬀerent splits of the data  CanICA [22] scores a meager 0.1  whilst the
hard-thresholded version tCanICA obtains 0.2  compared to 0.4 for Smooth-SODL and 0.1 for SODL.

7

0%10%20%30%40%50%60%70%80%90%explainedvarianceSmooth-SODL(γ=104)Smooth-SODL(γ=103)SODLtCanICACanICAPCAPicturevocab.EnglishreadingPenn.MatrixTestStrengthEndurancePictureseq.mem.Dexterity0.00.10.20.30.4R2-scoreSmooth-SODL(γ=104)Smooth-SODL(γ=103)SODLtCanICACanICAPCARAWIs spatial regularization really needed ? As rightly pointed out by one of the reviewers  one does
not need spatial regularization if data are abundant (like in the HCP). So we computed learning curves
of mean explained variance (EV) on test data  as a function of the amount training data seen by
both Smooth-SODL and SODL [16] (Table 1). In the beginning of the curve  our proposed spatially
regularized Smooth-SODL model starts oﬀ with more than 31% explained variance (computed on
241 subjects)  after having pooled only 17 subjects. In contrast  the vanilla SODL model [16] scores
a meager 2% explained variance; this corresponds to a 14-fold gain of Smooth-SODL over SODL. As
more and more data are pooled  both models explain more variance  the gap between Smooth-SODL
and SODL reduces  and both models perform comparably asymptotically.

Nb. subjects pooled mean EV for vanilla SODL Smooth-SODL (2)

17
92
167
241

2%
37%
47%
49%

31%
50%
54%
55%

gain factor

13.8
1.35
1.15
1.11

Table 1: Learning-curve for boost in explained variance of our proposed Smooth-SODL model over
the reference SODL model. Note the reduction in the explained variance gain as more data are pooled.
Thus our proposed Smooth-SODL method extracts structured denoised dictionaries that better capture
inter-subject variability in small  medium  and large-scale regimes alike.

Prediction of behavioral variables.
If Smooth-SODL captures the patterns of inter-subject variabil-
ity  then it should be possible to predict cognitive scores y like picture vocabulary  reading proﬁciency 
math aptitude  etc. (the behavioral variables are explained in the HCP wiki [12]) by projecting new
subjects’ data into this learned low-dimensional space (via solving the ridge problem (5) for each
sample Xt)  without loss of performance compared with using the raw Z-values values X. Let RAW
refer to the direct prediction of targets y from X  using the top 2000 most voxels most correlated with
the target variable. Results of for the comparison are shown in Fig. 2(c). Only variables predicted
with a a positive mean (across the diﬀerent methods and across subjects) R-score are reported. We
see that the RAW model  as expected over-ﬁts drastically  scoring an R2 of 0.3 on training data and
only 0.14 on test data. Overall  for this metric CanICA performs best than all the other models in
predicting the diﬀerent behavioral variables on test data. However  our proposed Smooth-SODL
model outperforms both SODL [16] and tCanICA  the thresholded version of CanICA.

7 Concluding remarks

To extract structured functionally discriminating patterns from massive brain data (i.e data-driven
atlases)  we have extended the online dictionary-learning framework ﬁrst developed in [16]  to learn
structured regions representative of brain organization. To this end  we have successfully augmented
[16] with a Laplacian penalty on the component maps  while conserving the low numerical complexity
of the latter. Through experiments  we have shown that the resultant model –Smooth-SODL model (2)–
extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject
variability in small medium  and large-scale regimes alike  compared to state-of-the-art models. We
believe such online multivariate online methods shall become the de facto way to do dimensionality
reduction and ROI extraction in the future.

Implementation. The authors’ implementation of the proposed Smooth-SODL (2) model will soon
be made available as part of the Nilearn package [2].

Acknowledgment. This work has been funded by EU FP7/2007-2013 under grant agreement no.
604102  Human Brain Project (HBP) and the iConnectome Digiteo. We would also like to thank the
Human Connectome Projection for making their wonderful data publicly available.

8

References

[1] A. Abraham et al. “Extracting brain regions from rest fMRI with Total-Variation constrained

dictionary learning”. In: MICCAI. 2013.

[2] A. Abraham et al. “Machine learning for neuroimaging with scikit-learn”. In: Frontiers in

Neuroinformatics (2014).

[3] L. Baldassarre  J. Mourao-Miranda  and M. Pontil. “Structured sparsity models for brain

decoding from fMRI data”. In: PRNI. 2012.

[4] A. Beck and M. Teboulle. “A Fast Iterative Shrinkage-Thresholding Algorithm for Linear

Inverse Problems”. In: SIAM J. Imaging Sci. 2 (2009).

[5] C. F. Beckmann and S. M. Smith. “Probabilistic independent component analysis for functional

magnetic resonance imaging”. In: Trans Med. Im. 23 (2004).

[6] D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[7] L. Condat. “Fast projection onto the simplex and the (cid:96)1 ball”. In: Math. Program. (2014).
[8] E. Dohmatob et al. “Benchmarking solvers for TV-l1 least-squares and logistic regression in

brain imaging”. In: PRNI. IEEE. 2014.
J. Duchi et al. “Eﬃcient projections onto the l 1-ball for learning in high dimensions”. In:
ICML. ACM. 2008.

[9]

[10] K. J. Friston et al. “Statistical Parametric Maps in Functional Imaging: A General Linear

Approach”. In: Hum Brain Mapp (1995).

[11] L. Grosenick et al. “Interpretable whole-brain prediction analysis with GraphNet”. In: Neu-

roImage 72 (2013).

[12] HCP wiki. https://wiki.humanconnectome.org/display/PublicData/HCP+Data+

Dictionary+Public-+500+Subject+Release. Accessed: 2010-09-30.

[13] M. Hebiri and S. van de Geer. “The Smooth-Lasso and other (cid:96)1 + (cid:96)2-penalized methods”. In:

[14] D. P. Hibar et al. “Genetic clustering on the hippocampal surface for genome-wide association

Electron. J. Stat. 5 (2011).

studies”. In: MICCAI. 2013.

[15] R. Jenatton  G. Obozinski  and F. Bach. “Structured sparse principal component analysis”. In:

AISTATS. 2010.
J. Mairal et al. “Online learning for matrix factorization and sparse coding”. In: Journal of
Machine Learning Research 11 (2010).

[16]

[17] A. Mensch et al. “Dictionary Learning for Massive Matrix Factorization”. In: ICML. ACM.

[18] R. Saxe  M. Brett  and N. Kanwisher. “Divide and conquer: a defense of functional localizers”.

2016.

In: Neuroimage 30 (2006).

as FSL”. In: Neuroimage 23 (2004).

NeuroImage 62 (2012).

[19] S. M. Smith et al. “Advances in functional and structural MR image analysis and implementation

[20] D. van Essen et al. “The Human Connectome Project: A data acquisition perspective”. In:

[21] E. Varol and C. Davatzikos. “Supervised block sparse dictionary learning for simultaneous
clustering and classiﬁcation in computational anatomy.” eng. In: Med Image Comput Comput
Assist Interv 17 (2014).

[22] G. Varoquaux et al. “A group model for stable multi-subject ICA on fMRI datasets”. In:

[23] G. Varoquaux et al. “Cohort-level brain mapping: learning cognitive atoms to single out

Neuroimage 51 (2010).

specialized regions”. In: IPMI. 2013.

[24] G. Varoquaux et al. “FAASTA: A fast solver for total-variation regularization of ill-conditioned

problems with application to brain imaging”. In: arXiv:1512.06999 (2015).

[25] G. Varoquaux et al. “Multi-subject dictionary learning to segment an atlas of brain spontaneous

activity”. In: Inf Proc Med Imag. 2011.

[26] Y. Ying and D.-X. Zhou. “Online regularized classiﬁcation algorithms”. In: IEEE Trans. Inf.

Theory 52 (2006).

9

,Elvis DOHMATOB
Arthur Mensch
Gael Varoquaux
Bertrand Thirion