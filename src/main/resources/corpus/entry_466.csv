2018,Algebraic tests of general Gaussian latent tree models,We consider general Gaussian latent tree models in which the observed variables are not restricted to be leaves of the tree. Extending related recent work  we give a full semi-algebraic description of the set of covariance matrices of any such model.  In other words  we find polynomial constraints that characterize when a matrix is the covariance matrix of a distribution in a given latent tree model. However  leveraging these constraints to test a given such model is often complicated by the number of constraints being large and by singularities of individual polynomials  which may invalidate standard approximations to relevant probability distributions. Illustrating with the star tree  we propose a new testing methodology that circumvents singularity issues by trading off some statistical estimation efficiency and handles cases with many constraints through recent advances on Gaussian approximation for maxima of sums of high-dimensional random vectors. Our test avoids the need to maximize the possibly multimodal likelihood function of such models and is applicable to models with larger number of variables.  These points are illustrated in numerical experiments.,Algebraic tests of general Gaussian latent tree models

Dennis Leung

Department of Data Sciences and Operations

University of Southern California

Department of Statistics  University of Washington &

Department of Mathematical Sciences  University of Copenhagen

dmhleung@uw.edu

Mathias Drton

md5@uw.edu

Abstract

We consider general Gaussian latent tree models in which the observed variables
are not restricted to be leaves of the tree. Extending related recent work  we
give a full semi-algebraic description of the set of covariance matrices of any
such model. In other words  we ﬁnd polynomial constraints that characterize
when a matrix is the covariance matrix of a distribution in a given latent tree
model. However  leveraging these constraints to test a given such model is often
complicated by the number of constraints being large and by singularities of
individual polynomials  which may invalidate standard approximations to relevant
probability distributions. Illustrating with the star tree  we propose a new testing
methodology that circumvents singularity issues by trading off some statistical
estimation efﬁciency and handles cases with many constraints through recent
advances on Gaussian approximation for maxima of sums of high-dimensional
random vectors. Our test avoids the need to maximize the possibly multimodal
likelihood function of such models and is applicable to models with larger number
of variables. These points are illustrated in numerical experiments.

1

Introduction

Latent tree models are associated to a tree-structured graph in which some nodes represent observed
variables and others represent unobserved (latent) variables. Due to their tractability  these models
have found many applications in ﬁelds ranging from the traditional life sciences  biology and
psychology to contemporary areas such as artiﬁcial intelligence and computer vision; refer to Mourad
et al. [2013] for a comprehensive review. In this paper  we study the problem of testing the goodness-
of-ﬁt of a postulated Gaussian latent tree model to an observed dataset. In a low dimensional
setting where the number of observed variables is small relative to the sample size at hand  testing is
usually based on the likelihood ratio which measures the divergence in maximum likelihood between
the postulated latent tree model and an unconstrained Gaussian model. This  however  requires
maximization of the possibly multimodal likelihood function of latent tree models. In contrast  recent
work of Shiers et al. [2016] takes a different approach and leverages known polynomial constraints
on the covariance matrix of the observed variables in a given Gaussian latent tree. Speciﬁcally  the
postulated latent tree is tested with an aggregate statistic formed from estimates of the polynomial
quantities involved. This approach can be traced back to Spearman [1904] and Wishart [1928]; also
see Drton et al. [2007  2008].
We make the following new contributions. In Section 2  we extend the polynomial characterization of
Shiers et al. [2016] to cases where observed nodes may also be inner nodes of the tree as considered 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

for example  in the tree learning algorithms of Choi et al. [2011]. Section 3 describes how we
may use polynomial equality constraints to test a star tree model. We base ourselves on the recent
groundbreaking work of Chernozhukov et al. [2013a]  form our test statistic as the maximum of
unbiased estimates of the relevant polynomials  and calibrate the critical value for testing based on
multiplier bootstrapping techniques. This new way of using the polynomials to furnish a test allows
us to handle latent trees with a larger number of observed variables and avoids potential singularity
issues caused by individual polynomials. Numerical experiments in Section 4 makes comparisons to
the likelihood ratio test and assesses the size of our tests in ﬁnite samples. Section 5 discusses future
research directions.

Notation. Let 1 ≤ r ≤ m be two positive integers. We let [m] = {1  . . .   m} and write(cid:8)m

(cid:9) :=

{I ⊆ [m] : |I| = r} for the collection of subsets of [m] with cardinality r. The supremum norm of
a vector is written (cid:107) · (cid:107)∞. For two random variables R1 and R2  the symbols R1 =d R2 indicates
that R1 and R2 have the same distributions  and R1 ≈d R2 indicates that the distributions are
approximately equal. N (µ  σ2) means a normal distribution with mean µ and standard deviation σ2.

r

2 Characterization of general Gaussian latent trees

We ﬁrst provide the deﬁnition of the models considered in this paper. A tree is an undirected graph in
which any two nodes are connected by precisely one path. Let T = (V  E) be a tree  where V is the
set of nodes  and E is the set of edges which we take to be unordered duples of nodes in V . We say
that T is a latent tree if it is paired with a set X = {X1  . . .   Xm} ⊂ V   corresponding to m observed
variables  such that v ∈ X whenever v ∈ V is of degree less than or equal to two. In particular  X
contains all leaf nodes of the tree T (i.e.  nodes of degree 1)  but it may contain additional nodes.
The nodes in V \ X correspond to latent variables that are not observed but each have at least three
other neighbors in the tree. This minimal degree requirements of 3 on the latent nodes ensures
identiﬁability [Choi et al.  2011  p.1778]. In the terminology of mathematical phylogenetics  T is a
semi-labeled tree on X with an injective labeling map; see Semple and Steel [2003  p.16]. However 
phylogenetic trees are latent trees restricted to have X equal to the set of leaves. While we have
deﬁned X as a set of nodes  it will be convenient to abuse notation slightly and let X also denote a
random vector (X1  . . .   Xm)(cid:48) whose coordinates correspond to the nodes in question. The context
will clarify whether we refer to nodes or random variables.
Now we present the polynomial characterization of a Gaussian latent tree graphical model that
extends the results in Shiers et al. [2016]. The Gaussian graphical model on T   denoted M(T )  is
the set of all |V |-variate Gaussian distributions respecting the pairwise Markov property of T   i.e. 
for any pair u  v ∈ V with (u  v) (cid:54)∈ E  the random variables associated to u and v are conditionally
independent given the variables corresponding to V \{u  v}. The T -Gaussian latent tree model on X 
denoted MX(T )  is the set of all m-variate Gaussian distributions that are the marginal distribution
for X under some distribution in M(T ). For a given distribution in M(T )  let ρpq be the Pearson
correlation of the pair (Xp  Xq) for any 1 ≤ p (cid:54)= q ≤ m. The pairwise Markov property implies that

(cid:89)

ρpq =

(u v)∈phT (Xp Xq)

ρ(cid:48)
uv 

(2.1)

where phT (Xp  Xq) denotes the set of edges on the unique path that connects Xp and Xq in T   and
ρ(cid:48)
uv is the Pearson correlation between a pair of nodes u and v in V . Of course  ρ(cid:48)
uv = ρpq if u = Xp
and v = Xq. In the sequel  we often abbreviate phT (Xp  Xq) as phT (p  q) for simplicity.
Suppose Σ = (σpq)1≤p q≤m is the covariance matrix of X. Our task is to test whether Σ comes
from MX(T ) against a saturated Gaussian graphical model. We assume that all edges in the tree T
correspond to a nonzero correlation  so that Σ contains no zero entries. The covariance matrices for
MX(T ) are parametrized via (2.1). As shown in Shiers et al. [2016]  this set of covariance matrices
may be characterized by leveraging results on pseudo-metrics deﬁned on X. Suppose w : E −→ R≥0
is a function that assigns non-negative weights to the edges in E. One can then deﬁne a pseudo-metric
δw : X × X −→ R≥0 by

δw(Xp  Xq) =

e∈phT (p q) w(e)

: p (cid:54)= q 
: p = q.

(cid:26) (cid:80)

0

2

This is known as a T -induced pseudo-metric on X. The following lemma characterizes all the
pseudo-metrics on X that are T -induced. The proof is a bit delicate and is given in our supplementary
material.
Lemma 2.1. Suppose δ : X × X −→ R≥0 is a pseudo-metric deﬁned on X. Let δpq = δ(Xp  Xq)
for any p  q ∈ [m] for simplicity. Then δ is a T -induced pseudo-metric if and only if for any four
distinct 1 ≤ p  q  r  s ≤ m such that phT (p  q) ∩ phT (r  s) = ∅ 

and for any three distinct 1 ≤ p  q  r ≤ m 

δpq + δrs ≤ δpr + δqs = δps + δqr 

(2.2)

(2.3)

δpq + δqr = δpr

if phT (p  r) = phT (p  q) ∪ phT (q  r).
Lemma 2.1 modiﬁes Corollary 1 in Shiers et al. [2016] by requiring the extra equality constraints in
(2.3) concerning three distinct variable indices. For any subset S ⊂ X  let T|S be the restriction of
T to S  that is  the minimal subtree of T induced by the elements in S with all the nodes of degree
two not in S suppressed [Semple and Steel  2003  p.110]; refer to Section 7 in our supplementary
material for the related graphical notions. Shiers et al. [2016] only consider phylogenetic trees in
which the observed variables X always correspond to the set of nodes in T with degree one. In
this case the constraint in (2.3) is vacuous. Indeed  if Xp  Xq  Xr are any three observed nodes
in T   then T|{Xp  Xq  Xr} must have the conﬁguration on the left panel of Figure 2.1  and it
can be seen that phT (πp  πq) ∪ phT (πq  πr) (cid:54)= phT (πp  πr) for any permutation (πp  πq  πr) of
(p  q  r). However  for a general latent tree T whose observed nodes are not conﬁned to be the leaves 
condition (2.3) is necessary for a pseudo-metric δ to be T -induced: T|{Xp  Xq  Xr} may take the
conﬁguration on the right panel of Figure 2.1  where for some permutation (πp  πq  πr) of (p  q  r) 
phT (πp  πr) = phT (πp  πq) ∪ phT (πq  πr)  and it must hold that

δπpπr = δπpπq + δπqπr

if δ is T -induced.
While condition (2.2) appears in the result of Shiers et al. [2016]  it may lead to different patterns
of constraints for a general latent tree. For four distinct indices 1 ≤ p  q  r  s ≤ m  there are
three possible partitions into two subsets of equal sizes  namely  {p  q}|{r  s}  {p  r}|{q  s} and
{p  s}|{q  r}. These three partitions correspond to the path pairs

(phT (p  q)  phT (r  s))  (phT (p  r)  phT (q  s)) and (phT (p  s)  phT (q  r))

(2.4)
respectively. Now refer to Figure 2.2 which shows all possible conﬁgurations of the restriction of T
to the four observed variables Xp  Xq  Xr  Xs. In Figure 2.2(a)-(c)  up to permutations of the indices
{p  q  r  s}  only one of three pairs in (2.4) can give an empty set when the intersection of its two
component paths is taken. In light of (2.2)  this implies that  for some permutation π of the indices
p  q  r  s 

δπpπq + δπrπs ≤ δπpπr + δπqπs = δπpπs + δπqπr .

(2.5)
On the contrary  in Figure 2.2(d) and (e)  it must be the case that each of the three path pairs in
(2.4) gives an empty set when an intersection is taken between its two component paths  giving the
equalities δpq + δrs = δpr + δqs = δps + δqr in consideration of (2.2).
Lemma 2.1 readily implies a characterization of the latent tree model MX(T ) via polynomial
constraints in the entries of the covariance matrix Σ = (σpq) as spelt out in the ensuing corollary. Its
proof employs similar arguments in Shiers et al. [2016] and is deferred to our supplementary material.

4

In what follows  we let Q ⊂(cid:8)m
other words  Q contains all S ∈(cid:8)m
will let L be the set of all triples S = {p  q  r} ∈ (cid:8)m

(cid:9) such that only one of
(cid:9) be the set of all quadruples {p  q  r  s} ∈(cid:8)m
(cid:9) such that T|S is one of the conﬁgurations in Figure 2.2(a)-(c).
(cid:9) such that T|S has the conﬁguration in

the three path pairs in (2.4) gives an empty set when the union of its two component paths is taken. In
Given {p  q  r  s} ∈ Q  we write {p  q}|{r  s} ∈ Q to indicate that {p  q  r  s} belongs to Q in a
way that it is the path pairs phT (p  q) and phT (r  s) that have empty intersection. Similarly  we
Figure 2.1(b). We will use the notation p − q − r ∈ L to indicate that q is the “middle point" such
that phT (p  q) ∩ phT (q  r) = ∅.
Corollary 2.2. Suppose Σ = (σpq)1≤p q≤m is the covariance matrix of X and has no zero entries.
The following are together necessary and sufﬁcient for the distribution of X to belong to MX(T ):

4

4

3

3

(a)
Figure 2.1: The possible restrictions of a latent tree to three distinct observed variables. Observed
variables correspond to solid black dots  latent variables to grey circles.

(b)

Figure 2.2: The possible restrictions of a latent tree to four distinct observed variables. From left to
right  (a)-(e). Observed variables correspond to solid black dots  latent variables to grey circles.

i. Inequality constraints:

(a) For any {p  q  r} ∈(cid:8)m
(b) For any {p  q  r} ∈(cid:8)m

3

3

(cid:9)  σpqσprσqr ≥ 0.
(cid:9)\L 

qqσ2
(c) For any {p  q}|{r  s} ∈ Q  σ2

σ2
pqσ2

pr  σ2

prσ2

qr − σ2

qr − σ2
pqσ2

prσ2
qs − σ2

rrσ2
rs ≤ 0.

pq  σ2

pqσ2

pr − σ2

qr ≤ 0.

ppσ2

ii. Equality constraints:

(a) For any p − q − r ∈ L  σpqσqr − σqqσpr = 0.
(b) For any {p  q}|{r  s} ∈ Q  σprσqs − σpsσqr = 0.
(c) For any {p  q  r  s} (cid:54)∈ Q  σpsσqr − σprσqs = σpqσrs − σprσqs = 0.

3 Testing a star tree model

In this section we illustrate how one can test a postulated Gaussian latent tree model using Corol-
lary 2.2. In order to focus the discussion we treat the simple but important special case of a star
tree  which corresponds to a single factor model. A single factor model with m observed variables
X = {X1 . . .   Xm} can be described by the linear system of equations
1 ≤ p ≤ m 

Xp = µp + βpH + p 

(3.1)
where µp is the mean of Xp  H ∼ N (0  1) is a latent variable  βp is the loading coefﬁcient for
variable Xp  and p ∼ N (0  σ2
p ) is the idiosyncratic error for variable Xp. All of H  1  . . .   m
are independent. The model postulates that X1  . . .   Xm are conditionally independent given H. It
thus corresponds to the graphical model associated with a star tree T(cid:63) = (V  E) with V = X ∪ {H} 
E = {(H  Xp)}1≤p≤m.
Let X1  . . .   Xn be i.i.d. draws from the distribution of X  which is assumed to be Gaussian. Our goal
is to test whether the distribution of X belongs to the single factor model MX(T(cid:63)). Without loss of
generality  we may assume that µp = 0 for all p ∈ [m] [Anderson  2003  Theorem 3.3.2]. We proceed
by testing whether all the constraints in Corollary 2.2 are simultaneously satisﬁed with respect to the
latent tree T(cid:63). For simplicity  we will focus on testing the equality constraints in Corollary 2.2(ii) 
and brieﬂy discuss how one can incorporate the inequality constraints in Corollary 2.2(i) in Section 5.
For T(cid:63)  both sets L and Q are empty  so that Corollary 2.2(ii)(a) and (b) are automatically satisﬁed.

Hence  we are only left with Corollary 2.2(ii)(c): For any {p  q  r  s} ∈(cid:8)m

(cid:9) 

4

σpsσqr − σprσqs = σpqσrs − σprσqs = 0.

(3.2)

The two polynomials above  equal to det(Σpq sr) and det(Σps qr) respectively  are known as tetrads
in the literature of factor analysis. It is well-known that they deﬁne equality constraints for a single
factor model [Bekker and de Leeuw  1987  Bollen and Ting  1993  Drton et al.  2007].

4

4

√

√

3.1 Estimating tetrads

The idea now is to estimate each one of the 2 ·(cid:0)m
test statistic. From the sample covariance matrix S = (spq) = n−1(cid:80)n

(cid:1) tetrads in (3.2)  and aggregate the estimates in a

i   a straightforward
sample tetrad estimate  say spssqr − sprsqs  can be computed.
If one deﬁne the vectors t =
(sps  sqr  spr  sqs)(cid:48) and t0 = (σps  σqr  σpr  σqs)(cid:48)  as well as the function g(t) = spssqr − sprsqs 
n(g(t)− g(t0)) → N (0 ∇g(t0)(cid:48)V ∇g(t0))  where V is the
by the delta method it is expected that
n(t− t0) and ∇g(t0) is the gradient of g(·) evaluated at t0. However 
limiting covariance matrix of
the distribution of this sample tetrad becomes asymptotically degenerate at singularities  that is  when
the gradient ∇g(t0) vanishes  which happens if the underlying true covariances are zero [Drton and
Xiao  2016]. Consequently  a standardized sample tetrad cannot be well approximated by a normal
distribution if the underlying correlations are weak. More generally  even for stronger correlations 
we found it difﬁcult to reliably estimate the variance of all sample tetrads in larger-scale models.
We propose alternative estimators for which sampling variability can be estimated more easily. Due to
the independence of samples  the tetrad det(Σpq sr) = σpsσqr − σprσqs can be estimated unbiasedly
with the differences

i=1 XiXT

Yi (pq)(sr) := Xp iXs iXq i+1Xr i+1 − Xp iXr iXq i+1Xs i+1  i = 1  . . .   n − 1 

(3.3)
where the subscripts in Yi (pq)(sr) is indicative of the row and column indices for the subma-
trix Σpq sr. These differences can then be averaged for an estimate of the tetrad. Similarly 
one can form Yi (ps)(qr) to estimate det(Σps qr) in (3.2).
If we arrange all the tetrads from
{det(Σpq sr)  det(Σps qr)}{p q r s}∈{m
mates {Yi (pq)(sr)  Yi (ps)(qr)}{p q r s}∈{m
theorem for 1-dependent sums ensures that for sufﬁciently large sample size n we have the distribu-
where ¯Y = (n − 1)−1(cid:80)n−1
tional approximation

(cid:1)-vector Θ  and correspondingly arrange the esti-
(cid:1)-vector Yi for each i  then the central limit

(3.4)
i=1 Yi and Υ = Cov[Y1  Y1] + 2Cov[Y1  Y2]. The latter limiting
covariance matrix will not degenerate to a singular matrix even if the underlying covariance matrix
for X has zeros at which some of the tetrads are singular (i.e. have zero gradient).

4} into a 2(cid:0)m
4} into a 2(cid:0)m

n − 1( ¯Y − Θ) ≈d N (0  Υ) 

√

4

4

3.2 Bootstrap test
The fact from (3.4) could serve as the starting point for a test of model M(T(cid:63)). However  the normal
approximation quickly becomes of concern when moving beyond a small number of variables m.

Indeed  the dimension of Θ  2(cid:0)m
(cid:1)  may well be close to the sample size n  or even larger. For
2(cid:0)8
(cid:1) = 140  more than half the sample size. A recent work of Zhang and Wu [2017]  which follows

instance  if n = 250  for a model with merely 8 observed variables the dimension of Θ is already

up on the groundbreaking paper of Chernozhukov et al. [2013a] on Gaussian approximation for
maxima of high dimensional independent sums  suggests that while the approximation in (3.4) may
be dubious  by taking a supremum norm on both sides  the Gaussian approximation

4

4

n − 1(cid:107)( ¯Y − Θ)(cid:107)∞ ≈d (cid:107)Z(cid:107)∞ 

(3.5)
where Z =d N (0  Υ)  can be valid even the dimension of Θ is large compared to n. In fact  the
original work of Chernozhukov et al. [2013a] suggested that asymptotically  the dimension can be
sub-exponential in the sample size for the Gaussian approximation to hold. In what follows  we will
discuss implementation of and experiments with a vanishing tetrad test based on (3.5). While it is
possible to adapt the supporting theory for the present application  the technical details are involved
and beyond the scope of this conference paper.
Since ¯Y from (3.4) and (3.5) is an estimator of the vector of tetrads Θ  it is natural to use (cid:107) ¯Y(cid:107)∞ as the
test statistic and reject the model M(T(cid:63)) for large values of (cid:107) ¯Y(cid:107)∞. The Gaussian approximation (3.5)
suggests that when M(T(cid:63)) is true  i.e. Θ = 0 
n − 1(cid:107)Y(cid:107)∞ is distributed as (cid:107)Z(cid:107)∞. Nevertheless 
to calibrate critical values based on the distribution of (cid:107)Z(cid:107)∞  one must estimate the unknown
covariance matrix Υ. Zhang and Wu [2017] suggested the batched mean estimator

√

√

(cid:33)T

(Yi − ¯Y)

 

(3.6)

(cid:32)(cid:88)

ω(cid:88)

b=1

i∈Lb

ˆΥ =

1
Bω

(cid:33)(cid:32)(cid:88)

i∈Lb

(Yi − ¯Y)

5

where for a batch size B and ω := (cid:98)(n − 1)/B(cid:99) one considers the non-overlapping sets of samples
Lb = {1 + (b − 1)B  . . .   bB}  b = 1  . . .   ω. The “batching" aims to capture the dependence among
the Yi’s  and has been widely studied in the time series literature [Bühlmann  2002  Lahiri  2003]. If
model M(T(cid:63)) is true  then (3.5) yields that

√

T :=

n − 1(cid:107) diag( ˆΥ)−1/2 ¯Y(cid:107)∞ ≈d (cid:107) diag( ˆΥ)−1/2 ˜Z(cid:107)∞ 

where the right-hand side is to interpreted conditionally on ˆΥ  with ˜Z ∼ N (0  ˆΥ) and diag( ˆΥ)
comprising only the diagonal of ˆΥ. More precisely  for a ﬁxed test level α ∈ (0  1)  if we deﬁne q1−α
to be the conditional (1 − α)-quantile of the distribution of (cid:107) diag( ˆΥ)−1/2 ˜Z(cid:107)∞ given ˆΥ  then

(3.7)
according to Zhang and Wu [2017  Corollary 5.4]. We will use T as our test statistic for the model
M(T(cid:63))  and calibrate the critical value based on (3.7) by simulating the conditional quantile q1−α
from (cid:107) diag( ˆΥ)−1/2 ˜Z(cid:107)∞ for ﬁxed ˆΥ.

P (T > q1−α) ≈ α 

3.3

Implementation

While our above presentation invoked the estimate ˆΥ  which is a matrix with O(m8) entries  we
may in fact bypass the problem of computing such a large covariance matrix for the tetrad estimates.
To simulate the conditional quantile q1−α in (3.7)  let e1  . . .   eω be i.i.d. standard normal random
variables  and consider the expression

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) diag( ˆΥ)−1/2

√

Bω

ω(cid:88)

b=1

eb

(cid:32)(cid:88)

i∈Lb

(cid:33)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞

(Yi − ¯Y)

 

(3.8)

which has exactly the same distribution as (cid:107) diag( ˆΥ)−1/2 ˜Z(cid:107)∞ conditioning on the data X1  . . .   Xn.
We emphasize the O(m4) diagonal entries of ˆΥ are easily computed as variances in (3.6).
In
conclusion  we perform the following multiplier bootstrap procedure: (i) Generate many  say E =
1000  sets of {e1  . . .   eω}  (ii) evaluate (3.8) for each of these E sets  and (iii) take q1−α to be the
1− α quantile from the resulting E numbers. Despite the bootstrap being a computationally intensive
process  it is not hard to see that the evaluation of (3.8) for all E sets of multipliers will involve
O(m4nE) operations  which even for moderate m is far less than the O(m8) operations needed to
obtain an entire covariance matrix for all tetrads.
Remark. It is instructive to make a comparison with the testing methodology in Shiers et al. [2016] 
where the focus was on lower-dimensional applications. Suppose τ : Σ (cid:55)→ Θ is the function that
maps the covariance matrix Σ into the vector Θ of tetrads in (3.2). To test the vanishing of the tetrads 
Shiers et al. [2016] form plug-in estimates ˆΘ = τ (S) for Θ with the sample covariance matrix

(cid:1)-vector τ (S)  they
S = n−1(cid:80)n
where (cid:99)Var[τ (S)] is a consistent estimate for Var[τ (S)]; see also Drton et al. [2008]. For a test of
model M(T(cid:63))  this statistic is now compared to a chi-square distribution with 2(cid:0)m
(cid:1) degrees of
the computational disadvantage that one explicitly uses the entire matrix (cid:99)Var[τ (S)] with its O(m8)

i . Letting Var[τ (S)] be the covariance matrix for the 2(cid:0)m

freedom. While this calibration is justiﬁed for sufﬁciently large sample size n by a joint normal
approximation analogous to (3.5)  it can be problematic for large m. Even more pressing can be

nτ (S)T ((cid:99)Var[τ (S)])−1τ (S) 

form a Hotelling’s T 2 type statistic as

i=1 XiXT

(3.9)

4

4

entries.

4 Numerical experiments

We now report on some experiments with the bootstrap test based on the sup-norm of the estimated
tetrads T proposed in Section 3. In the implementation we always use E = 1000 sets of normal
multipliers to simulate the quantile q1−α and work with batch size B = 3 in (3.8). We also benchmark
our methodology against the likelihood ratio test for factor models implemented by the function
factanal in the base library of R  which implements a likelihood ratio (LR) test with Bartlett
correction for more accurate asymptotic approximation. The critical value of the LR test is calibrated

(cid:1) − 1 degrees of freedom [Drton et al.  2009  p.99].

with the chi-square distribution with(cid:0)m−1

2

6

Figure 4.1: Empirical test sizes vs nominal test levels based on 500 experiments. Data are generated
based on MX(T(cid:63)) with parameters as prescribed in the text. Upper panels: (m  n) = (20  250).
Lower panels: (m  n) = (20  500). Left panels: Setup 1. Right panels: Setup 2. Open circles: Test
based on the statistic T . Crosses: LR test implemented by factanal.

4.1 Low dimensional setup

p  all equal 1/3.

We ﬁrst consider two experimental setups  each with data generated from the one-factor model in
(3.1) for both (m  n) = (20  250) and (m  n) = (20  500). The model parameters are as follows:
(i) Setup 1: all loadings βp and error variances σ2
p  are taken to be 1. (ii) Setup 2: β1 and β2 are
taken to be 10  while the other loadings are independently generated based on a normal distribution
with mean 0 and variance 0.2. The error variances σ2
For different nominal test levels α in the range (0  1) that are 0.01 apart  we compare the empirical
sizes of our test based on the statistic T and the likelihood ratio (LR) test implemented by the function
factanal  using 500 repetitions of experiments. The results are shown in Figure 4.1. The left two
panels correspond to Setup 1 and the right two panels correspond to Setup 2  while the upper panels
correspond to (m  n) = (20  250) and lower correspond to (m  n) = (20  500). While we show the
entire range (0  1) for the x-axis  practical interest is typically in the initial part where the nominal
error rate is in say (0  0.1).
In Setup 1  for both sample sizes  the empirical test sizes of the LR test align almost perfectly with the
45◦ line as one would expect from classical theory. The sizes of our test based on T also align better
with 45◦ line as sample sizes grow. Note that for nominal test levels that are of practical interest  T
also gives conservative test sizes for both sample sizes.
In Setup 2  where parameters are close to being “singular"  one can see the true advantage of using
T over the LR test. The empirical test sizes of the LR test with factanal do not align well with
the 45◦ line as one normally expect from classical theory  whereas the test sizes of our statistic T
lean closer to the 45◦ line as n increases. Particularly the performance of the LR test is problematic
since  by rejecting the true model (3.1) all too often  it fails to give even an approximate control
on type 1 error. Note that the values of β and σp  are such that  for the most part  the observed
variables X are rather weakly dependent on each other. If the observations were in fact independent
then the likelihood ratio test statistic does not exhibit a chi-square limiting distribution [Drton  2009 
Theorem 6.1]. This highlights the fact that  in addition to avoiding any non-convex optimization of
the likelihood function of the factor model  our approach based on the simple estimates from (3.3) is
not subject to non-standard limiting behaviors that plague the LR test when the parameter values lean
close to singularities of the parameter space [Drton  2009].

7

0.00.20.40.60.81.00.00.20.40.60.81.0NOMINAL LEVELTEST SIZE0.00.20.40.60.81.00.00.20.40.60.81.0NOMINAL LEVELTEST SIZE0.00.20.40.60.81.00.00.20.40.60.81.0NOMINAL LEVELTEST SIZE0.00.20.40.60.81.00.00.20.40.60.81.0NOMINAL LEVELTEST SIZEFigure 4.2: Empirical test size vs nominal test levels based on 500 experiments for data generated
from MX(T(cid:63)) under Setup 1 and (m  n) = (100  250). Open circles: Test based on T . Crosses: LR
test implemented by factanal.

4.2 Higher dimensional setup

Our last experiment aims to compare the test sizes of the two tests when the number of observed
variables m is relatively large compared to n. Data are exactly as in Setup 1  except that (m  n) =
(100  250). For such a model with large m  the number of tetrads involved in our testing methodology
is so large that even after taking the supremum norm one shouldn’t expect (3.5) to hold; for example 

when m = 50  the dimension of Θ is 2·(cid:0)50
(cid:1) = 460600  and one should be skeptical about the validity
(cid:1) tetrads  and proceed with the bootstrapping procedure in (3.8) with Yi being
10000 of the 2 ·(cid:0)m
test sizes for a practical range of nominal levels when the total number of tetrads being tested  2·(cid:0)20
(cid:1) 

estimates for this selected subset of tetrads alone. The choice of 10000 tetrads to be tested is based on
the fact that  in the previous experiments with (m  n) = (20  250)  our test gives reasonable empirical

of (3.5) when we only have the sample size n = 250. To implement our test  we ﬁrst randomly select

4

4

is approximately 10000. Since the subset of tetrads is randomly selected  our test is still expected to
approximately control the test sizes at nominal level. The results are reported in Figure 4.2 .
As seen  the test based on T has the main features seen in the ﬁrst experiment. In particular  it
successfully controls type I error rates for the practical range of α ∈ (0  0.1). In contrast  with m
increased to 100  the LR test drastically fails to control type I error rate. This is despite the fact that
the setup is regular with parameter values that are far from any model singularity. The reason for the
failure of the LR test is the fact that the dimension is on the same order as the sample size of 250. The
sample size is not large enough for chi-square asymptotics based on ﬁxed dimension m to “kick in”.

4

5 Discussion

In this paper we have established a full set of polynomial constraints on the covariance matrix of
the observed variables  in the form of both equalities and inequalities  that characterizes a general
Gaussian latent tree model whose observed nodes are not conﬁned to be the leaves. Focusing on
the special case of a star tree model  we also experimented with a new methodology for testing
the equality constraints by forming unbiased estimates of the polynomials involved. In simulation
studies  when the number of variables involved is large or the underlying parameters are close to
being “singular"  our test compares favorably with the likelihood ratio test in terms of test size.
Our results have paved the way for developing a full-ﬂedged algebraic test for a Gaussian latent tree
model. Although we have not pursued this generality in the present conference paper  we give a brief
discussion here. Of course  to do so one would ﬁrst need to write an efﬁcient graph algorithm to tease
out all the polynomials entailed by Corollary 2.2 for a given latent tree input. Then the current testing
methodology can be adopted by forming unbiased estimates of all these polynomials at hand  which
also brings to our attention that in Section 3 only the equality constraints in Corollary 2.2(ii) were used
to test the single factor model. For illustration  take the 3-degree monomial in Corollary 2.2 (i)(a) as

8

0.00.20.40.60.81.00.00.20.40.60.81.0NOMINAL LEVELTEST SIZEwhich is unbiased for σpqσprσqr  and then use (n−2)−1(cid:80)n−2
estimate becomes −(n − 2)−1(cid:80)n−2

an example. Like (3.3)  one may form a summand Yi (p q r) = Xp iXq iXp i+1Xr i+1Xq i+2Xr i+2 
i=1 Yi (p q r) as an averaged estimator. To
incorporate the constraints in Corollary 2.2 (i) into our test one can ﬁrst arrange all those inequalities
into “less than" conditions  i.e.  Corollary 2.2 (i)(a) becomes −σpqσprσqr ≤ 0 and the corresponding
i=1 Yi (p q r). Following that  in the deﬁnition of the test statistic
T   one can take a maximum over all the unbiased estimates for the “less than" versions of the
polynomials in Corollary 2.2(i)  in addition to the absolute values of the estimates for the polynomials
in Corollary 2.2(ii). The resulting test statistic shall also reject the model M(T(cid:63)) when its value is
too large. While critical values can still be calibrated with multiplier bootstrap  additional techniques
such as inequality selection can be incorporated to contain the power loss as a result of testing the
inequalities; see Chernozhukov et al. [2013b] for more details.
Another challenge is the determination of the batch size B in (3.6). In our simulation studies of
Section 4 we took B = 3 since we believe that a batch size of 3 should be enough to capture
dependence among the 1-dependent summands. Batch size determination has been widely studied
in the time series literature for low dimensional problems [Bühlmann  2002  Hall et al.  1995 
Lahiri  2003]. To the best of our knowledge  in high dimensions this is still a widely open problem.
Theoretical research on this is far beyond the scope of our current work.

Acknowledgments

Part of this work was undertaken while Dennis Leung was a postdoc at the Chinese University of
Hong Kong  and he would like to thank Professor Qi-Man Shao for some helpful discussions during
that time.

References
T. W. Anderson. An introduction to multivariate statistical analysis. Wiley Series in Probability and Statistics.

Wiley-Interscience [John Wiley & Sons]  Hoboken  NJ  third edition  2003.

Paul A. Bekker and Jan de Leeuw. The rank of reduced dispersion matrices. Psychometrika  52(1):125–135 

1987.

Kenneth A Bollen and Kwok-fai Ting. Conﬁrmatory tetrad analysis. Sociological methodology  pages 147–175 

1993.

Peter Bühlmann. Bootstraps for time series. Statistical Science  pages 52–72  2002.

Victor Chernozhukov  Denis Chetverikov  and Kengo Kato. Gaussian approximations and multiplier bootstrap

for maxima of sums of high-dimensional random vectors. Ann. Statist.  41(6):2786–2819  2013a.

Victor Chernozhukov  Denis Chetverikov  and Kengo Kato. Testing many moment inequalities. arXiv preprint

arXiv:1312.7614  2013b.

Myung Jin Choi  Vincent Y. F. Tan  Animashree Anandkumar  and Alan S. Willsky. Learning latent tree graphical

models. J. Mach. Learn. Res.  12:1771–1812  2011.

Mathias Drton. Likelihood ratio tests and singularities. Ann. Statist.  37(2):979–1012  2009.

Mathias Drton and Han Xiao. Wald tests of singular hypotheses. Bernoulli  22(1):38–59  2016.

Mathias Drton  Bernd Sturmfels  and Seth Sullivant. Algebraic factor analysis: tetrads  pentads and beyond.

Probab. Theory Related Fields  138(3-4):463–493  2007.

Mathias Drton  Hélène Massam  and Ingram Olkin. Moments of minors of Wishart matrices. Ann. Statist.  36

(5):2261–2283  2008.

Mathias Drton  Bernd Sturmfels  and Seth Sullivant. Lectures on algebraic statistics  volume 39 of Oberwolfach

Seminars. Birkhäuser Verlag  Basel  2009.

Peter Hall  Joel L. Horowitz  and Bing-Yi Jing. On blocking rules for the bootstrap with dependent data.

Biometrika  82(3):561–574  1995.

S. N. Lahiri. Resampling methods for dependent data. Springer Series in Statistics. Springer-Verlag  New York 

2003.

9

Raphaël Mourad  Christine Sinoquet  Nevin L. Zhang  Tengfei Liu  and Philippe Leray. A survey on latent tree

models and applications. J. Artiﬁcial Intelligence Res.  47:157–203  2013.

Charles Semple and Mike Steel. Phylogenetics  volume 24 of Oxford Lecture Series in Mathematics and its

Applications. Oxford University Press  Oxford  2003.

N. Shiers  P. Zwiernik  J. A. D. Aston  and J. Q. Smith. The correlation space of Gaussian latent tree models and

model selection without ﬁtting. Biometrika  103(3):531–545  2016.

C. Spearman. “General intelligence ” objectively determined and measured. The American Journal of Psychology 

15(2):201–292  1904.

John Wishart. Sampling errors in the theory of two factors. British Journal of Psychology  19(2):180–187  1928.

Danna Zhang and Wei Biao Wu. Gaussian approximation for high dimensional time series. Ann. Statist.  45(5):

1895–1919  2017.

10

,Dennis Leung
Mathias Drton