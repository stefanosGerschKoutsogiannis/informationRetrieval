2019,Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates,In this work  we improve upon the stepwise analysis of noisy iterative learning algorithms initiated by Pensia  Jog  and Loh (2018) and recently extended by Bu  Zou  and Veeravalli (2019). Our main contributions are significantly improved mutual information bounds for Stochastic Gradient Langevin Dynamics via data-dependent estimates. Our approach is based on the variational characterization of mutual information and the use of data-dependent priors that forecast the mini-batch gradient based on a subset of the training samples. Our approach is broadly applicable within the information-theoretic framework of Russo and Zou (2015) and Xu and Raginsky (2017). Our bound can be tied to a measure of flatness of the empirical risk surface. As compared with other bounds that depend on the squared norms of gradients  empirical investigations show that the terms in our bounds are orders of magnitude smaller.,Information-Theoretic Generalization Bounds for

SGLD via Data-Dependent Estimates

Jeffrey Negrea∗
University of Toronto 

Vector Institute

Mahdi Haghifam∗
University of Toronto 

Element AI

Gintare Karolina Dziugaite

Element AI

Ashish Khisti

University of Toronto

Daniel M. Roy

University of Toronto 

Vector Institute

Abstract

In this work  we improve upon the stepwise analysis of noisy iterative learning
algorithms initiated by Pensia  Jog  and Loh (2018) and recently extended by Bu 
Zou  and Veeravalli (2019). Our main contributions are signiﬁcantly improved
mutual information bounds for Stochastic Gradient Langevin Dynamics via data-
dependent estimates. Our approach is based on the variational characterization of
mutual information and the use of data-dependent priors that forecast the mini-
batch gradient based on a subset of the training samples. Our approach is broadly
applicable within the information-theoretic framework of Russo and Zou (2015)
and Xu and Raginsky (2017). Our bound can be tied to a measure of ﬂatness of the
empirical risk surface. As compared with other bounds that depend on the squared
norms of gradients  empirical investigations show that the terms in our bounds are
orders of magnitude smaller.

1

Introduction

Stochastic subgradient methods  especially stochastic gradient descent (SGD)  are at the core of re-
cent advances in deep-learning practice. Despite some progress  developing a precise understanding
of generalization error for that class of algorithms remains wide open. Concurrently  there has been
steady progress for noisy variants of SGD  such as stochastic gradient Langevin dynamics (SGLD)
[13  26  34] and its full-batch counterpart  the Langevin algorithm [13]. The introduction of Gaus-
sian noise to the iterates of SGD expands the set of theoretical frameworks that can be brought to
bear on the study of generalization. In pioneering work  Raginsky  Rakhlin  and Telgarsky [26]
exploit the fact that SGLD approximates Langevin diffusion  a continuous time Markov process  in
the small step size limit. One drawback of this and related analyses involving Markov processes is
the reliance on mixing. We hypothesize that SGLD is not mixing in practice  so results based upon
mixing may not be representative of empirical performance.
In recent work  Pensia  Jog  and Loh [24] perform a stepwise analysis of a family of noisy iterative
algorithms that includes SGLD and the Langevin algorithm. At the foundation of this work is the
framework of Russo and Zou [29] and Xu and Raginsky [35]  where mean generalization error is
controlled in terms of the mutual information between the dataset and the learned parameters. (See
also the study of on-average KL stability by Wang  Lei  and Fienberg [33].) However  because
the data distribution is unknown  so is any mutual information involving the data. This presents a
signiﬁcant barrier to understanding generalization in terms of mutual information.

∗Equal contribution authors  order of names was determined randomly.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

One of the key contributions of Pensia et al. is a bound on the mutual information between the data
and the ﬁnal weights  which they construct from a bound on the mutual information between the data
and the entire trajectory of weights. By exploiting properties of mutual information  they express
the latter as a sum of conditional mutual informations associated with each gradient step. While
these conditional mutual informations are also unknown  Pensia et al. obtain a bound in terms of the
Lipschitz constant for the objective function being optimized.
By passing to the full trajectory and exploiting Lipschitz continuity  Pensia et al. circumvent the
statistical barrier posed by the unknown mutual information. Their analysis  however  introduces
several sources of looseness. In particular  the use of Lipschitz constants  which lead to distribution-
independent bounds  eradicates any hope that these bounds will be non-vacuous for modern models
and datasets. Indeed  for deep neural networks  the Lipschitz constant for the empirical risk would be
prohibitively large  or in some cases inﬁnite  and would immediately render any bound that depends
on them vacuous in regimes of interest. In order to fully exploit the decomposition proposed by
[24]  one needs distribution-dependent bounds on the incremental mutual information at each step.
In fact  by a small change  the bounds established by Pensia et al. can be made to depend on
expected-squared-gradient-norms  rather than Lipschitz constants  producing distribution-dependent
bounds. The resulting bound would be similar to a PAC-Bayesian bound due to Mou et al. [22] 
which we consider to be the SGLD generalization result most similar to the present work. Writ-
ing ∑t≤T ηt for ∑T
learning rate or Lipschitz continuity of the loss or its gradient.

(cid:1) and does not place restrictions on the
(cid:1) generalization bound for SGLD that depends on expected-

their bound is O(cid:0)(cid:112)(β /n)∑t≤T ηt

Qiao [20] derive an O(cid:0)(1/n)(cid:112)β ∑t≤T ηt

t=1 ηt 

In other related work  Li  Luo  and

squared-gradient-norms. However their result requires the learning rate to scale inversely with the
inverse temperature and the Lipschitz constant of the loss  severely limiting the applicability of their
result to typical learning problems. Empirically  squared gradient norms are very large during train-
ing  which suggests that bounds based on these quantities may not explain empirical performance.
As we will show  the dependence on the expected-squared-gradient-norm is spurious.
The key contribution of the present work is the observation that variants of the mutual information
between the learned parameters and a subset of the data can be estimated using the rest of the
data. We refer to such estimates as data-dependent due to their intermediate dependence on part of
the data. The use of data-dependent estimates leads to distribution-dependent bounds that naturally
adapt to the model of interest and the data distribution. In particular  using data-dependent estimates 
we arrive at bounds in terms of the incoherence of gradients in the dataset. Roughly speaking 
the incoherence measures the amount by which batch gradients computed on subsets of the data
disagree  as quantiﬁed by squared norm. Crucially  the incoherence is never larger than the squared-
gradient-norm on average  and the incoherence is 0 for most iterations of SGLD with small batches.
We note that the mutual information between learned parameter and a single data point is used
to produce generalization bounds in work by Bu  Zou  and Veeravalli [6]  Raginsky et al. [27]  and
Wang  Lei  and Fienberg [33]. However  in the SGLD analysis of [6]  they do not use data-dependent
estimates. Instead  they also rely on Lipschitz constants  leading to bounds similar to [24].
In the process of developing tighter distribution-dependent bounds  we also observe that  in some
circumstances  one may obtain tighter estimates by working with conditional or disintegrated
information-theoretic quantities. In particular  doing so provides more opportunities to exchange
expectation and concave functions than are available with previous mutual information bounds. Us-
√
ing their own mutual information bound and the chain rule  [6] improve on the generalization error
logn where n is the sample size. The advantage of [6]
bound for SGLD from [24] by a factor of
that enables this improvement is that their bound is only penalized once per epoch at a randomly cho-
sen step. This effectively changes the order of an expectation and square-root  improving the bound.
Building upon [6  29  35]  we develop generalization bounds in terms of disintegrated information-
theoretic quantities that extract expectations from concave functions as much as possible.
Finally  much like the stepwise analysis of SGD carried out by Hardt  Recht  and Singer [14]  one
could consider an analysis in terms of uniform stability  e.g.  in terms of average leave-one-out KL
stability [12]. Under an assumption of uniform stability  [22] also showed that expected generaliza-
tion error decays rapidly at a O(1/n) rate. However  uniform stability has poor dependence on the
Lipschitz constant  and so  does not even hold in simple settings  like univariate logistic regression.
As such  we do not believe this framework is suitable for studying SGLD as applied in modern

2

machine learning. For other work on information-theoretic analyses generalization error  and on
SGLD  see [1  3  4  15  16  27  32].

1.1 Contributions

The present paper makes the following contributions:

• We provide novel information-theoretic generalization bounds that relate a learned parameter to
a random subset of the training data. These bounds depend on forms of on-average information
stability  but are different from those in existing work due to our use of disintegration.

• We introduce the technique of data-dependent priors for bounding mutual information in data-
dependent estimates of expected generalization error. Speciﬁcally  we use data-dependent priors
to forecast the dynamics of iterative algorithms using a randomly chosen subset of the data.
Each possible subset yields a generalization bound for the empirical risk over the complemen-
tary subset. Combining this with our information-theoretic generalization bounds  we recover
generalization error bounds for the empirical risk on the full dataset.

• We develop bounds for Langevin dynamics and SGLD that depend on a measure of the inco-
herence of empirical gradients. This quantity is typically orders of magnitude smaller than the
squared gradient norms or Lipschitz constants that other bounds depend upon. In our experi-
ments  the difference was a multiplicative factor between 102 and 104.

• Our generalization bound for SGLD is O(cid:0)min(cid:8)(cid:112)(β /bn)∑t≤T ηt   (1/n)∑t≤T

(cid:9)(cid:1) where ηt

(cid:112)βηt

is the learning rate at iteration t  T is the number of iterations  β is the inverse temperature  and
b is the minibatch size. This bound is currently state of the art for bounds without assumptions
on the smoothness of the loss or restrictions on the learning rate.

1.2 Preliminaries

n ∑m

Let D be an unknown distribution on a space Z and let W be a space of parameters. Consider a
loss function (cid:96) : Z × W → R and the corresponding risk function RD (w) = E(cid:96)(Z w). Given an i.i.d.
dataset of size n  S ∼ D n  we may form the empirical risk function ˆRS(w) = 1
i=1 (cid:96)(Zi w)  where
S = (Z1  . . .  Zn). In the setting of classiﬁcation and continuous parameter spaces  the loss function
is discontinuous and the empirical risk function does not convey useful gradient information. For
this reason  it is common to work with a surrogate loss  such as cross entropy. To that end  let
˜(cid:96) : Z × W → R denote a surrogate loss and let ˜RD (w) = E ˜(cid:96)(Z w) and ˜RS(w) = 1
˜(cid:96)(Zi w) be
the corresponding surrogate risk and empirical surrogate risk.
Our primary interest is in the generalization performance of learning algorithms. Abstractly  let W
be a random element in W satisfying W = A (S V )  where V is some auxiliary random element
independent from S and A is a measurable function representing a randomized learning algorithm
that maps the data S to a learned parameter W . Our focus will be the (mean) generalization error

of W   i.e.  E(cid:2)RD (W )− ˆRS(W )(cid:3). Note that we have averaged over both the choice of dataset and the

source of randomness V available to the learning algorithm A .
For random variables X and Y   write EY X = E[X|Y ] and PY [X] for the conditional expectation and
(regular) conditional distribution  respectively  of X given Y .2 Besides the usual notions of KL
divergence  mutual information  and conditional mutual information (see Appendix A for formal
deﬁnitions)  we rely on the following less common notion:
Deﬁnition 1.1. Let X  Y   and Z be arbitrary random elements. Let ⊗ form product measures. The
disintegrated mutual information between X and Y given Z is

n ∑m
i=1

IZ(X;Y ) = KL(PZ[(X Y )](cid:107)PZ[X]⊗ PZ[Y ]).

It follows immediately from deﬁnitions that I(X Y|Z) = EIZ(X Y ). Letting φ satisfy φ (Z) =
IZ(X;Y ) a.s.  deﬁne I(X Y|Z = z) = φ (z). This notation is necessarily well deﬁned only up to a
null set under the marginal distribution of Z.

2We ﬁx arbitrary versions and assume regular versions of conditional distributions exist.

3

2 Methods

In this section  we establish generalization bounds for learning algorithms in terms of information-
theoretic quantities (conditional mutual information  disintegrated mutual information  relative en-
tropy) that depend on the unknown data distribution and the probabilistic properties of the learning
algorithm. We then describe two complementary strategies that we employ to bound these otherwise
intractable quantities. In Section 3  we apply these methods to the study of the Langevin algorithm
and SGLD.
We make repeated use of generalized notions of priors and posteriors  which arise in the PAC-Bayes
literature ([7  21  31]  etc.) and relate to variational bounds on mutual information  which we will
now describe: Consider learned parameters W   data S  and auxiliary variables V   viewed as random
elements in W   Zn  etc.  respectively. In PAC-Bayes  a generalized posterior is an arbitrary random
measure on W . In our setting  the posterior  Q  (of W given S and V) is the conditional distribution
of W given S and V . (Formally  Q is a probability kernel  but one can think informally that Q =
f (S V ) for some measurable function taking values in the space of Borel probability measures  and
so we will simply say that Q is σ (S V )-measurable.)
Deﬁnition 2.1 (Data-dependent prior). Let Q be a σ (S V )-measurable posterior. A (generalized)
prior P is a random measure on W   measurable with respect to some sub-σ-algebra of σ (S V ). A
prior P is said to be data-dependent if it is not independent of S.
Let P be a F -measurable data-dependent prior  where σ (V ) ⊂ F . Using a variational characteriza-
tion of mutual information (see Appendix B.1)  we have

EF [KL(Q(cid:107)P)] ≥ IF (W ;S) a.s. 

(1)
with equality for P = PF [W ]. Therefore  if the expected KL divergence is small  W contains little in-
formation about S beyond what is already captured by F . If the special case where the disintegrated
mutual information is zero  then W is independent of S given F . In the context of generalization 
this implies that the data S not contained in F can be used to form an unbiased estimate of the risk
of W . The bounds we present below extend this logic to nonzero mutual information.
The utility of using data-dependent priors to control disintegrated mutual information depends on the
balance of two effects: On the one hand  I(W ;S) ≤ I(W ;S|F )  and so conditioning never improves a
theoretical bound and may make it looser. On the other hand  I(W ;S) depends on the unknown data
distribution and so distribution-independent bounds will often be very loose. In contrast  the KL
divergence based on P can exploit the information in F ⊂ σ (S V ) to obtain tighter data-dependent
bounds on IF (W ;S).
In order to construct data-dependent priors  we partition the dataset S in two halves  based on a
random subset J ⊂ {1  . . .  n} with #J = m nonrandom. Let J = { j1  . . .   jm}  The ﬁrst half  SJ =
(Z j1  . . .  Z jm)  contains m points  which we will use to construct a data-dependent prior P. The
second half  Sc
J are
independent of J  since m is nonrandom.)
This particular construction of data-dependent priors allow us to leverage a type of non-uniform
KL-stability: the prior P may exploit SJ to make a data-dependent forecast of Q  yielding a bound 
B  on the conditional expected generalization error (with respect to the remaining n− m data points
in Sc
J). Averaging over SJ  we obtain a bound on the (unconditional) expected generalization error.
J be deﬁned as above. Suppose that F is a σ-ﬁeld with σ (SJ) ⊂ F ⊥⊥
Deﬁnition 2.2. Let SJ Sc
σ (Sc
J). An expected generalization error bound based on a data-dependent estimate is one of the
form

J  containing the remaining n− m points  is independent of P. (Note that SJ and Sc

where B is F measurable  and satisﬁes EF(cid:2)RD (W )− ˆRSc

E(cid:2)RD (W )− ˆRS(W )(cid:3) ≤ E[B] 
J (W )(cid:3) ≤ B.

(2)

The idea of using data-dependent priors to obtain tighter bounds is standard in the PAC-Bayes lit-
erature [2  10  23  28]  but its utility in the present work is brought through by our introduction of
data-dependent estimates. In the following section  we derive information-theoretic bounds on ex-
pected generalization error that can exploit data-dependent priors to form data-dependent estimates.
We will then use these tools to study SGLD  without mixing assumptions.

4

2.1

Information-Theoretic Generalization Bounds based on Random Subsets of Data

Existing work by Xu and Raginsky [35] bounds the expected generalization error of a learning
algorithm in terms of the mutual information between the random parameters and the data. The
following result is a simple extension of [35  Thm. 1] that bounds the expected generalization error
in terms of the mutual information between the parameters and a random subset of the data.
Theorem 2.3 (Data-Dependent Mutual Information Bound). Let W be a random element in W   let
S ∼ D n  and let J ⊆ [n]  |J| = m  be uniformly distributed and independent from S and W . Suppose
that (cid:96)(Z w) is σ-subgaussian when Z ∼ D  for each w ∈ W . Let Q = PS[W ]  and let P be a σ (SJ)-
measurable data-dependent prior on W . Then

E(cid:2)RD (W )− ˆRS(W )(cid:3) ≤

(cid:115)

(cid:115)

2

σ 2
n− m

I(W ;Sc

J) ≤

2

σ 2
n− m

E[KL(Q(cid:107)P)].

The proof of this result can be found in Appendix B. When m = 0  this recovers [35  Thm. 1].
When the size of the subset is m = n− 1  this bound is weaker than [6  Prop. 1]  due to the order of
the concave square-root function and the expectation over the choice datapoint to be left out. This
difference is addressed by our next result.
Randomization is one way that learning algorithms can control the mutual information between (a
random subsets of) the data and the learned parameter. Let U be a random element independent
from S and J  representing some aspect of the source of randomness used by the learning algorithm.
Because S ⊥⊥ {J U} and S ∼ D n  we have (SJ U) ⊥⊥ Sc

J and thus

I(W ;Sc

J) ≤ I(W ;Sc

J|SJ U) = EISJ  U (W ;Sc
J) 

where the last equality follows from the deﬁnition of conditional mutual information. The next result
shows that we can pull the expectation over both SJ and U outside the concave square-root function.
In the case of SGLD  U will be the sequence of minibatch index sets.
Theorem 2.4 (Data-Dependent Disintegrated Mutual Information Bound). Let W   S  and J be as in
Theorem 2.3  and let U be independent from S and J. Suppose that (cid:96)(Z w) is σ-subgaussian when
Z ∼ D  for each w ∈ W . Let Q = PS U [W ] and let P be a σ (SJ U)-measurable data-dependent prior
on W . Then

(cid:115)
E(cid:2)RD (W )− ˆRS(W )(cid:3) ≤ E

(cid:115)
J) ≤ E

2

σ 2
n− m

ISJ  U (W ;Sc

2

σ 2
n− m

ESJ  UKL(Q(cid:107)P)

The proof of this result can be found in Appendix B. Since ISJ  U (W ;Sc
J) is (SJ U)-measurable  we
In the case that m = n − 1  our bound is
may use SJ and U to obtain a data-dependent bound.
similar to  but not strictly comparable to  [6  Prop. 1]. Our bound is incomparable due to our use
of disintegrated mutual information  ISJ (W ;Sc
J) and the fact that we take the expectations over the
dataset outside of the convex square-root function. The disintegrated mutual information cannot
be upper bounded by the full mutual information  I(W Sc
J)  which appears in [6] (even by taking
expectations under the square root using Jensen’s inequality). However  Theorem 2.4 is essentially
a disintegrated version of [6  Prop. 1]. In their actual SGLD expected generalization error bound 
[6] controls the unconditional mutual information using the Lipschitz constant of the surrogate loss.
Hence  one could easily recover the same bound using our result. The conditioning we have done 
however  allows us to control the mutual information more carefully in order to achieve a tighter
bound for SGLD than is provided by [6].
These bounds allow for a tradeoff: for large m  the mutual information is measured between the
parameter and a small random subset of the data  and so we expect the mutual information to be
1
small. (Indeed  this term will decrease monotonically in m.) At the same time  the
n−m term is
larger  reﬂecting the reduced effect of averaging over only n− m data to form our estimate of the
empirical risk. It is unclear without further context whether this bound is tighter in the regime of
small  intermediate  and large m. In fact  we ﬁnd that  for the bounds we derive in our applications 
m = n− 1 is optimal. This difference materially affects the quality and tightness of the bounds  as is
discussed in Remark 3.4. However  for m = n− 1 and bounded loss  the following bound is tighter 
while it is incomparable for other values of m.

5

Theorem 2.5 (Data-Dependent KL Bound). Let W   S  J  and U be as in Theorem 2.4. Let Q =
PS U [W ] and let P be a σ (SJ U)-measurable data-dependent prior on W . Suppose that (cid:96)(Z w) is
[a1 a2]-bounded a.s. when Z ∼ D  for each w ∈ W .

(cid:114)
E(cid:2)RD (W )− ˆRS(W )(cid:3) ≤ E

(a2 − a1)2

2

KL(Q(cid:107)P).

The proof of this result can be found in Appendix B. For an analytic comparison of the three bounds
in the case that m = n− 1  see Appendix F. Remark B.2 explains why this result is only stated for
bounded loss functions.

2.2 Decomposing KL Divergences and Mutual Information for Sequential Algorithms
Consider an iterative learning algorithm  and let W0 W1 W2  . . .WT ∈ W be the parameters during
the course of T iterations. In light of the variational bound for mutual information  we can obtain
a generalization bound for WT by bounding the expected KL divergences between the conditional
distribution PSJ [WT ] and some SJ-measurable “prior” distribution P(Z). Unfortunately  the ﬁrst
distribution has no known tractable representation. Pensia  Jog  and Loh [24] use monotonicity to
bound a mutual information involving the terminal parameter with one involving the full trajectory 
then use the chain rule to decompose this into a sum of conditional mutual informations. The same
principles allow us to ﬁrst bound the terminal KL divergence by the KL for the full trajectory  and
then decompose the KL divergence for the full trajectory over each individual step.
Setting some notation  let T be a nonnegative integer  let [T ]0 = {0 1 2  . . .  T}  let µ be a distribu-
tion on W [T ]0  and let X be a random variable with distribution µ. We are interested in naming certain
marginal and conditional distributions (disintegrations) related to µ. In particular  for t ∈ [T ]0  let

i) µt = P[Xt ]  the marginal law of Xt;
ii) µt| = PX0:(t−1)[Xt ]  the conditional law of Xt given X0:(t−1); and
iii) µ0:t = P[X0:t ]  the marginal law of X0:t.
Proposition 2.6 (Decomposition of KL Divergences). Let Q P be probability measures on W [T ]0.
Suppose that Q0 = P0. Then

KL(QT (cid:107)PT ) ≤ KL(Q(cid:107)P) = ∑T

t=1 EQ0:(t−1)[KL(Qt|(cid:107)Pt|)].
where  as per Section 1.2  Qt| is the conditional law of t-th iterate given the previous iterates  and
so KL(Qt|(cid:107)Pt|) is a random variable which depends the (W0  . . .Wt−1) ∼ Q0:t−1.
The proof of this result may be found in Appendix B.
Considering the KL between full trajectories may yield a loose upper bound on the KL between
terminal parameters (in particular  when the trajectory cannot be inferred from the terminus). We
gain  however  analytical tractability  as we will see in the next section when we analyze particular
algorithms stepwise. In fact  many bounds that appear in the literature implicitly require this form
of incrementation. Our approach based on the KL divergence and data-dependent priors gives us
much tighter control of the KL divergence contribution of each step.

3 Generalization Bounds for Speciﬁc Algorithms

Now that we have all of the theoretical tools required  we may establish bounds on the generalization
error of speciﬁc noisy iterative learning algorithms by inventing sensible data-dependent priors.
The use of a data-dependent prior which closely forecasts the true algorithm in each step is key
in establishing tighter generalization bounds. We ﬁrst consider the stochastic gradient Langevin
dynamics (SGLD) algorithm [34]  then handle its full batch counterpart the (unadjusted) Langevin
algorithm [9  11]  which we will refer to informally as Langevin dynamics (LD). Note that the
loss and risk functions used for training  ( ˜(cid:96)  ˜RD   ˜RS)  need not be the same loss functions used for
assessing performance and generalization error  ((cid:96) RD   ˆRS)  as explained in Section 1.2.

3.1 Stochastic Gradient Langevin Dynamics
Let ηt to be the learning rate at time t; βt be the inverse temperature at time t; and εt  i.i.d. N (0 Id).
Let bt be the minibatch size at time t. We are interested in stochastic gradient Langevin dynamics 

6

whose iterates are given by

Wt+1 = Wt − ηt∇ ˜RSt (Wt ) +(cid:112)2ηt /βt εt .

(3)
˜(cid:96)(w z)  and St is a subset of S of size bt sampled uniformly at random with
where ˜RSt (w) = 1
a sampling procedure which is independent of S  and independent of {εt}t≥0. The bt data points in
St are chosen without replacement.

bt ∑z∈St

3.1.1 A data-dependent prior for SGLD

Let SJ be a random subset of S  of size m  chosen independently from W0 W1  . . .  and independently
of the sequence of minibatches  {St}t≥0. Let the set of indices appearing in the t-th minibatch be
denoted by Kt  so that St = SKt for each t. By assumption  each Kt is a uniformly random subset of
{1  . . .  n} of size bt. We set U = (K1  . . .KT )  as to match the notation in the theorems of Section 2.1.
Let SJt = SJ ∩ St = SJ∩Kt and let b(cid:48)

t = St \ SJ = SKt\J and bc

t = #SJt. Let Sc

t = bt − b(cid:48)

t. Deﬁne

(cid:0)∇ ˜RSc
t (Wt )− ∇ ˜RSJ (Wt )(cid:1) .

ξt =

bc
t
bt

(4)

Let Q(S U) be the joint law of (W0  ... WT ) given a dataset S and minibatch sequence U. Then
Q(S U) is a random measure as it depends on the random dataset S and the sequence of indices U.
It follows from Eq. (3) that Q(S U)t| is multivariate normal with mean µQ t (S U) = Wt −ηt∇ ˜RS(Wt )
Id. Consider the data-dependent prior deﬁned so that its conditional Pt|(SJ U) is
and covariance 2 ηt
βt
a multivariate normal with covariance 2 ηt
β
µP t (SJ U) = Wt − ηt

Id  and with mean

(cid:18) b(cid:48)

bt − b(cid:48)

(cid:19)

∇ ˜RSJt (Wt ) +

∇ ˜RSJ (Wt )

.

t

t
bt

bt

Note that µQ t (S U)− µP t (SJ U) = ηtξt (S idx). Thus the one-step KL divergence satisﬁes

2KL(Qt+1|(S idx)(cid:107)Pt+1|(SJ U)) =

βtηt
4

(cid:107)ξt(cid:107)2

2

Applying Proposition 2.6  we have (almost surely over the choice of (S J U))

2KL(QT (S U)(cid:107)PT (SJ U)) ≤ T
∑

t=1

ES J UKL(Qt|(S U)(cid:107)Pt|(SJ U)) =

T

∑

t=1

ES J U βtηt
4

(cid:107)ξt(cid:107)2
2.

Note that ξt depends on the exact weight sequence  and hence is σ (S J U Wt−1)-measurable  but
not σ (S J U)-measurable. Hence  ES J U βtηt

2 is a σ (S J U)-measurable for each t.

8 (cid:107)ξt(cid:107)2

3.1.2 Expected Generalization Error Bounds for SGLD
Theorem 3.1 (Expected Generalization Error Bounds for SGLD). Let {Wt}t∈[T ] denote the iterates
of SGLD. Let the batch size be constant  bt = b. If (cid:96)(Z w) is σ-subgaussian for each w ∈ W   then

T

∑
t=1

n− m

(cid:118)(cid:117)(cid:117)(cid:116) σ 2
(cid:118)(cid:117)(cid:117)(cid:116) (a2 − a1)2

4

E(RD (WT )− RS(WT )) ≤ E

βtηt
4

ESJ  J U(cid:107)ξt(cid:107)2

2 ≤ σ
2

and if (cid:96)(Z w) is [a1 a2]-bounded  and if m = n− 1  then

E(RD (WT )− RS(WT )) ≤ E

T

∑
t=1

βtηt
4

ES J U(cid:107)ξt(cid:107)2

2 ≤

(cid:0) 1

T

∑
t=1

(n− 1)2

(cid:118)(cid:117)(cid:117)(cid:116) n
(cid:20) (a2 − a1)2n

4(n− 1)2b

(cid:1)βtηttr(E[ ˆΣt (S)])

b + 1
n

n−m−1

m

(cid:21)1/2

(cid:118)(cid:117)(cid:117)(cid:116) T

∑
t=1

E

(5)

βtηt
4

tr(ES[ ˆΣt (S)])

(6)

where ˆΣt (S) = VarWt  S
Z∼Unif(S)

(∇ ˜RZ(Wt )) is the ﬁnite population variance matrix of surrogate gradients.

Proof. The results are the direct combinations of Theorem 2.4 and Propositions 2.6 and B.1; and
Theorem 2.5 and Proposition 2.6  respectively  with our data-dependent prior. Jensen’s inequality is
used to move expectations under

√·. Lemma D.2 expresses the results in terms of ˆΣ.

7

2  our generalization error bounds in Eq. (5) is clearly O(cid:0)(cid:112)(β /bn)∑t≤T ηt
(cid:112)βηt

ξt = 0 whenever Kt ⊂ J  we ﬁnd that our ﬁrst bound in Eq. (5) is also O(cid:0)(1/n)∑t≤T

Remark 3.2. Suppose that βt = β   bt = b  and m = n− 1. Under uniform moment conditions on
ESJ  J U(cid:107)ξt(cid:107)2
this  notice that for non-negative random variables Ct and Bt ∼ Ber(p) 

(cid:1). Since
(cid:1). To see

E(cid:113)

t=1BtCt ≤ E[∑T
∑T

t=1 Bt

√
Ct ] = p∑T

√
Ct|Bt = 1].
t=1 E[
ESJ  J U(cid:107)ξt(cid:107)2

2 yields the stated rate.

When m = n− 1  taking Bt = Iξt(cid:54)=0  p = b/n  Ct = βtηt
3.2 Langevin Dynamics

8

Under the same notation as above  the iterates of the Langevin dynamics algorithm are given by

Wt+1 = Wt − ηt∇ ˜RS(Wt ) +(cid:112)2ηt /βt εt .

(cid:47)

(7)

3.2.1 Expected Generalization Error Bounds for LD

(cid:115)

We can recover bounds generalization error bounds for LD as a special case of SGLD when the
batch size is the dataset size  bt = n for all t. The data-dependent prior is the same as for SGLD.
Theorem 3.3 (Expected Generalization Error Bounds for Langevin Dynamics). Let {Wt}t∈[T ] denote
the iterates of the Langevin dynamics algorithm. If (cid:96)(Z w) is σ-subgaussian for each w ∈ W   then

E(RD (WT )− RS(WT ))4 ≤

σ 2

(n− 1)m

T

∑

t=1

βtηt
4

Etr( ˆΣt (S)) 

(8)

(cid:115) T

and if (cid:96)(Z w) is [a1 a2]-bounded and m = n− 1  then

(cid:115)

E(RD (WT )− RS(WT )) ≤ E

(a2 − a1)2

4

T

∑

t=1

βtηt
4

ESJ(cid:107)ξt(cid:107)2

2 ≤ a2 − a1
2(n− 1)

E

βtηt
4

EStr( ˆΣt (S)) 

∑

t=1

where ˆΣt (S) = VarWt  S
Z∼Unif(S)

(∇ ˜RZ(Wt )) is the ﬁnite population variance matrix of surrogate gradients.

For asymptotic properties of this bound when ˜(cid:96) is L-Lipschitz  as in [24]  see Appendix E. For a
simple analytic worked example of mean estimation using Langevin dynamics  refer to Appendix G.
Remark 3.4 (Dependence of our bounds on the subset size  m). The choice of m ∈ {1  . . .  n} can
make a material difference in the quality of the bound and whether it is vacuous or not. As seen
in Eq. (8)  if m is Ω(n) then the upper bound on expected generalization error is O(β /n). If β is
√
n)  as is typical in practice  then overall  the bound is O(n−1/2). If  on the other hand  m is o(n)
√
Ω(
√
n) then our
then the order of the bound with respect to n would be lower—in particular if m is O(
bound would not be decreasing in n for β of order Ω(
n).
(cid:47)

4 Empirical Results

We have developed bounds that depend on the gradient prediction residual of our data dependent
priors (which we call the incoherence of the gradients)  rather than on the gradient norms (as in
[22]) or Lipschitz constants (as in [6  24]). The extent to which this represents an advance is 
however  an empirical question. The functional form of our bounds and those in the cited work
are nearly identical. The ﬁrst key differences between our work and others is the replacement of
gradient norms ((cid:107)∇ ˜Rt(cid:107)2) and Lipschitz constants in other work with gradient prediction residual 
((cid:107)ξt(cid:107)) in our work. The second key difference is the order of expectations and square-roots  which
favor our bounds due to Jensen’s inequality. In this section  we perform an empirical comparison of
the gradient prediction residual of our data dependent priors and the gradient norm across various
architectures and datasets. This illustrates the ﬁrst of the differences  the quantities appearing in the
bound. Our results indicate that that our data-dependent priors yield signiﬁcantly tighter results  as
the sum of square gradient incoherences of our data dependent priors are between 102 and 104 times
smaller than the sum of square gradient norms in the experiments we ran.

8

(a) MLP for MNIST.

(b) CNN for MNIST.

(c) CNN for MNIST.

(d) CNN for MNIST.

(e) CNN for Fashion-MNIST.

(f) CNN for CIFAR-10.

Figure 1: Numerical results for various datasets and architectures. All x-axes show the number of Epochs of training. Fig. 1a shows the
effect of different amounts of heldout data on the summands appearing in our bound  and what those would be if we upper bounded the
incoherence (cid:107)ξ(cid:107) by (cid:107)∇ ˆR(cid:107) when it is not 0. Fig. 1b compares a Monte Carlo estimate of our bound with that of [22] and shows the effect of
inverse temperature on each. Fig. 1c compares a Monte Carlo estimate of our bound with that of [22] and shows the effect of learning rate on
each. Figs. 1d to 1f compare the summands appearing in our bound and those of [22] across datasets.

In Fig. 1  we compare (cid:107)ξt(cid:107)2 and (cid:107)∇ ˜Rt(cid:107)2 in order to assess the improvement our methods bring

over existing results for SGLD. Speciﬁcally  the values of each plot are the averages of(cid:112)ηβ(cid:107)ξt(cid:107)/b
and(cid:112)ηβ(cid:107)∇ ˜RSt(cid:107)/b over an epoch. These serve as estimates of the per-epoch contributions to the

respective summations in our Theorem 3.1 and the bound of Mou et al. (Thm. 2 therein  when there
is no L2-regularization). The average and standard error of both expressions taken over multiple runs
are displayed. Bounds from related work that depend on Lipschitz constants would further upper
bound what we show for [22]  by replacing (cid:107)∇ ˜Rt(cid:107) with a Lipschitz constant. The Lipschitz constant
could be lower bounded by the largest observed gradient norm  and would be off the chart.
From Fig. 1a  we see that the empirical performance reﬂects our analytical results that the bound is
tighter for large m. As can be inferred from Eq. (4)  the difference between (cid:107)ξt(cid:107)2 and (cid:107)∇ ˜Rt(cid:107)2 in-
creases with m. From Figs. 1d to 1f we see that the squared gradient incoherence  (cid:107)ξt(cid:107)2  are between
100 and 10 000 times smaller than the squared gradient norms  (cid:107)∇ ˜R(cid:107)2 in all of these examples.
Using Monte Carlo simulation  we compared estimates of our expected generalization error bounds
with (coupled) estimates of the bound from [22]. The results  in Figs. 1b and 1c  show that our
bounds are materially tighter  and remain non-vacuous after many more epochs. Fig. 1b also com-
pares the two generalization error bounds for different inverse temperature schedules. Fig. 1c com-
pares the two generalization error bounds based for different learning rate schedules. It can inferred
from Figs. 1b and 1c that our proposed bound yields to tighter values when the learning rate and
the inverse temperature are small. However  it should be noted that with small learning rate and the
inverse temperature  it would be difﬁcult to have a very low training error when the empirical risk
minimization is performed using SGLD.
The details of our model architectures  temperature  learning rate schedules and hyperparameter
selections may be found in Appendix H. We did not aim to achieve the state-of-the art predictive
performance. With further tuning  the prediction results could be improved.

Acknowledgments

JN is supported by an NSERC Vanier Canada Graduate Scholarship  and by the Vector Institute.
MH was supported by a MITACS Accelerate Fellowship with Element AI. DMR is supported by an
NSERC Discovery Grant and an Ontario Early Researcher Award. This research was carried out in
part while GKD and DMR were visiting the Simons Institute for the Theory of Computing.

9

13579111315Training epochs102100102104106Squared gradient norm  and squared norm of gradient residual||t||2#heldout(residual)  #heldout=1||Rt||2(mini-batch)  #heldout=1||t||2#heldout(residual)  #heldout=1000||Rt||2(mini-batch)  #heldout=100012345Epoch0.000.250.500.751.001.251.501.752.002.252.502.753.003.25Bound on the Expected Generalization ErrorOur Bound:(high)tMou et al. Bound:(high)tOur Bound:(low)tMou et al. Bound:(low)t123456Epoch0.000.250.500.751.001.251.501.752.002.252.502.753.003.25Bound on the Expected Generalization ErrorOur Bound:(large)tMou et al. Bound:(large)tOur Bound:(small)tMou et al. Bound:(small)t13579111315Training epochs103102101100101Squared gradient norm  and squared norm of gradient residual||t||2(residual)||Rt||2(mini-batch)135791113151719212325Training epochs103102101100101102Squared gradient norm  and squared norm of gradient residual||t||2(residual)||Rt||2(mini-batch)15913172125293337414549Training epochs101100101102Squared gradient norm  and squared norm of gradient residual||t||2(residual)||Rt||2(mini-batch)References
[1] A. Lopez and V. Jog. “Generalization error bounds using Wasserstein distances”. In: IEEE

Information Theory Workshop. 2018.

[2] A. Ambroladze  E. Parrado-Hernández  and J. Shawe-Taylor. “Tighter PAC-Bayes bounds”.

In: Advances in Neural Information Processing Systems. 2007  pp. 9–16.

[3] A. Asadi  E. Abbe  and S. Verdú. “Chaining mutual information and tightening generalization

bounds”. In: Advances in Neural Information Processing Systems. 2018  pp. 7234–7243.

[4] R. Bassily  S. Moran  I. Nachum  J. Shafer  and A. Yehudayoff. “Learners that Use Little

Information”. In: Algorithmic Learning Theory. 2018  pp. 25–55.

[5] S. Boucheron  G. Lugosi  and P. Massart. Concentration inequalities: A nonasymptotic theory

of independence. Oxford university press  2013.

[6] Y. Bu  S. Zou  and V. V. Veeravalli. “Tightening Mutual Information Based Bounds on Gener-
alization Error”. In: IEEE International Symposium on Information Theory (ISIT). To appear.
2019. arXiv: 1901.04609.

[7] O. Catoni. “PAC-Bayesian supervised classiﬁcation: the thermodynamics of statistical learn-
ing”. In: Institute of Mathematical Statistics Lecture Notes-Monograph Series. Vol. 56. 2007.
arXiv: 1901.04609.

[8] M. D. Donsker and S. S. Varadhan. “Asymptotic evaluation of certain Markov process expec-
tations for large time  I”. Communications on Pure and Applied Mathematics 28.1 (1975) 
pp. 1–47.

[9] A. Durmus and E. Moulines. “Nonasymptotic convergence analysis for the unadjusted

Langevin algorithm”. The Annals of Applied Probability 27.3 (2017)  pp. 1551–1587.

[10] G. K. Dziugaite and D. M. Roy. “Data-dependent PAC-Bayes priors via differential privacy”.
In: Advances in Neural Information Processing Systems (NIPS). Vol. 29. Cambridge  MA:
MIT Press  2018. arXiv: 1802.09583.

[11] D. L. Ermak. “A computer simulation of charged particles in solution. I. Technique and equi-

librium properties”. The Journal of Chemical Physics 62.10 (1975)  pp. 4189–4196.

[12] V. Feldman and T. Steinke. “Calibrating Noise to Variance in Adaptive Data Analysis”. In:

Conference On Learning Theory. 2018  pp. 535–544.

[13] S. B. Gelfand and S. K. Mitter. “Recursive stochastic algorithms for global optimization in

Rˆd”. SIAM Journal on Control and Optimization 29.5 (1991)  pp. 999–1018.

[14] M. Hardt  B. Recht  and Y. Singer. “Train faster  generalize better: Stability of stochastic
gradient descent”. In: International Conference on Machine Learning. 2016. arXiv: 1509.
01240.

[15] A. E. I. Ibrahim and M. Gastpar. Strengthened Information-theoretic Bounds on the General-

ization Error. 2019. arXiv: 1903.03787.
J. Jiao  Y. Han  and T. Weissman. “Dependence measures bounding the exploration bias for
general measurements”. In: IEEE International Symposium on Information Theory. 2017.

[16]

[17] O. Kallenberg. Foundations of modern probability. Springer Science & Business Media 

2006.
J. Kemperman. “On the Shannon capacity of an arbitrary channel”. In: Indagationes Mathe-
maticae (Proceedings). Vol. 77. 2. North-Holland. 1974  pp. 101–115.

[18]

[19] Y. LeCun  C. Cortes  and C. J. C. Burges. MNIST handwritten digit database.

http://yann.lecun.com/exdb/mnist/. 2010.
J. Li  X. Luo  and M. Qiao. On generalization error bounds of noisy gradient methods for
non-convex learning. 2019. arXiv: 1902.00621.

[20]

[21] D. A. McAllester. “Some PAC-Bayesian Theorems”. Machine Learning 37.3 (Dec. 1999) 

pp. 355–363. ISSN: 1573-0565.

[22] W. Mou  L. Wang  X. Zhai  and K. Zheng. “Generalization Bounds of SGLD for Non-convex
Learning: Two Theoretical Viewpoints”. In: Proceedings of the 31st Conference On Learn-
ing Theory. Ed. by S. Bubeck  V. Perchet  and P. Rigollet. Vol. 75. Proceedings of Machine
Learning Research. PMLR  June 2018  pp. 605–638.

[23] E. Parrado-Hernández  A. Ambroladze  J. Shawe-Taylor  and S. Sun. “PAC-Bayes bounds
with data dependent priors”. Journal of Machine Learning Research 13.Dec (2012)  pp. 3507–
3531.

10

[24] A. Pensia  V. Jog  and P.-L. Loh. “Generalization error bounds for noisy  iterative algorithms”.

In: 2018 IEEE International Symposium on Information Theory (ISIT). 2018  pp. 546–550.

[25] B. Poole  S. Ozair  A. v. d. Oord  A. A. Alemi  and G. Tucker. “On variational bounds of

mutual information” (2019). arXiv: 1905.06922.

[26] M. Raginsky  A. Rakhlin  and M. Telgarsky. “Non-convex learning via Stochastic Gradient
Langevin Dynamics: a nonasymptotic analysis”. In: Proc. Conference on Learning Theory
(COLT). 2017. arXiv: 1702.03849.

[27] M. Raginsky  A. Rakhlin  M. Tsao  Y. Wu  and A. Xu. “Information-theoretic analysis of sta-
bility and bias of learning algorithms”. In: 2016 IEEE Information Theory Workshop (ITW).
IEEE. 2016  pp. 26–30.

[28] O. Rivasplata  C. Szepesvari  J. S. Shawe-Taylor  E. Parrado-Hernandez  and S. Sun. “PAC-
Bayes bounds for stable algorithms with instance-dependent priors”. In: Advances in Neural
Information Processing Systems. 2018  pp. 9214–9224.

[29] D. Russo and J. Zou. How much does your data exploration overﬁt? Controlling bias via

information usage. 2015. arXiv: 1511.05219.

[30] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algo-

[31]

rithms. Cambridge university press  2014.
J. Shawe-Taylor and R. C. Williamson. “A PAC analysis of a Bayesian estimator”. In: Pro-
ceedings of the tenth annual conference on Computational learning theory. ACM. 1997 
pp. 2–9.

[32] V. Thomas  F. Pedregosa  B. van Merriënboer  P.-A. Mangazol  Y. Bengio  and N. L. Roux.

“Information matrices and generalization”. arXiv preprint arXiv:1906.07774 (2019).

[33] Y.-X. Wang  J. Lei  and S. E. Fienberg. “On-Average KL-Privacy and Its Equivalence to
Generalization for Max-Entropy Mechanisms”. In: Privacy in Statistical Databases. Ed. by J.
Domingo-Ferrer and M. Peji´c-Bach. Cham: Springer International Publishing  2016  pp. 121–
134. ISBN: 978-3-319-45381-1.

[34] M. Welling and Y. W. Teh. “Bayesian learning via stochastic gradient Langevin dynamics”.
In: Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011 
pp. 681–688.

[35] A. Xu and M. Raginsky. “Information-theoretic analysis of generalization capability of learn-
ing algorithms”. In: Advances in Neural Information Processing Systems. 2017  pp. 2524–
2533.

11

,Jeffrey Negrea
Mahdi Haghifam
Gintare Karolina Dziugaite
Ashish Khisti
Daniel Roy