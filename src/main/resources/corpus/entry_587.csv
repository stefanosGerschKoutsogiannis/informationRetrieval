2019,Approximation Ratios of Graph Neural Networks for Combinatorial Problems,In this paper  from a theoretical perspective  we study how powerful graph neural networks (GNNs) can be for learning approximation algorithms for combinatorial problems. 
To this end  we first establish a new class of GNNs that can solve a strictly wider variety of problems than existing GNNs. Then  we bridge the gap between GNN theory and the theory of distributed local algorithms. We theoretically demonstrate that the most powerful GNN can learn approximation algorithms for the minimum dominating set problem and the minimum vertex cover problem with some approximation ratios with the aid of the theory of distributed local algorithms. We also show that most of the existing GNNs such as GIN  GAT  GCN  and GraphSAGE cannot perform better than with these ratios. This paper is the first to elucidate approximation ratios of GNNs for combinatorial problems. Furthermore  we prove that adding coloring or weak-coloring to each node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capabilities.,Approximation Ratios of Graph Neural Networks for

Combinatorial Problems

Ryoma Sato1 2 Makoto Yamada1 2 3 Hisashi Kashima1 2

3JST PRESTO

1Kyoto University

2RIKEN AIP

{r.sato@ml.ist.i  myamada@i  kashima@i}.kyoto-u.ac.jp

Abstract

In this paper  from a theoretical perspective  we study how powerful graph neural
networks (GNNs) can be for learning approximation algorithms for combinatorial
problems. To this end  we ﬁrst establish a new class of GNNs that can solve
a strictly wider variety of problems than existing GNNs. Then  we bridge the
gap between GNN theory and the theory of distributed local algorithms. We
theoretically demonstrate that the most powerful GNN can learn approximation
algorithms for the minimum dominating set problem and the minimum vertex cover
problem with some approximation ratios with the aid of the theory of distributed
local algorithms. We also show that most of the existing GNNs such as GIN  GAT 
GCN  and GraphSAGE cannot perform better than with these ratios. This paper
is the ﬁrst to elucidate approximation ratios of GNNs for combinatorial problems.
Furthermore  we prove that adding coloring or weak-coloring to each node feature
improves these approximation ratios. This indicates that preprocessing and feature
engineering theoretically strengthen model capabilities.

1

Introduction

Graph neural networks (GNNs) [8  9  12  22] is a novel machine learning method for graph structures.
GNNs have achieved state-of-the-art performance in various tasks  including chemo-informatics [7] 
question answering systems [23]  and recommendation systems [31]  to name a few.
Recently  machine learning methods have been applied to combinatorial problems [4  11  16  27] to
automatically obtain novel and efﬁcient algorithms. Xu et al. [30] analyzed the capability of GNNs
for solving the graph isomorphism problem  and they found that GNNs cannot solve it but they are as
powerful as the Weisfeiler-Lehman graph isomorphism test.
The minimum dominating set problem  minimum vertex cover problem  and maximum matching
problem are examples of important combinatorial problems other than the graph isomorphism
problem. These problems are all NP-hard. Therefore  under the assumption that P �= NP  GNNs
cannot exactly solve these problems because they run in polynomial time with respect to input size.
For NP-hard problems  many approximation algorithms have been proposed to obtain sub-optimal
solutions in polynomial time [25]  and approximation ratios of these algorithms have been studied to
guarantee the performance of these algorithms.
In this paper  we study the approximation ratios of algorithms that GNNs can learn for combinatorial
problems. To analyze the approximation ratios of GNNs  we bridge the gap between GNN theory and
the theory of distributed local algorithms. Here  distributed local algorithms are distributed algorithms
that use only a constant number of synchronous communication rounds [1  10  24]. Thanks to their
relationship with distributed local algorithms  we can elucidate the lower bound of the approximation
ratios of algorithms that GNNs can learn for combinatorial problems. As an example of our results  if
the input feature of each node is the node degree alone  no GNN can solve (Δ + 1− ε)-approximation

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

for the minimum dominating set problem or (2 − ε)-approximation for the minimum vertex cover
problem  where ε > 0 is any real number and Δ is the maximum node degree.
In addition  thanks to this relationship  we ﬁnd vector-vector consistent GNNs (VVC-GNNs)  which
are a novel class of GNNs. VVC-GNNs have strictly stronger capability than existing GNNs and
have the same capability as a computational model of distributed local algorithms. Based on our
key ﬁnding  we propose the consistent port numbering GNNs (CPNGNNs)  which is the most
powerful GNN model among VVC-GNNs. That is  for any graph problem that a VVC-GNN can
solve  there exists a parameter of CPNGNNs that can also solve it. Interestingly  CPNGNNs are
strictly more powerful than graph isomorphism networks (GIN)  which were considered to be the
most powerful GNNs [30]. Furthermore  CPNGNNs achieve optimal approximation ratios among
GNNs: CPNGNNs can solve (Δ + 1)-approximation for the minimum dominating set problem and
2-approximation for the minimum vertex cover problem.
However  these approximation ratios are unsatisfactory because they are as high as those of simple
greedy algorithms. One of the reasons for these high approximation ratios is that we only use node
degrees as node features. We show that adding coloring or weak coloring to each node feature
strengthens the capability of GNNs. For example  if we use weak 2-coloring as a node feature in
addition to node degree  CPNGNNs can solve ( Δ+1
2 )-approximation for the minimum dominating
set problem. Considering that any graph has weak 2-coloring and that we can easily calculate
weak 2-coloring in linear time  it is interesting that such preprocessing and feature engineering can
theoretically strengthen the model capability.
The contributions of this paper are summarized as follows:

• We reveal the relationships between the theory of GNNs and distributed local algorithms.
Namely  we show that the set of graph problems that GNN classes can solve is the same as
the set of graph problems that distributed local algorithm classes can solve.

• We propose CPNGNNs  which is the most powerful GNN among the proposed GNN class.
• We elucidate the approximation ratios of GNNs for combinatorial problems including the
minimum dominating set problem and the minimum vertex cover problem. This is the ﬁrst
paper to elucidate the approximation ratios of GNNs for combinatorial problems.

2 Related Work

2.1 Graph Neural Networks

GNNs were ﬁrst introduced by Gori et al. [8] and Scarselli et al. [22]. They obtained the node
embedding by recursively applying the propagation function until convergence. Recently  Kipf
and Welling [12] proposed graph convolutional networks (GCN)  which signiﬁcantly outperformed
existing methods  including non-neural network-based approaches. Since then  many graph neural
networks have been proposed  such as GraphSAGE [9] and the graph attention networks (GATs) [26].
Vinyals et al. [27] proposed pointer networks  which can solve combinatorial problems on a plane 
such as the convex hull problem and the traveling salesman problem. Bello et al. [4] trained pointer
networks using reinforcement learning to automatically obtain novel algorithms for these problems.
Note that pointer networks are not GNNs. However  we introduce them here because they were the
ﬁrst to solve combinatorial problems using deep learning. Khalil et al. [11] and Li et al. [16] used
GNNs to solve combinatorial problems. They utilized search methods with GNNs  whereas we use
only GNNs to focus on the capability of GNNs.
Xu et al. [30] analyzed the capability of GNNs. They showed that GNNs cannot solve the graph
isomorphism problem and that the capability of GNNs is at most the same as that of the Weisfeiler-
Lehman graph isomorphism test. They also proposed the graph isomorphism networks (GIN) 
which are as powerful as the Weisfeiler-Lehman graph isomorphism test. Therefore  the GIN is
the most powerful GNNs. The motivation of this paper is the same as that of Xu et al.’s work [30]
but we consider not only the graph isomorphism problem but also the minimum dominating set
problem  minimum vertex cover problem  and maximum matching problem. Furthermore  we ﬁnd
the approximation ratios of these problems for the ﬁrst time and propose GNNs more powerful than
GIN.

2

Algorithm 1 Calculating the embedding of a node using GNNs
Require: Graph G = (V  E  X); Parameters θ; Aggregation function f (l)
Ensure: Embedding of nodes z ∈ Rn×dL+1
1: z(1)
v ← xv (∀v ∈ V )
2: for l = 1  . . .   L do
for v ∈ V do
3:
z(l+1)
v ← f (l)
4:
end for
5:
6: end for
7: return z(L+1)

θ (aggregated information from neighbor nodes of v)

θ (l = 1  . . .   L).

2.2 Distributed Local Algorithms

A distributed local algorithm is a distributed algorithm that runs in constant time. More speciﬁcally  in
a distributed local algorithm  we assume each node has inﬁnite computational resources and decides
the output within a constant number of communication rounds with neighboring nodes. For example 
distributed local algorithms are used for controlling wireless sensor networks [13]  constructing
self-stabilization algorithms [14  18]  and building sublinear-time algorithms [20].
Distributed local algorithms were ﬁrst studied by Angluin [1]  Linial [17]  and Naor and Stockmeyer
[18]. Angluin [1] showed that deterministic distributed algorithms cannot ﬁnd a center of a graph
without any unique node identiﬁers. Linial [17] showed that no distributed local algorithms can solve
3-coloring of cycles  and they require Ω(log∗ n) communication rounds for distributed algorithms to
solve the problem. Naor and Stockmeyer [18] showed positive results for distributed local algorithms
for the ﬁrst time. For example  distributed local algorithms can ﬁnd weak 2-coloring and solve a
variant of the dining philosophers problem. Later  several non-trivial distributed local algorithms
were found  including 2-approximation for the minimum vertex cover problem [2].
There are many computational models of distributed local algorithms. Some computational models
use unique identiﬁers of nodes [18]  port numbering [1]  and randomness [19  28]  and other models
do not [10]. Furthermore  some results use the following assumptions about the input: degrees are
bounded [2]  degrees are odd [18]  graphs are planar [6]  and graphs are bipartite [3]. In this paper 
we do not use any unique identiﬁers nor randomness  but we do use port numbering  and we assume
the degrees are bounded. We describe our assumptions in detail in Section 3.1.

3 Preliminaries

3.1 Problem Setting

Here  we ﬁrst describe the notation used in this paper and then we formulate the graph problem.
Notation. For a positive integer k ∈ Z+  let [k] be the set {1  2  . . .   k}. Let G = (V  E  X) be a
input graph  where V is a set of nodes  E is a set of edges  and X ∈ R|V |×d0 is a feature matrix. We
represent an edge of a graph G = (V  E  X) as an unordered pair {u  v} with u  v ∈ V . We write
n = |V | for the number of nodes and m = |E| for the number of edges. The nodes V are considered
to be numbered with [n]. (i.e.  we assume V = [n].) For a node v ∈ V   deg(u) denotes the degree of
node v and N (v) denotes the set of neighbors of node v.
A GNN model Nθ(G  v) is a function parameterized by θ that takes a graph G and a node v ∈
V as input and output the label yv ∈ Y of node v  where Y is a set of labels. We study the
expression capability of the function family Nθ for combinatorial graph problems with the following
assumptions.

Assumption 1 (Bounded-Degree Graphs). In this paper  we consider only bounded-degree graphs.
In other words  for a ﬁxed (but arbitrary) constant Δ  we assume that the degree of each node of
the input graphs is at most Δ. This assumption is natural because there are many bounded-degree
graphs in the real world. For example  degrees in molecular graphs are bounded by four  and the
degrees in computer networks are bounded by the number of LAN ports of routers. Moreover  the

3

bounded-degree assumption is often used in distributed local algorithms [17  18  24]. For each
positive integer Δ ∈ Z+  let F(Δ) be the set of all graphs with maximum degrees of Δ at most.
Assumption 2 (Node Features). We do not consider node features other than those that can be
derived from the input graph itself for focusing on graph theoretic properties. When there are no node
features available  the degrees of nodes are sometimes used [9  21  30]. Therefore  we use only the
degree of a node as the node feature (i.e.  z(1)
v = ONEHOT(deg(v))) unless speciﬁed. Later  we
show that using coloring or weak coloring of the input graph in addition to degrees of nodes as node
features makes models theoretically more powerful.

Graph Problems. A graph problem is a function Π that associates a set Π(G) of solutions with
each graph G = (V  E). Each solution S ∈ Π(G) is a function S : V → Y . Y is a ﬁnite set that is
independent of G. We say a GNN model Nθ solves a graph problem Π if for any Δ ∈ Z+  there
exists a parameter θ such that for any graph G ∈ F(Δ)  Nθ(G ·) is in Π(G). For example  let Y
be a set of labels of nodes  let L(G) : V → Y be the ground truth of a multi-label classiﬁcation
problem for a graph G (i.e.  L(G)(v) denotes the ground truth label of node v ∈ V )  and let Π(G) =
{f : V → {0  1} | |{v ∈ V | f (v) = L(G)(v)}| ≥ 0.9 · |V |}. This graph problem Π corresponds to
a multi-label classiﬁcation problem. A GNN model Nθ solves Π means there exists a parameter θ of
the model such that achieves an accuracy 0.9 for this problem. Other examples of graph problems are
combinatorial problems. Let C(G) ⊂ V be the minimum vertex cover of a graph G  let Y = {0  1} 
and let Π(G) = {f : V → {0  1} | D = {v | f (v) = 1} is a vertex cover and |D| ≤ 2 · |C(G)|}.
This graph problem Π corresponds to 2-approximation for the minimum vertex cover problem.

3.2 Known Model Classes

We introduce two known classes of GNNs  which include GraphSAGE [9]  GCN [12]  GAT [26] 
and GIN [30].

MB-GNNs. A layer of an existing GNN can be written as

z(l+1)
v

= f (l)

θ (z(l)

v   MULTISET(z(l)

u | u ∈ N (v))) 

where f (l)
θ is a learnable aggregation function. We call GNNs that can be written in this form multiset-
broadcasting GNNs (MB-GNNs) — multiset because they aggregate features from neighbors as a
multiset and broadcasting because for any v ∈ N (u)  the “message” [7] from u to v is the same (i.e. 
zu). GraphSAGE-mean [9] is an example of MB-GNNs because a layer of GraphSAGE-mean is
represented by the following equation:

z(l+1)
v

= CONCAT(z(l)
v  

W (l)z(l)

u ) 

1

|N (v)| �u∈N (v)

where CONCAT concatenates vectors into one vector. Other examples of MB-GNNs are GCN [12] 
GAT [26]  and GIN [30].

SB-GNNs. The another existing class of GNNs in the literature is set-broadcasting GNNs (SB-GNNs) 
which can be written as the following form:

GraphSAGE-pool [9] is an example of SB-GNNs because a layer of GraphSAGE-mean is represented
by the following equation:

z(l+1)
v

= f (l)

θ (z(l)

v   SET(z(l)

u | u ∈ N (v))).

z(l+1)
v

= max({σ(W (l)z(l)

u + b(l)) | u ∈ N (v)}).

Clearly  SB-GNNs are a subclass of MB-GNNs. Xu et al. [30] discussed the differences in capability
of SB-GNNs and MB-GNNs. We show that MB-GNNs are strictly stronger than SB-GNNs in another
way in this paper.

4 Novel Class of GNNs

In this section  we ﬁrst introduce a GNN class that is more powerful than MB-GNNs and SB-GNNs.
To make GNN models more powerful than MB-GNNs  we introduce the concept of port numbering
[1  10] to GNNs.

4

Port Numbering. A port of a graph G is a pair (v  i)  where v ∈ V and i ∈ [deg(v)]. Let
P (G) = {(v  i) | v ∈ V  i ∈ [deg(v)]} be the set of all ports of a graph G. A port numbering of a
graph G is the function p : P (G) → P (G) such that for any edge {u  v}  there exist i ∈ [deg(u)] and
j ∈ [deg(v)] such that p(u  i) = (v  j). We say that a port numbering is consistent if p is an involution
(i.e.  ∀(v  i) ∈ P (G) p(p(v  i)) = (v  i)). We deﬁne the functions ptail : V × Δ → V ∪ {−} and
pn : V × Δ → Δ ∪ {−} as follows:

ptail(v  i) =�u ∈ V (∃j ∈ [deg(u)] s.t. p(u  j) = (v  i))
pn(v  i) =�j ∈ [deg(ptail(v  i))] (p(ptail(v  i)  j) = (v  i))

−

−

(i ≤ deg(v))
(otherwise) 
(i ≤ deg(v))
(otherwise) 

where − is a special symbol that denotes the index being out of range. Note that these functions are
well-deﬁned because there always exists only one u ∈ V for ptail and j ∈ [deg(ptail(v  i))] for pn if
i ≤ deg(v). Intuitively  ptail(v  i) represents the node that sends messages to the port i of node v and
pn(v  i) represents the port number of the node ptail(v  i) that sends messages to the port i of node v.
The GNN class we introduce in the following uses a consistent port numbering to calculate embed-
dings. Intuitively  SB-GNNs and MB-GNNs send the same message to all neighboring nodes. GNNs
can send different messages to neighboring nodes by using port numbering  and this strengthens
model capability.

VVC-GNNs. Vector-vector consistent GNNs (VVC-GNNs) are a novel class of GNNs that we
introduce in this paper. They calculate an embedding with the following formula:

z(l+1)
v

= f (l)

θ (z(l)

v   z(l)

ptail(v 1)  pn(v  1)  z(l)

ptail(v 2)  pn(v  2)  . . .   z(l)

ptail(v Δ)  pn(v  Δ)).

If the index of z is the special symbol −  we also deﬁne the embedding as the special symbol −
(i.e.  z− = −). To calculate embeddings of nodes of a graph G using a GNN with port numbering 
we ﬁrst calculate one consistent port numbering p of G  and then we input G and p to the GNN.
Note that we can calculate a consistent port numbering of a graph in linear time by numbering
edges one by one. We say a GNN class N with port numbering solves a graph problem Π if for
any Δ ∈ Z+  there exists a GNN Nθ ∈ N and its parameter θ such that for any graph G ∈ F(Δ) 
for any consistent port numbering p of G  the output Nθ(G  p ·) is in Π(G). We show that using
port numbering theoretically improves model capability in Section 5.2. We propose CPNGNNs  an
example of VVC-GNNs  in Section 6.

5 GNNs with Distributed Local Algorithms

In this section  we discuss the relationship between GNNs and distributed local algorithms. Thanks
to this relationship  we can elucidate the theoretical properties of GNNs.

5.1 Relationship with Distributed Local Algorithms

A distributed local algorithm is a distributed algorithm that runs in constant time. More speciﬁcally  in
a distributed local algorithm  we assume each node has inﬁnite computational resources and decides
the output within a constant number of communication rounds with neighboring nodes. In this paper 
we show a clear relationship between distributed local algorithms and GNNs for the ﬁrst time.
There are several well-known models of distributed local algorithms [10]. Namely  in this paper 
we introduce the SB(1)  MB(1)  and VVC(1) models. As their names suggest  they correspond to
SB-GNNs  MB-GNNs  and VVC-GNNs  respectively.
Assumption 3 (Finite Node Features): The number of possible node features is ﬁnite.
Assumption 3 restricts node features be discrete. However  Assumption 3 does include the node
degree feature (∈ [Δ]) and node coloring feature (∈ {0  1}).
Theorem 1. Let L be SB  MB  or VVC. Under Assumption 3  the set of graph problems that at least
one L-GNN can solve is the same as the set of graph problems that at least one distributed local
algorithm on the L(1) model solve.

5

Rdl+1×(dl+Δ(dl+1))(l = 1  . . .   L).

Algorithm 2 CPNGNN: The most powerful VVC-GNN
Require: Graph G = (V  E  X); Maximum degree Δ ∈ Z+; Weight matrix W (l) ∈
Ensure: Output for the graph problem y ∈ Y n
1: calculate a consistent port numbering p
2: z(1)
v ← xv (∀v ∈ V )
3: for l = 1  . . .   L do
for v ∈ V do
4:
5:

v   z(l)

ptail(v 1)  pn(v  1)  z(l)

ptail(v 2)  pn(v  2)  . . .   z(l)

ptail(v Δ)  pn(v  Δ))

z(l+1)
v ← W (l) CONCAT(z(l)
z(l+1)
v ← RELU(z(l+1)
end for

)

v

6:
7:
8: end for
9: for v ∈ V do
10:
11:
12: end for
13: return y

zv ← MULTILAYERPERCEPTRON(z(L+1)
yv ← argmaxi∈[dL+1]zvi

v

)

# calculate the ﬁnal embedding of a node v.
# output the index of the maximum element.

All proofs are available in the supplementary materials. In fact  the following stronger properties
hold: (i) any L-GNN can be simulated by the L(1) model and (ii) any distributed local algorithm on
L(1) model can be simulated by an L-GNN. The former is obvious because GNNs communicate
with neighboring nodes in L rounds  where L is the number of layers. The latter is natural because
the deﬁnition of L-GNNs (Section 3.2 and 4) is intrinsically the same as the deﬁnition of the L(1)
model. Thanks to Theorem 1  we can prove which combinatorial problems GNNs can/cannot solve
by using theoretical results on distributed local algorithms.

5.2 Hierarchy of GNNs

There are obvious inclusion relations among classes of GNNs. Namely  SB-GNNs are a subclass of
MB-GNNs  and MB-GNNs are a subclass of VVC-GNNs. If a model class A is a subset of a model
class B  the graph problems that A solves is a subset of the graph problems that B solves. However 
it is not obvious whether the proper inclusion property holds or not. Let PSB-GNNs  PMB-GNNs  and
PVVC-GNNs be the sets of graph problems that SB-GNNs  MB-GNNs  and VVC-GNNs can solve only
with the degree features  respectively. Thanks to the relationship between GNNs and distributed local
algorithms  we can show that the proper inclusion properties of these classes hold.
Theorem 2. PSB-GNNs � PMB-GNNs � PVVC-GNNs.
An example graph problem that MB-GNNs cannot solve but VVC-GNNs can solve is the ﬁnding
single leaf problem [10]. The input graphs of the problem are star graphs and the ground truth contains
only a single leaf node. MB-GNNs cannot solve this problem because for each layer  the embeddings
of the leaf nodes are exactly same  and the GNN cannot distinguish these nodes. Therefore  if a GNN
includes one leaf node in the output  the other leaf nodes are also included to the output. On the other
hand  VVC-GNNs can distinguish each leaf node using port numbering and can appropriately output
only a single node. We conﬁrm this fact through experiments in the supplementary materials.

6 Most Powerful GNN for Combinatorial Problems

6.1 Consistent Port Numbering Graph Neural Networks (CPNGNNs)

In this section  we propose the most powerful VVC-GNNs  CPNGNNs. The most similar algorithm
to CPNGNNs is GraphSAGE [9]. The key differences between GraphSAGE and CPNGNNs are as
follows: (i) CPNGNNs use port numbering and (ii) GPNGNNs aggregate features of neighbors by
concatenation. We show pseudo code of CPNGNNs in Algorithm 2. Though CPNGNNs are simple 
they are the most powerful among VVC-GNNs. This claim is supported by Theorem 3  where we do
not limit node features to the node degree feature.

6

Theorem 3. Let PCPNGNNs be the set of graph problems that CPNGNNs can solve and PVVC-GNNs
be the set of graph problems that VVC-GNNs can solve. Then  under Appsumtion 3  PCPNGNNs =
PVVC-GNNs.
The advantages of CPNGNNs are twofold: they can solve a strictly wider set of graph problems than
existing models (Theorem 2 and 3). There are many distributed local algorithms that can be simulated
by CPNGNNs and we can prove that CPNGNNs can solve a variety of combinatorial problems (see
Section 6.2).

6.2 Combinatorial Problems that CPNGNNs Can/Cannot Solve

In Section 5.2  we found that there exist graph problems that certain GNNs can solve but others cannot.
However  there remains a question. What kind of graph problems can/cannot GNNs solve? In this
paper  we study combinatorial problems  including the minimum dominating set problem  maximum
matching problem  and minimum vertex cover problem. If GNNs can solve combinatorial problems 
we may automatically obtain new algorithms for combinatorial problems by simply training GNNs.
Note that from Theorems 2 and 3  if CPNGNNs cannot solve a graph problem  other GNNs cannot
solve the problem. Therefore  it is important to investigate the capability of GPNGNNs to study the
limitations of GNNs.

Minimum Dominating Set Problem. First  we investigate the minimum dominating set problem.
Theorem 4. The optimal approximation ratio of CPNGNNs for the minimum dominating set problem
is (Δ + 1). In other words  CPNGNNs can solve (Δ + 1)-approximation for the minimum dominating
set problem  but for any 1 ≤ α < Δ + 1  CPNGNNs cannot solve α-approximation for the minimum
dominating set problem.

Here  CPNGNNs can solve f (Δ) approximation for the minimum dominating set problem means
that for all Δ ∈ Z+  there exists a paramter θ such that for all input G ∈ F(Δ)  {v ∈ V |
CPNGNNθ(G  v) = 1} forms f (Δ) approximatoin of the minimum dominating set of G. However 
(Δ + 1)-approximation is trivial because it can be achieved by outputting all the nodes. Therefore 
Theorem 4 says that any GNN is as bad as the trivial algorithm in the worst case  which is unsatisfac-
tory. This is possibly because we only use the degree information of local nodes  and we may improve
the approximation ratio if we use information other than node degree. Interestingly  we can improve
the approximation ratio just by using weak 2-coloring as a feature of nodes. A weak 2-coloring is a
function c : V → {0  1} such that for any node v ∈ V   there exists a neighbor u ∈ N (v) such that
c(v) �= c(u). Note that any graph has a weak 2-coloring and that we can calculate a weak 2-coloring
in linear time by a breadth-ﬁrst search. In the theorems below  we use not only the degree deg(v) but
also the color c(v) as a feature vector of a node v ∈ V . There may be many weak 2-colorings of a
graph G. However  the choice of c is arbitrary.
Theorem 5. If the feature vector of a node is consisted of the degree and the color of a weak
2-coloring  the optimal approximation ratio of CPNGNNs for the minimum dominating set problem
is ( Δ+1
2 )-approximation for the minimum dominating
2   CPNGNN cannot solve α-approximation for the minimum
set problem  and for any 1 ≤ α < Δ+1
dominating set problem.

2 ). In other words  CPNGNN can solve ( Δ+1

In the minimum dominating set problem  we cannot improve the approximation ratio by using
2-coloring instead of weak 2-coloring.
Theorem 6. Even if the feature vector of a node is consisted of the degree and the color of a
2-coloring  for any 1 ≤ α < Δ+1
2   CPNGNNs cannot solve α-approximation for the minimum
dominating set problem.

Minimum Vertex Cover Problem. Next  we investigate the minimum vertex cover problem.
Theorem 7. The optimal approximation ratio of CPNGNNs for the minimum vertex cover problem
is 2. In other words  CPNGNNs can solve 2-approximation for the minimum vertex cover problem 
and for any 1 ≤ α < 2  CPNGNNs cannot solve α-approximation for the minimum vertex cover
problem.

The simple greedy algorithm can solve 2-approximation for the minimum vertex cover problem.
However  this result is not trivial because the algorithm that GNNs learn is not a regular algorithm but

7

a distributed local algorithm. The distributed local algorithm for 2-approximation for the minimum
vertex cover problem is known but not so simple [2]. This result also says that if one wants to
ﬁnd an approximation algorithm using a machine learning approach with better performance than
2-approximation  they must use a non-GNN model or combine GNNs with other methods (e.g.  a
search method).

Maximum Matching Problem. Lastly  we investigate the maximum matching problem. So far  we
have only investigated problems on nodes  not edges. We must specify how GNNs output edge labels.
Graph edge problems are deﬁned similarly to graph problems  but their solutionas are functions
E → Y . In this paper  we only consider Y = {0  1} and we only use VVC-GNNs for solving graph
edge problems. Let G ∈ F(Δ) be a graph and p be a port numbering of G. To solve graph edge
problems  GNNs output a vector y(v) ∈ {0  1}Δ for each node v ∈ V . For each edge {u  v}  GNNs
include the edge {u  v} in the output if and only if y(u)i = y(v)j = 1  where p(u  i) = (v  j) and
p(v  j) = (u  i). Intuitively  each node outputs “yes” or “no” to each incident edge (i.e.  a port) and
we include an edge in the output if both ends output “yes” to the edge. As with graph problems 
we say a class N of GNNs solves a graph edge problem Π if for any Δ ∈ Z+  there exists a GNN
Nθ ∈ N and its parameter θ such that for any graph G ∈ F(Δ) and any consistent port numbering p
of G  the output Nθ(G  p) is in Π(G).
We investigate the maximum matching problem in detail. In fact  GNNs cannot solve the maximum
matching problem at all.
Theorem 8. For any α ∈ R+  CPNGNNs that cannot solve α-approximation for the maximum
matching problem.

However  CPNGNNs can approximate the maximum matching problem with weak 2-coloring feature.
Theorem 9. If the feature vector of a node is consisted of the degree and the color of a weak 2-
coloring  the optimal approximation ratio of CPNGNNs for the maximum matching problem is ( Δ+1
2 ).
In other words  CPNGNNs can solve ( Δ+1
2 )-approximation for the maximum matching problem 
2   CPNGNNs cannot solve α-approximation for the maximum matching
and for any 1 ≤ α < Δ+1
problem.

Furthermore  if we use 2-coloring instead of weak 2-coloring  we can improve the approximation
ratio. In fact  it can achieve any approximation ratio. Note that only a bipartite graph has 2-coloring.
Therefore  the graph class is implicitly restricted to bipartite graphs in this case.
Theorem 10. If the feature vector of a node is consisted of the degree and the color of a 2-coloring 
for any 1 < α  CPNGNNs can solve α-approximation for the maximum matching problem.

In this paper  we consider only bounded-degree graphs. This assumption is natural  but it is also
important to consider graphs without degree bounds. Dealing with such graphs is difﬁcult because
graph problems on them are not constant size [24]. Note that solving graph problems becomes
more difﬁcult if we do not have the bounded-degree assumption. Therefore  GNNs cannot solve
(Δ + 1 − ε)-approximation for the minimum dominating set problems or (2 − ε)-approximation for
the minimum vertex cover problem in the general case.

7 Conclusion

In this paper  we introduced VVC-GNNs  which are a new class of GNNs  and CPNGNNs  which
are an example of VVC-GNNs. We showed that VVC-GNNs have the same ability to solve graph
problems as a computational model of distributed local algorithms. With the aid of distributed local
algorithm theory  we elucidated the approximation ratios of algorithms that CPNGNNs can learn for
combinatorial graph problems such as the minimum dominating set problem and the minimum vertex
cover problem. This paper is the ﬁrst to show the approximation ratios of GNNs for combinatorial
problems. Moreover  this is a lower bound of approximation ratios for all GNNs. We further showed
that adding coloring or weak coloring to a node feature improves these approximation ratios. This
indicates that preprocessing and feature engineering theoretically strengthen model capability.

8

Acknowledgments
This work was supported by JSPS KAKENHI Grant Number 15H01704. MY is supported by the
JST PRESTO program JPMJPR165A.

References
[1] Dana Angluin. Local and global properties in networks of processors (extended abstract). In
Proceedings of the 12th Annual ACM Symposium on Theory of Computing  pages 82–93  1980.

[2] Matti Åstrand  Patrik Floréen  Valentin Polishchuk  Joel Rybicki  Jukka Suomela  and Jara
Uitto. A local 2-approximation algorithm for the vertex cover problem. In Proceedings of 23rd
International Symposium on Distributed Computing  DISC 2009  pages 191–205  2009.

[3] Matti Åstrand  Valentin Polishchuk  Joel Rybicki  Jukka Suomela  and Jara Uitto. Local

algorithms in (weakly) coloured graphs. CoRR  abs/1002.0125  2010.

[4] Irwan Bello  Hieu Pham  Quoc V. Le  Mohammad Norouzi  and Samy Bengio. Neural combi-

natorial optimization with reinforcement learning. CoRR  abs/1611.09940  2016.

[5] George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS  2(4):303–

314  1989.

[6] Andrzej Czygrinow  Michal Hanckowiak  and Wojciech Wawrzyniak. Fast distributed approx-
imations in planar graphs. In Proceedings of 22nd International Symposium on Distributed
Computing  DISC 2008  pages 78–92  2008.

[7] Justin Gilmer  Samuel S. Schoenholz  Patrick F. Riley  Oriol Vinyals  and George E. Dahl.
In Proceedings of the 34th International

Neural message passing for quantum chemistry.
Conference on Machine Learning  ICML 2017  pages 1263–1272  2017.

[8] Marco Gori  Gabriele Monfardini  and Franco Scarselli. A new model for learning in graph
domains. In Proceedings of the International Joint Conference on Neural Networks  IJCNN
2005  volume 2  pages 729–734  2005.

[9] William L. Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on
large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017  NIPS 2017  pages 1025–1035  2017.

[10] Lauri Hella  Matti Järvisalo  Antti Kuusisto  Juhana Laurinharju  Tuomo Lempiäinen  Kerkko
Luosto  Jukka Suomela  and Jonni Virtema. Weak models of distributed computing  with
connections to modal logic. In Proceedings of the ACM Symposium on Principles of Distributed
Computing  PODC 2012  pages 185–194  2012.

[11] Elias B. Khalil  Hanjun Dai  Yuyu Zhang  Bistra Dilkina  and Le Song. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017  NIPS 2017  pages
6351–6361  2017.

[12] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. CoRR  abs/1609.02907  2016.

[13] Martin Kubisch  Holger Karl  Adam Wolisz  Lizhi Charlie Zhong  and Jan M. Rabaey. Dis-
tributed algorithms for transmission power control in wireless sensor networks. In Proceedings
of the 2003 IEEE Wireless Communications and Networking  WCNC 2003  pages 558–563 
2003.

[14] Christoph Lenzen  Jukka Suomela  and Roger Wattenhofer. Local algorithms: Self-stabilization
on speed. In Proceedings of 11th International Symposium on Stabilization  Safety  and Security
of Distributed Systems  SSS 2009  pages 17–34  2009.

[15] Christoph Lenzen and Roger Wattenhofer. Leveraging linial’s locality limit. In Proceedings of
22nd International Symposium on Distributed Computing  DISC 2008  pages 394–407  2008.

9

[16] Zhuwen Li  Qifeng Chen  and Vladlen Koltun. Combinatorial optimization with graph convolu-
tional networks and guided tree search. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018  NeurIPS 2018  pages
537–546  2018.

[17] Nathan Linial. Locality in distributed graph algorithms. SIAM J. Comput.  21(1):193–201 

1992.

[18] Moni Naor and Larry J. Stockmeyer. What can be computed locally? SIAM J. Comput. 

24(6):1259–1277  1995.

[19] Huy N. Nguyen and Krzysztof Onak. Constant-time approximation algorithms via local
improvements. In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer
Science  FOCS  pages 327–336  2008.

[20] Michal Parnas and Dana Ron. Approximating the minimum vertex cover in sublinear time and

a connection to distributed algorithms. Theor. Comput. Sci.  381(1-3):183–196  2007.

[21] Leonardo Filipe Rodrigues Ribeiro  Pedro H. P. Saverese  and Daniel R. Figueiredo. struc2vec:
Learning node representations from structural identity.
In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  KDD 2017 
pages 385–394  2017.

[22] Franco Scarselli  Marco Gori  Ah Chung Tsoi  Markus Hagenbuchner  and Gabriele Monfardini.

The graph neural network model. IEEE Trans. Neural Networks  20(1):61–80  2009.

[23] Michael Sejr Schlichtkrull  Thomas N. Kipf  Peter Bloem  Rianne van den Berg  Ivan Titov 
and Max Welling. Modeling relational data with graph convolutional networks. CoRR 
abs/1703.06103  2017.

[24] Jukka Suomela. Survey of local algorithms. ACM Comput. Surv.  45(2):24:1–24:40  2013.

[25] Vijay V. Vazirani. Approximation algorithms. Springer  2001.

[26] Petar Velickovic  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Liò  and Yoshua
Bengio. Graph attention networks. In Proceedings of the 6th International Conference on
Learning Representations  ICLR 2018  2018.

[27] Oriol Vinyals  Meire Fortunato  and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing
Systems 2015  NIPS 2015  pages 2692–2700  2015.

[28] Mirjam Wattenhofer and Roger Wattenhofer. Distributed weighted matching. In Proceedings of
18th International Symposium on Distributed Computing  DISC 2004  pages 335–348  2004.

[29] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Mach. Learn.  8(3-4):229–256  1992.

[30] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural

networks? CoRR  abs/1810.00826  2018.

[31] Rex Ying  Ruining He  Kaifeng Chen  Pong Eksombatchai  William L. Hamilton  and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems.
In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining  KDD 2018  pages 974–983  2018.

10

,Arturs Backurs
Piotr Indyk
Ludwig Schmidt
Ryoma Sato
Makoto Yamada
Hisashi Kashima