2013,Bayesian inference as iterated random functions with  applications to sequential inference in graphical models,We propose a general formalism of iterated random functions with semigroup property  under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are  illustrated by simulated examples.,Bayesian inference as iterated random functions with

applications to sequential inference in graphical

models

Arash A. Amini

Department of Statistics
University of Michigan

Ann Arbor  Michigan 48109

aaamini@umich.edu

XuanLong Nguyen

Department of Statistics
University of Michigan

Ann Arbor  Michigan 48109
xuanlong@umich.edu

Abstract

We propose a general formalism of iterated random functions with semigroup
property  under which exact and approximate Bayesian posterior updates can be
viewed as speciﬁc instances. A convergence theory for iterated random functions
is presented. As an application of the general theory we analyze convergence
behaviors of exact and approximate message-passing algorithms that arise in a
sequential change point detection problem formulated via a latent variable directed
graphical model. The sequential inference algorithm and its supporting theory are
illustrated by simulated examples.

1 Introduction

The sequential posterior updates play a central role in many Bayesian inference procedures. As an
example  in Bayesian inference one is interested in the posterior probability of variables of interest
given the data observed sequentially up to a given time point. As a more speciﬁc example which
provides the motivation for this work  in a sequential change point detection problem [1]  the key
quantity is the posterior probability that a change has occurred given the data observed up to present
time. When the underlying probability model is complex  e.g.  a large-scale graphical model  the cal-
culation of such quantities in a fast and online manner is a formidable challenge. In such situations
approximate inference methods are required – for graphical models  message-passing variational
inference algorithms present a viable option [2  3].

In this paper we propose to treat Bayesian inference in a complex model as a speciﬁc instance of an
abstract system of iterated random functions (IRF)  a concept that originally arises in the study of
Markov chains and stochastic systems [4]. The key technical property of the proposed IRF formal-
ism that enables the connection to Bayesian inference under conditionally independent sampling is
the semigroup property  which shall be deﬁned shortly in the sequel. It turns out that most exact and
approximate Bayesian inference algorithms may be viewed as speciﬁc instances of an IRF system.
The goal of this paper is to present a general convergence theory for the IRF with semigroup prop-
erty. The theory is then applied to the analysis of exact and approximate message-passing inference
algorithms  which arise in the context of distributed sequential change point problems using latent
variable and directed graphical model as the underlying modeling framework.

We wish to note a growing literature on message-passing and sequential inference based on graph-
ical modeling [5  6  7  8]. On the other hand  convergence and error analysis of message-passing
algorithms in graphical models is quite rare and challenging  especially for approximate algorithms 
and they are typically conﬁned to the speciﬁc form of belief propagation (sum-product) algorithm
[9  10  11]. To the best of our knowledge  there is no existing work on the analysis of message-
passing inference algorithms for calculating conditional (posterior) probabilities for latent random

1

variables present in a graphical model. While such an analysis is a byproduct of this work  the view-
point we put forward here that equates Bayesian posterior updates to a system of iterated random
functions with semigroup property seems to be new and may be of general interest.

In Sections 2– 3  we introduce the general IRF system and
The paper is organized as follows.
provide our main result on its convergence. The proof is deferred to Section 5. As an example of
the application of the result  we will provide a convergence analysis for an approximate sequential
inference algorithm for the problem of multiple change point detection using graphical models. The
problem setup and the results are discussed in Section 4.

2 Bayesian posterior updates as iterated random functions

In this paper we shall restrict ourselves to multivariate distributions of binary random variables.
To describe the general iteration  let Pd := P({0  1}d) be the space of probability measures on
{0  1}d. The iteration under consideration recursively produces a random sequence of elements of
Pd  starting from some initial value. We think of Pd as a subset of R2d
equipped with the ℓ1 norm
(that is  the total variation norm for discrete probability measures). To simplify  let m := 2d  and
for x ∈ Pd  index its coordinates as x = (x0  . . .   xm−1). For θ ∈ Rm
+   consider the function
qθ : Pd → Pd  deﬁned by

qθ(x) :=

x ⊙ θ
xT θ

(1)

where xT θ = Pi xiθi is the usual inner product on Rm and x ⊙ θ is pointwise multiplication

with coordinates [x ⊙ θ]i := xiθi  for i = 0  1  . . .   m − 1. This function models the prior-to-
posterior update according to the Bayes rule. One can think of θ as the likelihood and x as the prior
distribution (or the posterior in the previous stage) and qθ(x) as the (new) posterior based on the two.
The division by xT θ can be thought of as the division by the marginal to make a valid probability
vector. (See Example 1 below.)

We consider the following general iteration

Qn(x) = qθn(T (Qn−1(x))  n ≥ 1 
Q0(x) = x 

(2)

for some deterministic operator T : Pd → Pd and an i.i.d. random sequence {θn}n≥1 ⊂ Rm
changing operator T   one obtains different iterative algorithms.

+ . By

Our goal is to ﬁnd sufﬁcient conditions on T and {θn} for the convergence of the iteration to an
extreme point of Pd  which without loss of generality is taken to be e(0) := (1  0  0  . . .   0). Standard
techniques for proving the convergence of iterated random functions are usually based on showing
some averaged-sense contraction property for the iteration function [4  12  13  14]  which in our
case is qθn (T (·)). See [15] for a recent survey. These techniques are not applicable to our problem
since qθn is not in general Lipschitz  in any suitable sense  precluding qθn (T (·)) from satisfying the
aforementioned conditions.

Instead  the functions {qθn} have another property which can be exploited to prove convergence;
namely  they form a semi-group under pointwise multiplication 

qθ⊙ θ′ = qθ ◦ qθ′  

θ  θ′ ∈ Rm
+  

(3)

i=1

where ◦ denotes the composition of functions. If T is the identity  this property allows us to write
θi(x) — this is nothing but the Bayesian posterior update equation  under condi-
Qn(x) = q⊙ n
tionally independent sampling  while modifying T results in an approximate Bayesian inference
procedure. Since after suitable normalization  ⊙ n
θi concentrates around a deterministic quantity 
by the i.i.d. assumption on {θi}  this representation helps in determining the limit of {Qn(x)}. The
main result of this paper  summarized in Theorem 1  is that the same conclusions can be extended
to general Lipschitz maps T having the desired ﬁxed point.

i=1

2

3 General convergence theory

Consider a sequence {θn}n≥1 ⊂ Rm
(θ0

n  . . .   θm−1

n  θ1

+ of i.i.d.

) with θ0

n = 1 for all n  and

n

random elements  where m = 2d. Let θn =

θ∗
n :=

max

i=1 2 ... m−1

θi
n.

(4)

n = 1 is convenient for showing convergence to e(0). This is without loss of

The normalization θ0
generality  since qθ is invariant to scaling of θ  that is qθ = qβθ for any β > 0.
Assume the sequence {log θ∗
n} to be i.i.d. sub-Gaussian with mean ≤ −I∗ < 0 and sub-Gaussian
norm ≤ σ∗ ∈ (0  ∞). The sub-Gaussian norm in can be taken to be the ψ2 Orlicz norm (cf. [16 
Section 2.2])  which we denote by k · kψ2. By deﬁnition kY kψ2 := inf{C > 0 : Eψ2(|Y |/C) ≤ 1}
where ψ2(x) := ex2
Let k · k denote the ℓ1 norm on Rm. Consider the sequence {Qn(x)}n≥0 deﬁned in (2) based on
{θn} as above  an initial point x = (x0  . . .   xm−1) ∈ Pd and a Lipschitz map T : Pd → Pd. Let
LipT denote the Lipschitz constant of T   that is LipT := supx6=y kT (x) − T (y)k/kx − yk.

− 1.

Our main result regarding iteration (2) is the following.
Theorem 1. Assume that L := LipT ≤ 1 and that e(0) is a ﬁxed point of T . Then  for all n ≥ 0 
and ε > 0 

kQn(x) − e(0)k ≤ 2

1 − x0

x0

(cid:0)Le−I∗+ε(cid:1)n

(5)

with probability at least 1 − exp(−c nε2/σ2

∗)  for some absolute constant c > 0.

The proof of Theorem 1 is outlined in Section 5. Our main application of the theorem will be to the
study of convergence of stopping rules for a distributed multiple change point problem endowed with
latent variable graphical models. Before stating that problem  let us consider the classical (single)
change point problem ﬁrst  and show how the theorem can be applied to analyze the convergence of
the optimal Bayes rule.

Example 1.
In the classical Bayesian change point problem [1]  one observes a sequence
{X 1  X 2  X 3 . . . } of independent data points whose distributions change at some random time
λ. More precisely  given λ = k  X 1  X 2  . . .   X k−1 are distributed according to g  and
X k+1  X k+2  . . . according to f . Here  f and g are densities with respect to some underlying
measure. One also assumes a prior π on λ  usually taken to be geometric. The goal is to ﬁnd a
stopping rule τ which can predict λ based on the data points observed so far. It is well-known
that a rule based on thresholding the posterior probability of λ is optimal (in a Neyman-Pearson
sense). To be more speciﬁc  let Xn := (X 1  X 2  . . .   X n) collect the data up to time n and let
γn[n] := P(λ ≤ n|Xn) be the posterior probability of λ having occurred before (or at) time n.
Then  the Shiryayev rule

τ := inf{n ∈ N : γn[n] ≥ 1 − α}

(6)

is known to asymptotically have the least expected delay  among all stopping rules with false alarm
probability bounded by α.

Theorem 1 provides a way to quantify how fast the posterior γn[n] approaches 1  once the change
point has occurred  hence providing an estimate of the detection delay  even for ﬁnite number of
samples. We should note that our approach here is somewhat independent of the classical techniques
normally used for analyzing stopping rule (6). To cast the problem in the general framework of (2) 
let us introduce the binary variable Z n := 1{λ ≤ n}  where 1{·} denotes the indicator of an event.
Let Qn be the (random) distribution of Z n given Xn  in other words 

Qn := (cid:0)P(Z n = 1|Xn)  P(Z n = 0|Xn)).

Since γn[n] = P(Z = 1|Xn)  convergence of γn[n] to 1 is equivalent to the convergence of Qn to
e(0) = (1  0). We have

P (Z n|Xn) ∝Z n P (Z n  X n|Xn−1) = P (X n|Z n)P (Z n|Xn−1).

(7)

3

Note that P (X n|Z n = 1) = f (X n) and P (X n|Z n = 0) = g(X n). Let θn := (cid:0)1  g(X n)

f (X n)(cid:1) and

Rn−1 := (cid:0)P(Z n = 1|Xn−1)  P(Z n = 0|Xn−1)).

Then  (7) implies that Qn can be obtained by pointwise multiplication of Rn−1 by f (X n)θn and
normalization to make a probability vector. Alternatively  we can multiply by θn  since the proce-
dure is scale-invariant  that is  Qn = qθn(Rn−1) using deﬁnition (1). It remains to express Rn−1 in
terms of Qn−1. This can be done by using the Bayes rule and the fact that P (Xn−1|λ = k) is the
same for k ∈ {n  n + 1  . . . }. In particular  after some algebra (see [17])  one arrives at

γn−1[n] =

π(n)

π[n − 1]c +

π[n]c
π[n − 1]c γn−1[n − 1] 

(8)

where γk[n]

:= P(λ ≤ n|Xk)  π(n) is the prior on λ evaluated at time n  and π[k]c

:=
P∞
i=k+1 π(i). For the geometric prior with parameter ρ ∈ [0  1]  we have π(n) := (1 − ρ)n−1ρ and
π[k]c = ρk. The above recursion then simpliﬁes to γn−1[n] = ρ + (1 − ρ)γn−1[n − 1]. Expressing
in terms of Rn−1 and Qn−1  the recursion reads

Rn−1 = T (Qn−1)  where T(cid:16)(cid:16) x1

(cid:17)(cid:17) = ρ(cid:16) 1

0(cid:17) + (1 − ρ)(cid:16) x1

(cid:17).

x0
In other words  T (x) = ρe(0) + (1 − ρ)x for x ∈ P2.
Thus  we have shown that an iterative algorithm for computing γn[n] (hence determining rule (6)) 
can be expressed in the form of (2) for appropriate choices of {θn} and operator T . Note that T in
this case is Lipschitz with constant 1 − ρ which is always guaranteed to be ≤ 1.

x0

We can now use Theorem 1 to analyze the convergence of γn[n]. Let us condition on λ = k + 1 
that is  we assume that the change point has occurred at time k + 1. Then  the sequence {X n}n≥k+1
is distributed according to f   and we have Eθ∗
f = −I  where I is the KL divergence
between densities f and g. Noting that kQn − e(0)k = 2(1 − γn[n])  we immediately obtain the
following corollary.
Corollary 1. Consider Example 1 and assume that log(g(X)/f (X))  where X ∼ f   is sub-
g . Then  conditioned on λ = k + 1 

Gaussian with sub-Gaussian norm ≤ σ. Let I := R f log f

n = R f log g

we have for n ≥ 1 

(cid:12)(cid:12)γn+k[n + k] − 1(cid:12)(cid:12) ≤ (cid:2)(1 − ρ)e−I+ε(cid:3)n(cid:16) 1

γk[k]

with probability at least 1 − exp(−c nε2/σ2).

− 1(cid:17)

4 Multiple change point problem via latent variable graphical models

We now turn to our main application for Theorem 1  in the context of a multiple change point
problem. In [18]  graphical model formalism is used to extend the classical change point problem
(cf. Example 1) to cases where multiple distributed latent change points are present. Throughout
this section  we will use this setup which we now brieﬂy sketch.

One starts with a network G = (V  E) of d sensors or nodes  each associated with a change point λj .
Each node j observes a private sequence of measurements Xj = (X 1
j   . . . ) which undergoes a
change in distribution at time λj   that is 

j   X 2

X 1

j   X 2

j   . . .   X k−1

j

| λj = k iid∼ gj 

X k

j   X k+1

j

  · · · | λj = k iid∼ fj 

for densities gj and fj (w.r.t. some underlying measure). Each connected pair of nodes share
an additional sequence of measurements. For example  if nodes s1 and s2 are connected  that is 
e = (s1  s2) ∈ E  then they both observe Xe = (X 1
e   . . . ). The shared sequence undergoes a
change in distribution at some point depending on λs1 and λs2 . More speciﬁcally  it is assumed that
the earlier of the two change points causes a change in the shared sequence  that is  the distribution
of Xe conditioned on (λs1   λs2 ) only depends on λe := λs1 ∧ λs2   the minimum of the two  i.e. 

e   X 2

X 1

e   X 2

e   . . .   X k

e | λe = k iid∼ ge 

X k+1

e

  X k+2

e

  · · · | λe = k iid∼ fe.

4

Letting λ∗ := {λj}j∈V and Xn
variables as

∗ = {Xn

e }j∈V e∈E  we can write the joint density of all random

P (λ∗  Xn

∗ ) = Y

j∈V

P (Xn

j |λj) Y

e ∈E

P (Xn

e |λs1   λs2 ).

(9)

j   Xn
πj(λj ) Y

j∈V

where πj is the prior on λj   which we assume to be geometric with parameter ρj . Network G
induces a graphical model [2] which encodes the factorization (9) of the joint density. (cf. Fig. 1)

Suppose now that each node j wants to detect its change point λj   with minimum expected delay 
while maintaining a false alarm probability at most α. Inspired by the classical change point prob-
lem  one is interested in computing the posterior probability that the change point has occurred up
to now  that is 

γn
j [n] := P(λj ≤ n | Xn

∗ ).

(10)

The difference with the classical setting is the conditioning is done on all the data in the network (up
to time n). It is easy to verify that the natural stopping rule

τj = inf{n ∈ N : γn

j [n] ≥ 1 − α}

satisfy the false alarm constraint. It has also been shown that this rule is asymptotically optimal in
terms of expected detection delay. Moreover  an algorithm based on the well-known sum-product [2]
has been proposed  which allows the nodes to compute their posterior probabilities 10 by message-
passing. The algorithm is exact when G is a tree  and scales linearly in the number of nodes. More
precisely  at time n  the computational complexity is O(nd). The drawback is the linear dependence
on n  which makes the algorithm practically infeasible if the change points model rare events (where
n could grow large before detecting the change.)

In the next section  we propose an approximate message passing algorithm which has computational
complexity O(d)  at each time step. This circumvents the drawback of the exact algorithm and
allows for indeﬁnite run times. We then show how the theory developed in Section 3 can be used to
provide convergence guarantees for this approximate algorithm  as well as the exact one.

4.1 Fast approximate message-passing (MP)

We now turn to an approximate message-passing algorithm which  at each time step  has com-
putational complexity O(d). The derivation is similar to that used for the iterative algorithm in
Example 1. Let us deﬁne binary variables

The idea is to compute P (Z n
is proportional in Z n

∗ to P (Z n

Z n

∗ = (Z n

j = 1{λj ≤ n}  Z n
∗ ) recursively based on P (Z n−1
∗ ) P (Z n

1   . . .   Z n
d ).
|Xn−1
∗ |Xn−1

) = P (X n

∗ |Xn−1

∗ |Z n

∗   X n

∗ |Xn

∗

∗

∗

∗

(11)

). By Bayes rule  P (Z n
)  hence

∗ |Xn
∗ )

P (Z n

∗ |Xn

∗ ) ∝Z n

P (X n

j |Z n

P (X n

ij|Z n

i   Z n

∗ |Xn−1

∗

) 

(12)

∗ h Y

j∈V

j ) Y

{i j}∈E

j )i P (Z n

where we have used the fact that given Z n

∗   X n

∗ is independent of Xn−1

∗

. To simplify notation  let us

extend the edge set to eE := E ∪{{j} : j ∈ V }. This allows us to treat the private data of node j  i.e. 
Xj   as shared data of a self-loop in the extended graph (V  eE). Let ue(z; ξ) := [ge(ξ)]1−z[fe(ξ])z
for e ∈ eE  z ∈ {0  1}. Then  for i 6= j 
ij|Z n
j ; X n
j ) = uj(Z n
) in terms of P (Z n−1

It remains to express P (Z n
). It is possible to do this  exactly  at
a cost of O(2|V |). For brevity  we omit the exact expression. (See Lemma 1 for some details.) We
term the algorithm that employs the exact relationship  the “exact algorithm”.

i   Z n
|Xn−1

j ) = uij(Z n

j )  P (X n

∗ |Xn−1

i ∨ Z n

j ; X n

P (X n

j |Z n

ij).

(13)

∗

∗

∗

In practice  however  the exponential complexity makes the exact recursion of little use for large
networks. To obtain a fast algorithm (i.e.  O(poly(d))  we instead take a mean-ﬁeld type approxi-
mation:

ν(Z n

j ; γn−1

j

[n]) 

(14)

P (Z n

∗ |Xn−1

∗

) ≈ Y

j∈V

) = Y

j∈V

P (Z n

j |Xn−1

∗

5

where ν(z; β) := βz(1 − β)1−z. That is  we approximate a multivariate distribution by the product
of its marginals. By an argument similar to that used to derive (8)  we can obtain a recursion for the
marginals 

γn−1
j

[n] =

πj(n)

πj[n − 1]c +

πj[n]c
πj[n − 1]c γn−1

j

[n − 1] 

(15)

where we have used the notation introduced earlier in (8). Thus  at time n  the RHS of (14) is known
based on values computed at time n − 1 (with initial value γ0
j [0] = 0  j ∈ V ). Inserting this RHS
into (12) in place of P (Z n
∗ (instead of λ∗)
which has the same form as (9) with ν(Z n

)  we obtain a graphical model in variables Z n

[n]) playing the role of the prior π(λj ).

∗ |Xn−1

∗

j ; γn−1

In order to obtain the marginals γn
ij [n] with respect to the approximate
version of the joint distribution P (Z n
)  we need to marginalize out the latent variables
Z n
j ’s  for which a standard sum-product algorithm can be applied (see [2  3  18]). The message
update equations are similar to those in [18]; the difference is that the messages are now binary and
do not grow in size with n.

∗ ) and γn

∗   X n

j
j [n] = P (Z n
j = 1|Xn
∗

∗ |Xn−1

4.2 Convergence of MP algorithms

We now turn to the analysis of the approximate algorithm introduced in Section 4.1. In particular  we
will look at the evolution of {eP (Z n
∗ )}n∈N as a sequence of probability distribution on {0  1}d.
Here  eP signiﬁes that this sequence is an approximation. In order to make a meaningful comparison 

we also look at the algorithm which computes the exact sequence {P (Z n
∗ )}n∈N  recursively. As
mentioned before  this we will call the “exact algorithm”  the details of which are not of concern to
us at this point (cf. Prop. 1 for these details.)

∗ |Xn

∗ |Xn

To make this correspondence formal and the notation simpliﬁed  we use the symbol :≡ as follows

∗ |Xn

∗ ) and P (Z n

∗ |Xn

∗ )  as distributions for Z n

∗   to be elements of Pd ⊂ Rm.

Recall that we take eP (Z n

eyn :≡ eP (Z n

∗ |Xn

∗ ) 

yn :≡ P (Z n

∗ |Xn
∗ )

(16)

Xn

∗ . We have the following description.

where now eyn  yn ∈ Pd. Note that eyn and yn are random elements of Pd  due the randomness of
Proposition 1. The exact and approximate sequences  {yn} and {eyn}  follow general iteration (2)

with the same random sequence {θn}  but with different deterministic operators T   denoted respec-
tively with Tex and Tap. Tex is linear and given by a Markov transition kernel. Tap is a polynomial
map of degree d. Both maps are Lipschitz and we have

LipTex ≤ Lρ := (cid:16)1 −

dY

j=1

ρj(cid:17)  LipTap ≤ Kρ :=

dX

j=1

(1 − ρj).

(17)

Detailed descriptions of the sequence {θn} and the operators Tex and Tap are given in [17]. As
suggested by Theorem 1  a key assumption for the convergence of the approximate algorithm will
be Kρ ≤ 1. In contrast  we always have Lρ ≤ 1.

Recall that {λj} are the change points and their priors are geometric with parameters {ρj}. We
analyze the algorithms  once all the change points have happened. More precisely  we condition
on Mn0 := {maxj λj ≤ n0} for some n0 ∈ N. Then  one expects the (joint) posterior of Z n
∗ to
contract to the point Z∞
{yn} to converge to e(0). Theorem 2 below quantiﬁes this convergence in ℓ1 norm (equivalently 
total variation for measures).

j = 1  for all j ∈ V . In the vectorial notation  we expect both {eyn} and

Recall pre-change and post-change densities ge and fe  and let Ie denote their KL divergence  that

is  Ie := R fe log(fe/ge). We will assume that
is sub-Gaussian  for all e ∈ eE  where eE is extended edge notation introduced in Section 4.1. The

choice X ∼ fe is in accordance with conditioning on Mn0 . Note that EYe = −Ie < 0. We deﬁne

Ye := log(ge(X)/fe(X)) with X ∼ fe

(18)

σmax := max
e∈ eE

kYekψ2 

Imin := min
e∈ eE

Ie 

I∗(κ) := Imin − κ σmaxplog D..

where D := |V | + |E|. Theorem 1 and Lemma 1 give us the following. (See [17] for the proof.)

6

λ1

λ2

λ3

X23

X23

λ3

λ1

λ2

X12

λ3
X24 X45

λ1

λ4

 

1

λ5

mn
12
λ2

X12

 

1

mn
24

mn
32
X24 X45

λ4

mn
45

λ5

 

λ4

λ5

 

1

0.8

0.6

0.4

0.2

0

 

0.8

0.6

0.4

0.2

0

 

10

20

30

40

50

60

70

MP
APPROX

10

20

30

40

50

60

70

MP
APPROX

0.8

0.6

0.4

0.2

0

 

10

20

30

40

50

60

70

MP
APPROX

10

20

30

40

50

60

70

MP
APPROX

1

0.8

0.6

0.4

0.2

0

 

Figure 1:
illustrates one stage of message-passing to compute posterior probabilities γ
examples of posterior paths  n 7→ γ
for the subgraph on nodes {1  2  3  4}. The change points are designated with vertical dashed lines.

Top row illustrates a network (left)  which induces a graphical model (middle). Right panel
n
j [n]. Bottom row illustrates typical
n
j [n]  obtained by EXACT and approximate (APPROX) message passing 

Theorem 2. There exists an absolute constant κ > 0  such that if I∗(κ) > 0  the exact algorithm
converges at least geometrically w.h.p.  that is  for all n ≥ 1 

Kρ ≤ 1  the approximate algorithm also converges at least geometrically w.h.p.  i.e.  for all n ≥ 1 

kyn+n0 − e(0)k ≤ 2
with probability at least 1 − exp(cid:2)−c nε2/(σ2

1 − yn0

yn0

with the same (conditional) probability as the exact algorithm.

keyn+n0 − e(0)k ≤ 2

(cid:0)Lρe−I∗(κ)+ε(cid:1)n

(19)

maxD2 log D)(cid:3)  conditioned on Mn0 . If in addition 
1 − eyn0
eyn0

(cid:0)Kρe−I∗(κ)+ε(cid:1)n

(20)

4.3 Simulation results

We present some simulation results to verify the effectiveness of the proposed approximation al-
gorithm in estimating the posterior probabilities γn
j [n]. We consider a star graph on d = 4 nodes.
This is the subgraph on nodes {1  2  3  4} in Fig. 1. Conditioned on the change points λ∗  all data
sequences X∗ are assumed Gaussian with variance 1  pre-change mean 1 and post-change mean
zero. All priors are geometric with ρj = 0.1. We note that higher values of ρj yield even faster
convergence in the simulations  but we omit these ﬁgures due to space constraints. Fig. 1 illustrates
typical examples of posterior paths n 7→ γn
j [n]  for both the exact and approximate MP algorithms.
One can observe that the approximate path often closely follows the exact one. In some cases  they
might deviate for a while  but as suggested by Theorem 2  they approach one another quickly  once
the change points have occurred.

From the theorem and triangle inequality  it follows that under I∗(κ) > 0 and Kρ ≤ 1  kyn − eynk

converges to zero  at least geometrically w.h.p. This gives some theoretical explanation for the good
tracking behavior of approximate algorithm as observed in Fig. 1.

5 Proof of Theorem 1

For x ∈ Rm (including Pd)  we write x = (x0 ex) where ex = (x1  . . .   xm−1). Recall that e(0) =
(1  0  . . .   0) and kxk = Pm−1

i=0 |xi|. For x ∈ Pd  we have 1 − x0 = kexk  and
kx − e(0)k = k(x0 − 1 ex)k = 1 − x0 + kexk = 2(1 − x0).

For θ = (θ0  eθ) ∈ Rm

+   let

θ∗ := keθk∞ = max

i=1 ... m−1

θi 

θ† := (cid:0)θ0  (θ∗L)1m−1(cid:1) ∈ Rm

+

7

(21)

(22)

where 1m−1 is a vector in Rm−1 whose coordinates are all ones. We start by investigating how
kqθ(x) − e(0)k varies as a function of kx − e(0)k.
Lemma 1. For L ≤ 1  θ∗ > 0  and θ0 = 1 

N :=

sup

x y ∈ Pd 

kx−e(0)k ≤ Lky−e(0)k

kqθ(x) − e(0)k
kqθ†(y) − e(0)k

= 1;

(23)

Lemma 1 is proved in [17]. We now proceed to the proof of the theorem. Recall that T : Pd → Pd
is an L-Lipschitz map  and that e(0) is a ﬁxed point of T   that is  T (e(0)) = e(0). It follows that for
any x ∈ Pd  kT (x) − e(0)k ≤ Lkx − e(0)k. Applying Lemma 1  we get

kqθ(T (x)) − e(0)k ≤ kqθ†(x) − e(0)k

(24)

for θ ∈ Rm

+ with θ0 = 1  and x ∈ Pd. (This holds even if θ∗ = 0 where both sides are zero.)

Recall the sequence {θn}n≥1 used in deﬁning functions {Qn} accroding to (2)  and the assumption
that θ0
n = 1  for all n ≥ 1. Inequality (24) is key in allowing us to peel operator T   and bring
successive elements of {qθn} together. Then  we can exploit the semi-group property (3) on adjacent
elements of {qθn}.
To see this  for each θn  let θ∗
n and θ†
Qn−1(x)  and θ with θn  we can write

n be deﬁned as in (22). Applying (24) with x replaced with

kQn(x) − e(0)k ≤ kqθ†

n

= kqθ†

n

(Qn−1(x)) − e(0)k
(by (24))
(qθn−1(T (Qn−2(x)))) − e(0)k

= kqθ†

n⊙ θn−1

(T (Qn−2(x)))) − e(0)k (by semi-group property (3))

We note that (θ†

n ⊙ θn−1)∗ = Lθ∗

n

θ∗

n−1 and

(θ†

n ⊙ θn−1)

†

= (cid:0)1  L(θ†

n ⊙ θn−1)∗1m−1(cid:1) = (cid:0)1  L2θ∗

n

θ∗

n−1

1m−1(cid:1).

Here  ∗ and † act on a general vector in the sense of (22). Applying (24) once more  we get

kQn(x) − e(0)k ≤ kq(1 L2θ∗

n

1m−1)(Qn−2(x)) − e(0)k.

θ∗

n−1

The pattern is clear. Letting ηn := LnQn

k=1

θ∗

k  we obtain by induction

kQn(x) − e(0)k ≤ kq(1 ηn1m−1)(Q0(x)) − e(0)k.

Recall that Q0(x) := x. Moreover 

kq(1 ηn 1m−1)(x) − e(0)k = 2(cid:0)1 − [q(1 ηn 1m−1)(x)]0(cid:1) = 2(cid:0)1 − gηn(x0)(cid:1)

(25)

(26)

where the ﬁrst inequality is by (21)  and the second is easily veriﬁed by noting that all the elements
of (1  ηn1m−1)  except the ﬁrst  are equal. Putting (25) and (26) together with the bound 1−gθ(r) =
1−x0
r+θ(1−r) ≤ θ 1−r
x0 .
By sub-Gaussianity assumption on {log θ∗

r   which holds for θ > 0 and r ∈ (0  1]  we obtain kQn(x) − e(0)k ≤ 2ηn

θ(1−r)

k}  we have

P(cid:16) 1

n

nX

k=1

log θ∗

k − E log θ∗

1 > ε(cid:17) ≤ exp(−c nε2/σ2

∗) 

(27)

for some absolute constant c > 0. (Recall that σ∗ is an upper bound on the sub-Gaussian norm
k ≤ en(−I∗+ε)  which com-
θ∗
k log θ∗
pletes the proof.

1kψ2 .) On the complement of the event in 27  we have Qn

k=1

Acknowledgments

This work was supported in part by NSF grants CCF-1115769 and OCI-1047871.

8

References

[1] A. N. Shiryayev. Optimal Stopping Rules. Springer-Verlag  1978.

[2] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.

Morgan Kaufmann  1988.

[3] M. I. Jordan. Graphical models. Statistical Science  19:140–155  2004.

[4] P. Diaconis and D. Freedman. Iterated random functions. SIAM Rev.  41(1):45–76  1999.

[5] O. P. Kreidl and A. Willsky. Inference with minimum communication: a decision-theoretic

variational approach. In NIPS  2007.

[6] M. Cetin  L. Chen  J. W. Fisher III  A. Ihler  R. Moses  M. Wainwright  and A. Willsky. Dis-
tributed fusion in sensor networks: A graphical models perspective. IEEE Signal Processing
Magazine  July:42–55  2006.

[7] X. Nguyen  A. A. Amini  and R. Rajagopal. Message-passing sequential detection of multiple

change points in networks. In ISIT  2012.

[8] A. Frank  P. Smyth  and A. Ihler. A graphical model representation of the track-oriented
multiple hypothesis tracker. In Proceedings  IEEE Statistical Signal Processing (SSP). August
2012.

[9] A. T. Ihler  J. W. Fisher III  and A. S. Willsky. Loopy belief propagation: Convergence and

effects of message errors. Journal of Machine Learning Research  6:905–936  May 2005.

[10] Alexander Ihler. Accuracy bounds for belief propagation. In Proceedings of UAI 2007  July

2007.

[11] T. G. Roosta  M. Wainwright  and S. S. Sastry. Convergence analysis of reweighted sum-

product algorithms. IEEE Trans. Signal Processing  56(9):4293–4305  2008.

[12] D. Steinsaltz. Locally contractive iterated function systems. Ann. Probab.  27(4):1952–1979 

1999.

[13] W. B. Wu and M. Woodroofe. A central limit theorem for iterated random functions. J . Appl.

Probab.  37(3):748–755  2000.

[14] W. B. Wu and X. Shao. Limit theorems for iterated random functions.. :. J. Appl. Probab. 

[15]

41(2):425–436  2004.
¨O. Stenﬂo. A survey of average contractive iterated function systems. J. Diff. Equa. and Appl. 
18(8):1355–1380  2012.

[16] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes: With Applica-

tions to Statistics. Springer  1996.

[17] A. A. Amini and X. Nguyen. Bayesian inference as iterated random functions with applications

to sequential inference in graphical models. arXiv preprint.

[18] A. A. Amini and X. Nguyen. Sequential detection of multiple change points in networks:
IEEE Transactions on Information Theory  59(9):5824–5841 

a graphical model approach.
2013.

9

,Arash Amini
XuanLong Nguyen
Kinjal Basu
Ankan Saha