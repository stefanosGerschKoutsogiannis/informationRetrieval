2016,Optimal Learning for Multi-pass Stochastic Gradient Methods,We analyze the learning  properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular  we consider the square loss and show that    for  a universal step-size choice  the number of passes acts as a regularization parameter  and optimal finite sample bounds  can be achieved by early-stopping. Moreover  we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on  a unifying approach  encompassing both batch and stochastic gradient methods as special cases.,OptimalLearningforMulti-passStochasticGradientMethodsJunhongLinLCSL IIT-MIT USAjunhong.lin@iit.itLorenzoRosascoDIBRIS Univ.Genova ITALYLCSL IIT-MIT USAlrosasco@mit.eduAbstractWeanalyzethelearningpropertiesofthestochasticgradientmethodwhenmultiplepassesoverthedataandmini-batchesareallowed.Inparticular weconsiderthesquarelossandshowthatforauniversalstep-sizechoice thenumberofpassesactsasaregularizationparameter andoptimalﬁnitesampleboundscanbeachievedbyearly-stopping.Moreover weshowthatlargerstep-sizesareallowedwhenconsideringmini-batches.Ouranalysisisbasedonaunifyingapproach encompassingbothbatchandstochasticgradientmethodsasspecialcases.1IntroductionModernmachinelearningapplicationsrequirecomputationalapproachesthatareatthesametimestatisticallyaccurateandnumericallyefﬁcient[2].Thishasmotivatedarecentinterestinstochasticgradientmethods(SGM) sinceontheonehandtheyenjoygoodpracticalperformances especiallyinlargescalescenarios andontheotherhandtheyareamenabletotheoreticalstudies.Inparticular unlikeotherlearningapproaches suchasempiricalriskminimizationorTikhonovregularization theoreticalresultsonSGMnaturallyintegratestatisticalandcomputationalaspects.MostgeneralizationstudiesonSGMconsiderthecasewhereonlyonepassoverthedataisallowedandthestep-sizeisappropriatelychosen [5 14 29 26 9 16](possiblyconsideringaveraging[18]).Inparticular recentworksshowhowthestep-sizecanbeseentoplaytheroleofaregularizationparameterwhosechoicecontrolsthebiasandvariancepropertiesoftheobtainedsolution[29 26 9].Theselatterworksshowthatbalancingthesecontributions itispossibletoderiveastep-sizechoiceleadingtooptimallearningbounds.Suchachoicetypicallydependsonsomeunknownpropertiesofthedatageneratingdistributionsandinpracticecanbechosenbycross-validation.Whileprocessingeachdatapointonlyonceisnaturalinstreaming/onlinescenarios inpracticeSGMisoftenusedasatoolforprocessinglargedata-setsandmultiplepassesoverthedataaretypicallyconsidered.Inthiscase thenumberofpassesoverthedata aswellasthestep-size needthentobedetermined.Whiletheroleofmultiplepassesiswellunderstoodifthegoalisempiricalriskminimization[3] itseffectwithrespecttogeneralizationislessclearandafewrecentworkshaverecentlystartedtotacklethisquestion.Inparticular resultsinthisdirectionhavebeenderivedin[10]and[11].TheformerworkconsidersageneralstochasticoptimizationsettingandstudiesstabilitypropertiesofSGMallowingtoderiveconvergenceresultsaswellasﬁnitesamplebounds.Thelatterwork restrictedtosupervisedlearning furtherdevelopstheseresultstocomparetherespectiverolesofstep-sizeandnumberofpasses andshowhowdifferentparametersettingscanleadtooptimalerrorbounds.Inparticular itshowsthattherearetwoextremecases:onebetweenthestep-sizeorthenumberofpassesisﬁxedapriori whiletheotheroneactsasaregularizationparameterandneedstobechosenadaptively.Themainshortcomingoftheselatterresultsisthattheyareintheworstcase inthesensethattheydonotconsiderthepossibleeffectofcapacityassumptions[30 4]showntoleadtofasterratesforotherlearningapproachessuchasTikhonovregularization.Further theseresultsdonotconsiderthepossibleeffectofmini-batches ratherthanasinglepointineachgradient30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.step[21 8 24 15].ThislatterstrategyisoftenconsideredespeciallyforparallelimplementationofSGM.Thestudyinthispaper ﬁllsinthesegapsinthecasewherethelossfunctionistheleastsquaresloss.WeconsideravariantofSGMforleastsquares wheregradientsaresampleduniformlyatrandomandmini-batchesareallowed.Thenumberofpasses thestep-sizeandthemini-batchsizearethenparameterstobedetermined.Ourmainresultshighlighttherespectiverolesoftheseparametersandshowhowcantheybechosensothatthecorrespondingsolutionsachieveoptimallearningerrors.Inparticular weshowfortheﬁrsttimethatmulti-passSGMwithearlystoppingandauniversalstep-sizechoicecanachieveoptimallearningrates matchingthoseofridgeregression[23 4].Further ouranalysisshowshowthemini-batchsizeandthestep-sizechoicearetightlyrelated.Indeed largermini-batchsizesallowtoconsiderlargerstep-sizeswhilekeepingtheoptimallearningbounds.Thisresultcouldgiveaninsightonhowtoexploitmini-batchesforparallelcomputationswhilepreservingoptimalstatisticalaccuracy.Finallywenotethatarecentwork[19]istightlyrelatedtotheanalysisinthepaper.Thegeneralizationpropertiesofamulti-passincrementalgradientareanalyzedin[19] foracyclic ratherthanastochastic choiceofthegradientsandwithnomini-batches.Theanalysisinthislattercaseappearstobeharderandresultsin[19]givegoodlearningboundsonlyinrestrictedsettingandconsideringiteratesratherthantheexcessrisk.Comparedto[19]ourresultsshowhowstochasticitycanbeexploitedtogetfastercapacitydependentratesandanalyzetheroleofmini-batches.ThebasicideaofourproofistoapproximatetheSGMlearningsequenceintermsofthebatchGMsequence seeSubsection3.4forfurtherdetails.Thisthusallowsonetostudybatchandstochasticgradientmethodssimultaneously andmaybealsousefulforanalysingotherlearningalgorithms.Therestofthispaperisorganizedasfollows.Section2introducesthelearningsettingandtheSGMalgorithm.MainresultswithdiscussionsandproofsketchesarepresentedinSection3.Finally simplenumericalsimulationsaregiveninSection4tocomplementourtheoreticalresults.NotationForanya b∈R a∨bdenotesthemaximumofaandb.Nisthesetofallpositiveintegers.ForanyT∈N [T]denotestheset{1 ··· T}.Foranytwopositivesequences{at}t∈[T]and{bt}t∈[T] thenotationat.btforallt∈[T]meansthatthereexistsapositiveconstantC≥0suchthatCisindependentoftandthatat≤Cbtforallt∈[T].2LearningwithSGMWebeginbyintroducingthelearningsettingweconsider andthendescribetheSGMlearningalgorithm.Following[19] theformulationweconsiderisclosetothesettingoffunctionalregression andcoversthereproducingkernelHilbertspace(RKHS)settingasaspecialcase.Inparticular itreducestostandardlinearregressionforﬁnitedimensions.2.1LearningProblemsLetHbeaseparableHilbertspace withinnerproductandinducednormdenotedbyh· ·iHandk·kH respectively.LettheinputspaceX⊆HandtheoutputspaceY⊆R.LetρbeanunknownprobabilitymeasureonZ=X×Y ρX(·)theinducedmarginalmeasureonX andρ(·|x)theconditionalprobabilitymeasureonYwithrespecttox∈Xandρ.Consideringthesquarelossfunction theproblemunderstudyistheminimizationoftherisk infω∈HE(ω) E(ω)=ZX×Y(hω xiH−y)2dρ(x y) (1)whenthemeasureρisknownonlythroughasamplez={zi=(xi yi)}mi=1ofsizem∈N independentlyandidenticallydistributed(i.i.d.)accordingtoρ.Inthefollowing wemeasurethequalityofanapproximatesolutionˆω∈H(anestimator)consideringtheexcessrisk i.e. E(ˆω)−infω∈HE(ω).(2)Throughoutthispaper weassumethatthereexistsaconstantκ∈[1 ∞[ suchthathx x0iH≤κ2 ∀x x0∈X.(3)2.2StochasticGradientMethodWestudythefollowingSGM(withmini-batches withoutpenalizationorconstraints).2Algorithm1.Letb∈[m].Givenanysamplez theb-minibatchstochasticgradientmethodisdeﬁnedbyω1=0andωt+1=ωt−ηt1bbtXi=b(t−1)+1(hωt xjiiH−yji)xji t=1 ... T (4)where{ηt>0}isastep-sizesequence.Here j1 j2 ··· jbTareindependentandidenticallydistributed(i.i.d.)randomvariablesfromtheuniformdistributionon[m]1.Differentchoicesforthe(mini-)batchsizebcanleadtodifferentalgorithms.Inparticular forb=1 theabovealgorithmcorrespondstoasimpleSGM whileforb=m itisastochasticversionofthebatchgradientdescent.Theaimofthispaperistoderiveexcessriskboundsfortheabovealgorithmunderappropriateassumptions.Throughoutthispaper weassumethat{ηt}tisnon-increasing andT∈NwithT≥3.WedenotebyJttheset{jl:l=b(t−1)+1 ··· bt}andbyJtheset{jl:l=1 ··· bT}.3MainResultswithDiscussionsInthissection weﬁrststatesomebasicassumptions.Then wepresentanddiscussourmainresults.3.1AssumptionsThefollowingassumptionisrelatedtoamomenthypothesison|y|2.Itisweakerthantheoftenconsideredboundedoutputassumption andtriviallyveriﬁedinbinaryclassiﬁcationproblemswhereY={−1 1}.Assumption1.ThereexistsconstantsM∈]0 ∞[andv∈]1 ∞[suchthatZYy2ldρ(y|x)≤l!Mlv ∀l∈N (5)ρX-almostsurely.Topresentournextassumption weintroducetheoperatorL:L2(H ρX)→L2(H ρX) deﬁnedbyL(f)=RXhx ·iHf(x)ρX(x).UnderAssumption(3) Lcanbeprovedtobepositivetraceclassoperators andhenceLζwithζ∈Rcanbedeﬁnedbyusingthespectrumtheory[7].TheHilbertspaceofsquareintegralfunctionsfromHtoRwithrespecttoρX withinducednormgivenbykfkρ=(cid:0)RX|f(x)|2dρX(x)(cid:1)1/2 isdenotedby(L2(H ρX) k·kρ).ItiswellknownthatthefunctionminimizingRZ(f(x)−y)2dρ(z)overallmeasurablefunctionsf:H→Ristheregressionfunction whichisgivenbyfρ(x)=ZYydρ(y|x) x∈X.(6)DeﬁneanotherHilbertspaceHρ={f:X→R|∃ω∈Hwithf(x)=hω xiH ρX-almostsurely}.UnderAssumption3 itiseasytoseethatHρisasubspaceofL2(H ρX).LetfHbetheprojectionoftheregressionfunctionfρontotheclosureofHρinL2(H ρX).ItiseasytoseethatthesearchforasolutionofProblem(1)isequivalenttothesearchofalinearfunctionfromHρtoapproximatefH.Fromthispointofview boundsontheexcessriskofalearningalgorithmnaturallydependonthefollowingassumption whichquantiﬁeshowwell thetargetfunctionfHcanbeapproximatedbyHρ.Assumption2.Thereexistζ>0andR>0 suchthatkL−ζfHkρ≤R.Theaboveassumptionisfairlystandard[7 19]innon-parametricregression.Thebiggerζis themorestringenttheassumptionis sinceLζ1(L2(H ρX))⊆Lζ2(L2(H ρX))whenζ1≥ζ2.Inparticular forζ=0 weareassumingkfHkρ<∞ whileforζ=1/2 wearerequiringfH∈Hρ since[25 19]Hρ=L1/2(L2(H ρX)).Finally thelastassumptionrelatestothecapacityofthehypothesisspace.1Notethat therandomvariablesj1 ··· jbTareconditionallyindependentgiventhesamplez.3Assumption3.Forsomeγ∈]0 1]andcγ>0 Lsatisﬁestr(L(L+λI)−1)≤cγλ−γ forallλ>0.(7)TheLHSof(7)iscalledastheeffectivedimension orthedegreesoffreedom[30 4].Itcanberelatedtocovering/entropynumberconditions see[25]forfurtherdetails.Assumption3isalwaystrueforγ=1andcγ=κ2 sinceLisatraceclassoperatorwhichimpliestheeigenvaluesofL denotedasσi satisfytr(L)=Piσi≤κ2.Thisisreferredasthecapacityindependentsetting.Assumption3withγ∈]0 1]allowstoderivebettererrorrates.Itissatisﬁed e.g. iftheeigenvaluesofLsatisfyapolynomialdecayingconditionσi∼i−1/γ orwithγ=0ifLisﬁniterank.3.2MainResultsWestartwiththefollowingcorollary whichisasimpliﬁedversionofourmainresultsstatednext.Corollary3.1.UnderAssumptions2and3 letζ≥1/2and|y|≤MρX-almostsurelyforsomeM>0.ConsidertheSGMwith1)p∗=dm12ζ+γe b=1 ηt’1mforallt∈[(p∗m)] and˜ωp∗=ωp∗m+1.Ifmislargeenough withhighprobability2 thereholdsEJ[E(˜ωp∗)]−infω∈HE.m−2ζ2ζ+γ.Furthermore theabovealsoholdsfortheSGMwith32)orp∗=dm12ζ+γe b=√m ηt’1√mforallt∈[(p∗√m)] and˜ωp∗=ωp∗√m+1.Intheabove p∗isthenumberof‘passes’overthedata whichisdeﬁnedasdbtmeattiterations.Theaboveresultassertsthat atp∗passesoverthedata thesimpleSGMwithﬁxedstep-sizeachievesoptimallearningerrorbounds matchingthoseofridgeregression[4].Furthermore usingmini-batchallowstousealargerstep-sizewhileachievingthesameoptimalerrorbounds.Remark3.2(FiniteDimensionalCase).Withasimplemodiﬁcationofourproofs wecanderivesimilarresultsfortheﬁnitedimensionalcase i.e. H=Rd whereinthiscase γ=0.Inparticular lettingζ=1/2 underthesameassumptionsofCorollary3.1 ifoneconsiderstheSGMwithb=1andηt’1mforallt∈m2 thenwithhighprobability EJ[E(ωm2+1)]−infω∈HE.d/m providedthatm&dlogd.Ourmaintheoremofthispaperisstatednext andprovideserrorboundsforthestudiedalgorithm.Forthesakeofreadability weonlyconsiderthecaseζ≥1/2inaﬁxedstep-sizesetting.Generalresultsinamoregeneralsetting(ηt=η1t−θwith0≤θ<1 and/orthecaseζ∈]0 1/2])canbefoundintheappendix.Theorem3.3.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ ηt=ηκ−2forallt∈[T] withη≤18(logT+1).Ifm≥mδ thenthefollowingholdswithprobabilityatleast1−δ:forallt∈[T] EJ[E(ωt+1)]−infω∈HE≤q1(ηt)−2ζ+q2m−2ζ2ζ+γ(1+m−12ζ+γηt)2log2Tlog21δ+q3ηb−1(1∨m−12ζ+γηt)logT.(8)Here mδ q1 q2andq3arepositiveconstantsdependingonκ2 kTk M v ζ R cγ γ andmδalsoonδ(whichwillbegivenexplicitlyintheproof).Therearethreetermsintheupperboundsof(8).Theﬁrsttermdependsontheregularityofthetargetfunctionanditarisesfromboundingthebias whilethelasttwotermsresultfromestimatingthesamplevarianceandthecomputationalvariance(duetotherandomchoicesofthepoints) respectively.Toderiveoptimalrates itisnecessarytobalancethesethreeterms.Solvingthistrade-offproblemleadstodifferentchoicesonη T andb correspondingtodifferentregularizationstrategies asshowninsubsequentcorollaries.TheﬁrstcorollarygivesgeneralizationerrorboundsforSGM withauniversalstep-sizedependingonthenumberofsamplepoints.2Here ‘highprobability’referstothesamplez.3Here weassumethat√misaninteger.4Corollary3.4.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ b=1andηt’1mforallt∈[T] whereT≤m2.Ifm≥mδ thenwithprobabilityatleast1−δ thereholdsEJ[E(ωt+1)]−infω∈HE.(cid:26)(cid:16)mt(cid:17)2ζ+m−2ζ+22ζ+γ(cid:16)tm(cid:17)2(cid:27)·log2mlog21δ ∀t∈[T] (9)andinparticular EJ[E(ωT∗+1)]−infω∈HE.m−2ζ2ζ+γlog2mlog21δ (10)whereT∗=dm2ζ+γ+12ζ+γe.Here mδisexactlythesameasinTheorem3.3.Remark3.5.Ignoringthelogarithmictermandlettingt=pm Eq.(9)becomesEJ[E(ωpm+1)]−infω∈HE.p−2ζ+m−2ζ+22ζ+γp2.Asmallerpmayleadtoalargerbias whilealargerpmayleadtoalargersampleerror.Fromthispointofview phasaregularizationeffect.ThesecondcorollaryprovideserrorboundsforSGMwithaﬁxedmini-batchsizeandaﬁxedstep-size(whichdependonthenumberofsamplepoints).Corollary3.6.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ b=d√meandηt’1√mforallt∈[T] whereT≤m2.Ifm≥mδ thenwithprobabilityatleast1−δ thereholdsEJ[E(ωt+1)]−infω∈HE.(cid:26)(cid:16)√mt(cid:17)2ζ+m−2ζ+22ζ+γ(cid:16)t√m(cid:17)2(cid:27)log2mlog21δ ∀t∈[T] (11)andparticularly EJ[E(ωT∗+1)]−infω∈HE.m−2ζ2ζ+γlog2mlog21δ (12)whereT∗=dm12ζ+γ+12e.TheabovetwocorollariesfollowfromTheorem3.3withthesimpleobservationthatthedominatingtermsin(8)arethetermsrelatedtothebiasandthesamplevariance whenasmallstep-sizeischosen.Theonlyfreeparameterin(9)and(11)isthenumberofiterations/passes.Theidealstoppingruleisachievedbybalancingthetwotermsrelatedtothebiasandthesamplevariance showingtheregularizationeffectofthenumberofpasses.Sincetheidealstoppingruledependsontheunknownparametersζandγ ahold-outcross-validationprocedureisoftenusedtotunethestoppingruleinpractice.UsinganargumentsimilartothatinChapter6from[25] itispossibletoshowthatthisprocedurecanachievethesameconvergencerate.Wegivesomefurtherremarks.First theupperboundin(10)isoptimaluptoalogarithmicfactor inthesensethatitmatchestheminimaxlowerratein[4].Second accordingtoCorollaries3.4and3.6 bT∗m’m12ζ+γpassesoverthedataareneededtoobtainoptimalratesinbothcases.Finally incomparingthesimpleSGMandthemini-batchSGM Corollaries3.4and3.6showthatalargerstep-sizeisallowedtouseforthelatter.Inthenextresult boththestep-sizeandthestoppingrulearetunedtoobtainoptimalratesforsimpleSGMwithmultiplepasses.Inthiscase thestep-sizeandthenumberofiterationsaretheregularizationparameters.Corollary3.7.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ b=1andηt’m−2ζ2ζ+γforallt∈[T] whereT≤m2.Ifm≥mδ andT∗=dm2ζ+12ζ+γe then(10)holdswithprobabilityatleast1−δ.Remark3.8.Ifwemakenoassumptiononthecapacity i.e. γ=1 Corollary3.7recoverstheresultin[29]foronepassSGM.Thenextcorollaryshowsthatforsomesuitablemini-batchsizes optimalratescanbeachievedwithaconstantstep-size(whichisnearlyindependentofthenumberofsamplepoints)byearlystopping.Corollary3.9.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ b=dm2ζ2ζ+γeandηt’1logmforallt∈[T] whereT≤m2.Ifm≥mδ andT∗=dm12ζ+γe then(10)holdswithprobabilityatleast1−δ.5AccordingtoCorollaries3.7and3.9 aroundm1−γ2ζ+γpassesoverthedataareneededtoachievethebestperformanceintheabovetwostrategies.IncomparisonswithCorollaries3.4and3.6wherearoundmζ+12ζ+γpassesarerequired thelatterseemstorequirefewerpassesoverthedata.However inthiscase onemighthavetorunthealgorithmsmultipletimestotunethestep-size orthemini-batchsize.Finally thelastresultgivesgeneralizationerrorboundsfor‘batch’SGMwithaconstantstep-size(nearlyindependentofthenumberofsamplepoints).Corollary3.10.UnderAssumptions1 2and3 letζ≥1/2 δ∈]0 1[ b=mandηt’1logmforallt∈[T] whereT≤m2.Ifm≥mδ andT∗=dm12ζ+γe then(10)holdswithprobabilityatleast1−δ.Aswillbeseenintheprooffromtheappendix theaboveresultalsoholdswhenreplacingthesequence{ωt}bythesequence{νt}tgeneratedfrombatchGMin(14).Inthissense westudythegradient-basedlearningalgorithmssimultaneously.3.3DiscussionsWecompareourresultswithpreviousworks.Fornon-parametricregressionwiththesquareloss onepassSGMhasbeenstudiedin e.g. [29 22 26 9].Inparticular [29]provedcapacityindependentrateoforderO(m−2ζ2ζ+1logm)withaﬁxedstep-sizeη’m−2ζ2ζ+1 and[9]derivedcapacitydependenterrorboundsoforderO(m−2min(ζ 1)2min(ζ 1)+γ)(when2ζ+γ>1)fortheaverage.NotealsothataregularizedversionofSGMhasbeenstudiedin[26] wherethederivedconvergenceratethereisoforderO(m−2ζ2ζ+1)assumingthatζ∈[12 1].Incomparisonwiththeseexistingconvergencerates ourratesfrom(10)arecomparable eitherinvolvingthecapacitycondition orallowingabroaderregularityparameterζ(whichthusimprovestherates).Morerecently [19]studiedmultiplepassesSGMwithaﬁxedorderingateachpass alsocalledincrementalgradientmethod.Makingnoassumptiononthecapacity ratesoforderO(m−ζζ+1)(inL2(H ρX)-norm)withauniversalstep-sizeη’1/marederived.Incomparisons Corollary3.4achievesbetterrates whileconsideringthecapacityassumption.Notealsothat[19]provedsharprateinH-normforζ≥1/2inthecapacityindependentcase.Infact wecanextendouranalysistotheH-normforAlgorithm4.Wepostponethisextensiontoalongerversionofthispaper.Theideaofusingmini-batches(andparallelimplements)tospeedupSGMinageneralstochasticoptimizationsettingcanbefound e.g. in[21 8 24 15].Ourtheoreticalﬁndings especiallytheinterplaybetweenthemini-batchsizeandthestep-size cangivefurtherinsightsonparallelizationlearning.Besides ithasbeenshownin[6 8]thatforonepassmini-batchSGMwithaﬁxedstep-sizeη’b/√mandasmoothlossfunction assumingtheexistenceofatleastonesolutioninthehypothesisspacefortheexpectedriskminimization theconvergencerateisoforderO(p1/m+b/m)byconsideringanaveragingscheme.Whenadaptingtothelearningsettingweconsider thisreadsasthatiffH∈Hρ i.e. ζ=1/2 theconvergenceratefortheaverageisO(p1/m+b/m).Notethat fHdoesnotnecessarilybelongtoHρingeneral.Also ourderivedconvergenceratefromCorollary3.6isbetter whentheregularityparameterζisgreaterthan1/2 orγissmallerthan1.3.4ProofSketch(ErrorDecomposition)Thekeytoourproofisanovelerrordecomposition whichmaybealsousedinanalysingotherlearn-ingalgorithms.Onemayalsousetheapproachin[12 11]whichisbasedontheerrordecomposition i.e. forsomesuitablyintermediateelement˜ω∈H EE(ωt)−infw∈HE=[E(E(ωt)−Ez(ωt))+EEz(˜ω)−E(˜ω)]+E(Ez(ωt)−Ez(˜ω))+E(˜ω)−infω∈HE whereEzdenotestheempiricalrisk.However onecanonlyderiveasub-optimalconvergencerate sincetheproofprocedureinvolvesupperboundingthelearningsequencetoestimatethesampleerror(theﬁrsttermofRHS).Inthiscasethe‘regularity’oftheregressionfunctioncannotbefullyadaptedforboundingthebias(thelastterm).Thankstothepropertyofsquaresloss wecanexploitadifferenterrordecompositionleadingtobetterresults.Weﬁrstintroducetwosequences.Thepopulationiterationisdeﬁnedbyµ1=0andµt+1=µt−ηtZX(hµt xiH−fρ(x))xdρX(x) t=1 ... T.(13)6Theaboveiteratedprocedureisidealandcannotbeimplementedinpractice sincethedistributionρXisunknowningeneral.ReplacingρXbytheempiricalmeasureandfρ(xi)byyi wederivethesampleiteration(associatedwiththesamplez) i.e. ν1=0andνt+1=νt−ηt1mmXi=1(hνt xiiH−yi)xi t=1 ... T.(14)Clearly µtisdeterministicandνtisaH-valuedrandomvariabledependingonz.Giventhesamplez thesequence{νt}thasanaturalrelationshipwiththelearningsequence{ωt}t sinceEJ[ωt]=νt.(15)Indeed takingtheexpectationwithrespecttoJtonbothsidesof(4) andnotingthatωtdependsonlyonJ1 ··· Jt−1(givenanyz) onehasEJt[ωt+1]=ωt−ηt1mPmi=1(hωt xiiH−yi)xi andthus EJ[ωt+1]=EJ[ωt]−ηt1mPmi=1(hEJ[ωt] xiiH−yi)xi t=1 ... T whichsatisﬁestheiterativerelationshipgivenin(14).Byaninductionargument (15)canthenbeproved.LetSρ:H→L2(H ρX)bethelinearmapdeﬁnedby(Sρω)(x)=hω xiH ∀ω x∈H.Wehavethefollowingerrordecomposition.Proposition3.11.WehaveEJ[E(ωt)]−inff∈HE(f)≤2kSρµt−fHk2ρ+2kSρνt−Sρµtk2ρ+EJ[kSρωt−Sρνtk2].(16)Proof.Foranyω∈H wehave[25 19]E(ω)−inff∈HE(f)=kSρω−fHk2ρ.Thus E(ωt)−inff∈HE(f)=kSρωt−fHk2ρ andEJ[kSρωt−fHk2ρ]=EJ[kSρωt−Sρνt+Sρνt−fHk2ρ]=EJ[kSρωt−Sρνtk2ρ+kSρνt−fHk2ρ]+2EJhSρωt−Sρνt Sρνt−fHiρ.Using(15)totheabove wegetEJ[kSρωt−fHk2ρ]=EJ[kSρωt−Sρνtk2ρ+kSρνt−fHk2ρ].NowtheproofcanbeﬁnishedbyconsideringkSρνt−fHk2ρ=kSρνt−Sρµt+Sρµt−fHk2ρ≤2kSρνt−Sρµtk2ρ+2kSρµt−SρfHk2ρ.Therearethreetermsintheupperboundoftheerrordecomposition(16).WerefertothedeterministictermkSρµt−fHk2ρasthebias thetermkSρνt−Sρµtk2ρdependingonzasthesamplevariance andEJ[kSρωt−Sρνtk2ρ]asthecomputationalvariance.Thebiastermisdeterministicandiswellstudiedintheliterature seee.g. [28]andalso[19].Themainnoveltiesaretheestimateofthesampleandcomputationalvariances.Theproofoftheseresultsisquitelengthyandmakesuseofsomeideasfrom[28 23 1 29 26 20].Thesethreeerrortermswillbeestimatedintheappendix seeLemmaB.2 TheoremC.6andTheoremD.9.TheboundinTheorem3.3thusfollowspluggingtheseestimationsintheerrordecomposition.4NumericalSimulationsInordertoillustrateourtheoreticalresultsandtheerrordecomposition weﬁrstperformedsomesimulationsonasimpleproblem.Weconstructedm=100i.i.d.trainingexamplesoftheformy=fρ(xi)+ωi.Here theregressionfunctionisfρ(x)=|x−1/2|−1/2 theinputpointxiisuniformlydistributedin[0 1] andωiisaGaussiannoisewithzeromeanandstandarddeviation1 foreachi∈[m].WeperformthreeexperimentswiththesameH aRKHSassociatedwithaGaussiankernelK(x x0)=exp(−(x−x0)2/(2σ2))whereσ=0.2.Intheﬁrstexperiment werunmini-batchSGM wherethemini-batchsizeb=√m andthestep-sizeηt=1/(8√m).Inthesecondexperiment werunsimpleSGMwherethestep-sizeisﬁxedasηt=1/(8m) whileinthethirdexperiment werunbatchGMusingtheﬁxedstep-sizeηt=1/8.Formini-batchSGMandSGM thetotalerrorkSρωt−fρk2L2ˆρ thebiaskSρˆµt−fρk2L2ˆρ thesamplevariancekSρνt−Sρˆµtk2L2ˆρandthecomputationalvariancekSρωt−Sρνtk2L2ˆρ averagedover50trials aredepictedinFigures1aand1b respectively.ForbatchGM thetotalerrorkSρνt−fρk2L2ˆρ thebiaskSρˆµt−fρk2L2ˆρandthe702040608010012014016018020000.010.020.030.040.050.060.070.08PassErrorMinibatch SGM BiasSample ErrorComputational ErrorTotal Error(a)MinibatchSGM02040608010012014016018020000.010.020.030.040.050.060.070.08PassErrorSGM BiasSample ErrorComputational ErrorTotal Error(b)SGM02040608010012014016018020000.010.020.030.040.050.060.070.08PassErrorBatch GM BiasSample ErrorTotal Error(c)BatchGMFigure1:Errordecompositionsforgradient-basedlearningalgorithmsonsynthesisdata wherem=100.00.20.40.60.811.21.41.61.82x 10400.010.020.030.040.050.060.070.080.09PassErrorClassification Errors of Minibatch SGM Training ErrorValidation Error(a)MinibatchSGM00.20.40.60.811.21.41.61.82x 10400.010.020.030.040.050.060.070.08PassErrorClassification Errors of SGM Training ErrorValidation Error(b)SGM00.20.40.60.811.21.41.61.82x 10400.010.020.030.040.050.060.070.08PassErrorClassification Errors of GM Training ErrorValidation Error(c)BatchGMFigure2:MisclassiﬁcationErrorsforgradient-basedlearningalgorithmsonBreastCancerdataset.samplevariancekSρνt−ˆµtk2L2ˆρ averagedover50trialsaredepictedinFigure1c.Here wereplacetheunknownmarginaldistributionρXbyanempiricalmeasureˆρ=12000P2000i=1δˆxi whereeachˆxiisuniformlydistributedin[0 1].FromFigure1aor1b weseethatasthenumberofpassesincreases4 thebiasdecreases whilethesampleerrorincreases.Furthermore weseethatincomparisonswiththebiasandthesampleerror thecomputationalerrorisnegligible.Inalltheseexperiments theminimaltotalerrorisachievedwhenthebiasandthesampleerrorarebalanced.Theseempiricalresultsshowtheeffectsofthethreetermsfromtheerrordecomposition andcomplementthederivedbound(8) aswellastheregularizationeffectofthenumberofpassesoverthedata.Finally wetestedthesimpleSGM mini-batchSGM andbatchGM usingsimilarstep-sizesasthoseintheﬁrstsimulation ontheBreastCancerdata-set5.TheclassiﬁcationerrorsonthetrainingsetandthetestingsetofthesethreealgorithmsaredepictedinFigure2.Weseethatallofthesealgorithmsperformsimilarly whichcomplementtheboundsinCorollaries3.4 3.6and3.10.AcknowledgmentsThismaterialisbaseduponworksupportedbytheCenterforBrains MindsandMachines(CBMM) fundedbyNSFSTCawardCCF-1231216.L.R.acknowledgestheﬁnancialsupportoftheItalianMinistryofEducation UniversityandResearchFIRBprojectRBFR12M3AC.References[1]F.Bauer S.Pereverzev andL.Rosasco.Onregularizationalgorithmsinlearningtheory.JournalofComplexity 23(1):52–72 2007.[2]O.BousquetandL.Bottou.Thetradeoffsoflargescalelearning.InAdvancesinNeuralInformationProcessingSystems pages161–168 2008.[3]S.BoydandA.Mutapcic.Stochasticsubgradientmethods.NotesforEE364b StandfordUniversity Winter2007.4Notethattheterminology‘runningthealgorithmwithppasses’means‘runningthealgorithmwithdmp/beiterations’ wherebisthemini-batchsize.5https://archive.ics.uci.edu/ml/datasets/8[4]A.CaponnettoandE.DeVito.Optimalratesfortheregularizedleast-squaresalgorithm.FoundationsofComputationalMathematics 7(3):331–368 2007.[5]N.Cesa-Bianchi A.Conconi andC.Gentile.Onthegeneralizationabilityofon-linelearningalgorithms.IEEETransactionsonInformationTheory 50(9):2050–2057 2004.[6]A.Cotter O.Shamir N.Srebro andK.Sridharan.Bettermini-batchalgorithmsviaacceleratedgradientmethods.InAdvancesinNeuralInformationProcessingSystems pages1647–1655 2011.[7]F.CuckerandD.-X.Zhou.LearningTheory:anApproximationTheoryViewpoint volume24.CambridgeUniversityPress 2007.[8]O.Dekel R.Gilad-Bachrach O.Shamir andL.Xiao.Optimaldistributedonlinepredictionusingmini-batches.JournalofMachineLearningResearch 13(1):165–202 2012.[9]A.DieuleveutandF.Bach.Non-parametricstochasticapproximationwithlargestepsizes.AnnalsofStatistics 44(4):1363–1399 2016.[10]M.Hardt B.Recht andY.Singer.Trainfaster generalizebetter:Stabilityofstochasticgradientdescent.InInternationalConferenceonMachineLearning 2016.[11]J.Lin R.Camoriano andL.Rosasco.GeneralizationpropertiesandimplicitregularizationofmultiplepassesSGM.InInternationalConferenceonMachineLearning 2016.[12]J.Lin L.Rosasco andD.-X.Zhou.Iterativeregularizationforlearningwithconvexlossfunctions.JournalofMachineLearningResearch 17(77):1–38 2016.[13]S.Minsker.Onsomeextensionsofbernstein’sinequalityforself-adjointoperators.arXivpreprintarXiv:1112.5448 2011.[14]A.Nemirovski A.Juditsky G.Lan andA.Shapiro.Robuststochasticapproximationapproachtostochasticprogramming.SIAMJournalonOptimization 19(4):1574–1609 2009.[15]A.Ng.Machinelearning.Coursera StandfordUniversity 2016.[16]F.Orabona.Simultaneousmodelselectionandoptimizationthroughparameter-freestochasticlearning.InAdvancesinNeuralInformationProcessingSystems pages1116–1124 2014.[17]I.PinelisandA.Sakhanenko.Remarksoninequalitiesforlargedeviationprobabilities.TheoryofProbability&ItsApplications 30(1):143–148 1986.[18]B.T.Poljak.IntroductiontoOptimization.OptimizationSoftware 1987.[19]L.RosascoandS.Villa.Learningwithincrementaliterativeregularization.InAdvancesinNeuralInformationProcessingSystems pages1621–1629 2015.[20]A.Rudi R.Camoriano andL.Rosasco.Lessismore:Nyströmcomputationalregularization.InAdvancesinNeuralInformationProcessingSystems pages1648–1656 2015.[21]S.Shalev-Shwartz Y.Singer N.Srebro andA.Cotter.Pegasos:Primalestimatedsub-gradientsolverforsvm.MathematicalProgramming 127(1):3–30 2011.[22]O.ShamirandT.Zhang.Stochasticgradientdescentfornon-smoothoptimization:Convergenceresultsandoptimalaveragingschemes.InInternationalConferenceonMachineLearning pages71–79 2013.[23]S.SmaleandD.-X.Zhou.Learningtheoryestimatesviaintegraloperatorsandtheirapproxima-tions.ConstructiveApproximation 26(2):153–172 2007.[24]S.Sra S.Nowozin andS.J.Wright.OptimizationforMachineLearning.MITPress 2012.[25]I.SteinwartandA.Christmann.SupportVectorMachines.SpringerScienceBusinessMedia 2008.[26]P.TarresandY.Yao.Onlinelearningasstochasticapproximationofregularizationpaths:Optimalityandalmost-sureconvergence.IEEETransactionsonInformationTheory 60(9):5716–5735 2014.[27]J.A.Tropp.User-friendlytoolsforrandommatrices:Anintroduction.Technicalreport DTICDocument 2012.[28]Y.Yao L.Rosasco andA.Caponnetto.Onearlystoppingingradientdescentlearning.ConstructiveApproximation 26(2):289–315 2007.[29]Y.YingandM.Pontil.Onlinegradientdescentlearningalgorithms.FoundationsofComputa-tionalMathematics 8(5):561–596 2008.[30]T.Zhang.Learningboundsforkernelregressionusingeffectivedatadimensionality.NeuralComputation 17(9):2077–2098 2005.9,Junhong Lin
Lorenzo Rosasco