2018,Posterior Concentration for Sparse Deep Learning,We introduce Spike-and-Slab Deep Learning (SS-DL)  a fully Bayesian  alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables  provable recovery of smooth input-output maps with {\sl unknown} levels of smoothness. Indeed  we  show that  the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps  performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks  namely the choice of depth  width and sparsity level. These network attributes typically depend on  unknown smoothness  in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside  we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the  optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.,Posterior Concentration for Sparse Deep Learning

Nicholas G. Polson and Veronika Roˇcková

Booth School of Business

University of Chicago

Chicago  IL 60637

Abstract

We introduce Spike-and-Slab Deep Learning (SS-DL)  a fully Bayesian alternative
to dropout for improving generalizability of deep ReLU networks. This new type
of regularization enables provable recovery of smooth input-output maps with
unknown levels of smoothness. Indeed  we show that the posterior distribution
concentrates at the near minimax rate for α-Hölder smooth maps  performing
as well as if we knew the smoothness level α ahead of time. Our result sheds
light on architecture design for deep neural networks  namely the choice of depth 
width and sparsity level. These network attributes typically depend on unknown
smoothness in order to be optimal. We obviate this constraint with the fully Bayes
construction. As an aside  we show that SS-DL does not overﬁt in the sense that the
posterior concentrates on smaller networks with fewer (up to the optimal number
of) nodes and links. Our results provide new theoretical justiﬁcations for deep
ReLU networks from a Bayesian point of view.

1

Introduction

Deep learning constructs are powerful tools for pattern matching and prediction. Their empirical
success has been accompanied by a number of theoretical developments addressing (a) why and when
neural networks generalize well  (b) when do deep networks out-perform shallow ones and (c) which
activation functions and with how many layers. Despite the ﬂurry of research activity  there are still
many theoretical gaps in understanding why deep neural networks work so well. In this paper  we
provide several new insights by studying the speed of posterior concentration around the optimal
predictor  and in doing so we make a contribution to the Bayesian literature on deep learning rates.
Bayesian non-parametric methods are proliferating rapidly in statistics and machine learning  but
their theoretical study has not yet kept pace with their application. Lee (2000) showed consistency
of posterior distributions over single-layer sigmoidal neural networks. Our contribution builds
on this work in three fundamental aspects: (a) we focus on deep rather than single-layer  (b) we
focus on rectiﬁed linear units (ReLU) rather than sigmoidal squashing functions  (c) deploying (cid:96)0
regularization  we show that the posterior converges at an optimal speed beyond the mere fact that it
is consistent. To achieve these goals  we adopt a statistical perspective on deep learning through the
lens of non-parametric regression.
Using deep versus shallow networks can be justiﬁed theoretically in many ways. First  while both
shallow and deep neural networks (NNs) are universal approximators (i.e. can approximate any
continuous multivariate function arbitrarily well on a compact domain)  Mhaskar et al. (2017) show
that deep nets use exponentially fewer number of parameters to achieve the same level of approxi-
mation accuracy for compositional functions. Kolmogorov (1963) provided another motivation for
deep networks by showing that superpositions of univariate semi-afﬁne functions provide a universal
basis for representing multivariate functions. Telgarsky (2016) provides examples of functions that
cannot be represented efﬁciently with shallow networks and Kawaguchi et al (2017) explains why
deep networks generalize well. In related work  Poggio et al. (2017) show how deep networks can

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

avoid the curse of dimensionality for compositional functions. These theoretical results are growing
and our goal is to show how they can be leveraged to show posterior concentration rates for deep
learning. In particular  we will build on properties of deep ReLU networks characterized recently by
Schmidt-Hieber (2017).
Deep ReLU activating functions can also be justiﬁed by theory. Evidence exists that training deep
learning proceeds best when the neurons are either off or operate in a linear way. For example 
Glorot et al. (2011) show that ReLU functions outperform hyperbolic tangent or sigmoid squashing
functions  both in terms of statistical and computational performance. The success of ReLUs has
been partially attributed to their ability to avoid vanishing gradients and their expressibility. The
attractive approximation properties of ReLUs were discussed by many authors including Telgarsky
(2017)  Vitushkin (1964) or Montufar et al. (2014). Schmidt-Hieber (2017) points out a very curious
aspect of ReLU activators that their composition can yield rate-optimal reconstructions of smooth
functions of an arbitrary order  not only up to order 2 which would be expected from piecewise linear
approximators.
It is commonly perceived (Goodfellow et al.  2016) that generalizability of neural networks can
be improved with regularization. Regularization  loosely deﬁned as any modiﬁcation to a learning
algorithm that is intended to reduce its test error but not its training error (Goodfellow et al.  2016) 
can be achieved in many different ways. Beyond ReLU activators  another way to regularize is
by adding noise to the learning process. For example  the dropout regularization (Srivastava et al. 
2014) samples from (and averages over) thinned networks obtained by randomly dropping out nodes
together with their connections. While motivated as stochastic regularization  dropout can be regarded
as deterministic (cid:96)2 regularization obtained by margining out dropout noise (Wager  2014). Dropout
averaging over sparse architectures pertains  at least conceptually  to Bayesian model averaging
under spike-and-slab priors. Spike-and-slab approaches assign a prior distribution over sparsity
patterns (models) and perform model averaging with posterior model probabilities as weights (George
and McCulloch  1993). Similarly to dropout  the spike-and-slab approach effectively switches off
model coefﬁcients. However  dropout averages out patterns using equal weights rather than posterior
model probabilities. Our approach embeds (cid:96)0 regularization within the layers of deep learning and
capitalizes on its connection to Bayesian subset selection. We exploit spike-and-slab constructions not
necessarily as a tool for model selection  but rather as a fully Bayesian alternative to dropout in order
to (a) inject sparsity in deep learning to build stable network architectures  (b) achieve adaptation to
the unknown aspects of the regression function in order to achieve near-minimax performance for
estimating smooth regression surfaces.
Casting deep ReLU networks with (cid:96)0 penalization as a Bayesian hierarchical model  we study the
speed of posterior convergence around Hölder smooth regression functions. Our ﬁrst result states
that  with properly chosen width  depth and sparsity level  the convergence rate is near minimax
optimal when the smoothness is known. Going further  we show that adaptation to smoothness can
be achieved by assigning suitable complexity priors over the network width and sparsity.
The rest of the paper is outlined as follows. Section 2 describes our statistical framework for analyzing
deep learning predictors. Section 3 deﬁnes deep ReLU networks. Section 4 constructs an appropriate
spike-and-slab regularization for deep learning. Section 5 contains posterior concentration results for
sparse deep ReLU networks. Finally  Section 6 concludes with a discussion.

1.1 Notation
The ε-covering number of a set Ω for a semimetric d  denoted by E(ε; Ω; d)  is the minimal number
of d-balls of radius ε needed to cover set Ω. The notation an (cid:46) bn will be used to denote inequality
up to a constant  where an (cid:16) bn if an (cid:46) bn and bn (cid:46) an. The symbol (cid:98)a(cid:99) denotes the greatest
integer that is smaller than or equal to a  ∝ is equality up to a constant and (cid:107)f(cid:107)∞ is the supremum of
a function f.

2 Deep Learning: A Statistical Framework

Deep Learning  in its simplest form  reconstructs high-dimensional input-output mappings. To ﬁx
notation  let Y ∈ R denote a (low dimensional) output and x = (x1  . . .   xp)(cid:48) ∈ [0  1]p a (high
dimensional) set of inputs.

2

From a machine learning viewpoint  predicting an outcome from a set of features is typically framed
as noise-less non-parametric regression for recovering f0 : [0  1]p → R. Given inputs xi of training

data and outputs Yi = f0(xi) for 1 ≤ i ≤ n  the goal is to learn a deep learning architecture (cid:98)f DL
such that (cid:98)f DL
optimization problem for ﬁnding values (cid:98)B ∈ RT that minimize empirical risk (L2-recovery error on

B
i=1. Training neural networks is then positioned as an

training data) together with a regularization term  i.e.

B (x) ≈ f0(x) for x /∈ {xi}n
n(cid:88)

(cid:98)B = arg min

B

i=1

[f0(xi) − f DL

B (xi)]2 + φ(B)

(1)

where φ(B) is a penalty over the weights and offset parameters B. In practice  this is most often
carried out with some form of stochastic gradient descent (SGD) (see e.g. Polson and Sokolov (2017)
for an overview).
From a statistical viewpoint  deep learning is often embedded within non-parametric regression where
responses are linked to ﬁxed predictors in a stochastic fashion through

Yi = f0(xi) + εi 

(2)
We deﬁne by Hα
p = {f : [0  1]p → R;(cid:107)f(cid:107)Hα < ∞} the class of α-Hölder smooth functions on
a unit cube [0  1]p for some α > 0  where (cid:107)f(cid:107)Hα is the Hölder norm. The true generative model 
giving rise to (2)  will be denoted with P(n)
p   we want to reconstruct f0 with
B so that the empirical L2 distance

. Assuming f0 ∈ Hα

(cid:98)f DL

εi

f0

iid∼ N (0  1) 

1 ≤ i ≤ n.

(cid:107)(cid:98)f DL
B − f0(cid:107)2

n =

n(cid:88)
[(cid:98)f DL
B (xi) − f0(xi)]2

1
n

i=1

is at most a constant multiple away from the minimax rate εn = n−α/(2α+p) (up to a log factor).
Unlike related statistical developments (Schmidt-Hieber (2017)  Bauer and Kohler (2017))  we
approach the reconstruction problem from a purely Bayesian point of view. While the optimization
problem (1) has a Bayesian interpretation as MAP estimation under regularization priors  here we
study the behavior of the entire posterior  not just its mode.
Our approach rests on careful constructions of prior distributions π(f DL
B ) over deep learning archi-
tectures. In Bayesian non-parametrics  the quality of priors can be often quantiﬁed with the speed at
which the posterior distribution shrinks around the true regression function as n → ∞. Ideally  most
of the posterior mass should be concentrated in a ball centered around the true value f0 with a radius
proportional to the minimax rate εn. These statements are ultimately framed in a frequentist way 
describing the typical behavior of the posterior under the true generative model P(n)
In the construction of deep learning priors  a few questions emerge. How does one choose the
architecture f DL
B : how deep and what activation functions? The choice typically depends on how
quickly one can reconstruct f0. We focus on deep ReLU networks  motivated by the following
example of Mhaskar et al. (2017  remark 8).

f0

.

2.1 Motivating Example

1x2 +1)210
Mhaskar et al. (2017  remark 8) shows that the bivariate function f10(x1  x2) = (x2
can be approximated more efﬁciently by a deep ReLU neural net than a shallow combination of ridge
functions. To verify this observation  we simulate data from the following polynomial

1x2

2−x2

f1(x1  x2) = (x2

2 − x2

1x2

1x2 + 1)2

where (x1  x2) take values in [−1  1]2. We discretize the grid for a total training data of 201 ×
201 = 40401 observations. There exists an exact Kolmogorov representation for this function as a
superposition of semi-afﬁne functions if we use the identities for the inner polynomial functions

x2
1x2 =

(x1x2)2 =

1
2
1
4

1 + x2)2 − 1
(x2
2

1 − x2)2
(x2

(x1 + x2)4 +

7

4 · 33 (x1 − x2)4 − 1

2 · 33 (x1 + 2x2)4 − 23

33 (x1 + 2x2)4.

(3)

(4)

3

Following the theoretical results of Mhaskar et al. (2017)  we build an 11-layer deep ReLU network is
used to approximate this polynomial. There are 9 units in the ﬁrst hidden layer and 3 units in the further
layers. All activation functions are ReLU. For comparison  we also build a shallow network with
only 1 hidden layer but 2048 units. The MSE for the models  both trained with SGD in TensorFlow
and Keras  are: 11 layers  39 units with M SE(train) = 0.0229  M SE(validation) = 0.0112
and 1 layer  2048 units with M SE(train) = 0.0441  M SE(validation) = 0.09. Both models
outperform random forests.

3 Deep ReLU Networks

We now formally describe the generative model that gives rise to deep rectiﬁed linear unit networks.
To ﬁx notation  we write a deep neural network f DL
B (x) as an iterative mapping speciﬁed by
hierarchical layers of abstraction. With L ∈ N we denote the number of hidden layers and with
pl ∈ N the number of neurons at the lth layer. Setting p0 = p and pL+1 = 1  we denote with
p = (p0  . . .   pL+1)(cid:48) ∈ NL+2 the vector of neuron counts for the entire network. The deep network
is then characterized by a set of model parameters

B = {(W 1  b1)  (W 2  b2)  . . .   (W L  bL)} 

(5)
where bl ∈ Rpl are shift vectors and W L are pl × pl−1 weight matrixes that link neurons between the
(l − 1)th and lth layers. Nodes in the ReLU network are connected through the following activation
function σb : Rr → Rr

y1

y2
...
yr

σb

...

σ(y2 − b2)

σ(y1 − b1)
 =
  
(cid:0)W LσbL−1 . . . σb1(W 1x)(cid:1) .

σ(yr − br)

where σ(x) = ReLU (x) = max(x  0) denotes the rectiﬁed linear unit activation function.
Deep ReLU neural networks with L layers and a vector of p hidden nodes deﬁne an input-output
map f DL

B (x) : Rp → R of the form

f DL
B (x) = W L+1σbL

(6)

The representation (6) casts neural networks as nested embeddings that allow to express the data ﬂow
through a network using variable-size data structures. Varying the number of active neurons allows a
model to control the effective dimensionality for a given input and achieve desired approximation
accuracy. Similarly as Schmidt-Hieber (2017)  we will focus on a speciﬁc type of networks with an
equal number of hidden neurons  i.e. pl = 12pN for each 1 ≤ l ≤ L for some N ∈ N. We will see
later in Section 5  that the optimal network width multiplier N should relate to the dimensionality p
and smoothness α.

4 Spike-and-Slab Regularization

We focus on uniformly bounded s-sparse deep nets with bounded parameters

F(L  p  s) =(cid:8)f DL

B (x) as in (6) : (cid:107)f DL

B (cid:107)∞ < F and (cid:107)B(cid:107)∞ ≤ 1 and (cid:107)B(cid:107)0 ≤ s(cid:9)  

where s ∈ N is the sparsity level  i.e. an upper bound on the number of edges in the network  and
where F > 0.
The amount of regularization needed to achieve optimal performance typically depends on unknown
properties of functions one wishes to approximate such as their smoothness  compositional pattern
and/or the number of variables they depend on. Hierarchical Bayes procedures have the potential to
become fully adaptive and achieve (nearly) minimax performance  as if one knew these properties
ahead of time. We will leverage the fully Bayes framework and devise a hierarchical procedure which
can learn the optimal level of sparsity needed to achieve near-minimax rates of posterior convergence
of neural networks. The cornerstone of this development will be the spike-and-slab framework.
Denote with

T =

pl+1(pl + 1) − pL+1 < (12 p N + 1)L+1

(7)

L(cid:88)

l=0

4

the number of parameters in a fully connected network with L layers and a vector of p neurons 
where the inequality in (7) holds when L ≥ 2 and 12pN ≥ 2. We treat the stacked vector of model
coefﬁcients B = (β1  . . .   βT )(cid:48) in (5) as a random vector arising from the spike-and-slab prior
deﬁned hierarchically through

π(βj | γj) = γj(cid:101)π(βj) + (1 − γj)δ0(βj) 

where

is a uniform prior on an interval [−1  1]  δ0(β) is a dirac spike at zero  and where γj ∈ {0  1} for
whether or not βj is nonzero. We collate the binary indicators into a vector γ = (γ1  . . .   γT )(cid:48) ∈
{0  1}T that encodes the connectivity pattern. We assume that  given the sparsity level s = |γ|  all
architectures are equally likely a-priori  i.e.

(cid:101)π(β) =

I[−1 1](β)

1
2

π(γ | s) =

1(cid:0)T

(cid:1) .

s

(8)

(9)

(11)

The sparsity level s will be ﬁrst treated as ﬁxed and later assigned a prior with exponential decay.
The spike-and-slab construction (8) and (9) has been studied in linear models by Castillo and van
der Vaart (2012) and in trees/forests by Rockova and van der Pas (2017)  who showed that with a
suitable prior on s  the posterior can adapt to the unknown level of sparsity. We conclude a very
similar property for our proposed spike-and-slab deep learning.
It is worthwhile to point out that the prior (8) effectively zeroes out individual links rather than entire
groups of links attached to one node. The second approach was explored by Ghosh and Doshi-Velez
(2017)  who suggested assigning a Horseshoe prior on the node preactivators  diminishing inﬂuence
of individual neurons. The dropout procedure is also motivated as erasing nodes rather than links.

5 Posterior Concentration for Deep Learning

Reconstruction of f0 from the training data (Yi  xi)n
i=1 can be achieved using a Bayesian approach.
This requires placing a prior measure Π(·) on F(L  p  s)  the set of qualitative guesses of f0. Given
observed data Y (n) = (Y1  . . .   Yn)(cid:48)  inference about f0 is then carried out via the posterior distribu-
tion

(cid:16)

A(cid:12)(cid:12) Y (n) {xi}n

Π

(cid:17)

i=1

=

(cid:82)
(cid:81)n
(cid:82)(cid:81)n
i=1 Πf (Yi | xi)d Π(f )
i=1 Πf (Yi | xi)d Π(f )

A

where B is a σ-ﬁeld on F(L  p  s) and where Πf (Yi | xi) is the likelihood function for the output Yi
under f.
Our goal is to determine how fast the posterior probability measure concentrates around f0 as
n → ∞. This speed can be assessed by inspecting the size of the smallest (cid:107) · (cid:107)n-neighborhoods
around f0 that contain most of the posterior probability (Ghosal and van der Vaart  2007). For a
diameter ε > 0 and some M > 0  we denote with

∀A ∈ B 

Aε M = {f DL

B ∈ F(L  p  s) : (cid:107)f DL

B − f0(cid:107)n ≤ M ε}

the M ε-neighborhood centered around f0. Our goal is to show that

f0

εn Mn

Π(Ac

| Y (n)) → 0

-probability as n → ∞

in P(n)
(10)
for any Mn → ∞ and for εn → 0 such that n ε2
n → ∞. We will position our results using
εn = n−α/(2α+p) logδ(n) for some δ > 0  the near-minimax rate for a p-dimensional α-smooth
function. Proving techniques for statements of type (10) were established in several pioneering works
including Ghosal  Ghosh and van der Vaart (2000)  Ghosal and van der Vaart (2007)  Shen and
Wassermann (2001)  Wong and Shen (1995)  Walker et al. (2007).
The statement (10) can be proved by verifying the following three conditions (suitably adapted from
Theorem 4 of Ghosal and van der Vaart (2007)):

log E(cid:0) ε

sup
ε>εn

36 ; Aε 1 ∩ Fn;(cid:107).(cid:107)n

(cid:1) ≤ n ε2

n

5

Π(Aεn 1) ≥ e−d n ε2

n

Π(F\Fn) = o(e−(d+2) n ε2
n )

(12)
(13)
for some d > 2. Above  Fn ⊆ F(L  p  s) is an approximating space (sieve) that captures the essence
of the parameter space. Condition (11) restricts the size of the model as measured by the Le Cam
dimension (or local entropy). The Le Cam dimension  deﬁned here in terms of the log-covering
number of Aε 1∩Fn  gives rise to the minimax rate of convergence under certain conditions (Le Cam 
1973). The sieve should not be too large (Condition (11))  it should be rich enough to approximate f0
well and it should receive most of the prior mass (Condition (13)).
The prior concentration Condition (12) is needed to make sure that the prior rewards shrinking
neighborhoods of f0. This requirement is a bit at odds with Condition (11). The richer the model
class (i.e. the more layers/neurons)  the better the approximation to f0. It is essential that the prior
is supported on models that are good approximators  but that do not overﬁt. It is commonly agreed
(Ghosal and van der Vaart  2007) that the approximation gap should be no larger than a constant
multiple of εn. Below  we review some known results about expressibility of neural networks to
get insights into how many layers/neurons are needed to achieve the desired level of approximation
accuracy.

5.1 Function Class Approximation Rates

There is an extensive literature on the approximation properties of neural nets. Many tight approxima-
tion results are available for simple functions such as indicators f (x) = IB(x) where B is a unit ball
(Cheang and Barron  2000) or a half-space (Cheang (2010)  Kainen et al. (2003  2007) and K˚rkova
et al. (1997)). Recent results on the efﬁciency of ridge NNs (which arise as shallow learners of the
j x − bj) for sigmoidal σ(·)) are available in Ismailov(2017)  Klusowki and

form f =(cid:80)n

Barron (2016  2017). Pinkus (1999) and Petrushev (1999) provide some of the early bounds.
In general  one tries to characterize the asymptotic behavior of the approximation error as follows:
(14)

where f0 is a real-valued α-smooth function  (cid:98)f is the neural-network reconstruction and where N is

p ) ⇐⇒ (cid:107)f0 − (cid:98)f(cid:107) ≤ ε where N = O(ε− p

(cid:107)f0 − (cid:98)f(cid:107) = O(N

j=1 ajσ(wT

− α

the “size" of the network (typically the number of hidden nodes). Different bounds can be obtained
for different classes of f0 and different norms (cid:107) · (cid:107). The goal is to assess how complex the network
ought to be for it to approximate f0 well (up to a constant multiple of εn).
For deep networks  one also wants to ﬁnd the asymptotic behavior of the approximation error as a
function of depth  not only its size. The following Lemma will be an essential building block in the
proof of our main theorem. It summarizes the expressibility of deep ReLU networks by linking their
approximation error (when estimating Hölder smooth functions) to the network depth  width and
sparsity.
p for some α > 0. Then for any N ≥
Lemma 5.1. (Schmidt-Hieber  2017) Assume that f0 ∈ Hα
N = (p  12pN  . . .   12pN  1)  s(cid:63))(cid:48)
(15)

(α+1)p∨((cid:107)f0(cid:107)αH +1) there exists a neural network (cid:98)f ∈ F(L(cid:63)  pL(cid:63)
L(cid:63) = 8 + ((cid:98)log2(n)(cid:99) + 5)(1 + (cid:100)log2 p(cid:101))
s(cid:63) ≤ 94 p2(α + 1)2pN (L(cid:63) + (cid:100)log2 p(cid:101))

layers and sparsity level s(cid:63) satisfying

with

(16)

α ) 

such that

(cid:107)(cid:98)f − f0(cid:107)∞ ≤ (2(cid:107)f0(cid:107)Hα + 1)3p+1 N

+ (cid:107)f0(cid:107)Hα 2αN−α/p.

n

Proof. Apply Theorem 3 of Schmidt-Hieber (2017) with m = (cid:98)log2(n)(cid:99).
(cid:107)f −(cid:98)f DL(cid:107)∞ ≤ ε with sparsity s = c· ε− p
Remark 5.1. In a related result  Yarotsky (2017) shows that there exists a ReLU network that satisﬁes
α / log2(1/ε) + 1 and depth L = c· (log2(1/ε) + 1) where

c = c(p  α). Petersen and Voigtlaender (2017) extend this result to L2-smooth functions.
We assume that p = O(1) as n → ∞. Lemma (5.1) essentially states that in order to approximate
an α-Hölder smooth function with an error that is at most a constant multiple of εn  we have
to choose L (cid:16) log(n) layers with sparsity s ≤ CS(cid:98)np/(2α+p)(cid:99). This follows by setting N =
CN(cid:98)np/(2α+p)/ log(n)(cid:99).

6

6 Posterior Concentration for Sparse ReLU Networks

We formalize large sample statistical properties of posterior distributions over ReLU networks. First 
we consider a hierarchical prior distribution on F(L  p  s)  keeping L  p and s ﬁxed as if they were
known. The prior distribution now only consists of the prior on the connectivity pattern (9) and the
spike-and-slab prior on the weights/offsets (8).
Our ﬁrst result provides guidance for calibrating Bayesian deep sparse ReLU networks (choosing the
sparsity level and the number of neurons) when the level of smoothness α is known. The result can be
regarded as a Bayesian analogue of Theorem 1 of Schmidt-Hieber (2017)  who showed near-minimax
rate-optimality of a sparse multilayer ReLU network estimator that minimizes empirical least-squares.
This was the ﬁrst result on rate-optimality of deep ReLU networks in non-parametric regression 
obtained assuming that the sparsity s is known and that the function f0 is a composition of Hölder
functions. We build on this result and show that the entire posterior distribution for deep sparse ReLU
neural networks is concentrating at the near-minimax rate  when α is known and when f0 is a Hölder
smooth function. In the next section  we provide an adaptive result which no longer requires the
knowledge of α.
p   where p = O(1) as n → ∞ 
Theorem 6.1. (Deep ReLUs are near-minimax.) Assume f0 ∈ Hα
α < p and (cid:107)f0(cid:107)∞ ≤ F . Let L(cid:63) be as in (15)  s(cid:63) as in (16) and p(cid:63) = (p  12pN (cid:63)  . . .   12pN (cid:63)  1)(cid:48) ∈
NL(cid:63)+2  where N (cid:63) = CN (cid:98)np/(2α+p)/ log(n)(cid:99). Then the posterior probability concentrates at the
rate εn = n−α/(2α+p) logδ(n) for δ > 1 in the sense that

Π(f DL

B ∈ F(L(cid:63)  p(cid:63)  s(cid:63)) : (cid:107)f − f0(cid:107)n > Mn εn | Y (n)) → 0

in Pn

0 probability as n → ∞ for any Mn → ∞.

(17)

Proof. Supplementary Materials
Remark 6.1. In Theorem 5.1  we do not need to construct a sieve Fn  because s and N are ﬁxed.
We can simply take Fn = F(L(cid:63)  p(cid:63)  s(cid:63)) in which case F\Fn = ∅ and (13) holds trivially.

Theorem 6.1 continues the line of theoretical investigation of Bayesian machine learning procedures.
Lee (2000) obtained posterior consistency for single-layer sigmoidal networks. van der Pas and
Rockova (2017) and Rockova and van der Pas (2017) obtained concentration results for Bayesian
regression trees and forests. Compared to these developments  deep neural networks (NN) seem to be
more ﬂexible in estimating smooth regression functions. Indeed  trees or forests are ultimately step
function approximators and  as such  are near-minimax only for 0 < α ≤ 1 (Rockova and van der
Pas  2017). As we have shown in Theorem 6.1  Bayesian deep ReLU networks are near-minimax
when 0 < α < p  where p can be much larger than 1. One practical implication is that one would
expect NN’s to outperform trees for very smooth objects.

6.1 Bayesian Deep Learning Adapts to Smoothness

Theorem 6.1 was conceived for network architectures that are optimally tuned for α that is ﬁxed as if
it were known. However  such oracle information is rarely available  rendering the result less relevant
for practical design of networks. In this section  we devise a hierarchical prior construction (by
endowing the unknown network parameters with suitable priors)  under which the posterior performs
as well as if we knew α.
From the previous section (and discussion in Schmidt-Hieber (2017))  we know that the number of
layers L can be chosen without the knowledge of smoothness α. We will thus continue to assume
that the number of layers is ﬁxed and equal to L(cid:63) in (15).
Both the network width N and sparsity level s were chosen in an α-dependent way. To obviate this
constraint  we treat them as unknown with the following priors. For the network width multiplier N 
we deploy

π(N ) =

λN

(eλ − 1)N !

for N = 1  2  . . .

for some λ ∈ R.

(18)

7

The prior (18) is one of the classical complexity priors used frequently in the Bayesian non-parametric
literature (Coram and Lalley (2006)  Liu et al. (2017)  Rockova and van der Pas (2017)). Similarly 
the sparsity level s will be now treated as unknown with the following prior

π(s) ∝ e−λss

(19)
N = (p  12pN  . . .   12pN  1)(cid:48) ∈ NL(cid:63) the now random vector of network widths that
Denote with pL(cid:63)
depend on N and L(cid:63). Our parameter space now consists of shells of sparse deep nets with different
widths and sparsity levels  i.e.

for λs > 0.

F(L(cid:63)) =

F(L(cid:63)  pL(cid:63)

N   s) 

∞(cid:91)

T(cid:91)

N =1

s=0

Nn(cid:91)

sn(cid:91)

where T is the number of links in a fully connected network (deﬁned in (7)). We will design an
approximating sieve as follows:

Fn =

F(L(cid:63)  pL(cid:63)

N =1

s=0

N   s)

Nn = (cid:98)(cid:101)CN np/(2α+p) log2δ−1(n)(cid:99) (cid:16) nε2

(20)
for some suitable Nn ∈ N and sn ≤ T . Following our discussion earlier in this section  the sieve Fn
should be rich enough to include networks that approximate well. To this end  we choose Nn and sn
similar to the “optimal choices" obtained from the ﬁxed α case  i.e.

for (cid:101)CN > 0. With these choices  we show that the posterior distribution concentrates at the same rate
as before  but without assuming α.
Theorem 6.2. (Deep ReLUs adapt to smoothness.) Assume f0 ∈ Hα
p   where p = O(1) as n → ∞ 
α < p  and (cid:107)f0(cid:107)∞ ≤ F . Let L(cid:63) be as in (15) and assume priors (19) and (18). Then the posterior
probability concentrates at the rate εn = n−α/(2α+p) logδ(n) for δ > 1 in the sense that

sn = (cid:98)L(cid:63)Nn(cid:99) (cid:16) nε2

n/ log n 

and

n

B ∈ F(L(cid:63)) : (cid:107)f DL
0 probability as n → ∞ for any Mn → ∞.

Π(f DL

in Pn

B − f0(cid:107)n > Mn εn | Y (n)) → 0

(21)

Proof. Supplementary Materials.
Theorem 6.2 has a very important implication. It shows that  once we assign suitable complexity
priors over the network size and sparsity  we can perform as well as if we knew the smoothness α.
This type of adaptation for deep learning is  to the best of our knowledge  a new phenomenon. It
originates from the fully Bayesian treatment of deep learning. Similar adaptations were obtained for
Bayesian forests (Rockova and van der Pas (2017))  where the adaptation costs only a small fraction
of the log factor. Here  we have the same rate as in the non-adaptive case  suggesting that the analysis
could be potentially reﬁned a bit to obtain a sharper rate when α is known.
We conclude the paper with the following important corollary stating that Bayesian deep ReLU
networks with adaptive spike-and-slab priors do not overﬁt in the sense that the posterior probability
of using more than the optimal number of nodes and links goes to zero as n → ∞
Corollary 6.1. (Deep ReLUs do not overﬁt.) Under the assumptions in Theorem 6.2 we have

Π(N > Nn | Y (n)) → 0

and Π(s > sn | Y (n)) → 0

(22)

in Pn

0 probability as n → ∞.

Proof. This statement follows from Lemma 1 of Ghosal and van der Vaart (2007) and holds upon the
satisfaction of the conditions

Π(N > Nn) = o(e−(d+2)nε2
n )
that are veriﬁed in Supplementary Materials.
The key observation behind Corollary 5.1 is that the posterior does not overshoot in terms of the
width and sparsity  rewarding only small networks that are sparse. That is  the posterior concentrates
on networks with up to the optimal number sn of links. This is purely a by-product of Bayesian
regularization and  again  this property does not rely on any oracle information about α.

and Π(s > sn) = o(e−(d+2)nε2
n )

8

6.2

Implementation Considerations

For a ﬁxed architecture (i.e. N is non-random) and continuous spike-and-slab priors  one could
perform an Expectation-Maximization algorithm by iteratively (a) deploying SGD with (cid:96)1/(cid:96)2 regu-
larization and coefﬁcient speciﬁc penalties (M-step) and (b) computing conditional probability that
the coefﬁcient is non-negligible (E-step). The E-step is inexpensive and determines  one coefﬁcient
at a time  how much shrinkage should be deployed. The M-step can be readily obtained with existing
software. Such an EM strategy has been successfuly deployed in linear models (Rockova and George
(2014  2018) and related strategies have already been deployed for neural networks (via Variational
Bayes by Ullrich et al. (2017) or with Bayes by Backprop by Blundell et al. (2015)). The optimization
strategy is feasible for Gaussian/Laplace mixtures which are continuous approximations of the the
point-mass mixture prior that we analyze. Turning optimization into posterior sampling is feasible
with a weighted Bayesian bootstrap (Newton  Polson and Xu (2018)). Attaching a random weight to
each observation in the likelihood  modes of resulting posteriors constitute samples from the original
(unweighted) posterior. Regarding the adaptive architectures (when N is random)  they can be learned
as well using ideas from Liu  Rockova and Wang (2018).

7 Closing Remarks

The goal of this paper was to study posterior concentration for Bayesian deep learning and to provide
new theoretical justiﬁcations for neural networks from a Bayesian point of view. Our theoretical
results can be summarized in three points. First  in Theorem 6.1 we show that Bayesian deep ReLU
networks can be near-minimax  if tuned properly. Second  in Theorem 6.2 we show that  by assigning
suitable complexity priors over the network architecture  Bayesian deep ReLU networks can be
near-minimax tuning-free. In other words  they can adapt to unknown smoothness  giving rise
posteriors that concentrate around smooth surfaces at near-minimax rates. Third  in Corollary 6.1 we
provide some arguments for why Bayesian deep ReLU networks are less eager to overﬁt. The key
ingredients for these results were (a) sparsity through spike-and-slab regularization  (b) complexity
priors on the network width and sparsity level. Posterior concentration rate results of this type are
now slowly entering the machine learning community as a tool for (a) obtaining more insights into
Bayesian methods (van der Pas and Rockova (2017)  Rockova and van der Pas (2017)) and (b) prior
calibrations.
There are many non-parametric methods that can achieve near-minimax recovery of Hölder smooth
functions. The appeal of deep learning is their compositional structure which makes them ideal for
regression surfaces that are themselves compositions. Indeed  there is evidence that deep learning
has exponential advantage over shallow networks for approximating compositions. Schmidt-Hieber
(2017) showed that sparsely connected deep ReLU networks achieve a near-minimax rate in learning
for compositions of smooth functions. It is possible to adapt our techniques to obtain a Bayesian
analogue of his compositional result.
Generalizing the results to other activators is possible  provided that one can show that Hölder smooth
maps can be approximated well with sufﬁciently small networks. One could follow the general recipe
from Section 5 for e.g. sigmoidal functions. We focused on ReLU since they are typically preferred
over sigmoidal.

8 Acknowledgments

This work was supported by the James S. Kemper Research Fund at the University of Chicago Booth
School of Business. The authors would like to thank the anonymous referees and the area chair for
useful feedback.

9 References

Bauer  B. and Kohler  M. (2017). On Deep Learning as a remedy for the curse of dimensionality in
nonparametric regression. arXiv.

9

Blundell  C.  Cornebise  J.  Kavukcuoglu  K. and Wierstra  D. (2015). On Deep Learning as a remedy
for the curse of dimensionality in nonparametric regression. International Conference on Machine
Learning  37  1613-1622.

Castillo  I. and van der Vaart (2012). Needles and straw in a haystack: Posterior concentration for
possibly sparse sequences. Annals of Statistics  40  2069-2101.

Cheang  G. H. (2010). Approximation with neural networks activated by ramp sigmoids. Journal of
Approximation Theory  162  1450-1465.

Cheang  G. H.  and Barron  A. R. (2000). A better approximation for balls. Journal of Approximation
Theory  104  183-203.

Coram  M. and Lalley  S. (2010). Consistency of Bayes estimators of a binary regression function.
Annals of Statistics  34  1233-1269.

Dinh  R.  Pascanu  R.  Bengio  S. and Bengio  Y. (2017). Sharp Minima Can Generalize For Deep
Nets. arXiv.

George  E.I. and McCulloch  R. (1993). Variable selection via Gibbs sampling. Journal of the
American Statistical Association  88  881-889.

Ghosal  S.  Ghosh  J. and van der Vaart  A. (2000). Convergence rates of posterior distributions.
Annals of Statistics  28  500-531.

Ghosal  S. and van der Vaart  A. (2007). Convergence rates of posterior distributions for noniid
observations. Annals of Statistics  35  192-223.

Ghosh  S. and Doshi-Velez  F. (2017). Model selection in Bayesian neural networks via horseshoe
priors. Advances in Neural Information Processing Systems.

Glorot  X.  Border  A. and Bengio  Y. (2011). Deep sparse rectiﬁer neural networks. Proceedings of
the 14th International Conference on Artiﬁcial Intelligence and Statistics.

Goodfellow  I.  Bengio  Y. and Courville  A. (2016). Deep Learning. MIT Press.

Ismailov  V. (2017). Approximation by sums of ridge functions with ﬁxed directions. St. Petersburg
Mathematical Journal  28  741-772.

Kainen  P. C.  K˚urková  V.  and Vogt  A. (2003). Best approximation by linear combinations of
characteristic functions of half-spaces. Journal of Approximation Theory  122  151-159.

Kainen  P. C.  K˚rková  V.  and Vogt  A. (2007). A Sobolev-type upper bound for rates of approxi-
mation by linear combinations of Heaviside plane waves. Journal of Approximation Theory  147 
1-10.

Kawaguchi  K.  Kaelbling  L. P. and Bengio  Y. (2017). Generalization in deep learning. arXiv.

Klusowki  J.M. and Barron  A.R. (2016). Risk bounds for high-dimensional ridge function combina-
tions including neural networks. arXiv.

Klusowki  J.M. and Barron  A.R. (2017). Minimax lower bounds for ridge combinations including
neural networks. arXiv.

Kolmogorov  A. (1963). On the representation of continuous functions of many variables by su-
perposition of continuous functions of one variable and addition. American Mathematical Society
Translation  28  55?59.

K˚rková  V.  Kainen  P. C.  and Kreinovich  V. (1997). Estimates of the number of hidden units and
variation with respect to half-spaces. Neural Networks  10  1061-1068.

10

Le Cam  L. (1973). Convergence of estimates under dimensionality restrictions. Annals of Statistics 
1  38-53.

Lee  H. (2000). Consistency of posterior distributions for neural networks. Neural Networks  13 
629-642.

Liu  L.  Li  D. and Wong  W.H. (2017). Convergence rates of a partition based Bayesian multivariate
density estimation methods. Advances in Neural Information Processing Systems  30.

Liu  Y.  Rockova  V. and Wang  Y. (2018). ABC Bayesian Forests for Variable Selection. arXiv.

Mhaskar  H. N. (1996). Neural networks for optimal approximation of smooth and analytic functions.
Neural Computation  8(1)  164-177.

Mhaskar  H.  Liao  Q.  and Poggio  T. A. (2017). When and why are deep networks better than
shallow ones? In AAAI  2343-2349.

Montufar  G.F.  R. Pascanu  K. Cho and Y. Bengio (2014). On the number of linear regions of deep
neural networks. Advances in Neural Information Processing Systems  27  2924-2932.

Newton  M.  Polson  N.G. and Xu  J. (2018). Weighted Bayesian bootstrap for scalable Bayes. arXiv.

van der Pas  S. and Rockova  V. (2017). Bayesian dyadic trees and histograms for regression.
Advances in Neural Information Processing Systems.

Petersen  P. and F. Voigtlaender (2017). Optimal approximation of piecewise smooth functions using
deep ReLU neural networks. arXiv.

Petrushev  P. P. (1999). Approximation by ridge functions and neural networks. SIAM J. Math Anal. 
30  155-189.

Pinkus  A. (1999). Approximation theory of the MLP model is neural networks. Acta Numerica 
143-195.

Poggio  T.  Mhaskar  H.  Rosasco  L.  Miranda  B.  and Liao  Q. (2017). Why and when can deep-
but not shallow-networks avoid the curse of dimensionality: A review. International Journal of
Automation and Computing  14  503-519.

Polson  N. and Sokolov  V. (2017). Deep learning: a Bayesian perspective. Bayesian Analysis  12 
1275-1304.

Rockova  V. and George  E.I. (2014). EMVS: The EM Approach to Bayesian Variable Selection.
Journal of the American Statistical Association  109  828-846.

Rockova  V. and George  E.I. (2018). The Spike-and-Slab LASSO. Journal of the American Statistical
Association  113  431-444.

Rockova  V. and van der Pas  S. (2017). Posterior Concentration for Bayesian Regression Trees and
their Ensembles. arXiv.

Schmidt-Hieber  J. (2017). Nonparametric regression using deep neural networks with ReLU
activation function. arXiv:1708.06633.

Shen  X. and Wasserman  L. (2001). Rates of convergence of posterior distributions. Annals of
Statistics  29  687-714.

Srivastava  N.  Hinton  G.  Krizhevsky  A. Sutskever  I. and Salakhutdinov  R. (2015). Dropout: a
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research  15 
1929-1958.

11

Telgarsky  M. (2016). Beneﬁts of depth in neural networks. JMLR: Workshop and Conference
Proceedings  49 1-23.

Telgarsky  M. (2017). Neural Networks and Rational functions. arXiv.

Ullrich  K.  Meeds  E. and Welling  M. (2017). Soft weight-sharing for neural network compression.
International Conference on Learning Representations.

Vitushkin  A. G. (1964). Proof of the existence of analytic functions of several complex variables
which are not representable by linear superpositions of continuously differentiable functions of fewer
variables. Soviet Mathematics  5  793-796.

Walker  S.  Lijoi  A. and Prunster  I. (2007). On rates of Convergence of Posterior Distributions in
Inﬁnite Dimensional Models. Annals of Statistics  35  738-746.

Wager  S.  Wang  S. and Liang  P. (2014). Dropout training as adaptive regularization. Advances in
Neural Information Processing Systems.

Wong  W. H. and X. Shen (1995). Probability inequalities for Likelihood ratios and convergence rates
of sieve mles. Annals of Statistics  23  339-362.

Yarotsky  D. (2017). Error bounds for approximations with deep ReLU networks. Neural Networks 
94  103-114.

12

,Michael Schober
David Duvenaud
Philipp Hennig
Sagie Benaim
Lior Wolf
Nicholas Polson
Veronika Ročková