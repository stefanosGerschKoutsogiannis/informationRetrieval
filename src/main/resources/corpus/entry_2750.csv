2018,KDGAN: Knowledge Distillation with Generative Adversarial Networks,Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs  the classifier is trained by a teacher  i.e.  a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN)  which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However  it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations  we propose a three-player game named KDGAN consisting of a classifier  a teacher  and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses  the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution  we generate continuous samples to obtain low-variance gradient updates  which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.,KDGAN: Knowledge Distillation with

Generative Adversarial Networks

Xiaojie Wang

University of Melbourne
xiaojiew94@gmail.com

Yu Sun

Twitter Inc.

ysun@twitter.com

Rui Zhang∗

University of Melbourne

rui.zhang@unimelb.edu.au

Jianzhong Qi

University of Melbourne

jianzhong.qi@unimelb.edu.au

Abstract

Knowledge distillation (KD) aims to train a lightweight classiﬁer suitable to provide
accurate inference with constrained resources in multi-label learning. Instead of
directly consuming feature-label pairs  the classiﬁer is trained by a teacher  i.e.  a
high-capacity model whose training may be resource-hungry. The accuracy of the
classiﬁer trained this way is usually suboptimal because it is difﬁcult to learn the
true data distribution from the teacher. An alternative method is to adversarially
train the classiﬁer against a discriminator in a two-player game akin to generative
adversarial networks (GAN)  which can ensure the classiﬁer to learn the true data
distribution at the equilibrium of this game. However  it may take excessively long
time for such a two-player game to reach equilibrium due to high-variance gradient
updates. To address these limitations  we propose a three-player game named
KDGAN consisting of a classiﬁer  a teacher  and a discriminator. The classiﬁer and
the teacher learn from each other via distillation losses and are adversarially trained
against the discriminator via adversarial losses. By simultaneously optimizing the
distillation and adversarial losses  the classiﬁer will learn the true data distribution
at the equilibrium. We approximate the discrete distribution learned by the classiﬁer
(or the teacher) with a concrete distribution. From the concrete distribution  we
generate continuous samples to obtain low-variance gradient updates  which speed
up the training. Extensive experiments using real datasets conﬁrm the superiority
of KDGAN in both accuracy and training speed.

1

Introduction

In machine learning  it is common that more resources such as input features [47] or computational
resources [23]  which we refer to as privileged provision  are available at the stage of training a model
than those available at the stage of running the deployed model (i.e.  the inference stage). Figure 1
shows an example application of image tag recommendation  where more input features (called
privileged information [47]) are available at the training stage than those available at the inference
stage. Speciﬁcally  the training stage has access to images as well as image titles and comments
(textual information) as shown in Figure 1a  whereas the inference stage only has access to images
themselves as shown in Figure 1b. After a smart phone user uploads an image and is about to provide
tags for the image  it is inconvenient to type tags on the phone and thinking about tags for the image
also takes time  so it is very useful to recommend tags based on the image as shown in Figure 1b.
Another example application is unlocking mobile phones by face recognition. We usually deploy face
recognition models on mobile phones so that legit users can unlock the phones without depending

∗Corresponding author

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

s1: A user uploads an image.

s3: The user adds a title.

Lake

Lake mead

Nice lake.

s2: The user adds a tag.

s4: Another user comments.

New post
Add some tags...
Recommended Tags

lake

sky

(a) Training: After a user uploads an image  additional text such as
comments and titles besides the labeled tags is accumulated.

(b) Inference: We recommend bay and
sky right after an image is uploaded.

Figure 1: Image tag recommendation where the additional text is only available for training.

on remote services or internet connections. The training stage may be done on a powerful server
with signiﬁcantly more computational resources than the inference stage  which is done on a mobile
phone. Here  a key problem is how to use privileged provision  i.e.  resources only accessible for
training  to train a model with great inference performance [29].
Typical approaches to the problem are based on knowledge distillation (KD) [7  9  23]. As shown
by the left half of Figure 2  KD consists of a classiﬁer and a teacher [29]. To operate for resource-
constrained inference  the classiﬁer does not use privileged provision. On the other hand  the teacher
uses privileged provision by  e.g.  having a larger model capacity or taking more features as input.
Once trained  the teacher outputs a distribution over labels called soft labels [29] for each training
instance. Then  the teacher trains the classiﬁer to predict the soft labels via a distillation loss such as
the L2 loss on logits [7]. This training process is often called “distilling” the knowledge in the teacher
into the classiﬁer [23]. Since the teacher normally cannot perfectly model the true data distribution  it
is difﬁcult for the classiﬁer to learn the true data distribution from the teacher.
Generative adversarial networks (GAN) provide an alternative way to learn the true data distribution.
Inspired by Wang et al. [49]  we ﬁrst present a naive GAN (NaGAN) with two players. As shown by
the right part of Figure 2  NaGAN consists of a classiﬁer and a discriminator. The classiﬁer serves as
a generator that generates relevant labels given an instance while the discriminator aims to distinguish
the true labels from the generated ones. The classiﬁer learns from the discriminator to perfectly
model the true data distribution at the equilibrium via adversarial losses. One limitation of NaGAN is
that a large number of training instances and epochs is normally required to reach equilibrium [15] 
which restricts its applicability to domains where collecting labeled data is expensive. The slow
training speed is because in such a two-player framework  the gradients from the discriminator to
update the classiﬁer often vanish or explode during the adversarial training [4]. It is challenging to
train a classiﬁer to learn the true data distribution with limited training instances and epochs.
To address this challenge  we propose a three-player framework named KDGAN to distill knowledge
with generative adversarial networks. As shown in Figure 2  KDGAN consists of a classiﬁer  a
teacher  and a discriminator. In addition to the distillation loss in KD and the adversarial losses
in NaGAN mentioned above  we deﬁne a distillation loss from the classiﬁer to the teacher and an
adversarial loss between the teacher and the discriminator. Speciﬁcally  the classiﬁer and the teacher 
serving as generators  aim to fool the discriminator by generating pseudo labels that resemble the true
labels. Meanwhile  the classiﬁer and the teacher try to reach an agreement on what pseudo labels to
generate by distilling their knowledge into each other. By formulating the distillation and adversarial
losses as a minimax game  we enable the classiﬁer to learn the true data distribution at the equilibrium
(see Section 3.2). Besides  the classiﬁer receives gradients from the teacher via the distillation loss
and the discriminator via the adversarial loss. The gradients from the teacher often have low variance 
which reduces the variance of gradients and thus speeds up the adversarial training (see Section 3.3).
We further consider reducing the variance of the gradients from the discriminator to accelerate the
training of KDGAN. The gradients from the discriminator may have large variance when obtained
through the widely used policy gradient methods [49  52]. It is non-trivial to obtain low-variance
gradients from the discriminator because the classiﬁer and the teacher generate discrete samples 
which are not differentiable w.r.t. their parameters. We propose to relax the discrete distributions
learned by the classiﬁer and the teacher into concrete distributions [25  31] with the Gumbel-Max
trick [20  30]. We use the concrete distributions for generating continuous samples to enable end-
to-end differentiability and sufﬁcient control over the variance of gradients. Given the continuous
samples  we obtain low-variance gradients from the discriminator to accelerate the KDGAN training.
To summarize  our contributions are as follows:

2

• We propose a novel framework named KDGAN for multi-label learning  which trains a lightweight
classiﬁer suitable for resource-constrained inference using resources available only for training.
• We reduce the number of training epochs required to converge by decreasing the variance of
• We conduct extensive experiments in two applications  image tag recommendation and deep model
compression. The experiments validate the superiority of KDGAN over state-of-the-art methods.

gradients  which is achieved by the design of KDGAN and the Gumbel-Max trick.

2 Related Work

We brieﬂy review studies on knowledge distillation (KD) and generative adversarial networks (GAN).
KD aims to transfer the knowledge in a powerful teacher to a lightweight classiﬁer [9]. For example 
Ba and Caruana [7] train a shallow classiﬁer network to mimic a deep teacher network by matching
logits via the L2 loss. Hinton et al. [23] generalize this work by training a classiﬁer to predict
soft labels provided by a teacher. Sau and Balasubramanian [39] further add random perturbations
into soft labels to simulate learning from multiple teachers. Instead of using soft labels  Romero
et al. [36] propose to use middle layers of a teacher to train a classiﬁer. Unlike previous work on
classiﬁcation problems  Chen et al. [10] apply KD and hint learning to object detection problems.
There also exists work that leverages KD to transfer knowledge between different domains [21] 
e.g.  between high-quality and low-quality images [41]. Lopez-Paz et al. [29] unify KD with
privileged information [35  47  48] as generalized distillation where a teacher is pretrained by taking
as input privileged information. Compared to KD  the proposed KDGAN framework introduces a
discriminator to guarantee that the classiﬁer can learn the true data distribution at the equilibrium.
GAN is initially proposed to generate continuous data by training a generator and a discriminator
adversarially in a minimax game [17]. GAN has only recently been introduced to generate discrete
data [16  54  55] because discrete data makes it difﬁcult to pass gradients from a discriminator
backward to update a generator. For example  sequence GAN (SeqGAN) [52] models the process
of token sequence generation as a stochastic policy and adopts Monte Carlo search to update a
generator. Different from these GANs with two players  Li et al. propose a GAN with three players
called Triple-GAN [13]. Our KDGAN also consists of three players including two generators
and a discriminator  but differs from Triple-GAN in that: (1) Both generators in KDGAN learn a
conditional distribution over labels given features. However  the generators in Triple-GAN learn a
conditional distribution over labels given features and a conditional distribution over features given
labels  respectively. (2) The samples from both generators in KDGAN are all discrete data while
the samples from the generators in Triple-GAN include both discrete and continuous data. These
differences lead to different objective functions and training techniques  e.g.  KDGAN can use the
Gumbel-Max trick [20  30] to generate samples from both generators while Triple-GAN cannot do
this. There is also a rich body of studies on improving the training of GAN [5  33  56] such as feature
matching [38]  which are orthogonal to our work and can be used to improve the training of KDGAN.
We explore the idea of integrating KD and GAN. A similar idea has been studied in [51] where a
discriminator is introduced to train a classiﬁer. This previous study [51] differs from ours in that their
discriminator trains the classiﬁer to learn the data distribution produced by the teacher  while our
discriminator trains the classiﬁer to learn the true data distribution.
We apply the proposed KDGAN to address the problem of deep model compression and image
tag recommendation. We can also apply KDGAN to address the other problems where privileged
provision is available [44]. For example  we can consider contextual signals in the intent tracking
problem [42  43] or user reviews in the movie recommendation problem [50] as privileged provision.

3 Methods

We study the problem of training a lightweight classiﬁer from a teacher that is trained with privileged
provision (denoted by ) to satisfy stringent inference requirements. The inference requirements may
include (1) running in real time with limited computational resources  where privileged provision
is computational resources [23]; (2) lacking a certain type of input features  where privileged
provision is privileged information [47]. Following existing work [29]  we use multi-label learning
problems [12  18  53] as the target application scenarios of our methods for illustration purpose.

3

Teacher

Lc

DS

x

KDGAN

Classiﬁer

KD
t (y|x)
st = p
sc = pc(y|x)
yt ∼ q
t (y|x)

x

NaGAN

DS

Lt
yc ∼ qc(y|x)

Discriminator

x

AD

Ln
Lp
Ln
y ∼ pu(y|x)

AD

AD

Figure 2: Comparison among KD  NaGAN  and KDGAN. The classiﬁer (C) and the teacher (T )
learn discrete categorical distributions pc(y|x) and p
t (y|x); y is a true label generated from the true
data distribution pu(y|x); yc and yt are continuous samples generated from concrete distributions
qc(y|x) and q
DS are distillation
losses for C and T ; Lp
AD are adversarial losses for positive and negative feature-label pairs.

t (y|x); sc and st are soft labels produced by C and T ; Lc

DS and Lt

AD and Ln

Since privileged provision is only available at the training stage  the goal of the problem is to train a
lightweight classiﬁer that does not use privileged provision for effective inference.
To achieve this goal  we start with NaGAN  a naive adaptation of the two-player framework proposed
by Wang et al. in information retrieval (Section 3.1). Similar to other two-player frameworks [49] 
the naive adaptation requires a large number of training instances and epochs [15]  which is difﬁcult
to satisfy in practice [4]. To address the limitation  we propose a three-player framework named
KDGAN that can speed up the training while preserving the equilibrium (Sections 3.2 and 3.3).

3.1 NaGAN Formulation

We begin with NaGAN that combines a classiﬁer C with a discriminator D in a minimax game.
Since D is not meant for inference  it can leverage privileged provision. For example  D may have a
larger model capacity than C or take as input more features than those available to C. In NaGAN 
C generates pseudo labels y given features x following a categorical distribution pc(y|x)  while D
d(x  y) of a label y being from the true data distribution pu(y|x) given
computes the probability p
features x. With a slight abuse of notation  we also use x to refer to features including privileged
information when the context is clear. Following the value function of IRGAN [49]  we deﬁne the
value function V (c  d) for the minimax game in NaGAN as

min

c

max

d

V (c  d) = Ey∼pu [log p

d(x  y)] + Ey∼pc [log(1 − p

d(x  y))].

(1)

and p

d(x  y) = sigmoid(g(x  y)).

pc(y|x) = softmax(h(x  y))

Let h(x  y) and g(x  y) be the scoring functions for C and D. We deﬁne pc(y|x) and p

d(x  y) as
(2)
The scoring functions can be implemented in various ways  e.g.  h(x  y) can be a multilayer per-
ceptron [27]. We will detail the scoring functions for speciﬁc applications in Section 4. Such a
two-player framework is trained by updating C and D alternatively [49]. The training will proceed
until the equilibrium is reached  where C learns the true data distribution. At that point  D can do no
better than random guesses at deciding whether a given label is generated by C or not [6].
Our key observation is that the advantages and the disadvantages of KD and NaGAN are com-
plementary: (1) KD usually requires a small number of training instances and epochs but cannot
ensure the equilibrium where pc(y|x) = pu(y|x). (2) NaGAN ensures the equilibrium where
pc(y|x) = pu(y|x) [49] but normally requires a large number of training instances and epochs. We
aim to retain the advantages and avoid the disadvantages of both methods in a single framework.

3.2 KDGAN Formulation

We formulate KDGAN as a minimax game with a classiﬁer C  a teacher T   and a discriminator D.
Similar to the classiﬁer C  the teacher T generates pseudo labels based on a categorical distribution
t (y|x) = softmax(f (x  y)) where f (x  y) is also a scoring function. Both T and D use privileged
p
provision  e.g.  by having a large model capacity or taking privileged information as input. In KDGAN 
D aims to maximize the probability of correctly distinguishing the true and pseudo labels  whereas C
and T aim to minimize the probability that D rejects their generated pseudo labels. Meanwhile  C
learns from T by mimicking the learned distribution of T . To build a general framework  we also
enable T to learn from C because  in reality  a teacher’s ability can also be enhanced by interacting
with students (see Figure 6 in Appendix D for empirical evidence that T beneﬁts from learning from

4

Algorithm 1: Minibatch stochastic gradient descent training of KDGAN.
1 Pretrain a classiﬁer C  a teacher T   and a discriminator D with the training data {(x1  y1)  ...  (xn  yn)}.
2 for the number of training epochs do
3
4
5

for the number of training steps for the discriminator do

k} from pu(y|x)  qc(y|x)  and q

k}  and {yt

t (y|x).

1  ...  yc

1  ...  yt

i=1

(cid:0)∇d log p

Sample labels {y1  ...  yk}  {yc
Update D by ascending along its gradients
1
k

d(x  yi) + α∇d log(1 − p
for the number of training steps for the teacher do
i|x) log(1 − p

(cid:80)k
(cid:80)k
1  ...  yt
i=1(1 − α)∇t log q
(cid:80)k
1  ...  yc
i=1 α∇c log qc(yc

for the number of training steps for the classiﬁer do

Sample labels {yc

Sample labels {yt

d(x  zc

1
k

1
k

t (y|x) and update the teacher by descending along its gradients

k} from q
t (yt
k} from qc(y|x) and update C by descending along its gradients
i|x) log(1 − p

t (y|x)  pc(y|x)).

DS(pc(y|x)  p

i )) + β∇cLc

i )) + γ∇tLt

t (y|x)).

d(x  zt

DS(p

d(x  zc

i )) + (1 − α)∇d log(1 − p

d(x  zt

i ))(cid:1).

6
7
8

9
10
11

12

C). Such a mutual learning helps C and T reduce their probability of generating different pseudo
labels. Formally  we deﬁne the value function U (c  t  d) for the minimax game in KDGAN as

U (c  t  d) = Ey∼pu [log p

d(x  y)] + αEy∼pc[log(1 − p

t

d

max

[log(1 − p

DS(pc(y|x)  p

d(x  y))] + βLc

d(x  y))]
t (y|x)) + γLt

min
c t
+ (1 − α)Ey∼p
where α ∈ (0  1)  β ∈ (0  +∞)  and γ ∈ (0  +∞) are hyperparameters. We collectively refer to
the expectation terms as the adversarial losses and refer to Lc
DS as the distillation losses.
The distillation losses can be deﬁned in several ways [39]  e.g.  the L2 loss [7] or Kullback–Leibler
divergence [23]. Note that Lc
DS are used to train the classiﬁer and the teacher  respectively.
Theoretical Analysis. We show that the classiﬁer perfectly learns the true data distribution at the
t (y|x). It can be shown
equilibrium of KDGAN. To see this  let p
α(y|x):
that the adversarial losses w.r.t. pc(y|x) and p

t (y|x) are equal to an adversarial loss w.r.t. p

α(y|x) = αpc(y|x) + (1 − α)p

t (y|x)  pc(y|x)) 

DS and Lt

DS and Lt

DS(p

(3)

αEy∼pc[log(1 − p

d(x  y))] + (1 − α)Ey∼p

y pc(y|x) log(1 − p

(cid:0)αpc(y|x) + (1 − α)p

d(x  y)) + (1 − α)(cid:80)
t (y|x)(cid:1) log(1 − p

[log(1 − p
y p
d(x  y))

t

= α(cid:80)
=(cid:80)

y

d(x  y))]
t (y|x) log(1 − p

d(x  y))

(4)

= Ey∼p

α [log(1 − p

d(x  y))].
DS(pc(y|x)  p

Therefore  let LMD = βLc
Shannon divergence  the value function U (c  t  d) of the minimax game can be rewritten as

t (y|x)  pc(y|x)) and LJS be the Jensen-

t (y|x)) + γLt

DS(p

min

α

max

α [log(1 − p
DS(pc(y|x)  p

d(x  y))] + LMD
t (y|x)) + γLt

Ey∼pu[log p
2LJS(pu(y|x)||p

d(x  y)] + Ey∼p
α(y|x)) + βLc

DS(p
α(y|x) = pu(y|x) and Lc

(5)
α
d
t (y|x)  pc(y|x)) − log(4).
= min
Here  LJS reaches the minimum if and only if p
DS) reaches the
minimum if and only if pc(y|x) = p
t (y|x). Hence  the KDGAN equilibrium is reached if and only
t (y|x) = pu(y|x) where the classiﬁer learns the true data distribution. We summarize
if pc(y|x) = p
the above discussions in Lemma 4.1 (the necessary and sufﬁcient conditions of maximizing the value
function) and Theorem 4.2 (achieving the equilibrium)  respectively (see Appendix A for proofs).
Lemma 4.1. For any ﬁxed classiﬁer and teacher  the value function U (c  t  d) is maximized if and
only if the distribution of the discriminator is given by p
Theorem 4.2. The equilibrium of the minimax game minc t maxd U (c  t  d) is achieved if and only
if pc(y|x) = p

d(x  y) = pu(y|x)/(pu(y|x)+p
t (y|x) = pu(y|x). At that point  U (c  t  d) reaches the value − log(4).

DS (or Lt

α(y|x)).

3.3 KDGAN Training

In this section  we detail techniques for accelerating the training speed of KDGAN via reducing the
number of training epochs needed. As discussed in earlier studies [8  46]  the training speed is closely
related to the variance of gradients. Comparing with NaGAN  the KDGAN framework by design
can reduce the variance of gradients. This is because the high variance of a random variable can

5

be reduced by a low-variance random variable (detailed in Lemma 4.3) and as we will discuss  T
provides gradients of lower variance than D does. To reduce the variance of gradients from D and
attain sufﬁcient control over the variance  we further propose to obtain gradients from a continuous
space by relaxing the discrete samples  i.e.  pseudo labels  propagated between the classiﬁer (or the
teacher) and the discriminator into continuous samples with a reparameterization trick [25  31].
First  we show how KDGAN reduces the variance of gradients. As discussed above  C only receives
gradients ∇cV from D in NaGAN while it receives gradients ∇cU from both D and T in KDGAN:
AD  ∇cU = λ∇cLn
(6)
where λ ∈ (0  1)  ∇cLn
DS are gradients from D and T   respectively. Consistent with the
ﬁndings in existing work [23  39]  we also observe that ∇cLc
DS usually has a lower variance than
∇cLn
AD (see Figure 7 in Appendix D for empirical evidence that the variance of ∇cLc
DS is smaller
than that of ∇cLn
AD during the training process). Hence  it can be easily shown that the gradients
w.r.t. C in KDGAN have a lower variance than that in NaGAN (refer to Lemma 4.3):

∇cV = ∇cLn
AD and ∇cLc

AD + (1 − λ)∇cLc

DS 

Var(∇cLc

DS) ≤ Var(∇cLn

AD) ⇒ Var(∇cU ) ≤ Var(∇cV ).

(7)

∇cLn

Next  we further reduce the variance of gradients with a reparameterization trick  in particular  the
Gumbel-Max trick [20  30]. The essence of the Gumbel-Max trick is to reparameterize generating
discrete samples into a differentiable function of its parameters and an additional random variable
of a Gumbel distribution. To perform the Gumbel-Max trick on generating discrete samples from
the categorical distribution pc(y|x)  a concrete distribution [25  31] can be used. We use a concrete
distribution qc(y|x) to generate continuous samples and use the continuous samples to compute the
gradients ∇cLn
(8)
Here  z = onehot(argmax y) is a discrete pseudo label where y ∼ qc(y|x). We deﬁne qc(y|x) as

AD = ∇cEy∼pc[log(1 − p
d(x  y))] = Ey∼qc [∇c log qc(y|x) log(1 − p
(cid:32)
log pc(y|x) + g

AD of the adversarial loss w.r.t. the classiﬁer as

(9)
Here  τ ∈ (0  +∞) is a temperature parameter and Gumbel(0  1) is the Gumbel distribution2 [31].
We leverage the temperature parameter τ to control the variance of gradients over the training.
With a high temperature  the samples from the concrete distribution are smooth  which give low-
variance gradient estimates. Note that a disadvantage of the concrete distribution is that with a high
temperature  it becomes a less accurate approximation to the original categorical distribution  which
causes biased gradient estimates. We will discuss how to tune the temperature parameter in Section 4.
In addition to improving the training of C  we also apply the same techniques to improve the training
of T . We update D with the back-propagation algorithm [37] (detailed in Appendix B). The overall
logic of the KDGAN training is summarized in Algorithm 1. The three players can be ﬁrst pretrained
separately and then trained alternatively via minibatch stochastic gradient descent.

qc(y|x) = softmax

g ∼ Gumbel(0  1).

d(x  z))].

(cid:33)

 

τ

4 Experiments

The proposed KDGAN framework can be applied to a wide range of multi-label learning tasks where
privileged provision is available. To show the applicability of KDGAN  we conduct experiments
with the tasks of deep model compression (Section 4.1) and image tag recommendation (Section 4.2).
Note that privileged provision is referred to as computational resources in deep model compression
and privileged information in image tag recommendation  respectively.
We implement KDGAN based on Tensorﬂow [1] and here we brieﬂy describe our experimental setup3.
We use two formulations of the distillation losses including the L2 loss [7] and the Kullback–Leibler
divergence [23]. The two formulations exhibit comparable results and the results presented are based
on the L2 loss [7]. Since both T and D can use privileged provision  we implement their scoring
functions f (x  y) and g(x  y) using the same function s(x  y) but with different sets of parameters.
We search for the optimal values for the hyperparameters α in [0.0  1.0]  β in [0.001  1000]  and γ in
[0.0001  100] based on validation performance. We ﬁnd that a reasonable annealing schedule for the
temperature parameter τ is to start with a large value (1.0) and exponentially decay it to a small value
(0.1). We leave the exploration of the optimal schedule for future work.
2 The Gumbel distribution can be sampled by drawing u ∼ Uniform(0  1) and computing g = − log(− log u).
3 The code and the data are made available at https://github.com/xiaojiew1/KDGAN/.

6

Table 1: Average accuracy over 10 runs in model compression (n is the number of training instances).

Method

n = 100
74.02 ± 0.13
CODIS
68.34 ± 0.06
DISTN
66.53 ± 0.18
NOISY
67.35 ± 0.15
MIMIC
NaGAN 64.90 ± 0.31
KDGAN 77.95 ± 0.05

MNIST

n = 1  000
95.77 ± 0.10
93.97 ± 0.08
93.45 ± 0.11
93.78 ± 0.13
93.60 ± 0.22
96.42 ± 0.05

n = 10  000
98.89 ± 0.08
98.79 ± 0.07
98.58 ± 0.11
98.65 ± 0.05
98.95 ± 0.19
99.25 ± 0.02

n = 500
54.17 ± 0.20
50.92 ± 0.18
50.18 ± 0.28
51.74 ± 0.23
46.29 ± 0.32
57.56 ± 0.13

CIFAR-10

n = 5  000
77.82 ± 0.14
76.59 ± 0.15
75.42 ± 0.19
75.66 ± 0.17
76.11 ± 0.24
79.36 ± 0.04

n = 50  000
85.12 ± 0.11
83.32 ± 0.08
82.99 ± 0.12
84.33 ± 0.10
85.34 ± 0.27
86.50 ± 0.04

(a) Deep model compression over MNIST.

(b) Image tag recommendation on YFCC100M.

Figure 3: Training curves of the classiﬁer in the proposed NaGAN and KDGAN.

4.1 Deep Model Compression

Deep model compression aims to reduce the storage and runtime complexity of deep models and
to improve the deployability of such models on portable devices such as smart phones. Extensive
computational resources available for training are considered privileged provision in this task.
Dataset and Setup. We use the widely adopted MNIST [27] and CIFAR-10 [26] datasets. The MNIST
dataset has 60 000 grayscale images (50 000 for training and 10 000 for testing) with 10 different label
classes. Following an earlier work [39]  we do not preprocess the images on MNIST. The CIFAR-10
dataset has 60 000 colored images (50 000 for training and 10 000 for testing) with 10 different
label classes. We preprocess the images by subtracting per-pixel mean  and we augment the training
data by mirrored images. We vary the number of training instances in [100  10000] on MNIST and
in [500  50000] on CIFAR-10. The scoring functions h(x  y) and s(x  y) are implemented as an
MLP (1.2M parameters) and a LeNet (3.1M parameters) on MNIST; while h(x  y) and s(x  y) are
implemented as a LeNet (0.5M parameters) and a ResNet (1.7M parameters) on CIFAR-10 (detailed
in Appendix C). We evaluate various methods over 10 runs with different initialization of C and
report the mean accuracy and the standard deviation. Since the focus of this paper is to achieve a
better accuracy for a given architecture of the classiﬁer  we defer the discussion on the classiﬁer’s
ratio of compression and loss of accuracy w.r.t. the teacher to Table 3 in Appendix D.
Results and Discussions. First  we compare the proposed NaGAN and KDGAN with KD-based
methods including MIMIC [7]  DISTN [23]  NOISY [39]  and CODIS [2]. The results obtained by
varying the number of training images on MNIST and CIFAR-10 are summarized in Table 1. On both
datasets  KDGAN consistently outperforms the KD-based methods by a large margin. For example 
KDGAN achieves as much as 5.31% performance gain with 100 training images on MNIST. We
further compare NaGAN with the KD-based methods. We observe that NaGAN performs better
when a large amount of training data are available (e.g.  50 000 training images on CIFAR-10) while
KD-based methods perform better when a small number of training images are available (e.g.  500
training images on CIFAR-10). This is consistent with our analysis in Section 3.1 that NaGAN can
learn the true data distribution better  although this requires a large amount of training data.
Then  we compare NaGAN with KDGAN. As shown in Table 1  KDGAN achieves a larger per-
formance gain over NaGAN with fewer training instances. This indicates that KDGAN requires a
smaller number of training instances than NaGAN does to reach the same level of accuracy. This
can be explained by that KDGAN introduces T to provide soft labels for training C. The soft labels
generally have high entropy and reveal much useful information about each training instance. Hence 
the soft labels impose much more constraint on the parameters of C than the true labels  which can
reduce the number of training instances required to train C. We further investigate the training speed

7

04080120160200Training Epochs0.00.20.40.60.8AccuracyDISTNCODISNaGANKDGAN-WO-GMKDGAN080160240320400Training epochs0.00.10.20.3P@3TPROPREXMPNaGANKDGAN-WO-GMKDGAN(a) Effect of varying α

(b) Effect of varying β

(c) Effect of varying γ

Figure 4: Effects of hyperparameters in KDGAN on MNIST for deep model compression.

of NaGAN and KDGAN by the number of training epochs. Typical learning curves of C in NaGAN
and KDGAN are shown in Figure 3a. Due to the page limit  we only show the results using 100
training images on MNIST. We ﬁnd that KDGAN converges to a better accuracy with a smaller
number of training epochs (about 25 epochs) than NaGAN (about 135 epochs). After convergence 
the training curve in KDGAN is more stable than that in NaGAN. Moreover  we investigate the
beneﬁt provided by the Gumbel-Max trick for the KDGAN training. We perform the KDGAN
training without using the Gumbel-Max trick (referred to as KDGAN-WO-GM) and also plot the
accuracy against training epochs in Figure 3a. By comparing KDGAN with KDGAN-WO-GM  we
can see that the Gumbel-Max trick speeds up the training process by around 45% in terms of training
epochs. The Gumbel-Max trick also helps improve the accuracy from 0.7605 to 0.7795 (by around
2.5%). One possible reason is that the Gumbel-Max trick effectively reduces the gradient variance
from the discriminator as discussed in Section 3.3. This is also observed in our experiments  e.g.  by
comparing the gradient variance from the adversarial loss not using the Gumbel-Max trick in Figure
7a with the one using the Gumbel-Max trick in Figure 7b (see Appendix D for details).
Next  we study the reasons for the higher accuracy of KDGAN. We present how the accuracy of
KDGAN varies against the hyperparameters on the MNIST dataset in Figure 4 (Note the logarithmic
scale of the x-axis in Figures 4b and 4c). We ﬁnd that α and β have a relatively small effect on the
accuracy  which suggests that KDGAN is a robust framework. Besides  if we set β to a small value
(0.0001)  we get more than 2% accuracy drop when KDGAN is trained with 100 training instances.
This shows that T is important in training C when the number of training instances is small. We
further ﬁnd that a large value of γ causes the accuracy to deteriorate rapidly. This is because the
soft labels provided by C are usually noisy. Emphasizing on training T to predict the noisy labels
decreases the accuracy of T   which in turn decreases the accuracy of C. We obtain similar results for
the effects of the hyperparameters on the CIFAR-10 dataset.

4.2

Image Tag Recommendation

Image tag recommendation aims to recommend relevant tags (i.e.  labels) after a user uploads an
image to image-hosting websites such as Flickr4. As discussed before  we aim to recommend relevant
tags right after a user uploads an image. This way  the user can just select from the recommended
tags instead of inputting tags. Users may continue to add additional text for an uploaded image such
as image titles and descriptions. We only use such additional text at the training stage as privileged
information used by the teacher and the discriminator only. At the inference stage  our trained model
(i.e.  the classiﬁer) only takes an image as input to make tag recommendations.
Dataset and Setup. We use the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset5
in the experiments [45]. To simulate the case where additional text about images is available for
training  we randomly sample 20 000 images with titles or descriptions for training and another 2 000
images for testing. We create a dataset of images labeled with the 200 most popular tags and another
dataset of images labeled with 200 randomly sampled tags. Following an earlier study [3]  we use a
VGGNet [40] pretrained on ImageNet [14] to extract image features and a LSTM [24] with pretrained
word embeddings [34] to learn text features. We implement h(x  y) as an MLP with image features
as input and implement s(x  y) as an MLP with the element-wise product of image and text features
as input (detailed in Appendix C). We use precision (P@N)  F-score (F@N)  mean average precision
(MAP)  and mean reciprocal ranking (MRR) to evaluate performance.

4 https://www.flickr.com/.

5 Yahoo Webscope Program. http://webscope.sandbox.yahoo.com/.

8

.95.97.99n=100n=1000n=100000.00.20.40.60.81.0α.73.75.77Accuracy.95.97.99n=100n=1000n=10000-3-2-10123log10 β.73.75.77Accuracy-4-3-2-1012log10 γ0.60.70.80.91.0n=100n=1000n=10000AccuracyTable 2: Performance of various methods on the YFCC100M dataset in tag recommendation.

Method

P@3
.2320
KNN
.2420
TPROP
.2560
TFEAT
.2720
REXMP
NaGAN .2892
KDGAN .3047

Most Popular Tags

Randomly Sampled Tags

P@5
.1680
.1636
.1752
.1800
.1880
.1968

F@3
.2339
.2811
.2871
.3324
.3516
.3678

F@5 MAP MRR P@3
.1633
.1623
.1883
.1949
.2002
.1999
.2228
.2295
.2415
.2352
.2526
.2572

.5755
.6177
.6417
.7015
.7432
.7787

.5852
.6270
.6503
.7122
.7555
.7905

P@5
.1198
.1372
.1420
.1378
.1495
.1666

F@3
.1575
.1810
.2195
.2427
.2693
.2946

F@5 MAP MRR
.1088
.4092
.4636
.1252
.5309
.1495
.5331
.1669
.5911
.1867
.2009
.6452

.3970
.4512
.5149
.5205
.5791
.6302

(a) Effect of varying α

(b) Effect of varying β

(c) Effect of varying γ

Figure 5: Effects of hyperparameters in KDGAN on YFCC100M for image tag recommendation.

Results and Discussions. First  we compare C in KDGAN with KNN [32]  TPROP [19]  TFEAT [11] 
and REXMP [28]. The overall results are presented in Table 2. We ﬁnd that KDGAN achieves
signiﬁcant improvements over the other methods across all the measures. Although KDGAN does
not explicitly model the semantic similarity between two labels like what REXMP does  it still makes
better recommendations than REXMP does. The reason is that in KDGAN  T provides C with soft
labels at training. The soft labels contain a rich similarity structure over tags which cannot be modeled
well by any pairwise similarity between tags used in REXMP. For example  an image labeled with a
tag volleyball is supplied with a soft label assigning a probability of 10−2 to basketball  10−4
to baseball  and 10−8 to dragonfly. The reason that T generalizes is reﬂected in the relative
probabilities over tags  which can be used for guiding C to generalize better.
Next  we compare the training curves of NaGAN  KDGAN-WO-GM  and KDGAN. We only plot
the performance measured by P@3 in Figure 3b because the other measures exhibit similar training
curves. We ﬁnd that KDGAN learns a more accurate classiﬁer with a smaller number of training
epochs (about 100 epochs) than NaGAN (about 220 epochs) and KDGAN-WO-GM (about 150
epochs). After convergence  KDGAN consistently outperforms the best baseline REXMP.
Last  we investigate how the performance of KDGAN varies against the hyperparameters over
the YFCC100M dataset. The results are summarized in Figure 5  which are consistent with our
observations in the task of deep model compression.

5 Conclusion

We proposed a framework named KDGAN to distill knowledge with generative adversarial networks
for multi-label learning with privileged provision. We have deﬁned the KDGAN framework as a
minimax game where a classiﬁer  a teacher  and a discriminator are trained adversarially. We have
proved that the minimax game has an equilibrium where the classiﬁer perfectly models the true
data distribution. We use the concrete distribution to control the variance of gradients during the
adversarial training and obtained low-variance gradient estimates to accelerate the training. We have
shown that KDGAN outperforms the state-of-the-art methods in two important applications  image
tag recommendation and deep model compression. We show that KDGAN learns a more accurate
classiﬁer at a faster speed than a naive GAN (NaGAN) does. For future work  we will explore
adaptive methods for determining model hyperparameters to achieve better training dynamics.

9

.80.84.88P@3F@3MAPMRR0.00.20.40.60.81.0α.32.36.40Score.80.84.88P@3F@3MAPMRR-3-2-10123log10 β.32.36.40Score.72.80.88P@3F@3MAPMRR-5-4-3-2-1012log10 γ.28.34.40ScoreAcknowledgement

This work is supported by Australian Research Council Future Fellowship Project FT120100832 and
Discovery Project DP180102050. We thank the anonymous reviewers for their feedback on the paper.
We have incorporated responses to reviewers’ comments in the paper.

References

[1] M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving 

M. Isard  et al. Tensorﬂow: a system for large-scale machine learning. In OSDI  2016.

[2] R. Anil  G. Pereyra  A. Passos  R. Ormandi  G. E. Dahl  and G. E. Hinton. Large scale distributed

neural network training through online distillation. In ICLR  2018.

[3] S. Antol  A. Agrawal  J. Lu  M. Mitchell  D. Batra  C. Lawrence Zitnick  and D. Parikh. Vqa:

Visual question answering. In ICCV  2015.

[4] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial

networks. In ICLR  2017.

[5] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 

2017.

[6] S. Arora  R. Ge  Y. Liang  T. Ma  and Y. Zhang. Generalization and equilibrium in generative

adversarial nets (gans). In ICML  2017.

[7] J. Ba and R. Caruana. Do deep nets really need to be deep? In NeurIPS  2014.

[8] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning.

arXiv preprint arXiv:1606.04838  2016.

[9] C. Buciluˇa  R. Caruana  and A. Niculescu-Mizil. Model compression. In SIGKDD  2006.

[10] G. Chen  W. Choi  X. Yu  T. Han  and M. Chandraker. Learning efﬁcient object detection

models with knowledge distillation. In NeurIPS  2017.

[11] L. Chen  D. Xu  I. W. Tsang  and J. Luo. Tag-based image retrieval improved by augmented

features and group-based reﬁnement. IEEE Transactions on Multimedia  2012.

[12] W. Cheng  E. Hüllermeier  and K. J. Dembczynski. Label ranking methods based on the

plackett-luce model. In ICML  2010.

[13] L. Chongxuan  T. Xu  J. Zhu  and B. Zhang. Triple generative adversarial nets. In NeurIPS 

2017.

[14] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical

image database. In CVPR  2009.

[15] S. Feizi  C. Suh  F. Xia  and D. Tse. Understanding gans: the lqg setting. arXiv preprint

arXiv:1710.10793  2017.

[16] Z. Gan  L. Chen  W. Wang  Y. Pu  Y. Zhang  H. Liu  C. Li  and L. Carin. Triangle generative

adversarial networks. In NeurIPS  2017.

[17] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In NeurIPS  2014.

[18] M. Grbovic  N. Djuric  S. Guo  and S. Vucetic. Supervised clustering of label ranking data

using label preference information. Machine learning  2013.

[19] M. Guillaumin  T. Mensink  J. Verbeek  and C. Schmid. Tagprop: Discriminative metric learning

in nearest neighbor models for image auto-annotation. In ICCV  2009.

10

[20] E. Gumbel. Statistical theory of extreme values and some practical applications: A series of

lectures. US Government Printing Ofﬁce  Washington  1954.

[21] S. Gupta  J. Hoffman  and J. Malik. Cross modal distillation for supervision transfer. In CVPR 

2016.

[22] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

2016.

[23] G. Hinton  O. Vinyals  and J. Dean. Distilling the knowledge in a neural network. In NeurIPS

workshop  2014.

[24] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  1997.

[25] E. Jang  S. Gu  and B. Poole. Categorical reparameterization with gumbel-softmax. In ICLR 

2017.

[26] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical

report  University of Toronto  2009.

[27] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  1998.

[28] X. Li and C. G. Snoek. Classifying tag relevance with relevant positive and negative examples.

In ACMMM  2013.

[29] D. Lopez-Paz  L. Bottou  B. Schölkopf  and V. Vapnik. Unifying distillation and privileged

information. In ICLR  2016.

[30] C. J. Maddison  D. Tarlow  and T. Minka. A* sampling. In NeurIPS  2014.

[31] C. J. Maddison  A. Mnih  and Y. W. Teh. The concrete distribution: A continuous relaxation of

discrete random variables. In ICLR  2017.

[32] A. Makadia  V. Pavlovic  and S. Kumar. Baselines for image annotation. IJCV  2010.

[33] L. Metz  B. Poole  D. Pfau  and J. Sohl-Dickstein. Unrolled generative adversarial networks. In

ICLR  2017.

[34] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of

words and phrases and their compositionality. In NeurIPS  2013.

[35] D. Pechyony and V. Vapnik. On the theory of learnining with privileged information. In

NeurIPS  2010.

[36] A. Romero  N. Ballas  S. E. Kahou  A. Chassang  C. Gatta  and Y. Bengio. Fitnets: Hints for

thin deep nets. arXiv preprint arXiv:1412.6550  2014.

[37] D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Learning internal representations by error
propagation. Technical report  California Univ San Diego La Jolla Inst for Cognitive Science 
1985.

[38] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved

techniques for training gans. In NeurIPS  2016.

[39] B. B. Sau and V. N. Balasubramanian. Deep model compression: Distilling knowledge from

noisy teachers. arXiv preprint arXiv:1610.09650  2016.

[40] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. In ICLR  2015.

[41] J.-C. Su and S. Maji. Cross quality distillation. arXiv preprint arXiv:1604.00433  2016.

[42] Y. Sun  N. J. Yuan  Y. Wang  X. Xie  K. McDonald  and R. Zhang. Contextual intent tracking

for personal assistants. In SIGKDD  2016.

11

[43] Y. Sun  N. J. Yuan  X. Xie  K. McDonald  and R. Zhang. Collaborative nowcasting for contextual

recommendation. In WWW  2016.

[44] Y. Sun  N. J. Yuan  X. Xie  K. McDonald  and R. Zhang. Collaborative intent prediction with

real-time contextual data. TOIS  2017.

[45] B. Thomee  D. A. Shamma  G. Friedland  B. Elizalde  K. Ni  D. Poland  D. Borth  and L.-J. Li.

Yfcc100m: the new data in multimedia research. Communications of the ACM  2016.

[46] G. Tucker  A. Mnih  C. J. Maddison  J. Lawson  and J. Sohl-Dickstein. Rebar: Low-variance 

unbiased gradient estimates for discrete latent variable models. In NeurIPS  2017.

[47] V. Vapnik and R. Izmailov. Learning using privileged information: similarity control and

knowledge transfer. JMLR  2015.

[48] V. Vapnik and A. Vashist. A new learning paradigm: Learning using privileged information.

Neural networks  2009.

[49] J. Wang  L. Yu  W. Zhang  Y. Gong  Y. Xu  B. Wang  P. Zhang  and D. Zhang. Irgan: A minimax
game for unifying generative and discriminative information retrieval models. In SIGIR  2017.

[50] X. Wang  J. Qi  K. Ramamohanarao  Y. Sun  B. Li  and R. Zhang. A joint optimization approach

for personalized recommendation diversiﬁcation. In PAKDD  2018.

[51] Z. Xu  Y.-C. Hsu  and J. Huang. Learning loss for knowledge distillation with conditional

adversarial networks. arXiv preprint arXiv:1709.00513  2017.

[52] L. Yu  W. Zhang  J. Wang  and Y. Yu. Seqgan: Sequence generative adversarial nets with policy

gradient. In AAAI  2017.

[53] M.-L. Zhang and Z.-H. Zhou. A review on multi-label learning algorithms. TKDE  2014.

[54] Y. Zhang  Z. Gan  and L. Carin. Generating text via adversarial training. In NeurIPS workshop

on Adversarial Training  2016.

[55] Y. Zhang  Z. Gan  K. Fan  Z. Chen  R. Henao  D. Shen  and L. Carin. Adversarial feature

matching for text generation. In ICML  2017.

[56] J. Zhao  M. Mathieu  and Y. LeCun. Energy-based generative adversarial network. In ICLR 

2017.

12

,Nanyang Ye
Zhanxing Zhu
Rafal Mantiuk
Xiaojie Wang
Rui Zhang
Yu Sun
Jianzhong Qi