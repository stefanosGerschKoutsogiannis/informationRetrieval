2019,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However  one common challenge of all causality-based fairness notions is identifiability  i.e.  whether they can be uniquely measured from observational data  which is a critical barrier to applying these notions to real-world situations. In this paper  we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions  namely the path-specific counterfactual fairness (PC fairness). Based on that  we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.,PC-Fairness: A Uniﬁed Framework for Measuring

Causality-based Fairness

Yongkai Wu

University of Arkansas

yw009@uark.edu

Xintao Wu

University of Arkansas
xintaowu@uark.edu

Lu Zhang

University of Arkansas

lz006@uark.edu

Hanghang Tong

University of Illinois at Urbana-Champaign

htong@illinois.edu

Abstract

A recent trend of fair machine learning is to deﬁne fairness as causality-based
notions which concern the causal connection between protected attributes and
decisions. However  one common challenge of all causality-based fairness notions
is identiﬁability  i.e.  whether they can be uniquely measured from observational
data  which is a critical barrier to applying these notions to real-world situations.
In this paper  we develop a framework for measuring different causality-based fair-
ness. We propose a uniﬁed deﬁnition that covers most of previous causality-based
fairness notions  namely the path-speciﬁc counterfactual fairness (PC fairness).
Based on that  we propose a general method in the form of a constrained opti-
mization problem for bounding the path-speciﬁc counterfactual fairness under all
unidentiﬁable situations. Experiments on synthetic and real-world datasets show
the correctness and effectiveness of our method.

1

Introduction

Fair machine learning is now an important research ﬁeld which studies how to develop predictive
machine learning models such that decisions made with their assistance fairly treat all groups of
people irrespective of their protected attributes such as gender  race  etc. A recent trend in this ﬁeld is
to deﬁne fairness as causality-based notions which concern the causal connection between protected
attributes and decisions. Based on Pearl’s structural causal models [8]  a number of causality-based
fairness notions have been proposed for capturing fairness in different situations  including total effect
[19  16  20]  direct/indirect discrimination [19  16  7  20]  and counterfactual fairness [5  14  15  9].
One common challenge of all causality-based fairness notions is identiﬁability  i.e.  whether they can
be uniquely measured from observational data. As causality-based fairness notions are deﬁned based
on different types of causal effects  such as total effect on interventions  direct/indirect discrimination
on path-speciﬁc effects  and counterfactual fairness on counterfactual effects  their identiﬁability
depends on the identiﬁability of these causal effects. Unfortunately  in many situations these causal
effects are in general unidentiﬁable  referred to as unidentiﬁable situations [12]. Identiﬁability is a
critical barrier for the causality-based fairness to be applied to real applications. In previous works 
simplifying assumptions are proposed to evade this problem [5  19  4]. However  these simpliﬁcations
may severely damage the performance of predictive models. In [20] the authors propose a method
to bound indirect discrimination as the path-speciﬁc effect in unidentiﬁable situations  and in [14] a
method is proposed to bound counterfactual fairness. Nevertheless  the tightness of these methods is
not analyzed. In addition  it is not clear whether these methods can be applied to other unidentiﬁable
situations  and more importantly  a combination of multiple unidentiﬁable situations.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper  we propose a framework for handling different causality-based fairness notions. We ﬁrst
propose a general representation of all types of causal effects  i.e.  the path-speciﬁc counterfactual
effect  based on which we deﬁne a uniﬁed fairness notion that covers most previous causality-based
fairness notions  namely the path-speciﬁc counterfactual fairness (PC fairness). We summarize all
unidentiﬁable situations that are discovered in the causal inference literature. Then  we develop a
constrained optimization problem for bounding the PC fairness  which is motivated by the method
proposed in [2] for bounding confounded causal effects. The key idea is to parameterize the causal
model using so-called response-function variables  whose distribution captures all randomness
encoded in the causal model  so that we can explicitly traverse all possible causal models to ﬁnd
the tightest possible bounds. In the experiments  we evaluate the proposed method and compare it
with previous bounding methods using both synthetic and real-world datasets. The results show that
our method is capable of bounding causal effects under any unidentiﬁable situation or combinations.
When only path-speciﬁc effect or counterfactual effect is considered  our method provides tighter
bounds than methods in [20] or [14]. The proposed framework settles a general theoretical foundation
for causality-based fairness. We make no assumption about the hidden confounders so that hidden
confounders are allowed to exist in the causal model. We also make no assumption about the data
generating process and whether the observation data is generated by linear or non-linear functions
would not introduce bias into our results. We only assume that the causal graph is given  which is a
common assumption in structural causal models.
Relationship to other work. In [3]  the author introduces the term “path-speciﬁc counterfactual
fairness”  which states that a decision is fair toward an individual if it coincides with the one
that would have been taken in a counterfactual world in which the sensitive attribute along the
unfair pathways were different. They develop a correction method called PSCF for eliminating the
individual-level unfair information contained in the observations while retaining fair information.
Compared to [3]  we formally deﬁne a general fairness notion which  besides the individual-level
fairness  is also applied to fairness in any sub-group of the population. In addition  we further
consider the identiﬁability issue in causal inference that is inevitably brought by conditioning on the
individual level. Unidentiﬁable situation means that there exist two causal models which exactly
agree with the same observational distribution (hence cannot be distinguished using statistic methods
such as maximum likelihood)  but lead to very different causal effects. In our paper  we address
various unidentiﬁable situations by developing a general bounding method. The authors in [6]
study the conditional path-speciﬁc effect and develop a complete identiﬁcation algorithm with the
application to the problem of algorithmic fairness. Similar to our proposed notion  their notion is
also quantiﬁed via conditional distributions over the interventional variant. However  the conditional
path-speciﬁc effect generalizes the conditional causal effect  where the factual condition is assumed
to be “non-contradictory” (such as age in measuring the effect of smoking on lung cancer) [12]. The
path-speciﬁc counterfactual effect  on the other hand  generalizes the counterfactual effect  where
the factual condition can be contradictory to the observation. Formally  in the conditional path-
speciﬁc effect  the condition is performed on the pre-intervention distribution  but in the path-speciﬁc
counterfactual effect  the condition is performed on the post-intervention distribution.

2 Preliminaries

In our notations  an uppercase denotes a variable  e.g.  X; a bold uppercase denotes a set of variables 
e.g.  X; and a lowercase denotes a value or a set of values of the variables  e.g.  x and x.

2.1 Causal Model and Causal Graph
Deﬁnition 1 (Structural Causal Model [8]). A structural causal model M is represented by a
quadriple (cid:104)U  V  F  P (U)(cid:105) where

1. U is a set of exogenous variables that are determined by factors outside the model.
2. P (U) is a joint probability distribution deﬁned over U.
3. V is a set of endogenous variables that are determined by variables in U ∪ V.
4. F is a set of structural equations from U ∪ V to V. Speciﬁcally  for each V ∈ V  there is a
function fV ∈ F mapping from U ∪ (V\V ) to V   i.e.  v = fV (paV   uV )  where paV is a
realization of a set of endogenous variables PAV ∈ V \ V that directly determines V   and
uV is a realization of a set of exogenous variables that directly determines V .

2

Figure 1: Causal graphs of a Markovian model and a semi-Markovian models

In general  fV (·) can be an equation of any type. In some cases  people may assume that fV (·) is of
a speciﬁc type  e.g.  the nonlinear additive function if v = fV (paV ) + uV . On the other hand  if all
exogenous variables in U are assumed to be mutually independent  then the causal model is called
a Markovian model; otherwise  it is called a semi-Markovian model. In this paper  we don’t make
assumptions about the type of equations and independence relationships among exogenous variables.
The causal model M is associated with a causal graph G = (cid:104)V E(cid:105) where V is a set of nodes and E is
a set of edges. Each node of V corresponds to a variable of V in M. Each edge in E  denoted by a
directed arrow →  points from a node X ∈ U ∪ V to a different node Y ∈ V if fY uses values of
X as input. A causal path from X to Y is a directed path which traces arrows directed from X to
Y . The causal graph is usually simpliﬁed by removing all exogenous variables from the graph. In a
Markovian model  exogenous variables can be directly removed without loss of information. In a
semi-Markovian model  after removing exogenous variables we also need to add dashed bi-directed
edges between the children of correlated exogenous variables to indicate the existence of unobserved
common cause factors  i.e.  hidden confounders. Examples are demonstrated in Figure 1.

2.2 Causal Effects

Quantitatively measuring causal effects in the causal model is facilitated with the do-operator [8]
which forces some variable X to take certain value x  formally denoted by do(X = x) or do(x).
In a causal model M  the intervention do(x) is deﬁned as the substitution of structural equation
X = fX (PAX   UX ) with X = x. For an observed variable Y (Y (cid:54)= X) which is affected by the
intervention  its interventional variant is denoted by Yx. The distribution of Yx  also referred to as the
post-intervention distribution of Y under do(x)  is denoted by P (Yx = y) or simply P (yx).
By using the do-operator  the total causal effect is deﬁned as follows.
Deﬁnition 2 (Total Causal Effect [8]). The total causal effect of the value change of X from x0 to x1
on Y = y is given by

TCE(x1  x0) = P (yx1) − P (yx0).

The total causal effect is deﬁned as the effect of X on Y where the intervention is transferred along
all causal paths from X to Y . If we force the intervention to be transferred only along a subset of all
causal paths from X to Y   the causal effect is then called the path-speciﬁc effect  deﬁned as follows.
Deﬁnition 3 (Path-speciﬁc Effect [1]). Given a causal path set π  the π-speciﬁc effect of the value
change of X from x0 to x1 on Y = y through π (with reference x0) is given by

PEπ(x1  x0) = P (yx1|π x0|¯π) − P (yx0 ) 

where P (Yx1|π x0|¯π) represents the post-intervention distribution of Y where the effect of intervention
do(x1) is transmitted only along π while the effect of reference intervention do(x0) is transmitted
along the other paths.

Deﬁnition 2 and 3 consider the average causal effect over the entire population without any prior
observations. If we have certain observations about a subset of attributes O = o and use them as con-
ditions when inferring the causal effect  then the causal inference problem becomes a counterfactual
inference problem meaning that the causal inference is performed on the sub-population speciﬁed
by O = o only. Symbolically  the distribution of Yx conditioning on factual observation O = o is
denoted by P (yx|o). The counterfactual effect is deﬁned as follows.
Deﬁnition 4 (Counterfactual Effect [12]). Given a factual condition O = o  the counterfactual effect
of the value change of X from x0 to x1 on Y = y is given by

CE(x1  x0|o) = P (yx1|o) − P (yx0|o).

3

XYUXUYcorrelatedXYXYUXUYindependentXYAMarkovianmodelAsemi-MarkovianmodelTable 1: Connection between previous fairness notions and PC fairness

Description
Total effect
(System) Direct discrimination
(System) Indirect discrimination
Individual direct discrimination
Group direct discrimination
Counterfactual fairness
Counterfactual error rate

O = ∅ and π = Π

References Relating to PC fairness
[19  16]
[19  7  16] O = ∅ or {S} and π = πd = {S → ˆY }
[19  7  16] O = ∅ or {S} and π = πi ⊂ Π
O = {S  X} and π = πd = {S → ˆY }
[17]
O = Q = PAY \{S} and π = πd = {S → ˆY }
[18]
O = {S  X} and π = Π
[5  9  14]
O = {S  Y } and π = πd or πi
[15]

3 Path-speciﬁc Counterfactual Fairness

In this section  we deﬁne a uniﬁed fairness notion for representing different causality-based fairness
notions. The key component of our notion is a general representation of causal effects. Consider
an intervention on X which is transmitted along a subset of causal paths π to Y   conditioning on
observation O = o. Based on that  we deﬁne path-speciﬁc counterfactual effect as follows.
Deﬁnition 5 (Path-speciﬁc Counterfactual Effect). Given a factual condition O = o and a causal
path set π  the path-speciﬁc counterfactual effect of the value change of X from x0 to x1 on Y = y
through π (with reference x0) is given by

PCEπ(x1  x0|o) = P (yx1|π x0|¯π|o) − P (yx0|o).

In the context of fair machine learning  we use S ∈ {s+  s−} to denote the protected attribute 
Y ∈ {y+  y+} to denote the decision  and X to denote a set of non-protected attributes. The
underlying mechanism of the population over the space S × X × Y is represented by a causal model
M  which is associated with a causal graph G. A historical dataset D is drawn from the population 
which is used to construct a predictor h : X  S → ˆY . The causal model for the population over space
S × X × ˆY can be considered the same as M except that function fY is replaced with a predictor h.
We use Π to denote all causal paths from S to ˆY in the causal graph.
Then  we deﬁne the path-speciﬁc counterfactual fairness based on Deﬁnition 5.
Deﬁnition 6 (Path-speciﬁc Counterfactual Fairness (PC Fairness)). Given a factual condition
O = o where O ⊆ {S  X  Y } and a causal path set π  predictor ˆY achieves the PC fairness
if PCEπ(s1  s0|o) = 0 where s1  s0 ∈ {s+  s−}. We also say that ˆY achieves the τ-PC fairness if

(cid:12)(cid:12)PCEπ(s1  s0|o)(cid:12)(cid:12) ≤ τ.

We show that previous causality-based fairness notions can be expressed as special cases of the PC
fairness. Their connections are summarised in Table 1  where πd contains the direct edge from S to
ˆY   and πi is a path set that contains all causal paths passing through any redlining attributes (i.e.  a
set of attributes in X that cannot be legally justiﬁed if used in decision-making). Based on whether O
equals ∅ or not  the previous notions can be categorized into the ones that deal with the system level
(O = ∅) and the ones that have certain conditions (O (cid:54)= ∅). Based on whether π equals Π or not 
the previous notions can be categorized into the ones that deal with the total causal effect (π = Π) 
the ones that consider the direct discrimination (π = πd)  and the ones that consider the indirect
discrimination (π = πi).
In addition to unifying the existing notions  the notion of PC fairness also resolves new types of
fairness that the previous notions cannot do. One example is individual indirect discrimination 
which means discrimination along the indirect paths for a particular individual. Individual indirect
discrimination has not been studied yet in the literature  probably due to the difﬁculty in deﬁnition
and identiﬁcation. However  it can be directly deﬁned and analyzed using PC fairness by letting
O = {S  X} and π = πi.

4 Measuring Path-speciﬁc Counterfactual Fairness

In this section  we develop a general method for bounding the path-speciﬁc counterfactual effect
in any unidentiﬁable situation. In the causal inference ﬁeld  researchers have studied the reasons

4

Figure 2: The “bow graph”.

Figure 3: The “kite graph”.

Figure 4: The “w graph”.

for unidentiﬁability under different cases. When O = ∅ and π ⊂ Π  the reason for unidentiﬁability
can be the existence of the “kite graph” (see Figure 3) in the causal graph [1]. When O (cid:54)= ∅ and
π = Π  the reason for unidentiﬁability can be the existence of the “w graph” (see Figure 4) [11]. In
any situation  as long as there exists a “hedge graph” (where the simplest case is the “bow graph” as
shown in Figure 2)  then the causal effect is unidentiﬁable [12]. Obviously  all above unidentiﬁable
situations can exist in the path-speciﬁc counterfactual effect.
Our method is motivated by [2] which formulates the bounding problem as a constrained optimization
problem. The general idea is to parameterize the causal model and use the observational distribution
P (V) to impose constraints on the parameters. Then  the path-speciﬁc counterfactual effect of
interest is formulated as an objective function of maximization or minimization for estimating its
upper or lower bound. The bounds are guaranteed to be tight as we traverse all possible causal models
when solving the optimization problem. Thus  a byproduct of the method is a unique estimation of
the path-speciﬁc counterfactual effect in the identiﬁable situation.
For presenting our method  we ﬁrst introduce a key concept called the response-function variable.

4.1 Response-function Variable

Response-function variables are proposed in [2] for parameterizing the causal model. Consider
an arbitrary endogenous variable denoted by V ∈ V  its endogenous parents denoted by PAV   its
exogenous parents denoted by UV   and its associated structural function in the causal model denoted
by v = fV (paV   uV ). In general  UV can be a variable of any type with any domain size  and fV can
be any function  making the causal model very difﬁcult to be handled. However  we can note that  for
each particular value uV of UV   the functional mapping from PAV to V is a particular deterministic
response function. Thus  we can map each value of UV to a deterministic response function. Although
the domain size of UV is unknown which might be very large or even inﬁnite  the number of different
deterministic response functions is known and limited  given the domain sizes of PAV and V . This
means that the domain of UV can be divided into several equivalent regions  each corresponding to
the same response function. As a result  we can transform the original non-parameterized structural
function to a limited number of parameterized functions.
Formally  we represent equivalent regions of each endogenous variable V by the response-function
variable RV = {0 ···   NV − 1} where NV = |V ||PAV | is the total number of different deterministic
response functions mapping from PAV to V (NV = |V | if V has no parent). Each value rV represents
a pre-deﬁned response function. We also denote the mapping from UV to RV as rV = (cid:96)V (uV ).
Then  for any fV (paV   uV )  it can be re-formulated as

V (rV )) = fV ◦ (cid:96)−1
V   and denotes the response functions represented by rV .

V (paV   rV ) = gV (paV   rV ) 

fV (paV   uV ) = fV (paV   (cid:96)−1
where gV is the composition of fV and (cid:96)−1
We denote the set of all response-function variables by R = {RV : V ∈ V}.
Next  we show how joint distribution P (v) can be expressed as a linear function of P (r). According
to [13]  P (v) can be expressed as the summation over the probabilities of certain values u of U that
satisfy following corresponding requirements: for each V ∈ V  we must have fV (paV   uV ) = v
where v  paV are speciﬁed by v and uV is speciﬁed by u. In other words  denoting by V (u) the
u:V(u)=v P (u). Then  by mapping from
r:V(r)=v P (r)  where for each V ∈ V  V (r) = v means

value that V would obtain if U = u  we have P (v) =(cid:80)
U to R  we accordingly obtain P (v) =(cid:80)

that gV (paV   rV ) = v. As a result  by deﬁning an indicator function

(cid:26)1

0

I(v; paV   rV ) =

if gV (paV   rV ) = v 
otherwise 

5

XYXWYπ={X→W→Z→Y}ZXYYxxwe obtain

(cid:88)

r

P (r)

(cid:89)

V ∈V

P (v) =

I(v; paV   rV ) 

(1)

which is a linear expression of P (r).
Example 1. Consider the causal graph shown in Figure 1 with two endogenous variables X and Y  
and two exogenous variables UX and UY with unknown domains. Assume that both X and Y are
binary  i.e.  X ∈ {x0  x1} and Y ∈ {y0  y1}  and denote their response variables as RX and RY . For
Y   since there are a total number of 22 = 4 response functions  response-function variable RY and
response function gY can be deﬁned as follows:



y0 if rY = 0;
y0 if x = x0  rY = 1;
y1 if x = x1  rY = 1;
y1 if x = x0  rY = 2;
y0 if x = x1  rY = 2;
y1 if rY = 3.


(cid:26)0

1

rY = (cid:96)Y (uY ) =

gY (x  rY ) =

0 if fY (x0  uY ) = y0  fY (x1  uY ) = y0;
1 if fY (x0  uY ) = y0  fY (x1  uY ) = y1;
2 if fY (x0  uY ) = y1  fY (x1  uY ) = y0;
3 if fY (x0  uY ) = y1  fY (x1  uY ) = y1.

Similarly  response-function variable RX and response function gX can be deﬁned as

(cid:26)x0

x1

gX (rX ) =

if rX = 0;
if rX = 1.

rX = (cid:96)X (uX ) =

if fX (uX ) = x0;
if fX (uX ) = x1.

(cid:88)

As a result  the joint distribution over X  Y is given by

P (x  y) =

P (rX   rY )I(x; rX )I(y; x  rY ).

rX  rY

4.2 Expressing Path-speciﬁc Counterfactual Fairness
For bounding the path-speciﬁc counterfactual effect  i.e.  PCEπ(s1  s0|o) = P (ˆys1|π s0|¯π|o) −
P (ˆys0|o)  we also apply response-function variables to express it. We focus on the expression of
P (ˆys1|π s0|¯π|o)  and the expression of P (ˆys0|o) can be similarly obtained as a simpler case. Similar
to the previous section  we ﬁrst express P (ˆys1|π s0|¯π|o) as the summation over the probabilities
of certain values of U that satisfy corresponding requirements. However  as described below  the
requirements are much more complicated than previous ones due to the integration of intervention 
path-speciﬁc effect  and counterfactual.
Firstly  since the path-speciﬁc counterfactual effect is under a factual condition O = o  values u
must satisfy that O(u) = o  i.e.  for each O ∈ O  we must have fO(paO  uO) = o. Secondly  the
path-speciﬁc counterfactual effect is transmitted only along some path set π. According to [20]  for
the variables of X that lie on both π and ¯π  referred to as witness variables/nodes [1]  we need to
consider two sets of values  one obtained by treating them on π and the other obtained by treating
them on ¯π. Formally  non-protected attributes X are divided into three disjoint sets. We denote by
W the set of witness variables  denote by A the set of non-witness variables on π  and denote by
B the set of non-witness variables on ¯π. A simple example is given in Figure 5. We denote the
interventional variant of A by As1|π  the interventional variant of B by Bs0|¯π  the interventional
variant of W treated on π by Ws1|π  and the interventional variant of W treated on ¯π by Ws0|¯π.
Then  P (ˆys1|π s0|¯π|o) can be written as
P (ˆys1|π s0|¯π|o) =

P ( ˆYs1|π s0|¯π = y  As1|π = a  Bs0|¯π = b  Ws1|π = w1  Ws0|¯π = w0 | o).

(cid:88)

a b w1 w0

To obtain the above joint distribution  in addition to O(u) = o  values u must also satisfy that:

1. As1|π(u) = a  which means for each A ∈ A  we must have fA(pa1

A  uA) = a  where pa1
A
means that if PAA contains S or any witness node W   its value is speciﬁed by s1 or w1 if
edge S/W → Y belongs to a path in π  and speciﬁed by s0 or w0 otherwise;

2. Bs0|¯π(u) = b  which means for each B ∈ B  we must have fB(pa0

B  uB) = b  where pa0

B

means that if PAB contains S or any witness node W   its value is speciﬁed by s0 or w0;

6

3. Ws1|π(u) = w1  which means for each W ∈ W  we must have fW (pa1
4. Ws0|π(u) = w0  which means for each W ∈ W  we must have fW (pa0

W   uW ) = w1;
W   uW ) = w0.

Then  by mapping from U to R  we can obtain the requirements for R accordingly. Finally  denoting
the values of R that satisfy O(r) = o by ro  we obtain
(cid:88)
P (ˆys1|π s0|¯π|o) =
I(ˆy; pa1
ˆY

W   rW )I(w0; pa0

W   rW )  (2)

I(w1; pa1

I(a; pa1

(cid:89)

(cid:89)

I(b; pa0

B  rB)

(cid:89)

A  rA)

P (r)
P (o)

B∈B

W∈W

  r ˆY )

A∈A

a b w1
w0 r∈ro
which is still a linear expression of P (r).
Similarly  we can obtain

(cid:88)

v(cid:48) r∈ro

(cid:89)

V ∈V(cid:48)

P (ˆys0|o) =

P (r)
P (o)

I(ˆy; pa ˆY   r ˆY )

I(v; paV   rV ) 

(3)

where V(cid:48) = V\{S  Y }.
Example 2. Consider causal graphs shown in Figures 2  3  4 and following unidentiﬁable causal
effects: total causal effect TCE(x1  x0) in Figure 2  path-speciﬁc effect PEπ(x1  x0) in Figure 3 
and counterfactual effect CE(x1  x0|x0  y0) in Figure 4. By similarly deﬁning response functions as
in Example 1  for Figure 2 with R = {RX   RY }  we have

TCE(x1  x0) =

P (rX   rY )I(y; x0  rY ) 

P (rX   rY )I(y; x1  rY ) − (cid:88)

rX  rY

for Figure 3 with R = {RX   RW   RZ  RY }  we have

PEπ(x1  x0) =

P (r)I(y; z  w0  rY )I(z; w1  rZ)I(w1; x1  rW )I(w0; x0  rW )

P (r)I(y; z  w  rY )I(z; w  rZ)I(w; x0  rW ) 

for Figure 4 with R = {RX   RY }  we have
P (rX   rY )
P (x0  y0)

CE(x1  x0) =

rX  rY ∈ro

I(y; x1  rY ) − (cid:88)

P (rX   rY )
P (x0  y0)

I(y; x0  rY ).

rX  rY ∈ro

(cid:80)

Note that in Figures 2  the total causal effect is identiﬁable if UX and UY are independent. This
is reﬂected in our formulation such that when RX and RY are independent  we have P (yx1) =
P (rX )P (rY )I(y; x1  rY ) = P (y|x1)  which can be directly measured from observational

rX  rY

data. Similar phenomenons can be observed in other identiﬁable situations.

rX  rY

(cid:88)
(cid:88)
− (cid:88)
(cid:88)

z w1 w0 r

z w r

Figure 5: A causal graph with unidentiﬁable path-speciﬁc counterfactual fairness.

Example 3. Consider a causal graph shown in Figure 5  and the path-speciﬁc counterfactual effect
PCEπ(s1  s0|o) where π = {S → ˆY   S → W → A → ˆY } and o = {s0  w(cid:48)  a(cid:48)  b(cid:48)}. Any
pair of exogenous variables can be correlated. Response-function variables are given by R =
{RS  RW   RA  RB  R ˆY }. By similarly deﬁning response functions as in Example 1  we can obtain
P (ˆys1|π s0|¯π|o) =
I(ˆy; a  b  s1  r ˆY )I(a; w1  rA)I(b; w0  rB)I(w1; s1  rW )I(w0; s0  rW ) 

(cid:88)

a b w1 w0

r∈ro

P (r)
P (o)

(cid:88)

a b w r∈ro

and

P (ˆys0|o) =

P (r)
P (o)

I(ˆy; a  b  s0  r ˆY )I(a; w  rA)I(b; w  rA)I(w; s0  rW ).

7

SWˆYABπ={S→W→A→ˆY S→ˆY}4.3 Bounding Path-speciﬁc Counterfactual Fairness

In above two sections we express both joint distribution P (v) and the path-speciﬁc counterfactual
effect as linear functions of P (r). All causal models (represented by different P (r)) that agree with
the distribution of observational data D cannot be distinguished and should be considered in bounding
PC fairness. Therefore  ﬁnding the lower or upper bound of the path-speciﬁc counterfactual effect is
equivalent to ﬁnding the P (r) that minimizes or maximizes the path-speciﬁc counterfactual effect 
subject to that the derived joint distribution P (v) agrees with the observational distribution P (D).
This fact results in the following linear programming problem for deriving the lower/upper bound of
path-speciﬁc counterfactual effect.

min/max P (ˆys1|π s0|¯π|o) − P (ˆys0|o) 

(cid:88)

s.t.

P (V) = P (D) 

P (r) = 1  P (r) ≥ 0 

(4)

r

where P (ˆys1|π s0|¯π|o) is given by Eq. (2)  P (ˆys0|o) is given by Eq. (3)  and P (v) is given by
Equation (1).
The lower and upper bounds derived by solving the above optimization problem is guaranteed to
be the tightest  since the response function is an equivalent mapping that covers all possible causal
models thus we can explicitly traverse all possible causal models.
We use the derived bounds for examining τ-PC fairness: if the upper bound is less than τ and the
lower bound is greater than −τ  then τ-PC fairness must be satisﬁed; if the upper bound is less than
−τ or the lower bound is greater than τ  τ-PC fairness must not be satisﬁed; otherwise  it is uncertain
and cannot be determined from data.

5 Experiments

Datasets. For synthetic datasets  we manually build a causal model with complete knowledge of
exogenous variables and equations using Tetrad [10] according to the causal graphs. The causal
model consists of 4 endogenous variables  S  W   A  ˆY   all of which have two domain values. Then 
we consider two versions of the causal model: (1) we assume a shared exogenous variables  i.e.  a
hidden confounder  with 100 domain values (the causal graph is shown in Figure 6); (2) we assume
all exogenous variables are mutually independent (the causal graph is omitted due to the space
limit). The distribution of exogenous variables and structural equations of endogenous variables are
randomly assigned. Finally  we generate two datasets from each version of the causal model  denoted
by D1 and D2 respectively.
For the real-world dataset  we adopt the Adult dataset  which consists of 65 123 records with 11
attributes including edu  sex  income etc. Similar to [14]  we select 7 attributes  binarize their values 
and build the causal graph. Fairness threshold τ is set to 0.1. The datasets and implementation are
available at http://tiny.cc/pc-fairness-code.
Bounding Path-speciﬁc Counterfactual Fairness. We use D1 to validate our method in Eq. (4) for
bounding PCEπ(s+  s−|o) where O = {S  W  A} and π = {S → W → A → ˆY   S → ˆY }. The
ground truth can be computed by exactly executing the intervention under given conditions using the
complete causal model. The results are shown in Table 2  where the ﬁrst column indicates the indices
of o’s value combinations. As can be seen  the true values of PCEπ(s+  s−|o) fall into the range of
our bounds for all value combinations of O  which validates our method.
Comparing with previous bounding methods. We use D2 to compare with the previous methods
[20  14] which are derived under the Markovian assumption. We compare with [20] for bounding
PEπ(s+  s−) with π = {S → W → A → ˆY   S → ˆY }. We also compare with [14] for bounding
CE(s+  s−|o) with O = {S  W  A}. The results are shown in Table 3 where the bold indicates
that our method makes different judgments on discrimination detection due to the tighter bounds.
As can be seen  our method achieves much tighter bounds than previous methods  which can be
used to examine fairness more accurately. For example  when measuring indirect discrimination
using PEπ(s+  s−) (Row 1 in Table 3)  it is uncertain for [20] since the lower and upper bounds are
−0.2605 and 0.2656  but our method can guarantee that the decision is discriminatory as the lower

8

bound 0.1772 is larger than τ = 0.1. As another example  when measuring counterfactual fairness of
the 2nd groups of o using CE(s+  s−|o) (Row 3 in Table 3)  the method in [14] is uncertain since
the lower and upper bounds are −0.4383 −0.0212 but our method can guarantee that the decision is
fair due to the range of [−0.0783 −0.0212].
We also use the Adult datset to compare with the method in [14] for bounding CE(s+  s−|o) with
O = {age  edu  marital-status} and obtain similar results  which are shown in Table 4.

Table 2: Bounds and ground
truth of PC fairness on D1.
PCEπ(s+  s−|o)
# of o
lb

ub

1
2
3
4

-0.4548 0.5452
-0.5565 0.4435
-0.5065 0.4935
-0.4598 0.5402

PE

CE

T ruth
0.1507
-0.0928
0.0561
0.0548

Table 3: Compare with existing methods in [20  14] on D2.
Our method
ub
lb

Previous methods

ub

# of o T ruth
0.1793
N/A
0.3438
1
-0.0557
2
0.2318
3
0.0800
4

lb

-0.2605
0.0878
-0.4383
-0.1192
-0.2101

0.2656
0.5049
-0.0212
0.2979
0.2070

0.1772
0.0878
-0.0783
0.1282
0.0110

0.1836
0.5049
-0.0212
0.2847
0.1499

Table 4: Compare with the existing method in [14] on
the Adult dataset.

Method in [14]
lb
ub

Our Method
lb
ub

0.0541
-0.1314
0.1878
-0.0356
0.1676
-0.1634
0.1290
-0.1808

0.2946
0.1091
0.3210
0.0976
0.5289
0.1979
0.4689
0.1591

0.1498
-0.1314
0.2507
-0.0356
0.4419
-0.0731
0.3942
0.0014

0.1944
0.1091
0.2890
0.0976
0.5289
0.1979
0.4689
0.1591

# of o

0
1
2
3
4
5
6
7

Figure 6: The causal graph for the
synthetic dataset D1.

6 Conclusion

In this paper  we develop a general framework for measuring causality-based fairness. We propose
a uniﬁed deﬁnition that covers most of previous causality-based fairness notions  namely the path-
speciﬁc counterfactual fairness (PC fairness). Then  we formulate a linear programming problem to
bound PC fairness which can produce the tightest possible bounds. Experiments using synthetic and
real-world datasets show that  our method can bound causal effects under any unidentiﬁable situation
or combinations  and achieves tighter bounds than previous methods.
As the concern of scalability  the domain size of each response variable is exponential to the number
of parents  meaning that the joint domain size of all response variables are exponential to the total
in-degree of the causal graph. However  we notice that not all response variables are needed in the
formulation  and only those that directly lead to unidentiﬁcation are needed. For example  when
a hidden confounder causes unidentiﬁcation  only the children of the hidden confounder need to
have response variables in the formulation; and when a “kite graph” causes unidentiﬁcation  only the
witness variable need to have a response variable in the formulation. As a result  the total complexity
of the problem formulation could be signiﬁcantly decreased. How to construct fair predictive models
based on the derived bounds is another future research direction. One possible method would be to
incorporate the bounding formulation into a post-processing method. The new formulation will be
a min-max optimization problem  where the optimization variables will include response variables
P (r) as well as a post-processing mapping P (˜y|ˆy  paY ). The inner optimization is to maximize the
path-speciﬁc counterfactual effect to ﬁnd the upper bound  and the outer optimization is to minimize
both the loss function and the upper bound. We will to explore these ideas in the future work.

Acknowledgments

This work was supported in part by NSF 1646654  1920920  and 1940093.

9

SWˆYAReferences
[1] Chen Avin  Ilya Shpitser  and Judea Pearl. Identiﬁability of path-speciﬁc effects. In IJCAI-
05  Proceedings of the Nineteenth International Joint Conference on Artiﬁcial Intelligence 
Edinburgh  Scotland  UK  July 30 - August 5  2005  pages 357–363  2005.

[2] Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods  bounds
and applications. In UAI ’94: Proceedings of the Tenth Annual Conference on Uncertainty in
Artiﬁcial Intelligence  Seattle  Washington  USA  July 29-31  1994  pages 46–54  1994.

[3] Silvia Chiappa. Path-speciﬁc counterfactual fairness. In The Thirty-Third AAAI Conference
on Artiﬁcial Intelligence  AAAI 2019  The Thirty-First Innovative Applications of Artiﬁcial
Intelligence Conference  IAAI 2019  The Ninth AAAI Symposium on Educational Advances in
Artiﬁcial Intelligence  EAAI 2019  Honolulu  Hawaii  USA  January 27 - February 1  2019. 
pages 7801–7808  2019.

[4] Niki Kilbertus  Mateo Rojas-Carulla  Giambattista Parascandolo  Moritz Hardt  Dominik Janz-
ing  and Bernhard Schölkopf. Avoiding discrimination through causal reasoning. In Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017  4-9 December 2017  Long Beach  CA  USA  pages 656–666  2017.

[5] Matt J. Kusner  Joshua R. Loftus  Chris Russell  and Ricardo Silva. Counterfactual fairness.
In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017  4-9 December 2017  Long Beach  CA  USA  pages
4066–4076  2017.

[6] Daniel Malinsky  Ilya Shpitser  and Thomas S. Richardson. A potential outcomes calculus for
identifying conditional path-speciﬁc effects. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics  AISTATS 2019  16-18 April 2019  Naha  Okinawa  Japan  pages
3080–3088  2019.

[7] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence  (AAAI-18)  the 30th innovative Applications of
Artiﬁcial Intelligence (IAAI-18)  and the 8th AAAI Symposium on Educational Advances in
Artiﬁcial Intelligence (EAAI-18)  New Orleans  Louisiana  USA  February 2-7  2018  pages
1931–1940  2018.

[8] Judea Pearl. Causality: Models  Reasoning and Inference. Cambridge University Press  New

York  NY  USA  2nd edition  2009.

[9] Chris Russell  Matt J. Kusner  Joshua R. Loftus  and Ricardo Silva. When worlds collide:
Integrating different counterfactual assumptions in fairness. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017 
4-9 December 2017  Long Beach  CA  USA  pages 6414–6423  2017.

[10] Richard Scheines  Peter Spirtes  Clark Glymour  Christopher Meek  and Thomas Richardson.
The TETRAD Project: Constraint Based Aids to Causal Model Speciﬁcation. Multivariate
Behavioral Research  33(1):65–117  January 1998.

[11] Ilya Shpitser and Judea Pearl. What counterfactuals can be tested. In UAI 2007  Proceedings of
the Twenty-Third Conference on Uncertainty in Artiﬁcial Intelligence  Vancouver  BC  Canada 
July 19-22  2007  pages 352–359  2007.

[12] Ilya Shpitser and Judea Pearl. Complete identiﬁcation methods for the causal hierarchy. J.

Mach. Learn. Res.  9:1941–1979  2008.

[13] Jin Tian and Judea Pearl. Probabilities of causation: Bounds and identiﬁcation. In UAI ’00:
Proceedings of the 16th Conference in Uncertainty in Artiﬁcial Intelligence  Stanford University 
Stanford  California  USA  June 30 - July 3  2000  pages 589–598  2000.

[14] Yongkai Wu  Lu Zhang  and Xintao Wu. Counterfactual fairness: Unidentiﬁcation  bound and
algorithm. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial
Intelligence  IJCAI 2019  Macao  China  August 10-16  2019  pages 1438–1444  2019.

10

[15] Junzhe Zhang and Elias Bareinboim. Equality of opportunity in classiﬁcation: A causal
approach. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal 
Canada.  pages 3675–3685. 2018.

[16] Junzhe Zhang and Elias Bareinboim. Fairness in decision-making - the causal explanation
In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
formula.
(AAAI-18)  the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18)  and the 8th
AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18)  New Orleans 
Louisiana  USA  February 2-7  2018  pages 2037–2045  2018.

[17] Lu Zhang  Yongkai Wu  and Xintao Wu. Situation testing-based discrimination discovery: A
causal inference approach. In Proceedings of the Twenty-Fifth International Joint Conference
on Artiﬁcial Intelligence  IJCAI 2016  New York  NY  USA  9-15 July 2016  pages 2718–2724 
2016.

[18] Lu Zhang  Yongkai Wu  and Xintao Wu. Achieving non-discrimination in data release. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining  Halifax  NS  Canada  August 13 - 17  2017  pages 1335–1344  2017.

[19] Lu Zhang  Yongkai Wu  and Xintao Wu. A causal framework for discovering and removing
In Proceedings of the Twenty-Sixth International Joint
direct and indirect discrimination.
Conference on Artiﬁcial Intelligence  IJCAI 2017  Melbourne  Australia  August 19-25  2017 
pages 3929–3935  2017.

[20] Lu Zhang  Yongkai Wu  and Xintao Wu. Causal Modeling-Based Discrimination Discovery
and Removal: Criteria  Bounds  and Algorithms. IEEE Transactions on Knowledge and Data
Engineering  pages 1–1  2018.

11

,Akash Srivastava
Lazar Valkov
Chris Russell
Michael Gutmann
Charles Sutton
Yongkai Wu
Lu Zhang
Xintao Wu
Hanghang Tong