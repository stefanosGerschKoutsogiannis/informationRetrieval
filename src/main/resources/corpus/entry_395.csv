2019,Adaptive GNN for Image Analysis and Editing,Graph neural network (GNN) has powerful representation ability  but optimal configurations of GNN are non-trivial to obtain due to diversity of graph structure and cascaded nonlinearities. This paper aims to understand some properties of GNN from a computer vision (CV) perspective. In mathematical analysis  we propose an adaptive GNN model by recursive definition  and derive its relation with two basic operations in CV: filtering and propagation operations. The proposed GNN model is formulated as a label propagation system with guided map  graph Laplacian and node weight. It reveals that 1) the guided map and node weight determine whether a GNN leads to filtering or propagation diffusion  and 2) the kernel of graph Laplacian controls diffusion pattern. In practical verification  we design a new regularization structure with guided feature to produce GNN-based filtering and propagation diffusion to tackle the ill-posed inverse problems of quotient image analysis (QIA)  which recovers the reflectance ratio as a signature for image analysis or adjustment. A flexible QIA-GNN framework is constructed to achieve various image-based editing tasks  like face illumination synthesis and low-light image enhancement. Experiments show the effectiveness of the QIA-GNN  and provide new insights of GNN for image analysis and editing.,Adaptive GNN for Image Analysis and Editing

Lingyu Liang

South China Univ. of Tech.
lianglysky@gmail.com

Lianwen Jin∗

South China Univ. of Tech.
lianwen.jin@gmail.com

South China Univ. of Tech.

Peng Cheng Laboratory

Yong Xu∗

yxu@scut.edu.cn

Abstract

Graph neural network (GNN) has powerful representation ability  but optimal
conﬁgurations of GNN are non-trivial to obtain due to diversity of graph structure
and cascaded nonlinearities. This paper aims to understand some properties of
GNN from a computer vision (CV) perspective. In mathematical analysis  we
propose an adaptive GNN model by recursive deﬁnition  and derive its relation with
two basic operations in CV: ﬁltering and propagation operations. The proposed
GNN model is formulated as a label propagation system with guided map  graph
Laplacian and node weight. It reveals that 1) the guided map and node weight
determine whether a GNN leads to ﬁltering or propagation diffusion  and 2) the
kernel of graph Laplacian controls diffusion pattern. In practical veriﬁcation  we
design a new regularization structure with guided feature to produce GNN-based
ﬁltering and propagation diffusion to tackle the ill-posed inverse problems of
quotient image analysis (QIA)  which recovers the reﬂectance ratio as a signature
for image analysis or adjustment. A ﬂexible QIA-GNN framework is constructed to
achieve various image-based editing tasks  like face illumination synthesis and low-
light image enhancement. Experiments show the effectiveness of the QIA-GNN 
and provide new insights of GNN for image analysis and editing.

1

Introduction

Recently  many research efforts have been devoted to graph neural network (GNN) [1–3]  which
is a signiﬁcant deep learning technique for graph data under semi-supervised learning. Despite
its powerful representation ability  optimal conﬁgurations of GNN are not trivial to obtain due to
diversity of graph structure and cascaded nonlinearities. The layer structure or parameters are mostly
determined by experimentations with expertise. In this paper  we intend to understand some properties
of GNN mathematically from a computer vision (CV) perspective  and develop some GNN-based
operations for CV problems.
In image analysis and synthesis  there are two basic operations. One is ﬁltering to suppress or
extract feature/content in images [4–7]; the other is propagation that diffuses the visual feature from
the representative region throughout the entire image  so that similar pixels/regions have similar
visual appearance [8–12]. The reviving of neural networks with deep learning has introduced many
CNN-based networks to achieve ﬁltering or propagation [6  5  13–16]. However  the properties
of these models have not been clearly understood  since they found their inspirations in diverse
∗Corresponding authors: Lianwen Jin  Yong Xu. Lingyu Liang and Lianwen Jin are supported by Natural
Science Foundation of Guangdong Province (No. 2017A030312006  2019A1515011045)  the National Key
Research and Development Program of China (No. 2016YFB1001405)  NSFC (Grant No.: 61673182  61771199 
61502176)  GDSTP (No. 2017A010101027)  GZSTP (No. 201704020134) and Fundamental Research Funds
for the Central Universities (No. 2019MS023); Yong Xu is supported by National Nature Science Foundation of
China (61672241  U1611461)  Natural Science Foundation of Guangdong Province (2016A030308013)  and
Science and Technology Program of Guangzhou (201802010055).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

contexts and were formulated in diverse forms  like partial differential equation (PDE) [8]  variational
functional [10  11] or deep neural networks (DNN) [6  16].
Recent research indicates that standard signal processing models with elaborate prior knowledge
can achieve competitive performance with the state-of-the-art DNN-based methods [4  9  17  8  10].
It inspires us to analyze DNN based on existing visual operations. Speciﬁcally  we focus on GNN 
which is one of the semi-supervised methods in the deep learning on graphs [1]. We try to explore
different diffusion properties of GNN with its relation to ﬁltering or propagation models.
In mathematical analysis  we propose a GNN model by recursive deﬁnition  which is formulated as a
graph-based label propagation system with guided map  graph Laplacian and node weight. We derive
its relation with the basic CV operations of ﬁltering (e.g. edge-aware ﬁlter) and propagation (e.g.
learning to diffuse (LTD) model). It reveals that: 1) Guided map and node weight determine whether
a GNN leads to ﬁltering or propagation diffusion; 2) Kernel of graph Laplacian controls diffusion
pattern of a GNN.
In practical analysis  we applied the GNN models to design operations and systems for quotient
image analysis (QIA)  which recovers the reﬂectance ratio as a signature for illumination analysis or
adjustment [18]. QIA is an essential component for image analysis and synthesis [19–30]  especially
for face synthesis and low-light image enhancement.
The challenges of QIA is twofold. Firstly  QIA extracts intrinsic representations from ambiguous
and noisy data  which is an ill-posed inverse problem [31]. Secondly  slight errors in QIA may
lead to obvious visual artifacts  since human visual cognitive systems are highly sensitive to image
appearance changes.
To tackle these problems  we impose new adaptive kernel structures with different guided feature
and priors to the GNN model  and propose GNN-based operations that achieve speciﬁc diffusion
of ﬁltering and propagation for QIA. Then  we develop a GNN-based QIA system (QIA-GNN) for
image-based illumination synthesis. The system consists of three GNN subnetwork (denoted as
QIA-GNN-L1/L2/L3). QIA-GNN-L1 acts as a ﬁltering operation  which extracts the initial reﬂection
and illumination feature from input images; QIA-GNN-L2 acts as a propagation operation  which
adaptively propagates the initial illumination feature from the representative region to the whole
images with good visual consistency; QIA-GNN-L3 is the output layer that combines different
image layers and features to obtain the synthesized results. In this paper  we simply construct
QIA-GNN-L1/L2 under semi-supervised learning scheme  but the GNN-based system is ﬂexible to
achieve various illumination synthesis tasks  including face relighting  face swapping  transﬁguring
and low-light enhancement.
The main contributions are summarized as follows:

• We propose an adaptive GNN model for image analysis  and mathematically derive its
relation to ﬁltering and propagation models  like edge-aware ﬁlters [32] and the LTD
model [8]. It reveals that when a GNN is formulated as a graph-based label propagation
system with guided map  graph Laplacian and node weight  then guided map and node
weight determine whether it produces ﬁltering or propagation diffusion; kernel of graph
Laplacian controls its diffusion pattern.
• To tackle the inverse problems of QIA  we design a new adaptive kernel with different
guided feature and priors  and propose GNN-based operations that achieve speciﬁc diffusion
of ﬁltering and propagation for QIA. Then  a ﬂexible QIA-GNN system is constructed to
produce various illumination synthesis  such as face relighting  face swapping  transﬁguring
and low-light enhancement.

2 Proposed GNN with Adaptive Kernel
The original GNN was proposed in "pre-deep-learning" era [33]. For a graph G = (V  E) with N
nodes (V = {v1  ...vN})  a GNN can be formulated as a recursive equation:

(1)
where ui is the state of node vi; j ∈ N (i) is the neighborhood set of node vi; p and h denote features
of nodes and edges respectively; and F is a parametric function.

j∈N (i)

ui =

(cid:88)

F(cid:0)ui  uj  pi  pj  hi j

(cid:1)  

2

The GNN model of [33] was originally designed for classiﬁcation or regression problems under a
supervised learning scheme. This paper further extends and explores GNN in two aspects. Firstly 
we mathematically distinguish and analyze two intrinsic diffusion properties of GNN  i.e. ﬁltering
and propagation; then we use the GNN to unify many signiﬁcant CV operations  as discussed in
Sec. 2.1 and Sec. 2.2. Secondly  we generalize the formulation of Scarselli’s GNN [33] from data
classiﬁcation/regression to visual data manipulation  where we propose a new kernel structure for
QIA in Sec. 2.3 and a 3-layer QIA-GNN system to achieve multi-task illumination editing in Sec. 3.
Here  we propose an adaptive GNN based on the graph-based label propagation (LP) system [34].
Different from the original LP system for labelling nodes of a graph [35]  we formulate the diffusion
process from a visual diffusion perspective  and it can achieve both ﬁltering and propagation diffusion.
Let V be the visual element domain of an image. The image is mapped into a graph G = (V  E) 
where each node vi ∈ V  i = 1  ...  N corresponds to the visual element of the image  and pi is the
feature of node vi. Let u be the state of visual element deﬁned over V  i.e. u ∈ RN . The graph-based
LP system of the GNN can be reformulated as:

ut+1 − ut = Lut + Λ(g − ut)

(2)
• g(p) is the guided map deﬁned over V  i.e. g ∈ RN   which is used to guide the diffusion of
GNN. The representative visual elements of the guided map is deﬁned within S  where S is
a close subset of V with boundary ∂S for diffusion of propagation;
• Λ(p) = diag(λ(pi))i=1 ... N ∈ RN×N with λ ≥ 0 is the node weight  which determines
• L(p) ∈ RN×N is the graph Laplacian controlling the local diffusion pattern of GNN  where
the kernel function k(pi  pj) measures the similarity of a node with its neighborhood set as
follows:

the restricted region and the level of the guidance map g for u;



−(cid:80)

Lij =

k(pi  pj) 
pj∈Npi
0

k(pi  pj)

j ∈ N (i)
i = j

otherwise.

Eq. 2 can be solved iteratively using Jacobi method. It can be also proved that this LP system with
graph Laplacian of Eq. 3 converges to a unique solution based on the Banach ﬁxed-point theorem [36].
Studies indicate that GNNs can achieve state-of-the-art performance in various tasks [37–40]  but
the design of new GNNs is mostly based on empirical heuristics and trial-and-error. Recently  [3]
proposes a theoretical framework for analyzing the expressive power of GNNs based on Weisfeiler-
Lehman (WL) graph isomorphism test [41]  and validates the theory by experiments for graph-focused
tasks. In the following sections  we analyse the diffusion properties of GNN from a CV perspective.
The propagation and ﬁltering properties of GNN guide us to construct new GNN-based operations
and system for image analysis and synthesis.

2.1 Propagation Properties of Proposed GNN
With proper setting of {g  Λ  L}  the proposed GNN model (2) can produce propagation diffusion 
such as Zhu’s LP model [35] or Liu’s learning to diffuse (LTD) models [8].
In propagation diffusion  the representative elements of g are within S  where S ⊂ V with boundary
∂S. The value of the reaction weight Λii = λ(pi) is determined depending on vi ∈ S or not. The
GNN model identiﬁes the representative visual element of g and propagate the value from S to V.
When Eq. (2) is stable  the GNN model becomes:

(3)

(4)

with

Lu + Λ(g − u) = 0

(cid:26) λ(pi) is large  gi = spi 

vi ∈ S
vi ∈ V \ S 

(5)
where spi is the value corresponding to a node vi with feature pi in representative domain S;  is a
small constant to avoid degeneration or  = 0. The speciﬁc value of λ(pi) is task-dependent. One
typical setting is λ(pi) = 1 for pi ∈ S  while λ(pi) = 0 otherwise.

λ(pi) is small  gi =  

3

To clearly demonstrate the propagation properties of the GNN model  we reformulate the whole LP
system (4) with setting (5) for each node vi with its neighborhood j ∈ N (i):

 (cid:88)

j∈N (i)

 ui − (cid:88)
 (cid:88)

j∈N (i)

j∈N (i)

k(pi  pj) + λ(pi)

k(pi  qj)uj = λ(pi)gi

⇒ ui =

1

dpi + λ(pi)

k(pi  pj)uj + λ(pi)gi



(6)

where dpi =(cid:80)

j∈N (i) k(pi  pj). The value of ui in (6) is mainly controlled by λ(pi):
• For vi ∈ S  λ(pi) is large  then ui is dominated by the term of the guided map
• For vi ∈ V \ S  λ(pi) is small  then ui is determined by the diffusion of Eq. (6).

λ(pi)
dpi +λ(pi)gi;

2.1.1 Relation to Zhu’s Label Propagation (LP)
Let u = uLP = (uLP
data for vi ∈ S and uLP
below  the GNN leads to Zhu’s label propagation [35  42]:

l

u ) speciﬁes how each data is to be labeled  where uLP

  uLP
denotes labeled
u denotes unlabeled data for vi ∈ V\S. With the setting of {gLP   ΛLP   LLP}

l

denotes the initial label for vi ∈ S  and gLP

u = 0 for

  gLP

unlabeled data vi ∈ V\S;

• gLP = (gLP
• ΛLP = diag(λLP (pi))i=1 ... N   where

u )  where gLP

l

l

(cid:26) λLP (pi) (cid:29) dLP

pi
λLP (pi) = 0 

 

vi ∈ S
vi ∈ V\S;

• LLP uses the Gaussian kernel of width σ as the similarity measurement  where
. For LLP   we make a decomposition as LLP = DLP + WLP  
is the diagonal component of LLP   and WLP is the off-diagonal

− (cid:107)pi−pj(cid:107)2

kLP (pi  pj) = e
where DLP
component. Then  we obtain DLP

ii = dLP
pi

2σ2

j WLP
ij .

With these settings  we can reformulate Eq. (4) as:

uLP
i =

1
dLP
pi

j∈N (i) kLP (pi  pj)uj 

vi ∈ S
vi ∈ V\S

(7)

which is precisely one iteration of the label propagation [35].

2.1.2 Relation to Liu’s Learning to Diffusion (LTD) Model
Similarly  we could also derive the relation to Liu’s LTD model [8]. For vi ∈ S  let λ(pi) (cid:29) dpi 
then ui ≈ λ(pi)

dpi +λ(pi)gi ≈ gi = spi. The GNN model (4) with setting (5) becomes:
vi ∈ S
vi ∈ V\S

spi  
k(pi  pj)uj + λ(pi)gi

(cid:32) (cid:80)

ui (cid:39)

(cid:33)

1

 

(8)

dpi +λ(pi)

j∈N (i)



If k(pi  pj) = exp(−β(cid:107)pi − pj(cid:107)2)  the GNN system of Eq. (8) leads to the LTD model [8].

2.2 Filtering Properties of Proposed GNN

With certain setting  the GNN model (2) leads to diffusion that is similar to edge-aware ﬁlter  like
anisotropic diffusion [43] or optimization-based ﬁlter [44].

4

ii =(cid:80)
(cid:80)

gLP
l

 

(cid:40)

Figure 1: The QIA-GNN consists of three subnetworks: QIA-GNN-L1 acts as ﬁltering operation
for facial quotient image extraction; QIA-GNN-L2 acts as propagation operation for facial quotient
image propagation; QIA-GNN-L3 produces the result that combines the image layers and feature.

In ﬁltering diffusion  the representative elements of the guided map g cover the whole domain S = V
and reaction weight is a identity matrix Λ = E. The GNN becomes:

(9)
It can be regarded as the discrete form of bias anisotropic diffusion [45]  if the kernel of L is controlled
by the gradient of u  i.e. k = k((cid:107)∇u(cid:107)). Furthermore  if Λ = 0  we obtain the famous anisotropic
diffusion [43].

ut+1 − ut = Lut + (g − ut).

2.2.1 Relation to Farbman’s Optimization-Based Filter (OF)

Many important edge-aware ﬁlters [32] are deﬁned implicitly by a variational formulation  and we
called it optimization-based ﬁlters (OF) here. One of the representative OF model was proposed
by [44]  which is deﬁned by the minimization of a quadratic functional:

(cid:8)(u − g)(cid:62)(u − g) + u(cid:62)Lu(cid:9)  

u = argmin.

u

(10)

where u is the ﬁltered output  g is the original image  and L encodes the ﬁlter kernel.
When Eq. (9) is stable  the GNN model becomes: (E − L)u = g  which has the same solution of the
quadratic functional of (10).
Let u = uOF be the ﬁltered output  and g = gOF be the original image. With the setting below  the
GNN (9) obtains edge-aware smoothing as OF of [44]:

• LOF measures node similarity with vi and its 4-neighbor set j ∈ N4(i) using the kernel:
(11)
where pOF is the log-luminance channel of gOF to guide the edge-aware diffusion  α
controls the local diffusion pattern  β controls the global smoothness  and ε is a small
constant to avoid division by zero.

kOF (pi  pj) = β((cid:107)pOF

i − pOF

j (cid:107)α + ε)−1 

2.3 Adaptive Kernel for Quotient Image Analysis (QIA)

Based on the mathematical analysis  the diffusion pattern is controlled by the kernel of L. To verify
our analysis  we propose a new kernel with setting {d(M)  G} to design ﬁltering and propagation
operation for QIA  as shown bellow:

kQIA(pi  pj) =

d(M)p

(cid:107)G(pi) − G(pj) + (cid:107)α

p

 

(12)

where d is a spatially inhomogeneous smoothness parameter to control the smoothness of propagation
in different regions  which is determined by the conﬁdence map M; M can be obtained based on the
structure information of an image  such as facial components or semantic segmentation of a scene;
α controls the sensitivity of the term to the derivatives of the guided feature; G is the feature to
guide the propagation  (cid:107) · (cid:107)p represents the p-norm of guided feature space and  is a small constant
(typically  = 0.001) to avoid division by zero.

5

3 GNN for Quotient Image Analysis (QIA-GNN)

We apply the GNN with adaptive kernel to design adaptive ﬁltering and propagation operations for
QIA  and construct a GNN-based system (QIA-GNN) to achieve illumination-aware facial synthesis
and low-light image enhancement  as shown in Fig. 1. QIA-GNN contains three subnetwork  denoted
as QIA-GNN-L1/L2/L3:

1. QIA-GNN-L1: Quotient image extraction  where a GNN-based ﬁltering operation F (g  L)
is proposed to achieve two goals: 1) separating images into multiple facial layers; 2)
extracting quotient image Q in the representative region.

2. QIA-GNN-L2: Quotient image propagation  where a GNN-based propagation operation
P (g  Λ  L) is constructed that adaptively propagate Q to obtain illumination map T. Note
that different combination of F (g  L) and P (g  Λ  L) operations can produce different T.
3. QIA-GNN-L3: Image layer combination  which combines T and the image layers to

produce illumination editing.

We take face relighting as the main presentation in this paper  whose goal is to transfer the illumination
from the reference image R to the input image I in a consistent manner. We construct the ﬁltering
F relit(g  L) and propagation P relit(g  Λ  L) operations of face relighting to show how to construct a
GNN-based system with domain knowledge to solve the visual analysis problem.

3.1 Quotient Image Extraction (FQIA-GNN-L1)

QIA-GNN-L1 is constructed by the GNN-based ﬁltering operation F (g  L) with facial prior  which
separates the target I or reference R into facial layers and obtains the initial quotient image Q. Note
that some pre-processing  like landmark detection or face alignment  have been done for the input
images. It is implemented as follow: Firstly  both the input I and R are converted into CIELAB
color space  where the two chromaticity channels are regarded a color layers Ic (Rc). Secondly  the
luminance channel is decomposed into lighting layer IL (RL) and detail layer Id (Rd) by F (g  L) 
where lighting layers captures the main illumination variance and detail layer contains facial details.
Finally  the initial quotient image Qrelit is obtained by Qrelit = F relit(RL|g L)
F relit(IL|g L) .
F relit acts as inhomogeneous ﬁltering operation. To extract Qrelit  F relit should smooth out details
in background  eyes and eyebrows  while preserves the information in facial region. The setting of
F relit(g  L) is as follows:

g = RL lead to inhomogeneous smoothing of lighting layer IL and RL  respectively.

• The guided map g is regarded as the input image to be ﬁltered. For example  g = IL and
• We integrate facial prior to the kernel kQIA of L to preserve the illumination within facial
region  whiles smooth out the detail in eyes  eyebrows and background. Here we simply
set j ∈ N4(i) to obtain local ﬁltering. p = log(IL) is the feature to guide the diffusion.
Typically  the parameters are set as α = 1.2 and ε = 0.0001. d(M) is spatially determined
by different region  so that background  eyes and eyebrows are smoothed out  while the
informative illumination in the facial region is preserved.

3.2 Quotient Image Propagation (QIA-GNN-L2)
QIA-GNN-L2 is used to generate facial template T deﬁned on V by propagating the values of Q from
the facial region S to V  i.e. Trelit = P relit(Qrelit|g  Λ  L). Since human visual system correlates
with the gradient in an image  T should ﬁt the facial boundary closely and has the smooth transition
between different regions.
To generate Trelit deﬁned on V  we construct P relit to propagate the information of Qrelit from
the facial region S to the regions with missing and uncertain illumination  like eyes  eyebrows and
background V\S. The setting of P relit(g  Λ  L) is as follows:

• For guided map  g = Qrelit  where g contains illumination of the quotient image in the

representative region S.

6

• The reaction weight Λ determines which information is propagated to where. There-
the values of Λ is consistent to the spatial location of S and V\S as Λ =

fore 
diag(λrelit(pi))i=1 ... N   where

(cid:26) 1 

0 

vi ∈ S
vi ∈ V\S;

λrelit(pi) =

• For kQIA  we produce the conﬁdence map M that is consistent to V with smooth transition
of region boundary. The visual information of Qrelit are propagated from the representative
facial region S to the regions of eyes  eyebrows and background V\S. Smoothness parameter
d are controlled by M  so that d is large (typically d = 10) in V\S to produce illumination
propagation  and d is small (typically d = 0.4) in S to preserve the signiﬁcant illumination
detail.

3.3 Image Layer Combination (QIA-GNN-L3)

QIA-GNN-L3 is output GNN layer  which combines T with the facial layer of original/reference
to produce the ﬁnal face synthesis. For face relighting  we transfer illumination of the reference to
the original face by multiplying the Trelit and the lighting layer IL as OL = IL ◦ Trelit  where ◦ is
an element wise product. Finally  we recombine the other facial layers to obtain the face relighting
output O.

4 Experiment

4.1 Basic Evaluation

(a) Face Relighting (FR)

(b) Low-Light Image Enhancement (LIE)

Figure 2: Basic evaluation of QIA-GNN  where (a) shows face relighting with single target and
multiple references; (b) shows low-light image enhancement with illumination maps.

We use the QIA-GNN to achieve face relighting (FR) and low-light image enhancement (LIE)  as
shown in Fig. 2. Fig. 2a shows FR of the same target with different references  and we can observe
there is good consistency between illumination maps and the relighted results. Fig. 2b shows the
LIE of different images  which indicates the effectiveness of our QIA-GNN system to capture the
illumination feature in different scenes.

4.2 Qualitative Evaluation

We verify our QIA-GNN system for different editing tasks  including face relighting  face swapping 
transﬁguring and LIE. Fig. 3 illustrates the comparisons with the-state-of-arts  and indicates that
the QIA-GNN system are competitive to related methods. Note that most of the previous systems
are designed for speciﬁc tasks  while our GNN-QIA system is ﬂexible to perform multiple image
analysis or adjustment with the corresponding settings.
Face Relighting (FR). We compare our method with Li’s [46] and Chen’s [47] methods for face
relighting. The results show that our method allows to relight faces in two patterns. For the ﬁrst
pattern  we perform QIA for all the RGB channels and obtain result similar to Li’s method that
transfers both the shading and tone to the target. For the other pattern  we perform QIA only for the

7

Figure 3: Qualitative comparisons with related methods  including face relighting (red box) with
Li’s [46] and Chen’s [47]  face swapping (blue box) with Korshunova’s [20]  transﬁguring (green
box) with Kemelmacher’s [23] and Nirkin’s [24]  and low-light image enhancement (black box)
with CVC [48] and LIME [28].

luminance channel of inputs  and obtain result that transfers only the shading of the reference but
preserves the original tone of the target.
Furthermore  our method is complementary to previous methods in two aspects. For visual effect-
s  [46] and [47] fail to relight the region outside the face  while ours adaptively generates the missing
illumination in the background. For computation  [46] and [47] requires multiple operations derived
from different contexts  while our operations are based on the same GNN model  which can be
efﬁcient to implement and extend.
Illumination-Aware Face Swapping (FS). Fig. 3 also shows the comparison with the recent works
of [20] for face swapping. [20] proposed a new face synthesis system that trains a speciﬁc CNN
to transform an input (original) identity into a reference identity with preserved facial properties.
For example  the CageNet transforms the input identity into Nicolas Cage with the same expression.
Although [20] has considered the lighting adjustment problem and integrates the lighting loss for
the training of the CageNet  the shading and tone consistency could still be further improved by our
method. Note that the QIA-GNN-L2 is setup to propagate quotient feature within the facial region
for seamless blending  which is slightly different to the setting for relighting.
Transﬁguring (TF). Recently  [23] introduced a new face synthesis task  called transﬁguring  which
let users transﬁgure their appearance from images by changing hair style  hair color etc. Fig. 3
shows the comparison of [23] and [24] for transﬁguring. In some cases  some part of the faces is
under occlusion of hair. To tackle this problem  we integrate the region-aware mask of [10] into our
system and obtain competitive results compared with the state-of-the-art methods [23  24]. Since the
region-aware mask [10] is based on LP  it can be implemented by our GNN model  which indicates
the powerful representation of GNN and the ﬂexibility of our QIA-GNN system.
Low-Light Image Enhancement (LIE). Contrast enhancement have been extensively studied in
recent decades [25  49  50  27]  but the enhancement for low-light images is still an unsolved
problem [26  51  28–30  52]. The main challenges is twofold. Firstly  the intensity of the images
encodes many imaging factors  like illumination of the scene  reﬂection of the object  and the
viewpoint. Obtaining good low-light enhancement without over-sharpening should recover or estimate
some properties of the scene and object from image intensity [26  51  28–30  52]  but it is unfortunately
an inherent ill-posed problem [53  31]. Secondly  quality assessment of sharpened images in objective
manners is still an open problem [54  55]  and it lacks a benchmark to evaluate the performance of
different low-light enhancement methods. We focus on the ﬁrst aspect in this paper  and apply the
QIA-GNN with new regularization to adaptively enhance low-light image without over-sharping.

8

Tar./Ref.Face RelightingOurs Ours Li et al.Chen et al.Illumination-Aware Face SwappingOurs TargetRef. of CageNetKorshunova et al.TransfiguringRef. of SwiftNetOurs Korshunova et al.Tar./Ref.Ours Shlizerman et al.Nirkin et al.Low-Light Image EnhancementInputCVCLIMEOursIllum. MapImg1Img2Img3Img4Img5Img6Figure 4: Objective assessment of FS with [20] and TF with [23] and [24] by GMSD [56].

CVC [48]
LIME [28]

Ours

Img4
6.75
7.65
7.65

Img5
4.27
5.79
6.04

Img6
5.16
7.23
7.71

Img1
6.54
7.67
7.48

Img2
6.28
7.43
7.47

Img3
6.38
7.53
7.55

Table 1: Objective assessment of LIE with CVC [48]  LIME [28] and Ours by DE [57].

Based on the Retinex theory [53  29]  we obtain adaptive low-light enhancement via estimation and
adjustment of the illumination maps of images using the QIA-GNN. It uses the QIA-GNN-L1 as
ﬁltering operation to extracts the initial illumination map  and adjusts the map adaptively with smooth
transition by QIA-GNN-L2 which acts as propagation operation. Finally  the enhancement result is
produced by combining the illumination map with the image layer.
We made comparison with Contextual and Variational Contrast enhancement (CVC) [48] and the
recently proposed LIME [28] for low-lighting image enhancement  as shown in Fig. 3 (black box).
The results indicates that our method facilitates to adaptively brighten images without over-sharpening
the lighter regions of images in high dynamic range (HDR) manners. It also shows that our method
outperforms CVC [48] with better tonal consistent and achieve competitive performance with the
state-of-the-art LIME [28].

4.3 Quantitative Evaluation

For facial synthesis (FR  FS  TF)  we made a small scale user study to determine which is more
consistent to the original target with 10 volunteers (5 males and 5 females) for the results in Fig. 3 
and our GNN-based results have a higher rank score than the other methods. A larger scale user
study for more results would be performed in our future research. In addition  we used some metric
of image quality assessment for objective evaluation. For FS and TF  we used gradient magnitude
similarity deviation (GMSD) [56] to measure the visual similarity between the target and output pairs
(shown in Fig. 4)  where GMSD1<GMSD2 indicates that our method has better visual consistency
than Korshunova’s [20] for FS. Similarly  the results also indicate that our method (GMSD: 0.0909)
is competitive to the Kemelmacher’s [23] (GMSD: 0.2114) and Nirkin’s [24] (GMSD: 0.1472)  and
obtains better visual consistency.
For low-light image enhancement  Table 1 shows the quantitative comparison between CVC [48] 
LIME [28] and ours by discrete entropy (DE) [57]  where a higher value of DE indicates that the
image has richer details. The objective measurements of Img 1 to Img 6 indicate that our method is
superior to CVC [48] and competitive to the state-of-the-art LIME [28].

5 Conclusion

This paper proposes an adaptive GNN model by a LP system with guided map  graph Laplacian and
node weight from CV perspective. We mathematically analyze its diffusion properties and derive
its relation to edge-aware ﬁlter [32] and LTD model [8]. We ﬁnd that different combination of the
guided map and reaction weight determine whether a GNN leads to ﬁltering or propagation diffusion 
and the kernel of graph Laplacian controls the diffusion patterns. Based on the diffusion properties of
GNN  we design a new adaptive kernel with different guided feature and image priors  and propose
GNN-based operations that achieve speciﬁc diffusion of ﬁltering and propagation for QIA. We also
construct a QIA-GNN system  which is ﬂexible to produce various image-based editing  such as face
relighting  face swapping  transﬁguring and low-light image enhancement. Experiments show the
effectiveness of our methods and indicate that GNN can be a powerful tool for CV tasks.

9

GMSD2= 0.2874GMSD1= 0.0890Korshunova’sTargetOursOursNirkin’sTargetKemelmache’s0.21140.14720.0909Face Swapping (FS)Transfiguring(TF)References
[1] Ziwei Zhang  Peng Cui  and Wenwu Zhu  “Deep learning on graphs: A survey ” arXiv preprint

arXiv:1812.04202  2018.

[2] Jie Zhou  Ganqu Cui  Zhengyan Zhang  Cheng Yang  Zhiyuan Liu  and Maosong Sun  “Graph
neural networks: A review of methods and applications ” arXiv preprint arXiv:1812.08434 
2018.

[3] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka  “How powerful are graph neural

networks? ” Proc. of ICLR  2019.

[4] Thomas Nestmeyer and Peter V Gehler  “Reﬂectance adaptive ﬁltering improves intrinsic image

estimation ” in Proc. of CVPR  2017  vol. 2  p. 4.

[5] Hongteng Xu  Junchi Yan  Nils Persson  Weiyao Lin  and Hongyuan Zha  “Fractal dimension
invariant ﬁltering and its cnn-based implementation ” in Proc. of CVPR  2017  pp. 3825–3833.

[6] Yunjin Chen and Thomas Pock  “Trainable nonlinear reaction diffusion: A ﬂexible framework
for fast and effective image restoration ” IEEE Trans. Pattern Anal. Mach. Intell.  vol. 39  no. 6 
pp. 1256–1272  2017.

[7] Xin Tao  Hongyun Gao  Yi Wang  Xiaoyong Shen  Jue Wang  and Jiaya Jia  “Scale-recurrent

network for deep image deblurring ” arXiv preprint arXiv:1802.01770  2018.

[8] Risheng Liu  Guangyu Zhong  Junjie Cao  Zhouchen Lin  Shiguang Shan  and Zhongxuan
Luo  “Learning to diffuse: A new perspective to design PDEs for visual analysis ” IEEE Trans.
Pattern Anal. Mach. Intell.  pp. 2457–2471  2016.

[9] Yagiz Aksoy  Tunç Ozan Aydin  and Marc Pollefeys  “Designing effective inter-pixel informa-

tion ﬂow for natural image matting. ” in Proc. of CVPR  2017  pp. 228–236.

[10] Lingyu Liang  Lianwen Jin  and Deng Liu  “Edge-aware label propagation for mobile facial
enhancement on the cloud ” IEEE Trans. Circuits Syst. Video Technol.  vol. 27  no. 1  pp.
125–138  2017.

[11] Xiaowu Chen  Jianwei Li  Dongqing Zou  and Qinping Zhao  “Learn sparse dictionaries for edit

propagation ” IEEE Transactions on Image Processing  vol. 25  no. 4  pp. 1688–1698  2016.

[12] Xiaobo An and Fabio Pellacini  “AppProp: all-pairs appearance-space edit propagation ” ACM

Trans. Graph.  vol. 27  no. 3  pp. 40  2008.

[13] Richard Zhang  Jun-Yan Zhu  Phillip Isola  Xinyang Geng  Angela S Lin  Tianhe Yu  and
Alexei A Efros  “Real-time user-guided image colorization with learned deep priors ” arXiv
preprint arXiv:1705.02999  2017.

[14] Kai Zhang  Wangmeng Zuo  and Lei Zhang  “Ffdnet: Toward a fast and ﬂexible solution for

cnn based image denoising ” IEEE Transactions on Image Processing  2018.

[15] Michaël Gharbi  Jiawen Chen  Jonathan T Barron  Samuel W Hasinoff  and Frédo Durand 
“Deep bilateral learning for real-time image enhancement ” ACM Transactions on Graphics
(TOG)  vol. 36  no. 4  pp. 118  2017.

[16] Li Xu  Jimmy Ren  Qiong Yan  Renjie Liao  and Jiaya Jia  “Deep edge-aware ﬁlters ” in Proc.

of ICML  2015  pp. 1669–1678.

[17] Michael Elad and Peyman Milanfar  “Style transfer via texture synthesis. ” IEEE Trans. Image

Processing  vol. 26  no. 5  pp. 2338–2351  2017.

[18] Amnon Shashua and Tammy Riklin-Raviv  “The quotient image: Class-based re-rendering and
recognition with varying illuminations ” IEEE Trans. Pattern Anal. Mach. Intell.  vol. 23  no. 2 
pp. 129–139  2001.

[19] Soumyadip Sengupta  Angjoo Kanazawa  Carlos D Castillo  and David W Jacobs  “Sfsnet:
Learning shape  reﬂectance and illuminance of facesin the wild’ ” in Proc. of CVPR  2018  pp.
6296–6305.

10

[20] Iryna Korshunova  Wenzhe Shi  Joni Dambre  and Lucas Theis  “Fast face-swap using convolu-

tional neural networks ” Proc. ICCV  pp. 3677–3685  2017.

[21] Wayne Wu  Yunxuan Zhang  Cheng Li  Chen Qian  and Chen Change Loy  “Reenactgan:

Learning to reenact faces via boundary transfer ” in Proc. of ECCV  2018  pp. 622–638.

[22] Chen Li  Kun Zhou  Hsiang-Tao Wu  and Stephen Lin  “Physically-based simulation of

cosmetics via intrinsic image decomposition with facial priors ” IEEE Trans. PAMI  2018.

[23] Ira Kemelmacher-Shlizerman  “Transﬁguring portraits ” ACM Trans. Graph.  vol. 35  no. 4  pp.

94  2016.

[24] Yuval Nirkin  Iacopo Masi  Anh Tran Tuan  Tal Hassner  and Gerard Medioni  “On face
segmentation  face swapping  and face perception ” in IEEE International Conference on
Automatic Face & Gesture Recognition  2018  pp. 98–105.

[25] Amina Saleem  Azeddine Beghdadi  and Boualem Boashash  “A distortion-free contrast
enhancement technique based on a perceptual fusion scheme ” Neurocomputing  vol. 226  pp.
161–167  2017.

[26] Xixi Jia  Xiangchu Feng  Weiwei Wang  and Lei Zhang  “An extended variational image
decomposition model for color image enhancement ” Neurocomputing  vol. 322  pp. 216–228 
2018.

[27] Wei Ye and Kai-Kuang Ma  “Blurriness-guided unsharp masking ” IEEE Trans. Image Process. 

vol. 27  no. 9  pp. 4465–4477  2018.

[28] Xiaojie Guo  Yu Li  and Haibin Ling  “Lime: Low-light image enhancement via illumination

map estimation ” IEEE Trans. Image Process.  vol. 26  no. 2  pp. 982–993  2017.

[29] Yuanyuan Gao  Hai-Miao Hu  Bo Li  and Qiang Guo  “Naturalness preserved nonuniform
illumination estimation for image enhancement based on retinex ” IEEE Transactions on
Multimedia  vol. 20  no. 2  pp. 335–344  2018.

[30] Huanjing Yue  Jingyu Yang  Xiaoyan Sun  Feng Wu  and Chunping Hou  “Contrast enhancement
based on intrinsic image decomposition ” IEEE Trans. Image Process.  vol. 26  no. 8  pp. 3981–
3994  2017.

[31] Tomaso Poggio  Vincent Torre  and Koch Christof  “Computational vision and regularization

theory ” Nature  vol. 317  pp. 314–319  1985.

[32] Peyman Milanfar  “A tour of modern image ﬁltering: new insights and methods  both practical

and theoretical ” IEEE Signal Processing Magazine  vol. 30  no. 1  pp. 106–128  2013.

[33] Scarselli Franco  Gori Marco  Tsoi Ah Chung  Hagenbuchner Markus  and Monfardini Gabriele 
“The graph neural network model ” IEEE Transactions on Neural Networks  vol. 20  no. 1  pp.
61  2009.

[34] Olivier Chapelle  Bernhard Schölkopf  Alexander Zien  et al.  “Semi-supervised learning ”

2006.

[35] Xiaojin Zhu and Zoubin Ghahramani  “Learning from labeled and unlabeled data with label

propagation ” Tech. Rep.  2002.

[36] Kendall Atkinson and Weimin Han  Theoretical numerical analysis  vol. 39  Springer  2005.

[37] Thomas N Kipf and Max Welling  “Semi-supervised classiﬁcation with graph convolutional

networks ” Proc. of ICLR  2017.

[38] Will Hamilton  Zhitao Ying  and Jure Leskovec  “Inductive representation learning on large

graphs ” in Advances in Neural Information Processing Systems  2017  pp. 1024–1034.

[39] Adam Santoro  Felix Hill  David Barrett  Ari Morcos  and Timothy Lillicrap  “Measuring
abstract reasoning in neural networks ” in International Conference on Machine Learning  2018 
pp. 4477–4486.

11

[40] Muhan Zhang  Zhicheng Cui  Marion Neumann  and Yixin Chen  “An end-to-end deep learning

architecture for graph classiﬁcation ” in Proc. of AAAI  2018  pp. 4438–4445.

[41] Boris Weisfeiler and Andrei A Lehman  “A reduction of a graph to a canonical form and an
algebra arising during this reduction ” Nauchno-Technicheskaya Informatsia  vol. 2  no. 9  pp.
12–16  1968.

[42] Xiaojin Zhu  Zoubin Ghahramani  and John D Lafferty  “Semi-supervised learning using

gaussian ﬁelds and harmonic functions ” in Proc. of ICML 2003  2003  pp. 912–919.

[43] Pietro Perona and Jitendra Malik  “Scale-space and edge detection using anisotropic diffusion ”

IEEE Trans. Pattern Anal. Mach. Intell.  vol. 12  no. 7  pp. 629–639  1990.

[44] Zeev Farbman  Raanan Fattal  Dani Lischinski  and Richard Szeliski  “Edge-preserving decom-
positions for multi-scale tone and detail manipulation ” ACM Trans. Graph.  vol. 27  no. 3  pp.
67  2008.

[45] K Niklas Nordström  “Biased anisotropic diffusion: a uniﬁed regularization and diffusion

approach to edge detection ” Image and Vision Computing  vol. 8  no. 4  pp. 318–327  1990.

[46] Qing Li  Wotao Yin  and Zhigang Deng  “Image-based face illumination transferring using

logarithmic total variation models ” The Visual Computer  vol. 26  no. 1  pp. 41–49  2010.

[47] Xiaowu Chen  Hongyu Wu  Xin Jin  and Qinping Zhao  “Face illumination manipulation using
a single reference image by adaptive layer decomposition ” IEEE Trans. on Image Processing 
vol. 22  no. 11  pp. 4249–4259  2013.

[48] Turgay Celik and Tardi Tjahjadi  “Contextual and variational contrast enhancement ” IEEE

Trans. Image Process.  vol. 20  no. 12  pp. 3431–3441  2011.

[49] Bin Xiao  Han Tang  Yanjun Jiang  Weisheng Li  and Guoyin Wang  “Brightness and contrast
controllable image enhancement based on histogram speciﬁcation ” Neurocomputing  vol. 275 
pp. 2798–2809  2018.

[50] Shu Zhang  Ting Wang  Junyu Dong  and Hui Yu  “Underwater image enhancement via

extended multi-scale retinex ” Neurocomputing  vol. 245  pp. 1–9  2017.

[51] Kin Gwn Lore  Adedotun Akintayo  and Soumik Sarkar  “Llnet: A deep autoencoder approach

to natural low-light image enhancement ” Pattern Recognition  vol. 61  pp. 650–662  2017.

[52] Xueyang Fu  Delu Zeng  Yue Huang  Xiao-Ping Zhang  and Xinghao Ding  “A weighted
variational model for simultaneous reﬂectance and illumination estimation ” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  2016  pp. 2782–2790.

[53] Edwin H Land and John McCann  “Lightness and retinex theory ” JOSA  vol. 61  no. 1  pp.

1–11  1971.

[54] Lukáš Krasula  Patrick Le Callet  Karel Fliegel  and Miloš Klíma  “Quality assessment of
sharpened images: Challenges  methodology  and objective metrics ” IEEE Trans. Image
Process.  vol. 26  no. 3  pp. 1496–1508  2017.

[55] Ke Gu  Guangtao Zhai  Weisi Lin  and Min Liu  “The analysis of image contrast: From quality
assessment to automatic enhancement. ” IEEE Trans. Cybernetics  vol. 46  no. 1  pp. 284–297 
2016.

[56] W. Xue  L. Zhang  X. Mou  and A. C. Bovik  “Gradient magnitude similarity deviation: A
highly efﬁcient perceptual image quality index ” IEEE Trans. Image Process.  vol. 23  no. 2  pp.
684–695  2014.

[57] Claude Elwood Shannon  “A mathematical theory of communication ” The Bell System

Technical Journal  vol. 27  no. 3  pp. 379–423  1948.

12

,Lingyu Liang
LianWen Jin
Yong Xu