2014,On the Information Theoretic Limits of Learning Ising Models,We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models  given i.i.d. samples. While there have been recent results for specific graph classes  these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast  we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results  but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdos-Renyi graphs in a certain dense setting.,On the Information Theoretic Limits

of Learning Ising Models

Karthikeyan Shanmugam1∗  Rashish Tandon2†  Alexandros G. Dimakis1‡  Pradeep Ravikumar2(cid:63)

1Department of Electrical and Computer Engineering  2Department of Computer Science

The University of Texas at Austin  USA

∗karthiksh@utexas.edu  †rashish@cs.utexas.edu

‡dimakis@austin.utexas.edu  (cid:63)pradeepr@cs.utexas.edu

Abstract

We provide a general framework for computing lower-bounds on the sample com-
plexity of recovering the underlying graphs of Ising models  given i.i.d. samples.
While there have been recent results for speciﬁc graph classes  these involve fairly
extensive technical arguments that are specialized to each speciﬁc graph class. In
contrast  we isolate two key graph-structural ingredients that can then be used to
specify sample complexity lower-bounds. Presence of these structural properties
makes the graph class hard to learn. We derive corollaries of our main result that
not only recover existing recent results  but also provide lower bounds for novel
graph classes not considered previously. We also extend our framework to the
random graph setting and derive corollaries for Erd˝os-Rényi graphs in a certain
dense setting.

1

Introduction

Graphical models provide compact representations of multivariate distributions using graphs that
represent Markov conditional independencies in the distribution. They are thus widely used in a
number of machine learning domains where there are a large number of random variables  including
natural language processing [13]  image processing [6  10  19]  statistical physics [11]  and spatial
statistics [15]. In many of these domains  a key problem of interest is to recover the underlying
dependencies  represented by the graph  given samples i.e. to estimate the graph of dependencies
given instances drawn from the distribution. A common regime where this graph selection problem
is of interest is the high-dimensional setting  where the number of samples n is potentially smaller
than the number of variables p. Given the importance of this problem  it is instructive to have
lower bounds on the sample complexity of any estimator: it clariﬁes the statistical difﬁculty of the
underlying problem  and moreover it could serve as a certiﬁcate of optimality in terms of sample
complexity for any estimator that actually achieves this lower bound. We are particularly interested
in such lower bounds under the structural constraint that the graph lies within a given class of graphs
(such as degree-bounded graphs  bounded-girth graphs  and so on).
The simplest approach to obtaining such bounds involves graph counting arguments  and an appli-
cation of Fano’s lemma. [2  17] for instance derive such bounds for the case of degree-bounded
and power-law graph classes respectively. This approach however is purely graph-theoretic  and
thus fails to capture the interaction of the graphical model parameters with the graph structural con-
straints  and thus typically provides suboptimal lower bounds (as also observed in [16]). The other
standard approach requires a more complicated argument through Fano’s lemma that requires ﬁnd-
ing a subset of graphs such that (a) the subset is large enough in number  and (b) the graphs in
the subset are close enough in a suitable metric  typically the KL-divergence of the corresponding
distributions. This approach is however much more technically intensive  and even for the simple

1

classes of bounded degree and bounded edge graphs for Ising models  [16] required fairly extensive
arguments in using the above approach to provide lower bounds.
In modern high-dimensional settings  it is becoming increasingly important to incorporate structural
constraints in statistical estimation  and graph classes are a key interpretable structural constraint.
But a new graph class would entail an entirely new (and technically intensive) derivation of the
corresponding sample complexity lower bounds. In this paper  we are thus interested in isolating
the key ingredients required in computing such lower bounds. This key ingredient involves one
the following structural characterizations: (1) connectivity by short paths between pairs of nodes 
or (2) existence of many graphs that only differ by an edge. As corollaries of this framework  we
not only recover the results in [16] for the simple cases of degree and edge bounded graphs  but
to several more classes of graphs  for which achievability results have already been proposed[1].
Moreover  using structural arguments allows us to bring out the dependence of the edge-weights  λ 
on the sample complexity. We are able to show same sample complexity requirements for d-regular
graphs  as is for degree d-bounded graphs  whilst the former class is much smaller. We also extend
our framework to the random graph setting  and as a corollary  establish lower bound requirements
for the class of Erd˝os-Rényi graphs in a dense setting. Here  we show that under a certain scaling
of the edge-weights λ  Gp c/p requires exponentially many samples  as opposed to a polynomial
requirement suggested from earlier bounds[1].
2 Preliminaries and Deﬁnitions
Notation: R represents the real line. [p] denotes the set of integers from 1 to p. Let 1S denote the

vector of ones and zeros where S is the set of coordinates containing 1. Let A − B denote A(cid:84) Bc

¯θi j xixj

(cid:32)(cid:80)

and A∆B denote the symmetric difference for two sets A and B.
In this work  we consider the problem of learning the graph structure of an Ising model.
Ising
models are a class of graphical model distributions over binary vectors  characterized by the pair
(G(V  E)  ¯θ)  where G(V  E) is an undirected graph on p vertices and ¯θ ∈ R(p
2) : ¯θi j = 0 ∀(i  j) /∈
E  ¯θi j (cid:54)= 0 ∀ (i  j) ∈ E. Let X = {+1 −1}. Then  for the pair (G  ¯θ)  the distribution on X p is
where x ∈ X p and Z is the normalization factor  also
given as: fG ¯θ(x) = 1
i j
known as the partition function.
Thus  we obtain a family of distributions by considering a set of edge-weighted graphs Gθ  where
each element of Gθ is a pair (G  ¯θ). In other words  every member of the class Gθ is a weighted
undirected graph. Let G denote the set of distinct unweighted graphs in the class Gθ.
A learning algorithm that learns the graph G (and not the weights ¯θ) from n independent samples
(each sample is a p-dimensional binary vector) drawn from the distribution fG ¯θ(·)  is an efﬁciently
computable map φ : χnp → G which maps the input samples {x1  . . . xn} to an undirected graph
ˆG ∈ G i.e. ˆG = φ(x1  . . .   xn).
We now discuss two metrics of reliability for such an estimator φ. For a given (G  ¯θ)  the probability
. Given a graph class Gθ  one
of error (over the samples drawn) is given by p(G  ¯θ) = Pr
may consider the maximum probability of error for the map φ  given as:

Z exp

(cid:33)

(cid:16) ˆG (cid:54)= G
(cid:17)
(cid:16) ˆG (cid:54)= G
(cid:17)

.

pmax = max

(G θ)∈Gθ

Pr

(1)

(2)

The goal of any estimator φ would be to achieve as low a pmax as possible. Alternatively  there are
random graph classes that come naturally endowed with a probability measure µ(G  θ) of choosing
the graphical model. In this case  the quantity we would want to minimize would be the average
probability of error of the map φ  given as:

(cid:104)

(cid:16) ˆG (cid:54)= G
(cid:17)(cid:105)

pavg = Eµ

Pr

In this work  we are interested in answering the following question: For any estimator φ  what is the
minimum number of samples n  needed to guarantee an asymptotically small pmax or pavg ? The
answer depends on Gθ and µ(when applicable).

2

as D (fG(cid:107)fG(cid:48)) = (cid:80)

For the sake of simplicity  we impose the following restrictions1: We restrict to the set of zero-ﬁeld
ferromagnetic Ising models  where zero-ﬁeld refers to a lack of node weights  and ferromagnetic
refers to all positive edge weights. Further  we will restrict all the non-zero edge weights (¯θi j) in
the graph classes to be the same  set equal to λ > 0. Therefore  for a given G(V  E)  we have
¯θ = λ1E for some λ > 0. A deterministic graph class is described by a scalar λ > 0 and the family
of graphs G. In the case of a random graph class  we describe it by a scalar λ > 0 and a probability
measure µ  the measure being solely on the structure of the graph G (on G).
Since we have the same weight λ(> 0) on all edges  henceforth we will skip the reference to it  i.e.
the graph class will simply be denoted G and for a given G ∈ G  the distribution will be denoted
by fG(·)  with the dependence on λ being implicit. Before proceeding further  we summarize the
following additional notation. For any two distributions fG and fG(cid:48)  corresponding to the graphs
G and G(cid:48) respectively  we denote the Kullback-Liebler divergence (KL-divergence) between them
. For any subset T ⊆ G  we let CT () denote an
-covering w.r.t. the KL-divergence (of the corresponding distributions) i.e. CT ()(⊆ G) is a set of
graphs such that for any G ∈ T   there exists a G(cid:48) ∈ CT () satisfying D (fG(cid:107)fG(cid:48)) ≤ . We denote
the entropy of any r.v. X by H(X)  and the mutual information between any two r.v.s X and Y   by
I(X; Y ). The rest of the paper is organized as follows. Section 3 describes Fano’s lemma  a basic
tool employed in computing information-theoretic lower bounds. Section 4 identiﬁes key structural
properties that lead to large sample requirements. Section 5 applies the results of Sections 3 and
4 on a number of different deterministic graph classes to obtain lower bound estimates. Section 6
obtains lower bound estimates for Erd˝os-Rényi random graphs in a dense regime. All proofs can be
found in the Appendix (see supplementary material).
3 Fano’s Lemma and Variants

(cid:16) fG(x)

x∈X p fG(x) log

(cid:17)

fG(cid:48) (x)

Fano’s lemma [5] is a primary tool for obtaining bounds on the average probability of error  pavg. It
provides a lower bound on the probability of error of any estimator φ in terms of the entropy H(·)
of the output space  the cardinality of the output space  and the mutual information I(·   ·) between
the input and the output. The case of pmax is interesting only when we have a deterministic graph
class G  and can be handled through Fano’s lemma again by considering a uniform distribution on
the graph class.
Lemma 1 (Fano’s Lemma). Consider a graph class G with measure µ. Let  G ∼ µ  and let X n =
{x1  . . .   xn} be n independent samples such that xi ∼ fG  i ∈ [n]. Then  for pmax and pavg as
deﬁned in (1) and (2) respectively 

pmax ≥ pavg ≥ H(G) − I(G; X n) − log 2

log|G|

(3)

Thus in order to use this Lemma  we need to bound two quantities: the entropy H(G)  and the mutual
information I(G; X n). The entropy can typically be obtained or bounded very simply; for instance 
with a uniform distribution over the set of graphs G  H(G) = log |G|. The mutual information is
a much trickier object to bound however  and is where the technical complexity largely arises. We
can however simply obtain the following loose bound: I(G; X n) ≤ H(X n) ≤ np. We thus arrive
at the following corollary:
Corollary 1. Consider a graph class G. Then  pmax ≥ 1 − np+log 2
.
log|G|
Remark 1. From Corollary 1  we get: If n ≤ log|G|
  then pmax ≥ δ. Note that
(1 − δ) − log 2
log|G|
this bound on n is only in terms of the cardinality of the graph class G  and therefore  would not
involve any dependence on λ (and consequently  be very loose).

(cid:16)

(cid:17)

p

To obtain sharper lower bound guarantees that depends on graphical model parameters  it is useful
to consider instead a conditional form of Fano’s lemma[1  Lemma 9]  which allows us to obtain
lower bounds on pavg in terms conditional analogs of the quantities in Lemma 1. For the case of
pmax  these conditional analogs correspond to uniform measures on subsets of the original class G.
1Note that a lower bound for a restricted subset of a class of Ising models will also serve as a lower bound

for the class without that restriction.

3

The conditional version allows us to focus on potentially harder to learn subsets of the graph class 
leading to sharper lower bound guarantees. Also  for a random graph class  the entropy H(G) may
be asymptotically much smaller than the log cardinality of the graph class  log|G| (e.g. Erd˝os-Rényi
random graphs; see Section 6)  rendering the bound in Lemma 1 useless. The conditional version
allows us to circumvent this issue by focusing on a high-probability subset of the graph class.
Lemma 2 (Conditional Fano’s Lemma). Consider a graph class G with measure µ. Let  G ∼ µ 
and let X n = {x1  . . .   xn} be n independent samples such that xi ∼ fG  i ∈ [n]. Consider any
T ⊆ G and let µ (T ) be the measure of this subset i.e. µ (T ) = Prµ (G ∈ T ). Then  we have

H(G|G ∈ T ) − I(G; X n|G ∈ T ) − log 2

pavg ≥ µ (T )
pmax ≥ H(G|G ∈ T ) − I(G; X n|G ∈ T ) − log 2

log|T |

log|T |

and 

Given Lemma 2  or even Lemma 1  it is the sharpness of an upper bound on the mutual information
that governs the sharpness of lower bounds on the probability of error (and effectively  the number of
samples n). In contrast to the trivial upper bound used in the corollary above  we next use a tighter
bound from [20]  which relates the mutual information to coverings in terms of the KL-divergence 
applied to Lemma 2. Note that  as stated earlier  we simply impose a uniform distribution on G when
dealing with pmax. Analogous bounds can be obtained for pavg.
Corollary 2. Consider a graph class G  and any T ⊆ G. Recall the deﬁnition of CT () from Section

2. For any  > 0  we have pmax ≥(cid:16)

log|T |
Remark 2. From Corollary 2  we get: If n ≤ log|T |
δ.  is an upper bound on the radius of the KL-balls in the covering  and usually varies with λ.

log|T | − log|CT ()|
log|T |

(1 − δ) − log 2

1 − log|CT ()|+n+log 2

  then pmax ≥

(cid:17)

(cid:16)

(cid:17)

.



ρ

(cid:17)

(cid:16)

(1 − δ) − log 2
log|T |

But this corollary cannot be immediately used given a graph class: it requires us to specify a subset
T of the overall graph class  the term   and the KL-covering CT ().
We can simplify the bound above by setting  to be the radius of a single KL-ball w.r.t. some center 
covering the whole set T . Suppose this radius is ρ  then the size of the covering set is just 1. In this
case  from Remark 2  we get: If n ≤ log|T |
  then pmax ≥ δ. Thus  our goal
in the sequel would be to provide a general mechanism to derive such a subset T : that is large in
number and yet has small diameter with respect to KL-divergence.
We note that Fano’s lemma and variants described in this section are standard  and have been applied
to a number of problems in statistical estimation [1  14  16  20  21].
4 Structural conditions governing Correlation
As discussed in the previous section  we want to ﬁnd subsets T that are large in size  and yet have
a small KL-diameter. In this section  we summarize certain structural properties that result in small
KL-diameter. Thereafter  ﬁnding a large set T would amount to ﬁnding a large number of graphs in
the graph class G that satisfy these structural properties.
As a ﬁrst step  we need to get a sense of when two graphs would have corresponding distributions
with a small KL-divergence. To do so  we need a general upper bound on the KL-divergence be-
tween the corresponding distributions. A simple strategy is to simply bound it by its symmetric
divergence[16]. In this case  a little calculation shows :

D (fG(cid:107)fG(cid:48)) ≤ D (fG(cid:107)fG(cid:48)) + D (fG(cid:48)(cid:107)fG)

(cid:88)

=

(s t)∈E\E(cid:48)

(cid:88)

(s t)∈E(cid:48)\E

λ (EG [xsxt] − EG(cid:48) [xsxt]) +

λ (EG(cid:48) [xsxt] − EG [xsxt])

(4)
where E and E(cid:48) are the edges in the graphs G and G(cid:48) respectively  and EG[·] denotes the expectation
under fG. Also note that the correlation between xs and xt  EG[xsxt] = 2PG(xsxt = +1) − 1.

4

From Eq. (4)  we observe that the only pairs  (s  t)  contributing to the KL-divergence are the ones
that lie in the symmetric difference  E∆E(cid:48). If the number of such pairs is small  and the difference of
correlations in G and G(cid:48) (i.e. EG [xsxt]−EG(cid:48) [xsxt]) for such pairs is small  then the KL-divergence
would be small.
To summarize the setting so far  to obtain a tight lower bound on sample complexity for a class of
graphs  we need to ﬁnd a subset of graphs T with small KL diameter. The key to this is to identify
when KL divergence between (distributions corresponding to) two graphs would be small. And the
key to this in turn is to identify when there would be only a small difference in the correlations
between a pair of variables across the two graphs G and G(cid:48). In the subsequent subsections  we
provide two simple and general structural characterizations that achieve such a small difference of
correlations across G and G(cid:48).
4.1 Structural Characterization with Large Correlation

One scenario when there might be a small difference in correlations is when one of the correlations
is very large  speciﬁcally arbitrarily close to 1  say EG(cid:48)[xsxt] ≥ 1 −   for some  > 0. Then 
EG[xsxt] − EG(cid:48)[xsxt] ≤   since EG[xsxt] ≤ 1. Indeed  when s  t are part of a clique[16]  this is
achieved since the large number of connections between them force a higher probability of agree-
ment i.e. PG(xsxt = +1) is large.
In this work we provide a more general characterization of when this might happen by relying on the
following key lemma that connects the presence of “many” node disjoint “short” paths between a
pair of nodes in the graph to high correlation between them. We deﬁne the property formally below.
Deﬁnition 1. Two nodes a and b in an undirected graph G are said to be ((cid:96)  d) connected if they
have d node disjoint paths of length at most (cid:96).
Lemma 3. Consider a graph G and a scalar λ > 0. Consider the distribution fG(x) induced by
the graph. If a pair of nodes a and b are ((cid:96)  d) connected  then EG [xaxb] ≥ 1 −
.

2

1+

(1+(tanh(λ))(cid:96))d
(1−(tanh(λ))(cid:96))d

From the above lemma  we can observe that as (cid:96) gets smaller and d gets larger  EG [xaxb] approaches
its maximum value of 1. As an example  in a k-clique  any two vertices  s and t  are (2  k − 1)
connected. In this case  the bound from Lemma 3 gives us: EG [xaxb] ≥ 1 −
1+(cosh λ)k−1 . Of

course  a clique enjoys a lot more connectivity (i.e. also(cid:0)3  k−1

(cid:1) connected etc.  albeit with node

overlaps) which allows for a stronger bound of ∼ 1 − λkeλ
Now  as discussed earlier  a high correlation between a pair of nodes contributes a small term to the
KL-divergence. This is stated in the following corollary.
Corollary 3. Consider two graphs G(V  E) and G(cid:48)(V  E(cid:48)) and scalar weight λ > 0 such that
E − E(cid:48) and E(cid:48) − E only contain pairs of nodes that are ((cid:96)  d) connected in graphs G(cid:48) and G
respectively  then the KL-divergence between fG and fG(cid:48)  D (fG(cid:107)fG(cid:48)) ≤

eλk (see [16])2

.

2

2

2λ|E∆E(cid:48)|
(1+(tanh(λ))(cid:96))d
(1−(tanh(λ))(cid:96))d

4.2 Structural Characterization with Low Correlation

1+

Another scenario where there might be a small difference in correlations between an edge pair across
two graphs is when the graphs themselves are close in Hamming distance i.e. they differ by only a
few edges. This is formalized below for the situation when they differ by only one edge.
Deﬁnition 2 (Hamming Distance). Consider two graphs G(V  E) and G(cid:48)(V  E(cid:48)). The hamming
distance between the graphs  denoted by H(G  G(cid:48))  is the number of edges where the two graphs
differ i.e.
(5)
Lemma 4. Consider two graphs G(V  E) and G(cid:48)(V  E(cid:48)) such that H(G  G(cid:48)) = 1  and (a  b) ∈ E
is the single edge in E∆E(cid:48). Then  EfG [xaxb] − EfG(cid:48) [xaxb] ≤ tanh(λ). Also  the KL-divergence
between the distributions  D (fG(cid:107)f(cid:48)

H(G  G(cid:48)) = |{(s  t)| (s  t) ∈ E∆E(cid:48)}|

G) ≤ λ tanh(λ).

2Both the bound from [16] and the bound from Lemma 3 have exponential asymptotic behaviour (i.e. as k
grows) for constant λ. For smaller λ  the bound from [16] is strictly better. However  not all graph classes allow
for the presence of a large enough clique  for e.g.  girth bounded graphs  path restricted graphs  Erd˝os-Rényi
graphs.

5

The above bound is useful in low λ settings. In this regime λ tanh λ roughly behaves as λ2. So  a
smaller λ would correspond to a smaller KL-divergence.
4.3 Inﬂuence of Structure on Sample Complexity

Now  we provide some high-level intuition behind why the structural characterizations above would
be useful for lower bounds that go beyond the technical reasons underlying Fano’s Lemma that we
have speciﬁed so far. Let us assume that λ > 0 is a positive real constant. In a graph even when the
edge (s  t) is removed  (s  t) being ((cid:96)  d) connected ensures that the correlation between s and t is
still very high (exponentially close to 1). Therefore  resolving the question of the presence/absence
of the edge (s  t) would be difﬁcult – requiring lots of samples. This is analogous in principle to
the argument in [16] used for establishing hardness of learning of a set of graphs each of which is
obtained by removing a single edge from a clique  still ensuring many short paths between any two
vertices. Similarly  if the graphs  G and G(cid:48)  are close in Hamming distance  then their corresponding
distributions  fG and fG(cid:48)  also tend to be similar. Again  it becomes difﬁcult to tease apart which
distribution the samples observed may have originated from.
5 Application to Deterministic Graph Classes

In this section  we provide lower bound estimates for a number of deterministic graph families. This
is done by explicitly ﬁnding a subset T of the graph class G  based on the structural properties of
the previous section. See the supplementary material for details of these constructions. A common
underlying theme to all is the following: We try to ﬁnd a graph in G containing many edge pairs
(u  v) such that their end vertices  u and v  have many paths between them (possibly  node disjoint).
Once we have such a graph  we construct a subset T by removing one of the edges for these well-
connected edge pairs. This ensures that the new graphs differ from the original in only the well-
connected pairs. Alternatively  by removing any edge (and not just well-connected pairs) we can get
another larger family T which is 1-hamming away from the original graph.
5.1 Path Restricted Graphs
Let Gp η be the class of all graphs on p vertices with have at most η paths (η = o(p)) between any
two vertices. We have the following theorem :
Theorem 1. For the class Gp η  if n ≤ (1 − δ) max
pmax ≥ δ.
To understand the scaling  it is useful to think of cosh(2λ) to be roughly exponential in λ2 i.e.
cosh(2λ) ∼ eΘ(λ2)3. In this case  from the second term  we need n ∼ Ω
samples.
√
If η is scaling with p  this can be prohibitively large (exponential in λ2η). Thus  to have low sample
complexity  we must enforce λ = O(1/
η). In this case  the ﬁrst term gives n = Ω(η log p)  since
λ tanh(λ) ∼ λ2  for small λ.
We may also consider a generalization of Gp η. Let Gp η γ be the set of all graphs on p vertices such
that there are at most η paths of length at most γ between any two nodes (with η + γ = o(p)). Note
that there may be more paths of length > γ.
Theorem 2. Consider the graph class Gp η γ. For any ν ∈ (0  1)  let tν = p1−ν−(η+1)
(1 − δ) max

(cid:110) log(p/2)
λ tanh λ   1+cosh(2λ)η−1
(cid:16) eλ2η

(cid:16) p
(cid:17)(cid:17)
(cid:16) p

1+tanh(λ)γ+1
1−tanh(λ)γ+1

. If n ≤

cosh(2λ)η−1

(cid:17)(cid:111)

(cid:19)tν(cid:21)

ν log(p)

λ log

η

(cid:20)

1+

log

2(η+1)

  then

2λ

(cid:18)

2λ

γ

  then pmax ≥ δ.
(cid:17)

(cid:16) 1+tanh(λ)γ+1

λ tanh λ  

 log(p/2)
(cid:16) log p
λ tanh λ   ecλγ+1√

λ

The parameter ν ∈ (0  1) in the bound above may be adjusted based on the scaling of η and γ.
is ∼ eλγ+1. As an example 
Also  an approximate way to think of the scaling of
2. In this case  for some constant c  our bound imposes
for constant η and γ  we may choose v = 1
n ∼ Ω
. Now  same as earlier  to have low sample complexity  we must

1−tanh(λ)γ+1

(cid:17)

log p

p

3In fact  for λ ≤ 3  we have eλ2/2 ≤ cosh(2λ) ≤ e2λ2. For λ > 3  cosh(2λ) > 200

6

have λ = O(1/p1/2(γ+1))  in which case  we get a n ∼ Ω(p1/(γ+1) log p) sample requirement from
the ﬁrst term.
We note that the family Gp η γ is also studied in [1]  and for which  an algorithm is proposed. Under
certain assumptions in [1]  and the restrictions: η = O(1)  and γ is large enough  the algorithm in
[1] requires log p
samples  which is matched by the ﬁrst term in our lower bound. Therefore  the
λ2
algorithm in [1] is optimal  for the setting considered.
5.2 Girth Bounded Graphs
The girth of a graph is deﬁned as the length of its shortest cycle. Let Gp g d be the set of all graphs
with girth atleast g  and maximum degree d. Note that as girth increases the learning problem
becomes easier  with the extreme case of g = ∞ (i.e. trees) being solved by the well known Chow-
Liu algorithm[3] in O(log p) samples. We have the following theorem:
Theorem 3. Consider the graph class Gp g d. For any ν ∈ (0  1)  let dν = min

d  p1−ν

(cid:16)

(cid:17)

. If

g

 log(p/2)

λ tanh λ  

(cid:18)

1+

(cid:19)dν

1+tanh(λ)g−1
1−tanh(λ)g−1

2λ

ν log(p)

n ≤ (1 − δ) max

be the set of all graphs whose vertices have degree d or degree d − 1. Note that this class

5.3 Approximate d-Regular Graphs
Let Gapprox
is subset of the class of graphs with degree at most d. We have:
Theorem 4. Consider the class Gapprox

. If n ≤ (1−δ) max

p d

p d

then pmax ≥ δ.

  then pmax ≥ δ.
(cid:26) log( pd
(cid:16) pd

4 )

λ tanh λ   eλd

2λdeλ

4

(cid:17)(cid:27)

Note that the second term in the bound above is from [16]. Now  restricting λ to prevent exponential
growth in the number of samples  we get a sample requirement of n = Ω(d2 log p). This matches
the lower bound for degree d bounded graphs in [16]. However  note that Theorem 4 is stronger in
the sense that the bound holds for a smaller class of graphs i.e. only approximately d-regular  and
not d-bounded.
5.4 Approximate Edge Bounded Graphs
Let Gapprox
of graphs with edges at most k. Here  we have:
Theorem 5. Consider the class Gapprox

be the set of all graphs with number of edges ∈(cid:2) k
(cid:26) log( k

2   k(cid:3). This class is a subset of the class

  and let k ≥ 9. If we have number of samples n ≤ (1 −
  then pmax ≥ δ.

log(cid:0) k

(cid:1)(cid:27)

√
2k−1)
√

δ) max

p k

p k

2 )
λ tanh λ  

eλ(
2λeλ(

2k+1)

2

Note that the second term in the bound above is from [16]. If we restrict λ to prevent exponential
growth in the number of samples  we get a sample requirement of n = Ω(k log k). Again  we match
the lower bound for the edge bounded class in [16]  but through a smaller class.
6 Erd˝os-Rényi graphs G(p  c/p)
In this section  we relate the number of samples required to learn G ∼ G(p  c/p) for the dense case 
for guaranteeing a constant average probability of error pavg. We have the following main result
whose proof can be found in the Appendix.
Theorem 6. Let G ∼ G(p  c/p)  c = Ω(p3/4 + (cid:48))  (cid:48) > 0. For this class of random graphs  if
pavg ≤ 1/90  then n ≥ max (n1  n2) where:

n1 =

 4λp

H(c/p)(3/80) (1 − 80pavg − O(1/p))

3 exp(− p

36 ) + 4 exp(− p

3
2

144 ) +

   n2 =

H(c/p)(1 − 3pavg) − O(1/p)

p
4

(6)

(cid:18)

9

4λ

1+(cosh(2λ))

(cid:19)

c2
6p

7

Remark 3. In the denominator of the ﬁrst expression  the dominating term is

(cid:18)

9

4λ

1+(cosh(2λ))

(cid:19) .

c2
6p

Therefore  we have the following corollary.
Corollary 4. Let G ∼ G(p  c/p)  c = Ω(p3/4+(cid:48)

) for any (cid:48) > 0. Let pavg ≤ 1/90  then

(cid:16)

√
1. λ = Ω(
p/c) : Ω
√

(cid:17)

c2
6p

λH(c/p)(cosh(2λ))

samples are needed.

2. λ < O(

√
Remark 4. This means that when λ = Ω(

samples are required. Hence  for any efﬁcient algorithm  we require λ = O(cid:0)√

p/c) : Ω(c log p) samples are needed. (This bound is from [1] )

p/c(cid:1) and in this

p/c)  a huge number (exponential for constant λ) of

regime O (c log p) samples are required to learn.

6.1 Proof Outline
The proof skeleton is based on Lemma 2. The essence of the proof is to cover a set of graphs T  
with large measure  by an exponentially small set where the KL-divergence between any covered
and the covering graph is also very small. For this we use Corollary 3. The key steps in the proof
are outlined below:

2   cp

2 + cp

2 − cp

µ(T ) = 1 − o(1). A natural candidate is the ’typical’ set T p
graphs each with ( cp
2 ) edges in the graph.

1. We identify a subclass of graphs T   as in Lemma 2  whose measure is close to 1  i.e.
 which is deﬁned to be a set of
2. (Path property) We show that most graphs in T have property R: there are O(p2) pairs of
p ) node disjoint paths of length 2 with

nodes such that every pair is well connected by O( c2
high probability. The measure µ(R|T ) = 1 − δ1.

3. (Covering with low diameter) Every graph G in R(cid:84)T is covered by a graph G(cid:48) from

a covering set CR(δ2) such that their edge set differs only in the O(p2) nodes that are
well connected. Therefore  by Corollary 3  KL-divergence between G and G(cid:48) is very small
(δ2 = O(λp2 cosh(λ)−c2/p)).

4. (Efﬁcient covering in Size) Further  the covering set CR is exponentially smaller than T .
5. (Uncovered graphs have exponentially low measure) Then we show that the uncovered

graphs have large KL-divergence(cid:0)O(p2λ)(cid:1) but their measure µ(Rc |T ) is exponentially

small.

6. Using a similar (but more involved) expression for probability of error as in Corollary 2 

roughly we need O( log|T|

δ1+δ2

) samples.

The above technique is very general. Potentially this could be applied to other random graph classes.

7 Summary

In this paper  we have explored new approaches for computing sample complexity lower bounds
for Ising models. By explicitly bringing out the dependence on the weights of the model  we have
shown that unless the weights are restricted  the model may be hard to learn. For example  it is hard
to learn a graph which has many paths between many pairs of vertices  unless λ is controlled. For the
random graph setting  Gp c/p  while achievability is possible in the c = poly log p case[1]  we have
shown lower bounds for c > p0.75. Closing this gap remains a problem for future consideration.
The application of our approaches to other deterministic/random graph classes such as the Chung-
Lu model[4] (a generalization of Erd˝os-Rényi graphs)  or small-world graphs[18] would also be
interesting.

Acknowledgments

R.T. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803 
IIS-1320894  IIS-1447574  and DMS-1264033. K.S. and A.D. acknowledge the support of NSF via
CCF 1422549  1344364  1344179 and DARPA STTR and a ARO YIP award.

8

References
[1] Animashree Anandkumar  Vincent YF Tan  Furong Huang  Alan S Willsky  et al. High-
dimensional structure estimation in ising models: Local separation criterion. The Annals of
Statistics  40(3):1346–1375  2012.

[2] Guy Bresler  Elchanan Mossel  and Allan Sly. Reconstruction of markov random ﬁelds from
samples: Some observations and algorithms. In Proceedings of the 11th international work-
shop  APPROX 2008  and 12th international workshop  RANDOM 2008 on Approximation 
Randomization and Combinatorial Optimization: Algorithms and Techniques  APPROX ’08 /
RANDOM ’08  pages 343–356. Springer-Verlag  2008.

[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees.

IEEE Trans. Inf. Theor.  14(3):462–467  September 2006.

[4] Fan Chung and Linyuan Lu. Complex Graphs and Networks. American Mathematical Society 

August 2006.

[5] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in

Telecommunications and Signal Processing). Wiley-Interscience  2006.

[6] G. Cross and A. Jain. Markov random ﬁeld texture models. IEEE Trans. PAMI  5:25–39  1983.
[7] Amir Dembo and Andrea Montanari. Ising models on locally tree-like graphs. The Annals of

Applied Probability  20(2):565–592  04 2010.

[8] Abbas El Gamal and Young-Han Kim. Network information theory. Cambridge University
[9] Ashish Goel  Michael Kapralov  and Sanjeev Khanna. Perfect matchings in o(n\logn) time in

Press  2011.

regular bipartite graphs. SIAM Journal on Computing  42(3):1392–1404  2013.

[10] M. Hassner and J. Sklansky. Markov random ﬁeld models of digitized image texture.

ICPR78  pages 538–540  1978.

In

[11] E. Ising. Beitrag zur theorie der ferromagnetismus. Zeitschrift für Physik  31:253–258  1925.
[12] Stasys Jukna. Extremal combinatorics  volume 2. Springer  2001.
[13] C. D. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. MIT

Press  1999.

[14] Garvesh Raskutti  Martin J. Wainwright  and Bin Yu. Minimax rates of estimation for high-
dimensional linear regression over (cid:96)q-balls. IEEE Trans. Inf. Theor.  57(10):6976–6994  Octo-
ber 2011.

[15] B. D. Ripley. Spatial statistics. Wiley  New York  1981.
[16] Narayana P Santhanam and Martin J Wainwright.

binary graphical models in high dimensions.
58(7):4117–4134  2012.

Information-theoretic limits of selecting
Information Theory  IEEE Transactions on 

[17] R. Tandon and P. Ravikumar. On the difﬁculty of learning power law graphical models. In In

IEEE International Symposium on Information Theory (ISIT)  2013.

[18] Duncan J. Watts and Steven H. Strogatz. Collective dynamics of ’small-world’ networks.

Nature  393(6684):440–442  June 1998.

[19] J.W. Woods. Markov image modeling. IEEE Transactions on Automatic Control  23:846–850 

October 1978.

[20] Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of

convergence. Annals of Statistics  pages 1564–1599  1999.

[21] Yuchen Zhang  John Duchi  Michael Jordan  and Martin J Wainwright. Information-theoretic
In Ad-
lower bounds for distributed statistical estimation with communication constraints.
vances in Neural Information Processing Systems 26  pages 2328–2336. Curran Associates 
Inc.  2013.

9

,Rashish Tandon
Karthikeyan Shanmugam
Pradeep Ravikumar
Alexandros Dimakis
Zhongwen Xu
Hado van Hasselt
David Silver