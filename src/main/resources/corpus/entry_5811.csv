2007,Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes,We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast  greedy algorithm introduced by Hinton et.al. If the data is high-dimensional and highly-structured  a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.,Using Deep Belief Nets to Learn Covariance Kernels

for Gaussian Processes

Ruslan Salakhutdinov and Geoffrey Hinton

Department of Computer Science  University of Toronto

6 King’s College Rd  M5S 3G4  Canada

rsalakhu hinton@cs.toronto.edu

Abstract

We show how to use unlabeled data and a deep belief net (DBN) to learn a good
covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model
of the unlabeled data using the fast  greedy algorithm introduced by [7]. If the
data is high-dimensional and highly-structured  a Gaussian kernel applied to the
top layer of features in the DBN works much better than a similar kernel applied
to the raw input. Performance at both regression and classiﬁcation can then be
further improved by using backpropagation through the DBN to discriminatively
ﬁne-tune the covariance kernel.

1 Introduction
Gaussian processes (GP’s) are a widely used method for Bayesian non-linear non-parametric re-
gression and classiﬁcation [13  16]. GP’s are based on deﬁning a similarity or kernel function that
encodes prior knowledge of the smoothness of the underlying process that is being modeled. Be-
cause of their ﬂexibility and computational simplicity  GP’s have been successfully used in many
areas of machine learning.

Many real-world applications are characterized by high-dimensional  highly-structured data with a
large supply of unlabeled data but a very limited supply of labeled data. Applications such as infor-
mation retrieval and machine vision are examples where unlabeled data is readily available. GP’s
are discriminative models by nature and within the standard regression or classiﬁcation scenario 
unlabeled data is of no use. Given a set of i.i.d.
n=1 and their
associated target labels {yn}N
n=1 ∈ {−1  1} for regression/classiﬁcation  GP’s
n=1 ∈ R or {yn}N
model p(yn|xn) directly. Unless some assumptions are made about the underlying distribution of
the input data X = [Xl  Xu]  unlabeled data  Xu  cannot be used. Many researchers have tried to
use unlabeled data by incorporating a model of p(X). For classiﬁcation tasks  [11] model p(X) as
p(xn|yn)p(yn) and then infer p(yn|xn)  [15] attempts to learn covariance kernels
based on p(X)  and [10] assumes that the decision boundaries should occur in regions where the
data density  p(X)  is low. When faced with high-dimensional  highly-structured data  however 
none of the existing approaches have proved to be particularly successful.

labeled input vectors Xl = {xn}N

a mixture Pyn

In this paper we exploit two properties of DBN’s. First  they can be learned efﬁciently from unla-
beled data and the top-level features generally capture signiﬁcant  high-order correlations in the data.
Second  they can be discriminatively ﬁne-tuned using backpropagation. We ﬁrst learn a DBN model
of p(X) in an entirely unsupervised way using the fast  greedy learning algorithm introduced by [7]
and further investigated in [2  14  6]. We then use this generative model to initialize a multi-layer 
non-linear mapping F (x|W )  parameterized by W   with F : X → Z mapping the input vectors in
X into a feature space Z. Typically the mapping F (x|W ) will contain millions of parameters. The
top-level features produced by this mapping allow fairly accurate reconstruction of the input  so they
must contain most of the information in the input vector  but they express this information in a way
that makes explicit a lot of the higher-order structure in the input data.
After learning F (x|W )  a natural way to deﬁne a kernel function is to set K(xi  xj) =
exp (−||F (xi|W ) − F (xj|W )||2). Note that the kernel is initialized in an entirely unsupervised
way. The parameters W of the covariance kernel can then be ﬁne-tuned using the labeled data by

1

maximizing the log probability of the labels with respect to W . In the ﬁnal model most of the in-
formation for learning a covariance kernel will have come from modeling the input data. The very
limited information in the labels will be used only to slightly adjust the layers of features already
discovered by the DBN.

2 Gaussian Processes for Regression and Binary Classiﬁcation
For a regression task  we are given a data set D of i.i.d . labeled input vectors Xl = {xn}N
their corresponding target labels {yn}N
regression model:

n=1 and
n=1 ∈ R. We are interested in the following probabilistic

ǫ ∼ N (ǫ|0  σ2)

yn = f (xn) + ǫ 

(1)
A Gaussian process regression places a zero-mean GP prior over the underlying latent function f
we are modeling  so that a-priori p(f |Xl) =N (f |0  K)  where f = [f (x1)  ...  f (xn)]T and K is the
covariance matrix  whose entries are speciﬁed by the covariance function Kij = K(xi  xj). The
covariance function encodes our prior notion of the smoothness of f  or the prior assumption that
if two input vectors are similar according to some distance measure  their labels should be highly
correlated. In this paper we will use the spherical Gaussian kernel  parameterized by θ = {α  β}:

Integrating out the function values f  the marginal log-likelihood takes form:

Kij = α exp(cid:0) −

(xi − xj)T (xi − xj)(cid:1)

L = log p(y|Xl) = −

log 2π −

log |K + σ2I| −

yT (K + σ2I)−1y

1
2

1
2β

1
2

N
2

(2)

(3)

(5)

(6)

which can then be maximized with respect to the parameters θ and σ. Given a new test point x
  a
∗
prediction is obtained by conditioning on the observed data and θ. The distribution of the predicted
value y

takes the form:
  D  θ  σ2) = N (y

∗
p(y

at x
∗
|x
∗

∗
= K(x
∗

|kT
∗
∗
= K(x
∗

(K + σ2I)−1y  k

− kT
∗

(K + σ2I)−1k
∗

∗∗

+ σ2)

(4)

  Xl)  and k

where k
∗
For a binary classiﬁcation task  we similarly place a zero mean GP prior over the underlying latent
function f  which is then passed through the logistic function g(x) = 1/(1 + exp(−x)) to deﬁne a
prior p(yn = 1|xn) = g(f (xn)). Given a new test point x
  inference is done by ﬁrst obtaining the
∗
distribution over the latent function f

  x
∗

∗∗

):

).

= f (x
∗

∗

p(f

|x
∗

∗

  D) = Z p(f

|x
∗

∗

  Xl  f )p(f |Xl  y)df

which is then used to produce a probabilistic prediction:

p(y

= 1|x
∗

∗

  D) = Z g(f

∗

)p(f

|x
∗

∗

  D)df

∗

The non-Gaussian likelihood makes the integral in Eq. 5 analytically intractable. In our experiments 
we approximate the non-Gaussian posterior p(f |Xl  y) with a Gaussian one using expectation prop-
agation [12]. For more thorough reviews and implementation details refer to [13  16].

3 Learning Deep Belief Networks (DBN’s)
In this section we describe an unsupervised way of learning a DBN model of the input data X =
[Xl  Xu]  that contains both labeled and unlabeled data sets. A DBN can be trained efﬁciently by
using a Restricted Boltzmann Machine (RBM) to learn one layer of hidden features at a time [7].
Welling et. al. [18] introduced a class of two-layer undirected graphical models that generalize
RBM’s to exponential family distributions. This framework will allow us to model real-valued
images of face patches and word-count vectors of documents.

3.1 Modeling Real-valued Data
We use a conditional Gaussian distribution for modeling observed “visible” pixel values x (e.g.
images of faces) and a conditional Bernoulli distribution for modeling “hidden” features h (Fig. 1):

p(xi = x|h) = 1

√2πσi

exp(−

(x−bi−σiPj

2σ2
i

hj wij )2

)

p(hj = 1|x) = g(cid:0)bj +Pi wij

xi

σi(cid:1)

2

(7)

(8)

h

W

x

Binary
Hidden Features

Gaussian
Visible
Units

1000
W
3
1000

1000
W
2
1000

1000
W
1

RBM

RBM

RBM

target y

GP

1000
T
W
3
1000
T
W
2
1000
T
W
1

Feature
Representation
F(X|W)

Input X

Figure 1: Left panel: Markov random ﬁeld of the generalized RBM. The top layer represents stochastic binary
hidden features h and and the bottom layer is composed of linear visible units x with Gaussian noise. When
using a Constrained Poisson Model  the top layer represents stochastic binary latent topic features h and the
bottom layer represents the Poisson visible word-count vector x. Middle panel: Pretraining consists of learning
a stack of RBM’s. Right panel: After pretraining  the RBM’s are used to initialize a covariance function of the
Gaussian process  which is then ﬁne-tuned by backpropagation.

where g(x) = 1/(1 + exp(−x)) is the logistic function  wij is a symmetric interaction term between
input i and feature j  σ2
i is the variance of input i  and bi  bj are biases. The marginal distribution
over visible vector x is:

p(x) = Xh

exp (−E(x  h))

RuPg exp (−E(u  g))du

(9)

. The param-

where E(x  h) is an energy term: E(x  h) = Pi

eter updates required to perform gradient ascent in the log-likelihood is obtained from Eq. 9:

(xi−bi)2

2σ2
i

−Pj bjhj −Pi j hjwij

xi
σi

∆wij = ǫ

∂ log p(x)

∂wij

= ǫ(<zihj>data − <zihj>model)

(10)

where ǫ is the learning rate  zi = xi/σi  < ·>data denotes an expectation with respect to the data
distribution and < ·>model is an expectation with respect to the distribution deﬁned by the model.
To circumvent the difﬁculty of computing <·>model  we use 1-step Contrastive Divergence [5]:

∆wij = ǫ(<zihj>data − <zihj>recon)

(11)
The expectation < zihj >data deﬁnes the expected sufﬁcient statistics of the data distribution and
is computed as zip(hj = 1|x) when the features are being driven by the observed data from the
training set using Eq. 8. After stochastically activating the features  Eq. 7 is used to “reconstruct”
real-valued data. Then Eq. 8 is used again to activate the features and compute <zihj>recon when
the features are being driven by the reconstructed data. Throughout our experiments we set variances
i = 1 for all visible units i  which facilitates learning. The learning rule for the biases is just a
σ2
simpliﬁed version of Eq. 11.

3.2 Modeling Count Data with the Constrained Poisson Model
We use a conditional “constrained” Poisson distribution for modeling observed “visible” word count
data x and a conditional Bernoulli distribution for modeling “hidden” topic features h:

exp (λi +Pj hjwij )
Pk exp(cid:0)λk +Pj hjWkj(cid:1)

× N(cid:19)  p(hj = 1|x) = g(bj +Xi

p(xi = n|h) = Pois(cid:18)n 
where Pois(cid:0)n  λ(cid:1) = e−λλn/n!  wij is a symmetric interaction term between word i and feature
j  N = Pi xi is the total length of the document  λi is the bias of the conditional Poisson model

for word i  and bj is the bias of feature j. The Poisson rate  whose log is shifted by the weighted
combination of the feature activations  is normalized and scaled up by N . We call this the “Con-
strained Poisson Model” since it ensures that the mean Poisson rates across all words sum up to the
length of the document. This normalization is signiﬁcant because it makes learning stable and it
deals appropriately with documents of different lengths.

wij xi) (12)

3

The marginal distribution over visible count vectors x is given in Eq. 9 with an “energy” given by

E(x  h) = −Xi

λixi +Xi

log (xi!) −Xj

bjhj −Xi j

xihjwij

The gradient of the log-likelihood function is:

∆wij = ǫ

∂ log p(v)

∂wij

= ǫ(<xihj>data − <xihj>model)

(13)

(14)

3.3 Greedy Recursive Learning of Deep Belief Nets

A single layer of binary features is not the best way to capture the structure in the input data. We
now describe an efﬁcient way to learn additional layers of binary features.

After learning the ﬁrst layer of hidden features we have an undirected model that deﬁnes p(v  h)
by deﬁning a consistent pair of conditional probabilities  p(h|v) and p(v|h) which can be used to
sample from the model distribution. A different way to express what has been learned is p(v|h) and
p(h). Unlike a standard  directed model  this p(h) does not have its own separate parameters. It is a
complicated  non-factorial prior on h that is deﬁned implicitly by p(h|v) and p(v|h). This peculiar
decomposition into p(h) and p(v|h) suggests a recursive algorithm: keep the learned p(v|h) but
replace p(h) by a better prior over h  i.e. a prior that is closer to the average  over all the data
vectors  of the conditional posterior over h. So after learning an undirected model  the part we keep
is part of a multilayer directed model.

We can sample from this average conditional posterior by simply using p(h|v) on the training data
and these samples are then the “data” that is used for training the next layer of features. The only
difference from learning the ﬁrst layer of features is that the “visible” units of the second-level RBM
are also binary [6  3]. The learning rule provided in the previous section remains the same [5].
We could initialize the new RBM model by simply using the existing learned model but with the
roles of the hidden and visible units reversed. This ensures that p(v) in our new model starts out
being exactly the same as p(h) in our old one. Provided the number of features per layer does not
decrease  [7] show that each extra layer increases a variational lower bound on the log probability
of data. To suppress noise in the learning signal  we use the real-valued activation probabilities for
the visible units of every RBM  but to prevent hidden units from transmitting more than one bit of
information from the data to its reconstruction  the pretraining always uses stochastic binary values
for the hidden units.

The greedy  layer-by-layer training can be repeated several times to learn a deep  hierarchical model
in which each layer of features captures strong high-order correlations between the activities of
features in the layer below.

4 Learning the Covariance Kernel for a Gaussian Process
After pretraining  the stochastic activities of the binary features in each layer are replaced by deter-
ministic  real-valued probabilities and the DBN is used to initialize a multi-layer  non-linear map-
ping f (x|W ) as shown in ﬁgure 1. We deﬁne a Gaussian covariance function  parameterized by
θ = {α  β} and W as:

Note that this covariance function is initialized in an entirely unsupervised way. We can now maxi-
mize the log-likelihood of Eq. 3 with respect to the parameters of the covariance function using the
labeled training data[9]. The derivative of the log-likelihood with respect to the kernel function is:

∂L
∂Ky

=

1

2(cid:0)K−1

y yyT K−1

y − K−1
y (cid:1)

(16)

where Ky = K + σ2I is the covariance matrix. Using the chain rule we readily obtain the necessary
gradients:

∂L
∂θ

=

∂L
∂Ky

∂Ky
∂θ

and

∂L
W

=

∂L
∂Ky

∂Ky

∂F (x|W )

∂F (x|W )

∂W

(17)

4

Kij = α exp(cid:0) −

||F (xi|W ) − F (xj |W )||2(cid:1)

1
2β

(15)

−22.07

32.99

−41.15

66.38

27.49

Unlabeled

Training Data

Test Data

A

B

Figure 2: Top panel A: Randomly sampled examples of the training and test data. Bottom panel B: The same
sample of the training and test images but with rectangular occlusions.

Training GPstandard GP-DBNgreedy GP-DBNﬁne
ARD
labels
15.01
6.84
6.31
18.59
10.12
9.23

ARD Sph.
28.57
17.94
12.71
18.16
11.22
16.36
23.15
28.32
15.16
21.06
17.98
14.15

Sph.
15.28
7.25
6.42
19.75
10.56
9.13

ARD
18.37
8.96
8.77
19.42
11.01
10.43

A 100
500
1000
B 100
500
1000

Sph.
22.24
17.25
16.33
26.94
20.20
19.20

GPpca

Sph.
18.13 (10)
14.75 (20)
14.86 (20)
25.91 (10)
17.67 (10)
16.26 (10)

ARD
16.47 (10)
10.53 (80)
10.00 (160)
19.27 (20)
14.11 (20)
11.55 (80)

Table 1: Performance results on the face-orientation regression task. The root mean squared error (RMSE) on
the test set is shown for each method using spherical Gaussian kernel and Gaussian kernel with ARD hyper-
parameters. By row: A) Non-occluded face data  B) Occluded face data. For the GPpca model  the number of
principal components that performs best on the test data is shown in parenthesis.

where ∂F (x|W )/∂W is computed using standard backpropagation. We also optimize the observa-
tion noise σ2. It is necessary to compute the inverse of Ky  so each gradient evaluation has O(N 3)
complexity where N is the number of the labeled training cases. When learning the restricted Boltz-
mann machines that are composed to form the initial DBN  however  each gradient evaluation scales
linearly in time and space with the number of unlabeled training cases. So the pretraining stage
can make efﬁcient use of very large sets of unlabeled data to create sensible  high-level features and
when the amount of labeled data is small. Then the very limited amount of information in the labels
can be used to slightly reﬁne those features rather than to create them.

5 Experimental Results
In this section we present experimental results for several regression and classiﬁcation tasks that
involve high-dimensional  highly-structured data. The ﬁrst regression task is to extract the orienta-
tion of a face from a gray-level image of a large patch of the face. The second regression task is
to map images of handwritten digits to a single real-value that is as close as possible to the integer
represented by the digit in the image. The ﬁrst classiﬁcation task is to discriminate between images
of odd digits and images of even digits. The second classiﬁcation task is to discriminate between
two different classes of news story based on the vector of word counts in each story.

5.1 Extracting the Orientation of a Face Patch
The Olivetti face data set contains ten 64×64 images of each of forty different people. We con-
structed a data set of 13 000 28×28 images by randomly rotating (−90◦ to +90◦)  cropping  and
subsampling the original 400 images. The data set was then subdivided into 12 000 training images 
which contained the ﬁrst 30 people  and 1 000 test images  which contained the remaining 10 peo-
ple. 1 000 randomly sampled face patches from the training set were assigned an orientation label.
The remaining 11 000 training images were used as unlabeled data. We also made a more difﬁcult
version of the task by occluding part of each face patch with randomly chosen rectangles. Panel A
of ﬁgure 2 shows randomly sampled examples from the training and test data.

For training on the Olivetti face patches we used the 784-1000-1000-1000 architecture shown in
ﬁgure 1. The entire training set of 12 000 unlabeled images was used for greedy  layer-by-layer
training of a DBN model. The 2.8 million parameters of the DBN model may seem excessive for
12 000 training cases  but each training case involves modeling 625 real-values rather than just a
single real-valued label. Also  we only train each layer of features for a few passes through the
training data and we penalize the squared weights.

5

1.0 

0.8 

2
1
3
 
e
r
u
t
a
e
F

0.6 

0.4 

0.2 

Input Pixel Space

2

3

log β

4

5

6

Feature Space

More Relevant

45

40

35

30

25

20

15

10

5

0
1

90

80

70

60

50

40

30

20

10

 0  

0.2 

0.4 
0.6 
Feature 992

0.8 

1.0 

0
−1

0

1

2

log β

3

4

5

6

Figure 3: Left panel shows a scatter plot of the two most relevant features  with each point replaced by the
corresponding input test image. For better visualization  overlapped images are not shown. Right panel displays
the histogram plots of the learned ARD hyper-parameters log β.

After the DBN has been pretrained on the unlabeled data  a GP model was ﬁtted to the labeled
data using the top-level features of the DBN model as inputs. We call this model GP-DBNgreedy.
GP-DBNgreedy can be signiﬁcantly improved by slightly altering the weights in the DBN. The
GP model gives error derivatives for its input vectors which are the top-level features of the DBN.
These derivatives can be backpropagated through the DBN to allow discriminative ﬁne-tuning of
the weights. Each time the weights in the DBN are updated  the GP model is also reﬁtted. We call
this model GP-DBNﬁne. For comparison  we ﬁtted a GP model that used the pixel intensities of
the labeled images as its inputs. We call this model GPstandard. We also used PCA to reduce the
dimensionality of the labeled images and ﬁtted several different GP models using the projections
onto the ﬁrst m principal components as the input. Since we only want a lower bound on the error
of this model  we simply use the value of m that performs best on the test data. We call this model
GPpca. Table 1 shows the root mean squared error (RMSE) of the predicted face orientations using
all four types of GP model on varying amounts of labeled data. The results show that both GP-
DBNgreedy and GP-DBNﬁne signiﬁcantly outperform a regular GP model. Indeed  GP-DBNﬁne
with only 100 labeled training cases outperforms GPstandard with 1000.

To test the robustness of our approach to noise in the input we took the same data set and created
artiﬁcial rectangular occlusions (see Fig. 2  panel B). The number of rectangles per image was
drawn from a Poisson with λ = 2. The top-left location  length and width of each rectangle was
sampled from a uniform [0 25]. The pixel intensity of each occluding rectangle was set to the mean
pixel intensity of the entire image. Table 1 shows that the performance of all models degrades  but
their relative performances remain the same and GP-DBNﬁne on occluded data is still much better
than GPstandard on non-occluded data.

We have also experimented with using a Gaussian kernel with ARD hyper-parameters  which is a
common practice when the input vectors are high-dimensional:

Kij = α exp(cid:0) −

(xi − xj)T D(xi − xj)(cid:1)

1
2

(18)

where D is the diagonal matrix with Dii = 1/βi  so that the covariance function has a separate
length-scale parameter for each dimension. ARD hyper-parameters were optimized by maximizing
the marginal log-likelihood of Eq. 3. Table 1 shows that ARD hyper-parameters do not improve
GPstandard  but they do slightly improve GP-DBNﬁne and they strongly improve GP-DBNgreedy
and GPpca when there are 500 or 1000 labeled training cases.

The histogram plot of log β in ﬁgure 3 reveals that there are a few extracted features that are very
relevant (small β) to our prediction task. The same ﬁgure (left panel) shows a scatter plot of the two
most relevant features of GP-DBNgreedy model  with each point replaced by the corresponding in-
put test image. Clearly  these two features carry a lot of information about the orientation of the face.

6

Train
labels

A 100
500
1000
100
500
1000

B

GPstandard
ARD
2.27
1.62
1.36

Sph.
1.86
1.42
1.25

GP-DBNgreedy
ARD
Sph.
1.61
1.68
1.27
1.19
1.07
1.14

GP-DBNﬁne
ARD
1.58
1.22
1.10

Sph.
1.63
1.16
1.03

0.0884
0.0222
0.0129

0.1087
0.0541
0.0385

0.0528
0.0100
0.0058

0.0597
0.0161
0.0059

0.0501
0.0055
0.0050

0.0599
0.0104
0.0100

GPpca

Sph.
1.73 (20)
1.32 (40)
1.19 (40)
0.0785 (10)
0.0160 (40)
0.0091 (40)

ARD
2.00 (20)
1.36 (20)
1.22 (80)
0.0920 (10)
0.0235 (20)
0.0127 (40)

Table 2: Performance results on the digit magnitude regression task (A) and and discriminating odd vs. even
digits classiﬁcation task (B). The root mean squared error for regression task on the test set is shown for each
method. For classiﬁcation task the area under the ROC (AUROC) metric is used. For each method we show
1-AUROC on the test set. All methods were tried using both spherical Gaussian kernel  and a Gaussian kernel
with ARD hyper-parameters. For the GPpca model  the number of principal components that performs best on
the test data is shown in parenthesis.

Number of labeled
cases (50% in each class)
100
500
1000

GPstandard GP-DBNgreedy GP-DBNﬁne

0.1295
0.0875
0.0645

0.1180
0.0793
0.0580

0.0995
0.0609
0.0458

Table 3: Performance results using the area under the ROC (AUROC) metric on the text classiﬁcation task.
For each method we show 1-AUROC on the test set.
We suspect that the GP-DBNﬁne model does not beneﬁt as much from the ARD hyper-parameters
because the ﬁne-tuning stage is already capable of turning down the activities of irrelevant top-level
features.

5.2 Extracting the Magnitude Represented by a Handwritten Digit and Discriminating

between Images of Odd and Even Digits

The MNIST digit data set contains 60 000 training and 10 000 test 28×28 images of ten handwritten
digits (0 to 9). 100 randomly sampled training images of each class were assigned a magnitude label.
The remaining 59 000 training images were used as unlabeled data. As in the previous experiment 
we used the 784-1000-1000-1000 architecture with the entire training set of 60 000 unlabeled digits
being used for greedily pretraining the DBN model. Table 2  panel A  shows that GP-DBNﬁne and
GP-DBNgreedy perform considerably better than GPstandard both with and without ARD hyper-
parameters. The same table  panel B  shows results for the classiﬁcation task of discriminating be-
tween images of odd and images of even digits. We used the same labeled training set  but with each
digit categorized into an even or an odd class. The same DBN model was used  so the Gaussian co-
variance function was initialized in exactly the same way for both regression and classiﬁcation tasks.
The performance of GP-DBNgreedy demonstrates that the greedily learned feature representation
captures a lot of structure in the unlabeled input data which is useful for subsequent discrimination
tasks  even though these tasks are unknown when the DBN is being trained.

5.3 Classifying News Stories
The Reuters Corpus Volume II is an archive of 804 414 newswire stories The corpus covers four
major groups: Corporate/Industrial  Economics  Government/Social  and Markets. The data was
randomly split into 802 414 training and 2000 test articles. The test set contains 500 articles of each
major group. The available data was already in a convenient  preprocessed format  where common
stopwords were removed and all the remaining words were stemmed. We only made use of the 2000
most frequently used word stems in the training data. As a result  each document was represented
as a vector containing 2000 word counts. No other preprocessing was done.

For the text classiﬁcation task we used a 2000-1000-1000-1000 architecture. The entire unlabeled
training set of 802 414 articles was used for learning a multilayer generative model of the text docu-
ments. The bottom layer of the DBN was trained using a Constrained Poisson Model. Table 3 shows
the area under the ROC curve for classifying documents belonging to the Corporate/Industrial vs.
Economics groups. As expected  GP-DBNﬁne and GP-DBNgreedy work better than GPstandard.
The results of binary discrimination between other pairs of document classes are very similar to the
results presented in table 3. Our experiments using a Gaussian kernel with ARD hyper-parameters
did not show any signiﬁcant improvements. Examining the histograms of the length-scale parame-

7

ters β  we found that most of the input word-counts as well as most of the extracted features were
relevant to the classiﬁcation task.

6 Conclusions and Future Research
In this paper we have shown how to use Deep Belief Networks to greedily pretrain and discrimina-
tively ﬁne-tune a covariance kernel for a Gaussian Process. The discriminative ﬁne-tuning produces
an additional improvement in performance that is comparable in magnitude to the improvement pro-
duced by using the greedily pretrained DBN. For high-dimensional  highly-structured data  this is
an effective way to make use of large unlabeled data sets  especially when labeled training data is
scarce. Greedily pretrained DBN’s can also be used to provide input vectors for other kernel-based
methods  including SVMs [17  8] and kernel regression [1]  and our future research will concentrate
on comparing our method to other kernel-based semi-supervised learning algorithms [4  19].

Acknowledgments
We thank Radford Neal for many helpful suggestions. This research was supported by NSERC  CFI
and OTI. GEH is a fellow of CIAR and holds a CRC chair.

References

[1] J. K. Benedetti. On the nonparametric estimation of regression functions. Journal of the Royal Statistical

Society series B  39:248–253  1977.

[2] Y. Bengio and Y. Le Cun. Scaling learning algorithms towards AI. In L. Bottou  O. Chapelle  D. DeCoste 

and J. Weston  editors  Large-Scale Kernel Machines. MIT Press  2007.

[3] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep networks. In

Advances in Neural Information Processing Systems  2006.

[4] O. Chapelle  B. Sch¨olkopf  and A. Zien. Semi-Supervised Learning. MIT Press  2006.
[5] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation 

14(8):1711–1800  2002.

[6] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313  2006.

[7] Geoffrey E. Hinton  Simon Osindero  and Yee Whye Teh. A fast learning algorithm for deep belief nets.

Neural Computation  18(7):1527–1554  2006.

[8] F. Lauer  C. Y. Suen  and G. Bloch. A trainable feature extractor for handwritten digit recognition. Pattern

Recognition  40(6):1816–1824  2007.

[9] N. D. Lawrence and J. Qui˜nonero Candela. Local distance preservation in the GP-LVM through back
In William W. Cohen and Andrew Moore  editors  ICML  volume 148  pages 513–520.

constraints.
ACM  2006.

[10] N. D. Lawrence and M. I. Jordan. Semi-supervised learning via gaussian processes. In NIPS  2004.
[11] N. D. Lawrence and B. Sch¨olkopf. Estimating a kernel Fisher discriminant in the presence of label
noise. In Proc. 18th International Conf. on Machine Learning  pages 306–313. Morgan Kaufmann  San
Francisco  CA  2001.

[12] T. P. Minka. Expectation propagation for approximate bayesian inference. In Jack Breese and Daphne

Koller  editors  UAI  pages 362–369  San Francisco  CA  2001. Morgan Kaufmann Publishers.

[13] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. The MIT Press  2006.
[14] R. Salakhutdinov and G. E. Hinton. Learning a nonlinear embedding by preserving class neighbourhood

structure. In AI and Statistics  2007.

[15] M. Seeger. Covariance kernels from bayesian generative models.

In Thomas G. Dietterich  Suzanna

Becker  and Zoubin Ghahramani  editors  NIPS  pages 905–912. MIT Press  2001.

[16] M. Seeger. Gaussian processes for machine learning. Int. J. Neural Syst  14(2):69–106  2004.
[17] V. Vapnik. Statistical Learning Theory. Wiley  1998.
[18] M. Welling  M. Rosen-Zvi  and G. Hinton. Exponential family harmoniums with an application to infor-

mation retrieval. In NIPS 17  pages 1481–1488  Cambridge  MA  2005. MIT Press.

[19] Xiaojin Zhu  Jaz S. Kandola  Zoubin Ghahramani  and John D. Lafferty. Nonparametric transforms of

graph kernels for semi-supervised learning. In NIPS  2004.

8

,Carl-Johann Simon-Gabriel
Adam Scibior
Ilya Tolstikhin
Bernhard Schölkopf