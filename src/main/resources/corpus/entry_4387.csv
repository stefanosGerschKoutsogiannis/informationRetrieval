2010,Predictive Subspace Learning for Multi-view Data: a Large Margin Approach,Learning from multi-view data is important in many applications  such as image classification and annotation. In this paper  we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efficient inference and parameter estimation methods for the latent subspace model. Finally  we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification  annotation and retrieval.,PredictiveSubspaceLearningforMulti-viewData:aLargeMarginApproachNingChen†‡JunZhu‡EricP.Xing‡†chenn07@mails.tsinghua.edu.cn ‡{ningchen junzhu epxing}@cs.cmu.edu†Dept.ofCS&T TNListLab StateKeyLabofITS TsinghuaUniversity Beijing100084China‡SchoolofComputerScience CarnegieMellonUniversity Pittsburgh PA15213USAAbstractLearningfrommulti-viewdataisimportantinmanyapplications suchasimageclassiﬁcationandannotation.Inthispaper wepresentalarge-marginlearningframeworktodiscoverapredictivelatentsubspacerepresentationsharedbymul-tipleviews.OurapproachisbasedonanundirectedlatentspaceMarkovnetworkthatfulﬁllsaweakconditionalindependenceassumptionthatmulti-viewobserva-tionsandresponsevariablesareindependentgivenasetoflatentvariables.Weprovideefﬁcientinferenceandparameterestimationmethodsforthelatentsub-spacemodel.Finally wedemonstratetheadvantagesoflarge-marginlearningonrealvideoandwebimagedatafordiscoveringpredictivelatentrepresentationsandimprovingtheperformanceonimageclassiﬁcation annotationandretrieval.1IntroductionInmanyscientiﬁcandengineeringapplications suchasimageannotation[28]andweb-pageclas-siﬁcation[6] theavailabledatausuallycomefromdiversedomainsorareextractedfromdifferentaspects whichwillbereferredtoasviews.Standardpredictivemethods suchassupportvectormachines arebuiltwithallthevariablesavailable withouttakingintoconsiderationthepresenceofdistinctviews.Thesemethodswouldsacriﬁcethepredictiveperformance[7]andmayalsobeincapableofperformingview-levelanalysis[12] suchaspredictingthetagsforimageannotationandanalyzingtheunderlyingrelationshipsamongstviews.Differentfromtheexistingworkthathasbeendoneonexploringmulti-viewinformationtoalleviatethedifﬁcultsemi-supervisedlearning[6 12 2 14]andunsupervisedclustering[8]problems ourgoalistodevelopastatisticalframeworkthatlearnsapredictivesubspacerepresentationsharedbymultipleviewswhenlabelsareprovidedandperformview-levelanalysis particularlyview-levelpredictions.Todiscoverasubspacerepresentationsharedbymulti-viewdata theunsupervisedcanonicalcor-relationanalysis(CCA)[17]anditskernelizedversion[1]ignorethewidelyavailablesupervisedinformation suchasimagecategories.Therefore theycoulddiscoverasubspacewithweakpredic-tiveability.Themulti-viewﬁsherdiscriminantanalysis(FDA)[13]providesasupervisedapproachtoﬁndingsuchaprojectedsubspace.However thisdeterministicapproachcannotprovideview-levelpredictions suchasimageannotation;anditwouldalsoneedadensityestimatorinordertoapplytheinformationcriterion[9]todetectviewdisagreement.Inthispaper weconsideraprob-abilisticapproachtomodelmulti-viewdata whichcanperformboththeresponse-levelpredictions(e.g. imageclassiﬁcation)andview-levelpredictions(e.g. imageannotation).Speciﬁcally weproposealarge-marginlearningapproachtodiscoveringapredictivesubspacerep-resentationformulti-viewdata.Theapproachisbasedonagenericmulti-viewlatentspaceMarkovnetwork(MN)thatfulﬁllsaweakconditionalindependenceassumptionthatthedatafromdifferentviewsandtheresponsevariablesareconditionallyindependentgivenasetoflatentvariables.Thisconditionalindependenceismuchweakerthanthetypicalassumption(e.g. intheseminalworkof1co-training[6])thatmulti-viewdataareconditionallyindependentgiventheverylowdimensionalresponsevariables[14].AlthoughdirectedBayesiannetworks(BNs)(e.g. latentDirichletallocation(LDA)[5]andprobabilisticCCA[3])canalsobedesignedtofulﬁlltheconditionalindependence theposteriorinferencecanbehardbecauseallthelatentvariablesarecoupledtogethergiventheinputvariables[26].Therefore wegroundourapproachontheundirectedMNs.Undirectedlatentvariablemodelshaveshownpromisingperformanceinmanyapplications[26 20].Inthemulti-viewMN conditionedonlatentvariables eachviewdeﬁnesajointdistributionsimilartothatinaconditionalrandomﬁeld(CRF)[18]andthusitcaneffectivelyextractlatenttopicsfromstructureddata.Forexample consideringwordorderinginformationcouldimprovethequalityofdiscoveredlatenttopics[23]comparedtoamethod(e.g. LDA)solelybasedonthenaturalbag-of-wordrep-resentation andspatialrelationshipamongregionsinanimageisalsousefulforcomputervisionapplications[15].Tolearnthemulti-viewlatentspaceMN wedevelopalarge-marginapproach whichjointlymaximizesthedatalikelihoodandminimizesthehinge-lossontrainingdata.Thelearningandinferenceproblemsareefﬁcientlysolvedwithacontrastivedivergencemethod[25].Finally weconcentrateononespecialcaseofthelarge-marginmult-viewMNandextensivelyeval-uateitonrealvideoandwebimagedatasetsforimageclassiﬁcation annotationandretrievaltasks.Ourresultsshowthatthelarge-marginapproachcanachievesigniﬁcantimprovementsintermsofpredictionperformanceanddiscoveredlatentsubspacerepresentations.Thepaperisstructuredasfollows.Sec2andSec3presentthemulti-viewlatentspaceMNanditslarge-margintraining.Sec4presentsaspecialcase.Sec5presentsempiricalresultsandSec6concludes.2Multi-viewLatentSpaceMarkovNetworks...H1HKX1XNZ1ZMZ2X2Figure1:Multi-viewMarkovnetworkswithKlatentvariables.Theunsupervisedtwo-viewlatentspaceMarkovnetworkisshowninFig.1 whichconsistsoftwoviewsofinputdataX:={Xn}andZ:={Zm}andasetoflatentvariablesH:={Hk}.Foreaseofpresentation weassumethatthevariablesoneachviewareconnectedviaalinear-chain.Extensionstomultipleviewsandmorecomplexstructuresoneachviewcanbeeasilydone afterwehavepresentedtheconstructivedeﬁnitionofthemodeldistribution.ThemodelisconstructedbasedonanunderlyingconditionalindependenceassumptionthatgiventhelatentvariablesH thetwoviewsXandZareindependent.Graphically wecanseethatboththeexponentialfamilyHarmonium(EFH)[26]anditsextensionofdual-wingHarmonium(DWH)[28]arespecialcasesofmulti-viewlatentspaceMNs.Therefore itisnotsurprisingtoseethatmulti-viewMNsinheritthewidelyadvocatedpropertyofEFHthatthemodeldistributioncanbeconstructivelydeﬁnedbasedonlocalconditionalsoneachview.Speciﬁcally weﬁrstdeﬁnemarginaldistributionsofthedataoneachviewandthelatentvariables.Foreachview weconsidertheﬁrst-orderMarkovnetwork.Bytherandomﬁeldtheory wehavep(x)=expnXiθ>iφ(xi xi+1)−A(θ)o andp(z)=expnXjη>jψ(zj zj+1)−B(η)o whereφandψarefeaturefunctions AandBarelogpartitionfunctions.ForlatentvariablesH eachcomponenthkhasanexponentialfamilydistributionandthereforethemarginaldistributionis:p(h)=Ykp(hk)=Ykexpnλ>kϕ(hk)−Ck(λk)o whereϕ(hk)isthefeaturevectorofhk Ckisanotherlog-partitionfunction.Next thejointmodeldistributionisdeﬁnedbycombiningtheabovecomponentsinthelog-domainandintroducingadditionaltermsthatcoupletherandomvariablesX ZandH.Speciﬁcally wehavep(x z h)∝expnXiθ>iφ(xi xi+1)+Xjη>jψ(zj zj+1)+Xkλ>kϕ(hk)+Xikφ(xi xi+1)>Wkiϕ(hk)+Xjkψ(zj zj+1)>Ukjϕ(hk)o.(1)Then wecandirectlywritetheconditionaldistributionsoneachviewwithshiftedparameters p(x|h)=expnPiˆθ>iφ(xi xi+1)−A(ˆθ)o whereˆθi=θi+PkWkiϕ(hk);p(z|h)=expnPjˆη>jψ(zj zj+1)−B(ˆη)o whereˆηj=ηj+PkUkjϕ(hk);andp(h|x z)=Qkexpnˆλ>kϕ(hk)−Ck(ˆλk)o whereˆλk=λk+PiWkiφ(xi xi+1)+PjUkjψ(zj zj+1).2Wecanseethatconditionedonthelatentvariables bothp(x|h)andp(z|h)aredeﬁnedintheexponentialformwithapairwisepotentialfunction whichisverysimilartoconditionalrandomﬁelds[18].Reversely wecanstartwithdeﬁningthelocalconditionaldistributionsasaboveanddirectlywritethecompatiblejointdistribution whichisofthelog-linearformasin(1).WewilluseΘtodenotealltheparameters(θ η λ W U).Sincethelatentvariablesarenotdirectlyconnected thecomplexityofinferringtheposteriordistri-butionofHisthesameasinEFHwhenalltheinputdataareobserved asreﬂectedinthefactorizedformofp(h|x z).Therefore multi-viewlatentspaceMNsdonotincreasethecomplexityontestingifourtaskdependssolelyonthelatentrepresentation(i.e. expectationofH) suchasinformationretrieval[26] classiﬁcation clusteringetc.However thecomplexityofparameterestimationandinferringtheposteriordistributionofeachview(e.g. X)willbeincreased dependingonthestruc-tureontheview.Forthesimplecaseoflinear-chain theinferencecanbeefﬁcientlydonewithaforward-backwardmessagepassingscheme[18].Forageneralmodelstructure whichmaycontainmanyloops approximateinferencesuchasvariationalmethods[22]isneededtoperformthetask.Wewillprovidemoredetailswhenpresentingthelearningproblem.Uptonow wehavestickenonunsupervisedmulti-viewlatentspaceMNs whichareofwideuseindiscoveringlatentsubspacerepresentationssharedbymulti-viewdata.Inthispaper however wearemoreinterestedinthesupervisedsettingwhereeachinputsampleisassociatedwithasupervisedresponsevariable suchasimagecategories.Accordingly ourgoalistodiscoverapredictivesubspacebyexploringthesupervisedinformation.Thesupervisedmulti-viewlatentspaceMNsaredeﬁnedsimilarlyasabove butwithanadditionalviewofresponsevariablesY.Now theconditionalindependenceis:X ZandYareindependentifHisgiven.Aswehavestated thisassumptionismuchweakerthanthetypicalconditionalindependenceassumptionthatXandZareindependentgivenY.Basedontheconstructivedeﬁnition weonlyneedtospecifytheconditionaldistributionofYgivenH.Inprinciple Ycanbecontinuousordiscrete.Here weconsiderthediscretecase wherey∈{1 ··· T} anddeﬁnep(y|h)=exp{V>f(h y)}Py0exp{V>f(h y0)} (2)wheref(h y)isthefeaturevectorwhoseelementsfrom(y−1)K+1toyKarethoseofhandallothersare0.Accordingly VisastackingparametervectorofTsub-vectorsVy ofwhicheachonecorrespondstoaclasslabely.Then thejointdistributionp(x z h y)hasthesameformasinEq.(1) butwithanadditionaltermofV>f(h y)=V>yhintheexponential.WenotethatasupervisedversionofDWH whichwillbedenotedbyTWH(i.e. triplewingHarmo-nium) wasproposedin[29] andtheparameterestimationwasdonebymaximizingthejointdatalikelihood.However theresultantTWHmodeldoesnotyieldimprovedperformancecomparedtothenaivemethodthatcombinesanunsupervisedDWHfordiscoveringlatentrepresentationsandanSVMforclassiﬁcation.Thisobservationfurthermotivatesustodevelopamorediscriminativelearningapproachtoexploringthesupervisedinformationfordiscoveringpredictivelatentsubspacerepresentations.Asweshallsee integratingthelarge-marginprincipleintooneobjectivefunctionforjointlatentsubspacemodelandpredictionmodellearningcanyieldmuchbetterresults intermsofpredictionperformanceandpredictivenessofdiscoveredlatentsubspacerepresentations.3ParameterEstimation:aLargeMarginApproachTolearnthesupervisedmulti-viewlatentspaceMNs anaturalmethodisthemaximumlikelihoodestimation(MLE) whichhasbeenwidelyusedtotraindirected[24 30]andundirectedlatentvari-ablemodels[26 20 28 29].However likelihood-basedparameterestimationpaysadditionaleffortsindeﬁninganormalizedprobabilisticmodelasinEq.(2) ofwhichthenormalizationfactorcanmaketheinferencehard especiallyindirectedmodels[24].Moreover thestandardMLEcouldre-sultinnon-conclusiveresults asreportedin[29]andveriﬁedinourexperiments.Thesehavebeenmotivatingustodevelopamorediscriminativelearningapproach.Anarguablymorediscriminativewaytolearnaclassiﬁcationmodelistodirectlyestimatethedecisionboundary whichistheessen-tialideaunderlyingtheverysuccessfullarge-marginclassiﬁers(e.g. SVMs).Here weintegratethelarge-marginideaintothelearningofsupervisedmulti-viewlatentspaceMNsformulti-viewdataanalysis analogoustothedevelopmentofMedLDA[31] whichisdirectedandhassingle-view.Forbrevity weconsiderthegeneralmulti-classclassiﬁcation asdeﬁnedabove.33.1ProblemDeﬁnitionAsinthelog-linearmodelinEq.(2) weassumethatthediscriminantfunctionF(y h;V)islinear thatis F(y h;V)=V>f(h y) wherefandVaredeﬁnedthesameasabove.Forprediction wetaketheexpectationoverthelatentvariableHanddeﬁnethepredictionruleasy∗:=argmaxyEp(h|x z)[F(H y;V)]=argmaxyV>Ep(h|x z)[f(H y)] (3)wheretheexpectationcanbeefﬁcientlycomputedwiththefactorizedformofp(h|x z)whenxandzarefullyobserved.Ifmissingvaluesexistinxorz aninferenceprocedureisneededtocomputetheexpectationofthemissedcomponents asdetailedbelowinEq.(5).Then learningistoﬁndanoptimalV∗thatminimizesalossfunction.Here weminimizethehingeloss asusedinSVMs.GiventrainingdataD={(xd zd yd)}Dd=1 thehingelossofthepredictiverule(3)isRhinge(V):=1DXdmaxy[∆‘d(y)−V>Ep(h|x z)[∆fd(y)]] where∆‘d(y)isalossfunctionthatmeasureshowdifferentthepredictionyiscomparedtothetruelabelyd andEp(h|x z)[∆fd(y)]=Ep(h|x z)[f(Hd yd)]−Ep(h|x z)[f(Hd y)].ItcanbeprovedthatthehingelossisanupperboundoftheempiricallossRemp:=1DPd∆‘(y∗d).Applyingtheprincipleofregularizedriskminimization wedeﬁnethelearningproblemassolvingminΘ VL(Θ)+12C1kVk22+C2Rhinge(V) (4)whereL(Θ):=−Pdlogp(xd zd)isthenegativedatalikelihoodandC1andC2arenon-negativeconstants whichcanbeselectedviacross-validation.NotethatRhingeisalsoafunctionofΘ.Sinceproblem(4)jointlymaximizesthedatalikelihoodandminimizesatrainingloss itcanbeexpectedthatbysolvingthisproblemwecanﬁndapredictivelatentspacerepresentationp(h|x z)andapredictionmodelparameterV whichontheonehandtendtopredictasaccurateaspossibleontrainingdata whileontheotherhandtendtoexplainthedatawell.3.2OptimizationVariationalapproximationwithContrastiveDivergence:SincethedatalikelihoodL(Θ)isgenerallyintractabletocompute ourmethodisbasedontheefﬁcientcontrastivedivergencetechnique[16 25 26 28].Speciﬁcally wederiveavariationalapproximationLv(q0 q1)ofthenegativelog-likelihoodL(Θ) thatis:Lv(q0 q1):=R(q0(x z h) p(x z h))−R(q1(x z h) p(x z h)) whereR(q p)istherelativeentropy andq0isavariationaldistributionwithxandzclampedtotheirobservedvalueswhileq1isadistributionwithallvariablesfree.Forq(q0orq1)ingeneral wemakethestructuredmeanﬁeldassumption[27]that1q(x z h)=q(x)q(z)q(h).Solvingtheapproximateproblem:ApplyingthevariationalapproximationLvinproblem(4) wegetanapproximateobjectivefunctionL(Θ V q0 q1).Then wecandevelopanalternatingmini-mizationmethod whichiterativelyminimizesL(Θ V q0 q1)overq0and(Θ V).Thedistributionq1isreconstructedoncetheoptimalq0isachieved see[25]fordetails.Theproblemofsolvingq0andq1istheposteriorinferenceproblem.Speciﬁcally foravariationaldistributionq(canbeq0orq1)ingeneral wekeep(Θ V)ﬁxedandupdateeachmarginalasq(x)=p(x|Eq(H)[H]) q(z)=p(z|Eq(H)[H]) andq(h)=Ykp(hk|Eq(X)[X] Eq(Z)[Z]).(5)Forq0 (x z)areclampedattheirobservedvalues andonlyq0(h)isupdated whichcanbeveryefﬁcientlydonebecauseofitsfactorizedform.Thedistributionq1isachievedbyperformingtheaboveupdatesstartingfromq0.Severaliterationscanyieldagoodq1.Again wecanseethatbothq(x)andq(z)areCRFs withtheexpectationofHasthecondition.Therefore forlinear-chainmodels wecanuseamessagepassingscheme[18]toinfertheirmarginaldistributions asneededforparameterestimationandview-levelprediction(e.g. imageannotation) asweshallsee.Forgenerallystructuredmodels approximateinferencetechniques[22]canbeapplied.Afterwehaveinferredq0andq1 parameterestimationcanbedonebyalternatingbetween(1)estimatingVwithΘﬁxed:thisproblemislearningamulti-classSVM[11] whichcanbe1Theparametricformassumptionsofq asmadeinpreviouswork[28 29] arenotneeded.4efﬁcientlydonewithexistingsolvers;and(2)estimatingΘwithVﬁxed:thiscanbesolvedwithsub-gradientdescent wherethesub-gradientiscomputedas:∇θi=−Eq0[φ(xi xi+1)]+Eq1[φ(xi xi+1)] ∇ηj=−Eq0[ψ(zj zj+1)]+Eq1[ψ(zj zj+1)] ∇λk=−Eq0[ϕ(hk)]+Eq1[ϕ(hk)] ∇Wki=−Eq0[φ(xi xi+1)ϕ(hk)>]+Eq1[φ(xi xi+1)ϕ(hk)>]−C21DPd(V¯ydk−Vydk)∂Eq0[hk]∂Wki ∇Ukj=−Eq0[ψ(zj zj+1)ϕ(hk)>]+Eq1[ψ(zj zj+1)ϕ(hk)>]−C21DPd(V¯ydk−Vydk)∂Eq0[hk]∂Ukj where¯yd=argmaxy[∆‘d(y)+V>Eq0[f(Hd y)]istheloss-augmentedprediction andtheexpec-tationEq0[φ(xi xi+1)]isactuallythecountfrequencyofφ(xi xi+1) likewiseforEq0[ψ(zj zj+1)].Notethatinourintegratedmax-marginformulation thesub-gradientsofWandUcontainanadditionalterm(i.e. thethirdterm)comparedtothestandardDWH[28]withcontrastivedivergenceapproximation.Thisadditionaltermintroducesaregularizationeffecttothelatentsubspacemodel.Ifthepredictionlabelyddiffersfromthetruelabel¯yd thistermwillbenon-zeroanditbiasesthemodeltowardsdiscoveringabetterrepresentationforprediction.4ApplicationtoImageClassiﬁcation AnnotationandRetrievalWehavedevelopedthelarge-marginframeworkwithagenericmulti-viewlatentspaceMNtomodelstructureddata.Inordertocarefullyexaminethebasiclearningprincipleandcomparewithexistingwork inthispaper weconcentrateonasimpliﬁedbutveryrichcasethatthedataoneachviewarenotstructured whichhasbeenextensivelystudiedinEFH[26 28 29]forimageclassiﬁcation annotationandretrieval.WedenotethespecializedmodelbyMMH(max-marginHarmonium).Intheory extensionstomodelstructuredmulti-viewdatacanbeeasilydoneunderthegeneralframework andtheonlyneededchangeisonthestepofinferringq1 whichcanbetreatedasablackbox giventhewideliteratureonapproximateinference[22].Wedeferthesystematicalstudyinthisdirectiontothefullextensionofthiswork.Speciﬁcally weconsidertwo-views wherexisavectorofdiscretewordfeatures(e.g. imagetags)andzisavectorofreal-valuedfeatures(e.g. colorhistograms).EachxiisaBernoullivariablethatdenoteswhethertheithtermofadictionaryappearsornotinanimage andeachzjisarealnumberthatdenotesthenormalizedcolorhistogramofanimage.Weassumethateachreal-valuedhkfollowsaunivariateGaussiandistribution.Therefore wedeﬁnetheconditionaldistributionsasp(xi=1|h)=11+e−(αi+Wi·h) p(zj|h)=N(zj|σ2j(βj+Uj·h) σ2j) p(hk|x z)=N(hk|x>W·k+z>U·k 1) whereWi·andW·kdenotetheithrowandkthcolumnofW respectively.AlikeforUi·andU·k.Withtheabovedeﬁnitions wecanfollowexactlythesameprocedureasabovetodoparameterestimation.Forthestepofinferringq0andq1 thedistributionsofx zandhareallfullyfactorized.Therefore thesub-gradientscanbeeasilycomputed.DetailsaredeferredtotheAppendix.Testing:Forclassiﬁcationandretrieval weneedtoinfertheposteriordistributionofHanditsexpectation.Inthiscase wehaveEp(h|x z)[H]=v wherevk=x>W·k+z>U·k ∀1≤k≤K.Therefore theclassiﬁcationruleisy∗=argmaxyV>f(v y).Forretrieval theexpectationvofeachimageisusedtocomputeasimilarity(e.g. cosine)betweenimages.Forannotation weusextorepresenttags whichareobservedintraining.Intesting weinfertheposteriordistributionp(x|z) whichcanbeapproximatelycomputedbyrunningtheupdateequations(5)withzclampedatitsobservedvalues.Then tagswithhighprobabilitiesareselectedasannotation.5ExperimentsWereportempiricalresultsonTRECVID2003andﬂickrimagedatasets.Ourresultsdemonstratethatthelarge-marginapproachcanachievesigniﬁcantlybetterperformanceondiscoveringpredic-tivesubspacerepresentationsandthetasksofimageclassiﬁcation annotationandretrieval.5.1DatasetsandFeaturesTheﬁrstdatasetistheTRECVID2003videodataset[28] whichcontains1078manuallylabeledvideoshotsthatbelongto5categories.Eachshotisrepresentedasa1894-dimvectoroftextfeatures5−50−40−30−20−10010203040−80−60−40−200204060  12345Avg-KL = 0.605−80−60−40−200204060−50−40−30−20−1001020304050  12345Avg-KL = 0.319−40−30−20−1001020304050−80−60−40−200204060  12345Avg-KL = 0.198Figure2:t-SNE2Dembeddingofthediscoveredlatentspacerepresentationby(Left)MMH (Middle)DWHand(Right)TWHontheTRECVIDvideodataset(Betterviewedincolor).anda165-dimvectorofHSVcolorhistogram whichisextractedfromtheassociatedkeyframe.Weevenlysplitthisdatasetfortrainingandtesting.ThesecondoneisasubsetselectedfromNUS-WIDE[10] whichisabigimagedatasetconstructedfromﬂickrwebimages.Thisdatasetcontains3411imagesabout13animals includingcat tiger etc.SeeFig.6forexampleimagesforeachcategory.Foreachimage sixtypesoflow-levelfeatures[10]areextracted including634-dimrealvaluedfeatures(i.e. 64-dimcolorhistogram 144-dimcolorcorrelogram 73-dimedgedirectionhistogram 128-dimwavelettextureand225-dimblock-wisecolormoments)and500-dimbag-of-wordrepresentationbasedonSIFT[19]features.Werandomlyselect2054imagesfortrainingandusetherestfortesting.Theonlinetagsarealsodownloadedforevaluatingimageannotation.5.2DiscoveringPredictiveLatentSubspaceRepresentationsWeﬁrstevaluatethepredictivepowerofthediscoveredlatentsubspacerepresentations.Fig.2showsthe2Dembeddingofthediscovered10-dimlatentrepresentationsbythreemodels(i.e. MMH DWHandTWH)onthevideodata.Here weusethet-SNEalgorithm[21]toﬁndtheembedding.Wecanseethatclearlythelatentsubspacerepresentationsdiscoveredbythelarge-marginbasedMMHshowastronggroupingpatternfortheimagesbelongingtothesamecategory whileimagesfromdifferentcategoriestendtobeseparatedfromeachotheronthe2Dembeddingspace.Incontrast thelatentsubspacerepresentationsdiscoveredbythelikelihood-basedunsuper-visedDWHandsupervisedTWHdonotshowacleargroupingpattern exceptfortheﬁrstcategory.Imagesfromdifferentcategoriestendtomixtogether.Theseobservationssuggestthatthelarge-marginbasedlatentsubspacemodelcandiscovermorepredictiveordiscriminativelatentsubspacerepresentations whichwillresultinbetterpredictionperformance asweshallsee.Toquantitativelyevaluatethepredictivenessofthediscoveredlatentsubspacerepresentations wecomputethepair-wiseaverageKL-divergencebetweentheper-classaveragedistributionoverlatenttopics2.AsshownonthetopofeachplotinFig.2 thelarge-marginbasedMMHobtainsamuchlargeraverageKL-divergencethantheotherlikelihood-basedmethods.ThisagainsuggeststhatthelatentsubspacerepresentationsdiscoveredbyMMHaremorediscriminativeorpredictive.Weobtainthesimilarobservationsandconclusionsontheﬂickrdataset(seeFig.3forsomeexampletopics) wheretheaverageKL-divergencescoresof60-topicMMH DWHandTWHare3.23 2.56and0.463 respectively.Finally weexaminethepredictivepowerofdiscoveredlatenttopics.Fig.3showsﬁveexampletopicsdiscoveredbythelarge-marginMMHontheﬂickrimagedata.ForeachtopicHk weshowthe5top-rankedimagesthatyieldahighexpectedvalueofHk togetherwiththeassociatedtags.Also toqualitativelyvisualizethediscriminativepowerofeachtopicamongthe13categories weshowtheaverageprobabilityofeachcategorydistributedontheparticulartopic.Fromtheresults wecanseethatmanyofthediscoveredtopicsareverypredictiveforoneorseveralcategories.Forexample topics3and4arediscriminativeinpredictingthecategorieshawkandwhales respectively.Similarly topics1and5aregoodatpredictingsquirrelandzebra respectively.Wealsohavesometopicswhicharegoodatdiscriminatingasubsetofcategoriesagainstanothersubset.Forexample thetopic2isgoodatdiscriminating{squirrel wolf rabbit}against{tiger whales zebra};butitisnotverydiscriminativebetweensquirrelandwolf.2Tocomputethisscore weﬁrstturntheexpectedvalueofHtobenon-negativebysubtractingeachelementbythesmallestvalueandthennormalizeitintoadistributionovertheKtopics.Theper-classaverageiscomputedbyaveragingthetopicdistributionsoftheimageswithinthesameclass.Forapairofdistributionspandq theaverageKL-divergenceis1/2(R(p q)+R(q p)).6Topic10.0140.0150.0160.0170.0180.019probability squirrelliontigersnakerabbitwolfhawkwhalescatantlerelephantcowzebrasquirrel nature animal wildlife rabbit cute bunny interestingnessTopic20.0050.010.0150.020.0250.030.0350.04probability squirrelwolfrabbitcowlioncatsnakehawkantlerelephanttigerwhaleszebrawolf alaska animal nature wildlife africa squirrelTopic30.0120.0140.0160.0180.020.0220.024probability hawkantlercatrabbitwolfelephantsquirrelliontigerwhalescowzebrasnakehawk bird ﬂying wildlife wings nature fabulous texasTopic40.0150.020.0250.030.0350.040.045probability whaleszebraelephanttigercowcatantlerlionsnakerabbithawkwolfsquirrelocean boat animal wildlife diving sea sydney paciﬁc blueTopic50.0050.010.0150.020.025probability zebratigerantlersquirrelrabbitcatwolflionhawksnakecowelephantwhaleszebra zoo animal stripes africa mammal black white nature eyesFigure3:Examplelatenttopicsdiscoveredbya60-topicMMHontheﬂickranimaldataset.5.3PredictionPerformanceonImageClassiﬁcation Retrieval andAnnotation5.3.1ClassiﬁcationWeﬁrstcomparetheMMHwithSVM DWH TWH GaussianMixture(GM-Mix) GaussianMix-tureLDA(GM-LDA) andCorrespondenceLDA(CorrLDA)ontheTRECVIDdata.See[4]forthedetailsofthelastthreemodels.WeusetheSVMstruct3tosolvethesub-stepoflearningVinMMHandbuildanSVMclassiﬁer whichusesboththetextandcolorhistogramfeatureswithoutdistinguishingthemindifferentviews.ForeachoftheunsupervisedDWH GM-Mix GM-LDAandCorrLDA adownstreamSVMisbuiltwiththesametoolbasedonthediscoveredlatentrepresen-tations.Fig.4(a)showstheclassiﬁcationaccuracyofdifferentmodels whereCorrLDAisomittedbecauseofitstoolowperformance.Wecanseethatthemax-marginbasedmulti-viewMMHper-formsconsistentlybetterthananyothercompetitors.Incontrast thelikelihood-basedTWHdoesnotshowanyconclusiveimprovementscomparedtotheunsupervisedDWH.Theseresultsshowthatsupervisedinformationcanhelpindiscoveringpredictivelatentspacerepresentationsthataremoresuitableforpredictionifthemodelisappropriatelylearned e.g. byusingthelarge-marginmethod.ThesuperiorperformanceofMMHcomparedtotheﬂatSVMdemonstratestheusefulnessofmodelingmulti-viewinputsforprediction.Thereasonsfortheinferiorperformanceofothermodels(e.g. CorrLDAandGM-Mix)areanalyzedin[28 29].Fig.4(b)showstheclassiﬁcationaccuracyontheﬂickranimaldataset.Forbrevity wecompareMMHonlywiththebestperformedDWH TWHandSVM.Forthesemethods weusethe500-dimSIFTand634-dimrealfeatures whicharetreatedastwoviewsofinputsforMMH DWHandTWH.Also wecomparewiththesingle-viewMedLDA[31] whichusesSIFTfeaturesonly.Tobefair wealsoevaluateaversionofMMHthatusesSIFTfeatures anddenoteitbyMMH(SIFT).Again wecanseethatthelarge-marginbasedmulti-viewMMHperformsmuchbetterthananyothermethods includingSVMwhichignoresthepresenceofmulti-viewfeatures.Forthesingle-viewMMH(SIFT) itperformscomparably(slightlybetterthan)withthelarge-marginbasedMedLDA whichisadirectedBN.Withthesimilarlarge-marginprinciple MMHisanimportantextensionofMedLDAtotheundirectedlatentsubspacemodelsandformulti-viewdataanalysis.5.3.2RetrievalForimageretrieval eachtestimageistreatedasaqueryandtrainingimagesarerankedbasedontheircosinesimilaritywiththegivenquery whichiscomputedbasedonlatentsubspacerepresenta-tions.Animageisconsideredrelevanttothequeryiftheybelongtothesamecategory.Weevaluatetheretrievalresultsbycomputingtheaverageprecision(AP)scoreanddrawingprecision-recallcurves.Fig.4(c)comparesMMHwithfourothermodelswhenthetopicnumberchanges.Here 3http://svmlight.joachims.org/svmmulticlass.html75101520253035400.450.50.550.60.650.70.750.8# of latent topicsclassification accuracyMMHDWHTWHGM−MixGM−LDASVM(a)1020304050600.30.350.40.450.50.550.6# of latent topicsclassification accuracyMMHDWHTWHMMH(SIFT)MEDLDA(SIFT)SVM(b)00.510.20.30.40.50.6RecallPrecision00.510.20.30.40.50.6RecallPrecision0102030400.30.350.40.450.50.55# of latent topicsAverage PrecisionMMHDWHTWHGM−MixGM−LDA15 topicsAverage Precision20 topics(c)Figure4:Classiﬁcationaccuracyonthe(a)TRECVID2003and(b)ﬂickrdatasetsand(c)theaveragepreci-sioncurveandthetwoprecision-recallcurvesforimageretrievalonTRECVIDdata.squirrelcowcatsquirrelcatcownaturecatcloudyanimalrabbitnaturewildlifekittenlionnaturecutegreenelephantsquirrelanimalwolfanimalwolfzebratigerlionzebraanimalzoolionlioncatnaturezootigerhawkanimalwolfwildlifesnakeanimalsquirrelzoozoowolfwolfanimalelephantwhalesrabbitwildlifesnakeoceanelephantrabbitgreennaturewolfwateroceanbunnylandscapeelephantoceanmarinemarinemacroIndiaAustraliaﬂowersnakeantlerssquirrelsnakehawkantlerszebrasquirrelrabbitnaturebirdanimalnaturenaturebunnycatwildlifelionwildlifecatdeeranimalanimalcutehawkwolfcathawksnakenaturecowcatzebrabirdoceanzoooceankittenantlerswildlifeadorablewolfwolfcatsnatureanimalanimalaquariumanimalFigure6:Exampleimagesfromthe13categoriesontheﬂickranimaldatasetwithpredictedannotations.Tagsinbluearecorrectannotationswhileredonesarewrongpredictions.Theothertagsareneutral.weshowtheprecision-recallcurveswhenthetopicnumberissetat15and20.WecanseethatfortheAPmeasure MMHoutperformsallothermethodsinmostcases andMMHconsistentlyoutperformsalltheothermethodsinthemeasureofprecision-recallcurve.Ontheﬂickrdataset wehavesimilarobservations.TheAPscoresofthe60-topicMMH DWH andTWHare0.163 0.153and0.158 respectively.Duetospacelimitation wedeferthedetailstoafullextension.5.3.3AnnotationMMHDWHTWHsLDAF1@10.1650.1440.1450.077F1@20.2210.1860.1920.124F1@30.2450.2020.2180.146F1@40.2580.2080.2280.159F1@50.2620.2100.2360.169F1@60.2590.2080.2400.171F1@70.2560.2060.2390.175Figure5:Top-NF1-measure.Finally wereporttheannotationresultsontheﬂickrdataset withadictionaryof1000uniquetags.Theaveragenumberoftagsperimageisabout4.5.WecompareMMHwithDWHandTWHwithtwoviewsofinputs–XfortagandZforallthe634-dimreal-valuedfeatures.WealsocomparewiththesLDAannotationmodel[24] whichusesSIFTfeaturesandtagsasinputs.Weusethetop-NF1-measure[24] denotedbyF1@N.With60latenttopics thetop-NF-measurescoresareshowninFig.5.Wecanseethatthelarge-marginbasedMMHsigniﬁcantlyoutperformsallthecompetitors.Fig.6showsexampleimagesfromallthe13categories whereforeachcategorytheleftimageisgenerallyofagoodannotationqualityandtherightoneisrelativelyworse.6ConclusionsandFutureWorkWehavepresentedagenericlarge-marginlearningframeworkfordiscoveringpredictivelatentsub-spacerepresentationssharedbystructuredmulti-viewdata.Theinferenceandlearningcanbeefﬁ-cientlydonewithcontrastivedivergencemethods.Finally weconcentrateonaspecializedmodelwithapplicationstoimageclassiﬁcation annotationandretrieval.Extensiveexperimentsonrealvideoandwebimagedatasetsdemonstratetheadvantagesoflarge-marginlearningforbothpredic-tionandpredictivelatentsubspacediscovery.Infuturework weplantosystematicallyinvestigatethelarge-marginlearningframeworkonstructuredmulti-viewdataanalysis e.g. ontextmining[23]andcomputervision[15]applications.8AcknowledgmentsThisworkwasdonewhileN.ChenwasavisitingresearcheratCMUunderaCSCfellowshipandsupportsfromChineseNSFGrants(No.60625304 90716021 61075027) theNationalKeyProjectforBasicResearchofChina(GrantsNo.G2007CB311003 2009CB724002).J.ZhuandE.P.XingaresupportedbyONRN000140910758 NSFIIS-0713379 NSFCareerDBI-0546594 andanAlfredP.SloanResearchFellowship.References[1]S.Akaho.Akernelmethodforcanonicalcorrelationanalysis.InIMPS 2001.[2]K.AndoandT.Zhang.Two-viewfeaturegenerationmodelforsemi-supervisedlearning.InICML 2007.[3]F.R.BachandM.I.Jordan.Aprobabilisticinterpretationofcanonicalcorrelationanalysis.Technicalreport TechnicalReport688 Dept.ofStatistics.UniversityofCalifornia 2005.[4]D.M.BleiandM.I.Jordan.Modelingannotateddata.InACMSIGIR pages127–134 2003.[5]D.M.Blei A.Y.Ng andM.I.Jordan.Latentdirichletallocation.JMLR 3:993–1022 2003.[6]A.BlumandT.Mitchell.Combininglabeledandunlabeleddatawithco-trainnig.InCOLT 1998.[7]U.BrefeldandT.Scheffer.Co-EMsupportvectorlearning.InICML 2004.[8]K.Chaudhuri S.M.Kakade K.Livescu andK.Sridharan.Multi-viewclusteringviacanonicalcorrela-tionanalysis.InICML 2009.[9]C.M.Christoudias R.Urtasun andT.Darrell.Multi-viewlearninginthepresenceofviewdisagreement.InUAI 2008.[10]T.-S.Chua J.Tang R.Hong H.Li Z.Luo andY.-T.Zheng.NUS-WIDE:Areal-worldwebimagedatabasefromnationaluniversityofsingapore.InCIVR 2009.[11]K.CrammerandY.Singer.Onthealgorithmicimplementationofmulticlasskernel-basedvectorma-chines.JMLR (2):265–292 2001.[12]M.Culp G.Michailidis andK.Johnson.Onmulti-viewlearningwithadditivemodels.AnnalsofAppliedStatistics 3(1):292–318 2009.[13]T.Diethe D.R.Hardoon andJ.Shawe-Taylor.Multiviewﬁsherdiscriminantanalysis.InNIPSWorkshoponLearningfromMultipleSources 2008.[14]D.Foster S.Kakade andT.Zhang.Multi-viewdimensionalityreductionviacanonicalcorrelationanal-ysis.Technicalreport TechnicalReportTR-2008-4 TTI-Chicago 2008.[15]D.G¨okalpandS.Aksoy.Sceneclassiﬁcationusingbag-of-regionsrepresentations.InCVPR 2007.[16]G.E.Hinton.Trainingproductsofexpertsbyminimizingcontrastivedivergence.NeuralComputation 14(8):1771–1800 2002.[17]H.Hotelling.Relationsbetweentwosetsofvariates.Biometrika 28(3/4):321–377 1936.[18]J.Lafferty A.McCallum andF.Pereira.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandlabelingsequencedata.InICML 2001.[19]D.G.Lowe.Objectrecognitionfromlocalscale-invariantfeatures.InCVPR 1999.[20]R.SalakhutdinovandG.E.Hinton.Replicatedsoftmax:anundirectedtopicmodel.InNIPS 2009.[21]L.vanderMaatenandG.Hinton.Visualizingdatausingt-SNE.JMLR 9:2579–2605 2008.[22]M.J.WainwrightandM.I.Jordan.Graphicalmodels exponentialfamilies andvariationalinference.FoundationsandTrendsinMachineLearning 1(1–2):1–305 2008.[23]H.M.Wallach.Topicmodeling:Beyondbag-of-words.InICML 2006.[24]C.Wang D.M.Blei andL.Fei-Fei.Simultaneousimageclassiﬁcationandannotation.InCVPR 2009.[25]M.WellingandG.E.Hinton.Anewlearningalgorithmformeanﬁeldboltzmannmachines.InICANN 2001.[26]M.Welling M.Rosen-Zvi andG.E.Hinton.Exponentialfamilyharmoniumswithanapplicationtoinformationretrieval.InNIPS pages1481–1488 2004.[27]E.P.Xing M.I.Jordan andS.Russell.Ageneralizedmeanﬁeldalgorithmforvariationalinferenceinexponentialfamilies.InUAI 2003.[28]E.P.Xing R.Yan andA.G.Hauptmann.Miningassociatedtextandimageswithdual-wingharmoniums.InUAI 2005.[29]J.Yang Y.Liu E.P.Xing andA.G.Hauptmann.Harmoniummodelsforsemanticvideorepresentationandclassiﬁcation.InSDM 2007.[30]J.Zhang Z.Ghahramani andY.Yang.Flexiblelatentvariablemodelsformulti-tasklearning.MachineLearning 73(3):221–242 2008.[31]J.Zhu A.Ahmed andE.P.Xing.MedLDA:Maximummarginsupervisedtopicmodelsforregressionandclassiﬁcation.InICML 2009.9,Yichao Zhou
Haozhi Qi
Jingwei Huang
Yi Ma