2019,On the Inductive Bias of Neural Tangent Kernels,State-of-the-art neural networks are heavily over-parameterized  making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain over-parameterized regime  the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization  called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space (RKHS). In particular  we study smoothness  approximation  and stability properties of functions with finite norm  including stability to image deformations in the case of convolutional networks  and compare to other known kernels for similar architectures.,On the Inductive Bias of Neural Tangent Kernels

Alberto Bietti

Inria∗

Julien Mairal

Inria∗

alberto.bietti@inria.fr

julien.mairal@inria.fr

Abstract

State-of-the-art neural networks are heavily over-parameterized  making the opti-
mization algorithm a crucial ingredient for learning predictive models with good
generalization properties. A recent line of work has shown that in a certain over-
parameterized regime  the learning dynamics of gradient descent are governed by
a certain kernel obtained at initialization  called the neural tangent kernel. We
study the inductive bias of learning in such a regime by analyzing this kernel and
the corresponding function space (RKHS). In particular  we study smoothness 
approximation  and stability properties of functions with ﬁnite norm  including
stability to image deformations in the case of convolutional networks  and compare
to other known kernels for similar architectures.

1

Introduction

The large number of parameters in state-of-the-art deep neural networks makes them very expressive 
with the ability to approximate large classes of functions [26  41]. Since many networks can
potentially ﬁt a given dataset  the optimization method  typically a variant of gradient descent  plays
a crucial role in selecting a model that generalizes well [39].

A recent line of work [2  16  20  21  27  30  54] has shown that when training deep networks in a
certain over-parameterized regime  the dynamics of gradient descent behave like those of a linear
model on (non-linear) features determined at initialization. In the over-parameterization limit  these
features correspond to a kernel known as the neural tangent kernel. In particular  in the case of
a regression loss  the obtained model behaves similarly to a minimum norm kernel least squares
solution  suggesting that this kernel may play a key role in determining the inductive bias of the
learning procedure and its generalization properties. While it is still not clear if this regime is at play
in state-of-the-art deep networks  there is some evidence that this phenomenon of “lazy training” [16] 
where weights only move very slightly during training  may be relevant for early stages of training and
for the outmost layers of deep networks [29  53]  motivating a better understanding of its properties.

In this paper  we study the inductive bias of this regime by studying properties of functions in the
space associated with the neural tangent kernel for a given architecture (that is  the reproducing kernel
Hilbert space  or RKHS). Such kernels can be deﬁned recursively using certain choices of dot-product
kernels at each layer that depend on the activation function. For the convolutional case with rectiﬁed
linear unit (ReLU) activations and arbitrary patches and linear pooling operations  we show that the
NTK can be expressed through kernel feature maps deﬁned in a tree-structured hierarchy.

We study smoothness and stability properties of the kernel mapping for two-layer networks and
CNNs  which control the variations of functions in the RKHS. In particular  a useful inductive bias
when dealing with natural signals such as images is stability of the output to deformations of the
input  such as translations or small rotations. A precise notion of stability to deformations was
proposed by Mallat [35]  and was later studied in [11] in the context of CNN architectures  showing

∗Univ. Grenoble Alpes  Inria  CNRS  Grenoble INP  LJK  38000 Grenoble  France

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the beneﬁts of different architectural choices such as small patch sizes. In contrast to the kernels
studied in [11]  which for instance cover the limiting kernels that arise from training only the last
layer of a ReLU CNN  we ﬁnd that the obtained NTK kernel mappings for the ReLU activation lack
a desired Lipschitz property which is needed for stability to deformations in the sense of [11  12  35].
Instead  we show that a weaker smoothness property similar to Hölder smoothness holds  and this
allows us to show that the kernel mapping is stable to deformations  albeit with a different guarantee.

In order to balance our observations on smoothness  we also consider approximation properties for the
NTK of two-layer ReLU networks  by characterizing the RKHS using a Mercer decomposition of the
kernel in the basis of spherical harmonics [6  46  47]. In particular  we study the decay of eigenvalues
for this decomposition  which is then related to the regularity of functions in the space  and provides
rates of approximation for Lipschitz functions [6]. We ﬁnd that the full NTK has better approximation
properties compared to other function classes typically deﬁned for ReLU activations [6  17  19] 
which arise for instance when only training the weights in the last layer  or when considering Gaussian
process limits of ReLU networks (e.g.  [24  28  36  40]).

Contributions. Our main contributions can be summarized as follows:

• We provide a derivation of the NTK for convolutional networks with generic linear operators for
patch extraction and pooling  and express the corresponding kernel feature map hierarchically
using these operators.

• We study smoothness properties of the kernel mapping for ReLU networks  showing that it is
not Lipschitz but satisﬁes a weaker Hölder smoothness property. For CNNs  we then provide a
guarantee on deformation stability.

• We characterize the RKHS of the NTK for two-layer ReLU networks by providing a spectral
decomposition of the kernel and studying its spectral decay. This leads to improved approximation
properties compared to other function classes based on ReLU.

Related work. Neural tangent kernels were introduced in [27]  and similar ideas were used to obtain
more quantitative guarantees on the global convergence of gradient descent for over-parameterized
neural networks [2  3  16  20  21  30  50  54]. The papers [3  20  51] also derive NTKs for convo-
lutional networks  but focus on simpler architectures. Kernel methods for deep neural networks
were studied for instance in [17  19  34]. Stability to deformations was originally introduced in the
context of the scattering representation [12  35]  and later extended to neural networks through kernel
methods in [11]. The inductive bias of optimization in neural network learning was considered  e.g. 
by [1  4  13  39  48]. [6  25  45  49] study function spaces corresponding to two-layer ReLU networks.
In particular  [25] also analyzes properties of the NTK  but studies a speciﬁc high-dimensional limit
for generic activations  while we focus on ReLU networks  studying the corresponding eigenvalue
decays in ﬁnite dimension.

2 Neural Tangent Kernels

In this section  we provide some background on “lazy training” and neural tangent kernels (NTKs) 
and introduce the kernels that we study in this paper. In particular  we derive the NTK for generic
convolutional architectures on ℓ2 signals. For simplicity of exposition  we consider scalar-valued
functions  noting that the kernels may be extended to the vector-valued case  as done  e.g.  in [27].

2.1 Lazy training and neural tangent kernels

Multiple recent works studying global convergence of gradient descent in neural networks (e.g.  [2 
20  21  27  30  54]) show that when a network is sufﬁciently over-parameterized  weights remain
close to initialization during training. The model is then well approximated by its linearization around
initialization. For a neural network f (x; θ) with parameters θ and initialization θ0  we then have:2

This regime where weights barely move has also been referred to as “lazy training” [16]  in contrast
to other situations such as the “mean-ﬁeld” regime (e.g.  [15  38  37])  where weights move according

f (x; θ) ≈ f (x; θ0) + hθ − θ0 ∇θf (x; θ0)i.

(1)

2While we use gradients in our notations  we note that weak differentiability (e.g.  with ReLU activations) is

sufﬁcient when studying the limiting NTK [27].

2

to non-linear dynamics. Yet  with sufﬁcient over-parameterization  the (non-linear) features x 7→
∇θf (x; θ0) of the linearized model (1) become expressive enough to be able to perfectly ﬁt the
training data  by approximating a kernel method.

Neural Tangent Kernel (NTK). When the width of the network tends to inﬁnity  assuming an
appropriate initialization on weights  the features of the linearized model tend to a limiting kernel K 
called neural tangent kernel [27]:

h∇θf (x; θ0) ∇θf (x′  θ0)i → K(x  x′).

(2)

In this limit and under some assumptions  one can show that the weights move very slightly and the
kernel remains ﬁxed during training [27]  and that gradient descent will then lead to the minimum
norm kernel least-squares ﬁt of the training set in the case of the ℓ2 loss (see [27] and [37  Section
H.7]). Similar interpolating solutions have been found to perform well for generalization  both in
practice [10] and in theory [8  31]. When the number of neurons is large but ﬁnite  one can often show
that the kernel only deviates slightly from the limiting NTK  at initialization and throughout training 
thus allowing convergence as long as the initial kernel matrix is non-degenerate [3  16  20  21].

θ = (w⊤

j=1 vjσ(w⊤
1   . . .   w⊤

NTK for two-layer ReLU networks. Consider a two layer network of the form f (x; θ) =
q 2
mPm
j x)  where σ(u) = (u)+ = max(0  u) is the ReLU activation  x ∈ Rp  and
m  v⊤) are parameters with values initialized as N (0  1). Practitioners often include
the factorp2/m in the variance of the initialization of vj   but we treat it as a scaling factor follow-
ing [20  21  27]  noting that this leads to the same predictions. The factor 2 is simply a normalization
constant speciﬁc to the ReLU activation and commonly used by practitioners  which avoids vanishing
or exploding behavior for deep networks. The corresponding NTK is then given by [16  21]:

K(x  x′) = 2(x⊤x′) Ew∼N (0 I)[1{w⊤x ≥ 0}1{w⊤x′ ≥ 0}] + 2 Ew∼N (0 I)[(w⊤x)+(w⊤x′)+]

= kxkkx′kκ(cid:18) hx  x′i

kxkkx′k(cid:19)  

where

κ(u) := uκ0(u) + κ1(u)

κ0(u) =

1
π

(π − arccos(u))  

κ1(u) =

1

(3)

(4)

(5)

π (cid:16)u · (π − arccos(u)) +p1 − u2(cid:17) .

The expressions for κ0 and κ1 follow from standard calculations for arc-cosine kernels of degree 0
and 1 (see [17]). Note that in this two-layer case  the non-linear features obtained for ﬁnite neurons
correspond to a random features kernel [42]  which is known to approximate the full kernel relatively
well even with a moderate amount of neurons [7  42  43]. One can also extend the derivation to other
activation functions  which may lead to explicit expressions for the kernel in some cases [19].

NTK for fully-connected deep ReLU networks. We deﬁne a fully-connected neural network by

f (x; θ) =q 2

mnhwn+1  ani  with a1 = σ(W 1x)  and
W kak–1(cid:19)  

ak = σ(cid:18)r 2

mk–1

k = 2  . . .   n 

is the ReLU activation and is applied element-wise. Following [27]  the corresponding NTK is deﬁned

where W k ∈ Rmk×mk–1 and wn+1 ∈ Rmn are initialized with i.i.d. N (0  1) entries  and σ(u) = (u)+
recursively by K(x  x′) = Kn(x  x′) with K0(x  x′) = Σ0(x  x′) = x⊤x′  and for k ≥ 1 

Σk(x  x′) = 2 E(u v)∼N (0 Bk)[σ(u)σ(v)]
Kk(x  x′) = Σk(x  x′) + 2Kk–1(x  x′) E(u v)∼N (0 Bk)[σ′(u)σ′(v)] 

3

kernels of degrees 0 and 1 [17]  it is easy to show that

Σk–1(x  x′) Σk–1(x′  x′)(cid:19). Using a change of variables and deﬁnitions of arc-cosine
pΣk–1(x  x)Σk–1(x′  x′)! (6)

where Bk =(cid:18) Σk–1(x  x) Σk–1(x  x′)
2 E(u v)∼N (0 Bk)[σ(u)σ(v)] =pΣk–1(x  x)Σk–1(x′  x′)κ1 
2 E(u v)∼N (0 Bk)[σ′(u)σ′(v)] = κ0 
pΣk–1(x  x)Σk–1(x′  x′)!  

where κ0 and κ1 are deﬁned in (5).

Σk–1(x  x′)

Σk–1(x  x′)

(7)

Feature maps construction. We now provide a reformulation of the previous kernel in terms of
explicit feature maps  which provides a representation of the data and makes our study of stability
in Section 4 more convenient. For a given input Hilbert space H  we denote by ϕH 1 : H → H1
the kernel mapping into the RKHS H1 for the kernel (z  z′) ∈ H2 7→ kzkkz′kκ1(hz  z′i/kzkkz′k) 
and by ϕH 0 : H → H0 the kernel mapping into the RKHS H0 for the kernel (z  z′) ∈ H2 7→
κ0(hz  z′i/kzkkz′k). We will abuse notation and hide the input space  simply writing ϕ1 and ϕ0.
can be deﬁned as K(x  x′) = hΦn(x)  Φn(x′)i  with Φ0(x) = Ψ0(x) = x and for k ≥ 1 

Lemma 1 (NTK feature map for fully-connected network). The NTK for the fully-connected network

Ψk(x) = ϕ1(Ψk–1(x))

Φk(x) =(cid:18)ϕ0(Ψk–1(x)) ⊗ Φk–1(x)

ϕ1(Ψk–1(x))

where ⊗ is the tensor product.
2.2 Neural tangent kernel for convolutional networks

(cid:19)  

In this section we study NTKs for convolutional networks (CNNs) on signals  focusing on the

ReLU activation. We consider signals in ℓ2(Zd  Rm0 )  that is  signals x[u] with u ∈ Zd denoting
the location  x[u] ∈ Rm0   and Pu∈Zd kx[u]k2 < ∞ (for instance  d = 2 and m0 = 3 for RGB

images). The inﬁnite support allows us to avoid dealing with boundary conditions when considering
deformations and pooling. The precise study of ℓ2 membership is deferred to Section 4.

Patch extraction and pooling operators P k and Ak. Following [11]  we deﬁne two linear opera-
tors P k and Ak on ℓ2(Zd) for extracting patches and performing (linear) pooling at layer k  respec-
tively. For an H-valued signal x[u]  P k is deﬁned by P kx[u] = |Sk|−1/2(x[u + v])v∈Sk ∈ H|Sk| 
where Sk is a ﬁnite subset of Zd deﬁning the patch shape (e.g.  a 3x3 box). Pooling is deﬁned
as a convolution with a linear ﬁlter hk[u]  e.g.  a Gaussian ﬁlter at scale σk as in [11]  that is 

Akx[u] = Pv∈Zd hk[u − v]x[v]. In this discrete setting  we can easily include a downsampling
operation with factor sk by changing the deﬁnition of Ak to Akx[u] =Pv∈Zd hk[sku − v]x[v] (in
particular  if hk is a Dirac at 0  we obtain a CNN with “strided convolutions”). In fact  our NTK
derivation supports general linear operators Ak : ℓ2(Zd) → ℓ2(Zd) on scalar signals.
For deﬁning the NTK feature map  we also introduce the following non-linear point-wise operator M  
given for two signals x  y  by

where ϕ0/1 are kernel mappings of arc-cosine 0/1 kernels  as deﬁned in Section 2.1.

ϕ1(x[u]) (cid:19)  
M (x  y)[u] =(cid:18)ϕ0(x[u]) ⊗ y[u]

(8)

CNN deﬁnition and NTK. We consider a network f (x; θ) =q 2

mnhwn+1  aniℓ2   with

˜ak[u] =(W 1P 1x[u] 

mk–1

ak[u] = Akσ(˜ak)[u] 

q 2

W kP kak–1[u] 

if k = 1 
if k ∈ {2  . . .   n} 

k = 1  . . .   n 

4

where W k ∈ Rmk×mk–1|Sk| and wn ∈ ℓ2(Zd  Rmn ) are initialized with N (0  1) entries  and σ(˜xk)
denotes the signal with σ applied element-wise to ˜xk. We are now ready to state our result on the
NTK for this model.

Proposition 2 (NTK feature map for CNN). The NTK for the above CNN  obtained when the number

of feature maps m1  . . .   mn → ∞ (sequentially)  is given by K(x  x′) = hΦ(x)  Φ(x′)iℓ2(Zd) 
with Φ(x)[u] = AnM (xn  yn)[u]  where yn and xn are deﬁned recursively for a given input x by
y1[u] = x1[u] = P 1x[u]  and for k ≥ 2 

xk[u] = P kAk–1ϕ1(xk–1)[u]
yk[u] = P kAk–1M (xk–1  yk–1)[u] 

with the abuse of notation ϕ1(x)[u] = ϕ1(x[u]) for a signal x.

i [u] tend to a Gaussian process with covariance Σk(x  u; x′  u′) = hxk[u]  x′

The proof is given in Appendix A.2  where we also show that in the over-parameterization limit  the
pre-activations ˜ak
(this is related to recent papers [24  40] studying Gaussian process limits of Bayesian convolutional
networks). The proof is by induction and relies on similar arguments to [27] for fully-connected
networks  in addition to exploiting linearity of the operators P k and Ak  as well as recursive feature
maps for hierarchical kernels. The recent papers [3  51] also study NTKs for certain convolutional
networks; in contrast to these works  our derivation considers general signals in ℓ2(Zd)  supports
intermediate pooling or downsampling by changing Ak  and provides a more intuitive construction
through kernel mappings and the operators P k and Ak. Note that the feature maps xk are deﬁned
independently from the yk  and in fact correspond to more standard multi-layer deep kernel ma-
chines [11  17  19  33] or covariance functions of certain deep Bayesian networks [24  28  36  40].
They can also be seen as the feature maps of the limiting kernel that arises when only training weights
in the last layer and ﬁxing other layers at initialization (see  e.g.  [19]).

k[u′]i

3 Two-Layer Networks

In this section  we study smoothness and approximation properties of the RKHS deﬁned by neural
tangent kernels for two-layer networks. For ReLU activations  we show that the NTK kernel mapping
is not Lipschitz  but satisﬁes a weaker smoothness property. In Section 3.2  we characterize the RKHS
for ReLU activations and study its approximation properties and beneﬁts. Finally  we comment on
the use of other activations in Section 3.3.

3.1 Smoothness of two-layer ReLU networks

Here we study the RKHS H of the NTK for two-layer ReLU networks  deﬁned in (3)  focusing on
smoothness properties of the kernel mapping  denoted Φ(·). Recall that smoothness of the kernel
mapping guarantees smoothness of functions f ∈ H  through the relation

|f (x) − f (y)| ≤ kfkHkΦ(x) − Φ(y)kH.

(9)

We begin by showing that the kernel mapping for the NTK is not Lipschitz. This is in contrast to the
kernel κ1 in (5)  obtained by ﬁxing the weights in the ﬁrst layer and training only the second layer
weights (κ1 is 1-Lipschitz by [11  Lemma 1]).
Proposition 3 (Non-Lipschitzness). The kernel mapping Φ(·) of the two-layer NTK is not Lipschitz:

sup
x y

kΦ(x) − Φ(y)kH

kx − yk

→ +∞.

This is true even when looking only at points x  y on the sphere. It follows that the RKHS H contains
unit-norm functions with arbitrarily large Lipschitz constant.

Note that the instability is due to ϕ0  which comes from gradients w.r.t. ﬁrst layer weigts. We now
show that a weaker guarantee holds nevertheless  resembling 1/2-Hölder smoothness.

Proposition 4 (Smoothness for ReLU NTK). We have the following smoothness properties:

1. For x  y such that kxk = kyk = 1  the kernel mapping ϕ0 satisﬁes kϕ0(x)−ϕ0(y)k ≤pkx − yk.

5

2. For general non-zero x  y  we have kϕ0(x) − ϕ0(y)k ≤q

3. The kernel mapping Φ of the NTK then satisﬁes

min(kxk kyk)kx − yk.
kΦ(x) − Φ(y)k ≤pmin(kxk kyk)kx − yk + 2kx − yk.

1

We note that while such smoothness properties apply to the functions in the RKHS of the studied
limiting kernels  the neural network functions obtained at ﬁnite width and their linearizations around
initialization are not in the RKHS and thus may not preserve such smoothness properties  despite
preserving good generalization properties  as in random feature models [7  43]. This discrepancy
may be a source of instability to adversarial perturbations.

3.2 Approximation properties for the two-layer ReLU NTK

In the previous section  we found that the NTK κ for two-layer ReLU networks yields weaker
smoothness guarantees compared to the kernel κ1 obtained when the ﬁrst layer is ﬁxed. We now
show that the NTK has better approximation properties  by studying the RKHS through a spectral
decomposition of the kernel and the decay of the corresponding eigenvalues. This highlights a
tradeoff between smoothness and approximation.
The next proposition gives the Mercer decomposition of the NTK κ(hx  ui) in (4)  where x  y are in
the p − 1 sphere Sp−1 = {x ∈ Rp : kxk = 1}. The decomposition is given in the basis of spherical
harmonics  as is common for dot-product kernels [46  47]  and our derivation uses results by Bach [6]
on similar decompositions of positively homogeneous activations of the form σα(u) = (u)α
+. See
Appendix C for background and proofs.

Proposition 5 (Mercer decomposition of ReLU NTK). For any x  y ∈ Sp−1  we have the following
decomposition of the NTK κ:

κ(hx  yi) =

µk

∞

Xk=0

N (p k)

Xj=1

Yk j(x)Yk j(y) 

(10)

where Yk j  j = 1  . . .   N (p  k) are spherical harmonic polynomials of degree k  and the non-negative
eigenvalues µk satisfy µ0  µ1 > 0  µk = 0 if k = 2j + 1 with j ≥ 1  and otherwise µk ∼ C(p)k−p
as k → ∞  with C(p) a constant depending only on p. Then  the RKHS is described by:

N (p k)

f = Xk≥0 µk6=0

Xj=1

ak jYk j(·)

s.t. kfk2

H := Xk≥0 µk6=0

Xj=1

N (p k)

a2
k j
µk

H =


.

(11)

< ∞


The zero eigenvalues prevent certain functions from belonging to the RKHS  namely those with
non-zero Fourier coefﬁcients on the corresponding basis elements (note that adding a bias may prevent
such zero eigenvalues [9]). Here  a sufﬁcient condition for all such coefﬁcients to be zero is that the
function is even [6]. Note that for the arc-cosine 1 kernel κ1  we have a faster decay µk = O(k−p−2) 
leading to a “smaller” RKHS (see Lemma 17 in Appendix C and [6]). Moreover  the k−p asymptotic
equivalent comes from the term uκ0(u) in the deﬁnition (4) of κ  which comes from gradients of
ﬁrst layer weights; the second layer gradients yield κ1  whose contribution to µk becomes negligible
for large k. We use an identity also used in the recent paper [25] which compares similar kernels
in a speciﬁc high-dimensional limit for generic activations; in contrast to [25]  we focus on ReLUs
and study eigenvalue decays in ﬁnite dimension. We note that our decomposition uses a uniform
distribution on the sphere  which allows a precise study of eigenvalues and approximation properties
of the RKHS using spherical harmonics. When the data distribution is also uniform on the sphere  or
absolutely continuous w.r.t. the uniform distribution  our obtained eigenvalues are closely related to
those of integral operators for learning problems  which can determine  e.g.  non-parametric rates
of convergence (e.g.  [14  23]) as well as degrees-of-freedom quantities for kernel approximation
(e.g.  [7  43]). Such quantities often depend on the eigenvalue decay of the integral operator  which
can be obtained from µk after taking multiplicity into account. This is also related to the rate
of convergence of gradient descent in the lazy training regime  which depends on the minimum
eigenvalue of the empirical kernel matrix in [16  20  21].

We now provide sufﬁcient conditions for a function f : Sp−1 → R to be in H  as well as rates of

approximation of Lipschitz functions on the sphere  adapting results of [6] (speciﬁcally Proposition 2
and 3 in [6]) to our NTK setting.

6

Corollary 6 (Sufﬁcient condition for f ∈ H). Let f : Sp−1 → R be an even function such that all
i-th order derivatives exist and are bounded by η for 0 ≤ i ≤ s  with s ≥ p/2. Then f ∈ H with
kfkH ≤ C(p)η  where C(p) is a constant that only depends on p.
Corollary 7 (Approximation of Lipschitz functions). Let f : Sp−1 → R be an even function such
that f (x) ≤ η and |f (x) − f (y)| ≤ ηkx − yk  for all x  y ∈ Sp−1. There is a function g ∈ H with
kgkH ≤ δ  where δ is larger than a constant depending only on p  such that
log(cid:18) δ
η(cid:19) .

x∈Sp−1 |f (x) − g(x)| ≤ C(p)η(cid:18) δ

sup

η(cid:19)−1/(p/2−1)

For both results  there is an improvement over κ1  for which Corollary 6 requires s ≥ p/2 + 1
bounded derivatives  and Corollary 7 leads to a weaker rate in (δ/η)−1/(p/2) (see [6  Propositions 2
and 3] with α = 1). These results show that in the over-parameterized regime of the NTK  training
multiple layers leads to better approximation properties compared to only training the last layer 
which corresponds to using κ1 instead of κ. In the different regime of “convex neural networks”
(e.g.  [6  45]) where neurons can be selected with a sparsity-promoting penalty  the approximation
rates shown in [6] for ReLU networks are also weaker than for the NTK in the worst case (though the
regime presents beneﬁts in terms of adaptivity)  suggesting that perhaps in some situations the “lazy”
regime of the NTK could be preferred over the regime where neurons are selected using sparsity.

Homogeneous case. When inputs do not lie on the sphere Sp−1 but in Rp  the NTK for two-layer
ReLU networks takes the form of a homogeneous dot-product kernel (3)  which deﬁnes a different
RKHS ¯H that we characterize below in terms of the RKHS H of the NTK on the sphere.
Proposition 8 (RKHS of the homogeneous NTK). The RKHS ¯H of the kernel K(x  x′) =
kxkkx′kκ(hx  x′i/kxkkx′k) on Rp consists of functions of the form f (x) = kxkg(x/kxk) with g ∈
H  where H is the RKHS on the sphere  and we have kfk ¯H = kgkH.
Note that while such a restriction to homogeneous functions may be limiting  one may easily obtain
non-homogeneous functions by considering an augmented variable z = (x⊤  R)⊤ and deﬁning
f (x) = kzkg(z/kzk)  where g is now deﬁned on the p-sphere Sp. When inputs are in a ball of
radius R  this reformulation preserves regularity properties (see [6  Section 3]).

3.3 Smoothness with other activations

In this section  we look at smoothness of two-layer networks with different activation functions.
Following the derivation for the ReLU in Section 2.1  the NTK for a general activation σ is given by

Kσ(x  x′) = hx  x′i Ew∼N (0 1)[σ′(hw  xi)σ′(hw  x′i)] + Ew∼N (0 1)[σ(hw  xi)σ(hw  x′i)].

We then have the following the following result.
Proposition 9 (Lipschitzness for smooth activations). Assume that σ is twice differentiable and that
the quantities γj := Eu∼N (0 1)[(σ(j)(u))2] for j = 0  1  2 are bounded  with γ0 > 0. Then  for x  y
on the unit sphere  the kernel mapping Φσ of Kσ satisﬁes

kΦσ(x) − Φσ(y)k ≤s(γ0 + γ1) max(cid:18)1 

2γ1 + γ2

γ0 + γ1 (cid:19) · kx − yk.

The proof uses results from [19] on relationships between activations and the corresponding kernels 
as well as smoothness results for dot-product kernels in [11] (see Appendix B.3). If  for instance  we
consider the exponential activation σ(u) = eu−2  we have γj = 1 for all j (using results from [19])  so
that the kernel mapping is Lipschitz with constant √3. For the soft-plus activation σ(u) = log(1+eu) 
we may evaluate the integrals numerically  obtaining (γ0  γ1  γ2) ≈ (2.31  0.74  0.11)  so that the
kernel mapping is Lipschitz with constant ≈ 1.75.

4 Deep Convolutional Networks

In this section  we study smoothness and stability properties of the NTK kernel mapping for con-
volutional networks with ReLU activations. In order to properly deﬁne deformations  we consider

7

following [11  35]. The goal of deformation stability guarantees is to ensure that the data representa-
tion (in this case  the kernel mapping Φ) does not change too much when the input signal is slightly
deformed  for instance with a small translation or rotation of an image—a useful inductive bias for

continuous signals x(u) in L2(Rd) instead of ℓ2(Zd) (i.e.  we have kxk2 := R kx(u)k2du < ∞) 
natural signals. For a C 1-diffeomorphism τ : Rd → Rd  denoting Lτ x(u) = x(u − τ (u)) the action

operator of the diffeomorphism  we will show a guarantee of the form

kΦ(Lτ x) − Φ(x)k ≤ (ω(k∇τk∞) + Ckτk∞)kxk 

Properties of the operators.

where k∇τk∞ is the maximum operator norm of the Jacobian ∇τ (u) over Rd  kτk∞ = supu |τ (u)| 
ω is an increasing function and C a positive constant. The second term controls translation invariance 
and C typically decreases with the scale of the last pooling layer (σn below)  while the ﬁrst term
controls deformation stability  since k∇τk∞ measures the “size” of deformations. The function ω(t)
is typically a linear function of t in other settings [11  35]  here we will obtain a faster growth of
order √t for small t  due to the weaker smoothness that arises from the arc-cosine 0 kernel mappings.
In this continuous setup  P k is now given for a signal x ∈ L2 by
P kx(u) = λ(Sk)−1/2(x(u + v))v∈Sk   where λ is the Lebesgue measure. We then have kP kxk =
kxk  and considering normalized Gaussian pooling ﬁlters  we have kAkxk ≤ kxk by Young’s
inequality [11]. The non-linear operator M is deﬁned point-wise analogously to (8)  and satisﬁes
kM (x  y)k2 = kxk2 +kyk2. We thus have that the feature maps in the continuous analog of the NTK
construction in Proposition 2 are in L2 as long as x is in L2. Note that this does not hold for some
smooth activations  where kM (x  y)(u)k may be a positive constant even when x(u) = y(u) = 0 
leading to unbounded L2 norm for M (x  y). The next lemma studies the smoothness of M   extending
results from Section 3.1 to signals in L2.
Lemma 10 (Smoothness of operator M ). For two signals x  y ∈ L2(Rd)  we have

kM (x  y) − M (x′  y′)k ≤pmin(kyk ky′k)kx − x′k + kx − x′k + ky − y′k.

Assumptions on architecture. Following [11]  we introduce an initial pooling layer A0  corre-
sponding to an anti-aliasing ﬁlter  which is necessary for stability and is a reasonable assumption given
that in practice input signals are discrete  with high frequencies typically ﬁltered by an acquisition
device. Thus  we consider the kernel representation Φn(x) := Φ(A0x)  with Φ as in Proposition 2.
We also assume that patch sizes are controlled by the scale of pooling ﬁlters  that is

(12)

v∈Sk |v| ≤ βσk–1 
sup

(13)

for some constant β  where σk–1 is the scale of the pooling operation Ak–1  which typically increases
exponentially with depth  corresponding to a ﬁxed downsampling factor at each layer in the discrete
case. By a simple induction  we can show the following.

Lemma 11 (Norm and smoothness of Φn). We have kΦn(x)k ≤ √n + 1kxk  and
kΦn(x) − Φn(x′)k ≤ (n + 1)kx − x′k + O(n5/4)pkxkkx − x′k.

Deformation stability bound. We now present our main guarantee on deformation stability for
the NTK kernel mapping (the proof is given in Appendix B).

following stability bound:

Proposition 12 (Stability of NTK). Let Φn(x) = Φ(A0x)  and assume k∇τk∞ ≤ 1/2. We have the
kΦn(Lτ x) − Φn(x)k ≤(cid:18)C(β)1/2Cn7/4k∇τk1/2
σn kτk∞(cid:19)kxk 

∞ + C(β)C ′n2k∇τk∞ + √n + 1

C′′

where C  C′  C′′ are constants depending only on d  and C(β) also depends on β deﬁned in (13).

Compared to the bound in [11]  the ﬁrst term shows weaker stability due to faster growth with k∇τk∞ 
which comes from (12). The dependence on the depth n is also poorer (n2 instead of n)  however
note that in contrast to [11]  the norm and smoothness constants of Φn(x) in Lemma 11 grow with n
here  partially explaining this gap. We also note that choosing small β (i.e.  small patches in a
discrete setting) is more helpful to improve stability than a small number of layers n  given that C(β)
increases polynomially with β  while n typically decreases logarithmically with β when one seeks a
ﬁxed target level of translation invariance (see [11  Section 3.2]).

8

(a) CKN with arc-cosine 1 kernels

(b) NTK

Figure 1: Geometry of kernel mapping for CKN and NTK convolutional kernels  on digit images and
their deformations from the Inﬁnite MNIST dataset [32]. The curves show average relative distances
of a single digit to its deformations  combinations of translations and deformations  digits of the same
label  and digits of any label. See Appendix D for more details on the experimental setup.

By ﬁxing weights of all layers but the last  we would instead obtain feature maps of the form Anxn
(using notation from Proposition 2)  which satisfy the improved stability guarantee of [11]. The
question of approximation for the deep convolutional case is more involved and left for future work 
but it is reasonable to expect that the RKHS for the NTK is at least as large as that of the simpler
kernel with ﬁxed layers before the last  given that the latter appears as one of the terms in the NTK.
This again hints at a tradeoff between stability and approximation  suggesting that one may be able to
learn less stable but more discriminative functions in the NTK regime by training all layers.

Numerical experiments. We now study numerically the stability of (exact) kernel mapping rep-
resentations for convolutional networks with 2 hidden convolutional layers. We consider both a
convolutional kernel network (CKN  [11]) with arc-cosine kernels of degree 1 on patches (correspond-
ing to the kernel obtained when only training the last layer and keeping previous layers ﬁxed) and the
corresponding NTK. Figure 1 shows the resulting average distances  when considering collections of
digits and deformations thereof. In particular  we ﬁnd that for small deformations  the distance to the
original image tends to grow more quickly for the NTK compared to the CKN  as the theory suggests
(a square-root growth rate rather than a linear one). Note also that the relative distances are generally
larger for the NTK than for the CKN  suggesting the CKN may be more smooth.

5 Discussion

In this paper  we have studied the inductive bias of the “lazy training” regime for over-parameterized
neural networks  by considering the neural tangent kernel of different architectures  and analyzing
properties of the corresponding RKHS  which characterizes the functions that can be learned efﬁ-
ciently in this regime. We ﬁnd that the NTK for ReLU networks has better approximation properties
compared to other neural network kernels  but weaker smoothness properties  although these can
still guarantee a form of stability to deformations for CNN architectures  providing an important
inductive bias for natural signals. While these properties may help obtain better performance when
large amounts of data are available  they can also lead to a poorer estimation error when data is scarce 
a setting in which smoother kernels or better regularization strategies may be helpful.

It should be noted that while our study of functions in the RKHS may determine what target functions
can be learned by over-parameterized networks  the obtained networks with ﬁnite neurons do not
belong to the same RKHS  and hence may be less stable than such target functions  at least outside of
the training data  due to approximations both in the linearization (1) and between the ﬁnite neuron
and limiting kernels. Additionally  approximation of certain non-smooth functions in this regime
may require a very large number of neurons [52]. Finally  we note that while this “lazy” regime is
interesting and could partly explain the success of deep learning methods  it does not explain  for
instance  the common behavior in early layers where neurons move to select useful features in the
data  such as Gabor ﬁlters  as pointed out in [16]. In particular  such a behavior might provide better
statistical efﬁciency by adapting to simple structures in the data (see  e.g.  [6])  something which is
not captured in a kernel regime like the NTK. It would be interesting to study inductive biases in a
regime somewhere in between  where neurons may move at least in the ﬁrst few layers.

9

0123deformation size0.00.10.2mean relative distancedeformationsdeformations + translationsame labelall labels0123deformation size0.00.10.20.3mean relative distanceAcknowledgments

This work was supported by the ERC grant number 714381 (SOLARIS project)  the ANR 3IA
MIAI@Grenoble Alpes  and by the MSR-Inria joint centre. The authors thank Francis Bach and
Lénaïc Chizat for useful discussions.

References

[1] Z. Allen-Zhu  Y. Li  and Y. Liang. Learning and generalization in overparameterized neural
networks  going beyond two layers. In Advances in Neural Information Processing Systems
(NeurIPS)  2019.

[2] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the International Conference on Machine Learning (ICML) 
2019.

[3] S. Arora  S. S. Du  W. Hu  Z. Li  R. Salakhutdinov  and R. Wang. On exact computation with
an inﬁnitely wide neural net. In Advances in Neural Information Processing Systems (NeurIPS) 
2019.

[4] S. Arora  S. S. Du  W. Hu  Z. Li  and R. Wang. Fine-grained analysis of optimization and gener-
alization for overparameterized two-layer neural networks. In Proceedings of the International
Conference on Machine Learning (ICML)  2019.

[5] K. Atkinson and W. Han. Spherical harmonics and approximations on the unit sphere: an

introduction  volume 2044. Springer Science & Business Media  2012.

[6] F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine

Learning Research (JMLR)  18(19):1–53  2017.

[7] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions.

Journal of Machine Learning Research (JMLR)  18(21):1–38  2017.

[8] P. L. Bartlett  P. M. Long  G. Lugosi  and A. Tsigler. Benign overﬁtting in linear regression.

arXiv preprint arXiv:1906.11300  2019.

[9] R. Basri  D. Jacobs  Y. Kasten  and S. Kritchman. The convergence rate of neural networks
for learned functions of different frequencies. In Advances in Neural Information Processing
Systems (NeurIPS)  2019.

[10] M. Belkin  S. Ma  and S. Mandal. To understand deep learning we need to understand kernel
learning. In Proceedings of the International Conference on Machine Learning (ICML)  2018.

[11] A. Bietti and J. Mairal. Group invariance  stability to deformations  and complexity of deep
convolutional representations. Journal of Machine Learning Research (JMLR)  20(25):1–49 
2019.

[12] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Transactions on pattern

analysis and machine intelligence (PAMI)  35(8):1872–1886  2013.

[13] Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep

neural networks. In Advances in Neural Information Processing Systems (NeurIPS)  2019.

[14] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm.

Foundations of Computational Mathematics  7(3):331–368  2007.

[15] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized
In Advances in Neural Information Processing Systems

models using optimal transport.
(NeurIPS)  2018.

[16] L. Chizat  E. Oyallon  and F. Bach. On lazy training in differentiable programming. In Advances

in Neural Information Processing Systems (NeurIPS)  2019.

[17] Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information

Processing Systems (NIPS)  2009.

[18] F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American

mathematical society  39(1):1–49  2002.

10

[19] A. Daniely  R. Frostig  and Y. Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances in Neural Information
Processing Systems (NIPS)  2016.

[20] S. S. Du  J. D. Lee  H. Li  L. Wang  and X. Zhai. Gradient descent ﬁnds global minima of deep
neural networks. In Proceedings of the International Conference on Machine Learning (ICML) 
2019.

[21] S. S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-
parameterized neural networks. In Proceedings of the International Conference on Learning
Representations (ICLR)  2019.

[22] C. Efthimiou and C. Frye. Spherical harmonics in p dimensions. World Scientiﬁc  2014.

[23] S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithm.

arXiv preprint arXiv:1702.07254  2017.

[24] A. Garriga-Alonso  L. Aitchison  and C. E. Rasmussen. Deep convolutional networks as shallow
gaussian processes. In Proceedings of the International Conference on Learning Representations
(ICLR)  2019.

[25] B. Ghorbani  S. Mei  T. Misiakiewicz  and A. Montanari. Linearized two-layers neural networks

in high dimension. arXiv preprint arXiv:1904.12191  2019.

[26] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal

approximators. Neural networks  2(5):359–366  1989.

[27] A. Jacot  F. Gabriel  and C. Hongler. Neural tangent kernel: Convergence and generalization in

neural networks. In Advances in Neural Information Processing Systems (NeurIPS)  2018.

[28] J. Lee  Y. Bahri  R. Novak  S. S. Schoenholz  J. Pennington  and J. Sohl-Dickstein. Deep neural
networks as gaussian processes. In Proceedings of the International Conference on Learning
Representations (ICLR)  2018.

[29] J. Lee  L. Xiao  S. S. Schoenholz  Y. Bahri  J. Sohl-Dickstein  and J. Pennington. Wide neural
networks of any depth evolve as linear models under gradient descent. In Advances in Neural
Information Processing Systems (NeurIPS)  2019.

[30] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent

on structured data. In Advances in Neural Information Processing Systems (NeurIPS)  2018.

[31] T. Liang and A. Rakhlin. Just interpolate: Kernel" ridgeless" regression can generalize. Annals

of Statistics  2019.

[32] G. Loosli  S. Canu  and L. Bottou. Training invariant support vector machines using selective
sampling. In Large Scale Kernel Machines  pages 301–320. MIT Press  Cambridge  MA.  2007.

[33] J. Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. In

Advances in Neural Information Processing Systems (NIPS)  2016.

[34] J. Mairal  P. Koniusz  Z. Harchaoui  and C. Schmid. Convolutional kernel networks. In Advances

in Neural Information Processing Systems (NIPS)  2014.

[35] S. Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics 

65(10):1331–1398  2012.

[36] A. Matthews  M. Rowland  J. Hron  R. E. Turner  and Z. Ghahramani. Gaussian process

behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271  2018.

[37] S. Mei  T. Misiakiewicz  and A. Montanari. Mean-ﬁeld theory of two-layers neural networks:

dimension-free bounds and kernel limit. In Conference on Learning Theory (COLT)  2019.

[38] S. Mei  A. Montanari  and P.-M. Nguyen. A mean ﬁeld view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–E7671 
2018.

[39] B. Neyshabur  R. Tomioka  and N. Srebro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. In Proceedings of the International Conference on
Learning Representations (ICLR)  2015.

[40] R. Novak  L. Xiao  Y. Bahri  J. Lee  G. Yang  J. Hron  D. A. Abolaﬁa  J. Pennington  and
J. Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian
processes. In Proceedings of the International Conference on Learning Representations (ICLR) 
2019.

11

[41] A. Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica 

8:143–195  1999.

[42] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

Neural Information Processing Systems (NIPS)  2008.

[43] A. Rudi and L. Rosasco. Generalization properties of learning with random features.

In

Advances in Neural Information Processing Systems (NIPS)  2017.

[44] S. Saitoh. Integral transforms  reproducing kernels and their applications  volume 369. CRC

Press  1997.

[45] P. Savarese  I. Evron  D. Soudry  and N. Srebro. How do inﬁnite width bounded norm networks

look in function space? In Conference on Learning Theory (COLT)  2019.

[46] B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines  regularization 

optimization  and beyond. 2001.

[47] A. J. Smola  Z. L. Ovari  and R. C. Williamson. Regularization with dot-product kernels. In

Advances in Neural Information Processing Systems (NIPS)  2001.

[48] D. Soudry  E. Hoffer  M. S. Nacson  S. Gunasekar  and N. Srebro. The implicit bias of gradient

descent on separable data. Journal of Machine Learning Research (JMLR).

[49] F. Williams  M. Trager  C. Silva  D. Panozzo  D. Zorin  and J. Bruna. Gradient dynamics of
shallow low-dimensional relu networks. In Advances in Neural Information Processing Systems
(NeurIPS)  2019.

[50] B. Xie  Y. Liang  and L. Song. Diverse neural network learns true target functions.

In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2017.

[51] G. Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior 
gradient independence  and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760 
2019.

[52] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding

neural networks. In Advances in Neural Information Processing Systems (NeurIPS)  2019.

[53] C. Zhang  S. Bengio  and Y. Singer. Are all layers created equal?

arXiv preprint

arXiv:1902.01996  2019.

[54] D. Zou  Y. Cao  D. Zhou  and Q. Gu. Stochastic gradient descent optimizes over-parameterized

deep relu networks. Machine Learning  2019.

12

,Alberto Bietti
Julien Mairal