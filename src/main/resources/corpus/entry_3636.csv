2018,Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning,Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning  via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work (Efroni et al.  2018)  multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work  we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty  arising with soft-policy updates: even in the absence of approximations  and contrary to the 1-step-greedy case  monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty  we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.,Multiple-Step Greedy Policies in Online and

Approximate Reinforcement Learning

Yonathan Efroni∗

Gal Dalal∗

jonathan.efroni@gmail.com

gald@campus.technion.ac.il

Bruno Scherrer†

Shie Mannor∗

bruno.scherrer@inria.fr

shie@ee.technion.ac.il

Abstract

Multiple-step lookahead policies have demonstrated high empirical competence
in Reinforcement Learning  via the use of Monte Carlo Tree Search or Model
Predictive Control. In a recent work [5]  multiple-step greedy policies and their use
in vanilla Policy Iteration algorithms were proposed and analyzed. In this work 
we study multiple-step greedy algorithms in more practical setups. We begin by
highlighting a counter-intuitive difﬁculty  arising with soft-policy updates: even in
the absence of approximations  and contrary to the 1-step-greedy case  monotonic
policy improvement is not guaranteed unless the update stepsize is sufﬁciently
large. Taking particular care about this difﬁculty  we formulate and analyze online
and approximate algorithms that use such a multi-step greedy operator.

1

Introduction

The use of the 1-step policy improvement in Reinforcement Learning (RL) was theoretically inves-
tigated under several frameworks  e.g.  Policy Iteration (PI) [18]  approximate PI [2  9  13]  and
Actor-Critic [10]; its practical uses are abundant [22  12  25]. However  single-step based improve-
ment is not necessarily the optimal choice. It was  in fact  empirically demonstrated that multiple-step
greedy policies can perform conspicuously better. Notable examples arise from the integration of RL
and Monte Carlo Tree Search [4  28  23  3  25  24] or Model Predictive Control [15  6  27].
Recent work [5] provided guarantees on the performance of the multiple-step greedy policy and
generalizations of it in PI. Here  we establish it in the two practical contexts of online and approximate
PI. With this objective in mind  we begin by highlighting a speciﬁc difﬁculty: softly updating a policy
with respect to (w.r.t.) a multiple-step greedy policy does not necessarily result in improvement of
the policy (Section 4). We ﬁnd this property intriguing since monotonic improvement is guaranteed
in the case of soft updates w.r.t. the 1-step greedy policy  and is central to the analysis of many
RL algorithms [10  9  22]. We thus engineer several algorithms to circumvent this difﬁculty and
provide some non-trivial performance guarantees  that support the interest of using multi-step greedy
operators. These algorithms assume access to a generative model (Section 5) or to an approximate
multiple-step greedy policy (Section 6).

2 Preliminaries

Our framework is the inﬁnite-horizon discounted Markov Decision Process (MDP). An MDP is
deﬁned as the 5-tuple (S A  P  R  γ) [18]  where S is a ﬁnite state space  A is a ﬁnite action space 

∗Department of Electrical Engineering  Technion  Israel Institute of Technology
†INRIA  Villers les Nancy  France

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

vπ ∈ R|S| be the value of a policy π  deﬁned in state s as vπ(s) ≡ Eπ[(cid:80)∞
v(st). It is known that vπ =(cid:80)∞

P ≡ P (s(cid:48)|s  a) is a transition kernel  R ≡ r(s  a) is a reward function  and γ ∈ (0  1) is a discount
factor. Let π : S → P(A) be a stationary policy  where P(A) is a probability distribution on A. Let
t=0 γtr(st  π(st))|s0 = s].
For brevity  we respectively denote the reward and value at time t by rt ≡ r(st  πt(st)) and vt ≡
t=0 γt(P π)trπ = (I − γP π)−1rπ  with the component-wise values
[P π]s s(cid:48) (cid:44) P (s(cid:48) | s  π(s)) and [rπ]s (cid:44) r(s  π(s)). Lastly  let

qπ(s  a) = Eπ[

γtr(st  π(st)) | s0 = s  a0 = a].

Our goal is to ﬁnd a policy π∗ yielding the optimal value v∗ such that

t=0

v∗ = max

(I − γP π)−1rπ = (I − γP π∗

π

)−1rπ∗

.

(1)

(2)

∞(cid:88)

This goal can be achieved using the three classical operators (equalities hold component-wise):

∀v  π  T πv = rπ + γP πv 
T πv 

∀v  T v = max
∀v  G(v) = {π : T πv = T v} 

π

where T π is a linear operator  T is the optimal Bellman operator and both T π and T are γ-contraction
mappings w.r.t. the max norm. It is known that the unique ﬁxed points of T π and T are vπ and v∗ 
respectively. The set G(v) is the standard set of 1-step greedy policies w.r.t. v.

3 The h- and κ-Greedy Policies

(cid:34)h−1(cid:88)

t=0

In this section  we bring forward necessary deﬁnitions and results on two classes of multiple-step
greedy policies: h- and κ-greedy [5]. Let h ∈ N\{0}. The h-greedy policy πh outputs the ﬁrst
optimal action out of the sequence of actions solving a non-stationary  h-horizon control problem as
follows:

∀s ∈ S  πh(s) ∈ arg max

π0

max

π1 .. πh−1

Eπ0...πh−1

γtr(st  πt(st)) + γhv(sh) | s0 = s

.

Since the h-greedy policy can be represented as the 1-step greedy policy w.r.t. T h−1v  the set of
h-greedy policies w.r.t. v  Gh(v)  can be formally deﬁned as follows:

h v = T πT h−1v 

∀v  π  T π
∀v  Gh(v) = {π : T π

h v = T hv}.

Let κ ∈ [0  1]. The set of κ-greedy policies w.r.t. a value function v  Gκ(v)  is deﬁned using the
following operators:

(cid:35)

∀v  π  T π

κ v = (I − κγP π)−1(rπ + (1 − κ)γP πv)

∀v  Tκv = max
T π
κ v = max
κ v = Tκv}.
∀v  Gκ(v) = {π : T π

π

π

(I − κγP π)−1(rπ + (1 − κ)γP πv)

(3)

Remark 1. A comparison of (2) and (3) reveals that ﬁnding the κ-greedy policy is equivalent to
solving a κγ-discounted MDP with shaped reward rπ

= rπ + (1 − κ)γP πv.

def

v κ

In [5  Proposition 11]  the κ-greedy policy was explained to be interpolating over all geometrically
κ-weighted h-greedy policies. It was also shown that for κ = 0  the 1-step greedy policy is restored 
while for κ = 1  the κ-greedy policy is the optimal policy.

κ and Tκ are ξκ contraction mappings  where ξκ = γ(1−κ)

1−γκ ∈ [0  γ]. Their respective ﬁxed
Both T π
points are vπ and v∗. For brevity  where there is no risk of confusion  we shall denote ξκ by ξ.
Moreover  in [5] it was shown that both the h- and κ-greedy policies w.r.t. vπ are strictly better then
π  unless π = π∗.

2

−c
a0

s3

0
a0

s1

a1

1

s2

a0

0

a0

0

s0

0

a1

Figure 1: The Tightrope Walking MDP used in the counter example of Theorem 1.

Next  let

qπ
κ (s  a) = max

π(cid:48)

Eπ(cid:48)

∞(cid:88)

[

t=0

(κγ)t(r(st  π(cid:48)(st)) + γ(1 − κ)vπ(st+1) | s0 = s  a0 = a].

(4)

The latter is the optimal q-function of the surrogate  γκ-discounted MDP with vπ-shaped reward (see
Remark 1). Thus  we can obtain a κ-greedy policy  πκ ∈ Gκ(vπ)  directly from qπ
κ :

πκ(s) ∈ arg max
κ=0(s  a) is the 1-step greedy policy since qπ

κ (s  a)  ∀s ∈ S.
qπ

a

κ=0(s  a) = qπ(s  a).

See that the greedy policy w.r.t. qπ

4 Multi-step Policy Improvement and Soft Updates

In this section  we focus on policy improvement of multiple-step greedy policies  performed with soft
updates. Soft updates of the 1-step greedy policy have proved necessary and beneﬁcial in prominent
algorithms [10  9  22]. Here  we begin by describing an intrinsic difﬁculty in selecting the step-size
parameter α ∈ (0  1] when updating with multiple-step greedy policies. Speciﬁcally  denote by π(cid:48)
such multiple-step greedy policy w.r.t. vπ. Then  πnew = (1 − α)π + απ(cid:48) is not necessarily better
than π.
Theorem 1. For any MDP  let π be a policy and vπ its value. Let πκ ∈ Gκ(vπ) and πh ∈ Gh(vπ)
with κ ∈ [0  1] and h > 1. Consider the mixture policies with α ∈ (0  1] 

π(α  κ)

π(α  h)
Then we have the following equivalences:

def

= (1 − α)π + απκ 
= (1 − α)π + απh.

def

1. The inequality vπ(α κ) ≥ vπ holds for all MDPs if and only if α ∈ [κ  1].
2. The inequality vπ(α h) ≥ vπ holds for all MDPs if and only if α = 1.

The above inequalities hold entry-wise  with strict inequality in at least one entry unless vπ = v∗.

Proof sketch.
See Appendix A for the full proof. Here  we only provide a counterexample
demonstrating the potential non-monotonicity of π(α  κ) when the stepsize α is not big enough. One
can show the same for π(α  h) with the same example.
Consider the Tightrope Walking MDP in Fig. 1. It describes the act of walking on a rope: in the
initial state s0 the agent approaches the rope  in s1 the walking attempt occurs  s2 is the goal state
and s3 is repeatedly met if the agent falls from the rope  resulting in negative reward.
First  notice that by deﬁnition  ∀v  π∗ ∈ Gκ=1(v). We call this policy the “conﬁdent” policy.
Obviously  for any discount factor γ ∈ (0  1)  π∗(s0) = a1 and π∗(s1) = a1. Instead  consider the
“hesitant” policy π0(s) ≡ a0 ∀s. We now claim that for any α ∈ (0  1) and

c >

α
1 − α

3

(5)

the mixture policy  π(α  κ = 1) = (1 − α)π0 + απ∗  is not strictly better than π0. To see this  notice
that vπ0(s1) < 0 and vπ0 (s0) = 0; i.e.  the agent accumulates zero reward if she does not climb the
rope. Thus  while vπ0 (s0) = 0  taking any mixture of the conﬁdent and hesitant policies can result in
vπ(α κ=1)(s0) < 0  due to the portion of the transition to s1 and its negative contribution. Based on
this construction  let κ ∈ [0  1]. To ensure π∗ ∈ Gκ(vπ)  we ﬁnd it is necessary that

c ≤ κ
1 − κ

.

(6)

1−x   such a choice of c is indeed possible for α < κ.

To conclude  if both (5) and (6) are satisﬁed  the mixture policy does not improve over π0. Due to the
monotonicity of x
Theorem 1 guarantees monotonic improvement for the 1-step greedy policy as a special case when
κ = 0. Hence  we get that for any α ∈ (0  1]  the mixture of any policy π and the 1-step greedy policy
w.r.t. vπ is monotonically better then π. To the best of our knowledge  this result was not explicitly
stated anywhere. Instead  it appeared within proofs of several famous results  e.g  [10  Lemma 5.4] 
[9  Corollary 4.2]  and [21  Theorem 1].
In the rest of the paper  we shall focus on the κ-greedy policy and extend it to the online and the
approximate cases. The discovery that the κ-greedy policy w.r.t. vπ is not necessarily strictly better
than π will guide us in appropriately devising algorithms.

5 Online κ-Policy Iteration with Cautious Soft Updates

In [5]  it was shown that using the κ-greedy policy in the improvement stage leads to a convergent PI
procedure – the κ-PI algorithm. This algorithm repeats i) ﬁnding the optimal policy of small-horizon
surrogate MDP with shaped reward  and ii) calculating the value of the optimal policy and use it to
shape the reward of next iteration. Here  we devise a practical version of κ-PI  which is model-free 
online and runs in two timescales; i.e  it performs i) and ii) simultaneously.
The method is depicted in Algorithm 1. It is similar to the asynchronous PI analyzed in [16]  except
for two major differences. First  the fast timescale tracks both qπ  qπ
κ and not just qπ. Thus  it enables
access to both the 1-step-greedy and κ-greedy policies. The 1-step greedy policy is attained via the qπ
estimate  which is plugged into a q-learning [29] update rule for obtaining the κ-greedy policy. The
latter essentially solves the surrogate κγ-discounted MDP (see Remark 1). The second difference is in
the slow timescale  in which the policy is updated using a new operator  bs  as deﬁned below. To better
understand this operator  ﬁrst notice that in Stochastic Approximation methods such as Algorithm 1 
the policy is improved using soft updates with decaying stepsizes. However  as Theorem 1 states 
monotonic improvement is not guaranteed below a certain stepsize value. Hence  for q  qκ ∈ R|S×A|
and policy π  we set bs(q  qκ  π) to be the κ-greedy policy only when assured to have improvement:

bs(q  qκ  π) =

(cid:26)aκ(s)
= arg maxa q(s  a)  and vπ(s) =(cid:80)

if q(s  aκ) ≥ vπ(s) 
else 

a1-step(s)

def

1s=sk and φn(s  a)

k=1

def

def

= (cid:80)n

= (cid:80)n

def
= arg maxa qκ(s  a)  a1-step(s)

a π(a | s)q(s  a).
where aκ(s)
We respectively denote the state and state-action-pair visitation counters after the n-th time-step by
1(s a)=(sk ak). The stepsize sequences µf (·)  µs(·)
νn(s)
satisfy the common assumption (B2) in [16]  among which limn→∞ µs(n)/µf (n) → 0. The second
moments of {rn} are assumed to be bounded. Furthermore  let ν be some measure over the state
space  s.t. ∀s ∈ S  ν(s) > 0. Then  we assume to have a generative model G(ν  π)  using which we
sample state s ∼ ν  sample action a ∼ π(s)  apply action a and receive reward r and next state s(cid:48).
The fast-timescale update rules in lines 6 and 8 can be jointly written as the sum of H π
κ (q  qκ) (deﬁned
below) and a martingale difference noise.

k=1

4

Deﬁnition 1. Let q  qκ ∈ R|S||A|. The mapping H π
∀(s  a) ∈ S × A.

κ : R2|S||A| → R2|S||A| is deﬁned as follows

(cid:20)

(cid:21)

Algorithm 1 Two-Timescale Online κ-Policy-Iteration
1: initialize: π0  q0  qκ 0.
2: for n = 0  . . . do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for
12: return: π

n ∼ G(ν  πn)
sn  an  rn  s(cid:48)
# Fast-timescale updates
n(s(cid:48)
δn = rn + γvπ
qn+1(sn  an) ← qn(sn  an) + µf (φn+1(sn  an))δn
δκ n = rn + γ(1 − κ)vπ
n(s(cid:48)
n) + κγ maxa(cid:48) qκ n(s(cid:48)
qκ n+1(sn  an) ← qκ n(sn  an) + µf (φn+1(sn  an))δκ n
# Slow-timescale updates
πn+1(sn) ← πn(sn) + µs(νn+1(sn))(bsn (qn+1  qκ n+1  πn) − πn(sn))

n  a(cid:48)) − qκ n(sn  an)

n) − qn(sn  an)

H π

κ (q  qκ)(s  a)

def
=

r(s  a) + γ(1 − κ)Es(cid:48) aπ q(s(cid:48)  aπ) + κγEs(cid:48) maxa(cid:48) qκ(s(cid:48)  a(cid:48))

 

r(s  a) + γEs(cid:48) aπ q(s(cid:48)  aπ)

where s(cid:48) ∼ P (· | s  a)  aπ ∼ π(s(cid:48)).
The following lemma shows that  given a ﬁxed π  H π
(see Appendix B for the proof).
Lemma 2. H π

κ is a γ-contraction in the max-norm. Its ﬁxed point is [ qπ  qπ

κ is a contraction  equivalently to [16  Lemma 5.3]
κ ](cid:62)  as deﬁned in (1)  (4).
Finally  based on several intermediate results given in Appendix C and relaying on Lemma 2  we
establish the convergence of Algorithm 1.
Theorem 3. The coupled process (qn  qκ n  πn) in Algorithm 1 converges to the limit (q∗  q∗  π∗) 
where q∗ is the optimal q-function and π∗ is the optimal policy.
For κ = 1  the fast-timescale update rule in line 8 corresponds to that of q-learning [29]. For that κ 
Algorithm 1 uses an estimated optimal q-function to update the current policy when improvement is
assured. For κ < 1  the estimated κ-dependent optimal q-function (see (4)) is used  again with the
‘cautious’ policy update. Moreover  Algorithm 1 combines an off-policy algorithm  i.e.  q-learning 
with an on-policy Actor-Critic algorithm. To the best of our knowledge  this is the ﬁrst appearance of
these two approaches combined in a single algorithm.

6 Approximate κ-Policy Iteration with Hard Updates

Theorem 1 establishes the conditions required for guaranteed monotonic improvement of softly-
updated multiple-step greedy policies. The algorithm in Section 5 then accounts for these conditions
to ensure convergence. Contrarily  in this section  we derive and study algorithms that perform
hard policy-updates. Speciﬁcally  we generalize the prominent Approximate Policy Iteration (API)
[13  7  11] and Policy Search by Dynamic Programming (PSDP) [1  19]. For both  we obtain
performance guarantees that exhibit a tradeoff in the choice of κ  with optimal performance bound
achieved with κ > 0. That is  our approximate κ-generalized PI methods outperform the 1-step
greedy approximate PI methods in terms of best known guarantees.
For the algorithms here we assume an oracle that returns a κ-greedy policy with some error. Formally 
we denote by Gκ δ ν(v) the set of approximate κ-greedy policies w.r.t. v  with δ approximation error
under some measure ν.
Deﬁnition 2 (Approximate κ-greedy policy). Let v : S → R be a value function  δ ≥ 0 a real
number and ν a distribution over S. A policy π ∈ Gκ δ ν(v) if νT π
Such a device can be implemented using existing approximate methods  e.g.  Conservative Policy
Iteration (CPI) [9]  approximate PI or VI [7]  Policy Search [21]  or by having an access to an
approximate model of the environment. The approximate κ-greedy oracle assumed here is less

κ v ≥ νTκv − δ.

5

restrictive than the one assumed in [5]. There  a uniform error over states was assumed  whereas here 
the error is deﬁned w.r.t. a speciﬁc measure  ν. For practical purposes  ν can be thought of as the
initial sampling distribution to which the MDP is initialized. Lastly  notice that the larger κ is  the
harder it is to solve the surrogate κγ-discounted MDP since the discount factor is bigger [17  26  8];
i.e.  the computational cost of each call to the oracle increases.
Using the concept of concentrability coefﬁcients introduced in [13] (there  they were originally termed
“diffusion coefﬁcients”)  we follow the line of work in [13  14  7  19  11] to prove our performance
bounds. This allows a direct comparison of the algorithms proposed here with previously studied
approximate 1-step greedy algorithms. Namely  our bounds consist of concentrability coefﬁcients
C (1)  C (2)  C (2 k) and C π∗(1) from [19  11]  as well as two new coefﬁcients C π∗
Deﬁnition 3 (Concentrability coefﬁcients [19  11])). Let µ  ν be some measures over S. Let {c(i)}∞
be the sequence of the smallest values in [1 ∞) ∪ {∞} such that for every i  for all sequences
j=1 P πj ≤ c(i)ν. Let C (1)(µ  ν) = (1 −
i j=0 γi+jc(i + j + k). For brevity  we denote
i=0 be the sequence of the smallest values in
(i).

of deterministic stationary policies π1  π2  ..  πi  µ(cid:81)i
i=0 γic(i) and C (2 k)(µ  ν) = (1 − γ)2(cid:80)∞
γ)(cid:80)∞
[1 ∞)∪{∞} such that for every i  µ(cid:0)P π∗(cid:1)i ≤ cπ∗

(i)ν. Let C π∗(1)(µ  ν) = (1−γ)(cid:80)∞

C (2 0)(µ  ν) as C (2)(µ  ν). Similarly  let {cπ∗

κ and C π∗(1)

(i)}∞

i=0

i=0 γicπ∗

κ

.

We now introduce two new concentrability coefﬁcients suitable for bounding the worst-case perfor-
mance of PI algorithms with approximate κ-greedy policies.
Deﬁnition 4 (κ-Concentrability coefﬁcients). Let C π∗(1)
γ C π∗(1)(µ  ν) + (1 − ξ)κc(0).
κ µ ≤ C π∗
dπ∗
Also 
κ (µ  ν)ν  where
κ = (1 − κγ)(I − κγP π)−1 is
κ µ = (1 − ξ)µ(I − ξDπ∗
dπ∗
a stochastic matrix.

κ (µ  ν) ∈ [1 ∞) ∪ {∞} be the smallest value s.t.

)−1 is a probability measure and Dπ

let C π∗

(µ  ν) = ξ

κ P π∗

κ

(µ  ν); the latter was previously deﬁned in  e.g  [19  Deﬁnition 1].

In the deﬁnitions above  ν is the measure according to which the approximate improve-
ment
is guaranteed  while µ speciﬁes the distribution on which one measures the loss
Es∼µ[v∗(s) − vπk (s)] = µ(v∗ − vπk ) that we wish to bound. From Deﬁnition 4 it holds that
C π∗
κ=0(µ  ν) = C π∗
Before giving our performance bounds  we ﬁrst study the behavior of the coefﬁcients appearing in
them. The following lemma sheds light on the behavior of C π∗
κ (µ  ν). Speciﬁcally  it shows that
under certain constructions  C π∗
Lemma 4. Let ν(α) = (1 − α)ν + αµ. Then  for all κ(cid:48) > κ  there exists α∗ ∈ (0  1) such that
κ(cid:48) (µ  ν(α∗)) ≤ C π∗
C π∗
κ (µ  ν) > 1. For µ = ν this implies that
C π∗
κ (ν  ν) is a decreasing function of κ.
Deﬁnition 4 introduces two coefﬁcients with which we shall derive our bounds. Though traditional
arithmetic relations between them do not exist  they do comply to some notion of ordering.
Remark 2 (Order of concentrability coefﬁcients). In [19]  an order between the concentrability
coefﬁcients was introduced: a coefﬁcient A is said to be strictly better than B — a relation we denote
with A ≺ B — if and only if i) B < ∞ implies A < ∞ and ii) there exists an MDP for which A < ∞
and B = ∞. Particularly  it was argued that

κ (µ  ν) decreases3 as κ increases (see proof in Appendix D).

κ (µ  ν). The inequality is strict for C π∗

(µ  ν) ≺ C π∗(1)(µ  ν) ≺ C (1)(µ  ν) ≺ C (2)(µ  ν)  and

C π∗
C (2 k1)(µ  ν) ≺ C (2 k2)(µ  ν) if k2 < k1.
(µ  ν) is analogous to C π∗(1)(µ  ν)  while its deﬁnition might suggest improve-
κ (µ  ν) improves as κ increases  as

In this sense  C π∗(1)
ment as κ increases. Moreover  combined with the fact that C π∗
Lemma 4 suggests  C π∗

κ (µ  ν) is better than all previously deﬁned concentrability coefﬁcients.

κ

6.1 κ-Approximate Policy Iteration

A natural generalization of API [13  19  11] to the multiple-step greedy policy is κ-API  as given in
Algorithm 2. In each of its iterations  the policy is updated to the approximate κ-greedy policy w.r.t.
vπk−1; i.e  a policy from the set Gκ δ ν(vπk−1 ).

3A smaller coefﬁcient is obviously better. The best value for any concentrability coefﬁcient is 1.

6

Algorithm 2 κ-API

initialize κ ∈ [0  1]  ν  δ  vπ0
v ← vπ0
for k = 1  .. do
πk ← Gκ δ ν(v)
v ← vπk

end for
return π

Algorithm 3 κ-PSDP

initialize κ ∈ [0  1]  ν  δ  vπ0  Π = [ ]
v ← vπ0
for k = 1  .. do
πk ← Gκ δ ν(v)
v ← T πk
κ v
Π ←Append(Π  πk)

end for
return Π

(cid:16)

(cid:17)

 

(cid:16)
(cid:16)

(1 − κ)C (1)(µ  ν) + (1 − γκ)C π∗(1)

The following theorem gives a performance bound for κ-API (see proof in Appendix E)  with
Cκ−API(µ  ν) = (1 − κ)2C (2)(µ  ν) + (1 − γ)κ
κ−API(µ  ν) = (1 − κγ)
C (k 1)
κ−API(µ  ν) = (1 − κ)κ
C (k 2)
where g(κ) is a bounded function for κ ∈ [0  1].
Theorem 5. Let πk be the policy at the k-th iteration of κ-API and δ be the error as deﬁned in
Deﬁnition 2. Then

κ(1 − κγ)C π∗
 
(1 − γ)C (1)(µ  ν) + g(κ)(1 − κ)γkC (2 k)(µ  ν)

κ (µ  ν) + (1 − κ)2C (1)(µ  ν))

(µ  ν)

(cid:17)

(cid:17)

κ

 

(cid:24) log Rmax

δ(1−γ)
1−ξ

(cid:25)

Also  let k =

µ(v∗ − vπk ) ≤ Cκ−API(µ  ν)

(1 − γ)2
. Then µ(v∗ − vπk ) ≤ C(k 1)

κ−API(µ ν)
(1−γ)2

log

δ + ξk Rmax
1 − γ

.

(cid:16) Rmax

(1−γ)δ

(cid:17)

δ +

C(k 2)

κ−API(µ ν)
(1−γ)2

δ + δ.

(1−γ)2 δ + γkRmax
1−γ

For brevity  we now discuss the ﬁrst part of the statement; the same insights are true for the second
as well. The bound for the original API is restored for the 1-step greedy case of κ = 0  i.e 
µ(v∗ − vπk ) ≤ C(2)(µ ν)
[19  11]. As in the case of API  our bound consists of a ﬁxed
approximation error term and a geometrically decaying term. As for the other extreme  κ = 1 
we ﬁrst remind that in the non-approximate case  applying Tκ=1 amounts to solving the original
γ-discounted MDP in a single step [5  Remark 4]. In the approximate setup we investigate here 
this results in the vanishing of the second  geometrically decaying term  since ξ = 0 for κ = 1. We
are then left with a single constant approximation error: µ(v∗ − vπk ) ≤ c(0)δ. Notice that c(0) is
independent of π∗ (see Deﬁnition 3). It represents the mismatch between µ and ν [9].
Next  notice that  by deﬁnition (see Deﬁnition 3)  C (2)(µ  ν) > (1−γ)2c(0); i.e.  C(2)(µ ν)
(1−γ)2 δ > c(0)δ.
Given the discussion above  we have that the κ-API performance bound is strictly smaller with κ = 1
than with κ = 0. Hence  the bound suggests that κ-API is strictly better than the original API for
κ = 1. Since all expressions there are continuous  this behavior does not solely hold point-wise.
Remark 3 (Performance tradeoff). Naively  the above observation would lead to the choice of κ = 1.
However  it is reasonable to assume that δ  the error of the κ-greedy step  itself depends on κ  i.e 
δ ≡ δ(κ). The general form of such dependence is expected to be monotonically increasing: as the
effective horizon of the surrogate κγ-discounted MDP becomes larger  its solution is harder to obtain
(see Remark 1). Thus  Theorem 5 reveals a performance tradeoff as a function of κ.

6.2 κ-Policy Search by Dynamic Programming

We continue with generalizing another approximate PI method – PSDP [1  19]. We name it κ-PSDP
and introduce it in Algorithm 3. This algorithm updates the policy differently from κ-API. However 
similarly to κ-API  it uses hard updates. We will show this algorithm exhibits better performance
than any other previously analyzed approximate PI method [19].
The κ-PSDP algorithm  unlike κ-API  returns a sequence of deterministic policies  Π. Given this
sequence  we build a single  non-stationary policy by successively running Nk steps of Π[k]  followed

7

by Nk−1 steps of Π[k − 1]  etc  where {Ni}k
i=1 are i.i.d. geometric random variables with parameter
1 − κ. Once this process reaches π0  it runs π0 indeﬁnitely. We shall refer to this non-stationary
policy as σκ k. Its value vσκ k can be seen to satisfy

vσκ k = T Π[k]

κ

T Π[k−1]

κ

. . . T Π[1]

κ

vπ0.

This algorithm follows PSDP from [19]. Differently from it  the 1-step improvement is generalized to
the κ-greedy improvement and the non-stationary policy behaves randomly. The following theorem
gives a performance bound for it (see proof in Appendix F).
Theorem 6. Let σκ k be the policy at the k-th iteration of κ-PSDP and δ be the error as deﬁned in
Deﬁnition 2. Then

(cid:24) log Rmax

δ(1−γ)
1−ξ

(cid:25)

Also  let k =

µ(v∗ − vσκ k ) ≤ C π∗(1)
1 − ξ
. Then µ(v∗ − vσκ k ) ≤ Cπ∗

κ

(µ  ν)

κ (µ ν)
(1−ξ)2

δ + ξk Rmax
1 − γ

(cid:16) Rmax

log

(1−γ)δ

.

(cid:17)

δ + δ.

κ

Compared to κ-API from the previous section  the κ-PSDP bound consists of a different ﬁxed
approximation error and a shared geometrically decaying term. Regarding the former  notice that
C π∗(1)
(µ  ν) ≺ Cκ−API(µ  ν)  using the notation from Remark 2. This suggests that κ-PSDP is
strictly better than κ-API in the metrics we consider  and is in line with the comparison of the original
API to the original PSDP given in [19].
Similarly to the previous section  we again see that substituting κ = 1 gives a tighter bound than
κ = 0. The reason is that Cπ∗ (1)(µ ν)
δ > c(0)δ  by deﬁnition (see Deﬁnition 3); i.e.  we have that
κ-PSDP is generally better than PSDP. Also  contrarily to κ-API  here we directly see the performance
improvement as κ increases due to the decrease of C π∗
κ prescribed in Lemma 4  for the construction
given there. Moreover  the κ tradeoff discussion in Remark 3 applies here as well.
An additional advantage of this new algorithm over PSDP is reduced space complexity. This can be
seen from the 1 − ξ in the denominator in the choice of k in the second part of Theorem 6. It shows
that  since ξ is a strictly decreasing function of κ  better performance is guaranteed with signiﬁcantly
fewer iterations by increasing κ. Since the size of stored policy Π is linearly dependent on the number
of iterations  larger κ improves space efﬁciency.

1−γ

7 Discussion and Future Work

In this work  we introduced and analyzed online and approximate PI methods  generalized to the
κ-greedy policy  an instance of a multiple-step greedy policy. Doing so  we discovered two intriguing
properties compared to the well-studied 1-step greedy policy  which we believe can be impactful in
designing state-of-the-art algorithms. First  successive application of multiple-step greedy policies
with a soft  stepsize-based update does not guarantee improvement; see Theorem 1. To mitigate this
caveat  we designed an online PI algorithm with a ‘cautious’ improvement operator; see Section 5.
The second property we ﬁnd intriguing stemmed from analyzing κ generalizations of known approx-
imate hard-update PI methods. In Section 6  we revealed a performance tradeoff in κ  which can
be interpreted as a tradeoff between short-horizon bootstrap bias and long-rollout variance. This
corresponds to the known λ tradeoff in the famous TD(λ).
The two characteristics above lead to new compelling questions. The ﬁrst regards improvement
operators: would a non-monotonically improving PI scheme necessarily not converge to the optimal
policy? Our attempts to generalize existing proof techniques to show convergence in such cases have
fallen behind. Speciﬁcally  in the online case  Lemma 5.4 in [10] does not hold with multiple-step
greedy policies. Similar issues arise when trying to form a κ-CPI algorithm via  e.g.  an attempt to
generalize Corollary 4.2 in [9]. Another research question regards the choice of the parameter κ given
the tradeoff it poses. One possible direction for answering it could be investigating the concentrability
coefﬁcients further and attempting to characterize them for speciﬁc MDPs  either theoretically or via
estimation. Lastly  a next indisputable step would be to empirically evaluate implementations of the
algorithms presented here.

8

Acknowledgments

This work was partially funded by the Israel Science Foundation under contract 1380/16.

References
[1] J Andrew Bagnell  Sham M Kakade  Jeff G Schneider  and Andrew Y Ng. Policy search by
dynamic programming. In Advances in neural information processing systems  pages 831–838 
2004.

[2] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In
Decision and Control  1995.  Proceedings of the 34th IEEE Conference on  volume 1  pages
560–564. IEEE  1995.

[3] Bruno Bouzy and Bernard Helmstetter. Monte-carlo go developments. In Advances in computer

games  pages 159–174. Springer  2004.

[4] Cameron B Browne  Edward Powley  Daniel Whitehouse  Simon M Lucas  Peter I Cowling 
Philipp Rohlfshagen  Stephen Tavener  Diego Perez  Spyridon Samothrakis  and Simon Colton.
A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence
and AI in games  4(1):1–43  2012.

[5] Yonathan Efroni  Gal Dalal  Bruno Scherrer  and Shie Mannor. Beyond the one step greedy

approach in reinforcement learning. arXiv preprint arXiv:1802.03654  2018.

[6] Damien Ernst  Mevludin Glavic  Florin Capitanescu  and Louis Wehenkel. Reinforcement
learning versus model predictive control: a comparison on a power system problem. IEEE
Transactions on Systems  Man  and Cybernetics  Part B (Cybernetics)  39(2):517–529  2009.

[7] Amir-massoud Farahmand  Csaba Szepesvári  and Rémi Munos. Error propagation for approxi-
mate policy and value iteration. In Advances in Neural Information Processing Systems  pages
568–576  2010.

[8] Nan Jiang  Alex Kulesza  Satinder Singh  and Richard Lewis. The dependence of effective
planning horizon on model accuracy. In Proceedings of the 2015 International Conference on
Autonomous Agents and Multiagent Systems  pages 1181–1189. International Foundation for
Autonomous Agents and Multiagent Systems  2015.

[9] S.M. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning.

In International Conference on Machine Learning  pages 267–274  2002.

[10] Vijaymohan R Konda and Vivek S Borkar. Actor-critic–type learning algorithms for markov

decision processes. SIAM Journal on control and Optimization  38(1):94–123  1999.

[11] Alessandro Lazaric  Mohammad Ghavamzadeh  and Rémi Munos. Analysis of classiﬁcation-
based policy iteration algorithms. The Journal of Machine Learning Research  17(1):583–612 
2016.

[12] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap 
Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep rein-
forcement learning. In International Conference on Machine Learning  pages 1928–1937 
2016.

[13] Rémi Munos. Error bounds for approximate policy iteration. In Proceedings of the Twentieth
International Conference on International Conference on Machine Learning  pages 560–567.
AAAI Press  2003.

[14] Rémi Munos. Performance bounds in l_p-norm for approximate value iteration. SIAM journal

on control and optimization  46(2):541–561  2007.

[15] Rudy R Negenborn  Bart De Schutter  Marco A Wiering  and Hans Hellendoorn. Learning-
based model predictive control for markov decision processes. IFAC Proceedings Volumes 
38(1):354–359  2005.

[16] Steven Perkins and David S Leslie. Asynchronous stochastic approximation with differential

inclusions. Stochastic Systems  2(2):409–446  2013.

[17] Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower
discount factor. In Advances in neural information processing systems  pages 1265–1272  2009.

9

[18] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  1994.

[19] Bruno Scherrer. Approximate policy iteration schemes: a comparison.

Conference on Machine Learning  pages 1314–1322  2014.

In International

[20] Bruno Scherrer. Improved and Generalized Upper Bounds on the Complexity of Policy Iteration.
INFORMS  February 2016. Markov decision processes ; Dynamic Programming ; Analysis of
Algorithms.

[21] Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative
policy iteration as boosted policy search. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases  pages 35–50. Springer  2014.

[22] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897 
2015.

[23] Brian Sheppard. World-championship-caliber scrabble. Artiﬁcial Intelligence  134(1-2):241–

275  2002.

[24] David Silver  Thomas Hubert  Julian Schrittwieser  Ioannis Antonoglou  Matthew Lai  Arthur
Guez  Marc Lanctot  Laurent Sifre  Dharshan Kumaran  Thore Graepel  et al. Mastering
chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint
arXiv:1712.01815  2017.

[25] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  et al. Mastering the game of
go without human knowledge. Nature  550(7676):354  2017.

[26] Alexander L Strehl  Lihong Li  and Michael L Littman. Reinforcement learning in ﬁnite mdps:

Pac analysis. Journal of Machine Learning Research  10(Nov):2413–2444  2009.

[27] Aviv Tamar  Garrett Thomas  Tianhao Zhang  Sergey Levine  and Pieter Abbeel. Learning from
the hindsight plan—episodic mpc improvement. In Robotics and Automation (ICRA)  2017
IEEE International Conference on  pages 336–343. IEEE  2017.

[28] Gerald Tesauro and Gregory R Galperin. On-line policy improvement using monte-carlo search.

In Advances in Neural Information Processing Systems  pages 1068–1074  1997.

[29] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292 

1992.

10

,Maria-Florina Balcan
Hongyang Zhang
Yonathan Efroni
Gal Dalal
Bruno Scherrer
Shie Mannor