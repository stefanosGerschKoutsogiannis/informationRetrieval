2013,Stochastic Convex Optimization with Multiple  Objectives,In this paper  we are interested in the development of efficient algorithms  for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information.   We  cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds.  We first examine  a two stages exploration-exploitation based algorithm which first approximates  the stochastic objectives by sampling  and  then solves a constrained stochastic optimization problem by projected gradient method. This method  attains a  suboptimal  convergence rate  even under  strong assumption on the objectives. Our second approach is an efficient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method in constrained optimization and attains  the optimal convergence rate of $[O(1/ \sqrt{T})]$ in high probability for general Lipschitz continuous objectives.,Stochastic Convex Optimization with

Multiple Objectives

Mehrdad Mahdavi

Michigan State University
mahdavim@cse.msu.edu

Tianbao Yang

Rong Jin

NEC Labs America  Inc
tyang@nec-labs.com

Michigan State University
rongjin@cse.msu.edu

Abstract

In this paper  we are interested in the development of efﬁcient algorithms for con-
vex optimization problems in the simultaneous presence of multiple objectives
and stochasticity in the ﬁrst-order information. We cast the stochastic multi-
ple objective optimization problem into a constrained optimization problem by
choosing one function as the objective and try to bound other objectives by appro-
priate thresholds. We ﬁrst examine a two stages exploration-exploitation based
algorithm which ﬁrst approximates the stochastic objectives by sampling and
then solves a constrained stochastic optimization problem by projected gradient
method. This method attains a suboptimal convergence rate even under strong
assumption on the objectives. Our second approach is an efﬁcient primal-dual
p
stochastic algorithm.
It leverages on the theory of Lagrangian method in con-
T ) in
strained optimization and attains the optimal convergence rate of O(1=
high probability for general Lipschitz continuous objectives.

1 Introduction

Although both stochastic optimization [17  4  18  10  26  20  22] and multiple objective optimiza-
tion [9] are well studied subjects in Operational Research and Machine Learning [11  12  24]  much
less is developed for stochastic multiple objective optimization  which is the focus of this work.
Unlike multiple objective optimization where we have access to the complete objective functions  in
stochastic multiple objective optimization  only stochastic samples of objective functions are avail-
able for optimization. Compared to the standard setup of stochastic optimization  the fundamental
challenge of stochastic multiple objective optimization is how to make appropriate tradeoff between
different objectives given that we only have access to stochastic oracles for different objectives. In
particular  an algorithm for this setting has to ponder conﬂicting objective functions and accommo-
date the uncertainty in the objectives.
A simple approach toward stochastic multiple objective optimization is to linearly combine multiple
objectives with a ﬁxed weight assigned to each objective. It converts stochastic multiple objective
optimization into a standard stochastic optimization problem  and is guaranteed to produce Pareto
efﬁcient solutions. The main difﬁculty with this approach is how to decide an appropriate weight for
each objective  which is particularly challenging when the complete objective functions are unavail-
able. In this work  we consider an alternative formulation that casts multiple objective optimization
into a constrained optimization problem. More speciﬁcally  we choose one of the objectives as the
target to be optimized  and use the rest of the objectives as constraints in order to ensure that each
of these objectives is below a speciﬁed level. Our assumption is that although full objective func-
tions are unknown  their desirable levels can be provied due to the prior knowledge of the domain.
Below  we provide a few examples that demonstrate the application of stochastic multiple objective
∑
optimization in the form of stochastic constrained optimization.
Robust Investment. Let r 2 Rn denote random returns of the n risky assets  and w 2 W (cid:17)
fw 2 Rn
i wi = 1g denote the distribution of an investor’s wealth over all assets. The
return for an investment distribution is deﬁned as ⟨w; r⟩. The investor needs to consider conﬂicting
objectives such as rate of return  liquidity and risk in maximizing his wealth [2]. Suppose that r has
a unknown probability distribution with mean vector (cid:22) and covariance matrix (cid:6). Then the target

+ :

n

1

of the investor is to choose an optimal portfolio w that lies on the mean-risk efﬁcient frontier. In
mean-variance theory [15]  which trades off between the expected return (mean) and risk (variance)
of a portfolio  one is interested in minimizing the variance subject to budget constraints which leads
to a formulation like:

[⟨

⟩]

∑

min
w2Rn
+;

n
i wi=1

⊤

w; E[rr

]w

subject to E[⟨r; w⟩] (cid:21) (cid:13):

min

w

min
w2W

Neyman-Pearson Classiﬁcation.
In the Neyman-Pearson (NP) classiﬁcation paradigm (see
e.g  [19])  the goal is to learn a classiﬁer from labeled training data such that the probability of
a false negative is minimized while the probability of a false positive is below a user-speciﬁed level
(cid:13) 2 (0; 1). Let hypothesis class be a parametrized convex set W = fw 7! ⟨w; x⟩ : w 2 Rd;∥w∥ (cid:20)
Rg and for all (x; y) 2 (cid:4) (cid:17) Rd (cid:2) f(cid:0)1; +1g the loss function ℓ : W (cid:2) (cid:4) 7! R+ be a non-negative
convex function. While the goal of classical binary classiﬁcation problem is to minimize the risk as
minw2W [L(w) = E [ℓ(w; (x; y))]]  the Neyman-Pearson targets on
(w) (cid:20) (cid:13);

subject to L(cid:0)

L+(w)

+ : A((cid:24))w (cid:20) b((cid:24))g;

(w) = E[ℓ(w; (x; y))jy = (cid:0)1].

where L+(w) = E[ℓ(w; (x; y))jy = +1] and L(cid:0)
Linear Optimization with Stochastic Constraints.
In many applications in economics  most
notably in welfare and utility theory  and management parameters are known only stochastically
and it is unreasonable to assume that the objective functions and the solution domain are determin-
istically ﬁxed. These situations involve the challenging task of pondering both conﬂicting goals
and random data concerning the uncertain parameters of the problem. Mathematically  the goal in
multi-objective linear programming with stochastic information is to solve:
subject to w 2 W = fw 2 Rd

[⟨c1((cid:24)); w⟩ ;(cid:1)(cid:1)(cid:1) ;⟨cK((cid:24)); w⟩]

where (cid:24) is the randomness in the parameters  ci; i 2 [K] are the objective functions  and A and b
formulate the stochastic constraints on the solution where randomness is captured by (cid:24).
In this paper  we ﬁrst examine two methods that try to eliminate the multi-objective aspect or the
stochastic nature of stochastic multiple objective optimization and reduce the problem to a standard
convex optimization problem. We show that both methods fail to tackle the problem of stochastic
multiple objective optimization in general and require strong assumptions on the stochastic objec-
tives  which limits their applications to real world problems. Having discussed these negative results 
p
we propose an algorithm that can solve the problem optimally and efﬁciently. We achieve this by
T ) con-
an efﬁcient primal-dual stochastic gradient descent method that is able to attain an O(1=
vergence rate for all the objectives under the standard assumption of the Lipschitz continuity of
objectives which is known to be optimal (see for instance [3]). We note that there is a ﬂurry of re-
search on heuristics-based methods to address the multi-objective stochastic optimization problem
(see e.g.  [8] and [1] for a recent survey on existing methods). However  in contrast to this study 
most of these approaches do not have theoretical guarantees.
Finally  we would like to distinguish our work from robust optimization [5] and online learning
with long term constraint [13]. Robust optimization was designed to deal with uncertainty within
the optimization systems. Although it provides a principled framework for dealing with stochastic
constraints  it often ends up with non-convex optimization problems that are not computationally
tractable. Online learning with long term constraint generalizes online learning. Instead of requiring
the constraints to be satisﬁed by every solution generated by online learning  it allows the constraints
to be satisﬁed by the entire sequence of solutions. However  unlike stochastic multiple objective
optimization  in online learning with long term constraints  constraint functions are ﬁxed and known
before the start of online learning.
Outline. The remainder of the paper is organized as follows. In Section 2 we establish the necessary
notation and introduce the problem under consideration. Section 3 introduces the problem reduction
methods and elaborates their disadvantages. Section 4 presents our efﬁcient primal-dual stochastic
optimization algorithm. Finally  we conclude the paper with open questions in Section 5.
2 Preliminaries
Notation Throughout this paper  we use the following notation. We use bold-face letters to denote
}
{
′⟩ where W (cid:18) Rd
′ 2 W by ⟨w; w
vectors. We denote the inner product between two vectors w; w
is a compact closed domain. For m 2 N  we denote by [m] the set f1; 2;(cid:1)(cid:1)(cid:1) ; mg. We only consider
w 2 Rd : ∥w∥ (cid:20) R
the ℓ2 norm throughout the paper. The ball with radius R is denoted by B =
.
Statement of the Problem In this work  we generalize online stochastic convex optimization to
the case of multiple objectives.
In particular  at each iteration  the learner is asked to present a

2

solution wt  which will be evaluated by multiple loss functions f 0
t (w). A fun-
damental difference between single- and multi-objective optimization is that for the latter it is not
obvious how to evaluate the optimization quality. Since it is impossible to simultaneously min-
imize multiple loss functions and in order to avoid complications caused by handling more than
one objective  we choose one function as the objective and try to bound other objectives by ap-
propriate thresholds. Speciﬁcally  the goal of OCO with multiple objectives becomes to minimize
t (wt) and at the same time keep the other objective functions below a given threshold  i.e.

t (w); : : : ; f m

t (w); f 1

∑

T
t=1 f 0

T∑

t=1

1
T

t (wt) (cid:20) (cid:13)i; i 2 [m];
f i

where w1; : : : ; wT are the solutions generated by the online learner and (cid:13)i speciﬁes the level of loss
that is acceptable to the ith objective function. Since the general setup (i.e.  full adversarial setup) is
challenging for online convex optimization even with two objectives [14]  in this work  we consider
t (w); i 2 [m] are i.i.d samples from an unknown
a simple scenario where all the loss functions f i
distribution [21]. We also note that our goal is NOT to ﬁnd a Pareto efﬁcient solution (a solution
is Pareto efﬁcient if it is not dominated by any solution in the decision space). Instead  we aim to
ﬁnd a solution that (i) optimizes one selected objective  and (ii) satisﬁes all the other objectives with
respect to the speciﬁed thresholds.
We denote by (cid:22)f i(w) = Et[f i
t (w)]; i = 0; 1; : : : ; m the expected loss function of sampled function
t (w). In stochastic multiple objective optimization  we assume that we do not have direct access
f i
to the expected loss functions and the only information available to the solver is through a stochastic
{
oracle that returns a stochastic realization of the expected loss function at each call. We assume that
there exists a solution w strictly satisfying all the constraints  i.e. (cid:22)f i(w) < (cid:13)i; i 2 [m]. We denote
Our goal is to efﬁciently compute a solution bwT after T trials that (i) obeys all the constraints  i.e.
by w(cid:3) the optimal solution to multiple objective optimization  i.e. 
(cid:22)f 0(w) : (cid:22)f i(w) (cid:20) (cid:13)i; i 2 [m]
(cid:22)f i(bwT ) (cid:20) (cid:13)i; i 2 [m] and (ii) minimizes the objective (cid:22)f 0 with respect to the optimal solution w(cid:3) 
(1)
i.e. (cid:22)f 0(bwT ) (cid:0) (cid:22)f 0(w(cid:3)). For the convenience of discussion  we refer to f 0

t (w) and (cid:22)f 0(w) as the

t (w) and (cid:22)f i(w) for all i 2 [m] as the constraint functions.

objective function  and to f i
Before discussing the algorithms  we ﬁrst mention a few assumptions made in our analysis. We
assume that the optimal solution w(cid:3) belongs to B. We also make the standard assumption that
all the loss functions  including both the objective function and constraint functions  are Lipschitz
continuous  i.e.  jf i

′∥ for any w; w

)j (cid:20) L∥w (cid:0) w

w(cid:3) = arg min

′ 2 B.

t (w) (cid:0) f i

}

′

t (w

:

3 Problem Reduction and its Limitations

Here we examine two algorithms to cope with the complexity of stochastic optimization with multi-
ple objectives and discuss some negative results which motivate the primal-dual algorithm presented
in Section 4. The ﬁrst method transforms a stochastic multi-objective problem into a stochastic
single-objective optimization problem and then solves the latter problem by any stochastic program-
ming approach. Alternatively  one can eliminate the randomness of the problem by estimating the
stochastic objectives and transform the problem into a deterministic multi-objective problem.

∑

m
i=0 (cid:11)if i

3.1 Linear Scalarization with Stochastic Optimization
A simple approach to solve stochastic optimization problem with multiple objectives is to eliminate
the multi-objective aspect of the problem by aggregating the m + 1 objectives into a single objective
t (wt)  where (cid:11)i; i 2 f0; 1;(cid:1)(cid:1)(cid:1) ; mg is the weight of ith objective  and then solving the
resulting single objective stochastic problem by stochastic optimization methods. This approach
is in general known as the weighted-sum or scalarization method [1]. Although this naive idea
considerably facilitates the computational challenge of the problem  unfortunately  it is difﬁcult to
decide the weight for each objective  such that the speciﬁed levels for different objectives are obeyed.
Beyond the hardness of optimally determining the weight of individual functions  it is also unclear
how to bound the sub-optimality of ﬁnal solution for individual objective functions.

3.2 Projected Gradient Descent with Estimated Objective Functions
The main challenge of the proposed problem is that the expected constraint functions (cid:22)f i(w) are not
given. Instead  only a sampled function is provided at each trial t. Our naive approach is to replace
the expected constraint function (cid:22)f i(w) with its empirical estimation based on sampled objective
functions. This approach circumvents the problem of stochastically optimizing multiple objective

3

into the original online convex optimization with complex projections  and therefore can be solved
by projected gradient descent. More speciﬁcally  at trial t  given the current solution wt and received
loss functions f i

t (w); i = 0; 1; : : : ; m  we ﬁrst estimate the constraint functions as

bf i

t (w) =

t∑
(
t (w) (cid:20) (cid:13)i; i 2 [m]g.

1
t

k=1

k(w); i 2 [m];
f i
wt (cid:0) (cid:17)rf 0

}

and then update the solution by wt+1 = (cid:5)Wt
where (cid:17) > 0 is the step size 
(cid:5)W (w) = minz2W ∥z (cid:0) w∥ projects a solution w into domain W  and Wt is an approximate

t (wt)

domain given by Wt = fw : bf i

One problem with the above approach is that although it is feasible to satisfy all the constraints based
on the true expected constraint functions  there is no guarantee that the approximate domain Wt is
not empty. One way to address this issue is to estimate the expected constraint functions by burning
the ﬁrst bT trials  where b 2 (0; 1) is a constant that needs to be adjusted to obtain the optimal
performance  and keep the estimated constraint functions unchanged afterwards. Given the sampled
functions f i

bT∑
bT received in the ﬁrst bT trials  we compute the approximate domain W′ as

w : bf i(w) (cid:20) (cid:13)i + ^(cid:13)i; i = 1; : : : ; m

bf i(w) =

t (w); i 2 [m]; W′
f i

1; : : : ; f i

{

}

=

1
bT

t=1

where ^(cid:13)i > 0 is a relaxed constant introduced to ensure that with a high probability  the approximate
domain Wt is not empty provided that the original domain W is not empty.
To ensure the correctness of the above approach  we need to establish some kind of uniform (strong)
convergence assumption to make sure that the solutions obtained by projection onto the estimated
domain W′ will be close to the true domain W with high probability. It turns out that the following
assumption ensures the desired property.
obtained by averaging over bT i.i.d samples for (cid:22)f i(w); i 2 [m]. We assume that  with a high
probability 

Assumption 1 (Uniform Convergence). Let bf i(w); i = 0; 1;(cid:1)(cid:1)(cid:1) ; m be the estimated functions

(cid:12)(cid:12)(cid:12)bf i(w) (cid:0) (cid:22)f i(w)

(cid:12)(cid:12)(cid:12) (cid:20) O([bT ]

(cid:0)q); i = 0; 1;(cid:1)(cid:1)(cid:1) ; m:

sup
w2W

∑

where q > 0 decides the convergence rate.
It is straightforward to show that under Assumption 1  with a high probability  for any w 2 W 
we have w 2 W′  with appropriately chosen relaxation constant ^(cid:13)i; i 2 [m]. Using the estimated
domain W′  for trial t 2 [bT + 1; T ]  we update the solution by wt+1 = (cid:5)W′(wt (cid:0) (cid:17)rf 0
t (wt)).
There are however several drawbacks with this naive approach. Since the ﬁrst bT trials are used for
estimating the constraint functions  only the last (1(cid:0)b)T trials are used for searching for the optimal
solution. The total amount of violation of individual constraint functions for the last (1 (cid:0) b)T trials 
√
∑
(cid:22)f i(wt)  is O((1 (cid:0) b)b
(cid:0)qT 1(cid:0)q)  where each of the (1 (cid:0) b)T trials receives a
T
given by
(cid:0)q). Similarly  following the conventional analysis of online learning [26]  we
t=bT +1
violation of O([bT ]
√
t (wt) (cid:0) f 0
t (w(cid:3))) (cid:20) O(
(1 (cid:0) b)T ). Using the same trick as in [13]  to obtain
T
have
t=bT +1(f 0
a solution with zero violation of constraints  we will have a regret bound O((1 (cid:0) b)b
(cid:0)qT 1(cid:0)q +
(1 (cid:0) b)T )  which yields a convergence rate of O(T
(cid:0)q) which could be worse than
(cid:0)1=2) when q < 1=2. Additionally  this approach requires memorizing the
the optimal rate O(T
constraint functions of the ﬁrst bT trials. This is in contrast to the typical assumption of online
learning where only the solution is memorized.
Remark 1. We ﬁnally remark on the uniform convergence assumption  which holds when the con-
straint functions are linear [25]  but unfortunately does not hold for general convex Lipschitz func-
tions.
In particular  one can simply show examples where there is no uniform convergence for
stochastic convex Lipchitz functions in inﬁnite dimensional spaces [21]. Without uniform conver-
gence assumption  the approximate domain W′ may depart from the true W signiﬁcantly at some
unknown point  which makes the above approach to fail for general convex objectives.

(cid:0)1=2 + T

To address these limitations and in particular the dependence on uniform convergence assumption 
we present an algorithm that does not require projection when updating the solution and does not
require to impose any additional assumption on the stochastic functions except for the standard
Lipschitz continuity assumption. We note that our result is closely related to the recent studies of
learning from the viewpoint of optimization [23]  which state that solutions found by stochastic
gradient descent can be statistically consistent even when uniform convergence theorem does not
hold.

4

0 > 0; i 2 [m] and total iterations T

0 ); (cid:21)i

0;(cid:1)(cid:1)(cid:1) ; (cid:21)m

Algorithm 1 Stochastic Primal-Dual Optimization with Multiple Objectives
1: INPUT: step size (cid:17)  (cid:21)0 = ((cid:21)1
2: w1 = (cid:21)1 = 0
3: for t = 1; : : : ; T do
4:
5:
6:
7:

Submit the solution wt
Receive loss functions f i
Compute the gradients rf i
Update the solution w and (cid:21) by
)
wt+1 = (cid:5)B (wt (cid:0) (cid:17)rwLt(wt; (cid:21)t)) = (cid:5)B

[
[
rf 0

(
wt (cid:0) (cid:17)

t (wt); i = 0; 1; : : : ; m

t ; i = 0; 1; : : : ; m

(

(

t + (cid:17)r(cid:21)iLt(wt; (cid:21)t)
(cid:21)i

= (cid:5)[0;(cid:21)i
0]

(cid:21)i
t + (cid:17)

m∑
(cid:21)i
t
t (wt) (cid:0) (cid:13)i
f i

i=1

t (wt) +

])
rf i

:

(cid:21)i
t+1 = (cid:5)[0;(cid:21)i
0]

8: end for
9: Return ^wT =

T
t=1 wt=T

∑

])

;

t (wt)

m∑

m∑

4 An Efﬁcient Stochastic Primal-Dual Algorithm
We now turn to devise a tractable formulation of the problem  followed by an efﬁcient primal-dual
optimization algorithm and the statements of our main results. We show that with a high probabil-
p
ity  the solution found by the proposed algorithm will exactly satisfy the expected constraints and
achieves a regret bound of O(
T ). The main idea of the proposed algorithm is to design an appro-
priate objective that combines the loss function (cid:22)f 0(w) with (cid:22)f i(w); i 2 [m]. As mentioned before 
owing to the presence of conﬂicting goals and the randomness nature of the objective functions  we
resort to seek for a solution that satisﬁes all the objectives instead of an optimal one. To this end  we
deﬁne the following objective function

(cid:22)L(w; (cid:21)) = (cid:22)f 0(w) +

(cid:21)i( (cid:22)f i(w) (cid:0) (cid:13)i):

i=1

⊤ 2 (cid:3)  where (cid:3) (cid:18) Rm

Note that the objective function consists of both the primal variable w 2 W and dual variable
+ is a compact convex set that bounds the set of dual
(cid:21) = ((cid:21)1; : : : ; (cid:21)m)
variables and will be discussed later. In the proposed algorithm  we will simultaneously update
p
solutions for both w and (cid:21). By exploring convex-concave optimization theory [16]  we will show
that with a high probability  the solution of regret O(
As the ﬁrst step  we consider a simple scenario where the obtained solution is allowed to violate the
constraints. The detailed steps of our primal-dual algorithm is presented in Algorithm 1 . It follows
the same procedure as convex-concave optimization. Since at each iteration  we only observed a
randomly sampled loss functions f i

t (w); i = 0; 1; : : : ; m  the objective function given by

T ) exactly obeyes the constraints.

Lt(w; (cid:21)) = f 0

t (w) +

(cid:21)i(f i

t (w) (cid:0) (cid:13)i)

provides an unbiased estimate of (cid:22)L(w; (cid:21)). Given the approximate objective Lt(w; (cid:21))  the pro-
posed algorithm tries to minimize the objective Lt(w; (cid:21)) with respect to the primal variable w and
maximize the objective with respect to the dual variable (cid:21).
To facilitate the analysis  we ﬁrst rewrite the the constrained optimization problem

i=1

{
w : (cid:22)f i(w) (cid:20) (cid:13)i; i = 1; : : : m

w2B\W

(cid:22)f 0(w)

min

}

m∑

where W is deﬁned as W =

in the following equivalent form:

We denote by w(cid:3) and (cid:21)(cid:3) = ((cid:21)1(cid:3); : : : ; (cid:21)m(cid:3) )
convex-concave optimization problem  respectively  i.e. 

w2B max
min
(cid:21)2Rm

+

(cid:22)f 0(w) +

(2)
⊤ as the optimal primal and dual solutions to the above

i=1

(cid:21)i( (cid:22)f i(w) (cid:0) (cid:13)i):
m∑
(cid:21)i(cid:3)( (cid:22)f i(w) (cid:0) (cid:13)i);
m∑

i=1

(cid:21)i( (cid:22)f i(w(cid:3)) (cid:0) (cid:13)i):

w(cid:3) = arg min

w2B

(cid:22)f 0(w) +

(cid:21)(cid:3) = arg max
(cid:21)2Rm

+

(cid:22)f 0(w(cid:3)) +

i=1

5

(3)

(4)

The following assumption establishes upper bound on the gradients of L(w; (cid:21)) with respect to w
and (cid:21). We later show that this assumption holds under a mild condition on the objective functions.
Assumption 2 (Gradient Boundedness). The gradients rwL(w; (cid:21)) and r(cid:21)L(w; (cid:21)) are uniformly
bounded  i.e.  there exist a constant G > 0 such that
max (rwL(w; (cid:21));r(cid:21)L(w; (cid:21))) (cid:20) G;

for any w 2 B and (cid:21) 2 (cid:3):

p
O(1=

Under the preceding assumption 

T ) for both the regret and the violation of the constraints.

in the following theorem  we show that under appropriate

conditions  the average solution bwT generated by of Algorithm 1 attains a convergence rate of
(cid:21) (cid:21)i(cid:3) + (cid:18); i 2 [m]  where (cid:18) > 0 is a constant. Let bwT be the solution obtained
(cid:22)f 0(bwT ) (cid:0) (cid:22)f 0(w(cid:3)) (cid:20) (cid:22)((cid:14))p

Theorem 1. Set (cid:21)i
by Algorithm 1 after T iterations. Then  with a probability 1 (cid:0) (2m + 1)(cid:14)  we have
0
; i 2 [m]

and (cid:22)f i(bwT ) (cid:0) (cid:13)i (cid:20) (cid:22)((cid:14))

where D2 =

m
i=1[(cid:21)i

(R2 + D2)=2T ]=G  and

∑

√

T

√

0]2  (cid:17) = [
p
2G

(cid:22)((cid:14)) =

√

p
T

(cid:18)

1
(cid:14)

:

2 ln

R2 + D2 + 2G(R + D)

(5)
Remark 2. The parameter (cid:18) 2 R+ is a quantity that may be set to obtain sharper upper bound
on the violation of constraints and may be chosen arbitrarily. In particular  a larger value for (cid:18)
imposes larger penalty on the violation of the constraints and results in a smaller violation for the
objectives.

We also can develop an algorithm that allows the solution to exactly satisfy all the constraints. To
′

. We will run Algorithm 1 but with (cid:13)i replaced byb(cid:13)i. Let G
this end  we deﬁneb(cid:13)i = (cid:13)i (cid:0) (cid:22)((cid:14))
denote the upper bound in Assumption 2 for r(cid:21)L(w; (cid:21)) withb(cid:13)i is replaced by (cid:13)i; i 2 [m]. The
following theorem shows the property of the obtained average solution bwT .
Theorem 2. Let bwT be the solution obtained by Algorithm 1 with (cid:13)i replaced by b(cid:13)i and
0 = (cid:21)i(cid:3) + (cid:18); i 2 [m]. Then  with a probability 1 (cid:0) (2m + 1)(cid:14)  we have
(cid:21)i

p

T

(cid:18)

(cid:22)f 0(bwT ) (cid:0) (cid:22)f 0(w(cid:3)) (cid:20) (1 +

∑

p
m
i=1 (cid:21)i

T

((cid:14))

and (cid:22)f i(bwT ) (cid:20) (cid:13)i; i 2 [m];
√

′.
(R2 + D2)=2T ]=G

′
0)(cid:22)
′ and (cid:17) = [

′
where (cid:22)

((cid:14)) is same as (5) with G is replaced by G

4.1 Convergence Analysis

Here we provide the proofs of main theorems stated above. We start by proving Theorem 1 and then
extend it to prove Theorem 2.

+

⟩ (cid:0)⟨

(cid:20) ⟨

Proof. (of Theorem 1) Using the standard analysis of convex-concave optimization  from the con-
vexity of (cid:22)L(w;(cid:1)) with respect to w and concavity of (cid:22)L((cid:1); (cid:21)) with respect to (cid:21)  for any w 2 B and
⟩
0]; i 2 [m]  we have
(cid:21)i 2 [0; (cid:21)i
(cid:22)L(wt; (cid:21)) (cid:0) (cid:22)L(w; (cid:21)t)
wt (cid:0) w;rw (cid:22)L(wt; (cid:21)t)
(cid:21)t (cid:0) (cid:21);r(cid:21) (cid:22)L(wt; (cid:21)t)
⟨
= ⟨wt (cid:0) w;rwLt(wt; (cid:21)t)⟩ (cid:0) ⟨(cid:21)t (cid:0) (cid:21);r(cid:21)Lt(wt; (cid:21)t)⟩
wt (cid:0) w;rw (cid:22)L(wt; (cid:21)t) (cid:0) rwLt(wt; (cid:21)t)
(cid:20) ∥wt (cid:0) w∥2 (cid:0) ∥wt+1 (cid:0) w∥2

⟩ (cid:0)⟨
(∥rwLt(wt; (cid:21)t)∥2 + ∥r(cid:21)Lt(wt; (cid:21)t)∥2
)
⟩ (cid:0)⟨

wt (cid:0) w;rw (cid:22)L(wt; (cid:21)t) (cid:0) rwLt(wt; (cid:21)t)

(cid:21)t (cid:0) (cid:21);r(cid:21) (cid:22)L(wt; (cid:21)t) (cid:0) r(cid:21)Lt(wt; (cid:21)t)
where in the ﬁrst inequality we have added and subtracted the stochastic gradients used for up-
dating the solutions  the last inequality follows from the updating rules for wt+1 and (cid:21)t+1 and
non-expensiveness property of the orthogonal projection operation onto the convex domain.

(cid:21)t (cid:0) (cid:21);r(cid:21) (cid:22)L(wt; (cid:21)t) (cid:0) r(cid:21)Lt(wt; (cid:21)t)

∥(cid:21)t (cid:0) (cid:21)∥2 (cid:0) ∥(cid:21)t+1 (cid:0) (cid:21)∥2

⟩

⟨

⟩

(cid:17)
2

2(cid:17)

2(cid:17)

+

+

+

;

6

T∑

t=1

By adding all the inequalities together  we get

(cid:22)L(wt; (cid:21)) (cid:0) (cid:22)L(w; (cid:21)t)
(cid:20) ∥w (cid:0) w1∥2 + ∥(cid:21) (cid:0) (cid:21)1∥2
T∑

2(cid:17)

⟨

T∑

t=1

+

(cid:17)
2

wt (cid:0) w;rw (cid:22)L(wt; (cid:21)t) (cid:0) rwLt(wt; (cid:21)t)

+ (cid:17)G2T

wt (cid:0) w;rw (cid:22)L(wt; (cid:21)t) (cid:0) rwLt(wt; (cid:21)t)

√

+

t=1

(cid:20) R2 + D2

T∑

⟨

2(cid:17)

+

t=1

(cid:20) R2 + D2

2(cid:17)

∥rwLt(wt; (cid:21)t)∥2 + ∥r(cid:21)Lt(wt; (cid:21)t)∥2

⟩ (cid:0)⟨
⟩ (cid:0)⟨

⟩
(cid:21)t (cid:0) (cid:21);r(cid:21) (cid:22)L(wt; (cid:21)t) (cid:0) r(cid:21)Lt(wt; (cid:21)t)
⟩
(cid:21)t (cid:0) (cid:21);r(cid:21) (cid:22)L(wt; (cid:21)t) (cid:0) r(cid:21)Lt(wt; (cid:21)t)

(w.p. 1 (cid:0) (cid:14));

1
(cid:14)

∑

+ (cid:17)G2T + 2G(R + D)

2T ln

T

T

i=1

i=1

(cid:20)

(6)

R2 + D2

2
T

ln

1
(cid:14)

:

+ 2G(R + D)

p
2G

b(cid:21)i
T ( (cid:22)f i(w) (cid:0) (cid:13)i)

T

t=1 (cid:21)t=T   for any ﬁxed (cid:21)i 2 [0; (cid:21)i

average solutions bwT =
(cid:22)f 0(bwT ) +

By ﬁxing w = w(cid:3) and (cid:21) = 0 in (6)  we have (cid:22)f i(w(cid:3)) (cid:20) (cid:13)i; i 2 [m]  and therefore  with a
probability 1 (cid:0) (cid:14)  have

∑
t=1 wt=T andb(cid:21)T =
where the last inequality follows from the Hoefﬁding inequality for Martingales [6]. By expanding
the left hand side  substituting the stated value of (cid:17)  and applying the Jensen’s inequality for the
0]; i 2 [m]
(cid:21)i( (cid:22)f i(bwT ) (cid:0) (cid:13)i) (cid:0) (cid:22)f 0(w) (cid:0) m∑
m∑
and w 2 B  with a probability 1 (cid:0) (cid:14)  we have
√
√
√
√
(cid:21)j(cid:3)( (cid:22)f j(bwT ) (cid:0) (cid:13)j) (cid:0) (cid:22)f 0(w(cid:3)) (cid:0) m∑
(cid:21)j(cid:3)( (cid:22)f j(bwT ) (cid:0) (cid:13)j) (cid:0) (cid:22)f 0(w(cid:3)) (cid:0) m∑

(cid:22)f 0(bwT ) (cid:20) (cid:22)f 0(w(cid:3)) +
0( (cid:22)f i(bwT ) (cid:0) (cid:13)i) +
0( (cid:22)f i(bwT ) (cid:0) (cid:13)i) +
√
where the ﬁrst inequality utilizes (4) and the second inequality utilizes (3).
We thus have  with a probability 1 (cid:0) (cid:14) 

0; i 2 [m]  and (cid:21)j = (cid:21)j(cid:3); j ̸= i in (6).
b(cid:21)i
T ( (cid:22)f i(w(cid:3)) (cid:0) (cid:13)i)

(cid:22)f 0(bwT ) + (cid:21)i
(cid:21) (cid:22)f 0(bwT ) + (cid:21)i
(cid:21) (cid:18)( (cid:22)f i(bwT ) (cid:0) (cid:13)i);
(cid:22)f i(bwT ) (cid:0) (cid:13)i (cid:20)

To bound the violation of constraints we set w = w(cid:3)  (cid:21)i = (cid:21)i
We have

(cid:21)i(cid:3)( (cid:22)f i(w(cid:3)) (cid:0) (cid:13)i)

∑
∑

j̸=i

; i 2 [m]:

p
2G

+ 2G(R + D)

2
T

ln

1
(cid:14)

:

2G(R + D)

R2 + D2

R2 + D2

√

j̸=i

i=1

i=1

T

+

T

(cid:18)

2
T

ln

1
(cid:14)

p

2G
(cid:18)

We complete the proof by taking the union bound over all the random events.

We now turn to the proof of Theorem 2 that gives high probability bound on the convergence of the
modiﬁed algorithm which obeys all the constraints.

Proof. (of Theorem 2) Following the proof of Theorem 1  with a probability 1 (cid:0) (cid:14)  we have

(cid:22)f 0(bwT ) +

m∑

(cid:21)i( (cid:22)f i(bwT ) (cid:0)b(cid:13)i) (cid:0) (cid:22)f 0(w) (cid:0) m∑

b(cid:21)i
T ( (cid:22)f i(w) (cid:0)b(cid:13)i)

√

2
T

ln

1
(cid:14)

i=1
′

+ 2G

(R + D)

√

i=1

p
2G

′

(cid:20)

R2 + D2

T

7

Deﬁne ew(cid:3) ande(cid:21)(cid:3) be the saddle point for the following minimax optimization problem
(cid:21)i( (cid:22)f i(w) (cid:0)b(cid:13)i)
Following the same analysis as Theorem 1  for each i 2 [m]  by setting w = ew(cid:3)  (cid:21)i = (cid:21)i
(cid:21)j =e(cid:21)j(cid:3)  using the fact thate(cid:21)j(cid:3) (cid:20) (cid:21)j(cid:3)  we have  with a probability 1 (cid:0) (cid:14)
√

w2B max
min
(cid:21)2Rm

m∑

(cid:22)f 0(w) +

√

i=1

+

0  and

(cid:18)( (cid:22)f i(bwT ) (cid:0) (cid:13)i) (cid:20)

p

′
2G

R2 + D2

T

′
+ 2G

(R + D)

2
T

ln

1
(cid:14)

(cid:0) (cid:22)((cid:14))p
T

(cid:20) 0;

which completes the proof.

4.2

Implementation Issues

0; i 2 [m]  which requires to decide
In order to run Algorithm 1  we need to estimate the parameter (cid:21)i
the set (cid:3) by estimating an upper bound for the optimal dual variables (cid:21)i(cid:3); i 2 [m]. To this end  we
consider an alternative problem to the convex-concave optimization problem in (2)  i.e.

w2B max
min
(cid:21)(cid:21)0

(cid:22)f 0(w) + (cid:21) max
1(cid:20)i(cid:20)m

( (cid:22)f i(w) (cid:0) (cid:13)i):

(7)

Evidently w(cid:3) is the optimal primal solution to (7). Let (cid:21)a be the optimal dual solution to the problem
in (7). We have the following proposition that links (cid:21)i(cid:3); i 2 [m]  the optimal dual solution to (2) 
with (cid:21)a  the optimal dual solution to (7).
Proposition 1. Let (cid:21)a be the optimal dual solution to (7) and (cid:21)i(cid:3); i 2 [m] be the optimal solution to
(2). We have (cid:21)a =

∑

m

i=1 (cid:21)i(cid:3).

∑

m

m

∑

(cid:22)f 0(w) +

w2B max

i=1 (cid:21)i as claimed.

i=1 pi(cid:21)( (cid:22)f i(w)(cid:0) (cid:13)i)  where domain ∆m is
(cid:21)(cid:21)0;p2∆m
i=1 (cid:11)i = 1g. By redeﬁning (cid:21)i = pi(cid:21)  we have the problem in (7)
(cid:13)(cid:13)∑

Proof. We can rewrite (7) as min
deﬁned as ∆m = f(cid:11) 2 Rm
+ :
equivalent to (2) and consequently (cid:21) =
Given the result from Proposition 1  it is sufﬁcient to bound (cid:21)a. In order to bound (cid:21)a  we need to
make certain assumption about (cid:22)f i(w); i 2 [m]. The purpose of introducing this assumption is to
ensure that the optimal dual variable is well bounded from the above.
Assumption 3. We assume min
(cid:11)2∆m
Equipped with Assumption 3  we are able to bound (cid:21)a by (cid:28). To this end  using the ﬁrst order optimal-
ity condition of (2) [7]  we have (cid:21)a = ∥r (cid:22)f 0(w(cid:3))∥=∥@g(w)∥  where g(w) = max1(cid:20)i(cid:20)m
(cid:22)f i(w).
(cid:28) . By com-
∑
bining Proposition 1 with the upper bound on (cid:21)a  we obtain (cid:21)i(cid:3) (cid:20) L
Finally  we note that by having (cid:21)(cid:3) bounded  Assumption 2 is guaranteed by setting G2 =
i=1( (cid:22)f i(w) (cid:0) (cid:13)i)2) which follows from Lipschitz continuity of
max(L2
the objective functions. In a similar way we can set G

  under Assumption 3  we have (cid:21)a (cid:20) L
(cid:28) ; i 2 [m] as desired.
′ in Theorem 2 by replacing (cid:13)i withb(cid:13)i.

(cid:13)(cid:13) (cid:21) (cid:28)  where (cid:28) > 0 is a constant.

Since @g(w) 2{∑
∑

i=1 (cid:11)ir (cid:22)f i(w) : (cid:11) 2 ∆m
)2

i=1 (cid:11)ir (cid:22)f i(w)

; max
w2B

m
i=1 (cid:21)i
0

}

(

1 +

m

m

m

∑

m

5 Conclusions and Open Questions

In this paper we have addressed the problem of stochastic convex optimization with multiple ob-
jectives underlying many applications in machine learning. We ﬁrst examined a simple problem
reduction technique that eliminates the stochastic aspect of constraint functions by approximating
them using the sampled functions from each iteration. We showed that this simple idea fails to attain
the optimal convergence rate and requires to impose a strong assumption  i.e.  uniform convergence 
p
on the objective functions. Then  we presented a novel efﬁcient primal-dual algorithm which attains
the optimal convergence rate O(1=
T ) for all the objectives relying only on the Lipschitz continu-
ity of the objective functions. This work leaves few direction for further elaboration. In particular  it
would be interesting to see whether or not making stronger assumptions on the analytical properties
of objective functions such as smoothness or strong convexity may yield improved convergence rate.
Acknowledgments. The authors would like to thank the anonymous reviewers for their helpful and insight-
ful comments. The work of M. Mahdavi and R. Jin was supported in part by ONR Award N000141210431 and
NSF (IIS-1251031).

8

References
[1] F. B. Abdelaziz. Solution approaches for the multiobjective stochastic programming. European

Journal of Operational Research  216(1):1–16  2012.

[2] F. B. Abdelaziz  B. Aouni  and R. E. Fayedh. Multi-objective stochastic programming for

portfolio selection. European Journal of Operational Research  177(3):1811–1823  2007.

[3] A. Agarwal  P. L. Bartlett  P. D. Ravikumar  and M. J. Wainwright.

Information-theoretic
lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions
on Information Theory  58(5):3235–3249  2012.

[4] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for

machine learning. In NIPS  pages 451–459  2011.

[5] A. Ben-Tal  L. El Ghaoui  and A. Nemirovski. Robust optimization. Princeton University

Press  2009.

[6] S. Boucheron  G. Lugosi  and O. Bousquet. Concentration inequalities. In Advanced Lectures

on Machine Learning  pages 208–240  2003.

[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[8] R. Caballero  E. Cerd´a  M. del Mar Mu˜noz  and L. Rey. Stochastic approach versus multi-
objective approach for obtaining efﬁcient solutions in stochastic multiobjective programming
problems. European Journal of Operational Research  158(3):633–648  2004.

[9] M. Ehrgott. Multicriteria optimization. Springer  2005.
[10] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. Journal of Machine Learning Research - Proceedings
Track  19:421–436  2011.

[11] K.-J. Hsiao  K. S. Xu  J. Calder  and A. O. H. III. Multi-criteria anomaly detection using pareto

depth analysis. In NIPS  pages 854–862  2012.

[12] Y. Jin and B. Sendhoff. Pareto-based multiobjective machine learning: An overview and case
studies. IEEE Transactions on Systems  Man  and Cybernetics  Part C  38(3):397–415  2008.
[13] M. Mahdavi  R. Jin  and T. Yang. Trading regret for efﬁciency: online convex optimization

with long term constraints. JMLR  13:2465–2490  2012.

[14] S. Mannor  J. N. Tsitsiklis  and J. Y. Yu. Online learning with sample path constraints. Journal

of Machine Learning Research  10:569–590  2009.

[15] H. Markowitz. Portfolio selection. The journal of ﬁnance  7(1):77–91  1952.
[16] A. Nemirovski. Efﬁcient methods in convex programming. Lecture Notes  Available at

http://www2.isye.gatech.edu/ nemirovs  1994.

[17] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM J. on Optimization  19:1574–1609  2009.

[18] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex

stochastic optimization. In ICML  2012.

[19] P. Rigollet and X. Tong. Neyman-pearson classiﬁcation  convexity and stochastic constraints.

The Journal of Machine Learning Research  12:2831–2855  2011.

[20] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2):107–194  2012.

[21] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Stochastic convex optimization.

In COLT  2009.

[22] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for svm. In ICML  pages 807–814  2007.

[23] K. Sridharan. Learning from an optimization viewpoint. PhD Thesis  2012.
[24] K. M. Svore  M. N. Volkovs  and C. J. Burges. Learning to rank with multiple objective

functions. In WWW  pages 367–376. ACM  2011.

[25] H. Xu and F. Meng. Convergence analysis of sample average approximation methods for a
class of stochastic mathematical programs with equality constraints. Mathematics of Opera-
tions Research  32(3):648–668  2007.

[26] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

ICML  pages 928–936  2003.

9

,Mehrdad Mahdavi
Tianbao Yang
Rong Jin
Nicki Skafte
Søren Hauberg