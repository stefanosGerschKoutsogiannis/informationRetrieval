2010,Efficient Minimization of Decomposable Submodular Functions,Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions  and can be minimized in strongly polynomial time. Unfortunately  state-of-the-art algorithms for general submodular minimization are intractable for practical problems. In this paper  we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions. We develop an algorithm  SLG  that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task  and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.,Efï¬cient Minimization of

Decomposable Submodular Functions

Peter Stobbe

California Institute of Technology

Pasadena  CA 91125

stobbe@caltech.edu

Andreas Krause

California Institute of Technology

Pasadena  CA 91125

krausea@caltech.edu

Abstract

Many combinatorial problems arising in machine learning can be reduced to the problem
of minimizing a submodular function. Submodular functions are a natural discrete analog
of convex functions  and can be minimized in strongly polynomial time. Unfortunately 
state-of-the-art algorithms for general submodular minimization are intractable for larger
problems.
In this paper  we introduce a novel subclass of submodular minimization
problems that we call decomposable. Decomposable submodular functions are those
that can be represented as sums of concave functions applied to modular functions. We
develop an algorithm  SLG  that can efï¬ciently minimize decomposable submodular
functions with tens of thousands of variables. Our algorithm exploits recent results in
smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint
classiï¬cation-and-segmentation task  and show that it outperforms the state-of-the-art
general purpose submodular minimization algorithms by several orders of magnitude.

Introduction

1
Convex optimization has become a key tool in many machine learning algorithms. Many seemingly
multimodal optimization problems such as nonlinear classiï¬cation  clustering and dimensionality
reduction can be cast as convex programs. When minimizing a convex loss function  we can rest
assured to efï¬ciently ï¬nd an optimal solution  even for large problems. Convex optimization is a
structural property of continuous optimization problems. However  many machine learning prob-
lems  such as structure learning  variable selection  MAP inference in discrete graphical models 
require solving discrete  combinatorial optimization problems.
In recent years  another fundamental problem structure  which has similar beneï¬cial properties 
has emerged as very useful in many combinatorial optimization problems arising in machine learn-
ing: Submodularity is an intuitive diminishing returns property  stating that adding an element to a
smaller set helps more than adding it to a larger set. Similarly to convexity  submodularity allows
one to efï¬ciently ï¬nd provably (near-)optimal solutions. In particular  the minimum of a submodular
function can be found in strongly polynomial time [11]. Unfortunately  while polynomial-time solv-
able  exact techniques for submodular minimization require a number of function evaluations on the
order of n5 [12]  where n is the number of variables in the problem (e.g.  number of random variables
in the MAP inference task)  rendering the algorithms impractical for many real-world problems.
Fortunately  several submodular minimization problems arising in machine learning have structure
that allows solving them more efï¬ciently. Examples include symmetric functions that can be
solved in O(n3) evaluations using Queyranneâ€™s algorithm [19]  and functions that decompose into
attractive  pairwise potentials  that can be solved using graph cutting techniques [7]. In this paper 
we introduce a novel class of submodular minimization problems that can be solved efï¬ciently. In
particular  we develop an algorithm SLG  that can minimize a class of submodular functions that
we call decomposable: These are functions that can be decomposed into sums of concave functions
applied to modular (additive) functions. Our algorithm is based on recent techniques of smoothed
convex minimization [18] applied to the LovÂ´asz extension. We demonstrate the usefulness of

1

our algorithm on a joint classiï¬cation-and-segmentation task involving tens of thousands of
variables  and show that it outperforms state-of-the-art algorithms for general submodular function
minimization by several orders of magnitude.

2 Background on Submodular Function Minimization
We are interested in minimizing set functions that map subsets of some base set E to real numbers.
I.e.  given f : 2E ! R we wish to solve for A 2 arg minA f (A). For simplicity of notation  we
use the base set E = f1; : : : ng  but in an application the base set may consist of nodes of a graph 
pixels of an image  etc. Without loss of generality  we assume f (;) = 0. If the function f has no
structure  then there is no way solve the problem other than checking all 2n subsets. In this paper 
we consider functions that satisfy a key property that arises in many applications: submodularity
(c.f.  [16]). A set function f is called submodular iff  for all A; B 2 2E  we have

f (A [ B) + f (A \ B)  f (A) + f (B):

(1)
Submodular functions can alternatively  and perhaps more intuitively  be characterized in terms of
their discrete derivatives. First  we deï¬ne kf (A) = f (A[fkg)f (A) to be the discrete derivative
of f with respect to k 2 E at A; intuitively this is the change in fâ€™s value by adding the element k
to the set A. Then  f is submodular iff:

kf (A)  kf (B); for all A  B  E and k 2 E n B:

Note the analogy to concave functions; the discrete derivative is smaller for larger sets  in the same
way that (x+h)(x)  (y+h)(y) for all x  y; h  0 if and only if  is a concave function
on R. Thus a simple example of a submodular function is f (A) = (jAj) where  is any concave
function. Yet despite this connection to concavity  it is in fact â€˜easierâ€™ to minimize a submodular
function than to maximize it1  just as it is easier to minimize a convex function. One explanation for
this is that submodular minimization can be reformulated as a convex minimization problem.
To see this  consider taking a set function minimization problem  and reformulating it as a mini-
mization problem over the unit cube [0; 1]n  Rn. Deï¬ne eA 2 Rn to be the indicator vector of the
set A  i.e. 

eA[k] = 0 if k =2 A

1 if k 2 A

We use the notation x[k] for the kth element of the vector x. Also we drop brackets and commas
in subscripts  so ekl = efk;lg and ek = efkg as with the standard unit vectors. A continuous
extension of a set function f is a function ~f on the unit cube ~f : [0; 1]n ! R with the property
that f (A) = ~f (eA). In order to be useful  however  one needs the minima of the set function to be
related to minima of the extension:

A 2 arg min

A22E

f (A) ) eA 2 arg min
x2[0;1]n

~f (x):

(2)

A key result due to LovÂ´asz [16] states that each submodular function f has an extension ~f that not
only satisï¬es the above property  but is also convex and efï¬cient to evaluate. We can deï¬ne the
LovÂ´asz extension in terms of the submodular polyhedron Pf :

Pf = fv 2 Rn : v  eA  f (A); for all A 2 2Eg;

~f (x) = sup
v2Pf

v  x:

The submodular polyhedron Pf is deï¬ned by exponentially many inequalities  and evaluating ~f
requires solving a linear program over this polyhedron. Perhaps surprisingly  as shown by LovÂ´asz  ~f
can be very efï¬ciently computed as follows. For a ï¬xed x let  : E ! E be a permutation such that
x[(1)]  : : :  x[(n)]  and then deï¬ne the set Sk = f(1); : : : ; (k)g. Then we have a formula
for ~f and a subgradient:

~f (x) =

n

Xk=1

x[(k)](f (Sk)  f (Sk1));

@ ~f (x) 3

n

Xk=1

e(k)(f (Sk)  f (Sk1)):

Note that if two components of x are equal  the above formula for ~f is independent of the permuta-
tion chosen  but the subgradient is not unique.

1With the additional assumption that f is nondecreasing  maximizing a submodular function subject to a

cardinality constraint jAj  M is â€˜easyâ€™; a greedy algorithm is known to give a near-optimal answer [17].

2

Equation (2) was used to show that submodular minimization can be achieved in polynomial time
[16]. However  algorithms which directly minimize the Lovasz extension are regarded as imprac-
tical. Despite being convex  the LovÂ´asz extension is non-smooth  and hence a simple subgradient
descent algorithm would need O(1=2) steps to achieve O() accuracy.
Recently  Nesterov showed that if knowledge about the structure of a particular non-smooth convex
function is available  it can be exploited to achieve a running time of O(1=) [18]. One way this is
done is to construct a smooth approximation of the non-smooth function  and then use an accelerated
gradient descent algorithm which is highly effective for smooth functions. Connections of this work
with submodularity and combinatorial optimization are also explored in [4] and [2].
In fact  in
[2]  Bach shows that computing the smoothed LovÂ´asz gradient of a general submodular function is
equivalent to solving a submodular minimization problem. In this paper  we do not treat general
submodular functions  but rather a large class of submodular minimization functions that we call
decomposable. (To apply the smoothing technique of [18]  special structural knowledge about the
convex function is required  so it is natural that we would need special structural knowledge about
the submodular function to leverage those results.) We further show that we can exploit the discrete
structure of submodular minimization in a way that allows terminating the algorithm early with a
certiï¬cate of optimality  which leads to drastic performance improvements.

3 The Decomposable Submodular Minimization Problem

In this paper  we consider the problem of minimizing functions of the following form:

f (A) = c  eA +Xj

j(wj  eA);

(3)

where c; wj 2 Rn and 0  wj  1 and j : [0; wj  1] ! R are arbitrary concave functions. It can
be shown that functions of this form are submodular. We call this class of functions decomposable
submodular functions  as they decompose into a sum of concave functions applied to nonnegative
modular functions2. Below  we give examples of decomposable submodular functions arising in
applications.
We ï¬rst focus on the special case where all the concave functions are of the form j() =
dj min(yj; ) for some yj; dj > 0. Since these potentials are of key importance  we deï¬ne the
submodular functions 	w;y(A) = min(y; w  eA) and call them threshold potentials. In Section 5 
we will show in how to generalize our approach to arbitrary decomposable submodular functions.

Examples. The simplest example is a 2-potential  which has the form (jA\fk; lgj)  where (1)
(0)  (1)  (2). It can be expressed as a sum of a modular function and a threshold potential:

(jA \ fk; lgj) = (0) + ((2)  (1))ekl  eA + (2(1)  (0)  (2))	ekl;1(A)

Why are such potential functions interesting? They arise  for example  when ï¬nding the Maximum
a Posteriori conï¬guration of a pairwise Markov Random Field model in image classiï¬cation
schemes such as in [20]. On a high level  such an algorithm computes a value c[k] that corresponds
to the log-likelihood of pixel k being of one class vs. another  and for each pair of adjacent pixels 
a value dkl related to the log-likelihood that pixels k and l are of the same class. Then the algorithm

classiï¬es pixels by minimizing a sum of 2-potentials: f (A) = c  eA +Pk;l dkl(1  j1  ekl  eAj).

If the value dkl is large  this encourages the pixels k and l to be classiï¬ed similarly.
More generally  consider a higher order potential function: a concave function of the number of
elements in some activation set S  (jA \ Sj) where  is concave. It can be shown that this can
be written as a sum of a modular function and a positive linear combination of jSj  1 threshold
potentials. Recent work [14] has shown that classiï¬cation performance can be improved by adding
terms corresponding to such higher order potentials j(jRj \ Aj) to the objective function where the
functions j are piecewise linear concave functions  and the regions Rj of various sizes generated
from a segmentation algorithm. Minimization of these particular potential functions can then be
reformulated as a graph cut problem [13]  but this is less general than our approach.
Another canonical example of a submodular function is a set cover function. Such a function can
be reformulated as a combination of concave cardinality functions (details omitted here). So all

2A function is called modular if (1) holds with equality. It can be written as A 7! w  eA for some w 2 Rn.

3

functions which are weighted combinations of set cover functions can be expressed as threshold
potentials. However  threshold potentials with nonuniform weights are strictly more general than
concave cardinality potentials. That is  there exists w and y such that 	w;y(A) cannot be expressed

as Pj j(jRj \ Aj) for any collection of concave j and sets Rj.

Another example of decomposable functions arises in multiclass queuing systems [10]. These are
of the form f (A) = c  eA + u  eA(v  eA)  where u; v are nonnegative weight vectors and  is
a nonincreasing concave function. With the proper choice of j and wj (again details are omitted
here)  this can in fact be reformulated as sum of the type in Eq. 3 with n terms.
In our own experiments  shown in Section 6  we use an implementation of TextonBoost [20] and
augment it with quadratic higher order potentials. That is  we use TextonBoost to generate per-pixel

scores c  and then minimize f (A) = c  eA +Pj jA \ RjjjRj n Aj  where the regions Rj are regions

of pixels that we expect to be of the same class (e.g.  by running a cheap region-growing heuristic).
The potential function jA\RjjjRj nAj is smallest when A contains all of Rj or none of it. It gives the
largest penalty when exactly half of Rj is contained in A. This encourages the classiï¬cation scheme
to classify most of the pixels in a region Rj the same way. We generate regions with a basic region-
growing algorithm with random seeds. See Figure 1(a) for an illustration of examples of regions
that we use. In our experience  this simple idea of using higher-order potentials can dramatically
increase the quality of the classiï¬cation over one using only 2-potentials  as can be seen in Figure 2.
4 The SLG Algorithm for Threshold Potentials
We now present our algorithm for efï¬cient minimization of a decomposable submodular function f
based on smoothed convex minimization. We ï¬rst show how we can efï¬ciently smooth the LovÂ´asz
extension of f. We then apply accelerated gradient descent to the gradient of the smoothed function.
Lastly  we demonstrate how we can often obtain a certiï¬cate of optimality that allows us to stop
early  drastically speeding up the algorithm in practice.
4.1 The Smoothed Extension of a Threshold Potential
The key challenge in our algorithm is to efï¬ciently smooth the LovÂ´asz extension of f  so that we
can resort to algorithms for accelerated convex minimization. We now show how we can efï¬ciently
smooth the threshold potentials 	w;y(A) = min(y; w  eA) of Section 3  which are simple enough
to allow efï¬cient smoothing  but rich enough when combined to express a large class of submodular
functions. For x  0  the LovÂ´asz extension of 	w;y is

~	w;y(x) = sup v  x s.t. v  w; v  eA  y for all A 2 2E:

Note that when x  0  the arg max of the above linear program always contains a point v which
satisï¬es v  1 = y  and v  0. So we can restrict the domain of the dual variable v to those points
which satisfy these two conditions  without changing the value of ~	(x):

~	w;y(x) = max

v2D(w;y)

v  x where D(w; y) = fv : 0  v  w; v  1 = yg:

Restricting the domain of v allows us to deï¬ne a smoothed LovÂ´asz extension (with parameter )
that is easily computed:

~	

w;y(x) = max

v  x 

kvk2

v2D(w;y)


2

To compute the value of this function we need to solve for the optimal vector v  which is also the
gradient of this function  as we have the following characterization:

To derive an expression for v  we begin by forming the Lagrangian and deriving the dual problem:

r ~	

w;y(x) = arg max
v2D(w;y)

v  x 


2

kvk2 = arg min

:

(4)

x


v2D(w;y)

 v

~	

w;y(x) =

t2R;1;20max

min

v  x 


2

kvk2 + 1  v + 2  (w  v) + t(y  v  1)

=

min

t2R;1;20

kx  t1 + 1  2k2 + 2  w + ty:

v2Rn
1
2

If we ï¬x t  we can solve for the optimal dual variables 
we know the optimal primal variable is given by v = 1

1 = max(t1  x; 0); 

1 and 
 (x  t1 + 

2 componentwise. By strong duality 

1  

2). So we have:

2 = max(x  t1  w; 0) ) v = min (max ((x  t1)=; 0) ; w) :

4

This expresses v as a function of the unknown optimal dual variable t. For the simple case of
2-potentials  we can solve for t explicitly and get a closed form expression:

r ~	

ekl;1(x) = 8><
>:

ek
el
1
2 (ekl + 1

 (x[k]  x[l])(ek  el))

if x[k]  x[l] + 
if x[l]  x[k] + 
if jx[k]  x[l]j < 

However  in general to ï¬nd t we note that v must satisfy v  1 = y. So deï¬ne 

x;w(t) as:


x;w(t) = min(max((x  t1)=; 0); w)  1

Then we note this function is a monotonic continuous piecewise linear function of t  so we can use a
simple root-ï¬nding algorithm to solve 
x;w(t) = y. This root ï¬nding procedure will take no more
than O(n) steps in the worst case.
4.2 The SLG Algorithm for Minimizing Sums of Threshold Potentials
Stepping beyond a single threshold potential  we now assume that the submodular function to be
minimized can be written as a nonnegative linear combination of threshold potentials and a modular
function  i.e. 

Thus  we have the smoothed LovÂ´asz extension  and its gradient:

f (A) = c  eA +Xj

dj	wj ;yj (A):

~f (x) = c  x +Xj

dj ~	

wj ;yj (x) and r ~f (x) = c +Xj

djr ~	

wj ;yj (x):

We now wish to use the accelerated gradient descent algorithm of [18] to minimize this function.
This algorithm requires that the smoothed objective has a Lipschitz continuous gradient. That is  for
some constant L  it must hold that kr ~f (x1)  r ~f (x2)k  Lkx1  x2k; for all x1; x2 2 Rn.
Fortunately  by construction  the smoothed threshold extensions ~	
wj ;yj (x) all have 1= Lip-
schitz gradient  a direct consequence of the characterization in Equation 4. Hence we have
a loose upper bound for the Lipschitz constant of ~f : L  D
   where D = Pj dj. Fur-
thermore  the smoothed threshold extensions approximate the threshold extensions uniformly:
2 for all x  so j ~f (x)  ~f (x)j  D
j ~	
2 .

wj ;yj (x)  ~	wj ;yj (x)j  

One way to use the smoothed gradient is to specify an accuracy "  then minimize ~f  for sufï¬ciently
small  to guarantee that the solution will also be an approximate minimizer of ~f. Then we simply
apply the accelerated gradient descent algorithm of [18]. See also [3] for a description. Let PC(x) =
arg minx02C kx  x0k be the projection of x onto the convex set C. In particular  P[0;1]n (x) =
min(max(x; 0); 1). Algorithm 1 formalizes our Smoothed LovÂ´asz Gradient (SLG) algorithm:

Algorithm 1: SLG: Smoothed LovÂ´asz Gradient
Input: Accuracy "; decomposable function f.
begin

1;

2

 = "
for t = 0; 1; 2; : : : do

   x1 = z1 = 1

2D   L = D
gt = r ~f (xt1)=L; zt = P[0;1]n z1 Pt
if gapt  "=2 then stop;
xt = (2zt + (t + 1)yt)=(t + 3);

x" = yt;

Output: "-optimal x" to minx2[0;1]n ~f (x)

s=0 s+1

2  gs; yt = P[0;1]n (xt  gt);

The optimality gap of a smooth convex function at the iterate yt can be computed from its gradient:

gapt = max
x2[0;1]n

(yt  x)  r ~f (yt) = yt  r ~f (yt) + max(r ~f (yt); 0)  1:

In summary  as a consequence of the results of [18]  we have the following guarantee about SLG:

Theorem 1 SLG is guaranteed to provide an "-optimal solution after running for O( D

" ) iterations.

5

SLG is only guaranteed to provide an "-optimal solution to the continuous optimization problem.
Fortunately  once we have an "-optimal point for the LovÂ´asz extension  we can efï¬ciently round it to
set which is "-optimal for the original submodular function using Alg. 2 (see [9] for more details).
Algorithm 2: Set generation by rounding the continuous solution
Input: Vector x 2 [0; 1]n; submodular function f.
begin

By sorting  ï¬nd any permutation  satisfying: x[(1)]  : : :  x[(n)];
Sk = f(1); : : : ; (k)g; K  = arg mink2f0;1;:::;ng f (Sk); C = fSk : k 2 K g;

Output: Collection of sets C  such that f (A)  ~f (x) for all A 2 C

4.3 Early Stopping based on Discrete Certiï¬cates of Optimality
In general  if the minimum of f is not unique  the output of SLG may be in the interior of the unit
cube. However  if f admits a unique minimum A  then the iterates will tend toward the corner
eA. One natural question one may ask  if a trend like this is observed  is it necessary to wait for the
iterates to converge all the way to the optimal solution of the continuous problem minx2[0;1]n ~f (x) 
when one is actually iterested in solving the discrete problem minA22E f (A)? Below  we show that
it is possible to use information about the current iterates to check optimality of a set and terminate
the algorithm before the continuous problem has converged.
To prove optimality of a candidate set A  we can use a subgradient of ~f at eA. If g 2 @ ~f (eA)  then
we can compute an optimality gap:

f (A)  f   max
x2[0;1]n

(eA  x)  g = Xk2A

max(0; g[k](eA[k]  eEnA[k])):

(5)

In particular if g[k]  0 for k 2 A and g[k]  0 for k 2 E n A  then A is optimal. But if we only
have knowledge of candidate set A  then ï¬nding a subgradient g 2 @ ~f (eA) which demonstrates
optimality may be extremely difï¬cult  as the set of subgradients is a polyhedron with exponentially
many extreme points. But our algorithm naturally suggests the subgradient we could use; the gradi-
ent of the smoothed extension is one such subgradient â€“ provided a certain condition is satisï¬ed  as
described in the following Lemma.
Lemma 1 Suppose f is a decomposable submodular function  with LovÂ´asz extension ~f  and
smoothed extension ~f  as in the previous section. Suppose x 2 Rn and A 2 2E satisfy the fol-
lowing property:

min

x[k]  x[l]  2

k2A;l2EnA

Then r ~f (x) 2 @ ~f (eA)
This is a consequence of our formula for r ~	  but see the appendix of the extended paper [21] for
a detailed proof. Lemma 1 states that if the components of point x corresponding to elements of A
are all larger than all the other components by at least 2  then the gradient at x is a subgradient for
~f at eA (which by Equation 5 allows us to compute an optimality gap). In practice  this separation
of components naturally occurs as the iterates move in the direction of the point eA  long before
they ever actually reach the point eA. But even if the components are not separated  we can easily
add a positive multiple of eA to separate them and then compute the gradient there to get an
optimality gap. In summary  we have the following algorithm to check the optimality of a candidate
set: Of critical importance is how to choose the candidate set A. But by Equation 5  for a set to be
Algorithm 3: Set Optimality Check
Input: Set A; decomposable function f; scale ; x 2 Rn.
begin

 = 2 + maxk2A;l2EnA x[l]  x[k]; g = r ~f (x + eA);
gap = Pk2A max(0; g[k](eA[k]  eEnA[k]));

Output: gap  which satisï¬es gap  f (A)  f 

optimal  we want the components of the gradient r ~f (A + eA)[k] to be negative for k 2 A and
positive for k 2 E n A. So it is natural to choose A = fk : r ~f (x)[k]  0g. Thus  if adding eA
does not change the signs of the components of the gradient  then in fact we have found the optimal
set. This stopping criterion is very effective in practice  and we use it in all of our experiments.

6

(a) Example Regions for Potentials
Figure 1: (a) Example regions used for our higher-order potential functions (b-c) Comparision of
running times of submodular minimization algorithms on synthetic problems from DIMACS [1].

(b) Results for genrmf-long

(c) Results genrmf-wide

5 Extension to General Concave Potentials
To extend our algorithm to work on general concave functions  we note that an arbitrary concave
function can be expressed as an integral of threshold potential functions. This is a simple conse-
quence of integration by parts  which we state in the following lemma:
Lemma 2 For  2 C 2([0; T ]) 

(x) = (0) + 0(T )x Z T

0

min(x; y)00(y)dy;

8x 2 [0; T ]

This means that for a general sum of concave potentials as in Equation (3)  we have:

f (A) = c  eA +Xj

j(0) + 0(wj  1)wj  eA Z wj 1

0

	wj ;y(A)00

j (y)dy :

Then we can deï¬ne ~f and ~f  by replacing 	 with ~	 and ~	 respectively. Our SLG algorithm is
essentially unchanged  the conditions for optimality still hold  and so on. Conceptually  we just use
a different smoothed gradient  but calculating it is more involved. We need to compute the integrals
w;y(x) is a piecewise linear function with repect to y
which we can compute  we can evaluate the integral by parts so that we need only evaluate   but
not its derivatives. We omit the resulting formulas for space limitations.

of the form R r ~	

w;y(x)00(y)dy. Since r ~	

6 Experiments
Synthetic Data. We reproduce the experimental setup of [8] designed to compare submodular
minimization algorithms. Our goal is to ï¬nd the minimum cut of a randomly generated graph (which
requires submodular minimization of a sum of 2-potentials) with the graph generated by the speci-
ï¬cations in [1]. We compare against the state of the art combinatorial algorithms (LEX2  HYBRID 
SFM3  PR [6]) that are guaranteed to ï¬nd the exact solution in polynomial time  as well as the
Minimum Norm algorithm of [8]  a practical alternative with unknown running time. Figures 1(b)
and 1(c) compare the running time of SLG against the running times reported in [8]. In some cases 
SLG was 6 times faster than the MinNorm algorithm. However the comparison to the MinNorm
algorithm is inconclusive in this experiment  since while we used a faster machine  we also used a
simple MATLAB implementation. What is clear is that SLG scales at least as well as MinNorm on
these problems  and is practical for problem sizes that the combinatorial algorithms cannot handle.
Image Segmentation Experiments. We also tested our algorithm on the joint
image
segmentation-and-classiï¬cation task introduced in Section 3. We used an implementation of
TextonBoost [20]  then trained on and tested subsampled images from [5]. As seen in Figures 2(e)
and 2(g)  using only the per-pixel score from our TextonBoost implementation gets the general area
of the object  but does not do a good job of identifying the shape of a classiï¬ed object. Compare
to the ground truth in Figures 2(b) and 2(d). We then perform MAP inference in a Markov Random
Field with 2-potentials (as done in [20]). While this regularization  as shown in Figures 2(f) and
2(h)  leads to improved performance  it still performs poorly on classifying the boundary.

7

R3R1R2102103100102Problem Size (n)Running Time (s)SLGSFM3MinNormHYBRIDPRLEX2102103100102Problem Size (n)Running Time (s)SLGSFM3MinNormLEX2HYBRIDPR(a) Original Image

(b) Ground truth

(c) Original Image

(d) Ground Truth

(e) Pixel-based

(f) Pairwise Potentials

(g) Pixel-based

(h) Pairwise Potentials

(i) Concave Potentials

(j) Continuous

(k) Concave Potentials
Figure 2: Segmentation experimental results

(l) Continuous

Finally  we used SLG to regularize with higher order potentials. To generate regions for our poten-
tials  we randomly picked seed pixels and grew the regions based on HSV channels of the image.
We picked our seed pixels with a preference for pixels which were included in the least number of
previously generated regions. Figure 1(a) shows what the regions typically looked like. For our ex-

periments  we used 90 total regions. We used SLG to minimize f (A) = ceA+Pj jA\RjjjRj nAj 

where c was the output from TextonBoost  scaled appropriately. Figures 2(i) and 2(k) show the clas-
siï¬cation output. The continuous variables x at the end of each run are shown in Figures 2(j) and
2(l); while it has no formal meaning  in general one can interpret a very high or low value of x[k]
to correspond to high conï¬dence in the classiï¬cation of the pixel k. To generate the result shown
in Figure 2(k)  a problem with 104 variables and 90 concave potentials  our MATLAB/mex im-
plementation of SLG took 71.4 seconds. In comparison  the MinNorm implementation of the SFO
toolbox [15] gave the same result  but took 6900 seconds. Similar problems on an image of twice the
resolution (4104 variables) were tested using SLG  resulting in runtimes of roughly 1600 seconds.
7 Conclusion
We have developed a novel method for efï¬ciently minimizing a large class of submodular functions
of practical importance. We do so by decomposing the function into a sum of threshold potentials 
whose LovÂ´asz extensions are convenient for using modern smoothing techniques of convex opti-
mization. This allows us to solve submodular minimization problems with thousands of variables 
that cannot be expressed using only pairwise potentials. Thus we have achieved a middle ground
between graph-cut-based algorithms which are extremely fast but only able to handle very speciï¬c
types of submodular minimization problems  and combinatorial algorithms which assume nothing
but submodularity but are impractical for large-scale problems.

Acknowledgements This research was partially supported by NSF grant IIS-0953413  a gift from
Microsoft Corporation and an Okawa Foundation Research Grant. Thanks to Alex Gittens and
Michael McCoy for use of their TextonBoost implementation.

8

References
[1] Dimacs  The First international algorithm implementation challenge: The core experiments 

1990.

[2] F. Bach  Structured sparsity-inducing norms through submodular functions  Advances in Neu-

ral Information Processing Systems (2010).

[3] S. Becker  J. Bobin  and E.J. Candes  Nesta: A fast and accurate ï¬rst-order method for sparse

recovery  Arxiv preprint arXiv 904 (2009)  1â€“37.

[4] F.A. Chudak and K. Nagano  Efï¬cient solutions to relaxations of combinatorial problems with
submodular penalties via the LovÂ´asz extension and non-smooth convex optimization  Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  Society for
Industrial and Applied Mathematics  2007  pp. 79â€“88.

[5] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman  The
PASCAL Visual Object Classes Challenge 2009 (VOC2009) Results  http://www.pascal-
network.org/challenges/VOC/voc2009/workshop/index.html.

[6] L. Fleischer and S. Iwata  A push-relabel framework for submodular function minimization
and applications to parametric optimization  Discrete Applied Mathematics 131 (2003)  no. 2 
311â€“322.

[7] D. Freedman and P. Drineas  Energy minimization via graph cuts: Settling what is possi-
ble  IEEE Computer Society Conference on Computer Vision and Pattern Recognition  2005.
CVPR 2005  vol. 2  2005.

[8] Satoru Fujishige  Takumi Hayashi  and Shigueo Isotani  The Minimum-Norm-Point Algorithm

Applied to Submodular Function Minimization and Linear Programming  (2006)  1â€“19.

[9] E. Hazan and S. Kale  Beyond convexity: Online submodular minimization  Advances in
Neural Information Processing Systems 22 (Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I.
Williams  and A. Culotta  eds.)  2009  pp. 700â€“708.

[10] T. Itoko and S. Iwata  Computational geometric approach to submodular function minimiza-
tion for multiclass queueing systems  Integer Programming and Combinatorial Optimization
(2007)  267â€“279.

[11] S. Iwata  L. Fleischer  and S. Fujishige  A combinatorial strongly polynomial algorithm for

minimizing submodular functions  Journal of the ACM (JACM) 48 (2001)  no. 4  777.

[12] S. Iwata and J.B. Orlin  A simple combinatorial algorithm for submodular function minimiza-
tion  Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms 
Society for Industrial and Applied Mathematics  2009  pp. 1230â€“1237.

[13] P. Kohli  M.P. Kumar  and P.H.S. Torr  P3 & Beyond: Solving Energies with Higher Order

Cliques  2007 IEEE Conference on Computer Vision and Pattern Recognition (2007)  1â€“8.

[14] P. Kohli  L. LadickÂ´y  and P.H.S. Torr  Robust Higher Order Potentials for Enforcing Label

Consistency  International Journal of Computer Vision 82 (2009)  no. 3  302â€“324.

[15] A. Krause  SFO: A Toolbox for Submodular Function Optimization  The Journal of Machine

Learning Research 11 (2010)  1141â€“1144.

[16] L. LovÂ´asz  Submodular functions and convexity  Mathematical programming: the state of the

art  Bonn (1982)  235â€“257.

[17] G. Nemhauser  L. Wolsey  and M. Fisher  An analysis of the approximations for maximizing

submodular set functions  Mathematical Programming 14 (1978)  265â€“294.

[18] Yu. Nesterov  Smooth minimization of non-smooth functions  Mathematical Programming 103

(2004)  no. 1  127â€“152.

[19] M. Queyranne  Minimizing symmetric submodular functions  Mathematical Programming 82

(1998)  no. 1-2  3â€“12.

[20] J. Shotton  J. Winn  C. Rother  and A. Criminisi  TextonBoost for Image Understanding: Multi-
Class Object Recognition and Segmentation by Jointly Modeling Texture  Layout  and Context 
Int. J. Comput. Vision 81 (2009)  no. 1  2â€“23.

[21] P. Stobbe and A. Krause  Efï¬cient minimization of decomposable submodular functions 

arXiv:1010.5511 (2010).

9

,Jinzhuo Wang
Wenmin Wang
xiongtao Chen
Ronggang Wang
Wen Gao
Miao Zhang
Jingjing Li
JI WEI
Yongri Piao
Huchuan Lu