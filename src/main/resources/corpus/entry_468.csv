2019,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks,The skip-connections used in residual networks have become a standard architecture choice in deep learning due to the increased generalization and stability of networks with this architecture  although there have been limited theoretical guarantees for this improved performance.   In this work  we analyze overparameterized deep residual networks trained by gradient descent following random initialization  and demonstrate that (i) the class of networks learned by gradient descent constitutes a small subset of the entire neural network function class  and (ii) this subclass of networks is sufficiently large to guarantee small training error.  By showing (i) we are able to demonstrate that deep residual networks trained with gradient descent have a small generalization gap between training and test error  and together with (ii) this guarantees that the test error will be small. Our optimization and generalization guarantees require overparameterization that is only logarithmic in the depth of the network  which helps explain why residual networks are preferable to fully connected ones.,Algorithm-Dependent Generalization Bounds for

Overparameterized Deep Residual Networks

Spencer Frei∗ and Yuan Cao† and Quanquan Gu‡

Abstract

The skip-connections used in residual networks have become a standard architec-
ture choice in deep learning due to the increased training stability and generalization
performance with this architecture  although there has been limited theoretical un-
derstanding for this improvement. In this work  we analyze overparameterized deep
residual networks trained by gradient descent following random initialization  and
demonstrate that (i) the class of networks learned by gradient descent constitutes a
small subset of the entire neural network function class  and (ii) this subclass of
networks is sufﬁciently large to guarantee small training error. By showing (i) we
are able to demonstrate that deep residual networks trained with gradient descent
have a small generalization gap between training and test error  and together with
(ii) this guarantees that the test error will be small. Our optimization and gener-
alization guarantees require overparameterization that is only logarithmic in the
depth of the network  while all known generalization bounds for deep non-residual
networks have overparameterization requirements that are at least polynomial in
the depth. This provides an explanation for why residual networks are preferable
to non-residual ones.

1

Introduction

Deep learning has seen an incredible amount of success in a variety of settings over the past eight
years  from image recognition [15] to audio recognition [20] and more. Compared with its rapid
and widespread adoption  the theoretical understanding of why deep learning works so well has
lagged signiﬁcantly. This is particularly the case in the common setup of an overparameterized
network  where the number of parameters in the network greatly exceeds the number of training
examples and input dimension. In this setting  networks have the capacity to perfectly ﬁt training
data  regardless of if it is labeled with real labels or random ones [25]. However  when trained on real
data  these networks also have the capacity to truly learn patterns in the data  as evidenced by the
impressive performance of overparameterized networks on a variety of benchmark datasets. This
suggests the presence of certain mechanisms underlying the data  neural network architectures  and
training algorithms which enable the generalization performance of neural networks. A theoretical
analysis that seeks to explain why neural networks work so well would therefore beneﬁt from careful
attention to the speciﬁc properties that neural networks have when trained under common optimization
techniques.
Many recent attempts at uncovering the generalization ability of deep learning focused on general
properties of neural network function classes with ﬁxed weights and training losses. For instance 
∗Department of Statistics  University of California  Los Angeles  CA 90095  USA; e-mail:
†Department of Computer Science  University of California  Los Angeles  CA 90095  USA; e-mail:
‡Department of Computer Science  University of California  Los Angeles  CA 90095  USA; e-mail:

spencerfrei@ucla.edu

yuancao@cs.ucla.edu

qgu@cs.ucla.edu

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Bartlett et al. [4] proved spectrally normalized margin bound for deep fully connected networks
in terms of the spectral norms of the weights at each layer. Neyshabur et al. [18] proved a similar
bound using PAC-Bayesian approach. Arora et al. [2] developed a compression-based framework
for generalization of deep fully connected and convolutional networks  and also provided an explicit
comparison of recent generalization bounds in the literature. All these studies involved algorithm-
independent analyses of the neural network generalization  with resultant generalization bounds that
involve quantities that make the bound looser with increased overparameterization.
An important recent development in the practical deployment of neural networks has been the
introduction of skip connections between layers  leading to a class of architectures known as residual
networks. Residual networks were ﬁrst introduced by He et al. [13] to much fanfare  quickly becoming
a standard architecture choice for state-of-the-art neural network classiﬁers. The motivation for
residual networks came from the poor behavior of very deep traditional fully connected networks:
although deeper fully connected networks can clearly express any function that a shallower one can 
in practice (i.e. using gradient descent) it can be difﬁcult to choose hyperparameters that result in
small training error. Deep residual networks  on the other hand  are remarkably stable in practice 
in the sense that they avoid getting stuck at initialization or having unpredictable oscillations in
training and validation error  two common occurrences when training deep non-residual networks.
Moreover  deep residual networks have been shown to generalize with better performance and far
fewer parameters than non-residual networks [22  7  14]. We note that much of the recent neural
network generalization literature has focused on non-residual architectures [4  18  2  12  5] with
bounds for the generalization gap that grow exponentially as the depth of the network increases. Li
et al. [16] recently studied a class of residual networks and proved algorithm-independent bounds for
the generalization gap that become larger as the depth of the network increases  with a dependence on
the depth that is somewhere between sublinear and exponential (a precise characterization requires
further assumptions and/or analysis). We note that verifying the non-vacuousness of algorithm-
independent generalization bounds relies on empirical arguments about what values the quantities
that appear in the bounds generally take in practical networks (i.e. norms of weight matrices and
interlayer activations)  while algorithm-dependent generalization bounds such as the ones we provide
in this paper can be understood without relying on experiments.

1.1 Our Contributions

In this work  we consider fully connected deep ReLU residual networks and study optimization
and generalization properties of such networks that are trained with discrete time gradient descent
following Gaussian initialization.
We consider binary classiﬁcation under the cross-entropy loss and focus on data that come from
distributions D for which there exists a function f for which y· f (x) ≥ γ > 0 for all (x  y) ∈ suppD
from a large function class F (see Assumption 3.2). By analyzing the trajectory of the parameters of
the network during gradient descent  for any error threshold ε > 0  we are able to show:

1. Under the cross-entropy loss  we can study an analogous surrogate error and bound the true
classiﬁcation error by the true surrogate error. This method was introduced by Cao and Gu
[5].

2. If m∗ = ˜O(poly(γ−1)) · max(d  ε−2)  then provided every layer of the network has at least
m ≥ m∗ units  gradient descent with small enough step size ﬁnds a point with empirical
surrogate error at most ε in at most ˜O(poly(γ−1) · ε−1) steps with high probability. Here 
˜O(·) hides logarithmic factors that may depend on the depth L of the network  the margin γ 
number of samples n  error threshold ε  and probability level δ.

3. Provided m∗ = ˜O(poly(γ−1  ε−1)) and n = ˜O(poly(γ−1  ε−1))  the difference between
the empirical surrogate error and the true surrogate error is at most ε with high probability 
and therefore the above provide a bound on the true classiﬁcation error of the learned
network.

We emphasize that our guarantees above come with at most logarithmic dependence on the depth of
the network. Our methods are adapted from those used in the fully connected architecture by Cao and
Gu [5] to the residual network architecture. The main proof idea is that overparameterization forces
gradient descent-trained networks to stay in a small neighborhood of initialization where the learned

2

networks (i) are guaranteed to ﬁnd small surrogate training error  and (ii) come from a sufﬁciently
small hypothesis class to guarantee a small generalization gap between the training and test errors.
By showing that these competing phenomena occur simultaneously  we are able to derive the test
error guarantees of Corollary 3.7. The key insight of our analysis is that the Lipschitz constant of
the network output for deep residual networks as well as the semismoothness property (Lemma 4.2)
have at most logarithmic dependence on the depth  while the known analogues for non-residual
architectures all have polynomial dependence on the depth.

1.2 Additional Related Work

In the last year there has been a variety of works developing algorithm-dependent guarantees for
neural network optimization and generalization [17  1  28  9  3  5  27  6]. Li and Liang [17] were
among the ﬁrst to theoretically analyze the properties of overparameterized fully connected neural
networks trained with Gaussian random initialization  focusing on a two layer (one hidden layer)
model under a data separability assumption. Their work provided two signiﬁcant insights into the
training process of overparameterized ReLU neural networks: (1) the weights stay close to their
initial values throughout the optimization trajectory  and (2) the ReLU activation patterns for a
given example do not change much throughout the optimization trajectory. These insights were the
backbone of the authors’ strong generalization result for stochastic gradient descent (SGD) in the
two layer case. The insights of Li and Liang [17] provided a basis to various subsequent studies. Du
et al. [9] analyzed a two layer model using a method based on the Gram matrix using inspiration
from kernel methods  showing that gradient descent following Gaussian initialization ﬁnds zero
training loss solutions at a linear rate. Zou et al. [28] and Allen-Zhu et al. [1] extended the results
of Li and Liang to the arbitrary L hidden layer fully connected case  again considering (stochastic)
gradient descent trained from random initialization. Both authors showed that  provided the networks
were sufﬁciently wide  arbitrarily deep networks would converge to a zero training loss solution
at a linear rate  using an assumption about separability of the data. Recently  Zou and Gu [27]
provided an improved analysis of the global convergence of gradient descent and SGD for training
deep neural networks  which enjoys a milder over-parameterization condition and better iteration
complexity than previous work. Under the same data separability assumption  Zhang et al. [26]
showed that deep residual networks can achieve zero training loss for the squared loss at a linear rate
with overparameterization essentially independent of the depth of the network. We note that Zhang
et al. [26] studied optimization for the regression problem rather than classiﬁcation  and their results
do not distinguish the case with random labels from that with true labels; hence  it is not immediately
clear how to translate their analysis to a generalization bound for classiﬁcation under the cross-entropy
loss as we are able to do in this paper.
The above results provide a concrete answer to the question of why overparameterized deep neural
networks can achieve zero training loss using gradient descent. However  the theoretical tools of
Du et al. [9]  Allen-Zhu et al. [1]  Zou et al. [28]  Zou and Gu [27] apply to data with random
labels as well as true labels  and thus do not explain the generalization to unseen data observed
experimentally. Dziugaite and Roy [10] optimized PAC-Bayes bounds for the generalization error of
a class of stochastic neural networks that are perturbations of standard neural networks trained by
SGD. Cao and Gu [5] proved a guarantee for arbitrarily small generalization error for classiﬁcation
in deep fully connected neural networks trained with gradient descent using random initialization.
The same authors recently provided an improved result for deep fully connected networks trained by
stochastic gradient descent using a different approach that relied on the neural tangent kernel and
online-to-batch conversion [6]. E et al. [11] recently developed algorithm-dependent generalization
bounds for a special residual network architecture with many different kinds of skip connections by
using kernel methods.

2 Network Architecture and Optimization Problem

We begin with the notation of the paper. We denote vectors by lowercase letters and matrices by
uppercase letters  with the assumption that a vector v is a column vector and its transpose v(cid:62) is a
row vector. We use the standard O(·)  Ω(·)  Θ(·) complexity notations to ignore universal constants 
with ˜O(·)  ˜Ω(·) additionally ignoring logarithmic factors. For n ∈ N  we write [n] = {1  2  . . .   n}.
Denote the number of hidden units at layer l as ml  l = 1  . . .   L + 1. Let the l-th layer weights
be Wl ∈ Rml−1×ml  and concatenate all of the layer weights into a vector W = (W1  . . .   WL+1).

3

Denote by wl j the j-th column of Wl. Let σ(x) = max(0  x) be the ReLU nonlinearity  and let θ
be a constant scaling parameter. We consider a class of residual networks deﬁned by the following
architecture:

xl = xl−1 + θσ(cid:0)W (cid:62)

l xl−1

(cid:1)   l = 2  . . .   L 

x1 = σ(W (cid:62)
xL+1 = σ(W (cid:62)

1 x) 
L+1xL).

Above  we denote xl as the l-th hidden layer activations of input x ∈ Rd  with x0 := x. In order
for this network to be deﬁned  it is necessary that m1 = m2 = ··· = mL. We are free to choose
mL+1  as long as mL+1 = Θ(m1) (see Assumption 3.4). We deﬁne a constant  non-trainable vector
v = (1  1  . . .   1 −1 −1  . . .  −1)(cid:62) ∈ RmL+1 with equal parts +1 and −1’s that determines the
network output 

fW (x) = v(cid:62)xL+1.

products of matrices(cid:81)b

We note that our methods can be extended to the case of a trainable top layer weights v by choosing
the appropriate scale of initialization for v. We choose to ﬁx the top layer weights in this paper for
simplicity of exposition.
We will ﬁnd it useful to consider the matrix multiplication form of the ReLU activations  which we
describe below. Let 1(A) denote the indicator function of a set A  and deﬁne diagonal matrices
Σl(x) ∈ Rml×ml by [Σl(x)]j j = 1(w(cid:62)
l = 1  . . .   L + 1. By convention we denote
i=a Mi by Mb · Mb−1 · . . . · Ma when a ≤ b  and by the identity matrix when
l (x) of
l(cid:48)(cid:89)

a > b. With this convention  we can introduce notation for the l-to-l(cid:48) interlayer activations H l(cid:48)
the network. For 2 ≤ l ≤ l(cid:48) ≤ L and input x ∈ Rd we denote

l jxl−1 > 0) 

(cid:0)I + θΣr(x)W (cid:62)

(2 ≤ l ≤ l(cid:48) ≤ L)

H l(cid:48)
l (x) :=

(cid:1) .

(1)

r

r=l

l

1 (x) = H l(cid:48)

l (x) by H l(cid:48)

2 (x)Σ1(x)W (cid:62)

l when the dependence on the input is clear.

1   and if l(cid:48) = L + 1 > l  we denote H L+1

(x) =
l (x). Using this notation  we can write the output of the neural network as
l+1 (x)xl for any l ∈ {0} ∪ [L + 1] and x ∈ Rd. For notational simplicity  we will

If l = 1 < l(cid:48)  we denote H l(cid:48)
ΣL+1(x)W (cid:62)
L+1H L
fW (x) = v(cid:62)H L+1
denote Σl(x) by Σl and H l(cid:48)
i=1 ∼ D from a distribution D  where xi ∈ Rd and
We assume we have i.i.d. samples (xi  yi)n
yi ∈ {±1}. We note the abuse of notation in the above  where xl ∈ Rml refers to the l-th hidden
layer activations of an arbitrary input x ∈ Rd while xi refers to the i-th sample xi ∈ Rd. We shall
use xl i ∈ Rml when referring to the l-th hidden layer activations of a sample xi ∈ Rd (where i ∈ [n]
and l ∈ [L + 1])  while xl ∈ Rml shall refer to the l-th hidden layer activation of arbitrary input
x ∈ Rd.
Let (cid:96)(x) = log(1 + exp(−x)) be the cross-entropy loss. We consider the empirical risk minimization
problem optimized by constant step size gradient descent 

min
W

LS(W ) :=

1
n

(cid:96)(yi · fW (xi)) 

W (k+1)

l

= W (k)

l − η · ∇Wl LS(W (k))

(l ∈ [L + 1]).

We shall see below that a key quantity for studying the trajectory of the weights in the above
optimization regime is a surrogate loss deﬁned by the derivative of the cross-entropy loss. We denote
the empirical and true surrogate loss by

n(cid:88)

i=1

n(cid:88)

i=1

ES(W ) := − 1
n

(cid:96)(cid:48)(yi · fW (xi)) 

ED(W ) := E(x y)∼D[−(cid:96)(cid:48)(y · fW (x))] 

respectively. The empirical surrogate loss was ﬁrst introduced by Cao and Gu [5] for the study of
deep non-residual networks. Finally  we note here a formula for the gradient of the output of the
network with respect to different layer weights:

∇WlfW (x) = θ1(2≤l≤L)xl−1v(cid:62)H L+1

l+1 Σl(x) 

(1 ≤ l ≤ L + 1).

(2)

4

3 Main Theory

We ﬁrst go over the assumptions necessary for our proof and then shall discuss our main results. Our
assumptions align with those made by Cao and Gu [5] in the fully connected case. The ﬁrst main
assumption is that the input data is normalized.
Assumption 3.1. Input data are normalized: supp(Dx) ⊂ Sd−1 = {x ∈ Rd : (cid:107)x(cid:107)2 = 1}.
Data normalization is common in statistical learning theory literature  from linear models up to and
including recent work in neural networks [17  28  9  1  3  5]  and can easily be satisﬁed for arbitrary
training data by mapping samples x (cid:55)→ x/(cid:107)x(cid:107)2.
The next assumption is on the data generating distribution. Because overparameterized networks can
memorize data  any hope of demonstrating that neural networks have a small generalization gap must
restrict the class of data distribution processes to one where some type of learning is possible.
Assumption 3.2. Let p(u) denote the density of a standard d-dimensional Gaussian vector. Deﬁne

(cid:40)(cid:90)

Rd

(cid:41)
c(u)σ(u(cid:62)x)p(u)du : (cid:107)c(·)(cid:107)∞ ≤ 1

.

F =

Assume there exists f (·) ∈ F and constant γ > 0 such that y · f (x) ≥ γ for all (x  y) ∈ supp(D).
Assumption 3.2 was introduced by Cao and Gu [5] for the analysis of fully connected networks and
is applicable for distributions where samples can be perfectly classiﬁed by the random kitchen sinks
model of Rahimi and Recht [19]. One can view a function from this class as the inﬁnite width limit of
a one-hidden-layer neural network with regularizer given by a function c(·) with bounded (cid:96)∞-norm.
As pointed out by Cao and Gu [5]  this assumption includes the linearly separable case.
Our next assumption concerns the scaling of the weights at initialization.
Assumption 3.3 (Gaussian initialization). We say that the weight matrices Wl ∈ Rml−1×ml are
generated via Gaussian initialization if each of the entries of Wl are generated independently from
N (0  2/ml).

This assumption is common to much of the recent theoretical analyses of neural networks [17  28  1 
9  3  5] and is known as the He initialization due to its usage in the ﬁrst ResNet paper by He et al. [13].
This assumption guarantees that the spectral norms of the weights are controlled at initialization.
Our last assumption concerns the widths of the networks we consider and allows us to exclude
pathological dependencies between the width and other parameters that deﬁne the architecture and
optimization problem.
Assumption 3.4 (Widths are of the same order). We assume mL+1 = Θ(mL). We call m =
mL ∧ mL+1 the width of the network.
Our ﬁrst theorem shows that provided we have sufﬁcient overparameterization and sufﬁciently small
step size  the iterates W (k) of gradient descent stay within a small neighborhood of their initialization.
Additionally  the empirical surrogate error can be bounded by a term that decreases as we increase
the width m of the network.
Theorem 3.5. Suppose W (0) are generated via Gaussian initialization and that the residual scaling
parameter satisﬁes θ = 1/Ω(L). For τ > 0  denote a τ-neighborhood of the weights W (0) =
(W (0)

  . . .   W (0)

L+1) at initialization by

1

W(W (0)  τ ) :=

W = (W1  . . .   WL+1) :

≤ τ ∀l ∈ [L + 1]

.

(cid:13)(cid:13)(cid:13)Wl − W (0)

l

There exist absolute constants ν  ν(cid:48)  ν(cid:48)(cid:48)  C  C(cid:48) > 0 such that for any δ > 0  provided τ ≤
2   then if the width
νγ12 (log m)
of the network is such that 

2 ∧ γ4m−1)  and Kη ≤ ν(cid:48)(cid:48)τ 2γ4 (log(n/δ))

− 3
2   η ≤ ν(cid:48)(τ m− 1

− 1

(cid:110)

τ− 4

3 d log

∨ d log

mL

∨ τ− 2

3 (log m)−1 log

∨ γ−2

d log

m
τ δ

δ

L
δ

1
γ

∨ log

L
δ

∨ log

n
δ

then with probability at least 1 − δ  gradient descent starting at W (0) with step size η generates K
iterates W (1)  . . .   W (K) that satisfy:

(cid:13)(cid:13)(cid:13)F
(cid:18)

(cid:111)

(cid:19)

(cid:19)

m ≥ C(cid:48)(cid:18)

5

(i) W (k) ∈ W(W (0)  τ ) for all k ∈ [K].
(ii) There exists k ∈ {0  . . .   K − 1} with ES(W (k)) ≤ C · m− 1

2 · (Kη)

− 1

2(cid:0)log n

(cid:1) 1

4 · γ−2.

δ

This theorem allows us to restrict our attention from the large class of all deep residual neural networks
to the reduced complexity class of those with weights that satisfy W ∈ W(W (0)  τ ). Our analysis
provides a characterization of the radius of this reduced complexity class in terms of parameters
that deﬁne the network architecture and optimization problem. Additionally  this theorem allows
us to translate the optimization problem over the empirical loss LS(W ) into one for the empirical
surrogate loss ES(W (k))  a quantity that is simply related to the classiﬁcation error (its expectation is
bounded by a constant multiple of the classiﬁcation error under 0-1 loss; see Appendix A.2).
Our next theorem characterizes the Rademacher complexity of the class of residual networks with
weights in a τ-neighborhood of the initialization. Additionally  it connects the test accuracy with the
empirical surrogate loss and the Rademacher complexity.
Theorem 3.6. Let W (0) denote the weights at Gaussian initialization and suppose the residual scaling
parameter satisﬁes θ = 1/Ω(L). Suppose τ ≤ 1. Then there exist absolute constants C1  C2  C3 > 0
such that for any δ > 0  provided

(cid:17)

then with probability at least 1 − δ  we have the following bound on the Rademacher complexity 

m ≥ C1

τ− 2

3 (log m)−1 log(L/δ) ∨ τ− 4

(cid:16)
(cid:16)(cid:8)fW : W ∈ W(W (0)  τ )(cid:9)(cid:17) ≤ C2

Rn

so that for all W ∈ W(W (0)  τ ) 

P(x y)∼D (y · fW (x) < 0) ≤ 2ES(W ) + C2

(cid:18)

τ

3 d log(m/(τ δ)) ∨ d log(mL/δ)

 

4

τ

(cid:18)
3(cid:112)m log m +

3(cid:112)m log m +
(cid:19)

√
m√
τ
n

4

(cid:19)
(cid:114)

√
m√
τ
n

+ C3

 

log(1/δ)

n

.

(3)

We shall see in Section 4 that we are able to derive the above bound on the Rademacher complexity by
using a semi-smoothness property of the neural network output and an upper bound on the gradient
of the network output. Standard arguments from statistical learning theory provide the ﬁrst and third
terms in (3).
The missing ingredients needed to realize the result of Theorem 3.6 for networks trained by gradient
descent are supplied by Theorem 3.5  which gives (i) control of the growth of the empirical surrogate
error ES along the gradient descent trajectory  and (ii) the distance τ from initialization before which
we are guaranteed to ﬁnd small empirical surrogate error. Putting these together yields Corollary 3.7.
Corollary 3.7. Suppose that the residual scaling parameter satisﬁes θ = 1/Ω(L). Let ε  δ > 0 be
ﬁxed. Suppose that m∗ = ˜O(poly(γ−1)) · max(d  ε−14) · log(1/δ) and n = ˜O(poly(γ−1)) · ε−4.
Then for any m ≥ m∗  with probability at least 1 − δ over the initialization and training sample 
there is an iterate k ∈ {0  . . .   K − 1} with K = ˜O(poly(γ−1))· ε−2 such that gradient descent with
Gaussian initialization and step size η = O(γ4 · m−1) satisﬁes

P(x y)∼D[y · fW (k)(x) < 0] ≤ ε.

This corollary shows that for deep residual networks  provided we have sufﬁcient overparameteriza-
tion  gradient descent is guaranteed to ﬁnd networks that have arbitrarily high classiﬁcation accuracy.
In comparison with the results of Cao and Gu [5]  the width m  number of samples n  step size η 
and number of iterates K required for the guarantees for residual networks given in Theorem 3.5
and Corollary 3.7 all have (at most) logarithmic dependence on L as opposed to the exponential
dependence in the corresponding results for the non-residual architecture. Additionally  we note
that the step size and number of iterations required for our guarantees are independent of the depth 
and this is due to the advantage of the residual architecture. Our analysis shows that the presence
of skip connections in the network architecture removes the complications relating to the depth that
traditionally arise in the analysis of non-residual architectures for a variety of reasons. The ﬁrst is a
technical one from the proof  in which we show that the Lipschitz constant of the network output

6

and the semismoothness of the network depend at most logarithmically on the depth  so that the
network width does not blow up as the depth increases (see Lemmas 4.1 and 4.2 below). Second 
the presence of skip-connections allows for representations that are learned in the ﬁrst layer to be
directly passed to later layers without needing to use a wider network to relearn those representations.
This property was key to our proof of the gradient lower bound of Lemma 4.3 and has been used in
previous approximation results for deep residual networks  e.g.  Yarotsky [24].

4 Proof Sketch of the Main Theory

In this section we will provide a proof sketch of Theorems 3.5 and 3.6 and Corollary 3.7  following
the proof technique of Cao and Gu [5]. We will ﬁrst collect the key lemmas needed for their proofs 
leaving the proofs of these lemmas for Appendix B. We shall assume throughout this section that
the residual scaling parameter satisﬁes θ = 1/Ω(L)  which we note is a common assumption in the
literature of residual network analysis [8  1  26].
Our ﬁrst key lemma shows that the interlayer activations deﬁned in (1) are uniformly bounded in x
and l provided the network is sufﬁciently wide.
Lemma 4.1 (Hidden layer and interlayer activations are bounded). Suppose that W1  . . .   WL+1 are
generated via Gaussian initialization. Then there exist absolute constants C0  C1  C2 > 0 such that if
m ≥ C0d log (mL/δ)  then with probability at least 1 − δ  for any l  l(cid:48) = 1  . . .   L + 1 with l ≤ l(cid:48)
and x ∈ Sd−1  we have C1 ≤ (cid:107)xl(cid:107)2 ≤ C2 and

≤ C2.

(cid:13)(cid:13)(cid:13)H l(cid:48)

l

(cid:13)(cid:13)(cid:13)2

Due to the scaling of θ  we are able to get bounds on the interlayer and hidden layer activations that
do not grow with L. As we shall see  this will be key for the sublinear dependence on L for the results
of Theorems 3.5 and 3.6. The fully connected architecture studied by Cao and Gu [5] had additional
polynomial terms in L for both upper bounds for (cid:107)xl(cid:107)2 and
Our next lemma describes a semi-smoothness property of the neural network output fW and the
empirical loss LS.
Lemma 4.2 (Semismoothness of network output and objective loss). Let W1  . . .   WL+1 be generated
via Gaussian initialization  and let τ ≤ 1. Deﬁne

(cid:13)(cid:13)(cid:13)H l(cid:48)

(cid:13)(cid:13)(cid:13)2

.

l

+ θ

l=2

(cid:13)(cid:13)(cid:13)2

m ≥ C

τ− 2

(cid:13)(cid:13)(cid:13)(cid:99)Wl − ˜Wl

(cid:13)(cid:13)(cid:13)2

L(cid:88)

(cid:13)(cid:13)(cid:13)(cid:99)W1 − ˜W1

(cid:13)(cid:13)(cid:13)(cid:99)WL+1 − ˜WL+1

h((cid:99)W   ˜W ) :=
(cid:16)

There exist absolute constants C  C > 0 such that if
3 (log m)−1 log(L/δ) ∨ τ− 4

(cid:13)(cid:13)(cid:13)2
(cid:17)
then with probability at least 1 − δ  we have for all x ∈ Sd−1 and(cid:99)W   ˜W ∈ W(W  τ ) 
m · h((cid:99)W   ˜W )2
(cid:21)
(cid:17)(cid:62) ∇Wl f ˜W (x)
(cid:21)
(cid:17)(cid:62) ∇Wl LS( ˜W )

3(cid:112)m log m · h((cid:99)W   ˜W ) + C
L+1(cid:88)
3(cid:112)m log m · h((cid:99)W   ˜W ) · ES( ˜W ) + Cm · h((cid:99)W   ˜W )2
L+1(cid:88)

(cid:20)(cid:16)(cid:99)Wl − ˜Wl
(cid:20)(cid:16)(cid:99)Wl − ˜Wl

LS((cid:99)W ) − LS( ˜W ) ≤ Cτ

f(cid:99)W (x) − f ˜W (x) ≤ Cτ

3 d log(m/(τ δ)) ∨ d log(mL/δ)

+

tr

+

tr

.

.

.

 

1

l=1

1

+

√

and

l=1

The semismoothness of the neural network output function fW will be used in the analysis of
generalization by Rademacher complexity arguments. For the objective loss LS  we apply this lemma
for weights along the trajectory of gradient descent. Since the difference in the weights of two
l = −η∇Wl LS(W (k))  the last term
consecutive steps of gradient descent satisfy W (k+1)

− W (k)

l

7

(cid:13)(cid:13)∇Wl LS(W (k))(cid:13)(cid:13)2

in the bound for the objective loss LS will take the form −η(cid:80)L+1

l=1

F . Thus by
simultaneously demonstrating (i) a lower bound for the gradient for at least one of the layers and
(ii) an upper bound for the gradient at all layers (and hence an upper bound for h(W (k+1)  W (k))) 
we can connect the empirical surrogate loss ES(W (k)) at iteration k with that of the objective loss
LS(W (k)) that will lead us to Theorem 3.5. Compared with the fully connected architecture of Cao
and Gu [5]  our bounds do not have any polynomial terms in L.
Thus the only remaining key items needed for our proof are upper bounds and lower bounds for the
gradient of the objective loss  described in the following two lemmas.
Lemma 4.3. Let W = (W1  . . .   WL+1) be weights at Gaussian initialization. There ex-
ist absolute constants C  C  ν such that for any δ > 0  provided τ ≤ νγ3 and m ≥
W(W  τ )  we have

Cγ−2(cid:0)d log γ−1 + log(L/δ)(cid:1) ∨ C log(n/δ)  then with probability at least 1 − δ  for all ˜W ∈

Lemma 4.4. Let W = (W1  . . .   WL+1) be weights at Gaussian initialization. There exists an
absolute constant C > 0 such that for any δ > 0  provided m ≥ C (d ∨ log(L/δ)) and τ ≤ 1  we
have for all ˜W ∈ W(W  τ ) and all l 

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)∇WL+1LS( ˜W )
(cid:13)(cid:13)(cid:13)∇Wl LS( ˜W )
(cid:13)(cid:13)(cid:13)F

F

≥ C · mL+1 · γ4 · ES( ˜W )2.

≤ θ1(2≤l≤L) · C

m · ES( ˜W ).

√

Note that we provide only a lower bound for the gradient at the last layer. It may be possible to
improve the degrees of the polynomial terms of the results in Theorems 3.5 and 3.6 by deriving lower
bounds for the other layers as well.
With all of the key lemmas in place  we can proceed with a proof sketch of Theorems 3.5 and 3.6.
The complete proofs can be found in Appendix A.

Proof of Theorem 3.5. Consider hk = h(W (k+1)  W (k))  a quantity that measures the distance of
the weights between gradient descent iterations. It takes the form

(cid:34)(cid:13)(cid:13)(cid:13)∇W1LS(W (k))

(cid:13)(cid:13)(cid:13)2

+ θ

(cid:13)(cid:13)(cid:13)∇Wl LS(W (k))

(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)∇WL+1LS(W (k))
(cid:13)(cid:13)(cid:13)2

+

(cid:35)

.

hk = η

mES(W (k)). The gradient lower bound in Lemma 4.3
By Lemma 4.4 we can show that hk ≤ Cη
substituted into Lemma 4.2 shows that the dominating term in the semismoothness comes from the
gradient lower bound  so that we have for any k 

LS(W (k+1)) − LS(W (k)) ≤ −C · η · mL+1 · γ4 · ES(W (k))2.

L(cid:88)

l=2

√

We can telescope the above over k to get a bound on the loss at iteration k in terms of the bound
on the r.h.s. and the loss at initialization. A simple concentration argument shows that the loss at
initialization is small with mild overparameterization. By letting k∗ = argmin[K−1]ES(W (k))2  we
can thus show

(cid:16)

(cid:17) 1

ES(W (k∗)) ≤ C3 (Kη · m)

− 1

2

LS(W (0))

2 · γ−2 ≤ C3 (Kη · m)

− 1

2

(cid:16)

(cid:17) 1

log

n
δ

4 · γ−2.

We provide below a proof sketch of the bound for the Rademacher complexity given in Theorem 3.6 
leaving the rest for Appendix A.2.

Proof of Theorem 3.6. Let ξi be independent Rademacher random variables. We consider a ﬁrst-order
approximation to the network output at initialization 

FW (0) W (x) := fW (0)(x) +

tr

Wl − W (0)

l

(cid:21)
(cid:17)(cid:62) ∇Wl fW (0)(x)

 

L+1(cid:88)

(cid:20)(cid:16)

l=1

8

and bound the Rademacher complexity by two terms 

(cid:98)RS[F(W (0)  τ )] ≤ Eξ

(cid:34)

n(cid:88)

i=1

1
n

(cid:35)
ξi[f (xi) − FW (0) W (xi)]
n(cid:88)

L+1(cid:88)

(cid:20)(cid:16)

ξi

tr

Wl − W (0)

l

i=1

l=1

sup

W∈W(W (0) τ )

(cid:34)

+ Eξ

sup

W∈W(W (0) τ )

1
n

(cid:21)(cid:35)
(cid:17)(cid:62) ∇Wl fW (0)(x)

3

√

For the ﬁrst term  taking ˜W = W (0) in Lemma 4.2 results in |fW (x) − FW (0) W (x)| ≤
m log m. For the second term  since (cid:107)AB(cid:107)F ≤ (cid:107)A(cid:107)F (cid:107)B(cid:107)2  we reduce this term to a
C3τ 4
product of two terms. The ﬁrst involves the norm of the distance of the weights from initialization 
which is τ. The second is the norm of the gradient at initialization  which can be taken care of by
m. A

using Cauchy–Schwarz and the gradient formula (2) to get (cid:107)∇Wl fW (0)(cid:107)F ≤ C2θ1(2≤(cid:96)≤L)√

√
standard application of Jensen inequality gives the 1/

n term.

Finally  we can put together Theorems 3.5 and 3.6 by appropriately choosing the scale of τ  η  and K
to get Corollary 3.7. We leave the detailed algebraic calculations for Appendix A.3.

Proof of Corollary 3.7. We need only specify conditions on τ  η  Kη  and m such that the results of
Theorems 3.5 and 3.6 will hold  and making sure that each of the four terms in (3) are of the same
scale. This can be satisﬁed by imposing the condition Kη = ν(cid:48)(cid:48)γ4τ 2 (log(n/δ))

− 1

3(cid:112)m log m = C2τ(cid:112)m/n = C3

4

2 and

(cid:112)log(1/δ)/n = ε/4.

C3 (Kηm)

− 1

2 (log(n/δ))

1

4 · γ−2 = C2τ

5 Conclusions

In this paper  we derived algorithm-dependent optimization and generalization results for overpa-
rameterized deep residual networks trained with random initialization using gradient descent. We
showed that this class of networks is both small enough to ensure a small generalization gap and
also large enough to achieve a small training loss. Important to our analysis is the insight that the
introduction of skip connections allows for us to essentially ignore the depth as a complicating factor
in the analysis  in contrast with the well-known difﬁculty of achieving nonvacuous generalization
bounds for deep non-residual networks. This provides a theoretical understanding for the increased
stability and generalization of deep residual networks over non-residual ones observed in practice.

Acknowledgement

We would like to thank the anonymous reviewers for their helpful comments. This research was
sponsored in part by the National Science Foundation IIS-1903202 and IIS-1906169. QG is also
partially supported by the Salesforce Deep Learning Research Grant. The views and conclusions
contained in this paper are those of the authors and should not be interpreted as representing any
funding agencies.

References
[1] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-

parameterization. arXiv preprint  arXiv:1811.03962  2018.

[2] S. Arora  R. Ge  B. Neyshabur  and Y. Zhang. Stronger generalization bounds for deep nets via
a compression approach. In ICML  volume 80 of Proceedings of Machine Learning Research 
pages 254–263. PMLR  2018.

[3] S. Arora  S. S. Du  W. Hu  Z. Li  and R. Wang. Fine-grained analysis of optimization and gener-
alization for overparameterized two-layer neural networks. arXiv preprint  arXiv:1901.08584 
2019.

9

[4] P. L. Bartlett  D. J. Foster  and M. J. Telgarsky. Spectrally-normalized margin bounds for neural

networks. In NeurIPS  pages 6241–6250  2017.

[5] Y. Cao and Q. Gu. A generalization theory of gradient descent for learning over-parameterized

deep relu networks. arXiv preprint  arXiv:1902.01384  2019.

[6] Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep

neural networks. In Conference on Neural Information Processing Systems  2019.

[7] S. Choi  S. Seo  B. Shin  H. Byun  M. Kersner  B. Kim  D. Kim  and S. Ha. Temporal convolution

for real-time keyword spotting on mobile devices. arXiv preprint  arXiv:1904.03814  2019.

[8] S. S. Du  J. D. Lee  H. Li  L. Wang  and X. Zhai. Gradient descent ﬁnds global minima of deep
neural networks. CoRR  abs/1811.03804  2018. URL http://arxiv.org/abs/1811.03804.

[9] S. S. Du  X. Zhai  B. Póczos  and A. Singh. Gradient descent provably optimizes over-

parameterized neural networks. arXiv preprint  arXiv:1810.02054  2018.

[10] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep
(stochastic) neural networks with many more parameters than training data. In Proceedings
of the Thirty-Third Conference on Uncertainty in Artiﬁcial Intelligence  UAI 2017  Sydney 
Australia  August 11-15  2017  2017. URL http://auai.org/uai2017/proceedings/
papers/173.pdf.

[11] W. E  C. Ma  Q. Wang  and L. Wu. Analysis of the gradient descent algorithm for a deep neural

network model with skip-connections. arXiv preprint  arXiv:1904.05263  2019.

[12] N. Golowich  A. Rakhlin  and O. Shamir. Size-independent sample complexity of neural
networks. In COLT  volume 75 of Proceedings of Machine Learning Research  pages 297–299.
PMLR  2018.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

pages 770–778. IEEE Computer Society  2016.

[14] F. N. Iandola  M. W. Moskewicz  K. Ashraf  S. Han  W. J. Dally  and K. Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. arXiv 
arXiv:1602.07360  2016. URL http://arxiv.org/abs/1602.07360.

[15] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. Commun. ACM  60(6):84–90  2017.

[16] X. Li  J. Lu  Z. Wang  J. D. Haupt  and T. Zhao. On tighter generalization bound for deep neural

networks: Cnns  resnets  and beyond. arXiv preprint  arXiv:1806.05159  2018.

[17] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent

on structured data. In NeurIPS  pages 8168–8177  2018.

[18] B. Neyshabur  S. Bhojanapalli  and N. Srebro. A pac-bayesian approach to spectrally-normalized

margin bounds for neural networks. In ICLR. OpenReview.net  2018.

[19] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with

randomization in learning. In NeurIPS  pages 1313–1320. Curran Associates  Inc.  2008.

[20] T. N. Sainath and C. Parada. Convolutional neural networks for small-footprint keyword

spotting. In INTERSPEECH  pages 1478–1482. ISCA  2015.

[21] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press  New York  NY  USA  2014. ISBN 1107057132 
9781107057135.

[22] R. Tang and J. Lin. Deep residual learning for small-footprint keyword spotting. In 2018 IEEE
International Conference on Acoustics  Speech and Signal Processing  ICASSP 2018  Calgary 
AB  Canada  April 15-20  2018  pages 5484–5488  2018. doi: 10.1109/ICASSP.2018.8462688.

10

[23] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint 

arXiv:1011.3027  2010.

[24] D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks  94:
103–114  2017. doi: 10.1016/j.neunet.2017.07.002. URL https://doi.org/10.1016/j.
neunet.2017.07.002.

[25] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. In ICLR. OpenReview.net  2017.

[26] H. Zhang  D. Yu  W. Chen  and T. Liu. Training over-parameterized deep resnet is almost as

easy as training a two-layer network. arXiv preprint  arXiv:1903.07120  2019.

[27] D. Zou and Q. Gu. An improved analysis of training over-parameterized deep neural networks.

In Conference on Neural Information Processing Systems  2019.

[28] D. Zou  Y. Cao  D. Zhou  and Q. Gu. Stochastic gradient descent optimizes over-parameterized

deep relu networks. arXiv preprint  arXiv:1811.08888  2018.

11

,Spencer Frei
Yuan Cao
Quanquan Gu