2013,When in Doubt  SWAP: High-Dimensional Sparse Recovery from Correlated Measurements,We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise.  It is well known that standard computationally tractable sparse recovery algorithms  such as the Lasso  OMP  and their various extensions  perform poorly when the measurement matrix contains highly correlated columns.  We develop a simple greedy algorithm  called SWAP  that iteratively swaps variables until a desired loss function cannot be decreased any further.  SWAP is surprisingly effective in handling measurement matrices with high correlations.  We prove that SWAP can be easily used as a wrapper around standard sparse recovery algorithms for improved performance.  We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.,When in Doubt  SWAP: High-Dimensional

Sparse Recovery from Correlated Measurements

Divyanshu Vats
Rice University

Houston  TX 77251
dvats@rice.edu

Richard Baraniuk

Rice University

Houston  TX 77251
richb@rice.edu

Abstract

We consider the problem of accurately estimating a high-dimensional sparse vec-
tor using a small number of linear measurements that are contaminated by noise. It
is well known that standard computationally tractable sparse recovery algorithms 
such as the Lasso  OMP  and their various extensions  perform poorly when the
measurement matrix contains highly correlated columns. We develop a simple
greedy algorithm  called SWAP  that iteratively swaps variables until a desired
loss function cannot be decreased any further. SWAP is surprisingly effective in
handling measurement matrices with high correlations. We prove that SWAP can
easily be used as a wrapper around standard sparse recovery algorithms for im-
proved performance. We theoretically quantify the statistical guarantees of SWAP
and complement our analysis with numerical results on synthetic and real data.

1 Introduction

An important problem that arises in many applications is that of recovering a high-dimensional
sparse (or approximately sparse) vector given a small number of linear measurements. Depending
on the problem of interest  the unknown sparse vector can encode relationships between genes [1] 
power line failures in massive power grid networks [2]  sparse representations of signals [3  4]  or
edges in a graphical model [5 6]  to name just a few applications. The simplest  but still very useful 
setting is when the observations can be approximated as a sparse linear combination of the columns
in a measurement matrix X weighted by the non-zero entries of the unknown sparse vector. In
∗  in
this paper  we study the problem of recovering the location of the non-zero entries  say S
the unknown vector  which is equivalent to recovering the columns of X that y depends on. In the
literature  this problem is often to referred to as the sparse recovery or the support recovery problem.
Although several tractable sparse recovery algorithms have been proposed in the literature  statis-
∗ can only be provided under conditions that limit how
tical guarantees for accurately estimating S
correlated the columns of X can be. For example  if there exists a column  say Xi  that is nearly lin-
∗  some sparse recovery algorithms may falsely select
early dependent on the columns indexed by S
Xi. In certain applications  where X can be speciﬁed a priori  correlations can easily be avoided
by appropriately choosing X. However  in many applications  X cannot be speciﬁed by a practi-
tioner  and correlated measurement matrices are inevitable. For example  when the columns in X
correspond to gene expression values  it has been observed that genes in the same pathway produce
correlated values [1]. Additionally  it has been observed that regions in the brain that are in close
proximity produce correlated signals as measured using an MRI [7].
∗ for mea-
In this paper  we develop new sparse recovery algorithms that can accurately recover S
surement matrices that exhibit strong correlations. We propose a greedy algorithm  called SWAP 
∗ until a desired loss function
that iteratively swaps variables starting from an initial estimate of S
cannot be decreased any further. We prove that SWAP can accurately identify the true signal support

1

under relatively mild conditions on the restricted eigenvalues of the matrix X T X and under certain
conditions on the correlations between the columns of X. A novel aspect of our theory is that the
conditions we derive are only needed when conventional sparse recovery algorithms fail to recover
∗. This motivates the use of SWAP as a wrapper around sparse recovery algorithms for improved
S
performance. Finally  using numerical simulations  we show that SWAP consistently outperforms
many state of the art algorithms on both synthetic and real data corresponding to gene expression
values.
∗. The
As alluded to earlier  several algorithms now exist in the literature for accurately estimating S
theoretical properties of such algorithms either depend on the irrepresentability condition [5  8–10]
or various forms of the restricted eigenvalue conditions [11 12]. See [13] for a comprehensive review
of such algorithms and the related conditions. SWAP is a greedy algorithm with novel guarantees
for sparse recovery and we make appropriate comparisons in the text. Another line of research when
dealing with correlated measurements is to estimate a superset of S
The rest of the paper is organized as follows. Section 2 formally deﬁnes the sparse recovery problem.
Section 3 introduces SWAP. Section 4 presents theoretical results on the conditions needed for
provably correct sparse recovery. Section 5 discusses numerical simulations. Section 6 summarizes
the paper and discusses future work.

∗; see [14–18] for examples.

2 Problem Setup
Throughout this paper  we assume that y ∈ R
by the linear model

∗

n and X ∈ R

n×p are known and related to each other

+ w  

y = Xβ

∗ ∈ R

(1)
p is the unknown sparse vector that we seek to estimate. We assume that the columns
where β
2/n = 1 for all i ∈ [p]  where we use the notation [p] = {1  2  . . .   p}
of X are normalized  i.e.  (cid:2)Xi(cid:2)2
∗ accordingly.
throughout the paper. In practice  normalization can easily be done by scaling X and β
We assume that the entries of w are i.i.d. zero-mean sub-Gaussian random variables with parameter
σ so that E[exp(twi)] ≤ exp(t2σ2/2). The sub-Gaussian condition on w is common in the literature
and allows for a wide class of noise models  including Gaussian  symmetric Bernoulli  and bounded
∗ denote the location
random variables. We let k be the number of non-zero entries in β
∗ and we adopt this notation
of the non-zero entries. It is common to refer to S
throughout the paper.
∗. Thus  we mainly focus
Once S
∗. A classical strategy for sparse recovery is to
on the sparse recovery problem of estimating S
search for a support of size k that minimizes a suitable loss function. For a support S  we assume
the least-squares loss  which is deﬁned as follows:

∗ has been estimated  it is relatively straightforward to estimate β

∗ as the support of β

∗  and let S

L(S; y  X) := min
α∈R|S|

(cid:2)y − XSα(cid:2)2

2 =

(2)

(cid:1)(cid:1)Π

⊥

(cid:1)(cid:1)2

[S]y

2  

where XS refers to an n × |S| matrix that only includes the columns indexed by S and Π
⊥
[S] =
I − XS(X T
S is the orthogonal projection onto the null space of the linear operator XS. In
this paper  we design a sparse recovery algorithm that provably  and efﬁciently  ﬁnds the true support
for a broad class of measurement matrices that includes matrices with high correlations.

−1X T

S XS)

3 Overview of SWAP

We now describe our proposed greedy algorithm SWAP. Recall that our main goal is to ﬁnd a

support (cid:2)S that minimizes the loss deﬁned in (2). Suppose that we are given an estimate  say S(1)  of
the true support and let L(1) be the corresponding least-squares loss (see (2)). We want to transition
∗. Our
to another estimate S(2) that is closer (in terms of the number of true variables)  or equal  to S
main idea to transition from S(1) to an appropriate S(2) is to swap variables as follows:
Swap every i ∈ S(1) with i
sequently  we ﬁnd {(cid:2)i (cid:2)i
If mini i(cid:1) L(1)

i i(cid:1) < L(1)  there exists a support that has a lower loss than the original one. Sub-
(cid:4)}. We repeat the

i i(cid:1) and let S(2) = {S(1)\(cid:2)i} ∪ {(cid:2)i

(cid:4) ∈ (S(1))c and compute the resulting loss L(1)

(cid:4)} = arg mini i(cid:1) L(1)

i i(cid:1) = L({S(1)\i}∪i

; y  X).

(cid:4)

2

400

300

200

100

0
0

0.05

0.1
(a)

0.15

0.2

 

 

TLasso
S−TLasso
FoBa
S−FoBa
CoSaMP
S−CoSaMP
MaR
S−MaR

(b)

1

e

t

 

a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

0.5

0
3

4

5

6

Sparsity Level
(c)

s
n
o

i
t

a
r
e

t
I
 
f

o

 

#

 

n
a
e
M

10

5

0
3

4

7

8

7

8

5

6

Sparsity Level
(d)

Figure 1: Example of using SWAP on pseudo real data where the design matrix X corresponds to
gene expression values and y is simulated. The notation S-Alg refers to the SWAP based algorithms.
(a) Histogram of sparse eigenvalues of X over 10  000 random sets of size 10; (b) legend; (c) mean
true positive rate vs. sparsity; (d) mean number of iterations vs. sparsity.

Algorithm 1: SWAP(y  X  S)
Inputs: Measurements y  design matrix X  and initial support S.
1 Let r = 1  S(1) = S  and L(1) = L(S(1); y  X)
2 Swap i ∈ S(r) with i
3 if mini i(cid:1) L(r)

(cid:4) ∈ (S(r))c and compute the loss L(r)

i i(cid:1) < L(r) then

{(cid:2)i (cid:2)i(cid:4)} = argmini i(cid:1) L(r)
Let S(r+1) = {S(r)\(cid:2)i} ∪(cid:2)i(cid:4) and L(r+1) be the corresponding loss.
Return (cid:2)S = S(r).

i i(cid:1) (In case of a tie  choose a pair arbitrarily)

Let r = r + 1 and repeat steps 2-4.

4

5
6

7

else

i i(cid:1) = L({S(r)\i} ∪ i

(cid:4)

; y  X).

above steps to ﬁnd a sequence of supports S(1)  S(2)  . . .   S(r)  where S(r) has the property that
i i(cid:1) ≥ L(r). In other words  we stop SWAP when perturbing S(r) by one variable increases
mini i(cid:1) L(r)
or does not change the resulting loss. These steps are summarized in Algorithm 1.
Figure 1 illustrates the performance of SWAP for a matrix X that corresponds to 83 samples of
2308 gene expression values for patients with small round blue cell tumors [19]. Since there is no
ground truth available  we simulate the observations y using Gaussian w with σ = 0.5 and randomly
chosen sparse vectors with non-zero entries between 1 and 2. Figure 1(a) shows the histogram of the
AXA/n  where |A| = 10. We clearly see that
eigenvalues of 10 000 randomly chosen matrices X T
these eigenvalues are very small. This means that the columns of X are highly correlated with each
other. Figure 1(c) shows the mean fraction of variables estimated to be in the true support over 100
different trials. Figure 1(d) shows the mean number of iterations required for SWAP to converge.
Remark 3.1. The main input to SWAP is the initial support S. This parameter implicitly speciﬁes the
desired sparsity level. Although SWAP can be used with a random initialization S  we recommend
using SWAP in combination with another sparse recovery algorithm. For example  in Figure 1(c) 
we run SWAP using four different types of initializations. The dashed lines represent standard
sparse recovery algorithms  while the solid lines with markers represent SWAP algorithms. We
clearly see that all SWAP based algorithms outperform standard algorithms. Intuitively  since many
sparse recovery algorithms can perform partial support recovery  using such an initialization results
in a smaller search space when searching for the true support.
Remark 3.2. Since each iteration of SWAP necessarily produces a unique loss  the supports
S(1)  . . .   S(r) are all unique. Thus  SWAP clearly converges in a ﬁnite number of iterations. The
exact convergence rate depends on the correlations in the matrix X. Although we do not theoreti-
cally quantify the convergence rate  in all numerical simulations  and over a broad range of design
matrices  we observed that SWAP converged in roughly O(k) iterations. See Figure 1(d) for an
example.
Remark 3.3. Using the properties of orthogonal projections  we can write Line 2 of SWAP as a
difference of two rank one projection matrices. The main computational complexity is in computing

3

this quantity k(p − k) times for all i ∈ S(r) and i
(cid:4) ∈ (S(r)c. If the computational complexity of
computing a rank k orthogonal projection is Ik  then Line 2 can be implemented in time O(k(Ik +
p − k). When k (cid:6) p is small  then Ik = O(k3). When k is large  then several computational tricks
can be used to signiﬁcantly reduce the computational time.
Remark 3.4. SWAP differs signiﬁcantly from other greedy algorithms in the literature. When k
is known  the main distinctive feature of SWAP is that it always maintains a k-sparse estimate
of the support. Note that the same is true for the computationally intractable exhaustive search
algorithm [10]. Other competitive algorithms  such as forward-backwards (FoBa) [20] or CoSaMP
[21]  usually estimate a signal with higher sparsity level and iteratively remove variables until k
variables are selected. The same is true for multi-stage algorithms [22–25]. Intuitively  as we shall
see in Section 4  by maintaining a support of size k  the performance of SWAP only depends on
correlations among the columns of the matrix XA  where A is of size at most 2k and it includes the
true support. In contrast  for other sparse recovery algorithms  |A| ≥ 2k. In Figure 1  we compare
SWAP to several state of the art algorithms (see Section 5 for a description of the algorithms). In all
cases  SWAP results in superior performance.

4 Theoretical Analysis of SWAP

4.1 Some Important Parameters

In this Section  we collect some important parameters that determine the performance of SWAP.
First  we deﬁne the restricted eigenvalue as

ρk+(cid:2) := inf

: (cid:2)θ(cid:2)0 ≤ k + (cid:6)  |S

∗ ∩ supp(θ)| = k

.

(3)

(cid:4)

(cid:3)(cid:2)Xθ(cid:2)2

2

n(cid:2)θ(cid:2)2

2

The parameter ρk+(cid:2) is the minimum eigenvalue of certain blocks of the matrix X T X/n of size 2k
that includes the blocks X T
S∗XS∗ /n. Smaller values of ρk+(cid:2) correspond to correlated columns in
the matrix X. Next  we deﬁne the minimum absolute value of the non-zero entries in β

∗ as

βmin := min
i∈S∗

|β

i | .
∗

(4)

A smaller βmin will evidently require more number of observations for exact recovery of the support.
Finally  we deﬁne a parameter that characterizes the correlations between the columns of the matrix
∗ is the true support of the unknown
XS∗ and the columns of the matrix X(S∗)c  where recall that S
∗. For a set Ωk d that contains all supports of size k with atleast k− d active variables
sparse vector β
(cid:6)−1
from S

∗  deﬁne γd as

(cid:5)

(cid:1)(cid:1)(cid:1)(cid:1)Σ

S\i
i  ¯S

S\i
Σ
¯S  ¯S
S\i
i i

Σ

(cid:1)(cid:1)(cid:1)(cid:1)2

1

γ2
d := max

S∈Ωk d\S∗ min

i∈(S∗)c∩S

  ¯S = S

∗\S  

(5)

S∗ S∗(cid:2)2
−1

⊥
where ΣB = X T Π
[B]X/n. Popular sparse regression algorithms  such as the Lasso and the OMP 
can perform accurate support recovery when ζ2 = maxi∈(S∗)c (cid:2)Σi S∗Σ
1 < 1. We will show
in Section 3.2 that SWAP can perform accurate support recovery when γd < 1. Although the form
of γd is similar to ζ  there are several key differences  which we highlight as follows:
• Since Ωk d contains all supports such that |S
∗\S| ≤ d  it is clear that γd is the (cid:6)1 norm of a d × 1
vector  where d ≤ k. In contrast  ζ is the (cid:6)1 norm of a k × 1 vector. If indeed ζ < 1  i.e.  accurate
support recovery is possible using the Lasso  then SWAP can be initialized by the output of the
∗ minimizes
Lasso. In this case  γ(Ω) = 0 and SWAP also outputs the true support as long as S
the loss function. We make this statement precise in Theorem 4.1. Thus  it is only when ζ ≥ 1
that the parameter γd plays a role in the performance of SWAP.
• The parameter ζ directly computes correlations between the columns of X. In contrast  γd com-
putes correlations between the columns of X when projected onto the null space of a matrix XB 
where |B| = d − 1.
∗ and a minimum
over inactive variables in each support. The reason that the minimum appears in γd is because we
choose to swap variables that result in the smallest loss. In contrast  ζ is computed by taking a
maximum over all inactive variables.

• Notice that γd is computed by taking a maximum over supports in the set Ωd\S

4

4.2 Statement of Main Results

In this Section  we state the main results that characterize the performance of SWAP. Throughout
this Section  we assume the following:

(A1) The observations y and the measurement matrix X follow the linear model in (1)  where the

noise is sub-Gaussian with parameter σ  and the columns of X have been normalized.

(A2) SWAP is initialized with a support S(1) of size k and (cid:2)S is the output of SWAP. Since k is

typically unknown  a suitable value can be selected using standard model selection algorithms
such as cross-validation or stability selection [26].

Our ﬁrst result for SWAP is as follows.
Theorem 4.1. Suppose (A1)-(A2) holds and |S
1/(18σ2)  then P((cid:2)S = S

) → 1 as (n  p  k) → ∞.

∗

∗\S(1)| ≤ 1. If n > 4+log(k2(p−k))

minρ2k/2   where 0 < c2 ≤

c2β2

k+(cid:2)β2

min))  is weaker since 1/ρ3

The proof of Theorem 4.1 can be found in the extended version of our paper [27].
Informally 
Theorem 4.1 states that if the input to SWAP falsely detects at most one variable  then SWAP
is high-dimensional consistent when given a sufﬁcient number of observations n. The condition
∗ minimizes the loss function. This
on n is mainly enforced to guarantee that the true support S
condition is weaker than the sufﬁcient conditions required for other computationally tractable sparse
recovery algorithms. For example  the method FoBa is known to be superior to other methods
such as the Lasso and the OMP. As shown in [20]  FoBa requires that n = Ω(log(p)/(ρ3
min))
for high-dimensional consistent support recovery  where the choice of (cid:6)  which is greater than k 
depends on the correlations in the matrix X.
In contrast  the condition in (4.1)  which reduces
to n = Ω(log(p − k)/(ρ2kβ2
k+(cid:2) < 1/ρ2k for (cid:6) > k and p − k < p.
This shows that if a sparse recovery algorithm can accurately estimate the true support  then SWAP
does not introduce any false positives and also outputs the true support. Furthermore  if a sparse
regression algorithm falsely detects one variable  then SWAP can potentially recover the correct
support. Thus  using SWAP with other algorithms does not harm the sparse recovery performance
of other algorithms.
We now consider the more interesting case when SWAP is initialized by a support S(1) that falsely
detects more than one variable. In this case  SWAP will clearly needs more than one iteration to
recover the true support. Furthermore  to ensure that the true support can be recovered  we need to
impose some additional assumptions on the measurement matrix X. The particular assumption we
enforce will depend on the parameter γk deﬁned in (5). As mentioned in Section 4.1  γk captures
√
the correlations between the columns of XS∗ and the columns of X(S∗)c. To simplify the statement
in the next Theorem  deﬁne let g(δ  ρ  c) = g(δ  ρ  c) = (δ − 1) + 2c(
δ + 1/
(cid:8)
Theorem 4.2. Suppose (A1)-(A2) holds and |S
P((cid:2)S = S
c2 < 1/(18σ2)  g(γk  ρk 1  cσ) < 0  log

∗\S(1)| > 1.
> 4 + log(k2(p − k))  and n >

If for a constant c such that 0 <
  then

) → 1 as (n  p  k) → ∞.

√
ρ) + 2c2 .

2 log (p
k)
c2β2
minρ2
2k

(cid:7)

p
k

∗

Theorem 4.2 says that if SWAP is initialized with any support of size k  and γk satisﬁes the condi-
tion stated in the theorem  then SWAP will output the true support when given a sufﬁcient number
of observations. In the noiseless case  i.e.  when σ = 0  the condition required for accurate support
recovery reduces to γk < 1. The proof of Theorem 4.2  outlined in [27]  relies on imposing condi-
tions on each support of size k such that that there exists a swap so that the loss can be necessarily
∗  then SWAP will output the
decreased. Clearly  if such a property holds for each support  except S
true support since (i) there are only a ﬁnite number of possible supports  and (ii) each iteration of
SWAP results in a different support. The dependence on
in the expression for the number of
observations n arises from applying the union bound over all supports of size k.
(cid:7)
The condition in Theorem 4.2 is independent of the initialization S(1). This is why the sample
complexity  i.e.  the number of observations n required for consistent support recovery  scales as
. To reduce the sample complexity  we can impose additional conditions on the support
log
S(1) that is used to initialize SWAP. Under such assumptions  assuming that |S
∗\S(1)| > d  the

(cid:8)

(cid:7)

(cid:8)

p
k

p
k

5

performance of SWAP will depend on γd  which is less than γk  and n will scale as log
refer to [27] for more details.

(cid:7)

(cid:8)

p
d

. We

5 Numerical Simulations

In this section  we show how SWAP compares to other sparse recovery algorithms. Section 5.1
presents results for synthetic data and Section 5.2 presents results for real data.

5.1 Synthetic Data

To illustrate the advantages of SWAP  we use the following examples:

(A1) We sample the rows of X from a Gaussian distribution with mean zero and covariance Σ. The
covariance Σ is block-diagonal with blocks of size 10. The entries in each block ¯Σ are speci-
ﬁed as follows: ¯Σii = 1 for i ∈ [10] and ¯Σij = a for i (cid:11)= j. This construction of the design
matrix is motivated from [18]. The true support is chosen so that each variable in the support
∗ are chosen uniformly between 1
is assigned to a different block. The non-zero entries in β
and 2. We let σ = 1  p = 500  n = 100  200  k = 20  and a = 0.5  0.55  . . .   0.9  0.95.

(A2) We sample X from the same distribution as described in (A1). The only difference is that the
true support is chosen so that ﬁve different blocks contain active variables and each chosen
block contains four active variables. The rest of the parameters are also the same.

In both (A1) and (A2)  as a increases  the strength of correlations between the columns increases.
Further  the restricted eigenvalue parameter for (A1) is greater than the restricted eigenvalue param-
eter of (A2).
We use the following sparse recovery algorithms to initialize SWAP: (i) Lasso  (ii) Thresholded
Lasso (TLasso) [25]  (iii) Forward-Backward (FoBa) [20]  (iv) CoSaMP [21]  (v) Marginal Regres-
sion (MaR)  and (vi) Random. TLasso ﬁrst applies Lasso to select a superset of the support and then
selects the largest k as the estimated support. In our implementation  we used Lasso to select 2k
variables and then selected the largest k variables after least-squares. This algorithm is known to
have better performance that the Lasso. FoBa uses a combination of a forward and a backwards al-
gorithm. CoSaMP is an iterative greedy algorithm. MaR selects the support by choosing the largest
k variables in |X T y|. Finally  Random selects a random subset of size k. We use the notation S-
TLasso to refer to the algorithm that uses TLasso as an initialization for SWAP. A similar notation
follows for other algorithms.
Our results are shown in Figure 2. We use two metrics to assess the performance of SWAP. The
ﬁrst metric is the true positive rate (TPR)  i.e.  the number of active variables in the estimate divided
by the total number of active variables. The second metric is the the number of iterations needed
for SWAP to converge. Since all the results are over supports of size k  the false postive rate (FPR)
is simply 1 − TPR. All results for SWAP based algorithms have markers  while all results for non
SWAP based algorithms are represented in dashed lines.
From the TPR performance  we clearly see the advantages of using SWAP in practice. For different
choices the algorithm Alg  when n = 100  the performance of S-Alg is always better than the
performance of Alg. When the number of observations increase to n = 200  we observe that all
SWAP based algorithms perform better than standard sparse recovery algorithms. For (A1)  we
have exact support recovery for SWAP when a ≤ 0.9. For (A2)  we have exact support recovery
when a < 0.8. The reason for this difference is because of the differences in the placement of the
non-zero entries.
Figures 2(a) and 2(b) shows the mean number of iterations required by SWAP based algorithms as
the correlations in the matrix X increase. We clearly see that the number of iterations increase with
the degree of correlations. For algorithms that estimate a large fraction of the true support (TLasso 
FoBa  and CoSaMP)  the number of iterations is generally very small. For MaR and Random  the
number of iterations is larger  but still comparable to the sparsity level of k = 20.

6

 

 

R
P
T
n
a
e
M

1

0.8

0.6

0.4

0.2

0
0.5

Lasso
S−Lasso
TLasso
S−TLasso
FoBa
S−FoBa
CoSaMP
S−CoSaMP
MaR
S−MaR
S−Random

1

0.8

0.6

0.4

0.2

 

R
P
T
n
a
e
M

0.9

0
0.5

0.6

0.7

0.8

Degree of Correlation

0.6

0.7

0.8

0.9

Degree of Correlation

(a) Legend

(b) Example (A1)  n = 100

(c) Example (A1)  n = 100

1

0.8

0.6

0.4

 

R
P
T
n
a
e
M

0.9

0.5

25

20

15

10

5

s
n
o

i
t

a
r
e

t
I
 
f

o

 

 

#
n
a
e
M

0.9

0
0.5

0.6

0.7

0.8

Degree of Correlation

0.6

0.7

0.8

Degree of Correlation

0.6

0.7

0.8

0.9

Degree of Correlation

 

1

0.8

0.6

0.4

 

R
P
T
n
a
e
M

0.5

(d) Example (A2)  n = 100

(e) Example (A2)  n = 100

(f) Example (A2)  n = 100

1

0.8

0.6

0.4

0.2

 

R
P
T
n
a
e
M

0
0.5

30

20

10

s
n
o

i
t

a
r
e

t
I
 
f

o

 

#

 

n
a
e
M

0.9

0
0.5

0.6

0.7

0.8

Degree of Correlation

1

0.8

0.6

0.4

0.2

 

R
P
T
n
a
e
M

0.9

0
0.5

0.6

0.7

0.8

0.9

Degree of Correlation

0.6

0.7

0.8

Degree of Correlation

(g) Example (A1)  n = 200

(h) Example (A1)  n = 200

(i) Example (A2)  n = 200

Figure 2: Empirical true positive rate (TPR) and number of iterations required by SWAP.

5.2 Gene Expression Data

We now present results on two gene expression cancer datasets. The ﬁrst dataset1 contains expres-
sion values from patients with two different types cancers related to leukemia. The second dataset2
contains expression levels from patients with and without prostate cancer. The matrix X contains
the gene expression values and the vector y is an indictor of the type of cancer a patient has. Al-
though this is a classiﬁcation problem  we treat it as a recovery problem. For the leukemia data 
p = 5147 and n = 72. For the prostate cancer data  p = 12533 and n = 102. This is clearly a
high-dimensional dataset  and the goal is to identify a small set of genes that are predictive of the
cancer type.
Figure 3 shows the performance of standard algorithms vs. SWAP. We use leave-one-out cross-
validation and apply the sparse recovery algorithms described in Section 5.1 using multiple different
choices of the sparsity level. For each level of sparsity  we choose the sparse recovery algorithm
(labeled as standard) and the SWAP based algorithm that results in the minimum least-squares loss
over the training data. This allows us to compare the performance of using SWAP vs. not using
SWAP. For both datasets  we clearly see that the training and testing error is lower for SWAP based
algorithms. This means that SWAP is able to choose a subset of genes that has better predictive
performance than that of standard algorithms for each level of sparsity.

1see http://www.biolab.si/supp/bi-cancer/projections/info/leukemia.htm
2see http://www.biolab.si/supp/bi-cancer/projections/info/prostata.htm

7

r
o
r
r

i

 

E
n
a
r
T
−
V
C

3

2.5

2

1.5

1

0.5

 
2

 

SWAP
Standard

4

6

8

10

Sparsity Level

r
o
r
r

E

 
t
s
e
T
−
V
C

0.38

0.36

0.34

0.32

0.3

0.28

0.26

0.24

 
2

 

SWAP
Standard

4

6

8

10

Sparsity Level

4

3.5

3

2.5

r
o
r
r

i

E
 
n
a
r
T
−
V
C

2

 
2

 

SWAP
Standard

3

4

5

6

Sparsity Level

r
o
r
r

E

 
t
s
e
T
−
V
C

0.5

0.45

0.4

0.35

0.3

0.25

0.2

 
2

 

SWAP
Standard

3

4

5

6

Sparsity Level

(a) Training Error

(b) Testing Error

(c) Training Error

(d) Testing Error

Figure 3: (a)-(b) Leukemia dataset with p = 5147 and n = 72. (c)-(d) Prostate cancer dataset with
p = 12533 and n = 102.

6 Summary and Future Work

We studied the sparse recovery problem of estimating the support of a high-dimensional sparse
vector when given a measurement matrix that contains correlated columns. We presented a simple
algorithm  called SWAP  that iteratively swaps variables starting from an initial estimate of the
support until an appropriate loss function can no longer be decreased further. We showed that SWAP
is surprising effective in situations where the measurement matrix contains correlated columns. We
theoretically quantiﬁed the conditions on the measurement matrix that guarantee accurate support
recovery. Our theoretical results show that if SWAP is initialized with a support that contains some
active variables  then SWAP can tolerate even higher correlations in the measurement matrix. Using
numerical simulations on synthetic and real data  we showed how SWAP outperformed several
sparse recovery algorithms.
Our work in this paper sets up a platform to study the following interesting extensions of SWAP.
The ﬁrst is a generalization of SWAP so that a group of variables can be swapped in a sequential
manner. The second is a detailed analysis of SWAP when used with other sparse recovery algo-
rithms. The third is an extension of SWAP to high-dimensional vectors that admit structured sparse
representations.

Acknowledgement

The authors would like to thank Aswin Sankaranarayanan and Christoph Studer for feedback and
discussions. The work of D. Vats was partly supported by an Institute for Mathematics and Appli-
cations (IMA) Postdoctoral Fellowship.

References

[1] M. Segal  K. Dahlquist  and B. Conklin  “Regression approaches for microarray data analysis ”

Journal of Computational Biology  vol. 10  no. 6  pp. 961–980  2003.

[2] H. Zhu and G. Giannakis  “Sparse overcomplete representations for efﬁcient identiﬁcation of
power line outages ” IEEE Transactions on Power Systems  vol. 27  no. 4  pp. 2215 –2224 
nov. 2012.

[3] E. J. Cand`es  J. Romberg  and T. Tao  “Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information ” IEEE Trans. Information Theory  vol. 52 
no. 2  pp. 489–509  2006.

[4] M. F. Duarte  M. A. Davenport  D. Takhar  J. N. Laska  T. Sun  K. F. Kelly  and R. G. Baraniuk 
“Single-pixel imaging via compressive sampling ” IEEE Signal Processing Magazine  vol. 25 
no. 2  pp. 83–91  Mar. 2008.

[5] N. Meinshausen and P. B¨uhlmann  “High-dimensional graphs and variable selection with the

Lasso ” Annals of Statistics  vol. 34  no. 3  pp. 1436  2006.

[6] P. Ravikumar  M. Wainwright  and J. Lafferty  “High-dimensional Ising model selection using

(cid:6)1-egularized logistic regression ” Annals of Statistics  vol. 38  no. 3  pp. 1287–1319  2010.

8

[7] G. Varoquaux  A. Gramfort  and B. Thirion  “Small-sample brain mapping: sparse recovery
on spatially correlated designs with randomization and clustering ” in Proceedings of the 29th
International Conference on Machine Learning (ICML-12)  2012  pp. 1375–1382.

[8] P. Zhao and B. Yu  “On model selection consistency of Lasso ” Journal of Machine Learning

Research  vol. 7  pp. 2541–2563  2006.

[9] J. A. Tropp and A. C. Gilbert  “Signal recovery from random measurements via orthogonal
matching pursuit ” IEEE Transactions Information Theory  vol. 53  no. 12  pp. 4655–4666 
2007.

[10] M. J. Wainwright  “Sharp thresholds for noisy and high-dimensional recovery of sparsity using
(cid:6)1-constrained quadratic programming (Lasso) ” IEEE Transactions Information Theory  vol.
55  no. 5  May 2009.

[11] N. Meinshausen and B. Yu 

“Lasso-type recovery of sparse representations for high-

dimensional data ” Annals of Statistics  vol. 37  no. 1  pp. 246–270  2009.

[12] P. J. Bickel  Y. Ritov  and A. B. Tsybakov  “Simultaneous analysis of Lasso and Dantzig

selector ” Annals of Statistics  vol. 37  no. 4  pp. 1705–1732  2009.

[13] P. B¨uhlmann and S. Van De Geer  Statistics for High-Dimensional Data: Methods  Theory and

Applications  Springer-Verlag New York Inc  2011.

[14] H. Zou and T. Hastie  “Regularization and variable selection via the elastic net ” Journal of
the Royal Statistical Society: Series B (Statistical Methodology)  vol. 67  no. 2  pp. 301–320 
2005.

[15] Y. She  “Sparse regression with exact clustering ” Electronic Journal Statistics  vol. 4  pp.

1055–1096  2010.

[16] E. Grave  G. R. Obozinski  and F. R. Bach  “Trace Lasso: A trace norm regularization for
correlated designs ” in Advances in Neural Information Processing Systems 24  J. Shawe-
taylor  R. Zemel  P. Bartlett  F. Pereira  and K. Weinberger  Eds.  2011  pp. 2187–2195.

[17] J. Huang  S. Ma  H. Li  and C. Zhang  “The sparse laplacian shrinkage estimator for high-

dimensional regression ” Annals of Statistics  vol. 39  no. 4  pp. 2021  2011.

[18] P. B¨uhlmann  P. R¨utimann  S. van de Geer  and C.-H. Zhang  “Correlated variables in regres-
sion: clustering and sparse estimation ” Journal of Statistical Planning and Inference  vol. 143 
pp. 1835–1858  Nov. 2013.

[19] J. Khan  J. S. Wei  M. Ringner  L. H. Saal  M. Ladanyi  F. Westermann  F. Berthold  M.
Schwab  C. R. Antonescu  C. Peterson  et al.  “Classiﬁcation and diagnostic prediction of
cancers using gene expression proﬁling and artiﬁcial neural networks ” Nature medicine  vol.
7  no. 6  pp. 673–679  2001.

[20] T. Zhang  “Adaptive forward-backward greedy algorithm for learning sparse representations ”

IEEE Transactions Information Theory  vol. 57  no. 7  pp. 4689–4708  2011.

[21] D. Needell and J. A. Tropp  “CoSaMP: Iterative signal recovery from incomplete and inaccu-
rate samples ” Applied and Computational Harmonic Analysis  vol. 26  no. 3  pp. 301–321 
2009.

[22] T. Zhang  “Some sharp performance bounds for least squares regression with l1 regularization ”

The Annals of Statistics  vol. 37  no. 5A  pp. 2109–2144  2009.

[23] L. Wasserman and K. Roeder  “High dimensional variable selection ” Annals of statistics  vol.

37  no. 5A  pp. 2178  2009.

[24] T. Zhang  “Analysis of multi-stage convex relaxation for sparse regularization ” Journal of

Machine Learning Research  vol. 11  pp. 1081–1107  Mar. 2010.

[25] S. van de Geer  P. B¨uhlmann  and S. Zhou  “The adaptive and the thresholded lasso for poten-
tially misspeciﬁed models (and a lower bound for the lasso) ” Electronic Journal of Statistics 
vol. 5  pp. 688–749  2011.

[26] N. Meinshausen and P. B¨uhlmann  “Stability selection ” Journal of the Royal Statistical

Society: Series B (Statistical Methodology)  vol. 72  no. 4  pp. 417–473  2010.

[27] D. Vats and R. G. Baraniuk  “Swapping variables for high-dimensional sparse regression with

correlated measurements ” arXiv:1312.1706  2013.

9

,Divyanshu Vats
Richard Baraniuk