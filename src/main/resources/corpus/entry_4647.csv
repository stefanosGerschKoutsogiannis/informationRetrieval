2015,Regularization Path of Cross-Validation Error Lower Bounds,Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances.Nevertheless  current practice of regularization parameter tuning is more of an art than a science  e.g.  it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error.In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter  which we call regularization path of CV error lower bounds.The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters.We demonstrate through numerical experiments that a theoretically guaranteed a choice of regularization parameter in the above sense is possible with reasonable computational costs.,Regularization Path of

Cross-Validation Error Lower Bounds

Atsushi Shibagaki  Yoshiki Suzuki  Masayuki Karasuyama  and Ichiro Takeuchi

{shibagaki.a.mllab.nit suzuki.mllab.nit}@gmail.com

{karasuyama takeuchi.ichiro}@nitech.ac.jp

Nagoya Institute of Technology

Nagoya  466-8555  Japan

Abstract

Careful tuning of a regularization parameter is indispensable in many machine
learning tasks because it has a signiﬁcant impact on generalization performances.
Nevertheless  current practice of regularization parameter tuning is more of an art
than a science  e.g.  it is hard to tell how many grid-points would be needed in
cross-validation (CV) for obtaining a solution with sufﬁciently small CV error. In
this paper we propose a novel framework for computing a lower bound of the CV
errors as a function of the regularization parameter  which we call regularization
path of CV error lower bounds. The proposed framework can be used for provid-
ing a theoretical approximation guarantee on a set of solutions in the sense that
how far the CV error of the current best solution could be away from best possi-
ble CV error in the entire range of the regularization parameters. Our numerical
experiments demonstrate that a theoretically guaranteed choice of a regularization
parameter in the above sense is possible with reasonable computational costs.

1 Introduction
Many machine learning tasks involve careful tuning of a regularization parameter that controls the
balance between an empirical loss term and a regularization term. A regularization parameter is
usually selected by comparing the cross-validation (CV) errors at several different regularization
parameters. Although its choice has a signiﬁcant impact on the generalization performances  the
current practice is still more of an art than a science. For example  in commonly used grid-search  it
is hard to tell how many grid points we should search over for obtaining sufﬁciently small CV error.
In this paper we introduce a novel framework for a class of regularized binary classiﬁcation problems
that can compute a regularization path of CV error lower bounds. For an ε ∈ [0  1]  we deﬁne ε-
approximate regularization parameters to be a set of regularization parameters such that the CV
error of the solution at the regularization parameter is guaranteed to be no greater by ε than the best
possible CV error in the entire range of regularization parameters. Given a set of solutions obtained 
for example  by grid-search  the proposed framework allows us to provide a theoretical guarantee
of the current best solution by explicitly quantifying its approximation level ε in the above sense.
Furthermore  when a desired approximation level ε is speciﬁed  the proposed framework can be used
for efﬁciently ﬁnding one of the ε-approximate regularization parameters.
The proposed framework is built on a novel CV error lower bound represented as a function of the
regularization parameter  and this is why we call it as a regularization path of CV error lower bounds.
Our CV error lower bound can be computed by only using a ﬁnite number of solutions obtained by
arbitrary algorithms. It is thus easy to apply our framework to common regularization parameter tun-
ing strategies such as grid-search or Bayesian optimization. Furthermore  the proposed framework
can be used not only with exact optimal solutions but also with sufﬁciently good approximate solu-

1

Figure 1: An illustration of the proposed framework. One of our
algorithms presented in §4 automatically selected 39 regulariza-
tion parameter values in [10−3  103]  and an upper bound of the
validation error for each of them is obtained by solving an op-
timization problem approximately. Among those 39 values  the
one with the smallest validation error upper bound (indicated as
⋆ at C = 1.368) is guaranteed to be ε(= 0.1) approximate reg-
ularization parameter in the sense that the validation error for
the regularization parameter is no greater by ε than the smallest
possible validation error in the whole interval [10−3  103]. See
§5 for the setup (see also Figure 3 for the results with other op-
tions).

tions  which is computationally advantageous because completely solving an optimization problem
is often much more costly than obtaining a reasonably good approximate solution.
Our main contribution in this paper is to show that a theoretically guaranteed choice of a regular-
ization parameter in the above sense is possible with reasonable computational costs. To the best
of our knowledge  there is no other existing methods for providing such a theoretical guarantee on
CV error that can be used as generally as ours. Figure 1 illustrates the behavior of the algorithm for
obtaining ε = 0.1 approximate regularization parameter (see §5 for the setup).
Related works Optimal regularization parameter can be found if its exact regularization path can
be computed. Exact regularization path has been intensively studied [1  2]  but they are known to be
numerically unstable and do not scale well. Furthermore  exact regularization path can be computed
only for a limited class of problems whose solutions are written as piecewise-linear functions of the
regularization parameter [3]. Our framework is much more efﬁcient and can be applied to wider
classes of problems whose exact regularization path cannot be computed. This work was motivated
by recent studies on approximate regularization path [4  5  6  7]. These approximate regularization
paths have a property that the objective function value at each regularization parameter value is no
greater by ε than the optimal objective function value in the entire range of regularization param-
eters. Although these algorithms are much more stable and efﬁcient than exact ones  for the task
of tuning a regularization parameter  our interest is not in objective function values but in CV er-
rors. Our approach is more suitable for regularization parameter tuning tasks in the sense that the
approximation quality is guaranteed in terms of CV error.
As illustrated in Figure 1  we only compute a ﬁnite number of solutions  but still provide approxima-
tion guarantee in the whole interval of the regularization parameter. To ensure such a property  we
need a novel CV error lower bound that is sufﬁciently tight and represented as a monotonic function
of the regularization parameter. Although several CV error bounds (mostly for leave-one-out CV) of
SVM and other similar learning frameworks exist (e.g.  [8  9  10  11])  none of them satisfy the above
required properties. The idea of our CV error bound is inspired from recent studies on safe screening
[12  13  14  15  16] (see Appendix A for the detail). Furthermore  we emphasize that our contribu-
tion is not in presenting a new generalization error bound  but in introducing a practical framework
for providing a theoretical guarantee on the choice of a regularization parameter. Although gener-
alization error bounds such as structural risk minimization [17] might be used for a rough tuning of
a regularization parameter  they are known to be too loose to use as an alternative to CV (see  e.g. 
§11 in [18]). We also note that our contribution is not in presenting new method for regularization
parameter tuning such as Bayesian optimization [19]  random search [20] and gradient-based search
[21]. As we demonstrate in experiments  our approach can provide a theoretical approximation
guarantee of the regularization parameter selected by these existing methods.

2 Problem Setup

We consider linear binary classiﬁcation problems. Let {(xi  yi) ∈ Rd×{−1  1}}i∈[n] be the training
set where n is the size of the training set  d is the input dimension  and [n] := {1  . . .   n}. An inde-
pendent held-out validation set with size n′ is denoted similarly as {(x′i  y′i) ∈ Rd × {−1  1}}i∈[n′].
A linear decision function is written as f (x) = w⊤x  where w ∈ Rd is a vector of coefﬁcients 
and ⊤ represents the transpose. We assume the availability of a held-out validation set only for sim-
plifying the exposition. All the proposed methods presented in this paper can be straightforwardly

2

adapted to a cross-validation setup. Furthermore  the proposed methods can be kernelized if the loss
function satisﬁes a certain condition. In this paper we focus on the following class of regularized
convex loss minimization problems:

w∗C := arg min
w∈Rd

1

2∥w∥2 + C !i∈[n]

ℓ(yi  w⊤xi) 

(1)

where C > 0 is the regularization parameter  and ∥·∥ is the Euclidean norm. The loss function is
denoted as ℓ : {−1  1}× R → R. We assume that ℓ(· ·) is convex and subdifferentiable in the 2nd
argument. Examples of such loss functions include logistic loss  hinge loss  Huber-hinge loss  etc.
For notational convenience  we denote the individual loss as ℓi(w) := ℓ(yi  w⊤xi) for all i ∈ [n].
The optimal solution for the regularization parameter C is explicitly denoted as w∗C. We assume that
the regularization parameter is deﬁned in a ﬁnite interval [Cℓ  Cu]  e.g.  Cℓ = 10−3 and Cu = 103
as we did in the experiments.
For a solution w ∈ Rd  the validation error1 is deﬁned as

Ev(w) :=

I(y′iw⊤x′i < 0) 

(2)

1

n′ !i∈[n′]

where I(·) is the indicator function. In this paper  we consider two problem setups. The ﬁrst prob-
lem setup is  given a set of (either optimal or approximate) solutions w∗C1  . . .   w∗CT at T different
regularization parameters C1  . . .   CT ∈ [Cℓ  Cu]  to compute the approximation level ε such that
(3)

Ev(w∗Ct) − E∗v ≤ ε  where E∗v := min

Ev(w∗C) 

min

Ct∈{C1 ... CT }

C∈[Cl Cu]

by which we can ﬁnd how accurate our search (grid-search  typically) is in a sense of the deviation
of the achieved validation error from the true minimum in the range  i.e.  E∗v. The second problem
setup is  given the approximation level ε  to ﬁnd an ε-approximate regularization parameter within
an interval C ∈ [Cl  Cu]  which is deﬁned as an element of the following set

C(ε) :="C ∈ [Cl  Cu]### Ev(w∗C) − E∗v ≤ ε$.

Our goal in this second setup is to derive an efﬁcient exploration procedure which achieves the
speciﬁed validation approximation level ε. These two problem setups are both common scenarios in
practical data analysis  and can be solved by using our proposed framework for computing a path of
validation error lower bounds.

3 Validation error lower bounds as a function of regularization parameter

In this section  we derive a validation error lower bound which is represented as a function of the
regularization parameter C. Our basic idea is to compute a lower and an upper bound of the inner
product score w∗⊤C x′i for each validation input x′i  i ∈ [n′]  as a function of the regularization param-
eter C. For computing the bounds of w∗⊤C x′i  we use a solution (either optimal or approximate) for
a different regularization parameter ˜C ̸= C.
3.1 Score bounds

We ﬁrst describe how to obtain a lower and an upper bound of inner product score w∗⊤C x′i based on
an approximate solution ˆw ˜C at a different regularization parameter ˜C ̸= C.
Lemma 1. Let ˆw ˜C be an approximate solution of the problem (1) for a regularization parameter
value ˜C and ξi( ˆwC) be a subgradient of ℓi at w = ˆwC such that a subgradient of the objective
function is

g( ˆw ˜C) := ˆwC + ˜C !i∈[n]

ξi( ˆwC).

(4)

1 For simplicity  we regard a validation instance whose score is exactly zero  i.e.  w⊤x′i = 0  is correctly
classiﬁed in (2). Hereafter  we assume that there are no validation instances whose input vector is completely
0  i.e.  x′i = 0  because those instances are always correctly classiﬁed according to the deﬁnition in (2).

3

Then  for any C > 0  the score w∗⊤C x′i  i ∈ [n′]  satisﬁes
w∗⊤C x′i≥ LB(w∗⊤C x′i| ˆw ˜C) :=% α( ˆw ˜C  x′i) − 1
−β( ˆw ˜C  x′i) + 1
w∗⊤C x′i≤ U B(w∗⊤C x′i| ˆw ˜C) :=%−β( ˆw ˜C  x′i) + 1
α( ˆw ˜C  x′i) − 1

˜C
˜C

˜C
˜C

(β( ˆw ˜C  x′i) + γ(g( ˆw ˜C)  x′i))C  if C > ˜C 
(α( ˆw ˜C  x′i) + δ(g( ˆw ˜C)  x′i))C  if C < ˜C 

(α( ˆw ˜C  x′i) + δ(g( ˆw ˜C)  x′i))C  if C > ˜C 
(β( ˆw ˜C  x′i) + γ(g( ˆw ˜C)  x′i))C  if C < ˜C 

(5a)

(5b)

where
α(w∗˜C  x′i) :=

1
2
1
2

1
2
1
2

β(w∗˜C  x′i) :=

(∥w∗˜C∥∥x′i∥ + w∗⊤˜C x′i) ≥ 0 γ (g( ˆw ˜C)  x′i) :=
(∥w∗˜C∥∥x′i∥ − w∗⊤˜C x′i) ≥ 0 δ (g( ˆw ˜C)  x′i) :=

(∥g( ˆw ˜C)∥∥x′i∥ + g( ˆw ˜C)⊤x′i) ≥ 0 
(∥g( ˆw ˜C)∥∥x′i∥ − g(w ˜C)⊤x′i) ≥ 0.
The proof is presented in Appendix A. Lemma 1 tells that we have a lower and an upper bound of the
score w∗⊤C x′i for each validation instance that linearly change with the regularization parameter C.
When ˆw ˜C is optimal  it can be shown that (see Proposition B.24 in [22]) there exists a subgradient
such that g( ˆw ˜C) = 0  meaning that the bounds are tight because γ(g( ˆw ˜C)  x′i) = δ(g( ˆw ˜C)  x′i) = 0.
Corollary 2. When C = ˜C  the score w∗⊤˜C x′i  i ∈ [n′]  for the regularization parameter value ˜C
itself satisﬁes
w∗⊤˜C x′i≥LB(w∗⊤˜C x′i| ˆw ˜C) = ˆw⊤˜C x′i−γ(g( ˆw ˜C)  x′i)  w∗⊤˜C x′i≤ U B(w∗⊤˜C x′i| ˆw ˜C) = ˆw⊤˜C x′i+δ(g( ˆw ˜C)  x′i).
The results in Corollary 2 are obtained by simply substituting C = ˜C into (5a) and (5b).

3.2 Validation Error Bounds

Given a lower and an upper bound of the score of each validation instance  a lower bound of the
validation error can be computed by simply using the following facts:

y′i = +1 and U B(w∗⊤C x′i| ˆw ˜C) < 0 ⇒ mis-classiﬁed 
y′i = −1 and LB(w∗⊤C x′i| ˆw ˜C) > 0 ⇒ mis-classiﬁed.

(6a)
(6b)
Furthermore  since the bounds in Lemma 1 linearly change with the regularization parameter C  we
can identify the interval of C within which the validation instance is guaranteed to be mis-classiﬁed.
Lemma 3. For a validation instance with y′i = +1  if

˜C < C <

β( ˆw ˜C  x′i)

α( ˆw ˜C  x′i) + δ(g( ˆw ˜C)  x′i)

˜C or

α( ˆw ˜C  x′i)

β( ˆw ˜C  x′i) + γ(g( ˆw ˜C)  x′i)

˜C < C < ˜C 

then the validation instance (x′i  y′i) is mis-classiﬁed. Similarly  for a validation instance with y′i =
−1  if

˜C < C <

α( ˆw ˜C  x′i)

˜C or

β( ˆw ˜C  x′i)

˜C < C < ˜C 

β( ˆw ˜C  x′i) + γ(g( ˆw ˜C)  x′i)

α( ˆw ˜C  x′i) + δ(g( ˆw ˜C)  x′i)

then the validation instance (x′i  y′i) is mis-classiﬁed.
This lemma can be easily shown by applying (5) to (6).
As a direct consequence of Lemma 3  the lower bound of the validation error is represented as a
function of the regularization parameter C in the following form.
Theorem 4. Using an approximate solution ˆw ˜C for a regularization parameter ˜C  the validation
error Ev(w∗C) for any C > 0 satisﬁes
Ev(w∗C) ≥ LB(Ev(w∗C)| ˆw ˜C) :=

(7)

˜C(+!y′i=+1
I’
˜C(+!y′i=−1
I’

4

1

n′& !y′i=+1
+!y′i=−1

I’˜C<C<
I’˜C<C<

β( ˆw ˜C  x′i)

α( ˆw ˜C  x′i)

α( ˆw ˜C  x′i)+δ(g( ˆw ˜C)  x′i)

β( ˆw ˜C  x′i)+γ(g( ˆw ˜C)  x′i)

α( ˆw ˜C  x′i)

β( ˆw ˜C  x′i)+γ(g( ˆw ˜C)  x′i)

β( ˆw ˜C  x′i)

α( ˆw ˜C  x′i)+δ(g( ˆw ˜C)  x′i)

˜C<C< ˜C(
˜C<C< ˜C().

Algorithm 1: Computing the approximation level ε from the given set of solutions
Input: {(xi  yi)}i∈[n]  {(x′i  y′i)}i∈[n′]  Cl  Cu  W := {w ˜C1
1: Ebest
2: LB(E∗v ) ← minc∈[Cl Cu]* max ˜Ct∈{ ˜C1 ...  ˜CT } LB(Ev(w∗c )|w ˜Ct

v ← min ˜Ct∈{ ˜C1 ...  ˜CT } U B(Ev(w∗˜Ct

Output: ε = Ebest

  . . .   w ˜CT }

v − LB(E∗v )

)|w ˜Ct

)+

)

The lower bound (7) is a staircase function of the regularization parameter C.
Remark 5. We note that our validation error lower bound is inspired from recent studies on safe
screening [12  13  14  15  16]  which identiﬁes sparsity of the optimal solutions before solving the
optimization problem. A key technique used in those studies is to bound Lagrange multipliers at the
optimal  and we utilize this technique to prove Lemma 1  which is a core of our framework.

By setting C = ˜C  we can obtain a lower and an upper bound of the validation error for the regu-
larization parameter ˜C itself  which are used in the algorithm as a stopping criteria for obtaining an
approximate solution ˆw ˜C.
Corollary 6. Given an approximate solution ˆw ˜C  the validation error Ev(w∗˜C) satisﬁes
Ev(w∗˜C) ≥ LB(Ev(w∗˜C)| ˆw ˜C)
=

I  ˆw⊤˜C x′i + δ(g( ˆw ˜C)  x′i) < 0- + !y′i=−1
I  ˆw⊤˜C x′i − γ(g( ˆw ˜C)  x′i) ≥ 0- + !y′i=−1

I  ˆw⊤˜C x′i − γ(g( ˆw ˜C)  x′i) > 0-) 
I  ˆw⊤˜C x′i + δ(g( ˆw ˜C)  x′i) ≤ 0-). (8b)

n′& !y′i=+1
n′& !y′i=+1

Ev(w∗˜C) ≤ U B(Ev(w∗˜C)| ˆw ˜C)
= 1 −

(8a)

1

1

4 Algorithm
In this section we present two algorithms for each of the two problems discussed in §2. Due to the
space limitation  we roughly describe the most fundamental forms of these algorithms. Details and
several extensions of the algorithms are presented in supplementary appendices B and C.

4.1 Problem setup 1: Computing the approximation level ε from a given set of solutions
Given a set of (either optimal or approximate) solutions ˆw ˜C1
  . . .   ˆw ˜CT   obtained e.g.  by ordinary
grid-search  our ﬁrst problem is to provide a theoretical approximation level ε in the sense of (3)2.
This problem can be solved easily by using the validation error lower bounds developed in §3.2. The
in
algorithm is presented in Algorithm 1  where we compute the current best validation error Ebest
line 1  and a lower bound of the best possible validation error E∗v := minC∈[Cℓ Cu] Ev(w∗C) in line
2. Then  the approximation level ε can be simply obtained by subtracting the latter from the former.
We note that LB(E∗v )  the lower bound of E∗v  can be easily computed by using T evaluation error
lower bounds LB(Ev(w∗C)|w ˜Ct
4.2 Problem setup 2: Finding an ε-approximate regularization parameter

)  t = 1  . . .   T   because they are staircase functions of C.

v

Given a desired approximation level ε such as ε = 0.01  our second problem is to ﬁnd an ε-
approximate regularization parameter. To this end we develop an algorithm that produces a set of
optimal or approximate solutions ˆw ˜C1
  . . .   ˆw ˜CT such that  if we apply Algorithm 1 to this sequence 
then approximation level would be smaller than or equal to ε. Algorithm 2 is the pseudo-code of
this algorithm. It computes approximate solutions for an increasing sequence of regularization pa-
rameters in the main loop (lines 2-11).

2 When we only have approximate solutions ˆw ˜C1   . . .   ˆw ˜CT   Eq. (3) is slightly incorrect. The ﬁrst term of

the l.h.s. of (3) should be min ˜Ct∈{ ˜C1 ...  ˜CT } U B(Ev( ˆw ˜Ct )| ˆw ˜Ct ).

5

Let us now consider tth iteration in the main
loop  where we have already computed t−1 ap-
  . . .   ˆw ˜Ct−1 for ˜C1 <
proximate solutions ˆw ˜C1
. . . < ˜Ct−1. At this point 
Cbest := arg min

)| ˆw ˜Cτ
is the best (in worst-case) regularization param-
eter obtained so far and it is guaranteed to be an
ε-approximate regularization parameter in the
interval [Cl  ˜Ct] in the sense that the validation
error 
Ebest

˜Cτ∈{ ˜C1 ...  ˜Ct−1}

U B(Ev(w∗˜Cτ

min

:=

) 

Algorithm 2: Finding an ε approximate regular-
ization parameter with approximate solutions
Input:{(xi  yi)}i∈[n]  {(x′i  y′i)}i∈[n′]  Cl  Cu  ε
1: t ← 1  ˜Ct ← Cl  Cbest ← Cl  Ebest
v ← 1
2: while ˜Ct ≤ Cu do
ˆw ˜Ct←solve (1) approximately for C= ˜Ct
3:
) by (8b).
Compute U B(Ev(w∗˜Ct
4:
if U B(Ev(w∗˜Ct
5:
)| ˆw ˜Ct
6:
7:
8:
9:
10:
11: end while
Output: Cbest ∈C (ε).

v ← U B(Ev(w∗˜Ct
Ebest
Cbest ← ˜Ct
end if
Set ˜Ct+1 by (10)
t ← t + 1

)| ˆw ˜Ct
) < Ebest
)| ˆw ˜Ct

then
)

v

v

) 

U B(Ev(w∗˜Cτ

˜Cτ∈{ ˜C1 ...  ˜Ct−1}

)| ˆw ˜Cτ
is shown to be at most greater by ε than the
smallest possible validation error in the inter-
val [Cl  ˜Ct]. However  we are not sure whether
Cbest can still keep ε-approximation property
for C > ˜Ct. Thus  in line 3  we approx-
imately solve the optimization problem (1) at C = ˜Ct and obtain an approximate solution
ˆw ˜Ct. Note that the approximate solution ˆw ˜Ct must be sufﬁciently good enough in the sense that
) is sufﬁciently smaller than ε (typically 0.1ε). If the up-
U B(Ev(w∗˜Cτ
per bound of the validation error U B(Ev(w∗˜Cτ
and
Cbest (lines 5-8).
Our next task is to ﬁnd ˜Ct+1 in such a way that Cbest is an ε-approximate regularization parameter
in the interval [Cl  ˜Ct+1]. Using the validation error lower bound in Theorem 4  the task is to ﬁnd
the smallest ˜Ct+1 > ˜Ct that violates

) − LB(Ev(w∗˜Cτ

) is smaller than Ebest

  we update Ebest

)| ˆw ˜Cτ

)| ˆw ˜Cτ

)| ˆw ˜Cτ

v

v

v − LB(Ev(w∗C)| ˆw ˜Ct
Ebest

) ≤ ε  ∀C ∈ [ ˜Ct  Cu] 

(9)

In order to formulate such a ˜Ct+1  let us deﬁne
P := {i ∈ [n′]|y′i = +1  U B(w∗⊤˜Ct
Furthermore  let

x′i| ˆw ˜Ct

Γ:= "

  x′i)

β( ˆw ˜Ct
  x′i) + δ(g( ˆw ˜Ct

α( ˆw ˜Ct

)  x′i)

) < 0} N := {i ∈ [n′]|y′i = −1  LB(w∗⊤˜Ct
˜Ct$i∈P ∪"

  x′i) + γ(g( ˆw ˜Ct

α( ˆw ˜Ct

β( ˆw ˜Ct

)  x′i)

  x′i)

x′i| ˆw ˜Ct
˜Ct$i∈N

 

) > 0}.

and denote the kth-smallest element of Γ as kth(Γ) for any natural number k. Then  the smallest
˜Ct+1 > ˜Ct that violates (9) is given as

˜Ct+1← (⌊n′(LB(Ev(w∗˜Ct

)| ˆw ˜Ct

)−Ebest

v +ε)⌋+1)th(Γ).

(10)

5 Experiments

In this section we present experiments for illustrating the proposed methods. Table 2 summarizes
the datasets used in the experiments. They are taken from libsvm dataset repository [23]. All the
input features except D9 and D10 were standardized to [−1  1]3. For illustrative results  the in-
stances were randomly divided into a training and a validation sets in roughly equal sizes. For
quantitative results  we used 10-fold CV. We used Huber hinge loss (e.g.  [24]) which is convex
and subdifferentiable with respect to the second argument. The proposed methods are free from
the choice of optimization solvers. In the experiments  we used an optimization solver described in
[25]  which is also implemented in well-known liblinear software [26]. Our slightly modiﬁed code

3 We use D9 and D10 as they are for exploiting sparsity.

6

liver-disorders (D2)

ionosphere (D3)

australian (D4)

Figure 2: Illustrations of Algorithm 1 on three benchmark datasets (D2  D3  D4). The plots indicate
how the approximation level ε improves as the number of solutions T increases in grid-search (red) 
Bayesian optimization (blue) and our own method (green  see the main text).

(a) ε = 0.1 without tricks

(b) ε = 0.05 without tricks

(c) ε = 0.05 with tricks 1 and 2

Figure 3: Illustrations of Algorithm 2 on ionosphere (D3) dataset for (a) op2 with ε = 0.10  (b)
op2 with ε = 0.05 and (c) op3 with ε = 0.05  respectively. Figure 1 also shows the result for op3
with ε = 0.10.

(for adaptation to Huber hinge loss) is provided as a supplementary material  and is also available
on https://github.com/takeuchi-lab/RPCVELB. Whenever possible  we used warm-
start approach  i.e.  when we trained a new solution  we used the closest solutions trained so far
(either approximate or optimal ones) as the initial starting point of the optimizer. All the compu-
tations were conducted by using a single core of an HP workstation Z800 (Xeon(R) CPU X5675
(3.07GHz)  48GB MEM). In all the experiments  we set Cℓ = 10−3 and Cu = 103.

Results on problem 1 We applied Algorithm 1 in §4 to a set of solutions obtained by 1) grid-
search  2) Bayesian optimization (BO) with expected improvement acquisition function  and 3)
adaptive search with our framework which sequentially computes a solution whose validation lower
bound is smallest based on the information obtained so far. Figure 2 illustrates the results on three
datasets  where we see how the approximation level ε in the vertical axis changes as the number of
solutions (T in our notation) increases. In grid-search  as we increase the grid points  the approxi-
mation level ε tends to be improved. Since BO tends to focus on a small region of the regularization
parameter  it was difﬁcult to tightly bound the approximation level. We see that the adaptive search 
using our framework straightforwardly  seems to offer slight improvement from grid-search.

Results on problem 2 We applied Algorithm 2 to benchmark datasets for demonstrating theo-
retically guaranteed choice of a regularization parameter is possible with reasonable computational
costs. Besides the algorithm presented in §4  we also tested a variant described in supplementary
Appendix B. Speciﬁcally  we have three algorithm options. In the ﬁrst option (op1)  we used op-
In the second option (op2) 
timal solutions {w∗˜Ct}t∈[T ] for computing CV error lower bounds.
we instead used approximate solutions { ˆw ˜Ct}t∈[T ]. In the last option (op3)  we additionally used
speed-up tricks described in supplementary Appendix B. We considered four different choices of
ε ∈{ 0.1  0.05  0.01  0}. Note that ε = 0 indicates the task of ﬁnding the exactly optimal regular-

7

op1

op2

op2

op1

(using w∗˜C
T

(using w∗˜C
T

(using ˆw ˜C )
time
T
(sec)
0.031
0.061
0.194

32
70
324

Table 1: Computational costs. For each of the three options and ε ∈{ 0.10  0.05  0.01  0}  the
number of optimization problems solved (denoted as T ) and the total computational costs (denoted
as time) are listed. Note that  for op2  there are no results for ε = 0.
)
time
(sec)
1.916
4.099
16.31
57.57
8.492
16.18
57.79
1135
0.761
1.687
8.257
218.4
360.2
569.9
2901
106937

(using tricks)
time
(sec)
0.628
1.136
5.362
44.68
3.319
6.604
24.04
760.8
0.606
0.926
4.043
99.57
74.37
128.5
657.4
98631

62
123
728
2840
167
379
1735
42135
66
110
614
15218
89
200
1164
63300

92
207
1042
4276
289
601
2532
67490
72
192
1063
34920
134
317
1791
85427

(using ˆw ˜C )
time
T
(sec)
0.975
2.065
9.686

93
209
1069

62
129
778

27
65
181

201.0
280.7
1345

0.124
0.290
0.825

5.278
9.806
35.21

293
605
2788

74
195
1065

136
323
1822

0.266
0.468
0.716

0.088
0.173
0.418

0.604
1.162
6.238

223
540
2183

D3

D4

N.A.

D7

D8

D9

D1

D2

N.A.

N.A.

N.A.

N.A.

N.A.

N.A.

op3

T

D6

N.A.

63
109
440

0.108
0.171
0.631

N.A.

D10

Ev < 0.10
Ev < 0.05
157
258552

81.75
85610

Ev < 0.10
Ev < 0.05
162
31.02

N.A.

Ev < 0.10
Ev < 0.05
114
42040

36.81
23316

op3

T

(using tricks)
time
(sec)
0.041
0.057
0.157
0.629
0.084
0.218
0.623
3.805
0.277
0.359
0.940
6.344
0.093
0.153
0.399
1.205
0.091
0.137
0.401
2.451

33
57
205
383
131
367
1239
6275
43
73
270
815
23
47
156
345
45
77
258
968

ε

0.10
0.05
0.01
0
0.10
0.05
0.01
0
0.10
0.05
0.01
0
0.10
0.05
0.01
0
0.10
0.05
0.01
0

30
68
234
442
221
534
1503
10939
61
123
600
5412
27
64
167
342
62
108
421
2330

D5

)
time
(sec)
0.068
0.124
0.428
0.697
0.177
0.385
0.916
6.387
0.617
1.073
4.776
26.39
0.169
0.342
0.786
1.317
0.236
0.417
1.201
4.540

ization parameter. In some datasets  the smallest validation errors are less than 0.1 or 0.05  in which
cases we do not report the results (indicated as “Ev < 0.05” etc.). In trick1  we initially computed
solutions at four different regularization parameter values evenly allocated in [10−3  103] in the log-
arithmic scale. In trick2  the next regularization parameter ˜Ct+1 was set by replacing ε in (10) with
1.5ε (see supplementary Appendix B). For the purpose of illustration  we plot examples of valida-
tion error curves in several setups. Figure 3 shows the validation error curves of ionosphere (D3)
dataset for several options and ε.
Table 1 shows the number of optimization problems solved in the algorithm (denoted as T )  and
the total computation time in CV setups. The computational costs mostly depend on T   which gets
smaller as ε increases. Two tricks in supplementary Appendix B was effective in most cases for
reducing T . In addition  we see the advantage of using approximate solutions by comparing the
computation times of op1 and op2 (though this strategy is only for ε ̸= 0). Overall  the results
suggest that the proposed algorithm allows us to ﬁnd theoretically guaranteed approximate regular-
ization parameters with reasonable costs except for ε = 0 cases. For example  the algorithm found
an ε = 0.01 approximate regularization parameter within a minute in 10-fold CV for a dataset with
more than 50000 instances (see the results on D10 for ε = 0.01 with op2 and op3 in Table 1).

dataset name

heart

liver-disorders

ionosphere
australian

diabetes

D1
D2
D3
D4
D5

Table 2: Benchmark datasets used in the experiments.

sample size

input dimension

dataset name

sample size

270
345
351
690
768

13
6
34
14
8

D6
D7
D8
D9
D10

german.numer

svmguide3
svmguide1

a1a
w8a

1000
1284
7089
32561
64700

input dimension

24
21
4
123
300

6 Conclusions and future works

We presented a novel algorithmic framework for computing CV error lower bounds as a function
of the regularization parameter. The proposed framework can be used for a theoretically guaranteed
choice of a regularization parameter. Additional advantage of this framework is that we only need to
compute a set of sufﬁciently good approximate solutions for obtaining such a theoretical guarantee 
which is computationally advantageous. As demonstrated in the experiments  our algorithm is prac-
tical in the sense that the computational cost is reasonable as long as the approximation quality ε is
not too close to 0. An important future work is to extend the approach to multiple hyper-parameters
tuning setups.

8

References
[1] B. Efron  T. Hastie  I. Johnstone  and R. TIbshirani. Least angle regression. Annals of Statistics 

32(2):407–499  2004.

[2] T. Hastie  S. Rosset  R. Tibshirani  and J. Zhu. The entire regularization path for the support vector

machine. Journal of Machine Learning Research  5:1391–1415  2004.

[3] S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics  35:1012–1030 

2007.

[4] J. Giesen  J. Mueller  S. Laue  and S. Swiercy. Approximating Concavely Parameterized Optimization

Problems. In Advances in Neural Information Processing Systems  2012.

[5] J. Giesen  M. Jaggi  and S. Laue. Approximating Parameterized Convex Optimization Problems. ACM

Transactions on Algorithms  9  2012.

[6] J. Giesen  S. Laue  and Wieschollek P. Robust and Efﬁcient Kernel Hyperparameter Paths with Guaran-

tees. In International Conference on Machine Learning  2014.

[7] J. Mairal and B. Yu. Complexity analysis of the Lasso reguralization path. In International Conference

on Machine Learning  2012.

[8] V. Vapnik and O. Chapelle. Bounds on Error Expectation for Support Vector Machines. Neural Compu-

tation  12:2013–2036  2000.

[9] T. Joachims. Estimating the generalization performance of a SVM efﬁciently. In International Conference

on Machine Learning  2000.

[10] K. Chung  W. Kao  C. Sun  L. Wang  and C. Lin. Radius margin bounds for support vector machines with

the RBF kernel. Neural computation  2003.

[11] M. Lee  S. Keerthi  C. Ong  and D. DeCoste. An efﬁcient method for computing leave-one-out error in
support vector machines with Gaussian kernels. IEEE Transactions on Neural Networks  15:750–7  2004.
[12] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. Paciﬁc

Journal of Optimization  2012.

[13] Z. Xiang  H. Xu  and P. Ramadge. Learning sparse representations of high dimensional data on large

scale dictionaries. In Advances in Neural Information Processing Sysrtems  2011.

[14] K. Ogawa  Y. Suzuki  and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computa-

tion. In International Conference on Machine Learning  2013.

[15] J. Liu  Z. Zhao  J. Wang  and J. Ye. Safe Screening with Variational Inequalities and Its Application to

Lasso. In International Conference on Machine Learning  volume 32  2014.

[16] J. Wang  J. Zhou  J. Liu  P. Wonka  and J. Ye. A Safe Screening Rule for Sparse Logistic Regression. In

Advances in Neural Information Processing Sysrtems  2014.

[17] V. Vapnik. The Nature of Statistical Learning Theory. Springer  1996.
[18] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning. Cambridge University Press 

2014.

[19] J. Snoek  H. Larochelle  and R. Adams. Practical Bayesian Optimization of Machine Learning Algo-

rithms. In Advances in Neural Information Processing Sysrtems  2012.

[20] J. Bergstra and Y. Bengio. Random Search for Hyper-Parameter Optimization. Journal of Machine

Learning Research  13:281–305  2012.

[21] O. Chapelle  V. Vapnik  O. Bousquet  and S. Mukherjee. Choosing multiple parameters for support vector

machines. Machine Learning  46:131–159  2002.

[22] P D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  1999.
[23] C. Chang and C. Lin. LIBSVM : A Library for Support Vector Machines. ACM Transactions on Intelligent

Systems and Technology  2:1–39  2011.

[24] O. Chapelle. Training a support vector machine in the primal. Neural computation  19:1155–1178  2007.
[25] C. Lin  R. Weng  and S. Keerthi. Trust Region Newton Method for Large-Scale Logistic Regression. The

Journal of Machine Learning Research  9:627–650  2008.

[26] R. Fan  K. Chang  and C. Hsieh. LIBLINEAR: A library for large linear classiﬁcation. The Journal of

Machine Learning  9:1871–1874  2008.

9

,Atsushi Shibagaki
Yoshiki Suzuki
Masayuki Karasuyama
Ichiro Takeuchi
David Harwath
Antonio Torralba
James Glass
Alberto Bietti
Julien Mairal