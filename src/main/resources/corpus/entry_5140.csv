2008,Sparsity of SVMs that use the epsilon-insensitive loss,In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the epsilon-insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally  we briefly discuss a trade-off in epsilon between sparsity and accuracy if the SVM is used to estimate the conditional median.,Sparsity of SVMs that use the -insensitive loss

Ingo Steinwart

Information Sciences Group CCS-3
Los Alamos National Laboratory
Los Alamos  NM 87545  USA

ingo@lanl.gov

Andreas Christmann
University of Bayreuth

Department of Mathematics

D-95440 Bayreuth

Andreas.Christmann@uni-bayreuth.de

Abstract

In this paper lower and upper bounds for the number of support vectors are derived
for support vector machines (SVMs) based on the -insensitive loss function. It
turns out that these bounds are asymptotically tight under mild assumptions on the
data generating distribution. Finally  we brieﬂy discuss a trade-off in  between
sparsity and accuracy if the SVM is used to estimate the conditional median.

1 Introduction
Given a reproducing kernel Hilbert space (RKHS) of a kernel k : X × X → R and training set
D := ((x1  y1)  . . .   (xn  yn)) ∈ (X × R)n  the -insensitive SVM proposed by Vapnik and his
co-workers [10  11] for regression tasks ﬁnds the unique minimizer fD λ ∈ H of the regularized
empirical risk

(1)
where L denotes the -insensitive loss deﬁned by L(y  t) := max{0 |y − t| − } for all y  t ∈ R
and some ﬁxed  ≥ 0. It is well known  see e.g. [2  Proposition 6.21]  that the solution is of the form

L(yi  f(xi))  

H +

i=1

λ(cid:107)f(cid:107)2

n(cid:88)

1
n

n(cid:88)
n(cid:88)

i=1

i=1

(2)

(3)

fD λ =

i are a solution of the optimization problem

where the coefﬁcients β∗

i k(xi  · )  
β∗
n(cid:88)
Here we set C := 1/(2λn). Note that the equality constraint(cid:80)n

yiβi − 
−C ≤ βi ≤ C

|βi| − 1
2

n(cid:88)

maximize

i=1

βiβjk(xi  xj)

i j=1
for all i = 1  . . .   n.

i

subject to

(4)
i=1 βi = 0 needed in [2  Proposition
In the
6.21] is superﬂuous since we do not include an offset term b in the primal problem (1).
following  we write SV (fD λ) := {i : β∗
(cid:54)= 0} for the set of indices that belong to the support
vectors of fD λ. Furthermore  we write # for the counting measure  and hence #SV (fD λ) denotes
the number of support vectors of fD λ.
It is obvious from (2) that #SV (fD λ) has a crucial inﬂuence on the time needed to compute
fD λ(x). Due to this fact  the -insensitive loss was originally motivated by the goal to achieve
sparse decision functions  i.e.  decision functions fD λ with #SV (fD λ) < n. Although empiri-
cally it is well-known that the -insensitive SVM achieves this sparsity  there is  so far  no theo-
retical explanation in the sense of [5]. The goal of this work is to provide such an explanation by
establishing asymptotically tight lower and upper bounds for the number of support vectors. Based
on these bounds we then investigate the trade-off between sparsity and estimation accuracy of the
-insensitive SVM.

2 Main results

Before we can formulate our main results we need to introduce some more notations. To this end 
let P be a probability measure on X × R  where X is some measurable space. Given a measurable
f : X → R  we then deﬁne the L-risk of f by RL P(f) := E(x y)∼PL(y  f(x)). Moreover  recall
that P can be split into the marginal distribution PX on X and the regular conditional probability
P(·|x). Given a RKHS H of a bounded kernel k  [1] then showed that

fP λ := arg inf
f∈H

λ(cid:107)f(cid:107)2

H + RL P(f)

(cid:80)n

exists and is uniquely determined whenever RL P(0) < ∞. Let us write δ(x y) for the Dirac
measure at some (x  y) ∈ X × R. By considering the empirical measure D := 1
i=1 δ(xi yi) of a
training set D := ((x1  y1)  . . .   (xn  yn)) ∈ (X × R)n  we then see that the corresponding fD λ is
the solution of (1). Finally  we need to introduce the sets

n

:= (cid:8)(x  y) ∈ X × R : |f(x) − y| >  + δ(cid:9)
:= (cid:8)(x  y) ∈ X × R : |f(x) − y| ≥  − δ(cid:9)  

low(f)
Aδ
up(f)
Aδ

where f : X → R is an arbitrary function and δ ∈ R. Moreover  we use the short forms Alow(f) :=
low(f) and Aup(f) := A0
A0
Theorem 2.1 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Then  for all n ≥ 1  ρ > 0  δ > 0  and λ > 0 satisfying
δλ ≤ 4  we have

up(f). Now we can formulate our ﬁrst main result.

Pn(cid:16)
Pn(cid:16)

and

D ∈ (X × R)n :

#SV (fD λ)

n

D ∈ (X × R)n :

#SV (fD λ)

n

> P(cid:0)Aδ
< P(cid:0)Aδ

low(fP λ)(cid:1) − ρ
up(fP λ)(cid:1) + ρ

(cid:17) ≥ 1 − 3e− δ2λ2n
(cid:17) ≥ 1 − 3e− δ2λ2n

16 − e−2ρ2n

16 − e−2ρ2n .

lim

Before we present our second main result  we brieﬂy illustrate Theorem 2.1 for the case where we
ﬁx the regularization parameter λ and let n → ∞.
Corollary 2.2 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Then  for all ρ > 0 and λ > 0  we have

D ∈ (X × R)n : P(cid:0)Alow(fP λ)(cid:1) − ρ ≤ #SV (fD λ)

(cid:17)
n→∞ Pn(cid:16)
≤ P(cid:0)Aup(fP λ)(cid:1) + ρ
Aup(fP λ)\Alow(fP λ) =(cid:8)(x  fP λ(x) − ) : x ∈ X(cid:9) ∪(cid:8)(x  fP λ(x) + ) : x ∈ X(cid:9) .

Note that the above corollary exactly describes the asymptotic behavior of the fraction of support
vectors modulo the probability of the set

In particular  if the conditional distributions P(·|x)  x ∈ X  have no discrete components  then the
above corollary gives an exact description.
Of course  in almost no situation it is realistic to assume that λ stays ﬁxed if the sample size n grows.
Instead  it is well-known  see [1]  that the regularization parameter should vanish in order to achieve
consistency. To investigate this case  we need to introduce some additional notations from [6] that
are related to the L-risk. Let us begin by denoting the Bayes L-risk by R∗
L P := inf RL P(f) 
where P is a distribution and the inﬁmum is taken over all measurable functions f : X → R. In
addition  given a distribution Q on R  [6] and [7  Chapter 3] deﬁned the inner L-risks by

= 1 .

n

CL Q(t) :=

L(y  t) dQ(y)  

t ∈ R 

and the minimal inner L-risks were denoted by C∗

L Q := inf t∈R CL Q(t). Obviously  we have

RL P(f) =

CL P( · |x)

(cid:0)f(x)(cid:1) dPX(x)  

(5)

(cid:90)

R

(cid:90)

X

(cid:82)

and [6  Lemma 2.5]  see also [7  Lemma 3.4]  further established the intuitive formula R∗
X C∗

L P( · |x) dPX(x). Moreover  we need the sets of conditional minimizers

L P =

M∗(x) :=(cid:8)t ∈ R : CL P( · |x)(t) = C∗

(cid:9).

L P( · |x)

The following lemma collects some useful properties of these sets.
Lemma 2.3 Let P be a probability measure on X × R with R∗
empty and compact interval for PX-almost all x ∈ X.
Given a function f : X → R  Lemma 2.3 shows that for PX-almost all x ∈ X there exists a unique
t∗(x) ∈ M∗(x) such that

L P < ∞. Then M∗(x) is a non-

(6)
In other words  t∗(x) is the element in M∗(x) that has the smallest distance to f(x). In the follow-
ing  we sometimes write t∗
λ(x) := t∗(x) if f = fP λ and we wish to emphasize the dependence of
t∗(x) on λ. With the help of these elements  we ﬁnally introduce the sets

for all t ∈ M∗(x) .

(cid:12)(cid:12)t∗(x) − f(x)(cid:12)(cid:12) ≤(cid:12)(cid:12)t − f(x)(cid:12)(cid:12)

:= (cid:8)(x  y) ∈ X × R : |t∗(x) − y| >  + δ(cid:9)
:= (cid:8)(x  y) ∈ X × R : |t∗(x) − y| ≥  − δ(cid:9)  

low(f)
M δ
up(f)
M δ

up(f). Now we can formulate our second main result.

where δ ∈ R. Moreover  we again use the short forms Mlow(f) := M 0
M 0
Theorem 2.4 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL P(0) < ∞ and that H is dense in
L1(PX). Then  for all ρ > 0  there exist a δρ > 0 and a λρ > 0 such that for all λ ∈ (0  λρ] and all
n ≥ 1 we have

low(f) and Mup(f) :=

D ∈ (X × R)n : P(cid:0)Mlow(fP λ)(cid:1)− ρ ≤ #SV (fD λ)

≤ P(cid:0)Mup(fP λ)(cid:1) + ρ

(cid:17) ≥ 1− 8e−δ2

Pn(cid:16)

ρλ2n.

n

nn → ∞  then the
If we choose a sequence of regularization parameters λn such that λn → 0 and λ2
resulting SVM is L-risk consistent under the assumptions of Theorem 2.4  see [1]. For this case 
the following obvious corollary of Theorem 2.4 establishes lower and upper bounds on the number
of support vectors.
Corollary 2.5 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL P(0) < ∞ and that H is dense in
nn → ∞. Then  for all
L1(PX). Furthermore  let (λn) ∈ (0 ∞) be a sequence with λn → 0 and λ2
ρ > 0  the probability Pn of D ∈ (X × R)n satisfying

m→∞ P(cid:0)Mlow(fP λm)(cid:1) − ρ ≤ #SV (fD λn)

lim inf

n

≤ lim sup
m→∞

P(cid:0)Mup(fP λm)(cid:1) + ρ

converges to 1 for n → ∞.

In general  the probabilities of the sets Mlow(fP λ) and Mup(fP λ) are hard to control since  e.g. 
for ﬁxed x ∈ X and λ → 0 it seems difﬁcult to show that fP λ(x) is not “ﬂipping” from the left
hand side of M∗(x) to the right hand side. Indeed  for general M∗(x)  such ﬂipping would give
λ(x) ∈ M∗(x) for λ → 0  and hence would result in signiﬁcantly different sets
different values t∗
Mlow(fP λ) and Mup(fP λ). As a consequence  it seems hard to show that  for probability measures
P whose conditional distributions P(·|x)  x ∈ X  have no discrete components  we always have

P(cid:0)Mlow(fP λ)(cid:1) = lim sup

P(cid:0)Mup(fP λ)(cid:1) .

(7)

lim inf
λ→0

λ→0

However  there are situations in which this equality can easily be established. For example  assume
that the sets M∗(x) are PX-almost surely singletons. In this case  t∗
λ(x) is in fact independent of λ 
and hence so are Mlow(fP λ) and Mup(fP λ). Namely  in this case these sets contain the pairs (x  y)

for which y is not contained in the closed or open -tube around M∗(x)  respectively. Consequently 
(7) holds provided that the conditional distributions P(·|x)  x ∈ X  have no discrete components 
and hence Corollary 2.5 gives a tight bound on the number of support vectors. Moreover  if in
this case we additionally assume  = 0  i.e.  we consider the absolute loss  then we easily ﬁnd
P(Mlow(fP λ)) = P(Mup(fP λ)) = 1  and hence Corollary 2.5 shows that the corresponding SVM
does not tend to produce sparse decision functions. Finally  recall that for this speciﬁc loss function 
M∗(x) equals the median of P(·|x)  and hence M∗(x) is a singleton whenever the median of
P(·|x) is unique.
Let us now illustrate Corollary 2.5 for  > 0. To this end  we assume in the following that the
conditional distributions P(·|x) are symmetric  i.e.  for PX-almost all x ∈ X there exists a con-
ditional center c(x) ∈ R such that P(c(x) + A|x) = P(c(x) − A|x) for all measurable A ⊂ R.
Note that by considering A := [0 ∞) it is easy to see that c(x) is a median of P(·|x). Further-
more  the assumption RL P(0) < ∞ imposed in the results above ensures that the conditional
P(x) := E(Y |x) of P(·|x) exists PX-almost surely  and from this it is easy to conclude that
mean f∗
P(x) for PX-almost all x ∈ X. Moreover  from [8  Proposition 3.2 and Lemma 3.3] we
c(x) = f∗
immediately obtain the following lemma.
Lemma 2.6 Let P be a probability measure on X × R such that RL P(0) < ∞. Assume that the
conditional distributions P(·|x)  x ∈ X  are symmetric and that for PX-almost all x ∈ X there
exists a δ(x) > 0 such that for all δ ∈ (0  δ(x)] we have

P(x) + [−δ  δ](cid:12)(cid:12)x(cid:1) > 0  
P(cid:0)f∗
P(x) + [ − δ   + δ](cid:12)(cid:12)x(cid:1) > 0 .
P(cid:0)f∗

(8)
(9)
P(x) equals PX-almost surely

Then  for PX-almost all x ∈ X  we have M∗(x) = {f∗
the unique median of P(·|x).

P(x)} and f∗

P  whereas (9) means that the conditional distributions have some mass around f∗

Obviously  condition (8) means that the conditional distributions have some mass around their me-
P ± . More-
dian f∗
over  [8] showed that under the assumptions of Lemma 2.6  the corresponding -insensitive SVM can
be used to estimate the conditional median. Let us now illustrate how the value of  inﬂuences both
the accuracy of this estimate and the sparsity. To this end  let us assume for the sake of simplicity
that the conditional distributions P(·|x) have continuous Lebesgue densities p(·|x) : R → [0 ∞).
By the symmetry of the conditional distributions it is then easy to see that these densities are sym-
metric around f∗
P(x). Now  it follows from the continuity of the densities  that (8) is satisﬁed if
P(x) + |x) > 0. Let us ﬁrst consider the case where
p(f∗
the conditional distributions are equal modulo translations. In other words  we assume that there
exists a continuous Lebesgue density q : R → [0 ∞) which is symmetric around 0 such that for
PX-almost all x ∈ X we have

P(x)|x) > 0  whereas (9) is satisﬁed if p(f∗

q(y) = p(f∗

P(x) + y|x)  

y ∈ R.

Note that this assumption is essentially identical to a classical “signal plus noise” assumption. In
the following we further assume that q is unimodal  i.e.  q has its only local and global maximum
at 0. From this we easily see that (8) is satisﬁed  and (9) is satisﬁed if q() > 0. By Lemma
2.6 and the discussion around (7) we then conclude that under the assumptions of Corollary 2.5
the fraction of support vectors asymptotically approaches 2Q([ ∞))  where Q is the probability
measure deﬁned by q. This conﬁrms the intuition that larger values of  lead to sparser decision
functions. In particular  if Q([ ∞)) = 0  the corresponding SVM produces super sparse decision
functions  i.e.  decision functions whose number of support vectors does not grow linearly in the
sample size. However  not surprisingly  there is a price to be paid for this sparsity. Indeed  [8 
Lemma 3.3] indicates that the size of q() has a direct inﬂuence on the ability of fD λ to estimate
the conditional median f∗
P. Let us describe this in a little more detail. To this end  we ﬁrst ﬁnd by
[8  Lemma 3.3] and the convexity of t (cid:55)→ CL Q(t) that

CL Q(t) − C∗

L Q ≥ q() ·

(cid:26)t2/2
(cid:113)RL P(f) − R∗
P(cid:107)L1(PX ) ≤(cid:112)2/q()

t − 2/2

if t ∈ [0  ]
if t ≥ .

L P  

(cid:107)f − f∗

By a literal repetition of the proof of [8  Theorem 2.5] we then ﬁnd the self-calibration inequality

(10)

which holds for all f : X → R with RL P(f) − R∗
L P ≤ 2/2. Now  if we are in the situation
L P in probability for n → ∞  and thus
of Corollary 2.5  then we know that RL P(fD λn) → R∗
(10) shows that fD λn approximates the conditional median f∗
P with respect to the L1(PX)-norm.
However  the guarantee for this approximation becomes worse the smaller q() becomes  i.e.  the
In other words  the sparsity of the decision functions may be paid by less accurate
larger  is.
estimates of the conditional median. On the other hand  our results also show that moderate values
for  can lead to both reasonable estimates of the conditional median and relatively sparse decision
functions. In this regard we further note that one can also use [8  Lemma 3.3] to establish self-
calibration inequalities that measure the distance of f to f∗
P only up to . In this case  however  it
is obvious that such self-calibration inequalities are worse the larger  is  and hence the informal
conclusions above remain unchanged.
Finally  we like to mention that  if the conditional distributions are not equal modulo transla-
In particular  if we are in a situation with
tions  then the situation may become more involved.
P(x) + |x) = 0  self-
P(x)|x) = inf x p(f∗
p(f∗
calibration inequalities of the form (10) are in general impossible  and weaker self-calibration in-
equalities require additional assumptions on P. We refer to [8] where the case  = 0 is considered.

P(x) + |x) > 0 but inf x p(f∗

P(x)|x) > 0 and p(f∗

3 Proofs

Setting C := 1

2λn and introducing slack variables  we can restate the optimization problem (1) as

In the following we denote the (unique) solution of (11) by (f∗  ξ∗  ˜ξ∗)  where we note that we have
f∗ = fD λ. It is well-known  see e.g. [2  p. 117]  that the dual optimization problem of (11) is

for all i = 1  . . .   n.

maximize

subject to

yi(˜αi − αi) − 

i=1

0 ≤ αi  ˜αi ≤ C

where k is the kernel of the RKHS H. Furthermore  if (α∗
(12)  then we can recover the primal solution (f∗  ξ∗  ˜ξ∗) by

for all i = 1  . . .   n 
1  ˜α∗

(˜αi − αi)(˜αj − αj)k(xi  xj)

(12)

1  . . .   α∗

n  ˜α∗

n) denotes a solution of

minimize

subject to

n(cid:88)

n(cid:88)

(ξi + ˜ξi)

i=1

H + C

1
(cid:107)f(cid:107)2
2
f(xi) − yi ≤  + ξi 
yi − f(xi) ≤  + ˜ξi 
ξi  ˜ξi ≥ 0

n(cid:88)
(˜αi + αi) − 1
2

i=1

n(cid:88)

i j=1

n(cid:88)

(˜α∗

i − α∗

i )k(xi  · )  

f∗ =
i = max{0  f∗(xi) − yi − }  
ξ∗
i = max{0  yi − f∗(xi) − }  
˜ξ∗

i=1

(11)

(13)

(14)
(15)

for all i = 1  . . .   n. Moreover  the Karush-Kuhn-Tucker conditions of (12) are

i (f∗(xi) − yi −  − ξ∗
α∗
i ) = 0  
i (yi − f∗(xi) −  − ˜ξ∗
˜α∗
i ) = 0  
i − C)ξ∗
i = 0  
i − C)˜ξ∗
i = 0  
ξ∗
˜ξ∗
i = 0  
α∗
i ˜α∗
i = 0  

(16)
(17)
(18)
(19)
(20)
(21)
where i = 1  . . .   n. Finally  note that by setting βi := ˜αi − αi the problem (12) can be simpliﬁed
to (3)  and consequently  a solution β∗ of (3) is of the form β∗ = ˜α∗ − α∗. The following simple
lemma provides lower and upper bounds for the set of support vectors.

(α∗
(˜α∗

i

Lemma 3.1 Using the above notations we have

(cid:8)i : |fD λ(xi) − yi| > (cid:9) ⊂(cid:8)i : β∗

i (cid:54)= 0(cid:9) ⊂(cid:8)i : |fD λ(xi) − yi| ≥ (cid:9) .

Proof: Let us ﬁrst prove the inclusion on the left hand side. To this end  we begin by ﬁxing an index
i with fD λ(xi) − yi > . By fD λ = f∗ and (14)  we then ﬁnd ξ∗
i > 0  and hence (18) implies
i − α∗
i = −C (cid:54)= 0. The case
i = ˜α∗
α∗
i = C. From (21) we conclude ˜α∗
yi − fD λ(xi) >  can be shown analogously  and hence we obtain the ﬁrst inclusion. In order to
i − α∗
show the second inclusion we ﬁx an index i with β∗
i = ˜α∗
i and (21) we then have
i (cid:54)= 0 and ˜α∗
i (cid:54)= 0 or ˜α∗
i (cid:54)= 0. Let us ﬁrst consider the case α∗
either α∗
i = 0. The KKT condition (16)
together with fD λ = f∗ implies fD λ(xi)− yi −  = ξ∗
i ≥ 0 we get fD λ(xi)− yi ≥ .
i and since ξ∗
The second case ˜α∗
i = 0 can be shown analogously.

i = 0 and hence we have β∗
i (cid:54)= 0. By β∗

We further need the following Hilbert space version of Hoeffding’s inequality from [12  Chapter 3] 
see also [7  Chapter 6.2] for a slightly sharper inequality.
Theorem 3.2 Let (Ω A  P) be a probability space and H be a separable Hilbert space. Moreover 
let η1  . . .   ηn : Ω → H be independent random variables satisfying EPηi = 0 and (cid:107)ηi(cid:107)∞ ≤ 1 for
all i = 1  . . .   n. Then  for all τ ≥ 1 and all n ≥ τ  we have

(cid:16)(cid:13)(cid:13)(cid:13) 1

n

P

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)H

ηi

< 4

(cid:114) τ

n

(cid:17) ≥ 1 − 3e−τ .

Finally  we need the following theorem  see [7  Corollary 5.10]  which was essentially shown by
[13  5  3] .
Theorem 3.3 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. We write Φ : X → H for the canonical feature map of H 
i.e.  Φ(x) := k(·   x)  x ∈ X. Then for all λ > 0 there exists a function h : X × R → [−1  1] such
that for all n ≥ 1 and all D ∈ (X × R)n we have

(cid:107)fD λ − fP λ(cid:107)H ≤ λ−1(cid:107)EDhΦ − EPhΦ(cid:107)H  

where ED denotes the empirical average with respect to D.
Proof of of Theorem 2.1: In order to show the ﬁrst estimate we ﬁx a δ > 0 and a λ > 0 such that
δλ ≤ 4. Let τ := λ2δ2n/16 which implies n ≥ τ. Combining Theorems 3.2 and 3.3 we then obtain

1 − 3e−τ ≤ Pn(cid:0)D ∈ (X × R)n : (cid:107)EDhΦ − EPhΦ(cid:107)H ≤ 4(cid:112)τ /n(cid:1)

≤ Pn(cid:0)D ∈ (X × R)n : (cid:107)fD λ − fP λ(cid:107)H ≤ δ(cid:1) .

(22)
Let us now assume that we have a training set D ∈ (X × R)n such that (cid:107)fP λ − fD λ(cid:107)H ≤ δ. Given
a pair (x  y) ∈ Aδ

low(fP λ)  we then have

 + δ < |fP λ(x) − y| ≤ |fD λ(x) − y| + |fP λ(x) − fD λ(x)| ≤ |fD λ(x) − y| + δ

#SV (fD λ) ≥ #(cid:8)i : |fD λ(xi) − yi| > (cid:9) ≥ #(cid:8)i : |fP λ(xi) − yi| >  + δ(cid:9)

by the triangle inequality and (cid:107)k(cid:107)∞ ≤ 1 which implies (cid:107) · (cid:107)∞ ≤ (cid:107) · (cid:107)H. In other words  we have
low(fP λ) ⊂ Alow(fD λ). Consequently  Lemma 3.1 yields
Aδ
n(cid:88)

=

1Aδ

low(fP λ)(xi  yi) .

Combining this estimate with (22) we then obtain
≥ 1
n

D ∈ (X × R)n :

#SV (fD λ)

n

i=1

1Aδ

low(fP λ)(xi  yi)

n(cid:88)

i=1

Moreover  Hoeffding’s inequality  see  e.g. [4  Theorem 8.1]  shows

Pn(cid:16)
Pn(cid:16)

D ∈ (X × R)n :

n(cid:88)

i=1

1
n

low(fP λ)(xi  yi) > P(cid:0)Aδ

low(fP λ)(cid:1) − ρ

1Aδ

(cid:17) ≥ 1 − 3e− δ2λ2n
(cid:17) ≥ 1 − e−2ρ2n

16

.

for all ρ > 0 and n ≥ 1. From these estimates and a union bound we conclude the ﬁrst inequality.
In order to show the second estimate we ﬁrst observe that for training sets D ∈ (X × R)n with
(cid:107)fP λ − fD λ(cid:107)H ≤ δ we have Aup(fD λ) ⊂ Aδ

up(fP λ). Lemma 3.1 then shows

and hence (22) yields

Pn(cid:16)

D ∈ (X × R)n :

#SV (fD λ) ≤ n(cid:88)
n(cid:88)

#SV (fD λ)

i=1

≤ 1
n

i=1

1Aδ

up(fP λ)(xi  yi)  

(cid:17) ≥ 1 − 3e− δ2λ2n

16

.

1Aδ

up(fP λ)(xi  yi)

n

(cid:91)

δ>0

Using Hoeffding’s inequality analogously to the proof of the ﬁrst estimate we then obtain the second
estimate.

Proof of of Corollary 2.2: We ﬁrst observe that we have Aδ
Let us show

low(fP λ) for 0 ≤ δ(cid:48) ≤ δ.
(23)
Obviously  the inclusion “⊂” directly follows from the above monotonicity. Conversely  for (x  y) ∈
Alow(fP λ) we have |f(x) − y| >  and hence |f(x) − y| >  + δ for some δ > 0  i.e.  we have
shown (x  y) ∈ Aδ

low(fP λ). From (23) we now conclude

low(fP λ) = Alow(fP λ) .

low(fP λ) ⊂ Aδ(cid:48)

Aδ

δ>0

lim
δ(cid:38)0

P(cid:0)Aδ
low(fP λ)(cid:1) = P(cid:0)Alow(fP λ)(cid:1) .
(cid:92)
up(fP λ) ⊂ Aδ
up(fP λ) for 0 ≤ δ(cid:48) ≤ δ  and it is easy to check that
Aδ

up(fP λ) = Aup(fP λ) .

In addition  we have Aδ(cid:48)

(24)

(25)

up(fP λ) for all δ > 0 we have |f(x) − y| ≥  − δ for all δ > 0  from which
Indeed  if (x  y) ∈ Aδ
we conclude |f(x)− y| ≥   i.e. (x  y) ∈ Aup(fP λ). Conversely  the inclusion “⊃” directly follows
from the above monotonicity of the sets Aup. From (25) we then conclude

P(cid:0)Aδ
up(fP λ)(cid:1) = P(cid:0)Aup(fP λ)(cid:1) .

lim
δ(cid:38)0

Let us now ﬁx a decreasing sequence (δn) ⊂ (0  1) with δn → 0 and δ2
and (26) with the estimates of Theorem 2.1  we then obtain the assertion.

(26)
nn → ∞. Combining (24)

Proof of Lemma 2.3: Since the loss function L is Lipschitz continuous and convex in t  it is easy
to verify that t (cid:55)→ CL P( · |x)(t) is Lipschitz continuous and convex for PX-almost all x ∈ X  and
hence M∗(x) is a closed interval. In order to prove the remaining assertions it sufﬁces to show
that limt→±∞ CL P( · |x)(t) = ∞ for PX-almost all x ∈ X. To this end  we ﬁrst observe that
R∗
L P < ∞ implies C∗
L P( · |x) < ∞ for PX-almost all x ∈ X. Let us ﬁx such an x  a B > 0 
and a sequence (tn) ⊂ R with tn → −∞. By the shape of L  there then exists an r0 > 0 such
that L(y  t) ≥ 2B for all y  t ∈ R with |y − t| ≥ r0. Furthermore  there exists an M > 0 with
P([−M  M]| x) ≥ 1/2  and since tn → −∞ there further exists an n0 ≥ 1 such that tn ≤ −M −r0
for all n ≥ n0. For y ∈ [−M  M] we thus have y − tn ≥ r0  and hence we ﬁnally ﬁnd

CL P( · |x)(tn) ≥

L(y  tn) dP(y|x) ≥ B

[−M M ]

for all n ≥ n0. The case tn → ∞ can be shown analogously.
For the proof of Theorem 2.4 we need the following two intermediate results.
Theorem 3.4 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL P(0) < ∞ and that H is dense in
L1(PX). Then  for all δ > 0 and ρ > 0  there exists a λ0 > 0 such that for all λ ∈ (0  λ0] we have

(cid:0)(cid:8)x ∈ X : |fP λ(x) − t| > δ for all t ∈ M∗(x)(cid:9)(cid:1) < ρ .

PX

(cid:90)

Proof: Since H is dense in L1(PX) we have inf f∈H RL P(f) = R∗
hence limλ→0 RL P(fP λ) = R∗
Lemma 3.5 Let P be a probability measure on X × R and H be a separable RKHS with bounded
measurable kernel satisfying (cid:107)k(cid:107)∞ ≤ 1. Assume that RL P(0) < ∞ and that H is dense in
L1(PX). Then  for all δ > 0 and ρ > 0  there exists a λ0 > 0 such that for all λ ∈ (0  λ0] we have

L P. Now we obtain the assertion from [6  Theorem 3.16].

L P by [9  Theorem 3]  and

up(fP λ)(cid:1) − ρ .
low(fP λ)(cid:1) + ρ
low(fP λ) ∩(cid:8)(x  y) ∈ X × R : |fP λ(x) − t∗
∪(cid:8)(x  y) ∈ X × R : |fP λ(x) − t(x)| > δ for all t(x) ∈ M∗(x)(cid:9) .

P(cid:0)M 2δ
up(fP λ)(cid:1) ≥ P(cid:0)Aδ
λ(x)| ≤ δ(cid:9)(cid:17)

λ(x) for the real number deﬁned by (6) for f(x) := fP λ(x). Then we have

P(cid:0)M 2δ
low(fP λ)(cid:1) ≤ P(cid:0)Aδ
low(fP λ) ⊂ (cid:16)

Proof: We write t∗

M 2δ

M 2δ

and

Moreover  given an (x  y) ∈ M 2δ

low(fP λ) ∩ {(x  y) ∈ X × R : |fP λ(x) − t∗

λ(x)| ≤ δ}  we ﬁnd

 + 2δ < |t∗

λ(x) − y| ≤ |fP λ(x) − t∗

λ(x)| + |fP λ(x) − y| ≤ δ + |fP λ(x) − y|  

Aδ

Aδ

up(fP λ) ⊂ (cid:16)

i.e.  we have (x  y) ∈ Aδ
then yields the ﬁrst assertion. In order to prove the second estimate we ﬁrst observe that

low(fP λ). Estimating the probability of the remaining set by Theorem 3.4

up(fP λ) ∩(cid:8)(x  y) ∈ X × R : |fP λ(x) − t∗
∪(cid:8)(x  y) ∈ X × R : |fP λ(x) − t(x)| > δ for all t(x) ∈ M∗(x)(cid:9) .

λ(x)| ≤ δ(cid:9)(cid:17)

For (x  y) ∈ Aδ

up(fP λ) ∩ {(x  y) ∈ X × R : |fP λ(x) − t∗
λ(x)| + |t∗

 − δ ≤ |fP λ(x) − y| ≤ |fP λ(x) − t∗

λ(x) − y|  
up(fP λ). Again  the assertion now follows from Theorem 3.4 .

i.e.  we have (x  y) ∈ M 2δ
Proof of Theorem 2.4: Analogously to the proofs of (24) and (26)  we ﬁnd

λ(x)| ≤ δ} we further have
λ(x) − y| ≤ δ + |t∗

P(cid:0)M δ
low(fP λ)(cid:1) = P(cid:0)Mlow(fP λ)(cid:1)

P(cid:0)M δ
up(fP λ)(cid:1) = P(cid:0)Mup(fP λ)(cid:1)

lim
δ(cid:38)0

and

lim
δ(cid:38)0

Combining these equations with Theorem 2.1 and Lemma 3.5  we then obtain the assertion.

References
[1] A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression. Bernoulli 

13:799–819  2007.

[2] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University

Press  Cambridge  2000.

[3] E. De Vito  L. Rosasco  A. Caponnetto  M. Piana  and A. Verri. Some properties of regularized kernel

methods. J. Mach. Learn. Res.  5:1363–1390  2004.

[4] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer  New

York  1996.

[5] I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res.  4:1071–1105  2003.
[6] I. Steinwart. How to compare different loss functions. Constr. Approx.  26:225–287  2007.
[7] I. Steinwart and A. Christmann. Support Vector Machines. Springer  New York  2008.
[8] I. Steinwart and A. Christmann. How SVMs can estimate quantiles and the median. In J.C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20  pages 305–312.
MIT Press  Cambridge  MA  2008.

[9] I. Steinwart  D. Hush  and C. Scovel. Function classes that approximate the Bayes risk. In G. Lugosi
and H. U. Simon  editors  Proceedings of the 19th Annual Conference on Learning Theory  pages 79–93.
Springer  New York  2006.

[10] V. Vapnik  S. Golowich  and A. Smola. Support vector method for function approximation  regression
estimation  and signal processing. In M. Mozer  M. Jordan  and T. Petsche  editors  Advances in Neural
Information Processing Systems 9  pages 81–287. MIT Press  Cambridge  MA  1997.

[11] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons  New York  1998.
[12] V. Yurinsky. Sums and Gaussian Vectors. Lecture Notes in Math. 1617. Springer  Berlin  1995.
[13] T. Zhang. Convergence of large margin separable linear classiﬁcation. In T. K. Leen  T. G. Dietterich  and
V. Tresp  editors  Advances in Neural Information Processing Systems 13  pages 357–363. MIT Press 
Cambridge  MA  2001.

,Rajesh Ranganath
Dustin Tran
Jaan Altosaar
David Blei