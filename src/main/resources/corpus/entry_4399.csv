2015,Principal Differences Analysis: Interpretable Characterization of Differences between Distributions,We introduce principal differences analysis for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device  it requires no assumptions about the form of the underlying distributions  nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation.  In addition to deriving some convergence results  we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.,Principal Differences Analysis: Interpretable

Characterization of Differences between Distributions

Jonas Mueller
CSAIL  MIT

jonasmueller@csail.mit.edu

Abstract

Tommi Jaakkola

CSAIL  MIT

tommi@csail.mit.edu

We introduce principal differences analysis (PDA) for analyzing differences be-
tween high-dimensional distributions. The method operates by ﬁnding the pro-
jection that maximizes the Wasserstein divergence between the resulting univari-
ate populations. Relying on the Cramer-Wold device  it requires no assumptions
about the form of the underlying distributions  nor the nature of their inter-class
differences. A sparse variant of the method is introduced to identify features re-
sponsible for the differences. We provide algorithms for both the original minimax
formulation as well as its semideﬁnite relaxation. In addition to deriving some
convergence results  we illustrate how the approach may be applied to identify dif-
ferences between cell populations in the somatosensory cortex and hippocampus
as manifested by single cell RNA-seq. Our broader framework extends beyond
the speciﬁc choice of Wasserstein divergence.

1

Introduction

Understanding differences between populations is a common task across disciplines  from biomed-
ical data analysis to demographic or textual analysis. For example  in biomedical analysis  a set of
variables (features) such as genes may be proﬁled under different conditions (e.g. cell types  disease
variants)  resulting in two or more populations to compare. The hope of this analysis is to answer
whether or not the populations differ and  if so  which variables or relationships contribute most to
this difference. In many cases of interest  the comparison may be challenging primarily for three
reasons: 1) the number of variables proﬁled may be large  2) populations are represented by ﬁnite 
unpaired  high-dimensional sets of samples  and 3) information may be lacking about the nature of
possible differences (exploratory analysis).
We will focus on the comparison of two high dimensional populations. Therefore  given two un-
paired i.i.d. sets of samples Xpnq “ xp1q  . . .   xpnq „ PX and Ypmq “ yp1q  . . .   ypmq „ PY   the
goal is to answer the following two questions about the underlying multivariate random variables
X  Y P Rd: (Q1) Is PX “ PY ? (Q2) If not  what is the minimal subset of features S Ñ t1  . . .   du
such that the marginal distributions differ PXS ‰ PYS while PXSC « PYSC for the complement? A
ﬁner version of (Q2) may additionally be posed which asks how much each feature contributes to
the overall difference between the two probability distributions (with respect to the given scale on
which the variables are measured).
Many two-sample analyses have focused on characterizing limited differences such as mean shifts
[1  2]. More general differences beyond the mean of each feature remain of interest  however  includ-
ing variance/covariance of demographic statistics such as income. It is also undesirable to restrict
the analysis to speciﬁc parametric differences  especially in exploratory analysis where the nature
of the underlying distributions may be unknown. In the univariate case  a number of nonparametric
tests of equality of distributions are available with accompanying concentration results [3]. Popu-
lar examples of such divergences (also referred to as probability metrics) include: f-divergences

1

(Kullback-Leibler  Hellinger  total-variation  etc.)  the Kolmogorov distance  or the Wasserstein
metric [4]. Unfortunately  this simplicity vanishes as the dimensionality d grows  and complex
test-statistics have been designed to address some of the difﬁculties that appear in high-dimensional
settings [5  6  7  8].
In this work  we propose the principal differences analysis (PDA) framework which circumvents the
curse of dimensionality through explicit reduction back to the univariate case. Given a pre-speciﬁed
statistical divergence D which measures the difference between univariate probability distributions 
PDA seeks to ﬁnd a projection  which maximizes DpT X  T Y q subject to the constraints ||||2 §
1  1 • 0 (to avoid underspeciﬁcation). This reduction is justiﬁed by the Cramer-Wold device 
which ensures that PX ‰ PY if and only if there exists a direction along which the univariate linearly
projected distributions differ [9  10  11]. Assuming D is a positive deﬁnite divergence (meaning it is
nonzero between any two distinct univariate distributions)  the projection vector produced by PDA
can thus capture arbitrary types of differences between high-dimensional PX and PY . Furthermore 
the approach can be straightforwardly modiﬁed to address (Q2) by introducing a sparsity penalty on
 and examining the features with nonzero weight in the resulting optimal projection. The resulting
comparison pertains to marginal distributions up to the sparsity level. We refer to this approach as
sparse differences analysis or SPARDA.

2 Related Work

The problem of characterizing differences between populations  including feature selection  has re-
ceived a great deal of study [2  12  13  5  1]. We limit our discussion to projection-based methods
which  as a family of methods  are closest to our approach. For multivariate two-class data  the most
widely adopted methods include (sparse) linear discriminant analysis (LDA) [2] and the logistic
lasso [12]. While interpretable  these methods seek speciﬁc differences (e.g.  covariance-rescaled
average differences) or operate under stringent assumptions (e.g.  log-linear model). In contrast 
SPARDA (with a positive-deﬁnite divergence) aims to ﬁnd features that characterize a priori un-
speciﬁed differences between general multivariate distributions.
Perhaps most similar to our general approach is Direction-Projection-Permutation (DiProPerm) pro-
cedure of Wei et al. [5]  in which the data is ﬁrst projected along the normal to the separating hyper-
plane (found using linear SVM  distance weighted discrimination  or the centroid method) followed
by a univariate two-sample test on the projected data. The projections could also be chosen at
random [1]. In contrast to our approach  the choice of the projection in such methods is not opti-
mized for the test statistics. We note that by restricting the divergence measure in our technique 
methods such as the (sparse) linear support vector machine [13] could be viewed as special cases.
The divergence in this case would measure the margin between projected univariate distributions.
While suitable for ﬁnding well-separated projected populations  it may fail to uncover more general
differences between possibly multi-modal projected populations.

3 General Framework for Principal Differences Analysis

max

(1)

For a given divergence measure D between two univariate random variables  we ﬁnd the projection

PB ||||0§k DpT pXpnq  TpY pmqq(

p that solves
where B :“ t P Rd : ||||2 § 1  1 • 0u is the feasible set  ||||0 § k is the sparsity constraint 
and T pXpnq denotes the observed random variable that follows the empirical distribution of n sam-
ples of T X. Instead of imposing a hard cardinality constraint ||||0 § k  we may instead penalize
by adding a penalty term1 ´||||0 or its natural relaxation  the `1 shrinkage used in Lasso [12] 
sparse LDA [2]  and sparse PCA [14  15]. Sparsity in our setting explicitly restricts the comparison
to the marginal distributions over features with non-zero coefﬁcients. We can evaluate the null hy-
pothesis PX “ PY (or its sparse variant over marginals) using permutation testing (cf. [5  16]) with
statistic DppT pXpnq pTpY pmqq.

1In practice  shrinkage parameter  (or explicit cardinality constraint k) may be chosen via cross-validation

by maximizing the divergence between held-out samples.

2

The divergence D plays a key role in our analysis. If D is deﬁned in terms of density functions as in
f-divergence  one can use univariate kernel density estimation to approximate projected pdfs with
additional tuning of the bandwidth hyperparameter. For a suitably chosen kernel (e.g. Gaussian)  the
unregularized PDA objective (without shrinkage) is a smooth function of   and thus amenable to the
projected gradient method (or its accelerated variants [17  18]). In contrast  when D is deﬁned over
the cdfs along the projected direction – e.g. the Kolmogorov or Wasserstein distance that we focus
on in this paper – the objective is nondifferentiable due to the discrete jumps in the empirical cdf.
We speciﬁcally address the combinatorial problem implied by the Wasserstein distance. Moreover 
since the divergence assesses general differences between distributions  Equation (1) is typically
a non-concave optimization. To this end  we develop a semi-deﬁnite relaxation for use with the
Wasserstein distance.

4 PDA using the Wasserstein Distance

In the remainder of the paper  we focus on the squared L2 Wasserstein distance (a.k.a. Kantorovich 
Mallows  Dudley  or earth-mover distance)  deﬁned as

EPXY ||X ´ Y ||2

DpX  Y q “ min
PXY

s.t. pX  Y q „ PXY   X „ PX  Y „ PY

(2)
where the minimization is over all joint distributions over pX  Y q with given marginals PX and PY .
Intuitively interpreted as the amount of work required to transform one distribution into the other 
D provides a natural dissimilarity measure between populations that integrates both the fraction of
individuals which are different and the magnitude of these differences. While component analysis
based on the Wasserstein distance has been limited to [19]  this divergence has been successfully
used in many other applications [20]. In the univariate case  (2) may be analytically expressed as
the L2 distance between quantile functions. We can thus efﬁciently compute empirical projected
Wasserstein distances by sorting X and Y samples along the projection direction to obtain quantile
estimates.
Using the Wasserstein distance  the empirical objective in Equation (1) between unpaired sampled
populations txp1q  . . .   xpnqu and typ1q  . . .   ypmqu can be shown to be

||||0§k" min

max
PB

MPM 

nÿi“1

mÿj“1pT xpiq ´ T ypjqq2Mij* “ max

||||0§k" min

PB

MPM

T WM *

(3)

where M is the set of all n ˆ m nonnegative matching matrices with ﬁxed row sums “ 1{n and
column sums “ 1{m (see [20] for details)  WM :“∞i jrZij b ZijsMij  and Zij :“ xpiq ´ ypjq.
If we omitted (ﬁxed) the inner minimization over the matching matrices and set  “ 0  the solution
of (3) would be simply the largest eigenvector of WM. Similarly  for the sparse variant without
minizing over M  the problem would be solvable as sparse PCA [14  15  21]. The actual max-
min problem in (3) is more complex and non-concave with respect to . We propose a two-step
procedure similar to “tighten after relax” framework used to attain minimax-optimal rates in sparse
PCA [21]. First  we ﬁrst solve a convex relaxation of the problem and subsequently run a steepest
ascent method (initialized at the global optimum of the relaxation) to greedily improve the current
solution with respect to the original nonconvex problem whenever the relaxation is not tight.
Finally  we emphasize that PDA (and SPARDA) not only computationally resembles (sparse) PCA 
but the latter is actually a special case of the former in the Gaussian  paired-sample-differences
setting. This connection is made explicit by considering the two-class problem with paired samples
pxpiq  ypiqq where X  Y follow two multivariate Gaussian distributions. Here  the largest principal
component of the (uncentered) differences xpiq ´ ypiq is in fact equivalent to the direction which
maximizes the projected Wasserstein difference between the distribution of X ´ Y and a delta
distribution at 0.

4.1 Semideﬁnite Relaxation
The SPARDA problem may be expressed in terms of d ˆ d symmetric matrices B as

B

trpWM Bq

min
MPM

max
subject to trpBq “ 1  B © 0  ||B||0 § k2  rankpBq “ 1

(4)

3

where the correspondence between (3) and (4) comes from writing B “ b (note that any solution
of (3) will have unit norm). When k “ d  i.e.  we impose no sparsity constraint as in PDA  we can
relax by simply dropping the rank-constraint. The objective is then a supremum of linear functions
of B and the resulting semideﬁnite problem is concave over a convex set and may be written as:

max
BPBr

min
MPM

trpWM Bq

(5)
where Br is the convex set of positive semideﬁnite d ˆ d matrices with trace = 1. If B˚ P Rdˆd
denotes the global optimum of this relaxation and rankpB˚q “ 1  then the best projection for PDA
is simply the dominant eigenvector of B˚ and the relaxation is tight. Otherwise  we can truncate B˚
as in [14]  treating the dominant eigenvector as an approximate solution to the original problem (3).
To obtain a relaxation for the sparse version where k † d (SPARDA)  we follow [14] closely.
Because B “ b implies ||B||0 § k2  we obtain an equivalent cardinality constrained problem by
incorporating this nonconvex constraint into (4). Since trpBq “ 1 and ||B||F “ ||||2
2 “ 1  a convex
relaxation of the squared `0 constraint is given by ||B||1 § k. By selecting  as the optimal Lagrange
multiplier for this `1 constraint  we can obtain an equivalent penalized reformulation parameterized
by  rather than k [14]. The sparse semideﬁnite relaxation is thus the following concave problem

max

BPBr min

MPM

trpWM Bq ´ ||B||1(

(6)

While the relaxation bears strong resemblance to DSPCA relaxation for sparse PCA  the inner max-
imization over matchings prevents direct application of general semideﬁnite programming solvers.
Let MpBq denote the matching that minimizes trpWM Bq for a given B. Standard projected sub-
gradient ascent could be applied to solve (6)  where at the tth iterate the (matrix-valued) subgradient
is WMpBptqq. However  this approach requires solving optimal transport problems with large n ˆ m
matrices at each iteration. Instead  we turn to a dual form of (6)  assuming n • m (cf. [22  23])

mint0  trprZijbZijs Bq´ui´vju` 1

vj´||B||1 (7)
BPBr uPRn vPRm
(7) is simply a maximization over B P Br  u P Rn  and v P Rm which no longer requires matching
matrices nor their cumbersome row/column constraints. While dual variables u and v can be solved
in closed form for each ﬁxed B (via sorting)  we describe a simple sub-gradient approach that works
better in practice.

ui` 1

mÿj“1

mÿj“1

nÿi“1

nÿi“1

1
m

max

m

n

?d

d   . . .  

RELAX Algorithm: Solves the dualized semideﬁnite relaxation of SPARDA (7). Returns the
largest eigenvector of the solution to (6) as the desired projection direction for SPARDA.
Input: d-dimensional data xp1q  . . .   xpnq and yp1q  . . .   ypmq (with n • m)
Parameters:  • 0 controls the amount of regularization   ° 0 is the step-size used for B
updates  ⌘ ° 0 is the step-size used for updates of dual variables u and v  T is the maximum number
of iterations without improvement in cost after which algorithm terminates.
1: Initialize p0q –”?d

d ı  Bp0q – p0q b p0q P Br  up0q – 0nˆ1  vp0q – 0mˆ1

Zij – xpiq ´ ypjq
If trprZij b ZijsBptqq ´ uptqi ´ vptqj † 0 :

Bu – r1{n  . . .   1{ns P Rn  Bv – r1{m  . . .   1{ms P Rm  BB – 0dˆd
For i  j P t1  . . .   nu ˆ t1  . . .   mu:

2: While the number of iterations since last improvement in objective function is less than T :
3:
4:
5:
6:
7:
8:
9:
10:

End For
upt`1q – uptq ` ⌘ ¨ Bu and vpt`1q – vptq ` ⌘ ¨ Bv
Bpt`1q – Projection´Bptq ` 

Bui – Bui ´ 1{m   Bvj – Bvj ´ 1{m   BB – BB ` Zij b Zij{m

||BB||F ¨ BB ;   {||BB||F¯

Output: prelax P Rd deﬁned as the largest eigenvector (based on corresponding eigenvalue’s magni-

tude) of the matrix Bpt˚q which attained the best objective value over all iterations.

4

(Quadratic program)

2 : w P r0  1sd ||w||1 “ 1(

Projection Algorithm: Projects matrix onto positive semideﬁnite cone of unit-trace matrices Br
(the feasible set in our relaxation). Step 4 applies soft-thresholding proximal operator for sparsity.
Input: B P Rdˆd
Parameters:  • 0 controls the amount of regularization   “ {||BB||F • 0 is the actual step-size
used in the B-update.
1: Q⇤QT – eigendecomposition of B
2: w˚ – arg min ||w ´ diagp⇤q||2
3: rB – Q ¨ diagtw˚1   . . .   w˚du ¨ QT
4: If  ° 0: For r  s P t1  . . .   du2 :
Output: rB P Br
The RELAX algorithm (boxed) is a projected subgradient method with supergradients computed in
Steps 3 - 8. For scaling to large samples  one may alternatively employ incremental supergradient di-
rections [24] where Step 4 would be replaced by drawing random pi  jq pairs. After each subgradient
step  projection back into the feasible set Br is done via a quadratic program involving the current
solution’s eigenvalues. In SPARDA  sparsity is encouraged via the soft-thresholding proximal map
corresponding to the `1 penalty. The overall form of our iterations matches subgradient-proximal
updates (4.14)-(4.15) in [24]. By the convergence analysis in §4.2 of [24]  the RELAX algorithm (as
well as its incremental variant) is guaranteed to approach the optimal solution of the dual which also
solves (6)  provided we employ sufﬁciently large T and small step-sizes. In practice  fast and accu-
rate convergence is attained by: (a) renormalizing the B-subgradient (Step 10) to ensure balanced
updates of the unit-norm constrained B  (b) using diminishing learning rates which are initially set
larger for the unconstrained dual variables (or even taking multiple subgradient steps in the dual
variables per each update of B).

rBr s – signprBr sq ¨ maxt0 |rBr s| ´ u

4.2 Tightening after relaxation

It is unreasonable to expect that our semideﬁnite relaxation is always tight. Therefore  we can

sometimes further reﬁne the projection prelax obtained by the RELAX algorithm by using it as
a starting point in the original non-convex optimization. We introduce a sparsity constrained
tightening procedure for applying projected gradient ascent for the original nonconvex objective
Jpq “ minMPM T WM  where  is now forced to lie in BXSk and Sk :“ t P Rd : ||||0 § ku.
The sparsity level k is ﬁxed based on the relaxed solution (k “ ||prelax||0). After initializing
p0q “ prelax P Rd  the tightening procedure iterates steps in the gradient direction of J followed
by straightforward projections into the unit half-ball B and the set Sk (accomplished by greedily
truncating all entries of  to zero besides the largest k in magnitude).
Let Mpq again denote the matching matrix chosen in response to . J fails to be differentiable at
the r where Mprq is not unique. This occurs  e.g.  if two samples have identical projections under
r. While this situation becomes increasingly likely as n  m Ñ 8  J interestingly becomes smoother
overall (assuming the distributions admit density functions). For all other : Mp1q “ Mpq where
1 lies in a small neighborhood around  and J admits a well-deﬁned gradient 2WMpq. In prac-
tice  we ﬁnd that the tightening always approaches a local optimum of J with a diminishing step-
size. We note that  for a given projection  we can efﬁciently calculate gradients without recourse to
matrices Mpq or WMpq by sorting ptqT
ypmq. The
gradient is directly derivable from expression (3) where the nonzero Mij are determined by appropri-
ately matching empirical quantiles (represented by sorted indices) since the univariate Wasserstein
distance is simply the L2 distance between quantile functions [20]. Additional computation can be
saved by employing insertion sort which runs in nearly linear time for almost sorted points (in iter-
ation t ´ 1  the points have been sorted along the pt´1q direction and their sorting in direction ptq
is likely similar under small step-size). Thus the tightening procedure is much more efﬁcient than
the RELAX algorithm (respective runtimes are Opdn log nq vs. Opd3n2q per iteration).

xp1q  . . .   ptqT

yp1q  . . .   ptqT

xpnq and ptqT

5

We require the combined steps for good performance. The projection found by the tightening al-
gorithm heavily depends on the starting point p0q  ﬁnding only the closest local optimum (as in
Figure 1a). It is thus important that p0q is already a good solution  as can be produced by our
RELAX algorithm. Additionally  we note that as ﬁrst-order methods  both the RELAX and tighten-
ing algorithms are amendable to a number of (sub)gradient-acceleration schemes (e.g. momentum
techniques  adaptive learning rates  or FISTA and other variants of Nesterov’s method [18  17  25]).

4.3 Properties of semideﬁnite relaxation

We conclude the algorithmic discussion by highlighting basic conditions under which our PDA
relaxation is tight. Assuming n  m Ñ 8  each of (i)-(iii) implies that the B˚ which maximizes (5)
is nearly rank one  or equivalently B˚ « r br (see Supplementary Information §S4 for intuition).
Thus  the tightening procedure initialized atr will produce a global maximum of the PDA objective.
(i) There exists direction in which the projected Wasserstein distance between X and Y is
nearly as large as the overall Wasserstein distance in Rd. This occurs for example if
||ErXs ´ ErY s||2 is large while both ||CovpXq||F and ||CovpY q||F are small (the dis-
tributions need not be Gaussian).

(ii) X „ NpµX  ⌃Xq and Y „ NpµY   ⌃Y q with µX ‰ µY and ⌃X « ⌃Y .
(iii) X „ NpµX  ⌃Xq and Y „ NpµY   ⌃Y q with µX “ µY where the underlying covariance
structure is such that arg maxBPBr ||pB1{2⌃XB1{2q1{2 ´ pB1{2⌃Y B1{2q1{2||2
F is nearly
rank 1. For example  if the primary difference between covariances is a shift in the marginal
variance of some features  i.e. ⌃Y « V ¨ ⌃X where V is a diagonal matrix.

5 Theoretical Results

PB

In this section  we characterize statistical properties of an empirical divergence-maximizing projec-

DpT pXpnq  TpY pnqq  although we note that the algorithms may not succeed
tion p :“ arg max
in ﬁnding such a global maximum for severely nonconvex problems. Throughout  D denotes the
squared L2 Wasserstein distance between univariate distributions  C represents universal constants
that change from line to line. All proofs are relegated to the Supplementary Information §S3. We
make the following simplifying assumptions: (A1) n “ m (A2) X  Y admit continuous density
functions (A3) X  Y are compactly supported with nonzero density in the Euclidean ball of radius
R. Our theory can be generalized beyond (A1)-(A3) to obtain similar (but complex) statements
through careful treatment of the distributions’ tails and zero-density regions where cdfs are ﬂat.
Theorem 1. Suppose there exists direction ˚ P B such that Dp˚T X  ˚T Y q • . Then:
DppT pXpnq pTpY pnqq °  ´ ✏ with probability greater than 1 ´ 4 expˆ´ n✏2
16R4˙

Theorem 1 gives basic concentration results for the projections used in empirical applications our
method. To relate distributional differences between X  Y in the ambient d-dimensional space with
their estimated divergence along the univariate linear representation chosen by PDA  we turn to
Theorems 2 and 3. Finally  Theorem 4 provides sparsistency guarantees for SPARDA in the case
where X  Y exhibit large differences over a certain feature subset (of known cardinality).

with probability greater than

Theorem 2. If X and Y are identically distributed in Rd  then: DppT pXpnq pTpY pnqq † ✏

1 ´ C1ˆ1 ` R2
✏ ˙d

expˆ´ C2

R4 n✏2˙

To measure the difference between the untransformed random variables X  Y P Rd  we deﬁne the
following metric between distributions on Rd which is parameterized by a • 0 (cf. [11]):
(8)
TapX  Y q :“ | Prp|X1| § a  . . .  |Xd| § aq ´ Prp|Y1| § a  . . .  |Yd| § aq|

6

y expp´y2{2q 

In addition to (A1)-(A3)  we assume the following for the next two theorems: (A4) Y has sub-
Gaussian tails  meaning cdf FY satisﬁes: 1 ´ FY pyq § C
(A5) ErXs “ ErY s “
0 (note that mean differences can trivially be captured by linear projections  so these are not the
differences of interest in the following theorems)  (A6) Var(X`) = 1 for ` “ 1  . . .   d
Theorem 3. Suppose D a • 0 s.t. TapX  Y q ° hpgpqq where hpgpqq :“ mint1  2u with
1 :“ pa ` dqdpgpq ` dq ` expp´a2{2q ` exp´´1{p?2 q¯
2 :“`gpq ` expp´a2{2q˘ ¨ d
 :“ ||CovpXq||1  gpq :“ 4 ¨ p1 ` q´4  and  :“ sup↵PB supy |f↵T Y pyq|(
with f↵T Y pyq deﬁned as the density of the projection of Y in the ↵ direction.
Then:
with probability greater than 1 ´ C1 exp`´ C2
Theorem 4. Deﬁne C as in (11). Suppose there exists feature subset S Ä t1  . . .   du s.t. |S| “ k 
TpXS  YSq • hpg p✏pd ` 1q{Cqq  and remaining marginal distributions XSC   YSC are identical.
Then:

DppT pXpnq pTpY pnqq ° C ´ ✏

R4 n✏2˘

(10)

(11)

(9)

PB tDpT pXpnq  TpY pnqq : ||||0 § ku
satisﬁesppkqi ‰ 0 andppkqj “ 0 @ i P S  j P SC with probability greater than

ppkq :“ arg max

1 ´ C1ˆ1 ` R2

✏ ˙d´k

expˆ´ C2

R4 n✏2˙

6 Experiments

Figure 1a illustrates the cost function of PDA pertaining to two 3-dimensional distributions (see

details in Supplementary Information §S1). In this example  the point of convergencep of the tight-

ening method after random initialization (in green) is signiﬁcantly inferior to the solution produced
by the RELAX algorithm (in red). It is therefore important to use RELAX before tightening as we
advise.
The synthetic MADELON dataset used in the NIPS 2003 feature selection challenge consists of
points (n “ m “ 1000  d “ 500) which have 5 features scattered on the vertices of a ﬁve-
dimensional hypercube (so that interactions between features must be considered in order to dis-
tinguish the two classes)  15 features that are noisy linear combinations of the original ﬁve  and 480
useless features [26]. While the focus of the challenge was on extracting features useful to classi-
ﬁers  we direct our attention toward more interpretable models. Figure 1b demonstrates how well
SPARDA (red)  the top sparse principal component (black) [27]  sparse LDA (green) [2]  and the
logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their
respective regularization parameters (which determine the cardinality of the vector returned by each
method). The red asterisk indicates the SPARDA result with  automatically selected via our cross-
validation procedure (without information of the underlying features’ importance)  and the black
asterisk indicates the best reported result in the challenge [26].

MADELON

Two Sample Testing

0
2

6
.
0

s
e
r
u
a
e

t

f
 
t

n
a
v
e
e
R

l

5
1

0
1

5

l

e
u
a
v
p

 

4
.
0

2
.
0

(a)

0

0

100

200
300
Cardinality
(b)

400

500

0
0

.

10

20
Data dimension (d)

30

40

50

60

(c)

Figure 1: (a) example where PDA is nonconvex  (b) SPARDA vs. other feature selection methods 
(c) power of various tests for multi-dimensional problems with 3-dimensional differences.

7

The restrictive assumptions in logistic regression and linear discriminant analysis are not satisﬁed in
this complex dataset resulting in poor performance. Despite being class-agnostic  PCA was success-
fully utilized by numerous challenge participants [26]  and we ﬁnd that the sparse PCA performs
on par with logistic regression and LDA. Although the lasso fairly efﬁciently picks out 5 relevant
features  it struggles to identify the rest due to severe multi-colinearity. Similarly  the challenge-
winning Bayesian SVM with Automatic Relevance Determination [26] only selects 8 of the 20
relevant features. In many applications  the goal is to thoroughly characterize the set of differences
rather than select a subset of features that maintains predictive accuracy. SPARDA is better suited
for this alternative objective. Many settings of  return 14 of the relevant features with zero false
positives. If  is chosen automatically through cross-validation  the projection returned by SPARDA
contains 46 nonzero elements of which 17 correspond to relevant features.
Figure 1c depicts (average) p-values produced by SPARDA (red)  PDA (purple)  the overall Wasser-
stein distance in Rd (black)  Maximum Mean Discrepancy [8] (green)  and DiProPerm [5] (blue)
in two-sample synthetically controlled problems where PX ‰ PY and the underlying differences
have varying degrees of sparsity. Here  d indicates the overall number of features included of which
only the ﬁrst 3 are relevant (see Supplementary Information §S1 for details). As we evaluate the
signiﬁcance of each method’s statistic via permutation testing  all the tests are guaranteed to exactly
control Type I error [16]  and we thus only compare their respective power in determining PX ‰ PY
setting. The ﬁgure demonstrates clear superiority of SPARDA which leverages the underlying spar-
sity to maintain high power even with the increasing overall dimensionality. Even when all the
features differ (when d “ 3)  SPARDA matches the power of methods that consider the full space
despite only selecting a single direction (which cannot be based on mean-differences as there are
none in this controlled data). This experiment also demonstrate that the unregularized PDA retains
greater power than DiProPerm  a similar projection-based method [5].
Recent technological advances allow complete transcriptome proﬁling in thousands of individual
cells with the goal of ﬁne molecular characterization of cell populations (beyond the crude average-
tissue-level expression measure that is currently standard) [28]. We apply SPARDA to expression
measurements of 10 305 genes proﬁled in 1 691 single cells from the somatosensory cortex and

1 314 hippocampus cells sampled from the brains of juvenile mice [29]. The resulting p identiﬁes
many previously characterized subtype-speciﬁc genes and is in many respects more informative than
the results of standard differential expression methods (see Supplementary Information §S2 for de-
tails). Finally  we also apply SPARDA to normalized data with mean-zero & unit-variance marginals
in order to explicitly restrict our search to genes whose relationship with other genes’ expression is
different between hippocampus and cortex cells. This analysis reveals many genes known to be
heavily involved in signaling  regulating important processes  and other forms of functional inter-
action between genes (see Supplementary Information §S2.1 for details). These types of important
changes cannot be detected by standard differential expression analyses which consider each gene
in isolation or require gene-sets to be explicitly identiﬁed as features [28].

7 Conclusion

This paper introduces the overall principal differences methodology and demonstrates its numerous
practical beneﬁts of this approach. While we focused on algorithms for PDA & SPARDA tailored
to the Wasserstein distance  different divergences may be better suited for certain applications.
Further theoretical investigation of the SPARDA framework is of interest  particularly in the high-
dimensional d “ Opnq setting. Here  rich theory has been derived for compressed sensing and
sparse PCA by leveraging ideas such as restricted isometry or spiked covariance [15]. A natural
question is then which analogous properties of PX  PY theoretically guarantee the strong empirical
performance of SPARDA observed in our high-dimensional applications. Finally  we also envision
extensions of the methods presented here which employ multiple projections in succession  or adapt
the approach to non-pairwise comparison of multiple populations.

Acknowledgements
This research was supported by NIH Grant T32HG004947.

8

References
[1] Lopes M  Jacob L  Wainwright M (2011) A More Powerful Two-Sample Test in High Dimensions using

Random Projection. NIPS : 1206–1214.

[2] Clemmensen L  Hastie T  Witten D  Ersbø ll B (2011) Sparse Discriminant Analysis. Technometrics 53:

406–413.

[3] van der Vaart AW  Wellner JA (1996) Weak Convergence and Empirical Processes. Springer.
[4] Gibbs AL  Su FE (2002) On Choosing and Bounding Probability Metrics. International Statistical Review

70: 419–435.

[5] Wei S  Lee C  Wichers L  Marron JS (2015) Direction-Projection-Permutation for High Dimensional

Hypothesis Tests. Journal of Computational and Graphical Statistics .

[6] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on

adjacency. Journal of the Royal Statistical Society Series B 67: 515–530.

[7] Szekely G  Rizzo M (2004) Testing for equal distributions in high dimension. InterStat 5.
[8] Gretton A  Borgwardt KM  Rasch MJ  Scholkopf B  Smola A (2012) A Kernel Two-Sample Test. The

Journal of Machine Learning Research 13: 723–773.

[9] Cramer H  Wold H (1936) Some Theorems on Distribution Functions. Journal of the London Mathemat-

ical Society 11: 290–294.

[10] Cuesta-Albertos JA  Fraiman R  Ransford T (2007) A sharp form of the Cramer–Wold theorem. Journal

of Theoretical Probability 20: 201–209.

[11] Jirak M (2011) On the maximum of covariance estimators. Journal of Multivariate Analysis 102: 1032–

1046.

[12] Tibshirani R (1996) Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society Series B : 267–288.

[13] Bradley PS  Mangasarian OL (1998) Feature Selection via Concave Minimization and Support Vector

Machines. ICML : 82–90.

[14] D’Aspremont A  El Ghaoui L  Jordan MI  Lanckriet GR (2007) A direct formulation for sparse PCA

using semideﬁnite programming. SIAM Review : 434–448.

[15] Amini AA  Wainwright MJ (2009) High-dimensional analysis of semideﬁnite relaxations for sparse prin-

cipal components. The Annals of Statistics 37: 2877–2921.

[16] Good P (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses.

Spring-Verlag.

[17] Duchi J  Hazan E  Singer Y (2011) Adaptive Subgradient Methods for Online Learning and Stochastic

Optimization. Journal of Machine Learning Research 12: 2121–2159.

[18] Wright SJ (2010) Optimization Algorithms in Machine Learning. NIPS Tutorial .
[19] Sandler R  Lindenbaum M (2011) Nonnegative Matrix Factorization with Earth Mover’s Distance Metric

for Image Analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 33: 1590–1602.

[20] Levina E  Bickel P (2001) The Earth Mover’s distance is the Mallows distance: some insights from

statistics. ICCV 2: 251–256.

[21] Wang Z  Lu H  Liu H (2014) Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time.

NIPS 27: 3383–3391.

[22] Bertsekas DP (1998) Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc.
[23] Bertsekas DP  Eckstein J (1988) Dual coordinate step methods for linear network ﬂow problems. Mathe-

matical Programming 42: 203–243.

[24] Bertsekas DP (2011) Incremental gradient  subgradient  and proximal methods for convex optimization:

A survey. In: Optimization for Machine Learning  MIT Press. pp. 85–119.

[25] Beck A  Teboulle M (2009) A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Prob-

lems. SIAM Journal on Imaging Sciences 2: 183–202.

[26] Guyon I  Gunn S  Nikravesh M  Zadeh LA (2006) Feature Extraction: Foundations and Applications.

Secaucus  NJ  USA: Springer-Verlag.

[27] Zou H  Hastie T  Tibshirani R (2005) Sparse Principal Component Analysis. Journal of Computational

and Graphical Statistics 67: 301–320.

[28] Geiler-Samerotte KA  Bauer CR  Li S  Ziv N  Gresham D  et al. (2013) The details in the distributions:

why and how to study phenotypic variability. Current opinion in biotechnology 24: 752–9.

[29] Zeisel A  Munoz-Manchado AB  Codeluppi S  Lonnerberg P  La Manno G  et al. (2015) Cell types in the

mouse cortex and hippocampus revealed by single-cell RNA-seq. Science 347: 1138–1142.

9

,Jonas Mueller
Tommi Jaakkola
Bo Li
Yining Wang
Aarti Singh
Yevgeniy Vorobeychik