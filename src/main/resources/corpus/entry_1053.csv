2007,Compressed Regression,Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction  establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original $n$ input variables are compressed by a random linear transformation to $m \ll n$ examples in $p$ dimensions  and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for $\ell_1$-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one  a property called ``sparsistence.'' In addition  we show that $\ell_1$-regularized compressed regression asymptotically predicts as well as an oracle linear model  a property called ``persistence.'' Finally  we characterize the privacy properties of the compression procedure in information-theoretic terms  establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero.,Compressed Regression

Shuheng Zhou∗ John Lafferty∗† Larry Wasserman‡†

∗Computer Science Department

‡Department of Statistics

†Machine Learning Department

Carnegie Mellon University

Pittsburgh  PA 15213

Abstract

Recent research has studied the role of sparsity in high dimensional regression and
signal reconstruction  establishing theoretical limits for recovering sparse models
from sparse data.
In this paper we study a variant of this problem where the
original n input variables are compressed by a random linear transformation to
m (cid:28) n examples in p dimensions  and establish conditions under which a sparse
linear model can be successfully recovered from the compressed data. A primary
motivation for this compression procedure is to anonymize the data and preserve
privacy by revealing little information about the original data. We characterize
the number of random projections that are required for `1-regularized compressed
regression to identify the nonzero coefﬁcients in the true model with probabil-
ity approaching one  a property called “sparsistence.” In addition  we show that
`1-regularized compressed regression asymptotically predicts as well as an or-
acle linear model  a property called “persistence.” Finally  we characterize the
privacy properties of the compression procedure in information-theoretic terms 
establishing upper bounds on the rate of information communicated between the
compressed and uncompressed data that decay to zero.

1 Introduction

Two issues facing the use of statistical learning methods in applications are scale and privacy. Scale
is an issue in storing  manipulating and analyzing extremely large  high dimensional data. Privacy
is  increasingly  a concern whenever large amounts of conﬁdential data are manipulated within an
organization. It is often important to allow researchers to analyze data without compromising the
privacy of customers or leaking conﬁdential information outside the organization. In this paper we
show that sparse regression for high dimensional data can be carried out directly on a compressed
form of the data  in a manner that can be shown to guard privacy in an information theoretic sense.

The approach we develop here compresses the data by a random linear or afﬁne transformation 
reducing the number of data records exponentially  while preserving the number of original input
variables. These compressed data can then be made available for statistical analyses; we focus on
the problem of sparse linear regression for high dimensional data. Informally  our theory ensures
that the relevant predictors can be learned from the compressed data as well as they could be from
the original uncompressed data. Moreover  the actual predictions based on new examples are as
accurate as they would be had the original data been made available. However  the original data
are not recoverable from the compressed data  and the compressed data effectively reveal no more
information than would be revealed by a completely new sample. At the same time  the inference
algorithms run faster and require fewer resources than the much larger uncompressed data would
require. The original data need not be stored; they can be transformed “on the ﬂy” as they come in.

1

In more detail  the data are represented as a n × p matrix X. Each of the p columns is an attribute 
and each of the n rows is the vector of attributes for an individual record. The data are compressed

is a random m × p matrix. Such transformations have been called “matrix masking” in the privacy
literature [6]. The entries of  and  are taken to be independent Gaussian random variables  but

by a random linear transformation X 7→ eX ≡ X  where  is a random m × n matrix with
m (cid:28) n. It is also natural to consider a random afﬁne transformation X 7→eX ≡ X +   where 
other distributions are possible. We think of eX as “public ” while  and  are private and only
needed at the time of compression. However  even with  = 0 and  known  recovering X from
eX requires solving a highly under-determined linear system and comes with information theoretic
privacy guarantees  as we demonstrate.
In standard regression  a response variable Y = Xβ +  ∈ Rn is associated with the input variables 
where i are independent  mean zero additive noise variables. In compressed regression  we assume
that the response is also compressed  resulting in the transformed responseeY ∈ Rm given by Y 7→
eY ≡ Y = Xβ +  = eX β +e. Note that under compression ei   i ∈ {1  . . .   m}  in the
transformed noisee =  are no longer independent. In the sparse setting  the parameter β ∈ R p

is sparse  with a relatively small number s = kβk0 of nonzero coefﬁcients in β. The method we
focus on is `1-regularized least squares  also known as the lasso [17]. We study the ability of the
compressed lasso estimator to identify the correct sparse set of relevant variables and to predict well.

1

j 6= 0}.

2mkeY −eX βk2

We omit details and technical assumptions in the following theorems for clarity. Our ﬁrst result
shows that the lasso is sparsistent under compression  meaning that the correct sparse set of relevant
variables is identiﬁed asymptotically.
Sparsistence (Theorem 3.3): Ifthenumberofcompressedexamples m satisﬁes C1s2 log nps ≤
m ≤ √C2n/ log n andtheregularizationparameter λm satisﬁes λm → 0 and mλ2
m / log p →
∞  then the compressed lasso estimatoreβm = arg minβ
2 + λmkβk1 is sparsistent:
P(cid:0)supp(eβm ) = supp(β)(cid:1) → 1 asm → ∞ wheresupp(β) = {j :
Our second result shows that the lasso is persistent under compression. Roughly speaking  per-
sistence [10] means that the procedure predicts well  as measured by the predictive risk R(β) =
E(cid:0)Y − β T X(cid:1)2  where X ∈ R p is a new input vector and Y is the associated response. Persistence is
a weaker condition than sparsistency  and in particular does not assume that the true model is linear.
Persistence (Theorem 4.1): Givenasequenceofsetsofestimators Bn m ⊂ R p suchthat Bn m =
{β : kβk1 ≤ Ln m} with log2(np) ≤ m ≤ n thesequenceofcompressedlassoestimatorseβn m =
2 is persistent with the predictive risk R(β) = E(cid:0)Y − β T X(cid:1)2 over
argmin
uncompressed data with respect to Bn m  meaning that R(eβn m ) − infkβk1≤Ln m R(β)
n → ∞ incase Ln m = o (m/ log(np))1/4.
Our third result analyzes the privacy properties of compressed regression. We evaluate privacy in
information theoretic terms by bounding the average mutual information I (eX; X )/np per matrix

entry in the original data matrix X  which can be viewed as a communication rate. Bounding this
mutual information is intimately connected with the problem of computing the channel capacity of
certain multiple-antenna wireless communication systems [13].
Information Resistence (Propositions 5.1 and 5.2): The rate at which information about X is

kβk1≤Ln m keY − eX βk2

−→ 0  as

P

revealed by the compressed dataeX satisﬁes rn m = sup I (X;eX )

supremumisoverdistributionsontheoriginaldata X.

np

= O(cid:0) m

n(cid:1) → 0  where the

As summarized by these results  compressed regression is a practical procedure for sparse learning
in high dimensional data that has provably good properties. Connections with related literature are
brieﬂy reviewed in Section 2. Analyses of sparsistence  persistence and privacy properties appear in
Section 3–5. Simulations for sparsistence and persistence of the compressed lasso are presented in
Section 6. The proofs are included in the full version of the paper  available at http://arxiv.
org/abs/0706.0534.

2

2 Background and Related Work

In this section we brieﬂy review related work in high dimensional statistical inference  compressed
sensing  and privacy  to place our work in context.
Sparse Regression. An estimator that has received much attention in the recent literature is the
2 + λnkβk1  where λn is a regularization param-
eter. In [14] it was shown that the lasso is consistent in the high dimensional setting under certain
assumptions. Sparsistency proofs for high dimensional problems have appeared recently in [20]
and [19]. The results and method of analysis of Wainwright [19]  where X comes from a Gaussian
ensemble and i is i.i.d. Gaussian  are particularly relevant to the current paper. We describe this
Gaussian Ensemble result  and compare our results to it in Sections 3  6.Given that under com-

lassobβn [17]  deﬁned asbβn = arg min 1

2nkY − Xβk2

pression  the noisee =  is not i.i.d  one cannot simply apply this result to the compressed case.

Persistence for the lasso was ﬁrst deﬁned and studied by Greenshtein and Ritov in [10]; we review
their result in Section 4.
Compressed Sensing. Compressed regression has close connections to  and draws motivation from
compressed sensing [4  2]. However  in a sense  our motivation is the opposite of compressed
sensing. While compressed sensing of X allows a sparse X to be reconstructed from a small number
of random measurements  our goal is to reconstruct a sparse function of X. Indeed  from the point
of view of privacy  approximately reconstructing X  which compressed sensing shows is possible
if X is sparse  should be viewed as undesirable; we return to this point in Section ??. Several
authors have considered variations on compressed sensing for statistical signal processing tasks
[5  11]. They focus on certain hypothesis testing problems under sparse random measurements  and
a generalization to classiﬁcation of a signal into two or more classes. Here one observes y = x 
where y ∈ Rm  x ∈ Rn and  is a known random measurement matrix. The problem is to select
between the hypotheses eHi : y = (si + ). The proofs use concentration properties of random
projection  which underlie the celebrated Johnson-Lindenstrauss lemma. The compressed regression
problem we introduce can be considered as a more challenging statistical inference task  where the
problem is to select from an exponentially large set of linear models  each with a certain set of
relevant variables with unknown parameters  or to predict as well as the best linear model in some
class.
Privacy. Research on privacy in statistical data analysis has a long history  going back at least to [3].
We refer to [6] for discussion and further pointers into this literature; recent work includes [16]. The
work of [12] is closely related to our work at a high level  in that it considers low rank random linear
transformations of either the row space or column space of the data X. The authors note the Johnson-
Lindenstrauss lemma  and argue heuristically that data mining procedures that exploit correlations
or pairwise distances in the data are just as effective under random projection. The privacy analysis

system. We are not aware of previous work that analyzes the asymptotic properties of a statistical
estimator under random projection in the high dimensional setting  giving information-theoretic
guarantees  although an information-theoretic quantiﬁcation of privacy was proposed in [1]. We

is restricted to observing that recovering X from eX requires solving an under-determined linear
cast privacy in terms of the rate of information communicated about X througheX  maximizing over

all distributions on X  and identify this with the problem of bounding the Shannon capacity of a
multi-antenna wireless channel  as modeled in [13]. Finally  it is important to mention the active
area of cryptographic approaches to privacy from the theoretical computer science community  for
instance [9  7]; however  this line of work is quite different from our approach.

3 Compressed Regression is Sparsistent

In the standard setting  X is a n × p matrix  Y = Xβ +  is a vector of noisy observations under
a linear model  and p is considered to be a constant. In the high-dimensional setting we allow p to
grow with n. The lasso refers to the following: (P1) min kY − Xβk2
2 such that kβk1 ≤ L. In
Lagrangian form  this becomes: (P2) min 1
2 + λnkβk1. For an appropriate choice of
the regularization parameter λ = λ(Y  L)  the solutions of these two problems coincide.
In compressed regression we project each column X j ∈ Rn of X to a subspace of m dimensions 
using an m × n random projection matrix . LeteX = X be the compressed design matrix  and

2nkY − Xβk2

3

being the set of optimal solutions:

leteY = Y be the compressed response. Thus  the transformed noisee is no longer i.i.d.. The
compressed lasso is the following optimization problem  foreY = Xβ +  = eX +e  withem
(a) (eP2) min

2 + λmkβk1  (b) em = arg min

Although sparsistency is the primary goal in selecting the correct variables  our analysis establishes
conditions for the stronger property of sign consistency:

2m keY −eX βk2

2m keY −eX βk2

2 + λmkβk1.

β∈R p

(1)

1

1

Deﬁnition 3.1. (Sign Consistency) A set of estimators n is sign consistent with the true β if

−1 for x > =  or < 0 respectively. Asashorthand denotetheeventthatasignconsistentsolution

P(cid:0)∃bβn ∈ n s.t.sgn(bβn) = sgn(β)(cid:1) → 1 as n → ∞ wheresgn(·) isgivenbysgn(x) = 1  0  and
existswith E(cid:0)sgn(bβn) = sgn(β∗)(cid:1) :=(cid:8)∃bβ ∈ n suchthatsgn(bβ) = sgn(β∗)(cid:9).

Clearly  if a set of estimators is sign consistent then it is sparsistent.

All recent work establishing results on sparsity recovery assumes some form of incoherence condi-
tion on the data matrix X. To formulate such a condition  it is convenient to introduce an additional
6= 0} be the set of relevant variables and let Sc = {1  . . .   p} \ S
piece of notation. Let S = {j : β j
be the set of irrelevant variables. Then X S and X Sc denote the corresponding sets of columns of the
matrix X. We will impose the following incoherence condition; related conditions are used by [18]

in a deterministic setting. Let kAk∞ = maxiP p
Deﬁnition 3.2. (S-Incoherence) Let X be an n × p matrix and let S ⊂ {1  . . .   p} be nonempty.
Wesaythat X is S-incoherent incase
n X T

j=1 |Ai j| denote the matrix ∞-norm.

n X T

(2)

Sc X S(cid:13)(cid:13)(cid:13)∞ +(cid:13)(cid:13)(cid:13) 1

S X S − I|S|(cid:13)(cid:13)(cid:13)∞ ≤ 1 − η  forsome η ∈ (0  1].

(cid:13)(cid:13)(cid:13) 1

Although not explicitly required  we only apply this deﬁnition to X such that columns of X satisfy

2 = (n) ∀ j ∈ {1  . . .   p}. We can now state our main result on sparsistency.

(cid:13)(cid:13)X j(cid:13)(cid:13)2
Theorem 3.3. Suppose that  before compression  Y = Xβ∗ +   where each column of X is
normalized to have `2-norm n  and ε ∼ N (0  σ 2 In). Assume that X is S-incoherent  where S =
supp(β∗) anddeﬁnes = |S| andρm = mini∈S |β∗i |. Weobserve aftercompression eY =eX β∗ +e 
whereeY = Y eX = X ande =  where i j ∼ N (0  1
withC1 = 4e

η (cid:19) (ln p + 2 log n + log 2(s + 1)) ≤ m ≤r n
S X S)−1(cid:13)(cid:13)(cid:13)∞) → 0.
Thenthecompressedlassoissparsistent: P(cid:0)supp(eβm ) = supp(β)(cid:1) → 1 asm → ∞.

√6π ≈ 2.5044 andC2 =
log( p − s) → ∞  and (b)

n ). Leteβm ∈em asin(1b). If

√8e ≈ 7.6885 and λm → 0 satisﬁes

m + λm(cid:13)(cid:13)(cid:13)( 1

ρm (r log s

(cid:18) 16C1s2

4 Compressed Regression is Persistent

η2 +

mη2λ2
m

16 log n

n X T

4C2s

(a)

(4)

(3)

1

Persistence (Greenshtein and Ritov [10]) is a weaker condition than sparsistency. In particular  the
assumption that E(Y|X ) = β T X is dropped. Roughly speaking  persistence implies that a procedure
predicts well. We review the arguments in [10] ﬁrst; we then adapt it to the compressed case.
Uncompressed Persistence. Consider a new pair (X  Y ) and suppose we want to predict Y from X.
The predictive risk using predictor β T X is R(β) = E(Y − β T X )2. Note that this is a well-deﬁned
quantity even though we do not assume that E(Y|X ) = β T X. It is convenient to rewrite the risk in
the following way: deﬁne Q = (Y  X1  . . .   X p) and γ = (−1  β1  . . .   β p)T   then

R(β) = γ T γ   where  = E(Q QT ).

(5)

4

1
n

arg min

(6)

(7)

.

1
n

QT Q.

Compressed Persistence.

bRn(β) =

Let Q = (Q†
vectors and the training error is

2 · ·· Q†

1 Q†

n)T   where Q†

nXi=1
(Yi − X T

kβk1≤Ln bRn(β).

For the compressed case  again we want to predict (X  Y )  but

constants M and s  where Z = Q j Qk − E(Q j Qk )  where Q j   Qk denote elements of Q.
Following arguments in [10]  it can be shown that under Assumption 1 and given a sequence of sets

i = (Yi   X1i   . . .   X pi )T ∼ Q ∀i = 1  . . .   n are i.i.d. random
i β)2 = γ Tbnγ   where bn =
kβk1≤Ln R(β)  and the uncompressed lasso estimatorbβn = arg min

Given Bn = {β : kβk1 ≤ Ln} for Ln = o(cid:0)(n/ log n)1/4(cid:1)  we deﬁne the oracle predictor β∗ n =
Assumption 1. Suppose that  for each j and k  E(cid:0)|Z|q(cid:1) ≤ q!Mq−2s/2  for every q ≥ 2 and some
of estimators Bn = {β : kβk1 ≤ Ln} for Ln = o(cid:0)(n/ log n)1/4(cid:1)  the sequence of uncompressed
lasso estimatorsbβn = arg min β∈Bn bRn(β) is persistent  i.e.  R(bβn) − R(β∗ n)
now the estimator bβn m is based on the lasso from the compressed data of size mn. Let γ =
(−1  β1  . . .   β p)T as before and we replacebRn with
log(npn )(cid:17)1/4
Given compressed sample size mn  let Bn m = {β : kβk1 ≤ Ln m}  where Ln m = o(cid:16) mn
We deﬁne the compressed oracle predictor β∗ n m = arg min β : kβk1≤Ln m R(β) and the compressed
lasso estimatorbβn m = arg min β : kβk1≤Ln m bRn m (β).
Theorem 4.1. Under Assumption 1  we further assume that there exists a constant M1 > 0 such
that E(Q2
j ) < M1 ∀ j where Q j denotesthe j th elementof Q. Foranysequence Bn m ⊂ R p with
log2(npn) ≤ mn ≤ n  where Bn m consists of all coefﬁcient vectors β such thatkβk1 ≤ Ln m =
o(cid:0)(mn/ log(npn))1/4(cid:1) thesequenceofcompressedlassoproceduresbβn m = argminβ∈Bn m bRn m (β)
ispersistent: R(bβn m ) − R(β∗ n m )
(cid:8)bβn m   ∀mn such that log2(np) < mn ≤ n(cid:9) deﬁnes a subsequence of estimators. In Section 6 we

The main difference between the sequence of compressed lasso estimators and the original un-
compressed sequence is that n and mn together deﬁne the sequence of estimators for the com-
pressed data. Here mn is allowed to grow from (log2(np)) to n; hence for each ﬁxed n 

illustrate the compressed lasso persistency via simulations to compare the empirical risks with the
oracle risks on such a subsequence for a ﬁxed n.

bRn m (β) = γ Tbn m γ   wherebn m =

→ 0 when pn = O(cid:0)enc(cid:1) forc < 1/2.

1
mn

QT T Q.

P

→ 0.

P

5 Information Theoretic Analysis of Privacy

Next we derive bounds on the rate at which the compressed data eX reveal information about the

uncompressed data X. Our general approach is to consider the mapping X 7→ X +  as a noisy
communication channel  where the channel is characterized by multiplicative noise  and additive
noise . Since the number of symbols in X is np we normalize by this effective block length to
deﬁne the information rate rn m per symbol as rn m = sup p(X )
. Thus  we seek bounds on
the capacity of this channel. A privacy guarantee is given in terms of bounds on the rate rn m → 0

decaying to zero. Intuitively  if the mutual information satisﬁes I (X;eX ) = H (X ) − H (X |eX ) ≈ 0 
then the compressed dataeX reveal  on average  no more information about the original data X than

The underlying channel is equivalent to the multiple antenna model for wireless communication
[13]  where there are n transmitter and m receiver antennas in a Raleigh ﬂat-fading environment.
The propagation coefﬁcients between pairs of transmitter and receiver antennas are modeled by the
matrix entries i j ; they remain constant for a coherence interval of p time periods. Computing the

could be obtained from an independent sample.

I (X;eX )

np

5

channel capacity over multiple intervals requires optimization of the joint density of pn transmitted
signals  the problem studied in [13]. Formally  the channel is modeled as Z = X + γ   where
nPn
γ > 0  i j ∼ N (0  1)  i j ∼ N (0  1/n) and 1
i=1 E[X 2
i j ] ≤ P  where the latter is a power
constraint.

Theorem 5.1. Supposethat E[X 2
j ] ≤ P andthecompresseddataareformedby Z = X + γ  
where ism×n withindependententriesi j ∼ N (0  1/n) and ism× p withindependententries
i j ∼ N (0  1). Thentheinformationratern m satisﬁesrn m = sup p(X )
This result is implicitly contained in [13]. When  = 0  or equivalently γ = 0  which is the
case assumed in our sparsistence and persistence results  the above analysis yields the trivial bound
rn m ≤ ∞. We thus derive a separate bound for this case; however  the resulting asymptotic order
of the information rate is the same.

n log(cid:16)1 + P
γ 2(cid:17) .

I (X; Z )
np ≤ m

Theorem 5.2. Supposethat E[X 2
j ] ≤ P andthecompresseddataareformedby Z = X where
 is m × n with independent entries i j ∼ N (0  1/n). Then the information rate rn m satisﬁes
rn m = sup p(X )
Under our sparsistency lower bound on m  the above upper bounds are rn m = O(log(np)/n). We
note that these bounds may not be the best possible since they are obtained assuming knowledge of
the compression matrix   when in fact the privacy protocol requires that  and  are not public.

I (X; Z )
np ≤ m

2n log (2πeP) .

6 Experiments

In this section  we report results of simulations designed to validate the theoretical analysis presented
in previous sections. We ﬁrst present results that show the compressed lasso is comparable to the
uncompressed lasso in recovering the sparsity pattern of the true linear model. We then show results
on persistence that are in close agreement with the theoretical results of Section 4. We only include
Figures 1–2 here; additional plots are included in the full version.
Sparsistency. Here we run simulations to compare the compressed lasso with the uncompressed
lasso in terms of the probability of success in recovering the sparsity pattern of β∗. We use random
matrices for both X and   and reproduce the experimental conditions of [19]. A design parameter
is the compression factor f = n
m   which indicates how much the original data are compressed.
The results show that when the compression factor f is large enough  the thresholding behaviors
as speciﬁed in (8) and (9) for the uncompressed lasso carry over to the compressed lasso  when
X is drawn from a Gaussian ensemble.
is well below the
requirement that we have in Theorem 3.3 in case X is deterministic. In more detail  we consider the
Gaussian ensemble for the projection matrix   where i  j ∼ N (0  1/n) are independent. The noise
is  ∼ N (0  σ 2)  where σ 2 = 1. We consider Gaussian ensembles for the design matrix X with both
diagonal and Toeplitz covariance. In the Toeplitz case  the covariance is given by T (ρ)i  j = ρ|i− j|;
we use ρ = 0.1. [19] shows that when X comes from a Gaussian ensemble under these conditions 
there exist ﬁxed constants θ` and θu such that for any ν > 0 and s = supp(β)  if
(8)

In general  the compression factor f

n > 2(θu + ν)s log( p − s) + s + 1 

then the lasso identiﬁes true variables with probability approaching one. Conversely  if

n < 2(θ` − ν)s log( p − s) + s + 1 

(9)
then the probability of recovering the true variables using the lasso approaches zero. In the follow-
ing simulations  we carry out the lasso using procedure lars(Y  X ) that implements the LARS
algorithm of [8] to calculate the full regularization path. For the uncompressed case  we run
lars(Y  X ) such that Y = Xβ∗ +   and for the compressed case we run lars(Y  X ) such
show that the behavior under compression is close to the uncompressed case.

that Y = Xβ∗+ . The regularization parameter is λm = cp(log( p − s) log s)/m. The results
Persistence. Here we solve the following `1-constrained optimization problem eβ =
kβk1≤L kY − Xβk2 directly  based on algorithms described by [15]. We constrain the solu-
tion to lie in the ball Bn = {kβk1 ≤ Ln}  where Ln = n1/4/√log n. By [10]  the uncompressed lasso

arg min

6

Toeplitz r= 0.1; Fractional Power g=0.5  a=0.2

p=128

256

512

1024

Uncompressed
f = 120

s
s
e
c
c
u
s
 
f
o
 
b
o
r
P

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0

50

150

100
Compressed dimension m

200

250

300

s
s
e
c
c
u
s
 
f

o

 

b
o
r
P

s
s
e
c
c
u
s
 
f

o

 

b
o
r
P

0

.

1

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0

.

1

8
0

.

6
0

.

4
0

.

2
0

.

0

.

0

Identity; FP g= 0.5  a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
Control parameter q

1.5

2.5

3.0

Toeplitz r= 0.1; FP g=0.5  a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
Control parameter q

1.5

2.5

3.0

1

Figure 1: Plots of the number of samples versus the probability of success for recovering sgn(β∗).
Each point on a curve for a particular θ or m  where m = 2θ σ 2s log( p − s) + s + 1  is an average
over 200 trials; for each trial  we randomly draw Xn× p  m×n  and  ∈ Rn. The covariance  =
n E(cid:0)X T X(cid:1) and model β∗ are ﬁxed across all curves in the plot. The sparsity level is s( p) = 0.2 p1/2.
The four sets of curves in the left plot are for p = 128  256  512 and 1024  with dashed lines
marking m for θ = 1 and s = 2  3  5 and 6 respectively. In the plots on the right  each curve has
a compression factor f ∈ {5  10  20  40  80  120} for the compressed lasso  thus n = f m; dashed
lines mark θ = 1. For  = I   θu = θ` = 1  while for  = T (0.1)  θu ≈ 1.84 and θ` ≈ 0.46 [19] 
for the uncompressed lasso in (8) and in (9).

n=9000  p=128  s=9

Uncompressed predictive
Compressed predictive
Compressed empirical

k
s
R

i

8
1

6
1

4
1

2
1

0
1

8

0

2000

4000

6000

8000

Compressed dimension m

that(cid:13)(cid:13)β∗b(cid:13)(cid:13)1 > Ln and β∗b 6∈ Bn  and the uncompressed oracle predictive risk is R = 9.81. For each

Figure 2: Risk versus compressed dimension. We ﬁx n = 9000 and p = 128  and set s( p) = 3 and
Ln = 2.6874. The model is β∗ = (−0.9 −1.7  1.1  1.3 −0.5  2 −1.7 −1.3 −0.9  0  . . .   0)T so
value of m  a data point corresponds to the mean empirical risk  which is deﬁned in (7)  over 100
trials  and each vertical bar shows one standard deviation. For each trial  we randomly draw Xn× p
with i.i.d. row vectors xi ∼ N (0  T (0.1))  and Y = Xβ∗ + .

7

estimatorbβn is persistent over Bn. For the compressed lasso  given n and pn  and a varying com-
pressed sample size m  we take the ball Bn m = {β : kβk1 ≤ Ln m} where Ln m = m1/4/plog(npn).
The compressed lasso estimator bβn m for log2(npn) ≤ m ≤ n  is persistent over Bn m by Theo-

rem 4.1. The simulations conﬁrm this behavior.

7 Acknowlegments

This work was supported in part by NSF grant CCF-0625879.

References

[1] D. Agrawal and C. C. Aggarwal. On the design and quantiﬁcation of privacy preserving data mining

algorithms. In In Proceedings of the 20th Symposium on Principles of Database Systems  May 2001.

[2] E. Candès  J. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.

Communications in Pure and Applied Mathematics  59(8):1207–1223  August 2006.

[3] T. Dalenius. Towards a methodology for statistical disclosure control. Statistik Tidskrift  15:429–444 

1977.

[4] D. Donoho. Compressed sensing. IEEE Trans. Info. Theory  52(4):1289–1306  April 2006.
[5] M. Duarte  M. Davenport  M. Wakin  and R. Baraniuk. Sparse signal detection from incoherent projections.

In Proc. IEEE Int. Conf. on Acoustics  Speech  and Signal Processing  2006.

[6] G. Duncan and R. Pearson. Enhancing access to microdata while protecting conﬁdentiality: Prospects for

the future. Statistical Science  6(3):219–232  August 1991.

[7] C. Dwork. Differential privacy.

In 33rd International Colloquium on Automata  Languages and

Programming–ICALP 2006  pages 1–12  2006.

[8] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics  32(2):407–

499  2004.

[9] J. Feigenbaum  Y. Ishai  T. Malkin  K. Nissim  M. J. Strauss  and R. N. Wright. Secure multiparty compu-

tation of approximations. ACM Trans. Algorithms  2(3):435–472  2006.

[10] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the virtue of

over-parametrization. Journal of Bernoulli  10:971–988  2004.

[11] J. Haupt  R. Castro  R. Nowak  G. Fudge  and A. Yeh. Compressive sampling for signal classiﬁcation. In

Proc. Asilomar Conference on Signals  Systems  and Computers  October 2006.

[12] K. Liu  H. Kargupta  and J. Ryan. Random projection-based multiplicative data perturbation for privacy

preserving distributed data mining. IEEE Trans. on Knowl. and Data Engin.  18(1)  Jan. 2006.

[13] T. L. Marzetta and B. M. Hochwald. Capacity of a mobile multiple-antenna communication link in

Rayleigh ﬂat fading. IEEE Trans. Info. Theory  45(1):139–157  January 1999.

[14] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Technical Report 720  Department of Statistics  UC Berkeley  2006.

[15] M. Osborne  B. Presnell  and B. Turlach. On the lasso and its dual. J. Comp. and Graph. Stat.  9(2):319–

337  2000.

[16] A. P. Sanil  A. Karr  X. Lin  and J. P. Reiter. Privacy preserving regression modelling via distributed

computation. In Proceedings of Tenth ACM SIGKDD  2004.

[17] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B  58(1):267–288 

1996.

[18] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information

Theory  50(10):2231–2242  2004.

[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Technical Report

709  Department of Statistics  UC Berkeley  May 2006.

[20] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Research  7:2541–2567 

2007.

8

,Maksim Lapin
Matthias Hein
Bernt Schiele
Zihan Li
Matthias Fresacher
Jonathan Scarlett