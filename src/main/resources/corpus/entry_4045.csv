2013,Online Learning of Dynamic Parameters in Social Networks,This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches  the underlying state is dynamic  and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function  we introduce two update mechanisms  each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state  under which individuals can track the parameter with a bounded variance. Then  we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth  per individual. We observe that only one of the estimators recovers the optimal MSD  which underscores the impact of the objective function decomposition on the learning quality. Finally  we provide an upper bound on the regret of the proposed methods  measured as an average of errors in estimating the parameter in a finite time.,Online Learning of Dynamic Parameters

in Social Networks

1Department of Electrical and Systems Engineering  2Department of Statistics

Ali Jadbabaie 1

Shahin Shahrampour 1

Alexander Rakhlin 2

University of Pennsylvania
Philadelphia  PA 19104 USA

1{shahin jadbabai}@seas.upenn.edu 2rakhlin@wharton.upenn.edu

Abstract

This paper addresses the problem of online learning in a dynamic setting. We
consider a social network in which each individual observes a private signal about
the underlying state of the world and communicates with her neighbors at each
time period. Unlike many existing approaches  the underlying state is dynamic 
and evolves according to a geometric random walk. We view the scenario as an
optimization problem where agents aim to learn the true state while suffering the
smallest possible loss. Based on the decomposition of the global loss function  we
introduce two update mechanisms  each of which generates an estimate of the true
state. We establish a tight bound on the rate of change of the underlying state  un-
der which individuals can track the parameter with a bounded variance. Then  we
characterize explicit expressions for the steady state mean-square deviation(MSD)
of the estimates from the truth  per individual. We observe that only one of the
estimators recovers the optimal MSD  which underscores the impact of the objec-
tive function decomposition on the learning quality. Finally  we provide an upper
bound on the regret of the proposed methods  measured as an average of errors in
estimating the parameter in a ﬁnite time.

1

Introduction

In recent years  distributed estimation  learning and prediction has attracted a considerable attention
in wide variety of disciplines with applications ranging from sensor networks to social and economic
networks [1–6]. In this broad class of problems  agents aim to learn the true value of a parameter
often called the underlying state of the world. The state could represent a product  an opinion  a
vote  or a quantity of interest in a sensor network. Each agent observes a private signal about the
underlying state at each time period  and communicates with her neighbors to augment her imperfect
observations. Despite the wealth of research in this area when the underlying state is ﬁxed (see
e.g. [1–3  7])  often the state is subject to some change over time(e.g. the price of stocks) [8–11].
Therefore  it is more realistic to study models which allow the parameter of interest to vary. In
the non-distributed context  such models have been studied in the classical literature on time-series
prediction  and  more recently  in the literature on online learning under relaxed assumptions about
the nature of sequences [12]. In this paper we aim to study the sequential prediction problem in the
context of a social network and noisy feedback to agents.
We consider a stochastic optimization framework to describe an online social learning problem when
the underlying state of the world varies over time. Our motivation for the current study is the results
of [8] and [9] where authors propose a social learning scheme in which the underlying state follows
a simple random walk. However  unlike [8] and [9]  we assume a geometric random walk evolution
with an associated rate of change. This enables us to investigate the interplay of social learning 
network structure  and the rate of state change  especially in the interesting case that the rate is

1

greater than unity. We then pose the social learning as an optimization problem in which individuals
aim to suffer the smallest possible loss as they observe the stream of signals. Of particular relevance
to this work is the work of Duchi et al.
in [13] where the authors develop a distributed method
based on dual averaging of sub-gradients to converge to the optimal solution. In this paper  we
restrict our attention to quadratic loss functions regularized by a quadratic proximal function  but
there is no ﬁxed optimal solution as the underlying state is dynamic.
In this direction  the key
observation is the decomposition of the global loss function into local loss functions. We consider
two decompositions for the global objective  each of which gives rise to a single-consensus-step
belief update mechanism. The ﬁrst method incorporates the averaged prior beliefs among neighbors
with the new private observation  while the second one takes into account the observations in the
neighborhood as well. In both scenarios  we establish that the estimates are eventually unbiased  and
we characterize an explicit expression for the mean-square deviation(MSD) of the beliefs from the
truth  per individual. Interestingly  this quantity relies on the whole spectrum of the communication
matrix which exhibits the formidable role of the network structure in the asymptotic learning. We
observe that the estimators outperform the upper bound provided for MSD in the previous work [8].
Furthermore  only one of the two proposed estimators can compete with the centralized optimal
Kalman Filter [14] in certain circumstances. This fact underscores the dependence of optimality
on decomposition of the global loss function. We further highlight the inﬂuence of connectivity on
learning by quantifying the ratio of MSD for a complete versus a disconnected network. We see that
this ratio is always less than unity and it can get arbitrarily close to zero under some constraints.
Our next contribution is to provide an upper bound for regret of the proposed methods  deﬁned as
an average of errors in estimating the parameter up to a given time minus the long-run expected
loss due to noise and dynamics alone. This ﬁnite-time regret analysis is based on the recently
developed concentration inequalities for matrices and it complements the asymptotic statements
about the behavior of MSD.
Finally  we examine the trade-off between the network sparsity and learning quality in a microscopic
level. Under mild technical constraints  we see that losing each connection has detrimental effect on
learning as it monotonically increases the MSD. On the other hand  capturing agents communica-
tions with a graph  we introduce the notion of optimal edge as the edge whose addition has the most
effect on learning in the sense of MSD reduction. We prove that such a friendship is likely to occur
between a pair of individuals with high self-reliance that have the least common neighbors.

2 Preliminaries

2.1 State and Observation Model
We consider a network consisting of a ﬁnite number of agents V = {1  2  ...  N}. The agents
indexed by i ∈V seek the underlying state of the world  xt ∈ R  which varies over time and evolves
according to

xt+1 = axt + rt 

(1)

where rt is a zero mean innovation  which is independent over time with ﬁnite variance E[r2
r 
t ] = σ2
and a ∈ R is the expected rate of change of the state of the world  assumed to be available to all
agents  and could potentially be greater than unity. We assume the initial state x0 is a ﬁnite random
variable drawn independently by the nature. At time period t  each agent i receives a private signal
yi t ∈ R  which is a noisy version of xt  and can be described by the linear equation

yi t = xt + wi t 

(2)

where wi t is a zero mean observation noise with ﬁnite variance E[w2
w  and it is assumed to
be independent over time and agents  and uncorrelated to the innovation noise. Each agent i forms
an estimate or a belief about the true value of xt at time t conforming to an update mechanism that
will be discussed later. Much of the difﬁculty of this problem stems from the hardness of tracking a
dynamic state with noisy observations  especially when |a| > 1  and communication mitigates the
difﬁculty by virtue of reducing the effective noise.

i t] = σ2

2

2.2 Communication Structure

Agents communicate with each other to update their beliefs about the underlying state of the world.
The interaction between agents is captured by an undirected graph G = (V E)  where V is the set
of agents  and if there is a link between agent i and agent j  then {i  j}∈E . We let ¯Ni = {j ∈
be the set of neighbors of agent i  and Ni = ¯Ni ∪{ i}. Each agent i can only
V : {i  j}∈E}
communicate with her neighbors  and assigns a weight pij > 0 for any j ∈ ¯Ni. We also let pii ≥ 0
denote the self-reliance of agent i.
Assumption 1. The communication matrix P = [pij] is symmetric and doubly stochastic  i.e.  it
satisﬁes

pij ≥ 0

 

pij = pji

  and ￿j∈Ni

N￿j=1

pij =

pij = 1.

We further assume the eigenvalues of P are in descending order and satisfy

−1 <λ N (P ) ≤ ... ≤ λ2(P ) <λ 1(P ) = 1.

2.3 Estimate Updates

The goal of agents is to learn xt in a collaborative manner by making sequential predictions. From
optimization perspective  this can be cast as a quest for online minimization of the separable  global 
time-varying cost function

ft(¯x) =

min
¯x∈R

1
N

N￿i=1￿ ˆfi t(¯x) ￿ 1

2E￿yi t − ¯x￿2￿ =

1
N

N￿i=1￿ ˜fi t(¯x) ￿

N￿j=1

pij ˆfj t(¯x)￿ 

(3)

at each time period t. One approach to tackle the stochastic learning problem formulated above is to
employ distributed dual averaging regularized by a quadratic proximal function [13]. To this end  if
agent i exploits ˆfi t as the local loss function  she updates her belief as

while using ˜fi t as the local loss function results in the following update

pij ˆxj t

consensus update

ˆxi t+1 = a￿ ￿j∈Ni
￿
˜xi t+1 = a￿ ￿j∈Ni
￿

￿￿

￿￿

￿

consensus update

pij ˜xj t

￿￿

+ α(yi t − ˆxi t)

innovation update ￿ 
￿
￿
￿
+ α(￿j∈Ni
￿

innovation update

pijyj t − ˜xi t)

￿￿

￿

(4)

(5)

￿ 

where α ∈ (0  1] is a constant step size that agents place for their innovation update  and we refer
to it as signal weight. Equations (4) and (5) are distinct  single-consensus-step estimators differing
in the choice of the local loss function with (4) using only private observations while (5) averaging
observations over the neighborhood. We analyze both class of estimators noting that one might
expect (5) to perform better than (4) due to more information availability.
Note that the choice of constant step size provides an insight on the interplay of persistent innovation
and learning abilities of the network. We remark that agents can easily learn the ﬁxed rate of change
a by taking ratios of observations  and we assume that this has been already performed by the agents
in the past. The case of a changing a is beyond the scope of the present paper. We also point out that
the real-valued (rather than vector-valued) nature of the state is a simpliﬁcation that forms a clean
playground for the study of the effects of social learning  effects of friendships  and other properties
of the problem.

2.4 Error Process
Deﬁning the local error processes ˆξi t and ˜ξi t  at time t for agent i  as
˜ξi t ￿ ˜xi t − xt 

ˆξi t ￿ ˆxi t − xt

and

3

and stacking the local errors in vectors ˆξt  ˜ξt ∈ RN  respectively  such that
˜ξt ￿ [˜ξ1 t  ...  ˜ξN t]T 

(6)
one can show that the aforementioned collective error processes could be described as a linear dy-
namical system.
Lemma 2. Given Assumption 1  the collective error processes ˆξt and ˜ξt deﬁned in (6) satisfy

ˆξt ￿ [ˆξ1 t  ...  ˆξN t]T

and

respectively  where

and

ˆξt+1 = Qˆξt + ˆst

and

˜ξt+1 = Q˜ξt + ˜st 

Q = a(P − αIN ) 

ˆst = (αa)[w1 t  ...  wN t]T − rt1N

with 1N being vector of all ones.

and

˜st = (αa)P [w1 t  ...  wN t]T − rt1N  

(7)

(8)

(9)

Throughout the paper  we let ρ(Q)  denote the spectral radius of Q  which is equal to the largest
singular value of Q due to symmetry.

3 Social Learning: Convergence of Beliefs and Regret Analysis

In this section  we study the behavior of estimators (4) and (5) in the mean and mean-square sense 
and we provide the regret analysis.
In the following proposition  we establish a tight bound for a  under which agents can achieve
asymptotically unbiased estimates using proper signal weight.
Proposition 3 (Unbiased Estimates). Given the network G with corresponding communication ma-
trix P satisfying Assumption 1  the rate of change of the social network in (4) and (5) must respect
the constraint

to allow agents to form asymptotically unbiased estimates of the underlying state.

2

|a| <

 

1 − λN (P )

Proposition 3 determines the trade-off between the rate of change and the network structure. In other
words  changing less than the rate given in the statement of the proposition  individuals can always
track xt with bounded variance by selecting an appropriate signal weight. However  the proposition
does not make any statement on the learning quality. To capture that  we deﬁne the steady state
Mean Square Deviation(MSD) of the network from the truth as follows.
Deﬁnition 4 ((Steady State-)Mean Square Deviation). Given the network G with a rate of change
which allows unbiased estimation  the steady state of the error processes in (7) is deﬁned as follows

ˆΣ ￿ lim
t→∞

E[ˆξt ˆξT
t ]

and

˜Σ ￿ lim
t→∞

E[˜ξt ˜ξT
t ].

Hence  the (Steady State-)Mean Square Deviation of the network is the deviation from the truth in
the mean-square sense  per individual  and it is deﬁned as

ˆMSD ￿ 1
N

Tr( ˆΣ)

and

˜MSD ￿ 1
N

Tr( ˜Σ).

Theorem 5 (MSD). Given the error processes (7) with ρ(Q) < 1  the steady state MSD for (4) and
(5) is a function of the communication matrix P   and the signal weight α as follows
ˆMSD(P  α) = RM SD(α) + ˆWM SD(P  α)
where

˜MSD(P  α) = RM SD(α) + ˜WM SD(P  α)  (10)

and
ˆWM SD(P  α) ￿ 1
N

RM SD(α) ￿

σ2
r

1 − a2(1 − α)2  

N￿i=1

a2α2σ2
w

1 − a2(λi(P ) − α)2 and

˜WM SD(P  α) ￿ 1
N

N￿i=1

4

(11)

(12)

a2α2σ2

wλ2

i (P )

1 − a2(λi(P ) − α)2 .

Theorem 5 shows that the steady state MSD is governed by all eigenvalues of P contributing to
WM SD pertaining to the observation noise  while RM SD is the penalty incurred due to the inno-
vation noise. Moreover  (5) outperforms (4) due to richer information diffusion  which stresses the
importance of global loss function decomposition.
One might advance a conjecture that a complete network  where all individuals can communicate
with each other  achieves a lower steady state MSD in the learning process since it provides the
most information diffusion among other networks. This intuitive idea is discussed in the following
corollary beside a few examples.
Corollary 6. Denoting the complete  star  and cycle graphs on N vertices by KN  SN  and CN  re-
spectively  and denoting their corresponding Laplacians by LKN   LSN   and LCN   under conditions
of Theorem 5 
(a) For P = I − 1−α

N LKN   we have
lim
N→∞

ˆMSDKN = RM SD(α) + a2α2σ2
w.

(b) For P = I − 1−α

N LSN   we have

ˆMSDSN = RM SD(α) +

lim
N→∞

a2α2σ2
w

1 − a2(1 − α)2 .

(c) For P = I − βLCN   where β must preserve unbiasedness  we have
a2α2σ2
w

ˆMSDCN = RM SD(α) +￿ 2π

0

1 − a2(1 − β(2 − 2 cos(τ )) − α)2

lim
N→∞
(d) For P = I − 1

N LKN   we have

˜MSDKN = RM SD(α).

lim
N→∞

(13)

(14)

(15)

(16)

dτ
2π

.

Proof. Noting that the spectrum of LKN   LSN and LCN are  respectively [15]  {λN = 0 λ N−1 =
N )}N−1
N  ...  λ1 = N} {λN = 0 λ N−1 = 1  ... λ 2 = 1 λ 1 = N}  and {λi = 2 − 2 cos( 2πi
i=0  
substituting each case in (10)  and taking the limit over N  the proof follows immediately.

To study the effect of communication let us consider the estimator (4). Under purview of Theorem
5 and Corollary 6  the ratio of the steady state MSD for a complete network (13) versus a fully
disconnected network(P = IN ) can be computed as

ˆMSDKN

σ2
r + a2α2σ2

w(1 − a2(1 − α)2)
σ2
r + a2α2σ2
w

=

r ￿ σ2

ˆMSDdisconnected

lim
N→∞
w. The ratio above can get arbitrary close to zero which  indeed  highlights the inﬂuence

for σ2
of communication on the learning quality.
We now consider Kalman Filter(KF) [14] as the optimal centralized counterpart of (5). It is well-
known that the steady state KF satisﬁes a Riccati equation  and when the parameter of interest is
scalar  the Riccati equation simpliﬁes to a quadratic with the positive root

≈ 1 − a2(1 − α)2 

ΣKF =

a2σ2

w − σ2

w + Nσ 2

w + Nσ 2

r )2 + 4Nσ 2

wσ2
r

.

r +￿(a2σ2

w − σ2
2N

Therefore  comparing with the complete graph (16)  we have
σ2
r

1 − a2(1 − α)2  
and the upper bound can be made tight by choosing α = 1 for |a| <
we should choose an α< 1 to preserve unbiasedness as well.

ΣKF = σ2

lim
N→∞

r ≤

1

|λN (P )−1|

. If |a|≥

1

|λN (P )−1|

5

On the other hand  to evaluate the performance of estimator (4)  we consider the upper bound

MSDBound =

σ2
r + α2σ2
w

α

 

(17)

derived in [8]  for a = 1 via a distributed estimation scheme. For simplicity  we assume σ2
r =
σ2  and let β in (15) be any diminishing function of N. Optimizing (13)  (14)  (15)  and (17) over
α  we obtain

w = σ2

lim
N→∞

ˆMSDKN ≈ 1.55σ2 < lim
N→∞

ˆMSDCN ≈ 1.62σ2 < MSDBound = 2σ2 
which suggests a noticeable improvement in learning even in the star and cycle networks where the
number of individuals and connections are in the same order.

ˆMSDSN = lim
N→∞

Regret Analysis

We now turn to ﬁnite-time regret analysis of our methods. The average loss of all agents in predicting
the state  up until time T   is

1
T

1
N

T￿t=1

N￿i=1

(ˆxi t − xt)2 =

1
T

1
N

Tr(ˆξt ˆξT

t ) .

T￿t=1

As motivated earlier  it is not possible  in general  to drive this average loss to zero  and we need to
subtract off the limit. We thus deﬁne regret as

RT ￿ 1
T

T￿t=1
where ˆΣ is from Deﬁnition 4. We then have for the spectral norm ￿·￿ that

T￿t=1

Tr( ˆΣ) =

Tr(ˆξt ˆξT

t ) −

1
N

1
N

1
N

Tr￿ 1

T

t − ˆΣ￿  

ˆξt ˆξT

ξtξT

 

(18)

1
T

T￿t=1
RT ≤￿￿￿￿￿

1
T

T￿t=1

t − Σ￿￿￿￿￿

￿ A2
t  

where we dropped the distinguishing notation between the two estimators since the analysis works
for both of them. We  ﬁrst  state a technical lemma from [16] that we invoke later for bounding the
quantity RT . For simplicity  we assume that magnitudes of both innovation and observation noise
are bounded.
Lemma 7. Let {st}T
t=1 be an independent family of vector valued random variables  and let H
be a function that maps T variables to a self-adjoint matrix of dimension N. Consider a sequence
{At}T

t=1 of ﬁxed self-adjoint matrices that satisfy

where ωi and ω￿i range over all possible values of si for each index i. Letting Var = ￿￿T
for all c ≥ 0  we have

￿H(ω1  ... ω t  ... ω T ) − H(ω1  ... ω ￿t  ... ω T )￿2
P￿￿￿H(s1  ...  sT ) − E[H(s1  ...  sT )]￿￿ ≥ c￿ ≤ N e−c2/8Var.
￿1 − ρ2(Q)￿2￿ +
T￿ ￿ξ0￿2
1 − ρ2(Q)￿ +
with probability at least 1 − δ.
We mention that results that are similar in spirit have been studied for general unbounded stationary
ergodic time series in [17–19] by employing techniques from the online learning literature. On the
other hand  our problem has the network structure and the speciﬁc evolution of the hidden state  not
present in the above works.

Theorem 8. Under conditions of Theorem 5 together with boundedness of noise maxt≤T ￿st￿ ≤ s
for some s > 0  the regret function deﬁned in (18) satisﬁes
8s2￿2 log N
(1 − ρ(Q))2  

T￿ 2s￿ξ0￿
￿1 − ρ(Q)￿2￿ +

t=1 A2
t￿ 

T￿

RT ≤

1
√T

(19)

s2

1

1

1

δ

6

4 The Impact of New Friendships on Social Learning

In the social learning model we proposed  agents are cooperative and they aim to accomplish a global
objective. In this direction  the network structure contributes substantially to the learning process.
In this section  we restrict our attention to estimator (5)  and characterize the intuitive idea that mak-
ing(losing) friendships can inﬂuence the quality of learning in the sense of decreasing(increasing)
the steady state MSD of the network.
To commence  letting ei denote the i-th unit vector in the standard basis of RN  we exploit the
negative semi-deﬁnite  edge function matrix

(20)
for edge addition(removal) to(from) the graph. Essentially  if there is no connection between agents
i and j 

∆P (i  j) ￿ −(ei − ej)(ei − ej)T 

P￿ ￿ P + ￿∆P (i  j) 

(21)
for ￿< min{pii  pjj}  corresponds to a new communication matrix adding the edge {i  j} with a
weight ￿ to the network G  and subtracting ￿ from self-reliance of agents i and j.
Proposition 9. Let G− be the network resulted by removing the bidirectional edge {i  j} with the
weight ￿ from the network G  so P−￿ and P denote the communication matrices associated to G−
and G  respectively. Given Assumption 1  for a ﬁxed signal weight α the following relationship holds
(22)

˜MSD(P  α) ≤ ˜MSD(P−￿ α ) 

.

as long as P is positive semi-deﬁnite  and |a| < 1
|α|
Under a mild technical assumption  Proposition 9 suggests that losing connections monotonically
increases the MSD  and individuals tend to maintain their friendships to obtain a lower MSD as a
global objective. However  this does not elaborate on the existence of individuals with whom losing
or making connections could have an immense impact on learning. We bring this concept to light
in the following proposition with ﬁnding a so-called optimal edge which provides the most MSD
reduction  in case it is added to the network graph.
Proposition 10. Given Assumption 1  a positive semi-deﬁnite P   and |a| < 1
  to ﬁnd the optimal
|α|
edge with a pre-assigned weight ￿ ￿ 1 to add to the network G  we need to solve the following
optimization problem
N￿k=1￿hk(i  j) ￿ zk(i  j)￿2(1 − α2a2)λk(P ) + 2a2αλ2
k(P )￿
￿1 − a2(λk(P ) − α)2￿2

min
{i j} /∈E

￿ 

where

(23)

zk(i  j) ￿ (vT

k∆P (i  j)vk)￿ 

(24)

k=1 are the right eigenvectors of P . In addition  letting ζmax = maxk>1 |λk(P ) − α| 
and {vk}N
−2￿￿(1 − α2a2)(pii + pjj) + a2α([P 2]ii + [P 2]jj − 2[P 2]ij)￿
N￿k=1

hk(i  j) ≥ min
{i j} /∈E

min
{i j} /∈E

.

￿1 − a2ζ2

max￿2

(25)

Proof. Representing the ﬁrst order approximation of λk(P￿) using deﬁnition of zk(i  j) in (24)  we
have λk(P￿) ≈ λk(P ) + zk(i  j) for ￿ ￿ 1. Based on Theorem 5  we now derive
˜MSD(P￿ α ) − ˜MSD(P  α) ∝
N￿k=1
N￿k=1

N￿k=1￿λk(P￿) − λk(P )￿￿(1 − α2a2)(λk(P￿) + λk(P )) + 2a2αλk(P )λk(P￿)￿
￿1 − a2(λk(P ) − α)2￿￿1 − a2(λk(P￿) − α)2￿
k(P ) + (1 − α2a2 + 2a2αλk(P ))zk(i  j)￿
zk(i  j)￿2(1 − α2a2)λk(P ) + 2a2αλ2
￿1 − a2(λk(P ) − α)2￿￿1 − a2(λk(P ) − α + zk(i  j))2￿
zk(i  j)￿2(1 − α2a2)λk(P ) + 2a2αλ2
k(P )￿
￿1 − a2(λk(P ) − α)2￿2

+ O(￿2) 

≈

=

7

˜MSD(P￿ α ) − ˜MSD(P  α) is 
noting that zk(i  j) is O(￿) from the deﬁnition (24). Minimizing
hence  equivalent to optimization (23) when ￿ ￿ 1. Taking into account that P is positive semi-
deﬁnite  zk(i  j) ≤ 0 for k ≥ 2  and v1 = 1N /√N which implies z1(i  j) = 0  we proceed to the
lower bound proof using the deﬁnition of hk(i  j) and ζmax in the statement of the proposition  as
follows

Substituting zk(i  j) from (24) to above  we have

N￿k=1

hk(i  j) ≥

=

=

hk(i  j) =

N￿k=1

≥

1

k(P )￿

zk(i  j)￿2(1 − α2a2)λk(P ) + 2a2αλ2

zk(i  j)￿2(1 − α2a2)λk(P ) + 2a2αλ2
N￿k=2
￿1 − a2(λk(P ) − α)2￿2
N￿k=2
￿1 − a2ζ2
max￿2
max￿2￿ N￿k=1￿vT
k∆P (i  j)vk￿￿(1 − α2a2)λk(P ) + a2αλ2
max￿2 Tr￿∆P (i  j)
N￿k=1￿(1 − α2a2)λk(P ) + a2αλ2
max￿2 Tr￿∆P (i  j)￿(1 − α2a2)P + a2αP 2￿￿.

k(P )￿.
k(P )￿￿
k￿
k(P )￿vkvT

2￿

2￿

￿1 − a2ζ2
￿1 − a2ζ2
￿1 − a2ζ2

2￿

Using the facts that Tr(∆P (i  j)P ) = −pii− pjj + 2pij and Tr(∆P (i  j)P 2) = −[P 2]ii− [P 2]jj +
2[P 2]ij according to deﬁnition of ∆P (i  j) in (20)  and pij = 0 since we are adding a non-existent
edge {i  j}  the lower bound (25) is derived.
Beside posing the optimal edge problem as an optimization  Proposition 10 also provides an up-
per bound for the best improvement that making a friendship brings to the network. In view of
(25)  forming a connection between two agents with more self-reliance and less common neighbors 
minimizes the lower bound  which offers the most maneuver for MSD reduction.

5 Conclusion

We studied a distributed online learning problem over a social network. The goal of agents is to
estimate the underlying state of the world which follows a geometric random walk. Each individual
receives a noisy signal about the underlying state at each time period  so she communicates with her
neighbors to recover the true state. We viewed the problem with an optimization lens where agents
want to minimize a global loss function in a collaborative manner. To estimate the true state  we
proposed two methodologies derived from a different decomposition of the global objective. Given
the structure of the network  we provided a tight upper bound on the rate of change of the param-
eter which allows agents to follow the state with a bounded variance. Moreover  we computed the
averaged  steady state  mean-square deviation of the estimates from the true state. The key obser-
vation was optimality of one of the estimators indicating the dependence of learning quality on the
decomposition. Furthermore  deﬁning the regret as the average of errors in the process of learning
during a ﬁnite time T   we demonstrated that the regret function of the proposed algorithms decays

with a rate O(1/√T ). Finally  under mild technical assumptions  we characterized the inﬂuence of

network pattern on learning by observing that each connection brings a monotonic decrease in the
MSD.

Acknowledgments

We gratefully acknowledge the support of AFOSR MURI CHASE  ONR BRC Program on Decen-
tralized  Online Optimization  NSF under grants CAREER DMS-0954737 and CCF-1116928  as
well as Dean’s Research Fund.

8

References
[1] M. H. DeGroot  “Reaching a consensus ” Journal of the American Statistical Association 

vol. 69  no. 345  pp. 118–121  1974.

[2] A. Jadbabaie  P. Molavi  A. Sandroni  and A. Tahbaz-Salehi  “Non-bayesian social learning ”

Games and Economic Behavior  vol. 76  no. 1  pp. 210–225  2012.

[3] E. Mossel and O. Tamuz  “Efﬁcient bayesian learning in social networks with gaussian esti-

mators ” arXiv preprint arXiv:1002.0747  2010.

[4] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao  “Optimal distributed online prediction
using mini-batches ” The Journal of Machine Learning Research  vol. 13  pp. 165–202  2012.
[5] L. Xiao  S. Boyd  and S. Lall  “A scheme for robust distributed sensor fusion based on average
consensus ” in Fourth International Symposium on Information Processing in Sensor Networks.
IEEE  2005  pp. 63–70.

[6] S. Kar  J. M. Moura  and K. Ramanan  “Distributed parameter estimation in sensor networks:
Nonlinear observation models and imperfect communication ” IEEE Transactions on Informa-
tion Theory  vol. 58  no. 6  pp. 3575–3605  2012.

[7] S. Shahrampour and A. Jadbabaie  “Exponentially fast parameter estimation in networks using

distributed dual averaging ” arXiv preprint arXiv:1309.2350  2013.

[8] D. Acemoglu  A. Nedic  and A. Ozdaglar  “Convergence of rule-of-thumb learning rules in

social networks ” in 47th IEEE Conference on Decision and Control  2008  pp. 1714–1720.

[9] R. M. Frongillo  G. Schoenebeck  and O. Tamuz  “Social learning in a changing world ” in

Internet and Network Economics. Springer  2011  pp. 146–157.

[10] U. A. Khan  S. Kar  A. Jadbabaie  and J. M. Moura  “On connectivity  observability  and
stability in distributed estimation ” in 49th IEEE Conference on Decision and Control  2010 
pp. 6639–6644.

[11] R. Olfati-Saber  “Distributed kalman ﬁltering for sensor networks ” in 46th IEEE Conference

on Decision and Control  2007  pp. 5492–5498.

[12] N. Cesa-Bianchi  Prediction  learning  and games. Cambridge University Press  2006.
[13] J. C. Duchi  A. Agarwal  and M. J. Wainwright  “Dual averaging for distributed optimization:
convergence analysis and network scaling ” IEEE Transactions on Automatic Control  vol. 57 
no. 3  pp. 592–606  2012.

[14] R. E. Kalman et al.  “A new approach to linear ﬁltering and prediction problems ” Journal of

basic Engineering  vol. 82  no. 1  pp. 35–45  1960.

[15] M. Mesbahi and M. Egerstedt  Graph theoretic methods in multiagent networks.

University Press  2010.

Princeton

[16] J. A. Tropp  “User-friendly tail bounds for sums of random matrices ” Foundations of Compu-

tational Mathematics  vol. 12  no. 4  pp. 389–434  2012.

[17] G. Biau  K. Bleakley  L. Gy¨orﬁ  and G. Ottucs´ak  “Nonparametric sequential prediction of

time series ” Journal of Nonparametric Statistics  vol. 22  no. 3  pp. 297–317  2010.

[18] L. Gyorﬁ and G. Ottucsak  “Sequential prediction of unbounded stationary time series ” IEEE

Transactions on Information Theory  vol. 53  no. 5  pp. 1866–1872  2007.

[19] L. Gy¨orﬁ  G. Lugosi et al.  Strategies for sequential prediction of stationary time series.

Springer  2000.

9

,Shahin Shahrampour
Sasha Rakhlin
Ali Jadbabaie
Mehryar Mohri
Andres Munoz