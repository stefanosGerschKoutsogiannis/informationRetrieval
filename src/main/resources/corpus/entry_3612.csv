2011,Multiclass Boosting: Theory and Algorithms,The problem of  multiclass boosting is considered. A new framework based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived  and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost  based on coordinate descent  updates one predictor component at a time  2) GD-MCBoost  based on gradient descent  updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent  2) margin enforcing  and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.,Multiclass Boosting: Theory and Algorithms

Mohammad J. Saberian

Statistical Visual Computing Laboratory 

University of California  San Diego

saberian@ucsd.edu

Nuno Vasconcelos

Statistical Visual Computing Laboratory 

University of California  San Diego

nuno@ucsd.edu

Abstract

The problem of multi-class boosting is considered. A new framework  based on
multi-dimensional codewords and predictors is introduced. The optimal set of
codewords is derived  and a margin enforcing loss proposed. The resulting risk is
minimized by gradient descent on a multidimensional functional space. Two algo-
rithms are proposed: 1) CD-MCBoost  based on coordinate descent  updates one
predictor component at a time  2) GD-MCBoost  based on gradient descent  up-
dates all components jointly. The algorithms differ in the weak learners that they
support but are both shown to be 1) Bayes consistent  2) margin enforcing  and
3) convergent to the global minimum of the risk. They also reduce to AdaBoost
when there are only two classes. Experiments show that both methods outperform
previous multiclass boosting approaches on a number of datasets.

1

Introduction

Boosting is a popular approach to classiﬁer design in machine learning. It is a simple and effective
procedure to combine many weak learners into a strong classiﬁer. However  most existing boosting
methods were designed primarily for binary classiﬁcation.
In many cases  the extension to M-
ary problems (of M > 2) is not straightforward. Nevertheless  the design of multi-class boosting
algorithms has been investigated since the introduction of AdaBoost in [8].

Two main approaches have been attempted. The ﬁrst is to reduce the multiclass problem to a col-
lection of binary sub-problems. Methods in this class include the popular “one vs all” approach  or
methods such as “all pairs”  ECOC [4  1]  AdaBoost-M2 [7]  AdaBoost-MR [18] and AdaBoost-
MH [18  9]. The binary reduction can have various problems  including 1) increased complexity  2)
lack of guarantees of an optimal joint predictor  3) reliance on data representations  such as adding
one extra dimension that includes class numbers to each data point [18  9]  that may not necessarily
enable effective binary discrimination  or 4) using binary boosting scores that do not represent true
class probabilities [15]. The second approach is to boost an M-ary classiﬁer directly  using multi-
class weak learners  such as trees. Methods of this type include AdaBoost-M1[7]  SAMME[12] and
AdaBoost-Cost [16]. These methods require strong weak learners which substantially increase com-
plexity and have high potential for overﬁtting. This is particularly problematic because  although
there is a uniﬁed view of these methods under the game theory interpretation of boosting [16]  none
of them has been shown to maximize the multiclass margin. Overall  the problem of optimal and
efﬁcient M-ary boosting is still not as well understood as its binary counterpart.

In this work  we introduce a new formulation of multi-class boosting  based on 1) an alternative
deﬁnition of the margin for M-ary problems  2) a new loss function  3) an optimal set of codewords 
and 4) the statistical view of boosting  which leads to a convex optimization problem in a multidi-
mensional functional space. We propose two algorithms to solve this optimization: CD-MCBoost 
which is a functional coordinate descent procedure  and GD-MCBoost  which implements functional
gradient descent. The two algorithms differ in terms of the strategy used to update the multidimen-
sional predictor. CD-MCBoost supports any type of weak learners  updating one component of
the predictor per boosting iteration  GD-MCBoost requires multiclass weak learners but updates all

1

components simultaneously. Both methods directly optimize the predictor of the multiclass problem
and are shown to be 1) Bayes consistent  2) margin enforcing  and 3) convergent to the global min-
imum of the classiﬁcation risk. They also reduce to AdaBoost for binary problems. Experiments
show that they outperform comparable prior methods on a number of datasets.

2 Multiclass boosting

We start by reviewing the fundamental ideas behind the classical use of boosting for the design of
binary classiﬁers  and then extend these ideas to the multiclass setting.

2.1 Binary classiﬁcation

A binary classiﬁer  F (x)  is a mapping from examples x ∈ X to class labels y ∈ {−1  1}. The
optimal classiﬁer  in the minimum probability of error sense  is Bayes decision rule

(1)
This can be hard to implement  due to the difﬁculty of estimating the probabilities PY |X (y|x). This
difﬁculty is avoided by large margin methods  such as boosting  which implement the classiﬁer as

F (x) = arg miny∈{−1 1}PY |X (y|x).

F (x) = sign[f

∗(x)]

where f ∗(x) : X → R is the continuous valued predictor
∗(x) = arg min

f

R(f )

f

that minimizes the classiﬁcation risk

(2)

(3)

(4)
associated with a loss function L[.  .]. In practice  the optimal predictor is learned from a sample
D = {(xi  yi)}n

i=1 of training examples  and (4) is approximated by the empirical risk

R(f ) = EX Y {L[y  f (x)]}

n(cid:1)

R(f ) ≈

L[yi  f (xi)].

i=1

(5)

The loss L[.  .] is said to be Bayes consistent if (1) and (2) are equivalent. For large margin methods 
such as boosting  the loss is also a function of the classiﬁcation margin yf (x)  i.e.

(6)
for some non-negative function φ(.). This dependence on the margin yf (x) guarantees that the
classiﬁer has good generalization when the training sample is small
[19]. Boosting learns the
optimal predictor f ∗(x) : X → R as the solution of

L[y  f (x)] = φ(yf (x))

(cid:2)

minf (x) R(f )
s.t

f (x) ∈ span(H).

(7)

where H = {h1(x)  ...hp(x)} is a set of weak learners hi(x) : X → R  and the optimization is
carried out by gradient descent in the functional space span(H) of linear combinations of hi(x) [14].

2.2 Multiclass setting

To extend the above formulation to the multiclass setting  we note that the deﬁnition of the classiﬁca-
tion labels as ±1 plays a signiﬁcant role in the formulation of the binary case. One of the difﬁculties
of the multiclass extension is that these labels do not have an immediate extension to the multiclass
setting. To address this problem  we return to the classical setting  where the class labels of a M-ary
problem take values in the set {1  . . .   M }. Each class k is then mapped into a distinct class label
yk  which can be thought of as a codeword that identiﬁes the class.
In the binary case  these codewords are deﬁned as y1 = 1 and y2 = −1. It is possible to derive
an alternative form for the expressions of the margin and classiﬁer F (x) that depends explicitly on
codewords. For this  we note that (2) can be written as
F (x) = arg max

∗(x)

ykf

(8)

k

2

and the margin can be expressed as

(cid:2)

yf =

f
−f

if k = 1
if k = 2

=

(cid:2)

1

2 (y1f − y2f )
2 (y2f − y1f )

1

if k = 1
if k = 2

=

1
2

(ykf − max
l(cid:3)=k

ylf ).

(9)

The interesting property of these forms is that they are directly extensible to the M-ary classiﬁcation
case. For this  we assume that the codewords yk and the predictor f (x) are multi-dimensional  i.e.
yk  f (x) ∈ Rd for some dimension d which we will discuss in greater detail in the following section.
The margin of f (x) with respect to class k is then deﬁned as

M(f (x)  yk) =

1
2

[< f (x)  yk > − max
l(cid:3)=k

< f (x)  yl >]

and the classiﬁer as

where < .  . > is the standard dot-product. Note that this is equivalent to

F (x) = arg maxk < f (x)  yk > 

F (x) = arg max

k∈{1 ... M }

M(f (x)  yk) 

(10)

(11)

(12)

and thus F (x) is the class of largest margin for the predictor f (x). This deﬁnition is closely related to
previous notions of multiclass margin. For example  it generalizes that of [11]  where the codewords
yk are restricted to the binary vectors in the canonical basis of Rd  and is a special case of that in
[1]  where the dot products < f (x)  yk > are replaced by a generic function of f  x  and k. Given a
training sample D = {(xi  yi)}n

i=1  the optimal predictor f ∗(x) minimizes the risk

RM (f ) = EX Y {LM [y  f (x)]} ≈

LM [yi  f (xi)]}

(13)

n(cid:1)

i=1

where LM [.  .] is a multiclass loss function. A natural extension of (6) and (9) is a loss of the form
(14)

LM [y  f (x)] = φ(M(f (x)  y)).
To avoid the nonlinearity of the max operator in (10)  we rely on

M(cid:1)

LM [y  f (x)] =

k=1

− 1
e

2

[<f (x) y>−<f (x) y

k

>]

.

(15)

which is shown  in Appendix A  to upper bound 1 + e−M(f (x) y). It follows that the minimization of
the risk of (13) encourages predictors of large margin M(f ∗(xi)  yi)  ∀i. For M = 2  LM [y  f (x)]
reduces to

(16)
and the risk minimization problem is identical to that of AdaBoost [8]. In appendices B and C it
is shown that RM (f ) is convex and Bayes consistent  in the sense that if f ∗(x) is the minimizer of
(13)  then

L2[y  f (x)] = 1 + e

−yf (x)

< f

∗(x)  yk >= log PY |X (yk|x) + c ∀k

(17)

and (11) implements the Bayes decision rule

F (x) = arg maxkPY |X (yk|x).

(18)

2.3 Optimal set of codewords

From (15)  the choice of codewords yk has an impact in the optimal predictor f ∗(x)  which is
determined by the projections < f ∗(x)  yk >. To maximize the margins of (10)  the difference
between these projections should be as large as possible. To accomplish this we search for the set of
M distinct unit codewords Y = {y1  . . .   yM } ∈ Rd that are as dissimilar as possible

⎧⎪⎨
⎪⎩

maxd y1 ...yM [mini(cid:3)=j ||yi − yj||2]

s.t

||yk|| = 1 ∀k = 1..M.
yk ∈ Rd ∀k = 1..M.

3

(19)

1.5

1

0 5
0.5

0

-0.5

-1

-1.5

-1.5

-1

-0.5

0

0.5

1

1.5

1.5

1

0.5

0

-0.5

-1

-1.5

-1.5

1

0

-1

-1

-0.5

0

0.5

1

1.5

(M = 2)

(M = 3)

1

0

-1

-1

(M = 4)

1

0

Figure 1: Optimal codewords for M = 2  3  4.

To solve this problem we start by noting that  for d < M  the smallest distance of (19) can be
increased by simply increasing d  since this leads to a larger space. On the other hand  since M
points y1  ...yM lie in an  at most  M − 1 dimensional subspace of Rd  e.g. any three points belong
to a plane  there is no beneﬁt in increasing d beyond M − 1. On the contrary  as shown in Appendix
D  if d > M − 1 there exits a vector v ∈ Rd with equal projection on all codewords 

< yi  v >=< yj  v > ∀i  j = 1  ..  M.

(20)

Since the addition of v to the predictor f (x) does not change the classiﬁcation rule of (11)  this makes
the optimal predictor underdetermined. To avoid this problem  we set d = M − 1. In this case  as
shown in Appendix E  the vertices of a M −1 dimensional regular simplex1 centered at the origin [3]
are solutions of (19). Figure 1 presents the set of optimal codewords when M = 2  3  4. Note that
in the binary case this set consists of the traditional codewords yi ∈ {+1  −1}. In general  there is
no closed form solution for the vertices of a regular simplex of M vectors. However  these can be
derived from those of a regular simplex of M − 1 vectors  and a recursive solution is possible [3].

3 Risk minimization

We have so far deﬁned a proper margin loss function for M-ary classiﬁcation and identiﬁed an
optimal codebook. In this section  we derive two boosting algorithms for the minimization of the
classiﬁcation risk of (13). These algorithms are both based on the GradientBoost framework [14].
The ﬁrst is a functional coordinate descent algorithm  which updates a single component of the
predictor per boosting iteration. The second is a functional gradient descent algorithm that updates
all components simultaneously.

3.1 Coordinate descent

In the ﬁrst method  each component f ∗
the linear combination of weak learners that solves the optimization problem

i (x) of the optimal predictor f ∗(x) = [f ∗

1 (x)  ..f ∗

M −1(x)]  is

(cid:2)

minf1(x) ... fM −1(x) R([f1(x)  ...  fM −1(x)])
s.t

fj(x) ∈ span(H) ∀j = 1..M − 1.

(21)

1(x)  ...  f t

where H = {h1(x)  ...hp(x)} is a set of weak learners  hi(x) : X → R. These can be
stumps  regression trees  or member of any other suitable model family. We denote by f t(x) =
M −1(x)] the predictor available after t boosting iterations. At iteration t + 1 a single
[f t
component fj(x) of f (x) is updated with a step in the direction of the scalar functional g that most
decreases the risk R[f t
M −1]. For this  we consider the functional derivative of
R[f (x)] along the direction of the functional g : X → R  at point f (x) = f t(x)  with respect to the
jth component fj(x) of f (x) [10] 

1  ...  f t

j + α∗

j g  ...  f t

(cid:7)(cid:7)(cid:7)(cid:7)

δR[f t; j  g] =

∂R[f t + g1j]

∂

 

=0

(22)

1A regular M − 1 dimensional simplex is the convex hull of M normal vectors which have equal pair-wise

distances.

4

n(cid:1)

n(cid:1)

i=1

where 1j ∈ Rd is a vector whose jth element is one and the remainder zero  i.e. f t + g1j =
[f t

M −1]. Using the risk of (13)  it is shown in Appendix F that

j + g  ..f t

1  ..  f t

−δR[f t; j  g] =

g(xi)wj
i  

with

wj

i =

1
2

− 1
e

2 <f

t(xi) yi>

M(cid:1)

k=1

The direction of greatest risk decrease is the weak learner

i=1

< 1j  yi − yk > e

1

2 <f

t(xi) y

k

>.

∗
j (x) = arg max
g
g∈H

g(xi)wj
i  

and the optimal step size along this direction
∗
j = arg min
α∈R

α

The classiﬁer is thus updated as

R[f t(x) + αg

∗
j (x)1j].

f t+1 = f t(x) + α

(27)
This procedure is summarized in Algorithm 1-left and denoted CD-MCBoost. It starts with f 0(x) =
0 ∈ Rd and updates the predictor components sequentially. Note that  since (13) is a convex function
of f (x)  it converges to the global minimum of the risk.

j + α

M −1]

1  ...  f t

∗
j g

∗
j (x)1j = [f t

∗
j g

∗
j   ...  f t

3.2 Gradient descent

Alternatively  (13) can be minimized by learning a linear combination of multiclass weak learners.
In this case  the optimization problem is

(cid:2)

minf (x) R[f (x)]
s.t

f (x) ∈ span(H) 

(28)

where H = {h1(x)  ...  hp(x)} is a set of multiclass weak learners  hi(x) : X → RM −1  such as
decision trees. Note that to ﬁt tree classiﬁers in this deﬁnition their output (usually a class number)
should be translated into a class codeword. As before  let f t(x) ∈ RM −1 be the predictor available
after t boosting iterations. At iteration t + 1 a step is given along the direction g(x) ∈ H of largest
decrease of the risk R[f (x)]. For this  we consider the directional functional derivative of R[f (x)]
along the direction of the functional g : X → RM −1  at point f (x) = f t(x).

(23)

(24)

(25)

(26)

(29)

(30)

(31)

(32)

(33)

δR[f t; g] =

−δR[f t; g] =

As shown in Appendix G 

where wi ∈ RM −1

wi =

1
2

− 1
e

2 <f

t(xi) yi>

(cid:7)(cid:7)(cid:7)(cid:7)

.

=0

∂R[f t + g]

∂

n(cid:1)

< g(xi)  wi >

i=1

M(cid:1)

(yi − yk)e

1

2 <f

t(xi) y

k

>.

k=1

n(cid:1)

The direction of greatest risk decrease is the weak learner

g

∗(x) = arg max
g∈H

< g(xi)  wi > 

i=1

and the optimal step size along this direction

α

∗ = arg min
α∈R

R[f t(x) + αg

∗(x)].

The predictor is updated to f t+1(x) = f t(x)+α∗g∗(x). This procedure is summarised in Algorithm
1-right  and denoted GD-MCBoost. Since (13) is convex  it converges to the global minimum of the
risk.

5

Algorithm 1 CD-MCBoost and GD-MCBoost
Input: Number of classes M  set of codewords Y = {y1  . . .   yM }  number of iterations N and
dataset S = {(x1  y1)  ...  (xn  yn)}  where xi are examples and yi ∈ Y are their class codewords.
Initialization: set t = 0  and f t = 0 ∈ RM −1

CD-MCBoost

while t < N do

for j = 1 to M − 1 do

j (x)  α∗

Compute wj
Find g∗
Update f t+1
Update f t+1
k
t = t + 1

j

i with (24)

j using (25) and (26)
j (x) + α∗
j g∗
(x) = f t
k(x) ∀k (cid:5)= j
(x) = f t

j (x)

end for
end while

GD-MCBoost

while t < N do

Compute wi with (31)
Find g∗(x)  α∗ using (32) and (33)
Update f t+1(x) = f t(x) + α∗g∗(x)
t = t + 1

end while

Output: decision rule: F (x) = arg maxk < f N (x)  yk >

4 Comparison to previous methods

2

Multi-dimensional predictors and codewords have been used implicitly  [7  18  16  6]  or explicitly 
[12  9]  in all previous multiclass boosting methods.
“one vs all”  “all pairs” and “ECOC” [1]: as shown in [1]  these methods can be interpreted
as assigning a codeword yk to each class  where yk ∈ {+1  0  −1}l and l = M for “one vs all” 
l = M (M −1)
for “all pairs” and l is variable for “ECOC”  depending on the error correction code. In
all these methods  binary classiﬁers are learned independently for each of the codeword components.
This does not guarantee an optimal joint predictor. These methods are similar to CD-MCBoost in the
sense that the predictor components are updated individually at each boosting iteration. However 
in CD-MCBoost  the codewords are not restricted to {+1  0  −1} and the predictor components are
learned jointly.
AdaBoost-MH [18  9]: This method converts the M-ary classiﬁcation problem into a binary one 
learned from a M times larger training set  where each example x is augmented with a feature y that
identiﬁes a class. Examples such that x belongs to class y receive binary label 1  while the remaining
receive the label −1 [9]. In this way  the binary classiﬁer learns if the multiclass label y is correct
for x or not. AdaBoost-MH uses weak learners ht : X × {1  . . .   M } → R and the decision rule

¯F (x) = arg max

j∈{1 2 ..M }

ht(x  j)

t

(34)

where t is the iteration number. This is equivalent to the decision rule of (11) if f (x) is an M-
dimensional predictor with jth component fj(x) =
t ht(x  j)  and the label codewords are de-
ﬁned as yj = 1j. This method is comparable to CD-MCBoost in the sense that it does not require
multiclass weak learners. However  there are no guarantees that the weak learners in common use
are able to discriminate the complex classes of the augmented binary problem.
AdaBoost-M1 [7] and AdaBoost-Cost [16]: These methods use multiclass weak learners ht :
X → {1  2  ..M } and a classiﬁcation rule of the form

(cid:1)

(cid:8)

(cid:1)

¯F (x) = arg max

j∈{1 2 ..M }

αtht(x) 

t|ht(x)=j

(35)

where t is the boosting iteration and αt the coefﬁcient of weak learner ht(x). This is equivalent
(cid:8)
to the decision rule of (11) if f (x) is an M-dimensional predictor with jth component fj(x) =
t|ht(x)=j αtht(x) and label codewords yj = 1j. These methods are comparable to GD-MCBoost 
in the sense that they update the predictor components simultaneously. However  they have not been
shown to be Bayes consistent  and it is not clear that they can be interpreted as maximizing the
multiclass margin.

6

)
x
(

2

f

1

0.5

0

−0.5

−1
−1

y1
y2
y3

−0.5

0

0.5

(x)
f
1
t = 0

1

1.5

)
x
(

2

f

4

2

0

−2

−4
−2

class 1
class 2
class 3
y1
y2
y3

−1

0

1

f
(x)
1
t = 10

2

3

)
x
(

2

f

4

2

0

−2

−4
−2

Class 1
Class 2
Class 3
y1
y2
y3

−1

0

f
(x)
1

1

2

3

t = 100

Figure 2: Classiﬁer predictions of CD-MCBoost  on the test set  after t = 0  10  100 boosting iterations.

SAMME [12]: This method explicitly uses M-dimensional predictors with codewords

(cid:10)

−1

M − 1

−1

M − 1

 

  ...  1 

−1

M − 1

−1

M − 1

 

∈ RM  

(36)

(cid:9)

yj =

M 1j − 1
M − 1

=

and decision rule

¯F (x) = arg max

fj(x).

(37)

j∈{1 2 ..M }

(cid:8)
Since  as discussed in Section 2.3  the optimal detector is not unique when the predictor is M-
dimensional  this algorithm includes the additional constraint
j=1 fj(x) = 0 and solves a con-
strained optimization problem [12  9]. It is comparable to GD-MCBoost in the sense that it up-
dates the predictor components simultaneously  but uses the loss function LSAMM E[yk  f (x)] =
e− 1

 f (x)>. Using (36)  the minimization of this loss is equivalent to maximizing

M <y

M

k

(cid:1)

M(cid:4)(f (x)  yk) =< f (x)  yk >= fk(x) −

1

M − 1

fj(x) 

j(cid:3)=k

(38)

which is not a proper margin since M(cid:4)(f (x)  yk) > 0 does not imply correct classiﬁcation i.e.
fk(x) > fj(x) ∀j (cid:5)= k. Hence  SAMME does not guarantee a large margin solution for the
multiclass problem.

When compared to all these methods  MCBoost has the advantage of combining 1) a Bayes consis-
tent and margin enforcing loss function  2) an optimal set of codewords  3) the ability to boost any
type of weak learner  4) guaranteed convergence to the global minimum of (21)  for CD-MCBoost  or
(28)  for GD-MCBoost  and 5) equivalence to the classical AdaBoost algorithm for binary problems.
It is worth emphasizing that MCBoost can boost any type of weak learners of non-zero directional
derivative  i.e. non-zero (23) for CD-MCBoost and (30) for GD-MCBoost. This is independent
of the type of weak learner output  and unlike previous multiclass boosting approaches  which can
only boost weak learners of speciﬁc output types. Note that  although the weak learner selection
criteria of previous approaches can have interesting interpretations  e.g. based on weighted error
rates [16]  these only hold for speciﬁc weak learners. Finally  MCBoost extends the deﬁnition of
margin and loss function to multi-dimensional predictors. The derivation of Section 2 can easily be
generalized to the design of other multiclass boosting algorithms by the use of 1) alternative φ(v)
functions in (14) (e.g. those of the logistic [9] or Tangent [13] losses for increased outlier robustness 
asymmetric losses for cost-sensitive classiﬁcation  etc.)  and 2) alternative optimization approaches
(e.g. Newton’s method [9  17]).

5 Evaluation

A number of experiments were conducted to evaluate the MCBoost algorithms2.

5.1 Synthetic data

We start with a synthetic example  for which the optimal decision rule is known. This is a three class
problem  with two-dimensional Gaussian classes of means [1  2]  [−1  0]  [2  −1] and covariances of

2Codes for CD-MCBoost and GD-MCBoost are available from [2].

7

Table 1: Accuracy of multiclass boosting methods  using decision stumps  on six UCI data sets

method

One Vs All

AdaBoost-MH [18]

CD-MCBoost

letter

pendigit

landsat
isolet
84.80% 50.92% 86.56% 89.93% 87.11% 88.97%
47.70% 15.73% 24.41% 73.62% 79.16% 66.71%
85.70% 49.60% 89.51% 92.82% 88.01% 91.02%

optdigit

shuttle

Table 2: Accuracy of multiclass boosting methods  using trees of max depth 2  on six UCI data sets

method

AdaBoost-M1[7]

AdaBoost-SAMME[12]

AdaBoost-Cost [16]

GD-MCBoost

letter

−

optdigit

pendigit

landsat
72.85%
79.80% 45.65% 83.82% 87.53% 99.70% 61.00%
83.95% 42.00% 80.53% 86.20% 99.55% 63.69%
86.65% 59.65% 92.94% 92.32% 99.73% 84.28%

shuttle
96.45%

isolet

−

−

−

[1  0.5; 0.5  2] [1  0.3; 0.3  1] [.4  0.1; 0.1  0.8] respectively. Training and test sets of 1  000 examples
each were randomly sampled and the Bayes rule computed in closed form [5]. The associated Bayes
error rate was 11.67% in the training and 11.13% in the test set. A classiﬁer was learned with
CD-MCBoost and decision stumps.
Figure 2) shows predictions3 of f t(x) on the test set  for t = 0  10  100. Note that f 0(xi) = [0  0]
for all examples xi. However  as the iterations proceed  CD-MCBoost produces predictions that are
more aligned with the true class codewords  shown as dashed lines  while maximizing the distance
between examples of different classes (by increasing their distance to the origin). In this context 
“alignment of f (x) with yk” implies that < f (x)  yk >≥< f (x)  yj >  ∀j (cid:5)= k. This combination
of alignment and distance maximization results in higher margins  leading to more accurate and
robust classiﬁcation. The test error rate after 100 iterations of boosting was 11.30%  and very close
to the Bayes error rate of 11.13%.

5.2 CD-MCBoost

We next conducted a number of experiments to evaluate the performance of CD-MCBoost on the
six UCI datasets of Table 1. Among the methods identiﬁed as comparable in the previous section 
we implemented “one vs all” and AdaBoost-MH [18].
In all cases  decision stumps were used
as weak learners  and we used the training/test set decomposition speciﬁed for each dataset. The
“one vs all” detectors were trained with 20 iterations. The remaining methods were then allowed
to include the same number of weak learners in their ﬁnal decision rules. Table 1 presents the
resulting classiﬁcation accuracies. CD-MCBoost produced the most accurate classiﬁer in four of
the ﬁve datasets  and was a close second in the remaining one. “One vs all” achieved the next best
performance  with AdaBoost-MH producing the worst classiﬁers.

5.3 GD-MCBoost

Finally  the performance of GD-MCBoost was compared to AdaBoost-M1 [7]  AdaBoost-Cost [16]
and AdaBoost-SAMME [12]. The experiments were based on the UCI datasets of the previous sec-
tion  but the weak learners were now trees of depth 2. These were built with a greedy procedure
so as to 1) minimize the weighted error rate of AdaBoost-M1 [7] and AdaBoost-SAMME[12]  2)
minimize the classiﬁcation cost of AdaBoost-Cost [16]  or 3) maximize (32) for GD-MCBoost. Ta-
ble 2 presents the classiﬁcation accuracy of each method  for 50 training iterations. GD-MCBoost
achieved the best accuracy on all datasets  reaching substantially larger classiﬁcation rate than all
other methods in the most difﬁcult datasets  e.g.
from a previous best of 63.69% to 84.28% in
isolet  45.65% to 59.65% in letter  and 83.82% to 92.94% in pendigit. Among the remaining meth-
ods  AdaBoost-SAMME achieved the next best performance  although this was close to that of
AdaBoost-Cost. AdaBoost-M1 had the worst results  and was not able to boost the weak learners
used in this experiment for four of the six datasets. It should be noted that the results of Tables 1 and
2 are not directly comparable  since the classiﬁers are based on different types of weak learners and
have different complexities.

3We emphasize the fact that these are plots of f t(x) ∈ R2  not x ∈ R2.

8

References

[1] E. L. Allwein  R. E. Schapire  and Y. Singer. Reducing multiclass to binary: a unifying approach for

margin classiﬁers. J. Mach. Learn. Res.  1:113–141  September 2001.

[2] N. N. Author. Suppressed for anonymity.
[3] H. S. M. Coxeter. Regular Polytopes. Dover Publications  1973.
[4] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes.

Journal of Artiﬁcial Intelligence Research  2:263–286  1995.

[5] R. O. Duda  P. E. Hart  and D. G. Stork. Pattern Classiﬁcation. Wiley  New York  2. edition  2001.
[6] G. Eibl and R. Schapire. Multiclass boosting for weak classiﬁers.

In Journal of Machine Learning

Research  pages 6–189  2005.

[7] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Proceedings of the Thir-

teenth International Conference In Machine Learning  pages 148–156  1996.

[8] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Comp. and Sys. Science  1997.

[9] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of boosting.

Annals of Statistics  28  1998.

[10] B. A. Frigyik  S. Srivastava  and M. R. Gupta. An introduction to functional derivatives. Technical

Report(University of Washington)  2008.

[11] Y. Guermeur. Vc theory of large margin multi-category classiﬁers. J. Mach. Learn. Res.  8:2551–2594 

December 2007.

[12] S. R. Ji Zhu  Hui Zou and T. Hastie. Multi-class adaboost. Statistics and Its Interface  2:349–3660  2009.
[13] H. Masnadi-Shirazi  N. Vasconcelos  and V. Mahadevan. On the design of robust classiﬁers for computer

vision. In CVPR  2010.

[14] L. Mason  J. Baxter  P. Bartlett  and M. Frean. Boosting algorithms as gradient descent. In NIPS  2000.
[15] D. Mease and A. Wyner. Evidence contrary to the statistical view of boosting. J. Mach. Learn. Res. 

9:131–156  June 2008.

[16] I. Mukherjee and R. E. Schapire. A theory of multiclass boosting. In NIPS  2010.
[17] M. J. Saberian  H. Masnadi-Shirazi  and N. Vasconcelos. Taylorboost: First and second order boosting

algorithms with explicit margin control. In CVPR  2010.

[18] R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Mach.

Learn.  37:297–336  December 1999.

[19] V. N. Vapnik. Statistical Learning Theory. John Wiley Sons Inc  1998.

9

,Cem Subakan
Johannes Traa
Paris Smaragdis