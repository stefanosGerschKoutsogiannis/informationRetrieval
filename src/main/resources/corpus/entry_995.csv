2015,Collaboratively Learning Preferences from Ordinal Data,In personalized recommendation systems  it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly  in revenue management  it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model  a popular discrete choice model  captures  the structure of the hidden preferences  with a low-rank matrix. In order to predict the preferences  we want to learn the underlying model from noisy observations of the low-rank matrix  collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases  we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples  and  provide a matching information-theoretic lower bound.,CollaborativelyLearningPreferencesfromOrdinalDataSewoongOh KiranK.ThekumparampilUniversityofIllinoisatUrbana-Champaign{swoh thekump2}@illinois.eduJiamingXuTheWhartonSchool UPennjiamingx@wharton.upenn.eduAbstractInpersonalizedrecommendationsystems itisimportanttopredictpreferencesofauseronitemsthathavenotbeenseenbythatuseryet.Similarly inrevenuemanagement itisimportanttopredictoutcomesofcomparisonsamongthoseitemsthathaveneverbeencomparedsofar.TheMultiNomialLogitmodel apopulardiscretechoicemodel capturesthestructureofthehiddenpreferenceswithalow-rankmatrix.Inordertopredictthepreferences wewanttolearntheunderlyingmodelfromnoisyobservationsofthelow-rankmatrix collectedasrevealedpreferencesinvariousformsofordinaldata.Anaturalapproachtolearnsuchamodelistosolveaconvexrelaxationofnuclearnormminimization.Wepresenttheconvexrelaxationapproachintwocontextsofinterest:collaborativerankingandbundledchoicemodeling.Inbothcases weshowthattheconvexrelaxationisminimaxoptimal.Weproveanupperboundontheresultingerrorwithﬁnitesamples andprovideamatchinginformation-theoreticlowerbound.1IntroductionInrecommendationsystemsandrevenuemanagement itisimportanttopredictpreferencesonitemsthathavenotbeenseenbyauserorpredictoutcomesofcomparisonsamongthosethathaveneverbeencompared.Predictingsuchhiddenpreferenceswouldbehopelesswithoutfurtherassump-tionsonthestructureofthepreference.Motivatedbythesuccessofmatrixfactorizationmodelsoncollaborativeﬁlteringapplications wemodelhiddenpreferenceswithlow-rankmatricestocollab-orativelylearnpreferencematricesfromordinaldata.Thispaperconsidersthefollowingscenarios:•Collaborativeranking.Consideranonlinemarketthatcollectseachuser’spreferenceasarankingoverasubsetofitemsthatare‘seen’bytheuser.Suchdatacanbeobtainedbydirectlyaskingtocomparesomeitems orbyindirectlytrackingonlineactivitiesonwhichitemsareviewed howmuchtimeisspentonthepage orhowtheuserratedtheitems.Inordertomakepersonalizedrecommendations wewantamodelwhich(a)captureshowuserswhoprefersimilaritemsarealsolikelytohavesimilarpreferencesonunseenitems (b)predictswhichitemsausermightprefer bylearningfromsuchordinaldata.•Bundledchoicemodeling.Discretechoicemodelsdescribehowausermakesdecisionsonwhattopurchase.Typicalchoicemodelsassumethewillingnesstobuyanitemisindepen-dentofwhatelsetheuserbought.Inmanycases however wemake‘bundled’purchases:webuyparticularingredientstogetherforonerecipeorwebuytwoconnectingﬂights.Onechoice(theﬁrstﬂight)hasasigniﬁcantimpactontheother(theconnectingﬂight).Inordertooptimizetheassortment(whichﬂightschedulestooffer)formaximumexpectedrev-enue itiscrucialtoaccuratelypredictthewillingnessoftheconsumerstopurchase basedonpasthistory.Weconsideracasewheretherearetwotypesofproducts(e.g.jeansandshirts) andwant(a)amodelthatcapturessuchinteractingpreferencesforpairsofitems onefromeachcategory;and(b)topredicttheconsumer’schoiceprobabilitiesonpairsofitems bylearningsuchmodelsfrompastpurchasehistory.1WeuseadiscretechoicemodelknownasMultiNomialLogit(MNL)model[1](describedinSection2.1)torepresentthepreferences.Incollaborativerankingcontext MNLusesalow-rankmatrixtorepresentthehiddenpreferencesoftheusers.Eachrowcorrespondstoauser’spreferenceoveralltheitems andwhenpresentedwithasubsetofitemstheuserprovidesarankingoverthoseitems whichisanoisyversionofthehiddentruepreference.Thelow-rankassumptionnaturallycapturesthesimilaritiesamongusersanditems byrepresentingeachonalow-dimensionalspace.Inbundledchoicemodelingcontext thelow-rankmatrixnowrepresentshowpairsofitemsarematched.Eachrowcorrespondstoanitemfromtheﬁrstcategoryandeachcolumncorrespondstoanitemfromthesecondcategory.Anentryinthematrixrepresentshowmuchthepairispreferredbyarandomlychosenuserfromapoolofusers.Noticethatinthiscasewedonotmodelindividualpreferences butthepreferenceofthewholepopulation.Thepurchasehistoryofthepopulationistherecordofwhichpairwaschosenamongasubsetsofitemsthatwerepresented whichisagainanoisyversionofthehiddentruepreference.Thelow-rankassumptioncapturesthesimilaritiesanddis-similaritiesamongtheitemsinthesamecategoryandtheinteractionsacrosscategories.Contribution.Anaturalapproachtolearnsuchalow-rankmodel fromnoisyobservations istosolveaconvexrelaxationofnuclearnormminimization(describedinSection2.2) sincenuclearnormisthetightestconvexsurrogatefortherankfunction.Wepresentsuchanapproachforlearn-ingtheMNLmodelfromordinaldata intwocontexts:collaborativerankingandbundledchoicemodeling.Inbothcases weanalyzethesamplecomplexityofthealgorithm andprovideanupperboundontheresultingerrorwithﬁnitesamples.Weproveminimax-optimalityofourapproachbyprovidingamatchinginformation-theoreticlowerbound(uptoapoly-logarithmicfactor).Techni-cally weutilizetheRandomUtilityModel(RUM)[2 3 4]interpretation(outlinedinSection2.1)oftheMNLmodeltoproveboththeupperboundandthefundamentallimit whichcouldbeofinteresttoanalyzingmoregeneralclassofRUMs.Relatedwork.Inthecontextofcollaborativeranking MNLmodelshavebeenproposedtomodelpartialrankingsfromapoolofusers.Recently therehasbeennewalgorithmsandanalysesofthosealgorithmstolearnMNLmodelsfromsamples inthecasewheneachuserprovidespair-wisecomparisons[5 6].[6]proposessolvingaconvexrelaxationofmaximizingthelikelihoodovermatriceswithboundednuclearnorm.Itisshownthatthisapproachachievesstatisticallyoptimalgeneralizationerrorrate insteadofFrobeniusnormerrorthatweanalyze.Ouranalysistechniquesareinspiredby[5] whichproposedtheconvexrelaxationforlearningMNL butwhentheusersprovideonlypair-wisecomparisons.Inthispaper wegeneralizetheresultsof[5]byanalyzingmoregeneralsamplingmodelsbeyondpairwisecomparisons.Theremainderofthepaperisorganizedasfollows.InSection2 wepresenttheMNLmodelandproposeaconvexrelaxationforlearningthemodel inthecontextofcollaborativeranking.WeprovidetheoreticalguaranteesforcollaborativerankinginSection3.InSection4 wepresenttheproblemstatementforbundledchoicemodeling andanalyzeasimilarconvexrelaxationapproach.Notations.Weuse|||A|||Fand|||A|||∞todenotetheFrobeniusnormandthe‘∞norm |||A|||nuc=Piσi(A)todenotethenuclearnormwhereσi(A)denotethei-thsingularvalue and|||A|||2=σ1(A)forthespectralnorm.Weusehhu vii=PiuiviandkuktodenotetheinnerproductandtheEuclideannorm.Allonesvectorisdenotedby1andI(A)istheindicatorfunctionoftheeventA.ThesetoftheﬁstNintegersaredenotedby[N]={1 ... N}.2ModelandAlgorithmInthissection wepresentadiscretechoicemodelingforcollaborativeranking andproposeaninferencealgorithmforlearningthemodelfromordinaldata.2.1MultiNomialLogit(MNL)modelforcomparativejudgmentIncollaborativeranking wewanttomodelhowpeoplewhohavesimilarpreferencesonasubsetofitemsarelikelytohavesimilartastesonotheritemsaswell.Whenusersprovideratings asincollaborativeﬁlteringapplications matrixfactorizationmodelsarewidelyusedsincethelow-rankstructurecapturesthesimilaritiesbetweenusers.Whenusersprovideorderedpreferences weuseadiscretechoicemodelknownasMultiNomialLogit(MNL)[1]modelthathasasimilarlow-rankstructurethatcapturesthesimilaritiesbetweenusersanditems.2LetΘ∗bethed1×d2dimensionalmatrixcapturingthepreferenceofd1usersond2items wheretherowsandcolumnscorrespondtousersanditems respectively.Typically Θ∗isassumedtobelow-rank havingarankrthatismuchsmallerthanthedimensions.However inthefollowingweallowamoregeneralsettingwhereΘ∗mightbeonlyapproximatelylowrank.WhenauseriispresentedwithasetofalternativesSi⊆[d2] sherevealsherpreferencesasarankedlistoverthoseitems.Tosimplifythenotationsweassumealluserscomparethesamenumberkofitems buttheanalysisnaturallygeneralizestothecasewhenthesizemightdifferfromausertoauser.Letvi ‘∈Sidenotethe(random)‘-thbestchoiceofuseri.Eachusergivesaranking independentofotherusers’rankings fromP{vi 1 ... vi k}=kY‘=1eΘ∗i vi ‘Pj∈Si ‘eΘ∗i j (1)wherewithSi ‘≡Si\{vi 1 ... vi ‘−1}andSi 1≡Si.Forauseri thei-throwofΘ∗representstheunderlyingpreferencevectoroftheuser andthemorepreferreditemsaremorelikelytoberankedhigher.Theprobabilisticnatureofthemodelcapturesthenoiseintherevealedpreferences.Therandomutilitymodel(RUM) pioneeredby[2 3 4] describesthechoicesofusersasmanifes-tationsoftheunderlyingutilities.TheMNLmodelsisaspecialcaseofRUMwhereeachdecisionmakerandeachalternativearerepresentedbyar-dimensionalfeaturevectorsuiandvjrespectively suchthatΘ∗ij=hhui vjii resultinginalow-rankmatrix.WhenpresentedwithasetofalternativesSi thedecisionmakeriranksthealternativesaccordingtotheirrandomutilitydrawnfromUij=hhui vjii+ξij (2)foritemj whereξijfollowthestandardGumbeldistribution.Intuitively thisprovidesajustiﬁcationfortheMNLmodelasmodelingthedecisionmakersasrationalbeing seekingtomaximizeutility.Technically thisRUMinterpretationplaysacrucialroleinouranalysis inprovingrestrictedstrongconvexityinAppendixA.5andalsoinprovingfundamentallimitinAppendixC.ThereareafewcaseswheretheMaximumLikelihood(ML)estimationforRUMistractable.OnenotableexampleisthePlackett-Luce(PL)model whichisaspecialcaseoftheMNLmodelwhereΘ∗isrank-oneandallusershavethesamefeatures.PLmodelhasbeenwidelyappliedinecono-metrics[1] analyzingelections[7] andmachinelearning[8].Efﬁcientinferencealgorithmshasbeenproposed[9 10 11] andthesamplecomplexityhasbeenanalyzedfortheMLE[12]andfortheRankCentrality[13].AlthoughPLisquiterestrictive inthesensethatitassumesalluserssharethesamefeatures littleisknownaboutinferenceinRUMsbeyondPL.Recently toovercomesucharestriction mixedPLmodelshavebeenstudied whereΘ∗isrank-rbutthereareonlyrclassesofusersandallusersinthesameclasshavethesamefeatures.Efﬁcientinferencealgorithmswithprovableguaranteeshavebeenproposedbyapplyingrecentadvancesintensordecompositionmeth-ods[14 15] directlyclusteringtheusers[16 17] orusingsamplingmethods[18].However thismixturePLisstillrestrictive andbothclusteringandtensorbasedapproachesrelyheavilyonthefactthatthedistributionisa“mixture”andrequireadditionalincoherenceassumptionsonΘ∗.Formoregeneralmodels efﬁcientinferencealgorithmshavebeenproposed[19]butnoperformanceguaranteeisknownforﬁnitesamples.AlthoughtheMLEforthegeneralMNLmodelin(1)isintractable weprovideapolynomial-timeinferencealgorithmwithprovableguarantees.2.2NuclearnormminimizationAssumingΘ∗iswellapproximatedbyalow-rankmatrix weestimateΘ∗bysolvingthefollowingconvexrelaxationgiventheobservedpreferenceintheformofrankedlists{(vi 1 ... vi k)}i∈[d1].bΘ∈argminΘ∈ΩL(Θ)+λ|||Θ|||nuc (3)wherethe(negative)loglikelihoodfunctionaccordingto(1)isL(Θ)=−1kd1d1Xi=1kX‘=1hhΘ eieTvi ‘ii−logXj∈Si ‘exp(cid:0)hhΘ eieTjii(cid:1) (4)withSi={vi 1 ... vi k}andSi ‘≡Si\{vi 1 ... vi ‘−1} andappropriatelychosensetΩdeﬁnedin(7).Sincenuclearnormisatightconvexsurrogatefortherank theaboveoptimizationsearches3foralow-ranksolutionthatmaximizesthelikelihood.Nuclearnormminimizationhasbeenwidelyusedinrankminimizationproblems[20] butprovableguaranteestypicallyexistsonlyforquadraticlossfunctionL(Θ)[21 22].Ouranalysisextendssuchanalysistechniquestoidentifytheconditionsunderwhichrestrictedstrongconvexityissatisﬁedforaconvexlossfunctionthatisnotquadratic.3Collaborativerankingfromk-wisecomparisonsWeﬁrstprovidebackgroundontheMNLmodel andthenpresentmainresultsontheperformanceguarantees.Noticethatthedistribution(1)isindependentofshiftingeachrowofΘ∗byaconstant.Hence thereisanequivalentclassofΘ∗thatgivesthesamedistributionsfortherankedlists:[Θ∗]={A∈Rd1×d2|A=Θ∗+u1Tforsomeu∈Rd1}.(5)SincewecanonlyestimateΘ∗uptothisequivalentclass wesearchfortheonewhoserowssumtozero i.e.Pj∈[d2]Θ∗i j=0foralli∈[d1].Letα≡maxi j1 j2|Θ∗ij1−Θ∗ij2|denotethedynamicrangeoftheunderlyingΘ∗ suchthatwhenkitemsarecompared wealwayshave1ke−α≤11+(k−1)eα≤P{vi 1=j}≤11+(k−1)e−α≤1keα (6)forallj∈Si allSi⊆[d2]satisfying|Si|=kandalli∈[d1].Wedonotmakeanyassumptionsonαotherthanthatα=O(1)withrespecttod1andd2.Thepurposeofdeﬁningthedynamicrangeinthiswayisthatweseektocharacterizehowtheerrorscaleswithα.Giventhisdeﬁnition wesolvetheoptimizationin(3)overΩα=nA∈Rd1×d2(cid:12)(cid:12)|||A|||∞≤α and∀i∈[d1]wehaveXj∈[d2]Aij=0o.(7)Whileinpracticewedonotrequirethe‘∞normconstraint weneeditfortheanalysis.Forarelatedproblemofmatrixcompletion wherethelossL(θ)isquadratic eitherasimilarconditionon‘∞normisrequiredoradifferentconditiononincoherenceisrequired.3.1PerformanceguaranteeWeprovideanupperboundontheresultingerrorofourconvexrelaxation whenamulti-setofitemsSipresentedtouseriisdrawnuniformlyatrandomwithreplacement.Precisely foragivenk Si={ji 1 ... ji k}whereji ‘’sareindependentlydrawnuniformlyatrandomoverthed2items.Further ifanitemissampledmorethanonce i.e.ifthereexistsji ‘1=ji ‘2forsomeiand‘16=‘2 thenweassumethattheusertreatsthesetwoitemsasiftheyaretwodis-tinctitemswiththesameMNLweightsΘ∗i ji ‘1=Θ∗i ji ‘2.Theresultingpreferenceisthereforealwaysoverkitems(withpossiblymultiplecopiesofthesameitem) anddistributedaccordingto(1).Forexample ifk=3 itispossibletohaveSi={ji 1=1 ji 2=1 ji 3=2} inwhichcasetheresultingrankingcanbe(vi 1=ji 1 vi 2=ji 3 vi 3=ji 2)withprobability(eΘ∗i 1)/(2eΘ∗i 1+eΘ∗i 2)×(eΘ∗i 2)/(eΘ∗i 1+eΘ∗i 2).Suchsamplingwithreplacementisnecessaryfortheanalysis wherewerequireindependenceinthechoiceoftheitemsinSiinordertoapplythesymmetrizationtechnique(e.g.[23])toboundtheexpectationofthedeviation(cf.AppendixA.5).Similarsamplingassumptionshavebeenmadeinexistinganalysesonlearninglow-rankmodelsfromnoisyobservations e.g.[22].Letd≡(d1+d2)/2 andletσj(Θ∗)denotethej-thsingularvalueofthematrixΘ∗.Deﬁneλ0≡e2αsd1logd+d2(logd)2(log2d)4kd21d2.Theorem1.Underthedescribedsamplingmodel assume24≤k≤min{d21logd (d21+d22)/(2d1)logd (1/e)d2(4logd2+2logd1)} andλ∈[480λ0 c0λ0]withanyconstantc0=O(1)largerthan480.Then solvingtheoptimization(3)achieves1d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2F≤288√2e4αc0λ0√r(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F+288e4αc0λ0min{d1 d2}Xj=r+1σj(Θ∗) (8)foranyr∈{1 ... min{d1 d2}}withprobabilityatleast1−2d−3−d−32whered=(d1+d2)/2.4AproofisprovidedinAppendixA.Theaboveboundshowsanaturalsplittingoftheerrorintotwoterms onecorrespondingtotheestimationerrorfortherank-rcomponentandthesecondonecorrespondingtotheapproximationerrorforhowwellonecanapproximateΘ∗witharank-rmatrix.Thisboundholdsforallvaluesofrandonecouldpotentiallyoptimizeoverr.Weshowsuchresultsinthefollowingcorollaries.Corollary3.1(Exactlow-rankmatrices).SupposeΘ∗hasrankatmostr.UnderthehypothesesofTheorem1 solvingtheoptimization(3)withthechoiceoftheregularizationparameterλ∈[480λ0 c0λ0]achieveswithprobabilityatleast1−2d−3−d−32 1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F≤288√2e6αc0sr(d1logd+d2(logd)2(log2d)4)kd1.(9)Thenumberofentriesisd1d2andwerescaletheFrobeniusnormerrorappropriatelyby1/√d1d2.WhenΘ∗isarank-rmatrix thenthedegreesoffreedominrepresentingΘ∗isr(d1+d2)−r2=O(r(d1+d2)).Theabovetheoremshowsthatthetotalnumberofsamples whichis(kd1) needstoscaleasO(rd1(logd)+rd2(logd)2(log2d)4inordertoachieveanarbitrarilysmallerror.Thisisonlypoly-logarithmicfactorlargerthanthedegreesoffreedom.InSection3.2 weprovidealowerboundontheerrordirectly thatmatchestheupperbounduptoalogarithmicfactor.Thedependenceonthedynamicrangeα however issub-optimal.Itisexpectedthattheerrorincreaseswithα sincetheΘ∗scalesasα buttheexponentialdependenceintheboundseemstobeaweaknessoftheanalysis asseenfromnumericalexperimentsintherightpanelofFigure1.Althoughtheerrorincreasewithα numericalexperimentssuggeststhatitonlyincreasesatmostlinearly.However tighteningthescalingwithrespecttoαisachallengingproblem andsuchsub-optimaldependenceisalsopresentinexistingliteratureforlearningevensimplermodels suchastheBradley-Terrymodel[13]orthePlackett-Lucemodel[12] whicharespecialcasesoftheMNLmodelstudiedinthispaper.Apracticalissueinachievingtheaboverateisthechoiceofλ sincethedynamicrangeαisnotknowninadvance.Figure1illustratesthattheerrorisnotsensitivetothechoiceofλforawiderange.Anotherissueisthattheunderlyingmatrixmightnotbeexactlylowrank.Itismorerealistictoassumethatitisapproximatelylowrank.Following[22]weformalizethisnotionwith“‘q-ball”ofmatricesdeﬁnedasBq(ρq)≡{Θ∈Rd1×d2|Xj∈[min{d1 d2}]|σj(Θ∗)|q≤ρq}.(10)Whenq=0 thisisasetofrank-ρ0matrices.Forq∈(0 1] thisissetofmatriceswhosesingularvaluesdecayrelativelyfast.OptimizingthechoiceofrinTheorem1 wegetthefollowingresult.Corollary3.2(Approximatelylow-rankmatrices).SupposeΘ∗∈Bq(ρq)forsomeq∈(0 1]andρq>0.UnderthehypothesesofTheorem1 solvingtheoptimization(3)withthechoiceoftheregularizationparameterλ∈[480λ0 c0λ0]achieveswithprobabilityatleast1−2d−3 1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F≤2√ρq√d1d2288√2c0e6αsd1d2(d1logd+d2(logd)2(log2d)2)kd12−q2.(11)ThisisastrictgeneralizationofCorollary3.1.Forq=0andρ0=r thisrecoverstheexactlow-rankestimationbounduptoafactoroftwo.Forapproximatelow-rankmatricesinan‘q-ball weloseintheerrorexponent whichreducesfromoneto(2−q)/2.AproofofthisCorollaryisprovidedinAppendixB.TheleftpanelofFigure1conﬁrmsthescalingoftheerrorrateaspredictedbyCorollary3.1.Thelinesmergetoasinglelinewhenthesamplesizeisrescaledappropriately.Wemakeachoiceofλ=(1/2)p(logd)/(kd2) ThischoiceisindependentofαandissmallerthanproposedinTheorem1.Wegeneraterandomrank-rmatricesofdimensiond×d whereΘ∗=UVTwithU∈Rd×randV∈Rd×rentriesgeneratedi.i.dfromuniformdistributionover[0 1].Thenthe5 0.01 0.1 1 1000 10000r=3 d=50 0.01 0.1 1 1000 10000r=3 d=50r=6 d=50 0.01 0.1 1 1000 10000r=3 d=50r=6 d=50r=12 d=50 0.01 0.1 1 1000 10000r=3 d=50r=6 d=50r=12 d=50r=24 d=50RMSEsamplesizek 0.1 1 1 10 100 1000 10000 100000 RMSEλ√(logd)/(kd2)α=15α=10α=5Figure1:The(rescaled)RMSEscalesaspr(logd)/kasexpectedfromCorollary3.1forﬁxedd=50(left).Intheinset thesamedataisplottedversusrescaledsamplesizek/(rlogd).The(rescaled)RMSEisstableforabroadrangeofλandαforﬁxedd=50andr=3(right).row-meanissubtractedformeachrow andthenthewholematrixisscaledsuchthatthelargestentryisα=5.NotethatthisoperationdoesnotincreasetherankofthematrixΘ.Thisisbecausethisde-meaningcanbewrittenasΘ−Θ11T/d2andbothtermsintheoperationareofthesamecolumnspaceasΘwhichisofrankr.Therootmeansquarederror(RMSE)isplottedwhereRMSE=(1/d)|||Θ∗−bΘ|||F.Weimplementandsolvetheconvexoptimization(3)usingproximalgradientdescentmethodasanalyzedin[24].TherightpanelinFigure1illustratesthattheactualerrorisinsensitivetothechoiceofλforabroadrangeofλ∈[p(logd)/(kd2) 28p(logd)/(kd2)] afterwhichitincreaseswithλ.3.2Information-theoreticlowerboundforlow-rankmatricesForapolynomial-timealgorithmofconvexrelaxation wegaveintheprevioussectionaboundontheachievableerror.Wenextcomparethistothefundamentallimitofthisproblem bygivingalowerboundontheachievableerrorbyanyalgorithm(efﬁcientornot).Asimpleparametercountingargumentindicatesthatitrequiresthenumberofsamplestoscaleasthedegreesoffreedomi.e. kd1∝r(d1+d2) toestimatead1×d2dimensionalmatrixofrankr.Weconstructanappropriatepackingoverthesetoflow-rankmatriceswithboundedentriesinΩαdeﬁnedas(7) andshowthatnoalgorithmcanaccuratelyestimatethetruematrixwithhighprobabilityusingthegeneralizedFano’sinequality.Thisprovidesaconstructiveargumenttolowerboundtheminimaxerrorrate whichinturnestablishesthattheboundsinTheorem1issharpuptoalogarithmicfactor andprovesnootheralgorithmcansigniﬁcantlyimproveoverthenuclearnormminimization.Theorem2.SupposeΘ∗hasrankr.Underthedescribedsamplingmodel forlargeenoughd1andd2≥d1 thereisauniversalnumericalconstantc>0suchthatinfbΘsupΘ∗∈ΩαEh1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Fi≥cmin(αe−αrrd2kd1 αd2√d1d2logd) (12)wheretheinﬁmumistakenoverallmeasurablefunctionsovertheobservedrankedlists{(vi 1 ... vi k)}i∈[d1].AproofofthistheoremisprovidedinAppendixC.Thetermofprimaryinterestinthisboundistheﬁrstone whichshowsthescalingofthe(rescaled)minimaxrateaspr(d1+d2)/(kd1)(whend2≥d1) andmatchestheupperboundin(8).Itisthedominanttermintheboundwheneverthenumberofsamplesislargerthanthedegreesoffreedombyalogarithmicfactor i.e. kd1>r(d1+d2)logd ignoringthedependenceonα.Thisisatypicalregimeofinterest wherethesamplesizeiscomparabletothelatentdimensionoftheproblem.Inthisregime Theorem2establishesthattheupperboundinTheorem1isminimax-optimaluptoalogarithmicfactorinthedimensiond.64ChoicemodelingforbundledpurchasehistoryInthissection weusetheMNLmodeltostudyanotherscenarioofpracticalinterest:choicemodel-ingfrombundledpurchasehistory.Inthissetting weassumethatwehavebundledpurchasehistorydatafromnusers.Precisely therearetwocategoriesofinterestwithd1andd2alternativesineachcategoryrespectively.Forexample thereared1toothpastestochoosefromandd2toothbrushestochoosefrom.Forthei-thuser asubsetSi⊆[d1]ofalternativesfromtheﬁrstcategoryispresentedalongwithasubsetTi⊆[d2]ofalternativesfromthesecondcategory.Weusek1andk2todenotethenumberofalternativespresentedtoasingleuser i.e.k1=|Si|andk2=|Ti| andweassumethatthenumberofalternativespresentedtoeachuserisﬁxed tosimplifynotations.Giventhesesetsofalternatives eachusermakesa‘bundled’purchaseandweuse(ui vi)todenotethebundledpairofalternatives(e.g.atoothbrushandatoothpaste)purchasedbythei-thuser.Eachusermakesachoiceofthebestalternative independentofotherusers’schoices accordingtotheMNLmodelasP{(ui vi)=(j1 j2)}=eΘ∗j1 j2Pj01∈Si j02∈TieΘ∗j01 j02 (13)forallj1∈Siandj2∈Ti.Thedistribution(13)isindependentofshiftingallthevaluesofΘ∗byaconstant.Hence thereisanequivalentclassofΘ∗thatgivesthesamedistributionforthechoices:[Θ∗]≡{A∈Rd1×d2|A=Θ∗+c11Tforsomec∈R}.SincewecanonlyestimateΘ∗uptothisequivalentclass wesearchfortheonethatsumtozero i.e.Pj1∈[d1] j2∈[d2]Θ∗j1 j2=0.Letα=maxj1 j01∈[d1] j2 j02∈[d2]|Θ∗j1 j2−Θ∗j01 j02| denotethedynamicrangeoftheunderlyingΘ∗ suchthatwhenk1×k2alternativesarepresented wealwayshave1k1k2e−α≤P{(ui vi)=(j1 j2)}≤1k1k2eα (14)forall(j1 j2)∈Si×TiandforallSi⊆[d1]andTi⊆[d2]suchthat|Si|=k1and|Ti|=k2.Wedonotmakeanyassumptionsonαotherthanthatα=O(1)withrespecttod1andd2.AssumingΘ∗iswellapproximatebyalow-rankmatrix wesolvethefollowingconvexrelaxation giventheobservedbundledpurchasehistory{(ui vi Si Ti)}i∈[n]:bΘ∈argminΘ∈Ω0αL(Θ)+λ|||Θ|||nuc (15)wherethe(negative)loglikelihoodfunctionaccordingto(13)isL(Θ)=−1nnXi=1hhΘ euieTviii−logXj1∈Si j2∈Tiexp(cid:0)hhΘ ej1eTj2ii(cid:1) and(16)Ω0α≡nA∈Rd1×d2(cid:12)(cid:12)|||A|||∞≤α andXj1∈[d1] j2∈[d2]Aj1 j2=0o.(17)Comparedtocollaborativeranking (a)rowsandcolumnsofΘ∗correspondtoanalternativefromtheﬁrstandsecondcategory respectively;(b)eachsamplecorrespondstothepurchasechoiceofauserwhichfollowtheMNLmodelwithΘ∗;(c)eachpersonispresentedsubsetsSiandTiofitemsfromeachcategory;(d)eachsampleddatarepresentsthemostpreferredbundledpairofalternatives.4.1PerformanceguaranteeWeprovideanupperboundontheerrorachievedbyourconvexrelaxation whenthemulti-setofalternativesSifromtheﬁrstcategoryandTifromthesecondcategoryaredrawnuniformlyatrandomwithreplacementfrom[d1]and[d2]respectively.Precisely forgivenk1andk2 weletSi={j(i)1 1 ... j(i)1 k1}andTi={j(i)2 1 ... j(i)2 k2} wherej(i)1 ‘’sandj(i)2 ‘’sareindependentlydrawnuniformlyatrandomoverthed1andd2alternatives respectively.Similartotheprevioussection thissamplingwithreplacementisnecessaryfortheanalysis.Deﬁneλ1=se2αmax{d1 d2}logdnd1d2.(18)7Theorem3.Underthedescribedsamplingmodel assume16e2αmin{d1 d2}logd≤n≤min{d5 k1k2max{d21 d22}}logd andλ∈[8λ1 c1λ1]withanyconstantc1=O(1)largerthanmax{8 128/pmin{k1 k2}}.Then solvingtheoptimization(15)achieves1d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2F≤48√2e2αc1λ1√r(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F+48e2αc1λ1min{d1 d2}Xj=r+1σj(Θ∗) (19)foranyr∈{1 ... min{d1 d2}}withprobabilityatleast1−2d−3whered=(d1+d2)/2.AproofisprovidedinAppendixD.Optimizingoverrgivesthefollowingcorollaries.Corollary4.1(Exactlow-rankmatrices).SupposeΘ∗hasrankatmostr.UnderthehypothesesofTheorem3 solvingtheoptimization(15)withthechoiceoftheregularizationparameterλ∈[8λ1 c1λ1]achieveswithprobabilityatleast1−2d−3 1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F≤48√2e3αc1rr(d1+d2)logdn.(20)ThiscorollaryshowsthatthenumberofsamplesnneedstoscaleasO(r(d1+d2)logd)inordertoachieveanarbitrarilysmallerror.Thisisonlyalogarithmicfactorlargerthanthedegreesoffreedom.Weprovideafundamentallowerboundontheerror thatmatchestheupperbounduptoalogarithmicfactor.Forapproximatelylow-rankmatricesinan‘1-ballasdeﬁnedin(10) weshowanupperboundontheerror whoseerrorexponentreducesfromoneto(2−q)/2.Corollary4.2(Approximatelylow-rankmatrices).SupposeΘ∗∈Bq(ρq)forsomeq∈(0 1]andρq>0.UnderthehypothesesofTheorem3 solvingtheoptimization(15)withthechoiceoftheregularizationparameterλ∈[8λ1 c1λ1]achieveswithprobabilityatleast1−2d−3 1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F≤2√ρq√d1d2 48√2c1e3αrd1d2(d1+d2)logdn!2−q2.(21)SincetheproofisalmostidenticaltotheproofofCorollary3.2inAppendixB weomitit.Theorem4.SupposeΘ∗hasrankr.Underthedescribedsamplingmodel thereisauniversalconstantc>0suchthatthattheminimaxratewheretheinﬁmumistakenoverallmeasurablefunctionsovertheobservedpurchasehistory{(ui vi Si Ti)}i∈[n]islowerboundedbyinfbΘsupΘ∗∈ΩαEh1√d1d2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)bΘ−Θ∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Fi≥cmin(re−5αr(d1+d2)n α(d1+d2)√d1d2logd).(22)SeeAppendixE.1fortheproof.Theﬁrsttermisdominant andwhenthesamplesizeiscomparabletothelatentdimensionoftheproblem Theorem3isminimaxoptimaluptoalogarithmicfactor.5DiscussionWepresentedaconvexprogramtolearnMNLparametersfromordinaldata motivatedbytwosce-narios:recommendationsystemsandbundledpurchases.Wetaketheﬁrstprincipleapproachofidentifyingthefundamentallimitsandalsodevelopingefﬁcientalgorithmsmatchingthosefunda-mentaltradeoffs.Thereareseveralremainingchallenges.(a)Nuclearnormminimization whilepolynomial-time isstillslow.Wewantﬁrst-ordermethodsthatareefﬁcientwithprovableguaran-tees.Themainchallengeisprovidingagoodinitializationtostartsuchnon-convexapproaches.(b)Forsimplermodels suchasthePLmodel moregeneralsamplingoveragraphhasbeenstudied.Wewantanalyticalresultsformoregeneralsampling.(c)Thepracticaluseofthemodelandthealgorithmneedstobetestedonrealdatasetsonpurchasehistoryandrecommendations.AcknowledgmentsThisresearchissupportedinpartbyNSFCMMIawardMES-1450848andNSFSaTCawardCNS-1527754.8References[1]DanielMcFadden.Conditionallogitanalysisofqualitativechoicebehavior.1973.[2]LouisLThurstone.Alawofcomparativejudgment.Psychologicalreview 34(4):273 1927.[3]JacobMarschak.Binary-choiceconstraintsandrandomutilityindicators.InProceedingsofasymposiumonmathematicalmethodsinthesocialsciences volume7 pages19–38 1960.[4]D.R.Luce.IndividualChoiceBehavior.Wiley NewYork 1959.[5]YuLuandSahandNNegahban.Individualizedrankaggregationusingnuclearnormregularization.arXivpreprintarXiv:1410.0860 2014.[6]DohyungPark JoeNeeman JinZhang SujaySanghavi andInderjitSDhillon.Preferencecompletion:Large-scalecollaborativerankingfrompairwisecomparisons.2015.[7]IsobelClaireGormleyandThomasBrendanMurphy.Agradeofmembershipmodelforrankdata.BayesianAnalysis 4(2):265–295 2009.[8]Tie-YanLiu.Learningtorankforinformationretrieval.FoundationsandTrendsinInformationRetrieval 3(3):225–331 2009.[9]D.R.Hunter.Mmalgorithmsforgeneralizedbradley-terrymodels.AnnalsofStatistics pages384–406 2004.[10]JohnGuiverandEdwardSnelson.Bayesianinferenceforplackett-lucerankingmodels.Inproceedingsofthe26thannualinternationalconferenceonmachinelearning pages377–384.ACM 2009.[11]FrancoisCaronandArnaudDoucet.Efﬁcientbayesianinferenceforgeneralizedbradley–terrymodels.JournalofComputationalandGraphicalStatistics 21(1):174–196 2012.[12]B.Hajek S.Oh andJ.Xu.Minimax-optimalinferencefrompartialrankings.InAdvancesinNeuralInformationProcessingSystems pages1475–1483 2014.[13]S.Negahban S.Oh andD.Shah.Iterativerankingfrompair-wisecomparisons.InNIPS pages2483–2491 2012.[14]S.OhandD.Shah.Learningmixedmultinomiallogitmodelfromordinaldata.InAdvancesinNeuralInformationProcessingSystems pages595–603 2014.[15]W.Ding P.Ishwar andV.Saligrama.Atopicmodelingapproachtorankaggregation.BostonUniversityCenterforInfo.andSystemsEngg.TechnicalReporthttp://www.bu.edu/systems/publications 2014.[16]A.Ammar S.Oh D.Shah andL.Voloch.What’syourchoice?learningthemixedmulti-nomiallogitmodel.InProceedingsoftheACMSIGMETRICS/internationalconferenceonMeasurementandmodelingofcomputersystems 2014.[17]RuiWu JiamingXu RSrikant LaurentMassouli´e MarcLelarge andBruceHajek.Clusteringandinferencefrompairwisecomparisons.arXivpreprintarXiv:1502.04631 2015.[18]H.AzariSouﬁani H.Diao Z.Lai andD.C.Parkes.Generalizedrandomutilitymodelswithmultipletypes.InAdvancesinNeuralInformationProcessingSystems pages73–81 2013.[19]H.A.Souﬁani D.C.Parkes andL.Xia.Randomutilitytheoryforsocialchoice.InNIPS pages126–134 2012.[20]B.Recht M.Fazel andP.Parrilo.Guaranteedminimum-ranksolutionsoflinearmatrixequationsvianuclearnormminimization.SIAMreview 52(3):471–501 2010.[21]E.J.Cand`esandB.Recht.Exactmatrixcompletionviaconvexoptimization.FoundationsofComputa-tionalMathematics 9(6):717–772 2009.[22]S.NegahbanandM.J.Wainwright.Restrictedstrongconvexityand(weighted)matrixcompletion:Op-timalboundswithnoise.JournalofMachineLearningResearch 2012.[23]St´ephaneBoucheron G´aborLugosi andPascalMassart.Concentrationinequalities:Anonasymptotictheoryofindependence.OxfordUniversityPress 2013.[24]A.Agarwal S.Negahban andM.Wainwright.Fastglobalconvergenceratesofgradientmethodsforhigh-dimensionalstatisticalrecovery.InInNIPS pages37–45 2010.[25]J.Tropp.User-friendlytailboundsforsumsofrandommatrices.FoundationsofComput.Math. 2011.[26]S.VanDeGeer.EmpiricalProcessesinM-estimation volume6.Cambridgeuniversitypress 2000.[27]M.Ledoux.Theconcentrationofmeasurephenomenon.Number89.AmericanMathematicalSoc. 2005.9,David Pfau
Nicholas Bartlett
Frank Wood
Sewoong Oh
Kiran Thekumparampil
Shusen Wang
Fred Roosta
Peng Xu
Michael Mahoney
Wei-Da Chen
Shan-Hung (Brandon) Wu