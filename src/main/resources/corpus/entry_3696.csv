2013,Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic,The detection of anomalous activity in graphs is a statistical problem that arises in many applications  such as network surveillance  disease outbreak detection  and activity monitoring in social networks. Beyond its wide applicability  graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power. In this work  we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in Gaussian noise. Because this test is computationally infeasible  we provide a relaxation  called the Lov\'asz extended scan statistic (LESS) that uses submodularity to approximate the intractable generalized likelihood ratio. We demonstrate a connection between LESS and maximum a-posteriori inference in Markov random fields  which provides us with a poly-time algorithm for LESS. Using electrical network theory  we are able to control type 1 error for LESS and prove conditions under which LESS is risk consistent. Finally  we consider specific graph models  the torus  $k$-nearest neighbor graphs  and $\epsilon$-random graphs. We show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds.,Near-optimal Anomaly Detection in Graphs

using Lov´asz Extended Scan Statistic

James Sharpnack

Machine Learning Department

Carnegie Mellon University

Pittsburgh  PA 15213

jsharpna@gmail.com

Akshay Krishnamurthy

Computer Science Department

Carnegie Mellon University

Pittsburgh  PA 15213

akshaykr@cs.cmu.edu

Aarti Singh

Machine Learning Department

Carnegie Mellon University

Pittsburgh  PA 15213

aarti@cs.cmu.edu

Abstract

The detection of anomalous activity in graphs is a statistical problem that arises in
many applications  such as network surveillance  disease outbreak detection  and
activity monitoring in social networks. Beyond its wide applicability  graph struc-
tured anomaly detection serves as a case study in the difﬁculty of balancing com-
putational complexity with statistical power. In this work  we develop from ﬁrst
principles the generalized likelihood ratio test for determining if there is a well
connected region of activation over the vertices in the graph in Gaussian noise.
Because this test is computationally infeasible  we provide a relaxation  called the
Lov´asz extended scan statistic (LESS) that uses submodularity to approximate the
intractable generalized likelihood ratio. We demonstrate a connection between
LESS and maximum a-posteriori inference in Markov random ﬁelds  which pro-
vides us with a poly-time algorithm for LESS. Using electrical network theory 
we are able to control type 1 error for LESS and prove conditions under which
LESS is risk consistent. Finally  we consider speciﬁc graph models  the torus  k-
nearest neighbor graphs  and ǫ-random graphs. We show that on these graphs our
results provide near-optimal performance by matching our results to known lower
bounds.

1

Introduction

Detecting anomalous activity refers to determining if we are observing merely noise (business as
usual) or if there is some signal in the noise (anomalous activity). Classically  anomaly detection
focused on identifying rare behaviors and aberrant bursts in activity over a single data source or
channel. With the advent of large surveillance projects  social networks  and mobile computing 
data sources often are high-dimensional and have a network structure. With this in mind  statistics
needs to comprehensively address the detection of anomalous activity in graphs. In this paper  we
will study the detection of elevated activity in a graph with Gaussian noise.

In reality  very little is known about the detection of activity in graphs  despite a variety of real-world
applications such as activity detection in social networks  network surveillance  disease outbreak de-
tection  biomedical imaging  sensor network detection  gene network analysis  environmental moni-
toring and malware detection. Sensor networks might be deployed for detecting nuclear substances 
water contaminants  or activity in video surveillance. By exploiting the sensor network structure

1

(based on proximity)  one can detect activity in networks when the activity is very faint. Recent
theoretical contributions in the statistical literature[1  2] have detailed the inherent difﬁculty of such
a testing problem but have positive results only under restrictive conditions on the graph topology.
By combining knowledge from high-dimensional statistics  graph theory and mathematical pro-
gramming  the characterization of detection algorithms over any graph topology by their statistical
properties is possible.

Aside from the statistical challenges  the computational complexity of any proposed algorithms
must be addressed. Due to the combinatorial nature of graph based methods  problems can easily
shift from having polynomial-time algorithms to having running times exponential in the size of
the graph. The applications of graph structured inference require that any method be scalable to
large graphs. As we will see  the ideal statistical procedure will be intractable  suggesting that
approximation algorithms and relaxations are necessary.

1.1 Problem Setup
Consider a connected  possibly weighted  directed graph G deﬁned by a set of vertices V (|V | = p)
and directed edges E (|E| = m) which are ordered pairs of vertices. Furthermore  the edges may be
assigned weights  {We}e∈E  that determine the relative strength of the interactions of the adjacent
vertices. For each vertex  i ∈ V   we assume that there is an observation yi that has a Normal
distribution with mean xi and variance 1. This is called the graph-structured normal means problem 
and we observe one realization of the random vector

y = x + ξ 

(1)

where x ∈ Rp  ξ ∼ N (0  Ip×p). The signal x will reﬂect the assumption that there is an active
cluster (C ⊆ V ) in the graph  by making xi > 0 if i ∈ C and xi = 0 otherwise. Furthermore 
the allowable clusters  C  must have a small boundary in the graph. Speciﬁcally  we assume that
there are parameters ρ  µ (possibly dependent on p such that the class of graph-structured activation
patterns x is given as follows.

X =(x : x =

1C  C ∈ C)  

C = {C ⊆ V : out(C) ≤ ρ}

µ

p|C|

Here out(C) =P(u v)∈E Wu vI{u ∈ C  v ∈ ¯C} is the total weight of edges leaving the cluster C.

In other words  the set of activated vertices C have a small cut size in the graph G. While we assume
that the noise variance is 1 in (1)  this is equivalent to the more general model in which Eξ2
i = σ2
with σ known. If we wanted to consider known σ2 then we would apply all our algorithms to y/σ
and replace µ with µ/σ in all of our statements. For this reason  we call µ the signal-to-noise ratio
(SNR)  and proceed with σ = 1.

In graph-structured activation detection we are concerned with statistically testing the null against
the alternative hypotheses 

H0 : y ∼ N (0  I)
H1 : y ∼ N (x  I)  x ∈ X

(2)

H0 represents business as usual (such as sensors returning only noise) while H1 encompasses all of
the foreseeable anomalous activity (an elevated group of noisy sensor observations). Let a test be a
mapping T (y) ∈ {0  1}  where 1 indicates that we reject the null. It is imperative that we control
both the probability of false alarm  and the false acceptance of the null. To this end  we deﬁne our
measure of risk to be

R(T ) = E0[T ] + sup
x∈X

Ex[1 − T ]

where Ex denote the expectation with respect to y ∼ N (x  I). These terms are also known as the
probability of type 1 and type 2 error respectively. This setting should not be confused with the
Bayesian testing setup (e.g. as considered in [2  3]) where the patterns  x  are drawn at random.
We will say that H0 and H1 are asymptotically distinguished by a test  T   if in the setting of large
graphs  limp→∞ R(T ) = 0. If such a test exists then H0 and H1 are asymptotically distinguishable 
otherwise they are asymptotically indistinguishable (which occurs whenever the risk does not tend
to 0). We will be characterizing regimes for µ in which our test asymptotically distinguishes H0
from H1.

2

Throughout the study  let the edge-incidence matrix of G be ∇ ∈ Rm×p such that for e = (v  w) ∈
E  ∇e v = −We  ∇e w = We and is 0 elsewhere. For directed graphs  vertex degrees refer to dv =
out({v}). Let k.k denote the ℓ2 norm  k.k1 be the ℓ1 norm  and (x)+ be the positive components
of the vector x. Let [p] = {1  . . .   p}  and we will be using the o notation  namely if non-negative
sequences satisfy an/bn → 0 then an = o(bn) and bn = ω(an).
1.2 Contributions

Section 3 highlights what is known about the hypothesis testing problem 2  particularly we provide
a regime for µ in which H0 and H1 are asymptotically indistinguishable. In section 4.1  we derive
the graph scan statistic from the generalized likelihood ratio principle which we show to be a com-
putationally intractable procedure. In section 4.2  we provide a relaxation of the graph scan statistic
(GSS)  the Lov´asz extended scan statistic (LESS)  and we show that it can be computed with suc-
cessive minimum s − t cut programs (a graph cut that separates a source vertex from a sink vertex).
In section 5  we give our main result  Theorem 5  that provides a type 1 error control for both test
statistics  relating their performance to electrical network theory. In section 6  we show that GSS
and LESS can asymptotically distinguish H0 and H1 in signal-to-noise ratios close to the lowest
possible for some important graph models. All proofs are in the Appendix.

2 Related Work

Graph structured signal processing. There have been several approaches to signal processing over
graphs. Markov random ﬁelds (MRF) provide a succinct framework in which the underlying signal
is modeled as a draw from an Ising or Potts model [4  5]. We will return to MRFs in a later section 
as it will relate to our scan statistic. A similar line of research is the use of kernels over graphs. The
study of kernels over graphs began with the development of diffusion kernels [6]  and was extended
through Green’s functions on graphs [7]. While these methods are used to estimate binary signals
(where xi ∈ {0  1}) over graphs  little is known about their statistical properties and their use in
signal detection. To the best of our knowledge  this paper is the ﬁrst connection made between
anomaly detection and MRFs.

Normal means testing. Normal means testing in high-dimensions is a well established and funda-
mental problem in statistics. Much is known when H1 derives from a smooth function space such as
Besov spaces or Sobolev spaces[8  9]. Only recently have combinatorial structures such as graphs
been proposed as the underlying structure of H1. A signiﬁcant portion of the recent work in this area
[10  3  1  2] has focused on incorporating structural assumptions on the signal  as a way to mitigate
the effect of high-dimensionality and also because many real-life problems can be represented as
instances of the normal means problem with graph-structured signals (see  for an example  [11]).

Graph scan statistics. In spatial statistics  it is common  when searching for anomalous activity
to scan over regions in the spatial domain  testing for elevated activity[12  13]. There have been
scan statistics proposed for graphs  most notably the work of [14] in which the authors scan over
neighborhoods of the graphs deﬁned by the graph distance. Other work has been done on the theory
and algorithms for scan statistics over speciﬁc graph models  but are not easily generalizable to
arbitrary graphs [15  1]. More recently  it has been found that scanning over all well connected
regions of a graph can be computationally intractable  and so approximations to the intractable
likelihood-based procedure have been studied [16  17]. We follow in this line of work  with a
relaxation to the intractable generalized likelihood ratio test.

3 A Lower Bound and Known Results

In this section we highlight the previously known results about the hypothesis testing problem (2).
This problem was studied in [17]  in which the authors demonstrated the following lower bound 
which derives from techniques developed in [3].

Theorem 1. [17] Hypotheses H0 and H1 deﬁned in Eq. (2) are asymptotically indistinguishable if

µ = o smin(cid:26) ρ

dmax

where dmax is the maximum degree of graph G.

log(cid:18) pd2

ρ2 (cid:19)  √p(cid:27)!

max

3

Now that a regime of asymptotic indistinguishability has been established  it is instructive to consider
test statistics that do not take the graph into account (viz. the statistics are unaffected by a change
in the graph structure). Certainly  if we are in a situation where a naive procedure perform near-
optimally  then our study is not warranted. As it turns out  there is a gap between the performance
of the natural unstructured tests and the lower bound in Theorem 1.
Proposition 2. [17] (1) The thresholding test statistic  maxv∈[p] |yv|  asymptotically distinguishes
H0 from H1 if µ = ω(|C| log(p/|C|)).
(2) The sum test statistic Pv∈[p] yv  asymptotically distinguishes H0 from H1 if µ = ω(p/|C|).
As opposed to these naive tests one can scan over all clusters in C performing individual likelihood
ratio tests. This is called the scan statistic  and it is known to be a computationally intractable
combinatorial optimization. Previously  two alternatives to the scan statistic have been developed:
the spectral scan statistic [16]  and one based on the uniform spanning tree wavelet basis [17]. The
former is indeed a relaxation of the ideal  computationally intractable  scan statistic  but in many
important graph topologies  such as the lattice  provides sub-optimal statistical performance. The
uniform spanning tree wavelets in effect allows one to scan over a subclass of the class  C  but tends
to provide worse performance (as we will see in section 6) than that presented in this work. The
theoretical results in [17] are similar to ours  but they suffer additional log-factors.

4 Method

As we have noted the fundamental difﬁculty of the hypothesis testing problem is the composite
nature of the alternative hypothesis. Because the alternative is indexed by sets  C ∈ C(ρ)  with a
low cut size  it is reasonable that the test statistic that we will derive results from a combinatorial
optimization program. In fact  we will show we can express the generalized likelihood ratio (GLR)
statistic in terms of a modular program with submodular constraints. This will turn out to be a
possibly NP-hard program  as a special case of such programs is the well known knapsack problem
[18]. With this in mind  we provide a convex relaxation  using the Lov´asz extension  to the ideal
GLR statistic. This relaxation conveniently has a dual objective that can be evaluated with a binary
Markov random ﬁeld energy minimization  which is a well understood program. We will reserve
the theoretical statistical analysis for the following section.

Submodularity. Before we proceed  we will introduce the reader to submodularity and the Lov´asz
extension. (A very nice introduction to submodularity can be found in [19].) For any set  which we

may as well take to be the vertex set [p]  we say that a function F : {0  1}p → R is submodular
if for any A  B ⊆ [p]  F (A) + F (B) ≥ F (A ∩ B) + F (A ∪ B). (We will interchangeably use
the bijection between 2[p] and {0  1}p deﬁned by C → 1C .) In this way  a submodular function
experiences diminishing returns  as additions to large sets tend to be less dramatic than additions to
small sets. But while this diminishing returns phenomenon is akin to concave functions  for opti-
mization purposes submodularity acts like convexity  as it admits efﬁcient minimization procedures.

Moreover  for every submodular function there is a Lov´asz extension f : [0  1]p → R deﬁned in the
following way: for x ∈ [0  1]p let xji denote the ith largest element of x  then

Submodular functions as a class is similar to convex functions in that it is closed under addition and
non-negative scalar multiplication. The following facts about Lov´asz extensions will be important.
Proposition 3. [19] Let F be submodular and f be its Lov´asz extension. Then f is convex  f (x) =
F (x) if x ∈ {0  1}p  and

min{F (x) : x ∈ {0  1}p} = min{f (x) : x ∈ [0  1]p}

We are now sufﬁciently prepared to develop the test statistics that will be the focus of this paper.

4.1 Graph Scan Statistic

It is instructive  when faced with a class of probability distributions  indexed by subsets C ⊆ 2[p] 
to think about what techniques we would use if we knew the correct set C ∈ C (which is often
called oracle information). One would in this case be only testing the null hypothesis H0 : x = 0

4

f (x) = xj1F ({j1}) +

(F ({j1  . . .   ji}) − F ({j1  . . .   ji−1}))xji

p

Xi=2

against the simple alternative H1 : x ∝ 1C . In this situation  we would employ the likelihood
ratio test because by the Neyman-Pearson lemma it is the uniformly most powerful test statistic.
The maximum likelihood estimator for x is 1C 1⊤C y/|C| (the MLE of µ is 1⊤C y/p|C|) and the

likelihood ratio turns out to be

exp(cid:26)−

1

2kyk2(cid:27) / exp(−

1

2(cid:13)(cid:13)(cid:13)(cid:13)

1C 1⊤C y

|C| − y(cid:13)(cid:13)(cid:13)(cid:13)

2) = exp(cid:26) (1⊤C y)2
2|C| (cid:27)

1⊤C y

p|C|

Hence  the log-likelihood ratio is proportional to (1⊤C y)2/|C| and thresholding this at z2
us a size α test.

1−α/2 gives

This reasoning has been subject to the assumption that we had oracle knowledge of C. A
natural statistic  when C is unknown  is the generalized log-likelihood ratio (GLR) deﬁned by
max(1⊤C y)2/|C| s.t. C ∈ C. We will work with the graph scan statistic (GSS) 

ˆs = max

s.t. C ∈ C(ρ) = {C : out(C) ≤ ρ}

(3)

which is nearly equivalent to the GLR. (We can in fact evaluate ˆs for y and −y  taking a maximum
and obtain the GLR  but statistically this is nearly the same.) Notice that there is no guarantee that
the program above is computationally feasible. In fact  it belongs to a class of programs  speciﬁcally
modular programs with submodular constraints that is known to contain NP-hard instantiations 
such as the ratio cut program and the knapsack program [18]. Hence  we are compelled to form a
relaxation of the above program  that will with luck provide a feasible algorithm.

4.2 Lov´asz Extended Scan Statistic

It is common  when faced with combinatorial optimization programs that are computationally in-

Generally  the hope is that optimizing the relaxation will approximate the combinatorial program

feasible  to relax the domain from the discrete {0  1}p to a continuous domain  such as [0  1]p.
well. First we require that we can relax the constraint out(C) ≤ ρ to the hypercube [0  1]p. This
will be accomplished by replacing it with its Lov´asz extension k(∇x)+k1 ≤ ρ. We then form the
relaxed program  which we will call the Lov´asz extended scan statistic (LESS) 

ˆl = max
t∈[p]

max

x

x⊤y
√t

s.t. x ∈ X (ρ  t) = {x ∈ [0  1]p : k(∇x)+k1 ≤ ρ  1⊤x ≤ t}

(4)

We will ﬁnd that not only can this be solved with a convex program  but the dual objective is a
minimum binary Markov random ﬁeld energy program. To this end  we will brieﬂy go over binary
Markov random ﬁelds  which we will ﬁnd can be used to solve our relaxation.

Binary Markov Random Fields. Much of the previous work on graph structured statistical proce-
dures assumes a Markov random ﬁeld (MRF) model  in which there are discrete labels assigned to
each vertex in [p]  and the observed variables {yv}v∈[p] are conditionally independent given these
labels. Furthermore  the prior distribution on the labels is drawn according to an Ising model (if
the labels are binary) or a Potts model otherwise. The task is to then compute a Bayes rule from
the posterior of the MRF. The majority of the previous work assumes that we are interested in the
maximum a-posteriori (MAP) estimator  which is the Bayes rule for the 0/1-loss. This can generally
be written in the form 

min

x∈{0 1}p Xv∈[p]

−lv(xv|yv) + Xv6=u∈[p]

Wv uI{xv 6= xu}

where lv is a data dependent log-likelihood. Such programs are called graph-representable in [20] 
and are known to be solvable in the binary case with s-t graph cuts. Thus  by the min-cut max-ﬂow
theorem the value of the MAP objective can be obtained by computing a maximum ﬂow. More
recently  a dual-decomposition algorithm has been developed in order to parallelize the computation
of the MAP estimator for binary MRFs [21  22].

We are now ready to state our result regarding the dual form of the LESS program  (4).
Proposition 4. Let η0  η1 ≥ 0  and deﬁne the dual function of the LESS 
y⊤x − η01⊤x − η1k∇xk0

g(η0  η1) = max

x∈{0 1}p

5

The LESS estimator is equal to the following minimum of convex optimizations

ˆl = max
t∈[p]

1
√t

min

η0 η1≥0

g(η0  η1) + η0t + η1ρ

g(η0  η1) is the objective of a MRF MAP problem  which is poly-time solvable with s-t graph cuts.

5 Theoretical Analysis

So far we have developed a lower bound to the hypothesis testing problem  shown that some com-
mon detectors do not meet this guarantee  and developed the Lov´asz extended scan statistic from
ﬁrst principles. We will now provide a thorough statistical analysis of the performance of LESS.
Previously  electrical network theory  speciﬁcally the effective resistances of edges in the graph 
has been useful in describing the theoretical performance of a detector derived from uniform span-
ning tree wavelets [17]. As it turns out the performance of LESS is also dictated by the effective
resistances of edges in the graph.

Effective Resistance. Effective resistances have been extensively studied in electrical network the-
ory [23]. We deﬁne the combinatorial Laplacian of G to be ∆ = D − W (Dv v = out({v}) is the
diagonal degree matrix). A potential difference is any z ∈ R|E| such that it satisﬁes Kirchoff ’s poten-
tial law: the total potential difference around any cycle is 0. Algebraically  this means that ∃x ∈ Rp
such that ∇x = z. The Dirichlet principle states that any solution to the following program gives
an absolute potential x that satisﬁes Kirchoff’s law:

minxx⊤∆x s.t. xS = vS

for source/sinks S ⊂ [p] and some voltage constraints vS ∈ R|S|. By Lagrangian calculus  the
solution to the above program is given by x = ∆†v where v is 0 over SC and vS over S  and †
indicates the Moore-Penrose pseudoinverse. The effective resistance between a source v ∈ V and
a sink w ∈ V is the potential difference required to create a unit ﬂow between them. Hence  the
effective resistance between v and w is rv w = (δv − δw)⊤∆†(δv − δw)  where δv is the Dirac delta
function. There is a close connection between effective resistances and random spanning trees. The
uniform spanning tree (UST) is a random spanning tree  chosen uniformly at random from the set of
all distinct spanning trees. The foundational Matrix-Tree theorem [24  23] states that the probability
of an edge  e  being included in the UST is equal to the edge weight times the effective resistance
Were. The UST is an essential component of the proof of our main theorem  in that it provides a
mechanism for unravelling the graph while still preserving the connectivity of the graph.

We are now in a position to state the main theorem  which will allow us to control the type 1 error
(the probability of false alarm) of both the GSS and its relaxation the LESS.

Theorem 5. Let rC = max{P(u v)∈E:u∈C Wu vr(u v) : C ∈ C} be the maximum effective re-

sistance of the boundary of a cluster C. The following statements hold under the null hypothesis
H0 : x = 0:

1. The graph scan statistic  with probability at least 1 − α  is smaller than

ˆs ≤ √rC +r 1

2

log p!p2 log(p − 1) +p2 log 2 +p2 log(1/α)

2. The Lov´asz extended scan statistic  with probability at least 1 − α is smaller than

ˆl ≤

log(2p) + 1

r(cid:16)√rC +q 1

2 log p(cid:17)

2

log p

+ 2vuut √rC +r 1

2

log p!2

log p

+p2 log p +p2 log(1/α)

(5)

(6)

The implication of Theorem 5 is that the size of the test may be controlled at level α by selecting
thresholds given by (5) and (6) for GSS and LESS respectively. Notice that the control provided
for the LESS is not signiﬁcantly different from that of the GSS. This is highlighted by the following
Corollary  which combines Theorem 5 with a type 2 error bound to produce an information theoretic
guarantee for the asymptotic performance of the GSS and LESS.

6

Corollary 6. Both the GSS and the LESS asymptotically distinguish H0 from H1 if

µ
σ

= ω(cid:16)max{prC log p  log p}(cid:17)

To summarize we have established that the performance of the GSS and the LESS are dictated by
the effective resistances of cuts in the graph. While the condition in Cor. 6 may seem mysterious 
the guarantee in fact nearly matches the lower bound for many graph models as we now show.

6 Speciﬁc Graph Models

Theorem 5 shows that the effective resistance of the boundary plays a critical role in characterizing
the distinguishability region of both the the GSS and LESS. On speciﬁc graph families  we can
compute the effective resistances precisely  leading to concrete detection guarantees that we will see
nearly matches the lower bound in many cases. Throughout this section  we will only be working
with undirected  unweighted graphs.

Recall that Corollary 6 shows that an SNR of ω(cid:0)√rC log p(cid:1) is sufﬁcient while Theorem 1 shows
that Ω(cid:16)pρ/dmax log p(cid:17) is necessary for detection. Thus if we can show that rC ≈ ρ/dmax  we

would establish the near-optimality of both the GSS and LESS. Foster’s theorem lends evidence to
the fact that the effective resistances should be much smaller than the cut size:

Theorem 7. (Foster’s Theorem [25  26])

re = p − 1

Xe∈E

Roughly speaking  the effective resistance of an edge selected uniformly at random is ≈ (p−1)/m =
d−1
ave so the effective resistance of a cut is ≈ ρ/dave. This intuition can be formalized for speciﬁc
models and this improvement by the average degree bring us much closer to the lower bound.

6.1 Edge Transitive Graphs

An edge transitive graph  G  is one for which there is a graph automorphism mapping e0 to e1 for any
pair of edges e0  e1. Examples include the l-dimensional torus  the cycle  and the complete graph
Kp. The existence of these automorphisms implies that every edge has the same effective resistance 
and by Foster’s theorem  we know that these resistances are exactly (p − 1)/m. Moreover  since
edge transitive graphs must be d-regular  we know that m = Θ(pd) so that re = Θ(1/d). Thus as
a corollary to Theorem 5 we have that both the GSS and LESS are near-optimal (optimal modulo
logarithmic factors whenever ρ/d ≤ √p) on edge transitive graphs:

Corollary 8. Let G be an edge-transitive graph with common degree d. Then both the GSS and
LESS distinguish H0 from H1 provided that:

6.2 Random Geometric Graphs

µ = ω(cid:16)max{pρ/d log p  log p}(cid:17)

Another popular family of graphs are those constructed from a set of points in RD drawn according
to some density. These graphs have inherent randomness stemming from sampling of the density 
and thus earn the name random geometric graphs. The two most popular such graphs are symmetric
k-nearest neighbor graphs and ǫ-graphs. We characterize the distinguishability region for both.
In both cases  a set of points z1  . . .   zp are drawn i.i.d. from a density f support over RD  or a subset
of RD. Our results require mild regularity conditions on f   which  roughly speaking  require that
supp(f ) is topologically equivalent to the cube and has density bounded away from zero (See [27]
for a precise deﬁnition). To form a k-nearest neighbor graph Gk  we associate each vertex i with a
point zi and we connect vertices i  j if zi is amongst the k-nearest neighbors  in ℓ2  of zj or vice
versa. In the the ǫ-graph  Gǫ we connect vertices i  j if ||zi  zj|| ≤ ǫ for some metric τ .
The relationship re ≈ 1/d  which we used for edge-transitive graphs  was derived in Corollaries 8
and 9 in [27] The precise concentration arguments  which have been done before [17]  lead to the
following corollary regarding the performance of the GSS and LESS on random geometric graphs:

7

Figure 1: A comparison of detection procedures: spectral scan statistic (SSS)  UST wavelet detector

(Wavelet)  and LESS. The graphs used are the square 2D Torus  kNN graph (k ≈ p1/4)  and ǫ-graph
(with ǫ ≈ p−1/3); with µ = 4  4  3 respectively  p = 225  and |C| ≈ p1/2.
Corollary 9. Let Gk be a k-NN graph with k/p → 0  k(k/p)2/D → ∞ and suppose the density
f meets the regularity conditions in [27]. Then both the GSS and LESS distinguish H0 from H1
provided that:

If Gǫ is an ǫ-graph with ǫ → 0  nǫD+2 → ∞ then both distinguish H0 from H1 provided that:

µ = ω(cid:16)max{pρ/k log p  log p}(cid:17)
µ = ω(cid:18)max(cid:26)r ρ
pǫD log p  log p(cid:27)(cid:19)

The corollary follows immediately form Corollary 6 and the proofs in [17]. Since under the regu-
larity conditions  the maximum degree is Θ(k) and Θ(pǫD) in k-NN and ǫ-graphs respectively  the

corollary establishes the near optimality (again provided that ρ/d ≤ √p) of both test statistics.

We performed some experiments using the MRF based algorithm outlined in Prop. 4. Each exper-
iment is made with graphs with 225 vertices  and we report the true positive rate versus the false
positive rate as the threshold varies (also known as the ROC.) For each graph model  LESS provides
gains over the spectral scan statistic[16] and the UST wavelet detector[17]  each of the gains are
signiﬁcant except for the ǫ-graph which is more modest.

7 Conclusions

To summarize  while Corollary 6 characterizes the performance of GSS and LESS in terms of ef-
fective resistances  in many speciﬁc graph models  this can be translated into near-optimal detection
guarantees for these test statistics. We have demonstrated that the LESS provides guarantees similar
to that of the computationally intractable generalized likelihood ratio test (GSS). Furthermore  the
LESS can be solved through successive graph cuts by relating it to MAP estimation in an MRF.
Future work includes using these concepts for localizing the activation  making the program robust
to missing data  and extending the analysis to non-Gaussian error.

Acknowledgments

This research is supported in part by AFOSR under grant FA9550-10-1-0382 and NSF under grant
IIS-1116458. AK is supported in part by a NSF Graduate Research Fellowship. We would like to
thank Sivaraman Balakrishnan for his valuable input in the theoretical development of the paper.

References

[1] E. Arias-Castro  E.J. Candes  and A. Durand. Detection of an anomalous cluster in a network. The Annals

of Statistics  39(1):278–304  2011.

[2] L. Addario-Berry  N. Broutin  L. Devroye  and G. Lugosi. On combinatorial testing problems. The Annals

of Statistics  38(5):3063–3092  2010.

[3] E. Arias-Castro  E.J. Candes  H. Helgason  and O. Zeitouni. Searching for a trail of evidence in a maze.

The Annals of Statistics  36(4):1726–1757  2008.

[4] V. Cevher  C. Hegde  M.F. Duarte  and R.G. Baraniuk. Sparse signal recovery using markov random

ﬁelds. Technical report  DTIC Document  2009.

8

[5] P. Ravikumar and J.D. Lafferty. Quadratic programming relaxations for metric labeling and markov

random ﬁeld map estimation. 2006.

[6] R.I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In Proceedings

of the Nineteenth International Conference on Machine Learning  pages 315–322. Citeseer  2002.

[7] A. Smola and R. Kondor. Kernels and regularization on graphs. Learning theory and kernel machines 

pages 144–158  2003.

[8] Y.I. Ingster. Minimax testing of nonparametric hypotheses on a distribution density in the l

p metrics.

Theory of Probability and its Applications  31:333  1987.

[9] Y.I. Ingster and I.A. Suslina. Nonparametric goodness-of-ﬁt testing under Gaussian models  volume 169.

Springer Verlag  2003.

[10] E. Arias-Castro  D. Donoho  and X. Huo. Near-optimal detection of geometric objects by fast multiscale

methods. IEEE Trans. Inform. Theory  51(7):2402–2425  2005.

[11] L. Jacob  P. Neuvial  and S. Dudoit. Gains in power from structured two-sample tests of means on graphs.

Arxiv preprint arXiv:1009.5173  2010.

[12] Daniel B Neill and Andrew W Moore. Rapid detection of signiﬁcant spatial clusters. In Proceedings
of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining  pages
256–265. ACM  2004.

[13] Deepak Agarwal  Andrew McGregor  Jeff M Phillips  Suresh Venkatasubramanian  and Zhengyuan Zhu.
Spatial scan statistics: approximations and performance study. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and data mining  pages 24–33. ACM  2006.

[14] Carey E Priebe  John M Conroy  David J Marchette  and Youngser Park. Scan statistics on enron graphs.

Computational & Mathematical Organization Theory  11(3):229–247  2005.

[15] Chih-Wei Yi. A uniﬁed analytic framework based on minimum scan statistics for wireless ad hoc and

sensor networks. Parallel and Distributed Systems  IEEE Transactions on  20(9):1233–1245  2009.

[16] J. Sharpnack  A. Rinaldo  and A. Singh. Changepoint detection over graphs with the spectral scan statistic.

Arxiv preprint arXiv:1206.0773  2012.

[17] James Sharpnack  Akshay Krishnamurthy  and Aarti Singh. Detecting activations over graphs using

spanning tree wavelet bases. arXiv preprint arXiv:1206.0937  2012.

[18] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity.

Courier Dover Publications  1998.

[19] Francis Bach. Convex analysis and optimization with submodular functions: a tutorial. arXiv preprint

arXiv:1010.4207  2010.

[20] Vladimir Kolmogorov and Ramin Zabin. What energy functions can be minimized via graph cuts? Pattern

Analysis and Machine Intelligence  IEEE Transactions on  26(2):147–159  2004.

[21] Petter Strandmark and Fredrik Kahl. Parallel and distributed graph cuts by dual decomposition.

In
Computer Vision and Pattern Recognition (CVPR)  2010 IEEE Conference on  pages 2085–2092. IEEE 
2010.

[22] David Sontag  Amir Globerson  and Tommi Jaakkola. Introduction to dual decomposition for inference.

Optimization for Machine Learning  1  2011.

[23] R. Lyons and Y. Peres. Probability on trees and networks. Book in preparation.  2000.

[24] G. Kirchhoff. Ueber die auﬂ¨osung der gleichungen  auf welche man bei der untersuchung der linearen

vertheilung galvanischer str¨ome gef¨uhrt wird. Annalen der Physik  148(12):497–508  1847.

[25] R.M. Foster. The average impedance of an electrical network. Contributions to Applied Mechanics

(Reissner Anniversary Volume)  pages 333–340  1949.

[26] P. Tetali. Random walks and the effective resistance of networks. Journal of Theoretical Probability 

4(1):101–109  1991.

[27] Ulrike Von Luxburg  Agnes Radl  and Matthias Hein. Hitting and commute times in large graphs are

often misleading. ReCALL  2010.

[28] R Tyrell Rockafellar. Convex analysis  volume 28. Princeton university press  1997.

[29] Thomas H Cormen  Charles E Leiserson  Ronald L Rivest  and Clifford Stein. Introduction to algorithms.

MIT press  2001.

[30] Wai Shing Fung and Nicholas JA Harvey. Graph sparsiﬁcation by edge-connectivity and random spanning

trees. arXiv preprint arXiv:1005.0265  2010.

[31] Michel Ledoux. The concentration of measure phenomenon  volume 89. American Mathematical Soc. 

2001.

9

,James Sharpnack
Akshay Krishnamurthy
Aarti Singh
Saizheng Zhang
Yuhuai Wu
Tong Che
Zhouhan Lin
Roland Memisevic
Russ Salakhutdinov
Yoshua Bengio