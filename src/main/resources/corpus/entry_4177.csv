2019,Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering,We study high-dimensional sparse estimation tasks in a robust setting where a constant fraction
of the dataset is adversarially corrupted. Specifically  we focus on the fundamental problems of robust
sparse mean estimation and robust sparse PCA.
We give the first practically viable robust estimators for these problems. 
In more detail  our algorithms are sample and computationally efficient 
and achieve near-optimal robustness guarantees. 
In contrast to prior provable algorithms which relied on the ellipsoid method  
our algorithms use spectral techniques to iteratively remove outliers from the dataset. 
Our experimental evaluation on synthetic data shows that our algorithms are scalable and 
significantly outperform a range of previous approaches  nearly matching the best error rate without corruptions.,Outlier-Robust High-Dimensional Sparse Estimation

via Iterative Filtering

Ilias Diakonikolas

University of Wisconsin - Madison
ilias.diakonikolas@gmail.com

Sushrut Karmalkar

UT Austin

s.sushrut@gmail.com

Daniel Kane

University of California  San Diego

Eric Price
UT Austin

Alistair Stewart
Web3 Foundation

dakane@ucsd.edu

ecprice@cs.utexas.edu

stewart.al@gmail.com

Abstract

We study high-dimensional sparse estimation tasks in a robust setting where a
constant fraction of the dataset is adversarially corrupted. Speciﬁcally  we focus
on the fundamental problems of robust sparse mean estimation and robust sparse
PCA. We give the ﬁrst practically viable robust estimators for these problems. In
more detail  our algorithms are sample and computationally efﬁcient and achieve
near-optimal robustness guarantees. In contrast to prior provable algorithms which
relied on the ellipsoid method  our algorithms use spectral techniques to iteratively
remove outliers from the dataset. Our experimental evaluation on synthetic data
shows that our algorithms are scalable and signiﬁcantly outperform a range of
previous approaches  nearly matching the best error rate without corruptions.

1

Introduction

1.1 Background

The task of leveraging sparsity to extract meaningful information from high-dimensional datasets
is a fundamental problem of signiﬁcant practical importance  motivated by a range of data analysis
applications. Various formalizations of this general problem have been investigated in statistics and
machine learning for at least the past two decades  see  e.g.  [HTW15] for a recent textbook on the
topic. This paper focuses on the unsupervised setting and in particular on estimating the parameters
of a high-dimensional distribution under sparsity assumptions. Concretely  we study the problems
of sparse mean estimation and sparse PCA under natural data generating models.
The classical setup in statistics is that the data was generated by a probabilistic model of a given
type. This is a simplifying assumption that is only approximately valid  as real datasets are typically
exposed to some source of contamination. The ﬁeld of robust statistics [Hub64  HR09  HRRS86]
aims to design estimators that are robust in the presence of model misspeciﬁcation. In recent years 
designing computationally efﬁcient robust estimators for high-dimensional settings has become a
pressing challenge in a number of applications. These include the analysis of biological datasets 
where natural outliers are common [RPW+02  PLJD10  LAT+08] and can contaminate the down-
stream statistical analysis  and data poisoning attacks [BNJT10]  where even a small fraction of fake
data (outliers) can substantially degrade the learned model [BNL12  SKL17].
This discussion motivates the design of robust estimators that can tolerate a constant fraction of
adversarially corrupted data. We will use the following model of corruptions (see  e.g.  [DKK+16]):

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Deﬁnition 1.1. Given 0 < ε < 1/2 and a family of distributions D on Rd  the adversary operates as
follows: The algorithm speciﬁes some number of samples N  and N samples X1  X2  . . .   XN are
drawn from some (unknown) D ∈ D. The adversary is allowed to inspect the samples  removes εN
of them  and replaces them with arbitrary points. This set of N points is then given to the algorithm.
We say that a set of samples is ε-corrupted if it is generated by the above process.

is to output a hypothesis vector(cid:98)µ that approximates µ in (cid:96)2-norm. In the context of robust sparse

Our model of corruptions generalizes several other robustness models  including Huber’s contami-
nation model [Hub64] and the malicious PAC model [Val85  KL93].
In the context of robust sparse mean estimation  we are given an ε-corrupted set of samples from an
unknown mean Gaussian distribution N (µ  I)  where µ ∈ Rd is assumed to be k-sparse  and the goal
PCA (in the spiked covariance model)  we are given an ε-corrupted set of samples from N (0  ρvvT ) 
where v ∈ Rd is assumed to be k-sparse and the goal is to approximate v. In both settings  we would
like to design computationally efﬁcient estimators with sample complexity poly(k  log d  1/ε)  i.e. 
close to the information theoretic minimum  that achieve near-optimal error guarantees.
Until recently  even for the simplest high-dimensional parameter estimation settings  no polynomial
time robust learning algorithms with dimension-independent error guarantees were known. Two
concurrent works [DKK+16  LRV16] made the ﬁrst progress on this front for the unsupervised set-
ting. Speciﬁcally  [DKK+16  LRV16] gave the ﬁrst polynomial time algorithms for robustly learning
the mean and covariance of high-dimensional Gaussians and other models. These works focused on
the dense regime and as a result did not obtain algorithms with sublinear sample complexity in
the sparse setting. Building on [DKK+16]  more recent work [BDLS17] obtained sample efﬁcient
polynomial time algorithms for the robust sparse setting  and in particular for the problems of robust
sparse mean estimation and robust sparse PCA studied in this paper. These algorithms are based the
unknown convex programming methodology of [DKK+16] and in particular inherently rely on the
ellipsoid algorithm. Moreover  the separation oracle required for the ellipsoid algorithm turns out
to be another convex program — corresponding to an SDP to solve sparse PCA. As a consequence 
the running time of these algorithms  while polynomially bounded  is impractically high.

1.2 Our Results and Techniques

The main contribution of this paper is the design of signiﬁcantly faster robust estimators for the
aforementioned high-dimensional sparse problems. More speciﬁcally  our algorithms are iterative
and each iteration involves a simple spectral operation (computing the largest eigenvalue of an ap-
proximate matrix). Our algorithms achieve the same error guarantee as [BDLS17] with similar sam-
ple complexity. At the technical level  we enhance the iterative ﬁltering methodology of [DKK+16]
to the sparse setting  which we believe is of independent interest and could lead to faster algorithms
for other robust sparse estimation tasks as well.
For robust sparse mean estimation  we show:
Theorem 1.2 (Robust Sparse Mean Estimation). Let D ∼ N (µ  I) be a Gaussian distribution on Rd
with unknown k-sparse mean vector µ  and ε > 0. Let S be an ε-corrupted set of samples from D of

size N = (cid:101)Ω(k2 log(d)/ε2). There exists an algorithm that  on input S  k  and ε runs in polynomial
time returns(cid:98)µ such that with probability at least 2/3 it holds (cid:107)(cid:98)µ − µ(cid:107)2 = O(ε(cid:112)log(1/ε)).

Some comments are in order. First  the sample complexity of our algorithm is asymptotically the
same as that of [BDLS17]  and matches the lower bound of [DKS17] against Statistical Query
algorithms for this problem. The major advantage of our algorithm over [BDLS17] is that while
their algorithm made use of the ellipsoid method  ours uses only spectral techniques and is scalable.
For robust sparse PCA in the spiked covariance model  we show:
Theorem 1.3 (Robust Sparse PCA). Let D ∼ N (0  I + ρvvT ) be a Gaussian distribution on Rd
with spiked covariance for an unknown k-sparse unit vector v  and 0 < ρ < O(1). For ε > 0 
let S be an ε-corrupted set of samples from D of size N = Ω(k4 log4(d/ε)/ε2). There exists an
algorithm that  on input S  k  and ε  runs in polynomial time and returns ˆv ∈ Rd such that with
probability at least 2/3 we have that (cid:107)ˆvˆvT − vvT(cid:107)F = O (ε log(1/ε)/ρ).

2

The sample complexity upper bound in Theorem 1.3 is somewhat worse than the information theo-
retic optimum of Θ(k2 log d/ε2). While the ellipsoid-based algorithm of [BDLS17] achieves near-
optimal sample complexity (within logarithmic factors)  our algorithm is practically viable as it only
uses spectral operations. We also note that the sample complexity in our above theorem is not known
to be optimal for our algorithm. It seems quite plausible  via a tighter analysis  that our algorithm in
fact has near-optimal sample complexity as well.

For both of our algorithms  in the most interesting regime of k (cid:28) √

d  the running time per iter-
ation is dominated by the O(N d2) computation of the empirical covariance matrix. The number
of iterations is at most εN  although it typically is much smaller  so both algorithms take at most
O(εN 2d2) time.

1.3 Related Work

There is extensive literature on exploiting sparsity in statistical estimation (see  e.g.  [HTW15]).
In this section  we summarize the related work that is directly related to the results of this paper.
Sparse mean estimation is arguably one of the most fundamental sparse estimation tasks and is
closely related to the Gaussian sequence model [Tsy08  Joh17]. The task of sparse PCA in the spiked
covariance model  initiated in [Joh01]  has been extensively investigated (see Chapter 8 of [HTW15]
and references therein). In this work  we design algorithms for the aforementioned problems that
are robust to a constant fraction of outliers.
Learning in the presence of outliers is an important goal in statistics studied since the 1960s [Hub64].
See  e.g.  [HR09  HRRS86] for book-length introductions in robust statistics. Until recently  all
known computationally efﬁcient high-dimensional estimators could tolerate a negligible fraction
of outliers  even for the task of mean estimation. Recent work [DKK+16  LRV16] gave the ﬁrst
efﬁcient robust estimators for basic high-dimensional unsupervised tasks  including mean and co-
variance estimation. Since the dissemination of [DKK+16  LRV16]  there has been a ﬂurry of
research activity on computationally efﬁcient robust learning in high dimensions [BDLS17  CSV17 
DKK+17  DKS17  DKK+18a  SCV18  DKS18b  DKS18a  HL18  KSS18  PSBR18  DKK+18b 
KKM18  DKS19  LSLC18a  CDKS18  CDG18  CDGW19].
In the context of robust sparse estimation  [BDLS17] obtained sample-efﬁcient and polynomial
time algorithms for robust sparse mean estimation and robust sparse PCA. The main difference
between [BDLS17] and the results of this paper is that the [BDLS17] algorithms use the ellipsoid
method (whose separation oracle is an SDP). Hence  these algorithms are prohibitively slow for
practical applications. More recent work [LSLC18b] gave an iterative method for robust sparse
mean estimation  which however requires multiple solutions to a convex relaxation for sparse PCA
in each iteration. Finally  [LLC19] proposed an algorithm for robust sparse mean estimation via iter-
ative trimmed hard thresholding. While this algorithm seems practically viable in terms of runtime 
it can only tolerate 1/(

k log(nd)) – i.e.  sub-constant – fraction of corruptions.

√

1.4 Paper Organization

In Section 2  we describe our algorithms and provide a detailed sketch of their analysis. In Section 3 
we report detailed experiments demonstrating the performance of our algorithms on synthetic data in
various parameter regimes. Due to space limitations  the full proofs of correctness for our algorithms
can be found in the full version of this paper.

2 Algorithms

In this section  we describe our algorithms in tandem with a detailed outline of the intuition behind
them and a sketch of their analysis. Due to space limitations  the proof of correctness is deferred to
the full version of our paper.
At a high-level  our algorithms use the iterative ﬁltering methodology of [DKK+16]. The main idea
is to iteratively remove a small subset of the dataset  so that eventually we have removed all the
important outliers and the standard estimator (i.e.  the estimator we would have used in the noiseless
case) works. Before we explain our new ideas that enhance the ﬁltering methodology to the sparse
setting  we provide a brief technical description of the approach.

3

Overview of Iterative Filtering. The basic idea of iterative ﬁltering [DKK+16] is the following:
In a given iteration  carefully pick some test statistic (such as v · x for a well-chosen v). If there
were no outliers  this statistic would follow a nice distribution (with good concentration properties).
This allows us to do some sort of statistical hypothesis testing of the “null hypothesis” that each
xi is an inlier  rejecting it (and believing that xi is an outlier) if v · xi is far from the expected
distribution. Because there are a large number of such hypotheses  one uses a procedure reminiscent
of the Benjamini-Hochberg procedure [BH95] to ﬁnd a candidate set of outliers with low false
discovery rate (FDR)  i.e.  a set with more outliers than inliers in expectation. This procedure looks
for a threshold T such that the fraction of points with test statistic above T is at least a constant
factor more than it “should” be. If such a threshold is found  those points are mostly outliers and can
be safely removed. The key goal is to judiciously design a test statistic such that either the outliers
aren’t particularly important—so the naive empirical solution is adequate—or at least one point will
be ﬁltered out.
In other words  the goal is to ﬁnd a test statistic such that  if the distribution of the test statistic is
“close” to what it would be in the outlier-free world  then the outliers cannot perturb the answer too
much. An additional complication is that the test statistics depend on the data (such as v · x  where
v is the principal component of the data) making the distribution on inliers also nontrivial. This
consideration drives the sample complexity of the algorithms.
In the algorithms we describe below  we use a speciﬁc parameterized notion of a good set. We
deﬁne these precisely in the supplementary material  brieﬂy  any large enough sample drawn from
the uncorrupted distribution will satisfy the structural properties required for the set to be good.
We now describe how to design such test statistics for our two sparse settings.

Notation Before we describe our algorithms  we set up some notation. We deﬁne hk : Rd → Rd
to be the thresholding operator that keeps the k entries of v with the largest magnitude and sets the
rest to 0. For a ﬁnite set S  we will use a ∈u S to mean that a is chosen uniformly at random from
S. For M ∈ Rd × Rd and U ⊆ [d]  let MU denote the matrix M restricted to the U × U submatrix.

2:

3:

4:
5:
6:
7:

Algorithm 1 Robust Sparse Mean Estimation via Iterative Filtering
1: procedure ROBUST-SPARSE-MEAN(S  k  ε  τ)
input: A multiset S such that there exists an (ε  k  τ )-good set G with ∆(G  S) ≤ 2ε.

the largest magnitude k2 − k off-diagonal entries  with ties broken so that if (i  j) ∈ U then
(j  i) ∈ U.

output: Multiset S(cid:48) with smaller fraction of corrupted samples or a vector (cid:98)µ with (cid:107)(cid:98)µ − µ(cid:107)2 ≤
ε(cid:112)log(1/ε).
Compute the sample mean (cid:101)µ = EX∈uS[X] and the sample covariance matrix (cid:101)Σ   i.e. 
(cid:101)Σ = ((cid:101)Σi j)1≤i j≤d with(cid:101)Σi j = EX∈uS[(Xi −(cid:101)µi)(Xj −(cid:101)µj)].
Let U ⊆ [d] × [d] be the set of the k largest magnitude entries of the diagonal of(cid:101)Σ − I and
if (cid:107)((cid:101)Σ − I)(U )(cid:107)F ≤ O(ε log(1/ε)) then return(cid:98)µ := hk((cid:101)µ).
Compute the largest eigenvalue λ∗ of ((cid:101)Σ − I)U(cid:48) and a corresponding unit eigenvector v∗.
if λ∗ ≥ Ω(ε(cid:112)log(1/ε)) then: Let δ(cid:96) := 3
PrX∈uS [|v∗ · (X −(cid:101)µ)| ≥ T + δ(cid:96)] ≥ 9 · erfc(T /
return the multiset S(cid:48) = {x ∈ S : |v∗ · (x −(cid:101)µ)| ≤ T + δ(cid:96)}.
(cid:17)
(U )(x −(cid:101)µ) − Tr(((cid:101)Σ − I)(U ))

(cid:16)
(x −(cid:101)µ)T ((cid:101)Σ − I)T

ελ∗. Find T > 0 such that
3ε2

√

Set U(cid:48) = {i ∈ [d] : (i  j) ∈ U}.

√

2) +

.

T 2 ln(k ln(N d/τ ))

/(cid:107)((cid:101)Σ − I)(U )(cid:107)F .

8:
9:
10:

Let p(x) =
Find T > 4 such that

PrX∈uS[|p(X)| ≥ T ] ≥ 9 exp(−T /4) + 3ε2/(T ln2 T ) .

return the multiset S(cid:48) = {x ∈ S : |p(x)| ≤ T}.

11:

4

Robust Sparse Mean Estimation. Here we brieﬂy describe the motivation and analysis of Algo-
rithm 1  describing a single iteration of our ﬁlter for the robust sparse mean setting.
In order to estimate the k-sparse mean µ  it sufﬁces to ensure that our estimate µ(cid:48) has |v · (µ(cid:48) − µ)|
small for any 2k-sparse unit vector v. The now-standard idea in robust statistics [DKK+16] is that
if a small number of corrupted samples sufﬁce to cause a large change in our estimate of v · µ  then
this must lead to a substantial increase in the sample variance of v · x  which we can detect.

Thus  a very basic form of a robust algorithm might be to compute a sample covariance matrix (cid:101)Σ 
and let v be the 2k-sparse unit vector that maximizes vT(cid:101)Σv. If this number is close to 1  it certiﬁes

that our estimate µ(cid:48) — obtained by truncating the sample mean to its k-largest entries — is a good
estimate of the true mean µ. If not  this will allow us to ﬁlter our sample set by throwing away the
values where v · x is furthest from the true mean. This procedure guarantees that we have removed
more corrupted samples than uncorrupted ones. We then repeat the ﬁlter until the empirical variance
in every sparse direction is close to 1.
Unfortunately  the optimization problem of ﬁnding the optimal v is computationally challenging 

requiring a convex program. To circumvent the need for a convex program  we notice that vT(cid:101)Σv −
1 = ((cid:101)Σ−I)·(vvT ) is large only if(cid:101)Σ−I has large entries on the (2k)2 non-zero entries of vvT . Thus 
if the 4k2 largest entries of(cid:101)Σ − I had small (cid:96)2-norm  this would certify that no such bad v existed
matrix consisting of the large entries of(cid:101)Σ (for the moment assume that they are all off diagonal  but
this is not needed). We know that the sample mean of p(x) = (x− µ(cid:48))T A(x− µ(cid:48)) =(cid:101)Σ· A = (cid:107)A(cid:107)2
F .
On the other hand  if µ(cid:48) approximates µ on the O(k2) entries in question  we would have that
(cid:107)p(cid:107)2 = (cid:107)A(cid:107)F . This means that if (cid:107)A(cid:107)F is reasonably large  an ε-fraction of corrupted points
F = (cid:107)A(cid:107)F(cid:107)p(cid:107)2. This means that many of these errors must
changed the mean of p from 0 to (cid:107)A(cid:107)2
have had |p(x)| (cid:28) (cid:107)A(cid:107)F /ε(cid:107)p(cid:107)2. This becomes very unlikely for good samples if (cid:107)A(cid:107)F is much
larger than ε (by standard results on the concentration of Gaussian polynomials). Thus  if µ(cid:48) is
approximately µ on these O(k2) coordinates  we can produce a ﬁlter. To ensure this  we can use
existing ﬁlter-based algorithms to approximate the mean on these O(k2) coordinates. This results

and would allow us to return the truncated sample mean. In case these entries have large (cid:96)2-norm 
we show that we can produce a ﬁlter that removes more bad samples than good ones. Let A be the

in Algorithm 1. For the analysis  we note that if the entries of A are small  then vT ((cid:101)Σ− I)v must be

small for any unit k-sparse v  which certiﬁes that the truncated sample mean is good. Otherwise  we
can ﬁlter the samples using the ﬁrst kind of ﬁlter. This ensures that our mean estimate is sufﬁciently
close to the true mean that we can then ﬁlter using the second kind of ﬁlter.
It is not hard to show that the above works if we are given sufﬁciently many samples  but to obtain
a tight analysis of the sample complexity  we need a number of subtle technical ideas. The detailed
analysis of the sample complexity is deferred to the full version of our paper.

Robust Sparse PCA Here we brieﬂy describe the motivation and analysis of Algorithm 2  de-
scribing a single iteration of our ﬁlter for the sparse PCA setting.
Note that estimating the k-sparse vector v is equivalent to estimating E[XX T − I] = vvT . In fact 
estimating E[XX T − I] to error ε in Frobenius norm allows one to estimate v within error ε in
(cid:96)2-norm. Thus  we focus on he task of robustly approximating the mean of Y = XX T − I.
Our algorithm is going to take advantage of one fact about X that even errors cannot hide: that
Var[v · X] is large. This is because removing uncorrupted samples cannot reduce the variance by
much more than an ε-fraction  and adding samples can only increase it. This means that an adversary
attempting to fool our algorithm can only do so by creating other directions where the variance is
large  or simply by adding other large entries to the sample covariance matrix in order to make it
hard to ﬁnd this particular k-sparse eigenvector. In either case  the adversary is creating large entries
in the empirical mean of Y that should not be there. This suggests that the largest entries of the
empirical mean of Y   whether errors or not  will be of great importance.
These large entries will tell us where to focus our attention. In particular  we can ﬁnd the k2 largest
entries of the empirical mean of Y and attempt to ﬁlter based on them. When we do so  one of two
things will happen: Either we remove bad samples and make progress or we verify that these entries
ought to be large  and thus must come from the support of v. In particular  when we reach the second

5

Algorithm 2 Robust Sparse PCA via Iterative Filtering

1: procedure ROBUST-SPARSE-PCA(S  k (cid:101)Σ  ε  δ  τ)
input: A multiset S  an estimate of the true covariance(cid:101)Σ  a real number δ ∈ R.

output: A multiset S(cid:48) with smaller fraction of corrupted samples or a matrix Σ(cid:48) with (cid:107)Σ(cid:48) − Σ(cid:107)F ≤

O(

√
εδ + ε log(1/ε))
For any x ∈ Rd deﬁne γ(x) := vec(xxT − I) ∈ Rd2.
Compute ˜µ := ES[γ(x)]  ˆµ = hk2(µ) and Q := Supp(ˆµ).
Compute

2:
3:
4:

5:

6:
7:
8:

MQ := ES[(γ(x) − ˜µ)(γ(x) − ˜µ)T ]Q×Q ∈ Rk2 × Rk2

CovX∼N (0 (cid:101)Σ)(γ(x)Q).

Let λ  v∗ be the maximum eigenvalue and corresponding eigenvector of MQ −
if λ < C · (δ + ε log2(1/ε))  where C is a sufﬁciently large constant then
Let ˆµ = median ({γ(x) · v∗ | x ∈ S}). Find a number T > log(1/ε) satisfying

Compute w  the largest eigenvector of mat(˜µ)Q. return wwT + I.

PrS[|γ(x)Q · v∗ − ˆµ| > CT + 3] >

ε

T 2 log2(T )

.

return S(cid:48) = {x ∈ S | |(γ(x)Q · v∗) − ˆµ| < T}.

case  since the adversary cannot shrink the empirical variance of v · X by much  almost all of the
entries on the support of v must remain large  and this can be captured by our algorithm.
The above algorithm works under a set of deterministic conditions on the good set of samples that
are satisﬁed with high probability with poly(k) log(d)/ε2 samples. Our current analysis does not
establish the information-theoretically optimal sample size of O(k2 log(d)/ε2)  though we believe
that this plausible via a tighter analysis.
We note that a naive implementation of this algorithm will achieve error poly(ε) in our ﬁnal estimate
for v  while our goal is to obtain ˜O(ε) error. To achieve this  we need to overcome two difﬁculties:
First  when trying to ﬁlter Y on subsets of its coordinates  we do not know the true variance of Y  
and thus cannot expect to obtain ˜O(ε) error. This is ﬁxed with a bootstrapping method similar to that
in [Kan18] to estimate the covariance of a Gaussian. In particular  we do not know Var[Y ] a priori 
but after we run the algorithm  we obtain an approximation to v  which gives an approximation to
Var[Y ]. This in turn lets us get a better approximation to v and a better approximation to Var[Y ];
and so on.

3 Experiments

For every experiment  we run 10 trials and plot the median value of the measurement. We shade the
interquartile range around each measurement as a measure of the conﬁdence of that measurement.
Each experiment was run on a computer with a 2.7 GHz Intel Core i5 processor with an 8GB 1867
MHz DDR3 RAM.

3.1 Robust Sparse Mean Estimation

The performance of robust estimation algorithms depend heavily on the noise model. The “hard”
noise distributions for one algorithm may be easy for a different algorithm  if that one can identify
and ﬁlter out the outliers. We therefore consider three different synthetic data distributions: two that
demonstrate the ε
k worst-case performance of other algorithms  and one that demonstrates the

ε(cid:112)log(1/ε) performance of our full algorithm.

√

The algorithms we consider are RME sp  our algorithm; RME sp L  a version of our algorithm with
only the linear ﬁlter and not the quadratic one; NP  the “naive pruning” algorithm that drops samples
with obviously-outlier coordinates  then outputs the empirical mean; oracle  which is told exactly

6

(a) Unlike RANSAC  our algorithm RME sp can ﬁl-
ter out the noise and match the oracle’s perfor-
mance. RME also matches the oracle  but needs
more samples.
Figure 1: Constant-bias noise is easy for our algorithm  since it is caught by the linear ﬁlter.

(b) For ﬁxed m  as k increases  RANSAC and NP
both diverge from RME and RME sp.

(a) With sufﬁciently many samples  the quadratic
ﬁlter can ﬁlter out the noise  matching the oracle.
The linear ﬁlter alone does not  even with a large
number of samples.

(b) For k (cid:28) √

√
d  the linear ﬁlter alone does
not ﬁlter out the noise  leading to an ε
k depen-
dence for RME sp L. Our algorithm RME sp nearly
matches oracle.

Figure 2: The linear-hiding noise model shows that the quadratic ﬁlter is necessary.

(a) This noise model gives Ω(ε(cid:112)log(1/ε)) error
Figure 3: The ﬂipping noise model demonstrates that the error can remain Ω(ε(cid:112)log(1/ε)).

to the oracle  and RME sp is at most twice this.

(b) This gap persists regardless of m.

(a) The constant noise model is easy to remove
and does not take many samples.

(b) The linear-hiding noise model is harder and
requires more samples to get the same guarantee.

Figure 4: Sample complexity required to do well—in this case  70% of errors being less than 1.2—
depends on the noise model.

7

0100200300m0.00.51.01.52.0L2 Lossd = 300  k = 10  eps = 0.1. Noise is constant +2 biasedoracleRMENPRME_spRANSAC020406080100k0.00.51.01.52.0L2 Lossd = 300  eps = 0.1  m = 5000. Noise is constant +2 biasedRME_spNPoracleRMERANSAC20004000600080001000012000m0.00.20.40.60.81.0L2 Lossd = 1000  k = 40  eps = 0.1. Noise is linear-hidingoracleRME_sp_LRME_spNP050100150200k0.000.250.500.751.00L2 Lossd = 300  eps = 0.1  m = 5000. Noise is linear-hidingRME_spNPRME_sp_Loracle0.00.10.20.30.40.5eps0.00.20.40.60.81.0L2 Lossd = 10  k = 1  m = 5000. Noise is tail-flippingoracleRME_sp02004006008001000m0.00.20.40.60.8L2 Lossd = 10  k = 1  eps = 0.1. Noise is tail-flippingoracleRME_sp51015k050100150samplesd = 200  eps = 0.1. Noise is a constant +2 biasedRME_sp51015k0204060samplesd = 200  eps = 0.1. Noise is linear-hidingRME_sp(a) Not only does RME not have small error for
small sample complexity 
interestingly it also
takes longer to terminate.

(b) RME sp takes time close to RME sp L until the
quadratic ﬁlter begins to apply (as can be seen in
Figure 2)  after which it takes much longer.

(c) The runtimes for our sparse algorithms does
not change very much as we increase k for the
linear-hiding noise.

(d) The runtimes for our sparse algorithms ap-
pears to increases with d linearly the case of
linear-hiding noise.

Figure 5: Runtimes for robust mean estimation.

which coordinates are inliers and outputs their empirical mean; RME  which applies the non-sparse
robust mean estimation algorithm of [DKK+17]; and RANSAC  which computes the mean of a ran-
domly chosen set of points  half the size of the entire set. One mean is preferred to another if it has
more points in a ball of radius
d around it. For algorithms that have non-sparse outputs  we
sparsify to the largest k coordinates before measuring the (cid:96)2 distance to the true mean.
Our distributions are:

(cid:112)

d +

√

√

• Constant-bias noise. Noise that biases every coordinate consistently (e.g.  if the outliers
add 2 to every coordinate  or set every coordinate to µi + 1) is difﬁcult for naive algorithms
(such as coordinate-wise median  NP  RANSAC) to deal with  but ideal for the linear ﬁlter. In
Figure 1 we consider the noise that adds 2 to every coordinate.
• Linear-hiding noise. To demonstrate that the quadratic ﬁlter in our algorithm is necessary 
we use the following data distribution. The inliers are drawn from N (0  I). The outliers
are evenly split between two types: N (1S  I) for some size-k set S  and N (0  2I − IS).
The diagonal of the empirical covariance does not reveal S  so our linear ﬁlter fails to prune
anything  leading to ε
k error for RME sp L; the quadratic ﬁlter successfully removes all
the outliers. This is shown in Figure 2.
• Flipping noise. For both those types of noise  with sufﬁciently many samples our ﬁnal
algorithm will prune out essentially all the outliers; there also exist noise models where

noise model that picks a k-sparse direction v  and replaces the ε fraction of points furthest
in the −v direction with points in the +v direction. In fact  for this noise even the oracle

Ω(ε(cid:112)log(1/ε)) noise will remain at all times. In Figure 3 we demonstrate this for the
method also has Ω(ε(cid:112)log(1/ε)) error from the missing points  but our algorithm has twice
formance of RME sp seems to be within a constant factor of the O(ε(cid:112)log(1/ε)) worst-case per-

Discussion. Matching our theoretical results  with sufﬁciently many samples the worst-case per-

the error from the unﬁlterable added points.

formance of oracle. This is not true for the naive algorithms NP  RANSAC  or the simpliﬁcation
RME sp L of our algorithm  which all have an ε
k dependence. While our theoretical results show

√

8

0100200300m0.000.050.100.150.20secd = 300  k = 10  eps = 0.1. Noise is constant +2 biasedNP runtimeRME_sp runtimeRME runtime20004000600080001000012000m0123secd = 1000  k = 40  eps = 0.1. Noise is linear-hidingNP runtimeRME_sp_L runtimeRME_sp runtimeRME runtime050100150200k0.00.10.20.30.40.5secd = 300  eps = 0.1  m = 5000. Noise is linear-hidingNP runtimeRME_sp_L runtimeRME_sp runtime100200300d0.000.020.040.060.080.10seck = 20  eps = 0.1  m = 3000. Noise is linear-hidingNP runtimeRME_sp_L runtimeRME_sp runtime(a) The natural dense algorithm RDPCA requires
more samples than the sparse algorithm to get er-
ror < 0.1

√
(b) For a ﬁxed m  RSPCA performs better than
d and then performs worse.
RDPCA when k <
until coming close to RDPCA. Note that the vari-
ance of RSPCA is smaller than that of RDPCA.
Figure 6: Sample complexity of RSPCA is better than RDPCA for smaller sparsity.

that (cid:101)O(k2) samples sufﬁce  the empirical results given in Figure 4 are consistent with (cid:101)O(k) being

sufﬁcient.
Our algorithm runs much faster than the ellipsoid based approach. For instance for k = 10  d =
300  m = 50 for the case of constant-biased noise our algorithm takes time 0.015 seconds to ﬁnish.
In comparison the very ﬁrst iteration for the SDP-based solution takes 10 seconds to solve with
CVXOPT; the full ellipsoid-based algorithm  if implemented  would take many times that.

3.2 Robust Sparse PCA

In Figure 6 we compare our robust sparse PCA algorithm RSPCA to a dense algorithm RDPCA for
robust PCA. RDPCA looks at the empirical covariance matrix and then in the direction of maximum
variance robustly estimates standard deviation. The algorithm then ﬁlters points using a modiﬁed
version of the linear ﬁlter from [DKK+17] and hence requires a sample complexity of ˜O(d). For this
algorithm  we only consider a single simple noise model. We draw outlier samples from N (0  I +
uuT ) where u has disjoint support from the true vector v.

√

d; this

The sparse algorithm seems to perform better than the dense algorithm for k up to roughly
is better than what we can prove  which is that it should be better up to at least d1/4.

4 Conclusions

In this paper  we have presented iterative ﬁltering algorithms for two natural and fundamental robust
sparse estimation tasks: sparse mean estimation and sparse PCA. In both cases  our algorithms

achieve near-optimal (cid:101)O(ε) error with sample complexity primarily dependent on the sparsity k 

and only logarithmically on the ambient dimension d. Our theoretical guarantees are comparable
to those of [BDLS17]  with the signiﬁcant advantage that our algorithms only use simple spectral
techniques rather than the ellipsoid algorithm. This makes our algorithms practically viable and
easy to implement. Our implementations perform essentially as expected: in sparse settings they
√
require signiﬁcantly fewer samples than dense robust estimation  and have accuracy avoiding the

k dependence of common benchmark techniques like RANSAC.

5 Acknowledgements

The authors would like to thank the following sources of support.
Ilias Diakonikolas was supported by the NSF Award CCF-1652862 (CAREER) and a Sloan Re-
search Fellowship. Sushrut Karmalkar was supported by NSF Award CNS-1414023. Daniel Kane
was supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship. Eric
Price was supported in part by NSF Award CCF-1751040 (CAREER). A part of this work was
performed when Alistair Stewart was a postdoctoral researcher at USC.

9

01000200030004000m0.00.20.40.60.81.0L2 Lossd = 200  k = 4  eps = 0.1Robust sparse PCARobust dense PCA0510152025k0.00.20.40.60.81.0L2 Lossd = 300  eps = 0.1  m = 2000Robust sparse PCARobust dense PCAReferences
[BDLS17] S. Balakrishnan  S. S. Du  J. Li  and A. Singh. Computationally efﬁcient robust sparse
estimation in high dimensions. In Proc. 30th Annual Conference on Learning Theory
(COLT)  pages 169–212  2017.

[BH95] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical and
powerful approach to multiple testing. Journal of the Royal statistical society: series
B (Methodological)  57(1):289–300  1995.

[BNJT10] M. Barreno  B. Nelson  A. D. Joseph  and J. D. Tygar. The security of machine learn-

ing. Machine Learning  81(2):121–148  2010.

[BNL12] B. Biggio  B. Nelson  and P. Laskov. Poisoning attacks against support vector ma-
chines. In Proceedings of the 29th International Conference on Machine Learning 
ICML 2012  2012.

[CDG18] Y. Cheng  I. Diakonikolas  and R. Ge. High-dimensional robust mean estimation in
nearly-linear time. CoRR  abs/1811.09380  2018. Conference version in SODA 2019 
p. 2755-2771.

[CDGW19] Y. Cheng  I. Diakonikolas  R. Ge  and D. P. Woodruff. Faster algorithms for high-
dimensional robust covariance estimation. In Conference on Learning Theory  COLT
2019  pages 727–757  2019.

[CDKS18] Y. Cheng  I. Diakonikolas  D. M. Kane  and A. Stewart. Robust learning of ﬁxed-
structure Bayesian networks. In Proc. 33rd Annual Conference on Neural Information
Processing Systems (NIPS)  2018.

[CSV17] M. Charikar  J. Steinhardt  and G. Valiant. Learning from untrusted data. In Proc. 49th

Annual ACM Symposium on Theory of Computing (STOC)  pages 47–60  2017.

[DKK+16] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proc. 57th
IEEE Symposium on Foundations of Computer Science (FOCS)  pages 655–664  2016.
Journal version in SIAM Journal on Computing  48(2)  p. 742–864  2019.

[DKK+17] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Being
robust (in high dimensions) can be practical. In Proc. 34th International Conference
on Machine Learning (ICML)  pages 999–1008  2017.

[DKK+18a] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robustly
learning a Gaussian: Getting optimal error  efﬁciently. In Proc. 29th Annual Sympo-
sium on Discrete Algorithms (SODA)  pages 2683–2702  2018.

[DKK+18b] I. Diakonikolas  G. Kamath  D. M Kane  J. Li  J. Steinhardt  and A. Stewart. Sever: A
robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815 
2018.

[DKS17] I. Diakonikolas  D. M. Kane  and A. Stewart. Statistical query lower bounds for robust
estimation of high-dimensional Gaussians and Gaussian mixtures. In Proc. 58th IEEE
Symposium on Foundations of Computer Science (FOCS)  pages 73–84  2017.

[DKS18a] I. Diakonikolas  D. M. Kane  and A. Stewart. Learning geometric concepts with nasty
noise. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC)  pages
1061–1073  2018.

[DKS18b] I. Diakonikolas  D. M. Kane  and A. Stewart. List-decodable robust mean estimation
and learning mixtures of spherical Gaussians. In Proc. 50th Annual ACM Symposium
on Theory of Computing (STOC)  pages 1047–1060  2018.

[DKS19] I. Diakonikolas  W. Kong  and A. Stewart. Efﬁcient algorithms and lower bounds
for robust linear regression. In Proc. 30th Annual Symposium on Discrete Algorithms
(SODA)  2019.

10

[HL18] S. B. Hopkins and J. Li. Mixture models  robustness  and sum of squares proofs. In
Proc. 50th Annual ACM Symposium on Theory of Computing (STOC)  pages 1021–
1034  2018.

[HR09] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York  2009.

[HRRS86] F. R. Hampel  E. M. Ronchetti  P. J. Rousseeuw  and W. A. Stahel. Robust statistics.

The approach based on inﬂuence functions. Wiley New York  1986.

[HTW15] T. Hastie  R. Tibshirani  and M. Wainwright. Statistical Learning with Sparsity: The

Lasso and Generalizations. Chapman & Hall/CRC  2015.

[Hub64] P. J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.  35(1):73–

101  03 1964.

[Joh01] I. M. Johnstone. On the distribution of the largest eigenvalue in principal components

analysis. The Annals of Statistics  29(2):295–327  2001.

[Joh17] I. M. Johnstone. Gaussian estimation: Sequence and wavelet models. Available at

http://statweb.stanford.edu/~imj/GE_08_09_17.pdf  2017.

[Kan18] D. M. Kane. Robust covariance estimation. Talk given at TTIC Workshop on Com-
putational Efﬁciency and High-Dimensional Robust Statistics  2018. Available at
http://www.iliasdiakonikolas.org/tti-robust/Kane-Covariance.pdf.

[KKM18] A. Klivans  P. Kothari  and R. Meka. Efﬁcient algorithms for outlier-robust regres-
sion. In Proc. 31st Annual Conference on Learning Theory (COLT)  pages 1420–1430 
2018.

[KL93] M. J. Kearns and M. Li. Learning in the presence of malicious errors. SIAM Journal

on Computing  22(4):807–837  1993.

[KSS18] P. K. Kothari  J. Steinhardt  and D. Steurer. Robust moment estimation and improved
clustering via sum of squares. In Proc. 50th Annual ACM Symposium on Theory of
Computing (STOC)  pages 1035–1046  2018.

[LAT+08] J.Z. Li  D.M. Absher  H. Tang  A.M. Southwick  A.M. Casto  S. Ramachandran 
H.M. Cann  G.S. Barsh  M. Feldman  L.L. Cavalli-Sforza  and R.M. Myers. World-
wide human relationships inferred from genome-wide patterns of variation. Science 
319:1100–1104  2008.

[LLC19] L. Liu  T. Li  and C. Caramanis. High dimensional robust estimation of sparse models

via trimmed hard thresholding. CoRR  abs/1901.08237  2019.

[LRV16] K. A. Lai  A. B. Rao  and S. Vempala. Agnostic estimation of mean and covariance.
In Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS)  2016.

[LSLC18a] L. Liu  Y. Shen  T. Li  and C. Caramanis. High dimensional robust sparse regression.

arXiv preprint arXiv:1805.11643  2018.

[LSLC18b] L. Liu  Y. Shen  T. Li  and C. Caramanis. High dimensional robust sparse regression.

CoRR  abs/1805.11643  2018.

[PLJD10] P. Paschou  J. Lewis  A. Javed  and P. Drineas. Ancestry informative markers for ﬁne-
scale individual assignment to worldwide populations. Journal of Medical Genetics 
47:835–847  2010.

[PSBR18] A. Prasad  A. S. Suggala  S. Balakrishnan  and P. Ravikumar. Robust estimation via

robust gradient estimation. arXiv preprint arXiv:1802.06485  2018.

[RPW+02] N. Rosenberg  J. Pritchard  J. Weber  H. Cann  K. Kidd  L.A. Zhivotovsky  and M.W.

Feldman. Genetic structure of human populations. Science  298:2381–2385  2002.

11

[SCV18] J. Steinhardt  M. Charikar  and G. Valiant. Resilience: A criterion for learning in
the presence of arbitrary outliers. In Proc. 9th Innovations in Theoretical Computer
Science Conference (ITCS)  pages 45:1–45:21  2018.

[SKL17] J. Steinhardt  P. Wei Koh  and P. S. Liang. Certiﬁed defenses for data poisoning attacks.

In Advances in Neural Information Processing Systems 30  pages 3520–3532  2017.

[Tsy08] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Com-

pany  Incorporated  2008.

[Val85] L. Valiant. Learning disjunctions of conjunctions. In Proceedings of the Ninth Inter-

national Joint Conference on Artiﬁcial Intelligence  pages 560–566  1985.

12

,Abhishek Kumar
Prasanna Sattigeri
Kahini Wadhawan
Leonid Karlinsky
Rogerio Feris
Bill Freeman
Gregory Wornell
Ilias Diakonikolas
Daniel Kane
Sushrut Karmalkar
Eric Price
Alistair Stewart