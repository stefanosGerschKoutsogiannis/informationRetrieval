2016,Fast and Provably Good Seedings for k-Means,Seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-Means. However  k-means++ seeding  the state of the art algorithm  does not scale well to massive datasets as it is inherently sequential and requires k full passes through the data. It was recently shown that Markov chain Monte Carlo sampling can be used to efficiently approximate the seeding step of k-means++. However  this result requires assumptions on the data generating distribution.  We propose a simple yet fast seeding algorithm that produces *provably* good clusterings even *without assumptions* on the data. Our analysis shows that the algorithm allows for a favourable trade-off between solution quality and computational cost  speeding up k-means++ seeding by up to several orders of magnitude. We validate our theoretical results in extensive experiments on a variety of real-world data sets.,Fast and Provably Good Seedings for k-Means

Olivier Bachem

ETH Zurich

olivier.bachem@inf.ethz.ch

Department of Computer Science

Department of Computer Science

Mario Lucic

ETH Zurich

lucic@inf.ethz.ch

S. Hamed Hassani

Department of Computer Science

ETH Zurich

hamed@inf.ethz.ch

Andreas Krause

Department of Computer Science

ETH Zurich

krausea@ethz.ch

Abstract

Seeding – the task of ﬁnding initial cluster centers – is critical in obtaining high-
quality clusterings for k-Means. However  k-means++ seeding  the state of the
art algorithm  does not scale well to massive datasets as it is inherently sequential
and requires k full passes through the data. It was recently shown that Markov
chain Monte Carlo sampling can be used to efﬁciently approximate the seeding
step of k-means++. However  this result requires assumptions on the data gener-
ating distribution. We propose a simple yet fast seeding algorithm that produces
provably good clusterings even without assumptions on the data. Our analysis
shows that the algorithm allows for a favourable trade-off between solution quality
and computational cost  speeding up k-means++ seeding by up to several orders
of magnitude. We validate our theoretical results in extensive experiments on a
variety of real-world data sets.

1

Introduction

k-means++ (Arthur & Vassilvitskii  2007) is one of the most widely used methods to solve k-Means
clustering. The algorithm is simple and consists of two steps: In the seeding step  initial cluster
centers are found using an adaptive sampling scheme called D2-sampling. In the second step  this
solution is reﬁned using Lloyd’s algorithm (Lloyd  1982)  the classic iterative algorithm for k-Means.
The key advantages of k-means++ are its strong empirical performance  theoretical guarantees on
the solution quality  and ease of use. Arthur & Vassilvitskii (2007) show that k-means++ produces
clusterings that are in expectation O(log k)-competitive with the optimal solution without any
assumptions on the data. Furthermore  this theoretical guarantee already holds after the seeding
step. The subsequent use of Lloyd’s algorithm to reﬁne the solution only guarantees that the solution
quality does not deteriorate and that it converges to a locally optimal solution in ﬁnite time. In
contrast  using naive seeding such as selecting data points uniformly at random followed by Lloyd’s
algorithm can produce solutions that are arbitrarily bad compared to the optimal solution.
The drawback of k-means++ is that it does not scale easily to massive data sets since both its
seeding step and every iteration of Lloyd’s algorithm require the computation of all pairwise distances
between cluster centers and data points. Lloyd’s algorithm can be parallelized in the MapReduce
framework (Zhao et al.  2009) or even replaced by fast stochastic optimization techniques such as
online or mini-batch k-Means (Bottou & Bengio  1994; Sculley  2010). However  the seeding step
requires k inherently sequential passes through the data  making it impractical even for moderate k.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

This highlights the need for a fast and scalable seeding algorithm. Ideally  it should also retain the
theoretical guarantees of k-means++ and provide equally competitive clusterings in practice. Such
an approach was presented by Bachem et al. (2016) who propose to approximate k-means++ using a
Markov chain Monte Carlo (MCMC) approach and provide a fast seeding algorithm. Under natural
assumptions on the data generating distribution  the authors show that the computational complexity
of k-means++ can be greatly decreased while retaining the same O(log k) guarantee on the solution
quality. The drawback of this approach is that these assumptions may not hold and that checking
their validity is expensive (see detailed discussion in Section 3).
Our contributions. The goal of this paper is to provide fast and competitive seedings for k-Means
clustering without prior assumptions on the data. As our key contributions  we
(1) propose a simple yet fast seeding algorithm for k-Means 
(2) show that it produces provably good clusterings without assumptions on the data 
(3) provide stronger theoretical guarantees under assumptions on the data generating distribution 
(4) extend the algorithm to arbitrary distance metrics and various divergence measures 
(5) compare the algorithm to previous results  both theoretically and empirically  and
(6) demonstrate its effectiveness on several real-world data sets.

2 Background and related work

We will start by formalizing the problem and reviewing several recent results. Let X denote a set of
n points in Rd. For any ﬁnite set C ⇢ Rd and x 2X   we deﬁne
c2Ckx  ck2
2.

d(x  C)2 = min

The objective of k-Means clustering is to ﬁnd a set C of k cluster centers in Rd such that the
quantization error C(X ) is minimized  where

C(X ) = Xx2X

d(x  C)2.

We denote the optimal quantization error with k centers by k

OP T (X )  the mean of X by µ(X )  and
the variance of X by Var(X ) =Px2X d(x  µ(X ))2. We note that 1
D2-sampling. Given a set of centers C  the D2-sampling strategy  as the name suggests  is to sample
each point x 2X with probability proportional to the squared distance to the selected centers 

OP T (X ) = Var(X ).

(1)

p(x | C) =

d(x  C)2

Px02X d(x0  C)2 .

The seeding step of k-means++ builds upon D2-sampling: It ﬁrst samples an initial center uniformly
at random. Then  k  1 additional centers are sequentially added to the previously sampled centers
using D2-sampling. The resulting computational complexity is ⇥(nkd)  as for each x 2X the
distance d(x  C)2 in (1) needs to be updated whenever a center is added to C.
Metropolis-Hastings. The Metropolis-Hastings algorithm (Hastings  1970) is a MCMC method for
sampling from a probability distribution p(x) whose density is known only up to constants. Consider
the following variant that uses an independent proposal distribution q(x) to build a Markov chain:
Start with an arbitrary initial state x1 and in each iteration j 2 [2  . . .   m] sample a candidate yj using
q(x). Then  either accept this candidate (i.e.  xj = yj) with probability

⇡(xj1  yj) = min✓ p(yj)

p(xj1)

q(xj1)
q(yj)

  1◆

(2)

or reject it otherwise (i.e.  xj = xj1). The stationary distribution of this Markov chain is p(x).
Hence  for m sufﬁciently large  the distribution of xm is approximately p(x).
Approximation using MCMC (K-MC2). Bachem et al. (2016) propose to speed up k-means++ by
replacing the exact D2-sampling in (1) with a fast approximation based on MCMC sampling. In each
iteration j 2 [2  3  . . .   k]  one constructs a Markov chain of length m using the Metropolis-Hasting

2

algorithm with an independent and uniform proposal distribution q(x) = 1/n. The key advantage is
that the acceptance probability in (2) only depends on d(yj  C)2 and d(xj1  C)2 since

min✓ p(yj)

p(xj1)

q(xj1)
q(yj)

  1◆ = min✓ d(yj  C)2

d(xj1  C)2   1◆ .

Critically  in each of the k  1 iterations  the algorithm does not require a full pass through the data 
but only needs to compute the distances between m points and up to k  1 centers. As a consequence 
the complexity of K-MC2 is Omk2d compared to O(nkd) for k-means++ seeding.
To bound the quality of the solutions produced by K-MC2  Bachem et al. (2016) analyze the mixing
time of the described Markov chains. To this end  the authors deﬁne the two data-dependent quantities:

↵(X ) = max
x2X

d(x  µ(X ))2

Px02X d(x0  µ(X ))2  

and (X ) =

1
OP T (X )
k
OP T (X )

.

(3)

In order to bound each term  the authors assume that the data is generated i.i.d. from a distribution F
and impose two conditions on F . First  they assume that F exhibits exponential tails and prove that

in this case ↵(X ) 2Olog2 n with high probability. Second  they assume that “F is approximately
uniform on a hypersphere”. This in turn implies that (X ) 2O (k) with high probability. Under
these assumptions  the authors prove that the solution generated by K-MC2 is in expectation O(log k)-
competitive with the optimal solution if m 2 ⇥k log2 n log k. In this case  the total computational
complexity of K-MC2 is Ok3d log2 n log k which is sublinear in the number of data points.

Other related work. A survey on seeding methods for k-Means was provided by Celebi et al.
(2013). D2-sampling and k-means++ have been extensively studied in the literature. Previous work
was primarily focused on related algorithms (Arthur & Vassilvitskii  2007; Ostrovsky et al.  2006;
Jaiswal et al.  2014  2015)  its theoretical properties (Ailon et al.  2009; Aggarwal et al.  2009) and
bad instances (Arthur & Vassilvitskii  2007; Brunsch & Röglin  2011). As such  these results are
complementary to the ones presented in this paper.
An alternative approach to scalable seeding was investigated by Bahmani et al. (2012). The au-
thors propose the k-meansk algorithm that retains the same O(log k) guarantee in expectation as
k-means++. k-meansk reduces the number of sequential passes through the data to O(log n) by
oversampling cluster centers in each of the rounds. While this allows one to parallelize each of the
O(log n) rounds  it also increases the total computational complexity from O(nkd) to O(nkd log n).
This method is feasible if substantial computational resources are available in the form of a cluster.
Our approach  on the other hand  has an orthogonal use case: It aims to efﬁciently approximate
k-means++ seeding with a substantially lower complexity.

3 Assumption-free K-MC2

Building on the MCMC strategy introduced by Bachem et al. (2016)  we propose an algorithm which
addresses the drawbacks of the K-MC2 algorithm  namely:
(1) The theoretical results of K-MC2 hold only if the data is drawn independently from a distribution
satisfying the assumptions stated in Section 2. For example  the results do not extend to heavy-
tailed distributions which are often observed in real world data.
(2) Verifying the assumptions  which in turn imply the required chain length  is computationally hard
and potentially more expensive than running the algorithm. In fact  calculating ↵(X ) already
requires two full passes through the data  while computing (X ) is NP-hard.
(3) Theorem 2 of Bachem et al. (2016) does not characterize the tradeoff between m and the expected
solution quality: It is only valid for the speciﬁc choice of chain length m =⇥k log2 n log k.
As a consequence  if the assumptions do not hold  we obtain no theoretical guarantee with regards
to the solution quality. Furthermore  the constants in Theorem 2 are not known and may be large.
Our approach addresses these shortcomings using three key elements. Firstly  we provide a proposal
distribution that renders the assumption on ↵(X ) obsolete. Secondly  a novel theoretic analysis
allows us to obtain theoretical guarantees on the solution quality even without assumptions on (X ).
Finally  our results characterize the tradeoff between increasing the chain length m and improving
the expected solution quality.

3

// Preprocessing step

Algorithm 1 ASSUMPTION-FREE K-MC2(AFK-MC2)
Require: Data set X   # of centers k  chain length m
1: c1 Point uniformly sampled from X
2: for all x 2X do
2 d(x  c1)2/Px02X d(x0  c1)2 + 1
3:

q(x) 1
// Main loop
4: C1 { c1}
5: for i = 2  3  . . .   k do
x Point sampled from X using q(x)
6:
dx d(x  Ci1)2
7:
for j = 2  3  . . .   m do
8:
y Point sampled from X using q(y)
9:
dy d(y  Ci1)2
10:
if dyq(x)
dxq(y) > Unif(0  1) then x y  dx dy
11:
12:
13: return Ck

Ci Ci1 [{ x}

2n

Proposal distribution. We argue that the choice of the proposal distribution is critical. Intuitively 
the uniform distribution can be a very bad choice if  in any iteration  the true D2-sampling distribution
is “highly” nonuniform. We suggest the following proposal distribution: We ﬁrst sample a center
c1 2X uniformly at random and deﬁne for all x 2X the nonuniform proposal

.

(4)

q(x | c1) =

1
2

+

1
2

d(x  c1)2

Px02X d(x0  c1)2
|
}

{z

(A)

1

|X||{z}(B)

The term (A) is the true D2-sampling distribution with regards to the ﬁrst center c1. For any data
set  it ensures that we start with the best possible proposal distribution in the second iteration. We
will show that this proposal is sufﬁcient even for later iterations  rendering any assumptions on ↵
obsolete. The term (B) regularizes the proposal distribution and ensures that the mixing time of
K-MC2 is always matched up to a factor of two.
Algorithm. Algorithm 1 details the proposed fast seeding algorithm ASSUMPTION-FREE K-MC2. In
the preprocessing step  it ﬁrst samples an initial center c1 uniformly at random and then computes the
proposal distribution q(· | c1). In the main loop  it then uses independent Markov chains of length m
to sample centers in each of the k  1 iterations. The complexity of the main loop is Omk2d.
The preprocessing step of ASSUMPTION-FREE K-MC2 requires a single pass through the data to
compute the proposal q(· | c1). There are several reasons why this additional complexity of O(nd)
is not an issue in practice: (1) The preprocessing step only requires a single pass through the data
compared to k passes for the seeding of k-means++. (2) It is easily parallelized. (3) Given random
access to the data  the proposal distribution can be calculated online when saving or copying the data.
(4) As we will see in Section 4  the effort spent in the preprocessing step pays off: It often allows
for shorter Markov chains in the main loop. (5) Computing ↵(X ) to verify the ﬁrst assumption of
K-MC2 is already more expensive than the preprocessing step of ASSUMPTION-FREE K-MC2.
Theorem 1. Let ✏ 2 (0  1) and k 2 N. Let X be any set of n points in Rd and C be the output of
Algorithm 1 with m = 1 + 8

✏ log 4k

✏ . Then  it holds that
E [C(X )]  8(log2 k + 2)k
✏ k2d log k

OP T (X ) + ✏ Var(X ).

✏.

The computational complexity of the preprocessing step is O(nd) and the computational complexity
of the main loop is O 1
This result shows that ASSUMPTION-FREE K-MC2 produces provably good clusterings for arbitrary
data sets without assumptions. The guarantee consists of two terms: The ﬁrst term  i.e.  8(log2 k +
OP T (X )  is the theoretical guarantee of k-means++. The second term  ✏ Var(X )  quantiﬁes the
2)k
potential additional error due to the approximation. The variance is a natural notion as the mean is
the optimal quantizer for k = 1. Intuitively  the second term may be interpreted as a scale-invariant
and additive approximation error.

4

Theorem 1 directly characterizes the tradeoff between improving the solution quality and the resulting
increase in computational complexity. As m is increased  the solution quality converges to the
theoretical guarantee of k-means++. At the same time  even for smaller chain lengths m  we obtain
a provable bound on the solution quality. In contrast  the guarantee of K-MC2 on the solution quality
only holds for a speciﬁc choice of m.
For completeness  ASSUMPTION-FREE K-MC2 may also be analyzed under the assumptions made
in Bachem et al. (2016). While for K-MC2 the required chain length m is linear in ↵(X ) 
ASSUMPTION-FREE K-MC2 does not require this assumption. In fact  we will see in Section 4 that
this lack of dependence of ↵(X ) leads to a better empirical performance. If we assume (X ) 2O (k) 
we obtain the following result similar to the one of K-MC2 (albeit with a shorter chain length m).
Corollary 1. Let k 2 N and X be a set of n points in Rd satisfying (X ) 2O (k). Let C be the
output of Algorithm 1 with m =⇥( k log k). Then it holds that
E [C(X )]  8(log2 k + 3)k

OP T (X ).

The computational complexity of the preprocessing is O(nd) and the computational complexity of the
main loop is Ok3d log k.

3.1 Proof sketch for Theorem 1
In this subsection  we provide a sketch of the proof of Theorem 1 and defer the full proof to
Section A of the supplementary materials. Intuitively  we ﬁrst bound how well a single Markov chain
approximates one iteration of exact D2-sampling. Then  we analyze how the approximation error
accumulates across iterations and provide a bound on the expected solution quality.
For the ﬁrst step  consider any set C ✓X of previously sampled centers. Let c1 2 C denote the
ﬁrst sampled center that was used to construct the proposal distribution q(x | c1) in (4). In a single
iteration  we would ideally sample a new center x 2X using D2-sampling  i.e.  from p(x | C) as
deﬁned in (1). Instead  Algorithm 1 constructs a Markov chain to sample a new center x 2X as the
next cluster center. We denote by ˜pc1
m(x | C) the implied probability of sampling a point x 2X using
this Markov chain of length m.
The following result shows that in any iteration either C is ✏1-competitive compared to c1 or the
Markov chain approximates D2-sampling well in terms of total variation distance1.
Lemma 1. Let ✏1 ✏ 2 2 (0  1) and c1 2X . Consider any set C ✓X with c1 2 C. For m 
1 + 2
✏1

  at least one of the following holds:

log 1
✏2

(i)
(ii)

C(X ) <✏ 1c1(X )  or
kp(· | C)  ˜pc1

m(· | C)kTV  ✏2.

In the second step  we bound the expected solution quality of Algorithm 1 based on Lemma 1. While
the full proof requires careful propagation of errors across iterations and a corresponding inductive
argument  the intuition is based on distinguishing between two possible cases of sampled solutions.
First  consider the realizations of the solution C that are ✏1-competitive compared to c1. By deﬁnition 
C(X ) <✏ 1c1(X ). Furthermore  the expected solution quality of these realizations can be bounded
by 2✏1 Var(X ) since c1 is chosen uniformly at random and hence in expectation c1(X )  2 Var(X ).
Second  consider the realizations that are not ✏1-competitive compared to c1. Since the quantization
error is non-increasing in sampled centers  Lemma 1 implies that all k  1 Markov chains result in a
good approximation of the corresponding D2-sampling. Intuitively  this implies that the approxima-
tion error in terms of total variation distance across all k 1 iterations is at most ✏2(k 1). Informally 
the expected solution quality is thus bounded with probability 1  ✏2(k  1) by the expected quality
of k-means++ and with probability ✏2(k  1) by c1(X ).
Theorem 1 can then be proven by setting ✏1 = ✏/4 and ✏2 = ✏/4k and choosing m sufﬁciently large.

1Let ⌦ be a ﬁnite sample space on which two probability distributions p and q are deﬁned. The total variation

distance kp  qkTV between p and q is given by 1

2Px2⌦ |p(x)  q(x)|.

5

Table 1: Data sets used in experimental evaluation

DATA SET
CSN (EARTHQUAKES)
KDD (PROTEIN HOMOLOGY)
RNA (RNA SEQUENCES)
SONG (MUSIC SONGS)
SUSY (SUPERSYM. PARTICLES)
WEB (WEB USERS)

N

80 000
145 751
488 565
515 345
5 000 000
45 811 883

D
17
74
8
90
18
5

K
200
200
200
2 000
2 000
2 000

EVAL

T
T
T
H
H
H

↵(X )
546
1 268

69
526
201
2

Table 2: Relative error of ASSUMPTION-FREE K-MC2 and K-MC2 in relation to k-means++.

K-MEANS++
RANDOM
K-MC2 (m = 20)
K-MC2 (m = 100)
K-MC2 (m = 200)
AFK-MC2 (m = 20)
AFK-MC2 (m = 100)
AFK-MC2 (m = 200)

CSN
0.00%

SONG
RNA
0.00%
0.00%
9.67%
399.54% 314.78% 915.46%
0.41% -0.03%
32.51%
65.34%
0.04% -0.08%
9.84%
14.81%
0.02% -0.04%
5.97%
5.48%
8.31%
1.45%
0.00%
0.01%
0.81% -0.02% -0.06%
0.25%
0.24%
-0.29%
0.04% -0.05%

WEB
SUSY
0.00%
0.00%
4.30% 107.57%
0.86%
-0.01%
0.09%
1.32%
0.06%
-0.16%

KDD
0.00%

31.91%
3.39%
0.65%
-0.12%
-0.11%
-0.03%

3.2 Extension to other clustering problems
While we only consider k-Means clustering and the Euclidean distance in this paper  the results are
more general. They can be directly applied  by transforming the data  to any metric space for which
there exists a global isometry on Euclidean spaces. Examples would be the Mahalanobis distance and
Generalized Symmetrized Bregman divergences (Acharyya et al.  2013).
The results also apply to arbitrary distance measures (albeit with different constants) as D2-sampling
can be generalized to arbitrary distance measures (Arthur & Vassilvitskii  2007). However  Var(X )
needs to be replaced by 1
OP T (X ) in Theorem 1 since the mean may not be the optimal quantizer (for
k = 1) for a different distance metric. The proposed algorithm can further be extended to different
potential functions of the form k·k l and used to approximate the corresponding Dl-sampling (Arthur
& Vassilvitskii  2007)  again with different constants. Similarly  the results also apply to bregman++
(Ackermann & Blömer  2010) which provides provably competitive solutions for clustering with a
broad class of Bregman divergences (including the KL-divergence and Itakura-Saito distance).
4 Experimental results
In this section2  we empirically validate our theoretical results and compare the proposed algorithm
ASSUMPTION-FREE K-MC2 (AFK-MC2) to three alternative seeding strategies:
(1) RANDOM  a
“naive” baseline that samples k centers from X uniformly at random  (2) the full seeding step of
k-means++  and (3) K-MC2. For both ASSUMPTION-FREE K-MC2 and K-MC2  we consider the
different chain lengths m 2{ 1  2  5  10  20  50  100  150  200}.
Table 1 shows the six data sets used in the experiments with their corresponding values for k. We
choose an experimental setup similar to Bachem et al. (2016): For half of the data sets  we both train
the algorithm and evaluate the corresponding solution on the full data set (denoted by T in the EVAL
column of Table 1). This corresponds to the classical k-Means setting. In practice  however  one is
often also interested in the generalization error. For the other half of the data sets  we retain 250 000
data points as the holdout set for the evaluation (denoted by H in the EVAL column of Table 1).
For all methods  we record the solution quality (either on the full data set or the holdout set) and mea-
sure the number of distance evaluations needed to run the algorithm. For ASSUMPTION-FREE K-MC2
this includes both the preprocessing and the main loop. We run every algorithm 200 times with
different random seeds and average the results. We further compute and display 95% conﬁdence
intervals for the solution quality.

2An implementation of ASSUMPTION-FREE K-MC2 has been released at http://olivierbachem.ch.

6

Figure 1: Quantization error in relation to the chain length m for ASSUMPTION-FREE K-MC2 and
K-MC2 as well as the quantization error for k-means++ and RANDOM (with no dependence on m).
ASSUMPTION-FREE K-MC2 substantially outperforms K-MC2 except on WEB. Results are averaged
across 200 iterations and shaded areas denote 95% conﬁdence intervals.

Figure 2: Quantization error
in relation to the number of distance evaluations for
ASSUMPTION-FREE K-MC2  K-MC2 and k-means++. ASSUMPTION-FREE K-MC2 provides a
speedup of up to several orders of magnitude compared to k-means++. Results are averaged across
200 iterations and shaded areas denote 95% conﬁdence intervals.

7

100101102103Chainlengthm0.51.01.52.02.53.03.54.04.55.0Trainingerror105CSNafk-mc2k-mc2k-means++random100101102103Chainlengthm1234567891011KDDafk-mc2k-mc2k-means++random100101102103Chainlengthm0123456789107RNAafk-mc2k-mc2k-means++random100101102103Chainlengthm6.56.66.76.86.97.07.17.27.3Holdouterror1011SONGafk-mc2k-mc2k-means++random100101102103Chainlengthm5.005.055.105.155.205.255.30105SUSYafk-mc2k-mc2k-means++random100101102103Chainlengthm0.81.01.21.41.61.82.02.22.4102WEBafk-mc2k-mc2k-means++random104105106107108Distanceevaluations0.51.01.52.02.53.03.54.04.55.0Trainingerror105CSNk-means++afk-mc2k-mc2104105106107108Distanceevaluations1234567891011KDDk-means++afk-mc2k-mc2104105106107108Distanceevaluations012345678107RNAk-means++afk-mc2k-mc2106107108109Distanceevaluations6.56.66.76.86.97.07.17.27.3Holdouterror1011SONGk-means++afk-mc2k-mc21061071081091010Distanceevaluations5.005.055.105.155.205.255.30105SUSYk-means++afk-mc2k-mc210610710810910101011Distanceevaluations0.81.01.21.41.61.82.02.22.4102WEBk-means++afk-mc2k-mc2Table 3: Relative speedup (in terms of distance evaluations) in relation to k-means++.

K-MEANS++
K-MC2 (m = 20)
K-MC2 (m = 100)
K-MC2 (m = 200)
AFK-MC2 (m = 20)
AFK-MC2 (m = 100)
AFK-MC2 (m = 200)

WEB
KDD
CSN
1.0⇥
1.0⇥
1.0⇥
40.0⇥ 72.9⇥ 244.3⇥ 13.3⇥ 237.5⇥ 2278.1⇥
8.0⇥ 14.6⇥
455.6⇥
4.0⇥
227.8⇥
7.3⇥
33.3⇥ 53.3⇥ 109.7⇥ 13.2⇥ 212.3⇥ 1064.7⇥
7.7⇥ 13.6⇥
371.0⇥
204.5⇥
7.0⇥
3.9⇥

RNA SONG
1.0⇥
1.0⇥
2.7⇥
48.9⇥
1.3⇥
24.4⇥
39.2⇥
2.6⇥
1.3⇥
21.8⇥

SUSY
1.0⇥
47.5⇥
23.8⇥
46.4⇥
23.5⇥

Discussion. Figure 1 shows the expected quantization error for the two baselines  RANDOM and
k-means++  and for the MCMC methods with different chain lengths m. As expected  the seeding
step of k-means++ strongly outperforms RANDOM on all data sets. As the chain length m increases 
the quality of solutions produced by both ASSUMPTION-FREE K-MC2 and K-MC2 quickly converges
to that of k-means++ seeding.
On all data sets except WEB  ASSUMPTION-FREE K-MC2 starts with a lower initial error due to the
improved proposal distribution and outperforms K-MC2 for any given chain length m. For WEB 
both algorithms exhibit approximately the same performance. This is expected as ↵(X ) of WEB is
very low (see Table 1). Hence  there is only a minor difference between the nonuniform proposal of
ASSUMPTION-FREE K-MC2 and the uniform proposal of K-MC2. In fact  one of the key advantages
of ASSUMPTION-FREE K-MC2 is that its proposal adapts to the data set at hand.
As discussed in Section 3  ASSUMPTION-FREE K-MC2 requires an additional preprocessing step
to compute the nonuniform proposal. Figure 2 shows the expected solution quality in relation
to the total computational complexity in terms of number of distance evaluations. Both K-MC2
and ASSUMPTION-FREE K-MC2 generate solutions that are competitive with those produced by
the seeding step of k-means++. At the same time  they do this at a fraction of the computational
cost. Despite the preprocessing  ASSUMPTION-FREE K-MC2 clearly outperforms K-MC2 on the data
sets with large values for ↵(X ) (CSN  KDD and SONG). The additional effort of computing the
nonuniform proposal is compensated by a substantially lower expected quantization error for a given
chain size. For the other data sets  ASSUMPTION-FREE K-MC2 is initially disadvantaged by the cost
of computing the proposal distribution. However  as m increases and more time is spent computing
the Markov chains  it either outperforms K-MC2 (RNA and SUSY) or matches its performance (WEB).
Table 3 details the practical signiﬁcance of the proposed algorithm. The results indicate that in
practice it is sufﬁcient to run ASSUMPTION-FREE K-MC2 with a chain length independent of n.
Even with a small chain length  ASSUMPTION-FREE K-MC2 produces competitive clusterings at
a fraction of the computational cost of the seeding step of k-means++. For example on CSN 
ASSUMPTION-FREE K-MC2 with m = 20 achieves a relative error of 1.45% and a speedup of 33.3⇥.
At the same time  K-MC2 would have exhibited a substantial relative error of 65.34% while only
obtaining a slightly better speedup of 40.0⇥.
5 Conclusion

In this paper  we propose ASSUMPTION-FREE K-MC2  a simple and fast seeding algorithm for
k-Means. In contrast to the previously introduced algorithm K-MC2  it produces provably good
clusterings even without assumptions on the data. As a key advantage  ASSUMPTION-FREE K-MC2
allows to provably trade off solution quality for a decreased computational effort. Extensive experi-
ments illustrate the practical signiﬁcance of the proposed algorithm: It obtains competitive clusterings
at a fraction of the cost of k-means++ seeding and it outperforms or matches its main competitor
K-MC2 on all considered data sets.

Acknowledgments
This research was partially supported by ERC StG 307036  a Google Ph.D. Fellowship and an IBM
Ph.D. Fellowship.

8

References
Acharyya  Sreangsu  Banerjee  Arindam  and Boley  Daniel. Bregman divergences and triangle

inequality. In SIAM International Conference on Data Mining (SDM)  pp. 476–484  2013.

Ackermann  Marcel R and Blömer  Johannes. Bregman clustering for separable instances. In SWAT 

pp. 212–223. Springer  2010.

Aggarwal  Ankit  Deshpande  Amit  and Kannan  Ravi. Adaptive sampling for k-means clustering.
In Approximation  Randomization  and Combinatorial Optimization. Algorithms and Techniques 
pp. 15–28. Springer  2009.

Ailon  Nir  Jaiswal  Ragesh  and Monteleoni  Claire. Streaming k-means approximation. In Neural

Information Processing Systems (NIPS)  pp. 10–18  2009.

Arthur  David and Vassilvitskii  Sergei. k-means++: The advantages of careful seeding. In Symposium
on Discrete Algorithms (SODA)  pp. 1027–1035. Society for Industrial and Applied Mathematics 
2007.

Bachem  Olivier  Lucic  Mario  Hassani  S. Hamed  and Krause  Andreas. Approximate k-means++

in sublinear time. In Conference on Artiﬁcial Intelligence (AAAI)  February 2016.

Bahmani  Bahman  Moseley  Benjamin  Vattani  Andrea  Kumar  Ravi  and Vassilvitskii  Sergei.

Scalable k-means++. Very Large Data Bases (VLDB)  5(7):622–633  2012.

Bottou  Leon and Bengio  Yoshua. Convergence properties of the k-means algorithms. In Neural

Information Processing Systems (NIPS)  pp. 585–592  1994.

Brunsch  Tobias and Röglin  Heiko. A bad instance for k-means++. In Theory and Applications of

Models of Computation  pp. 344–352. Springer  2011.

Cai  Haiyan. Exact bound for the convergence of Metropolis chains. Stochastic Analysis and

Applications  18(1):63–71  2000.

Celebi  M Emre  Kingravi  Hassan A  and Vela  Patricio A. A comparative study of efﬁcient
initialization methods for the k-means clustering algorithm. Expert Systems with Applications  40
(1):200–210  2013.

Hastings  W Keith. Monte Carlo sampling methods using Markov chains and their applications.

Biometrika  57(1):97–109  1970.

Jaiswal  Ragesh  Kumar  Amit  and Sen  Sandeep. A simple D2-sampling based PTAS for k-means

and other clustering problems. Algorithmica  70(1):22–46  2014.

Jaiswal  Ragesh  Kumar  Mehul  and Yadav  Pulkit. Improved analysis of D2-sampling based PTAS
for k-means and other clustering problems. Information Processing Letters  115(2):100–103  2015.
Lloyd  Stuart. Least squares quantization in PCM. IEEE Transactions on Information Theory  28(2):

129–137  1982.

Ostrovsky  Rafail  Rabani  Yuval  Schulman  Leonard J  and Swamy  Chaitanya. The effectiveness of
Lloyd-type methods for the k-means problem. In Foundations of Computer Science (FOCS)  pp.
165–176. IEEE  2006.

Sculley  D. Web-scale k-means clustering. In World Wide Web (WWW)  pp. 1177–1178. ACM  2010.
Zhao  Weizhong  Ma  Huifang  and He  Qing. Parallel k-means clustering based on MapReduce. In

Cloud Computing  pp. 674–679. Springer  2009.

9

,Anqi Liu
Brian Ziebart
Olivier Bachem
Mario Lucic
Hamed Hassani
Andreas Krause
Brian Lubars
Chenhao Tan