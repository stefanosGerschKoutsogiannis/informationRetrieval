2015,Linear Multi-Resource Allocation with Semi-Bandit Feedback,We study an idealised sequential resource allocation problem. In each time step the learner chooses an allocation of several resource types between a number of tasks. Assigning more resources to a task increases the probability that it is completed. The problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy. Our main contribution is the new setting and an algorithm with nearly-optimal regret analysis. Along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise. We also present some new results for stochastic linear bandits on the hypercube that significantly out-performs existing work  especially in the sparse case.,Linear Multi-Resource Allocation with Semi-Bandit

Feedback

Tor Lattimore

Department of Computing Science

University of Alberta  Canada

tor.lattimore@gmail.com

Koby Crammer

Department of Electrical Engineering

The Technion  Israel

koby@ee.technion.ac.il

Csaba Szepesv´ari

Department of Computing Science

University of Alberta  Canada
szepesva@ualberta.ca

Abstract

We study an idealised sequential resource allocation problem. In each time step
the learner chooses an allocation of several resource types between a number of
tasks. Assigning more resources to a task increases the probability that it is com-
pleted. The problem is challenging because the alignment of the tasks to the re-
source types is unknown and the feedback is noisy. Our main contribution is the
new setting and an algorithm with nearly-optimal regret analysis. Along the way
we draw connections to the problem of minimising regret for stochastic linear
bandits with heteroscedastic noise. We also present some new results for stochas-
tic linear bandits on the hypercube that signiﬁcantly improve on existing work 
especially in the sparse case.

1

Introduction

Economist Thomas Sowell remarked that “The ﬁrst lesson of economics is scarcity: There is never
enough of anything to fully satisfy all those who want it.”1 The optimal allocation of resources is
an enduring problem in economics  operations research and daily life. The problem is challenging
not only because you are compelled to make difﬁcult trade-offs  but also because the (expected)
outcome of a particular allocation may be unknown and the feedback noisy.
We focus on an idealised resource allocation problem where the economist plays a repeated resource
allocation game with multiple resource types and multiple tasks to which these resources can be
assigned. Speciﬁcally  we consider a (nearly) linear model with D resources and K tasks. In each
time step t the economist chooses an allocation of resources Mt ∈ RD×K where Mtk ∈ RD is the
kth column and represents the amount of each resource type assigned to the kth task. We assume
that the kth task is completed successfully with probability min{1 (cid:104)Mtk  νk(cid:105)} and νk ∈ RD is an
unknown non-negative vector that determines how the success rate of a given task depends on the
quantity and type of resources assigned to it. Naturally we will limit the availability of resources
k=1 Mtdk ≤ 1 for all resource types d. At the end of each time
step the economist observes which tasks were successful. The objective is to maximise the number
of successful tasks up to some time horizon n that is known in advance. This model is a natural
generalisation of the one used by Lattimore et al. [2014]  where it was assumed that there was a
single resource type only.

by demanding that Mt satisﬁes(cid:80)K

1He went on to add that “The ﬁrst lesson of politics is to disregard the ﬁrst lesson of economics.” Sowell

[1993]

1

An example application might be the problem of allocating computing resources on a server between
a number of Virtual Private Servers (VPS). In each time step (some ﬁxed interval) the controller
chooses how much memory/cpu/bandwidth to allocate to each VPS. A VPS is said to fail in a given
round if it fails to respond to requests in a timely fashion. The requirements of each VPS are
unknown in advance  but do not change greatly with time. The controller should learn which VPS
beneﬁt the most from which resource types and allocate accordingly.
The main contribution of this paper besides the new setting is an algorithm designed for this problem
along with theoretical guarantees on its performance in terms of the regret. Along the way we present
some additional results for the related problem of minimising regret for stochastic linear bandits on
the hypercube. We also prove new concentration results for weighted least squares estimation  which
may be independently interesting.
The generalisation of the work of Lattimore et al. [2014] to multiple resources turns out to be fairly
non-trivial. Those with knowledge of the theory of stochastic linear bandits will recognise some
similarity. In particular  once the nonlinearity of the objective is removed  the problem is equivalent
to playing K linear bandits in parallel  but where the limited resources constrain the actions of the
learner and correspondingly the returns for each task. Stochastic linear bandits have recently been
generating a signiﬁcant body of research (e.g.  Auer [2003]  Dani et al. [2008]  Rusmevichientong
and Tsitsiklis [2010]  Abbasi-Yadkori et al. [2011  2012]  Agrawal and Goyal [2012] and many oth-
ers). A related problem is that of online combinatorial optimisation. This has an extensive literature 
but most results are only applicable for discrete action sets  are in the adversarial setting  and can-
not exploit the additional structure of our problem. Nevertheless  we refer the interested reader to
(say) the recent work by Kveton et al. [2014] and references there-in. Also worth mentioning is that
the resource allocation problem at hand is quite different to the “linear semi-bandit” proposed and
analysed by Krishnamurthy et al. [2015] where the action set is also ﬁnite (the setting is different in
many other ways besides).
Given its similarity  it is tempting to apply the techniques of linear bandits to our problem. When
doing so  two main difﬁculties arise. The ﬁrst is that our payoffs are non-linear:
the expected
reward is a linear function only up to a point after which it is clipped. In the resource allocation
problem this has a natural interpretation  which is that over-allocating resources beyond a certain
point is fruitless. Fortunately  one can avoid this difﬁculty rather easily by ensuring that with high
probability resources are never over-allocated. The second problem concerns achieving good regret
regardless of the task speciﬁcs. In particular  when the number of tasks K is large and resources are
at a premium the allocation problem behaves more like a K-armed bandit where the economist must
choose the few tasks that can be completed successfully. For this kind of problem regret should scale
in the worst case with
K only [Auer et al.  2002  Bubeck and Cesa-Bianchi  2012]. The standard
linear bandits approach  on the other hand  would lead to a bound on the regret that depends linearly
on K. To remedy this situation  we will exploit that if K is large and resources are scarce  then
many tasks will necessarily be under-resourced and will fail with high probability. Since the noise
model is Bernoulli  the variance of the noise for these tasks is extremely low. By using weighted
least-squares estimators we are able to exploit this and thereby obtain an improved regret. An added
beneﬁt is that when resources are plentiful  then all tasks will succeed with high probability under
the optimal allocation  and in this case the variance is also low. This leads to a poly-logarithmic
regret for the resource-laden case where the optimal allocation fully allocates every task.

√

2 Preliminaries
If F is some event  then ¬F is its complement (i.e.  it is the event that F does not occur). If A is
positive deﬁnite and x is a vector  then (cid:107)x(cid:107)2
A = x(cid:62)Ax stands for the weighted 2-norm. We write |x|
to be the vector of element-wise absolute values of x. We let ν ∈ RD×K be a matrix with columns
ν1  . . . νK. All entries in ν are non-negative  but otherwise we make no global assumptions on ν. At
each time step t the learner chooses an allocation matrix Mt ∈ M where
Mdk ≤ 1 for all d

M ∈ [0  1]D×K :

K(cid:88)

(cid:41)

.

(cid:40)

M =

The assumption that each resource type has a bound of 1 is non-restrictive  since the units of any
resource can be changed to accommodate this assumption. We write Mtk ∈ [0  1]D for the kth

k=1

2

column of Mt. The reward at time step t is (cid:107)Yt(cid:107)1 where Ytk ∈ {0  1} is sampled from a Bernoulli
distribution with parameter ψ((cid:104)Mtk  νk(cid:105)) = min{1 (cid:104)Mtk  νk(cid:105)}. The economist observes all Ytk 
however  not just the sum. The optimal allocation is denoted by M∗ and deﬁned by

M∗ = arg max
M∈M

ψ((cid:104)Mk  νk(cid:105)) .

K(cid:88)
(cid:34) n(cid:88)

k=1

We are primarily concerned with designing an allocation algorithm that minimises the expected
(pseudo) regret of this problem  which is deﬁned by
k   νk(cid:105)) − E

ψ((cid:104)Mtk  νk(cid:105))

K(cid:88)

K(cid:88)

ψ((cid:104)M∗

Rn = n

 

(cid:35)

where the expectation is taken over both the actions of the algorithm and the observed reward.

k=1

t=1

k=1

Optimal Allocations

If ν is known  then the optimal allocation can be computed by constructing an appropriate linear
program. Somewhat surprisingly it may also be computed exactly in O(K log K + D log D) time
using Algorithm 1 below. The optimal allocation is not so straight-forward as  e.g.  simply allocating
resources to the incomplete task for which the corresponding ν is largest in some dimension. For
example  for K = 2 tasks and d = 2 resource types:

(cid:18)

(cid:19)

(cid:18) 0

(cid:19)

=⇒ M∗ =

M∗

1 M∗

2

=

1

.

1/2

1/2

(cid:18)

(cid:19)

(cid:18) 0

1/2

(cid:19)

1/2
1

ν =

ν1 ν2

=

Algorithm 1

A = {k : (cid:104)Mk  νk(cid:105) < 1} and B = {d : Bd > 0}

Input: ν
M = 0 ∈ RD×K and B = 1 ∈ RD
while ∃ k  d s.t (cid:104)Mk  νk(cid:105) < 1 and Bd > 0 do

We see that even though ν22 is the largest param-
eter  the optimal allocation assigns only half of the
second resource (d = 2) to this task. The right ap-
proach is to allocate resources to incomplete tasks
using the ratios as prescribed by Algorithm 1. The
intuition for allocating in this way is that resources
should be allocated as efﬁciently as possible  and ef-
ﬁciency is determined by the ratio of the expected
success due to the allocation of a resource and the
amount of resources allocated.
Theorem 1. Algorithm 1 returns M∗.
The proof of Theorem 1 and an implementation of Algorithm 1 may be found in the supplementary
material.
We are interested primarily in the case when ν is unknown  so Algorithm 1 will not be directly
applicable. Nevertheless  the algorithm is useful as a module in the implementation of a subsequent
algorithm that estimates ν from data.

(cid:18) νdk
(cid:19)
(cid:27)

i∈A\{k}
νdi
1 − (cid:104)Mk  νk(cid:105)

k  d = arg max
(k d)∈A×B

end while
return M

Mdk = min

(cid:26)

Bd 

min

νdk

3 Optimistic Allocation Algorithm

We follow the optimism in the face of uncertainty principle.
In each time step t  the algorithm
constructs an estimator ˆνkt for each νk and a corresponding conﬁdence set Ctk for which νk ∈ Ctk
holds with high probability. The algorithm then takes the optimistic action subject to the assumption
that νk does indeed lie in Ctk for all k. The main difﬁculty is the construction of the conﬁdence sets.
Like other authors [Dani et al.  2008  Rusmevichientong and Tsitsiklis  2010  Abbasi-Yadkori et al. 
2011] we deﬁne our conﬁdence sets to be ellipses  but the use of a weighted least-squares estimator
means that our ellipses may be signiﬁcantly smaller than the sets that would be available by using
these previous works in a straightforward way. The algorithm accepts as input the number of tasks
and resource types  the horizon and constants α > 0 and β where constant β is deﬁned by

N =(cid:0)4n4D2(cid:1)D
(cid:18) 6nN

(cid:115)

 

αB + 2

log

δ =

 

1
nK

(cid:32)

β =

1 +

√

(cid:107)νk(cid:107)2
2  

B ≥ max

(cid:18) 3nN

(cid:19)(cid:19)(cid:33)2

k

so that

.

(1)

δ

δ

log

3

Note that B must be a known bound on maxk (cid:107)νk(cid:107)2
2  which might seem like a serious restriction 
until one realizes that it is easy to add an initialisation phase where estimates are quickly made
while incurring minimal additional regret  as was also done by Lattimore et al. [2014]. The value
of α determines the level of regularisation in the least squares estimation and will be tuned later to
optimise the regret.

Algorithm 2 Optimistic Allocation Algorithm
1: Input K  D  n  α  β
2: for t ∈ 1  . . .   n do
3:

// Compute conﬁdence sets for all tasks k:

4: Gtk = αI +(cid:80)
(cid:80)

ˆνtk = G−1

5:

tk

(cid:110)

6:

τ <t γτ kMτ kM(cid:62)
τ <t γτ kMτ Yτ k

τ k

˜νk : (cid:107)˜νk − ˆνtk(cid:107)2

≤ β
Ctk =
// Compute optimistic allocation:

Gtk

7:
8: Mt = arg maxMt∈M max˜νk∈Ctk ψ((cid:104)Mtk  ˜νk(cid:105))
9:
10:

// Observe success indicators Ytk for all tasks k:
Ytk ∼ Bernoulli(ψ((cid:104)Mtk  νk(cid:105)))
// Compute weights for all tasks k:
11:
γ−1
12:
tk = arg max˜νk∈C(cid:48)
13: end for

(cid:104)Mtk  ˜νk(cid:105) (1 − (cid:104)Mtk  ˜νk(cid:105))

tk

(cid:111)

(cid:110)

˜νk : (cid:107)˜νk − ˆνtk(cid:107)2

Gtk

(cid:111)

≤ 4β

and C(cid:48)

tk =

Computational Efﬁciency

We could not ﬁnd an efﬁcient implementation of Algorithm 2 because solving the bilinear optimi-
sation problem in Line 8 is likely to be NP-hard (Bennett and Mangasarian [1993] and also Petrik
and Zilberstein [2011]). In our experiments we used a simple algorithm based on optimising for M
and ν in alternative steps combined with random restarts  but for large D and K this would likely
not be efﬁcient. In the supplementary material we present an alternative algorithm that is efﬁcient 
but relies on the assumption that (cid:107)νk(cid:107)1 ≤ 1 for all k. In this regime it is impossible to over-allocate
resources and this fact can be exploited to obtain an efﬁcient and practical algorithm with strong
guarantees. Along the way  we are able to construct an elegant algorithm for linear bandits on the
hypercube that enjoys optimal regret and adapts to sparsity.
Computing the weights γtk (Line 12) is (somewhat surprisingly) straight-forward. Deﬁne

ptk = (cid:104)Mtk  ˆνtk(cid:105) − 2(cid:112)β (cid:107)Mtk(cid:107)G

¯ptk = (cid:104)Mtk  ˆνtk(cid:105) + 2(cid:112)β (cid:107)Mtk(cid:107)G

Then the weights can be computed by

γ−1
tk =

and

−1
tk

¯ptk(1 − ¯ptk)

ptk(1 − ptk)

1
4

if ¯ptk ≤ 1
if ptk ≥ 1
2
otherwise .

2

.

−1
tk

(2)

A curious reader might wonder why the weights are computed by optimising within conﬁdence set
C(cid:48)
tk  which has double the radius of Ctk. The reason is rather technical  but essentially if the true
parameter νk were to lie on the boundary of the conﬁdence set  then the corresponding weight could
become inﬁnite. For the analysis to work we rely on controlling the size of the weights. It is not
clear whether or not this trick is really necessary.

4 Worst-case Regret for Algorithm 2

√
We now analyse the regret of Algorithm 2. First we offer a worst-case bound on the regret that
n). We then turn our attention to the resource-laden case
depends on the time-horizon like O(
where the optimal allocation satisﬁes (cid:104)M∗
k   νk(cid:105) = 1 for all k. In this instance we show that the
dependence on the horizon is only poly-logarithmic  which would normally be unexpected when the

4

action-space is continuous. The improvement comes from the weighted estimation that exploits the
fact that the variance of the noise under the optimal allocation vanishes.
Theorem 2. Suppose Algorithm 2 is run with bound B ≥ maxk (cid:107)νk(cid:107)2

Rn ≤ 1 + 4D

Choosing α = B−1 log(cid:0) 6nN

δ

2βnK

(cid:115)
log(cid:0) 3nN
(cid:18)

δ

Rn ∈ O

k

max

(cid:19)

2. Then

(cid:107)νk(cid:107)∞ + 4(cid:112)β/α

(cid:18)
(cid:1)(cid:1) and assuming that B ∈ O(maxk (cid:107)νk(cid:107)2
D3/2(cid:113)

(cid:19)
(cid:107)νk(cid:107)2 log n

nK max

log(1 + 4n2) .

.

k

2)  then

The proof of Theorem 2 will follow by carefully analysing the width of the conﬁdence sets as the
algorithm makes allocations. We start by proving the validity of the conﬁdence sets  and then prove
the theorem.

Weighted Least Squares Estimation

For this sub-section we focus on the problem of estimating a single unknown ν = νk. Let
M1  . . .   Mn be a sequence of allocations to task k with Mt ∈ RD. Let {Ft}n
t=0 be a ﬁltration
with Ft containing information available at the end of round t  which means that Mt is Ft−1-
measurable. Let γ1  . . .   γn be the sequence of weights chosen by Algorithm 2. The sequence of
outcomes is Y1  . . .   Yn ∈ {0  1} for which E[Yt|Ft−1] = ψ((cid:104)Mt  ν(cid:105)). The weighted regularised
τ and the corresponding weighted least squares estimator

gram matrix is Gt = αI +(cid:80)

τ <t γτ Mτ M(cid:62)

is

(cid:88)

ˆνt = G−1

t

γtMτ Yτ .

τ <t

Theorem 3. If (cid:107)ν(cid:107)2
probability at least 1 − δ = 1/(nK).

2 ≤ B and β is chosen as in Eq. (1)  then (cid:107)ˆνt − ν(cid:107)2

Gt

≤ β for all t ≤ n with

Similar results exist in the literature for unweighted least-squares estimators (for example  Dani
et al. [2008]  Rusmevichientong and Tsitsiklis [2010]  Abbasi-Yadkori et al. [2011]). In our case 
however  Gt is the weighted gram matrix  which may be signiﬁcantly larger than an unweighted
version when the weights become large. The proof of Theorem 3 is unfortunately too long to include
in the main text  but it may be found in the supplementary material.

Analysing the Regret
We start with some technical lemmas. Let F be the failure event that (cid:107)ˆνtk − νk(cid:107)2
t ≤ n and 1 ≤ k ≤ K.
Lemma 4 (Abbasi-Yadkori et al. [2012]). Let x1  . . .   xn be an arbitrary sequence of vectors with
(cid:107)xt(cid:107)2

2 ≤ c and let Gt = I +(cid:80)t−1

> β for some

1 (cid:107)xt(cid:107)2

t=1 min

(cid:1).

(cid:110)

Gtk

(cid:111) ≤ 2D log(cid:0)1 + c·n
(cid:111) ≤ 8D log(1 + 4n2).

−1
t

D

G

s . Then(cid:80)n
s=1 xsx(cid:62)
(cid:110)
n(cid:88)

γtk min

t=1

Corollary 5. If F does not hold  then

1 (cid:107)Mtk(cid:107)2

−1
tk

G

The proof is omitted  but follows rather easily by showing that γtk can be moved inside the minimum
at a price of increasing the loss at most by a factor of four  and then applying Lemma 4. See the
supplementary material for the formal proof.

Lemma 6. Suppose F does not hold  then

(cid:18)

(cid:107)νk(cid:107)∞ + 4(cid:112)β/α

(cid:19)

.

tk ≤ D
γ−1

max

k

K(cid:88)

k=1

5

Proof. We exploit the fact that γ−1
is small:

tk is an estimate of the variance  which is small whenever (cid:107)Mtk(cid:107)1

tk

˜νk∈C(cid:48)

(cid:104)Mtk  ˜νk(cid:105)

γ−1
tk = arg max

(cid:104)Mtk  ˜νk(cid:105) (1 − (cid:104)Mtk  ˜νk(cid:105)) ≤ arg max
˜νk∈C(cid:48)

= (cid:104)Mtk  ν(cid:105) + arg max
˜νk∈Ctk(cid:48)

(cid:104)Mtk  ˜νk − ν(cid:105) (a)≤ (cid:107)Mtk(cid:107)1 (cid:107)νk(cid:107)∞ + 4(cid:112)β (cid:107)Mtk(cid:107)G
(cid:17)
(cid:16)(cid:107)νk(cid:107)∞ + 4(cid:112)β/α
tk ≤ I/α and basic

(b)≤ (cid:107)Mtk(cid:107)1 (cid:107)νk(cid:107)∞ + 4(cid:112)β (cid:107)Mtk(cid:107)I/α
linear algebra  (c) since (cid:107)Mtk(cid:107)I/α = (cid:112)1/α(cid:107)Mtk(cid:107)2 ≤ (cid:112)1/α(cid:107)Mtk(cid:107)1. The result is completed
since the resource constraints implies that(cid:80)K

where (a) follows from Cauchy-Schwartz and the fact that νk ∈ C(cid:48)

tk  (b) since G−1

(c)≤ (cid:107)Mtk(cid:107)1

−1
tk

tk

 

k=1 (cid:107)Mtk(cid:107)1 ≤ D.
(cid:34)
1{¬F} n(cid:88)

Proof of Theorem 2. By Theorem 3 we have that F holds with probability at most δ = 1/(nK).
If F does not hold  then by the deﬁnition of the conﬁdence set we have νk ∈ Ctk for all t and k.
Therefore
Rn = E

(cid:35)
k − Mtk  νk(cid:105)

k   νk(cid:105) − ψ((cid:104)Mtk  νk(cid:105))) ≤ 1 + E

n(cid:88)

K(cid:88)

K(cid:88)

((cid:104)M∗

(cid:104)M∗

.

t=1

k=1

Note that we were able to replace ψ((cid:104)Mtk  νk(cid:105)) = (cid:104)Mtk  νk(cid:105)  since if F does not hold  then Mtk
will never be chosen in such a way that resources are over-allocated. We will now assume that F
does not hold and bound the argument in the expectation. By the optimism principle we have:

t=1

k=1

n(cid:88)

K(cid:88)

t=1

k=1

(cid:104)M∗

(cid:111)

(cid:107)˜νtk − νk(cid:107)Gtk

min

t=1

t=1

t=1

k=1

k=1

k=1

min

(c)≤ 2

K(cid:88)
k − Mtk  νk(cid:105) (a)≤ n(cid:88)
K(cid:88)
(b)≤ n(cid:88)
n(cid:88)
K(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)n
n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)n
n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)nD
(cid:32)
(cid:118)(cid:117)(cid:117)(cid:116)2βnK

(g)≤ 4D

(f )≤ 2

(d)≤ 2

(e)≤ 2

t=1

t=1

β

β

max

k

(cid:32)

min{1 (cid:104)Mtk  ˜νtk − νk(cid:105)}

(cid:111)

−1
tk

−1
tk

(cid:112)β

(cid:110)
1 (cid:107)Mtk(cid:107)G
(cid:110)
1 (cid:107)Mtk(cid:107)G
(cid:32) K(cid:88)
(cid:110)
1 (cid:107)Mtk(cid:107)G
(cid:33)(cid:32) K(cid:88)
(cid:32) K(cid:88)
(cid:114)

γ−1

min

k=1

k=1

k=1

tk

(cid:107)νk(cid:107)∞ + 4

(cid:107)νk(cid:107)∞ + 4

max

k

β
α

−1
tk

(cid:111)(cid:33)2
(cid:110)
(cid:32) K(cid:88)

β

γtk min

(cid:33) n(cid:88)
(cid:33)
(cid:114)

t=1

β
α

1 (cid:107)Mtk(cid:107)2

−1
tk

G

γtk min

k=1

log(1 + 4n2) .

(cid:111)(cid:33)
(cid:110)

1 (cid:107)Mtk(cid:107)2

G

(cid:111)(cid:33)

−1
tk

where (a) follows from the assumption that νk ∈ Ctk for all t and k and since Mt is chosen opti-
mistically  (b) by the Cauchy-Schwarz inequality  (c) by the deﬁnition of ˜νkt  which lies inside Ctk 
(d) by Jensen’s inequality  (e) by Cauchy-Schwarz again  (f) follows from Lemma 6. Finally (g)
follows from Corollary 5.

5 Regret in Resource-Laden Case

√
We now show that if there are enough resources such that the optimal strategy can complete every
task with certainty  then the regret of Algorithm 2 is poly-logarithmic (in contrast to O(
n) other-
wise). As before we exploit the low variance  but now the variance is small because (cid:104)Mtk  νk(cid:105) is

6

Theorem 7. If(cid:80)K

close to 1  while in the previous section we argued that this could not happen too often (there is no
contradiction as the quantity maxk (cid:107)νk(cid:107) appeared in the previous bound).
k   νk(cid:105) = K  then Rn ≤ 1 + 8βKD log(1 + 4n2).

k=1 (cid:104)M∗

(1 − (cid:104)Mtk  ν(cid:105))
(cid:107)¯ν − ν(cid:107)Gtk

max
¯ν ν∈C(cid:48)

tk

4(cid:112)β .

−1
tk

Applying the optimism principle and using the bound above combined with Corollary 5 gives the
result:

tk

tk

tk

−1
tk

γ−1
tk = max
ν∈C(cid:48)
≤ max
¯ν ν∈C(cid:48)

Proof. We start by showing that the weights are large:
(cid:104)Mtk  ν(cid:105) (1 − (cid:104)Mtk  ν(cid:105)) ≤ max
ν∈C(cid:48)
(cid:104)Mtk  ¯ν − ν(cid:105) ≤ (cid:107)Mtk(cid:107)G
(cid:34)
1{¬F} n(cid:88)
K(cid:88)
(cid:34)
K(cid:88)
1{¬F} n(cid:88)
(cid:34)
K(cid:88)
1{¬F} n(cid:88)
(cid:34)
K(cid:88)
1{¬F} n(cid:88)

ERn ≤ 1 + E

≤ 1 + 2E

= 1 + 2E

min

min

t=1

k=1

t=1

k=1

t=1

k=1

≤ 1 + 8β E
≤ 1 + 8βKD log(1 + 4n2) .

t=1

k=1

≤ (cid:107)Mtk(cid:107)G
(cid:35)
(cid:111)(cid:35)

(cid:35)

(cid:111)(cid:112)β
(cid:111)(cid:35)

−1
tk

min{1 (cid:104)Mtk  ˜νkt − νk(cid:105)}

(cid:112)β

−1
tk

(cid:110)
1 (cid:107)Mtk(cid:107)G
(cid:110)

1  γ−1

tk γtk (cid:107)Mtk(cid:107)G

min

1  γtk (cid:107)Mtk(cid:107)2

−1
tk

G

(cid:110)

6 Experiments

We present two experiments to demonstrate the behaviour of Algorithm 2. All code and data is
available in the supplementary material. Error bars indicate 95% conﬁdence intervals  but sometimes
they are too small to see (the algorithm is quite conservative  so the variance is very low). We used
B = 10 for all experiments. The ﬁrst experiment demonstrates the improvements obtained by
using a weighted estimator over an unweighted one  and also serves to give some idea of the rate of
learning. For this experiment we used D = K = 2 and n = 106 and

(cid:18) 1

(cid:19)

K(cid:88)

k=1

=⇒ M∗ =

0

1/2

1/2

and

(cid:104)M∗

k   νk(cid:105) = 2  

(cid:18)

(cid:19)

(cid:18)8/10 2/10

(cid:19)

4/10

2

ν =

ν1

ν2

=

where the kth column is the parameter/allocation for the kth task. We ran two versions of the
algorithm. The ﬁrst  exactly as given in Algorithm 2 and the second identical except that the weights
were ﬁxed to γtk = 4 for all t and k (this value is chosen because it corresponds to the minimum
inverse variance for a Bernoulli variable). The data was produced by taking the average regret over
8 runs. The results are given in Fig. 1. In Fig. 2 we plot γtk. The results show that γtk is increasing
linearly with t. This is congruent with what we might expect because in this regime the estimation
error should drop with O(1/t) and the estimated variance is proportional to the estimation error.

Note that the estimation error for the algorithm with γtk = 4 will be O((cid:112)1/t).

For the second experiment we show the algorithm adapting to the environment. We ﬁx n = 5 × 105
and D = K = 2. For α ∈ (0  1) we deﬁne

(cid:18)1/2 α/2

(cid:19)

1/2 α/2

να =

(cid:18)1

1

(cid:19)

0
0

K(cid:88)

k=1

=⇒ M∗ =

and

(cid:104)M∗

k   νk(cid:105) = 1 .

The unusual proﬁle of the regret as α varies can be attributed to two factors. First  if α is small then
the algorithm quickly identiﬁes that resources should be allocated ﬁrst to the ﬁrst task. However  in
the early stages of learning the algorithm is conservative in allocating to the ﬁrst task to avoid over-
allocation. Since the remaining resources are given to the second task  the regret is larger for small

7

α because the gain from allocating to the second task is small. On the other hand  if α is close to 1 
then the algorithm suffers the opposite problem. Namely  it cannot identify which task the resources
should be assigned to. Of course  if α = 1  then the algorithm must simply learn that all resources
can be allocated safely and so the regret is smallest here. An important point is that the algorithm
never allocates all its resources at the start of the process because this risks over-allocation  so even
in “easy” problems the regret will not vanish.

Figure 1: Weighted vs unweighted estimation

Figure 2: Weights

Figure 3: “Gap” dependence

80 000

60 000

40 000

20 000

t
e
r
g
e
R

0

0

40

γ

20

0

Weighted Estimator
Unweighted Estimator

30 000

t
e
r
g
e
R

20 000

10 000

γt1
γt2

1 000 000

0

1 000 000

t

t

0

0.0

0.5
α

1.0

7 Conclusions and Summary

We introduced the stochastic multi-resource allocation problem and developed a new algorithm that
enjoys near-optimal worst-case regret. The main drawback of the new algorithm is that its com-
putation time is exponential in the dimension parameters  which makes practical implementations
challenging unless both K and D are relatively small. Despite this challenge we were able to im-
plement that algorithm using a relatively brutish approach to solving the optimisation problem  and
this was sufﬁcient to present experimental results on synthetic data showing that the algorithm is
behaving as the theory predicts  and that the use of the weighted least-squares estimation is leading
to a real improvement.
Despite the computational issues  we think this is a reasonable ﬁrst step towards a more practical al-
gorithm as well as a solid theoretical understanding of the structure of the problem. As a consolation
(and on their own merits) we include some other results:

allocation is impossible.

• An efﬁcient (both in terms of regret and computation) algorithm for the case where over-
• An algorithm for linear bandits on the hypercube that enjoys optimal regret bounds and
• Theoretical analysis of weighted least-squares estimators  which may have other applica-

adapts to sparsity.

tions (e.g.  linear bandits with heteroscedastic noise).

There are many directions for future research. The most natural is to improve the practicality of the
algorithm. We envisage such an algorithm might be obtained by following the program below:

• Generalise the Thompson sampling analysis for linear bandits by Agrawal and Goyal
[2012]. This is a highly non-trivial step  since it is no longer straight-forward to show
that such an algorithm is optimistic with high probability. Instead it will be necessary to
make do with some kind of local optimism for each task.
• The method of estimation depends heavily on the algorithm over-allocating its resources
only with extremely low probability  but this signiﬁcantly slows learning in the initial
phases when the conﬁdence sets are large and the algorithm is acting conservatively. Ideally
we would use a method of estimation that depended on the real structure of the problem 
but existing techniques that might lead to theoretical guarantees (e.g.  empirical process
theory) do not seem promising if small constants are expected.

It is not hard to think up extensions or modiﬁcations to the setting. For example  it would be
interesting to look at an adversarial setting (even deﬁning it is not so easy)  or move towards a
non-parametric model for the likelihood of success given an allocation.

8

References
Yasin Abbasi-Yadkori  Csaba Szepesv´ari  and David Tax. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems  pages 2312–2320  2011.

Yasin Abbasi-Yadkori  David Pal  and Csaba Szepesvari. Online-to-conﬁdence-set conversions and

application to sparse stochastic bandits. In AISTATS  volume 22  pages 1–9  2012.

Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.

arXiv preprint arXiv:1209.3352  2012.

Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Ma-

chine Learning Research  3:397–422  2003.

Peter Auer  Nicol´o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine Learning  47:235–256  2002.

Kristin P Bennett and Olvi L Mangasarian. Bilinear separation of two sets inn-space. Computational

Optimization and Applications  2(3):207–227  1993.

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-
armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorpo-
rated  2012. ISBN 9781601986269.

Varsha Dani  Thomas P Hayes  and Sham M Kakade. Stochastic linear optimization under bandit

feedback. In COLT  pages 355–366  2008.

Akshay Krishnamurthy  Alekh Agarwal  and Miroslav Dudik. Efﬁcient contextual semi-bandit

learning. arXiv preprint arXiv:1502.05890  2015.

Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvari. Tight regret bounds for stochas-

tic combinatorial semi-bandits. arXiv preprint arXiv:1410.0949  2014.

Tor Lattimore  Koby Crammer  and Csaba Szepesv´ari. Optimal resource allocation with semi-bandit
feedback. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 
2014.

Marek Petrik and Shlomo Zilberstein. Robust approximate bilinear programming for value function

approximation. The Journal of Machine Learning Research  12:3027–3063  2011.

Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research  35(2):395–411  2010.

Thomas Sowell. Is Reality Optional?: And Other Essays. Hoover Institution Press  1993.

9

,Tor Lattimore
Koby Crammer
Csaba Szepesvari
Alhussein Fawzi
Hamza Fawzi
Omar Fawzi