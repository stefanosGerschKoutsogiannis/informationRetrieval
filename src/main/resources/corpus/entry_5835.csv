2011,Variance Penalizing AdaBoost,This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds.  VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus  the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).,Variance Penalizing AdaBoost

Pannagadatta K. Shivaswamy
Department of Computer Science

Cornell University  Ithaca NY
pannaga@cs.cornell.edu

Department of Compter Science

Columbia University  New York NY

Tony Jebara

jebara@cs.columbia.edu

Abstract

This paper proposes a novel boosting algorithm called VadaBoost which is mo-
tivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a
cost function that balances the sample mean and the sample variance of the expo-
nential loss. Each step of the proposed algorithm minimizes the cost efﬁciently
by providing weighted data to a weak learner rather than requiring a brute force
evaluation of all possible weak learners. Thus  the proposed algorithm solves a
key limitation of previous empirical Bernstein boosting methods which required
brute force enumeration of all possible weak learners. Experimental results con-
ﬁrm that the new algorithm achieves the performance improvements of EBBoost
yet goes beyond decision stumps to handle any weak learner. Signiﬁcant perfor-
mance gains are obtained over AdaBoost for arbitrary weak learners including
decision trees (CART).

1

Introduction

Many machine learning algorithms implement empirical risk minimization or a regularized variant
of it. For example  the popular AdaBoost [4] algorithm minimizes exponential loss on the training
examples. Similarly  the support vector machine [11] minimizes hinge loss on the training examples.
The convexity of these losses is helpful for computational as well as generalization reasons [2].
The goal of most learning problems  however  is not to obtain a function that performs well on
training data  but rather to estimate a function (using training data) that performs well on future
unseen test data. Therefore  empirical risk minimization on the training set is often performed
while regularizing the complexity of the function classes being explored. The rationale behind this
regularization approach is that it ensures that the empirical risk converges (uniformly) to the true
unknown risk. Various concentration inequalities formalize the rate of convergence in terms of the
function class complexity and the number of samples.
A key tool in obtaining such concentration inequalities is Hoeffding’s inequality which relates the
empirical mean of a bounded random variable to its true mean. Bernstein’s and Bennett’s inequalities
relate the true mean of a random variable to the empirical mean but also incorporate the true variance
of the random variable. If the true variance of a random variable is small  these bounds can be
signiﬁcantly tighter than Hoeffding’s bound. Recently  there have been empirical counterparts of
Bernstein’s inequality [1  5]; these bounds incorporate the empirical variance of a random variable
rather than its true variance. The advantage of these bounds is that the quantities they involve are
empirical. Previously  these bounds have been applied in sampling procedures [6] and in multi-
armed bandit problems [1]. An alternative to empirical risk minimization  called sample variance
penalization [5]  has been proposed and is motivated by empirical Bernstein bounds.
A new boosting algorithm is proposed in this paper which implements sample variance penalization.
The algorithm minimizes the empirical risk on the training set as well as the empirical variance. The
two quantities (the risk and the variance) are traded-off through a scalar parameter. Moreover  the

1

algorithm proposed in this article does not require exhaustive enumeration of the weak learners
(unlike an earlier algorithm by [10]).
i=1 is provided where Xi ∈ X and yi ∈ {±1} are drawn inde-
Assume that a training set (Xi  yi)n
pendently and identically distributed (iid) from a ﬁxed but unknown distribution D. The goal is to
learn a classiﬁer or a function f : X → {±1} that performs well on test examples drawn from the
same distribution D. In the rest of this article  G : X → {±1} denotes the so-called weak learner.
The notation Gs denotes the weak learner in a particular iteration s. Further  the two indices sets Is
and Js  respectively  denote examples that the weak learner Gs correctly classiﬁed and misclassiﬁed 
i.e.  Is := {i|Gs(Xi) = yi} and Js := {j|Gs(Xj) (cid:54)= yj}.

Algorithm 1 AdaBoost Require: (Xi  yi)n

i=1  and weak learners H

Initialize the weights: wi ← 1/n for i = 1  . . .   n; Initialize f to predict zero on all inputs.
for s ← 1 to S do

wi /(cid:80)

2 log

Estimate a weak learner Gs(·) from training examples weighted by (wi)n
αs = 1
if αs ≤ 0 then break end if
f (·) ← f (·) + αsGs(·)

wi ← wi exp(−yiGs(Xi)αs)/Zs where Zs is such that(cid:80)n

j:Gs(Xj )(cid:54)=yj

i:Gs(Xi)=yi

i=1 wi = 1.

wj

i=1.

(cid:16)(cid:80)

(cid:17)

end for

Algorithm 2 VadaBoost Require: (Xi  yi)n

i=1  scalar parameter 1 ≥ λ ≥ 0  and weak learners H

Initialize the weights: wi ← 1/n for i = 1  . . .   n; Initialize f to predict zero on all inputs.
for s ← 1 to S do

(cid:16)(cid:80)

i + (1 − λ)wi

ui ← λnw2
Estimate a weak learner Gs(·) from training examples weighted by (ui)n
αs = 1
if αs ≤ 0 then break end if
f (·) ← f (·) + αsGs(·)

wi ← wi exp(−yiGs(Xi)αs)/Zs where Zs is such that(cid:80)n

ui /(cid:80)

j:Gs(Xj )(cid:54)=yj

i:Gs(Xi)=yi

4 log

(cid:17)

i=1 wi = 1.

uj

i=1.

end for

2 Algorithms

AdaBoost iteratively builds(cid:80)S
tion: minf∈F(cid:0) 1

In this section  we brieﬂy discuss AdaBoost [4] and then propose a new algorithm called the Vad-
aBoost. The derivation of VadaBoost will be provided in detail in the next section.
AdaBoost (Algorithm 1) assigns a weight wi to each training example. In each step of the AdaBoost 
a weak learner Gs(·) is obtained on the weighted examples and a weight αs is assigned to it. Thus 
s=1 αsGs(·). If a training example is correctly classiﬁed  its weight
is exponentially decreased; if it is misclassiﬁed  its weight is exponentially increased. The process
is repeated until a stopping criterion is met. AdaBoost essentially performs empirical risk minimiza-
s=1 αsGs(·).
Recently an alternative to empirical risk minimization has been proposed. This new criterion  known
as the sample variance penalization [5] trades-off the empirical risk with the empirical variance:

i=1 e−yif (Xi)(cid:1) by greedily constructing the function f (·) via(cid:80)S
(cid:80)n

n

(cid:115)

n(cid:88)

i=1

arg min
f∈F

1
n

l(f (Xi)  yi) + τ

ˆV[l(f (X)  y)]

n

 

(1)

where τ ≥ 0 explores the trade-off between the two quantities. The motivation for sample variance
penalization comes from the following theorem [5]:

2

Theorem 1 Let (Xi  yi)n
f : X → R. Then  for a loss l : R × Y → [0  1]  for any δ > 0  w.p. at least 1 − δ  ∀f ∈ F
18 ˆV[l(f (X)  y)] ln(M(n)/δ)

i=1 be drawn iid from a distribution D. Let F be a class of functions
n(cid:88)

15 ln(M(n)/δ)

(cid:115)

l(f (Xi)  yi) +

(n − 1)

+

n

E[l(f (X)  y)] ≤ 1
n

i=1

 

(2)

where M(n) is a complexity measure.

From the above uniform convergence result  it can be argued that future loss can be minimized by
minimizing the right hand side of the bound on training examples. Since the variance ˆV[l(f (X)  y)]
has a multiplicative factor involving M(n)  δ and n  for a given problem  it is difﬁcult to specify the
relative importance between empirical risk and empirical variance a priori. Hence  sample variance
penalization (1) necessarily involves a trade-off parameter τ.
Empirical risk minimization or sample variance penalization on the 0 − 1 loss is a hard problem;
this problem is often circumvented by minimizing a convex upper bound on the 0 − 1 loss. In this
paper  we consider the exponential loss l(f (X)  y) := e−yf (X). With the above loss  it was shown
by [10] that sample variance penalization is equivalent to minimizing the following cost 

(cid:32) n(cid:88)

(cid:33)2

n

n(cid:88)

(cid:32) n(cid:88)

(cid:33)2 .

e−yif (Xi)

+ λ

e−2yif (Xi) −

e−yif (Xi)

(3)

i=1

i=1

i=1

Theorem 1 requires that the loss function be bounded. Even though the exponential loss is un-
bounded  boosting is typically performed only for a ﬁnite number of iterations in most practical
applications. Moreover  since weak learners typically perform only slightly better than random
guessing  each αs in AdaBoost (or in VadaBoost) is typically small thus limiting the range of the
function learned. Furthermore  experiments will conﬁrm that sample variance penalization results
in a signiﬁcant empirical performance improvement over empirical risk minimization.
Our proposed algorithm is called VadaBoost1 and is described in Algorithm 2. VadaBoost iteratively
performs sample variance penalization (i.e.  it minimizes the cost (3) iteratively). Clearly  VadaBoost
shares the simplicity and ease of implementation found in AdaBoost.

3 Derivation of VadaBoost

a function of α:

(cid:80)s−1
In the sth iteration  our objective is to choose a weak learner Gs and a weight αs such that
t=1 αtGt(xi)/Zs. Given a can-
t=1 αtGt(·) + αGs(·) can be expressed as

(cid:80)s
didate weak learner Gs(·)  the cost (3) for the function(cid:80)s−1
t=1 αtGt(·) reduces the cost (3). Denote by wi the quantity e−yi
(cid:88)

2 .

(cid:88)

j∈J

wjeα

(4)

wie−α +

(cid:88)

i e−2α + n
w2

n

V (α; w  λ  I  J) :=

(cid:88)

j∈J

(cid:88)

j∈J

(cid:88)

i∈I

j e2α−
w2

wie−α +

2

wjeα

+λ

i∈I

i∈I

up to a multiplicative factor. In the quantity above  I and J are the two index sets (of correctly
classiﬁed and incorrectly classiﬁed examples) over Gs. Let the vector w whose ith component is
wi denote the current set of weights on the training examples. Here  we have dropped the sub-
scripts/superscripts s for brevity.

Lemma 2 The update of αs in Algorithm 2 minimizes the cost

U (α; w  λ  I  J) :=

i + (1 − λ)wi

e−2α +

(cid:32)(cid:88)

i∈I

(cid:0)λnw2

(cid:1)(cid:33)

(cid:0)λnw2

j + (1 − λ)wj

(cid:1) e2α.

(5)

(cid:88)

j∈J

1The V in VadaBoost emphasizes the fact that Algorithm 2 penalizes the empirical variance.

3

Proof By obtaining the second derivative of the above expression (with respect to α)  it is easy to
see that it is convex in α. Thus  setting the derivative with respect to α to zero gives the optimal
choice of α as shown in Algorithm 2.

2

(cid:88)

j∈J

wie−α +

wjeα

(cid:33)(cid:88)

j∈J

wi




wj

i=1 wi = 1 (i.e. normalized weights). Then 
V (α; w  λ  I  J) ≤ U (α; w  λ  I  J) and V (0; w  λ  I  J) = U (0; w  λ  I  J). That is  U is an upper
bound on V and the bound is exact at α = 0.
Proof Denoting 1 − λ by ¯λ  we have:

V (α; w  λ  I  J) =

i∈I



j∈J

i∈I

= ¯λ

= λ

= λ

i∈I

j∈J

i∈I

i∈I

w2

j e2α

j∈J

w2

j e2α

j∈J

wjeα

+ λ

2

wie−α +

wjeα

+ λ

wie−α +

j e2α −
w2

(cid:88)

i∈I

i e−2α + n
w2

(cid:88)

(cid:88)
2

j∈J

(cid:88)
n
(cid:88)
n
(cid:88)

Theorem 3 Assume that 0 ≤ λ ≤ 1 and (cid:80)n
n
2
(cid:88)
i e−2α + n
w2
n
(cid:88)
(cid:88)
i e−2α + n
(cid:32)(cid:88)
w2
+ ¯λ
(cid:88)
(cid:33)2
 + ¯λ
(cid:32)(cid:88)
(cid:33)1 −(cid:88)
 + 2¯λ
(cid:33)(cid:88)
(cid:33)
(cid:32)(cid:88)
(cid:1) e2α
(cid:88)
(cid:0)λnw2
(cid:0)−e2α − e−2α + 2(cid:1)
(cid:1) e2α = U (α; w  λ  I  J).
(cid:88)
(cid:32)(cid:88)
(cid:0)λnw2
i∈I wi +(cid:80)
expanded. On the next line  we used the fact that(cid:80)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:32)
1 −(cid:88)
(cid:1)(cid:33)
(cid:33)(cid:88)
(cid:1)(cid:33)

(cid:32)(cid:88)
 e−2α


(cid:88)
(cid:0)λnw2
(cid:32)(cid:88)
(cid:0)λnw2

+ ¯λ

j∈J =(cid:80)n

(cid:32)(cid:88)

i e−2α + n
w2

e−2α +

wi

wj

e2α + 2

wj

j∈J

wj

j∈J

wj

j∈J

wj

j∈J

wi

i∈I

=

i∈I

w2

j e2α

j∈J

i + ¯λwi

e−2α +

i + ¯λwi

e−2α +

wi

i∈I

wi

e2α

i∈I

≤

i∈I

i∈I

i∈I

+

j∈J

i∈I

j∈J

j∈J

wi

i∈I

j + ¯λwj

j + ¯λwj

On line two  terms were simply regrouped. On line three  the square term from line two was
i=1 wi = 1. On the ﬁfth
line  we once again regrouped terms; the last term in this expression (which is e2α + e−2α − 2)
can be written as (eα−e−α)2. When α = 0 this term vanishes. Hence the bound is exact at α = 0.

Corollary 4 VadaBoost monotonically decreases the cost (3).

The above corollary follows from:

V (αs; w  λ  I  J) ≤ U (αs; w  λ  I  J) < U (0; w  λ  I  J) = V (0; w  λ  I  J).

In the above  the ﬁrst inequality follows from Theorem (3). The second strict inequality holds
because αs is a minimizer of U from Lemma (2); it is not hard to show that U (αs; w  λ  I  J) is
strictly less than U (0; w  λ  I  J) from the termination criterion of VadaBoost. The third equality
again follows from Theorem (3). Finally  we notice that V (0; w  λ  I  J) merely corresponds to the

t=1 αtGt(·). Thus  we have shown that taking a step αs decreases the cost (3).

cost (3) at(cid:80)s−1

4

Figure 1: Typical Upper bound U (α; w  λ  I  J) and the actual cost function V (α; w  λ  I  J) values
under varying α. The bound is exact at α = 0. The bound gets closer to the actual function value as
λ grows. The left plot shows the bound for λ = 0 and the right plot shows it for λ = 0.9

We point out that we use a different upper bound in each iteration since V and U are parameterized
by the current weights in the VadaBoost algorithm. Also note that our upper bound holds only for
0 ≤ λ ≤ 1. Although the choice 0 ≤ λ ≤ 1 seems restrictive  intuitively  it is natural to have a
higher penalization on the empirical mean rather than the empirical variance during minimization.
Also  a closer look at the empirical Bernstein inequality in [5] shows that the empirical variance

term is multiplied by(cid:112)1/n while the empirical mean is multiplied by one. Thus  for large values of

n  the weight on the sample variance is small. Furthermore  our experiments suggest that restricting
λ to this range does not signiﬁcantly change the results.

4 How good is the upper bound?

First  we observe that our upper bound is exact when λ = 1. Also  our upper bound is loosest for
the case λ = 0. We visualize the upper bound and the true cost for two settings of λ in Figure 1.
Since the cost (4) is minimized via an upper bound (5)  a natural question is: how good is this ap-
proximation? We evaluate the tightness of this upper bound by considering its impact on learning
efﬁciency. As is clear from ﬁgure (1)  when λ = 1  the upper bound is exact and incurs no inefﬁ-
ciency. In the other extreme when λ = 0  the cost of VadaBoost coincides with AdaBoost and the
bound is effectively at its loosest. Even in this extreme case  VadaBoost derived through an upper
bound only requires at most twice the number of iterations as AdaBoost to achieve a particular cost.
The following theorem shows that our algorithm remains efﬁcient even in this worst-case scenario.

Theorem 5 Let OA denote the squared cost obtained by AdaBoost after S iterations. For weak
learners in any iteration achieving a ﬁxed error rate  < 0.5  VadaBoost with the setting λ = 0
attains a cost at least as low as OA in no more than 2S iterations.

Proof Denote the weight on the example i in sth iteration by ws

i . The weighted error rate of the sth

classiﬁer is s =(cid:80)

exp(−yi

(cid:80)S
n(cid:81)S

s=1 αsGs(Xi))
s=1 Zs

.

j∈Js

ws

wS+1

i

=

j . We have  for both algorithms 
i exp(−yiαSGS(Xi))
wS
(cid:88)
(cid:88)

Zs

=

Z a

s =

ws

j eαs +

j∈js

i∈Is

i e−αs = 2(cid:112)s(1 − s).

ws

The value of the normalization factor in the case of AdaBoost is

Similarly  the value of the normalization factor for VadaBoost is given by

(6)

(7)

(8)

(cid:88)

j∈Js

(cid:88)

i∈Is

Z v

s =

ws

j eαs +

√

s +

√

1 − s).

i e−αs = ((s)(1 − s))
ws

1

4 (

5

00.30.60.912αCost  Actual Cost:VUpper Bound:U00.30.60.9123αCost  Actual Cost:VUpper Bound:UThe squared cost function of AdaBoost after S steps is given by

OA =

S(cid:88)

(cid:33)2
(cid:32) n(cid:88)
We used (6)  (7) and the fact that(cid:80)n

αsyiGs(X))

exp(−yi

s=1

i=1

λ = 0 the cost of VadaBoost satisﬁes2

(cid:32) n(cid:88)
S(cid:89)

i=1

i

i=1 wS+1

exp(−yi

S(cid:88)

(cid:33)2
(2s(1 − s) +(cid:112)s(1 − s)).

αsyiGs(X))

s=1

= n2

OV =

(cid:32)

S(cid:89)

n(cid:88)

(cid:33)2

=

n

Z a
s

ws+1

i

= n2

s=1

i=1

(cid:33)2

Z a
s

= n2

(cid:32) S(cid:89)

s=1

S(cid:89)

s=1

4s(1 − s).

= 1 to derive the above expression. Similarly  for

(cid:32)

S(cid:89)

n(cid:88)

(cid:33)2

(cid:32) S(cid:89)

(cid:33)2

=

n

Z a
s

ws+1

i

= n2

Z v
s

s=1

i=1

s=1

s=1

Now  suppose that s =  for all s. Then  the squared cost achieved by AdaBoost is given by
n2(4(1 − ))S. To achieve the same cost value  VadaBoost  with weak learners with the same
error rate needs at most S
times. Within the range of interest for   the term
multiplying S above is at most 2.

√
log(4(1−))
log(2(1−)+

(1−))

Although the above worse-case bound achieves a factor of two  for  > 0.4  VadaBoost requires
only about 33% more iterations than AdaBoost. To summarize  even in the worst possible scenario
where λ = 0 (when the variational bound is at its loosest)  the VadaBoost algorithm takes no more
than double (a small constant factor) the number of iterations of AdaBoost to achieve the same cost.

Algorithm 3 EBBoost:

Require: (Xi  yi)n

i=1  scalar parameter λ ≥ 0  and weak learners H
Initialize the weights: wi ← 1/n for i = 1  . . .   n; Initialize f to predict zero on all inputs.
(cid:19)
for s ← 1 to S do

(cid:18) (1−λ)((cid:80)
Get a weak learner Gs(·) that minimizes (3) with the following choice of αs:
(1−λ)((cid:80)
i∈Is
αs = 1
i∈Js
if αs < 0 then break end if
f (·) ← f (·) + αsGs(·)

wi ← wi exp(−yiGs(Xi)αs)/Zs where Zs is such that(cid:80)n

wi)2+λn(cid:80)
wi)2+λn(cid:80)

i∈Is
i∈Js

4 log

w2
i
w2
i

i=1 wi = 1.

end for

5 A limitation of the EBBoost algorithm

A sample variance penalization algorithm known as EBBoost was previously explored [10]. While
this algorithm was simple to implement and showed signiﬁcant improvements over AdaBoost  it
suffers from a severe limitation:
it requires enumeration and evaluation of every possible weak
learner per iteration. Recall the steps implementing EBBoost in Algorithm 3. An implementation
of EBBoost requires exhaustive enumeration of weak learners in search of the one that minimizes
cost (3). It is preferable  instead  to ﬁnd the best weak learner by providing weights on the training
examples and efﬁciently computing the rule whose performance on that weighted set of examples
is guaranteed to be better than random guessing. However  with the EBBoost algorithm  the weight

i +(cid:0)(cid:80)
(cid:1)2 and the weight on correctly classi-
(cid:1)2; these aggregate weights on misclassiﬁed examples and

on all the misclassiﬁed examples is(cid:80)
ﬁed examples is(cid:80)

i +(cid:0)(cid:80)

correctly classiﬁed examples do not translate into weights on the individual examples. Thus  it be-
comes necessary to exhaustively enumerate weak learners in Algorithm 3. While enumeration of
weak learners is possible in the case of decision stumps  it poses serious difﬁculties in the case of
weak learners such as decision trees  ridge regression  etc. Thus  VadaBoost is the more versatile
boosting algorithm for sample variance penalization.

i∈Js
wi

i∈Js

i∈Is

i∈Is

w2

w2

wi

2The cost which VadaBoost minimizes at λ = 0 is the squared cost of AdaBoost  we do not square it again.

6

Table 1: Mean and standard errors with decision stump as the weak learner.

RLP-Boost
16.21 ± 0.1
22.29 ± 0.2
3.18 ± 0.1
0.01 ± 0.0
3.60 ± 0.1
0.98 ± 0.0
0.68 ± 0.0
2.06 ± 0.1
4.51 ± 0.1
2.77 ± 0.1
13.02 ± 0.6
5.81 ± 0.1
8.55 ± 0.2
3.29 ± 0.1
2.44 ± 0.1
10.95 ± 0.1
24.16 ± 0.1
4.96 ± 0.3

RQP-Boost
16.04 ± 0.1
21.79 ± 0.2
3.09 ± 0.1
0.00 ± 0.0
3.41 ± 0.1
0.88 ± 0.0
0.63 ± 0.0
1.95 ± 0.1
4.25 ± 0.1
2.72 ± 0.1
12.86 ± 0.6
5.75 ± 0.1
8.47 ± 0.1
3.07 ± 0.1
2.36 ± 0.1
10.60 ± 0.1
23.61 ± 0.1
4.72 ± 0.3

Dataset
a5a
abalone
image
mushrooms
musk
mnist09
mnist14
mnist27
mnist38
mnist56
ringnorm
spambase
splice
twonorm
w4a
waveform
wine
wisc

AdaBoost
16.15 ± 0.1
21.64 ± 0.2
3.37 ± 0.1
0.02 ± 0.0
3.84 ± 0.1
0.89 ± 0.0
0.64 ± 0.0
2.11 ± 0.1
4.45 ± 0.1
2.79 ± 0.1
13.16 ± 0.6
5.90 ± 0.1
8.83 ± 0.2
3.16 ± 0.1
2.60 ± 0.1
10.99 ± 0.1
23.62 ± 0.2
5.32 ± 0.3

EBBoost
16.05 ± 0.1
21.52 ± 0.2
3.14 ± 0.1
0.02 ± 0.0
3.51 ± 0.1
0.85 ± 0.0
0.58 ± 0.0
1.86 ± 0.1
4.12 ± 0.1
2.56 ± 0.1
11.74 ± 0.6
5.64 ± 0.1
8.33 ± 0.1
2.98 ± 0.1
2.38 ± 0.1
10.96 ± 0.1
23.52 ± 0.2
4.38 ± 0.2

VadaBoost
16.22 ± 0.1
21.63 ± 0.2
3.14 ± 0.1
0.01 ± 0.0
3.59 ± 0.1
0.84 ± 0.0
0.60 ± 0.0
2.01 ± 0.1
4.32 ± 0.1
2.62 ± 0.1
12.46 ± 0.6
5.78 ± 0.1
8.48 ± 0.1
3.09 ± 0.1
2.50 ± 0.1
10.75 ± 0.1
23.41 ± 0.1
5.00 ± 0.2

Table 2: Mean and standard errors with CART as the weak learner.

Dataset
a5a
abalone
image
mushrooms
musk
mnist09
mnist14
mnist27
mnist38
mnist56
ringnorm
spambase
splice
twonorm
w4a
waveform
wine
wisc

AdaBoost
17.59 ± 0.2
21.87 ± 0.2
1.93 ± 0.1
0.01 ± 0.0
2.36 ± 0.1
0.73 ± 0.0
0.52 ± 0.0
1.31 ± 0.0
1.89 ± 0.1
1.23 ± 0.1
7.94 ± 0.4
6.14 ± 0.1
4.02 ± 0.1
3.40 ± 0.1
2.90 ± 0.1
11.09 ± 0.1
21.94 ± 0.2
4.61 ± 0.2

VadaBoost
17.16 ± 0.1
21.30 ± 0.2
1.98 ± 0.1
0.01 ± 0.0
2.07 ± 0.1
0.72 ± 0.0
0.50 ± 0.0
1.24 ± 0.0
1.72 ± 0.1
1.17 ± 0.0
7.78 ± 0.4
5.76 ± 0.1
3.67 ± 0.1
3.27 ± 0.1
2.90 ± 0.1
10.59 ± 0.1
21.18 ± 0.2
4.18 ± 0.2

RLP-Boost
18.24 ± 0.1
22.16 ± 0.2
1.99 ± 0.1
0.02 ± 0.0
2.40 ± 0.1
0.76 ± 0.0
0.55 ± 0.0
1.32 ± 0.0
1.88 ± 0.1
1.20 ± 0.0
8.60 ± 0.5
6.25 ± 0.1
4.03 ± 0.1
3.50 ± 0.1
2.90 ± 0.1
11.11 ± 0.1
22.44 ± 0.2
4.63 ± 0.2

RQP-Boost
17.99 ± 0.1
21.84 ± 0.2
1.95 ± 0.1
0.01 ± 0.0
2.29 ± 0.1
0.71 ± 0.0
0.52 ± 0.0
1.29 ± 0.0
1.87 ± 0.1
1.19 ± 0.1
7.84 ± 0.4
6.03 ± 0.1
3.97 ± 0.1
3.38 ± 0.1
2.90 ± 0.1
10.82 ± 0.1
22.18 ± 0.2
4.37 ± 0.2

6 Experiments

In this section  we evaluate the empirical performance of the VadaBoost algorithm with respect to
several other algorithms. The primary purpose of our experiments is to compare sample variance
penalization versus empirical risk minimization and to show that we can efﬁciently perform sample
variance penalization for weak learners beyond decision stumps. We compared VadaBoost against
EBBoost  AdaBoost  regularized LP and QP boost algorithms [7]. All the algorithms except Ad-
aBoost have one extra parameter to tune.
Experiments were performed on benchmark datasets that have been previously used in [10]. These
datasets include a variety of tasks including all digits from the MNIST dataset. Each dataset was
divided into three parts: 50% for training  25% for validation and 25% for test. The total number
of examples was restricted to 5000 in the case of MNIST and musk datasets due to computational
restrictions of solving LP/QP.
The ﬁrst set of experiments use decision stumps as the weak learners. The second set of experiments
used Classiﬁcation and Regression Trees or CART [3] as weak learners. A standard MATLAB
implementation of CART was used without modiﬁcation. For all the datasets  in both experiments 

7

AdaBoost  VadaBoost and EBBoost (in the case of stumps) were run until there was no drop in the
error rate on the validation for the next 100 consecutive iterations. The values of the parameters for
VadaBoost and EBBoost were chosen to minimize the validation error upon termination. RLP-Boost
and RQP-Boost were given the predictions obtained by AdaBoost. Their regularization parameter
was also chosen to minimize the error rate on the validation set. Once the parameter values were
ﬁxed via the validation set  we noted the test set error corresponding to that parameter value. The
entire experiment was repeated 50 times by randomly selecting train  test and validation sets. The
numbers reported here are average from these runs.
The results for the decision stump and CART experiments are reported in Tables 1 and 2. For each
dataset  the algorithm with the best percentage test error is represented by a dark shaded cell. All
lightly shaded entries in a row denote results that are not signiﬁcantly different from the minimum
error (according to a paired t-test at a 1% signiﬁcance level). With decision stumps  both EBBoost
and VadaBoost have comparable performance and signiﬁcantly outperform AdaBoost. With CART
as the weak learner  VadaBoost is once again signiﬁcantly better than AdaBoost.
We gave a guarantee on the number of itera-
tions required in the worst case for Vadaboost
(which approximately matches the AdaBoost
cost (squared) in Theorem 5). An assumption
in that theorem was that the error rate of each
weak learner was ﬁxed. However  in practice 
the error rates of the weak learners are not con-
stant over the iterations. To see this behavior
in practice  we have shown the results with the
MNIST 3 versus 8 classiﬁcation experiment. In
ﬁgure 2 we show the cost (plus 1) for each al-
gorithm (the AdaBoost cost has been squared)
versus the number of iterations using a loga-
rithmic scale on the Y-axis. Since at λ = 0 
EBBoost reduces to AdaBoost  we omit its plot
at that setting. From the ﬁgure  it can be seen
that the number of iterations required by VadaBoost is roughly twice the number of iterations re-
quired by AdaBoost. At λ = 0.5  there is only a minor difference in the number of iterations
required by EBBoost and VadaBoost.

Figure 2: 1+ cost vs the number of iterations.

7 Conclusions

This paper identiﬁed a key weakness in the EBBoost algorithm and proposed a novel algorithm
that efﬁciently overcomes the limitation to enumerable weak learners. VadaBoost reduces a well
motivated cost by iteratively minimizing an upper bound which  unlike EBBoost  allows the boosting
method to handle any weak learner by estimating weights on the data. The update rule of VadaBoost
has a simplicity that is reminiscent of AdaBoost. Furthermore  despite the use of an upper bound 
the novel boosting method remains efﬁcient. Even when the bound is at its loosest  the number
of iterations required by VadaBoost is a small constant factor more than the number of iterations
required by AdaBoost. Experimental results showed that VadaBoost outperforms AdaBoost in terms
of classiﬁcation accuracy and efﬁciently applying to any family of weak learners. The effectiveness
of boosting has been explained via margin theory [9] though it has taken a number of years to settle
certain open questions [8]. Considering the simplicity and effectiveness of VadaBoost  one natural
future research direction is to study the margin distributions it obtains. Another future research
direction is to design efﬁcient sample variance penalization algorithms for other problems such as
multi-class classiﬁcation  ranking  and so on.

Acknowledgements This material is based upon work supported by the National Science Founda-
tion under Grant No. 1117631  by a Google Research Award  and by the Department of Homeland
Security under Grant No. N66001-09-C-0080.

8

100200300400500600700100101102103104105Iterationcost + 1  AdaBoostEBBoost λ=0.5VadaBoost λ=0VadaBoost λ=0.5References
[1] J-Y. Audibert  R. Munos  and C. Szepesv´ari. Tuning bandit algorithms in stochastic environ-

ments. In ALT  2007.

[2] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[3] L. Breiman  J.H. Friedman  R.A. Olshen  and C.J. Stone. Classiﬁcation and Regression Trees.

Chapman and Hall  New York  1984.

[4] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[5] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In

COLT  2009.

[6] V. Mnih  C. Szepesv´ari  and J-Y. Audibert. Empirical Bernstein stopping. In COLT  2008.
[7] G. Raetsch  T. Onoda  and K.-R. Muller. Soft margins for AdaBoost. Machine Learning 

43:287–320  2001.

[8] L. Reyzin and R. Schapire. How boosting the margin can also boost classiﬁer complexity. In

ICML  2006.

[9] R. E. Schapire  Y. Freund  P. L. Bartlett  and W. S. Lee. Boosting the margin: a new explanation

for the effectiveness of voting methods. Annals of Statistics  26(5):1651–1686  1998.

[10] P. K. Shivaswamy and T. Jebara. Empirical Bernstein boosting. In AISTATS  2010.
[11] V. Vapnik. The Nature of Statistical Learning Theory. Springer  New York  NY  1995.

9

,Feras Saad
Vikash Mansinghka