2017,Introspective Classification with Convolutional Nets,We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST  CIFAR-10  and SVHN using state-of-the-art CNN architectures  and observe improved classification results.,Introspective Classiﬁcation with Convolutional Nets

Long Jin

UC San Diego

longjin@ucsd.edu

Justin Lazarow
UC San Diego

jlazarow@ucsd.edu

Zhuowen Tu
UC San Diego
ztu@ucsd.edu

Abstract

We propose introspective convolutional networks (ICN) that emphasize the im-
portance of having convolutional neural networks empowered with generative
capabilities. We employ a reclassiﬁcation-by-synthesis algorithm to perform train-
ing using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively:
(1) synthesize pseudo-negative samples; and (2) enhance itself by improving the
classiﬁcation. The single CNN classiﬁer learned is at the same time generative
— being able to directly synthesize new samples within its own discriminative
model. We conduct experiments on benchmark datasets including MNIST  CIFAR-
10  and SVHN using state-of-the-art CNN architectures  and observe improved
classiﬁcation results.

Introduction

1
Great success has been achieved in obtaining powerful discriminative classiﬁers via supervised
training  such as decision trees [34]  support vector machines [42]  neural networks [23]  boosting
[7]  and random forests [2]. However  recent studies reveal that even modern classiﬁers like deep
convolutional neural networks [20] still make mistakes that look absurd to humans [11]. A common
way to improve the classiﬁcation performance is by using more data  in particular “hard examples” 
to train the classiﬁer. Different types of approaches have been proposed in the past including
bootstrapping [31]  active learning [37]  semi-supervised learning [51]  and data augmentation [20].
However  the approaches above utilize data samples that are either already present in the given
training set  or additionally created by humans or separate algorithms.
In this paper  we focus on improving convolutional neural networks by endowing them with synthesis
capabilities to make them internally generative. In the past  attempts have been made to build
connections between generative models and discriminative classiﬁers [8  27  41  15]. In [44]  a
self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially
learning weak classiﬁers using the given data and self-generated negative samples; the generative via
discriminative learning work in [40] generalizes the concept that unsupervised generative modeling
can be accomplished by learning a sequence of discriminative classiﬁers via self-generated pseudo-
negatives. Inspired by [44  40] in which self-generated samples are utilized  as well as recent success
in deep learning [20  9]  we propose here an introspective convolutional network (ICN) classiﬁer and
study how its internal generative aspect can beneﬁt CNN’s discriminative classiﬁcation task. There is
a recent line of work using a discriminator to help with an external generator  generative adversarial
networks (GAN) [10]  which is different from our objective here. We aim at building a single CNN
model that is simultaneously discriminative and generative.
The introspective convolutional networks (ICN) being introduced here have a number of properties. (1)
We introduce introspection to convolutional neural networks and show its signiﬁcance in supervised
classiﬁcation. (2) A reclassiﬁcation-by-synthesis algorithm is devised to train ICN by iteratively
augmenting the negative samples and updating the classiﬁer. (3) A stochastic gradient descent
sampling process is adopted to perform efﬁcient synthesis for ICN. (4) We propose a supervised
formulation to directly train a multi-class ICN classiﬁer. We show consistent improvement over
state-of-the-art CNN classiﬁers (ResNet [12]) on benchmark datasets in the experiments.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2 Related work
Our ICN method is directly related to the generative via discriminative learning framework [40].
It also has connection to the self-supervised learning method [44]  which is focused on density
estimation by combining weak classiﬁers. Previous algorithms connecting generative modeling
with discriminative classiﬁcation [8  27  41  15] fall in the category of hybrid models that are direct
combinations of the two. Some existing works on introspective learning [22  3  38] have a different
scope to the problem being tackled here. Other generative modeling schemes such as MiniMax
entropy [50]  inducing features [6]  auto-encoder [1]  and recent CNN-based generative modeling
approaches [48  47] are not for discriminative classiﬁcation and they do not have a single model that
is both generative and discriminative. Below we discuss the two methods most related to ICN  namely
generative via discriminative learning (GDL) [40] and generative adversarial networks (GAN) [10].
Relationship with generative via discriminative learning (GDL) [40]
ICN is largely inspired by GDL and it follows a similar pipeline developed in [40]. However  there is
also a large improvement of ICN to GDL  which is summarized below.
• CNN vs. Boosting. ICN builds on top of convolutional neural networks (CNN) by explicitly
• Supervised classiﬁcation vs. unsupervised modeling. ICN focuses on the supervised classiﬁ-
cation task with competitive results on benchmark datasets whereas GDL was originally applied
to generative modeling and its power for the classiﬁcation task itself was not addressed.
• SGD sampling vs. Gibbs sampling. ICN carries efﬁcient SGD sampling for synthesis through
backpropagation which is much more efﬁcient than the Gibbs sampling strategy used in GDL.
• Single CNN vs. Cascade of classiﬁers. ICN maintains a single CNN classiﬁer whereas GDL
• Automatic feature learning vs. manually speciﬁed features. ICN has greater representational
power due to the end-to-end training of CNN whereas GDL relies on manually designed features.

revealing the introspectiveness of CNN whereas GDL adopts the boosting algorithm [7].

consists of a sequence of boosting classiﬁers.

Comparison with Generative Adversarial Networks (GANs) [10]
Recent efforts in adversarial learning [10] are also very interesting and worth comparing with.
• Introspective vs. adversarial. ICN emphasizes being introspective by synthesizing samples
from its own classiﬁer while GAN focuses on adversarial — using a distinct discriminator to
guide the generator.
• Supervised classiﬁcation vs. unsupervised modeling. The main focus of ICN is to develop a
classiﬁer with introspection to improve the supervised classiﬁcation task whereas GAN is mostly
for building high-quality generative models under unsupervised learning.
• Single model vs. two separate models. ICN retains a CNN discriminator that is itself a generator
whereas GAN maintains two models  a generator and a discriminator  with the discriminator in
GAN trained to classify between “real” (given) and “fake” (generated by the generator) samples.
• Reclassiﬁcation-by-synthesis vs. minimax. ICN engages an iterative procedure  reclassiﬁcation-
by-synthesis  stemmed from the Bayes theory whereas GAN has a minimax objective function to
optimize. Training an ICN classiﬁer is the same as that for the standard CNN.
• Multi-class formulation. In a GAN-family work [36]  a semi-supervised learning task is devised
by adding an additional “not-real” class to the standard k classes in multi-class classiﬁcation; this
results in a different setting to the standard multi-class classiﬁcation with additional model param-
eters. ICN instead  aims directly at the supervised multi-class classiﬁcation task by maintaining
the same parameter setting within the softmax function without additional model parameters.

Later developments alongside GAN [35  36  49  3] share some similar aspects to GAN  which
also do not achieve the same goal as ICN does. Since the discriminator in GAN is not meant to
perform the generic two-class/multi-class classiﬁcation task  some special settings for semi-supervised
learning [10  35  49  3  36] were created. ICN instead has a single model that is both generative and
discriminative  and thus  an improvement to ICN’s generator leads to a direct means to ameliorate its
discriminator. Other work like [11] was motivated from an observation that adding small perturbations
to an image leads to classiﬁcation errors that are absurd to humans; their approach is however taken
by augmenting positive samples from existing input whereas ICN is able to synthesize new samples
from scratch. A recent work proposed in [21] is in the same family of ICN  but [21] focuses on
unsupervised image modeling using a cascade of CNNs.

2

3 Method
The pipeline of ICN is shown in Figure 1  which has an immediate improvement over GDL [40]
in several aspects that have been described in the previous section. One particular gain of ICN is
its representation power and efﬁcient sampling process through backpropagation as a variational
sampling strategy.
3.1 Formulation
We start the discussion by introducing the basic formulation and borrow the notation from [40].
Let x be a data sample (vector) and y ∈ {−1  +1} be its label  indicating either a negative or
a positive sample (in multi-class classiﬁcation y ∈ {1  ...  K}). We study binary classiﬁcation
ﬁrst. A discriminative classiﬁer computes p(y|x)  the probability of x being positive or negative.
p(y = −1|x) + p(y = +1|x) = 1. A generative model instead models p(y  x) = p(x|y)p(y)  which
captures the underlying generation process of x for class y. In binary classiﬁcation  positive samples
are of primary interest. Under the Bayes rule:

p(x|y = +1) =

p(y = +1|x)p(y = −1)
p(y = −1|x)p(y = +1)

p(x|y = −1) 

(1)

(2)

which can be further simpliﬁed when assuming equal priors p(y = +1) = p(y = −1):

p(x|y = +1) =

p(y = +1|x)
1 − p(y = +1|x)

p(x|y = −1).

Figure 1: Schematic illustration of our reclassiﬁcation-by-synthesis algorithm for ICN training. The top-left
ﬁgure shows the input training samples where the circles in red are positive samples and the crosses in blue are
the negatives. The bottom ﬁgures are the samples progressively self-generated by the classiﬁer in the synthesis
steps and the top ﬁgures show the decision boundaries (in purple) progressively updated in the reclassiﬁcation
steps. Pseudo-negatives (purple crosses) are gradually generated and help tighten the decision boundaries.
We make two interesting and important observations from Eqn. (2): 1) p(x|y = +1) is dependent
on the faithfulness of p(x|y = −1)  and 2) a classiﬁer C to report p(y = +1|x) can be made
simultaneously generative and discriminative. However  there is a requirement: having an in-
formative distribution for the negatives p(x|y = −1) such that samples drawn x ∼ p(x|y = −1)

3

Reclassification Step: training on the given training data + generated pseudo-negatives Synthesis Step:synthesize pseudo-negative samplesConvolutional Neural NetworksIntrospective Convolutional Networks:•Synthesis•ReclassificationInitial Classification (given training data)Final Classification (given training data +self-generated pseudo-negatives)SynthesisClassificationhave good coverage to the entire space of x ∈ Rm  especially for samples that are close to the
positives x ∼ p(x|y = +1)  to allow the classiﬁer to faithfully learn p(y = +1|x). There seems
to exist a dilemma. In supervised learning  we are only given a set of limited amount of training
data  and a classiﬁer C is only focused on the decision boundary to separate the given samples and
the classiﬁcation on the unseen data may not be accurate. This can be seen from the top left plot
in Figure 1. This motivates us to implement the synthesis part within learning — make a learned
discriminative classiﬁer generate samples that pass its own classiﬁcation and see how different these
generated samples are to the given positive samples. This allows us to attain a single model that has
two aspects at the same time: a generative model for the positive samples and an improved classiﬁer
for the classiﬁcation.
Suppose we are given a training set S = {(xi  yi)  i = 1..n} and x ∈ Rm and y ∈ {−1  +1}. One
can directly train a discriminative classiﬁer C  e.g. a convolutional neural networks [23] to learn
p(y = +1|x)  which is always an approximation due to various reasons including insufﬁcient training
samples  generalization error  and classiﬁer limitations. Previous attempts to improve classiﬁcation
by data augmentation were mostly done to add more positive samples [20  11]; we instead argue
the importance of adding more negative samples to improve the classiﬁcation performance. The
dilemma is that S = {(xi  yi)  i = 1..n} is limited to the given data. For clarity  we now use p−(x)
to represent p(x|y = −1). Our goal is to augment the negative training set by generating confusing
pseudo-negatives to improve the classiﬁcation (note that in the end pseudo-negative samples drawn
x ∼ p−
t (x) will become hard to distinguish from the given positive samples. Cross-validation
can be used to determine when using more pseudo-negatives is not reducing the validation error).
We call the samples drawn from x ∼ p−
t (x) pseudo-negatives (deﬁned in [40]). We expand
e = S ∪ St
S = {(xi  yi)  i = 1..n} by St
pn = {(xi −1)  i = n + 1  ...  n + tl}.
St

pn = ∅ and for t ≥ 1

pn  where S0

pn includes all the pseudo-negative samples self-generated from our model up to time t. l indicates
St
the number of pseudo-negatives generated at each round. We deﬁne a reference distribution p−
r (x) =
U (x)  where U (x) is a Gaussian distribution (e.g. N (0.0  0.32) independently). We carry out
learning with t = 0...T to iteratively obtain qt(y = +1|x) and qt(y = −1|x) by updating classiﬁer
C t on St
e = S reports discriminative probability
pn. The initial classiﬁer C 0 on S0
q0(y = +1|x). The reason for using q is because it is an approximation to the true p due to limited
samples drawn in Rm. At each time t  we then compute

e = S ∪ St

where Zt =(cid:82) qt(y=+1|x)

set:

qt(y=−1|x) p−
St+1
pn = St

p−
t (x) =

1
Zt

qt(y = +1|x)
qt(y = −1|x)

p−
r (x) 

(3)

r (x)dx. Draw new samples xi ∼ p−

t (x) to expand the pseudo-negative

pn ∪ {(xi −1)  i = n + tl + 1  ...  n + (t + 1)l}.

(4)
We name the speciﬁc training algorithm for our introspective convolutional network (ICN) classiﬁer
reclassiﬁcation-by-synthesis  which is described in Algorithm 1. We adopt convolutional neural
networks (CNN) classiﬁer to build an end-to-end learning framework with an efﬁcient sampling
process (to be discussed in the next section).
3.2 Reclassiﬁcation-by-synthesis
We present our reclassiﬁcation-by-synthesis algorithm for ICN in this section. A schematic illustration
is shown in Figure 1. A single CNN classiﬁer is being trained progressively which is simultaneously a
discriminator and a generator. With the pseudo-negatives being gradually generated  the classiﬁcation
boundary gets tightened  and hence yields an improvement to the classiﬁer’s performance. The
reclassiﬁcation-by-synthesis method is described in Algorithm 1. The key to the algorithm includes
two steps: (1) reclassiﬁcation-step  and (2) synthesis-step  which will be discussed in detail below.
3.2.1 Reclassiﬁcation-step
The reclassiﬁcation-step can be viewed as training a normal classiﬁer on the training set St
where S = {(xi  yi)  i = 1..n} and S0
use CNN as our base classiﬁer. When training a classiﬁer C t on St
learned in C t by a high-dimensional vector Wt = (w(0)
parameters. w(1)

e = S∪St
pn = {(xi −1)  i = n + 1  ...  n + tl} for t ≥ 1. We
e  we denote the parameters to be
) which might consist of millions of
) and w(0)

denotes the weights of the top layer combining the features φ(x; w(0)

pn = ∅. St

pn

  w(1)

t

t

t

t

t

4

carries all the internal representations. Without loss of generality  we assume a sigmoid function for
the discriminative probability

qt(y|x; Wt) = 1/(1 + exp{−yw(1)

· φ(x; w(0)
) deﬁnes the feature extraction function for x. Both w(1)

)}) 
and w(0)

where φ(x; w(0)
can be learned by
the standard stochastic gradient descent algorithm via backpropagation to minimize a cross-entropy
loss with an additional term on the pseudo-negatives:

t

t

t

t

t

L(Wt) = − i=1..n(cid:88)

ln qt(yi|xi; Wt) − i=n+1..n+tl(cid:88)

(xi yi)∈S

(xi −1)∈St

pn

ln qt(−1|xi; Wt).

(5)

Algorithm 1 Outline of the reclassiﬁcation-by-synthesis algorithm for discriminative classiﬁer
training.

r (x) = U (x) and train an initial CNN binary classiﬁer C 0

Input: Given a set of training data S = {(xi  yi)  i = 1..n} with x ∈ Rm and y ∈ {−1  +1}.
Initialization: Obtain a reference distribution: p−
on S  q0(y = +1|x). S0
pn = ∅. U (x) is a zero mean Gaussian distribution.
For t=0..T
−
1. Update the model: p
t (x) = 1
Zt
2. Synthesis-step: sample l pseudo-negative samples xi ∼ p
current model p
3. Augment the pseudo-negative set with St+1
4. Reclassiﬁcation-step: Update CNN classiﬁer to C t+1 on St+1
5. t ← t + 1 and go back to step 1 until convergence (e.g. no improvement on the validation set).
End

−
t (x) using an SGD sampling procedure.

e = S ∪ St+1

qt(y=+1|x)
qt(y=−1|x) p−

pn ∪ {(xi −1)  i = n + tl + 1  ...  n + (t + 1)l}.

−
t (x)  i = n + tl + 1  ...  n + (t + 1)l from the

pn   resulting in qt+1(y = +1|x).

pn = St

r (x).

3.2.2 Synthesis-step
In the reclassiﬁcation step  we obtain qt(y|x; Wt) which is then used to update p−
Eqn. (3):

qt(y = +1|x; Wt)
qt(y = −1|x; Wt)

1
Zt

p−
t (x) =

p−
r (x).
In the synthesis-step  our goal is to draw fair samples from p−
t (x) (fair samples refer to typical
samples by a sampling process after convergence w.r.t the target distribution). In [40]  various Markov
chain Monte Carlo techniques [28] including Gibbs sampling and Iterated Conditional Modes (ICM)
have been adopted  which are often slow. Motivated by the DeepDream code [32] and Neural
r (x) by increasing qt(y=+1|x;Wt)
Artistic Style work [9]  we update a random sample x drawn from p−
qt(y=−1|x;Wt)
using backpropagation. Note that the partition function (normalization) Zt is a constant that is not
dependent on the sample x. Let

(6)

t (x) according to

· φ(x; w(0)
and take its ln  which is nicely turned into the logit of qt(y = +1|x; Wt)

= exp{w(1)

gt(x) =

qt(y = +1|x; Wt)
qt(y = −1|x; Wt)

t

t

)} 

(7)

(8)

ln gt(x) = w(1)

· φ(x; w(0)
r (x)  we directly increase w(1)T

t

t

).
φ(x; w(0)

Starting from x drawn from p−
) using stochastic gradient
ascent on x via backpropagation  which allows us to obtain fair samples subject to Eqn. (6). Gaussian
noise can be added to Eqn. (8) along the line of stochastic gradient Langevin dynamics [43] as

t

t

∆x =

∇(w(1)

t


2

· φ(x; w(0)

t

)) + η

where η ∼ N (0  ) is a Gaussian distribution and  is the step size that is annealed in the sampling
process.
Sampling strategies. When conducting experiments  we carry out several strategies using stochastic
gradient descent algorithm (SGD) and SGD Lagenvin including: i) early-stopping for the sampling
process after x becomes positive (aligned with contrastive divergence [4] where a short Markov chain
is simulated); ii) stopping at a large conﬁdence for x being positive  and iii) sampling for a ﬁxed 
large number of steps. Table 2 shows the results on these different options and no major differences
in the classiﬁcation performance are observed.

5

t+1(x)] ≤ KL[p+(x)||p−

Building connections between SGD and MCMC is an active area in machine learning [43  5  30]. In
[43]  combining SGD and additional Gaussian noise under annealed stepsize results in a simulation
of Langevin dynamics MCMC. A recent work [30] further shows the similarity between constant
SGD and MCMC  along with analysis of SGD using momentum updates. Our progressively learned
discriminative classiﬁer can be viewed as carving out the feature space on φ(x)  which essentially
becomes an equivalent class for the positives; the volume of the equivalent class that satisﬁes
the condition is exponentially large  as analyzed in [46]. The probability landscape of positives
(equivalent class) makes our SGD sampling process not particularly biased towards a small limited
modes. Results in Figure 2 illustrates that large variation of the sampled/synthesized examples.
3.3 Analysis
t (x) t=∞→ p+(x) can be derived (see the supplementary material)  inspired by
The convergence of p−
the proof from [40]: KL[p+(x)||p−
t (x)] where KL denotes the Kullback-
Leibler divergence and p(x|y = +1) ≡ p+(x)  under the assumption that classiﬁer at t + 1 improves
over t.
Remark. Here we pay particular attention to the negative samples which live in a space that is
often much larger than the positive sample space. For the negative training samples  we have
yi = −1 and xi ∼ Q−(x)  where Q−(x) is a distribution on the given negative examples in
the original training set. Our reclassiﬁcation-by-synthesis algorithm (Algorithm 1) essentially
constructs a mixture model ˜p(x) ≡ 1
t (x) by sequentially generating pseudo-negative
samples to augment our training set. Our new distribution for augmented negative sample set thus
becomes Q−
n+T l Q−(x) + T l
n+T l ˜p(x)  where ˜p(x) encodes pseudo-negative samples that
are confusing and similar to (but are not) the positives. In the end  adding pseudo-negatives might
degrade the classiﬁcation result since they become more and more similar to the positives. Cross-
validation can be used to decide when adding more pseudo-negatives is not helping the classiﬁcation
task. How to better use the pseudo-negative samples that are increasingly faithful to the positives
is an interesting topic worth further exploring. Our overall algorithm thus is capable of enhancing
classiﬁcation by self-generating confusing samples to improve CNN’s robustness.
3.4 Multi-class classiﬁcation
One-vs-all. In the above section  we discussed the binary classiﬁcation case. When dealing with
multi-class classiﬁcation problems  such as MNIST and CIFAR-10  we will need to adapt our
proposed reclassiﬁcation-by-synthesis scheme to the multi-class case. This can be done directly using
a one-vs-all strategy by training a binary classiﬁer Ci using the i-th class as the positive class and
then combine the rest classes into the negative class  resulting in a total of K binary classiﬁers. The
training procedure then becomes identical to the binary classiﬁcation case. If we have K classes 
then the algorithm will train K individual binary classiﬁers with
  w(1)K

(cid:80)T−1
t=0 p−

new(x) ≡ n

)  ...  (w(0)K

< (w(0)1

  w(1)1

) > .

T

t

t

t

t

The prediction function is simply

f (x) = arg max

k

exp{w(1)k

t

· φ(x; w(0)k

t

)}.

The advantage of using the one-vs-all strategy is that the algorithm can be made nearly identical to
the binary case at the price of training K different neural networks.
Softmax function.
It is also desirable to build a single CNN classiﬁer to perform multi-class
classiﬁcation directly. Here we propose a formulation to train an end-to-end multiclass classiﬁer
directly. Since we are directly dealing with K classes  the pseudo-negative data set will be slightly
different and we introduce negatives for each individual class by S0

pn = ∅ and:

pn = {(xi −k)  k = 1  ...  K  i = n + (t − 1) × k × l + 1  ...  n + t × k × l}
St

Suppose we are given a training set S = {(xi  yi)  i = 1..n} and x ∈ Rm and y ∈ {1  ..  K}. We
want to train a single CNN classiﬁer with

Wt =< w(0)

t

  w(1)1

t

  ...  w(1)K

t

>

t

denotes the internal feature and parameters for the single CNN  and w(1)k

where w(0)
top-layer weights for the k-th class. We therefore minimize an integrated objective function
t )})

L(Wt)=−(1−α)(cid:80)n

+α(cid:80)n+t×K×l

ln(1+exp{w

·φ(xi;w0

(1)|yi|

(0)
t

i=1 ln

i=n+1

t

t

(cid:80)K

(1)yi
exp{w
t
exp{w

k=1

·φ(xi;w
(1)k
t

·φ(xi;w

)}
(0)
t

)}

denotes the

(9)

6

(1)|yi|
t

The ﬁrst term in Eqn. (9) encourages a softmax loss on the original training set S. The second term
in Eqn. (9) encourages a good prediction on the individual pseudo-negative class generated for the
k-th class (indexed by |yi| for w
  e.g. for pseudo-negative samples belong to the k-th class 
|yi| = | − k| = k). α is a hyperparameter balancing the two terms. Note that we only need to build
a single CNN sharing w(0)
for all the K classes. In particular  we are not introducing additional
model parameters here and we perform a direct K-class classiﬁcation where the parameter setting is
identical to a standard CNN multi-class classiﬁcation task; to compare  an additional “not-real” class
is created in [36] and the classiﬁcation task there [36] thus becomes a K + 1 class classiﬁcation.
4 Experiments

t

Figure 2: Synthesized pseudo-negatives for the MNIST dataset by our ICN classiﬁer. The top row shows some
training examples. As t increases  our classiﬁer gradually synthesize pseudo-negative samples that become
increasingly faithful to the training samples.
We conduct experiments on three standard benchmark datasets  including MNIST  CIFAR-10 and
SVHN. We use MNIST as a running example to illustrate our proposed framework using a shallow
CNN; we then show competitive results using a state-of-the-art CNN classiﬁer  ResNet [12] on
MNIST  CIFAR-10 and SVHN. In our experiments  for the reclassiﬁcation step  we use the SGD
optimizer with mini-batch size of 64 (MNIST) or 128 (CIFAR-10 and SVHN) and momentum equal
to 0.9; for the synthesis step  we use the Adam optimizer [17] with momentum term β1 equal to 0.5.
All results are obtained by averaging multiple rounds.
Training and test time. In general  the training time for ICN is around double that of the baseline
CNNs in our experiments: 1.8 times for MNIST dataset  2.1 times for CIFAR-10 dataset and 1.7
times for SVHN dataset. The added overhead in training is mostly determined by the number of
generated pseudo-negative samples. For the test time  ICN introduces no additional overhead to the
baseline CNNs.
4.1 MNIST
We use the standard MNIST [24] dataset  which
consists of 55  000 training  5  000 validation
and 10  000 test samples. We adopt a simple
network  containing 4 convolutional layers  each
having a 5 × 5 ﬁlter size with 64  128  256 and
512 channels  respectively. These convolutional
layers have stride 2  and no pooling layers are
used. LeakyReLU activations [29] are used after
each convolutional layer. The last convolutional
layer is ﬂattened and fed into a sigmoid output
(in the one-vs-all case).
In the reclassiﬁcation step  we run SGD (for 5
epochs) on the current training data St
e  includ-
ing previously generated pseudo-negatives. Our
initial learning rate is 0.025 and is decreased by
a factor of 10 at t = 25. In the synthesis step  we use the backpropagation sampling process as
discussed in Section 3.2.2. In Table 2  we compare different sampling strategies. Each time we
synthesize a ﬁxed number (200 in our experiments) of pseudo-negative samples.
We show some synthesized pseudo-negatives from the MNIST dataset in Figure 2. The samples in
the top row are from the original training dataset. ICN gradually synthesizes pseudo-negatives  which
are increasingly faithful to the original data. Pseudo-negative samples will be continuously used
while improving the classiﬁcation result.

Table 1: Test errors on the MNIST dataset. We compare
our ICN method with the baseline CNN  Deep Belief
Network (DBN) [14]  and CNN w/ Label Smoothing
(LS) [39]. Moreover  the two-step experiments combin-
ing CNN + GDL [40] and combining CNN + DCGAN
[35] are also reported  and see descriptions in text for
more details.
Method
DBN

CNN + DCGAN
ICN-noise (ours)

CNN w/ LS
CNN + GDL

0.85
0.84
0.89
0.78

One-vs-all (%)

Softmax (%)

CNN (baseline)

0.87

-

-

1.11
0.77
0.69

-
-

0.77
0.72

ICN (ours)

7

One-vs-all (%)

Softmax (%)

Table 2: Comparison of different sampling strategies in the
synthesis step in ICN.

Comparison of different sampling
strategies. We perform SGD and SGD
Langevin (with injected Gaussians)  and
try several options via backpropagation
for the sampling strategies. Option 1:
early-stopping once the generated sam-
ples are classiﬁed as positive; option 2:
stopping at a high conﬁdence for sam-
ples being positive; option 3: stopping
after a large number of steps. Table 2
shows the results and we do not observe signiﬁcant differences in these choices.
Ablation study. We experiment using random noise as synthesized pseudo-negatives in an ablation
study. From Table 1  we observe that our ICN outperforms the CNN baseline and the ICN-noise
method in both one-vs-all and softmax cases.

Sampling Strategy
SGD (option 1)
SGD Langevin (option 1)
SGD (option 2)
SGD Langevin (option 2)
SGD (option 3)
SGD Langevin (option 3)

0.81
0.80
0.78
0.78
0.81
0.80

0.72
0.72
0.72
0.74
0.75
0.73

Figure 3: MNIST test error against the number of training examples (std dev. of the test error is also displayed).
The effect of ICN is more clear when having fewer training examples.

Effects on varying training sizes. To better understand the effectiveness of our ICN method  we
carry out an experiment by varying the number of training examples. We use training sets with
different sizes including 500  2000  10000  and 55000 examples. The results are reported in Figure 3.
ICN is shown to be particularly effective when the training set is relatively small  since ICN has the
capability to synthesize pseudo-negatives by itself to aid training.
Comparison with GDL and GAN. GDL [40] focuses on unsupervised learning; GAN [10] and
DCGAN [35] show results for unsupervised learning and semi-supervised classiﬁcation. To apply
GDL and GAN to the supervised classiﬁcation setting  we design an experiment to perform a two-step
implementation. For GDL  we ran the GDL code [40] and obtained the pseudo-negative samples
for each individual digit; the pseudo-negatives are then used as augmented negative samples to
train individual one-vs-all CNN classiﬁers (using an identical CNN architecture to ICN for a fair
comparison)  which are combined to form a multi-class classiﬁer in the end. To compare with
DCGAN [35]  we follow the same procedure: each generator trained by DCGAN [35] using the
TensorFlow implementation [16] was used to generate positive samples  which are then augmented
to the negative set to train the individual one-vs-all CNN classiﬁers (also using an identical CNN
architecture to ICN)  which are combined to create the overall multi-class classiﬁer. CNN+GDL
achieves a test error of 0.85% and CNN+DCGAN achieves a test error of 0.84% on the MNIST
dataset  whereas ICN reports an error of 0.78% using the same CNN architecture. As the supervised
learning task was not directly speciﬁed in DCGAN [35]  some care is needed to design the optimal
setting to utilize the generated samples from DCGAN in the two-step approach (we made attempts to
optimize the results). GDL [40] can be made into a discriminative classiﬁer by utilizing the given
negative samples ﬁrst but boosting [7] with manually designed features was adopted which may not
produce competitive results as CNN classiﬁer does. Nevertheless  the advantage of ICN being an
integrated end-to-end supervised learning single-model framework can be observed.
To compare with generative model based deep learning approach  we report the classiﬁcation result
of DBN [14] in Table 1. DBN achieves a test error of 1.11% using the softmax function. We also
compare with Label Smoothing (LS)  which has been used in [39] as a regularization technique by
encouraging the model to be less conﬁdent. In LS  for a training example with ground-truth label 
the label distribution is replaced with a mixture of the original ground-truth distribution and a ﬁxed
distribution. LS achieves a test error of 0.69% in the softmax case.

8

In addition  we also adopt ResNet-32 [13] (using the softmax function) as another baseline CNN
model  which achieves a test error of 0.50% on the MNIST dataset. Our ResNet-32 based ICN
achieves an improved result of 0.47%.
Robustness to external adversarial examples. To show the improved robustness of ICN in dealing
with confusing and challenging examples  we compare the baseline CNN with our ICN classiﬁer on
adversarial examples generated using the “fast gradient sign” method from [11]. This “fast gradient
sign” method (with  = 0.25) can cause a maxout network to misclassify 89.4% of adversarial
examples generated from the MNIST test set [11]. In our experiment  we set  = 0.125. Starting
with 10  000 MNIST test examples  we ﬁrst determine those which are correctly classiﬁed by the
baseline CNN in order to generate adversarial examples from them. We ﬁnd that 5  111 generated
adversarial examples successfully fool the baseline CNN  however  only 3  134 of these examples
can fool our ICN classiﬁer  which is a 38.7% reduction in error against adversarial examples. Note
that the improvement is achieved without using any additional training data  nor knowing a prior
about how these adversarial examples are generated by the speciﬁc “fast gradient sign method” [11].
On the contrary  of the 2  679 adversarial examples generated from the ICN classiﬁer side that fool
ICN using the same method  2  079 of them can still fool the baseline CNN classiﬁer. This two-way
experiment shows the improved robustness of ICN over the baseline CNN.

Method

One-vs-all (%)

Softmax (%)

w/o Data Augmentation

Table 3: Test errors on the CIFAR-10 dataset. In both one-
vs-all and softmax cases  ICN shows improvement over the
baseline ResNet model. The result of convolutional DBN is
from [19].

4.2 CIFAR-10
The CIFAR-10 dataset [18] consists of
60  000 color images of size 32 × 32. This
set of 60  000 images is split into two sets 
50  000 images for training and 10  000 im-
ages for testing. We adopt ResNet [13] as
our baseline model [45]. For data augmen-
tation  we follow the standard procedure
in [26  25  13] by augmenting the dataset
by zero-padding 4 pixels on each side; we
also perform cropping and random ﬂipping.
The results are reported in Table 3.
In
both one-vs-all and softmax cases  ICN
outperforms the baseline ResNet classiﬁers.
Our proposed ICN method is orthogonal to
many existing approaches which use vari-
ous improvements to the network structures
in order to enhance the CNN performance.
We also compare ICN with Convolutional
DBN [19]  ResNet-32 w/ Label Smoothing (LS) [39] and ResNet-32+DCGAN [35] methods as
described in the MNIST experiments. LS is shown to improve the baseline but is worse than our ICN
method in most cases except for the MNIST dataset.

w/ Data Augmentation
ResNet-32 (baseline)

Convolutional DBN
ResNet-32 (baseline)

ResNet-32 + DCGAN

ResNet-32 w/ LS

ResNet-32 + DCGAN

ResNet-32 w/ LS

6.70

-

6.75
6.58
6.52

ICN-noise (ours)

7.06
6.89

-

6.90
6.70

ICN-noise (ours)

12.99
13.28
12.94

21.1
12.38
12.65

-

11.94
11.46

13.44

-

-

ICN (ours)

ICN (ours)

4.3 SVHN
We use the standard SVHN [33] dataset. We combine the
training data with the extra data to form our training set
and use the test data as the test set. No data augmentation
has been applied. The result is reported in Table 4. ICN is
shown to achieve competitive results.

5 Conclusion

Table 4: Test errors on the SVHN dataset.

Method

Softmax (%)

ResNet-32 (baseline)

ResNet-32 w/ LS

ResNet-32 + DCGAN

ICN-noise (ours)

ICN (ours)

2.01
1.96
1.98
1.99
1.95

In this paper  we have proposed an introspective convolutional nets (ICN) algorithm that performs
internal introspection. We observe performance gains within supervised learning using state-of-the-art
CNN architectures on standard machine learning benchmarks.
Acknowledgement This work is supported by NSF IIS-1618477  NSF IIS-1717431  and a Northrop
Grumman Contextual Robotics grant. We thank Saining Xie  Weijian Xu  Fan Fan  Kwonjoon Lee 
Shuai Tang  and Sanjoy Dasgupta for helpful discussions.

9

References

[1] P. Baldi. Autoencoders  unsupervised learning  and deep architectures. In ICML Workshop on

Unsupervised and Transfer Learning  pages 37–49  2012.

[2] L. Breiman. Random forests. Machine learning  45(1):5–32  2001.
[3] A. Brock  T. Lim  J. Ritchie  and N. Weston. Neural photo editing with introspective adversarial

networks. In ICLR  2017.

[4] M. A. Carreira-Perpinan and G. Hinton. On contrastive divergence learning. In AISTATS 

volume 10  pages 33–40  2005.

[5] T. Chen  E. B. Fox  and C. Guestrin. Stochastic gradient hamiltonian monte carlo. In ICML 

2014.

[6] S. Della Pietra  V. Della Pietra  and J. Lafferty. Inducing features of random ﬁelds. IEEE

transactions on pattern analysis and machine intelligence  19(4):380–393  1997.

[7] Y. Freund and R. E. Schapire. A Decision-theoretic Generalization of On-line Learning And
An Application to Boosting. Journal of computer and system sciences  55(1):119–139  1997.
[8] J. Friedman  T. Hastie  and R. Tibshirani. The elements of statistical learning  volume 1.

Springer series in statistics Springer  Berlin  2001.

[9] L. A. Gatys  A. S. Ecker  and M. Bethge. A neural algorithm of artistic style. arXiv preprint

arXiv:1508.06576  2015.

[10] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In NIPS  2014.

[11] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. In

ICLR  2015.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

2016.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In European

Conference on Computer Vision  pages 630–645. Springer  2016.

[14] G. E. Hinton  S. Osindero  and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation  18(7):1527–1554  2006.

[15] T. Jebara. Machine learning: discriminative and generative  volume 755. Springer Science &

Business Media  2012.

[16] T. Kim. DCGAN-tensorﬂow. https://github.com/carpedm20/DCGAN-tensorflow.
[17] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR  2015.
[18] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. CS Dept.  U Toronto 

Tech. Rep.  2009.

[19] A. Krizhevsky and G. Hinton. Convolutional deep belief networks on cifar-10. Unpublished

manuscript  40  2010.

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet Classiﬁcation with Deep Convolutional

Neural Networks. In NIPS  2012.

[21] J. Lazarow  L. Jin  and Z. Tu. Introspective neural networks for generative modeling. In ICCV 

2017.

[22] D. B. Leake. Introspective learning and reasoning. In Encyclopedia of the Sciences of Learning 

pages 1638–1640. Springer  2012.

[23] Y. LeCun  B. Boser  J. S. Denker  D. Henderson  R. Howard  W. Hubbard  and L. Jackel.
Backpropagation applied to handwritten zip code recognition. In Neural Computation  1989.

[24] Y. LeCun and C. Cortes. The MNIST database of handwritten digits  1998.
[25] C.-Y. Lee  P. W. Gallagher  and Z. Tu. Generalizing pooling functions in convolutional neural

networks: Mixed  gated  and tree. In AISTATS  2016.

[26] C.-Y. Lee  S. Xie  P. Gallagher  Z. Zhang  and Z. Tu. Deeply-supervised nets. In AISTATS 

2015.

[27] P. Liang and M. I. Jordan. An asymptotic analysis of generative  discriminative  and pseudolike-

lihood estimators. In ICML  2008.

10

[28] J. S. Liu. Monte Carlo strategies in scientiﬁc computing. Springer Science & Business Media 

2008.

[29] A. L. Maas  A. Y. Hannun  and A. Y. Ng. Rectiﬁer nonlinearities improve neural network

acoustic models. In ICML  2013.

[30] S. Mandt  M. D. Hoffman  and D. M. Blei. Stochastic gradient descent as approximate bayesian

inference. arXiv preprint arXiv:1704.04289  2017.

[31] C. Z. Mooney  R. D. Duval  and R. Duvall. Bootstrapping: A nonparametric approach to

statistical inference. Number 94-95. Sage  1993.

[32] A. Mordvintsev  C. Olah  and M. Tyka. Deepdream - a code example for visualizing neural

networks. Google Research  2015.

[33] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading Digits in Natural
In NIPS Workshop on Deep Learning and

Images with Unsupervised Feature Learning.
Unsupervised Feature Learning  2011.

[34] J. R. Quinlan. Improved use of continuous attributes in c4. 5. Journal of artiﬁcial intelligence

research  4:77–90  1996.

[35] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolu-

tional generative adversarial networks. In ICLR  2016.

[36] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved

techniques for training gans. In NIPS  2016.

[37] B. Settles. Active learning literature survey. University of Wisconsin  Madison  52(55-66):11 

2010.

[38] A. Sinha  M. Sarkar  A. Mukherjee  and B. Krishnamurthy. Introspection: Accelerating neural

network training by learning weight evolution. arXiv preprint arXiv:1704.04959  2017.

[39] C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. Rethinking the inception architec-

ture for computer vision. In CVPR  2016.

[40] Z. Tu. Learning generative models via discriminative approaches. In CVPR  2007.
[41] Z. Tu  K. L. Narr  P. Dollár  I. Dinov  P. M. Thompson  and A. W. Toga. Brain anatomical
structure segmentation by hybrid discriminative/generative models. Medical Imaging  IEEE
Transactions on  27(4):495–508  2008.

[42] V. N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York  Inc.  1995.
[43] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In

ICML  2011.

[44] M. Welling  R. S. Zemel  and G. E. Hinton. Self supervised boosting. In NIPS  2002.
[45] Y. Wu.

https://github.com/ppwwyyxx/tensorpack/tree/

Tensorpack toolbox.

master/examples/ResNet.

[46] Y. N. Wu  S. C. Zhu  and X. Liu. Equivalence of julesz ensembles and frame models. Interna-

tional Journal of Computer Vision  38(3)  2000.

[47] J. Xie  Y. Lu  S.-C. Zhu  and Y. N. Wu. Cooperative training of descriptor and generator

networks. arXiv preprint arXiv:1609.09408  2016.

[48] J. Xie  Y. Lu  S.-C. Zhu  and Y. N. Wu. A theory of generative convnet. In ICML  2016.
[49] J. Zhao  M. Mathieu  and Y. LeCun. Energy-based generative adversarial network. In ICLR 

2017.

[50] S. C. Zhu  Y. N. Wu  and D. Mumford. Minimax entropy principle and its application to texture

modeling. Neural Computation  9(8):1627–1660  1997.

[51] X. Zhu. Semi-supervised learning literature survey. Computer Science  University of Wisconsin-

Madison  Technical Report 1530  2005.

11

,Long Jin
Justin Lazarow
Zhuowen Tu