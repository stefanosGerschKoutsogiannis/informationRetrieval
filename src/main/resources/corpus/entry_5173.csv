2013,Speedup Matrix Completion with Side Information: Application to Multi-Label Learning,In standard matrix completion theory  it is required to have at least $O(n\ln^2 n)$ observed entries to perfectly recover a low-rank matrix $M$ of size $n\times n$  leading to a large number of observations when $n$ is large. In many real tasks  side information in addition to the observed entries is often available. In this work  we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that  under appropriate conditions  with the assistance of side information matrices  the number of observed entries needed for a perfect recovery of matrix $M$ can be dramatically reduced to $O(\ln n)$. We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning.,Speedup Matrix Completion with Side Information:

Application to Multi-Label Learning

Miao Xu1
1National Key Laboratory for Novel Software Technology 

Zhi-Hua Zhou1

Rong Jin2

rongjin@cse.msu.edu

Nanjing University  Nanjing 210023  China

2Department of Computer Science and Engineering 
Michigan State University  East Lansing  MI 48824

{xum  zhouzh}@lamda.nju.edu.cn

Abstract

In standard matrix completion theory  it is required to have at least O(n ln2 n) ob-
served entries to perfectly recover a low-rank matrix M of size n × n  leading to
a large number of observations when n is large. In many real tasks  side informa-
tion in addition to the observed entries is often available. In this work  we develop
a novel theory of matrix completion that explicitly explore the side information
to reduce the requirement on the number of observed entries. We show that  un-
der appropriate conditions  with the assistance of side information matrices  the
number of observed entries needed for a perfect recovery of matrix M can be dra-
matically reduced to O(ln n). We demonstrate the effectiveness of the proposed
approach for matrix completion in transductive incomplete multi-label learning.

1 Introduction

Matrix completion concerns the problem of recovering a low-rank matrix from a limited number
of observed entries. It has broad applications including collaborative ﬁltering [35]  dimensionality
reduction [41]  multi-class learning [4  31]  clustering [15  42]  etc. Recent studies show that  with a
high probability  we can efﬁciently recover a matrix M ∈ Rn×m of rank r from O(r(n+m) ln2(n+
m)) observed entries when the observed entries are uniformly sampled from M [11  12  34].
Although the sample complexity for matrix completion  i.e.  the number of observed entries required
for perfectly recovering a low rank matrix  is already near optimal (up to a logarithmic factor)  its
linear dependence on n and m requires a large number of observations for recovering large matri-
ces  signiﬁcantly limiting its application to real-world problems. Moreover  current techniques for
matrix completion require solving an optimization problem that can be computationally prohibitive
when the size of the matrix is very large. In particular  although a number of algorithms have been
developed for matrix completion [10  22  23  25  27  28  39]  most of them require updating the full
matrix M at each iteration of optimization  leading to a high computational cost and a large storage
requirement when both n and m are large. Several recent efforts [5  19] try to address this issue  at
a price of losing performance guarantee in recovering the target matrix.
On the other hand  in several applications of matrix completion  besides the observed entries  side
information is often available that can potentially beneﬁt the process of matrix completion. Below
we list a few examples:
• Collaborative ﬁltering aims to predict ratings of individual users based on the ratings from
other users [35]. Besides the ratings provided by users  side information  such as the textual
description of items and the demographical information of users  is often available and can
be used to facilitate the prediction of missing ratings.

1

• Link prediction aiming to predict missing links between users in a social network based on
the existing ones can be viewed as a matrix completion problem [20]  where side informa-
tion  such as attributes of users (e.g.  browse patterns and interaction among users)  can be
used to assist completing the user-user-link matrix.

Although several studies exploit side information for matrix recovery [1  2  3  16  29  32  33]  most
of them focus on matrix factorization techniques  which usually result in non-convex optimization
problems without guarantee of perfectly recovering the target matrix. In contrast  matrix comple-
tion deals with convex optimization problems and perfect recovery is guaranteed under appropriate
conditions.
In this work  we focus on exploiting side information to improve the sample complexity and scala-
bility of matrix completion. We assume that besides the observed entries in the matrix M  there exist
two side information matrices A ∈ Rn×ra and B ∈ Rm×rb  where r ≤ ra ≤ n and r ≤ rb ≤ m.
We further assume the target matrix and the side information matrices share the same latent informa-
tion; that is  the column and row vectors in M lie in the subspaces spanned by the column vectors in
A and B  respectively. Unlike the standard theory of matrix completion that needs to ﬁnd the opti-
mal matrix M of size n× m  our optimization problem is reduced to searching for an optimal matrix
of size ra × rb  making the recovery signiﬁcantly more efﬁcient both computationally and storage
wise provided ra ≪ n and/or rb ≪ m. We show that  with the assistance of side information matri-
ces  with a high probability  we can perfectly recover M with O(r(ra + rb) ln(ra + rb) ln(n + m))
observed entries  a sample complexity that is sublinear in n and m.
We demonstrate the effectiveness of matrix completion with side information in transductive in-
complete multi-label learning [17]  which aims to assign multiple labels to individual instances in
a transductive learning setting. We formulate transductive incomplete multi-label learning as a ma-
trix completion problem  i.e.  completing the instance-label matrix based on the observed entries
that correspond to the given label assignments. Both the feature vectors of instances and the class
correlation matrix can be used as side information. Our empirical study shows that the proposed
approach is particularly effective when the number of given label assignments is small  verifying
our theoretical result  i.e.  side information can be used to reduce the sample complexity.
The rest of the paper is organized as follows: Section 2 brieﬂy reviews some related work. Section 3
presents our main contribution. Section 4 presents our empirical study. Finally Section 5 concludes
with future issues.

2 Related work

Matrix Completion The objective of matrix completion is to ﬁll out the missing entries of a matrix
based on the observed ones. Early work on matrix completion  also referred to as maximum margin
matrix factorization [37]  was developed for collaborative ﬁltering. Theoretical studies show that  it
is sufﬁcient to perfectly recover a matrix M ∈ Rn×m of rank r when the number of observed entries
is O(r(n + m) ln2(n + m)) [11  12  34]. A more general matrix recovery problem  referred to as
matrix regression  was examined in [30  36]. Unlike these studies  our proposed approach reduces
the sample complexity with the help of side information matrices.
Several computational algorithms [10  22  23  25  27  28  39] have been developed to efﬁciently
solve the optimization problem of matrix completion. The main problem with these algorithms lies
in the fact that they have to explicitly update the full matrix of size n × m  which is expensive both
computationally and storage wise for large matrices. This issue has been addressed in several recent
studies [5  19]  where the key idea is to store and update the low rank factorization of the target
matrix. A preliminary convergence analysis is given in [19]  however  none of these approaches
guarantees perfect recovery of the target matrix  even with signiﬁcantly large number of observed
entries. In contrast  our proposed approach reduces the computational cost by explicitly exploring
the side information matrices and still delivers the promise of perfect recovery.
Several recent studies involve matrix recovery with side information. [2  3  29  33] are based on
graphical models by assuming special distribution of latent factors; these algorithms  as well as [16]
and [32]  consider side information in matrix factorization. The main limitation lies in the fact that
they have to solve non-convex optimization problems  and do not have theoretical guarantees on
matrix recovery. Matrix completion with inﬁnite dimensional side information was exploited in [1] 

2

yet lacking guarantee of perfect recovery. In contrast  our work is based on matrix completion theory
that deals with a general convex optimization problem and is guaranteed to make a perfect recovery
of the target matrix.

Multi-label Learning Multi-label learning allows each instance to be assigned to multiple classes
simultaneously  making it more challenging than multi-class learning. The simplest approach for
multi-label learning is to train one binary model for each label  which is also referred to as BR
(Binary Relevance) [7]. Many advanced algorithms have been developed to explicitly explore the
dependence among labels ( [44] and references therein).
In this work  we will evaluate our proposed approach by transductive incomplete multi-label learn-
⊤ ∈ Rn×d be the feature matrix with xi ∈ Rd  where n is
ing [17]. Let X = (x1  . . .   xn)
the number of instances and d is the dimension. Let C1  . . .  Cm denote the m labels  and let
T ∈ {−1  +1}n×m be the instance-label matrix  where Ti;j = +1 when xi is associated with
the label Cj  and Ti;j = −1 when xi is not associated with the label Cj. Let Ω denote the subset of
the observed entries in T that corresponds to the given label assignments of instances. The objective
of transductive incomplete multi-label learning is to predict the missing entries in T based on the
feature matrix X and the given label assignments in Ω. The main challenge lies in the fact that only
a partial label assignment is given for each training instance. This is in contrast to many studies on
common semi-supervised or transductive multi-label learning [18  24  26  43] where each labeled
instance receives a complete set of label assignments. This is also different from multi-label learn-
ing with weak labels [8  38] which assumes that only the positive labels can be observed. Here we
assume the observed labels can be either positive or negative.
In [17]  a matrix completion based approach was proposed for transductive incomplete multi-label
learning. To effectively exploit the information in the feature matrix X  the authors proposed to
complete the matrix T
= [X  T ] that combines the input features with label assignments into a
single matrix. Two algorithms MC-b and MC-1 were presented there  differing only in the treatment
of bias term  whereas the convergence of MC-1 was examined in [9]. The main limitation of both
algorithms lies in their high computational cost when both the number of instances and features are
large. Unlike MC-1 and MC-b  our proposed approach does not need to deal with the big matrix
′  and is computationally more efﬁcient. Besides the computational advantage  we show that our
T
proposed approach signiﬁcantly improves the sample complexity of matrix completion by exploiting
side information matrices.

′

3 Speedup Matrix Completion with Side Information

We ﬁrst describe the framework of matrix completion with side information  and then present its
theoretical guarantee and application to multi-label learning

3.1 Matrix Completion using Side Information
Let M ∈ Rn×m be the target matrix of rank r to be recovered. Without loss of generality  we
assume n ≥ m. Let λk  k ∈ {1  . . .   r} be the kth largest singular value of M  and let uk ∈ Rn
and vk ∈ Rm be the corresponding left and right singular vectors  i.e.  M = U ΣV
⊤  where
Σ = diag(λ1  . . .   λr)  U = (u1  . . .   ur) and V = (v1  . . .   vr).
Let Ω ⊆ {1  . . .   n} × {1  . . .   m} be the subset of indices of observed entries sampled uniformly
from all entries in M. Given Ω  we deﬁne a linear operator RΩ(M ) : Rn×m 7→ Rn×m as

{

[RΩ(M )]i;j =

Mi;j

0

(i  j) ∈ Ω
(i  j) /∈ Ω

Using RΩ(·)  the standard matrix completion problem is:

∥ ˜M∥tr

s. t. RΩ( ˜M ) = RΩ(M ) 

min

˜M∈Rn(cid:2)m

(1)

where ∥ · ∥tr is the trace norm.
Let A = (a1  . . .   ara) ∈ Rn×ra and B = (b1  . . .   brb) ∈ Rm×rb be the side information matrices 
where r ≤ ra ≤ n and r ≤ rb ≤ m. Without loss of generality  we assume that ra ≥ rb and that

3

⊤
i aj = δi;j and b

⊤
both A and B are orthonormal matrices  i.e.  a
i bj = δi;j for any i and j  where
δi;j is the Kronecker delta function that outputs 1 if i = j and 0  otherwise. In case when the side
information is not available  A and B will be set to identity matrix.
The objective is to complete a matrix M of rank r with the side information matrices A and B. We
make the following assumption in order to fully exploit the side information:
Assumption A: the column vectors in M lie in the subspace spanned by the column vectors in A 
and the row vectors in M lie in the subspace spanned by the column vectors in B.
To understand the implication of this assumption  let us consider the problem of transductive incom-
plete multi-label learning [17]  where the objective is to complete the instance-label matrix based on
the observed entries corresponding to the given label assignments  and the side information matrices
A and B are given by the feature vectors of instances and the label correlation matrix  respectively.
Assumption A essentially implies that all the label assignments can be accurately predicted by a
linear combination of feature vectors of instances.
⊤ and therefore  our goal is to learn Z0 ∈
Using Assumption A  we can write M as M = AZ0B
Rra×rb. Following the standard theory for matrix completion [11  12  34]  we can cast the matrix
completion task into the following optimization problem:
s. t. RΩ(AZB

) = RΩ(M ).

∥Z∥tr

(2)

⊤

min

Z∈Rra(cid:2)rb

Unlike the standard algorithm for matrix completion that requires solving an optimization problem
involved matrix of n × m  the optimization problem given in (2) only deals with a matrix Z of
ra × rb  and therefore can be solved signiﬁcantly more efﬁciently if ra ≪ n and rb ≪ m.

(

n
r
mn
r

(

)

 

)

 

3.2 Theoretical Result

We deﬁne µ0 and µ1  the coherence measurements for matrix M as

µ0 = max

µ1 = max
i;j

∥PU ei∥2 
max
1≤i≤n
⊤

]i;j)2 

([U V

m
r

max
1≤j≤m

∥PV ej∥2

where ei is the vector with the ith entry equal to 1 and all others equal to 0  and PU and PV project
a vector onto the subspace spanned by the column vectors of U and V   respectively. We also deﬁne
the coherence measure for matrices A and B as

n∥Ai;∗∥2

m∥Bj;∗∥2

µAB = max

max
1≤i≤n

  max
1≤j≤m

ra

rb
where Ai;∗ and Bi;∗ stand for the ith row of A and B  respectively.
2 (1 + log2 ra − log2 r)  Ω0 =
Theorem 1. Let µ = max(µ0  µAB).
Deﬁne q0 = 1
3 µ2(rarb + r2) ln n. Assume Ω1 ≥ q0Ω0. With a
128(cid:12)
3 µ max(µ1  µ)r(ra + rb) ln n and Ω1 = 8(cid:12)
probability at least 1 − 4(q0 + 1)n
−(cid:12)+1 − 2q0n
−(cid:12)+2  Z0 is the unique optimizer to the problem in
(2) provided

|Ω| ≥ 64β
3

µ max(µ1  µ) (1 + log2 ra − log2 r) r(ra + rb) ln n.

Compared to the standard matrix completion theory [34]  the side information matrices reduce sam-
ple complexity from O(r(n + m) ln2(n + m)) to O(r(ra + rb) ln(ra + rb) ln n). When ra ≪ n and
rb ≪ m  the side information allows us signiﬁcantly reduce the number of observed entries required
for perfectly recovering matrix M. We defer the technical proof of Theorem 1 to the supplementary
material due to page limit. Note that although we follow the framework of [34] for analysis  namely
ﬁrst proving the result under deterministic conditions  and then showing that the deterministic con-
ditions hold with a high probability  our technical proof is quite different due to the involvement of
side information matrices A and B.

4

3.3 Application to Multi-Label Learning

Similar to the Singular Vector Thresholding (SVT) method [10]  we approximate the problem in ( 2)
by an unconstrained optimization problem  i.e. 
L(Z) = λ∥Z∥tr +

(cid:13)(cid:13)RΩ(AZB

⊤ − M )

(cid:13)(cid:13)2

min

(3)

F  

Z∈Rra(cid:2)rb

1
2

where λ > 0 is introduced to weight the trace norm regularization term against the regression error.
We develop an algorithm that exploits the smoothness of the loss function and therefore achieves
O(1/T 2) convergence  where T is the number of iterations. Details of the algorithm can be found
in the supplementary material. We refer to the proposed algorithm as Maxide.
For transductive incomplete multi-label learning  we abuse our notation by deﬁning n as the number
of instances  m as the number of labels  and d as the dimensionality of input patterns. Our goal is
to complete the instance-label matrix M ∈ Rn×m by using (i) the feature matrix X ∈ Rn×d and
(ii) the observed entries Ω in M (i.e.  the given label assignments). We thus set the side information
matrix A to include the top left singular vectors of X  and B = I to indicate that no side information
is available for the dependence among labels. We note that the low rank assumption of instance-label
matrix M implies a linear dependence among the label prediction functions. This assumption has
been explored extensively in the previous studies of multi-label learning [17  21  38].

4 Experiments

We evaluate the proposed algorithm for matrix completion with side information on both synthet-
ic and real data sets. Our implementation is in Matlab except that the operation RΩ(L × R) is
implemented in C. All the results were obtained on a Linux server with CPU 2.53GHz and 48GB
memory.

4.1 Experiments on Synthetic Data
To create the side information matrices A and B  we ﬁrst generate a random matrix F ∈ Rn×m 
with each entry Fi;j drawn independently from N (0  1). Side information matrix A includes the
ﬁrst ra left singular vectors of F   and B includes the ﬁrst rb right singular vectors. To create Z0 
we generate two Gaussian random matrices ZA ∈ Rra×r and ZB ∈ Rrb×r  where each entry
is sampled independently from N (0  1). The singular value decomposition of AZA and BZB is
2   respectively. We create a diagonal matrix Σ ∈
given by AZA = U Σ1V T
Rr×r  whose diagonal entries are drawn independently from N (0  104). Z0 is then given by Z0 =
)T where † denotes the pseudo inverse of a matrix. Finally  the target
(ZAΣ
matrix M is given by M = AZ0B

1 and BZB = V Σ2V T

†
†
1(V T
1 )

†
2(V T
2 )

)Σ(ZBΣ

⊤.

†

Settings and Baselines Our goal is to show that the proposed algorithm is able to accu-
rately recover the target matrix with signiﬁcantly smaller number of entries and less compu-
tational time.
In this study  we only consider square matrices (i.e.  m = n)  with n =
1  000; 5  000; 10  000; 20  000; 30  000 and rank r = 10; 50; 100. Both ra and rb of side informa-
tion matrices are set to be 2r  and |Ω|  the number of observed entries  is set to be r(2n − r)  which
is signiﬁcantly smaller than the number of observed entries used in previous studies [10  25  27].
We repeat each experiment 10 times  and report the result averaged over 10 runs. We compare the
proposed Maxide algorithm with three state-of-the-art matrix completion algorithms: Singular Vec-
tor Thresholding (SVT) [10]  Fixed Point Bregman Iterative Method (FPCA) [27] and Augmented
Lagrangian Method (ALM) [25]. In addition to these matrix completion methods  we also com-
pare with a trace norm minimizing method (TraceMin) [6]. For all the baseline  we use the codes
provided by their original authors with their default parameter settings.
Results We measure the performance of matrix completion by the relative error ∥AZB
⊤ −
M∥F /∥M∥F and report the results of both relative error and running time in Table 1. For TraceMin 
we observe that for n = 1  000 and r = 10  it gives the result of 1.75 × 10
−7 within 2.94 × 104
seconds  which is really slow compared to our proposal. For n = 1  000 and r = 50  it gives no
result within one week. In Table 1  we ﬁrst observed that for all the cases  the relative error achieved

5

Table 1: Results on synthesized data sets. n is the size of a squared matrix and r is its rank. Rate is the
number of observed entries divided by the size of the matrix  that is  |Ω|=(nm). Time measures the running
time in seconds and Relative error measures ∥AF B
⊤ − M∥F =∥M∥F . The best performance for each setting
are bolded. We do not report the results for FPCA and SVT when n ≥ 5; 000 because they were unable to
ﬁnish the computation after 50 hours.

n

r

Time

Rate
1; 000 10 1:99 × 10
50 9:75 × 10
100 1:900 × 10
5; 000 10 3:96 × 10
50 1:99 × 10
100 3:96 × 10
10; 000 10 2:00 × 10
50 9:98 × 10
100 1:99 × 10
20; 000 10 1:00 × 10
50 4:99 × 10
30; 000 10 6:67 × 10

Alg.
SVT 3:23 × 103
SVT 3:51 × 103
SVT 3:82 × 103

(cid:0)2 Maxide 1:89 × 101 6:42 × 10
(cid:0)2 Maxide 6:44 × 101 5:28 × 10
(cid:0)1 Maxide 1:94 × 102 1:91 × 10
(cid:0)3 Maxide 3:50 × 101 6:38 × 10
(cid:0)2 Maxide 4:56 × 102 1:43 × 10
(cid:0)2 Maxide 1:29 × 103 2:44 × 10
(cid:0)3 Maxide 6:18 × 101 1:63 × 10
(cid:0)3 Maxide 8:39 × 102 9:97 × 10
(cid:0)2 Maxide 4:47 × 103 1:67 × 10
(cid:0)3 Maxide 1:22 × 102 3:54 × 10
(cid:0)3 Maxide 2:16 × 103 4:51 × 10
(cid:0)4 Maxide 4:37 × 102 3:25 × 10

Time

Relative error Algo.
(cid:0)7 FPCA 5:55 × 103 8:79 × 10
8:76 × 104 ALM 2:92 × 101 8:46 × 10
(cid:0)8 FPCA 7:60 × 103 5:53 × 10
2:77 × 105 ALM 7:72 × 101 5:58 × 10
(cid:0)8 FPCA 1:71 × 104 4:63 × 10
7:45 × 104 ALM 8:57 × 101 3:59 × 10
(cid:0)4 ALM 1:24 × 103 9:07 × 10
(cid:0)7 ALM 1:79 × 103 7:26 × 10
(cid:0)8 ALM 2:14 × 103 5:51 × 10
(cid:0)3 ALM 7:16 × 103 9:10 × 10
(cid:0)2 ALM 7:87 × 103 7:19 × 10
(cid:0)7 ALM 9:50 × 103 6:41 × 10
(cid:0)3 ALM 3:62 × 104 9:49 × 10
(cid:0)4 ALM 4:09 × 104 8:51 × 10
(cid:0)3 ALM 8:69 × 104 9:53 × 10

Relative error
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1
(cid:0)1

by the baseline methods is Ω(1)  implying that none of them is able to make accurate recovery of
the target matrix given the small number of observed entries. In contrast  our proposed algorithm
is able to recover the target matrix with small relative error. In addition  our proposed algorithm
is computationally more efﬁcient than the baseline methods. The improvement in computational
efﬁciency becomes more signiﬁcant for large matrices.

4.2 Application to Transductive Incomplete Multi-Label Learning

We evaluate the proposed algorithm for transductive incomplete multi-label learning on thirteen
benchmark data sets  including eleven data sets for web page classiﬁcation from “yahoo.com” [40] 
and two image classiﬁcation data sets NUS-WIDE [14] and Flickr [45]. For the eleven “yahoo.com”
data sets  the number of instances is n = 5  000 and the number of dimensions varies from 438 to
1 047  with the number of labels varies from 21 to 40. Detailed information of these eleven data sets
can be found in [40]. For NUS-WIDE data set  we have n = 209  347 images each represented by
a bag-of-words model with d = 500 visual words  and 81 labels. For the Flickr data set  we only
keep the ﬁrst 1  000 most popular keywords for labels  leaving us with n = 565  444 images  each
represented by a d = 297-dimension vector.

Settings and Baselines For each data set  we randomly sample 10% instances for testing (unla-
beled data) and use the remaining 90% data for training. No label assignment is provided for any test
instance. To create partial label assignments for training data  for each label Cj  we expose the label
assignment of Cj for ω% randomly sampled positive and negative training instances and keep the
label assignment of Cj unknown for the rest of the training instances. To examine the performance of
the proposed algorithm  we vary the ω% in the range {10%  20%  40%}. We repeat each experiment
10 times  and report the result averaged over 10 trials. The regularization parameter λ is selected
{−10;−9;:::;9;10} by cross validation on training data for smaller data sets and set as 1 for larger
from 2
−5  respectively  for the proposed algorithm  and the
ones. Parameters γ and ϵ are set to be 2 and 10
maximum number of iterations is set to be 100. The Average Precision [44]  which measures the
average number of relevant labels ranked before a particular relevant label  is computed over the test
data (the metric on all the data is provided in the supplementary material) and used as our evaluation
metric.
We compare the proposed Maxide method with MC-1 and MC-b  the state-of-the-art methods for
transductive incomplete multi-label learning developed in [17]. In addition  we also compare with
two reference methods for multi-label learning that train one binary classiﬁer for each label; that
is  the Binary Relevance method [7] based on Linear kernel (BR-L) and the method based on RBF
kernel (BR-R)  where the kernel width is set to 1. For the eleven data sets from “yahoo.com” 

6

LIBSVM [13] is used by BR-L and BR-R to learn both a linear and nonlinear SVM classiﬁer. For
the two image data sets  due to their large size  only BR-L method is included in comparison and
LIBLINEAR is used for the implementation of BR-L due to its high efﬁciency for large data sets. A
similar strategy is used to determine the optimal λ as our proposal.

Results Table 2 summarizes the results on transductive incomplete multi-label learning. We ob-
serve that the proposed Maxide algorithm outperforms the baseline methods  for most setting on
several data sets (e.g.  Business  Education  and Recreation)  and the improvements are signiﬁcant.
More impressively  for most data sets  the proposed algorithm is three order faster than MC-1 and
MC-b. For the NUS-WIDE data set  none of MC-1 and MC-b  the two existing matrix completion
based algorithms for transductive incomplete multi-label learning  is able to ﬁnish within one week.
For the Flickr data set  MC-1 and MC-b are not runnable due to the out of memory problem. For the
NUS-WIDE and Flickr data sets  our proposed Maxide method gets an average of more than 50%
improvement against BR-L  the only runnable baseline  on the Average Precision.

5 Conclusion

In this paper  we develop the theory of matrix completion with side information. We show theoreti-
cally that  with side information matrices A ∈ Rn×ra and B ∈ Rm×rb  we can perfectly recover an
n × m rank-r matrix with only O(r(ra + rb) ln(ra + rb) ln(n + m)) observed entries  a signiﬁcant
improvement compared to the sample complexity O(r(n + m) ln2(n + m)) for the standard theory
for matrix completion. We present the Maxide algorithm that can efﬁciently solve the optimization
problem for matrix completion with side information. Empirical studies with synthesized data sets
and transductive incomplete multi-label learning show the promising performance of the proposed
algorithm.

Acknowledgement This research was partially supported by 973 Program (2010CB327903)  NS-
FC (61073097  61273301)  and ONR Award (N000141210431).

References
[1] J. Abernethy  F. Bach  T. Evgeniou  and J.-P. Vert. A new approach to collaborative ﬁltering: Operator

estimation with spectral regularization. JMLR  10:803–826  2009.

[2] R. Adams  G. Dahl  and I. Murray. Incorporating side information in probabilistic matrix factorization

with gaussian processes. In UAI  2010.

[3] D. Agarwal and B.-C. Chen. Regression-based latent factor models. In KDD  2009.
[4] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. MLJ  73(3):243–272  2008.
[5] H. Avron  S. Kale  S. Kasiviswanathan  and V. Sindhwani. Efﬁcient and practical stochastic subgradient

descent for nuclear norm regularization. In ICML  2012.

[6] F. Bach. Consistency of trace norm minimization. JMLR  9:1019–1048  2008.
[7] M. R. Boutell  J. Luo  X. Shen  and C. M. Brown. Learning multi-label scene classiﬁcation. Pattern

Recognition  37(9):1757–1771  2004.

[8] S. Bucak  R. Jin  and A. Jain. Multi-label learning with incomplete class assignments. In CVPR  2011.
[9] R. Cabral  F. Torre  J. Costeira  and A. Bernardino. Matrix completion for multi-label image classiﬁcation.

In NIPS  2011.

[10] J.-F. Cai  E. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[11] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. CACM  55(6):111–119  2012.
[12] E. Cand`es and T. Tao. The power of convex relaxation: near-optimal matrix completion.
IEEE TIT 

56(5):2053–2080  2010.

[13] C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM TIST  2(3):27  2011.
[14] T.-S. Chua  J. Tang  R. Hong  H. Li  Z. Luo  and Y.-T. Zheng. Nus-wide: A real-world web image database

from national university of singapore. In CIVR  2009.

[15] B. Eriksson  L. Balzano  and R. Nowak. High-rank matrix completion and subspace clustering with

missing data. CoRR  2011.

7

Table 2: Results on transductive incomplete multi-label learning. Algo. speciﬁes the name of the algorithms.
Time is the CPU time measured in seconds. AP is Average Precision measured based on test data; the higher the
AP  the better the performance. !% represents the percentage of training instances with observed label assign-
ment for each label. The best result and its comparable ones (pairwise single-tailed t-tests at 95% conﬁdence
level) are bolded.

!% = 40%

!% = 10%

!% = 20%

Data

Arts

Business

Computers

Education

Entertainment

Health

Recreation

Reference

Science

Social

Society

NUS-WIDE

Flickr

Algo.

Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
MC-b
MC-1
BR-R
BR-1
Maxide
BR-1
Maxide
BR-1

time

3:09 × 100
2:47 × 104
2:39 × 104
1:63 × 101
1:77 × 101
3:24 × 100
2:94 × 104
3:25 × 104
1:02 × 101
1:19 × 101
4:67 × 100
5:58 × 104
6:56 × 104
2:34 × 101
2:70 × 101
4:40 × 100
3:82 × 104
4:68 × 104
1:77 × 101
1:94 × 101
2:77 × 100
4:86 × 104
4:40 × 104
1:89 × 101
2:04 × 101
4:31 × 100
4:98 × 104
5:82 × 104
2:03 × 101
2:16 × 101
2:75 × 100
3:56 × 104
3:48 × 104
1:97 × 101
2:24 × 101
5:11 × 100
9:38 × 104
1:11 × 105
2:28 × 101
2:71 × 101
6:21 × 100
6:80 × 104
8:50 × 104
2:93 × 101
3:60 × 101
7:18 × 100
1:71 × 105
2:22 × 105
3:09 × 101
3:71 × 101
3:69 × 100
4:75 × 104
4:14 × 104
2:50 × 101
2:84 × 101
1:47 × 103
1:24 × 102
1:33 × 104
2:48 × 104

AP

0:548
0:428
0:430
0:540
0:540
0:868
0:865
0:865
0:846
0:846
0:635
0:597
0:600
0:622
0:621
0:566
0:472
0:484
0:535
0:535
0:631
0:474
0:489
0:628
0:627
0:725
0:609
0:626
0:725
0:725
0:559
0:381
0:381
0:548
0:547
0:635
0:565
0:576
0:644
0:644
0:513
0:395
0:411
0:506
0:506
0:721
0:582
0:602
0:717
0:717
0:580
0:550
0:550
0:571
0:572

0:513
0:329
0:124
0:064

time

3:60 × 100
1:59 × 104
2:05 × 104
2:98 × 101
3:07 × 101
3:89 × 100
1:83 × 104
2:18 × 104
1:78 × 101
1:96 × 101
5:81 × 100
3:38 × 104
4:40 × 104
4:13 × 101
4:50 × 101
5:41 × 100
2:40 × 104
3:02 × 104
3:16 × 101
3:28 × 101
3:41 × 100
3:13 × 104
4:15 × 104
3:38 × 101
3:44 × 101
5:36 × 100
2:99 × 104
3:82 × 104
3:61 × 101
3:59 × 101
3:38 × 100
2:41 × 104
3:25 × 104
3:48 × 101
3:74 × 101
6:47 × 100
5:38 × 104
6:53 × 104
3:89 × 101
4:34 × 101
7:67 × 100
3:94 × 104
4:97 × 104
5:06 × 101
5:91 × 101
9:09 × 100
9:65 × 104
1:17 × 105
5:35 × 101
6:00 × 101
4:54 × 100
2:93 × 104
3:65 × 104
4:54 × 101
4:92 × 101
2:10 × 103
2:38 × 102
1:89 × 104
4:74 × 104

AP

0:572
0:444
0:494
0:563
0:563
0:860
0:851
0:855
0:841
0:841
0:660
0:599
0:608
0:649
0:648
0:604
0:478
0:536
0:568
0:568
0:650
0:467
0:492
0:638
0:640
0:746
0:607
0:632
0:742
0:741
0:592
0:381
0:430
0:574
0:573
0:666
0:561
0:576
0:670
0:669
0:543
0:403
0:470
0:535
0:535
0:748
0:595
0:625
0:746
0:746
0:594
0:545
0:561
0:590
0:590

0:519
0:398
0:124
0:074

time

4:42 × 100
9:54 × 103
1:27 × 104
5:71 × 101
7:10 × 101
5:04 × 100
1:08 × 104
1:21 × 104
3:32 × 101
4:30 × 101
7:79 × 100
1:87 × 104
2:30 × 104
7:68 × 101
8:25 × 101
6:73 × 100
1:32 × 104
1:55 × 104
6:01 × 101
6:94 × 101
4:56 × 100
1:73 × 104
2:27 × 104
6:47 × 101
6:41 × 101
7:11 × 100
1:71 × 104
2:03 × 104
6:83 × 101
7:05 × 101
4:44 × 100
1:30 × 104
1:90 × 104
6:53 × 101
6:86 × 101
8:49 × 100
2:75 × 104
3:22 × 104
7:08 × 101
7:48 × 101
1:02 × 101
2:06 × 104
2:52 × 104
9:30 × 101
1:04 × 102
1:21 × 101
4:56 × 104
5:41 × 104
9:74 × 101
1:02 × 102
5:80 × 100
1:62 × 104
2:04 × 104
8:59 × 101
9:58 × 101
3:53 × 103
4:81 × 102
2:67 × 104
1:11 × 105

AP

0:596
0:434
0:473
0:574
0:575
0:872
0:858
0:862
0:854
0:854
0:675
0:604
0:618
0:662
0:661
0:618
0:474
0:564
0:583
0:583
0:679
0:468
0:578
0:668
0:667
0:769
0:610
0:645
0:757
0:757
0:614
0:378
0:421
0:596
0:596
0:696
0:575
0:575
0:693
0:692
0:568
0:394
0:414
0:557
0:557
0:754
0:594
0:604
0:751
0:751
0:616
0:552
0:590
0:600
0:601

0:522
0:466
0:124
0:077

[16] Y. Fang and L. Si. Matrix co-factorization for recommendation with rich side information and implicit
feedback. In Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in

8

Recommender Systems  2011.

[17] A. Goldberg  X. Zhu  B. Recht  J.-M. Xu  and R. Nowak. Transduction with matrix completion: Three

birds with one stone. In NIPS  2010.

[18] Y. Guo and D. Schuurmans. Semi-supervised multi-label classiﬁcation - a simultaneous large-margin 

subspace learning approach. In ECML  2012.

[19] P. Jain  P. Netrapalli  and S. Sanghavi. Provable matrix sensing using alternating minimization. In NIPS

Workshop on Optimization for Machine Learning  2012.

[20] A. Jalali  Y. Chen  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex optimization.

In ICML  2011.

[21] S. Ji  L. Tang  S. Yu  and J. Ye. Extracting shared subspace for multi-label classiﬁcation. In KDD  2008.
[22] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In ICML  2009.
[23] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. IEEE TIT  56(6):2980–

2998  2010.

[24] X. Kong  M. Ng  and Z.-H. Zhou. Transductive multi-label learning via label set propagation.

TKDE  25(3):704–719  2013.

IEEE

[25] Z. Lin  M. Chen  L. Wu  and Y. Ma. The augmented lagrange multiplier method for exact recovery of

corrupted low-rank matrices. Technical report  UIUC  2009.

[26] Y. Liu  R. Jin  and L. Yang. Semi-supervised multi-label learning by constrained non-negative matrix

factorization. In AAAI  2006.

[27] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-

tion. Mathematical Programming  128(1-2):321–353  2011.

[28] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning large incom-

plete matrices. JMLR  11:2287–2322  2010.

[29] A. Menon  K. Chitrapura  S. Garg  D. Agarwal  and N. Kota. Response prediction using collaborative

ﬁltering with hierarchies and side-information. In KDD  2011.

[30] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high dimensional

scaling. Annual of Statistics  39(2):1069–1097  2011.

[31] G. Obozinski  B. Taskar  and M. Jordan. Joint covariate selection and joint subspace selection for multiple

classiﬁcation problems. Statistics and Computing  20(2):231–252  2010.

[32] W. Pan  E. Xiang  N. Liu  and Q. Yang. Transfer learning in collaborative ﬁltering for sparsity reduction.

In AAAI  2010.

[33] I. Porteous  A. Asuncion  and M. Welling. Bayesian matrix factorization with side information and

dirichlet process mixtures. In AAAI  2010.

[34] B. Recht. A simpler approach to matrix completion. JMLR  12:3413–3430  2011.
[35] J. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.

ICML  2005.

In

[36] A. Rhode and A. Tsybakov. Estimation of high dimensional low rank matrices. Annual of Statistics 

39(2):887–930  2011.

[37] N. Srebro  Jason D. Rennie  and T. Jaakkola. Maximum-margin matrix factorization. In NIPS. 2005.
[38] Y.-Y. Sun  Y. Zhang  and Z.-H. Zhou. Multi-label learning with weak label. In AAAI  2010.
[39] K.-C. Toh and Y. Sangwoon. An accelerated proximal gradient algorithm for nuclear norm regularized

linear least squares problems. Paciﬁc Journal of Optimization  2010.

[40] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. In NIPS  2002.
[41] K. Weinberger and L. Saul. Unsupervised learning of image manifolds by semideﬁnite programming.

IJCV  70(1):77–90  2006.

[42] J. Yi  T. Yang  R. Jin  A. Jain  and M. Mahdavi. Robust ensemble clustering by matrix completion. In

ICDM  2012.

[43] G. Yu  C. Domeniconi  H. Rangwala  G. Zhang  and Z. Yu. Transductive multi-label ensemble classiﬁca-

tion for protein function prediction. In KDD  2012.

[44] M.-L. Zhang and Z.-H. Zhou. A review on multi-label learning algorithms. IEEE TKDE  in press.
[45] J. Zhuang and S. Hoi. A two-view learning approach for image tag ranking. In WSDM  2011.

9

,Miao Xu
Rong Jin
Zhi-Hua Zhou
Jun Shu
Qi Xie
Lixuan Yi
Qian Zhao
Sanping Zhou
Zongben Xu
Deyu Meng