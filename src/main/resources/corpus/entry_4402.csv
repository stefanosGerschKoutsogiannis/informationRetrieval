2019,Prior-Free Dynamic Auctions with Low Regret Buyers,We study the problem of how to repeatedly sell to a buyer running a no-regret  mean-based algorithm. Previous work [Braverman et al.  2018] shows that it is possible to design effective mechanisms in such a setting that extract almost all of the economic surplus  but these mechanisms require the buyer's values each round to be drawn independently and identically from a fixed distribution. In this work  we do away with this assumption and consider the prior-free setting where the buyer's value each round is chosen adversarially (possibly adaptively). 

We show that even in this prior-free setting  it is possible to extract a $(1-\varepsilon)$-approximation of the full economic surplus for any $\varepsilon > 0$. The number of options offered to a buyer in any round scales independently of the number of rounds $T$ and polynomially in $\varepsilon$. We show that this is optimal up to a polynomial factor; any mechanism achieving this approximation factor  even when values are drawn stochastically  requires at least $\Omega(1/\varepsilon)$ options.

Finally  we examine what is possible when we constrain our mechanism to a natural auction format where overbidding is dominated. Braverman et al. [2018] show that even when values are drawn from a known stochastic distribution supported on $[1/H  1]$  it is impossible in general to extract more than $O(\log\log H / \log H)$ of the economic surplus. We show how to achieve the same approximation factor in the prior-independent setting (where the distribution is unknown to the seller)  and an approximation factor of $O(1 / \log H)$ in the prior-free setting (where the values are chosen adversarially).,Prior-Free Dynamic Auctions with Low Regret

Buyers

Yuan Deng

Duke University

Jon Schneider
Google Research

Balasubramanian Sivan

Google Research

ericdy@cs.duke.edu

jschnei@google.com

balusivan@google.com

Abstract

We study the problem of how to repeatedly sell to a buyer running a no-regret 
mean-based algorithm. Previous work [Braverman et al.  2018] shows that it is
possible to design effective mechanisms in such a setting that extract almost all
of the economic surplus  but these mechanisms require the buyer’s values each
round to be drawn independently and identically from a ﬁxed distribution. In this
work  we do away with this assumption and consider the prior-free setting where
the buyer’s value each round is chosen adversarially (possibly adaptively).
We show that even in this prior-free setting  it is possible to extract a (1 − ε)-
approximation of the full economic surplus for any ε > 0. The number of options
offered to a buyer in any round scales independently of the number of rounds T
and polynomially in ε. We show that this is optimal up to a polynomial factor;
any mechanism achieving this approximation factor  even when values are drawn
stochastically  requires at least Ω(1/ε) options. Finally  we examine what is
possible when we constrain our mechanism to a natural auction format where
overbidding is dominated. Braverman et al. [2018] show that even when values are
drawn from a known stochastic distribution supported on [1/H  1]  it is impossible
in general to extract more than O(log log H/ log H) of the economic surplus. We
show how to achieve the same approximation factor in the prior-independent setting
(where the distribution is unknown to the seller)  and an approximation factor of
O(1/ log H) in the prior-free setting (where the values are chosen adversarially).

1

Introduction

Revenue optimal auction design in settings where a seller interacts repeatedly with a buyer (like in
the sale of Internet ads) is a problem of high commercial relevance. The promise of dynamic auctions 
that allow the linking of buyers’ decisions across time  is the signiﬁcantly higher revenue they can
achieve over running independent/decoupled auctions across time. The technical challenges that
dynamic auctions introduce  along with their practical impact has inspired a lot of recent work in this
area [Papadimitriou et al.  2016  Ashlagi et al.  2016  Mirrokni et al.  2018].

Traditionally  almost all work in dynamic mechanism design operates in the regime where the
players’ types (e.g. bidders’ values) are drawn stochastically from a ﬁxed distribution. In many
situations this is far from a realistic assumption – for example  if the values of a buyer are modelled
as a distribution  this underlying distribution likely drifts over time and is also subject to shocks
determined by uncontrolled exogenous events. But this assumption is also in many ways critical: in a
dynamic mechanism in an adversarial setting  a fully rational buyer (who cares about the effect of his
current action on his future utility) would be unable to compute his future utility at any point of time
in the game and thus unable to meaningfully best-respond.

On the other hand  auctions for digital ads have become increasingly more complex over time. The
design space of dynamic auctions  in which a buyer bids on many items over the course of many

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

rounds  is very rich and has room for exceedingly complex auctions. A bidder may have difﬁculty
behaving fully rationally in such an auction: the bidder may not have accurate priors for bidders 
the bidder may not completely understand the mechanism  and ﬁnding an equilibrium might be
computationally hard. Instead of acting fully rationally  a bidder might instead choose to try to learn
how to bid over time  for example by using a no-regret learning algorithm. Recently  several streams
of work (e.g. Agrawal et al. [2018]  Braverman et al. [2018]) have explored the problem of how
to design dynamic auctions for such bidders. In all cases these works assume  as is standard  that
bidders’ values are stochastically generated. However  one intriguing feature of modelling a bidder
as a learning agent is that it no longer restricts us to the stochastic setting – the actions taken by a
learning algorithm are perfectly well-deﬁned in (and ostensibly even designed for) the prior-free
setting where values are drawn adversarially. This opens a wealth of questions of how to robustly
design dynamic mechanisms that perform well in the worst-case against some class of learning agents.
In this paper  we explore this question for one of the simplest problems in dynamic mechanism design:
repeatedly selling a single item to a single buyer for T rounds.

We build off the setting of [Braverman et al.  2018]  where they model the buyer as a learner running
a mean-based low-regret algorithm. Intuitively  mean-based algorithms prefer to select actions that
have performed historically well on average (it can be shown that many classic learning algorithms 
like EXP3  Multiplicative Weights  and Follow-the-Perturbed-Leader  are all mean-based low-regret
algorithms). In [Braverman et al.  2018]  the authors show that surprisingly  when the buyer’s values
vt ∈ [0  1] are drawn from a ﬁxed distribution  it is possible to design a simple mechanism that obtains

almost the full economic surplus (i.e.  Val = E[Pt vt]) as revenue. Their mechanism  however  relies

crucially on the fact that the buyer’s values are drawn from the same distribution every round. In
particular  it is straightforward to verify that there exist sequences of values for the buyer that result
in this mechanism receiving asymptotically zero total revenue.

In this paper we design mechanisms for this problem in the prior-free setting  when the buyer’s values
vt ∈ [0  1] are chosen adversarially (possibly adaptively). In the course of doing this  we aim to
minimize the complexity of our mechanisms  measured in terms of the number of distinct options (i.e.
“bids”) the mechanism presents to the bidder in any round. We call this quantity the option-complexity
of the mechanism. Note that in mechanisms with high option-complexity it becomes harder to learn
how to bid. If the option-complexity of the mechanism begins to scale with the number of rounds T  
this may even nullify any sort of low-regret or mean-based guarantee the learning algorithm has (it
may not even be possible to explore all potential options).

Upper bound in the adversarial setting: We design a non-adaptive (i.e.  does not use histor-
ical bids/allocation/prices) option-based mechanism that yields a revenue of Val − O(εT ) with

ε3 (cid:17) options  where the instance (v1  · · ·   vT ) is chosen by a (possibly adaptive) adversary

O(cid:16) ln(1/ε)
and Val is the total economic surplus deﬁned by Val = PT

t=1 vt.

Lower bound in the stochastic (and hence adversarial) setting: We show that even if values are
drawn from an unknown stochastic distribution (i.e. in every round the buyer’s value was drawn
independently from some distribution D)  any non-adaptive option-based mechanism needs to offer
at least Ω(1/ε) options to attain a Val − O(εT ) revenue. This implies the option-complexity of our
algorithm is tight up to a polynomial factor in 1/ε.

Upper bound in the stochastic setting with unknown distribution via critical mechanisms: Fi-
nally  although our mechanisms have relatively low option-complexity  they can still appear unnatural
and complex. We examine what is possible by further restricting our mechanisms to critical mecha-
nisms [Braverman et al.  2018]  by imposing the desiderata of individual rationality  monotonicity of
price and allocation in bid  and overbidding being dominated (see Section 2). Braverman et al. [2018]
show that the seller can use a critical mechanism to extract a good revenue but not all of surplus 
in particular showing the seller can always guarantee revenue equal to an O( log log H
log H ) fraction of
total economic surplus when buyer values lie in the interval [ 1
H   1]  and that this competitive ratio
is tight. This critical mechanism requires full knowledge of the value distribution D. We design a
critical mechanism that achieves this same approximation factor  but in a prior-independent setting
where the distribution D is unknown. In addition  we show that it is possible to achieve a slightly

2

worse competitive ratio of O(
prior-free mechanisms for the single-shot instance of this problem.

log H ) in the prior-free (adversarial values) setting by adapting existing

1

We emphasize that all the mechanisms we present are non-adaptive (i.e. allocation and payment rules
at all times are ﬁxed starting at the beginning of the protocol  and are not functions of the historical
bids/allocations/payments) as in [Braverman et al.  2018].

1.1 Related Work

Our work is closely related to the dynamic mechanism design literature  such as [Balseiro et al. 
2017  Liu and Psomas  2017  Agrawal et al.  2018  Mirrokni et al.  2018  Balseiro et al.  2019]  which
studies how to sell items online to a ﬁxed set of strategic buyers  whose valuations are ﬁxed or drawn
from some distributions. However  the buyers are fully strategic such that their bidding strategies aim
to maximize their accumulative utility throughout the auction.

No-regret algorithms were ﬁrst introduced in the context of the multi-armed bandit problem and have
been widely studied (see Bubeck et al. [2012] for a survey). Applications of low-regret learning to
algorithmic game theory are widespread (e.g. [Roughgarden  2012  Syrgkanis and Tardos  2013 
Nekipelov et al.  2015  Daskalakis and Syrgkanis  2016]). Most applications to dynamic auction
design are from the perspective of seller attempting to learn the optimal auction against strategic
buyers [Amin et al.  2013  2014  Cole and Roughgarden  2014  Morgenstern and Roughgarden  2015 
Devanur et al.  2016  Morgenstern and Roughgarden  2016  Gonczarowski and Nisan  2017  Cai and
Daskalakis  2017  Dudík et al.  2017  Drutsa  2017  2018  Liu et al.  2018]. Recent study takes the
perspective of buyers and applies learning algorithms to help them learn how to bid in repeated and
dynamic auctions [Feng et al.  2018  Balseiro et al.  2018].

In contrast to these works  we take the perspective of the sellers to design online auctions against
the buyers who are running no-regret algorithms in bidding. As pointed out in a seminal empirical
work [Nekipelov et al.  2015]  bidders’ behavior on Bing is largely consistent with a no-regret learning
algorithm  which motivates a question of designing a dynamic mechanism against such a no-regret
learning behavior. Braverman et al. [2018] initiated the study of mechanism design against a no-regret
buyer when the buyer’s valuations are drawn from a ﬁxed and known distribution. In contrast to their
works  we design mechanisms against a no-regret buyer in a prior-free / prior-independent setting.

2 Model and Preliminaries

Our setting is similar to the setting considered in [Braverman et al.  2018]: we consider a multiple
round auction where every round a seller attempts to sell an item to a buyer running a low-regret (in
fact  mean-based) algorithm to learn how to bid.

Speciﬁcally  we consider a T -round auction with one buyer and one seller. In each round t  there is
one item for sale. At the beginning of this round  the buyer learns his private valuation vt ∈ V ⊆ [0  1]
for this item. These valuations vt can be generated in one of two ways: (1) Adversarial  where vt is
chosen arbitrarily by a (possibly adaptive) adversary; and (2) Stochastic  where vt is independently
drawn from some distribution D. This distribution D may either be known to the seller or not and we
will mostly consider the case where D is unknown to the seller (i.e.  the prior-independent setting).

For simplicity  we assume the values vt belong to a ﬁnite set V. This is solely for the purpose of
providing a ﬁnite number of different contexts to the buyer’s learning algorithm and otherwise does
not affect our mechanism at all.

To measure the performance of our mechanisms  we compare the revenue extracted from the mecha-
nism to the welfare  the total value the buyer assigns to all the items.

Deﬁnition 1. The welfare Val(v1  · · ·   vT ) is equal to PT

t=1 vt.

The welfare clearly provides an upper bound on the revenue of our mechanisms. In cases where vt is
drawn from some distribution D  we will write Val(D) = Ex∼D[x] · T to denote the expected welfare
under this distribution.

3

2.1 Mechanism format

Since the buyer is running a learning algorithm  it is especially important to specify the manner of
interaction between the buyer and the seller. We consider two classes of mechanisms for the seller:
option-based mechanisms  and critical mechanisms.

In a option-based mechanism  the seller offers the buyer K options (labeled 1 through K) each round.
If the buyer selects choice i at time t  the buyer receives the item with probability ai t and pays a price
pi t. A natural measure of complexity for such mechanisms is the number of options K presented to
the buyer  which we refer to as the option-complexity of the mechanism. Limiting this complexity is
especially important when interacting with learning agents  as they require some time to explore each
option (indeed  as K approaches T   the low regret guarantees of the learning algorithms we consider
become vacuous).

Critical mechanisms [Braverman et al.  2018] are a subset of option-based mechanisms that are
reasonable. In a critical mechanism  the buyer interacts with the mechanism each round by submitting
a bid b. The buyer then receives the item with probability at(b) and pays a price pt(b). These
allocation/payment rules should satisfy the following properties:

• Individual rationality: pt(b) satisﬁes pt(b) ≤ b · at(b)  i.e. a bidder should never be charged

more than their bid in expectation.

• Monotonicity: pt(b) and at(b) are weakly increasing in b  i.e.  submitting a higher bid should

never decrease the winning probability or the payment.

• Overbidding is dominated: If the bidder’s value is v  it should never be in their interest to

submit a bid b > v  i.e. if b > v then v · at(v) − pt(v) > v · at(b) − pt(b) for all t.

In both option-based mechanisms and critical mechanisms  we assume that the seller is completely
non-adaptive and sets the allocation / payment functions at the beginning of the protocol.

2.2 No-regret learner

In contrast to a utility-maximizing buyer  we consider a buyer who follows some no-regret strategy
for the multi-armed bandit problem. In a classic multi-armed bandit problem with T rounds  the
learner (in our setting  the buyer) selects one of K options (‘arms’) on round t and receives a reward
ri t ∈ [0  1] if he selects option i. The rewards can be chosen adversarially and the learner’s objective
is to maximize his total reward.

maxiPT

Let it be the arm pulled by the learner at round t. The regret for a (possibly randomized) strategy
A is deﬁned as the difference between performance of the strategy A and the best arm: Reg(A) =
t=1 ri t − rit t. A strategy A for the multi-armed bandit problem is no-regret if the expected
regret is sub-linear in T   i.e.  E[Reg(A)] = o(T ). In addition to the bandits setting in which the
learner only learns the reward of the arm he pulls  our results also apply to the experts setting in
which the learner can learn the rewards of all arms for every round. In our setting  the buyer learns
ai t and pi t  allowing him to compute the reward as ri t = ai t · vt − pi t. Moreover  the buyer has
the additional information of her value vt  and thus is in fact facing a contextual bandit problem.

Contextual Bandits
In a contextual bandit problem  the learner is additionally provided a context
ct from a ﬁnite set C. The reward of pulling arm i under context c on round t is now given by ri t(c).
In the experts setting  the learner can obtain the values of ri t(ct) for all arms i under context ct after
round t  while the learner only learns ri t(ct) for the arm i he pulls in the bandits setting.

considering the best context-speciﬁc policy π: Reg(M) = maxπ:C→[K]PT

The notion of regret for a strategy M can be easily extended to the contextual bandit problem by
t=1 rπ(ct) t(ct) − rit t(ct).
As before  a strategy M is no-regret if E[Reg(M)] = o(T ). When the size of the context C is a
constant with respect to T   a no-regret strategy M for the contextual bandits can be simply constructed
from a no-regret strategy A for the classic bandit problem: maintain a separate instance of A for
every context c ∈ C [Bubeck et al.  2012].

Among no-regret strategies  we are interested in a special class of mean-based strategies:

Deﬁnition 2 (Mean-based Strategy). Let σi t(c) = Pt

s=1 ri s(c) be the cumulative rewards for
pulling arm i under context c for the ﬁrst t rounds. A strategy is γ-mean-based if whenever σi t(ct) <

4

σj t(ct) − γT   the probability for the strategy to pull arm i on round t is at most γ. A strategy is
mean-based if it is γ-mean-based with γ = o(1).

Intuitively  mean-based strategies are strategies that will pick the arm that historically performs
the best. Braverman et al. [2018] shows that many no-regret algorithms are mean-based  including
commonly used variants of EXP3 (for the bandits setting)  the Multiplicative Weights algorithm (for
the experts setting) and the Follow-the-Perturbed-Leader algorithm (for the experts setting).

3 Option-based Mechanisms

In this section  we demonstrate a mechanism that can extract full welfare from a mean-based no-regret
learner even when the values are chosen adversarially.

3.1 Warm-up: Extracting Full Welfare for V = {1  2}

Consider an additive approximation target ε > 0. It is without loss of generality to consider the case
with 2(1 − ε) > 1: when 2(1 − ε) ≤ 1  the seller can simply implement a scheme with only one
option that always allocates the item and charges a payment 2(1 − ε). We design a option-based
mechanism with K = ⌈ log ε
log(1−ε) ⌉ + 1 choices in addition to the null choice in which the buyer
receives and pays nothing for the entire time horizon. For the 0-th option  the buyer receives the item
with probability a0 t = 1 and pays a price p0 t = 2(1 − ε) for all t. As for the remaining K − 1
(1−ε)i−1 T . We will divide the timeline of the i-th option with 1 ≤ i ≤ K into ﬁve
options  let κi =
sessions (see Table 1 for details).

ε

For convenience  let Si = (κi  κi+1]. Intuitively  the i-th option is active when t ∈ Si  which spans
Li = κi+1 − κi = ε2
(1−ε)i T rounds. Among these Li rounds  the item is always allocated to the buyer
with probability 1 while the payment changes in a way such that: the payment for the ﬁrst εLi rounds
is 0  the payment for the last εLi rounds is 2  and the payment for the remaining rounds is 1.

Session

Start Time

End Time

Allocation Prob.

Payment

∅1
0

1

2
∅2

0
κi
κi + ε3
(1−ε)i T
κi+1 − ε3
(1−ε)i T
κi+1

1
0
Table 1: Construction of the i-th option

T

κi
κi + ε3
(1−ε)i T
κi+1 − ε3
(1−ε)i T
κi+1

0
1

1

0
0

1

2
0

Assume the buyer is running a γ-mean-based algorithm. To analyze the revenue guarantee of our

mechanism  we consider an arbitrary sequence of valuations (v1  · · ·   vT ) and Val = Pt vt. The high

level idea behind this construction is that for the high valuations  i.e  vt = 2  the utility σi t(2) keeps
increasing as t increases for the high option (i = 0) while for the low options (i > 0)  it only increases
within the active period Si. Therefore  with sufﬁciently large t  we have σ0 t(2) > σi t(2) for all
i > 0 and therefore  the buyer with high valuation will play the high option with high probability. As
for vt = 1  the buyer plays the high option with probability at most γ since its payment is too high
and we argue that the buyer will play the i-th option with high probability when t ∈ Si.

High valuation Assume that vt = 2. First notice that the cumulative utility for playing the 0-th
option is σ0 t(2) = εt · 2. Suppose t ∈ Si∗ for some i∗. For i < i∗  the active period of the i-th
option with i < i∗ is already past and the cumulative utility for playing the i-th option is at most

σi t(2) ≤ Li · 2 =

ε2

(1 − ε)i T · 2 ≤

ε2

(1 − ε)i∗−1 T · 2 = ε · κi∗ · 2 = σ0 t(2) − ε · (t − κi∗ ) · 2

As for the i∗-th option  we have σi∗ t(2) ≤ (t − κi∗ ) · 2 = σ0 t(2) − (κi∗ − (1 − ε)t) · 2.
Moreover  for any i-th option with i > i∗  we simply have σi t(2) = 0. Therefore  the buyer
with valuation vt = 2 for t ∈ Si∗ will play the 0-th option with probability at least 1 − Kγ

5

2ε · T < t < κi∗+1 − γ

when εt · 2 > γT   ε · (t − κi∗ ) · 2 > γT   and (κi∗ − (1 − ε)t) · 2 > γT   which implies that
κi∗ + γ
2(1−ε) · T . Therefore  for each time period Si with 1 ≤ i ≤ K  there
are at least Li −(cid:16) γ
2(1−ε)(cid:17) T rounds where the buyer has probability at least 1 − Kγ to play

the 0-th option  which contributes 2(1 − ε) revenue per round. Therefore  the expected revenue loss
from time period Si is at most

2ε + γ

2ε · Li +(cid:18) γ

2ε

+

γ

2(1 − ε)(cid:19) T · 2 + Kγ · Li · 2

where 2ε · Li is the revenue loss of charging 2(1 − ε) and Kγ · Li · 2 is the expected revenue loss
from playing an option other than the 0-th option. Thus  the total expected revenue loss from the
rounds when vt = 2 is at most

(εT ) · 2 +Xi

(cid:20)2ε · Li +(cid:18) γ

2ε

+

γ

2(1 − ε)(cid:19) T · 2 + Kγ · Li · 2(cid:21) = O(εT )

where (εT ) · 2 is the revenue loss from the ﬁrst εT rounds.

Low valuation Assume that vt = 1. First notice that after the ﬁrst εT rounds  the cumulative utility
for playing the 0-th option is σ0 t(1) = (1 − 2(1 − ε))t = −Ω(T ). Since there is a null arm that
provides cumulative utility 0  the buyer’s probability of playing the 0-th option is at most γ.
Suppose t ∈ Si∗ for some i∗. From our construction of the i-th option for any i 6= i∗  the buyer’s
cumulative utility of playing the i-th option is exactly 0: the buyer’s utility gain is 0 in from session
∅1  ∅2  and 1  while her utility gain from session 0 is exactly cancelled out with his utility loss from
session 2  which leads to σi t(1) = 0 for t > κi+1 or t < κi. As for the i∗-th option  we have

σi∗ t(1) = 


ε3

t − κi
for t in session 0
(1−ε)i∗ T for t in session 1
κi+1 − t
for t in session 2

Therefore  once κi + γT < t < κi+1 − γT   the buyer with vt = 1 will play the i∗-th option with
probability 1 − Kγ. Therefore  the expected revenue loss within the time period Si is 2γT + Kγ · Li 
where Kγ · Li is the expected revenue loss from playing an option other than the i∗-th option. Thus 
i=1 Kγ · Li = O(εT ) where

the total revenue loss from the rounds with vt = 1 is at most εT +PK

εT is the revenue loss from the ﬁrst εT rounds.

3.2 Extracting Full Welfare for V = {1  · · ·   H}

the buyer receives and pays nothing for the entire time horizon. For convenience  let Gi = Pi

We provide an option-based mechanism with K = H ·⌈ 3H 2
ε ⌉ options that achieves an additive revenue
loss O(ln H·εT ) for V = {1  · · ·   H}. As usual  we assume that there is always a null choice in which
1
τ be
H +(j−1)· εT
3H 2
ε ⌉. Although κi j only depends on j  we still use the notation κi j for

the sum of the harmonic series up to i and α = 1− 1
where i ∈ V and 1 ≤ j ≤ ⌈ 3H 2
clarity. We will divide the timeline of the (i  j)-th option into ﬁve sessions (see Table 2).

3H . Moreover  κi j = (GH +2α)· εT

τ =1

Session

init

0

ready

1
∅

Start Time

0

α · εT
H

κi j
κi j+1

κi j − (Gi + α) · εT
H

End Time

Allocation Prob.

Payment

κi j − (Gi + α) · εT
H

α · εT
H

κi j
κi j+1

T

0
0
1
1
0

i
0
0
i
H

Table 2: Construction of the (i  j)-th option

Assume the buyer is running a γ-mean-based algorithm. To analyze the revenue guarantee of our

mechanisms  we consider an arbitrary sequence of valuations (v1  · · ·   vT ) and Val = Pt vt.

Intuitively  the (i  j)-th option starts with a init session in which it does not allocate the item but
charges a payment i  followed by a 0 session in which the option allocates and charges nothing.

6

Therefore  the buyer will not play the (i  j)-th option before its ready session. In the ready session 
the option allocates the item for free while in the 1 session  the option allocates the item with a
payment i. Our construction ensures that if vt = i for t ∈ (κi j  κi j+1]  then the buyer will play the
option (i  j) with high probability  which generates revenue i.
Lemma 3. If t ∈ (κi j + γT  κi j+1 − γT ]  then for any option (i′  j′) with i′
σ(i j) t(i) − σ(i′ j ′) t(i) > γT .

6= i or j′

6= j 

Therefore  for vt = i with t ∈ (κi j + γT  κi j+1 − γT ]  the buyer will play option (i  j) with
probability at least 1 − Kγ  which generates revenue i per round. Thus  the revenue loss is at most

H · (GH + 2α) ·

εT
H

+ H · 2γT · K + Kγ · H · T = O(ln H · εT )

H is the revenue loss for the ﬁrst maxi κi 1 = (GH + 2α) · εT

where H · (GH + 2α) · εT
H rounds 
H · 2γT · K is the revenue loss for t ∈ (κi j  κi j + γT ] or t ∈ (κi j+1 − γT  κi j+1]  and Kγ · H · T
is the revenue loss from playing an undesired option.
Theorem 4. If the buyer with V = {1  2  · · ·   H} is running a mean-based algorithm  for any
constant ε > 0  there exists a non-adaptive option-based mechanism with O( H 3 ln H
) options for the
seller which obtains revenue at least Val − O(εT ).

ε

3.3 Extracting Full Welfare for V ⊆ [0  1]

Let ε be parameter for the target additive revenue loss O(εT ). For ease of presentation  we will
rescale V to [0  H] such that H = 1/ε  and thus  it sufﬁces to show that we can obtain O(T ) loss in
the scaled version. First notice that it sufﬁces to consider V ⊆ [1  H] since for all valuations less than
1  we will suffer revenue loss at most 1 from each of them.
Lemma 5. Consider vt such that i < vt < i + 1 and t ∈ (κi j  κi j+1]. Then  for any option (i′  j′)
with i′ 6∈ {i  i + 1} or j′ > j  max{σ(i j) t(vt)  σ(i+1 j) t(vt)} − σ(i′ j ′) t(vt) > γT .

Therefore  with probability at least 1 − Kγ  the buyer satisfying the requirement of Lemma 5 will
play either option (i  j′) or option (i + 1  j′) with j′ ≤ j. Recall that it is in fact that κi j = κi+1 j
for all i. Therefore  if the buyer plays option (i + 1  j)  it will generate revenue i + 1 since option
(i + 1  j) is also in its 1 session. Moreover  if the buyer plays option (i  j′) or (i + 1  j′) with j′ < j 
then the option is already in its ∅ session and the buyer needs to pay H.

Thus  the revenue loss from vt is at most 1. Applying a similar argument as in Section 3.2  we can
conclude that the expected revenue loss is O(T ). Rescale it back to V = [0  1]  we have
Theorem 6. If the buyer with V ⊆ [0  1] is running a mean-based algorithm  for any constant ε > 0 
there exists a non-adaptive option-based mechanism with O( ln 1/ε
) options for the seller which
obtains revenue at least Val − O(εT ).

ε3

Meanwhile  we provide a lower-bound on the option-complexity  which implies the option-complexity
of our algorithm is tight up to a polynomial factor in 1
ε .
Theorem 7. If the buyer with V ⊆ [0  1] is running a mean-based algorithm  an option-based
mechanism  which obtains expected revenue at least Val − O(εT )  must have Ω( 1

ε ) options.

plays the i-th option. Suppose there are K options in total and let Pi(c) = PT

Proof. We ﬁrst prove a lower bound for V = {1  2  · · ·   H} and the theorem will be a simple corollary
of this lower bound. Let Ii t(c) be a binary variable indicating whether the buyer with value vt = c
t=1 Pr[Ii t(c) = 1] · pi t
be the expected total revenue obtained from the i-th option when the buyer’s valuations are vt = c for
all t. Since the expected total revenue is at least Val − O(εT )  when the buyer’s valuations are vt = 1
for all t in which the total expected revenue is at least T − µεT for some constant µ  there must
exist an option i∗ such that Pi∗ (1) ≥ (1−µε)T
. Moreover  let t∗ = sup{t | σi∗ t(1) ≥ −γT }. t∗ is
well-deﬁned since σi∗ 0(1) = 0. Notice that for all t > t∗  since the buyer is running a mean-based
algorithm  we have Pr[Ii∗ t(1)] ≤ γ due to the presence of the null option. Therefore  we have

K

Xt≤t∗

pi∗ t + Xt>t∗

γ · pi∗ t ≥ Pi∗ (1) ≥

⇒ Xt≤t∗

pi∗ t ≥

(1 − µε)T

K

− γHT.

(1 − µε)T

K

7

where we use the fact that 0 ≤ pi∗ t ≤ H. Notice that the cumulative utility σi∗ t∗ (H) is

σi∗ t∗ (H) = Xt≤t∗

H · ai∗ t − pi∗ t = H · σi∗ t∗ (1) + (H − 1) Xt≤t∗

pi∗ t

≥

(H − 1)(1 − µε)T

K

− γH 2T

Consider an environment when the buyer’s valuations are vt = H for all t. Since the buyer is running
a no-regret algorithm  her cumulative utility for the ﬁrst t∗ rounds is at least σi∗ t∗ (H) − o(T ). This
is true because although the standard no-regret guarantee only applies to the ﬁnal round T   the regret
for the ﬁrst t rounds must also be o(T )  for any t < T . For the sake of contradiction  assume that
the regret for the ﬁrst t rounds is Ω(T ). Notice that the no-regret algorithm does not depend on the
future. Therefore  consider an environment where the rewards for all options after round t are set to
be 0  which results in a Ω(T ) regret for the ﬁnal round T . A contradiction.
In addition  notice that the revenue loss from the ﬁrst t∗ rounds is at least the buyer’s cumulative
utility  and thus  the revenue loss is at least σi∗ t∗ (H) − o(T ) = (H−1)T
K − O(εT ). Finally  since
the total revenue loss for T rounds is at least the total revenue loss for the ﬁrst t∗ rounds  in order to
achieve O(εT ) revenue loss  we must have K = Ω( H

ε ).

Observe that our proof only uses two sequences of valuations: a sequence with all 1 and a sequence
of all H. Thus  our lower bound also applies to the stochastic settings with unknown distributions.

4 Critical mechanisms

In this section we examine what the seller can accomplish when restricted to a critical mechanism.

With option-based mechanisms  we have shown in the previous section that it is possible to extract
arbitrarily close to the full welfare even when the buyer’s values are chosen adversarially. In contrast
to this  Braverman et al. [2018] show that with a critical mechanism  it is impossible to achieve even
a constant-factor approximation to the buyer’s welfare  even when the buyer’s values are drawn from
a distribution known to the seller.
Theorem 8 (Corollary C.13 of [Braverman et al.  2018]). Let R(D) be the maximum possible revenue
a seller using a non-adaptive critical mechanism can achieve when the buyer’s values are drawn
independently each round from distribution D. Then the ratio R(D)/Val(D) can grow arbitrarily
small. If D is supported on an interval [1  H]  then this ratio can be as small as O(log log H/ log H).

In Braverman et al. [2018]  the authors also demonstrate how to construct a simple mechanism which
achieves this maximum possible revenue (and hence this O(log log H/ log H) competitive ratio to
the welfare)  but their construction requires detailed knowledge of the distribution D.

4.1 Values from an unknown distribution

We show that it is possible achieve this same competitive ratio to the welfare in the prior-independent
setting  where the seller does not know the distribution D but only a range [1  H] it is supported on. In
our mechanism  at each time t the seller speciﬁes a reserve price f (t)  where f is a decreasing function

C · (1 − η − t

with range [1  H] such that f (t) = max(cid:16)exp(cid:16) 1
T )(cid:17)  1(cid:17)  where η = (1 + log H)−ε
and C = 1−η
1+log H for ε ∈ (0  1). In each round  if the buyer bids above f (t) they receive the item
and pay b; otherwise  they do not receive the item and pay nothing. More formally  the allocation
and payment rules (at(b)  pt(b)) are deﬁned as follows: if b ≥ f (t)  then pt(b) = b  and at(b) = 1;
otherwise  pt(b) = at(b) = 0.
Theorem 9. There is a non-adaptive critical mechanism for the seller which obtains expected revenue
at least O(log log H/ log H)Val(D) from any buyer running a mean-based algorithm whose values
are drawn independently each round from some distribution D supported on [1  H]. This mechanism
depends only on H and not on D.

Consider the function x(v) : [1  H] → [0  T ] where x(v) = 1 − 1
T · minf (t)≤v t. Note that x(v)
equals the number of rounds where a bidder with value v has value higher than the reserve price f (t)

8

(in particular  x(v) is an increasing function of v). It can be shown (Braverman et al. [2018]  Section
C) that if the buyer is mean-based  the revenue obtained by the seller by using such an auction is
given by R(D) = T · Ev∼D [vx(v) − maxw(v − w)x(w)] − o(T ).
Lemma 10. If the seller is using a ﬁrst-price auction with decreasing reserve price 
R(D)/Val(D) is maximized when D is a singleton distribution.

then

Proof of Theorem 9. Note that for this choice of f   x(v) = η+C log v. By Lemma 10  R(D)/Val(D)
is maximized when D is a singleton distribution. We therefore have that:

R(D)

Val(D)T

≥ min

v

vx(v) − maxw(v − w)x(w)

= min

v w<v(cid:16)x(v) −(cid:16)1 −

w

v (cid:17) x(w)(cid:17)

= min

v w<v(cid:16)C log

v
w

v
w
v

+

For a ﬁxed w  this is minimized when v = w(cid:0) η

R(D)

Val(D)T

≥ min

w (cid:16)C log(cid:16) η

C

≥

4.2 Adversarial values

(η + C log w)(cid:17) .
C + log w(cid:1). It follows that
+ log w(cid:17) + 1(cid:17) ≥ C log(cid:16) η
C(cid:17)
= Θ(cid:18) log log H

1 + log H

log H (cid:19) .

(1 − (1 + log H)−ε) log((1 + log H)1−ε)

Additionally  when the buyer’s values are drawn adversarially  we show that it is possible to achieve
a slightly worse competitive ratio of O(1/ log H). This follows naturally from the known fact that
it is possible to achieve the same approximation guarantee against a strategic buyer with value in
[1  H] playing a single-round version of this game (see e.g. Chapter 6 of Hartline [2013]) – we
simply show that if we run this mechanism every round  mean-based buyers will learn to bid in
the same manner as strategic buyers. Our mechanism (equivalent to the mechanism presented in
Theorem 6.5 of Hartline [2013]) is as follows: for each b and for all t  we set the allocation probability
at(b) = (1 + log b)/(1 + log H) and the expected price charged to pt(b) = b/(1 + log H). Note that
this mechanism can also be interpreted as a second-price auction where the seller draws a random
reserve from the distribution with a cumulative density function F (r) = 1+log r
1+log H . It can be seen
that for any v ∈ [1  H]  the expected utility U (v  b) = v · at(b) − pt(b) of bidding b with value v  is
maximized when b = v. A strategic buyer therefore will always bid their value  and pay 1/(1+log H)
of their value in total.

Intuitively  the mean-based guarantee ensures that a mean-based buyer will (most of the time) choose
a bid close to v  and thus it contributes a similar amount of revenue as a strategic buyer.
Theorem 11. There is a non-adaptive critical mechanism for the seller which obtains expected
revenue at least O(1/ log H)Val from any buyer running a mean-based algorithm whose values are
adversarially set but lie in the interval [1  H]. This mechanism depends only on H and not on D.

5 Conclusion

In this work  we design mechanisms against a no-regret  mean-based buyer in prior-independent
and prior-free setting. We show that using option-based mechanism can extract almost full welfare
in a prior-free setting. For critical mechanisms  our mechanism in the prior-independent setting
matches the best-known guarantee for the prior-dependent setting in the literature  and we obtain a
slightly worse guarantee for the prior-free setting. A nature direction for future work is to understand
what can be achieved in an environment with multiple learning buyers. Moreover  while both our
works and [Braverman et al.  2018] focus on the revenue guarantee of the seller against a no-regret
buyer  it is interesting to understand what kinds of the buyer’s learning strategy can lead to a good
utility performance. Furthermore  what combinations of the buyer’s learning strategy and the seller’s
mechanism can achieve a socially desirable outcome?

9

References

Shipra Agrawal  Constantinos Daskalakis  Vahab S. Mirrokni  and Balasubramanian Sivan. Robust
repeated auctions under heterogeneous buyer behavior. In Proceedings of the 2018 ACM Conference
on Economics and Computation  Ithaca  NY  USA  June 18-22  2018  page 171  2018.

Kareem Amin  Afshin Rostamizadeh  and Umar Syed. Learning prices for repeated auctions with
strategic buyers. In Advances in Neural Information Processing Systems  pages 1169–1177  2013.

Kareem Amin  Afshin Rostamizadeh  and Umar Syed. Repeated contextual auctions with strategic

buyers. In Advances in Neural Information Processing Systems  pages 622–630  2014.

Itai Ashlagi  Constantinos Daskalakis  and Nima Haghpanah. Sequential mechanisms with ex-
post participation guarantees. In Proceedings of the 2016 ACM Conference on Economics and
Computation  EC ’16  Maastricht  The Netherlands  July 24-28  2016  pages 213–214  2016.

Santiago Balseiro  Negin Golrezaei  Mohammad Mahdian  Vahab Mirrokni  and Jon Schneider.

Contextual bandits with cross-learning. arXiv preprint arXiv:1809.09582  2018.

Santiago R. Balseiro  Vahab S. Mirrokni  and Renato Paes Leme. Dynamic mechanisms with
martingale utilities. In Proceedings of the 2017 ACM Conference on Economics and Computation 
EC ’17  Cambridge  MA  USA  June 26-30  2017  page 165  2017.

Santiago R. Balseiro  Vahab S. Mirrokni  Renato Paes Leme  and Song Zuo. Dynamic double
auctions: Towards ﬁrst best. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
Discrete Algorithms  SODA 2019  San Diego  California  USA  January 6-9  2019  pages 157–172 
2019.

Mark Braverman  Jieming Mao  Jon Schneider  and Matt Weinberg. Selling to a no-regret buyer. In
Proceedings of the 2018 ACM Conference on Economics and Computation  pages 523–538. ACM 
2018.

Sébastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.

Yang Cai and Constantinos Daskalakis. Learning multi-item auctions with (or without) samples. In

FOCS  2017.

Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings
of the Forty-sixth Annual ACM Symposium on Theory of Computing  STOC ’14  pages 243–252 
New York  NY  USA  2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591867.
URL http://doi.acm.org/10.1145/2591796.2591867.

Constantinos Daskalakis and Vasilis Syrgkanis. Learning in auctions: Regret is hard  envy is
easy. In IEEE 57th Annual Symposium on Foundations of Computer Science  FOCS 2016  9-11
October 2016  Hyatt Regency  New Brunswick  New Jersey  USA  pages 219–228  2016. doi:
10.1109/FOCS.2016.31. URL https://doi.org/10.1109/FOCS.2016.31.

Nikhil R. Devanur  Zhiyi Huang  and Christos-Alexandros Psomas. The sample complexity of
auctions with side information. In Proceedings of the Forty-eighth Annual ACM Symposium on
Theory of Computing  STOC ’16  pages 426–439  New York  NY  USA  2016. ACM. ISBN
978-1-4503-4132-5. doi: 10.1145/2897518.2897553. URL http://doi.acm.org/10.1145/
2897518.2897553.

Alexey Drutsa. Horizon-independent optimal pricing in repeated auctions with truthful and strategic
buyers. In Proceedings of the 26th International Conference on World Wide Web  pages 33–42.
International World Wide Web Conferences Steering Committee  2017.

Alexey Drutsa. Weakly consistent optimal pricing algorithms in repeated posted-price auctions with

strategic buyer. In International Conference on Machine Learning  pages 1318–1327  2018.

Miroslav Dudík  Nika Haghtalab  Haipeng Luo  Robert E. Schapire  Vasilis Syrgkanis  and Jen-

nifer Wortman Vaughan. Oracle-efﬁcient learning and auction design. In FOCS  2017.

10

Zhe Feng  Chara Podimata  and Vasilis Syrgkanis. Learning to bid without knowing your value. In
Proceedings of the 2018 ACM Conference on Economics and Computation  pages 505–522. ACM 
2018.

Yannai A. Gonczarowski and Noam Nisan. Efﬁcient empirical revenue maximization in single-
parameter auction environments. In Proceedings of the 49th Annual ACM SIGACT Symposium
on Theory of Computing  STOC 2017  pages 856–868  New York  NY  USA  2017. ACM. ISBN
978-1-4503-4528-6. doi: 10.1145/3055399.3055427. URL http://doi.acm.org/10.1145/
3055399.3055427.

Jason D Hartline. Mechanism design and approximation. Book draft. October  122  2013.

Jinyan Liu  Zhiyi Huang  and Xiangning Wang. Learning optimal reserve price against non-myopic

bidders. In Advances in Neural Information Processing Systems  pages 2038–2048  2018.

Siqi Liu and Christos-Alexandros Psomas. On the competition complexity of dynamic mechanism

design. CoRR  abs/1709.07955  2017. URL http://arxiv.org/abs/1709.07955.

Vahab S. Mirrokni  Renato Paes Leme  Pingzhong Tang  and Song Zuo. Non-clairvoyant dynamic
mechanism design. In Proceedings of the 2018 ACM Conference on Economics and Computation 
Ithaca  NY  USA  June 18-22  2018  page 169  2018.

Jamie Morgenstern and Tim Roughgarden. The pseudo-dimension of near-optimal auctions. In
Proceedings of the 28th International Conference on Neural Information Processing Systems
- Volume 1  NIPS’15  pages 136–144  Cambridge  MA  USA  2015. MIT Press. URL http:
//dl.acm.org/citation.cfm?id=2969239.2969255.

Jamie Morgenstern and Tim Roughgarden. Learning simple auctions. In Vitaly Feldman  Alexander
Rakhlin  and Ohad Shamir  editors  29th Annual Conference on Learning Theory  volume 49
of Proceedings of Machine Learning Research  pages 1298–1318  Columbia University  New
York  New York  USA  23–26 Jun 2016. PMLR. URL http://proceedings.mlr.press/v49/
morgenstern16.html.

Denis Nekipelov  Vasilis Syrgkanis  and Eva Tardos. Econometrics for learning agents. In Proceedings
of the Sixteenth ACM Conference on Economics and Computation  EC ’15  pages 1–18  New
York  NY  USA  2015. ACM. ISBN 978-1-4503-3410-5. doi: 10.1145/2764468.2764522. URL
http://doi.acm.org/10.1145/2764468.2764522.

Christos Papadimitriou  George Pierrakos  Christos-Alexandros Psomas  and Aviad Rubinstein. On
the complexity of dynamic mechanism design. In Proceedings of the Twenty-seventh Annual
ACM-SIAM Symposium on Discrete Algorithms  SODA ’16  pages 1458–1475  Philadelphia  PA 
USA  2016. Society for Industrial and Applied Mathematics. ISBN 978-1-611974-33-1. URL
http://dl.acm.org/citation.cfm?id=2884435.2884535.

Tim Roughgarden. The price of anarchy in games of incomplete information.

In Proceedings
of the 13th ACM Conference on Electronic Commerce  EC ’12  pages 862–879  New York 
NY  USA  2012. ACM.
ISBN 978-1-4503-1415-2. doi: 10.1145/2229012.2229078. URL
http://doi.acm.org/10.1145/2229012.2229078.

Vasilis Syrgkanis and Eva Tardos. Composable and efﬁcient mechanisms. In Proceedings of the
Forty-ﬁfth Annual ACM Symposium on Theory of Computing  STOC ’13  pages 211–220  New
York  NY  USA  2013. ACM. ISBN 978-1-4503-2029-0. doi: 10.1145/2488608.2488635. URL
http://doi.acm.org/10.1145/2488608.2488635.

11

,Yuan Deng
Jon Schneider
Balasubramanian Sivan