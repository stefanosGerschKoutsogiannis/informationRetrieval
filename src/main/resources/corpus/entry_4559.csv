2019,Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices,We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution $\nu = e^{-f}$ on $\R^n$. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming $\nu$ satisfies log-Sobolev inequality and $f$ has bounded Hessian. Notably  we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in R\'enyi divergence of order $q > 1$ assuming the limit of ULA satisfies either log-Sobolev or Poincar\'e inequality.,Rapid Convergence of the Unadjusted Langevin

Algorithm: Isoperimetry Sufﬁces

Santosh S. Vempala
College of Computing

Georgia Institute of Technology

Atlanta  GA 30332

vempala@gatech.edu

Andre Wibisono

College of Computing

Georgia Institute of Technology

Atlanta  GA 30332

wibisono@gatech.edu

Abstract

We study the Unadjusted Langevin Algorithm (ULA) for sampling from a proba-
bility distribution ⌫ = ef on Rn. We prove a convergence guarantee in Kullback-
Leibler (KL) divergence assuming ⌫ satisﬁes log-Sobolev inequality and f has
bounded Hessian. Notably  we do not assume convexity or bounds on higher deriva-
tives. We also prove convergence guarantees in Rényi divergence of order q > 1
assuming the limit of ULA satisﬁes either log-Sobolev or Poincaré inequality.

1

Introduction

Sampling is a fundamental algorithmic task. Many applications require sampling from probability
distributions in high-dimensional spaces  and in modern applications the probability distributions
are complicated and non-logconcave. While the setting of logconcave functions is well-studied  it
is important to have efﬁcient sampling algorithms with good convergence guarantees beyond the
logconcavity assumption. There is a close interplay between sampling and optimization  either via
optimization as a limit of sampling (annealing) [34  55]  or via sampling as optimization in the space
of distributions [36  62]. Motivated by the widespread use of non-convex optimization and sampling 
there is resurgent interest in understanding non-logconcave sampling.
In this paper we study a simple algorithm  the Unadjusted Langevin Algorithm (ULA)  for sampling
from a target probability distribution ⌫ = ef on Rn. ULA requires oracle access to the gradient rf
of the log density f =  log ⌫. In particular  ULA does not require knowledge of f  which makes it
applicable in practice where we often only know ⌫ up to a normalizing constant.
As the step size ✏ ! 0  ULA recovers the Langevin dynamics  which is a continuous-time stochastic
process in Rn that converges to ⌫. We recall the optimization interpretation of the Langevin dynamics
for sampling as the gradient ﬂow of the Kullback-Leibler (KL) divergence with respect to ⌫ in the
space of probability distributions with the Wasserstein metric [36]. When ⌫ is strongly logconcave 
the KL divergence is a strongly convex objective function  so the Langevin dynamics as gradient
ﬂow converges exponentially fast [6  60]. From the classical theory of Markov chains and diffusion
processes  there are several known conditions milder than logconcavity that are sufﬁcient for rapid
convergence in continuous time. These include isoperimetric inequalities such as Poincaré inequality
or log-Sobolev inequality (LSI). Along the Langevin dynamics in continuous time  Poincaré inequality
implies an exponential convergence rate in 2-divergence  while LSI—which is stronger—implies an
exponential convergence rate in KL divergence (as well as in Rényi divergence).
However  in discrete time  sampling under Poincaré inequality or LSI is a more challenging problem.
ULA is an inexact discretization of the Langevin dynamics  and it converges to a biased limit
⌫✏ 6= ⌫. When ⌫ is strongly logconcave and smooth  it is known how to control the bias and
prove a convergence guarantee on KL divergence along ULA [17  21  22  24]. When ⌫ is strongly

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

logconcave  there are many sampling algorithms with provable rapid convergence; these include
the ball walk and hit-and-run [37  43  44  42] (which give truly polynomial algorithms)  various
discretizations of the overdamped or underdamped Langevin dynamics [21  22  24  8  26] (which
have polynomial dependence on smoothness parameters but low dependence on dimension)  and
the Hamiltonian Monte Carlo [47  48  25  39  16]. It is of great interest to extend these results to
non-logconcave densities ⌫  where existing results require strong assumptions with bounds that grow
exponentially with the dimension or other parameters [2  18  45  49]. There are recent works that
analyze convergence of sampling using various techniques such as reﬂection coupling [28]  kernel
methods [29]  and higher-order integrators [40]  albeit still under some strong conditions such as
distant dissipativity  which is similar to strong logconcavity outside a bounded domain.
In this paper we study the convergence along ULA under minimal (and necessary) isoperimetric
assumptions  namely  LSI and Poincaré inequality. These are sufﬁcient for fast convergence in
continuous time; moreover  in the case of logconcave distribution  the log-Sobolev and Poincaré
constants can be bounded and lead to convergence guarantees for efﬁcient sampling in discrete time.
However  do they sufﬁce on their own without the assumption of logconcavity?
We note that LSI and Poincaré inequality apply to a wider class of measures than logconcave
distributions. In particular  LSI and Poincaré inequality are preserved under bounded perturbation and
Lipschitz mapping  whereas logconcavity is destroyed. Given these properties  it is easy to exhibit
examples of non-logconcave distributions satisfying LSI or Poincaré inequality. For example  we
can take a small perturbation of a convex body to make it nonconvex but still satisﬁes isoperimetry;
then the uniform probability distribution (or a smooth version of it) on the body is not logconcave but
satisﬁes LSI and Poincaré inequality. Similarly  we can start with a strongly logconcave distribution
and make bounded perturbations; then the resulting (normalized) probability distribution is not
logconcave  but it satisﬁes LSI and Poincaré inequality. See Figure 1 for an illustration.

H⌫(⇢k)  e↵✏kH⌫(⇢0) + 8✏nL2
↵ .
16L2n reaches error H⌫(⇢k)   after k  1

↵✏ log 2H⌫ (⇢0)



Figure 1: Illustrations of non-logconcave distributions satisfying isoperimetry: uniform distribution
on a nonconvex set (left) and a perturbation of a logconcave distribution (right).
We measure the mode of convergence using KL divergence and Rényi divergence of order q  1 
which is stronger. Our ﬁrst main result says the only further assumption we need is smoothness. We
say ⌫ = ef is L-smooth if rf is L-Lipschitz. Here H⌫(⇢) is the KL divergence between ⇢ and ⌫.
See Theorem 2 in Section 3.1 for more detail.
Theorem 2. Assume ⌫ = ef satisﬁes log-Sobolev inequality with constant ↵> 0 and is L-smooth.
ULA with step size 0 <✏  ↵

4L2 satisﬁes

iterations.

↵2 to achieve H⌫(⇢k)   using ULA with step size ✏ =⇥( ↵

For 0 << 4n  ULA with ✏  ↵
For example  if we start with a Gaussian ⇢0 = N (x⇤  1
L I) where x⇤ is a stationary point of f (which
we can ﬁnd  e.g.  via gradient descent)  then H⌫(⇢0) = ˜O(n) (see Lemma 1)  and Theorem 2 gives an
iteration complexity of k = ˜⇥ L2n
L2n ).
The result above matches previous known bounds for ULA when ⌫ is strongly logconcave [17  21 
22  24]. Our result complements the recent work of Ma et al. [45] who study the underdamped
version of the Langevin dynamics under LSI and show an iteration complexity for the discrete-
time algorithm that has better dependence on the dimension (p n
 above for ULA) 
but under an additional smoothness assumption (f has bounded third derivatives) and with higher
polynomial dependence on other parameters. Our result also complements the work of Mangoubi
and Vishnoi [49] who study the Metropolis-adjusted version of ULA (MALA) for non-logconcave ⌫
and show a log( 1
 ) iteration complexity from a warm start  under the additional assumption that f
has bounded third and fourth derivatives in an appropriate 1-norm.

 in place of n

2

We note that in general some isoperimetry condition is needed for rapid mixing of Markov chains
(such as Langevin dynamics and ULA)  otherwise there are bad regions in the state space from which
the chains take arbitrarily long to escape. Smoothness or bounded Hessian is a common assumption
needed for the analysis of discrete-time algorithms (such as gradient descent or ULA).
In the second part of this paper  we study the convergence of Rényi divergence of order q > 1
along ULA. Rényi divergence is a family of generalizations of KL divergence [56  59  11]  which
becomes stronger as the order q increases. There are physical and operational interpretations of
Rényi divergence [31  3]. Rényi divergence has been useful in many applications  including for
the exponential mechanism in differential privacy [27  1  12  52]  lattice-based cryptography [4] 
information-theoretic encryption [35]  variational inference [41]  machine learning [32  50]  informa-
tion theory and statistics [20  53]  and black hole physics [23].
Our second result proves a convergence bound for the Rényi divergence of order q > 1. While
Rényi divergence is a stronger measure of convergence than KL divergence  the situation is more
complicated. First  we can hope to converge to the biased limit ⌫✏ only for ﬁnite q for any step
size ✏ (as we illustrate with an example). Second  it is unclear how to bound the Rényi divergence
between ⌫✏ and ⌫. We ﬁrst show the following convergence guarantees of Rényi divergence along the
continuous-time Langevin dynamics under LSI or Poincaré inequality; see Theorem 3 and Theorem 5.
Here Rq ⌫ (⇢) is the Rényi divergence of order q between ⇢ and ⌫.
Theorem 3. Suppose ⌫ satisﬁes LSI with constant ↵> 0. Let q  1. Along the Langevin dynamics 

Rq ⌫ (⇢t)  e 2↵t

q Rq ⌫ (⇢0).

Theorem 5. Suppose ⌫ satisﬁes Poincaré inequality with constant ↵> 0. Let q  2. Along the
Langevin dynamics 

Rq ⌫ (⇢t) (Rq ⌫ (⇢0)  2↵t

q
q Rq ⌫ (⇢0)

e 2↵t

if Rq ⌫ (⇢0)  1 and as long as Rq ⌫ (⇢t)  1 
if Rq ⌫ (⇢0)  1.

Notice that under Poincaré inequality  compared to LSI  the convergence is slower in the beginning
before it becomes exponential. For a reasonable starting distribution (such as a Gaussian centered at
a stationary point)  this leads to an extra factor of n compared to the convergence under LSI. We then
turn to discrete time and show the convergence of Rényi divergence along ULA to the biased limit ⌫✏
under the assumption that ⌫✏ itself satisﬁes either LSI or Poincaré inequality. We combine this with a
decomposition result on Rényi divergence to derive a convergence guarantee for Rényi divergence to
⌫ along ULA; see Theorem 4 and Theorem 6.
In what follows  we review KL divergence and its properties along the Langevin dynamics in Section 2 
and prove a convergence guarantee for KL divergence along ULA under LSI in Section 3. We provide
a review of Rényi divergence and its properties along the Langevin dynamics in Section 4. We then
prove the convergence guarantee for Rényi divergence along ULA under LSI in Section 5  and under
Poincaré inequality in Section 6. We conclude with a discussion in Section 7.

2 Review of KL divergence along Langevin dynamics

In this section we review the deﬁnition of Kullback-Leibler (KL) divergence  log-Sobolev inequality 
and the convergence of KL divergence along the Langevin dynamics in continuous time under
log-Sobolev inequality. See Appendix A.1 for a review on notation.

2.1 KL divergence
Let ⇢  ⌫ be probability distributions on Rn  represented via their probability density functions with
respect to the Lebesgue measure on Rn. We assume ⇢  ⌫ have full support and smooth densities.
Recall the Kullback-Leibler (KL) divergence of ⇢ with respect to ⌫ is

H⌫(⇢) =ZRn

⇢(x)
⌫(x)

⇢(x) log

dx.

(1)

KL divergence is the relative form of Shannon entropy H(⇢) = RRn ⇢(x) log ⇢(x) dx. Whereas

Shannon entropy can be positive or negative  KL divergence is nonnegative and minimized at ⌫:

3

H⌫(⇢)  0 for all ⇢  and H⌫(⇢) = 0 if and only if ⇢ = ⌫. Therefore  KL divergence serves as a
measure of (albeit asymmetric) “distance” of a probability distribution ⇢ from a base distribution ⌫.
KL divergence is a relatively strong measure of distance; for example  Pinsker’s inequality implies
that KL divergence controls total variation distance. Furthermore  under log-Sobolev (or Talagrand)
inequality  KL divergence also controls the quadratic Wasserstein W2 distance  as we review below.
We say ⌫ = ef is L-smooth if f has bounded Hessian: LI  r2f (x)  LI for all x 2 Rn. We
provide the proof of Lemma 1 in Appendix B.1.1.
Lemma 1. Suppose ⌫ = ef is L-smooth. Let ⇢ = N (x⇤  1
L I) where x⇤ is a stationary point of f.
Then H⌫(⇢)  f (x⇤) + n
2.2 Log-Sobolev inequality

2⇡ .
2 log L

Recall we say ⌫ satisﬁes the log-Sobolev inequality (LSI) with a constant ↵> 0 if for all smooth
function g : Rn ! R with E⌫[g2] < 1 

E⌫[g2 log g2]  E⌫[g2] log E⌫[g2] 

2
↵E⌫[krgk2].

Recall the relative Fisher information of ⇢ with respect to ⌫ is

J⌫(⇢) =ZRn

⇢(x)r log

⇢(x)

⌫(x)

2

dx.

(2)

(3)

(4)

LSI is equivalent to the following relation between KL divergence and Fisher information for all ⇢:

1
2↵

J⌫(⇢).

H⌫(⇢) 
⌫ in (2); conversely  to obtain (2) we choose ⇢ = g2⌫

Indeed  to obtain (4) we choose g2 = ⇢
E⌫ [g2] in (4).
LSI is an isoperimetry condition and implies  among others  concentration of measure and sub-
Gaussian tail property [38]. LSI was ﬁrst shown by Gross [30] for the case of Gaussian ⌫. It was
extended by Bakry and Émery [6] to strongly log-concave ⌫; namely  when f =  log ⌫ is ↵-strongly
convex  then ⌫ satisﬁes LSI with constant ↵. However  LSI applies more generally. For example 
the classical perturbation result by Holley and Stroock [33] states that LSI is stable under bounded
perturbation. Furthermore  LSI is preserved under a Lipschitz mapping. In one dimension  there is
an exact characterization of when a probability distribution on R satisﬁes LSI [9]. Moreover  LSI
satisﬁes a tensorization property [38]: If ⌫1 ⌫ 2 satisfy LSI with constants ↵1 ↵ 2 > 0  respectively 
then ⌫1 ⌦ ⌫2 satisﬁes LSI with constant min{↵1 ↵ 2} > 0. Thus  there are many examples of
non-logconcave distributions ⌫ on Rn satisfying LSI (with a constant independent of dimension).
There are also Lyapunov function criteria and exponential integrability conditions that can be used to
verify when a probability distribution satisﬁes LSI; see for example [14  15  51  61  7].

2.2.1 Talagrand inequality
Recall the Wasserstein distance between ⇢ and ⌫ is

W2(⇢  ⌫) = inf
⇧

E⇧[kX  Y k2]

1
2

(5)

where the inﬁmum is over joint distributions ⇧ of (X  Y ) with the correct marginals X ⇠ ⇢  Y ⇠ ⌫.
Recall we say ⌫ satisﬁes Talagrand inequality with a constant ↵> 0 if for all ⇢:

↵
2

W2(⇢  ⌫)2  H⌫(⇢).

(6)

Talagrand’s inequality implies concentration of measure of Gaussian type. It was ﬁrst studied by
Talagrand [58] for Gaussian ⌫  and extended by Otto and Villani [54] to all ⌫ satisfying LSI; namely 
if ⌫ satisﬁes LSI with constant ↵> 0  then ⌫ also satisﬁes Talagrand’s inequality with the same
constant [54  Theorem 1]. Therefore  under LSI  KL divergence controls the Wasserstein distance.
Moreover  when ⌫ is log-concave  LSI and Talagrand’s inequality are equivalent [54  Corollary 3.1].
We recall in Appendix A.2 the geometric interpretation of LSI and Talagrand’s inequality from [54].

4

(7)

(8)

2.3 Langevin dynamics
The Langevin dynamics for target distribution ⌫ = ef is a continuous-time stochastic process
(Xt)t0 in Rn that evolves following the stochastic differential equation:

dXt = rf (Xt) dt + p2 dWt

where (Wt)t0 is the standard Brownian motion in Rn with W0 = 0.
If (Xt)t0 evolves following the Langevin dynamics (7)  then their probability density function
(⇢t)t0 evolves following the Fokker-Planck equation:

@⇢t
@t

= r· (⇢trf ) + ⇢t = r· ⇣⇢tr log

⇢t

⌫⌘ .

Here r· is the divergence and  is the Laplacian operator. We provide a derivation in Appendix A.3.
From (8)  if ⇢t = ⌫  then @⇢t
@t = 0  so ⌫ is the stationary distribution for the Langevin dynamics (7).
Moreover  the Langevin dynamics brings any distribution Xt ⇠ ⇢t closer to the target distribution ⌫ 
as the following lemma shows.
Lemma 2. Along the Langevin dynamics (7) (or equivalently  the Fokker-Planck equation (8)) 

d
dt

H⌫(⇢t) = J⌫(⇢t).

(9)

We provide the proof of Lemma 2 in Appendix B.1.2. Since J⌫(⇢)  0  the identity (9) shows KL
divergence is decreasing along the Langevin dynamics  so indeed the distribution ⇢t converges to ⌫.

2.3.1 Exponential convergence of KL divergence along Langevin dynamics under LSI
When ⌫ satisﬁes LSI  KL divergence converges exponentially fast along the Langevin dynamics.
Theorem 1. Suppose ⌫ satisﬁes LSI with constant ↵> 0. Along the Langevin dynamics (7) 

H⌫(⇢t)  e2↵tH⌫(⇢0).

(10)

Furthermore  W2(⇢t ⌫ ) q 2

↵ H⌫(⇢0) e↵t.

We provide the proof of Theorem 1 in Appendix B.1.3. We also recall the optimization interpretation
of Langevin dynamics as the gradient ﬂow of KL divergence in the space of distributions with
the Wasserstein metric [36  60  54]. Then the exponential convergence rate in Theorem 1 is a
manifestation of the general fact that gradient ﬂow converges exponentially fast under gradient
domination condition. This provides a justiﬁcation for using the Langevin dynamics for sampling
from ⌫  as a natural steepest descent ﬂow that minimizes the KL divergence H⌫.

3 Unadjusted Langevin Algorithm

Suppose we wish to sample from a smooth target probability distribution ⌫ = ef in Rn. The
Unadjusted Langevin Algorithm (ULA) with step size ✏> 0 is the discrete-time algorithm

xk+1 = xk  ✏rf (xk) + p2✏z k

(11)
where zk ⇠N (0  I) is an independent standard Gaussian random variable in Rn. Let ⇢k denote the
probability distribution of xk that evolves following ULA.
As ✏ ! 0  ULA recovers the Langevin dynamics (7) in continuous-time. However  for ﬁxed ✏> 0 
ULA converges to a biased limiting distribution ⌫✏ 6= ⌫. Therefore  KL divergence H⌫(⇢k) does not
tend to 0 along ULA  as it has an asymptotic bias H⌫(⌫✏) > 0.
↵ I). The ULA iteration is xk+1 = (1 ✏↵)xk +p2✏zk. For 0 <✏< 2
Example 1. Let ⌫ = N (0  1
↵ 
2 ) and the bias is H⌫(⌫✏) = n
2 ). In particular 
the limit is ⌫✏ = N0 
2
↵(1 ✏↵
H⌫(⌫✏)  n✏2↵2
2 )2 = O(✏2).
16(1 ✏↵

2 ) + log(1  ✏↵

✏↵
2(1 ✏↵

1

5

3.1 Convergence of KL divergence along ULA under LSI
When ⌫ satisﬁes LSI and a smoothness condition  we can prove a convergence guarantee in KL
divergence along ULA. Recall we say ⌫ = ef is L-smooth if LI  r2f (x)  LI for all x 2 Rn.
A key part in our analysis is the following lemma which bounds the decrease in KL divergence along
one iteration of ULA. Here xk+1 ⇠ ⇢k+1 is the output of one step of ULA (11) from xk ⇠ ⇢k.
Lemma 3. Suppose ⌫ satisﬁes LSI with constant ↵> 0 and is L-smooth. If 0 <✏  ↵
each step of ULA (11) 

4L2   then along

H⌫(⇢k+1)  e↵✏H⌫(⇢k) + 6✏2nL2.

(12)

We provide the proof of Lemma 3 in Appendix B.2.1. The proof of Lemma 3 compares the evolution
of KL divergence along one step of ULA with the evolution along Langevin dynamics in continuous
time (which converges exponentially fast under LSI)  and bounds the discretization error; see Figure 2
for an illustration. This comparison technique has been used in many papers. Our proof structure is
similar to that of Cheng and Bartlett [17]  whose analysis needs ⌫ to be strongly logconcave.
With Lemma 3  we can prove our main result on the convergence rate of ULA under LSI. We provide
the proof of Theorem 2 in Appendix B.2.2.
Theorem 2. Assume ⌫ = ef satisﬁes log-Sobolev inequality with constant ↵> 0 and is L-smooth.
ULA with step size 0 <✏  ↵

4L2 satisﬁes

H⌫(⇢k)  e↵✏kH⌫(⇢0) + 8✏nL2
↵ .
16L2n reaches error H⌫(⇢k)   after k  1



iterations.

↵✏ log 2H⌫ (⇢0)

L I)  where x⇤ is a stationary point of f (which we can ﬁnd 
2⇡ = ˜O(n) by Lemma 1. Theorem 2

For 0 << 4n  ULA with ✏  ↵
L2n. Suppose
In particular  suppose < 4n and we choose the largest permissible step size ✏ =⇥ ↵
we start with a Gaussian ⇢0 = N (x⇤  1
e.g.  via gradient descent)  so H⌫(⇢0)  f (x⇤) + n
↵2⌘. Since LSI implies
states that to achieve H⌫(⇢k)    ULA has iteration complexity k = ˜⇥⇣ L2n
Talagrand’s inequality  Theorem 2 also yields a convergence guarantee in Wasserstein distance. As
k ! 1  Theorem 2 implies the following bound on the bias of ULA under LSI. However  we note
the bound O(✏) may be loose  since from Example 1 we see H⌫(⌫✏) =⇥( ✏2) in Gaussian case.
Corollary 1. Suppose ⌫ satisﬁes LSI with constant ↵> 0 and is L-smooth. For 0 <✏  ↵
4L2   the
biased limit ⌫✏ of ULA with step size ✏ satisﬁes H⌫(⌫✏)  8nL2✏
4 Review of Rényi divergence along Langevin dynamics

and W2(⌫  ⌫✏)2  16nL2✏

2 log L

↵2

↵

.

4.1 Rényi divergence
Rényi divergence [56] is a family of generalizations of KL divergence. See [59  11] for properties of
Rényi divergence.
For q > 0  q 6= 1  the Rényi divergence of order q of a probability distribution ⇢ with respect to ⌫ is
(13)

log Fq ⌫ (⇢)

Rq ⌫ (⇢) =

1
q  1

⌫(x)

6

where

Fq ⌫ (⇢) = E⌫h⇣ ⇢

⌫⌘qi =ZRn

⇢(x)q

⌫(x)q dx =ZRn

⇢(x)q
⌫(x)q1 dx.

(14)

q1 logR ⇢(x)q dx. The case
Rényi divergence is the relative form of Rényi entropy [56]: Hq(⇢) = 1
q = 1 is deﬁned via limit  and recovers the KL divergence (1): R1 ⌫(⇢) = limq!1 Rq ⌫ (⇢) = H⌫(⇢).
Rényi divergence has the property that Rq ⌫ (⇢)  0 for all ⇢  and Rq ⌫ (⇢) = 0 if and only if ⇢ = ⌫.
Furthermore  the map q 7! Rq ⌫ (⇢) is increasing (see Section B.3.1). Therefore  Rényi divergence
provides an alternative measure of “distance” of ⇢ from ⌫  which becomes stronger as q increases. In
⌫(x) is ﬁnite if and only if ⇢ is warm relative to ⌫. It is

= log supx

⇢(x)

possible that Rq ⌫ (⇢) = 1 for large enough q  as the following example shows.

particular  R1 ⌫(⇢) = log ⇢
⌫1

22   then Rq ⌫ (⇢) = 1.

2 log 2

2  n

Example 2. Let ⇢ = N (0  2I) and ⌫ = N (0  2I). If 2 > 2 and q  2
Otherwise  Rq ⌫ (⇢) = n

2(q1) logq  (q  1) 2
2.
The following is analogous to Lemma 1. We provide the proof of Lemma 4 in Appendix B.3.2.
Lemma 4. Suppose ⌫ = ef is L-smooth. Let ⇢ = N (x⇤  1
Then for all q  1  Rq ⌫ (⇢)  f (x⇤) + n
4.1.1 Log-Sobolev inequality
For q > 0  we deﬁne the Rényi information of order q of ⇢ with respect to ⌫ as

2⇡ .
2 log L

L I) where x⇤ is a stationary point of f.

The case q = 1 recovers relative Fisher information (3): G1 ⌫(⇢) = E⌫h ⇢
We have the following relation under log-Sobolev inequality. Note the case q = 1 recovers LSI (4).
We provide the proof of Lemma 5 in Appendix B.3.3.
Lemma 5. Suppose ⌫ satisﬁes LSI with constant ↵> 0. Let q  1. For all ⇢ 

4

(15)

2
q2 E⌫hr⇣ ⇢
⌫⌘ q
2i.
⌫2i = J⌫(⇢).
⌫r log ⇢

Gq ⌫ (⇢) = E⌫h⇣ ⇢

⌫⌘qr log

⇢

⌫

2i = E⌫h⇣ ⇢

⌫⌘q2r

⇢

⌫

2i =

Gq ⌫ (⇢)
Fq ⌫ (⇢) 

2↵
q2 Rq ⌫ (⇢).

(16)

4.2 Langevin dynamics
Along the Langevin dynamics (7) for ⌫  we can compute the rate of change of the Rényi divergence.
Lemma 6. For all q > 0  along the Langevin dynamics (7) 

d
dt

Rq ⌫ (⇢t) = q

Gq ⌫ (⇢t)
Fq ⌫ (⇢t)

.

(17)

dt Rq ⌫ (⇢t)  0  so Rényi
We provide the proof of Lemma 6 in Appendix B.3.4. In particular  d
divergence is always decreasing along the Langevin dynamics. Furthermore  analogous to how the
Langevin dynamics is the gradient ﬂow of KL divergence under the Wasserstein metric  one can
also show that the Langevin dynamics is the the gradient ﬂow of Rényi divergence with respect to a
suitably deﬁned metric (which depends on ⌫) on the space of distributions; see [13].

4.2.1 Convergence of Rényi divergence along Langevin dynamics under LSI
When ⌫ satisﬁes LSI  Rényi divergence converges exponentially fast along the Langevin dynamics.
Note the case q = 1 recovers the exponential convergence rate of KL divergence from Theorem 1.
Theorem 3. Suppose ⌫ satisﬁes LSI with constant ↵> 0. Let q  1. Along the Langevin dynamics 

Rq ⌫ (⇢t)  e 2↵t

q Rq ⌫ (⇢0).

We provide the proof of Theorem 3 in Appendix B.3.5. Theorem 3 shows that if the initial Rényi diver-
gence is ﬁnite  then it converges exponentially fast. However  even if initially the Rényi divergence is
1  it will be ﬁnite along the Langevin dynamics  after which time Theorem 3 applies. This is because
when ⌫ satisﬁes LSI  the Langevin dynamics satisﬁes a hypercontractivity property [30  10  60]; see
Section B.3.6. Furthermore  as shown in [13]  we can combine the exponential convergence rate
above with the hypercontractivity property to improve the exponential rate to be 2↵  independent of
q  at the cost of some initial waiting time; here we leave the rate as above for simplicity.

5 Rényi divergence along ULA

In this section we prove a convergence guarantee for Rényi divergence along ULA under the
assumption that the biased limit satisﬁes LSI. As before  let ⌫ = ef   and let ⌫✏ denote the biased
limit of ULA (11) with step size ✏> 0. We note that the bias Rq ⌫ (⌫✏) may be 1 for large enough q.

7

Example 3. As in Examples 1 and 2  let ⌫ = N (0  1

Rq ⌫ (⌫✏) =( n

2(q1)q log1  ✏↵

1

↵ I)  so ⌫✏ = N0 
2  log1  q✏↵
2 

1

2 ) I. The bias is

↵(1 ✏↵
if 1 < q < 2
✏↵  
if q  2
✏↵ .

Thus  for each ﬁxed q > 1  there is an asymptotic bias Rq ⌫ (⌫✏) which is ﬁnite for small enough ✏.
In Example 3 we have Rq ⌫ (⌫✏) = O(✏2). In general  we assume for each q > 1 there is a growth
function gq(✏) that controls the bias: Rq ⌫ (⌫✏)  gq(✏) for small ✏> 0  and lim✏!0 gq(✏) = 0.
5.1 Decomposition of Rényi divergence
For order q > 1  we have the following decomposition of Rényi divergence.
Lemma 7. Let q > 1. For all probability distribution ⇢ 

Rq ⌫ (⇢) ✓ q  1

q  1◆ R2q ⌫✏(⇢) + R2q1 ⌫(⌫✏).

2

(18)

We provide the proof of Lemma 7 in Appendix B.4.1. The ﬁrst term in the bound above is the
Rényi divergence with respect to the biased limit  which converges exponentially fast under LSI (see
Lemma 8). The second term in (18) is the bias  which is controlled by the growth function g2q1(✏).
5.2 Rapid convergence of Rényi divergence with respect to ⌫✏ along ULA
We show Rényi divergence with respect to the biased limit ⌫✏ converges exponentially fast along
ULA  assuming ⌫✏ itself satisﬁes LSI.
Assumption 1. The probability distribution ⌫✏ satisﬁes LSI with a constant  ⌘ ✏ > 0.
We can verify Assumption 1 in the Gaussian case. However  it is unclear how to verify Assumption 1
in general. One might hope to prove that if ⌫ satisﬁes LSI  then Assumption 1 holds.
2 ) I satisﬁes LSI with  = ↵(1  ✏↵
Example 4. Let ⌫ = N (0  1
Under Assumption 1  we can prove an exponential convergence rate to the biased limit ⌫✏.
Lemma 8. Assume Assumption 1. Suppose ⌫ = ef is L-smooth  and let 0 <✏  minn 1
For q  1  along ULA (11) 

↵ I)  so ⌫✏ = N0 

9o.

↵(1 ✏↵

3L   1

2 ).

1

Rq ⌫✏(⇢k)  e ✏k

q Rq ⌫✏(⇢0).

(19)

We provide the proof of Lemma 8 in Appendix B.4.2. In the proof of Lemma 8  we decompose each
step of ULA as a sequence of two operations; see Figure 3 for an illustration. In the ﬁrst part we
take a gradient step  which is a deterministic bijective map  so it preserves Rényi divergence. In the
second part we add an independent Gaussian  which is evolution along the heat ﬂow  and we derive a
formula on the decrease in Rényi divergence (which is similar to (17) along the Langevin dynamics).

5.3 Convergence of Rényi divergence along ULA under LSI
We combine Lemma 7 and Lemma 8 to obtain the following characterization of the convergence of
Rényi divergence along ULA under LSI. We provide the proof of Theorem 4 in Appendix B.4.3.
3L   1

Theorem 4. Assume Assumption 1. Suppose ⌫ = ef is L-smooth  and let 0 <✏  minn 1
Let q > 1  and suppose R2q ⌫✏(⇢0) < 1. Then along ULA (11) 
q  1◆ R2q ⌫✏(⇢0)e ✏k
2q1 

q () = sup{✏> 0 : gq(✏)  }. Theorem 4 states that to achieve Rq ⌫ (⇢k)   

✏ log R2q ⌫✏ (⇢0)
2✏-smooth  so if we choose ⇢0

Rq ⌫ (⇢k) ✓ q  1

2  for k = O 1

it sufﬁces to run ULA with step size ✏ =⇥ min 1
2 < 1

iterations. Suppose  is small so g1

2q1 

L. Note ⌫✏ is 1

2q + g2q1(✏).

For > 0  let g1

9o.

L   g1

(20)

2



8

1

g1

then g1
in Example 3  then g1

to be a Gaussian with covariance 2✏I  we have R2q ⌫✏(⇢0) = ˜O(n) by Lemma 4. Therefore 

Theorem 4 yields an iteration complexity of k = ˜O

q () =⌦( )  so the iteration complexity is k = ˜O 1

q () =⌦( p)  so the iteration complexity is k = ˜O 1

2q1(/2). For example  if gq(✏) = O(✏) 
 with ✏ =⇥( ). If gq(✏) = O(✏2)  as
p with ✏ =⇥( p).
6 Poincaré inequality
We recall ⌫ satisﬁes Poincaré inequality (PI) with a constant ↵> 0 if for all smooth g : Rn ! R 
(21)
where Var⌫(g) = E⌫[g2]E⌫[g]2 is the variance of g under ⌫. Poincaré inequality is an isoperimetry
condition which is weaker than LSI. LSI implies PI with the same constant; in fact  PI is a linearization
of LSI (4)  i.e.  when ⇢ = (1+⌘g)⌫ as ⌘ ! 0 [57  60]. Furthermore  it is known Talagrand’s inequality
implies PI with the same constant  and PI is also a linearization of Talagrand’s inequality [54].
Poincaré inequality is better behaved than LSI [15]  and there are various Lyapunov criteria and
integrability conditions to verify when a distribution satisﬁes Poincaré inequality [5  51  19].

Var⌫(g)  1

↵E⌫[krgk2]

e 2↵t

q
q Rq ⌫ (⇢0)

6.1 Convergence of Rényi divergence along Langevin dynamics under Poincaré inequality
When ⌫ satisﬁes Poincaré inequality  Rényi divergence converges along the Langevin dynamics. The
convergence is initially linear  then becomes exponential once Rényi divergence falls below 1.
Theorem 5. Suppose ⌫ satisﬁes Poincaré inequality with constant ↵> 0. Let q  2. Along the
Langevin dynamics 

Rq ⌫ (⇢t) (Rq ⌫ (⇢0)  2↵t

if Rq ⌫ (⇢0)  1 and as long as Rq ⌫ (⇢t)  1 
if Rq ⌫ (⇢0)  1.
We provide the proof of Theorem 5 in Appendix B.5.2. Theorem 5 states that starting from Rq ⌫ (⇢0) 
1  the Langevin dynamics reaches Rq ⌫ (⇢t)   in t  O q

6.2 Rapid convergence of Rényi divergence with respect to ⌫✏ along ULA
We assume the biased limit ⌫✏ satisﬁes Poincaré inequality.
Assumption 2. The distribution ⌫✏ satisﬁes Poincaré inequality with a constant  ⌘ ✏ > 0.
Under Assumption 2 we can show Rényi divergence with respect to ⌫✏ converges at a rate similar to
the Langevin dynamics; see Lemma 18 in Appendix B.5.3.

↵Rq ⌫ (⇢0) + log 1

 time.

6.3 Convergence of Rényi divergence along ULA under Poincaré inequality
We combine Lemma 7 and Lemma 18 to obtain the following convergence of Rényi divergence along
ULA under Poincaré inequality. We provide the proof of Theorem 6 in Appendix B.5.4.

Theorem 6. Assume Assumption 2. Suppose ⌫ = ef is L-smooth  and let 0 <✏  min 1
Let q > 1 and assume 1  R2q ⌫✏(⇢0) < 1. Along ULA (11)  for k  k0 := 2q

9 .
3L   1
✏ (R2q ⌫✏(⇢0)  1) 
(22)

Rq ⌫ (⇢k) ✓ q  1

q  1◆ e ✏(kk0)

2

2q + g2q1(✏).

This yields an iteration complexity for ULA under Poincaré which is a factor of n larger than the
complexity under LSI; see Appendix B.5.5.

7 Discussion

In this paper we proved convergence guarantees on KL and Rényi divergence along ULA under
isoperimetry and bounded Hessian  without assuming convexity or bounds on higher derivatives.
It would be interesting to verify when Assumptions 1 and 2 hold or whether they follow from
isoperimetry and bounded Hessian of the target density. Another intriguing question is whether there
is an afﬁne-invariant version of the Langevin dynamics. This might lead to a sampling algorithm with
logarithmic dependence on smoothness parameters  rather than the current polynomial dependence.

9

Acknowledgment
The ﬁrst author was supported in part by NSF awards CCF-1563838 and CCF-1717349. The authors
would like to thank Kunal Talwar for explaining the application of Rényi divergence to data privacy.
The authors thank Yu Cao  Jianfeng Lu  and Yulong Lu for alerting us to their work [13] on Rényi
divergence. The authors also thank Xiang Cheng and Peter Bartlett for helpful comments on an
earlier version of this paper.

References
[1] Martin Abadi  Andy Chu  Ian Goodfellow  H Brendan McMahan  Ilya Mironov  Kunal Talwar  and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security  pages 308–318. ACM  2016.

[2] David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings
of the Twenty-third Annual ACM Symposium on Theory of Computing  STOC ’91  pages 156–163  New
York  NY  USA  1991. ACM.

[3] John C Baez. Rényi entropy and free energy. arXiv preprint arXiv:1102.2098  2011.

[4] Shi Bai  Tancrède Lepoint  Adeline Roux-Langlois  Amin Sakzad  Damien Stehlé  and Ron Steinfeld.
Improved security proofs in lattice-based cryptography: using the rényi divergence rather than the statistical
distance. Journal of Cryptology  31(2):610–640  2018.

[5] Dominique Bakry  Franck Barthe  Patrick Cattiaux  Arnaud Guillin  et al. A simple proof of the Poincaré
inequality for a large class of probability measures. Electronic Communications in Probability  13:60–66 
2008.

[6] Dominique Bakry and Michel Émery. Diffusions hypercontractives. In Séminaire de Probabilités XIX

1983/84  pages 177–206. Springer  1985.

[7] Jean-Baptiste Bardet  Nathaël Gozlan  Florent Malrieu  and Pierre-André Zitt. Functional inequalities
for Gaussian convolutions of compactly supported measures: Explicit bounds and dimension dependence.
Bernoulli  24(1):333–353  2018.

[8] Espen Bernton. Langevin Monte Carlo and JKO splitting. In Conference On Learning Theory  COLT 2018 

Stockholm  Sweden  6-9 July 2018  pages 1777–1798  2018.

[9] Sergej G Bobkov and Friedrich Götze. Exponential integrability and transportation cost related to logarith-

mic Sobolev inequalities. Journal of Functional Analysis  163(1):1–28  1999.

[10] Sergey G Bobkov  Ivan Gentil  and Michel Ledoux. Hypercontractivity of Hamilton–Jacobi equations.

Journal de Mathématiques Pures et Appliquées  80(7):669–696  2001.

[11] SG Bobkov  GP Chistyakov  and Friedrich Götze. Rényi divergence and the central limit theorem. The

Annals of Probability  47(1):270–323  2019.

[12] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simpliﬁcations  extensions  and lower

bounds. In Theory of Cryptography Conference  pages 635–658. Springer  2016.

[13] Yu Cao  Jianfeng Lu  and Yulong Lu. Exponential decay of Rényi divergence under Fokker–Planck

equations. Journal of Statistical Physics  pages 1–13  2018.

[14] Djalil Chafaï. Entropies  convexity  and functional inequalities: On -entropies and -Sobolev inequalities.

Journal of Mathematics of Kyoto University  44(2):325–363  2004.

[15] Djalil Chafai and Florent Malrieu. On ﬁne properties of mixtures with respect to concentration of measure
and Sobolev type inequalities. In Annales de l’IHP Probabilités et statistiques  volume 46  pages 72–96 
2010.

[16] Zongchen Chen and Santosh S. Vempala. Optimal convergence rate of Hamiltonian Monte Carlo for

strongly logconcave distributions. arXiv preprint arXiv:1905.02313  2019.

[17] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Firdaus Janoos 
Mehryar Mohri  and Karthik Sridharan  editors  Proceedings of Algorithmic Learning Theory  volume 83
of Proceedings of Machine Learning Research  pages 186–211. PMLR  07–09 Apr 2018.

10

[18] Xiang Cheng  Niladri S Chatterji  Yasin Abbasi-Yadkori  Peter L Bartlett  and Michael I Jordan. Sharp
convergence rates for Langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648 
2018.

[19] Thomas A Courtade. Bounds on the Poincaré constant for convolution measures. arXiv preprint

arXiv:1807.00027  2018.

[20] Imre Csiszár. Generalized cutoff rates and Rényi’s information measures. IEEE Transactions on information

theory  41(1):26–34  1995.

[21] Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte
Carlo and gradient descent. In Proceedings of the 2017 Conference on Learning Theory  volume 65 of
Proceedings of Machine Learning Research  pages 678–689. PMLR  07–10 Jul 2017.

[22] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with

inaccurate gradient. Stochastic Processes and their Applications  2019.

[23] Xi Dong. The gravity dual of Rényi entropy. Nature communications  7:12472  2016.

[24] Alain Durmus  Szymon Majewski  and Bła˙zej Miasojedow. Analysis of Langevin Monte Carlo via convex

optimization. arXiv preprint arXiv:1802.09188  2018.

[25] Alain Durmus  Eric Moulines  and Eero Saksman. On the convergence of Hamiltonian Monte Carlo. arXiv

preprint arXiv:1705.00166  2017.

[26] Raaz Dwivedi  Yuansi Chen  Martin J. Wainwright  and Bin Yu. Log-concave sampling: Metropolis-
Hastings algorithms are fast! In Conference On Learning Theory  COLT 2018  Stockholm  Sweden  6-9
July 2018  pages 793–797  2018.

[27] Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887 

2016.

[28] Andreas Eberle  Arnaud Guillin  and Raphael Zimmer. Couplings and quantitative contraction rates for

Langevin dynamics. The Annals of Probability  47(4):1982–2010  2019.

[29] Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In Doina Precup and
Yee Whye Teh  editors  Proceedings of the 34th International Conference on Machine Learning  volume 70
of Proceedings of Machine Learning Research  pages 1292–1301  International Convention Centre  Sydney 
Australia  06–11 Aug 2017. PMLR.

[30] Leonard Gross. Logarithmic Sobolev inequalities. American Journal of Mathematics  97(4):1061–1083 

1975.

[31] Peter Harremoës. Interpretations of Rényi entropies and divergences. Physica A: Statistical Mechanics

and its Applications  365(1):57–62  2006.

[32] Yun He  A Ben Hamza  and Hamid Krim. A generalized divergence measure for robust image registration.

IEEE Transactions on Signal Processing  51(5):1211–1220  2003.

[33] Richard Holley and Daniel Stroock. Logarithmic Sobolev inequalities and stochastic Ising models. Journal

of Statistical Physics  46(5):1159–1194  1987.

[34] Richard Holley and Daniel Stroock. Simulated annealing via Sobolev inequalities. Communications in

Mathematical Physics  115(4):553–569  1988.

[35] Mitsugu Iwamoto and Junji Shikata. Information theoretic security for encryption based on conditional
Rényi entropies. In International Conference on Information Theoretic Security  pages 103–121. Springer 
2013.

[36] Richard Jordan  David Kinderlehrer  and Felix Otto. The variational formulation of the Fokker–Planck

equation. SIAM Journal on Mathematical Analysis  29(1):1–17  January 1998.

[37] R. Kannan  L. Lovász  and M. Simonovits. Random walks and an O⇤(n5) volume algorithm for convex

bodies. Random Structures and Algorithms  11:1–50  1997.

[38] Michel Ledoux. Concentration of measure and logarithmic Sobolev inequalities. Séminaire de probabilités

de Strasbourg  33:120–216  1999.

11

[39] Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster
polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing  pages 1115–1121. ACM  2018.

[40] Xuechen Li  Denny Wu  Lester Mackey  and Murat A Erdogdu. Stochastic Runge–Kutta accelerates

Langevin Monte Carlo and beyond. arXiv preprint arXiv:1906.07868  2019.

[41] Yingzhen Li and Richard E Turner. Rényi divergence variational inference. In D. D. Lee  M. Sugiyama 
U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29 
pages 1073–1081. Curran Associates  Inc.  2016.

[42] L. Lovász and S. Vempala. Fast algorithms for logconcave functions: sampling  rounding  integration and

optimization. In FOCS  pages 57–68  2006.

[43] L. Lovász and S. Vempala. The geometry of logconcave functions and sampling algorithms. Random

Struct. Algorithms  30(3):307–358  2007.

[44] László Lovász and Santosh S. Vempala. Hit-and-run from a corner. SIAM Journal on Computing 

35(4):985–1005  2006.

[45] Yi-An Ma  Niladri Chatterji  Xiang Cheng  Nicolas Flammarion  Peter Bartlett  and Michael I Jordan. Is

there an analog of Nesterov acceleration for MCMC? arXiv preprint arXiv:1902.00996  2019.

[46] Michael C. Mackey. Time’s Arrow: The Origins of Thermodynamics Behavior. Springer-Verlag  1992.

[47] Oren Mangoubi and Aaron Smith. Rapid mixing of Hamiltonian Monte Carlo on strongly log-concave

distributions. arXiv preprint arXiv:1708.07114  2017.

[48] Oren Mangoubi and Nisheeth Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte
Carlo. In Advances in Neural Information Processing Systems 31  pages 6027–6037. Curran Associates 
Inc.  2018.

[49] Oren Mangoubi and Nisheeth K Vishnoi. Nonconvex sampling with the Metropolis-adjusted Langevin

algorithm. arXiv preprint arXiv:1902.08452  2019.

[50] Yishay Mansour  Mehryar Mohri  and Afshin Rostamizadeh. Multiple source adaptation and the Rényi
divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  pages
367–374. AUAI Press  2009.

[51] Georg Menz and André Schlichting. Poincaré and logarithmic Sobolev inequalities by decomposition of

the energy landscape. Ann. Probab.  42(5):1809–1884  09 2014.

[52] Ilya Mironov. Rényi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium

(CSF)  pages 263–275. IEEE  2017.

[53] D Morales  L Pardo  and I Vajda. Rényi statistics in directed families of exponential experiments. Statistics:

A Journal of Theoretical and Applied Statistics  34(2):151–174  2000.

[54] Felix Otto and Cédric Villani. Generalization of an inequality by Talagrand and links with the logarithmic

Sobolev inequality. Journal of Functional Analysis  173(2):361–400  2000.

[55] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochastic gradient
Langevin dynamics: A nonasymptotic analysis. In Satyen Kale and Ohad Shamir  editors  Proceedings
of the 2017 Conference on Learning Theory  volume 65 of Proceedings of Machine Learning Research 
pages 1674–1703  Amsterdam  Netherlands  07–10 Jul 2017. PMLR.

[56] Alfréd Rényi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability  Volume 1: Contributions to the Theory of Statistics.
The Regents of the University of California  1961.

[57] OS Rothaus. Diffusion on compact Riemannian manifolds and logarithmic Sobolev inequalities. Journal

of functional analysis  42(1):102–109  1981.

[58] M Talagrand. Transportation cost for Gaussian and other product measures. Geometric and Functional

Analysis  6:587–600  01 1996.

[59] Tim Van Erven and Peter Harremos. Rényi divergence and Kullback-Leibler divergence. IEEE Transactions

on Information Theory  60(7):3797–3820  2014.

12

[60] Cédric Villani. Topics in optimal transportation. Number 58 in Graduate Studies in Mathematics. American

Mathematical Society  2003.

[61] Feng-Yu Wang and Jian Wang. Functional inequalities for convolution of probability measures. In Annales
de l’Institut Henri Poincaré  Probabilités et Statistiques  volume 52  pages 898–914. Institut Henri Poincaré 
2016.

[62] Andre Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a
composite optimization problem. In Conference On Learning Theory  COLT 2018  Stockholm  Sweden 
6-9 July 2018  pages 2093–3027  2018.

13

,Santosh Vempala
Andre Wibisono