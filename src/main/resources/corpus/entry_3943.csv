2013,On model selection consistency of penalized M-estimators: a geometric theory,Penalized M-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure. Often  the penalties are \emph{geometrically decomposable}  \ie\ can be expressed as a sum of (convex) support functions. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning.,On model selection consistency of M-estimators with

geometrically decomposable penalties

Jason D. Lee  Yuekai Sun

Institute for Computational and Mathematical Engineering

Stanford University

{jdl17 yuekai}@stanford.edu

Jonathan E. Taylor

Department of Statisticis

Stanford University

jonathan.taylor@stanford.edu

Abstract

Penalized M-estimators are used in diverse areas of science and engineering to ﬁt
high-dimensional models with some low-dimensional structure. Often  the penal-
ties are geometrically decomposable  i.e. can be expressed as a sum of support
functions over convex sets. We generalize the notion of irrepresentable to geomet-
rically decomposable penalties and develop a general framework for establishing
consistency and model selection consistency of M-estimators with such penalties.
We then use this framework to derive results for some special cases of interest in
bioinformatics and statistical learning.

1

Introduction

The principle of parsimony is used in many areas of science and engineering to promote “simple”
models over more complex ones.
In machine learning  signal processing  and high-dimensional
statistics  this principle motivates the use of sparsity inducing penalties for model selection and
signal recovery from incomplete/noisy measurements. In this work  we consider M-estimators of
the form:

minimize

θ∈Rp

(cid:96)(n)(θ) + λρ(θ)  subject to θ ∈ S 

(1.1)

where (cid:96)(n) is a convex  twice continuously differentiable loss function  ρ is a penalty function  and
S ⊆ Rp is a subspace. Many commonly used penalties are geometrically decomposable  i.e. can
be expressed as a sum of support functions over convex sets. We describe this notion of decompos-
able in Section 2 and then develop a general framework for analyzing the consistency and model
selection consistency of M-estimators with geometrically decomposable penalties. When special-
ized to various statistical models  our framework yields some known and some new model selection
consistency results.
This paper is organized as follows: First  we review existing work on consistency and model selec-
tion consistency of penalized M-estimators. Then  in Section 2  we describe the notion of geomet-
rically decomposable and give some examples of geometrically decomposable penalties. In Section
3  we generalize the notion of irrepresentable to geometrically decomposable penalties and state our
main result (Theorem 3.4). We prove our main result in the Supplementary Material and develop a
converse result concerning the necessity of the irrepresentable condition in the Supplementary Ma-
terial. In Section 4  we use our main result to derive consistency and model selection consistency
results for the generalized lasso (total variation) and maximum likelihood estimation in exponential
families.

1

1.1 Consistency of penalized M-estimators

The consistency of penalized M-estimators has been studied extensively. The three most well-
studied problems are (i) the lasso [2  26]  (ii) generalized linear models (GLM) with the lasso
penalty [10]  and (iii) inverse covariance estimators with sparsity inducing penalties (equivalent
to sparse maximum likelihood estimation for a Gaussian graphical model) [21  20]. There are also
consistency results for M-estimators with group and structured variants of the lasso penalty [1  7].
Negahban et al. [17] proposes a uniﬁed framework for establishing consistency and convergence
rates for M-estimators with penalties ρ that are decomposable with respect to a pair of subspaces
M  ¯M:

ρ(x + y) = ρ(x) + ρ(y)  for all x ∈ M  y ∈ ¯M⊥.

Many commonly used penalties such as the lasso  group lasso  and nuclear norm are decomposable
in this sense. Negahban et al. prove a general result that establishes the consistency of M-estimators
with decomposable penalties. Using their framework  they derive consistency results for special
cases like sparse and group sparse regression. The current work is in a similar vein as Negahban et
al. [17]  but we focus on establishing the more stringent result of model selection consistency rather
than consistency. See Section 3 for a comparison of the two notions of consistency.
The model selection consistency of penalized M-estimators has also been extensively studied. The
most commonly studied problems are (i) the lasso [30  26]  (ii) GLM’s with the lasso penalty [4  19 
28]  (iii) covariance estimation [15  12  20] and (more generally) structure learning [6  14]. There are
also general results concerning M-estimators with sparsity inducing penalties [29  16  11  22  8  18 
24]. Despite the extensive work on model selection consistency  to our knowledge  this is the ﬁrst
work to establish a general framework for model selection consistency for penalized M-estimators.

2 Geometrically decomposable penalties
Let C ⊂ Rp be a closed convex set. Then the support function over C is

(2.1)
Support functions are sublinear and should be thought of as semi-norms. If C is a norm ball  i.e.
C = {x | (cid:107)x(cid:107) ≤ 1}  then hC is the dual norm:

hC(x) = supy {yT x | y ∈ C}.

(cid:107)y(cid:107)∗

= supx {xT y | (cid:107)x(cid:107) ≤ 1}.

The support function is a supremum of linear functions  hence the subdifferential consists of the
linear functions that attain the supremum:

∂hC(x) = {y ∈ C | yT x = hC(x)}.

The support function (as a function of the convex set C) is also additive over Minkowski sums  i.e.
if C and D are convex sets  then

hC+D(x) = hC(x) + hD(x).

We use this property to express penalty functions as sums of support functions. E.g. if ρ is a norm
and the dual norm ball can be expressed as a (Minkowski) sum of convex sets C1  . . .   Ck  then ρ
can be expressed as a sum of support functions:

ρ(x) = hC1 (x) + ··· + hCk (x).

If a penalty ρ can be expressed as

(2.2)
where A and I are closed convex sets and S is a subspace  then we say ρ is a geometrically de-
composable penalty. This form is general; if ρ can be expressed as a sum of support functions 
i.e.

ρ(θ) = hA(θ) + hI (θ) + hS⊥ (θ) 

ρ(θ) = hC1 (θ) + ··· + hCk (θ) 

then we can set A  I  and S⊥ to be sums of the sets C1  . . .   Ck to express ρ in geometrically
decomposable form (2.2). In many cases of interest  A + I is a norm ball and hA+I = hA + hI is
the dual norm. In our analysis  we assume

1Given the extensive work on consistency of penalized M-estimators  our review and referencing is neces-

sarily incomplete.

2

1. A and I are bounded.
2. I contains a relative neighborhood of the origin  i.e. 0 ∈ relint(I).

We do not require A + I to contain a neighborhood of the origin. This generality allows for unpe-
nalized variables.
The notation A and I should be as read as “active” and “inactive”: span(A) should contain the true
parameter vector and span(I) should contain deviations from the truth that we want to penalize. E.g.
if we know the sparsity pattern of the unknown parameter vector  then A should span the subspace
of all vectors with the correct sparsity pattern.
The third term enforces a subspace constraint θ ∈ S because the support function of a subspace is
the (convex) indicator function of the orthogonal complement:
x ∈ S

(cid:26)0

hS⊥ (x) = 1S(x) =

∞ otherwise.

Such subspace constraints arise in many problems  either naturally (e.g. the constrained lasso [9]) or
after reformulation (e.g. group lasso with overlapping groups). We give three examples of penalized
M-estimators with geometrically decomposable penalties  i.e.

minimize

θ∈Rp

(cid:96)(n)(θ) + λρ(θ) 

(2.3)

where ρ is a geometrically decomposable penalty. We also compare our notion of geometrically
decomposable to two other notions of decomposable penalties by Negahban et al. [17] and Van De
Geer [25] in the Supplementary Material.

2.1 The lasso and group lasso penalties
Two geometrically decomposable penalties are the lasso and group lasso penalties. Let A and I
be complementary subsets of {1  . . .   p}. We can decompose the lasso penalty component-wise to
obtain

where hB∞ A and hB∞ I are support functions of the sets

(cid:107)θ(cid:107)1 = hB∞ A(θ) + hB∞ I (θ) 

B∞ A =(cid:8)θ ∈ Rp | (cid:107)θ(cid:107)∞ ≤ 1 and θI = 0(cid:9)
B∞ I =(cid:8)θ ∈ Rp | (cid:107)θ(cid:107)∞ ≤ 1 and θA = 0(cid:9).

If the groups do not overlap  then we can also decompose the group lasso penalty group-wise (A

and I are now sets of groups) to obtain(cid:88)

(cid:107)θg(cid:107)2 = hB(2 ∞) A (θ) + hB(2 ∞) I (θ).

g∈G

hB(2 ∞) A and hB(2 ∞) I are support functions of the sets

B(2 ∞) A =(cid:8)θ ∈ Rp | max
B(2 ∞) I =(cid:8)θ ∈ Rp | max

g∈G (cid:107)θg(cid:107)2 ≤ 1 and θg = 0  g ∈ A(cid:9)
g∈G (cid:107)θg(cid:107)2 ≤ 1 and θg = 0  g ∈ I(cid:9).

If the groups overlap  then we can duplicate the parameters in overlapping groups and enforce equal-
ity constraints.

2.2 The generalized lasso penalty
Another geometrically decomposable penalty is the generalized lasso penalty [23]. Let D ∈ Rm×p
be a matrix and A and I be complementary subsets of {1  . . .   m}. We can express the generalized
lasso penalty in decomposable form:

(cid:107)Dθ(cid:107)1 = hDT B∞ A (θ) + hDT B∞ I (θ).

(2.4)

3

hDT B∞ A and hDT B∞ I are support functions of the sets

DT B∞ A = {x ∈ Rp | x = DTAy (cid:107)y(cid:107)∞ ≤ 1}
DT B∞ I = {x ∈ Rp | x = DTI y (cid:107)y(cid:107)∞ ≤ 1}.

(2.5)
(2.6)
We can also formulate any generalized lasso penalized M-estimator as a linearly constrained  lasso
penalized M-estimator. After a change of variables  a generalized lasso penalized M-estimator is
equivalent to

minimize
θ∈Rk γ∈Rp

(cid:96)(n)(D†θ + γ) + λ(cid:107)θ(cid:107)1   subject to γ ∈ N (D) 

where N (D) is the nullspace of D. The lasso penalty can then be decomposed component-wise to
obtain

(cid:107)θ(cid:107)1 = hB∞ A(θ) + hB∞ I (θ).

We enforce the subspace constraint θ ∈ N (D) with the support function of R(D)⊥. This yields the
convex optimization problem

minimize
θ∈Rk γ∈Rp

(cid:96)(n)(D†θ + γ) + λ(hB∞ A (θ) + hB∞ I (θ) + hN (D)⊥ (γ)).

There are many interesting applications of the generalized lasso in signal processing and statistical
learning. We refer to Section 2 in [23] for some examples.

2.3

“Hybrid” penalties

A large class of geometrically decomposable penalties are so-called “hybrid” penalties:
inﬁmal
convolutions of penalties to promote solutions that are sums of simple components  e.g. θ = θ1 + θ2 
where θ1 and θ2 are simple. If the constituent simple penalties are geometrically decomposable  then
the resulting hybrid penalty is also geometrically decomposable.
For example  let ρ1 and ρ2 be geometrically decomposable penalties  i.e. there are sets A1  I1  S1
and A2  I2  S2 such that

ρ1(θ) = hA1 (θ) + hI1(θ) + hS⊥
ρ2(θ) = hA2 (θ) + hI2(θ) + hS⊥

1

(θ)

(θ)

2

The M-estimator with penalty ρ(θ) = inf γ {ρ1(γ) + ρ2(θ − γ)} is equivalent to the solution to the
convex optimization problem

minimize

θ∈R2p

(cid:96)(n)(θ1 + θ2) + λ(ρ1(θ1) + ρ2(θ2)).

(2.7)

This is an M-estimator with a geometrically decomposable penalty:

minimize

θ∈R2p

(cid:96)(n)(θ1 + θ2) + λ(hA(θ) + hI (θ) + hS⊥ (θ)).

hA  hI and hS⊥ are support functions of the sets

A = {(θ1  θ2) | θ1 ∈ A1 ⊂ Rp  θ2 ∈ A2 ⊂ Rp}
I = {(θ1  θ2) | θ1 ∈ I1 ⊂ Rp  θ2 ∈ I2 ⊂ Rp}
S = {(θ1  θ2) | θ1 ∈ S1 ⊂ Rp  θ2 ∈ S2 ⊂ Rp}.

There are many interesting applications of the hybrid penalties in signal processing and statistical
learning. Two examples are the huber function  ρ(θ) = inf θ=γ1+γ2 (cid:107)γ1(cid:107)1 +(cid:107)γ2(cid:107)2
2  and the multitask
group regularizer  ρ(Θ) = inf Θ=B+S (cid:107)B(cid:107)1 ∞ +(cid:107)S(cid:107)1. See [27] for recent work on model selection
consistency in hybrid penalties.

3 Main result

We assume the unknown parameter vector θ(cid:63) is contained in the model subspace

M := span(I)⊥ ∩ S 

(3.1)

4

and we seek estimates of θ(cid:63) that are “correct”. We consider two notions of correctness: (i) an
estimate ˆθ is consistent (in the (cid:96)2 norm) if the estimation error in the (cid:96)2 norm decays to zero in

probability as sample size grows: (cid:13)(cid:13)(cid:13)ˆθ − θ(cid:63)(cid:13)(cid:13)(cid:13)2

p→ 0 as n → ∞ 

and (ii) ˆθ is model selection consistent if the estimator selects the correct model with probability
tending to one as sample size grows:

Pr(ˆθ ∈ M ) → 1 as n → ∞.

NOTATION: We use PC to denote the orthogonal projector onto span(C) and γC to denote the
gauge function of a convex set C containing the origin:

γC(x) = inf
x

{λ ∈ R+ | x ∈ λC}.

Further  we use κ(ρ) to denote the compatibility constant between a semi-norm ρ and the (cid:96)2 norm
over the model subspace:

κ(ρ) := sup

Finally  we choose a norm (cid:107)·(cid:107)ε to make(cid:13)(cid:13)∇(cid:96)(n)(θ(cid:63))(cid:13)(cid:13)ε small. This norm is usually the dual norm to

{ρ(x) | (cid:107)x(cid:107)2 ≤ 1  x ∈ M}.

the penalty.
Before we state our main result  we state our assumptions on the problem. Our two main assump-
tions are stated in terms of the Fisher information matrix:

x

Assumption 3.1 (Restricted strong convexity). We assume the loss function (cid:96)(n) is locally strongly
convex with constant m over the model subspace  i.e.

Q(n) = ∇2(cid:96)(n)(θ(cid:63)).

(cid:96)(n)(θ1) − (cid:96)(n)(θ2) ≥ ∇(cid:96)(n)(θ2)T (θ1 − θ2) +

(cid:107)θ1 − θ2(cid:107)2

2

(3.2)

m
2

for some m > 0 and all θ1  θ2 ∈ Br(θ(cid:63)) ∩ M.
We require this assumption to make the maximum likelihood estimate unique over the model sub-
space. Otherwise  we cannot hope for consistency. This assumption requires the loss function to be
curved along certain directions in the model subspace and is very similar to Negahban et al.’s notion
of restricted strong convexity [17] and Buhlmann and van de Geer’s notion of compatibility [3].
Intuitively  this assumption means the “active” predictors are not overly dependent on each other.
We also require ∇2(cid:96)(n) to be locally Lipschitz continuous  i.e.

(cid:107)∇2(cid:96)(n)(θ1) − ∇2(cid:96)(n)(θ2)(cid:107)2 ≤ L(cid:107)θ1 − θ2(cid:107)2 .

for some L > 0 and all θ1  θ2 ∈ Br(θ(cid:63)) ∩ M. This condition automatically holds for all twice-
continuously differentiable loss functions  hence we do not state this condition as an assumption.
To obtain model selection consistency results  we must ﬁrst generalize the key notion of irrepre-
sentable to geometrically decomposable penalties.
Assumption 3.2 (Irrepresentability). There exist τ ∈ (0  1) such that

sup

z

{V (PM⊥ (Q(n)PM (PM Q(n)PM )†PM z − z)) | z ∈ ∂hA(Br(θ(cid:63)) ∩ M )}
< 1 − τ 

where V is the inﬁmal convolution of γI and 1S⊥

V (z) = inf
u

{γI (u) + 1S⊥ (z − u)}.

If uI (z) and uS⊥ (u) achieve V (z) (i.e. V (z) = γI (uI (z)))  then V (u) < 1  means uI (z) ∈
relint(I). Hence the irrepresentable condition requires any z ∈ M⊥ to be decomposable into
uI + uS⊥  where uI ∈ relint(I) and uS⊥ ∈ S⊥.

5

Lemma 3.3. V is a bounded semi-norm over M⊥  i.e. V is ﬁnite and sublinear over M⊥.

Let (cid:107)·(cid:107)ε be an error norm  usually chosen to make(cid:13)(cid:13)∇(cid:96)(n)(θ(cid:63))(cid:13)(cid:13)ε small. V is a bounded semi-norm

over M⊥  hence there exists some ¯τ such that

V (PM⊥ (Q(n)PM (PM Q(n)PM )†PM x − x)) ≤ ¯τ (cid:107)x(cid:107)ε

(3.3)
¯τ surely exists because (i) (cid:107)·(cid:107)ε is a norm  so the set {x ∈ Rp | (cid:107)x(cid:107)ε ≤ 1} is compact  and (ii) V is
ﬁnite over M⊥  so the left side of (3.3) is a continuous function of x. Intuitively  ¯τ quantiﬁes how
large the irrepresentable term can be compared to the error norm.
The irrepresentable condition is a standard assumption for model selection consistency and has
been shown to be almost necessary for sign consistency of the lasso [30  26]. Intuitively  the ir-
representable condition requires the active predictors to be not overly dependent on the inactive
predictors. In Supplementary Material  we show our (generalized) irrepresentable condition is also
necessary for model selection consistency with some geometrically decomposable penalties.
Theorem 3.4. Suppose Assumption 3.1 and 3.2 are satisﬁed. If we select λ such that

and

(cid:107)∇(cid:96)(n)(θ(cid:63))(cid:107)ε

2¯τ
τ

2¯τ κ((cid:107)·(cid:107)ε)(2κ(hA)+ τ

¯τ κ((cid:107)·(cid:107)∗

ε ))2

τ

mr
2κ(hA)+ τ

¯τ κ((cid:107)·(cid:107)∗

ε )  

λ >

L

λ < min

 m2
(cid:0)κ(hA) + τ

ε)(cid:1) λ 

2¯τ κ((cid:107)·(cid:107)∗

(cid:13)(cid:13)(cid:13)ˆθ − θ(cid:63)(cid:13)(cid:13)(cid:13)2

≤ 2

1.
2. ˆθ ∈ M := span(I)⊥ ∩ S.

m

then the penalized M-estimator is unique  consistent (in the (cid:96)2 norm)  and model selection consistent 
i.e. the optimal solution to (2.3) satisﬁes

Remark 1. Theorem 3.4 makes a deterministic statement about the optimal solution to (2.3). To
use this result to derive consistency and model selection consistency results for a statistical model 
we must ﬁrst verify Assumptions (3.1) and (3.2) are satisﬁed with high probability. Then  we must
choose an error norm (cid:107)·(cid:107)ε and select λ such that
2¯τ
τ

(cid:107)∇(cid:96)(n)(θ(cid:63))(cid:107)ε

λ >

and

λ < min

with high probability.

 m2

L

2¯τ κ((cid:107)·(cid:107)ε)(2κ(hA)+ τ

¯τ κ((cid:107)·(cid:107)∗

ε ))2

τ

mr
2κ(hA)+ τ

¯τ κ((cid:107)·(cid:107)∗
ε )

In Section 4  we use this theorem to derive consistency and model selection consistency results for
the generalized lasso and penalized likelihood estimation for exponential families.

4 Examples

We use Theorem 3.4 to establish the consistency and model selection consistency of the generalized
lasso and a group lasso penalized maximum likelihood estimator in the high-dimensional setting.
Our results are nonasymptotic  i.e. we obtain bounds in terms of sample size n and problem dimen-
sion p that hold with high probability.

4.1 The generalized lasso
Consider the linear model y = X T θ(cid:63) +   where X ∈ Rn×p is the design matrix  and θ(cid:63) ∈ Rp
are unknown regression parameters. We assume the columns of X are normalized so (cid:107)xi(cid:107)2 ≤ √
n.
 ∈ Rn is i.i.d.  zero mean  sub-Gaussian noise with parameter σ2.

6

We seek an estimate of θ(cid:63) with the generalized lasso:

minimize

θ∈Rp

1
2n

(cid:107)y − Xθ(cid:107)2

2 + λ(cid:107)Dθ(cid:107)1  

(4.1)

where D ∈ Rm×p. The generalized lasso penalty is geometrically decomposable:

(cid:107)Dθ(cid:107)1 = hDT B∞ A (θ) + hDT B∞ I (θ).

hDT B∞ A and hDT B∞ I are support functions of the sets

DT B∞ A = {x ∈ Rp | x = DT y  yI = 0 (cid:107)y(cid:107)∞ ≤ 1}
DT B∞ I = {x ∈ Rp | x = DT y  yA = 0 (cid:107)y(cid:107)∞ ≤ 1}.

The sample ﬁsher information matrix is Q(n) = 1
Lipschitz constant of Q(n) is zero. The restricted strong convexity constant is

n X T X. Q(n) does not depend on θ  hence the

m = λmin(Q(n)) = inf
x

{xT Q(n)x | (cid:107)x(cid:107)2 = 1}.

The model subspace is the set

span(DT B∞ I)⊥ = R(DTI )⊥ = N (DI) 

where I is a subset of the row indices of D. The compatibility constants κ((cid:96)1)  κ(hA) are

κ((cid:96)1) = sup
x
κ(hA) = sup
x

{(cid:107)x(cid:107)1 | (cid:107)x(cid:107)2 ≤ 1  x ∈ N (DI)}

(cid:8)hDT B∞ A(x) | (cid:107)x(cid:107)2 ≤ 1  x ∈ M(cid:9) ≤ (cid:107)DA(cid:107)2
(cid:113) log p
n   then there exists c such that Pr(cid:0)λ ≥ 2¯τ

√
If we select λ > 2

(cid:1) ≤ 1 −
2 exp(cid:0)−cλ2n(cid:1). Thus the assumptions of Theorem 3.4 are satisﬁed with probability at least 1 −
then  with probability at least 1 − 2 exp(cid:0)−cλ2n(cid:1)  the solution to the generalized

2 exp(−cλ2n)  and we deduce the generalized lasso is consistent and model selection consistent.
Corollary 4.1. Suppose y = Xθ(cid:63) +   where X ∈ Rn×p is the design matrix  θ(cid:63) are unknown
coefﬁcients  and  is i.i.d.  zero mean  sub-Gaussian noise with parameter σ2. If we select λ >
√
2
lasso is unique  consistent  and model selection consistent  i.e. the optimal solution to (4.1) satisﬁes

(cid:112)|A|.
(cid:13)(cid:13)∇(cid:96)(n)(θ(cid:63))(cid:13)(cid:13)∞

2σ ¯τ
τ

2σ ¯τ
τ

τ

n

(cid:113) log p
(cid:13)(cid:13)(cid:13)ˆθ − θ(cid:63)(cid:13)(cid:13)(cid:13)2
(cid:112)|A| + τ
i = 0  for any i such that(cid:0)Dθ(cid:63)(cid:1)
2. (cid:0)D ˆθ(cid:1)

(cid:16)(cid:107)DA(cid:107)2

≤ 2

1.

m

(cid:17)

i = 0.

2¯τ κ((cid:96)1)

λ 

4.2 Learning exponential families with redundant representations

Suppose X is a random vector  and let φ be a vector of sufﬁcient statistics. The exponential family
associated with these sufﬁcient statistics is the set of distributions with the form

Pr(x; θ) = exp(cid:0)θT φ(x) − A(θ)(cid:1)  

Suppose we are given samples x(1)  . . .   x(n) drawn i.i.d. from an exponential family with unknown
parameters θ(cid:63) ∈ Rp. We seek a maximum likelihood estimate (MLE) of the unknown parameters:
(4.2)

ML (θ) + λ(cid:107)θ(cid:107)2 1   subject to θ ∈ S.
(cid:96)(n)

minimize

θ∈Rp

where (cid:96)(n)

ML is the (negative) log-likelihood function

n(cid:88)

i=1

ML (θ) = − 1
(cid:96)(n)
n

log Pr(x(i); θ) = − 1
n

7

n(cid:88)

i=1

θT φ(x(i)) + A(θ)

and (cid:107)θ(cid:107)2 1 is the group lasso penalty

(cid:88)

g∈G

(cid:107)θg(cid:107)2 .

(cid:107)θ(cid:107)2 1 =

It is also straightforward to change the maximum likelihood estimator to the more computationally
tractable pseudolikelihood estimator [13  6]  the neighborhood selection procedure [15]  and include
covariates [13]. For brevity  we only explain the details for the maximum likelihood estimator.
Many undirected graphical models can be naturally viewed as exponential families. Thus estimat-
ing the parameters of exponential families is equivalent to learning undirected graphical models  a
problem of interest in many application areas such as bioinformatics.
Below  we state a corollary that results from applying Theorem 3.4 to exponential families. Please
see the supplementary material for the proof and deﬁnitions of the quantities involved.
Corollary 4.2. Suppose we are given samples x(1)  . . .   x(n) drawn i.i.d. from an exponential family
with unknown parameters θ(cid:63). If we select

√
2

2L1 ¯τ
τ

(cid:114)
(cid:1)4
(cid:0)2 + τ

λ >

(cid:40) 32L1L2

2 ¯τ 2

(maxg∈G |g|) log |G|

n

and the sample size n is larger than

m4τ 4

max

then  with probability at least 1 − 2(cid:0) maxg∈G |g|(cid:1) exp(−cλ2n)  the penalized maximum likelihood

¯τ )2(maxg∈G |g|)|A| log |G| 

estimator is unique  consistent  and model selection consistent  i.e. the optimal solution to (4.2)
satisﬁes

16L1
m2r2 (2 + τ

¯τ

(maxg∈G |g|)|A|2 log |G|

(cid:13)(cid:13)(cid:13)ˆθ − θ(cid:63)(cid:13)(cid:13)(cid:13)2
(cid:1)(cid:112)|A|λ 
(cid:13)(cid:13)2
2. ˆθg = 0  g ∈ I and ˆθg (cid:54)= 0 if (cid:13)(cid:13)θ(cid:63)

(cid:0)1 + τ

≤ 2

1.

2¯τ

m

g

(cid:0)1 + τ

2¯τ

(cid:1)(cid:112)|A|λ.

> 1
m

5 Conclusion

We proposed the notion of geometrically decomposable and generalized the irrepresentable con-
dition to geometrically decomposable penalties. This notion of decomposability builds on those
by Negahban et al. [17] and Cand´es and Recht [5] and includes many common sparsity inducing
penalties. This notion of decomposability also allows us to enforce linear constraints.
We developed a general framework for establishing the model selection consistency of M-estimators
with geometrically decomposable penalties. Our main result gives deterministic conditions on the
problem that guarantee consistency and model selection consistency; in this sense  it extends the
work of [17] from estimation consistency to model selection consistency. We combine our main
result with probabilistic analysis to establish the consistency and model selection consistency of the
generalized lasso and group lasso penalized maximum likelihood estimators.

Acknowledgements

We thank Trevor Hastie and three anonymous reviewers for their insightful comments. J. Lee was
supported by a National Defense Science and Engineering Graduate Fellowship (NDSEG) and an
NSF Graduate Fellowship. Y. Sun was supported by the NIH  award number 1U01GM102098-01.
J.E. Taylor was supported by the NSF  grant DMS 1208857  and by the AFOSR  grant 113039.

References
[1] F. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res.  9:1179–1225 

2008.

8

[2] P.J. Bickel  Y. Ritov  and A.B. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Ann. Statis. 

37(4):1705–1732  2009.

[3] P. B¨uhlmann and S. van de Geer. Statistics for high-dimensional data: Methods  theory and applications.

2011.

[4] F. Bunea. Honest variable selection in linear and logistic regression models via (cid:96)1 and (cid:96)1+(cid:96)2 penalization.

Electron. J. Stat.  2:1153–1194  2008.

[5] E. Cand`es and B. Recht. Simple bounds for recovering low-complexity models. Math. Prog. Ser. A  pages

1–13  2012.

[6] J. Guo  E. Levina  G. Michailidis  and J. Zhu. Asymptotic properties of the joint neighborhood selection

method for estimating categorical markov networks. arXiv preprint.

[7] L. Jacob  G. Obozinski  and J. Vert. Group lasso with overlap and graph lasso. In Int. Conf. Mach. Learn.

(ICML)  pages 433–440. ACM  2009.

[8] A. Jalali  P. Ravikumar  V. Vasuki  S. Sanghavi  and UT ECE. On learning discrete graphical models

using group-sparse regularization. In Int. Conf. Artif. Intell. Stat. (AISTATS)  2011.

[9] G.M. James  C. Paulson  and P. Rusmevichientong. The constrained lasso. Technical report  University

of Southern California  2012.

[10] S.M. Kakade  O. Shamir  K. Sridharan  and A. Tewari. Learning exponential families in high-dimensions:

Strong convexity and sparsity. In Int. Conf. Artif. Intell. Stat. (AISTATS)  2010.

[11] M. Kolar  L. Song  A. Ahmed  and E. Xing. Estimating time-varying networks. Ann. Appl. Stat.  4(1):94–

123  2010.

[12] C. Lam and J. Fan. Sparsistency and rates of convergence in large covariance matrix estimation. Ann.

Statis.  37(6B):4254  2009.

[13] J.D. Lee and T. Hastie. Learning mixed graphical models. arXiv preprint arXiv:1205.5012  2012.
[14] P.L. Loh and M.J. Wainwright. Structure estimation for discrete graphical models: Generalized covariance

matrices and their inverses. arXiv:1212.0478  2012.

[15] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso. Ann.

Statis.  34(3):1436–1462  2006.

[16] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models.

Electron. J. Stat.  2:605–633  2008.

[17] S.N. Negahban  P. Ravikumar  M.J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statist. Sci.  27(4):538–557  2012.

[18] G. Obozinski  M.J. Wainwright  and M.I. Jordan. Support union recovery in high-dimensional multivariate

regression. Ann. Statis.  39(1):1–47  2011.

[19] P. Ravikumar  M.J. Wainwright  and J.D. Lafferty. High-dimensional ising model selection using (cid:96)1-

regularized logistic regression. Ann. Statis.  38(3):1287–1319  2010.

[20] P. Ravikumar  M.J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by

minimizing (cid:96)1-penalized log-determinant divergence. Electron. J. Stat.  5:935–980  2011.

[21] A.J. Rothman  P.J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation.

Electron. J. Stat.  2:494–515  2008.

[22] Y. She. Sparse regression with exact clustering. Electron. J. Stat.  4:1055–1096  2010.
[23] R.J. Tibshirani and J.E. Taylor. The solution path of the generalized lasso. Ann. Statis.  39(3):1335–1371 

2011.

[24] S. Vaiter  G. Peyr´e  C. Dossal  and J. Fadili. Robust sparse analysis regularization. IEEE Trans. Inform.

Theory  59(4):2001–2016  2013.

[25] S. van de Geer. Weakly decomposable regularization penalties and structured sparsity. arXiv preprint

arXiv:1204.4813  2012.

[26] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained

quadratic programming (lasso). IEEE Trans. Inform. Theory  55(5):2183–2202  2009.

[27] E. Yang and P. Ravikumar. Dirty statistical models.

827–835  2013.

In Adv. Neural Inf. Process. Syst. (NIPS)  pages

[28] E. Yang  P. Ravikumar  G.I. Allen  and Z. Liu. On graphical models via univariate exponential family

distributions. arXiv:1301.4183  2013.

[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc.

Ser. B Stat. Methodol.  68(1):49–67  2006.

[30] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res.  7:2541–2563  2006.

9

,Jason Lee
Yuekai Sun
Jonathan Taylor
Tian Lin
Jian Li
Wei Chen
Hao Wang
Xingjian SHI
Dit-Yan Yeung
Maxime Bucher
Tuan-Hung VU
Matthieu Cord
Patrick Pérez