2018,Semidefinite relaxations for certifying robustness to adversarial examples,Despite their impressive performance on diverse tasks  neural networks fail catastrophically in the presence of adversarial inputs—imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses  ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper  we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation.,Semideﬁnite relaxations

for certifying robustness to adversarial examples

Aditi Raghunathan  Jacob Steinhardt and Percy Liang

Stanford University

{aditir  jsteinhardt  pliang}@cs.stanford.edu

Abstract

Despite their impressive performance on diverse tasks  neural networks fail catas-
trophically in the presence of adversarial inputs—imperceptibly but adversarially
perturbed versions of natural inputs. We have witnessed an arms race between
defenders who attempt to train robust networks and attackers who try to construct
adversarial examples. One promise of ending the arms race is developing certiﬁed
defenses  ones which are provably robust against all attackers in some family. These
certiﬁed defenses are based on convex relaxations which construct an upper bound
on the worst case loss over all attackers in the family. Previous relaxations are loose
on networks that are not trained against the respective relaxation. In this paper 
we propose a new semideﬁnite relaxation for certifying robustness that applies to
arbitrary ReLU networks. We show that our proposed relaxation is tighter than pre-
vious relaxations and produces meaningful robustness guarantees on three different
foreign networks whose training objectives are agnostic to our proposed relaxation.

1

Introduction

Many state-of-the-art classiﬁers have been shown to fail catastrophically in the presence of small
imperceptible but adversarial perturbations. Since the discovery of such adversarial examples [25] 
numerous defenses have been proposed in attempt to build classiﬁers that are robust to adversarial
examples. However  defenses are routinely broken by new attackers who adapt to the proposed defense 
leading to an arms race. For example  distillation was proposed [22] but shown to be ineffective [5].
A proposed defense based on transformations of test inputs [20] was broken in only ﬁve days [2].
Recently  seven defenses published at ICLR 2018 fell to the attacks of Athalye et al. [3].
A recent body of work aims to break this arms race by training classiﬁers that are certiﬁably robust to
all attacks within a ﬁxed attack model [13  23  29  8]. These approaches construct a convex relaxation
for computing an upper bound on the worst-case loss over all valid attacks—this upper bound serves
as a certiﬁcate of robustness. In this work  we propose a new convex relaxation based on semideﬁnite
programming (SDP) that is signiﬁcantly tighter than previous relaxations based on linear programming
(LP) [29  8  9] and handles arbitrary number of layers (unlike the formulation in [23]  which was
restricted to two). We summarize the properties of our relaxation as follows:

1. Our new SDP relaxation reasons jointly about intermediate activations and captures interactions
that the LP relaxation cannot. Theoretically  we prove that there is a square root dimension gap between
the LP relaxation and our proposed SDP relaxation for neural networks with random weights.

2. Empirically  the tightness of our proposed relaxation allows us to obtain tight certiﬁcates for
foreign networks—networks that were not speciﬁcally trained towards the certiﬁcation procedure.
For instance  adversarial training against the Projected Gradient Descent (PGD) attack [21] has led
to networks that are “empirically” robust against known attacks  but which have only been certiﬁed
against small perturbations (e.g.  = 0.05 in the (cid:96)∞-norm for the MNIST dataset [9]). We use our SDP

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

to provide the ﬁrst non-trivial certiﬁcate of robustness for a moderate-size adversarially-trained model
on MNIST at  = 0.1.

3. Furthermore  training a network to minimize the optimum of particular relaxation produces
networks for which the respective relaxation provides good robustness certiﬁcates [23]. Notably and
surprisingly  on such networks  our relaxation provides tighter certiﬁcates than even the relaxation
that was optimized for during training.
Related work. Certiﬁcation methods which evaluate the performance of a given network against all
possible attacks roughly fall into two categories. The ﬁrst category leverages convex optimization
and our work adds to this family. Convex relaxations are useful for various reasons. Wong and
Kolter [29]  Raghunathan et al. [23] exploited the theory of duality to train certiﬁably robust networks
on MNIST. In recent work  Dvijotham et al. [8]  Wong et al. [30] extended this approach to train
bigger networks with improved certiﬁed error and on larger datasets. Solving a convex relaxation for
certiﬁcation typically involves standard techniques from convex optimization. This enables scalable
certiﬁcation by providing valid upper bounds at every step in the optimization [9].
The second category draws techniques from formal veriﬁcation such as SMT [16  17  7  14]  which
aim to provide tight certiﬁcates for any network using discrete optimization. These techniques  while
providing tight certiﬁcates on arbitrary networks  are often very slow and worst-case exponential in
network size. In prior work  certiﬁcation would take up to several hours or longer for a single example
even for a small network with around 100 hidden units [7  16]. However  in concurrent work  Tjeng
and Tedrake [26] impressively scaled up exact veriﬁcation through careful preprocessing and efﬁcient
pruning that dramatically reduces the search space. In particular  they concurrently obtain non-trivial
certiﬁcates of robustness on a moderately-sized network trained using the adversarial training objective
of [21] on MNIST at perturbation level  = 0.1.

2 Setup

Our main contribution is a semideﬁnite relaxation of an optimization objective that arises in certiﬁcation
of neural networks against adversarial examples. In this section  we set up relevant notation and present
the optimization objective that will be the focus of the rest of the paper.
Notation. For a vector z ∈ Rn  we use zi to denote the ith coordinate of z. For a matrix Z ∈ Rm×n 
Zi∈Rn denotes the ith row. For any function f :R→R and a vector z∈Rn  f (z) is a vector in Rn with
(f (z))i = f (zi)  e.g.  z2∈Rn represents the function that squares each component. For z y∈Rn  z(cid:23) y
denotes that zi≥ yi for i = 1 2 ... n. We use z1(cid:12)z2 to represent the elementwise product of the vectors
z1 and z2. We use B(¯x) def= {x|(cid:107)x− ¯x(cid:107)∞≤ } to denote the (cid:96)∞ ball around ¯x. When it is necessary to
distinguish vectors from scalars (in Section 4.1)  we use (cid:126)x to represent a vector in Rn that is semantically
associated with the scalar x. Finally  we denote the vector of all zeros by 0 and the vector of all ones by 1.
Multi-layer ReLU networks for classiﬁcation. We focus on multi-layer neural networks with ReLU
activations. A network f with L hidden layers is deﬁned as follows: let x0 ∈ Rd denote the input
and x1  ...   xL denote the activation vectors at the intermediate layers. Suppose the network has
mi units in layer i. xi is related to xi−1 as xi = ReLU(W i−1xi−1) = max(W i−1xi−1 0)  where
W i−1∈Rmi×mi−1 are the weights of the network. For simplicity of exposition  we omit the bias terms
associated with the activations (but consider them in the experiments). We are interested in neural
networks for classiﬁcation where we classify an input into one of k classes. The output of the network
is f (x0)∈ Rk such that f (x0)j = c(cid:62)
j xL represents the score of class j. The class label y assigned to
the input x0 is the class with the highest score: y = argmaxj=1 ... kf (x0)j.
Attack model and certiﬁcate of robustness. We study classiﬁcation in the presence of an attacker A
that takes a clean test input ¯x∈Rd and returns an adversarially perturbed input A(¯x). In this work  we
focus on attackers that are bounded in the (cid:96)∞ norm: A(¯x)∈ B(¯x) for some ﬁxed  > 0. The attacker
is successful on a clean input label pair (¯x ¯y) if f (A(¯x))(cid:54)= ¯y  or equivalently if f (A(¯x))y > f (x0)¯y
for some y(cid:54)= ¯y.
We are interested in bounding the error against the worst-case attack (we assume the attacker has full
y(¯x  ¯y) denote the worst-case margin of an incorrect class
knowledge of the neural network). Let (cid:96)(cid:63)
y that can be achieved in the attack model:

(f (A(x))y−f (A(x))¯y).

(1)

y(¯x ¯y) def= max
(cid:96)(cid:63)

A(x)∈B(¯x)

2

y(¯x ¯y) < 0 for all y(cid:54)= ¯y. Computing (cid:96)(cid:63)

A network is certiﬁably robust on (¯x ¯y) if (cid:96)(cid:63)
y(¯x ¯y) for a neural
network involves solving a non-convex optimization problem  which is intractable in general. In this
work  we study convex relaxations to efﬁciently compute an upper bound Ly(¯x ¯y)≥ (cid:96)(cid:63)
y(¯x ¯y). When
Ly(¯x ¯y) < 0  we have a certiﬁcate of robustness of the network on input (¯x ¯y).
Optimization objective. For a ﬁxed class y  the worst-case margin (cid:96)(cid:63)
y(¯x  ¯y) of a neural network f
with weights W can be expressed as the following optimization problem. The decision variable is
the input A(x)  which we denote here by x0 for notational convenience. The quantity we are interested
in maximizing is f (x0)y − f (x0)¯y = (cy − c¯y)(cid:62)xL  where xL is the ﬁnal layer activation. We set
up the optimization problem by jointly optimizing over all the activations x0 x1 x2 ...xL  imposing
consistency constraints dictated by the neural network  and restricting the input x0 to be within the
attack model. Formally 
y(¯x ¯y) = max
(cid:96)(cid:63)
x0 ... xL
subject to xi =ReLU(W i−1xi−1) for i = 1 2 ... L

(cy−c¯y)(cid:62)xL

(2)

(Neural network constraints)
(Attack model constraints)

(cid:107)x0

j− ¯xj(cid:107)∞≤  for j = 1 2 ... d

Computing (cid:96)(cid:63)
this objective to a convex semideﬁnite program and discuss some properties of this relaxation.

y is computationally hard in general. In the following sections  we present how to relax

3 Semideﬁnite relaxations

In this section  we present our approach to obtaining a computationally tractable upper bound to the
solution of the optimization problem described in (2).
Key insight. The source of the non-convexity in (2) is the ReLU constraints. Consider a ReLU
constraint of the form z = max(x 0). The key observation is that this constraint can be expressed
equivalently as the following three linear and quadratic constraints between z and x: (i) z(z−x) = 0 
(ii) z≥ x  and (iii) z≥ 0. Constraint (i) ensures that z is equal to either x or 0 and constraints (ii) and
(iii) together then ensure that z is at least as large as both. This reformulation allows us to replace
the non-linear ReLU constraints of the optimization problem in 2 with linear and quadratic constraints 
turning it into a quadratically constrained quadratic program (QCQP). We ﬁrst show how this QCQP
can be relaxed to a semideﬁnite program (SDP) for networks with one hidden layer. The relaxation
for multiple layers is a straightforward extension and is presented in Section 5.

3.1 Relaxation for one hidden layer

Consider a neural network with one hidden layer containing m nodes. Let the input be denoted
by x ∈ Rd. The hidden layer activations are denoted by z ∈ Rm and related to the input x as
z =ReLU(W x) for weights W ∈Rm×d.
Suppose that we have lower and upper bounds l u ∈ Rd on the inputs such that lj ≤ xj ≤ uj. For
example  in the (cid:96)∞ attack model we have l = ¯x−1 and u = ¯x+1 where ¯x is the clean input. For the
multi-layer case  we discuss how to obtain these bounds for the intermediate activations in Section 5.2.
We are interested in optimizing a linear function of the hidden layer: f (x) = c(cid:62)z  where c∈Rm. For
instance  while computing the worst case margin of an incorrect label y over true label ¯y  c = cy−c¯y.
We use the key insight that the ReLU constraints can be written as linear and quadratic constraints 
allowing us to embed these constraints into a QCQP. We can also express the input constraint
lj ≤ xj ≤ uj as a quadratic constraint  which will be useful later. In particular  lj ≤ xj ≤ uj if and only
if (xj−lj)(xj−uj)≤ 0  thereby yielding the quadratic constraint x2
j ≤ (lj +uj)xj−ljuj. This gives
us the ﬁnal QCQP below:

(cid:96)(cid:63)
y(¯x ¯y) = fQCQP = max
x z

c(cid:62)z

s.t. z≥ 0  z≥ W x  z2 = z(cid:12)(W x)

x2≤ (l+u)(cid:12)x−l(cid:12)u

(3)

(ReLU constraints)
(Input constraints)

We now relax the non-convex QCQP (3) to a convex SDP. The basic idea is to introduce a new set
of variables representing all linear and quadratic monomials in x and z; the constraints in (3) can then
be written as linear functions of these new variables.

3

(a)

(b)

Figure 1: (a) Plot showing the feasible regions for the vectors (cid:126)x (green) and (cid:126)z (red). The input constraints
restrict (cid:126)x to lie within the green circle. The ReLU constraint (cid:126)z⊥(cid:126)z−(cid:126)x forces (cid:126)z to lie on the dashed red
circle and the constraint (cid:126)z·(cid:126)e≥(cid:126)x·(cid:126)e restricts it to the solid arc. (b) For a ﬁxed value of input (cid:126)x·(cid:126)e  when the
angle made by (cid:126)x with (cid:126)e increases  the arc spanned by (cid:126)z has a larger projection on (cid:126)e and leading to a looser
relaxation. Secondly  for a ﬁxed value of (cid:126)x·(cid:126)e  as θ increases  the norm (cid:107)(cid:126)x(cid:107) increases and vice versa.

In particular  let v def=

. We deﬁne a matrix P def= vv(cid:62) and use symbolic indexing P [·] to index

(cid:34) 1

x
z

(cid:35)
 P [1] P [x(cid:62)]

P [z(cid:62)]
P [x] P [xx(cid:62)] P [xz(cid:62)]
P [z] P [zx(cid:62)] P [zz(cid:62)]

.

the elements of P   i.e P =

fSDP = max

P

c(cid:62)P [z]

The SDP relaxation of (3) can be written in terms of the matrix P as follows.

s.t P [z]≥ 0  P [z]≥ W P [x]  diag(P [zz(cid:62)]) = diag(W P [xz(cid:62)])

diag(P [xx(cid:62)])≤ (l+u)(cid:12)P [x]−l(cid:12)u
P [1] = 1  P (cid:23) 0

(4)

(ReLU constraints)
(Input constraints)
(Matrix constraints).

When the matrix P admits a rank-one factorization vv(cid:62)  the entries of the matrix P exactly correspond
to linear and quadratic monomials in x and z. In this case  the ReLU and input constraints of the SDP
are identical to the constraints of the QCQP. However  this rank-one constraint on P would make the
feasible set non-convex. We instead consider the relaxed constraint on P that allows factorizations of
the form P = V V (cid:62)  where V can be full rank. Equivalently  we consider the set of matrices P such that
P (cid:23) 0. This set is convex and is a superset of the original non-convex set. Therefore  the above SDP
is a relaxation of the QCQP in 3 with fSDP≥ fQCQP  providing an upper bound on (cid:96)(cid:63)
y(¯x ¯y) that could
serve as a certiﬁcate of robustness. We note that this SDP relaxation is different from the one proposed
in [23]  which applies only to neural networks with one hidden layer. In contrast  the construction
presented here naturally generalizes to multiple layers  as we show in Section 5. Moreover  we will see
in Section 6 that our new relaxation often yields substantially tighter bounds than the approach of [23].

4 Analysis of the relaxation

Before extending the SDP relaxation deﬁned in (4) to multiple layers  we will provide some geometric
intuition for the SDP relaxation.

4.1 Geometric interpretation

First consider the simple case where m = d = 1 and W = c = 1  so that the problem is to maximize
z subject to z =ReLU(x) and l≤ x≤ u. In this case  the SDP relaxation of (4) is as follows:

fSDP = max

P

P [z]

(5)

s.t P [z]≥ 0  P [z]≥ P [x]  P [z2] = P [xz]

P [x2]≤ (l+u)P [x]−lu
P [1] = 1  P (cid:23) 0

(ReLU constraints)
(Input constraints)
(Matrix constraints).

4

(a)

(b)

(c)

Figure 2: (a) Visualization of the LP and SDP for a single ReLU unit with input x and output z. The LP
is bounded by the line joining the extreme points. (b) Let z1 =ReLU(x1 +x2) and z2 =ReLU(x1−x2).
On ﬁxing the inputs x1 and x2 (both equal to 0.5)  we plot the feasible activations of the LP and SDP
relaxation. The LP feasible set is a simple product over the independent sets  while the SDP enforces
joint constraints to obtain a more complex convex set. (c) We plot the set (z1 z2) across all feasible
inputs (x1 x2) for the same setup as (b) and the objective of maximizing z1 +z2. We see that fSDP < fLP.

(cid:35)

(cid:34) ←(cid:126)e→

←(cid:126)x→
←(cid:126)z→

The SDP operates on a PSD matrix P and imposes linear constraints on the entries of the matrix. Since
feasible P can be written as V V (cid:62)  the entries of P can be thought of as dot products between vectors 

and constraints as operating on these dot products. For the simple example above  V def=
for
some vectors (cid:126)e (cid:126)x (cid:126)z∈R3. The constraint P [1] = 1  for example  imposes (cid:126)e·(cid:126)e = 1 i.e.  (cid:126)e is a unit vector.
The linear monomials P [x] P [z] correspond to projections on this unit vector  (cid:126)x·(cid:126)e and (cid:126)z·(cid:126)e. Finally 
the quadratic monomials P [xz]  P [x2] and P [z2] correspond to (cid:126)x·(cid:126)z  (cid:107)(cid:126)x(cid:107)2 and (cid:107)(cid:126)z(cid:107)2 respectively. We
now reason about the input and ReLU constraints and visualize the geometry (see Figure 1a).
Input constraints. The input constraint P [x2] ≤ (l + u)P [x] − lu equivalently imposes (cid:107)(cid:126)x(cid:107)2 ≤
(l + u)((cid:126)x·(cid:126)e)− lu. Geometrically  this constrains vector (cid:126)x on a sphere with center at 1
2 (l + u)(cid:126)e and
2 (l−u). Notice that this implicitly bounds the norm of (cid:126)x. This is illustrated in Figure 1a where
radius 1
the green circle represents the space of feasible vectors (cid:126)x  projected onto the plane containing (cid:126)e and (cid:126)x.
ReLU constraints. The constraint on the quadratic terms (P [z2] = P [zx]) is the core of the SDP. It
says that the vector (cid:126)z is perpendicular to (cid:126)z−(cid:126)x. We can visualize (cid:126)z on the plane containing (cid:126)x and (cid:126)e
in Figure 1a; the component of (cid:126)z perpendicular to this plane is not relevant to the SDP  because it’s
neither constrained nor appears in the objective. The feasible (cid:126)z trace out a circle with 1
2 (cid:126)x as the center
(because the angle inscribed in a semicircle is a right angle). The linear constraints restrict (cid:126)z to the
arc that has a larger projection on (cid:126)e than (cid:126)x  and is positive.
Remarks. This geometric picture allows us to make the following important observation about the

objective value max(cid:0)(cid:126)z·(cid:126)e(cid:1) of the SDP relaxation. The largest value that (cid:126)z·(cid:126)e can take depends on the

angle θ that (cid:126)x makes with (cid:126)e. In particular  as θ decreases  the relaxation becomes tighter and as the
vector deviates from (cid:126)e  the relaxation gets looser. Figure 1b provides an illustration. For large θ  the
radius of the circle that (cid:126)z traces increases  allowing (cid:126)z·(cid:126)e to take large values.
That leads to the natural question: For a ﬁxed input value (cid:126)x·(cid:126)e (corresponding to x)  what controls
θ? Since (cid:126)x·(cid:126)e =(cid:107)(cid:126)x(cid:107)cosθ  as the norm of (cid:126)x increases  θ increases. Hence a constraint that forces (cid:107)(cid:126)x(cid:107)
to be close to (cid:126)x·(cid:126)e will cause the output (cid:126)z·(cid:126)e to take smaller values. Porting this intuition into the matrix
interpretation  this suggests that constraints forcing P [x2] =(cid:107)(cid:126)x(cid:107)2 to be small lead to tighter relaxations.

4.2 Comparison with linear programming relaxation

In contrast to the SDP  another approach is to relax the objective and constraints in (2) to a linear
program (LP) [18  10  9]. As we will see below  a crucial difference from the LP is that our SDP can
“reason jointly” about different activations of the network in a stronger way than the LP can. We brieﬂy
review the LP approach and then elaborate on this difference.
Review of the LP relaxation. We present the LP relaxation for a neural network with one hidden
layer  where the hidden layer activations z ∈ Rm are related to the input x∈ Rd as z = ReLU(W x).
As before  we have bounds l u∈Rd such that l≤ x≤ u.

5

In the LP relaxation  we replace the ReLU constraints at hidden node j with a convex outer envelope as
illustrated in Figure 2a. The envelope is lower bounded by the linear constraints z≥ W x and z≥ 0. In
order to construct the upper bounding linear constraints  we compute the extreme points s = min
W x
l≤x≤u
and t = max
W x and construct lines that connect (s  ReLU(s)) and (t  ReLU(t)). The ﬁnal LP
l≤x≤u
for the neural network is then written by constructing the convex envelopes for each ReLU unit and
optimizing over this set as follows:

fLP = max c(cid:62)z
s.t z≥ 0  z≥ W x 

z≤(cid:16)ReLU(t)−ReLU(s)

t−s

l≤ x≤ u

(cid:17)·(W x−s)+ReLU(s) 

(6)

(Lower bound lines)

(Upper bound lines)

(Input constraints).

The extreme points s and t are the optima of a linear transformation (by W ) over a box in Rd and can
be computed using interval arithmetic. In the (cid:96)∞ attack model where l = ¯x−1 and u = ¯x+1  we
have sj = W ¯x−(cid:107)Wj(cid:107)1 and tj = W ¯x+(cid:107)Wj(cid:107)1 for j = 1 2 ...m.
From Figure 2a  we see that for a single ReLU unit taken in isolation  the LP is tighter than the SDP.
However  when we have multiple units  the SDP is tighter than the LP. We illustrate this with a simple
example in 2 dimensions with 2 hidden nodes (See Figure 2b).
Simple example to compare the LP and SDP. Consider a two dimensional example with input
x = [x1 x2] and lower and upper bounds l = [− −] and u = [ ]  respectively. The hidden layer
activations z1 and z2 are related to the input as z1 = ReLU(x1 + x2) and z2 = ReLU(x1− x2). The
objective is to maximize z1 +z2.
The LP constrains z1 and z2 independently. To see this  let us set the input x to a ﬁxed value and look
at the feasible values of z1 and z2. In the LP  the convex outer envelope that bounds z1 only depends on
the input x and the bounds l and u and is independent of the value of z2. Similarly  the outer envelope
of z2 does not depend on the value of z1  and the feasible set for (z1 z2) is simply the product of the
individual feasible sets.
In contrast  the SDP has constraints that couple z1 and z2. As a result  the feasible set of (z1 z2) is a
strict subset of the product of the individual feasible sets. Figure 2b plots the LP and SDP feasible sets
[z1 z2] for x = [ 
2 ]. Recall from the geometric observations (Section 4.1) that the arc of (cid:126)z1 depends
on the conﬁguration of (cid:126)x1 + (cid:126)x2  while that of (cid:126)z2 depends on (cid:126)x1− (cid:126)x2. Since the vectors (cid:126)x1 + (cid:126)x2 and
(cid:126)x1− (cid:126)x2 are dependent  the feasible sets of (cid:126)z1 and (cid:126)z2 are also dependent on each other. An alternative
way to see this is from the matrix constraint that P (cid:23) 0 in 4. This matrix constraint does not factor
into terms that decouple the entries P [z1] and P [z2]  hence z1 and z2 cannot vary independently.
When we reason about the relaxation over all feasible points x  the joint reasoning of the SDP allows
it to achieve a better objective value. Figure 2c plots the feasible sets [z1 z2] over all valid x where
the optimal value of the SDP  fSDP  is less than that of the LP  fLP.
We can extend the preceding example to exhibit a dimension-dependent gap between the LP and the
√
SDP for random weight matrices. In particular  for a random network with m hidden nodes and input
dimension d  with high probability  fLP = Θ(md) while fSDP = Θ(m
Proposition 1. Suppose that the weight matrix W ∈ Rm×d is generated randomly by sampling each
element Wij uniformly and independently from {−1 +1}. Also let the output vector c be the all-1s
vector  1. Take ¯x = 0 and  = 1. Then  for some universal constant γ 

m). More formally:

√
d+d

2   

md almost surely  while

fLP≥ 1
√
2
fSDP≤ γ·(m

√

d+d

m) with probability 1−exp(−(d+m)).

We defer the proof of this to Section A.

5 Multi-layer networks

The SDP relaxation to evaluate robustness for multi-layer networks is a straightforward generalization
of the relaxation presented for one hidden layer in Section 3.1.

6

PGD-attack
SDP-cert (this work)
LP-cert
Grad-cert

Grad-NN [23] LP-NN [29]
18%
20%
22%
93%

15%
20%
97%
35%

PGD-NN
9%
18%
100%
n/a

Table 1: Fraction of non-certiﬁed examples on MNIST. Different certiﬁcation techniques (rows) on
different networks (columns). SDP-cert is consistently better than other certiﬁcates. All numbers
are reported for (cid:96)∞ attacks at  = 0.1.

5.1 General SDP
The interactions between xi−1 and xi in (2) (via the ReLU constraint) are analogous to the interaction
between the input and hidden layer for the one layer case. Suppose we have bounds li−1 ui−1∈Rmi−1
on the inputs to the ReLU units at layer i such that li−1≤ xi−1≤ ui−1. We discuss how to obtain these
bounds and their signiﬁcance in Section 5.2. Writing the constraints for each layer iteratively gives
us the following SDP:

(¯x ¯y) = max

f SDP
y
s.t. for i = 1 ... L

(cy−c¯y)(cid:62)P [xL]

P

P [xi]≥ 0  P [xi]≥ W i−1P [xi−1] 
diag(P [xi(xi)(cid:62)]) = diag(W P [xi−1(xi)(cid:62)]) 
diag(P [xi−1(xi−1)(cid:62)])≤ (li−1 +ui−1)(cid:12)P [xi−1]−li−1(cid:12)ui−1 
P [1] = 1  P (cid:23) 0

(7)

(ReLU constraints for layer i)
(Input constraints for layer i)
(Matrix constraints).

5.2 Bounds on intermediate activations

From the geometric interpretation of Section 4.1  we made the important observation that adding
constraints that keep P [x2] small aid in obtaining tighter relaxations. For the multi-layer case  since
the activations at layer i− 1 act as input to the next layer i  adding constraints that restrict P [(xi
j)2]
will lead to a tighter relaxation for the overall objective. The SDP automatically obtains some bound on
j)2] from the bounds on the input  hence the SDP solution is well-deﬁned and ﬁnite even without
P [(xi
j)2] by relating it to the linear monomial
these bounds. However  we can tighten the bound on P [(xi
j. One simple way to obtain bounds on activations
P [(xi
j is to treat each hidden unit separately  using simple interval arithmetic to obtain
xi

j)] via bounds on the value of the activation xi

l0 = ¯x−1 (Attack model) 
li = [W i−1]+li−1 +[W i−1]−ui−1 

u0 = ¯x+1 (Attack model) 
ui = [W i−1]+ui−1 +[W i−1]−li−1 

(8)

where ([M ]+)ij = max(Mij 0) and ([M ]−)ij = min(Mij 0).
In our experiments on real networks (Section 6)  we observe that these simple bounds are sufﬁcient
to obtain good certiﬁcates. However tighter bounds could potentially lead to tighter certiﬁcates.

6 Experiments

In this section  we evaluate the performance of our certiﬁcate (7) on neural networks trained using
different robust training procedures  and compare against other certiﬁcates in the literature.
Networks. We consider feedforward networks that are trained on the MNIST dataset of handwritten
digits using three different robust training procedures.

1. Grad-NN. We use the two-layer network with 500 hidden nodes from [23]  obtained by using
an SDP-based bound on the gradient of the network (different from the SDP presented here) as a
regularizer. We obtained the weights of this network from the authors of [23].

2. LP-NN. We use a two-layer network with 500 hidden nodes (matching that of Grad-NN) trained

via the LP-based robust training procedure of [29]. The authors of [29] provided the weights.

7

(a)

(b)

Figure 3: Histogram of PGD margins for (a) points that are certiﬁed by the SDP and (b) points that
are not certiﬁed by the SDP.

3. PGD-NN. We consider a fully-connected network with four layers containing 200 100 and 50
hidden nodes (i.e.  the architecture is 784-200-100-50-10). We train this network using adversarial
training [12] against the strong PGD attack [21]. We train to minimize a weighted combination
of the regular cross entropy loss and adversarial loss. We tuned the hyperparameters based on the
performance of the PGD attack on a holdout set. The stepsize of the PGD attack was set to 0.1  number
3.
of iterations to 40  perturbation size  = 0.3 and weight on adversarial loss to 1
The training procedures for SDP-NN and LP-NN yield certiﬁcates of robustness (described in their
corresponding papers)  but the training procedure of PGD-NN does not. Note that all the networks
are “foreign networks” to our SDP  as their training procedures do not incorporate the SDP relaxation.
Certiﬁcation procedures. Recall from Section 2 that an upper bound on the maximum incorrect
margin can be used to obtain certiﬁcates. We consider certiﬁcates from three different upper bounds.
1. SDP-cert. This is the certiﬁcate we propose in this work. This uses the SDP upper bound that we
deﬁned in Section 5. The exact optimization problem is presented in (7) and the bounds on intermediate
activations are obtained using the interval arithmetic procedure presented in (8).

2. LP-cert. This uses the upper bound based on the LP relaxation discussed in Section 4.2 which
forms the basis for several existing works on scalable certiﬁcation [9  10  28  29]. The LP uses
layer-wise bounds for intermediate nodes  similar to li ui in our SDP formulation (7). For Grad-NN
and LP-NN with a single hidden layer  the layerwise bounds can be computed exactly using interval
arithmetic. For the four-layer PGD-NN  in order to have a fair comparison with SDP-cert  we use
the same procedure (interval arithmetic) (8).

3. Grad-cert. We use the upper bound proposed in [23]. This upper bound is based on the maximum

norm of the gradient of the network predictions and only holds for two-layer networks.
Table 1 presents the performance of the three different certiﬁcation procedures on the three networks.
For each certiﬁcation method and network  we evaluate the associated upper bounds on the same
1000 random test points and report the fraction of points that were not certiﬁed. Computing the exact
worst-case adversarial error is not computationally tractable. Therefore  to provide a comparison 
we also compute a lower bound on the adversarial error—the error obtained by the PGD attack.
Performance of proposed SDP-cert. SDP-cert provides non-vacuous certiﬁcates for all networks
considered. In particular  we can certify that the four layer PGD-NN has an error of at most 18% at
 = 0.1. To compare  a lower bound on the robust error (PGD attack error) is 9%. On the two-layer
networks  SDP-cert improves the previously-known bounds. For example  it certiﬁes that Grad-NN
has an error of at most 20% compared to the previously known 35%. Similarly  SDP-cert improves
the bound for LP-NN from 22% to 20%.
The gap between the lower bound (PGD) and upper bound (SDP) is because of points that cannot
be misclassiﬁed by PGD but are also not certiﬁed by the SDP. In order to further investigate these
points  we look at the margins obtained by the PGD attack to estimate the robustness of different points.
Formally  let xPGD be the adversarial example generated by the PGD attack on clean input ¯x with true
[f (xPGD)¯y−f (xPGD)y]  the margin of the closest incorrect class. A small
label ¯y. We compute min
y(cid:54)=¯y
value indicates that the xPGD was close to being misclassiﬁed. Figure 3 shows the histograms of the
above PGD margin. The examples which are not certiﬁed by the SDP have much smaller margins
than those examples that are certiﬁed: the average PGD margin is 1.2 on points that are not certiﬁed

8

0246810Margin050100150200250300Number of pointsPGD Margin of closest incorrect class for SDP verified points0.00.51.01.52.02.53.03.54.0Margin051015202530Number of pointsPGD Margin of closest incorrect class for SDP unverified pointsand 4.5 on points that are certiﬁed. From Figure 3  we see that a large number of the SDP uncertiﬁed
points have very small margin  suggesting that these points might be misclassiﬁed by stronger attacks.
Remark. As discussed in Section 5  we could consider a version of the SDP that does not include the
constraints relating linear and quadratic terms at the intermediate layers of the network. Empirically 
such an SDP produces vacuous certiﬁcates (> 90% error). Therefore  these constraints at intermediate
layers play a signiﬁcant role in improving the empirical performance of the SDP relaxation.
Comparison with other certiﬁcation approaches. From Table 1  we observe that SDP-cert
consistently performs better than both LP-cert and Grad-cert for all three networks.
Grad-cert and LP-cert provide vacuous (> 90% error) certiﬁcates on networks that are not trained
to minimize these certiﬁcates. This is because these certiﬁcates are tight only under some special cases
that can be enforced by training. For example  LP-cert is tight when the ReLU units do not switch
linear regions [29]. While a typical input causes only 20% of the hidden units of LP-NN to switch
regions  75% of the hidden units of Grad-NN switch on a typical input. Grad-cert bounds the gradient
uniformly across the entire input space. This makes the bound loose on arbitrary networks that could
have a small gradient only on the data distribution of interest.
Comparison to concurrent work [26]. A variety of robust MNIST networks are certiﬁed by Tjeng
and Tedrake [26]. On Grad-NN  their certiﬁed error is 30% which is looser than our SDP certiﬁed error
(20%). They also consider the CNN counterparts of LP-NN and PGD-NN  trained using the procedures
of [29] and [21]. The certiﬁed errors are 4.4% and 7.2% respectively. This reduction in the errors is
due to the CNN architecture. Further discussion on applying our SDP to CNNs appears in Section 7.
Optimization setup. We use the YALMIP toolbox [19] with MOSEK as a backend to solve the
different convex programs that arise in these certiﬁcation procedures. On a 4-core CPU  the average
SDP computation took around 25 minutes and the LP around 5 minutes per example.

7 Discussion

In this work  we focused on fully connected feedforward networks for computational efﬁciency. In
principle  our proposed SDP can be directly used to certify convolutional neural networks (CNNs);
unrolling the convolution would result in a (large) feedforward network. Naively  current off-the-shelf
solvers cannot handle the SDP formulation of such large networks. Robust training on CNNs leads
to better error rates: for example  adversarial training against the PGD adversary on a four-layer
feedforward network has error 9% against the PGD attack  while a four-layer CNN trained using a
similar procedure has error less than 3% [21]. An immediate open question is whether the network
in [21]  which has so far withstood many different attacks  is truly robust on MNIST. We are hopeful
that we can scale up our SDP to answer this question  perhaps borrowing ideas from work on highly
scalable SDPs [1] and explicitly exploiting the sparsity and structure induced by the CNN architecture.
Current work on certiﬁcation of neural networks against adversarial examples has focused on
perturbations bounded in some norm ball. In our work  we focused on the common (cid:96)∞ attack because
the problem of securing multi-layer ReLU networks remains unsolved even in this well-studied attack
model. Different attack models lead to different constraints only at the input layer; our SDP framework
can be applied to any attack model where these input constraints can be written as linear and quadratic
constraints. In particular  it can also be used to certify robustness against attacks bounded in (cid:96)2 norm.
[13] provide alternative bounds for (cid:96)2 norm attacks based on the local gradient.
Guarantees for the bounded norm attack model in general are sufﬁcient but not necessary for robustness
against adversaries in the real world. Many successful attacks involve inconspicious but clearly visible
perturbations [11  24  6  4]  or large but semantics-preserving perturbations in the case of natural
language [15]. These perturbations do not currently have well-deﬁned mathematical models and
present yet another layer of challenge. However  we believe that the mathematical ideas we develop
for the bounded norm will be useful building blocks in the broader adversarial game.

9

the

for

are

All

code 

data
platform at

Codalab

available
and experiments
https://worksheets.codalab.org/worksheets/

Reproducibility.
on
0x6933b8cdbbfd424584062cdf40865f30/.
Acknowledgements. This work was partially supported by a Future of Life Institute Research Award
and Open Philanthrophy Project Award. JS was supported by a Fannie & John Hertz Foundation
Fellowship and an NSF Graduate Research Fellowship. We thank Eric Wong for providing relevant
experimental results. We are also grateful to Moses Charikar  Zico Kolter and Eric Wong for several
helpful discussions and anonymous reviewers for useful feedback.

this paper

References
[1] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: more tractable alternatives

to sum of squares and semideﬁnite optimization. arXiv preprint arXiv:1706.02586  2017.

[2] A. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint

arXiv:1707.07397  2017.

[3] A. Athalye  N. Carlini  and D. Wagner. Obfuscated gradients give a false sense of security:

Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420  2018.

[4] T. B. Brown  D. Mané  A. Roy  M. Abadi  and J. Gilmer. Adversarial patch. arXiv preprint

arXiv:1712.09665  2017.

[5] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy  pages 39–57  2017.

[6] N. Carlini  P. Mishra  T. Vaidya  Y. Zhang  M. Sherr  C. Shields  D. Wagner  and W. Zhou. Hidden

voice commands. In USENIX Security  2016.

[7] N. Carlini  G. Katz  C. Barrett  and D. L. Dill. Ground-truth adversarial examples. arXiv  2017.

[8] K. Dvijotham  S. Gowal  R. Stanforth  R. Arandjelovic  B. O’Donoghue  J. Uesato  and P. Kohli.

Training veriﬁed learners with learned veriﬁers. arXiv preprint arXiv:1805.10265  2018.

[9] K. Dvijotham  R. Stanforth  S. Gowal  T. Mann  and P. Kohli. A dual approach to scalable

veriﬁcation of deep networks. arXiv preprint arXiv:1803.06567  2018.

[10] R. Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In International
Symposium on Automated Technology for Veriﬁcation and Analysis (ATVA)  pages 269–286  2017.

[11] I. Evtimov  K. Eykholt  E. Fernandes  T. Kohno  B. Li  A. Prakash  A. Rahmati  and D. Song.

Robust physical-world attacks on machine learning models. arXiv  2017.

[12] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples.

In International Conference on Learning Representations (ICLR)  2015.

[13] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classiﬁer against
adversarial manipulation. In Advances in Neural Information Processing Systems (NIPS)  pages
2263–2273  2017.

[14] S. Huang  N. Papernot  I. Goodfellow  Y. Duan  and P. Abbeel. Adversarial attacks on neural

network policies. arXiv  2017.

[15] R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In

Empirical Methods in Natural Language Processing (EMNLP)  2017.

[16] G. Katz  C. Barrett  D. Dill  K. Julian  and M. Kochenderfer. Reluplex: An efﬁcient SMT solver

for verifying deep neural networks. arXiv preprint arXiv:1702.01135  2017.

[17] G. Katz  C. Barrett  D. L. Dill  K. Julian  and M. J. Kochenderfer. Towards proving the adversarial

robustness of deep neural networks. arXiv  2017.

[18] J. Z. Kolter and E. Wong. Provable defenses against adversarial examples via the convex outer

adversarial polytope (published at ICML 2018). arXiv preprint arXiv:1711.00851  2017.

10

[19] J. Löfberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In CACSD  2004.

[20] J. Lu  H. Sibai  E. Fabry  and D. Forsyth. No need to worry about adversarial examples in object

detection in autonomous vehicles. arXiv preprint arXiv:1707.03501  2017.

[21] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations (ICLR) 
2018.

[22] N. Papernot  P. McDaniel  X. Wu  S. Jha  and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In IEEE Symposium on Security and Privacy  pages
582–597  2016.

[23] A. Raghunathan  J. Steinhardt  and P. Liang. Certiﬁed defenses against adversarial examples.

In International Conference on Learning Representations (ICLR)  2018.

[24] M. Sharif  S. Bhagavatula  L. Bauer  and M. K. Reiter. Accessorize to a crime: Real and stealthy
attacks on state-of-the-art face recognition. In ACM SIGSAC Conference on Computer and
Communications Security  pages 1528–1540  2016.

[25] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR) 
2014.

[26] V. Tjeng and R. Tedrake. Verifying neural networks with mixed integer programming. arXiv

preprint arXiv:1711.07356  2017.

[27] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv  2010.

[28] T. Weng  H. Zhang  H. Chen  Z. Song  C. Hsieh  D. Boning  I. S. Dhillon  and L. Daniel. Towards
fast computation of certiﬁed robustness for relu networks. arXiv preprint arXiv:1804.09699  2018.

[29] E. Wong and J. Z. Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning (ICML)  2018.

[30] E. Wong  F. Schmidt  J. H. Metzen  and J. Z. Kolter. Scaling provable adversarial defenses. arXiv

preprint arXiv:1805.12514  2018.

11

,Aditi Raghunathan
Jacob Steinhardt
Percy Liang