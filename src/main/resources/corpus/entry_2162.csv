2019,From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization,We present a new algorithm (ASEBO) for optimizing high-dimensional blackbox functions. ASEBO adapts to the geometry of the function and learns optimal sets of sensing directions  which are used to probe it  on-the-fly. It addresses the exploration-exploitation trade-off of blackbox optimization with expensive blackbox queries by continuously learning the bias of the lower-dimensional model used to approximate gradients of smoothings of the function via compressed sensing and contextual bandits methods. To obtain this model  it leverages techniques from the emerging theory of active subspaces in a novel ES blackbox optimization context. As a result  ASEBO learns the dynamically changing intrinsic dimensionality of the gradient space and adapts to the hardness of different stages of the optimization without external supervision. Consequently  it leads to more sample-efficient blackbox optimization than state-of-the-art algorithms. We provide theoretical results and test ASEBO advantages over other methods empirically by evaluating it on the set of reinforcement learning policy optimization tasks as well as functions from the recently open-sourced Nevergrad library.,From Complexity to Simplicity: Adaptive ES-Active

Subspaces for Blackbox Optimization

Krzysztof Choromanski⇤
Google Brain Robotics
kchoro@google.com

Aldo Pacchiano⇤

UC Berkeley

Jack Parker-Holder⇤
University of Oxford

pacchiano@berkeley.edu

jackph@robots.ox.ac.uk

Yunhao Tang⇤

Columbia University

yt2541@columbia.edu

Vikas Sindhwani

Google Brain Robotics

sindhwani@google.com

Abstract

We present a new algorithm (ASEBO) for optimizing high-dimensional blackbox
functions. ASEBO adapts to the geometry of the function and learns optimal
sets of sensing directions  which are used to probe it  on-the-ﬂy. It addresses the
exploration-exploitation trade-off of blackbox optimization with expensive black-
box queries by continuously learning the bias of the lower-dimensional model used
to approximate gradients of smoothings of the function via compressed sensing and
contextual bandits methods. To obtain this model  it leverages techniques from the
emerging theory of active subspaces [8] in a novel ES blackbox optimization con-
text. As a result  ASEBO learns the dynamically changing intrinsic dimensionality
of the gradient space and adapts to the hardness of different stages of the optimiza-
tion without external supervision. Consequently  it leads to more sample-efﬁcient
blackbox optimization than state-of-the-art algorithms. We provide theoretical
results and test ASEBO advantages over other methods empirically by evaluating
it on the set of reinforcement learning policy optimization tasks as well as functions
from the recently open-sourced Nevergrad library.

1

Introduction

Consider a high-dimensional function F : Rd ! R. We assume that querying it is expensive.
Examples include reinforcement learning (RL) blackbox functions taking as inputs vectors ✓ encoding
policies ⇡ : S!A mapping states to actions and outputting total (expected/discounted) rewards
obtained by agents applying ⇡ in given environments [6]. For this class of functions evaluations
usually require running a simulator. Other examples include wind conﬁguration design optimization
problems for high speed civil transport aircrafts  optimizing computer codes (e.g. NASA synthetic
tool FLOPS/ENGENN used to size the aircraft and propulsion system [2])  crash tests  medical and
chemical reaction experiments [37].
Evolution strategy (ES) methods have traditionally been used in low-dimensional regimes (e.g.
hyperparameter tuning)  and considered ill-equipped for higher dimensional problems due to poor
sampling complexity [27]. However  a ﬂurry of recent work has shown they can scale better than
previously believed [33  11  29  25  7  30  21]. This is thanks to a couple of reasons.
First of all  new ES methods apply several efﬁcient heuristics (ﬁltering  various normalization
techniques as in [25] and new exploration strategies as in [11]) in order to substantially improve

⇤Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

sampling complexity. Other recent methods [29  7] are based on more accurate Quasi Monte Carlo
(MC) estimators of the gradients of Gaussian smoothings of blackbox functions with theoretical
guarantees. These approaches provide better quality gradient sensing mechanisms. Additionally 
in applications such as RL  new compact structured policy architectures (such as low-displacement
rank neural networks from [7] or even linear policies [14]) are used to reduce the number of policies’
parameters and dimensionality of the optimization problem.
Recent research also shows that ES-type blackbox optimization in RL leads to more stable policies
than policy gradient methods since ES methods search for parameters that are robust to perturbations
[19]. Unlike policy gradient methods  ES aims to ﬁnd parameters maximizing expected reward (rather
than just a reward) in respect to Gaussian perturbations.
Finally  pure ES methods as opposed to state-of-the-art policy optimization techniques (TRPO  PPO
or ARS [32  15  31  25])  can be applied also for blackbox optimization problems that do not exhibit
MDP structure required for policy gradient methods and cannot beneﬁt from state normalization
algorithm central to ARS. This has led to their recent popularity for non-differentiable tasks [17  14].
In this paper we introduce a new adaptive sample-efﬁcient blackbox optimization algorithm. ASEBO
adapts to the geometry of blackbox functions and learns optimal sets of sensing directions  which
are used to probe them  on-the-ﬂy. To do this  it leverages techniques from the emerging theory of
active subspaces [8  10  9  20] in a novel ES blackbox optimization context. Active subspaces and
their extensions are becoming popular as effective techniques for dimensionality reduction (see for
instance: active manifolds [5] or ResNets for learning isosurfaces [36]). However  to the best of our
knowledge we are the ﬁrst to apply active subspace ideas for ES optimization.
ASEBO addresses the exploration-exploitation trade-off of blackbox optimization with expensive
function queries by continuously learning the bias of the lower-dimensional model used to approx-
imate gradients of smoothings of the function with compressed sensing and contextual bandits
methods. The adaptiveness is what distinguishes it from some recently introduced guided ES methods
such as [24] that rely on ﬁxed hyperparameters that are hard to tune in advance (e.g. the length of
the buffer deﬁning lower dimensional space for gradient search). We provide theoretical results and
empirically evaluate ASEBO on a set of RL blackbox optimization tasks as well as non-RL blackbox
functions from the recently open-sourced Nevergrad library [34]  showing that it consistently learns
optimal inputs with fewer queries to a blackbox function than other methods.
ASEBO versus CMA-ES: There have been a variety of works seeking to reduce sampling complexity
for ES methods through the use of metric learning. The prominent class of the covariance matrix
adaptation evolution strategy (CMA-ES) methods derives state-of-the-art derivative free blackbox
optimization algorithms  which seek to learn and maintain a fully parameterized Gaussian distribution.
CMA-ES suffers from quadratic time complexity for each evaluation which can be limiting for high
dimensional problems. As such  a series of attempts have been made to produce scalable variants
of CMA-ES  by restricting the covariance matrix to the diagonal (sep-CMA-ES [28]) or a low rank
approximation as in VD-CMA-ES [3] and LM-CMA-ES [22]. Two recent algorithms  VkD-CMA-ES
[4] and LM-MA-ES [23]  seek to combine the above ideas and have been shown to be successful in
large-scale settings  including RL policy learning [26]. Although these methods are able to quickly
learn and adapt the covariance matrix  they are heavily dependent on hyperparameter selection [4  35]
and lack the means to avoid learning a bias. As our experiments show  this can severely hurt their
performance. The best CMA-ES variants often struggle with RL tasks of challenging objecive
landscapes  displaying inconsistent performance across tasks. Furthermore  they require careful
hyperparameter tuning for good performance (see: analysis in Section 4  Fig. 3).

2 Adaptive Sample-Efﬁcient Blackbox Optimization

Before we describe ASEBO  we explain key theoretical ideas behind the algorithm. ASEBO uses
active 
online PCA to maintain and update on-the-ﬂy subspaces which we call ES-active subspaces LES
accurately approximating the gradient data space at a given phase of the algorithm. The bias of the
obtained gradient estimators is measured by sensing the length of its component from the orthogonal
complement LES ?active via compressed sensing or computing optimal probabilities for exploration (e.g.
sensing from LES ?active) via contextual bandits methods [1]. The algorithm corrects its probabilistic
distributions used for choosing directions for gradient sensing based on these measurements. As
we show  we can measure that bias accurately using only a constant number of additional function

2

(a) HC: active subspace (b) SW: active subspace

(c) HC: # of samples

(d) SW: # of samples

Figure 1: The motivation behind ASEBO. Two ﬁrst plots: ES baseline for HalfCheetah and
Swimmer tasks from the OpenAI Gym library for 212-dimensional policies - the plot shows how
the dimensionality of the space capturing a given percentage of variance of approximate gradient data
depends on the iteration of the algorithm. This information is never exploited by the algorithm  even
though 99.5% of the variance resides in the much lower-dimensional space (100 dimensions). Two
last plots: ASEBO taking advantage of this information (# of sample/sensing directions reﬂects the
hardness of the optimization at each iteration and is strongly correlated with the PCA dimensionality.

queries  regardless of the dimensionality. This in turn determines an exploration strategy  as we
explain later. Estimated gradients are then used to update parameters.

2.1 Preliminaries
Consider a blackbox function F : Rd ! R. We do not assume that F is differentiable. The
Gaussian smoothing [27] F of F parameterized by smoothing parameter > 0 is given as:
2RRd F (✓ + g)e kgk2
F(✓) = Eg2N (0 Id)[F (✓ + g)] = (2⇡) d
2 dg. The gradient of the Gaussian
smoothing of F is given by the formula:
1
 Eg⇠N (0 Id)[F (✓ + g)g].
rF(✓) =

(1)

MCF(✓) = 1

MCF(✓) = 1

kPk

Formula 1 on rF(✓) leads straightforwardly to several unbiased Monte Carlo (MC) estimators
of rF(✓)  where the most popular ones are: the forward ﬁnite difference estimator [7] deﬁned
as: brFD
i=1(F (✓ + gi)  F (✓))gi  and an antithetic ES gradient estimator [30]
2kPk
given as: brAT
i=1(F (✓ + gi)  F (✓  gi))gi  where typically g1  ...  gk are
taken independently at random from N (0  Id) of from more complex joint distributions for variance
reduction (see: [7]). We call samples g1  ...  gk the sensing directions since they are used to sense
gradients rF(✓). The antithetic formula can be alternatively rationalized as giving the renormalized
gradient of F (if F is smooth)  if not taking into account cubic and higher-order terms of the Taylor
expansion F (✓ + v) = F (✓) +rF >v + 1
2 v>H(✓)v (where H(✓) stands for the Hessian of F in ✓).
Standard ES methods apply different gradient-based techniques such as SGD or Adam  fed with
the above MC estimators of rF to conduct blackbox optimization. The number of samples k per
iteration of the optimization procedure is usually of the order O(d). This becomes a computational
bottleneck for high-dimensional blackbox functions F (for instance  even for relatively small RL
tasks with policies encoded by compact neural networks we still have d > 100 parameters).
2.2 ES-active subspaces via online PCA with decaying weights
The ﬁrst idea leading to the ASEBO algorithm is that in practice one does not need to estimate the
gradient of F accurately (after all ES-type methods do not even aim to compute the gradient of F   but
rather focus on rF). Poor scalability of ES-type blackbox optimization algorithms is caused by high-
dimensionality of the gradient vector. However  during the optimization process the space spanned
by gradients may be locally well approximated by a lower-dimensional subspace L and sensing
the gradient in that subspace might be more effective. In some recent papers such as [24] such a
subspace is deﬁned simply as L = span{brAT
MCF(✓is+1)}  where
{brAT
MCF(✓i) brAT
MCF(✓is+1)} stands for the batch of last s approximated
gradients during the optimization process and s is a ﬁxed hyperparameter. Even though L will
dynamically change during the optimization  such an approach has several disadvantages in practice.
Tuning parameter s is very difﬁcult or almost impossible and the assumption that the dimensionality
of L should be constant during optimization is usually false. In our approach  dimensionality of L
varies and depends on the hardness of the optimization in different optimization stages.

MCF(✓i1)  ... brAT

MCF(✓i) brAT

MCF(✓i1)  ... brAT

3

We apply Principal Component Analysis (PCA  [18]) to create a subspace L capturing particular
variance > 0 of the approximate gradients data. This data is either: the approximate gradients
computed in previous iterations from the antithetic formula or: the elements of the sum from that
equation that are averaged over to obtain these gradients. For clarity of the exposition  from now on
we will assume the former  but both variants are valid. Choosing  is in practice much easier than s
and leads to subspaces L of varying dimensionalities throughout the optimization procedure  called
by us from now on ES-active subspaces LES
Algorithm 1 ASEBO Algorithm
Hyperparameters: number of iterations of full sampling l  smoothing parameter > 0  step size ⌘ 
PCA threshold ✏  decay rate   total number of iterations T .
Input: blackbox function F   vector ✓0 2 Rd where optimization starts. Cov0 2{ 0}d⇥d  p0 = 0.
Output: vector ✓T .
for t = 0  . . .   T  1 do

active.

if t < l then

else

active

MCF (✓t) = 1

i=1 i  ✏Pd

j=1(F (✓t + gj)  F (✓t  gj))gj.

MCF(✓t))>.

MCF(✓t)(brAT

MCF (✓t) as: brAT

Take nt = d. Sample g1 ···   gnt from N (0  Id) (independently).
1. Take top r eigenvalues i of Covt  where r is smallest such that:Pr
i=1 i 
using its SVD as described in text and take nt = r.
2. Take the corresponding eigenvectors u1  ...  ur 2 Rd and let U 2 Rd⇥r be obtained
by stacking them together. Let Uact 2 Rd⇥r be obtained from stacking together some
def= span{u1  ...  ur}. Let U? 2 Rd⇥(dr) be obtained from
orthonormal basis of LES
stacking together some orthonormal basis of the orthogonal complement LES ?active of LES
active.
3. Sample nt vectors g1  ...  gnt as follows: with probability 1  pt from N (0  U?(U?)>)
and with probability pt from N (0  Uact(Uact)>).
4. Renormalize g1  ...  gnt such that marginal distributions kgik2 are (d).
2ntPnt
1. Compute brAT
2. Set Covt+1 = Covt + (1  )  where = brAT
3. Set pt+1 = popt for popt output by Algorithm 2 and: ✓t+1 ✓t + ⌘brAT
These will be in turn applied to deﬁne covariance matrices encoding probabilistic distributions applied
to construct sensing directions used for estimating rF(✓). The additional advantage of our approach
is that PCA automatically ﬁlters out gradient noise.
We use our own online version of PCA with decaying weights (decay rate is deﬁned by parameter
> 0). By tuning  we can deﬁne the rate at which historical approximate gradient data is used
to choose the right sensing directions  which will continuously decay. We consider a stream of
approximate gradients brAT
MCF(✓0)  ...brAT
MCF(✓i)  ... obtained during the optimization procedure.
We maintain and update on-the-ﬂy the covariance matrix Covt  where t stands for the number of
completed iterations  in the form of the symmetric matrix SVD-decomposition Covt = Q>t ⌃tQt 2
Rd. When the new approximate gradient brAT
MCF(✓t) arrives  the update of the covariance matrix is
driven by the following equation  reﬂecting data decay process  where xt = brAT
To conduct the update cheaply  it sufﬁces to observe that the RHS of Equation 2 can be rewritten
as: Q>t ⌃tQt + (1  )xtx>t = Q>t (⌃t + (1  )Qtxt(Qtxt)>)Qt. Now  using the fact that
for a matrix of the form D + uu>  we can get its decomposition in time O(d2) [13]  we obtain an
algorithm performing updates in quadratic time. That in practice sufﬁces since the bottleneck of the
computations is in querying F and additional overhead related to updating LES
ES-active subspaces versus active subspaces: Our mechanism for constructing LES
active is inspired
by the recent theory of active subspaces [8]  developed to determine the most important directions in
the space of input parameters of high-dimensional blackbox functions such as computer simulations.
The active subspace of a differentiable function F : Rd ! R  square-integrable with respect to the
given probabilistic density function ⇢ : Rd ! R  is given as a linear subspace Lactive deﬁned by the

Q>t+1⌃t+1Qt+1 = Q>t ⌃tQt + (1  )xtx>t  

MCF(✓t):

(2)

MCF (✓t).

active is negligible.

4

ﬁrst r (for a ﬁxed r < d) eigenvectors of the following d ⇥ d symmetric positive deﬁnite matrix:

Cov =Zx2Rd rF (x)rF (x)>⇢(x)dx

(3)

Density function ⇢ determines where compact representation of F is needed. In our approach we do
not assume that rF exists  but the key difference between LES
active and Lactive lies somewhere else.
The goal of ASEBO is to avoid approximating the exact gradient rF (x) 2 Rd which is what makes
standard ES methods very expensive and which is done in [9] via gradient sketching techniques
combined with ﬁnite difference approaches (standard methods of choice for ES baselines).
Algorithm 2 Explore estimator via exponentiated sampling
Hyperparameters: smoothing parameter   horizon C  learning rate ↵  probability regularizer  
initial probability parameter qt
Input: subspaces: LES
Output:
for l = 1 ···   C + 1 do

active  LES ?active  function F   vector ✓t

0 2 (0  1).

l1 +  and sample at

l).
l ⇠ Ber(pt

)  otherwise sample gl ⇠N (0  I

).

LES ?active

active

l1 = (1  2)qt

1. Compute pt
3. If at
4. Compute vl = 1

l = 1  sample gl ⇠N (0  ILES
l (dim(LES
active)+2)
(pt
l )3
l )(dim(LES ?active)+2)

2 (F (✓t + gl)  F (✓t  gl)).
⌘
(1pt
qt
l1 exp(↵el(1))
l1) exp(↵el(2)).

5. Set el = (1  2)24 ⇣ at
⇣ (1at

qt
l1 exp(↵el(1))+(1qt

l =

l )3

⌘35 v2

l .

6. Set qt
Return: pC.

active is itself used to deﬁne sensing directions and
Instead  in ASEBO an ES-active subspace LES
active. This drastically reduces
the number of chosen samples k is given by the dimensionality of LES
sampling complexity  but comes at a price of risking the optimization to be trapped in the ﬁxed
lower-dimensional space that will not be representative for gradient data as optimization progresses.
We propose a solution requiring only a constant number of extra queries to F in the next sections.

2.3 Exploration-exploitation trade-off: Adaptive Exploration Mechanism
The procedure described above needs to be accompanied with an exploration strategy that will
determine how frequently to choose sensing directions outside the constructed on-the-ﬂy lower-
dimensional ES-subspace LES
active. Our exploration strategies will be encoded by hybrid probabilistic
distributions for sampling sensing directions. The frequency of sensing from the distributions with
covariance matrices obtained from LES
active (corresponding to exploitation) and from its orthogonal
complement LES ?active or entire space (corresponding to exploration) will be given by weights encoding
the importance of exploitation versus exploration in any given iteration of the optimization. For a
active and by x? its projection onto LES ?active.
vector x 2 Rd denote by xactive its projection onto LES
The useful metric that can be used to update the above weights in an online manner in the tth iteration
of the algorithm is the ratio: r = k(rF(✓t))activek2
. Smaller values of r indicate that current active
k(rF(✓t))?k2
subspace is not representative enough for the gradient and more aggressive exploration needs to be

MCF(✓t1))activek2
MCF(✓t1))?k2

conducted. In practice  we do not compute r explicitly  but rather its approximated versionbr.
One can simply take: br = k(brAT
k(brAT

MCF(✓t1) is obtained in the pre-
vious iteration. But we can do better. It sufﬁces to separately estimate k(rF(✓t))activek2 and
k(rF(✓t))?k2. However we do not aim to estimate (rF(✓t))active and (rF(✓t))?. That would
be equivalent to computing exact estimate of rF(✓t)  defeating the purpose of ASEBO. Instead 
we note that estimating the length of the unknown high-dimensional vector is much simpler than
estimating the vector itself and can be done in the probabilistic manner with arbitrary precision via the
set of dot-product queries of size independent from dimensionality d via compressed sensing methods.
We reﬁne this approach and propose more accurate contextual bandits method that also relies on
dot-product queries applied in the ES-context  but aims to directly approximate optimal probabilities

  where brAT

5

of sampling from LES
active rather than approximating gradients components’ lengths (see Algorithm
2 box  the compressed sensing baseline is presented in the Appendix). The related computational
overhead is measured in constant number of extra function queries  negligible in practice.
2.4 The Algorithm

active versus from outside LES

ASEBO is given in the Algorithm 1 box. The algorithm we apply to score relative importance of
sampling from the ES-active subspace LES
active is in the Algorithm 2 box.
As we have already mentioned  it uses bandits method do determine optimal probability of sampling
from LES
active. In the next section we show that by using these techniques we can substantially reduce
the variance of ES blackbox gradient estimators if ES-active subspaces approximate the gradient
data well (which is the case for RL blackbox functions as presented in Fig. 1). Horizon lengths C in
Algorithm 2 which determines the number of extra function queries should be in practice chosen as
small constants. In each iteration of Algorithm 1 the number of function queries is proportional to
the dimensionality of the ES-active subspace LES
3 Theoretical Results

active rather than the original space.

1

2

).

We provide here theoretical guarantees for the ASEBO sampling mechanism (in Algorithm 1)  where

sensing directions {gi} at time t are sampled from the hybrid distribution bP : with probability pt
from N (0  ILactive) and with probability 1  pt from N (0  IL?active
Following notation in Algorithm 1  let Uact 2 Rd⇥r be obtained by stacking together vectors of
some orthonormal basis of LES
active) = r and let U? 2 Rd⇥(dr) be obtained my
stacking together vectors of some orthonormal basis of its orthogonal complement LES ?active. Denote
by  a smoothing parameter. We make the following regularity assumptions on F :
Assumption 1. F is LLipschitz  i.e. for all ✓  ✓0 2 Rd  |F (✓)  F (✓0)| Lk✓  ✓0k2.
Assumption 2. F has a ⌧-smooth third order derivative tensor with respect to > 0  so that
F (✓ + g) = F (✓) + rF (✓)>g + 2
6 3F 000(✓)[v  v  v] for some v 2 Rd
(kvk2  kgk2) satisfying |F 000(✓)[v  v  v]  ⌧kvk3

active  where dim(LES

2 g>H(✓)g + 1

2.
2  ⌧kgk3

F (✓+g)g+F (✓+g)(g)

MC k=1 F(✓) = C1

MC k=1 F(✓)i  rF (✓)  ✏.

Observe that: Eg⇠bP⇥gg>⇤ = ⇣ptUact (Uact)> + (1  pt)U?U?>⌘. Deﬁne C1 =
⇣ptUact (Uact)> + (1  pt)U?U?>⌘. Let brAT asebo
be the gradient estimator corresponding to bP . We will assume that  is small enough  i.e.
35q ✏ min(pt 1pt)
for some precision parameter ✏> 0. Our ﬁrst result shows that under
< 1
these assumptions  baseline and ASEBO estimators of rF (✓) are also good estimators of rF (✓):
Lemma 3.1. If F satisﬁes Assumptions 1 and 2  the estimatorsbrAT base
MC k=1F(✓) andbrAT asebo
MC k=1 F(✓)
MC k=1F(✓)i  rF (✓)  ✏ and
are close to the true gradient rF (✓)  i.e.: Eg⇠N (0 Id)hbrAT base
Eg⇠bPhbrAT asebo
We show now that under sampling strategy given by distribution bP   the variance of the gradient
estimator can be made smaller by choosing the probability parameter pt appropriately. Denote:
active) and d? = dim(LES ?active). Let := dactive+2
1pt sU?  krF (✓)k2.
sUact + d?+2
dactive = dim(LES
Theorem 3.2. The following holds for sUact = k(Uact)>rF (✓)k2
2 and sU? = k(U?)>rF (✓)k2
2:
1. The variance of brAT asebo
MC k=1 F(✓) is close to   i.e. |Var[brAT asebo
MC k=1 F(✓)]  | ✏.
p(sUact )(dactive+2)
p(sUact )(dactive+2)+p(sU? )(dU? +2)
satisﬁes:
hp(sUact)(dactive + 2) +p(sU?)(d? + 2)i2
 krF (✓)k2.

⇤ :=
the optimal variance Varopt corresponding to pt
⇤

and
|Varopt  | ✏ for =

3.1 Variance reduction via non isotropic sampling

2. The choice of pt that minimizes  satisﬁes pt

⌧d 3 max(L 1)

pt

6

.



{z

MC k=1F(✓)].

2 and sU? = ↵krF (✓)k2
2 

Furthermore  slack variable  is always nonnegative.

MC k=1F(✓)]+✏|p(sU?)(dactive + 2) p(sUact)(d? + 2)|2  2krF (✓)k2
}

|
for
MC k=1F(✓)] ⇡ (d + 1)krF (✓)k2 whereas Varopt =
1: Varopt ⌧

3. Varopt  Var[brAT base
Theorem implies that when sUact = (1  ↵)krF (✓)k2
some ↵ 2 (0  1)  we have: Var[brAT base
O ((1  ↵)(dactive + 1) + ↵(d? + 1)). When dactive << d and ↵<<
Var[brAT base
3.2 Adaptive Mirror Descent
active and pt  the gradient estimator
In Theorem 3.2 we showed that for appropriate choices of LES
brAT asebo
MC k=1 F(✓) will have signiﬁcantly smaller variance than brAT base
MC k=1F(✓). In this section we
show that Algorithm 2 provides an adaptive way to choose pt. Using tools from online learning
theory  we provide regret guarantees that quantify the rate at which this Algorithm 2 minimizes the
variance of brAT asebo
l. The main component  of the variance of brAT asebo
l = pt

l (2) sU?  krF (✓)k2 (Theorem 3.2). We have:

MC k=1 F(✓) and converges to the optimal pt
⇤

MC k=1 F(✓) as a function of pt
l

l) = dactive+2

l (1) sUact + d?+2

pt

pt

Let pt
1pt
equals = `(pt

l

.

Theorem 3.3. Let 2 be the a 2-d simplex. Under assumptions: 1 and 2  if < 1
↵ =

2C(d+1)  Algorithm 2 satisﬁes:

and ✏ = 3

qC[(dactive+2)2s2

22
Uact +(d?+2)s2

]

U?

35q ✏ min(pt 1pt)

⌧d 3 max(L 1)  

1

C E" CXl=1

l)# 

`(pt

min

p2+(12)2

`(p) 

Varopt
2pC

+

1
C

4 Experiments

In our experiments we use different classes of high-dimensional blackbox functions: RL blackbox
functions (where the input is a high-dimensional vector encoding a neural network policy ⇡ : S!A
mapping states s to actions a and the output is the cumulative reward obtained by an agent applying
this policy in a particular environment) and functions from the recently open-sourced Nevergrad
library [34]. In practice one can setup the hyperparameters used by Algorithm 2 as follows:  =
0 = 0.1. For each algorithm we used k = 5 seeds and obtained
0.01  C = 10 ↵ = 0.01  = 0.1  qt
curves are median-curves with inter-quartile ranges presented as shadowed regions.

Figure 2: Comparison of different blackbox optimization algorithms on OpenAI Gym tasks. All
curves are median-curves obtained from k = 5 seeds and with inter-quartile ranges presented as
shadowed regions. For Reacher we present only 3 curves since LM-MA-ES and TRPO did not learn.

4.1 RL blackbox functions

We used the following environments from the OpenAI Gym library: Swimmer-v2  HalfCheetah-
v2  Walker2d-v2  Reacher-v2  Pusher-v2 and Thrower-v2. In all experiments we used policies

7

encoded by neural network architectures of two hidden layers and with tanh nonlinearities  with
> 100 parameters. For gradient-based optimization we use Adam. For this class of blackbox
functions we compared ASEBO with other generic blackbox methods as well as those specializing
in optimizing RL blackbox functions F   namely: (1) CMA-ES variants; we compare against two
recently introduced algorithms designed for high-dimensional settings (we use the implementation
of VkD-CMA-ES in the pycma open-source implementation from https : //github.com/CMA-
ES/pycma)  and that of LM-MA-ES from [26])  (2) Augmented Random Search (ARS) [25]
(we use implementation released by the authors at http : //github.com/modestyachts/ARS)  (3)
Proximal Policy Optimization (PPO) [32] and Trust Region Policy Optimization (TRPO) [31] (we
use OpenAI baseline implementation [12]). The results for four environments are on Fig. 2.

Table 1: Median rewards obtained across k = 5 seeds for seven RL environments. For each
environment the top two performing algorithms are bolded  while the bottom two are shown in red.

Median reward after # timesteps

Environment Timesteps ASEBO
3821
HalfCheetah
358
Swimmer
Walker2d
9941
99949
Hopper
11
Reacher
46
Pusher
89
Thrower

5.107
107
5.107
107
105
105
105

ES
1530
36
347
626
10
-48
-96

-144
367
1
42

ARS VkD-CMA LM-MA TRPO PPO
1514
2420
52
348
1112
2377
1310
1091
-196
-12
-316
45
-90
-175

-1391
-1001
-796

-512
110
3011
1663
-112
-120
85

1632
297

18065
100199

-173
-467
-737

Sampling complexity is measured in the number of timesteps (environment transitions) used by the
algorithms. ASEBO is the only algorithm that performs consistently across all seven environments
(see: Table 1)  outperforming CMA-ES variants on all tasks aside from VkD-CMA-ES on Swimmer
and LM-MA-ES on Walker2d. For environments such as Reacher  Thrower and Pusher  these
methods perform poorly  drastically underperforming even Vanilla ES. On Fig. 3  we demonstrate
the common problem of state-of-the-art CMA-ES methods: if the number of samples n is not carefully
tuned  the algorithm does not learn. ASEBO does not have this problem since n is learned on-the-ﬂy.

Figure 3: Sensitivity analysis for CMA-ES variants on the HalfCheetah (HC) and Walker2d (WA)
tasks. In each setting  we run k = 5 seeds  solely changing the number of samples per iteration (or
population size) n.

Figure 4: Comparison of median-curves obtained from k = 5 seeds for different algorithms on
Nevergrad functions [34]. Inter-quartile ranges are presented as shadowed regions.

8

4.2 Nevergrad blackbox functions

We tested functions:
sphere  rastrigin  rosenbrock and lunacek (from the class of Bi-
Rastrigin/Lunacek’s No.02 functions). All tested functions are 1000-dimensional. The results
are presented on Fig. 4. ASEBO is the most reliable method across different functions.

5 Conclusion

We proposed a new algorithm for optimizing high-dimensional blackbox functions. ASEBO adjusts
on-the-ﬂy the strategy of choosing gradient sensing directions to the hardness of the problem at the
current stage of optimization and can be applied for both RL and non-RL problems. We provided
theoretical guarantees for our method and exhaustive empirical validation.

References
[1] S. Agrawal  N. R. Devanur  and L. Li. Contextual bandits with global constraints and objective.

CoRR  abs/1506.03374  2015.

[2] S. Ahmad and K. B. Thomas. Flight optimization system ( ﬂops ) hybrid electric aircraft design

capability. 2013.

[3] Y. Akimoto  A. Auger  and N. Hansen. Comparison-based natural gradient optimization in high

dimension. GECCO  2014.

[4] Y. Akimoto and N. Hansen. Projection-Based Restricted Covariance Matrix Adaptation for

High Dimension. GECCO  2016.

[5] R. A. Bridges  A. D. Gruber  C. Felder  M. E. Verma  and C. Hoff. Active manifolds: A

non-linear analogue to active subspaces. CoRR  abs/1904.13386  2019.

[6] G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang  and W. Zaremba.

OpenAI Gym  2016.

[7] K. Choromanski  M. Rowland  V. Sindhwani  R. E. Turner  and A. Weller. Structured evolution
with compact architectures for scalable policy optimization. In Proceedings of the 35th Interna-
tional Conference on Machine Learning  ICML 2018  Stockholmsm¨assan  Stockholm  Sweden 
July 10-15  2018  pages 969–977  2018.

[8] P. G. Constantine. Active Subspaces - Emerging Ideas for Dimension Reduction in Parameter

Studies  volume 2 of SIAM spotlights. SIAM  2015.

[9] P. G. Constantine  A. Eftekhari  and M. B. Wakin. Computing active subspaces efﬁciently
with gradient sketching. In 6th IEEE International Workshop on Computational Advances in
Multi-Sensor Adaptive Processing  CAMSAP 2015  Cancun  Mexico  December 13-16  2015 
pages 353–356  2015.

[10] P. G. Constantine  C. Kent  and T. Bui-Thanh. Accelerating markov chain monte carlo with

active subspaces. SIAM J. Scientiﬁc Computing  38(5)  2016.

[11] E. Conti  V. Madhavan  F. P. Such  J. Lehman  K. O. Stanley  and J. Clune. Improving exploration
in evolution strategies for deep reinforcement learning via a population of novelty-seeking
agents. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montr´eal 
Canada.  pages 5032–5043  2018.

[12] P. Dhariwal  C. Hesse  O. Klimov  A. Nichol  M. Plappert  A. Radford  J. Schulman  S. Sidor 
Y. Wu  and P. Zhokhov. Openai baselines. https://github.com/openai/baselines  2017.

[13] G. H. Golub. Some modiﬁed matrix eigenvalue problems. SIAM  15  1973.

[14] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. NeurIPS  2018.

9

[15] P. H¨am¨al¨ainen  A. Babadi  X. Ma  and J. Lehtinen. Ppo-cma: Proximal policy optimization

with covariance matrix adaptation. CoRR  abs/1810.02541  2018.

[16] N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distributions in evolution
strategies: The covariance matrix adaptation. In Evolutionary Computation  1996.  Proceedings
of IEEE International Conference on  pages 312–317. IEEE  1996.

[17] R. Houthooft  Y. Chen  P. Isola  B. Stadie  F. Wolski  O. Jonathan Ho  and P. Abbeel. Evolved

policy gradients. NeurIPS  2018.

[18] I. Jolliffe. Principal component analysis. Series: Springer Series in Statistics  XXIX  2002.

[19] J. Lehman  J. Chen  J. Clune  and K. O. Stanley. ES is more than just a traditional ﬁnite-
difference approximator. In Proceedings of the Genetic and Evolutionary Computation Confer-
ence  GECCO 2018  Kyoto  Japan  July 15-19  2018  pages 450–457  2018.

[20] C. Li  H. Farkhoor  R. Liu  and J. Yosinski. Measuring the intrinsic dimension of objective land-
scapes. In 6th International Conference on Learning Representations  ICLR 2018  Vancouver 
BC  Canada  April 30 - May 3  2018  Conference Track Proceedings  2018.

[21] G. Liu  L. Zhao  F. Yang  J. Bian  T. Qin  N. Yu  and T.-Y. Liu. Trust region evolution strategies.

In AAAI  2019.

[22] I. Loshchilov. A computationally efﬁcient limited memory cma-es for large scale optimization.

GECCO  2014.

[23] I. Loshchilov  T. Glasmachers  and H. Beyer. Large scale black-box optimization by limited-

memory matrix adaptation. IEEE Transactions on Evolutionary Computation  2019.

[24] N. Maheswaranathan  L. Metz  G. Tucker  and J. Sohl-Dickstein. Guided evolutionary strategies:

escaping the curse of dimensionality in random search. CoRR  abs/1806.10230  2018.

[25] H. Mania  A. Guy  and B. Recht. Simple random search provides a competitive approach to

reinforcement learning. CoRR  abs/1803.07055  2018.

[26] N. M¨uller and T. Glasmachers. Challenges in high-dimensional reinforcement learning with

evolution strategies. Parallel Problem Solving from Nature – PPSN XV  2018.

[27] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Found.

Comput. Math.  17(2):527–566  Apr. 2017.

[28] R. Ros and N. Hansen. A simple modiﬁcation in cma-es achieving linear time and space
complexity. In G. Rudolph  T. Jansen  N. Beume  S. Lucas  and C. Poloni  editors  Parallel
Problem Solving from Nature – PPSN X  pages 296–305  2008.

[29] M. Rowland  K. Choromanski  F. Chalus  A. Pacchiano  T. Sarlos  R. E. Turner  and A. Weller.

Geometrically coupled monte carlo sampling. In NeurIPS  2018.

[30] T. Salimans  J. Ho  X. Chen  S. Sidor  and I. Sutskever. Evolution strategies as a scalable

alternative to reinforcement learning. 2017.

[31] J. Schulman  S. Levine  P. Abbeel  M. I. Jordan  and P. Moritz. Trust region policy optimization.
In Proceedings of the 32nd International Conference on Machine Learning  ICML 2015  Lille 
France  6-11 July 2015  pages 1889–1897  2015.

[32] J. Schulman  F. Wolski  P. Dhariwal  A. Radford  and O. Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347  2017.

[33] F. P. Such  V. Madhavan  E. Conti  J. Lehman  K. O. Stanley  and J. Clune. Deep neuroevo-
lution: Genetic algorithms are a competitive alternative for training deep neural networks for
reinforcement learning. CoRR  abs/1712.06567  2017.

[34] O. Teytaud and J. Rapin. Nevergrad: An open source tool for derivative-free optimization.

https://code.fb.com/ai-research/nevergrad/  2018.

10

[35] K. Varelas  A. Auger  D. Brockhoff  N. Hansen  O. A. Elhara  Y. Semet  R. Kassab  and
F. Barbaresco. A comparative study of large-scale variants of cma-es. PPSN XV 2018 - 15th
International Conference on Parallel Problem Solving from Nature  2018.

[36] G. Zhang and J. Hinkle. Resnet-based isosurface learning for dimensionality reduction in

high-dimensional function approximation with limited data. CoRR  2019.

[37] Z. Zhou  X. Li  and R. N. Zare. Optimizing chemical reactions with deep reinforcement learning.

ACS Central Science  3(12):1337–1344  2017. PMID: 29296675.

11

,Krzysztof Choromanski
Aldo Pacchiano
Jack Parker-Holder
Yunhao Tang
Vikas Sindhwani