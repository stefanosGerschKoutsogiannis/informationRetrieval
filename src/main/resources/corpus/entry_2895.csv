2017,Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction,Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects  i.e. multi-scale feature generation and fusion. Different from previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture  we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore  to refine and robustly fuse the representations learned at different scales  the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.,Learning Deep Structured Multi-Scale Features using

Attention-Gated CRFs for Contour Prediction

Dan Xu1 Wanli Ouyang2 Xavier Alameda-Pineda3 Elisa Ricci4

Xiaogang Wang5 Nicu Sebe1

1The University of Trento  2The University of Sydney  3Perception Group  INRIA

4University of Perugia  5The Chinese University of Hong Kong

dan.xu@unitn.it  wanli.ouyang@sydney.edu.au  xavier.alameda-pineda@inria.fr

elisa.ricci@unipg.it  xgwang@ee.cuhk.edu.hk  niculae.sebe@unitn.it

Abstract

Recent works have shown that exploiting multi-scale representations deeply learned
via convolutional neural networks (CNN) is of tremendous importance for accurate
contour detection. This paper presents a novel approach for predicting contours
which advances the state of the art in two fundamental aspects  i.e. multi-scale
feature generation and fusion. Different from previous works directly consider-
ing multi-scale feature maps obtained from the inner layers of a primary CNN
architecture  we introduce a hierarchical deep model which produces more rich
and complementary representations. Furthermore  to reﬁne and robustly fuse the
representations learned at different scales  the novel Attention-Gated Conditional
Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly
available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the
latent AG-CRF model and of the overall hierarchical framework.

1

Introduction

Considered as one of the fundamental tasks in low-level vision  contour detection has been deeply
studied in the past decades. While early works mostly focused on low-level cues (e.g. colors  gradients 
textures) and hand-crafted features [3  25  22]  more recent methods beneﬁt from the representational
power of deep learning models [31  2  38  19  24]. The ability to effectively exploit multi-scale
feature representations is considered a crucial factor for achieving accurate predictions of contours
in both traditional [29] and CNN-based [38  19  24] approaches. Restricting the attention on deep
learning-based solutions  existing methods [38  24] typically derive multi-scale representations by
adopting standard CNN architectures and considering directly the feature maps associated to different
inner layers. These maps are highly complementary: while the features from the ﬁrst layers are
responsible for predicting ﬁne details  the ones from the higher layers are devoted to encode the
basic structure of the objects. Traditionally  concatenation and weighted averaging are very popular
strategies to combine multi-scale representations (see Fig. 1.a). While these strategies typically lead
to an increased detection accuracy with respect to single-scale models  they severly simplify the
complex relationship between multi-scale feature maps.
The motivational cornerstone of this study is the following research question: is it worth modeling
and exploiting complex relationships between multiple scales of a deep representation for contour
detection? In order to provide an answer and inspired by recent works exploiting graphical models
within deep learning architectures [5  39]  we introduce Attention-Gated Conditional Random Fields
(AG-CRFs)  which allow to learn robust feature map representations at each scale by exploiting the in-
formation available from other scales. This is achieved by incorporating an attention mechanism [27]
seamlessly integrated into the multi-scale learning process under the form of gates [26]. Intuitively 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the attention mechanism will further enhance the quality of the learned multi-scale representation 
thus improving the overall performance of the model.
We integrated the proposed AG-CRFs into a two-level hierarchical CNN model  deﬁning a novel
Attention-guided Multi-scale Hierarchical deepNet (AMH-Net) for contour detection. The hierarchi-
cal network is able to learn richer multi-scale features than conventional CNNs  the representational
power of which is further enhanced by the proposed AG-CRF model. We evaluate the effectiveness
of the overall model on two publicly available datasets for the contour detection task  i.e. BSDS500
[1] and NYU Depth v2 [33]. The results demonstrate that our approach is able to learn rich and
complementary features  thus outperforming state-of-the-art contour detection methods.
Related work. In the last few years several deep learning models have been proposed for detecting
contours [31  2  41  38  24  23]. Among these  some works explicitly focused on devising multi-scale
CNN models in order to boost performance. For instance  the Holistically-Nested Edge Detection
method [38] employed multiple side outputs derived from the inner layers of a primary CNN and
combine them for the ﬁnal prediction. Liu et al. [23] introduced a framework to learn rich deep
representations by concatenating features derived from all convolutional layers of VGG16. Bertasius
et al. [2] considered skip-layer CNNs to jointly combine feature maps from multiple layers. Maninis
et al. [24] proposed Convolutional Oriented Boundaries (COB)  where features from different layers
are fused to compute oriented contours and region hierarchies. However  these works combine the
multi-scale representations from different layers adopting concatenation and weighted averaging
schemes while not considering the dependency between the features. Furthermore  these works do
not focus on generating more rich and diverse representations at each CNN layer.
The combination of multi-scale representations has been also widely investigated for other pixel-level
prediction tasks  such as semantic segmentation [43]  visual saliency detection [21] and monocular
depth estimation [39]  and different deep architectures have been designed. For instance  to effectively
aggregate the multi-scale information  Yu et al. [43] introduced dilated convolutions. Yang et al. [42]
proposed DAG-CNNs where multi-scale feature outputs from different ReLU layers are combined
through element-wise addition operator. However  none of these works incorporate an attention
mechanism into a multi-scale structured feature learning framework.
Attention models have been successfully exploited in deep learning for various tasks such as image
classiﬁcation [37]  speech recognition [4] and image caption generation [40]. However  to our
knowledge  this work is the ﬁrst to introduce an attention model for estimating contours. Furthermore 
we are not aware of previous studies integrating the attention mechanism into a probabilistic (CRF)
framework to control the message passing between hidden variables. We model the attention as
gates [26]  which have been used in previous deep models such as restricted Boltzman machine
for unsupervised feature learning [35]  LSTM for sequence learning [12  6] and CNN for image
classiﬁcation [44]. However  none of these works explore the possibility of jointly learning multi-scale
deep representations and an attention model within a uniﬁed probabilistic graphical model.

2 Attention-Gated CRFs for Deep Structured Multi-Scale Feature Learning

2.1 Problem Deﬁnition and Notation

Given an input image I and a generic front-end CNN model with parameters Wc  we consider a set
of S multi-scale feature maps F = {fs}S
s=1. Being a generic framework  these feature maps can
be the output of S intermediate CNN layers or of another representation  thus s is a virtual scale.
The feature map at scale s  fs can be interpreted as a set of feature vectors  fs = {f i
i=1  where
N is the number of pixels. Opposite to previous works adopting simple concatenation or weighted
averaging schemes [16  38]  we propose to combine the multi-scale feature maps by learning a set
i=1 with a novel Attention-Gated CRF model sketched in Fig.1.
of latent feature maps hs = {hi
Intuitively  this allows a joint reﬁnement of the features by ﬂowing information between different
scales. Moreover  since the information from one scale may or may not be relevant for the pixels at
another scale  we utilise the concept of gate  previously introduced in the literature in the case of
graphical models [36]  in our CRF formulation. These gates are binary random hidden variables that
se sr ∈ {0  1} is the
permit or block the ﬂow of information between scales at every pixel. Formally  gi
i=1.
gate at pixel i of scale sr (receiver) from scale se (emitter)  and we also write gse sr = {gi
se sr}N
sr is updated taking (also) into account the
Precisely  when gi

= 1 then the hidden variable hi

s}N

s}N

se sr

2

Figure 1: An illustration of different schemes for multi-scale deep feature learning and fusion. (a)
the traditional approach (e.g. concatenation  weighted average)  (b) CRF implementing multi-scale
feature fusion (c) the proposed AG-CRF-based approach.
information from the se-th layer  i.e. hse. As shown in the following  the joint inference of the hidden
features and the gates leads to estimating the optimal features as well as the corresponding attention
model  hence the name Attention-Gated CRFs.

2.2 Attention-Gated CRFs

s=1 and  accessorily the attention gate variables G = {gse sr}S

Given the observed multi-scale feature maps F of image I  the objective is to estimate the hidden multi-
se sr=1.
scale representation H = {hs}S
To do that  we formalize the problem within a conditional random ﬁeld framework and write the Gibbs
distribution as P (H  G|I  Θ) = exp (−E(H  G  I  Θ)) /Z (I  Θ)  where Θ is the set of parameters
and E is the energy function. As usual  we exploit both unary and binary potentials to couple the
hidden variables between them and to the observations. Importantly  the proposed binary potential is
gated  and thus only active when the gate is open. More formally the general form1 of the energy
function writes:

E(H  G  I  Θ) =

φh(hi

s  f i
s)

+

gi
se sr ψh(hi

sr   hj
se

(cid:88)

i

(cid:88)
(cid:124)

s

(cid:123)(cid:122)

(cid:88)

i j

(cid:88)
(cid:124)

se sr

(cid:125)

(cid:123)(cid:122)

Unary potential

Gated pairwise potential

.

)

(cid:125)

(1)

The ﬁrst term of the energy function is a classical unary term that relates the hidden features to the
observed multi-scale CNN representations. The second term synthesizes the theoretical contribution
of the present study because it conditions the effect of the pair-wise potential ψh(hi
) upon
the gate hidden variable gi
se sr. Fig. 1c depicts the model formulated in Equ.(1). If we remove the
attention gate variables  it becomes a general multi-scale CRFs as shown in Fig. 1b.
Given that formulation  and as it is typically the case in conditional random ﬁelds  we exploit the
mean-ﬁeld approximation in order to derive a tractable inference procedure. Under this generic form 
the mean-ﬁeld inference procedure writes:

se   hj
sr

(cid:17)

(cid:16)
(cid:16)

(cid:88)

(cid:88)
(cid:110)(cid:88)

j

s(cid:48)(cid:54)=s
E
q(hj

s(cid:48) )

j

q(hi

s) ∝ exp

q(gi

s(cid:48) s) ∝ exp

φh(hi

s  f i

s) +

gi
s(cid:48) s

E
q(hi

s)

E
q(gi

s(cid:48)  s){gi

(cid:110)

ψh(hi

q(hj

s(cid:48) s}E
(cid:111)(cid:111)(cid:17)
s(cid:48) ){ψh(hi
s  hj

s(cid:48))

 

s  hj

s(cid:48))}

 

(2)

(3)

where Eq stands for the expectation with respect to the distribution q.
Before deriving these formulae for our precise choice of potentials  we remark that  since the
gate is a binary variable  the expectation of its value is the same as q(gi
s(cid:48) s = 1). By deﬁning:
Mi

  the expected value of the gate writes:

(cid:110)(cid:80)

s(cid:48) s = E

(cid:111)(cid:111)

E
q(hj

ψh(hi

s  hj

(cid:110)

s(cid:48))

q(hi

s(cid:48) )

s)

j

s s(cid:48) = E
αi

q(gi

s(cid:48) s){gi

s(cid:48) s} =

q(gi

s(cid:48) s = 1)
s(cid:48) s = 0) + q(gi

q(gi

s(cid:48) s = 1)

= σ(cid:0)

(cid:1)  

−Mi

s(cid:48) s

(4)

where σ() denotes the sigmoid function. This ﬁnding is specially relevant in the framework of CNN
since many of the attention models are typically obtained after applying the sigmoid function to the

1One could certainly include a unary potential for the gate variables as well. However this would imply that
there is a way to set/learn the a priori distribution of opening/closing a gate. In practice we did not observe any
notable difference between using or skipping the unary potential on g.

3

fs+1fs+1(a) Multi-Scale Neural Network(b) Multi-Scale CRFsfsfs1fs1fsIIhs1hs1hshshs+1hs+1fs+1······(c) Attention-Gated CRFsfs1fsIgs1 sgs s+1hs1hshs+1············s(cid:48) s depends on
features derived from a feed-forward network. Importantly  since the quantity Mi
the expected values of the hidden features hi
s  the AG-CRF framework extends the unidirectional
connection from the features to the attention model  to a bidirectional connection in which the
expected value of the gate allows to reﬁne the distribution of the hidden features as well.

2.3 AG-CRF Inference

In order to construct an operative model we need to deﬁne the unary and gated potentials φh and ψh.
In our case  the unary potential corresponds to an isotropic Gaussian:

φh(hi

s  f i

s) = −

ai
s
2 (cid:107)hi

s − f i

s(cid:107)2 

(5)

s > 0 is a weighting factor.

where ai
The gated binary potential is speciﬁcally designed for a two-fold objective. On the one hand  we
would like to learn and further exploit the relationships between hidden vectors at the same  as well
as at different scales. On the other hand  we would like to exploit previous knowledge on attention
models and include linear terms in the potential. Indeed  this would implicitly shape the gate variable
to include a linear operator on the features. Therefore  we chose a bilinear potential:

s  hj

s(cid:48)) = ˜hi

ψh(hi

sKi j
s s(cid:48)

s = (hi(cid:62)s   1)(cid:62) and Ki j

(6)
s s(cid:48) ∈ R(Cs+1)×(Cs(cid:48) +1) being Cs the size  i.e. the number of channels 
where ˜hi
s s(cid:48); lj i(cid:62)s(cid:48) s   1)  then Li j
of the representation at scale s. If we write this matrix as Ki j
s s(cid:48)
exploits the relationships between hidden variables  while li j
s(cid:48) s implement the classically used
linear relationships of the attention models. In order words  ψh models the pair-wise relationships
between features with the upper-left block of the matrix. Furthemore  ψh takes into account the linear
relationships by completing the hidden vectors with the unity. In all  the energy function writes:

s s(cid:48) = (Li j

s s(cid:48) and lj i

s s(cid:48)  li j

˜hj
s(cid:48) 

(cid:88)

(cid:88)

s

i

(cid:88)

se sr

(cid:88)
(cid:88)

i j

ai
s
2 (cid:107)hi

s − f i
s(cid:107)2 +
(cid:88)

gi
se sr

˜hi
sr Ki j

sr se

˜hj
se .

(cid:17)

(Li j
s s(cid:48)

¯hj
s(cid:48) + li j

s s(cid:48))

 

E(H  G  I  Θ) = −

(cid:16)

ai
s
2

(7)

(8)

(cid:17)

Under these potentials  we can consequently update the mean-ﬁeld inference equations to:

q(hi

s) ∝ exp

s(cid:107) − 2hi(cid:62)s f i
s(cid:48) is the expected a posteriori value of hj
s(cid:48).

((cid:107)hi

−

s) +

where ¯hj
(cid:17)
The previous expression implies that the a posteriori distribution for hi
vector of the Gaussian and the function M write:
¯hi
s =

(cid:16)¯hi

(cid:88)

(cid:88)

(cid:88)

ai
sf i

(cid:16)

s+

¯hj
s(cid:48)+li j

(Li j
s s(cid:48)

s(cid:48) s =

sLi j
s s(cid:48)

s s(cid:48))

αi

s s(cid:48)

s s(cid:48)hi(cid:62)s
αi

s(cid:48)(cid:54)=s

j

Mi

j

1
ai
s

s(cid:48)(cid:54)=s

j

s is a Gaussian. The mean

s(cid:48) + ¯hi(cid:62)s li j
¯hj

s s(cid:48) + ¯hj(cid:62)s(cid:48) lj i
s(cid:48) s

which concludes the inference procedure. Furthermore  the proposed framework can be simpliﬁed to
obtain the traditional attention models. In most of the previous studies  the attention variables are
computed directly from the multi-scale features instead of computing them from the hidden variables.
Indeed  since many of these studies do not propose a probabilistic formulation  there are no hidden
variables and the attention is computed sequentially through the scales. We can emulate the same
behavior within the AG-CRF framework by modifying the gated potential as follows:

˜ψh(hi

s  f j

s  hj

s(cid:48)  f i

s(cid:48)) = hi

(9)
This means that we keep the pair-wise relationships between hidden variables (as in any CRF) and let
the attention model be generated by a linear combination of the observed features from the CNN  as it
is traditionally done. The changes in the inference procedure are straightforward and reported in the
supplementary material due to space constraints. We refer to this model as partially-latent AG-CRFs
(PLAG-CRFs)  whereas the more general one is denoted as fully-latent AG-CRFs (FLAG-CRFs).

s s(cid:48) + f j(cid:62)s(cid:48) lj i
s(cid:48) s.

s(cid:48) + f i(cid:62)s li j

s s(cid:48)hj

sLi j

2.4

Implementation with neural network for joint learning

In order to infer the hidden variables and learn the parameters of the AG-CRFs together with those
of the front-end CNN  we implement the AG-CRFs updates in neural network with several steps:

4

Figure 2: An overview of the proposed AMH-Net for contour detection.

(i) message passing from the se-th scale to the current sr-th scale is performed with hse→sr ←
Lse→sr ⊗ hse  where ⊗ denotes the convolutional operation and Lse→sr denotes the corresponding
convolution kernel  (ii) attention map estimation q(gse sr = 1) ← σ(hsr (cid:12) (Lse→sr ⊗ hse ) +
lse→sr ⊗ hse + lsr→se ⊗ hsr )  where Lse→sr  lse→sr and lsr→se are convolution kernels and (cid:12)
represents element-wise product operation  and (iii) attention-gated message passing from other scales
and adding unary term: ¯hsr = fsr ⊕ asr
(q(gse sr = 1) (cid:12) hse→sr )  where asr encodes the
sr for weighting the message and can be implemented as a 1 × 1 convolution. The
effect of the ai
symbol ⊕ denotes element-wise addition. In order to simplify the overall inference procedure  and
because the magnitude of the linear term of ψh is in practice negligible compared to the quadratic
term  we discard the message associated to the linear term. When the inference is complete  the ﬁnal
estimate is obtained by convolving all the scales.

(cid:80)

se(cid:54)=sr

3 Exploiting AG-CRFs with a Multi-scale Hierarchical Network

  f C

l

l and f M

are further aligned to the dimensions of the feature map f D

AMH-Net Architecture. The proposed Attention-guided Multi-scale Hierarchical Network (AMH-
Net)  as sketched in Figure 2  consists of a multi-scale hierarchical network (MH-Net) together with
the AG-CRF model described above. The MH-Net is constructed from a front-end CNN architecture
such as the widely used AlexNet [20]  VGG [34] and ResNet [17]. One prominent feature of MH-Net
is its ability to generate richer multi-scale representations. In order to do that  we perform distinct
non-linear mappings (deconvolution D  convolution C and max-pooling M) upon fl  the CNN
feature representation from an intermediate layer l of the front-end CNN. This leads to a three-way
representation: f D
. Remarkably  while D upsamples the feature map  C maintains its
l
original size and M reduces it  and different kernel size is utilized for them to have different receptive
ﬁelds  then naturally obtaining complementary inter- and multi-scale representations. The f C
l and
l by the deconvolutional operation.
f M
l
The hierarchy is implemented in two levels. The ﬁrst level uses an AG-CRF model to fuse the three
representations of each layer l  thus reﬁning the CNN features within the same scale. The second
level of the hierarchy uses an AG-CRF model to fuse the information coming from multiple CNN
layers. The proposed hierarchical multi-scale structure is general purpose and able to involve an
arbitrary number of layers and of diverse intra-layer representations.
End-to-End Network Optimization. The parameters of the model consist of the front-end CNN
parameters  Wc  the parameters to produce the richer decomposition from each layer l  Wl  the
parameters of the AG-CRFs of the ﬁrst level of the hierarchy  {WI
l=1  and the parameters of
l}L
the AG-CRFs of the second level of the hierarchy  WII. L is the number of intermediate layers
used from the front-end CNN. In order to jointly optimize all these parameters we adopt deep
supervision [38] and we add an optimization loss associated to each AG-CRF module. In addition 
since the contour detection problem is highly unbalanced  i.e. contour pixels are signiﬁcantly less than
non-contour pixels  we employ the modiﬁed cross-entropy loss function of [38]. Given a training data

5

AG-CRFAG-CRFAG-CRFAG-CRFCDMDDCMDDDCMDDDCCCCCCDLCDLFront-End CNNC..............................DDDDLDLCConvolutionMMax-poolingDDeconvolutionLLossHIERARCHY 1HIERARCHY 2flfClfMlfDlfM0lfC0l(cid:88)
set D = {(Ip  Ep)}P
β

(cid:96)(cid:0)W(cid:1) =

(cid:88)

p=1 consisting of P RGB-contour groundtruth pairs  the loss function (cid:96) writes:
(10)

p = 1|Ip; W(cid:1) +(cid:0)1 − β(cid:1) (cid:88)

p = 0|Ip; W(cid:1) 

log P(cid:0)ek

log P(cid:0)ek

p

p

p∈E+
ek
p | + |E−p |)  E+

p∈E−p
ek

p |/(|E+

where β = |E+
p is the set of contour pixels of image p and W is the set of
all parameters. The optimization is performed via the back-propagation algorithm with standard
stochastic gradient descent.
AMH-Net for contour detection. After training of the whole AMH-Net  the optimized network
parameters W are used for the contour detection task. Given a new test image I  the L + 1 classiﬁers
produce a set of contour prediction maps { ˆEl}L+1
l=1 = AMH-Net(I; W). The ˆEl are obtained
(cid:80)
from the AG-CRFs with elementary operations as detailed in the supplementary material. We
inspire from [38] to fuse the multiple scale predictions thus obtaining an average prediction ˆE =

ˆEl/(L + 1).

l

4 Experiments

4.1 Experimental Setup

Datasets. To evaluate the proposed approach we employ two different benchmarks: the BSDS500
and the NYUDv2 datasets. The BSDS500 dataset is an extended dataset based on BSDS300 [1]. It
consists of 200 training  100 validation and 200 testing images. The groundtruth pixel-level labels for
each sample are derived considering multiple annotators. Following [38  41]  we use all the training
and validation images for learning the proposed model and perform data augmentation as described
in [38]. The NYUDv2 [33] contains 1449 RGB-D images and it is split into three subsets  comprising
381 training  414 validation and 654 testing images. Following [38] in our experiments we employ
images at full resolution (i.e. 560 × 425 pixels) both in the training and in the testing phases.
Evaluation Metrics. During the test phase standard non-maximum suppression (NMS) [9] is ﬁrst
applied to produce thinned contour maps. We then evaluate the detection performance of our approach
according to different metrics  including the F-measure at Optimal Dataset Scale (ODS) and Optimal
Image Scale (OIS) and the Average Precision (AP). The maximum tolerance allowed for correct
matches of edge predictions to the ground truth is set to 0.0075 for the BSDS500 dataset  and to .011
for the NYUDv2 dataset as in previous works [9  14  38].
Implementation Details. The proposed AMH-Net is implemented under the deep learning frame-
work Caffe [18]. The implementation code is available on Github2. The training and testing phase
are carried out on an Nvidia Titan X GPU with 12GB memory. The ResNet50 network pretrained on
ImageNet [8] is used to initialize the front-end CNN of AMH-Net. Due to memory constraints  our
implementation only considers three scales  i.e. we generate multi-scale features from three different
layers of the front-end CNN (i.e. res3d  res4f  res5c). In our CRF model we consider dependencies
between all scales. Within the AG-CRFs  the kernel size for all convolutional operations is set to
3 × 3 with stride 1 and padding 1. To simplify the model optimization  the parameters ai
sr are set
as 0.1 for all scales during training. We choose this value as it corresponds to the best performance
after cross-validation in the range [0  1]. The initial learning rate is set to 1e-7 in all our experiments 
and decreases 10 times after every 10k iterations. The total number of iterations for BSDS500 and
NYUD v2 is 40k and 30k  respectively. The momentum and weight decay parameters are set to 0.9
and 0.0002  as in [38]. As the training images have different resolution  we need to set the batch size
to 1  and for the sake of smooth convergence we updated the parameters only every 10 iterations.

4.2 Experimental Results
In this section  we present the results of our evaluation  comparing our approach with several state
of the art methods. We further conduct an in-depth analysis of our method  to show the impact of
different components on the detection performance.
Comparison with state of the art methods. We ﬁrst consider the BSDS500 dataset and compare
the performance of our approach with several traditional contour detection methods  including
Felz-Hut [11]  MeanShift [7]  Normalized Cuts [32]  ISCRA [30]  gPb-ucm [1]  SketchTokens [22] 

2https://github.com/danxuhk/AttentionGatedMulti-ScaleFeatureLearning

6

Figure 3: Qualitative results on the BSDS500 (left) and the NYUDv2 (right) test samples. The 2nd
(4th) and 3rd (6th) columns are the ground-truth and estimated contour maps respectively.

Table 1: BSDS500 dataset: quantitative results.

Table 2: NYUDv2 dataset: quantitative results.

Method
Human
Felz-Hutt[11]
Mean Shift[7]
Normalized Cuts[32]
ISCRA[30]
gPb-ucm[1]
Sketch Tokens[22]
MCG[28]
DeepEdge[2]
DeepContour[31]
LEP[46]
HED[38]
CEDN[41]
COB [24]
RCF [23] (not comp.)
AMH-Net (fusion)

ODS OIS
.800
.800
.640
.610
.680
.640
.641
.674
.752
.724
.760
.726
.746
.727
.779
.747
.753
.772
.773
.756
.793
.757
.808
.788
.804
.788
.793
.820
.830
.811
.798
.829

AP
-

.560
.560
.447
.783
.727
.780
.759
.807
.797
.828
.840
.834
.859

–
.869

Method

ODS OIS

gPb-ucm [1]
OEF [15]
Silberman et al. [33]
SemiContour [45]
SE [10]
gPb+NG [13]
SE+NG+ [14]

HED (RGB) [38]
HED (HHA) [38]
HED (RGB + HHA) [38]
RCF (RGB) + HHA) [23]

AMH-Net (RGB)
AMH-Net (HHA)
AMH-Net (RGB+HHA)

.632
.651
.658
.680
.685
.687
.710

.720
.682
.746
.757

.744
.716
.771

.661
.667
.661
.700
.699
.716
.723

.734
.695
.761
.771

.758
.729
.786

AP

.562

–
–

.690
.679
.629
.738

.734
.702
.786

–

.765
.734
.802

MCG [28]  LEP [46]  and more recent CNN-based methods  including DeepEdge [2]  DeepCon-
tour [31]  HED [38]  CEDN [41]  COB [24]. We also report results of the RCF method [23]  although
they are not comparable because in [23] an extra dataset (Pascal Context) was used during RCF
training to improve the results on BSDS500. In this series of experiments we consider AMH-Net with
FLAG-CRFs. The results of this comparison are shown in Table 1 and Fig. 4a. AMH-Net obtains
an F-measure (ODS) of 0.798  thus outperforms all previous methods. The improvement over the
second and third best approaches  i.e. COB and HED  is 0.5% and 1.0%  respectively  which is not
trivial to achieve on this challenging dataset. Furthermore  when considering the OIS and AP metrics 
our approach is also better  with a clear performance gap.
To perform experiments on NYUDv2  following previous works [38] we consider three different
types of input representations  i.e. RGB  HHA [14] and RGB-HHA data. The results corresponding
to the use of both RGB and HHA data (i.e. RGB+HHA) are obtained by performing a weighted
average of the estimates obtained from two AMH-Net models trained separately on RGB and HHA
representations. As baselines we consider gPb-ucm [1]  OEF [15]  the method in [33]  SemiCon-
tour [45]  SE [10]  gPb+NG [13]  SE+NG+ [14]  HED [38] and RCF [23]. In this case the results
are comparable to the RCF [23] since the experimental protocol is exactly the same. All of them
are reported in Table 2 and Fig. 4b. Again  our approach outperforms all previous methods. In
particular  the increased performance with respect to HED [38] and RCF [23] conﬁrms the beneﬁt of
the proposed multi-scale feature learning and fusion scheme. Examples of qualitative results on the
BSDS500 and the NYUDv2 datasets are shown in Fig. 3.
Ablation Study. To further demonstrate the effectiveness of the proposed model and analyze the
impact of the different components of AMH-Net on the countour detection task  we conduct an

7

(a) BSDS500

(b) NYUDv2

Figure 4: Precision-Recall Curves on the BSDS500 and NYUDv2 test sets.

Table 3: Performance analysis on NYUDv2 RGB data.

ablation study considering the NYUDv2 dataset (RGB data). We tested the following models:
(i) AMH-Net (baseline)  which removes the ﬁrst-level hierarchy and directly concatenates the
feature maps for prediction  (ii) AMH-Net (w/o AG-CRFs)  which employs the proposed multi-scale
hierarchical structure but discards the AG-CRFs  (iii) AMH-Net (w/ CRFs)  obtained by replacing
our AG-CRFs with a multi-scale CRF model without attention gating  (iv) AMH-Net (w/o deep
supervision) obtained removing intermediate loss functions in AMH-Net and (v) AMH-Net with the
proposed two versions of the AG-CRFs model  i.e. PLAG-CRFs and FLAG-CRFs. The results of
our comparison are shown in Table 3  where we also consider as reference traditional multi-scale
deep learning models employing multi-scale representations  i.e. Hypercolumn [16] and HED [38].
These results clearly show the advantages of
our contributions. The ODS F-measure of
AMH-Net (w/o AG-CRFs) is 1.1% higher
than AMH-Net (baseline)  clearly demon-
strating the effectiveness of the proposed hi-
erarchical network and conﬁrming our intu-
ition that exploiting more richer and diverse
multi-scale representations is beneﬁcial. Ta-
ble 3 also shows that our AG-CRFs plays
a fundamental role for accurate detection 
as AMH-Net (w/ FLAG-CRFs) leads to an
improvement of 1.9% over AMH-Net (w/o
AG-CRFs) in terms of OSD. Finally  AMH-Net (w/ FLAG-CRFs) is 1.2% and 1.5% better than
AMH-Net (w/ CRFs) in ODS and AP metrics respectively  conﬁrming the effectiveness of embedding
an attention mechanism in the multi-scale CRF model. AMH-Net (w/o deep supervision) decreases
the overall performance of our method by 1.9% in ODS  showing the crucial importance of deep su-
pervision for better optimization of the whole AMH-Net. Comparing the performance of the proposed
two versions of the AG-CRF model  i.e. PLAG-CRFs and FLAG-CRFs  we can see that AMH-Net
(FLAG-CRFs) slightly outperforms AMH-Net (PLAG-CRFs) in both ODS and OIS  while bringing a
signiﬁcant improvement (around 2%) in AP. Finally  considering HED [38] and Hypercolumn [16] 
it is clear that our AMH-Net (FLAG-CRFs) is signiﬁcantly better than these methods. Importantly 
our approach utilizes only three scales while for HED [38] and Hypercolumn [16] we consider ﬁve
scales. We believe that our accuracy could be further boosted by involving more scales.
5 Conclusions

Method
Hypercolumn [16]
HED [38]
AMH-Net (baseline)
AMH-Net (w/o AG-CRFs)
AMH-Net (w/ CRFs)
AMH-Net (w/o deep supervision)
AMH-Net (w/ PLAG-CRFs)
AMH-Net (w/ FLAG-CRFs)

ODS OIS
.729
.718
.720
.734
.720
.711
.732
.722
.742
.732
.738
.725
.737
.749
.758
.744

AP
.731
.734
.724
.739
.750
.747
.746
.765

We presented a novel multi-scale convolutional neural network for contour detection. The proposed
model introduces two main components  i.e. a hierarchical architecture for generating more rich
and complementary multi-scale feature representations  and an Attention-Gated CRF model for
robust feature reﬁnement and fusion. The effectiveness of our approach is demonstrated through
extensive experiments on two public available datasets and state of the art detection performance is

8

Recall00.10.20.30.40.50.60.70.80.91Precision00.10.20.30.40.50.60.70.80.91[F=0.800] Human[F=0.798] AMH-Net[F=0.793] COB[F=0.788] CEDN[F=0.788] HED[F=0.757] LEP[F=0.756] DeepContour[F=0.753] DeepEdge[F=0.747] MCG[F=0.727] SketchTokens[F=0.726] UCM[F=0.724] ISCRA[F=0.641] Normalized Cuts[F=0.640] MeanShift[F=0.610] Felz-HutRecall00.10.20.30.40.50.60.70.80.91Precision00.10.20.30.40.50.60.70.80.91[F=0.800] Human[F=0.771] AMH-Net[F=0.746] HED[F=0.706] SE+NG+[F=0.695] SE[F=0.685] gPb+NG[F=0.680] SemiContour[F=0.658] Silberman[F=0.651] OEF[F=0.632] gPb-ucmachieved. The proposed approach addresses a general problem  i.e. how to generate rich multi-scale
representations and optimally fuse them. Therefore  we believe it may be also useful for other
pixel-level tasks.

References
[1] P. Arbeláez  M. Maire  C. Fowlkes  and J. Malik. Contour detection and hierarchical image segmentation.

TPAMI  33(5)  2011.

[2] G. Bertasius  J. Shi  and L. Torresani. Deepedge: A multi-scale bifurcated deep network for top-down

contour detection. In CVPR  2015.

[3] J. Canny. A computational approach to edge detection. TPAMI  (6):679–698  1986.
[4] J. K. Chorowski  D. Bahdanau  D. Serdyuk  K. Cho  and Y. Bengio. Attention-based models for speech

recognition. In NIPS  2015.

[5] X. Chu  W. Ouyang  X. Wang  et al. Crf-cnn: Modeling structured information in human pose estimation.

In NIPS  2016.

[6] J. Chung  C. Gulcehre  K. Cho  and Y. Bengio. Empirical evaluation of gated recurrent neural networks on

sequence modeling. arXiv preprint arXiv:1412.3555  2014.

[7] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. TPAMI  24(5) 

2002.

[8] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image

database. In CVPR  2009.

[9] P. Dollár and C. L. Zitnick. Structured forests for fast edge detection. In ICCV  2013.
[10] P. Dollár and C. L. Zitnick. Fast edge detection using structured forests. TPAMI  37(8):1558–1570  2015.
[11] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient graph-based image segmentation. IJCV  59(2)  2004.
[12] F. A. Gers  N. N. Schraudolph  and J. Schmidhuber. Learning precise timing with lstm recurrent networks.

Journal of machine learning research  3(Aug):115–143  2002.

[13] S. Gupta  P. Arbelaez  and J. Malik. Perceptual organization and recognition of indoor scenes from rgb-d

images. In CVPR  2013.

[14] S. Gupta  R. Girshick  P. Arbeláez  and J. Malik. Learning rich features from rgb-d images for object

detection and segmentation. In ECCV  2014.

[15] S. Hallman and C. C. Fowlkes. Oriented edge forests for boundary detection. In CVPR  2015.
[16] B. Hariharan  P. Arbeláez  R. Girshick  and J. Malik. Hypercolumns for object segmentation and ﬁne-

grained localization. In CVPR  2015.

[17] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. arXiv preprint

arXiv:1512.03385  2015.

[18] Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and T. Darrell. Caffe:

Convolutional architecture for fast feature embedding. In ACM MM  2014.

[19] I. Kokkinos. Pushing the boundaries of boundary detection using deep learning. arXiv preprint

arXiv:1511.07386  2015.

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  2012.

[21] G. Li and Y. Yu. Visual saliency based on multiscale deep features. In CVPR  2015.
[22] J. J. Lim  C. L. Zitnick  and P. Dollár. Sketch tokens: A learned mid-level representation for contour and

object detection. In CVPR  2013.

[23] Y. Liu  M.-M. Cheng  X. Hu  K. Wang  and X. Bai. Richer convolutional features for edge detection. arXiv

preprint arXiv:1612.02103  2016.

[24] K.-K. Maninis  J. Pont-Tuset  P. Arbeláez  and L. Van Gool. Convolutional oriented boundaries. In ECCV 

2016.

[25] D. R. Martin  C. C. Fowlkes  and J. Malik. Learning to detect natural image boundaries using local

brightness  color  and texture cues. TPAMI  26(5):530–549  2004.

[26] T. Minka and J. Winn. Gates. In NIPS  2009.
[27] V. Mnih  N. Heess  A. Graves  et al. Recurrent models of visual attention. In NIPS  pages 2204–2212 

2014.

[28] J. Pont-Tuset  P. Arbelaez  J. Barron  F. Marques  and J. Malik. Multiscale combinatorial grouping for

image segmentation and object proposal generation. TPAMI  2016.

[29] X. Ren. Multi-scale improves boundary detection in natural images. In ECCV  2008.
[30] Z. Ren and G. Shakhnarovich. Image segmentation by cascaded region agglomeration. In CVPR  2013.
[31] W. Shen  X. Wang  Y. Wang  X. Bai  and Z. Zhang. Deepcontour: A deep convolutional feature learned by

positive-sharing loss for contour detection. In CVPR  2015.

[32] J. Shi and J. Malik. Normalized cuts and image segmentation. TPAMI  22(8)  2000.

9

[33] N. Silberman  D. Hoiem  P. Kohli  and R. Fergus. Indoor segmentation and support inference from rgbd

images. In ECCV  2012.

[34] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

[35] Y. Tang. Gated boltzmann machine for recognition under occlusion. In NIPS Workshop on Transfer

Learning by Learning Rich Generative Models  2010.

[36] J. Winn. Causality with gates. In AISTATS  2012.
[37] T. Xiao  Y. Xu  K. Yang  J. Zhang  Y. Peng  and Z. Zhang. The application of two-level attention models in

deep convolutional neural network for ﬁne-grained image classiﬁcation. In CVPR  2015.

[38] S. Xie and Z. Tu. Holistically-nested edge detection. In ICCV  2015.
[39] D. Xu  E. Ricci  W. Ouyang  X. Wang  and N. Sebe. Multi-scale continuous crfs as sequential deep

networks for monocular depth estimation. CVPR  2017.

[40] K. Xu  J. Ba  R. Kiros  K. Cho  A. C. Courville  R. Salakhutdinov  R. S. Zemel  and Y. Bengio. Show 

attend and tell: Neural image caption generation with visual attention. In ICML  2015.

[41] J. Yang  B. Price  S. Cohen  H. Lee  and M.-H. Yang. Object contour detection with a fully convolutional

encoder-decoder network. 2016.

[42] S. Yang and D. Ramanan. Multi-scale recognition with dag-cnns. In ICCV  2015.
[43] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions.

arXiv:1511.07122  2015.

arXiv preprint

[44] X. Zeng  W. Ouyang  J. Yan  H. Li  T. Xiao  K. Wang  Y. Liu  Y. Zhou  B. Yang  Z. Wang  et al. Crafting

gbd-net for object detection. arXiv preprint arXiv:1610.02579  2016.

[45] Z. Zhang  F. Xing  X. Shi  and L. Yang. Semicontour: A semi-supervised learning approach for contour

detection. In CVPR  2016.

[46] Q. Zhao. Segmenting natural images with the least effort as humans. In BMVC  2015.

10

,Keerthiram Murugesan
Hanxiao Liu
Jaime Carbonell
Yiming Yang
Dan Xu
Wanli Ouyang
Xavier Alameda-Pineda
Xiaogang Wang
Nicu Sebe