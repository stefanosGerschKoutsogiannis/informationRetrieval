2015,Large-Scale Bayesian Multi-Label Learning via Topic-Based Label Embeddings,We present a scalable Bayesian multi-label learning model based on learning low-dimensional label embeddings. Our model assumes that each label vector is generated as a weighted combination of a set of topics (each topic being a distribution over labels)  where the combination weights (i.e.  the embeddings) for each label vector are conditioned on the observed feature vector. This construction  coupled with a Bernoulli-Poisson link function for each label of the binary label vector  leads to a model with a computational cost that scales in the number of positive labels in the label matrix. This makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse. Using a data-augmentation strategy leads to full local conjugacy in our model  facilitating simple and very efficient Gibbs sampling  as well as an Expectation Maximization algorithm for inference. Also  predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form. We report results on several benchmark data sets  comparing our model with various state-of-the art methods.,Large-Scale Bayesian Multi-Label Learning via

Topic-Based Label Embeddings

Piyush Rai†∗  Changwei Hu∗  Ricardo Henao∗  Lawrence Carin∗

piyush@cse.iitk.ac.in  {ch237 r.henao lcarin}@duke.edu

†CSE Dept  IIT Kanpur

∗ECE Dept  Duke University

Abstract

We present a scalable Bayesian multi-label learning model based on learning low-
dimensional label embeddings. Our model assumes that each label vector is gen-
erated as a weighted combination of a set of topics (each topic being a distribution
over labels)  where the combination weights (i.e.  the embeddings) for each label
vector are conditioned on the observed feature vector. This construction  coupled
with a Bernoulli-Poisson link function for each label of the binary label vector 
leads to a model with a computational cost that scales in the number of posi-
tive labels in the label matrix. This makes the model particularly appealing for
real-world multi-label learning problems where the label matrix is usually very
massive but highly sparse. Using a data-augmentation strategy leads to full local
conjugacy in our model  facilitating simple and very efﬁcient Gibbs sampling  as
well as an Expectation Maximization algorithm for inference. Also  predicting
the label vector at test time does not require doing an inference for the label em-
beddings and can be done in closed form. We report results on several benchmark
data sets  comparing our model with various state-of-the art methods.

Introduction

1
Multi-label learning refers to the problem setting in which the goal is to assign to an object (e.g.  a
video  image  or webpage) a subset of labels (e.g.  tags) from a (possibly very large) set of labels.
The label assignments of each example can be represented using a binary label vector  indicating the
presence/absence of each label. Despite a signiﬁcant amount of prior work  multi-label learning [7 
6] continues to be an active area of research  with a recent surge of interest [1  25  18  13  10] in
designing scalable multi-label learning methods to address the challenges posed by problems such as
image/webpage annotation [18]  computational advertising [1  18]  medical coding [24]  etc.  where
not only the number of examples and data dimensionality are large but the number of labels can also
be massive (several thousands to even millions).
Often  in multi-label learning problems  many of the labels tend to be correlated with each other.
To leverage the label correlations and also handle the possibly massive number of labels  a common
approach is to reduce the dimensionality of the label space  e.g.  by projecting the label vectors to
a subspace [10  25  21]  learning a prediction model in that space  and then projecting back to the
original space. However  as the label space dimensionality increases and/or the sparsity in the label
matrix becomes more pronounced (i.e.  very few ones)  and/or if the label matrix is only partially
observed  such methods tend to suffer [25] and can also become computationally prohibitive.
To address these issues  we present a scalable  fully Bayesian framework for multi-label learning.
Our framework is similar in spirit to the label embedding methods based on reducing the label space
dimensionality [10  21  25]. However  our framework offers the following key advantages: (1)
computational cost of training our model scales in the number of ones in the label matrix  which
makes our framework easily scale in cases where the label matrix is massive but sparse; (2) our
likelihood model for the binary labels  based on a Bernoulli-Poisson link  more realistically models
the extreme sparsity of the label matrix as compared to the commonly employed logistic/probit link;
and (3) our model is more interpretable - embeddings naturally correspond to topics where each
topic is a distribution over labels. Moreover  at test time  unlike other Bayesian methods [10]  we do
not need to infer the label embeddings of the test example  thereby leading to faster predictions.

1

In addition to the modeling ﬂexibility that leads to a robust  interpretrable  and scalable model  our
framework enjoys full local conjugacy  which allows us to develop simple Gibbs sampling  as well
as an Expectation Maximization (EM) algorithm for the proposed model  both of which are simple
to implement in practice (and amenable for parallelization).

2 The Model

We assume that the training data are given in the form of N examples represented by a feature matrix
X ∈ RD×N   along with their labels in a (possibly incomplete) label matrix Y ∈ {0  1}L×N . The
goal is to learn a model that can predict the label vector y∗ ∈ {0  1}L for a test example x∗ ∈ RD.
We model the binary label vector yn of the nth example by thresholding a count-valued vector mn
(1)
which  for each individual binary label yln ∈ yn  l = 1  . . .   L  can also be written as yln =
1(mln ≥ 1). In Eq. (1)  mn = [m1n  . . .   mLn] ∈ ZL denotes a latent count vector of size L and
is assumed drawn from a Poisson

yn = 1(mn ≥ 1)

(2)
Eq (2) denotes drawing each component of mn independently  from a Poisson distribution  with rate
equal to the corresponding component of λn ∈ RL

+  which is deﬁned as

mn ∼ Poisson(λn)

(3)
+ (typically K (cid:28) L). Note that the K columns of V can be thought
Here V ∈ RL×K
of as atoms of a label dictionary (or “topics” over labels) and un can be thought of as the atom
weights or embedding of the label vector yn (or “topic proportions”  i.e.  how active each of the K
topics is for example n). Also note that Eq. (1)-(3) can be combined as

and un ∈ RK

λn = Vun

+

+

yn = f (λn) = f (Vun)

(4)
where f jointly denotes drawing the latent counts mn from a Poisson (Eq. 2) with rate λn = Vun 
followed by thresholding mn at 1 (Eq. 1).
In particular  note that marginalizing out mn from
Eq. 1 leads to yn ∼ Bernoulli(1 − exp(−λn)). This link function  termed as the Bernoulli-Poisson
link [28  9]  has also been used recently in modeling relational data with binary observations.
In Eq. (4)  expressing the label vector yn ∈ {0  1}L in terms of Vun is equivalent to a low-rank
assumption on the L × N label matrix Y = [y1 . . . yN ]: Y = f (VU)  where V = [v1 . . . vK] ∈
RL×K

and U = [u1 . . . uN ] ∈ RK×N

  which are modeled as follows

vk ∼ Dirichlet(η1L)
ukn ∼ Gamma(rk  pkn(1 − pkn)−1)
pkn = σ(w(cid:62)
k xn)
wk ∼ Nor(0  Γ)
1   . . .   τ−1

(5)
(6)
(7)
(8)
σ(z) = 1/(1 + exp(−z))  Γ = diag(τ−1
D )  and hyperparameters rk  τ1  . . .   τD are given
improper gamma priors. Since columns of V are Dirichlet drawn  they correspond to distributions
(i.e.  topics) over the labels. It is important to note here that the dependence of the label embedding
un = {ukn}K
k=1 on the feature vector xn is achieved by making the scale parameter of the gamma
k=1 depend on {pkn}K
prior on {ukn}K
k=1 which in turn depends on the features xn via regression
weight W = {wk}K
k=1 (Eq. 6 and 8).

+

Figure 1: Graphical model for the generative process of the label vector. Hyperpriors omitted for brevity.

2

2.1 Computational scalability in the number of positive labels

For the Bernoulli-Poisson likelihood model for binary labels  we can write the conditional poste-
rior [28  9] of the latent count vector mn as

(mn|yn  V  un) ∼ yn (cid:12) Poisson+(Vun)

(9)
where Poisson+ denotes the zero-truncated Poisson distribution with support only on the positive
integers  and (cid:12) denotes the element-wise product. Eq. 9 suggests that the zeros in yn will result
in the corresponding elements of the latent count vector mn being zero  almost surely (i.e.  with
probability one). As shown in Section 3  the sufﬁcient statistics of the model parameters do not
depend on latent counts that are equal to zero; such latent counts can be simply ignored during the
inference. This aspect leads to substantial computational savings in our model  making it scale only
in the number of positive labels in the label matrix. In the rest of the exposition  we will refer to our
model as BMLPL to denote Bayesian Multi-label Learning via Positive Labels.

2.2 Asymmetric Link Function

In addition to the computational advantage (i.e.  scaling in the number of non-zeros in the label ma-
trix)  another appealing aspect of our multi-label learning framework is that the Bernoulli-Poisson
likelihood is also a more realistic model for highly sparse binary data as compared to the commonly
used logistic/probit likelihood. To see this  note that the Bernoulli-Poisson model deﬁnes the prob-
ability of an observation y being one as p(y = 1|λ) = 1 − exp(−λ) where λ is the positive rate
parameter. For a positive λ on the X axis  the rate of growth of the plot of p(y = 1|λ) on the Y axis
from 0.5 to 1 is much slower than the rate it drops from 0.5 to 0. This benavior of the Bernoulli-
Poisson link will encourage a much fewer number of nonzeros in the observed data as compared to
the number of zeros. On the other hand  a logistic and probit approach both 0 and 1 at the same rate 
and therefore cannot model the sparsity/skewness of the label matrix like the Bernoulli-Poisson link.
Therefore  in contrast to multilabel learning models based on logistic/probit likelihood function or
standard loss functions such as the hinge-loss [25  14] for the binary labels  our proposed model
provides better robustness against label imbalance.

3

Inference

A key aspect of our framework is that the conditional posteriors of all the model parameters are
available in closed form using data augmentation strategies that we will describe below. In particular 
since we model binary label matrix as thresholded counts  we are also able to leverage some of the
inference methods proposed for Bayesian matrix factorization of count-valued data [27] to derive an
efﬁcient Gibbs sampler for our model.
Inference in our model requires estimating V ∈ RL×K
  and the
+
hyperparameters of the model. As we will see below  the latent count vectors {mn}N
n=1 (which are
functions of V and U) provide sufﬁcient statistics for the model parameters. Each element of mn
(if the corresponding element in yn is one) is drawn from a truncated Poisson distribution

  W ∈ RD×K  U ∈ RK×N

+

mln ∼ Poisson+(Vl :un) = Poisson+(λln)

k=1 λkln = (cid:80)K

(10)
k=1 vlkukn. Thus we can also write

k=1 mlkn where mlkn ∼ Poisson+(λkln) = Poisson+(vlkukn).

Vl : denotes the lth row of V and λln = (cid:80)K
mln =(cid:80)K
tion mln =(cid:80)K
k=1 mlkn as a draw from a multinomial
(cid:80)K

On the other hand  if yln = 0 then mln = 0 with probability one (Eq. (9))  and therefore need not
be sampled because it does not affect the sufﬁcient statistics of the model parameters.
Using the equivalence of Poisson and multinomial distribution [27]  we can express the decomposi-

[ml1n  . . .   mlKn] ∼ Mult(mln; ζl1n  . . .   ζlKn)

(11)
where ζlkn =
. This allows us to exploit the Dirichlet-multinomial conjugacy and
helps designing efﬁcient Gibbs sampling and EM algorithms for doing inference in our model. As
discussed before  the computational cost of both algorithms scales in the number of ones in the
label matrix Y  which males our model especially appealing for dealing with multilabel learning
problems where the label matrix is massive but highly sparse.

vlkukn
k=1 vlkukn

3

3.1 Gibbs Sampling

Gibbs sampling for our model proceeds as follows
Sampling V: Using Eq. 11 and the Dirichlet-multinomial conjugacy  each column of V ∈ RL×K
can be sampled as

+

vk ∼ Dirichlet(η + m1k  . . .   η + mLk)

n mlnk  ∀l = 1  . . .   L.

where mlk =(cid:80)
where mkn =(cid:80)
Sampling W: Since mkn =(cid:80)

l mlnk and pkn = σ(w(cid:62)

k xn).

Sampling U: Using the gamma-Poisson conjugacy  each entry of U ∈ RK×N

+

ukn ∼ Gamma(rk + mkn  pkn)

(12)

can be sampled as

(13)

Further  since p(ukn|r  pkn) is gamma  we can integrate out ukn from p(mkn|ukn) which gives

l mlnk and mlnk ∼ Poisson+(vlkukn)  p(mkn|ukn) is also Poisson.

mkn = NegBin(rk  pkn)
where NegBin(.  .) denotes the negative Binomial distribution.
Although the negative Binomial is not conjugate to the Gaussian prior on wk  we leverage the P´olya-
Gamma strategy [17] data augmentation to “Gaussianify” the negative Binomial likelihood. Doing
this  we are able to derive closed form Gibbs sampling updates wk  k = 1  . . .   K. The P´olya-
Gamma (PG) strategy is based on sampling a set of auxiliary variables  one for each observation
(which  in the context of sampling wk  are the latent counts mkn). For sampling wk  we draw N
P´olya-Gamma random variables [17] ωk1  . . .   ωkN (one for each training example) as

ωkn ∼ PG(mkn + rk  w(cid:62)
where PG(.  .) denotes the P´olya-Gamma distribution [17].
Given these PG variables  the posterior distribution of wk is Gaussian Nor(µwk   Σwk ) where

k xn)

Σwk = (XΩkX(cid:62) + Γ−1)−1
µwk = Σwk Xκk

(14)

(15)
(16)

where Ωk = diag(ωk1  . . .   ωkN ) and κk = [(mk1 − rk)/2  . . .   (mkN − rk)/2](cid:62).
Sampling the hyperparameters: The hyperparameter rk is given a gamma prior and can be sam-
pled easily. The other hyperparameters τ1  . . .   τD are estimated using Type-II maximum likelihood
estimation [22].

3.2 Expectation Maximization

The Gibbs sampler described in Section 3.1 is efﬁcient and has a computational complexity that
scales in the number of ones in the label matrix. To further scale up the inference  we also develop
an efﬁcient Expectation-Maximization (EM) inference algorithm for our model. In the E-step  we
need to compute the expectations of the local variables U  the latent counts  and the P´olya-Gamma
variables ωk1  . . .   ωkN   for k = 1  . . .   K. These expectations are available in closed form and can
thus easily be computed. In particular  the expectation of each P´olya-Gamma variable ωkn is very
efﬁcient to compute and is available in closed form [20]

E[ωkn] =

(mkn + rk)
2w(cid:62)

k xn

tanh(w(cid:62)

k xn/2)

(17)

The M-step involves a maximization w.r.t. V and W  which essentially involves solving for their
maximum-a-posteriori (MAP) estimates  which are available in closed form. In particular  as shown
in [20]  estimating wk requires solving a linear system which  in our case  is of the form

(18)
where Sk = XΩkX(cid:62) + Γ−1  dk = Xκk  Ωk and κk are deﬁned as in Section 3.1  except that the
P´olya-Gamma random variables are replaced by their expectations given by Eq. 17. Note that Eq. 18

Skwk = dk

4

can be straighforwardly solved as wk = S−1
k dk. However  convergence of the EM algorithm [20]
does not require solving for wk exactly in each EM iteration and running a couple of iterations of
any of the various iterative methods that solves a linear system of equations can be used for this step.
We use the Conjugate Gradient [2] method to solve this  which also allows us to exploit the sparsity
in X and Ωk to very efﬁciently solve this system of equations  even when D and N are very large.
Although in this paper  we only use the batch EM  it is possible to speed it up even further using
an online version of this EM algorithm  as shown in [20]. The online EM processes data in small
minibatches and in each EM iteration updates the sufﬁcient statistics of the global parameters. In
our case  these sufﬁcient statistics include Sk and dk  for k = 1  . . .   K  and can be updated as

S(t+1)
k
d(t+1)
k

= (1 − γt)S(t)
= (1 − γt)d(t)

k + γtX(t)Ω(t)
k + γtX(t)κ(t)

k

k X(t)(cid:62)

where X(t) denotes the set of examples in the current minibatch  and Ω(t)
that are computed using the data from the current minibatch.

k and κ(t)

k denote quantities

3.3 Predicting Labels for Test Examples
Predicting the label vector y∗ ∈ {0  1}L for a new test example x∗ ∈ RD can be done as

p(y∗ = 1|x∗) =

(1 − exp(−Vu∗))p(u∗)du∗

(cid:90)

u∗

If using Gibbs sampling  the integral above can be approximated using samples {u(m)∗ }M
m=1 from
the posterior of u∗. It is also possible to integrate out u∗ (details skipped for brevity) and get closed
form estimates of probability of each label yl∗ in terms of the model parameters V and W  and it is
given by

p(yl∗ = 1|x∗) = 1 − K(cid:89)

1
[Vlk exp(w(cid:62)
k x∗) + 1]rk

(19)

k=1

4 Computational Cost
Computing the latent count mln for each nonzero entry yln in Y requires computing
[ml1n  . . .   mlKn]  which takes O(K) time;
therefore computing all the latent counts takes
O(nnz(Y)K) time  which is very efﬁcient if Y has very few nonzeros (which is true of most real-
world multi-label learning problems). Estimating V  U  and the hyperparameters is relatively cheap
and can be done very efﬁciently. The P´olya-Gamma variables  when doing Gibbs sampling  can be
efﬁciently sampled using methods described in [17]; and when doing EM  these can be even more
cheaply computed because the P´olya-Gamma expectations  which are available in closed form (as
a hyperbolic tan function)  can be very efﬁciently computed [20]. The most dominant step is esti-
mating W; when doing Gibbs sampling  if done na¨ıvely  it would O(DK 3) time if sampling W
row-wise  and O(KD3) time if sampling column-wise. However  if using the EM algorithm  esti-
mating W can be done much more efﬁciently  e.g.  using Conjugate Gradient updates because  it is
not even required to solved for W exactly in each iteration of the EM algorithm [20]. Also note that
since most of the parameters updates for different k = 1  . . .   K  n = 1  . . .   N are all independent
of each other  our Gibbs sampler and the EM algorithms can be easily parallelized/block-updated.

5 Connection: Topic Models with Meta-Data

+

As discussed earlier  our multi-label learning framework is similar in spirit to a topic model as the
label embeddings naturally correspond to topics - each Dirichlet-drawn column vk of the matrix
V ∈ RL×K
can be seen as representing a “topic”. In fact  our model  interestingly  can directly be
seen as a topic model [3  27] where we have side-information associated with each document (e.g. 
document features). For example  if each document yn ∈ {0  1}L (in a bag-of-words representation
with vocabulary of size L) may also have some meta-data xn ∈ RD associated with it. Our model
can therefore also be used to perform topic modeling of text documents with such meta-data [15  12 
29  19] in a robust and scalable manner.

5

6 Related Work

Despite a signiﬁcant number of methods proposed in the recent years  learning from multi-label
data continues to remain an active area of research  especially due to the recent surge of interest in
learning when the output space (i.e.  the number of labels) is massive. To handle the huge dimen-
sionality of the label space  a common approach is to embed the labels in a lower-dimensional space 
e.g.  using methods such as Canonical Correlation Analysis or other methods for jointly embedding
feature and label vectors [26  5  23]  Compressed Sensing[8  10]  or by assuming that the matrix
consisting of the weight vectors of all the labels is a low-rank matrix [25]. Another interesting line
of work on label embedding methods makes use of random projections to reduce the label space
dimensionality [11  16]  or use methods such as multitask learning (each label is a task).
Our proposed framework is most similar in spirit to the aforementioned class of label embedding
based methods (we compare with some of these in our experiments). In contrast to these methods 
our framework reduces the label-space dimensionality via a nonlinear mapping (Section 2)  our
framework has accompanying inference algorithms that scale in the number of positive labels 2.1 
has an underlying generative model that more realistically models the imbalanced nature of the labels
in the label matrix (Section 2.2)  can deal with missing labels  and is easily parallelizable. Also  the
connection to topic models provide a nice interpretability to the results  which is usually not possible
with the other methods (e.g.  in our model  the columns of the matrix V can be seen as a set of topics
over the labels; in Section 7.2  we show an experiment on this). Moreover  although in this paper  we
have focused on the multi-label learning problem  our framework can also be applied for multiclass
problems via the one-vs-all reduction  in which case the label matrix is usually very sparse (each
column of the label matrix represents the labels of a single one-vs-all binary classiﬁcation problem).
Finally  although not a focus of this paper  some other important aspects of the multi-label learning
problem have also been looked at in recent work. For example  fast prediction at test time is an
important concern when the label space is massive. To deal with this  some recent work focuses
on methods that only incur a logarithmic cost (in the number of labels) at test time [1  18]  e.g.  by
inferring and leveraging a tree structure over the labels.

7 Experiments

We evaluate the proposed multi-label learning framework on four benchmark multi-label data sets -
bibtex  delicious  compphys  eurlex [25]  with their statistics summarized in Table 1. The data sets
we use in our experiments have both feature and label dimensions that range from a few hundreds
to a several thousands. In addition  the feature and/or label matrices are also quite sparse.

Data set
bibtex
delicious
compphys
eurlex

D
1836
500
33 284
5000

L
159
983
208
3993

Ntrain
4880
12920
161
17413

Training set

¯L
2.40
19.03
9.80
5.30

¯D
68.74
18.17
792.78
236.69

Ntest
2515
3185
40
1935

Test set
¯L
2.40
19.00
11.83
5.32

¯D
68.50
18.80
899.20
240.96

Table 1: Statistics of the data sets used in our experiments. ¯L denotes average number of positive
labels per example; ¯D denotes the average number of nonzero features per example.

We compare the proposed model BMLPL with four state-of-the-art methods. All these methods 
just like our method  are based on the assumption that the label vectors live in a low dimensional
space.

bedding the label vectors conditioned on the features.

method that uses the idea of doing compressed sensing on the labels [8].

• CPLST: Conditional Principal Label Space Transformation [5]: CPLST is based on em-
• BCS: Bayesian Compressed Sensing for multi-label learning [10]: BCS is a Bayesian
• WSABIE: It assumes that the feature as well as the label vectors live in a low dimensional
• LEML: Low rank Empirical risk minimization for multi-label learning [25]. For LEML  we
report the best results across the three loss functions (squared  logistic  hinge) they propose.

space. The model is based on optimizing a weighted approximate ranking loss [23].

6

Table 2 shows the results where we report the Area Under the ROC Curve (AUC) for each method on
all the data sets. For each method  as done in [25]  we vary the label space dimensionality from 20%
- 100% of L  and report the best results. For BMLPL  both Gibbs sampling and EM based inference
perform comparably (though EM runs much faster than Gibbs); here we report results obtained with
EM inference only (Section 7.4 provides another comparison between these two inference methods).
The EM algorithms were run for 1000 iterations and they converged in all the cases.
As shown in the results in Table 2  in almost all of the cases  the proposed BMLPL model performs
better than the other methods (except for compphys data sets where the AUC is slightly worse than
LEML). The better performance of our model justiﬁes the ﬂexible Bayesian formulation and also
shows the evidence of the robustness provided by the asymmetric link function against sparsity and
label imbalance in the label matrix (note that the data sets we use have very sparse label matrices).

bibtex
delicious
compphys
eurlex

CPLST
0.8882
0.8834
0.7806
-

BCS
0.8614
0.8000
0.7884
-

WSABIE
0.9182
0.8561
0.8212
0.8651

LEML
0.9040
0.8894
0.9274
0.9456

BMLPL
0.9210
0.8950
0.9211
0.9520

Table 2: Comparison of the various methods in terms of AUC scores on all the data sets. Note: CPLST and
BCS were not feasible to run on the eurlex data  so we are unable to report those numbers here.
7.1 Results with Missing Labels

Our generative model for the label matrix can also handle missing labels (the missing labels may
include both zeros or ones). We perform an experiment on two of the data sets - bibtex and compphys
- where only 20% of the labels from the label matrix are revealed (note that  of all these revealed
labels  our model uses only the positive labels)  and compare our model with LEML and BCS (both
are capable of handling missing labels). The results are shown in Table 3. For each method  we
set K = 0.4L. As the results show  our model yields better results as compared to the competing
methods even in the presence of missing labels.

bibtex
compphys

BCS
0.7871
0.6442

LEML
0.8332
0.7964

BMLPL
0.8420
0.8012

Table 3: AUC scores with only 20% labels observed.

7.2 Qualitative Analysis: Topic Modeling on Eurlex Data

Since in our model  each column of the L × K matrix V represents a distribution (i.e.  a “topic”)
over the labels  to assess its ability of discovering meaningful topics  we run an experiment on the
Eurlex data with K = 20 and look at each column of V. The Eurlex data consists of 3993 labels
(each of which is a tags; a document can have a subset of the tags)  so each column in V is of that
size. In Table 4  we show ﬁve of the topics (and top ﬁve labels in each topic  based on the magnitude
of the entries in the corresponding column of V). As shown in Table 4  our model is able to discover
clear and meaningful topics from the Eurlex data  which shows its usefulness as a topic model when
each document yn ∈ {0  1}L has features in form of meta data xn ∈ RD associated with it.

Topic 1 (Nuclear)
nuclear safety
nuclear power station
radioactive efﬂuent
radioactive waste
radioactive pollution

Topic 2 (Agreements)
EC agreement
trade agreement
EC interim agreement
trade cooperation
EC coop. agree.

Topic 3 (Environment)
environmental protection
waste management
env. monitoring
dangerous substance
pollution control measures

Topic 4 (Stats & Data)
community statistics
statistical method
agri. statistics
statistics
data transmission

Topic 5 (Fishing Trade)
ﬁshing regulations
ﬁshing agreement
ﬁshery management
ﬁshing area
conservation of ﬁsh stocks

Table 4: Most probable words in different topics.

7

7.3 Scalability w.r.t. Number of Positive Labels

To demonstrate the linear scalability in the number of positive labels  we run an experiment on the
Delicious data set by varying the number of positive labels used for training the model from 20% to
100% (to simulate this  we simply treat all the other labels as zeros  so as to have a constant label
matrix size). We run each experiment for 100 iterations (using EM for the inference) and report
the running time for each case. Fig. 2 (left) shows the results which demonstrates the roughly linear
scalability w.r.t. the number of positive labels. This experiment is only meant for a small illustration.
Note than the actual scalability will also depend on the relative values of D and L and the sparsity
of Y. In any case  the amount of computations the involve the labels (both positive and negatives)
only depend on the positive labels  and this part  for our model  is clearly linear in the number of
positive labels in the label matrix.

Figure 2: (Left) Scalability w.r.t. number of positive labels. (Right) Time vs accuracy comparison for Gibbs
and EM (with exact and with CG based M steps)

7.4 Gibbs Sampling vs EM

We ﬁnally show another experiment comparing both Gibbs sampling and EM for our model in terms
of accuracy vs running time. We run each inference method only for 100 iterations. For EM  we
try two settings: EM with an exact M step for W  and EM with an approximate M step where
we run 2 steps of conjugate gradient (CG). Fig. 2 (right)  shows a plot comparing each inference
method in terms of the accuracy vs running time. As Fig. 2 (right) shows  the EM algorithms (both
exact as well as the one that uses CG) attain reasonably high AUC scores in a short amount of time 
which the Gibbs sampling takes much longer per iteration and seems to converge rather slowly.
Moreover  remarkably  EM with 2 iterations CG in each M steps seems to perform comparably
to the EM with an exact M step  while running considerably faster. As for the Gibbs sampler 
although it runs slower than the EM based inference  it should be noted that the Gibbs sampler
would still be considerably faster than other fully Bayesian methods for multi-label prediction (such
as BCS [10]) because it only requires evaluating the likelihoods over the positive labels in the label
matrix). Moreover  the step involving sampling of the W matrix can be made more efﬁcient by using
cholesky decompositions which can avoid matrix inversions needed for computing the covariance
of the Gaussian posterior on wk.
8 Discussion and Conclusion

We have presented a scalable Bayesian framework for multi-label learning. In addition to providing
a ﬂexible model for sparse label matrices  our framework is also computationally attractive and
can scale to massive data sets. The model is easy to implement and easy to parallelize. Both full
Bayesian inference via simple Gibbs sampling and EM based inference can be carried out in this
model in a computationally efﬁcient way. Possible future work includes developing online Gibbs
and online EM algorithms to further enhance the scalability of the proposed framework to handle
even bigger data sets. Another possible extension could be to additionally impose label correlations
more explicitly (in addition to the low-rank structure already imposed by the current model)  e.g. 
by replacing the Dirichlet distribution on the columns of V with logistic normal distributions [4].
Because our framework allows efﬁciently computing the predictive distribution of the labels (as
shown in Section 3.3)  it can be easily extend for doing active learning on the labels [10]. Finally 
although here we only focused on multi-label learning  our framework can be readily used as a robust
and scalable alternative to methods that perform binary matrix factorization with side-information.
Acknowledgements This research was supported in part by ARO  DARPA  DOE  NGA and ONR

8

20%40%60%60%100%200300400500600700800Fraction of Positive LabelsTime Taken10−21001021040.650.70.750.80.850.9TimeAUC EM−CGEM−ExactGibbsReferences
[1] Rahul Agrawal  Archit Gupta  Yashoteja Prabhu  and Manik Varma. Multi-label learning with millions of

labels: Recommending advertiser bid phrases for web pages. In WWW  2013.

[2] Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont  1999.
[3] David M Blei  Andrew Y Ng  and Michael I Jordan. Latent dirichlet allocation. JMLR  2003.
[4] Jianfei Chen  Jun Zhu  Zi Wang  Xun Zheng  and Bo Zhang. Scalable inference for logistic-normal topic

models. In NIPS  2013.

[5] Yao-Nan Chen and Hsuan-Tien Lin. Feature-aware label space dimension reduction for multi-label clas-

siﬁcation. In NIPS  2012.

[6] Eva Gibaja and Sebasti´an Ventura. Multilabel learning: A review of the state of the art and ongoing

research. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery  2014.

[7] Eva Gibaja and Sebasti´an Ventura. A tutorial on multilabel learning. ACM Comput. Surv.  2015.
[8] Daniel Hsu  Sham Kakade  John Langford  and Tong Zhang. Multi-label prediction via compressed

sensing. In NIPS  2009.

[9] Changwei Hu  Piyush Rai  and Lawrence Carin. Zero-truncated poisson tensor factorization for massive

binary tensors. In UAI  2015.

[10] Ashish Kapoor  Raajay Viswanathan  and Prateek Jain. Multilabel classiﬁcation using bayesian com-

pressed sensing. In NIPS  2012.

[11] Nikos Karampatziakis and Paul Mineiro. Scalable multilabel prediction via randomized methods. arXiv

preprint arXiv:1502.02710  2015.

[12] Dae I Kim and Erik B Sudderth. The doubly correlated nonparametric topic model. In NIPS  2011.
[13] Xiangnan Kong  Zhaoming Wu  Li-Jia Li  Ruofei Zhang  Philip S Yu  Hang Wu  and Wei Fan. Large-scale

multi-label learning with incomplete label assignments. In SDM  2014.

[14] Xin Li  Feipeng Zhao  and Yuhong Guo. Conditional restricted boltzmann machines for multi-label

learning with incomplete labels. In AISTATS  2015.

[15] David Mimno and Andrew McCallum. Topic models conditioned on arbitrary features with dirichlet-

multinomial regression. In UAI  2008.

[16] Paul Mineiro and Nikos Karampatziakis. Fast label embeddings for extremely large output spaces. In

ICLR Workshop  2015.

[17] Nicholas G Polson  James G Scott  and Jesse Windle. Bayesian inference for logistic models using p´olya–

gamma latent variables. Journal of the American Statistical Association  108(504):1339–1349  2013.

[18] Yashoteja Prabhu and Manik Varma. FastXML: a fast  accurate and stable tree-classiﬁer for extreme

multi-label learning. In KDD  2014.

[19] Maxim Rabinovich and David Blei. The inverse regression topic model. In ICML  2014.
[20] James G Scott and Liang Sun. Expectation-maximization for logistic regression.

arXiv:1306.0040  2013.

arXiv preprint

[21] Farbound Tai and Hsuan-Tien Lin. Multilabel classiﬁcation with principal label space transformation.

Neural Computation  2012.

[22] Michael E Tipping. Bayesian inference: An introduction to principles and practice in machine learning.

In Advanced lectures on machine Learning  pages 41–62. Springer  2004.

[23] Jason Weston  Samy Bengio  and Nicolas Usunier. WSABIE: Scaling up to large vocabulary image

annotation. In IJCAI  2011.

[24] Yan Yan  Glenn Fung  Jennifer G Dy  and Romer Rosales. Medical coding classiﬁcation by leveraging

inter-code relationships. In KDD  2010.

[25] Hsiang-Fu Yu  Prateek Jain  Purushottam Kar  and Inderjit S Dhillon. Large-scale multi-label learning

with missing labels. In ICML  2014.

[26] Yi Zhang and Jeff G Schneider. Multi-label output codes using canonical correlation analysis. In AISTATS 

2011.

[27] M. Zhou  L. A. Hannah  D. Dunson  and L. Carin. Beta-negative binomial process and poisson factor

analysis. In AISTATS  2012.

[28] Mingyuan Zhou. Inﬁnite edge partition models for overlapping community detection and link prediction.

In AISTATS  2015.

[29] Jun Zhu  Ni Lao  Ning Chen  and Eric P Xing. Conditional topical coding: an efﬁcient topic model

conditioned on rich features. In KDD  2011.

9

,Piyush Rai
Ricardo Henao
Lawrence Carin