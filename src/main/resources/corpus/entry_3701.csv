2011,Nearest Neighbor based Greedy Coordinate Descent,Increasingly  optimization problems in machine learning  especially those arising from high-dimensional statistical estimation  have a large number of variables. Modern statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structure to the problem  such as sparsity. A central question is whether similar advances can be made in their computational complexity as well. In this paper  we propose strategies that indicate that such advances can indeed be made. In particular  we investigate the greedy coordinate descent algorithm  and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse. We then propose a suite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search.   We also devise a more amenable form of greedy descent for composite non-smooth objectives; as well as several approximate variants of such greedy descent. We develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing. Without tuning the latter data structure  we are not only able to significantly speed up the vanilla greedy method  but also outperform cyclic descent when the problem size becomes large. Our results indicate the effectiveness of our nearest neighbor strategies  and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods.,Nearest Neighbor based Greedy Coordinate Descent 

Inderjit S. Dhillon 

Department of Computer Science 

University of Texas at Austin 

inderjit@cs.utexas.edu 

Pradeep Raviknmar 

Department of Computer Science 

University of Texas at Austin 

pradeepr@cs.utexas.edu 

Ambuj Tewari 

Department of Computer Science 

University of Texas at Austin 
ambuj@cs.utexas.edu 

Abstract 

Increasingly  optimization problems in machine learning  especially those arising 
from bigh-dimensional statistical estimation  bave a large number of variables. 
Modem statistical estimators developed over the past decade have statistical or 
sample complexity that depends only weakly on the number of parameters when 
there is some structore to the problem  such as sparsity. A central question is 
whether similar advances can be made in their computational complexity as well. 
In this paper  we propose strategies that indicate that such advances can indeed be 
made. In particular  we investigate the greedy coordinate descent algorithm  and 
note that performing the greedy step efficiently weakens the costly dependence on 
the problem size provided the solution is sparse. We then propose a snite of meth(cid:173)
ods that perform these greedy steps efficiently by a reduction to nearest neighbor 
search. We also devise a more amenable form of greedy descent for composite 
non-smooth objectives; as well as several approximate variants of such greedy 
descent. We develop a practical implementation of our algorithm that combines 
greedy coordinate descent with locality sensitive hashing. Without tuning the lat(cid:173)
ter data structore  we are not only able to significantly speed up the vanilla greedy 
method  hot also outperform cyclic descent when the problem size becomes large. 
Our resnlts indicate the effectiveness of our nearest neighbor strategies  and also 
point to many open questions regarding the development of computational geo(cid:173)
metric techniques tailored towards first-order optimization methods. 

1 Introduction 
Increasingly  optimization problems in machine learning are very high-dimensional  where the num(cid:173)
ber of variables is very large. This has led to a renewed interest in iterative algorithms that reqnire 
bounded time per iteration. Such iterative methods take various forms such as so-called row-action 
methods [6] which enforce constraints in the optimization problem sequentially  or first-order meth(cid:173)
ods [4] which only compute the gradient or a coordinate of the gradient per step. But the overall time 
complexity of these methods still has a high polynomial dependence on the number of parameters. 
Modem statistical estimators developed over the past decade have statistical or sample complexity 
that depends only weakly on the number ofpararneters [5  15  18]. Can similar advances be made 
in their computational complexity? 

Towards this  we investigate one of the simplest classes of first order methods: coordinate descent  
which only updates a single coordinate of the iterate at every step. The coordinate descent class 
of algorithms has seen a renewed interest after recent papers [8  10  19] have shown considerable 
empirical success in application to large problems. Saba and Tewari [13] even show that under 

1 

certain conditions  the convergence rate of cyclic coordinate descent is at least as fast as that of 
gradient descent. 

In this paper  we focus on high-dimensional optimization problems where the solution is sparse. 
We were motivated to investigate coordinate descent algorithms by the intuition that they could 
leverage the sparsity structure of the solution by judiciously choosing the coordinate to be updated. 
In particular  we show that a greedy selection of the coordinates succeeds in weakening the costly 
dependence on problem size with the caveat that we could perform the greedy step efficiently. The 
naive greedy updates would however take time that scales at least linearly in the problem dimension 
O(P) since it has to compute the coordinate with the maximum gradient. We thus come to the other 
main question of this paper: Can the greedy steps in a greedy coordinate scheme be peiformed 
efficiently? Surprisingly  we are able to answer in the affirmative  and we show this by a reduction 
to nearest neighbor search. This allows us to leverage the significant amount of recent research 
on sublinear methods for nearest neighbor search  provided it suffices to have approximate nearest 
neighbors. The upshot of our results is a suite of methods that depend weakly on the problem size 
or number of parameters. We also investigate several notions of approximate greedy coordinate 
descent for which we are able to derive similar rates. For the composite objective case  where the 
objective is the sum of a smooth component and a separable non-smooth component  we propose 
and analyze a "look-ahead" variant of greedy coordinate descent. 

The development in this paper thus raises a new line of research on connections between computa(cid:173)
tional geometry and first-order optimization methods. For instance  given our results  it would be of 
interest to develop approximate nearest neighbor methods tuned to greedy coordinate descent. As an 
instance of such a connection  we show that if the covariates underlying the optimization objective 
satisfy a mutual incoherence condition  then a very simple nearest neighbor data structure suffices to 
yield a good approximation. Finally  we provide simulations that not ouly show that greedy coordi(cid:173)
nate descent with approximate nearest neighbor search performs overwheltuingly better than vanilla 
greedy coordinate descent  but also that it starts outperforming cyclic descent when the problem size 
increases: the larger the number of variables  the greater the relative improvement in performance. 
The results of this paper natorally lead to several open problems: can effective computational ge(cid:173)
ometric data structures be tailored towards greedy coordinate descent? Can these be adapted to 
(a) other first-order methods  perhaps based on sampling  and (b) different regularized variants that 
uncover structored sparsity. We hope this paper fosters further research and cross-fertilization of 
ideas in computational geometry and optimization. 
2 Setup and Notation 
We start our treatment with differentiable objective functions  and then extend this to encompass 
non-differentiable functions which arise as the sum of a smooth component and a separable non(cid:173)
smooth component. Let C : JR" --+ IR be a convex differentiable function. We do not assume that 
the function is strongly convex: indeed most optimizations arising out of high-dimensional machine 
learning problems are convex but typically not strungly so. Our analysis requires that the function 
satisfies the following coordinate-wise Lipschitz condition: 
A •• omptionAt. The loss function C satisfies 

IIV'C(w) - V'C(v)ll~ ::; "1 ·llw - vIiI  for some "1> o. 

We note that this condition is weaker than the standard Lipschitz conditions on the gradients. In par(cid:173)
ticular  we say that C has "2-Lipschitz continuous gradient w.r.t. 11·112 when IIV' C(w) - V' C(v)112 ::; 
"2 . IIw - vl12' Note that "1 ::; "2; indeed "1 could be up to p times smaller than "2. E.g. when 
C(w) = 1/2w T Aw with a positive setui-definite matrix A   we have "1 = max; A; ;  the maximum 
entry on the diagonal  while "2 = max; >';(A)  the maxium eigenvalue of A. It is thus possible for 
"2 to be much larger than " : for instance "2 = P'" when A is the all I's matrix. 
We are interested in the general optimization problem  
min C(w). 
wE"" 

(I) 

We will focus on the case where the solution is bounded and sparse. We thus assume: 
A •• omptionAl. The solution w' of(J) satisfies: Ilw'll~ ::; B for some constant B < 00 indepen(cid:173)
dent ofp  and that Ilw'lIo = 8  i.e.  solution is 8-sparse. 

2.t Coordinate Descent 
Coordinate descent solves (I) iteratively by optimizing over a single coordinate while holding others 
fixed. lYPically  the choice of the coordinate to be updated is cyclic. One caveat with this scheme 

2 

however is that it could be expensive to compute the one-dimensional optimum for general functions 
£ . Moreover when £  is not smooth  such coordinatewise descent is not guaranteed to converge to 
the global optimum in general  unless the non-differentiable component is separable [16]. A line 
of recent work [16  17  14] has thus focused on a "gradient descent" version of coordinate descent  
that iteratively uses a local quadratic upper bound rY of the function C. For the case where the 
optimization function is the sum of a smooth function aod the i l regularizer  this variant is also 
ca\led Iterative Soft Thresholding [7]. A template for such coordinate gradient descent is the set of 
;  VjC(w')ej. Friedman et aI. [8]  Genkin et aI. [10]  Wu and Laoge [19] 
iterates: w' = W'-I -
aod others have shown considerable empirical success in applying these to large problems. 
2.2 Greedy Coordinate Descent 
10 this section  we focus on a simple deterministic variant of coordinate descent that picks the coor(cid:173)
dinate that attains the coordinalewise maximum of the gradient vector: 
Algorithm 1 Greedy Coordinate Gradient Descent 

Initialize: Set the initial value of wO
• 
fort = 1  ... do 

j = argmruq IVIC(w')I. 
w' = w'-I - ;  VjC(w')ej. 

end for 

Lemma 1. Suppose the convex differentiable function C satisfies Assumptions Al and A2. Then 
the iterates of Algorithm 1 satisfy: 

C(w') _ C(w*) :<; ~I Ilw ~ w II.. 

° *. 

Letting c(P) denote the time required to solve each greedy step mruq IV IC( w') I  the greedy version 
of coordinate descent achieves the rate C(w') - C(w*) = 0(.' c(P)IT) at time T. Note that the 
dependence on the problem size p is restricted to the greedy step: if we could solve this maximization 
more efficiently  then we have a powerful "active-set" method. While brute force maximization for 
the greedy step would take O(P) time  ifit cao be done in 0(1) time  then at time T  the iterate w 
satisfies C( w) - C( w*) = 0(.' IT) which would be independent of the problem size. 
3 Nearest Neighbor aod Fast Greedy 
10 this section  we examine whether the greedy step cao be performed in sublinear time. We focus in 
particular on optimization problems arising from statistical learoing problems where the optimiza(cid:173)
tion objective can be written as 

n 

C(w) = ~i(wTx' y')  

i=l 

(2) 

for some loss functioni : RxR r-> R  and a set of observations {(Xi  yi)}:'~I' with Xi E RP  yi E R. 
Note that such an optimization objective arises in most statisticallearoing problems. For instance  
consider linear regression  with response y = (w  x) + E  where E ~ N(O  1). Then given observa(cid:173)
tions {(xi  yi)}:'~I' the maximum likelihood problem has the form of (2)  with i(u  v) = (u - v)'. 
LetJing g( u  v) = V ui( u  v) denote the gradient of the sample loss with respect to its first ar(cid:173)
gument  and ri(w) = g(wT Xi  yi)  the gradient of the objective (2) may be written as VjC(w) = 
L~~I x~ r'(w) = (Xj  r(w)) . It then follows that the greedy coordinate descent step in Algorithm 1 
reduces to the following simple problem: 

  
maxi (xj r(w)) I· 

(3) 

We can now see why the greedy step (3) cao be performed efficiently: it cao be cast as a nearness 
problem. Iodeed  assume that the data is standardized so that IIxj II = 1 for j = 1  ...  po Let 
x = {XI  ...   xp  -X" ...   -xp} include the negated data vectors. Then  it cao be seen that 

argmax I (Xj  r) I == arg min IIxj - rll~· 

 E[Pj 

 Ej'pj 

(4) 

Thus  the greedy step amounts to a nearest neighbor problem of computing the nearest point to r in 
the set {Xj} ~~I' While this would take O(pn) time via brute force  the hope is to leverage the state of 

3 

the art in nearest neighbor search [II] to perform this greedy selection in sublinear time. Regarding 
the time taken to compute the gradient r(w)  note that after any coordinate descent update  we can 
update r' in 0(1) time if we cache the values {(w  x')}  so that r can be updated in O(n) time. 
The reduction to nearest neighbor search however comes with a caveat: nearest neighbor search vari(cid:173)
ants that run in sublinear time only compute approximate nearest neighbors. This in turn aroounts 
to performing the greedy step approximately. In the next few subsections  we investigate the conse(cid:173)
quences of such approximations. 

3.1 Multiplicative Greedy 
We first consider a variant where the greedy step is performed under a mnltiplicative approximation  
where we choose a coordinate it such that  for some c E (0 1]  

IIV.c(w')];  I 2: c·IIV.c(w')lloo. 

(5) 

As the following lemma shows  the approximate greedy steps have little qualitative effect (proof in 
Supplementary Material). 

Lemma 2. The greedy coordinate descent iterates  with the greedy step computed as in (5)  satisfy: 

.c(w') _ .c(w*) :0; ~ . ""lwO; w*ll~ . 

The price for the approximate greedy updates is thus just a constant factor 1/ c 2: I reduction in the 
convergence rate. 

Note that the equivalence of (4) need not hold under multiplicative approximations. That is  approx(cid:173)
imate nearest neighbor techuiques that obtain a nearest neighbor upto a multiplicative factor  do not 
guarantee a mnltiplicative approximation for the inner product in the greedy step in turn. As the next 
lemma shows this still achieves the required qualitative rate. 

Lemma 3. Suppose the greedy step is performed as in (5) with a multiplicative approximation factor 
of (I +  =) (due to approximate nearest neighbor search for instance). Then  at any iteration t  the 
greedy coordinate descent iterates satisfy either of the following two conditions  for any' > 0: 

(a) V.c(w') is small (i.e. the iterate is near-stationary): IIV.c(w')lloo :0; C::::<:) Ilr(w')1I2' or 
(b) .c(w') - .c(w*) < 

. ~ lIwo_w'll: 

t 

1+'00 

- EIIII(l/f)+l 

3.2 Additive Greedy 
Another natural variant is the following additive approximate greedy coordioate descent  where we 
choose the coordinate i  such that 

(6) 
for some 'odd. As the lemma below shows  the approximate greedy steps have little qualitative effect 
Lemma 4. The greedy coordinate descent iterates  with the greedy step computed as in (6)  satisfy: 

.c(w') - .c(w*) :0; ""lwO; w*ll~ + 'odd. 

Note that we need obtain an additive approximation in the greedy step only upto the order of the 
final precision desired of the optimization problem. In particular  for statistical estimation problems 
the desired optimization accuracy need not be lower than the statisical precision  which is typically 
of the order of slog(P) /..;n. Indeed  given the conoections elucidated above to greedy coordinate 
descent  it is an interesting futore problem to develop approximate nearest neighbor methods with 
additive approximations. 

4 Tailored Nearest Neighbor Data Structures 
In this section  we show that one could develop approximate nearest neighbor methods tailored to 
the statistical estimation setting. 

4 

4.1 Qnadtree nnder Mntnallncoherence 
We will show that just a vanilla quadtree yields a good approximation when the covariates satisfY 
a technical statistical condition of mutual coherence. A quadtree is a tree data structure which 
partitions the space. Each internal node u in the quadtree has a representative point  denoted by 
rep(u)  and a list of children nodes  denoted by children(u)  which partition the space under u. For 
further details  we refer to Har-Peled [II]. The spread <li(D) of the set of points D is defined as 
<li(D) = m~;~; 1IIIx;-x'llll  and is the mtio between the diameter of D and the closest pair distance of 
points in D. Following Har-Peled [II]  we can show that the depth of the quadtree by the standard 
construction is bounded by O(log <li( D) + log n) and can be constructed in time O(p log( n<li (D))). 
Here  we show that a standard nearest neighbor algorithm using quadtrees Har-Peled [II]  Arya 
and Mount [2]  rewritten below to allow for arbitrary approximation factor (1 + <=)  suffices under 
appropriate statistical conditions. 

mIDi~J Xi 

:1:; 

Input: quadtree T  approx. factor (1 + <nn)  query r. 
Initialize: ; = 0; Ao = {root(T)}. 
while Ai oF {} do 

for each node v E Ai do 

Uonn = nu(T  { ..... } u rep( children( v))). 
for each node u E children( v) do 

rep(u)II - diam(u) < Ilr - u onnll/(1 + <=)  then AHl = Am u {u}. 

if Ilr -
end for 

end for 
i+-;+1 

end while 
Return U ann. 

Lemma S. Let (1 + <nn) be the approximation factor for the approximate nearest neighbor search. 
Let nn(T) be the true nearest neighbor to r. Then the output ..... of Algorithm 4.1 satisfies 

Ilr -

..... 112 =:; (1 + <=)IIT - nn(r) 112. 

Proof Let u be the last node in the quadtree containing nn( T) thrown away by the algorithm. Then  
1 ~ ::11  

Ilr - nn(T)11 2: liT - rep(u)II-llrep(u) - nn(T) II 2: liT - rep(u)ll- diam(u) 2: IIr

whence the statement in the theorem follows. 

D 

The next lemma shows the time taken by the algorithm. Again  we rewrite the analysis ofHar-Peled 
[II]  Arya and Mount [2] to allow for arbitrary approximation factors. 
Lemma 6. The time taken by algorithm 4.1 to compute a (1 + <nn)-nearest neighbor to T from 
D = {Xl '"  Xp} is 0 (IOg(<li(D)) + (1 +  ;"f). 

As the next lemma shows  the spread is controlled when the mutual coherence of the covariates is 
small. In particular  define f.'(D) = ma.x;"j (Xi  Xj). We require that the mutual coherence f.'(D) be 
small and in particular be bounded away from I. Such a condition is typically imposed as sufficient 
condition for sparse parameter recovery [5  15]. Intriguingly  this very condition allows us to provide 
guarantees for optimization. This thus adds to the burgeoning set of recent papers that are finding 
that conditions imposed for strong statistical guarantees are useful in torn for obtaining faster mtes 
in the corresponding optimization problems. 
Under this condition  the closest pair distance can be bounded as  Ilxi - Xj 112 = 2 - 2 (Xi  Xj) 2: 
f.')  which in torn allows us to control the spread: <li(D) =:; ~ = J l~~' which thus 
2(1 -
yields the corollary: 
Lemma 7. Suppose the mutual coherence of the covariates D = {Xl  ...  xp} is bounded so that 
f.'(D) < 1. Then the time taken by algorithm 4.1 to compute a (1 + <nn)-nearest neighbor to r from 
is 0 (log ( ~~) + (1+  ;  )} 

5 

While this data structure is quite useful in most settings  it requires that the mutual coherence of the 
covariates be bounded  and further the time required is exponential (but weakly so) in the number of 
samples. However  following [I  II]  we can use random projections to bring the runtime down to 
O(P  ;;')  and the preprocessing time to O(np logpf.;;.2). 
5 Overall TIme Complexity 
In the previous sections  we saw that the greedy step for generalized linear models is equivalent to 
nearest neighbor search: given any query r  we want to find its nearest neighbor among the p points 
D = {X" ...   xp } each in IRn. Standard data structures include quadtrees which spatially partition 
the data  and KD trees which partition the data according to their point mass. 

rl12 ~ (1 + f=)llx; -

Approximate nearest neighbor search [11] estimates an approximate nearest neighbor  upto a multi(cid:173)
plicative approximation say f=: so that if the nearest neighbor to r is x; and the algorithm outputs 
Xk  then it guarantees that Ilx> -
rll. Any such nearest neighbor algorithm  
given a query r  incurs time depends on the number of points p (typically sublinearly)  their dimen(cid:173)
sion n  and the approxinlation factor (1 + f=). Let us denote this cost by C (n p  f=). 
From our analysis of multiplicative approxinlate greedy (see Lemma 3)  given a multiplicative ap(cid:173)
proximation factor (1 + f=) in the approximate nearest neighbor method  the approximate greedy 
. 1\:1   3 for some constant K > O. Thus  the num-
coordinate descent has the convergence rate: K 
'-
ber of iterations required to obtain a solution with accuracy fopt is given by  T greedy = ~:~. 
Since each of these greedy steps have cost C (n p  f=)  the overall cost CG is given as: CG = 
C  (n  p  f=) . ! "::~. Of course these approxinlate nearest neighbor methods also require some 
pre-processing time C _ (p  n  f=)  but this can typically be amortized across multiple runs of the 
optimization problem with the same covariates (for a regularization path for instance). It could also 
be reused across different models  and for other forms of data analysis. Examples include: 
(a). Locality Sensitive Hashing [12] uses random shifting windows and random projections to hash 
the data points such that distant points do not collide with high probability. Let p = 1/(1 + f=) < 
1. Then here  C_(p n f=) = 0 (np1+p. ;; 2) while C (n p f=) = O(npp). Thus  for sparse 
solutions B = o(y'P)  the runtime cost scales as CG = 0 (npp. ;; 'f;;pi). 
(b). Allon and Chazelle [1] use multiple lookup tables after random projections to obtain a nearest 
neighbor data structore with costs and C_(p n f=) = O(P  ;;')  and C (p n f=) = O(nlogn + 
. ;; 3 log" p). Thus the runtime cost here scales as CG = 0 (nlogn.:::-;;; 10" p) . 
In Section 4  we showed that when the covariates are mutually incoherent  then we can use a 
(0). 
simple quadtree  and random Gaussian projections to obtain C_(P  n  f=) = O(np logp. ;; 2) and 
C (p  n  f=) = O(P  ;;'). Thus the runtime cost here scales as CG = 0 (p'';;' f;if';;") . 

6 Non-Smooth Objectives 
Now we consider the more general composite objective case where the objective is the sum of a 
differentiable  and a separable non-differentiable function: 
min C(w) + :R.(w)   

(7) 

wERp 

where we assume C is convex and differentiable and satisfies the Lipshitz condition in Assump(cid:173)
tion AI  and :R.(w) = L; :R.;(w;) where:R.; : IR >-+ IR could be non-differentiable. Again  we 
assume that Assumption 2 holds. The natursl counterpart of the greedy algorithms in the previ(cid:173)
ous sections would be to pick the coordinate with the maximum absolute value of the subgradient. 
However  we did not observe good performance for this variant either theoretically or in simula(cid:173)
tions. Thus  we now stody a lookahead variant that picks the coordinate with the maximum absolute 
value of the sum of the gradient of the smooth component and the subgradient of the non-smooth 
component at the next iterate. 
Denote [V'C(w')]; by 0;  and compute the next iterate w~+' as argmiIlw g;(w - w;) + T(w(cid:173)
W;)2 + R;(w). Let p; = 8R;(wJ+1) denote the subgradient at this next iterate  and let 

(8) 

6 

Algorithm 2 A Greedy Coordinate Descent Algorithm for Composite Objectives 
1: Initialize: W O +- 0 
2: fort ~ 1 2 3  ... do 
3: 
4' wt + 1 +- wt +n~ e· 
'IJt 3t' 
• 
5: end for 

j  +- argmax;EiPlll1jl (withl1j as defined in (8)) 

Then pick the coordinate as argmax;EiPlll1j I. The next lemma states that this variant performs 
qualitatively similar to its smooth counterpart in Algorithm 1. 
Lemma 8. The greedy coordinate descent iterates of Algorithm 2 satisfY: 
o ~ w'II~. 

C(w') +R(w') _ C(w') _ R(w') :;; ~' Ilw

The greedy step for composite objectives in Algorithm 2 at any iteration t entails solving the max(cid:173)
imization problem: max; 111; I  where 11; is as defined in (8). Let us focus on the case where the 
regularizer R is the i  norm  so that R(w) ~ >'L~~1Iw;l  for some>. > O. Using the no(cid:173)
tation from above  we thus have the following objective: min"  ~ L~~1 i(WTXi Yi) + >.llwI11. 
(x;  r(w')) /It } - w;  where 
Then 11; from (8) can be writteu in this case as: 11; ~ 8  "  (w; -
8 r (u) ~ sign(u)max{lul- r O} is the soft-thresholding function. So the greedy step reduces 
to maximizing max; 18  <. (wj -
(x;  r(w')) /1t1) - wj over j. The next lemma shows that by 
focusing the maximization on the inner products (x;  r(w)) we lose at most a factor of > /" : 
Lemma9. I (x; r(w')) /" 1-111;11:;; >'/"" 
The Lemma in tum implies that if j' E argmax;EiPl I (x;  r(w')) /"11  then 

111;  I :;; I (x;" r( w') / 1t11 + >./ 1t1 ~ ;' W; j I (x;  r( w') / 1t11 + >./ 1t1 :;; ~'f"j 111; I + 2>'/"" 

Typical setting of>. for statistical estimation is at the level of the statistical precision of the problem 
(and indeed of the order of 0(1/ v'ii) even for low-dimensional problems). Thus  as in the previous 
section  we estimate the coordinate j that maximizes the inner product I (x;  r(w)) I  which in tum 
can be approximated using approximate nearest neighbor search. So  even for composite objectives  
we can reduce the greedy step to performing a nearest neighbor search. Note however that this can be 
performed sublinearly only at the cost of recovering an approximate nearest neighbor. Note that this 
in tum entails that we wonld be performing each greedy step in coordinate descent approximately. 
7 Experimental Results 
We conducted speed trials in MATLAB comparing 3 algorithms: greedy (Algorithm 2)  greedy.LSH 
(coordinate to update chosen by LSH) and cyclic on i   -regularized problems: L~~1 i(wT Xi  Yi) + 
>.llwl11 wherei(y  t) was either (y_t)2 /2 (squared loss) or 10g(1 +exp( -ty)) (logistic loss) and we 
chose >. ~ 0.01. All these algorithms  after selecting a coordinate to update  minimize the function 
fully along that coordinate. For squared loss  this minimum can be obtained in closed form while 
for logistic we performed 6 steps of the (I-dimensional) Newton method. The data was generated 
as follows: a matrix X E !RH " was chosen with i.i.d. standard normal entries aod the each column 
was normalized to i 2-norm 1. Then  we set Y ~ X w" for a k-sparse vector w" E !R" (with non(cid:173)
zero entries placed raodomly). The labels Yi were chosen to be either Yi or sigu(Yi) depending on 
whether the squared or logistic loss was being optimized. The rows of X became the instances Xi. 

Figure I shows the objective function value versus CPU time plots for the logistic loss with p ~ 
10' 105  106 . As p grows we keep k ~ 100 constant aod scale n as L4klog(P)J. In this case  
greedy.LSH not only speeds up naive greedy significaotly but also beats cyclic coordinate descent 
In fact  cyclic appears to be stalled especially for p ~ 105  106 • The reason for this is that cyclic  
in the time allotted  was only able to complete 52% 40% aod 27% of a full sweep through the 
p coordinates for p ~ 10' 105 aod 106 respectively. Furthermore  cyclic had generated far less 
sparse final iterates than greedy.LSH in all 3 cases. Figure 2 shows the same plots but for squared 
loss. Here  since each coordinate minimization is closed form aod thus very quick  greedy.LSH 
has a harder time competing with it. Greedy.LSH is still way faster thao naive greedy aod start 
to beat cyclic at p ~ 106• The trend of greedy.LSH catching up with cyclic as p grows is clearly 

7 

. - . p"10000  .. '00 (\oG""_) 

I=-  --S·L51 

-- G 

10 
~11 . . ~"_) 

20 

15 

25 

30 

SOD 

1000 
2000 
CPI.Inno~"~ 

1500 

250CI 

3ODO 

Figure 1: (best viewed in color) Objective VB. CPU time plots for logistic loss using p = 104  105

  106 

" . " . " 

cpunno~n_) 

" . 

" . -- - - - -

CPUr_(i'I ......... j 

Figure 2: (best viewed in color) Objective vs. CPU time plots for squared loss using p = 104

  10' 106 

demonstrated by these plots. In contrast with the logistic case  here cyclic as able to finish several 
full sweeps through the p coordinate  namely 13.4 10.5 and 7.9 sweeps for p = 104  105 and 106 
respectively. even though cyclic got lower objective values  it was at the expense of sparsity: cylic's 
final iterates were usually 10 times denser than those of greedy.LSH. 

Figure 3 shows the plots for the objective versus number of coordinate descent steps. We clearly see 
that cyclic is wasteful in terms of number of coordinate updates and greedy achieves much greater 
descent in the objective per coordinate update. Moreover  greedy.LSH is much closer to greedy in 
its per coordinate-update performance (to the extent that it is hard to tell them apart in some of these 
plots). This plot thus suggests the improvements possible with better nearest-neighbor implementa(cid:173)
tions that perform the greedy step even faster than our non-optimized greedy.LSH implementation. 
Cyclic coordinate descent is one of the most competitive methods for large scale i 1 -regularized 
problems [9]. We are able to outperform it for large problems using a homegrown implementation 
that was not optimized for performance. This provides strong reasons to believe that with a careful 
well-toned LSH implementation  and indeed with better data structures than LSH  nearest neighbor 
based greedy methods should be able to scale to problems beyond the reach of current methods. 

Acknowledgments 

We gratefully acknowledge the support of NSF under grants IIS-1018426 & CCF-0728879. ISD 
acknowledges support from the Moncrief Grand Challenge Award. 

- . 1"'10011C1  t-l00 ( ... __ ) 

1m::  1 -_a 
:I 
:r 
• 
.1 " " 
• 
I: 
I· " 
• 
0 • 
" • .. .. .. -  ~ 
" • 

Num ..... ot _ _ _ IIo .. 

" -'-. 

..-..eoi  pol000DD  1<-100 <-cI- _) 

1=:='"1 J 
.'  
I" "'" 

_ . p-l0D00D0  k-1OD~_} 

1====·LS1 

2 

10 

NUrrDI''''_____ ~1a' 

e 

S 

4 

Figure 3: (best viewed in color) Objective vs. no. of coordinate updates: squared loss using p = 10'  105

  106 

8 

References 
[1] N. Ailon and B. Chazelle. Approximate nearest neighbors and the fastjohnson-lindenstrauss 

transform. In Proc. 38th STOC  pages 557-563  2006. 

[2] S. Arya and D. M. Mount. Approximate nearest neighbor queries in fixed dimensions. In Proc. 

4th ACM-SIAM SODA  pages 271-280  1993. 

[3] S. Arya  T. Malamatos  aod D. M. Mount. Space-time tradeoffs for approximate nearest neigh(cid:173)

bor searching. Journal of the ACM  57(1)  2009. 

[4] D.P. Bertsekas. Nonlinear programming. Athena Scientific  Behnont  MA  1995. 
[5] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than 

n. Annals of Statistics  2006. 

[6] Y. Censor and S. A. Zenios. Parallel optimization: Theory  algorithms  and applications. 

Oxford University Press  1997. 

[7] 1. Daubechies  M. Defrise  and C. De Mol. Ao iterative thresholding algorithm for linear 
inverse problems with a sparsity constraint. Comm. Pure Appl. Math.  57(11):1413-1457  
2004. 

[8] J. Friedman  T. Hastie  H. Holling  and R. Tibshirani. Pathwise coordinate optimization. 2007. 
[9] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Regularization paths for generalized 

linear models via coordinate descent Journal of Statistical Software  33(1): 1-22  20 I O. 

[10] A. Genkin  D. D. Lewis  and D. Madigan. Large-scale bayesian logistic regression for text 

categorization. Technometrics  49(3):291-304  2007. 

[11] S. Har-Peled. Lectures notes on geometric approximation algorithms. 

2009. URI. 

http://valis.cs.uiuc.edu/-sariel/teach/notes/aprx/lec/. 

[12] P. Indyk aod R. Motwani. Approximate nearest neighbors: towards removing the curse of 

dimensionality. In Proc. 30th STOC  pages 604-613 1998. 

[13] A. Saba and A. Tewari. On the finite time convergence of cyclic coordinate descent methods. 

preprint  2010. 

[14] S. Shalev-Shwartz and A. Tewari. Stochastic methods for i  regularized loss minimization. In 

ICML 2009. 

[15] J. Tropp. Just relax: Convex programming methods for identifYing sparse signals in noise. 

IEEE 17ans. Info Theory  52(3): I 030-1051  March 2006. 

[16] P. Tseng and S. Yun. A block-coordinate gradient descent method for linearly constrained 
nonsmooth separable optimization. Journal of Optimization Theory and Applications  140(3): 
513-535  . 

[17] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable mini(cid:173)

mization. Math. Prog. E  117:387-423  . 

[18] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using 
i -constrained quadratic programming (lasso). IEEE Transactions on l1ifo. Theory  55:2183-
2202 2009. 

[19] T. T. Wu and K. Laoge. Coordinate descent algorithms for lasso penalized regression. Annals 

of Applied Statistics  2:224-244  2008. 

9 

,Alkis Gotovos
Hamed Hassani
Andreas Krause
Xenia Miscouridou
Francois Caron
Yee Whye Teh