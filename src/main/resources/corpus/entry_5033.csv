2013,Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors,In this work  we propose a general method for recovering low-rank three-order tensors  in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent  we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efficiency  we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efficient and effective than previous work. The proposed method can be easily applied to simultaneously  rectify and align multiple images or videos frames. In this context  the state-of-the-art algorithms RASL'' and "TILT'' can be viewed as two special cases of our work  and yet each only performs part of the function of our method.",Simultaneous Rectiﬁcation and Alignment via Robust

Recovery of Low-rank Tensors

Xiaoqin Zhang  Di Wang

Institute of Intelligent System and Decision

Wenzhou University

zhangxiaoqinnan@gmail.com  wangdi@wzu.edu.cn

Zhengyuan Zhou

Department of Electrical Engineering

Stanford University

zyzhou@stanford.edu

Abstract

Yi Ma

Visual computing group
Microsoft Research Asia
mayi@microsoft.com

In this work  we propose a general method for recovering low-rank three-order
tensors  in which the data can be deformed by some unknown transformation and
corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are
interdependent  we introduce auxiliary variables and relax the hard equality con-
straints by the augmented Lagrange multiplier method. To improve the computa-
tional efﬁciency  we introduce a proximal gradient step to the alternating direction
minimization method. We have provided proof for the convergence of the lin-
earized version of the problem which is the inner loop of the overall algorithm.
Both simulations and experiments show that our methods are more efﬁcient and
effective than previous work. The proposed method can be easily applied to si-
multaneously rectify and align multiple images or videos frames. In this context 
the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special
cases of our work  and yet each only performs part of the function of our method.

1 Introduction

In recent years  with the advances in sensorial and information technology  massive amounts of
high-dimensional data are available to us. It has become an increasingly pressing challenge to de-
velop efﬁcient and effective computational tools that can automatically extract the hidden structures
and hence useful information from such data. Many revolutionary new tools have been developed
that enable people to recover low-dimensional structures in the form of sparse vectors or low-rank
matrices in high dimensional data. Nevertheless  instead of vectors and matrices  many practical
data are given in their natural form as higher-order tensors  such as videos  hyper-spectral images 
and 3D range data. These data are often subject to all types of geometric deformation or corruptions
due to change of viewpoints  illuminations or occlusions. The true intrinsic structures of the data
will not be fully revealed unless these nuisance factors are undone in the processing stage.
In the literature  it has been shown that for matrix data  if the data is a deformed or corrupted version
of an intrinsically low-rank matrix  one can recover the rectiﬁed low-rank structure despite different
types of deformation (linear or nonlinear) and severe corruptions. Such concepts and methods have
been successfully applied to rectify the so-called low-rank textures [1] and to align multiple corre-
lated images (such as video frames or human faces) [2  3  4  5  6]. However  when applied to the data
of higher-order tensorial form  such as videos or 3D range data  these tools are only able to harness
one type of low-dimensional structure at a time  and are not able to exploit the low-dimensional

1

tensorial structures in the data. For instance  the previous work of TILT rectiﬁes a low-rank textural
region in a single image [1] while RASL aligns multiple correlated images [6]. They are highly
complementary to each other: they exploit spatial and temporal linear correlations respectively in
a given sequence of images. A natural question arises: can we simultaneously harness all such
low-dimensional structures in an image sequence by viewing it as a three-order tensor?
Actually  many existing visual data can be naturally viewed as three-order (or even higher-order)
tensors (e.g. color images  videos  hyper-spectral images  high-dynamical range images  3D range
data etc.). Important structures or useful information will very often be lost if we process them as
a 1D signal or a 2D matrix. For tensorial data  however  one major challenge lies in an appropriate
deﬁnition of the rank of a tensor  which corresponds to the notion of intrinsic “dimension” or “degree
of freedom” for the tensorial data. Traditionally  there are two deﬁnitions of tensor rank  which are
based on PARAFAC decomposition [7] and Tucker decomposition [8] respectively. Similar to the
deﬁnition of matrix rank  the rank of a tensor based on PARAFAC decomposition is deﬁned as the
minimum number of rank-one decompositions of a given tensor. However  this deﬁnition rank is
a nonconvex and nonsmooth function on the tensor space  and direct minimization of this function
is an NP-hard problem. An alternative deﬁnition of tensor rank is based on the so-called Tucker
decomposition  which results in a vector of the ranks of a set of matrices unfolded from the tensor.
Due to the recent breakthroughs in the recovery of low-rank matrices [9]  the latter deﬁnition has
received increasing attention. Gandy et al. [10] adopt the sum of the ranks of the different unfolding
matrices as the rank of the tensor data  which is in turn approximated by the sum of their nuclear
norms. They then apply the alternating direction method (ADM) to solve the tensor completion
problem with Gaussian observation noise. Instead of directly adding up the ranks of the unfolding
matrices  a weighted sum of the ranks of the unfolding matrices is introduced by Liu et al. [12] and
they also proposed several optimization algorithms to estimate missing values for tensorial visual
data (such as color images). In [13]  three different strategies have been developed to extend the
trace-norm regularization to tensors: (1) tensors treated as matrices; (2) traditional constrained op-
timization of low rank tensors as in [12]; (3) a mixture of low-rank tensors. The above-mentioned
work all addresses the tensor completion problem in which the locations of the missing entries are
known  and moreover  observation noise is assumed to be Gaussian. However  in practice  a fraction
of the tensorial entries can be arbitrarily corrupted by some large errors  and the number and the
locations of the corrupted entries are unknown. Li et al. [14] have extended the Robust Principal
Component Analysis [9] from recovering a low-rank matrix to the tensor case. More precisely  they
have proposed a method to recover a low-rank tensor with sparse errors. However  there are two
issues that limit the practicality of such methods: (1) The tensorial data are assumed to be well
aligned and rectiﬁed. (2) The optimization method can be improved in both accuracy and efﬁciency 
which will be discussed and validated in Section 4.
Inspired by the previous work and motivated by the above observations  we propose a more general
method for the recovery of low-rank tensorial data  especially three-order tensorial data  since our
main interests are visual data. The main contributions of our work are three-fold: (1) The data sam-
ples in the tensor do not need to be well-aligned or rectiﬁed  and can be arbitrarily corrupted with a
small fraction of errors. (2) This framework can simultaneously perform rectiﬁcation and alignment
when applied to imagery data such as image sequences and video frames. In particular  existing
work of RASL and TILT can be viewed as two special cases of our method. (3) To resolve the in-
terdependence among the nuclear norms of the unfolding matrices  we introduce auxiliary variables
and relax the hard equality constraints using the augmented Lagrange multiplier method. To further
improve the efﬁciency  we introduce a proximal gradient step to the alternating direction minimiza-
tion method. The optimization is more efﬁcient and effective than the previous work [6  14]  and
the convergence (of the linearized version) is guaranteed (the proof is shown in the supplementary
material).

2 Basic Tensor Algebra
We provide a brief notational summary here. Lowercase letters (a  b  c··· ) denote scalars; bold low-
ercase (a  b  c··· ) letters denote vectors; capital letters (A  B  C ··· ) denote matrices; calligraphic
letters (A B C ··· ) denote tensors. In the following subsections  the tensor algebra and the tensor
rank are brieﬂy introduced.

2

Figure 1: Illustration of unfolding a 3-order tensor.

2.1 Tensor Algebra
We denote an N-order tensor as A ∈ RI1×I2×···×IN   where In(n = 1  2  . . .   N ) is a positive
integer. Each element in this tensor is represented as ai1···in···iN   where 1 ≤ in ≤ In. Each
∏
order of a tensor is associated with a ‘mode’. By unfolding a tensor along a mode  a tensor’s
unfolding matrix corresponding to this mode is obtained. For example  the mode-n unfolding matrix
i̸=n Ii) of A  represented as A(n) = unfoldn(A)  consists of In-dimensional mode-
A(n) ∈ RIn×(
n column vectors which are obtained by varying the nth-mode index in and keeping indices of the
other modes ﬁxed. Fig. 1 shows an illustration of unfolding a 3-order tensor. The inverse operation
of the mode-n unfolding is the mode-n folding which restores the original tensor A from the mode-n
unfolding matrix A(n)  represented as A = foldn(A(n)). The mode-n rank rn of A is deﬁned as the
rank of the mode-n unfolding matrix A(n): rn = rank(A(n)). The operation of mode-n product of a
tensor and a matrix forms a new tensor. The mode-n product of tensor A and matrix U is denoted as
A ×n U. Let matrix U ∈ RJn×In. Then  A ×n U ∈ RI1×···×In−1×Jn×In+1×···×IN and its elements
are calculated by:
∑
∑
(1)
√⟨A A⟩.
·∑
· ·
The scalar product of two tensors A and B with the dimension is deﬁned as ⟨A B⟩ =
∑
ai1···iN bi1···iN . The Frobenius norm of A ∈ RI1×I2×···×IN is deﬁned as: ||A||F =
The l0 norm ||A||0 is deﬁned to be the number of non-zero entries in A and the l1 norm ||A||1 =
| respectively. Observe that ||A||F = ||A(k)||F   ||A||0 = ||A(k)||0 and ||A||1 =
||A(k)||1 for any 1 ≤ k ≤ N.
2.2 Tensor Rank
Traditionally  there are two deﬁnitions of tensor rank  which are based on PARAFAC decomposition
[7] and Tucker decomposition [8]  respectively.
As stated in [7]  in analogy to SVD  the rank of a tensor A can be deﬁned as the minimum number
r for decomposing the tensor into rank-one components as follows:

(A ×n U )i1···in−1jnin+1···iN =

ai1···in···iN ujnin.

|ai1···iN

∑

i1;:::;iN

iN

in

i1

i2

j=1

λju(1)

A =

◦ u(2)

◦ · · · ◦ u(N )

j = D ×1 U (1) ×2 U (2) · · · ×N U (N ) 

(2)
where ◦ denotes outer product  D ∈ Rr×r×···×r is an N-order diagonal tensor whose jth diagonal
element is λj  and U (n) = [u(n)
]. The above decomposition model is called PARAFAC.
However  this rank deﬁnition is a highly nonconvex and discontinuous function on the tensor space.
In general  direct minimization of such a function is NP-hard.
Another kind of rank deﬁnition considers the mode-n rank rn of tensors  which is inspired by the
Tucker decomposition [8]. The tensor A can be decomposed as follows:

1   . . .   u(n)

r

A = G ×1 U (1) ×2 U (2) · · · ×N U (N ) 

(3)

⊤ ×2 U (2)

where G = A ×1 U (1)
is the core tensor controlling the interaction
between the N mode matrices U (1)  . . .   U (N ). In the sense of Tucker decomposition  an appropriate
deﬁnition of tensor rank should satisfy the follow condition: a low-rank tensor is a low-rank matrix
when unfolded appropriately. This means the rank of a tensor can be represented by the rank of the

⊤ · · · ×N U (N )

⊤

3

r∑

j

j

I3I1I2I2I2I2I1I3I3I1I3I3I3I2I1I3I1I2I1I1I1I3I2I2A(1)A(2)A(3)tensor’s unfolding matrices. As illustrated in [8]  the orthonormal column vectors of U (n) span the
column space of the mode-n unfolding matrix A(n)  (1 ≤ n ≤ N )  so that if U (n) ∈ RIn×rn  n =
1  . . .   N  then the rank of the mode-n unfolding matrix A(n) is rn. Accordingly  we call A a rank-
(r1  . . .   rN ) tensor. We adopt this tensor rank deﬁnition in this paper.
3 Low-rank Structure Recovery for Tensors
In this section  we ﬁrst formulate the problem of recovering low-rank tensors despite deformation
and corruption  and then introduce an iterative optimization method to solve the low-rank recovery
problem. Finally  the relationship between our work and the previous work is discussed to show
why our work can simultaneously realize rectiﬁcation and alignment.

A = L + E 

3.1 Problem Formulation
Without loss of generality  in this paper we focus on 3-order tensors to study the low-rank recovery
problem. Most practical data and applications we experiment with belong to this class of tensors.
Consider a low-rank 3-order data tensor A ∈ RI1×I2×I3. In real applications  the data are inevitably
corrupted by noise or errors. Rather than modeling the noise with a small Gaussian  we model it
with an additive sparse error term E which fulﬁlls the following conditions: (1) only a small fraction
of entries are corrupted; (2) the errors are large in magnitude; (3) the number and the location of the
corrupted data are unknown.
Based on the above assumptions  the original tensor data A can be represented as
(4)
where L is a low-rank tensor. In this paper  the notion of low-rankness will become clear once we
introduce our objective function in a few paragraphs. The ultimate goal of this work is to recover L
from the erroneous observations A.
An explicit assumption in Eq. (4) is that it requires the tensor to be well aligned. For real data
such as video and face images  the image frames (face images) should be well aligned to ensure
that the three-order tensor of the image stack to have low-rank. However  for most practical data 
precise alignments are not always guaranteed and even small misalignments will break the low-rank
structure of the data. To compensate for possible misalignments  we adopt a set of transformations
∈ Rp (p is the dimension of the transformations) which act on the two-dimensional
−1
τ
slices (matrices) of the tensor data1. Based on the set of transformations Γ = {τ1  . . .   τI3
}  Eq. (4)
1
can be changed to
(5)
where A ◦ Γ means applying the transformation τi to each matrix A(:  :  i)  i = 1  . . .   I3.
When both corruption and misalignment are modeled  the low-rank structure recovery for tensors
can be formalized as follows.
minL;E;(cid:0)

s.t. A ◦ Γ = L + E.

rank(L) + γ||E||0 

A ◦ Γ = L + E 

  . . .   τ

−1
I3

(6)

The above optimization problem is not directly tractable for the following two reasons: (1) both rank
and ℓ0-norm are nonconvex and discontinuous; (2) the equality constraint A ◦ Γ = L + E is highly
nonlinear due to the domain transformation Γ.
To relax the limitation (1)  we ﬁrst recall the tensor rank deﬁnition in Section 2.2. In our work  we
adopt the rank deﬁnition based on the Tucker decomposition which can be represented as follows:
L is a rank-(r1  r2  r3) tensor where ri is the rank of unfolding matrix L(i). In this way  tensor rank
can be converted to calculating a set of matrices’ rank. We know that the nuclear (or trace) norm is
the convex envelop of the rank of matrix: ||L(i)||∗ =
k=1 σk(L(i))  where σk(L(i)) is kth singular
value of matrix L(i). Therefore  we deﬁne the nuclear norm of a three-order tensor as follows:

∑

m

||L||∗ =

∑
(7)
i=1 αi = 1 to make the deﬁnition consistent with the form of matrix. The rank of L
We assume
is replaced by ||L||∗ to make a convex relaxation of the optimization problem. It is well know that
1In most applications  a three-order tensor can be naturally partitioned into a set of matrices (such as image

αi||L(i)||∗  N = 3.

i=1

N

frames in a video) and transformations should be applied on these matrices

N∑

4

s.t. A ◦ Γ = L + E.
(

(8)

ℓ1-norm is a good convex surrogate of the ℓ0-norm. We hence replace the ||E||0 with ||E||1 and the
optimization problem in (6) becomes

3∑

i=1

minL;E;(cid:0)

αi||L(i)||∗ + γ||E||1 

3∑

For limitation (2)  linearization with respect to the transformation Γ parameters is a popular way to
approximate the above constraint when the change in τ is small or incremental. Accordingly  the
ﬁrst-order approximation to the above problem is as follows.
s.t. A ◦ Γ + fold3

(9)
where Ji represents the Jacobian of A(:  :  i) with respect to the transformation parameters τi  and
ϵi denotes the standard basis for Rn.
3.2 Optimization Algorithm

αi||L(i)||∗ + γ||E||1 

= L + E 

n∑

minL;E;∆Γ

Ji∆Γϵiϵ

⊤
i )

i=1

i=1

⊤

(

)

Although the problem in (9) is convex  it is still difﬁcult to solve due to the interdependent nuclear
norm terms. To remove these interdependencies and to optimize these terms independently  we in-
troduce three auxiliary matrices {Mi  i = 1  2  3} to replace {L(i)  i = 1  2  3}  and the optimization
problem changes to

∑
αi||Mi||∗ + γ||E||1

(

.
= fold3

⊤)

⊤
i )

s.t. A ◦ Γ + ∆~Γ = L + E  L(i) = Mi  i = 1  2  3 

(10)

where we deﬁne ∆~Γ
To relax the above equality constraints  we apply the Augmented Lagrange Multiplier (ALM)
method [15] to the above problem  and obtain the following augmented Lagrangian function

n
i=1 Ji∆Γϵiϵ

for simplicity.

(

3∑

minL;E;∆~Γ

i=1

f(cid:22)(Mi L E  ∆~Γ Y  Qi) =

3∑
αi||Mi||∗ + γ||E||1 − ⟨Y T ⟩ +
3∑

i=1

(−⟨Qi  Oi⟩ +

||Oi||2
F ) 

1
2µi

||T ||2

F

1
2µ

+

(11)
where we deﬁne T = L + E − A ◦ Γ − ∆~Γ and Oi = L(i) − Mi. Y and Qi are the Lagrange
multiplier tensor and matrix respectively. ⟨· ·⟩ denotes the inner product of matrices or tensors. µ
and µi are positive scalars. To have fewer parameters  we set µ = µi  i = 1  2  3 and µi is replaced
by µ in the following sections including experiments and supplementary materials.
A typical iterative minimization process based on the alternating direction method of multipliers
(ADMM) [15  16] can be written explicitly as

i=1

i

[M k+1
∆~Γk+1 :
Y k+1 :
Qk+1
:

 Lk+1 E k+1] : = arg min
= arg min
= Y k − T k+1/µ;
∆~Γ
= Qk
i

− (Lk+1

i

(i)

f(cid:22)(M k+1

i

− M k+1

i

Mi;L;E f(cid:22)(Mi L E  ∆~Γk Y k  Qk

i );

 Lk+1 E k+1  ∆~Γ Y k  Qk
i );

)/µ 

i = 1  2  3.

(12)



However  minimizing the augmented Lagrangian function f(cid:22)(Mi L E  ∆~Γk Y k  Qk
i ) with respect
to Mi  L and E using ADMM is expensive in practice  and moreover  the global convergence can not
be guaranteed. Therefore  we propose to solve the above problem by taking one proximal gradient

step.

2(cid:22)(cid:28)1

(cid:13)(cid:13)(cid:13)Mi −
(
(
3∑
(cid:13)(cid:13)E −(E k − τ1
(

(cid:13)(cid:13)(cid:13)(cid:13)L −
(
αi||Mi||∗ + 1
Lk − τ1
(cid:13)(cid:13)(cid:13)∆~Γ −
(

1 + 1
2(cid:22)(cid:28)1

∆~Γk − τ2

i=1

1

M k+1

i

: = arg min
Mi

2(cid:22)(cid:28)1

Lk+1 : = arg minL
E k+1 : = arg minE γ ∥E∥
∆~Γk+1 : = arg min
∆~Γ

2(cid:22)(cid:28)2

1

)(cid:13)(cid:13)(cid:13)2

)

i

M k
i

− Lk

− τ1(M k

(Lk − foldi(M k
))(cid:13)(cid:13)2
(T k − µY k

F ;

  i = 1  2  3;

(i) + µQk
i )
+ T k − µY k

F

i + µQk
i )

∆~Γk − Lk+1 − E k+1 + A ◦ Γ + µY k

;

))(cid:13)(cid:13)(cid:13)(cid:13)2
))(cid:13)(cid:13)(cid:13)2

F

F

.
(13)

In detail  the solutions of each term are obtained as follows.

5

M k+1

− Lk

= UiD(cid:11)i(cid:22)(cid:28)1 (Λ)V
(i) + µQk

i ) and D(cid:21)(·) is the shrinkage operator:

⊤
i

 

i

:

• For term M k+1
⊤
i = M k
i

− τ1(M k
where UiΛV
D(cid:21)(x) = sgn(x) max(|x| − λ  0).
( 3∑
• For term Lk+1:

i

i

Lk+1 = Lk − τ1

(Lk − foldi(M k
(E k − τ1
(
E k+1 = D(cid:13)(cid:22)(cid:28)1
∆~Γk − Lk+1 − E k+1 + A ◦ Γ + µY k
n∑
Here  ∆~Γk+1 is a tensor  we can transform it to its original form as follows.

)
(T k − µY k

+ T k − µY k
))
)

• For term E k+1:
• For term ∆~Γk+1:

∆~Γk+1 = ∆~Γk − τ2

i + µQk
i )

i=1

.

)

.

.

∆Γk+1 =

⊤
i (∆~Γk+1)
J +
(3)ϵiϵ

⊤
i  

i=1

is pseudo-inverse of Ji and (∆~Γk+1)(3) is the mode-3 unfolding

i = (J

⊤
i Ji)

−1J
where J +
matrix of tensor ∆~Γk+1.
• For terms Y k+1 and Qk+1

⊤
i

:

i

Y k+1 = Y k − T k+1/µ; Qk+1

i = Qk
i

− (Lk+1

(i)

− M k+1

i

)/µ 

i = 1  2  3.

The global convergence of the above optimization process is guaranteed by the following theorem.
Theorem 1 The sequence {M k
i   i = 1  2  3} generated by the above proxi-
mal gradient descent scheme with τ1 < 1/5 and τ2 < 1 converges to the optimal solution to Problem
(10).

i  Lk E k  ∆~Γk Y k  Qk

Proof. The proof of convergence can be found in the supplementary material.
As we see in Eq. (10)  the optimization problem is similar to the problems addressed in [6  1].
However  the proposed work differs from these earlier work in the following respects:

1. RASL and TILT can be viewed as two special cases of our work. Consider the mode-
3 unfolding matrix A(3) in the bottom row of Fig. 1. Suppose the tensor is formed by
stacking a set of images along the third mode. Setting α1 = 0  α2 = 0 and α3 = 1 
our method reduces to RASL. While for the mode-1 and mode-2 unfolding matrices (see
Fig. 1)  if we set α1 = 0.5  α2 = 0.5 and α3 = 0  our method reduces to TILT. In this
sense  our formulation is more general as it tends to simultaneously perform rectiﬁcation
and alignment.

2. Our work vs. RASL: In the image alignment applications  RASL treats each image as a
vector and does not make use of any spatial structure within each image. In contrast  as
shown in Fig. 1  in our work  the low-rank constraint on the mode-1 and mode-2 unfolding
matrices effectively harnesses the spatial structures within images.

3. Our work vs. TILT: TILT deals with only one image and harnesses spatial low-rank struc-
tures to rectify the image. However  TILT ignores the temporal correlation among multiple
images. Our work combines the merits of RASL and TILT  and thus can extract more
structural information in the visual data.

4 Experimental Results
In this section  we compare the proposed algorithm with two algorithms: RASL [6] and Li’s work
[14] (TILT [1] is not adopted for comparison because it can deal with only one sample). We choose
them for comparison because: (1) They represent the latest work that address similar problems as
ours. (2) The effectiveness and efﬁciency of our optimization method for recovery of low-rank ten-
sors can be validated by comparing our work with RASL and Li’s work; These algorithms are tested
with several synthetic and real-world datasets  and the results are both qualitatively and quantita-
tively analyzed.

6

Figure 2: Results on synthetic data.

(a) original data

(b) RASL

(c) Li’s work
Figure 3: Results on the ﬁrst data set.

(d) Our work

||Lo−Lr||F

Results on Synthetic Data. This part tests the above three algorithms with synthetic data. To
make a fair comparison  some implementation details are clariﬁed as follows: (1) Since domain
transformations are not considered in Li’s work  we assume the synthetic data are well aligned.
(2) To eliminate the inﬂuence of different optimization methods  RASL is implemented with the
following four optimization methods: APG (Accelerated Proximal Gradient)  APGP (Accelerated
Proximal Gradient with partial SVDs)  ALM (Augmented Lagrange Multiplier) and IALM (Inexact
Augmented Lagrange Multiplier)2. Moreover  since RASL is applied to one mode of the tensor  to
make it more competitive  we apply RASL to each mode of the tensor and take the mode that has
the minimal reconstruction error.
For synthetic data  we ﬁrst randomly generate two data tensors: (1) a pure low-rank tensor Lo ∈
R50×50×50 whose rank is (10 10 10); (2) an error tensor E ∈ R50×50×50 in which only a fraction c
of entries are non-zero (To ensure the error to be sparse  the maximal value of c is set to 40%). Then
the testing tensor A can be obtained as A = Lo + E. All the above three algorithms are applied to
recover the low-rank structure of A  which is represented as Lr. Therefore  the reconstruction error
is deﬁned as error =
. The result of a single run is a random variable  because the data
are randomly generated  so the experiment is repeated 50 times to generate statistical averages.
The left column of Fig. 2 shows the reconstruction error  from which we can see that our work
can achieve the most accurate result of reconstruction among all the algorithms. Even when 40% of
entries are corrupted  the reconstruction error of our work is about 0.08. As shown in right column of
Fig. 2  comparing with Li’s work and RASL+ALM  our work can achieve about 3-4 times speed-up.
Moveover  the result shows that the average running time of our work is higher than RASL+APG 
RASL+APGP and RASL+IALM. However  these three methods only optimize on a single mode
while our work optimize on all three modes and the variables evolved in (10) are about three times
of those in RASL. The above results demonstrate the effectiveness and efﬁciency of our proposed
optimization method for low-rank tensor recovery.
Results on Real World Data.
In this part  we apply all three algorithms (RASL here is solved
by ALM which gives the best results) to several real-world datasets. The ﬁrst dataset contains 16
images of the side of a building  taken from various viewpoints by a perspective camera  and with
various occlusions due to tree branches. Fig. 3 illustrates the low-rank recovery results on this data
set  in which Fig. 3(a) shows the original image and Fig. 3(b)-(d) show the results of the three
algorithms. Compared with RASL  we can see that our work and Li’s work not only remove the

||Lo||F

2For more detail  please refer to http://perception.csl.illinois.edu/matrix-rank/sample code.html

7

0510152025303540−0.0500.050.10.150.20.250.3c(%)Error  Li’s workRASL+APGRASL+APGPRASL+ALMRASL+IALMOur work05101520253035400102030405060c(%)Running time (second)  Li’s workRASL+APGRASL+APGPRASL+ALMRASL+IALMOur work(a) original data

(b) RASL

(c) Li’s work

(d) Our work

Figure 4: Results on the second data set.

(a) original data

(b) RASL

(c) Li’s work
Figure 5: Results on the third data set.

(d) Our work

branches from the windows  but also rectiﬁy window position. Moreover  the result obtained by our
work is noticeably sharper than Li’s work.
The second data set contains 100 images of the handwritten number “3”  with a fair amount of
diversity. For example  as shown in Fig. 4(a)  the number “3” in the column 1 and row 6 is barely
recognizable. The results of the three algorithms on this dataset are shown in Fig. 4(b)-(d). We
can see that our work has achieved better performance than the other two algorithms from human’s
perception  in which the 3’s are more clear and their poses are upright.
The third data set contains 140 frames of a video showing Al Gore talking. As shown in Fig. 5 
the face alignment results obtained by our work is signiﬁcantly better than those obtained by the
other two algorithms. The reason is that human face has a rich spatial low-rank structures due to
symmetry  and our method simultaneously harnesses both temporal and spatial low-rank structures
for rectiﬁcation and alignment.
5 Conclusion
We have in this paper proposed a general low-rank recovery framework for arbitrary tensor data 
which can simultaneously realize rectiﬁcation and alignment. We have adopted a proximal gradi-
ent based alternating direction method to solve the optimization problem  and have shown that the
convergence of our algorithm is guaranteed. By comparing our work with the three state-of-the-art
work through extensive simulations  we have demonstrated the effectiveness and efﬁciency of our
method.
6 Acknowledgment
This work is partly supported by NSFC (Grant Nos.
61100147  61203241 and 61305035) 
Zhejiang Provincial Natural Science Foundation (Grants Nos. LY12F03016  LQ12F03004 and
LQ13F030009).

8

References
[1] Z. Zhang  A. Ganesh  X. Liang  and Y. Ma  “TILT: Transform-Invariant Low-rank Textures”  Interna-

tional Journal of Computer Vision  99(1): 1-24  2012.

[2] G. Huang  V. Jain  and E. Learned-Miller  “Unsupervised joint alignment of complex images”  Interna-

tional Conference on Computer Vision pp. 1-8  2007.

[3] E. Learned-Miller  “Data Driven Image Models Through Continuous Joint Alignment”  IEEE Trans. on

Pattern Analysis and Machine Intelligence  28(2):236C250  2006.

[4] M. Cox  S. Lucey  S. Sridharan  and J. Cohn  “Least Squares Congealing for Unsupervised Alignment of

Images”  International Conference on Computer Vision and Pattern Recognition  pp. 1-8  2008.

[5] A. Vedaldi  G. Guidi  and S. Soatto  “Joint Alignment Up to (Lossy) Transforamtions”  International

Conference on Computer Vision and Pattern Recognition  pp. 1-8  2008.

[6] Y. Peng  A. Ganesh  J. Wright  W. Xu  and Y. Ma  “RASL: Robust Alignment by Sparse and Low-
rank Decomposition for Linearly Correlated Images”  IEEE Trans. on Pattern Analysis and Machine
Intelligence  34(11): 2233-2246  2012.

[7] J. Kruskal  “Three-way arrays: rank and uniqueness of trilinear decompositions  with application to

arithmetic complexity and statistics”  Linear Algebra and its Applications  18(2): 95-138  1977.

[8] T. Kolda and B. Bader  “Tensor decompositions and applications”  SIAM Review  51(3): 455-500  2009.
[9] E. Candes  X. Li  Y. Ma  and J. Wright  “Robust principal component analysis?”  Journal of the ACM 

2011.

[10] S. Gandy  B. Recht  and I. Yamada  “Tensor Completion and Low- N-Rank Tensor Recovery via Convex

Optimization”  Inverse Problem  2011.

[11] M. Signoretto  L. Lathauwer  and J. Suykens  “Nuclear Norms for Tensors and Their Use for Convex

Multilinear Estimation”  Linear Algebra and Its Applications  2010.

[12] J. Liu  P. Musialski  P. Wonka  and J. Ye  “Tensor Completion for Estimating Missing Values in Visual

Data”  IEEE Trans. on Pattern Analysis and Machine Intelligence  35(1): 208-220  2013.

[13] R. Tomioka  K. Hayashi  and H. Kashima  “Estimation of low-rank tensors via convex optimization” 

Technical report  arXiv:1010.0789  2011.

[14] Y. Li  J. Yan  Y. Zhou  and J. Yang  “Optimum Subspace Learning and Error Correction for Tensors” 

European Conference on Computer Vision  pp. 790-803  2010.

[15] Z. Lin  M. Chen  L. Wu  and Y. Ma  “The augmented lagrange multiplier method for exact recovery of

corrupted low-rank matrices”  Technical Report UILU-ENG-09-2215  UIUC Technical Report  2009.

[16] J. Yang and X. Yuan  “Linearized augmented lagrangian and alternating direction methods for nuclear

norm minimization”  Mathematics of Computation  82(281): 301-329  2013.

9

,Xiaoqin Zhang
Di Wang
Zhengyuan Zhou
Yi Ma
Balamurugan Palaniappan
Francis Bach