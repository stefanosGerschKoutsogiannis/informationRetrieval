2019,MaxGap Bandit: Adaptive Algorithms for Approximate Ranking,This paper studies the problem of adaptively sampling from K distributions (arms) in order to identify the largest gap between any two adjacent means. We call this the MaxGap-bandit problem. This problem arises naturally in approximate ranking  noisy sorting  outlier detection  and top-arm identification in bandits.  The key novelty of the MaxGap bandit problem is that it aims to adaptively determine the natural partitioning of the distributions into a subset with larger means and a subset with smaller means  where the split is determined by the largest gap rather than a pre-specified rank or threshold. Estimating an arm’s gap requires sampling its neighboring arms in addition to itself  and this dependence results in a novel hardness parameter that characterizes the sample complexity of the problem. We propose elimination and UCB-style algorithms and show that they are minimax optimal. Our experiments show that the UCB-style algorithms require 6-8x fewer samples than non-adaptive sampling to achieve the same error.,MaxGap Bandit: Adaptive Algorithms for

Approximate Ranking

Sumeet Katariya ⇤

UW-Madison and Amazon
sumeetsk@gmail.com

Ardhendu Tripathy ⇤

UW-Madison

astripathy@wisc.edu

Robert Nowak
UW-Madison

rdnowak@wisc.edu

Abstract

This paper studies the problem of adaptively sampling from K distributions (arms)
in order to identify the largest gap between any two adjacent means. We call
this the MaxGap-bandit problem. This problem arises naturally in approximate
ranking  noisy sorting  outlier detection  and top-arm identiﬁcation in bandits. The
key novelty of the MaxGap bandit problem is that it aims to adaptively determine
the natural partitioning of the distributions into a subset with larger means and a
subset with smaller means  where the split is determined by the largest gap rather
than a pre-speciﬁed rank or threshold. Estimating an arm’s gap requires sampling
its neighboring arms in addition to itself  and this dependence results in a novel
hardness parameter that characterizes the sample complexity of the problem. We
propose elimination and UCB-style algorithms and show that they are minimax
optimal. Our experiments show that the UCB-style algorithms require 6-8x fewer
samples than non-adaptive sampling to achieve the same error.

1

Introduction

Consider an algorithm that can draw i.i.d. samples from K unknown distributions. The goal is
to partially rank the distributions according to their (unknown) means. This model encompasses
many problems including best-arms identiﬁcation (BAI) in multi-armed bandits  noisy sorting and
ranking  and outlier detection. Partial ranking is often preferred to complete ranking because correctly
ordering distributions with nearly equal means is an expensive task (in terms of number of required
samples). Moreover  in many applications it is arguably unnecessary to resolve the order of such
close distributions. This observation motivates algorithms that aim to recover a partial ordering of
groups of distributions having similar means. This entails identifying large “gaps” in the ordered
sequence of means. The focus of this paper is the fundamental problem of ﬁnding the largest gap by
sampling adaptively. Identiﬁcation of the largest gap separates the distributions into two groups  and
recursive application can identify any desired number of groupings in a partial order.
As an illustration  consider a subset of images from the Chicago streetview dataset [17] shown in
Fig. 1. In this study  people were asked to judge how safe each scene looks [18]  and a larger mean
indicates a safer looking scene. While each person has a different sense of how safe an image looks 
when aggregated there are clear trends in the safety scores (denoted by µ(i)) of the images. Fig. 1
schematically shows the distribution of scores given by people as a bell curve below each image.
Assuming the sample means are close to their true means  one can nominally classify them as ‘safe’ 
‘maybe unsafe’ and ‘unsafe’ as indicated in Fig. 1. Here we have implicitly used the large gaps
µ(2) µ(3) and µ(4) µ(5) to mark the boundaries. Note that ﬁnding the safest image (BAI) is hard as
we need a lot of human responses to decide the larger mean between the two rightmost distributions;
it is also arguably unnecessary. A common way to address this problem is to specify a tolerance ✏ [7] 

⇤Authors contributed equally and are listed alphabetically.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

µ(6)

µ(5)

unsafe

µ(3)
µ(4)
maybe unsafe

large gap

µ(1)

µ(2)
safe

Figure 1: Six representative images from Chicago streetview dataset and their safety (Borda) scores.

and stop sampling if the means are less than ✏ apart; however determining this can require ⌦(1/✏2)
samples. Distinguishing the top 2 distributions from the rest is easy and can be efﬁciently done
using top-m arm identiﬁcation [15]  however this requires the experimenter to prescribe the location
m = 2 where a large gap exists which is unknown. Automatically identifying natural splits in the
set of distributions is the aim of the new theory and algorithms we propose. We call this problem of
adaptive sampling to ﬁnd the largest gap the MaxGap-bandit problem.

1.1 Notation and Problem Statement

We will use multi-armed bandit terminology and notation throughout the paper. The K distributions
will be called arms and drawing a sample from a distribution will be refered to as sampling the arm.
Let µi 2 R denote the mean of the i-th arm  i 2{ 1  2  . . .   K} =: [K]. We add a parenthesis around
the subscript j to indicate the j-th largest mean  i.e.  µ(K)  µ(K1) ··· µ(1). For the i-th arm 
we deﬁne its gap i to be the maximum of its left and right gaps  i.e. 
(1)
We deﬁne µ(0) = 1 and µ(K+1) = 1 to account for the fact that extreme arms have only one gap.
The goal of the MaxGap-bandit problem is to (adaptively) sample the arms and return two clusters

i = max{µ(`)  µ(`+1)   µ(`1)  µ(`)} where µi = µ(`).

and C2 = {(m + 1)  . . .   (K)} 
where m is the rank of the arm with the largest gap between adjacent means  i.e. 

C1 = {(1)  (2)  . . .   (m)}

m = arg max
j2[K1]

µ(j)  µ(j+1).

(2)

The mean values are unknown as is the ordering of the arms according to their means. A solution to
the MaxGap-bandit problem is an algorithm which given a probability of error > 0  samples the

arms and upon stopping partitions [K] into two clusters bC1 and bC2 such that

(3)
This setting is known as the ﬁxed-conﬁdence setting [10]  and the goal is to achieve the probably
correct clustering using as few samples as possible. In the sequel  we assume that m is uniquely
deﬁned and let max = i⇤ where µi⇤ = µ(m).

P(bC1 6= C1)  .

1.2 Comparison to a Naive Algorithm: Sort then search for MaxGap

2 gaps since the MaxGap-bandit problem
The MaxGap-bandit problem is not equivalent to BAI onK
requires identifying the largest gap between adjacent arm means (BAI onK
2 gaps would always
identify µ(1)  µ(K) as the largest gap). This suggests a naive two-step algorithm: we ﬁrst sample
the arms enough number of times so as to identify all pairs of adjacent arms (i.e.  we sort the arms
according to their means)  and then run a BAI bandit algorithm [13] on the (K  1) gaps between
adjacent arms to identify the largest gap (an unbiased sample of the gap can be obtained by taking the
difference of the samples of the two arms forming the gap).
We analyze the sample complexity of this naive algorithm in Appendix A   and discuss the results
here for an example. Consider the arrangement of means shown in Fig. 2 where there is one large gap
max and all the other gaps are equal to min ⌧ max. The naive algorithm’s sample complexity is
⌦(K/2

min)  as the ﬁrst sorting step requires these many samples  which can be very large.

2

Is this sorting of the arm means necessary? For in-
stance  we do not need to sort K real numbers in
order to cluster them according to the largest gap 1.
The algorithms we propose in this paper solve the
MaxGap-bandit problem without necessarily sorting
the arm means. For the conﬁguration in Fig. 2 they require ˜O(K/2
approximately (max/min)2 samples.
The analysis of our algorithms suggests a novel hardness parameter for the MaxGap-bandit problem
that we discuss next. We let i j := µj  µi for all i  j 2 [K]. We show in Section 5 that the number
of samples taken from distribution i due to its right gap is inversely proportional to the square of
(4)

Figure 2: Arm means with one large gap.

max) samples  giving a saving of

:= max

r
i

j:i j >0

mini j   max  i j .

i analogously. The total number of samples drawn from distribution
For the left gap of i we deﬁne l
i}. The intuition for Eq. (4) is that
i is inversely proportional to the square of i := min{r
distribution i can be eliminated quickly if there is another distribution j that has a moderately large
gap from i (so that this gap can be quickly detected)  but not too large (so that the gap is easy to
distinguish from max)  and (4) chooses the best j. We discuss (4) in detail in Section 5  where we

show that our algorithms use eOPi2[K]/{(m) (m+1)} 2

gap with probability at least 1  . This sample complexity is minimax optimal.
1.3 Summary of Main Results and Paper Organization

log(K/i) samples to ﬁnd the largest

i   l

i

In addition to motivating and formulating the MaxGap-bandit problem  we make the following
contributions. First  we design elimination and UCB-style algorithms as solutions to the MaxGap-
bandit problem that do not require sorting the arm means (Section 3). These algorithms require
computing upper bounds on the gaps i  which can be formulated as a mixed integer optimization
problem. We design a computationally efﬁcient dynamic programming subroutine to solve this
optimization problem and this is our second contribution (Section 4). Third  we analyze the sample
complexity of our proposed algorithms  and discover a novel problem-hardness parameter (Section 5).
This parameter arises because of the arm interactions in the MaxGap-bandit problem where  in order
to reduce uncertainty in the value of an arm’s gap  we not only need to sample the said arm but also
its neighboring arms. Fourth  we show that this sample complexity is minimax optimal (Section 6).
Finally  we evaluate the empirical performance of our algorithms on simulated and real datasets and
observe that they require 6-8x fewer samples than non-adaptive sampling to achieve the same error
(Section 7).

2 Related Work

One line of related research is best-arm identiﬁcation (BAI) in multi-armed bandits. A typical goal
in this setting is to identify the top-m arms with largest means  where m is a prespeciﬁed number
[15  16  1  3  9  4  14  7  20]. As explained in Section 1  our motivation behind formulating the
MaxGap-bandit problem is to have an adaptive algorithm which ﬁnds the “natural” set of top arms as
delineated by the largest gap in consecutive mean values. Our work can also be used to automatically
detect “outlier” arms [23].
The MaxGap-bandit problem is different from the standard multi-armed bandit because of the local
dependence of an arm’s gap on other arms. Other best-arm settings where an arm’s reward can
inform the quality of other arms include linear bandits [22] and combinatorial bandits [5  11]. In
these problems  the decision space is known to the learner  i.e.  the vectors corresponding to the
arms in linear bandits and the subsets of arms over which the objective function is to be optimized in
combinatorial bandits is known to the learner. However in our problem  we do not know the sorted
order of the arm means  i.e.  the set of all valid gaps is unknown a priori. Our problem does not
reduce to these settings.

1First ﬁnd the smallest and largest numbers  say a and b respectively. Divide the interval [a  b] into K + 1
equal-width bins and map each number to its corresponding bin  while maintaining the smallest and largest
number in each bin. Since at least one bin is empty by the pigeonhole principle  the largest gap is between two
numbers belonging to different bins. Calculate all gaps between bins and cluster based on the largest of those.

3

∆min∆maxAnother related problem is noisy sorting and ranking. Here the typical goal is to sort a list using noisy
pairwise comparisons. Our framework encompasses noisy ranking based on Borda scores [1]. The
Borda score of an item is the probability that it is ranked higher in a pairwise comparison with another
item chosen uniformly at random. In our setting  the Borda score is the mean of each distribution.
Much of the theoretical computer science literature on this topic assumes a bounded noise model
for comparisons (i.e.  comparisons are probably correct with a positive margin) [8  6  2  21]. This is
unrealistic in many real-world applications since near equals or outright ties are not uncommon. The
largest gap problem we study can be used to (partially) order items into two natural groups  one with
large means and one with small means. Previous related work considered a similar problem with
prescribed (non-adaptive) quantile groupings [18].

3 MaxGap Bandit Algorithms

We propose elimination [7] and UCB [13] style algorithms for the MaxGap-bandit problem. These
algorithms operate on the arm gaps instead of the arm means. The subroutine to construct conﬁdence
intervals on the gaps (denoted by Ua(t)) using conﬁdence intervals on the arm means (denoted
by [la(t)  ra(t)]) is described in Algorithm 4 in Section 4  and this subroutine is used by all three
algorithms described in this section.

3.1 Elimination Algorithm: MaxGapElim

At each time step  MaxGapElim (Algorithm 1) samples all arms in an active set consisting of arms a
whose gap upper bound Ua is larger than the global lower bound L on the maximum gap  and stops
when there are only two arms in the active set.

Algorithm 1 MaxGapElim
1: Initialize active set A = [K]
2: for t = 1  2  . . . do
3:
4:
5:
6:
7:

// rounds
8 a 2 A  sample arm a  compute [la(t)  ra(t)] using (5).
//arm conﬁdence intervals
8 a 2 A  compute Ua(t) using Algorithm 4.
// upper bound on arm max gap
Compute L(t) using (9).
// lower bound on max gap
8 a 2 A  if Ua(t)  L(t)  A = A \ a.
// Elimination
If |A| = 2  stop. Return clusters using max gap in the empirical means. // Stopping condition

3.2 UCB algorithms: MaxGapUCB and MaxGapTop2UCB

MaxGapUCB (Algorithm 2) is motivated from the principle of “optimism in the face of uncertainty”.
It samples all arms with the highest gap upper bound. Note that there are at least two arms with the
highest gap upper bound because any gap is shared by at least two arms (one on the right and one on
the left). The stopping condition is akin to the stopping condition in Jamieson et al. [13].

Algorithm 2 MaxGapUCB
1: Initialize U = [K].
2: for t = 1  2  . . . do
3:
4:
5:
6:

8a 2U   sample a and update [la(t)  ra(t)] using (5).
8a 2 [K]  compute Ua(t) using Algorithm 4.
Let M1(t) = maxj2[K] Uj(t). Set U = {a : Ua(t) = M1(t)}.
If 9 i  j such that Ti(t) + Tj(t)  cPa /2{i j} Ta(t)  stop.

// highest gap-UCB arms
// stopping condition

Alternatively  we can use an LUCB [16]-type algorithm that samples arms which have the two highest
gap upper bounds  and stops when the second-largest gap upper bound is smaller than the global
lower bound L(t) . We refer to this algorithm as MaxGapTop2UCB (Algorithm 3).

4

Algorithm 3 MaxGapTop2UCB
1: Initialize U1 [U 2 = [K].
2: for t = 1  2  . . . do
3:
4:
5:
6:
7:

8a 2U 1 [U 2  sample a and update [la(t)  ra(t)] using (5).
8a 2 [K]  compute Ua(t) using Algorithm 4.
Let M1(t) = maxj2[K] Uj(t). Set U1 = {a : Ua(t) = M1(t)}.
// highest gap-UCB arms
Let M2(t) = maxj2[K]\U1 Uj(t). Set U2 = {a : Ua(t) = M2(t)}. // 2nd highest gap-UCB
Compute L(t) using (9). If M2(t) < L(t)  stop.

Algorithm 4 Procedure to ﬁnd Ua(t)
1: Set P r
2: Ur

a = {i : li(t) 2 [la(t)  ra(t)]}.

a(t) = max
i2P r

a {Gr
a Gl

a(li(t)  t)}  where Gr
a(rj(t)  t)   where Gl

a(t)  Ul

a = {i : ri(t) 2 [la(t)  ra(t)]}.

3: Set P l
4: Ul
5: return Ua(t) max{Ur

a(t) = max
i2P l

a(t)}

a(x  t) is given by (7).

a(x  t) is given by (19).

// eqn. (8)

// eqn. (20)

4 Conﬁdence Bounds for Gaps

In this section we explain how to construct conﬁdence bounds for the arm gaps (denoted by Ua
and L) using conﬁdence bounds for the arm means (denoted by [la  ra]). These bounds are key
ingredients for the algorithms described in Section 3.

Given i.i.d. samples from arm a  an empirical meanbµa and conﬁdence interval on the arm mean

can be constructed using standard methods. Let Ta(t) denote the number of samples from arm a
after t time steps of the algorithm. Throughout our analysis and experimentation we use conﬁdence
intervals on the mean of the form

la(t) = ˆµa(t)  cTa(t) and ra(t) = ˆµa(t) + cTa(t)  where cs =q log(4Ks2/)

s

The conﬁdence intervals are chosen so that [12]

.

(5)

(6)

P(8 t 2 N 8 a 2 [K]  µa 2 [la(t)  ra(t)])  1  .

Conceptually  the conﬁdence intervals on the arm means can be used to construct upper conﬁdence
bounds on the mean gaps {i}i2[K] in the following manner. Consider all possible conﬁgurations
of the arm means that satisfy the conﬁdence interval constraints in (5). Each conﬁguration ﬁxes
the gaps associated with any arm a 2 [K]. Then the maximum gap value over all conﬁgurations
is the upper conﬁdence bound on arm a’s gap; we denote it as Ua. The above procedure can be
formulated as a mixed integer linear program (see Appendix B.1). In the algorithms in Section 3 
this optimization problem needs to be solved at every time t and for every arm a 2 [K] before
querying a new sample  which can be practically infeasible. In Algorithm 4  we give an efﬁcient
O(K2) time dynamic programming algorithm to compute Ua. We next explain the main ideas used
in this algorithm  and refer the reader to Appendix B.2 for the proofs.
a := µ(`)  µ(`+1)  where ` is the
Each arm a has a right and left gap  r
a := µ(`1)  µ(`) and l
a(t) and Ul
rank of a  i.e.  µa = µ(`). We construct separate upper bounds Ur
a(t) for these gaps and
a(t)}. Here we provide an intuitive description for how the
then deﬁne Ua(t) = max{Ur
a(t)  Ul
bounds are computed  focusing on Ur
a(t) as an example. To start  suppose the true mean of arm a is
known exactly  while the means of other arms are only known to lie within their conﬁdence intervals.
If there exist arms that cannot go to the left of arm a  one can see that the largest right gap for a is
obtained by placing all arms that can go to the left of a at their leftmost positions  and all remaining
arms at their rightmost positions  as shown in Fig. 3(a). If however all arms can go to the left of
arm a  the conﬁguration that gives the largest right gap for a is obtained by placing the arm with the
largest upper bound at its right boundary  and all other arms at their left boundaries  as illustrated in
Fig. 3(b). We deﬁne a function Gr
a(x  t) that takes as input a known position x for the mean of arm a

5

a

a

(a)

(b)

Figure 3: Computing maximum right gap of blue arm when its true mean is known (at position
indicated by blue x)  while the other means are known only to lie within their conﬁdence intervals.
(a) If there exist arms that cannot go to the left of blue (red  green  purple)  the largest right gap for
blue is obtained by placing all arms that can go to the left of blue at their left boundaries and the
remaining arms at their rightmost positions. (b) If all arms can go to the left of blue  the largest right
gap for blue is obtained by placing the arm with the largest right conﬁdence bound (purple) at its
right boundary and all other arms at their left boundaries.

and the conﬁdence intervals of all other arms at time t  and returns the maximum right gap for arm a
using the above idea as follows.

Gr

a(x  t) =⇢minj:lj (t)>x rj(t)  x if {j : lj(t) > x}6 = ; 

maxj6=a rj(t)  x

otherwise.

(7)

However  the true mean of arm a is not known exactly but only that it lies within its conﬁdence
interval. The insight that helps here is that Gr
a(x  t) must achieve its maximum when x is at one of
the ﬁnite locations in {lj(t) : la(t)  lj(t)  ra(t)}. We deﬁne P r
a := {j : la(t)  lj(t)  ra(t)}
as the set of arms relevant for the right gap of a  and then the maximum possible right gap of a is

Ur

a(t) = max{Gr
a can be similarly obtained. We explain this and give a proof of

a(lj(t)  t) : j 2 P r
a}.

An upper bound for the left gap Ul
correctness in Appendix B.2.
The algorithms also use a single global lower bound on the maximum gap. To do so  we sort the items
according to their empirical means  and ﬁnd partitions of items that are clearly separated in terms
of their conﬁdence intervals. At time t  let (i)t denote the arm with the ith-largest empirical mean 

i.e. bµ(K)t(t)  . . .bµ(2)t(t) bµ(1)t(t) (this can be different from the true ranking which is denoted

by (·) without the subscript t). We detect a nonzero gap at arm k if maxa2{(k+1)t ... (K)t} ra(t) <
mina2{(1)t ... (k)t} la(t). Thus  a lower bound on the largest gap is

(8)

L(t) = max

k2[K1]✓

min

a2{(1)t ... (k)t}

la(t) 

max

a2{(k+1)t ... (K)t}

ra(t)◆ .

(9)

5 Analysis

In this section  we ﬁrst state the accuracy and sample complexity guarantees for MaxGapElim and
MaxGapUCB  and then discuss our results. The proofs can be found in the Supplementary material.
Theorem 1. With probability 1    MaxGapElim  MaxGapUCB and MaxGapTop2UCB cluster the
arms according to the maximum gap  i.e.  they satisfy (3).
The number of times arm a is sampled by both the algorithms depends on a = min{l

a  r

r
a =

l
a =

max

j:0<a j <max

max

j:0<j a<max

min{a j  (max  a j)}
min{j a  (max  j a)}  .

a} where
(10)

(11)

The maxima is assumed to be 1 in (10) and (11) if there is no j that satisﬁes the constraint to account
for edge arms. The quantity a acts as a measure of hardness for arm a; Theorem 2 states that
MaxGapElim and MaxGapUCB sample arm a at most ˜O(1/2
a) number of times (up to log factors).

6

Theorem 2. With probability 1    the sample complexity of MaxGapElim and MaxGapUCB is
bounded by

O0@
Xa2[K]\{(m) (m+1)}

log(K/a)

2
a

1A

L ∆(t)

µ10

µ7

µ4

µ 11

r

U∆7(t)

a)2) (resp. O((l

Figure 4: Arm a = 7 is eliminated when a helper
arm j = 4 is found.

7(t)  r4(t)l7(t). Considering only the right gap for simplicity  as soon as Ur

Next  we provide intuition for why the sample complexity depends on the parameters in (10) and (11).
In particular  we show that O((r
a)2)) is the number of samples of a required to
rule out arm a’s right (resp. left) gap from being the largest gap.
Let us focus on the right gap for simplicity. To
understand how (10) naturally arises  consider
Fig. 4  which denotes the conﬁdence intervals
on the means at some time t. A lower bound
on the gap L(t) can be computed between the
left and right conﬁdence bounds of arms 10 and
11 respectively as shown. Consider the com-
putation of the upper bound Ur
7(t) on the right gap of arm a = 7. Arm 4 lies to the right of
arm 7 with high probability (unlike the arms with dashed conﬁdence intervals)  so the upper bound
7(t) < L(t)  arm 7
Ur
can be eliminated as a candidate for the maximum gap. Thus  an arm a is removed from consideration
as soon as we ﬁnd a helper arm j (arm 4 in Fig. 4) that satisﬁes two properties: (1) the conﬁdence in-
terval of arm j is disjoint from that of arm a  and (2) the upper bound Ur
a(t) = rj(t) la(t) < L(t).
The ﬁrst of these conditions gives rise to the term a j in (10)  and the second condition gives rise to
the term (max  a j). Since any arm j that satisﬁes these conditions can act as a helper for arm a 
we take the maximum over all arms j to yield the smallest sample complexity for arm a.
This also shows that if all arms are either very close to a or at a distance approximately max from a 
7(t) = r4(t)  l7(t) > L(t) and arm 7 cannot be eliminated. Thus arm a
then the upper bound Ur
could have a small gap with respect to its adjacent arms  but if there is a large gap in the vicinity of
arm a  it cannot be eliminated quickly. This illustrates that the maximum gap identiﬁcation problem
is not equivalent to best-arm identiﬁcation (BAI) on gaps. Section 6 formalizes this intuition.
Key Differences compared to BAI Analysis: The analysis of MaxGapUCB is very different from
the standard UCB analysis. On a high-level  in BAI  the number of samples of a sub-optimal arm i is
bounded by observing that

Arm i is pulled =) µi + 2cTi(t)  ˆµi + cTi(t)  ˆµ(1) + cT(1)(t)  µ(1)

(12)
The last inequality directly bounds the number of samples Ti(t) of a sub-optimal arm i. In MaxGapUCB 
the gap upper bound is obtained using the conﬁdence intervals of two arms  and the fact that a sub-
optimal gap (i  j) has the highest gap-UCB implies that

=) 2cTi(t)  µ(1)  µi = i.

(µj + 2cTj (t))  (µi  2cTi(t))  (ˆµj + cTj (t))  (ˆµi  2cTi(t))  max

=) 2(cTj (t) + cTi(t))  max  ij.

Thus unlike the reasoning in (12)  the number of samples from arm i is coupled to the number of
samples from arm j  and Ti(t) ! 1 if j is not sampled enough. We show in our analysis that this
cannot happen in MaxGapUCB. Furthermore  any arm i is coupled with multiple other arms since
the ordering of the arms is unknown  and may have to be sampled even if its own gap is small - a
phenomenon absent in standard BAI analysis because of the independence of the arm means. In our
proof  we account for all samples of an arm by deﬁning states the arm can belong to (called levels) 
and arguing about the conﬁdence intervals of the arms in unison.

6 Minimax Lower Bound

In this section  we demonstrate that the MaxGap problem is fundamentally different from best-arm
identiﬁcation (BAI) on gaps. We construct a problem instance and prove a lower bound on the number
of samples needed by any probably correct algorithm. The lower bound matches the upper bounds in
the previous section for this instance.

7

Lemma 1. Consider a model B with K = 4 normal distributions Pi = N (µi  1)  where
for some ⌫  ✏> 0. Then any algorithm that is correct with probability at least 1   must collect
⌦(1/✏2) samples of arm 4 in expectation.

µ4 = 0  µ3 = ✏  µ2 = ⌫ + 2✏  µ1 = 2⌫ + 2✏ 

Figure 5: Changing the original bandit model
B to B0. µ4 is shifted to the right by 2.1✏. As
a result  the maximum gap in B0 is between
green and purple.

Proof Outline: The proof uses a standard change of
measure argument [10]. We construct another prob-
lem instance B0 which has a different maximum gap
clustering compared to B (see Fig. 5; the maxgap
clustering in B is {4  3}[{ 2  1}  while the maxgap
clustering in B0 is {4  3  2}[{ 1})  and show that in
order to distinguish between B and B0  any proba-
bly correct algorithm must collect at least ⌦(1/✏2)
samples of arm 4 in expectation (see Appendix E for
details). From the deﬁnition of a using (10) (11)  it
is easy to check that 4 = ✏. Therefore  for problem instance B our algorithms ﬁnd the maxgap
clustering using at most O(log(✏/)/✏2) samples of arm 4 (c.f. Theorem 2). This essentially matches
the lower bound above.
This example illustrates why the maximum gap identiﬁcation problem is different from a simple BAI
on gaps. Suppose an oracle told a BAI algorithm the ordering of the arm means. Using the ordering
it can convert the 4-arm maximum gap problem B to a BAI problem on 3 gaps  with distributions
P4 3 = N (✏  2) P3 2 = N (⌫ + ✏  2)  and P2 1 = N (⌫  2). The BAI algorithm can sample arms i
and i + 1 to get a sample of the gap (i + 1  i). We know from standard BAI analysis [13] that the
gap (4  3) can be eliminated from being the largest by sampling it (and hence arm 4) O(1/⌫2) times 
which can be arbitrarily lower than the 1/✏2 lower bound in Lemma 1. Thus the ordering information
given to the BAI algorithm is crucial for it to quickly identify the larger gaps. The problem we solve
in this paper is identifying the maximum gap when the ordering information is not available.

7 Experiments

We conduct three experiments. First  we verify the validity of our sample complexity bounds
in Section 7.1. We then study the performance of our adaptive algorithms on simulated data in
Section 7.2  and on the Streetview dataset in Section 7.3. The code for all experiments is publicly
available [19].

7.1 Sample Complexity

(a)

In Fig. 6(b) and Fig. 6(c)  we plot the empiri-
cal stopping time against the theoretical sample
complexity (Theorem 2) for different arm con-
ﬁgurations. We choose the arm conﬁguration
in Fig. 6(a) containing K = 15 arms and three
unique gaps - a small gap 3 and two large gaps
2 < 1 = max = 0.4. The hardness param-
eter is changed by increasing 2 (from 0.35 to
0.39) and bringing it closer to 1. The rewards
are normally distributed with  = 0.05. We see
a linear relationship in Fig. 6(b) which suggests that the sample complexity expression in Theorem 2 is
correct up to constants. In Fig. 6(c) we include random sampling and see that our adaptive algorithms
require up to 5x fewer samples when run until completion. Fig. 6(c) also shows that our adaptive
algorithms always outperform random sampling  and the gains increase with hardness. We used a
lower bound based stopping condition for Random  Elimination  Top2UCB  and set c = 5 in the
UCB stopping condition (value of c chosen empirically as in [13]).

Figure 6: Stopping time experiments.

(b)

(c)

7.2 Simulated Data

In the second experiment  we study the performance on a simulated set of means containing two large
gaps. The mean distribution plotted in Fig. 7(a) has K = 24 arms (N (·  1))  with two large mean

8

BB'∆2∆1∆3(a)

(b)

(c)

Figure 7: (a) Two large gaps. (b) Clustering error probability for means shown in Fig. 7(a). (c) The
proﬁle of samples allocated by MaxGapUCB to each arm in (a) at different time steps.
gaps 10 9 = 0.98  19 18 = 1.0  and remaining small gaps (i+1 i = 0.2 for i /2{ 9  18}). We
expect to see a big advantage for adaptive sampling in this example because almost every sub-optimal
arm has a helper arm (see Section 5) which can help eliminate it quickly  and adaptive algorithms can
then focus on distinguishing the two large gaps. A non-adaptive algorithm on the other hand would
continue sampling all arms. We plot the fraction of times C1 6= {1  . . .   18} in 120 runs in Fig. 7(b) 
and see that the active algorithms identify the largest gap in 8x fewer samples. To visualize the
adaptive allocation of samples to the arms  we plot in Fig. 7(c) the number of samples queried for each
arm at different time steps by MaxGapUCB. Initially  MaxGapUCB allocates samples uniformly
over all the arms. After a few time steps  we see a bi-modal proﬁle in the number of samples. Since
all arms that achieve the largest U are sampled  we see that several arms that are near the pairs (10  9)
and (19  18) are also sampled frequently. As time progresses  only the pairs (10  9) and (19  18) get
sampled  and eventually more samples are allocated to the larger gap (19  18) among the two.

7.3 Streetview Dataset

For our third experiment we study performance on the
Streetview dataset [17  18] whose means are plotted in Fig. 8(a).
We have K = 90 arms  where each arm is a normal distribution
with mean equal to the Borda safety score of the image and stan-
dard deviation  = 0.05. The largest gap of 0.029 is between
arms 2 and 3  and the second largest gap is 0.024. In Fig. 8(b) 
we plot the fraction of times ˆC1 6= {1  2} in 120 runs as a
function of the number of samples  for four algorithms  viz. 
random (non-adaptive) sampling  MaxGapElim  MaxGapUCB 
and MaxGapTop2UCB. The error bars denote standard deviation
over the runs. MaxGapUCB and MaxGapTop2UCB require 6-7x
fewer samples than random sampling.

8 Conclusion

(a)

(b)
Figure 8: (a) Borda safety scores
for Streetview images. (b) Proba-
bility of returning a wrong cluster.

In this paper  we proposed the MaxGap-bandit problem: a novel maximum-gap identiﬁcation problem
that can be used as a basic primitive for clustering and approximate ranking. Our analysis shows
a novel hardness parameter for the problem  and our experiments show 6-8x gains compared to
non-adaptive algorithms. We use simple Hoeffding based conﬁdence intervals in our analysis
for simplicity  but better bounds can be obtained using tighter conﬁdence intervals [13]. Several
extensions of this basic problem are possible. An ✏-relaxation of the MaxGap Bandit is useful when
the largest and second-largest gaps are close to each other. Other possibilities include identifying
the largest gap within a top quantile of the arms  or clustering with a constraint that the returned
clusters are of similar cardinality. All of these extensions will likely require new ideas  as it is unclear
how to obtain a lower bound for the gap associated with every arm. Finding an instance-dependent
lower bound for MaxGap-bandit is an intriguing problem. Finally  one way to cluster the distributions
into more than two clusters is to apply the max-gap identiﬁcation algorithms recursively; however it
would be interesting to come up with algorithms that can perform this clustering directly.

9

(1)(9)(10)(18)(19)(24)∆max=1∆2=0.98(1)(90)∆maxAcknowledgments

Ardhendu Tripathy would like to thank Ervin Tánczos for helpful discussions. The authors would also
like to thank the reviewers for their comments and suggestions. This work was partially supported by
AFOSR/AFRL grants FA8750-17-2-0262 and FA9550-18-1-0166.

References
[1] Arpit Agarwal  Shivani Agarwal  Sepehr Assadi  and Sanjeev Khanna. Learning with limited
rounds of adaptivity: Coin tossing  multi-armed bandits  and ranking from pairwise comparisons.
In Conference on Learning Theory  pages 39–75  2017.

[2] Mark Braverman  Jieming Mao  and S Matthew Weinberg. Parallel algorithms for select and
partition with noisy comparisons. In Proceedings of the forty-eighth annual ACM symposium
on Theory of Computing  pages 851–862. ACM  2016.

[3] Sebastien Bubeck  Tengyao Wang  and Nitin Viswanathan. Multiple identiﬁcations in multi-
armed bandits. In Proceedings of the 30th International Conference on International Conference
on Machine Learning - Volume 28  ICML’13  pages I–258–I–265. JMLR.org  2013. URL
http://dl.acm.org/citation.cfm?id=3042817.3042848.

[4] Lijie Chen  Jian Li  and Mingda Qiao. Nearly instance optimal sample complexity bounds for

top-k arm selection. In Artiﬁcial Intelligence and Statistics  pages 101–110  2017.

[5] Shouyuan Chen  Tian Lin  Irwin King  Michael R Lyu  and Wei Chen. Combinatorial pure
exploration of multi-armed bandits. In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence 
and K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 27  pages
379–387. Curran Associates  Inc.  2014. URL http://papers.nips.cc/paper/
5433-combinatorial-pure-exploration-of-multi-armed-bandits.
pdf.

[6] Susan Davidson  Sanjeev Khanna  Tova Milo  and Sudeepa Roy. Top-k and clustering with

noisy comparisons. ACM Transactions on Database Systems (TODS)  39(4):35  2014.

[7] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Action elimination and stopping conditions
for the multi-armed bandit and reinforcement learning problems. Journal of machine learning
research  7(Jun):1079–1105  2006.

[8] Uriel Feige  Prabhakar Raghavan  David Peleg  and Eli Upfal. Computing with noisy informa-

tion. SIAM Journal on Computing  23(5):1001–1018  1994.

[9] Victor Gabillon  Mohammad Ghavamzadeh  and Alessandro Lazaric. Best arm identiﬁcation:
A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence. In Advances in Neural Information
Processing Systems  pages 3212–3220  2012.

[10] Aurélien Garivier and Emilie Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence.

In Conference on Learning Theory  pages 998–1027  2016.

[11] Weiran Huang  Jungseul Ok  Liang Li  and Wei Chen. Combinatorial pure exploration with
continuous and separable reward functions and its applications. In IJCAI  volume 18  pages
2291–2297  2018.

[12] Kevin Jamieson and Robert Nowak. Best-arm identiﬁcation algorithms for multi-armed bandits
in the ﬁxed conﬁdence setting. In 2014 48th Annual Conference on Information Sciences and
Systems (CISS)  pages 1–6. IEEE  2014.

[13] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil’ucb: An optimal
exploration algorithm for multi-armed bandits. In Conference on Learning Theory  pages
423–439  2014.

[14] Kwang-Sung Jun  Kevin G Jamieson  Robert D Nowak  and Xiaojin Zhu. Top arm identiﬁcation

in multi-armed bandits with batch arm pulls. In AISTATS  pages 139–148  2016.

10

[15] Shivaram Kalyanakrishnan and Peter Stone. Efﬁcient selection of multiple bandit arms: Theory

and practice. In ICML  volume 10  pages 511–518  2010.

[16] Shivaram Kalyanakrishnan  Ambuj Tewari  Peter Auer  and Peter Stone. Pac subset selection in

stochastic multi-armed bandits. In ICML  volume 12  pages 655–662  2012.

[17] Sumeet Katariya  Lalit Jain  Nandana Sengupta  James Evans  and Robert Nowak.
Chicago streetview dataset. 2018. URL https://github.com/sumeetsk/coarse_
ranking/.

[18] Sumeet Katariya  Lalit Jain  Nandana Sengupta  James Evans  and Robert Nowak. Adaptive
sampling for coarse ranking. In International Conference on Artiﬁcial Intelligence and Statistics 
pages 1839–1848  2018.

[19] Sumeet Katariya  Ardhendu Tripathy  and Robert Nowak. Code for maxgap bandit algorithms

and experiments. 2019. URL https://github.com/sumeetsk/maxgap_bandit.

[20] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed

bandit problem. Journal of Machine Learning Research  5(Jun):623–648  2004.

[21] Cheng Mao  Jonathan Weed  and Philippe Rigollet. Minimax rates and efﬁcient algorithms for
noisy sorting. In Firdaus Janoos  Mehryar Mohri  and Karthik Sridharan  editors  Proceedings
of Algorithmic Learning Theory  volume 83 of Proceedings of Machine Learning Research 
pages 821–847. PMLR  07–09 Apr 2018. URL http://proceedings.mlr.press/
v83/mao18a.html.

[22] Marta Soare  Alessandro Lazaric  and Rémi Munos. Best-arm identiﬁcation in linear bandits.

In Advances in Neural Information Processing Systems  pages 828–836  2014.

[23] Honglei Zhuang  Chi Wang  and Yifan Wang. Identifying outlier arms in multi-armed bandit.

In Advances in Neural Information Processing Systems  pages 5204–5213  2017.

11

,Sumeet Katariya
Ardhendu Tripathy
Robert Nowak