2018,Learning Confidence Sets using Support Vector Machines,The goal of confidence-set learning in the binary classification setting is to construct two sets  each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes  while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches  we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically  we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method.,Learning Conﬁdence Sets using Support Vector

Machines

Department of Mathematical Sciences

Department of Mathematical Sciences

Wenbo Wang

Binghamton University
Binghamton  NY 13902

Xingye Qiao*

Binghamton University
Binghamton  NY 13902

wang2@math.binghamton.edu

qiao@math.binghamton.edu

Abstract

The goal of conﬁdence-set learning in the binary classiﬁcation setting [14] is to
construct two sets  each with a speciﬁc probability guarantee to cover a class. An
observation outside the overlap of the two sets is deemed to be from one of the two
classes  while the overlap is an ambiguity region which could belong to either class.
Instead of plug-in approaches  we propose a support vector classiﬁer to construct
conﬁdence sets in a ﬂexible manner. Theoretically  we show that the proposed
learner can control the non-coverage rates and minimize the ambiguity with high
probability. Efﬁcient algorithms are developed and numerical studies illustrate the
effectiveness of the proposed method.

1

Introduction

In binary classiﬁcation problems  the training data consist of independent and identically distributed
pairs (Xi  Yi)  i = 1  2  ...  n drawn from an unknown joint distribution P   with Xi ∈ X ⊂ Rp 
and Yi ∈ {−1  1}. While the misclassiﬁcation rate is a good assessment of the overall classiﬁcation
performance  it does not directly provide conﬁdence for the classiﬁcation decision. Lei [14] proposed
a new framework for classiﬁers  named classiﬁcation with conﬁdence  using notions of conﬁdence and
efﬁciency. In particular  a classiﬁer φ(x) therein is set-valued  i.e.  the decision may be {−1} {1}  or
{−1  1}. Such a classiﬁer corresponds to two overlapped regions in the sample space X   C−1 and
C1  and they satisfy that C−1 ∪ C1 = X . With these regions  we have the set-valued classiﬁer

{−1}  when x ∈ C−1\C1

{1}  when x ∈ C1\C−1
{−1  1}  when x ∈ C−1 ∩ C1

.

φ(x) =

Those points in the ﬁrst two sets are classiﬁed to a single class as by traditional classiﬁers. However 
those in the overlap receive a decision of {−1  1}  hence may belong to either class. When the option
of {−1  1} is forbidden  the set-valued classiﬁer degenerates to a traditional classiﬁer.
Lei [14] deﬁned the notion of conﬁdence as the probability 100(1−αj)% that set Cj covers population
class j for j = ±1 (recalling the conﬁdence interval in statistics). The notion of efﬁciency is opposite
to ambiguity  which refers to the size (or probability measure) of the overlapped region named
the ambiguity region. In this framework  one would like to encourage classiﬁers to minimize the
ambiguity when controlling the non-coverage rates. Lei [14] showed that the best such classiﬁer  the
Bayes optimal rule  depends on the conditional class probability function η(x) = P (Y = 1|X = x).
Lei [14] then proposed to use the plug-in method  namely to ﬁrst estimate η(x) using  for instance 
logistic regression  then plug the estimation into the Bayes solution. Needless to say  its empirical
performance highly depends on the estimation accuracy of η(x). However  it is well known that the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

latter can be more difﬁcult than mere classiﬁcation [24  9  26]  especially when the dimension p is
large [27].
Support vector machine [SVM; 5] is a popular classiﬁcation method with excellent performance for
many real applications. Fernández-Delgado et al. [7] compared 179 classiﬁers on 121 real data sets
and concluded that SVM was among the best and most powerful classiﬁers. To avoid estimating the
conditional class probability η(x)  we propose a support vector classiﬁer to construct conﬁdence sets
by empirical risk minimization. Our method is more ﬂexible as it takes advantage of the powerful
prediction power of support vector machine.
We show in theory that the population minimizer of our optimization is to some extent equivalent to
the Bayes optimal rule in [14]. Moreover  in the ﬁnite-sample case  our classiﬁer can control both
non-coverage rates while minimizing the ambiguity.
A closely related problem is the Neyman-Pearson (NP) classiﬁcation [4  19] whose goal is to ﬁnd a
boundary for a speciﬁc null hypothesis class. It aims to minimize the probability that an observation
from the alternative class falls into this region (the type II error) while controlling the type I error 
i.e.  the non-coverage rate for the null class. See Tong et al. [22] for a survey. Our problem can be
understood as a two-sided NP classiﬁcation problem. Other related areas of work are conformal
learning  set-valued classiﬁcation  or classiﬁcation with reject and reﬁne options. See [21]  [6]  [22] 
[23]  [11]  [2] and [28].
The rest of the article is organized as follows. Some background information is provided in Section
2. Our main method is introduced in Section 3. A comprehensive theoretical study is conducted in
Section 4  including the Fisher consistency and novel statistical learning theory. In Section 5  we
present efﬁcient algorithms to implement our method. The usefulness of our method is demonstrated
using simulation and real data in Section 6. Detailed proofs are in the Supplementary Material.

2 Background and notations

We ﬁrst formally deﬁne the problem and give some useful notations.
It is desirable to keep the ambiguity as small as possible. On the other hand  we would like as many
class j observations as possible to be covered by Cj. Consider predetermined non-coverage rates α−1
and α1 for the two classes. Let P−1 and P1 be the probability measure of X conditional on Y = −1
and +1. Conceptually  we formulate classiﬁcation with conﬁdence as the optimization below.

P (C−1 ∩ C1)

min
C−1 C1

(1)
Here the constraint that Pj(Cj) ≥ 1 − αj means that 100(1 − αj)% of the observations from class j
should be covered by region Cj.

subject to Pj(Cj) ≥ 1 − αj  j = ±1  C−1 ∪ C1 = X .

Figure 1: The left panel shows the two deﬁnite regions and the ambiguity region in the case of
symmetric Gaussian distributions. The right penal illustrates the weight function (see Section 3).
−1 = {x : η(x) ≤ t−1} and
Under certain conditions  the Bayes solution of this problem is: C∗
1 = {x : η(x) ≥ t1} with t−1 and t1 satisfying that P−1(η(X) ≤ t−1) = 1 − α−1 and
C∗
P1(η(X) ≥ t1) = 1 − α1. A simple illustrative toy example with two Gaussian distributions on R is
shown in Figure 1. The two boundaries are shown as the vertical lines  which lead to three decision

2

−3−2−101230.00.10.20.30.4xdensityClass −1Class +1{−1}{+1}{−1 +1}a1a-1C-1C1−2−1012−0.50.00.51.01.52.02.53.0uloss0−1 losshinge lossweightweighted hinge lossregions  {−1}  {+1}  and {−1  +1}. The non-coverage rate α−1 for class −1 is shown on the right
tail of the red curve (similarly  α1 for class 1 on the left tail of blue curve.) In reality  the underlying
distribution will be more complicated than a simple multivariate Gaussian distribution and the true
boundary may be beyond linearity. In these cases  ﬂexible approaches such as SVM will work better.
Conﬁdence sets may be seen as equivalent to classiﬁcation with reject options [11  2  10] via different
parameterizations. The Bayes rule in this article is different from the Bayes rule in the literature of
classiﬁcation with reject options. In that context  the Bayes rule depends on a comparison between
η(·) and a predetermined cost of rejection d. But it does not lead to a guarantee of the coverage
probabilities for the corresponding conﬁdence sets. Here instead  the cutoff for the Bayes rule is
calibrated to achieve the desired coverage probabilities.

3 Learning conﬁdence sets using SVM

n(cid:88)

i=1

n(cid:88)

i=1

(cid:80)n

(cid:88)

i:yi=j

(cid:88)

i:yi=j

To avoid estimating η  we propose to solve the empirical counterpart of (1) directly using SVM. Here 
we present two variants of our method. We start with an original version to illustrate the basic idea.
Then we introduce an improvement.
Unlike the regular SVM  the proposed classiﬁer has two (not one) separating boundaries. They are
deﬁned as {x : f (x) = −ε} and {x : f (x) = +ε} where f is the discriminant function  and ε ≥ 0.
The positive region C1 is {x : f (x) ≥ −ε} and the negative region C−1 is {x : f (x) ≤ ε}. Hence
when −ε ≤ f (x) ≤ ε  observation x falls into the ambiguity region {−1  1}.
Deﬁne R(f  ε) = P (|Y f (X)| ≤ ε) the probability measure of the ambiguity. We may rewrite
problem (1) in terms of the function f and threshold ε 

min
ε∈R+ f

R(f  ε) 

subject to Pj(Y f (X) < −ε) ≤ αj  j = ±1.

(2)

Replacing the probability measures above by the empirical measures  we can obtain 

min
ε∈R+ f

1
n

1{−ε ≤ f (xi) ≤ ε} 

subject to 1
nj

1{yif (xi) < −ε} ≤ αj  j = ±1.

It is easy to show that as long as the equalities in the constraints are achieved at the optimum  we can
obtain the same minimizer if the objective function is changed to 1
n
For efﬁcient and realistic optimization  we replace the indicator function 1{u ≤ 0} in the objective
function and constraints by the Hinge loss function (1 − u)+. The practice of using a surrogate loss
to bound the non-coverage rates has been widely used in the literature of NP classiﬁcation  see [19].
To simplify the presentation  we denote Ha(u) = (1 + a − u)+ as the a-Hinge Loss and it can be
seen that Ha(x) coincides with the original Hinge loss when a = 0. Our initial classiﬁer can be
represented by the following optimization:

1{yif (xi) − ε ≤ 0}.

i=1

min
ε∈R+ f

1
n

Hε(yif (xi)) + λJ(f ) 

subject to 1
nj

H−ε(yif (xi)) ≤ αj  j = ±1.

(3)

Here J is a regularization term to control the complexity of the discriminant function f. When f
takes the linear form of f (x) = xT β + b  J(f ) can be L2-norm (cid:107)β(cid:107)2 or L1-norm |β|.
In SVM  yf (x) is called the functional margin  which measures the signed distance from x to the
boundary {x : f (x) = 0}. Positive and large value of yf (x) means the observation is correctly
classiﬁed  and is far away from the boundary. In our situation  we compare yf (x) with +ε and −ε
respectively. If yf (x) < −ε  then x is not covered by Cy (hence is misclassiﬁed  in the classiﬁcation
language). On the other hand  if yf (x) ≤ ε  then x either satisﬁes that yf (x) < −ε as above  or
falls into the ambiguity  which is why we try to minimize the sum of Hε(yif (xi)).

yi=j H−ε(yif (xi)) for both classes  we aim to control the non-coverage rates.
Since H−ε(u) ≥ 1{u < −ε} (the latter indicates the occurrence of non-coverage) for nega-
tively large u. It may be more conservative by using the Hinge loss than the indicator function
1{yif (xi) < −ε} in the constraint to control the non-coverage rates. We alleviate this problem by
imposing a weight wi to each observation in the constraint. In particular  this weight is chosen to
be wi = max{1  H−ε(y ˆf (x))}−1  where ˆf is a reasonable guess of the ﬁnal minimizer f. Our goal

By constraining(cid:80)

3

is to weight the Hinge loss in the constraint  wiH−ε(yif (xi))  so that it approximates the indicator
function 1{yif (xi) < −ε}. This may be illustrated by Figure 1 in which the blue bold line is the
result of multiplying the weight (red dashed) by the Hinge loss (purple dotted)  which is close to the
indicator function (black dot-dashed). Note that by weighting the Hinge loss  the impact of those
observations with very negatively large u = yf (x) value is reduced to 1. The adaptive weighted
version of our method changes constraint (3) to 1
nj

(cid:80)
i:yi=j wiH−ε(yif (xi)) ≤ αj  j = ±1.

In practice  we adopt an iterative approach  and use the estimated f from the previous iteration to
calculate the weight for each observation at the current iteration. We start with equal weights for each
observation  solve the optimization problem with the weights obtained in the last iteration  and then
calculate the new weights for the next iteration. [25] ﬁrst used this idea in their work of adaptively
weighted large margin classiﬁers for the purpose of robust classiﬁcation.

4 Theoretical Properties

In this section we study the theoretical properties of the proposed method. We start with population
level properties in Section 4.1. In Section 4.2  we discuss the ﬁnite-sample properties using novel
statistical learning theory.

4.1 Fisher consistency and excess risk

Assume that P−1 and P1 are continuous with density function p−1 and p1  and πj = P (Y = j)
is positive for j = ±1. Moreover  η(X) is continuous and has positive density function almost
everywhere  and t−1 and t1 are quantiles of η(X). They satisﬁes P−1(η(X) ≤ t−1) = 1 − α−1
and P1(η(X) ≥ t1) = 1 − α1. We need to make assumptions on the difﬁculty level of the
classiﬁcation task. In particular  the classiﬁcation should be difﬁcult enough so that overlapping
regions is meaningful (otherwise  there will be almost no ambiguity even at small non-coverage
rates.)
Assumption 1. t−1 ≥ 1
Assumption 2. ∃c > 0  t−1 − c ≥ 1
2 ≥ t1 + c.
1 = {x : η(x) ≥ t1}
Each assumption implies that the union of C∗
is X . Otherwise  there will be a gap around the boundary {x : η(x) = 1/2}. It is easy to see that
Assumption 2 is stronger than Assumption 1.
Fisher consistency concerns the Bayes optimal rule  which is the minimizer of problem (2). In (4)
below  we replace the loss function in the objective function of (2) with risk under the Hinge loss.

−1 = {x : η(x) ≤ t−1} and C∗

2 ≥ t1.

min RH (f  ε) 

subject to Pj(Y f (X) < −ε) ≤ αj  j = ±1 

(4)

where RH (f  ε) = E[Hε(Y f (X))].
Theorem 1 shows that for any ﬁxed ε  the minimizer of (4) is the same as the Bayes rule [14].
Theorem 1. Under Assumption 1  for any ﬁxed ε ≥ 0  function

ε · sign(η(x) − 1
2 ) 
−(1 + ε) 
is the minimizer to (4) and a minimizer to (2).

f∗(x) =

η(x) > t−
t+ ≤ η(x) ≤ t−
f (x) < t+

.

 1 + ε 

A key result in many machine learning literature (such as [3]  [30] or [2]) was that the excess risk
of 0-1 classiﬁcation loss is bounded by the excess risk of surrogate loss. Here we show a similar
result for the conﬁdence set problem. That is  the excess ambiguity R(f  ε) − R(f∗  ε) vanishes as
RH (f  ε) − RH (f∗  ε) goes to 0.
Theorem 2. Under Assumption (2)  for any ε ≥ 0  and ∀f satisfying the constraints in (2)  there
exists C

2c > 0 such that the following inequality holds 

4c2 + 1

= 1

(cid:48)

(cid:48)

(RH (f  ε) − RH (f∗  ε)) ≥ R(f ) − R(f∗).

C

Note that C

(cid:48)

does not depend on ε.

4

4.2 Finite-sample properties
Denote the Reproducing Kernel Hilbert Space (RKHS) with bounded norm as HK(s) = {f :
X → R|f (x) = h(x) + b  h ∈ HK ||h||HK ≤ s  b ∈ R} and r = supx∈X K(x  x). For a
ﬁxed ε  deﬁne the space of constrained discriminant functions as Fε((α−1  α1)) = {f : X →
(cid:80)
R|E(H−ε(Y f (X))|Y = j) ≤ αj  j = ±1}  and its empirical counterpart as ˆFε((α−  α+)) =
{f : X → R|n−1
i:yi=j H−ε(yif (xi)) ≤ αj  j = ±1}. Moreover  we deﬁne the feasible
function space Fε(κ  s) = HK(s) ∩ Fε((α−1 − κ√
)) and its empirical counterpart
ˆFε(κ  s) = HK(s)∩ ˆFε((α−1− κ√
)). Lastly  consider a subset of the Cartesian product
of the above feasible function space and the space for ε  F(κ  s) = {(f  ε)  f ∈ Fε(κ  s)  ε ≥ 0}
and its empirical counterpart ˆF(κ  s) = {(f  ε)  f ∈ ˆFε(κ  s)  ε ≥ 0}. Then optimization problem
(3) of our proposed method can be written as

  α1 − κ√

  α1− κ√

n−1

n−1

n1

n1

j

n(cid:88)

i=1

min

(f ε)∈ ˆF (0 s)

1
n

Hε(yif (xi)).

(5)

√

√
sr/

In Theorem 3  we give the ﬁnite-sample upper bound for the non-coverage rate.
Theorem 3. Let (f  ε) be a solution to optimization problem (5)  then with probability at least 1− 2ζ 
Z =
Pj(Y f (X) < −ε) ≤ E[H−ε(Y f (X))|Y = j] ≤ 1
nj

n  Tn(ζ) = {2srlog(1/ζ)/n}1/2 and r = supX K(x  x)

H−ε(yif (xi)) + 3Tnj (ζ) + Z(nj).

(cid:88)

yi=j

Theorem 3 suggests that if we want to control the non-coverage rate on average at the nominal α−1 or
α1 rates with high probability  we should choose the α−1 or α1 values to be slightly smaller than the
desired ones in optimization (3) in practice. In particular  we need to make 1
yi=j H−ε(yif (xi))+
nj
3Tnj (ζ)+Z(nj) ≤ αj. Note that the remainder terms 3Tnj (ζ)+Z(nj) will vanish as n−1  n1 → ∞.
The next theorem ensures that the empirical ambiguity probability from solving (5) based on a
ﬁnite sample will converge to the ambiguity given by the solution on an inﬁnite sample (under the
constraints E(H−ε(Y f (X))|Y = j) ≤ αj  j = ±1).
Theorem 4. Let ( ˆf   ˆε) be the solution of the optimization problem (6)

(cid:80)

Hε(yif (xi)).

(6)

√

with κ = (6log( 1
ζ ) + 1)
(i). ˆf ∈ Fˆε(0  s)  and
(ii). RH ( ˆf   ˆε) − min

(f ε)∈F (0 s)

sr. Then with probability 1 − 6ζ  and large enough n−1 and n1 we have
RH (f  ˆε) ≤ κ(2n−1/2 + 4 min{α−1  α1}−1 min{√

n1}−1).

n−1 

√

n(cid:88)

i=1

min

(f ε)∈ ˆF (κ s)

1
n

In our study we analyze formula (5) where J(f ) appears in the constraint instead of the regularized
formula (3) for technical convenience. This comes at a price of a ﬁxed upper bound s on J(f ). We
can revise the statements of Theorems 3 and 4 so that s increases with n to inﬁnity (with a price of a
slower convergence rate.) It is possible to derive the results for the regularized version based on (3).
Since at the optimality it is easy to show that J(f ) ≤ 2/λ (this is done by showing that the objective
is at most 2  when f ≡ 0 and ε = 1 ) we may rewrite s in Theorem 3 in terms of λ.

5 Algorithms

In this section  we give details of the algorithm. Similar to the SVM implementation  we propose to
solve the dual problem. We start with the linear SVM with L2 norm for illustrative purposes. After
introducing two sets of slack variables  ηi = (1−ε−yi(xT
i β+b))+ 
we can show that (3) is equivalent to (7) 

i β+b))+ and ξi = (1+ε−yi(xT

ξi

(7)

min

Θ

1
2

||β||2

2 + λ(cid:48) n(cid:88)

i

5

subject to yi(xT

ξi ≥ 0 

(cid:88)
i β + b) ≥ 1 + ε − ξi 
wiηi ≤ n−1α−1 

yi=−1

yi(xT
ηi ≥ 0 

i β + b) ≥ 1 − ε − ηi
wiηi ≤ n1α1 

(cid:88)

yi=1

for all i = 1  2  ...  n 

ε ≥ 0.

Here Θ is the collection of all variables of interest  namely Θ = {ε  β  b {ξi}n
then solve it via the quadratic programming below 

i=1 {ηi}n

i=1}. We can

n(cid:88)

n(cid:88)

i=1

j=1

min
Θ(cid:48)

1
2

(ζi + τi)(ζj + τj)yiyjx(cid:48)

ζi − n(cid:88)
n(cid:88)

i=1

i=1

i=1

ixj − n(cid:88)
n(cid:88)
i ζiyixi +(cid:80)n

i=1

τi + n−1α−1θ−1 + n1α1θ1

(8)

n(cid:88)

i=1

ζi − n(cid:88)

i=1

τi ≥ 0.

subject to 0 ≤ ζi ≤ λ(cid:48) 

0 ≤ τi ≤ θyiwi 

ζiyi +

τiyi = 0 

i=1 {τi}n

Here Θ(cid:48) = {{ζi}n
i=1  θ−1  θ1} consists of all the variables in the dual problem. The above
optimization may be solved by any efﬁcient quadratic programming routine. After solving the dual
i τiyixi. Then we can plug β into the primal

problem  we can ﬁnd β by β = (cid:80)n
Kernel Hilbert Space (RKHS) with a positive deﬁnite kernel K  f (x) =(cid:80)n

problem and ﬁnd b and ε by linear programming.
For nonlinear f  we can adopt the widely used ‘kernel trick’. Assume f belongs to a Reproducing
i=1 ciK(xi  x) + b. In
this case the dual problem is the same as above except that x(cid:48)
ixj is replaced by K(xi  xj). After
the solution has been found  we then have ci = ζi + τi. Common choices for the kernel function
includes the Gaussian kernel and the polynomial kernel.

6 Numerical Studies

In this section  we compare our conﬁdence-support vector machine (CSVM) method and methods
based on the plug-in principal  including L2 penalized logistic regression [12]  kernel logistic
regression [31]  kNN [1]  random forest [15] and SVM [5] using both simulated and real data.
In the study  we use solver Cplex to solve the quadratic programming problem arising in CSVM. For
other methods  we use existing R packages glmnet  gelnet  class  randomForest and e1071.

6.1 Simulation

We study the numerical performance over a large variety of sample sizes. In each case  an independent
tuning set with the same sample size as the training set is generated for parameter tuning. The testing
set has 20000 observations (10000 or nearly 10000 for each class). We run the simulation multiple
times (1 000 times for Example 1 and 100 times for Example 2 and 3) and report the average and
standard error. Both non-coverage rates are set to 0.05.
the best parameter λ and the hyper-parameter for kernel methods as follows.
We select
We search for the optimal ρ in the Gaussian kernel exp (−(cid:107)x − y(cid:107)2/ρ2) from the grid
10ˆ{−0.5 −0.25  0  0.25  0.5  0.75  1} and the optimal degree for polynomial kernel from
{2  3  4}. For each ﬁxed candidate hyper-parameter  we choose λ from a grid of candidate values
ranging from 10−4 to 102 by the following two-step searching scheme. We ﬁrst do a rough search
with a larger stride {10−4  10−3.5  . . .   102} and get the best parameter λ1. Then we do a ﬁne search
from λ1×{10−0.5  10−.4  . . .   100.5}. After that  we choose the optimal pair which gives the smallest
tuning ambiguity and has the two non-coverage rates for the tuning set controlled.
To adapt traditional classiﬁcation methods to the conﬁdence set learning problem  we use the plug-in
principal [14]. To improve the performance  we make use of the suggested robust implementation in
[14] for all the methods. Speciﬁcally  we ﬁrst obtain an estimate of η (such as by logistic regression 
kernel logistic regression  kNN and random forest) or a monotone proxy of it (such as the discriminant
function f in CSVM and SVM)  then choose thresholds ˆt−1 and ˆt1 which are two sample quantiles

of(cid:98)η(x) (or f (x)) among the tuning set so that the non-coverage rates for the tuning set match the
nominal rates. The ﬁnal predicted sets are induced by thresholding(cid:98)η(x) (or f (x)) using ˆt−1 and ˆt1.

Because there are two non-coverage rates and one ambiguity size to compare here  how to make fair
comparison becomes a tricky problem since one classiﬁer can sacriﬁce the non-coverage rate to gain

6

Figure 2: Scatter plots of the ﬁrst two dimensions for the simulated data with Bayes rules showing
the two deﬁnite regions and the ambiguity region.

2 )  Σ1 = diag( 1

in ambiguity. One by-product of the robust implementation above is that the non-coverage rate of
most of the methods will become very similar and we only need to compare the size of the ambiguity.
We also include a naive SVM approach (’SVM_r’ in plots below) whose discriminant function is
obtained in the traditional way  but which induces conﬁdence sets by thresholding in the same way
described above.
We consider three different simulation scenarios. In the ﬁrst scenario we compare the linear ap-
proaches (SVM and penalized logistic regression)  while in the next two cases we consider nonlinear
methods. In all cases  we add additional noise dimensions to the data. These noise covariates are
normally distributed with mean 0 and Σ = diag(1/p)  where p is the total dimension of the data.
Example 1 (Linear model with nonlinear Bayes rule): In this scenario  we have two normally
distributed classes with different covariance matrices. In particular  denote X|Y = j ∼ N (µj  Σj)
for j = ±1  then µ−1 = (−2  1)T   µ1 = (1  0)T   and Σ−1 = diag(2  1
2   2). The
prior probabilities of both classes are the same. Lastly  we add eight dimensions of noise covariates
to the data. The data are illustrated in the left penal of Figure 2. We compare linear CSVM  and the
plug-in methods L2 penalized logistic regression [8] and naive linear SVM to estimate η.
Example 2 (Moderate dimensional polynomial boundary): This case is similar to the one in [29].
First we generate x1 ∼ Unif[−1  1] and x2 ∼ Unif[−1  1]. Deﬁne functions fj(x) = j(−3.6x2
1 +
2 − 0.8)  j = ±1. Then we set η(x) = f1(x)/(f−1(x) + f1(x))  where x = (x1  x2). We then
7.2x2
add 98 covariates on top of the 2-dimensional signal. The data are illustrated in the middle penal of
Figure 2. In this scenario  we choose to use the polynomial kernel for all the kernel based methods.
Example 3 (High-dimensional donut): We ﬁrst generate a two-dimensional data  (ri  θi) where
θi ∼ Unif[0  2π]  ri|(Y = −1) ∼ Uniform[0  1.2]  and ri|(Y = +1) ∼ Unif[0.8  2]. Then we
deﬁne the two-dimensional Xi = (ri cos(θi)  ri sin(θi)). The data are illustrated in the right penal
of Figure 2. We then add 498 covariates on top of the 2-dimensional signal. We use the Gaussian
kernel  K(x  y; ρ) = exp (−(cid:107)x − y(cid:107)2/ρ2) for all the kernel based methods.
Our methods are improved using the robust implementation. The results are reported in Figure 3.
We also show the performance of CSVM with weighting but without robust implementation. For
Example 1  our CSVM method gives a signiﬁcantly smaller ambiguity than either logistic regression
or naive SVM. In Example 2 and Example 3  our method gives a smaller or at least comparable
ambiguity to the best plug-in method  which is kernel logistic regression. Our weighted CSVM
performs the best when sample size is small in the linear case and it outperforms kNN  Random
Forest and naive SVM in nonlinear cases. It is not surprising that the naive SVM method performs
signiﬁcantly worse than all other methods in the nonlinear settings  as the hinge loss is well known
to not lead to consistent estimates for class probabilities (see [18]). The non-coverage rates (not
shown here) of CSVM  random forest  kernel logistic regression and naive SVM methods are close to
each other while CSVM without robust implementation and kNN have similar non-coverage rates. A
detailed comparison can be found in the Supplementary Material.

6.2 Real Data Analysis

We conduct the comparison on the hand-written zip code data [13]. The data set consists of many
16 × 16 pixel images of handwritten digits. It is widely used in the classiﬁcation literature. There are

7

−4−202−2−101234x1x2−1.0−0.50.00.51.0−1.0−0.50.00.51.0x1x2−2−1012−2−1012x1x2Class −1Class +1both training and testing sets deﬁned in it. Lei [14] used the same dataset for illustrating the plug-in
methods. We choose this dataset to directly compare with the plug-in methods.
Following Lei [14]  to form a binary classiﬁcation problem  we use the subset of the data containing
digits {0  6  8  9}. Images with digits 0  6  9 are labeled as class −1 (they are digits with one circle)
and those with digit 8 (two circles) are labeled as class +1. Previous studies [21] pointed out that
there was discrepancies between the training and testing set of this data set. So in this study we ﬁrst
mixed the training and testing data and then randomly split into new training  tuning and testing data.
The training and tuning data both have sample size 800  with 600 from class −1 and 200 from class 1
to preserve the unbalance nature of the data set. During training  we oversample class 1 by counting
each observation three times to alleviate the unbalanced classes issue.
Although Lei [14] set both nominal non-coverage rates to be 0.05 in their study which focused on
linear methods  it needs to be pointed out that many nonlinear classiﬁers  such as SVM with Gaussian
kernel  can achieve this non-coverage rate without introducing any ambiguity. Therefore we reduce
the non-coverage rate to 0.01 for both classes to make the task more challenging.
We apply Gaussian kernel for CSVM  and compare with kernel logistic regression with Gaussian
kernel  random forest  kNN and naive SVM with Gaussian kernel on this data set.

Figure 4: An illustration of CSVM method using t-SNE. The left penal shows the true labels  and the
right panel the predicted label for weighted CSVM.

The results are summarized in Table 1 with numbers in percentage. CSVM gives better results than
all the plug-in methods. We plot the zip code data using t-distributed stochastic neighbor embedding
(t-SNE) [17] to give a visualization of our method and the data.

Figure 3: Outcome of ambiguities in three simulation settings. Non-coverage rates are similar among
different methods and are not shown here. CSVM has the smallest ambiguity.

8

−20−1001020−20−1001020tSNE1tSNE2−20−1001020−20−1001020tSNE1tSNE2labelAmbiguityClass −1Class 10.00.10.250100150200sampleambiguity0.000.250.500.75100150200250300sampleambiguity0.000.250.500.75100200300400500sampleambiguityclassifiercsvm_r_wcsvm_wknn_rlogi_rrf_rsvm_rClassiﬁer
Non-coverage(-1)
Non-coverage(+1)
Ambiguity

CSVM
0.05(0.005)
0.56(0.06)
8.29(0.18)

CSVM(r)
1.02(0.05)
1.19(0.11)
2.52(0.13)

KNN(r)
0.81(0.04)
1.04(0.09)
10.21(2.12)

KLR(r)
0.98(0.05)
1.25(0.10)
3.46(0.17)

RF(r)
0.95(0.04)
1.10(0.11)
7.55(0.37)

naive SVM(r)
1.00(0.05)
1.27(0.11)
2.66(0.13)

Table 1: CSVM gives better or comparable outcome to the best plug-in method.

It can be seen that the ambiguity region mainly lies on the boundary between the two classes. In
particular  they cover those points which appear to be closer to the class other than the one they really
belong to. Moreover  it can be seen that the union of the ambiguity region and the predicted region
for either class  covers almost all the ground of that class (deﬁned by the true labels). This is not
surprising since the non-coverage rate of CSVM is set to be a small number of 1% in this case.

7 Conclusion and future works

In this work  we propose to learn conﬁdence sets using support vector machine. Instead of a plug-in
approach  we use empirical risk minimization to train the classiﬁer. Theoretical studies have shown
the effectiveness of our approach in controlling the non-coverage rate and minimizing the ambiguity.
We make use of many well understood advantages of SVM to solve the problem. For instance the
‘kernel trick’ allows more ﬂexibility and empowers us to conduct classiﬁcation in nonlinear cases.
Hinge loss function is not the only surrogate loss that can be used. There are many other useful loss
functions with good properties in different scenarios [16].
Conﬁdence set learning for multi-class case is also an interesting future work. This has a natural
connection to the literature of multi-class classiﬁcation with conﬁdence [20]  classiﬁcation with reject
and reﬁne options [28] and conformal learning [21].

References
[1] Naomi S Altman. An introduction to kernel and nearest-neighbor nonparametric regression.

The American Statistician  46(3):175–185  1992.

[2] Peter L Bartlett and Marten H Wegkamp. Classiﬁcation with a reject option using a hinge loss.

Journal of Machine Learning Research  9(Aug):1823–1840  2008.

[3] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  101(473):138–156  2006.

[4] Adam Cannon  James Howse  Don Hush  and Clint Scovel. Learning with the neyman-pearson
and min-max criteria. Los Alamos National Laboratory  Tech. Rep. LA-UR  pages 02–2951 
2002.

[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning  20(3):

273–297  1995.

[6] Christophe Denis and Mohamed Hebiri. Conﬁdence sets with expected sizes for multiclass

classiﬁcation. arXiv preprint arXiv:1608.08783  2016.

[7] Manuel Fernández-Delgado  Eva Cernadas  Senén Barro  and Dinani Amorim. Do we need
hundreds of classiﬁers to solve real world classiﬁcation problems. J. Mach. Learn. Res  15(1):
3133–3181  2014.

[8] Jerome Friedman  Trevor Hastie  and Rob Tibshirani. Regularization paths for generalized

linear models via coordinate descent. Journal of statistical software  33(1):1  2010.

[9] Johannes Fürnkranz and Eyke Hüllermeier. Preference learning: An introduction. In Preference

learning  pages 1–17. Springer  2010.

[10] Yves Grandvalet  Alain Rakotomamonjy  Joseph Keshet  and Stéphane Canu.

Sup-
port vector machines with a reject option.
In D. Koller  D. Schuurmans  Y. Ben-
gio  and L. Bottou  editors  Advances in Neural Information Processing Systems 21 
pages 537–544. Curran Associates  Inc.  2009. URL http://papers.nips.cc/paper/
3594-support-vector-machines-with-a-reject-option.pdf.

9

[11] Radu Herbei and Marten H Wegkamp. Classiﬁcation with reject option. Canadian Journal of

Statistics  34(4):709–721  2006.

[12] Saskia Le Cessie and Johannes C Van Houwelingen. Ridge estimators in logistic regression.

Applied statistics  pages 191–201  1992.

[13] Yann LeCun  Bernhard Boser  John S Denker  Donnie Henderson  Richard E Howard  Wayne
Hubbard  and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation  1(4):541–551  1989.

[14] Jing Lei. Classiﬁcation with conﬁdence. Biometrika  page asu038  2014.

[15] Andy Liaw  Matthew Wiener  et al. Classiﬁcation and regression by randomforest. R news  2

(3):18–22  2002.

[16] Yufeng Liu  Hao Helen Zhang  and Yichao Wu. Hard or soft classiﬁcation? large-margin uniﬁed

machines. Journal of the American Statistical Association  106(493):166–177  2011.

[17] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9(Nov):2579–2605  2008.

[18] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized

likelihood methods. Advances in large margin classiﬁers  10(3):61–74  1999.

[19] Philippe Rigollet and Xin Tong. Neyman-pearson classiﬁcation  convexity and stochastic

constraints. Journal of Machine Learning Research  12(Oct):2831–2855  2011.

[20] Mauricio Sadinle  Jing Lei  and Larry Wasserman. Least ambiguous set-valued classiﬁers with

bounded error levels. Journal of the American Statistical Association  (just-accepted)  2017.

[21] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine

Learning Research  9(Mar):371–421  2008.

[22] Xin Tong  Yang Feng  and Anqi Zhao. A survey on neyman-pearson classiﬁcation and sugges-
tions for future research. Wiley Interdisciplinary Reviews: Computational Statistics  8(2):64–81 
2016.

[23] Vladimir Vovk  Ilia Nouretdinov  Valentina Fedorova  Ivan Petej  and Alex Gammerman. Crite-
ria of efﬁciency for set-valued classiﬁcation. Annals of Mathematics and Artiﬁcial Intelligence 
pages 1–26  2017.

[24] Junhui Wang  Xiaotong Shen  and Yufeng Liu. Probability estimation for large-margin classiﬁers.

Biometrika  95(1):149–167  2007.

[25] Yichao Wu and Yufeng Liu. Adaptively weighted large margin classiﬁers. Journal of Computa-

tional and Graphical Statistics  22(2):416–432  2013.

[26] Yichao Wu  Hao Helen Zhang  and Yufeng Liu. Robust model-free multiclass probability

estimation. Journal of the American Statistical Association  105(489):424–436  2010.

[27] Chong Zhang and Yufeng Liu. Multicategory large-margin uniﬁed machines. The Journal of

Machine Learning Research  14(1):1349–1386  2013.

[28] Chong Zhang  Wenbo Wang  and Xingye Qiao. On reject and reﬁne options in multicategory

classiﬁcation. Journal of the American Statistical Association  2017. accepted.

[29] Hao Helen Zhang  Yufeng Liu  Yichao Wu  Ji Zhu  et al. Variable selection for the multicategory
svm via adaptive sup-norm regularization. Electronic Journal of Statistics  2:149–167  2008.

[30] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics  pages 56–85  2004.

[31] Ji Zhu and Trevor Hastie. Kernel logistic regression and the import vector machine. Journal of

Computational and Graphical Statistics  14(1):185–205  2005.

10

,Wenbo Wang
Xingye Qiao