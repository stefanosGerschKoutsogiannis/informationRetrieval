2019,Universal Approximation of Input-Output Maps by Temporal Convolutional Nets,There has been a recent shift in sequence-to-sequence modeling from recurrent network architectures  to convolutional network architectures due to computational advantages in training and operation while still achieving competitive performance. For systems having limited long-term temporal dependencies  the approximation capability of recurrent networks is essentially equivalent to that of temporal convolutional nets (TCNs). We prove that TCNs can approximate a large class of input-output maps having approximately finite memory to arbitrary error tolerance. Furthermore  we derive quantitative approximation rates for deep ReLU TCNs in terms of the width and depth of the network and modulus of continuity of the original input-output map  and apply these results to input-output maps of systems that admit finite-dimensional state-space realizations (i.e.  recurrent models).,Universal Approximation of Input-Output Maps by

Temporal Convolutional Nets

Joshua Hanson

jmh4@illinois.edu

University of Illinois

Urbana  IL 61801

Maxim Raginsky
University of Illinois

Urbana  IL 61801

maxim@illinois.edu

Abstract

There has been a recent shift in sequence-to-sequence modeling from recurrent
network architectures to convolutional network architectures due to computational
advantages in training and operation while still achieving competitive performance.
For systems having limited long-term temporal dependencies  the approximation
capability of recurrent networks is essentially equivalent to that of temporal con-
volutional nets (TCNs). We prove that TCNs can approximate a large class of
input-output maps having approximately ﬁnite memory to arbitrary error tolerance.
Furthermore  we derive quantitative approximation rates for deep ReLU TCNs
in terms of the width and depth of the network and modulus of continuity of the
original input-output map  and apply these results to input-output maps of systems
that admit ﬁnite-dimensional state-space realizations (i.e.  recurrent models).

1

Introduction

Until recently  recurrent networks have been considered the de facto standard for modeling input-
output maps that transform sequences to sequences. Convolutional network architectures are becom-
ing favorable alternatives for several applications due to reduced computational overhead incurred
during both training and regular operation  while often performing as well as or better than recurrent
architectures in practice. The computational advantage of convolutional networks follows from the
lack of feedback elements  which enables shifted copies of the input sequence to be processed in
parallel rather than sequentially [Gehring et al.  2017]. Convolutional architectures have demonstrated
exceptional accuracy in sequence modeling tasks that have typically been approached using recurrent
architectures  such as machine translation  audio generation  and language modeling [Dauphin et al. 
2017  Kalchbrenner et al.  2016  van den Oord et al.  2016  Wu et al.  2016  Gehring et al.  2017 
Johnson and Zhang  2017].
One explanation for this shift is that both convolutional and recurrent architectures are inherently
suited to modeling systems with limited long-term dependencies. Recurrent models possess inﬁnite
memory (the output at each time is a function of the initial conditions and the entire history of
inputs until that time)  and thus are strictly more expressive than ﬁnite-memory autoregressive
models. However  in synthetic stress tests designed to measure the ability to model long-term
behavior  recurrent architectures often fail to learn long sequences [Bai et al.  2018]. Furthermore 
this unlimited memory property is usually unnecessary  which is supported in theory [Sharan et al. 
2018] and in practice [Chelba et al.  2017  Gehring et al.  2017]. In situations where it is only
important to learn ﬁnite-length sequences  feedforward architectures based on temporal convolutions
(temporal convolutional nets  or TCNs) can achieve similar results and even outperform recurrent
nets [Dauphin et al.  2017  Yin et al.  2017  Bai et al.  2018].
These results prompt a closer look at the conditions under which convolutional architectures provide
better approximation than recurrent architectures. Recent work by Miller and Hardt [2019] has shown

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

that recurrent models that are exponentially stable (in the sense that the effect of the initial conditions
on the output decays exponentially with time) can be efﬁciently approximated by feedforward models.
A key consequence is that exponentially stable recurrent models can be approximated by systems
that only consider a ﬁnite number of recent values of the input sequence for determining the value of
the subsequent output.
However  this notion of stability is inherently tied to a particular state-space realization  and it is
not difﬁcult to come up with examples of sequence-to-sequence maps that have both a stable and
an unstable state-space realization (e.g.  simply by adding unstable states that do not affect the
output). This suggests that the question of approximating sequence-to-sequence maps by feedforward
convolutional maps should be studied by abstracting away the notion of stability and only requiring
that the system output depend appreciably on recent input values and negligibly on input values in the
distant past. The formalization of this property was introduced by Sandberg [1991] under the name of
approximately ﬁnite memory  building on earlier work by Boyd and Chua [1985]. Outputs of systems
characterized by this property can be approximated by the output of the same system when applied to
a truncated version of the input sequence. These systems are naturally suited to be modeled using
TCNs  which by construction only operate on values of the input sequence for times within a ﬁnite
horizon into the past.
In this work  we aim to develop quantitative results for the approximation capability of TCNs for
modeling input-output maps that have the properties of causality  time invariance  and approximately
ﬁnite memory. In Section 2  we introduce the necessary deﬁnitions and review the approximately
ﬁnite memory property due to Sandberg [1991]. Section 3 gives the main result for approximating
input-output maps by ReLU TCNs  together with a quantitative result on the equivalence between
approximately ﬁnite memory and a related notion of fading memory [Boyd and Chua  1985  Park and
Sandberg  1992]. These results are applied in Section 4 to recurrent models that are incrementally
stable [Tran et al.  2017]  i.e.  the inﬂuence of the initial condition is asymptotically negligible. We
show that incrementally stable recurrent models have approximately ﬁnite memory  and then use
this formalism to derive a generalization of the result of Miller and Hardt [2019]. We provide a
comparison in Section 5 to other architectures used for approximating input-output maps. All omitted
proofs are provided in the Supplementary Material.

Input-output maps and approximately ﬁnite memory

2
Let S denote the set of all real-valued sequences u = (ut)t∈Z+  where Z+ := {0  1  2  . . .}. An
input-output map (or i/o map  for short) is a nonlinear operator F : S → S that maps an input
sequence u ∈ S to an output sequence y = Fu ∈ S. (We are considering real-valued input and
output sequences for simplicity; all our results carry over to vector-valued sequences at the expense of
additional notation.) We will denote the application and the composition of i/o maps by concatenation.
In this paper  we are concerned with i/o maps F that are:

• causal — for any t ∈ Z+  u0:t = v0:t implies (Fu)t = (Fv)t  where u0:t := (u0  . . .   ut);
• time-invariant — for any k ∈ Z+ 

(cid:40)

for t ≥ k
for 0 ≤ t < k
 
where R : S → S is the right shift operator (Ru)t := ut−11{t≥1}.

(Fu)t−k 
0 

(FRku)t =

(cid:12)(cid:12)(Fu)t − (FWt mu)t

The key notion we will work with is that of approximately ﬁnite memory [Sandberg  1991]:
Deﬁnition 2.1. An i/o map F has approximately ﬁnite memory on a set of inputs M ⊆ S if for any
ε > 0 there exists m ∈ Z+  such that
sup
u∈M

(1)
where Wt m : S → S is the windowing operator (Wt mu)τ := uτ 1{max{t−m 0}≤τ≤t}. We will
denote by m∗
F(0) < ∞  then we say that F has ﬁnite memory on M. If F is causal and time-invariant  this is
If m∗
equivalent to the existence of an integer m ∈ Z+ and a nonlinear functional f : Rm+1 → R  such

F(ε) the smallest m ∈ Z+  for which (1) holds.

(cid:12)(cid:12) ≤ ε 

sup
t∈Z+

2

that f (0  . . .   0) = 0 and  for any u ∈ M and any t ∈ Z+ 

(Fu)t = f (ut−m  ut−m+1  . . .   ut) 

(2)

with the convention that us = 0 if s < 0. In this work  we will focus on the important case when f is
a feedforward neural net with rectiﬁed linear unit (ReLU) activations ReLU(x) := max{x  0}. That
is  there exist k afﬁne maps Ai : Rdi → Rdi+1 with d1 = m + 1 and dk+1 = 1  such that f is given
by the composition

f = Ak ◦ ReLU◦Ak−1 ◦ ReLU◦ . . . ◦ ReLU◦A1 

where  for any r ≥ 1  ReLU(x1  . . .   xr) := (ReLU(x1)  . . .   ReLU(xr)). Here  k is the depth
(number of layers) and max{d2  . . .   dk} is the width (largest number of units in any hidden layer).
Deﬁnition 2.2. An i/o map F is a ReLU temporal convolutional net (or ReLU TCN  for short) with
context length m if (2) holds for some feedforward ReLU neural net f : Rm+1 → R.
Remark 2.3. While such an F is evidently causal  it is generally not time-invariant unless
f (0  . . .   0) = 0.

3 The universal approximation theorem

In this section  we state and prove our main result: any causal and time-invariant i/o map that has
approximately ﬁnite memory and satisﬁes an additional continuity condition can be approximated
arbitrarily well by a ReLU temporal convolutional net. In what follows  we will consider i/o maps
with uniformly bounded inputs  i.e.  inputs in the set
M(R) := {u ∈ S : (cid:107)u(cid:107)∞ := sup
t∈Z+

for some R > 0.

|ut| ≤ R}

For any t ∈ Z+ and any u ∈ M(R)  the ﬁnite subsequence u0:t = (u0  . . .   ut) is an element of the
cube [−R  R]t+1 ⊂ Rt+1; conversely  any vector x ∈ [−R  R]t+1 can be embedded into M(R) by
setting us = xs1{0≤s≤t}. To any causal and time-invariant i/o map F we can associate the nonlinear
functional ˜Ft : Rt+1 → R deﬁned in the obvious way: for any x = (x0  x1  . . .   xt) ∈ Rt+1 

˜Ft(x) := (Fu)t 

where u ∈ S is any input such that us = xs for s ∈ {0  1  . . .   t} (the values of us for s > t can be
arbitrary by causality). We impose the following assumptions on F:
Assumption 3.1. The i/o map F has approximately ﬁnite memory on M(R).
Assumption 3.2. For any t ∈ Z+  the functional ˜Ft : Rt+1 → R is uniformly continuous on
[−R  R]t+1 with modulus of continuity

ωt F(δ) := sup

and inverse modulus of continuity
ω−1

(cid:110)|˜Ft(x) − ˜Ft(x(cid:48))| : x  x(cid:48) ∈ [−R  R]t+1 (cid:107)x − x(cid:48)(cid:107)∞ ≤ δ
t F (ε) := sup(cid:8)δ > 0 : ωt F(δ) ≤ ε(cid:9) .

(cid:111)

 

where (cid:107)x(cid:107)∞ := max0≤i≤t |xi| is the (cid:96)∞ norm on Rt+1.
The following qualitative universal approximation result was obtained by Sandberg [1991]: if a causal
and time-invariant i/o map F satisﬁes the above two assumptions  then  for any ε > 0  there exists an
afﬁne map A : Rm+1 → Rd and a lattice map (cid:96) : Rd → R  such that

(cid:12)(cid:12)(Fu)t − (cid:96) ◦ A(ut−m:t)(cid:12)(cid:12) < ε 

sup

u∈M(R)

sup
t∈Z+

(3)

where we say that a map (cid:96) : Rd → R is a lattice map if (cid:96)(x0  . . .   xd−1) is generated from
x = (x0  . . .   xd−1) by a ﬁnite number of min and max operations that do not depend on x. Any
lattice map can be implemented using ReLU units  so (3) is a ReLU TCN approximation guarantee.
Our main result is a quantitative version of Sandberg’s theorem:

3

Theorem 3.3. Let F be a causal and time-invariant i/o map satisfying Assumptions 3.1 and 3.2. Then 
F(γε) 

for any ε > 0 and any γ ∈ (0  1)  there exists a ReLU TCN(cid:98)F with context length m = m∗
width m + 2  and depth(cid:0)

(cid:1)m+2  such that

O(R)

−1
m F((1−γ)ε)

ω

(cid:107)Fu −(cid:98)Fu(cid:107)∞ < ε.

sup

u∈M(R)

(4)

Proof. Let m = m∗

Remark 3.4. The role of the additional parameter γ ∈ (0  1) is to trade off the context length and
the depth of the ReLU TCN.

Remark 3.5. While the approximating ReLU TCN(cid:98)F is clearly causal  it may not be time-invariant
unless (cid:98)f (0  . . .   0) = 0  where (cid:98)f is the ReLU net constructed in the proof below.
there exists a ReLU net (cid:98)f : Rm+1 → R of width m + 2 and depth(cid:0)
(cid:1)m+2  such that
|˜Fm(x) − (cid:98)f (x)| < (1 − γ)ε
[Hanin and Sellke  2018]. Consider the TCN(cid:98)F deﬁned by (Fu)t := (cid:98)f (ut−m  . . .   ut). Fix an input

F(γε). Since ˜Fm : Rm+1 → R is continuous with modulus of continuity ωm F(·) 

u ∈ M(R) and consider two cases:
1) If t ≥ m  then ut−m:t = (Lt−mWt mu)0:m  where L : S → S is the left shift operator (Lu)t :=
ut+1. Therefore 

−1
m F((1−γ)ε)

x∈[−R R]m+1

sup

O(R)

ω

(FWt mu)t

(a)

= (FRt−mLt−mWt mu)t

(b)

= (FLt−mWt mu)m

(c)
= ˜Fm(ut−m:t) 

where (a) uses the fact that t ≥ m  (b) is by time invariance of F  and (c) is by the deﬁnition of ˜Fm.
2) If t < m  then ut−m:t = (Rm−tWt mu)0:m (recall the convention that  for any v  we set vs = 0
whenever s < 0). Therefore

(FWt mu)t

(a)

= (Rm−tFWt mu)m

(b)

= (FRm−tWt mu)m

(c)
= ˜Fm(ut−m:t) 

where (a) uses the fact that m > t  (b) is by time invariance  and (c) is by the deﬁnition of ˜Fm.
In either case  the triangle inequality gives

|(Fu)t − ((cid:98)Fu)t| ≤ |(Fu)t − (FWt mu)t| + |(FWt mu)t − ((cid:98)Fu)t|

= |(Fu)t − (FWt mu)t| + |˜Fm(ut−m:t) − (cid:98)f (ut−m:t)|

< γε + (1 − γ)ε = ε.

Since this holds for all t and all u with (cid:107)u(cid:107)∞ ≤ R  the result follows.

3.1 The fading memory property
F(·) and on the modulus of
In order to apply Theorem 3.3  we need control on the context length m∗
continuity ωt F(·). In general  these quantities are difﬁcult to estimate. However  it was shown by
Park and Sandberg [1992] that the property of approximately ﬁnite memory is closely related to the
notion of fading memory  ﬁrst introduced by Boyd and Chua [1985]. Intuitively  an i/o map F has
fading memory if the outputs at any time t due to any two inputs u and v that were close to one
another in recent past will also be close.
Let W denote the subset of S consisting of all sequences w  such that wt ∈ (0  1] for all t and wt ↓ 0
as t → ∞. We will refer to the elements of W as weighting sequences. Then we have the following
deﬁnition  due to Park and Sandberg [1992]:
Deﬁnition 3.6. We say that an i/o map F has fading memory on M ⊆ S with respect to w ∈ W if for
any ε > 0 there exists δ > 0 such that  for all u  v ∈ M and all t ∈ Z+ 

s∈{0 ... t} wt−s|us − vs| < δ =⇒ |(Fu)t − (Fv)t| < ε.

max

(5)

4

The weighting sequence w governs the rate at which the past values of the input are discounted in
determining the current output. To capture the best trade-offs in (5)  we will also use a w-dependent
modulus of continuity:

(cid:110)|(Fu)t − (Fv)t| : t ∈ Z+  u  v ∈ M  max

αw F(δ) := sup

s∈{0 ... t} wt−s|us − vs| ≤ δ

(cid:111)

.

It was shown by Park and Sandberg [1992] that an i/o map satisﬁes Assumptions 3.1 and (3.2) if and
only if it has fading memory with respect to some (and hence any) w ∈ W. The following result
provides a quantitative version of this equivalence:
Proposition 3.7. Let F be an i/o map.

1. If F satisﬁes Assumptions 3.1 and 3.2  then it has fading memory on M with respect to any

weighting sequence w ∈ W  and

w F(ε) ≥ wm∗
α−1

F (ε/3)ω−1
F (ε/3) F(ε/3).
m∗

(6)

2. If F has fading memory on M(R) with respect to some w ∈ W  then it has satisﬁes

Assumptions 3.1 and 3.2  and

(cid:110)
m ∈ Z+ : wm ≤ α−1

w F(ε)
R

(cid:111)

F(ε; R) ≤ inf
m∗

and

ωt F(δ) ≤ αw F(δ).

(7)

4 Recurrent systems
So far  we have considered arbitrary i/o maps F : S → S. However  many such maps admit state-space
realizations [Sontag  1998] — there exist a state transition map f : Rn × R → Rn  an output map
g : Rn → R  and an initial condition ξ ∈ Rn  such that the output sequence y = Fu is detemined
recursively by

xt+1 = f (xt  ut)

yt = g(xt)

(8a)
(8b)

with x0 = ξ. The i/o map F realized in this way is evidently causal  and it is time-invariant if
f (ξ  0) = ξ and g(ξ) = 0. In this section  we will identify the conditions under which recurrent
models satisfy Assumptions 3.1 and 3.2. Along the way  we will derive the approximation results of
Miller and Hardt [2019] as a special case.

4.1 Approximately ﬁnite memory and incremental stability
Consider the system in (8). Given any input u ∈ S  any ξ ∈ Rn  and any s  t ∈ Z+ with t ≥ s  we
s t(ξ) the state at time t when xs = ξ. Let M be a subset of S. We say that X ⊆ Rn
denote by ϕu
is a positively invariant set of (8) for inputs in M if  for all ξ ∈ X  all u ∈ M  and all 0 ≤ s ≤ t 
s t(ξ) ∈ X. We will be interested in systems with the following property [Tran et al.  2017]:
ϕu
Deﬁnition 4.1. The system (8) is uniformly asymptotically incrementally stable for inputs in M on a
positively invariant set X if there exists a function β : R+ × R+ → R+ of class KL1  such that the
inequality

(cid:107)ϕu

s t(ξ) − ϕu

s t(ξ(cid:48))(cid:107) ≤ β((cid:107)ξ − ξ(cid:48)(cid:107)  t − s)

(9)
holds for all inputs u ∈ M  all initial conditions ξ  ξ(cid:48) ∈ X  and all 0 ≤ s ≤ t  where (cid:107) · (cid:107) is the (cid:96)2
norm on Rn.
In other words  a system is incrementally stable if the inﬂuence of any initial condition in X on the
state trajectory is asymptotically negligible. A key consequence is the following estimate:

1A function β : R+ × R+ → R+ is of class KL if it is continuous and strictly increasing in its ﬁrst argument 
continuous and strictly decreasing in its second argument  β(0  t) = 0 for any t  and limt→∞ β(r  t) = 0 for
any r [Sontag  1998].

5

Proposition 4.2. Let u  ˜u be two input sequences in M. Then  for any ξ ∈ X and any t ∈ Z+ 

(cid:107)ϕu

0 t(ξ) − ϕ ˜u

0 t(ξ)(cid:107) ≤ t−1(cid:88)

s=0

β(cid:0)(cid:107)f (˜xs  us) − f (˜xs  ˜us)(cid:107)  t − s − 1(cid:1)  

where xs and ˜xs denote the states at time s due to inputs u and ˜u  respectively  with x0 = ˜x0 = ξ.
Consider a state-space model (8) with a positively invariant set X  with the following assumptions:
Assumption 4.3. The state transition map f (x  u) is Lf -Lipschitz in u for all x ∈ X and the output
map g(x) is Lg-Lipschitz in x ∈ X.
Assumption 4.4. For any initial condition ξ ∈ X there exists a compact set Sξ ⊆ X such that
0 t(ξ) ∈ Sξ for all u ∈ M(R) and all t ∈ Z+.
ϕu
Assumption 4.5. The system (8) is uniformly asymptotically incrementally stable on X for inputs in
(cid:88)
M(R)  and the function β in (9) satisﬁes the summability condition

β(C  t) < ∞

t∈Z+

for any C ≥ 0. (For example  if β(C  k) = Ck−α for some α > 1  then this condition is satisﬁed.)
We are now in position to prove the main result of this section:
Theorem 4.6. Suppose that Assumptions 4.3–4.5 are satisﬁed. Then the i/o map F of the system (8)
satisﬁes Assumptions 3.1 and 3.2 with

F(ε) ≤ min
m∗

m ∈ Z+ :

β(diam(Sξ)  k) < ε/Lg

(cid:110)

(cid:88)

k≥m

(cid:111)

and

ωt F(δ) ≤ Lg

t−1(cid:88)

s=0

β(Lf δ  s) 

∀t ∈ Z+.

Proof. Fix some t  m ∈ Z+. For an arbitrary input u ∈ M(R)  let ˜u = Wt mu  where we may
assume without loss of generality that t ≥ m. Then ˜us = us1{t−m≤s≤t}  and therefore

β(cid:0)(cid:107)f (˜xs  us) − f (˜xs  ˜us)(cid:107)  t − s − 1(cid:1) =

β(cid:0)(cid:107)f (˜xs  us) − f (˜xs  0)(cid:107)  t − s − 1(cid:1)

t−1(cid:88)

s=0

(10)

(11)

(12)

(13)

t−m−1(cid:88)
≤ t−m−1(cid:88)
∞(cid:88)

s=0

s=0

≤

β(diam(Sξ)  t − s − 1)

β(diam(Sξ)  s).

(14)

By the summability condition (11)  the summation in (14) converges to 0 as m ↑ ∞. Thus  if we
choose m so that the right-hand side of (14) is smaller than ε/Lg  it follows from Proposition 4.2 that

s=m

|(Fu)t − (FWt mu)t| = |g(ϕu

0 t(ξ)) − g(ϕ ˜u

0 t(ξ))| ≤ Lg(cid:107)ϕu

0 t(ξ) − ϕ ˜u

0 t(ξ)| < ε.

This proves (12). Now ﬁx any two u  ˜u ∈ M(R) with (cid:107)u0:t − ˜u0:t(cid:107)∞ < δ. Then
max0≤s≤t (cid:107)f (x  us) − f (x  ˜us)(cid:107) ≤ Lf δ for all x ∈ X  so Proposition 4.2 gives

|˜Ft(u0:t) − ˜Ft(˜u0:t)| = |g(ϕu
≤ Lg(cid:107)ϕu
≤ Lg

0 t(ξ)) − g(ϕ ˜u
0 t(ξ) − ϕ ˜u
t−1(cid:88)

β(Lf δ  s) 

0 t(ξ))|
0 t(ξ)(cid:107)

which proves (13).

s=0

6

4.2 Exponential incremental stability and the Demidovich criterion
Miller and Hardt [2019] consider the case of contracting systems: there exists some λ ∈ (0  1) and a
set U ⊆ Rm  such that

(cid:107)f (x  u) − f (x(cid:48)  u)(cid:107) ≤ λ(cid:107)x − x(cid:48)(cid:107)

(15)
for all x  x(cid:48) ∈ Rn and all u ∈ U. Such a system is uniformly exponentially incrementally stable on
any positively invariant set X  with β(C  t) = Cλt. In this section  we obtain their result as a special
case of a more general stability criterion  known in the literature on nonlinear system stability as the
Demidovich criterion [Pavlov et al.  2006]. The following result is a simpliﬁed version of a more
general result of Tran et al. [2017]:
Proposition 4.7 (the discrete-time Demidovich criterion). Consider the recurrent system (8) with a
convex positively invariant set X  where the state transition map f (x  u) is differentiable in x for any
u ∈ U. Suppose that there exists a symmetric positive deﬁnite matrix P and a constant µ ∈ (0  1) 
such that

the system (8) is uniformly exponentially incrementally stable with β(C  t) =(cid:112)κ(P )Cµt/2  where

(16)
∂x f (x  u) is the Jacobian of f (·  u) with respect to x. Then

for all x ∈ X and all u ∈ U  where ∂

f (x  u)(cid:62)P

f (x  u) − µP (cid:22) 0

∂
∂x

∂
∂x

κ(P ) is the condition number of P .
Proof. Fix any u ∈ U and ξ  ξ(cid:48) ∈ X  and deﬁne the function Φ : [0  1] → R by
Φ(s) := (f (ξ  u) − f (ξ(cid:48)  u))(cid:62)P f (sξ + (1 − s)ξ(cid:48)  u).

Then

Φ(1) − Φ(0) = (f (ξ  u) − f (ξ(cid:48)  u))(cid:62)P (f (ξ  u) − f (ξ(cid:48)  u)).

By the mean-value theorem  there exists some ¯s ∈ [0  1]  such that
= (f (ξ  u) − f (ξ(cid:48)  u))(cid:62)P

Φ(1) − Φ(0) =

Φ(s)

(cid:12)(cid:12)(cid:12)s=¯s

d
ds

f ( ¯ξ  u)(ξ − ξ(cid:48)) 

∂
∂x

where ¯ξ = ¯sξ + (1 − ¯s)ξ(cid:48) ∈ X  since X is convex. From (16)  (17)  and (18) it follows that

(f (ξ  u) − f (ξ(cid:48)  u))(cid:62)P (f (ξ  u) − f (ξ(cid:48)  u))

≤ (ξ − ξ(cid:48))(cid:62) ∂
f ( ¯ξ  u)(cid:62)P
∂x
≤ µ(ξ − ξ(cid:48))(cid:62)P (ξ − ξ(cid:48)).

∂
∂x

f ( ¯ξ  u)(ξ − ξ(cid:48))

Deﬁne the function V : X × X → R+ by V (ξ  ξ(cid:48)) := (ξ − ξ(cid:48))(cid:62)P (ξ − ξ(cid:48)). From the above estimate 
it follows that V is a Lyapunov function for the dynamics  i.e.  for any u ∈ U and ξ  ξ(cid:48) ∈ X 

V (f (ξ  u)  f (ξ(cid:48)  u)) ≤ µV (ξ  ξ(cid:48)).

Consequently  for any input u with ut ∈ U for all t and any ξ  ξ(cid:48) ∈ X 
0 t(ξ)  ut)  f (ϕu
0 t(ξ(cid:48))).
0 t(ξ)  ϕu

0 t+1(ξ(cid:48))) = V (f (ϕu
≤ µV (ϕu

0 t+1(ξ)  ϕu

V (ϕu

0 t(ξ(cid:48))  ut))

Iterating  we obtain the inequality V (ϕu

(cid:107)ϕu

0 t(ξ) − ϕu

0 t(ξ)  ϕu
0 t(ξ)(cid:107)2 ≤ λmax(P )
λmin(P )

0 t(ξ(cid:48))) ≤ µtV (ξ  ξ(cid:48)). Finally  since P (cid:31) 0 

µt(cid:107)ξ − ξ(cid:48)(cid:107)2 = κ(P )(cid:107)ξ − ξ(cid:48)(cid:107)2µt 

(17)

(18)

(19)

and the proof is complete.
Theorem 4.8. Suppose the system (8) satisﬁes Assumption 4.3 and the Demidovich criterion with
U = [−R  R]  its positively invariant set X contains 0  and f (0  0) = 0. Then its i/o map F with zero
initial condition x0 = 0 satisﬁes Assumptions 3.1 and 3.2 with

(cid:112)κ(P )Lf Lgδ
1 − √

µ

.

(20)

F(ε) ≤ 2 log( 2κ(P )Lf LgR
µ)2ε )
m∗

(1−√
log 1
µ

and

ωt F(δ) ≤

7

Proof. Since P is symmetric and positive deﬁnite  (cid:107)x(cid:107)P :=
λmin(P )(cid:107) · (cid:107)2 ≤ (cid:107) · (cid:107)2

P ≤ λmax(P )(cid:107) · (cid:107)2. Then  for all ξ ∈ X  u ∈ M(R)  and t 

x(cid:62)P x is a norm on Rn with

√

(cid:107)ϕu

0 t+1(ξ)(cid:107)P = (cid:107)f (ϕu
≤ (cid:107)f (ϕu
≤ √
µ(cid:107)ϕu

0 t(ξ)  ut)(cid:107)P
0 t(ξ)  ut) − f (0  ut)(cid:107)P + (cid:107)f (0  ut) − f (0  0)(cid:107)P

0 t(ξ)(cid:107)P +(cid:112)λmax(P )Lf R 
0 t(ξ)(cid:107)P ≤ √
(cid:107)ϕu

µ(cid:107)ξ(cid:107)P +

where we have used the Lyapunov bound (19). Unrolling the recursion gives the estimate

(cid:112)λmax(P )Lf R
1 − √
Thus  Assumption 4.4 is satisﬁed  where Sξ is the ball of (cid:96)2-radius(cid:112)κ(P )

(cid:16)(cid:107)ξ(cid:107) + Lf R

centered
at 0. Assumption 4.5 is also satisﬁed by Proposition 4.7. The estimates in (20) follow from
Theorem 4.6.

u∈M(R)

sup
t∈Z+

1−√

µ

(cid:17)

sup

µ

.

The following result now follows as a direct consequence of Theorems 3.3 and 4.8:
Corollary 4.9. If the system (8) satisﬁes the conditions of Theorem 4.8  then its i/o map F with zero

initial condition can be ε-approximated in the sense of Theorem 3.3 by a ReLU TCN(cid:98)F with width

polylog( 1

ε ) and depth quasipoly( 1

ε ).2

4.3 Contractivity vs. the Demidovich criterion

If the contractivity condition (15) holds and f (x  u) is differentiable in x  then the Demidovich
criterion is satisﬁed with P = In and µ = λ2. In that case  we immediately obtain the exponential
estimate β(C  t) ≤ Cλt. However  the Demidovich criterion covers a wider class of nonlinear
systems. As an example  consider a discrete-time nonlinear system of Lur’e type (cf. Sandberg and
Xu [1993]  Kim and Braatz [2014]  Sarkans and Logemann [2016] and references therein):

xt+1 = Axt + Bψ(ut − yt)

(21a)
(21b)
Here  the state xt is n-dimensional while the input ut and the output yt are scalar  so A ∈ Rn×n 
B ∈ Rn×1  and C ∈ R1×n. The map ψ : R → R is a ﬁxed differentiable nonlinearity. The system in
(21) has the form (8) with f (x  u) = Ax + Bψ(u − Cx) and g(x) = Cx  and can be realized as the
negative feedback interconnection of the discrete-time linear system

yt = Cxt

yt = Cxt

xt+1 = Axt + Bvt

(22a)
(22b)
and the nonlinear element ψ using the feedback law vt = ψ(ut − yt). We make the following
assumptions (see  e.g.  Sontag [1998] for the requisite control-theoretic background):
Assumption 4.10. The nonlinearity ψ : R → R satisﬁes ψ(0) = 0  and there exist real numbers
−∞ < a ≤ b < ∞ such that a ≤ ψ(cid:48)(·) ≤ b.
Assumption 4.11. A is a Schur matrix  i.e.  its spectral radius ρ(A) is strictly smaller than 1; the
pair (A  B) is controllable  i.e.  the n × n matrix [B | AB | . . . | An−1B] has rank n; and the pair
(A  C) is observable  i.e.  the n × n matrix [C(cid:62) | A(cid:62)C(cid:62) | . . . | (A(cid:62))n−1C(cid:62)] has rank n.
Assumption 4.12. Let T := {z ∈ C : |z| = 1} denote the unit circle in the complex plane. The
rational function G(z) := C(zIn − A)−1B satisﬁes
(cid:107)G(cid:107)H∞(T) := sup
z∈T
for some γ > 0 such that r2 ≤ γ2 for all a ≤ r ≤ b.

|G(z)| < γ−1

(23)

2We say that a given quantity N has quasipolynomial growth in 1/ε  and write N ≤ quasipoly(1/ε)  if

N = O(exp(polylog( 1

ε ))).

8

Remark 4.13. Assumption 4.10 imposes a slope condition on ψ and is standard in the analysis of
Lur’e systems [Tsypkin  1964  Sandberg  1991  Kim and Braatz  2014]. The function G(z) is the
transfer function of the linear system (22). Assumption 4.11 states that the triple (A  B  C) is a
minimal realization of G. The quantity (cid:107)G(cid:107)H∞(T) appearing in Eq. (23) in Assumption 4.12 is the
H∞-norm of G on the unit circle in the complex plane. Assumptions 4.11 and 4.12 are also common
and are in the spirit of the well-known circle criterion [Tsypkin  1964  Sandberg and Xu  1993].

With these preliminaries out of the way  we have the following:
Proposition 4.14. Suppose that the system (21) satisﬁes Assumptions 4.10–4.12. Then it satisﬁes the
discrete-time Demidovich criterion with X = Rn and U = R  and moreover µ > ρ(A)2.
The crucial ingredient in the proof is the Discrete-Time Bounded-Real Lemma [Vaidyanathan  1985] 
which guarantees the existence of the matrix P appearing in the Demidovich criterion. The main
takeaway here is that the function f (x  u) = Ax + Bψ(u − Cx) need not be contractive (i.e.  it may
be the case that P (cid:54)= In)  but it will be contractive in the (cid:107) · (cid:107)P norm.

5 Comparison of architectures

So far  we have shown that any i/o map F with approximately ﬁnite memory can be approximated
by a ReLU temporal convolutional net. We have also considered recurrent models and shown
that any incrementally stable recurrent model has approximately ﬁnite memory and can therefore
be approximated by a ReLU TCN. As far as their approximation capabilities are concerned  both
recurrent models and autoregressive models like TCNs are equivalent  since any ﬁnite-memory i/o
map of the form (2) admits the state-space realization

x1
t+1 = x2
yt = f (x1

t   x2
t   x2

t+1 = x3
t   . . .   xm

t   . . .   xm−1
t   ut)

t+1 = xm

t   xm

t+1 = ut

0  . . .   xm

of the tapped delay line type  with zero initial condition (x1
0 ) = (0  . . .   0). (Compared
to (8)  we are allowing a direct ‘feedthrough’ connection from the input ut to the output yt.) The
advantage of autoregressive models like TCNs shows up during training and regular operation  since
shifted copies of the input sequence can be efﬁciently processed in parallel rather than sequentially.
Another point worth mentioning is that  while the construction in the proof of Theorem 3.3 makes
use of ReLU nets as a universal function approximator  any other family of universal approximators
can be used instead  for example  multivariate polynomials or rational functions. In fact  if one uses
multivariate polynomials to approximate the functionals ˜Ft  the resulting family of i/o maps is known
as the (discrete-time) ﬁnite Volterra series [Boyd and Chua  1985]  and has been used widely in the
analysis of nonlinear systems. However  TCNs generally provide a more parsimonious representation.
To see this  consider the following (admittedly contrived) example of an i/o map:

(cid:32) ∞(cid:88)

(cid:32) m(cid:88)

(cid:33)

(cid:33)

(Fu)t = ReLU

(24)
where the ﬁlter coefﬁcients ht have the exponential decay property |ht| ≤ Cλt for some C > 0
and λ ∈ (0  1). It is not hard to show that F has exponentially fading memory  and a very simple
ε-approximation by a TCN is obtained by zeroing out all of the ﬁlter coefﬁcients hs  s > m ∼ log( 1
ε ):

hsut−s

s=0

 

((cid:98)Fu)t = ReLU

hsut−s

.

s=0

However  any ε-approximation for F using Volterra series would need poly( 1
ε ) terms  since the best
polynomial ε-approximation of the ReLU on any compact interval has degree Ω( 1
ε ) [DeVore and
Lorentz  1993  Chap. 9  Thm. 3.3]. On the other hand  if we consider an i/o map of the form (24)  but
with a degree-d univariate polynomial instead of ReLU  then we can ε-approximate it with a TCN of
depth O(d + log d

ε ) units [Liang and Srikant  2017].

ε ) and O(d log d

Acknowledgments

This work was supported in part by the National Science Foundation under the Center for Advanced
Electronics through Machine Learning (CAEML) I/UCRC award no. CNS-16-24811.

9

References
Shaojie Bai  J. Zico Kolter  and Vladlen Koltun. An empirical evaluation of generic convolutional and
recurrent networks for sequence modeling  2018. URL https://arxiv.org/abs/1803.01271.

Stephen Boyd and Leon O. Chua. Fading memory and the problem of approximating nonlinear
operators with Volterra series. IEEE Transactions on Circuits and Systems  CAS-32(11):1150–
1161  1985.

Ciprian Chelba  Mohammad Norouzi  and Samy Bengio. N-gram language modeling using recurrent

neural network estimation  2017. URL https://arxiv.org/abs/1703.10724.

Yann N. Dauphin  Angela Fan  Michael Auli  and David Grangier. Language modeling with gated

convolutional networks. In International Conference on Machine Learning  2017.

Ronald A. DeVore and George G. Lorentz. Constructive Approximation. Springer-Verlag  Berlin 

1993.

Jonas Gehring  Michael Auli  David Grangier  Denis Yarats  and Yann N. Dauphin. Convolutional

sequence to sequence learning. In International Conference on Machine Learning  2017.

Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width 

2018. URL http://arxiv.org/abs/1710.11278.

Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text categoriza-
tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers)  pages 562–570  Vancouver  Canada  July 2017. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P17-1052. URL https://www.aclweb.org/anthology/
P17-1052.

Nal Kalchbrenner  Lasse Espeholt  Karen Simonyan  Aaron van den Oord  Alex Graves  and Koray
Kavukcuoglu. Neural machine translation in linear time  2016. URL https://arxiv.org/abs/
1610.10099.

Kwang-Ki K. Kim and Richard D. Braatz. Observer-based output feedback control of discrete-time
Lur’e systems with sector-bounded slope-restricted nonlinearities. International Journal of Robust
and Nonlinear Control  24:2458–2472  2014.

Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? In International

Conference on Learning Representations  2017.

John Miller and Moritz Hardt. Stable recurrent models. In International Conference on Learning

Representations  2019.

Jooyoung Park and Irwin W. Sandberg. Criteria for the approximation of nonlinear systems. IEEE
Transactions on Circuits and Systems — I: Fundamental Theory and Applications  39(8):673–676 
1992.

Alexey Pavlov  Nathan van de Wouw  and Henk Nijmeijer. Uniform Output Regulation of Nonlinear

Systems: A Convergent Dynamics Approach. Birkhäuser  2006.

Irwin W. Sandberg. Structure theorems for nonlinear systems. Multidimensional Systems and Signal

Processing  2:267–286  1991.

Irwin W. Sandberg and Lilian Y. Xu. Steady-state errors in discrete-time control systems. Automatica 

29(2):523–526  1993.

Elvijs Sarkans and Hartmut Logemann. Input-to-state stability of discrete-time Lur’e systems. SIAM

Journal on Control and Optimization  54(3):1739–1768  2016.

Vatsal Sharan  Sham Kakade  Percy Liang  and Gregory Valiant. Prediction with a short memory. In

Symposium on Theory of Computing  2018.

Eduardo D. Sontag. Mathematical Control Theory: Deterministic ﬁnite Dimensional Systems.

Springer-Verlag  1998.

10

Duc N. Tran  Björn S. Rüfﬂer  and Christopher M. Kellett. Convergence properties for discrete-time

nonlinear systems. IEEE Transactions on Automatic Control  2017.

Yakov Z. Tsypkin. A criterion of absolute stability for sampled-data systems with monotone
characteristics of the nonlinear element. Doklady Akademii Nauk SSSR  155(5):1029–1032  1964.
In Russian.

Palghat P. Vaidyanathan. The discrete-time bounded-real lemma in digital ﬁltering. IEEE Transactions

on Circuits and Systems  CAS-32(9):918–924  September 1985.

Aaron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex Graves 
Nal Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio  2016. URL https://arxiv.org/abs/1609.03499.

Yonghui Wu  Mike Schuster  Zhifeng Chen  Quoc V. Le  Mohammad Norouzi  Wolfgang Macherey 
Maxim Krikun  Yuan Cao  Qin Gao  Klaus Macherey  Jeff Klingner  Apurva Shah  Melvin Johnson 
Xiaobing Liu  Łukasz Kaiser  Stephan Gouws  Yoshikiyo Kato  Taku Kudo  Hideto Kazawa  Keith
Stevens  George Kurian  Nishant Patil  Wei Wang  Cliff Young  Jason Smith  Jason Riesa  Alex
Rudnick  Oriol Vinyals  Greg Corrado  Macduff Hughes  and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation  2016.

Wenpeng Yin  Katharina Kann  Mo Yu  and Hinrich Schütze. Comparative study of CNN and RNN

for natural language processing  2017. URL https://arxiv.org/abs/1702.01923.

11

,Joshua Hanson
Maxim Raginsky