2010,Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition,Recent studies have shown that multiple kernel learning is very effective for object recognition  leading to the popularity  of kernel learning in computer vision problems. In this work  we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under  consideration  share the same combination of kernel functions  and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL  their computational cost is linear in the number of classes  making them unscalable when the number of classes is large  a challenge  frequently  encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is $O(m^{1/3}\sqrt{ln m})$  where $m$ is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy  the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL.,Multi-label Multiple Kernel Learning by Stochastic

Approximation: Application to Visual Object Recognition

Serhat S. Bucak∗

Rong Jin∗

Anil K. Jain∗†

bucakser@cse.msu.edu

rongjin@cse.msu.edu

jain@cse.msu.edu

Dept. of Comp. Sci. & Eng.∗
Michigan State University

East Lansing  MI 48824 U.S.A.

Dept. of Brain & Cognitive Eng.†
Korea University  Anam-dong 

Seoul  136-713  Korea

Abstract

Recent studies have shown that multiple kernel learning is very effective for object recognition 
leading to the popularity of kernel learning in computer vision problems. In this work  we develop
an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the
classes under consideration share the same combination of kernel functions  and the objective is
to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms
have been developed for ML-MKL  their computational cost is linear in the number of classes 
making them unscalable when the number of classes is large  a challenge frequently encountered
in visual object recognition. We address this computational challenge by developing a framework
for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis
shows that the complexity of our algorithm is O(m1/3√lnm)  where m is the number of classes.
Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy 
the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL.

1

Introduction

Recent studies have shown promising performance of kernel methods for object classiﬁcation  recognition and local-
ization [1]. Since the choice of kernel functions can signiﬁcantly affect the performance of kernel methods  kernel
learning  or more speciﬁcally Multiple Kernel Learning (MKL) [2  3  4  5  6  7]  has attracted considerable amount
of interest in computer vision community. In this work  we focuss on kernel learning for object recognition because
the visual content of an image can be represented in many ways  depending on the methods used for keypoint detec-
tion  descriptor/feature extraction  and keypoint quantization. Since each representation leads to a different similarity
measure between images (i.e.  kernel function)  the related fusion problem can be cast into a MKL problem.

A number of algorithms have been developed for MKL. In [2]  MKL is formulated as a quadratically constraint
quadratic program (QCQP). [8] suggests an algorithm based on sequential minimization optimization (SMO) to im-
prove the efﬁciency of [2]. [9] shows that MKL can be formulated as a semi-inﬁnite linear program (SILP) and can
be solved efﬁciently by using off-the-shelf SVM implementations. In order to improve the scalability of MKL  several
ﬁrst order optimization methods have been proposed  including the subgradient method [10]  the level method [11]  the
method based on equivalence between group lasso and MKL [12  13  14]. Besides L1-norm [15] and L2-norm [16] 
Lp-norm [17] has also been proposed to regularize the weights for kernel combination. Other then the framework
based on maximum margin classiﬁcation  MKL can also be formulated by using kernel alignment [18] and Fisher
discriminative analysis frameworks [19].

1

Although most efforts in MKL focus on binary classiﬁcation problems  several recent studies have attempted to extend
MKL to multi-class and multi-label learning [3  20  21  22  23]. Most of these studies assume that either the same or
similar kernel functions are used by different but related classiﬁcation tasks. Even though studies show that MKL for
multi-class and multi-label learning can result in signiﬁcant improvement in classiﬁcation accuracy  the computational
cost is often linear in the number of classes  making it computationally expensive when dealing with a large number of
classes. Since most object recognition problems involve many object classes  whose number might go up to hundreds
or sometimes even to thousands  it is important to develop an efﬁcient learning algorithm for multi-class and multi-
label MKL that is sublinear in the number of classes.

In this work  we develop an efﬁcient algorithm for Multi-Label MKL (ML-MKL) that assumes all the classiﬁers
share the same combination of kernels. We note that although this assumption signiﬁcantly constrains the choice of
kernel functions for different classes  our empirical studies with object recognition show that it does not affect the
classiﬁcation performance. A similar phenomenon was also observed in [21]. A naive implementation of ML-MKL
with shared kernel combination will lead to a computational cost linear in the number of classes. We alleviate this
computational challenge by exploring the idea of combining worst case analysis with stochastic approximation. Our
analysis reveals that the convergence rate of the proposed algorithm is O(m1/3√ln m)  which is signiﬁcantly better
than a linear dependence on m  where m is the number of classes. Our empirical studies show that the proposed MKL
algorithm yields similar performance as the state-of-the-art algorithms for ML-MKL  but with a signiﬁcantly shorter
running time  making it suitable for multi-label learning with a large number of classes.

The rest of this paper is organized as follows. Section 2 presents the proposed algorithm for Multi-Label MKL 
along with its convergence analysis. Section 3 summarizes the experimental results for object recognition. Section 4
concludes this work.

2 Multi-label Multiple Kernel Learning (ML-MKL)

1   . . .   yk

i j = κa(xi  xj).

n)> ∈ {−1  +1}n  the assignment of the kth class to all the training instances: yk

We denote by D = {x1  . . .   xn} the collection of n training instances  and by m the number of classes. We introduce
i = +1 if xi is
yk = (yk
assigned to the k-th class and yk
i = −1 otherwise. We introduce κa(x  x0) : Rd × Rd 7→ R  a = 1  . . .   s  the s kernel
functions to be combined. We denote by {Ka ∈ Rn×n  a = 1  . . .   s} the collection of s kernel matrices for the data
points in D  i.e.  K a
We introduce p = (p1  . . .   ps)  a probability distribution  for combining kernels. We denote by K(p) =Ps
a=1 paKa
the combined kernel matrices. We introduce the domain P for the probability distribution p  i.e.  P = {p ∈ Rs
+ :
p>1 = 1}. Our goal is to learn from the training examples the optimal kernel combination p for all the m classes.
The simplest approach for multi-label multiple kernel learning with shared kernel combination is to ﬁnd the optimal
kernel combination p by minimizing the sum of regularized loss functions of all m classes  leading to the following
optimization problem:

min
p∈P

min

{fk∈H(p)}m

k=1( mXk=1

Hk =

mXk=1( 1

2|fk|2

H(p) +

nXi=1

i fk(xi)(cid:1)))  
`(cid:0)yk

where `(z) = max(0  1 − z) and H(p) is a Reproducing Kernel Hilbert Space endowed with kernel κ(x  x0; p) =
Ps
a=1 paκa(x  x0). Hk is the regularized loss function for the kth class. It is straightforward to verify the following
dual problem of (1):

(1)

(2)

α∈Q1(L(p  α) =

max

min
p∈P

mXk=1(cid:26)[αk]>1 −

(αk ◦ yk)>K(p)(αk ◦ yk)(cid:27))  

1
2

where Q1 =(cid:8)α = (α1  . . .   αm) : αk ∈ [0  C]n  k = 1  . . .   m(cid:9). To solve the optimization problem in Eq. (2)  we
can view it as a minimization problem  i.e.  minp∈P A(p)  where A(p) = maxα∈Q1 L(p  α). We then follow the
subgradient descent approach in [10] and compute the gradient of A(p) as

∂pi A(p) = −

1
2

mXk=1

(αk(p) ◦ yk)>Ki(αk(p) ◦ yk) 

2

where αk(p) = arg maxα∈[0 C]n[αk]>1 − (αk ◦ yk)>K(p)(αk ◦ yk). We refer to this approach as Multi-label
Multiple Kernel Learning by Sum  or ML-MKL-Sum. Note that this approach is similar to the one proposed
in [21]. The main computational problem with ML-MKL-Sum is that by treating every class equally  in each iteration
of subgradient descent  it requires solving m kernel SVMs  making it unscalable to a very large number of classes.
Below we present a formulation for multi-label MKL whose computational cost is sublinear in the number of classes.

2.1 A Minimax Framework for Multi-label MKL

In order to alleviate the computational difﬁculty arising from a large number of classes  we search for the combined
kernel matrix K(p) that minimizes the worst classiﬁcation error among m classes  i.e. 

min
p∈P

min

{fk∈H(p)}m

k=1

max
1≤k≤m

Hk

(3)

Eq. (3) differs from Eq. (1) in that it replacesPm
of using maxk Hk instead ofPk Hk is that by using an appropriately designed method  we may be able to ﬁgure

out the most difﬁcult class in a few iterations  and spend most of the computational cycles on learning the optimal
kernel combination for the most difﬁcult class. In this way  we are able to achieve a running time that is sublinear
in the number of classes. Below  we present an optimization strategy for Eq. (3) based on the idea of stochastic
approximation.

k=1 Hk with max1≤k≤m Hk. The main computational advantage

A direct approach is to solve the optimization problem in Eq. (3) by its dual form. It is straightforward to derive the
dual problem of Eq. (3) as follows (more details can be found in the supplementary documents)

where

min
p∈P

max

β∈BL(p  β) =( mXk=1(cid:26)[βk]>1 −
B =((β1  . . .   βm) : βk ∈ Rn

1
2

2)2
(βk ◦ yk)>K(p)(βk ◦ yk)(cid:27) 1
λk = 1) .
mXk=1

.

+  k = 1  . . .   m  βk ∈ [0  Cλk]n s.t.

The challenge in solving Eq. (4) is that the solutions {β1  . . .   βm} in domain B are correlated with each other  making
it impossible to solve each βk independently by an off-the-shelf SVM solver. Although a gradient descent approach
can be developed for optimizing Eq. (4)  it is unable to explore the sparse structure in βk making it less efﬁcient than
state-of-the-art SVM solvers. In order to effectively explore the power of off-the-shelf SVM solvers  we rewrite (3) as
follows

γ∈Γ (L(p  γ) = max

max

α∈Q1

min
p∈P

γk(cid:26)αk>

mXk=1

1
2

1 −

(αk ◦ yk)>K(p)(αk ◦ yk)(cid:27))  

(4)

(5)

(7)

where Γ = {(γ1  . . .   γm) ∈ Rm
using Eq. (5) is that we can resort to a SVM solver to efﬁciently ﬁnd αk for a given combination of kernels K(p).
Given Eq. (5)  we develop a subgradient descent approach for solving the optimization problem. In particular  in each
iteration of subgradient descent  we compute the gradient L(p  γ) with respect to p and γ as follows

+ : γ>1 = 1}. In Eq. (5)  we replace max1≤k≤m with maxγ∈Γ. The advantage of

∇paL(p  γ) = −

1
2

γk(αk ◦ yk)>Ka(αk ◦ yk)  ∇γkL(p  γ) = [αk]>1 −

1
2

(αk ◦ yk)>K(p)(αk ◦ yk) 

(6)

mXk=1

where αk = arg maxα∈[0 C]n α>1− (α◦ yk)>K(p)(α◦ yk)/2  i.e.  a SVM solution to the combined kernel K(p).
Following the mirror prox descent method [24]  we deﬁne potential functions Φp = ηp
a=1 pa ln pa for p and
Φγ =Pm

i=1 γi ln γi for γ  and have the following equations for updating pt and γt

ηγ Ps

pa
t+1 =

pa
t
Z p
t

exp(−ηp∇paL(pt  γt))  γk

t+1 =

exp(−ηγ∇γkL(pt  γt)) 

γk
t
Z γ
t

3

where Z p
optimizing p and γ  respectively.

t and Z γ

t are normalization factors that ensure p>

t 1 = γ>

t 1 = 1. ηp > 0 and ηγ > 0 are the step sizes for

Unfortunately  the algorithm described above shares the same shortcoming as the other approaches for multiple la-
bel multiple kernel learning  i.e.  it requires solving m SVM problems in each iteration  and therefore its compu-
tational complexity is linear in the number of classes. To alleviate this problem  we modify the above algorithm
by introducing the stochastic approximation method. In particular  in each iteration t  instead of computing the full
gradients that requirs solving m SVMs  we sample one classiﬁcation task according to the multinomial distribution
t ). Let jt be the index of the sampled classiﬁcation task. Using the sampled task jt  we estimate the
M ulti(γ1

t   . . .   γm

k (pt  γt)  as follows

1
2

a(pt  γt) andbgγ

a(pt  γt) = −

gradient of L(p  γ) with respect to pa and γk  denoted bybgp

bgp
k (pt  γt) = (cid:26)
bgγ
a(pt  γt) andbgγ
The computation ofbgp
a(pt  γt)] = ∇paL(pt  γt)  Et[bgγ
Et[bgp

(αjt ◦ yjt )>Ka(αjt ◦ yjt) 
γk(cid:0)α>

where Et[·] stands for the expectation over the randomly sampled task jt.

k 1 − 1

0

1

i (pt  γt) only requires αjt and therefore only needs to solve one SVM problem 
instead of m SVMs. The key property of the estimated gradients in Eqs. (8) and (9) is that their expectations equal to
the true gradients  as summarized by Proposition 1. This property is the key to the correctness of this algorithm.
Proposition 1. We have

2 (αk ◦ yk)>K(p)(αk ◦ yk)(cid:1) k = jt

k 6= jt

(8)

(9)

.

i (pt  γt)] = ∇γiL(pt  γt) 

Given the estimated gradients  we will follow Eq. (7) for updating p and γ in each iteration. Since bgγ
proportional to 1/γt  to ensure the norm ofbgγ

t+1 
smoothing effect  without modifying γt+1  we will sample directly from γ0
δ
m

i (pt  γt) is
i (pt  γt) to be bounded  we need to smooth γt+1. In order to have the

t+1(1 − δ) +

  k = 1  . . .   m 

where δ > 0 is a small probability mass used for smoothing and

∀γ ∈ Γ ∃γ0 ∈ Γ0  s.t. γ0k
t+1 ← γk
Γ0 =(cid:26)γ0>1 = 1  γ0

k ≥

δ
m

  k = 1  . . .   m(cid:27) .

We refer to this algorithm as Multi-label Multiple Kernel Learning by Stochastic Approximation  or ML-MKL-
SA for short. Algorithm 1 gives the detailed description.

2.2 Convergence Analysis

Since Eq. (5) is a convex-concave optimization problem  we introduce the following citation for measuring the quality
of a solution (p  γ)

∆(p  γ) = max

γ 0∈Γ L(p  γ0) − min

0∈P L(p0  γ).

p

(11)

We denote by (p∗  γ∗) the optimal solution to Eq. (5).
Proposition 2. We have the following properties for ∆(p  γ)

1. ∆(p  γ) ≥ 0 for any solution p ∈ P and γ ∈ Γ
2. ∆ (p∗  γ∗) = 0
3. ∆(p  γ) is jointly convex in both p and γ

We have the following theorem for the convergence rate for Algorithm 1. The detailed proof can be found in the
supplementary document.

obtained by Algorithm 1

Theorem 1. After running Algorithm 1 over T iterations  we have the following inequality for the solutionbp andbγ

1
ηγT

(ln m + ln s) + ηγ(cid:18)d

m2
2δ2 λ2

0n2C 4 + n2C 2(cid:19)  

where d is a constant term  E[·] stands for the expectation over the sampled task indices of all iterations  and λ0 =
max
1≤a≤s

λmax(Ka)  where λmax(Z) stands for the maximum eigenvalue of matrix Z.

E [∆ (bp bγ)] ≤

4

Algorithm 1 Multi-label Multiple Kernel Learning: ML-MKL-SA
1: Input

• ηp  ηγ: step sizes
• K 1  . . .   K s: s kernel matrices
• y1  . . .   ym: the assignments of m different classes to n training instances
• T : number of iterations
• δ: smoothing parameter
• γ1 = 1/m and p1 = 1/s

2: Initialization

3: for t = 1  . . .   T do
4:
5:
6:
7:

Sample a classiﬁcation task jt according to the distribution M ulti(γ1
Compute αjt = arg maxα∈[0 C]n α>1 − (α ◦ yjt )>K(p)(α ◦ yjt )/2 using an off shelf SVM solver.
Update pt+1  γt+1 and γ0

i (pt  γt) using Eq. (8) and (9).

t   . . .   γm

t+1 as follows

t ).

Compute the estimated gradientsbgp

a(pt  γt))  a = 1  . . .   s.

pa
t+1 =

[γt+1]k =

a(pt  γt) andbgγ
exp(−ηγbgp
exp(ηγbgγ

pa
t
Z p
t
γk
t
Z γ
t

k (pt  γt))  k = 1  . . .   m; γ0

t+1 = (1 − δ)γt+1 +

δ
m

1.

8: end for

γt 

pt.

1
T

TXt=1

1
T

TXt=1

9: Compute the ﬁnal solutionbp andbα as
bγ =
3p(ln m)/T   after running Algorithm 1 (on the original paper) over T

iterations  we have E[∆(bp bγ)] ≤ O(nm1/3p(ln m)/T ) in terms of m n and T .
algorithm on the order of O(m1/3p(ln m)/T )  sublinear in the number of classes m.

Since we only need to solve one kernel SVM at each iteration  we have the computational complexity for the proposed

Corollary 1. With δ = m

3 and ηγ = 1

bp =

n m− 1

(10)

2

3 Experiments

In this section  we empirically evaluate the proposed multiple kernel learning algorithm2 by demonstrating its efﬁ-
ciency and effectiveness on the visual object recognition task.

3.1 Data sets

We use three benchmark data sets for visual object recognition: Caltech-101  Pascal VOC 2006 and Pascal VOC 2007.
Caltech-101 contains 101 different object classes in addition to a “background” class. We use the same settings as [25]
in which 30 instances of each class are used for training and 15 instances for testing. Pascal VOC 2006 data set [26]
consists of 5  303 images distributed over 10 classes  of which 2  618 are used for training. Pascal VOC 2007 [27]
consists of 5  011 training images and 4  932 test images that are distributed over 20 classes. For both data sets  we
used the default train-test partition provided by VOC Challenge. Unlike Caltech-101 data set  where each image is
assigned to one class  images in VOC data sets can be assigned to multiple classes simultaneously  making it more
suitable for multi-label learning.

2Codes can be downloaded from http://www.cse.msu.edu/˜bucakser/ML-MKL-SA.rar

5

Table 1: Classiﬁcation accuracy (AUC) and running times (second) of all ML-MKL algorithms on three data sets.
Abbreviations SA  GMKL  Sum  Simple  VSKL  AVG stand for ML-MKL-SA  Generalized MKL  ML-MKL-Sum 
SimpleMKL  variable sparsity kernel learning and average kernel  respectively

Accuracy (AUC)

dataset
CALTECH-101
VOC2006
VOC2007

SA
0.80
0.75
0.50

GMKL
0.79
0.75
0.49

Sum Simple VSKL AVG
0.77
0.80
0.72
0.74
0.47
0.45

0.78
0.74
0.42

0.77
0.74
0.46

SA
191.17
245.10
1329.40

GMKL
18292.00
2586.90
30333.14

Training Time (sec)
Simple
9869.40
11549.00
18536.37

Sum
1814.50
890.65
1372.60

VSKL
AVG
21266.05 N/A
7368.27
N/A
11370.48 N/A

1

0.8

0.6

0.4

0.2

i

s
t
n
e
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

1

0.8

0.6

0.4

0.2

i

s
t
n
e
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

1

0.8

0.6

0.4

0.2

i

s
t
n
e
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

500

1000
time(sec)

1500

ML-MKL-Sum

1

0.8

0.6

0.4

0.2

s
t
n
e
c
i
f
f

i

e
o
c
 
l

e
n
r
e
k

0

0

0.5

1

time(sec)

1.5

2
x 104

GMKL

0.5

1

time(sec)

1.5

2
x 104

VSKL

200

400

time(sec)

600

800

ML-MKL-SA

Figure 1: The evolution of kernel weights over time for CALTECH-101 data set. For GMKL and VSKL  the curves
display the kernel weights that are averaged over all the classes since a different kernel combination is learnt for each
class.

3.2 Kernels

We extracted 9 kernels for Caltech-101 data set by using the software provided in [28]. Three different feature
extraction methods are used for kernel construction: (i) GB: geometric blur descriptors are applied to the detected
keypoints [29]; RBF kernel is used in which the distance between two images is computed by averaging the distance
of the nearest descriptor pairs for the image pair. (ii) PHOW gray/color: keypoints based on dense sampling; SIFT
descriptors are quantized to 300 words and spatial histograms with 2x2 and 4x4 subdivisions are built to generate
chi-squared kernels [30]. (iii) SSIM: self-similarity features taken from [31] are used and spatial histograms based on
300 visual words are used to form the chi-squared kernel.
For VOC data sets  a different procedure  based on the reports of VOC challenges [1]  is used to construct multiple
visual dictionaries  and each dictionary results in a different kernel. To obtain multiple visual dictionaries  we deploy
(i) three keypoint detectors  i.e.  dense sampling  HARHES [32] and HESLAP [33]  (ii) two keypoint descriptors 
i.e.  SIFT [33] and SPIN [34])  (iii) two different numbers of visual words  i.e.  500 and 1  000 visual words  (iv)
two different kernel functions  i.e.  linear kernel and chi-squared kernel. The bandwidth of the chi-squared kernels
is calculated using the procedure in [25]. Using the above variants in visual dictionary construction  we constructed
22 kernels for both VOC2007 and VOC2006 data sets. In addition to the K-means implementation in [28]  we also
applied a hierarchical clustering algorithm [35] to descriptor quantization for VOC 2007 data set  leading to four more
kernels for VOC2007 data set.

3.3 Baseline Methods

We ﬁrst compare the proposed algorithm ML-MKL-SA to the following MKL algorithms that learn a different kernel
combination for each class: (i) Generalized multiple kernel learning method (GMKL) [25]  which reports promising
results for object recognition  (ii) SimpleMKL [10]  which learns the kernel combination by a subgradient approach
and (iii) Variable Sparsity Kernel Learning (VSKL)  a miror-prox descent based algorithm for MKL [36]. We also
compare ML-MKL-SA to ML-MKL-Sum  which learns a kernel combination shared by all classes as described in
Section 2 using the optimization method in [21]. In all implementations of ML multiple kernel learning algorithms we
use LIBSVM implementation of one-versus-all SVM where needed.

3.4 Experimental Results

To evaluate the effectiveness of different algorithms for multi-label multiple kernel learning  we ﬁrst compute the area
under precision-recall curve (AUC) for each class  and report the value of AUC averaged over all the classes. We

6

C
U
A

0.8

0.78

0.76

0.74

0.72

 
0

 

 

C
U
A

0.75

0.74

0.73

0.72

0.71

0.7

 
0

200

ML−MKL−SA
ML−MKL−SUM

1500

2000

500

1000

time(sec)

CALTECH-101

ML−MKL−SA
ML−MKL−SUM

800

1000

400

600

time(sec)
VOC-2006

0.5

0.48

0.46

0.44

0.42

C
U
A

 

 

200

400

600

800
time(sec)

1000

1200

1400

ML−MKL−SA
ML−MKL−SUM

VOC-2007

Figure 2: The evolution of classiﬁcation accuracy over time for ML-MKL-SA and ML-MKL-Sum on three data sets

C
U
A

0.81

0.805

0.8

0.795

0.79

0.785

0.78

0.775

 

50

100

 

δ=0
δ=0.2
δ=0.6
δ=1

300

350

400

0.84

0.82

C
U
A

0.8

0.78

0.76

 

 

η=0.01
η=0.001
η=0.0001

200

400
800
number of iterations

600

1000

1200

150

200

250

number of iterations

Figure 3: Classiﬁcation accuracy (AUC) of the proposed
algorithm Ml-MKL-SA on CALTECH-101 using differ-
ent values of δ (for ηp = ηγ = 0.01).

Figure 4: Classiﬁcation accuracy (AUC) of the proposed
algorithm Ml-MKL-SA on CALTECH-101 using differ-
ent values of ηp = ηγ = η for (δ = 0).

evaluate the efﬁciency of algorithms by their running times for training. All methods are coded in MATLAB and
are implemented on machines with 2 dual-core AMD Opterons running at 2.2GHz  8GB RAM and linux operating
system.

For the proposed method  itarations stop when bpt−bpt−1
is smaller than 0.01. Unless stated  the smoothing parameter
δ is set to be 0.2. For simplicity we take η = ηp = ηγ in all the following experiments. Step size η is chosen as 0.0001
for CALTECH-101 data set and 0.001 for VOC data sets in order to achieve the best computational efﬁciency.

bpt

Table 1 summarizes the classiﬁcation accuracies (AUC) and the running times of all the algorithms over the three
data sets. We ﬁrst note that the proposed MKL method for multi-labeled data  i.e.  ML-MKL-SA  yields the best
performance among the methods in comparison  which justiﬁes the assumption of using the same kernel combination
for all the classes. Note that a simple approach that uses the average of all kernels yields reasonable performance 
although its classiﬁcation accuracy is signiﬁcantly worse than the proposed approach ML-MKL-SA. Second  we
observe that except for the average kernel method that does not require learning the kernel combination weights  ML-
MKL-SA and ML-MKL-Sum are signiﬁcantly more efﬁcient than the other baseline approaches. This is not surprising
as ML-MKL-SA and ML-MKL-Sum compute a single kernel combination for all classes. Third  compared to ML-
MKL-Sum  we observe that ML-MKL-SA is overall more efﬁcient  and signiﬁcantly more efﬁcient for CALTECH-
101 data set. This is because the number of classes in CALTECH-101 is signiﬁcantly larger than that of the two VOC
challenge data sets. This result further conﬁrms that the proposed algorithm is scalable to the data sets with a large
number of classes.

Fig. 1 shows the change in the kernel weights over time for the proposed method and the three baseline methods (i.e. 
ML-MKL-Sum  GMKL  and VSKL) on CALTECH-101 data set. We observe that  overall  ML-MKL-SA shares a
similar pattern as GMKL and VSKL in the evolution curves of kernel weights  but is ten times faster than the two
baseline methods. Although ML-MKL-Sum is signiﬁcantly more efﬁcient than GMKL and VSKL  the kernel weights
learned by ML-MKL-Sum vary signiﬁcantly  particularly at the beginning of the learning process  making it a less
stable algorithm than the proposed algorithm ML-MKL-SA. To further compare ML-MKL-SA with ML-MKL-Sum 
in Fig. 2  we show how the classiﬁcation accuracy is changed over time for both methods for all three data sets.
We again observe the unstable behavior of ML-MKL-Sum: the classiﬁcation accuracy of ML-MKL-Sum could vary
signiﬁcantly over a relatively short period of time  making it less desirable method for MKL.

7

To evaluate the sensitivity of the proposed method to parameters δ and η  we conducted experiments with varied
values for the two parameters. Fig. 3 shows how the classiﬁcation accuracy (AUC) of the proposed algorithm changes
over iterations on CALTECH-101 using four different values of δ. We observe that the ﬁnal classiﬁcation accuracy
is comparable for different values of δ  demonstrating the robustness of the proposed method to the choice of δ. We
also note that the two extreme cases  i.e  δ = 0 and δ = 1  give the worst performance  indicating the importance of
choosing an optimal value for δ. Fig. 4 shows the classiﬁcation accuracy for three different values of η on CALTECH-
101 data set. We observe that the proposed algorithm achieves similar classiﬁcation accuracy when η is set to be a
relatively small value (i.e.  η = 0.001 and η = 0.0001). This result demonstrates that the proposed algorithm is in
general insensitive to the choice of step size (η).

4 Conclusion and Future Work

In this paper  we present an efﬁcient optimization framework for multi-label multiple kernel learning that combines
worst-case analysis with stochastic approximation. Compared to the other algorithms for ML-MKL  the key advantage
of the proposed algorithm is that its computational cost is sublinear in the number of classes  making it suitable for
handling a large number of classes. We verify the effectiveness of the proposed algorithm by experiments in object
recognition on several benchmark data sets. There are two directions that we plan to explore in the future. First  we
aim to further improve the efﬁciency of ML-MKL by reducing its dependence on the number of training examples and
speeding up the convergence rate. Second  we plan to improve the effectiveness and efﬁciency of multi-label learning
by exploring the correlation and structure among the classes.

5 Acknowledgements

This work was supported in part by National Science Foundation (IIS-0643494)  US Army Research (ARO Award
W911NF-08-010403) and Ofﬁce of Naval Research (ONR N00014-09-1-0663). Any opinions  ﬁndings and conclu-
sions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views
of NFS  ARO  and ONR. Part of Anil Jain’s research was supported by WCU (World Class University) program
through the National Research Foundation of Korea funded by the Ministry of Education  Science and Technology
(R31-2008-000-10008-0).

References

[1] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman  “The PASCAL Visual Object Classes Challenge

2009 (VOC2009) Results.” http://www.pascal-network.org/challenges/VOC/voc2009/workshop/index.html.

[2] G. Lanckriet  T. De Bie  N. Cristianini  M. Jordan  and W. Noble  “A statistical framework for genomic data fusion ” Bioin-

formatics  vol. 20  pp. 2626–2635  2004.

[3] S. Ji  L. Sun  R. Jin  and J. Ye  “Multi-label multiple kernel learning ” in Proceedings of Neural Information Processings

Systems  2008.

[4] G. Lanckriet  N. Cristianini  P. Bartlett  L. Ghaoui  and M. Jordan  “Learning the kernel matrix with semideﬁnite program-

ming ” Journal of Machine Learning Research  vol. 5  pp. 27–72  2004.

[5] O. Chapelle and A. Rakotomamonjy  “Second order optimization of kernel parameters ” in NIPS Workshop on Kernel Learn-

ing: Automatic Selection of Optimal Kernels  2008.

[6] P. Gehler and S. Nowozin  “On feature combination for multiclass object classiﬁcation ” in Proceedings of the IEEE Interna-

tional Conference on Computer Vision  2009.

[7] P. Gehler and S. Nowozin  “Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel classiﬁers ” in

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2009.

[8] F. Bach  G. Lanckriet  and M. Jordan  “Multiple kernel learning  conic duality  and the smo algorithm ” in Proceedings of the

21st International Conference on Machine Learning  2004.

[9] S. Sonnenburg  G. Ratsch  and C. Schafer  “A general and efﬁcient multiple kernel learning algorithm ” in Proceedings of

Neural Information Processings Systems  pp. 1273–1280  2006.

[10] A. Rakotomamonjy  F. Bach  Y. Grandvalet  and S. Canu  “SimpleMKL ” Journal of Machine Learning Research  vol. 9 

pp. 2491–2521  2008.

8

[11] Z. Xu  R. Jin  I. King  and M. R. Lyu  “An extended level method for efﬁcient multiple kernel learning ” in Proceedings of

Neural Information Processings Systems  pp. 1825–1832  2008.

[12] Z. Xu  R. Jin  H. Yang  I. King  and M. R. Lyu  “Simple and efﬁcient multiple kernel learning by group lasso ” in Proceedings

of the 27th International Conference on Machine Learning  2010.

[13] F. Bach  “Consistency of the group lasso and multiple kernel learning ” Journal of Machine Learning Research  vol. 9 

pp. 1179–1225  2008.

[14] Z. Xu  R. Jin  S. Zhu  M. R. Lyu  and I. King  “Smooth optimization for effective multiple kernel learning ” in Proceedings of

the AAAI Conference on Artiﬁcial Intelligence  2010.

[15] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet  “More efﬁciency in multiple kernel learning ” in Proceedings of the

24th International Conference on Machine Learning  2007.

[16] M. Kloft  U. Brefeld  A. Sonnenburg  and A. Zien  “Comparing sparse and non-sparse multiple kernel learning ” in NIPS

Workshop on Understanding Multiple Kernel Learning Methods  2009.

[17] M. Kloft  U. Brefeld  A. Sonnenburg  P. Laskov  K.-R. Muller  and A. Zien  “Efﬁcient and accurate lp-norm multiple kernel

learning ” in Proceedings of Neural Information Processings Systems  2009.

[18] S. Hoi  M. Lyu  and E. Chang  “Learning the uniﬁed kernel machines for classiﬁcation ” in Proceedings of the Conference on

Knowledge Discovery and Data Mining  p. 187196  2006.

[19] J. Ye  J. Chen  and J. S.  “Discriminant kernel and regularization parameter learning via semideﬁnite programming ” in

Proceedings of the International Conference on Machine Learning  p. 10951102  2007.

[20] A. Zien and S. Cheng  “Multiclass multiple kernel learning ” in Proceedings of the 24th International Conference on Machine

Learning  2007.

[21] L. Tang  J. Chen  and J. Ye  “On multiple kernel learning with multiple labels ” in Proceedings of the 21st International Jont

Conference on Artiﬁcal Intelligence  2009.

[22] J. Yang  Y. Li  Y. Tian  L. Duan  and W. Gao  “Group-sensitive multiple kernel learning for object categorization ” in Pro-

ceedings of the IEEE International Conference on Computer Vision  2009.

[23] F. Orabona  L. Jie  and B. Caputo  “Online-batch strongly convex multi kernel learning ” in Proceedings of the IEEE Confer-

ence on Computer Vision and Pattern Recognition  2010.

[24] A. Nemirovski  “Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone

operators and smooth convex-concave saddle point problems ” SIAM Journal on Optimization  vol. 15  pp. 229–251  2004.

[25] M. Varma and D. Ray  “Learning the discriminative power-invariance trade-off ” in Proceedings of the IEEE International

Conference on Computer Vision  October 2007.

[26] M. Everingham  A. Zisserman  C. K. I. Williams  and L. Van Gool  “The PASCAL Visual Object Classes Challenge 2006

(VOC2006) Results.” http://www.pascal-network.org/challenges/VOC/voc2006/results.pdf.

[27] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman  “The PASCAL Visual Object Classes Challenge

2007 (VOC2007) Results.” http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.

[28] A. Vedaldi and B. Fulkerson  “VLFeat: An open and portable library of computer vision algorithms.” http://www.

vlfeat.org/  2008.

[29] A. Berg  T. Berg  and J. Malik  “Shape matching and object recognition using low distortion correspondences ” in Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  2005.

[30] S. Lazebnik  C. Schmid  and P. Ponce  “Beyond bag of features: Spatial pyramid matching for recognizing natural scene

categories ” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2006.

[31] E. Shechtman and I. M.  “Matching local self-similarities across images and videos ” in Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition  2007.

[32] K. Mikolajczyk and C. Schmid  “Distinctive image features from scale-invariant keypoints ” IEEE Transactions on Pattern

Analysis and Machine Intelligence  vol. 27  no. 10  pp. 1615–1630  2005.

[33] D. Lowe  “Distinctive image features from scale-invariant keypoints ” International Journal of Computer Vision  vol. 2  no. 60 

pp. 91–110  2004.

[34] S. Lazebnik  C. Schmid  and P. Ponce  “Sparse texture representation using afﬁne-invariant neighborhoods ” in Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  2003.

[35] M. Muja and D. G. Lowe  “Fast approximate nearest neighbors with automatic algorithm conﬁguration ” in Proceedings of

the International Conference on Computer Vision Theory and Application  pp. 331–340  INSTICC Press  2009.

[36] J. Saketha Nath  G. Dinesh  S. Raman  C. Bhattacharyya  A. Ben-Tal  and K. Ramakrishan  “On the algorithmics and ap-
plications of a mixed-norm based kernel learning formulation ” in Proceedings of Neural Information Processings Systems 
2009.

9

,Omer Levy
Yoav Goldberg
Kohei Hayashi
Yuichi Yoshida
Giacomo De Palma
Bobak Kiani
Seth Lloyd