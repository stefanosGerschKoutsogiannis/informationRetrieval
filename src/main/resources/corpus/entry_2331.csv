2019,Glyce: Glyph-vectors for Chinese Character Representations,It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However  due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of  standard computer vision models on character data  an effective way to utilize the glyph information remains to be found.

In this paper  we address this gap by presenting  Glyce  the glyph-vectors for Chinese character representations. We make three major innovations:   (1) We use historical Chinese scripts (e.g.  bronzeware script  seal script  traditional Chinese  etc) to enrich the pictographic evidence in characters;    (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and   (3) We use image-classification as an auxiliary task in a  multi-task learning setup to increase the model's ability to generalize.   

We show that glyph-based models are able to consistently outperform word/char ID-based models  in a wide range of Chinese NLP tasks. When combing with BERT   we  are able to  set new state-of-the-art results for a variety of Chinese NLP tasks  including  language modeling  tagging (NER  CWS  POS)  
sentence pair classification (BQ  LCQMC   XNLI  NLPCC-DBQA)  
single sentence classification tasks (ChnSentiCorp  the Fudan corpus  iFeng) 
dependency parsing  and semantic role labeling. 
For example  the proposed model achieves an F1 score of 81.6 on the OntoNotes dataset of NER  +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the the Fudan corpus for text classification.,Glyce: Glyph-vectors for Chinese Character

Representations

Yuxian Meng*  Wei Wu*  Fei Wang*  Xiaoya Li*  Ping Nie  Fan Yin

Muyu Li  Qinghong Han  Xiaofei Sun and Jiwei Li

{yuxian meng  wei wu  fei wang  xiaoya li  ping nie  fan yin 
muyu li  qinghong han  xiaofei sun  jiwei li}@shannonai.com

Shannon.AI

Abstract

It is intuitive that NLP tasks for logographic languages like Chinese should beneﬁt
from the use of the glyph information in those languages. However  due to the
lack of rich pictographic evidence in glyphs and the weak generalization ability of
standard computer vision models on character data  an effective way to utilize the
glyph information remains to be found.
In this paper  we address this gap by presenting Glyce  the glyph-vectors for
Chinese character representations. We make three major innovations: (1) We use
historical Chinese scripts (e.g.  bronzeware script  seal script  traditional Chinese 
etc) to enrich the pictographic evidence in characters; (2) We design CNN structures
(called tianzege-CNN) tailored to Chinese character image processing; and (3)
We use image-classiﬁcation as an auxiliary task in a multi-task learning setup to
increase the model’s ability to generalize.
We show that glyph-based models are able to consistently outperform word/char
ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-
of-the-art results for a variety of Chinese NLP tasks  including tagging (NER  CWS 
POS)  sentence pair classiﬁcation  single sentence classiﬁcation tasks  dependency
parsing  and semantic role labeling. For example  the proposed model achieves an
F1 score of 80.6 on the OntoNotes dataset of NER  +1.5 over BERT; it achieves an
almost perfect accuracy of 99.8% on the Fudan corpus for text classiﬁcation. 1 2

1

Introduction

Chinese is a logographic language. The logograms of Chinese characters encode rich information of
their meanings. Therefore  it is intuitive that NLP tasks for Chinese should beneﬁt from the use of
the glyph information. Taking into account logographic information should help semantic modeling.
Recent studies indirectly support this argument: Radical representations have proved to be useful
in a wide range of language understanding tasks [Shi et al.  2015  Li et al.  2015  Yin et al.  2016 
Sun et al.  2014  Shao et al.  2017]. Using the Wubi scheme — a Chinese character encoding method
that mimics the order of typing the sequence of radicals for a character on the computer keyboard
—- is reported to improve performances on Chinese-English machine translation [Tan et al.  2018].
Cao et al. [2018] gets down to units of greater granularity  and proposed stroke n-grams for character
modeling.
Recently  there have been some efforts applying CNN-based algorithms on the visual features of
characters. Unfortunately  they do not show consistent performance boosts [Liu et al.  2017  Zhang

1* indicates equal contribution.
2code is available at https://github.com/ShannonAI/glyce.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Chinese

金文
隶书
篆书
魏碑

English

Bronzeware script

Clerical script

Seal script
Tablet script

繁体中文

Traditional Chinese

简体中文(宋体)
简体中文(仿宋体)

Simpliﬁed Chinese - Song

Simpliﬁed Chinese - FangSong

草书

Cursive script

Time Period

Shang and Zhou dynasty (2000 BC – 300 BC)

Han dynasty (200BC-200AD)

Han dynasty and Wei-Jin period (100BC - 420 AD)
Northern and Southern dynasties 420AD - 588AD

600AD - 1950AD (mainland China).

still currently used in HongKong and Taiwan

1950-now
1950-now

Jin Dynasty to now

Table 1: Scripts and writing styles used in Glyce.

and LeCun  2017]  and some even yield negative results [Dai and Cai  2017]. For instance  Dai and
Cai [2017] run CNNs on char logos to obtain Chinese character representations and used them in the
downstream language modeling task. They reported that the incorporation of glyph representations
actually worsens the performance and concluded that CNN-based representations do not provide
extra useful information for language modeling. Using similar strategies  Liu et al. [2017] and Zhang
and LeCun [2017] tested the idea on text classiﬁcation tasks  and performance boosts were observed
only in very limited number of settings. Positive results come from Su and Lee [2017]  which found
glyph embeddings help two tasks: word analogy and word similarity. Unfortunately  Su and Lee
[2017] only focus on word-level semantic tasks and do not extend improvements in the word-level
tasks to higher level NLP tasks such as phrase  sentence or discourse level. Combined with radical
representations  Shao et al. [2017] run CNNs on character ﬁgures and use the output as auxiliary
features in the POS tagging task.
We propose the following explanations for negative results reported in the earlier CNN-based models
[Dai and Cai  2017]: (1) not using the correct version(s) of scripts: Chinese character system has a
long history of evolution. The characters started from being easy-to-draw  and slowly transitioned
to being easy-to-write. Also  they became less pictographic and less concrete over time. The most
widely used script version to date  the Simpliﬁed Chinese  is the easiest script to write  but inevitably
loses the most signiﬁcant amount of pictographic information. For example  ”人” (human) and ”入”
(enter)  which are of irrelevant meanings  are highly similar in shape in simpliﬁed Chinese  but very
different in historical languages such as bronzeware script. (2) not using the proper CNN structures:
unlike ImageNet images [Deng et al.  2009]  the size of which is mostly at the scale of 800*600 
character logos are signiﬁcantly smaller (usually with the size of 12*12). It requires a different CNN
architecture to capture the local graphic features of character images; (3) no regulatory functions
were used in previous work: unlike the classiﬁcation task on the imageNet dataset  which contains
tens of millions of data points  there are only about 10 000 Chinese characters. Auxiliary training
objectives are thus critical in preventing overﬁtting and promoting the model’s ability to generalize.
In this paper  we propose GLYCE  the GLYph-vectors for Chinese character representations. We
treat Chinese characters as images and use CNNs to obtain their representations. We resolve the
aforementioned issues by using the following key techniques:

• We use the ensemble of the historical and the contemporary scripts (e.g.  the bronzeware
script  the clerical script  the seal script  the traditional Chinese etc)  along with the scripts
of different writing styles (e.g  the cursive script) to enrich pictographic information from
the character images.
• We utilize the Tianzige-CNN (田字格) structures tailored to logographic character modeling.
• We use multi-task learning methods by adding an image-classiﬁcation loss function to

increase the model’s ability to generalize.

Glyce is found to improve a wide range of Chinese NLP tasks. We are able to obtain the SOTA
performances on a wide range of Chinese NLP tasks  including tagging (NER  CWS  POS)  sen-
tence pair classiﬁcation (BQ  LCQMC  XNLI  NLPCC-DBQA)  single sentence classiﬁcation tasks
(ChnSentiCorp  the Fudan corpus  iFeng)  dependency parsing  and semantic role labeling.

2

Figure 1: Illustration of the Tianzege-CNN used in Glyce.

2 Glyce

2.1 Using Historical Scripts

As discussed in Section 1  pictographic information is heavily lost in the simpliﬁed Chinese script.
We thus propose using scripts from various time periods in history and also of different writing styles.
We collect the following major historical script with details shown in Table 1. Scripts from different
historical periods  which are usually very different in shape  help the model to integrate pictographic
evidence from various sources; Scripts of different writing styles help improve the model’s ability to
generalize. Both strategies are akin to widely-used data augmentation strategies in computer vision.

2.2 The Tianzige-CNN Structure for Glyce

Directly using deep CNNs He et al. [2016]  Szegedy et al. [2016]  Ma et al. [2018a] in our task
results in very poor performances because of (1) relatively smaller size of the character images:
the size of Imagenet images is usually at the scale of 800*600  while the size of Chinese character
images is signiﬁcantly smaller  usually at the scale of 12*12; and (2) the lack of training examples:
classiﬁcations on the imageNet dataset utilizes tens of millions of different images. In contrast 
there are only about 10 000 distinct Chinese characters. To tackle these issues  we propose the
Tianzige-CNN structure  which is tailored to Chinese character modeling as illustrated in Figure 1.
Tianzige (田字格) is a traditional form of Chinese Calligraphy. It is a four-squared format (similar
to Chinese character 田) for beginner to learn writing Chinese characters. The input image ximage
is ﬁrst passed through a convolution layer with kernel size 5 and output channels 1024 to capture
lower level graphic features. Then a max-pooling of kernel size 4 is applied to the feature map which
reduces the resolution from 8 × 8 to 2 × 2  . This 2 × 2 tianzige structure presents how radicals are
arranged in Chinese characters and also the order by which Chinese characters are written. Finally 
we apply group convolutions [Krizhevsky et al.  2012  Zhang et al.  2017] rather than conventional
convolutional operations to map tianzige grids to the ﬁnal outputs . Group convolutional ﬁlters are
much smaller than their normal counterparts  and thus are less prone to overﬁtting. It is fairly easy to
adjust the model from single script to multiple scripts  which can be achieved by simply changing the
input from 2D (i.e.  dfont × dfont) to 3D (i.e.  dfont × dfont × Nscript)  where dfont denotes the font size
and Nscript the number of scripts we use.

2.3 Image Classiﬁcation as an Auxiliary Objective

To further prevent overﬁtting  we use the task of image classiﬁcation as an auxiliary training objective.
The glyph embedding himage from CNNs will be forwarded to an image classiﬁcation objective to
predict its corresponding charID. Suppose the label of image x is z. The training objective for the
image classiﬁcation task L(cls) is given as follows:
L(cls) = − log p(z|x)

(1)
Let L(task) denote the task-speciﬁc objective for the task we need to tackle  e.g.  language modeling 
word segmentation  etc. We linearly combine L(task) and L(cl)  making the ﬁnal objective training
function as follows:

= − log softmax(W × himage)

(2)
where λ(t) controls the trade-off between the task-speciﬁc objective and the auxiliary image-
1  where λ0 ∈ [0  1]
classiﬁcation objective. λ is a function of the number of epochs t: λ(t) = λ0λt

L = (1 − λ(t)) L(task) + λ(t)L(cls)

3

Figure 2: Combing glyph information with BERT.

denotes the starting value  λ1 ∈ [0  1] denotes the decaying value. This means that the inﬂuence from
the image classiﬁcation objective decreases as the training proceeds  with the intuitive explanation
being that at the early stage of training  we need more regulations from the image classiﬁcation task.
Adding image classiﬁcation as a training objective mimics the idea of multi-task learning.

2.4 Combing Glyph Information with BERT

The glyph embeddings can be directly output to downstream models such as RNNs  LSTMs  trans-
formers.
Since large scale pretraining systems using language models  such as BERT [Devlin et al.  2018] 
ELMO [Peters et al.  2018] and GPT [Radford et al.  2018]  have proved to be effective in a wide range
of NLP tasks  we explore the possibility of combining glyph embeddings with BERT embeddings.
Such a strategy will potentially endow the model with the advantage of both glyph evidence and
large-scale pretraining. The overview of the combination is shown in Figure 2. The model consists of
four layers: the BERT layer  the glyph layer  the Glyce-BERT layer and the task-speciﬁc output layer.
• BERT Layer Each input sentence S is concatenated with a special CLS token denoting the
start of the sentence  and a SEP token  denoting the end of the sentence. Given a pre-trained
BERT model  the embedding for each token of S is computed using BERT. We use the
output from the last layer of the BERT transformer to represent the current token.

addition is then concatenated with BERT to obtain the full Glyce representations.

• Glyph Layer the output glyph embeddings of S from tianzege-CNNs.
• Glyce-BERT layer Position embeddings are ﬁrst added to the glyph embeddings. The
• Task-speciﬁc output layer Glyce representations are used to represent the token at that
position  similar as word embeddings or Elmo emebddings [Peters et al.  2018]. Contextual-
aware information has already been encoded in the BERT representation but not glyph
representations. We thus need additional context models to encode contextual-aware glyph
representations. Here  we choose multi-layer transformers [Vaswani et al.  2017]. The
output representations from transformers are used as inputs to the prediction layer. It is
worth noting that the representations the special CLS and SEP tokens are maintained at the
ﬁnal task-speciﬁc embedding layer.

3 Tasks

In this section  we describe how glypg embeddings can be used for different NLP tasks. In the vanilla
version  glyph embeddings are simply treated as character embeddings  which are fed to models built
on top of the word-embedding layers  such as RNNs  CNNs or more sophisticated ones. If combined

4

Figure 3: Using Glyce-BERT model for different tasks.

with BERT  we need to speciﬁcally handle the integration between the glyph embeddings and the
pretrained embeddings from BERT in different scenarios  as will be discussed in order below:

Sequence Labeling Tasks Many Chinese NLP tasks  such as name entity recognition (NER) 
Chinese word segmentation (CWS) and part speech tagging (POS)  can be formalized as character-
level sequence labeling tasks  in which we need to predict a label for each character. For glyce-BERT
model  the embedding output from the task-speciﬁc layer (described in Section 2.4) is fed to the CRF
model for label predictions.

Single Sentence Classiﬁcation For text classiﬁcation tasks  a single label is to be predicted for the
entire sentence. In the BERT model  the representation for the CLS token in the ﬁnal layer of BERT is
output to the softmax layer for prediction. We adopt the similar strategy  in which the representation
for the CLS token in the task-speciﬁc layer is fed to the softmax layer to predict labels.

Sentence Pair Classiﬁcation For sentence pair classiﬁcation task like SNIS [Bowman et al.  2015] 
a model needs to handle the interaction between the two sentences and outputs a label for a pair of
sentences. In the BERT setting  a sentence pair (s1  s2) is concatenated with one CLS and two SEP
tokens  denoted by [CLS  s1  SEP  s2  SEP]. The concatenation is fed to the BERT model  and the
obtained CLS representation is then fed to the softmax layer for label prediction. We adopt the similar
strategy for Glyce-BERT  in which [CLS  s1  SEP  s2  SEP] is subsequently passed through the BERT
layer  Glyph layer  Glyce-BERT layer and the task-speciﬁc output layer. The CLS representation from
the task-speciﬁc output layer is fed to the softmax function for the ﬁnal label prediction.

4 Experiments

To enable apples-to-apples comparison  we perform grid parameter search for both baselines and the
proposed model on the dev set. Tasks that we work on are described in order below.

4.1 Tagging

NER For the task of Chinese NER  we used the widely-used OntoNotes  MSRA  Weibo and resume
datasets. Since most datasets don’t have gold-standard segmentation  the task is normally treated
as a char-level tagging task: outputting an NER tag for each character. The currently most widely
used non-BERT model is Lattice-LSTMs [Yang et al.  2018  Zhang and Yang  2018]  achieving better
performances than CRF+LSTM [Ma and Hovy  2016].

CWS : The task of Chinese word segmentation (CWS) is normally treated as a char-level tagging
problem. We used the widely-used PKU  MSR  CITYU and AS benchmarks from SIGHAN 2005
bake-off for evaluation.

5

POS The task of Chinese part of speech tagging is normally formalized as a character-level sequence
labeling task  assigning labels to each of the characters within the sequence. We use the CTB5  CTB9
and UD1 (Universal Dependencies) benchmarks to test our models.

OntoNotes

P
Model
74.36
CRF-LSTM
Lattice-LSTM
76.35
Glyce+Lattice-LSTM 82.06

BERT
Glyce+BERT

Model
CRF-LSTM
Lattice-LSTM
Lattice-LSTM+Glyce

BERT
Glyce+BERT

78.01
81.87

MSRA

P
92.97
93.57
93.86

94.97
95.57

R
69.43
71.56
68.74

80.35
81.40

R
90.80
92.79
93.92

94.62
95.51

F
71.81
73.88
74.81
(+ 0.93)
79.16
80.62
(+1.46)

F
91.87
93.18
93.89
(+0.71)
94.80
95.54
(+0.74)

Model
CRF-LSTM
Lattice-LSTM
Lattice-LSTM+Glyce

BERT
Glyce+BERT

Weibo

P
51.16
52.71
53.69

67.12
67.68

resume

Model
CRF-LSTM
Lattice-LSTM
Lattice-LSTM+Glyce

P
94.53
94.81
95.72

BERT
Glyce+BERT

96.12
96.62

R
51.07
53.92
55.30

66.88
67.71

R
94.29
94.11
95.63

95.45
96.48

F
50.95
53.13
54.32
(+1.19)
67.33
67.60
(+0.27)

F
94.41
94.46
95.67
(+1.21)
95.78
96.54
(+0.76)

Table 2: Results for NER tasks.

Results for NER  CWS and POS are respectively shown in Tables 2  3 and 4. When comparing with
non-BERT models  Lattice-Glyce performs better than all non-BERT models across all datasets in
all tasks. BERT outperforms non-BERT models in all datasets except Weibo. This is due to the
discrepancy between the dataset which BERT is pretrained on (i.e.  wikipedia) and weibo. The
Glyce-BERT model outperforms BERT and sets new SOTA results across all datasets  manifesting
the effectiveness of incorporating glyph information. We are able to achieve SOTA performances on
all of the datasets using either Glyce model itself or BERT-Glyce model.

Model
Yang et al. [2017]
Ma et al. [2018b]
Huang et al. [2019]
BERT
Glyce+BERT

Model
Yang et al. [2017]
Ma et al. [2018b]
Huang et al. [2019]
BERT
Glyce+BERT

PKU
P
-
-
-
96.8
97.1

MSR
P
-
-
-
98.1
98.2

R
-
-
-
96.3
96.4

R
-
-
-
98.2
98.3

F
96.3
96.1
96.6
96.5
96.7
(+0.2)

F
97.5
98.1
97.9
98.1
98.3
(+0.2)

CITYU

Model
Yang et al. [2017]
Ma et al. [2018b]
Huang et al. [2019]
BERT
Glyce+BERT

P
-
-
-
97.5
97.9

Model
Yang et al. [2017]
Ma et al. [2018b]
Huang et al. [2019]
BERT
Glyce+BERT

AS
P
-
-
-
96.7
96.6

R
-
-
-
97.7
98.0

R
-
-
-
96.4
96.8

F
96.9
97.2
97.6
97.6
97.9
(+0.3)

F
95.7
96.2
96.6
96.5
96.7
(+0.2)

Table 3: Results for CWS tasks.

4.2 Sentence Pair Classiﬁcation

For sentence pair classiﬁcation tasks  we need to output a label for each pair of sentence. We employ
the following four different datasets: (1) BQ (binary classiﬁcation task) [Bowman et al.  2015]; (2)
LCQMC (binary classiﬁcation task) [Liu et al.  2018]  (3) XNLI (three-class classiﬁcation task)
[Williams and Bowman]  and (4) NLPCC-DBQA (binary classiﬁcation task) 3.

3https://github.com/xxx0624/QA_Model

6

Model
Shao et al. [2017] (Sig)
Shao et al. [2017] (Ens)
Lattice-LSTM
Glyce+Lattice-LSTM

CTB5
P
93.68
93.95
94.77
95.49

BERT
Glyce+BERT

95.86
96.50

Model
Shao et al. [2017] (Sig)
Lattice-LSTM
Glyce+Lattice-LSTM

CTB6
P
-
92.00
92.72

BERT
Glyce+BERT

94.91
95.56

R
94.47
94.81
95.51
95.72

96.26
96.74

R
-
90.86
91.14

94.63
95.26

F
94.07
94.38
95.14
95.61
(+0.47)
96.06
96.61
(+0.55)

F
90.81
91.43
91.92
(+0.49)
94.77
95.41
(+0.64)

Model
Shao et al. [2017] (Sig)
Shao et al. [2017] (Ens)
Lattice-LSTM
Lattice-LSTM+Glyce

CTB9
P
91.81
92.28
92.53
92.28

BERT
Glyce+BERT

Model
Shao et al. [2017] (Sig)
Shao et al. [2017] (Ens)
Lattice-LSTM
Lattice-LSTM+Glyce

BERT
Glyce+BERT

92.43
93.49

UD1
P
89.28
89.67
90.47
91.57

95.42
96.19

R
94.47
92.40
91.73
92.85

92.15
92.84

R
89.54
89.86
89.70
90.19

94.17
96.10

F
91.89
92.34
92.13
92.38
(+0.25)
92.29
93.15
(+0.86)

F
89.41
89.75
90.09
90.87
(+0.78)
94.79
96.14
(+1.35)

Table 4: Results for POS tasks.

The current non-BERT SOTA model is based on the bilateral multi-perspective matching model
(BiMPM) [Wang et al.  2017]  which speciﬁcally tackles the subunit matching between sentences.
Glyph embeddings are incorporated into BiMPMs  forming the Glyce+BiMPM baseline. Results
regarding each model on different datasets are given in Table 5. As can be seen  BiPMP+Glyce
outperforms BiPMPs  achieving the best results among non-bert models. BERT outperforms all
non-BERT models  and BERT+Glyce performs the best  setting new SOTA results on all of the four
benchmarks.

P
Model
BiMPM
82.3
Glyce+BiMPM 81.9

BQ
R
81.2
85.5

BERT
Glyce+BERT

83.5
84.2

85.7
86.9

P
Model
BiMPM
-
Glyce+BiMPM -

XNLI
R
-
-

BERT
Glyce+BERT

-
-

-
-

F
81.7
83.7
(+2.0)
84.6
85.5
(+0.9)

F
-
-

-
-

A
81.9
83.3
(+1.4)
84.8
85.8
(+1.0)

A
67.5
67.7
(+0.2)
78.4
79.2
(+0.8)

P
Model
BiMPM
77.6
Glyce+BiMPM 80.4

LCQMC
R
93.9
93.4

BERT
Glyce+BERT

83.2
86.8

94.2
91.2

NLPCC-DBQA

P
Model
BiMPM
78.8
Glyce+BiMPM 76.3

R
56.5
59.9

BERT
Glyce+BERT

79.6
81.1

86.0
85.8

F
85.0
86.4
(+1.4)
88.2
88.8
(+0.6)

F
65.8
67.1
(+1.3)
82.7
83.4
(+0.7)

A
83.4
85.3
(+1.9)
87.5
88.7
(+1.2)

A
-
-
-
-
-
-

Table 5: Results for sentence-pair classiﬁcation tasks.

Model
LSTM

LSTM + Glyce

BERT

Glyce+BERT

ChnSentiCorp

91.7
93.1
(+ 1.4)
95.4
95.9
(+0.5)

the Fudan corpus

95.8
96.3
(+0.5)
99.5
99.8
(+0.3)

iFeng
84.9
85.8
(+0.9)
87.1
87.5
(+0.4)

Table 6: Accuracies for Single Sentence Classiﬁcation task.

7

Dependency Parsing

Model
Ballesteros et al. [2016]
Kiperwasser and Goldberg [2016]
Cheng et al. [2016]
Biafﬁne
Biafﬁne+Glyce

UAS
87.7
87.6
88.1
89.3
90.2
(+0.9)

LAS
86.2
86.1
85.7
88.2
89.0
(+0.8)

Semantic Role Labeling

Model
Roth and Lapata [2016]
Marcheggiani and Titov [2017]
He et al. [2018]
k-order pruning+Glyce

P
76.9
84.6
84.2
85.4
(+0.8)

R
73.8
80.4
81.5
82.1
(+0.6)

F
75.3
82.5
82.8
83.7
(+0.9)

Table 7: Results for dependency parsing and SRL.

4.3 Single Sentence Classiﬁcation

For single sentence/document classiﬁcation  we need to output a label for a text sequence. The label
could be either a sentiment indicator or a news genre. Datasets that we use include: (1) ChnSentiCorp
(binary classiﬁcation); (2) the Fudan corpus (5-class classiﬁcation) [Li  2011]; and (3) Ifeng (5-class
classiﬁcation).
Results for different models on different tasks are shown in Table 6. We observe similar phenomenon
as before: Glyce+BERT achieves SOTA results on all of the datasets. Speciﬁcally  the Glyce+BERT
model achieves an almost perfect accuracy (99.8) on the Fudan corpus.

4.4 Dependency Parsing and Semantic Role Labeling

For dependency parsing [Chen and Manning  2014  Dyer et al.  2015]  we used the widely-used
Chinese Penn Treebank 5.1 dataset for evaluation. Our implementation uses the previous state-of-the-
art Deep Biafﬁne model Dozat and Manning [2016] as a backbone. We replaced the word vectors
from the biafﬁne model with Glyce-word embeddings  and exactly followed its model structure and
training/dev/test split criteria. We report scores for unlabeled attachment score (UAS) and labeled
attachment score (LAS). Results for previous models are copied from [Dozat and Manning  2016 
Ballesteros et al.  2016  Cheng et al.  2016]. Glyce-word pushes SOTA performances up by +0.9 and
+0.8 in terms of UAS and LAS scores.
For the task of semantic role labeling (SRL) [Roth and Lapata  2016  Marcheggiani and Titov  2017 
He et al.  2018]  we used the CoNLL-2009 shared-task. We used the current SOTA model  the k-order
pruning algorithm [He et al.  2018] as a backbone.4 We replaced word embeddings with Glyce
embeddings. Glyce outperforms the previous SOTA performance by 0.9 with respect to the F1 score 
achieving a new SOTA score of 83.7.
BERT does not perform competitively in these two tasks  and results are thus omitted.

5 Ablation Studies

In this section  we discuss the inﬂuence of different factors of the proposed model. We use the
LCQMC dataset of the sentence-pair prediction task for illustration. Factors that we discuss include
training strategy  model architecture  auxiliary image-classiﬁcation objective  etc.

5.1 Training Strategy

This section talks about a training tactic (denoted by BERT-glyce-joint)  in which given task-speciﬁc
supervisions  we ﬁrst ﬁne-tune the BERT model  then freeze BERT to ﬁne-tune the glyph layer 
and ﬁnally jointly tune both layers until convergence. We compare this strategy with other tactics 
including (1) the Glyph-Joint strategy  in which BERT is not ﬁne-tuned in the beginning: we ﬁrst

4Code open sourced at https://github.com/bcmi220/srl_syn_pruning

8

Strategy
BERT-glyce-joint
Glyph-Joint
joint
only BERT

P
86.8
82.5
81.5
83.2

R
91.2
94.0
95.1
94.2

F
88.8
87.9
87.8
88.2

Acc
88.7
87.1
86.8
87.5

Strategy
Transformers
BiLSMTs
CNNs
BiMPM

Precision Recall
86.8
81.8
81.5
81.1

91.2
94.9
94.8
94.6

F1
88.8
87.9
87.6
87.3

Accuracy
88.7
86.9
86.6
86.2

Table 8: Impact of different training strategies.

Table 9: Impact of structures for the task-speciﬁc output layer.

Strategy
W image-cls
WO image-cls

P
86.8
83.9

R
91.2
93.6

F
88.8
88.4

Acc
88.7
87.9

Vanilla-CNN
He et al. [2016]
Tianzige-CNN

P
85.3
84.5
86.8

R
89.8
90.8
91.2

F
87.4
87.5
88.8

Table 10: Impact of the auxilliary image classiﬁca-
tion training objective.

Table 11: Impact of CNN structures.

freeze BERT to tune the glyph layer  and then jointly tune both layers until convergence; and (2) the
joint strategy  in which we directly jointly training two models until converge.
Results are shown in Table 8. As can be seen  the BERT-glyce-joint outperforms the rest two strategies.
Our explanation for the inferior performance of the joint strategy is as follows: the BERT layer is
pretrained but the glyph layer is randomly initialized. Given the relatively small amount of training
signals  the BERT layer could be mislead by the randomly initialized glyph layer at the early stage of
training  leading to inferior ﬁnal performances.

5.2 Structures of the task-speciﬁc output layer

The concatenation of the glyph embedding and the BERT embedding is fed to the task-speciﬁc output
layer. The task-speciﬁc output layer is made up with two layers of transformer layers. Here we
change transformers to other structures such as BiLSTMs and CNNs to explore the inﬂuence. We
also try the BiMPM structure Wang et al. [2017] to see the results.
Performances are shown in Table 9. As can be seen  transformers not only outperform BiLSTMs and
CNNs  but also the BiMPM structure  which is speciﬁcally built for the sentence pair classiﬁcation
task. We conjecture that this is because of the consistency between transformers and the BERT
structure.

5.3 The image-classiﬁcation training objective

We also explore the inﬂuence of the image-classiﬁcation training objective  which outputs the glyph
representation to an image-classiﬁcation objective. Table 10 represents its inﬂuence. As can be seen 
this auxiliary training objective given a +0.8 performance boost.

5.4 CNN structures

Results for different CNN structures are shown in Table 11. As can be seen  the adoption of tianzige-
CNN structure introduces a performance boost of F1 about +1.0. Directly using deep CNNs in
our task results in very poor performances because of (1) relatively smaller size of the character
images: the size of ImageNet images is usually at the scale of 800*600  while the size of Chinese
character images is signiﬁcantly smaller  usually at the scale of 12*12; and (2) the lack of training
examples: classiﬁcations on the ImageNet dataset utilizes tens of millions of different images. In
contrast  there are only about 10 000 distinct Chinese characters. We utilize the Tianzige-CNN (田字
格) structures tailored to logographic character modeling for Chinese. This tianzige structure is of
signiﬁcant importance in extracting character meanings.

6 Conclusion

In this paper  we propose Glyce  Glyph-vectors for Chinese Character Representations. Glyce treats
Chinese characters as images and uses Tianzige-CNN to extract character semantics. Glyce provides
a general way to model character semantics of logographic languages. It is general and fundamental.
Just like word embeddings  Glyce can be integrated to any existing deep learning system.

9

References
Xinlei Shi  Junjie Zhai  Xudong Yang  Zehua Xie  and Chao Liu. Radical embedding: Delving
deeper to chinese radicals. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 2: Short Papers)  volume 2  pages 594–598  2015.

Yanran Li  Wenjie Li  Fei Sun  and Sujian Li. Component-enhanced chinese character embeddings.

arXiv preprint arXiv:1508.06669  2015.

Rongchao Yin  Quan Wang  Peng Li  Rui Li  and Bin Wang. Multi-granularity chinese word
embedding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing  pages 981–986  2016.

Yaming Sun  Lei Lin  Nan Yang  Zhenzhou Ji  and Xiaolong Wang. Radical-enhanced chinese
In International Conference on Neural Information Processing  pages

character embedding.
279–286. Springer  2014.

Yan Shao  Christian Hardmeier  J¨org Tiedemann  and Joakim Nivre. Character-based joint segmen-
tation and pos tagging for chinese using bidirectional rnn-crf. arXiv preprint arXiv:1704.01314 
2017.

Mi Xue Tan  Yuhuang Hu  Nikola I Nikolov  and Richard HR Hahnloser. wubi2en: Character-level

chinese-english translation through ascii encoding. arXiv preprint arXiv:1805.03330  2018.

Shaosheng Cao  Wei Lu  Jun Zhou  and Xiaolong Li. cw2vec: Learning chinese word embeddings

with stroke n-gram information. 2018.

Frederick Liu  Han Lu  Chieh Lo  and Graham Neubig. Learning character-level compositionality

with visual features. arXiv preprint arXiv:1704.04859  2017.

Xiang Zhang and Yann LeCun. Which encoding is the best for text classiﬁcation in chinese  english 

japanese and korean? arXiv preprint arXiv:1708.02657  2017.

Falcon Z Dai and Zheng Cai. Glyph-aware embedding of chinese characters. arXiv preprint

arXiv:1709.00028  2017.

Tzu-Ray Su and Hung-Yi Lee. Learning chinese word representations from glyphs of characters.

arXiv preprint arXiv:1708.04755  2017.

Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition  2009. CVPR 2009.
IEEE Conference on  pages 248–255. Ieee  2009.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  June 2016.

Ningning Ma  Xiangyu Zhang  Hai-Tao Zheng  and Jian Sun. Shufﬂenet v2: Practical guidelines for

efﬁcient cnn architecture design. arXiv preprint arXiv:1807.11164  5  2018a.

Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems  pages 1097–1105 
2012.

Ting Zhang  Guo-Jun Qi  Bin Xiao  and Jingdong Wang. Interleaved group convolutions. In Computer

Vision and Pattern Recognition  2017.

Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805  2018.

10

Matthew E Peters  Mark Neumann  Mohit Iyyer  Matt Gardner  Christopher Clark  Kenton Lee  and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365 
2018.

Alec Radford  Karthik Narasimhan  Tim Salimans  and Ilya Sutskever.

Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf  2018.

Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
In Advances in Neural Information

Kaiser  and Illia Polosukhin. Attention is all you need.
Processing Systems  pages 5998–6008  2017.

Samuel R. Bowman  Gabor Angeli  Christopher Potts  and Christopher D. Manning. A large annotated
corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing  EMNLP 2015  Lisbon  Portugal  September 17-21 
2015  pages 632–642  2015.

Jie Yang  Yue Zhang  and Shuailong Liang. Subword encoding in lattice LSTM for chinese word

segmentation. CoRR  abs/1810.12594  2018.

Yue Zhang and Jie Yang. Chinese NER using lattice LSTM. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics  ACL 2018  Melbourne  Australia  July
15-20  2018  Volume 1: Long Papers  pages 1554–1564  2018.

Xuezhe Ma and Eduard H. Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics  ACL
2016  August 7-12  2016  Berlin  Germany  Volume 1  2016.

Jie Yang  Yue Zhang  and Fei Dong. Neural word segmentation with rich pretraining. In Proceedings
of the 55th Annual Meeting of the Association for Computational Linguistics  ACL 2017  Vancouver 
Canada  July 30 - August 4  Volume 1: Long Papers  pages 839–849  2017.

Ji Ma  Kuzman Ganchev  and David Weiss. State-of-the-art chinese word segmentation with bi-lstms.

CoRR  abs/1808.06511  2018b. URL http://arxiv.org/abs/1808.06511.

Weipeng Huang  Xingyi Cheng  Kunlong Chen  Taifeng Wang  and Wei Chu. Toward fast and
accurate neural chinese word segmentation with multi-criteria learning. CoRR  abs/1903.04190 
2019. URL http://arxiv.org/abs/1903.04190.

Xin Liu  Qingcai Chen  Chong Deng  Huajun Zeng  Jing Chen  Dongfang Li  and Buzhou Tang.
Lcqmc: A large-scale chinese question matching corpus. In Proceedings of the 27th International
Conference on Computational Linguistics  pages 1952–1962  2018.

Adina Williams and Samuel R Bowman. The multi-genre nli corpus 0.2: Repeval shared task

preliminary version description paper.

Zhiguo Wang  Wael Hamza  and Radu Florian. Bilateral multi-perspective matching for natural
language sentences. In Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial
Intelligence  IJCAI 2017  Melbourne  Australia  August 19-25  2017  pages 4144–4150  2017.

Ronglu Li. Fudan corpus for text classiﬁcation. 2011.

Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural networks.
In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP)  pages 740–750  2014.

Chris Dyer  Miguel Ballesteros  Wang Ling  Austin Matthews  and Noah A Smith. Transition-based
dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075  2015.

Timothy Dozat and Christopher D Manning. Deep biafﬁne attention for neural dependency parsing.

arXiv preprint arXiv:1611.01734  2016.

Miguel Ballesteros  Yoav Goldberg  Chris Dyer  and Noah A Smith. Training with exploration

improves a greedy stack-lstm parser. arXiv preprint arXiv:1603.03793  2016.

11

Hao Cheng  Hao Fang  Xiaodong He  Jianfeng Gao  and Li Deng. Bi-directional attention with

agreement for dependency parsing. arXiv preprint arXiv:1608.02076  2016.

Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirectional

lstm feature representations. arXiv preprint arXiv:1603.04351  2016.

Michael Roth and Mirella Lapata. Neural semantic role labeling with dependency path embeddings.

arXiv preprint arXiv:1605.07515  2016.

Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for

semantic role labeling. arXiv preprint arXiv:1703.04826  2017.

Shexia He  Zuchao Li  Hai Zhao  and Hongxiao Bai. Syntax for semantic role labeling  to be  or not
to be. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers)  volume 1  pages 2061–2071  2018.

12

,Xiangru Lian
Yijun Huang
Yuncheng Li
Ji Liu
Eli Gutin
Vivek Farias
Yuxian Meng
Wei Wu
Fei Wang
Xiaoya Li
Ping Nie
Fan Yin
Muyu Li
Qinghong Han
Xiaofei Sun
Jiwei Li