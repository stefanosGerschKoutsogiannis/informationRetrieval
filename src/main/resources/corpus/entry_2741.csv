2012,Joint Modeling of a Matrix with Associated Text via Latent Binary Features,A new methodology is developed for joint analysis of a matrix and accompanying documents  with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model  inferring latent binary features (topics) for each document. A new matrix decomposition is developed  with latent binary features associated with the rows/columns  and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data  with the associated documents defined by the legislation. State-of-the-art results are manifested for prediction of votes on a new piece of legislation  based only on the observed text legislation. The coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data.,Joint Modeling of a Matrix with Associated Text

via Latent Binary Features

XianXing Zhang
Duke University

xianxing.zhang@duke.edu

Lawrence Carin
Duke University
lcarin@duke.edu

Abstract

A new methodology is developed for joint analysis of a matrix and accompanying
documents  with the documents associated with the matrix rows/columns. The
documents are modeled with a focused topic model  inferring interpretable latent
binary features for each document. A new matrix decomposition is developed 
with latent binary features associated with the rows/columns  and with imposition
of a low-rank constraint. The matrix decomposition and topic model are coupled
by sharing the latent binary feature vectors associated with each. The model is
applied to roll-call data  with the associated documents deﬁned by the legislation.
Advantages of the proposed model are demonstrated for prediction of votes on
a new piece of legislation  based only on the observed text of legislation. The
coupling of the text and legislation is also shown to yield insight into the properties
of the matrix decomposition for roll-call data.

1

Introduction

The analysis of legislative roll-call data provides an interesting setting for recent developments in
the joint analysis of matrices and text [23  8]. While the roll-call data matrix is typically binary 
the modeling framework is general  in that it may be readily extended to categorical  integer or
real observations. The problem is made interesting because  in addition to the matrix of votes  we
have access to the text of the legislation (e.g.  characteristic of the columns of the matrix  with each
column representing a piece of legislation and each row a legislator). While roll-call data provides
an interesting proving ground  the basic methodologies are applicable to any setting for which one
is interested in analysis of matrices  and there is text associated with the rows or columns (e.g.  the
text may correspond to content on a website; each column of the matrix may represent a website 
and each row an individual  with the matrix representing number of visits).
The analysis of roll-call data is of signiﬁcant interest to political scientists [15  6]. In most such
research the binary data are typically analyzed with a probit or logistic link function  and the under-
lying real matrix is assumed to have rank one. Each legislator and piece of legislation exists at a
point along this one dimension  which is interpreted as characterizing a (one-dimensional) political
philosophy (e.g.  from “conservative” to “liberal”).
Roll-call data analysis have principally been interested in inferring the position of legislators in
the one-dimensional latent space  with this dictated in part by the fact that the ability to perform
prediction is limited. As in much matrix-completion research [17  18]  one typically can only infer
votes that are missing at random. It is not possible to predict the votes of legislators on a new piece
of legislation (for which  for example  an entire column of votes is missing). This has motivated the
joint analysis of roll-call votes and the associated legislation [23  8]: by modeling the latent space
of the text legislation with a topic model  and making connections between topics and the latent
space of the matrix decomposition  one may infer votes of an entire missing column of the matrix 
assuming access to the text associated with that new legislation.

1

While the research in [23  8] showed the potential of joint text-matrix analysis  there were several
open questions that motivate this paper. In [23  8] a latent Dirichlet allocation (LDA) [5] topic model
was employed for the text. It has been demonstrated that LDA yields inferior perplexity scores when
compared to modern Bayesian topic models  such as the focused topic model (FTM) [24]. Another
signiﬁcant issue with [23  8] concerns how the topic (text) and matrix models are coupled. In [23  8]
the frequency with which a given topic is utilized in the text legislation is used to infer the associated
matrix parameters (e.g.  to infer the latent feature vector associated with the respective column of
the matrix). This is undesirable  because the frequency with which a topic is used in the document
is characteristic of the style of writing: their may be a topic that is only mentioned brieﬂy in the
document  but that is critical to the outcome of the vote  while other topics may not impact the vote
but are discussed frequently in the legislation. We also wish to move beyond the rank-one matrix
assumption in [15  6  8].
Motivated by these limitations  in this paper the FTM is employed to model the text of legislation 
with each piece of legislation characterized by a latent binary vector that deﬁnes the sparse set of
associated topics. A new probabilistic low-rank matrix decomposition is developed for the votes 
utilizing latent binary features; this leverages the merits of what were previously two distinct lines
of matrix factorization methods [13  17]. Unlike previous approaches  the rank is not ﬁxed a priori
but inferred adaptively  with theoretical justiﬁcations. For a piece of legislation  the latent binary
feature vectors for the FTM and matrix decomposition are shared  yielding a new means of jointly
modeling text and matrices. This linkage between text and matrices is innovative as: (i) it’s based
on whether a topic is relevant to a document/legislation  not based on the frequency with which the
topic is used in the document (i.e.  not based on the style of writing); (ii) it enables interpretation of
the underlying latent binary features [13  9] based upon available text data. The rest of the paper is
organized as follows. Section 2 ﬁrst reviews the focused topic model  then introduces a new low-
rank matrix decomposition method  and the joint model of the two. Section 3 discusses posterior
inference. In Section 4 quantitative results are presented for prediction of columns of roll-call votes
based on the associated text legislation  and the joint model is demonstrated qualitatively to infer
meaning/insight for the characteristics of legislation and voting patterns  and Section 5 concludes.

2 Model and Analysis

2.1 Focused topic modeling

Focused topic model (FTM) [24] were developed to address a limitation of related models based
on the hierarchical Dirichlet process (HDP) [21]: the HDP shares a set of “global” topics across
all documents  and each topic is in general manifested with non-zero probability in each document.
This property of HDP tends to yield less “focused” or descriptive topics. It is desirable to share
a set of topics across all documents  but with the additional constraint that a given document only
utilize a small subset of the topics; this tends to yield more descriptive/focused topics  characteristic
of detailed properties of the documents. A FTM is manifested as a compound linkage of the Indian
buffet process (IBP) [10] and the Dirichlet process (DP). Each document draws latent binary features
from an IBP to select a ﬁnite subset of atoms/topics from the DP. In the model details  the DP is
represented in terms of a normalized gamma process [7] with weighting by the binary feature vector 
constituting a document-speciﬁc topic distribution in which only a subset of topics are manifested
with non-zero probability.
The key components of the FTM are summarized as follows [24]:

bjt|πt ∼ Bernoulli(bjt|πt) 

θj|{bj:  λ} ∼ Dirichlet(θj|bj: (cid:12) λ) 

πt =(cid:81)t

l=1 νt 

νt|αr ∼ Beta(νt|αr  1)

λt|γ ∼ Gamma(λt|γ  1)

(1)

1∈ {0  1} indicates if document j uses topic t  which is modeled as drawn from an IBP
where bjt
parameterized by αr under the stick breaking construction [20]  as shown in the ﬁrst line of (1).
λ = {λt}Kr
t=1 represents the relative mass on Kr topics (Kr could be inﬁnite in principle); λ is
shared across all documents  analogous to the “top layer” of the HDP. θj is the topic distribution
for the jth document  and the expression bj: (cid:12) λ denotes the pointwise vector product between
1Throughout this paper notation bij are used to denote the entry locates at the ith row and jth column in

matrix B  bj: and b:k are used to represent the jth row and kth column in B respectively.

2

k=1  a word is drawn as wjn|zjn {βk}Kr

bj: and λ  thereby selecting a subset of topics for document j (those for which the corresponding
components of bj: are non-zero). The rest of the FTM is constructed similar to LDA [5]  where for
each token n in document j  a topic indicator is drawn as zjn|θj ∼ Mult(zjn|1  θj). Conditional
on zjn and the topics {βk}Kr
k=1 ∼ Mult(wjn|1  βzjn )  where
βk|η ∼ Dirichlet(βk|η).
Although in (1) bj: is mainly designed to map the global prevalence of topics across the corpus 
λ  to a within-document proportion of topic usage  θj  latent features bj: are informative in their
own right  as they indicate which subset of topics is relevant to a given document. The document-
dependent topic usage bj: may be more important than θj when characterizing the meaning of a
document: θj speciﬁes the frequency with which each of the selected topics is utilized in document
j (this is related to writing style – verbosity or parsimony – and less related to meaning); it may be
more important to just know what underlying topics are used in the document  characterized by bj:.
We therefore make the linkage between documents and an associated matrix via the bj:  not based
on θj (where [23  8] base the document-matrix linkage via θj or it’s empirical estimate).

2.2 Matrix factorization with binary latent factors and a low-rank assumption
Binary matrix factorization (BMF) [13  14] is a general framework in which real latent matrix X ∈
RP×N is decomposed as X = LHRT   where L ∈ {0  1}P×Kl  R ∈ {0  1}N×Kr are binary  and
H ∈ RKl×Kr is real. The rows of L and R are modeled via IBPs  parameterized by αl and αr
respectively  and Kl and Kr are the truncation levels for the IBPs  which again can be inﬁnite in
principle. The observed matrix is Y  which may be real  binary  or categorial [12]. The observations
are modeled in an element-wise fashion: yij = f (xij). We focus on binary observed matrices 
Y ∈ {0  1}P×N   and utilize f (·) as a probit model [2]:

(cid:26) 1

if ˆxij ≥ 0
if ˆxij < 0

(2)

0

yij =
with ˆxij = xij + ij  where ij ∼ N (0  1).
We generalize the BMF framework by imposing that H is low-rank. Speciﬁcally  we impose the
:k  where u:k and v:k are column vectors (thus their outer product

rank-1 expansion H =(cid:80)Kc
To motivate this model  consider the representation H =(cid:80)Kc
LHRT   which implies X =(cid:80)Kc

k=1 u:kvT
u:k ∼ N (u:k|0  IKl )

is a rank-1 matrix)  each of them is modeled here by a Gaussian distribution:

v:k ∼ N (v:k|0  IKr )

k=1 u:kvT

(3)
and Kc is the number of such rank-1 matrices such that Kc < min(Kl  Kr)  i.e.  H is low-rank.
:k in the decomposition X =
k=1(Lu:k)(Rv:k)T . Therefore  we may also express X = ΨΦT  
with Ψ ∈ RP×Kc and Φ ∈ RN×Kc; the kth column of Ψ is deﬁned by Lu:k and the kth column
of Φ deﬁned by Rv:k. Consequently  the low-rank assumption for H yields a low-rank model
X = ΨΦT   precisely as in [17  18]. Thus the deﬁnition of Ψ and Φ via the binary matrices
L and R and the linkage matrix H merges previously two distinct lines of matrix factorization
methods. In the context of the application considered here  the decomposition X = LHRT will
prove convenient  as we may share the binary matrices L or R among the topic usage of available
documents. The binary features in L and R are therefore characteristic of the presence/absence of
underlying topics  or related latent processes  and the matrix H provides the mapping of how these
binary features map to observed data.
However  how to specify Kc remains an open question for the above low-rank construction. As a
contribution of this paper  we provide a new means of imposing a low-rank model within the prior.
We model the “signiﬁcance” of each rank-1 term in the expansion explicitly  using a stochastic
process {sk}Kc
:k  Kc can be inﬁnity in
principle. As a result  the hierarchical representation in modeling the latent matrix X in probit model
can be summarized as:

k=1  therefore H can be decomposed as H = (cid:80)Kc
ˆxij|(cid:110)
ˆxij|(cid:80)Kc

li:  rj: {u:k  v:k  sk}Kc

(cid:111) ∼ N(cid:16)

k=1 sk(li:u:k)(rj:v:k)T   1

k=1 sku:kvT

(cid:17)

(4)

k=1

Note that sk in (4) is similar to the singular value of SVD in spirit. Intuitively  we wish to impose
|sk| to decrease “fast” as the increase of index k  and the rank-1 matrices with large indices will have

3

negligible impact over (4)  therefore Kc plays a role similar to the truncation level in stick breaking
construction for DP [11] and IBP [20]. To achieve this end  we model each sk as a Gaussian random
variable with a conjugate multiplicative gamma process (MGP) placed on its precision parameter:

sk|τk ∼ N(cid:0)sk|0  τ−1

(cid:1)  

k

τk =(cid:81)k

δl|αc ∼ Gamma(δl|αc  1)

l=1 δl 

Theorem 1. When αc > 1  the sequence(cid:80)Kc

(5)
The MGP was originally proposed in [3] for learning sparse factor models and further extended for
tree-structured sparse factor models [26] and change-point stick breaking process [25]  one of its
properties is that it increasingly shrinks sk towards zero with the increase of index k. Next we make
the above intuition rigorous. Theorem 1 below formally states that if sk is modeled by MGP as in
(5)  the rank-1 expansion in (4) will converge when Kc → ∞.
k=1 sk(li:u:k)(rj:v:k)T converges in (cid:96)2  as Kc → ∞.
Although in MGP Kc is unbounded [3]  for computational considerations we would like to truncate
it to a ﬁnite value Kc (cid:28) max (P  N )  without much loss of information. As justiﬁcation  the
following theoretical bound is obtained  in a manner similar to its counterparts in DP [11].
Lemma 1. Denoting M Kc
} < ab(1−1/αc)

  where a = maxk E(li:u:k)2  b = maxk E(rj:v:k)2.

k=Kc+1 sk(li:u:k)(rj:v:k)T   then ∀ > 0 we have p{(M Kc

ij )2 >

ij =(cid:80)∞

αKc

c

Lemma 1 states that  when αc > 1 the approximation error introduced by the truncation level Kc
decays exponentially fast to 0  as Kc → ∞.
In Section 3 an MCMC method is developed to
adaptively choose Kc at each iteration  which alleviates us from ﬁxing it a priori. The proof of
Theorem 1 and Lemma 1 can be found in the Supplemental Material.

2.3

Joint learning of FTM and BMF

Via the FTM and BMF framework of the previous subsections  each piece of legislation j is rep-
resented as two latent binary feature vectors bj: and rj:. To jointly model the matrix of votes with
associated text of legislation  a natural choice is to impose bj: = rj:. As a result  the full joint model
can be speciﬁed by equations (1) - (5)  with bjt in (1) replaced by rjt. Note that the joint model links
the topics characteristic of the text  to the latent binary features characteristic of legislation in the
matrix decomposition; and such linkage leverages statistical strength of the two data source across
the latent variables of the joint model during posterior inference. A graphical representation of the
joint model can be found in the Supplemental Material.
In the context of the model for Y = f (X)  with X = LHRT   if one were to learn L and H
based upon available training data  then a new legislation y:N +1 could be predicted if we had access
to r:N +1. Via the construction above  not only do we gain a predictive advantage  because the
new legislation’s latent binary features r:N +1 can be obtained from modeling its document as in
(1)  but also the model provides powerful interpretative insights. Speciﬁcally the topics inferred
from the documents may be used to interpret the latent binary features associated with the matrix
factorization. These advantages will be demonstrated through experiments on legislative roll-call
data in Section 4.

2.4 Related work

The ideal point topic model (IPTM) was developed in [8]  where the supervised Latent Dirichlet
Allocation (sLDA) [4] model was used to link empirical topic-usage frequencies to the latent factors
via regression. In that work the dimension of the latent factors was set to 1  e.g.  ﬁxing Kc = 1 in our
nomenclature. In [23] the authors proposed to jointly analyze the voting matrix and the associated
text through a mixture model  where each legislation’s latent feature factor is clustered to a mixture
component in coupled with that legislation’s document topic distribution θ. Note that in their case
each piece of legislation can only belong to one cluster  while in our case the latent binary features
for each document can be effectively treated as being grouped to multiple clusters [13] (a mixed-
membership model  manifested in terms of the binary feature vectors). Similar research in linking
collaborative ﬁltering and topic models can also be found in web content recommendation [1]  movie
recommendation[19]  and scientiﬁc paper recommendation [22]. None of these methods makes use
of the binary indicators as the characterization of associated documents  but perform linking via the
topic distribution θ and the latent (real) features in different ways.

4

ten as p(v:k|−) ∝ (cid:81)N

3 Posterior Inference
We use Gibbs sampling for posterior inference over the latent variables  and only sampling equations
that are unique for this model are discussed here. The rest are similar to those in [24  13]. In the
following we use p(·|−) to denote the conditional posterior of one variable given on all others.
Sampling {v:k  u:k}k=1:Kc Based on (3) and (4) the conditional posterior of v:k can be writ-
k=1 sk(Lu:k)(rj:v:k)  1)N (v:k|0  IKr ). It can be shown that
and covari-
j: +

p(v:k|−) = N (v:k|µv:k   Σv:k )  with mean µv:k = skΣv:k
ance matrix Σv:k = [IKr + s2
k
Lu:krj:v:k. By repeating the above procedure p(u:k|−) can be derived similarly.
Sampling {sk}k=1:Kc Based on (4) and (5) the conditional posterior of sk can be written
It can be shown that
and variance

j=1 N ( ˆx:j|(cid:80)Kc
(cid:80)N
j=1(Lu:krj:)T (Lu:krj:)]−1  where ˜x−k
j=1 N ( ˆx:j|(cid:80)Kc
(cid:1)  with mean µsk = σ2
(cid:16)

(cid:80)N
j=1((Lu:k)(rj:v:k))T ˜x−k
(cid:80)Kc

as p(sk|−) ∝ (cid:81)N
p(sk|−) = N(cid:0)sk|µsk   σ2
= 1/(τk +(cid:80)N
l =(cid:81)l

(cid:80)N
j=1(Lu:krj:)T ˜x−k

k=1 sk(Lu:k)(rj:v:k)  1)N (sk|0  τ−1
k ).

:j = ˆx:j − LUVT rT

  1 + 1
2

l s2
l

sk

sk

:j

:j

j=1((Lu:k)(rj:v:k))T ((Lu:k)(rj:v:k))).

δk|αc + Kc−k+1
t=1 t(cid:54)=k δt. τk can then be reconstructed from δ1:k as in (5).

σ2
sk
Sampling {τk  δk}k=1:Kc Based on (5)  given a ﬁxed truncation level Kc it can be sampled directly
from its posterior distribution: p(δk|−) = Gamma
  where
ν(k)
Sampling {rjt}j=1:N t=1:Kr
Similar to the derivation in [24]  p(rjt = 1|−) = 1 if Njt >
0  where Njt denotes the number of times document j used topic t. When Njt = 0 
based on (1) and (4) the conditional posterior of rjt can be written as p(rjt = 1|−) ∝
(cid:80)Kc
πt+2λt (1−πt) exp{− 1
:j ]}  where ht: represents the tth row of H =
k=1 sku:kvT

t: ) − 2(LhT
:k; and p(rjt = 0|−) ∝ 2λt (1−πt)

i=1:P is sampled as described in [13].

πt+2λt (1−πt). {lit}t=1:Kl

t: )T ˜x−k

t: )T (LhT

l=k ν(k)

2 [(LhT

(cid:17)

πt

2

Adaptive sampler for MGP The above Gibbs sampler needs a predeﬁned truncation level Kc.
In [3  26] the authors proposed an adaptive sampler  tuning Kc as the sampler progresses  with
convergence of the chain guaranteed [16]. Speciﬁcally  the adaptation procedure is triggered with
probability p(t) = exp(z0 + z1t) at the tth iteration  with z0  z1 chosen so that adaptation occurs
frequently at the beginning of the chain but decreases exponentially fast. When the adaptation is
triggered in the tth iteration  let qκ(t) = {k|d∞(skLu:kvT
:kRT ) ≤ κ} denotes in iteration t the
indices of the rank-1 matrices with the maximum-valued entry less than some pre-deﬁned threshold
κ  which intuitively has a negligible contribution at the tth iteration  and thus are deleted and Kc will
decrease. On the other hand  if qκ(t) is empty then it suggests that more rank-1 matrices are needed 
in this case we increase Kc by one and draw u:Kc  v:Kc from their prior distributions respectively.

4 Experimental Results

4.1 Experiment setting

We have performed joint matrix and text analysis  considering the House of Representatives (House) 
sessions 106 - 111 2; we model each session’s roll-call votes separately as binary matrix Y. Entry
yij = 1 denotes that the ith legislator’s response to legislation j is either “Yea” or “Yes”   and yij
= 0 denotes that the corresponding response is either “Nay” or “No”. The data are preprocessed in
the same way as described in [8]. We recommend to set the IBP hyperparameters αl = αr = 1 
MGP hyperparameters αc = 3  FTM hyperparameters γ = 5 and topic model hyperparameter
η = 0.01. We also considered using a random-walk MH algorithm with non-informative gamma
prior to infer those hyperparameters  as described in [24  3]  and the Markov chain manifested
similar mixing performance. The truncation level Kc in the MGP is not ﬁxed  but inferred from
the adaptive sampler  with threshold parameter κ set to 0.05 (it is recommended to be set small for
most applications). In the study below  for each model we run 5000 iterations of the Gibbs sampler 
with the ﬁrst 1000 iterations discarded as burn-in  and 400 samples are collected  taking every tenth
iteration afterwards  to perform Bayesian estimate on the object of interest.

2These data are available from thomas.loc.gov

5

4.2 Predicting random missing votes

In this section we study the classical problem of estimating the values of matrix data that are missing
uniformly at random (in-matrix missing votes)  without the use of associated documents. We com-
pare the model proposed in (4) to the probabilistic matrix factorization (PMF) found in [17  18]. This
is done by decomposing the latent matrix X = ΨΦT   where each row of Ψ and ΦT are drawn from
a Gaussian distribution with mean and covariance matrix modeled by a Gaussian-Wishart distribu-
tion. To study the behavior of the proposed MGP prior in (5)  we (i) vary the number of columns
(rank) Kc in Ψ and Φ as a free parameter  and call this model PMF; and (ii) incorporate MGP into
the decomposition of X = ΨSΦT where S ∈ RKc×Kc is a diagonal matrix with each diagonal ele-
ment speciﬁed as sk. The model in (ii) is called PMF+MGP. Additionally  to check if the low-rank
assumption detailed in Section 2.2 is effective for BMF  we also compare the performance of the
BMF model originally proposed in [13]  which we term BMF-Original.
We compared these models on predicting the missing values selected uniformly at random  with
different percentage (90%  95%  99%) of missingness. This study has been done on House data
from the 106 to 111 sessions; however  to conserve space we only summarized the experimental
results on the 110th House data  in Figure 1; similar results are observed across all sessions. In
Figure 1 each panel corresponds to a certain percentage of missingness; the horizontal axis is the
number of columns (rank)  which varies as a free parameter of PMF  while the vertical axis is
the prediction accuracy. MGP is observed to be generally effective in modeling the rank across
three panels  and the low-rank assumption is critical to get good performance for the BMF. When
the percentage of missingness is relatively low  e.g.  90% or 95%  PMF performs better than BMF 
however when the percentage of missingness is high e.g.  99%  the BMF (with low rank assumption)
is very competitive with PMF. This is probably because of the way BMF encourages the sharing of
statistical strength among all rows and columns via the matrix H as described in [13]  which is most
effective when data is scarce.

4.3 Predicting new bills based on text

We study the predictive power of the proposed model when the legislative roll-call votes and the
associated bill documents are modeled jointly  as described in Section 2.3. We compare our proposed
model with the IPTM in [8]  where the authors ﬁxed the rank Kc = 1 in IPTM; we term this
model IPTM(Kc = 1).
In [8] the authors suggested that ﬁxing the rank to one might be over-
restrictive  thus we also propose to model the rank in the ideal point model using MGP  in a similar
way to how this was done for the PMF model  and call this model IPTM. We also compare our
model with that in [23]  where the authors proposed to combine the factor analysis model and topic
model via a compounded mixture model  with all sessions of roll-call data are modeled jointly
via a Markov process. Since our main goal is to predict new bills but not modeling the matrices
dynamically  in the following experiments we remove the Markov process but model each session of
House data separately; we call this model FATM. In [23] the authors proposed to use a beta-Bernoulli
distributed binary variable bk to model if the kth rank-1 matrix is used in matrix decomposition.
When performing posterior inference we ﬁnd that bk tends to be easily trapped in local maxima 
while MGP  which models the signiﬁcance of usage (but not the binary usage) of each kth rank-1
matrix via sk  smoother estimates and better mixing were observed.
For each session the bills are partitioned into 6-folds  and we iteratively remove a fold  and train the
model with the remaining folds; predictions are then performed on the bills in the removed fold. The
experiment results are summarized in Figure 2. Note that since rj: is modeled via the stick-breaking
construction of IBP as in (1)  the total number of latent binary features Kr is unbounded  and we
face the risk of having the latent binary features important for explaining voting Y and important for
explaining the associated text learned separately. This may lead to the undesirable consequence that
the latent features learned from text are not discriminative in predicting a new piece of legislation.
To reduce such risk  in practice we could either set αr such that it strongly favor fewer latent binary
features  or we can truncate the stick breaking construction at a pre-deﬁned level Kr. For a clearer
comparison with other models  where the number of topics are ﬁxed  we choose the second approach
and let Kr vary as the maximum number of possible topics.
Across all sessions IPTM consistently performs better than its counterpart when Kc = 1; this again
demonstrates the effectiveness of MGP in modeling the rank. Although there is no signiﬁcant advan-

6

Figure 1: Comparison of prediction accuracy for votes missing uniformly at random  for the 110th House
data. Different panels corresponds to different percentage of missingness  for each panel the vertical axis
represents accuracy and horizontal axis represents the rank set for PMF. For PMF+MGP and our proposed
method  inferred rank Kc is shown for the most-probable collection sample.

Figure 2: Prediction accuracy for held-out legislation across 106th - 111th House data; prediction of an entire
column of missing votes based on text. In each panel the vertical axis represents accuracy and the horizontal
axis represents the number of topics used for each model. Results are averaged across 6-folds  with variances
are too small to see.

tage of our proposed model when the truncation on the number of topics Kr (horizontal axis) is small
(e.g.  30-50)  over-ﬁtting is observed for all models except our proposed model. As we increase the
number of topics  the performance of other models drop signiﬁcantly (vertical axis). Across all ﬁve
sessions  the best quantitative results are obtained by the proposed model when Kr > 100.

4.4 Latent binary feature interpretation

In this study we partition all the bills into two groups: (i) bills for which there is near-unanimous
agreement  with “Yea” or “Yes” more than 90%; (ii) contentious bills with percentage of votes
received as “Yea” or “Yes” less than 60%. By linking the inferred binary latent features to the
topics for those two groups  we can get insight into the characteristics of legislation and voting
patterns  e.g.  what inﬂuenced a near-unanimous yes vote  and what inﬂuenced more contention.
Figure 3 compares the latent feature usage pattern of those two groups; the horizontal axis represents
the latent features  where we set Kr = 100 for illustration purpose  and the vertical axis is the
aggregated frequency that a feature/topic is used by all the bills in each of those two groups. The
frequency is normalized within each group for easy interpretation. For each group  we select three
discriminative features: ones heavily used in one group but rarely used in the other (these selected
features are highlighted in blue/red). For example  in the left panel the features highlighted in blue
are widely used by bills in the left group  but rarely used by bills in the right group. As observed

7

0.960.9790% Missing0.950.960.9795% Missing0890.90.9199% MissingKc= 12Kc= 9Kc= 12Kc= 12Kc= 11Kc= 130.930.940.950.920.930.940.950860.870.880.89Kc 12c0.92151020500.90.9115102050ProposedBMF‐OriginalPMFPMF+MGP0.850.86151020500.890.910.93108th0.880.90.92107th0.880.90.92106th0.810.830.850.870.80.820.840.860.80.820.840.860.790.8130501001502003000.780.830501001502003000.7830501001502003000.870.890.91109th0880.90.92110th0.910.930.95111th0790.810.830.850.820.840.860.880830.850.870.890.770.7930501001502003000.83050100150200300ProposedFATMIPTMIPTM(Kc = 1)0.810.833050100150200300Figure 3: Comparison of the frequencies of binary features usage between two groups of bills  left: near-
unanimous afﬁrmative bills (e.g.  bills with percentage of votes received as “Yes” or “Yea” is more than 90%).
Right: contentious bills (e.g.  bills with percentage of votes received as “Yes” or “Yea” is less than 60%).
Data from 110th House  when Kr = 100. The vertical axis represents the normalized frequency of using
feature/topic within the corresponding group. The six most discriminative features/topics (labeled in the ﬁgure)
are shown in Table 1
Table 1: Six discriminative topics of unanimous agreed/highly debated bills learned from the 110th house of
representatives  with top-ten most probable words shown. (R) and (B) represent the topics depicted in Figure 3
as red and blue respectively.

TOPIC 31(R)

TOPIC 38 (R)

TOPIC 62 (B)

TOPIC 73 (B)

TOPIC 83 (R)

TOPIC 22 (B)

CHILDREN

CHILD
YOUTH

PORNOGRAPHY

INTERNET

FATHER
FAMILY
PARENT
SCHOOL

CONCURRENT RESOLUTION

TAX

ADJOURN

MAJORITY LEADER

DESIGNEE
AVIATION
RECESS

MINORITY LEADER

FEBRUARY

MOTION OFFER

CORPORATION

TAXABLE
CREDIT
PENALTY
REVENUE
TAXPAYER
SPECIAL

FILE

PEOPLE
WORLD
HOME

SANITATION

WATER

INTERNATIONAL

SOUTHERN

COMPENSATION

ASSOCIATION

ECONOMIC

NATION
ATTACK

TERRORIST

PEOPLE

SEPTEMBER
VOLUNTEER

CITIZEN
PAKISTAN
LEGITIMATE

FUTURE

CLAUSE
PRINT
WAIVE

SUBSTITUTE

COMMITTEE AMENDMENT

READ
DEBATE
OFFER

DIVIDE AND CONTROL

MOTION

EMERGENCY

STAND

SUBSTITUTE

from Figure 3  the learned binary features are discriminative  as the usage pattern for those two
groups are quite different.
We also study the interpretation of those latent features by linking them to the topics inferred from
the texts. As an example  those six highlighted features are linked to their corresponding topics
and depicted in Table 1  with the top-ten most probable words within each topic shown. For the
unanimous agreed bills  we can read from Table 1 that they are highly probable to be related to
topics about the education of youth (Topic 22)  or the prevention of terrorist (Topic 73). While the
bills from the contentious group tend to more related to making amendments to an existing piece of
legislation (Topic 83) or discussing taxation (Topic 38). Note that compared to conventional topic
modeling  these inferred topics are not only informative in semantic meaning of the bills  but also
discriminative in predicting the outcome of the bills.

5 Conclusion

A new methodology has been developed for the joint analysis of a matrix with associated text 
based on sharing latent binary features modeled via the Indian buffet process. The model has been
demonstrated on analysis of voting data from the US House of Representatives. Imposition of a low-
rank representation for the latent real matrix has proven important  with this done in a new manner
via the multiplicative gamma process. Encouraging quantitative results are demonstrated  and the
model has also been shown to yield interesting insights into the meaning of the latent features. The
sharing of latent binary features provides a general joint learning framework for Indian buffet process
based models [9]  where focused topic model and binary matrix factorization are two examples 
exploring other possibilities in different scenarios could be an interesting direction.

Acknowledgements
The authors would like to thank anonymous reviewers for providing useful comments. The research
reported here was supported by ARO  DOE  NGA  ONR  and DARPA (under the MSEE program).

8

010203040506070809010000.010.020.030.040.050.060.070.08Binary feature usage pattern for unanimous agreed bills 010203040506070809010000.020.040.060.080.1Binary feature usage pattern for highly debated billsTopic 22Topic 62Topic 73Topic 31Topic 83Topic 38Topic 22Topic 38Topic 31Topic 62Topic 73Topic 83References
[1] D. Agarwal and B. Chen. fLDA: matrix factorization through latent Dirichlet allocation. In

WSDM  2010.

[2] J. H. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. Jour-

nal of the American Statistical Association  1993.

[3] A. Bhattacharya and D. B. Dunson. Sparse Bayesian inﬁnite factor models. Biometrika  2011.
[4] D. M. Blei and Jon D. McAuliffe. Supervised topic models. In Advances in Neural Information

Processing Systems  2007.

[5] D. M. Blei  A. Ng  and M. I. Jordan. Latent Dirichlet allocation. JMLR  2003.
[6] J. Clinton  S. Jackman  and D. Rivers. The statistical analysis of roll call data. Am. Political

Sc. Review  2004.

[7] T. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics 

1973.

[8] S. Gerrish and D.M. Blei. Predicting legislative roll calls from text. In ICML  2011.
[9] T. L. Grifﬁths and Z. Ghahramani. The indian buffet process: An introduction and review.

Journal of Machine Learning Research  12:1185–1224  2011.

[10] T.L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Advances in Neural Information Processing Systems  2005.

[11] H. Ishwaran and L.F. James. Gibbs sampling methods for stick-breaking priors. J. American

Statistical Association  2001.

[12] P. McCullagh and J. Nelder. Generalized Linear Models. Chapman and Hall  1989.
[13] E. Meeds  Z. Ghahramani  R. Neal  and S. Roweis. Modeling dyadic data with binary latent

factors. In Advances in Neural Information Processing Systems. 2007.

[14] K. Miller  T. Grifﬁths  and M.I. Jordan. Nonparametric latent feature models for link predic-

tion. In Advances in Neural Information Processing Systems  2009.

[15] K.T. Poole. Recent developments in analytical models of voting in the U.S. congress. Am.

Political Sc. Review  1988.

[16] G. O. Roberts and J. S. Rosenthal. Coupling and ergodicity of adaptive MCMC. Journal of

Applied Probability  2007.

[17] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization.

Information Processing Systems  2007.

In Advances in Neural

[18] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain

Monte Carlo. In ICML  2008.

[19] H. Shan and A. Banerjee. Generalized probabilistic matrix factorizations for collaborative

ﬁltering. In ICDM  2010.

[20] Y. W. Teh  D. Görür  and Z. Ghahramani. Stick-breaking construction for the Indian buffet

process. In AISTATS  2007.

[21] Y. W. Teh  M. I. Jordan  Matthew J. Beal  and D. M. Blei. Hierarchical Dirichlet processes.

Journal of the American Statistical Association  2006.

[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientiﬁc articles.

In KDD  2011.

[23] E. Wang  D. Liu  J. Silva  D. B. Dunson  and L. Carin. Joint analysis of time-evolving binary
matrices and associated documents. In Advances in Neural Information Processing Systems 
2010.

[24] S. Williamson  C. Wang  K. A. Heller  and D. M. Blei. The IBP compound Dirichlet process

and its application to focused topic modeling. In ICML  2010.

[25] X. Zhang  D. Dunson  and L. Carin. Hierarchical topic modeling for analysis of time-evolving

personal choices. In Advances in Neural Information Processing Systems 24. 2011.

[26] X. Zhang  D. Dunson  and L. Carin. Tree-structured inﬁnite sparse factor model. In ICML 

2011.

9

,Mehmet Gönen
Adam Margolin
Emmanuel Abbe
Colin Sandon
Dong Liu
Haochen Zhang
Zhiwei Xiong