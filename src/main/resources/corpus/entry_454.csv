2019,Adversarial Training and Robustness for Multiple Perturbations,Defenses against adversarial examples  such as adversarial training  are typically tailored to a single perturbation type (e.g.  small $\ell_\infty$-noise). For other perturbations  these defenses offer no guarantees and  at times  even increase the model's vulnerability.
Our aim is to understand the reasons underlying this robustness trade-off  and to train models that are simultaneously robust to multiple perturbation types.

We prove that a trade-off in robustness to different types of $\ell_p$-bounded and spatial perturbations must exist in a natural and simple statistical setting.
We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes  as well as an efficient attack for the $\ell_1$-norm  and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. 
In particular  we find that adversarial training with first-order $\ell_\infty  \ell_1$ and $\ell_2$ attacks on MNIST achieves merely $50\%$ robust accuracy  partly because of gradient-masking.
Finally  we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.,Adversarial Training and Robustness for

Multiple Perturbations

Florian Tramèr
Stanford University

Dan Boneh

Stanford University

Abstract

Defenses against adversarial examples  such as adversarial training  are typically
tailored to a single perturbation type (e.g.  small (cid:96)∞-noise). For other perturbations 
these defenses offer no guarantees and  at times  even increase the model’s vulnera-
bility. Our aim is to understand the reasons underlying this robustness trade-off 
and to train models that are simultaneously robust to multiple perturbation types.
We prove that a trade-off in robustness to different types of (cid:96)p-bounded and spatial
perturbations must exist in a natural and simple statistical setting. We corroborate
our formal analysis by demonstrating similar robustness trade-offs on MNIST
and CIFAR10. We propose new multi-perturbation adversarial training schemes 
as well as an efﬁcient attack for the (cid:96)1-norm  and use these to show that models
trained against multiple attacks fail to achieve robustness competitive with that of
models trained on each attack individually. In particular  we ﬁnd that adversarial
training with ﬁrst-order (cid:96)∞  (cid:96)1 and (cid:96)2 attacks on MNIST achieves merely 50%
robust accuracy  partly because of gradient-masking. Finally  we propose afﬁne
attacks that linearly interpolate between perturbation types and further degrade the
accuracy of adversarially trained models.

1

Introduction

Adversarial examples [37  15] are proving to be an inherent blind-spot in machine learning (ML)
models. Adversarial examples highlight the tendency of ML models to learn superﬁcial and brittle
data statistics [19  13  18]  and present a security risk for models deployed in cyber-physical systems
(e.g.  virtual assistants [5]  malware detectors [16] or ad-blockers [39]).
Known successful defenses are tailored to a speciﬁc perturbation type (e.g.  a small (cid:96)p-ball [25  28  42]
or small spatial transforms [11]). These defenses provide empirical (or certiﬁable) robustness
guarantees for one perturbation type  but typically offer no guarantees against other attacks [35 
31]. Worse  increasing robustness to one perturbation type has sometimes been found to increase
vulnerability to others [11  31]. This leads us to the central problem considered in this paper:

Can we achieve adversarial robustness to different types of perturbations simultaneously?

Note that even though prior work has attained robustness to different perturbation types [25  31  11] 
these results may not compose. For instance  an ensemble of two classiﬁers—each of which is robust
to a single type of perturbation—may be robust to neither perturbation. Our aim is to study the extent
to which it is possible to learn models that are simultaneously robust to multiple types of perturbation.
To gain intuition about this problem  we ﬁrst study a simple and natural classiﬁcation task  that has
been used to analyze trade-offs between standard and adversarial accuracy [41]  and the sample-
complexity of adversarial generalization [30]. We deﬁne Mutually Exclusive Perturbations (MEPs) as
pairs of perturbation types for which robustness to one type implies vulnerability to the other. For this
task  we prove that (cid:96)∞ and (cid:96)1-perturbations are MEPs and that (cid:96)∞-perturbations and input rotations

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) MNIST models trained on (cid:96)1  (cid:96)2 & (cid:96)∞ attacks.

(b) MNIST models trained on (cid:96)∞ and RT attacks.

(c) CIFAR10 models trained on (cid:96)1 and (cid:96)∞ attacks.
(d) CIFAR10 models trained on (cid:96)∞ and RT attacks.
Figure 1: Robustness trade-off on MNIST (top) and CIFAR10 (bottom). For a union of (cid:96)p-balls
(left)  or of (cid:96)∞-noise and rotation-translations (RT) (right)  we train models Advmax on the strongest
perturbation-type for each input. We report the test accuracy of Advmax against each individual
perturbation type (solid line) and against their union (dotted brown line). The vertical lines show the
adversarial accuracy of models trained and evaluated on a single perturbation type.

and translations [11] are also MEPs. Moreover  for these MEP pairs  we ﬁnd that robustness to either
perturbation type requires fundamentally different features. The existence of such a trade-off for this
simple classiﬁcation task suggests that it may be prevalent in more complex statistical settings.
To complement our formal analysis  we introduce new adversarial training schemes for multiple
perturbations. For each training point  these schemes build adversarial examples for all perturbation
types and then train either on all examples (the “avg” strategy) or only the worst example (the “max”
strategy). These two strategies respectively minimize the average error rate across perturbation types 
or the error rate against an adversary that picks the worst perturbation type for each input.
For adversarial training to be practical  we also need efﬁcient and strong attacks [25]. We show that
Projected Gradient Descent [22  25] is inefﬁcient in the (cid:96)1-case  and design a new attack  Sparse (cid:96)1
Descent (SLIDE)  that is both efﬁcient and competitive with strong optimization attacks [8] 
We experiment with MNIST and CIFAR10. MNIST is an interesting case-study  as distinct models
from prior work attain strong robustness to all perturbations we consider [25  31  11]  yet no single
classiﬁer is robust to all attacks [31  32  11]. For models trained on multiple (cid:96)p-attacks ((cid:96)1  (cid:96)2  (cid:96)∞
for MNIST  and (cid:96)1  (cid:96)∞ for CIFAR10)  or on both (cid:96)∞ and spatial transforms [11]  we conﬁrm a
noticeable robustness trade-off. Figure 1 plots the test accuracy of models Advmax trained using our
“max” strategy. In all cases  robustness to multiple perturbations comes at a cost—usually of 5-10%
additional error—compared to models trained against each attack individually (the horizontal lines).
Robustness to (cid:96)1  (cid:96)2 and (cid:96)∞-noise on MNIST is a striking failure case  where the robustness trade-off
is compounded by gradient-masking [27  40  1]. Extending prior observations [25  31  23]  we show
that models trained against an (cid:96)∞-adversary learn representations that mask gradients for attacks in
other (cid:96)p-norms. When trained against ﬁrst-order (cid:96)1  (cid:96)2 and (cid:96)∞-attacks  the model learns to resist
(cid:96)∞-attacks while giving the illusion of robustness to (cid:96)1 and (cid:96)2 attacks. This model only achieves
52% accuracy when evaluated on gradient-free attacks [3  31]. This shows that  unlike previously
thought [41]  adversarial training with strong ﬁrst-order attacks can suffer from gradient-masking.
We thus argue that attaining robustness to (cid:96)p-noise on MNIST requires new techniques (e.g.  training
on expensive gradient-free attacks  or scaling certiﬁed defenses to multiple perturbations).
MNIST has sometimes been said to be a poor dataset for evaluating adversarial examples defenses 
as some attacks are easy to defend against (e.g.  input-thresholding or binarization works well for
(cid:96)∞-attacks [41  31]). Our results paint a more nuanced view: the simplicity of these (cid:96)∞-defenses

2

0246810Epochs0.000.250.500.751.00AccuracyAdv∞Adv1Adv2Advmaxtestedonℓ∞Advmaxtestedonℓ1Advmaxtestedonℓ2Advmaxtestedonall0246810Epochs0.000.250.500.751.00AccuracyAdv∞AdvRTAdvmaxtestedonℓ∞AdvmaxtestedonRTAdvmaxtestedonboth020000400006000080000Steps0.40.50.60.70.8AccuracyAdv∞Adv1Advmaxtestedonℓ∞Advmaxtestedonℓ1Advmaxtestedonboth020000400006000080000Steps0.40.50.60.70.8AccuracyAdv∞AdvRTAdvmaxtestedonℓ∞AdvmaxtestedonRTAdvmaxtestedonbothbecomes a disadvantage when training against multiple (cid:96)p-norms. We thus believe that MNIST
should not be abandoned as a benchmark just yet. Our inability to achieve multi-(cid:96)p robustness for this
simple dataset raises questions about the viability of scaling current defenses to more complex tasks.
Looking beyond adversaries that choose from a union of perturbation types  we introduce a new afﬁne
adversary that may linearly interpolate between perturbations (e.g.  by compounding (cid:96)∞-noise with
a small rotation). We prove that for locally-linear models  robustness to a union of (cid:96)p-perturbations
implies robustness to afﬁne attacks. In contrast  afﬁne combinations of (cid:96)∞ and spatial perturbations
are provably stronger than either perturbation individually. We show that this discrepancy translates
to neural networks trained on real data. Thus  in some cases  attaining robustness to a union of
perturbation types remains insufﬁcient against a more creative adversary that composes perturbations.
Our results show that despite recent successes in achieving robustness to single perturbation types 
many obstacles remain towards attaining truly robust models. Beyond the robustness trade-off 
efﬁcient computational scaling of current defenses to multiple perturbations remains an open problem.
The code used for all of our experiments can be found here: https://github.com/ftramer/
MultiRobustness
Proofs of all theorems  experimental setups  and additional experiments are in the full version of this
extended abstract [38].

2 Theoretical Limits to Multi-perturbation Robustness

We study statistical properties of adversarial robustness in a natural statistical model introduced in [41] 
and which exhibits many phenomena observed on real data  such as trade-offs between robustness and
accuracy [41] or a higher sample complexity for robust generalization [31]. This model also proves
useful in analyzing and understanding adversarial robustness for multiple perturbations. Indeed 
we prove a number of results that correspond to phenomena we observe on real data  in particular
trade-offs in robustness to different (cid:96)p or rotation-translation attacks [11].
We follow a line of works that study distributions for which adversarial examples exist uncondition-
ally [41  21  33  12  14  26]. These distributions  including ours  are much simpler than real-world
data  and thus need not be evidence that adversarial examples are inevitable in practice. Rather  we
hypothesize that current ML models are highly vulnerable to adversarial examples because they learn
superﬁcial data statistics [19  13  18] that share some properties of these simple distributions.
In prior work  a robustness trade-off for (cid:96)∞ and (cid:96)2-noise is shown in [21] for data distributed over
two concentric spheres. Our conceptually simpler model has the advantage of yielding results beyond
(cid:96)p-norms (e.g.  for spatial attacks) and which apply symmetrically to both classes. Building on
work by Xu et al. [43]  Demontis et al. [9] show a robustness trade-off for dual norms (e.g.  (cid:96)∞ and
(cid:96)1-noise) in linear classiﬁers.

2.1 Adversarial Risk for Multiple Perturbation Models
Consider a classiﬁcation task for a distribution D over examples x ∈ Rd and labels y ∈ [C]. Let
f : Rd → [C] denote a classiﬁer and let l(f (x)  y) be the zero-one loss (i.e.  1f (x)(cid:54)=y).
We assume n perturbation types  each characterized by a set S of allowed perturbations for an input
x. The set S can be an (cid:96)p-ball [37  15] or capture other perceptually small transforms such as image
rotations and translations [11]. For a perturbation r ∈ S  an adversarial example is ˆx = x + r (this
is pixel-wise addition for (cid:96)p perturbations  but can be a more complex operation  e.g.  for rotations).
For a perturbation set S and model f  we deﬁne Radv(f ; S) := E(x y)∼D [maxr∈S l(f (x + r)  y)]
as the adversarial error rate. To extend Radv to multiple perturbation sets S1  . . .   Sn  we can consider
the average error rate for each Si  denoted Ravg
adv. This metric most clearly captures the trade-off in
robustness across independent perturbation types  but is not the most appropriate from a security
perspective on adversarial examples. A more natural metric  denoted Rmax
adv   is the error rate against an
adversary that picks  for each input  the worst perturbation from the union of the Si. More formally 
(1)
adv ≥ Ravg
adv.

(cid:80)
i Radv(f ; Si) .
adv(f ; S1  . . .   Sn) := 1
n
adv  which also hold for Ravg
max since Rmax

Rmax
adv (f ; S1  . . .   Sn) := Radv(f ;∪iSi)   Ravg
Most results in this section are lower bounds on Ravg

3

Two perturbation types S1  S2 are Mutually Exclusive Perturbations (MEPs)  if Ravg
1/|C| for all models f (i.e.  no model has non-trivial average risk against both perturbations).

adv(f ; S1  S2) ≥

2.2 A binary classiﬁcation task

We analyze the adversarial robustness trade-off for different perturbation types in a natural statistical
model introduced by Tsipras et al. [41]. Their binary classiﬁcation task consists of input-label pairs
(x  y) sampled from a distribution D as follows (note that D is (d + 1)-dimensional):

u.a.r∼ {−1  +1} 

y

(cid:26)+y  w.p. p0 

x0 =

−y  w.p. 1 − p0

√
where p0 ≥ 0.5  N (µ  σ2) is the normal distribution and η = α/
For this distribution  Tsipras et al. [41] show a trade-off between standard and adversarial accuracy
(for (cid:96)∞ attacks)  by drawing a distinction between the “robust” feature x0 that small (cid:96)∞-noise cannot
manipulate  and the “non-robust” features x1  . . .   xd that can be fully overridden by small (cid:96)∞-noise.

d for some positive constant α.

 

x1  . . .   xd

i.i.d∼ N (yη  1)  

(2)

2.3 Small (cid:96)∞ and (cid:96)1 Perturbations are Mutually Exclusive

is highly vulnerable to (cid:96)1-perturbations of size  ≥ 1. The second classiﬁer  h(x) = sign((cid:80)d
is robust to (cid:96)1-perturbations of average norm below E[(cid:80)d

The starting point of our analysis is the observation that the robustness of a feature depends on the
considered perturbation type. To illustrate  we recall two classiﬁers from [41] that operate on disjoint
feature sets. The ﬁrst  f (x) = sign(x0)  achieves accuracy p0 for all (cid:96)∞-perturbations with  < 1 but
√
i=1 xi)
√
d)  yet it is fully subverted
i=1 xi] = Θ(
by a (cid:96)∞-perturbation that shifts the features x1  . . .   xd by ±2η = Θ(1/
d). We prove that this
tension between (cid:96)∞ and (cid:96)1 robustness  and of the choice of “robust” features  is inherent for this task:
Theorem 1. Let f be a classiﬁer for D. Let S∞ be the set of (cid:96)∞-bounded perturbations with  = 2η 
and S1 the set of (cid:96)1-bounded perturbations with  = 2. Then  Ravg
The proof is in Appendix F. The bound shows that no classiﬁer can attain better Ravg
than a trivial constant classiﬁer f (x) = 1  which satisﬁes Radv(f ; S∞) = Radv(f ; S1) = 1/2.
Similar to [9]  our analysis extends to arbitrary dual norms (cid:96)p and (cid:96)q with 1/p + 1/q = 1 and p < 2.
The perturbation required to ﬂip the features x1  . . .   xn has an (cid:96)p norm of Θ(d
2 ) = ω(1) and an
(cid:96)q norm of Θ(d
p ) = o(1). Thus  feature x0 is more robust than features x1  . . .   xn
with respect to the (cid:96)q-norm  whereas for the dual (cid:96)p-norm the situation is reversed.

adv(f ; S∞  S1) ≥ 1/2 .

adv (and thus Rmax
adv )

2 ) = Θ(d

p− 1

2− 1

1

q − 1

1

1

2.4 Small (cid:96)∞ and Spatial Perturbations are (nearly) Mutually Exclusive

We now analyze two other orthogonal perturbation types  (cid:96)∞-noise and rotation-translations [11]. In
some cases  increasing robustness to (cid:96)∞-noise has been shown to decrease robustness to rotation-
translations [11]. We prove that such a trade-off is inherent for our binary classiﬁcation task.
To reason about rotation-translations  we assume that the features xi form a 2D grid. We also let x0
be distributed as N (y  α−2)  a technicality that does not qualitatively change our prior results. Note
that the distribution of the features x1  . . .   xd is permutation-invariant. Thus  the only power of a
rotation-translation adversary is to “move” feature x0. Without loss of generality  we identify a small
rotation-translation of an input x with a permutation of its features that sends x0 to one of N ﬁxed
positions (e.g.  with translations of ±3px as in [11]  x0 can be moved to N = 49 different positions).
A model can be robust to these permutations by ignoring the N positions that feature x0 can be moved
to  and focusing on the remaining permutation-invariant features. Yet  this model is vulnerable to
(cid:96)∞-noise  as it ignores x0. In turn  a model that relies on feature x0 can be robust to (cid:96)∞-perturbations 
but is vulnerable to a spatial perturbation that “hides” x0 among other features. Formally  we show:
Theorem 2. Let f be a classiﬁer for D (with x0 ∼ N (y  α−2)). Let S∞ be the set of (cid:96)∞-bounded
perturbations with  = 2η  and SRT be the set of perturbations for an RT adversary with budget N.
Then  Ravg
The proof  given in Appendix G  is non-trivial and yields an asymptotic lower-bound on Ravg
can also provide tight numerical estimates for concrete parameter settings (see Appendix G.1).

adv(f ; S∞  SRT) ≥ 1/2 − O(1/

adv. We

N ) .

√

4

2.5 Afﬁne Combinations of Perturbations
We deﬁned Rmax
adv as the error rate against an adversary that may choose a different perturbation type
for each input. If a model were robust to this adversary  what can we say about the robustness to
a more creative adversary that combines different perturbation types? To answer this question  we
introduce a new adversary that mixes different attacks by linearly interpolating between perturbations.
For a perturbation set S and β ∈ [0  1]  we denote β · S the set of perturbations scaled down by β.
For an (cid:96)p-ball with radius   this is the ball with radius β · . For rotation-translations  the attack
budget N is scaled to β · N. For two sets S1  S2  we deﬁne Safﬁne(S1  S2) as the set of perturbations
that compound a perturbation r1 ∈ β · S1 with a perturbation r2 ∈ (1 − β) · S2  for any β ∈ [0  1].
Consider one adversary that chooses  for each input  (cid:96)p or (cid:96)q-noise from balls Sp and Sq  for p  q > 0.
The afﬁne adversary picks perturbations from the set Safﬁne deﬁned as above. We show:
Claim 3. For a linear classiﬁer f (x) = sign(wT x + b)  we have Rmax
adv (f ; Sp  Sq) = Radv(f ; Safﬁne).
Thus  for linear classiﬁers  robustness to a union of (cid:96)p-perturbations implies robustness to afﬁne
adversaries (this holds for any distribution). The proof  in Appendix H extends to models that are
locally linear within balls Sp and Sq around the data points. For the distribution D of Section 2.2  we
can further show that there are settings (distinct from the one in Theorem 1) where: (1) robustness
against a union of (cid:96)∞ and (cid:96)1-perturbations is possible; (2) this requires the model to be non-linear;
(3) yet  robustness to afﬁne adversaries is impossible (see Appendix I for details). Our experiments
in Section 4 show that neural networks trained on CIFAR10 have a behavior that is consistent with
locally-linear models  in that they are as robust to afﬁne adversaries as against a union of (cid:96)p-attacks.
In contrast  compounding (cid:96)∞ and spatial perturbations yields a stronger attack  even for linear models:
Theorem 4. Let f (x) = sign(wT x + b) be a linear classiﬁer for D (with x0 ∼ N (y  α−2)). Let
S∞ be some (cid:96)∞-ball and SRT be rotation-translations with budget N > 2. Deﬁne Safﬁne as above.
Assume w0 > wi > 0 ∀i ∈ [1  d]. Then Radv(f ; Safﬁne) > Rmax
This result (the proof is in Appendix J) draws a distinction between the strength of afﬁne combinations
of (cid:96)p-noise  and combinations of (cid:96)∞ and spatial perturbations. It also shows that robustness to a
union of perturbations can be insufﬁcient against a more creative afﬁne adversary. These results are
consistent with behavior we observe in models trained on real data (see Section 4).

adv (f ; S∞  SRT).

3 New Attacks and Adversarial Training Schemes

We complement our theoretical results with empirical evaluations of the robustness trade-off on
MNIST and CIFAR10. To this end  we ﬁrst introduce new adversarial training schemes tailored to
the multi-perturbation risks deﬁned in Equation (1)  as well as a novel attack for the (cid:96)1-norm.

Multi-perturbation adversarial training. Let

m(cid:88)

i=1

ˆRadv(f ; S) =

L(f (x(i) + r)  y(i))  

max
r∈S

bet the empirical adversarial risk  where L is the training loss and D is the training set. For a
single perturbation type  ˆRadv can be minimized with adversarial training [25]: the maximal loss is
approximated by an attack procedure A(x)  such that maxr∈S L(f (x + r)  y) ≈ L(f (A(x))  y).
For i ∈ [1  d]  let Ai be an attack for the perturbation set Si. The two multi-attack robustness metrics
introduced in Equation (1) immediately yield the following natural adversarial training strategies:
1. “Max” strategy: For each input x  we train on the strongest adversarial example from all attacks 

i.e.  the max in ˆRadv is replaced by L(f (Ak∗ (x))  y)  for k∗ = arg maxk L(f (Ak(x))  y).
That is  the max in ˆRadv is replaced by 1

2. “Avg” strategy: This strategy simultaneously trains on adversarial examples from all attacks.

(cid:80)n
i=1 L(f (Ai(x)  y)).

n

The sparse (cid:96)1-descent attack (SLIDE). Adversarial training is contingent on a strong and efﬁcient
attack. Training on weak attacks gives no robustness [40]  while strong optimization attacks (e.g.  [6 

5

Input: Input x ∈ [0  1]d  steps k  step-size γ  percentile q  (cid:96)1-bound 
Output: ˆx = x + r s.t. (cid:107)r(cid:107)1 ≤ 
r ← 0d
for 1 ≤ i ≤ k do

g ← ∇rL(θ  x + r  y)
ei = sign(gi) if |gi| ≥ Pq(|g|)  else 0
r ← r + γ · e/(cid:107)e(cid:107)1
r ← ΠS

(r)

1

end
Algorithm 1: The Sparse (cid:96)1 Descent Attack (SLIDE). Pq(|g|) denotes the qth percentile of |g| and
ΠS

1 is the projection onto the (cid:96)1-ball (see [10]).

8]) are prohibitively expensive. Projected Gradient Descent (PGD) [22  25] is a popular choice of
attack that is both efﬁcient and produces strong perturbations. To complement our formal results 
we want to train models on (cid:96)1-perturbations. Yet  we show that the (cid:96)1-version of PGD is highly
inefﬁcient  and propose a better approach suitable for adversarial training.
PGD is a steepest descent algorithm [24]. In each iteration  the perturbation is updated in the steepest
descent direction arg max(cid:107)v(cid:107)≤1 vT g  where g is the gradient of the loss. For the (cid:96)∞-norm  the
steepest descent direction is sign(g) [15]  and for (cid:96)2  it is g/(cid:107)g(cid:107)2. For the (cid:96)1-norm  the steepest
descent direction is the unit vector e with ei∗ = sign(gi∗ )  for i∗ = arg maxi |gi|.
This yields an inefﬁcient attack  as each iteration updates a single index of the perturbation r. We
thus design a new attack with ﬁner control over the sparsity of an update step. For q ∈ [0  1]  let
Pq(|g|) be the qth percentile of |g|. We set ei = sign(gi) if |gi| ≥ Pq(|g|) and 0 otherwise  and
normalize e to unit (cid:96)1-norm. For q (cid:29) 1/d  we thus update many indices of r at once. We introduce
another optimization to handle clipping  by ignoring gradient components where the update step
cannot make progress (i.e.  where xi + ri ∈ {0  1} and gi points outside the domain). To project
r onto an (cid:96)1-ball  we use an algorithm of Duchi et al. [10]. Algorithm 1 describes our attack. It
outperforms the steepest descent attack as well as a recently proposed Frank-Wolfe algorithm for
(cid:96)1-attacks [20] (see Appendix B). Our attack is competitive with the more expensive EAD attack [8]
(see Appendix C).

4 Experiments

We use our new adversarial training schemes to measure the robustness trade-off on MNIST and
CIFAR10.1 MNIST is an interesting case-study as distinct models achieve strong robustness to
different (cid:96)p and spatial attacks[31  11]. Despite the dataset’s simplicity  we show that no single
model achieves strong (cid:96)∞  (cid:96)1 and (cid:96)2 robustness  and that new techniques are required to close this
gap. The code used for all of our experiments can be found here: https://github.com/ftramer/
MultiRobustness

255 ) and (cid:96)1( = 2000

Training and evaluation setup. We ﬁrst use adversarial training to train models on a single
perturbation type. For MNIST  we use (cid:96)1( = 10)  (cid:96)2( = 2) and (cid:96)∞( = 0.3). For CIFAR10 we use
255 ). We also train on rotation-translation attacks with ±3px translations
(cid:96)∞( = 4
and ±30◦ rotations as in [11]. We denote these models Adv1  Adv2  Adv∞  and AdvRT. We then use
the “max” and “avg” strategies from Section 3 to train models Advmax and Advavg against multiple
perturbations. We train once on all (cid:96)p-perturbations  and once on both (cid:96)∞ and RT perturbations.
We use the same CNN (for MNIST) and wide ResNet model (for CIFAR10) as Madry et al. [25].
Appendix A has more details on the training setup  and attack and training hyper-parameters.
We evaluate robustness of all models using multiple attacks: (1) we use gradient-based attacks
for all (cid:96)p-norms  i.e.  PGD [25] and our SLIDE attack with 100 steps and 40 restarts (20 restarts
on CIFAR10)  as well as Carlini and Wagner’s (cid:96)2-attack [6] (C&W)  and an (cid:96)1-variant—EAD [8];
1Kang et al. [20] recently studied the transfer between (cid:96)∞  (cid:96)1 and (cid:96)2-attacks for adversarially trained models
on ImageNet. They show that models trained on one type of perturbation are not robust to others  but they do not
attempt to train models against multiple attacks simultaneously.

6

Table 1: Evaluation of MNIST models trained on (cid:96)∞  (cid:96)1 and (cid:96)2 attacks (left) or (cid:96)∞ and rotation-
translation (RT) attacks (right). Models Adv∞  Adv1  Adv2 and AdvRT are trained on a single
attack  while Advavg and Advmax are trained on multiple attacks using the “avg” and “max” strategies.
The columns show a model’s accuracy on individual perturbation types  on the union of them
(1 − Rmax
adv). The best results are in bold (at 95%
conﬁdence). Results in red indicate gradient-masking  see Appendix C for a breakdown of all attacks.
Model Acc.
(cid:96)2
99.4
Nat
8.5
Adv∞ 99.1 91.1 12.1 11.3
0.0 78.5 50.6
Adv1
0.4 68.0 71.8
Adv2
Advavg 97.3 76.7 53.9 58.3
Advmax 97.2 71.7 62.6 56.0

adv 1 − Ravg
(cid:96)∞ RT 1 − Rmax
Model Acc.
adv
99.4
0.0
0.0
0.0
Nat
0.0
Adv∞ 99.1 91.4
0.2
0.2
45.8
AdvRT 99.3
0.0 94.6
47.3
0.0
87.3
82.9
Advavg 99.2 88.2 86.4
83.8
87.6
Advmax 98.9 89.6 85.6

1 − Rmax
adv 1 − Ravg
adv
7.0
0.0
6.8
38.2
43.0
0.0
46.7
0.4
63.0
49.9
52.4
63.4

adv )  and the average accuracy across them (1 − Ravg

(cid:96)∞ (cid:96)1
0.0 12.4

98.9
98.5

(2) to detect gradient-masking  we use decision-based attacks: the Boundary Attack [3] for (cid:96)2  the
Pointwise Attack [31] for (cid:96)1  and the Boundary Attack++ [7] for (cid:96)∞; (3) for spatial attacks  we use
the optimal attack of [11] that enumerates all small rotations and translations. For unbounded attacks
(C&W  EAD and decision-based attacks)  we discard perturbations outside the (cid:96)p-ball.
For each model  we report accuracy on 1000 test points for: (1) individual perturbation types; (2) the
union of these types  i.e.  1−Rmax
adv ; and (3) the average of all perturbation types  1−Ravg
adv. We brieﬂy
(cid:80)n
discuss the optimal error that can be achieved if there is no robustness trade-off. For perturbation sets
S1  . . . Sn  let R1  . . .  Rn be the optimal risks achieved by distinct models. Then  a single model can
at best achieve risk Ri for each Si  i.e.  OPT(Ravg
i=1 Ri. If the errors are fully correlated 
so that a maximal number of inputs admit no attack  we have OPT(Rmax
adv ) = max{R1  . . .  Rn}.
Our experiments show that these optimal error rates are not achieved.

adv) = 1
n

Results on MNIST. Results are in Table 1. The left table is for the union of (cid:96)p-attacks  and the
right table is for the union of (cid:96)∞ and RT attacks. In both cases  the multi-perturbation training
strategies “succeed”  in that models Advavg and Advmax achieve higher multi-perturbation accuracy
than any of the models trained against a single perturbation type.
The results for (cid:96)∞ and RT attacks are promising  although the best model Advmax only achieves 1 −
Rmax
adv = 83.8% and 1 − Ravg
adv ) =
min{91.4%  94.6%} = 91.4% and 1 − OPT(Ravg
adv) = (91.4% + 94.6%)/2 = 93%. Thus  these
models do exhibit some form of the robustness trade-off analyzed in Section 2.
The (cid:96)p results are surprisingly mediocre and re-raise questions about whether MNIST can be consid-
ered “solved” from a robustness perspective. Indeed  while training separate models to resist (cid:96)1  (cid:96)2
or (cid:96)∞ attacks works well  resisting all attacks simultaneously fails. This agrees with the results of
Schott et al. [31]  whose models achieve either high (cid:96)∞ or (cid:96)2 robustness  but not both simultaneously.
We show that in our case  this lack of robustness is partly due to gradient masking.

adv = 87.6%  which is far less than the optimal values  1 − OPT(Rmax

First-order adversarial training and gradient masking on MNIST. The model Adv∞ is not
robust to (cid:96)1 and (cid:96)2-attacks. This is unsurprising as the model was only trained on (cid:96)∞-attacks. Yet 
comparing the model’s accuracy against multiple types of (cid:96)1 and (cid:96)2 attacks (see Appendix C) reveals
a more curious phenomenon: Adv∞ has high accuracy against ﬁrst-order (cid:96)1 and (cid:96)2-attacks such as
PGD  but is broken by decision-free attacks. This is an indication of gradient-masking [27  40  1].
This issue had been observed before [31  23]  but an explanation remained illusive  especially since
(cid:96)∞-PGD does not appear to suffer from gradient masking (see [25]). We explain this phenomenon by
inspecting the learned features of model Adv∞  as in [25]. We ﬁnd that the model’s ﬁrst layer learns
threshold ﬁlters z = ReLU(α · (x − )) for α > 0. As most pixels in MNIST are zero  most of the
zi cannot be activated by an -bounded (cid:96)∞-attack. The (cid:96)∞-PGD thus optimizes a smooth (albeit ﬂat)
loss function. In contrast  (cid:96)1- and (cid:96)2-attacks can move a pixel xi = 0 to ˆxi >  thus activating zi  but
have no gradients to rely on (i.e  dzi/dxi = 0 for any xi ≤ ). Figure 3 in Appendix D shows that
the model’s loss resembles a step-function  for which ﬁrst-order attacks such as PGD are inadequate.
Note that training against ﬁrst-order (cid:96)1 or (cid:96)2-attacks directly (i.e.  models Adv1 and Adv2 in Table 1) 
seems to yield genuine robustness to these perturbations. This is surprising in that  because of gradient

7

Table 2: Evaluation of CIFAR10 models trained against (cid:96)∞ and (cid:96)1 attacks (left) or (cid:96)∞ and
rotation-translation (RT) attacks (right). Models Adv∞  Adv1 and AdvRT are trained against a
single attack  while Advavg and Advmax are trained against two attacks using the “avg” and “max”
strategies. The columns show a model’s accuracy on individual perturbation types  on the union of
them (1 − Rmax
adv )  and the average accuracy across them (1 − Ravg
adv). The best results are in bold (at
95% conﬁdence). A breakdown of all (cid:96)1 attacks is in Appendix C.
adv 1 − Ravg
(cid:96)∞ RT 1 − Rmax
Model Acc.
adv
95.7
5.9
3.0
0.0
Nat
0.0
Adv∞ 92.0 71.0
8.9
8.7
40.0
94.9
0.0 82.5
41.3
0.0
AdvRT
73.0
65.2
Advavg
93.6 67.8 78.2
Advmax 93.1 69.6 75.2
65.7
72.4

Model Acc.
(cid:96)∞ (cid:96)1
95.7
Nat
0.0
0.0
Adv∞ 92.0 71.0 16.4
90.8 53.4 66.2
Adv1
Advavg
91.1 64.1 60.8
Advmax 91.2 65.7 62.5

1 − Rmax
adv 1 − Ravg
adv
0.0
0.0
16.4
44.9
60.0
53.1
62.5
59.4
61.1
64.1

Table 3: Evaluation of afﬁne attacks. For models trained with the “max” strategy  we evaluate
against attacks from a union SU of perturbation sets  and against an afﬁne adversary that interpolates
between perturbations. Examples of afﬁne attacks are in Figure 4.

Attacks
Dataset
MNIST
(cid:96)∞ & RT
CIFAR10 (cid:96)∞ & RT
CIFAR10 (cid:96)∞ & (cid:96)1

acc. on SU acc. on Safﬁne
62.6
56.0
58.0

83.8
65.7
61.1

masking  model Adv∞ actually achieves lower training loss against ﬁrst-order (cid:96)1 and (cid:96)2-attacks than
models Adv1 and Adv2. That is  Adv1 and Adv2 converged to sub-optimal local minima of their
respective training objectives  yet these minima generalize much better to stronger attacks.
The models Advavg and Advmax that are trained against (cid:96)∞  (cid:96)1 and (cid:96)2-attacks also learn to use
thresholding to resist (cid:96)∞-attacks while spuriously masking gradient for (cid:96)1 and (cid:96)2-attacks. This is
evidence that  unlike previously thought [41]  training against a strong ﬁrst-order attack (such as PGD)
can cause the model to minimize its training loss via gradient masking. To circumvent this issue 
alternatives to ﬁrst-order adversarial training seem necessary. Potential (costly) approaches include
training on gradient-free attacks  or extending certiﬁed defenses [28  42] to multiple perturbations.
Certiﬁed defenses provide provable bounds that are much weaker than the robustness attained by
adversarial training  and certifying multiple perturbation types is likely to exacerbate this gap.

Results on CIFAR10. The left table in Table 2 considers the union of (cid:96)∞ and (cid:96)1 perturbations 
while the right table considers the union of (cid:96)∞ and RT perturbations. As on MNIST  the models
Advavg and Advmax achieve better multi-perturbation robustness than any of the models trained on a
single perturbation  but fail to match the optimal error rates we could hope for. For (cid:96)1 and (cid:96)∞-attacks 
we achieve 1 − Rmax
adv = 64.1%  again signiﬁcantly below the optimal values 
1 − OPT(Rmax
adv) = (71.0% + 66.2%)/2 =
68.6%. The results for (cid:96)∞ and RT attacks are qualitatively and quantitatively similar. 2
Interestingly  models Advavg and Advmax achieve 100% training accuracy. Thus  multi-perturbation
robustness increases the adversarial generalization gap [30]. These models might be resorting to
more memorization because they fail to ﬁnd features robust to both attacks.

adv ) = min{71.0%  66.2%} = 66.2% and 1 − OPT(Ravg

adv = 61.1% and 1 − Ravg

Afﬁne Adversaries. Finally  we evaluate the afﬁne attacks introduced in Section 2.5. These attacks
take afﬁne combinations of two perturbation types  and we apply them on the models Advmax (we
omit the (cid:96)p-case on MNIST due to gradient masking). To compound (cid:96)∞ and (cid:96)1-noise  we devise
an attack that updates both perturbations in alternation. To compound (cid:96)∞ and RT attacks  we pick
random rotation-translations (with ±3βpx translations and ±30β◦ rotations)  apply an (cid:96)∞-attack
with budget (1 − β) to each  and retain the worst example.

2An interesting open question is why the model Advavg trained on (cid:96)∞ and RT attacks does not attain optimal
average robustness Ravg
adv. Indeed  on CIFAR10  detecting the RT attack of [11] is easy  due to the black in-painted
pixels in a transformed image. The following “ensemble” model thus achieves optimal Ravg
adv (but not necessarily
optimal Rmax
adv ): on input ˆx  return AdvRT( ˆx) if there are black in-painted pixels  otherwise return Adv∞( ˆx).
The fact that model Advavg did not learn such a function might hint at some limitation of adversarial training.

8

The results in Table 3 match the predictions of our formal analysis: (1) afﬁne combinations of (cid:96)p
perturbations are no stronger than their union. This is expected given Claim 3 and prior observations
that neural networks are close to linear near the data [15  29]; (2) combining of (cid:96)∞ and RT attacks
does yield a stronger attack  as shown in Theorem 4. This demonstrates that robustness to a union of
perturbations can still be insufﬁcient to protect against more complex combinations of perturbations.

5 Discussion and Open Problems

Despite recent success in defending ML models against some perturbation types [25  11  31]  extend-
ing these defenses to multiple perturbations unveils a clear robustness trade-off. This tension may be
rooted in its unconditional occurrence in natural and simple distributions  as we proved in Section 2.
Our new adversarial training strategies fail to achieve competitive robustness to more than one attack
type  but narrow the gap towards multi-perturbation robustness. We note that the optimal risks Rmax
adv
and Ravg
adv that we achieve are very close. Thus  for most data points  the models are either robust to all
perturbation types or none of them. This hints that some points (sometimes referred to as prototypical
examples [4  36]) are inherently easier to classify robustly  regardless of the perturbation type.
We showed that ﬁrst-order adversarial training for multiple (cid:96)p-attacks suffers from gradient masking
on MNIST. Achieving better robustness on this simple dataset is an open problem. Another challenge
is reducing the cost of our adversarial training strategies  which scale linearly in the number of pertur-
bation types. Breaking this linear dependency requires efﬁcient techniques for ﬁnding perturbations
in a union of sets  which might be hard for sets with near-empty intersection (e.g.  (cid:96)∞ and (cid:96)1-balls).
The cost of adversarial training has also be reduced by merging the inner loop of a PGD attack and
gradient updates of the model parameters [34  44]  but it is unclear how to extend this approach to a
union of perturbations (some of which are not optimized using PGD  e.g.  rotation-translations).
Hendrycks and Dietterich [17]  and Geirhos et al. [13] recently measured robustness of classiﬁers
to multiple common (i.e.  non-adversarial) image corruptions (e.g.  random image blurring). In that
setting  they also ﬁnd that different classiﬁers achieve better robustness to some corruptions  and
that no single classiﬁer achieves the highest accuracy under all forms. The interplay between multi-
perturbation robustness in the adversarial and common corruption case is worth further exploration.

References
[1] A. Athalye  N. Carlini  and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing

defenses to adversarial examples. In International Conference on Machine Learning (ICML)  2018.

[2] A. C. Berry. The accuracy of the gaussian approximation to the sum of independent variates. Transactions

of the american mathematical society  49(1):122–136  1941.

[3] W. Brendel  J. Rauber  and M. Bethge. Decision-based adversarial attacks: Reliable attacks against

black-box machine learning models. In International Conference on Learning Representations  2018.

[4] N. Carlini  U. Erlingsson  and N. Papernot. Prototypical examples in deep learning: Metrics  characteristics 

and utility. 2018.

[5] N. Carlini  P. Mishra  T. Vaidya  Y. Zhang  M. Sherr  C. Shields  D. Wagner  and W. Zhou. Hidden voice

commands. In USENIX Security Symposium  pages 513–530  2016.

[6] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on

Security and Privacy  2017.

[7] J. Chen and M. I. Jordan. Boundary attack++: Query-efﬁcient decision-based adversarial attack. arXiv

preprint arXiv:1904.02144  2019.

[8] P.-Y. Chen  Y. Sharma  H. Zhang  J. Yi  and C.-J. Hsieh. Ead: elastic-net attacks to deep neural networks

via adversarial examples. In AAAI Conference on Artiﬁcial Intelligence  2018.

[9] A. Demontis  P. Russu  B. Biggio  G. Fumera  and F. Roli. On security and sparsity of linear classiﬁers
In Joint IAPR International Workshops on Statistical Techniques in Pattern
for adversarial settings.
Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)  pages 322–332. Springer 
2016.

9

[10] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the l1-ball for learning

in high dimensions. In International Conference on Machine Learning (ICML)  2008.

[11] L. Engstrom  B. Tran  D. Tsipras  L. Schmidt  and A. Madry. A rotation and a translation sufﬁce: Fooling

CNNs with simple transformations. arXiv preprint arXiv:1712.02779  2017.

[12] A. Fawzi  H. Fawzi  and O. Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural

Information Processing Systems  pages 1186–1195  2018.

[13] R. Geirhos  P. Rubisch  C. Michaelis  M. Bethge  F. A. Wichmann  and W. Brendel. ImageNet-trained
CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International
Conference on Learning Representations (ICLR)  2019.

[14] J. Gilmer  L. Metz  F. Faghri  S. S. Schoenholz  M. Raghu  M. Wattenberg  and I. Goodfellow. Adversarial

spheres. arXiv preprint arXiv:1801.02774  2018.

[15] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. In Interna-

tional Conference on Learning Representations (ICLR)  2015.

[16] K. Grosse  N. Papernot  P. Manoharan  M. Backes  and P. McDaniel. Adversarial examples for malware

detection. In European Symposium on Research in Computer Security  2017.

[17] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and

perturbations. In International Conference on Learning Representations (ICLR)  2019.

[18] A. Ilyas  S. Santurkar  D. Tsipras  L. Engstrom  B. Tran  and A. Madry. Adversarial examples are not bugs 

they are features. arXiv preprint arXiv:1905.02175  2019.

[19] J. Jo and Y. Bengio. Measuring the tendency of CNNs to learn surface statistical regularities. arXiv preprint

arXiv:1711.11561  2017.

[20] D. Kang  Y. Sun  T. Brown  D. Hendrycks  and J. Steinhardt. Transfer of adversarial robustness between

perturbation types. arXiv preprint arXiv:1905.01034  2019.

[21] M. Khoury and D. Hadﬁeld-Menell. On the geometry of adversarial examples  2019.

[22] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial machine learning at scale.

Conference on Learning Representations (ICLR)  2017.

In International

[23] B. Li  C. Chen  W. Wang  and L. Carin. Second-order adversarial attack and certiﬁable robustness. arXiv

preprint arXiv:1809.03113  2018.

[24] A. Madry and Z. Kolter. Adversarial robustness: Theory and practice. In Tutorial at NeurIPS 2018  2018.

[25] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models resistant to

adversarial attacks. In International Conference on Learning Representations (ICLR)  2018.

[26] S. Mahloujifar  D. I. Diochnos  and M. Mahmoody. The curse of concentration in robust learning: Evasion

and poisoning attacks from concentration of measure. arXiv preprint arXiv:1809.03063  2018.

[27] N. Papernot  P. McDaniel  I. Goodfellow  S. Jha  Z. B. Celik  and A. Swami. Practical black-box attacks

against machine learning. In ASIACCS  pages 506–519. ACM  2017.

[28] A. Raghunathan  J. Steinhardt  and P. Liang. Certiﬁed defenses against adversarial examples. In Interna-

tional Conference on Learning Representations (ICLR)  2018.

[29] M. T. Ribeiro  S. Singh  and C. Guestrin. Why should i trust you?: Explaining the predictions of any

classiﬁer. In KDD. ACM  2016.

[30] L. Schmidt  S. Santurkar  D. Tsipras  K. Talwar  and A. Madry. Adversarially robust generalization requires

more data. In Advances in Neural Information Processing Systems  pages 5019–5031  2018.

[31] L. Schott  J. Rauber  M. Bethge  and W. Brendel. Towards the ﬁrst adversarially robust neural network

model on mnist. In International Conference on Learning Representations (ICLR)  2019.

[32] L. Schott  J. Rauber  M. Bethge  and W. Brendel. Towards the ﬁrst adversarially robust neural network

model on mnist (OpenReview comment on spatial transformations)  2019.

[33] A. Shafahi  W. R. Huang  C. Studer  S. Feizi  and T. Goldstein. Are adversarial examples inevitable? In

International Conference on Learning Representations (ICLR)  2019.

10

[34] A. Shafahi  M. Najibi  A. Ghiasi  Z. Xu  J. Dickerson  C. Studer  L. S. Davis  G. Taylor  and T. Goldstein.

Adversarial training for free! arXiv preprint arXiv:1904.12843  2019.

[35] Y. Sharma and P.-Y. Chen. Attacking the madry defense model with l1-based adversarial examples. arXiv

preprint arXiv:1710.10733  2017.

[36] P. Stock and M. Cisse. Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering
biases. In Proceedings of the European Conference on Computer Vision (ECCV)  pages 498–512  2018.

[37] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus. Intriguing

properties of neural networks. In International Conference on Learning Representations (ICLR)  2014.

[38] F. Tramèr and D. Boneh. Adversarial training and robustness for multiple perturbations.

Information Processing Systems (NeurIPS) 2019  2019. arXiv preprint arXiv:1904.13000.

In Neural

[39] F. Tramèr  P. Dupré  G. Rusak  G. Pellegrino  and D. Boneh. Ad-versarial: Perceptual ad-blocking meets

adversarial machine learning. arXiv preprint arXiv:1811:03194  Nov 2018.

[40] F. Tramèr  A. Kurakin  N. Papernot  I. Goodfellow  D. Boneh  and P. McDaniel. Ensemble adversarial
training: Attacks and defenses. In International Conference on Learning Representations (ICLR)  2018.

[41] D. Tsipras  S. Santurkar  L. Engstrom  A. Turner  and A. Madry. Robustness may be at odds with accuracy.

In International Conference on Learning Representations (ICLR)  2019.

[42] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial

polytope. In International Conference on Machine Learning  pages 5283–5292  2018.

[43] H. Xu  C. Caramanis  and S. Mannor. Robustness and regularization of support vector machines. Journal

of Machine Learning Research  10(Jul):1485–1510  2009.

[44] D. Zhang  T. Zhang  Y. Lu  Z. Zhu  and B. Dong. You only propagate once: Painless adversarial training

using maximal principle. arXiv preprint arXiv:1905.00877  2019.

11

,Florian Tramer
Dan Boneh