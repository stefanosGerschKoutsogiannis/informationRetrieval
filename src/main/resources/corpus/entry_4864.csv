2017,Scalable Generalized Linear Bandits: Online Computation and Hashing,Generalized Linear Bandits (GLBs)  a natural extension of the stochastic linear bandits  has been popular and successful in recent years.  However  existing GLBs scale poorly with the number of rounds and the number of arms  limiting their utility in practice.  This paper proposes new  scalable solutions to the GLB problem in two respects.  First  unlike existing GLBs  whose per-time-step space and time complexity grow at least linearly with time $t$  we propose a new algorithm that performs online computations to enjoy a constant space and time complexity.  At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes \emph{any} online learning algorithm and turns it into a GLB algorithm.  As a special case  we apply GLOC to the online Newton step algorithm  which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work.  Second  for the case where the number $N$ of arms is very large  we propose new algorithms in which each next arm is selected via an inner product search.  Such methods can be implemented via hashing algorithms (i.e.  ``hash-amenable'') and result in a time complexity sublinear in $N$.  While a Thompson sampling extension of GLOC is hash-amenable  its regret bound for $d$-dimensional arm sets scales with $d^{3/2}$  whereas GLOC's regret bound scales with $d$.  Towards closing this gap  we propose a new hash-amenable algorithm whose regret bound scales with $d^{5/4}$.  Finally  we propose a fast approximate hash-key computation (inner product) with a better accuracy than the state-of-the-art  which can be of independent interest.  We conclude the paper with preliminary experimental results confirming the merits of our methods.,Scalable Generalized Linear Bandits:

Online Computation and Hashing

Kwang-Sung Jun

UW-Madison

kjun@discovery.wisc.edu

Aniruddha Bhargava

UW-Madison

aniruddha@wisc.edu

Robert Nowak
UW-Madison

rdnowak@wisc.edu

Rebecca Willett
UW-Madison

willett@discovery.wisc.edu

Abstract

Generalized Linear Bandits (GLBs)  a natural extension of the stochastic linear
bandits  has been popular and successful in recent years. However  existing GLBs
scale poorly with the number of rounds and the number of arms  limiting their
utility in practice. This paper proposes new  scalable solutions to the GLB problem
in two respects. First  unlike existing GLBs  whose per-time-step space and time
complexity grow at least linearly with time t  we propose a new algorithm that
performs online computations to enjoy a constant space and time complexity. At
its heart is a novel Generalized Linear extension of the Online-to-conﬁdence-set
Conversion (GLOC method) that takes any online learning algorithm and turns it
into a GLB algorithm. As a special case  we apply GLOC to the online Newton
step algorithm  which results in a low-regret GLB algorithm with much lower
time and memory complexity than prior work. Second  for the case where the
number N of arms is very large  we propose new algorithms in which each next
arm is selected via an inner product search. Such methods can be implemented
via hashing algorithms (i.e.  “hash-amenable”) and result in a time complexity
sublinear in N. While a Thompson sampling extension of GLOC is hash-amenable 
its regret bound for d-dimensional arm sets scales with d3/2  whereas GLOC’s
regret bound scales with d. Towards closing this gap  we propose a new hash-
amenable algorithm whose regret bound scales with d5/4. Finally  we propose a
fast approximate hash-key computation (inner product) with a better accuracy than
the state-of-the-art  which can be of independent interest. We conclude the paper
with preliminary experimental results conﬁrming the merits of our methods.

Introduction

1
This paper considers the problem of making generalized linear bandits (GLBs) scalable. In the
stochastic GLB problem  a learner makes successive decisions to maximize her cumulative rewards.
Speciﬁcally  at time t the learner observes a set of arms Xt ⊆ Rd. The learner then chooses an arm
xt ∈ Xt and receives a stochastic reward yt that is a noisy function of xt: yt = µ(x(cid:62)
) + ηt  where
t θ
∗ ∈ Rd is unknown  µ:R→R is a known nonlinear mapping  and ηt ∈ R is some zero-mean noise.
θ
This reward structure encompasses generalized linear models [29]; e.g.  Bernoulli  Poisson  etc.
The key aspect of the bandit problem is that the learner does not know how much reward she would
∗ is thus biased by the history of the
have received  had she chosen another arm. The estimation on θ
selected arms  and one needs to mix in exploratory arm selections to avoid ruling out the optimal
arm. This is well-known as the exploration-exploitation dilemma. The performance of a learner is
evaluated by its regret that measures how much cumulative reward she would have gained additionally
if she had known the true θ
A linear case of the problem above (µ(z) = z) is called the (stochastic) linear bandit problem. Since
the ﬁrst formulation of the linear bandits [7]  there has been a ﬂurry of studies on the problem [11 

∗. We provide backgrounds and formal deﬁnitions in Section 2.

∗

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

34  1  9  5]. In an effort to generalize the restrictive linear rewards  Filippi et al. [15] propose the
GLB problem and provide a low-regret algorithm  whose Thompson sampling version appears later
in Abeille & Lazaric [3]. Li et al. [27] evaluates GLBs via extensive experiments where GLBs exhibit
lower regrets than linear bandits for 0/1 rewards. Li et al. [28] achieves a smaller regret bound when
the arm set Xt is ﬁnite  though with an impractical algorithm.
However  we claim that all existing GLB algorithms [15  28] suffer from two scalability issues that
limit their practical use: (i) under a large time horizon and (ii) under a large number N of arms.
First  existing GLBs require storing all the arms and rewards appeared so far  {(xs  ys)}t
s=1  so
the space complexity grows linearly with t. Furthermore  they have to solve a batch optimization
problem for the maximum likelihood estimation (MLE) at each time step t whose per-time-step time
complexity grows at least linearly with t. While Zhang et al. [41] provide a solution whose space
and time complexity do not grow over time  they consider a speciﬁc 0/1 reward with the logistic link
function  and a generic solution for GLBs is not provided.
Second  existing GLBs have linear time complexities in N. This is impractical when N is very large 
which is not uncommon in applications of GLBs such as online advertisements  recommendation
systems  and interactive retrieval of images or documents [26  27  40  21  25] where arms are items in
a very large database. Furthermore  the interactive nature of these systems requires prompt responses
as users do not want to wait. This implies that the typical linear time in N is not tenable. Towards a
sublinear time in N  locality sensitive hashings [18] or its extensions [35  36  30] are good candidates
as they have been successful in fast similarity search and other machine learning problems like active
learning [22]  where the search time scales with N ρ for some ρ < 1 (ρ is usually optimized and
often ranges from 0.4 to 0.8 depending on the target search accuracy). Leveraging hashing in GLBs 
however  relies critically on the objective function used for arm selections. The function must take a
form that is readily optimized using existing hashing algorithms.1 For example  algorithms whose
objective function (a function of each arm x ∈ Xt) can be written as a distance or inner product
between x and a query q are hash-amenable as there exist hashing methods for such functions.
To be scalable to a large time horizon  we propose a new algorithmic framework called Generalized
Linear Online-to-conﬁdence-set Conversion (GLOC) that takes in an online learning (OL) algorithm
with a low ‘OL’ regret bound and turns it into a GLB algorithm with a low ‘GLB’ regret bound. The
key tool is a novel generalization of the online-to-conﬁdence-set conversion technique used in [2]
∗  which is then
(also similar to [14  10  16  41]). This allows us to construct a conﬁdence set for θ
used to choose an arm xt according to the well-known optimism in the face of uncertainty principle.
By relying on an online learner  GLOC inherently performs online computations and is thus free from
the scalability issues in large time steps. While any online learner equipped with a low OL regret
bound can be used  we choose the online Newton step (ONS) algorithm and prove a tight OL regret
bound  which results in a practical GLB algorithm with almost the same regret bound as existing
inefﬁcient GLB algorithms. We present our proposed algorithms and their regret bounds in Section 3.
For large number N of arms  our proposed algorithm
GLOC is not hash-amenable  to our knowledge  due
to its nonlinear criterion for arm selection. As the ﬁrst
attempt  we derive a Thompson sampling [5  3] exten-
sion of GLOC (GLOC-TS)  which is hash-amenable
due to its linear criterion. However  its regret bound
scales with d3/2 for d-dimensional arm sets  which
is far from d of GLOC. Towards closing this gap  we
propose a new algorithm Quadratic GLOC (QGLOC)
with a regret bound that scales with d5/4. We summarize the comparison of our proposed GLB
algorithms in Table 1. In Section 4  we present GLOC-TS  QGLOC  and their regret bound.
Note that  while hashing achieves a time complexity sublinear in N  there is a nontrivial overhead
of computing the projections to determine the hash keys. As an extra contribution  we reduce this
overhead by proposing a new sampling-based approximate inner product method. Our proposed
sampling method has smaller variance than the state-of-the-art sampling method proposed by [22  24]
when the vectors are normally distributed  which ﬁts our setting where projection vectors are indeed
normally distributed. Moreover  our method results in thinner tails in the distribution of estimation

Table 1: Comparison of GLBs algorithms for
d-dimensional arm sets T is the time horizon.
QGLOC achieves the smallest regret among
hash-amenable algorithms.

√
Regret
√
˜O(d
T )
√
˜O(d3/2
T )
˜O(d5/4
T )

Hash-amenable

Algorithm

GLOC

GLOC-TS
QGLOC





1 Without this designation  no currently known bandit algorithm achieves a sublinear time complexity in N.

2

error than the existing method  which implies a better concentration. We elaborate more on reducing
the computational complexity of QOFUL in Section 5.

2 Preliminaries
We review relevant backgrounds here. A refers to a GLB algorithm  and B refers to an online
learning algorithm. Let Bd(S) be the d-dimensional Euclidean ball of radius S  which overloads
the notation B. Let A·i be the i-th column vector of a matrix A. Deﬁne ||x||A :=
x(cid:62)Ax and
vec(A) := [A·1; A·2;··· ; A·d] ∈ Rd2 Given a function f : R → R  we denote by f(cid:48) and f(cid:48)(cid:48) its
ﬁrst and second derivative  respectively. We deﬁne [N ] := {1  2  . . .   N}.
Generalized Linear Model (GLM) Consider modeling the reward y as one-dimensional exponen-
tial family such as Bernoulli or Poisson. When the feature vector x is believed to correlate with
y  one popular modeling assumption is the generalized linear model (GLM) that turns the natural
parameter of an exponential family model into x(cid:62)θ

∗ where θ

√

(cid:18) yz − m(z)

(cid:19)

∗ is a parameter [29]:
+ h(y  τ )

 

P(y | z = x(cid:62)θ

∗

∗

g(τ )

) = exp

)))  where y(cid:48) = 2y − 1.

(1)
where τ ∈ R+ is a known scale parameter and m  g  and h are normalizers. It is known that
m(cid:48)(z) = E[y | z] =: µ(z) and m(cid:48)(cid:48)(z) = Var(y | z). We call µ(z) the inverse link function.
Throughout  we assume that the exponential family being used in a GLM has a minimal representation 
which ensures that m(z) is strictly convex [38  Prop. 3.1]. Then  the negative log likelihood (NLL)
(cid:96)(z  y) := −yz + m(z) of a GLM is strictly convex. We refer to such GLMs as the canonical GLM.
In the case of Bernoulli rewards y ∈ {0  1}  m(z) = log(1 + exp(z))  µ(z) = (1 + exp(−z))−1 
and the NLL can be written as the logistic loss: log(1 + exp(−y(cid:48)(x(cid:62)
t θ
Generalized Linear Bandits (GLB) Recall that xt is the arm chosen at time t by an algorithm.
We assume that the arm set Xt can be of an inﬁnite cardinality  although we focus on ﬁnite arm sets in
hashing part of the paper (Section 4). One can write down the reward model (1) in a different form:
(2)
) + ηt 
where ηt is conditionally R-sub-Gaussian given xt and {(xs  ηs)}t−1
s=1. For example  Bernoulli
) and −µ(x(cid:62)
reward model has ηt as 1 − µ(x(cid:62)
∗
) otherwise. Assume that
t θ
t θ
∗||2 ≤ S  where S is known. One can show that the sub-Gaussian scale R is determined by µ:
||θ
L  where L is the Lipschitz constant of µ. Throughout  we assume
R = supz∈(−S S)
that each arm has (cid:96)2-norm at most 1: ||x||2 ≤ 1 ∀x ∈ Xt ∀t. Let xt ∗ := maxx∈Xt x(cid:62)θ
∗. The
performance of a GLB algorithm A is analyzed by the expected cumulative regret (or simply regret):
t makes the dependence on A explicit.
RegretA
We remark that our results in this paper hold true for a strictly larger family of distributions than the
canonical GLM  which we call the non-canonical GLM and explain below. The condition is that the
reward model follows (2) where the R is now independent from µ that satisﬁes the following:
Assumption 1. µ is L-Lipschitz on [−S  S] and continuously differentiable on (−S  S). Furthermore 
inf z∈(−S S) µ(cid:48)(z) = κ for some ﬁnite κ > 0 (thus µ is strictly increasing).
Deﬁne µ(cid:48)(z) at ±S as their limits. Under Assumption 1  m is deﬁned to be an integral of µ. Then 
one can show that m is κ-strongly convex on B1(S). An example of the non-canonical GLM is the
probit model for 0/1 reward where µ is the Gaussian CDF  which is popular and competitive to the
Bernoulli GLM as evaluated by Li et al. [27]. Note that canonical GLMs satisfy Assumption 1.

yt = µ(x(cid:62)
t θ
) w.p. µ(x(cid:62)
t θ

(cid:112)µ(cid:48)(z) ≤ √

T :=(cid:80)T

t=1 µ(x(cid:62)
t ∗θ

)  where xA

) − µ((xA

t )(cid:62)θ

∗

∗

∗

∗

∗

3 Generalized Linear Bandits with Online Computation
We describe and analyze a new GLB algorithm called Generalized Linear Online-to-conﬁdence-set
Conversion (GLOC) that performs online computations  unlike existing GLB algorithms.
GLOC employs the optimism in the face of uncertainty principle  which dates back to [7]. That is  we
∗ with high probability
maintain a conﬁdence set Ct (deﬁned below) that traps the true parameter θ
(w.h.p.) and choose the arm with the largest feasible reward given Ct−1 as a constraint:

(3)
The main difference between GLOC and existing GLBs is in the computation of the Ct’s. Prior
methods involve “batch" computations that involve all past observations  and so scale poorly with

x∈Xt θ∈Ct−1

max

(xt  ˜θt) := arg

(cid:104)x  θ(cid:105)

3

t θt and zt := [z1;··· ; zt]. Let(cid:98)θt := V

t. In contrast  GLOC takes in an online learner B  and uses B as a co-routine instead of relying on
a batch procedure to construct a conﬁdence set. Speciﬁcally  at each time t GLOC feeds the loss
t θ  yt) into the learner B which then outputs its parameter prediction θt. Let
function (cid:96)t(θ) := (cid:96)(x(cid:62)
Xt ∈ Rt×d be the design matrix consisting of x1  . . .   xt. Deﬁne Vt := λI + X(cid:62)
t Xt  where λ
−1
is the ridge parameter. Let zt := x(cid:62)
t X(cid:62)
t zt be the ridge
regression estimator taking zt as responses. Theorem 1 below is the key result for constructing our
conﬁdence set Ct  which is a function of the parameter predictions {θs}t
s=1 and the online (OL)
regret bound Bt of the learner B. All the proofs are in the supplementary material (SM).
Theorem 1. (Generalized Linear Online-to-Conﬁdence-Set Conversion) Suppose we feed loss func-
(cid:80)t
tions {(cid:96)s(θ)}t
s=1 into online learner B. Let θs be the parameter predicted at time step s by B.
Assume that B has an OL regret bound Bt: ∀θ ∈ Bd(S) ∀t ≥ 1 
(cid:113)
s=1 (cid:96)s(θs) − (cid:96)s(θ) ≤ Bt .
(4)
≤ α(Bt) + λS2 −(cid:16)||zt||2
2 −(cid:98)θ
κ4δ2 ). Then  with probability (w.p.) at least 1− δ 
1 + 2
(5)
Ct := {θ ∈ Rd : ||θ −(cid:98)θt||2

Note that the center of the ellipsoid is the ridge regression estimator on the predicted natural
parameters zs = x(cid:62)
s θs rather than the rewards. Theorem 1 motivates the following conﬁdence set:
(6)
∗ for all t ≥ 1  w.p. at least 1 − δ. See Algorithm 1 for pseudocode. One way to solve
which traps θ
the optimization problem (3) is to deﬁne the function θ(x) := maxθ∈Ct−1 x(cid:62)θ  and then use the
Lagrangian method to write:

∗ −(cid:98)θt||2

Let α(Bt) := 1 + 4

∀t ≥ 1 ||θ

κ Bt + 8R2

(cid:62)
t X(cid:62)
t zt

κ Bt + 4R4

κ2 log( 2

≤ βt}

=: βt .

(cid:17)

Vt

Vt

δ

xGLOC
t

:= arg max
x∈Xt

.

−1
t−1

(7)

We prove the regret bound of GLOC in the following theorem.
Theorem 2. Let {βt} be a nondecreasing sequence such that βt ≥ βt. Then  w.p. at least 1 − δ 

x(cid:62)(cid:98)θt−1 +(cid:112)βt−1||x||V
(cid:19)
(cid:18)

(cid:113)

Algorithm 1 GLOC
1: Input: R > 0  δ ∈ (0  1)  S > 0  λ > 0  κ > 0 
an online learner B with known regret bounds
{Bt}t≥1.

RegretGLOC

T

L

√

βT dT log T

Compute xt by solving (3).
Pull xt and then observe yt.
Receive θt from B.
Feed into B the loss (cid:96)t(θ) = (cid:96)(x(cid:62)
Update Vt = Vt−1 + xtx(cid:62)

= O
Although any low-regret online learner can be
combined with GLOC  one would like to ensure
that βT is O(polylog(T )) in which case the total
regret can be bounded by ˜O(
T ). This means
that we must use online learners whose OL regret
grows logarithmically in T such as [20  31]. In
this work  we consider the online Newton step
(ONS) algorithm [20].
Online Newton Step (ONS) for Generalized
Linear Models Note that ONS requires the loss
functions to be α-exp-concave. One can show
that (cid:96)t(θ) is α-exp-concave [20  Sec. 2.2]. Then 
GLOC can use ONS and its OL regret bound to
solve the GLB problem. However  motivated by
the fact that the OL regret bound Bt appears in the
radius
βt of the conﬁdence set while a tighter
conﬁdence set tends to reduce the bandit regret
in practice  we derive a tight data-dependent OL
regret bound tailored to GLMs.
We present our version of ONS for GLMs (ONS-
GLM) in Algorithm 2. (cid:96)(cid:48)(z  y) is the ﬁrst deriva-
tive w.r.t. z and the parameter  is for inverting
matrices conveniently (usually  = 1 or 0.1). The
only difference from the original ONS [20] is that
we rely on the strong convexity of m(z) instead
of the α-exp-concavity of the loss thanks to the
GLM structure.2 Theorem 3 states that we achieve the desired polylogarithmic regret in T .

2: Set V0 = λI.
3: for t = 1  2  . . . do
4:
5:
6:
7:
8:
9:
10:
11: end for
Algorithm 2 ONS-GLM
1: Input: κ > 0   > 0  S > 0.
2: A0 = I.
3: Set θ1 ∈ Bd(S) arbitrarily.
4: for t = 1  2  3  . . . do
5:
Output θt .
6:
Observe xt and yt.
Incur loss (cid:96)(x(cid:62)
7:
8: At = At−1 + xtx(cid:62)
t+1 = θt − (cid:96)(cid:48)(x(cid:62)
−1
θ(cid:48)
9:
t xt
θt+1 = arg minθ∈Bd(S) ||θ − θ(cid:48)
10:
11: end for

Compute(cid:98)θt = V

Deﬁne Ct as in (6).

−1
t X(cid:62)

t θt  yt) .

t θt yt)

√

A

κ

t

t θ  yt).
t and zt = x(cid:62)
t θt
t zt and βt as in (5).

t+1||2

At

2 A similar change to ONS has been applied in [16  41].

4

Theorem 3. Deﬁne gs := (cid:96)(cid:48)(x(cid:62)

s θs  ys). The regret of ONS-GLM satisﬁes  for any  > 0 and t ≥ 1 

(cid:80)t
s=1 (cid:96)s(θs) − (cid:96)s(θ

(cid:80)t

∗

) ≤ 1

s=1 g2
d log t) ∀t ≥ 1 w.h.p.

2κ

s||xs||2

−1
A
s

+ 2κS2 =: BONS

 

t

t

κ
d log t).

= O( L2+R2 log(t)

where BONS
t = O( L2+ ¯R2
BONS
We emphasize that the OL regret bound is data-dependent. A conﬁdence set constructed by combining
Theorem 1 and Theorem 3 directly implies the following regret bound of GLOC with ONS-GLM.
Corollary 1. Deﬁne βONS

in (5). With probability at least 1 − 2δ 

by replacing Bt with BONS

If maxs≥1 |ηs| is bounded by ¯R w.p. 1 

κ

t

(cid:110)
θ ∈ Rd : ||θ −(cid:98)θt||2

t

(cid:111)

≤ βONS

t

:=
. Then  w.p. at least 1 − 2δ  ∀T ≥ 1  RegretGLOC
where ˆO ignores log log(t). If |ηt| is bounded by ¯R  RegretGLOC

Vt

T

.

t

T

(8)

=

=

∀t ≥ 1  θ

∗ ∈ CONS
Corollary 2. Run GLOC with CONS
ˆO

T log3/2(T )

(cid:17)

t

(cid:16) L(L+R)
(cid:16) L(L+ ¯R)

κ

(cid:17)

√
d
√
d

T log(T )

.

ˆO

κ

√

√

√

√

T log T )  which is

We make regret bound comparisons ignoring log log T factors. For generic arm sets  our dependence
on d is optimal for linear rewards [34]. For the Bernoulli GLM  our regret has the same order as Zhang
et al. [41]. One can show that the regret of Filippi et al. [15] has the same order as ours if we use their
assumption that the reward yt is bounded by Rmax. For unbounded noise  Li et al. [28] have regret
log T factor smaller than ours and has LR in place of L(L + R).
O((LR/κ)d
While L(L + R) could be an artifact of our analysis  the gap is not too large for canonical GLMs.
L. If L ≤ 1  R satisﬁes R > L  and so
Let L be the smallest Lipschitz constant of µ. Then  R =
L(L + R) = O(LR). If L > 1  then L(L + R) = O(L2)  which is larger than LR = O(L3/2). For
the Gaussian GLM with known variance σ2  L = R = 1.3 For ﬁnite arm sets  SupCB-GLM of Li
et al. [28] achieves regret of ˜O(
dT log N ) that has a better scaling with d but is not a practical
algorithm as it wastes a large number of arm pulls. Finally  we remark that none of the existing GLB
algorithms are scalable to large T . Zhang et al. [41] is scalable to large T   but is restricted to the
Bernoulli GLM; e.g.  theirs does not allow the probit model (non-canonical GLM) that is popular and
shown to be competitive to the Bernoulli GLM [27].
Discussion
The trick of obtaining a conﬁdence set from an online learner appeared ﬁrst in [13  14]
for the linear model  and then was used in [10  16  41]. GLOC is slightly different from these studies
and rather close to Abbasi-Yadkori et al. [2] in that the conﬁdence set is a function of a known regret
bound. This generality frees us from re-deriving a conﬁdence set for every online learner. Our result
is essentially a nontrivial extension of Abbasi-Yadkori et al. [2] to GLMs.
One might have notice that Ct does not use θt+1 that is available before pulling xt+1 and has the
most up-to-date information. This is inherent to GLOC as it relies on the OL regret bound directly.
One can modify the proof of ONS-GLM to have a tighter conﬁdence set Ct that uses θt+1 as we
show in SM Section E. However  this is now speciﬁc to ONS-GLM  which looses generality.

4 Hash-Amenable Generalized Linear Bandits
We now turn to a setting where the arm set is ﬁnite but very large. For example  imagine an interactive
retrieval scenario [33  25  6] where a user is shown K images (e.g.  shoes) at a time and provides
relevance feedback (e.g.  yes/no or 5-star rating) on each image  which is repeated until the user is
satisﬁed. In this paper  we focus on showing one image (i.e.  arm) at a time.4 Most existing algorithms
require maximizing an objective function (e.g.  (7))  the complexity of which scales linearly with the
number N of arms. This can easily become prohibitive for large numbers of images. Furthermore 
the system has to perform real-time computations to promptly choose which image to show the user
in the next round. Thus  it is critical for a practical system to have a time complexity sublinear in N.
One naive approach is to select a subset of arms ahead of time  such as volumetric spanners [19].
However  this is specialized for an efﬁcient exploration only and can rule out a large number of
good arms. Another option is to use hashing methods. Locality-sensitive hashing and Maximum

3 The reason why R is not σ here is that the sufﬁcient statistic of the GLM is y/σ  which is equivalent to

dealing with the normalized reward. Then  σ appears as a factor in the regret bound.

4 One image at a time is a simpliﬁcation of the practical setting. One can extend it to showing multiple

images at a time  which is a special case of the combinatorial bandits of Qin et al. [32].

5

that chooses an arm xt = arg maxx∈Xt x(cid:62) ˙θt where ˙θt ∼ N ((cid:98)θt−1  βt−1V

Inner Product Search (MIPS) are effective and well-understood tools but can only be used when the
objective function is a distance or an inner product computation; (7) cannot be written in this form.
In this section  we consider alternatives to GLOC which are compatible with hashing.
Thompson Sampling We present a Thompson sampling (TS) version of GLOC called GLOC-TS
−1
t−1). TS is known to
perform well in practice [8] and can solve the polytope arm set case in polynomial time5 whereas
algorithms that solve an objective function like (3) (e.g.  [1]) cannot since they have to solve an
NP-hard problem [5]. We present the regret bound of GLOC-TS below. Due to space constraints  we
present the pseudocode and the full version of the result in SM.

Theorem 4. (Informal) If we run GLOC-TS with ˙θt ∼ N ((cid:98)θt−1  βONS
(cid:16) L(L+R)
(cid:16) L(L+ ¯R)

−1
t−1)  RegretGLOC-TS
.
T log(T )

w.h.p. If ηt is bounded by ¯R  then ˆO

T log3/2(T )

t−1 V

(cid:17)

(cid:17)

d3/2

d3/2

√

√

ˆO

=

T

κ

κ

Notice that the regret now scales with d3/2 as expected from the analysis of linear TS [4]  which
is higher than scaling with d of GLOC. This is concerning in the interactive retrieval or product
recommendation scenario since the relevance of the shown items is harmed  which makes us wonder
if one can improve the regret without loosing the hash-amenability.
Quadratic GLOC We now propose a new hash-amenable algorithm called Quadratic GLOC
(QGLOC). Recall that GLOC chooses the arm xGLOC by (7). Deﬁne r = minx∈X ||x||2 and

||x||V

−1
t−1

mt−1 :=

min

β1/4
t−1

4c0mt−1

xQGLOC
t

4c0mt−1

V

||x||2

−1
t−1

V

x:||x||2∈[r 1]

(cid:1)1/2

= arg max
x∈Xt

(cid:16)(cid:16) 1

(cid:0) L+R

(cid:0) L+R

(cid:104)qt  φ(x)(cid:105)  

(cid:1)3/2(cid:17)

:= arg max
x∈Xt
β1/4
t−1

√
for all x ∈ X and that mt−1 ≥ r/

−1
t−1
which is r times the square root of the smallest eigenvalue of V
||x||V
alternative way to deﬁne mt−1 without relying on r  which we present in SM.
Let c0 > 0 be the exploration-exploitation tradeoff parameter (elaborated upon later). At time t 
QGLOC chooses the arm

 
(9)
−1
t−1. It is easy to see that mt−1 ≤
t + λ using the deﬁnition of Vt−1. There is an

where qt = [(cid:98)θt−1; vec(

. By setting c0 = (cid:0) L+R

(cid:104)(cid:98)θt−1  x(cid:105) +
(10)
−1
t−1)] ∈ Rd+d2 and φ(x) := [x; vec(xx(cid:62))]. The key property of
QGLOC is that the objective function is now quadratic in x  thus the name Quadratic GLOC  and
can be written as an inner product. Thus  QGLOC is hash-amenable. We present the regret bound of
QGLOC (10) in Theorem 5. The key step of the proof is that the QGLOC objective function (10)
plus c0β3/4mt−1 is a tight upper bound of the GLOC objective function (7).
Theorem 5. Run QGLOC with CONS
.
√
Ld5/4
O
bound is O( L(L+R)
Note that one can have a better dependence on log T when ηt is bounded (available in the proof).
The regret bound of QGLOC is a d1/4 factor improvement over that of GLOC-TS; see Table 1.
Furthermore  in (10) c0 is a free parameter that adjusts the balance between the exploitation (the ﬁrst
term) and exploration (the second term). Interestingly  the regret guarantee does not break down when
adjusting c0 in Theorem 5. Such a characteristic is not found in existing algorithms but is attractive
to practitioners  which we elaborate in SM.
Maximum Inner Product Search (MIPS) Hashing While MIPS hashing algorithms such as [35 
36  30] can solve (10) in time sublinear in N  these necessarily introduce an approximation error.
Ideally  one would like the following guarantee on the error with probability at least 1 − δH:
Deﬁnition 1. Let X ⊆ Rd(cid:48)
satisfy |X| < ∞. A data point ˜x ∈ X is called cH-MIPS w.r.t. a given
query q if it satisﬁes (cid:104)q  ˜x(cid:105) ≥ cH · maxx∈X (cid:104)q  x(cid:105) for some cH < 1. An algorithm is called cH-MIPS
if  given a query q ∈ Rd(cid:48)
Unfortunately  existing MIPS algorithms do not directly offer such a guarantee  and one must build a
series of hashing schemes with varying hashing parameters like Har-Peled et al. [18]. Under the ﬁxed
budget setting T   we elaborate our construction that is simpler than [18] in SM.

  it retrieves x ∈ X that is cH-MIPS w.r.t. q.

(cid:1)−1/2  the regret

least 1 − 2δ  RegretQGLOC

κ
T log2(T )).

+ c0
d5/4

Then  w.p.

T log2(T )

T

=

c0

κ

κ

(cid:17)

at

t

√

κ

5ConﬁdenceBall1 algorithm of Dani et al. [11] can solve the problem in polynomial time as well.

6

(cid:16)
(cid:16) log(dT )

log
N ρ∗

log(c

−1
H )

(N + log(N )d(cid:48))

Time and Space Complexity Our construction involves saving Gaussian projection vectors that
are used for determining hash keys and saving the buckets containing pointers to the actual arm
vectors. The time complexity for retrieving a cH-MIPS solution involves determining hash keys
and evaluating inner products with the arms in the retrieved buckets. Let ρ∗ < 1 be an opti-
mized value for the hashing (see [35] for detail). The time complexity for d(cid:48)-dimensional vec-
  and the space complexity (except the original data) is
tors is O

(cid:16) log(dT )

N ρ∗

(cid:17)

log(N )d(cid:48)(cid:17)
(cid:17)

log(c

−1
H )

. While the time and space complexity grows with the time hori-
O
zon T   the dependence is mild; log log(T ) and log(T )  respectively. QGLOC uses d(cid:48) = d + d2 6 and
GLOC-TS uses d(cid:48) = d(cid:48). While both achieve a time complexity sublinear in N  the time complexity
of GLOC-TS scales with d that is better than scaling with d2 of QGLOC. However  GLOC-TS has a
d1/4-factor worse regret bound than QGLOC.
Discussion While it is reasonable to incur small errors in solving the arm selection criteria like (10)
and sacriﬁce some regret in practice  the regret bounds of QGLOC and GLOC-TS do not hold
anymore. Though not the focus of our paper  we prove a regret bound under the presence of the
hashing error in the ﬁxed budget setting for QGLOC; see SM. Although the result therein has an
inefﬁcient space complexity that is linear in T   it provides the ﬁrst low regret bound with time
sublinear in N  to our knowledge.

(a)

(b)

5 Approximate Inner Product Computations with L1 Sampling
While hashing allows a time complexity sub-
linear in N  it performs an additional com-
putation for determining the hash keys. Con-
sider a hashing with U tables and length-k hash
keys. Given a query q and projection vectors
a(1)  . . .   a(U k)  the hashing computes q(cid:62)a(i) 
∀i ∈ [U k] to determine the hash key of q. To
reduce such an overhead  approximate inner
product methods like [22  24] are attractive
since hash keys are determined by discretizing
the inner products; small inner product errors
often do not alter the hash keys.
In this section  we propose an improved approximate inner product method called L1 sampling which
we claim is more accurate than the sampling proposed by Jain et al. [22]  which we call L2 sampling.
Consider an inner product q(cid:62)a. The main idea is to construct an unbiased estimate of q(cid:62)a. That is 
let p ∈ Rd be a probability vector. Let

Figure 1: (a) A box plot of estimators. L1 and L2
have the same variance  but L2 has thicker tails. (b)
The frequency of L1 inducing smaller variance than
L2 in 1000 trials. After 100 dimensions  L1 mostly
has smaller variance than L2.

(cid:80)m
and Gk := qik aik /pik   k ∈ [m] .
(11)
k=1 Gk as an estimate of q(cid:62)a  the time complexity
It is easy to see that EGk = q(cid:62)a. By taking 1
is now O(mU k) rather than O(d(cid:48)U k). The key is to choose the right p. L2 sampling uses p(L2) :=
i /||q||2
2]i. Departing from L2  we propose p(L1) that we call L1 sampling and deﬁne as follows:
[q2
(12)
We compare L1 with L2 in two different point of view. Due to space constraints  we summarize the
key ideas and defer the details to SM.
The ﬁrst is on their concentration of measure. Lemma 1 below shows an error bound of L1 whose
failure probability decays exponentially in m. This is in contrast to decaying polynomially of L2 [22] 
which is inferior.7
Lemma 1. Deﬁne Gk as in (11) with p = p(L1). Then  given a target error  > 0 

p(L1) := [|q1|;··· ;|qd(cid:48)|]/||q||1 .

i.i.d.∼ Multinomial(p)

ik

m

P(cid:0)(cid:12)(cid:12) 1

m

k=1 Gk − q(cid:62)a(cid:12)(cid:12) ≥ (cid:1) ≤ 2 exp
(cid:80)m

(cid:16)−

(cid:17)

2||q||2

m2
1||a||2

max

(13)

To illustrate such a difference  we ﬁx q and a in 1000 dimension and apply L2 and L1 sampling 20K
times each with m = 5 where we scale down the L2 distribution so its variance matches that of L1.

6 Note that this does not mean we need to store vec(xx(cid:62)) since an inner product with it is structured.
7 In fact  one can show a bound for L2 that fails with exponentially-decaying probability. However  the bound
introduces a constant that can be arbitrarily large  which makes the tails thick. We provide details on this in SM.

7

L2L1-505100101102103d0.70.80.91Algorithm
QGLOC
QGLOC-Hash
GLOC-TS
GLOC-TS-Hash

Cum. Regret
266.6 (±19.7)
285.0 (±30.3)
277.0 (±36.1)
289.1 (±28.1)

(a)

(b)

(c)

√

cd log t. For GLOC  we replace βONS

t with c(cid:80)t

Figure 2: Cumulative regrets with conﬁdence intervals under the (a) logit and (b) probit model. (c)
Cumulative regrets with conﬁdence intervals of hash-amenable algorithms.
Figure 1(a) shows that L2 has thicker tails than L1. Note this is not a pathological case but a typical
case for Gaussian q and a. This conﬁrms our claim that L1 is safer than L2.
Another point of comparison is the variance of L2 and L1. We show that the variance of L1 may or
may not be larger than L2 in SM; there is no absolute winner. However  if q and a follow a Gaussian
distribution  then L1 induces smaller variances than L2 for large enough d; see Lemma 9 in SM.
Figure 1(b) conﬁrms such a result. The actual gap between the variance of L2 and L1 is also nontrivial
under the Gaussian assumption. For instance  with d = 200  the average variance of Gk induced by
L2 is 0.99 whereas that induced by L1 is 0.63 on average. Although a stochastic assumption on the
vectors being inner-producted is often unrealistic  in our work we deal with projection vectors a that
are truly normally distributed.
6 Experiments
We now show our experiment results comparing GLB algorithms and hash-amenable algorithms.
GLB Algorithms We compare GLOC with two different algorithms: UCB-GLM [28] and Online
∗ ∈ Rd and N arms (X ) uniformly
Learning for Logit Model (OL2M) [41].8 For each trial  we draw θ
at random from the unit sphere. We set d = 10 and Xt = X   ∀t ≥ 1. Note it is a common practice to
scale the conﬁdence set radius for bandits [8  27]. Following Zhang et al. [41]  for OL2M we set the
squared radius γt = c log(det(Zt)/det(Z1))  where c is a tuning parameter. For UCB-GLM  we set
the radius as α =
. While parameter
tuning in practice is nontrivial  for the sake of comparison we tune c ∈ {101  100.5  . . .   10−3} and
report the best one. We perform 40 trials up to time T = 3000 for each method and compute
conﬁdence bounds on the regret.
We consider two GLM rewards: (i) the logit model (the Bernoulli GLM) and (ii) the probit model
(non-canonical GLM) for 0/1 rewards that sets µ as the probit function. Since OL2M is for the
logit model only  we expect to see the consequences of model mismatch in the probit setting. For
GLOC and UCB-GLM  we specify the correct reward model. We plot the cumulative regret under the
logit model in Figure 2(a). All three methods perform similarly  and we do not ﬁnd any statistically
signiﬁcant difference based on paired t test. The result for the probit model in Figure 2(b) shows that
OL2M indeed has higher regret than both GLOC and UCB-GLM due to the model mismatch in the
probit setting. Speciﬁcally  we verify that at t = 3000 the difference between the regret of UCB-GLM
and OL2M is statistically signiﬁcant. Furthermore  OL2M exhibits a signiﬁcantly higher variance in
the regret  which is unattractive in practice. This shows the importance of being generalizable to any
GLM reward. Note we observe a big increase in running time for UCB-GLM compared to OL2M
and GLOC.
Hash-Amenable GLBs To compare hash-amenable GLBs  we use the logit model as above but now
with N=100 000 and T =5000. We run QGLOC  QGLOC with hashing (QGLOC-Hash)  GLOC-TS 
and GLOC-TS with hashing (GLOC-TS-Hash)  where we use the hashing to compute the objective
function (e.g.  (10)) on just 1% of the data points and save a signiﬁcant amount of computation.
Details on our hashing implementation is found in SM. Figure 2(c) summarizes the result. We observe
that QGLOC-Hash and GLOC-TS-Hash increase regret from QGLOC and GLOC-TS  respectively 
but only moderately  which shows the efﬁcacy of hashing.
7 Future Work
In this paper  we have proposed scalable algorithms for the GLB problem: (i) for large time horizon
T and (ii) for large number N of arms. There exists a number of interesting future work. First 

−1
s

A

s=1 g2

s||xs||2

8We have chosen UCB-GLM over GLM-UCB of Filippi et al. [15] as UCB-GLM has a lower regret bound.

8

we would like to extend the GLM rewards to the single index models [23] so one does not need to
know the function µ ahead of time under mild assumptions. Second  closing the regret bound gap
√
between QGLOC and GLOC without loosing hash-amenability would be interesting: i.e.  develop
a hash-amenable GLB algorithm with O(d
T ) regret. In this direction  a ﬁrst attempt could be to
design a hashing scheme that can directly solve (7) approximately.
Acknowledgments This work was partially supported by the NSF grant IIS-1447449 and the
MURI grant 2015-05174-04. The authors thank Yasin Abbasi-Yadkori and Anshumali Shrivastava
for providing constructive feedback and Xin Hunt for her contribution at the initial stage.

References
[1] Abbasi-Yadkori  Yasin  Pal  David  and Szepesvari  Csaba. Improved Algorithms for Linear
Stochastic Bandits. Advances in Neural Information Processing Systems (NIPS)  pp. 1–19 
2011.

[2] Abbasi-Yadkori  Yasin  Pal  David  and Szepesvari  Csaba. Online-to-Conﬁdence-Set Con-
versions and Application to Sparse Stochastic Bandits. In Proceedings of the International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[3] Abeille  Marc and Lazaric  Alessandro. Linear Thompson Sampling Revisited. In Proceedings
of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  volume 54 
pp. 176–184  2017.

[4] Agrawal  Shipra and Goyal  Navin. Thompson Sampling for Contextual Bandits with Linear

Payoffs. CoRR  abs/1209.3  2012.

[5] Agrawal  Shipra and Goyal  Navin. Thompson Sampling for Contextual Bandits with Linear
Payoffs. In Proceedings of the International Conference on Machine Learning (ICML)  pp.
127–135  2013.

[6] Ahukorala  Kumaripaba  Medlar  Alan  Ilves  Kalle  and Glowacka  Dorota. Balancing Ex-
ploration and Exploitation: Empirical Parameterization of Exploratory Search Systems. In
Proceedings of the ACM International Conference on Information and Knowledge Management
(CIKM)  pp. 1703–1706  2015.

[7] Auer  Peter and Long  M. Using Conﬁdence Bounds for Exploitation-Exploration Trade-offs.

Journal of Machine Learning Research  3:397–422  2002.

[8] Chapelle  Olivier and Li  Lihong. An Empirical Evaluation of Thompson Sampling. In Advances

in Neural Information Processing Systems (NIPS)  pp. 2249–2257  2011.

[9] Chu  Wei  Li  Lihong  Reyzin  Lev  and Schapire  Robert E. Contextual Bandits with Linear
Payoff Functions. In Proceedings of the International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS)  volume 15  pp. 208–214  2011.

[10] Crammer  Koby and Gentile  Claudio. Multiclass Classiﬁcation with Bandit Feedback Using

Adaptive Regularization. Mach. Learn.  90(3):347–383  2013.

[11] Dani  Varsha  Hayes  Thomas P  and Kakade  Sham M. Stochastic Linear Optimization under
Bandit Feedback. In Proceedings of the Conference on Learning Theory (COLT)  pp. 355–366 
2008.

[12] Datar  Mayur  Immorlica  Nicole  Indyk  Piotr  and Mirrokni  Vahab S. Locality-sensitive
Hashing Scheme Based on P-stable Distributions. In Proceedings of the Twentieth Annual
Symposium on Computational Geometry  pp. 253–262  2004.

[13] Dekel  Ofer  Gentile  Claudio  and Sridharan  Karthik. Robust selective sampling from single
and multiple teachers. In In Proceedings of the Conference on Learning Theory (COLT)  2010.

[14] Dekel  Ofer  Gentile  Claudio  and Sridharan  Karthik. Selective sampling and active learning
from single and multiple teachers. Journal of Machine Learning Research  13:2655–2697 
2012.

9

[15] Filippi  Sarah  Cappe  Olivier  Garivier  Aurélien  and Szepesvári  Csaba. Parametric Bandits:
The Generalized Linear Case. In Advances in Neural Information Processing Systems (NIPS) 
pp. 586–594. 2010.

[16] Gentile  Claudio and Orabona  Francesco. On Multilabel Classiﬁcation and Ranking with

Bandit Feedback. Journal of Machine Learning Research  15:2451–2487  2014.

[17] Guo  Ruiqi  Kumar  Sanjiv  Choromanski  Krzysztof  and Simcha  David. Quantization based

Fast Inner Product Search. Journal of Machine Learning Research  41:482–490  2016.

[18] Har-Peled  Sariel  Indyk  Piotr  and Motwani  Rajeev. Approximate nearest neighbor: towards

removing the curse of dimensionality. Theory of Computing  8:321–350  2012.

[19] Hazan  Elad and Karnin  Zohar. Volumetric Spanners: An Efﬁcient Exploration Basis for

Learning. Journal of Machine Learning Research  17(119):1–34  2016.

[20] Hazan  Elad  Agarwal  Amit  and Kale  Satyen. Logarithmic Regret Algorithms for Online

Convex Optimization. Mach. Learn.  69(2-3):169–192  2007.

[21] Hofmann  Katja  Whiteson  Shimon  and de Rijke  Maarten. Contextual Bandits for Information
Retrieval. In NIPS Workshop on Bayesian Optimization  Experimental Design and Bandits:
Theory and Applications  2011.

[22] Jain  Prateek  Vijayanarasimhan  Sudheendra  and Grauman  Kristen. Hashing Hyperplane
Queries to Near Points with Applications to Large-Scale Active Learning. In Advances in
Neural Information Processing Systems (NIPS)  pp. 928–936  2010.

[23] Kalai  Adam Tauman and Sastry  Ravi. The Isotron Algorithm: High-Dimensional Isotonic

Regression. In Proceedings of the Conference on Learning Theory (COLT)  2009.

[24] Kannan  Ravindran  Vempala  Santosh  and Others. Spectral algorithms. Foundations and

Trends in Theoretical Computer Science  4(3–4):157–288  2009.

[25] Konyushkova  Ksenia and Glowacka  Dorota. Content-based image retrieval with hierarchical
Gaussian Process bandits with self-organizing maps. In 21st European Symposium on Artiﬁcial
Neural Networks  2013.

[26] Li  Lihong  Chu  Wei  Langford  John  and Schapire  Robert E. A Contextual-Bandit Approach
to Personalized News Article Recommendation. Proceedings of the International Conference
on World Wide Web (WWW)  pp. 661–670  2010.

[27] Li  Lihong  Chu  Wei  Langford  John  Moon  Taesup  and Wang  Xuanhui. An Unbiased Ofﬂine
Evaluation of Contextual Bandit Algorithms with Generalized Linear Models. In Proceedings
of the Workshop on On-line Trading of Exploration and Exploitation 2  volume 26  pp. 19–36 
2012.

[28] Li  Lihong  Lu  Yu  and Zhou  Dengyong. Provable Optimal Algorithms for Generalized Linear

Contextual Bandits. CoRR  abs/1703.0  2017.

[29] McCullagh  P and Nelder  J A. Generalized Linear Models. London  1989.

[30] Neyshabur  Behnam and Srebro  Nathan. On Symmetric and Asymmetric LSHs for Inner
Product Search. Proceedings of the International Conference on Machine Learning (ICML)  37:
1926–1934  2015.

[31] Orabona  Francesco  Cesa-Bianchi  Nicolo  and Gentile  Claudio. Beyond Logarithmic Bounds
in Online Learning. In Proceedings of the International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS)  volume 22  pp. 823–831  2012.

[32] Qin  Lijing  Chen  Shouyuan  and Zhu  Xiaoyan. Contextual Combinatorial Bandit and its

Application on Diversiﬁed Online Recommendation. In SDM  pp. 461–469  2014.

[33] Rui  Yong  Huang  T S  Ortega  M  and Mehrotra  S. Relevance feedback: a power tool for
interactive content-based image retrieval. IEEE Transactions on Circuits and Systems for Video
Technology  8(5):644–655  1998.

10

[34] Rusmevichientong  Paat and Tsitsiklis  John N. Linearly Parameterized Bandits. Math. Oper.

Res.  35(2):395–411  2010.

[35] Shrivastava  Anshumali and Li  Ping. Asymmetric LSH ( ALSH ) for Sublinear Time Maximum
Inner Product Search ( MIPS ). Advances in Neural Information Processing Systems 27  pp.
2321–2329  2014.

[36] Shrivastava  Anshumali and Li  Ping. Improved Asymmetric Locality Sensitive Hashing (ALSH)
for Maximum Inner Product Search (MIPS). In Proceedings of the Conference on Uncertainty
in Artiﬁcial Intelligence (UAI)  pp. 812–821  2015.

[37] Slaney  Malcolm  Lifshits  Yury  and He  Junfeng. Optimal parameters for locality-sensitive

hashing. Proceedings of the IEEE  100(9):2604–2623  2012.

[38] Wainwright  Martin J and Jordan  Michael I. Graphical Models  Exponential Families  and

Variational Inference. Found. Trends Mach. Learn.  1(1-2):1–305  2008.

[39] Wang  Jingdong  Shen  Heng Tao  Song  Jingkuan  and Ji  Jianqiu. Hashing for Similarity

Search: A Survey. CoRR  abs/1408.2  2014.

[40] Yue  Yisong  Hong  Sue Ann Sa  and Guestrin  Carlos. Hierarchical exploration for accelerating
contextual bandits. Proceedings of the International Conference on Machine Learning (ICML) 
pp. 1895–1902  2012.

[41] Zhang  Lijun  Yang  Tianbao  Jin  Rong  Xiao  Yichi  and Zhou  Zhi-hua. Online Stochastic
Linear Optimization under One-bit Feedback. In Proceedings of the International Conference
on Machine Learning (ICML)  volume 48  pp. 392–401  2016.

11

,Mijung Park
Gergo Bohner
Jakob Macke
Kwang-Sung Jun
Aniruddha Bhargava
Robert Nowak
Rebecca Willett
Chaosheng Dong
Yiran Chen
Bo Zeng