2014,Restricted Boltzmann machines modeling human choice,We extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans. These phenomena include the similarity effect  the attraction effect  and the compromise effect. We formally quantify the strength of these phenomena that can be represented by our choice model  which illuminates the flexibility of our choice model. We then show that our choice model can be represented as a restricted Boltzmann machine and that its parameters can be learned effectively from data. Our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.,Restricted Boltzmann machines modeling human

choice

Takayuki Osogami
IBM Research - Tokyo

osogami@jp.ibm.com

Makoto Otsuka

IBM Research - Tokyo
motsuka@ucla.edu

Abstract

We extend the multinomial logit model to represent some of the empirical phe-
nomena that are frequently observed in the choices made by humans. These phe-
nomena include the similarity effect  the attraction effect  and the compromise
effect. We formally quantify the strength of these phenomena that can be repre-
sented by our choice model  which illuminates the ﬂexibility of our choice model.
We then show that our choice model can be represented as a restricted Boltzmann
machine and that its parameters can be learned effectively from data. Our numer-
ical experiments with real data of human choices suggest that we can train our
choice model in such a way that it represents the typical phenomena of choice.

1

Introduction

Choice is a fundamental behavior of humans and has been studied extensively in Artiﬁcial Intelli-
gence and related areas. The prior work suggests that the choices made by humans can signiﬁcantly
depend on available alternatives  or the choice set  in rather complex but systematic ways [13]. The
empirical phenomena that result from such dependency on the choice set include the similarity ef-
fect  the attraction effect  and the compromise effect. Informally  the similarity effect refers to the
phenomenon that a new product  S  reduces the share of a similar product  A  more than a dissimilar
product  B (see Figure 1 (a)). With the attraction effect  a new dominated product  D  increases the
share of the dominant product  A (see Figure 1 (b)). With the compromise effect  a product  C  has
a relatively larger share when two extreme products  A and B  are in the market than when only
one of A and B is in the market (see Figure 1 (c)). We call these three empirical phenomena as the
typical choice phenomena.
However  the standard choice model of the multinomial logit model (MLM) and its variants cannot
represent at least one of the typical choice phenomena [13]. More descriptive models have been
proposed to represent the typical choice phenomena in some representative cases [14  19]. However 
it is unclear when and to what degree the typical choice phenomena can be represented. Also  no
algorithms have been proposed for training these descriptive models from data.

(a) Similarity

(b) Attraction

(c) Compromise

Figure 1: Choice sets that cause typical choice phenomena.

1

SABABDABCWe extend the MLM to represent the typical choice phenomena  which is our ﬁrst contribution.
We show that our choice model can be represented as a restricted Boltzmann machine (RBM). Our
choice model is thus called the RBM choice model. An advantage of this representation as an RBM
is that training algorithms for RBMs are readily available. See Section 2.
We then formally deﬁne the measure of the strength for each typical choice phenomenon and quan-
tify the strength of each typical choice phenomenon that the RBM choice model can represent. Our
analysis not only gives a guarantee on the ﬂexibility of the RBM choice model but also illuminates
why the RBM choice model can represent the typical choice phenomena. These deﬁnitions and
analysis constitute our second contribution and are presented in Section 3.
Our experiments suggest that we can train the RBM choice model in such a way that it represents
the typical choice phenomena. We show that the trained RBM choice model can then adequately
predict real human choice on the means of transportation [2]. These experimental results constitute
our third contribution and are presented in Section 4.

2 Choice model with restricted Boltzmann machine
We extend the MLM to represent the typical choice phenomena. Let I be the set of items. For A ∈
X ⊆ I  we study the probability that an item  A  is selected from a choice set  X . This probability
is called the choice probability. The model of choice  equipped with the choice probability  is called
a choice model. We use A  B  C  D  S  or X to denote an item and X  Y  or a set such as {A  B} to
denote a choice set.
For the MLM  the choice probability of A from X can be represented by

p(A|X ) =

 

(1)
where we refer to λ(X|X ) as the choice rate of X from X . The choice rate of the MLM is given by
(2)
where bX can be interpreted as the attractiveness of X. One could deﬁne bX through uX  the
vector of the utilities of the attributes for X  and α  the vector of the weight on each attribute (i.e. 
bX ≡ α·uX). Observe that λMLM(X|X ) is independent of X as long as X ∈ X . This independence
causes the incapability of the MLM in representing the typical choice phenomena.
We extend the choice rate of (2) but keep the choice probability in the form of (1). Speciﬁcally  we
consider the following choice rate:

λMLM(X|X ) = exp(bX ) 

(cid:80)
λ(A|X )
X∈X λ(X|X )

λ(X|X ) ≡ exp(bX )

(cid:0)1 + exp(cid:0)T kX + U k

X

(cid:1)(cid:1)  

(3)

(cid:89)
T kX ≡ (cid:88)

k∈K

where we deﬁne

T k
Y .

X  that depend on k. The set of these indices is denoted by K.

(4)
Y ∈X
X for X ∈ X   k ∈ K  that take values in (−∞ ∞).
Our choice model has parameters  bX   T kX   U k
Equation (3) modiﬁes exp(bX ) by multiplying factors. Each factor is associated with an index  k 
and has parameters  T kX and U k
We now show that our choice model can be represented as a restricted Boltzmann machine (RBM).
This means that we can use existing algorithms for RBMs to learn the parameters of the RBM choice
model (see Appendix A.1).
An RBM consists of a layer of visible units  i ∈ V  and a layer of hidden units  k ∈ H. A visible unit 
i . The units within each layer are disconnected
i  and a hidden unit  k  are connected with weight  W k
from each other. Each unit is associated with a bias. The bias of a visible unit  i  is denoted by bvis
.
The bias of a hidden unit  k  is denoted by bhid
k . A visible unit  i  is associated with a binary variable 
zi  and a hidden unit  k  is associated with a binary variable  hk  which takes a value in {0  1}.
For a given conﬁguration of binary variables  the energy of the RBM is deﬁned as

i

i hk + bvis

i zi + bhid

k hk

(5)

(cid:1)  

Eθ(z  h) ≡ −(cid:88)

(cid:88)

i∈V

k∈H

(cid:0)zi W k

2

Figure 2: RBM choice model

.

(cid:80)
z(cid:48)(cid:80)

z(cid:48) or(cid:80)

where θ ≡ {W  bvis  bhid} denotes the parameters of the RBM. The probability of realizing a partic-
ular conﬁguration of (z  h) is given by
Pθ(z  h) ≡

The summation with respect to a binary vector (i.e. (cid:80)

exp(−Eθ(z  h))
h(cid:48) exp(−Eθ(z(cid:48)  h(cid:48)))

the possible binary vectors of a given length. The length of z(cid:48) is |V|  and the length of h(cid:48) is |H|.
The RBM choice model can be represented as an RBM having the structure in Figure 2. Here  the
layer of visible units is split into two parts: one for the choice set and the other for the selected item.
The corresponding binary vector is denoted by z = (v  w). Here  v is a binary vector associated
with the part for the choice set. Speciﬁcally  v has length |I|  and vX = 1 denotes that X is in the
choice set. Analogously  w has length |I|  and wA = 1 denotes that A is selected. We use T k
X to
A to
denote the weight between a hidden unit  k  and a visible unit  X  for the choice set. We use U k
denote the weight between a hidden unit  k  and a visible unit  A  for the selected item. The bias is
zero for all of the hidden units and for all of the visible units for the choice set. The bias for a visible
unit  A  for the selected item is denoted by bA. Finally  let H = K.
The choice rate (3) of the RBM choice model can then be represented by

h(cid:48)) denotes the summation over all of

(6)

λ(A|X ) =

where we deﬁne the binary vectors  vX   wA  such that vX
Observe that the right-hand side of (7) is
exp(−Eθ((vX   wA)  h)) =

(cid:88)

exp

h

h

(cid:88)

h

exp(cid:0)−Eθ
(cid:88)

= exp(bA)

= exp(bA)

(cid:0)(cid:0)vX   wA(cid:1)   h(cid:1)(cid:1)  
i = 1 iff i ∈ X and wA
(cid:32)(cid:88)
(cid:88)
(cid:88)
(cid:89)
(cid:88)
exp(cid:0)(cid:0)T kX + U k
(cid:1) hk
(cid:1)
(cid:89)
(cid:88)
exp(cid:0)(cid:0)T kX + U k
(cid:1) hk

T k
X hk +

X∈X

U k

A

A

h

k

k

k

hk∈{0 1}

k

(cid:1)  

j = 1 iff j = A.

(cid:33)

A hk + bA

(7)

(8)

(9)

(10)

(12)

which is equivalent to (3).
The RBM choice model assumes that one item from a choice set is selected. In the context of the
RBM  this means that wA = 1 for only one A ∈ X ⊆ I. Using (6)  our choice probability (1) can
be represented by

p(A|X ) =

(11)
This is the conditional probability of realizing the conﬁguration  (vX   wA)  given that the realized
conﬁguration is either of the (vX   wX ) for X ∈ X . See Appendix A.2for an extension of the RBM
choice model.

h Pθ((vX   wX )  h)

.

(cid:80)
(cid:80)
X∈X(cid:80)
h Pθ((vX   wA)  h)

3 Flexibility of the RBM choice model
In this section  we formally study the ﬂexibility of the RBM choice model. Recall that λ(X|X ) in
(3) is modiﬁed from λMLM(X|X ) in (2) by a factor 

1 + exp(cid:0)T kX + U k

X

(cid:1)  

3

k......X......A......TXkUAkHiddenChoice setSelected itembAfor each k in K  so that λ(X|X ) can depend on X through T kX . We will see how this modiﬁcation
allows the RBM choice model to represent each of the typical choice phenomena.
The similarity effect refers to the following phenomenon [14]:

p(A|{A  B}) > p(B|{A  B})

and

p(A|{A  B  S}) < p(B|{A  B  S}).

(13)

Motivated by (13)  we deﬁne the strength of the similarity effect as follows:
Deﬁnition 1. For A  B ∈ X   the strength of the similarity effect of S on A relative to B with X is
deﬁned as follows:

A B S X ≡ p(A|X )
p(B|X )

ψ(sim)

p(B|X ∪ {S})
p(A|X ∪ {S})

.

(14)

A B S X = 1  adding S into X does not change the ratio between p(A|X ) and p(B|X ).
When ψ(sim)
Namely  there is no similarity effect. When ψ(sim)
p(A|X ) by a factor
A B S X by the addition of S into X . This corresponds to the similarity effect of (13). When
of ψ(sim)
ψ(sim)
A B S X < 1  this ratio decreases by an analogous factor. We will study the strength of this (rather
general) similarity effect without the restriction that S is “similar” to A (see Figure 1 (a)).
Because p(X|X ) has a common denominator for X = A and X = B  we have

A B S X > 1  we can increase p(B|X )

ψ(sim)

A B S X =

λ(A|X )
λ(B|X )

λ(B|X ∪ {S})
λ(A|X ∪ {S})

.

(15)

The MLM cannot represent the similarity effect  because the λMLM(X|X ) in (2) is independent of
X . For any choice sets  X and Y  we must have
λMLM(A|X )
λMLM(B|X )

λMLM(A|Y)
λMLM(B|Y)

(16)

=

.

The equality (16) is known as the independence from irrelevant alternatives (IIA).
The RBM choice model can represent an arbitrary strength of the similarity effect. Speciﬁcally  by
adding an element  ˆk  into K of (3)  we can set λ(A|X∪{S})
at an arbitrary value without affecting
the value of λ(B|Y) ∀B (cid:54)= A  for any Y. We prove the following theorem in Appendix C:
Theorem 1. Consider an RBM choice model where the choice rate of X from X is given by (2). Let
ˆλ(X|X ) be the corresponding choice rate after adding ˆk into K. Namely 

λ(A|X )

ˆλ(X|X ) = λ(X|X )

(17)
Consider an item A ∈ X and an item S (cid:54)∈ X . For any c ∈ (0 ∞) and ε > 0  we can then choose
T ˆk· and U ˆk· such that

1 + exp

.

ˆkX + U
T

(cid:16)

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆλ(B|Y)

λ(B|Y)

− 1

ˆk
X

(cid:17)(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   ∀Y  B s.t. B (cid:54)= A.

c =

ˆλ(A|X ∪ {S})

ˆλ(A|X )

;

ε >

(18)

(19)

By (15) and Theorem 1  the strength of the similarity effect after adding ˆk into K is

ˆψ(sim)

A B S X =

ˆλ(A|X )

ˆλ(A|X ∪ {S})

ˆλ(B|X ∪ {S})

ˆλ(B|X )

≈ 1
c

λ(B|X ∪ {S})

λ(B|X )

.

Because c can take an arbitrary value in (0 ∞)  the additional factor  (12) with k = ˆk  indeed allows
A B S X to take any positive value without affecting the value of λ(B|Y) ∀B (cid:54)= A  for any Y. The
ˆψ(sim)
ﬁrst part of (18) guarantees that this additional factor does not change p(X|Y) for any X if A /∈ Y.
Note that what we have shown is not limited to the similarity effect of (13). The RBM choice model
can represent an arbitrary phenomenon where the choice set affects the ratio of the choice rate.

4

According to [14]  the attraction effect is represented by

p(A|{A  B}) < p(A|{A  B  D}).

X∈X λMLM(X|X ) ≤ (cid:80)

of Y  and we must have(cid:80)

(20)
The MLM cannot represent the attraction effect  because the λMLM(X|Y) in (2) is independent
X∈Y λMLM(X|Y) for X ⊂ Y  which in turn
implies the regularity principle: p(X|X ) ≥ p(X|Y) for X ⊂ Y.
Motivated by (20)  we deﬁne the strength of the attraction effect as the magnitude of the change in
the choice probability of an item when another item is added into the choice set. Formally 
Deﬁnition 2. For A ∈ X   the strength of the attraction effect of D on A with X is deﬁned as follows:
(21)

A D X ≡ p(A|X ∪ {D})

ψ(att)

.

p(A|X )

A D X ≤ 1.
A D X > 1. We study the strength of

When there is no attraction effect  adding D into X can only decrease p(A|X ); hence  ψ(att)
The standard deﬁnition of the attraction effect (20) implies ψ(att)
this attraction effect without the restriction that A “dominates” D (see Figure 1 (b)).
We prove the following theorem in Appendix C:
Theorem 2. Consider the two RBM choice models in Theorem 1. The ﬁrst RBM choice model has
the choice rate given by (3)  and the second RBM choice model has the choice rate given by (17).
Let p(·|·) denote the choice probability for the ﬁrst RBM choice model and ˆp(·|·) denote the choice
probability for the second RBM choice model. Consider an item A ∈ X and an item D (cid:54)∈ X . For
any r ∈ (p(A|X ∪ {D})  1/p(A|X )) and ε > 0  we can choose T ˆk·   U ˆk· such that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆλ(B|Y)

λ(B|Y)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   ∀Y  B s.t. B (cid:54)= A.

− 1

(22)

ˆp(A|X ∪ {D})

ˆp(A|X )

r =

;

ε >

We expect that the range  (p(A|X ∪ {D})  1/p(A|X ))  of r in the theorem covers the attraction
effect in practice. Also  this range is the widest possible in the following sense. The factor (12) can
only increase λ(X|Y) for any X Y. The form of (1) then implies that  to decrease p(A|Y)  we must
increase λ(X|Y) for X (cid:54)= A. However  increasing λ(X|Y) for X (cid:54)= A is not allowed due to the
second part of (22) with ε → 0. Namely  the additional factor  (12) with k = ˆk  can only increase
p(A|Y) for any Y under the condition of the second part of (22). The lower limit  p(A|X ∪ {D}) 
is achieved when ˆp(A|X ) → 1  while keeping ˆp(A|X ∪ {D}) ≈ p(A|X ∪ {D}). The upper limit 
1/p(A|X )  is achieved when ˆp(A|X ∪ {D}) → 1  while keeping ˆp(A|X ) ≈ p(A|X ).
According to [18]  the compromise effect is formally represented by

> p(C|{A  C}) and

> p(C|{B  C}). (23)

(cid:88)

p(C|{A  B  C})

p(X|{A  B  C})

(cid:88)

p(C|{A  B  C})

p(X|{A  B  C})

X∈{B C}

X∈{A C}
The MLM cannot represent the compromise effect  because the λMLM(X|Y) in (2) is independent
of Y  which in turn makes the inequalities in (23) equalities.
Motivated by (23)  we deﬁne the strength of the compromise effect as the magnitude of the change
in the conditional probability of selecting an item  C  given that either C or another item  A  is
selected when yet another item  B  is added into the choice set. More precisely  we also exchange
the roles of A and B  and study the minimum magnitude of those changes:
Deﬁnition 3. For a choice set  X   and items  A  B  C  such that A  B  C ∈ X   let

φA B C X ≡

where  for Y such that A  C ∈ Y  we deﬁne
qAC(C|Y) ≡

qAC(C|X )

qAC(C|X \ {B})

 

(cid:80)
X∈{A C} p(X|Y)

p(C|Y)

.

The strength of the compromise effect of A and B on C with X is then deﬁned as

A B C X ≡ min{φA B C X   φB A C X} .
ψ(com)

(24)

(25)

(26)

5

Here  we do not have the restriction that C is a “compromise” between A and B (see Figure 1 (c)).
In Appendix C:we prove the following theorem:
Theorem 3. Consider a choice set  X   and three items  A  B  C ∈ X . Consider the two RBM choice
models in Theorem 2. Let ˆψ(com)

A B C X be deﬁned analogously to (26) but with ˆp(·|·). Let

q ≡ max{qAC(C|X \ {B})  qBC(C|X \ {A})}
q ≡ min{qAC(C|X )  qBC(C|X )} .

Then  for any r ∈ (q  1/q) and ε > 0  we can choose T k·   U k· such that

(27)
(28)

(29)

r = ˆψ(com)

A B C X ;

ε >

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆλ(X|Y)

λ(X|Y)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   ∀Y  X s.t. X (cid:54)= C.

− 1

We expect that the range of r in the theorem covers the compromising effect in practice. Also 
this range is best possible in the sense analogous to what we have discussed with the range in
Theorem 2. Because the additional factor  (12) with k = ˆk  can only increase p(C|Y) for any Y
under the condition of the second part of (29)  it can only increase qXC(C|Y) for X ∈ {A  B}. The
lower limit  q  is achieved when qXC(C|X \ {X}) → 1  while keeping qXC(C|X ) approximately
unchanged  for X ∈ {A  B}. The upper limit  1/q  is achieved when qXC(C|X ) → 1  while
keeping qXC(C|X \ {X}) approximately unchanged  for X ∈ {A  B}.

4 Numerical experiments

We now validate the effectiveness of the RBM choice model in predicting the choices made by
humans. Here we use the dataset from [2]  which is based on the survey conducted in Switzerland 
where people are asked to choose a means of transportation from given options. A subset of the
dataset is used to train the RBM choice model  which is then used to predict the choice in the
remaining dataset. In Appendix B.2 we also conduct an experiment with artiﬁcial dataset and show
that the RBM choice model can indeed be trained to represent each of the typical choice phenomena.
This ﬂexibility in the representation is the basis of the predictive accuracy of the RBM choice model
to be presented in this section. All of our experiments are run on a single core of a Windows PC
with main memory of 8 GB and Core i5 CPU of 2.6 GHz.
The dataset [2] consists of 10 728 choices that 1 192 people have made from a varying choice set.
For those who own a car  the choice set has three items: a train  a maglev  and a car. For those who
do not own a car  the choice set consists of a train and a maglev. The train can operate at the interval
of 30  60  or 120 minutes. The maglev can operate at the interval of 10  20  or 30 minutes. The
trains (or maglevs) with different intervals are considered to be distinct items in our experiment.
Figure 3 (a) shows the empirical choice probability for each choice set. Each choice set consists of
a train with a particular interval (blue  shaded) and a maglev with a particular interval (red  mesh)
possibly with a car (yellow  circles). The interval of the maglev varies as is indicated at the bottom
of the ﬁgure. The interval of the train is indicated at the left side of the ﬁgure. For each combination
of the intervals of the train and the maglev  there are two choice sets  with or without a car.
We evaluate the accuracy of the RBM choice model in predicting the choice probability for an
arbitrary choice set  when the RBM choice model is trained with the data of the choice for the
remaining 17 choice sets (i.e.  we have 18 test cases). We train the RBM choice model (or the
MLM) by the use of discriminative training with stochastic gradient descent using the mini-batch
of size 50 and the learning rate of η = 0.1 (see Appendix A.1).Each run of the evaluation uses the
entire training dataset 50 times for training  and the evaluation is repeated ﬁve times by varying the
initial values of the parameters. The elements of T and U are initialized independently with samples
the number of items under consideration  and |K| is the number of hidden nodes. The elements of b
are initialized with samples from the uniform distribution on [−1  1].
Figure 3 (b) shows the Kullback-Leibler (KL) divergence between the predicted distribution of the
choice and the corresponding true distribution. The dots connected with a solid line show the the

from the uniform distribution on [−10/(cid:112)max(|I| |K|) −10/(cid:112)max(|I| |K|)]  where |I| = 7 is

6

(a) Dataset

(b) Error

(c) RBM

(d) MLM

Figure 3: Dataset (a)  the predictive error of the RBM choice model against the number of hidden
units (b)  and the choice probabilities learned by the RBM choice model (c) and the MLM (d).

average KL divergence over all of the 18 test cases and ﬁve runs with varying initialization. The
average KL divergence is also evaluated for training data and is shown with a dashed line. The
conﬁdence interval represents the corresponding standard deviation. The wide conﬁdence interval is
largely due to the variance between test instances (see Figure 4 in the appendix.The horizontal axis
shows the number of the hidden units in the RBM choice model  where zero hidden units correspond
to the MLM. The average KL divergence is reduced from 0.12 for the MLM to 0.02 for the RBM
choice model with 16 hidden units  an improvement by a factor of six.
Figure 3 (c)-(d) shows the choice probabilities given by (a) the RBM choice model with 16 hidden
units and (b) the MLM  after these models are trained for the test case where the choice set consists
of the train with 30-minute interval (Train30) and the maglev with 20-minute interval (Maglev20).
Observe that the RBM choice model gives the choice probabilities that are close to the true choice
probabilities shown in Figure 3 (a)  while the MLM has difﬁculty in ﬁtting these choice probabilities.
Taking a closer look at Figure 3 (a)  we can observe that the MLM is fundamentally incapable of
learning this dataset. For example  Train30 is more popular than Maglev20 for people who do not
own cars  while the preference is reversed for car owners (i.e.  the attraction effect). The attraction
effect can also be seen for the combination of Maglev30 and Train60. As we have discussed in
Section 3  the MLM cannot represent such attraction effects  but the RBM choice model can.

5 Related work

We now review the prior work related to our contributions. We will see that all of the existing
choice models either cannot represent at least one of the typical choice phenomena or do not have
systematic training algorithms. We will also see that the prior work has analyzed choice models
with respect to whether those choice models can represent typical choice phenomena or others but
only in speciﬁc cases of speciﬁc strength. On the contrary  our analysis shows that the RBM choice
model can represent the typical choice phenomena for all cases of the speciﬁed strength.
A majority of the prior work on the choice model is about the MLM and its variants such as the
hierarchical MLM [5]  the multinomial probit model [6]  and  generally  random utility models [17].

7

0.00.20.40.60.81.0Train300.00.20.40.60.81.0Train60Maglev100.00.20.40.60.81.0Train120Maglev20Maglev30Train30Train60Train120Maglev10Maglev20Maglev30Car0124816Number of hidden units0.000.050.100.150.200.250.30Average KL divergenceTrainingTest0.00.20.40.60.81.0Train300.00.20.40.60.81.0Train60Maglev100.00.20.40.60.81.0Train120Maglev20Maglev30Train30Train60Train120Maglev10Maglev20Maglev30Car0.00.20.40.60.81.0Train300.00.20.40.60.81.0Train60Maglev100.00.20.40.60.81.0Train120Maglev20Maglev30Train30Train60Train120Maglev10Maglev20Maglev30CarIn particular  the attraction effect cannot be represented by these variants of the MLM [13].
In
general  when the choice probability depends only on the values that are determined independently
for each item (e.g.  the models of [3  7])  none of the typical choice phenomena can be represented
[18]. Recently  Hruschka has proposed a choice model based on an RBM [9]  but his choice model
cannot represent any of the typical choice phenomena  because the corresponding choice rate is
independent of the choice set. It is thus nontrivial how we use the RBM as a choice model in such a
way that the typical choice phenomena can be represented. In [11]  a hierarchical Bayesian choice
model is shown to represent the attraction effect in a speciﬁc case.
There also exist choice models that have been numerically shown to represent all of the typical
choice phenomena for some speciﬁc cases. For example  sequential sampling models  including
the decision ﬁeld theory [4] and the leaky competing accumulator model [19]  are meant to directly
mimic the cognitive process of the human making a choice [12]. However  no paper has shown an
algorithm that can train a sequential sampling model in such a way that the trained model exhibits the
typical choice phenomena. Shenoy and Yu propose a hierarchical Bayesian model to represent the
three typical choice phenomena [16]. Although they perform inferences of the posterior distributions
that are needed to compute the choice probabilities with their model  they do not show how to train
their model to ﬁt the choice probabilities to given data. Their experiments show that their model
represents the typical choice phenomena in particular cases  where the parameters of the model are
set manually. Rieskamp et al. classify choice models according to whether a choice model can never
represent a certain phenomenon or can do so in some cases to some degree [13]. The phenomena
studied in [13] are not limited to the typical choice phenomena  but they list the typical choice
phenomena as the ones that are robust and signiﬁcant. Also  Otter et al. exclusively study all of the
typical choice phenomena [12].
Luce is a pioneer of the formal analysis of choice models  which however is largely qualitative [10].
For example  Lemma 3 of [10] can tell us whether a given choice model satisﬁes the IIA in (16)
for all cases or it violates the IIA for some cases to some degree. We address the new question of
to what degree a choice model can represent each of the typical choice phenomena (e.g.  to what
degree the RBM choice model can violate the IIA).
Finally  our theorems can be contrasted with the universal approximation theorem of RBMs  which
states that an arbitrary distribution can be approximated arbitrarily closely with a sufﬁcient number
of hidden units [15  8]. This is in contrast to our theorems  which show that a single hidden unit
sufﬁces to represent the typical choice phenomena of the strength that is speciﬁed in the theorems.

6 Conclusion

The RBM choice model is developed to represent the typical choice phenomena that have been
reported frequently in the literature of cognitive psychology and related areas. Our work motivates a
new direction of research on using RBMs to model such complex behavior of humans. Particularly
interesting behavior includes the one that is considered to be irrational or the one that results from
cognitive biases (see e.g. [1]). The advantages of the RBM choice model that are demonstrated in
this paper include their ﬂexibility in representing complex behavior and the availability of effective
training algorithms.
The RBM choice model can incorporate the attributes of the items in its parameters. Speciﬁcally 
one can represent the parameters of the RBM choice model as functions of uX  the attributes of
X ∈ I analogously to the MLM  where bX can be represented as bX = α· uX as we have discussed
after (2). The focus of this paper is in designing the fundamental structure of the RBM choice model
and analyzing its fundamental properties  and the study about the RBM choice model with attributes
will be reported elsewhere. Although the attributes are important for generalization of the RBM
model to unseen items  our experiments suggest that the RBM choice model  without attributes  can
learn the typical choice phenomena from a given choice set and generalize it to unseen choice sets.

Acknowledgements

A part of this research is supported by JST  CREST.

8

References
[1] D. Ariely. Predictably Irrational: The Hidden Forces That Shape Our Decisions. Harper

Perennial  revised and expanded edition  2010.

[2] M. Bierlaire  K. Axhausen  and G. Abay. The acceptance of modal innovation: The case of
Swissmetro. In Proceedings of the First Swiss Transportation Research Conference  March
2001.

[3] E. Bonilla  S. Guo  and S. Sanner. Gaussian process preference elicitation.

In J. Lafferty 
C. K. I. Williams  J. Shawe-Taylor  R. S. Zemel  and A. Culotta  editors  Advances in Neural
Information Processing Systems 23  pages 262–270. 2010.

[4] J. R. Busemeyer and J. T. Townsend. Decision ﬁeld theory: A dynamic cognition approach to

decision making. Psychological Review  100:432–459  1993.

[5] O. Chapelle and Z. Harchaoui. A machine learning approach to conjoint analysis. In L. K.
Saul  Y. Weiss  and L. Bottou  editors  Advances in Neural Information Processing Systems 17 
pages 257–264. 2005.

[6] B. Eric  N. de Freitas  and A. Ghosh. Active preference learning with discrete choice data.
In J. C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information
Processing Systems 20  pages 409–416. 2008.

[7] V. F. Farias  S. Jagabathula  and D. Shah. A nonparametric approach to modeling choice with

limited data. Management Science  59(2):305–322  2013.

[8] Y. Freund and D. Haussler. Unsupervised learning of distributions on binary vectors using two
layer networks. Technical Report UCSC-CRL-94-25  University of California  Santa Cruz 
June 1994.

[9] H. Hruschka. Analyzing market baskets by restricted Boltzmann machines. OR Spectrum 

pages 1–22  2012.

[10] R. D. Luce. Individual choice behavior: A theoretical analysis. John Wiley and Sons  New

York  NY  1959.

[11] T. Osogami and T. Katsuki. A hierarchical Bayesian choice model with visibility.

In Pro-
ceedings of the 22nd International Conference on Pattern Recognition (ICPR 2014)  pages
3618–3623  August 2014.

[12] T. Otter  J. Johnson  J. Rieskamp  G. M. Allenby  J. D. Brazell  A. Diederich  J. W. Hutchinson 
S. MacEachern  S. Ruan  and J. Townsend. Sequential sampling models of choice: Some recent
advances. Marketing Letters  19(3-4):255–267  2008.

[13] J. Rieskamp  J. R. Busemeyer  and B. A. Mellers. Extending the bounds of rationality: Evi-
dence and theories of preferential choice. Journal of Economic Literature  44:631–661  2006.
[14] R. M. Roe  J. R. Busemeyer  and J. T. Townsend. Multialternative decision ﬁeld theory: A
dynamic connectionist model of decision making. Psychological Review  108(2):370–392 
2001.

[15] N. L. Roux and Y. Bengio. Representational power of restricted Boltzmann machines and deep

belief networks. Neural Computation  20(6):1631–1649  2008.

[16] P. Shenoy and A. J. Yu. Rational preference shifts in multi-attribute choice: What is fair?
In Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci 2013)  pages
1300–1305  2013.

[17] K. Train. Discrete Choice Methods with Simulation. Cambridge University Press  second

edition  2009.

[18] A. Tversky and I. Simonson.

39(10):1179–1189  1993.

Context-dependent preferences. Management Science 

[19] M. Usher and J. L. McClelland. Loss aversion and inhibition in dynamical models of multial-

ternative choice. Psychological Review  111(3):757–769  2004.

9

,Marcus Rohrbach
Sandra Ebert
Bernt Schiele
Takayuki Osogami
Makoto Otsuka