2017,Learning to Pivot with Adversarial Networks,Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work   we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or  equivalently  fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.,Learning to Pivot with Adversarial Networks

Gilles Louppe

New York University
g.louppe@nyu.edu

Michael Kagan

SLAC National Accelerator Laboratory

makagan@slac.stanford.edu

Kyle Cranmer

New York University

kyle.cranmer@nyu.edu

Abstract

Several techniques for domain adaptation have been proposed to account for
differences in the distribution of the data used for training and testing. The majority
of this work focuses on a binary domain label. Similar problems occur in a scientiﬁc
context where there may be a continuous family of plausible data generation
processes associated to the presence of systematic uncertainties. Robust inference
is possible if it is based on a pivot – a quantity whose distribution does not depend
on the unknown values of the nuisance parameters that parametrize this family
of data generation processes. In this work  we introduce and derive theoretical
results for a training procedure based on adversarial networks for enforcing the
pivotal property (or  equivalently  fairness with respect to continuous attributes) on
a predictive model. The method includes a hyperparameter to control the trade-
off between accuracy and robustness. We demonstrate the effectiveness of this
approach with a toy example and examples from particle physics.

1

Introduction

Machine learning techniques have been used to enhance a number of scientiﬁc disciplines  and they
have the potential to transform even more of the scientiﬁc process. One of the challenges of applying
machine learning to scientiﬁc problems is the need to incorporate systematic uncertainties  which
affect both the robustness of inference and the metrics used to evaluate a particular analysis strategy.
In this work  we focus on supervised learning techniques where systematic uncertainties can be
associated to a data generation process that is not uniquely speciﬁed. In other words  the lack of
systematic uncertainties corresponds to the (rare) case that the process that generates training data is
unique  fully speciﬁed  and an accurate representative of the real world data. By contrast  a common
situation when systematic uncertainty is present is when the training data are not representative
of the real data. Several techniques for domain adaptation have been developed to create models
that are more robust to this binary type of uncertainty. A more generic situation is that there are
several plausible data generation processes  speciﬁed as a family parametrized by continuous nuisance
parameters  as is typically found in scientiﬁc domains. In this broader context  statisticians have for
long been working on robust inference techniques based on the concept of a pivot – a quantity whose
distribution is invariant with the nuisance parameters (see e.g.  (Degroot and Schervish  1975)).
Assuming a probability model p(X  Y  Z)  where X are the data  Y are the target labels  and Z are the
nuisance parameters  we consider the problem of learning a predictive model f (X) for Y conditional
on the observed values of X that is robust to uncertainty in the unknown value of Z. We introduce a
ﬂexible learning procedure based on adversarial networks (Goodfellow et al.  2014) for enforcing that
f (X) is a pivot with respect to Z. We derive theoretical results proving that the procedure converges
towards a model that is both optimal and statistically independent of the nuisance parameters (if
that model exists) or for which one can tune a trade-off between accuracy and robustness (e.g.  as
driven by a higher level objective). In particular  and to the best of our knowledge  our contribution is
the ﬁrst solution for imposing pivotal constraints on a predictive model  working regardless of the

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Classiﬁer f

Adversary r

Z

X

...

θf

f (X; θf )

Lf (θf )

...

θr

γ1(f (X; θf ); θr)

γ2(f (X; θf ); θr)

. . .

P(γ1  γ2  . . . )

pθr (Z|f (X; θf ))

Lr(θf   θr)

Figure 1: Architecture for the adversarial training of a binary classiﬁer f against a nuisance parameters Z. The
adversary r models the distribution p(z|f (X; θf ) = s) of the nuisance parameters as observed only through
the output f (X; θf ) of the classiﬁer. By maximizing the antagonistic objective Lr(θf   θr)  the classiﬁer f
forces p(z|f (X; θf ) = s) towards the prior p(z)  which happens when f (X; θf ) is independent of the nuisance
parameter Z and therefore pivotal.

type of the nuisance parameter (discrete or continuous) or of its prior. Finally  we demonstrate the
effectiveness of the approach with a toy example and examples from particle physics.

2 Problem statement
We begin with a family of data generation processes p(X  Y  Z)  where X ∈ X are the data  Y ∈ Y
are the target labels  and Z ∈ Z are the nuisance parameters that can be continuous or categorical. Let
us assume that prior to incorporating the effect of uncertainty in Z  our goal is to learn a regression
function f : X → S with parameters θf (e.g.  a neural network-based probabilistic classiﬁer) that
minimizes a loss Lf (θf ) (e.g.  the cross-entropy). In classiﬁcation  values s ∈ S = R|Y| correspond
to the classiﬁer scores used for mapping hard predictions y ∈ Y  while S = Y for regression.
We augment our initial objective so that inference based on f (X; θf ) will be robust to the value
z ∈ Z of the nuisance parameter Z – which remains unknown at test time. A formal way of enforcing
robustness is to require that the distribution of f (X; θf ) conditional on Z (and possibly Y ) be
invariant with the nuisance parameter Z. Thus  we wish to ﬁnd a function f such that

p(f (X; θf ) = s|z) = p(f (X; θf ) = s|z(cid:48))

(1)
for all z  z(cid:48) ∈ Z and all values s ∈ S of f (X; θf ). In words  we are looking for a predictive function
f which is a pivotal quantity with respect to the nuisance parameters. This implies that f (X; θf ) and
Z are independent random variables.
As stated in Eqn. 1  the pivotal quantity criterion is imposed with respect to p(X|Z) where Y is
marginalized out. In some situations however (see e.g.  Sec. 5.2)  class conditional independence of
f (X; θf ) on the nuisance Z is preferred  which can then be stated as requiring

for one or several speciﬁed values y ∈ Y.

p(f (X; θf ) = s|z  y) = p(f (X; θf ) = s|z(cid:48)  y)

(2)

3 Method

Joint training of adversarial networks was ﬁrst proposed by (Goodfellow et al.  2014) as a way to
build a generative model capable of producing samples from random noise z. More speciﬁcally  the
authors pit a generative model g : Rn → Rp against an adversarial classiﬁer d : Rp → [0  1] whose
antagonistic objective is to recognize real data X from generated data g(Z). Both models g and d are
trained simultaneously  in such a way that g learns to produce samples that are difﬁcult to identify by
d  while d incrementally adapts to changes in g. At the equilibrium  g models a distribution whose
samples can be identiﬁed by d only by chance. That is  assuming enough capacity in d and g  the
distribution of g(Z) eventually converges towards the real distribution of X.

2

Algorithm 1 Adversarial training of a classiﬁer f against an adversary r.
Inputs: training data {xi  yi  zi}N
1: for t = 1 to T do
2:
3:
4:

Sample minibatch {xm  zm  sm = f (xm; θf )}M
With θf ﬁxed  update r by ascending its stochastic gradient ∇θr E(θf   θr) :=

i=1; Outputs: ˆθf   ˆθr.

for k = 1 to K do

m=1 of size M;

M(cid:88)

m=1

∇θr

log pθr (zm|sm);

end for
Sample minibatch {xm  ym  zm  sm = f (xm; θf )}M

5:
6:
7: With θr ﬁxed  update f by descending its stochastic gradient ∇θf E(θf   θr) :=

m=1 of size M;

M(cid:88)

(cid:2)− log pθf (ym|xm) + log pθr (zm|sm)(cid:3)  

∇θf

where pθf (ym|xm) denotes 1(ym = 0)(1 − sm) + 1(ym = 1)sm;

8: end for

m=1

In this work  we repurpose adversarial networks as a means to constrain the predictive model f
in order to satisfy Eqn. 1. As illustrated in Fig. 1  we pit f against an adversarial model r :=
pθr (z|f (X; θf ) = s) with parameters θr and associated loss Lr(θf   θr). This model takes as input
realizations s of f (X; θf ) and produces as output a function modeling the posterior probability
density pθr (z|f (X; θf ) = s). Intuitively  if p(f (X; θf ) = s|z) varies with z  then the corresponding
correlation can be captured by r. By contrast  if p(f (X; θf ) = s|z) is invariant with z  as we require 
then r should perform poorly and be close to random guessing. Training f such that it additionally
minimizes the performance of r therefore acts as a regularization towards Eqn. 1.
If Z takes discrete values  then pθr can be represented as a probabilistic classiﬁer R → R|Z| whose
jth output (for j = 1  . . .  |Z|) is the estimated probability mass pθr (zj|f (X; θf ) = s). Similarly  if
Z takes continuous values  then we can model the posterior probability density p(z|f (X; θf ) = s)
with a sufﬁciently ﬂexible parametric family of distributions P(γ1  γ2  . . . )  where the parameters γj
depend on f (X  θf ) and θr. The adversary r may take any form  i.e. it does not need to be a neural
network  as long as it exposes a differentiable function pθr (z|f (X; θf ) = s) of sufﬁcient capacity
to represent the true distribution. Fig. 1 illustrates a concrete example where pθr (z|f (X; θf ) = s)
is a mixture of gaussians  as modeled with a mixture density network (Bishop  1994)). The jth
output corresponds to the estimated value of the corresponding parameter γj of that distribution (e.g. 
the mean  variance and mixing coefﬁcients of its components). The estimated probability density
pθr (z|f (X; θf ) = s) can then be evaluated for any z ∈ Z and any score s ∈ S.
As with generative adversarial networks  we propose to train f and r simultaneously  which we carry
out by considering the value function

(3)

(4)

E(θf   θr) = Lf (θf ) − Lr(θf   θr)

that we optimize by ﬁnding the minimax solution
ˆθf   ˆθr = arg min
θf

max

θr

E(θf   θr).

Without loss of generality  the adversarial training procedure to obtain (ˆθf   ˆθr) is formally presented
in Algorithm 1 in the case of a binary classiﬁer f : Rp → [0  1] modeling p(Y = 1|X). For reasons
further explained in Sec. 4  Lf and Lr are respectively set to the expected value of the negative
log-likelihood of Y |X under f and of Z|f (X; θf ) under r:

Lf (θf ) = Ex∼XEy∼Y |x[− log pθf (y|x)] 

(5)
(6)
The optimization algorithm consists in using stochastic gradient descent alternatively for solving
Eqn. 4. Finally  in the case of a class conditional pivot  the settings are the same  except that the
adversarial term Lr(θf   θr) is restricted to Y = y.

Lr(θf   θr) = Es∼f (X;θf )Ez∼Z|s[− log pθr (z|s)].

3

4 Theoretical results
In this section  we show that in the setting of Algorithm 1 where Lf and Lr are respectively set
to expected value of the negative log-likelihood of Y |X under f and of Z|f (X; θf ) under r  the
minimax solution of Eqn. 4 corresponds to a classiﬁer f which is a pivotal quantity.
In this setting  the nuisance parameter Z is considered as a random variable of prior p(Z)  and
our goal is to ﬁnd a function f (·; θf ) such that f (X; θf ) and Z are independent random variables.
Importantly  classiﬁcation of Y with respect to X is considered in the context where Z is marginalized
out  which means that the classiﬁer minimizing Lf is optimal with respect to Y |X  but not necessarily
with Y |X  Z. Results hold for a nuisance parameters Z taking either categorical or continuous values.
By abuse of notation  H(Z) denotes the differential entropy in this latter case. Finally  the proposition
below is derived in a non-parametric setting  by assuming that both f and r have enough capacity.
Proposition 1. If there exists a minimax solution (ˆθf   ˆθr) for Eqn. 4 such that E(ˆθf   ˆθr) =
H(Y |X) − H(Z)  then f (·; ˆθf ) is both an optimal classiﬁer and a pivotal quantity.

Proof. For ﬁxed θf   the adversary r is optimal at

ˆˆθr = arg max

θr

E(θf   θr) = arg min
θr

(7)
(z|f (X; θf ) = s) = p(z|f (X; θf ) = s) for all z and all s  and Lr reduces to the
in which case pˆˆθr
expected entropy Es∼f (X;θf )[H(Z|f (X; θf ) = s)] of the conditional distribution of the nuisance
parameters. This expectation corresponds to the conditional entropy of the random variables Z and
f (X; θf ) and can be written as H(Z|f (X; θf )). Accordingly  the value function E can be restated
as a function depending on θf only:

Lr(θf   θr) 

(8)

(9)

E(cid:48)(θf ) = Lf (θf ) − H(Z|f (X; θf )).

In particular  we have the lower bound

H(Y |X) − H(Z) ≤ Lf (θf ) − H(Z|f (X; θf ))

where the equality holds at ˆθf = arg minθf E(cid:48)(θf ) when:

• ˆθf minimizes the negative log-likelihood of Y |X under f  which happens when ˆθf are the
parameters of an optimal classiﬁer. In this case  Lf reduces to its minimum value H(Y |X).
• ˆθf maximizes the conditional entropy H(Z|f (X; θf ))  since H(Z|f (X; θ)) ≤ H(Z) from
the properties of entropy. Note that this latter inequality holds for both the discrete and the
differential deﬁnitions of entropy.

By assumption  the lower bound is active  thus we have H(Z|f (X; θf )) = H(Z) because of the
second condition  which happens exactly when Z and f (X; θf ) are independent variables. In other
words  the optimal classiﬁer f (·; ˆθf ) is also a pivotal quantity.

Proposition 1 suggests that if at each step of Algorithm 1 the adversary r is allowed to reach its
optimum given f (e.g.  by setting K sufﬁciently high) and if f is updated to improve Lf (θf ) −
H(Z|f (X; θf )) with sufﬁciently small steps  then f should converge to a classiﬁer that is both
optimal and pivotal  provided such a classiﬁer exists. Therefore  the adversarial term Lr can be
regarded as a way to select among the class of all optimal classiﬁers a function f that is also pivotal.
Despite the former theoretical characterization of the minimax solution of Eqn. 4  let us note that
formal guarantees of convergence towards that solution by Algorithm 1 in the case where a ﬁnite
number K of steps is taken for r remains to be proven.
In practice  the assumption of existence of an optimal and pivotal classiﬁer may not hold because the
nuisance parameter directly shapes the decision boundary. In this case  the lower bound

(10)
is strict: f can either be an optimal classiﬁer or a pivotal quantity  but not both simultaneously. In
this situation  it is natural to rewrite the value function E as

H(Y |X) − H(Z) < Lf (θf ) − H(Z|f (X; θf ))

Eλ(θf   θr) = Lf (θf ) − λLr(θf   θr) 

(11)

4

Figure 2: Toy example. (Left) Conditional probability densities of the decision scores at Z = −σ  0  σ without
adversarial training. The resulting densities are dependent on the continuous parameter Z  indicating that f is
not pivotal. (Middle left) The associated decision surface  highlighting the fact that samples are easier to classify
for values of Z above σ  hence explaining the dependency. (Middle right) Conditional probability densities of
the decision scores at Z = −σ  0  σ when f is built with adversarial training. The resulting densities are now
almost identical to each other  indicating only a small dependency on Z. (Right) The associated decision surface 
illustrating how adversarial training bends the decision function vertically to erase the dependency on Z.

where λ ≥ 0 is a hyper-parameter controlling the trade-off between the performance of f and its
independence with respect to the nuisance parameter. Setting λ to a large value will preferably
enforces f to be pivotal while setting λ close to 0 will rather constraint f to be optimal. When the
lower bound is strict  let us note however that there may exist distinct but equally good solutions θf   θr
minimizing Eqn. 11. In this zero-sum game  an increase in accuracy would exactly be compensated
by a decrease in pivotality and vice-versa. How to best navigate this Pareto frontier to maximize a
higher-level objective remains a question open for future works.
Interestingly  let us ﬁnally emphasize that our results hold using only the (1D) output s of f (·; θf ) as
input to the adversary. We could similarly enforce an intermediate representation of the data to be
pivotal  e.g. as in (Ganin and Lempitsky  2014)  but this is not necessary.

5 Experiments

In this section  we empirically demonstrate the effectiveness of the approach with a toy example
and examples from particle physics. Notably  there are no other other approaches to compare to in
the case of continuous nuisance parameters  as further explained in Sec. 6. In the case of binary
parameters  we do not expect results to be much different from previous works. The source code to
reproduce the experiments is available online 1.

5.1 A toy example with a continous nuisance parameter

As a guiding toy example  let us consider the binary classiﬁcation of 2D data drawn from multivariate
gaussians with equal priors  such that

(cid:18)
(cid:18)

x ∼ N

x|Z = z ∼ N

(cid:20) 1
(cid:20)1

−0.5

0

(0  0) 

(1  1 + z) 

(cid:21)(cid:19)

−0.5
1

(cid:21)(cid:19)

0
1

when Y = 0 

when Y = 1.

(12)

(13)

The continuous nuisance parameter Z here represents our uncertainty about the location of the mean
of the second gaussian. Our goal is to build a classiﬁer f (·; θf ) for predicting Y given X  but such
that the probability distribution of f (X; θf ) is invariant with respect to the nuisance parameter Z.
Assuming a gaussian prior z ∼ N (0  1)  we generate data {xi  yi  zi}N
i=1  from which we train a
neural network f minimizing Lf (θf ) without considering its adversary r. The network architecture
comprises 2 dense hidden layers of 20 nodes respectively with tanh and ReLU activations  followed
by a dense output layer with a single node with a sigmoid activation. As shown in Fig. 2  the resulting
classiﬁer is not pivotal  as the conditional probability densities of its decision scores f (X; θf ) show

1https://github.com/glouppe/paper-learning-to-pivot

5

0.00.20.40.60.81.0f(X)0.00.51.01.52.02.53.03.54.0p(f(X))p(f(X)|Z=−σ)p(f(X)|Z=0)p(f(X)|Z=+σ)1.00.50.00.51.01.52.01.00.50.00.51.01.52.02.53.0Z=−σZ=0Z=+σµ0µ1|Z=z0.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0f(X)0.00.51.01.52.02.53.03.54.0p(f(X))p(f(X)|Z=−σ)p(f(X)|Z=0)p(f(X)|Z=+σ)1.00.50.00.51.01.52.01.00.50.00.51.01.52.02.53.0Z=−σZ=0Z=σµ0µ1|Z=z0.120.240.360.480.600.720.84Figure 3: Toy example. Training curves for Lf (θf ) 
Lr(θf   θr) and Lf (θf ) − λLr(θf   θr). Initialized
with a pre-trained classiﬁer f  adversarial training
was performed for 200 iterations  mini-batches of
size M = 128  K = 500 and λ = 50.

Figure 4: Physics example. Approximate median
signiﬁcance as a function of the decision threshold
on the output of f. At λ = 10  trading accuracy
for independence to pileup results in a net beneﬁt in
terms of statistical signiﬁcance.

large discrepancies between values z of the nuisance parameters. While not shown here  a classiﬁer
trained only from data generated at the nominal value Z = 0 would also not be pivotal.
Let us now consider the joint training of f against an adversary r implemented as a mixture density
network modeling Z|f (X; θf ) as a mixture of ﬁve gaussians. The network architecture of r comprises
2 dense hidden layers of 20 nodes with ReLU activations  followed by an output layer of 15 nodes
corresponding to the means  standard deviations and mixture coefﬁcients of the gaussians. Output
nodes for the mean values come with linear activations  output nodes for the standard deviations
with exponential activations to ensure positivity  while output nodes for the mixture coefﬁcients
implement the softmax function to ensure positivity and normalization. When running Algorithm 1
as initialized with the classiﬁer f obtained previously  adversarial training effectively reshapes the
decision function so it that becomes almost independent on the nuisance parameter  as shown in
Fig. 2. The conditional probability densities of the decision scores f (X; θf ) are now very similar to
each other  indicating only a residual dependency on the nuisance  as theoretically expected. The
dynamics of adversarial training is illustrated in Fig. 3  where the losses Lf   Lr and Lf − λLr are
evaluated after each iteration. In the ﬁrst iterations  we observe that the global objective Lf − λLr
is minimized by making the classiﬁer less accurate  hence the corresponding increase of Lf   but
which results in a classiﬁer that is more pivotal  hence the associated increase of Lr and the total
net beneﬁt. As learning goes  minimizing E requires making predictions that are more accurate 
hence decreasing Lf   or that are even less dependent on Z  hence shaping pθr towards the prior p(Z).
Indeed  Lf eventually starts decreasing  while remaining bounded from below by minθf Lf (θf ) as
approximated by the dashed line in the ﬁrst plot. Similarly  Lr tends towards the differential entropy
H(Z) of the prior (where H(Z) = log(σ
2πe) = 1.419 in the case of a standard normal)  as shown
by the dashed line in the second plot. Finally  let us note that the ideal situation of a classiﬁer that
is both optimal and pivotal is unreachable for this problem  as shown in the third plot by the offset
between Lf − λLr and the dashed line approximating H(Y |X) − λH(Z).

√

5.2 High energy physics examples

Binary Case Experiments at high energy colliders like the LHC (Evans and Bryant  2008) are
searching for evidence of new particles beyond those described by the Standard Model (SM) of
particle physics. A wide array of theories predict the existence of new massive particles that would
decay to known particles in the SM such as the W boson. The W boson is unstable and can decay to
two quarks  each of which produce collimated sprays of particles known as jets. If the exotic particle is
heavy  then the W boson will be moving very fast  and relativistic effects will cause the two jets from
its decay to merge into a single ‘W -jet’. These W -jets have a rich internal substructure. However 
jets are also produced ubiquitously at high energy colliders through more mundane processes in the

6

0.450.500.550.600.650.70Lf1.361.371.381.391.401.411.42Lr050100150200T70.570.069.569.068.568.067.5Lf−λLr0.00.20.40.60.81.0threshold on f(X)1012345678AMSλ=0|Z=0λ=0λ=1λ=10λ=500SM  which leads to a challenging classiﬁcation problem that is beset with a number of sources of
systematic uncertainty. The classiﬁcation challenge used here is common in jet substructure studies
(see e.g. (CMS Collaboration  2014; ATLAS Collaboration  2015  2014)): we aim to distinguish
normal jets produced copiously at the LHC (Y = 0) and from W -jets (Y = 1) potentially coming
from an exotic process. We reuse the datasets used in (Baldi et al.  2016a).
Challenging in its own right  this classiﬁcation problem is made all the more difﬁcult by the presence
of pileup  or multiple proton-proton interactions occurring simultaneously with the primary interaction.
These pileup interactions produce additional particles that can contribute signiﬁcant energies to jets
unrelated to the underlying discriminating information. The number of pileup interactions can vary
with the running conditions of the collider  and we want the classiﬁer to be robust to these conditions.
Taking some liberty  we consider an extreme case with a categorical nuisance parameter  where
Z = 0 corresponds to events without pileup and Z = 1 corresponds to events with pileup  for which
there are an average of 50 independent pileup interactions overlaid.
We do not expect that we will be able to ﬁnd a function f that simultaneously minimizes the
classiﬁcation loss Lf and is pivotal. Thus  we need to optimize the hyper-parameter λ of Eqn. 11 with
respect to a higher-level objective. In this case  the natural higher-level context is a hypothesis test of
a null hypothesis with no Y = 1 events against an alternate hypothesis that is a mixture of Y = 0 and
Y = 1 events. In the absence of systematic uncertainties  optimizing Lf simultaneously optimizes
the power of a classical hypothesis test in the Neyman-Pearson sense. When we include systematic
uncertainties we need to balance the classiﬁcation performance against the robustness to uncertainty
in Z. Since we are still performing a hypothesis test against the null  we only wish to impose the
pivotal property on Y = 0 events. To this end  we use as a higher level objective the Approximate
Median Signiﬁcance (AMS)  which is a natural generalization of the power of a hypothesis test when
systematic uncertainties are taken into account (see Eqn. 20 of Adam-Bourdarios et al. (2014)).
For several values of λ  we train a classiﬁer using Algorithm 1 but consider the adversarial term
Lr conditioned on Y = 0 only  as outlined in Sec. 2. The architecture of f comprises 3 hidden
layers of 64 nodes respectively with tanh  ReLU and ReLU activations  and is terminated by a single
ﬁnal output node with a sigmoid activation. The architecture of r is the same  but uses only ReLU
activations in its hidden nodes. As in the previous example  adversarial training is initialized with
f pre-trained. Experiments are performed on a subset of 150000 samples for training while AMS
is evaluated on an independent test set of 5000000 samples. Both training and testing samples are
weighted such that the null hypothesis corresponded to 1000 of Y = 0 events and the alternate
hypothesis included an additional 100 Y = 1 events prior to any thresholding on f. This allows us
to probe the efﬁcacy of the method proposed here in a representative background-dominated high
energy physics environment. Results reported below are averages over 5 runs.
As Fig. 4 illustrates  without adversarial training (at λ = 0|Z = 0 when building a classiﬁer at the
nominal value Z = 0 only  or at λ = 0 when building a classiﬁer on data sampled from p(X  Y  Z)) 
the AMS peaks at 7. By contrast  as the pivotal constraint is made stronger (for λ > 0) the AMS
peak moves higher  with a maximum value around 7.8 for λ = 10. Trading classiﬁcation accuracy
for robustness to pileup thereby results in a net beneﬁt in terms of the power of the hypothesis
test. Setting λ too high however (e.g. λ = 500) results in a decrease of the maximum AMS  by
focusing the capacity of f too strongly on independence with Z  at the expense of accuracy. In effect 
optimizing λ yields a principled and effective approach to control the trade-off between accuracy and
robustness that ultimately maximizes the power of the enveloping hypothesis test.

Continous Case Recently  an independent group has used our approach to learn jet classiﬁers that
are independent of the jet mass (Shimmin et al.  2017)  which is a continuous attribute. The results of
their studies show that the adversarial training strategy works very well for real-world problems with
continuous attributes  thus enhancing the sensitivity of searches for new physics at the LHC.

6 Related work

Learning to pivot can be related to the problem of domain adaptation (Blitzer et al.  2006; Pan et al. 
2011; Gopalan et al.  2011; Gong et al.  2013; Baktashmotlagh et al.  2013; Ajakan et al.  2014;
Ganin and Lempitsky  2014)  where the goal is often stated as trying to learn a domain-invariant
representation of the data. Likewise  our method also relates to the problem of enforcing fairness

7

in classiﬁcation (Kamishima et al.  2012; Zemel et al.  2013; Feldman et al.  2015; Edwards and
Storkey  2015; Zafar et al.  2015; Louizos et al.  2015)  which is stated as learning a classiﬁer that is
independent of some chosen attribute such as gender  color or age. For both families of methods  the
problem can equivalently be stated as learning a classiﬁer which is a pivotal quantity with respect
to either the domain or the selected feature. As an example  unsupervised domain adaptation with
labeled data from a source domain and unlabeled data from a target domain can be recast as learning
a predictive model f (i.e.  trained to minimize Lf evaluated on labeled source data only) that is also a
pivot with respect to the domain Z (i.e.  trained to maximize Lr evaluated on both source and target
data). In this context  (Ganin and Lempitsky  2014; Edwards and Storkey  2015) are certainly among
the closest to our work  in which domain invariance and fairness are enforced through an adversarial
minimax setup composed of a classiﬁer and an adversarial discriminator. Following this line of work 
our method can be regarded as a uniﬁed generalization that also supports a continuously parametrized
family of domains or as enforcing fairness over continuous attributes.
Most related work is based on the strong and limiting assumption that Z is a binary random variable
(e.g.  Z = 0 for the source domain  and Z = 1 for the target domain). In particular  (Pan et al.  2011;
Gong et al.  2013; Baktashmotlagh et al.  2013; Zemel et al.  2013; Ganin and Lempitsky  2014;
Ajakan et al.  2014; Edwards and Storkey  2015; Louizos et al.  2015) are all based on the minimization
of some form of divergence between the two distributions of f (X)|Z = 0 and f (X)|Z = 1. For this
reason  these works cannot directly be generalized to non-binary or continuous nuisance parameters 
both from a practical and theoretical point of view. Notably  Kamishima et al. (2012) enforces
fairness through a prejudice regularization term based on empirical estimates of p(f (X)|Z). While
this approach is in principle sufﬁcient for handling non-binary nuisance parameters Z  it requires
accurate empirical estimates of p(f (X)|Z = z) for all values z  which quickly becomes impractical
as the cardinality of Z increases. By contrast  our approach models the conditional dependence
through an adversarial network  which allows for generalization without necessarily requiring an
exponentially growing number of training examples.
A common approach to account for systematic uncertainties in a scientiﬁc context (e.g. in high energy
physics) is to take as ﬁxed a classiﬁer f built from training data for a nominal value z0 of the nuisance
parameter  and then propagate uncertainty by estimating p(f (x)|z) with a parametrized calibration
procedure. Clearly  this classiﬁer is however not optimal for z (cid:54)= z0. To overcome this issue  the
classiﬁer f is sometimes built instead on a mixture of training data generated from several plausible
values z0  z1  . . . of the nuisance parameter. While this certainly improves classiﬁcation performance
with respect to the marginal model p(X  Y )  there is no reason to expect the resulting classiﬁer to
be pivotal  as shown previously in Sec. 5.1. As an alternative  parametrized classiﬁers (Cranmer
et al.  2015; Baldi et al.  2016b) directly take (nuisance) parameters as additional input variables 
hence ultimately providing the most statistically powerful approach for incorporating the effect
of systematics on the underlying classiﬁcation task. In practice  parametrized classiﬁers are also
computationally expensive to build and evaluate. In particular  calibrating their decision function 
i.e. approximating p(f (x  z)|y  z) as a continuous function of z  remains an open challenge. By
contrast  constraining f to be pivotal yields a classiﬁer that can be directly used in a wider range of
applications  since the dependence on the nuisance parameter Z has already been eliminated.

7 Conclusions

In this work  we proposed a ﬂexible learning procedure for building a predictive model that is
independent of continuous or categorical nuisance parameters by jointly training two neural networks
in an adversarial fashion. From a theoretical perspective  we motivated the proposed algorithm by
showing that the minimax value of its value function corresponds to a predictive model that is both
optimal and pivotal (if that models exists) or for which one can tune the trade-off between power and
robustness. From an empirical point of view  we conﬁrmed the effectiveness of our method on a toy
example and a particle physics example.
In terms of applications  our solution can be used in any situation where the training data may not be
representative of the real data the predictive model will be applied to in practice. In the scientiﬁc
context  the presence of systematic uncertainty can be incorporated by considering a family of data
generation processes  and it would be worth revisiting those scientiﬁc problems that utilize machine
learning in light of this technique. The approach also extends to cases where independence of the
predictive model with respect to observed random variables is desired  as in fairness for classiﬁcation.

8

Acknowledgements

We would like to thank the authors of (Baldi et al.  2016a) for sharing the data used in their studies.
KC and GL are both supported through NSF ACI-1450310  additionally KC is supported through
PHY-1505463 and PHY-1205376. MK is supported by the US Department of Energy (DOE) under
grant DE-AC02-76SF00515 and by the SLAC Panofsky Fellowship.

References
Adam-Bourdarios  C.  Cowan  G.  Germain  C.  Guyon  I.  Kégl  B.  and Rousseau  D. (2014). The
higgs boson machine learning challenge. In NIPS 2014 Workshop on High-energy Physics and
Machine Learning  volume 42  page 37.

Ajakan  H.  Germain  P.  Larochelle  H.  Laviolette  F.  and Marchand  M. (2014). Domain-adversarial

neural networks. arXiv preprint arXiv:1412.4446.

ATLAS Collaboration (2014). Performance of Boosted W Boson Identiﬁcation with the ATLAS

Detector. Technical Report ATL-PHYS-PUB-2014-004  CERN  Geneva.
√
CERN  Geneva.

ATLAS Collaboration (2015). Identiﬁcation of boosted  hadronically-decaying W and Z bosons in
s = 13 TeV Monte Carlo Simulations for ATLAS. Technical Report ATL-PHYS-PUB-2015-033 

Baktashmotlagh  M.  Harandi  M.  Lovell  B.  and Salzmann  M. (2013). Unsupervised domain
adaptation by domain invariant projection. In Proceedings of the IEEE International Conference
on Computer Vision  pages 769–776.

Baldi  P.  Bauer  K.  Eng  C.  Sadowski  P.  and Whiteson  D. (2016a). Jet substructure classiﬁcation

in high-energy physics with deep neural networks. Physical Review D  93(9):094034.

Baldi  P.  Cranmer  K.  Faucett  T.  Sadowski  P.  and Whiteson  D. (2016b). Parameterized neural

networks for high-energy physics. Eur. Phys. J.  C76(5):235.

Bishop  C. M. (1994). Mixture density networks.

Blitzer  J.  McDonald  R.  and Pereira  F. (2006). Domain adaptation with structural correspondence
In Proceedings of the 2006 conference on empirical methods in natural language

learning.
processing  pages 120–128. Association for Computational Linguistics.

CMS Collaboration (2014). Identiﬁcation techniques for highly boosted W bosons that decay into

hadrons. JHEP  12:017.

Cranmer  K.  Pavez  J.  and Louppe  G. (2015). Approximating likelihood ratios with calibrated

discriminative classiﬁers. arXiv preprint arXiv:1506.02169.

Degroot  M. H. and Schervish  M. J. (1975). Probability and statistics. 1st edition.

Edwards  H. and Storkey  A. J. (2015). Censoring representations with an adversary. arXiv preprint

arXiv:1511.05897.

Evans  L. and Bryant  P. (2008). LHC Machine. JINST  3:S08001.

Feldman  M.  Friedler  S. A.  Moeller  J.  Scheidegger  C.  and Venkatasubramanian  S. (2015).
Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  pages 259–268. ACM.

Ganin  Y. and Lempitsky  V. (2014). Unsupervised Domain Adaptation by Backpropagation. arXiv

preprint arXiv:1409.7495.

Gong  B.  Grauman  K.  and Sha  F. (2013). Connecting the dots with landmarks: Discriminatively
learning domain-invariant features for unsupervised domain adaptation. In Proceedings of The
30th International Conference on Machine Learning  pages 222–230.

9

Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A.  and
Bengio  Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing
Systems  pages 2672–2680.

Gopalan  R.  Li  R.  and Chellappa  R. (2011). Domain adaptation for object recognition: An
unsupervised approach. In Computer Vision (ICCV)  2011 IEEE International Conference on 
pages 999–1006. IEEE.

Kamishima  T.  Akaho  S.  Asoh  H.  and Sakuma  J. (2012). Fairness-aware classiﬁer with prejudice

remover regularizer. Machine Learning and Knowledge Discovery in Databases  pages 35–50.

Louizos  C.  Swersky  K.  Li  Y.  Welling  M.  and Zemel  R. (2015). The variational fair autoencoder.

arXiv preprint arXiv:1511.00830.

Pan  S. J.  Tsang  I. W.  Kwok  J. T.  and Yang  Q. (2011). Domain adaptation via transfer component

analysis. Neural Networks  IEEE Transactions on  22(2):199–210.

Shimmin  C.  Sadowski  P.  Baldi  P.  Weik  E.  Whiteson  D.  Goul  E.  and Søgaard  A. (2017).

Decorrelated Jet Substructure Tagging using Adversarial Neural Networks.

Zafar  M. B.  Valera  I.  Rodriguez  M. G.  and Gummadi  K. P. (2015). Fairness constraints: A

mechanism for fair classiﬁcation. arXiv preprint arXiv:1507.05259.

Zemel  R. S.  Wu  Y.  Swersky  K.  Pitassi  T.  and Dwork  C. (2013). Learning fair representations.

ICML (3)  28:325–333.

10

,Gilles Louppe
Michael Kagan
Kyle Cranmer
Sven Bambach
David Crandall
Linda Smith
Chen Yu