2018,Overlapping Clustering Models  and One (class) SVM to Bind Them All,People belong to multiple communities  words belong to multiple topics  and books cover multiple genres; overlapping clusters are commonplace. Many existing overlapping clustering methods model each person (or word  or book) as a non-negative weighted combination of "exemplars" who belong solely to one community  with some small noise. Geometrically  each person is a point on a cone whose corners are these exemplars. This basic form encompasses the widely used Mixed Membership Stochastic Blockmodel of networks and its degree-corrected variants  as well as topic models such as LDA. We show that a simple one-class SVM yields provably consistent parameter inference for all such models  and scales to large datasets. Experimental results on several simulated and real datasets show our algorithm (called SVM-cone) is both accurate and scalable.,Overlapping Clustering Models  and One (class) SVM

to Bind Them All

Xueyu Mao  Purnamrita Sarkar  Deepayan Chakrabarti

The University of Texas at Austin

xmao@cs.utexas.edu  purna.sarkar@austin.utexas.edu  deepay@utexas.edu

Abstract

People belong to multiple communities  words belong to multiple topics  and books
cover multiple genres; overlapping clusters are commonplace. Many existing over-
lapping clustering methods model each person (or word  or book) as a non-negative
weighted combination of “exemplars” who belong solely to one community  with
some small noise. Geometrically  each person is a point on a cone whose corners
are these exemplars. This basic form encompasses the widely used Mixed Member-
ship Stochastic Blockmodel of networks [1] and its degree-corrected variants [16] 
as well as topic models such as LDA [9]. We show that a simple one-class SVM
yields provably consistent parameter inference for all such models  and scales to
large datasets. Experimental results on several simulated and real datasets show
our algorithm (called SVM-cone) is both accurate and scalable.

1

Introduction

Clustering has many real-world applications: market segmentation  product recommendation  doc-
ument clustering  ﬁnding protein complexes in gene networks  among others. The simplest form
of a clustering model assumes that every record or entity belongs to exactly one cluster. More
general forms allow for overlapping clusters  where each entity may belong to different clusters or
communities to different degrees. For example  George Orwell’s 1984 belongs to both the dystopian
ﬁction and political ﬁction genres  and Pink Floyd’s music is both progressive and psychedelic. In
this paper  we show that many existing overlapping clustering models can be written in a general
form  whose parameters can then be inferred using a one-class SVM.
In many clustering problems  overlapping or otherwise  we have access to a data matrix ˆZ ∈ Rn×m 
which is a noisy version of an ideal matrix Z  i.e. ˆZ = Z + R where the norm of the rows of
R is small. Also  Z = GZP   where ZP are ideal “exemplars” of the various communities  and
G ∈ Rn×K≥0
Consider the Stochastic Blockmodel (SBM) [13] for networks. In this model  each node belongs to
one of K communities  and the probability Pij of an edge between nodes i and j is a function of their
respective communities. Recent results [21] show that the eigenvectors ˆV of the adjacency matrix
concentrate row-wise around the eigenvectors V of P. The matrix V is also blockwise constant 
mapping all nodes in one cluster to one point [27]. Hence  ˆV = GVP + R  where G ∈ {0  1}n×K
is a binary membership matrix where each row sums to one. The Mixed Membership Stochastic
Blockmodel (MMSB) [1] relaxes this by allowing the entries of G to be in [0  1]. Since the rows of G
sum to one  the ideal matrix Z arranges points in a simplex. The corners of this simplex represent the
“pure” nodes  i.e. nodes belonging to exactly one community. Most algorithms ﬁrst ﬁnd the corners 
and then estimate model parameters via regression [20  21  16  23  28]. Other notable methods
include tensor based approaches [14  2]  Bayesian inference [12]  etc. Related models and inference
methods for overlapping networks have been presented in [24  17  26  19]  etc.

gives the community memberships of each entity. We will now give some examples.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a) Simplex methods can fail

(b) Normalized points

(c) Supporting hyperplane

Figure 1: (a) Simplex-based corner-ﬁnding methods require points on a simplex  with uniformly
small errors. Projecting points to a simplex with normal vector q works well  but a very similar q0
does not. Some points (such as A(cid:13)) get projected to far-off points  amplifying errors in their positions.
(b) Instead  we normalize points to the unit sphere  and (c) ﬁnd corners from the support vectors of a
one-class SVM.

The MMSB model does not allow for degree heterogeneity  which can be achieved via the Degree-
corrected Mixed Membership Stochastic Blockmodel (DCMMSB) [16]. In DCMMSB  each node
has an extra degree parameter  with a high parameter value leading to more edges for that node. Now 
G is non-negative  but its rows do not sum to one. Thus the points lie inside a cone  and the pure
nodes lie on the corner rays of this cone. Other network models also give rise to such cones [32  17].
Existing algorithms for degree corrected overlapping models use a range of different techniques.
OCCAM [32] uses a k-medians step on the regularized eigenvectors of the adjacency matrix to get
the corners. While the algorithm is computationally efﬁcient  a key assumption is that the k-medians
loss function attains its minimum at the locations of the pure nodes and there is a curvature around
this minimum. This condition is typically hard to check. In [16]  the authors show an interesting
result that the second to K eigenvectors  element-wise divided by the ﬁrst eigenvector entries form
a simplex. The authors provide an algorithm for ﬁnding this simplex with K corners in K − 1
dimensions. The algorithm requires a combinatorial search step  which is prohibitive for large K.
Topic models [9] are another example of overlapping clustering models. Here the documents can be
generated from a mixture of topics  which are the analog of communities in networks. The normalized
word co-occurrence matrix forms a simplex structure  with the corners representing anchor words  i.e.
words that belong to exactly one topic. While there are many existing inference methods  the ones
that provide consistency guarantees are typically based on analyzing tensors or ﬁnding corners in
simplexes [4  18  11  3  15  7  6  8].
In this paper  we provide an overarching framework which incorporates all the above problems  from
Mixed membership models (with or without degree correction) to topic models. As discussed before 
in all the above models  the ideal data matrix lies inside a cone (a simplex is a special type of a cone).
The goal is to infer G  which depends on ﬁnding the correct corner rays.
Let us illustrate why seemingly obvious methods fail to obtain the corner rays. The simplest idea
would be to generate a random plane (the “simplex”)  and project points to the intersection of the line
joining these points to this random plane as shown in Figure 1a. Corners of this simplex correspond
to the corner rays. However  extending the idea to the sample or empirical cone is difﬁcult  because if
the simplex is not good  some points can get projected to arbitrarily far points  which will amplify
their error. As Figure 1a shows  the set of good simplexes may be quite limited  and ﬁnding a good
simplex is difﬁcult.
We will illustrate our idea with the “ideal cone.” First  we row-normalize the ideal data matrix to have
unit ‘2 norm (similar to Ng et al. [22]  Qin and Rohe [25]). This projects all points inside the cone to
the surface of the sphere  with the points on the corner rays being projected to the corners (Figure 1b).
Then  we show a rather fascinating result  namely  for all the above models  the corners can be
obtained via the support vectors of a one class SVM [29]  where all normalized points are in the
positive class  and the origin is in the negative class (Figure 1c). Observe that a hyperplane through

2

the corners separates all the points from the origin. We also show that if the row-wise error of R is
small  the SVM approach can be used to infer G from empirical cones. Finally  we show that since
the row-wise error of R is indeed small for different degree-corrected overlapping network models
and topic models  we can use our algorithm to infer the parameters consistently. We provide error
bounds for parameter estimates at the per-node and per-word level  in contrast to typical bounds for
the entire parameter matrix. We conclude with experimental results on simulated and real datasets.

>0 being a positive
a cross-community

a community-membership matrix  and B ∈ RK×K≥0

P VP   where VP = V(I  :) is full rank and ΓP = Γ(I I).

2 Proposed work
Consider a population matrix P of the form P = ρΓΘBΘT Γ  with Γ ∈ Rn×n
diagonal matrix  Θ ∈ Rn×K≥0
connection matrix. We will make the following assumptions which are common in the literature:
Assumption 2.1. (a) Pure nodes: Each community has at least one “pure” node  which belongs
solely to that community. (b) Non-zero rows: No row of Θ is identically 0. (c) B is full rank.
The form of the population matrix P  alongside Assumption 2.1  induces a conic structure on the
rows of the eigenvectors of P.
Lemma 2.1. Let there be K communities (rank(P) = K)  and let I be indices of K pure nodes  one
from each community. Let P = VEVT be the top-K eigen-decomposition of P  where columns of
V ∈ Rn×K are the K principal eigenvectors and E ∈ RK×K is a diagonal matrix of the K principal
eigenvalues. Then  V = ΓΘΓ−1
P )ij ≥ 0 for all (i  j)  the rows of V fall within a cone with corners VP . This suggests
Since (ΓΘΓ−1
the following idealized problem:
Problem 1 (Ideal cone problem). We are given a matrix Z ∈ Rn×m such that Z = MYP   where
M ∈ Rn×K≥0
  no row of M is 0  and YP ∈ RK×m corresponds to K (unknown) rows of Z  each
scaled to unit ‘2 norm. Infer M from Z.
The rows of YP are unit vectors representing the corner rays of the cone. Each row of Z is constructed
from a non-negative weighted combination of these unit vectors  with the weights being given by the
corresponding rows of M. Rows of Z that lie on some corner correspond to rows of M that have
zero in all but one component. Observe that M is invariant to the choice of K corner rows of Z used
to construct YP .
Now consider solving the ideal cone problem with the eigenvector matrix  i.e.  Z = V. From
Lemma 2.1  the corner rows correspond to the pure nodes. Choosing one such row from each corner
gives us a set of pure node indices I. Hence  M = ΓΘΓ−1
P   where N is a diagonal matrix with
Nii = 1/keT
i Zk and NP = N(I  :). We also have the identity ρΓP BΓP = VP EVT
P . Coupled
with model-speciﬁc identiﬁability conditions (details are provided in the supplementary material) 
these can be used to infer Θ and ρB (Γ are typically considered nuisance parameters).
In practice  we only have an observation matrix A that is stochastically generated from the population
matrix P. Hence  we must actually solve:
Problem 2 (Empirical cone problem). We are given a matrix ˆZ ∈ Rn×m such that
maxi∈[n] keT
i (Y − ˆY)k2 ≤   where Y = NZ is the row-normalized version of Z  and ˆY is con-
structed similarly from ˆZ. Again  Z = MYP   where M ≥ 0  no row of M is 0  and YP = Y(I  :)
corresponds to K (unknown) rows of Y with indices I. Infer M from ˆZ.
We will ﬁrst present the solution to the ideal cone problem. We will then show that the same algorithm
with some post-processing solves the empirical cone problem up to O() error. Finally  we apply our
algorithm to infer parameters for a variety of models  and present error bounds for each.
Notation: We shall refer to the ith row of Z as zT
The same pattern will be used for rows mT

P N−1

i expressed using a column vector  i.e.  zi = ZT ei.

i of M  and other matrices as well.

2.1 The Ideal Cone Problem
Observe that given the corner indices I (i.e.  given YP )  ﬁnding M such that Z = MYP is a simple
regression problem. Thus  the only difﬁculty is in ﬁnding the corner indices.

3

Our key insight is that under certain conditions  the ideal cone problem can be solved easily by
a one-class SVM applied to the rows of Y. Figure 1 plots the normalized rows yi of Y for an
example cone. Observe that a hyperplane through the corners separates all the points from the origin.
This suggests that the normalized corners are the support vectors found by a one-class SVM:

maximize b

s.t. wT yi ≥ b (for i = 1  . . .   n) and kwk2 ≤ 1.

(1)

P )−11 > 0.

We show next that this intuition is correct. Deﬁne the following condition.
Condition 1. The matrix YP satisﬁes (YP YT
Theorem 2.2. Each support vector selected by the one-class SVM (Eq. 1) is a corner of Z. Also  if
Condition 1 holds  there is at least one support vector for every corner.
Thus  under Condition 1  we can get all the corners from the support vectors  and then ﬁnd M via
regression of Z on YP . Condition 1 is always satisﬁed for our problem setting  as shown next.
Theorem 2.3. Let P be a population matrix satisfying Assumption 2.1. Let Z = V  where V is the
rank-K eigenvector matrix. Let Y = NZ as deﬁned above. Then  Condition 1 is true.

Thus  the ideal cone problem is easily solved by a one-class SVM. Next  we show that the same
method sufﬁces for the empirical cone problem too.

2.2 The Empirical Cone Problem
Now  instead of the normalized eigenvector rows Y  we are given the empirical matrix ˆY with rows
i ( ˆY − Y)k ≤ . Once again  we focus on ﬁnding the corner indices  using
i /kˆzik  where maxi keT
ˆzT
which M can be inferred by regression. We will show that running a one-class SVM on the rows of
ˆY yields “near-corners ” after some post-processing. We will need a stronger form of Condition 1:
Condition 2. The matrix YP satisﬁes (YP YT
It is easy to show that the solution (w  b) of the population SVM under Condition 1 is given by

P )−11 ≥ η1 for some constant η > 0.

w = b−1 · YT
P β

b =(cid:0)1T (YP YT

P )−11(cid:1)−1/2

β = (YP YT
1T (YP YT

P )−11
P )−11 .

(2)

Thus  Condition 1 implies that w is a convex combination of the corners  while Condition 2 addition-
ally requires a minimum contribution from each corner.
Lemma 2.4 (SVM solution is nearly ideal). Let ( ˆw ˆb) be the solution for the one-class SVM (Eq. 1)
applied to the rows of ˆY. Under Condition 2  we have |ˆb − b| ≤  and k ˆw − wk ≤ ζ  for
ζ =

√

≤

4K

4

ηb2

λK(YP YT
P )

η(λK(YP YT

P ))1.5 .

Unlike the ideal cone scenario  the rows ˆYP corresponding to the corners need not be support vectors
for the empirical cone. However  they are not far off.
Lemma 2.5 (Corners are nearly support vectors). The corners of the population cone are close to
the supporting hyperplane: ˆb1 ≤ ˆYP ˆw ≤ ˆb1 + (ζ + 2)1.
This suggests that we should consider all points that are up to (ζ + 2) away from the supporting
hyperplane when searching for corners. The next Lemma shows that each such point is a “near-
corner.”
i YPk  which
Recall that each row ˆyT
i YP   where
can be rewritten as a scaled convex combination of the normalized corners: yT
i 1
i 1. For a corner  ri = 1 and φi = ej for some j.
i 1 = 1. Speciﬁcally  ri = mT
i YP k and φi = mi
φT
kmT
We now show that every point i that is close to the supporting hyperplane is nearly a corner of the
ideal cone.
Lemma 2.6 (Points close to support vectors are near-corners). If ˆwT ˆyi ≤ ˆb + (ζ + 2) for some
point i ∈ [n]  then 1 ≤ ri ≤ 1 + (ζ+4)

is a noisy version of a population row yT

i YP /kmT
i = riφT

b− and φij ≥ 1 −

P ) for some j ∈ [K].

i = mT

bλK(YP YT

2ζ

mT

i

4

√

η(λK(YP YT
K1.5

Consider the set of points Sc = {i | ˆwT ˆyi ≤ ˆb + (ζ + 2)} that are close to the supporting
hyperplane. Lemmas 2.5 and 2.6 show that Sc contains all corners  and possibly other points that
are all near-corners. This suggests that we can cluster the vectors {ˆyi | i ∈ S} into K clusters  each
corresponding to one corner and possibly extra near-corners close to that corner. Randomly selecting
one point from each cluster gives us the set of inferred corners.
Lemma 2.7 (Each corner has its own cluster). There exist exactly K clusters in Sc  as long as
 ≤ c
Let C be the indices of the near-corners picked by this clustering step. Since Z = MYP   this
suggests M can be obtained via regression: M ≈ ˆZ ˆYT
C)−1Π  where ˆYC := ˆY(C  :) and
Π is a permutation matrix that matches the ordering of ideal corners and the empirical near-corners.
Theorem 2.8. If Condition 2 and the condition on  in Lemma 2.7 holds  then for any i ∈ [n] 
keT
  where cM is a global constant  and κ(.)
is the ratio of the largest and smallest nonzero singular values of a matrix.

C)−1Π)k ≤ cM κ(YP YT

  for some global constant c.

i (M − ˆZ ˆYT

P )keT
(λK(YP YT

P ))3
κ(YP YT
P )

i ZkKζ
P ))2.5

C( ˆYC ˆYT

C( ˆYC ˆYT

Algorithm 1 shows all the steps of our method (SVM-cone). The algorithm requires an estimate of
δ := (ζ + 2)  and returns the inferred M and near-corners C. When the row-wise error bound  is
unknown  we can start with δ = 0 and incrementally increase it until K distinct clusters are found.

Algorithm 1 SVM-cone
Input: ˆZ ∈ Rn×m  number of corners K  estimated distance of corners from hyperplane δ
Output: Estimated conic combination matrix ˆM and near-corner set C
1: Normalize rows of ˆZ by ‘2 norm to get ˆY with rows ˆyT
2: Run one-class SVM on ˆyi to get the normal ˆw and distance ˆb of the supporting hyperplane
3: Cluster points {ˆyi | ˆwT ˆyi ≤ ˆb + δ} that are close to the hyperplane into K clusters
4: Pick one point from each cluster to get near-corner set C
5: ˆM = ˆZ ˆYT

C( ˆYC ˆYT

C)−1

i

3 Applications
Many network models and topic models have population matrices of the form P = ρΓΘBΘT Γ. We
have already shown that in such cases  the eigenvector matrix V forms an ideal cone (Lemma 2.1) 
and that Condition 1 holds. It is easy to see that the same holds for VVT as well. This suggests that
SVM-cone can be applied to the matrix ˆV ˆVT   where ˆV is the empirical top-K eigenvector matrix.
We shall show that this yields per-node error bounds in estimating community memberships and
per-word error bounds for word-topic distributions.

3.1 Network models
Deﬁne a “DCMMSB-type” model as a model with population matrix P = ρΓΘBΘT Γ and an
empirical adjacency matrix A with Aji = Aij ∼ Bernoulli(Pij) for all i > j. Assume that rows of
Θ have unit ‘p norm  for p = 1 (DCMMSB) or p = 2 (OCCAM [32]). Let vi = VT ei  ˆvi = ˆVT ei 
yi = Vvi/kVvik  and ˆyi = ˆVˆvi/k ˆVˆvik. Denote γmax = maxi Γii and γmin = mini Γii.
Theorem 3.1 (Small row-wise error in Network Models). Consider a DCMMSB-type model
nρ)
with θi ∼ Dirichlet(α)  and α0
λ∗(B)

min(q n

27 log n   γ2
min
γ2
max
2(1 + α0)

If ν := α0
αmin

for some constant ξ > 1  κ(ΘT Γ2Θ) = Θ(1)  and α0 = O(1) 

≥ 8(1 + α0)(log n)ξ

:= αT 1.

√

≤

 

ν
then

γ2
min

nρ

 = max

kyi − ˆyik = ˜O

i

(cid:18) γmax min{K2  (κ(P))2}K0.5ν(1 + α0)

(cid:19)

minλ∗(B)√
γ3

nρ

with probability at least 1 − O(Kn−2). Here λ∗(B) is the smallest singular value of B.

5

nρ(log n)ξ)  and maxi kV(:  i)k = O(√

Similar results for the non-Dirichlet case follow easily as long as nρ = Ω((log n)2ξ)  λK(P) =
Ω(√
ρ) with high probability. This shows that the rows of
ˆV ˆVT are close to those of VVT   and the latter forms an ideal cone satisfying Condition 1. Hence 
the conic combination for each node can be recovered by Algorithm 1 applied to ˆV ˆVT . In fact  we
can run the algorithm on ˆV itself; the output depends only on the SVM dual variables β (Eq. 2) 
which are the same whether the input is ˆV or ˆV ˆVT . The output is the same conic combination matrix
ˆM and the same set C of nearly-pure nodes.

For identiﬁability of Θ  we need another condition. We will assume thatP Γii = n and all diagonal

entries of B are equal (details are provided in the supplementary material). The next theorem
shows that SVM-cone can be used to consistently infer the parameters of DCMMSB as well as
OCCAM [32].
Theorem 3.2 (Consistent inference of community memberships for each node). Consider DCMMSB-
type models where the conditions of Theorem 3.1 are satisﬁed and κ(ΘT Γ2Θ) = Θ(1). Let ˆD
Cei. Let ˆΘ = ˆF−1 ˆM ˆD  where ˆF
be a diagonal matrix with entries ˆDii =
is a diagonal matrix with entries ˆFii = keT
ˆM ˆDk2 (for
OCCAM [32]). Then there exists a permutation matrix Π such that

eT
ˆM ˆDk1 (for DCMMSB) and ˆFii = keT

q
(cid:18) γmaxK2.5 min{K2  (κ(P))2}n3/2
K(ΘT Γ2Θ)√

ˆYC ˆVˆE ˆVT ˆYT

(cid:19)

keT

ρ

γminηλ∗(B)λ2

i (Θ − ˆΘΠ)k = ˜O
with probability at least 1 − O(Kn−2).
Remark 3.1. The error bound is small when the clusters are well separated (large λ∗(B))  the network
is dense (large ρ)  there are few blocks (small K)  and the membership vectors Θ are drawn from a
balanced Dirichlet distribution (small ν  and hence small κ(P))  which leads to balanced block sizes.
Remark 3.2. For DCMMSB-type models  η ≥ γ2
. Also  under the conditions of
Theorem 3.1  η ≥ γ2
min
3νγ2
Observe that these are per-node error bounds  as against a simpler bound on kΘ − ˆΘk. Clearly 
the same results extend to the special case of the Mixed Membership Stochastic Blockmodel [1] and
the Stochastic Blockmodel [13] as well (the assumption of equal diagonal entries of B is no longer
needed  since Γii = 1 is enough for parameter identiﬁability [21]).

with high probability. Proofs are in the supplementary material.

min mini(eT

λ1(ΘT Γ2Θ)

i ΘT 1)

max

i

i

i

3.2 Topic Models
Let T ∈ RV ×K≥0
be a matrix of the word to topic probabilities with unit column sum  and let
be the topic to document matrix. Then A := TH is the probability matrix for words
H ∈ RK×D≥0
appearing in documents. The actual counts of words in documents are assumed to be generated iid as
Aij ∼ Binomial(N Aij) for i ∈ [V ]  j ∈ [D].
The word co-occurrence probability matrix is given by AAT /D = T(HHT /D)TT . Setting
Γii = kT(i  :)k1  Θ = Γ−1T  and B = HHT /D  we ﬁnd that AAT /D = ΓΘBΘT Γ with
Θ1 = 1. This clearly matches the form of P in the DCMMSB model. Hence  its eigenvector matrix
has the desired conic structure with weight matrix M = TΓ−1
P   with the “pure nodes” being
anchor words that only occur in a single topic. We now show that the row-wise error between the
empirical and population eigenvector matrices decays with increasing number of documents D and
number of words in a document N.
i AAT ek. We assume that when it is not zero  it goes to inﬁnity  in
Assumption 3.1. Let gik = eT
particular  gik ≥ N log max(V  D)  which gives D/N → ∞. We also assume that λi(HHT ) =
Θ(D)  for i ∈ [K]  and κ(TTT ) = Θ(1).
These assumptions are similar to ones made in other theoretical literature on topic models [18].
We will construct a matrix A1AT2   where A1 and A2 are obtained by dividing the words in each
document uniformly randomly in two equal parts. This ensures that E[A1AT2 ] = AAT   which in
turn helps establishing concentration of empirical singular vectors as shown in the following lemma.
For simplicity denote N1 = N/2.

P N−1

6

(c) Varying sparsity for
OCCAM model

(b) Varying sparsity for
DCMMSB

(d) Varying sparsity for
SBMO [17]

(a) Varying degree hetero-
geneity for DCMMSB
Figure 2: Relative error in estimation of community memberships: Plots (a) and (b) compare SVM-
cone against the closest baseline (GeoNMF) on the degree-corrected MMSB model. We then compare
against (c) OCCAM and (d) SAAC on networks drawn from their generative models.
Lemma 3.3 (Small row-wise error in Topic Models). Let ˆV denote the matrix of the top-K singular
vectors of U = A1AT2 /N 2
1   and let the population counterpart of this be V. Let vi = VT ei 
ˆvi = ˆVT ei  yi = Vvi/kVvik  and ˆyi = ˆVˆvi/k ˆVˆvik. Under Assumption 3.1  we have:
K log max(V  D)

 r

!

minj keT

1
i Tk1

pλK(TT T) OP

 = max

kyi − ˆyik =

i

with probability at least 1 − O(1/D2).
Thus  Algorithm 1 run on ˆV ˆVT (or equivalently  just ˆV) can be used to ﬁnd the conic combination
weights ˆM ≈ M. Since M being the product of T with a diagonal matrix where T has unit column
sum  we can extract ˆT = ˆM ˆD−1  where ˆD is a diagonal matrix with ˆDii = k ˆM(:  i)k1.
Theorem 3.4 (Consistent inference of word-topic probabilities for each word). Under Assumption 3.1 
there exists a permutation matrix Π such that with probability at least 1 − O(1/D2) 

DN

 

keT

i ( ˆT − TΠT )k

keT

i Tk

K4 maxj keT
η(minj keT

= OP
i Tk1/λ1(TT T) ≥ mini keT

j Tk1
j Tk1)2

Remark 3.3. We have η ≥ mini keT

rlog max(V  D)

!

DN

.

i Tk1/K (supplementary material).

4 Experiments

We ran experiments on simulated and real-world datasets to verify the accuracy and scalability of
SVM-cone. We compared SVM-cone against several competing baselines. For network models 
GeoNMF detects the corners of a simplex formed by the MMSB model by constructing the graph
Laplacian and picking nodes that have large norms in the Laplacian [20]. It assumes balanced
communities (i.e.  the rows of Θ are drawn from a Dirichlet with identical community weights). SVI
uses stochastic variational inference for MMSB [12]. BSNMF [24] presents a Bayesian approach to
Symmetric Nonnegative Matrix Factorization; it can be applied to do inference for MMSB models
with B = cI where c ∈ [0  1]. OCCAM works on a variant of MMSB where each row of Θ has
unit ‘2 norm  and the model allows for degree heterogeneity [32]. SAAC [17] uses alternating
optimization on a version of the stochastic blockmodel where each node can be a member of
multiple communities  but the membership weight is binary. For topic models  RecoverL2 [5] uses a
combinatorial algorithm to pick anchor words from the word co-occurrence matrix and then recovers
the word-topic vectors by optimizing a quadratic loss function. TSVD [7] uses a thresholded SVD
based procedure to recover the topics. GDM [31] is a geometric algorithm that involves a weighted
clustering procedure augmented with geometric corrections. We could not obtain the code for [18].

4.1 Networks with overlapping communities

In this section  we present experiments on simulated and large real networks.

4.1.1 Simulations
We test the recovery of population parameters (Θ  B) given adjacency matrices A generated from
the corresponding population matrices P (Γ are nuisance parameters). We generate networks with

7

(a) DBLP coauthorship

(b) DBLP bipartite

(c) Wall-clock time

Figure 3: Accuracy of estimated community memberships for (a) the DBLP coauthorship network
and (b) the biparite author-paper DBLP network. (c) The wall-clock time of the competing methods
on the DBLP coauthorship network respectively.
n = 5000 nodes and K = 3 communities. The rows of Θ are drawn from Dirichlet(α) for DCMMSB
and OCCAM; for DCMMSB  α = (1/3  1/3  1/3); for OCCAM  α = (1/6  1/6  1/6) and the rows
are normalized to have unit ‘2 norm. We set Bii = 1 and Bij = 0.1 for all i 6= j. The default degree
parameters for DCMMSB are as follows: for all nodes i that are predominantly in the j-th community
(θij > 0.5)  we set Γii to 0.3  0.5  and 0.7 for the 3 respective communities; all other nodes have
Γii = 1. For OCCAM  we draw degree parameters from a Beta(1  3) distribution.

Varying degree parameters Γ: We set the degree parameters for predominant nodes in the 3
communities as 0.5 + Γ  0.5  and 0.5 + Γ respectively. Figure 2a shows SVM-cone outperforms
GeoNMF consistently for all choices of Γ.
Varying network sparsity ρ: Figure 2b shows the relative error in estimating Θ as a function of
the network sparsity ρ. Increasing ρ increases the average degree of nodes in the network without
affecting the skew induced by their degree parameters Γ. As expected  all methods tend to improve
with increasing degree. Our method dominates GeoNMF over the entire range of average degrees.
Figures 2c and 2d show results for networks generated under the models used by OCCAM and SAAC
respectively. SVM-cone is comparable or better than these methods even on their generative models.
The smaller error bars on SVM-cone show that it is more stable than SAAC.

4.1.2 Real-world experiments

K maxσ

We tested SVM-cone on large network datasets and word-document datasets. For networks  we
used the 5 DBLP coauthorship networks1 (used in [20]  where each ground truth community
corresponds to a group of conferences on the same topic. We also use bipartite author-paper
PK
variants for these 5 networks. Following [20]  we evaluate results by the rank correlation be-
tween the predicted vector for community i against the true vector  averaged over all communities:
i=1 RC( ˆΘ(:  i)  Θ(:  σ(i)))  where σ is a permutation over the K com-
RCavg( ˆΘ  Θ) = 1
munities. We have −1 ≤ RCavg( ˆΘ  Θ) ≤ 1  with higher numbers implying a better match between
ˆΘ and Θ. We do not use metrics like NMI [30] or ExNVI [32] that require binary overlapping
membership vectors to avoid thresholding issues on real-valued membership vectors.
We ﬁnd that SVM-cone outperforms competing baselines on 2 of the 5 DBLP coauthorship datasets 
and is similar on the remaining three (Figure 3a). The closest competitor is GeoNMF [20]  which
assumes that all nodes have the same degree parameter  and the community sizes are balanced. Both
assumptions are reasonable for the dataset  since the number of coauthors (the degree) does not
vary signiﬁcantly among authors  and the communities are formed from conferences where no one
conference dominates the others. The differences between SVM-cone and the competition is starker
on the bipartite dataset (Figure 3b). There is severe degree heterogeneity: an author can be connected
to many papers  while each paper only has a few authors at best. Our method is able to accommodate
such differences between the nodes  and hence yields much better accuracy than others.
Finally  Figure 3c shows the wall-clock time for running the various methods on DBLP coauthorship
networks (wall-clock time on DBLP bipartite author-paper networks is included in the supplementary
material). Our method is among the fastest. This is expected; the only computationally intensive step
is the one-class SVM and top-K eigen-decomposition (or SVD)  for which off-the-shelf efﬁcient and
scalable implementations already exist [10].

1http://www.cs.utexas.edu/~xmao/coauthorship

8

DBLP1DBLP2DBLP3DBLP4DBLP50.000.050.100.150.200.250.300.350.40RCavgSVM-coneGeoNMFSVIBSNMFOCCAMSAACDBLP1DBLP2DBLP3DBLP4DBLP50.00.10.20.30.40.5RCavgSVM-coneGeoNMFSVIBSNMFOCCAMSAACDBLP1DBLP2DBLP3DBLP4DBLP5100101102103104105Runningtime/sSVM-coneGeoNMFSVIBSNMFOCCAMSAAC4.2 Topic Models

We generate semi-synthetic data following [5] and [7] using NIPS1  New York Times1 (NYT) 
PubMed1  and 20NewsGroup2 (20NG). Dataset statistics are included in the supplementary material.
We use Matlab R2018a built-in Gibbs Sampling function for learning topic models to learn the word by
topic matrix  which should retain the characteristics of real data distributions. Then we draw the topic-
document matrix from Dirichlet with symmetric hyper-parameter 0.01. We set K = 40 for the ﬁrst 3
P
datasets and K = 20 for 20NG. The word counts matrix is sampled with N = 1000  300  100  200
respectively  which matches the mean document length of the real datasets. We evaluate the perfor-
i j |T (i  j) − ˆT (i  π(j))|  where
mance of different algorithms using ‘1 reconstruction error 1
π(.) is a permutation function that matches the topics. Table 1 shows the ‘1 reconstruction error and
wall-clock running time of different algorithms with datasets generated from different number of
documents. Each setting is repeated 5 times  and we report the mean and standard deviation of the
results. SVM-cone is much faster than the other methods. Its accuracy is comparable to RecoverL2 
and signiﬁcantly better than TSVD and GDM. The supplementary material also shows the top-10
words of 5 topics learned from SVM-cone for each dataset.

K

Table 1: ‘1 reconstruction error and wall-clock time on semi-synthetic datasets
Documents

RecoverL2

GDM

Corpus

NIPS

NYT

PubMed

20NG

20000

40000

60000

20000

40000

60000

20000

40000

60000

20000

40000

60000

‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s
‘1 Error
Time/s

0.059 (± 0.000)
100.11 (± 8.81)
0.043 (± 0.000)
143.34 (± 0.53)
0.036 (± 0.000)
247.34 (± 20.84)
0.125 (± 0.000)
78.15 (± 7.14)
0.103 (± 0.000)
140.84 (± 15.50)
0.095 (± 0.000)
184.69 (± 20.65)
0.163 (± 0.000)
54.32 (± 5.94)
0.122 (± 0.000)
78.99 (± 9.99)
0.098 (± 0.000)
98.19 (± 15.06)
0.100 (± 0.000)
40.74 (± 0.64)
0.074 (± 0.000)
94.42 (± 9.92)
0.058 (± 0.000)
142.34 (± 20.31)

TSVD

0.237 (± 0.017)
18.54 (± 2.04)
0.250 (± 0.045)
21.97 (± 1.49)
0.269 (± 0.064)
35.77 (± 3.28)
0.207 (± 0.025)
25.11 (± 6.39)
0.197 (± 0.045)
50.18 (± 13.14)
0.166 (± 0.028)
42.96 (± 7.95)
0.239 (± 0.032)
15.75 (± 2.34)
0.255 (± 0.018)
26.44 (± 4.49)
0.275 (± 0.041)
24.57 (± 4.59)
0.111 (± 0.051)
7.51 (± 0.42)
0.081 (± 0.043)
16.04 (± 2.28)
0.133 (± 0.045)
23.36 (± 5.85)

0.081 (± 0.057)
119.66 (± 4.41)
0.061 (± 0.038)
220.92 (± 3.10)
0.059 (± 0.038)
406.87 (± 36.57)
0.223(± 0.008)
193.43 (± 12.02)
0.216 (± 0.010)
394.16 (± 30.42)
0.210 (± 0.010)
595.54 (± 91.57)
0.277 (± 0.051)
205.95 (± 11.27)
0.251 (± 0.041)
459.17 (± 30.71)
0.269 (± 0.052)
649.97 (± 26.48)
0.137 (± 0.071)
102.86 (± 4.05)
0.131 (± 0.072)
273.51 (± 16.45)
0.096 (± 0.063)
388.47 (± 43.22)

SVM-cone
0.071 (± 0.004)
5.33 (± 0.39)
0.051 (± 0.002)
9.07 (± 0.00)
0.041 (± 0.002)
17.63 (± 5.29)
0.131 (± 0.003)
4.51 (± 0.70)
0.106 (± 0.001)
8.04 (± 1.15)
0.096 (± 0.002)
11.82 (± 1.91)
0.181 (± 0.002)
2.06 (± 0.46)
0.138 (± 0.001)
3.73 (± 0.37)
0.114 (± 0.001)
5.44 (± 0.38)
0.090 (± 0.003)
1.85 (± 0.26)
0.064 (± 0.001)
4.33 (± 0.71)
0.052 (± 0.002)
5.89 (± 0.67)

5 Conclusions

We showed that many distinct models for overlapping clustering can be placed under one general
framework  where the data matrix is a noisy version of an ideal matrix and each row is a non-negative
weighted sum of “exemplars.” In other words  the connection probabilities of one node to others
in a network is a non-negative combination of the connection probabilities of K “pure” nodes to
others in the network. Each pure node is an examplar of a single community  and we require one
pure node from each of the K communities. This geometrically corresponds to a cone  with the
pure nodes being its corners. This subsumes Mixed-Membership Stochastic Blockmodels and their
degree-corrected variants  as well as commonly used topic models. We showed that a one-class SVM
applied to the normalized rows of the data matrix can ﬁnd both the corners and the weight matrix.
We proved the consistency of our SVM-cone algorithm  and used it to develop consistent parameter
inference methods for several widely used network and topic models. Experiments on simulated and
large real-world datasets show both the accuracy and the scalability of SVM-cone.

1https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
2http://qwone.com/~jason/20Newsgroups/

9

Acknowledgments

X.M. and P.S. were partially supported by NSF grant DMS 1713082. D.C. was partially supported by
a Facebook Faculty Research Award.

References
[1] E. M. Airoldi  D. M. Blei  S. E. Fienberg  and E. P. Xing. Mixed membership stochastic blockmodels.

JMLR  9:1981–2014  2008.

[2] A. Anandkumar  R. Ge  D. Hsu  and S. M. Kakade. A tensor approach to learning mixed membership

community models. JMLR  15(1):2239–2312  2014.

[3] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor decompositions for learning

latent variable models. The Journal of Machine Learning Research  15(1):2773–2832  2014.

[4] S. Arora  R. Ge  R. Kannan  and A. Moitra. Computing a nonnegative matrix factorization–provably. In

STOC  pages 145–162. ACM  2012.

[5] S. Arora  R. Ge  Y. Halpern  D. Mimno  A. Moitra  D. Sontag  Y. Wu  and M. Zhu. A practical algorithm
for topic modeling with provable guarantees. In International Conference on Machine Learning  pages
280–288  2013.

[6] P. Awasthi  B. Kalantari  and Y. Zhang. Robust vertex enumeration for convex hulls in high dimensions. In
A. Storkey and F. Perez-Cruz  editors  International Conference on Artiﬁcial Intelligence and Statistics 
volume 84  pages 1387–1396. PMLR  09–11 Apr 2018.

[7] T. Bansal  C. Bhattacharyya  and R. Kannan. A provable svd-based algorithm for learning topics in
dominant admixture corpus. In Advances in Neural Information Processing Systems  pages 1997–2005 
2014.

[8] X. Bing  F. Bunea  and M. Wegkamp. A fast algorithm with minimax optimal guarantees for topic models

with an unknown number of topics. arXiv preprint arXiv:1805.06837  2018.

[9] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent dirichlet allocation. Journal of machine Learning research 

3(Jan):993–1022  2003.

[10] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent

systems and technology (TIST)  2(3):27  2011.

[11] W. Ding  M. H. Rohban  P. Ishwar  and V. Saligrama. Topic discovery through data dependent and random

projections. In International Conference on Machine Learning  pages 1202–1210  2013.

[12] P. K. Gopalan and D. M. Blei. Efﬁcient discovery of overlapping communities in massive networks. PNAS 

110(36):14534–14539  2013.

[13] P. W. Holland  K. B. Laskey  and S. Leinhardt. Stochastic blockmodels: First steps. Social networks  5(2):

109–137  June 1983. ISSN 0378-8733.

[14] S. B. Hopkins and D. Steurer. Bayesian estimation from few samples: community detection and related

problems. In FOCS  pages 379–390. IEEE  2017.

[15] K. Huang  X. Fu  and N. D. Sidiropoulos. Anchor-free correlated topic modeling: Identiﬁability and

algorithm. In Advances in Neural Information Processing Systems  pages 1786–1794  2016.

[16] J. Jin  Z. T. Ke  and S. Luo. Estimating network memberships by simplex vertex hunting. arXiv preprint

arXiv:1708.07852  2017.

[17] E. Kaufmann  T. Bonald  and M. Lelarge. A spectral algorithm with additive clustering for the recovery
of overlapping communities in networks. In International Conference on Algorithmic Learning Theory 
pages 355–370. Springer  2016.

[18] Z. T. Ke and M. Wang. A new svd approach to optimal topic estimation. arXiv preprint arXiv:1704.07016 

2017.

[19] P. Latouche  E. Birmelé  C. Ambroise  et al. Overlapping stochastic block models with application to the

french political blogosphere. The Annals of Applied Statistics  5(1):309–336  2011.

10

[20] X. Mao  P. Sarkar  and D. Chakrabarti. On mixed memberships and symmetric nonnegative matrix

factorizations. In ICML  pages 2324–2333  2017.

[21] X. Mao  P. Sarkar  and D. Chakrabarti. Estimating mixed memberships with sharp eigenvector deviations.

arXiv preprint arXiv:1709.00407  2017.

[22] A. Y. Ng  M. I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Advances in

neural information processing systems  pages 849–856  2002.

[23] M. Panov  K. Slavnov  and R. Ushakov. Consistent estimation of mixed memberships with successive
In International Workshop on Complex Networks and their Applications  pages 53–64.

projections.
Springer  2017.

[24] I. Psorakis  S. Roberts  M. Ebden  and B. Sheldon. Overlapping community detection using bayesian

non-negative matrix factorization. Physical Review E  83(6):066114  2011.

[25] T. Qin and K. Rohe. Regularized spectral clustering under the degree-corrected stochastic blockmodel. In

Advances in Neural Information Processing Systems  pages 3120–3128  2013.

[26] A. Ray  J. Ghaderi  S. Sanghavi  and S. Shakkottai. Overlap graph clustering via successive removal. In

52nd Annual Allerton Conference  pages 278–285. IEEE  2014.

[27] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.

The Annals of Statistics  pages 1878–1915  2011.

[28] P. Rubin-Delanchy  C. E. Priebe  M. Tang  and J. Cape. A statistical interpretation of spectral embedding:

the generalised random dot product graph. arXiv preprint arXiv:1709.05506  2017.

[29] B. Schölkopf  J. C. Platt  J. Shawe-Taylor  A. J. Smola  and R. C. Williamson. Estimating the support of a

high-dimensional distribution. Neural computation  13(7):1443–1471  2001.

[30] A. Strehl and J. Ghosh. Cluster ensembles—a knowledge reuse framework for combining multiple

partitions. Journal of machine learning research  3(Dec):583–617  2002.

[31] M. Yurochkin and X. Nguyen. Geometric dirichlet means algorithm for topic inference. In Advances in

Neural Information Processing Systems  pages 2505–2513  2016.

[32] Y. Zhang  E. Levina  and J. Zhu. Detecting overlapping communities in networks using spectral methods.

arXiv preprint arXiv:1412.3432  2014.

11

,Marek Petrik
Xueyu Mao
Purnamrita Sarkar
Deepayan Chakrabarti