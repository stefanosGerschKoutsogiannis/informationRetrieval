2012,Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach,In many graph-based machine learning and data mining approaches  the quality of the graph is critical. However  in real-world applications  especially in semi-supervised learning and unsupervised learning  the evaluation of the quality of a graph is often expensive and sometimes even impossible  due the cost or the unavailability of ground truth. In this paper  we proposed a robust approach with convex optimization to ``forge'' a graph: with an input of a graph  to learn a graph with higher quality. Our major concern is that an ideal graph shall satisfy all the following constraints: non-negative  symmetric  low rank  and positive semidefinite. We develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees. With only one non-sensitive parameter  our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings. As a preprocessing of graphs  our method has a wide range of potential applications machine learning and data mining.,Forging The Graphs: A Low Rank and Positive

Semideﬁnite Graph Learning Approach

Dijun Luo  Chris Ding  Heng Huang  Feiping Nie
Department of Computer Science and Engineering

The University of Texas at Arlington

dijun.luo@gmail.com  chqding@uta.edu

heng@uta.edu  feipingnie@gmail.com

Abstract

In many graph-based machine learning and data mining approaches  the quality
of the graph is critical. However  in real-world applications  especially in semi-
supervised learning and unsupervised learning  the evaluation of the quality of a
graph is often expensive and sometimes even impossible  due the cost or the un-
availability of ground truth. In this paper  we proposed a robust approach with
convex optimization to “forge” a graph: with an input of a graph  to learn a graph
with higher quality. Our major concern is that an ideal graph shall satisfy all the
following constraints: non-negative  symmetric  low rank  and positive semidef-
inite. We develop a graph learning algorithm by solving a convex optimization
problem and further develop an efﬁcient optimization to obtain global optimal so-
lutions with theoretical guarantees. With only one non-sensitive parameter  our
method is shown by experimental results to be robust and achieve higher accuracy
in semi-supervised learning and clustering under various settings. As a prepro-
cessing of graphs  our method has a wide range of potential applications machine
learning and data mining.

1 Introduction

Many machine learning algorithms use graphs as input  such as clustering [16  14]  manifold based
dimensional reduction [2  15]  and graph-based semi-supervised learning [23  22].
In these ap-
proaches  we are particularly interested in the similarity among objects. However  the observation
of similarity graphs often contain noise which sometimes mislead the learning algorithm  especially
in unsupervised and semi-supervised learning. Deriving graphs with high quality becomes attractive
in machine learning and data mining research.

A robust and stable graph learning algorithm is especially desirable in unsupervised and semi-
supervised learning  because the unavailability or high cost of ground truth in real world appli-
cations. In this paper  we develop a novel graph learning algorithm based on convex optimization 
which leads to robust and competitive results.

1.1 Motivation and Main Problem

In this section  the properties of similarity matrix are revisited from point of view of normalized
cut clustering [19]. Given a symmetric similarity matrix W ∈ Rn×n on n objects  normalized cut
solves the following optimization problem [10].

trH⊺(D − W)H s.t. H⊺DH = I 

min
H≥0

(1)

1

where H ∈ {0  1}n×K is the cluster indicator matrix  or equivalently 

trF⊺ ˜WF s.t. F⊺F = I 

max
F≥0

(2)

where F = [f1  f2  · · ·   fK]  H = [h1  h2  · · ·   hK]  fk = D
D− 1
number of groups. Eq. (2) can be further rewritten as 

2   D = diag(d1  d2  · · ·   dn)  di = Pn

2 WD− 1

1

1

2 hkk  1 ≤ k ≤ K  ˜W =
j=1 Wij   I is the identity matrix  and K is the

2 hk/kD

k ˜W − FF⊺kF s.t. F⊺F = I 

min
F≥0

(3)

where k · kF denotes the Frobenius norm. We notice that

(4)
for any G ∈ Rn×n. Our goal is to minimize the LHS (left-hand side); Instead  we can minimize the
RHS which is the upper-bound of LHS.

k ˜W − G + G − FF⊺kF ≤ k ˜W − GkF + kG − FF⊺kF  

Thus we need to ﬁnd the intermediate matrix G  i.e.  we learn a surrogate graph which is close but
not identical to ˜W. Our upper-bounding approach offers ﬂexibility which allows us to impose cer-
tain desirable properties. Note that matrix FF⊺ holds the following properties: (P1) symmetric  (P2)
nonnegative  (P3) low rank  and (P4) positive semideﬁnite. This suggests a convex graph learning

min

G

kG − ˜Wk2

F s.t. G < 0  kGk∗ ≤ c  G = G⊺  G ≥ 0 

(5)

where G < 0 denotes the positive semideﬁnite constraint  k · k∗ denotes the trace norm  i.e. the sum
of the singular values [8]  and c is a model parameter which controls the rank of G. The constraint
of G ≥ 0 is to force the similarity to be naturally non-negative. By intuition  one might impose row
rank constraint of rank(G) ≤ c. But this leads to a non-convex optimization  which is undesirable
in unsupervised and semi-supervised learning. Following matrix completion methods [5]  the trace
constraint in Eq. (5) is a good surrogate for the low rank constraint. For notational convenience  the
normalized similarity matrix ˜W is denoted as W in the rest of the paper.
By solving Eq. (5)  we are actually seeking a similarity matrix which satisﬁes all the properties of
a perfect similarity matrix (P1–P4) and which is close to the original input matrix G. Our whole
paper is here dedicated to solve Eq. (5) and to demonstrate the usefulness of its optimal solution in
clustering and semi-supervised learning using both theoretical and empirical evidences.

1.2 Related Work

Our method can be viewed as a preprocessing for similarity matrix and a large number of machine
learning and data mining approaches require a similarity matrix (interpreted as a weighted graph) as
input. For example  in unsupervised clustering  Normalized Cut [19]  Ratio Cut [11]  Cheeger Cut
[3] have been widely applied in various real world applications. In graphical models for relational
data  e.g. Mixed Membership Block models [1] can be also interpreted as generative models on
the similarity matrices among objects. Thus a similarity matrix preprocessing model can be widely
applied.

A large number of approaches have been developed to learn similarity matrix with different empha-
sis. Local Linear Embedding (LLE ) [17  18] and Linear Label Propagation [21] can be viewed as
obtaining a similarity matrix using sparse coding. Another way to perform the similarity matrix pre-
processing is to take a graph as input and to obtain a reﬁned graph by learning  such as bi-stochastic
graph learning [13]. Our method falls in this category. We will compare our method with these
methods in the experimental section.

On the optimization techniques for problems with multiple constraints  there also exist many related
researches. First  von Neumann provided a convergence proof of successive projection method that
it guarantees to converge to feasible solution in convex optimization with multiple constraints  which
was employed in the paper by Liu et al. [13]. In this paper  we develop a novel optimization algo-
rithm to solve the optimization problem with multiple convex constraints (including the inequality
constraints)  which is guaranteed to ﬁnd the global solution. More explicitly  we develop a variant
of inexact Augmented Lagrangian Multiplier method to handle inequality constraints. We also de-
velop a useful Lemma to handle the subproblems with trace norm constraint in the main algorithm.
Interestingly  one of the derived subproblems is the ℓ1 ball projection problem  which can be solved
elegantly by simple thresholding.

2

(a)

(b)

(c1)

(c2)

(d)

2

l

s
e
u
a
v
n
e
g
E

i

1.5

1

0.5

0

−0.5

−1

 
0

 

Eq. (5)
Eq. (6)

5

10

15

20

25

30

Sorting index

(a): A perfect
Figure 1: A toy example of low rank and positive semideﬁnite graph learning.
similarity matrix. (b): Adding noise from (a). (c1): the optimal solution of Eq. (5) by using the
matrix in (b) as G. (c2): the optimal solution of Eq. (6) by using the matrix in (b) as G. (d): sorted
eigenvalues for the two solutions of Eq. (5) and Eq. (6).

2 A Toy Example

We ﬁrst emphasize the usefulness of the positive semideﬁnite and low rank constraints in problem of
Eq. (5) using a toy example. In this toy example  we also solve the following problem for contrast 

min

G

kG − Wk2

F s.t. G = G⊺  Ge = e  G ≥ 0 

(6)

where e = [1  1  · · ·   1]T and the constraints of positive semideﬁnite and low rank are removed from
Eq. (5) and instead a bi-stochastic constraint is applied (Ge = e). Notice that the model deﬁned in
Eq. (6) is used in the bi-stochastic graph learning [13]. We solve Eqs. (5) and (6) for the same input
G and compare the solution to see the effect of positive semideﬁnite and low rank constraints.
In the toy example  we ﬁrst generate a perfect similarity matrix W in which Wij = 1 if data points
i  j are in the same group and Wij = 0 otherwise. Three groups of data points (10 data points in
each group) are considered. G are shown in Figure 1 (a) with black color denoting zeros values. We
then randomly add some positive noise on G which is shown in Figure 1 (b). Then we solve Eqs.
(5) and (6) and the results of G is shown in Figure 1 (c1) and (c2). The observation is that Eq. (5)
recover the perfect similarity much more accurately than Eq. (6). The reason is that in model of
Eq. (6)  the low rank and positive semideﬁnite constraints are ignored and the result deviates from
the ground truth.
We show the eigenvalues distributions of G for solution in Figure 1 (d) for both methods in Eqs. (5)
and (6). One can observe that the solution Eq. (5) gives low rank and positive semideﬁnite results 
while the solution for Eq. (6) is full rank and has negative eigenvalues.

Since the solution of Eq. (5) is always positive  symmetric  low rank  and positive semideﬁnite  we
called our solution the Non-negative Low-rank Kernel (NLK).

2.1 NLK for Semi-supervised Learning

Although NKL is mainly developed for unsupervised learning  it can be easily extended to incor-
porate the label information in semi-supervised learning [23]. Assume we are given a set of data
X = [x1  x2  · · ·   xℓ  xℓ+1  · · ·   xn] where the ﬁrst l data points are labeled as [y1  y2  · · ·   yℓ].
Then we have more information to learn a better similarity matrix. Here we add additional con-
straints on Eq. (5) by enforcing the similarity to be zeros for those paris of data points in different
classes  i.e. Gij = 0 if yi 6= yj  1 ≤ i  j ≤ ℓ. By considering all the constraints  we optimize the
following 

min

G

kG − Wk2

F s.t. G < 0  kGk∗ ≤ c  G = G⊺  G ≥ 0  Gij = 0  ∀yi 6= yj.

(7)

We will demonstrate the advantage of these semi-supervision constraints in the experimental section.
The computational algorithm is given in §3.3.

3

3 Optimization

The optimization problemss in Eqs. (5) and (7) are non-trivial since there are multiple constraints 
including both equality and inequality constraints. Our strategy is to introduce two extra copies of
optimization variable X and Y to split the constraints into several directly solvable subproblems 

min

G

min

X

min

Y

kG − Wk2

kX − Wk2

F   s.t. G ≥ 0
F   s.t. X < 0  with X = G.

kY − Wk2

F   s.t.

kYk∗ ≤ c  with Y = G

More formally  we solve the following problem 

minG kG − Wk2
F
s.t. G ≥ 0

X = G  X < 0 
Y = G 

|Yk∗ ≤ c 

(8a)

(8b)

(8c)

(9a)
(9b)
(9c)
(9d)

One should notice that problem in Eqs. (9a) – (9d) is equivalent to our main problem in Eq. (5).
In the rest of this section  we will employ a variant of Augmented Lagrangian Multiplier (AML)
method to solve Eqs. (9a) – (9c).

3.1 Seeking Global Solutions: A Variant of ALM

The augmented Lagrangian multiplier function of Eqs. (9a) – (9d) is

Φ(G  X  Y) =kG − Wk2

F − hΛ  X − Gi +

µ
2

kG − Xk2

F − hΣ  Y − Gi +

µ
2

kG − Yk2
F  
(10)

with constraints of G ≥ 0  X < 0  and kYk∗ ≤ c  where Λ  Σ are the Lagrangian multipliers.
Then ALM method leads to the following updating steps 

G ← arg min
G≥0
X ← arg min
X<0

Φ(G  X  Y  Z)

Φ(G  X  Y  Z)

Y ← arg min

kYk∗≤c

Φ(G  X  Y  Z)

Λ ← Λ − µ (G − X)
Σ ← Σ − µ (G − Y)
t ← t + 1.
µ ← γµ 

(11a)

(11b)

(11c)

(11d)
(11e)
(11f)

Notice that the symmetric constraint is removed here. We will later show that given a symmetric
input W  the output of our algorithm automatically satisﬁes the symmetric constraints.

3.2 Solving the Subproblems in ALM

X and Y updating algorithms in Eqs. (11b and 11c) contain eigenvalue constraints  which appear
complicated. Fortunately they have closed form solution. To show that  we ﬁrst introduce the
following useful Lemma.
Lemma 3.1. Consider the following problem 

min

X

kX − Ak2

F   s.t. φi(X) ≤ ci  1 ≤ i ≤ m 

(12)

where φi(X) ≤ ci is any constraint on eigenvalues of X  i = 1  2  · · ·   m and m is the num-
ber of constraints. Then there exists a diagonal matrix S such that USU⊺ is an optimizer of
Eq. (12)  where UΣU⊺ = A is the eigenvector decomposition of A. S relates to eigenvalues of
Σ = diag(λ1  · · ·   λn) and satisfying the constraints.

4

Proof. Let VSV⊺ = X and UDU⊺ = A be the eigenvector decomposition of X and A  respec-
tively. By applying von Neumann’s trace inequality  the following holds for any X and A 

Then

trVSV⊺A = trX⊺A ≤ trSD = tr(USU⊺)(UDU⊺) = USU⊺A 

trX⊺A ≤ trSD.

(13)

(14)

which leads to

kUSU⊺ − Ak2

(15)
Now assume X = VSV⊺ is a minimizer of Eq. (12). By comparing two solutions of X = VSV⊺
and Z = USU⊺  one should notice (a) that Z satisﬁes all the constraints of φi(Z) = φi(X) ≤
ci  1 ≤ i ≤ m in Eq. (12) and (b) that Z gives equal or less value of the objective  thus Z = USU⊺
ia also a minimizer of Eq. (12).

F ≤ kVSV⊺ − Ak2
F .

Lemma 3.1 shows an interesting property of the matrix approximation with eigenvalue or singular
value constraint: the optimal solution matrix shares the same subspace of the input matrix. This
is useful  because once the subspace is determined  the whole optimization becomes much easier.
Thus the lemma provides a powerful mathematical tool in computation of optimization problem
with eigenvalue and singular value constraints. Here  we apply Lemma 3.1 to solve the updating of
X and Y in §3.2.2 - 3.2.3.

3.2.1 Updating G

By ignoring the irrelevant terms with respect to G   we can rewrite Eq. (11a) as following 

G ← arg min
G≥0

k(2 + 2µ)G − (2W + µ(X + Y) + Λ + Σ) k2

F + const

= max(cid:18) 2W + µ(X + Y) + Λ + Σ

2 + 2µ

  0(cid:19) .

3.2.2 Updating X

For Eq. (11b)  we need to solve the following subproblem

min

X

kX − Pk2

F   X < 0  where P = G + Λ/µ.

(16)

(17)

(18)

Notice that X < 0 is constraint on the eigenvalues of X. Then we can directly apply Lemma 3.1  X
can be written as USU⊺ and Eq. (18) becomes

min

S

kUSU⊺ − UDU⊺k2

F   s.t. S ≥ 0 

(19)

where UDU⊺ = P is the eigenvector decomposition of P. Let S = diag(s1  s2  · · ·   sn) and
D = diag(d1  d2  · · ·   dn). Then Eq. (19) can be further rewritten as 

min

s1 s2 ···  sn

n

Xi=1

(si − di)2   s.t. si ≥ 0  i = 1  2  · · ·   n.

(20)

Eq. (20) has simple closed form solution as si = max(di  0)  i = 1  2  · · ·   n.

3.2.3 Updating Y

Eq. (11c) can be rewritten as 

where Q = G + 1

F   kYk∗ ≤ c 
µ Σ. The corresponding Lagrangian function is 

min

kY − Qk2

Y

(21)

L(Y  λ) = kY − Qk2

(22)
Since we do not know the true Lagrangian multiplier λ  we cannot directly apply the singular value
thresholding technique [4]. However  we ﬁnd Lemma 3.1 useful again. We notice that Y is symmet-
ric and the constraint of kYk∗ ≤ c becomes a constraint on the eigenvalues of Y. Let Y = USU⊺
and by directly applying Lemma 3.1  Eq. (21) can be further written as 

F + λ (kYk∗ − c) .

min

S

kUSU⊺ − UDU⊺k2

F   s.t.

n

Xi=1

|si| ≤ c 

(23)

5

or 

min

s

ks − dk2  s.t.

n

Xi=1

|si| ≤ c 

(24)

where S = diag(s)  s = [s1  s2  · · ·   sn]⊺  D = diag(d)  and d = [d1  d2  · · ·   dn]⊺.
Interestingly  the above problem is a standard ℓ1 ball optimization problem which has been studied
for a long time and Duchi et al. has recently provided a simple and elegant solution [7]. The ﬁnal

solution is to search the least θ ≥ 0 such thatPi max(|di| − θ  0) ≤ c  i.e.

θ = arg min

θ

θ s.t.

n

Xi=1

max(|di| − θ  0) ≤ c.

(25)

This can be easily done by sorting the |di| and try the θ values between two consecutive sorted |di|.
And the solution is

si = sign(di) max(|di| − θ  0).

(26)

Notice that in each step of algorithm  the solution has closed form solution and that the output of
G is always symmetric  which indicates that the constraint of G = G⊺ is automatically satisﬁed in
each step.

3.3 NLK Algorithm For Semi-supervised Learning

In many real world settings  we know partially of data class labels and hope to further utilize such
information  as described in Eq. (7). Fortunately  the corresponding optimization problem remains
convex. The augmented Lagrangian multiplier function is

Φ(G  X  Y) = kG − Wk2

F − hΛ  X − Gi + µ

2 kX − Gk2

F − hΣ  Y − Gi

+ µ

2 kY − Gk2

F +P(i j)∈T (cid:0) µ

2 G2

ij − Ωij Gij(cid:1)  

(27)

This is identical to Eq. (10)  except we added Ω as additional Lagrangian multiplier for the semi-
supervised constraints  i.e. the desired similarity Gij = 0 for (i  j) having different known class
labels. Here T = {(i  j) : yi 6= yj  i  j = 1  2  · · ·   ℓ}.
We modify Algorithm of Eqs. (11a–11f) to solve this problem. The updating of X and Y remains
the same as NLK algorithm described previously. To update G  we set ∂Φ(G  X  Y)/∂G = 0 and
obtain

Gij ←




max(cid:18) 2Wij + µ(Xij + Yij ) + Λij + Σij + Ωij
max(cid:18) 2Wij + µ(Xij + Yij ) + Λij + Σij
  0(cid:19)

2 + 2µ

2 + 3µ

  0(cid:19)

if yi 6= yj 

otherwise.

(28)

For Lagrangian multiplier Ω  the corresponding updating is

Ωij ← Ωij − µGij   ∀yi 6= yj.

(29)

Thus the semi-supervised learning algorithm is nearly identical to the unsupervised learning algo-
rithm — one strength of our uniﬁed NLK approach.

We summarize the NLK algorithms for unsupervised and semi-supervised learning in Algorithm 1.
In the algorithm  Lines 4 and 9 are updated for semi-supervised learning while other lines are shared.

6

Algorithm 1 NLK Algorithm For Supervised Learning and Semi-supervised Learning
Require: Weighted graph W  model parameters c  optimization parameter γ  partial label y for

semi-supervised learning.

1: Initialization: G = W  Λ = 0  Σ = 0  Ω = 0  µ = 1.
2: while Not converged do
3:
4:
5:
6:
7:
8:
9:
10:
11: end while
12: return G

For unsupervised learning  G ← max(cid:16) 2W+µ(X+Y)+Λ+Σ
For semi-supervised learning  update G using Eq. (28).
X ← UD+U⊺ where UDU⊺ = G + Λ/µ.
Y ← USU⊺ where UDU⊺ = G + Σ/µ and S is computed by Eq. (26).
Λ ← Λ − µ (X − G)
Σ ← Σ − µ (Y − G)
For semi-supervised learning  Ωij ← Ωij − µGij   ∀yi 6= yj.
µ ← γµ.

  0(cid:17) .

2+2µ

3.4 Theoretical Analysis of The Algorithm

Since the objective function and all the constraints are convex  we have the following [12]
Theorem 3.2. Algorithm 1 converges to the global solution of Eq. (5) or Eq. (7).

Notice that this conclusion is stronger than that in the related research papers [13] for graph learning.

4 Experimental Validation

As mentioned in the introduction section  the optimization results for NLK (Eq. (5)) can be used
as preprocessing for any graph based methods. Here we evaluated NLK on several state-of-the-art
graph based learning models  include Normalized Cut (Ncut) [19] for unsupervised learning and
Gaussian Fields and Harmonic Functions (GFHF) and local and global consistency learning (LGC)
for semi-supervised learning. We compare the clustering in both clustering accuracy and normalized
mutual information (NMI). For the semi-supervised learning model (Eq. (7))  we evaluate the our
models on GFHF and LGC models. For semi-supervised learning  we measure the classiﬁcation
accuracy. We verify the algorithms on four data sets: AT&T (n = 400  p = 644  K = 40)  BinAlpha
(n = 1404  p = 320  K = 36)  Segment (n = 2310  p = 19  K = 7)  and Vehicle(n = 946  p =
18  K = 4) from UCI data [9]  where n  p  and K are the number of data points  features  and
classes  respectively.

4.1 Experimental Settings

For clustering  we compare three similarity matrices: (1) original from Gaussian kernel matrix 

wij = exp(cid:0)−kxi − xjk2/2σ2(cid:1)  where σ is set to the average pairwise distances among all the data

points. (2) the BBS (Bregmanian Bi-Stochastication) [20]  and our method (NLK). The clustering
algorithm of Normalized Cut [19] is applied on the three similarity matrices. Then we have total
three clustering approaches: Normalized Cut (Ncut)  BBS+Ncut  and NLK+Ncut. For each cluster-
ing method  we try 100 random trials for different clustering initializations. For the semi-supervised
learning  we test three basic graph-based semi-supervised learning models. Gaussian Fields and
Harmonic Functions (GFHF) [23]  Local and Global Consistency learning (LGC) [22]  and Green’s
function (Green) [6]. We compare 4 types of similarity matrices: original Gaussian kernel matrix  as
discussed before  BBS  NLK  and NLK with semi-supervised constraints (model in Eq. (7)  denoted
by NLK Semi). Then we totally have 3 × 4 methods. For each method  we random split the data
to 30%/70% where 30% is is used as labeled data an the other 70% as the testing data. We try 100
random split and we report the average and standard deviations.
4.2 Parameter Settings

For all the similarity learning approaches (BBS  NLK  and NLK Semi)  we set the convergent crite-
ria as follows. If kGt+1 − Gtk2
F < 10−10 we stop the algorithms. For our methods (NLK

F /kGtk2

7

Table 1: Clustering accuracy and NMI comparison over 3 methods  Normalized Cut (Ncut) 
BBS+Ncut  and NLK+Ncut on 4 data sets. The best results are highlighted in bold.

Ncut

Accuracy
BBS+Ncut

NMI

0.607 ± 0.022 0.686 ± 0.021 0.767 ± 0.006 0.785 ± 0.025 0.836 ± 0.026 0.873 ± 0.025
AT&T
BinAlpha 0.431 ± 0.018 0.444 ± 0.022 0.490 ± 0.009 0.618 ± 0.013 0.629 ± 0.015 0.673 ± 0.011
0.613 ± 0.018 0.593 ± 0.009 0.616 ± 0.002 0.528 ± 0.016 0.579 ± 0.013 0.538 ± 0.002
Segment
0.383 ± 0.001 0.383 ± 0.000 0.426 ± 0.000 0.121 ± 0.001 0.122 ± 0.000 0.184 ± 0.000
Vehicle

NLK+Ncut

Ncut

BBS+Ncut

NLK+Ncut

and NLK Semi)  there is one model parameter c  which is always set to be c = 0.5kWk∗ where W
is the input similarity matrix.

4.3 Experimental Results

We show that clustering results in Table 1 where we compare both measurements (accuracy  NMI)
over 3 methods on 4 data sets. For each method  we report the average performance and the corre-
sponding standard deviation. Out of 4 data sets  our method outperforms all the other methods with
all the measurements on 3 data sets (AT&T  BinAlpha  and Vehicle).

In
We also test the semi-supervised learning performance over the 12 methods on 4 data sets.
each method on each data  we show the original performance values with dots. Shown are also the
average accuracies and the corresponding standard deviations. Out of 4 data sets  our method (NLK
and NLK Semi) outperform the other methods.

(a) GFHF
Original
BBS
NLK
NLK_Semi

AT&T

BinAlpha

Segmentation

Vehicle

0.4807 ± 0.0419

0.5981 ± 0.0465

0.7104 ± 0.0277

0.7121 ± 0.0289

0.4720 ± 0.0772

0.4715 ± 0.0803

0.4843 ± 0.0770

0.4931 ± 0.0777

0.7445 ± 0.0442

0.7926 ± 0.0331

0.8038 ± 0.0290

0.8500 ± 0.0343

(b) LGC

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.7

0.8

0.9

1

0.65

0.7

0.75

0.8

Original
BBS
NLK
NLK_Semi

0.5

(c) Green
Original
BBS
NLK
NLK_Semi

0.6561 ± 0.0480

0.6605 ± 0.0480

0.6649 ± 0.0452

0.7213 ± 0.0451

0.4250 ± 0.0745

0.4252 ± 0.0755

0.4367 ± 0.0791

0.4805 ± 0.0789

0.4881 ± 0.0760

0.5892 ± 0.0656

0.6850 ± 0.0526

0.6997 ± 0.0518

0.6

0.7

0.8

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.7148 ± 0.0261

0.7176 ± 0.0269

0.7340 ± 0.0326

0.7774 ± 0.0349

0.5497 ± 0.0768

0.5527 ± 0.0752

0.5545 ± 0.0785

0.5584 ± 0.0806

0.6887 ± 0.0320

0.7143 ± 0.0325

0.7539 ± 0.0366

0.7648 ± 0.0395

0.7

0.8

0.9

1

0.2

0.4

0.6

0.8

0.7

0.8

0.9

1

0.4

0.6

0.8

0.6729 ± 0.0195

0.6802 ± 0.0188

0.6857 ± 0.0179

0.7257 ± 0.0213

0.4936 ± 0.0671

0.5112 ± 0.0666

0.6233 ± 0.0292

0.6707 ± 0.0313

0.5299 ± 0.0266

0.5352 ± 0.0255

0.5965 ± 0.0215

0.6132 ± 0.0274

Figure 2: Semi-supervised learning performance over the 12 methods on 4 data sets. Original
accuracy value for each random split is plotted with dots. Shown are also the average accuracies and
the corresponding standard deviations.

5 Conclusions and Discussion

In this paper  we derive a similarity learning model based on convex optimizations. We demonstrate
that the low rank and positive semideﬁnite constraints are nature in the similarity. Further more 
we develop new sufﬁcient algorithm to obtain global solution with theoretical guarantees. We also
develop more optimization techniques that are potentially useful in the related eigenvalues or singu-
lar values constraints optimization. The presented model is veriﬁed on extensive experiments  and
the results show that our method enhances the quality of the similarity matrix signiﬁcantly  in both
clustering and semi-supervised learning.
Acknowledgement This research was partially supported by NSF-CCF 0830780  NSF-DMS
0915228  NSF-CCF 0917274  NSF-IIS 1117965.

8

References

[1] E. Airoldi  D. Blei  E. Xing  and S. Fienberg. A latent mixed membership model for relational
data. In Proceedings of the 3rd international workshop on Link discovery  pages 82–89. ACM 
2005.

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-

sentation. Neural computation  15(6):1373–1396  2003.

[3] T. B¨uhler and M. Hein. Spectral Clustering based on the graph p-Laplacian. In Proceedings of

the 26th Annual International Conference on Machine Learning  pages 81–88. ACM  2009.

[4] J. Cai  E. Candes  and Z. Shen. A singular value thresholding algorithm for matrix completion.

IEEE Trans. Inform. Theory  56(5)  2053-2080  (5):2053–2080  2008.

[5] E. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–

936  2010.

[6] C. Ding  R. Jin  T. Li  and H. Simon. A learning framework using Green’s function and
kernel regularization with application to recommender system. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge discovery and data mining  pages 260–269.
ACM  2007.

[7] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the l 1-
ball for learning in high dimensions. In Proceedings of the 25th international conference on
Machine learning  pages 272–279. ACM  2008.

[8] M. Fazel. Matrix rank minimization with applications. PhD thesis  Stanford University  2002.
[9] A. Frank and A. Asuncion. UCI machine learning repository  2010.
[10] M. Gu  H. Zha  C. Ding  X. He  H. Simon  and J. Xia. Spectral relaxation models and structure
analysis for k-way graph clustering and bi-clustering. UC Berkeley Math Dept Tech Report 
2001.

[11] L. Hagen and A. Kahng. New spectral methods for ratio cut partitioning and cluster-
ing. Computer-Aided Design of Integrated Circuits and Systems  IEEE Transactions on 
11(9):1074–1085  2002.

[12] R. Lewis  V. Torczon  and L. R. Center. A globally convergent augmented lagrangian pattern
search algorithm for optimization with general constraints and simple bounds. SIAM Journal
on Optimization  12(4):1075–1089  2002.

[13] W. Liu and S. Chang. Robust multi-class transductive learning with graphs. 2009.
[14] D. Luo  C. Ding  and H. Huang. Graph evolution via social diffusion processes. Machine

Learning and Knowledge Discovery in Databases  pages 390–404  2011.

[15] D. Luo  C. Ding  F. Nie  and H. Huang. Cauchy graph embedding. ICML2011  pages 553–560 

2011.

[16] A. Ng  M. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances

in neural information processing systems  2:849–856  2002.

[17] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-

ence  290(5500):2323  2000.

[18] H. Seung and D. Lee.
290(5500):2268–9  2000.

The manifold ways of perception.

Science(Washington) 

[19] J. Shi and J. Malik. Normalized cuts and image segmentation. Pattern Analysis and Machine

Intelligence  IEEE Transactions on  22(8):888–905  2002.

[20] F. Wang  P. Li  and A. K¨onig. Learning a Bi-Stochastic Data Similarity Matrix. In 2010 IEEE

International Conference on Data Mining  pages 551–560. IEEE  2010.

[21] F. Wang and C. Zhang. Label propagation through linear neighborhoods. IEEE Transactions

on Knowledge and Data Engineering  pages 55–67  2007.

[22] D. Zhou  O. Bousquet  T. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global
consistency. In Advances in Neural Information Processing Systems 16: Proceedings of the
2003 Conference  pages 595–602  2004.

[23] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and

harmonic functions. In ICML 2003.

9

,Matus Telgarsky
Sanjoy Dasgupta
Reza Babanezhad Harikandeh
Mark Schmidt
Scott Sallinen
Pan Li
Olgica Milenkovic
Liuyi Yao
Sheng Li
Yaliang Li
Mengdi Huai
Jing Gao
Aidong Zhang