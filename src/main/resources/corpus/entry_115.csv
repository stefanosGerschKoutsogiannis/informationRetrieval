2018,A Lyapunov-based Approach to Safe Reinforcement Learning,In many real-world reinforcement learning (RL) problems  besides optimizing the main objective function  an agent must concurrently avoid violating a number of constraints. In particular  besides optimizing performance  it is crucial to guarantee the safety of an agent during training as well as deployment (e.g.  a robot should avoid taking actions - exploratory or not - which irrevocably harm its hard- ware). To incorporate safety in RL  we derive algorithms under the framework of constrained Markov decision processes (CMDPs)  an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions  which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings  we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness  we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.,A Lyapunov-based Approach to Safe Reinforcement

Learning

Yinlam Chow

DeepMind

yinlamchow@google.com

Oﬁr Nachum
Google Brain

ofirnachum@google.com

Mohammad Ghavamzadeh

Facebook AI Research

mgh@fb.com
Abstract

Edgar Duenez-Guzman

DeepMind

duenez@google.com

In many real-world reinforcement learning (RL) problems  besides optimizing the
main objective function  an agent must concurrently avoid violating a number of
constraints. In particular  besides optimizing performance  it is crucial to guar-
antee the safety of an agent during training as well as deployment (e.g.  a robot
should avoid taking actions - exploratory or not - which irrevocably harm its hard-
ware). To incorporate safety in RL  we derive algorithms under the framework
of constrained Markov decision processes (CMDPs)  an extension of the standard
Markov decision processes (MDPs) augmented with constraints on expected cu-
mulative costs. Our approach hinges on a novel Lyapunov method. We deﬁne
and present a method for constructing Lyapunov functions  which provide an ef-
fective way to guarantee the global safety of a behavior policy during training
via a set of local linear constraints. Leveraging these theoretical underpinnings 
we show how to use the Lyapunov approach to systematically transform dynamic
programming (DP) and RL algorithms into their safe counterparts. To illustrate
their effectiveness  we evaluate these algorithms in several CMDP planning and
decision-making tasks on a safety benchmark domain. Our results show that our
proposed method signiﬁcantly outperforms existing baselines in balancing con-
straint satisfaction and performance.

Introduction

1
Reinforcement learning (RL) has shown exceptional successes in a variety of domains such as video
games [25] and recommender systems [40]  where the main goal is to optimize a single return.
However  in many real-world problems  besides optimizing the main objective (the return)  there
can exist several conﬂicting constraints that make RL challenging. In particular  besides optimizing
performance it is crucial to guarantee the safety of an agent in deployment [5  32  33]  as well as
during training [2]. For example  a robot should avoid taking actions which irrevocably harm its
hardware; a recommender system must avoid presenting harmful or offending items to users.
Sequential decision-making in non-deterministic environments has been extensively studied in the
literature under the framework of Markov decision processes (MDPs). To incorporate safety into the
RL process  we are particularly interested in deriving algorithms under the context of constrained
Markov decision processes (CMDPs)  which is an extension of MDPs with expected cumulative
constraint costs. The additional constraint component of CMDPs increases ﬂexibility in modeling
problems with trajectory-based constraints  when compared with other approaches that customize
immediate costs in MDPs to handle constraints [34]. As shown in numerous applications from robot
motion planning [30  26  11]  resource allocation [24  18]  and ﬁnancial engineering [1  41]  it is
more natural to deﬁne safety over the whole trajectory  instead of over particular state and action
pairs. Under this framework  we denote an agent’s behavior policy to be safe if it satisﬁes the
cumulative cost constraints of the CMDP.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Despite the capabilities of CMDPs  they have not been very popular in RL. One main reason is that 
although optimal policies of ﬁnite CMDPs are Markov and stationary  and with known models the
CMDP can be solved using linear programming (LP) [3]  it is unclear how to extend this algorithm
to handle cases when the model is unknown  or when the state and action spaces are large or contin-
uous. A well-known approach to solve CMDPs is the Lagrangian method [4  15]  which augments
the standard expected reward objective with a penalty on constraint violation. With a ﬁxed Lagrange
multiplier  one can use standard dynamic programming (DP) or RL algorithms to solve for an opti-
mal policy. With a learnable Lagrange multiplier  one must solve the resulting saddle point problem.
However  several studies [21] showed that iteratively solving the saddle point is apt to run into nu-
merical stability issues. More importantly  the Lagrangian policy is only safe asymptotically and
makes little guarantee with regards to safety of the behavior policy during each training iteration.
Motivated by these observations  several recent works have derived surrogate algorithms for solving
CMDPs  which transform the original constraint to a more conservative one that yields an easier
problem to solve. A straight-forward approach is to replace the cumulative constraint cost with
a conservative stepwise surrogate constraint [9] that only depends on the current state-action pair.
Since this surrogate constraint can be easily embedded into the admissible control set  this formu-
lation can be modeled by an MDP that has a restricted set of admissible actions. Another surrogate
algorithm was proposed by [14] in which the algorithm ﬁrst computes a uniform super-martingale
constraint value function surrogate w.r.t. all policies  and then ﬁnds a CMDP feasible policy by op-
timizing the surrogate problem using the lexicographical ordering method [39]. These methods are
advantageous in the sense that (i) there are RL algorithms available to handle the surrogate problems
(for example see [12] for the step-wise surrogate and [27] for the super-martingale surrogate)  (ii)
the policy returned by this method is safe  even during training. However  the main drawback of
these approaches is their conservativeness. Characterizing sub-optimality performance of the cor-
responding solution policy also remains a challenging task. On the other hand  recently in policy
gradient  [2] proposed the constrained policy optimization (CPO) method that extends trust-region
policy optimization (TRPO) to handle the CMDP constraints. While this algorithm is scalable and
its policy is safe during training  applying this methodology to more general RL algorithms (that are
not in the family of proximal PG algorithms) is quite non-trivial.
Lyapunov functions have been extensively used in control theory to analyze the stability of dynamic
systems [20  28]. A Lyapunov function is a type of scalar potential function that keeps track of the
energy that a system continually dissipates. Besides modeling physical energy  Lyapunov functions
can also represent abstract quantities  such as the steady-state performance of a Markov process [16].
In many ﬁelds  Lyapunov functions provide a powerful paradigm to translate global properties of
a system to local ones and vice-versa. Using Lyapunov functions in RL was ﬁrst studied by [31] 
where Lyapunov functions were used to guarantee closed-loop stability of an agent. Recently [6]
used Lyapunov functions to guarantee a model-based RL agent’s ability to re-enter an “attraction
region” during exploration. However  no previous works have used Lyapunov approaches to explic-
itly model constraints in a CMDP. Furthermore  one major drawback of these approaches is that the
Lyapunov functions are hand-crafted  and there are no principled guidelines on designing Lyapunov
functions that can guarantee the agent’s performance.
The contribution of this paper is four-fold. First  we formulate the problem of safe RL as a CMDP
and propose a novel Lyapunov approach to solve it. While the main challenge of other Lyapunov-
based methods is to design a Lyapunov function candidate  we propose an LP-based algorithm to
construct Lyapunov functions w.r.t. generic CMDP constraints. We also show that our method is
guaranteed to always return a feasible policy  and under certain technical assumptions  it achieves
optimality. Second  leveraging the theoretical underpinnings of the Lyapunov approach  we present
two safe DP algorithms – safe policy iteration (SPI) and safe value iteration (SVI) – and analyze the
feasibility and performance of these algorithms. Third  to handle unknown environments and large
state/action spaces  we develop two scalable safe RL algorithms – (i) safe DQN  an off-policy ﬁtted
Q-iteration method  and (ii) safe DPI  an approximate policy iteration method. Fourth  to illustrate
the effectiveness of these algorithms  we evaluate them in several tasks on a benchmark 2D planning
problem and show that they outperform common baselines in terms of balancing performance and
constraint satisfaction.
2 Preliminaries
We consider RL problems in which the agent’s interaction with the system is modeled as a
Markov decision process (MDP). A MDP is a tuple (X  A  c  P  x0)  where X = X (cid:48) ∪ {xTerm}
is the state space  with transient state space X (cid:48) and terminal state xTerm; A is the action space;

2

set of Markov stationary policies  i.e.  ∆(x) = {π(·|x) : X → R≥0s : (cid:80)

c(x  a) ∈ [0  Cmax] is the immediate cost function (negative reward); P (·|x  a) is the transition
probability distribution; and x0 ∈ X (cid:48) is the initial state. Our results easily generalize to random ini-
tial states and random costs  but for simplicity we will focus on the case of deterministic initial state
and immediate cost. In a more general setting where cumulative constraints are taken into account 
we deﬁne a constrained Markov decision process (CMDP)  which extends the MDP model by in-
troducing additional costs and associated constraints. A CMDP is deﬁned by (X  A  c  d  P  x0  d0) 
where the components X  A  c  P  x0 are the same for the unconstrained MDP; d(x) ∈ [0  Dmax] is
the immediate constraint cost; and d0 ∈ R≥0 is an upper-bound on the expected cumulative (through
time) constraint cost. To formalize the optimization problem associated with CMDPs  let ∆ be the
a π(a|x) = 1}  for any
state x ∈ X . Also let T∗ be a random variable corresponding to the ﬁrst-hitting time of the terminal
state xTerm induced by policy π. In this paper  we follow the standard notion of transient MDPs and
assume that the ﬁrst-hitting time is uniformly bounded by an upper bound T for any stationary poli-
cies [10]. This assumption implies that every stationary policy is proper [7]  whose induced Markov
chain has an absorbing property (see [13] for an example). While this assumption may seem restric-
tive  it is a standard one in stochastic shortest path problems for showing that the Bellman operator
is a contraction. Its justiﬁcation follows from the fact that sample trajectories collected in most RL
algorithms consist of a ﬁnite stopping time (also known as a time-out); In general this assumption
may also be relaxed in cases where a discount factor γ < 1 is applied on future costs. For notational
convenience  at each state x ∈ X (cid:48)  we deﬁne the generic Bellman operator w.r.t. policy π ∈ ∆ and

(cid:104)
h(x  a)+(cid:80)
generic cost function h: Tπ h[V ](x) =(cid:80)
a π(a|x)
E(cid:2)(cid:80)T∗−1
c(xt  at) | x0  π(cid:3)  and the safety constraint is deﬁned as Dπ(x0) ≤ d0  where the safety
Given a policy π ∈ ∆  an initial state x0 
constraint function is given by Dπ(x0) := E(cid:2)(cid:80)T∗−1
t=0 d(xt) | x0  π(cid:3). In general the CMDP problem

the cost function is deﬁned as Cπ(x0)

x(cid:48)∈X (cid:48)P (x(cid:48)|x  a)V (x(cid:48))

(cid:105)

:=

t=0

.

we wish to solve is given as follows:

Problem OPT : Given an initial state x0 and a threshold d0 
minπ∈∆
policy is denoted by π∗.

(cid:9) . If there is a non-empty solution  the optimal

(cid:8)Cπ(x0) : Dπ(x0) ≤ d0

solve

Under the transient CMDP assumption  Theorem 8.1 in [3] shows that if the feasibility set is non-
empty  then there exists an optimal policy in the class of stationary Markovian policies ∆. To
motivate the CMDP formulation studied in this paper  in Appendix A  we include two real-world
examples in modeling safety using (i) the reachability constraint  and (ii) the constraint that limits
the agent’s visits to undesirable states. Recently there has been a number of works on CMDP
algorithms; their details can be found in Appendix B.
3 A Lyapunov Approach to Solve CMDPs
In this section  we develop a novel methodology for solving CMDPs using the Lyapunov approach.
To start with  without loss of generality we assume to have access to a baseline feasible policy of
the OPT problem  namely πB ∈ ∆.1 We deﬁne a non-empty2 set of Lyapunov functions w.r.t. the
(cid:111)
(cid:110)
initial state x0 ∈ X and constraint threshold d0 as
LπB (x0  d0) =
L : X →R≥0 : TπB  d[L](x)≤ L(x) ∀x ∈ X (cid:48); L(x) = 0  ∀x ∈ X\X (cid:48); L(x0) ≤ d0
(cid:8)π(·|x) ∈ ∆ : Tπ d[L](x) ≤ L(x)(cid:9) the set of L−induced Markov stationary policies. Since Tπ d
(1)
For any arbitrary Lyapunov function L ∈ LπB (x0  d0)  we denote by FL(x) =
is a contraction mapping [7]  any L−induced policy π has the following property: Dπ(x) =
π d[L](x) ≤ L(x)  ∀x ∈ X (cid:48). Together with the property of L(x0) ≤ d0  this further
limk→∞ T k
implies any L−induced policy is a feasible policy of the OPT problem. However  in general the
set FL(x) does not necessarily contain any optimal policies of the OPT problem   and our main
contribution is to design a Lyapunov function (w.r.t. a baseline policy) that provides this guarantee.
In other words  our main goal is to construct a Lyapunov function L ∈ LπB (x0  d0) such that

.

L(x) ≥ Tπ∗ d[L](x)  L(x0) ≤ d0.

(2)

1One example of πB is a policy that minimizes the constraint  i.e.  πB(·|x) ∈ arg minπ∈∆(x) Dπ(x).
2To see this  the constraint cost function DπB (x) is a valid Lyapunov function  i.e.  DπB (x0) ≤ d0 
  ∀x ∈ X (cid:48).

DπB (x) = 0  ∀x ∈ X \ X (cid:48)  and DπB (x) = TπB  d[DπB ](x) = E

(cid:104)(cid:80)T∗−1

t=0 d(xt) | πB  x

(cid:105)

3

(cid:104)(cid:80)T∗−1

(cid:105)
t=0 d(xt) + (xt) | πB  x

L∗ (x) ≥ TπB  d[L∗ ](x)  L∗ (x) ≥ max(cid:8)Dπ∗ (x) DπB (x)(cid:9) ≥ 0  ∀x ∈ X (cid:48).

Before getting into the main results  we consider the following important technical lemma  which
states that with appropriate cost-shaping  one can always transform the constraint value function
Dπ∗ (x) w.r.t. optimal policy π∗ into a Lyapunov function that is induced by πB  i.e.  L(x) ∈
LπB (x0  d0). The proof of this lemma can be found in Appendix C.1.
Lemma 1. There exists an auxiliary constraint cost  : X (cid:48) → R such that a Lyapunov function is
  ∀x ∈ X (cid:48)  and L(x) = 0  ∀x ∈ X \ X (cid:48).
given by L(x) = E
Moreover  L is equal to the constraint value function w.r.t. π∗  i.e.  L(x) = Dπ∗ (x).
From the structure of L  one can see that the auxiliary constraint cost function  is uniformly
bounded by ∗(x) := 2TDmaxDT V (π∗||πB)(x) 3 i.e.  (x) ∈ [−∗(x)  ∗(x)]  for any x ∈ X (cid:48).
However  in general it is unclear how to construct such a cost-shaping term  without explicitly
knowing π∗ a-priori. Rather  inspired by this result  we consider the bound ∗ to propose a Lyapunov
function candidate L∗. Immediately from its deﬁnition  this function has the following properties:
(3)
The ﬁrst property is due to the facts that: (i) ∗ is a non-negative cost function; (ii) TπB  d+∗ is a
contraction mapping  which by the ﬁxed point theorem [7] implies L∗ (x) = TπB  d+∗ [L∗ ](x) ≥
TπB  d[L∗ ](x)  ∀x ∈ X (cid:48). For the second property  from the above inequality one concludes that
the Lyapunov function L∗ is a uniform upper-bound to the constraint cost  i.e.  L∗ (x) ≥ DπB (x) 
because the constraint cost DπB (x) w.r.t. policy πB is the unique solution to the ﬁxed-point equation
TπB  d[V ](x) = V (x)  x ∈ X (cid:48). On the other hand  by construction  ∗(x) is an upper-bound of the
cost-shaping term (x). Therefore  Lemma 1 implies that the Lyapunov function L∗ is a uniform
upper-bound to the constraint cost w.r.t. optimal policy π∗  i.e.  L∗ (x) ≥ Dπ∗ (x).
To show that L∗ is a Lyapunov function that satisﬁes (2)  we propose the following condition that
enforces a baseline policy πB to be sufﬁciently close to an optimal policy π∗.
Assumption 1. The feasible baseline policy πB satisﬁes the condition maxx∈X (cid:48) ∗(x) ≤ Dmax ·
min
This condition characterizes the maximum allowable distance between πB and π∗  such that the set
of L∗−induced policies contains an optimal policy. To formalize this claim  we have the following
main result showing that L∗ ∈ LπB (x0  d0)  and the set of policies FL∗ contains an optimal policy.
Theorem 1. Suppose the baseline policy πB satisﬁes Assumption 1  then on top of the properties in
(3)  the Lyapunov function candidate L∗ also satisﬁes the properties in (2)  and thus  its induced
feasible set of policies FL∗ contains an optimal policy.
The proof of this theorem is given in Appendix C.2. Suppose the distance between the baseline and
optimal policies can be estimated effectively. Using the above result  one can immediately determine
if the set of L∗−induced policies contain an optimal policy. Equipped with the set of L∗−induced
feasible policies  consider the following safe Bellman operator:

  where D = maxx∈X (cid:48) maxπ Dπ(x).

(cid:110) d0−DπB (x0)

TDmax

(cid:111)

  TDmax−D
TDmax+D

(cid:26) minπ∈FL∗ (x) Tπ c[V ](x)

0

T [V ](x) =

if x ∈ X (cid:48)
otherwise .

(4)

Using standard analysis of Bellman operators  one can show that T is a monotonic and contraction
operator (see Appendix C.3 for the proof). This further implies that the solution of the ﬁxed-point
equation T [V ](x) = V (x)  ∀x ∈ X is unique. Let V ∗ be such a value function. The following
theorem shows that under Assumption 1  V ∗(x0) is a solution to the OPT problem.
Theorem 2. Suppose that the baseline policy πB satisﬁes Assumption 1. Then  the ﬁxed-point
solution at x = x0  i.e.  V ∗(x0)  is equal to the solution of the OPT problem. Furthermore  an
optimal policy can be constructed by π∗(·|x)∈ arg minπ∈FL∗ (x) Tπ c[V ∗](x)  ∀x∈X (cid:48).
The proof of this theorem can be found in Appendix C.4. This shows that under Assumption 1 
an optimal policy of the OPT problem can be found using standard DP algorithms. Note that
verifying whether πB satisﬁes this assumption is still challenging  because one requires a good
estimate of DT V (π∗||πB). Yet to the best of our knowledge  this is the ﬁrst result that connects
(cid:80)
the optimality of CMDP to Bellman’s principle of optimality. Another key observation is that in
a∈A |πB(a|x) − π∗(a|x)|.

3The deﬁnition of total variation distance is given by DT V (π∗||πB)(x) = 1

2

4

practice  we will explore ways of approximating ∗ via bootstrapping and empirically show that this
approach achieves good performance  while guaranteeing safety at each iteration. In particular  in
the next section  we will illustrate how to systematically construct a Lyapunov function using an
LP in both planning and RL (when the model is unknown and/or we use function approximation)
scenarios in order to guarantee safety during learning.
4 Safe Reinforcement Learning Using Lyapunov Functions
Motivated by the challenge of computing a Lyapunov function L∗ such that its induced set of

policies contains π∗  in this section  we approximate ∗ with an auxiliary constraint cost(cid:101)  which is
and the safety condition L(cid:101)(x0) ≤ d0. The larger the(cid:101)  the larger the set of policies FL(cid:101). Thus  by
the largest auxiliary cost that satisﬁes the Lyapunov condition: L(cid:101)(x) ≥ TπB  d[L(cid:101)](x)  ∀x ∈ X (cid:48) 
choosing the largest such auxiliary cost  we hope to have a better chance of including the optimal
policy π∗ in the set of feasible policies. So  we consider the following LP problem:

(x) : d0 − DπB (x0) ≥ 1(x0)
(cid:62)

(I − {P (x

(cid:48)|x  πB)}x x(cid:48)∈X (cid:48) )

−1

.

(5)

(cid:101) ∈ arg max

:X (cid:48)→R≥0

(cid:110)(cid:88)

x∈X (cid:48)

(cid:111)

Here 1(x0) represents a one-hot vector in which the non-zero element is located at x = x0.
visiting probability E[(cid:80)T∗−1
On the other hand  whenever πB is a feasible policy  the problem in (5) always has a non-empty
solution.4 Furthermore  note that 1(x0)(cid:62)(I − {P (x(cid:48)|x  πB)}x x(cid:48)∈X (cid:48))−11(x) represents the total
t=0 1{xt = x} | x0  πB] from the initial state x0 to any state x ∈ X (cid:48) 
which is a non-negative quantity. Therefore  using the extreme point argument in LP [23]  one
i.e. (cid:101)(x) = (d0 − DπB (x0)) · 1{x = x}/E[(cid:80)T∗−1
can simply conclude that the maximizer of problem (5) is an indicator function whose non-zero
x ∈ arg minx∈X (cid:48) E[(cid:80)T∗−1
element locates at state x that corresponds to the minimum total visiting probability from x0 
t=0 1{xt = x} | x0  πB] ≥ 0  ∀x ∈ X (cid:48)  where
restrict the structure of(cid:101)(x) to be a constant function  i.e. (cid:101)(x) = (cid:101)  ∀x ∈ X (cid:48). Then  one can
t=0 1{xt = x} | x0  πB]. On the other hand  suppose that we further
show that the maximizer is given by(cid:101)(x) = (d0 − DπB (x0))/E[T∗ | x0  πB]  ∀x ∈ X (cid:48)  where
sonable approximation is to replace the denominator of(cid:101) with the upper-bound T.
Using this Lyapunov function L(cid:101)  we propose the safe policy iteration (SPI) in Algorithm 1  in which
the Lyapunov function is updated via bootstrapping  i.e.  at each iteration L(cid:101) is recomputed using (5) 

1(x0)(cid:62)(I − {P (x(cid:48)|x  πB)}x x(cid:48)∈X (cid:48))−1[1  . . .   1](cid:62) = E[T∗ | x0  πB] is the expected stopping time
of the transient MDP. In cases where computing the expected stopping time is expensive  one rea-

w.r.t. the current baseline policy. Properties of SPI are summarized in the following proposition.

Algorithm 1 Safe Policy Iteration (SPI)

Input: Initial feasible policy π0;
for k = 0  1  2  . . . do

Step 0: With πb = πk  evaluate the Lyapunov function Lk  where k is a solution of (5)
Step 1: Evaluate the cost value function Vπk (x) = Cπk (x)
Step 2: Update the policy by solving the problem πk+1(·|x) ∈ argminπ∈FLk
end for
Return Final policy πk∗

(x) Tπ c[Vπk ](x) ∀x ∈ X (cid:48)

Proposition 1. Algorithm 1 has the following properties: (i) Consistent Feasibility  i.e.  suppose that
the current policy πk is feasible  then the updated policy πk+1 is also feasible  i.e.  Dπk (x0) ≤ d0
implies Dπk+1(x0) ≤ d0; (ii) Monotonic Policy Improvement  i.e.  the cumulative cost induced by
πk+1 is lower than or equal to that by πk  i.e.  Cπk+1(x) ≤ Cπk (x)  ∀x ∈ X (cid:48); (iii) Convergence 
i.e.  if we add a strictly concave regularizer to the optimization problem (5) and a strictly convex
regularizer to the policy optimization step  then the policy sequence asymptotically converges.5
The proof of this proposition is given in Appendix C.5  and the sub-optimality performance bound
of SPI can be found in Appendix C.6. Analogous to SPI  we also propose a safe value iteration
(SVI)  in which the Lyapunov function estimate is updated at every iteration via bootstrapping 
using the current optimal value estimate. Details of SVI is given in Algorithm 2 and its properties
are summarized in the following proposition  whose proof is given in Appendix C.7.

4This is due to the fact that d0 − DπB (x0) ≥ 0  and thus (cid:101)(x) = 0 is a feasible solution.

5The strict concavity property in the objective function is mainly for the purpose of tie-breaking. One

standard example is the entropy regularizer with a small regularization term.

5

Proposition 2. Algorithm 2 has: (i) Consistent Feasibility and (ii) Convergence.
To justify the notion of bootstrapping in both SVI and SPI  the Lyapunov function is updated based
on the best baseline policy (the policy that is feasible and by far has the lowest cumulative cost).
Once the current baseline policy πk is sufﬁciently close to an optimal policy π∗  then by Theorem 1 
one may conclude that the L(cid:101)−induced set of policies contains an optimal policy. Although these
algorithms do not have optimality guarantees  empirically  they often return a near-optimal policy.
At each iteration  the policy optimization step in SPI and SVI requires solving |X (cid:48)| LP sub-problems 
where each of them has |A| + 2 constraints and has a |A|−dimensional decision-variable. Collec-
tively  at each iteration its complexity is O(|X (cid:48)||A|2(|A| + 2)). While in the worst case SVI con-
verges in K = O(T) steps [7] and SPI converges in K = O(|X (cid:48)||A|T log T) steps [38]  in practice 
K is much smaller than |X (cid:48)||A|. Therefore  even with the additional complexity of policy evaluation
in SPI that is O(T|X (cid:48)|2)  or the complexity of updating Q−function in SVI that is O(|A|2|X (cid:48)|2)  the
complexity of these methods is O(K|X (cid:48)||A|3 + K|X (cid:48)|2|A|2)  which in practice is much lower than
that of the dual LP method  whose complexity is O(|X (cid:48)|3|A|3) (see Appendix B for more details).

Algorithm 2 Safe Value Iteration (SVI)

Step 0: Compute Q-function Qk+1(x  a) = c(x  a) +(cid:80)
Input: Initial Q-function Q0; Initial Lyapunov function L0 w.r.t. auxiliary cost function 0(x) = 0;
for k = 0  1  2  . . . do
and policy πk(·|x) ∈ arg minπ∈FLk
Step 1: With πB = πk  construct the Lyapunov function Lk+1  where k+1 is a solution to (5);
end for
Return Final policy πk∗

x(cid:48) P (x(cid:48)|x  a) minπ∈FLk

(x) π(·|x)(cid:62)Qk(x ·)

(x(cid:48)) π(·|x(cid:48))(cid:62)Qk(x(cid:48) ·)

(cid:110)

QL(x ·) ≤(cid:101)

(cid:48)

(cid:111)

4.1 Lyapunov-based Safe RL Algorithms
In order to improve scalability of SVI and SPI  we develop two off-policy safe RL algorithms  namely
safe DQN and safe DPI  which replace the value and policy updates in safe DP with function ap-
proximations. Their pseudo-codes can be found in Appendix D. Before going into their details  we
ﬁrst introduce the policy distillation method  which will be later used in the safe RL algorithms.
Policy Distillation: Consider the following LP problem for policy optimization in SVI and SPI:

(cid:48)

where QL(x  a) = d(x) +(cid:101)(cid:48)(x) +(cid:80)

(·|x) ∈ arg min
π∈∆

π(·|x)
(cid:62)

π

Q(x ·) : (π(·|x) − πB(·|x))
x(cid:48) P (x(cid:48)|x  a)L(cid:101)(cid:48)(x(cid:48)) is the state-action Lyapunov function.

(6)

(x)

(cid:62)

 

1
m

m=1

(cid:80)M

When the state-space is large (or continuous)  we shall use function approximation. Consider a
(cid:80)T−1
parameterized policy πφ with weights φ. Utilizing the distillation concept [36]  after comput-
ing the optimal action probabilities w.r.t. a batch of states  the policy πφ is updated by solving
t=0 DJSD(πφ(·|xt m) (cid:107) π(cid:48)(·|xt m))  where DJSD is the Jensen-Shannon
φ∗ ∈ arg minφ
divergence. Pseudo-code of distillation is given in Algorithm 3 in Appendix D.
Safe Q−learning (SDQN): Here we sample an off-policy mini-batch of state  action  cost  and
next-state from the replay buffer  and use it to update the value function estimates that minimize
the MSE losses of the Bellman residuals. We ﬁrst construct the state-action Lyapunov function

estimate (cid:98)QL(x  a; θD  θT ) = (cid:98)QD(x  a; θD) +(cid:101)(cid:48) · (cid:98)QT (x  a; θT ) by learning the constraint value net-
work (cid:98)QD and stopping time value network (cid:98)QT . With a current baseline policy πk  one can use
by(cid:101)(cid:48)(x) =(cid:101)(cid:48) = (d0 − πk(·|x0)(cid:62)(cid:98)QD(x0 ·; θD))/πk(·|x0)(cid:62)(cid:98)QT (x0 ·; θT ). Equipped with the Lya-

function approximation to approximate the auxiliary constraint cost (which is the solution to (5))

punov function  at each iteration  one can do a standard DQN update  except that the optimal action
probabilities are computed by solving (6). Details of SDQN is given in Algorithm 4 in Appendix D.
Safe Policy Improvement (SDPI): Similar to SDQN  in this algorithm  we ﬁrst sample an off-
policy mini-batch from the replay buffer and use it to update the value function estimates (w.r.t. ob-
jective  constraint  and stopping-time estimate) that minimize MSE losses. Different from SDQN 
in SDPI the value estimation is done using policy evaluation  which means that the objective
Q−function is trained to minimize the Bellman residual w.r.t. actions generated by the current pol-

icy πk  instead of the greedy actions. Using the same construction as in SDQN for auxiliary cost(cid:101)(cid:48)
and state-action Lyapunov function (cid:98)QL  we then perform a policy improvement step by computing

6

Figure 1: Results of various planning algorithms on the grid-world environment with obstacles 
with x-axis showing the obstacle density. From the leftmost column  the ﬁrst ﬁgure illustrates the
2D planning domain example (ρ = 0.25). The second and the third ﬁgures show the average return
and the average cumulative constraint cost of the CMDP methods  respectively. The fourth ﬁgure
displays all the methods used in the experiment. The shaded regions indicate the 80% conﬁdence
intervals. Clearly the safe DP algorithms compute policies that are safe and have good performance.

a set of greedy action probabilities from (6) and constructing an updated policy πk+1 using pol-
icy distillation. Assuming both value and policy approximations have low error  SDPI resembles
several interesting properties of SPI  such as maintaining safety during training and monotonically
improving the policy. To improve learning stability  instead of the full policy update  one can further
consider a partial update πk+1 = (1−α)πk+απ(cid:48)  where α ∈ (0  1) is a mixing constant that controls
safety and exploration [2  19]. Details of SDPI is summarized in Algorithm 5 in Appendix D.
5 Experiments
Motivated by the safety issues of RL in [22]  we validate our safe RL algorithms using a stochastic
2D grid-world motion planning problem. In this domain  an agent (e.g.  a robotic vehicle) starts
in a safe region and its objective is to travel to a given destination. At each time step  the agent
can move to any of its four neighboring states. Due to sensing and control noise  however  with
probability δ a move to a random neighboring state occurs. To account for fuel usage  the stage-wise
cost of each move until reaching the destination is 1  while the reward achieved for reaching the
destination is 1000. Thus  we would like the agent to reach the destination in the shortest possible
number of moves. In between the starting and destination points  there are number of obstacles
that the agent may pass through but should avoid for safety; each time the agent hits an obstacle it
incurs a constraint cost of 1. Thus  in the CMDP setting  the agent’s goal is to reach the destination
in the shortest possible number of moves  while hitting the obstacles at most d0 times or less. For
demonstration purposes  we choose a 25 × 25 grid-world (see Figure 1) with a total of 625 states.
We also have a density ratio ρ ∈ (0  1) that sets the obstacle-to-terrain ratio. When ρ is close to 0 
the problem is obstacle-free  and if ρ is close to 1  then the problem becomes more challenging. In
the normal problem setting  we choose a density ρ = 0.3  an error probability δ = 0.05  a constraint
threshold d0 = 5  and a maximum horizon of 200 steps. The initial state is located in (24  24) and
the goal is placed in (0  α)  where α ∈ [0  24] is a uniform random variable. To account for statistical
signiﬁcance  the results of each experiment are averaged over 20 trials.
CMDP Planning: In this task  we have explicit knowledge of the reward function and transition
probability. The main goal is to compare our safe DP algorithms (SPI and SVI) with the following
common CMDP baseline methods: (i) Step-wise Surrogate  (ii) Super-martingale Surrogate  (iii)
Lagrangian  and (iv) Dual LP. Since the methods in (i) and (ii) are surrogate algorithms  we will
also evaluate these methods with both value iteration and policy iteration. To illustrate the level of
sub-optimality  we will also compare the returns and constraint costs of these methods with baselines
that are generated by maximizing return or minimizing constraint cost of two separate MDPs. The
main objective here is to illustrate that safe DP algorithms are less conservative than other surrogate
methods  are more numerically stable than the Lagrangian method  and are more computationally
efﬁcient than the Dual LP method (see Appendix F)  without using function approximations.
Figure 1 presents the results on returns and cumulative constraint costs of the aforementioned CMDP
methods over a spectrum of ρ values  ranging from 0 to 0.5. In each method  the initial policy is a
conservative baseline policy πB that minimizes the constraint cost. The empirical results indicate
that although the polices generated by the four surrogate algorithms are feasible  they do not have
signiﬁcant policy improvement  i.e.  return values are close to that of the initial baseline policy.
Over all density settings  the SPI algorithm consistently computes a solution that is feasible and has
good performance. The policy returned by SVI is always feasible and has near-optimal performance

7

Discrete obs  d0 = 5 Discrete obs  d0 = 1

Image obs  d0 = 5

Image obs  d0 = 1

s
d
r
a
w
e
R

s
t
n
i
a
r
t
s
n
o
C

Figure 2: Results of various RL algorithms on the grid-world environment with obstacles  with x-
axis in thousands of episodes. We include runs using discrete observations (a one-hot encoding of
the agent’s position) and image observations (showing the entire RGB 2D map of the world). We
discover that the Lyapunov-based approaches can perform safe learning  despite the fact that the
model of the environment is not known and that deep function approximation is necessary.
when the obstacle density is low. However  due to numerical instability  its performance degrades as
ρ grows. Similarly  the Lagrangian methods return a near-optimal solution over most settings  but
due to numerical issues their solutions start to violate constraint as ρ grows.
Safe Reinforcement Learning: Here we present the results of RL algorithms on this safety task.
We evaluate their learning performance on two variants: one in which the observation is a one-hot
encoding of the agent’s location  and the other in which the observation is the 2D image represen-
tation of the grid map. In each of these  we evaluate performance when d0 = 1 and d0 = 5. We
compare our proposed safe RL algorithms  SDPI and SDQN  with their unconstrained counterparts 
DPI and DQN  as well as the Lagrangian approach to safe RL  in which the Lagrange multiplier is
optimized via extensive grid search. Details of the experimental setup are given in Appendix F. To
make the tasks more challenging  we initialize the RL algorithms with a randomized baseline policy.
Figure 2 shows the results of these methods across all task variants. We observe that SDPI and
SDQN can adequately solve the tasks and compute good return performance (similar to that of
DQN and DPI in some cases)  while guaranteeing safety. Another interesting observation in the
SDQN and SDPI algorithms is that  once the algorithm ﬁnds a safe policy  then all updated policies
remain safe throughout training. On the contrary  the Lagrangian approach often achieves worse
rewards and is more apt to violate the constraints during training  6  and the performance is very
sensitive to the initial conditions. Furthermore  in some cases (in experiment with d0 = 5 and with
discrete observations) the Lagrangian method cannot guarantee safety throughout training.
6 Conclusion
In this paper  we formulated the problem of safe RL as a CMDP and proposed a novel Lyapunov
approach to solve CMDPs. We also derived an effective LP-based method to generate Lyapunov
functions  such that the corresponding algorithm guarantees feasibility and optimality under certain
conditions. Leveraging these theoretical underpinnings  we showed how Lyapunov approaches can
be used to transform DP (and RL) algorithms into their safe counterparts that only require straight-
forward modiﬁcations in the algorithm implementations. We empirically validated our theoretical
ﬁndings in using the Lyapunov approach to guarantee safety and robust learning in RL. In general 
our work represents a step forward in deploying RL to real-world problems in which guaranteeing
safety is of paramount importance. Future research will focus on two directions. On the algorithmic
perspective  one major extension is to apply the Lyapunov approach to policy gradient algorithms
and compare its performance with CPO in continuous action problems. On the practical aspect 
future work includes evaluating the Lyapunov-based RL algorithms on several real-world testbeds.

6In Appendix F  we also report the results from the Lagrangian method in which the Lagrange multiplier is

learned using gradient ascent method [10] and we observe similar (or even worse) behaviors.

8

051015200200400600800100005101520020040060080010000510152002004006008001000051015200200400600800100005101520051015202530051015200510152005101520051015200510152005101520SDPISDQNLagrangeDPILagrangeDQNDPIDQNReferences
[1] N. Abe  P. Melville  C. Pendus  C. Reddy  D. Jensen  V. Thomas  J. Bennett  G. Anderson 
B. Cooley  M. Kowalczyk  et al. Optimizing debt collections using constrained reinforcement
learning. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 75–84  2010.

[2] J. Achiam  D. Held  A. Tamar  and P. Abbeel. Constrained policy optimization. In International

Conference of Machine Learning  2017.

[3] E. Altman. Constrained Markov decision processes  volume 7. CRC Press  1999.

[4] Eitan Altman. Constrained Markov decision processes with total cost criteria: Lagrangian
approach and dual linear program. Mathematical methods of operations research  48(3):387–
417  1998.

[5] D. Amodei  C. Olah  J. Steinhardt  P. Christiano  J. Schulman  and D. Man´e. Concrete prob-

lems in AI safety. arXiv preprint arXiv:1606.06565  2016.

[6] F. Berkenkamp  M. Turchetta  A. Schoellig  and A. Krause. Safe model-based reinforcement
In Advances in Neural Information Processing Systems 

learning with stability guarantees.
pages 908–919  2017.

[7] D. Bertsekas. Dynamic programming and optimal control. Athena scientiﬁc Belmont  MA 

1995.

[8] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[9] M El Chamie  Y. Yu  and B. Ac¸ıkmes¸e. Convex synthesis of randomized policies for controlled
Markov chains with density safety upper bound constraints. In American Control Conference 
pages 6290–6295  2016.

[10] Y. Chow  M. Ghavamzadeh  L. Janson  and M. Pavone. Risk-constrained reinforcement learn-

ing with percentile risk criteria. arXiv preprint arXiv:1512.01629  2015.

[11] Y. Chow  M. Pavone  B. Sadler  and S. Carpin. Trading safety versus performance: Rapid de-
ployment of robotic swarms with robust performance constraints. Journal of Dynamic Systems 
Measurement  and Control  137(3)  2015.

[12] G. Dalal  K. Dvijotham  M. Vecerik  T. Hester  C. Paduraru  and Y. Tassa. Safe exploration in

continuous action spaces. arXiv preprint arXiv:1801.08757  2018.

[13] R. Fruit and A. Lazaric. Exploration–exploitation in mdps with options. In AISTATS  2017.

[14] Z. G´abor and Z. Kalm´ar. Multi-criteria reinforcement learning. In International Conference of

Machine Learning  1998.

[15] P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under

constraints. Journal of Artiﬁcial Intelligence Research  24:81–108  2005.

[16] P. Glynn  A. Zeevi  et al. Bounding stationary expectations of markov processes. In Markov
processes and related topics: a Festschrift for Thomas G. Kurtz  pages 195–214. Institute of
Mathematical Statistics  2008.

[17] S. Gu  T. Lillicrap  I. Sutskever  and S. Levine. Continuous deep Q-learning with model-based

acceleration. In International Conference on Machine Learning  pages 2829–2838  2016.

[18] S. Junges  N. Jansen  C. Dehnert  U. Topcu  and J. Katoen. Safety-constrained reinforcement
learning for MDPs. In International Conference on Tools and Algorithms for the Construction
and Analysis of Systems  pages 130–146  2016.

[19] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In
International Conference on International Conference on Machine Learning  pages 267–274 
2002.

[20] Hassan K Khalil. Noninear systems. Prentice-Hall  New Jersey  2(5):5–1  1996.

9

[21] J. Lee  I. Panageas  G. Piliouras  M. Simchowitz  M. Jordan  and B. Recht. First-order methods

almost always avoid saddle points. arXiv preprint arXiv:1710.07406  2017.

[22] J. Leike  M. Martic  V. Krakovna  P. Ortega  T. Everitt  A. Lefrancq  L. Orseau  and S. Legg.

Ai safety gridworlds. arXiv preprint arXiv:1711.09883  2017.

[23] D. Luenberger  Y. Ye  et al. Linear and nonlinear programming  volume 2. Springer  1984.

[24] N. Mastronarde and M. van der Schaar. Fast reinforcement learning for energy-efﬁcient wire-

less communication. IEEE Transactions on Signal Processing  59(12):6262–6266  2011.

[25] V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Riedmiller.

Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602  2013.

[26] T. Moldovan and P. Abbeel. Safe exploration in Markov decision processes. arXiv preprint

arXiv:1205.4810  2012.

[27] H. Mossalam  Y. Assael  D. Roijers  and S. Whiteson. Multi-objective deep reinforcement

learning. arXiv preprint arXiv:1610.02707  2016.

[28] M. Neely. Stochastic network optimization with application to communication and queueing

systems. Synthesis Lectures on Communication Networks  3(1):1–211  2010.

[29] G. Neu  A. Jonsson  and V. G´omez. A uniﬁed view of entropy-regularized markov decision

processes. arXiv preprint arXiv:1705.07798  2017.

[30] M. Ono  M. Pavone  Y. Kuwata  and J. Balaram. Chance-constrained dynamic programming
with application to risk-aware robotic space exploration. Autonomous Robots  39(4):555–571 
2015.

[31] T. Perkins and A. Barto. Lyapunov design for safe reinforcement learning. Journal of Machine

Learning Research  3:803–832  2002.

[32] M. Pirotta  M. Restelli  and L. Bascetta. Adaptive step-size for policy gradient methods. In

Advances in Neural Information Processing Systems  pages 1394–1402  2013.

[33] M. Pirotta  M. Restelli  A. Pecorino  and D. Calandriello. Safe policy iteration. In International

Conference on Machine Learning  pages 307–315  2013.

[34] K. Regan and C. Boutilier. Regret-based reward elicitation for Markov decision processes.
In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  pages
444–451  2009.

[35] D. Roijers  P. Vamplew  S. Whiteson  and R. Dazeley. A survey of multi-objective sequential

decision-making. Journal of Artiﬁcial Intelligence Research  48:67–113  2013.

[36] A. Rusu  S. Colmenarejo  C. Gulcehre  G. Desjardins  J. Kirkpatrick  R. Pascanu  V. Mnih 
K. Kavukcuoglu  and R. Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295  2015.

[37] T. Schaul  J. Quan  I. Antonoglou  and D. Silver. Prioritized experience replay. arXiv preprint

arXiv:1511.05952  2015.

[38] Bruno Scherrer. Performance bounds for λ policy iteration and application to the game of

Tetris. Journal of Machine Learning Research  14(Apr):1181–1227  2013.

[39] M. Schmitt and L. Martignon. On the complexity of learning lexicographic strategies. Journal

of Machine Learning Research  7:55–83  2006.

[40] G. Shani  D. Heckerman  and R. Brafman. An MDP-based recommender system. Journal of

Machine Learning Research  6:1265–1295  2005.

[41] A. Tamar  D. Di Castro  and S. Mannor. Policy gradients with variance related risk criteria. In

International Conference of Machine Learning  2012.

[42] H. van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems 

pages 2613–2621  2010.

10

,Yinlam Chow
Ofir Nachum
Edgar Duenez-Guzman
Mohammad Ghavamzadeh
Nicolas Carrara
Edouard Leurent
Romain Laroche
Tanguy Urvoy
Odalric-Ambrym Maillard
Olivier Pietquin