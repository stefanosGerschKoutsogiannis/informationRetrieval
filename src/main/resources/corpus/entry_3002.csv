2014,Learning Shuffle Ideals Under Restricted Distributions,The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set $U$ is the collection of all strings containing some string $u \in U$ as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity  the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper  we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction  we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.,Learning Shufﬂe Ideals

Under Restricted Distributions

Dongqu Chen

Yale University

Department of Computer Science

dongqu.chen@yale.edu

Abstract

The class of shufﬂe ideals is a fundamental sub-family of regular languages. The
shufﬂe ideal generated by a string set U is the collection of all strings containing
some string u ∈ U as a (not necessarily contiguous) subsequence. In spite of
its apparent simplicity  the problem of learning a shufﬂe ideal from given data is
known to be computationally intractable. In this paper  we study the PAC learn-
ability of shufﬂe ideals and present positive results on this learning problem under
element-wise independent and identical distributions and Markovian distributions
in the statistical query model. A constrained generalization to learning shufﬂe
ideals under product distributions is also provided. In the empirical direction  we
propose a heuristic algorithm for learning shufﬂe ideals from given labeled strings
under general unrestricted distributions. Experiments demonstrate the advantage
for both efﬁciency and accuracy of our algorithm.

1

Introduction

The learnablity of regular languages is a classic topic in computational learning theory. The applica-
tions of this learning problem include natural language processing (speech recognition  morpholog-
ical analysis)  computational linguistics  robotics and control systems  computational biology (phy-
logeny  structural pattern recognition)  data mining  time series and music ([7  16  18–21  23  25]).
Exploring the learnability of the family of formal languages is signiﬁcant to both theoretical and
applied realms. In the classic PAC learning model deﬁned by Valiant [26]  unfortunately  the class
of regular languages  or equivalently the concept class of deterministic ﬁnite automata (DFA)  is
known to be inherently unpredictable ([1  9  22]). In a modiﬁed version of Valiant’s model which
allows the learner to make membership queries  Angluin [2] has shown that the concept class of
regular languages is PAC learnable.
Throughout this paper we study the PAC learnability of a fundamental subclass of regular languages 
the class of (extended) shufﬂe ideals. The shufﬂe ideal generated by an augmented string U is the
collection of all strings containing some u ∈ U as a (not necessarily contiguous) subsequence 
where an augmented string is a ﬁnite concatenation of symbol sets (see Figure 1 for an illustration).
The special class of shufﬂe ideals generated by a single string is called the principal shufﬂe ideals.
In spite of its simplicity  the class of shufﬂe ideals plays a prominent role in formal language theory.
The boolean closure of shufﬂe ideals is the important language family known as piecewise-testable
languages ([24]). The rich structure of this language family has made it an object of intensive study
in complexity theory and group theory ([12  17]). In the applied direction  Kontorovich et al. [15]
show the shufﬂe ideals capture some rudimentary phenomena in human language morphology.
Unfortunately  even such a simple class is not PAC learnable  unless RP=NP ([3]). However  in
most application scenarios  the strings are drawn from some particular distribution we are interested
in. Angluin et al. [3] prove under the uniform string distribution  principal shufﬂe ideals are PAC

1

Figure 1: The DFA accepting precisely the shufﬂe ideal of U = (a|b|d)a(b|c) over Σ = {a  b  c  d}.

learnable. Nevertheless  the requirement of complete knowledge of the distribution  the dependence
on the symmetry of the uniform distribution and the restriction of principal shufﬂe ideals lead to the
lack of generality of the algorithm. Our main contribution in this paper is to present positive results
on learning the class of shufﬂe ideals under element-wise independent and identical distributions
and Markovian distributions. Extensions of our main results include a constrained generalization
to learning shufﬂe ideals under product distributions and a heuristic method for learning principal
shufﬂe ideals under general unrestricted distributions.
After introducing the preliminaries in Section 2  we present our main result in Section 3: the ex-
tended class of shufﬂe ideals is PAC learnable from element-wise i.i.d. strings. That is  the dis-
tributions of the symbols in a string are identical and independent of each other. A constrained
generalization to learning shufﬂe ideals under product distributions is also provided. In Section 4 
we further show the PAC learnability of principal shufﬂe ideals when the example strings drawn
from Σ≤n are generated by a Markov chain with some lower bound assumptions on the transition
matrix.
In Section 5  we propose a greedy algorithm for learning principal shufﬂe ideals under
general unrestricted distributions. Experiments demonstrate the advantage for both efﬁciency and
accuracy of our heuristic algorithm.

2 Preliminaries
We consider strings over a ﬁxed ﬁnite alphabet Σ. The empty string is λ. Let Σ∗ be the Kleene
star of Σ and Σ∪ be the collection of all subsets of Σ. As strings are concatenations of symbols  we
similarly deﬁne augmented strings as concatenations of unions of symbols.

Deﬁnition 1 (Alphabet  simple string and augmented string) Let Σ be a non-empty ﬁnite set of
symbols  called the alphabet. A simple string over Σ is any ﬁnite sequence of symbols from Σ  and
Σ∗ is the collection of all simple strings. An augmented string over Σ is any ﬁnite concatenation of
symbol sets from Σ∪  and (Σ∪)

∗ is the collection of all augmented strings.

Denote by s the cardinality of Σ. Because an augmented string only contains strings of the same
length  the length of an augmented string U  denoted by |U|  is the length of any u ∈ U. We use
exponential notation for repeated concatenation of a string with itself  that is  vk is the concatenation
of k copies of string v. Starting from index 1  we denote by vi the i-th symbol in string v and use
∗
notation v[i  j] = vi . . . vj for 1 ≤ i ≤ j ≤ |v|. Deﬁne the binary relation (cid:118) on (cid:104)(Σ∪)
  Σ∗(cid:105) as
follows. For a simple string w  w (cid:118) v holds if and only if there is a witness (cid:126)i = (i1 < i2 < . . . <
i|w|) such that vij = wj for all integers 1 ≤ j ≤ |w|. For an augmented string W   W (cid:118) v if and only
if there exists some w ∈ W such that w (cid:118) v. When there are several witnesses for W (cid:118) v  we may
order them coordinate-wise  referring to the unique minimal element as the leftmost embedding. We
will write IW(cid:118)v to denote the position of the last symbol of W in its leftmost embedding in v (if the
latter exists; otherwise  IW(cid:118)v = ∞).
Deﬁnition 2 (Extended/Principal Shufﬂe Ideal) The (extended) shufﬂe ideal of an augmented
string U ∈ (Σ∪)L is a regular language deﬁned as X(U ) = {v ∈ Σ∗ | ∃u ∈ U  u (cid:118) v} =
Σ∗U1Σ∗U2Σ∗ . . . Σ∗ULΣ∗. A shufﬂe ideal is principal if it is generated by a simple string.

A shufﬂe ideal is an ideal in order theory and was originally deﬁned for lattices. Denote by
the
class of principal shufﬂe ideals and by X the class of extended shufﬂe ideals. Unless otherwise
stated  in this paper shufﬂe ideal refers to the extended ideal. An example is given in Figure 1. The
feasibility of determining whether a string is in the class X(U ) is obvious.
Lemma 1 Evaluating relation U (cid:118) x and meanwhile determining IU(cid:118)x is feasible in time O(|x|).

2

In a computational learning model  an algorithm is usually given access to an oracle providing
information about the sample. In Valiant’s work [26]  the example oracle EX(c D) was deﬁned 
where c is the target concept and D is a distribution over the instance space. On each call  EX(c D)
draws an input x independently at random from the instance space I under the distribution D  and
returns the labeled example (cid:104)x  c(x)(cid:105).
Deﬁnition 3 (PAC Learnability: [26]) Let C be a concept class over the instance space I. We
say C is probably approximately correctly (PAC) learnable if there exists an algorithm A with the
following property: for every concept c ∈ C  for every distribution D on I  and for all 0 <  < 1/2
and 0 < δ < 1/2  if A is given access to EX(c D) on I and inputs  and δ  then with probability
at least 1 − δ  A outputs a hypothesis h ∈ H satisfying Prx∈D[c(x) (cid:54)= h(x)] ≤ . If A runs in time
polynomial in 1/  1/δ and the representation size of c  we say that C is efﬁciently PAC learnable.

If the error parameter
We refer to  as the error parameter and δ as the conﬁdence parameter.
is set to 0  the learning is exact ([6]). Kearns [11] extended Valiant’s model and introduced the
statistical query oracle STAT(c D). Kearns’ oracle takes as input a statistical query of the form
(χ  τ ). Here χ is any mapping of a labeled example to {0  1} and τ ∈ [0  1] is called the noise
tolerance. STAT(c D) returns an estimate for the expectation IEχ  that is  the probability that χ = 1
when the labeled example is drawn according to D. A statistical query can have a condition so IEχ
can be a conditional probability. This estimate is accurate within additive error τ.

Deﬁnition 4 (Legitimacy and Feasibility: [11]) A statistical query χ is legimate and feasible if
and only if with respect to 1/  1/τ and representation size of c:

1. Query χ maps a labeled example (cid:104)x  c(x)(cid:105) to {0  1};
2. Query χ can be efﬁciently evaluated in polynomial time;

3. The condition of χ  if any  can be efﬁciently evaluated in polynomial time;

4. The probability of the condition of χ  if any  should be at least polynomially large.

Throughout this paper  the learnability of shufﬂe ideals is studied in the statistical query model.
Kearns [11] proves that oracle STAT(c D) is weaker than oracle EX(c D). In words  if a concept
class is PAC learnable from STAT(c D)  then it is PAC learnable from EX(c D)  but not necessarily
vice versa.

3 Learning shufﬂe ideals from element-wise i.i.d. strings

Although learning the class of shufﬂe ideals has been proved hard  in most scenarios the string
distribution is restricted or even known. A very usual situation in practice is that we have some prior
knowledge of the unknown distribution. One common example is the string distributions where each
symbol in a string is generated independently and identically from an unknown distribution. It is
element-wise i.i.d. because we view a string as a vector of symbols. This case is general enough to
cover some popular distributions in applications such as the uniform distribution and the multinomial
distribution. In this section  we present as our main result a statistical query algorithm for learning
the concept class of extended shufﬂe ideals from element-wise i.i.d. strings and provide theoretical
guarantees of its computational efﬁciency and accuracy in the statistical query model. The instance
space is Σn. Denote by U the augmented pattern string that generates the target shufﬂe ideal and by
L = |U| the length of U.

3.1 Statistical query algorithm
Before presenting the algorithm  we deﬁne function θV a(·) and query χV a(· ·) for any augmented
string V ∈ (Σ∪)

≤n and any symbol a ∈ Σ as as follows.

(cid:26) a

if V (cid:54)(cid:118) x[1  n − 1]
if V (cid:118) x[1  n − 1]

θV a(x) =

xIV (cid:118)x+1

χV a(x  y) =

1
2

(y + 1) given θV a(x) = a

3

where y = c(x) is the label of example string x. More precisely  y = +1 if x ∈ X(U ) and y = −1
otherwise. Our learning algorithm uses statistical queries to recover string U ∈ (Σ∪)L one element
at a time. It starts with the empty string V = λ. Having recovered V = U [1  (cid:96)] where 0 ≤ (cid:96) < L 
we infer U(cid:96)+1 as follows. For each a ∈ Σ  the statistical query oracle is called with the query χV a
at the error tolerance τ claimed in Theorem 1. Our key technical observation is that the value of
IEχV a effectively selects U(cid:96)+1. The query results of χV a will form two separate clusters such that
the maximum difference (variance) inside one cluster is smaller than the minimum difference (gap)
between the two clusters  making them distinguishable. The set of symbols in the cluster with larger
query results is proved to be U(cid:96)+1. Notice that this statistical query only works for 0 ≤ (cid:96) < L. To
complete the algorithm  we address the trivial case (cid:96) = L with query Pr[y = +1 | V (cid:118) x] and the
algorithm halts if the query answer is close to 1.

3.2 PAC learnability of ideal X

We show the algorithm described above learns the class of shufﬂe ideals from element-wise i.i.d.
strings in the statistical query learning model.
Theorem 1 Under element-wise independent and identical distributions over instance space I =
Σn  concept class X is approximately identiﬁable with O(sn) conditional statistical queries from
STAT(X D) at tolerance

or with O(sn) statistical queries from STAT(X D) at tolerance
4



(cid:18)

¯τ =

1 −

τ =

40sn2 + 4

2

(cid:19)

20sn2 + 2

16sn(10sn2 + )

We provide the main idea of the proofs in this section and defer the details and algebra to Appendix
A. The proof starts from the legitimacy and feasibility of the algorithm. Since χV a computes a
binary mapping from labeled examples to {0  1}  the legitimacy is trivial. But χV a is not feasible
for symbols in Σ of small occurrence probabilities. We avoid the problematic cases by reducing the
original learning problem to the same problem with a polynomial lower bound assumption Pr[xi =
a] ≥ /(2sn) − 2/(20sn2 + 2) for any a ∈ Σ and achieve feasibility.
The correctness of the algorithm is based on the intuition that the query result IEχV a+ of a symbol
a+ ∈ U(cid:96)+1 should be greater than that of a symbol a− (cid:54)∈ U(cid:96)+1 and the difference is large enough
to tolerate the noise from the oracle. To prove this  we ﬁrst consider the exact learning case. Deﬁne
an inﬁnite string U(cid:48) = U [1  (cid:96)]U [(cid:96) + 2  L]U∞
(cid:96)+1 and let x(cid:48) = xΣ∞ be the extension of x obtained by
padding it on the right with an inﬁnite string generated from the same distribution as x. Let Q(j  i)
be the probability that the largest g such that U(cid:48)[1  g] (cid:118) x(cid:48)[1  i] is j  or formally
Q(j  i) = Pr[U(cid:48)[1  j] (cid:118) x(cid:48)[1  i] ∧ U(cid:48)[1  j + 1] (cid:54)(cid:118) x(cid:48)[1  i]]

By taking the difference between IEχV a+ and IEχV a− in terms of Q(j  i)  we get the query tolerance
for exact learning.
Lemma 2 Under element-wise independent and identical distributions over instance space I =
Σn  concept class X is exactly identiﬁable with O(sn) conditional statistical queries from
STAT(X D) at tolerance

τ(cid:48) =

Q(L − 1  n − 1)

1
5

Lemma 2 indicates bounding the quantity Q(L − 1  n − 1) is the key to the tolerance for PAC
learning. Unfortunately  the distribution {Q(j  i)} doesn’t seem to have any strong properties we
know of providing a polynomial lower bound. Instead we introduce new quantity

R(j  i) = Pr[U(cid:48)[1  j] (cid:118) x(cid:48)[1  i] ∧ U(cid:48)[1  j] (cid:54)(cid:118) x(cid:48)[1  i − 1]]

being the probability that the smallest g such that U(cid:48)[1  j] (cid:118) x(cid:48)[1  g] is i. An important property of
distribution {R(j  i)} is its strong unimodality as deﬁned below.

4

Deﬁnition 5 (Unimodality: [8]) A distribution {P (i)} with all support on the lattice of integers is
unimodal if and only if there exists at least one integer K such that P (i) ≥ P (i − 1) for all i ≤ K
and P (i + 1) ≤ P (i) for all i ≥ K. We say K is a mode of distribution {P (i)}.

Throughout this paper  when referring to the mode of a distribution  we mean the one with the largest
index  if the distribution has multiple modes with equal probabilities.
Deﬁnition 6 (Strong Unimodality: [10]) A distribution {H(i)} is strongly unimodal if and only if
the convolution of {H(i)} with any unimodal distribution {P (i)} is unimodal.

Since a distribution with all mass at zero is unimodal  a strongly unimodal distribution is also uni-
modal. In this paper  we only consider distributions with all support on the lattice of integers. So the
convolution of {H(i)} and {P (i)} is

{H ∗ P}(i) =

H(j)P (i − j) =

H(i − j)P (j)

∞(cid:88)

j=−∞

∞(cid:88)

j=−∞

We prove the strong unimodality of {R(j  i)} with respect to i via showing it is the convolution of
two log-concave distributions by induction. We do an initial statistical query to estimate Pr[y = +1]
to handle two marginal cases Pr[y = +1] ≤ /2 and Pr[y = +1] ≥ 1−/2. After that an additional
query Pr[y = +1 | V (cid:118) x] is made to tell whether (cid:96) = L. If the algorithm doesn’t halt  it means
(cid:96) < L and both Pr[y = +1] and Pr[y = −1] are at least /2 − 2τ. By upper bounding Pr[y = +1]
and Pr[y = −1] using linear sums of R(j  i)  the strong unimodality of {R(j  i)} gives a lower
bound for R(L  n)  which further implies one for Q(L − 1  n − 1) and completes the proof.

3.3 A generalization to instance space Σ≤n

ﬁxed length i. Because instance space Σ≤n =(cid:83)

We have proved the extended class of shufﬂe ideals is PAC learnable from element-wise i.i.d. ﬁxed-
length strings. Nevertheless  in many real-world applications such as natural language processing
and computational linguistics  it is more natural to have strings of varying lengths. Let n be the
maximum length of the sample strings and as a consequence the instance space for learning is Σ≤n.
Here we show how to generalize the statistical query algorithm in Section 3.1 to the more general
instance space Σ≤n.
Let Ai be the algorithm in Section 3.1 for learning shufﬂe ideals from element-wise i.i.d. strings of
i≤n Σi  we divide the sample S into n subsets {Si}
where Si = {x | |x| = i}. An initial statistical query then is made to estimate probability Pr[|x| = i]
for each i ≤ n at tolerance /(8n). We discard all subsets Si with query answer ≤ 3/(8n) in the
learning procedure  because we know Pr[|x| = i] ≤ /(2n). As there are at most (n − 1) such
Si of low occurrence probabilities. The total probability that an instance comes from one of these
negligible sets is at most /2. Otherwise  Pr[|x| = i] ≥ /(4n) and we apply algorithm Ai on each
Si with query answer ≥ 3/(8n) with error parameter /2. Because the probability of the condition
is polynomially large  the algorithm is feasible. Finally  the total error over the whole instance space
will be bounded by  and concept class X is PAC learnable from element-wise i.i.d. strings over
instance space Σ≤n.
Corollary 1 Under element-wise independent and identical distributions over instance space I =
Σ≤n  concept class X is approximately identiﬁable with O(sn2) conditional statistical queries from
STAT(X D) at tolerance

or with O(sn2) statistical queries from STAT(X D) at tolerance

τ =

160sn2 + 8

2

(cid:19)

(cid:18)

¯τ =

1 −



5

40sn2 + 2

512sn2(20sn2 + )

5

3.4 A constrained generalization to product distributions

element-wise independence between its elements. That is  Pr[X = x] =(cid:81)|x|

A direct generalization from element-wise independent and identical distributions is product dis-
tributions. A random string  or a random vector of symbols under a product distribution has
i=1 Pr[Xi = xi]. Al-
though strings under product distributions share many independence properties with element-wise
i.i.d. strings  the algorithm in Section 3.1 is not directly applicable to this case as the distribution
{R(j  i)} deﬁned above is not unimodal with respect to i in general. However  the intuition that
given IV (cid:118)x = h  the strings with xh+1 ∈ U(cid:96)+1 have higher probability of positivity than that of the
strings with xh+1 (cid:54)∈ U(cid:96)+1 is still true under product distributions. Thus we generalize query χV a
and deﬁne for any V ∈ (Σ∪)

≤n  a ∈ Σ and h ∈ [0  n − 1] 

˜χV a h(x  y) =

1
2

(y + 1)

given IV (cid:118)x = h and xh+1 = a

where y = c(x) is the label of example string x. To ensure the legitimacy and feasibility of the
algorithm  we have to attach a lower bound assumption that Pr[xi = a] ≥ t > 0  for ∀1 ≤ i ≤ n and
∀a ∈ Σ. Appendix C provides a constrained algorithm based on this intuition. Let P (+|a  h) denote
IE ˜χV a h. If the difference P (+|a+  h)− P (+|a−  h) is large enough for some h with nonnegligible
Pr[IV (cid:118)x = h]  then we are able to learn the next element in U. Otherwise  the difference is very
small and we will show that there is an interval starting from index (h + 1) which we can skip
with little risk. The algorithm is able to classify any string whose classiﬁcation process skips O(1)
intervals. Details of this constrained generalization are deferred to Appendix C.

4 Learning principal shufﬂe ideals from Markovian strings

Markovian strings are widely studied in natural language processing and biological sequence mod-
eling. Formally  a random string x is Markovian if the distribution of xi+1 only depends on the
value of xi: Pr[xi+1 | x1 . . . xi] = Pr[xi+1 | xi] for any i ≥ 1. If we denote by π0 the distribution
of x1 and deﬁne s × s stochastic matrix M by M (a1  a2) = Pr[xi+1 = a1 | xi = a2]  then a
random string can be viewed as a Markov chain with initial distribution π0 and transition matrix
M. We choose Σ≤n as the instance space in this section and assume independence between the
string length and the symbols in the string. We assume Pr[|x| = k] ≥ t for all 1 ≤ k ≤ n and
min{M (· ·)  π0(·)} ≥ c for some positive t and c. We will prove the PAC learnability of class
under this lower bound assumption. Denote by u be the target pattern string and let L = |u|.

4.1 Statistical query algorithm

recovered v = u[1  (cid:96)]  we infer u(cid:96)+1 by Ψv a =(cid:80)n

k=h+1 IEχv a k  where

Starting with empty string v = λ  the pattern string u is recovered one symbol at a time. Having

χv a k(x  y) =

(y + 1)

given Iv(cid:118)x = h  xh+1 = a and |x| = k

1
2

0 ≤ (cid:96) < L and h is chosen from [0  n − 1] such that the probability Pr[Iv(cid:118)x = h] is polynomially
large. The statistical queries χv a k are made at tolerance τ claimed in Theorem 2 and the symbol
with the largest query result of Ψv a is proved to be u(cid:96)+1. Again  the case where (cid:96) = L is addressed
by query Pr[y = +1 | v (cid:118) x]. The learning procedure is completed if the query result is close to 1.

4.2 PAC learnability of principal ideal

With query Ψv a  we are able to recover the pattern string u approximately from STAT(
proper tolerance as stated in Theorem 2:
Theorem 2 Under Markovian string distributions over instance space I = Σ≤n  given Pr[|x| =
k] ≥ t > 0 for ∀1 ≤ k ≤ n and min{M (· ·)  π0(·)} ≥ c > 0  concept class
is approximately
identiﬁable with O(sn2) conditional statistical queries from STAT(

 D) at tolerance

(u) D) at

τ =



3n2 + 2n + 2

6

or with O(sn2) statistical queries from STAT(

 D) at tolerance
3ctn2

¯τ =

(3n2 + 2n + 2)2

Please refer to Appendix B for a complete proof of Theorem 2. Due to the probability lower bound
assumptions  the legitimacy and feasibility are obvious. To calculate the tolerance for PAC learning 
we ﬁrst consider the exact learning tolerance. Let x(cid:48) be an inﬁnite string generated by the Markov
chain deﬁned above. For any 0 ≤ (cid:96) ≤ L − j  we deﬁne quantity R(cid:96)(j  i) by
R(cid:96)(j  i) = Pr[u[(cid:96) + 1  (cid:96) + j] (cid:118) x(cid:48)[m + 1  m + i]∧ u[(cid:96) + 1  (cid:96) + j] (cid:54)(cid:118) x(cid:48)[m + 1  m + i− 1] | x(cid:48)
m = u(cid:96)]
Intuitively  R(cid:96)(j  i) is the probability that the smallest g such that u[(cid:96) + 1  (cid:96) + j] (cid:118) x(cid:48)[m + 1  m + g]
is i  given x(cid:48)
Lemma 3 Under Markovian string distributions over instance space I = Σ≤n  given Pr[|x| =
k] ≥ t > 0 for ∀1 ≤ k ≤ n and min{M (· ·)  π0(·)} ≥ c > 0  the concept class
is exactly
 D) at tolerance
identiﬁable with O(sn2) conditional statistical queries from STAT(

m = u(cid:96). We have the following conclusion on the exact learning tolerance.

(cid:40)

n(cid:88)

k=h+1

τ(cid:48) = min
0≤(cid:96)<L

1

3(n − h)

R(cid:96)+1(L − (cid:96) − 1  k − h − 1)

(cid:41)

h(x  y) = 1

The algorithm ﬁrst deals with the marginal case where P [y = +1] ≤  through query Pr[y = +1].
If it doesn’t halt  we know Pr[y = +1] is at least (3n2 + 2n)/(3n2 + 2n + 2). We then make a
2 (y + 1) · 1{Iv(cid:118)x=h} for each h from (cid:96) to n − 1. It can be shown that
statistical query χ(cid:48)
at least one h will give an answer ≥ (3n + 1)/(3n2 + 2n + 2). This implies lower bounds for
Pr[Iv(cid:118)x = h] and Pr[y = +1 | Iv(cid:118)x = h]. The former guarantees the feasibility while the latter
can serve as a lower bound for the sum in Lemma 3 after some algebra and completes the proof.
The assumption on M and π0 can be weakened to M (u(cid:96)+1  u(cid:96)) = Pr[x2 = u(cid:96)+1 | x1 = u(cid:96)] ≥ c
and π0(u1) ≥ c for all 1 ≤ (cid:96) ≤ L − 1. We ﬁrst make a statistical query to estimate M (a  u(cid:96))
for (cid:96) ≥ 1 or π0(a) for (cid:96) = 0 for each symbol a ∈ Σ at tolerance c/3. If the result is ≤ 2c/3
then M (a  u(cid:96)) ≤ c or π0(a) ≤ c and we won’t consider symbol a at this position. Otherwise 
M (a  u(cid:96)) ≥ c/3 or π0(a) ≥ c/3 and the queries in the algorithm are feasible.
Corollary 2 Under Markovian string distributions over instance space I = Σ≤n  given Pr[|x| =
k] ≥ t > 0 for ∀1 ≤ k ≤ n  π0(u1) ≥ c and M (u(cid:96)+1  u(cid:96)) ≥ c > 0 for ∀1 ≤ (cid:96) ≤ L − 1  concept
 D)
class
at tolerance

is approximately identiﬁable with O(sn2) conditional statistical queries from STAT(

(cid:27)

(cid:26)

or with O(sn2) statistical queries from STAT(

τ = min

(cid:26)

 



3n2 + 2n + 2

c
3
 D) at tolerance
tnc2

¯τ = min

ctn2

(3n2 + 2n + 2)2  

3(3n2 + 2n + 2)

(cid:27)

5 Learning shufﬂe ideals under general distributions

Although the string distribution is restricted or even known in most application scenarios  one might
be interested in learning shufﬂe ideals under general unrestricted and unknown distributions without
any prior knowledge. Unfortunately  under standard complexity assumptions  the answer is negative.
Angluin et al. [3] have shown that a polynomial time PAC learning algorithm for principal shufﬂe
ideals would imply the existence of polynomial time algorithms to break the RSA cryptosystem 
factor Blum integers  and test quadratic residuosity.
Theorem 3 ([3]) For any alphabet of size at least 2  given two disjoint sets of strings S  T ⊂ Σ≤n 
the problem of determining whether there exists a string u such that u (cid:118) x for each x ∈ S and
u (cid:54)(cid:118) x for each x ∈ T is NP-complete.

7

As ideal
over instance space Σn? The answer is again no.

is a subclass of ideal X  we know learning ideal X is only harder. Is the problem easier

Lemma 4 Under general unrestricted string distributions  a concept class is PAC learnable over
instance space Σ≤n if and only if it is PAC learnable over instance space Σn.

The proof of Lemma 4 is presented in Appendix D using the same idea as our generalization in
Section 3.3. Note that Lemma 4 holds under general string distributions. It is not necessarily true
when we have assumptions on the marginal distribution of string length.
Despite the infeasibility of PAC learning a shufﬂe ideal in theory  it is worth exploring the possi-
bilities to do the classiﬁcation problem without theoretical guarantees  since most applications care
more about the empirical performance than about theoretical results. For this purpose we propose a
heuristic greedy algorithm for learning principal shufﬂe ideals based on reward strategy as follows.
sumes k elements in x if min{Iva(cid:118)x  n + 1} − Iv(cid:118)x = k. The reward strategy depends on the ratio
r+/r−: the algorithm receives r− reward from each element it consumes in a negative example or

Upon having recovered v = (cid:98)u[1  (cid:96)]  for a symbol a ∈ Σ and a string x of length n  we say a con-
r+ penalty from each symbol it consumes in a positive string. A symbol is chosen as (cid:98)u(cid:96)+1 if it
brings us most reward. The algorithm will halt once(cid:98)u exhausts any positive example and makes a
((cid:98)u[1  (cid:96) − 1]) is returned
examples x such that (cid:98)u (cid:118) x and #(−) is the number of negative examples x such that (cid:98)u (cid:118) x.

false negative error  which means we have gone too far. Finally the ideal
as the hypothesis. The performance of this greedy algorithm depends a great deal on the selection of
parameter r+/r−. A clever choice is r+/r− = #(−)/#(+)  where #(+) is the number of positive

A more recommended but more complex strategy to determine the parameter r+/r− in practice is
cross validation.
A better studied approach to learning regular languages  especially the piecewise-testable ones  in
recent works is kernel machines ([13  14]). An obvious advantage of kernel machines over our
greedy method is its broad applicability to general classiﬁcation learning problems. Nevertheless 
the time complexity of the kernel machine is O(N 3 + n2N 2) on a training sample set of size N
([5])  while our greedy method only takes O(snN ) time due to its great simplicity. Because N
is usually huge for the demand of accuracy  kernel machines suffer from low efﬁciency and long
running time in practice. To make a comparison between the greedy method and kernel machines
for empirical performance  we conducted a series of experiments on a real world dataset [4] with
string length n as a variable. The experiment results demonstrate the empirical advantage on both
efﬁciency and accuracy of the greedy algorithm over the kernel method  in spite of its simplicity.
As this is a theoretical paper  we defer the details on the experiments to Appendix D  including the
experiment setup and ﬁgures of detailed experiment results.

6 Discussion

We have shown positive results for learning shufﬂe ideals in the statistical query model under
element-wise independent and identical distributions and Markovian distributions  as well as a con-
strained generalization to product distributions. It is still open to explore the possibilities of learning
shufﬂe ideals under less restricted distributions with weaker assumptions. Also a lot more work
needs to be done on approximately learning shufﬂe ideals in applications with pragmatic approaches.
In the negative direction  even a family of regular languages as simple as the shufﬂe ideals is not
efﬁciently properly PAC learnable under general unrestricted distributions unless RP=NP. Thus  the
search for a nontrivial properly PAC learnable family of regular languages continues. Another the-
oretical question that remains is how hard the problem of learning shufﬂe ideals is  or whether PAC
learning a shufﬂe ideal is as hard as PAC learning a deterministic ﬁnite automaton.

Acknowledgments

We give our sincere gratitude to Professor Dana Angluin of Yale University for valuable discussions
and comments on the learning problem and the proofs. Our thanks are also due to Professor Joseph
Chang of Yale University for suggesting supportive references on strong unimodality of probability
distributions and to the anonymous reviewers for their helpful feedback.

8

References
[1] D. Angluin. On the complexity of minimum inference of regular sets. Information and Control  39(3):337

– 350  1978.

[2] D. Angluin. Learning regular sets from queries and counterexamples.

75(2):87–106  Nov. 1987.

Information and Computation 

[3] D. Angluin  J. Aspnes  S. Eisenstat  and A. Kontorovich. On the learnability of shufﬂe ideals. Journal of

Machine Learning Research  14:1513–1531  2013.

[4] K. Bache and M. Lichman. NSF research award abstracts 1990-2003 data set. UCI Machine Learning

Repository  2013.

[5] L. Bottou and C.-J. Lin. Support vector machine solvers. Large scale kernel machines  pages 301–320 

2007.

[6] N. H. Bshouty. Exact learning of formulas in parallel. Machine Learning  26(1):25–41  Jan. 1997.
[7] C. de la Higuera. A bibliographical study of grammatical inference. Pattern Recognition  38(9):1332–

1348  Sept. 2005.

[8] B. Gnedenko and A. N. Kolmogorov. Limit distributions for sums of independent random variables.

Addison-Wesley series in statistics  1949.

[9] E. M. Gold. Complexity of automaton identiﬁcation from given data. Information and Control  37(3):302

– 320  1978.

[10] I. Ibragimov. On the composition of unimodal distributions. Theory of Probability and Its Applications 

1(2):255–260  1956.

[11] M. Kearns. Efﬁcient noise-tolerant learning from statistical queries.

45(6):983–1006  Nov. 1998.

Journal of the ACM (JACM) 

[12] O. Kl´ıma and L. Pol´ak. Hierarchies of piecewise testable languages. Proceedings of the 12th International

Conference on Developments in Language Theory  pages 479–490  2008.

[13] L. A. Kontorovich  C. Cortes  and M. Mohri. Kernel methods for learning languages. Theoretical Com-

puter Science  405(3):223–236  Oct. 2008.

[14] L. A. Kontorovich and B. Nadler. Universal kernel-based learning with applications to regular languages.

The Journal of Machine Learning Research  10:1095–1129  June 2009.

[15] L. A. Kontorovich  D. Ron  and Y. Singer. A markov model for the acquisition of morphological structure.

Technical Report CMU-CS-03-147  10  June 2003.

[16] K. Koskenniemi. Two-level model for morphological analysis. Proceedings of the Eighth International

Joint Conference on Artiﬁcial Intelligence - Volume 2  pages 683–685  1983.

[17] M. Lothaire. Combinatorics on Words (Encyclopedia of Mathematics and Its Applications - Vol 17).

Addison-Wesley  1983.

[18] M. Mohri. On some applications of ﬁnite-state automata theory to natural language processing. Journal

of Natural Language Engineering  2(1):61–80  Mar. 1996.

[19] M. Mohri. Finite-state transducers in language and speech processing. Computational Linguistics 

23(2):269–311  June 1997.

[20] M. Mohri  P. J. Moreno  and E. Weinstein. Efﬁcient and robust music identiﬁcation with weighted ﬁnite-
state transducers. IEEE Transactions on Audio  Speech  and Language Processing  18(1):197–207  Jan.
2010.

[21] M. Mohri  F. Pereira  and M. Riley. Weighted ﬁnite-state transducers in speech recognition. Computer

Speech and Language  16(1):69 – 88  2002.

[22] L. Pitt and M. K. Warmuth. The minimum consistent DFA problem cannot be approximated within any

polynomial. Journal of the ACM (JACM)  40(1):95–142  Jan. 1993.

[23] O. Rambow  S. Bangalore  T. Butt  A. Nasr  and R. Sproat. Creating a ﬁnite-state parser with application
semantics. Proceedings of the 19th International Conference on Computational Linguistics - Volume 2 
pages 1–5  2002.

[24] I. Simon. Piecewise testable events. Proceedings of the 2nd GI Conference on Automata Theory and

Formal Languages  pages 214–222  1975.

[25] R. Sproat  W. Gale  C. Shih  and N. Chang. A stochastic ﬁnite-state word-segmentation algorithm for

Chinese. Computational Linguistics  22(3):377–404  Sept. 1996.

[26] L. G. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142  Nov. 1984.

9

,Dongqu Chen
Francesco Locatello
Michael Tschannen
Gunnar Raetsch
Martin Jaggi