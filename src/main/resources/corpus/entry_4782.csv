2013,Robust Transfer Principal Component Analysis with Rank Constraints,Principal component analysis (PCA)  a well-established technique for data analysis and processing  provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However  the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper  we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally  we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices  with shared common principal components across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors  and signiﬁcantly outperform both standard PCA and robust PCA.,Robust Transfer Principal Component Analysis

with Rank Constraints

Yuhong Guo

Department of Computer and Information Sciences
Temple University  Philadelphia  PA 19122  USA

yuhong@temple.edu

Abstract

Principal component analysis (PCA)  a well-established technique for data analy-
sis and processing  provides a convenient form of dimensionality reduction that is
effective for cleaning small Gaussian noises presented in the data. However  the
applicability of standard principal component analysis in real scenarios is limited
by its sensitivity to large errors. In this paper  we tackle the challenge problem of
recovering data corrupted with errors of high magnitude by developing a novel
robust transfer principal component analysis method. Our method is based on the
assumption that useful information for the recovery of a corrupted data matrix can
be gained from an uncorrupted related data matrix. Speciﬁcally  we formulate the
data recovery problem as a joint robust principal component analysis problem on
the two data matrices  with common principal components shared across matrices
and individual principal components speciﬁc to each data matrix. The formulated
optimization problem is a minimization problem over a convex objective function
but with non-convex rank constraints. We develop an efﬁcient proximal projected
gradient descent algorithm to solve the proposed optimization problem with con-
vergence guarantees. Our empirical results over image denoising tasks show the
proposed method can effectively recover images with random large errors  and sig-
niﬁcantly outperform both standard PCA and robust PCA with rank constraints.

1

Introduction

Dimensionality reduction  as an important form of unsupervised learning  has been widely explored
for analyzing complex data such as images  video sequences  text documents  etc. It has been used to
discover important latent information about observed data matrices for visualization  feature recov-
ery  embedding and data cleaning. The fundamental assumption roots in dimensionality reduction
is that the intrinsic structure of high dimensional observation data lies on a low dimensional linear
subspace. Principal component analysis (PCA) [7] is a classic and one of most commonly used di-
mensionality reduction method. It seeks the best low-rank approximation of the given data matrix
under a well understood least-squares reconstruction loss  and projects data onto uncorrelated low
dimensional subspace. Moreover  it admits an efﬁcient procedure for computing optimal solutions
via the singular value decomposition. These properties make PCA a well suited reduction method
when the observed data is mildly corrupted with small Gaussian noise [12]. But standard PCA is
very sensitive to the high magnitude errors of the observed data. Even a small fraction of large
errors can cause severe degradation in PCA’s estimate of the low rank structure.

Real-life data  however  is often corrupted with large errors or even missing observations. To tackle
dimensionality reduction with arbitrarily large errors and outliers  a number of approaches that ro-
bustify PCA have been developed in the literature  including ℓ1-norm regularized robust PCA [14] 
inﬂuence function techniques [5  13]  and alternating ℓ1-norm minimization [8]. Nevertheless  the

1

capacity of these approaches on recovering the low-rank structure of a corrupted data matrix can
still be degraded with the increasing of the fraction of the large errors.

In this paper  we propose a novel robust transfer principal component analysis method to recover the
low rank representation of heavily corrupted data by leveraging related uncorrupted auxiliary data.
Seeking knowledge transfer from a related auxiliary data source for the target learning problem has
been popularly studied in supervised learning. It is also known that modeling related data sources
together provides rich information for discovering theirs shared subspace representations [4]. We
extend such a transfer learning scheme into the PCA framework to perform joint robust principal
component analysis over a corrupted target data matrix and a related auxiliary source data matrix
by enforcing the two robust PCA operations on the two data matrices to share a subset of com-
mon principal components  while maintaining their unique variations through individual principal
components speciﬁc for each data matrix. This robust transfer PCA framework combines aspects
of both robust PCA and transfer learning methodologies. We expect the critical low rank structure
shared between the two data matrices can be effectively transferred from the uncorrupted auxiliary
data to recover the low dimensional subspace representation of the heavily corrupted target data
in a robust manner. We formulate this robust transfer PCA as a joint minimization problem over
a convex combination of least squares losses with non-convex matrix rank constraints. Though a
simple relaxation of the matrix rank constraints into convex nuclear norm constraints can lead to a
convex optimization problem  it is very difﬁcult to control the rank of the low-rank representation
matrix we aim to recover through the nuclear norm. We thus develop a proximal projected gradient
descent optimization algorithm to solve the proposed optimization problem with rank constraints 
which permits a convenient closed-form solution for each proximal step based on singular value
decomposition and converges to a stationary point. Our experiments over image denoising tasks
show the proposed method can effectively recover images corrupted with random large errors  and
signiﬁcantly outperform both standard PCA and robust PCA with rank constraints.
Notations: In this paper  we use In to denote an n × n identify matrix  use On m to denote an n × m
matrix with all 0 values  use k · kF to denote the matrix Frobenius norm  and use k · k∗ to denote the
nuclear norm (trace norm).

2 Preliminaries

Assume we are given an observed data matrix X ∈ Rn×d consisting of n observations of d-
dimensional feature vectors  which was generated by corrupting some entries of a latent low-rank
matrix M ∈ Rn×d with an error matrix E ∈ Rn×d such that X = M + E. We aim to to recover
the low-rank matrix M by projecting the high dimensional observations X into a low dimensional
manifold representation matrix Z ∈ Rn×k over the low dimensional subspace B ∈ Rk×d  such that
M = ZB  BB⊤ = Ik for k < d.

2.1 PCA

Given the above setup  standard PCA assumes the error matrix E contains small i.i.d. Gaussian
noises  and seeks optimal low dimensional encoding matrix Z and basis matrix B to reconstruct X
by X = ZB + E. Under a least squares reconstruction loss  PCA is equivalent to the following
self-supervised regression problem
min
Z B

s.t. BB⊤ = Ik.

kX − ZBk2
F

(1)

That is  standard PCA seeks the best rank-k estimate of the latent low-rank matrix M = ZB by
solving

min
M

kX − M k2
F

s.t. rank(M ) ≤ k.

(2)

Although the optimization problem in (1) or (2) is not convex and does not appear to be easy  it can
be efﬁciently solved by performing a singular value decomposition (SVD) over X  and permits the
following closed-form solution

(3)
where Vk is comprised of the top k right singular vectors of X. With the convenient solution  stan-
dard PCA has been widely used for modern data analysis and serves as an efﬁcient and effective
dimensionality reduction procedure when the error E is small and i.i.d. Gaussian [7].

k   Z ∗ = XB∗  M ∗ = Z ∗B∗ 

B∗ = V ⊤

2

2.2 Robust PCA

The validity of standard PCA however breaks down when corrupted errors in the observed data
matrix are large. Note that even a single grossly corrupted entry in the observation matrix X can
render the recovered M ∗ matrix to be shifted away from the true low-rank matrix M. To recover
the intrinsic low-rank matrix M from the observation matrix X corrupted with sparse large errors
E  a polynomial-time robust PCA method has been developed in [14]  which induces the following
optimization problem

min
M E

rank(M ) + γkEk0

s.t. X = M + E.

(4)

By relaxing the non-convex rank function and the ℓ0-norm into their convex envelopes of nuclear
norm and ℓ1-norm respectively  a convex relaxation of the robust PCA can be yielded

min
M E

kM k∗ + λkEk1

s.t. X = M + E.

(5)

With an appropriate choice of λ parameter  one can exactly recover the M  E matrices that generated
the observations X by solving this convex program.

To produce a scalable optimization for robust PCA  a more convenient relaxed formulation has been
considered in [14]

min
M E

kM k∗ + λkEk1 +

α
2

kM + E − Xk2
F

(6)

where the original equality constraint is replaced with a reconstruction loss penalty term. This for-
mulation apparently seeks the lowest rank M that can best reconstruct the observation matrix X
subjecting to sparse errors E.

Robust PCA though can effectively recover the low-rank matrix given very sparse large errors in the
observed data  its performance can be degraded when the observation data is heavily corrupted with
dense large errors. In this work  we propose to tackle this problem by exploiting information from
related uncorrupted auxiliary data.

3 Robust Transfer PCA

Exploring labeled information in a related auxiliary data set to assist the learning problem on a target
data set has been widely studied in supervised learning scenarios within the context of transfer learn-
ing  domain adaptation and multi-task learning [10]. Moreover  it has also been shown that modeling
related data sources together can provide useful information for discovering their shared subspace
representations in an unsupervised manner [4]. The principle behind these knowledge transfer learn-
ing approaches is that related data sets can complement each other on identifying the intrinsic latent
structure shared between them.

Following this transfer learning scheme  we present a robust transfer PCA method for recovering
low-rank matrix from a heavily corrupted observation matrix. Assume we are given a target data
matrix Xt ∈ Rnt×d corrupted with errors of large magnitude  and a related source data matrix
Xs ∈ Rns×d. The robust transfer PCA aims to achieve the following robust joint matrix factorization

Xs = NsBc + ZsBs + Es 
Xt = NtBc + ZtBt + Et 

(7)
(8)

where Bc ∈ Rkc×d is the orthogonal basis matrix shared between the two data matrices  Bs ∈ Rks×d
and Bt ∈ Rkt×d are the orthogonal basis matrices speciﬁc to each data matrix respectively 
Ns ∈ Rns×kc   Nt ∈ Rnt×kc  Zs ∈ Rns×ks   Zt ∈ Rnt×kt are the corresponding low dimensional re-
construction coefﬁcient matrices  Es ∈ Rns×d and Et ∈ Rnt×d represent the additive errors in each
data matrix. Let Zc = [Ns; Nt]. Given constant matrices As = [Ins   Ons nt ] and At = [Ont ns   Int ] 
we can re-express Ns and Nt in term of the uniﬁed matrix Zc such that Ns = AsZc and Nt = AtZc.
The learning problem of robust transfer PCA can then be formulated as the following joint minimiza-

3

tion problem

min

Zc Zs Zt Bc Bs Bt Es Et

αs
2
αt
2

kAsZcBc + ZsBs + Es − Xsk2

F +

(9)

kAtZcBc + ZtBt + Et − Xtk2

F + βskEsk1 + βtkEtk1

s.t. BcB⊤

c = Ikc   BsB⊤

s = Iks   BtB⊤

t = Ikt

which minimizes the least squares reconstruction losses on both data matrices with ℓ1-norm regu-
larizers over the additive error matrices. The intuition is that by sharing the common column basis
vectors in Bc  one can best capture the common intrinsic low-rank structure of the data based on lim-
ited observations from both data sets  and by allowing data embedding onto individual basis vectors
Bs  Bt  the additional low-rank structure speciﬁc to each data set can be captured. Nevertheless  this
is a difﬁcult non-convex optimization problem as both the objective function and the orthogonality
constraints are non-convex. To simplify this optimization problem  we introduce replacements

Mc = ZcBc  Ms = ZsBs  Mt = ZtBt

(10)

and rewrite the optimization problem (9) equivalently into the formulation below

αt
2

kAtMc + Mt + Et − Xtk2
F

(11)

min

Mc Ms Mt Es Et

s.t.

kAsMc + Ms + Es − Xsk2

F +

αs
2
+ βskEsk1 + βtkEtk1
rank(Mc) ≤ kc 

rank(Ms) ≤ ks 

rank(Mt) ≤ kt

which has a ℓ1-norm regularized convex objective function  but is subjecting to non-convex inequal-
ity rank constraints. A standard convexiﬁcation of the rank constraints is to replace rank functions
with their convex envelopes  nuclear norms [3  14  1  6  15]. For example  one can replace the rank
constraints in (11) with relaxed nuclear norm regularizers in the objective function

min

Mc Ms Mt Es Et

kAsMc + Ms + Es − Xsk2

αs
2
+ βskEsk1 + βtkEtk1 + λckMck∗ + λskMsk∗ + λtkMtk∗

kAtMc + Mt + Et − Xtk2
F

αt
2

F +

(12)

Many efﬁcient and scalable algorithms have been proposed to solve such nuclear norm regular-
ized convex optimization problems  including proximal gradient algorithm [6  14]  ﬁxed point and
Bregman iterative method [9]. However  though the nuclear norm is a convex envelope of the rank
function  it is not always a high-quality approximation of the rank function [11]. Moreover  it is very
difﬁcult to select the appropriate trade-off parameters λs  λt for the nuclear norm regularizers in (12)
to recover the low-rank matrix solutions in the original optimization in (11). In principal component
analysis problems it is much more convenient to have explicit control on the rank of the low-rank so-
lution matrices. Therefore instead of solving the nuclear norm based convex optimization problem
(12)  we develop a scalable and efﬁcient proximal gradient algorithm to solve the rank constraint
based minimization problem (11) directly  which is shown to converge to a stationary point.
After solving the optimization problem (11)  the low-rank approximation of the corrupted matrix Xt
can be obtained as ˆXt = AtMc + Mt.

4 Proximal Projected Gradient Descent Algorithm

Proximal gradient methods have been popularly used for unconstrained convex optimization prob-
lems with continuous but non-smooth regularizers [2]. In this work  we develop a proximal projected
gradient algorithm to solve the non-convex optimization problem with matrix rank constraints in
(11). Let Θ = [Mc; Ms; Mt; Es; Et] be the optimization variable set of (11). We denote the objec-
tive function of (11) as F (Θ) such that

F (Θ) = f (Θ) + g(Θ)

f (Θ) =

αs
2

kAsMc + Ms + Es − Xsk2

F +

αt
2

kAtMc + Mt + Et − Xtk2
F

g(Θ) = βskEsk1 + βtkEtk1

(13)

(14)

(15)

4

Algorithm 1 Proximal Projected Gradient Descent

c

s

  E(1)

  M (1)

  M (1)

Input: data matrices Xs  Xt  parameters αs  αt  βs  βt  kc  ks  kt.
Set η = 3 max(αs  αt)  k = 1.
Initialize M (1)
While not converged do
• Set Θ(k) = [M (k)
• Update M (k+1)
E(k+1)
• Set k = k + 1.

; M (k)
; E(k)
].
= pMs(η  Θ(k))  M (k+1)
= pMc (η  Θ(k))  M (k+1)
= pEs(η  Θ(k))  E(k+1)

= pEt(η  Θ(k)).

; M (k)

; E(k)

  E(1)

s

s

s

s

s

c

c

t

t

t

t

t

t

End While

= pMt (η  Θ(k)) 

Here f (Θ) is a convex and continuously differentiable function while g(Θ) is a convex but non-
smooth function. For any η > 0  we consider the following quadratic approximation of F (Θ) at a

Qη(Θ eΘ) = f (eΘ) + hΘ − eΘ  ∇f (eΘ)i +

given point eΘ = [fMc;fMs;fMt; eEs; eEt]
where ∇f (eΘ) is the gradient of the function f (·) at point eΘ. Let C = {Θ : rank(Mc) ≤ kc 
rank(Ms) ≤ ks  rank(Mt) ≤ kt}. The minimization over Qη(Θ eΘ) can be conducted as
F(cid:27)
∇f (eΘ))(cid:13)(cid:13)2

Qη(Θ eΘ) = arg min

p(η eΘ) = arg min

which admits the following closed-form solution through singular value decomposition and soft-
thresholding:

Θ∈C (cid:26)g(Θ) +

2(cid:13)(cid:13)Θ − (eΘ −

kΘ − eΘk2

F + g(Θ)

(16)

(17)

1
η

η
2

Θ∈C

η

for U ΣV ⊤ = SVD(fMc − 1
for U ΣV ⊤ = SVD(fMs − 1
for U ΣV ⊤ = SVD(fMt − 1

η )+ ◦ sign(cEs)  with cEs = fEs − 1
η )+ ◦ sign(cEt)  with cEt = fEt − 1

η ∇Mc f (eΘ))
η ∇Ms f (eΘ))
η ∇Mt f (eΘ))
η ∇Es f (eΘ)
η ∇Et f (eΘ)

kc  

ks  

pMc (η eΘ) = UkcΣkc V ⊤
pMs (η eΘ) = UksΣks V ⊤
pMt (η eΘ) = UktΣkt V ⊤
pEs (η eΘ) = (|cEs| − βs
pEt (η eΘ) = (|cEt| − βt

kt  

where Uk and Vk denote the top k singular vectors from U and V respectively  and Σk denotes
the diagonal matrix with the corresponding top k singular values for k ∈ {kc  ks  kt} respectively;

the operator “◦” denotes matrix Hadamard product  and the operator (·)+ = max(·  0); ∇Mc f (eΘ) 
∇Ms f (eΘ)  ∇Mt f (eΘ)  ∇Es f (eΘ)  and ∇Et f (eΘ) denote parts of the gradient matrix ∇f (eΘ) corre-

sponding to Mc  Ms  Mt  Es  Et respectively.
Our proximal projected gradient algorithm is an iterative procedure. After ﬁrst initializing the pa-
rameter matrices to zeros  in each k-th iteration  it updates the model parameters by minimizing the
approximation function Q(Θ  Θ(k)) at the given point Θ(k)  using the closed-form update equations
above. The overall procedure is given in Algorithm 1. Below we discuss the convergence property
of this algorithm.

Lemma 1 For η = 3 max(αs  αt)  we have F (Θ) ≤ Qη(Θ eΘ) for every feasible Θ eΘ.

Proof: First it is easy to check that η = 3 max(αs  αt) is a Lipschitz constant of ∇f (Θ)  such that

k∇f (Θ) − ∇f (eΘ)kF ≤ ηkΘ − eΘkF

for any feasible pair Θ eΘ

Thus f (·) is a continuously differentiable function with Lipschitz continuous gradient and Lipschitz
constant η. Following [2  Lemma 2.1]  we have

(18)

f (Θ) ≤ f (eΘ) + hΘ − eΘ  ∇f (eΘ)i +

η
2

kΘ − eΘk2

F

for any feasible pair Θ eΘ

(19)

5

Based on (16) and (19)  we can then derive

F (Θ) = f (Θ) + g(Θ) ≤ f (eΘ) + hΘ − eΘ  ∇f (eΘ)i +

(cid:3)
Based on this lemma  we can see the update steps of Algorithm 1 satisfy

kΘ − eΘk2

F + g(Θ) = Qη(Θ eΘ)

η
2

F (Θ(k+1)) ≤ Qη(Θ(k+1)  Θ(k)) ≤ Qη(Θ(k)  Θ(k)) = F (Θ(k))

(20)

(21)

Therefore the sequence of points  Θ(1)  Θ(2)  . . .   Θ∗  produced by Algorithm 1 have nonincreasing
function values F (Θ(1)) ≥ F (Θ(2)) ≥ . . . ≥ F (Θ∗)  and converge to a stationary point.

5 Experiments

We evaluate the proposed approach using image denoising tasks constructed on the Yale Face
Database  which contains 165 grayscale images of 15 individuals. There are 11 images per subject 
one per different facial expression or conﬁguration.

Our goal is to investigate the performance of the proposed approach on recovering data corrupted
with large and dense errors. Thus we constructed noisy images by adding large errors. Let X 0
t denote
a target image matrix from one subject  which has values between 0 and 255. We randomly select
a fraction of its pixels to add large errors to reach value 255  where the fraction of noisy pixels is
controlled using a noise level parameter σ. The obtained noisy target image matrix is Xt. We then
s from the same or different subject as the source matrix to help
use an uncorrupted image matrix X 0
the image denoising of Xt by recovering its low-rank approximation matrix ˆXt. In the experiments 
we compared the performance of the following methods on image denoising with large errors:

• R-T-PCA: This is the proposed robust transfer PCA method. For this method  we used

parameters αs = αt = 1  βs = βt = 0.1  unless otherwise speciﬁed.

• R-S-PCA: This is a robust shared PCA method that applies a rank-constrained version of
s ; Xt] to recover a low-rank approx-

the robust PCA in [14] on the concatenated matrix [X 0
imation matrix ˆXt with rank kc + kt.

• R-PCA: This is a robust PCA method that applies a rank-constrained version of the robust

PCA in [14] on Xt to recover a low-rank approximation matrix ˆXt with rank kc + kt.

• S-PCA: This is a shared PCA method that applies PCA on concatenated matrix [X 0

s ; Xt] to

recover a low-rank approximation matrix ˆXt with rank kc + kt.

• PCA: This method applies PCA on the noisy target matrix Xt to recover a low-rank ap-

proximation matrix ˆXt with rank kc + kt.

• R-2Step-PCA: This method exploits the auxiliary source matrix by ﬁrst performing robust
PCA over the concatenated matrix [X 0
s ; Xt] to produce a shared matrix Mc with rank kc 
and then performing robust PCA over the residue matrix (Xt − AtMc) to produce a matrix
Mt with rank kt. The ﬁnal low-rank approximation of Xt is given by ˆXt = AtMc + Mt.

All the methods are evaluated using the root mean square error (RMSE) between the true target
t and the low-rank approximation matrix ˆXt recovered from the noisy image matrix.
image matrix X 0
Unless speciﬁed otherwise  we used kc = 8  ks = 3  kt = 3 in all experiments.

5.1

Intra-Subject Experiments

We ﬁrst conducted experiments by constructing 15 transfer tasks for the 15 subjects. Speciﬁcally  for
each subject  we used the ﬁrst image matrix as the target matrix and used each of the remaining 10
image matrices as the source matrix each time. For each source matrix  we repeated the experiments
5 times by randomly generating noisy target matrix using the procedure described above. Thus in
total  for each experiment  we have results from 50 runs. The average denoising results in terms of
root mean square error (RMSE) with noise level σ = 5% are reported in Table 1. The standard devi-
ations for these results range between 0.001 and 0.015. We also present one visualization result for
Task-1 in Figure 1. We can see that the proposed method R-T-PCA outperforms all other methods

6

Table 1: The average denoising results in terms of RMSE at noise level σ = 5%.

Tasks
Task-1
Task-2
Task-3
Task-4
Task-5
Task-6
Task-7
Task-8
Task-9
Task-10
Task-11
Task-12
Task-13
Task-14
Task-15

R-T-PCA R-S-PCA R-PCA S-PCA PCA R-2Step-PCA

0.143
0.134
0.136
0.140
0.142
0.156
0.172
0.203
0.140
0.198
0.191
0.151
0.193
0.176
0.159

10

20

30

40

50

60

0.185
0.167
0.153
0.162
0.166
0.195
0.206
0.222
0.159
0.209
0.211
0.189
0.218
0.201
0.170

0.218
0.201
0.226
0.201
0.241
0.196
0.300
0.223
0.203
0.259
0.283
0.194
0.277
0.240
0.266

0.330
0.320
0.386
0.369
0.382
0.290
0.477
0.348
0.317
0.394
0.389
0.337
0.436
0.366
0.413

0.365
0.353
0.430
0.406
0.414
0.310
0.523
0.386
0.349
0.439
0.423
0.366
0.474
0.392
0.464

0.212
0.202
0.215
0.215
0.208
0.202
0.264
0.243
0.201
0.255
0.274
0.213
0.257
0.224
0.245

Source

Target

Noise: 5%

10

20

30

40

50

60

10

20

30

40

50

60

R−T−PCA

R−S−PCA

R−PCA

S−PCA

PCA

R−2Step−PCA

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

Figure 1: Denoising results on Task-1.

across all the 15 tasks. The comparison between the two groups of methods  {R-T-PCA  R-S-PCA 
S-PCA} and {R-PCA  PCA}  shows that a related source matrix is indeed useful for denoising the
target matrix. The superior performance of R-T-PCA over R-2Step-PCA demonstrates the effective-
ness of our joint optimization framework over its stepwise alternative. The superior performance
of R-T-PCA over R-S-PCA and S-PCA demonstrates the efﬁcacy of our transfer PCA framework
in exploiting the auxiliary source matrix over methods that simply concatenate the auxiliary source
matrix and target matrix.

5.2 Cross-Subject Experiments

Next  we conducted transfer experiments using source matrix and target matrix from different sub-
jects. We randomly constructed 5 transfer tasks  Task-6-1  Task-8-2  Task-9-4  Task-12-8 and Task-
14-11  where the ﬁrst number in the task name denotes the source subject index and second number
denotes the target subject index. For example  to construct Task-6-1  we used the ﬁrst image matrix
from subject-6 as the source matrix and used the ﬁrst image matrix from subject-1 as the target ma-
trix. For each task  we conducted experiments with two different noise levels  5% and 10%. We re-
peated each experiment 10 times using randomly generated noisy target matrix. The average results
in terms of RMSE are reported in Table 2 with standard deviations less than 0.015. We can see that
with the increase of noise level  the performance of all methods degrades. But at each noise level  the
comparison results are similar to what we observed in previous experiments: The proposed method
outperforms all other methods. These results also suggest that even a remotely related source image
can be useful. All these experiments demonstrate the efﬁcacy of the proposed method in exploiting
uncorrupted auxiliary data matrix for denoising target images corrupted with large errors.

7

Table 2: The average denoising results in terms of RMSE.

Tasks

R-T-PCA R-S-PCA R-PCA S-PCA PCA R-2Step-PCA

Task-6-1

Task-8-2

Task-9-4

Task-12-8

Task-14-11

σ=5%
σ=10%
σ=5%
σ=10%
σ=5%
σ=10%
σ=5%
σ=10%
σ=5%
σ=10%

0.147
0.203
0.132
0.154
0.148
0.204
0.207
0.244
0.172
0.319

0.177
0.246
0.159
0.211
0.170
0.240
0.231
0.272
0.215
0.368

0.224
0.326
0.234
0.323
0.229
0.344
0.245
0.359
0.274
0.431

0.337
0.490
0.313
0.457
0.373
0.546
0.373
0.518
0.403
0.592

0.370
0.526
0.354
0.500
0.410
0.585
0.397
0.548
0.424
0.612

0.218
0.291
0.200
0.276
0.212
0.282
0.249
0.317
0.268
0.372

 

R−T−PCA

0.19

0.18

0.17

0.16

0.15

 

E
S
M
R
e
g
a
r
e
v
A

 

0 0.1

0.25

0.5

β Value

 

1

R−T−PCA
R−S−PCA
R−PCA
S−PCA
PCA
R−2Step−PCA

0.5

0.4

0.3

0.2

E
S
M
R
 
e
g
a
r
e
v
A

 

(6 3 3)

(10 3 3)
(8 3 3)
  k
  k
Different settings of k
s
c
t

(8 5 5)

(10 5 5)

Figure 2: Parameter analysis on Task-6-1 with σ = 5%.

5.3 Parameter Analysis

The optimization problem (11) for the proposed R-T-PCA method has a number of parameters to
be set: αs  αt  βs  βt  kc  ks and kt. To investigate the inﬂuence of these parameters over the per-
formance of the proposed method  we conducted two experiments using the ﬁrst cross-subject
task  Task-6-1  with noise level σ = 5%. Given that the source and target matrices are similar
in size  in these experiments we set αs = αt = 1  βs = βt and ks = kt. In the ﬁrst experiment  we
set (kc  ks  kt) = (8  3  3) and study the performance of R-T-PCA with different βs = βt= β val-
ues  for β ∈ {0.01  0.025  0.05  0.1  0.25  0.5  1}. The average RMSE results over 10 runs are pre-
sented in the left sub-ﬁgure of Figure 2. We can see that R-T-PCA is quite robust to β within
the range of values  {0.05  0.1  0.25  0.5  1}. In the second experiment  we ﬁxed βs = βt = 0.1
and compared R-T-PCA with other methods across a few different settings of (kc  ks  kt)  with
(kc  ks  kt) ∈ {(6  3  3)  (8  3  3)  (8  5  5)  (10  3  3)  (10  5  5)}. The average comparison results in
terms of RMSE are presented in the right sub-ﬁgure of Figure 2. We can see that though the per-
formance of all methods varies across different settings  R-T-PCA is less sensitive to the parameter
changes comparing to the other methods and it produced the best result across different settings.

6 Conclusion

In this paper  we developed a novel robust transfer principal component analysis method to recover
the low-rank representation of corrupted data by leveraging related uncorrupted auxiliary data. This
robust transfer PCA framework combines aspects of both robust PCA and transfer learning method-
ologies. We formulated this method as a joint minimization problem over a convex combination of
least squares losses with non-convex matrix rank constraints  and developed a proximal projected
gradient descent algorithm to solve the proposed optimization problem  which permits a convenient
closed-form solution for each proximal step based on singular value decomposition and converges to
a stationary point. Our experiments over image denoising tasks demonstrated the proposed method
can effectively exploit auxiliary uncorrupted image to recover images corrupted with random large
errors and signiﬁcantly outperform a number of comparison methods.

8

References

[1] F. Bach. Consistency of trace norm minimization. Journal of Machine Learning Research 

9:1019–1048  2008.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sciences  2  No. 1:183–202  2009.

[3] M Fazel. Matrix Rank Minimization with Applications. PhD thesis  Stanford University  2002.
[4] S. Gupta  D. Phung  B. Adams  and S. Venkatesh. Regularized nonnegative shared subspace

learning. Data Mining and Knowledge Discovery  26:57–97  2013.

[5] P. Huber. Robust Statistics. New York  New York  1981.
[6] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization.

International Conference on Machine Learning (ICML)  2009.

In Proc. of

[7] I. Jolliffe. Principal Component Analysis. Springer-Verlag  New York  New York  1986.
[8] Q. Ke and T. Kanade. Robust l1 norm factorization in the presence of outliers and missing data
by alternative convex programming. In Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2005.

[9] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank

minimization. Mathematical Programming  2009.

[10] S. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and

Data Engineering  2010.

[11] B. Recht  M. Fazel  and P. Parrilo. Guaranteed minimum rank solutions to linear matrix equa-

tions via nuclear norm minimization. SIAM Review  52  no 3:471–501  2010.

[12] M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal

Statistical Society  B  6(3):611–622  1999.

[13] F. De La Torre and M. Black. A framework for robust subspace learning. International Journal

of Computer Vision (IJCV)  54(1-3):117–142  2003.

[14] J. Wright  Y. Peng  Y. Ma  A. Ganesh  and S. Rao. Robust principal component analysis:
Exact recovery of corrupted low-rank matrices by convex optimization. In Advances in Neural
Information Processing Systems (NIPS)  2009.

[15] X. Zhang  Y. Yu  M. White  R. Huang  and D. Schuurmans. Convex sparse coding  subspace
learning  and semi-supervised extensions. In Proc. of AAAI Conference on Artiﬁcial Intelli-
gence (AAAI)  2011.

9

,Yuhong Guo
Isabel Valera
Zoubin Ghahramani
Ting-Chun Wang
Ming-Yu Liu
Jun-Yan Zhu
Guilin Liu
Andrew Tao
Jan Kautz
Bryan Catanzaro