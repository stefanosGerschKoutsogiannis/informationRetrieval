2012,The representer theorem for Hilbert spaces: a necessary and sufficient condition,The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper  we extend such result by weakening the assumptions on the regularization term. In particular  the main result of this paper implies that  for a sufficiently large family of regularization functionals  radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data.,The representer theorem for Hilbert spaces: a

necessary and sufﬁcient condition

Francesco Dinuzzo and Bernhard Sch¨olkopf
Max Planck Institute for Intelligent Systems

Spemannstrasse 38 72076 T¨ubingen

[fdinuzzo@tuebingen.mpg.de  bs@tuebingen.mpg.de]

Germany

Abstract

The representer theorem is a property that lies at the foundation of regularization
theory and kernel methods. A class of regularization functionals is said to admit
a linear representer theorem if every member of the class admits minimizers that
lie in the ﬁnite dimensional subspace spanned by the representers of the data.
A recent characterization states that certain classes of regularization functionals
with differentiable regularization term admit a linear representer theorem for any
choice of the data if and only if the regularization term is a radial nondecreasing
function. In this paper  we extend such result by weakening the assumptions on
the regularization term. In particular  the main result of this paper implies that 
for a sufﬁciently large family of regularization functionals  radial nondecreasing
functions are the only lower semicontinuous regularization terms that guarantee
existence of a representer theorem for any choice of the data.

1

Introduction

Regularization [1] is a popular and well-studied methodology to address ill-posed estimation prob-
lems [2] and learning from examples [3]. In this paper  we focus on regularization problems deﬁned
over a real Hilbert space H. A Hilbert space is a vector space endowed with a inner product and a
norm that is complete1. Such setting is general enough to take into account a broad family of ﬁnite-
dimensional regularization techniques such as regularized least squares or support vector machines
(SVM) for classiﬁcation or regression  kernel principal component analysis  as well as a variety of
methods based on regularization over reproducing kernel Hilbert spaces (RKHS).
The focus of our study is the general problem of minimizing an extended real-valued regularization
functional J : H → R ∪ {+∞} of the form

J(w) = f (L1w  . . .   L(cid:96)w) + Ω(w) 

(1)
where L1  . . .   L(cid:96) are bounded linear functionals on H. The functional J is the sum of an error
term f  which typically depends on empirical data  and a regularization term Ω that enforces certain
desirable properties on the solution. By allowing the error term f to take the value +∞  problems
with hard constraints on the values Liw (for instance  interpolation problems) are included in the
framework. Moreover  by allowing Ω to take the value +∞  regularization problems of the Ivanov
type are also taken into account.
In machine learning  the most common class of regularization problems concerns a situation where
a set of data pairs (xi  yi) is available  H is a space of real-valued functions  and the objective
functional to be minimized is of the form

J(w) = c ((x1  y1  w(x1)) ···   (x(cid:96)  y(cid:96)  w(x(cid:96))) + Ω(w).

1Meaning that Cauchy sequences are convergent.

1

(cid:96)(cid:88)

i=1

(cid:96)(cid:88)

i=1

It is easy to see that this setting is a particular case of (1)  where the dependence on the data pairs
(xi  yi) can be absorbed into the deﬁnition of f  and Li are point-wise evaluation functionals  i.e.
such that Liw = w(xi). Several popular techniques can be cast in such regularization framework.
Example 1 (Regularized least squares). Also known as ridge regression when H is ﬁnite-
dimensional. Corresponds to the choice

c ((x1  y1  w(x1)) ···   (x(cid:96)  y(cid:96)  w(x(cid:96))) = γ

(yi − w(xi))2 

and Ω(w) = (cid:107)w(cid:107)2  where the complexity parameter γ ≥ 0 controls the trade-off between ﬁtting of
training data and regularity of the solution.
Example 2 (Support vector machine). Given binary labels yi = ±1  the SVM classiﬁer (without
bias) can be interpreted as a regularization method corresponding to the choice

c ((x1  y1  w(x1)) ···   (x(cid:96)  y(cid:96)  w(x(cid:96))) = γ

max{0  1 − yiw(xi)} 

and Ω(w) = (cid:107)w(cid:107)2. The hard-margin SVM can be recovered by letting γ → +∞.
Example 3 (Kernel principal component analysis). Kernel PCA can be shown to be equivalent to a
regularization problem where
c ((x1  y1  w(x1)) ···   (x(cid:96)  y(cid:96)  w(x(cid:96))) =
 
and Ω is any strictly monotonically increasing function of the norm (cid:107)w(cid:107)  see [4]. In this problem 
there are no labels yi  but the feature extractor function w is constrained to produce vectors with
unitary empirical variance.

0 
+∞  otherwise

w(xi) − 1

(cid:80)(cid:96)

(cid:80)(cid:96)

j=1 w(xj)

(cid:17)2

(cid:40)

(cid:16)

1
(cid:96)

i=1

= 1

(cid:96)

The possibility of choosing general continuous linear functionals Li in (1) allows to consider a much
broader class of regularization problems. Some examples are the following.
Example 4 (Tikhonov deconvolution). Given a “input signal” u : X → R  assume that the convo-
lution u ∗ w is well-deﬁned for any w ∈ H  and the point-wise evaluated convolution functionals

Ω(w) =

φ(w) ≤ 1

+∞  otherwise .

(cid:26) 0 

2

(cid:90)

X

(cid:90)

X

are continuous. A possible way to recover w from noisy measurements yi of the “output signal” is
to solve regularization problems such as

Liw = (u ∗ w)(xi) =

u(s)w(xi − s)ds 

(cid:32)

(cid:96)(cid:88)

i=1

min
w∈H

γ

(cid:33)

(yi − (u ∗ w)(xi))2 + (cid:107)w(cid:107)2

 

where the objective functional is of the form (1).
Example 5 (Learning from probability measures). In certain learning problems  it may be appropri-
ate to represent input data as probability distributions. Given a ﬁnite set of probability measures Pi
on a measurable space (X  A)  where A is a σ-algebra of subsets of X   introduce the expectations

Liw = EPi(w) =

w(x)dPi(x).

Then  given output labels yi  one can learn a input-output relationship by solving regularization
problems of the form

(cid:0)c ((y1  EP1 (w)) ···   (y(cid:96)  EP(cid:96)(w)) + (cid:107)w(cid:107)2(cid:1) .

min
w∈H

If the expectations are bounded linear functionals  such regularization functional is of the form (1).
Example 6 (Ivanov regularization). By allowing the regularization term Ω to take the value +∞ 
we can also take into account the whole class of Ivanov-type regularization problems of the form

min
w∈H f (L1w  . . .   L(cid:96)w) 

subject to

φ(w) ≤ 1 

by reformulating them as the minimization of a functional of the type (1)  where

1.1 The representer theorem

Let’s now go back to the general formulation (1). By the Riesz representation theorem [5  6]  J can
be rewritten as

J(w) = f ((cid:104)w  w1(cid:105)  . . .  (cid:104)w  w(cid:96)(cid:105)) + Ω(w) 

where wi is the representer of the linear functional Li with respect to the inner product. Consider
the following deﬁnition.
Deﬁnition 1. A family F of regularization functionals of the form (1) is said to admit a linear
representer theorem if  for any J ∈ F  and any choice of bounded linear functionals Li  there exists
a minimizer w∗ that can be written as a linear combination of the representers:

(cid:96)(cid:88)

i=1

∗ =

w

ciwi.

If a linear representer theorem holds  the regularization problem under study can be reduced to a
(cid:96)-dimensional optimization problem on the scalar coefﬁcients ci  independently of the dimension
of H. This property is fundamental in practice: without a ﬁnite-dimensional parametrization  it
wouldn’t be possible to employ numerical optimization techniques to compute a solution. Sufﬁ-
cient conditions under which a family of functionals admits a representer theorem have been widely
studied in the literature of statistics  inverse problems  and machine learning. The theorem also pro-
vides the foundations of learning techniques such as regularized kernel methods and support vector
machines  see [7  8  9] and references therein.
Representer theorems are of particular interest when H is a reproducing kernel Hilbert space
(RKHS) [10]. Given a non-empty set X   a RKHS is a space of functions w : X → R such that
point-wise evaluation functionals are bounded  namely  for any x ∈ X   there exists a non-negative
real number Cx such that

|w(x)| ≤ Cx(cid:107)w(cid:107) 

∀w ∈ H.

It can be shown that a RKHS can be uniquely associated to a positive-semideﬁnite kernel function
K : X × X → R (called reproducing kernel)  such that the so-called reproducing property holds:

w(x) = (cid:104)w  Kx(cid:105) 

∀ (x  w) ∈ X × H 

where the kernel sections Kx are deﬁned as

Kx(y) = K(x  y) 

∀y ∈ X .

The reproducing property states that the representers of point-wise evaluation functionals coincide
with the kernel sections. Starting from the reproducing property  it is also easy to show that the
representer of any bounded linear functional L is given by a function KL ∈ H such that

KL(x) = LKx 

∀x ∈ X .

Therefore  in a RKHS  the representer of any bounded linear functional can be obtained explicitly
in terms of the reproducing kernel.
If the regularization functional (1) admits minimizers  and the regularization term Ω is a nondecreas-
ing function of the norm  i.e.

Ω(w) = h((cid:107)w(cid:107))  with h : R → R ∪ {+∞}  nondecreasing 

(2)
the linear representer theorem follows easily from the Pythagorean identity. A proof that the con-
dition (2) is sufﬁcient appeared in [11] in the case where H is a RKHS and Li are point-wise
evaluation functionals. Earlier instances of representer theorems can be found in [12  13  14]. More
recently  the question of whether condition (2) is also necessary for the existence of linear repre-
senter theorems has been investigated [15]. In particular  [15] shows that  if Ω is differentiable (and
certain technical existence conditions hold)  then (2) is a necessary and sufﬁcient condition for cer-
tain classes of regularization functionals to admit a representer theorem. The proof of [15] heavily
exploits differentiability of Ω  but the authors conjecture that the hypothesis can be relaxed. In the
following  we indeed show that (2) is necessary and sufﬁcient for the family of regularization func-
tionals of the form (1) to admit a linear representer theorem  by merely assuming that Ω is lower
semicontinuous and satisﬁes basic conditions for the existence of minimizers. The proof is based on
a characterization of radial nondecreasing functions deﬁned on a Hilbert space.

3

2 A characterization of radial nondecreasing functions

In this section  we present a characterization of radial nondecreasing functions deﬁned over Hilbert
spaces. We will make use of the following deﬁnition.
Deﬁnition 2. A subset S of a Hilbert space H is called star-shaped with respect to a point z ∈ H if

(1 − λ)z + λx ∈ S 

∀x ∈ S 

∀λ ∈ [0  1].

It is easy to verify that a convex set is star-shaped with respect to any point of the set  whereas a
star-shaped set does not have to be convex.
The following Theorem provides a geometric characterization of radial nondecreasing functions
deﬁned on a Hilbert space that generalizes the analogous result of [15] for differentiable functions.
Theorem 1. Let H denote a Hilbert space such that dimH ≥ 2  and Ω : H → R ∪ {+∞} a lower
semicontinuous function. Then  (2) holds if and only if

Ω(x + y) ≥ max{Ω(x)  Ω(y)} 

∀x  y ∈ H : (cid:104)x  y(cid:105) = 0.

(3)

Proof. Assume that (2) holds. Then  for any pair of orthogonal vectors x  y ∈ H  we have

(cid:16)(cid:112)(cid:107)x(cid:107)2 + (cid:107)y(cid:107)2

(cid:17) ≥ max{h ((cid:107)x(cid:107))   h ((cid:107)y(cid:107))}

Ω(x + y) = h ((cid:107)x + y(cid:107)) = h

= max{Ω(x)  Ω(y)}.

Conversely  assume that condition (3) holds. Since dimH ≥ 2  by ﬁxing a generic vector x ∈
X \ {0} and a number λ ∈ [0  1]  there exists a vector y such that (cid:107)y(cid:107) = 1 and

where

In view of (3)  we have

λ = 1 − cos2 θ 

cos θ =

(cid:104)x  y(cid:105)
(cid:107)x(cid:107)(cid:107)y(cid:107) .

Ω(x) = Ω(x − (cid:104)x  y(cid:105)y + (cid:104)x  y(cid:105)y)

≥ Ω(x − (cid:104)x  y(cid:105)y) = Ω(cid:0)x − cos2 θx + cos2 θx − (cid:104)x  y(cid:105)y(cid:1)

≥ Ω (λx) .

Since the last inequality trivially holds also when x = 0  we conclude that
∀λ ∈ [0  1] 

Ω(x) ≥ Ω(λx) 

∀x ∈ H 

(4)

so that Ω is nondecreasing along all the rays passing through the origin. In particular  the minimum
of Ω is attained at x = 0.
Now  for any c ≥ Ω(0)  consider the sublevel sets

Sc = {x ∈ H : Ω(x) ≤ c} .

From (4)  it follows that Sc is not empty and star-shaped with respect to the origin. In addition  since
Ω is lower semicontinuous  Sc is also closed. We now show that Sc is either a closed ball centered
at the origin  or the whole space. To this end  we show that  for any x ∈ Sc  the whole ball

B = {y ∈ H : (cid:107)y(cid:107) ≤ (cid:107)x(cid:107)} 

is contained in Sc. First  take any y ∈ int(B) \ span{x}  where int denotes the interior. Then  y has
norm strictly less than (cid:107)x(cid:107)  that is

0 < (cid:107)y(cid:107) < (cid:107)x(cid:107) 

and is not aligned with x  i.e.

y (cid:54)= λx 

∀λ ∈ R.

4

Let θ ∈ R denote the angle between x and y. Now  construct a sequence of points xk as follows:

(cid:26) x0 = y 

xk+1 = xk + akuk 

(cid:18) θ

(cid:19)

n

where

ak = (cid:107)xk(cid:107) tan

 

n ∈ N

and uk is the unique unitary vector that is orthogonal to xk  belongs to the two-dimensional subspace
span{x  y}  and is such that (cid:104)uk  x(cid:105) > 0  that is
(cid:107)uk(cid:107) = 1 

uk ∈ span{x  y} 

(cid:104)uk  xk(cid:105) = 0 

(cid:104)uk  x(cid:105) > 0.

(cid:18)

1 + tan2

(cid:19)(cid:19)k+1

(cid:18) θ

n

.

(5)

See Figure 1 for a geometrical illustration of the sequence xk.
By orthogonality  we have

(cid:107)xk+1(cid:107)2 = (cid:107)xk(cid:107)2 + a2

k = (cid:107)xk(cid:107)2

1 + tan2

n
In addition  the angle between xk+1 and xk is given by

(cid:18)

(cid:18) θ
(cid:18) ak(cid:107)xk(cid:107)

(cid:19)(cid:19)
(cid:19)

=

= (cid:107)y(cid:107)2

θ
n

 

θk = arctan

so that the total angle between y and xn is given by

n−1(cid:88)

k=0

θk = θ.

Since all the points xk belong to the subspace spanned by x and y  and the angle between x and xn
is zero  we have that xn is positively aligned with x  that is
λ ≥ 0.

xn = λx 

Now  we show that n can be chosen in such a way that λ ≤ 1. Indeed  from (5) we have

λ2 =

and it can be veriﬁed that

(cid:18)(cid:107)xn(cid:107)

(cid:107)x(cid:107)

(cid:18)(cid:107)y(cid:107)

(cid:107)x(cid:107)

(cid:19)2
(cid:18)

=

lim

n→+∞

1 + tan2

(cid:19)2(cid:18)
(cid:18) θ

n

1 + tan2

(cid:19)(cid:19)n

= 1 

(cid:18) θ

(cid:19)(cid:19)n

n

 

n−1(cid:88)

λx − y =

(xk+1 − xk) 

therefore λ ≤ 1 for a sufﬁciently large n. Now  write the difference vector in the form

and observe that

k=0

(cid:104)xk+1 − xk  xk(cid:105) = 0.

By using (4) and proceeding by induction  we have

c ≥ Ω(λx) = Ω (xn − xn−1 + xn−1) ≥ Ω(xn−1) ≥ ··· ≥ Ω(x0) = Ω(y) 

so that y ∈ Sc. Since Sc is closed and the closure of int(B) \ span{x} is the whole ball B  every
point y ∈ B is also included in Sc. This proves that Sc is either a closed ball centered at the origin 
or the whole space H.
Finally  for any pair of points such that (cid:107)x(cid:107) = (cid:107)y(cid:107)  we have x ∈ SΩ(y)  and y ∈ SΩ(x)  so that

Ω(x) = Ω(y).

5

Figure 1: The sequence xk constructed in the proof of Theorem 1 is associated with a geometrical
construction known as spiral of Theodorus. Starting from any y in the interior of the ball (excluding
points aligned with x)  a point of the type λx (with 0 ≤ λ ≤ 1) can be reached by using a ﬁnite
number of right triangles.

3 Representer theorem: a necessary and sufﬁcient condition

In this section  we prove that condition (2) is necessary and sufﬁcient for suitable families of regu-
larization functionals of the type (1) to admit a linear representer theorem.
Theorem 2. Let H denote a Hilbert space of dimension at least 2. Let F denote a family of func-
tionals J : H → R ∪ {+∞} of the form (1) that admit minimizers  and assume that F contains a
set of functionals of the form

p (w) = γf ((cid:104)w  p(cid:105)) + Ω (w)  
J γ

(6)
where f (z) is uniquely minimized at z = 1. Then  for any lower semicontinuous Ω  the family F
admits a linear representer theorem if and only if (2) holds.

∀γ ∈ R+ 

∀p ∈ H 

Proof. The ﬁrst part of the theorem (sufﬁciency) follows from an orthogonality argument. Take any
functional J ∈ F. Let R = span{w1  . . .   w(cid:96)} and let R⊥ denote its orthogonal complement. Any
minimizer w∗ of J can be uniquely decomposed as

If (2) holds  then we have

w

∗ = u + v 

u ∈ R 

v ∈ R⊥

.

J(w
so that u ∈ R is also a minimizer.
Now  let’s prove the second part of the theorem (necessity). First of all  observe that the functional

∗) − J(u) = h((cid:107)w

∗(cid:107)) − h((cid:107)u(cid:107)) ≥ 0 

obtained by setting p = 0 in (6)  belongs to F. By hypothesis  J γ
by the representer theorem  the only admissible minimizer of J0 is the origin  that is

0 admits minimizers. In addition 

J γ
0 (w) = γf (0) + Ω(w) 

Ω(y) ≥ Ω(0) 

∀y ∈ H.

(7)

Now take any x ∈ H \ {0} and let

p =

x
(cid:107)x(cid:107)2 .

By the representer theorem  the functional J γ

p of the form (6) admits a minimizer of the type
w = λ(γ)x.

Now  take any y ∈ H such that (cid:104)x  y(cid:105) = 0. By using the fact that f (z) is minimized at z = 1  and
the linear representer theorem  we have
γf (1) + Ω (λ(γ)x) ≤ γf (λ(γ)) + Ω (λ(γ)x) = J γ
By combining this last inequality with (7)  we conclude that

p (x + y) = γf (1) + Ω (x + y) .

p (λ(γ)x) ≤ J γ
∀x  y ∈ H : (cid:104)x  y(cid:105) = 0 

Ω (x + y) ≥ Ω (λ(γ)x)  

∀γ ∈ R+.

(8)

Now  there are two cases:

6

xy• Ω (x + y) = +∞
• Ω (x + y) = C < +∞.

In the ﬁrst case  we trivially have

Ω (x + y) ≥ Ω(x).

In the second case  using (7) and (8)  we obtain

0 ≤ γ (f (λ(γ)) − f (1)) ≤ Ω (x + y) − Ω (λ(γ)x) ≤ C − Ω(0) < +∞ 

∀γ ∈ R+.

(9)

Let γk denote a sequence such that limk→+∞ γk = +∞  and consider the sequence

ak = γk (f (λ(γk)) − f (1)) .

From (9)  it follows that ak is bounded. Since z = 1 is the only minimizer of f (z)  the sequence ak
can remain bounded only if

lim
k→+∞ λ(γk) = 1.

By taking the limit inferior in (8) for γ → +∞  and using the fact that Ω is lower semicontinuous  we
obtain condition (3). It follows that Ω satisﬁes the hypotheses of Theorem 1  therefore (2) holds.

The second part of Theorem 2 states that any lower semicontinuous regularization term Ω has to be
of the form (2) in order for the family F to admit a linear representer theorem. Observe that Ω is not
required to be differentiable or even continuous. Moreover  it needs not to have bounded lower level
sets. For the necessary condition to hold  the family F has to be broad enough to contain at least
a set of regularization functionals of the form (6). The following examples show how to apply the
necessary condition of Theorem 2 to classes of regularization problems with standard loss functions.

• Let L : R2 → R ∪ {+∞} denote any loss function of the type

such that(cid:101)L(t) is uniquely minimized at t = 0. Then  for any lower semicontinuous regula-

ration term Ω  the family of regularization functionals of the form

L(y  z) =(cid:101)L(y − z) 
(cid:96)(cid:88)

J(w) = γ

L (yi (cid:104)w  wi(cid:105)) + Ω(w) 

i=1

admits a linear representer theorem if and only if (2) holds. To see that the hypotheses of
Theorem 2 are satisﬁed  it is sufﬁcient to consider the subset of functionals with (cid:96) = 1 
y1 = 1  and w1 = p ∈ H. These functionals can be written in the form (6) with

f (z) = L(1  z).

• The class of regularization problems with the hinge (SVM) loss of the form

(cid:96)(cid:88)

J(w) = γ

max{0  1 − yi(cid:104)w  wi(cid:105)} + Ω(w) 

with Ω lower semicontinuous  admits a linear representer theorem if and only if Ω satisfy
(2). For instance  by choosing (cid:96) = 2  and

i=1

(y1  w1) = (1  p) 

(y2  w2) = (−1  p/2) 

we obtain regularization functionals of the form (6) with

f (z) = max{0  1 − z} + max{0  1 + z/2} 

and it is easy to verify that f is uniquely minimized at z = 1.

7

4 Conclusions

Sufﬁciently broad families of regularization functionals deﬁned over a Hilbert space with lower
semicontinuous regularization term admit a linear representer theorem if and only if the regulariza-
tion term is a radial nondecreasing function. More precisely  the main result of this paper (Theorem
2) implies that  for any sufﬁciently large family of regularization functionals  nondecreasing func-
tions of the norm are the only lower semicontinuous (extended-real valued) regularization terms that
guarantee existence of a representer theorem for any choice of the data functionals Li.
As a concluding remark  it is important to observe that other types of regularization terms are possi-
ble if the representer theorem is only required to hold for a restricted subset of the data functionals.
Exploring necessary conditions for the existence of representer theorems under different types of
restrictions on the data functionals is an interesting future research direction.

5 Acknowledgments

The authors would like to thank Andreas Argyriou for useful discussions.

References
[1] A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill Posed Problems. W. H. Winston  Washing-

ton  D. C.  1977.

[2] G. Wahba. Spline Models for Observational Data. SIAM  Philadelphia  USA  1990.
[3] F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American

mathematical society  39:1–49  2001.

[4] B. Sch¨olkopf  A. J. Smola  and K-R M¨uller. Nonlinear component analysis as a kernel eigen-

value problem. Neural Computation  10(5):1299–1319  1998.

[5] F. Riesz. Sur une esp`ece de g´eom´etrie analytique des syst`emes de fonctions sommables.

Comptes rendus de l’Acad´emie des sciences Paris  144:1409–1411  1907.

[6] M. Fr´echet. Sur les ensembles de fonctions et les op´erations lin´eaires. Comptes rendus de

l’Acad´emie des sciences Paris  144:1414–1416  1907.

[7] V. Vapnik. Statistical Learning Theory. Wiley  New York  NY  USA  1998.
[8] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regulariza-
tion  Optimization  and Beyond. (Adaptive Computation and Machine Learning). MIT Press 
2001.

[9] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press  New York  NY  USA  2004.

[10] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical

Society  68:337–404  1950.

[11] B. Sch¨olkopf  R. Herbrich  and A. J. Smola. A generalized representer theorem. In In Pro-
ceedings of the Annual Conference on Computational Learning Theory  pages 416–426  2001.
[12] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of

Mathematical Analysis and Applications  33(1):82–95  1971.

[13] D. Cox and F. O’ Sullivan. Asymptotic analysis of penalized likelihood and related estimators.

The Annals of Statistics  18:1676–1695  1990.

[14] T. Poggio and F. Girosi. Networks for approximation and learning. In Proceedings of the IEEE 

volume 78  pages 1481–1497  1990.

[15] A. Argyriou  C. A. Micchelli  and M. Pontil. When is there a representer theorem? Vector

versus matrix regularizers. Journal of Machine Learning Research  10:2507–2529  2009.

8

,Xinghao Pan
Stefanie Jegelka
Joseph Gonzalez
Joseph Bradley
Michael Jordan