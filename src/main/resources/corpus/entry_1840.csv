2018,Equality of Opportunity in Classification: A Causal Approach,The Equalized Odds (for short  EO)  is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups -- e.g.  in the context of law enforcement  an African-American defendant who would not commit a future crime will have an equal opportunity of being released  compared to a non-recidivating Caucasian defendant. Despite this noble goal  it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system  where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO  and the underlying  elusive  and frequently unobserved  causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary  non-parametric structural causal model. This will  in turn  allow legal and data analysts to interpret currently deployed classifiers through causal lens  linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures  we develop a learning procedure to construct a classifier that is statistically efficient  interpretable  and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.,Equality of Opportunity in Classiﬁcation:

A Causal Approach

Junzhe Zhang

Purdue University  USA
zhang745@purdue.edu

Abstract

Elias Bareinboim

Purdue University  USA

eb@purdue.edu

The Equalized Odds (for short  EO) is one of the most popular measures of dis-
crimination used in the supervised learning setting. It ascertains fairness through
the balance of the misclassiﬁcation rates (false positive and negative) across the
protected groups – e.g.  in the context of law enforcement  an African-American
defendant who would not commit a future crime will have an equal opportunity of
being released  compared to a non-recidivating Caucasian defendant. Despite this
noble goal  it has been acknowledged in the literature that statistical tests based
on the EO are oblivious to the underlying causal mechanisms that generated the
disparity in the ﬁrst place (Hardt et al. 2016). This leads to a critical disconnect
between statistical measures readable from the data and the meaning of discrimina-
tion in the legal system  where compelling evidence that the observed disparity is
tied to a speciﬁc causal process deemed unfair by society is required to characterize
discrimination. The goal of this paper is to develop a principled approach to con-
nect the statistical disparities characterized by the EO and the underlying  elusive 
and frequently unobserved  causal mechanisms that generated such inequality. We
start by introducing a new family of counterfactual measures that allows one to
explain the misclassiﬁcation disparities in terms of the underlying mechanisms
in an arbitrary  non-parametric structural causal model. This will  in turn  allow
legal and data analysts to interpret currently deployed classiﬁers through causal
lens  linking the statistical disparities found in the data to the corresponding causal
processes. Leveraging the new family of counterfactual measures  we develop a
learning procedure to construct a classiﬁer that is statistically efﬁcient  interpretable 
and compatible with the basic human intuition of fairness. We demonstrate our
results through experiments in both real (COMPAS) and synthetic datasets.

Introduction

1
The goal of supervised learning is to provide a statistical basis upon which individuals with different
group memberships can be reliably classiﬁed. For instance  a bank may want to learn a function from
a set of background factors so as to determine whether a customer will repay her loan; a university
may train a classiﬁer to predict the future GPA of an applicant to decide whether to accept her into
the program. The growing adoption of automated systems based on standard classiﬁcation algorithms
throughout society (including in law enforcement  education  and ﬁnance [13  4  8  21  1]) has raised
concerns about potential issues due to unfairness and discrimination.
A recent high-proﬁle example is a risk assessment tool called COMPAS 
which has been widely used across the US to inform decisions in the criminal
justice system. Fig. 1 graphically describes this setting – X represents the
Figure 1: COMPAS
race (0 for Caucasian  1 for African-American) of a defendant and Y stands
for the recidivism outcome (0 for no  1 otherwise)  which are mediated by the prior convictions W   and
confounded by other demographic information Z (e.g.  age  gender) of the defendant. The COMPAS

W

Z

X

Y

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

X

Z

W

Y

ˆY

X

Z

W

Y

ˆY

X

Z

W

Y

ˆY

X

Z

W

Y

ˆY

(a) f (x  z  w)

(b) f1(x) = x

(c) f2(w) = w

(d) f3(z) = z

Figure 2: (a-d) Causal diagrams of classiﬁers f  f1  f2  f3 in COMPAS. Nodes represent variables 
directed arrows for functional relationships  and bi-directed arrows for unknown associations.

tool is a classiﬁer f (x  z  w) (shown in Fig. 2(a)) providing a prediction ˆY on whether the defendant
is expected to commit a future crime. An analysis performed by the news organization ProPublica
revealed that the odds of receiving a positive prediction ( ˆY = 1) for defendants who did not recidivate
were on average higher among African-Americans than their Caucasians counterparts [1]. In words 
the error rates of COMPAS disproportionately misclassiﬁed African-American defendants.
Many attempts have been made to model discrimination in the classiﬁcation setting [26  14  11  9  15].
A recent  noteworthy framework comes under the rubric of Equalized Odds [7] (also referred to as
Error Rate Balance [5])  which constrains the classiﬁcation algorithm such that its disparate error rate
ERx0 x1 (ˆy|y) = P (ˆy|x1  y)− P (ˆy|x0  y) is equalized (and equal to 0) across different demographics
x0  x1  i.e.  the odds of misclassiﬁcation does not disproportionately affect any population sub-group.
In the COMPAS example  the condition ERx0 x1( ˆY = 1|Y = 0) = 0 implies that an African-
American defendant who does not commit a future crime will have an equal opportunity of getting
released  compared to non-recidivating Caucasian defendants. This notion of fairness is natural in
many learning settings and  indeed  has been implemented in a number of algorithms [7  6  25  23].
Unfortunately  the framework of equalized odds is not without its problems. To witness  consider
a binary instance of Fig. 1 where the values of X and Z are determined such that x = z and W
is decided by the function w ← x. We are concerned with the ER disparity induced by different
classiﬁers f1  f2  f3 (Fig. 2(b-d))  where  for instance  ˆy ← f1(x) = x (i.e.  f1 takes only X as input 
and ignores the other features). Remarkably  a simple analysis shows that ERx0 x1 ( ˆY = 1|Y = 0) is
the same (and equal to 1) in all three classiﬁers  despite their fundamentally different mechanisms
associating X and ˆY . Note that f1  f2  f3 corresponds to the direct path X → ˆY   the indirect path
X → W → ˆY   and the remaining spurious (non-causal) paths (e.g.  X ↔ Z → ˆY )  respectively.
This observation is not entirely new  and is part of a pattern noted by [7] – statistical tests based on
the disparate ER are oblivious to the underlying causal mechanisms that generated the data. This
realization has dramatic implications to the applicability of supervised learning in the real world since
it seems to suggest that commonsense notions of discrimination  for example  the unequalized false
positive rate caused by direct discrimination (X → ˆY )  cannot be formally articulated  measured
from data  and  therefore  controlled. More importantly  the legal frameworks of anti-discrimination
laws in the US (e.g.  Title VII) require that to establish a prima facie case of discrimination  the
plaintiff must demonstrate “a strong causal connection” between the alleged discriminatory practice
and the observed statistical disparity  otherwise the case will be dismissed (Texas Dept. of Housing
and Community Affairs v. Inclusive Communities Project  Inc.  576 U.S. __ (2015)). Without a robust
causal basis  an evidence of disparate ER on its own is not sufﬁcient to lead to any legal liability.
More recently  the use of causal reasoning to help open the black-box of decision-making systems
has attracted considerable interest in the community  leading to ﬁne-grained explanations of observed
statistical biases [11  10  25  9]. One of the main tasks of causal inference is to explain “how
nature works ” or more technically  to decompose a composite statistical measure (e.g  the total
variation TVx0 x1(ˆy) = P (ˆy|x1) − P (ˆy|x0))  into its most elementary and interpretable components
[24  17  29]. In particular  [28] introduced the causal explanation formula  which allows fairness
analysts to decompose TV into detailed counterfactual measures describing the effects along direct 
indirect  and spurious paths from X to ˆY . While [28] explains how the statistical inequality in the
observed outcome is brought about  it is unclear how to apply such insight to correct the problematic
behaviors of an alleged  discriminatory policy. Furthermore  the explanation formula allows the
decomposition of marginal measures such as TV  but it’s unable to explain disparities represented by
conditional ones  such as the ER (e.g.  non-recidivating African-American defendants).
This paper aims to overcome these challenges. We develop a causal framework to link the disparities
realized through the ER and the (unobserved) causal mechanisms by which the protected attribute X

2

affects change in the prediction ˆY . Speciﬁcally  (1) we introduce a family of counterfactual measures
capable of describing the ER in terms of the direct  indirect  and spurious paths from X to ˆY on
an arbitrary structural causal model (Defs. 1-3) and we prove different qualitative and quantitative
properties of these measures (Thms. 1-2); (2) we derive adjustment-like formulas to estimate the
counterfactual ERs from observational data (Thms. 3-4)  which are accompanied with an efﬁcient
algorithm (Alg. 1  Thm. 5) to ﬁnd the corresponding admissible sets; (3) we operationalize the
proposed counterfactual estimands through a novel procedure to learn a fair classiﬁer subject to
constraints over the effect along the underlying causal mechanisms (Algs. 2-3  Thm. 6).

2 Preliminaries and Notations
We use capital letters to denote variables (X)  and small letters for their values (x). We use the
abbreviation P (x) to represent the probabilities P (X = x). For arbitrary sets A and B  let A\B
denote the set difference {x : x ∈ A and x (cid:54)∈ B}  and let |A| be the dimension of set A.
The basic semantical framework of our analysis rests on structural causal models (SCM) [16  Ch. 7].
A SCM is a tuple (cid:104)M  P (u)(cid:105)  where M consists of a set of endogenous (observed) variables V and
exogenous (unobserved) variables U. The values of each Vi ∈ V are determined by a structural
function fVi taking as arguments a combination of other endogenous and exogenous variables (i.e. 
Vi ← fVi(P Ai  Ui)  P Ai ⊆ V   Ui ⊆ U )). Values of U are drawn from the distribution P (u). Each
SCM is associated with a directed acyclic graph (DAG) G = (cid:104)V   E(cid:105)  termed a causal diagram  where
nodes V represent endogenous variables and directed edges E stand for functional relations (e.g.  see
Fig. 1). By convention  U are not explicitly shown; a bi-directed arrow between Vi and Vj indicates
the presence of an unobserved confounder (UC) Uk affecting both Vi  Vj  i.e.  Vi ← Uk → Vj.
A path is a sequence of edges where each pair of adjacent edges in the sequence share a node. We use
d-separation and blocking interchangeably  following the convention in [16]. A path from a node X
to a node ˆY consists exclusively of direct arrows pointing away from X is called causal; all the other
non-causal paths are called spurious. The causal paths could be further categorized into the direct
path X → ˆY and the indirect paths  e.g.  X → W → ˆY of Fig. 2(a). Let (X → ˆY )G  (X i−→ ˆY )G
and (X s←→ ˆY )G denote  respectively  the direct  indirect and spurious paths between X and ˆY in
a DAG G. A descendant of X is any node which X has a causal path to (including X itself). The
descendant set of a set X is all descendants of any node in X  which we denote by De(X)G.
An intervention on a set of variables X ⊆ V   denoted by do(x)  is an operation where values of
X are set to constants x  regardless of how they were ordinarily determined (through the functions
fX). We denote by (cid:104)Mx  P (u)(cid:105) a sub-model of a SCM (cid:104)M  P (u)(cid:105) induced by do(x). The potential
response of ˆY to intervention do(x)  denoted by ˆYx(u)  is the solution of ˆY with U = u in the
sub-model Mx; it can be read as the counterfactual sentence “the value that ˆY would have obtained
in situation U = u  had X been x.” Statistically  averaging U’s distribution (P (u)) leads to the
counterfactual variable ˆYx. For a more detailed discussion on SCMs  please refer to [16  2].

3 Counterfactual Analysis of Unequalized Classiﬁcation Errors
In this section  we investigate the unequalized odds of misclassiﬁcation observed in COMPAS by
devising three simple thought experiments. These experiments could be generalized into a set of
novel counterfactual measures  providing a ﬁne-grained explanation of how the ER disparity of a
classiﬁer f ( ˆpa) is brought about. Throughout our analysis  we will let X be the protected attribute  ˆY
be the prediction and Y be the true outcome; ˆPA is a set of (possible) input features of the predictor
ˆY . We will denote by value x1 the disadvantaged group and x0 the advantaged group. Given the
space constraints  all proofs are included in the full technical report [27  Appendix A].
We consider ﬁrst the impact of the direct discrimination (i.e.  the direct path X → ˆY ) on the ER
disparity observed in the COMPAS. We will devise a thought experiment concerning with a Caucasian
defendant who does not recidivate (i.e.  x0  y). Imagine a hypothetical situation where this defendant
were a non-recidivating African-American (x1  y)  while keeping the prior convictions W and other
demographic information Z ﬁxed at the level that the defendant x0  y currently has. We then measure
the prediction ˆY in this imagined world (counterfactually)  compared to what the defendant currently
receives from COMPAS (factually). If the prediction were different in these two situations  e.g.  ˆY

3

Z

Z

ˆY

ˆY

W

x0

x0 x1

−
(a) P (ˆyx1 y Wx0 y Z|x0  y)
Figure 3: Graphical representation of the coun-
terfactual direct ER in COMPAS.

changes from 0 to 1  we could then say the path X → ˆY is active  i.e.  the direct discrimination
against African-American defendants exists.
Figs. 3(a-b) represent this thought experiment
graphically. Fig. 3(b) shows the conditional
SCM (cid:104)M  P (u|x0  y)(cid:105) of the non-recidivating
Caucasian defendant (x0  y): variables X  Z  W
are correlated by conditioning on the collider Y
[16  pp. 339]; we omit the true outcome Y for
simplicity. Using this model as the baseline (i.e. 
what factually happened in reality)  we change in Fig. 3(a) the input of X to the direct path X → ˆY to
x1 (edges in G represent functional relations)  while keeping the value of X to other variables (W  Z)
ﬁxed at the baseline level x0  y. In this reality  variable Zx0 y = Z since Z is a non-descendant
node of X and Y [16  pp. 232]; the intervention on Y is omitted since Y does not directly affect the
prediction ˆY . Since the direct path X → ˆY is the only difference between models of Figs. 3(a-b)  the
change in ˆY thus measure the inﬂuence of X → ˆY . Indeed  this hypothetical procedure could be
generalized  applicable to any classiﬁer in an arbitrary SCM  which we summarize as follows.
Deﬁnition 1 (Counterfactual Direct Error Rate). Given a SCM (cid:104)M  P (u)(cid:105) and a classiﬁer f ( ˆpa) 
the counterfactual direct error rate for a sub-population x  y (with prediction ˆy (cid:54)= y) is deﬁned as:
(1)

|x  y) − P (ˆyx0 y|x  y)

(ˆy|x  y) = P (ˆyx1 y ( ˆPA\X)x0  y

(b) P (ˆy|x0  y)

ERd

x0 x1

W

Z

could be further simpliﬁed as ˆYx1 ( ˆPA\X)x0 y

In Eq. 1  ˆYx1 y ( ˆPA\X)x0 y
since Y is not an input of
f ( ˆpa). The subscript ( ˆPA\X)x0 y is the solution of the input features (besides X) ( ˆPA\X)(u)
in the sub-model Mx0 y; values of U are drawn from the distribution P (u) such that X(u) =
x  Y (u) = y. The query of Eq. 1 could be read as: “For an individual with the protected attribute
X = x and the true outcome Y = y  how would the prediction ˆY change had X been x1  while
keeping all the other features ˆPA\X at the level that they would attain had X = x0 and Y = y 
compared to the prediction ˆY she/he would receive had X been x0 and Y been y?”
Similarly  we could devise a thought experiment
to measure the effect of the indirect discrimina-
tion  mediated by the prior convictions W   i.e.  the
indirect path X → W → ˆY . Consider again the
non-recidivating Caucasian defendant x0  y. We
conceive a scenario where the prior convictions
W of the defendant x0  y changes to the level that
it would have achieved had the defendant been a non-recidivating African-American x1  y  while
keeping the other features X  Z ﬁxed at the level that they currently are. Fig. 4(a) describes this
hypothetical scenario: we change only input value of edge X → W to x1  while keeping all the
other paths untouched (at the baseline). We then measure the prediction ˆY in both the counterfactual
(Fig. 4(a)) and factual (Fig. 4(b)) world and compare their differences. The change in the prediction of
these models thus represent the inﬂuence of indirect path X → W → ˆY . We generalize this thought
experiment and provide an estimand of the indirect paths for any SCM and classiﬁer f  namely:
Deﬁnition 2 (Counterfactual Indirect Error Rate). Given a SCM (cid:104)M  P (u)(cid:105) and a classiﬁer f ( ˆpa) 
the counterfactual indirect error rate for a sub-population x  y (with prediction ˆy (cid:54)= y) is deﬁned as:
(2)

−
(a) P (ˆyx0 y Wx1 y Z|x0  y)
Figure 4: Graphical representations of the coun-
terfactual indirect ER in COMPAS.

|x  y) − P (ˆyx0 y|x  y).

(ˆy|x  y) = P (ˆyx0 y ( ˆPA\X)x1 y

(b) P (ˆy|x0  y)

x0 x1

ERi

x0 x1

x0

W

W

ˆY

ˆY

Z

Z

Finally  we introduce a hypothetical procedure mea-
suring the inﬂuence of the spurious relations between
the protected attribute X and prediction ˆY through
the population attributes that are non-descendants
of both X and ˆY   e.g.  the path X ↔ Z → ˆY in
Fig. 2(a). We consider a Caucasian x0  y and an
African-American x1  y defendants who both would
not recidivate. We measure the prediction ˆY these defendants would receive had they both been

ˆY
ˆY
(a) P (ˆyx0 y|x1  y)
(b) P (ˆyx0 y|x0  y)
Figure 5: Graphical representations of the
counterfactual spurious ER in COMPAS.

x1 x0

−

x0

W

W

Z

4

non-recidivating Caucasians (x0  y). Figs. 5 (a-b) describes this experimental setup. Since the causal
inﬂuence of X (on ˆY ) are ﬁxed at x0 in both models  the difference in ˆY must be due to the population
characteristics that are not affected by X i.e.  the spurious X − ˆY relationships.
Deﬁnition 3 (Counterfactual Spurious Error Rate). Given a SCM (cid:104)M  P (u)(cid:105) and a classiﬁer f ( ˆpa) 
the counterfactual spurious error rate for a sub-population x  y (with prediction ˆy (cid:54)= y) is deﬁned as:
(3)

(ˆy|y) = P (ˆyx0 y|x1  y) − P (ˆyx0 y|x0  y)

ERs

x0 x1

Def. 3 generalizes the thought experiment described above to an arbitrary SCM. In the above
equation  the distribution P (ˆyx0 y|x0  y) coincides with P (ˆy|x0  y) since variable ˆYx0 y = ˆY given
that X = x0  Y = y (the composition axiom [16  Ch. 7.3]). Eq. 3 can be read as the counterfactual
sentence: “For two demographics x0  x1 with the same true outcome Y = y  how would the
prediction ˆY differ had they both been x0  y?”
3.1 Properties of Counterfactual Error Rates
Theorem 1. Given a SCM (cid:104)M  P (u)(cid:105) and a classiﬁer f ( ˆpa)  for any x0  x1  x  ˆy  y  the counter-
(ˆy|x  y) = 0;
factual ERs of Defs. 1-3 obey the following properties : (1) (X (cid:54)→ Y )G|Y ⇒ ERd
(2) |(X i−→ Y )G|Y | = 0 ⇒ ERi
(ˆy|x  y) = 0; (3) |(X s←→ Y )G|Y | = 0 ⇒ ERs
(ˆy|x  y) = 0 
where G|Y is the causal diagram of a conditional SCM (cid:104)My  P (u|y)(cid:105).
The conditional causal diagram G|Y is obtained from the original model G by (1) removing the
node Y and (2) adding bi-directed arrows between nodes whose associated exogenous variables are
correlated in P (u|y)1 (e.g.  Fig. 3(b)). Thm. 1 says that Defs. 1-3 provide prima facie evidence for
(ˆy|x  y) (cid:54)= 0 implies that the path X → ˆY is active 
discrimination detection. For instance  ERd
i.e.  the direct discrimination exists. It is expected that the proposed counterfactual measures capture
the relative strength of different active pathways connecting node X and ˆY in the underlying SCM.
We now derive how the counterfactual ERs are quantitatively related with the unequalized odds of
misclassiﬁcation induced by an arbitrary classiﬁer.
Theorem 2 (Causal Explanation Formula of Equalized Odds). For any x0  x1  ˆy  y  ERx0 x1(ˆy|x  y) 
(ˆy|y) obey the following non-parametric relationship:
ERd

(ˆy|x  y) and ERs

(ˆy|x  y)  ERi

x0 x1

x0 x1

x0 x1

x0 x1

x0 x1

x0 x1

x0 x1

ERx0 x1(ˆy|y) = ERd

x0 x1

(ˆy|x0  y) − ERi

x1 x0

(ˆy|x0  y) − ERs

x1 x0

(ˆy|y).

(4)

Thm. 2 guarantees that the disparate ER with the transition from x0 to x1 is equal to the sum of
the counterfactual direct ER with this transition  minus the indirect and spurious ER with reverse
transition  from x1 to x0  on the sub-population x0  y. Together with Thm. 1  each decomposing
term in Eq. 4 thus estimates the adverse impact of its corresponding discriminatory mechanism
(ˆy1|x0  y) explains how much the
on the total ER disparity. For instance  in COMPAS  ERd
direct racial discrimination accounts for the unequalized false positive rate ERx0 x1(ˆy1|y0) between
non-recidivating African American (x1  y) and Caucasian (x0  y) defendants. Perhaps surprisingly 
this result holds non-parametrically  which means that the counterfactual ERs decompose following
Thm. 2 for any functional form of the classiﬁer and the underlying causal models where the dataset
was generated. Owed to their generality and ubiquity  we refer to this equation as the “Causal
Explanation Formula” for the disparate ER in classiﬁcation tasks.

x0 x1

Connections with Other Counterfactual Measures Defs. 1-3 can be seen as a generalization of
the marginal counterfactual measures  including the counterfactual effects introduced in [28] and the
natural effects in [17  11  15]. Unable to consider the additional evidence (in classiﬁcation  the true
outcome Y = y)  the fairness analysis framework based on these marginal measures fails to provide a
ﬁne-grained quantitative explanation of the ER disparity (as in  Thm. 2). The counterfactual fairness
[10] is another counterfactual measure. As noted in [28]  however  it considers only the effects along
the causal paths from the protected attribute X and the outcome ˆY   thus unable to provide a full
account of the X − ˆY associations  including the spurious relations. We provide in Appendix B [27]
a more detailed discussion about the relationships between our measures and the existing ones.

1G|Y explicitly represents the change of information ﬂow due to conditioning on the true outcome Y : the
information via arrows pointing away from Y is intercepted; measuring the collider Y makes its (marginally
independent) common causes dependent  also known as the “explaining away” effect [16  pp. 339].

5

4 Estimating Counterfactual Error Rates
The Explanation Formula provides the precise relation between the counterfactual ERs  but it does
not specify how they should be estimated from data. When the underlying SCM is provided  the
counterfactual direct  indirect and spurious ERs (Defs. 1-3) are all well-deﬁned and computable via
the three-step algorithm of “predictions  interventions and counterfactuals” described in [16  Ch. 7.1].
However  the SCMs are not fully known in many applications  and one must estimate the proposed
counterfactual measures from the passively-collected (observational) data. Let a classiﬁer f ( ˆpa)
be denoted by f ( ˆw  ˆz)  where ˆZ ⊆ ˆPA are non-descendants of both X and Y and the subset of
features ˆW = ˆPA\ ˆZ. We ﬁrst characterize a set of classiﬁers where such estimation is still feasible.
Deﬁnition 4 (Explanation Criterion). Given a DAG G and a classiﬁer ˆy ← f ( ˆw  ˆz)  a set of
covariates C satisﬁes the explanation criterion relative to f (called the explaining set) if and only if
(1) ˆZ ⊆ C; (2) C ∩ Forb({X  Y }  ˆW\X) = ∅ where Forb({X  Y }  ˆW\X) is a set of descendants
Wi ∈ De(W )G for some W (cid:54)∈ {X  Y } on a proper causal path2 from {X  Y } to ˆW\X in G; and (3)
all spurious paths from {X  Y } to ˆW\X in G are blocked by C. A classiﬁer f is counterfactually
explainable (ctf-explainable) if and only if it has an explaining set C satisfying Conditions 1-3.

Consider again the COMPAS model of Fig. 1. The classiﬁer f (x  w  z) has input features ˆW =
{X  W} and ˆZ = {Z}. The set C = {Z} does not satisfy the explanation criterion relative to f
since it does not block the spurious path Y ← W . Indeed  one could show that there exists no set
C satisfying Def. 4 relative to f  i.e.  f (x  w  z) is not ctf-explainable. However  if we remove the
prior convictions W from the feature set  the new classiﬁer f (x  z) is ctf-explainable with C = {Z}:
ˆZ = C = {Z} satisﬁes Condition 1; Conditions 2-3 follow immediately since ˆW\X = ∅.
Defs. 4 constitutes a sufﬁcient condition upon which the counterfactual ERs could  at least in principle 
be estimated from the observational data. This yields identiﬁcation formulas as shown next:
Theorem 3. Given a causal diagram G and a classiﬁer f ( ˆw  ˆz)  if f is ctf-explainable (Def. 4) with
(ˆy|y) can be estimated as follows:
an explaining set C  ERd
(ˆy|x  y) =
(5)

(ˆy|x  y)  ERi
(P (ˆyx1  ˆw\x  ˆz) − P (ˆyx0  ˆw\x  ˆz))P ( ˆw\x|x0  c  y)P (c|x  y) 

(cid:88)
(cid:88)
(cid:88)
P (ˆyx1  ˆw\x  ˆz)P ( ˆw\x|x1  c  y)(P (c|x1  y) − P (c|x0  y)).

P (ˆyx1  ˆw\x  ˆz)(P ( ˆw\x|x1  c  y) − P ( ˆw\x|x0  c  y))P (c|x  y) 

(ˆy|x  y) and ERs

ERs

x0 x1

(ˆy|y) =

(ˆy|x  y) =

(6)

(7)

ERi

x0 x1

ERd

x0 x1

x0 x1

x0 x1

x0 x1

ˆw c

ˆw c

ˆw c

where P (ˆy ˆw  ˆz) is well-deﬁned  computable from the classiﬁer f ( ˆw  ˆz)3.
In Eqs. 5-7  the conditional distributions P (c|x  y) and P ( ˆw\x|x0  c  y) do not involve any counter-
factual variable  which means that they are readily estimable by any method from the observational
data (e.g.  through deep nets). Continuing from the COMPAS example  we could thus estimate the
counterfactual ERs of f (x  z) from the distribution P (x  y  z  w) using Thm. 3 with C = {Z}.

Inverse Propensity Weighting Estimators Eqs. 5-7 involve summing over all possible values of
ˆW   C  which may present computational and sample complexity challenges as the cardinalities
of ˆW   C grow very rapidly. There exist robust statistical estimation techniques  known as the
inverse propensity weighting (IPW) [12  18]  to circumvent such issues. Given the observed data
D = {Yi  ˆWi  Ci}n
(ˆy|x  y) =

(ˆy|x  y) as follows:
ˆP (x|Ci  y)I{Xi=x0 Yi=y}

i=1  we propose the IPW estimator for ERd

n(cid:88)

) − P (ˆyx0  ˆWi\Xi  ˆZi

(P (ˆyx1  ˆWi\Xi  ˆZi

ˆERd

x0 x1

x0 x1

(8)

))

 

ˆP (x0|Ci  y) ˆP (x  y)

1
n

i=1

where I{·} is an indicator function and ˆP (x  y) is the sample mean estimator of P (x  y) (X  Y are
ﬁnite). ˆP (x|c  y) is a reliable estimator of the conditional distributions P (x|c  y) and  in practice 
could be estimated by assuming some parametric models such as logistic regression.

2A causal path from {X  Y } to ˆW \X is proper if it does not intersect {X  Y } except at the end point [20].
3For a deterministic f ( ˆw  ˆz)  the probabilities P (ˆy ˆw  ˆz) = I{ˆy=f ( ˆw  ˆz)} where I{·} is an indicator function.

6

Algorithm 1: FindExpSet

Input: Feature set { ˆW   ˆZ}  DAG G = (cid:104)V   E(cid:105)
Output: Explaining set C (Def. 4) relative to
f ( ˆw  ˆz) in G  or ⊥ if f is not ctf-explainable.
1: Apply FindSep [22] to ﬁnd a set C with
ˆZ ⊆ C ⊆ V \Forb({X  Y }  ˆW \X) such that it
d-separates {X  Y } and ˆW \X in Gpbd
.
{X Y }  ˆW \X

2: return C

Algorithm 2: Causal-SFFS

Input: Samples D = {Yi  Vi}n
diagram G
Output: A family of ctf-explainable classiﬁers F
Initialization:

ˆPA0 = ∅  k = 0.

i=1  a causal

1: while k < |V | do
2:

Let subset ˆVk be deﬁned as

{vi ∈ V \ ˆPAk : FindExpSet( ˆPAk ∪ vi  G) (cid:54)=⊥}.

Algorithm 3: Ctf-FairLearning

Input: Samples D  DAG G  d  i  s > 0
Output: A fair classiﬁer f
1: Let F = C-SFFS(D  G).
2: Obtain a fair classiﬁer f from F by solving Eq. 9

subject to |ERd| ≤ d  |ERi| ≤ i  |ERs| ≤ s.

3:
4:
5:

Let vk+1 = arg maxvi∈ ˆVk
Let ˆPAk+1 = ˆPAk ∪ vk+1; k = k + 1.
Continue with the conditional exclusion of [19 

J( ˆPAk ∪ {vi}).

Step 2-3] and update the counter k.
6: end while
7: return F = {∀f : ˆPAk → ˆY }.

Theorem 4. For a ctf-explainable classiﬁer f ( ˆw  ˆz)  ˆERd
for ERd

(ˆy|x  y) (Eq. 5) if the model for P (x|c  y) is correctly speciﬁed.

x0 x1

x0 x1

(ˆy|x  y) (Eq. 8) is a consistent estimator

{X Y }  ˆW \X

We provide IPW estimators for counterfactual indirect and spurious ERs in Appendix A [27].
4.1 Finding Adjustment Set for Explainable Classiﬁers
A few natural questions arise here is (1) how to systematically test whether a classiﬁer f is ctf-
explainable  and (2) if so  to ﬁnd a set C satisfying the explanation criterion so that the counterfactual
ERs could be identiﬁed. In this section  we will develop an efﬁcient method to answer these questions.
Given a DAG G  by Gpbd
we denote the proper backdoor graph obtained from G by removing
the ﬁrst edge of every proper causal path from {X  Y } to ˆW\X [22]. We formulate next in graphical
terms a set of identiﬁcation conditions equivalent to the explanation criterion deﬁned in Def. 4.
Deﬁnition 5 (Constructive Explanation Criterion). Given a DAG G and a classiﬁer f ( ˆw  ˆz)  co-
variates C satisfy the constructive explanation criterion relative to f if and only if (1) ˆZ ⊆ C ⊆
V \Forb({X  Y }  ˆW\X)  where Forb({X  Y }  ˆW\X) is a set of nodes forbidden by Def. 4; (2) C
d-separates {X  Y } and ˆW\X in the proper backdoor graph Gpbd
Theorem 5. Given a causal diagram G and a classiﬁer f  covariates C satisﬁes the explanation
criterion (Def. 4) to f if and only if it satisﬁes the constructive explanation criterion (Def. 5) to f.

{X Y }  ˆW \X

.

Thm. 5 allows us to use the algorithmic framework developed by [22] for constructing d-separating
sets in DAGs. We summarize this procedure as FindExpSet  in Alg. 1. Speciﬁcally  the sub-routine
FindSep ﬁnd a covariates set C with ˆZ ⊆ C ⊆ V \Forb({X  Y }  ˆW\X)  such that C d-separates
all paths between {X  Y } and ˆW\X in Gpbd
  i.e.  the explaining set relative to classiﬁer
f ( ˆw  ˆz) (Def. 4). This algorithm can be solved in O(n + m) runtime where n is the number of nodes
and m is the number of edges in the proper backdoor graph Gpbd

{X Y }  ˆW \X

.

{X Y }  ˆW \X

5 Achieving Equalized Counterfactual Error Rates
So far we have focused on analyzing the unequalized counterfactual ERs of an existing predictor
in the environment. A more interesting problem is how to obtain an optimal classiﬁer such that its
induced counterfactual ERs along with a speciﬁc discriminatory mechanism are equalized.
Given ﬁnite samples D = {Yi  Vi}n
i=1 drawn from P (y  v) (where the protected attribute X ∈ V ) 
the associated causal diagram G  and a set of candidate ctf-explainable classiﬁers F  the goal of
the supervised learning is to obtain an optimal classiﬁer f∗( ˆpa) from F such that a loss function
L(D  f ) measuring the distance between the prediction ˆY and the true outcome Y is minimized. We
will elaborate later about how to construct the ctf-explainable set F. Among the quantities evolved
by Thm. 3  the counterfactual distribution P (ˆyx  ˆw\x  ˆz) is deﬁned from the classiﬁer f and the other
conditional distributions (e.g.  P (c|x  y)) are estimable from the data D. We could thus represent
a counterfactual ER (e.g.  direct) of a classiﬁer f ∈ F as a function g(D  f ) (e.g.  Eq. 8). A fair

7

classiﬁer is obtained by minimizing L(D  f ) subject to a box constraint over g(D  f )  namely 

f∈F L(D  f ) s.t. |g(D  f )| ≤  
min

(9)

(cid:124)

(cid:124)

x0 x1

x0 x1

where  ∈ R+ and the smaller  is  the fairer the learned classiﬁer would be. In general  the constraints
|g(D  f )| ≤  are non-convex and solving the problem of Eq. 9 seems to be difﬁcult. However  this
optimization problem could be signiﬁcantly simpler in certain cases  solvable using standard convex
optimization methods [3]. We provide two canonical settings that ﬁt this requirement.
First  we assume that the features V are discrete  and let θˆy x  ˆw\x  ˆz denote the probabilities
P (ˆyx  ˆw\x  ˆz). The counterfactual constraints |g(D  f )| ≤  are thus reducible to a set of linear
inequalities on the parameter space {θ}. Second  consider a classiﬁer making decision based on a
φ(x  ˆw\x  ˆz) (e.g.  logistic regression)  where φ(·) is the basis function.
decision boundary ˜Y = θ
(˜y|x  y) = 0
The boundary ˜Y acts as a proxy to the prediction ˆY . For instance  the condition ERd
(ˆy|x  y) = 0. The same reasoning applies to the counterfactual indirect and spurious
implies ERd
ERs. We will employ the techniques in [25] and approximate the constraints |g(D  f )| ≤  using the
counterfactual ERs of X on the boundary ˜Y . Assume that we are interested in the mean effect and
φ(x  ˆw\x  ˆz). Given the convexity of L(D  f ) 
replace the quantities P (ˆyx  ˆw\x  ˆz) in Thm. 3 with θ
Eq. 9 is a convex optimization problem and can thus be efﬁciently solved using standard methods.
5.1 Constructing Counterfactually Explainable Classiﬁers
The counterfactual explainability (Def. 4) of a classiﬁer f relies on its input feature ˆPA: the smaller
the set ˆPA is  the easier it would be to ﬁnd a explaining set C relative to f ( ˆpa). In practice  some
features contain critical information about the prediction task  which means that their exclusion
could lead to poorer performance. This observation suggests a novel feature selection problem in
the fairness-aware classiﬁcation task: we would like to ﬁnd a subset ˆPA from the available features
V such that each classiﬁer in the candidate set F = {∀f : ˆPA → ˆY } is ctf-explainable  without
signiﬁcant loss of prediction accuracy.
Our solution builds on the procedure FindExpSet (Alg. 1) and the classic method of Sequential
Floating Forward Selection (SFFS) [19]. Let ˆPAk be the set of k features. The score function
J( ˆpa k) evaluates the candidate subset ˆPAk and returns a measure of its “goodness”. In practice 
this score could be obtained by computing the statistical measures of dependence  or by evaluating
the best in-class predictive accuracy for classiﬁers in {∀f : ˆPAk → ˆY } on the validation data. We
denote our method by Causal SFFS (C-SFFS) and summarize it in Alg. 2. Starting with a subset
ˆPAk  C-SFFS (Step 2-3) adds one feature which gives the highest score J. FindExpSet ensures that
the resulting subset ˆPAk+1 induces a ctf-explainable classiﬁer f ( ˆpa k+1). Step 5 repeatedly removes
the least signiﬁcant feature vd from the newly-formed ˆPAk until no feature could be excluded to
improve the score J. During the exclusion phase  we do not apply FindExpSet  since removing
features from a ctf-explainable classiﬁer does not violate the explanation criterion (Def. 4). It follows
immediately from the soundness of FindExpSet that C-SFFS always returns a ctf-explainable set F.
Theorem 6. For F = C-SFFS(D  G)  each classiﬁer f ∈ F is ctf-explainable.

We summarize in Alg. 3 the procedure of training an optimal classiﬁer satisfying the fairness
constraints over the counterfactual ERs. ERd  ERi  and ERs stand for the counterfactual quantities
(ˆy|y)  respectively. We use C-SFFS (Alg. 2) to
ERd
obtain a candidate set F such that each f ∈ F is ctf-explainable. The fair classiﬁer is computed by
solving the optimization problem in Eq. 9 subject to the box constraints over ERd  ERi  and ERs.

(ˆy|x0  y)  and ERs

(ˆy|x0  y)  ERi

x0 x1

x1 x0

x1 x0

6 Simulations and Experiments
In this section  we will illustrate our approach on both synthetic and real datasets. We focus on the false
positive rate ERx0 x1(ˆy1|y0) across demographics x0 = 0  x1 = 1  where ˆy1 = 1  y0 = 0  and the
(ˆy1|y0) (following
corresponding components ERd
Thm. 2). We shorten the notation and write ERx0 x1 (ˆy1|y0) = ER  and similarly to ERd  ERi and
ERs. Details of the experiments are provided in Appendix C [27].

(ˆy1|x0  y0) and ERs

(ˆy1|x0  y0)  ERi

x0 x1

x1 x0

x1 x0

8

(a) Standard Prediction Model

(b) COMPAS

X

Z

W

Y

D

ctf = 0.191  ERd

ctf = −0.194  ERd

Figure 6: Standard fair-
ness prediction model

eo = 0.620) than in the unconstrained fopt (ERd

Figure 7: Results of Experiments 1-2. Measures that are not estimable via the explanation criterion
are shaded and highlighted. ER stands for the false positive rate ERx0 x1 (ˆy1|y0); ERd  ERi and ERs
represent the corresponding counterfactual direct  indirect  and spurious ERs (Thm. 2). Classiﬁer
fopt  fer  and fctf in Exp. 1 correspond to  respectively  color blue  orange  and yellow in Fig. (a); fopt 
fer  fopt-  fer-  and fctf- in Exp. 2 correspond to blue  orange  yellow  purple  and green in Fig. (b).
Experiment 1: Standard Prediction Model We consider a general-
ized COMPAS model containing the common descendant D  shown in
Fig. 6  which we call here the standard fairness prediction model (for
short  standard prediction model). We train two classiﬁers with the same
feature set {X  W  Z  D} where the ﬁrst is obtained via the standard 
unconstrained optimization  which we call fopt  and the second one con-
strains the disparate ER to half of that of fopt  which we call fer. We
further compute the counterfactual ERs (Defs. 1-3). The results are shown in Fig. 7(a). We ﬁrst
conﬁrm that the procedure fer is sound in the sense that feo (90.4%) achieves a comparable predictive
accuracy to fopt (90.4%) while reducing the disparate ER in half (ERer = −0.238  ERopt = −0.476).
Second  ERd is larger in fer (ERd
opt = 0.381). This ma-
terializes the concern acknowledged in [7]  namely  that optimizing based on ER may not be enforcing
any type of real-life fairness notion related to the underlying causal mechanism. To circumvent this
issue  we train a classiﬁer with the same feature set such that its counterfactual ERs are reduced to
half of that of the unconstrained fopt  called fctf . The results (Fig. 7(a)) support the counterfactual
approach: fctf (90.1%) reports ER comparable to fer (ERctf = −0.238)  but a smaller signiﬁcant
ctf = −0.236).
direct  indirect and spurious ER disparities (ERd
Experiment 2: COMPAS In the COMPAS model of Fig. 1  we are interested in predicting whether
a defendant would recidivate  while avoiding the direct discrimination (the threshold  = 0.01). We
compute a classiﬁer fer with a feature set {X  Z  W} subject to |ERer| ≤ . We also include
an unconstrained classiﬁer fopt as the baseline. The results (Fig. 7(b)) reveal that fer (73.7%)
and fopt (74.6%) are comparable in prediction accuracy while fer has much smaller disparate ER
(ERer = −0.005  ERopt = −0.077). Given that the underlying causal model is not fully known  we
could only estimate the counterfactual direct ER from passively-collected samples. Since classiﬁers
with feature set {X  W  Z} are not ctf-explainable in the COMPAS model (Def.4)  ERd of fer and
fopt cannot be identiﬁed via Thm. 3. Previous analysis (Experiment 1) implies that ERd could be
signiﬁcant even when ER is small  which suggests one should be wary of the direct discrimination
of fer and fopt. To overcome this issue  we remove W from the feature set and obtain fopt- and
fer- following a similar procedure. We estimate their ERd via Thm. 3 with covariates C = {Z}.
The results show that the direct discrimination are signiﬁcant in both fer- and fopt- (ERd
eo− = 0.015 
opt− = −0.066). To remove the direct discrimination  we train a classiﬁer fctf- following Alg. 3
ERd
with the features {X  Z} and d = . The results support the efﬁcacy of Alg. 3: fctf- performs slightly
ctf− = −0.001).
worse in prediction accuracy (72.1%) but ascertains no direct discrimination (ERd
7 Conclusions
We introduced a new family of counterfactual measures capable of explaining disparities in the
misclassiﬁcation rates (false positive and false negative) across different demographics in terms of
the causal mechanisms underlying the speciﬁc prediction process. We then developed machinery
based on these measures to allow data scientists (1) to diagnose whether a classiﬁer is operating in a
discriminatory fashion against speciﬁc groups  and (2) to learn a new classiﬁer subject to fairness
constraints in terms of ﬁne-grained misclassiﬁcation rates. In practice  this approach constitutes a
formal solution to the notorious lack of interpretability of the equalized odds. We hope the causal
machinery put forwarded here will help data scientists to analyze already deployed systems as well
as to construct new classiﬁers that are fair even when the training data comes from an unfair world.

9

Acknowledgments

This research is supported in parts by grants from IBM Research  Adobe Research  NSF IIS-1704352 
and IIS-1750807 (CAREER).

References
[1] J. Angwin  J. Larson  S. Mattu  and L. Kirchner. Machine bias: There’s software used across

the country to predict future criminals. and it’s biased against blacks. ProPublica  23  2016.

[2] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the

National Academy of Sciences  113:7345–7352  2016.

[3] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[4] T. Brennan  W. Dieterich  and B. Ehret. Evaluating the predictive validity of the compas risk

and needs assessment system. Criminal Justice and Behavior  36(1):21–40  2009.

[5] A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction

instruments. Big data  5(2):153–163  2017.

[6] G. Goh  A. Cotter  M. Gupta  and M. P. Friedlander. Satisfying real-world goals with dataset
constraints. In Advances in Neural Information Processing Systems  pages 2415–2423  2016.

[7] M. Hardt  E. Price  N. Srebro  et al. Equality of opportunity in supervised learning. In Advances

in Neural Information Processing Systems  pages 3315–3323  2016.

[8] A. E. Khandani  A. J. Kim  and A. W. Lo. Consumer credit-risk models via machine-learning

algorithms. Journal of Banking & Finance  34(11):2767–2787  2010.

[9] N. Kilbertus  M. R. Carulla  G. Parascandolo  M. Hardt  D. Janzing  and B. Schölkopf. Avoiding
discrimination through causal reasoning. In Advances in Neural Information Processing Systems 
pages 656–666  2017.

[10] M. J. Kusner  J. Loftus  C. Russell  and R. Silva. Counterfactual fairness. In Advances in Neural

Information Processing Systems  pages 4069–4079  2017.

[11] X. W. Lu Zhang  Yongkai Wu. A causal framework for discovering and removing direct and
indirect discrimination. In Proceedings of the Twenty-Sixth International Joint Conference on
Artiﬁcial Intelligence  IJCAI-17  pages 3929–3935  2017.

[12] J. K. Lunceford and M. Davidian. Stratiﬁcation and weighting via the propensity score in
estimation of causal treatment effects: a comparative study. Statistics in medicine  23(19):2937–
2960  2004.

[13] J. F. Mahoney and J. M. Mohen. Method and system for loan origination and underwriting 

Oct. 23 2007. US Patent 7 287 008.

[14] K. Mancuhan and C. Clifton. Combating discrimination using bayesian networks. Artiﬁcial

Intelligence and Law  22(2):211–238  Jun 2014.

[15] R. Nabi and I. Shpitser. Fair inference on outcomes.

Conference on Artiﬁcial Intelligence  2018.

In Proceedings of the 32nd AAAI

[16] J. Pearl. Causality: Models  Reasoning  and Inference. Cambridge University Press  New York 

2000. 2nd edition  2009.

[17] J. Pearl. Direct and indirect effects. In Proc. of the Seventeenth Conference on Uncertainty in

Artiﬁcial Intelligence  pages 411–420. Morgan Kaufmann  CA  2001.

[18] J. Pearl  M. Glymour  and N. P. Jewell. Causal inference in statistics: a primer. John Wiley &

Sons  2016.

[19] P. Pudil  J. Novoviˇcová  and J. Kittler. Floating search methods in feature selection. Pattern

recognition letters  15(11):1119–1125  1994.

10

[20] I. Shpitser  T. VanderWeele  and J. Robins. On the validity of covariate adjustment for estimating
causal effects. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artiﬁcial
Intelligence  pages 527–536. AUAI  Corvallis  OR  2010.

[21] L. Sweeney. Discrimination in online ad delivery. Queue  11(3):10  2013.

[22] B. van der Zander  M. Li´skiewicz  and J. Textor. Constructing separators and adjustment sets in
ancestral graphs. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence.
AUAI  2014.

[23] B. Woodworth  S. Gunasekar  M. I. Ohannessian  and N. Srebro. Learning non-discriminatory

predictors. In Conference on Learning Theory  pages 1920–1953  2017.

[24] S. Wright. The method of path coefﬁcients. The annals of mathematical statistics  5(3):161–215 

1934.

[25] M. B. Zafar  I. Valera  M. Gomez Rodriguez  and K. P. Gummadi. Fairness beyond disparate
treatment & disparate impact: Learning classiﬁcation without disparate mistreatment.
In
Proceedings of the 26th International Conference on World Wide Web  pages 1171–1180.
International World Wide Web Conferences Steering Committee  2017.

[26] M. B. Zafar  I. Valera  M. G. Rogriguez  and K. P. Gummadi. Fairness constraints: Mechanisms

for fair classiﬁcation. In Artiﬁcial Intelligence and Statistics  pages 962–970  2017.

[27] J. Zhang and E. Bareinboim. Equality of opportunity in classiﬁcation: A causal approach.

Technical Report R-37  AI Lab  Purdue University.  2018.

[28] J. Zhang and E. Bareinboim. Fairness in decision-making — the causal explanation formula. In

Proceedings of AAAI Conference on Artiﬁcial Intelligence  pages 2037–2045  2018.

[29] J. Zhang and E. Bareinboim. Non-parametric path analysis in structural causal models. In

Proceedings of the 34th Conference on Uncertainty in Artiﬁcial Intelligence  2018.

11

,Junzhe Zhang
Elias Bareinboim