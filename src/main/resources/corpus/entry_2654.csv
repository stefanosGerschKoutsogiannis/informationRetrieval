2017,Information Theoretic Properties of Markov Random Fields  and their Algorithmic Applications,Markov random fields are a popular model for high-dimensional probability distributions. Over the years  many mathematical  statistical and algorithmic problems on them have been studied. Until recently  the only known algorithms for provably learning them relied on exhaustive search  correlation decay or various incoherence assumptions. Bresler gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models.   Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models  to arbitrary Markov random fields with higher order interactions. As an application  we obtain algorithms for learning Markov random fields on bounded degree graphs on $n$ nodes with $r$-order interactions in $n^r$ time and $\log n$ sample complexity. Our algorithms also extend to various partial observation models.,Information Theoretic Properties of Markov Random

Fields  and their Algorithmic Applications

Linus Hamilton∗

Frederic Koehler †

Ankur Moitra ‡

Abstract

Markov random ﬁelds are a popular model for high-dimensional probability distri-
butions. Over the years  many mathematical  statistical and algorithmic problems
on them have been studied. Until recently  the only known algorithms for provably
learning them relied on exhaustive search  correlation decay or various incoher-
ence assumptions. Bresler [4] gave an algorithm for learning general Ising models
on bounded degree graphs. His approach was based on a structural result about
mutual information in Ising models.
Here we take a more conceptual approach to proving lower bounds on the mutual
information. Our proof generalizes well beyond Ising models  to arbitrary Markov
random ﬁelds with higher order interactions. As an application  we obtain algo-
rithms for learning Markov random ﬁelds on bounded degree graphs on n nodes
with r-order interactions in nr time and log n sample complexity. Our algorithms
also extend to various partial observation models.

1

Introduction

1.1 Background

Markov random ﬁelds are a popular model for deﬁning high-dimensional distributions by using a
graph to encode conditional dependencies among a collection of random variables. More precisely 
the distribution is described by an undirected graph G = (V  E) where to each of the n nodes u ∈ V
we associate a random variable Xu which takes on one of ku different states. The crucial property
is that the conditional distribution of Xu should only depend on the states of u’s neighbors. It turns
out that as long as every conﬁguration has positive probability  the distribution can be written as

Pr(a1 ··· an) = exp

θi1···i(cid:96)(a1 ··· an) − C

(1)

(cid:32) r(cid:88)

(cid:88)

(cid:96)=1

i1<i2<···<i(cid:96)

(cid:33)

Here θi1···i(cid:96) : [ki1] × . . . × [ki(cid:96) ] → R is a function that takes as input the conﬁguration of states on
the nodes i1  i2 ··· i(cid:96) and is assumed to be zero on non-cliques. These functions are referred to as
clique potentials. In the equation above  C is a constant that ensures the distribution is normalized
and is called the log-partition function. Such distributions are also called Gibbs measures and arise
frequently in statistical physics and have numerous applications in computer vision  computational
biology  social networks and signal processing. The Ising model corresponds to the special case
∗Massachusetts Institute of Technology. Department of Mathematics. Email: luh@mit.edu. This work
†Massachusetts Institute of Technology. Department of Mathematics. Email: fkoehler@mit.edu.
‡Massachusetts Institute of Technology. Department of Mathematics and the Computer Science and Arti-
ﬁcial Intelligence Lab. Email: moitra@mit.edu. This work was supported in part by NSF CAREER Award
CCF-1453261  NSF Large CCF-1565235  a David and Lucile Packard Fellowship and an Alfred P. Sloan Fel-
lowship.

was supported in part by Hertz Fellowship.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

where every node has two possible states and the only non-zero clique potentials correspond to
single nodes or to pairs of nodes.
Over the years  many sorts of mathematical  statistical and algorithmic problems have been stud-
ied on Markov random ﬁelds. Such models ﬁrst arose in the context of statistical physics where
they were used to model systems of interacting particles and predict temperatures at which phase
transitions occur [6]. A rich body of work in mathematical physics aims to rigorously understand
such phenomena. It is also natural to seek algorithms for sampling from the Gibbs distribution when
given its clique potentials. There is a natural Markov chain to do so  and a number of works have
identiﬁed a critical temperature (in our model this is a part of the clique potentials) above which the
Markov chain mixes rapidly and below which it mixes slowly [14  15]. Remarkably in some cases
these critical temperatures also demarcate where approximate sampling goes from being easy to
being computationally hard [19  20]. Finally  various inference problems on Markov random ﬁelds
lead to graph partitioning problems such as the metric labelling problem [12].
In this paper  we will be primarily concerned with the structure learning problem. Given samples
from a Markov random ﬁeld  our goal is to learn the underlying graph G with high probability.
The problem of structure learning was initiated by Chow and Liu [7] who gave an algorithm for
learning Markov random ﬁelds whose underlying graph is a tree by computing the maximum-weight
spanning tree where the weight of each edge is equal to the mutual information of the variables at its
endpoints. The running time and sample complexity are on the order of n2 and log n respectively.
Since then  a number of works have sought algorithms for more general families of Markov random
ﬁelds. There have been generalizations to polytrees [10]  hypertrees [21] and tree mixtures [2].
Others works construct the neighborhood by exhaustive search [1  8  5]  impose certain incoherence
conditions [13  17  11] or require that there are no long range correlations (e.g. between nodes at
large distance in the underlying graph) [3  5].
In a breakthrough work  Bresler [4] gave a simple greedy algorithm that provably works for any
bounded degree Ising model  even if it has long-range correlations. This work used mutual informa-
tion as its underlying progress measure and for each node it constructed its neighborhood. For a set
S of nodes  let XS denote the random variable representing their joint state. Then the key fact is the
following:
Fact 1.1. For any node u  for any S ⊆ V \ {u} that does not contain all of u’s neighbors  there is a
node v (cid:54)= u which has non-negligible conditional mutual information (conditioned on XS) with u.
This fact is simultaneously surprising and not surprising. When S contains all the neighbors of u 
then Xu has zero conditional mutual information (again conditioned on XS) with any other node
because Xu only depends on XS. Conversely shouldn’t we expect that if S does not contain the en-
tire neighborhood of u  that there is some neighbor that has nonzero conditional mutual information
with u? The difﬁculty is that the inﬂuence of a neighbor on u can be cancelled out indirectly by the
other neighbors of u. The key fact above tells us that it is impossible for the inﬂuences to all cancel
out. But is this fact only true for Ising models or is it an instance of a more general phenomenon
that holds over any Markov random ﬁeld?

1.2 Our Techniques

In this work  we give a vast generalization of Bresler’s [4] lower bound on the conditional mutual
information. We prove that it holds in general Markov random ﬁelds with higher order interactions
provided that we look at sets of nodes. More precisely we prove  in a Markov random ﬁeld with
non-binary states and order up to r interactions  the following fundamental fact:
Fact 1.2. For any node u  for any S ⊆ V \ {u} that does not contain all of u’s neighbors  there
is a set I of at most r − 1 nodes which does not contain u where Xu and XI have non-negligible
conditional mutual information (conditioned on XS).

Our approach goes through a two-player game that we call the GUESSINGGAME between Alice and
Bob. Alice samples a conﬁguration X1  X2  . . . Xn and reveals I and XI for a randomly chosen
set of u’s neighbors with |I| ≤ r − 1. Bob’s goal is to guess Xu with non-trivial advantage over
its marginal distribution. We give an explicit strategy for Bob that achieves positive expected value.
Our approach is quite general because we base Bob’s guess on the contribution of XI to the overall
clique potentials that Xu participates in  in a way that the expectation over I yields an unbiased

2

estimator of the total clique potential. The fact that the strategy has positive expected value is then
immediate  and all that remains is to prove a quantitative lower bound on it using the law of total
variance. From here  the intuition is that if the mutual information I(Xu; XI ) were zero for all sets
I then Bob could not have positive expected value in the GUESSINGGAME.

1.3 Our Results

Let Γ(u) denote the neighbors of u. We require certain conditions (Deﬁnition 2.3) on the clique
potentials to hold  which we call α  β-non-degeneracy  to ensure that the presence or absence of
each hyperedge can be information-theoretically determined from few samples (essentially that no
clique potential is too large and no non-zero clique potential is too small). Under this condition  we
prove:
Theorem 1.3. Fix any node u in an α  β-non-degenerate Markov random ﬁeld of bounded degree
and a subset of the vertices S which does not contain the entire neighborhood of u. Then taking I
uniformly at random from the subsets of the neighbors of u not contained in S of size s = min(r −
1 |Γ(u) \ S|)  we have EI [I(Xu; XI|XS)] ≥ C.

See Theorem 4.3 which gives the precise dependence of C on all of the constants  including α  β 
the maximum degree D  the order of the interactions r and the upper bound K on the number of
states of each node. We remark that C is exponentially small in D  r and β and there are examples
where this dependence is necessary [18].
Next we apply our structural result within Bresler’s [4] greedy framework for structure learning to
obtain our main algorithmic result. We obtain an algorithm for learning Markov random ﬁelds on
bounded degree graphs with a logarithmic number of samples  which is information-theoretically
optimal [18]. More precisely we prove:
Theorem 1.4. Fix any α  β-non-degenerate Markov random ﬁeld on n nodes with r-order interac-
tions and bounded degree. There is an algorithm for learning G that succeeds with high probability
given C(cid:48) log n samples and runs in time polynomial in nr.
Remark 1.5. It is easy to encode an r − 1-sparse parity with noise as a Markov random ﬁeld with
order r interactions. This means if we could improve the running time to no(r) this would yield
the ﬁrst no(k) algorithm for learning k-sparse parities with noise  which is a long-standing open
question. The best known algorithm of Valiant [22] runs in time n0.8k.
See Theorem 5.1 for a more precise statement. The constant C(cid:48) depends doubly exponentially on
D. In the special case of Ising models with no external ﬁeld  Vuffray et al. [23] gave an algorithm
based on convex programming that reduces the dependence on D to singly exponential. In greedy
approaches based on mutual information like the one we consider here  doubly-exponential depen-
dence on D seems intrinsic. As in Bresler’s [4] work  we construct a superset of the neighborhood
that contains roughly 1/C nodes where C comes from Theorem 1.3. Recall that C is exponentially
small in D. Then to accurately estimate conditional mutual information when conditioning on the
states of this many nodes  we need doubly exponential in D many samples.
Our results extend to a model where we are only allowed partial observations. More precisely  for
each sample we are allowed to specify a set J of size at most C(cid:48)(cid:48) and all we observe is XJ. We
prove:
Theorem 1.6. Fix any α  β-non-degenerate Markov random ﬁeld on n nodes with r-order inter-
actions and bounded degree. There is an algorithm for learning G with C(cid:48)(cid:48)-bounded queries that
succeeds with high probability given C(cid:48) log n samples and runs in time polynomial in nr.

See Theorem 5.3 for a more precise statement. This is a natural scenario that arises when it is
too expensive to obtain a sample where the states of all nodes are known. We also consider a model
where each node’s state is erased (and unobserved) independently with some ﬁxed probability p. See
the supplementary material for a precise statement. The fact that we can straightforwardly obtain
algorithms for these alternative settings demonstrates the ﬂexibility of greedy  information-theoretic
approaches to learning.

3

2 Preliminaries

For reference  all fundamental parameters of the graphical model (max degree  etc.) are deﬁned in
the next two subsections. In terms of these fundamental parameters  we deﬁne additional parameters
γ and δ in (3)  C(cid:48)(γ  K  α) in Theorem 4.3  and τ in (5) and L in (6).

2.1 Markov Random Fields and the Canonical Form

(cid:96)(a(cid:48)

1  . . .   a(cid:48)

(cid:96)) where the i(cid:48)

1···i(cid:48)

1  . . .   a(cid:48)

(cid:96) are the corresponding copies of a1  . . .   a(cid:96).

Let K be an upper bound on the maximum number of states of any node. Recall the joint probability
distribution of the model  given in (1). For notational convenience  even when i1  . . .   i(cid:96) are not
sorted in increasing order  we deﬁne θi1···i(cid:96)(a1  . . .   a(cid:96)) = θi(cid:48)
1  . . .   i(cid:48)
are the sorted version of i1  . . .   i(cid:96) and the a(cid:48)
The parameterization in (1) is not unique. It will be helpful to put it in a normal form as below. A
tensor ﬁber is the vector given by ﬁxing all of the indices of the tensor except for one; this generalizes
the notion of row/column in matrices. For example for any 1 ≤ m ≤ (cid:96)  i1 < . . . < im <
. . . i(cid:96) and a1  . . .   am−1  am+1  . . . a(cid:96) ﬁxed  the corresponding tensor ﬁber is the set of elements
θi1···i(cid:96)(a1  . . .   am  . . .   a(cid:96)) where am ranges from 1 to kim.
Deﬁnition 2.1. We say that the weights θ are in canonical form4 if for every tensor θi1···i(cid:96)  the sum
over all of the tensor ﬁbers of θi1···i(cid:96) is zero.
Moreover we say that a tensor with the property that the sum over all tensor ﬁbers is zero is a
centered tensor. Hence having a Markov random ﬁeld in canonical form just means that all of the
tensors corresponding to its clique potentials are centered. We observe that every Markov random
ﬁeld can be put in canonical form:
Claim 2.2. Every Markov random ﬁeld can be put in canonical form

(cid:96)

2.2 Non-Degeneracy
Let H = (V  H) denote a hypergraph obtained from the Markov random ﬁeld as follows. For every
non-zero tensor θi1···i(cid:96) we associate a hyperedge (i1 ··· i(cid:96)). We say that a hyperedge h is maximal if
no other hyperedge of strictly larger size contains h. Now G = (V  E) can be obtained by replacing
every hyperedge with a clique. Let D be a bound on the maximum degree. Recall that Γ(u) denotes
the neighbors of u. We will require the following conditions in order to ensure that the presence and
absence of every maximal hyperedge is information-theoretically determined:
Deﬁnition 2.3. We say that a Markov random ﬁeld is α β-non-degenerate if

(a) Every edge (i  j) in the graph G is contained in some hyperedge h ∈ H where the corre-

sponding tensor is non-zero.

(b) Every maximal hyperedge h ∈ H has at least one entry lower bounded by α in absolute

value.

(c) Every entry of θi1i2···i(cid:96) is upper bounded by a constant β in absolute value.

We will refer to a hyperedge h with an entry lower bounded by α in absolute value as α-
nonvanishing.

2.3 Bounds on Conditional Probabilities

First we review properties of the conditional probabilities in a Markov random ﬁeld as well as
introduce some convenient notation which we will use later on. Fix a node u and its neighborhood
U = Γ(u). Then for any R ∈ [ku] we have

P (Xu = R|XU ) =

(cid:80)ku
exp(E X
u R)
B=1 exp(E X

u B)

(2)

4This is the same as writing the log of the probability mass function according to the Efron-Stein decom-
position with respect to the uniform measure on colors; this decomposition is known to be unique. See e.g.
Chapter 8 of [16]

4

where we deﬁne

r(cid:88)

(cid:88)

(cid:96)=1

i2<···<i(cid:96)

E X
u R =

θui2···i(cid:96)(R  Xi2 ···   Xi(cid:96))

and i2  . . .   i(cid:96) range over elements of the neighborhood U; when (cid:96) = 1 the inner sum is just θu(R).
Let X∼u = X[n]\{u}. To see that the above is true  ﬁrst condition on X∼u  and observe that the
probability for a certain Xu is proportional to exp(E X
u R)  which gives the right hand side of (2).
Then apply the tower property for conditional probabilities.
Therefore if we deﬁne (where |T|max denotes the maximum entry of a tensor T )

(cid:18) D

r(cid:88)

(cid:19)

(cid:96) − 1

 

δ :=

exp(−2γ)

1
K

γ := sup

u

(cid:96)=1

then for any R

r(cid:88)

(cid:88)

|θui2···i(cid:96)|max ≤ β

i2<···<i(cid:96)
P (Xu = R|XU ) ≥ exp(−γ)

(cid:96)=1

K exp(γ)

=

1
K

exp(−2γ) = δ

(3)

(4)

Observe that if we pick any node i and consider the new Markov random ﬁeld given by conditioning
on a ﬁxed value of Xi  then the value of γ for the new Markov random ﬁeld is non-increasing.

3 The Guessing Game

Here we introduce a game-theoretic framework for understanding mutual information in general
Markov random ﬁelds. The GUESSINGGAME is deﬁned as follows:

1. Alice samples X = (X1  . . .   Xn) and X(cid:48) = (X(cid:48)

1  . . .   X(cid:48)

n) independently from the Markov random

ﬁeld

2. Alice samples R uniformly at random from [ku]
3. Alice samples a set I of size s = min(r − 1  du) uniformly at random from the neighbors of u
4. Alice tells Bob I  XI and R

5. Bob wagers w with |w| ≤ γK(cid:0) D

r−1
6. Bob gets ∆ = w1Xu=R − w1X(cid:48)

(cid:1)

u=R

Bob’s goal is to guess Xu given knowledge of the states of some of u’s neighbors. The Markov
random ﬁeld (including all of its parameters) are common knowledge. The intuition is that if Bob
can obtain a positive expected value  then there must be some set I of neighbors of u which have
non-zero mutual information. In this section  will show that there is a simple  explicit strategy for
Bob that yields positive expected value.

3.1 A Good Strategy for Bob

Here we will show an explicit strategy for Bob that has positive expected value. Our analysis will
rest on the following key lemma:

Lemma 3.1. There is a strategy for Bob that wagers at most γK(cid:0) D
u R − (cid:88)

[w|X∼u  R] = E X

(cid:1) in absolute value that satisﬁes

r−1
E X

u B

B(cid:54)=R

Proof. First we explicitly deﬁne Bob’s strategy. Let

Φ(R  I  XI ) =

Cu (cid:96) s

1{i1···i(cid:96)}⊆I θui1···i(cid:96) (R  Xi1  . . .   Xi(cid:96))

E
I XI

s(cid:88)

(cid:96)=1

(cid:88)

i1<i2<···<i(cid:96)

5

where Cu (cid:96) s =

(du
s )
(du−(cid:96)
s−(cid:96) )

. Then Bob wagers

w = Φ(R  I  XI ) − (cid:88)

B(cid:54)=R

Φ(B  I  XI )

Notice that the strategy only depends on XI because all terms in the summation where {i1 ··· i(cid:96)}
are not a subset of I have zero contribution.
The intuition behind this strategy is that the weighting term satisifes

Cu (cid:96) s =

1

Pr[{i1  . . . i(cid:96)} ⊂ I]

E
I XI

[Φ(R  I  XI )|X∼u  R] =

Thus when we take the expectation over I and XI we get

r(cid:88)
(cid:88)
u R −(cid:80)
i2<···<i(cid:96)
u B. To complete the proof  notice that Cu (cid:96) s ≤
B(cid:54)=R E X

θui2···i(cid:96)(R  Xi2 ···   Xi(cid:96) ) = E X

u R

(cid:96)=1

(cid:0) D
(cid:1) which using the deﬁnition of γ implies that |Φ(R  I  XI )| ≤ γ(cid:0) D
and hence EI XI [w|X∼u  R] = E X
r−1
Bob wagers at most the desired amount (in absolute value).
Theorem 3.2. There is a strategy for Bob that wagers at most γK(cid:0) D

Now we are ready to analyze the strategy:

r−1

(cid:1) for any state B  and thus
(cid:1) in absolute value which

r−1

satisﬁes

E[∆] ≥ 4α2δr−1

r2re2γ

Proof. We will use the strategy from Lemma 3.1. First we ﬁx X∼u  X(cid:48)

E
I XI

[∆|X∼u  X(cid:48)

[w|X∼u  R]
which follows because ∆ = r1Xu=R − r1X(cid:48)
similarly X(cid:48)

∼u  R] = E
I XI

u does not depend on X∼u . Now using (2) we calculate:

∼u and R. Then we have

(cid:16)
∼u  R]
u=R and because r and Xu do not depend on X(cid:48)

Pr[Xu = R|X∼u  R] − Pr[X(cid:48)

u = R|X(cid:48)

(cid:17)

∼u and

(cid:80)
exp(E X
u R)
(cid:16) (cid:88)
B exp(E X

u B)
exp(E X

(cid:80)
− exp(E X(cid:48)
u R)
B exp(E X(cid:48)
u B)
u R + E X(cid:48)
u B) − exp(E X

. Thus putting it all together we have

exp(E X

u R + E X(cid:48)

u B) − exp(E X

u B + E X(cid:48)
u R)

(cid:17)

u B + E X(cid:48)
u R)
(cid:17)

Pr[Xu = R|X∼u  R] − Pr[X(cid:48)

u = R|X(cid:48)

∼u  R] =

B(cid:54)=R

=

1
D

(cid:17)
(cid:17)(cid:16) (cid:88)

B(cid:54)=R

(cid:16)(cid:80)

where D =

B exp(E X

u B)

[∆|X∼u  X(cid:48)

∼u  R] =

1
D

E
I XI

Now it is easy to see that(cid:88)

distinct R G B

B exp(E X(cid:48)
u B)
E X

u B

(cid:17)(cid:16)(cid:80)
(cid:16)E X
u R − (cid:88)
(cid:88)

B(cid:54)=R

G(cid:54)=R

E X

u B

exp(E X

u R + E X(cid:48)

u G) − exp(E X

u G + E X(cid:48)
u R)

 = 0

which follows because when we interchange R and G the entire term multiplies by a negative one
and so we can pair up the terms in the summation so that they exactly cancel. Using this identity we
get

[∆|X∼u  X(cid:48)

∼u] =

E
I XI

1

kuD

u R − E X

u B

exp(E X

u R + E X(cid:48)

u B) − exp(E X

u B + E X(cid:48)
u R)

(cid:16)E X

(cid:88)

(cid:88)

B(cid:54)=R

R

(cid:17)

(cid:17)(cid:16)

6

where we have also used the fact that R is uniform on ku. And ﬁnally using the fact that X∼u and
X(cid:48)
∼u are identically distributed we can sample Y∼u and Z∼u and ﬂip a coin to decide whether we
set X∼u = Y∼u and X(cid:48)
[∆|Y∼u  Z∼u] =

∼u = Z∼u or vice-versa. Now we have

(cid:16)E Y

u B − eE Y

(cid:88)

(cid:88)

u B − E Z

u R + E Z

u R − E Y

(cid:17)(cid:16)

u B +E Z

u R+E Z

eE Y

(cid:17)

u B

u R

1

E
I XI

2kuD

B(cid:54)=R

R

With the appropriate notation it is easy to see that the above sum is strictly positive. Let aR B =
E Y
u R + E Z

u B. With this notation:

u B and bR B = E Z
[∆|Y∼u  Z∼u] =

u R + E Y
1

E
I XI

(cid:88)

(cid:88)

(cid:16)

2Dku

B(cid:54)=R

R

(cid:17)(cid:16)

aR B − bR B

exp(aR B) − exp(bR B)

(cid:17)

Since exp(x) is a strictly increasing function it follows that as long as aR B (cid:54)= bR B for some term
in the sum  the sum is positive. In Lemma 3.3 we prove that the expectation over Y and Z of this
sum is at least 4α2δr−1

r2re2γ   which completes the proof.

In the supplementary material we show how to use the law of total variance to give a quantitative
lower bound on the sum that arose in the proof of Theorem 3.2. More precisely we show:
Lemma 3.3.

u R−E Y

u B −E Z

u R +E Z

u B

exp(E Y

u R +E Z

u B)− exp(E Y

u B +E Z

u R)

(cid:17)(cid:105) ≥ 4α2δr−1

r2re2γ

(cid:104)(cid:88)

(cid:88)

(cid:16)E Y

B(cid:54)=R

R

E
Y Z

(cid:17)(cid:16)

4

Implications for Mutual Information

In this section we show that Bob’s strategy implies a lower bound on the mutual information between
node u and a subset I of its neighbors of size at most r − 1. We then extend the argument to work
with conditional mutual information as well.

4.1 Mutual Information in Markov Random Fields

Recall that the goal of the GUESSINGGAME is for Bob to use information about the states of nodes
I to guess the state of node u. Intuitively  if XI conveys no information about Xu then it should
contradict the fact that Bob has a strategy with positive expected value. We make this precise below.
Our argument proceeds in two steps. First we upper bound the expected value of any strategy.
Lemma 4.1. For any strategy 
E[∆] ≤ γK

(cid:104)| Pr[Xu = R|XI ] − Pr[Xu = R]|(cid:105)

(cid:18) D

(cid:19)

E

r − 1

I XI  R

Intuitively this follows because Bob’s optimal strategy given I  XI and R is to guess

w = sgn(Pr[Xu = R|XI ] − Pr[Xu = R])γK

Next we lower bound the mutual information using (essentially) the same quantity. We prove

Lemma 4.2. (cid:114) 1

I(Xu; XI ) ≥ 1

K r E

XI  R

2

(cid:104)| Pr(Xu = R|XI ) − Pr(Xu = R)|(cid:105)

These bounds together yield a lower bound on the mutual information. In the supplementary ma-
terial  we show how to extend the lower bound for mutual information to conditional mutual infor-
mation. The main idea is to show there is a setting of XS where the hyperedges do not completely
cancel out in the Markov random ﬁeld we obtain by conditioning on XS.
Theorem 4.3. Fix a vertex u such that all of the maximal hyperedges containing u are α-
nonvanishing  and a subset of the vertices S which does not contain the entire neighborhood of

7

u. Then taking I uniformly at random from the subsets of the neighbors of u not contained in S of
size s = min(r − 1 |Γ(u) \ S|) 

(cid:34)(cid:114) 1

2

(cid:35)

I(Xu; XI|XS)

≥ C(cid:48)(γ  K  α)

E
I

where explicitly

5 Applications

C(cid:48)(γ  K  α) :=

r2rK r+1(cid:0) D

4α2δr+d−1
r−1

(cid:1)γe2γ

We now employ the greedy approach of Bresler [4] which was previously used to learn Ising mod-
els on bounded degree graphs. Suppose we are given m independent samples from the Markov

random ﬁeld. Let (cid:99)Pr denote the empirical distribution and let (cid:98)E denote the expectation under this

distribution.
We compute empirical estimates for a certain information theoretic quantity νu I|S (deﬁned in the
supplementary material) as follows

(cid:98)EXS [|(cid:99)Pr(Xu = R  XI = G|XS) −(cid:99)Pr(Xu = R|XS)(cid:99)Pr(XI = G|XS)|]

(cid:98)νu I|S := E

R G

where R is a state drawn uniformly at random from [ku]  and G is an |I|-tuple of states drawn
independently uniformly at random from [ki1 ]× [ki2]× . . .× [ki|I|] where I = (i1  i2  . . . i|I|). Also
we deﬁne τ (which will be used as a thresholding constant) as

τ := C(cid:48)(γ  k  α)/2

(5)

and L  which is an upper bound on the size of the superset of a neighborhood of u that the algorithm
will construct 

L := (8/τ 2) log K = (32/C(cid:48)(γ  k  α)2) log K.

(6)

Then the algorithm MRFNBHD at node u is:

1. Fix input vertex u. Set S := ∅.
2. While |S| ≤ L and there exists a set of vertices I ⊂ [n] \ S of size at most r − 1 such that

(cid:98)νu I|S > τ  set S := S ∪ I.
3. For each i ∈ S  if(cid:98)νu i|S\i < τ then remove i from S.

4. Return set S as our estimate of the neighborhood of u.

(cid:16)

m ≥ 60K 2L
τ 2δ2L

(cid:17)

Theorem 5.1. Fix ω > 0. Suppose we are given m samples from an α  β-non-degenerate Markov
random ﬁeld with r-order interactions where the underlying graph has maximum degree at most D
and each node takes on at most K states. Suppose that

log(1/ω) + log(L + r) + (L + r) log(nK) + log 2

.

Then with probability at least 1 − ω  MRFNBHD when run starting from each node u recovers the
correct neighborhood of u  and thus recovers the underlying graph G. Furthermore  each run of the
algorithm takes O(mLnr) time.

In many situations  it is too expensive to obtain full samples from a Markov random ﬁeld (e.g. this
could involve needing to measure every potential symptom of a patient). Here we consider a model
where we are allowed only partial observations in the form of a C-bounded query:
Deﬁnition 5.2. A C-bounded query to a Markov random ﬁeld is speciﬁed by a set S with |S| ≤ C
and we observe XS

8

Our algorithm MRFNBHD can be made to work with C-bounded queries instead of full observations.
We prove:
Theorem 5.3. Fix an α  β-non-degenerate Markov random ﬁeld with r-order interactions where the
underlying graph has maximum degree at most D and each node takes on at most K states. The
bounded queries modiﬁcation to the algorithm returns the correct neighborhood of every vertex u
using m(cid:48)Lrnr-bounded queries of size at most L + r where

(cid:17)

(cid:16)

m(cid:48) =

60K 2L
τ 2δ2L

log(Lrnr/ω) + log(L + r) + (L + r) log(nK) + log 2

 

with probability at least 1 − ω.
In the supplementary material  we extend our results to the setting where we observe partial samples
where the state of each node is revealed independently with probability p  and the choice of which
nodes to reveal is independent of the sample.

Acknowledgements: We thank Guy Bresler for valuable discussions and feedback.

References
[1] Pieter Abbeel  Daphne Koller  and Andrew Y Ng. Learning factor graphs in polynomial time and sample

complexity. Journal of Machine Learning Research  7(Aug):1743–1788  2006.

[2] Anima Anandkumar  Daniel J Hsu  Furong Huang  and Sham M Kakade. Learning mixtures of tree

graphical models. In Advances in Neural Information Processing Systems  pages 1052–1060  2012.

[3] Animashree Anandkumar  Vincent YF Tan  Furong Huang  and Alan S Willsky. High-dimensional struc-
ture estimation in ising models: Local separation criterion. The Annals of Statistics  pages 1346–1375 
2012.

[4] Guy Bresler. Efﬁciently learning ising models on arbitrary graphs. In Proceedings of the Forty-Seventh

Annual ACM on Symposium on Theory of Computing  pages 771–782. ACM  2015.

[5] Guy Bresler  Elchanan Mossel  and Allan Sly. Reconstruction of markov random ﬁelds from samples:
Some observations and algorithms. In Approximation  Randomization and Combinatorial Optimization.
Algorithms and Techniques  pages 343–356. Springer  2008.

[6] Stephen G Brush. History of the lenz-ising model. Reviews of modern physics  39(4):883  1967.

[7] C Chow and Cong Liu. Approximating discrete probability distributions with dependence trees. IEEE

transactions on Information Theory  14(3):462–467  1968.

[8] Imre Csisz´ar and Zsolt Talata. Consistent estimation of the basic neighborhood of markov random ﬁelds.
In Information Theory  2004. ISIT 2004. Proceedings. International Symposium on  page 170. IEEE 
2004.

[9] Gautam Dasarathy  Aarti Singh  Maria-Florina Balcan  and Jong Hyuk Park. Active learning algorithms

for graphical model selection. J. Mach. Learn. Res  page 199207  2016.

[10] Sanjoy Dasgupta. Learning polytrees.

In Proceedings of the Fifteenth conference on Uncertainty in

artiﬁcial intelligence  pages 134–141. Morgan Kaufmann Publishers Inc.  1999.

[11] Ali Jalali  Pradeep Ravikumar  Vishvas Vasuki  and Sujay Sanghavi. On learning discrete graphical
models using group-sparse regularization. In Proceedings of the Fourteenth International Conference on
Artiﬁcial Intelligence and Statistics  pages 378–387  2011.

[12] Jon Kleinberg and Eva Tardos. Approximation algorithms for classiﬁcation problems with pairwise re-
lationships: Metric labeling and markov random ﬁelds. Journal of the ACM (JACM)  49(5):616–639 
2002.

[13] Su-In Lee  Varun Ganapathi  and Daphne Koller. Efﬁcient structure learning of markov networks using l
1-regularization. In Proceedings of the 19th International Conference on Neural Information Processing
Systems  pages 817–824. MIT Press  2006.

[14] Fabio Martinelli and Enzo Olivieri. Approach to equilibrium of glauber dynamics in the one phase region.

Communications in Mathematical Physics  161(3):447–486  1994.

9

[15] Elchanan Mossel  Dror Weitz  and Nicholas Wormald. On the hardness of sampling independent sets

beyond the tree threshold. Probability Theory and Related Fields  143(3):401–439  2009.

[16] Ryan O’Donnell. Analysis of Boolean Functions. Cambridge University Press  New York  NY  USA 

2014.

[17] Pradeep Ravikumar  Martin J Wainwright  John D Lafferty  et al. High-dimensional ising model selection

using ?1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.

[18] Narayana P Santhanam and Martin J Wainwright. Information-theoretic limits of selecting binary graph-

ical models in high dimensions. IEEE Transactions on Information Theory  58(7):4117–4134  2012.

[19] Allan Sly. Computational transition at the uniqueness threshold. In Foundations of Computer Science

(FOCS)  2010 51st Annual IEEE Symposium on  pages 287–296. IEEE  2010.

[20] Allan Sly and Nike Sun. The computational hardness of counting in two-spin models on d-regular graphs.
In Foundations of Computer Science (FOCS)  2012 IEEE 53rd Annual Symposium on  pages 361–369.
IEEE  2012.

[21] Nathan Srebro. Maximum likelihood bounded tree-width markov networks. In Proceedings of the Seven-
teenth conference on Uncertainty in artiﬁcial intelligence  pages 504–511. Morgan Kaufmann Publishers
Inc.  2001.

[22] Gregory Valiant. Finding correlations in subquadratic time  with applications to learning parities and
In Foundations of Computer Science (FOCS)  2012 IEEE 53rd Annual Symposium on  pages

juntas.
11–20. IEEE  2012.

[23] Marc Vuffray  Sidhant Misra  Andrey Lokhov  and Michael Chertkov. Interaction screening: Efﬁcient and
sample-optimal learning of ising models. In Advances in Neural Information Processing Systems  pages
2595–2603  2016.

10

,Hanie Sedghi
Anima Anandkumar
Linus Hamilton
Frederic Koehler
Claudia Shi
David Blei
Victor Veitch