2014,Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling,A new Bayesian formulation is developed for nonlinear support vector machines (SVMs)  based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model  in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling  from the standpoints of accuracy and interpretability,Bayesian Nonlinear Support Vector Machines and

Discriminative Factor Modeling

Ricardo Henao  Xin Yuan and Lawrence Carin
Department of Electrical and Computer Engineering

Duke University  Durham  NC 27708

{r.henao xin.yuan lcarin}@duke.edu

Abstract

A new Bayesian formulation is developed for nonlinear support vector machines
(SVMs)  based on a Gaussian process and with the SVM hinge loss expressed as
a scaled mixture of normals. We then integrate the Bayesian SVM into a factor
model  in which feature learning and nonlinear classiﬁer design are performed
jointly; almost all previous work on such discriminative feature learning has as-
sumed a linear classiﬁer.
Inference is performed with expectation conditional
maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive
set of experiments demonstrate the utility of using a nonlinear Bayesian SVM
within discriminative feature learning and factor modeling  from the standpoints
of accuracy and interpretability.

1

Introduction

There has been signiﬁcant interest recently in developing discriminative feature-learning models  in
which the labels are utilized within a max-margin classiﬁer. For example  such models have been
employed in the context of topic modeling [1]  where features are the proportion of topics associated
with a given document. Such topic models may be viewed as a stochastic matrix factorization of
a matrix of counts. The max-margin idea has also been extended to factorization of more general
matrices  in the context of collaborative prediction [2  3]. These studies have demonstrated that the
use of the max-margin idea  which is closely related to support vector machines (SVMs) [4]  often
yields better results than designing discriminative feature-learning models via a probit or logit link.
This is particularly true for high-dimensional data (e.g.  a corpus characterized by a large dictionary
of words)  as in that case the features extracted from the high-dimensional data may signiﬁcantly
outweigh the importance of the small number of labels in the likelihood. Margin-based classiﬁers
appear to be attractive in mitigating this challenge [1].

Joint matrix factorization  feature learning and classiﬁer design are well aligned with hierarchical
models. The Bayesian formalism is well suited to such models  and much of the aforementioned
research has been constituted in a Bayesian setting. An important aspect of this prior work utilizes
the recent recognition that the SVM loss function may be expressed as a location-scale mixture of
normals [5]. This is attractive for joint feature learning and classiﬁer design  which is leveraged in
this paper. However  the Bayesian SVM setup developed in [5] assumed a linear classiﬁer decision
function  which is limiting for sophisticated data  for which a nonlinear classiﬁer is more effective.

The ﬁrst contribution of this paper concerns the extension of the work in [5] for consideration of a
kernel-based  nonlinear SVM  and to place this within a Bayesian scaled-mixture-of-normals con-
struction  via a Gaussian process (GP) prior. The second contribution is a generalized formulation of
this mixture model  for both the linear and nonlinear SVM  which is important within the context of
Markov Chain Monte Carlo (MCMC) inference  yielding improved mixing. This new construction
generalizes the form of the SVM loss function.

1

The manner we employ a GP in this paper is distinct from previous work [6  7  8]  in that we ex-
plicitly impose a max-margin-based SVM cost function. In the previous GP-based classiﬁer design 
all data contributed to the learned classiﬁcation function  while here a relatively small set of support
vectors play a dominant role. This identiﬁcation of support vectors is of interest when the number of
training samples is large (simplifying subsequent prediction). The key reason to invoke a Bayesian
form of the SVM [5]  instead of applying the widely studied optimization-based SVM [4]  is that the
former may be readily integrated into sophisticated hierarchical models. As an example of that  we
here consider discriminative factor modeling  in which the factor scores are employed within a non-
linear SVM. We demonstrate the advantage of this in our experiments  with nonlinear discriminative
factor modeling for high-dimensional gene-expression data.

We present MCMC and expectation conditional maximization inference for the model. Conditional
conjugacy of the hierarchical model yields simple and efﬁcient computations. Hence  while the non-
linear SVM is signiﬁcantly more ﬂexible than its linear counterpart  computations are only modestly
more complicated. Details on the computational approaches  insights on the characteristics of the
model  and demonstration on real data constitute a third contribution of this paper.

2 Mixture Representation for SVMs
Previous model for linear SVM Assume N observations {xn  yn}N
n=1  where xn ∈ Rd is a
feature vector and yn ∈ {−1  1} is its label. The support vector machine (SVM) seeks to ﬁnd a
classiﬁcation function f (x) by solving a regularized learning problem

argminf (x) nγPN

n=1 max(1 − ynf (xn)  0) + R(f (x))o  

where max(1 − ynf (xn)  0) is the hinge loss  R(f (x)) is a regularization term that controls the
complexity of f (x)  and γ is a tuning parameter controlling the tradeoff between error penalization
and the complexity of the classiﬁcation function. The decision boundary is deﬁned as {x : f (x) =
0} and sign(f (x)) is the decision rule  classifying x as either −1 or 1 [4].
Recently  [5] showed that for the linear classiﬁer f (x) = β⊤x  minimizing (1) is equivalent to
estimating the mode of the pseudo-posterior of β

p(β|X  y  γ) ∝ QN

n=1 L(yn|xn  β  γ)p(β|·)  

where y = [y1 . . . yN ]⊤  X = [x1 . . . xN ]  L(yn|xn  β  γ) is the pseudo-likelihood function 
and p(β|·) is the prior distribution for the vector of coefﬁcients β. Choosing β to maximize the
log of (2) corresponds to (1)  where the prior is associated with R(f (x)).
In [5] it was shown
that L(yn|xn  β  γ) admits a location-scale mixture of normals representation by introducing latent
variables λn  such that
(cid:19) dλn . (3)
L(yn|xn  β  γ) = e−2γmax(1−ynβ⊤

(1 + λn − ynβ⊤xn)2

xn 0) = Z ∞

exp(cid:18)−

√γ
√2πλn

2γ−1λn

0

Expression (2) is termed a pseudo-posterior because its likelihood term is unnormalized with respect
to yn. Note that an improper ﬂat prior is imposed on λn.

The original formulation of [5] has the tuning parameter γ as part of the prior distribution of β 
while here in (3) it is included instead in the likelihood. This is done because (i) it puts λn and
the regularization term γ together  and (ii) it allows more freedom in the choice of the prior for β.
Additionally  it has an interesting interpretation  in that the SVM loss function behaves like a global-
local shrinkage distribution [9]. Speciﬁcally  γ−1 corresponds to a “global” scaling of the variance 
and λn represents the “local” scaling for component n. The {λn} deﬁne the relative variances for
each of the N data  and γ−1 provides a global scaling.

One of the beneﬁts of a Bayesian formulation for SVMs is that we can ﬂexibly specify the behavior
of β while being able to adaptively regularize it by specifying a prior p(γ) as well. For instance  [5]
gave three examples of prior distributions for β: Gaussian  Laplace  and spike-slab.

We can extend the results of [5] to a slightly more general loss function  by imposing a proper prior

for the latent variables λn. In particular  by specifying λn ∼ Exp(γ0) and letting un = 1−ynβ⊤xn 

(1)

(2)

L(yn|xn  β  γ) =Z ∞

0

γ0√γ
√2πλ

e− γ

2

(un+λn )2

λn

e−γ0λn dλn =

2

γ0
c

e−γ(c|un|+un)  

(4)

where c = p1 + 2γ0γ−1 > 1. The proof relies (see Supplementary Material) on the identity 
R ∞
0 a(2πλ)−1/2 exp{− 1
2 (a2λ + b2λ−1)}dλ = e−|ab| [10]. From (4) we see that as γ0 → 0 we
recover (3) by noting that 2max(un  0) = |un| + un.
In general we may use the prior λn ∼
Ga(aλ  γ0)  with aλ = 1 for the exponential distribution.
In the next section we discuss other
choices for aλ. This means that the proposed likelihood is no longer equivalent to the hinge loss but
to a more general loss  termed below a skewed Laplace distribution.

γ0

  (5)

e−γ(c−1)|un|  

c (cid:26)e−γ(c+1)un  

0 N (un| − λn  γ−1λn)Exp(λn|γ0)dλn =

Skewed Laplace distribution We can write the likelihood function in (4) in terms of un as
if un ≥ 0
if un < 0

L(un|γ  γ0) = Z ∞
which corresponds to a Laplace distribution  with negative skewness  denoted as sLa(un|γ  γ0).
Unlike the density derived from the hinge loss (γ0 → 0)  this density is properly normalized  thus
it corresponds to a valid probability density function. For the special case γ0 = 0  the integral
diverges  hence the normalization constant does not exist  which stems from exp(−2γmax(un  0))
being constant for −∞ < un < 0.
From (5) we see that sLa(un|γ  γ0) can be represented either as mixture of normals or mixture
of exponentials. Other properties of the distribution  such as its moments  can be obtained using
the results for general asymmetric Laplace distributions in [11]. Examining (5) we can gain some
intuition about the behavior of the likelihood function for the classiﬁcation problem: (i) When
ynβ⊤xn = 1  λn = 0 and xn lies on the margin boundary.
(ii) When ynβ⊤xn > 1  xn is
correctly classiﬁed  outside the margin and |1 − ynβ⊤xn| is exponential with rate γ(c − 1). (iii)
xn is correctly classiﬁed but lies inside the margin when 0 < ynβ⊤xn < 1  and xn is misclassiﬁed
when ynβ⊤xn < 0. In both cases  1 − ynβ⊤xn is exponential with rate γ(c + 1). (iv) Finally  if
ynβ⊤xn = 0  xn lies on the decision boundary.
Since c + 1 > c− 1 for every c > 1  the distribution for case (ii) decays slower than the distribution
for case (iii). Alternatively  in terms of the loss function  observations satisfying (iii) get more
penalized than those satisfying (ii). In the limiting case  γ0 → 0 we have c → 1  and case (ii) is
not penalized at all  recovering the behavior of the hinge loss. In the SVM literature  an observation
xn is called a support vector if it satisﬁes cases (i) or (iii). In the latter case  λn is the distance
from ynβ⊤xn to the margin boundary [4]. The key thing that the Exp(λ0) prior imposes on λn 
relative to the ﬂat prior on λn ∈ [0 ∞)  is that it constrains that λn not be too large (discouraging
ynβ⊤xn ≫ 1 for correct classiﬁcations  which is even more relevant for nonlinear SVMs); we
discuss this further below.

Extension to nonlinear SVM We now assume that the decision function f (x) is drawn from
a zero-mean Gaussian process GP(0  k(x ·  θ))  with kernel parameters θ. Evaluated at the N
points at which we have data  f ∼ N (0  K)  where K is a N × N covariance matrix with entries
kij = k(xi  xj  θ) for i  j ∈ {1  . . .   N} [7]; f = [f1 . . . fN ]⊤ ∈ RN corresponds to the continuous
f (x) evaluated at {xn}N
n=1. Together with (5)  for un = 1− ynfn  where fn = f (xn)  the full prior
f ∼ N (0  K)   λn ∼ Exp(γ0)   γ ∼ Ga(a0  b0) .

speciﬁcation for the nonlinear SVM is

It is straightforward to prove the equality in (5) holds for fn in place of β⊤xn  as in (6).

(6)

For nonlinear SVMs as above  being able to set γ0 > 0 is particularly beneﬁcial. It prevents fn
from being arbitrarily large (hence preventing 1 − ynfn ≪ 0). This implies that isolated observa-
tions far away from linear decision boundary (even when correctly classiﬁed when learning) tend
to be support vectors in a nonlinear SVM  yielding more conservative learned nonlinear decision

boundaries. Figure 1 shows examples of log N (1 − ynfn;−λn  γ−1λn) Exp(λn; γ0) for γ = 100
and γ0 = {0.01  100}. The vertical lines denote the margin boundary (ynfn = 1) and the decision
boundary (ynfn = 0). We see that when γ0 is small  the density has a very pronounced negative
skewness (like in the hinge loss of the original SVM) whereas when γ0 is large  the density tends to
be more of a symmetric shape.

Inference

3
We wish to compute the posterior p(f   λ  γ|y  X)  where λ = [λ1 . . . λN ]⊤. We describe and have

implemented three inference procedures: Markov chain Monte Carlo (MCMC)  a point estimate via
expectation-conditional maximization (ECM) and a GP approximation for fast inference.

3

102

n
λ

100

10−2

 
−3

−2

−1

x 105

 

−1

−2

−3

−4

1

2

3

102

n
λ

100

10−2

 
−3

−2

−1

x 105

 

−1

−2

−3

−4

1

2

3

0

1 − ynfn

0

1 − ynfn

Figure 1: Examples of log N (1 − ynfn; −λn  γ−1λn)Exp(λn; γ0) for γ = 100 and γ0 = 0.01 (left) and
γ0 = 100 (right). The vertical lines denote the margin boundary (ynfn = 1) and the decision boundary
(ynfn = 0).

MCMC Inference is implemented by repeatedly sampling from the conditional posterior of pa-
rameters in (6). Conditional conjugacy allows us to express the following distributions in closed
form:

f |y  λ  γ ∼ N (m  S)   m = γSYΛ

−1(1 + λ)   S = γ

−1K(K + γ

−1Λ)−1Λ  

−1

λ

n |fn  yn  γ ∼ IG p1 + 2γ0γ−1
where Λ = diag(λ)  Y = diag(y)  ǫ = 1 + λ − Yf   and IG(µ  γ) is the inverse Gaussian
distribution with parameters µ and γ [10].

  γ + 2γ0!   γ|y  f   λ ∼ Ga(cid:18)a0 +

ǫ(cid:19)  

|1 − ynfn|

N  b0 +

1
2

1
2

(7)

Λ

−1

ǫ

⊤

In MCMC γ0 plays a crucial role  because it controls the prior variance of the latent variables λn 
thus greatly improving mixing  particularly that of γ. We also veriﬁed empirically that for small
values of γ0  γ is consistently underestimated. In practice we ﬁx γ0 = 0.1  however  a conjugate
prior (gamma) exists  and sampling from its conditional posterior is straightforward if desired.

The parameters of the covariance function θ in the GP require Metropolis-Hastings type algorithms 
as in most cases no closed form for their conditional posterior is available. However  the problem is
relatively well studied. We have found that slice sampling methods [12]  in particular the surrogate
data sampler of [13]  work well in practice  and are employed here.

For the case of SVMs  MCMC is naturally important as a way of quantifying the uncertainty of the
parameters of the model. Further  it allows us to use the hierarchy in (6) as a building block in more
sophisticated models  or to bring more ﬂexibility to f through specialized prior speciﬁcations. As an
example of this  Section 5 describes a speciﬁcation for a nonlinear discriminative factor model.

ECM The expectation-conditional maximization algorithm is a generalization of the expectation-
maximization (EM) algorithm. It can be used when there are multiple parameters that need to be
estimated [14]. From (6) we identify f and γ as the parameters to be estimated  and λn as the
latent variables. The Q function in EM-style algorithms is the complete data log-posterior  where
expectations are taken w.r.t. the posterior distribution evaluated at the current value of the parameter
of interest. From (7) we see that λn appears in the conditional posterior p(f|y  K  λ  γ) as ﬁrst order
terms  thus we can write

hλ−1

n |yn  f (i)

n i = E[λ−1

n   γ(i)] = p1 + 2γ0(γ(i))−1|u(i)
n |−1  
n = 1 − ynf (i)
n and γ(i) are the estimates of fn and γ at the i-th iteration  and u(i)
where f (i)
(7) and (8) we can obtain the EM updates: f (i+1) = K(K + (γ(i))−1hΛi)−1Y(1 + hλi) and
+ hλni(cid:17)−1

γ(i+1) = (cid:0)a0 − 1 + 1

2 N(cid:1)(cid:16)b0 + 1

n i(u(i+1)

n=1hλ−1

)2 + 2u(i+1)

2 PN

n

n

.

In the ECM setting  learning the parameters of the covariance function is not as straightforward as in
MCMC. However  we can borrow from the GP literature [7] and use the fact that we can marginalize
f while conditioning on λ and γ:

(8)

n . From

Z(y  X  λ  γ  θ) = N (Y(1 + λ)  K + γ−1Λ) .

(9)

Note that K is a function of X and θ. Estimation of θ is done by maximizing log Z(y  X  λ  γ  θ).
For this we need only compute the partial derivatives of (9) w.r.t. θ  and then use a gradient-based

4

optimizer. This is commonly known as Type II maximum likelihood (ML-II) [7]. In practice we
alternate between EM updates for {f   γ} and θ updates for a pre-speciﬁed number of iterations
(typically the model converges after 20 iterations).

Speeding up inference Perhaps one of the most well known shortcomings of GP is that its cubic
complexity is prohibitive for large scale problems. However there is an extensive literature about
approximations for fast GP models [15]. Here we use the Fully Independent Training Conditional
(FITC) approximation [16]  as it offers an attractive balance between complexity and performance

[15]. The basic idea behind FITC is to assume that f is generated i.i.d. from pseudo-inputs {vm}M
via fm ∈ RM such that fm ∼ N (0  Kmm)  where Kmm is a M×M covariance matrix. Speciﬁcally 

m=1

from (5) we have

p(u|fm) = QN

n=1 p(un|fm) = N (KnmK−1

mmfm  diag(K − Qnn) + γ−1Λ)  

where u = 1 − Yf   Kmn is the cross-covariance matrix between {vm}M
Qnn = KnmK−1

mmKmn. If we marginalize out fm thus
Z(y  X  λ  γ  θ) = N (Y(1 + λ)  Qnn + diag(K − Qnn) + γ−1Λ) .

(10)
Note that if we drop the diag(·) term in (10) due to the i.i.d. assumption for f   we recover the full
GP marginal from (9). Similar to the ML-II approach previously described  for a ﬁxed M we can
maximize log Z(y  X  λ  γ  θ) w.r.t. θ and {vm}M
m=1 using a gradient-based optimizer but with the
added beneﬁt of having decreased the computational cost from O(N 3) to O(N M 2) [16].

m=1 and {xn}N

n=1  and

Predictions Making predictions under the model in (6)  with conditional posterior distributions in
(7)  can be achieved using standard results of the multivariate normal distribution. The predictive
distribution of f⋆ for a new observation x⋆ given the dataset {X  y} can be written as

(11)
where Σ = (K + γ−1Λ)−1  k⋆ = k(x⋆  x⋆  θ) and k⋆ = [k(x⋆  x1  θ) . . . k(x⋆  xN   θ)]⊤.
Furthermore  we can directly use the probit link Φ(f⋆) to compute

f⋆|x⋆  X  y ∼ N (k⋆ΣY(1 + λ)  k⋆ − k⊤

⋆ Σk⋆)  

p(y⋆ = 1|x⋆  X  y) =Z Φ(f⋆)p(f⋆|x⋆  X  y)df⋆ = Φ(cid:0)k⋆ΣY(1 + λ)(1 + k⋆ − k⊤

⋆ Σk⋆)−1(cid:1)  

which follows from [7]. Computing the class membership probability is not possible in standard
SVMs  because in such optimization-based methods one does not obtain the variance of the predic-
tive distribution; this variance is an attractive component of the Bayesian construction.

The mean of the predictive distribution (11) is tightly related to the predictor in standard SVMs  in
the sense that both are manifestations of the representer theorem. In particular

E[f⋆|x⋆  X  y] = PN

n=1 αnk(x⋆  xn  θ)  

where α = (K + γ−1Λ)−1Y(1 + λ). From the expectations of λn and f conditioned on γ and
γ0 it is possible to show that α is a vector with elements γ(1 − c) ≤ αn ≤ γ(1 + c)  where
c = p1 + 2γ0γ−1. We differentiate three types of elements in α as follows

(12)

α = 


ynγ(1 + c) 
α0
n  
ynγ(1 − c)  

if ynfn < 1
if ynfn = 1 (λn = 0)
if ynfn > 1

 

(13)

0 0 (y0 − γ(1 + c)K0 aya − γ(1 − c)K0 byb)  where α0

with α0 = K−1
n is an element of α0  and
0  a and b are subsets of {1  . . .   N} for which λn = 0  ynfn < 1 and ynfn > 1  respectively.
This implies α and so the prediction rule in (12) depend on data for which λn > 0 only through
γ and γ0. Note also that we do not need the values of λ but whether or not they are different than
zero. When γ0 → 0 then c → 1 and α becomes a sparse vector bounded above by 2γ. This result
for standard SVMs can be found independently from the Karush-Kuhn-Tucker conditions for its
objective function [4].

For ECM and variational Bayes EM inference (the latter discussed below in Section 5)  we set
γ0 = 0 and therefore α is sparse  with αn = 0 when ynfn > 1  as in traditional SVMs. This
property of the proposed use of GPs within the Bayesian SVM formulation is a signiﬁcant advantage
relative to traditional classiﬁer design based directly on GPs  for which we do not have such sparsity
in general. For MCMC inference  we ﬁnd the sampler mixes better when γ0 6= 0. Details on the
derivations of (13) and the concavity of the problem may be found in Supplementary Material.

5

4 Related Work

A key contribution of this paper concerns extension of the linear Bayesian SVM developed in [5]
to a nonlinear Bayesian SVM. This has been implemented by replacing the linear f (x) = β⊤x
considered in [5] with an f (x) drawn from a GP. The most relevant previous work is that for which
a classiﬁer is directly implemented via a GP  without an explicit connection to the margin associated
with the SVM [7]. Speciﬁcally  GP-based classiﬁers have been developed by [17]. In [7] the f is
drawn from a GP  as in (6)  but f is used directly with a probit or logit link function  to estimate class
membership probability. Previous GP-based classiﬁers did not use f within a margin-based classiﬁer

as in (6)  implemented here via p(un) = N (−λn  γ−1λn)  where un = 1−ynfn. It has been shown

empirically that nonlinear SVMs and GP classiﬁers often perform similarly [8]. However  for the
latter  inference can be challenging due to the non-conjugacy of multivariate normal distribution
to the link function. Common inference strategies employ iterative approximate inference schemes 
such as the Laplace approximation [17] or expectation propagation (EP) [18]. The model we propose
here is locally fully conjugate (except for the GP kernel parameters) and inference can be easily
implemented using EM style algorithms  or via MCMC. Besides  the prediction rule of the GP
classiﬁer  which has a form almost identical to (12)  is generally not sparse and therefore lacks the
interpretation that may be provided by the relatively few support vectors.

5 Discriminative Factor Models

Combinations of factor models and linear classiﬁers have been widely used in many applications 
such as gene expression  proteomics and image analysis  as a way to perform classiﬁcation and
feature selection simultaneously [19  20]. One of the most common modeling approaches can be
written as

xn = Awn + ǫn  ǫn ∼ N (0  ψ−1I)   L(yn|β  wn ·)  

where A is a d×K matrix of factor loadings  wn ∈ RK is a vector of factor scores  ǫn is observation
noise (and/or model residual)  β is a vector of K linear classiﬁer coefﬁcients and L(·) is for instance
but not limited to the linear SVM likelihood in (5) (a logit or probit link may also be used). One of
many possible prior speciﬁcation for the above model is

ak ∼ N (0  Φk)   wn ∼ N (0  I)   ψ ∼ Ga(aψ  bψ)   β ∼ N (0  G)  

where ak is a column of A  Φk = diag(φ1k  . . .   φdk)  φik ∼ Exp(ν)  G = diag(g1  . . .   gK ) and
each element of A is distributed aik ∼ Laplace(ν) after marginalizing out {φik} [10]. Shrinkage
in A is typically a requirement when N ≪ d or when its columns  ak  need to be interpreted. For
simplicity  we can set G = I  however a shrinkage prior for the elements gk of G might be useful in
some applications  as a mechanism for factor score selection. Although the described model usually

works well in practice  it assumes that there is a linear mapping from Rd to RK   such that K ≪ d 
in which the classes {−1  1} are linearly separable. We can relax this assumption by imposing
the hierarchical model in (6) in place of β. This implies that matrix K from (6) has now entries
kij = k(wi  wj   θ). Inference using MCMC is straightforward except for the conditional posterior
of the factor scores. This model is related to latent-variable GP models (GP-LVM) [21]  in that we
infer the latent {wi} that reside within a GP kernel. However  here {wi} are also factor scores in a
factor model  and the GP is used within the context of a Bayesian SVM classiﬁer; neither of latter
two have been considered previously.

For the nonlinear Bayesian SVM classiﬁer we no longer have a closed form for the conditional of
wn  due to the covariance function of the GP prior. Thus  we require a Metropolis-Hastings type
algorithm. Here we use elliptical slice sampling [22]. Speciﬁcally  we sample wn from
p(wn|A  W\n  ψ  y  λ  γ  θ) ∝ p(wn|xn  A  ψ)Z(y  wn  W\n  λ  γ  θ)  

(14)
where p(wn|xn  A  ψ) ∼ N (SNψAxn  SN)  W = [w1 . . . wN ]  W\n is matrix W without
column n  S−1
N = ψA⊤A + I  and we have marginalized out f as in (9) with W in place of X.
The elliptical slice sampler proposes samples from p(wn|xn  A  ψ) while biasing them towards
more likely conﬁgurations of λ. Provided that λ ultimately controls the predictive distribution of
the classiﬁer in (11)  samples of wn will at the same time attempt to ﬁt the data and to improve
classiﬁcation performance. From (14)  note that we sample one column of W at a time  while
keeping the others ﬁxed. Details of the elliptical slice sampler are found in [22]. In applications in
which sampling from (14) is time prohibitive  we can use instead a variational Bayes EM (VB-EM)
approach. In the E-step  we approximate the posterior of A  {Φk}  ψ  f   λ and γ by a factorized
distribution q(A)Qk q(Φk)q(ψ)q(f )q(λ)q(γ) and in the M-step we optimize W and θ  using L-
BFGS [23]. Details of the implementation can be found in the Supplementary Material.

6

6 Experiments

In all experiments we set the covariance function to (i) either the square exponential (SE)  which

has the form k(xi  xj   θ) = exp(cid:0)−kxi − xjk2(cid:14) θ2)  where θ2 is known as the characteristic length

scale; or (ii) the automatic relevance determination (ARD) SE in which each dimension of x has
its own length scale [7]. All code used in the experiments was written in Matlab and executed on a
2.8GHz workstation with 4Gb RAM.

d
34
60
9
7
8

N
351
208
683
200
768
1540

Data set
Ionosphere
Sonar
Wisconsin
Crabs
Pima
USPS 3 vs 5

BSVM SVM GPC
7.41
12.50
2.64
2.5

Table 1: Benchmark data results. Mean % error
from 10-fold cross-validation.

Benchmark data We ﬁrst compare the perfor-
mance of the proposed Bayesian hierarchy for
nonlinear SVM (BSVM) against EP-based GP
classiﬁcation (GPC) and an optimization-based
SVM  on six well known benchmark datasets.
In particular  we use the same data and settings
as [8]  speciﬁcally 10-fold cross-validation and
SE covariance function. The parameters of the
SVM {γ  θ} are obtained by grid search using
an internal 5-fold cross-validation. GPC uses ML-II and a modiﬁed SE function k(xi  xj   θ) =
1 exp(cid:0)−kxi − xjk2(cid:14) θ2
θ2
2)  where θ1 acts as regularization trade-off similar to γ in our formulation
[7]. For our model we set 200 as the maximum number of iterations of the ECM algorithm and run
ML-II every 20 iterations. Table 1 shows mean errors for the methods under consideration. We see
that all three perform similarly as one might expect thus error bars are not showed  however BSVM
slightly outperforms the others in 4 out of 6 datasets. From the three methods  the SVM is clearly
faster than the others. GP classiﬁcation and our model essentially scale cubically with N   however 
ours is relatively faster mainly due to overhead computations needed by the EP algorithm. More
speciﬁcally  running times for the larger dataset (USPS 3 vs 5) were approximately 1000  1200 and
5000 seconds for SVM  BSVM and GPC  respectively.

5.71
11.54
3.07
2.0

5.98
11.06
2.93
1.5

22.01
1.69

24.22
1.56

21.88
1.49

256

102

Error
Time

3 vs. 5 (N = 767)

4 vs. non-4 (N = 7291)

FITC-GPC
3.69 ± 0.26

Table 2: FITC results (mean % error) for USPS data.

FITC-BSVM FITC-GPC
2.59 ± 0.17
3.49 ± 0.29

In order to test the approximation intro-
duced in Section 3 (to accelerate GP in-
ference) we use the traditional splitting of
USPS  7291 for model ﬁtting and the re-
maining 2007 for testing  on two different
tasks: 3 vs. 5 and 4 vs. non-4. Table 2
shows mean error rates and standard deviations for FITC versions of BSVM and GPC  for M = 100
pseudo-inputs and 10 repetitions. We see that FITC-BSVM slightly outperforms FITC-GPC in both
tasks while being relatively faster. As baselines  full BSVM and GPC on the 3 vs. 5 task perform
roughly similar at 2.46% error. We also veriﬁed (results not shown) that increasing M consistently
decreases error rates for both FITC-BSVM and FITC-GPC.

FITC-BSVM
2.44 ± 0.17

604

116

46

USPS data We applied the model proposed in Section 5 to the well known 3 vs. 5 subset of the
USPS handwritten digits dataset  consisting of 1540 gray scale 16 × 16 images  rescaled within
[−1  1]. We use the resampled version  this is  767 for model ﬁtting and the remaining 773 for test-
ing. As baselines  we also perform inference as a two step procedure  ﬁrst ﬁtting the factor model
(FM)  followed by a linear (L) or a nonlinear (N) SVM classiﬁer. We also consider learning jointly
the factor model but with a linear SVM (LDFM)  and a two step procedure consisting of LDFM fol-
lowed by a nonlinear SVM. Our proposed nonlinear discriminative factor model is denoted NDFM.
VB-EM versions of LDFM and NDFM are denoted as VLDFM and VNDFM  respectively. MCMC
details for the linear SVM part can be found in [5]. For inference  we set K = 10  a SE covari-
ance function and run the sampler for 1200 iterations  from which we discard the ﬁrst 600 and keep
every 10-th for posterior summaries. We observed in general good mixing regardless of random
initialization  and results remained very similar for different Markov chains.

Table 3 shows classiﬁcation results for the eight classiﬁers considered; we see that the nonlinear
classiﬁers perform substantially better than the linear counterparts. In addition  the proposed non-
linear joint model (NDFM) is the best of all ﬁve. The nonlinear classiﬁer is powerful enough to
perform well in both two step procedures. We found that VNDFM is not performing as good as
NDFM because the data likelihood is dominating over the labels likelihood in the updates for the
factor scores  which is not surprising considering the marked size differences between the two. On
the positive side  runtime for VNDFM is approximately two orders of magnitude smaller than that
of NDFM. We also tried a joint nonlinear model with a probit link as in GP classiﬁcation and we

7

Table 3: Mean % error with standard deviations and runtime (seconds) for USPS and gene expression data.

FM+L

FM+N

LDFM

VLDFM

LDFM+N

VLDFM+N

NDFM

VNDFM

Error
Time

6.21 ± 0.32

44

3.36 ± 0.26

840

5.95 ± 0.31

120

5.56 ± 0.18

60

3.62 ± 0.26

920

3.62 ± 0.19

160

2.72 ± 0.13

20000

3.23 ± 0.16

210

Error
Time

22.70 ± 0.92

105

19.52 ± 1.02

136

22.70 ± 0.92

126

22.31 ± 0.78

25

20.31 ± 0.88

158

19.52 ± 0.88

57

18.33 ± 0.84

18.33 ± 0.84

1100

103

Gene expression (10-fold cross-validation)

USPS (Test set)

found its classiﬁcation performance (a mean error rate of 3.10%) being slightly worse than that for
NDFM. In addition  we found that using ARD SE covariance functions to automatically select for
features of A and larger values of K did not substantial changed the results.

Gene expression data The dataset originally introduced in [24] consists of gene expression mea-
surements from primary breast tumor samples for a study focused towards ﬁnding expression pat-
terns potentially related to mutations of the p53 gene. The original data were normalized using RMA
and ﬁltered to exclude genes showing trivial variation. The ﬁnal dataset consists of 251 samples and
2995 normalized gene expression values. The labeling variable indicates whether or not a sample
exhibits the mutation. We use the same baseline and inference settings from our previous experi-
ment  but validation is done by 10-fold cross-validation. In preliminary results we found that factor
score selection improves results  hence for the linear classiﬁer (L) we used an exponential prior for
the variances of β  gk ∼ Exp(ρ)  and for the nonlinear case (N) we set an ARD SE covariance
function for K. Table 3 summarizes the results  the nonlinear variants outperform their linear coun-
terparts and our joint model perform slightly better than the others. Additionally  the joint nonlinear
model with GP and probit link yielded an error rate of 19.52%.

As a way of quantifying whether the features (factor loadings) produced by FM  LDFM and NDFM
are meaningful from a biological point of view  we performed Gene Ontology (GO) searches for the
gene lists encoded by each column of A. In order to quantify the strength of the association between
GO annotations and our gene lists we obtained Bonferroni corrected p-values [25]. We thresholded
the elements of matrix A such that |aik| > 0.1. Using the 10 lists from each model we found that
FM  LDFM and NDFM produced respectively 5  5 and 8 factors signiﬁcantly associated to GO terms
relevant to breast cancer. The GO terms are: fatty acid metabolism  induction of programmed cell
death (apoptosis)  anti-apoptosis  regulation of cell cycle  positive regulation of cell cycle  cell cycle
and Wnt signaling pathway. The strongest associations in all models are unsurprisingly apoptosis
and positive regulation of cell cycle  however  only NDFM produced a signiﬁcant association to
anti-apoptosis which we believe is responsible for the edge in performance of NDFM in Table 3.

7 Conclusion

We have introduced a fully Bayesian version of nonlinear SVMs  extending the previous restriction
to linear SVMs [5]. Almost all of the existing joint feature-learning and classiﬁer-design mod-
els assumed linear classiﬁers [2  3  26]. We have demonstrated in our experiments that there is a
substantial performance improvement manifested by the nonlinear classiﬁer. In addition  we have
extended the Bayesian equivalent of the hinge loss to a more general loss function  for both linear
and nonlinear classiﬁers. We have demonstrated that this approach enhances modeling ﬂexibility 
and yields improved MCMC mixing. The Bayesian setup allows one to directly compute class
membership probabilities. We showed how to use the nonlinear SVM as a module in a larger model 
and presented compelling results to highlight its potential. Point estimate inference using ECM is
conceptually simpler and easier to implement than MCMC or GP classiﬁcation  although MCMC is
attractive for integrating the factor model and classiﬁer (for example). We showed how FITC and
VB-EM based approximations can be used in conjunction with the SVM nonlinear classiﬁer and
discriminative factor modeling  respectively  as a way to scale inference in a principled way.

Acknowledgments

The research reported here was funded in part by ARO  DARPA  DOE  NGA and ONR.

8

References

[1] J. Zhu  A. Ahmed  and E. P. Xing. MedLDA: maximum margin supervised topic models for regression

and classiﬁcation. ICML  pages 1257–1264  2009.

[2] M. Xu  J. Zhu  and B. Zhang. Fast max-margin matrix factorization with data augmentation. ICML  pages

978–986  2013.

[3] M. Xu  J. Zhu  and B. Zhang. Nonparametric max-margin matrix factorization for collaborative predic-

tion. NIPS 25  pages 64–72  2012.

[4] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning  20(3):273–297  1995.

[5] N. G. Polson and S. L. Scott. Data augmentation for support vector machines. Bayesian Analysis  6(1):1–

23  2011.

[6] M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms. Neural Compu-

tation  12(11):2655–2684  2000.

[7] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press  2006.

[8] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁca-

tion. JMLR  6:1679–1704  2005.

[9] N. G. Polson and J. G. Scott. Shrink globally  act locally: sparse Bayesian regularization and prediction.

Bayesian Statistics  9:501–538  2010.

[10] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. JRSSB  36(1):99–102  1974.

[11] T. J. Kozubowski and K. Podgorski. A class of asymmetric distributions. Actuarial Research Clearing

House  1:113–134  1999.

[12] R. M. Neal. Slice sampling. AOS  31(3):705–741  2003.

[13] I. Murray and R. P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. NIPS

23  pages 1723–1731  2010.

[14] X.-L. Meng and D. B. Rubin. Maximum likelihood estimation via the ECM algorithm: A general frame-

work. Biometrika  80(2):267–278  1993.

[15] J Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process

regression. JMLR  6:1939–1959  2005.

[16] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. NIPS 18  pages 1257–

1264  2006.

[17] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. PAMI  20(12):1342–

1351  1998.

[18] Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  MIT  2001.

[19] C. M. Carvalho  J. Chang  J. E. Lucas  J. R. Nevins  Q. Wang  and M. West. High-dimensional sparse

factor modeling: Applications in gene expression genomics. JASA  103(484):1438–1456  2008.

[20] M. Zhou  H. Chen  J. Paisley  L. Ren  G. Sapiro  and L. Carin. Non-parametric Bayesian dictionary

learning for sparse image representations. NIPS 22  pages 2295–2303  2009.

[21] N.D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. NIPS

16  2003.

[22] I. Murray  R. P. Adams  and D. J. C. MacKay. Elliptical slice sampling. AISTATS  pages 541–548  2010.

[23] D. C. Liu and J. Nocedal. On the limited memory method for large scale optimization. Mathematical

Programming B  pages 503–528  1989.

[24] L. D. Miller  J. Smeds  J. George  V. B. Vega  L. Vergara  A. Ploner  Y. Pawitan  P. Hall  S. Klaar 
E. T. Liu  et al. An expression signature for p53 status in human breast cancer predicts mutation status 
transcriptional effects  and patient survival. PNAS  102(38):13550–13555  2005.

[25] J. T. Chang and J. R. Nevins. GATHER: a systems approach to interpreting genomic signatures. Bioin-

formatics  22(23):2926–2933  2006.

[26] J. Mairal  F. Bach  J. Ponce  G. Sapiro  and A. Zisserman. Supervised dictionary learning. NIPS 21  pages

1033–1040  2009.

9

,Ricardo Henao
Xin Yuan
Lawrence Carin