2017,The Importance of Communities for Learning to Influence,We consider the canonical problem of influence maximization in social networks. Since the seminal work of Kempe  Kleinberg  and Tardos there have been two  largely disjoint efforts on this problem. The first studies the problem associated with learning the generative model that produces cascades  and the second focuses on the algorithmic challenge of identifying a set of influencers  assuming the generative model is known. Recent results on learning and optimization imply that in general  if the generative model is not known but rather learned from training data  no algorithm for influence maximization can yield a constant factor approximation guarantee using polynomially-many samples  drawn from any distribution.  In this paper we describe a simple algorithm for maximizing influence from training data. The main idea behind the algorithm is to leverage the strong community structure of social networks and identify a set of individuals who are influentials but whose communities have little overlap. Although in general  the approximation guarantee of such an algorithm is unbounded  we show that this algorithm performs well experimentally. To analyze its performance  we prove this algorithm obtains a constant factor approximation guarantee on graphs generated through the stochastic block model  traditionally used to model networks with community structure.,The Importance of Communities for

Learning to Inﬂuence

Eric Balkanski
Harvard University

ericbalkanski@g.harvard.edu

Nicole Immorlica
Microsoft Research

nicimm@microsoft.com

Yaron Singer

Harvard University

yaron@seas.harvard.edu

Abstract

We consider the canonical problem of inﬂuence maximization in social networks.
Since the seminal work of Kempe  Kleinberg  and Tardos [KKT03] there have been
two  largely disjoint efforts on this problem. The ﬁrst studies the problem associated
with learning the generative model that produces cascades  and the second focuses
on the algorithmic challenge of identifying a set of inﬂuencers  assuming the
generative model is known. Recent results on learning and optimization imply that
in general  if the generative model is not known but rather learned from training data 
no algorithm for inﬂuence maximization can yield a constant factor approximation
guarantee using polynomially-many samples  drawn from any distribution.
In this paper we describe a simple algorithm for maximizing inﬂuence from training
data. The main idea behind the algorithm is to leverage the strong community
structure of social networks and identify a set of individuals who are inﬂuentials
but whose communities have little overlap. Although in general  the approximation
guarantee of such an algorithm is unbounded  we show that this algorithm performs
well experimentally. To analyze its performance  we prove this algorithm obtains a
constant factor approximation guarantee on graphs generated through the stochastic
block model  traditionally used to model networks with community structure.

1

Introduction

For well over a decade now  there has been extensive work on the canonical problem of inﬂuence
maximization in social networks. First posed by Domingos and Richardson [DR01  RD02] and
elegantly formulated and further developed by Kempe  Kleinberg  and Tardos [KKT03]  inﬂuence
maximization is the algorithmic challenge of selecting individuals who can serve as early adopters of
a new idea  product  or technology in a manner that will trigger a large cascade in the social network.
In their seminal paper  Kempe  Kleinberg  and Tardos characterize a family of natural inﬂuence
processes for which selecting a set of individuals that maximize the resulting cascade reduces to
maximizing a submodular function under a cardinality constraint. Since submodular functions can be
maximized within a 1  1/e approximation guarantee  one can then obtain desirable guarantees for
the inﬂuence maximization problem. There have since been two  largely separate  agendas of research
on the problem. The ﬁrst line of work is concerned with learning the underlying submodular function
from observations of cascades [LK03  AA05  LMF+07  GBL10  CKL11  GBS11  NS12  GLK12 
DSSY12  ACKP13  DSGRZ13  FK14  DBB+14  CAD+14  DGSS14  DLBS14  NPS15  HO15].
The second line of work focuses on algorithmic challenges revolving around maximizing inﬂuence 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

assuming the underlying function that generates the diffusion process is known [KKT05  MR07 
SS13  BBCL14  HS15  HK16  AS16].
In this paper  we consider the problem of learning to inﬂuence where the goal is to maximize inﬂuence
from observations of cascades. This problem synthesizes both problems of learning the function
from training data and of maximizing inﬂuence given the inﬂuence function. A natural approach for
learning to inﬂuence is to ﬁrst learn the inﬂuence function from cascades  and then apply a submodular
optimization algorithm on the function learned from data. Somewhat counter-intuitively  it turns
out that this approach yields desirable guarantees only under very strong learnability conditions1.
In some cases  when there are sufﬁciently many samples  and one can observe exactly which node
attempts to inﬂuence whom at every time step  these learnability conditions can be met. A slight
relaxation however (e.g. when there are only partial observations [NPS15  HXKL16])  can lead to
sharp inapproximability.
A recent line of work shows that even when a function is statistically learnable  optimizing the
function learned from data can be inapproximable [BRS17  BS17]. In particular  even when the
submodular function f : 2N ! R is a coverage function (which is PMAC learnable [BDF+12  FK14]) 
one would need to observe exponentially many samples {Si  f (Si)}m
i=1 to obtain a constant factor
approximation guarantee. Since coverage functions are special cases of the well studied models of
inﬂuence (independent cascade  linear and submodular threshold)  this implies that when the inﬂuence
function is not known but learned from data  the inﬂuence maximization problem is intractable.

Learning to inﬂuence social networks. As with all impossibility results  the inapproximability
discussed above holds for worst case instances  and it may be possible that such instances are rare
for inﬂuence in social networks. In recent work  it was shown that when a submodular function has
bounded curvature  there is a simple algorithm that can maximize the function under a cardinality
constraint from samples [BRS16]. Unfortunately  simple examples show that submodular functions
that dictate inﬂuence processes in social networks do not have bounded curvature. Are there other
reasonable conditions on social networks that yield desirable approximation guarantees?

Main result.
In this paper we present a simple algorithm for learning to inﬂuence. This algorithm
leverages the idea that social networks exhibit strong community structure. At a high level  the
algorithm observes cascades and aims to select a set of nodes that are inﬂuential  but belong to
different communities. Intuitively  when an inﬂuential node from a certain community is selected
to initiate a cascade  the marginal contribution of adding another node from that same community
is small  since the nodes in that community were likely already inﬂuenced. This observation can
be translated into a simple algorithm which performs very well in practice. Analytically  since
community structure is often modeled using stochastic block models  we prove that the algorithm
obtains a constant factor approximation guarantee in such models  under mild assumptions.

1.1 Technical overview

The analysis for the approximation guarantees lies at the intersection of combinatorial optimization
and random graph theory. We formalize the intuition that the algorithm leverages the community
structure of social networks in the standard model to analyze communities  which is the stochastic
block model. Intuitively  the algorithm obtains good approximations by picking the nodes that have
the largest individual inﬂuence while avoiding picking multiple nodes in the same community by
pruning nodes with high inﬂuence overlap. The individual inﬂuence of nodes and their overlap
are estimated by the algorithm with what we call ﬁrst and second order marginal contributions of
nodes  which can be estimated from samples. We then uses phase transition results of Erd˝os–Rényi
random graphs and branching processes techniques to compare these individual inﬂuences for nodes
in different communities in the stochastic block model and bound the overlap of pairs of nodes.

The optimization from samples model. Optimization from samples was recently introduced by
[BRS17] in the context of submodular optimization  we give the deﬁnition for general set functions.

1In general  the submodular function f : 2N ! R needs to be learnable everywhere within arbitrary precision 
i.e. for every set S one needs to assume that the learner can produce a surrogate function ˜f : 2N ! R s.t. for
every S ✓ N the surrogate guarantees to be (1  ✏)f (S)  ˜f (S)  (1 + ✏)f (S)  for ✏ 2 o(1)[HS16  HS17].

2

Pr

T2M

Deﬁnition 1. A class of functions F = {f : 2N ! R} is ↵-optimizable from samples over
distribution D under constraint M if there exists an algorithm s.t. for all f 2F   given a set of
i=1 where the sets Si are drawn i.i.d. from D  the algorithm returns S 2M
samples {(Si  f (Si))}m
s.t.:
S1 ... Sm⇠DE[f (S)]  ↵ · max

f (T )  1   

where the expectation is over the decisions of the algorithm and m 2 poly(|N|  1/).
We focus on bounded product distributions D  so every node a is  independently  in S ⇠D with some
probability pa 2 [1/ poly(n)  1  1/ poly(n)]. We assume this is the case throughout the paper.
Inﬂuence process. We assume that the inﬂuence process follows the standard independent cascade
model. In the independent cascade model  a node a inﬂuences each of its neighbors b with some
probability qab  independently. Thus  given a seed set of nodes S  the set of nodes inﬂuenced is the
number of nodes connected to some node in S in the random subgraph of the network which contains
every edge ab independently with probability qab .We deﬁne f (S) to be the expected number of nodes
inﬂuenced by S according to the independent cascade model over some weighted social network.

The learning to inﬂuence model: optimization from samples for inﬂuence maximization. The
learning to inﬂuence model is an interpretation of the optimization from samples model [BRS17]
for the speciﬁc problem of inﬂuence maximization in social networks. We are given a collection of
samples {(Si |cc(Si)|)}m
i=1 where sets Si are the seed sets of nodes and |cc(Si)| is the number of
nodes inﬂuenced by Si  i.e.  the number of nodes that are connected to Si in the random subgraph of
the network. This number of nodes is a random variable with expected value f (Si) := E[|cc(Si)|]
over the realization of the inﬂuence process. Each sample is an independent realization of the
inﬂuence process. The goal is then to ﬁnd a set of nodes S under a cardinality constraint k which
maximizes the inﬂuence in expectation  i.e.  ﬁnd a set S of size at most k which maximizes the
expected number of nodes f (S) inﬂuenced by seed set S.

2 The Algorithm

We present the main algorithm  COPS. This algorithm is based on a novel optimization from samples
technique which detects overlap in the marginal contributions of two different nodes  which is useful
to avoid picking two nodes who have intersecting inﬂuence over a same collection of nodes.

2.1 Description of COPS

COPS  consists of two steps.
It ﬁrst orders nodes in decreasing order of ﬁrst order marginal
contribution  which is the expected marginal contribution of a node a to a random set S ⇠D . Then 
it iteratively removes nodes a whose marginal contribution overlaps with the marginal contribution of
at least one node before a in the ordering. The solution is the k ﬁrst nodes in the pruned ordering.

Algorithm 1 COPS  learns to inﬂuence networks with COmmunity Pruning from Samples.
Input: Samples S = {(S  f (S))}  acceptable overlap ↵.

Order nodes according to their ﬁrst order marginal contributions
Iteratively remove from this ordering nodes a whose marginal contribution has overlap of at least
↵ with at least one node before a in this ordering.
return k ﬁrst nodes in the ordering

The strong performance of this algorithm for the problem of inﬂuence maximization is best explained
with the concept of communities. Intuitively  this algorithm ﬁrst orders nodes in decreasing order of
their individual inﬂuence and then removes nodes which are in a same community. This second step
allows the algorithm to obtain a diverse solution which inﬂuences multiple different communities
of the social network. In comparison  previous algorithms in optimization from samples [BRS16 
BRS17] only use ﬁrst order marginal contributions and perform well if the function is close to linear.
Due to the high overlap in inﬂuence between nodes in a same community  inﬂuence functions are far

3

from being linear and these algorithms have poor performance for inﬂuence maximization since they
only pick nodes from a very small number of communities.

2.2 Computing overlap using second order marginal contributions

We deﬁne second order marginal contributions  which are used to compute the overlap between the
marginal contribution of two nodes.
Deﬁnition 2. The second order expected marginal contribution of a node a to a random set S
containing node b is

vb(a) :=

[f (S [{ a})  f (S)].

E

S⇠D:a62S b2S

The ﬁrst order marginal contribution v(a) of node a is deﬁned similarly as the marginal contribution
of a node a to a random set S  i.e.  v(a) := ES⇠D:a62S[f (S [{ a})  f (S)]. These contributions can
be estimated arbitrarily well for product distributions D by taking the difference between the average
value of samples containing a and b and the average value of samples containing b but not a (see
Appendix B for details).
The subroutine OVERLAP(a  b  ↵)  ↵ 2 [0  1]  compares the second order marginal contribution of
a to a random set containing b and the ﬁrst order marginal contribution of a to a random set. If b
causes the marginal contribution of a to decrease by at least a factor of 1  ↵  then we say that a has
marginal contribution with overlap of at least ↵ with node b.

Algorithm 2 OVERLAP(a  b  ↵)  returns true if a and b have marginal contributions that overlap by
at least a factor ↵.
Input: Samples S = {(S  f (S))}  node a  acceptable overlap ↵

If second order marginal contribution vb(a) is at least a factor of 1  ↵ smaller than ﬁrst order
marginal contribution v(a) 
return Node a has overlap of at least ↵ with node b

OVERLAP is used to detect nodes in a same community. In the extreme case where two nodes a and b
are in a community C where any node in C inﬂuences all of community C  then the second order
marginal contribution vb(a) of a to random set S containing b is vb(a) = 0 since b already inﬂuences
all of C so a does not add any value  while v(a) ⇡| C|. In the opposite case where a and b are in
two communities which are not connected in the network  we have v(a) = vb(a) since adding b to a
random set S has no impact on the value added by a.

2.3 Analyzing community structure

The main beneﬁt from COPS is that it leverages the community structure of social networks. To
formalize this explanation  we analyze our algorithm in the standard model used to study the
community structure of networks  the stochastic block model. In this model  a ﬁxed set of nodes
V is partitioned in communities C1  . . .   C`. The network is then a random graph G = (V  E)
where edges are added to E independently and where an intra-community edge is in E with much
larger probability than an inter-community edge. These edges are added with identical probability
qsb
C for every edge in a same community  but with different probabilities for edges inside different
communities Ci and Cj. We illustrate this model in Figure 1.

3 Dense Communities and Small Seed Set in the Stochastic Block Model
In this section  we show that COPS achieves a 1  O(|Ck|1) approximation  where Ck is the kth
largest community  in the regime with dense communities and small seed set  which is described
below. We show that the algorithm picks a node from each of the k largest communities with
high probability  which is the optimal solution. In the next section  we show a constant factor
approximation algorithm for a generalization of this setting  which requires a more intricate analysis.
In order to focus on the main characteristics of the community structure as an explanation for the
performance of the algorithm  we make the following simplifying assumptions for the analysis. We

4

Figure 1: An illustration of the stochastic block model with communities C1  C2  C3 and C4 of sizes 6  4  4
and 4. The optimal solution for inﬂuence maximization with k = 4 is in green. Picking the k ﬁrst nodes in the
ordering by marginal contributions without pruning  as in [BRS16]  leads to a solution with nodes from only C1
(red). By removing nodes with overlapping marginal contributions  COPS obtains a diverse solution.

C and qsb

ﬁrst assume that there are no inter-community edges.2 We also assume that the random graph obtained
from the stochastic block model is redrawn for every sample and that we aim to ﬁnd a good solution
in expectation over both the stochastic block model and the independent cascade model.
Formally  let G = (V  E) be the random graph over n nodes obtained from an independent cascade
process over the graph generated by the stochastic block model. Similarly as for the stochastic block
model  edge probabilities for the independent cascade model may vary between different communities
and are identical within a single community C  where all edges have weights qic
C. Thus  an edge e
between two nodes in a community C is in E with probability pC := qic
C · qsb
C   independently for
every edge  where qic
C are the edge probabilities in the independent cascade model and the
stochastic block model respectively. The total inﬂuence by seed set S is then |ccG(Si)| where ccG(S)
is the set of nodes connected to S in G and we drop the subscript when it is clear from context. Thus 
the objective function is f (S) := EG[|cc(S)|]. We describe the two assumptions for this section.
Dense communities. We assume that for the k largest communities C  pC > 3 log |C|/|C| and
C has super-constant size (|C| = !(1)). This assumption corresponds to communities where the
probability pC that a node ai 2 C inﬂuences another node aj 2 C is large. Since the subgraph G[C]
of G induced by a community C is an Erd˝os–Rényi random graph  we get that G[C] is connected
with high probability (see Appendix C).
Lemma 3. [ER60] Assume C is a “dense" community  then the subgraph G[C] of G is connected
with probability 1  O(|C|2).
Small seed set. We also assume that the seed sets S ⇠D are small enough so that they rarely
intersect with a ﬁxed community C  i.e.  PrS⇠D[S\C = ;]  1o(1). This assumption corresponds
to cases where the set of early inﬂuencers is small  which is usually the case in cascades.
The analysis in this section relies on two main lemmas. We ﬁrst show that the ﬁrst order marginal
contribution of a node is approximately the size of the community it belongs to (Lemma 4). Thus  the
ordering by marginal contributions orders elements by the size of the community they belong to. Then 
we show that any node a 2 C that is s.t. that there is a node b 2 C before a in the ordering is pruned
(Lemma 5). Regarding the distribution S ⇠D generating the samples  as previously mentioned  we
consider any bounded product distribution. This implies that w.p. 1  1/ poly(n)  the algorithm can
compute marginal contribution estimates ˜v that are all a 1/ poly(n)-additive approximation to the
true marginal contributions v (See Appendix B for formal analysis of estimates). Thus  we give the
analysis for the true marginal contributions  which  with probability 1  1/ poly(n) over the samples 
easily extends for arbitrarily good estimates.
The following lemma shows that the ordering by ﬁrst order marginal contributions corresponds to the
ordering by decreasing order of community sizes that nodes belong to.
Lemma 4. For all a 2 C where C is one of the k largest communities  the ﬁrst order marginal
contribution of node a is approximately the size of its community  i.e.  (1  o(1))|C| v(a) | C|.
Proof. Assume a is a node in one of the k largest communities. Let Da and Da denote the
distributions S ⇠D conditioned on a 2 S and a 62 S respectively. We also denote marginal
contributions by fS(a) := f (S [{ a})  f (S). We obtain
smaller to qsb

2The analysis easily extends to cases where inter-community edges form with probability signiﬁcantly

C  for all C.

5

v(a) =

E

S⇠Da G

[fS(a)]  Pr
S⇠Da

[S \ C = ;] · Pr

G

[cc(a) = C] ·

E

S⇠Da : S\C=; 

G : cc(a)=C

[fS(a)]

= Pr

S⇠Da

[S \ C = ;] · Pr

G

[cc(a) = C] · |C|

 (1  o(1)) · |C|

where the last inequality is by the small seed set assumption and since C is connected with probability
1  o(1) (Lemma 3 and |C| = !(1) by dense community assumption). For the upper bound  v(a) is
trivially at most the size of a’s community since there are no inter-community edges.

The next lemma shows that the algorithm does not pick two nodes in a same community.
Lemma 5. With probability 1  o(1)  for all pairs of nodes a  b such that a  b 2 C where C is one of
the k largest communities  OVERLAP(a  b  ↵) = True for any constant ↵ 2 [0  1).
Proof. Let a  b be two nodes in one of the k largest communities C and Da b denote the distribution
S ⇠D conditioned on a 62 S and b 2 S. Then 

[fS(a)]  Pr[b 2 cc(a)] · 0 + Pr[b 62 cc(a)] · |C| = o(1)  o(1) · v(a)

vb(a) =

E

S⇠Da b

where the last equality is since G[C] is not connected w.p. O(|C|2) by Lemma 3 and since
|C| = !(1) by the dense community assumption  which concludes the proof.
By combining Lemmas 4 and 5  we obtain the main result for this section (proof in Appendix D).
Theorem 6. In the dense communities and small seed set setting  COPS with ↵-overlap allowed 
for any constant ↵ 2 (0  1) is a 1  o(1)-approximation algorithm for learning to inﬂuence from
samples from a bounded product distribution D.
4 Constant Approximation for General Stochastic Block Model

In this section  we relax assumptions from the previous section and show that COPS is a constant
factor approximation algorithm in this more demanding setting. Recall that G is the random graph
obtained from both the stochastic block model and the independent cascade model. A main observa-
tion that is used in the analysis is to observe that the random subgraph G[C]  for some community C 
is an Erd˝os–Rényi random graph G|C| pC .
Relaxation of the assumptions.
Instead of only considering dense communities where pC =
⌦((log |C|)/|C|)  we consider both tight communities C where pC  (1 + ✏)/|C| for some constant
✏> 0 and loose communities C where pC  (1  ✏)/|C| for some constant ✏> 0.3 We also
relax the small seed set assumption to the reasonable non-ubiquitous seed set assumption. Instead
of having a seed set S ⇠D rarely intersect with a ﬁxed community C  we only assume that
PrS⇠D[S \ C = ;]  ✏ for some constant ✏> 0. Again  since seed sets are of small sizes in practice 
it seems reasonable that with some constant probability a community does not contain any seeds.

Overview of analysis. At a high level  the analysis exploits the remarkably sharp threshold for the
phase transition of Erd˝os–Rényi random graphs. This phase transition (Lemma 7) tells us that a tight
community C contains w.h.p. a giant connected component with a constant fraction of the nodes
from C. Thus  a single node from a tight community inﬂuences a constant fraction of its community
in expectation. The ordering by ﬁrst order marginal contributions thus ensures a constant factor
approximation of the value from nodes in tight communities (Lemma 10). On the other hand  we show
that a node from a loose community inﬂuences only at most a constant number of nodes in expectation
(Lemma 8) by using branching processes. Since the algorithm checks for overlap using second order
marginal contributions  the algorithm picks at most one node from any tight community (Lemma 11).
Combining all the pieces together  we obtain a constant factor approximation (Theorem 12).

3Thus  we consider all possible sizes of communities except communities of size that converges to exactly

1/pC  which is unlikely to occur in practice.

6

We ﬁrst state the result for the giant connected component in a tight community  which is an immediate
corollary of the prominent giant connected component result in the Erd˝os–Rényi model.
Lemma 7. [ER60] Let C be a tight community with |C| = !(1)  then G[C] has a “giant" connected
component containing a constant fraction of the nodes in C w.p. 1  o(1).
The following lemma analyzes the inﬂuence of a node in a loose community through the lenses of
Galton-Watson branching processes to show that such a node inﬂuences at most a constant number of
nodes in expectation. The proof is deferred to Appendix E.
Lemma 8. Let C be a loose community  then f ({a})  c for all a 2 C and some constant c.
We can now upper bound the value of the optimal solution S?. Let C1  . . .   Ct be the t  k tight
communities that have at least one node in Ci that is in the optimal solution S? and that are of
super-constant size  i.e.  |C| = !(1). Without loss  we order these communities in decreasing order
of their size |Ci|.
Lemma 9. Let S? be the optimal set of nodes and Ci and t be deﬁned as above. There exists a
constant c such that f (S?) Pt
Proof. Let S?
B be a partition of the optimal nodes in nodes that are in tight communities
with super-constant individual inﬂuence and nodes that are not in such a community. The inﬂuence
A) is trivially upper bounded byPt
f (S?
B) 
Pa2S?
f ({a})  c· where the ﬁrst inequality is by submodularity and the second since nodes in
loose communities have constant individual inﬂuence by Lemma 8 and nodes in tight community
without super-constant individual inﬂuence have constant inﬂuence by deﬁnition. We conclude that
by submodularity  f (S?)  f (S?
Next  we argue that the solution returned by the algorithm is a constant factor away fromPt
i=1 |Ci|.
Lemma 10. Let a be the ith node in the ordering by ﬁrst order maginal contribution after the pruning
and Ci be the ith largest tight community with super-constant individual inﬂuence and with at least
one node in the optimal solution S?. Then  f ({a})  ✏|Ci| for some constant ✏> 0.
Proof. By deﬁnition of Ci  we have |C1|···|
Ci| that are all tight communities. Let b be a
node in Cj for j 2 [i]  1gc(C) be the indicator variable indicating if there is a giant component in
community C  and gc(C) be this giant component. We get

i=1 |Ci|. Next  there exists some constant c s.t. f (S?

B) Pt

A) + f (S?

i=1 |Ci| + c · k.

i=1 |Ci| + c · k.

A and S?

B

v(b)  Pr[1gc(Cj )] · Pr
S⇠Db

[S \ Cj = ;] · Pr[b 2 gc(Cj)] · E[|gc(Cj)| : b 2 gc(Cj)]

 (1  o(1)) · ✏1 · ✏2 · ✏3|Cj| ✏|Cj|

for some constants ✏1 ✏ 2 ✏ 3 ✏ > 0 by Lemma 7 and the non-ubiquitous assumption. Similarly as in
Theorem 6  if a and b are in different communities  OVERLAP(a  b  ↵) = False for ↵ 2 (0  1]. Thus 
there is at least one node b 2 [i
j=1Cj at position i or after in the ordering after the pruning  and
v(b)  ✏|Cj| for some j 2 [i]. By the ordering by ﬁrst order marginal contributions and since node a
is in ith position  v(a)  v(b)  and we get that f ({a})  v(a)  v(b)  ✏|Cj| ✏|Ci|.
Next  we show that the algorithm never picks two nodes from a same tight community and defer the
proof to Appendix E.
Lemma 11. If a  b 2 C and C is a tight community  then OVERLAP(a  b  ↵) = True for ↵ = o(1).
We combine the above lemmas to obtain the approximation guarantee of COPS (proof in Appendix E).
Theorem 12. With overlap allowed ↵ = 1/ poly(n)  COPS is a constant factor approximation
algorithm for learning to inﬂuence from samples drawn from a bounded product distribution D in the
setting with tight and loose communities and non-ubiquitous seed sets.

5 Experiments

In this section  we compare the performance of COPS and three other algorithms on real and synthetic
networks. We show that COPS performs well in practice  it outperforms the previous optimization
from samples algorithm and gets closer to the solution obtained when given complete access to the
inﬂuence function.

7

DBLP

Greedy
COPS
MargI
Random

400

300

200

100

e
c
n
a
m
r
o
f
r
e
P

300

250

200

150

100

50

e
c
n
a
m
r
o
f
r
e
P

DBLP

400

300

200

100

e
c
n
a
m
r
o
f
r
e
P

Facebook

300

250

200

150

100

50

e
c
n
a
m
r
o
f
r
e
P

Facebook

0
0.0

0.4

0.8

1.2
q

1.6

2.0
x10$2&
x10$2&

0

0

3

6

k

9

12

15

0
0.6

0.7

0.8

0.9

q

1

1.1
x10$2&
x10$2&

0

0

3

6

k

9

12

15

Stochastic Block Model 1

600

e
c
n
a
m
r
o
f
r
e
P

400

200

1500

1200

e
c
n
a
m
r
o
f
r
e
P

900

600

300

Stochastic Block Model 2

Preferential Attachment

Erdős–Rényi 

800

600

400

200

e
c
n
a
m
r
o
f
r
e
P

250

200

150

100

50

e
c
n
a
m
r
o
f
r
e
P

0
0.0

0.2

0.4

0.6

0.8

1.0
x105%

n

0
0.0

0.2

0.4

0.6

0.8

1.0
x105%
x105%

n

0
0.0

0.2

0.4

0.6

0.8

1.0
x105%
x105%

n

0
0.0

0.2

0.4

0.6

0.8

1.0
x105%
x105%

n

Figure 2: Empirical performance of COPS against the GREEDY upper bound  the previous optimization from
samples algorithm MARGI and a random set.

Experimental setup. The ﬁrst synthetic network considered is the stochastic block model  SBM 1 
where communities have random sizes with one community of size signiﬁcantly larger than the other
communities. We maintained the same expected community size as n varied. In the second stochastic
block model  SBM 2  all communities have same expected size and the number of communities was
ﬁxed as n varied. The third and fourth synthetic networks were an Erd˝os–Rényi (ER) random graph
and the preferential attachment model (PA). Experiments were also conducted on two real networks
publicly available ([LK15]). The ﬁrst is a subgraph of the Facebook social network with n = 4k
and m = 88k. The second is a subgraph of the DBLP co-authorship network  which has ground
truth communities as described in [LK15]  where nodes of degree at most 10 were pruned to obtain
n = 54k  m = 361k and where the 1.2k nodes with degree at least 50 were considered as potential
nodes in the solution.

Benchmarks. We considered three different benchmarks to compare the COPS algorithm against.
The standard GREEDY algorithm in the value query model is an upper bound since it is the optimal
efﬁcient algorithm given value query access to the function and COPS is in the more restricted setting
with only samples. MARGI is the optimization from samples algorithm which picks the k nodes
with highest ﬁrst order marginal contribution ([BRS16]) and does not use second order marginal
contributions. RANDOM simply returns a random set. All the samples are drawn from the product
distribution with marginal probability k/n  so that samples have expected size k. We further describe
the parameters of each plot in Appendix F.

Empirical evaluation. COPS signiﬁcantly outperforms the previous optimization from samples
algorithm MARGI  getting much closer to the GREEDY upper bound. We observe that the more there
is a community structure in the network  the better the performance of COPS is compared to MARGI 
e.g.  SBM vs ER and PA (which do not have a community structure). When the edge weight q := qi.c.
for the cascades is small  the function is near-linear and MARGI performs well  whereas when it is
large  there is a lot of overlap and COPS performs better. The performance of COPS as a function
of the overlap allowed (experiment in Appendix F) can be explained as follows: Its performance
slowly increases as the the overlap allowed increases and COPS can pick from a larger collection of
nodes until it drops when it allows too much overlap and picks mostly very close nodes from a same
community. For SBM 1 with one larger community  MARGI is trapped into only picking nodes from
that larger community and performs even less well than RANDOM. As n increases  the number of
nodes inﬂuenced increases roughly linearly for SBM 2 when the number of communities is ﬁxed
since the number of nodes per community increases linearly  which is not the case for SBM 1.

8

References

[AA05] Eytan Adar and Lada A. Adamic. Tracking information epidemics in blogspace. In WI  2005.

[ACKP13] Bruno D. Abrahao  Flavio Chierichetti  Robert Kleinberg  and Alessandro Panconesi. Trace

complexity of network inference. In KDD  2013.

[AS16] Rico Angell and Grant Schoenebeck. Don’t be greedy: Leveraging community structure to ﬁnd

high quality seed sets for inﬂuence maximization. arXiv preprint arXiv:1609.06520  2016.

[BBCL14] Christian Borgs  Michael Brautbar  Jennifer T. Chayes  and Brendan Lucier. Maximizing social
inﬂuence in nearly optimal time. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium
on Discrete Algorithms  SODA 2014  Portland  Oregon  USA  January 5-7  2014  pages 946–957 
2014.

[BDF+12] Ashwinkumar Badanidiyuru  Shahar Dobzinski  Hu Fu  Robert Kleinberg  Noam Nisan  and
Tim Roughgarden. Sketching valuation functions. In Proceedings of the twenty-third annual
ACM-SIAM symposium on Discrete Algorithms  pages 1025–1035. Society for Industrial and
Applied Mathematics  2012.

[BHK] Avrim Blum  John Hopcroft  and Ravindran Kannan. Foundations of data science.

[BRS16] Eric Balkanski  Aviad Rubinstein  and Yaron Singer. The power of optimization from samples. In
Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages 4017–4025  2016.

[BRS17] Eric Balkanski  Aviad Rubinstein  and Yaron Singer. The limitations of optimization from samples.

In STOC  2017.

[BS17] Eric Balkanski and Yaron Singer. The sample complexity of optimizing a convex function. In

COLT  2017.

[CAD+14] Justin Cheng  Lada A. Adamic  P. Alex Dow  Jon M. Kleinberg  and Jure Leskovec. Can cascades

be predicted? In WWW  2014.

[CKL11] Flavio Chierichetti  Jon M. Kleinberg  and David Liben-Nowell. Reconstructing patterns of

information diffusion from incomplete observations. In NIPS  2011.

[DBB+14] Abir De  Sourangshu Bhattacharya  Parantapa Bhattacharya  Niloy Ganguly  and Soumen
In CIKM 

Chakrabarti. Learning a linear inﬂuence model from transient opinion dynamics.
2014.

[DGSS14] Hadi Daneshmand  Manuel Gomez-Rodriguez  Le Song  and Bernhard Schölkopf. Estimating dif-
fusion network structures: Recovery conditions  sample complexity & soft-thresholding algorithm.
In ICML  2014.

[DLBS14] Nan Du  Yingyu Liang  Maria-Florina Balcan  and Le Song.

information diffusion networks. In ICML  2014.

Inﬂuence function learning in

[DR01] Pedro Domingos and Matthew Richardson. Mining the network value of customers. In KDD 

2001.

[DSGRZ13] Nan Du  Le Song  Manuel Gomez-Rodriguez  and Hongyuan Zha. Scalable inﬂuence estimation

in continuous-time diffusion networks. In NIPS  2013.

[DSSY12] Nan Du  Le Song  Alexander J. Smola  and Ming Yuan. Learning networks of heterogeneous

inﬂuence. In NIPS  2012.

[ER60] Paul Erdos and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad.

Sci  5(1):17–60  1960.

[FK14] Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals.

In COLT  2014.

[GBL10] Amit Goyal  Francesco Bonchi  and Laks VS Lakshmanan. Learning inﬂuence probabilities in

social networks. In KDD  2010.

[GBS11] Manuel Gomez-Rodriguez  David Balduzzi  and Bernhard Schölkopf. Uncovering the temporal

dynamics of diffusion networks. In ICML  2011.

9

[GLK12] Manuel Gomez-Rodriguez  Jure Leskovec  and Andreas Krause. Inferring networks of diffusion

and inﬂuence. ACM Transactions on Knowledge Discovery from Data  5(4):21  2012.

[HK16] Xinran He and David Kempe. Robust inﬂuence maximization. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  San Francisco  CA 
USA  August 13-17  2016  pages 885–894  2016.

[HO15] Jean Honorio and Luis Ortiz. Learning the structure and parameters of large-population graphical

games from behavioral data. Journal of Machine Learning Research  16:1157–1210  2015.

[HS15] Thibaut Horel and Yaron Singer. Scalable methods for adaptively seeding a social network. In
Proceedings of the 24th International Conference on World Wide Web  WWW 2015  Florence  Italy 
May 18-22  2015  pages 441–451  2015.

[HS16] Thibaut Horel and Yaron Singer. Maximization of approximately submodular functions.

In
Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages 3045–3053  2016.

[HS17] Avinatan Hassidim and Yaron Singer. Submodular maximization under noise. In COLT  2017.

[HXKL16] Xinran He  Ke Xu  David Kempe  and Yan Liu. Learning inﬂuence functions from incomplete
observations. In Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages
2065–2073  2016.

[KKT03] David Kempe  Jon M. Kleinberg  and Éva Tardos. Maximizing the spread of inﬂuence through a

social network. In KDD  2003.

[KKT05] David Kempe  Jon M. Kleinberg  and Éva Tardos. Inﬂuential nodes in a diffusion model for social
networks. In Automata  Languages and Programming  32nd International Colloquium  ICALP
2005  Lisbon  Portugal  July 11-15  2005  Proceedings  pages 1127–1138  2005.

[LK03] David Liben-Nowell and Jon M. Kleinberg. The link prediction problem for social networks. In

CIKM  2003.

[LK15] Jure Leskovec and Andrej Krevl. Snap datasets  stanford large network dataset collection. 2015.

[LMF+07] Jure Leskovec  Mary McGlohon  Christos Faloutsos  Natalie S. Glance  and Matthew Hurst.

Patterns of cascading behavior in large blog graphs. In SDM  2007.

[MR07] Elchanan Mossel and Sébastien Roch. On the submodularity of inﬂuence in social networks. In

STOC  2007.

[NPS15] Harikrishna Narasimhan  David C. Parkes  and Yaron Singer. Learnability of inﬂuence in net-
works. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural
Information Processing Systems 2015  December 7-12  2015  Montreal  Quebec  Canada  pages
3186–3194  2015.

[NS12] Praneeth Netrapalli and Sujay Sanghavi. Learning the graph of epidemic cascades. In SIGMET-

RICS/Performance  2012.

[RD02] Matthew Richardson and Pedro Domingos. Mining knowledge-sharing sites for viral marketing.

In KDD  2002.

[SS13] Lior Seeman and Yaron Singer. Adaptive seeding in social networks. In 54th Annual IEEE
Symposium on Foundations of Computer Science  FOCS 2013  26-29 October  2013  Berkeley  CA 
USA  pages 459–468  2013.

10

,Eric Balkanski
Nicole Immorlica
Yaron Singer