2013,A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables,We address the problem of learning a sparse Bayesian network  structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure  must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure that satisfies the DAG constraint in the second stage. Although this approach is effective in a low-dimensional setting  it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting.  In this paper  we propose a single-stage method  called A* lasso  that recovers the optimal  sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efficiency of the well-known exact methods  based on dynamic programming. We also present a heuristic scheme that further improves the efficiency of A* lasso without significantly compromising the quality of solutions and   demonstrate this on benchmark Bayesian networks and real data.,A* Lasso for Learning a Sparse Bayesian Network

Structure for Continuous Variables

Jing Xiang

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA 15213
jingx@cs.cmu.edu

Seyoung Kim

Lane Center for Computational Biology

Carnegie Mellon University

Pittsburgh  PA 15213

sssykim@cs.cmu.edu

Abstract

We address the problem of learning a sparse Bayesian network structure for con-
tinuous variables in a high-dimensional space. The constraint that the estimated
Bayesian network structure must be a directed acyclic graph (DAG) makes the
problem challenging because of the huge search space of network structures. Most
previous methods were based on a two-stage approach that prunes the search
space in the ﬁrst stage and then searches for a network structure satisfying the
DAG constraint in the second stage. Although this approach is effective in a low-
dimensional setting  it is difﬁcult to ensure that the correct network structure is not
pruned in the ﬁrst stage in a high-dimensional setting. In this paper  we propose
a single-stage method  called A* lasso  that recovers the optimal sparse Bayesian
network structure by solving a single optimization problem with A* search algo-
rithm that uses lasso in its scoring system. Our approach substantially improves
the computational efﬁciency of the well-known exact methods based on dynamic
programming. We also present a heuristic scheme that further improves the ef-
ﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions.
We demonstrate our approach on data simulated from benchmark Bayesian net-
works and real data.

1

Introduction

Bayesian networks have been popular tools for representing the probability distribution over a large
number of variables. However  learning a Bayesian network structure from data has been known
to be an NP-hard problem [1] because of the constraint that the network structure has to be a di-
rected acyclic graph (DAG). Many of the exact methods that have been developed for recovering the
optimal structure are computationally expensive and require exponential computation time [15  7].
Approximate methods based on heuristic search are more computationally efﬁcient  but they recover
a suboptimal structure. In this paper  we address the problem of learning a Bayesian network struc-
ture for continuous variables in a high-dimensional space and propose an algorithm that recovers the
exact solution with less computation time than the previous exact algorithms  and with the ﬂexibility
of further reducing computation time without a signiﬁcant decrease in accuracy.
Many of the existing algorithms are based on scoring each candidate graph and ﬁnding a graph with
the best score  where the score decomposes for each variable given its parents in a DAG. Although
methods may differ in the scoring method that they use (e.g.  MDL [9]  BIC [14]  and BDe [4]) 
most of these algorithms  whether exact methods or heuristic search techniques  have a two-stage
learning process. In Stage 1  candidate parent sets for each node are identiﬁed while ignoring the
DAG constraint. Then  Stage 2 employs various algorithms to search for the best-scoring network
structure that satisﬁes the DAG constraint by limiting the search space to the candidate parent sets
from Stage 1. For Stage 1  methods such as sparse candidate [2]  max-min parents children [17]  and

1

total conditioning [11] algorithms have been previously proposed. For Stage 2  exact methods based
on dynamic programming [7  15] and A* search algorithm [19] as well as inexact methods such as
heuristic search technique [17] and linear programming formulation [6] have been developed. These
approaches have been developed primarily for discrete variables  and regardless of whether exact or
inexact methods are used in Stage 2  Stage 1 involved exponential computation time and space.
For continuous variables  L1-regularized Markov blanket (L1MB) [13] was proposed as a two-stage
method that uses lasso to select candidate parents for each variable in Stage 1 and performs heuristic
search for DAG structure and variable ordering in Stage 2. Although a two-stage approach can
reduce the search space by pruning candidate parent sets in Stage 1  Huang et al. [5] observed that
applying lasso in Stage 1 as in L1MB is likely to miss the true parents in a high-dimensional setting 
thereby limiting the quality of the solution in Stage 2. They proposed the sparse Bayesian network
(SBN) algorithm that formulates the problem of Bayesian network structure learning as a single-
stage optimization problem and transforms it into a lasso-type optimization to obtain an approximate
solution. Then  they applied a heuristic search to reﬁne the solution as a post-processing step.
In this paper  we propose a new algorithm  called A* lasso  for learning a sparse Bayesian net-
work structure with continuous variables in high-dimensional space. Our method is a single-stage
algorithm that ﬁnds the optimal network structure with a sparse set of parents while ensuring the
DAG constraint is satisﬁed. We ﬁrst show that a lasso-based scoring method can be incorporated
within dynamic programming (DP). While previous approaches based on DP required identifying
the exponential number of candidate parent sets and their scores for each variable in Stage 1 before
applying DP in Stage 2 [7  15]  our approach effectively combines the score computation in Stage
1 within Stage 2 via lasso optimization. Then  we present A* lasso which signiﬁcantly prunes the
search space of DP by incorporating the A* search algorithm [12]  while guaranteeing the optimality
of the solution. Since in practice  A* search can still be expensive compared to heuristic methods 
we explore heuristic schemes that further limit the search space of A* lasso. We demonstrate in
our experiments that this heuristic approach can substantially improve the computation time without
signiﬁcantly compromising the quality of the solution  especially on large Bayesian networks.

2 Background on Bayesian Network Structure Learning

A Bayesian network is a probabilistic graphical model deﬁned over a DAG G with a set of p =
(cid:81)p
|V | nodes V = {v1  . . .   vp}  where each node vj is associated with a random variable Xj [8].
The probability model associated with G in a Bayesian network factorizes as p(X1  . . .   Xp) =
j=1 p(Xj|Pa(Xj))  where p(Xj|Pa(Xj)) is the conditional probability distribution for Xj given
its parents Pa(Xj) with directed edges from each node in Pa(Xj) to Xj in G. We assume continuous
random variables and use a linear regression model for the conditional probability distribution of
each node Xj = Pa(Xj)(cid:48)βj +   where βj = {βjk’s for Xk ∈ Pa(Xj)} is the vector of unknown
parameters to be estimated from data and  is the noise distributed as N (0  1).
Given a dataset X = [x1  . . .   xp]  where xj is a vector of n observations for random variable Xj 
our goal is to estimate the graph structure G and the parameters βj’s jointly. We formulate this
problem as that of obtaining a sparse estimate of βj’s  under the constraint that the overall graph
structure G should not contain directed cycles. Then  the nonzero elements of βj’s indicate the
presence of edges in G. We obtain an estimate of Bayesian network structure and parameters by
minimizing the negative log likelihood of data with sparsity enforcing L1 penalty as follows:

p(cid:88)

min

β1 ... βp

p(cid:88)

(cid:107) xj − x−j

(cid:48)βj (cid:107)2

2 +λ

(cid:107) βj (cid:107)1

s.t. G ∈ DAG 

(1)

j=1

j=1

where x−j represents all columns of X excluding xj  assuming all other variables are candidate
parents of node vj. Given the estimate of βj’s  the set of parents for node vj can be found as the
support of βj  S(βj) = {vi|βji (cid:54)= 0}. The λ is the regularization parameter that determines the
amount of sparsity in βj’s and can be determined by cross-validation. We notice that if the acyclicity
constraint is ignored  Equation (1) decomposes into individual lasso estimations for each node:

LassoScore(vj|V \vj) = min

βj

(cid:107) xj − x−j

(cid:48)βj (cid:107)2

2 +λ (cid:107) βj (cid:107)1 

2

where V \vj represents the set of all nodes in V excluding vj. The above lasso optimization problem
can be solved efﬁciently with the shooting algorithm [3]. However  the main challenge in optimizing
Equation (1) arises from ensuring that the βj’s satisfy the DAG constraint.

3 A* Lasso for Bayesian Network Structure Learning

3.1 Dynamic Programming with Lasso

The problem of learning a Bayesian network structure that satisﬁes
the constraint of no directed cycles can be cast as that of learning an
optimal ordering of variables [8]. Once the optimal variable ordering
is given  the constraint of no directed cycles can be trivially enforced
by constraining the parents of each variable in the local conditional
probability distribution to be a subset of the nodes that precede the
1   . . .   πV|V |] denote an
given node in the ordering. We let ΠV = [πV
indicates the node v ∈ V in
ordering of the nodes in V   where πV
j
the jth position of the ordering  and ΠV≺vj denote the set of nodes in
V that precede node vj in ordering ΠV .
Figure 1: Search space of
Algorithms based on DP have been developed to learn the optimal
variable ordering for three
variables V = {v1  v2  v3}.
variable ordering for Bayesian networks [16]. These approaches are
based on the observation that the score of the optimal ordering of the
full set of nodes V can be decomposed into (a) the optimal score for the ﬁrst node in the ordering 
given a choice of the ﬁrst node and (b) the score of the optimal ordering of the nodes excluding the
ﬁrst node. The optimal variable ordering can be constructed by recursively applying this decompo-
sition to select the ﬁrst node in the ordering and to ﬁnd the optimal ordering of the set of remaining
nodes U ⊂ V . This recursion is given as follows  with an initial call of the recursion with U = V :
(2)

OptScore(U ) = min
vj∈U
πU
1 = argmin

vj∈U

OptScore(U\vj) + BestScore(vj|V \U )
OptScore(U\vj) + BestScore(vj|V \U ) 

(3)

where BestScore(vj|V \U ) is the optimal score of vj under the optimal choice of parents from V \U.
In order to obtain BestScore(vj|V \U ) in Equations (2) and (3)  for the case of discrete variables 
many previous approaches enumerated all possible subsets of V as candidate sets of parents for node
vj to precompute BestScore(vj|V \U ) in Stage 1 before applying DP in Stage 2 [7  15]. While this
approach may perform well in a low-dimensional setting  in a high-dimensional setting  a two-stage
method is likely to miss the true parent sets in Stage 1  which in turn affects the performance of Stage
2 [5]. In this paper  we consider the high-dimensional setting and present a single-stage method that
applies lasso to obtain BestScore(vj|V \U ) within DP as follows:

BestScore(vj|V \U ) = LassoScore(vj|V \U )

=

min

βj  S(βj )⊆V \U

(cid:107) xj − x−j

(cid:48)βj (cid:107)2

2 +λ (cid:107) βj (cid:107)1 .

The constraint S(βj) ⊆ V \U in the above lasso optimization can be trivially maintained by setting
the βjk for vk ∈ U to 0 and optimizing only for the other βjk’s. When applying the recursion in
Equations (2) and (3)  DP takes advantage of the overlapping subproblems to prune the search space
of orderings  since the problem of computing OptScore(U) for U ⊆ V can appear as a subproblem
of scoring orderings of any larger subsets of V that contain U.
The problem of ﬁnding the optimal variable ordering can be viewed as that of ﬁnding the shortest
path from the start state to the goal state in a search space given as a subset lattice. The search
space consists of a set of states  each of which is associated with one of the 2|V | possible subsets
of nodes in V . The start state is the empty set {} and the goal state is the set of all variables V . A
valid move in this search space is deﬁned from a state for subset Qs to another state for subset Qs(cid:48) 
only if Qs(cid:48) contains one additional node to Qs. Each move to the next state corresponds to adding a
node at the end of the ordering of the nodes in the previous state. The cost of such a move is given
by BestScore(v|Qs)  where v = Qs(cid:48)\Qs. Each path from the start state to the goal state gives one

3

{υ1 υ2} {υ3} {υ2} {} {υ1} {υ1 υ3} {υ2 υ3} {υ1 υ2 υ3} possible ordering of nodes. Figure 1 illustrates the search space  where each state is associated with
a Qs. DP ﬁnds the shortest path from the start state to the goal state that corresponds to the optimal
variable ordering by considering all possible paths in this search space and visiting all 2|V | states.

3.2 A* Lasso for Pruning Search Space
As discussed in the previous section  DP considers all 2|V | states in the subset lattice to ﬁnd the
optimal variable ordering. Thus  it is not sufﬁciently efﬁcient to be practical for problems with
more than 20 nodes. On the other hand  a greedy algorithm is computationally efﬁcient because
it explores a single variable ordering by greedily selecting the most promising next state based on
BestScore(v|Qs)  but it returns a suboptimal solution.
In this paper  we propose A* lasso that
incorporates the A* search algorithm [12] to construct the optimal variable ordering in the search
space of the subset lattice. We show that this strategy can signiﬁcantly prune the search space
compared to DP  while maintaining the optimality of the solution.
When selecting the next move in the process of constructing a path in the search space  instead of
greedily selecting the move  A* search also accounts for the estimate of the future cost given by a
heuristic function h(Qs) that will be incurred to reach the goal state from the candidate next state.
Although the exact future cost is not known until A* search constructs the full path by reaching
the goal state  a reasonable estimate of the future cost can be obtained by ignoring the directed
acyclicity constraint. It is well-known that A* search is guaranteed to ﬁnd the shortest path if the
heuristic function h(Qs) is admissible [12]  meaning that h(Qs) is always an underestimate of the
true cost of reaching the goal state. Below  we describe an admissible heuristic for A* lasso.
While exploring the search space  A* search algorithm assigns a score f (Qs) to each state and
its corresponding subset Qs of variables for which the ordering has been determined. A* search
algorithm computes this score f (Qs) as the sum of the cost g(Qs) that has been incurred so far to
reach the current state from the start state and an estimate of the cost h(Qs) that will be incurred to
reach the goal state from the current state:

(4)
More speciﬁcally  given the ordering ΠQs of variables in Qs that has been constructed along the
path from the start state to the state for Qs  the cost that has been incurred so far is deﬁned as

f (Qs) = g(Qs) + h(Qs).

g(Qs) =

LassoScore(vj|ΠQs≺vj )

and the heuristic function for the estimate of the future cost to reach the goal state is deﬁned as:

(cid:88)
(cid:88)

vj∈Qs

(5)

(6)

h(Qs) =

vj∈V \Qs

LassoScore(vj|V \vj)

Note that the heuristic function is admissible  or an underestimate of the true cost  since the con-
straint of no directed cycles is ignored and each variable in V \Qs is free to choose any variables in
V as its parents  which lowers the lasso objective value.
When the search space is a graph where multiple paths can reach the same state  we can further
improve efﬁciency if the heuristic function has the property of consistency in addition to admis-
sibility. A consistent heuristic always satisﬁes h(Qs) ≤ h(Qs(cid:48)) + LassoScore(vk|Qs)  where
LassoScore(vk|Qs) is the cost of moving from state Qs to state Qs(cid:48) with {vk} = Qs(cid:48)\Qs. Consis-
tency ensures that the ﬁrst path found by A* search to reach the given state is always the shortest
path to that state [12]. This allows us to prune the search when we reach the same state via a different
path later in the search. The following proposition states that our heuristic function is consistent.
Proposition 1 The heuristic in Equation (6) is consistent.
Proof For any successor state Qs(cid:48) of Qs  let vk = Qs(cid:48)\Qs.

(cid:88)
(cid:88)

vj∈V \Qs

h(Qs) =

=

LassoScore(vj|V \vj)

vj∈V \Qs vj(cid:54)=vk

≤ h(Qs(cid:48)) + LassoScore(vk|Qs) 

4

LassoScore(vj|V \vj) + LassoScore(vk|V \vk)

Compute LassoScore(v|Qs) with lasso shooting algorithm;
g(Qs(cid:48) ) ← g(Qs) + LassoScore(v|Qs);
h(Qs(cid:48) ) ← h(Qs) − LassoScore(v|V \v);
f (Qs(cid:48) ) ← g(Qs(cid:48) ) + h(Qs(cid:48) );
ΠQs(cid:48) ← [ΠQs   v];
OPEN.insert(L = (Qs(cid:48)   f (Qs(cid:48) )  g(Qs(cid:48) )  ΠQs(cid:48) ));
CLOSED ← CLOSED ∪{Qs(cid:48)};

end

end

end

: X  V   λ

Input
Output: Optimal variable ordering ΠV
Initialize OPEN to an empty queue;
Initialize CLOSED to an empty set;
Compute LassoScore(vj|V \vj) for all vj ∈ V ;
OPEN.insert((Qs = {}  f (Qs) = h({})  g(Qs) = 0  ΠQs = [ ]));
while true do

(Qs  f (Qs)  g(Qs)  ΠQs ) ← OPEN.pop();
if h(Qs) = 0 then

Return ΠV ← ΠQs;
end
foreach v ∈ V \Qs do
Qs(cid:48) ← Qs ∪ {v};
if Qs(cid:48) /∈ CLOSED then

Algorithm 1: A* lasso for learning Bayesian network structure

where LassoScore(vk|Qs) is the true cost of moving from state Qs to Qs(cid:48). The inequal-
ity above holds because vk has fewer parents to choose from in LassoScore(vk|Qs) than in
LassoScore(vk|V \vk). Thus  our heuristic in Equation (6) is consistent.
Given a consistent heuristic  many paths that go through the same state can be pruned by maintaining
an OPEN list and a CLOSED list during A* search. In practice  the OPEN list can be implemented
with a priority queue and the CLOSED list can be implemented with a hash table. The OPEN list is
a priority queue that maintains all the intermediate results (Qs  f (Qs)  g(Qs)  ΠQs )’s for a partial
construction of the variable ordering up to Qs at the frontier of the search  sorted according to the
score f (Qs). During search  A* lasso pops from the OPEN list the partial construction of ordering
with the lowest score f (Qs)  visits the successor states by adding another node to the ordering ΠQs 
and queues the results onto the OPEN list. Any state that has been popped by A* lasso is placed
in the CLOSED list. The states that have been placed in the CLOSED list are not considered again 
even if A* search reaches these states through different paths later in the search.
The full algorithm for A* lasso is given in Algorithm 1. As in DP with lasso  A* lasso is a single-
stage algorithm that solves lasso within A* search. Every time A* lasso moves from state Qs to
the next state Qs(cid:48) in the search space  LassoScore(vj|ΠQs≺vj ) for {vj} = Qs(cid:48)\Qs is computed with
the shooting algorithm and added to g(Qs) to obtain g(Qs(cid:48)). The heuristic score h(Qs(cid:48)) can be
precomputed as LassoScore(vj|V \vj) for all vj ∈ V for a simple look-up during A* search.
3.3 Heuristic Schemes for A* Lasso to Improve Scalability
Although A* lasso substantially prunes the search space compared to DP  it is not sufﬁciently efﬁ-
cient for large graphs  because it still considers a large number of states in the exponentially large
search space. One simple strategy for further pruning the search space would be to limit the size of
the priority queue in the OPEN list  forcing A* lasso to discard less promising intermediate results
ﬁrst. In this case  limiting the queue size to one is equivalent to a greedy algorithm with a scoring
function in Equation (4). In our experiments  we found that such a naive strategy substantially re-
duced the quality of solutions because the best-scoring intermediate results tend to be the results at
the early stage of the exploration. They are at the shallow part of the search space near the start state
because the admissible heuristic underestimates the true cost.
Instead  given a limited queue size  we propose to distribute the intermediate results to be discarded
across different depths/layers of the search space. For example  given the depth of the search space

5

Table 1: Comparison of computation time of different methods

Dataset (Nodes) DP
0.20 (64)
Dsep (6)
1.07 (256)
Asia (8)
Bowling (9)
2.42 (512)
Inversetree (11) 8.44 (2048)
Rain (14)
Cloud (16)
Funnel (18)
Galaxy (20)
Factor (27)
Insurance (27)
Water (32)
Mildew (35)
Alarm (37)
Barley (48)
Hailﬁnder (56)

A* lasso
0.14 (15)
0.26 (34)
0.48 (94)
1.68 (410)

– (–)
– (–)
– (–)
– (–)
– (–)
– (–)
– (–)

– (–)
– (–)
– (–)
1.8 (423)
13.97 (461)
1216 (1.60e4) 76.64 (2938) 64.38 (1811)
26.16 (526)
1.6e4 (6.6e4) 137.36 (2660) 108.39 (1945)
25.19 (513)
4.2e4 (2.6e5) 1527.0 (2.3e4) 88.87 (2310)
27.59 (642)
1.3e5 (1.0e6) 2.40e4 (8.2e4) 110.05 (3093)
125.91 (801)
1389.7 (3912)
– (–)
442.65 (720)
2874.2 (3448)
– (–)
301.67 (687)
2397.0 (3442)
– (–)
802.76 (715)
– (–)
3928.8 (3737)
2732.3 (3426)
– (–)
384.87 (738)
10766.0 (4072) 1869.4 (807)
– (–)
– (–)
9752.0 (3939)
2580.5 (816)
Table 2: A* lasso computation time under different edge strengths βj’s

A* Qlimit 1000 A* Qlimit 200 A* Qlimit 100 A* Qlimit 5 L1MB SBN
8.76
– (–)
2.65
0.17 (11)
– (–)
8.9
2.79
0.22 (12)
8.75
– (–)
2.85
0.23 (13)
3.03
– (–)
8.56
0.2 (16)
12.26 10.19
1.67 (17)
14.56
4.72
2.14 (19)
4.76
10.08
2.73 (21)
11.0
6.59
3.03 (23)
9.04
13.91
3.96 (30)
10.96 29.45
16.31 (33)
32.73 14.96
12.14 (38)
15.25 116.33
29.3 (36)
12.42 (42)
39.78
7.91
109.14 (52) 23.25 483.33
112.61 (57) 44.36 826.41

– (–)
– (–)
– (–)
1.16 (248)
7.88 (270)
9.92 (244)
11.53 (248)
12.02 (323)
59.92 (397)
202.9 (395)
130.71 (343)
339.04 (368)
158.0 (378)
913.46 (430)
1058.3 (390)

Dataset (Nodes)
Dsep (6)
Asia (8)
Bowling (9)
Inversetree (11)
Rain (14)
Cloud (16 )
Funnel (18)
Galaxy (20)

(1.2 1.5)
0.14 (15)
0.26 (34)
0.48 (94)
1.68 (410)
76.64 (2938)
137.36 (2660)
1526.7 (22930)
24040 (82132)

(1 1.2)
0.14 (16)
0.23 (37)
0.49 (103)
2.09 (561)
66.93 (2959)
229.12 (7805)
2060.2 (33271)
66710 (168492)

(0.8 1)
0.17 (30)
0.29 (59)
0.54 (128)
2.25 (620)
97.26 (4069)
227.43 (8858)
3744.4 (40644)
256490 (220821)

|V |  if we need to discard k intermediate results  we discard k/|V | intermediate results at each depth.
In our experiments  we found that this heuristic scheme substantially improves the computation time
of A* lasso with a small reduction in the quality of the solution. We also considered other strategies
such as inﬂating heuristics [10] and pruning edges in preprocessing with lasso  but such strategies
substantially reduced the quality of solutions.

4 Experiments
4.1 Simulation Study

We perform simulation studies in order to evaluate the accuracy of the estimated structures and
measure the computation time of our method. We created several small networks under 20 nodes and
obtained the structure of several benchmark networks between 27 and 56 nodes from the Bayesian
Network Repository (the left-most column in Table 1). In addition  we used the tiling technique [18]
to generate two networks of approximately 300 nodes so that we could evaluate our method on
larger graphs. Given the Bayesian network structures  we set the parameters βj for each conditional
probability distribution of node vj such that βjk ∼ ±U nif orm[l  u] for predetermined values for u
and l if node vk is a parent of node vj and βjk = 0 otherwise. We then generated data from each
Bayesian network by forward sampling with noise  ∼ N (0  1) in the regression model  given the
true variable ordering. All data were mean-centered.
We compare our method to several other methods including DP with lasso for an exact method 
L1MB for heuristic search  and SBN for an optimization-based approximate method. We down-
loaded the software implementations of L1MB and SBN from the authors’ website. For L1MB 
we increased the authors’ recommended number of evaluations 2500 to 10 000 in Stage 2 heuristic
search for all networks except the two larger networks of around 300 nodes (Alarm 2 and Hailﬁnder
2)  where we used two different settings of 50 000 and 100 000 evaluations. We also evaluated A*
lasso with the heuristic scheme with the queue sizes of 5  100  200  and 1000.
DP  A* lasso  and A* lasso with a limited queue size require a selection of the regularization pa-
rameter λ with cross-validation. In order to determine the optimal value for λ  for different values
of λ  we trained a model on a training set  performed an ordinary least squares re-estimation of the
non-zero elements of βj to remove the bias introduced by the L1 penalty  and computed prediction
errors on the validation set. Then  we selected the value of λ that gives the smallest prediction error
as the optimal λ. We used a training set of 200 samples for relatively small networks with under

6

Figure 2: Precision/recall curves for the recovery of skeletons of benchmark Bayesian networks.

Figure 3: Precision/recall curves for the recovery of v-structures of benchmark Bayesian networks.
60 nodes and a training set of 500 samples for the two large networks with around 300 nodes. We
used a validation set of 500 samples. For L1MB and SBN  we used a similar strategy to select the
regularization parameters  while mainly following the strategy suggested by the authors and in their
software implementation.
We present the computation time for the different methods in Table 1. For DP  A* lasso  and A* lasso
with limited queue sizes  we also record the number of states visited in the search space in paren-
theses in Table 1. All methods were implemented in Matlab and were run on computers with 2.4
GHz processors. We used a dataset generated from a true model with βjk ∼ ±U nif orm[1.2  1.5].
It can be seen from Table 1 that DP considers all possible states 2|V | in the search space that grows
exponentially with the number of nodes. It is clear that A* lasso visits signiﬁcantly fewer states
than DP  visiting about 10% of the number of states in DP for the funnel and galaxy networks. We
were unable to obtain the computation time for A* lasso and DP for some of the larger graphs in
Table 1 as they required signiﬁcantly more time. Limiting the size of the queue in A* lasso reduces
both the computation time and the number of states visited. For smaller graphs  we do not report the
computation time for A* lasso with limited queue size  since it is identical to the full A* lasso. We
notice that the computation time for A* lasso with a small queue of 5 or 100 is comparable to that
of L1MB and SBN.
In general  we found that the extent of pruning of the search space by A* lasso compared to DP
depends on the strengths of edges (βj values) in the true model. We applied DP and A* lasso to
datasets of 200 samples generated from each of the networks under each of the three settings for the
true edge strengths  ±U nif orm[1.2  1.5]  ±U nif orm[1  1.2]  and ±U nif orm[0.8  1]. As can be
seen from the computation time and the number of states visited by DP and A* lasso in Table 2  as
the strengths of edges increase  the number of states visited by A* lasso and the computation time
tend to decrease. The results in Table 2 indicate that the efﬁciency of A* lasso is affected by the
signal-to-noise ratio.

7

00.5100.51RecallPrecisionFactors  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionAlarm  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionBarley  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionHailfinder  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionInsurance  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionMildew  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionWater  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionAlarm 2  L1MB−5e4L1MB−1e5SBNA*−Qlim=5A*−Qlim=10000.5100.51RecallPrecisionHailfinder 2  L1MB−5e4L1MB−1e5SBNA*−Qlim=5A*−Qlim=10000.5100.51RecallPrecisionFactors  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionAlarm  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionBarley  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionHailfinder  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionInsurance  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionMildew  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionWater  L1MBSBNA*−Qlim=100A*−Qlim=200A*−Qlim=100000.5100.51RecallPrecisionAlarm 2  L1MB−5e4L1MB−1e5SBNA*−Qlim=5A*−Qlim=10000.5100.51RecallPrecisionHailfinder 2  L1MB−5e4L1MB−1e5SBNA*−Qlim=5A*−Qlim=100Figure 4:
Prediction errors
for benchmark Bayesian net-
works.
The x-axis labels
indicate different benchmark
Bayesian networks for 1: Fac-
tors  2: Alarm  3: Barley  4:
Hailﬁnder  5:
Insurance  6:
Mildew  7: Water  8: Alarm 2 
and 9: Hailﬁnder 2.

In order to evaluate the accuracy of the Bayesian network struc-
tures recovered by each method  we make use of the fact that two
Bayesian network structures are indistinguishable if they belong to
the same equivalence class  where an equivalence class is deﬁned
as the set of networks with the same skeleton and v-structures. The
skeleton of a Bayesian network is deﬁned as the edge connectiv-
ities ignoring edge directions and a v-structure is deﬁned as the
local graph structure over three variables  with two variables point-
ing to the other variables (i.e.  A → B ← C). We evaluated the
performance of the different methods by comparing the estimated
network structure with the true network structure in terms of skele-
ton and v-structures and computing the precision and recall.
The precision/recall curves for the skeleton and v-structures of
the models estimated by the different methods are shown in Fig-
ures 2 and 3  respectively. Each curve was obtained as an average
over the results from 30 different datasets for the two large graphs
(Alarm 2 and Hailﬁnder 2) and from 50 different datasets for all
the other Bayesian networks. All data were simulated under the
setting βjk ∼ ±U nif orm[0.4  0.7]. For the benchmark Bayesian
networks  we used A* lasso with different queue sizes  including 100  200  and 1000  whereas for
the two large networks (Alarm 2 and Hailﬁnder 2) that require more computation time  we used A*
lasso with queue size of 5 and 100. As can be seen in Figures 2 and 3  all methods perform relatively
well on identifying the true skeletons  but ﬁnd it signiﬁcantly more challenging to recover the true
v-structures. We ﬁnd that although increasing the size of queues in A* lasso generally improves the
performance  even with smaller queue sizes  A* lasso outperforms L1MB and SBN in most of the
networks. While A* lasso with a limited queue size preforms consistently well on smaller networks 
it signiﬁcantly outperforms the other methods on the larger graphs such as Alarm 2 and Hailﬁnder 2 
even with a queue size of 5 and even when the number of evaluations for L1MB has been increased
to 50 000 and 100 000. This demonstrates that while limiting the queue size in A* lasso will not
guarantee the optimality of the solution  it still reduces the computation time of A* lasso dramati-
cally without substantially compromising the quality of the solution. In addition  we compare the
performance of the different methods in terms of prediction errors on independent test datasets in
Figure 4. We ﬁnd that the prediction errors of A* lasso are consistently lower even with a limited
queue size.
4.2 Analysis of S&P Stock Data
We applied the methods on the daily stock price data of the S&P 500
companies to learn a Bayesian network that models the dependencies
in prices among different stocks. We obtained the stock prices of 125
companies over 1500 time points between Jan 3  2007 and Dec 17  2012.
We estimated a Bayesian network using the ﬁrst 1000 time points with
the different methods  and then computed prediction errors on the last
500 time points. For L1MB  we used two settings for the number of
evaluations  50 000 and 100 000. We applied A* lasso with different
queue limits of 5  100  and 200. The prediction accuracies for the various
methods are shown in Figure 5. Our method obtains lower prediction
errors than the other methods  even with the smaller queue sizes.

Figure 5: Prediction er-
rors for S&P stock price
data.

5 Conclusions
In this paper  we considered the problem of learning a Bayesian network structure and proposed
A* lasso that guarantees the optimality of the solution while reducing the computational time of
the well-known exact methods based on DP. We proposed a simple heuristic scheme that further
improves the computation time but does not signiﬁcantly reduce the quality of the solution.

Acknowledgments

This material is based upon work supported by an NSF CAREER Award No. MCB-1149885  Sloan
Research Fellowship  and Okawa Foundation Research Grant to SK and by a NSERC PGS-D to JX.

8

12345678951015202530NetworkPrediction Error  L1MB−5e4L1MB−1e5L1MBSBNA*−Qlim=5A*−Qlim=100A*−Qlim=200A*−Qlim=1000Prediction Error5.05.25.45.65.86.0L1MB−5e4L1MB−1e5SBNA*−Q5A*−Q100A*−Q200References
[1] David Maxwell Chickering. Learning Bayesian networks is NP-complete. In Learning from

data  pages 121–130. Springer  1996.

[2] Nir Friedman  Iftach Nachman  and Dana Pe´er. Learning Bayesian network structure from
massive datasets: the “Sparse Candidate” algorithm. In Proceedings of the Fifteenth conference
on Uncertainty in Artiﬁcial Intelligence  pages 206–215. Morgan Kaufmann Publishers Inc. 
1999.

[3] Wenjiang J Fu. Penalized regressions: the bridge versus the lasso. Journal of Computational

and Graphical Statistics  7(3):397–416  1998.

[4] David Heckerman  Dan Geiger  and David M Chickering. Learning Bayesian networks: The

combination of knowledge and statistical data. Machine learning  20(3):197–243  1995.

[5] Shuai Huang  Jing Li  Jieping Ye  Adam Fleisher  Kewei Chen  Teresa Wu  and Eric Reiman.
A sparse structure learning algorithm for Gaussian Bayesian network identiﬁcation from
IEEE Transactions on Pattern Analysis and Machine Intelligence 
high-dimensional data.
35(6):1328–1342  2013.

[6] Tommi Jaakkola  David Sontag  Amir Globerson  and Marina Meila. Learning Bayesian net-
work structure using LP relaxations. In Proceedings of the Thirteenth International Conference
on Artiﬁcial intelligence and Statistics (AISTATS)  2010.

[7] Mikko Koivisto and Kismat Sood. Exact Bayesian structure discovery in Bayesian networks.

Journal of Machine Learning Research  5:549–573  2004.

[8] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

MIT press  2009.

[9] Wai Lam and Fahiem Bacchus. Learning Bayesian belief networks: An approach based on the

MDL principle. Computational intelligence  10(3):269–293  1994.

[10] Maxim Likhachev  Geoff Gordon  and Sebastian Thrun. ARA*: Anytime A* with provable
bounds on sub-optimality. Advances in Neural Information Processing Systems (NIPS)  16 
2003.

[11] Jean-Philippe Pellet and Andr´e Elisseeff. Using Markov blankets for causal structure learning.

The Journal of Machine Learning Research  9:1295–1342  2008.

[12] Stuart Jonathan Russell  Peter Norvig  John F Canny  Jitendra M Malik  and Douglas D Ed-
wards. Artiﬁcial intelligence: a modern approach  volume 74. Prentice hall Englewood Cliffs 
1995.

[13] Mark Schmidt  Alexandru Niculescu-Mizil  and Kevin Murphy. Learning graphical model
structure using L1-regularization paths. In Proceedings of the National Conference on Artiﬁcial
Intelligence  volume 22  page 1278  2007.

[14] Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics  6(2):461–464 

1978.

[15] Ajit Singh and Andrew Moore. Finding optimal Bayesian networks by dynamic programming.

Technical Report 05-106  School of Computer Science  Carnegie Mellon University  2005.

[16] Marc Teyssier and Daphne Koller. Ordering-based search: A simple and effective algorithm
for learning Bayesian networks. In Proceedings of the Twentieth conference on Uncertainty in
Artiﬁcial Intelligence  pages 584–590  2005.

[17] Ioannis Tsamardinos  Laura E Brown  and Constantin F Aliferis. The max-min hill-climbing

Bayesian network structure learning algorithm. Machine Learning  65(1):31–78  2006.

[18] Ioannis Tsamardinos  Alexander Statnikov  Laura E Brown  and Constantin F Aliferis. Gen-
erating realistic large Bayesian networks by tiling. In the Nineteenth International FLAIRS
conference  pages 592–597  2006.

[19] Changhe Yuan  Brandon Malone  and Xiaojian Wu. Learning optimal Bayesian networks using
A* search. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial
Intelligence  pages 2186–2191. AAAI Press  2011.

9

,Jing Xiang
Seyoung Kim
Eunho Yang
Aurelie Lozano
Pradeep Ravikumar
Kareem Amin
Nan Jiang
Satinder Singh