2014,Provable Tensor Factorization with Missing Data,We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need  to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions  our method can recover a three-mode $n\times n\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5 \log^4 n)$ randomly sampled entries. In the process of proving this result  we solve two challenging sub-problems for tensors with missing data. First  in analyzing the initialization step  we prove a generalization of a celebrated result by Szemer\'edie et al. on the spectrum of random graphs. Next  we prove global convergence of alternating minimization with a good initialization. Simulations suggest that the dependence of the sample size on dimensionality $n$ is indeed tight.,ProvableTensorFactorizationwithMissingDataPrateekJainMicrosoftResearchBangalore Indiaprajain@microsoft.comSewoongOhDept.ofIndustrialandEnterpriseSystemsEngineeringUniversityofIllinoisatUrbana-ChampaignUrbana IL61801swoh@illinois.eduAbstractWestudytheproblemoflow-ranktensorfactorizationinthepresenceofmissingdata.Weaskthefollowingquestion:howmanysampledentriesdoweneed toefﬁcientlyandexactlyreconstructatensorwithalow-rankorthogonaldecomposi-tion?Weproposeanovelalternatingminimizationbasedmethodwhichiterativelyreﬁnesestimatesofthesingularvectors.Weshowthatundercertainstandardas-sumptions ourmethodcanrecoverathree-moden×n×ndimensionalrank-rtensorexactlyfromO(n3/2r5log4n)randomlysampledentries.Intheprocessofprovingthisresult wesolvetwochallengingsub-problemsfortensorswithmissingdata.First inanalyzingtheinitializationstep weproveageneralizationofacelebratedresultbySzemer´edieetal.onthespectrumofrandomgraphs.WeshowthatthisinitializationstepaloneissufﬁcienttoachievetherootmeansquarederrorontheparametersboundedbyC(r2n3/2(logn)4/|Ω|)from|Ω|ob-servedentriesforsomeconstantCindependentofnandr.Next weproveglobalconvergenceofalternatingminimizationwiththisgoodinitialization.Simulationssuggestthatthedependenceofthesamplesizeonthedimensionalitynisindeedtight.1IntroductionSeveralreal-worldapplicationsroutinelyencountermulti-waydatawithstructurewhichcanbemod-eledaslow-ranktensors.Moreover inseveralsettings manyoftheentriesofthetensoraremissing whichmotivatedustostudytheproblemoflow-ranktensorfactorizationwithmissingentries.Forexample whenrecordingelectricalactivitiesofthebrain theelectroencephalography(EEG)signalcanberepresentedasathree-wayarray(temporal spectral andspatialaxis).Oftentimessignalsarelostduetomechanicalfailureorlooseconnection.Givennumerousmotivatingapplications severalmethodshavebeenproposedforthistensorcompletionproblem.However withtheexceptionof2-waytensors(i.e. matrices) theexistingmethodsforhigher-ordertensorsdonothavetheoreticalguaranteesandtypicallysufferfromthecurseoflocalminima.Ingeneral ﬁndingafactorizationofatensorisanNP-hardproblem evenwhenalltheentriesareavailable.However itwasrecentlydiscoveredthatbyrestrictingattentiontoasub-classoftensorssuchaslow-CPrankorthogonaltensors[1]orlow-CPrankincoherent1tensors[2] onecanefﬁ-cientlyﬁndaprovablyapproximatefactorization.Inparticular exactrecoveryofthefactorizationispossibleforatensorwithalow-rankorthogonalCPdecomposition[1].Weaskthequestionofre-coveringsuchaCP-decompositionwhenonlyasmallnumberofentriesarerevealed andshowthatexactreconstructionispossibleevenwhenwedonotobserveanyentryinmostoftheﬁbers.Problemformulation.WestudytensorsthathaveanorthonormalCANDECOMP/PARAFAC(CP)tensordecompositionwithasmallnumberofcomponents.Moreover forsimplicityofnotationand1Thenotionofincoherenceweassumein(2)canbethoughtofasincoherencebetweentheﬁbersandthestandardbasisvectors.1exposition weonlyconsidersymmetricthirdordertensors.Wewouldliketostressthatourtech-niquesgeneralizeseasilytohandlenon-symmetrictensorsaswellashigher-ordertensors.Formally weassumethatthetruetensorThasthethefollowingform:T=rX‘=1σ‘(u‘⊗u‘⊗u‘)∈Rn×n×n (1)withr(cid:28)n u‘∈Rnwithku‘k=1 andu‘’sareorthogonaltoeachother.WeletU∈Rn×rbeatall-orthogonalmatrixwhereu‘’sisthe‘-thcolumnofUandUi⊥Ujfori6=j.Weuse⊗todenotethestandardouterproductsuchthatthe(i j k)-thelementofTisgivenby:Tijk=PaσaUiaUjaUka.Wefurtherassumethattheui’sareunstructured whichisformalizedbythenotionofincoherencecommonlyassumedinmatrixcompletionproblems.Theincoherenceofasymmetrictensorwithorthogonaldecompositionisµ(T)≡maxi∈[n] ‘∈[r]√n|Ui‘| (2)where[n]={1 ... n}isthesetoftheﬁrstnintegers.Tensorcompletionbecomesincreasinglydifﬁcultfortensorswithlargerµ(T) becausethe‘mass’ofthetensorcanbeconcentratedonafewentriesthatmightnotberevealed.Outofn3entriesofT asubsetΩ⊆[n]×[n]×[n]isrevealed.WeusePΩ(·)todenotetheprojectionofamatrixontotherevealedsetsuchthatPΩ(T)ijk=(cid:26)Tijkif(i j k)∈Ω 0otherwise.WewanttorecoverTexactlyusingthegivenentries(PΩ(T)).Weassumethateach(i j k)foralli≤j≤kisincludedinΩwithaﬁxedprobabilityp(sinceTissymmetric weincludeallpermutationsof(i j k)).Thisisequivalenttoﬁxingthetotalnumberofsamples|Ω|andselectingΩuniformlyatrandomoverall(cid:0)n3|Ω|(cid:1)choices.Thegoalistoensureexactrecoverywithhighprobabilityandfor|Ω|thatissub-linearinthenumberofentries(n3).Notations.ForatensorT∈Rn×n×n wedeﬁnealinearmappingusingU∈Rn×masT[U U U]∈Rm×m×msuchthatT[U U U]ijk=Pa b cTabcUaiUbjUck.ThespectralnormofatensoriskTk2=maxkxk=1T[x x x].TheHilbert-Schmidtnorm(Frobeniusnormformatrices)ofatensoriskTkF=(Pi j kT2ijk)1/2.TheEuclideannormofavectoriskuk2=(Piu2i)1/2.WeuseC C0todenoteanypositivenumericalconstantsandtheactualvaluemightchangefromlinetoline.1.1AlgorithmIdeally onewouldliketominimizetherankofatensorthatexplainsallthesampledentries.minimizebTrank(bT)(3)subjecttoTijk=bTijkforall(i j k)∈Ω.However evencomputingtherankofatensorisNP-hardingeneral wheretherankisdeﬁnedastheminimumrforwhichCP-decompositionexists[3].Instead weﬁxtherankofbTbyexplicitlymodelingbTasbT=P‘∈[r]σ‘(u‘⊗u‘⊗u‘) andsolvethefollowingproblem:minimizebT rank(bT)=r(cid:13)(cid:13)(cid:13)PΩ(T)−PΩ(cid:0)bT(cid:1)(cid:13)(cid:13)(cid:13)2F=minimize{σ‘ u‘}‘∈[r](cid:13)(cid:13)(cid:13)PΩ(T)−PΩ(cid:0)X‘∈[r]σ‘(u‘⊗u‘⊗u‘)(cid:1)(cid:13)(cid:13)(cid:13)2F(4)Recently [4 5]showedthatanalternatingminimizationtechniquecanrecoveramatrixwithmissingentriesexactly.Wegeneralizeandmodifythealgorithmforthecaseofhigherordertensorsandstudyitrigorouslyfortensorcompletion.However duetospecialstructureinhigher-ordertensors ouralgorithmaswellasanalysisissigniﬁcantlydifferentthanthematrixcase(seeSection2.2formoredetails).Toperformtheminimization werepeattheouter-loopgettingreﬁnedestimatesforallrcomponents.Intheinner-loop weloopovereachcomponentandsolveforuqwhileﬁxingtheothers{u‘}‘6=q.2Moreprecisely wesetbT=ut+1q⊗uq⊗uq+P‘6=qσ‘u‘⊗u‘⊗u‘in(4)andthenﬁndoptimalut+1qbyminimizingtheleastsquaresobjectivegivenby(4).Thatis eachinneriterationisasimpleleastsquaresproblemovertheknownentries hencecanbeimplementedefﬁcientlyandisalsoembarrassinglyparallel.Algorithm1AlternatingMinimizationforTensorCompletion1:Input:PΩ(T) Ω r τ µ2:Initializewith[(u01 σ1) (u02  σ2) ... (u0r σr)]=RTPM(PΩ(T) r)(RTPMof[1])3:[u1 u2 ... ur]=Threshold([u01 u02 ... u0r] µ)(Clippingschemeof[4])4:forallt=1 2 ... τdo5:/*OUTERLOOP*/6:forallq=1 2 ... rdo7:/*INNERLOOP*/8:ˆut+11=argminut+1qkPΩ(T−ut+1q⊗uq⊗uq−P‘6=qσ‘·u‘⊗u‘⊗u‘)k2F9:σt+1q=kˆuqt+1k210:ut+1q=ˆut+11/kˆut+1qk211:endfor12:[u1 u2 ... ur]←[ut+11 ut+12 ... ut+1r]13:[σ1 σ2 ... σr]←[σt+11 σt+12 ... σt+1r]14:endfor15:Output:bT=Pq∈[r]σq(uq⊗uq⊗uq)Themainnoveltyinourapproachisthatwereﬁneallrcomponentsiterativelyasopposedtothesequentialdeﬂationtechniqueusedbytheexistingmethodsfortensordecomposition(forfullyob-servedtensors).Insequentialdeﬂationmethods components{u1 u2 ... ur}areestimatedse-quentiallyandestimateofsayu2isnotusedtoreﬁneu1.Incontrast ouralgorithmiteratesoverallrestimatesintheinnerloop soastoobtainreﬁnedestimatesforallui’sintheouterloop.Webelievethatsuchatechniquecouldbeappliedtoimprovetheerrorboundsof(fullyobserved)tensordecompositionmethodsaswell.Asourmethodisdirectlysolvinganon-convexproblem itcaneasilygetstuckinlocalminima.Thekeyreasonourapproachcanovercomethecurseoflocalminimaisthatwestartwithaprovablygoodinitialpointwhichisonlyasmalldistanceawayfromtheoptima.Toobtainsuchaninitialestimate wecomputealow-rankapproximationoftheobservedtensorusingRobustTensorPowerMethod(RTPM)[1].RTPMisageneralizationofthewidelyusedpowermethodforcomputingleadingsingularvectorsofamatrixandcanapproximatethelargestsingularvectorsuptothespectralnormofthe“error”tensor.Hence thechallengeistoshowthattheerrortensorhassmallspectralnorm(seeTheorem2.1).Weperformathresholdingstepsimilarto[4](seeLemmaA.4)aftertheRTPMsteptoensurethattheestimateswegetareincoherent.OuranalysisrequiresthesampledentriesΩtobeindependentofthecurrentiteratesui ∀i whichingeneralisnotpossibleasui’sarecomputedusingΩ.Toavoidthisissue wedividethegivensamples(Ω)intoequalr·τpartsrandomlywhereτisthenumberofouterloops(seeAlgorithm1).1.2MainResultTheorem1.1.Consideranyrank-rsymmetrictensorT∈Rn×n×nwithanorthogonalCPdecom-positionin(1)satisfyingµ-incoherenceasdeﬁnedin(2).Foranypositiveε>0 thereexistsapositivenumericalconstantCsuchthatifentriesarerevealedwithprobabilityp≥Cµ6r5σ4max(logn)4log(rkTkF/ε)σ4minn3/2 whereσmax max‘σ‘andσmin min‘σ‘ thenthefollowingholdswithprobabilityatleast1−n−5log2(4√rkTkF/ε):•theproblem(3)hasauniqueoptimalsolution;and•log2(4√rkTkFε)iterationsofAlgorithm1producesanestimatebTs.t.kT−bTkF≤ε.3Theaboveresultcanbegeneralizedtok-modetensorsinastraightforwardmanner whereexactre-coveryisguaranteedif p≥Cµ6r5σ2k−2max(logn)4log(rkTkF/ε)σ4minnk/2.However forsimplicityofnotationsandtoemphasizekeypointsofourproof weonlyfocuson3-modetensorsinSection2.3.WeprovideaproofofTheorem1.1inSection2.Foranincoherent well-conditioned andlow-ranktensorwithµ=O(1)andσmin=Θ(σmax) alternatingminimizationrequiresO(r5n3/2(logn)4)samplestogetwithinanarbitrarilysmallnormalizederror.Thisisavanishingfractionofthetotalnumberofentriesn3.EachstepinthealternatingminimizationrequiresO(r|Ω|)operations hencethealternatingminimizationonlyrequiresO(r|Ω|log(rkTkF/ε))operations.TheinitializationsteprequiresO(rc|Ω|)operationsforsomepositivenumericalconstantcasprovedin[1].Whenr(cid:28)n thecomputationalcomplexityscaleslinearlyinthesamplesizeuptoalogarithmicfactor.Aﬁberinathirdordertensorisann-dimensionalvectordeﬁnedbyﬁxingtwooftheaxesandindexingoverremainingoneaxis.Theabovetheoremimpliesthatamongn2ﬁbersoftheform{T[I ej ek]}j k∈[n] exactrecoveryispossibleevenifonlyO(n3/2(logn)4)ﬁbershavenon-zerosamples thatismostoftheﬁbersarenotsampledatall.Thisshouldbecomparedtothematrixcompletionsettingwhereallﬁbersarerequiredtohaveatleastonesample.However unlikematrices thefundamentallimitofhigherordertensorcompletionisnotknown.BuildingonthepercolationofErd¨os-Ren´yigraphsandthecoupon-collectorsproblem itisknownthatmatrixcompletionhasmultiplerank-rsolutionswhenthesamplesizeislessthanCµrnlogn[6] henceexactrecoveryisimpossible.But suchargumentsdonotgeneralizedirectlytohigherorder;seeSection2.5formorediscussion.Interestingly simulationsinSection1.3suggeststhatforr=O(√n) thesamplecomplexityscalesas(r1/2n3/2logn).Thatis assumingthesamplecomplexityprovidedbysimulationsiscorrect ourresultachievesoptimaldependenceonn(uptologfactors).However thedependencyonrissub-optimal(seeSection2.5foradiscussion).1.3EmpiricalResultsTheorem1.1guaranteesexactrecoverywhenp≥Cr5(logn)4/n3/2.Numericalexperimentsshowthattheaveragerecoveryrateconvergestoauniversalcurveoverα wherep∗=αr1/2lnn/((1−ρ)n3/2)inFigure1.Ourboundistightinitsdependencynuptoapoly-logarithmicfactor butislooseinitsdependencyintherankr.Further itisabletorecovertheoriginalmatrixexactlyevenwhenthefactorsarenotstrictlyorthogonal.WegenerateorthogonalmatricesU=[u1 ... ur]∈Rn×runiformlyatrandomwithn=50andr=3unlessspeciﬁedotherwise.Forarank-rtensorT=Pri=1ui⊗ui⊗ui werandomlyrevealeachentrywithprobabilityp.Atensorisexactlyrecoveredifthenormalizedrootmeansquarederror RMSE=kT−ˆTkF/kTkF islessthan10−72.Varyingnandr weplottherecoveryrateaveragedover100instancesasafunctionofα.ThedegreesoffreedominrepresentingasymmetrictensorisΩ(rn).Henceforlarge rweneednumberofsamplesscalingasr.Hence thecurrentdependenceofp∗=O(√r)canonlyholdforr=O(n).Fornotstrictlyorthogonalfactors thealgorithmisrobust.Amorerobustapproachforﬁndinganinitialguesscouldimprovetheperformancesigniﬁcantly especiallyfornon-orthogonaltensors. 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6 7 8 9n=50n=100n=200α 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6 7 8 9r=2r=3r=4r=5α 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6 7 8 9 10 = 0 = 0.2 = 0.3 = 0.4ρρρραFigure1:Averagerecoveryrateconvergestoauniversalcurveoverαwhenp=αr1/2lnn/((1−ρ)n3/2) whereρ=maxi6=j∈[r]hui ujiandr=O(√n).2AMATLABimplementationofAlgorithm1usedtoruntheexperimentsisavailableathttp://web.engr.illinois.edu/∼swoh/software/optspace.41.4RelatedWorkTensordecompositionandcompletion:TheCPmodelproposedin[7 8 9]isamultidimensionalgeneralizationofsingularvaluedecompositionofmatrices.ComputingtheCPdecompositionin-volvestwosteps:ﬁrstapplyawhiteningoperatortothetensortogetalowerdimensionaltensorwithorthogonalCPdecomposition.Suchawhiteningoperatoronlyexistswhenr≤n.Then applyknownpower-methodtechniquesforexactorthogonalCPdecomposition[1].Weusethisalgorithmaswellastheanalysisfortheinitialstepofouralgorithm.FormotivationandexamplesoforthogonalCPmodelswereferto[10 1].Recently manyheuristicsfortensorcompletionhavebeendevelopedsuchastheweightedleastsquares[11] Gauss-Newton[12] alternatingleast-squares[13 14] tracenormminimization[15].However notheoreticalguaranteesareknownfortheseapproaches.Inadifferentcontext [16]showsthatminimizingaweightedtracenormofﬂattenedtensorprovidesexactrecoveryusingO(rn3/2)samples buteachobservationneedstobeadenserandomprojectionofthetensorasopposedtoobservingjustasingleentry whichisthecaseinthetensorcompletionproblem.In[17] anadaptivesamplingmethodwithanestimationalgorithmwasproposedthatprovablyrecoversak-moderank-rtensorwithO(nrk−0.5µk−1klog(r)).However theestimationalgorithmaswellstheanalysiscruciallyreliesonadaptivesamplinganddoesnotgeneralizetorandomsamples.Relationtomatrixcompletion:Matrixcompletionhasbeenstudiedextensivelyinthelastdecadesincetheseminalpaper[18].Sincethen provableapproacheshavebeendeveloped suchas nuclearnormminimization[18 19] OptSpace[20 21] andAlternatingMinimization[4].However severalaspectsoftensorfactorizationmakesitchallengingtoadoptmatrixcompletionapproachesdirectly.First thereisnonaturalconvexsurrogateofthetensorrankfunctionanddevelopingsuchafunctionisinfactatopicofactiveresearch[22 16].Next evenwhenallentriesarerevealed tensorde-compositionmethodssuchassimultaneouspoweriterationareknowntogetstuckatlocalextrema makingitchallengingtoapplymatrixdecompositionmethodsdirectly.Third fortheinitializationstep thebestlow-rankapproximationofamatrixisuniqueandﬁndingitistrivial.However fortensors ﬁndingthebestlow-rankapproximationisnotoriouslydifﬁcult.Ontheotherhand someaspectsoftensordecompositionmakesitpossibletoprovestrongerresults.Matrixcompletionaimstorecovertheunderlyingmatrixonly sincethefactorsarenotuniquelydeﬁnedduetoinvarianceunderrotations.However fororthogonalCPmodels wecanhopetorecovertheindividualsingularvectorsui’sexactly.Infact Theorem1.1showsthatourmethodindeedrecoverstheindividualsingularvectorsexactly.Spectralanalysisoftensorsandhypergraphs:Theorem2.1andLemma2.2shouldbecomparedtocopiouslineofworkonspectralanalysisofmatrices[23 20] withanimportantmotivationofdevelopingfastalgorithmsforlow-rankmatrixapproximations.Weproveananalogousguaranteeforhigherordertensorsandprovideafastalgorithmforlow-ranktensorapproximation.Theorem2.1isalsoageneralizationofthecelebratedresultofFriedman-Kahn-Szemer´edi[24]andFeige-Ofek[25]onthesecondeigenvalueofrandomgraphs.Weprovideanupperboundthelargestsecondeigenvalueofarandomhypergraph whereeachedgeincludesthreenodesandeachofthe(cid:0)n3(cid:1)edgesisselectedwithprobabilityp.2AnalysisoftheAlternatingMinimizationAlgorithmInthissection weprovideaproofofTheorem1.1andtheproofsketchesoftherequiredmaintechnicaltheorems.WerefertotheAppendixforformalproofsofthetechnicaltheoremsandlemmas.Therearetwokeycomponents:a)theanalysisoftheinitializationstep(Section2.1);andb)theconvergenceofalternatingminimizationgivenasufﬁcientlyaccurateinitialization(Section2.2).WeusethesetwoanalysestoproveTheorem1.1inSection2.3.2.1InitializationAnalysisWeﬁrstshowthat(1/p)PΩ(T)isclosetoTinspectralnorm anduseitboundtheerrorofrobustpowermethodapplieddirectlytoPΩ(T).Thenormalizationby(1/p)compensatesforthefactthatmanyentriesaremissing.Foraproofofthistheorem werefertoAppendixA.5Theorem2.1(Initialization).Forp=α/n3/2satisfyingα≥logn thereexistsapositiveconstantC>0suchthat withprobabilityatleast1−n−5 1Tmaxn3/2pkPΩ(T)−pTk2≤C(logn)2√α (5)whereTmax≡maxi j kTijk andkTk2≡maxkuk=1T[u u u]isthespectralnorm.NoticethatTmaxisthemaximumentryinthetensorTandthefactor1/(Tmaxn3/2p)correspondstonormalizationwiththeworstcasespectralnormofpT sincekpTk2≤Tmaxn3/2pandthemaxi-mumisachievedbyT=Tmax(1⊗1⊗1).ThefollowingtheoremguaranteesthatO(n3/2(logn)2)samplesaresufﬁcienttoensurethatwegetarbitrarilysmallerror.AformalproofisprovidedintheAppendix.Togetherwithananalysisofrobusttensorpowermethod[1 Theorem5.1] thenexterrorboundfollowsfromdirectlysubstituting(5)andusingthefactthatforincoherenttensorsTmax≤σmaxµ(T)3r/n3/2.Noticethattheestimatescanbecomputedefﬁciently requiringonlyO(logr+loglogα)iterations eachiterationrequiringO(αn3/2)operations.Thisisclosetothetimere-quiredtoreadthe|Ω|’αn3/2samples.Onecaveatisthatweneedtorunrobustpowermethodpoly(rlogn)times eachwithfreshrandominitializations.Lemma2.2.Foraµ-incoherenttensorwithorthogonaldecompositionT=Pr‘=1σ∗‘(u∗‘⊗u∗‘⊗u∗‘)∈Rn×n×n thereexistspositivenumericalconstantsC C0suchthatwhenα≥C(σmax/σmin)2r5µ6(logn)4 runningC0(logr+loglogα)iterationsoftherobusttensorpowermethodappliedtoPΩ(T)achievesku∗‘−u0‘k2≤C0σ∗max|σ∗‘|µ3r(logn)2√α |σ∗‘−σ‘||σ∗‘|≤C0σ∗max|σ∗‘|µ3r(logn)2√α forall‘∈[r]withprobabilityatleast1−n−5 whereσ∗max=max‘∈[r]|σ∗‘|andσ∗min=min‘∈[r]|σ∗‘|.2.2AlternatingMinimizationAnalysisWenowprovideconvergenceanalysisforthealternatingminimizationpartofAlgorithm1torecoverrank-rtensorT.Ouranalysisassumesthatkui−u∗ik2≤cσmin/rσmax ∀iwherecisasmallconstant(dependentonrandtheconditionnumberofT).TheabovementionedassumptioncanbesatisﬁedusingourinitializationanalysisandbyassumingΩislarge-enough.Atahigh-level ouranalysisshowsthateachstepofAlgorithm1ensuresgeometricdecayofadistancefunction(speciﬁedbelow)whichis“similar”tomaxjkutj−u∗jk2.Formally letT=Pr‘=1σ∗‘·u∗‘⊗u∗‘⊗u∗‘.WLOG wecanassumethatthatσ∗‘≤1.Also let[U Σ]={(u‘ σ‘) 1≤‘≤r} bethet-thstepiteratesofAlgorithm1.Weassumethatu∗‘ ∀‘areµ-incoherentandu‘ ∀‘are2µ-incoherent.Deﬁne ∆σ‘=|σ‘−σ∗‘|σ∗‘ u‘=u∗‘+d‘ (∆σ‘)t+1=|σt+1‘−σ∗‘|σ∗‘ andut+1‘=u∗‘+dt+1‘.Now deﬁnethefollowingdistancefunction:d∞([U Σ] [U∗ Σ∗])≡max‘(kd‘k2+∆σ‘).Thenexttheoremshowsthatthisdistancefunctiondecreasesgeometricallywithnumberofitera-tionsofAlgorithm1.AproofofthistheoremisprovidedinAppendixB.4.Theorem2.3.Ifd∞([U Σ] [U∗ Σ∗])≤11600rσ∗minσ∗maxanduiis2µ-incoherentforall1≤i≤r thenthereexistsapositiveconstantCsuchthatforp≥Cr2(σ∗max)2µ3log2n(σ∗min)2n3/2wehavew.p.≥1−1n7 d∞([Ut+1 Σt+1] [U∗ Σ∗])≤12d∞([U Σ] [U∗ Σ∗]) where[Ut+1 Σt+1]={(ut+1‘ σt+1‘) 1≤‘≤r}arethe(t+1)-thstepiteratesofAlgorithm1.Moreover eachut+1‘is2µ-incoherentforall‘.6 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 0.0001 0.01 1 0 5 10 15 20 25 30p=0.0025  fit error RMSEp=0.1  fit error RMSEiterationserrorFigure2:Algorithm1exhibitslinearconvergenceuntilmachineprecision.FortheestimatebTtatthet-thiterations theﬁterrorkPΩ(T−bTt)kF/kPΩ(T)kFcloselytracksthenormalizedrootmeansquarederrorkT−bTtkF/kTkF suggestingthatitservesasagoodstoppingcriterion.Notethatournumberofsamplesdependonthenumberofiterationsτ.Butduetolinearconver-gence oursamplecomplexityincreasesonlybyafactoroflog(1/)whereisthedesiredaccu-racy.DifferencefromMatrixAltMin:Here wewouldliketohighlightdifferencesbetweenouranalysisandanalysisofthealternatingminimizationmethodformatrixcompletion(matrixAltMin)[4 5].Inthematrixcase thesingularvectorsu∗i’sneednotbeunique.Hence theanalysisisrequiredtoguaranteeadecayinthesubspacedistancedist(U U∗);typically principalanglebasedsubspacedistanceisusedforanalysis.Incontrast orthonormalu∗i’suniquelydeﬁnethetensorandhenceonecanobtaindistanceboundskui−u∗ik2foreachcomponentuiindividually.Ontheotherotherhand aniterationofthematrixAltMiniteratesoverallthevectorsui 1≤i≤r whereristherankofthecurrentiterateandhencedon’thavetoconsidertheerrorinestimationoftheﬁxedcomponentsU[r]\q={u‘ ∀‘6=q} whichisachallengefortheanalysisofAlgorithm1andrequirescarefuldecompositionandboundsoftheerrorterms.2.3ProofofTheorem1.1LetT=Prq=1σ∗q(u∗q⊗u∗q⊗u∗q).DenotetheinitialestimatesU0=[u01 ... u0r]andσ0=[σ01 ... σ0r]tobetheoutputofrobusttensorpowermethodatstep5ofAlgorithm1.Withachoiceofp≥C(σ∗max)4µ6r4(logn)4/(σ∗min)4n3/2asperourassumption Lemma2.2ensuresthatwehaveku0q−u∗qk≤σ∗min/(4800rσmax)and|σ0q−σ∗q|≤|σ∗q|σ∗min/(4800rσmax)withprobabilityatleast1−n−5.Thisrequiresrunningrobusttensorpowermethodfor(rlogn)crandominitializationsforsomepositiveconstantc eachrequiringO(|Ω|)operationsignoringlogarithmicfactors.Toensurethatwehavesufﬁcientlyincoherentinitialiterate weperformthresholdingproposedin[4].Inparticular wethresholdalltheelementsofu0i(obtainedfromRTPMmethod seeStep3ofAlgorithm1)thatarelarger(inmagnitude)thanµ/√ntobesign(u‘(i))µ√nandthenre-normalizetoobtainui.UsingLemmaA.4 thisprocedureensuresthattheobtainedinitialestimateuisatisﬁesthetwocriteriathatisrequiredbyTheorem2.3:a)kui−u∗ik2≤11600r·σ∗minσ∗max andb)uiis2µ-incoherent.Withthisinitialization Theorem2.3tellsusthatO(log2(4r1/2kTkF/ε)iterations(eachiterationrequiresO(r|Ω|)operations)issufﬁcienttoachieve:kuq−u∗qk2≤ε4r1/2kTkFand|σq−σ∗q|≤|σ∗q|ε4r1/2kTkF forallq∈[r]withprobabilityatleast1−n−7log2(4r1/2kTkF/ε).Thedesiredboundfollowsfromthenextlemmawithachoiceof˜ε=ε/4r1/2kTkF.ForaproofwerefertoAppendixB.6.Lemma2.4.Foranorthogonalrank-rtensorT=Prq=1σ∗q(u∗q⊗u∗q⊗u∗q)andanyrank-rtensorbT=Prq=1σq(uq⊗uq⊗uq)satisfyingku−u∗k2≤˜εand|σ−σ∗|≤|σ∗|˜εforallq∈[r]andforallpositive˜ε>0 wehavekT−bTkF≤4r1/2kTkF˜ε.72.4FundamentallimitandrandomhypergraphsFormatrices itisknownthatexactmatrixcompletionisimpossibleiftheunderlyinggraphisdisconnected.ForErd¨os-Ren´yigraphs whensamplesizeislessthanCµrnlogn noalgorithmcanrecovertheoriginalmatrix[6].However fortensorcompletionandrandomhypergraphs suchasimpleconnectiondoesnotexist.Itisnotknownhowthepropertiesofthehypergraphisrelatedtorecovery.Inthisspirit arank-onethird-ordertensorcompletionhasbeenstudiedinaspeciﬁccontextofMAX-3LINproblems.Consideraseriesoflinearequationsovernbinaryvariablesx=[x1...xn]∈{±1}n.Aninstanceofa3LINproblemconsistsofasetoflinearequationsonGF(2) whereeachequationinvolveexactlythreevariables e.g.x1⊕x2⊕x3=+1 x2⊕x3⊕x4=−1 x3⊕x4⊕x5=+1(6)Weuse−1todenotetrue(or1inGF(2))and+1todenotefalse(or0inGF(2)).Thentheexclusive-oroperationdenotedby⊕istheintegermultiplication.theMAX-3LINproblemistoﬁndasolutionxthatsatisﬁesasmanynumberofequationsaspossible.ThisisanNP-hardproblemingeneral andhencerandominstancesoftheproblemwithaplantedsolutionhasbeenstudied[26].Algorithm1providesaprovableguaranteeforMAX-3LINwithrandomassignments.Corollary2.5.ForrandomMAX-3LINproblemwithaplantedsolution underthehypothesesofTheorem1.1 Algorithm1ﬁndsthecorrectsolutionwithhighprobability.Noticethatthistensorhasincoherenceoneandrankone.ThisimpliesexactreconstructionforP≥C(logn)4/n3/2.Thissigniﬁcantlyimprovesoveramessage-passingapproachtoMAX-3LINin[26] whichisguaranteedtoﬁndtheplantedsolutionforp≥C(loglogn)2/(nlogn).ItwassuggestedthatanewnotionofconnectivitycalledpropagationconnectivityisasufﬁcientconditionforthesolutionofrandomMAX-3LINproblemwithaplantedsolutiontobeunique[26 Proposition2].Precisely itisclaimedthatifthehypergraphcorrespondingtoaninstanceofMAX-3LINispropagationconnected thentheoptimalsolutionforMAX-3LINisuniqueandthereisanefﬁcientalgorithmthatﬁndsit.However theexamplein6ispropagationconnectedbutthereisnouniquesolution:both[1 1 1 −1 −1]and[1 −1 −1 1 −1]satisfytheequations.Hence propagationconnectivityisnotasufﬁcientconditionforuniquenessoftheMAX-3LINsolution.2.5OpenProblemsandFutureDirectionsTensorcompletionfornon-orthogonaldecomposition.Numericalsimulationssuggeststhatnon-orthogonalCPmodelscanberecoveredexactly(withouttheusualwhiteningstep).Itwouldbein-terestingtoanalyzeouralgorithmundernon-orthogonalCPmodel.However wewouldliketopointherethatevenwithfullyobservedtensor exactfactorizationisknownonlyfororthonormaltensors.Now giventhatourmethodguaranteesnotonlycompletionbutalsotensorfactorization(whichisessentialforlargescaleapplications) ourmethodwouldrequireasimilarcondition.Optimaldependenceonr.Thenumericalresultssuggestthethresholdsamplesizescalingas√r.ThisissurprisingsincethedegreesoffreedomindescribingaCPmodelscaleslinearlyinr im-plyingthatthe√rscalingonlyholdsforr=O(√n).Incomparison formatrixcompletionthethresholdscalesasr.Itisimportanttounderstandwhythischangeindependenceinrhappensforhigherordertensors andidentifyhowitdependsonkfork-thordertensorcompletion.Mis-speciﬁedrandµ.Thealgorithmrequirestheknowledgeoftherankrandtheincoherenceµ.Thealgorithmisnotsensitivetotheknowledgeofµ.Infact allthenumericalexperimentsarerunwithoutspecifyingtheincoherence andwithouttheclippingstep.Aninterestingdirectionistounderstandthepriceofmis-speciﬁedrankandtoestimatethetruerankfromdata.References[1]AnandkumarAnima GeRong HsuDaniel M.KakadeSham andMatusTelgarsky.Tensordecompositionsforlearninglatentvariablemodels.CoRR abs/1210.7559 2012.[2]A.Anandkumar R.Ge andM.Janzamin.Guaranteednon-orthogonaltensordecompositionviaalternatingrank-1updates.arXivpreprintarXiv:1402.5180 2014.[3]V.DeSilvaandL.-H.Lim.Tensorrankandtheill-posednessofthebestlow-rankapproxima-tionproblem.SIAMJournalonMatrixAnalysisandApplications 30(3):1084–1127 2008.8[4]P.Jain P.Netrapalli andS.Sanghavi.Low-rankmatrixcompletionusingalternatingmini-mization.InSTOC pages665–674 2013.[5]M.Hardt.Ontheprovableconvergenceofalternatingminimizationformatrixcompletion.arXivpreprintarXiv:1312.0925 2013.[6]E.J.Cand`esandT.Tao.Thepowerofconvexrelaxation:Near-optimalmatrixcompletion.InformationTheory IEEETransactionson 56(5):2053–2080 2010.[7]F.L.Hitchcock.Theexpressionofatensororapolyadicasasumofproducts.1927.[8]JDouglasCarrollandJih-JieChang.Analysisofindividualdifferencesinmultidimen-sionalscalingviaann-waygeneralizationofeckart-youngdecomposition.Psychometrika 35(3):283–319 1970.[9]RichardAHarshman.Foundationsoftheparafacprocedure:modelsandconditionsforanexplanatorymultimodalfactoranalysis.1970.[10]T.ZhangandG.H.Golub.Rank-oneapproximationtohighordertensors.SIAMJournalonMatrixAnalysisandApplications 23(2):534–550 2001.[11]E.Acar D.M.Dunlavy T.G.Kolda andM.Mørup.Scalabletensorfactorizationsforincom-pletedata.ChemometricsandIntelligentLaboratorySystems 106(1):41–56 2011.[12]G.TomasiandR.Bro.Parafacandmissingvalues.ChemometricsandIntelligentLaboratorySystems 75(2):163–180 2005.[13]RasmusBro.Multi-wayanalysisinthefoodindustry:models algorithms andapplications.PhDthesis KøbenhavnsUniversitetKøbenhavnsUniversitet 1998.[14]BWalczakandDLMassart.Dealingwithmissingdata:Parti.ChemometricsandIntelligentLaboratorySystems 58(1):15–27 2001.[15]J.Liu P.Musialski P.Wonka andJ.Ye.Tensorcompletionforestimatingmissingvaluesinvisualdata.PatternAnalysisandMachineIntelligence IEEETrans.on 35(1):208–220 2013.[16]C.Mu B.Huang J.Wright andD.Goldfarb.Squaredeal:Lowerboundsandimprovedrelaxationsfortensorrecovery.arXivpreprintarXiv:1307.5870 2013.[17]A.KrishnamurthyandA.Singh.Low-rankmatrixandtensorcompletionviaadaptivesam-pling.InAdvancesinNeuralInformationProcessingSystems pages836–844 2013.[18]E.J.Cand`esandB.Recht.Exactmatrixcompletionviaconvexoptimization.FoundationsofComputationalMathematics 9(6):717–772 2009.[19]S.NegahbanandM.J.Wainwright.Restrictedstrongconvexityand(weighted)matrixcom-pletion:Optimalboundswithnoise.JournalofMachineLearningResearch 2012.[20]R.H.Keshavan A.Montanari andS.Oh.Matrixcompletionfromafewentries.InformationTheory IEEETransactionson 56(6):2980–2998 2010.[21]R.HKeshavan A.Montanari andS.Oh.Matrixcompletionfromnoisyentries.JournalofMachineLearningResearch 11(2057-2078):1 2010.[22]R.TomiokaandT.Suzuki.Convextensordecompositionviastructuredschattennormregu-larization.InNIPS pages1331–1339 2013.[23]Y.Azar A.Fiat A.Karlin F.McSherry andJ.Saia.Spectralanalysisofdata.InProc.ofthe33rdannualACMsymposiumonTheoryofcomputing pages619–626.ACM 2001.[24]J.Friedman J.Kahn andE.Szemer´edi.Onthesecondeigenvalueinrandomregulargraphs.InProceedingsoftheTwenty-FirstAnnualACMSymposiumonTheoryofComputing pages587–598 Seattle Washington USA may1989.ACM.[25]U.FeigeandE.Ofek.Spectraltechniquesappliedtosparserandomgraphs.RandomStruct.Algorithms 27(2):251–275 2005.[26]R.BerkeandM.Onsj¨o.Propagationconnectivityofrandomhypergraphs.InStochasticAlgo-rithms:FoundationsandApplications pages117–126.Springer 2009.9,Prateek Jain
Sewoong Oh