2016,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit  with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results  offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models  assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques  and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.,A Theoretically Grounded Application of Dropout in

Recurrent Neural Networks

Yarin Gal

University of Cambridge

{yg279 zg201}@cam.ac.uk

Abstract

Zoubin Ghahramani

Recurrent neural networks (RNNs) stand at the forefront of many recent develop-
ments in deep learning. Yet a major difﬁculty with these models is their tendency to
overﬁt  with dropout shown to fail when applied to recurrent layers. Recent results
at the intersection of Bayesian modelling and deep learning offer a Bayesian inter-
pretation of common deep learning techniques such as dropout. This grounding of
dropout in approximate Bayesian inference suggests an extension of the theoretical
results  offering insights into the use of dropout with RNN models. We apply this
new variational inference based dropout technique in LSTM and GRU models 
assessing it on language modelling and sentiment analysis tasks. The new approach
outperforms existing techniques  and to the best of our knowledge improves on the
single model state-of-the-art in language modelling with the Penn Treebank (73.4
test perplexity). This extends our arsenal of variational tools in deep learning.

Introduction

1
Recurrent neural networks (RNNs) are sequence-based models of key importance for natural language
understanding  language generation  video processing  and many other tasks [1–3]. The model’s input
is a sequence of symbols  where at each time step a simple neural network (RNN unit) is applied to a
single symbol  as well as to the network’s output from the previous time step. RNNs are powerful
models  showing superb performance on many tasks  but overﬁt quickly. Lack of regularisation in
RNN models makes it difﬁcult to handle small data  and to avoid overﬁtting researchers often use
early stopping  or small and under-speciﬁed models [4].
Dropout is a popular regularisation technique with deep networks [5  6] where network units are
randomly masked during training (dropped). But the technique has never been applied successfully
to RNNs. Empirical results have led many to believe that noise added to recurrent layers (connections
between RNN units) will be ampliﬁed for long sequences  and drown the signal [4]. Consequently 
existing research has concluded that the technique should be used with the inputs and outputs of the
RNN alone [4  7–10]. But this approach still leads to overﬁtting  as is shown in our experiments.
Recent results at the intersection of Bayesian research and deep learning offer interpretation of
common deep learning techniques through Bayesian eyes [11–16]. This Bayesian view of deep
learning allowed the introduction of new techniques into the ﬁeld  such as methods to obtain principled
uncertainty estimates from deep learning networks [14  17]. Gal and Ghahramani [14] for example
showed that dropout can be interpreted as a variational approximation to the posterior of a Bayesian
neural network (NN). Their variational approximating distribution is a mixture of two Gaussians
with small variances  with the mean of one Gaussian ﬁxed at zero. This grounding of dropout in
approximate Bayesian inference suggests that an extension of the theoretical results might offer
insights into the use of the technique with RNN models.
Here we focus on common RNN models in the ﬁeld (LSTM [18]  GRU [19]) and interpret these
as probabilistic models  i.e. as RNNs with network weights treated as random variables  and with
suitably deﬁned likelihood functions. We then perform approximate variational inference in these
probabilistic Bayesian models (which we will refer to as Variational RNNs). Approximating the
posterior distribution over the weights with a mixture of Gaussians (with one component ﬁxed at
zero and small variances) will lead to a tractable optimisation objective. Optimising this objective is
identical to performing a new variant of dropout in the respective RNNs.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

yt1

yt

yt+1

yt1

yt

yt+1

xt1
(a) Naive dropout RNN

xt

xt+1

xt1

(b) Variational RNN

xt

xt+1

Figure 1: Depiction of the dropout technique following our Bayesian interpretation (right)
compared to the standard technique in the ﬁeld (left). Each square represents an RNN unit  with
horizontal arrows representing time dependence (recurrent connections). Vertical arrows represent
the input and output to each RNN unit. Coloured connections represent dropped-out inputs  with
different colours corresponding to different dropout masks. Dashed lines correspond to standard
connections with no dropout. Current techniques (naive dropout  left) use different masks at different
time steps  with no dropout on the recurrent layers. The proposed technique (Variational RNN  right)
uses the same dropout mask at each time step  including the recurrent layers.
In the new dropout variant  we repeat the same dropout mask at each time step for both inputs  outputs 
and recurrent layers (drop the same network units at each time step). This is in contrast to the existing
ad hoc techniques where different dropout masks are sampled at each time step for the inputs and
outputs alone (no dropout is used with the recurrent connections since the use of different masks
with these connections leads to deteriorated performance). Our method and its relation to existing
techniques is depicted in ﬁgure 1. When used with discrete inputs (i.e. words) we place a distribution
over the word embeddings as well. Dropout in the word-based model corresponds then to randomly
dropping word types in the sentence  and might be interpreted as forcing the model not to rely on
single words for its task.
We next survey related literature and background material  and then formalise our approximate
inference for the Variational RNN  resulting in the dropout variant proposed above. Experimental
results are presented thereafter.

2 Related Research
In the past few years a considerable body of work has been collected demonstrating the negative
effects of a naive application of dropout in RNNs’ recurrent connections. Pachitariu and Sahani [7] 
working with language models  reason that noise added in the recurrent connections of an RNN leads
to model instabilities. Instead  they add noise to the decoding part of the model alone. Bayer et al. [8]
apply a deterministic approximation of dropout (fast dropout) in RNNs. They reason that with dropout 
the RNN’s dynamics change dramatically  and that dropout should be applied to the “non-dynamic”
parts of the model – connections feeding from the hidden layer to the output layer. Pham et al. [9]
assess dropout with handwriting recognition tasks. They conclude that dropout in recurrent layers
disrupts the RNN’s ability to model sequences  and that dropout should be applied to feed-forward
connections and not to recurrent connections. The work by Zaremba  Sutskever  and Vinyals [4] was
developed in parallel to Pham et al. [9]. Zaremba et al. [4] assess the performance of dropout in RNNs
on a wide series of tasks. They show that applying dropout to the non-recurrent connections alone
results in improved performance  and provide (as yet unbeaten) state-of-the-art results in language
modelling on the Penn Treebank. They reason that without dropout only small models were used
in the past in order to avoid overﬁtting  whereas with the application of dropout larger models can
be used  leading to improved results. This work is considered a reference implementation by many
(and we compare to this as a baseline below). Bluche et al. [10] extend on the previous body of work
and perform exploratory analysis of the performance of dropout before  inside  and after the RNN’s
unit. They provide mixed results  not showing signiﬁcant improvement on existing techniques. More
recently  and done in parallel to this work  Moon et al. [20] suggested a new variant of dropout in
RNNs in the speech recognition community. They randomly drop elements in the LSTM’s internal
cell ct and use the same mask at every time step. This is the closest to our proposed approach
(although fundamentally different to the approach we suggest  explained in §4.1)  and we compare to
this variant below as well.

2

Existing approaches are based on an empirical experimentation with different ﬂavours of dropout 
following a process of trial-and-error. These approaches have led many to believe that dropout
cannot be extended to a large number of parameters within the recurrent layers  leaving them with
no regularisation. In contrast to these conclusions  we show that it is possible to derive a variational
inference based variant of dropout which successfully regularises such parameters  by grounding our
approach in recent theoretical research.

3 Background
We review necessary background in Bayesian neural networks and approximate variational inference.
Building on these ideas  in the next section we propose approximate inference in the probabilistic
RNN which will lead to a new variant of dropout.
3.1 Bayesian Neural Networks
Given training inputs X = {x1  . . .   xN} and their corresponding outputs Y = {y1  . . .   yN}  in
Bayesian (parametric) regression we would like to infer parameters ! of a function y = f !(x) that
are likely to have generated our outputs. What parameters are likely to have generated our data?
Following the Bayesian approach we would put some prior distribution over the space of parameters 
p(!). This distribution represents our prior belief as to which parameters are likely to have generated
our data. We further need to deﬁne a likelihood distribution p(y|x  !). For classiﬁcation tasks we
may assume a softmax likelihood 

py = d|x  ! = Categorical exp(f !

d (x))/Xd0

exp(f !

d0 (x))!

or a Gaussian likelihood for regression. Given a dataset X  Y  we then look for the posterior
distribution over the space of parameters: p(!|X  Y). This distribution captures how likely various
function parameters are given our observed data. With it we can predict an output for a new input
point x⇤ by integrating

(1)

(2)

p(y⇤|x⇤  X  Y) =Z p(y⇤|x⇤  !)p(!|X  Y)d!.

One way to deﬁne a distribution over a parametric set of functions is to place a prior distribution over
a neural network’s weights  resulting in a Bayesian NN [21  22]. Given weight matrices Wi and bias
vectors bi for layer i  we often place standard matrix Gaussian prior distributions over the weight
matrices  p(Wi) = N (0  I) and often assume a point estimate for the bias vectors for simplicity.
3.2 Approximate Variational Inference in Bayesian Neural Networks
We are interested in ﬁnding the distribution of weight matrices (parametrising our functions) that have
generated our data. This is the posterior over the weights given our observables X  Y: p(!|X  Y).
This posterior is not tractable in general  and we may use variational inference to approximate it (as
was done in [23–25  12]). We need to deﬁne an approximating variational distribution q(!)  and then
minimise the KL divergence between the approximating distribution and the full posterior:

KLq(!)||p(!|X  Y) / Z q(!) log p(Y|X  !)d! + KL(q(!)||p(!))

= 

NXi=1Z q(!) log p(yi|f !(xi))d! + KL(q(!)||p(!)).

We next extend this approximate variational inference to probabilistic RNNs  and use a q(!) distribu-
tion that will give rise to a new variant of dropout in RNNs.

4 Variational Inference in Recurrent Neural Networks
In this section we will concentrate on simple RNN models for brevity of notation. Derivations for
LSTM and GRU follow similarly. Given input sequence x = [x1  ...  xT ] of length T   a simple RNN
is formed by a repeated application of a function fh. This generates a hidden state ht for time step t:

ht = fh(xt  ht1) = (xtWh + ht1Uh + bh)

for some non-linearity . The model output can be deﬁned  for example  as fy(hT ) = hT Wy + by.
We view this RNN as a probabilistic model by regarding ! = {Wh  Uh  bh  Wy  by} as random

3

h (xT   f !

h (...f !

variables (following normal prior distributions). To make the dependence on ! clear  we write f !
y
for fy and similarly for f !
h . We deﬁne our probabilistic model’s likelihood as above (section 3.1).
The posterior over random variables ! is rather complex  and we use variational inference with
approximating distribution q(!) to approximate it.
Evaluating each sum term in eq. (2) above with our RNN model we get

Z q(!) log p(y|f !

with h0 = 0. We approximate this with Monte Carlo (MC) integration with a single sample:

h (x1  h0)...))◆d!
b! ⇠ q(!)

resulting in an unbiased estimator to each sum term.
This estimator is plugged into equation (2) to obtain our minimisation objective

y (hT ))d! =Z q(!) log p✓yf !
h (xT   hT1)◆d!
yf !
=Z q(!) log p✓yf !
yf !
⇡ log p✓yfb!
h (x1  h0)...))◆ 
yfb!
h (xT   fb!
log p✓yifb!i
h (xi 1  h0)...))◆ + KL(q(!)||p(!)).
NXi=1
y fb!i
h (xi T   fb!i
h bbi
y bbi
h bUi
Note that for each sequence xi we sample a new realisationb!i = {cWi
h cWi
y}  and that
each symbol in the sequence xi = [xi 1  ...  xi T ] is passed through the function fb!i
h with the same
h bbi
h bUi
weight realisationscWi
h used at every time step t  T .
q(wk) = pN (wk; 0  2I) + (1  p)N (wk; mk  2I)

Following [17] we deﬁne our approximating distribution to factorise over the weight matrices and
their rows in !. For every weight matrix row wk the approximating distribution is:

with mk variational parameter (row vector)  p given in advance (the dropout probability)  and small
2. We optimise over mk the variational parameters of the random weight matrices; these correspond
to the RNN’s weight matrices in the standard view1. The KL in eq. (3) can be approximated as L2
regularisation over the variational parameters mk [17].

h (...fb!i

h (...fb!

L⇡ 

(3)

(4)

Evaluating the model output fb!
rows in each weight matrix W during the forward pass – i.e. performing dropout. Our objective L is
identical to that of the standard RNN. In our RNN setting with a sequence input  each weight matrix
row is randomly masked once  and importantly the same mask is used through all time steps.2
Predictions can be approximated by either propagating the mean of each layer to the next (referred to
as the standard dropout approximation)  or by approximating the posterior in eq. (1) with q(!) 

y (·) with sampleb! ⇠ q(!) corresponds to randomly zeroing (masking)

p(y⇤|x⇤  X  Y) ⇡Z p(y⇤|x⇤  !)q(!)d! ⇡

1
K

KXk=1

p(y⇤|x⇤ b!k)

Implementation and Relation to Dropout in RNNs

withb!k ⇠ q(!)  i.e. by performing dropout at test time and averaging results (MC dropout).

4.1
Implementing our approximate inference is identical to implementing dropout in RNNs with the
same network units dropped at each time step  randomly dropping inputs  outputs  and recurrent
connections. This is in contrast to existing techniques  where different network units would be
dropped at different time steps  and no dropout would be applied to the recurrent connections (ﬁg. 1).
Certain RNN models such as LSTMs and GRUs use different gates within the RNN units. For
example  an LSTM is deﬁned using four gates: “input”  “forget”  “output”  and “input modulation” 

i = sigmht1Ui + xtWi
o = sigmht1Uo + xtWo

f = sigmht1Uf + xtWf
g = tanhht1Ug + xtWg

1Graves et al. [26] further factorise the approximating distribution over the elements of each row  and use a
Gaussian approximating distribution with each element (rather than a mixture); the approximating distribution
above seems to give better performance  and has a close relation with dropout [17].

2In appendix A we discuss the relation of our dropout interpretation to the ensembling one.

4

ct = f  ct1 + i  g

(5)
with ! = {Wi  Ui  Wf   Uf   Wo  Uo  Wg  Ug} weight matrices and  the element-wise product.
Here an internal state ct (also referred to as cell) is updated additively.
Alternatively  the model could be re-parametrised as in [26]:

ht = o  tanh(ct)

i
f
o
g

0B@

1CA =0B@

sigm
sigm
sigm
tanh

1CA✓✓ xt
ht1◆ · W◆

with ! = {W}  W a matrix of dimensions 2K by 4K (K being the dimensionality of xt). We name
this parametrisation a tied-weights LSTM (compared to the untied-weights LSTM in eq. (5)).
Even though these two parametrisations result in the same deterministic model  they lead to different
approximating distributions q(!). With the ﬁrst parametrisation one could use different dropout
masks for different gates (even when the same input xt is used). This is because the approximating
distribution is placed over the matrices rather than the inputs: we might drop certain rows in one
weight matrix W applied to xt and different rows in another matrix W0 applied to xt. With the
second parametrisations we would place a distribution over the single matrix W. This leads to a
faster forward-pass  but with slightly diminished results as we will see in the experiments section.
In more concrete terms  we may write our dropout variant with the second parametrisation (eq. (6)) as

(6)

(7)

i
f
o
g

0B@

1CA =0B@

sigm
sigm
sigm
tanh

1CA✓✓ xt  zx
ht1  zh◆ · W◆

x

with zx  zh random masks repeated at all time steps (and similarly for the parametrisation in eq. (5)).
In comparison  Zaremba et al. [4]’s variant replaces zx in eq. (7) with a time-dependent mask: xt  zt
where zt
x is sampled anew every time step (whereas zh is removed and the recurrent connection ht1
is not dropped). On the other hand  Moon et al. [20]’s variant changes eq. (5) by adapting the internal
cell ct = ct  zc with the same mask zc used at all time steps. Note that unlike [20]  by viewing
dropout as an operation over the weights our technique trivially extends to RNNs and GRUs.
4.2 Word Embeddings Dropout
In datasets with continuous inputs we often apply dropout to the input layer – i.e. to the input vector
itself. This is equivalent to placing a distribution over the weight matrix which follows the input and
approximately integrating over it (the matrix is optimised  therefore prone to overﬁtting otherwise).
But for models with discrete inputs such as words (where every word is mapped to a continuous
vector – a word embedding) this is seldom done. With word embeddings the input can be seen as
either the word embedding itself  or  more conveniently  as a “one-hot” encoding (a vector of zeros
with 1 at a single position). The product of the one-hot encoded vector with an embedding matrix
WE 2 RV ⇥D (where D is the embedding dimensionality and V is the number of words in the
vocabulary) then gives a word embedding. Curiously  this parameter layer is the largest layer in most
language applications  yet it is often not regularised. Since the embedding matrix is optimised it can
lead to overﬁtting  and it is therefore desirable to apply dropout to the one-hot encoded vectors. This
in effect is identical to dropping words at random throughout the input sentence  and can also be
interpreted as encouraging the model to not “depend” on single words for its output.
Note that as before  we randomly set rows of the matrix WE 2 RV ⇥D to zero. Since we repeat the
same mask at each time step  we drop the same words throughout the sequence – i.e. we drop word
types at random rather than word tokens (as an example  the sentence “the dog and the cat” might
become “— dog and — cat” or “the — and the cat”  but never “— dog and the cat”). A possible
inefﬁciency implementing this is the requirement to sample V Bernoulli random variables  where
V might be large. This can be solved by the observation that for sequences of length T   at most T
embeddings could be dropped (other dropped embeddings have no effect on the model output). For
T ⌧ V it is therefore more efﬁcient to ﬁrst map the words to the word embeddings  and only then to
zero-out word embeddings based on their word type.

5 Experimental Evaluation
We start by implementing our proposed dropout variant into the Torch implementation of Zaremba
et al. [4]  that has become a reference implementation for many in the ﬁeld. Zaremba et al. [4] have

5

set a benchmark on the Penn Treebank that to the best of our knowledge hasn’t been beaten for
the past 2 years. We improve on [4]’s results  and show that our dropout variant improves model
performance compared to early-stopping and compared to using under-speciﬁed models. We continue
to evaluate our proposed dropout variant with both LSTM and GRU models on a sentiment analysis
task where labelled data is scarce. We ﬁnish by giving an in-depth analysis of the properties of the
proposed method  with code and many experiments deferred to the appendix due to space constraints.
5.1 Language Modelling
We replicate the language modelling experiment of Zaremba  Sutskever  and Vinyals [4]. The
experiment uses the Penn Treebank  a standard benchmark in the ﬁeld. This dataset is considered
a small one in the language processing community  with 887  521 tokens (words) in total  making
overﬁtting a considerable concern. Throughout the experiments we refer to LSTMs with the dropout
technique proposed following our Bayesian interpretation as Variational LSTMs  and refer to existing
dropout techniques as naive dropout LSTMs (different masks at different steps  applied to the input
and output of the LSTM alone). We refer to LSTMs with no dropout as standard LSTMs.
We implemented a Variational LSTM for both the medium model of [4] (2 layers with 650 units in
each layer) as well as their large model (2 layers with 1500 units in each layer). The only changes
we’ve made to [4]’s setting are 1) using our proposed dropout variant instead of naive dropout  and
2) tuning weight decay (which was chosen to be zero in [4]). All other hyper-parameters are kept
identical to [4]: learning rate decay was not tuned for our setting and is used following [4]. Dropout
parameters were optimised with grid search (tying the dropout probability over the embeddings
together with the one over the recurrent layers  and tying the dropout probability for the inputs and
outputs together as well). These are chosen to minimise validation perplexity3. We further compared
to Moon et al. [20] who only drop elements in the LSTM internal state using the same mask at all
time steps (in addition to performing dropout on the inputs and outputs). We implemented their
dropout variant with each model size  and repeated the procedure above to ﬁnd optimal dropout
probabilities (0.3 with the medium model  and 0.5 with the large model). We had to use early stopping
for the large model with [20]’s variant as the model starts overﬁtting after 16 epochs. Moon et al.
[20] proposed their dropout variant within the speech recognition community  where they did not
have to consider embeddings overﬁtting (which  as we will see below  affect the recurrent layers
considerably). We therefore performed an additional experiment using [20]’s variant together with
our embedding dropout (referred to as Moon et al. [20]+emb dropout).
Our results are given in table 1. For the variational LSTM we give results using both the tied weights
model (eq. (6)–(7)  Variational (tied weights))  and without weight tying (eq. (5)  Variational (untied
weights)). For each model we report performance using both the standard dropout approximation
(averaging the weights at test time – propagating the mean of each approximating distribution as input
to the next layer)  and using MC dropout (obtained by performing dropout at test time 1000 times 
and averaging the model outputs following eq. (4)  denoted MC). For each model we report average
perplexity and standard deviation (each experiment was repeated 3 times with different random seeds
and the results were averaged). Model training time is given in words per second (WPS).
It is interesting that using the dropout approximation  weight tying results in lower validation error
and test error than the untied weights model. But with MC dropout the untied weights model performs
much better. Validation perplexity for the large model is improved from [4]’s 82.2 down to 77.3 (with
weight tying)  or 77.9 without weight tying. Test perplexity is reduced from 78.4 down to 73.4 (with
MC dropout and untied weights). To the best of our knowledge  these are currently the best single
model perplexities on the Penn Treebank.
It seems that Moon et al. [20] underperform even compared to [4]. With no embedding dropout the
large model overﬁts and early stopping is required (with no early stopping the model’s validation
perplexity goes up to 131 within 30 epochs). Adding our embedding dropout  the model performs
much better  but still underperforms compared to applying dropout on the inputs and outputs alone.
Comparing our results to the non-regularised LSTM (evaluated with early stopping  giving similar
performance as the early stopping experiment in [4]) we see that for either model size an improvement
can be obtained by using our dropout variant. Comparing the medium sized Variational model to the
large one we see that a signiﬁcant reduction in perplexity can be achieved by using a larger model.
This cannot be done with the non-regularised LSTM  where a larger model leads to worse results.

3Optimal probabilities are 0.3 and 0.5 respectively for the large model  compared [4]’s 0.6 dropout probability 

and 0.2 and 0.35 respectively for the medium model  compared [4]’s 0.5 dropout probability.

6

Medium LSTM

Validation




Large LSTM
Test
127.4
118.7
86.0
78.4

128.3
122.9
88.8
82.2




Moon et al. [20]

Moon et al. [20] +emb dropout

Zaremba et al. [4]

Variational (tied weights)

121.1
100.7
88.9
86.2

Test
121.7
97.0
86.5
82.7

Non-regularized (early stopping)

Variational (tied weights  MC)
Variational (untied weights)

WPS
WPS Validation
2.5K
5.5K
3K
4.8K
3K
4.8K
2.5K
5.5K
4.7K 77.3 ± 0.2 75.0 ± 0.1
2.4K
81.8 ± 0.2 79.7 ± 0.1


74.1 ± 0.0
79.0 ± 0.1
1.6K
2.7K 77.9 ± 0.3 75.2 ± 0.2
81.9 ± 0.2 79.7 ± 0.1
Variational (untied weights  MC)
73.4 ± 0.0 
78.6 ± 0.1 
Table 1: Single model perplexity (on test and validation sets) for the Penn Treebank language
modelling task. Two model sizes are compared (a medium and a large LSTM  following [4]’s setup) 
with number of processed words per second (WPS) reported. Both dropout approximation and MC
dropout are given for the test set with the Variational model. A common approach for regularisation is
to reduce model complexity (necessary with the non-regularised LSTM). With the Variational models
however  a signiﬁcant reduction in perplexity is achieved by using larger models.
This shows that reducing the complexity of the model  a possible approach to avoid overﬁtting 
actually leads to a worse ﬁt when using dropout.
We also see that the tied weights model achieves very close performance to that of the untied weights
one when using the dropout approximation. Assessing model run time though (on a Titan X GPU) 
we see that tying the weights results in a more time-efﬁcient implementation. This is because the
single matrix product is implemented as a single GPU kernel  instead of the four smaller matrix
products used in the untied weights model (where four GPU kernels are called sequentially). Note
though that a low level implementation should give similar run times.
We further experimented with a model averaging experiment following [4]’s setting  where several
large models are trained independently with their outputs averaged. We used Variational LSTMs
with MC dropout following the setup above. Using 10 Variational LSTMs we improve [4]’s test set
perplexity from 69.5 to 68.7 – obtaining identical perplexity to [4]’s experiment with 38 models.
Lastly  we report validation perplexity with reduced learning rate decay (with the medium model).
Learning rate decay is often used for regularisation by setting the optimiser to make smaller steps
when the model starts overﬁtting (as done in [4]). By removing it we can assess the regularisation
effects of dropout alone. As can be seen in ﬁg. 2  even with early stopping  Variational LSTM achieves
lower perplexity than naive dropout LSTM and standard LSTM. Note though that a signiﬁcantly
lower perplexity for all models can be achieved with learning rate decay scheduling as seen in table 1
5.2 Sentiment Analysis
We next evaluate our dropout variant with both LSTM and GRU models on a sentiment analysis task 
where labelled data is scarce. We use MC dropout (which we compare to the dropout approximation
further in appendix B)  and untied weights model parametrisations.
We use the raw Cornell ﬁlm reviews corpus collected by Pang and Lee [27]. The dataset is composed
of 5000 ﬁlm reviews. We extract consecutive segments of T words from each review for T = 200 
and use the corresponding ﬁlm score as the observed output y. The model is built from one embedding
layer (of dimensionality 128)  one LSTM layer (with 128 network units for each gate; GRU setting is
built similarly)  and ﬁnally a fully connected layer applied to the last output of the LSTM (resulting
in a scalar output). We use the Adam optimiser [28] throughout the experiments  with batch size 128 
and MC dropout at test time with 10 samples.

Figure 2: Medium model validation perplexity for the Penn Treebank language modelling task.
Learning rate decay was reduced to assess model overﬁtting using dropout alone. Even with early
stopping  Variational LSTM achieves lower perplexity than naive dropout LSTM and standard LSTM.
Lower perplexity for all models can be achieved with learning rate decay scheduling  seen in table 1.

7

(c) GRU test error: variational 

naive dropout  and standard LSTM.

(b) LSTM test error: variational 
naive dropout  and standard LSTM.

(a) LSTM train error: variational 
naive dropout  and standard LSTM.
Figure 3: Sentiment analysis error for Variational LSTM / GRU compared to naive dropout LSTM /
GRU and standard LSTM / GRU (with no dropout).
The main results can be seen in ﬁg. 3. We compared Variational LSTM (with our dropout variant
applied with each weight layer) to standard techniques in the ﬁeld. Training error is shown in ﬁg. 3a
and test error is shown in ﬁg. 3b. Optimal dropout probabilities and weight decay were used for each
model (see appendix B). It seems that the only model not to overﬁt is the Variational LSTM  which
achieves lowest test error as well. Variational GRU test error is shown in ﬁg. 14 (with loss plot given
in appendix B). Optimal dropout probabilities and weight decay were used again for each model.
Variational GRU avoids overﬁtting to the data and converges to the lowest test error. Early stopping in
this dataset will result in smaller test error though (lowest test error is obtained by the non-regularised
GRU model at the second epoch). It is interesting to note that standard techniques exhibit peculiar
behaviour where test error repeatedly decreases and increases. This behaviour is not observed with
the Variational GRU. Convergence plots of the loss for each model are given in appendix B.
We next explore the effects of dropping-out different parts of the model. We assessed our Variational
LSTM with different combinations of dropout over the embeddings (pE = 0  0.5) and recurrent
layers (pU = 0  0.5) on the sentiment analysis task. The convergence plots can be seen in ﬁgure 4a. It
seems that without both strong embeddings regularisation and strong regularisation over the recurrent
layers the model would overﬁt rather quickly. The behaviour when pU = 0.5 and pE = 0 is quite
interesting: test error decreases and then increases before decreasing again. Also  it seems that when
pU = 0 and pE = 0.5 the model becomes very erratic.
Lastly  we tested the performance of Variational LSTM with different recurrent layer dropout
probabilities  ﬁxing the embedding dropout probability at either pE = 0 or pE = 0.5 (ﬁgs. 4b-4c).
These results are rather intriguing. In this experiment all models have converged  with the loss getting
near zero (not shown). Yet it seems that with no embedding dropout  a higher dropout probability
within the recurrent layers leads to overﬁtting! This presumably happens because of the large number
of parameters in the embedding layer which is not regularised. Regularising the embedding layer with
dropout probability pE = 0.5 we see that a higher recurrent layer dropout probability indeed leads to
increased robustness to overﬁtting  as expected. These results suggest that embedding dropout can be
of crucial importance in some tasks.
In appendix B we assess the importance of weight decay with our dropout variant. Common practice
is to remove weight decay with naive dropout. Our results suggest that weight decay plays an
important role with our variant (it corresponds to our prior belief of the distribution over the weights).

6 Conclusions
We presented a new technique for recurrent neural network regularisation. Our RNN dropout variant
is theoretically motivated and its effectiveness was empirically demonstrated.

(a) Combinations of pE = 0  0.5

with pU = 0  0.5.

(b) pU = 0  ...  0.5 with

ﬁxed pE = 0.

(c) pU = 0  ...  0.5 with

ﬁxed pE = 0.5.

Figure 4: Test error for Variational LSTM with various settings on the sentiment analysis task.
Different dropout probabilities are used with the recurrent layer (pU) and embedding layer (pE).

8

References
[1] Martin Sundermeyer  Ralf Schlüter  and Hermann Ney. LSTM neural networks for language modeling. In

INTERSPEECH  2012.

[2] N Kalchbrenner and P Blunsom. Recurrent continuous translation models. In EMNLP  2013.
[3] Ilya Sutskever  Oriol Vinyals  and Quoc VV Le. Sequence to sequence learning with neural networks. In

NIPS  2014.

[4] Wojciech Zaremba  Ilya Sutskever  and Oriol Vinyals. Recurrent neural network regularization. arXiv

preprint arXiv:1409.2329  2014.

[5] Geoffrey E others Hinton. Improving neural networks by preventing co-adaptation of feature detectors.

arXiv preprint arXiv:1207.0580  2012.

[6] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout:

A simple way to prevent neural networks from overﬁtting. JMLR  2014.

[7] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language models:

when are they needed? arXiv preprint arXiv:1301.5650  2013.

[8] J Bayer et al. On fast dropout and its applicability to recurrent networks. arXiv preprint arXiv:1311.0701 

2013.

[9] Vu Pham  Theodore Bluche  Christopher Kermorvant  and Jerome Louradour. Dropout improves recurrent

neural networks for handwriting recognition. In ICFHR. IEEE  2014.

[10] Théodore Bluche  Christopher Kermorvant  and Jérôme Louradour. Where to apply dropout in recurrent

neural networks for handwriting recognition? In ICDAR. IEEE  2015.

[11] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approxi-

mate inference in deep generative models. In ICML  2014.

[12] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty in neural

network. In ICML  2015.

[13] Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of

Bayesian neural networks. In ICML  2015.

[14] Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate

variational inference. arXiv:1506.02158  2015.

[15] Diederik Kingma  Tim Salimans  and Max Welling. Variational dropout and the local reparameterization

trick. In NIPS. Curran Associates  Inc.  2015.

[16] Anoop Korattikara Balan  Vivek Rathod  Kevin P Murphy  and Max Welling. Bayesian dark knowledge.

In NIPS. Curran Associates  Inc.  2015.

[17] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty

in deep learning. arXiv:1506.02142  2015.

[18] S Hochreiter and J Schmidhuber. Long short-term memory. Neural computation  9(8)  1997.
[19] Kyunghyun Cho et al. Learning phrase representations using RNN encoder–decoder for statistical machine

translation. In EMNLP  Doha  Qatar  October 2014. ACL.

[20] Taesup Moon  Heeyoul Choi  Hoshik Lee  and Inchul Song. RnnDrop: A Novel Dropout for RNNs in

ASR. In ASRU Workshop  December 2015.

[21] David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural computation  4

(3):448–472  1992.

[22] R M Neal. Bayesian learning for neural networks. PhD thesis  University of Toronto  1995.
[23] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description

length of the weights. In COLT  pages 5–13. ACM  1993.

[24] David Barber and Christopher M Bishop. Ensemble learning in Bayesian neural networks. NATO ASI

SERIES F COMPUTER AND SYSTEMS SCIENCES  168:215–238  1998.

[25] Alex Graves. Practical variational inference for neural networks. In NIPS  2011.
[26] Alan Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep recurrent

neural networks. In ICASSP. IEEE  2013.

[27] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with

respect to rating scales. In ACL. ACL  2005.

[28] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[29] James Bergstra et al. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python

for Scientiﬁc Computing Conference (SciPy)  June 2010. Oral Presentation.

[30] fchollet. Keras. https://github.com/fchollet/keras  2015.

9

,Dino Sejdinovic
Arthur Gretton
Wicher Bergsma
Daniel Soudry
Ron Meir
Yarin Gal
Zoubin Ghahramani