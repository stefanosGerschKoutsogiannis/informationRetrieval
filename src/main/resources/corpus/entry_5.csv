2014,(Almost) No Label No Cry,In Learning with Label Proportions (LLP)  the objective is to learn a supervised classifier when  instead of labels  only label proportions for bags of observations are known. This setting has broad practical relevance  in particular for privacy preserving data processing. We first show that the mean operator  a statistic which aggregates all labels  is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then  we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting  introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains  whose size ranges up to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover  in many cases  our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains  half a dozen proportions can suffice  i.e. roughly 40K times less than the total number of labels.,(Almost) No Label No Cry

Giorgio Patrini1 2  Richard Nock1 2  Paul Rivera1 2  Tiberio Caetano1 3 4

Australian National University1  NICTA2  University of New South Wales3  Ambiata4

Sydney  NSW  Australia

{name.surname}@anu.edu.au

Abstract

In Learning with Label Proportions (LLP)  the objective is to learn a supervised
classiﬁer when  instead of labels  only label proportions for bags of observations
are known. This setting has broad practical relevance  in particular for privacy
preserving data processing. We ﬁrst show that the mean operator  a statistic which
aggregates all labels  is minimally sufﬁcient for the minimization of many proper
scoring losses with linear (or kernelized) classiﬁers without using labels. We pro-
vide a fast learning algorithm that estimates the mean operator via a manifold
regularizer with guaranteed approximation bounds. Then  we present an itera-
tive learning algorithm that uses this as initialization. We ground this algorithm
in Rademacher-style generalization bounds that ﬁt the LLP setting  introducing
a generalization of Rademacher complexity and a Label Proportion Complexity
measure. This latter algorithm optimizes tractable bounds for the corresponding
bag-empirical risk. Experiments are provided on fourteen domains  whose size
ranges up to ≈300K observations. They display that our algorithms are scalable
and tend to consistently outperform the state of the art in LLP. Moreover  in many
cases  our algorithms compete with or are just percents of AUC away from the
Oracle that learns knowing all labels. On the largest domains  half a dozen pro-
portions can sufﬁce  i.e. roughly 40K times less than the total number of labels.

1

Introduction

Machine learning has recently experienced a proliferation of problem settings that  to some extent 
enrich the classical dichotomy between supervised and unsupervised learning. Cases as multiple
instance labels  noisy labels  partial labels as well as semi-supervised learning have been studied
motivated by applications where fully supervised learning is no longer realistic. In the present work 
we are interested in learning a binary classiﬁer from information provided at the level of groups of
instances  called bags. The type of information we assume available is the label proportions per
bag  indicating the fraction of positive binary labels of its instances. Inspired by [1]  we refer to this
framework as Learning with Label Proportions (LLP). Settings that perform a bag-wise aggregation
of labels include Multiple Instance Learning (MIL) [2]. In MIL  the aggregation is logical rather
than statistical: each bag is provided with a binary label expressing an OR condition on all the labels
contained in the bag. More general setting also exist [3] [4] [5].
Many practical scenarios ﬁt the LLP abstraction. (a) Only aggregated labels can be obtained due to
the physical limits of measurement tools [6] [7] [8] [9]. (b) The problem is semi- or unsupervised
but domain experts have knowledge about the unlabelled samples in form of expectation  as pseudo-
measurement [5].
(c) Labels existed once but they are now given in an aggregated fashion for
privacy-preserving reasons  as in medical databases [10]  fraud detection [11]  house price market 
election results  census data  etc. . (d) This setting also arises in computer vision [12] [13] [14].
Related work. Two papers independently introduce the problem  [12] and [9]. In the ﬁrst the authors
propose a hierarchical probabilistic model which generates labels consistent with the proportions 
and make inference through MCMC sampling. Similarly  the second and its follower [6] offer a

1

variety of standard machine learning methods designed to generate self-consistent labels. [15] gives
a Bayesian interpretation of LLP where the key distribution is estimated through an RBM. Other
ideas rely on structural learning of Bayesian networks with missing data [7]  and on K-MEANS clus-
tering to solve preliminary label assignment in order to resort to fully supervised methods [13] [8].
Recent SVM implementations [11] [16] outperform most of the other known methods. Theoretical
works on LLP belong to two main categories. The ﬁrst contains uniform convergence results  for the
estimators of label proportions [1]  or the estimator of the mean operator [17]. The second contains
approximation results for the classiﬁer [17]. Our work builds upon their Mean Map algorithm  that
relies on the trick that the logistic loss may be split in two  a convex part depending only on the
observations  and a linear part involving a sufﬁcient statistic for the label  the mean operator. Being
able to estimate the mean operator means being able to ﬁt a classiﬁer without using labels. In [17] 
this estimation relies on a restrictive homogeneity assumption that the class-conditional estimation
of features does not depend on the bags. Experiments display the limits of this assumption [11][16].
Contributions. In this paper we consider linear classiﬁers  but our results hold for kernelized for-
mulations following [17]. We ﬁrst show that the trick about the logistic loss can be generalized 
and the mean operator is actually minimally sufﬁcient for a wide set of “symmetric” proper scoring
losses with no class-dependent misclassiﬁcation cost  that encompass the logistic  square and Mat-
sushita losses [18]. We then provide an algorithm  LMM  which estimates the mean operator via a
Laplacian-based manifold regularizer without calling to the homogeneity assumption. We show that
under a weak distinguishability assumption between bags  our estimation of the mean operator is
all the better as the observations norm increase. This  as we show  cannot hold for the Mean Map
estimator. Then  we provide a data-dependent approximation bound for our classiﬁer with respect
to the optimal classiﬁer  that is shown to be better than previous bounds [17]. We also show that
the manifold regularizer’s solution is tightly related to the linear separability of the bags. We then
provide an iterative algorithm  AMM  that takes as input the solution of LMM and optimizes it fur-
ther over the set of consistent labelings. We ground the algorithm in a uniform convergence result
involving a generalization of Rademacher complexities for the LLP setting. The bound involves
a bag-empirical surrogate risk for which we show that AMM optimizes tractable bounds. All our
theoretical results hold for any symmetric proper scoring loss. Experiments are provided on four-
teen domains  ranging from hundreds to hundreds of thousands of examples  comparing AMM and
LMM to their contenders: Mean Map  InvCal [11] and ∝SVM [16]. They display that AMM and
LMM outperform their contenders  and sometimes even compete with the fully supervised learner
while requiring few proportions only. Tests on the largest domains display the scalability of both
algorithms. Such experimental evidence seriously questions the safety of privacy-preserving sum-
marization of data  whenever accurate aggregates and informative individual features are available.
Section (2) presents our algorithms and related theoretical results. Section (3) presents experiments.
Section (4) concludes. A Supplementary Material [19] includes proofs and additional experiments.

2 LLP and the mean operator: theoretical results and algorithms

.= {1  2  ...  m}. Let Σm

Learning setting Hereafter  boldfaces like p denote vectors  whose coordinates are denoted pl for
l = 1  2  .... For any m ∈ N∗  let [m]
.= {σ ∈ {−1  1}m} and X ⊆ Rd.
Examples are couples (observation  label) ∈ X × Σ1  sampled i.i.d. according to some unknown
but ﬁxed distribution D. Let S .= {(xi  yi)  i ∈ [m]} ∼ Dm denote a size-m sample. In Learning
with Label Proportions (LLP)  we do not observe directly S but S|y  which denotes S with labels
removed; we are given its partition in n > 0 bags  S|y = ∪jSj  j ∈ [n]  along with their respective
.= mj/m with mj = card(Sj). (This
label proportions ˆπj
generalizes to a cover of S  by copying examples among bags.) The “bag assignment function” that
partitions S is unknown but ﬁxed. In real world domains  it would rather be known  e.g. state  gender 
age band. A classiﬁer is a function h : X → R  from a set of classiﬁers H. HL denotes the set of
linear classiﬁers  noted hθ(x) .= θ(cid:62)x with θ ∈ X. A (surrogate) loss is a function F : R → R+.
i F (yih(xi)) denote the empirical surrogate risk on S corresponding to
loss F . For the sake of clarity  indexes i  j and k respectively refer to examples  bags and features.

We let F (S  h) .= (1/m)(cid:80)

.= ˆP[y = +1|Sj] and bag proportions ˆpj

The mean operator and its minimal sufﬁciency We deﬁne the (empirical) mean operator as:

(cid:88)

i

µS

.=

1
m

2

yixi .

(1)

Algorithm 1 Laplacian Mean Map (LMM)

Input Sj  ˆπj  j ∈ [n]; γ > 0 (7); w (7); V (8); permissible φ (2); λ > 0;
Step 1 : let ˜B

± ← arg minX∈R2n×d (cid:96)(L  X) using (7) (Lemma 2)

Step 2 : let ˜µS ←(cid:80)

j − (1 − ˆπj)˜b−
˜b+
j )

j ˆpj(ˆπj

Step 3 : let ˜θ∗ ← arg minθ Fφ(S|y  θ  ˜µS) + λ(cid:107)θ(cid:107)2
Return ˜θ∗

2 (3)

Table 1: Correspondence between permissible functions φ and the corresponding loss Fφ.

loss name
logistic loss
square loss

Matsushita loss

−φ(x)

Fφ(x)
(1 − x)2

log(1 + exp(−x)) −x log x − (1 − x) log(1 − x)
−x +

(cid:112)x(1 − x)

x(1 − x)

1 + x2

√

The estimation of the mean operator µS appears to be a learning bottleneck in the LLP setting
[17]. The fact that the mean operator is sufﬁcient to learn a classiﬁer without the label information
motivates the notion of minimal sufﬁcient statistic for features in this context. Let F be a set of
loss functions  H be a set of classiﬁers  I be a subset of features. Some quantity t(S) is said to be
a minimal sufﬁcient statistic for I with respect to F and H iff: for any F ∈ F  any h ∈ H and
any two samples S and S(cid:48)  the quantity F (S  h) − F (S(cid:48)  h) does not depend on I iff t(S) = t(S(cid:48)).
This deﬁnition can be motivated from the one in statistics by building losses from log likelihoods.
The following Lemma motivates further the mean operator in the LLP setting  as it is the minimal
sufﬁcient statistic for a broad set of proper scoring losses that encompass the logistic and square
losses [18]. The proper scoring losses we consider  hereafter called “symmetric” (SPSL)  are twice
differentiable  non-negative and such that misclassiﬁcation cost is not label-dependent.

Lemma 1 µS is a minimal sufﬁcient statistic for the label variable  with respect to SPSL and HL.

([19]  Subsection 2.1) This property  very useful for LLP  may also be exploited in other weakly
supervised tasks [2]. Up to constant scalings that play no role in its minimization  the empirical
surrogate risk corresponding to any SPSL  Fφ(S  h)  can be written with loss:

.=

Fφ(x)

(2)
and φ is a permissible function [20  18]  i.e. dom(φ) ⊇ [0  1]  φ is strictly convex  differentiable and
symmetric with respect to 1/2. φ(cid:63) is the convex conjugate of φ. Table 1 shows examples of Fφ. It
follows from Lemma 1 and its proof  that any Fφ(Sθ)  can be written for any θ ≡ hθ ∈ HL as:

bφ

 

.= aφ +

φ(0) + φ(cid:63)(−x)
φ(0) − φ(1/2)

φ(cid:63)(−x)

(cid:32)(cid:88)

(cid:88)

i

σ

(cid:33)

Fφ(σθ(cid:62)xi)

θ(cid:62)µS

− 1
2

.= Fφ(S|y  θ  µS)  

(3)

Fφ(S  θ) =

bφ
2m

where σ ∈ Σ1.

The Laplacian Mean Map (LMM) algorithm The sum in eq. (3) is convex and differentiable
in θ. Hence  once we have an accurate estimator of µS  we can then easily ﬁt θ to minimize
Fφ(S|y  θ  µS). This two-steps strategy is implemented in LMM in algorithm 1. µS can be retrieved
from 2n bag-wise  label-wise unknown averages bσ
j :

µS = (1/2)

ˆpj

(2ˆπj + σ(1 − σ))bσ
j  

(1/mj)(cid:80)

with bσ
j

.= ES[x|σ  j] denoting these 2n unknowns (for j ∈ [n]  σ ∈ Σ1)  and let bj
xi∈Sj

j s are solution of a set of n identities that are (in matrix form):

xi. The 2n bσ

(4)

.=

(5)

n(cid:88)

j=1

(cid:88)

σ∈Σ1

B − Π

(cid:62)B± = 0  

3

where B .= [b1|b2|...|bn](cid:62) ∈ Rn×d  Π
the matrix of unknowns:

.= [DIAG( ˆπ)|DIAG(1 − ˆπ)](cid:62) ∈ R2n×n and B± ∈ R2n×d is

B±

.=

(cid:104)

(cid:12)(cid:12)(cid:12) b-1
(cid:124)

(cid:124)

(cid:123)(cid:122)
(cid:125)
2 |...|b+1
1 |b+1
b+1
)(cid:62)
(B+

n

(cid:123)(cid:122)
(cid:125)
2|...|b-1
1|b-1
)(cid:62)
(B–

n

(cid:105)(cid:62)

.

(6)

±

and γ > 0. Dw

= arg minX∈R2n×d (cid:96)(L  X)  with:

(cid:62)X)(cid:1) + γtr(cid:0)X(cid:62)LX(cid:1)  

.= tr(cid:0)(B(cid:62) − X(cid:62)
(cid:20) La

System (5) is underdetermined  unless one makes the homogeneity assumption that yields the Mean
Map estimator [17]. Rather than making such a restrictive assumption  we regularize the cost that
brings (5) with a manifold regularizer [21]  and search for ˜B
Π)Dw(B − Π

(cid:96)(L  X)
.= DIAG(w) is a user-ﬁxed bias matrix with w ∈ Rn

(7)
+ ∗ (and w (cid:54)= ˆp in general) and:
(8)
.= D − V ∈ Rn×n is the Laplacian of the bag similarities. V is a symmetric similarity
where La
j(cid:48) vjj(cid:48) ∀j ∈ [n].
matrix with non negative coordinates  and the diagonal matrix D satisﬁes djj
The size of the Laplacian is O(n2)  which is very small compared to O(m2) if there are not many
bags. One can interpret the Laplacian regularization as smoothing the estimates of bσ
j w.r.t the
similarity of the respective bags.

∈ R2n×2n  

|
0
| La

.= εI +

.=(cid:80)
=(cid:0)ΠDwΠ(cid:62) + γL(cid:1)−1

Lemma 2 The solution ˜B

± to minX∈R2n×d (cid:96)(L  X) is ˜B

±

ΠDwB.

(cid:21)

L

0

([19]  Subsection 2.2). This Lemma explains the role of penalty εI in (8) as ΠDwΠ(cid:62) and L have
respectively n- and (≥ 1)-dim null spaces  so the inversion may not be possible. Even when this does
not happen exactly  this may incur numerical instabilities in computing the inverse. For domains
where this risk exists  picking a small ε > 0 solves the problem. Let ˜bσ
j denote the row-wise
following (6)  from which we compute ˜µS following (4) when we use these
decomposition of ˜B
j  ∀j ∈ [n] to our estimates
2n estimates in lieu of the true bσ
˜µj

 ∀j ∈ [n]  granted that µS =(cid:80)

j − (1− ˆπj)b−

j . We compare µj

j − (1 − ˆπj)˜b−
˜b+

.= ˆπjb+

j ˆpj ˜µj.

.= ˆπj

±

j

Theorem 3 Suppose that γ satisﬁes γ
[µ1|µ2|...|µn](cid:62) ∈ Rn×d  ˜M .= [ ˜µ1| ˜µ2|...| ˜µn](cid:62) ∈ Rn×d and ς(V  B±)
maxj(cid:54)=j(cid:48) vjj(cid:48))2(cid:107)B±(cid:107)F . The following holds:
(cid:107)M − ˜M(cid:107)F ≤ √

(cid:18)√

2 min

2 ≤ ((ε(2n)−1) + maxj(cid:54)=j(cid:48) vjj(cid:48))/ minj wj. Let M .=
.= ((ε(2n)−1) +

(9)

n

w2
j

j

j ˆpjµj and ˜µS =(cid:80)
(cid:19)−1 × ς(V  B±) .

√

(cid:18) ASSOC(Sj  Sj)

([19]  Subsection 2.3) The multiplicative factor to ς in (9) is roughly O(n5/2) when there is no large
discrepancy in the bias matrix Dw  so the upperbound is driven by ς(.  .) when there are not many
bags. We have studied its variations when the “distinguishability” between bags increases. This
setting is interesting because in this case we may kill two birds in one shot  with the estimation of
M and the subsequent learning problem potentially easier  in particular for linear separators. We
consider two examples for vjj(cid:48)  the ﬁrst being (half) the normalized association [22]:

1
2

ASSOC(Sj(cid:48)  Sj(cid:48))

+

vnc
jj(cid:48)

vG s
jj(cid:48)

= NASSOC(Sj  Sj(cid:48))  

(10)

ASSOC(Sj  Sj ∪ Sj(cid:48))

ASSOC(Sj(cid:48)  Sj ∪ Sj(cid:48))

Here  ASSOC(Sj  Sj(cid:48)) .= (cid:80)

.=
.= exp(−(cid:107)bj − bj(cid:48)(cid:107)2/s)   s > 0 .
j(cid:48) (cid:107)x − x(cid:48)(cid:107)2

(11)
2 [22]. To put these two similarity measures in
the context of Theorem 3  consider the setting where we can make assumption (D1) that there
exists a small constant κ > 0 such that (cid:107)bj − bj(cid:48)(cid:107)2
2 ∀j  j(cid:48) ∈ [n]. This is a
weak distinguishability property as if no such κ exists  then the centers of distinct bags may just
be confounded. Consider also the additional assumption  (D2)  that there exists κ(cid:48) > 0 such that
i∈Sj (cid:107)xi − xi(cid:48)(cid:107)2 is a bag’s diameter. In the following
maxj d2
Lemma  the little-oh notation is with respect to the “largest” unknown in eq. (4)  i.e. maxσ j (cid:107)bσ
j (cid:107)2.

j ≤ κ(cid:48) ∀j ∈ [n]  where dj

2 ≥ κ maxσ j (cid:107)bσ

.= maxxi x(cid:48)

x∈Sj  x(cid:48)∈S

j (cid:107)2

(cid:19)

4

Algorithm 2 Alternating Mean Map (AMMOPT)

Input LMM parameters + optimization strategy OPT ∈ {min  max} + convergence predicate PR
Step 1 : let ˜θ0 ← LMM(LMM parameters) and t ← 0
Step 2 : repeat

Step 2.1 : let σt ← arg OPTσ∈Σ ˆπ Fφ(S|y  θt  µS(σ))
Step 2.2 : let ˜θt+1 ← arg minθ Fφ(S|y  θ  µS(σt)) + λ(cid:107)θ(cid:107)2
Step 2.3 : let t ← t + 1
until predicate PR is true

2

Return ˜θ∗ .= arg mint Fφ(S|y  ˜θt+1  µS(σt))

Lemma 4 There exists ε∗ > 0 such that ∀ε ≤ ε∗  the following holds: (i) ς(Vnc  B±) = o(1) under
assumptions (D1 + D2); (ii) ς(VG s  B±) = o(1) under assumption (D1)  ∀s > 0.

([19]  Subsection 2.4) Hence  provided a weak (D1) or stronger (D1+D2) distinguishability assump-
tion holds  the divergence between M and ˜M gets smaller with the increase of the norm of the
unknowns bσ
j . The proof of the Lemma suggests that the convergence may be faster for VG s. The
following Lemma shows that both similarities also partially encode the hardness of solving the clas-
siﬁcation problem with linear separators  so that the manifold regularizer “limits” the distortion of
the ˜b±
Lemma 5 Take vjj(cid:48) ∈ {vG .
Sj  Sj(cid:48) are not linearly separable  and if vjj(cid:48) < κl then Sj  Sj(cid:48) are linearly separable.

jj(cid:48)}. There exists 0 < κl < κn < 1 such that (i) if vjj(cid:48) > κn then

. s between two bags that tend not to be linearly separable.

jj(cid:48)   vnc

([19]  Subsection 2.5) This Lemma is an advocacy to ﬁt s in a data-dependent way in vG s
jj(cid:48) . The
question may be raised as to whether ﬁnite samples approximation results like Theorem 3 can be
proven for the Mean Map estimator [17]. [19]  Subsection 2.6 answers by the negative.
In the Laplacian Mean Map algorithm (LMM  Algorithm 1)  Steps 1 and 2 have now been described.
Step 3 is a differentiable convex minimization problem for θ that does not use the labels  so it does
not present any technical difﬁculty. An interesting question is how much our classiﬁer ˜θ∗ in Step 3
diverges from the one that would be computed with the true expression for µS  θ∗. It is not hard to
show that Lemma 17 in Altun and Smola [23]  and Corollary 9 in Quadrianto et al. [17] hold for
LMM so that (cid:107) ˜θ∗ − θ∗(cid:107)2
2. The following Theorem shows a data-dependent
∗ xi ∈ φ(cid:48)([0  1]) ∀i
approximation bound that can be signiﬁcantly better  when it holds that θ(cid:62)
(φ(cid:48) is the ﬁrst derivative). We call this setting proper scoring compliance (PSC) [18]. PSC always
holds for the logistic and Matsushita losses for which φ(cid:48)([0  1]) = R. For other losses like the square
loss for which φ(cid:48)([0  1]) = [−1  1]  shrinking the observations in a ball of sufﬁciently small radius
is sufﬁcient to ensure this.
Theorem 6 Let fk ∈ Rm denote the vector encoding the kth feature variable in S : fki = xik
(k ∈ [d]). Let ˜F denote the feature matrix with column-wise normalized feature vectors: ˜fk
.=
2  with:

2)(d−1)/(2d)fk. Under PSC  we have (cid:107) ˜θ∗ − θ∗(cid:107)2

2 ≤ (2λ + q)−1(cid:107) ˜µS − µS(cid:107)2

2 ≤ (2λ)−1(cid:107) ˜µS − µS(cid:107)2

(d/(cid:80)

∗ xi  ˜θ(cid:62)

k(cid:48) (cid:107)fk(cid:48)(cid:107)2

q

.=

(cid:62)˜F

×

det ˜F
m

2e−1

bφφ(cid:48)(cid:48) (φ(cid:48)−1(q(cid:48)/λ))

(> 0)  

(12)

for some q(cid:48) ∈ I .= [±(x∗ + max{(cid:107)µS(cid:107)2 (cid:107) ˜µS(cid:107)2})]. Here  x∗ .= maxi (cid:107)xi(cid:107)2 and φ(cid:48)(cid:48)

.= (φ(cid:48))(cid:48).

([19]  Subsection 2.7) To see how large q can be  consider the simple case where all eigenvalues of
(cid:62)˜F  λk(˜F
(cid:62)˜F) ∈ [λ◦ ± δ] for small δ. In this case  q is proportional to the average feature “norm”:
˜F

tr(cid:0)F(cid:62)F(cid:1)

md

(cid:62)˜F

det ˜F
m

=

(cid:80)

i (cid:107)xi(cid:107)2
md

2

+ o(δ) .

+ o(δ) =

5

.= {σ ∈ Σm :(cid:80)

i:xi∈Sj

µS(σ) .= (1/m)(cid:80)

The Alternating Mean Map (AMM) algorithm Let us denote Σ ˆπ
σi =
(2ˆπj − 1)mj ∀j ∈ [n]} the set of labelings that are consistent with the observed proportions ˆπ  and
i σixi the biased mean operator computed from some σ ∈ Σ ˆπ. Notice that the
true mean operator µS = µS(σ) for at least one σ ∈ Σ ˆπ. The Alternating Mean Map algorithm 
(AMM  Algorithm 2)  starts with the output of LMM and then optimizes it further over the set of
consistent labelings. At each iteration  it ﬁrst picks a consistent labeling in Σ ˆπ that is the best (OPT
= min) or the worst (OPT = max) for the current classiﬁer (Step 2.1) and then ﬁts a classiﬁer ˜θ on the
given set of labels (Step 2.2). The algorithm then iterates until a convergence predicate is met  which
tests whether the difference between two values for Fφ(.  .  .) is too small (AMMmin)  or the number
of iterations exceeds a user-speciﬁed limit (AMMmax). The classiﬁer returned ˜θ∗ is the best in the
sequence. In the case of AMMmin  it is the last of the sequence as risk Fφ(S|y  .  .) cannot increase.
Again  Step 2.2 is a convex minimization with no technical difﬁculty. Step 2.1 is combinatorial. It
can be solved in time almost linear in m [19] (Subsection 2.8).

Lemma 7 The running time of Step 2.1 in AMM is ˜O(m)  where the tilde notation hides log-terms.

Bag-Rademacher generalization bounds for LLP We relate the “min” and “max” strategies of
AMM by uniform convergence bounds involving the true surrogate risk  i.e. integrating the unknown
distribution D and the true labels (which we may never know). Previous uniform convergence
bounds for LLP focus on coarser grained problems  like the estimation of label proportions [1].
We rely on a LLP generalization of Rademacher complexity [24  25]. Let F : R → R+ be a
loss function and H a set of classiﬁers. The bag empirical Rademacher complexity of sample S 
ES[σ(x)F (σ(cid:48)(x)h(x))]. The usual empirical
Rb
m for card(Σ ˆπ) = 1. The Label Proportion Complexity of H is:
Rademacher complexity equals Rb
.= ED2m
EI/2
(13)

ES[σ1(x)(ˆπs|2(x) − ˆπ(cid:96)|1(x))h(x)] .

.= Eσ∼Σm suph∈H{Eσ(cid:48)∼Σ ˆπ

m  is deﬁned as Rb
m

L2m

1  I/2

2

sup
h∈H

l   l = 1  2 is a random (uniformly) subset of [2m] of cardinal m. Let S(I/2

l ) be the
then
l ). Else  ˆπs|2(xi) is its bag’s
2 ) and ˆπ(cid:96)|1(xi) is its label (i.e. a bag’s label proportion that would
1 ) − 1 ∈ Σ1. L2m tends to be all the smaller as

Here  each of I/2
size-m subset of S that corresponds to the indexes. Take l = 1  2 and any xi ∈ S. If i (cid:54)∈ I/2
ˆπs|l(xi) = ˆπ(cid:96)|l(xi) is xi’s bag’s label proportion measured on S\S(I/2
label proportion measured on S(I/2
contain only xi). Finally  σ1(x) .= 2 × 1x∈S(I/2
classiﬁers in H have small magnitude on bags whose label proportion is close to 1/2.
Theorem 8 Suppose ∃h∗ ≥ 0 s.t. |h(x)| ≤ h∗ ∀x ∀h. Then  for any loss Fφ  any training sample
of size m and any 0 < δ ≤ 1  with probability > 1 − δ  the following bound holds over all h ∈ H:
ED[Fφ(yh(x))] ≤ EΣ ˆπ

ES[Fφ(σ(x)h(x))] + 2Rb

(cid:19)(cid:114) 1

(cid:18) 2h∗

m + L2m + 4

.(14)

+ 1

log

l

bφ

2m

2
δ

Furthermore  under PSC (Theorem 6)  we have for any Fφ:

m ≤ 2bφEΣm sup
Rb
h∈H

{ES[σ(x)(ˆπ(x) − (1/2))h(x)]} .

(15)

([19]  Subsection 2.9) Despite similar shapes (13) (15)  Rb
are pure (ˆπj ∈ {0  1} ∀j)  L2m = 0. When bags are impure (ˆπj = 1/2 ∀j)  Rb
impure  the bag-empirical surrogate risk  EΣ ˆπ
and AMMmax respectively minimize a lowerbound and an upperbound of this risk.

m and L2m behave differently: when bags
m = 0. As bags get
ES[Fφ(σ(x)h(x))]  also tends to increase. AMMmin

3 Experiments

Algorithms We compare LMM  AMM (Fφ = logistic loss) to the original MM [17]  InvCal [11]  conv-
∝SVM and alter-∝SVM [16] (linear kernels). To make experiments extensive  we test several ini-
tializations for AMM that are not displayed in Algorithm 2 (Step 1): (i) the edge mean map estimator 
.= 1 (AMM1)  and ﬁnally
˜µEMM
AMM10ran which runs 10 random initial models ((cid:107)θ0(cid:107)2 ≤ 1)  and selects the one with smallest risk;
S

i xi) (AMMEMM)  (ii) the constant estimator ˜µ1

.= 1/m2((cid:80)

i yi)((cid:80)

S

6

(a)

(b)

(c)

(d)

Figure 1: Relative AUC (wrt MM) as homogeneity assumption is violated (a). Relative AUC (wrt
Oracle) vs entropy on heart for LMM(b)  AMMmin(c). Relative AUC vs n/m for AMMmin

G s (d).

Table 2: Small domains results. #win/#lose for row vs column. Bold faces means p-val < .001 for
Wilcoxon signed-rank tests. Top-left subtable is for one-shot methods  bottom-right iterative ones 
bottom-left compare the two. Italic is state-of-the-art. Grey cells highlight the best of all (AMMmin
G ).

algorithm

MM

M G
G s
M
L
nc
InvCal
MM
G
G s
10ran

M
M
A

n
i
m

x MM
a
m

M
M
A

G
G s
10ran
M conv-∝
alter-∝

V
S

36/4
38/3
28/12
4/46
33/16
38/11
35/14
27/22
25/25
27/23
25/25
23/27
21/29
0/50

G

30/6
3/37
3/47
26/24
35/14
33/17
24/26
23/27
22/28
21/29
21/29
2/48
0/50

LMM
G s

nc

InvCal

MM

AMMmin
G

G s

10ran

MM

AMMmax
G

G s

10ran

conv-
∝SVM

2/37
4/46
25/25
30/20
30/20
22/28
22/28
21/28
22/28
19/31
2/48
0/50

4/46
32/18
37/13
35/15
26/24
25/25
26/24
24/26
24/26
2/48
0/50

46/4
47/3
47/3
44/6
45/5
45/5
45/5
50/0
2/48
20/30

31/7
24/11
20/30
15/35
17/33
15/35
19/31
4/46
0/50

7/15
16/34
13/37
14/36
13/37
15/35
3/47
0/50

(cid:46) e.g. AMMmin

G s wins on AMMmin

G 7 times  loses 15  with 28 ties

19/31
13/37
14/36
13/37
17/33
3/47
0/50

8/42
10/40
12/38
7/43
4/46
3/47

13/14
15/22
19/30
3/47
3/47

16/22
20/29
3/47
2/48

17/32
4/46
1/49

0/50
0/50

27/23

this is the same procedure of alter-∝SVM. Matrix V (eqs. (10)  (11)) used is indicated in subscript:
LMM/AMMG  LMM/AMMG s  LMM/AMMnc respectively denote vG s with s = 1  vG s with s learned
on cross validation (CV; validation ranges indicated in [19]) and vnc. For space reasons  results
not displayed in the paper can be found in [19]  Section 3 (including runtime comparisons  and de-
tailed results by domain). We split the algorithms in two groups  one-shot and iterative. The latter 
including AMM  (conv/alter)-∝SVM  iteratively optimize a cost over labelings (always consistent
with label proportions for AMM  not always for (conv/alter)-∝SVM). The former (LMM  InvCal) do
not and are thus much faster. Tests are done on a 4-core 3.2GHz CPUs Mac with 32GB of RAM.
AMM/LMM/MM are implemented in R. Code for InvCal and ∝SVM is [16].
Simulated domains  MM and the homogeneity assumption The testing metric is the AUC. Prior
to testing on our domains  we generate 16 domains that gradually move away the bσ
j away from each
other (wrt j)  thus violating increasingly the homogeneity assumption [17]. The degree of violation
is measured as (cid:107)B± − B±(cid:107)F   where B± is the homogeneity assumption matrix  that replaces all bσ
by bσ for σ ∈ {−1  1}  see eq. (5). Figure 1 (a) displays the ratios of the AUC of LMM to the
AUC of MM. It shows that LMM is all the better with respect to MM as the homogeneity assumption
is violated. Furthermore  learning s in LMM improves the results. Experiments on the simulated
domain of [16] on which MM obtains zero accuracy also display that our algorithms perform better
(1 iteration only of AMMmax brings 100% AUC).
Small and large domains experiments We convert 10 small domains [19] (m ≤ 1000) and 4 bigger
ones (m > 8000) from UCI[26] into the LLP framework. We cast to one-against-all classiﬁcation
when the problem is multiclass. On large domains  the bag assignment function is inspired by [1]:
we craft bags according to a selected feature value  and then we remove that feature from the data.
This conforms to the idea that bag assignment is structured and non random in real-world problems.
Most of our small domains  however  do not have a lot of features  so instead of clustering on one
feature and then discard it  we run K-MEANS on the whole data to make the bags  for K = n ∈ 2[5].
Small domains results We performe 5-folds nested CV comparisons on the 10 domains = 50 AUC
values for each algorithm. Table 2 synthesises the results [19]  splitting one-shot and iterative algo-

j

7

1.01.11.21.3246divergenceAUC rel. to MMMM LMMG LMMG sLMMnc 0.60.70.80.91.00.60.81.0entropyAUC rel. to OracleMM LMMG LMMG sLMMnc 0.60.70.80.91.00.60.81.0entropyAUC rel. to OracleAMMMM AMMG AMMG s AMMnc AMM10ranBiggerdomainsSmalldomains0.20.40.60.81.010^−510^−310^−1#bag/#instancesAUC rel. to OracleAMMGTable 3: AUCs on big domains (name: #instances×#features). I=cap-shape  II=habitat 

III=cap-colour  IV=race  V=education  VI=country  VII=poutcome  VIII=job (number of bags);

for each feature  the best result over one-shot  and over iterative algorithms is bold faced.

adult: 48842 × 89

marketing: 45211 × 41

census: 299285 × 381

algorithm

EMM
MM
LMMG
LMMG s
AMMEMM
AMMMM
AMMG
AMMG s
AMM1
AMMEMM
AMMMM
AMMG
AMMG s
AMM1
Oracle

n
i
m

M
M
A

x
a
m

M
M
A

mushroom: 8124 × 108
III(10)
I(6)
76.68
55.61
5.02
51.99
14.70
73.92
89.43
94.91
69.43
85.12
15.74
89.81
89.18
50.44
3.28
89.24
95.90
97.31
26.67
93.04
99.70
59.45
99.30
95.50
84.26
95.84
95.01
1.29
99.8
99.82

II(7)
59.80
98.79
98.57
98.24
99.45
99.01
99.45
99.57
98.49
3.32
55.16
65.32
65.32
73.48
99.81

IV(5)
43.91
80.93
81.79
84.89
49.97
83.73
83.41
81.18
81.32
54.46
82.57
82.75
82.69
75.22
90.55

V(16)
47.50
76.65
78.40
78.94
56.98
77.39
82.55
78.53
75.80
69.63
71.63
72.16
70.95
67.52
90.55

VI(42)
66.61
74.01
78.78
80.12
70.19
80.67
81.96
81.96
80.05
56.62
81.39
81.39
81.39
77.67
90.50

V(4)
63.49
54.64
54.66
49.27
61.39
52.85
51.61
52.03
65.13
51.48
48.46
50.58
66.88
66.70
79.52

VII(4)
54.50
50.71
51.00
51.00
55.73
75.27
75.16
75.16
64.96
55.63
51.34
47.27
47.27
61.16
75.55

VIII(12)
44.31
49.70
51.93
65.81
43.10
58.19
57.52
53.98
66.62
57.48
56.90
34.29
34.29
71.94
79.43

IV(5)
56.05
75.21
75.80
84.88
87.86
89.68
87.61
89.93
89.09
71.20
50.75
48.32
80.33
57.97
94.31

VIII(9)
56.25
90.37
71.75
60.71
87.71
84.91
88.28
83.54
88.94
77.14
66.76
67.54
74.45
81.07
94.37

VI(42)
57.87
75.52
76.31
69.74
40.80
68.36
76.99
52.13
56.72
66.71
58.67
77.46
52.70
53.42
94.45

rithms. LMMG s outperforms all one-shot algorithms. LMMG and LMMG s are competitive with many
iterative algorithms  but lose against their AMM counterpart  which proves that additional optimiza-
tion over labels is beneﬁcial. AMMG and AMMG s are conﬁrmed as the best variant of AMM  the
ﬁrst being the best in this case. Surprisingly  all mean map algorithms  even one-shots  are clearly
superior to ∝SVMs. Further results [19] reveal that ∝SVM performances are dampened by learning
classiﬁers with the “inverted polarity” — i.e. ﬂipping the sign of the classiﬁer improves its perfor-
mances. Figure 1 (b  c) presents the AUC relative to the Oracle (which learns the classiﬁer knowing
all labels and minimizing the logistic loss)  as a function of the Gini entropy of bag assignment 
gini(S) .= 4Ej[ˆπj(1 − ˆπj)]. For an entropy close to 1  we were expecting a drop in performances.
The unexpected [19] is that on some domains  large entropies (≥ .8) do not prevent AMMmin to
compete with the Oracle. No such pattern clearly emerges for ∝SVM and AMMmax [19].
Big domains results We adopt a 1/5 hold-out method. Scalability results [19] display that every
method using vnc and ∝SVM are not scalable to big domains; in particular  the estimated time for a
single run of alter-∝SVM is >100 hours on the adult domain. Table 3 presents the results on the big
domains  distinguishing the feature used for bag assignment. Big domains conﬁrm the efﬁciency of
LMM+AMM. No approach clearly outperforms the rest  although LMMG s is often the best one-shot.
Synthesis Figure 1 (d) gives the AUCs of AMMmin
G over the Oracle for all domains [19]  as a function
of the “degree of supervision”  n/m (=1 if the problem is fully supervised). Noticeably  on 90% of
the runs  AMMmin
G gets an AUC representing at least 70% of the Oracle’s. Results on big domains
can be remarkable: on the census domain with bag assignment on race  5 proportions are sufﬁcient
for an AUC 5 points below the Oracle’s — which learns with 200K labels.

4 Conclusion

In this paper  we have shown that efﬁcient learning in the LLP setting is possible  for general loss
functions  via the mean operator and without resorting to the homogeneity assumption. Through its
estimation  the sufﬁciency allows one to resort to standard learning procedures for binary classiﬁca-
tion  practically implementing a reduction between machine learning problems [27]; hence the mean
operator estimation may be a viable shortcut to tackle other weakly supervised settings [2] [3] [4]
[5]. Approximation results and generalization bounds are provided. Experiments display results that
are superior to the state of the art  with algorithms that scale to big domains at affordable computa-
tional costs. Performances sometimes compete with the Oracle’s — that learns knowing all labels
—  even on big domains. Such experimental ﬁnding poses severe implications on the reliability of
privacy-preserving aggregation techniques with simple group statistics like proportions.

Acknowledgments

NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program. The ﬁrst author
would like to acknowledge that part of this research was conducted during his internship at the
Commonwealth Bank of Australia. We thank A. Menon and D. Garc´ıa-Garc´ıa for useful discussions.

8

References
[1] F.-X. Yu  S. Kumar  T. Jebara  and S.-F. Chang. On learning with label proportions. CoRR  abs/1402.5902 

2014.

[2] T.-G. Dietterich  R.-H. Lathrop  and T. Lozano-P´erez. Solving the multiple instance problem with axis-

parallel rectangles. Artiﬁcial Intelligence  89:31–71  1997.

[3] G.-S. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of condi-

tional random ﬁelds. In 46 th ACL  2008.

[4] J. Grac¸a  K. Ganchev  and B. Taskar. Expectation maximization and posterior constraints. In NIPS*20 

pages 569–576  2007.

[5] P. Liang  M.-I. Jordan  and D. Klein. Learning from measurements in exponential families. In 26 th ICML 

pages 641–648  2009.

[6] D.-J. Musicant  J.-M. Christensen  and J.-F. Olson. Supervised learning by training on aggregate outputs.

In 7 th ICDM  pages 252–261  2007.

[7] J. Hern´andez-Gonz´alez  I. Inza  and J.-A. Lozano. Learning bayesian network classiﬁers from label

proportions. Pattern Recognition  46(12):3425–3440  2013.

[8] M. Stolpe and K. Morik. Learning from label proportions by optimizing cluster model selection. In 15th

ECMLPKDD  pages 349–364  2011.

[9] B.-C. Chen  L. Chen  R. Ramakrishnan  and D.-R. Musicant. Learning from aggregate views.

22 th ICDE  pages 3–3  2006.

In

[10] J. Wojtusiak  K. Irvin  A. Birerdinc  and A.-V. Baranova. Using published medical results and non-

homogenous data in rule learning. In 10 th ICMLA  pages 84–89  2011.

[11] S. R¨uping. Svm classiﬁer estimation from group probabilities. In 27 th ICML  pages 911–918  2010.
[12] K. Hendrik and N. de Freitas. Learning about individuals from group statistics.

In 21 th UAI  pages

332–339  2005.

[13] S. Chen  B. Liu  M. Qian  and C. Zhang. Kernel k-means based framework for aggregate outputs classi-

ﬁcation. In 9 th ICDMW  pages 356–361  2009.

[14] K.-T. Lai  F.X. Yu  M.-S. Chen  and S.-F. Chang. Video event detection by inferring temporal instance

labels. In 11 th CVPR  2014.

[15] K. Fan  H. Zhang  S. Yan  L. Wang  W. Zhang  and J. Feng. Learning a generative classiﬁer from label

proportions. Neurocomputing  139:47–55  2014.

[16] F.-X. Yu  D. Liu  S. Kumar  T. Jebara  and S.-F. Chang. ∝SVM for Learning with Label Proportions. In

30th ICML  pages 504–512  2013.

[17] N. Quadrianto  A.-J. Smola  T.-S. Caetano  and Q.-V. Le. Estimating labels from label proportions. JMLR 

10:2349–2374  2009.

[18] R. Nock and F. Nielsen. Bregman divergences and surrogates for learning. IEEE Trans.PAMI  31:2048–

2059  2009.

[19] G. Patrini  R. Nock  P. Rivera  and T-S. Caetano. (Almost) no label no cry - supplementary material”. In

NIPS*27  2014.

[20] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In

28 th ACM STOC  pages 459–468  1996.

[21] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: A geometric framework for learning

from labeled and unlabeled examples. JMLR  7:2399–2434  2006.

[22] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans.PAMI  22:888–905  2000.
[23] Y. Altun and A.-J. Smola. Unifying divergence minimization and statistical inference via convex duality.

In 19th COLT  pages 139–153  2006.

[24] P.-L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. JMLR  3:463–482  2002.

[25] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error

of combined classiﬁers. Ann. of Stat.  30:1–50  2002.

[26] K. Bache and M. Lichman. UCI machine learning repository  2013.
[27] A. Beygelzimer  V. Dani  T. Hayes  J. Langford  and B. Zadrozny. Error limiting reductions between

classiﬁcation tasks. In 22 th ICML  pages 49–56  2005.

9

,Giorgio Patrini
Tiberio Caetano
Zelun Luo
Yuliang Zou
Judy Hoffman
Li Fei-Fei
Wenye Li
Jingwei Mao
Yin Zhang
Shuguang Cui