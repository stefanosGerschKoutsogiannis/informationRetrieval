2018,PAC-Bayes Tree: weighted subtrees with guarantees,We present a weighted-majority classification approach over subtrees of a fixed tree  which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore  the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees.,PAC-Bayes Tree: weighted subtrees with guarantees

Tin Nguyen∗
MIT EECS

tdn@mit.edu

Samory Kpotufe

Princeton University ORFE
samory@princeton.edu

Abstract

We present a weighted-majority classiﬁcation approach over subtrees of a ﬁxed
tree  which provably achieves excess-risk of the same order as the best tree-pruning.
Furthermore  the computational efﬁciency of pruning is maintained at both training
and testing time despite having to aggregate over an exponential number of subtrees.
We believe this is the ﬁrst subtree aggregation approach with such guarantees.
The guarantees are obtained via a simple combination of insights from PAC-Bayes
theory  which we believe should be of independent interest  as it generically implies
consistency for weighted-voting classiﬁers w.r.t. Bayes – while  in contrast  usual
PAC-bayes approaches only establish consistency of Gibbs classiﬁers.

1

Introduction

Classiﬁcation trees endure as popular tools in data analysis  offering both efﬁcient prediction and
interpretability – yet they remain hard to analyze in general. So far there are two main approaches
with generalization guarantees: in both approaches  a large tree (possibly overﬁtting the data) is ﬁrst
obtained; one approach is then to prune back this tree down to a subtree2 that generalizes better;
the alternative approach is to combine all possible subtrees of the tree by weighted majority vote.
Interestingly  while both approaches are competitive with other practical heuristics  it remains unclear
whether the alternative of weighting subtrees enjoys the same strong generalization guarantees as
pruning; in particular  no weighting scheme to date has been shown to be statistically consistent  let
alone attain the same tight generalization rates (in terms of excess risk) as pruning approaches.
In this work  we consider a new weighting scheme based on PAC-Bayesian insights [1]  that (a) is
consistent and attains the same generalization rates as the best pruning of a tree  (b) is efﬁciently
computable at both training and testing time  and (c) competes against pruning approaches on
real-world data. To the best of our knowledge  this is the ﬁrst practical scheme with such guarantees.
The main technical hurdle has to do with a subtle tension between goals (a) and (b) above. Namely 
let T0 denote a large tree built on n datapoints  usually a binary tree with O(n) nodes; the family
of subtrees T of T0 is typically of exponential size in n [2]  so a naive voting scheme that requires
visiting all subtrees is impractical; on the other hand it is known that if the weights decompose
favorably over the leaves of T (e.g.  multiplicative over leaves) then efﬁcient classiﬁcation is possible.
Unfortunately  while various such multiplicative weights have been designed for voting with subtrees
[3  4  5]  they are not known to yield statistically consistent prediction. In fact  the best known result
to date [5] presents a weighting scheme which can provably achieve an excess risk3 (over the Bayes
classiﬁer) of the form oP (1) + C · minT R(hT )  where R(hT ) denotes the misclassiﬁcation rate of
a classiﬁer hT based on subtree T . In other words  the excess risk might never go to 0 as sample size
increases  which in contrast is a basic property of the pruning alternative. Furthermore  the approach
∗The majority of the research was done when the author was an undergraduate student at Princeton University

ORFE.

2Considering only subtrees that partition the data space.
3The excess risk of a classiﬁer h over the Bayes hB (which minimizes R(h) over any h) is R(h) − R(hB).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

of [5]  based on l1-risk minimization  does not trivially extend to multiclass classiﬁcation  which is
most common in practice. Our approach is designed for multiclass by default.
Statistical contribution. PAC-Bayesian theory [1  6  7  8] offers useful insights into designing
weighting schemes with generalization guarantees (w.r.t. a prior distribution P over classiﬁers).
However  a direct application of existing results fails to yield a consistent weighted-majority scheme.
This is because PAC-Bayes results are primarily concerned with so-called Gibbs classiﬁers  which
in our context corresponds to predicting with a random classiﬁer hT drawn according to a weight-
distribution Q over subtrees of T0. Instead  we are interested in Q-weighted majority classiﬁers
hQ. Unfortunately the corresponding error R(hQ) can be twice the risk R(Q) = EhT ∼QR(hT ) of
the corresponding Gibbs classiﬁer: this then results (at best – see overview in Section 2.2) in an
excess risk of the form (R(hQ) − R(hB)) ≤ (R(Q) − R(hB)) + R(Q) = oP (1) + R(hB)  which 
similar to [5]  does not go to 0. So far  this problem is best addressed in PAC-Bayes results such as
the MinCq bound in [6  8] on R(hQ)  which is tighter in the presence of low correlation between
base classiﬁers. In contrast  our PAC-Bayes result applies even without low correlation between base
classiﬁers  and allows an excess risk oP (1) + (C/n) · minT log(1/P (T )) → 0 (Proposition 2). This
ﬁrst result is in fact of general interest since it extends beyond subtrees to any family of classiﬁers 
and is obtained by carefully combining existing arguments from PAC-Bayes analysis.
However  our basic PAC-Bayes result alone does not ensure convergence at the same rate as that
of the best pruning approaches. This requires designing a prior P that scales properly with the
size of subtrees T of T0. For instance  suppose P were uniform over all subtrees of T0  then
log(1/(P (T )) = Ω(n)  yielding a vacuous excess risk. We show through information-theoretic
arguments that an appropriate prior P can be designed to yield rates of convergence of the same
order as that of the best pruning of T0. In particular  our resulting weighting scheme maintains ideal
properties of pruning approaches such as adaptivity to the intrinsic dimension of data (see e.g. [9]).
Algorithmic contribution. We show that we can design a prior P which  while meeting the above
statistical constraints  yields posterior weights that decompose favorably over the leaves of a subtree T .
As a result of this decomposition  the weights of all subtrees can be recovered by simply maintaining
corresponding weights at the nodes of the original tree T0 for efﬁcient classiﬁcation in time O(log n)
(this is illustrated in Figure 1). We then propose an efﬁcient approach to obtain weights at the nodes
of T0  consisting of concurrent top-down and bottom-up dynamic programs that run in O(n) time.
These match the algorithmic complexity of the most efﬁcient pruning approaches  and thus offer a
practical alternative.
Our theoretical results are then veriﬁed in experiments over many real-world datasets. In particular
we show that our weighted-voting scheme achieves similar or better error than pruning on practical
problems  as suggested by our theoretical results.
Paper Organization. We start in Section 2 with theoretical setup and an overview of PAC-Bayes
analysis. This is followed in Section 3 with an overview of our statistical results  and in Section 4
with algorithmic results. Our experimental analysis is then presented in Section 5.

2 Preliminaries

2.1 Classiﬁcation setup
We consider a multiclass setup where the input X ⊂ X   for a bounded subset X of RD  possibly of
lower intrinsic dimension. For simplicity of presentation we assume X ⊂ [0  1]D (as in normalized
data). The output Y ⊂ [L]  where we use the notation [L] = {1  2  . . .   L} for L ∈ N.
We are to learn a classiﬁer h : X (cid:55)→ [L]  given an i.i.d. training sample {Xi  Yi}2n
an unknown distribution over X  Y . Throughout  we let S .
i=1 and S0
which will serve later to simplify dependencies in our analysis.
Our performance measure is as follows.
Deﬁnition 1. The risk of a classiﬁer h is given as R(h) = E[h(X) (cid:54)= Y ]. This is minimized by
P (Y = l|X = x). Therefore  for any classiﬁer ˆh learned
the Bayes classiﬁer hB(x)
over a sample {Xi  Yi}i  we are interested in the excess-risk E(ˆh)

i=1 of size 2n  from
= {Xi  Yi}2n
.
i=n+1 

= R(ˆh) − R(hB).
.

= {Xi  Yi}n

.
= argmaxl∈[L]

2

Figure 1: A partition tree T0 over input space X   and a query x ∈ X to classify. The leaves of T0 are the 4
cells shown left  and the root is X . A query x follows a single path (shown in bold) from the root down to a leaf.
A key insight towards efﬁcient weighted-voting is that this path visits all leaves (containing x) of any subtree of
T0. Therefore  weighted voting might be implemented by keeping a weight w(A) at any node A along the path 
where w(A) aggregates the weights Q(T ) of every subtree T that has A as a leaf. This is feasible if we can
restrict Q(T ) to be multiplicative over the leaves of T   without trading off accuracy.

Here we are interested in aggregations of classiﬁcation trees  deﬁned as follows.
Deﬁnition 2. A hierarchical partition or (space) partition-tree T of X is a collection of nested
partitions of X ; this is viewed as a tree where each node is a subset A of X   each child A(cid:48) of
a node A is a subset of A  and whose collection of leaves  denoted π(T )  is a partition of X . A
classiﬁcation tree hT on X is a labeled partition-tree T of X : each leaf A ∈ π(T ) is assigned a
label l = l(A) ∈ [L]; the classiﬁcation rule is simply hT (x) = l(A) for any x ∈ A.
Given an initial tree T0  we will consider only subtrees T of T0 that form a hierarchical partition of
X   and we henceforth use the term subtrees (of T0) without additional qualiﬁcation.
Finally  aggregation (of subtrees of T0) consists of majority-voting as deﬁned below.
Deﬁnition 3. Let H denote a discrete family of classiﬁers h : X (cid:55)→ [L]  and let Q denote a
distribution over H. The Q-majority classiﬁer hQ

= hQ(H) is one satisfying for any x ∈ X
.

(cid:88)

hQ(x) = argmax

l∈[L]

h∈H h(x)=l

Q(h).

Our oracle rates of Theorem 1 requires no additional assumptions; however  the resulting corollary is
stated under standard distributional conditions that characterize convergence rates for tree-prunings.

2.2 PAC-Bayes Overview

PAC-Bayes analysis develops tools to bound the error of a Gibbs classiﬁer  i.e. one that randomly
samples a classiﬁer h ∼ Q over a family of classiﬁers H. In this work we are interested in families
{hT} deﬁned over subtrees of an initial tree T0. Here we present some basic PAC-Bayes result which
we extend for our analysis. While these results are generally presented for classiﬁcation risk R
(deﬁned above)  we keep our presentation generic  as we show later that a different choice of risk
leads to stronger results for R than what is possible through direct application of existing results.
Generic Setup. Consider a random vector Z  and an i.i.d sample Z[n] = {Zi}n
i=1. Let Z be the
support of Z  and L = {(cid:96)h : h ∈ H} be a loss class indexed by h ∈ H – discrete  and where
(cid:96)h : Z → [0  1]. For h ∈ H  the loss (cid:96)h induces the following risk and empirical counterparts:

RL(h)

.
= EZ(cid:96)h(Z) 

(cid:98)RL(h  Z[n])

n(cid:88)

i=1

.
=

1
n

(cid:96)h(Zi).

In particular  for the above classiﬁcation risk R  and Z (cid:44) (X  Y )  we have (cid:96)h(Z) = 1{h(X) (cid:54)= Y }.
Given a distribution Q over H  the risk (and empirical counterpart) of the Gibbs classiﬁer is then

RL(Q)

= Eh∼QRL(h) 
.

(cid:98)RL(Q  Z[n])

= Eh∼Q(cid:98)RL(h  Z[n]).

.

3

PAC-Bayesian results bound RL(Q) in terms of (cid:98)RL(Q  Z[n])  uniformly over any distribution Q 

provided a ﬁxed prior distribution P over H. We will build on the following form of [10] which
yields an upper-bound that is convex in Q (and therefore can be optimized for a good posterior Q∗).
Proposition 1 (PAC-Bayes on RL [10]). Fix a prior P supported on H  and let n ≥ 8 and δ ∈ (0  1).
With probability at least 1 − δ over Z[n]  simultaneously for all λ ∈ (0  2) and all posteriors Q over
H:

RL(Q) ≤ (cid:98)RL(Q  Z[n])

1 − λ/2

√
Dkl (Q(cid:107)P ) + log (2
λ(1 − λ/2)n

+

n/δ)

 

.
= EQlog Q(h)

where Dkl (Q(cid:107)P )
Choice of posterior Q∗. Let Q∗ minimize the above upper-bound  and let h∗ minimize RL over H.
Then  by letting Qh∗ put all mass on h∗  we automatically get that  with probability at least 1 − 2δ:

P (h) is the Kullback-Leibler divergence between Q and P .

(cid:18)(cid:98)RL(h∗  Z[n]) +

log(1/P (h∗)) + log(n/δ)

(cid:114)

n

(cid:19)

(cid:33)

RL(Q∗) ≤ RL(Qh∗ ) ≤ C ·

(cid:32)

log(1/P (h∗)) + log(n/δ)

log(1/δ)

≤ C ·

RL(h∗) +

(1)

where the last inequality results from bounding |RL(h∗) − (cid:98)RL(h∗  Z[n])| using Chernoff.

n

n

+

 

Unfortunately  such direct application is not enough for our purpose when RL = R. We want
to bound the excess risk E(hQ) for a Q-majority classiﬁer hQ over h(cid:48)s ∈ H. It is known that
R(hQ) ≤ 2R(Q) which yields a bound of the form (1) on R(hQ∗ ); however this implies at best
that R(hQ∗ ) → 2R(hB) even if E(h∗) → 0 (which is generally the case for optimal tree-pruning
h∗
T [9]). This is a general problem in converting from Gibbs error to that of majority-voting  and is
studied for instance in [6  8] where it is shown that R(hQ) can actually be smaller in some situations.
Improved choice of Q∗. Here  we want to design Q∗ such that R(hQ∗ ) → R(hB) (i.e. E(hQ∗ ) →
T ) → 0 always. Our solution relies on a proper choice of loss (cid:96)h
0) at the same rate as E(h∗
that relates most directly to excess risk E that the 0-1 loss 1{h(x) (cid:54)= y}. A ﬁrst candidate is to
= 1{h(x) (cid:54)= y} − 1{hB(x) (cid:54)= y} since E(h) = E eh(X  Y ); however
.
deﬁne (cid:96)h(x  y) as eh(x  y)
eh(x  y) /∈ [0  1] and can take negative values. This is resolved by considering an intermediate loss
eh(x) = EY |xeh(x  Y ) ∈ [0  1] to be related back to eh(x  y) by integration in a suitable order.

3 Statistical results

3.1 Basic PAC-Bayes result

We start with the following intermediate loss family over classiﬁers h  w.r.t. the Bayes classiﬁer hB.
Deﬁnition 4. Let eh(x  y)

= 1{h(x) (cid:54)= y} − 1{hB(x) (cid:54)= y}  and eh(x) = EY |xeh(x  Y )  and
.
.
=

eh(Xi)  and (cid:98)E(h S)

(cid:101)E(h S)

n(cid:88)

n(cid:88)

eh(Xi  Yi).

.
=

1
n

i=1

1
n

i=1

Our ﬁrst contribution is a basic PAC-Bayes result which the rest of our analysis builds on.
Proposition 2 (PAC-Bayes on excess risk). Let H denote a discrete family of classiﬁers  and ﬁx
a prior distribution P with support H. Let n ≥ 8 and δ ∈ (0  1). Suppose  there exists bounded

(cid:17) ≥ 1 − δ 

For any λ ∈ (0  2)  consider the following posterior over H:

functions (cid:98)∆n(h S)  ∆n(h)  h ∈ H (depending on δ) such that
P(cid:16)∀h ∈ H  (cid:101)E(h S) ≤ (cid:98)E(h S) +(cid:98)∆n(h S)
e−nλ((cid:98)R(h S)+(cid:98)∆n(h S))P (h) 
E(h) + ∆n(h) +

Q∗
λ(h) =

E(hQ∗

) ≤

1
c

L

log(1/P (h))

λ

1 − λ/2

inf
h∈H

λn

P(cid:16)(cid:98)∆n(h S) ≤ ∆n(h)
for c = Eh∼P e−nλ((cid:98)R(h S)+(cid:98)∆n(h S)).

inf
h∈H

(cid:17) ≥ 1 − δ.

(cid:113)

log 2

+

√
n
δ + λ
λn

2n log 1
δ

(2)

 .

Then  with probability at least 1 − 4δ over S  simultaneously for all λ ∈ (0  2):

4

Proposition 2 builds on Proposition 1 by ﬁrst taking RL(h) to be E(h)  (cid:98)RL(h) to be (cid:101)E(h)  and Z

to be X. The bound in Proposition 2 is then obtained by optimizing over Q for ﬁxed λ. Since this
bound is on excess error (rather than error)  optimizing over λ can only improve constants  while the
choice of prior P is crucial in obtaining optimal rates as |H| → ∞. Such choice is treated next.
3.2 Oracle risk for trees (as H .
We start with the following deﬁnitions on classiﬁers of interest and related quantities.
Deﬁnition 5. Let T0 be a binary partition-tree of X obtained from data S0  of depth D0. Consider a
family of classiﬁcation trees H(T0)
= {hT} indexed by subtrees T of T0  and where hT deﬁnes a
.
ﬁxed labeling l(A) of nodes A ∈ π(T )  e.g.  l(A)
Furthermore  for any node A of T0  let ˆp(A S) denote the empirical mass of A under S and p(A) be
the population mass. Then for any subtree T of T0  let |T| be the number of nodes in T and deﬁne

= majority label in Y if A ∩ S0 (cid:54)= ∅.
.

= H(T0) grows in size with T0)

(cid:114)

(cid:88)

A∈π(T )

(cid:98)∆n(hT  S)
(cid:115)
(cid:88)

.
=

(cid:18)

ˆp(A S)

2 log(|T0| /δ)

n

  and

(cid:19) log(|T0| /δ)

(3)

.

(4)

∆n(hT )

.
=

A∈π(T )

8 max

p(A) 

(2 + log D) · D0 + log(1/δ)

n

n

Remark 1. In practice  we might start with a space partitioning tree T (cid:48)
0 (e.g.  a dyadic tree  or
KD-tree) which partitions [0  1]D  rather than the support X . We then view T0 as the intersection of
0 with X .
T (cid:48)
deﬁnition of (cid:98)∆n(hT  S) and ∆n(hT ) satisﬁes the conditions of Proposition 2  and (b) that there
Our main theorem below follows from Proposition 2 on excess risk  by showing (a) that the above
be a proper distribution (i.e.(cid:80)
exists a proper prior P such that log(1/P (T )) ∼ |π(T )|  i.e.  depends just on the subtree complexity
rather than on that of T0. The main technicality in showing (b) stems from the fact that P needs to
T P (T ) = 1) without requiring too large a normalization constant
(remember that the number of subtrees can be exponential in the size of T0). This is established
through arguments from coding theory  and in particular Kraft-McMillan inequality.
= (1/CP )e−3D0·|π(T )| for a nor-
.
Theorem 1 (Oracle risk for trees). Let the prior satisfy P (hT )
malizing constant CP   and consider the corresponding posterior Q∗
λ as deﬁned in Equation 2  such
that  with probability at least 1 − 4δ over S  for all λ ∈ (0  2)  the excess risk E(hQ∗
) of the
majority-classiﬁer is at most

λ

(cid:0)

(cid:1) · min

hT ∈H(T0)

L

1 − λ/2

E(hT ) + ∆n(hT ) +

3D0 · |π(T )|

λn

+

log 2

√
n
δ + λ
λn

2n log 1
δ

(cid:113)

 .

From Theorem 1 we can deduce that the majority classiﬁer hQ∗
is consistent whenever the approach
of pruning to the best subtree is consistent (typically  minhT E(hT ) + (D0 |π(T )|)/n = oP (1)).
Furthermore  we can infer that E(hQ∗
) converges at the same rate as pruning approaches: the terms
∆n(hT ) and D0 · |π(T )|/n can be shown to be typically  of lower or similar order as E(hT ) for the
best subtree classiﬁer hT . These remarks are formalized next and result in Corollary 1 below.

λ

λ

3.3 Rate of convergence

Much of known rates for tree-pruning are established for dyadic trees (see e.g. [9  11])  due to their
simplicity  under nonparametric assumptions on E[Y |X]. Thus  we adopt such standard assumptions
here to illustrate the rates achievable by hQ∗
  following the more general statement of Theorem 1.
The ﬁrst standard assumption below restricts how fast class probabilities change over space.
Assumption 1. Consider the so-called regression function η(x) ∈ RL with coordinate ηl(x)
EY |x1{Y = l}   l ∈ [L]. We assume η is α-Hölder for α ∈ (0  1]  i.e. 

.
=

λ

∃λ such that ∀x  x(cid:48) ∈ X  

(cid:107)η(x) − η(x(cid:48))(cid:107) ≤ λ(cid:107)x − x(cid:48)(cid:107)α .

5

Next  we illustrate some of the key conditions veriﬁed by dyadic trees which standard results build
on. In particular  we want the diameters of nodes of T0 to decrease relatively fast from the root down.
Assumption 2 (Conditions on T0). The tree T0 is obtained as the intersection of X with dyadic
partition of [0  1]D (e.g. by cycling though coordinates) of depth D0 = O(D log n) and partition size
|T0| = O(n). In particular  we emphasize that the following conditions on subtrees then hold.
For any subtree T of T0  let r(T ) denote the maximum diameter of leaves of T (viewed as subsets of
X ). There exist C1  C2  d > 0 such that:
For all (C1/n) < r ≤ 1  there exists a subtree T of T0 such that r(T ) ≤ r and |π(T )| ≤ C2r−d.
The above conditions on subtrees are known to approximately hold for other procedures such as
KD-trees  and PCA-trees; in this sense  analyses of dyadic trees do yield some insights into the
performance other approaches. The quantity d captures the intrinsic dimension (e.g.  doubling or box
dimension) of the data space X or is often of the same order [12  13  14].
Under the above two assumptions  it can be shown through standard arguments that the excess error
of the best pruning  namely minhT ∈H(T0) E(hT ) is of order n−α/(2α+d)  which is tight (see e.g.
minimax lower-bounds of [15]). The following corollary to Theorem 1 states that such a rate  up to a
logarithmic factor of n  is also attained by majority classiﬁcation under Q∗
λ.
Corollary 1 (Adaptive rate of convergence). Assume that for any cell A of T0  the labeling l(A)
corresponds to the majority label in A (under S0) if A ∩ S0 (cid:54)= ∅  or l(A) = 1 otherwise. Then  under
Assumptions 1 and 2  and the conditions of Theorem 1  there exists a constant C such that:

(cid:18) log n

(cid:19)α/(2α+d)

.

n

ES0 SE(hQ∗

λ

) ≤ C

4 Algorithmic Results

=(cid:80)

.

(cid:80)

Here we show that hQ can be efﬁciently implemented by storing appropriate weights at nodes of
hT :A∈π(T ) Q(hT ) aggregate weights over all subtrees T of T0 having A as a
T0. Let wQ(A)
leaf. Then hQ(x) = argmaxl∈[L]
A∈path(x) l(A)=l wQ(A)  where path(x) denotes all nodes of T0
containing x. Thus  hQ(x) is computable from weights proportional to wQ(A) at every node.
We show in what follows that we can efﬁciently obtain w(A) = C·wQ∗
by ensuring that Q∗
Theorem 1: we have Q∗

(A) by dynamic-programming
λ(hT ) is multiplicative over π(T ). This is the case  given our choice of prior from

) · exp((cid:80)

A∈π(T ) φ(A)) where

λ(hT ) = (1/CQ∗

λ

λ

φ(A)

= −λ
.

1{Yi (cid:54)= l(A)} − nλ

ˆp(A S)

2 log(|T0| /δ)

− 3D0.

(cid:114)

(cid:88)

i:Xi∈A∩S

.
= CQ∗

· wQ∗

We can then compute w(A)
(A) via dynamic-programming. The intuition is similar
to that in [5]  however  the particular form of our weights require a two-pass dynamic program
(bottom-up and top-down) rather than the single pass in [5]. Namely  w(A) divides into subweights
that any node A(cid:48) might contribute up or down the tree. Let

λ

λ

α(A)

.
=

exp

φ(A(cid:48))

 

(5)

(cid:88)

(cid:18) (cid:88)

hT :A∈π(T )

A(cid:48)(cid:54)=A A(cid:48)∈π(T )

so that w(A) = eφ(A) · α(A). As we will show (proof of Theorem 2)  α(A) decomposes into
contributions from the parent Ap and sibling As of A  i.e.  α(A) = α(Ap)β(As) where β(As) is
given as (writing T A

0 for the subtree of T0 rooted at A  and T (cid:22) T (cid:48) when T is a subtree of T (cid:48)):

n

(cid:19)

(cid:88)

(cid:18) (cid:88)

(cid:19)

φ(A(cid:48))

.

β(As) =

exp

T(cid:22)T As

0

A(cid:48)∈π(T )

(6)

The contributions β(A) are ﬁrst computed using the bottom-up Algorithm 1  and the contributions
α(A) and ﬁnal weights w(A) are then computed using the top-down Algorithm 2. For ease of
presentation  these routines run on a full-binary tree version ¯T0 of T0  obtained by adding a dummy
child to each node A that has a single child in T0. Each dummy node A(cid:48) has φ(A(cid:48)) = 0.

6

Algorithm 1 Bottom-up pass

for A ∈ π( ¯T0) do
β(A) ← eφ(A)
end for
for i ← D0 to 0 do

Ai ← set of nodes of ¯T0 at depth i
for A ∈ Ai \ π( ¯T0) do
N ← the children nodes of A

β(A) ← eφ(A) +(cid:81)

A(cid:48)∈N β(A(cid:48))

end for

end for

Algorithm 2 Top-down pass

α(root) ← 1
for i ← 1 to D0 do
Ai ← set of nodes of ¯T0 at depth i
for A ∈ Ai do

Ap  As ← parent of node A  sibling of node A
α(A) ← α(Ap)β(As)
w(A) ← eφ(A)α(A)

end for

end for

Theorem 2 (Computing w(A)). Running Algorithm 1  then 2  we obtain w(A)
where Q∗
2| ¯T0| ≤ 4|T0|  where |T| is the number of nodes in T .

(A) 
λ is as deﬁned in Theorem 1. Furthermore  the combined runtime of Algorithms 1  then 2 is

λ

λ

.
= CQ∗

· wQ∗

5 Experiments

Table 1: UCI datasets

Name (abbreviation)
Spambase (spam)
EEG Eye State (eeg)
Epileptic Seizure Recognition (epileptic)
Crowdsourced Mapping (crowd)
Wine Quality (wine)
Optical Recognition of Handwritten Digits (digit)
Letter Recognition (letter)

Features count
57
14
178
28
12
64
16

Labels count
2
2
2
6
11
10
26

Train size
2601
12980
9500
8546
4497
3620
18000

.

n

|π(T S)|

(cid:113)
max(cid:0)ˆp(A S) 

Here we present experiments on real-world datasets  for two common partition-tree approaches 
dyadic trees and KD-trees. The various datasets are described in Table 1.
The main baseline we compare against  is a popular efﬁcient pruning heuristic where a subtree of T0

is selected to minimize the penalized error C1(hT ) = (cid:98)R(hT  S) + λ
C2(hT ) = (cid:98)R(hT  S) + λ(cid:80)

We also compare against other tree-based approaches that are theoretically driven and efﬁcient.
First is a pruning approach proposed in [16]  which picks a subtree minimizing the penalized error
n   where (cid:107)A(cid:107) denotes the depth of node
A in T0. We note that  here we choose a form of C2 that avoids theoretical constants that were of a
technical nature  but instead let λ account for such. We report this approach as SN-pruning. Second
is the majority classiﬁer of [5]  which however is geared towards binary classiﬁcation as it requires
regression-type estimates in [0  1] at each node. This is denoted HS-vote.
All the above approaches have efﬁcient dynamic programs that run in time O(|T0|)  and all predict in
time O(height(T0)). The same holds for our PAC-Bayes approach as discussed above in Section 4.
Practical implementation of PAC-Bayes tree. Our implementation rests on the theoretical in-
sights of Theorem 1  however we avoid some of the technical details that were needed for rigor 

(cid:1) · (cid:107)A(cid:107)

(cid:107)A(cid:107)
n

A

7

(cid:113) ˆp(A S)

A∈π(T S)

such as sample splitting and overly conservative constants in concentration results. Instead we
advise cross-validating for such constants in the prior and posterior deﬁnitions. Namely  we ﬁrst
set P (hT ) ∝ exp(−|π(T S)|)  where π(T S) denotes the leaves of T containing data. We set

. The posterior is then set as Q∗(hT ) ∝ exp(−n(λ1(cid:98)R(hT  S) +

∆n(hT  S) =(cid:80)

n

λ2∆n(hT  S)))P (hT )  where λ1  λ2 account for concentration terms to be tuned to the data.
Finally  we use the entire data to construct T0 and compute weights  i.e.  S0 = S  as inter-
dependencies are in fact less of an issue in practice. We note  that the above alternative theoretical
approaches  SN-pruning and HS-vote  are also assumed (in theory) to work on a sample independent
choice of T0 (or equivalently built and labeled on a separate sample S0)  but are implemented here on
the entire data to similarly take advantage of larger data sizes. The baseline pruning heuristic is by
default always implemented on the full data.
Experimental setup and results. The data is preprocessed as follows: for dyadic trees  data is scaled
to be in [0  1]D  while for KD-trees data is normalized accross each coordinate by standard deviation.
Testing data is ﬁxed to be of size 2000  while each experiment is ran 5 times (with random choice of
training data of size reported in Table 1) and average performance is reported. In each experiment 
all parameters are chosen by 2-fold cross-validation for each of the procedures. The log-grid is 10
values  equally spaced in logarithm  from 2−8 to 26 while the linear-grid is 10 linearly-spaced values
between half the best value of the log-search and twice the best value of the log-search.
Table 2 reports classiﬁcation performance of the various theoretical methods relative to the baseline
pruning heuristic. We see that proposed PAC-Bayes tree achieves competitive performance against all
other alternatives. All the approaches have similar performance accross datasets  with some working
slightly better on particular datasets. Figure 2 further illustrates typical performance on multiclass
problems as training size varies.

Table 2: Ratio of classiﬁcation error over that of the default pruning baseline: bold indicates best results across
methods  while blue indicates improvement over baseline; N/A means the algorithm was not run on the task.

T0 ≡ dyadic tree

Dataset
spam
eeg
epileptic
crowd
wine
digit
letter

SN-pruning
1.118
0.979
0.993
0.991
1.035
1.000
1.005

PAC-Bayes tree HS-vote
0.975
0.993
0.992
1.020
0.991
0.936
0.993

1.224
1.029
0.951
N/A
N/A
N/A
N/A

SN-pruning
1.048
1.000
0.977
1.001
1.010
0.994
1.000

T0 ≡ KD tree
PAC-Bayes tree HS-vote
1.020
0.990
0.987
1.017
0.997
0.997
1.001

1.075
1.000
0.907
N/A
N/A
N/A
N/A

Figure 2: Classiﬁcation error versus training size

8

References
[1] David A McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999.

[2] László A Székely and Hua Wang. On subtrees of trees. Advances in Applied Mathematics  34(1):138–155 

2005.

[3] Trevor Hastie and Daryl Pregibon. Shrinking trees. AT & T Bell Laboratories  1990.

[4] Wray Buntine and Tim Niblett. A further comparison of splitting rules for decision-tree induction. Machine

Learning  8(1):75–85  1992.

[5] David P Helmbold and Robert E Schapire. Predicting nearly as well as the best pruning of a decision tree.

Machine Learning  27(1):51–68  1997.

[6] Alexandre Lacasse  François Laviolette  Mario Marchand  Pascal Germain  and Nicolas Usunier. PAC-
Bayes bounds for the risk of the majority vote and the variance of the Gibbs classiﬁer. In Advances in
Neural information processing systems  pages 769–776  2007.

[7] John Langford and John Shawe-Taylor. PAC-Bayes & margins.

processing systems  pages 439–446  2003.

In Advances in neural information

[8] Pascal Germain  Alexandre Lacasse  Francois Laviolette  Mario Marchand  and Jean-Francis Roy. Risk
Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm. Journal of
Machine Learning Research  16:787–860  2015.

[9] C. Scott and R.D. Nowak. Minimax-optimal classiﬁcation with dyadic decision trees. IEEE Transactions

on Information Theory  52  2006.

[10] Niklas Thiemann  Christian Igel  Olivier Wintenberger  and Yevgeny Seldin. A Strongly Quasiconvex
PAC-Bayesian bound. In Steve Hanneke and Lev Reyzin  editors  Proceedings of the 28th International
Conference on Algorithmic Learning Theory  volume 76 of Proceedings of Machine Learning Research 
pages 466–492  Kyoto University  Kyoto  Japan  15–17 Oct 2017. PMLR.

[11] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution Free Theory of Nonparametric Regression.

Springer  New York  NY  2002.

[12] Nakul Verma  Samory Kpotufe  and Sanjoy Dasgupta. Which spatial partition trees are adaptive to intrinsic
dimension? In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  pages
565–574. AUAI Press  2009.

[13] Samory Kpotufe and Sanjoy Dasgupta. A tree-based regressor that adapts to intrinsic dimension. Journal

of Computer and System Sciences  78(5):1496–1515  2012.

[14] Santosh Vempala. Randomly-oriented kd trees adapt to intrinsic dimension. In FSTTCS  volume 18  pages

48–57. Citeseer  2012.

[15] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classiﬁers. The Annals of

Statistics  35(2):608–633  2007.

[16] Clayton Scott. Dyadic Decision Trees. PhD thesis  Rice University  2004.

9

,Tin Nguyen
Samory Kpotufe