2010,Random Projection Trees Revisited,The Random Projection Tree (RPTree) structures proposed in [Dasgupta-Freund-STOC-08] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPTree-Max and the RPTree-Mean data structures. Our result for RPTree-Max gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s >= 2. We also prove a packing lemma for this data structure. Our final result shows that low-dimensional manifolds possess bounded Local Covariance Dimension. As a consequence we show that RPTree-Mean adapts to manifold dimension as well.,Random Projection Trees Revisited

Aman Dhesi∗

Purushottam Kar

Department of Computer Science

Department of Computer Science and Engineering

Princeton University

Princeton  New Jersey  USA.
adhesi@princeton.edu

Indian Institute of Technology
Kanpur  Uttar Pradesh  INDIA.

purushot@cse.iitk.ac.in

Abstract

The Random Projection Tree (RPTREE) structures proposed in [1] are space par-
titioning data structures that automatically adapt to various notions of intrinsic
dimensionality of data. We prove new results for both the RPTREE-MAX and
the RPTREE-MEAN data structures. Our result for RPTREE-MAX gives a near-
optimal bound on the number of levels required by this data structure to reduce
the size of its cells by a factor s ≥ 2. We also prove a packing lemma for this data
structure. Our ﬁnal result shows that low-dimensional manifolds have bounded
Local Covariance Dimension. As a consequence we show that RPTREE-MEAN
adapts to manifold dimension as well.

1 Introduction

The Curse of Dimensionality [2] has inspired research in several directions in Computer Science and
has led to the development of several novel techniques such as dimensionality reduction  sketching
etc. Almost all these techniques try to map data to lower dimensional spaces while approximately
preserving useful information. However  most of these techniques do not assume anything about
the data other than that they are imbedded in some high dimensional Euclidean space endowed with
some distance/similarity function.

As it turns out  in many situations  the data is not simply scattered in the Euclidean space in a random
fashion. Often  generative processes impose (non-linear) dependencies on the data that restrict the
degrees of freedom available and result in the data having low intrinsic dimensionality. There exist
several formalizations of this concept of intrinsic dimensionality. For example  [1] provides an
excellent example of automated motion capture in which a large number of points on the body of
an actor are sampled through markers and their coordinates transferred to an animated avatar. Now 
although a large sample of points is required to ensure a faithful recovery of all the motions of the
body (which causes each captured frame to lie in a very high dimensional space)  these points are
nevertheless constrained by the degrees of freedom offered by the human body which are very few.

Algorithms that try to exploit such non-linear structure in data have been studied extensively re-
sulting in a large number of Manifold Learning algorithms for example [3  4  5]. These techniques
typically assume knowledge about the manifold itself or the data distribution. For example  [4]
and [5] require knowledge about the intrinsic dimensionality of the manifold whereas [3] requires a
sampling of points that is “sufﬁciently” dense with respect to some manifold parameters.

Recently in [1]  Dasgupta and Freund proposed space partitioning algorithms that adapt to the in-
trinsic dimensionality of data and do not assume explicit knowledge of this parameter. Their data
structures are akin to the k-d tree structure and offer guaranteed reduction in the size of the cells
after a bounded number of levels. Such a size reduction is of immense use in vector quantization [6]
and regression [7]. Two such tree structures are presented in [1] – each adapting to a different notion

∗Work done as an undergraduate student at IIT Kanpur

1

of intrinsic dimensionality. Both variants have already found numerous applications in regression
[7]  spectral clustering [8]  face recognition [9] and image super-resolution [10].

1.1 Contributions

The RPTREE structures are new entrants in a large family of space partitioning data structures such
as k-d trees [11]  BBD trees [12]  BAR trees [13] and several others (see [14] for an overview). The
typical guarantees given by these data structures are of the following types :

1. Space Partitioning Guarantee : There exists a bound L(s)  s ≥ 2 on the number of levels
one has to go down before all descendants of a node of size ∆ are of size ∆/s or less. The
size of a cell is variously deﬁned as the length of the longest side of the cell (for box-shaped
cells)  radius of the cell  etc.

2. Bounded Aspect Ratio : There exists a certain “roundedness” to the cells of the tree - this
notion is variously deﬁned as the ratio of the length of the longest to the shortest side of the
cell (for box-shaped cells)  the ratio of the radius of the smallest circumscribing ball of the
cell to that of the largest ball that can be inscribed in the cell  etc.

3. Packing Guarantee : Given a ﬁxed ball B of radius R and a size parameter r  there exists a
bound on the number of disjoint cells of the tree that are of size greater than r and intersect
B. Such bounds are usually arrived at by ﬁrst proving a bound on the aspect ratio for cells
of the tree.

These guarantees play a crucial role in algorithms for fast approximate nearest neighbor searches
[12] and clustering [15]. We present new results for the RPTREE-MAX structure for all these types
of guarantees. We ﬁrst present a bound on the number of levels required for size reduction by any
given factor in an RPTREE-MAX. Our result improves the bound obtainable from results presented
in [1]. Next  we prove an “effective” aspect ratio bound for RPTREE-MAX. Given the randomized
nature of the data structure it is difﬁcult to directly bound the aspect ratios of all the cells. Instead
we prove a weaker result that can nevertheless be exploited to give a packing lemma of the kind
mentioned above. More speciﬁcally  given a ball B  we prove an aspect ratio bound for the smallest
cell in the RPTREE-MAX that completely contains B.
Our ﬁnal result concerns the RPTREE-MEAN data structure. The authors in [1] prove that this
structure adapts to the Local Covariance Dimension of data (see Section 5 for a deﬁnition). By
showing that low-dimensional manifolds have bounded local covariance dimension  we show its
adaptability to the manifold dimension as well. Our result demonstrates the robustness of the notion
of manifold dimension - a notion that is able to connect to a geometric notion of dimensionality such
as the doubling dimension (proved in [1]) as well as a statistical notion such as Local Covariance
Dimension (this paper).

Due to lack of space we relegate some proofs to the Supplementary Material document and present
proofs of only the main theorems here. All results cited from other papers are presented as Facts in
this paper. We will denote by B(x  r)  a closed ball of radius r centered at x. We will denote by d 
the intrinsic dimensionality of data and by D  the ambient dimensionality (typically d ≪ D).
2 The RPTREE-MAX structure

The RPTREE-MAX structure adapts to the doubling dimension of data (see deﬁnition below). Since
low-dimensional manifolds have low doubling dimension (see [1] Theorem 22) hence the structure
adapts to manifold dimension as well.
Deﬁnition 1. The doubling dimension of a set S ⊂ RD is the smallest integer d such that for any
ball B(x  r) ⊂ RD  the set B(x  r) ∩ S can be covered by 2d balls of radius r/2.
The RPTREE-MAX algorithm is presented data imbedded in RD having doubling dimension d. The
algorithm splits data lying in a cell C of radius ∆ by ﬁrst choosing a random direction v ∈ RD 
projecting all the data inside C onto that direction  choosing a random value δ in the range [−1  1] ·
6∆/√D and then assigning a data point x to the left child if x · v < median({z · v : z ∈ C}) + δ
and the right child otherwise. Since it is difﬁcult to get the exact value of the radius of a data set 

2

the algorithm settles for a constant factor approximation to the value by choosing an arbitrary data
point x ∈ C and using the estimate ˜∆ = max({kx − yk : y ∈ C}).
The following result is proven in [1] :
Fact 2 (Theorem 3 in [1]). There is a constant c1 with the following property. Suppose an RPTREE-
MAX is built using a data set S ⊂ RD . Pick any cell C in the RPTREE-MAX; suppose that
S ∩ C has doubling dimension ≤ d. Then with probability at least 1/2 (over the randomization in
constructing the subtree rooted at C)  every descendant C′ more than c1d log d levels below C has
radius(C′) ≤ radius(C)/2.
In Sections 2  3 and 4  we shall always assume that the data has doubling dimension d and shall
not explicitly state this fact again and again. Let us consider extensions of this result to bound the
number of levels it takes for the size of all descendants to go down by a factor s > 2. Let us analyze
the case of s = 4. Starting off in a cell C of radius ∆  we are assured of a reduction in size by a
factor of 2 after c1d log d levels. Hence all 2c1d log d nodes at this level have radius ∆/2 or less. Now
we expect that after c1d log d more levels  the size should go down further by a factor of 2 thereby
giving us our desired result. However  given the large number of nodes at this level and the fact
that the success probability in Fact 2 is just greater than a constant bounded away from 1  it is not
possible to argue that after c1d log d more levels the descendants of all these 2c1d log d nodes will be
of radius ∆/4 or less. It turns out that this can be remedied by utilizing the following extension of
the basic size reduction result in [1]. We omit the proof of this extension.
Fact 3 (Extension of Theorem 3 in [1]). For any δ > 0  with probability at least 1− δ  every descen-
dant C′ which is more than c1d log d + log(1/δ) levels below C has radius(C′) ≤ radius(C)/2.
This gives us a way to boost the conﬁdence and do the following : go down L = c1d log d + 2 levels
from C to get the the radius of all the 2c1d log d+2 descendants down to ∆/2 with conﬁdence 1− 1/4.
Afterward  go an additional L′ = c1d log d + L + 2 levels from each of these descendants so that
for any cell at level L  the probability of it having a descendant of radius > ∆/4 after L′ levels is
less than 1
2 that all descendants
of C after 2L + c1d log d + 2 have radius ≤ ∆/4. This gives a way to prove the following result :
Theorem 4. There is a constant c2 with the following property. For any s ≥ 2  with probability at
least 1−1/4  every descendant C′ which is more than c2·s·d log d levels below C has radius(C′) ≤
radius(C)/s.

4·2L . Hence conclude with conﬁdence at least 1 − 1

4·2L · 2L ≥ 1

4 − 1

Proof. Refer to Supplementary Material

Notice that the dependence on the factor s is linear in the above result whereas one expects it to
be logarithmic. Indeed  typical space partitioning algorithms such as k-d trees do give such guar-
antees. The ﬁrst result we prove in the next section is a bound on the number of levels that is
poly-logarithmic in the size reduction factor s.

3 A generalized size reduction lemma for RPTREE-MAX

In this section we prove the following theorem :
Theorem 5 (Main). There is a constant c3 with the following property. Suppose an RPTREE-MAX
is built using data set S ⊂ RD . Pick any cell C in the RPTREE-MAX; suppose that S ∩ C
has doubling dimension ≤ d. Then for any s ≥ 2  with probability at least 1 − 1/4 (over the
randomization in constructing the subtree rooted at C)  for every descendant C′ which is more than
c3 · log s · d log sd levels below C  we have radius(C′) ≤ radius(C)/s.
Compared to this  data structures such as [12] give deterministic guarantees for such a reduction in
D log s levels which can be shown to be optimal (see [1] for an example). Thus our result is optimal
but for a logarithmic factor. Moving on with the proof  let us consider a cell C of radius ∆ in the
RPTREE-MAX that contains a dataset S having doubling dimension ≤ d. Then for any ǫ > 0  a
repeated application of Deﬁnition 1 shows that the S can be covered using at most 2d log(1/ǫ) balls
of radius ǫ∆. We will cover S ∩ C using balls of radius
so that O(cid:0)(sd)d(cid:1) balls would
sufﬁce. Now consider all pairs of these balls  the distance between whose centers is ≥ ∆
960s√d

s − ∆

960s√d

.

∆

3

neutral split

good split

∆

B1

B2

bad split

Figure 1: Balls B1 and B2 are of radius ∆/s√d and their centers are ∆/s − ∆/s√d apart.

If random splits separate data from all such pairs of balls i.e. for no pair does any cell contain data
from both balls of the pair  then each resulting cell would only contain data from pairs whose centers
are closer than ∆

. Thus the radius of each such cell would be at most ∆/s.

s − ∆

960s√d

We ﬁx such a pair of balls calling them B1 and B2. A split in the RPTREE-MAX is said to be good
with respect to this pair if it sends points inside B1 to one child of the cell in the RPTREE-MAX
and points inside B2 to the other  bad if it sends points from both balls to both children and neutral
otherwise (See Figure 1). We have the following properties of a random split :
Lemma 6. Let B = B(x  δ) be a ball contained inside an RPTREE-MAX cell of radius ∆ that
contains a dataset S of doubling dimension d. Lets us say that a random split splits this ball if the
split separates the data set S into two parts. Then a random split of the cell splits B with probability
atmost 3δ√d
∆ .

Proof. Refer to Supplementary Material

Lemma 7. Let B1 and B2 be a pair of balls as described above contained in the cell C that contains
data of doubling dimension d. Then a random split of the cell is a good split with respect to this pair
with probability at least 1

56s .

Proof. Refer to Supplementary Material. Proof similar to that of Lemma 9 of [1].

Lemma 8. Let B1 and B2 be a pair of balls as described above contained in the cell C that contains
data of doubling dimension d. Then a random split of the cell is a bad split with respect to this pair
with probability at most

1

320s .

Proof. The proof of a similar result in [1] uses a conditional probability argument. However the
technique does not work here since we require a bound that is inversely proportional to s. We instead
make a simple observation that the probability of a bad split is upper bounded by the probability that
one of the balls is split since for any two events A and B  P [A ∩ B] ≤ min{P [A]   P [B]}. The
result then follows from an application of Lemma 6.

We are now in a position to prove Theorem 5. What we will prove is that starting with a pair of balls
in a cell C  the probability that some cell k levels below has data from both the balls is exponentially
small in k. Thus  after going enough number of levels we can take a union bound over all pairs of

balls whose centers are well separated (which are O(cid:0)(sd)2d(cid:1) in number) and conclude the proof.
contained inside C with radii ∆/960s√d and centers separated by at least ∆/s − ∆/960s√d. Let

Proof. (of Theorem 5) Consider a cell C of radius ∆ in the RPTREE-MAX and ﬁx a pair of balls

4

j denote the probability that a cell i levels below C has a descendant j levels below itself that
pi
contains data points from both the balls. Then the following holds :

Lemma 9. p0

68s(cid:1)l
k ≤ (cid:0)1 − 1

pl
k−l.

Proof. Refer to Supplementary Material. Proof similar to that of Lemma 11 of [1].

k ≤ (cid:0)1 − 1

68s(cid:1)k as a corollary. However using this result would require us
Note that this gives us p0
Ω((sd)2d) which results in a bound that is worse
to go down k = Ω(sd log(sd)) levels before p0
(by a factor logarithmic in s) than the one given by Theorem 4. This can be attributed to the small
probability of a good split for a tiny pair of balls in large cells. However  here we are completely
neglecting the fact that as we go down the levels  the radii of cells go down as well and good splits
become more frequent.

k =

1

s/2 then the good and bad split probabilities are 1

Indeed setting s = 2 in Theorems 7 and 8 tells us that if the pair of balls were to be contained in a
cell of radius ∆
640 respectively. This paves
way for an inductive argument : assume that with probability > 1 − 1/4  in L(s) levels  the size of
all descendants go down by a factor s. Denote by pl
g the probability of a good split in a cell at depth
b the corresponding probability of a bad split. Set l∗ = L(s/2) and let E be the event that
l and by pl
the radius of every cell at level l∗ is less than ∆

s/2 . Let C′ represent a cell at depth l∗. Then 

112 and 1

pl∗
g ≥ P [good split in C′|E] · P [E] ≥
pl∗
b = P [bad split in C′|E] · P [E] + P [bad split in C′|¬E] · P [¬E]

1

112 ·(cid:18)1 −

1

4(cid:19) ≥

1
150

≤

1
640 ·

1
640 · 1 +

1
4 ≤
Notice that now  for any m > 0  we have pl∗
k = l∗ + c4d log(sd) and applying Lemma 9 gives us p0
4(sd)2d . Thus we have

m ≤ (cid:0)1 − 1

512

1

1

213(cid:1)m. Thus  for some constant c4  setting
213(cid:1)c4d log(sd)
k ≤ (cid:0)1 − 1
≤

(cid:0)1 − 1

68s(cid:1)l∗

L(s) ≤ L(s/2) + c4d log(sd)

which gives us the desired result on solving the recurrence i.e. L(s) = O (d log s log sd).
4 A packing lemma for RPTREE-MAX

In this section we prove a probabilistic packing lemma for RPTREE-MAX. A formal statement of
the result follows :
Theorem 10 (Main). Given any ﬁxed ball B(x  R) ⊂ RD  with probability greater than 1/2 (where
the randomization is over the construction of the RPTREE-MAX)  the number of disjoint RPTREE-
r (cid:1)O(d log d log(dR/r)).
MAX cells of radius greater than r that intersect B is at most (cid:0) R
r (cid:1)O(1)
r (cid:1)D which behaves like (cid:0) R
Data structures such as BBD-trees give a bound of the form O(cid:0) R
r (cid:1)O(log R
r ) for ﬁxed d. We will prove the
for ﬁxed D. In comparison  our result behaves like (cid:0) R
result in two steps : ﬁrst of all we will show that with high probability  the ball B will be completely
inscribed in an RPTREE-MAX cell C of radius no more than O(cid:16)Rd√d log d(cid:17). Thus the number of

disjoint cells of radius at least r that intersect this ball is bounded by the number of descendants of
C with this radius. To bound this number we then invoke Theorem 5 and conclude the proof.

4.1 An effective aspect ratio bound for RPTREE-MAX cells

In this section we prove an upper bound on the radius of the smallest RPTREE-MAX cell that
completely contains a given ball B of radius R. Note that this effectively bounds the aspect ratio
of this cell. Consider any cell C of radius ∆ that contains B. We proceed with the proof by ﬁrst

5

useless split

useful split

C

Bi

B

∆
2

Figure 2: Balls Bi are of radius ∆/512√d and their centers are ∆/2 far from the center of B.

showing that the probability that B will be split before it lands up in a cell of radius ∆/2 is at most
a quantity inversely proportional to ∆. Note that we are not interested in all descendants of C - only
the ones ones that contain B. That is why we argue differently here. We consider balls of radius
∆/512√d surrounding B at a distance of ∆/2 (see Figure 2). These balls are made to cover the
annulus centered at B of mean radius ∆/2 and thickness ∆/512√d – clearly dO(d) balls sufﬁce.
Without loss of generality assume that the centers of all these balls lie in C.
Notice that if B gets separated from all these balls without getting split in the process then it will
lie in a cell of radius < ∆/2. Fix a Bi and call a random split of the RPTREE-MAX useful if
it separates B from Bi and useless if it splits B. Using a proof technique similar to that used in
Lemma 7 we can show that the probability of a useful split is at least 1
192 whereas Lemma 6 tells us
that the probability of a useless split is at most 3R√d
∆ .
Lemma 11. There exists a constant c5 such that the probability of a ball of radius R in a cell of
radius ∆ getting split before it lands up in a cell of radius ∆/2 is at most c5Rd√d log d

.

∆

Proof. Refer to Supplementary Material

We now state our result on the “effective” bound on aspect ratios of RPTREE-MAX cells.
Theorem 12. There exists a constant c6 such that with probability > 1 − 1/4  a given (ﬁxed) ball
B of radius R will be completely inscribed in an RPTREE-MAX cell C of radius no more than
c6 · Rd√d log d.

Proof. Refer to Supplementary Material

Proof. (of Theorem 10) Given a ball B of radius R  Theorem 12 shows that with probability at

least 3/4  B will lie in a cell C of radius at most R′ = O(cid:16)Rd√d log d(cid:17). Hence all cells of

radius atleast r that intersect this ball must be either descendants or ancestors of C. Since we want
an upper bound on the largest number of such disjoint cells  it sufﬁces to count the number of
descendants of C of radius no less than r. We know from Theorem 5 that with probability at least
3/4 in log(R′/r)d log(dR′/r) levels the radius of all cells must go below r. The result follows by
observing that the RPTREE-MAX is a binary tree and hence the number of children can be at most
2log(R′/r)d log(dR′/r). The success probability is at least (3/4)2 > 1/2.

6

M

Tp(M)

p

Figure 3: Locally  almost all the energy of the data is concentrated in the tangent plane.

5 Local covariance dimension of a smooth manifold

The second variant of RPTREE  namely RPTREE-MEAN  adapts to the local covariance dimension
(see deﬁnition below) of data. We do not go into the details of the guarantees presented in [1] due
to lack of space. Informally  the guarantee is of the following kind : given data that has small local
covariance dimension  on expectation  a data point in a cell of radius r in the RPTREE-MEAN will
be contained in a cell of radius c7 · r in the next level for some constant c7 < 1. The randomization
is over the construction of RPTREE-MEAN as well as choice of the data point. This gives per-level
improvement albeit in expectation whereas RPTREE-MAX gives improvement in the worst case but
after a certain number of levels.
We will prove that a d-dimensional Riemannian submanifold M of RD has bounded local covari-
ance dimension thus proving that RPTREE-MEAN adapts to manifold dimension as well.
Deﬁnition 13. A set S ⊂ RD has local covariance dimension (d  ǫ  r) if there exists an isometry
M of RD under which the set S when restricted to any ball of radius r has a covariance matrix for
which some d diagonal elements contribute a (1 − ǫ) fraction of its trace.
This is a more general deﬁnition than the one presented in [1] which expects the top d eigenvalues
of the covariance matrix to account for a (1 − ǫ) fraction of its trace. However  all that [1] requires
for the guarantees of RPTREE-MEAN to hold is that there exist d orthonormal directions such that
a (1 − ǫ) fraction of the energy of the dataset i.e. Px∈S kx − mean(S)k2 is contained in those d
dimensions. This is trivially true when M is a d-dimensional afﬁne set. However we also expect
that for small neighborhoods on smooth manifolds  most of the energy would be concentrated in the
tangent plane at a point in that neighborhood (see Figure 3). Indeed  we can show the following :
Theorem 14 (Main). Given a data set S ⊂ M where M is a d-dimensional Riemannian manifold
with condition number τ  then for any ǫ ≤ 1
For manifolds  the local curvature decides how small a neighborhood should one take in order to
expect a sense of “ﬂatness” in the non-linear surface. This is quantiﬁed using the Condition Number
τ of M (introduced in [16]) which restricts the amount by which the manifold can curve locally.
The condition number is related to more prevalent notions of local curvature such as the second
fundamental form [17] in that the inverse of the condition number upper bounds the norm of the
second fundamental form [16]. Informally  if we restrict ourselves to regions of the manifold of
radius τ or less  then we get the requisite ﬂatness properties. This is formalized in [16] as follows.
For any hyperplane T ⊂ RD and a vector v ∈ Rd  let vk(T ) denote the projection of v onto T .
Fact 15 (Implicit in Lemma 5.3 of [16]). Suppose M is a Riemannian manifold with condition
number τ. For any p ∈ M and r ≤ √ǫτ  ǫ ≤ 1
4   let M′ = B(p  r) ∩ M. Let T = Tp(M) be the
tangent space at p. Then for any x  y ∈ M′  kxk(T ) − yk(T )k2 ≥ (1 − ǫ)kx − yk2.
This already seems to give us what we want - a large fraction of the length between any two points
on the manifold lies in the tangent plane - i.e.
in d dimensions. However in our case we have

4   S has local covariance dimension(cid:16)d  ǫ 

√ǫτ
3 (cid:17).

to show that for some d-dimensional plane P   Px∈S k(x − µ)

k

(P )k2 > (1 − ǫ)Px∈S kx − µk2

7

where µ = mean(S). The problem is that we cannot apply Fact 15 since there is no surety that the
mean will lie on the manifold itself. However it turns out that certain points on the manifold can act
as “proxies” for the mean and provide a workaround to the problem.

Proof. (of Theorem 14) Refer to Supplementary Material

6 Conclusion

In this paper we considered the two random projection trees proposed in [1]. For the RPTREE-
MAX data structure  we provided an improved bound (Theorem 5) on the number of levels required
to decrease the size of the tree cells by any factor s ≥ 2. However the bound we proved is poly-
logarithmic in s. It would be nice if this can be brought down to logarithmic since it would directly
improve the packing lemma (Theorem 10) as well. More speciﬁcally the packing bound would

r ) for ﬁxed d.

r (cid:1)O(1)instead of (cid:0) R

r (cid:1)O(log R

As far as dependence on d is concerned  there is room for improvement in the packing lemma. We
have shown that the smallest cell in the RPTREE-MAX that completely contains a ﬁxed ball B of

become(cid:0) R
radius R has an aspect ratio no more than O(cid:16)d√d log d(cid:17) since it has a ball of radius R inscribed in
it and can be circumscribed by a ball of radius no more than O(cid:16)Rd√d log d(cid:17). Any improvement in

the aspect ratio of the smallest cell that contains a given ball will also directly improve the packing
lemma.

3

Moving on to our results for the RPTREE-MEAN  we demonstrated that it adapts to manifold di-
mension as well. However the constants involved in our guarantee are pessimistic. For instance 
the radius parameter in the local covariance dimension is given as √ǫτ
- this can be improved to
√ǫτ
if one can show that there will always exists a point q ∈ B(x0  r) ∩ M at which the function
2
g : x ∈ M 7−→ kx − µk attains a local extrema.
We conclude with a word on the applications of our results. As we already mentioned  packing
lemmas and size reduction guarantees for arbitrary factors are typically used in applications for
nearest neighbor searching and clustering. However  these applications (viz [12]  [15]) also require
that the tree have bounded depth. The RPTREE-MAX is a pure space partitioning data structure that
can be coerced by an adversarial placement of points into being a primarily left-deep or right-deep
tree having depth Ω(n) where n is the number of data points.
Existing data structures such as BBD Trees remedy this by alternating space partitioning splits with
data partitioning splits. Thus every alternate split is forced to send at most a constant fraction
of the points into any of the children thus ensuring a depth that is logarithmic in the number of
data points. A similar technique is used in [7] to bound the depth of the version of RPTREE-
MAX used in that paper. However it remains to be seen if the same trick can be used to bound the
depth of RPTREE-MAX while maintaining the packing guarantees because although such “space
partitioning” splits do not seem to hinder Theorem 5  they do hinder Theorem 10 (more speciﬁcally
they hinder Theorem 11).

We leave open the question of a possible augmentation of the RPTREE-MAX structure  or a better
analysis  that can simultaneously give the following guarantees :

1. Bounded Depth : depth of the tree should be o(n)  preferably (log n)O(1)

2. Packing Guarantee : of the form(cid:0) R

r )O(1)

r (cid:1)(d log R

3. Space Partitioning Guarantee : assured size reduction by factor s in (d log s)O(1) levels

Acknowledgments

The authors thank James Lee for pointing out an incorrect usage of the term Assouad dimension in
a previous version of the paper. Purushottam Kar thanks Chandan Saha for several fruitful discus-
sions and for his help with the proofs of the Theorems 5 and 10. Purushottam is supported by the
Research I Foundation of the Department of Computer Science and Engineering  IIT Kanpur.

8

References

[1] Sanjoy Dasgupta and Yoav Freund. Random Projection Trees and Low dimensional Manifolds. In 40th

Annual ACM Symposium on Theory of Computing  pages 537–546  2008.

[2] Piotr Indyk and Rajeev Motwani. Approximate Nearest Neighbors : Towards Removing the Curse of

Dimensionality. In 30th Annual ACM Symposium on Theory of Computing  pages 604–613  1998.

[3] Joshua B. Tenenbaum  Vin de Silva  and John C. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(22):2319–2323  2000.

[4] Piotr Indyk and Assaf Naor. Nearest-Neighbor-Preserving Embeddings. ACM Transactions on Algo-

rithms  3  2007.

[5] Richard G. Baraniuk and Michael B. Wakin. Random Projections of Smooth Manifolds. Foundations of

Computational Mathematics  9(1):51–77  2009.

[6] Yoav Freund  Sanjoy Dasgupta  Mayank Kabra  and Nakul Verma. Learning the structure of manifolds
using random projections. In Twenty-First Annual Conference on Neural Information Processing Systems 
2007.

[7] Samory Kpotufe. Escaping the curse of dimensionality with a tree-based regressor.

Conference on Learning Theory  2009.

In 22nd Annual

[8] Donghui Yan  Ling Huang  and Michael I. Jordan. Fast Approximate Spectral Clustering. In 15th ACM

SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 907–916  2009.

[9] John Wright and Gang Hua. Implicit Elastic Matching with Random Projections for Pose-Variant Face
Recognition. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition  pages
1502–1509  2009.

[10] Jian Pu  Junping Zhang  Peihong Guo  and Xiaoru Yuan. Interactive Super-Resolution through Neighbor

Embedding. In 9th Asian Conference on Computer Vision  pages 496–505  2009.

[11] Jon Louis Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communica-

tions of the ACM  18(9):509–517  1975.

[12] Sunil Arya  David M. Mount  Nathan S. Netanyahu  Ruth Silverman  and Angela Y. Wu. An Opti-
mal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions. Journal of the ACM 
45(6):891–923  1998.

[13] Christian A. Duncan  Michael T. Goodrich  and Stephen G. Kobourov. Balanced Aspect Ratio Trees:

Combining the Advantages of k-d Trees and Octrees. Journal of Algorithms  38(1):303–333  2001.

[14] Hanan Samet. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann Publish-

ers  2005.

[15] Tapas Kanungo  David M. Mount  Nathan S. Netanyahu  Christine D. Piatko  Ruth Silverman  and An-
gela Y. Wu. A local search approximation algorithm for k-means clustering. Computational Geometry 
28(2-3):89–112  2004.

[16] Partha Niyogi  Stephen Smale  and Shmuel Weinberger. Finding the Homology of Submanifolds with
High Conﬁdence from Random Samples. Discrete & Computational Geometry  39(1-3):419–441  2008.
[17] Sebasti´an Montiel and Antonio Ros. Curves and Surfaces  volume 69 of Graduate Studies in Mathemat-

ics. American Mathematical Society and Real Sociedad Matem´atica Epa˜nola  2005.

9

,Maximilian Balandat
Walid Krichene
Claire Tomlin
Alexandre Bayen