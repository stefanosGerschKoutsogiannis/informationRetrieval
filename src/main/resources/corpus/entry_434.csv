2019,Addressing Failure Prediction by Learning Model Confidence,Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper  we propose a new target criterion for model confidence  corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time  we propose to learn TCP criterion on the training set  introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures  small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods  from MCP to Bayesian uncertainty  as well as recent approaches specifically designed for failure prediction.,Addressing Failure Prediction
by Learning Model Conﬁdence

Charles Corbière1 2

charles.corbiere@valeo.com

Nicolas Thome1

nicolas.thome@cnam.fr

Avner Bar-Hen1
avner@cnam.fr

Matthieu Cord2 3

matthieu.cord@lip6.fr

Patrick Pérez2

patrick.perez@valeo.com

1CEDRIC  Conservatoire National des Arts et Métiers  Paris  France

2valeo.ai  Paris  France

3Sorbonne University  Paris  France

Abstract

Assessing reliably the conﬁdence of a deep neural network and predicting its fail-
ures is of primary importance for the practical deployment of these models. In this
paper  we propose a new target criterion for model conﬁdence  corresponding to
the True Class Probability (TCP). We show how using the TCP is more suited than
relying on the classic Maximum Class Probability (MCP). We provide in addition
theoretical guarantees for TCP in the context of failure prediction. Since the true
class is by essence unknown at test time  we propose to learn TCP criterion on
the training set  introducing a speciﬁc learning scheme adapted to this context.
Extensive experiments are conducted for validating the relevance of the proposed
approach. We study various network architectures  small and large scale datasets for
image classiﬁcation and semantic segmentation. We show that our approach con-
sistently outperforms several strong methods  from MCP to Bayesian uncertainty 
as well as recent approaches speciﬁcally designed for failure prediction.

1

Introduction

Deep neural networks have seen a wide adoption  driven by their impressive performance in various
tasks including image classiﬁcation [25]  object recognition [43  33  37]  natural language processing
[34  35]  and speech recognition [18  15]. Despite their growing success  safety remains a great
concern when it comes to implement these models in real-world conditions [1  19]. Estimating when a
model makes an error is even more crucial in applications where failing carries serious repercussions 
such as in autonomous driving  medical diagnosis or nuclear power plant monitoring [32].
This paper addresses the challenge of failure prediction with deep neural networks [17  20  16].
The objective is to provide conﬁdence measures for model’s predictions that are reliable and whose
ranking among samples enables to distinguish correct from incorrect predictions. Equipped with such
a conﬁdence measure  a system could decide to stick to the prediction or  on the contrary  to hand
over to a human or a back-up system with  e.g. other sensors  or simply to trigger an alarm.
In the context of classiﬁcation  a widely used baseline for conﬁdence estimation with neural networks
is to take the value of the predicted class’ probability  namely the Maximum Class Probability (MCP) 
given by the softmax layer output. Although recent evaluations of MCP for failure prediction with

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: When ranking test samples according to Maximum Class Probability (a)  output by a
convolutional model trained on CIFAR-10 dataset  we observe that correct predictions (in green) and
incorrect ones (in red) overlap considerably  making it difﬁcult to distinguish them. On the other
hand  ranking samples according to True Class Probability (b) alleviates this issue and allows a better
separation for failure prediction. (Distributions of both correct and incorrect samples are plotted in
relative density for visualization purpose).

modern deep models reveal reasonable performances [17]  they still suffer from several conceptual
drawbacks. Softmax probabilities are indeed known to be non-calibrated [13  40]  sensitive to
adversarial attacks [12  44]  and inadequate for detecting in- from out-of-distribution examples [17 
30  26].
Another important issue related to MCP  which we speciﬁcally address in this work  relates to ranking
of conﬁdence scores: this ranking is unreliable for the task of failure prediction [41  20]. As illustrated
in Figure 1(a) for a small convolutional network trained on CIFAR-10 dataset  MCP conﬁdence
values for erroneous and correct predictions overlap. It is worth mentioning that this problem comes
from the fact that MCP leads by design to high conﬁdence values  even for erroneous ones  since the
largest softmax output is used. On the other hand  the probability of the model with respect to the true
class naturally reﬂects a better behaved model conﬁdence  as illustrated in Figure 1(b). This leads to
errors’ conﬁdence distributions shifted to smaller values  while correct predictions are still associated
with high values  allowing a much better separability between these two types of prediction.
Based on this observation  we propose a novel approach for failure prediction with deep neural
networks. We introduce a new conﬁdence criteria based on the idea of using the TCP (section 2.1) 
for which we provide theoretical guarantees in the context of failure prediction. Since the true class
is obviously unknown at test time  we introduce a method to learn a given target conﬁdence criterion
from data (section 2.2). We also discuss connections and differences between related works for
failure prediction  in particular Bayesian deep learning and ensemble approaches  as well as recent
approaches designing alternative criteria for failure prediction (section 2.3). We conduct extensive
comparative experiments across various tasks  datasets and network architectures to validate the
relevance of our proposed approach (section 3.2). Finally  a thorough analysis of our approach
regarding the choice of loss function  criterion and learning scheme is presented in section 3.3.

2 Failure prediction by learning model conﬁdence

We are interested in the problem of deﬁning relevant conﬁdence criteria for failure prediction with
deep neural networks  in the context of classiﬁcation. We also address semantic image segmentation 
which can be seen as a pixel-wise classiﬁcation problem  where a model outputs a dense segmentation
mask with a predicted class assigned to each pixel. As such  all the following material is formulated
for classiﬁcation  and implementation details for segmentation are speciﬁed when necessary.
Let us consider a dataset D which consists of N i.i.d. training samples D = {(xi  y∗
i )}N
i=1 where
xi ∈ Rd is a d-dimensional feature and y∗
i ∈ Y = {1  ...  K} is its true class. We view a classiﬁcation
neural network as a probabilistic model: given an input x  the network assigns a probabilistic
predictive distribution P (Y |w  x) by computing the softmax output for each class k and where w

2

are the parameters of the network. From this predictive distribution  one can infer the class predicted
by the model as ˆy = argmax

P (Y = k|w  x).

k∈Y

During training  network parameters w are learned following a maximum likelihood estimation
framework where one minimizes the Kullback-Leibler (KL) divergence between the predictive
distribution and the true distribution. In classiﬁcation  this is equivalent to minimizing the cross-
entropy loss w.r.t. w  which is the negative sum of the log-probabilities over positive labels:

LCE(w;D) = − 1
N

y∗
i log P (Y = y∗

i |w  xi).

(1)

N(cid:88)

i=1

2.1 Conﬁdence criterion for failure prediction

Instead of trying to improve the accuracy of a given trained model  we are interested in knowing if
it can be endowed with the ability to recognize when its prediction may be wrong. A conﬁdence
criterion is a quantitative measure to estimate the conﬁdence of the model prediction. The higher the
value  the more certain the model about its prediction. As such  a suitable conﬁdence criterion should
correlate erroneous predictions with low values and successful predictions with high values. Here 
we speciﬁcally focus on the ability of the conﬁdence criterion to separate successful and erroneous
predictions in order to distinguish them.
For a given input x  a standard approach is to compute the softmax probability of the predicted class
ˆy  that is the Maximum Class Probability: MCP(x) = max

k∈Y P (Y = k|w  x) = P (Y = ˆy|w  x).

By taking the largest softmax probability  MCP leads to high conﬁdence values both for errors and
correct predictions  making it hard to distinguish them  as shown in Figure 1(a). On the other hand 
when the model is misclassifying an example  the probability associated to the true class y∗ would be
more likely close to a low value  reﬂecting the fact that the model made an error. Thus  we propose to
consider the True Class Probability as a suitable conﬁdence criterion for failure prediction:

TCP : Rd × Y → R

(2)
Theoretical guarantees. With TCP  the following properties hold (see derivation in supplementary
1.1). Given an example (x  y∗) 

(x   y∗) → P (Y = y∗|w  x)

• TCP(x  y∗) > 1/2 ⇒ ˆy = y∗  i.e. the example is properly classiﬁed by the model 
• TCP(x  y∗) < 1/K ⇒ ˆy (cid:54)= y∗  i.e. the example is wrongly classiﬁed by the model.

Within the range [1/K  1/2]  there is no theoretical guarantee that correct and incorrect predictions
will not overlap in terms of TCP. However  when using deep neural networks  we observe that the
actual overlap area is extremely small in practice  as illustrated in Figure 1(b) on the CIFAR-10
dataset. One possible explanation comes from the fact that modern deep neural networks output
overconﬁdent predictions and therefore non-calibrated probabilities [13]. We provide consolidated
results and analysis on this aspect in Section 3 and in the supplementary 1.2.
We also introduce a normalized variant of the TCP conﬁdence criterion  which consists in computing
the ratio between TCP and MCP:

TCPr(x  y∗) =

P (Y = y∗|w  x)
P (Y = ˆy|w  x)

.

(3)

The TCPr criterion presents stronger theoretical guarantees than TCP  since correct predictions will
be  by design  assigned the value of 1  whereas errors will range in [0  1[. On the other hand  learning
this criterion may be more challenging since all correct predictions must match a single scalar value.

2.2 Learning TCP conﬁdence with deep neural networks

Using TCP as conﬁdence criterion on a model’s output would be of great help when it comes
to predicting failures. However  the true class y∗ of an output is obviously not available when

3

Figure 2: Our approach is based on two sub-networks. The classiﬁcation model with parameters
w is composed of a succession of convolutional and dense layers (‘ConvNet’) followed by a ﬁnal
dense layer with softmax activation. The conﬁdence network  ‘ConﬁdNet’  builds upon features maps
extracted by ConvNet  and is composed of a succession of layers which output a conﬁdence score
ˆc(x  θ) ∈ [0  1].

estimating conﬁdence on test samples. Thus  we propose to learn TCP conﬁdence c∗(x  y∗) =
P (Y = y∗|w  x) 1  our target conﬁdence value. We introduce a conﬁdence neural network  termed
ConﬁdNet  with parameters θ  which outputs a conﬁdence prediction ˆc(x  θ). During training  we
seek θ such that ˆc(x  θ) is close to c∗(x  y∗) on training samples (see Figure 2).
ConﬁdNet builds upon a classiﬁcation neural network M  whose parameters w are preliminary
learned using cross-entropy loss LCE in (1). We are not concerned with improving model M’s
accuracy. As a consequence  its classiﬁcation layers (last fully connected layer and subsequent
operations) will be ﬁxed from now on.

Conﬁdence network design. During initial classiﬁcation training  model M learns to extract
increasingly complex features that are fed to the classiﬁcation layers. To beneﬁt from these rich
representations  we build ConﬁdNet on top of them: ConﬁdNet passes these features through a
succession of dense layers with a ﬁnal sigmoid activation that outputs a scalar ˆc(x  θ) ∈ [0  1].
Note that in semantic segmentation  models consist of fully convolutional networks where hidden
representations are 2D feature maps. ConﬁdNet can beneﬁt from this spatial information by replacing
dense layers by 1 × 1 convolutions with adequate number of channels.

Loss function. Since we want to regress a score between 0 and 1  we use the (cid:96)2 loss to train
ConﬁdNet:

N(cid:88)

i=1

Lconf (θ;D) =

1
N

(ˆc(xi  θ) − c∗(xi  y∗

i ))2.

(4)

In the experimental part  we also tried more direct approaches for failure prediction such as a binary
cross entropy loss (BCE) between the conﬁdence network score and a incorrect/correct prediction
target. We also tried implementing Focal loss [31]  a BCE variant which focuses on hard examples.
Finally  one can also see failure detection as a ranking problem where good predictions must be
ranked before erroneous ones according to a conﬁdence criterion. To this end  we also implemented a
ranking loss [36  7] applied locally on training batch inputs.

Learning scheme. Our complete conﬁdence model  from input image to conﬁdence score  shares
its ﬁrst encoding part (‘ConvNet’ in Fig.2) with the classiﬁcation model M. The training of ConﬁdNet

1or its normalized variant TCPr(x  y∗).

4

starts by ﬁxing entirely M (freezing w) and learning θ using loss (4). In a next step  we can then
ﬁne-tune the ConvNet encoder. However  as model M has to remain ﬁxed to compute similar
classiﬁcation predictions  we have now to decouple the feature encoders used for classiﬁcation and
conﬁdence prediction respectively. We also deactivate dropout layers in this last training phase and
reduce learning rate to mitigate stochastic effects that may lead the new encoder to deviate too much
from the original one used for classiﬁcation. Data augmentation can thus still be used.

2.3 Related works

Conﬁdence estimation has already raised interest in the machine learning community over the past
decade. Blatz et al. [3] introduce a method similar to our BCE baseline for conﬁdence estimation in
machine translation but their approach is not dedicated to training deep neural networks. Similarly 
[42  29] mention the use of bi-directional lattice RNN speciﬁcally designed for conﬁdence estimation
in speech recognition  whereas ConﬁdNet offers a model- and task-agnostic approach which can
be plugged into any deep neural network. Post-hoc selective classiﬁcation methods [11] identify a
threshold over a conﬁdence-rate function (e.g.  MCP) to satisfy a user-speciﬁed risk level  whereas
we focus here on relative metrics. Recently  Hendricks et al. [17] established a standard baseline for
deep neural networks which relies on MCP retrieved from softmax distribution. As stated before 
MCP presents several limits regarding both failure prediction and out-of-distribution detection as
it outputs high conﬁdence values. This limit is alleviated in our TCP criterion which also provides
some interesting theoretical guarantees regarding conﬁdence threshold.
In [20]  Jiang et al. propose a new conﬁdence measure  ‘Trust Score’  which measures the agreement
between the classiﬁer and a modiﬁed nearest-neighbor classiﬁer on the test examples. More precisely 
the conﬁdence criterion used in Trust Score [20] is the ratio between the distance from the sample
to the nearest class different from the predicted class and the distance to the predicted class. One
clear drawback of this approach is its lack of scalability  since computing nearest neighbors in large
datasets is extremely costly in both computation and memory. Another more fundamental limitation
related to the Trust Score itself is that local distance computation becomes less meaningful in high
dimensional spaces [2]  which is likely to negatively affect performances of this method. In contrast 
ConﬁdNet is based on a training approach which learns a sub-manifold in the error/success space 
which is arguably less prone to the curse of dimensionality and  therefore  facilitate discrimination
between these classes.
Bayesian approaches for uncertainty estimation in neural networks gained a lot of attention recently 
especially due to the elegant connection between efﬁcient stochastic regularization techniques 
e.g. dropout [10]  and variational inference in Bayesian neural networks [10  9  4  21  22]. Gal and
Ghahramani proposed in [10] using Monte Carlo Dropout (MCDropout) to estimate the posterior
predictive network distribution by sampling several stochastic network predictions. When applied
to regression  the predictive distribution uncertainty can be summarized by computing statistics 
e.g. variance. When using MCDropout for uncertainty estimation in classiﬁcation tasks  however 
the predictive distribution is averaged to a point-wise softmax estimate before computing standard
uncertainty criteria  e.g. entropy or variants such as mutual information. It is worth mentioning that
these entropy-based criteria measure the softmax output dispersion  where the uniform distribution has
maximum entropy. It is not clear how well these dispersion measures are adapted for distinguishing
failures from correct predictions  especially with deep neural networks which output overconﬁdent
predictions [13]: for example  it might be very challenging to discriminate a peaky prediction
corresponding to a correct prediction from an incorrect overconﬁdent one. We illustrate this issue in
section 3.2.
In tasks closely related to failure prediction  other approaches also identiﬁed the issue of MCP
regarding high conﬁdence predictions [17  30  26  28  13  40]. Guo et al. [13]  for conﬁdence
calibration  and Liang et al. [30]  for out-of-distribution detection  proposed to use temperature
scaling to mitigate conﬁdence values. However  this doesn’t affect the ranking of the conﬁdence
score and therefore the separability between errors and correct predictions. DeVries et al. [6] share
with us the same purpose of learning conﬁdence in neural networks. Their work differs by focusing
on out-of-distribution detection and learning jointly a distribution conﬁdence score and classiﬁcation
probabilities. In addition  they use predicted conﬁdence score to interpolate output probabilities and
target whereas we speciﬁcally deﬁne TCP  a criterion suited for failure prediction.

5

Lakshminarayanan et al. [26] propose an alternative to Bayesian neural networks by leveraging
ensemble of neural networks to produce well-calibrated uncertainty estimates. Part of their approach
relies on using a proper scoring rule as training criterion. It is interesting to note that our TCP criterion
corresponds actually to the exponential cross-entropy loss value of a model prediction  which is a
proper scoring rule in the case of multi-class classiﬁcation.

3 Experiments

In this section  we evaluate our approach to predict failure in both classiﬁcation and segmentation
settings. First  we run comparative experiments against state-of-the-art conﬁdence estimation and
Bayesian uncertainty estimation methods on various datasets. These results are then completed by
a thorough analysis of the inﬂuence of the conﬁdence criterion  the training loss and the learning
scheme in our approach. Finally  we provide a few visualizations to get additional insight into the
behavior of our approach. Our code is available at https://github.com/valeoai/ConﬁdNet.

3.1 Experimental setup

Datasets. We run experiments on image datasets of varying scale and complexity: MNIST [27]
and SVHN [39] datasets provide relatively simple and small (28 × 28) images of digits (10 classes).
CIFAR-10 and CIFAR-100 [24] propose more complex object recognition tasks on low resolution
images. We also report experiments for semantic segmentation on CamVid [5]  a standard road scene
dataset. Further details about these datasets  as well as on architectures  training and metrics can be
found in supplementary 2.1.
Network architectures. The classiﬁcation deep architectures follow those proposed in [20] for fair
comparison. They range from small convolutional networks for MNIST and SVHN to larger VGG-16
architecture for the CIFAR datasets. We also added a multi-layer perceptron (MLP) with 1 hidden
layer for MNIST to investigate performances on small models. For CamVid  we implemented a
SegNet semantic segmentation model  following [21].
Our conﬁdence prediction network  ConﬁdNet  is attached to the penultimate layer of the classiﬁcation
network. It is composed of a succession of 5 dense layers. Variants of this architecture have been
tested  leading to similar performances (see supplementary 2.2 for more details). Following our
speciﬁc learning scheme  we ﬁrst train ConﬁdNet layers before ﬁne-tuning the duplicate ConvNet
encoder dedicated to conﬁdence estimation. In the context of semantic segmentation  we adapt
ConﬁdNet by making it fully convolutional.
Evaluation metrics. We measure the quality of failure prediction following the standard metrics
used in the literature [17]: AUPR-Error  AUPR-Success  FPR at 95% TPR and AUROC. We will
mainly focus on AUPR-Error  which computes the area under the Precision-Recall curve using errors
as the positive class.

3.2 Comparative results on failure prediction

To demonstrate the effectiveness of our method  we implemented competitive conﬁdence and un-
certainty estimation approaches including Maximum Class Probability (MCP) as a baseline [17] 
Trust Score [20]  and Monte-Carlo Dropout (MCDropout) [10]. For Trust Score  we used the code
provided by the authors2. Further implementation details and parameter settings are available in the
supplementary 2.1.
Comparative results are summarized in Table 1. First of all  we observe that our approach outperforms
baseline methods in every setting  with a signiﬁcant gap on small models/datasets. This conﬁrms both
that TCP is an adequate conﬁdence criterion for failure prediction and that our approach ConﬁdNet
is able to learn it. TrustScore method also presents good results on small datasets/models such as
MNIST where it improved baseline. While ConﬁdNet still performs well on more complex datasets 
Trust Score’s performance drops  which might be explained by high dimensionality issues with
distances as mentioned in section 2.3. For its application to semantic segmentation where each
training pixel is a ‘neighbor’  computational complexity forced us to reduce drastically the number
of training neighbors and of test samples. We sampled randomly in each train and test image a

2https://github.com/google/TrustScore

6

Dataset

MNIST
MLP

MNIST

Small ConvNet

Table 1: Comparison of failure prediction methods on various datasets. All methods share the same
classiﬁcation network. Note that for MCDropout  test accuracy is averaged over random sampling.
All values are percentages.
Model
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)
Baseline (MCP) [17]
MCDropout [10]
TrustScore [20]
ConﬁdNet (Ours)

FPR-95%-TPR AUPR-Error AUPR-Success AUC
97.13
97.15
97.52
97.83
98.63
98.65
98.20
98.82
93.20
92.85
92.16
93.44
91.53
92.08
88.47
92.12
85.67
86.09
84.17
86.28
84.42
84.58
68.33
85.02

99.94
99.94
99.95
99.95
99.99
99.99
99.98
99.99
99.54
99.52
99.48
99.55
99.19
99.27
98.76
99.24
92.49
92.96
91.58
92.68
96.37
96.40
92.72
96.58

37.70
38.22
52.18
57.37
35.05
38.50
35.88
45.89
48.18
43.87
43.32
50.72
45.36
46.40
38.10
49.94
71.99
72.59
66.82
73.68
48.53
49.35
20.42
50.51

14.87
15.15
12.31
11.79
5.56
5.26
10.00
3.33
31.28
36.60
34.74
28.58
47.50
49.02
55.70
44.94
67.86
64.68
71.74
62.96
63.87
62.95

61.52

SVHN

Small ConvNet

CIFAR-10
VGG16

CIFAR-100

VGG16

CamVid
SegNet

(a) CIFAR-10

(b) SVHN

Figure 3: Risk-coverage curves. ‘Selective risk’ (y-axis) represents the percentage of errors in the
remaining test set for a given coverage percentage.

small percentage of pixels to compute TrustScore. ConﬁdNet  in contrast  is as fast as the original
segmentation network.
We also improve state-of-art performances from MCDropout. While MCDropout leverages ensem-
bling based on dropout layers  taking as conﬁdence measure the entropy on the average softmax
distribution may not be always adequate. In Figure 4  we show side-by-side two samples with a
similar distribution entropy. Left image is misclassiﬁed while right one enjoys a correct prediction.
Entropy is a symmetric measure in regards to class probabilities: a correct prediction with [0.65  0.35]
distribution is evaluated as conﬁdent as an incorrect one with [0.35  0.65] distribution. In contrast 
our approach can discriminate an incorrect from a correct prediction despite both having similarly
spread distributions.

7

Figure 4: Illustrating the limits of MCDropout with entropy as conﬁdence estimation on SVHN test
samples. Red-border image (a) is misclassiﬁed by the classiﬁcation model; green-border image (b)
is correctly classiﬁed. Prediction exhibit similar high entropy in both cases. For each sample  we
provide a plot of their softmax predictive distribution.

Risk-coverage curves [8  11] depicting the performance of ConﬁdNet and other baselines for CIFAR-
10 and SVHN datasets appear in Figure 3. ‘Coverage’ corresponds to the probability mass of the
non-rejected region after using a threshold as selection function [11]. For both datasets  ConﬁdNet
presents a better coverage potential for each selective risk that a user can choose beforehand. In
addition  we can see that the improvement is more pronounced at high coverage rates - e.g.
in
[0.8; 0.95] for CIFAR-10 (Fig. 3a) and in [0.86; 0.96] for SVHN (Fig. 3b) - which highlights the
capacity of ConﬁdNet to identify successfully critical failures.

3.3 Effect of learning variants

CIFAR-100

VGG-16
72.68%
73.68%

43.94%
45.89%

MNIST

SmallConvNet

Conﬁdence training
+ Fine-tuning ConvNet

Table 2: Effect of learning scheme on AUPR-Error

We ﬁrst evaluate the effect
of ﬁne-tuning ConvNet in
our approach. Without
ﬁne-tuning  ConﬁdNet al-
ready achieves signiﬁcant
improvements w.r.t. base-
line  as shown in Table 2.
By allowing subsequent
ﬁne-tuning as described in
section 2.2  ConﬁdNet performance is further boosted in every setting  around 1-2%. Note that using
a vanilla ﬁne-tuning without deactivating dropout layers did not bring any improvement.
Given the small number of errors available due to deep neural network over-ﬁtting  we also experi-
mented with training ConﬁdNet on a hold-out dataset. We report results on all datasets in Table 3 for
validation sets with 10% of samples. We observe a general performance drop when using a validation
set for training TCP conﬁdence. The drop is especially pronounced for small datasets (MNIST) 
where models reach >97% train and val accuracies. Consequently  with a high accuracy and a small
validation set  we do not get a larger absolute number of errors using val set compared to train set.
One solution would be to increase validation set size but this would damage model’s prediction per-
formance. By contrast  we take care with our approach to base our conﬁdence estimation on models
with levels of test predictive performance that are similar to those of baselines. On CIFAR-100  the
gap between train accuracy and val accuracy is substantial (95.56% vs. 65.96%)  which may explain
the slight improvement for conﬁdence estimation using val set (+0.17%). We think that training
ConﬁdNet on val set with models reporting low/middle test accuracies could improve the approach.

Table 3: Comparison between training ConﬁdNet on train set or on validation set

AUPR-Error (%)

ConﬁdNet (using train set)
ConﬁdNet (using val set)

MNIST
MLP
57.34%
33.41%

MNIST

SmallConvNet

SVHN

SmallConvNet

43.94%
34.22%

50.72%
47.96%

CIFAR-10 CIFAR-100 CamVid
SegNet
VGG-16
50.28%
49.94%
48.93%
50.15%

VGG-16
73.68%
73.85%

On Table 4  we compare training ConﬁdNet with MSE loss to binary classiﬁcation cross-
entropy loss (BCE). Even though BCE speciﬁcally addresses the failure prediction task  we

8

Focal
observe that
loss and ranking loss were also tested and presented similar results (see supplementary 2.3).

it achieves lower performances on CIFAR-10 and CamVid datasets.

Table 4: Effect of loss and normalized criterion on AUPR-Error

We intuitively think that
TCP regularizes training
by providing more ﬁne-
grained information about
the quality of the classiﬁer
regarding a sample’s pre-
diction. This is especially
important in the difﬁcult
learning conﬁguration where only very few error samples are available due to the good performance
of the classiﬁer. We also evaluate the impact of regression to the normalized criterion T CP r: per-
formance is lower than the one of TCP on small datasets such as CIFAR-10 where few errors are
present  but higher on larger datasets such as CamVid where each pixel is a sample. This emphasizes
once again the complexity of incorrect/correct classiﬁcation training.

Loss Criterion
TCPr
BCE
48.78%
49.94% 47.95%
50.51% 48.96%
51.35%

Dataset

CIFAR-10
CamVid

TCP

3.4 Qualitative assessments

In this last subsection  we provide an illustration on CamVid (Figure 5) to better understand our
approach for failure prediction. Compared to MCP baseline  our approach produces higher conﬁdence
scores for correct pixel predictions and lower ones on erroneously predicted pixels  which allow an
user to better detect errors area in semantic segmentation.

Figure 5: Comparison of inverse conﬁdence (uncertainty) map between ConﬁdNet (e) and MCP (f) on
one CamVid scene. The top row shows the input image (a) with its ground-truth (b) and the semantic
segmentation mask (c) predicted by the original classiﬁcation model. The error map associated to
the predicted segmentation is shown in (d)  with erroneous predictions ﬂagged in white. ConﬁdNet
(55.53% AP-Error) allows a better prediction of these errors than MCP (54.69% AP-Error).

4 Conclusion

In this paper  we deﬁned a new conﬁdence criterion  TCP  which provides both theoretical guarantees
and empirical evidences to address failure prediction. We proposed a speciﬁc method to learn this
criterion with a conﬁdence neural network built upon a classiﬁcation model. Results showed a
signiﬁcant improvement from strong baselines on various classiﬁcation and semantic segmentation
datasets  which validate the effectiveness of our approach. Further works involve exploring methods
to artiﬁcially generate errors  such as in adversarial training. ConﬁdNet could also be applied for
uncertainty estimation in domain adaptation [45  14] or in multi-task learning [23  38].

9

References
[1] Dario Amodei  Chris Olah  Jacob Steinhardt  Paul F. Christiano  John Schulman  and Dan Mané.

Concrete problems in AI safety. arXiv preprint arXiv:1606.06565  2016. 1

[2] Kevin Beyer  Jonathan Goldstein  Raghu Ramakrishnan  and Uri Shaft. When is “nearest

neighbor” meaningful? In ICDT  1999. 5

[3] John Blatz  Erin Fitzgerald  George Foster  Simona Gandrabur  Cyril Goutte  Alex Kulesza 
In

Alberto Sanchis  and Nicola Uefﬁng. Conﬁdence estimation for machine translation.
COLING  2004. 5

[4] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty

in neural networks. In ICML  2015. 5

[5] Gabriel J. Brostow  Julien Fauqueur  and Roberto Cipolla. Semantic object classes in video: A

high-deﬁnition ground truth database. Pattern Recogn. Lett.  30(2):88–97  2009. 6

[6] Terrance DeVries and Graham W Taylor. Learning conﬁdence for out-of-distribution detection

in neural networks. arXiv preprint arXiv:1802.04865  2018. 5

[7] Thibaut Durand  Nicolas Thome  and Matthieu Cord. Mantra: Minimum maximum latent

structural SVM for image classiﬁcation and ranking. In ICCV  2015. 4

[8] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classiﬁcation. J.

Mach. Learn. Res.  11:1605–1641  2010. 8

[9] Yarin Gal. Uncertainty in Deep Learning. PhD thesis  University of Cambridge  2016. 5

[10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model

uncertainty in deep learning. In ICML  2016. 5  6  7

[11] Yonatan Geifman and Ran El-Yaniv. Selective classiﬁcation for deep neural networks. In NIPS 

2017. 5  8

[12] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572  2014. 2

[13] Chuan Guo  Geoff Pleiss  Yu Sun  and Kilian Q. Weinberger. On calibration of modern neural

networks. In ICML  2017. 2  3  5

[14] Ligong Han  Yang Zou  Ruijiang Gao  Lezi Wang  and Dimitris Metaxas. Unsupervised domain

adaptation via calibrating uncertainties. In CVPR Workshops  2019. 9

[15] Awni Hannun  Carl Case  Jared Casper  Bryan Catanzaro  Greg Diamos  Erich Elsen  Ryan
Prenger  Sanjeev Satheesh  Shubho Sengupta  Adam Coates  and Andrew Y. Ng. Deep speech:
Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567  2014. 1

[16] Simon Hecker  Dengxin Dai  and Luc Van Gool. Failure prediction for autonomous driving. In

IV  2018. 1

[17] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. In ICLR  2017. 1  2  5  6  7

[18] Geoffrey Hinton  Li Deng  Dong Yu  George E Dahl  Abdel-rahman Mohamed  Navdeep
Jaitly  Andrew Senior  Vincent Vanhoucke  Patrick Nguyen  Tara N Sainath  et al. Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal Processing Magazine  29(6):82–97  2012. 1

[19] Joel Janai  Fatma Güney  Aseem Behl  and Andreas Geiger. Computer vision for au-
tonomous vehicles: Problems  datasets and state-of-the-art. arXiv preprint arXiv:1704.05519 
abs/1704.05519  2017. 1

[20] Heinrich Jiang  Been Kim  Melody Guan  and Maya Gupta. To trust or not to trust a classiﬁer.

In NIPS  2018. 1  2  5  6  7

10

[21] Alex Kendall  Vijay Badrinarayanan    and Roberto Cipolla. Bayesian SegNet: Model un-
certainty in deep convolutional encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680  2015. 5  6

[22] Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for

computer vision? In NIPS  2017. 5

[23] Alex Kendall  Yarin Gal  and Roberto Cipolla. Multi-task learning using uncertainty to weigh

losses for scene geometry and semantics. In CVPR  June 2018. 9

[24] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s

thesis  Department of Computer Science  University of Toronto  2009. 6

[25] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep

convolutional neural networks. In NIPS  2012. 1

[26] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems 30  2017. 2  5  6

[27] Yann LeCun and Corinna Cortes.

http://yann.lecun.com/exdb/mnist  1998. 6

The MNIST database of handwritten digits.

[28] Kimin Lee  Honglak Lee  Kibok Lee  and Jinwoo Shin. Training conﬁdence-calibrated classiﬁers

for detecting out-of-distribution samples. In ICLR  2018. 5

[29] Qiujia Li  Preben Ness  Anton Ragni  and M.J.F. Gales. Bi-directional lattice recurrent neural
networks for conﬁdence estimation. In IEEE International Conference on Acoustics  Speech
and Signal Processing  10 2018. 5

[30] Shiyu Liang  Yixuan Li  and R. Srikant. Enhancing the reliability of out-of-distribution image

detection in neural networks. In ICLR  2018. 2  5

[31] T. Lin  P. Goyal  R. Girshick  K. He  and P. Dollár. Focal loss for dense object detection. In

ICCV  2017. 4

[32] Ondrej Linda  Todd Vollmer  and Milos Manic. Neural network based intrusion detection

system for critical infrastructures. In IJCNN  2009. 1

[33] Wei Liu  Dragomir Anguelov  Dumitru Erhan  Christian Szegedy  Scott Reed  Cheng-Yang Fu 

and Alexander C. Berg. SSD: Single shot multibox detector. In ECCV  2016. 1

[34] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781  abs/1301.3781  2013. 1

[35] Tomas Mikolov  Martin Karaﬁát  Lukás Burget  Jan Cernocký  and Sanjeev Khudanpur. Recur-
rent neural network based language model. In Takao Kobayashi  Keikichi Hirose  and Satoshi
Nakamura  editors  INTERSPEECH  2010. 1

[36] Pritish Mohapatra  Michal Rolínek  C.V. Jawahar  Vladimir Kolmogorov  and M. Pawan Kumar.

Efﬁcient optimization for rank-based loss functions. In CVPR  June 2018. 4

[37] Taylor Mordan  Nicolas Thome  Gilles Henaff  and Matthieu Cord. End-to-end learning of
latent deformable part-based representations for object detection. International Journal of
Computer Vision  pages 1–21  07 2018. 1

[38] Taylor Mordan  Nicolas Thome  Gilles Henaff  and Matthieu Cord. Revisiting multi-task

learning with ROCK: a deep residual auxiliary block for visual detection. In NIPS  2018. 9

[39] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y. Ng.
Reading digits in natural images with unsupervised feature learning. In NIPS Workshop  2011.
6

[40] L. Neumann  A. Zisserman  and A. Vedaldi. Relaxed softmax: Efﬁcient conﬁdence auto-

calibration for safe pedestrian detection. In NIPS Workshops  2018. 2  5

11

[41] Anh Mai Nguyen  Jason Yosinski  and Jeff Clune. Deep neural networks are easily fooled: High

conﬁdence predictions for unrecognizable images. In CVPR  2015. 2

[42] A. Ragni  Q. Li  M. J. F. Gales  and Y. Wang. Conﬁdence estimation and deletion prediction

using bidirectional recurrent neural networks. In SLT Workshop  2018. 5

[43] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster R-CNN: Towards real-time

object detection with region proposal networks. In NIPS  2015. 1

[44] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian J.

Goodfellow  and Rob Fergus. Intriguing properties of neural networks. In ICLR  2014. 2

[45] Tuan-Hung Vu  Himalaya Jain  Maxime Bucher  Matthieu Cord  and Patrick Pérez. ADVENT:
Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR 
2019. 9

12

,He He
Hal Daume III
Jason Eisner
Charles Corbière
Nicolas THOME
Avner Bar-Hen
Matthieu Cord
Patrick Pérez