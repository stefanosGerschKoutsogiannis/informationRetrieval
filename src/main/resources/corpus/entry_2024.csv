2014,Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning,Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology  geology  and sociology applications. Existing models usually assume simple inter-dependence among variables  space  and time  and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis  which can conveniently incorporate different properties in spatio-temporal data  such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks  and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.,Fast Multivariate Spatio-temporal Analysis

via Low Rank Tensor Learning

Mohammad Taha Bahadori⇤
Dept. of Electrical Engineering
Univ. of Southern California

Los Angeles  CA 90089
mohammab@usc.edu

Rose Yu⇤

Dept. of Computer Science
Univ. of Southern California

Los Angeles  CA 90089

qiyu@usc.edu

Yan Liu

Dept. of Computer Science
Univ. of Southern California

Los Angeles  CA 90089
yanliu.cs@usc.edu

Abstract

Accurate and efﬁcient analysis of multivariate spatio-temporal data is critical in
climatology  geology  and sociology applications. Existing models usually assume
simple inter-dependence among variables  space  and time  and are computation-
ally expensive. We propose a uniﬁed low rank tensor learning framework for mul-
tivariate spatio-temporal analysis  which can conveniently incorporate different
properties in spatio-temporal data  such as spatial clustering and shared structure
among variables. We demonstrate how the general framework can be applied to
cokriging and forecasting tasks  and develop an efﬁcient greedy algorithm to solve
the resulting optimization problem with convergence guarantee. We conduct ex-
periments on both synthetic datasets and real application datasets to demonstrate
that our method is not only signiﬁcantly faster than existing methods but also
achieves lower estimation error.

1

Introduction

Spatio-temporal data provide unique information regarding “where” and “when”  which is essential
to answer many important questions in scientiﬁc studies from geology  climatology to sociology. In
the context of big data  we are confronted with a series of new challenges when analyzing spatio-
temporal data because of the complex spatial and temporal dependencies involved.
A plethora of excellent work has been conducted to address the challenge and achieved successes to
a certain extent [9  14]. Often times  geostatistical models use cross variogram and cross covariance
functions to describe the intrinsic dependency structure. However  the parametric form of cross
variogram and cross covariance functions impose strong assumptions on the spatial and temporal
correlation  which requires domain knowledge and manual work. Furthermore  parameter learning
of those statistical models is computationally expensive  making them infeasible for large-scale
applications.
Cokriging and forecasting are two central tasks in multivariate spatio-temporal analysis. Cokriging
utilizes the spatial correlations to predict the value of the variables for new locations. One widely
adopted method is multitask Gaussian process (MTGP) [5]  which assumes a Gaussian process prior
over latent functions to directly induce correlations between tasks. However  for a cokriging task
with M variables of P locations for T time stamps  the time complexity of MTGP is O(M 3P 3T )
[5]. For forecasting  popular methods in multivariate time series analysis include vector autoregres-
sive (VAR) models  autoregressive integrated moving average (ARIMA) models  and cointegration
models. An alternative method for spatio-temporal analysis is Bayesian hierarchical spatio-temporal
models with either separable and non-separable space-time covariance functions [7]. Rank reduced

⇤Authors have equal contributions.

1

models have been proposed to capture the inter-dependency among variables [1]. However  very
few models can directly handle the correlations among variables  space and time simultaneously in
a scalable way. In this paper  we aim to address this problem by presenting a uniﬁed framework for
many spatio-temporal analysis tasks that are scalable for large-scale applications.
Tensor representation provides a convenient way to capture inter-dependencies along multiple di-
mensions. Therefore it is natural to represent the multivariate spatio-temporal data in tensor. Recent
advances in low rank learning have led to simple models that can capture the commonalities among
each mode of the tensor [16  21]. Similar argument can be found in the literature of spatial data re-
covery [12]  neuroimaging analysis [27]  and multi-task learning [21]. Our work builds upon recent
advances in low rank tensor learning [16  12  27] and further considers the scenario where additional
side information of data is available. For example  in geo-spatial applications  apart from measure-
ments of multiple variables  geographical information is available to infer location adjacency; in
social network applications  friendship network structure is collected to obtain preference similarity.
To utilize the side information  we can construct a Laplacian regularizer from the similarity matrices 
which favors locally smooth solutions.
We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure
learning framework [3  25  22]. Greedy low rank tensor learning is efﬁcient  as it does not require
full singular value decomposition of large matrices as opposed to other alternating direction methods
[12]. We also provide a bound on the difference between the loss function at our greedy solution
and the one at the globally optimal solution. Finally  we present experiment results on simulation
datasets as well as application datasets in climate and social network analysis  which show that our
algorithm is faster and achieves higher prediction accuracy than state-of-art approaches in cokriging
and forecasting tasks.

2 Tensor formulation for multivariate spatio-temporal analysis
The critical element in multivariate spatio-temporal analysis is an efﬁcient way to incorporate the
spatial temporal correlations into modeling and automatically capture the shared structures across
variables  locations  and time. In this section  we present a uniﬁed low rank tensor learning frame-
work that can perform various types of spatio-temporal analysis. We will use two important appli-
cations  i.e.  cokriging and forecasting  to motivate and describe the framework.

2.1 Cokriging

In geostatistics  cokriging is the task of interpolating the data of one variable for unknown locations
by taking advantage of the observations of variables from known locations. For example  by making
use of the correlations between precipitation and temperature  we can obtain more precise estimate
of temperature in unknown locations than univariate kriging. Formally  denote the complete data
for P locations over T time stamps with M variables as X2 RP⇥T⇥M. We only observe the
measurements for a subset of locations ⌦ ⇢{ 1  . . .   P} and their side information such as longitude
and latitude. Given the measurements X⌦ and the side information  the goal is to estimate a tensor
W2 RP⇥T⇥M that satisﬁes W⌦ = X⌦. Here X⌦ represents the outcome of applying the index
operator I⌦ to X: : m for all variables m = 1  . . .   M. The index operator I⌦ is a diagonal matrix
whose entries are one for the locations included in ⌦ and zero otherwise.
Two key consistency principles have been identiﬁed for effective cokriging [10  Chapter 6.2]: (1)
Global consistency: the data on the same structure are likely to be similar. (2) Local consistency: the
data in close locations are likely to be similar. The former principle is akin to the cluster assumption
in semi-supervised learning [26]. We incorporate these principles in a concise and computationally
efﬁcient low-rank tensor learning framework.
To achieve global consistency  we constrain the tensor W to be low rank. The low rank assumption
is based on the belief that high correlations exist within variables  locations and time  which leads to
natural clustering of the data. Existing literature have explored the low rank structure among these
three dimensions separately  e.g.  multi-task learning [20] for variable correlation  ﬁxed rank kriging
[8] for spatial correlations. Low rankness assumes that the observed data can be described with a
few latent factors. It enforces the commonalities along three dimensions without an explicit form
for the shared structures in each dimension.

2

For local consistency  we construct a regularizer via the spatial Laplacian matrix. The Laplacian
matrix is deﬁned as L = D  A  where A is a kernel matrix constructed by pairwise similarity
and diagonal matrix Di i = Pj(Ai j). Similar ideas have been explored in matrix completion
[17]. In cokriging literature  the local consistency is enforced via the spatial covariance matrix. The
Bayesian models often impose the Gaussian process prior on the observations with the covariance
matrix K = Kv ⌦ Kx where Kv is the covariance between variables and Kx is that for locations.
The Laplacian regularization term corresponds to the relational Gaussian process [6] where the
covariance matrix is approximated by the spatial Laplacian.
In summary  we can perform cokriging and ﬁnd the value of tensor W by solving the following
optimization problem:
cW = argmin
where the Frobenius norm of a tensor A is deﬁned as kAkF = qPi j k A2

i j k and µ  ⇢ > 0
are the parameters that make tradeoff between the local and global consistency  respectively. The
low rank constraint ﬁnds the principal components of the tensor and reduces the complexity of
the model while the Laplacian regularizer clusters the data using the relational information among
the locations. By learning the right tradeoff between these two techniques  our method is able to
beneﬁt from both. Due to the various deﬁnitions of tensor rank  we use rank as supposition for rank
complexity  which will be speciﬁed in later section.

tr(W>: : mLW: : m)) s.t.

W (kW⌦ X ⌦k2

rank(W)  ⇢ 

MXm=1

F + µ

(1)

2.2 Forecasting

for m = 1  . . .   M and t = K + 1  . . .   T 

X: t m = W: : mXt m + E: t m 

Forecasting estimates the future value of multivariate time series given historical observations.
For ease of presentation  we use the classical VAR model with K lags and coefﬁcient tensor
W2 RP⇥KP⇥M as an example. Using the matrix representation  the VAR(K) process deﬁnes
the following data generation process:
(2)
where Xt m = [X >: t1 m  . . .  X >: tK m]> denotes the concatenation of K-lag historical data before
time t. The noise tensor E is a multivariate Gaussian with zero mean and unit variance .
Existing multivariate regression methods designed to capture the complex correlations  such as
Tucker decomposition [21]  are computationally expensive. A scalable solution requires a simpler
model that also efﬁciently accounts for the shared structures in variables  space  and time. Similar
global and local consistency principles still hold in forecasting. For global consistency  we can use
low rank constraint to capture the commonalities of the variables as well as the spatial correlations
on the model parameter tensor  as in [9]. For local consistency  we enforce the predicted value
for close locations to be similar via spatial Laplacian regularization. Thus  we can formulate the
forecasting task as the following optimization problem over the model coefﬁcient tensor W:
cW = argmin

tr(bX >: : mLbX: : m)) s.t. rank(W)  ⇢  bX: t m = W: : mXt m

(3)
Though cokriging and forecasting are two different tasks  we can easily see that both formulations
follow the global and local consistency principles and can capture the inter-correlations from spatial-
temporal data.

W (kbXXk

MXm=1

2
F + µ

2.3 Uniﬁed Framework

We now show that both cokriging and forecasting can be formulated into the same tensor learning
framework. Let us rewrite the loss function in Eq. (1) and Eq. (3) in the form of multitask regression
and complete the quadratic form for the loss function. The cokriging task can be reformulated as
follows:

kW: : mH  (H>)1X⌦ mk2

rank(W)  ⇢

(4)

F) s.t.

W ( MXm=1

cW = argmin

3

where we deﬁne HH> = I⌦ + µL.1 For the forecasting problem  HH> = IP + µL and we have:

W ( MXm=1

TXt=K+1

kHW: : mXt m  (H1)X: t mk2

cW = argmin
By slight change of notation (cf. Appendix D)  we can easily see that the optimal solution of both
problems can be obtained by the following optimization problem with appropriate choice of tensors
Y and V:

rank(W)  ⇢ 

(5)

F) s.t.

W ( MXm=1

cW = argmin

kW: : mY: : m V : : mk2

rank(W)  ⇢.

(6)

F) s.t.

After unifying the objective function  we note that tensor rank has different notions such as CP
rank  Tucker rank and mode n-rank [16  12]. In this paper  we choose the mode-n rank  which is
computationally more tractable [12  24]. The mode-n rank of a tensor W is the rank of its mode-n
unfolding W(n).2 In particular  for a tensor W with N mode  we have the following deﬁnition:

mode-n rank(W) =

rank(W(n)).

(7)

A common practice to solve this formulation with mode n-rank constraint is to relax the rank con-
straint to a convex nuclear norm constraint [12  24]. However  those methods are computationally
expensive since they need full singular value decomposition of large matrices. In the next section 
we present a fast greedy algorithm to tackle the problem.

NXn=1

3 Fast greedy low rank tensor learning

To solve the non-convex problem in Eq. (6) and ﬁnd its optimal solution  we propose a greedy
learning algorithm by successively adding rank-1 estimation of the mode-n unfolding. The main
idea of the algorithm is to unfold the tensor into a matrix  seek for its rank-1 approximation and
then fold back into a tensor with same dimensionality. We describe this algorithm in three steps:
(i) First  we show that we can learn rank-1 matrix estimations efﬁciently by solving a generalized
eigenvalue problem  (ii) We use the rank-1 matrix estimation to greedily solve the original tensor
rank constrained problem  and (iii) We propose an enhancement via orthogonal projections after
each greedy step.

Optimal rank-1 Matrix Learning The following lemma enables us to ﬁnd such optimal rank-1
estimation of the matrices.
Lemma 1. Consider the following rank constrained problem:

A:rank(A)=1nkY  AXk2
Fo  

bA1 = argmin

(8)

problem:

where Y 2 Rq⇥n  X 2 Rp⇥n  and A 2 Rq⇥p. The optimal solution of bA1 can be written as
bA1 =bubv>  kbvk2 = 1 wherebv is the dominant eigenvector of the following generalized eigenvalue
andbu can be computed as

(XY >Y X>)v = (XX>)v

(10)

(9)

1

The lemma can be found in e.g.
(9) is a
generalized eigenvalue problem whose dominant eigenvector can be found efﬁciently [13]. If XX>
is full rank  as assumed in Theorem 2  the problem is simpliﬁed to a regular eigenvalue problem
whose dominant eigenvector can be efﬁciently computed.

[2] and we also provide a proof in Appendix A. Eq.

bu =

bv>XX>bv

Y X>bv.

1We can use Cholesky decomposition to obtain H. In the rare cases that I⌦ + µL is not full rank  ✏IP is

added where ✏ is a very small positive value.

2The mode-n unfolding of a tensor is the matrix resulting from treating n as the ﬁrst mode of the matrix 

and cyclically concatenating other modes. Tensor refolding is the reverse direction operation [16].

4

Algorithm 1 Greedy Low-rank Tensor Learning
1: Input: transformed data Y V of M variables  stopping criteria ⌘
2: Output: N mode tensor W
3: Initialize W 0
4: repeat
5:
6:

for n = 1 to N do
Bn argmin
n L (W;Y V) L (refold(W(n) + Bn);Y V)

B: rank(B)=1L(refold(W(n) + B);Y V)

{n}

n

end if

end for
n⇤ argmax
if n⇤ >⌘ then

7:
8:
9:
10:
11:
12:
13: W argminrow(A(1))✓row(W(1))
14: until n⇤ <⌘

W W + refold(Bn⇤  n⇤)

col(A(1))✓col(W(1)) L(A;Y V)

# Optional Orthogonal Projection Step.

Greedy Low n-rank Tensor Learning The optimal rank-1 matrix learning serves as a basic ele-
ment in our greedy algorithm. Using Lemma 1  we can solve the problem in Eq. (6) in the Forward
Greedy Selection framework as follows: at each iteration of the greedy algorithm  it searches for the
mode that gives the largest decrease in the objective function. It does so by unfolding the tensor in
that mode and ﬁnding the best rank-1 estimation of the unfolded tensor. After ﬁnding the optimal
mode  it adds the rank-1 estimate in that mode to the current estimation of the tensor. Algorithm
F . Note
that we can ﬁnd the optimal rank-1 solution in only one of the modes  but it is enough to guarantee
the convergence of our greedy algorithm.
Theorem 2 bounds the difference between the loss function evaluated at each iteration of the greedy
algorithm and the one at the globally optimal solution.
Theorem 2. Suppose in Eq. (6) the matrices Y>: : mY: : m for m = 1  . . .   M are positive deﬁnite.
The solution of Algo. 1 at its kth iteration step satisﬁes the following inequality:

1 shows the details of this approach  where L(W;Y V) =PM

m=1 kW: : mY: : m V : : mk2

L(Wk;Y V) L (W⇤;Y V) 

(kYk2kW⇤(1)k⇤)2

(k + 1)

 

(11)

where W⇤ is the global minimizer of the problem in Eq. (6) and kYk2 is the largest singular value
of a block diagonal matrix created by placing the matrices Y(:  :  m) on its diagonal blocks.
The detailed proof is given in Appendix B. The key idea of the proof is that the amount of decrease
in the loss function by each step in the selected mode is not smaller than the amount of decrease if we
had selected the ﬁrst mode. The theorem shows that we can obtain the same rate of convergence for
learning low rank tensors as achieved in [23] for learning low rank matrices. The greedy algorithm
in Algorithm 1 is also connected to mixture regularization in [24]: the mixture approach decomposes
the solution into a set of low rank structures while the greedy algorithm successively learns a set of
rank one components.

Greedy Algorithm with Orthogonal Projections
It is well-known that the forward greedy algo-
rithm may make steps in sub-optimal directions because of noise. A common solution to alleviate the
effect of noise is to make orthogonal projections after each greedy step [3  22]. Thus  we enhance the
forward greedy algorithm by projecting the solution into the space spanned by the singular vectors
of its mode-1 unfolding. The greedy algorithm with orthogonal projections performs an extra step in
line 13 of Algorithm 1: It ﬁnds the top k singular vectors of the solution: [U  S  V ] svd(W(1)  k)
where k is the iteration number. Then it ﬁnds the best solution in the space spanned by U and V by
solving bS minS L(U SV > Y V) which has a closed form solution. Finally  it reconstructs the
solution: W refold(UbSV >  1). Note that the projection only needs to ﬁnd top k singular vectors

which can be computed efﬁciently for small values of k.

5

E
S
M
R
 
n
o
i
t
a
m

i
t
s
E

 
r
e
t
e
m
a
r
a
P

1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
 
0

 

Forward
Orthogonal
ADMM
Trace
MTL−L1
MTL−L21
MTL−Dirty

50

100
150
# of Samples
(a) RMSE

200

250

l

y
t
i
x
e
p
m
o
C
 
k
n
a
R
 
e
r
u
t
x
M

i

20

15

10

5

0

−5
 
0

Forward

Orthogonal

ADMM

Trace

Forward Greedy
Orthogonal Greedy
ADMM

 

1200

1000

800

600

400

200

)
c
e
S

i

(
 
e
m
T
 
n
u
R

150

200

0
 
101

50

100

# of Samples
(b) Rank

# of Variables
(c) Scalability

 

102

Figure 1: Tensor estimation performance comparison on the synthetic dataset over 10 random runs.
(a) parameter Estimation RMSE with training time series length  (b) Mixture Rank Complexity with
training time series length  (c) running time for one single round with respect to number of variables.
4 Experiments
We evaluate the efﬁcacy of our algorithms on synthetic datasets and real-world application datasets.

4.1 Low rank tensor learning on synthetic data

For empirical evaluation  we compare our method with multitask learning (MTL) algorithms  which
also utilize the commonalities between different prediction tasks for better performance. We use the
following baselines: (1) Trace norm regularized MTL (Trace)  which seeks the low rank structure
only on the task dimension; (2) Multilinear MTL [21]  which adapts the convex relaxation of low
rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [11] and
Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1   MTL-L21
[20]  and MTL-LDirty [15]  which investigate joint sparsity of the tasks with Lp norm regularization.
For MTL-L1   MTL-L21 [20] and MTL-LDirty  we use MALSAR Version 1.1 [28].
We construct a model coefﬁcient tensor W of size 20 ⇥ 20 ⇥ 10 with CP rank equals to 1.
Then  we generate the observations Y and V according to multivariate regression model V: : m =
W: : mY: : m +E: : m for m = 1  . . .   M  where E is tensor with zero mean Gaussian noise elements.
We split the synthesized data into training and testing time series and vary the length of the training
time series from 10 to 200. For each training length setting  we repeat the experiments for 10 times
and select the model parameters via 5-fold cross validation. We measure the prediction performance
via two criteria: parameter estimation accuracy and rank complexity. For accuracy  we calculate the
RMSE of the estimation versus the true model coefﬁcient tensor. For rank complexity  we calculate
the mixture rank complexity [24] as M RC = 1
The results are shown in Figure 1(a) and 1(b). We omit the Tucker decomposition as the results are
not comparable. We can clearly see that the proposed greedy algorithm with orthogonal projections
achieves the most accurate tensor estimation. In terms of rank complexity  we make two observa-
tions: (i) Given that the tensor CP rank is 1  greedy algorithm with orthogonal projections produces
the estimate with the lowest rank complexity. This can be attributed to the fact that the orthogonal
projections eliminate the redundant rank-1 components that fall in the same spanned space. (ii) The
rank complexity of the forward greedy algorithm increases as we enlarge the sample size. We be-
lieve that when there is a limited number of observations  most of the new rank-1 elements added
to the estimate are not accurate and the cross-validation steps prevent them from being added to the
model. However  as the sample size grows  the rank-1 estimates become more accurate and they are
preserved during the cross-validation.
To showcase the scalability of our algorithm  we vary the number of variables and generate a series
of tensor W2 R20⇥20⇥M for M from 10 to 100 and record the running time (in seconds) for three
tensor learning algorithms  i.e  forward greedy  greedy with orthogonal projections and ADMM. We
measure the run time on a machine with a 6-core 12-thread Intel Xenon 2.67GHz processor and
12GB memory. The results are shown in Figure 1(c). The running time of ADMM increase rapidly
with the data size while the greedy algorithm stays steady  which conﬁrms the speedup advantage
of the greedy algorithm.

nPN
n=1 rank(W(n)).

6

Table 1: Cokriging RMSE of 6 methods averaged over 10 runs. In each run  10% of the locations
are assumed missing.

DATASET
USHCN
CCDS
YELP

FOURSQUARE

ADMM FORWARD ORTHOGONAL
0.8051
0.8292
0.7730
0.1373

0.7594
0.5555
0.6993
0.1338

0.7210
0.4532
0.6958
0.1334

NA
NA

SIMPLE ORDINARY MTGP
0.8760
1.0007
1.0296
0.7634

0.7803
0.7312

NA
NA

NA
NA

4.2 Spatio-temporal analysis on real world data

We conduct cokriging and forecasting experiments on four real-world datasets:
USHCN The U.S. Historical Climatology Network Monthly (USHCN)3 dataset consists of
monthly climatological data of 108 stations spanning from year 1915 to 2000.
It has three cli-
mate variables: (1) daily maximum  (2) minimum temperature averaged over month  and (3) total
monthly precipitation.
CCDS The Comprehensive Climate Dataset (CCDS)4 is a collection of climate records of North
America from [19]. The dataset was collected and pre-processed by ﬁve federal agencies. It contains
monthly observations of 17 variables such as Carbon dioxide and temperature spanning from 1990 to
2001. The observations were interpolated on a 2.5⇥2.5 degree grid  with 125 observation locations.
Yelp The Yelp dataset5 contains the user rating records for 22 categories of businesses on Yelp
over ten years. The processed dataset includes the rating values (1-5) binned into 500 time intervals
and the corresponding social graph for 137 active users. The dataset is used for the spatio-temporal
recommendation task to predict the missing user ratings across all business categories.
Foursquare The Foursquare dataset [18] contains the users’ check-in records in Pittsburgh area
from Feb 24 to May 23  2012  categorized by different venue types such as Art & Entertainment 
College & University  and Food. The dataset records the number of check-ins by 121 users in each
of the 15 category of venues over 1200 time intervals  as well as their friendship network.

4.2.1 Cokriging
We compare the cokriging performance of our proposed method with the classical cokriging ap-
proaches including simple kriging and ordinary cokriging with nonbias condition [14] which are
applied to each variables separately. We further compare with multitask Gaussian process (MTGP)
[5] which also considers the correlation among variables. We also adapt ADMM for solving the
nuclear norm relaxed formulation of the cokriging formulation as a baseline (see Appendix C for
more details). For USHCN and CCDS  we construct a Laplacian matrix by calculating the pairwise
Haversine distance of locations. For Foursquare and Yelp  we construct the graph Laplacian from
the user friendship network.
For each dataset  we ﬁrst normalize it by removing the trend and diving by the standard deviation.
Then we randomly pick 10% of locations (or users for Foursquare) and eliminate the measurements
of all variables over the whole time span. Then  we produce the estimates for all variables of each
timestamp. We repeat the procedure for 10 times and report the average prediction RMSE for all
timestamps and 10 random sets of missing locations. We use the MATLAB Kriging Toolbox6 for
the classical cokriging algorithms and the MTGP code provided by [5].
Table 1 shows the results for the cokriging task. The greedy algorithm with orthogonal projections is
signiﬁcantly more accurate in all three datasets. The baseline cokriging methods can only handle the
two dimensional longitude and latitude information  thus are not applicable to the Foursquare and
Yelp dataset with additional friendship information. The superior performance of the greedy algo-
rithm can be attributed to two of its properties: (1) It can obtain low rank models and achieve global
consistency; (2) It usually has lower estimation bias compared to nuclear norm relaxed methods.

3http://www.ncdc.noaa.gov/oa/climate/research/ushcn
4http://www-bcf.usc.edu/˜liu32/data/NA-1990-2002-Monthly.csv
5http://www.yelp.com/dataset_challenge
6http://globec.whoi.edu/software/kriging/V3/english.html

7

Table 2: Forecasting RMSE for VAR process with 3 lags  trained with 90% of the time series.
DATASET TUCKER ADMM FORWARD ORTHO ORTHONL TRACE MTLl1 MTLl21 MTLdirty
USHCN 0.8975
0.9735
CCDS
0.9438
1.0950
0.1504
0.1492
FSQ

0.9273 0.9528 0.9543
0.8632 0.9105 0.9171
0.1245 0.1495 0.1495

0.9227
0.8448
0.1407

0.9175
0.8555
0.1234

0.9171
0.8810
0.1241

0.9069
0.8325
0.1223

Table 3: Running time (in seconds) for cokriging and forecasting.
FORECASTING

COKRIGING

DATASET USHCN
93.03
ORTHO
ADMM
791.25

CCDS
16.98
320.77

YELP
78.47
2928.37

FSQ
91.51
720.40

USHCN CCDS
21.38
75.47
235.73
45.62

FSQ
37.70
33.83

p=1PM

as w(t) = PP

4.2.2 Forecasting
We present the empirical evaluation on the forecasting task by comparing with multitask regression
algorithms. We split the data along the temporal dimension into 90% training set and 10% testing
set. We choose VAR(3) model and during the training phase  we use 5-fold cross-validation.
As shown in Table 2  the greedy algorithm with orthogonal projections again achieves the best pre-
diction accuracy. Different from the cokriging task  forecasting does not necessarily need the cor-
relations of locations for prediction. One might raise the question as to whether the Laplacian reg-
ularizer helps. Therefore  we report the results for our formulation without Laplacian (ORTHONL)
for comparison. For efﬁciency  we report the running time (in seconds) in Table 3 for both tasks of
cokriging and forecasting. Compared with ADMM  which is a competitive baseline also capturing
the commonalities among variables  space  and time  our greedy algorithm is much faster for most
datasets.
As a qualitative study  we plot the map of most pre-
dictive regions analyzed by the greedy algorithm us-
ing CCDS dataset in Fig. 2. Based on the concept
of how informative the past values of the climate
measurements in a speciﬁc location are in predict-
ing future values of other time series  we deﬁne the
aggregate strength of predictiveness of each region
m=1 |Wp t m|. We can see that
two regions are identiﬁed as the most predictive re-
gions: (1) The southwest region  which reﬂects the
impact of the Paciﬁc ocean and (2) The southeast re-
gion  which frequently experiences relative sea level
rise  hurricanes  and storm surge in Gulf of Mexico.
Another interesting region lies in the center of Col-
orado  where the Rocky mountain valleys act as a
funnel for the winds from the west  providing locally
divergent wind patterns.
5 Conclusion
In this paper  we study the problem of multivariate spatio-temporal data analysis with an emphasis
on two tasks: cokriging and forecasting. We formulate the problem into a general low rank tensor
learning framework which captures both the global consistency and the local consistency principle.
We develop a fast and accurate greedy solver with theoretical guarantees for its convergence. We
validate the correctness and efﬁciency of our proposed method on both the synthetic dataset and real-
application datasets. For future work  we are interested in investigating different forms of shared
structure and extending the framework to capture non-linear correlations in the data.
Acknowledgment
We thank the anonymous reviewers for their helpful feedback and comments. The research was
sponsored by the NSF research grants IIS-1134990  IIS- 1254206 and Okawa Foundation Research
Award. The views and conclusions are those of the authors and should not be interpreted as repre-
senting the ofﬁcial policies of the funding agency  or the U.S. Government.

Figure 2: Map of most predictive regions
analyzed by the greedy algorithm using 17
variables of the CCDS dataset. Red color
means high predictiveness whereas blue de-
notes low predictiveness.

8

References
[1] T. Anderson. Estimating linear restrictions on regression coefﬁcients for multivariate normal

distributions. The Annals of Mathematical Statistics  pages 327–351  1951.

[2] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from

examples without local minima. Neural networks  2(1):53–58  1989.

[3] A. Barron  A. Cohen  W. Dahmen  and R. DeVore. Approximation and learning by greedy

algorithms. The Annals of Statistics  2008.

[4] D. Bertsekas and J. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods.

[5] E. Bonilla  K. Chai  and C. Williams. Multi-task Gaussian Process Prediction. In NIPS  2007.
[6] W. Chu  V. Sindhwani  Z. Ghahramani  and S. Keerthi. Relational learning with Gaussian

[7] N. Cressie and H. Huang. Classes of nonseparable  spatio-temporal stationary covariance

Prentice Hall Inc  1989.

processes. In NIPS  2006.

functions. JASA  1999.

[8] N. Cressie and G. Johannesson. Fixed rank kriging for very large spatial data sets. JRSS B

(Statistical Methodology)  70(1):209–226  2008.

[9] N. Cressie  T. Shi  and E. Kang. Fixed rank ﬁltering for spatio-temporal data. J. Comp. Graph.

Stat.  2010.

[10] N. Cressie and C. Wikle. Statistics for spatio-temporal data. John Wiley & Sons  2011.
[11] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems
via ﬁnite element approximation. Computers & Mathematics with Applications  2(1):17–40 
1976.

[12] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via

convex optimization. Inverse Problems  2011.

[13] M. Gu  A. Ruhe  G. Sleijpen  H. van der Vorst  Z. Bai  and R. Li. 5. Generalized Hermitian

Eigenvalue Problems. Society for Industrial and Applied Mathematics  2000.

[14] E. Isaaks and R. Srivastava. Applied geostatistics. London: Oxford University  2011.
[15] A. Jalali  S. Sanghavi  C. Ruan  and P. Ravikumar. A dirty model for multi-task learning. In

NIPS  2010.

In UbiComp  2012.

[16] T. Kolda and B. Bader. Tensor decompositions and applications. SIAM review  2009.
[17] W.-J. Li and D.-Y. Yeung. Relation regularized matrix factorization. In IJCAI  2009.
[18] X. Long  L. Jin  and J. Joshi. Exploring trajectory-driven local geographic topics in foursquare.

[19] A. Lozano  H. Li  A. Niculescu-Mizil  Y. Liu  C. Perlich  J. Hosking  and N. Abe. Spatial-

temporal causal modeling for climate change attribution. In KDD  2009.

[20] F. Nie  H. Huang  X. Cai  and C. H. Ding. Efﬁcient and robust feature selection via joint

`2 1-norms minimization. In NIPS  2010.

[21] B. Romera-Paredes  H. Aung  N. Bianchi-Berthouze  and M. Pontil. Multilinear multitask

learning. In ICML  2013.

rank constraint. In ICML  2011.

[22] S. Shalev-Shwartz  A. Gonen  and O. Shamir. Large-scale convex minimization with a low-

[23] S. Shalev-Shwartz  N. Srebro  and T. Zhang. Trading Accuracy for Sparsity in Optimization

Problems with Sparsity Constraints. SIAM Journal on Optimization  2010.

[24] R. Tomioka  K. Hayashi  and H. Kashima. Convex Tensor Decomposition via Structured

Schatten Norm Regularization. NIPS  2013.

[25] T. Zhang. Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representa-

tions. IEEE Trans Inf Theory  pages 4689–4708  2011.

[26] D. Zhou  O. Bousquet  T. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global

[27] H. Zhou  L. Li  and H. Zhu. Tensor regression with applications in neuroimaging data analysis.

consistency. In NIPS  2003.

JASA  2013.

[28] J. Zhou  J. Chen  and J. Ye. MALSAR: Multi-tAsk Learning via StructurAl Regularization.

http://www.public.asu.edu/˜jye02/Software/MALSAR/  2011.

9

,Mohammad Taha Bahadori
Qi (Rose) Yu
Yan Liu