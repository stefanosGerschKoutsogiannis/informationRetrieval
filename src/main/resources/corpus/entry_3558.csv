2015,Multi-Layer Feature Reduction for Tree Structured Group Lasso via Hierarchical Projection,Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features  where each node encodes a group of features. It has been applied successfully in many real-world applications. However  with extremely large feature dimensions  solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper  we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion  which are guaranteed to be irrelevant to the response. Thus  we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm  MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover  we can integrate MLFre---that has a low computational cost---with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.,Multi-Layer Feature Reduction for Tree Structured

Group Lasso via Hierarchical Projection

1Computational Medicine and Bioinformatics

2Department of Electrical Engineering and Computer Science

Jie Wang1  Jieping Ye1 2

University of Michigan  Ann Arbor  MI 48109

{jwangumi  jpye}@umich.edu

Abstract

Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree
structured sparsity over the features  where each node encodes a group of features.
It has been applied successfully in many real-world applications. However  with
extremely large feature dimensions  solving TGL remains a signiﬁcant challenge
due to its highly complicated regularizer. In this paper  we propose a novel Multi-
Layer Feature reduction method (MLFre) to quickly identify the inactive nodes
(the groups of features with zero coefﬁcients in the solution) hierarchically in a
top-down fashion  which are guaranteed to be irrelevant to the response. Thus  we
can remove the detected nodes from the optimization without sacriﬁcing accura-
cy. The major challenge in developing such testing rules is due to the overlaps
between the parents and their children nodes. By a novel hierarchical projec-
tion algorithm  MLFre is able to test the nodes independently from any of their
ancestor nodes. Moreover  we can integrate MLFre—that has a low computation-
al cost—with any existing solvers. Experiments on both synthetic and real data
sets demonstrate that the speedup gained by MLFre can be orders of magnitude.

Introduction

1
Tree structured group Lasso (TGL) [13  30] is a powerful regression technique in uncovering the
hierarchical sparse patterns among the features. The key of TGL  i.e.  the tree guided regularization 
is based on a pre-deﬁned tree structure and the group Lasso penalty [29]  where each node represents
a group of features. In recent years  TGL has achieved great success in many real-world applications
such as brain image analysis [10  18]  gene data analysis [14]  natural language processing [27  28] 
and face recognition [12]. Many algorithms have been proposed to improve the efﬁciency of TGL
[1  6  11  7  16]. However  the application of TGL to large-scale problems remains a challenge due
to its highly complicated regularizer.
As an emerging and promising technique in scaling large-scale problems  screening has received
much attention in the past few years. Screening aims to identify the zero coefﬁcients in the sparse
solutions by simple testing rules such that the corresponding features can be removed from the
optimization. Thus  the size of the data matrix can be signiﬁcantly reduced  leading to substantial
savings in computational cost and memory usage. Typical examples include TLFre [25]  FLAMS
[22]  EDPP [24]  Sasvi [17]  DOME [26]  SAFE [8]  and strong rules [21]. We note that strong rules
are inexact in the sense that features with nonzero coefﬁcients may be mistakenly discarded  while
the others are exact. Another important direction of screening is to detect the non-support vectors for
support vector machine (SVM) and least absolute deviation (LAD) [23  19]. Empirical studies have
shown that the speedup gained by screening methods can be several orders of magnitude. Moreover 
the exact screening methods improve the efﬁciency without sacriﬁcing optimality.
However  to the best of our knowledge  existing screening methods are only applicable to sparse
models with simple structures such as Lasso  group Lasso  and sparse group Lasso. In this paper  we

1

propose a novel Multi-Layer Feature reduction method  called MLFre  for TGL. MLFre is exact and
it tests the nodes hierarchically from the top level to the bottom level to quickly identify the inactive
nodes (the groups of features with zero coefﬁcients in the solution vector)  which are guaranteed to
be absent from the sparse representation. To the best of our knowledge  MLFre is the ﬁrst screening
method that is applicable to TGL with the highly complicated tree guided regularization.
The major technical challenges in developing MLFre for TGL lie in two folds. The ﬁrst is that
most existing exact screening methods are based on evaluating the norm of the subgradients of the
sparsity-inducing regularizers with respect to the variables or groups of variables of interests. How-
ever  for TGL  we only have access to a mixture of the subgradients due to the overlaps between
parents and their children nodes. Therefore  our ﬁrst major technical contribution is a novel hier-
archical projection algorithm that is able to exactly and efﬁciently recover the subgradients with
respect to every node from the mixture (Sections 3 and 4). The second technical challenge is that
most existing exact screening methods need to estimate an upper bound involving the dual optimum.
This turns out to be a complicated nonconvex optimization problem for TGL. Thus  our second major
technical contribution is to show that this highly nontrivial nonconvex optimization problem admits
closed form solutions (Section 5). Experiments on both synthetic and real data sets demonstrate that
the speedup gained by MLFre can be orders of magnitude (Section 6). Please see supplements for
detailed proofs of the results in the main text.
Notation: Let (cid:107)·(cid:107) be the (cid:96)2 norm  [p] = {1  . . .   p} for a positive integer p  G ⊆ [p]  and ¯G = [p]\G.
For u ∈ Rp  let ui be its ith component. For G ⊆ [p]  we denote uG = [u]G = {v : vi = ui if i ∈
G  vi = 0 otherwise} and HG = {u ∈ Rp : u ¯G = 0}.
If G1  G2 ⊆ [n] and G1 ⊂ G2  we
emphasize that G2 \ G1 (cid:54)= ∅. For a set C  let intC  ri C  bd C  and rbd C be its interior  relative
interior  boundary  and relative boundary  respectively [5]. If C is closed and convex  the projection
operator is PC(z) := argminu∈C(cid:107)z − u(cid:107)  and its indicator function is IC(·)  which is 0 on C and ∞
elsewhere. Let Γ0(Rp) be the class of proper closed convex functions on Rp. For f ∈ Γ0(Rp)  let
∂f be its subdifferential and dom f := {z : f (z) < ∞}. We denote by γ+ = max(γ  0).
2 Basics
We brieﬂy review some basics of TGL. First  we introduce the so-called index tree.
Deﬁnition 1. [16] For an index tree T of depth d  we denote the node(s) of depth i by Ti =
{Gi
(i): Gi
j1
(ii): If Gi
When the tree structure is available (see supplement for an example)  the TGL problem is

j ⊂ [p]  and ni ≥ 1  ∀ i ∈ [d]. We assume that
}  where n0 = 1  G0
= ∅  ∀ i ∈ [d] and j1 (cid:54)= j2 (different nodes of the same depth do not overlap).
(cid:96) ⊂ Gi
j.
(cid:88)d

1  . . .   Gi
ni
∩ Gi
j is a parent node of Gi+1

(cid:88)ni

1 = [p]  Gi

  then Gi+1

j2

(cid:96)

1

(TGL)

j are the coefﬁcients
j  respectively  and λ > 0 is the regularization

and wi

j

(cid:107). The following hold:

j

j

min

β

i=0

j=1

(i): Let φi

(cid:107) and Bi

j(β) = (cid:107)βGi

2(cid:107)y − Xβ(cid:107)2 + λ

Theorem 2. For the TGL problem  let φ(β) =(cid:80)d

j(cid:107)βGi
(cid:107) 
wi
where y ∈ RN is the response vector  X ∈ RN×p is the data matrix  βGi
(cid:80)ni
vector and positive weight corresponding to node Gi
parameter. We derive the Lagrangian dual problem of TGL as follows.
j(cid:107)βGi
j=1 wi
(cid:88)d
(cid:88)ni
j}. We can write ∂φ(0) as
λ − θ(cid:107)2 : θ ∈ F(cid:9) .
2(cid:107) y
(cid:88)ni

(cid:88)d
(cid:88)ni
j = {ζ ∈ HGi
(cid:8) 1
2(cid:107)y(cid:107)2 − 1

(ii): Let F = {θ : XT θ ∈ ∂φ(0)}. The Lagrangian dual of TGL is

y = Xβ∗(λ) + λθ∗(λ) 

XT θ∗(λ) ∈(cid:88)d

: (cid:107)ζ(cid:107) ≤ wi

wi

j∂φi

∂φ(0) =

j(0) =

wi

j∂φi

j(β∗(λ)).

sup

θ

i=0

j=1

i=0

j=1

Bi
j.

i=0

j

j

i=0

j=1

(iii): Let β∗(λ) and θ∗(λ) be the optimal solution of problems (TGL) and (2)  respectively. Then 

(1)

(2)

(3)

(4)

The dual problem of TGL in (2) is equivalent to a projection problem  i.e.  θ∗(λ) = PF (y/λ). This
geometric property plays a fundamentally important role in developing MLFre (see Section 5).

2

3 Testing Dual Feasibility via Hierarchical Projection
Although the dual problem in (2) has nice geometric properties  it is challenging to determine the
feasibility of a given θ due to the complex dual feasible set F. An alternative approach is to test
if XT θ = P∂φ(0)(XT θ). Although ∂φ(0) is very complicated  we show that P∂φ(0)(·) admits a
closed form solution by hierarchically splitting P∂φ(0)(·) into a sum of projection operators with
respect to a collection of simpler sets. We ﬁrst introduce some notations. For an index tree T   let

(cid:110)(cid:88)
(cid:110)(cid:88)

Ai
Bt
k : Gt
j =
Ci
Bt
k : Gt
j =
j  the set Ai
j is the sum of Bt

t k

t k

(cid:111)
(cid:111)
k ⊆ Gi
k ⊂ Gi
(6)
k corresponding to all its descendant nodes and itself  and

 ∀ i ∈ 0 ∪ [d]  j ∈ [ni] 
 ∀ i ∈ 0 ∪ [d]  j ∈ [ni].

(5)

j

j

For a node Gi
the set Ci

j = Bi

1  Ai

j  Bi
j = Bi

j  and Ci
j  we have
j ∀ leaf node Gi
j 

j the sum excluding itself. Therefore  by the deﬁnitions of Ai
j  Ai
∂φ(0) = A0

j ∀ non-leaf node Gi
j + Ci
(·) = PB0
(·) into the sum of two projections onto B0

(7)
(·). This motivates the ﬁrst pillar of this paper 
which implies that P∂φ(0)(·) = PA0
i.e.  Lemma 3  which splits PB0
1  respectively.
1+C0
Lemma 3. Let G ⊆ [p]  B = {u ∈ HG : (cid:107)u(cid:107) ≤ γ} with γ > 0  C ⊆ HG a nonempty closed convex
set  and z an arbitrary point in HG. Then  the following hold:
(i): [2] PB(z) = min{1  γ/(cid:107)z(cid:107)}z if z (cid:54)= 0. Otherwise  PB(z) = 0.
(ii): IB+C(z) = IB(z − PC(z))  i.e.  PC(z) ∈ argminu∈C IB(z − u).
(iii): PB+C(z) = PC(z) + PB(z − PC(z)).
By part (iii) of Lemma 3  we can split PA0

(XT θ) in the following form:

1 and C0

1+C0

1

1

1

PA0

1

(XT θ) = PC0

(XT θ) + PB0

1

(XT θ − PC0

1

1

(XT θ)).

1

(8)
(XT θ) if we

(9)

As PB0
have PC0

1

(·) admits a closed form solution by part (i) of Lemma 3  we can compute PA0
(XT θ) computed. By Eq. (5) and Eq. (6)  for a non-leaf node Gi
j  we note that
j) = {k : Gi+1

  where Ic(Gi

k ⊂ Gi
j}.

(cid:88)

Ci
j =

Ai+1

k

1

1

k∈Ic(Gi
j )

nonempty closed convex sets  and C =(cid:80)

Inspired by (9)  we have the following result.
Lemma 4. Let {G(cid:96) ⊂ [p]}(cid:96) be a set of nonoverlapping index sets  {C(cid:96) ⊆ HG(cid:96)}(cid:96) be a set of
Remark 1. For Lemma 4  if all C(cid:96) are balls centered at 0  then PC(z) admits a closed form solution.
By Lemma 4 and Eq. (9)  we can further splits PC0
([XT θ]G1

(cid:96) C(cid:96). Then  PC(z) =(cid:80)

(cid:96) PC(cid:96)(zG(cid:96) ) for z ∈ Rp.

(XT θ) in Eq. (8) in the following form.
1}.
)  where Ic(G0
k ⊂ G0
k is a leaf node  Eq. (7) implies that A1
k = B1

k and
Consider the right hand side of Eq. (10). If G1
(·) admits a closed form solution by part (i) of Lemma 3. Otherwise  we continue to split
thus PA1
(·) by Lemmas (3) and (4). This procedure will always terminate as we reach the leaf nodes
PA1
[see the last equality in Eq. (7)]. Therefore  by a repeated application of Lemmas (3) and (4)  the
following algorithm computes the closed form solution of PA0

1) = {k : G1

(cid:88)

k∈Ic(G0
1)

(XT θ) =

(·).

PA1

(10)

PC0

k

k

k

k

1

1

1

(·).

1

Algorithm 1 Hierarchical Projection: PA0
Input: z ∈ Rp  the index tree T as in Deﬁnition 1  and positive weights wi
Output: u0 = PA0
1: Set ui ← 0 ∈ Rp  ∀ i ∈ 0 ∪ [d + 1]  vi ← 0 ∈ Rp  ∀ i ∈ 0 ∪ [d].
2: for i = d to 0 do
3:
4:

(z)  vi for ∀ i ∈ 0 ∪ [d].

for j = 1 to ni do

− ui+1

vi

) 

1

j for all nodes Gi

j in T .

/*hierarchical projection*/

end for

5:
6: end for

ui

Gi
j

Gi
j

(zGi

j

= PBi
← ui+1

j

Gi
j

+ vi

Gi
j

.

Gi
j

(11)

(12)

3

i.e.  O((cid:80)d
(cid:80)ni
j=1 |Gi

(cid:80)ni
j=1 |Gi

i=0

j|)  where |Gi

j| is the number of features contained in the node Gi

The time complexity of Algorithm 1 is similar to that of solving its proximal operator [16] 
j. As
j| ≤ p by Deﬁnition 1  the time complexity of Algorithm 1 is O(pd)  and thus O(p log p)
for a balanced tree  where d = O(log p). The next result shows that u0 returned by Algorithm 1 is
the projection of z onto A0
Theorem 5. For Algorithm 1  the following hold:
(i): ui
Gi
j

1. Indeed  we have more general results as follows.

= PAi

zGi

j

j

(cid:16)
(cid:16)

(cid:17)
(cid:17)

  ∀ i ∈ 0 ∪ [d]  j ∈ [ni].
  for any non-leaf node Gi
j.

(ii): ui+1
Gi
j

= PCi

j

zGi

j

4 MLFre Inspired by the KKT Conditions and Hierarchical Projection
In this section  we motivate MLFre via the KKT condition in Eq. (4) and the hierarchical projection
in Algorithm 1. Note that for any node Gi

j  we have

(cid:40){ζ ∈ HGi

j

j[β∗(λ)]Gi
wi

j

wi

j∂φi

j(β∗(λ)) =

: (cid:107)ζ(cid:107) ≤ wi
j} 
/(cid:107)[β∗(λ)]Gi

j

if [β∗(λ)]Gi
(cid:107)  otherwise.

j

= 0 

(13)

(cid:88)d

(cid:88)ni

Moreover  the KKT condition in Eq. (4) implies that

∃{ξi

j∂φi
j(cid:107) < wi

j(β∗(λ)) : ∀ i ∈ 0 ∪ [d]  j ∈ [ni]} such that XT θ∗(λ) =
j  we can see that [β∗(λ)]Gi

j ∈ wi
Thus  if (cid:107)ξi
= 0. However  we do not have direct access to ξi
j
even if θ∗(λ) is known  because XT θ∗(λ) is a mixture (sum) of all ξi
j as shown in Eq. (14). Indeed 
Algorithm 1 turns out to be much more useful than testing the feasibility of a given θ: it is able to
j(β∗(λ)) from XT θ∗(λ). This will serve as a cornerstone in developing MLFre.
split all ξi
Theorem 6 rigorously shows this property of Algorithm 1.
Theorem 6. Let vi  i ∈ 0 ∪ [d] be the output of Algorithm 1 with input XT θ∗(λ)  and {ξi
j : i ∈
0 ∪ [d]  j ∈ [ni]} be the set of vectors that satisfy Eq. (14). Then  the following hold.
(i) If [β∗(λ)]Gi

= 0  and [β∗(λ)]Gl

j ∈ wi

j∂φi

(14)

ξi
j.

j=1

i=0

j

j

(ii) If Gi

j is a non-leaf node  and [β∗(λ)]Gi

r

PAi

j  then

(cid:16)
(cid:16)

(cid:54)= 0 for all Gl
[XT θ∗(λ)]Gi

(cid:17)
r ⊃ Gi
(cid:17)
j(β∗(λ))  ∀ i ∈ 0 ∪ [d]  j ∈ [ni].

=(cid:80){(k t):Gt
=(cid:80){(k t):Gt

[XT θ∗(λ)]Gi

(cid:54)= 0  then

PCi

j

j

j

j

j

k⊆Gi

j} ξt
k.

k⊂Gi

k.
j} ξt

(iii) vi
Gi
j

∈ wi

j∂φi

By plugging Eq. (11) and part (ii) of Theorem 5 into (15)  we have [β∗(λ)]Gi

= 0 if

j

Combining Eq. (13) and part (iii) of Theorem 6  we can see that

(cid:107)vi

(cid:107) < wi

j ⇒ [β∗(λ)]Gi

= 0.

j

(cid:17)(cid:17)(cid:13)(cid:13)(cid:13) < wi

[XT θ∗(λ)]Gi

j

Gi
j

(cid:16)
(cid:17)(cid:13)(cid:13)(cid:13) < wi

− PCi

j

[XT θ∗(λ)]Gi
[XT θ∗(λ)]Gi
Moreover  the deﬁnition of PBi

(b):

j

j

(cid:16)
(cid:16)

j

(a):

(cid:13)(cid:13)(cid:13)PBi
(cid:13)(cid:13)(cid:13)PBi
(cid:13)(cid:13)(cid:13)[XT θ∗(λ)]Gi
(cid:13)(cid:13)(cid:13)[XT θ∗(λ)]Gi

j

j

j

(cid:16)
[XT θ∗(λ)]Gi
j ⇒ [β∗(λ)]Gi

j

− PCi

(cid:13)(cid:13)(cid:13) < wi

j

j 

(cid:17)(cid:13)(cid:13)(cid:13) < wi

implies that we can simplify (R1) and (R2) to the following form:

j ⇒ [β∗(λ)]Gi

j

= 0  if Gi

j is a non-leaf node  (R1’)

(15)

(R1)

(R2)

j  if Gi

j is a non-leaf node 

if Gi

j is a leaf node.

j

(R2’)
However  (R1’) and (R2’) are not applicable to detect inactive nodes as they involve θ∗(λ). Inspired
: θ ∈ Θ} and
by SAFE [8]  we ﬁrst estimate a set Θ containing θ∗(λ). Let [XT Θ]Gi
(16)

= {[XT θ]Gi

j is a leaf node.

if Gi

(cid:16)

(cid:17)

= 0 

Si

.

j

j

j

− PCi

j

zGi

j

j(z) = zGi

j

4

(cid:111)
< wi
j ⇒ [β∗(λ)]Gi

(cid:110)(cid:13)(cid:13)Si
j (ζ)(cid:13)(cid:13) : ζGi
(cid:13)(cid:13)(cid:13) : ζGi
(cid:110)(cid:13)(cid:13)(cid:13)ζGi

j

j

supζ

j

j

j

j

j

= 0 

if Gi

= 0  if Gi

j ⇒ [β∗(λ)]Gi

∈ Ξi
∈ [XT Θ]Gi

Then  we can relax (R1’) and (R2’) as
(cid:111)
j ⊇ [XT Θ]Gi
< wi

j is a non-leaf node  (R1∗)
(R2∗)
j is a leaf node.
supζ
In view of (R1∗) and (R2∗)  we sketch the procedure to develop MLFre in the following three steps.
Step 1 We estimate a set Θ that contains θ∗(λ).
Step 2 We solve for the supreme values in (R1∗) and (R2∗)  respectively.
Step 3 We develop MLFre by plugging the supreme values obtained in Step 2 to (R1∗) and (R2∗).
4.1 The Effective Interval of the Regularization Parameter λ
The geometric property of the dual problem in (2)  i.e.  θ∗(λ) = PF (y/λ)  implies that θ∗(λ) =
y/λ if y/λ ∈ F. Moreover  (R1) for the root node G0
1 leads to β∗(λ) = 0 if y/λ is an interior point
of F. Indeed  the following theorem presents stronger results.
Theorem 7. For TGL  let λmax = max{λ : y/λ ∈ F} and S0
(i): λmax = {λ : (cid:107)S0
(ii): y
For more discussions on λmax  please refer to Section H in the supplements.
5 The Proposed Multi-Layer Feature Reduction Method for TGL
We follow the three steps in Section 4 to develop MLFre. Speciﬁcally  we ﬁrst present an accurate
estimation of the dual optimum in Section 5.1  then we solve for the supreme values in (R1∗) and
(R2∗) in Section 5.2  and ﬁnally we present the proposed MLFre in Section 5.3.
5.1 Estimation of the Dual Optimum
We estimate the dual optimum by the geometric properties of projection operators [recall that
θ∗(λ) = PF (y/λ)]. We ﬁrst introduce a useful tool to characterize the projection operators.
Deﬁnition 8. [2] For a closed convex set C and a point z0 ∈ C  the normal cone to C at z0 is

1}.
1(XT y/λ)(cid:107) = w0
λ ∈ F ⇔ λ ≥ λmax ⇔ θ∗(λ) = y
λ ⇔ β∗(λ) = 0.

1(·) be deﬁned by Eq. (16). Then 

NC(z0) = {ζ : (cid:104)ζ  z − z0(cid:105) ≤ 0  ∀ z ∈ C}.

Theorem 7 implies that θ∗(λ) is known with λ ≥ λmax. Thus  we can estimate θ∗(λ) in terms of a
known θ∗(λ0). This leads to Theorem 9 that bounds the dual optimum by a small ball.
Theorem 9. For TGL  suppose that θ∗(λ0) is known with λ0 ≤ λmax. For λ ∈ (0  λ0)  we deﬁne

if λ0 < λmax 
if λ0 = λmax 

 

(cid:40) y

(cid:17)

(cid:16)

− θ∗(λ0) 
λ0
XT y
XS0
1
λ − θ∗(λ0) 

λmax

n(λ0) =

r(λ  λ0) = y
r⊥(λ  λ0) = r(λ  λ0) − (cid:104)r(λ λ0) n(λ0)(cid:105)

(cid:107)n(λ0)(cid:107)2

n(λ0).

Then  the following hold:
(i): n(λ0) ∈ NF (θ∗(λ0)).
(ii): (cid:107)θ∗(λ) − (θ∗(λ0) + 1
Theorem 9 indicates that θ∗(λ) lies inside the ball of radius 1

2 r⊥(λ  λ0))(cid:107) ≤ 1

2(cid:107)r⊥(λ  λ0)(cid:107).

o(λ  λ0) = θ∗(λ0) + 1

2 r⊥(λ  λ0).

2(cid:107)r⊥(λ  λ0)(cid:107) centered at

5.2 Solving the Nonconvex Optimization Problems in (R1∗) and (R2∗)
We solve for the supreme values in (R1∗) and (R2∗). For notational convenience  let

Θ = {θ : (cid:107)θ − o(λ  λ0)(cid:107) ≤ 1
j = {ζ : ζ ∈ HGi
Ξi

2(cid:107)r⊥(λ  λ0)(cid:107)} 

 (cid:107)ζ − [XT o(λ  λ0)]Gi
Theorem 9 implies that θ∗(λ) ∈ Θ  and thus [XT Θ]Gi
MLFre by (R1∗) and (R2∗)  we need to solve the following optimization problems:
j(ζ)(cid:107) : ζ ∈ Ξi
j(λ  λ0) = supζ{(cid:107)Si
j} 
j is a non-leaf node 
si
j(λ  λ0) = supζ{(cid:107)ζ(cid:107) : ζ ∈ Ξi
j} 
j is a leaf node.
si

(cid:107)2}.
2(cid:107)r⊥(λ  λ0)(cid:107)(cid:107)XGi
j for all non-leaf nodes Gi

(cid:107) ≤ 1
⊆ Ξi

if Gi
if Gi

j

j

j

j

(17)
(18)
j. To develop

(19)
(20)

5

Before we solve problems (19) and (20)  we ﬁrst introduce some notations.
Deﬁnition 10. For a non-leaf node Gi
j \ ∪k∈Ic(Gi
Gi
j(cid:48) ∈ {ni+1 + 1  ni+1 + 2  . . .   ni+1 + n(cid:48)
i + 1. We set the weights wi

j of an index tree T   let Ic(Gi
j}. If
for
j by Gi+1
i+1}  where n(cid:48)
i+1 is the number of virtual nodes of depth
j(cid:48) = 0 for all virtual nodes Gi
j(cid:48).

j) = {k : Gi+1
j \ ∪k∈Ic(Gi
j(cid:48) = Gi

(cid:54)= ∅  we deﬁne a virtual child node of Gi

k ⊂ Gi
j )Gi+1

j )Gi+1

k

k

Another useful concept is the so-called unique path between the nodes in the tree.
Lemma 11. [16] For any non-root node Gi
j to the root G0
  where l ∈ 0 ∪ [i]  r0 = 1  and ri = j. Then  the following hold:
the nodes on this path be Gl
rl
j ⊂ Gl
Gi
j ∩ Gl
Gi

  ∀ l ∈ 0 ∪ [i − 1].
r = ∅  ∀ r (cid:54)= rl  l ∈ [i − 1]  r ∈ [ni].

j  we can ﬁnd a unique path from Gi

rl

Solving Problem (19) We consider the following equivalent problem of (19).

j(λ  λ0))2 = supζ{ 1

2(cid:107)Si

j(ζ)(cid:107)2 : ζ ∈ Ξi
j} 

1

2 (si

if Gi

j is a non-leaf node.

Although both the objective function and feasible set of problem (23) are convex  it is nonconvex as
we need to ﬁnd the supreme value. We derive the closed form solutions of (19) and (23) as follows.
(cid:107)2  and vi  i ∈ 0 ∪ [d] be the
Theorem 12. Let c = [XT o(λ  λ0)]Gi
output of Algorithm 1 with input XT o(λ  λ0).
(i): Suppose that c /∈ Ci
j(λ  λ0) = (cid:107)vi
(ii): Suppose that node Gi
(iii): Suppose that node Gi

j has a virtual child node. Then  for any c ∈ Ci
j has no virtual child node. Then  the following hold.

2(cid:107)r⊥(λ  λ0)(cid:107)(cid:107)XGi

j(λ  λ0) = γ.

j. Then  si

(cid:107) + γ.

  γ = 1

j  si

Gi
j

j

j

1. Let

(21)
(22)

(23)

(iii.a): If c ∈ rbd Ci
(iii.b): If c ∈ ri Ci

j  then si

j(λ  λ0) = γ.
j  then  for any node Gt

the nodes on the path from Gt

k to Gi

k ⊂ Gi
j  where t ∈ {i + 1  . . .   d} and k ∈ [nt + n(cid:48)
(cid:88)t
j be Gl
rl

t]  let
  where l = i  . . .   t  ri = j  and rt = k  and

(cid:16)

(cid:107)(cid:17)

.

− (cid:107)vl

wl
rl

Gl
rl

l=i+1

(24)

(cid:16)

Then  si

j(λ  λ0) =

  Gt

Γ(Gi+1
ri+1

k) =
γ − min{(k t):Gt

k⊂Gi

j} Γ(Gi+1
ri+1

  Gt
k)

(cid:17)

.

+

Solving Problem (20) We can solve problem (20) by the Cauchy-Schwarz inequality.
Theorem 13. For problem (20)  we have si

j(λ  λ0) = (cid:107)[XT o(λ  λ0)]Gi

(cid:107) + 1

2(cid:107)r⊥(λ  λ0)(cid:107)(cid:107)XGi

j

(cid:107)2.

j

5.3 The Multi-Layer Screening Rule
In real-world applications  the optimal parameter values are usually unknown. Commonly used
approaches to determine an appropriate parameter value  such as cross validation and stability se-
lection  solve TGL many times along a grid of parameter values. This process can be very time
consuming. Motivated by this challenge  we present MLFre in the following theorem by plugging
the supreme values found by Theorems 12 and 13 into (R1∗) and (R2∗)  respectively.
Theorem 14. For the TGL problem  suppose that we are given a sequence of parameter values
λmax = λ0 > λ1 > ··· > λK. For each integer k = 0  . . .  K − 1  we compute θ∗(λk) from a given
β∗(λk) via Eq. (3). Then  for i = 1  . . .   d  MLFre takes the form of

si
j(λk+1  λk) < wi

j ⇒ [β∗(λ)]Gi

j

= 0  ∀ j ∈ [ni].

(MLFre)

Remark 2. We apply MLFre to identify inactive nodes hierarchically in a top-down fashion. Note
that  we do not need to apply MLFre to node Gi
Remark 3. To simplify notations  we consider TGL with a single tree  in the proof. However  all
major results are directly applicable to TGL with multiple trees  as they are independent from each
other. We note that  many sparse models  such as Lasso  group Lasso  and sparse group Lasso  are
special cases of TGL with multiple trees.

j if one of its ancestor nodes passes the rule.

6

(a) synthetic 1  p = 20000

(b) synthetic 1  p = 50000

(c) synthetic 1  p = 100000

(d) synthetic 2  p = 20000

(e) synthetic 2  p = 50000

(f) synthetic 2  p = 100000

Figure 1: Rejection ratios of MLFre on two synthetic data sets with different feature dimensions.

  where |Gi

p0

(cid:80)
k∈Gi |Gi
k|

6 Experiments
We evaluate MLFre on both synthetic and real data sets by two measurements. The ﬁrst measure is
the rejection ratios of MLFre for each level of the tree. Let p0 be the number of zero coefﬁcients in
the solution vector and Gi be the index set of the inactive nodes with depth i identiﬁed by MLFre.
k| is the
The rejection ratio of the ith layer of MLFre is deﬁned by ri =
k. The second measure is speedup  namely  the ratio of the
number of features contained in node Gi
running time of the solver without screening to the running time of solver with MLFre.
For each data set  we run the solver combined with MLFre along a sequence of 100 parameter values
equally spaced on the logarithmic scale of λ/λmax from 1.0 to 0.05. The solver for TGL is from the
SLEP package [15]. It also provides an efﬁcient routine to compute λmax.
6.1 Simulation Studies
We perform experiments on two synthetic data
sets  named synthetic 1 and synthetic 2  which
are commonly used in the literature [21  31].
The true model is y = Xβ∗ + 0.01   ∼
N (0  1). For each of the data set  we ﬁx N =
250 and select p = 20000  50000  100000. We
create a tree with height 4  i.e.  d = 3. The
solver MLFre MLFre+solver speedup
average sizes of the nodes with depth 1  2 and
16.04
3 are 50  10  and 1  respectively. Thus  if
483.96
20000
29.78
50000 1175.91
p = 100000  we have roughly n1 = 2000 
40.60
100000 2391.43
n2 = 10000  and n3 = 100000. For synthet-
12.43
470.54
ic 1  the entries of the data matrix X are i.i.d.
20000
25.53
50000 1122.30
standard Gaussian with zero pair-wise correla-
36.81
100000 2244.06
tion  i.e.  corr (xi  xj) = 0 for the ith and jth
columns of X with i (cid:54)= j. For synthetic 2 
42.50
39.29
the entries of X are drawn from standard Gaus-
36.88
sian with pair-wise correlation corr (xi  xj) =
0.5|i−j|. To construct β∗  we ﬁrst randomly select 50% of the nodes with depth 1  and then ran-
domly select 20% of the children nodes (with depth 2) of the remaining nodes with depth 1. The
components of β∗ corresponding to the remaining nodes are populated from a standard Gaussian 
and the remaining ones are set to zero.

Table 1: Running time (in seconds) for solving
TGL along a sequence of 100 tuning parame-
ter values of λ equally spaced on the logarithmic
scale of λ/λmax from 1.0 to 0.05 by (a): the solver
[15] without screening (see the third column); (b):
the solver with MLFre (see the ﬁfth column).

30.17
39.49
58.91
37.87
43.97
60.96
492.08
556.19
564.36

synthetic 1

1.03
2.95
6.57
1.19
3.13
6.18
ADNI+GMV 406262 20911.92 81.14
ADNI+WMV 406262 21855.03 80.83
ADNI+WBV 406262 20812.06 82.10

synthetic 2

Dataset

p

7

(a) ADNI+GMV

(b) ADNI+WMV

(c) ADNI+WBV

respectively. We can see that MLFre identiﬁes almost all inactive nodes  i.e. (cid:80)3

almost all of the inactive nodes  i.e.  (cid:80)3
increases  MLFre identiﬁes more inactive nodes  i.e. (cid:80)3

Figure 2: Rejection ratios of MLFre on ADNI data set with grey matter volume (GMV)  white mater
volume (WMV)  and whole brain volume (WBV) as response vectors  respectively.
Fig. 1 shows the rejection ratios of all three layers of MLFre. We can see that MLFre identiﬁes
i=1 ri ≥ 90%  and the ﬁrst layer contributes the most.
Moreover  Fig. 1 also indicates that  as the feature dimension (and the number of nodes in each level)
i=1 ri ≈ 100%. Thus  we can expect a more
signiﬁcant capability of MLFre in identifying inactive nodes on data sets with higher dimensions.
Table 1 shows the running time of the solver with and without MLFre. We can observe signiﬁcant
speedups gained by MLFre  which are up to 40 times. Take synthetic 1 with p = 100000 for
example. The solver without MLFre takes about 40 minutes to solve TGL at 100 parameter values.
Combined with MLFre  the solver only needs less than one minute for the same task. Table 1 also
shows that the computational cost of MLFre is very low—that is negligible compared to that of the
solver without MLFre. Moreover  as MLFre identiﬁes more inactive nodes with increasing feature
dimensions  Table 1 shows that the speedup gained by MLFre becomes more signiﬁcant as well.
6.2 Experiments on ADNI data set
We perform experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) data set
(http://adni.loni.usc.edu/). The data set consists of 747 patients with 406262 single
nucleotide polymorphisms (SNPs). We create the index tree such that n1 = 4567  n2 = 89332 
and n3 = 406262. Fig. 2 presents the rejection ratios of MLFre on the ADNI data set with grey
matter volume (GMV)  white matter volume (WMV)  and whole brain volume (WBV) as response 
i=1 ri ≈ 100%. As
a result  we observe signiﬁcant speedups gained by MLFre—that are about 40 times—from Table
1. Speciﬁcally  with GMV as response  the solver without MLFre takes about six hours to solve
TGL at 100 parameter values. However  combined with MLFre  the solver only needs about eight
minutes for the same task. Moreover  Table 1 also indicates that the computational cost of MLFre is
very low—that is negligible compared to that of the solver without MLFre.
7 Conclusion
In this paper  we propose a novel multi-layer feature reduction (MLFre) method for TGL. Our major
technical contributions lie in two folds. The ﬁrst is the novel hierarchical projection algorithm that
is able to exactly and efﬁciently recover the subgradients of the tree-guided regularizer with respect
to each node from their mixture. The second is that we show a highly nontrivial nonconvex problem
admits a closed form solution. To the best of our knowledge  MLFre is the ﬁrst screening method
that is applicable to TGL. An appealing feature of MLFre is that it is exact in the sense that the
identiﬁed inactive nodes are guaranteed to be absent from the sparse representations. Experiments
on both synthetic and real data sets demonstrate that MLFre is very effective in identifying inactive
nodes  leading to substantial savings in computational cost and memory usage without sacriﬁcing
accuracy. Moreover  the capability of MLFre in identifying inactive nodes on higher dimensional
data sets is more signiﬁcant. We plan to generalize MLFre to more general and complicated sparse
models  e.g.  over-lapping group Lasso with logistic loss. In addition  we plan to apply MLFre to
other applications  e.g.  brain image analysis [10  18] and natural language processing [27  28].
Acknowledgments
This work is supported in part by research grants from NIH (R01 LM010730  U54 EB020403) and
NSF (IIS- 0953662  III-1539991  III-1539722).

8

References
[1] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization with sparsity-inducing penalties. Foun-

dations and Trends in Machine Learning  4(1):1–106  Jan. 2012.

[2] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer  2011.

[3] M. Bazaraa  H. Sherali  and C. Shetty. Nonlinear Programming: Theory and Algorithms. Wiley-

Interscience  2006.

[4] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization  Second Edition. Canadian

Mathematical Society  2006.

[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[6] X. Chen  Q. Lin  S. Kim  J. Carbonell  and E. Xing. Smoothing proximal gradient method for general

structured sparse regression. Annals of Applied Statistics  pages 719–752  2012.

[7] W. Deng  W. Yin  and Y. Zhang. Group sparse optimization by alternating direction method. Technical

report  Rice CAAM Report TR11-06  2011.

[8] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. Paciﬁc

Journal of Optimization  8:667–698  2012.

[9] J.-B. Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and sufﬁcient con-

ditions for global optimality. In Nonsmooth optimization and related topics. Springer  1988.

[10] R. Jenatton  A. Gramfort  V. Michel  G. Obozinski  E. Eger  F. Bach  and B. Thirion. Multiscale mining of
fmri data with hierarchical structured sparsity. SIAM Journal on Imaging Science  pages 835–856  2012.
[11] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding.

Journal of Machine Learning Research  12:2297–2334  2011.

[12] K. Jia  T. Chan  and Y. Ma. Robust and practical face recognition via structured sparsity. In European

Conference on Computer Vision  2012.

[13] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity.

International Conference on Machine Learning  2010.

In

[14] S. Kim and E. Xing. Tree-guided group lasso for multi-response regression with structured sparsity  with

an application to eqtl mapping. The Annals of Applied Statistics  2012.

[15] J. Liu  S. Ji  and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University  2009.
[16] J. Liu and J. Ye. Moreau-Yosida regularization for grouped tree structure learning. In Advances in neural

information processing systems  2010.

[17] J. Liu  Z. Zhao  J. Wang  and J. Ye. Safe screening with variational inequalities and its application to

lasso. In International Conference on Machine Learning  2014.

[18] M. Liu  D. Zhang  P. Yap  and D. Shen. Tree-guided sparse coding for brain disease classiﬁcation. In

Medical Image Computing and Computer-Assisted Intervention  2012.

[19] K. Ogawa  Y. Suzuki  and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computa-

tion. In ICML  2013.

[20] A. Ruszczy´nski. Nonlinear Optimization. Princeton University Press  2006.
[21] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B  74:245–
266  2012.

[22] J. Wang  W. Fan  and J. Ye. Fused lasso screening rules via the monotonicity of subdifferentials. IEEE

Transactions on Pattern Analysis and Machine Intelligence  PP(99):1–1  2015.

[23] J. Wang  P. Wonka  and J. Ye. Scaling svm and least absolute deviations via exact data reduction. In

International Conference on Machine Learning  2014.

[24] J. Wang  P. Wonka  and J. Ye. Lasso screening rules via dual polytope projection. Journal of Machine

Learning Research  16:1063–1101  2015.

[25] J. Wang and J. Ye. Two-Layer feature reduction for sparse-group lasso via decomposition of convex sets.

Advances in neural information processing systems  2014.

[26] Z. J. Xiang  H. Xu  and P. J. Ramadge. Learning sparse representation of high dimensional data on large

scale dictionaries. In NIPS  2011.

[27] D. Yogatama  M. Faruqui  C. Dyer  and N. Smith. Learning word representations with hierarchical sparse

coding. In International Conference on Machine Learning  2015.

[28] D. Yogatama and N. Smith. Linguistic structured sparsity in text categorization. In Proceedings of the

Annual Meeting of the Association for Computational Linguistics  2014.

[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society Series B  68:49–67  2006.

[30] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  2009.

[31] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society Series B  67:301–320  2005.

9

,Alexander Zimin
Gergely Neu
Meisam Razaviyayn
Mingyi Hong
Zhi-Quan Luo
Jie Wang
Jieping Ye
Antti Tarvainen
Harri Valpola
Pei Wang
Nuno Nvasconcelos