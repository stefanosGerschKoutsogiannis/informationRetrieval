2019,An adaptive Mirror-Prox method for variational inequalities with singular operators,Lipschitz continuity is a central requirement for achieving the optimal O(1/T) rate of convergence in monotone  deterministic variational inequalities (a setting that includes convex minimization  convex-concave optimization  nonatomic games  and many other problems). However  in many cases of practical interest  the operator defining the variational inequality may become singular at the boundary of the feasible region  precluding in this way the use of fast gradient methods that attain this rate (such as Nemirovski's mirror-prox algorithm and its variants). To address this issue  we propose a novel smoothness condition which we call Bregman smoothness  and which relates the variation of the operator to that of a suitably chosen Bregman function. Leveraging this condition  we derive an adaptive mirror prox algorithm which attains an O(1/T) rate of convergence in problems with possibly singular operators  without any prior knowledge of the problem's Bregman constant (the Bregman analogue of the Lipschitz constant). We also present an extension of our algorithm to stochastic variational inequalities where the algorithm achieves a $O(1/\sqrt{T})$ convergence rate.,An Adaptive Mirror-Prox Algorithm for

Variational Inequalities with Singular Operators

Kimon Antonakopoulos

Univ. Grenoble Alpes  CNRS  Inria  Grenoble INP

LIG 38000 Grenoble  France.

E. Veronica Belmega

ETIS/ENSEA

Univ. de Cergy-Pontoise-CNRS  France

kimon.antonakopoulos@inria.fr

belmega@ensea.fr

Panayotis Mertikopoulos

Univ. Grenoble Alpes  CNRS  Inria  Grenoble INP 

LIG 38000 Grenoble  France.

panayotis.mertikopoulos@imag.fr

Abstract

Lipschitz continuity is a central requirement for achieving the optimal O(1/T ) rate
of convergence in monotone  deterministic variational inequalities (a setting that
includes convex minimization  convex-concave optimization  nonatomic games 
and many other problems). However  in many cases of practical interest  the
operator deﬁning the variational inequality may exhibit singularities at the boundary
of the feasible region  precluding in this way the use of fast gradient methods
that attain this optimal rate (such as Nemirovski’s mirror-prox algorithm and its
variants). To address this issue  we consider a regularity condition which relates the
variation of the operator to that of a suitably chosen Bregman function. Leveraging
this Bregman continuity condition  we derive an adaptive mirror-prox algorithm
which attains the optimal O(1/T ) rate of convergence in problems with possibly
singular operators  without any prior knowledge of the degree of smoothness (the
√
Bregman analogue of the Lipschitz constant). We also show that  under Bregman
continuity  the mirror-prox algorithm achieves a O(1/
T ) convergence rate in
stochastic variational inequalities.

1

Introduction

The seminal introduction of generative adversarial networks (GANs) [18] has ushered in a new
optimization paradigm in deep learning: instead of focusing on the minimization of an empirical loss
function  GAN training hinges on a zero-sum game between a generator and a discriminator. In fact 
in many cases GAN training goes even beyond the min-max setting  either because there are more
than two networks involved  or because the objectives of the generator and the discriminator are not
entirely opposed – e.g.  as in the widely used ACGAN framework of Odena et al. [43]. In these cases 
the most compact way of representing the problem’s training landscape is by means of a variational
inequality (VI).
Tracing their origins to the work of Stampacchia [49] on the Signorini problem  variational inequalities
have since found a broad range of applications in physics  engineering  economics – and  more
recently  machine learning. One of the main reasons for their extensive applicability is that they
comprise a ﬂexible optimization framework which can simultaneously account for loss function
minimization  saddle-point  game-theoretic  and ﬁxed point problems. As a result  there has been
considerable interest in the literature to develop optimal algorithms for solving VI problems; for an
appetizing introduction  see [16] and references therein.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

One of the most widely studied methods for this purpose is ordinary gradient descent – also known as
the forward-backward (FB) algorithm in operator theory [6].1 In monotone  deterministic variational
inequalities  the convergence of the method is guaranteed under a condition known as cocoercivity.
By the Baillon–Haddad theorem  if the operator deﬁning the variational inequality is a gradient
ﬁeld (i.e.  in loss minimization problems)  this condition is equivalent to Lipschitz smoothness of
the associated loss function [4  6]. However  cocoercivity may fail to hold even in simple  bilinear
min-max problems  in which case gradient descent provably fails to converge – see e.g.  [17  35  36]
for a precise statement.
The ﬁrst algorithm achieving convergence in (pseudo-)monotone variational inequalities without
cocoercivity is the extra-gradient (EG) algorithm of Korpelevich [24]  which only requires Lipschitz
continuity of the underlying operator.2 The asymptotic convergence result of Korpelevich [24]
was subsequently extended by Nemirovski [38] who introduced the mirror-prox (MP) algorithm  a
Bregman variant of the EG algorithm with ergodic averaging. As was shown in [38]  the mirror-prox
algorithm attains a O(1/T ) ergodic convergence rate in monotone variational inequalities with
Lipschitz continuous operators  and this rate cannot be improved without further assumptions.
However  in many applications and problems of practical interest  Lipschitz continuity may also fail
to hold  either because the loss proﬁle of the problem grows too rapidly (e.g.  as in support vector
machines or GAN models with Kullback-Leibler losses)  or because the problem exhibits singularities
near the boundary of the feasible region (e.g.  as in resource allocation and inverse problems). In
these cases  one would still want to apply a fast method like mirror-prox  but the lack of smoothness
means that there are no convergence guarantees – asymptotically  ergodically  or otherwise.

Our contributions. Our starting point is the observation that this failure stems from the fact that
Lipschitz continuity of the operator is deﬁned relative to a global norm. Because of this  the standard
Lipschitz framework is not well-suited to problems with singularities or rapid growth: a global norm
is oblivious to the geometry of the feasible region (and  in particular  its boundary)  so it cannot
capture the ﬁner features of the problem’s loss landscape.
To overcome this limitation  we introduce a novel regularity condition  which we call Bregman conti-
nuity  and which is made-to-order for the singularity landscape of the problem at hand. Speciﬁcally 
instead of deﬁning Lipschitz continuity relative to a global norm  we deﬁne it in terms of a family of
local norms and a suitably chosen Bregman function. This leads to an intricate interplay between
different geometric notions of distance (the Bregman divergence and the local norm)  but it also
introduces the ﬂexibility required to tackle variational inequalities with singular operators.
Under this assumption  we show that the mirror-prox algorithm attains the optimal O(1/T ) conver-
gence rate in variational inequalities with (possibly) singular operators  provided that the method is
run with the same Bregman function that is used to deﬁne Bregman continuity. As in the standard Lip-
schitz framework  the method’s convergence requires a step-size of the form γ < 1/β  where β is the
Bregman constant of the operator (i.e.  the Bregman analogue of the Lipschitz constant). Estimating
this constant can be fairly challenging in practice (if not downright impossible)  so we also introduce
an adaptive mirror-prox (AMP) method which attains the same O(1/T ) rate without requiring any a
priori estimation of β – essentially  the Bregman constant is learned at the same time as the problem’s
√
landscape. Finally  we provide a variant of the method for stochastic variational inequalities  and we
establish a O(1/
T ) convergence rate in this setting. To the best of our knowledge  these are the
ﬁrst results of this kind in the literature.

Related work. Owing to their optimal rate guarantees  the extra-gradient and mirror-prox algo-
rithms have been at the forefront of an extensive literature which is impossible to adequately review
here. As a purely indicative list of contributions in the Lipschitz continuous setting (and with no
illusion of being comprehensive)  we refer the reader to Juditsky et al. [21]  Chambolle and Pock [13] 
Malitsky [28]  Iusem et al. [20] and Mokhtari et al. [37] for some recent developments. Especially in

1When used to ﬁnd a zero of a composite operator  the FB algorithm is known as a “splitting” method; see
e.g.  Bruck Jr. [12]  Passty [44]  [14]  and references therein.
2An operator A(x) is cocoercive if (cid:104)A(x(cid:48)) − A(x)  x(cid:48) − x(cid:105) ≥ (1/β)(cid:107)A(x(cid:48)) − A(x)(cid:107)2 for some β > 0 and
all x  x(cid:48). Note that Lipschitz continuity is strictly weaker than cocoercivity: the operator A(x1  x2) = (−x2  x1)
is Lipschitz continuous over R2  but it is not cocoercive; see Section 2 for a detailed discussion.

2

learning theory  there has been a surge of interest motivated by the application of EG/MP methods to
GAN training  see e.g.  [15  17  36  51] and references therein.3
Going beyond the Lipschitz regime  Bauschke et al. [7] recently introduced a “Lipschitz-like” smooth-
ness condition for convex minimization problems and used it to establish a O(1/T ) value convergence
rate for mirror descent methods (as opposed to mirror-prox). Always in the context of loss min-
imization problems  Bolte et al. [9] subsequently extended the results of Bauschke et al. [7] to
unconstrained non-convex problems that satisfy the Kurdyka–Łojasiewicz (KL) inequality  while Lu
et al. [27] considered functions that are also relatively strongly convex and showed that mirror descent
achieves a geometric convergence rate in this context. Finally  in a very recent preprint  Hanzely et al.
[19] examined the rate of convergence of an accelerated variant of mirror descent under the same
Lipschitz-like smoothness assumption.
The condition of Bauschke et al. [7] is remarkably simple as it only posits that the problem’s
loss function f is such that βh − f is convex for some reference Bregman function h and some
β > 0. A straightforward extension of this condition to an operator setting would be to require
the monotonicity of β∇h − A  where A is the operator deﬁning the variational inequality under
study. However  the cornerstone of this “Lipschitz-like” condition is a descent lemma which does not
carry over to variational inequalities  so it does not seem possible to extend the analysis of Bauschke
et al. [7] to an operator setting. Lu [26] also considered a “relative continuity” condition for loss

minimization problems positing that (cid:107)∇f (x)(cid:107) ≤ M inf x(cid:48)(cid:112)2D(x(cid:48)  x)/(cid:107)x(cid:48) − x(cid:107) (where f is the

problem’s objective and D is the Bregman divergence of h). Written this way  the condition of Lu
[26] can also be extended to an operator setting  but this would provide a surrogate for operator
boundedness  not Lipschitz continuity (since A = ∇f in minimization problems). Since the optimal
O(1/T ) convergence rate of the mirror-prox algorithm is tied to the regularity of A – as opposed
to its boundedness – the condition of Lu [26] does not seem applicable to the setting under study.
Accordingly  there is no overlap in results or methodology with this particular strand of the literature.
Finally  in a very recent paper  Bach and Levy [3] introduced a universal variant of the mirror-prox
algorithm which is model-agnostic and achieves an optimal convergence rate in stochastic and/or
smooth settings. Achieving optimal rates in the setting of Bach and Levy [3] relies crucially on the
operator being Lipschitz continuous (albeit with a possibly unknown constant) and the feasible region
having a ﬁnite Bregman diameter. The algorithm we propose in this work is not universal but it is
adaptive  and it does not require either Lipschitz continuity or a ﬁnite Bregman diameter. In this
manner  our work also provides an important ﬁrst step towards extending the universal analysis of
Bach and Levy [3] to VI problems with singularities.

2 Preliminaries
Let X be a convex – but not necessarily closed or compact – subset of a d-dimensional normed space
V  and let V∗ denote the dual space of V. The variational inequality (VI) problem associated to a
continuous operator A : X → V∗ consists of ﬁnding x∗ ∈ X such that

(cid:104)A(x∗)  x − x∗(cid:105) ≥ 0 for all x ∈ X .

(VI)
Following [16]  we will refer to this problem as VI(X   A) and we will write X ∗ ≡ Sol(X   A) for
its set of solutions. Note also that  if X is not closed  A may exhibit a singularity at a residual point
x ∈ bd(X ) \ X in the sense that A does not admit a continuous extension to x.
In the literature  this formulation of the problem is often referred to as a Stampacchia variational
inequality (SVI) [16] or a “strong” variational inequality [21  40]. For illustration purposes  we
present some archetypal examples of such problems below:
Example 2.1 (Loss minimization). If A = ∇f for some convex loss function f on X = Rd  solutions
of (VI) coincide with the global minimizers of f.
Example 2.2 (Min-max optimization). Suppose that A = (∇x1f −∇x2f ) for some real-valued
function f (x1  x2) with x1 ∈ X1  x2 ∈ X2. If f is convex-concave (i.e.  convex in x1 and concave in
3We note here that the method is sometimes referred to as “optimistic mirror descent”. This terminology is
due to Rakhlin and Sridharan [45  46] and may refer either to the mirror-prox method itself  or to a variant with
“gradient extrapolation from the past”  as in [17].

3

and

f (x∗

1  x∗

1  x∗
1  x∗

2) of (VI) is a global saddle-point of f  i.e. 
2) ≥ f (x∗
2) ≤ f (x1  x∗
2)

x2)  any solution x∗ = (x∗
f (x∗
(2.1)
for all x1 ∈ X1  x2 ∈ X2. Problems of this type have attracted considerable interest in the ﬁelds of
machine learning and artiﬁcial intelligence because they constitute the basic optimization framework
for GANs [18]. For a series of recent papers focusing on the interplay between GAN and saddle-point
problems / variational inequalities  see [15  17  25  36  51] and references therein.
Example 2.3 (Resource sharing problems). Consider a set of resources r ∈ R = {1  . . .   R} serving
a stream of demands that arrive at a rate of ρ per unit of time (for instance  a GPU cluster or a
computing grid processing a stream of jobs). If the load on the r-th resource is xr  the expected
service time in the standard Kleinrock model [23] is given by the M/M/1 loss function

1  x2)

(cid:96)r(xr) =

1

cr − xr

 

(2.2)

where cr denotes the capacity of the resource. In this setting  the set of feasible resource allocations
is X ≡ {(x1  . . .   xR) : 0 ≤ xr < cr  x1 + ··· + xR = ρ} 4 and we say that a resource allocation
proﬁle x∗ ∈ X ∗ is at Nash/Wardrop equilibrium [42  50] if

(2.3)
i.e.  when no job would be better served by transferring it to a different priority queue. In this
case  if we let A(x) = ((cid:96)1(x1)  . . .   (cid:96)R(xR))  a standard calculation shows that x∗ is an equilibrium
allocation if and only if it solves the associated variational inequality problem for A.

for all x ∈ X and all r ∈ R such that x∗

r) ≤ (cid:96)r(xr)

(cid:96)r(x∗

r > 0

for all x  x(cid:48) ∈ X .

(cid:104)A(x(cid:48)) − A(x)  x(cid:48) − x(cid:105) ≥ 0

The most widely used assumption in the literature for solving VI problems is monotonicity  i.e. 
(2.4)
When A = ∇f  this condition is equivalent to f being convex; likewise  when A = (∇x1 f −∇x2f )
as in Example 2.2  monotonicity is equivalent to f being convex-concave [6]; ﬁnally  by direct
calculation  it is straightforward to see that the operator deﬁned in Example 2.3 is monotone. For an
introduction to the theory of monotone operators  we refer the reader to Facchinei and Pang [16] and
Bauschke and Combettes [6].
Now  drawing on Nesterov [40  41] and Juditsky et al. [21]  if A is monotone  the quality of a
candidate solution ˆx ∈ X can be assessed via the restricted gap (or merit) function

GapC(ˆx) = sup
x∈C

(2.5a)
where C is a nonempty convex subset of X . The rationale behind this deﬁnition is that  if x∗ solves
(VI)  monotonicity gives (cid:104)A(x)  x∗ − x(cid:105) ≤ (cid:104)A(x∗)  x∗ − x(cid:105) ≤ 0  so the quantity being maximized
in (2.5) is small if ˆx is an approximate solution of (VI). Formally  we have:
Lemma 1. Suppose that A is monotone. If x∗ solves (VI)  we have GapC(x∗) = 0 whenever x∗ ∈ C.
Conversely  if GapC(ˆx) = 0 and C contains a neighborhood of ˆx in X   ˆx is a solution of (VI).
This lemma extends a similar result by Nesterov [40]  so we defer its proof to the paper’s supplement.
In view of all this  we will employ the gap function GapC(ˆx) as our main ﬁgure of merit and we will
use it to state our convergence rate guarantees in the sequel.

(cid:104)A(x)  ˆx − x(cid:105) 

3 Bregman continuity

(cid:107)A(x(cid:48)) − A(x)(cid:107)∗ ≤ β(cid:107)x(cid:48) − x(cid:107)

In addition to monotonicity  a standard assumption for solving variational inequalities is that of
Lipschitz continuity  i.e. 
(Lip)
for some β > 0 and for all x  x(cid:48) ∈ X . This deﬁnition involves two distinct (but related) measures of
distance: (i) the primal norm on V which measures distances between the primal points x  x(cid:48) ∈ X ; and
(ii) the dual norm on V∗ which measures the distance between the dual vectors A(x)  A(x(cid:48)) ∈ V∗.5
Importantly  both of these notions are global  i.e.  they do not depend on the point in space at which
they are calculated; as such  Lipschitz continuity is oblivious to the geometry of X (and  in particular 
its boundary). In the sequel  we describe a way to overcome this limitation by introducing two distinct
notions of distance that are tailored to the geometry of X and the singularity landscape of A.

4For posterity  note here that X is convex but it is not necessarily closed.
5Recall here that the dual norm of v ∈ V∗ is deﬁned as (cid:107)v(cid:107)∗ = maxz∈V{|(cid:104)v  z(cid:105)| : (cid:107)z(cid:107) ≤ 1}.

4

Local norms. The ﬁrst measure of distance that we deﬁne is that of local norm on X :
Deﬁnition 1. Let Z = span(X − X ) denote the tangent hull of X   i.e.  the subspace of V spanned
by all possible displacement vectors of the form z = x(cid:48) − x  x  x(cid:48) ∈ X . A local norm on X is a
continuous assignment of a norm (cid:107)·(cid:107)x on Z at each x ∈ X .6 The induced dual local norm is then
deﬁned as

(cid:107)v(cid:107)x ∗ = maxz∈Z{|(cid:104)v  z(cid:105)| : (cid:107)z(cid:107)x ≤ 1}

(3.1)
For ease of presentation  we tacitly assume in what follows that (cid:107)z(cid:107)x ≥ µ(cid:107)z(cid:107) for some µ > 0 and
all x ∈ X   z ∈ Z. This can always be achieved by taking (cid:107)·(cid:107)x ← (cid:107)·(cid:107)x + µ(cid:107)·(cid:107) so there is no loss of
generality. Note in particular that this implies that (cid:107)v(cid:107)x ∗ ≤ (1/µ)(cid:107)v(cid:107) for all x ∈ X and all v ∈ Z∗.
For intuition  we present some key examples below:
Example 3.1 (Euclidean geometry). Let X = Rd so Z = Rd. The Euclidean norm on X is given by
the standard expression (cid:107)z(cid:107)2
Example 3.2 (Shahshahani p-norm). Let X = Rd
on X is deﬁned for all p > 1 as

++ so  again  Z = Rd. The Shahshahani p-norm
(cid:1)1/p

(cid:107)z(cid:107)x =(cid:0)|z1|p/x1 + ··· + |zd|p/xd

j   and the associated dual norm is the same.

for all x ∈ X   z ∈ Z.

2 =(cid:80)d

for all v ∈ V∗.

j=1 z2

(3.2)

(cid:107)v(cid:107)x ∗ =(cid:0)xq−1

By a straightforward application of Hölder’s inequality  the corresponding dual norm is given by

|v1|q + ··· + xq−1

(3.3)
with the usual convention p−1 + q−1 = 1. In particular  for p → 1+  we get the limiting expression
(3.4)
This metric plays a major role in  among others  game theory  optimal transport  machine learning 
information theory  and many other ﬁelds – see e.g.  [1  2  22  31  34  47  48] and references therein.

(cid:107)v(cid:107)x ∗ = max{x1|v1|  . . .   xd|vd|}.

d

1

|vd|q(cid:1)1/q

Local Bregman functions and the associated divergence. The notion of a dual local norm pre-
sented above will be our principal measure of distance in V∗. To proceed  we will also need to adapt
the notion of a Bregman (or distance-generating) function on X :
Deﬁnition 2. Let (cid:107)·(cid:107)x be a local norm on X . We say that h : V → R is a Bregman function on X if:

1. h is proper  l.s.c.  convex  and dom h = X .
2. The subdifferential of h admits a continuous selection  i.e.  a continuous function ∇h such

that ∇h(x) ∈ ∂h(x) for all x ∈ X ◦ ≡ dom ∂h.

3. h is strongly convex relative to the underlying local norm  i.e. 

h(p) ≥ h(x) + (cid:104)∇h(x)  p − x(cid:105) + 1

2 K(cid:107)p − x(cid:107)2

x

for some K > 0 and all p ∈ X   x ∈ X ◦.

The Bregman divergence induced by h is then deﬁned for all p ∈ X   x ∈ X ◦  as

D(p  x) = h(p) − h(x) − (cid:104)∇h(x)  p − x(cid:105).

As an immediate consequence of the above  we have:
Lemma 2. A Bregman function h is K-strongly convex relative to (cid:107)·(cid:107)x if and only if

D(p  x) ≥ 1

2 K(cid:107)p − x(cid:107)2

x

for all p ∈ X and all x ∈ X ◦.

(3.5)

(3.6)

(3.7)

The main difference between Deﬁnition 2 and the standard assumptions in the literature [7  10  11 
21  30  32  33  39–41] is the strong convexity requirement relative to the local norm (cid:107)·(cid:107)x (whose
choice  in turn  is aimed to capture the singularity landscape of the operator). We illustrate this with
two examples below:

6By that  we have in mind the deﬁnition of an absolutely homogeneous Finsler metric [5]. Speciﬁcally  a
local norm is viewed here as continuous nonnegative function F : X × V → R+ with the following propoerties:
for all x ∈ X and all z1  z2 ∈ V  we have (i) F (x  z1 + z2) ≤ F (x  z1) + F (x  z2); (ii) F (x  λz) = |λ|z; and
(iii) F (x  z) > 0 for all z ∈ V \ {0}. The local norm of z at x is then deﬁned as (cid:107)z(cid:107)x = F (x  z).

5

Example 3.3. Suppose that X = Rd is endowed with the Euclidean norm as in Example 3.1.
Then  setting h(x) = (1/2)(cid:107)x(cid:107)2
2 for the
associated Bregman divergence. Obviously  h is 1-strongly convex relative to (cid:107)·(cid:107)2.
Example 3.4. Let X = [0  1)d (so X is neither open nor closed)  and consider the local norm
(cid:107)z(cid:107)2

2  we get the standard expression D(p  x) = (1/2)(cid:107)p − x(cid:107)2

i /(1 − xi)2 for x ∈ X   z ∈ Rd (cf. Example 3.2 above). If we set

x =(cid:80)d

i=1|z|2

a straightforward calculation gives

D(p  x) =

i=1 1/(1 − xi)

h(x) =(cid:80)d
(1 − pi)(1 − xi)2 ≥ d(cid:88)

(pi − xi)2

i=1

d(cid:88)

i=1

(pi − xi)2
(1 − xi)2 = (cid:107)p − x(cid:107)2

x 

(3.8)

(3.9)

i.e.  h is strongly convex relative to (cid:107)·(cid:107)x. Importantly  since (cid:107)·(cid:107)x ≥ (cid:107)·(cid:107)2  this Bregman function is
also strongly convex relative to the standard Euclidean norm. However  even though the Euclidean
regularizer of Example 3.3 is strongly convex relative to any global norm on X   it cannot be strongly
convex relative to the local norm (cid:107)·(cid:107)x because of the singularity of the latter when xi → 1−.

(cid:107)A(x(cid:48)) − A(x)(cid:107)x ∗ ≤ β(cid:112)2D(x  x(cid:48))

Bregman continuity. We are now in a position to introduce the notion of Bregman continuity:
Deﬁnition 3. Let h be a local Bregman function relative to some local norm (cid:107)·(cid:107)x on X . We say that
the operator A : X → V∗ is β-Bregman continuous if

for all x  x(cid:48) ∈ X .

we recover the standard Lipschitz continuity condition: (cid:107)A(x(cid:48)) − A(x)(cid:107)∗ ≤ β(cid:112)2D(x(cid:48)  x) =

(BC)
Of course  in the case of a global norm with Bregman function h(x) = (1/2)(cid:107)x(cid:107)2 (cf. Example 3.3) 
β(cid:107)x(cid:48) − x(cid:107). On the other hand  the example below shows that an operator can be Bregman continuous
without being Lipschitz continuous relative to any global norm:
Example 3.5. Consider the operator A(x) = (cr/(1 − xr/cr))r∈R deﬁned in Example 2.3. Renor-
malizing cr to 1 for clarity and using the Bregman data of Examples 3.2 and 3.4  we get:

d(cid:88)

i=1

i)2 ≤ d(cid:88)

i=1

i − xi)2
(x(cid:48)
(1 − x(cid:48)

(cid:107)A(x(cid:48)) − A(x)(cid:107)2

x ∗ =

√

i − xi)2
(x(cid:48)
(1 − xi)(1 − x(cid:48)

i)2 = D(x  x(cid:48))

(3.10)

2)-Bregman continuous relative to h. However  given the singularity of A(x) as

i.e.  A is (1/
xi → 1−  we see that A cannot be Lipschitz continuous relative to any global norm on X .
Importantly  this example suggests the following rule of thumb: if the Jacobian of A exhibits a
singularity of the form O(φ(x)) near the residual set cl(X ) \ X of X   taking (cid:107)·(cid:107)x = Θ(φ(x)) and
h(x) = Θ(φ(x)) allows A to be Bregman continuous  despite this singularity. This heuristic provides
a principled choice of Bregman data under which A satisﬁes (BC).

4 The mirror-prox algorithm

In this section  we present the main algorithmic method that we will use to solve (VI) under Bregman
continuity. Our core assumptions in that regard will be:
Assumption 1. The solution set X ∗ ≡ Sol(X   A) of (VI) is nonempty.
Assumption 2. A is monotone and β-Bregman continuous.

In addition to the above  we assume that the optimizer gains access to A via an oracle which  when
called at the t-th stage of a sequence Xt ∈ X   returns (possibly imperfect) feedback of the form

(4.1)
where Ut ∈ V∗ is an additive noise variable. The two cases of interest that we consider here are
(i) when Ut = 0 for all t; and (ii) when Ut satisﬁes the statistical hypotheses:

Vt = A(Xt) + Ut 

a) Zero-mean:

E[Ut | Ft] = 0.

(4.2a)

6

b) Finite variance: E[(cid:107)Ut(cid:107)2∗ | Ft] ≤ σ2.

(4.2b)
with Ft denoting the history (natural ﬁltration) of Xt. For obvious reasons  we will refer to the ﬁrst
case (Ut = 0) as a perfect oracle  and to the second one as a stochastic oracle.
Following Nemirovski [38] and Juditsky et al. [21]  the mirror-prox (MP) algorithm can be stated in
recursive form as follows:

Xt+1/2 = PXt(−γtVt)
Xt+1 = PXt(−γtVt+1/2)

(MP)

where γt > 0 is a variable step-size sequence (discussed in detail below)  and the so-called “prox-
mapping” P : X ◦ × V∗ → X is deﬁned as

(4.3)
with D(· ·) denoting the divergence of an underlying Bregman function h : X → R. For concreteness 
we also assume in what follows that (MP) is initialized at the so-called “prox-center” of X   i.e. 

Px(y) = arg min

x(cid:48)∈X

{(cid:104)y  x − x(cid:48)(cid:105) + D(x(cid:48)  x)}

(4.4)
Remark 1. In general  calculating mirror steps can be computationally expensive – just like Euclidean
projections in several cases. In what follows  we tacitly assume that our setting is “prox-friendly”
[21  38  40] in the sense that the update (4.3) can be computed efﬁciently (e.g.  as in Example 3.4).

X1 = xc ≡ arg minx∈X h(X ).

Heuristically  the main idea behind (MP) is that  at each t = 1  2  . . .   the oracle is called at the
algorithm’s base state Xt to generate an intermediate  leading state Xt+1/2; subsequently  the base
state is updated with oracle information from the leading state Xt+1/2 and the process repeats. In
this way  (MP) essentially tries to “anticipate” the change of A along a prox-step  and to exploit
this “forward” information in order to achieve a faster convergence rate than ordinary forward-
backward/gradient descent schemes. For this anticipatory scheme to work  the variation of the
operator A must be sufﬁciently gradual  hence the need for Lipschitz continuity in the classical
analysis of the algorithm [21  38  40]. If this variation is unbounded (e.g.  if A exhibits singularities) 
this look-ahead mechanism could break down completely and the algorithm might fail to converge
altogether. Our ﬁrst result below is that  despite such singularities  Bregman continuity allows us to
recover the optimal convergence rate of (MP):
Theorem 1. Assume that A satisﬁes Assumptions 1 and 2  and let GapH denote the restricted gap
function for the Bregman zone CH = {x ∈ X : D(x  xc) ≤ H}. Suppose further that (MP) is run
with a K-strongly convex Bregman function and oracle feedback of the form (4.1). Then  for all
t=1 γt enjoys the following gap bounds:

H > 0  the averaged sequence ¯XT =(cid:80)T

(cid:14)(cid:80)T

t=1 γtXt+1/2
a) If σ2 = 0 and the algorithm’s step-size satisﬁes

0 < γmin ≡ inf t γt ≤ supt γt ≡ γmax ≤ √

K/β 

we have

b) Otherwise  if σ2 > 0 and γt ≤(cid:112)K/2/β  we have

GapH ( ¯XT ) ≤ H
γmin

1
T

E[GapH ( ¯XT )] = O(cid:16) H+σ2(cid:80)T
(cid:80)T
t=1 γ2
t=1 γt
√
T   we get E[GapH ( ¯Xt)] = O(1/

t

T ).

(cid:17)

In particular  if γt ∝ 1/

√

(4.5)

(4.6)

(4.7)

As we show in the supplement  the key step in the proof of the deterministic part of Theorem 1 is the
following energy inequality for an arbitrary target point p ∈ CH:

D(p  Xt+1) ≤ D(p  Xt) − γt(cid:104)A(Xt+1/2)  Xt+1/2 − p(cid:105) −(cid:16)

D(Xt+1/2  Xt)

(4.8)

(cid:17)

1 − β2γ2
K

t

There are two points where the Bregman structure of the algorithm can be seen in (4.8): in the energy
iterates D(p  Xt)  but also in the comparison of the algorithm’s base and leading state in the term
D(Xt+1/2  Xt). In the “vanilla” setting  Lipschitz continuity is used to obtain a comparison of these

7

Algorithm 1: adaptive mirror-prox (AMP)
Require: local norm (cid:107)·(cid:107)x  K-strongly convex Bregman function h  shrink ratio θ ∈ (0  1)
1: take X1 = arg min h  γ1 > 0
2: for t = 1  2  . . . do
3:
4:
5:
6:

# base state query
# leading state update
# leading state query
# base state update

# initialization

get oracle feedback Vt at Xt
set Xt+1/2 = PXt (−γtVt)
get oracle feedback Vt+1/2 at Xt+1/2
set Xt+1 = PXt (−γtVt+1/2)
set βt =
set γt+1 = min{γt  θ

(cid:112)2D(Xt+1/2  Xt)

(cid:107)Vt+1/2 − Vt(cid:107)Xt+1/2 ∗

√
K/βt}

7:

8:
9: end for

# estimate Bregman constant

# update step-size

successive states in terms of a global norm difference of the form (cid:107)Xt+1/2 − Xt(cid:107)2. However  this
step also requires A to vary gradually relative to (cid:107)·(cid:107)  which is of course impossible if A exhibits
singularities. The key novelty in our setting is the use of the Bregman divergence as a comparator
for the algorithm’s successive states: it is at this point that the triple interplay between the operator 
the local norm and the chosen Bregman function is made manifest  and it is what makes Bregman
continuity particularly well-suited for tackling singular problems of this kind. This requires a careful
treatment of the various Bregman differences involved  so we defer the details to the supplement.

5 The adaptive mirror-prox algorithm

A crucial assumption underlying the analysis of the previous section is that the optimizer must know
in advance – or be otherwise able to estimate – the Bregman constant β. In practice  this can be
difﬁcult to achieve  so it is important to be able to run (MP) with an adaptive step-size policy. Our
starting point is the observation that  with perfect oracle feedback  one can estimate β by setting

(cid:107)A(Xt+1/2) − A(Xt)(cid:107)Xt+1/2 ∗

(cid:112)2D(Xt+1/2  Xt)

βt =

(5.1)
whenever Xt+1/2 (cid:54)= Xt; obviously  if A is β-Bregman continuous  we have βt ≤ β.7 However  the
γt ∝ √
fact that the Bregman constant is being under-estimated means that a step-size policy of the form
K/βt would over-estimate the inverse Bregman constant 1/β  so the resulting step-size

policy would have no reason to satisfy (4.5).
To overcome this obstacle  we introduce the following comparison mechanism: ﬁrst  at each t =
1  2  . . .   we use the estimation (5.1) to test the step-size ¯γt =
K/βt. Then  to avoid the growth
phenomenon outlined above  we shrink ¯γt by a constant factor of θ and  to avoid running into
vanishing step-size issues  we take the previous step-size employed if the shrunk one would be
smaller. Formally  we consider the adaptive step-size policy:

√

(cid:26)min{γt  θ

√

K/βt}

γt+1 =

γt

if Xt (cid:54)= Xt+1/2 
otherwise 

(5.2)

ergodic average ¯XT =(cid:80)T

with βt deﬁned as in (5.1) and θ ∈ (0  1) chosen arbitrarily.
For concreteness  we call the resulting algorithm adaptive mirror-prox (AMP) and we provide a
pseudocode implementation in Algorithm 1 above. In terms of performance  we have:
(cid:14)(cid:80)T
Theorem 2. Assume that A satisﬁes Assumptions 1 and 2  and (MP) is run with perfect oracle
feedback and the adaptive step-size policy (5.2). Then  with notation as in Theorem 1  the algorithm’s
t=1 γt enjoys the gap bound GapH ( ¯XT ) = O(1/T ).
We ﬁnd this result particularly appealing because it yields the optimal O(1/T ) convergence rate of
the mirror-prox algorithm  even for possibly singular operators  and even if the operator’s Bregman
constant is unknown. Its proof relies on using the speciﬁc form of the step-size policy (5.2) to control
the second term in the energy inequality (4.8); we provide the detailed arguments in the supplement.

t=1 γtXt+1/2

7In a Euclidean setting  similar ideas can be found in  e.g.  [8  29]. We ignore the origins of this technique.

8

Figure 1: Different variants of the mirror-prox algorithm in the resource sharing problem of Example 2.3. The
algorithm labeled “extra-gradient” refers to Euclidean regularization and a constant step size as indicated in the
legend; “mirror-prox” was run with the Bregman function of Example 3.4 and step-sizes as in the legend; ﬁnally 
“adaptive mirror-prox” corresponds to Algorithm 1  i.e.  mirror-prox with the adaptive step-size policy (5.2).
6 Numerical experiments
We performed a series of numerical experiments on the resource sharing problem described in
Example 2.3 with a set of R = 1000 servers being shared by N = 100 commodities  each with a
demand drawn uniformly at random from [0  1]; the capacity cr of each server r = 1  . . .   R was
also drawn randomly from [0  100]. Subsequently  we ran two variants of the mirror-prox method:
(MP) with Euclidean regularization  and (MP) with the Bregman function deﬁned in Example 3.4.
For all methods  we ran a range of different constant step-sizes (we present the most representative
values  namely γ = 0.001  γ = 0.005  and γ = 0.010). Subsequently  we also ran Algorithm 1 and
we plotted the distance from the solution to the induced variational inequality problem as a function
of the number of iterations. The main conclusions that can be drawn are as follows:
1. The Euclidean version of the mirror-prox algorithm (i.e.  the extra-gradient algorithm) is unstable
and does not converge; this is due to the fact that the gradients received are very large (recall
that the problem is not Lipschitz continuous)  so the algorithm does not exhibit descent or
convergence.

2. The MP variant with the non-Euclidean regularizer of Example 3.4 is convergent (since the
VI problem under study is Bregman continuous relative to this Bregman function). However 
depending on the method’s step-size  the convergence is relatively slow  and there is no easy way
to estimate the problem’s Bregman constant in order to choose a “good” step-size.

3. By contrast  the AMP algorithm converges signiﬁcantly faster than variants with a constant step-
size. This is due to the fact that  initially  a greedier step-size is able to take larger steps towards
the problem’s solution  so initializing Algorithm 1 with a large step-size helps signiﬁcantly.

7 Concluding remarks
In this work  we introduced a novel regularity condition to account for variational inequalities (both
deterministic and stochastic) with possible singularities. This condition  which we call Bregman
continuity  is tailored to the operator’s singularity landscape and  as such  provides the necessary
bedrock to achieve optimal convergence rates via a properly chosen version of the mirror-prox
algorithm (with or without knowledge of the problem’s Bregman constant). This opens up several
interesting research directions: First  an appealing extension would be to develop a “model-agnostic”
version of the method (which would concurrently provide optimal rates in stochastic and deterministic
settings) or to combine it with backtracking / linesearch to accelerate convergence. Finally  it would
also be interesting to examine the method’s local convergence properties in non-monotone problems
(deterministic or stochastic). We relegate these questions to future work.

Acknowledgments

The authors gratefully acknowledge ﬁnancial support from the French National Research Agency
(ANR) under grants ORACLESS (ANR–16–CE33–0004–01) and ELIOT (ANR-18-CE40-0030).

9

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△○�����-��������(γ=����)□�����-��������(γ=����)◇�����-��������(γ=����)●������-����(γ=����)■������-����(γ=����)◆������-����(γ=����)△����������������������������������������-����-����-����������������������������������������������������������References
[1] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton

University Press  2008.

[2] F. Alvarez  J. Bolte  and O. Brahic. Hessian Riemannian gradient ﬂows in convex programming. SIAM

Journal on Control and Optimization  43(2):477–501  2004.

[3] F. Bach and K. Y. Levy. A universal algorithm for variational inequalities adaptive to smoothness and

noise. In COLT ’19: Proceedings of the 32nd Annual Conference on Learning Theory  2019.

[4] J.-B. Baillon and G. Haddad. Quelques propriétés des opérateurs angle-bornés et n-cycliquement mono-

tones. Israel Journal of Mathematics  26:137–150  1977.

[5] D. D.-W. Bao  S.-S. Chern  and Z. Shen. An Introduction to Riemann-Finsler Geometry. Number 200 in

Graduate Texts in Mathematics. Springer-Verlag  New York  NY  2000.

[6] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer  New York  NY  USA  2 edition  2017.

[7] H. H. Bauschke  J. Bolte  and M. Teboulle. A descent lemma beyond Lipschitz gradient continuity:
First-order methods revisited and applications. Mathematics of Operations Research  42(2):330–348  May
2017.

[8] R. I. Bo¸t  E. R. Csetnek  and P. T. Vuong. The forward-backward-forward method from continuous and
discrete perspective for pseudo-monotone variational inequalities in Hilbert spaces. https://arxiv.org/
abs/1808.08084  2018.

[9] J. Bolte  S. Sabach  M. Teboulle  and Y. Vaisbourd. First order methods beyond convexity and Lipschitz
gradient continuity with applications to quadratic inverse problems. SIAM Journal on Optimization  28(3):
2131–2151  2018.

[10] M. Bravo and P. Mertikopoulos. On the robustness of learning in games with stochastically perturbed
payoff observations. Games and Economic Behavior  103  John Nash Memorial issue:41–66  May 2017.
[11] M. Bravo  D. S. Leslie  and P. Mertikopoulos. Bandit learning in concave N-person games. In NIPS ’18:

Proceedings of the 32nd International Conference on Neural Information Processing Systems  2018.

[12] R. E. Bruck Jr. On the weak convergence of an ergodic iteration for the solution of variational inequalities
for monotone operators in Hilbert space. Journal of Mathematical Analysis and Applications  61(1):
159–164  November 1977.

[13] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to

imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  May 2011.

[14] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale

Modeling and Simulation  4(4):1168–1200  2005.

[15] C. Daskalakis  A. Ilyas  V. Syrgkanis  and H. Zeng. Training GANs with optimism.

Proceedings of the 2018 International Conference on Learning Representations  2018.

In ICLR ’18:

[16] F. Facchinei and J.-S. Pang. Finite-Dimensional Variational Inequalities and Complementarity Problems.

Springer Series in Operations Research. Springer  2003.

[17] G. Gidel  H. Berard  G. Vignoud  P. Vincent  and S. Lacoste-Julien. A variational inequality perspective
on generative adversarial networks. In ICLR ’19: Proceedings of the 2019 International Conference on
Learning Representations  2019.

[18] I. J. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In NIPS ’14: Proceedings of the 27th International Conference on
Neural Information Processing Systems  2014.

[19] F. Hanzely  P. Richtarik  and L. Xiao. Accelerated Bregman proximal gradient methods for relatively

smooth convex optimization. https://arxiv.org/abs/1808.03045  2018.

[20] A. N. Iusem  A. Jofré  R. I. Oliveira  and P. Thompson. Extragradient method with variance reduction for

stochastic variational inequalities. SIAM Journal on Optimization  27(2):686–724  2017.

[21] A. Juditsky  A. S. Nemirovski  and C. Tauvel. Solving variational inequalities with stochastic mirror-prox

algorithm. Stochastic Systems  1(1):17–58  2011.

[22] S. M. Kakade  S. Shalev-Shwartz  and A. Tewari. Regularization techniques for learning with matrices.

The Journal of Machine Learning Research  13:1865–1890  2012.

[23] L. Kleinrock. Queueing Systems  volume 1: Theory. John Wiley & Sons  New York  NY  1975.
[24] G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Èkonom. i

Mat. Metody  12:747–756  1976.

[25] T. Liang and J. Stokes. Interaction matters: A note on non-asymptotic local convergence of generative
adversarial networks. In AISTATS ’19: Proceedings of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics  2019.

10

[26] H. Lu. "Relative-continuity" for non-Lipschitz non-smooth convex optimization using stochastic (or

deterministic) mirror descent. https://arxiv.org/abs/1710.04718  2017.

[27] H. Lu  R. M. Freund  and Y. Nesterov. Relatively-smooth convex optimization by ﬁrst-order methods and

applications. SIAM Journal on Optimization  28(1):333–354  2018.

[28] Y. Malitsky. Projected reﬂected gradient methods for monotone variational inequalities. SIAM Journal on

Optimization  25(1):502–520  2015.

[29] Y. Malitsky. Golden ratio algorithms for variational inequalities. https://arxiv.org/abs/1803.08832 

2018.

[30] P. Mertikopoulos and W. H. Sandholm. Learning in games via reinforcement and regularization. Mathe-

matics of Operations Research  41(4):1297–1324  November 2016.

[31] P. Mertikopoulos and W. H. Sandholm. Riemannian game dynamics. Journal of Economic Theory  177:

315–364  September 2018.

[32] P. Mertikopoulos and M. Staudigl. On the convergence of gradient-like ﬂows with noisy gradient input.

SIAM Journal on Optimization  28(1):163–197  January 2018.

[33] P. Mertikopoulos and Z. Zhou. Learning in games with continuous action sets and unknown payoff

functions. Mathematical Programming  173(1-2):465–507  January 2019.

[34] P. Mertikopoulos  E. V. Belmega  R. Negrel  and L. Sanguinetti. Distributed stochastic optimization via

matrix exponential learning. IEEE Trans. Signal Process.  65(9):2277–2290  May 2017.

[35] P. Mertikopoulos  C. H. Papadimitriou  and G. Piliouras. Cycles in adversarial regularized learning. In

SODA ’18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms  2018.

[36] P. Mertikopoulos  B. Lecouat  H. Zenati  C.-S. Foo  V. Chandrasekhar  and G. Piliouras. Optimistic mirror
descent in saddle-point problems: Going the extra (gradient) mile. In ICLR ’19: Proceedings of the 2019
International Conference on Learning Representations  2019.

[37] A. Mokhtari  A. Ozdaglar  and S. Pattathil. A uniﬁed analysis of extra-gradient and optimistic gradient
methods for saddle point problems: proximal point approach. https://arxiv.org/abs/1901.08511v2 
2019.

[38] A. S. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization  15(1):229–251  2004.

[39] A. S. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[40] Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems.

Mathematical Programming  109(2):319–344  2007.

[41] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming  120(1):

221–259  2009.

[42] N. Nisan  T. Roughgarden  É. Tardos  and V. V. Vazirani  editors. Algorithmic Game Theory. Cambridge

University Press  2007.

[43] A. Odena  C. Olah  and J. Shlens. Conditional image synthesis with auxiliary classiﬁer GANs. https:

//arxiv.org/abs/1610.09585  October 2016.

[44] G. B. Passty. Ergodic convergence to a zero of the sum of monotone operators in Hilbert space. Journal of

Mathematical Analysis and Applications  72(2):383–390  December 1979.

[45] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In COLT ’13: Proceedings of

the 26th Annual Conference on Learning Theory  2013.

[46] A. Rakhlin and K. Sridharan. Optimization  learning  and games with predictable sequences. In NIPS ’13:

Proceedings of the 26th International Conference on Neural Information Processing Systems  2013.

[47] S. M. Shahshahani. A New Mathematical Framework for the Study of Linkage and Selection. Number 211
in Memoirs of the American Mathematical Society. American Mathematical Society  Providence  RI  1979.
[48] S. Sra  S. Nowozin  and S. J. Wright. Optimization for Machine Learning. MIT Press  Cambridge  MA 

USA  2012.

[49] G. Stampacchia. Formes bilineaires coercitives sur les ensembles convexes. Comptes Rendus Hebdo-

madaires des Séances de l’Académie des Sciences  1964.

[50] J. G. Wardrop. Some theoretical aspects of road trafﬁc research. In Proceedings of the Institute of Civil

Engineers  Part II  volume 1  pages 325–378  1952.

[51] A. Yadav  S. Shah  Z. Xu  D. Jacobs  and T. Goldstein. Stabilizing adversarial nets with prediction methods.

In ICLR ’18: Proceedings of the 2018 International Conference on Learning Representations  2018.

11

,Kimon Antonakopoulos
Veronica Belmega
Panayotis Mertikopoulos