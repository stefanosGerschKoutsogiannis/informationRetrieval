2018,Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation,Shift-and-invert preconditioning  as a classic acceleration technique for the leading eigenvector computation  has received much attention again recently  owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work  we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme. The shift-and-inverted power method is included as a special case with adaptive step-sizes. Particularly  two other step-size settings  i.e.  constant step-sizes and Barzilai-Borwein (BB) step-sizes  are examined theoretically and/or empirically. We present a novel convergence analysis for the constant step-size setting that achieves a rate at $\tilde{O}(\sqrt{\frac{\lambda_{1}}{\lambda_{1}-\lambda_{p+1}}})$  where $\lambda_{i}$ represents the $i$-th largest eigenvalue of the given real symmetric matrix and $p$ is the multiplicity of $\lambda_{1}$. Our experimental studies show that the proposed algorithm can be significantly faster than the shift-and-inverted power method in practice.,Gradient Descent Meets Shift-and-Invert

Preconditioning for Eigenvector Computation

National Engineering Laboratory of Deep Learning Technology and Application  China

Cognitive Computing Lab (CCL)  Baidu Research

Zhiqiang Xu

NKDEGE=C"(>=E@K?

Abstract

Shift-and-invert preconditioning  as a classic acceleration technique for the lead-
ing eigenvector computation  has received much attention again recently  owing
to fast least-squares solvers for efﬁciently approximating matrix inversions in
power iterations.
In this work  we adopt an inexact Riemannian gradient de-
scent perspective to investigate this technique on the effect of the step-size scheme.
The shift-and-inverted power method is included as a special case with adaptive
step-sizes. Particularly  two other step-size settings  i.e.  constant step-sizes and
Barzilai-Borwein (BB) step-sizes  are examined theoretically and/or empirically.
We present a novel convergence analysis for the constant step-size setting that
achieves a rate at ~O(
)  where (cid:21)i represents the i-th largest eigenvalue
of the given real symmetric matrix and p is the multiplicity of (cid:21)1. Our experimen-
tal studies show that the proposed algorithm can be signiﬁcantly faster than the
shift-and-inverted power method in practice.

(cid:21)1(cid:0)(cid:21)p+1

√

(cid:21)1

1

Introduction

1p
(cid:21)1(cid:0)(cid:21)2

Eigenvector computation is a fundamental problem in numerical algebra and often of central impor-
tance to a variety of scientiﬁc and engineering computing tasks such as principal component analysis
[Fan et al.  2018]  spectral clustering [Ng et al.  2001]  low-rank matrix approximation [Hastie et al. 
2015  Liu and Li  2014]  among others. Classic solvers for this problem are power methods and
Lanczos algorithms [Golub and Van Loan  1996]. Although Lanczos algorithms possess the optimal
convergence rate ~O(
)  it seems not amenable to stochastic optimization. People thus tend to
develop faster algorithms on top of power methods [Arora et al.  2012  2013  Hardt and Price  2014 
Shamir  2015  Garber and Hazan  2015  Garber et al.  2016  Lei et al.  2016  Wang et al.  2017]. One
notable technique among them is the shift-and-invert preconditioning that has revived recently for
this purpose [Garber and Hazan  2015  Garber et al.  2016  Wang et al.  2017  Gao et al.  2017]. Us-
ing this technique  each power iteration step can be reduced to approximately solving a linear system
subproblem that can leverage fast least-squares solvers  e.g.  accelerated gradient descent (AGD)
[Nesterov  2014] or stochastic variance reduced gradient (SVRG) [Johnson and Zhang  2013].
In this work  we take a Riemannian gradient descent view to investigate the shift-and-invert precon-
ditioning for the leading eigenvector computation on the effect of the step-size scheme. The resulting
algorithm thus is termed as the shift-and-inverted Riemannian gradient descent eigensolver  or SI-
rgEIGS for short. It includes the shift-and-invert preconditioned power method (termed as SI-PM for
short) as a special case with adaptive step-sizes. Applying the shift-and-invert preconditioning tech-
nique needs to locate an appropriate upper bound of the largest eigenvalue  i.e.  (cid:27) > (cid:21)1  as the shift
parameter. We reply on the crude phase of the shift-and-inverted power method [Garber and Hazan 
2015  Garber et al.  2016] to get this upper bound in theory. However  in practice  the plain power

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

method often works via the proposed heuristics in experiments. In addition  the crude phase can
warm-start the Riemannian gradient descent method. Similarly  Shamir [2016a] adopted the plain
power method to warm-start the stochastic variance reduced projected gradient descent without pre-
conditioning for principal component analysis (VR-PCA). The crude phase only consumes non-
dominant time due to the independence of the ﬁnal accuracy parameter ϵ [Wang et al.  2017]. The
algorithm then steps into an accurate phase by calling the Riemannian gradient descent solver on the
shift-and-inverted matrix ((cid:27)I (cid:0) A)

(cid:0)1  i.e.  solving the following problem:

h(x) = (cid:0) 1
2

⊤

((cid:27)I (cid:0) A)

(cid:0)1x:

x

min

m YY

(1)
x2Rn(cid:2)1:∥x∥2=1
In each gradient descent step  we have to solve a linear system ((cid:27)I (cid:0) A)z = xt(cid:0)1 in order to
get the Euclidean gradient ((cid:27)I (cid:0) A)
(cid:0)1xt(cid:0)1. The key advantage of the preconditioning technique
is that we only need to solve the system to an approximate level commensurate with the quality
of the current iterate. This can be easily accomplished by performing convex optimization on the
associated least-squares problem (see Equation (3)). Another advantage of the reduction to convex
optimization is that it enables stochastic optimization [Garber and Hazan  2015  Garber et al.  2016] 
⊤  where Y 2 Rm(cid:2)d. Approximate solutions
especially for the covariance structure of A = 1
to the linear systems requires one to cope with inexact Riemannian gradients. In fact  as we will
see for Problem (1)  the inexact Riemannian gradient method includes the shift-and-inverted power
method as a special case with adaptive step-sizes. In the present paper  two other step-size schemes 
i.e.  constant step-sizes and Barzilai-Borwein (BB) step-sizes  are examined theoretically and/or
empirically. Different from Shamir [2015] and Wang et al. [2017] which only consider the positive
eigengap between (cid:21)1 and (cid:21)2  i.e.  (cid:21)1 > (cid:21)2  for the constant step-size setting we explicitly take care
of all the cases of this eigengap and achieve a uniﬁed convergence rate at ~O(
) via a novel
analysis (e.g.  the potential function and the way we cope with the solution space)  where p < n is
the multiplicity of (cid:21)1 and (cid:21)1 (cid:0) (cid:21)p+1 > 0 always holds without loss of generality. To the best of our
knowledge  this is the ﬁrst time that a gradient descent solver for the problem with ﬁxed step-sizes
reaches this type of rate  which is a nearly biquadratic improvement over ~O(
((cid:21)1(cid:0)(cid:21)2)2 ) [Shamir 
2015]. In addition  the rate logarithmically depends on the initial iterate  instead of quadratically
as in Shamir [2015]. Theoretical properties are veriﬁed on synthetic data in experiments. For real
data  we explore an automatic step-size scheme  i.e  Barzilai-Borwein (BB) step-sizes  to eliminate
the difﬁculty of hand-tuning step-sizes. Experimental results indicate that the shift-and-inverted
Riemannian gradient descent method can be signiﬁcantly faster than the shift-and-inverted power
method that has gained much popularity recently.
The rest of the paper is organized as follows. We brieﬂy discuss recent literature in Section 2 and
then present our shift-and-inverted Riemannian gradient descent solver with theoretical analysis in
Section 3. Experiments are reported in Section 4. The paper then ends with discussions in Section
5.

(cid:21)1(cid:0)(cid:21)p+1

√

(cid:21)1

1

2 Related Work

Recent research on eigenvector computation has been mainly focusing on theoretically scaling up
related algorithms. Halko et al. [2011] surveyed and extended randomized algorithms for truncated
singular value decomposition (SVD)  while Musco and Musco [2015] proposed randomized block
Krylov methods for stronger and faster approximate SVD. Convergence rates for both versions are
provided in Musco and Musco [2015]. Hardt and Price [2014] studied the noisy power method for
the small noise case  and Balcan et al. [2016] extended this method to achieve an improved gap de-
pendency by using subspace iterates of larger dimensions. Garber et al. [2016] presented a robust
analysis of the shift-and-invert preconditioned power method and achieved optimal convergence
rates. Allen-Zhu and Li [2016] reproved the result for this method and extended to the case that
k > 1 by deﬂation via a careful analysis  while Wang et al. [2017] improved the associated anal-
ysis and advocated coordinate descent as the solver for linear systems. Lei et al. [2016] proposed
a different coordinate-wise power method. Sa et al. [2017] proposed the accelerated (stochastic)
power method with optimal rate. However  its empirical performance seems not as good as ex-
pected in our experiments. Our work is more related to another line of work on gradient descent
solvers. Arora et al. [2012] proposed the stochastic power method without theoretical guarantees
which runs the projected stochastic gradient descent (PSGD) for the PCA problem. Arora et al.

2

Table 1: Typical convergence rates. ~O notations hide logarithmic factors  e.g.  log 1

ϵ   log

1

(cid:21)1(cid:0)(cid:21)2

.

Paper
PSGD [Arora et al.  2013]
Oja’s algorithm [Balsubramani et al.  2013]
Noisy PM [Hardt and Price  2014]
VR-PCA [Shamir  2015]
Power Method (PM) [Musco and Musco  2015]
Block Krylov [Musco and Musco  2015]
SGD-PCA [Shamir  2016b]
Shift-and-Inverted PM [Garber et al.  2016]
Coordiante-wise PM [Lei et al.  2016]
Accelerated PM [Sa et al.  2017]
This work

Rate
O(1=ϵ2)
O(1=(((cid:21)1 (cid:0) (cid:21)2)2ϵ))
~O(1=((cid:21)1 (cid:0) (cid:21)2))
~O(1=((cid:21)1 (cid:0) (cid:21)2)2)
~O(1=((cid:21)1 (cid:0) (cid:21)2))
(cid:21)1 (cid:0) (cid:21)2)
~O(1=
O(1=(((cid:21)1 (cid:0) (cid:21)2)ϵ))
(cid:21)1 (cid:0) (cid:21)2)
~O(1=
√
~O(1=((cid:21)1 (cid:0) (cid:21)2))
(cid:21)1 (cid:0) (cid:21)2)
~O(1=
(cid:21)1 (cid:0) (cid:21)p+1)
~O(1=

p
p
p

[2013] subsequently extended this method via the convex relaxation with theoretical guarantees.
Balsubramani et al. [2013] achieved a better guarantee for PCA via the martingale analysis. [Shamir 
2015  2016a] proposed the VR-PCA which extended the projected stochastic variance reduced gra-
dient (SVRG) to the non-convex PCA problem with global convergence guarantees for the case that
(cid:21)1 > (cid:21)2. Shamir [2016b] also studied SGD for the non-convex PCA problem and established its
sub-linear convergence rates. Wen and Yin [2013] proposed a practical curvilinear search method
for addressing the eigenvalue problem but without theoretical analysis. It actually belongs to the
Riemannian gradient descent method. By proving an explicit Łojasiewicz exponent at 1
2  Liu et al.
[2016] established the local and linear convergence rate of the Riemannian gradient method with a
line-search procedure for quadratic optimization problems under orthogonality constraints. Details
of typical theoretical results are summarized in Table 1.

3 Shift-and-Inverted Riemannian Gradient Descent Solver

In this section  we present our shift-and-inverted Riemannian gradient descent solver. Without loss
of generality  eigenvalues of the given real symmetric matrix A are assumed to be in [0; 1] and the
multiplicity of the largest eigenvalue (cid:21)1 is p    i.e.  1 (cid:21) (cid:21)1 = (cid:1)(cid:1)(cid:1) = (cid:21)p > (cid:21)p+1 (cid:21) (cid:1)(cid:1)(cid:1) (cid:21) (cid:21)n (cid:21) 0.
Deﬁne the i-th eigengap of A as ∆i = (cid:21)i (cid:0) (cid:21)i+1. Most of existing work handle only the case that
∆1 = (cid:21)1(cid:0)(cid:21)2 > 0  ignoring the case that ∆1 = 0. In this work  the two cases are uniﬁed via ∆p > 0
which holds always without loss of generality1  i.e.  p < n. Suppose that corresponding eigenvectors
are v1;(cid:1)(cid:1)(cid:1) ; vn. Our goal then is to ﬁnd one of the leading eigenvectors  i.e.  v 2 span(v1;(cid:1)(cid:1)(cid:1) ; vp)
and ∥v∥2 = 1. Let Vp = (v1;(cid:1)(cid:1)(cid:1) ; vp) and denote B = ((cid:27)I(cid:0)A)
(cid:0)1 as the shift-and-inverted matrix 
satisfying (cid:22)1 = (cid:1)(cid:1)(cid:1) = (cid:22)p > (cid:22)p+1 (cid:21) (cid:1)(cid:1)(cid:1) (cid:21) (cid:22)n 
where (cid:27) > (cid:21)1. B’s eigenvalues then are (cid:22)i = 1
(cid:27)(cid:0)(cid:21)i
while eigenvectors remain unchanged. Accordingly  deﬁne the i-th eigengap of B as (cid:28)i = (cid:22)i(cid:0) (cid:22)i+1.
In particular 

(cid:22)p (cid:0) (cid:22)p+1

(cid:22)1

(cid:28)p
(cid:22)1

=

=

∆p

(cid:27) (cid:0) (cid:21)p+1

:

1

(cid:27)(cid:0)(cid:21)p

=

1

(cid:27)(cid:0)(cid:21)p+1

(cid:0)
(cid:27)(cid:0)(cid:21)1

1

A faster rate can be obtained if the relative eigengap can be enlarged from A to B  which is ex-
actly the idea behind the shift-and-invert preconditioning. To this end  we follow Garber and Hazan
[2015] and Wang et al. [2017]’s procedure (see the supplementary material) to choose an appropri-
ate constant (cid:27) such that it is only slightly larger than (cid:21)1  i.e.  (cid:27) = (cid:21)1 + c∆p where c 2 [ 1
2 ]  as
guaranteed by the following theorem.
Theorem 3.1 [Garber and Hazan  2015  Wang et al.  2017] Let ϵ(x) = l(x) (cid:0) minx l(x) be the
If the initial to ﬁnal error ratio for the least-
function error with the least-squares subproblem.
squares subproblems can be maintained as ϵ(z0)
(cid:17)2 where m =

ϵ(z(cid:3)) = 32(cid:1)102m+1

4 ; 3

and ϵ(w0)

ϵ(w(cid:3)) = 1024

(cid:17)2m

1If p = n  the objective function h(x) is constant and Problem (1) is trivial.

3

⌈8 log
iterations in the outer repeat-until loop.

p ~y0∥2

16
∥V⊤

2

⌉  then we have the output (cid:27) = (cid:21)1 + c∆p for certain c 2 [ 1

4 ; 3

2 ] after S = O(log 1
(cid:17) )

(cid:21) 2

= 1
c+1

We then have (cid:28)p
5 and can run Riemannian gradient descent to solve Problem (1). The
(cid:22)1
algorithmic steps are described in Algorithm 1 which caters to all the step-size settings. Riemannian
gradient descent with normalization retraction [Absil et al.  2008]  i.e.  R(x; (cid:24)) = x+(cid:24)
for any (cid:24)
∥x+(cid:24)∥2
from the tangent space of the sphere ∥x∥2 = 1 at x  can be written as

xt = R(xt(cid:0)1;(cid:0)(cid:11)t ~∇h(xt(cid:0)1))
= R(xt(cid:0)1;(cid:0)(cid:11)t(I (cid:0) xt(cid:0)1x
t(cid:0)1)∇h(xt(cid:0)1))
⊤
= R(xt(cid:0)1; (cid:11)t(I (cid:0) xt(cid:0)1x
⊤
t(cid:0)1)Bxt(cid:0)1);

(2)
where ~∇h(xt(cid:0)1) and ∇h(xt(cid:0)1) represent the Riemannian gradient2 and Euclidean gradient  respec-
tively. As ( (cid:22)1
)2 = O(1)  gradient descent takes only a logarithmic number of iterations O(log 1
ϵ )
(cid:28)p
to converge now  which does not have the quadratic dependence on
any more [Shamir  2015 
2016a  Xu et al.  2017  Xu and Gao  2018  Xu et al.  2018]. However  we need to calculate the Eu-
clidean gradient ∇h(x) = (cid:0)Bx  which by inverting the shifted matrix (cid:27)I (cid:0) A will be inefﬁcient.
Fortunately  as stated in Line 3  Algorithm 1  we can make it efﬁcient by solving an equivalent least-
squares subproblem  and an approximate solution to the subproblem will sufﬁce. It is worth noting
that when (cid:11)t = 1=x

⊤
t Bxt Algorithm 1 will recover the shift-and-inverted power method.

(cid:21)1(cid:0)(cid:21)2

(cid:21)1

Algorithm 1 Shift-and-Inverted Riemannian Gradient Descent Eigensolver
1: Input: matrix A  shift (cid:27)  and initial x0.
2: for t = 1; 2;(cid:1)(cid:1)(cid:1) do
3:

approximate negative Euclidean gradient
yt(cid:0)1 (cid:25) arg min

lt(z) = z

z

⊤

(cid:0)1z=2 (cid:0) x
B

by a fast least-squares solver  e.g.  AGD  starting from z0 = xt(cid:0)1=x
approximate Riemannian gradient ^gt(cid:0)1 = (cid:0)(I (cid:0) xt(cid:0)1x
choose a step size (cid:11)t > 0
set xt = (xt(cid:0)1 (cid:0) (cid:11)t^gt(cid:0)1)=∥xt(cid:0)1 (cid:0) (cid:11)t^gt(cid:0)1∥2
terminate if stopping criterion is met

⊤
t(cid:0)1)yt(cid:0)1

4:
5:
6:
7:
8: end for

⊤
t(cid:0)1z
⊤
t(cid:0)1B

(cid:0)1xt(cid:0)1

(3)

3.1 Analysis

We now provide the convergence analysis of Algorithm 1 under the constant step-size setting. To
measure the progress of iterates to one of the leading eigenvectors  we use a novel potential function
deﬁned by
p xt∥2 =
for analysis. As ∥V
⊤
cos (cid:18)(xt; Vp)  where (cid:18)(xt; Vp) 2 [0; (cid:25)
2 ] represents the principal angle [Golub and Van Loan  1996]
between xt and the space of the leading eigenvectors span(Vp). Particularly  it is worth noting that

p xt∥2 (cid:20) ∥Vp∥2∥xt∥2 = 1  we have (xt) (cid:21) 0.
⊤

 (xt; Vp) = (cid:0)2 log ∥V

In fact  ∥V

p xt∥2
⊤

where (cid:18)(xt; v) 2 [0; (cid:25)
span(Vp) is equal to the minimum angle between x and any v 2 span(Vp). Thus  we can write

2 ]. That is  the angle between a vector x and a p-dimensional subspace

(cid:18)(xt; Vp) = min

v2span(Vp)

(cid:18)(xt; v);

 (xt; Vp) = min
v2Vp;1

 (xt; v);

xtj = (cid:0)2 log cos (cid:18)(xt; v)
where Vp;1   fv 2 span(Vp) : ∥v∥2 = 1g and (xt; v) = (cid:0)2 log jv
for any v 2 Vp;1. This property will play an important role in our analysis. It is easy to see that if
2It can be obtained by projecting the Euclidean gradient onto the tangent space [Absil et al.  2008] at xt(cid:0)1 

⊤

i.e.  ~∇h(xt(cid:0)1) = Pxt(cid:0)1

∇h(xt(cid:0)1)  where Pxt(cid:0)1 = I (cid:0) xt(cid:0)1x

⊤
t(cid:0)1.

4

 (xt; Vp) goes to 0  xt must converge to certain vector v 2 Vp;1. We also use another potential
function

sin2 (cid:18)(xt; Vp) = 1 (cid:0) ∥V

p xt∥2
⊤
2:

Our main results then can be stated as follows.
Theorem 3.2 Given a shift parameter (cid:27) = (cid:21)1 + c∆1 for c 2 (0; 3
2 ]  Algorithm 1 with ﬁxed step-
sizes and using accelerated gradient descent as a least-squares solver is able to converge to one of
the leading eigenvectors of A  i.e.  (xT ; Vp) < ϵ  after T = O(log (x0;Vp)
) gradient steps  and
the overall complexity is O(

log (x0;Vp)

√

).

ϵ

(cid:21)1
∆p

log (cid:21)1
∆p

ϵ

⊤

Bx (cid:20) ((cid:22)1 (cid:0) (cid:22)n) sin2 (cid:18)(x; Vp) and ∥ ~∇h(x)∥2 (cid:20)

To prove the theorem  we need the following auxiliary lemmas whose proofs are given in the supple-
mentary material.
Lemma 3.3 (cid:28)p sin2 (cid:18)(x; Vp) (cid:20) (cid:22)1 (cid:0) x
2(cid:22)1 sin (cid:18)(x; Vp).
Lemma 3.4
(cid:21)
1(cid:0)log(1(cid:0)x) .
x(cid:0) log(1(cid:0)x)
Lemma 3.5 [Wang et al.  2017] Let z⋆ = arg min lt(z) = Bxt(cid:0)1  (cid:24)t = yt (cid:0) z⋆  and ϵt = lt(yt)(cid:0)
lt(z⋆). Then ∥(cid:24)t∥2 (cid:20) p
sin2 (cid:18)(xt(cid:0)1; Vp). Moreover  Nesterov’s
) complexity for solving Problem (3) to

(cid:20) log(1 + x) (cid:20) x for any x > (cid:0)1  while for any x 2 (0; 1) it holds that

2(cid:22)1ϵt and lt(z0) (cid:0) lt(z⋆) (cid:20) (cid:22)2
log lt(z0)(cid:0)lt(z⋆)

√

1
2(cid:22)n

1+x

x

1

accelerated gradient descent takes O(
sub-optimality ϵt.

(cid:21)1
∆p

ϵt

Since the least-squares solver for Problem (3) is warm-started with z0 =
  the initial
error lt(z0)(cid:0) lt(z⋆) is much smaller than the error from the random initial z0. We can also try other
least-squares solvers  such as SVRG [Johnson and Zhang  2013]  accelerated SVRG [Garber et al. 
2016]  and coordinate descent [Wang et al.  2017].

⊤
t(cid:0)1B(cid:0)1xt(cid:0)1
x

xt(cid:0)1

Proof of Theorem 3.2

Proof For brevity  denote (cid:18)t = (cid:18)(xt; Vp) and t = (xt; Vp) throughout the proof. First  for any
v 2 Vp;1 

 (xt+1; v) = (cid:0)2 log jv
= (cid:0)2 log jv

⊤
⊤

xt+1j
(xt (cid:0) (cid:11)t+1^gt)j + 2 log ∥xt (cid:0) (cid:11)t+1^gt∥2:

From Lemma 3.5 and Equation (2)  we can write ^gt = ~∇h(xt) (cid:0) (I (cid:0) xtx
⊤
t )(cid:24)t  where (cid:24)t is the
error with the approximate negative Euclidean gradient in Line 4 of Algorithm 1 incurred from
least-squares subproblems (3). We then can expand

(4)

(5)

jv
= jv
(cid:21) jv

(cid:21) jv
(cid:21) jv

⊤
⊤
⊤
(cid:0)2(cid:11)t+1jv
⊤

(xt (cid:0) (cid:11)t+1^gt)j2
(xt (cid:0) (cid:11)t+1 ~∇h(xt)) + (cid:11)t+1v
(xt (cid:0) (cid:11)t+1 ~∇h(xt))j2 + (cid:11)2

t )(cid:24)tj2
(I (cid:0) xtx
⊤
⊤
jv
(I (cid:0) xtx
t )(cid:24)tj2
⊤
⊤
(I (cid:0) xtx
t )(cid:24)tj
(xt (cid:0) (cid:11)t+1 ~∇h(xt))j (cid:1) jv
⊤
⊤
(xt (cid:0) (cid:11)t+1 ~∇h(xt))j2 (cid:0) 2(cid:11)t+1jv
(xt (cid:0) (cid:11)t+1 ~∇h(xt))j (cid:1) jv
⊤
(I (cid:0) xtx
∥v
t )∥2∥(cid:24)t∥2
⊤
⊤
(xt (cid:0) (cid:11)t+1 ~∇h(xt))j2(1 (cid:0) 2(cid:11)t+1
jv⊤(xt (cid:0) (cid:11)t+1 ~∇h(xt))j );

t+1

⊤

⊤

⊤

(I (cid:0) xtx

t )(cid:24)tj
⊤

where the last inequality is by the Cauchy-Schwartz inequality. To proceed  we note that

Bxt (cid:0) v
Together with Lemma 3.3  we then have

⊤ ~∇h(xt) = (cid:0)(v

⊤

v

⊤

t Bxt) = (cid:0)((cid:22)1 (cid:0) x
⊤

⊤
t Bxt)v

⊤

xt:

xtx

⊤

jv

(xt (cid:0) (cid:11)t+1 ~∇h(xt))j = (1 + (cid:11)t+1((cid:22)1 (cid:0) x

(cid:21) (1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t)jv

⊤

xtj

t Bxt))jv
⊤
xtj:

⊤

5

In addition  one can write

⊤

∥v

(I (cid:0) xtx

⊤

t )∥2 = ∥v
⊤

?
x
t
= (1 (cid:0) (v
2 = (xt (cid:0) (cid:11)t+1^gt)

⊤

x

⊤

⊤

?
t )

?
t (x

∥2 = (v
⊤
xt)2)1=2 = sin (cid:18)(xt; v);
(xt (cid:0) (cid:11)t+1^gt) = 1 + (cid:11)2

∥^gt∥2
2
t+1(4(cid:22)2
where the last inequality is due to Lemma 3.3. By (4)-(7)  one can arrive at

t+1(∥ ~∇h(xt)∥2

2) (cid:20) 1 + 2(cid:11)2

(cid:20) 1 + 2(cid:11)2

2 + ∥(cid:24)t∥2

t+1

and
∥xt (cid:0) (cid:11)t+1^gt∥2

v)1=2 = (v

⊤

(I (cid:0) xtx

⊤
t )v)1=2

1 sin2 (cid:18)t + ∥(cid:24)t∥2
2);

 (xt+1; v) (cid:20) (xt; v) (cid:0) 2 log(1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t) (cid:0) log(1 (cid:0) 2(cid:11)t+1∥(cid:24)t∥2 tan (cid:18)(xt; v)
Taking the minimum with respect to v over Vp;1 on both sides and noting that ∥(cid:24)t∥2 (cid:20) p

1 sin2 (cid:18)t + ∥(cid:24)t∥2

1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t

+ log(1 + 2(cid:11)2

t+1(4(cid:22)2

2)):

2(cid:22)1ϵt by

Lemma 3.5  we then get

 t+1 (cid:20) t (cid:0) 2 log(1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t) (cid:0) log(1 (cid:0) 2(cid:11)t+1

p

2(cid:22)1ϵt tan (cid:18)t
1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t

)

(6)

(7)

)

+ log(1 + 2(cid:11)2

t+1(4(cid:22)2

1 sin2 (cid:18)t + 2(cid:22)1ϵt)):

Letting ϵt =

(cid:28) 2
p
(cid:22)1

sin2(2(cid:18)t)

32

  the above inequality can be reduced:

 t+1 (cid:20) t (cid:0) 2 log(1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t) (cid:0) log(1 (cid:0) (cid:11)t+1(cid:28)p sin2 (cid:18)t
1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t

)

(cid:28) 2
p
4

+ log(1 + 2(cid:11)2

t+1(4(cid:22)2

1 sin2 (cid:18)t +

sin2 (cid:18)t cos2 (cid:18)t))

(cid:20) t (cid:0) log(1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t) + log(1 + 10(cid:11)2
(cid:20) t (cid:0) (cid:11)t+1(cid:28)p sin2 (cid:18)t
1 sin2 (cid:18)t)
1 + (cid:11)t+1(cid:28)p sin2 (cid:18)t
(cid:20) t (cid:0) (cid:11)t+1(
(cid:0) 10(cid:11)t+1(cid:22)2

t+1(cid:22)2
+ 10(cid:11)2
(cid:0) 10(cid:11)t+1(cid:22)2

1 > 0  i.e.  (cid:11)t+1 <

1) sin2 (cid:18)t:

1 + (cid:11)t+1(cid:28)p

(cid:28)p

(cid:28)p

t+1(cid:22)2

1 sin2 (cid:18)t)

(by Lemma 3.4)

1(1+(cid:11)t+1(cid:28)p)  we then get t+1 < t and

20(cid:22)2

(cid:28)p

Thus  if
 t+1 (cid:20) t (cid:0) (cid:11)t+1(cid:28)p sin2 (cid:18)t

2(1+(cid:11)t+1(cid:28)p)

2(1+(cid:11)t+1(cid:28)p) . Note that

sin2 (cid:18)t

sin2 (cid:18)t =

(cid:0) log(1 (cid:0) sin2 (cid:18)t)

 t
1 + t
where the ﬁrst inequality is by Lemma 3.4. If (cid:11)t (cid:17) (cid:11)  we then can arrive at
(cid:1)

1 (cid:0) log(1 (cid:0) sin2 (cid:18)t)

 T (cid:20) (1 (cid:0)

(cid:11)(cid:28)p

(cid:11)(cid:28)p

 t

=

1

(cid:1)

(cid:1) t (cid:21)

2(1 + (cid:11)(cid:28)p)

(cid:21) t

1 + 0

;

1

1 + 0

)T 0

(cid:20) expf(cid:0)T

2(1 + (cid:11)(cid:28)p)
(cid:11)(cid:28)p

1 + 0
(cid:1)
1

) T(cid:0)1 (cid:20) (1 (cid:0)
g 0   (cid:4):

 0
ϵ

) = O((

(cid:22)1
(cid:28)p

)2 log

 0
ϵ

) = O(log

 0
ϵ

):

√

Setting (cid:4) = ϵ and noting (cid:11) <

2(1 + (cid:11)(cid:28)p)(1 + 0)

T =

(cid:28)p

2(1 + (cid:11)(cid:28)p)

1 + 0
1(1+(cid:11)(cid:28)p) yields
20(cid:22)2
 0
ϵ

1
(cid:11)(cid:28)p

= O(

log

log

(cid:11)(cid:28)p

√
√

On the other hand  by Lemma 3.5  the complexity for computing yt is
(cid:22)2
1
(cid:22)n
(cid:28) 2
p
(cid:22)1

lt(z0) (cid:0) lt(Bxt(cid:0)1)
√

(cid:21)1
∆p

(cid:21)1
∆p

) = O(

log

log

O(

ϵt

sin2 (cid:18)t
sin2(2(cid:18)t)

√

32

= O(

(cid:21)1
∆p

(log

(cid:22)1
(cid:22)n

√

(cid:21)1
∆p

(log

(cid:22)1
(cid:22)n

√

+ t)) = O(

+ 0)) = O(

Thus  the overall complexity is O(

(cid:21)1
∆p

log (cid:21)1
∆p

log 0

ϵ ) = ~O(

(cid:21)1
∆p

).

6

)

(cid:21)1
∆p

log

(cid:21)1
∆p

);

(cid:3)

Relative function error

Potential function sin2 (cid:18)t

Figure 1: Synthetic data.

4 Experiments

We test our algorithm on both synthetic and real data. Throughout experiments  our SI-rgEIGS
solver is warm-started by a few power iterations  and four iterations of Nesterov’s AGD are run to
approximately solve the least-squares subproblems. The same initial x0 is used for different solvers.
All the algorithms are implemented in matlab and running single threaded. All the ground-truth
information is obtained by matlab’s eigs function for benchmarking purpose. The implementation
of our algorithm is available at DJJFICEJDK>?DEGE=CNK 51HC-1/5.

4.1 Synthetic Data

We follow Shamir [2015] to generate synthetic data. Note that A’s full eigenvalue decomposition
⊤
n   where (cid:6) is diagonal. Thus  it sufﬁces to generate random orthog-
can be written as A = Vn(cid:6)V
onal matrix Vn and set (cid:6) = diag(1; 1 (cid:0) ∆; 1 (cid:0) 1:1∆;(cid:1)(cid:1)(cid:1) ; 1 (cid:0) 1:4∆; g1=n;(cid:1)(cid:1)(cid:1) ; gn(cid:0)6=n) with gi
being standard normal samples  i.e.  gi (cid:24) N (0; 1). Here we set n = 1000 and (cid:27) = 1:005  and three
solvers are compared: Rimennian gradient descent solver with/without shift-and-invert precondi-
tioning under the constant step-size setting  and the shift-and-inverted power method [Garber et al. 
2016]. Constant step-sizes are hand-tuned. Figure 1 reports the performance of three algorithms 
in terms of relative function error (f (xt) (cid:0) f (v1))=f (v1) or the potential sin2 (cid:18)t  where we use
f (x). We see that Riemannian gradient de-
f (x) = x
scent with shift-and-invert preconditioning indeed outperforms the counterpart without precondition-
ing which is also worse than the SI-PM. This demonstrates the effectiveness of the shift-and-invert
preconditioning for acceleration again. Second  unexpectedly  SI-rgEIGS runs faster than SI-PM 
despite an extra log factor in theory. This may hint at the possibility of removing this factor in
analysis of our method. Last  note that convergence behaviors are consistent in terms of two quality
measures.

Ax and then f (v1) = (cid:21)1 =

⊤

max

x2Rn(cid:2)1:∥x∥2=1

4.2 Real Data

We now demonstrate the performance of Algorithm 1 on real data from the sparse matrix collection 
and also compare with the accelerated power method with optimal momentum (cid:12) = (cid:21)2
2=4 (abbrevi-
ated as APM-OM) [Sa et al.  2017]. However  two issues need to be ﬁxed. First  the crude phase of
Garber and Hazan [2015]  Wang et al. [2017] for locating the shift parameter is hard to use as there

7

00.10.20.30.40.50.60.70.8time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1) = 5 10-3rgEIGS =1.84rgEIGS =1.92rgEIGS =2.00SI-rgEIGS =0.012SI-rgEIGS =0.016SI-rgEIGS =0.020SI-PM00.10.20.30.40.50.60.70.8time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1) = 5 10-3rgEIGS =1.84rgEIGS =1.92rgEIGS =2.00SI-rgEIGS =0.012SI-rgEIGS =0.016SI-rgEIGS =0.020SI-PM00.511.522.533.544.5time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1) = 5 10-4rgEIGS =1.37rgEIGS =1.99rgEIGS =2.00SI-rgEIGS =0.011SI-rgEIGS =0.015SI-rgEIGS =0.019SI-PM00.511.522.533.544.5time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1) = 5 10-4rgEIGS =1.37rgEIGS =1.99rgEIGS =2.00SI-rgEIGS =0.011SI-rgEIGS =0.015SI-rgEIGS =0.019SI-PMFigure 2: Real data.

AxTc + (cid:12)∥ ~∇f (xTc)∥2

are three parameters that need to be tuned. We use heuristics based on Lemma 3.3. The lemma
shows that (cid:21)1 (cid:20) x
t Axt + ((cid:21)1 (cid:0) (cid:21)n) sin2 (cid:18)t and ∥ ~∇f (xt)∥2 (cid:20) 2(cid:21)1 sin (cid:18)t. Then we have the upper
⊤
⊤
2 and wart-start xTc from the
bound on (cid:21)1: (cid:27) = x
crude phrase. We ﬁnd that setting (cid:12) = 1=∥ ~∇f (xTc)∥2 works well on our data. Second  hand-tuning
Tc
of step-sizes  even for constant step-sizes  is a difﬁcult task. We thus use an automatic step-size
scheme  speciﬁcally  Barzilai-Borwein (BB) step-size  which is a non-monotone step-size scheme
and performs well in practice [Wen and Yin  2013]. In our context  it is set as follows:
(^gt (cid:0) ^gt(cid:0)1)j

2 for proper constant (cid:12) > 1

∥xt (cid:0) xt(cid:0)1∥2

j(xt (cid:0) xt(cid:0)1)

⊤

(cid:11)t+1 =

j(xt (cid:0) xt(cid:0)1)⊤(^gt (cid:0) ^gt(cid:0)1)j ; or (cid:11)t+1 =

2

∥^gt (cid:0) ^gt(cid:0)1∥2

2

:

Note that we use inexact Riemannian gradients ^gt here  instead of exact ones ~∇h(x) as in the
traditional case. Nonetheless  it still performs well and signiﬁcantly better than the shift-and-inverted
power method as observed in Figure 2. See the supplementary material for the description of the
real data.

5 Discussions

√

In this work  we investigated Riemannian gradient descent with shift-and-invert preconditioning
for the leading eigenvector computation on the effect of step-size schemes  in comparison to the
recently popular shift-and-inverted power method. Speciﬁcally  the constant step-size scheme and
the Barzilai-Borwein (BB) step-size scheme were considered theoretically and/or empirically. The
algorithm was theoretically analyzed under the constant step-size setting and shown for the ﬁrst
time to able to achieve a rate of the type ~O(
) and a logarithmic dependence on the initial
iterate. It is a nearly biquadratic improvement for the gradient descent solver  covering both ∆1 > 0
and ∆1 = 0. Experimental results demonstrated that the shift-and-invert preconditioning can indeed
accelerate gradient descent solver. Unexpectedly  the adaptive step-size setting with the shift-and-
inverted power method is outperformed by the considered step-size settings  especially the BB step-
size scheme on real data  albeit with a provable optimal rate. For future work  we may further
investigate if the log factor log
can be removed from the overall complexity and test our
algorithms with other least-squares solvers for deeper understanding of its performance.

(cid:21)1(cid:0)(cid:21)p+1

(cid:21)1(cid:0)(cid:21)p+1

(cid:21)1

(cid:21)1

8

00.020.040.060.080.10.12time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)hangGlider5SI-rgEIGSSI-PMAPM-OM00.020.040.060.080.10.120.14time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1)hangGlider5SI-rgEIGSSI-PMAPM-OM00.20.40.60.811.21.4time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)Boeing35SI-rgEIGSSI-PMAPM-OM00.511.5time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1)Boeing35SI-rgEIGSSI-PMAPM-OM0123456time (seconds)10-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)indef_dSI-rgEIGSSI-PMAPM-OM0123456time (seconds)10-610-510-410-310-210-1100sin2(xt   v1)indef_dSI-rgEIGSSI-PMAPM-OM00.511.522.533.54time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)indef_aSI-rgEIGSSI-PMAPM-OM00.511.522.533.54time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1)indef_aSI-rgEIGSSI-PMAPM-OM00.050.10.150.20.250.30.350.4time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)dimacs10_ctSI-rgEIGSSI-PMAPM-OM00.050.10.150.20.250.30.350.4time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1)dimacs10_ctSI-rgEIGSSI-PMAPM-OM02468101214time (seconds)10-1410-1210-1010-810-610-410-2100( f(xt) - f(v1) ) / f(v1)dimacs10_nvSI-rgEIGSSI-PMAPM-OM024681012141618time (seconds)10-1410-1210-1010-810-610-410-2100sin2(xt   v1)dimacs10_nvSI-rgEIGSSI-PMAPM-OMAcknowledgments

Authors would like to thank the reviewers  AC  and SAC for their valuable comments.

References
P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press  2008.

Zeyuan Allen-Zhu and Yuanzhi Li. Even faster svd decomposition yet without agonizing pain. In

Advances in Neural Information Processing Systems  pages 974–982  2016.

Raman Arora  Andrew Cotter  Karen Livescu  and Nathan Srebro.

Stochastic optimiza-
In 50th Annual Allerton Conference on Communication  Con-
tion for PCA and PLS.
trol  and Computing  Allerton 2012  Allerton Park & Retreat Center  Monticello  IL  USA 
October 1-5  2012  pages 861–868  2012.
doi: 10.1109/Allerton.2012.6483308. URL
DJJFI@EHC')AHJ  $"&!!&.

Raman Arora  Andrew Cotter  and Nati Srebro. Stochastic optimization of PCA with capped MSG.
In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural
Information Processing Systems 2013. Proceedings of a meeting held December 5-8  2013  Lake
Tahoe  Nevada  United States.  pages 1815–1823  2013.

Maria-Florina Balcan  Simon Shaolei Du  Yining Wang  and Adams Wei Yu. An improved gap-
In Proceedings of the 29th Conference on
dependency analysis of the noisy power method.
Learning Theory  COLT 2016  New York  USA  June 23-26  2016  pages 284–309  2016. URL
DJJFHHCFH?AA@ECIF=FAHIL"'>=?=$=DJ.

Akshay Balsubramani  Sanjoy Dasgupta  and Yoav Freund. The fast convergence of incremental pca.
In C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q. Weinberger  editors  Advances
in Neural Information Processing Systems 26  pages 3174–3182. Curran Associates  Inc.  2013.

Jianqing Fan  Qiang Sun  Wen-Xin Zhou  and Ziwei Zhu. Principal component analysis for big data.

arXiv preprint arXiv:1801.01602  2018.

Chao Gao  Dan Garber  Nathan Srebro  Jialei Wang  and Weiran Wang. Stochastic canonical corre-

lation analysis. CoRR  abs/1702.06533  2017. URL DJJF=HNELHC=>I% $#!!.

Dan Garber and Elad Hazan. Fast and simple pca via convex optimization.

arXiv:1509.05647  2015.

arXiv preprint

Dan Garber  Elad Hazan  Chi Jin  Sham M. Kakade  Cameron Musco  Praneeth Netrapalli  and
Aaron Sidford. Faster eigenvector computation via shift-and-invert preconditioning. In Interna-
tional Conference on Machine Learning  pages 2626–2634  2016.

Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins University

Press  Baltimore  MD  USA  1996. ISBN 0-8018-5414-8.

Nathan Halko  Per-Gunnar Martinsson  and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review  53
(2):217–288  2011. doi: 10.1137/090771806.

Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In

Advances in Neural Information Processing Systems  pages 2861–2869  2014.

Trevor Hastie  Rahul Mazumder  Jason D. Lee  and Reza Zadeh. Matrix completion and low-rank
SVD via fast alternating least squares. Journal of Machine Learning Research  16:3367–3402 
2015. URL DJJF@=?HC?EJ=JE?BE@ ' $.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8 
2013  Lake Tahoe  Nevada  United States.  pages 315–323  2013.

9

Qi Lei  Kai Zhong  and Inderjit S. Dhillon. Coordinate-wise power method.

In Advances in
Neural Information Processing Systems 29: Annual Conference on Neural Information Process-
ing Systems 2016  December 5-10  2016  Barcelona  Spain  pages 2056–2064  2016. URL
DJJFF=FAHIEFI??F=FAH$!?H@E=JAMEIAFMAHAJD@.

Guangcan Liu and Ping Li. Recovery of coherent data via low-rank dictionary pursuit.

In Ad-
vances in Neural Information Processing Systems 27: Annual Conference on Neural Information
Processing Systems 2014  December 8-13 2014  Montreal  Quebec  Canada  pages 1206–1214 
2014.

Huikang Liu  Weijie Wu  and Anthony Man-Cho So. Quadratic optimization with orthogonality
In

constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods.
ICML  pages 1158–1167  2016.

Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. In Advances in Neural Information Processing Sys-
tems  pages 1396–1404  2015.

Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing

Company  Incorporated  1 edition  2014. ISBN 1461346916  9781461346913.

I. Jordan  and Yair Weiss.

Andrew Y. Ng  Michael
ysis and an algorithm.
[Neural
ber 3-8  2001  Vancouver  British Columbia  Canada]  pages 849–856  2001.
DJJFMMM ?I?KA@K/HKFI125125 F=FAHIFIC))!#FIC.

On spectral clustering: Anal-
Information Processing Systems 14
Information Processing Systems: Natural and Synthetic  NIPS 2001  Decem-
URL

In Advances in Neural

Christopher De Sa  Bryan D. He 
Accelerated stochastic power

Xu.
DJJF=HNELHC=>I%% $%.

Ioannis Mitliagkas  Christopher Ré 
iteration.

CoRR  abs/1707.02670  2017.

and Peng
URL

Ohad Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In Pro-
ceedings of the 32nd International Conference on Machine Learning  ICML 2015  Lille  France 
6-11 July 2015  pages 144–152  2015.

Ohad Shamir. Fast stochastic algorithms for SVD and PCA: convergence properties and convexity.

In International Conference on Machine Learning  pages 248–256  2016a.

Ohad Shamir. Convergence of stochastic gradient descent for PCA. In ICML  pages 257–265  2016b.

Jialei Wang  Weiran Wang  Dan Garber 
computation.

wise
DJJF=HNELHC=>I% %&!".

leading eigenvector

and Nathan Srebro.

Efﬁcient coordinate-
URL

2017.

CoRR 

abs/1702.07834 

Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints.
Mathematical Programming  142(1-2):397–434  2013. doi: 10.1007/s10107-012-0584-1. URL
DJJF@N@EHC%I% #&".

Zhiqiang Xu and Xin Gao. On truly block eigensolvers via riemannian optimization.

In In-
ternational Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2018  9-11 April
2018  Playa Blanca  Lanzarote  Canary Islands  Spain  pages 168–177  2018.
URL
DJJFFH?AA@ECIHFHAIIL&"NK&>DJ.

Zhiqiang Xu  Yiping Ke  and Xin Gao. A fast stochastic riemannian eigensolver. In UAI  2017.

Zhiqiang Xu  Xin Cao  and Xin Gao. Convergence analysis of gradient descent for eigen-
the Twenty-Seventh International Joint Confer-
vector computation.
ence on Artiﬁcial Intelligence  IJCAI-18  pages 2933–2939. International Joint Conferences
on Artiﬁcial Intelligence Organization  7 2018.
URL
DJJFI@EHC "'$!E?=E &"%.

10.24963/ijcai.2018/407.

In Proceedings of

doi:

10

,Falk Lieder
Dillon Plunkett
Jessica Hamrick
Stuart Russell
Nicholas Hay
Tom Griffiths
Andrew Miller
Albert Wu
Jeff Regier
Mr. Prabhat
Ryan Adams
Rong Ge
Jason Lee
Tengyu Ma
Zhiqiang Xu