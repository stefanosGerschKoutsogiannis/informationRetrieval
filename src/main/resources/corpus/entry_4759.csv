2008,Robust Kernel Principal Component Analysis,Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA  data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function  and linear PCA in the feature space is performed via the kernel trick. However  due to the implicitness of the feature space  some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem  and extends it to a unified framework for treating noise  missing data  and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments  in both synthetic and real data  show that our algorithm outperforms existing methods.,Robust Kernel Principal Component Analysis

Minh Hoai Nguyen & Fernando De la Torre

Carnegie Mellon University  Pittsburgh  PA 15213  USA.

Abstract

Kernel Principal Component Analysis (KPCA) is a popular generalization of lin-
ear PCA that allows non-linear feature extraction. In KPCA  data in the input
space is mapped to higher (usually) dimensional feature space where the data can
be linearly modeled. The feature space is typically induced implicitly by a kernel
function  and linear PCA in the feature space is performed via the kernel trick.
However  due to the implicitness of the feature space  some extensions of PCA
such as robust PCA cannot be directly generalized to KPCA. This paper presents
a technique to overcome this problem  and extends it to a uniﬁed framework for
treating noise  missing data  and outliers in KPCA. Our method is based on a novel
cost function to perform inference in KPCA. Extensive experiments  in both syn-
thetic and real data  show that our algorithm outperforms existing methods.

1 Introduction
Principal Component Analysis (PCA) [9] is one of the primary statistical techniques for feature
extraction and data modeling. One drawback of PCA is its limited ability to model non-linear
structures that exist in many computing applications. Kernel methods [18] enable us to extend PCA
to model non-linearities while retaining its computational efﬁciency.
In particular  Kernel PCA
(KPCA) [19] has repeatedly outperformed PCA in many image modeling tasks [19  14].

Unfortunately  realistic visual data is often corrupted by undesirable artifacts due to occlusion (e.g.
a hand in front of a face  Fig. 1.d)  illumination (e.g. specular refection  Fig. 1.e)  noise (e.g. from
capturing device  Fig.1.b)  or from the underlying data generation method (e.g. missing data due
to transmission  Fig. 1.c). Therefore  robustness to noise  missing data  and outliers is a desired
property to have for algorithms in computer vision.

a

b

f

c

g

d

h

e

i

x

z?

Principal 
Subspace

Figure 1: Several types of data corruption and
results of our method. a) original image  b) cor-
ruption by additive Gaussian noise  c) missing
data  d) hand occlusion  e) specular reﬂection.
f) to i) are the results of our method for recover-
ing uncorrupted data from b) to e) respectively.

Input Space

Feature Space

Figure 2: Using KPCA principal subspace to
ﬁnd z  a clean version of corrupted sample x.

Throughout the years  several extensions of PCA have been proposed to address the problems of
outliers and missing data  see [6] for a review. However  it still remains unclear how to generalize
those extensions to KPCA; since directly migrating robust PCA techniques to KPCA is not possible

1

due to the implicitness of the feature space. To overcome this problem  in this paper  we propose
Robust KPCA (RKPCA)  a uniﬁed framework for denoising images  recovering missing data  and
handling intra-sample outliers. Robust computation in RKPCA does not suffer from the implicit-
ness of the feature space because of a novel cost function for reconstructing “clean” images from
corrupted data. The proposed cost function is composed of two terms  requiring the reconstructed
image to be close to the KPCA principal subspace as well as to the input sample. We show that
robustness can be naturally achieved by using robust functions to measure the closeness between the
reconstructed and the input data.

2 Previous work
2.1 KPCA and pre-image
KPCA [19  18  20] is a non-linear extension of principal component analysis (PCA) using kernel
methods. The kernel represents an implicit mapping of the data to a (usually) higher dimensional
space where linear PCA is performed.

Let X denote the input space and H the feature space. The mapping function ϕ : X → H is
implicitly induced by a kernel function k : X × X → ℜ that deﬁnes the similarity between data in
the input space. One can show that if k(·  ·) is a kernel then the function ϕ(·) and the feature space
H exist; furthermore k(x  y) = hϕ(x)  ϕ(y)i [18].
However  directly performing linear PCA in the feature space might not be feasible because the
feature space typically has very high dimensionality (including inﬁnity). Thus KPCA is often done
via the kernel trick. Let D = [d1 d2 ... dn]  see notation1  be a training data matrix  such that
di ∈ X ∀i = 1  n. Let k(·  ·) denote a kernel function  and K denote the kernel matrix (element
ij of K is kij = k(di  dj)). KPCA is computed in closed form by ﬁnding ﬁrst m eigenvectors
(ai’s) corresponding to the largest eigenvalues (λi’s) of the kernel matrix K (i.e. KA = AΛ). The
eigenvectors in the feature space V can be computed as V = ΓA  where Γ = [ϕ(d1)...ϕ(dn)]. To
ensure orthonormality of {vi}m
i=1  KPCA imposes that λihai  aii = 1. It can be shown that {vi}m
i=1
form an orthonormal basis of size m that best preserves the variance of data in the feature space [19].
Assume x is a data point in the input space  and let Pϕ(x) denote the projection of ϕ(x) onto
the principal subspace {vi}m
1 is a set of orthonormal vectors  we have Pϕ(x) =
Pm

i=1 hϕ(x)  vii vi. The reconstruction error (in feature space) is given by:

1 . Because {vi}m

Eproj(x) = ||ϕ(x) − Pϕ(x)||2
where r(x) = ΓT ϕ(x)  and M = X aiaT

2 = hϕ(x)  ϕ(x)i − X hϕ(x)  vii2 = k(x  x) − r(x)T Mr(x) 
(1)

.

i

The pre-image of the projection is the z ∈ X that satisﬁes ϕ(z) = Pϕ(x); z is also referred to as
the KPCA reconstruction of x. However  the pre-image of Pϕ(x) usually does not exist  so ﬁnding
the KPCA reconstruction of x means ﬁnding z such that ϕ(z) is as close to Pϕ(x) as possible.
It should be noted that the closeness between ϕ(z) and Pϕ(x) can be deﬁned in many ways  and
different cost functions lead to different optimization problems. Sch¨olkopf et al [17] and Mika et
al [13] propose to approximate the reconstruction of x by arg minz ||ϕ(z) − Pϕ(x)||2
2. Two other
objective functions have been proposed by Kwok & Tsang [10] and Bakir etal [2].

2.2 KPCA-based algorithms for dealing with noise  outliers and missing data
Over the years  several methods extending KPCA algorithms to deal with noise  outliers  or missing
data have been proposed. Mika et al [13]  Kwok & Tsang [10]  and Bakir et al [2] show how
denoising can be achieved by using the pre-image. While these papers present promising denoising
results for handwritten digits  there are at least two problems with these approaches. Firstly  because
the input image x is noisy  the similarity measurement between x and other data point di (i.e.
k(x  di) the kernel) might be adversely affected  biasing the KPCA reconstruction of x. Secondly 

1Bold uppercase letters denote matrices (e.g. D)  bold lowercase letters denote column vectors (e.g. d). dj
represents the jth column of the matrix D. dij denotes the scalar in the row ith and column jth of the matrix
D and the ith element of the column vector dj. Non-bold letters represent scalar variables. 1k ∈ Rk×1 is a
column vector of ones. Ik ∈ Rk×k is the identity matrix.

2

x

z?

Principal 
Subspace

x

z?

Principal 
Subspace

Input Space

a

Feature Space

Input Space

Feature Space

b

Figure 3: Key difference between previous work (a) and ours (b). In (a)  one seeks z such that ϕ(z)
is close to Pϕ(x). In (b)  we seek z such that ϕ(z) is close to both ϕ(x) and the principal subspace.

current KPCA reconstruction methods equally weigh all the features (i.e. pixels); it is impossible to
weigh the importance of some features over the others.

Other existing methods also have limitations. Some [7  22  1] only consider robustness of the princi-
pal subspace; they do not address robust ﬁtting. Lu etal [12] present an iterative approach to handle
outliers in training data. At each iteration  the KPCA model is built  and the data points that have the
highest reconstruction errors are regarded as outliers and discarded from the training set. However 
this approach does not handle intra-sample outliers (outliers that occur at a pixel level [6]). Several
other approaches also considering Berar etal [3] propose to use KPCA with polynomial kernels to
handle missing data. However  it is not clear how to extend this approach to other kernels. Further-
more  with polynomial kernels of high degree  the objective function is hard to optimize. Sanguinetti
& Lawrence [16] propose an elegant framework to handle missing data. The framework is based on
the probabilistic interpretation inherited from Probabilistic PCA [15  21  11]. However  Sanguinetti
& Lawrence [16] do not address the problem of outliers.

This paper presents a novel cost function that uniﬁes the treatment of noise  missing data and outliers
in KPCA. Experiments show that our algorithm outperforms existing approaches [6  10  13  16].

3 Robust KPCA
3.1 KPCA reconstruction revisited
Given an image x ∈ X   Fig. 2 describes the task of ﬁnding the KPCA-reconstructed image of x
(uncorrupted version of x to which we will refer as KPCA reconstruction). Mathematically  the task
is to ﬁnd a point z ∈ X such that ϕ(z) is in the principal subspace (denote PS) and ϕ(z) is as close
to ϕ(x) as possible. In other words  ﬁnding the KPCA reconstruction of x is to optimize:

arg min

z

||ϕ(z) − ϕ(x)||2 s.t. ϕ(z) ∈ PS .

(2)

However  since there might not exist z ∈ X such that ϕ(z) ∈ PS  the above optimization problem
needs to be relaxed. There is a common relaxation approach used by existing methods for computing
the KPCA reconstruction of x. This approach conceptually involves two steps:(i) ﬁnding Pϕ(x)
which is the closest point to ϕ(x) among all the points in the principal subspace  (ii) ﬁnding z
such that ϕ(z) is as close to Pϕ(x) as possible. This relaxation is depicted in Fig. 3a. If L2 norm
is used to measure the closeness between ϕ(z) and Pϕ(x)  the resulting KPCA reconstruction is
arg minz ||ϕ(z) − Pϕ(x)||2
2 .
This approach for KPCA reconstruction is not robust. For example  if x is corrupted with intra-
sample outliers (e.g. occlusion)  ϕ(x) and Pϕ(x) will also be adversely affected. As a conse-
quence  ﬁnding z that minimizes ||ϕ(z) − Pϕ(x)||2
2 does not always produce a “clean” version of
x. Furthermore  it is unclear how to incorporate robustness to the above formulation.

Here  we propose a novel relaxation of (2) that enables the incorporation of robustness. The KPCA
reconstruction of x is taken as:

arg min

z

||ϕ(x) − ϕ(z)||2

2 + C ||ϕ(z) − Pϕ(z)||2
2
}

Eproj (z)

{z

|

3

.

(3)

Algorithm 1 RKPCA for missing attribute values in training data

Input: training data D  number of iterations m  number of partitions k.
Initialize: missing values by the means of known values.
for iter = 1 to m do

Randomly divide D into k equal partitions D1  ...  Dk
for i = 1 to k do

Train RKPCA using data D \ Di
Run RKPCA ﬁtting for Di with known missing attributes.

end for
Update missing values of D

end for

Intuitively  the above cost function requires the KPCA reconstruction of x is a point z that ϕ(z)
is close to both ϕ(x) and the principal subspace. C is a user-deﬁned parameter that controls the
relative importance of these two terms. This approach is depicted in Fig. 3b.

It is possible to generalize the above cost function further. The ﬁrst term of Eq. 3 is not necessarily
2 is replaced
||ϕ(x) − ϕ(z)||2
by a robust function E0 : X × X → ℜ for measuring similarity between x and z. Furthermore 
there is no reason why E0 should be restricted to the metric of the feature space. In short  the KPCA
reconstruction of x can be taken as:

2. In fact  for the sake of robustness  it is preferable that ||ϕ(x) − ϕ(z)||2

arg min

z

E0(x  z) + CEproj(z) .

(4)

By choosing appropriate forms for E0  one can use KPCA to handle noise  missing data  and intra-
sample outliers. We will show that in the following sections.

3.2 Dealing with missing data in testing samples
Assume the KPCA has been learned from complete and noise free data. Given a new image x
with missing values  a logical function E0 that does not depend on the the missing values could be:
E0(x  z) = −exp(−γ2||W(x − z)||2
2)  where W is a diagonal matrix; the elements of its diagonal
are 0 or 1 depending on whether the corresponding attributes of x have missing values or not.

3.3 Dealing with intra-sample outliers in testing samples
To handle intra-sample outliers  we could use a robust function for E0. For instance: E0(x  z) =
−exp(−γ2 Pd
y2+σ2  
and σ is a parameter of the function. This function is also used in [6] for Robust PCA.

i=1 ρ(xi − zi  σ))  where ρ(·  ·) is the Geman-McClure function  ρ(y  σ) = y2

3.4 Dealing with missing data and intra-sample outliers in training data
Previous sections have shown how to deal with outliers and missing data in the testing set (assuming
KPCA has been learned from a clean training set). If we have missing data in the training samples
[6]  a simple approach is to iteratively alternate between estimating the missing values and updat-
ing the KPCA principal subspace until convergence. Algorithm 1 outlines the main steps of this
approach. An algorithm for handling intra-sample outliers in training data could be constructed
similarly.
Alternatively  a kernel matrix could be computed ignoring the missing values  that is  each kij =
exp(−γ2||WiWj(xi − xj)||2
2)  where γ2 =
trace(WiWj) . However  the positive deﬁniteness of the
resulting kernel matrix cannot be guaranteed.

1

3.5 Optimization
In general  the objective function in Eq. 4 is not concave  hence non-convex optimization tech-
niques are required. In this section  we restrict our attention to the Gaussian kernel (k(x  y) =
exp(−γ||x − y||2)) that is the most widely used kernel. If E0 takes the form of Sec.3.2  we need to
(5)

 

maximize E(z) = exp(−γ2||W(x − z)||2)
}

{z

E1(z)

|

+C ∗ r(z)T Mr(z)
}

{z

E2(z)

|

4

where r(·)  and M are deﬁned in Eq.1. Note that optimizing this function is not harder than op-
timizing the objective function used by Mika et al [13]. Here  we also derive a ﬁxed-point opti-
mization algorithm. The necessary condition for a minimum has to satisfy the following equation:
∇zE(z) = ∇zE1(z) + ∇zE2(z) = 0 . The expression for the gradients are given by:

∇zE1(z) = −2γ2 exp(−γ2||W(x − z)||2)W2
(z − x)  ∇zE2(z) = −4γ[(1T
}

{z

W2

|

n Q1n)z − DQ1n]  

where Q is a matrix such that qij = mijexp(−γ||z − di||2 − γ||z − dj||2). A ﬁxed-point update is:

γ2
γ

1
z = [
2C
|

W2 + (1T

1
]−1(
n Q1n)In
2C
|
}

γ2
γ

W2x + DQ1n
)
}

{z

u

(6)

{z

W3

i=1(vT

i ϕ(z))2  so 1T

3 u. The diagonal matrix W−1
3

The above equation is the update rule for z at every iteration of the algorithm. The algorithm stops
when the difference between two successive z’s is smaller than a threshold.
Note that W3 is a diagonal matrix with non-negative entries since Q is a positive semi-deﬁnite
matrix. Therefore  W3 is not invertible only if there are some zero elements on the diagonal. This
only happens if some elements of the diagonal of W are 0 and 1T
n Q1n = 0. It can be shown that
n Q1n = Pm
n Q1n = 0 when ϕ(z)⊥vi  ∀i. However  this rarely occurs in
1T
practice; moreover  if this happens we can restart the algorithm from a different initial point.
Consider the update rule given in Eq.6: z = W−1
acts as a normal-
ization factor of u. Vector u is a weighted combination of two terms  the training data D and x.
Furthermore  each element of x is weighted differently by W2 which is proportional to W. In the
case of missing data (some entries in the diagonal of W  and therefore W2  will be zero)  missing
components of x would not affect the computation of u and z. Entries corresponding to the missing
components of the resulting z will be pixel-weighted combinations of the training data. The contri-
bution of x also depends on the ratio γ2/γ  C  and the distance from the current z to x. Similar to
the observation of Mika et al [13]  the second term of vector u pulls z towards a single Gaussian
cluster. The attraction force generated by a training data point di reﬂects the correlation between
ϕ(z) and ϕ(di)  the correlation between ϕ(z) and eigenvectors vj’s  and the contributions of ϕ(di)
to the eigenvectors. The forces from the training data  together with the attraction force by x  draw
z towards a Gaussian cluster that is close to x.
γ2
One can derive a similar update rule for z if E0 takes the form in Sec.3.3. z = [ 1
γ W4 +
2C
5  where W5
(1T
is a diagonal matrix; the ith entry of the diagonal is σ/((zi − xi)2 + σ2). The parameter σ is updated
at every iteration as follows: σ = 1.4826 × median({|zi − xi|}d

γ W4x + DQ1n)  with W4 = exp(−γ2 Pd

i=1 ρ(xi − zi  σ))W2

n Q1n)In]−1( 1
2C

γ2

i=1) [5].

4 Experiments
4.1 RKPCA for intra-sample outliers
In this section  we compare RKPCA with three approaches for handling intra-sample outliers: (i)
Robust Linear PCA [6]  (ii) Mika etal’s KPCA reconstruction [13]  and (iii) Kwok & Tsang’s KPCA
reconstruction [10]. The experiments are done on the CMU Multi-PIE database [8].

The Multi-PIE database consists of facial images of 337 subjects taken under different illuminations 
expressions and poses  at four different sessions. We only make use of the directly-illuminated
frontal face images under ﬁve different expressions (smile  disgust  squint  surprise and scream) 
see Fig. 4. Our dataset contains 1100 images  700 are randomly selected for training  100 are used
for validation  and the rest is reserved for testing. Note that no subject in the testing set appears
in the training set. Each face is manually labeled with 68 landmarks  as shown in Fig. 4a. A
shape-normalized face is generated for every face by warping it towards the mean shape using afﬁne
transformation. Fig. 4b shows an example of such a shape-normalized face. The mean shape is used
as the face mask and the values inside the mask are vectorized.

To quantitatively compare different methods  we introduce synthetic occlusions of different sizes
(20  30  and 40 pixels) into the test images. For each occlusion size and test image pair  we generate

5

Occ.Sz Region Type Base Line Mika et al Kwok&Tsang Robust PCA Ours

20

30

40

20

30

40

%
0
8

y
g
r
e
n
E

%
5
9

y
g
r
e
n
E

Whole face
Occ. Reg.

14.0±5.5 13.5±3.3
71.5±5.5 22.6±7.9
Non-occ Reg. 0.0±0.0 11.3±2.3
Whole face 27.7±10.2 17.5±4.8
Occ. Reg.
70.4±3.9 24.2±7.1
Non-occ Reg. 0.0±0.0 13.3±3.0
Whole face 40.2±12.7 20.9±5.9
Occ. Reg.
70.6±3.6 25.7±7.2
Non-occ Reg. 0.0±0.0 15.2±4.2
14.2±5.3 12.6±3.1
Whole face
71.2±5.4 29.2±8.4
Occ. Reg.
8.6±1.6
26.8±9.5 17.4±4.4
70.9±4.4 30.0±7.6
Non-occ Reg. 0.0±0.0 10.1±1.9
Whole face 40.0±11.9 22.0±5.9
Occ. Reg.
70.7±3.6 30.1±7.2
Non-occ Reg. 0.0±0.0 12.1±3.3

Non-occ Reg. 0.0±0.0
Whole face
Occ. Reg.

14.1±3.4
17.3±6.6
13.2±2.9
16.6±4.6
19.3±6.6
14.7±3.8
18.8±5.8
21.1±7.1
16.1±5.3
13.8±3.2
17.3±6.4
12.9±2.9
16.2±4.1
19.5±6.5
14.1±3.2
18.9±6.0
21.4±7.4
15.9±5.2

8.1±2.3
10.8±2.4
13.3±5.5 16.1±6.1
6.0±1.7
10.1±2.2
12.2±3.2 10.9±4.2
15.4±5.1 18.4±5.8
5.7±4.3
9.6±2.3
16.4±7.1 14.3±6.3
20.1±8.0 19.8±6.3
8.8±8.1
9.4±2.3
7.0±2.1
9.1±2.3
18.6±7.1 18.1±6.1
4.1±1.6
6.5±1.4
13.4±5.0 10.2±3.7
23.8±7.8 21.0±6.3
3.1±1.7
6.3±1.4
22.7±11.7 14.3±5.8
32.4±11.9 22.4±7.0
5.0±6.7
7.0±2.5

Figure 4:
a) 68
landmarks  b) a
shape-normalized
face  c) synthetic
occlusion.

Figure 5: Results of several methods on MPIE database. This shows the means
and standard deviations of the absolute differences between reconstructed im-
ages and the ground-truths. The statistics are available for three types of face
regions (whole face  occluded region  and non-occluded region)  different oc-
clusion sizes  and different energy settings. Our method consistently outper-
forms other methods for different occlusion sizes and energy levels.

a square occlusion window of that size  drawing the pixel values randomly from 0 to 255. A syn-
thetic testing image is then created by pasting the occlusion window at a random position. Fig. 4c
displays such an image with occlusion size of 20. For every synthetic testing image and each of
the four algorithms  we compute the mean (at pixel level) of the absolute differences between the
reconstructed image and the original test image without occlusion. We record these statistics for
occluded region  non-occluded region and the whole face. The average statistics together with stan-
dard deviations are then calculated over the set of all testing images. These results are displayed
in Fig. 5. We also experiment with several settings for the energy levels for PCA and KPCA. The
energy level essentially means the number of components of PCA/KPCA subspace. In the interest
of space  we only display results for two settings 80% and 95%. BaseLine is the method that does
nothing; the reconstructed images are exactly the same as the input testing images. As can be seen
from Fig.5  our method consistently outperforms others for all energy levels and occlusion sizes
(using the whole-face statistics). Notably  the performance of our method with the best parameter
settings is also better than the performances of other methods with their best parameter settings.

The experimental results for Mika et al  Kwok & Tsang  Robust PCA [6] are generated using our
own implementations. For Mika et al  Kwok & Tsang’s methods  we use Gaussian kernels with
γ = 10−7. For our method  we use E0 deﬁned in Sec. 3.3. The kernel is Gaussian with γ =
10−7  γ2 = 10−6  and C = 0.1. The parameters are tuned using validation data.

4.2 RKPCA for incomplete training data
To compare the ability to handle missing attributes in training data of our algorithm with other
methods  we perform some experiments on the well known Oil Flow dataset [4]. This dataset is
also used by Sanguinetti & Lawrence [16]. This dataset contains 3000 12-dimensional synthetically
generated data points modeling the ﬂow of a mixture of oil  water and gas in a transporting pipe-line.

We test our algorithm with different amount of missing data (from 5% to 50%) and repeat each
experiment for 50 times. For each experiment  we randomly choose 100 data points and randomly
remove some attribute values at some certain rate. We run Algorithm 1 to recover the values of the
missing attributes  with m = 25  k = 10  γ = 0.0375 (same as [16])  γ2 = 0.0375  C = 107. The
squared difference between the reconstructed data and the original groundtruth data is measured 
and the mean and standard deviation for 50 runs are calculated. Note that this experiment setting is
exactly the same as the setting by [16].

6

Table 1: Reconstruction errors for 5 different methods and 10 probabilities of missing values for the
Oil Flow dataset. Our method outperforms other methods for all levels of missing data.

0.10

0.15

0.20

0.30
0.05
13 ± 4 28 ± 4 43 ± 7 53 ± 8 70 ± 9 81 ± 9

p(del)
mean
1-NN
PPCA 3.7 ± .6 9 ± 2 17 ± 5 25 ± 9 50 ± 10 90 ± 30 110 ± 30 110 ± 20 120 ± 30 140 ± 30
PKPCA 5 ± 1
61 ± 8 70 ± 10 100 ± 20
Ours 3.2 ± 1.9 8 ± 4 12 ± 4 19 ± 6 27 ± 8 34 ± 10 44 ± 9 53 ± 12 69 ± 13 83 ± 15

14 ± 5 30 ± 10 60 ± 20 90 ± 20 NA

12 ± 3 19 ± 5 24 ± 6 32 ± 6 40 ± 7

45 ± 4

0.35
97 ± 9 109 ± 8 124 ± 7 139 ± 7

0.40

0.50

0.45

0.25

5 ± 3

NA

NA

NA

NA

Experimental results are summarized in Tab. 1. The results of our method are shown in the last
column. The results of other methods are copied verbatim from [16]. The mean method is a widely
used heuristic where the missing value of an attribute of a data point is ﬁlled by the mean of known
values of the same attribute of other data points. The 1-NN method is another widely used heuristic
in which the missing values are replaced by the values of the nearest point  where the pairwise
distance is calculated using only the attributes with known values. PPCA is the probabilistic PCA
method [11]  and PKPCA is the method proposed by [16]. As can be seen from Tab. 1  our method
outperforms other methods for all levels of missing data.

4.3 RKPCA for denoising
This section describes denoising experiments on the Multi-PIE database with Gaussian additive
noise. For a fair evaluation  we only compare our algorithm with Mika et al’s  Kwok & Tsang’s
and Linear PCA. These are the methods that perform denoising based on subspaces and do not rely
explicitly on the statistics of natural images. Quantitative evaluations show that the denoising ability
of our algorithm is comparable with those of other methods.

Figure 6: Example of denoised images. a) original image  b) corrupted by Gaussian noise  c) de-
noised using PCA  d) using Mika etal  e) using Kwok & Tsang method  f) result of our method.

The set of images used in these experiments is exactly the same as those in the occlusion experiments
described in Sec. 4.1. For every testing image  we synthetically corrupt it with Gaussian additive
noise with standard deviation of 0.04. An example for a pair of clean and corrupted images are
shown in Fig. 6a and 6b. For every synthetic testing image  we compute the mean (at pixel level)
of the absolute difference between the denoised image and the ground-truth. The results of different
methods with different energy settings are summarized in Tab. 2. For these experiments  we use E0
deﬁned in Sec. 3.2 with W being the identity matrix. We use Gaussian kernel with γ = γ2 = 10−7 
and C = 1. These parameters are tuned using validation data.

Table 2: Results of image denoising on the Multi-PIE database. BaseLine is the method that does
nothing. The best energy setting for all methods is 100%. Our method is better than the others.

Energy
80%
95%
100%

Base Line
8.14±0.16
8.14±0.16
8.14±0.16

Mika

9.07±1.86
6.37±1.30
5.55±0.97

Kwok& Tsang
11.79±2.56
11.55±2.52
11.52±2.52

PCA

10.04±1.99
6.70±1.20
6.44±0.39

Ours

7.01±1.27
5.70±0.96
5.43±0.78

Tab. 2 and Fig. 6 show the performance of our method is comparable with others. In fact  the quan-
titative results show that our method is marginally better than Mika etal’s method and substantially
better than the other two. In terms of visual appearance (Fig. 6c-f)  the reconstruction image of our
method preserves much more ﬁne details than the others.

7

5 Conclusion
In this paper  we have proposed Robust Kernel PCA  a uniﬁed framework for handling noise  occlu-
sion and missing data. Our method is based on a novel cost function for Kernel PCA reconstruction.
The cost function requires the reconstructed data point to be close to the original data point as well
as to the principal subspace. Notably  the distance function between the reconstructed data point
and the original data point can take various forms. This distance function needs not to depend on
the kernel function and can be evaluated easily. Therefore  the implicitness of the feature space is
avoided and optimization is possible. Extensive experiments  in two well known data sets  show that
our algorithm outperforms existing methods.

References

[1] Alzate  C. & Syukens  J.A. (2005) ‘Robust Kernel Principal Component Analysis uisng Huber’s Loss

Function.’ 24th Benelux Meeting on Systems and Control.

[2] Bakir  G.H.  Weston  J. & Sch¨olkopf  B. (2004) ‘Learning to Find Pre-Images.’ in Thrun  S.  Saul  L. &

Sch¨olkopf  B. (Eds) Advances in Neural Information Processing Systems.

[3] Berar  M.  Desvignes  M.  Bailly  G.  Payan  Y. & Romaniuk  B. (2005) ‘Missing Data Estimation using

Polynomial Kernels.’ Proceedings of International Conference on Advances in Pattern Recognition.

[4] Bishop  C.M.  Svens´en  M. & Williams  C.K.I. (1998) ‘GTM: The Generative Topographic Mapping.’

Neural Computation  10(1)  215–234.
[5] Black  M.J. & Anandan  P. (1996)

Piecewise-smooth Flow Fields.’ Computer Vision and Image Understanding  63(1)  75–104.

‘The Robust Estimation of Multiple Motions: Parametric and

[6] de la Torre  F. & Black  M.J. (2003) ‘A Framework for Robust Subspace Learning.’ International Journal

of Computer Vision  54(1–3)  117–142.

[7] Deng  X.  Yuan  M. & Sudijanto  A. (2007) ‘A Note on Robust Principal Component Analysis.’ Contem-

porary Mathematics  443  21–33.

[8] Gross  R.  Matthews  I.  Cohn  J.  Kanade  T. & Baker  S. (2007) ‘The CMU Multi-pose  Illumination 

and Expression (Multi-PIE) Face Database.’ Technical report  Carnegie Mellon University.TR-07-08.

[9] Jolliffe  I. (2002) Principal Component Analysis. 2 edn. Springer-Verlag  New York.
[10] Kwok  J.T.Y. & Tsang  I.W.H. (2004) ‘The Pre-Image Problem in Kernel Methods.’ IEEE Transactions

on Neural Networks  15(6)  1517–1525.

[11] Lawrence  N.D. (2004) ‘Gaussian Process Latent Variable Models for Visualization of High Dimensional
Data.’ in Thrun  S.  Saul  L. & Sch¨olkopf  B. (Eds) Advances in Neural Information Processing Systems.
[12] Lu  C.  Zhang  T.  Zhang  R. & Zhang  C. (2003) ‘Adaptive Robust Kernel PCA Algorithm.’ Proceedings

of IEEE International Conference on Acoustics  Speech  and Signal Processing.

[13] Mika  S.  Sch¨olkopf  B.  Smola  A.  M¨uller  K.R.  Scholz  M. & R¨atsch  G. (1999)
De-Noising in Feature Spaces.’ Advances in Neural Information Processing Systems.

‘Kernel PCA and

[14] Romdhani  S.  Gong  S. & Psarrou  A. (1999) ‘Multi-view Nonlinear Active Shape Model Using Kernel

PCA.’ British Machine Vision Conference  483–492.

[15] Roweis  S. (1998) ‘EM Algorithms for PCA and SPCA.’ in Jordan  M.  Kearns  M. & Solla  S. (Eds)

Advances in Neural Information Processing Systems 10.

[16] Sanguinetti  G. & Lawrence  N.D. (2006)

Conference on Machine Learning.

‘Missing Data in Kernel PCA.’ Proceedings of European

[17] Sch¨olkopf  B.  Mika  S.  Smola  A.  R¨atsch  G. & M¨uller  K.R. (1998) ‘Kernel PCA Pattern Reconstruc-

tion via Approximate Pre-Images.’ International Conference on Artiﬁcial Neural Networks.

[18] Sch¨olkopf  B. & Smola  A. (2002) Learning with Kernels: Support Vector Machines  Regularization 

Optimization  and beyond. MIT Press  Cambridge  MA.

[19] Sch¨olkopf  B.  Smola  A. & Mller  K. (1998) ‘Nonlinear Component Analysis as a Kernel Eigenvalue

Problem.’ Neural Computation  10  1299–1319.

[20] Shawe-Taylor  J. & Cristianini  N. (2004) Kernel Methods for Pattern Analysis. Cambridge Uni. Press.
[21] Tipping  M. & Bishop  C.M. (1999) ‘Probabilistic Principal Component Analysis.’ Journal of the Royal

Statistical Society B  61  611–622.

[22] Wang  L.  Pang  Y.W.  Shen  D.Y. & Yu  N.H. (2007) ‘An Iterative Algorithm for Robust Kernel Principal

Component Analysis.’ Conference on Machine Learning and Cybernetics.

8

,Carl Vondrick
Hamed Pirsiavash
Aude Oliva
Antonio Torralba