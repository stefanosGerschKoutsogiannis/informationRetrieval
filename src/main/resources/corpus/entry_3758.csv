2014,Neurons as Monte Carlo Samplers: Bayesian ￼Inference and Learning in Spiking Networks,We propose a two-layer spiking network capable of performing approximate inference and learning for a hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models  the hidden world state is not observed by the sensory neurons  and the temporal dynamics of the hidden state is unknown. We demonstrate how this network can sequentially learn the hidden Markov model using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.,Neurons as Monte Carlo Samplers: Bayesian
Inference and Learning in Spiking Networks

Yanping Huang

University of Washington
huangyp@cs.uw.edu

Rajesh P.N. Rao

University of Washington

rao@cs.uw.edu

Abstract

We propose a spiking network model capable of performing both approximate
inference and learning for any hidden Markov model. The lower layer sensory
neurons detect noisy measurements of hidden world states. The higher layer neu-
rons with recurrent connections infer a posterior distribution over world states
from spike trains generated by sensory neurons. We show how such a neuronal
network with synaptic plasticity can implement a form of Bayesian inference sim-
ilar to Monte Carlo methods such as particle ﬁltering. Each spike in the population
of inference neurons represents a sample of a particular hidden world state. The
spiking activity across the neural population approximates the posterior distribu-
tion of hidden state. The model provides a functional explanation for the Poisson-
like noise commonly observed in cortical responses. Uncertainties in spike times
provide the necessary variability for sampling during inference. Unlike previous
models  the hidden world state is not observed by the sensory neurons  and the
temporal dynamics of the hidden state is unknown. We demonstrate how such
networks can sequentially learn hidden Markov models using a spike-timing de-
pendent Hebbian learning rule and achieve power-law convergence rates.

1

Introduction

Humans are able to routinely estimate unknown world states from ambiguous and noisy stimuli 
and anticipate upcoming events by learning the temporal dynamics of relevant states of the world
from incomplete knowledge of the environment. For example  when facing an approaching tennis
ball  a player must not only estimate the current position of the ball  but also predict its trajectory
by inferring the ball’s velocity and acceleration before deciding on the next stroke. Tasks such as
these can be modeled using a hidden Markov model (HMM)  where the relevant states of the world
are latent variables X related to sensory observations Z via a likelihood model (determined by the
emission probabilities). The latent states themselves evolve over time in a Markovian manner  the
dynamics being governed by a transition probabilities. In these tasks  the optimal way of combin-
ing such noisy sensory information is to use Bayesian inference  where the level of uncertainty for
each possible state is represented as a probability distribution [1]. Behavioral and neuropsychophys-
ical experiments [2  3  4] have suggested that the brain may indeed maintain such a representation
and employ Bayesian inference and learning in a great variety of tasks in perception  sensori-motor
integration  and sensory adaptation. However  it remains an open question how the brain can se-
quentially infer the hidden state and learn the dynamics of the environment from the noisy sensory
observations.
Several models have been proposed based on populations of neurons to represent probability dis-
tribution [5  6  7  8]. These models typically assume a static world state X. To get around this
limitation  ﬁring-rate models [9  10] have been proposed to used responses in populations of neu-
rons to represent the time-varying posterior distributions of arbitrary hidden Markov models with
discrete states. For the continuous state space  similar models based on line attractor networks [11]

1

have been introduced for implementing the Kalman ﬁlter  which assumes all distributions are Gaus-
sian and the dynamics is linear. Bobrowski et al. [12] proposed a spiking network model that can
compute the optimal posterior distribution in continuous time. The limitation of these models is that
model parameters (the emission and transition probabilities) are assumed to be known a priori. Den-
eve [13  14] proposed a model for inference and learning based on the dynamics of a single neuron.
However  the maximum number of world state in her model is limited to two.
In this paper  we explore a neural implementation of HMMs in networks of spiking neurons that
perform approximate Bayesian inference similar to the Monte Carlo method of particle ﬁltering [15].
We show how the time-varying posterior distribution P (Xt|Z1:t) can be directly represented by
mean spike counts in sub-populations of neurons. Each model neuron in the neuron population
behaves as a coincidence detector  and each spike is viewed as a Monte Carlo sample of a particular
world state. At each time step  the probability of a spike in one neuron is shown to approximate
the posterior probability of the preferred state encoded by the neuron. Nearby neurons within the
same sub-population (analogous to a cortical column) encode the same preferred state. The model
thus provides a concrete neural implementation of sampling ideas previously suggested in [16  17 
18  19  20]. In addition  we demonstrate how a spike-timing based Hebbian learning rule in our
network can implement an online version of the Expectation-Maximization(EM) algorithm to learn
the emission and transition matrices of HMMs.

2 Review of Hidden Markov Models

For clarity of notation  we brieﬂy review the equations behind a discrete-time “grid-based” Bayesian
ﬁlter for a hidden Markov model. Let the hidden state be {Xk ∈ X  k ∈ N} with dynamics
Xk+1 | (Xk = x(cid:48)) ∼ f (x|x(cid:48))  where f (x|x(cid:48)) is the transition probability density  X is a discrete
state space of Xk  N is the set of time steps  and “∼” denotes distributed according to. We focus
on estimating Xk by constructing its posterior distribution  based only on noisy measurements or
observations {Zk} ∈ Z where Z can be discrete or continuous. {Zk} are conditional independent
given {Xk} and are governed by the emission probabilities Zk | (Xk = x) ∼ g(z|x).
The posterior probability P (Xk = i|Z1:k) = ωi
(Eq 1) and a measurement update (or correction) stage (Eq 2):

k|k may be updated in two stages: a prediction stage

P (Xk+1 = i | Z1:k) =

ωi
P (Xk+1 = i | Z1:k+1) = ωi

k+1|k =(cid:80)X
k|kf (xi|xj) 
j=1 ωj
(cid:80)X
k+1|kg(Zk+1|xi)
ωi
j=1 ωj

k+1|kg(Zk+1|xj )

k+1|k+1 =

(1)

(2)

.

This process is repeated for each time step. These two recursive equations above are the foundation
for any exact or approximate solution to Bayesian ﬁltering  including well-known examples such as
Kalman ﬁltering when the original continuous state space has been discretized into X bins.

3 Neural Network Model

We now describe the two-layer spiking neural network model we use (depicted in the central panel of
Figure 1(a)). The noisy observation Zk is not directly observed by the network  but sensed through
an array of Z sensory neurons  The lower layer consists of an array of sensory neurons  each of
which will be activated at time k if the observation Zk is in the receptive ﬁeld. The higher layer
consists of an array of inference neurons  whose activities can be deﬁned as:

s(k) = sgn(a(k) × b(k))

(3)
where s(k) describes the binary response of an inference neuron at time k  the sign function
sgn(x) = 1 only when x > 0. a(k) represents the sum of neuron’s recurrent inputs  which is
determined by the recurrent weight matrix W among the inference neurons and the population re-
sponses sk−1 from the previous time step. b(k) represents the sum of feedforward inputs  which is
determined by the feed-forward weight matrix M as well as the activities in sensory neurons.
Note that Equation 3 deﬁnes the output of an abstract inference neuron which acts as a coincidence
detector and ﬁres if and only if both recurrent and sensory inputs are received. In the supplementary
materials  we show that this abstract model neuron can be implemented using the standard leaky-
integrate-and-ﬁre (LIF) neurons used to model cortical neurons.

2

(a)

(b)

Figure 1: a. Spiking network model for sequential Monte Carlo Bayesian inference. b. Graphical
representation of spike distribution propagation

l  i = 1  . . .X   l = 1  . . .  L}. We have si

3.1 Neural Representation of Probability Distributions
Similar to the idea of grid-based ﬁltering  we ﬁrst divide the inference neurons into X sub-
populations. s = {si
l(k) = 1 if there is a spike in the
l-th neuron of the i-th sub-population at time step k. Each sub-population of L neurons share the
same preferred world state  there being X such sub-populations representing each of X preferred
states. One can  for example  view such a neuronal sub-population as a cortical column  within
which neurons encode similar features [21].
Figure 1(a) illustrates how our neural network encodes a simple hidden Markov model with X =
Z = 1  . . .   100. Xk = 50 is a static state and P (Zk|Xk) is normally distributed. The network
utilizes 10 000 neurons for the Monte Carlo approximation  with each state preferred by a sub-
population of 100 neurons. At time k  the network observe Zk and the corresponding sensory
neuron whose receptive ﬁeld contains Zk is activated and sends inputs to the inference neurons.
Combining with recurrent inputs from the previous time step  the responses in the inference neurons
are updated at each time step. As shown in the raster plot of Figure 1(a)  the spikes across the entire
inference layer population form a Monte-Carlo approximation to the current posterior distribution:

L(cid:88)

ni
k|k :=

l(k) ∝ ωi
si
k|k

(4)

l=1

where ni

regarded as the instantaneous ﬁring rate for sub-population i. Nk = (cid:80)X

k|k is the number of spiking neurons in the ith sub-population at time k  which can also be
k|k is the total spike
k|k} represents the un-normalized conditional

count in the inference layer population. The set {ni
probabilities of Xk  so that ˆP (Xk = i|Z1:k) = ωi

i=1 ni

k|k = ni

k|k/Nk.

3.2 Bayesian Inference with Stochastic Synaptic Transmission

In this section  we assume the network is given the model parameters in a HMM and there is no
learning in connection weights in the network. To implement the prediction Eq 1 in a spiking
network  we initialize the recurrent connections between the inference neurons as the transition
probabilities: Wij = f (xj|xi)/CW   where CW is a scaling constant. We will discuss how our
network learns the HMM parameters from random initial synaptic weights in section 4.
We deﬁne the recurrent weight Wij to be the synaptic release probability between the i-th neuron
sub-population and the j-th neuron sub-population in the inference layer. Each neuron that spikes
at time step k will randomly evoke  with probability Wij  one recurrent excitatory post-synaptic
potential (EPSP) at time step k + 1  after some network delay. We deﬁne the number of recurrent
EPSPs received by neuron l in the j-th sub-population as aj
l is the sum of Nk independent
(but not identically distributed) Bernoulli trials:

l . Thus  aj

aj
l (k + 1) =

i
l(cid:48)si

l(cid:48)(k) 

∀l = 1 . . .L.

(5)

X(cid:88)

L(cid:88)

i=1

l(cid:48)=1

3

where P (i
binomial” distribution [22] and in the limit approaches the Poisson distribution:

l = 1) = Wij and P (i

l = 0) = 1 − Wij. The sum aj

l follows the so-called “Poisson

l (k + 1) ≥ 1) (cid:39)(cid:88)

P (aj

i

Wijni

k|k =

Nk
CW

ωj

k+1|k

(6)

l and the proof of equation 6 are provided in the supple-

The detailed analysis of the distribution of ai
mentary materials.
The deﬁnition of model neuron in Eq 3 indicates that recurrent inputs alone are not strong enough
to make the inference neurons ﬁre – these inputs leave the neurons partially activated. We can
view these partially activated neurons as the proposed samples drawn from the prediction density
P (Xk+1|Xk). Let nj

k+1|k be the number of proposed samples in j-th sub-population  we have

E[nj

k+1|k|{ni

k|k}] = L

Wij ni

k|k = L Nk
CW

k+1|k ∝ Var[nj
ωj

k+1|k|{ni

k|k}]

(7)

X(cid:88)

i=1

Thus  the prediction probability in equation 1 is represented by the expected number of neurons that
receive recurrent inputs.
When a new observation Zk+1 is received  the network will correct the prediction distribution based
on the current observation. Similar to rejection sampling used in sequential Monte Carlo algo-
rithms [15]  these proposed samples are accepted with a probability proportional to the observation
likelihood P (Zk+1|Xk+1). We assume for simplicity that receptive ﬁelds of sensory neurons do not
overlap with each other (in the supplementary materials  we discuss the more general overlapping
case). Again we deﬁne the feedforward weight Mij to be the synaptic release probability between
sensory neuron i and inference neurons in the j-th sub-population. A spiking sensory neuron i
causes an EPSP in a neuron in the j-th sub-population with probability Mij  which is initialized
proportional to the likelihood:

P (bi

l(k + 1) ≥ 1) = g(Zk+1|xi)/CM

(8)

where CM is a scaling constant such that Mij = g(Zk+1 = zi | xj)/CM .
Finally  an inference neuron ﬁres a spike at time k + 1 if and only if it receives both recurrent and
sensory inputs. The corresponding ﬁring probability is then the product of the probabilities of the
two inputs:P (si

l(k + 1) ≥ 1)P (bi

l(k + 1) = 1) = P (ai

l(k + 1) ≥ 1)

l=1 si

l(k + 1) be the number of spikes in i-th sub-population at time k + 1  we

k+1|k+1 =(cid:80)L

Let ni
have

E[ni

Var[ni

k+1|k+1|{ni
k+1|k+1|{ni

k|k}] = L Nk
CW CM
k|k}] (cid:39) L Nk
CW CM

P (Zk+1|Z1:k)ωi
g(Zk+1|xi)ωi

k+1|k

k+1|k+1

(9)

(10)

Equation 9 ensures that the expected spike distribution at time k + 1 is a Monte Carlo approximation
to the updated posterior probability P (Xk+1|Z1:k+1). It also determines how many neurons are
activated at time k + 1. To keep the number of spikes at different time steps relatively constant  the
scaling constant CM   CW and the number of neurons L could be of the same order of magnitude:
for example  CW = L = 10 ∗ N1 and CM (k + 1) = 10 ∗ Nk/N1  resulting in a form of divisive
inhibition [23]. If the overall neural activity is weak at time k  then the global inhibition regulating
M is decreased to allow more spikes at time k + 1. Moreover  approximations in equations 6 and
10 become exact when N 2
k
C2
W

→ 0.

3.3 Filtering Examples

Figure 1(b) illustrates how the model network implements Bayesian inference with spike samples.
The top three rows of circles in the left panel in Figure 1(b) represent the neural activities in the
inference neurons  approximating respectively the prior  prediction  and posterior distributions in
the right panel. At time k  spikes (shown as ﬁlled circles) in the posterior population represent the

4

(a)

(b)

(c)

Figure 2: Filtering results for uni-modal (a) and bi-modal posterior distributions ((b) and (c) - see
text for details).

distribution P (Xk|Z1:k). With recurrent weights W ∝ f (Xk+1|Xk)  spiking neurons send EPSPs
to their neighbors and make them partially activated (shown as half-ﬁlled circles in the second row).
The distribution of partially activated neurons is a Monte-Carlo approximation to the prediction
distribution P (Xk+1|Z1:k). When a new observation Zk+1 arrives  the sensory neuron (ﬁlled circles
the bottom row) whose receptive ﬁeld contains Zk+1 is activated  and sends feedforward EPSPs to
the inference neurons using synaptic weights M = g(Z|X). The inference neurons at time k +1 ﬁre
only if they receive both recurrent and feedforward inputs. With the ﬁring probability proportional to
the product of prediction probability P (Xk+1|Z1:k) and observation likelihood g(Zk+1|Xk+1)  the
spike distribution at time k + 1 (ﬁlled circles in the third row) again represents the updated posterior
P (Xk+1|Z1:k+1).
We further tested the ﬁltering results of the proposed neural network with two other example HMMs.
The ﬁrst example is the classic stochastic volatility model  where X = Z = R. The transition model
of the hidden volatility variable f (Xk+1|Xk) = N (0.91Xk  1.0)  and the emission model of the
observed price given volatility is g(Zk|Xk) = N (0  0.25 exp(Xk)). The posterior distribution of
this model is uni-modal. In simulation we divided X into 100 bins  and initial spikes N1 = 1000.
We plotted the expected volatility with estimated standard deviation from the population posterior
distribution in Figure 2(a). We found that the neural network does indeed produce a reasonable
estimate of volatility and plausible conﬁdence interval. The second example tests the network’s
ability to approximate bi-modal posterior distributions by comparing the time varying population
posterior distribution with the true one using heat maps (Figures 2(b) and 2(c)). The vertical axis
represents the hidden state and the horizontal axis represents time steps. The magnitude of the
probability is represented by the color. In this example  X = {1  . . .   8} and there are 20 time steps.

3.4 Convergence Results and Poisson Variability

k =

ni
k|k
Nk

In this section  we discuss some convergence results for Bayesian ﬁltering using the proposed spik-
ing network and show our population estimator of the posterior probability is a consistent one. Let
be the population estimator of the true posterior probability P (Xk = i|Z1:k) at time k.
ˆP i
Suppose the true distribution is known only at initial time k = 1: ˆP i
1|1. We would like to
investigate how the mean and variance of ˆP i
k vary over time. We derived the updating equations for
mean and variance (see supplementary materials) and found two implications. First  the variance of
neural response is roughly proportional to the mean. Thus  rather than representing noise  Poisson
variability in the model occurs as a natural consequence of sampling and sparse coding. Second  the
k ] ∝ 1/N1. Therefore Var[ ˆP j
variance Var[ ˆP j
k is a consistent
estimator of ωj
k|k. We tested the above two predictions using numerical experiments on arbitrary
HMMs  where we choose X = {1  2  . . . 20}  Zk ∼ N (Xk  5)  the transition matrix f (xj|xi) ﬁrst

uniformly drawn from [0  1]  and then normalized to ensure(cid:80)

k ] → 0 as N1 → ∞  showing that ˆP j

1 = ωi

j f (xj|xi) = 1.

k ] − E2[ ˆP j
In Figures 3(a-c)  each data point represents Var[ ˆP j
k ]
along the horizontal axis  calculated over 100 trials with the same random transition matrix f  and
k = 1  . . . 10  j = 1  . . . 20. The solid lines represent a least squares power law ﬁt to the data:
Var[ ˆP j
k ])CE . For 100 different random transition matrices f  the means

k ] along the vertical axis and E[ ˆP j

k ] = CV ∗ (E[ ˆP j

k ] − E2[ ˆP j

5

(a)

(b)

(c)

(d)

(e)

Figure 3: Variance versus Mean of estimator for different initial spike counts

(f)

k|k] ∝ E[nj

of the exponential term CE were 1.2863  1.13  and 1.037  with standard deviations 0.13  0.08  and
0.03 respectively  for N1 = 100 and X = 4  20  and 100. The mean of CE continues to approach
1 when X is increased  as shown in ﬁgure 3(d). Since Var[ ˆP j
k ]) implies
Var[nj
k|k] (see supplementary material for derivation)  these results verify the Poisson
variability prediction of our neural network.
The term CV represents the scaling constant for the variance. Figure 3(e) shows that the mean of
CV over 100 different transition matrices f (over 100 different trials with the same f) is inversely
proportional to initial spike count N1  with power law ﬁt CV = 1.77N−0.9245
. This indicates that
k converges to 0 if N1 → ∞. The bias between estimated and true posterior
the variance of ˆP j
probability can be calculated as:

k ] ∝ (E[ ˆP j

k ] − E2[ ˆP j

1

bias(f) =

1
XK

(E[ˆPi

k] − ωi

k|k)2

X(cid:88)

K(cid:88)

i=1

k=1

The relationship between the mean of the bias (over 100 different f) versus initial count N1 is shown
in ﬁgure 3(f). We also have an inverse proportionality between bias and N1. Therefore  as the ﬁgure
shows  for arbitrary f  the estimator ˆP j

k is a consistent estimator of ωj

k|k.

4 On-line parameter learning

In the previous section  we assumed that the model parameters  i.e.  the transition probabilities
f (Xk+1|Xk) and the emission probabilities g(Zk|Xk)  are known. In this section  we describe how
these parameters θ = {f  g} can be learned from noisy observations {Zk}. Traditional methods
to estimate model parameters are based on the Expectation-Maximization (EM) algorithm  which
maximizes the (log) likelihood of the unknown parameters log Pθ(Z1:k) given a set of observations
collected previously. However  such an “off-line” approach is biologically implausible because (1)
it requires animals to store all of the observations before learning  and (2) evolutionary pressures
dictate that animals update their belief over θ sequentially any time a new measurement becomes
available.
We therefore propose an on-line estimation method where observations are used for updating pa-
rameters as they become available and then discarded. We would like to ﬁnd the parameters θ that
t=1 log Pθ(Zt|Zt−1). Our approach is based on re-
cursively calculating the sufﬁcient statistics of θ using stochastic approximation algorithms and the

maximize the log likelihood: log Pθ(Z1:k) =(cid:80)k

6

10−510−310010−110−710−510−210−410−610−3E[pjk] − E2[pjk]Var[pjk]y = 0.028804 * x1.286310010−510−310−110−510−710−3E[pjk] − E2[pjk]Var[pjk]y = 0.00355627 * x1.1310−510−410−310−210−110010−910−810−710−610−510−4E[pjk] − E2[pjk]Var[pjk]y = 0.000303182 * x1.037Figure 4: Performance of the Hebbian Learning Rules.

Monte Carlo method  and employs an online EM algorithm obtained by approximating the expected
sufﬁcient statistic ˆT (θk) using the stochastic approximation (or Robbins-Monoro) procedure. Based
on the detailed derivations described in the supplementary materials  we obtain a Hebbian learning
rule for updating the synaptic weights based on the pre-synaptic and post-synaptic activities:

M k

ij = γk

W k

ij = γk

× ˜ni(k)(cid:80)
i ˜ni(k)
× nj
k|k
Nk

nj
k|k
Nk
ni
k−1|k−1
Nk−1

) × M k−1

ij

+ (1 − γk

+ (1 − γk

nj
k|k
Nk
ni
k−1|k−1
Nk−1

when nj

k|k > 0 

(11)

) × W k−1

ij

when ni

k−1|k−1 > 0 

(12)

where ˜ni(k) is the number of pre-synaptic spikes in the i-th sub-population of sensory neurons at
time k  γk is the learning rate.
Learning both emission and transition probability matrices at the same time using the online EM
algorithm with stochastic approximation is in general very difﬁcult because there are many local
minima in the likelihood function. To verify the correctness of our learning algorithms individually 
we ﬁrst divide the learning process into two phases. The ﬁrst phase involves learning the emission
probability g when the hidden world state is stationary  i.e.  Wij = fij = δij. This corresponds to
learning the observation model of static objects at the center of gaze before learning the dynamics
f of objects. After an observation model g is learned  we relax the stationarity constraint  and allow
the spiking network to update the recurrent weights W to learn the arbitrary transition probability f.
Figure 4 illustrates the performance of learning rules (11) and (12) for a discrete HMM with X = 4
and Z = 12. X and Z values are spaced equally apart: X ∈ {1  . . .   4} and Z ∈ { 2
3}.
3   . . .   4 1
The transition probability matrix f then involves 4×4 = 16 parameters and the emission probability
matrix g involves 12 × 4 = 48 parameters.
In Figure 4(a)  we examine the performance of learning rule (11) for the feedforward weights
M k  with ﬁxed transition matrix. The true emission probability matrix has the form g.j =∼
N (xj  σ2

Z). The solid blue curve shows the mean square error (Frobenius norm)(cid:13)(cid:13)M k − g(cid:13)(cid:13)F =

ij − gij)2 between the learned feedforward weights M k and the true emission probabil-
ity matrix g over trials with different g . The dotted lines show ± 1 standard deviation for MSE
based on 10 different trials. σZ varied from trial to trial and was drawn uniformly between 0.2
and 0.4  representing different levels of observation noises. The initial spike distribution was uni-
i j = 1Z . The learning rate was set
form ni
k   although a small constant learning rate such as γk = 10−5 also gives rise to similar
to γk = 1
learning results. A notable feature in Figure 4(a) is that the average MSE exhibits a fast power-
law decrease. The red solid line in Figure 4(a) represents the power-law ﬁt to the average MSE:
M SE(k) ∝ k−1.1. Furthermore  the standard deviation of MSE approaches zero as k grows large.

0|0 ∀i  j = 1 . . .  X and the initial estimate M 0

(cid:113)(cid:80)

0|0 = nj

ij(M k

3   1  4

7

(cid:113)(cid:80)

ij(W k

mean square error(cid:13)(cid:13)W k − f(cid:13)(cid:13)F =

Figure 4(a) thus shows the asymptotic convergence of equation (11) irrespective of the σZ of the
true emission matrix g.
We next examined the performance of learning rule 12 for the recurrent weights W k  given the
learned emission probability matrix g (the true transition probabilities f are unknown to the net-
work). The initial estimator W 0
ij = 1X . Similarly  Performance was evaluated by calculating the
ij − fij)2 between the learned recurrent weight W k
and the true f. Different randomly chosen transition matrices f were tested. When σZ = 0.04  the
observation noise is 0.04
1/3 = 12% of the separation between two observed states. Hidden state identi-
ﬁcation in this case is relatively easy. The red solid line in ﬁgure 4(b) represents the power-law ﬁt to
the average MSE: M SE(k) ∝ k−0.36. Similar convergence results can still be obtained for higher
σZ  e.g.  σZ = 0.4 (ﬁgure 4(c)). In this case  hidden state identiﬁcation is much more difﬁcult as
the observation noise is now 1.2 times the separation between two observed states. This difﬁculty
is reﬂected in a slower asymptotic convergence rate  with a power-law ﬁt M SE(k) ∝ k−0.21  as
indicated by the red solid line in ﬁgure 4(c).
Finally  we show the results for learning both emission and transition matrices simultaneously in
ﬁgure 4(d e). In this experiment  the true emission and transition matrices are deterministic  the
ij ∝
weight matrices are initialized as the sum of the true one and a uniformly random one: W 0
ij ∝ gij +  where  is a uniform distributed noise between 0 and 1/NX. Although
fij +  and M 0
the asymptotic convergence rate for this case is much slower  it still exhibits desired power-law
convergences in both M SEW (k) ∝ k−0.02 and M SEM (k) ∝ k−0.08 over 100 trials starting with
different initial weight matrices.

5 Discussion

Our model suggests that  contrary to the commonly held view  variability in spiking does not re-
ﬂect “noise” in the nervous system but captures the animal’s uncertainty about the outside world.
This suggestion is similar to some previous models [17  19  20]  including models linking ﬁring rate
variability to probabilistic representations [16  8] but differs in the emphasis on spike-based repre-
sentations  time-varying inputs  and learning. In our model  a probability distribution over a ﬁnite
sample space is represented by spike counts in neural sub-populations. Treating spikes as random
samples requires that neurons in a pool of identical cells ﬁre independently. This hypothesis is sup-
ported by a recent experimental ﬁndings [21] that nearby neurons with similar orientation tuning and
common inputs show little or no correlation in activity. Our model offers a functional explanation
for the existence of such decorrelated neuronal activity in the cortex.
Unlike many previous models of cortical computation  our model treats synaptic transmission be-
tween neurons as a stochastic process rather than a deterministic event. This acknowledges the
inherent stochastic nature of neurotransmitter release and binding. Synapses between neurons usu-
ally have only a small number of vesicles available and a limited number of post-synaptic receptors
near the release sites. Recent physiological studies [24] have shown that only 3 NMDA receptors
open on average per release during synaptic transmission. These observations lend support to the
view espoused by the model that synapses should be treated as probabilistic computational units
rather than as simple scalar parameters as assumed in traditional neural network models.
The model for learning we have proposed builds on prior work on online learning [25  26]. The
online algorithm used in our model for estimating HMM parameters involves three levels of approx-
imation. The ﬁrst level involves performing a stochastic approximation to estimate the expected
complete-data sufﬁcient statistics over the joint distribution of all hidden states and observations.
Cappe and Moulines [26] showed that under some mild conditions  such an approximation produces
a consistent  asymptotically efﬁcient estimator of the true parameters. The second approximation
comes from the use of ﬁltered rather than smoothed posterior distributions. Although the conver-
gence reported in the methods section is encouraging  a rigorous proof of convergence remains to
be shown. The asymptotic convergence rate using only the ﬁltered distribution is about one third
the convergence rate obtained for the algorithms in [25] and [26]  where the smoothed distribution
is used. The third approximation results from Monte-Carlo sampling of the posterior distribution.
As discussed in the methods section  the Monte Carlo approximation converges in the limit of large
numbers of particles (spikes).

8

References
[1] R.S. Zemel  Q.J.M. Huys  R. Natarajan  and P. Dayan. Probabilistic computation in spiking populations.

Advances in Neural Information Processing Systems  17:1609–1616  2005.

[2] D. Knill and W. Richards. Perception as Bayesian inference. Cambridage University Press  1996.
[3] K. Kording and D. Wolpert. Bayesian integration in sensorimotor learning. Nature  427:244–247  2004.
[4] K. Doya  S. Ishii  A. Pouget  and R. P. N. Rao. Bayesian Brain: Probabilistic Approaches to Neural

Coding. Cambridge  MA: MIT Press  2007.

[5] K. Zhang  I. Ginzburg  B.L. McNaughton  and T.J.Sejnowski. Interpreting neuronal population activity
by reconstruction: A uniﬁed framework with application to hippocampal place cells. Journal of Neuro-
science  16(22)  1998.

[6] R. S. Zemel and P. Dayan. Distributional population codes and multiple motion models. Advances in

neural information procession system  11  1999.

[7] S. Wu  D. Chen  M. Niranjan  and S.I. Amari. Sequential Bayesian decoding within a population of

neurons. Neural Computation  15  2003.

[8] W.J. Ma  J.M. Beck  P.E. Latham  and A. Pouget. Bayesian inference with probabilistic population codes.

Nature Neuroscience  9(11):1432–1438  2006.

[9] R.P.N. Rao. Bayesian computation in recurrent neural circuits. Neural Computation  16(1):1–38  2004.
[10] J.M. Beck and A. Pouget. Exact inferences in a neural implementation of a hidden Markov model. Neural

Computation  19(5):1344–1361  2007.

[11] R.C. Wilson and L.H. Finkel. A neural implmentation of the kalman ﬁlter. Advances in Neural Informa-

tion Processing Systems  22:2062–2070  2009.

[12] O. Bobrowski  R. Meir  and Y. Eldar. Bayesian ﬁltering in spiking neural networks: noise adaptation and

multisensory integration. Neural Computation  21(5):1277–1320  2009.

[13] S. Deneve. Bayesian spiking neurons i: Inference. Neural Computation  20:91–117  2008.
[14] S. Deneve. Bayesian spiking neurons ii: Learning. Neural Computation  20:118–145  2008.
[15] A. Doucet  N. de Freitas  and N. Gordon. Sequential Monte Carlo methods in practice. Springer-Verlag 

2001.

[16] P.O. Hoyer  A. Hyrinen  and A.H. Arinen. Interpreting neural response variability as Monte Carlo sam-

pling of the posterior. Advances in Neural Information Processing Systems 15  2002.

[17] M G Paulin. Evolution of the cerebellum as a neuronal machine for Bayesian state estimation. J. Neural

Eng.  2:S219–S234  2005.

[18] N.D. Daw and A.C. Courville. The pigeon as particle lter. Advances in Neural Information Processing

Systems  19  2007.

[19] L. Buesing  J. Bill  B. Nessler  and W. Maass. Neural dynamics as sampling: A model for stochastic

computation in recurrent networks of spiking neurons. PLoS Comput Biol  7(11)  2011.

[20] P. Berkes  G. Orban  M. Lengye  and J. Fisher. Spontaneous cortical activity reveals hallmarks of an

optimal internal model of the environment. Science  331(6013)  2011.

[21] A. S. Ecker  P. Berens  G.A. Kelirls  M. Bethge  N. K. Logothetis  and A. S. Tolias. Decorrelated neuronal

ﬁring in cortical microcircuits. Science  327(5965):584–587  2010.

[22] Jr. Hodges  J. L. and Lucien Le Cam. The Poisson approximation to the Poisson binomial distribution.

The Annals of Mathematical Statistics  31(3):737–740  1960.

[23] Frances S. Chance and L. F. Abbott. Divisive inhibition in recurrent networks. Network  11:119–129 

2000.

[24] E.A. Nimchinsky  R. Yasuda  T.G. Oertner  and K. Svoboda. The number of glutamate receptors opened

by synaptic stimulation in single hippocampal spines. J Neurosci  24:2054–2064  2004.

[25] G. Mongillo and S. Deneve. Online learning with hidden Markov models. Neural Computation  20:1706–

1716  2008.

[26] O. Cappe and E. Moulines. Online EM algorithm for latent data models  2009.

9

,Remi Gribonval
Pierre Machart
Yanping Huang
Rajesh Rao