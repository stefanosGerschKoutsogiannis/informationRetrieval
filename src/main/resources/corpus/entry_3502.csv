2018,Model-based targeted dimensionality reduction for neuronal population data,Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use "targeted" approaches that work by identifying multiple  distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables  such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However  existing targeted methods have been developed outside of the confines of probabilistic modeling  making some aspects of the procedures ad hoc  or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data.  The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates  and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm  demixed PCA.,Model-based targeted dimensionality reduction for

neuronal population data

Mikio C. Aoi∗

Princeton Neuroscience Institute

Princeton University 
Princeton  NJ 08544

maoi@princeton.edu

Jonathan W. Pillow †

Princeton Neuroscience Institute

Princeton University 
Princeton  NJ 08544

pillow@princeton.edu

Abstract

Summarizing high-dimensional data using a small number of parameters is a
ubiquitous ﬁrst step in the analysis of neuronal population activity. Recently
developed methods use “targeted" approaches that work by identifying multiple 
distinct low-dimensional subspaces of activity that capture the population response
to individual experimental task variables  such as the value of a presented stimulus
or the behavior of the animal. These methods have gained attention because they
decompose total neural activity into what are ostensibly different parts of a neuronal
computation. However  existing targeted methods have been developed outside of
the conﬁnes of probabilistic modeling  making some aspects of the procedures ad
hoc  or limited in ﬂexibility or interpretability. Here we propose a new model-based
method for targeted dimensionality reduction based on a probabilistic generative
model of the population response data. The low-dimensional structure of our model
is expressed as a low-rank factorization of a linear regression model. We perform
efﬁcient inference using a combination of expectation maximization and direct
maximization of the marginal likelihood. We also develop an efﬁcient method
for estimating the dimensionality of each subspace. We show that our approach
outperforms alternative methods in both mean squared error of the parameter
estimates  and in identifying the correct dimensionality of encoding using simulated
data. We also show that our method provides more accurate inference of low-
dimensional subspaces of activity than a competing algorithm  demixed PCA.

1

Introduction

Neuroscience has recently seen a massive expansion in the number of neurons that can be recorded
from a single animal  largely due to transformative technological advancements in electrode design
and two-photon imaging. One of the effects of our increased measurement capacity is an increased
interest in the properties of the activity of groups of neurons (i.e. population activity)  as opposed
to analyzing the activity of single-neurons independently [1]. One goal of analyzing population
activity is to characterize the ways in which groups of neurons coordinate to perform task-relevant
computations.
Dimensionality reduction is central to the analysis of population activity [1]. Concomitant with the
broader use of classical dimensionality reduction methods like PCA and ICA come the recognition
that these methods often do not take full advantage of well-characterized properties of neuronal
population data such as tensor structure or temporal correlations in spike rates and a number of recent
data analysis techniques have been developed to improve our ability to meet such speciﬁc challenges

∗http://mikioaoi.com
†http://pillowlab.princeton.edu/jpillow/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

[2–7]. Of particular interest have been methods of dimensionality reduction for population data that
distinguish between the effects of various inputs and outputs  or "task variables " such as a stimulus
strength  an experimental context  or a behavioral outcome [8–11]. We will refer to these methods
collectively as "targeted" methods.
Although several targeted methods of dimensionality reduction exist  two recent methods stand
out among existing methods: demixed principle components analysis (dPCA) [8] and targeted
dimensionality reduction (TDR) [9]. Both of these methods were developed for the analysis of
neuronal population data that inherently have observations of neuronal activity structured as matrices
(ex. neurons by row  time by columns) and both attempt to identify low-dimensional subspaces that
best describe the population responses to an individual task variable.
The most recent version of dPCA [8] is a general method with relatively weak modeling assumptions 
arbitrary dimensionality  and a fast estimation algorithm based on low-rank regression [12]. However 
dPCA requires that all observed neurons display ﬁring rates for all possible combinations of task
variables  a condition that may be too strict to be applicable for complex experiments. In contrast 
TDR [9] utilizes a linear regression-based approach that circumvents the need to have observed every
neuron at every combination of task variables by imposing an explicit relationship between regressors
and outputs. However  the TDR method is limited to a one-dimensional subspace per task variable. It
is not clear that only one dimension is sufﬁcient to describe the population activity associated with a
given task variable. For example  sequential activation of neurons during decision making has been
observed in rodents  where the precise ordering of activations depends on which decision the animal
makes [13] and population code “morphing" has been observed in monkeys where decision encoding
changes over time [14]. These types of dynamic encoding schemes are inherently high-dimensional
and any method constrained to too-few dimensions will fail to fully characterize such activity. Lastly 
none of the existing methods have principled approaches to identifying the dimensionality of the data 
making post hoc analysis particularly difﬁcult.
Here we propose a model-based method for targeted dimensionality reduction based on an extension
of the framework proposed by [9]. Our approach overcomes a number of the drawbacks of existing
methods. Using a probabilistic generative model of the data  we can infer the optimal dimensionality
of the low-dimensional subspaces required to faithfully describe underlying latent structure in the
data. In the following  we describe the model  which we call model-based targeted dimensionality
reduction (MBTDR)  its assumptions  and an efﬁcient estimation procedure for model parameters
and dimensionality. We then demonstrate the accuracy of our estimation algorithm against alternative
methods of estimation.

2 Explicitly low-dimensional model of population activity

2.1 High-dimensional description

Our model begins with a description of trial-by-trial neuronal activity in terms of a linear regression
with respect to the task variables. We assume that the activity yi k(t) of the ith neuron at time t on
trial k can be described by a linear combination of P task variables x(p)
k   p = 1  . . .   P (ex. stimulus
variables  behavioral outcomes  and nonlinear combinations)  such that

k βi 2(t) + ··· + x(P )

yi k(t) = x(1)

k βi 1(t) + x(2)
where the values of the P task variables x(p)
are known  the βi p(t) are unknown coefﬁcients  and
k
i k(t) is noise. This basic model structure is identical to that of the regression model used in [9] and
has been successfully employed in characterizing neuronal activity of single neurons [15].
To represent all neurons simultaneously  we simply concatenate all i = 1  . . .   n responses into a
vector and write

k βi P (t) + i k(t).

yk(t) = x(1)

k β1(t) + x(2)

k β2(t) + ··· + x(P )

k βP (t) + k(t) 

where yk(t) = (y1 k(t)  . . .   yn k(t))(cid:62)  βp(t) = (β1 p(t)  . . .   βn p(t))(cid:62)  and k(t) =
(1 k(t)  . . .   n k(t))(cid:62).
Neuronal recordings are often performed in experiments where trial epochs are of ﬁxed dura-
tion. We can take advantage of this structure by regarding the observation on each trial to be

2

a matrix  Yk = (yk(1)  . . .   yk(T ))  which is a linear combination of P coefﬁcient matrices
Bp = (βp(1)  . . .   βp(T )) giving the observation model

k B2 + ··· + x(P )

Yk = x(1)

k B1 + x(2)

(1)
where Ek = (k(1)  . . .   k(T )). A schematic illustration of this basic setting is shown in Figure 1.
In general  not all neurons are observed simultaneously. Most often they are observed sequentially
or in sequential blocks. Suppose we do not observe all rows of Yk on all trials but instead observe
nk ≤ n neurons. If we let Yk be a latent matrix of all recorded neurons from all trials  then we can
describe the observed neurons on any given trial by Zk = HkYk  where Hk is a nk × n matrix
where each row is a 1-hot vector providing the index of an observed neuron.

k BP + Ek 

2.2 Low-dimensional description of observations

With no additional constraints our observation model (1) is extremely high dimensional and is
effectively a separate linear regression for each neuron at every time point. This would only be a
sensible model if we believed that neurons were not in fact coordinating activity between each other
or across time. We would like to be able to express the prior belief that there are correlations across
the population but that correlations in activity due to different values of stimuli are not necessarily
the same as those due to the behavior of the animal.
To accomplish this we can describe each characteristic response matrix Bp by a low-rank factorization 
i.e. Bp = WpSp  where Wp and Sp are n × rp and rp × T respectively  where rp = rank(Bp).
Equivalently  we can say that rp is the dimensionality of the encoding of task variable p. This
formulation has an intuitive interpretation  illustrated schematically in Figure 1A: the characteristic
i (t) of each neuron to the pth task variable can be expressed as a linear combination of rp
response βp
weighted basis functions βp
j (t)  where rp is the dimensionality of the encoding 
{s(p)
j (t)}rp
j=1 are neuron-dependent
mixing weights.
The example in Figure 1A displays a model with two task variables (x1  x2)  where the x1 subspace
is 1D and the x2 subspace is 2D. The columns of Wp’s weight each time-varying basis function
differently for each neuron. Collectively  these weights deﬁne the subspace of activity that encodes
task variable xp. For x1  the encoding is 1D because only one basis function is needed to describe
the population response to x1. The x2 response is slightly more complex  with different responses at
different times  requiring at least two basis functions.

j=1 are a common set of time-varying basis functions  and {w(p)

i (t) =(cid:80)rp

j=1 w(p)

i j }rp

i j s(p)

3 Model estimation

The goal of inference is to estimate the factors of Bp and the ranks rp. Our proposed estimation
strategy is to estimate one set of factors ({Wp} or {Sp}) while integrating out the other. For
example  if we deﬁne a prior over the mixing weights {Wp} denoted by p(W)  and a data likelihood
p(Z|W  S) then the marginal likelihood of the matrix of time-varying basis functions S can be
obtained by

(cid:90) ∞

−∞

p(Z|S  λ) =

p(Z|W  S  λ)p(W)dW.

In principle  either set of factors may be selected. In practice however the set of factors with lowest
dimension should be selected to keep computational costs low. In this paper we focus on the case
where T (cid:28) n and we therefore will estimate {Sp} while integrating over {Wp}. The fact that either
set of factors may be determined in this way means that there is a duality between rows and columns
imposed by this model that is similar in principle to the duality between factors and latent states for
probabilistic principle components analysis [16].
In practice inference can be considerably simpliﬁed if we let the noise distribution and prior distribu-
tion of W both be Gaussian  which permits closed-form expression of the marginal and posterior
densities. We will let all elements of W to be independent standard normal  (i.e.  w ∼ N (0  I˜rn)
In addition  we let the noise covariance on all trials be given by
Ek ∼ MN (0  D−1  IT )  where MN (M  A  B) denotes the matrix normal distribution with row
covariance A and column covariance B  and D ≡ diag(λ1  . . .   λn) where λi is the inverse noise

where ˜r = (cid:80)

p rank(Bp)).

3

Figure 1: A: Schematic illustration of low-rank regression model. The n × T response matrix can be
decomposed into two response matrices (β1  β2)  each corresponding to one task variable (upper panel).
Each response matrix can be factorized into a small number of row and columns vectors  making the
population response a linear combination of a small number of common basis functions weighted differently
for each neuron. B: Results of simulation study evaluating parameter estimation accuracy for different
estimation procedures. Legend indicates the method used. Abscissa indicates the number of trials used for
the simulations. Error bars indication 95% conﬁdence intervals over 100 runs. C: Duration of computation
for methods and trials used in B.

variance of neuron i. We therefore assume that the weights are a priori independent and that the
noise variance is independent across both neurons and time. In principle  our framework supports the
application of more structured priors and noise covariances but we will save the exploration of more
elaborate models for future work.

˜N =(cid:80)

3.1 Marginal likelihood of timecourses S
Since our model is linear and Gaussian  the marginalized density p(Z|S  λ) is also Gaussian and
can be easily derived using standard Gaussian identities [17]. However  a naive derivation of the
marginal likelihood requires the log determinant and inverse of a matrix which is ˜N T × ˜N T   where
i Ni  such that Ni is the number of observed trials for neuron i. Thus  if all neurons are
observed on all trials  then the dimensions of the marginal covariance will be nN T × nN T   which
can be prohibitively large for even moderately sized datasets since the determinant and inverse in
general will have computational complexity O( ˜N 3T 3). Luckily  the expression for the marginal
likelihood can be dramatically reduced by exploiting the factorization of regression parameters.
If we let S ≡ blkdiag(S1  . . .   SP )  and λ = (λ1  . . .   λn) then we can derive (see Supplementary
Material for details) the following expression for the marginal likelihood in terms of S and λ 

(cid:32)

n(cid:88)

(cid:0)−NiT log λi + λiyi

i S](cid:1)(cid:33)

.

(cid:96)(s  λ) = − 1

2

˜nT log 2π +

(cid:62)yi + log |Ci| − λ2

i Trace[RiS(cid:62)C−1

i=1

where the matrices Ri and Ci are deﬁned by

(2)

(cid:62) ⊗ IT )yiyi

(cid:62)(Xi ⊗ IT ) 

Ri = (Xi

(cid:62)Xi and yi = (cid:0)y1

(3)
respectively  where Xi is the Ni × P design matrix that only includes trials where neuron i was
observed  Ai = Xi
i being the length-T response of
neuron i on trial k.
The expression in (2) reveals two things about the structure of dependencies within the model. First 
we notice that the likelihood factorizes over neurons  making evaluation of the likelihood potentially

(cid:62)(cid:1) (cid:62)  with yk

(cid:62)  . . .   yN

Ci = λiS(Ai ⊗ IT )S(cid:62) + I˜r 

i

i

4

# trials102103MSE0.10.20.30.40.5SVDbilinearMMLEM# trials102103time (sec)10-1100ABC=+21time1T=+1212neurons1nhighly parallelizable. Second  the trace term is remenicent of the quadratic term of a matrix normal
model  indicating that we can intuitively think of the posterior covariance Ci and the rank-1 matrix
Ri as the neuron-dependent contributions to the row and column covariances of S  respectively.
Maximum marginal likelihood (MML) estimates for S and λi can be obtained by directly maximizing
(2) by gradient ascent.

3.2 Posterior distribution of neural weights W

Once an estimate of S and λ is obtained we can do posterior inference on W. Because our model is
linear and Gaussian  the posterior density p(W|Z  S  λ) is also Gaussian and admits closed-form
expressions for the posterior expectation and variance of W. Because of our low-rank model structure 
the posterior of the weight matrices {Wp} factorizes over neurons and we can estimate the weights
W for each neuron separately and achieve computational savings relative to joint estimation over all
neurons simultaneously.
We can deﬁne a ˜r × 1 vector ωi that contains all of the weights for neuron i. Collectively  the ωi can
be expressed as

 ω1

...
ωn

 = vec

 W1

...
WP

 .

(cid:62)

(cid:62)

This notation allows us to do efﬁcient posterior inference over ωi  where the posterior expectation
and covariance of ωi are given by

Eωi|S Z[ωi] = λiC−1

i S(Xi

(cid:62) ⊗ IT )ζi 

Covωi|S Z[ωi] = C−1

i

 

where Ci is deﬁned as in (3).

3.3 Decoding

Once estimates of Bp are obtained we can decode new trials using the observation likelihood. This is
a distinct feature of our method that is not available to dPCA and TDR. The former methods are used
for estimation of the encoding but must learn a separate decoder to decode task variables from the
activity. Because of the probabilistic formulation of our model we can do encoding and decoding
within the same framework  allowing us to directly ask questions about how the structure of the
encoding inﬂuences the ability of down-stream populations to decode the information in the recorded
population. While we do not pursue decoding further in this paper we included a description of the
optimal decoder in the Supplementary Material.

4 ECME algorithm for parameter estimation

In general  maximization of the marginal likelihood (2) can be relatively slow when the number of
parameters is large. We therefore derive a “expectation-conditional maximization  either" algorithm
(ECME) [18] where parameters are block-wise estimated by either maximizing the conditional
expectation of the complete data log likelihood or the marginal likelihood. Our algorithm has
closed-form updates for each parameter block.
Note that  for Bayesian linear regression with Gaussian likelihood and prior  an otherwise unstructured
model would have  for M parameters  an ECME update with computational complexity of O(M 3).
In contrast  due to the additional low-rank structure of our model  and despite each M-step updating
˜rT + n parameters  our M-step updates have computational complexity O(˜r3)  where there are
typically ˜r (cid:28) min{n  T}. This means that the actual computational cost of ECME is limited only by
the underlying dimensionality of the data  and not to the total number of parameters per se.
As we demonstrate in Section 6.1  while our EM algorithm provides parameter estimates that are
only slightly worse in mean-squared error as maximizing the marginal likelihood directly  this small
additional error has a serious impact on dimensionality estimation. We therefore use our ECME
algorithm to provide fast  high-quality initialization for maximizing the marginal likelihood by
gradient descent.

5

5 A greedy algorithm for rank estimation
While our model can identify subspaces of any dimension up to Dmax = min{n  T}  the dimension-
ality of each subspace must be speciﬁed a priori. Although we may use standard model selection
techniques to compare the goodness of ﬁt between models with alternative conﬁgurations an exhaus-
tive search over all possible models would require searching over DP
max possible conﬁgurations. We
therefore developed a greedy algorithm for estimating the optimal dimensionality. A summary of the
procedure is presented in Algorithm 1.
Recall that the dimensionality of each task-variable encoding corresponds to the rank of each Bp.
We begin the algorithm by ﬁrst estimating the model parameters with rank rp = 1 for all p (although
in principle we may start at rp = 0  denoting the null model for all elements of Bp)  giving us a
p=1 rp and at the ﬁrst iteration ˜r1 = P . At the jth iteration 
we estimate the parameters of P models  where each model has the dimension of one of the task
variables increased by 1  while keeping all other dimensionalities the same as in the previous iteration.
We then have P models  each with total dimensionality ˜rj+1 = ˜rj + 1. We then evaluate the AIC of
each of these P models and keep the model that displayed the greatest decrease in AIC relative the
the previous iteration for the next iteration. In this way we grow the total dimensionality of the model
by one on each iteration. The algorithm is formally outlined in Algorithm 1. 3

model with total dimensionality ˜r =(cid:80)P

Algorithm 1 Estimation of dimensionality
Let r ≡ (r1  . . .   rP )  ep is the elemental vector  AIC(r) is the Akaike information criterion for a
model with ranks r
1: procedure DIMEST(r0 Data)
r ← r0  AIC0 ← AIC(r0)
2:
repeat
3:
4:
5:
6:
7:
8:
9:

end for
if There is no p s.t. AICp < AIC0 then Break
end if
p∗ ← arg min
r0 ← r  AIC0 ← AICp∗

(cid:46) Calculate AIC for +1 rank for each task variable

for p = 1  . . .   P do

AICp ← AIC(r + ep)

(cid:46) +1 rank for variable that most decreases AIC

AICp  rp∗ ← rp∗ + 1

(cid:46) Initialize

p

10:
11:
12:
13: end procedure

until There is no p s.t. AICp < AIC0
return r

(cid:46) Stop when AIC can no longer be decreased

6 Simulation studies

6.1 Evaluation of parameter estimation with simulated data

We applied our greedy algorithm on simulated data in order to determine if it could accurately recover
the true ranks using n = 100 neurons and T = 15 time points. For each run of our simulations
we ﬁrst selected a random dimensionality between 1-6 for each of P = 3 task variables (two
graded variables with values drawn from {−2 −1  0  1  2} and one binary task variable with values
{−1  1}). Using these dimensionalities  the elements of Wp and Sp were drawn independently from
a N (0  1) distribution. To give us heterogeneous noise variances  the noise variance for each neuron
was drawn from an exponential distribution with mean parameter σ2 = 50. The resulting average
SNR for any one task variable was -0.26 (±0.75  log10 units). We then simulated observations
according to our model with varying numbers of trials (N ∈ {50  200  500  1000  1500  2000}). In
order to simulate incomplete observations  we set the probability of observing any given neuron on
any given trial to πobs = .4. While we conducted experiments with varying numbers of trials and
observation probabilities we generally found that decreased observation probabilities acted effectively
as a decrease in sample size with a concomitant decrease in estimation accuracy. The results were

3Demonstration

code

is

available

for

http://www.mikioaoi.com/samplecode/RDRdemo.zip

download

at

the

ﬁrst

author’s website

at

6

not particularly sensitive to the precise observation probability in this regime and we report only the
results for the settings listed above.
For each set of observations  we estimated the parameters of the model in one of three ways  which
we describe below.
We consider the following four methods for parameter optimization:
1. Linear regression and SVD. The elements of Bp for all p were estimated by linear regression for
each neuron and time point independently. Each estimate of the complete matrix Bp could then
(cid:62)  where Dp is the
be expressed by its singular value decomposition (SVD) as Bp = UpDpVp
n × T diagonal matrix of d = min{n  T} singular values. We then set the smallest d − rp singular
values to zero with the resulting matrix of rp nonzero singular values denoted by D(rp)
. The rank-rp
(cid:62)  with
estimates of Wp and Sp are then given by W(rp)
the corresponding rank-rp estimate of Bp given by B(rp)
The corresponding likelihood is given by

p = UpD(rp)1/2
p = W(rp)

and S(rp)

p = D(rp)1/2
.

p

p S(rp)

p

Vp

(cid:96)(Bp|Z  H(cid:48)  ˆD) ∝(cid:88)

Trace[(Zk −(cid:88)

x(p)Bp)(cid:62)H(cid:48)DH(cid:48)(cid:62)(Zk −(cid:88)

p

p

x(p)Bp]

(4)

k

p

p

2. Bilinear optimization. After initializing with the rank-rp estimates of Wp and Sp from the SVD
method  the parameters can be further reﬁned by bilinear regression. On each iteration  the values of
Wp’s are ﬁxed  which leads to closed-form updates for conditional maximum likelihood estimates
of Sp’s and vice versa. Thus  the algorithm will alternate between estimating Wps and Sps until
convergence. The bilinear regression method uses the same likelihood as shown in (4).
3. ECME. As described in the Supplementary Material.
4. Maximum marginal likelihood (MML). After initializing with the ECME estimates of Wp and Sp 
we estimate Sp by maximizing the marginal likelihood given by (2). No estimation of the Wp factors
is required since the marginal likelihood only depends on Sp.
For each setting of trial number N  we repeated this process 100 times and evaluated how well our
algorithm estimated the true model parameters. The results are presented in Figure 1B C.
We found that ECME and MML both produced mean-squared error (MSE) that was substantially
smaller at all sample sizes that either the SVD or bilinear methods. While the differences in MSE
between the ECME algorithm and MML were small  Figure 1C shows that the ECME algorithm was
substantially faster than either MML or bilinear regression.

6.2 Evaluation of dimension estimation with simulated data

For each of the 100 runs of our simulation experiments we also evaluated how well our algorithm
estimated the dimension of the characteristic responses by evaluating the difference between the true
and estimated dimension of each task variable and counting the number of times that difference was
observed. The results are presented in Figure 2A.
We found that all four methods tended to under-estimate the dimensionality as the number of trials
decreased but that this underestimation was less pronounced for the ECME and MML methods  for
which the vast majority of estimates resulted in the correct ranks even in the case of N = 50. Note
that not only is this half the number of trials as neurons but since each neuron was only observed on
about 40% of the trials this gives an average of only 20 trials per neuron. Therefore  our procedure
recovers the true rank of the model the vast majority of the time even under conditions of vary small
trial number relative to the size of observations.
We were surprised that despite the modest difference in MSE between the ECME and MML estimation
algorithms  the dimension estimation seemed to be sensitive to these differences  with the ECME
performing worse than MML despite the fact that these methods are in theory maximizing the same
objective function. Nevertheless we propose that  due to the ECME algorithm’s superior speed 
ECME be used as an efﬁcient initializer for MML estimation. We found that initializing the rank
estimates this way limits the use of MML for rank estimation to just a few iterations.

7

For neuroscience applications  the observed spike counts are better described by a Poisson distribution
than by a Gaussian. We therefore evaluated the robustness of our algorithm to this type of model
misspeciﬁcation by performing the same dimensionality estimation experiment with 2000 trials
with observations drawn from Poisson(y(t)) distribution at each time bin. Our results are virtually
indistinguishable from experiments using Gaussian observations (Fig. 2B  dashed line).

Figure 2: Simulation studies. A: Results of simulation study evaluating performance of Algorithm 1 for
dimensionality estimation by means of different parameter estimation procedures. The legend indicates the
sample size. Abscissa indicates the error in dimensionality estimate. Ordinate gives the number of estimated
subspaces that obtained the corresponding error. Dashed line indicates model-mismatch experiment with
Poisson observations and sample size 2000. B: Results of subspace estimation by our MML method
compared with dPCA. The MML method out-performs dPCA at all but the highest SNR  where performance
is similar.

7 Comparison with dPCA

7.1 Simulation experiments

The central goal of both our method and dPCA is to recover a basis that deﬁnes a set of low-
dimensional subspaces that describe how the population varies with respect to each task variable (or
pre-deﬁned combination of task variables). In order to compare the quality of the subspaces identiﬁed
by each method  we conducted a simple simulation study. The simulation setting was identical to
those described in Section 6.1 using 100 trials per run except that  to keep the simulations as simple
as possible  we deﬁned just two binary task variables that were drawn randomly on each trial. The
experiment was repeated for 100 runs.
We performed both dPCA and estimated the parameters on each run using MML and then compared
the % mean-squared error between the true subspace and the estimated subspaces. We deﬁned the
true subspace based on the left singular vectors of the Bp matrices used for simulating the data. If
U is the true subspace and ˆU is the estimated subspace then the % mean squared error is given by
(cid:107)U − ˆU ˆU(cid:62)U(cid:107)2
The basis for the subspace estimated by MBTDR can be obtained by ﬁrst estimating each Bp = WS 
where S was estimated by MML and W was estimated by its posterior mean. We then used the
left singular vectors of the estimated Bp to deﬁne the estimated basis. For the dPCA estimate  the
analogous subspace is deﬁned by their “encoding" subspace [8]. For both methods we assumed the
correct dimensionality. We used the version of dPCA that is for non-sequential estimation and uses
cross-validated regularization parameters. The results are presented in Figure 2B.
When the subspace is recoverable (i.e. principle angle is signiﬁcantly less than 90 degrees)  our
method is virtually always closer to the true subspace. It is notable that the principle angle is an
extremely sensitive measure of errors between subspaces and that both methods provide reasonable
results when checked by eye. It is also notable that any differences are observable at all  which give
us conﬁdence that these results are quite strong.

2/(cid:107)U(cid:107)2
2.

8

Bdim. estimation error-4-20Number of models050100150200250300SVD-4-20Bilinear-4-20ECME-4-20MML2000100050020050PoissA00.20.40.60.81subspace MSE (MBTDR)00.10.20.30.40.50.60.70.80.91subspace MSE (dPCA)0.3-0.7-1-1.3-1.7-2-2.3mean SNR (dB)8 Concluding remarks

We have introduced a new  model-based method to identify low-dimensional subspaces of neuronal
activity that describe the response of neuronal populations to variations in task variables. We have
also introduced a procedure of estimating both the parameters of this model and the dimensionality
of each of the corresponding subspaces of activity. We compared our method in simulations to dPCA
and showed that our method better recovers the low-dimensional subspace of activity for noisy data.
There are a number of additional advantages to using a model-based method for dimension reduction.
The ﬁrst is that our modeling framework is general enough that we could include even more structure
to the model such as structured prior covariances and noise covariance. Our modeling approach also
allows us to answer otherwise elusive questions about what quantities of the data are important. For
example  virtually all other targeted methods effectively use peri-stimulus time histograms (PSTH’s)
as the sufﬁcient statistics for subspace estimation. One interesting revelation of our model is that the
(cid:62)yi  Ni) and
PSTHs are not sufﬁcient statistics. The sufﬁcient statistics of our model are (IRi  Ai  yi
these sufﬁcient statistics can not be derived directly from the PSTHs. This suggests that methods that
rely solely on the PSTHs may not be capturing important characteristics of the data.

Acknowledgments

This work was supported by grants from the Simons Foundation (SCGB AWD1004351 and
AWD543027)  the NIH (R01EY017366  R01NS104899) and a U19 NIH-NINDS BRAIN Initiative
Award (NS104648-01).

References
[1] John P Cunningham and M Yu Byron. Dimensionality reduction for large-scale neural record-

ings. Nature neuroscience  17(11):1500–1509  2014.

[2] Jeffrey S Seely  Matthew T Kaufman  Stephen I Ryu  Krishna V Shenoy  John P Cunningham 
and Mark M Churchland. Tensor analysis reveals distinct population structure that parallels the
different computational roles of areas m1 and v1. PLoS Comput Biol  12(11):e1005164  2016.

[3] Ari S Morcos and Christopher D Harvey. History-dependent variability in population dynamics

during evidence accumulation in cortex. Nature Neuroscience  2016.

[4] B. M. Yu  J. P. Cunningham  G. Santhanam  S. I. Ryu  K. V. Shenoy  and M. Sahani. Gaussian-
process factor analysis for low-dimensional single-trial analysis of neural population activity.
Journal of Neurophysiology  102(1):614  2009.

[5] Yuan Zhao and Il Memming Park. Variational latent gaussian process for recovering single-trial

dynamics from population spike trains. arXiv preprint arXiv:1604.03053  2016.

[6] Afsheen Afshar  Gopal Santhanam  Byron M Yu  Stephen I Ryu  Maneesh Sahani  and Krishna V
Shenoy. Single-trial neural correlates of arm movement preparation. Neuron  71(3):555–564 
Aug 2011.

[7] Mark M. Churchland  Byron M. Yu  Maneesh Sahani  and Krishna V Shenoy. Techniques for
extracting single-trial activity patterns from large-scale neural recordings. Curr Opin Neurobiol 
17(5):609–618  Oct 2007.

[8] Dmitry Kobak  Wieland Brendel  Christos Constantinidis  Claudia E Feierstein  Adam Kepecs 
Zachary F Mainen  Xue-Lian Qi  Ranulfo Romo  Naoshige Uchida  and Christian K Machens.
Demixed principal component analysis of neural population data. eLife  5:e10989  2016.

[9] Valerio Mante  David Sussillo  Krishna V Shenoy  and William T Newsome. Context-dependent

computation by recurrent dynamics in prefrontal cortex. Nature  503(7474):78–84  2013.

[10] Christian K. Machens. Demixing population activity in higher cortical areas. Frontiers in

Computational Neuroscience  4(0)  2010.

9

[11] C.K. Machens  R. Romo  and C.D. Brody. Functional  but not anatomical  separation of "what"

and "when" in prefrontal cortex. The Journal of Neuroscience  30(1):350–360  2010.

[12] A.J. Izenman. Reduced-rank regression for the multivariate linear model. Journal of multivariate

analysis  5(2):248–264  1975.

[13] Christopher D Harvey  Philip Coen  and David W Tank. Choice-speciﬁc sequences in parietal

cortex during a virtual-navigation decision task. Nature  484(7392):62–68  2012.

[14] Aishwarya Parthasarathy  Roger Herikstad  Jit Hon Bong  Felipe Salvador Medina  Camilo
Libedinsky  and Shih-Cheng Yen. Mixed selectivity morphs population codes in prefrontal
cortex. Nature neuroscience  20(12):1770–1779  2017.

[15] Carlos D Brody  Adrián Hernández  Antonio Zainos  and Ranulfo Romo. Timing and neural
encoding of somatosensory parametric working memory in macaque prefrontal cortex. Cerebral
cortex  13(11):1196–1207  2003.

[16] N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent

variable models. The Journal of Machine Learning Research  6:1816  2005.

[17] C. M. Bishop. Pattern recognition and machine learning. Springer New York:  2006.

[18] Chuanhai Liu and Donald B Rubin. The ecme algorithm: a simple extension of em and ecm

with faster monotone convergence. Biometrika  81(4):633–648  1994.

10

,Mikio Aoi
Jonathan Pillow
Yilun Xu
Peng Cao
Yuqing Kong
Yizhou Wang