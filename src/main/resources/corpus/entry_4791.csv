2019,Robustness to Adversarial Perturbations in Learning from Incomplete Data,What is the role of unlabeled data in an inference problem  when the presumed underlying distribution is adversarially perturbed? To provide a concrete answer to this question  this paper unifies two major learning frameworks: Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). We develop a generalization theory for our framework based on a number of novel complexity measures  such as an adversarial extension of Rademacher complexity and its semi-supervised analogue. Moreover  our analysis is able to quantify the role of unlabeled data in the generalization under a more general condition compared to the existing theoretical works in SSL. Based on our framework  we also present a hybrid of DRL and EM algorithms that has a guaranteed convergence rate. When implemented with deep neural networks  our method shows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.,Robustness to Adversarial Perturbations

in Learning from Incomplete Data

Amir Najaﬁ

Department of Computer Engineering

Sharif University of Technology

Tehran  Iran

najafy@ce.sharif.edu

Shin-ichi Maeda

Preferred Networks  Inc.

Tokyo  Japan

ichi@preferred.jp

Masanori Koyama

Preferred Networks  Inc.

Tokyo  Japan

masomatics@preferred.jp

Takeru Miyato

Preferred Networks  Inc.

Tokyo  Japan

miyato@preferred.jp

Abstract

What is the role of unlabeled data in an inference problem  when the presumed
underlying distribution is adversarially perturbed? To provide a concrete answer to
this question  this paper uniﬁes two major learning frameworks: Semi-Supervised
Learning (SSL) and Distributionally Robust Learning (DRL). We develop a general-
ization theory for our framework based on a number of novel complexity measures 
such as an adversarial extension of Rademacher complexity and its semi-supervised
analogue. Moreover  our analysis is able to quantify the role of unlabeled data in the
generalization under a more general condition compared to the existing theoretical
works in SSL. Based on our framework  we also present a hybrid of DRL and
EM algorithms that has a guaranteed convergence rate. When implemented with
deep neural networks  our method shows a comparable performance to those of the
state-of-the-art on a number of real-world benchmark datasets.

1

Introduction

Robustness to adversarial attacks is an essential feature in the design of modern classiﬁers —in
particular  of deep neural networks [1  2]. Adversarial Training (AT) [3]  Virtual AT [4] and Distil-
lation [5] are examples of promising approaches to defend against a point-wise adversary who can
alter input data-points in a separate manner. However  as shown by [6]  a good defense against a
distributional adversary who shifts the input distribution instead of data-points could improve the
robustness of a classiﬁer more effectively. This has led to the development of Distributionally Robust
Learning (DRL) [7]  which has recently attracted intensive research interest [8  9  10  11]. Despite
of all the advancements in supervised DRL  the number of studies that tackle this problem from a
semi-supervised angle is slim to none [12]. Motivated by this fact  we propose a distributionally
robust framework that handles Semi-Supervised Learning (SSL) scenarios. Our work is an extension
of self-learning [13  14  15]  which encompasses methods such as Expectation-Maximization (EM)
algorithm. It can also cope with any existing classiﬁer such as neural networks. Intuitively  we ﬁrst
infer soft-labels for the unlabeled data  and then search for suitable classiﬁcation rules that show low
sensitivity to adversarial perturbation around these soft-label distributions.
Parts of this paper can be considered as a semi-supervised extension of [9]. Computational complexity
of our framework is comparable to those of its supervised rivals. Moreover  we design a Stochastic
Gradient Descent (SGD)-based algorithm with a guaranteed convergence rate to optimize our model.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Generalization Bound
Convergence Guarantee
Adversarial Robustness
Semi-Supervised Learning

DRL PL VAT SSDRL
(cid:88)
(cid:88)
(cid:88)
×

×
×
(cid:88)
(cid:88)

×
×
×
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table 1: Comparison between the
proposed framework (SSDRL) and
some existing methods: DRL of
[9]  Pseudo Labeling (PL) [19] 
and Virtual Adversarial Training
(VAT) [4].

In order to address the generalization  we introduce a set of novel complexity measures such as
Adversarial Rademacher Complexity and Minimum Supervision Ratio (MSR)  which are based on
the hypothesis set and input data distribution. We show that if the ratio of the labeled samples in a
dataset (supervision ratio) exceeds MSR  true adversarial risk can be bounded. Also  proper parameter
adjustment can arbitrarily decrease MSR at the cost of increasing the generalization bound; This
means our theoretical guarantees hold for all supervision ratios. The theoretical contribution of our
work is summarized in Table 1. We have also tested our method  denoted by SSDRL  via extensive
computer experiments on datasets such as MNIST [16]  SVHN [17]  and CIFAR-10 [18]. When
equipped with deep neural networks  SSDRL outperforms rivals such as Pseudo-Labeling (PL) [19]
and the supervised DRL of [9] on all the datasets. Also  SSDRL outperforms VAT [4] on SVHN 
while it demonstrates a comparable performance on MNIST and CIFAR-10.
The rest of the paper is organized as follows: Section 1.1 speciﬁes the notations  and Section 1.2
reviews the related works. The proposed framework is presented in Section 2  where numerical
optimization is explained in Section 2.1 and its generalization is addressed in Section 2.2. Section 3
is devoted to experimental results. Finally  Section 4 concludes the paper.

1.1 Notations
We extend the notations used in [9]. Assume Z to be an input space  Θ to be a parameter set  and
(cid:96) : Z × Θ → R a parametric loss function. Observation space Z can either be the feature space X
in unsupervised scenarios  or the space of feature-label pairs  i.e.  Z (cid:44) X × Y  where Y denotes
the set of labels. For simplicity  we only consider ﬁnite label-sets. By M (Z)  we mean the set of
all probability measures supported on Z. Let us denote the function c : Z × Z → [0  +∞) as the
transportation cost. Under some conditions on c  Deﬁnition B.1 (supplementary) formulates the
Wasserstein distance Wc (P  Q) between two distributions P  Q ∈ M (Z)  w.r.t. c [8]. Wc (P  Q)
measures the minimal cost of moving P to Q  where the cost of moving one unit of mass from z to z(cid:48)
is given by c (z  z(cid:48)). Also  for  ≥ 0 and a distribution Q ∈ M (Z)  we deﬁne an -ambiguity set as
B (Q) (cid:44) {P ∈ M (Z)| Wc (P  Q) ≤ }. Training dataset is shown by D (cid:44) {Z1  . . .   Zn}  which
includes i.i.d. samples drawn from a ﬁxed (and unknown) distribution P0 ∈ M (Z)  while n denotes
the dataset size. For a dataset D  let ˆPD ∈ M (Z) be ˆPD (cid:44) 1
i=1 δZi  where δz denotes the
Dirac delta function at point z ∈ Z. Accordingly  E and ˆED represent the statistical and empirical
expectation operators  respectively. For a distribution P ∈ M (X × Y)  PX denotes the marginal
y∈Y P (·  y) over X   and P|X ∈ M (Y) is the conditional distributions over
labels given feature vector X ∈ X . To simplify the notations  for z = (X  y) ∈ X × Y and a
function f  the notations f (z) and f (X  y) have been used  interchangeably.

distribution PX (·) (cid:44)(cid:80)

(cid:80)n

n

1.2 Background and Related Works

DRL minimizes a worst-case risk against an adversary  who has a limited budget to alter the data
distribution Q ∈ M (Z) in order to inﬂict the maximum possible damage. Here  Q can either be the
true measure P0  or the empirical one ˆPD [10]. Mathematically  DRL is formulated as [8  11]:

inf
θ∈Θ

sup

P∈B(Q)

EP {(cid:96) (Z; θ)} .

(1)

Wasserstein metric has been widely used to quantify the strength of adversarial attacks [8  9  11  12] 
thanks to (i) its relations to adversarial robustness [20] and (ii) suitable dual-form properties [11]. In
[8]  authors have reformulated DRL into a convex program for Logistic Regression. Convergence and
generalization of DRL  in a general context  have been addressed in [9]  while adjusting the ambiguity

2

set size  i.e.   has been tackled in [21]. In [22]  the authors have investigated the convergence of
the inner maximization of (1)  and its effects on the later stages of adversarial training. An analysis
on DRL methods with f-divergences is given in [10]. Also  sample complexity of DRL has been
reviewed by [23] and [24].
Abundance of unlabeled data has made SSL methods widely popular [4  25]. See [14] for a review
on classical SSL approaches. Many robust SSL algorithms have been proposed so far [26  27] 
however  their notion of robustness is different from the one considered here. More recent works
include [28  29  30  31]. In [29]  [30] and [31]  authors have mainly focused on a Gaussian model for
theoretical validation  while [28] empirically shows that robust self-training eliminates the accuracy-
robustness tradeoff by leveraging unlabeled data. In [32]  a pessimistic SSL approach is proposed
that provably enhances the performance by incorporating the unlabeled data.We show that a special
case of our method reduces to an adversarial extension of [32]. Guarantees on the generalization of
SSL can only be made under certain assumptions on the choice of hypothesis set and the true data
distribution [14  15  33]. For example  a compatibility function is introduced in [15] to restrict the
relation between the model set and input data distribution. Also  in [34]  author has theoretically
analyzed SSL under the cluster assumption.The main reason for making such assumptions is that
lack of any knowledge on the relation of a feature vector to its label  simply makes unlabeled data
useless for classiﬁcation. In Section 2.2  we propose a novel compatibility criterion under a general
setting which enables us to establish a generalization theory for our work.
Finally  the only existing work that also falls in the cross section of DRL and SSL is [12]. However 
this method severely restricts the shifted distributions  so that the adversary can only choose from a
set of delta-spikes over labeled and augmented unlabeled samples.

2 Proposed Framework
From now on  let Z (cid:44) X × Y. In SSL  a dataset D consists of two non-overlapping parts: Dl
(labeled) and Dul (unlabeled). Let us denote Il and Iul as the index sets corresponding to these parts 
respectively. Thus  we have Dl = {(X i  yi)| i ∈ Il}  and Dul = {X i| i ∈ Iul}. The hidden labels
in Dul can be modeled by random variables supported on Y. Note that DRL in (1) cannot readily
apply to this partially-labeled setting  since (1) needs complete knowledge of all the feature-label
pairs in D. To overcome this problem  ﬁrst let us make the following deﬁnition:
Deﬁnition 1. The consistent set of probability distributions ˆP (D) ⊆ M (Z) with respect to a
partially-labeled dataset D = Dl ∪ Dul is deﬁned as

ˆP (D) (cid:44)(cid:110)(cid:16) nl

(cid:17) ˆPDl +

n

(cid:16) nul

n

(cid:17) ˆPDul · Ω(cid:12)(cid:12) Ω ∈ MX (Y)
(cid:111)

 

where nl and nul (with n = nl + nul) are the sizes of Dl and Dul  respectively  and MX (Y) denotes
the set of all conditional distributions supported on Y  given features in X .
All possible (soft-)labelings of unlabeled samples in Dul are collected in ˆP (D). Note that the
empirical measure corresponding to the true complete dataset is also included in the consistent set.
Our aim is to choose a suitable measure from this set  and then use it for (1).
We take a known family of SSL approaches  called self-learning [13  35]  and then combine it with
DRL. Self-learning methods  e.g. EM algorithm [36]  transfer the knowledge from labeled samples to
unlabeled ones through pseudo-labeling. More precisely  a learner is trained on the supervised portion
of a dataset  and then employs its learned rules to assign pseudo-labels to the remaining unlabeled
part. However  such methods are prone to over-ﬁtting if the information ﬂow from Dl to Dul is
not properly controlled. One way to overcome this issue is to use soft-labeling  which maintains a
minimum level of uncertainty over unlabeled samples. By combining the above arguments with the
core idea of DRL in (1)  we propose the following learning scheme:

(cid:18) 1 − η

(cid:19)

λ

ˆEDul

(cid:1)(cid:9)(cid:41)

(cid:8)H(cid:0)S|X

 

(2)

(cid:40)

inf
θ∈Θ

inf

S∈ ˆP(D)

sup

P∈B(S)

EP {(cid:96) (X  y; θ)} +

where λ is a user-deﬁned parameter  η (cid:44) nl/n is the supervision ratio  and H (·) denotes the Shannon
entropy. For now  let us assume λ < 0.

3

(cid:8)H(cid:0)S|X

(cid:1)(cid:9) prevents hard decisions for

Minimization over S ∈ ˆP (D) acts as a knowledge transfer module that ﬁnds the optimal distribution
in ˆP (D). Again  note that distributions in ˆP (D) vary only in the way they assign (soft-)labels to
unlabeled data. The scheme in (2) is based on optimism in the sense that  for any θ ∈ Θ  learner
is instructed to pick the labels that are more likely to reduce the average loss function (cid:96) (·; θ) for
each unlabeled sample. This is the core idea of self-learning. However  a pessimistic learner does
the opposite  i.e. picks the less likely labels with large loss values and hence does not trust the
loss function. The negative regularization term 1−η
labels and promotes soft-labeling by bounding the Shannon entropy of label-conditionals from
below. A smaller |λ| gives softer labels. In the extreme case  choosing λ = −∞ ends up in an
adversarial version of the self-training in [14]. It should be noted that according to (2)  learner is
forced to show less sensitivity near all (labeled and unlabeled) training data  just as one expects from
a semi-supervised DRL.
We show that (2) can be efﬁciently solved given that some smoothness conditions hold for (cid:96) and c.
Before that  Theorem 1 shows that the optimization corresponding to the knowledge transfer module
has an analytic solution  which implies the computational cost of (2) is only slightly higher than those
of its fully-supervised counterparts  such as [9].
Theorem 1 (Lagrangian-Relaxation). For any continuous loss (cid:96) : Z×Θ → R and c : Z×Z → R≥0 
parameters  ≥ 0  γ ≥ 0 and λ ∈ R ∪ {±∞}  and a partially-labeled dataset D with size n  let us
deﬁne the empirical Semi-Supervised Adversarial Risk (SSAR)  denoted by ˆRSSAR (θ; D)  as

ˆEDul

λ

ˆRSSAR (θ; D) (cid:44) 1
n

φγ (X i  yi; θ) +

1
n

softmin

(λ)
y∈Y

{φγ (X i  y; θ)} + γ 

(3)

(cid:88)

i∈Il

(cid:88)

i∈Iul

where the adversarial loss φγ (X  y; θ)  and the soft-minimum operator softmin(λ)
q ∈ RY are deﬁned as

y∈Y (q) for any

 1

|Y|

(cid:88)

y∈Y

   (4)

eλqy

φγ (X  y; θ) (cid:44) sup
z(cid:48)∈Z

(cid:96) (z(cid:48); θ)−γc (z(cid:48)  (X  y))   and

softmin

(λ)
y∈Y

(q) (cid:44) 1
λ

log

respectively. Let θ∗ ∈ Θ be a minimizer of (2) for some given  ≥ 0 and λ < 0. Then  there exists
γ ≥ 0 such that θ∗ is also a minimizer of (3) with the same parameter setting.
Proof of Theorem 1 is given in Appendix D. Note that softmin equals to : (i) min operator for
λ = −∞  (ii) average for λ = 0  and (iii) max for λ = +∞. Also   and γ are non-negative dual
parameters and ﬁxing either of them uniquely determines the other one. Therefore  one can adjust γ
(for example via cross-validation)  instead of . See [9] for a similar discussion.
A more subtle look at (3) shows that in the dual context of the proposed scheme  one is free to also
consider positive values for λ. The sign of λ indicates optimism (λ ≤ 0)  or pessimism (λ > 0)
during the (soft-)label assignment. The choice between optimism vs. pessimism depends on the
compatibility of the model set Θ with the true distribution P0. In Section 2.2  we show that enabling
λ to take values in R rather than R− is crucial for establishing a generalization bound for (3). In
other words  for a very bad hypothesis set  one must choose to be pessimistic to be able to generalize
well. To see situations where pessimism in SSL helps  reader can refer to [32].

2.1 Numerical Optimization

We propose a numerical optimization scheme for solving (3) which has a convergence guarantee.
Lemmas E.1 and E.2 (supplementary) explicitly compute the gradients of (3). This way  one can
simply apply the mini-batch SGD to solve for (2) via Algorithm 1. Note that due to the strong
concavity property in Lemma E.1  δ can be chosen arbitrarily small. Other parameters such as γ
(or equivalently ) and λ should be adjusted via cross-validation. The computational complexity
of Algorithm 1 is no more than η + |Y| (1 − η) times of that of [9]  where the latter can only
handle supervised data1. Note that Algorithm 1 reduces to [9] in fully-supervised scenarios  and
coincides with Pseudo-Labeling and EM algorithm when (γ = ∞  λ = −∞) and (γ = ∞  λ = −1) 
1In scenarios where |Y| is very large  one can employ heuristic methods to reduce the set of possible labels

for an unlabeled data sample and gain more efﬁciency at the expense of degradation in performance

4

Algorithm 1 Stochastic Gradient Descent for SSDRL
1: Inputs: D  γ  λ  (k ≤ n  δ  α  T )
2: Initialize θ0 ∈ Θ  and set t ← 0.
3: for t = 0 → T − 1 do
Randomly select index set I ⊆ [n] with size k.
4:
for i ∈ Il ∩ I do
5:
Compute a δ-approx of z∗
6:
end for
7:
for (i  y) ∈ (Iul ∩ I) × Y do
8:
9:
end for
10:
Compute the sub-gradient of ˆRSSAR (θ; D) from (E.3) (Lemma E.2) at point θ = θt
11:
using only samples in I  and denote it with ∂θ ˆRSSAR (θt; D).
θt+1 ← ProjΘ
12:
13: end for
14: Output: θ∗ ← θT

Compute a δ-approximate of z∗

θt − α∂θ ˆRSSAR (θt; D)

i (y; θt) from Lemma E.2.

i (θt) from Lemma E.2.

(cid:16)

(cid:17)

respectively. The following theorem guarantees the convergence of Algorithm 1 to a local minimizer
of (3).
Theorem 2. Assume loss function (cid:96)  transportation cost c  γ ≥ 0 and |λ| < ∞ satisfy the conditions
of Lemma E.2. Also  assume (cid:96) is differentiable w.r.t. both z and θ  with Lipschitz gradients. Also 
let (cid:107)∇θ(cid:96) (z; θ)(cid:107)2 ≤ σ for some σ ≥ 0 all over Z × Θ. Denote θ0 ∈ Θ to be an initial hypothesis 
and θ∗ ∈ Θ as a local minimizer of (3). Assume the partially-labeled dataset D includes n i.i.d.
training samples. Also  let ∆ ˆR (cid:44) ˆRSSAR (θ0; D) − ˆRSSAR (θ∗; D). Then  for a ﬁxed step size α∗ 
the outputs of Algorithm 1 with parameters k = 1  δ > 0  α = α∗ after T iterations  say θ1  . . .   θT  
satisfy the following inequality:

E(cid:110)(cid:13)(cid:13)∇θ ˆRSSAR (θt; D)(cid:13)(cid:13)2

(cid:111) ≤ 4σ2

2

T(cid:88)

t=1

1
T

(cid:115)

(cid:18) B
(cid:19)
σ2 + (1 − η)|λ||Y|

∆ ˆR
T

+ Cδ 

(5)

where constants B and C and step size α∗ only depend on γ and Lipschitz constants of (cid:96).
The proof of Theorem 2 with explicit formulations for constants B and C and step size α∗ are

given in Appendix D (supplementary). Theorem 2 guarantees a convergence rate of O(cid:0)T −1/2(cid:1) for

Algorithm 1  if one neglects δ. Note that the presence of δ is necessary since one cannot ﬁnd the
exact maximizer of (E.2) in ﬁnite steps. However  due to Lemma E.1  δ can become inﬁnitesimally
small. Theorem D.1 (supplementary) guarantees the convergence of Algorithm 1 in hard-decision
regimes  i.e. λ = ±∞. Note that (cid:96) is not necessarily convex w.r.t. θ  e.g. neural nets. However  given
a convex loss (cid:96)  Theorem D.2 gives us a conditions on λ to guarantee the convexity of (3) as well.

2.2 Generalization Guarantee

EP {(cid:96) (Z; θ∗)}  where θ∗ denotes the
We intend to bound the true adversarial risk  i.e. supP∈B(P0)
optimizer of the empirical risk in (3). However  the two major concerns are: (i) we are training our
model against an adversary  and (ii) our training dataset is partially labeled. We ﬁrst address these
issues and then present our main contribution in Theorem 3.
Classical Rademacher complexity  denoted by Rn (F)  measures how well a function set F can learn
noise  and thus is exposed to over-ﬁtting on small datasets. We give a novel adversarial extension for
Rn which also appears in our generalization bound. Moreover  we show it converges to zero when
n → ∞  for all function sets with a ﬁnite VC-dimension. Before that  let us deﬁne the set of -Monge
maps as A (cid:44) {a : Z → Z| c (z  a (z)) ≤   ∀z ∈ Z}.

5

Deﬁnition 2 (Semi-Supervised Monge (SSM) Rademacher Complexity). For Z (cid:44) X × Y  assume a
function set F ⊆ RZ and a distribution P0 ∈ M (Z). For  ≥ 0 and n ∈ N  let us deﬁne

gl (n) (cid:44) EZ1:n σ

gul (n) (cid:44)(cid:88)

EX 1:n σ

1
n

sup
f∈F

(cid:40)

and

(cid:40)

n(cid:88)

i=1

(cid:20)
n(cid:88)

σi

sup
a∈A

(cid:20)

σi

1
n

sup
f∈F

(cid:21)(cid:41)

f (a (Zi))

(cid:21)(cid:41)

sup
a∈A

f (a (X i  y))

 

where Z1:n
Rademacher variables. Then  for η ∈ [0  1]  the SSM Rademacher complexity of F is deﬁned as

y∈Y
i.i.d.∼ P0X . σ ∈ {−1  +1}n indicates a vector of i.i.d. symmetric

i.i.d.∼ P0 and X 1:n

i=1

R(SSM)
n ( η) (F) (cid:44) ηgl ((cid:100)nη(cid:101)) + (1 − η) gul ((cid:100)n (1 − η)(cid:101)) .

By setting  = 0 and η = 1  the above deﬁnition reduces to the classical Rademacher complexity
Rn. We deﬁne a function set to be learnable  if limn→∞ Rn = 0. Similarly  a function class F is
said to be adversarially learnable w.r.t. parameters (  η)  if limn→∞ R(SSM)
n ( η) (F) = 0. But  how
can we numerically compute this measure in practice? The main difference between Rn and SSM
Rademacher complexity is that the latter adversarially alters the input distribution. However  many
distribution-free bounds already exist for Rn [37]  which apply to almost all practical function sets 
e.g. classiﬁers with a bounded VC-dimension (e.g. neural nets)  restricted regression tools and etc.
We show that by having a distribution-free bound on the Rademacher complexity of F  one can
also bound the SSM Rademacher complexity. Mathematically speaking  assume that there exists
an asymptotically decreasing upper-bound ∆ (n) such that Rn (F) ≤ ∆ (n)   ∀P0 ∈ M (X × Y).
Then  for all η ∈ [0  1] and  ≥ 0  we have (Lemma E.3):

R(SSM)
n ( η) (F) ≤ η∆ ((cid:100)nη(cid:101)) + (1 − η)|Y| ∆ ((cid:100)n (1 − η)(cid:101))  

(6)
where the r.h.s. of (6) goes to zero as n → ∞. This includes almost all practical classiﬁers  e.g.
neural nets  support vector machines  random forests and etc. For example  consider the 0− 1 loss for
a classiﬁer with a VC-dimension of dim (Θ). Then  due to Dudley’s entropy bound and Haussler’s
upper-bound [37]  there exists constant C such that  regardless of  or P0  we have (Lemma E.3):

(cid:114)

(cid:16)√

η +(cid:112)1 − η |Y|(cid:17)

.

(7)

∆ (n) ≤ C

dim (Θ)

n

  and thus R(SSM)

n ( η) (F) ≤ C

dim (Θ)

n

(cid:114)

2.2.1 Minimum Supervision Ratio

As discussed earlier  generalization of SSL frameworks generally requires a compatibility assumption
on the hypothesis set F and data distribution P0. In Appendix C (and in particular  Deﬁnition
C.4)  we introduce a new compatibility function  denoted by MSR  which has the following form:
MSR(F  P0) (λ  margin) : R∪{±∞}×R≥0 → [0  1]. Intuitively  MSR(F  P0) quantiﬁes the strength
of information theoretic relation between the marginal measure P0X and the conditional P0|X . It
also measures the richness of F to learn such relations. Due to Theorem 3  in order to bound the
true risk when unlabeled data are involved  one needs η ≥ MSR(F  P0) (λ  margin)  for some λ and
margin ≥ 0. Here  λ denotes the pessimism of the learner and margin ≥ 0 speciﬁes a safety margin
for small-size datasets. MSR is an increasing function w.r.t. margin  while it decreases with λ. In
particular  MSR(F  P0) (+∞  margin) = 0  for all margin ≥ 0.
For a negative λ (optimistic learning)  MSR remains small as long as there exists a strong dependency
between P0X and label conditionals P0|X . This dependency can be obtained  for example  by the
cluster assumption. Additionally  some loss functions in F need to be capable of capturing such
dependency  e.g. by resembling the true negative log-likelihood − log P0 (X  y). Conversely  absence
of such properties will increase the MSR toward 1  which forces the learner to choose a large λ (in the
extreme case +∞) in order to use the bound of Theorem 3. Not to mention that a large λ increases
the empirical loss and loosens the bound. This fact  however  should not be surprising since improper
usage of unlabeled data in certain cases can degrade the generalization. Lemma C.3 (supplementary)
shows that one can analytically compute MSR function for a particular case of interest  i.e. when
cluster assumption holds for P0 and the loss function family F is chosen properly.
This way  the following Theorem gives a generalization bound for (3):

6

(a) MNIST

(b) SVHN

(c) CIFAR-10

Figure 1: Comparison of the test error-rates on adversarial examples attained via [9] among different
methods.

and θ ∈ Θ let φγ (z; θ) to be deﬁned as in (4)  and Φ (cid:44)(cid:8)φγ (·; θ) (cid:12)(cid:12) θ ∈ Θ(cid:9). For a supervision ratio

Theorem 3 (Generalization). For a space Z (cid:44) X × Y  assume the set of continuous functions
L (cid:44) {(cid:96) (·; θ)| θ ∈ Θ}  with (cid:96) (·; θ) : Z → R and (cid:107)(cid:96)(cid:107)∞ ≤ B for some B ≥ 0. For γ ≥ 0  z ∈ Z
η ∈ [0  1]  assume a partially labeled dataset D = {(X i  yi)}n
i=1 including n i.i.d. samples drawn
from P0 ∈ M (Z)  where labels can be observed with probability of η  independently. For 0 < δ ≤ 1
and λ ∈ R ∪ {±∞}  assume η satisﬁes the following condition:

(cid:33)
n ( η) (L)
Then  with probability at least 1 − δ  the following bound holds for all  ≥ 0:

η ≥ MSR(Φ P0)

(cid:114)

(cid:32)

λ  4B

log (1/δ)

+ 4R(SSM)

.

2n

(cid:114)

(8)

(9)

sup

P∈B(P0)

EP {(cid:96) (Z; θ∗)} ≤ min
θ∈Θ

ˆRSSAR (θ; D) + 2B

log (1/δ)

2n

+ 2R(SSM)

n ( η) (L)  

where θ∗ is the minimizer of ˆRSSAR (θ; D).
Proof of Theorem 3 is given in Appendix D. Condition in (8) can always be satisﬁed based on
Lemma C.2  as long as λ and n are sufﬁciently large and L is adversarially learnable. A strongly-
compatible pair (Φ  P0) encourages optimism  where learner can choose a negative λ. However  in
some situations increasing λ might be necessary for (8) to hold; In fact  for a weakly-compatible
(Φ  P0)  λ must be positive or even +∞ (the latter always satisﬁes (8) regardless of n or η). Note
that a larger λ increases the empirical risk ˆRSSAR (θ∗; D)  which also increases the bound in (9).
Interestingly  λ = +∞ coincides with the setting of [32]  which makes it as a special case of our
analysis. The limiting cases of Theorem 3  i.e.  = 0 and η = 1  lead to a new bound for non-robust
SSL  and an existing bound for the supervised DRL of [9]  respectively.

3 Experimental Results

This section demonstrates our experimental results on some real-world datasets  and also compares
SSDRL with its state-of-the-art rival methodologies. Deep Neural Networks (DNN) are considered
for the loss {(cid:96) (·; θ)|θ ∈ Θ}. Architecture and other speciﬁcations about our DNNs are explained in
details in Appendix A. The rival frameworks in this section are Virtual Adversarial Training (VAT)
[4]  Pseudo-Labeling (PL) [19]  and the supervised DRL of [9]  which we simply denote as DRL. We
have also implemented a fast version of SSDRL  called F-SSDRL  where for each unlabeled training
sample considers only a limited number of more favorable labels in Algorithm 1. Here  by more
favorable labels  we refer to those labels that result in a smaller non-robust loss (cid:96) (·; θ). As a result 
F-SSDRL runs much faster than SSDRL without much degradation in performance. Surprisingly  we
found out that F-SSDRL often yields even better performances in practice compared to SSDRL (see
Appendix A for more details).
Figure 1 shows the misclassiﬁcation rate vs. γ−1 on adversarial test examples attained by computing
φγ (·; θ) (same attack strategy as [9]). Recall γ as the dual-counterpart of the Wasserstein radius  in
(2). Thus  γ−1 somehow quantiﬁes the strength of the adversarial attacks  as suggested by [9]. Results

7

1011001011021/eval020406080Error rate (%)DRL  0DRL  1DRL  2PLVAT  0=1VAT  2F-SSDRL  0F-SSDRL  1F-SSDRL  21011001011021/eval203040506070Error rate (%)DRL  0=1DRL  2PLVAT  0VAT  1=2F-SSDRL  0=1F-SSDRL  21011001011021/eval30405060Error rate (%)DRL  0DRL  1DRL  2PLVAT  0=1=2F-SSDRL  0F-SSDRL  1F-SSDRL  2(a) MNIST

(b) SVHN

(c) CIFAR-10

Figure 2: Comparison of the test error-rates on adversarial examples calculated by PGM [38]  under
(cid:96)2-norm constraint.

Table 2: Test error-rates on clean
examples. For DRL  VAT and F-
SSDRL  rows 1 to 3 correspond to
the parameter (γi for DRL and F-
SSDRL  and εi for VAT) that yields
the lowest error rates on: (i = 1)
clean examples  (i = 2) adversarial
examples by [9]  and (i = 3) ad-
versarial examples by PGM  respec-
tively.

Method

Test Error-Rate(%)

MNIST

SVHN

CIFAR-10

DRL
γ1
γ2
γ3
PL
VAT
ε1
ε2
ε3
F-SSDRL
γ1
γ2
γ3

4.67±0.38
4.77±0.15
5.95±0.13
8.70±0.47

1.30±0.04
1.30±0.04
1.32±0.10

1.29±0.09
1.51±0.03
3.58±1.13

10.89±0.53
10.89±0.53
14.45±0.93
6.39±0.46

5.47±0.33
7.01±0.24
7.01±0.24

6.19±0.22
6.19±0.22
6.74±0.22

21.62±0.40
23.77±0.65
21.97±0.35
21.19±0.25

15.19±0.55
15.19±0.55
15.19±0.55

17.94±0.20
18.35±0.24
18.64±0.16

have been depicted for MNIST  SVHN and CIFAR-10 datasets. Figure 2 demonstrates the same
procedure for adversarial examples generated by Projected-Gradient Method (PGM) [38]; In this
case  the error-rate is depicted vs. PGM’s strength of attack  i.e. ε. For VAT and SSDRL  curves have
been shown for different choices of hyper-parameters  i.e.  (γi or εi)   i = 1  2  3  which correspond
to the lowest error rates on: (i = 1) clean examples  (i = 2) adversarial examples by [9]  and (i = 3)
adversarial examples by PGM  respectively. Values of (γi  εi)  and the choices of λ  transportation
cost c  and the supervision ratio η with more details on the experiments can be found in Appendix A.
According to Figures 1 and 2  the proposed method is always superior to DRL and PL. Also  SSDRL
outperforms VAT on SVHN dataset regardless of the attack type  while it has a comparable error-rate
on MNIST and CIFAR-10 based on Figures 1a and 2c  respectively. The superiority over DRL
highlights the fact that exploitation of unlabeled data has improved the performance. However 
SSDRL under-performs VAT on MNIST and CIFAR-10 datasets if the order of attacks are reversed 
even though performances are still close. According to Figure 2a  accuracy of PL degrades quite
slowly as PGM’s ε increases  although the loss values increase in Figure A.7a. This phenomenon is
due to the fact that the adversarial directions for increasing the loss and error-rate are not correlated
in this particular case.
Table 2 shows the test error-rates on clean examples for F-SSDRL  VAT  PL and DRL on MNIST 
SVHN and CIFAR-10 datasets. In fact  Table 2 quantiﬁes the non-adversarial generalization that one
can attain in practice via distributional robustness. Again  F-SSDRL outperforms both PL and DRL
in all experimental settings. It also surpasses VAT on SVHN dataset. F-SSDRL under-performs VAT
on MNIST and CIFAR-10  however  the differences between error-rates remain small which means
the two methods have comparable performances.

8

101100101102eval01020304050Error rate (%)DRL  0DRL  1DRL  2PLVAT  0=1VAT  2F-SSDRL  0F-SSDRL  1F-SSDRL  2101100101102eval20406080Error rate (%)DRL  0=1DRL  2PLVAT  0VAT  1=2F-SSDRL  0=1F-SSDRL  2101100101102eval204060Error rate (%)DRL  0DRL  1DRL  2PLVAT  0=1=2F-SSDRL  0F-SSDRL  1F-SSDRL  24 Conclusions

This paper investigates the application of distributionally robust learning in partially labeled datasets.
The core idea is to take a well-known semi-supervised framework  known as self-learning  and
make it robust to adversarial attacks. A novel framework  called SSDRL  has been proposed which
encompasses many existing methods such as Pseud-Labeling (PL) and EM algorithm as its special
cases. Computational complexity of our method is shown to be comparable with its supervised
counterparts. We have derived convergence and generalization guarantees for SSDRL  where for the
latter  a number of novel complexity measures are proposed. In particular  an adversarial extension of
Rademacher complexity is proposed and shown to converge to zero for almost all practical learning
frameworks  including neural networks  that have a ﬁnite VC-dimension. Moreover  our theoretical
analysis reveals a more general and fundamental condition to assess the role of unlabeled data
in generalization by introducing a new complexity measure called Minimum Supervision Ratio
(MSR). This is in contrast to many existing works that need more restrictive assumptions  such
as cluster assumption to be applicable. Computer simulation on real-world datasets demonstrate
a comparable-to-superior performance for SSDRL compared with those of the state-of-the-art. In
future  we try to improve the generalization  for example  by empirically computing the MSR. Also 
ﬁtting more SSL methods into the core idea of our work is another research direction.

References
[1] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus  “Intrigu-
ing properties of neural networks ” in International Conference on Learning Representations 
2014.

[2] A. Nguyen  J. Yosinski  and J. Clune  “Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images ” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  2015  pp. 427–436.

[3] I. Goodfellow  J. Shlens  and C. Szegedy  “Explaining and harnessing adversarial examples ” in

International Conference on Learning Representations  2015.

[4] T. Miyato  S. Maeda  S. Ishii  and M. Koyama  “Virtual adversarial training: a regularization
method for supervised and semi-supervised learning ” IEEE transactions on pattern analysis
and machine intelligence  2018.

[5] N. Papernot  P. McDaniel  X. Wu  S. Jha  and A. Swami  “Distillation as a defense to adver-
sarial perturbations against deep neural networks ” in Security and Privacy (SP)  2016 IEEE
Symposium on.

IEEE  2016  pp. 582–597.

[6] M. Staib and S. Jegelka  “Distributionally robust deep learning as a generalization of adversarial

training ” in NIPS workshop on Machine Learning and Computer Security  2017.

[7] A. Ben-Tal  D. Den Hertog  A. De Waegenaere  B. Melenberg  and G. Rennen  “Robust solutions
of optimization problems affected by uncertain probabilities ” Management Science  vol. 59 
no. 2  pp. 341–357  2013.

[8] S. Shaﬁeezadeh-Abadeh  P. M. Esfahani  and D. Kuhn  “Distributionally robust logistic regres-

sion ” in Advances in Neural Information Processing Systems  2015  pp. 1576–1584.

[9] A. Sinha  H. Namkoong  and J. Duchi  “Certiﬁable distributional robustness with principled

adversarial training ” in International Conference on Learning Representations  2018.

[10] W. Hu  G. Niu  I. Sato  and M. Sugiyama  “Does distributionally robust supervised learning give
robust classiﬁers?” in International Conference on Machine Learning  2018  pp. 2034–2042.
[11] P. M. Esfahani and D. Kuhn  “Data-driven distributionally robust optimization using the wasser-
stein metric: Performance guarantees and tractable reformulations ” Mathematical Program-
ming  pp. 1–52  2017.

[12] J. Blanchet and Y. Kang  “Semi-supervised learning based on distributionally robust optimiza-

tion ” arXiv preprint arXiv:1702.08848  2017.

[13] Y. Grandvalet and Y. Bengio  “Semi-supervised learning by entropy minimization ” in Advances

in Neural Information Processing Systems  2005  pp. 529–536.

[14] X. Zhu  “Semi-supervised learning literature survey ” Computer Science  University of

Wisconsin-Madison  vol. 2  no. 3  p. 4  2006.

9

[15] O. Chapelle  B. Scholkopf  and A. Zien  “Semi-supervised learning (chapelle  o. et al.  eds.;
2006)[book reviews] ” IEEE Transactions on Neural Networks  vol. 20  no. 3  pp. 542–542 
2009.

[16] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document

recognition ” Proceedings of the IEEE  vol. 86  no. 11  pp. 2278–2324  1998.

[17] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng  “Reading digits in nat-
ural images with unsupervised feature learning ” in NIPS Workshop on Deep Learning and
Unsupervised Feature Learning  vol. 2011  2011  p. 5.

[18] A. Krizhevsky and G. Hinton  “Learning multiple layers of features from tiny images ” Citeseer 

Tech. Rep.  2009.

[19] D.-H. Lee  “Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep
neural networks ” in ICML Workshop on Challenges in Representation Learning  vol. 2  2013.
[20] Z. Cranko  A. K. Menon  R. Nock  C.-S. Ong  Z. Shi  and C. Walder  “Monge blunts bayes:
Hardness results for adversarial training ” in International Conference on Machine Learning 
2019  pp. 1406–1415.

[21] J. Duchi  P. Glynn  and H. Namkoong  “Statistics of robust optimization: A generalized

empirical likelihood approach ” arXiv preprint arXiv:1610.03425  2016.

[22] Y. Wang  X. Ma  J. Bailey  J. Yi  B. Zhou  and Q. Gu  “On the convergence and robustness of
adversarial training ” in International Conference on Machine Learning  2019  pp. 6586–6595.
[23] L. Schmidt  S. Santurkar  D. Tsipras  K. Talwar  and A. Madry  “Adversarially robust general-
ization requires more data ” in Advances in Neural Information Processing Systems  2018  pp.
5014–5026.

[24] D. Cullina  A. N. Bhagoji  and P. Mittal  “PAC-learning in the presence of evasion adversaries ”

arXiv preprint arXiv:1806.01471  2018.

[25] Z. Dai  Z. Yang  F. Yang  W. W. Cohen  and R. R. Salakhutdinov  “Good semi-supervised
learning that requires a bad gan ” in Advances in Neural Information Processing Systems  2017 
pp. 6510–6520.

[26] A. Balsubramani and Y. Freund  “Scalable semi-supervised aggregation of classiﬁers ” in

Advances in Neural Information Processing Systems  2015  pp. 1351–1359.

[27] Y. Yan  Z. Xu  I. W. Tsang  G. Long  and Y. Yang  “Robust semi-supervised learning through
label aggregation.” in Association for the Advancement of Artiﬁcial Intelligence  2016  pp.
2244–2250.

[28] A. Raghunathan  S. M. Xie  F. Yang  J. Duchi  and P. Liang  “Adversarial training can hurt gen-
eralization ” in ICML Workshop on Identifying and Understanding Deep Learning Phenomena 
2019.

[29] R. Zhai  T. Cai  D. He  C. Dan  K. He  J. Hopcroft  and L. Wang  “Adversarially robust

generalization just requires more unlabeled data ” arXiv preprint arXiv:1906.00555  2019.

[30] Y. Carmon  A. Raghunathan  L. Schmidt  P. Liang  and J. C. Duchi  “Unlabeled data improves
adversarial robustness ” in Advances in Neural Information Processing Systems (NeurIPS) 
2019.

[31] R. Stanforth  A. Fawzi  P. Kohli et al.  “Are labels required for improving adversarial robustness?”

arXiv preprint arXiv:1905.13725  2019.

[32] M. Loog  “Contrastive pessimistic likelihood estimation for semi-supervised classiﬁcation ”
IEEE transactions on pattern analysis and machine intelligence  vol. 38  no. 3  pp. 462–475 
2016.

[33] A. Singh  R. Nowak  and X. Zhu  “Unlabeled data: Now it helps  now it doesn’t ” in Advances

in Neural Information Processing Systems  2009  pp. 1513–1520.

[34] P. Rigollet  “Generalization error bounds in semi-supervised classiﬁcation under the cluster

assumption ” Journal of Machine Learning Research  vol. 8  no. Jul  pp. 1369–1392  2007.

[35] M.-R. Amini and P. Gallinari  “Semi-supervised logistic regression ” in European Conference

on Artiﬁcial Intelligence  2002  pp. 390–394.

10

[36] S. Basu  A. Banerjee  and R. Mooney  “Semi-supervised clustering by seeding ” in International

Conference on Machine Learning  2002  pp. 27–34.

[37] M. Mohri  A. Rostamizadeh  and A. Talwalkar  Foundations of machine learning. MIT press 

2012.

[38] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu  “Towards deep learning models
resistant to adversarial attacks ” in International Conference on Learning Representations  2018.

11

,Amir Najafi
Shin-ichi Maeda
Masanori Koyama
Takeru Miyato