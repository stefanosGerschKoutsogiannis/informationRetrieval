2013,Learning Feature Selection Dependencies in Multi-task Learning,A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However  expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed  additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore  the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies  but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered  only from the training data.,Learning Feature Selection Dependencies in

Multi-task Learning

Daniel Hern´andez-Lobato
Computer Science Department

Universidad Aut´onoma de Madrid
daniel.hernandez@uam.es

Jos´e Miguel Hern´andez-Lobato

Department of Engineering
University of Cambridge
jmh233@cam.ac.uk

Abstract

A probabilistic model based on the horseshoe prior is proposed for learning de-
pendencies in the process of identifying relevant features for prediction. Exact
inference is intractable in this model. However  expectation propagation offers
an approximate alternative. Because the process of estimating feature selection
dependencies may suffer from over-ﬁtting in the model proposed  additional data
from a multi-task learning scenario are considered for induction. The same model
can be used in this setting with few modiﬁcations. Furthermore  the assumptions
made are less restrictive than in other multi-task methods: The different tasks
must share feature selection dependencies  but can have different relevant features
and model coefﬁcients. Experiments with real and synthetic data show that this
model performs better than other multi-task alternatives from the literature. The
experiments also show that the model is able to induce suitable feature selection
dependencies for the problems considered  only from the training data.

1

Introduction

Many linear regression problems are characterized by a large number d of features or explaining
attributes and by a reduced number n of training instances. In this large d but small n scenario
there is an inﬁnite number of potential model coefﬁcients that explain the training data perfectly
well. To avoid over-ﬁtting problems and to obtain estimates with good generalization properties  a
typical regularization is to assume that the model coefﬁcients are sparse  i.e.  most coefﬁcients are
equal to zero [1]. This is equivalent to considering that only a subset of the features or attributes
are relevant for prediction. The sparsity assumption can be introduced by carrying out Bayesian
inference under a sparsity enforcing prior for the model coefﬁcients [2  3]  or by minimizing a loss
function penalized by some sparse regularizer [4  5]. Among the priors that enforce sparsity  the
horseshoe has some attractive properties that are very convenient for the scenario described [3]. In
particular  this prior has heavy tails  to model coefﬁcients that signiﬁcantly differ from zero  and an
inﬁnitely tall spike at the origin  to favor coefﬁcients that take negligible values.
The estimation of the coefﬁcients under the sparsity assumption can be improved by introducing
dependencies in the process of determining which coefﬁcients are zero [6  7]. An extreme case of
these dependencies appears in group feature selection methods in which groups of coefﬁcients are
considered to be jointly equal or different from zero [8  9]. However  a practical limitation is that
the dependency structure (groups) is often assumed to be given. Here  we propose a model based on
the horseshoe prior that induces the dependencies in the feature selection process from the training
data. These dependencies are expressed by a correlation matrix that is speciﬁed by O(d) parameters.
Unfortunately  the estimation of these parameters from the training data is difﬁcult since we consider
n < d instances only. Thus  over-ﬁtting problems are likely to appear. To improve the estimation
process we assume a multi-task learning setting  where several learning tasks share feature selection
dependencies. The method proposed can be adapted to such a scenario with few modiﬁcations.

1

Traditionally  methods for multi-task learning under the sparsity assumption have considered com-
mon relevant and irrelevant features among tasks [8  10  11  12  13  14]. Nevertheless  recent re-
search cautions against this assumption when the supports and values of the coefﬁcients for each
task can vary widely [15]. The model proposed here limits the impact of this problem because it is
has fewer restrictions. The tasks used for induction can have  besides different model coefﬁcients 
different relevant features. They must share only the dependency structure for the selection process.
The model described here is most related to the method for sparse coding introduced in [16]  where
spike-and-slab priors [2] are considered for multi-task linear regression under the sparsity assump-
tion and dependencies in the feature selection process are speciﬁed by a Boltzmann machine. Fitting
exactly the parameters of a Boltzmann machine to the observed data has exponential cost in the num-
ber of dimensions of the learning problem. Thus  when compared to the proposed model  the model
considered in [16] is particularly difﬁcult to train. For this  an approximate algorithm based on
block-coordinate optimization has been described in [17]. The algorithm alternates between greedy
MAP estimation of the sparsity patterns of each task and maximum pseudo-likelihood estimation of
the Boltzmann parameters. Nevertheless  this algorithm lacks a proof of convergence and we have
observed that is prone to get trapped in sub-optimal solutions.
Our experiments with real and synthetic data show the better performance of the model proposed
when compared to other methods that try to overcome the problem of different supports among
tasks. These methods include the model described in [16] and the model for dirty data proposed
in [15]. These experiments also illustrate the beneﬁts of the proposed model for inducing depen-
dencies in the feature selection process. Speciﬁcally  the dependencies obtained are suitable for the
multi-task learning problems considered. Finally  a difﬁculty of the model proposed is that exact
Bayesian inference is intractable. Therefore  expectation propagation (EP) is employed for efﬁcient
approximate inference. In our model EP has a cost that is O(Kn2d)  where K is the number of
learning tasks  n is the number of samples of each task  and d is the dimensionality of the data.
The rest of the paper is organized as follows: Section 2 describes the proposed model for learning
feature selection dependencies. Section 3 shows how to use expectation propagation to approximate
the quantities required for induction. Section 4 compares this model with others from the literature
on synthetic and real data regression problems. Finally  Section 5 gives the conclusions of the paper
and some ideas for future work.

2 A Model for Learning Feature Selection Dependencies

We describe a linear regression model that can be used for learning dependencies in the process
of identifying relevant features or attributes for prediction. For simplicity  we ﬁrst deal with the
case of a single learning task. Then  we show how this model can be extended to address multi-
task learning problems. In the single task scenario we consider some training data in the form of
n d-dimensional vectors summarized in a design matrix X = (x1  . . .   xn)T and associated targets
y = (y1  . . .   yn)T  with yi ∈ R. A linear predictive rule is assumed for y given X. Namely 
y = Xw +   where w is a vector of latent coefﬁcients and  is a vector of independent Gaussian
noise with variance σ2  i.e.   ∼ N (0  σ2I). Given X and y  the likelihood for w is:
N (yi|wTxi  σ2) = N (y|Xw  σ2I) .

p(yi|xi  w) =

p(y|X  w) =

n(cid:89)

n(cid:89)

(1)

i=1

i=1

Consider the under-determined scenario n < d. In this case  the likelihood is not strictly concave
and inﬁnitely many values of w ﬁt the training data perfectly well. A strong regularization technique
that is often used in this context is to assume that only some features are relevant for prediction [1].
This is equivalent to assuming that w is sparse with many zeros. This inductive bias can be naturally
incorporated into the model using a horseshoe sparsity enforcing prior for w [3].
The horseshoe prior lacks a closed form but can be deﬁned as a scale mixture of Gaussians:
j τ 2)C+(λj|0  1) dλj  

N (wj|0  λ2

p(wj|τ ) =

p(w|τ ) =

p(wj|τ )  

(2)

d(cid:89)

(cid:90)

j=1

where λj is a latent scale for coefﬁcient wj  C+(·|0  1) is a half-Cauchy distribution with zero loca-
tion and unit scale and τ > 0 is a global shrinkage parameter that controls the level of sparsity. The

2

smaller the value of τ the sparser the prior and vice-versa. Figure 1 (left) and (middle) show a com-
parison of the horseshoe with other priors from the literature. The horseshoe has an inﬁnitely tall
spike at the origin which favors coefﬁcients with small values  and has heavy tails which favor co-
efﬁcients that take values that signiﬁcantly differ from zero. Furthermore  assume that τ = σ2 = 1
j ). Then  the posterior mean for wj is (1 − κj)yj  where
and that X = I  and deﬁne κj = 1/(1 + λ2
κj is a random shrinkage coefﬁcient that can be interpreted as the amount of weight placed at the
origin [3]. Figure 1 (right) shows the prior density for κj that results from the horseshoe. It is from
the shape of this ﬁgure that the horseshoe takes its name. We note that one expects to see two things
under this prior: relevant coefﬁcients (κj ≈ 0  no shrinkage)  and zeros (κj ≈ 1  total shrinkage).
The horseshoe is therefore very convenient for the sparse inducing scenario described before.

Figure 1: (left) Density of different priors  horseshoe  Gaussian  Student-t and Laplace near the
origin. Note the inﬁnitely tall spike of the horseshoe. (middle) Tails of the different priors considered
before. (right) Prior density of the shrinkage parameter κj for the horseshoe prior.

A limitation of the horseshoe is that it does not consider dependencies in the feature selection pro-
cess. Speciﬁcally  the fact that one feature is actually relevant for prediction has no impact at all
in the prior relevancy or irrelevancy of other features. We now describe how to introduce these
dependencies in the horseshoe. Consider the deﬁnition of a Cauchy distribution as the ratio of two
independent standard Gaussian random variables [18]. An equivalent representation of the prior is:

p(w|ρ2  γ2) =

N (wj|0  u2

j )N (uj|0  ρ2)N (vj|0  γ2) dujdvj .

j /v2

(3)

(cid:90) d(cid:89)

j=1

(cid:90)  d(cid:89)

j=1

where uj and vj are latent variables introduced for each dimension j. In particular  λj = ujγ/vjρ.
Furthermore  τ has been incorporated into the prior for uj and vj using τ 2 = ρ2/γ2. The latent
variables uj and vj can be interpreted as indicators of the relevance or irrelevance of feature j. The
larger u2
A simple way of introducing dependencies in the feature selection process is to consider correlations
among variables uj and vj  with j = 1  . . .   d. These correlations can be introduced in (3) as follows:

j  the more relevant the feature. Conversely  the larger v2

j   the more irrelevant.

N (u|0  ρ2C)N (v|0  γ2C) dudv  

(4)

p(w|ρ2  γ2  C) =

N (wj|0  u2

j /v2
j )

where u = (u1  . . .   ud)T  v = (v1  . . .   vd)T  C is a correlation matrix that speciﬁes the dependen-
cies in the feature selection process  and ρ2 and γ2 act as regularization parameters that control the
level of sparsity. When C = I  (4) factorizes and gives the same prior as the one in (2) and (3). In
practice  however  C has to be estimated from the data. This can be problematic since it will involve
the estimation of O(d2) free parameters which can lead to over-ﬁtting. To alleviate this problem and
also to allow for efﬁcient approximate inference we consider a special form for C:
M11  . . .   1/

(5)
where diag(a1  . . .   ad) denotes a diagonal matrix with entries a1  . . .   ad; D is a diagonal matrix
whose entries are all equal to some small positive constant (this matrix guarantees that C−1 exists);
the products by ∆ ensure that the entries of C are in the range (−1  1); and P is a d × m matrix
of real entries which speciﬁes the correlation structure of C. Thus  C is fully determined by P and
will only have O(md) free parameters with m < d. The value of m is a regularization parameter
that limits the complexity of C. The larger its value  the more expressive C is. For computational
reasons described later on we will set in our experiments m equal to n  the number of data instances.

M = (D + PPT)  

∆ = diag(1/

C = ∆M∆  

(cid:112)

(cid:112)

Mdd)  

3

−3−2−101230.00.10.20.30.40.50.60.7Prob.DensityHorseshoeGaussianStudent−t(df=1)Laplace45670.0000.0050.0100.0150.0200.025Prob.DensityHorseshoeGaussianStudent−t(df=1)Laplace0.00.20.40.60.81.0012345Prob.Density2.1

Inference  Prediction and Learning Feature Selection Dependencies

Denote by z = (wT  uT  vT)T the vector of latent variables of the model described above. Based on
the formulation of the previous section  the joint probability distribution of y and z is:

N(cid:0)wj|0  u2

j /v2
j

(cid:1) . (6)

d(cid:89)

j=1

p(y  z|X  σ2  ρ2  γ2  C) = N (y|Xw  σ2I)N (u|0  ρ2C)N (v|0  γ2C)

Figure 2 shows the factor graph corresponding to this joint probability distribution. This graph
summarizes the interactions between the random variables in the model. All the factors in (6) are
Gaussian  except the ones corresponding to the prior for wj given uj and vj  N (wj|0  u2
Given the observed targets y one is typically interested in inferring the latent variables z of the
model. For this  Bayes’ theorem can be used:

j ).
j /v2

p(z|X  y  σ2  ρ2  γ2  C) =

p(y  z|X  σ2  ρ2  γ2  C)
p(y|X  σ2  ρ2  γ2  C)

 

(7)

where the numerator in the r.h.s. of (7) is the joint distribution (6) and the denominator is simply a
normalization constant (the model evidence) which can be used for Bayesian model selection [19].
The posterior distribution in (7) is useful to compute a predictive distribution for the target ynew
associated to a new unseen data instance xnew:

(cid:90)

p(ynew|xnew  X  σ2  ρ2  γ2  C) =

p(ynew|xnew  w) p(z|X  σ2  ρ2  γ2  C) dz .

(8)

Similarly  one can marginalize (7) with respect to w to obtain a posterior distribution for u and v
which can be useful to identify the most relevant or irrelevant features.
Ideally  however  one should also infer C  the cor-
relation matrix that describes the dependencies in
the feature selection process  and compute a pos-
terior distribution for it. This can be complicated 
even for approximate inference methods. Denote
by Z the model evidence  i.e.  the denominator in
the r.h.s. of (7). A simpler alternative is to use
gradient ascent to maximize log Z (and therefore
Z) with respect to P  the matrix that completely
speciﬁes C. This corresponds to type-II maxi-
mum likelihood (ML) estimation and allows to
determine P from the training data alone  without
resorting to cross-validation [19]. The gradient of
log Z with respect to P  i.e.  ∂ log Z/∂P can be
used for this task. The other hyper-parameters of
the model σ2  ρ2 and γ2 can be found following
a similar approach.
Unfortunately  neither (7)  (8) nor the model ev-
idence can be computed in closed form. Specif-
ically  it is not possible to compute the required
integrals analytically. Thus  one has to resort to
approximate inference. For this  we use expecta-
tion propagation [20]. See Section 3 for details.

Figure 2:
the probabilistic
model. The factor f (·) corresponds to the likeli-
hood N (y|Xw  σ2I)  and each gj(·) to the prior
for wj given uj and vj  N (wj|0  u2
j ). Finally 
hu(·) and hv(·) correspond to N (u|0  ρ2C) and
N (v|0  γ2C)  respectively. Only the targets y are
observed  the other variables are latent.

Factor graph of

j /v2

2.2 Extension to the Multi-Task Learning Setting

In the single-task learning setting maximizing the model evidence with respect to P is not expected
to be effective to improve the prediction accuracy. The reason is the difﬁculty of obtaining an ac-
curate estimate of P. This matrix has m × d free parameters and these have to be induced from a
small number of n < d training instances. The estimation process is hence likely to be affected by
over-ﬁtting. One way to mitigate over-ﬁtting problems is to consider additional data for the estima-
tion process. These additional data may come from a multi-task learning setting  where there are K

4

.........related but different tasks available for induction. A simple assumption is that all these tasks share a
common dependency structure C for the feature selection process  although the model coefﬁcients
and the actual relevant features may differ between tasks. This assumption is less restrictive than as-
suming jointly relevant and irrelevant features across tasks and can be incorporated into the learning
process using the described model with few modiﬁcations. By using the data from the K tasks for
the estimation of P we expect to obtain better estimates and to improve the prediction accuracy.
Assume there are K learning tasks available for induction and that each task k = 1  . . .   K consists
of a design matrix Xk with nk d-dimensional data instances and target values yk. As in (1)  a linear
predictive rule with additive Gaussian noise σ2
k is considered for each task. Let wk be the model
coefﬁcients of task k. Assume for the model coefﬁcients of each task a horseshoe prior as the one
speciﬁed in (4) with a shared correlation matrix C  but with task speciﬁc hyper-parameters ρ2
k and
k. Denote by uk and vk the vectors of latent Gaussian variables of the prior for task k. Similarly  let
γ2
zk = (wT
k)T be the vector of latent variables of task k. Then  the joint posterior distribution
of the latent variables of the different tasks factorizes as follows:

k  uT

k  vT

p(cid:0){z}K

k=1|{Xk  yk  τ 2

k   ρ2

k  σ2

p(yk  zk|Xk  σ2
p(yk|Xk  σ2
k  ρ2

k  ρ2

k  γ2

k  C)

k  γ2

k  C)

 

(9)

k=1  C(cid:1) =

k}K

K(cid:89)
k  C) =(cid:81)K

k=1

k  ρ2
k  ρ2

r.h.s. of (9)  i.e.  ZMT =(cid:81)K

where each factor in the r.h.s. of (9) is given by (7). This indicates that the K models for each task
k ∀k. Denote by ZMT the denominator in the
can be learnt independently given C and σ2
k=1 p(yk|Xk  σ2
k=1 Zk  with Zk the evidence for task
k. Then  ZMT is the model evidence for the multi-task setting. As in single-task learning  speciﬁc
values for the hyper-parameters of each task and C can be found by a type-II maximum likelihood
(ML) approach. For this  log ZMT is maximized using gradient ascent. Speciﬁcally  the gradient
of log ZMT with respect to σ2
k and P can be easily computed in terms of the gradient of
each log Zk. In summary  if there is a method to approximate the required quantities for learning
a single task using the model proposed  implementing a multi-task learning method that assumes
shared feature selection dependencies but task dependent hyper-parameters is straight-forward.

k and γ2
k  γ2

k  γ2

k  ρ2

3 Approximate Inference

d(cid:89)

Expectation propagation (EP) [20] is used to approximate the posterior distribution and the evidence
of the model described in Section 2. For the clarity of presentation we focus on the model for a single
learning task. The multi-task extension of Section 2.2 is straight-forward. Consider the posterior
distribution of z  (6). Up to a normalization constant this distribution can be written as

p(z|X  y  σ2  ρ2  γ2) ∝ f (w)hu(u)hv(v)

gj(z)  

(10)

j=1

are Gaussian. EP approximates (10) by a distribution q(z) ∝ f (w)hu(u)hv(v)(cid:81)d

where the factors in the r.h.s. of (10) are displayed in Figure 2. Note that all factors except the gj’s
j=1 ˜gj(z)  which
is obtained by replacing each non-Gaussian factor gj in (10) with an approximate factor ˜gj that is
Gaussian but need not be normalized. Since the Gaussian distribution belongs to the exponential
family of distributions  which is closed under the product and division operations [21]  q is Gaussian
with natural parameters equal to the sum of the natural parameters of each factor.
EP iteratively updates each ˜gj until convergence by ﬁrst computing q\j ∝ q/˜gj and then minimiz-
ing the Kullback-Leibler (KL) divergence between gjq\j and qnew  KL(gjq\j||qnew)  with respect to
j = sjqnew/q\j  where sj is the normalization
qnew. The new approximate factor is obtained as ˜gnew
constant of gjq\j. This update rule ensures that ˜gj looks similar to gj in regions of high posterior
probability in terms of q\j [20]. Minimizing the KL divergence is a convex problem whose optimum
is found by matching the means and the covariance matrices between gjq\j and qnew. These expec-
tations can be readily obtained from the derivatives of log sj with respect to the natural parameters
of q\j [21]. Unfortunately  the computation of sj is intractable under the horseshoe. As a practical
alternative  our EP implementation employes numerical quadrature to evaluate sj and its derivatives.
Importantly  gj  and therefore ˜gj  depend only on wj  uj and vj  so a three-dimensional quadrature

5

will sufﬁce. However  using similar arguments to those in [7] more efﬁcient alternatives exist. As-
sume that q\j(wj  uj  vj) = N (wj|mj  ηj)N (uj|0  νj)N (vj|0  ξj)  i.e.  q\j factorizes with respect
to wj  uj and vj and that the mean of uj and vj is zero. Since gj is symmetric with respect to uj
and vj then E[uj] = E[vj] = E[ujvj] = E[ujwj] = E[vjwj] = 0 under gjq\j. Thus  if the initial
approximate factors ˜gj factorize with respect to wj  uj and vj  and have zero mean with respect to
uj and vj  any updated factor will also satisfy these properties and q\j will have the assumed form.
The crucial point here is that the dependencies introduced by gj do not lead to correlations that need
to be tracked under a Gaussian approximation. In this situation  the integral of gjq\j with respect to
wj is given by the convolution of two Gaussians and the integral of the result with respect to uj and
vj can be simpliﬁed using arguments similar to those employed to obtain (3). Namely 

(cid:90)

(cid:18)

(cid:19)

sj =

N

mj|0 

νj
ξj

λ2
j + ηj

C+(λj|0  1)dλj  

(11)

where mj  ηj  νj and ξj are the parameters of q\j. The derivatives of log sj with respect to the
natural parameters of q\j can also be evaluated using a one-dimensional quadrature. Therefore 
each update of ˜gj requires ﬁve quadratures: one to evaluate sj and four to evaluate its derivatives.
Instead of sequentially updating each ˜gj  we follow [7] and update these factors in parallel. For this 
we compute all q\j at the same time and update each ˜gj. The marginals of q are strictly required
for this task. These can be efﬁciently obtained using the low rank representation structure of the
covariance matrix of q that results from the fact that all the ˜gj’s are factorizing univariate Gaussians
and from the assumed form for C in (5). Speciﬁcally  if m (the number of columns of P) is equal
to n  the cost of this operation (and hence the cost of EP) is O(n2d). Lastly  we damp the update of
each ˜gj as follows: ˜gj = (˜gnew
respectively denote the new and the
old ˜gj  and α ∈ [0  1] is a parameter that controls the amount of damping. Damping signiﬁcantly
improves the convergence of EP and leaves the ﬁxed points of the algorithm invariant [22].
After EP has converged  q can be used instead of the exact posterior in (8) to make predictions.
Similarly  the model evidence in (7) can be approximated by ˜Z  the normalization constant of q:

j )1−α  where ˜gnew

)α(˜gold

and ˜gold
j

j

j

(cid:90)

d(cid:89)

˜Z =

f (w)hu(u)hv(v)

˜gj(z)dwdudv .

(12)

j=1

Since all the factors in (12) are Gaussian  log ˜Z can be readily computed and maximized with respect
to σ2  ρ2  γ2 and P to ﬁnd good values for these hyper-parameters. Speciﬁcally  once EP has
converged  the gradient of the natural parameters of the ˜gj’s with respect to these hyper-parameters
is zero [21]. Thus  the gradient of log ˜Z with respect to σ2  ρ2  γ2 and P can be computed in terms
of the gradient of the exact factors. The derivations are long and tedious and hence omitted here 
but by careful consideration of the covariance structure of q it is possible to limit the complexity of
these computations to O(n2d) if m is equal to n. Therefore  to ﬁt a model that maximizes log ˜Z we
alternate between running EP to obtain the estimate of log ˜Z and its gradient  and doing a gradient
ascent step to maximize this estimate with respect to σ2  ρ2  γ2 and P. The derivation details of the
EP algorithm and an R-code implementation of it can be found in the supplementary material.

4 Experiments

We carry out experiments to evaluate the performance of the model described in Section 2. We refer
to this model as HSDep. Other methods from the literature are also evaluated. The ﬁrst one  HSST 
is a particular case of HSDep that is obtained when each task is learnt independently and correlations
in the feature selection process are ignored (i.e.  C = I). A multi-task learning model  HSMT  which
assumes common relevant and irrelevant features among tasks is also considered. The details of
this model are omitted  but it follows [10] closely. It assumes a horseshoe prior in which the scale
parameters λj in (2) are shared among tasks  i.e.  each feature is either relevant or irrelevant in all
tasks. A variant of HSM T   SSMT  is also evaluated. SSMT considers a spike-and-slab prior for joint
feature selection across all tasks  instead of a horseshoe prior. The details about the prior of SSMT
are given in [10]. EP is used for approximate inference in both HSMT and SSMT. The dirty model 
DM  described in [15] is also considered. This model assumes shared relevant and irrelevant features

6

among tasks. However  some tasks are allowed to have speciﬁc relevant features. For this  a loss
function is minimized via combined (cid:96)1 and (cid:96)1/(cid:96)∞ block regularization. Particular cases of DM are
the lasso [4] and the group lasso [8]. Finally  we evaluate the model introduced in [16]. This model 
BM  uses spike-and-slab priors for feature selection and speciﬁes dependencies in this process using
a Boltzmann machine. BM is trained using the approximate block-coordinate algorithm described
in [17]. All models considered assume Gaussian additive noise around the targets.

4.1 Experiments with Synthetic Data

k and γ2

k and γ2

In HSST ρ2

k and γ2

Method
HSST
HSMT
SSMT
DM
BM
HSDep

A ﬁrst batch of experiments is carried out using synthetic data. We generate K = 64 dif-
ferent tasks of n = 64 samples and d = 128 features.
In each task  the entries of Xk are
sampled from a standard Gaussian distribution and the model coefﬁcients  wk  are all set to
zero except for the i-th group of 8 consecutive coefﬁcients  with i chosen randomly for each
task from the set {1  2  . . .   16}. The values of these 8 non-zero coefﬁcients are uniformly dis-
tributed in the interval [−1  1]. Thus  in each task there are only 8 relevant features for pre-
diction. Given each Xk and each wk  the targets yk are obtained using (1) with σ2
k = 0.5
∀k. The hyper-parameters of each method are set as follows:
k are found
by type-II ML. In HSMT ρ2 and γ2 are set to the average value found
by HSST for ρ2
k  respectively. In SSMT the parameters of the
spike-and-slab prior are found by type-II ML. In HSDep m = n.
Furthermore  ρ2
k take the values found by HSST while P is
obtained using type-II ML. In all models we set the variance of the
noise for task k  σ2
k  equal to 0.5. Finally  in DM we try different
hyper-parameters and report the best results observed. After train-
ing each model on the data  we measure the average reconstruction
error of wk. Denote by ˆwk the estimate of the model coefﬁcients
for task k (this is the posterior mean except in BM and DM). The
reconstruction error is measured as || ˆwk − wk||2/||wk||2  where
|| · ||2 is the (cid:96)2-norm and wk are the exact coefﬁcients of task k.
Figure 3 (top) shows the average reconstruction error of each
method over 50 repetitions of the experiments described. HSDep ob-
tains the lowest error. The observed differences in performance are
signiﬁcant according to a Student’s t-test (p-value < 5%). BM per-
forms worse than HSDep because the greedy MAP estimation of the
sparsity patterns of each task is sometimes trapped in sub-optimal
solutions. The poor results of HSMT  SSMT and DM are due to the
assumption made by these models of all tasks sharing relevant fea-
tures  which is not satisﬁed. Figure 3 (bottom) shows the average
entries in absolute value of the correlation matrix C estimated by
HSDep. The matrix has a block diagonal form  with blocks of size
8 × 8 (8 is the number of relevant coefﬁcients in each task). Thus 
within each block the corresponding latent variables uj and vj are
strongly correlated  indicating jointly relevant or irrelevant features.
This is the expected estimation for the scenario considered.

Figure 3: (top) Average recon-
struction error of each method.
(bottom) Average absolute value
of the entries of the matrix C es-
timated by HSDep in gray scale
(white=0 and black=1). Black
squares are groups of jointly rel-
evant / irrelevant features.

Error

0.29±0.01
0.38±0.03
0.77±0.01
0.37±0.01
0.24±0.02
0.21±0.01

4.2 Reconstruction of Images of Hand-written Digits from the MNIST

A second batch of experiments considers the reconstruction of images of hand-written digits ex-
tracted from the MNIST data set [23]. These images are in gray scale with pixel values between 0
and 255. Most pixels are inactive and equal to 0. Thus  the images are sparse and suitable to be
reconstructed using the model proposed. The images are reduced to size 10× 10 pixels and the pixel
intensities are normalized to lie in the interval [0  1]. Then  K = 100 tasks of n = 75 samples each
are generated. For this  we randomly choose 50 images corresponding to the digit 3 and 50 images
corresponding to the digit 5 (these digits are chosen because they differ signiﬁcantly). Similar results
(not shown) to the ones reported here are obtained for other pairs of digits. For each task  the entries
of Xk are sampled from a standard Gaussian. The model coefﬁcients  wk  are simply the pixel
values of each image (i.e.  d = 100). Importantly  unlike in the previous experiments  the model
coefﬁcients are not synthetically generated but correspond to actual images. Furthermore  since the

7

tasks contain images of different digits they are expected to have different relevant features. Given
k = 0.1 ∀k. The objective is to reconstruct
Xk and wk  the targets yk are generated using (1) with σ2
wk from Xk and yk for each task k. The hyper-parameters are set as in Section 4.1 with σ2
k = 0.1
∀k. The reconstruction error is also measured as in that section.
Figure 4 (top) shows the average reconstruction error of each method over 50 repetitions of the ex-
periments described. Again  HSDep performs best. Furthermore  the differences in performance are
also statistically signiﬁcant. The second best result corresponds to HSMT  probably due to back-
ground pixels which are irrelevant in all the tasks and to the heavy-tails of the horseshoe prior.
HSST  SSM T   BM and DM perform signiﬁcantly worse. DM performs poorly probably because of
the inferior shrinking properties of the (cid:96)1 norm compared to the horseshoe [3]. The poor results of
SSMT are due to the lack of heavy-tails in the spike-and-slab prior. In BM we have observed that
the greedy MAP estimation of the task supports is more frequently trapped in sub-optimal solutions.
Furthermore  the algorithm described in [17] fails to converge most times in this scenario. Figure 4
(right  bottom) shows a representative subset of the images reconstructed by each method. The best
reconstructions correspond to HSDep. Finally  Figure 4 (left  bottom) shows in gray scale the average
correlations in absolute value induced by HSDep for the selection process of each pixel of the image
with respect to the selection of a particular pixel which is displayed in green. Correlations are high
to avoid the selection of background pixels and to select pixels that actually correspond to the digits
3 and 5. The correlations induced are hence appropriate for the multi-task problem considered.

HSST

0.36±0.02

HSMT

0.25±0.02

SSMT

0.39±0.01

DM

0.37±0.01

BM

0.52±0.03

HSDep

0.20±0.01

Error

Figure 4: (top) Average reconstruction error each method. (left  bottom) Average absolute value correlation in
a gray scale (white=0 and black=1) between the latent variables uj and vj corresponding to the pixel displayed
in green and the variables uj and vj corresponding to all the other pixels of the image. (right  bottom) Examples
of actual and reconstructed images by each method. The best reconstruction results correspond to HSDep.

5 Conclusions and Future Work

We have described a linear sparse model for learning dependencies in the feature selection process.
The model can be used in a multi-task learning setting with several tasks available for induction
that need not share relevant features  but only dependencies in the feature selection process. Exact
inference is intractable in such a model. However  expectation propagation provides an efﬁcient
approximate alternative with a cost in O(Kn2d)  where K is the number of tasks  n is the number
of samples of each task  and d is the dimensionality of the data. Experiments with real and synthetic
data illustrate the beneﬁts of the proposed method. Speciﬁcally  this model performs better than
other multi-task alternatives from the literature. Our experiments also show that the proposed model
is able to induce relevant feature selection dependencies from the training data alone. Future paths
of research include the evaluation of this model in practical problems of sparse coding  i.e.  when
all tasks share a common design matrix X that has to be induced from the data alongside with the
model coefﬁcients  with potential applications to image denoising and image inpainting [24].
Acknowledgment: Daniel Hern´andez-Lobato is supported by the Spanish MCyT (Ref. TIN2010-
21575-C02-02). Jos´e Miguel Hern´andez-Lobato is supported by Infosys Labs  Infosys Limited.

8

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllReferences
[1] I. M. Johnstone and D. M. Titterington. Statistical challenges of high-dimensional data. Philosophical
Transactions of the Royal Society A: Mathematical  Physical and Engineering Sciences  367(1906):4237 
2009.

[2] T. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. Journal of the

American Statistical Association  83(404):1023–1032  1988.

[3] C. M. Carvalho  N. G. Polson  and J. G. Scott. Handling sparsity via the horseshoe. Journal of Machine

Learning Research W&CP  5:73–80  2009.

[4] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  58(1):267–288  1996.

[5] M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning

Research  1:211–244  2001.

[6] J. M. Hern´andez-Lobato  D. Hern´andez-Lobato  and A. Su´arez. Network-based sparse Bayesian classiﬁ-

cation. Pattern Recognition  44:886–900  2011.

[7] M. Van Gerven  B. Cseke  R. Oostenveld  and T. Heskes. Bayesian source localization with the multivari-
ate Laplace prior. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors 
Advances in Neural Information Processing Systems 22  pages 1901–1909  2009.

[8] Julia E. Vogt and Volker Roth. The group-lasso: (cid:96)1 ∞ regularization versus (cid:96)1 2 regularization. In Goesele
et al.  editor  32nd Anual Symposium of the German Association for Pattern Recognition  volume 6376 
pages 252–261. Springer  2010.

[9] Y. Kim  J. Kim  and Y. Kim. Blockwise sparse regression. Statistica Sinica  16(2):375  2006.
[10] D. Hern´andez-Lobato  J. M. Hern´andez-Lobato  T. Helleputte  and P. Dupont. Expectation propagation
for Bayesian multi-task feature selection. In Jos´e L. Balc´azar  Francesco Bonchi  Aristides Gionis  and
Mich`ele Sebag  editors  Proceedings of the European Conference on Machine Learning  volume 6321 
pages 522–537. Springer  2010.

[11] G. Obozinski  B. Taskar  and M. I. Jordan. Joint covariate selection and joint subspace selection for

multiple classiﬁcation problems. Statistics and Computing  pages 1–22  2009.

[12] T. Xiong  J. Bi  B. Rao  and V. Cherkassky. Probabilistic joint feature selection for multi-task learning.
In Proceedings of the Seventh SIAM International Conference on Data Mining  pages 332–342. SIAM 
2007.

[13] T. Jebara. Multi-task feature and kernel selection for svms. In Proceedings of the twenty-ﬁrst international

conference on Machine learning  pages 55–62. ACM  2004.

[14] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning.

In B. Sch¨olkopf  J. Platt  and
T. Hoffman  editors  Advances in Neural Information Processing Systems 19  pages 41–48. MIT Press 
Cambridge  MA  2007.

[15] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. In J. Lafferty 
C. K. I. Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta  editors  Advances in Neural Information
Processing Systems 23  pages 964–972. 2010.

[16] P. Garrigues and B. Olshausen. Learning horizontal connections in a sparse coding model of natural
In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information

images.
Processing Systems 20  pages 505–512. MIT Press  Cambridge  MA  2008.

[17] T. Peleg  Y. C Eldar  and M. Elad. Exploiting statistical dependencies in sparse representations for signal

recovery. Signal Processing  IEEE Transactions on  60(5):2286–2303  2012.

[18] A. Papoulis. Probability  Random Variables  and Stochastic Processes. Mc-Graw Hill  1984.
[19] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer 

August 2006.

[20] T. Minka. A Family of Algorithms for approximate Bayesian Inference. PhD thesis  Massachusetts Insti-

tute of Technology  2001.

[21] M. W. Seeger. Expectation propagation for exponential families. Technical report  Department of EECS 

University of California  Berkeley  2006.

[22] T. Minka. Power EP. Technical report  Carnegie Mellon University  Department of Statistics  2004.
[23] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[24] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse coding.

Journal of Machine Learning Research  11:19–60  2010.

9

,Daniel Hernández-Lobato
José Miguel Hernández-Lobato
Tomoya Murata
Taiji Suzuki
Shuyang Sun
Jiangmiao Pang
Jianping Shi
Shuai Yi
Wanli Ouyang