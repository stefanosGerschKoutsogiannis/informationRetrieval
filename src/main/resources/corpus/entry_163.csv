2016,A Powerful Generative Model Using Random Weights for the Deep Image Representation,To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained  random weight networks? To address this issue  we explore new and powerful generative models for three popular deep visualization tasks using untrained  random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third  by recasting the content of an image in the style of various artworks  we create artistic images with high perceptual quality  highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization. It may possibly lead to a way to compare network architectures without training.,A Powerful Generative Model Using Random Weights

for the Deep Image Representation

Kun He∗  Yan Wang †

Department of Computer Science and Technology

Huazhong University of Science and Technology  Wuhan 430074  China

brooklet60@hust.edu.cn  yanwang@hust.edu.cn

John Hopcroft

Department of Computer Science

Cornell University  Ithaca 14850  NY  USA

jeh@cs.cornell.edu

Abstract

To what extent is the success of deep visualization due to the training? Could
we do deep visualization using untrained  random weight networks? To address
this issue  we explore new and powerful generative models for three popular deep
visualization tasks using untrained  random weight convolutional neural networks.
First we invert representations in feature spaces and reconstruct images from white
noise inputs. The reconstruction quality is statistically higher than that of the same
method applied on well trained networks with the same architecture. Next we
synthesize textures using scaled correlations of representations in multiple layers
and our results are almost indistinguishable with the original natural texture and
the synthesized textures based on the trained network. Third  by recasting the
content of an image in the style of various artworks  we create artistic images with
high perceptual quality  highly competitive to the prior work of Gatys et al. on
pretrained networks. To our knowledge this is the ﬁrst demonstration of image
representations using untrained deep neural networks. Our work provides a new
and fascinating tool to study the representation of deep network architecture and
sheds light on new understandings on deep visualization. It may possibly lead to a
way to compare network architectures without training.

1

Introduction

In recent years  Deep Neural Networks (DNNs)  especially Convolutional Neural Networks (CNNs) 
have demonstrated highly competitive results on object recognition and image classiﬁcation [1  2  3  4].
With advances in training  there is a growing trend towards understanding the inner working of these
deep networks. By training on a very large image data set  DNNs develop a representation of images
that makes object information increasingly explicit at various levels of the hierarchical architecture.
Signiﬁcant visualization techniques have been developed to understand the deep image representations
on trained networks [5  6  7  8  9  10  11].
Inversion techniques have been developed to create synthetic images with feature representations
similar to the representations of an original image in one or several layers of the network. Feature
representations are a function Φ of the source image x0. An approximate inverse Φ−1 is used to

∗The three authors contributing equally.
†Corresponding author.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

construct a new image x from the code Φ(x0) by reducing some statistical discrepancy between
Φ(x) and Φ(x0). Mahendran et al. [7] use the pretrained CNN AlexNet [2] and deﬁne a squared
Euclidean loss on the activations to capture the representation differences and reconstruct the image.
Gatys et al. [8  12] deﬁne a squared loss on the correlations between feature maps of some layers
and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].
Gatys et al. [13] then combine the loss on the correlations as a proxy to the style of a painting and the
loss on the activations to represent the content of an image  and successfully create artistic images
by converting the artistic style to the content image  inspiring several followups [14  15]. Another
stream of visualization aims to understand what each neuron has learned in a pretrained network
and synthesize an image that maximally activates individual features [5  9] or the class prediction
scores [6]. Nguyen et al. further try multifaceted visualization to separate and visualize different
features that a neuron learns [16].
Feature inversion and neural activation maximization both start from a white noise image and calculate
the gradient via backpropagation to morph the white noise image and output a natural image. In
addition  some regularizers are incorporated as a natural image prior to improve the visualization
quality  including α−norm [6]  total variation [7]  jitter [7]  Gaussian blur [9]  data-driven patch
priors [17]  etc. The method of visualizing the feature representation on the intermediate layers sheds
light on the information represented at each layer of the pretrained CNN.
A third set of researchers trains a separate feed-forward CNN with deconvolutional layers using
representations or correlations of the feature maps produced in the original network as the input and
the source image as the target to learn the inversion of the original network. The philosophy is to
train another neural network to inverse the representation and speedup the visualization on image
reconstruction [10  18]  texture synthesis [19] or even style transfer [15]. Instead of designing a
natural prior  some researchers incorporate adversarial training [20] to improve the realism of the
generated images [18]. Their trained deconvolutional network could give similar qualitative results as
the inversion technique does and is two or three orders of magnitude faster  as the previous inversion
technique needs a forward and backward pass through the pretrained network. This technique is
slightly different from the previous two in that it does not focus on understanding representations
encoded in the original CNN but on the visualization of original images by training another network.
It is well recognized that deep visualization techniques conduct a direct analysis of the visual in-
formation contained in image representations  and help us understand the representation encoded
at the intermediate layers of the well trained DNNs. In this paper  we raise a fundamental issue
that other researchers rarely address: Could we do deep visualization using untrained  random
weight DNNs? What kind of deep visualization could be applied on random weight DNNs?
This would allow us to separate the contribution of training from the contribution of the net-
work structure. It might even give us a method to evaluate deep network architectures without
spending days and signiﬁcant computing resources in training networks so that we could com-
pare them. Also  it will be useful not to have to store the weights  which can have signiﬁcant
impact for mobile applications. Though Gray et al. demonstrated that the VGG architecture with
random weights failed in generating textures and resulted in white noise images in an experiment
indicating the trained ﬁlters might be crucial for texture generation [8]  we conjecture the success
of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep
network hierarchical structure rather than from the training  and that the architecture itself may
cause the inversion invariant to the original image. Gatys et al.’s unsuccessful attempt on the texture
synthesis using the VGG architecture with random weights may be due to their inappropriate scale of
the weighting factors.
To verify our hypothesis  we try three popular inversion tasks for visualization using the CNN
architecture with random weights. Our results strongly suggest that this is true. Applying inversion
techniques on the untrained VGG with random weights  we reconstruct high perceptual quality
images. The results are qualitatively better than the reconstructed images produced on the pretrained
VGG with the same architecture. Then  we try to synthesize natural textures using the random weight
VGG. With automatic normalization to scale the squared correlation loss for different activation
layers  we succeed in generating similar textures as the prior work of Gatys et al. [8] on well-trained
VGG. Furthermore  we continue the experiments on style transfer  combining the content of an image
and the style of an artwork  and create artistic imagery using random weight CNN.

2

To our knowledge this is the ﬁrst demonstration of image representations using untrained deep neural
networks. Our work provides a new and fascinating tool to study the perception and representation of
deep network architecture  and shed light on new understandings on deep visualization. Our work
will inspire more possibilities of using the generative power of CNNs with random weights  which
do not need long training time on multi-GPUs. Furthermore  it is very hard to prove why trained
deep neural networks work so well. Based on the networks with random weights  we might be able
to prove some properties of the deep networks. Our work using random weights shows a possible
way to start developing a theory of deep learning since with well-trained weights  theorems might be
impossible.

2 Methods

In order to better understand the deep representation in the CNN architecture  we focus on three
tasks: inverting the image representation  synthesizing texture  and creating artistic style images.
Our methods are similar in spirit to existing methods [7  8  13]. The main difference is that we
use untrained weights instead of trained weights  and we apply weighting factors determined by a
pre-process to normalize the different impact scales of different activation layers on the input layer.
Compared with purely random weight CNN  we select a random weight CNN among a set of random
weight CNNs to get slightly better results.
For the reference network  we choose VGG-19 [3]  a convolutional neural network trained on the
1.3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [22]. The VGG
architecture has 16 convolutional and 5 pooling layers  followed by 3 fully connected layers. Gatys
et al. re-train the VGG-19 network using average pooling instead of maximum pooling  which they
suggest could improve the gradient ﬂow and obtain slightly better results [8]. They only consider
the convolutional and pooling layers for texture synthesis  and they rescale the weights such that the
mean activation of each ﬁlter over the images and positions is 1. Their trained network is denoted as
VGG in the following discussion. We adopt the same architecture  replacing the weights with purely
random values from a Gaussian distribution N (0  σ). The standard deviation  σ  is set to a small
number like 0.015 in the experiments. The VGG-based random weight network created as described
in the following subsection is used as our reference network  denoted as ranVGG in the following
discussion.
Inverting deep representations. Given a representation function F l : RH×W×C → RNl×Ml for
the lth layer of a deep network and F l(x0) for an input image x0  we want to reconstruct an image x
that minimizes the L2 loss among the representations of x0 and x.
ωl

x∗ = argmin
x∈RH×W ×C

Lcontent(x  x0  l) = argmin
x∈RH×W ×C

2NlMl

(cid:107)F l(x) − F l(x0)(cid:107)2

2

(1)

Here H and W denote the size of the image  C = 3 the color channels  and ωl the weighting factor.
We regard the feature map matrix F l as the representation function of the lth layer which has Nl× Ml
dimensions where Nl is the number of distinct feature maps  each of size Ml when vectorised. F l
ik
denotes the activation of the ith ﬁlter at position k.
The representations are a chain of non-linear ﬁlter banks even if untrained random weights are applied
to the network. We initialize the pre_image with white noise  and apply the L_BFGS gradient descent
using standard error backpropagation to morph the input pre_image to the target.

(2)

(3)

xt+1 = xt −

(cid:18) ∂L(x  x0  l)
N lM l (F l(xt) − F l(x0))i k

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)xt

∂F l
∂x

∂F l

ωl

=

(cid:12)(cid:12)(cid:12)(cid:12)xt

∂L(x  x0  l)

∂F l

i k

The weighting factor ωl is applied to normalize the gradient impact on the morphing image x. We use
a pre-processing procedure to determine the value of ωl. For the current layer l  we approximately
calculate the maximum possible gradient by Equation (4)  and back propagate the gradient to the
input layer. Then we regard the reciprocal of the absolute mean gradient over all pixels and RGB
channels as the value of ωl such that the gradient impact of different layers is approximately of the
same scale. This normalization doesn’t affect the reconstruction from the activations of a single layer 

3

but is added for the combination of content and style for the style transfer task.

H(cid:88)

C(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) W(cid:88)

i=1

∂L(x0  x(cid:48)  l)

j=1

k=1

∂xi j k

(cid:12)(cid:12)(cid:12)(cid:12)F l(x(cid:48))=0

1
ωl

=

1

W HC

(4)

To stabilize the reconstruction quality  we apply a greedy approach to build a “stacked" random
weight network ranVGG based on the VGG-19 architecture. Select one single image as the reference
image and starting from the ﬁrst convolutional layer  we build the stacked random weight VGG by
sampling  selecting and ﬁxing the weights of each layer in forward order. For the current layer l 
ﬁx the weights of the previous l − 1 layers and sample several sets of random weights connecting
the lth layer. Then reconstruct the target image using the rectiﬁed representation of layer l  and
choose weights yielding the smallest loss. Experiments in the next section show our success on the
reconstruction by using the untrained  random weight CNN  ranVGG.
Texture synthesis. Can we synthesize natural textures based on the feature space of an untrained
deep network? To address this issue  we refer to the method proposed by Gatys et al.[8] and use the
correlations between feature responses on each layer as the texture representation. The inner product
between pairwise feature maps i and j within each layer l  Gl
jk  deﬁnes a gram matrix
Gl = F l(F l)T . We seek a texture image x that minimizes the L2 loss among the correlations of the
representations of several candidate layers for x and a groundtruth image x0.

ikF l

k F l

Ltexture = argmin
x∈RH×W ×C
where the contribution of layer l to the total loss is deﬁned as

x∗ = argmin
x∈RH×W ×C

µlE(x  x0  l) 

E(x  x0  l) =

1
l M 2
4N 2
l

(cid:107)Gl(F l(x)) − Gl(F l(x0))(cid:107)2
2.

The derivative of E(x  x0  l) with respect to the activations F l in layer l is [8]:

∂E(x  x0  l)

∂F l

i k

=

1
l M 2
N 2
l

{(F l(x))T [Gl(F l(x)) − Gl(F l(x0))]}i k

ij =(cid:80)
(cid:88)

l∈L

(5)

(6)

(7)

(8)

The weighting factor µl is deﬁned similarly to ωl  but here we use the loss contribution E(x  x0  l) of
layer l to get its gradient impact on the input layer.

(cid:12)(cid:12)(cid:12)(cid:12) W(cid:88)
H(cid:88)

i

j

C(cid:88)

k

1
µl

=

1

W HC

(cid:12)(cid:12)(cid:12)(cid:12)F l(x(cid:48))=0

∂E(x0  x(cid:48)  l)

∂xi j k

We then perform the L_BFGS gradient descent using standard error backpropagation to morph the
input image to a synthesized texture image using the untrained ranVGG.
Style transfer. Can we use the untrained deep network to create artistic images? Referring to the
prior work of Gatys et al.[13] from the feature responses of VGG trained on ImageNet  we use an
untrained VGG and succeed in separating and recombining content and style of arbitrary images.
The objective requires terms for content and style respectively with suitable combination factors. For
content we use the method of reconstruction on medium layer representations  and for style we use
the method of synthesising texture on some lower through higher layer representation correlations.
Let xc be the content image and xs the style image. We combine the content of the former and the
style of the latter by optimizing the following objective:

αLcontent(x  xc) + βLtexture(x  xs) + γR(x)

(9)

x∗ = argmin
x∈RH×W×C

Here α and β are the contributing factors for content and style respectively. We apply a regularizer
R(x)  total variation(TV) [7] deﬁned as the squared sum on the adjacent pixel’s difference of x  to
encourage the spatial smoothness in the output image.

3 Experiments

This section evaluates the results obtained by our model using the untrained network ranVGG 3.

3https://github.com/mileyan/random_weights

4

The input image is required to be of size 256 × 256 if we want to invert the representation of the fully
connected layers. Else  the input could be of arbitrary size.

Inverting deep representations. We select several source images from the ILSVRC 2012 chal-
lenge [1] validation data as examples for the inversion task  and choose a monkey image as the
reference image to build the stacked ranVGG (Note that using other image as the reference image
also returns similar results). As compared with the inverting technique of Mahendran et al. [7]  we
only consider the Euclidean loss over the activations and ignore the regularizer they used to capture
the natural image prior. ranVGG contains 19 layers of random weights (16 convolutional layers and 3
fully connected layers)  plus 5 pooling layers. Mahendran et al. use a reference network AlexNet [2]
which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers)  plus
3 pooling layers.
Figure 1 shows that we reach higher perceptive reconstructions. The reason may lie in the fact
that the VGG architecture uses ﬁlters with a small receptive ﬁeld of 3 × 3 and we adopt average
pooling. Though shallower than VGG  their reference network  AlexNet  adopts larger ﬁlters and
uses maximum pooling  which makes it harder to get images well inverted and easily leads to spikes.
That’s why they used regularizers to polish the reconstructed image. Figure 2 shows more examples
(house  ﬂamingo  girl).
Figure 3 shows the variations on an example image  the girl. As compared with the VGG with purely
random weights  ranVGG (the VGG with stacked random weights) exhibits lower variations and
lower reconstruction distances. As compared with the trained VGG  both stacked ranVGG and VGG
with purely random weights exhibit lower reconstruction distance with lower variations. ranVGG
demonstrates a more stable and high performance for the inversion task and is slightly better than an
purely random VGG. So we will use ranVGG for the following experiments.
To compare the convergence of ranVGG and VGG  Figure 4 shows the loss (average Euclidean
distance) along the gradient descent iterations on an example image  the house. The reconstruction
converges much quicker on ranVGG and yields higher perceptual quality results. Note that the
reconstruction on VGG remains the same even if we double the iteration limits to 4000 iterations.

Texture synthesis. Figure 5 shows the textures synthesized by our model on ranVGG for several
natural texture images (ﬁfth row) selected from a texture website4 and an artwork named Starry
Night by Vincent van Gohn 1989. Each row of images was generated using an increasing number
of convolutional layers to constrain the gradient descent. conv1_1 for the ﬁrst row  conv1_1 and
conv2_1 for the second row  etc (the labels at each row indicate the top-most layer included). The
joint matching of conv1_1  conv2_1  and con3_1 (third row) already exhibits high quality texture
representations. Adding one more layer of conv4_1 (fourth row) could slightly improve the natural
textures. By comparison  results of Gatys et al.[8] on the trained VGG using four convolutional layers
up to conv4_1 are as shown at the bottom row.
Our experiments show that with suitable weighted factors  calculated automatically by our method 
ranVGG could synthesize complex natural textures that are almost indistinguishable with the original
texture and the synthesized texture on the trained VGG. Trained VGG generates slightly better
textures on neatly arranged original textures (cargo at the second column of Figure 5).

Style transfer. We select conv2_2 as the content layer  and use the combination of conv1_1 
conv2_1  ...  conv5_1 as the style. We set the ratio of α : β : γ = 100 : 1 : 1000 in the experiments.
We ﬁrst compare our style transfer results with the prior work of Gatys et al.[13] on several well-
known artworks for the style: Starry Night by Vincent van Gohn 1989  Der Schrei by Edward Munch
1893  Picasso by Pablo Picasso 1907  Woman with a Hat by Henri Matisse 1905  Meadow with
Poplars by Claude Monet 1875. As shown in Figure 6  the second row  by recasting the content of a
university image in the style of the ﬁve artworks  we obtain different artistic images based on the
untrained ranVGG (second row). Our results are comparable to their work [13] on the pretrained
VGG (third row)  and are in the same order of magnitude. They have slightly smoother lines and
textures which may attributed to the training. We further try the content and style combination on
some Chinese paintings and scenery photographs  as shown in Figure 7  and create high perceptual
artistic Chinese paintings that well combine the style of the painting and the content of the sceneries.

4http://www.textures.com/

5

pool1

pool2

pool3/conv3

pool4/conv4

pool5

G
G
V
n
a
r

n
o
s
r
u
O

G
G
V
n
o
s
r
u
O

t
e
N
x
e
l
A
n
o

]
7
[

Figure 1: Reconstructions from layers of ranVGG (top) and the pretrained VGG (middle) and
[7] (bottom). As AlexNet only contains 3 pooling layers  we compare their results on conv3 and
conv4 with ours on pool3 and pool4. Our method on ranVGG demonstrates a higher perceptive
quality  especially on the higher layers. Note that VGG is much deeper than AlexNet even when we
compare on the same pooling layer.

ranVGG

VGG

ranVGG

VGG

ranVGG

VGG

1
l
o
o
p

3
l
o
o
p

5
l
o
o
p

Figure 2: Reconstructions from different pooling layers of the untrained ranVGG and the
pretrained VGG. ranVGG demonstrates a higher perceptive quality  especially on the higher layers.
The pretrained VGG could rarely reconstruct even the contours from representations of the ﬁfth
pooling layer.

Figure 3: Variations in samples on the girl image  with maxi-
mum  minimum  mean and quartiles.

Figure 4: Reconstruction quali-
ties of conv5_1 during the gradi-
ent descent iterations.

6

Camouﬂage

Cargo

Floors

Flowers

Leaves

Nigh Starry

1
_
1
v
n
o
c

1
_
2
v
n
o
c

1
_
3
v
n
o
c

1
_
4
v
n
o
c

l
a
n
i
g
i
r
o

1
_
4
v
n
o
c

d
e
n
i
a
r
t

Figure 5: Generated textures using random weights. Each row corresponds to a different pro-
cessing stage in ranVGG. Considering only the lowest layer  conv1_1  the synthesised textures are
of lowest granularity  showing very local structure. Increasing the number of layers on which we
match the texture representation (conv1_1 plus conv2_1 for the second row  etc)  we have higher
organizations of the previous local structure. The third row and the fourth row show high-quality
synthesized textures of the original images. The lowest row corresponds to the result of using the
trained VGG to match the texture representation from conv1_1  conv2_1 conv3_1 and conv4_1.

Starry Night

Der Schrei

Photograph

Picasso Woman with a Hat

Meadow with Poplars

l
a
n
i
g
i
r

O

G
G
V
n
a
r

n
o

s
r
u
O

G
G
V
n
o

]
3
1
[

Figure 6: Artistic style images of ours on the untrained ranVGG (medium row) and of Gatys
et al.[8] on the pretrained VGG (bottom row). We select a university image (ﬁrst row  center) and
several well-known artworks for the style (ﬁrst row  others images). The third column under the
photograph are for the Picasso. We obtain similar quality results as compared with Gatys et al.[13].

7

Chinese painting

Photograph

Created image

Figure 7: Style transfer of Chinese paintings on the untrained ranVGG. We select several
Chinese paintings for the style (ﬁrst column)  including The Great Wall by Songyan Qian 1975 
a painting of anonymous author and Beautiful landscape by Ping Yang. We select the mountain
photographs (second column) as the content images. The created images performed on the untrained
ranVGG are shown in the third column  which seem to have learned how to paint the rocks and clouds
from paintings of the ﬁrst column and transfer the style to the content to “draw” Chinese landscape
paintings.

4 Discussion

Our work offers a testable hypothesis about the representation of image appearance based only on
the network structure. The success on the untrained  random weight networks on deep visualization
raises several fundamental questions in the area of deep learning. Researchers have developed many
visualization techniques to understand the representation of well trained deep networks. However  if
we could do the same or similar visualization using an untrained network  then the understanding
is not for the training but for the network architecture. What is the difference of a trained network
and a random weight network with the same architecture  and how could we explore the difference?
What else could one do using the generative power of untrained  random weight networks? Explore
other visualization tasks in computer vision developed on the well-trained network  such as image
morphing [23]  would be a promising aspect.
Training deep neural networks not only requires a long time but also signiﬁcant high performance
computing resources. The VGG network  which contains 11-19 weight layers depending on the
typical architecture [3]  takes 2 to 3 weeks on a system equipped with 4 NVIDIA Titan Black GPUs
for training a single net. The residual network ResNet  which achieved state-of-the-art results in
image classiﬁcation and detection in 2015 [4]  takes 3.5 days for the 18-layer model and 14 days for
the 101-layer model using 4 NVIDIA Kepler GPU.5 Could we evaluate a network structure without
taking a long time to train it? There are some prior works to deal with this issue but they deal with
much shallow networks [21]. In future work  we will address this issue by utilizing the untrained
network to attempt to compare networks quickly without having to train them.

Acknowledgments

This research work was supported by US Army Research Ofﬁce(W911NF-14-1-0477) and
National Science Foundation of China(61472147) and National Science Foundation of Hubei
Province(2015CFB566).

5http://torch.ch/blog/2016/02/04/resnets.html

8

References
[1] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision  115(3):211–252  2015.

[2] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  pages 1097–1105  2012.

[3] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

ICLR  2015.

[4] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In CVPR  2016.

[5] Dumitru Erhan  Yoshua Bengio  Aaron Courville  and Pascal Vincent. Visualizing higher-layer features of

a deep network. University de Montreal Technical Report 4323  2009.

[6] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks: Visualising

image classiﬁcation models and saliency maps. In ICLR  2014.

[7] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them.

In CVPR  pages 5188–5196  2015.

[8] Leon A. Gatys  Alexander S. Ecker  and Matthias Bethge. Texture synthesis using convolutional neural

networks. In NIPS  pages 262–270  May 2015.

[9] Jason Yosinski  Jeff Clune  Anh Nguyen  Thomas Fuchs  and Hod Lipson. Understanding neural networks

through deep visualization. In Deep Learning Workshop at ICML  2015.

[10] Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In

CVPR  pages 4829–4837  2016.

[11] Anh Nguyen  Jason Yosinski  and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence

predictions for unrecognizable images. In CVPR  2015.

[12] L. A. Gatys  A. S. Ecker  and M. Bethge. Texture synthesis and the controlled generation of natural stimuli

using convolutional neural networks. arXiv:1505.07376  2015.

[13] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. A neural algorithm of artistic style.

arXiv:1508.06576  2015.

[14] Yaroslav Nikulin and Roman Novak. Exploring the neural algorithm of artistic style. arXiv:1602.07188 

2016.

[15] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In ECCV  2016.

[16] Anh Mai Nguyen  Jason Yosinski  and Jeff Clune. Multifaceted feature visualization: Uncovering the

different types of features learned by each neuron in deep neural networks. arXiv:1602.03616  2016.

[17] Donglai Wei  Bolei Zhou  Antonio Torralba  and William T. Freeman. Understanding intra-class knowledge

inside CNN. arXiv:1507.02379  2015.

[18] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on

deep networks. In NIPS  2016.

[19] Dmitry Ulyanov  Vadim Lebedev  Andrea Vedaldi  and Victor Lempitsky. Texture networks: Feed-forward

synthesis of textures and stylized images. In ICML  2016.

[20] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron

Courville  and Yoshua Bengio. Generative adversarial nets. In NIPS  pages 2672–2680  2014.

[21] Andrew Saxe  Pang W Koh  Zhenghao Chen  Maneesh Bhand  Bipin Suresh  and Andrew Y Ng. On

random weights and unsupervised feature learning. In ICML  pages 1089–1096  2011.

[22] Yangqing Jia  Evan Shelhamer  Jeff Donahue  Sergey Karayev  Jonathan Long  Ross Girshick  Sergio
In

Guadarrama  and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding.
Proceedings of the ACM International Conference on Multimedia  ACM  pages 675–678  2014.

[23] Jacob R. Gardner  Paul Upchurch  Matt J. Kusner  Yixuan Li  Kilian Q. Weinberger  and John E. Hopcroft.

Deep manifold traversal: Changing labels with convolutional features. arXiv:1511.06421  2015.

9

,Kun He
Yan Wang
John Hopcroft
An Bian
Kfir Levy
Andreas Krause
Joachim Buhmann
Sanghyun Woo
Dahun Kim
Donghyeon Cho
In So Kweon