2018,How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD,Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives $f(x)$. However  in terms of making the gradients small  the original SGD does not give an optimal rate  even when $f(x)$ is convex.

If $f(x)$ is convex  to find a point with gradient norm $\varepsilon$  we design an algorithm SGD3 with a near-optimal rate $\tilde{O}(\varepsilon^{-2})$  improving the best known rate $O(\varepsilon^{-8/3})$. If $f(x)$ is nonconvex  to find its $\varepsilon$-approximate local minimum  we design an algorithm SGD5 with rate $\tilde{O}(\varepsilon^{-3.5})$  where previously SGD variants only achieve $\tilde{O}(\varepsilon^{-4})$. This is no slower than the best known stochastic version of Newton's method in all parameter regimes.,How To Make the Gradients Small Stochastically:

Even Faster Convex and Nonconvex SGD∗

Zeyuan Allen-Zhu
Microsoft Research AI
Redmond  WA 98052

zeyuan@csail.mit.edu

Abstract

Stochastic gradient descent (SGD) gives an optimal convergence rate when mini-
mizing convex stochastic objectives f (x). However  in terms of making the gra-
dients small  the original SGD does not give an optimal rate  even when f (x) is
convex.
If f (x) is convex  to ﬁnd a point with gradient norm ε  we design an algorithm

SGD3 with a near-optimal rate (cid:101)O(ε−2)  improving the best known rate O(ε−8/3)
sign an algorithm SGD5 with rate (cid:101)O(ε−3.5)  where previously SGD variants only
achieve (cid:101)O(ε−4) [6  14  30]. This is no slower than the best known stochastic

of [17]. If f (x) is nonconvex  to ﬁnd its ε-approximate local minimum  we de-

version of Newton’s method in all parameter regimes [27].

1

Introduction

In convex optimization and machine learning  the classical goal is to design algorithms to decrease
objective values  that is  to ﬁnd points x with f (x)− f (x∗) ≤ ε. In contrast  the rate of convergence
for the gradients  that is 

the number of iterations T needed to ﬁnd a point x with (cid:107)∇f (x)(cid:107) ≤ ε 

is a harder problem and sometimes needs new algorithmic ideas [25]. For instance  in the full-
gradient setting  accelerated gradient descent alone is suboptimal for this new goal  and one needs
additional tricks to get the fastest rate [25]. We review these tricks in Section 1.1.
In the convex (online) stochastic optimization  to the best of our knowledge  tight bounds are not
yet known for ﬁnding points with small gradients. The best recorded rate was T ∝ ε−8/3 [17]  and
it was raised as an open question [1] regarding how to improve it.
In this paper  we design two new algorithms  SGD2 which gives rate T ∝ ε−5/2 using Nesterov’s
tricks  and SGD3 which gives an even better rate T ∝ ε−2 log3 1
ε which is optimal up to log factors.
Motivation. Studying the rate of convergence for the minimizing gradients can be important at
least for the following two reasons.
• In many situations  points with small gradients ﬁt better our ﬁnal goals.

∗The full version of this paper can be found on https://arxiv.org/abs/1801.02982. When this
paper was submitted to NeurIPS 2018  the “non-convex SGD” results were not included. We encourage the
readers to go to our full version to ﬁnd out these “non-convex SGD” results.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Nesterov [25] considers the dual approach for solving constrained minimization problems. He
argued that “the gradient value (cid:107)∇f (x)(cid:107) serves as the measure of feasibility and optimality of
the primal solution ” and thus is the better goal for minimization purpose.2
In matrix scaling [7  10]  given a non-negative matrix  one wants to re-scale its rows and columns
to make it doubly stochastic. This problem has been applied in image reconstruction  operations
research  decision and control  and other scientiﬁc disciplines (see survey [20]). The goal for
matrix scaling is to ﬁnd points with small gradients  but not small objectives.
• Designing algorithms to ﬁnd points with small gradients can help us understand non-convex

optimization better and design faster non-convex machine learning algorithms.
Without strong assumptions  non-convex optimization theory is always in terms of ﬁnding points
with small gradients (i.e.  approximate stationary points or local minima). Therefore  to under-
stand non-convex stochastic optimization better  perhaps we should ﬁrst ﬁgure out the best rate
for convex stochastic optimization. In addition  if new algorithmic ideas are needed  can we also
apply them to the non-convex world? We ﬁnd positive answers to this question  and also obtain
better rates for standard non-convex optimization tasks.

1.1 Review: Prior Work on Deterministic Convex Optimization

Suppose f (x) is a Lipschitz smooth convex function with smoothness parameter L. Then  it is well-
known that accelerated gradient descent (AGD) [23  24] ﬁnds a point x satisfying f (x)− f (x∗) ≤ δ
) gradient computations of ∇f (x). To turn this into a gradient guarantee  we can
using T = O(
apply the smoothness property of f (x) which gives (cid:107)∇f (x)(cid:107)2 ≤ L(f (x) − f (x∗)). This means

√
L√
δ

AGD converges in rate T ∝ L
ε .

Nesterov [25] proposed two different tricks to improve upon such rate.

Nesterov’s First Trick: GD After AGD. Recall that starting from a point x0  if we perform T steps
of gradient descent (GD) xt+1 = xt − 1
t=0 (cid:107)∇f (xt)(cid:107)2 ≤ L(f (x0) −
f (x∗)). In addition  if this x0 is already the output of AGD for another T iterations  then it satisﬁes

(cid:1). Putting the two inequalities together  we have minT−1

(cid:8)(cid:107)∇f (xt)(cid:107)2(cid:9) ≤

f (x0) − f (x∗) ≤ O(cid:0) L
O(cid:0) L2

(cid:1). We call this method “GD after AGD ” and

t=0

T 2

L∇f (xt)  then it satisﬁes(cid:80)T−1

T 3

“GD after AGD” converges in rate T ∝ L2/3
ε2/3 .

Nesterov’s Second Trick: AGD After Regularization. Alternatively  we can also regularize
2(cid:107)x − x0(cid:107)2. This new function g(x) is σ-strongly convex 
f (x) by deﬁning g(x) = f (x) + σ
so AGD converges linearly  meaning that using T ∝ √
L√
ε gradients we can ﬁnd a point x
σ log L
satisfying (cid:107)∇g(x)(cid:107)2 ≤ L(g(x) − g(x∗)) ≤ ε2. If we choose σ ∝ ε  then this implies (cid:107)∇f (x)(cid:107) ≤
(cid:107)∇g(x)(cid:107) + ε ≤ 2ε. We call this method “AGD after regularization ” and
“AGD after regularization” converges in rate T ∝ L1/2

This is optimal up to a log factor  because ﬁrst-order methods need T = Ω((cid:112)L/δ) gradient com-

putations to ﬁnd f (x)− f (x∗) ≤ δ [23]  but f (x)− f (x∗) ≤ (cid:107)∇f (x)(cid:107)·(cid:107)x− x∗(cid:107) ≤ O((cid:107)∇f (x)(cid:107)).

ε .
ε1/2 log L

1.2 Our Results: Stochastic Convex Optimization
Consider the stochastic setting where the convex objective f (x) := Ei[fi(x)] and the algorithm can
only compute stochastic gradients ∇fi(x) at any point x for a random i. Let T be the number of
stochastic gradient computations. It is well-known that stochastic gradient descent (SGD) ﬁnds a
point x with f (x) − f (x∗) ≤ δ in (see for instance textbooks [8  18  26])

T = O(cid:0) V

σδ

(cid:1) if f (x) is σ-strongly convex.

T = O(cid:0) V

δ2

(cid:1) iterations

or

2Nesterov [25] studied miny∈Q{g(y) : Ay = b} with convex Q and strongly convex g(y). The dual
problem is minx{f (x)} where f (x) := miny∈Q{g(y) + (cid:104)x  b − Ay(cid:105)}. Let y∗(x) ∈ Q be the (unique)
minimizer of the internal problem  then g(y∗(x)) − f (x) = (cid:104)x ∇f (x)(cid:105) ≤ (cid:107)x(cid:107) · (cid:107)∇f (x)(cid:107).

2

SGD

SGD1

SGD2

SGD3

SGDsc
SGD1sc
SGD3sc

SGD

SCSG

online convex

online strongly

convex

online nonconvex
(σ-nonconvex)

gradient complexity T

(folklore  see Theorem 4.2)

(see [17] or Theorem 1)

ε

algorithm

(naive) O(cid:0)ε−4(cid:1)
(SGD after SGD) O(cid:0)ε−8/3(cid:1)
(SGD after regularization) O(cid:0)ε−5/2(cid:1)
(SGD + recursive regularization) O(cid:0)ε−2 · log3 1
(cid:1)
(naive) O(cid:0)ε−2 · κ(cid:1)
(SGD after SGD) O(cid:0)ε−2 · κ1/2(cid:1)
(SGD + recursive regularization) O(cid:0)ε−2 · log3 κ(cid:1)
(naive) O(cid:0)ε−4(cid:1)
O(cid:0)ε−10/3(cid:1)
O(cid:0)ε−2 + σε−4(cid:1)
O(cid:0)ε−3 + σ1/3ε−10/3(cid:1)
(cid:101)O(cid:0)ε−4(cid:1)
(cid:101)O(cid:0)ε−3.5(cid:1)
(cid:101)O(cid:0)ε−3.5(cid:1)
O(cid:0)ε−3.25(cid:1)

(see Theorem 2)

(see Theorem 3)

(see Theorem 4.2)

(see Theorem 1)

(see Theorem 3)

(see [16])

(see [21])

(see Theorem 4)

(see [3])

(see [6  14  30])

(see Theorem 5)

(see [27]

(see [3])

2nd-order
smooth

no

needed

SGD4
Natasha1.5
SGD variants
SGD5
cubic Newton

Natasha2

Table 1: Comparison of ﬁrst-order online stochastic methods for ﬁnding (cid:107)∇f (x)(cid:107) ≤ ε. Following tradition 
in these bounds  we hide variance and smoothness parameters in big-O and only show the dependency
σ ≥ 1 (if the objective is σ-strongly convex)  or the nonconvexity
on ε  the condition number κ = L
parameter σ.

Both rates are asymptotically optimal in terms of decreasing objective  and V is an absolute bound
on the variance of the stochastic gradients. Using the same argument (cid:107)∇f (x)(cid:107)2 ≤ L(f (x)− f (x∗))
as before  SGD ﬁnds a point x with (cid:107)∇f (x)(cid:107) ≤ ε in

T = O(cid:0) LV

σε2

(cid:1) if f (x) is σ-strongly convex.

T = O(cid:0) L2V

(cid:1) iterations

ε4

(SGD)

or

These rates are not optimal. We investigate three approaches to improve such rates.
New Approach 1: SGD after SGD. Recall in Nesterov’s ﬁrst trick  he replaced the use of the
inequality (cid:107)∇f (x)(cid:107)2 ≤ L(f (x) − f (x∗)) by T steps of gradient descent. In the stochastic setting 
can we replace this inequality with T steps of SGD? We call this algorithm SGD1 and prove that
Theorem 1 (informal). For convex stochastic optimization  SGD1 ﬁnds x with (cid:107)∇f (x)(cid:107) ≤ ε in

(cid:16) L2/3V

(cid:17)

(cid:16) L1/2V

(cid:17)

or

ε8/3

T = O

T = O

σ1/2ε2

iterations

if f (x) is σ-strongly convex.

(SGD1)
The rate T ∝ ε−8/3  in the special case of unconstrained minimization  was ﬁrst discovered by
Ghadimi and Lan [17] using a more complicated algorithm. The rate T ∝ 1
σ1/2ε2 does not seem to
be known before.
New Approach 2: SGD after regularization. Recall that in Nesterov’s second trick  he deﬁned
2(cid:107)x−x0(cid:107)2 as a regularized version of f (x)  and applied the strongly-convex version
g(x) = f (x)+ σ
of AGD to minimize g(x). Can we apply this trick to the stochastic setting?
Note the parameter σ has to be on the magnitude of ε because ∇g(x) = ∇f (x) + σ(x− x0) and we
wish to make sure (cid:107)∇f (x)(cid:107) = (cid:107)∇g(x)(cid:107)± ε. Therefore  if we apply SGD1 to minimize g(x) to ﬁnd
a point (cid:107)∇g(x)(cid:107) ≤ ε  the convergence rate is T ∝ 1
ε2.5 . We call this algorithm SGD2.
Theorem 2 (informal). For convex stochastic optimization  SGD2 ﬁnds x with (cid:107)∇f (x)(cid:107) ≤ ε in

σ1/2ε2 = 1

T = O(cid:0) L1/2V

(cid:1) iterations.

ε5/2

(SGD2)

ε5/2 rate does not seem to be known before.

Again  this T ∝ 1
New Approach 3: SGD and recursive regularization.
sub-optimality gap is due to the choice of σ ∝ ε which ensures (cid:107)σ(x − x0)(cid:107) ≤ ε.

In the second approach above  the ε0.5

3

Intuitively  if x0 were sufﬁciently close to x∗ (and thus were also close to the approximate minimizer
x)  then we could choose σ (cid:29) ε so that (cid:107)σ(x − x0)(cid:107) ≤ ε still holds. In other words  an appropriate
warm start x0 could help us break the ε−2.5 barrier and get a better convergence rate. However  how
to ﬁnd such x0? We ﬁnd it by constructing a “less warm” starting point and so on. This process is
summarized by the following algorithm which recursively ﬁnds the warm starts.
Starting from f (0)(x) := f (x)  we deﬁne f (s)(x) := f (s−1)(x) + σs

2 (cid:107)x −(cid:98)xs(cid:107)2 where σs = 2σs−1
and(cid:98)xs is an approximate minimizer of f (s−1)(x) that is simply calculated from the naive SGD. We
(cid:1) if f (x) is σ-strongly convex. (SGD3)
T = O(cid:0) log3(L/ε)·V

call this method SGD3  and prove that
Theorem 3 (informal). For convex stochastic optimization  SGD3 ﬁnds x with (cid:107)∇f (x)(cid:107) ≤ ε in

T = O(cid:0) log3(L/σ)·V

(cid:1) iterations

or

ε2

ε2

Our new rates in Theorem 3 not only improve the best known result of T ∝ ε−8/3  but also are
near optimal because Ω(V/ε2) is clearly a lower bound: even to decide whether a point x has
(cid:107)∇f (x)(cid:107) ≤ ε or (cid:107)∇f (x)(cid:107) > 2ε requires Ω(V/ε2) samples of the stochastic gradient.Perhaps
interestingly  our dependence on the smoothness parameter L (or the condition number κ := L/σ if
strongly convex) is only polylogarithmic  as opposed to polynomial in all previous results.

1.3 Roadmap

We introduce notions in Section 2 and formalize the convex problem in Section 3. We review clas-
sical (convex) SGD theorems with objective decrease in Section 4. We give an auxiliary lemma in
Section 5 show our SGD3 results in Section 6. We apply our techniques to non-convex optimization
and give algorithms SGD4 and SGD5 in Section 7. We discuss more related work in Appendix A 
and show our results on SGD1 and SGD2 respectively in Appendix B and Appendix C.

2 Preliminaries
Throughout this paper  we denote by (cid:107) · (cid:107) the Euclidean norm. We use i ∈R [n] to denote that i
is generated from [n] = {1  2  . . .   n} uniformly at random. We denote by ∇f (x) the gradient of
function f if it is differentiable  and ∂f (x) any subgradient if f is only Lipschitz continuous. We
denote by I[event] the indicator function of probabilistic events.
We denote by (cid:107)A(cid:107)2 the spectral norm of matrix A. For symmetric matrices A and B  we write
A (cid:23) B to indicate that A − B is positive semideﬁnite (PSD). Therefore  A (cid:23) −σI if and only if
all eigenvalues of A are no less than −σ. We denote by λmin(A) and λmax(A) the minimum and
maximum eigenvalue of a symmetric matrix A.
Recall some deﬁnitions on strong convexity and smoothness (and they have other equivalent deﬁni-
tions  see textbook [23]).
Deﬁnition 2.1. For a function f : Rd → R 
• f is σ-strongly convex if ∀x  y ∈ Rd  it satisﬁes f (y) ≥ f (x) + (cid:104)∂f (x)  y − x(cid:105) + σ
2(cid:107)x − y(cid:107)2.
• f is of σ-bounded nonconvexity (or σ-nonconvex for short) if ∀x  y ∈ Rd  it satisﬁes f (y) ≥
• f is L-Lipschitz smooth (or L-smooth for short) if ∀x  y ∈ Rd  (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107).
• f is L2-second-order smooth if ∀x  y ∈ Rd  it satisﬁes (cid:107)∇2f (x) − ∇2f (y)(cid:107)2 ≤ L2(cid:107)x − y(cid:107).
Deﬁnition 2.2. For composite function F (x) = ψ(x) + f (x) where ψ(x) is proper convex  given a
parameter η > 0  the gradient mapping of F (·) at point x is
x+ = arg min

(cid:8)ψ(y) + (cid:104)∇f (x)  y(cid:105) +

f (x) + (cid:104)∂f (x)  y − x(cid:105) − σ

(cid:107)y − x(cid:107)2(cid:9)

(cid:0)x − x+(cid:1)

2(cid:107)x − y(cid:107)2. 3

where

GF η(x) :=

1
η

1
2η

In particular  if ψ(·) ≡ 0  then GF η(x) ≡ ∇f (x).
Recall the following property about gradient mapping —see for instance [29  Lemma 3.7])

y

3Previous authors also refer to this notion as “approximate convex”  “almost convex”  “hypo-convex” 
“semi-convex”  or “weakly-convex.” We call it σ-nonconvex to stress the point that σ can be as large as L
(any L-smooth function is automatically L-nonconvex).

4

Lemma 2.3. Let F (x) = ψ(x) + f (x) where ψ(x) is proper convex and f (x) is σ-strongly convex
and L-smooth. For every x  y ∈ {x ∈ Rd : ψ(x) < +∞}  letting x+ = x − η · GF η(x)  we have

F (y) ≥ F (x+) + (cid:104)GF η(x)  y − x(cid:105) +

(cid:107)GF η(x)(cid:107)2 +

η
2

(cid:107)y − x(cid:107)2 .

σ
2

∀η ∈(cid:0)0 

(cid:3) :

1
L

The following deﬁnition and properties of Fenchel dual for convex functions is classical  and can be
found for instance in the textbook [26].
Deﬁnition 2.4. Given proper convex function h(y)  its Fenchel dual h∗(β) := maxy{y(cid:62)β − h(y)}.
Proposition 2.5. ∇h∗(β) = arg maxy{y(cid:62)β − h(y)}.
Proposition 2.6. If h(·) is σ-strongly convex  then h∗(·) is 1

σ -smooth.

3 Problem Formalization

Throughout this paper (except our nonconvex application Section 7)  we minimize convex stochastic
composite objective:

(cid:8)F (x) = ψ(x) + f (x) := ψ(x) + 1

i∈[n] fi(x)(cid:9)  
(cid:80)

n

minx∈Rd

(3.1)

where
1. ψ(x) is proper convex (a.k.a. the proximal term) 
2. fi(x) is differentiable for every i ∈ [n] 
3. f (x) is L-smooth and σ-strongly convex for some σ ∈ [0  L] that could be zero 
4. n can be very large of even inﬁnite (so f (x) = Ei[fi(x)]) 4 and
5. the stochastic gradients ∇fi(x) have a bounded variance (over the domain of ψ(·))  that is

∀x ∈ {y ∈ Rd | ψ(y) < +∞} : Ei∈R[n](cid:107)∇f (x) − ∇fi(x)(cid:107)2 ≤ V .

We emphasize that the above assumptions are all classical.
In the rest of the paper  we deﬁne T   the gradient complexity  as the number of computations of
∇fi(x). We search for points x so that the gradient mapping (cid:107)GF η(x)(cid:107) ≤ ε for any η ≈ 1
L. Recall
from Deﬁnition 2.2 that if there is no proximal term (i.e.  ψ(x) ≡ 0)  then GF η(x) = ∇f (x) for
any η > 0. We want to study the best tradeoff between the gradient complexity T and the error ε.
We say an algorithm is online if its gradient complexity T is independent of n. This tackles the big-
data scenario when n is extremely large or even inﬁnite (i.e.  f (x) = Ei[fi(x)] for some random
variable i). The stochastic gradient descent (SGD) method and all of its variants studied in this
paper are online. In contrast  GD  AGD [23  24]  and Katyusha [2] are ofﬂine methods because their
gradient complexity depends on n (see Table 2 in appendix).

4 Review: SGD with Objective Value Convergence

Recall that stochastic gradient descent (SGD) repeatedly performs proximal updates of the form

xt+1 = arg miny∈Rd{ψ(y) + 1

2α(cid:107)y − xt(cid:107)2 + (cid:104)∇fi(xt)  y(cid:105)}  

where α > 0 is some learning rate  and i is chosen in 1  2  . . .   n uniformly at random per iteration.
Note that if ψ(y) ≡ 0 then xt+1 = xt − α∇fi(xt). For completeness’ sake  we summarize it in
Algorithm 1. If f (x) is also known to be strongly convex  to get the tightest convergence rate  one
can repeatedly apply SGD with decreasing learning rate α [19]. We summarize this algorithm as
SGDsc in Algorithm 2.
The following theorem describes the rates of convergence in objective values for SGD and SGDsc
respectively. Their proofs are classical (and included in Appendix D); however  for our exact state-
ments  we cannot ﬁnd them recorded anywhere.5

However  we still introduce n to simplify notations.

4All of the results in this paper apply to the case when n is inﬁnite  because we focus on online methods.
5In the special case ψ(x) ≡ 0  Theorem 4.1(a) and 4.1(b) are folklore (see for instance [26]). If ψ(x) (cid:54)≡ 0 
Theorem 4.1(a) is recorded when ψ(x) is Lipschitz or smooth [13]  but we would not like to impose such

5

Algorithm 1 SGD(F  x0  α  T )
Input: function F (x) = ψ(x) + 1
n
(cid:5) if f (x) = 1
1: for t = 0 to T − 1 do
i ← a random index in [n];
2:
xt+1 ← arg miny∈Rd{ψ(y) + 1
3:
4: return x = x1+···+xT

n

.

T

Algorithm 2 SGDsc(F  x0  σ  L  T )
Input: function F (x) = ψ(x) + 1
n
1: for t = 1 to N = (cid:98) T
8L/σ(cid:99) do
2: for k = 1 to K = (cid:98)log2(σT /16L)(cid:99) do
3: return x = xN +K.

(cid:9)(cid:1)

(cid:80)n
i=1 fi(x) is L-smooth  optimal choice α = Θ(cid:0) min(cid:8) (cid:107)x0−x∗(cid:107)
(cid:80)n
i=1 fi(x); initial vector x0; learning rate α > 0; T ≥ 1.
√VT
  1
L
2α(cid:107)y − xt(cid:107)2 + (cid:104)∇fi(xt)  y(cid:105)};
(cid:80)n
i=1 fi(x); initial vector x0; parameters 0 < σ ≤ L; T ≥ L
xt ← SGD(cid:0)F  xt−1  1
σ .
(cid:5) f (x) is σ-strongly convex and f (x) = 1
i=1 fi(x) is L-smooth

xN +k ← SGD(cid:0)F  xN +k−1  1

(cid:80)n

2L   4L

(cid:1);

(cid:1);

2kL   2k+2L

σ

σ

n

Theorem 4.1. Let x∗ ∈ arg minx{F (x)}. To solve Problem (3.1) given a starting vector x0 ∈ Rd 
(a) SGD(F  x0  α  T ) outputs x satisfying E[F (x)] − F (x∗) ≤ αV
as long as α <

(cid:107)x0−x∗(cid:107)2

1/L. In particular  if α is tuned optimally  it satisﬁes

E[F (x)] − F (x∗) ≤ O(cid:0) L(cid:107)x0−x∗(cid:107)2
E[F (x)] − F (x∗) ≤ O(cid:0) V
(cid:1) +(cid:0)1 − σ

T

σT

L

+

2(1−αL) +
√V(cid:107)x0−x∗(cid:107)
(cid:1)Ω(T )

√

T

2αT

(cid:1) .

σ(cid:107)x0 − x∗(cid:107)2 .

σ   then SGDsc(F  x0  σ  L  T ) outputs x satisfying

(b) If f (x) is σ-strongly convex and T ≥ L

∀η ∈(cid:0)0 

(cid:3) :

As a sanity check  if V = 0  the convergence rate of SGD matches that of GD. (However  if V = 0 
one can apply accelerated gradient descent of Nesterov [22  23] instead for a faster rate.)
To turn Theorem 4.1 into a rate of convergence for the gradients  we can simply apply Lemma 2.3
which implies

η
2

1
L

(4.1)
Theorem 4.2. Let x∗ ∈ arg minx{F (x)}. To solve Problem (3.1) given a starting vector x0 ∈ Rd
and any η = C

(cid:107)GF η(x)(cid:107)2 ≤ F (x) − F (x+) ≤ F (x) − F (x∗) .

(a) SGD outputs x satisfying E[(cid:107)GF η(x)(cid:107)2] ≤ O(cid:0) L2(cid:107)x0−x∗(cid:107)2

L where C ∈ (0  1] is some absolute constant 

σ   then SGDsc outputs x satisfying E[(cid:107)GF η(x)(cid:107)2] ≤ O(cid:0) LV

(b) if T ≥ L
Corollary 4.3. Hiding V  L  (cid:107)x0 − x∗(cid:107) in the big-O notation  classical SGD ﬁnds x with
F (x) − F (x∗) ≤ O(T −1/2)
F (x) − F (x∗) ≤ O((σT )−1) (cid:107)GF η(x)(cid:107) ≤ O((σT )−1/2)

(cid:107)GF η(x)(cid:107) ≤ O(T −1/4)

(cid:1).
(cid:1)Ω(T )
(cid:1)+(cid:0)1− σ

for Problem (3.1)  or
if f (·) is σ-strongly convex for σ > 0.

σL(cid:107)x0−x∗(cid:107)2.

√V(cid:107)x0−x∗(cid:107)

+ L

√

σT

L

T

T

5 An Auxiliary Lemma on Regularization

Consider a regularized objective

G(x) := ψ(x) + g(x) := ψ(x) +(cid:0)f (x) +

S(cid:88)

(cid:107)x −(cid:98)xs(cid:107)2(cid:1)  

σs
2

(5.1)

assumptions. A variant of Theorem 4.1(b) is recorded for the accelerated version of SGD [15]  but with a

slightly worse rate T = O(cid:0) V

σT + L(cid:107)x0−x∗(cid:107)2

T 2

s=1

(cid:1). If the readers ﬁnd either statement explicitly stated somewhere 

please let us know and we would love to include appropriate citations.

6

where(cid:98)x1  . . .  (cid:98)xS are ﬁxed vectors in Rd. The following lemma says that  if we ﬁnd an approximate
is(cid:101)σ-strongly convex with(cid:101)σ :=(cid:80)S
(cid:3)  we
an arbitrary vector in the domain of {x ∈ Rd : ψ(x) < +∞}. Then  for every η ∈(cid:0)0 

stationary point x of G(x)  then it is also an approximate stationary point of F (x) up to some
additive error.
Lemma 5.1. Suppose ψ(x) is proper convex and f (x) is convex and L-smooth. By deﬁnition  g(x)
s=1 σs. Let x∗ be the unique minimizer of G(y) in (5.1)  and x be

1

L+(cid:101)σ

have

s=1

(cid:107)GF η(x)(cid:107) ≤ S(cid:88)
σs(cid:107)x∗ −(cid:98)xs(cid:107) + 3(cid:107)GG η(x)(cid:107) .
(cid:88)
σs(x −(cid:98)xs)(cid:107) x≤ (cid:107)∇g(x)(cid:107) +
σs(cid:107)x −(cid:98)xs(cid:107)
(cid:88)
σs(cid:107)x∗ −(cid:98)xs(cid:107) +(cid:101)σ(cid:107)x∗ − x(cid:107) z≤ 2(cid:107)∇g(x)(cid:107) +

(cid:88)

s

(cid:88)

(cid:107)∇f (x)(cid:107) = (cid:107)∇g(x) +

s

y≤ (cid:107)∇g(x)(cid:107) +

σs(cid:107)x∗ −(cid:98)xs(cid:107) .
Above  inequalities x and y both use the triangle inequality; and inequality z is due to the(cid:101)σ-strong
(cid:3)

convexity of g(x) (see for instance [23  Sec. 2.1.3]).

Proof of Lemma 5.1. See full version.

s

s

Remark 5.2. Lemma 5.1 should be easy to prove in the special case of ψ(x) ≡ 0. Indeed 

6 Approach 3: SGD and Recursive Regularization

σs
2

F (0)(x) := F (x)

for s = 1  2  . . .   S

In this section  add a logarithmic number of regularizers to the objective  each centered at a different
but carefully chosen point. Speciﬁcally  given parameters σ1  . . .   σS > 0  we deﬁne functions

and F (s)(x) := F (s−1)(x) +

(cid:107)x −(cid:98)xs(cid:107)2
where each(cid:98)xs (for s ≥ 1) is an approximate minimizer of F (s−1)(x).
calculate each(cid:98)xs  we apply SGDsc for T
If f (x) is σ-strongly convex  then we choose S ≈ log2

σ and let σ0 = σ and σs = 2σs−1. To
S iterations. This totals to a gradient complexity of T . We
summarize this method as SGD3sc in Algorithm 3.
2(cid:107)x − x0(cid:107)2 for some small
If f (x) is not strongly convex  then we regularize it by G(x) = F (x) + σ
parameter σ > 0  and then apply SGD3sc. We summarize this ﬁnal method as SGD3 in Algorithm 4.
We prove the following main theorem:
Theorem 3 (SGD3). Let x∗ ∈ arg minx{F (x)}. To solve Problem (3.1) given a starting vector
x0 ∈ Rd and any η = C
(a) If f (x) is σ-strongly convex for σ ∈ (0  L] and T ≥ L

L for some absolute constant C ∈ (0  1].
σ log L

σ   then SGD3sc(F  x0  σ  L  T ) outputs

L

x satisfying

E[(cid:107)GF η(x)(cid:107)] ≤ O

(cid:16)√V · log3/2 L

(cid:17)

σ

√

T

(b) If σ ∈ (0  L] and T ≥ L
E[(cid:107)GF η(x)(cid:107)] ≤ O

σ   then SGD3(F  x0  σ  L  T ) outputs x satisfying

√V·log3/2 L

σ(cid:107)x0 − x∗(cid:107) .
If σ is appropriately chosen  then we ﬁnd x with E[(cid:107)GF η(x)(cid:107)] ≤ ε in gradient complexity

√

L

T

σ log L
σ(cid:107)x0 − x∗(cid:107) +

(cid:16)
(cid:16)V · log3 L(cid:107)x0−x∗(cid:107)

ε

ε2

T ≤ O

σ(cid:107)x0 − x∗(cid:107) .

+(cid:0)1 − σ
(cid:17)

(cid:1)Ω(T / log(L/σ))
+(cid:0)1 − σ

L

σ

(cid:1)Ω(T / log(L/σ))
(cid:17)

L(cid:107)x0 − x∗(cid:107)

.

ε

L(cid:107)x0 − x∗(cid:107)

+

log

ε

Remark 6.1. All expected guarantees of the form E[(cid:107)GF η(x)(cid:107)2] ≤ ε2 or E[(cid:107)GF η(x)(cid:107)] ≤ ε through-
out this paper can be made into high-conﬁdence bound by repeating the algorithm multiple times 
each time estimating the value of (cid:107)GF η(x)(cid:107) using roughly O( V
ε2 ) stochastic gradient computations 
and ﬁnally outputting the point x that leads to the smallest value (cid:107)GF η(x)(cid:107).

7

n

σ

L

(cid:1).

(cid:80)n

(cid:5) f (x) = 1

σ log L
σ (cid:99) do

Algorithm 3 SGD3sc(F  x0  σ  L  T )
Input: function F (x) = ψ(x) + 1
n

(cid:80)n
of iterations T ≥ Ω(cid:0) L
i=1 fi(x); initial vector x0; parameters 0 < σ ≤ L; number
1: F (0)(x) := F (x);(cid:98)x0 ← x0; σ0 ← σ;
i=1 fi(x) is σ-strongly convex and L-smooth
(cid:1);
(cid:98)xs ← SGDsc(cid:0)F (s−1) (cid:98)xs−1  σs−1  3L  T
2: for s = 1 to S = (cid:98)log2
3:
2 (cid:107)x −(cid:98)xs(cid:107)2;
4:
6: return x =(cid:98)xS.
5:
(cid:80)n
i=1 fi(x); initial vector x0; parameters L ≥ σ > 0; T ≥ 1.
i=1 fi(x) is convex and L-smooth

σs ← 2σs−1;
F (s)(x) := F (s−1)(x) + σs

Algorithm 4 SGD3(F  x0  σ  L  T )
Input: function F (x) = ψ(x) + 1
n
2(cid:107)x − x0(cid:107)2;

(cid:5) f (x) = 1

1: G(x) := F (x) + σ
2: return x ← SGD3sc(G  x0  σ  L + σ  T ).

(cid:80)n

S

n

6.1 Proof of Theorem 3

Before proving Theorem 3  we state a few properties regarding the relationships between the

objective-optimality of(cid:98)xs and point distances.
Claim 6.2. Suppose for every s = 1  . . .   S the vector(cid:98)xs satisﬁes
s−1)(cid:3) ≤ δs
(a) for every s ≥ 1  E[(cid:107)(cid:98)xs − x∗
(b) for every s ≥ 1  E[(cid:107)(cid:98)xs − x∗
(c) if σs = 2σs−1 for all s ≥ 1  then E(cid:2)(cid:80)S

E(cid:2)F (s−1)((cid:98)xs) − F (s−1)(x∗
s−1(cid:107)]2 ≤ E[(cid:107)(cid:98)xs − x∗
s −(cid:98)xs(cid:107)2] ≤ δs
s(cid:107)]2 ≤ E[(cid:107)x∗
s=1 σs(cid:107)x∗

s−1 ∈ arg minx{F (s−1)(x)}  then 

s−1(cid:107)2] ≤ 2δs
; and

where x∗

σs−1

σs

 

(6.1)

δsσs .

s=1

√

S −(cid:98)xs(cid:107)(cid:3) ≤ 4(cid:80)S
E(cid:2)F (s−1)((cid:98)xs) − F (s−1)(x∗

Proof of Claim 6.2.
s−1(cid:107)]2

(a) E[(cid:107)(cid:98)xs − x∗

x≤ E[(cid:107)(cid:98)xs − x∗

. Here 
inequality x is because E[X]2 ≤ E[X 2]  and inequality y is due to the strong convexity of
F (s−1)(x).

σs−1

s−1(cid:107)2]

y≤ 2
σs−1

s−1)] ≤ 2δs

(b) We derive that

s −(cid:98)xs(cid:107)2

σs(cid:107)x∗

s −(cid:98)xs(cid:107)2 + F (s)((cid:98)xs) − F (s)(x∗

x≤ σs
2

(cid:107)x∗

y≤ F (s−1)((cid:98)xs) − F (s−1)(x∗
s−1. Taking expectation we have E[(cid:107)x∗
s=1 σs(cid:107)x∗

s−1) .

s)

s) = F (s−1)((cid:98)xs) − F (s−1)(x∗
s −(cid:98)xs(cid:107)2] ≤ δs
s −(cid:98)xs(cid:107)]2 ≤ E[(cid:107)x∗
s−1(cid:107)
s − x∗

σs

(c) Deﬁne Pt :=(cid:80)t

Here  inequality x is due to the strong convexity of F (s)(x)  and inequality y is because of the
minimality of x∗

t −(cid:98)xs(cid:107) for each t ≥ 0  1  . . .   S. Then by triangle inequality we have
δsσs + σs · E(cid:2)(cid:107)x∗

s −(cid:98)xs(cid:107) +(cid:0)(cid:80)s−1
(cid:1) · (cid:107)x∗
(cid:112)
s−1 −(cid:98)xs(cid:107)(cid:3) ≤ 4
s −(cid:98)xs(cid:107) + (cid:107)x∗

Using the parameter choice of σs = 2σs−1  and plugging in Claim 6.2(a) and Claim 6.2(b)  we
have
(cid:3)

E[Ps − Ps−1] ≤(cid:112)

Ps − Ps−1 ≤ σs(cid:107)x∗

t=1 σt

δsσs .

.

8

s−1(cid:107)2] .

3L

(cid:1)Ω(T /S)E[σs−1(cid:107)(cid:98)xs−1 − x∗
(cid:1)Ω(T /S)
(cid:1)Ω(T /S)E[F (s−2)((cid:98)xs−1)−F (s−2)(x∗

σ0(cid:107)x0 − x∗(cid:107)2 .

L

s−2)] .

σ0(cid:107)x0 − x∗(cid:107)2 .

2(cid:107)GF (S−1) η((cid:98)xS)(cid:107)2 ≤ F (S−1)((cid:98)xS) −

Proof of Theorem 3(a). We ﬁrst note that  when writing f (s−1)(x) = F (s−1)(x) − ψ(x)  each
t=1 σt ≤ 3L Lipschitz smooth. Therefore 

σ0T

σs−1T

0 = x∗)

If s > 1  this means

applying Theorem 4.1(b)  we have

E[F (s−1)((cid:98)xs)−F (s−1)(x∗

f (s−1) is at least σs−1-strongly convex and L +(cid:80)s−1
(cid:1) +(cid:0)1 − σs−1
s−1)] ≤ O(cid:0) SV
E[F (s−1)((cid:98)xs) − F (s−1)(x∗
If s = 1  this means (recalling(cid:98)x0 = x0 and x∗
E[F (0)((cid:98)xs) − F (0)(x∗)] ≤ O(cid:0) SV
(cid:1) +(cid:0)1 − σ0
(cid:1)+(cid:0)1− σs−1
s−1)] ≤ O(cid:0) SV
δs = O(cid:0) SV
(cid:1)Ω(sT /S)
Using Lemma 2.3 with F (S−1) and y = x =(cid:98)xS  we have η
F (S−1)((cid:98)x+
S ) ≤ F (S−1)((cid:98)xS) − F (S−1)(x∗
E(cid:2)(cid:107)GF η((cid:98)xS)(cid:107)(cid:3) ≤ E(cid:104) S−1(cid:88)
(cid:16) S(cid:88)

(cid:1) +(cid:0)1 − σ0

σs−1T

(cid:17)

σsT

s=1

L

L

S−1) and therefore

= O(LδS) .
Plugging this into Lemma 5.1 (with G(x) = F (S−1)(x)) and Claim 6.2(c)  we have

E(cid:2)(cid:107)GF (S−1) η((cid:98)xS)(cid:107)(cid:3)2 ≤ E(cid:2)(cid:107)GF (S−1) η((cid:98)xS)(cid:107)2(cid:3) ≤ 2δS
S−1 −(cid:98)xs(cid:107) + 3(cid:107)GF (S−1) η((cid:98)xS)(cid:107)(cid:105) ≤ O
(cid:16) S−1(cid:88)
(cid:112)
(cid:17) ≤ O
(cid:1)Ω(T /S)
+(cid:0)1 − σ0
σ0(cid:107)x0 − x∗(cid:107) . (cid:3)

(cid:16) S3/2V 1/2

σs(cid:107)x∗

(cid:112)

(cid:112)

δsσs +

(cid:17)

δsσs

= O

LδS

s=1

η

Together  this means to satisfy (6.1)  it sufﬁces to choose δs so that

s=1

T 1/2

L

Proof of Theorem 3(b). Deﬁne G(x) := F (x)+ σ
G(·). Note that x∗

on G(x) and Lemma 5.1 with S = 1 and(cid:98)x1 = x0  we have
(cid:17)

G be the (unique) minimizer of
G may be different from x∗ which is a minimizer of F (·). Applying Theorem 3(a)

(cid:16)

2(cid:107)x−x0(cid:107)2 and let x∗

√V·log3/2 L

+(cid:0)1 − σ

(cid:1)Ω(T / log(L/σ))
G − x0(cid:107)2 = (G(x∗) − F (x∗)) + (F (x∗

√

L

T

σ

E[(cid:107)GF η(x)(cid:107)] ≤ O

σ(cid:107)x0 − x∗
2(cid:107)x∗ − x0(cid:107)2 − σ

G(cid:107) +
2(cid:107)x∗

Now  by deﬁnition σ
we have (cid:107)x∗

G − x0(cid:107) ≤ (cid:107)x∗ − x0(cid:107). This completes the proof.

σ(cid:107)x0 − x∗
G(cid:107)
G)) ≥ 0 so
G) − G(x∗
(cid:3)

Acknowledgements

We would like to thank Lin Xiao for suggesting reference [29  Lemma 3.7]  an anonymous re-
searcher from the Simons Institute for suggesting reference [25]  Yurii Nesterov for helpful discus-
sions  Xinyu Weng for discussing the motivations  S´ebastien Bubeck  Yuval Peres  and Lin Xiao for
discussing notations  Chi Jin for discussing reference [27]  and Dmitriy Drusvyatskiy for discussing
the notion of Moreau envelope.

References
[1] Open problem session of “fast iterative methods in optimization” workshop. Simons Institute

for the Theory of Computing  UC Berkeley  October 2017.

[2] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods.

In STOC  2017.

[3] Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD.

2018.

In NeurIPS 

9

[4] Zeyuan Allen-Zhu and Elad Hazan. Optimal Black-Box Reductions Between Optimization

Objectives. In NeurIPS  2016.

[5] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Online Learning of

Eigenvectors and Faster MMWU. In ICML  2017.

[6] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding Local Minima via First-Order Oracles. In

NeurIPS  2018.

[7] Zeyuan Allen-Zhu  Yuanzhi Li  Rafael Oliveira  and Avi Wigderson. Much Faster Algorithms

for Matrix Scaling. In FOCS  2017.

[8] S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends

in Machine Learning  8(3-4):231–357  2015.

[9] Yair Carmon  John C. Duchi  Oliver Hinder  and Aaron Sidford. Accelerated Methods for

Non-Convex Optimization. ArXiv e-prints  abs/1611.00756  November 2016.

[10] M. B. Cohen  A. Madry  D. Tsipras  and A. Vladu. Matrix Scaling and Balancing via Box
In FOCS  pages 902–913  Oct

Constrained Newton’s Method and Interior Point Methods.
2017.

[11] Damek Davis and Dmitriy Drusvyatskiy. Complexity of ﬁnding near-stationary points of con-

vex functions stochastically. ArXiv e-prints  abs/1802.08556  2018.

[12] Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate

o(k−1/4) on weakly convex functions. ArXiv e-prints  abs/1802.02988  2018.

[13] John Duchi and Yoram Singer. Efﬁcient Online and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research  10:2899–2934  2009. ISSN 15324435. doi:
10.1561/2400000003.

[14] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online
stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference
on Learning Theory  COLT 2015  2015.

[15] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization I: A generic algorithmic framework. SIAM Journal
on Optimization  22(4):1469–1492  2012.

[16] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[17] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear
and stochastic programming. Mathematical Programming  pages 1–26  feb 2015. ISSN 0025-
5610.

[18] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-

tion  2(3-4):157–325  2016. ISSN 2167-3888. doi: 10.1561/2400000013.

[19] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for
stochastic strongly-convex optimization. The Journal of Machine Learning Research  15(1):
2489–2512  2014.

[20] Martin Idel. A review of matrix scaling and sinkhorn’s normal form for matrices and positive

maps. ArXiv e-prints  abs/1609.06349  2016.

[21] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex Finite-Sum Optimization

Via SCSG Methods. In NeurIPS  2017.

[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate
In Doklady AN SSSR (translated as Soviet Mathematics Doklady)  volume 269 

O(1/k2).
pages 543–547  1983.

10

[23] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  vol-

ume I. Kluwer Academic Publishers  2004. ISBN 1402075537.

[24] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  December 2005. ISSN 0025-5610. doi: 10.1007/s10107-004-0552-5.

[25] Yurii Nesterov. How to make the gradients small. Optima  88:10–11  2012.

[26] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and

Trends in Machine Learning  4(2):107–194  2012. ISSN 1935-8237.

[27] Nilesh Tripuraneni  Mitchell Stern  Chi Jin  Jeffrey Regier  and Michael I Jordan. Stochas-
tic Cubic Regularization for Fast Nonconvex Optimization. ArXiv e-prints  abs/1711.02838 
November 2017.

[28] Blake Woodworth and Nati Srebro. Tight Complexity Bounds for Optimizing Composite Ob-

jectives. In NeurIPS  2016.

[29] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance

Reduction. SIAM Journal on Optimization  24(4):2057—-2075  2014.

[30] Yi Xu and Tianbao Yang. First-order Stochastic Algorithms for Escaping From Saddle Points

in Almost Linear Time. ArXiv e-prints  abs/1711.01944  November 2017.

11

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
hengshuai yao
Csaba Szepesvari
Richard Sutton
Joseph Modayil
Shalabh Bhatnagar
Zeyuan Allen-Zhu