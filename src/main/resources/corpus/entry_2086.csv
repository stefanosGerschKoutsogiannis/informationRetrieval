2016,Feature selection in functional data classification with recursive maxima hunting,Dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction.  In this work  we introduce recursive maxima hunting (RMH) for variable selection in classification problems with functional data. In this context  variable selection techniques are especially attractive because they reduce the dimensionality  facilitate the interpretation and can improve the accuracy of the predictive models. The method  which is a recursive extension of maxima hunting (MH)  performs variable selection by identifying the maxima of a relevance function  which measures the strength of the correlation of the predictor functional variable with the class label. At each stage  the information associated with the selected variable is removed by subtracting the conditional expectation of the process. The results of an extensive empirical evaluation are used to illustrate that  in the problems investigated  RMH has comparable or higher predictive accuracy than standard simensionality reduction techniques  such as PCA and PLS  and state-of-the-art feature selection methods for functional data  such as maxima hunting.,Feature selection in functional data classiﬁcation with

recursive maxima hunting

Jos´e L. Torrecilla

Computer Science Department

Universidad Aut´onoma de Madrid

28049 Madrid  Spain

Alberto Su´arez

Computer Science Department

Universidad Aut´onoma de Madrid

28049 Madrid  Spain

joseluis.torrecilla@uam.es

alberto.suarez@uam.es

Abstract

Dimensionality reduction is one of the key issues in the design of effective machine
learning methods for automatic induction. In this work  we introduce recursive
maxima hunting (RMH) for variable selection in classiﬁcation problems with func-
tional data. In this context  variable selection techniques are especially attractive
because they reduce the dimensionality  facilitate the interpretation and can im-
prove the accuracy of the predictive models. The method  which is a recursive
extension of maxima hunting (MH)  performs variable selection by identifying the
maxima of a relevance function  which measures the strength of the correlation of
the predictor functional variable with the class label. At each stage  the information
associated with the selected variable is removed by subtracting the conditional
expectation of the process. The results of an extensive empirical evaluation are
used to illustrate that  in the problems investigated  RMH has comparable or higher
predictive accuracy than standard dimensionality reduction techniques  such as
PCA and PLS  and state-of-the-art feature selection methods for functional data 
such as maxima hunting.

1

Introduction

In many important prediction problems from different areas of application (medicine  environmental
monitoring  etc.) the data are characterized by a function  instead of by a vector of attributes  as is
commonly assumed in standard machine learning problems. Some examples of these types of data
are functional magnetic resonance imaging (fMRI) (Grosenick et al.  2008) and near-infrared spectra
(NIR) (Xiaobo et al.  2010). Therefore  it is important to develop methods for automatic induction
that take into account the functional structure of the data (inﬁnite dimension  high redundancy  etc.)
(Ramsay and Silverman  2005; Ferraty and Vieu  2006). In this work  the problem of classiﬁcation
of functional data is addressed. For simplicity  we focus on binary classiﬁcation problems (Ba´ıllo
et al.  2011). Nonetheless  the proposed method can be readily extended to a multiclass setting. Let
X(t)  t ∈ [0  1] be a continuous stochastic process in a probability space (Ω F  P). A functional
datum Xn(t) is a realization of this process (a trajectory). Let {Xn(t)  Yn}Ntrain
  t ∈ [0  1] be a
set of trajectories labeled by the dichotomous variable Yn ∈ {0  1}. These trajectories come from
one of two different populations; either P0  when the label is Yn = 0  or P1  when the label is
Yn = 1. For instance  the data could be the ECG’s from either healthy or sick persons (P0 and P1 
respectively). The classiﬁcation problem consist in deciding to which population a new unlabeled
observation X test(t) belongs (e.g.  to decide from his or her ECG whether a person is healthy or
not). Speciﬁcally  we are interested in the problem of dimensionality reduction for functional data
classiﬁcation. The goal is to achieve the optimal discrimination performance using only a ﬁnite  small
set of values from the trajectory as input to a standard classiﬁer (in our work  k-nearest neighbors).

n=1

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In general  to properly handle functional data  some kind of reduction of information is neces-
sary. Standard dimensionality reduction methods in functional data analysis (FDA) are based
on principal component analysis (PCA) (Ramsay and Silverman  2005) or partial least squares
(PLS) (Preda et al.  2007). In this work  we adopt a different approach based on variable selec-
tion (Guyon et al.  2006). The goal is to replace the complete function X(t) by a d-dimensional
vector (X(t1)  . . .   X(td)) for a set of “suitable chosen” points {t1  . . .   td} (for instance  instants in
a heartbeat in ECG’s)  where d is small.
Most previous work on feature selection in supervised learning with functional data is quite recent
and focuses on regression problems; for instance  on the analysis of fMRI images (Grosenick et al. 
2008; Ryali et al.  2010) and NIR spectra (Xiaobo et al.  2010). In particular  adaptations of lasso
and other embedded methods have been proposed to this end (see  e.g.  Kneip and Sarda (2011);
Zhou et al. (2013); Aneiros and Vieu (2014)). In most cases  functional data are simply treated as
high-dimensional vectors for which the standard methods apply. Speciﬁcally  G´omez-Verdejo et al.
(2009) propose feature extraction from the functional trajectories before applying a multivariate
variable selector based on measuring the mutual information. Similarly  Fernandez-Lozano et al.
(2015) compare different standard feature selection techniques for image texture classiﬁcation. The
method of minimum Redundancy Maximum Relevance (mRMR) introduced by Ding and Peng (2005)
has been applied to functional data in Berrendero et al. (2016a). In that work distance correlation
(Sz´ekely et al.  2007) is used instead of mutual information to measure nonlinear dependencies  with
good results. A fully functional perspective is adopted in Ferraty et al. (2010) and Delaigle et al.
(2012). In these articles  a wrapper approach is used to select the optimal set of instants in which the
trajectories should be monitored by minimizing a cross-validation estimate of the classiﬁcation error.
Berrendero et al. (2015) introduce a ﬁlter selection procedure based on computing the Mahalanobis
distance and Reproducing Kernel Hilbert Space techniques. Logistic regression models have been
applied to the problem of binary classiﬁcation with functional data in Lindquist and McKeague (2009)
and McKeague and Sen (2010)  assuming Brownian and fractional Brownian trajectories  respectively.
Finally  the selection of intervals or elementary functions instead of variables is addressed in Li and
Yu (2008); Fraiman et al. (2016) or Tian and James (2013).
From the analysis of previous work one concludes that  in general  it is preferable  both in terms of
accuracy and interpretability  to adopt a fully functional approach to the problem. In particular  if
the data are characterized by functions that are continuous  values of the trajectory that are close to
each other tend to be highly redundant and convey similar information. Therefore  if the value of the
process at a particular instant has high discriminant capacity  one could think of discarding nearby
values. This idea is exploited in maxima hunting (MH) (Berrendero et al.  2016b).
In this work  we introduce recursive Maxima Hunting (RMH)  a novel variable selection method for
feature selection in functional data classiﬁcation that takes advantage of the good properties of MH
while addressing some of its deﬁciencies. The extension of MH consists in removing the information
conveyed by each selected local maximum before searching for the next one in a recursive manner.
The rest of the paper is organized as follows: Maxima hunting for feature selection in classiﬁcation
problems with functional data is introduced in Section 2. Recursive maxima hunting  which is the
method proposed in this work  is described in Section 3. The improvements that can be obtained with
this novel feature selection method are analyzed in an exhaustive empirical evaluations whose results
are presented and discussed in Section 4.

2 Maxima Hunting

Maxima hunting (MH) is a method for feature selection in functional classiﬁcation based on measuring
dependencies between values selected from {X(t)  t ∈ [0  1]} and the response variable (Berrendero
et al.  2016b). In particular  one selects the values {X(t1)  . . .   X(td)} whose dependence with the
class label (i.e.  the response variable) is locally maximal. Different measures of dependency can
be used for this purpose. In Berrendero et al. (2016b)  the authors propose the distance correlation
(Sz´ekely et al.  2007). The distance covariance between the random variables X ∈ Rp and Y ∈ Rq 
whose components are assumed to have ﬁnite ﬁrst-order moments  is

| ϕX Y (u  v) − ϕX (u)ϕY (v) |2 w(u  v)dudv 

(1)

2

(cid:90)

V 2(X  Y ) =

Rp+q

|v|1+q

where ϕX Y   ϕX  ϕY are the characteristic functions of (X  Y )  X and Y   respectively  w(u  v) =
(cpcq|u|1+p
Γ((1+d)/2) is half the surface area of the unit sphere in Rd+1  and | · |d
)−1  cd = π(1+d)/2
stands for the Euclidean norm in Rd.
In terms of V 2(X  Y )  the square of the distance correlation is

p

q

R2(X  Y ) =

√

V 2(X Y )

V 2(X X)V 2(Y Y )

0 

  V 2(X)V 2(Y ) > 0
V 2(X)V 2(Y ) = 0.

(2)

(cid:40)

The distance correlation is a measure of statistical independence; that is  R2(X  Y ) = 0 if and only
if X and Y are independent. Besides being deﬁned for random variables of different dimensions  it
has other valuable properties. In particular  it is rotationally invariant and scale equivariant (Sz´ekely
and Rizzo  2012). A further advantage over other measures of independence  such as the mutual
information  is that the distance correlation can be readily estimated using a plug-in estimator that
does not involve any parameter tuning. The almost sure convergence of the estimator V 2
n is proved in
Sz´ekely et al. (2007  Thm. 2).
To summarize  in maxima hunting  one selects the d different local maxima of the distance correlation
between X(t)  the values of random process at different instants t ∈ [0  1]  and the response variable
(3)

R2(X(t)  Y ) 

i = 1  2  . . .   d.

X(ti) = argmax
t∈[0 1]

Maxima Hunting is easy to interpret. It is also well-motivated from the point of view of FDA  because
it takes advantage of functional properties of the data  such as continuity  which implies that similar
information is conveyed by the values of the function at neighboring points. In spite of the simplicity
of the method  it naturally accounts for the relevance and redundancy trade-off in feature selection (Yu
and Liu  2004): the local maxima (3) are relevant for discrimination. Points around them  which do
not maximize the distance correlation with the class label  are automatically excluded. Furthermore  it
is also possible to derive a uniform convergence result  which provides additional theoretical support
for the method. Finally  the empirical investigation carried out in Berrendero et al. (2016b) shows
that MH performs well in standard benchmark classiﬁcation problems for functional data. In fact 
for some problems  one can show that the optimal (Bayes) classiﬁcation rules depends only on the
maxima of R2(X(t)  Y ).
However  maxima hunting presents also some limitations. First  it is not always a simple task
to estimate the local maxima  especially in functions that are very smooth or that vary abruptly.
Furthermore  there is no guarantee that different maxima are not redundant. In most cases  the local
maxima of R2(X(t)  Y ) are indeed relevant for classiﬁcation. However  there are important points
for which this quantity does not attain a maximum.
As an example  consider the family of classiﬁcation problems introduced in Berrendero et al. (2016b 
Prop. 3)  in which the goal is to discriminate trajectories generated by a standard Brownian motion
process  B(t)  and trajectories from the process B(t) + Φm k(t)  where

( 2k−1

(cid:105)
(cid:1) − X(cid:0) 2k−2
(cid:1)(cid:1) +(cid:0)X(cid:0) 2k−1
(cid:16)(cid:107)Φ(cid:48)

2m )(s)

2m   2k

(cid:17)

2m

2m

2m

(4)

( 2k−2

2m−1

Φm k(t) =

2m   2k−1

Assuming a balanced class distribution (P(Y = 0) = P(Y = 1) = 1/2)  the optimal classiﬁcation
1√
2m+1 .

2m )(s) − I
rule is g∗(x) = 1 if and only if(cid:0)X(cid:0) 2k−1
(cid:1) − X(cid:0) 2k
(cid:1)(cid:1) >
(cid:1) (cid:39) 0.3085 
= 1 − normcdf(cid:0) 1
(cid:1). However  the
the standard normal. The relevance function has a single maximum at X(cid:0) 2k−1

The optimal classiﬁcation error is L∗ = 1 − normcdf
where  (cid:107) · (cid:107) denotes the L2[0  1] norm  and normcdf(·) is the cumulative distribution function of

ds  m  k ∈ N  1 ≤ k ≤ 2m−1.

m k(t)(cid:107)
2

Bayes classiﬁcation rule involves three relevant variables  two of which are clearly not maxima of
R2(X(t)  Y ). In spite of the simplicity of these types of functional classiﬁcation problems  they are
important to analyze  because the set of functions Φm k  with m > 0 and k > 0 form an orthonormal
basis of the Dirichlet space D[0  1]  the space of continuous functions whose derivatives are in
L2[0  1]. Furthermore  this space is the reproducing kernel Hilbert space associated with Brownian
motion and plays and important role in functional classiﬁcation (M¨orters and Peres  2010; Berrendero
et al.  2015). In fact  any trend in the Brownian process can be approximated by a linear combination
or by a mixture of Φm k(t).

2m

2m

2

(cid:90) t

0

√

(cid:104)I

3

Figure 1: First row: Individual and average trajectories for the classiﬁcation of B(t) vs. B(t) + 2Φ3 3(t)
initially (left) and after the ﬁrst (center) and second (right) corrections. Second row: Values of R2(X(t)  Y ) as
a function of t. The variables required for optimal classiﬁcation are marked with vertical dashed lines.

To illustrate the workings of maxima hunting and its limitations we analyze in detail the classiﬁcation
problem B(t) vs. B(t) + 2Φ3 3(t)  which is of the type considered above. In this case  the optimal
classiﬁcation rule depends on the maximum X(5/8)  and on X(1/2) and X(3/4)  which are not
maxima  and would therefore not be selected by the MH algorithm. The optimal error is L∗ = 15.87%.
To illustrate the importance of selecting all the relevant variables  we perform simulations in which
we compare the accuracy of the linear Fisher discriminant with the maxima hunting selection  and
with the optimal variable selection procedures. In these experiments  independent training and test
samples of size 1000 are generated. The values reported are averages over 100 independent runs.
Standard deviations are given between parentheses. The average prediction error when only the
maximum of the trajectories is considered is 37.63%(1.44%). When all three variables are used the
empirical error is 15.98%(1%)  which is close to the Bayes error. When other points in addition
to the maximum are used (i.e.  (X(t1)  X(5/8)  X(t2)  with t1 and t2 randomly chosen so that
0 ≤ t1 < 5/8 < t2 ≤ 1) the average classiﬁcation error is 22.32%(2.18%). In the top leftmost plot
of Figure 1 trajectories from both classes  together with the corresponding averages (thick lines) are
shown. The relevance function R2(X(t)  Y ) is plotted below. The relevant variables  which are
required for optimal classiﬁcation  are marked by dashed vertical lines.

3 Recursive Maxima Hunting

As a variable selection process  MH avoids  at least partially  the redundancy introduced by the
continuity of the functions that characterize the instances. However  this local approach cannot detect
redundancies among different local maxima. Furthermore  there could be points in the trajectory
that do not correspond to maxima of the relevance function  but which are relevant when considered
jointly with the maxima. The goal of recursive maxima hunting (RMH) is to select the maxima of
R2(X(t)  Y ) in a recursive manner by removing at each step the information associated to the most
recently selected maximum. This avoids the inﬂuence of previously selected maxima  which can
obscure ulterior dependencies. The inﬂuence of a selected variable X(t0) on the rest of the trajectory
can be eliminated by subtracting the conditional expectation E(X(t)|X(t0)) from X(t). Assuming
that the underlying process is Brownian

E(X(t)|X(t0)) =

min(t  t0)

t0

X(t0) 

t ∈ [0  1].

(5)

In the subsequent iterations  there are two intervals: [t  t0] and [t0  1]. Conditioned on the value at
X(t0)  the process in the interval [t0  1] is still Brownian motion. By contrast  for the interval [0  t0]
the process is a Brownian bridge  whose conditional expectation is

E(X(t)|X(t0) =

min(t  t0) − t t0

t0(1 − t0)

X(t0) =

(cid:26) t

X(t0) 
X(t0) 

t0
1−t
1−t0

t < t0
t > t0.

(6)

As illustrated by the results in the experimental section  the Brownian hypothesis is a robust assump-
tion. Nevertheless  if additional information on the underlying stochastic processes is available  it can

4

01/41/23/41X(t)-2-1012Initialsteptime01/41/23/41R2(X(t);Y)00 101/41/23/41-2-1012After-rstcorrectiontime01/41/23/4100.401/41/23/41-2-1012Aftersecondcorrectiontime01/41/23/4100.1be incorporated to the algorithm during the calculation of the conditional expectation in Equations (5)
and (6).
The center and right plots in Figure 1 illustrate the behavior of RMH in the example described in
the previous section. The top center plot diplays the trajectories and corresponding averages (thick
lines) for both classes after applying the correction (5) with t0 = 5/8  which is the ﬁrst maximum of
the distance correlation function (bottom leftmost plot in Figure 1). The variable X(5/8) is clearly
uninformative once this correction has been applied. The distance correlation R2(X(t)  Y ) for the
corrected trajectories is displayed in the bottom center plot. Also in this plot the relevant variables
are marked by vertical dashed lines. It is clear that the subsequent local maxima at t = 1/2  in the
subinterval [0  5/8]  and at t = 3/4  in the subinterval  and [5/8  1] correspond to the remaining
relevant variables. The last column shows the corresponding plots after the correction is applied
anew (equations (6) with t0 = 1/2 in [0  5/8] and (5) with t0 = 3/4 in [5/8  1]). After this second
correction  the discriminant information has been removed. In consequence  the distance correlation
function  up to sample ﬂuctuations  is zero.
An important issue in the application of this method is how to decide when to stop the recursive
search. The goal is to avoid including irrelevant and/or redundant variables. To address the ﬁrst
problem  we only include maxima that are sufﬁciently prominent R2(X(tmax)  Y ) > s  where
0 < s < 1 can be used to gauge the relative importance of the maximum. Redundancy is avoided
by excluding points around a selected maximum tmax for which R2(X(tmax)  X(t)) ≥ r  for some
redundancy threshold 0 < r < 1  which is typically close to one. As a result of these two conditions
only a ﬁnite (typically small) number of variables are selected. This data-driven stopping criterion
avoids the need to set the number of selected variables beforehand or to determine this number by a
costly validation procedure. The sensitivity of the results to the values of r and s will be studied in
Section 4. Nonetheless  RMH has a good and robust performance for a wide range of reasonable
values of these parameters (r close to 1 and s close to 0). The pseudocode of the RMH algorithm is
given in Algorithm 1.

Algorithm 1 Recursive Maxima Hunting
1: function RMH(X(t)  Y )
t∗ ← [ ]
2:
RMH rec(X(t) Y  0 1)
3:
return t∗
4:
5: end function
6: procedure RMH REC(X(t)  Y  tinf   tsup)
7:

(cid:8)R2(X(t)  Y )(cid:9)

(cid:46) Vector of selected points initially empty
(cid:46) Recursive search of the maxima of R2(X(t)  Y )
(cid:46) Vector of selected points

tmax ← argmax
tinf ≤t≤tsup
if R2(X(tmax)  Y ) > s then
t∗ ← [t∗ tmax]
(cid:46) Include tmax in t∗ the vector of selected points
X(t) ← X(t) − E(X(t) | X(tmax))  t ∈ [tinf   tsup] (cid:46) Correction of type (5) or (6) as required
return

else

8:
9:
10:
11:
12:
13:
14:
15:

16:
17:
18:
19:
20:

21:
22:
23:
24:
25: end procedure

end if
return

(cid:8)t : R2 (X(tmax)  X(t)) ≤ r(cid:9)

end if
(cid:46) Exclude redundant points to the left of tmax
max ← max
t−
tinf ≤t<tmax
if t−
max > tinf then
RMH rec(X(t)  Y  tinf   t−

(cid:8)t : R2 (X(tmax)  X(t)) ≤ r(cid:9)

end if
(cid:46) Exclude redundant points to the right of tmax
max ← min
t+
tmax<t≤tsup
max < tsup then
if t+
RMH rec(X(t)  Y  t+

max  tsup)

max)

5

(cid:46) Recursion on left subinterval

(cid:46) Recursion on right subinterval

4 Empirical study

√

To assess the performance of RMH  we have carried out experiments in simulated and real-world
data in which it is compared with some well-established dimensionality reduction methods  such
as PCA (Ramsay and Silverman  2005) and partial least squares (Delaigle and Hall  2012b)  and
with Maxima Hunting (Berrendero et al.  2016b). In these experiments  k-nearest neighbors (kNN)
with the Euclidean distance is used for classiﬁcation. kNN has been selected because it is a simple 
nonparametric classiﬁer with reasonable overall predictive accuracy. The value k in kNN is selected
by 10-fold CV from integers in [1 
Ntrain]  where Ntrain is the size of the training set. Since RMH
is a ﬁlter method for variable selection  the results are expected to be similar when other types of
classiﬁers are used. As a reference  the results of kNN using complete trajectories (i.e.  without
dimensionality reduction) are also reported. This approach is referred to as Base. Note that  in this
case  the performance of kNN need not be optimal because of the presence of irrelevant attributes.
RMH requires determining the values of two hyperparameters: the redundancy threshold r (0 < r < 1
typically close to 1)  and the relevance threshold s (0 < s < 1 typically close to 0). Through extensive
simulations we have observed that RMH is quite robust for a wide range of appropriate values of
these parameters. In particular  the results are very similar for values of r in the interval [0.75  0.95].
The predictive accuracy is somewhat more sensitive to the choice of s: If the value of s is too small 
irrelevant variables can be selected. If s is too large  it is possible that relevant points are excluded.
For most of the experiments performed  the optimal values of s are between 0.025 and 0.1. In view
of these observations  the experiments are made using r = 0.8. The value of s is selected from the set
{0.025  0.05  0.1} by 10-fold CV. A more careful determination of r and s is beneﬁcial  especially in
some extreme problems (e.g.  with very smooth or with rapidly-varying trajectories). In RMH  the
number of selected variables  which is not determined beforehand  depends indirectly on the values
of r and s. In the other methods  the number of selected variables is determined using 10-fold CV 
with maximum of 30.
A ﬁrst batch of experiments is carried out on simulated data generated from the model

(cid:26) P0 : B(t)

P1 : B(t) + m(t)

t ∈ [0  1]
t ∈ [0  1]

 

 
 

where B(t) is standard Brownian motion  m(t) is a deterministic trend  and P(Y = 0) = P(Y =
1) = 1/2. Using Berrendero et al. (2015  Theorem 2)  it is possible to compute the optimal
classiﬁcation rules g∗ and the corresponding Bayes errors L∗. To ensure a wide coverage  we
consider two problems in which the Bayes rule depends only on a few variables and two problems in
which complete trajectories are needed for optimal classiﬁcation: (i) Peak: m(t) = 2Φ3 3(t). The
optimal rule depends only on X(1/2)  X(5/8) and X(3/4). The Bayes error is L∗ (cid:39) 0.1587. This
is the example analyzed in the previous section. (ii) Peak2: m(t) = 2Φ3 2(t) + 3Φ3 3(t) − 2Φ2 2(t).
The optimal rule depends only on X(1/4)  X(3/8)  X(1/2)  X(5/8)  X(3/4)  and X(1). The
Bayes error is L∗ (cid:39) 0.0196. (iii) Square: m(t) = 2t2. The Bayes error is L∗ (cid:39) 0.1241. (iv) Sin:
m(t) = 1/2 sin(2φt). The Bayes error is L∗ (cid:39) 0.1333. In Figure 2 we have plotted some trajectories
corresponding to class 1 instances  together with their corresponding averages (thick lines). Class 0
trajectories are realizations of a standard Brownian process. In these experiments  training samples
of different sizes (Ntrain = {50  100  200  500  1000}) and an independent test set of size 1000 are
generated. The trajectories are discretized in 200 points. Half of the trajectories belong to each class
in both the training and test sets. The values reported are averages over 200 independent repetitions.
Figure 3 displays the average classiﬁcation error (ﬁrst row) and the average number of selected
variable /components (second row) as a function of the training sample size for each model and
classiﬁcation method. Horizontal dashed lines are used to indicate the Bayes error level in the
different problems. From the results reported in Figure 3  one concludes that RMH has the best
overall performance. It is always more accurate than the Base method. This observation justiﬁes
performing variable selection not only for the sake of dimensionality reduction  but also to improve
the classiﬁcation accuracy. RMH is also better than the original MH in all the problems investigated:
there is both an improvement of the prediction error  and a reduction of the numeber of variables
used for classiﬁcation. In peak and peak2  problems in which the relevant variables are known  RMH
generally selects the correct ones. As expected  PLS performs better than PCA. However  both MH
and RMH outperform these projection methods  except in sin  where their accuracies are similar.
Both PLS and RMH are effective dimensionality reduction methods with comparable performance.

6

Figure 2: Class 1 trajectories and averages (thick lines) for the different synthetic problems.

Figure 3: Average classiﬁcation error (ﬁrst row) and average number of selected variables/components (second
row) as a function of the size of the training.

However  the components selected in PLS are  in general  more difﬁcult to interpret because they
involve whole trajectories. Finally  the accuracy of RMH is very close to the Bayes level for higher
sample sizes  even when the optimal rule requires using complete trajectories (square and sin).
To assess the performance of RMH in real-world functional classiﬁcation problems  we have carried
out a second batch of experiments in four datasets  which are commonly used as benchmarks in the
FDA literature. Instances in Growth correspond to curves of the heights of 54 girls and 38 boys
from the Berkeley Growth Study. Observations are discretized in 31 non-equidistant ages between 1
and 18 years (Ramsay and Silverman  2005; Mosler and Mozharovskyi  2014). The Tecator dataset
consists of 215 near-infrared absorbance spectra of ﬁnely chopped meat. The spectral curves consist
of 100 equally spaced points. The class labels are determined in terms of fat content (above or below
20%). The curves are fairly smooth. In consequence  we have followed the general recommendation
and used the second derivative for classiﬁcation (Ferraty and Vieu  2006; Galeano et al.  2014).
The Phoneme data consists of 4509 log-periodograms observed at 256 equidistant points. Here  we
consider the binary problem of distinguishing between the phonemes “aa” (695) and “ao” (1022)
(Galeano et al.  2014). Following Delaigle and Hall (2012a)  the curves are smoothed with a local
linear method and truncated to the ﬁrst 50 variables. The Medﬂies are records of daily egg-laying
patterns of a thousand ﬂies. The goal is to discriminate between short- and long-lived ﬂies. Following
Mosler and Mozharovskyi (2014)  curves equal to zero are excluded. There are 512 30-day curves
(starting from day 5) of ﬂies who live at most 34 days  266 of these are long-lived (reach the day
44). The classes in Growth and Tecator are well separated. In consequence  they are relatively easy
problems. By contrast  Phoneme and Medﬂies are notoriously difﬁcult classiﬁcation tasks. Some
trajectories of each problem and each class  together with the corresponding averages (thick lines) 
are plotted in Figure 4. To estimate the classiﬁcation error  the datasets are partitioned at random
into a training set (with 2/3 of the observations) and a test set (1/3). This procedure is repeated
200 times. The boxplots of the results for each dataset and method are shown in Figure 5. Errors
are shown in ﬁrst row and the number of selected variables/components in the second one. From

7

−3−2−10123X(t)|Y=1Peak−3−2−10123Peak2−3−2−10123Square−3−2−10123Sin50100200500100000.050.10.150.20.250.3Peak250100200500100005101520Ntrain5010020050010000.150.20.250.30.350.40.45ClassiﬁcationerrorPeak50100200500100005101520NtrainNumberofvariables5010020050010000.140.160.180.20.220.240.26SinPCAPLSMHRMHBaseL*50100200500100005101520Ntrain5010020050010000.120.140.160.180.20.220.24Square50100200500100005101520NtrainFigure 4: Trajectories for each of the classes and their corresponding averages (thick lines).

Figure 5: Classiﬁcation error (ﬁrst row) and number of variables/components selected (second row) by RMH.

these results we observe that  in general  dimensionality reduction is effective: the accuracy of the
four considered methods is similar or better than the Base method  in which complete trajectories
are used for classiﬁcation. In particular  Base does not perform well when the trajectories are not
smooth (Medﬂies). The best overall performance corresponds to RMH. In the easy problems (Growth
and Tecator)  all methods behave similarly and give good results. In Growth  RMH is slightly more
accurate. However  it tends to select more variables than the other methods. In the more difﬁcult
problems  (Phoneme and Medﬂies)  RMH yields very accurate predictions while selecting only two
variables. In these problems it exhibits the best performance  except in Phoneme  where Base is
more accurate. The variables selected by RMH and MH are directly interpretable  which is an
advantage over projection-based methods (PCA  PLS). Finally  let us point out that the accuracy of
RMH is comparable and often better that state-of-the-art functional classiﬁcation methods. See  for
instance  Berrendero et al. (2016a); Delaigle et al. (2012); Delaigle and Hall (2012a); Mosler and
Mozharovskyi (2014); Galeano et al. (2014). In most of these works no dimensionality reduction is
applied. Nevertheless  these comparisons must be done carefully because the evaluation protocol and
the classiﬁers used vary in the different studies. In any case  RMH is a ﬁlter method  which means
that it could be more effective if used in combination with other types of classiﬁers or adapted and
used as a wrapper or  even  as an embedded variable selection method.

Acknowledgments

The authors thank Dr. Jos´e R. Berrendero for his insightful suggestions. We also acknowledge
ﬁnancial support from the Spanish Ministry of Economy and Competitiveness  project TIN2013-
42351-P and from the Regional Government of Madrid  CASI-CAM-CM project (S2013/ICE-2845).

8

10203050100150200X(t)|Y=0Growth10203050100150200X(t)|Y=120406080100−4−2024x 10−3Tecator20406080100−4−2024x 10−3102030405010152025Phoneme10203040501015202551015202530050100150Medﬂies51015202530050100150PCA PLS MHRMH Base00.050.10.150.20.250.3ClassiﬁcationerrorGrowthPCA PLS MHRMH024681012NumberofvariablesPCA PLS MHRMH Base00.020.040.060.08 TecatorPCA PLS MHRMH051015 PCA PLS MHRMH Base0.160.180.20.220.24 PhonemePCA PLS MHRMH024681012 PCA PLS MH RMH Base0.30.40.50.6 MedﬂiesPCA PLS MHRMH051015202530 References
Aneiros  G. and P. Vieu (2014). Variable selection in inﬁnite-dimensional problems. Statistics & Probability

Ba´ıllo  A.  A. Cuevas  and R. Fraiman (2011). Classiﬁcation methods for functional data  pp. 259–297. Oxford:

Letters 94  12–20.

Oxford University Press.

Berrendero  J. R.  A. Cuevas  and J. L. Torrecilla (2015). On near perfect classiﬁcation and functional Fisher

rules via reproducing kernels. arXiv:1507.04398  1–27.

Berrendero  J. R.  A. Cuevas  and J. L. Torrecilla (2016a). The mRMR variable selection method: a comparative

study for functional data. Journal of Statistical Computation and Simulation 86(5)  891–907.

Berrendero  J. R.  A. Cuevas  and J. L. Torrecilla (2016b). Variable selection in functional data classiﬁcation: a

maxima hunting proposal. Statistica Sinica 26(2)  619–638.

Delaigle  A. and P. Hall (2012a). Achieving near perfect classiﬁcation for functional data. Journal of the Royal

Statistical Society B 74(2)  267–286.

The Annals of Statistics 40(1)  322–352.

Biometrika 99(2)  299–313.

Delaigle  A. and P. Hall (2012b). Methodology and theory for partial least squares applied to functional data.

Delaigle  A.  P. Hall  and N. Bathia (2012). Componentwise classiﬁcation and clustering of functional data.

Ding  C. and H. Peng (2005). Minimum redundancy feature selection from microarray gene expression data.

Journal of Bioinformatics and Computational Biology 3(2)  185–205.

Fernandez-Lozano  C.  J. A. Seoane  M. Gestal  T. R. Gaunt  J. Dorado  and C. Campbell (2015). Texture

classiﬁcation using feature selection and kernel-based techniques. Soft Computing 19(9)  2469–2480.

Ferraty  F.  P. Hall  and P. Vieu (2010). Most-predictive design points for functional data predictors.

Ferraty  F. and P. Vieu (2006). Nonparametric Functional Data Analysis: Theory and Practice. Springer.
Fraiman  R.  Y. Gim´enez  and M. Svarc (2016). Feature selection for functional data. Journal of Multivariate

Biometrika 97(4)  807–824.

Analysis 146  191–208.

Galeano  P.  E. Joseph  and R. E. Lillo (2014). The Mahalanobis distance for functional data with applications to

classiﬁcation. Technometrics 57(2)  281–291.

G´omez-Verdejo  V.  M. Verleysen  and J. Fleury (2009). Information-theoretic feature selection for functional

data classiﬁcation. Neurocomputing 72(16)  3580–3589.

Grosenick  L.  S. Greer  and B. Knutson (2008). Interpretable classiﬁers for FMRI improve prediction of

purchases. Neural Systems and Rehabilitation Engineering  IEEE Transactions on 16(6)  539–548.

Guyon  I.  S. Gunn  M. Nikravesh  and L. A. Zadeh (2006). Feature Extraction: Foundations and Applications.

Springer.

The Annals of Statistics 39(5)  2410–2447.

Data Analysis 52(10)  4790–4800.

Kneip  A. and P. Sarda (2011). Factor models and variable selection in high-dimensional regression analysis.

Li  B. and Q. Yu (2008). Classiﬁcation of functional data: A segmentation approach. Computational Statistics &

Lindquist  M. A. and I. W. McKeague (2009). Logistic regression with Brownian-like predictors. Journal of the

American Statistical Association 104(488)  1575–1585.

McKeague  I. W. and B. Sen (2010). Fractals with point impact in functional linear regression. Annals of

Statistics 38(4)  2559.

tics 22(2)  223–235.

M¨orters  P. and Y. Peres (2010). Brownian Motion. Cambridge University Press.
Mosler  K. and P. Mozharovskyi (2014). Fast DD-classiﬁcation of functional data. Statistical Papers 55  49–59.
Preda  C.  G. Saporta  and C. L´ev´eder (2007). PLS classiﬁcation of functional data. Computational Statis-

Ramsay  J. O. and B. W. Silverman (2005). Functional Data Analysis. Springer.
Ryali  S.  K. Supekar  D. A. Abrams  and V. Menon (2010). Sparse logistic regression for whole-brain

classiﬁcation of fMRI data. NeuroImage 51(2)  752–764.

Sz´ekely  G. J. and M. L. Rizzo (2012). On the uniqueness of distance covariance. Statistics & Probability

Letters 82(12)  2278–2282.

Sz´ekely  G. J.  M. L. Rizzo  and N. K. Bakirov (2007). Measuring and testing dependence by correlation of

distances. The Annals of Statistics 35(6)  2769–2794.

Tian  T. S. and G. M. James (2013). Interpretable dimension reduction for classifying functional data. Computa-

tional Statistics & Data Analysis 57(1)  282–296.

Xiaobo  Z.  Z. Jiewen  M. J. Povey  M. Holmes  and M. Hanpin (2010). Variables selection methods in

near-infrared spectroscopy. Analytica Chimica Acta 667(1)  14–32.

Yu  L. and H. Liu (2004). Efﬁcient feature selection via analysis of relevance and redundancy. The Journal of

Zhou  J.  N.-Y. Wang  and N. Wang (2013). Functional linear model with zero-value coefﬁcient function at

Machine Learning Research 5  1205–1224.

sub-regions. Statistica Sinica 23(1)  25–50.

9

,José Torrecilla
Alberto Suárez