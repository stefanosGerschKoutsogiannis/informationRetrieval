2012,A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,In this paper  we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality  and with limited number of samples  recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix  or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into  smaller sub-problems  and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound  we propose a clustering algorithm that attempts to minimize this bound  and in practice  is able to find effective partitions of the variables. We further use the approximate solution  i.e.  solution resulting from solving the sub-problems   as an initial point to solve the original problem  and achieve a much faster computational procedure.  As an example  a recent state-of-the-art method  QUIC requires 10 hours to solve a problem (with 10 000 nodes) that arises from a climate application  while our proposed algorithm  Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem.,A Divide-and-Conquer Procedure for Sparse Inverse

Covariance Estimation

Cho-Jui Hsieh

Dept. of Computer Science
University of Texas  Austin

cjhsieh@cs.utexas.edu

Inderjit S. Dhillon

Dept. of Computer Science
University of Texas  Austin

inderjit@cs.utexas.edu

Pradeep Ravikumar

Dept. of Computer Science

University of Texas

pradeepr@cs.utexas.edu

Arindam Banerjee

Dept. of Computer Science & Engineering

University of Minnesota  Twin Cities

banerjee@cs.umn.edu

Abstract

We consider the composite log-determinant optimization problem  arising from
the (cid:96)1 regularized Gaussian maximum likelihood estimator of a sparse inverse
covariance matrix  in a high-dimensional setting with a very large number of vari-
ables. Recent work has shown this estimator to have strong statistical guarantees
in recovering the true structure of the sparse inverse covariance matrix  or alter-
natively the underlying graph structure of the corresponding Gaussian Markov
Random Field  even in very high-dimensional regimes with a limited number of
samples. In this paper  we are concerned with the computational cost in solving
the above optimization problem. Our proposed algorithm partitions the problem
into smaller sub-problems  and uses the solutions of the sub-problems to build a
good approximation for the original problem. Our key idea for the divide step to
obtain a sub-problem partition is as follows: we ﬁrst derive a tractable bound on
the quality of the approximate solution obtained from solving the corresponding
sub-divided problems. Based on this bound  we propose a clustering algorithm
that attempts to minimize this bound  in order to ﬁnd effective partitions of the
variables. For the conquer step  we use the approximate solution  i.e.  solution
resulting from solving the sub-problems  as an initial point to solve the original
problem  and thereby achieve a much faster computational procedure.

Introduction

1
Let {x1  x2  . . .   xn} be n sample points drawn from a p-dimensional Gaussian distribution
N (µ  Σ)  also known as a Gaussian Markov Random Field (GMRF)  where each xi is a p-
dimensional vector. An important problem is that of recovering the covariance matrix  or its inverse 
given the samples in a high-dimensional regime where n (cid:28) p  and p could number in the tens of
thousands. In such settings  the computational efﬁciency of any estimator becomes very important.
A popular approach for such high-dimensional inverse covariance matrix estimation is to impose the
structure of sparsity on the inverse covariance matrix (which can be shown to encourage conditional
independences among the Gaussian variables)  and to solve the following (cid:96)1 regularized maximum
likelihood problem:

arg min
Θ(cid:31)0

(cid:80)n
i=1(xi − ˜µ)(xi − ˜µ)T is the sample covariance matrix and ˜µ = 1

{− log det Θ + tr(SΘ) + λ(cid:107)Θ(cid:107)1} = arg min
Θ(cid:31)0

f (Θ) 

where S = 1
i=1 xi is the
n
sample mean. The key focus in this paper is on developing computationally efﬁcient methods to
solve this composite log-determinant optimization problem.

n

(cid:80)n

(1)

1

Due in part to its importance  many optimization methods [4  1  9  7  6] have been developed in
recent years for solving (1). However  these methods have a computational complexity of at least
O(p3) (typically this is the complexity per iteration). It is therefore hard to scale these procedures
to problems with tens of thousands of variables. For instance  in a climate application  if we are
modeling a GMRF over random variables corresponding to each Earth grid point  the number of
nodes can easily number in the tens of thousands. For this data  a recently proposed state-of-the-art
method QUIC [6]  that uses a Newton-like method to solve (1)  for instance takes more than 10
hours to converge.
A natural strategy when the computational complexity of a procedure scales poorly with the problem
size is a divide and conquer strategy: Given a partition of the set of nodes  we can ﬁrst solve the
(cid:96)1 regularized MLE over the sub-problems invidually  and than in the second step  aggregate the
solutions together to get ¯Θ. But how do we come up with a suitable partition? The main contribution
of this paper is to provide a principled answer to this question. As we show  our resulting divide and
conquer procedure produces overwhelming improvements in computational efﬁciency.
Interestingly  [8] recently proposed a decomposition-based method for GMRFs. They ﬁrst observe
the following useful property of the composite log-determinant optimization problem in (1): if we
threshold the off-diagonal elements of the sample covariance matrix S  and the resulting thresholded
matrix is block-diagonal  then the corresponding inverse covariance matrix has the same block-
diagonal sparsity structure as well. Using this property  they decomposed the problem along these
block-diagonal components and solved these separately  thus achieving a sharp computational gain.
A major drawback to this approach of [8] however is that often the decomposition of the thresholded
sample covariance matrix can be very unbalanced — indeed  in many of our real-life examples  we
found that the decomposition resulted in one giant component and several very small components.
In these cases  the approach in [8] is only a bit faster than directly solving the entire problem.
In this paper  we propose a different strategy based on the following simple idea. Suppose we are
given a particular partitioning  and solve the sub-problems speciﬁed by the partition components.
The resulting decomposed estimator ¯Θ clearly need not be equal to (cid:96)1 regularized MLE (1). How-
ever  can we use bounds on the deviation to propose a clustering criterion? We ﬁrst derive a bound
on (cid:107) ¯Θ − Θ∗(cid:107)F based on the off-diagonal error of the partition. Based on this bound  we propose
a normalized-cut spectral clustering algorithm to minimize the off-diagonal error  which is able to
ﬁnd a balanced partition such that ¯Θ is very close to Θ∗. Interestingly  we show that this clustering
criterion can also be motivated as leveraging a property more general than that in [8] of the (cid:96)1 reg-
ularized MLE (1). In the “conquering” step  we then use ¯Θ to initialize an iterative solver for the
original problem (1). As we show  the resulting algorithm is much faster than other state-of-the-art
methods. For example  our algorithm can achieve an accurate solution for the climate data problem
in 1 hour  whereas directly solving it takes 10 hours.
In section 2  we outline the standard skeleton of a divide and conquer framework for GMRF es-
timation. The key step in such a framework is to come up with a suitable and efﬁcient clustering
criterion. In the next section 3  we then outline our clustering criteria. Finally  in Section 4 we show
that in practice  our method achieves impressive improvements in computational efﬁciency.
2 The Proposed Divide and Conquer Framework
We ﬁrst set up some notation. In this paper  we will consider each p × p matrix X as an adjacency
matrix  where V = {1  . . .   p} is the node set  Xij is the weighted link between node i and node j.
c=1 to denote a disjoint partitioning of the node set V  and each Vc will be called a
We will use {Vc}k
partition or a cluster.
Given a partition {Vc}k
tions to get the inverse covariance matrices {Θ(c)}k

c=1  our divide and conquer algorithm ﬁrst solves GMRF for all node parti-

c=1  and then uses the following matrix

Θ(1)

...

0

0

¯Θ =

  

0

Θ(2)
...

0

. . .
. . .

0
0

...
...
0 Θ(k)

(2)

to initialize the solver for the whole GMRF. In this paper we use X (c) to denote the submatrix
XVc Vc for any matrix X. Notice that in our framework any sparse inverse covariance solver can

2

be used  however  in this paper we will focus on using the state-of-the-art method QUIC [6] as the
base solver  which was shown to have super-linear convergence when close to the solution. Using a
better starting point enables QUIC to more quickly reach this region of super-linear convergence  as
we will show later in our experiments.
The skeleton of the divide and conquer framework is quite simple and is summarized in Algorithm 1.
In order that Algorithm 1 be efﬁcient  we require that ¯Θ deﬁned in (2) should be close to the optimal
solution of the original problem Θ∗. In the following  we will derive a bound for (cid:107)Θ∗− ¯Θ(cid:107)F . Based
on this bound  we propose a spectral clustering algorithm to ﬁnd an effective partitioning of the
nodes.

: Empirical covariance matrix S  scalar λ

Algorithm 1: Divide and Conquer method for Sparse Inverse Covariance Estimation
Input
Output: Θ∗  the solution of (1)
Obtain a partition of the nodes {Vc}k
for c = 1  . . .   k do

c=1 ;

Solve (1) on S(c) and subset of variables in Vc to get Θ(c);

end
Form ¯Θ by Θ(1)  Θ(2)  . . .   Θ(k) as in (2) ;
Use ¯Θ as an initial point to solve the whole problem (1) ;

1
2
3
4
5
6

2.1 Hierarchical Divide and Conquer Algorithm

Assume we conduct a k-way clustering  then the initial time for solving sub-problems is at least
O(k(p/k)3) = O(p3/k2) where p denotes the dimensionality  When we consider k = 2  the divide
and conquer algorithm can be at most 4 times faster than the original one. One can increase k 
however  a larger k entails a worse initial point for training the whole problem.
Based on this observation  we consider the hierarchical version of our divide-and-conquer algorithm.
For solving subproblems we can again apply a divide and conquer algorithm. In this way  the initial
time can be much less than O(p3/k2) if we use divide and conquer algorithm hierarchically for
each level. In the experiments  we will see that this hierarchical method can further improve the
performance of the divide-and-conquer algorithm.

3 Main Results: Clustering Criteria for GMRF
This section outlines the main contribution of this paper; in coming up with suitable efﬁcient clus-
tering criteria for use within the divide and framework structure in the previous section.

3.1 Bounding the distance between Θ∗ and ¯Θ

To start  we discuss the following result from [8]  which we reproduce using the notation in this
paper for convenience. Speciﬁcally  [8] shows that when all the between cluster edges in S have
absolute values smaller than λ  Θ∗ will have a block-diagonal structure.
c=1  if |Sij| ≤ λ for all i  j in different
Theorem 1 ([8]). For any λ > 0 and a given partition {Vc}k
partitions  then Θ∗ = ¯Θ  where Θ∗ is the optimal solution of (1) and ¯Θ is as deﬁned in (2).
As a consequence  if a partition {Vc}k
c=1 satisﬁes the assumption of Theorem 1  ¯Θ and Θ∗ will be
the same  and the last step of Algorithm 1 is not needed anymore. Therefore the result in [8] may be
viewed as a special case of our Divide-and-Conquer Algorithm 1.
However  in most real examples  a perfect partitioning as in Theorem 1 does not exist  which moti-
vates a divide and conquer framework that does not need as stringent assumptions as in Theorem 1.
To allow a more general relationship between Θ∗ and ¯Θ  we ﬁrst prove a similar property for the
following generalized inverse covariance problem:

Θ∗ = arg min
Θ(cid:31)0

{− log det Θ + tr(SΘ) +

3

(cid:88)

i j

Λij|Θij|} = arg min
Θ(cid:31)0

fΛ(Θ).

(3)

In the following  we use 1λ to denote a matrix with all elements equal to λ. Therefore (1) is a special
case of (3) with Λ = 1λ. In (3)  the regularization parameter Λ is a p×p matrix  where each element
corresponds to a weighted regularization of each element of Θ. We can then prove the following
theorem  as a generalization of Theorem 1.
Theorem 2. For any matrix regularization parameter Λ (Λij > 0 ∀i  j) and a given partition
{Vc}k
c=1  if |Sij| ≤ Λij for all i  j in different partitions  then the solution of (3) will be the block
diagonal matrix ¯Θ deﬁned in (2)  where Θ(c) is the solution for (3) with sample covariance S(c) and
regularization parameter Λ(c).

Proof. Consider the dual problem of (3):

|Wij − Sij| ≤ Λij ∀i  j 

max
W(cid:31)0

log det W s.t.

lution of (4) with the objective function value (cid:80)k
ity [2]  det ˆW ≤ (cid:81)k

(4)
based on the condition stated in the theorem  we can easily verify ¯W = ¯Θ−1 is a feasible so-
c=1 log det ¯W (c). To show that ¯W is the op-
timal solution of (4)  we consider an arbitrary feasible solution ˆW . From Fischer’s inequal-
c=1 det ˆW (c) for ˆW (cid:31) 0. Since ¯W (c) is the optimizer of the c-th block 
det ¯W (c) ≥ det ˆW (c) for all c  which implies log det ˆW ≤ log det ¯W . Therefore ¯Θ is the primal
optimal solution.

Next we apply Theorem 2 to develop a decomposition method. Assume our goal is to solve (1) and
we have clusters {Vc}k
c=1 which may not satisfy the assumption in Theorem 1. We start by choosing
a matrix regularization weight ¯Λ such that

¯Λij =

max(|Sij|  λ)

if i  j are in the same cluster 
if i  j are in different clusters.

(5)

(cid:26)λ

(cid:26)0

Now consider the generalized inverse covariance problem (3) with this speciﬁed ¯Λ. By construction 
the assumption in Theorem 2 holds for ¯Λ  so we can decompose this problem into k sub-problems;
for each cluster c ∈ {1  . . .   k}  the subproblem has the following form:

Θ(c) = arg min
Θ(cid:31)0

{− log det Θ + tr(S(c)Θ) + λ(cid:107)Θ(cid:107)1} 

where S(c) is the sample covariance matrix of cluster c. Therefore  ¯Θ is the optimal solution of
problem (3) with ¯Λ as the regularization parameter.
Based on this observation  we will now provide another view of our divide and conquer algorithm as
follows. Considering the dual problem of the sparse inverse covariance estimation with the weighted
regularization deﬁned in (4)  Algorithm 1 can be seen to solve (4) with Λ = ¯Λ deﬁned in (5) to get
the initial point ¯W   and then solve (4) with Λ = 1λ for all elements. Therefore we initially solve
the problem with looser bounded constraints to get an initial guess  and then solve the problem with
tighter constraints. Intuitively  if the relaxed constraints ¯Λ are close to the real constraint 1λ  the
solutions ¯W and W ∗ will be close to each other. So in the following we derive a bound based on
this observation.
For convenience  we use P λ to denote the original dual problem (4) with Λ = 1λ  and P ¯Λ to denote
the relaxed dual problem with different edge weights across edges as deﬁned in (5). Based on the
above discussions  W ∗ = (Θ∗)−1 is the solution of P λ and ¯W = ¯Θ−1 is the solution of P ¯Λ. We
deﬁne E as the following matrix:

Eij =

max(|Sij| − λ  0)

if i  j are in the same cluster 
otherwise.

(6)

If E = 0  all the off-diagonal elements are below the threshold λ  so W ∗ = ¯W by Theorem 2. In
the following we consider a more interesting case where E (cid:54)= 0. In this case (cid:107)E(cid:107)F measures how
much the off-diagonal elements exceed the threshold λ  and a good clustering algorithm should be
able to ﬁnd a partition to minimize (cid:107)E(cid:107)F . In the following theorem we show that (cid:107)W ∗ − ¯W(cid:107)F can
be bounded by (cid:107)E(cid:107)F   therefore (cid:107)Θ∗ − ¯Θ(cid:107)F can also be bounded by (cid:107)E(cid:107)F :

4

Theorem 3. If there exists a γ > 0 such that (cid:107)E(cid:107)2 ≤ (1 − γ)

1(cid:107) ¯W(cid:107)2

  then

p max(σmax( ¯W )  σmax(W ∗))

(cid:107)W ∗− ¯W(cid:107)F <
(cid:107)E(cid:107)F  
(cid:107)Θ∗ − ¯Θ(cid:107)F ≤ p max(σmax( ¯Θ)  σmax(Θ∗))2σmax( ¯Θ)
where σmin(·)  σmax(·) denote the minimum/maximum singular values.

γ min(σmin(Θ∗)  σmin( ¯Θ))

γσmin( ¯W )

(cid:107)E(cid:107)F  

(7)

(8)

Proof. To prove Theorem 3  we need the following Lemma  which is proved in the Appendix:
Lemma 1. If A is a positive deﬁnite matrix and there exists a γ > 0 such that (cid:107)A−1B(cid:107)2 ≤ 1 − γ 
then

log det(A + B) ≥ log det A − p/(γσmin(A))(cid:107)B(cid:107)F .

(9)

p

Since P ¯Λ has a relaxed bounded constraint than P λ  ¯W may not be a feasible solution of P λ.
However  we can construct a feasible solution ˆW = ¯W − G ◦ E  where Gij = sign(Wij) and
◦ indicates the entrywise product of two matrices. The assumption of this theorem implies that
(cid:107)G ◦ E(cid:107)2 ≤ (1 − γ)/(cid:107) ¯W(cid:107)2  so (cid:107) ¯W −1(G ◦ E)(cid:107) ≤ (1 − γ). From Lemma 1 we have log det ˆW ≥
log det ¯W −
γσmin( ¯W )(cid:107)E(cid:107)F . Since W ∗ is the optimal solution of P λ and ˆW is a feasible solution
of P λ  log det W ∗ ≥ log det ˆW ≥ log det ¯W −
γσmin( ¯W )(cid:107)E(cid:107)F . Also  since ¯W is the optimal
solution of P ¯Λ and W ∗ is a feasible solution of P ¯Λ  we have log det W ∗ < log det ¯W . Therefore 
| log det ¯W − log det W ∗| <
By the mean value theorem and some calculations  we have |f (W ∗) − f ( ¯W )| >
max(σmax( ¯W ) σmax(W ∗))   which implies (7).
To establish the bound on Θ  we use the mean value theorem again with g(W ) = W −1 = Θ 
∇g(W ) = Θ ⊗ Θ where ⊗ is kronecker product. Moreover  σmax(Θ ⊗ Θ) = (σmax(Θ))2  so we
can combine with (7) to prove (8).

γσmin( ¯W )(cid:107)E(cid:107)F .

(cid:107) ¯W−W ∗(cid:107)F

p

p

3.2 Clustering algorithm

In order to obtain computational savings  the clustering algorithm for the divide-and-conquer algo-
rithm (Algorithm 1) should satisfy three conditions: (1) minimize the distance between the approx-
imate and the true solution (cid:107) ¯Θ − Θ∗(cid:107)F   (2) be cheap to compute  and (3) partition the nodes into
balanced clusters.
Assume the real inverse covariance matrix Θ∗ is block-diagonal  then it is easy to show that W ∗
is also block-diagonal. This is the case considered in [8]. Now let us assume Θ∗ has almost a
block-diagonal structure but a few off-diagonal entries are not zero. Assume Θ∗ = Θbd + veieT
where Θbd is the block-diagonal part of Θ∗ and ei denotes the i-th standard basis vector  then from
Sherman-Morrison formula 

j

W ∗ = (Θ∗)−1 = (Θbd)−1 −

v

1 + v(Θbd)ij

θbd
i (θbd

j )T  

where θbd
is the ith column vector of Θbd. Therefore adding one off-diagonal element to Θbd will
i
introduce at most one nonzero off-diagonal block in W . Moreover  if block (i  j) of W is already
nonzero  adding more elements in block (i  j) of Θ will not introduce any more nonzero blocks in
W . As long as just a few entries in off-diagonal blocks of Θ∗ are nonzero  W will be block-diagonal
with a few nonzero off-diagonal blocks. Since (cid:107)W ∗−S∗(cid:107)∞ ≤ λ  we are able to use the thresholding
matrix Sλ to guess the clustering structure of Θ∗.
In the following  we show this observation is consistent with the bound we get in Theorem 3. From
i |σi(E)|. Since it is computationally
p(cid:107)E(cid:107)F   so that minimizing (cid:107)E(cid:107)F

(8)  ideally we want to ﬁnd a partition to minimize (cid:107)E(cid:107)∗ =(cid:80)
difﬁcult to optimize this directly  we can use the bound (cid:107)E(cid:107)∗ ≤ √
can be cast as a relaxation of the problem of minimizing (cid:107) ¯Θ − Θ∗(cid:107)F .

5

(cid:80)

k(cid:88)

c=1

To ﬁnd a partition minimizing (cid:107)E(cid:107)F   we want to ﬁnd a partition {Vc}k
off-diagonal block entries of Sλ is minimized  where Sλ is deﬁned as

c=1 such that the sum of

(Sλ)ij = max(|Sij| − λ  0)2 ∀ i (cid:54)= j and Sλ

(10)
At the same time  we want to have balanced clusters. Therefore  we minimize the following normal-
ized cut objective value [10]:

N Cut(Sλ {Vc}k

c=1) =

Sλ
ij

i∈Vc j /∈Vc
d(Vc)

where d(Vc) =

Sλ
ij.

(11)

In (11)  d(Vc) is the volume of the vertex set Vc for balancing cluster sizes  and the numerator is
the sum of off-diagonal entries  which corresponds to (cid:107)E(cid:107)2
F . As shown in [10  3]  minimizing the
normalized cut is equivalent to ﬁnding cluster indicators x1  . . .   xc to maximize
= trace(Y T (I − D−1/2SλD−1/2)Y ) 

c (D − Sλ)xc
xT

k(cid:88)

(12)

ij = 0 ∀i = j.
p(cid:88)
(cid:88)

i∈Vc

j=1

min

where D is a diagonal matrix with Dii = (cid:80)p

c Dx

xT

c=1

x

j=1 Sλ

ij  Y = D1/2X and X = [x1 . . . xc]. There-
fore  a common way for getting cluster indicators is to compute the leading k eigenvectors of
D−1/2SλD−1/2 and then conduct kmeans on these eigenvectors.
The time complexity of normalized cut on Sλ is mainly from computing the leading k eigenvectors
of D−1/2SλD−1/2  which is at most O(p3). Since most state-of-the-art methods for solving (1)
require O(p3) per iteration  the cost for clustering is no more than one iteration for the original
solver. If Sλ is sparse  as is common in real situations  we could speed up the clustering phase by
using the Graclus multilevel algorithm  which is a faster heuristic to minimize normalized cut [3].

4 Experimental Results
In this section  we ﬁrst show that the normalized cut criterion for the thresholded matrix Sλ in (10)
can capture the block diagonal structure of the inverse covariance matrix before solving (1). Using
the clustering results  we show that our divide and conquer algorithm signiﬁcantly reduces the time
needed for solving the sparse inverse covariance estimation problem.
We use the following datasets:

1. Leukemia: Gene expression data — originally provided by [5]  we use the data after the

pre-processing done in [7].

2. Climate: This dataset is generated from NCEP/NCAR Reanalysis data 1  with focus on
the daily temperature at several grid points on earth. We treat each grid point as a random
variable  and use daily temperature in year 2001 as features.

3. Stock: Financial dataset downloaded from Yahoo Finance 2. We collected 3724 stocks 

each with daily closing price recorded in latest 300 days before May 15  2012.

4. Synthetic: We generated synthetic data containing 20  000 nodes with 100 randomly gen-
erated group centers µ1  . . .   µ100  each of dimension 200  such that each group c has half
of its nodes with feature µc and the other half with features −µc. We then add Gaussian
noise to the features.

The data statistics are summarized in Table 1.

c=1  we use the following “within-cluster ratio” to determine its

(Θ∗

ij)2

.

(13)

4.1 Clustering quality on real datasets
Given a clustering partition {Vc}k
performance on Θ∗:

(cid:80)k

c=1

(cid:80)

(cid:80)

i j:i(cid:54)=j and i j∈Vc

i(cid:54)=j(Θ∗

ij)2

R({Vc}k

c=1) =

1www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.surface.html
2http://ﬁnance.yahoo.com/

6

Table 1: Dataset Statistics

Leukemia Climate Stock Synthetic

p
n

1255
72

10512
1464

3724
300

20000
200

Table 2: Within-cluster ratios (see (13)) on real datasets. We can see that our proposed clustering
method Spectral Sλ is very close to the clustering based on ˆΘ = Θ∗ ◦ Θ∗  which we cannot see
before solving (1).

Leukemia

Climate

Stock

Synthetic

λ = 0.5 λ = 0.3 λ = 0.005 λ = 0.001 λ = 0.0005 λ = 0.0001 λ = 0.005 λ = 0.001

random clustering 0.26
0.91
0.93

spectral on Sλ
spectral on ˆΘ

0.24
0.84
0.84

0.24
0.87
0.90

0.25
0.65
0.71

0.24
0.96
0.97

0.24
0.87
0.85

0.25
0.98
0.99

0.24
0.93
0.93

c=1) are indicative of better performance of the clustering algorithm.

Higher values of R({Vc}k
In section 3.1  we presented theoretical justiﬁcation for using normalized cut on the thresholded
matrix Sλ. Here we show that this strategy shows great promise on real datasets. Table 2 shows the
within-cluster ratios (13) of the inverse covariance matrix using different clustering methods. We
include the following methods in our comparison:

• Random partition: partition the nodes randomly into k clusters. We use this as a baseline.
• Spectral clustering on thresholded matrix Sλ: Our proposed method.
• Spectral clustering on ˆΘ = Θ∗ ◦ Θ∗  which is the element-wise square of Θ∗: This is the
best clustering method we can conduct  which directly minimizes within-cluster ratio of
the Θ∗ matrix. However  practically we cannot use this method as we do not know Θ∗.

We can observe in Table 2 that our proposed spectral clustering on Sλ achieves almost the same
performance as spectral clustering on Θ∗ ◦ Θ∗ even though we do not know Θ∗.
Also  Figure 1 gives a pictorial view of how our clustering results help in recovering the sparse
inverse covariance matrix at different levels. We run a hierarchical 2-way clustering on the Leukemia
dataset  and plot the original Θ∗ (solution of (1))  ¯Θ with 1-level clustering and ¯Θ with 2-level
clustering. We can see that although our clustering method does not look at Θ∗  the clustering result
matches the nonzero pattern of Θ∗ pretty well.

4.2 The performance of our divide and conquer algorithm

Next  we investigate the time taken by our divide and conquer algorithm on large real and synthetic
datasets. We include the following methods in our comparisons:

• DC-QUIC-1: Divide and Conquer framework with QUIC and with 1 level clustering.

(a) The inverse covariance ma-
trix Θ∗.
Figure 1: The clustering results and the nonzero patterns of inverse covariance matrix Θ∗ on
Leukemia dataset. Although our clustering method does not look at Θ∗  the clustering results
match the nonzero pattern in Θ∗ pretty well.

(c) The recovered ¯Θ from level 2
clusters.

(b) The recovered ¯Θ from level-1
clusters.

7

(a) Leukemia

(b) Stock

(c) Climate

(d) Synthetic

Figure 2: Comparison of algorithms on real datasets. The results show that DC-QUIC is much faster
than other state-of-the-art solvers.

estimation [6].

• DC-QUIC-3: Divide and Conquer QUIC with 3 levels of hierarchical clustering.
• QUIC: The original QUIC  which is a state-of-the-art second order solver for sparse inverse
• QUIC-conn: Using the decomposition method described in [8] and using QUIC to solve
• Glasso: The block coordinate descent algorithm proposed in [4].
• ALM: The alternating linearization algorithm proposed and implemented by [9].

each smaller sub-problem.

All of our experiments are run on an Intel Xeon E5440 2.83GHz CPU with 32GB main memory.
Figure 2 shows the results. For DC-QUIC and QUIC-conn  we show the run time of the whole
process  including the preprocessing time. We can see that in the largest synthetic dataset  DC-
QUIC is more than 10 times faster than QUIC  and thus also faster than Glasso and ALM. For the
largest real dataset: Climate with more than 10 000 points  QUIC takes more than 10 hours to get a
reasonable solution (relative error=0)  while DC-QUIC-3 converges in 1 hour. Moreover  on these
4 datasets QUIC-conn using the decomposition method of [8] provides limited savings  in part
because the connected components for the thresholded covariance matrix for each dataset turned
out to have a giant component  and multiple smaller components. DC-QUIC however was able to
leverage a reasonably good clustered decomposition  which dramatically reduced the inference time.

Acknowledgements

We would like to thank Soumyadeep Chatterjee and Puja Das for help with the climate and stock
data. C.-J.H.  I.S.D and P.R. acknowledge the support of NSF under grant IIS-1018426. P.R. also
acknowledges support from NSF IIS-1149803. A.B. acknowledges support from NSF grants IIS-
0916750  IIS-0953274  and IIS-1029711.

8

References
[1] O. Banerjee  L. E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum
likelihood estimation for multivariate Gaussian or binary data. The Journal of Machine Learn-
ing Research  9  6 2008.

[2] R. Bhatia. Matrix Analysis. Springer Verlag  New York  1997.
[3] I. S. Dhillon  Y. Guan  and B. Kulis. Weighted graph cuts without eigenvectors: A multi-
level approach. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 
29:11:1944–1957  2007.

[4] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graph-

ical lasso. Biostatistics  9(3):432–441  July 2008.

[5] T. R. Golub  D. K. Slonim  P. Tamayo  C. Huard  M. Gaasenbeek  J. P. Mesirov  H. Coller 
M. L. Loh  J. R. Downing  M. A. Caligiuri  and C. D. Bloomﬁeld. Molecular classication of
cancer: class discovery and class prediction by gene expression monitoring. Science  pages
531–537  1999.

[6] C.-J. Hsieh  M. Sustik  I. S. Dhillon  and P. Ravikumar. Sparse inverse covariance matrix

estimation using quadratic approximation. In NIPS  2011.

[7] L. Li and K.-C. Toh. An inexact interior point method for l1-reguarlized sparse covariance

selection. Mathematical Programming Computation  2:291–315  2010.

[8] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for

large-scale graphical lasso. Journal of Machine Learning Research  13:723–736  2012.

[9] K. Scheinberg  S. Ma  and D. Glodfarb. Sparse inverse covariance selection via alternating

linearization methods. NIPS  2010.

[10] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Analysis

and Machine Intelligence  22(8):888–905  2000.

9

,Nataliya Shapovalova
Michalis Raptis
Leonid Sigal
Greg Mori
Charles Kervrann
Satyen Kale
Chansoo Lee
David Pal