2015,Unified View of Matrix Completion under General Structural Constraints,Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper  we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\em any} norm regularization.We consider two estimators for the general problem of structured matrix completion  and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining  and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space  a certain \textit{\modified}~complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths  and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further  we provide several non-trivial examples of structures included in our framework  notably including the recently proposed spectral $k$-support norm.,Uniﬁed View of Matrix Completion under General

Structural Constraints

Suriya Gunasekar
UT at Austin  USA

suriya@utexas.edu

Arindam Banerjee

UMN Twin Cities  USA

banerjee@cs.umn.edu

Joydeep Ghosh
UT at Austin  USA

ghosh@ece.utexas.edu

Abstract

In this paper  we present a uniﬁed analysis of matrix completion under general
low-dimensional structural constraints induced by any norm regularization. We
consider two estimators for the general problem of structured matrix completion 
and provide uniﬁed upper bounds on the sample complexity and the estimation
error. Our analysis relies on results from generic chaining  and we establish
two intermediate results of independent interest: (a) in characterizing the size
or complexity of low dimensional subsets in high dimensional ambient space  a
certain partial complexity measure encountered in the analysis of matrix comple-
tion problems is characterized in terms of a well understood complexity measure
of Gaussian widths  and (b) it is shown that a form of restricted strong convexity
holds for matrix completion problems under general norm regularization. Further 
we provide several non-trivial examples of structures included in our framework 
notably the recently proposed spectral k-support norm.

1

Introduction

The task of completing the missing entries of a matrix from an incomplete subset of (potentially
noisy) entries is encountered in many applications including recommendation systems  data impu-
tation  covariance matrix estimation  and sensor localization among others. Traditionally ill–posed
high dimensional estimation problems  where the number of parameters to be estimated is much
higher than the number of observations  has been extensively studied in the recent literature. How-
ever  matrix completion problems are particularly ill–posed as the observations are both limited
(high dimensional)  and the measurements are extremely localized  i.e.  the observations consist of
individual matrix entries. The localized measurement model  in contrast to random Gaussian or
sub–Gaussian measurements  poses additional complications in high dimensional estimation.
For well–posed estimation in high dimensional problems including matrix completion  it is imper-
ative that low dimensional structural constraints are imposed on the target. For matrix completion 
the special case of low–rank constraint has been widely studied. Several existing work propose
tractable estimators with near–optimal recovery guarantees for (approximate) low–rank matrix com-
pletion [8  7  28  26  18  19  22  11  20  21]. A recent work [16] addresses the extension to structures
with decomposable norm regularization. However  the scope of matrix completion extends for low
dimensional structures far beyond simple low–rankness or decomposable norm structures.
In this paper  we present a uniﬁed statistical analysis of matrix completion under a general set of low
dimensional structures that are induced by any suitable norm regularization. We provide statistical
analysis of two generalized matrix completion estimators  the constrained norm minimizer  and the
generalized matrix Dantzig selector (Section 2.2). The main results in the paper (Theorem 1a–1b)
provide uniﬁed upper bounds on the sample complexity and estimation error of these estimators for
matrix completion under any norm regularization. Existing results on matrix completion with low
rank or other decomposable structures can be obtained as special cases of our general results.

1

Our uniﬁed analysis of sample complexity is motivated by recent work on high dimensional estima-
tion using global (sub) Gaussian measurements [10  1  35  3  37  5]. A key ingredient in the recovery
analysis of high dimensional estimation involves establishing a certain variation of Restricted Isom-
etry Property (RIP) [9] of the measurement operator. It has been shown that such properties are sat-
isﬁed by Gaussian and sub–Gaussian measurement operators with high probability. Unfortunately 
as has been noted before by Candes et al. [8]  owing to highly localized measurements  such con-
ditions are not satisﬁed in the matrix completion problem  and the existing results based on global
(sub) Gaussian measurements are not directly applicable. In fact  a key question we consider is:
given the radically limited measurement model in matrix completion  by how much would the sam-
ple complexity of estimation increase beyond the known sample complexity bounds for global (sub)
Gaussian measurements. Our results upper bounds the sample complexity for matrix completion to
within a log d factor over that for estimation under global (sub) Gaussian measurements [10  3  5].
While the result was previously known for low rank matrix completion using nuclear norm min-
imization [26  20]  with a careful use of generic chaining  we show that the log d factor sufﬁces
for structures induced by any norm! As a key intermediate result  we show that a useful form of
restricted strong convexity (RSC) [27] holds for the localized measurements encountered in matrix
completion under general norm regularized structures. The result substantially generalizes existing
RSC results for matrix completion under the special cases of nuclear norm and decomposable norm
regularization [26  16].
For our analysis  we use tools from generic chaining [33] to characterize the main results (Theo-
rem 1a–1b) in terms of the Gaussian width (Deﬁnition 1) of certain error sets. Gaussian widths
provide a powerful geometric characterization for quantifying the complexity of a structured low di-
mensional subset in a high dimensional ambient space. Numerous tools have been developed in the
literature for bounding the Gaussian width of structured sets. A uniﬁed characterization of results in
terms of Gaussian width has the advantage that this literature can be readily leveraged to derive new
recovery guarantees for matrix completion under suitable structural constraints (Appendix D.2).
In addition to the theoretical elegance of such a uniﬁed framework  identifying useful but potentially
non–decomposable low dimensional structures is of signiﬁcant practical interest. The broad class
of structures enforced through symmetric convex bodies and symmetric atomic sets [10] can be
analyzed under this paradigm (Section 2.1). Such specialized structures can capture the constraints
in certain applications better than simple low–rankness. In particular  we discuss in detail  a non–
trivial example of the spectral k–support norm introduced by McDonald et al. [25].
To summarize the key contributions of the paper:
• Theorem 1a–1b provide uniﬁed upper bounds on sample complexity and estimation error for
matrix completion estimators using general norm regularization: a substantial generalization of the
existing results on matrix completion under structural constraints.
• Theorem 1a is applied to derive statistical results for the special case of matrix completion under
spectral k–support norm regularization.
• An intermediate result  Theorem 5 shows that under any norm regularization  a variant of Re-
stricted Strong Convexity (RSC) holds in the matrix completion setting with extremely localized
measurements. Further  a certain partial measure of complexity of a set is encountered in matrix
completion analysis (12). Another intermediate result  Theorem 2 provides bounds on the par-
tial complexity measures in terms of a better understood complexity measure of Gaussian width.
These intermediate results are of independent interest beyond the scope of the paper.
Notations and Preliminaries
Indexes i  j are typically used to index rows and columns respectively of matrices  and index k is
used to index the observations. ei  ej  ek  etc. denote the standard basis in appropriate dimensions∗.
Notation G and g are used to denote a matrix and vector respectively  with independent standard
Gaussian random variables. P(.) and E(.) denote the probability of an event and the expectation of
a random variable  respectively. Given an integer N  let [N ] = {1  2  . . .   N}. Euclidean norm in a

vector space is denoted as (cid:107)x(cid:107)2 =(cid:112)(cid:104)x  x(cid:105). For a matrix X with singular values σ1 ≥ σ2 ≥ . . . 
common norms include the Frobenius norm (cid:107)X(cid:107)F =(cid:112)(cid:80)
i   the nuclear norm (cid:107)X(cid:107)∗ =(cid:80)

i σi 
the spectral norm (cid:107)X(cid:107)op = σ1  and the maximum norm (cid:107)X(cid:107)∞ = maxij |Xij|. Also let  Sd1d2−1 =

i σ2

∗for brevity we omit the explicit dependence of dimension unless necessary

2

{X ∈ Rd1×d2 : (cid:107)X(cid:107)F = 1} and Bd1d2 = {X ∈ Rd1×d2 : (cid:107)X(cid:107)F ≤ 1}. Finally  given a norm (cid:107).(cid:107)
deﬁned on a vectorspace V  its dual norm is given by (cid:107)X(cid:107)∗ = sup(cid:107)Y (cid:107)≤1(cid:104)X  Y (cid:105).
Deﬁnition 1 (Gaussian Width). Gaussian width of a set S ⊂ Rd1×d2 is a widely studied measure of
complexity of a subset in high dimensional ambient space and is given by:

wG(S) = EG sup
X∈S

(cid:104)X  G(cid:105) 

(1)

√

where recall that G is a matrix of independent standard Gaussian random variables. Some key results
on Gaussian width are discussed in Appendix D.2.
Deﬁnition 2 (Sub–Gaussian Random Variable [36]). The sub–Gaussian norm of a random variable
X is given by: (cid:107)X(cid:107)Ψ2 = supp≥1 p−1/2(E|X|p)1/p. X is b–sub–Gaussian if (cid:107)X(cid:107)Ψ2 ≤ b < ∞.
Equivalently  X is sub–Gaussian if one of the following conditions are satisﬁed for some constants
k1  k2  and k3 [Lemma 5.5 of [36]].
(1) ∀p ≥ 1  (E|X|p)1/p ≤ b
(3) E[ek2X 2/b2
Deﬁnition 3 (Restricted Strong Convexity (RSC)). A function L is said to satisfy Restricted Strong
Convexity (RSC) at Θ with respect to a subset S  if for some RSC parameter κL > 0 
Deﬁnition 4 (Spikiness Ratio [26]). For X ∈Rd1×d2  a measure of its “spikiness” is given by:

(2) ∀t > 0  P(|X| > t) ≤ e1−t2/k2
(4) if EX = 0  then ∀s > 0  E[esX ] ≤ ek3s2b2/2.

∀∆ ∈ S L(Θ + ∆) − L(Θ) − (cid:104)∇L(Θ)  ∆(cid:105) ≥ κL(cid:107)∆(cid:107)2
F .

d1d2(cid:107)X(cid:107)∞
(3)
(cid:107)X(cid:107)F
Deﬁnition 5 (Norm Compatibility Constant [27]). The compatibility constant of a norm R : V → R
under a closed convex cone C ⊂ V is deﬁned as follows:

] ≤ e  or

αsp(X) =

1b2 

√

(2)

p 

.

ΨR(C) = sup

X∈C\{0}

R(X)
(cid:107)X(cid:107)F

.

(4)

2 Structured Matrix Completion
Denote the ground truth target matrix as Θ∗ ∈ Rd1×d2; let d = d1 + d2. In the noisy matrix comple-
tion  observations consists of individual entries of Θ∗ observed through an additive noise channel.
Sub–Gaussian Noise: Given  a list of independently sampled standard basis Ω = {Ek = eik e(cid:62)
ik ∈ [d1]  jk ∈ [d2]} with potential duplicates  observations (yk)k ∈ R|Ω| are given by:
(5)
where η ∈ R|Ω| is the noise vector of independent sub–Gaussian random variables with E[ηk] = 0
and Var(ηk) = 1  and ξ2 is scaled variance of noise per observation. Further let (cid:107)ηk(cid:107)Ψ2 ≤ b for a
constant b (recall (cid:107).(cid:107)Ψ2 from Deﬁnition 2). Also  without loss of generality  assume normalization
(cid:107)Θ∗(cid:107)F = 1.
Uniform Sampling: Assume that the entries in Ω are drawn independently and uniformly:

yk = (cid:104)Θ∗  Ek(cid:105) + ξηk  for k = 1  2  . . .  |Ω| 

jk

:

Ek ∼ uniform{eie(cid:62)

j : i ∈ [d1]  j ∈ [d2]}  for Ek ∈ Ω.

Let {ek} be the standard basis of R|Ω|. Given Ω  deﬁne PΩ : Rd1×d2 → R|Ω| as:

(6)

(7)
Structural Constraints For matrix completion with |Ω| < d1d2  low dimensional structural con-
straints on Θ∗ are necessary for well–posedness. We consider a generalized constraint setting
wherein for some low–dimensional model space M  Θ∗ ∈ M is enforced through a surrogate
norm regularizer R(.). We make no further assumptions on R other than it being a norm in Rd1×d2.
Low Spikiness In matrix completion under uniform sampling model  further restrictions on Θ∗ (be-
yond low dimensional structure) are required to ensure that the most informative entries of the matrix
are observed with high probability [8]. Early work assumed stringent matrix incoherence conditions
for low–rank completion to preclude such matrices [7  18  19]  while more recent work [11  26]  re-
lax these assumptions to a more intuitive restriction of the spikiness ratio  deﬁned in (3). However 
under this relaxation only an approximate recovery is typically guaranteed in low–noise regime  as
opposed to near exact recovery under incoherence assumptions [26  11].
Assumption 1 (Spikiness Ratio). There exists α∗ > 0  such that
≤ α∗√

(cid:107)Θ∗(cid:107)∞ = αsp(Θ∗)

(cid:107)Θ∗(cid:107)F√

(cid:3)

.

d1d2

d1d2

PΩ(X) =(cid:80)|Ω|

k=1(cid:104)X  Ek(cid:105)ek

3

2.1 Special Cases and Applications

We brieﬂy introduce some interesting examples of structural constraints with practical applications.
Example 1 (Low Rank and Decomposable Norms). Low–rankness is the most common structure
used in many matrix estimation problems including collaborative ﬁltering  PCA  spectral clustering 
etc. Convex estimators using nuclear norm (cid:107)Θ(cid:107)∗ regularization has been widely studied statistically
[8  7  28  26  18  19  22  11  20  21]. A recent work [16] extends the analysis of low rank matrix
completion to general decomposable norms  i.e. R :∀X  Y ∈ (M M⊥) R(X+Y ) =R(X)+R(Y ).
Example 2 (Spectral k–support Norm). A non–trivial and signiﬁcant example of norm regular-
ization that is not decomposable is the spectral k–support norm recently introduced by McDon-
ald et al. [25]. Spectral k–support norm is essentially the vector k–support norm [2] applied on the
singular values σ(Θ) of a matrix Θ ∈ Rd1×d2. Without loss of generality  let ¯d = d1 = d2.
Let Gk = {g ⊆ [ ¯d] : |g| ≤ k} be the set of all subsets [ ¯d] of cardinality at most k  and let
V(Gk) = {(vg)g∈Gk : vg ∈ R ¯d  supp(vg) ⊆ g}. The spectral k–support norm is given by:

(cid:107)Θ(cid:107)k–sp = inf

v∈V(Gk)

(cid:107)vg(cid:107)2 :

vg = σ(Θ)

 

(8)

(cid:110) (cid:88)

g∈Gk

(cid:88)

g∈Gk

(cid:111)

McDonald et al. [25] showed that spectral k–support norm is a special case of cluster norm [17]. It
was further shown that in multi–task learning  wherein the tasks (columns of Θ∗) are assumed to be
clustered into dense groups  the cluster norm provides a trade–off between intra–cluster variance 
(inverse) inter–cluster variance  and the norm of the task vectors. Both [17] and [25] demonstrate
superior empirical performance of cluster norms (and k–support norm) over traditional trace norm
and spectral elastic net minimization on bench marked matrix completion and multi–task learning
datasets. However  statistical analysis of consistent matrix completion using spectral k–support
norm regularization has not been previously studied. In Section 3.2  we discuss the consequence of
our main theorem for this non–trivial special case.
Example 3 (Additive Decomposition). Elementwise sparsity is a common structure often assumed
in high–dimensional estimation problems. However  in matrix completion  elementwise sparsity
conﬂicts with Assumption 1 (and more traditional incoherence assumptions). Indeed  it is easy to
see that with high probability most of the |Ω| (cid:28) d1d2 uniformly sampled observations will be
zero  and an informed prediction is infeasible. However  elementwise sparse structures can often
k Θ(k)  such that each
component matrix Θ(k) is in turn structured (e.g.
low rank+sparse used for robust PCA [6]). In
such structures  there is no scope for recovering sparse components outside the observed indices 
and it is assumed that: Θ(k) is sparse ⇒ supp(Θ(k)) ⊆ Ω. In such cases  our results are applicable
under additional regularity assumptions that enforces non–spikiness on the superposed matrix. A
candidate norm regularizer for such structures is the weighted inﬁmum convolution of individual
structure inducing norms [6  39] 

be modelled within an additive decomposition framework  wherein Θ∗ =(cid:80)

Rw(Θ) = inf(cid:8)(cid:88)

wkRk(Θ(k)) :

(cid:88)

Θ(k) = Θ(cid:9).

Example 4 (Other Applications). Other potential applications including cut matrices [30  10]  struc-
tures induced by compact convex sets  norms inducing structured sparsity assumptions on the spec-
trum of Θ∗  etc. can also be handled under the paradigm of this paper.

k

k

2.2 Structured Matrix Estimator
Let R be the norm surrogate for the structural constraints on Θ∗  and R∗ denote its dual norm. We
propose and analyze two convex estimators for the task of structured matrix completion:
Constrained Norm Minimizer

R(Θ)

s.t. (cid:107)PΩ(Θ) − y(cid:107)2 ≤ λcn.

(cid:98)Θcn = argmin

(cid:107)Θ(cid:107)∞≤ α∗√

d1d2

Generalized Matrix Dantzig Selector
R(Θ)

(cid:98)Θds = argmin

(cid:107)Θ(cid:107)∞≤ α∗√

d1d2

√
|Ω| R∗P ∗

d1d2

Ω(PΩ(Θ) − y) ≤ λds 

s.t.

4

(9)

(10)

Ω : RΩ → Rd1×d2 is the linear adjoint of PΩ  i.e. (cid:104)PΩ(X)  y(cid:105) = (cid:104)X  P ∗

where recall that P ∗
Note: Theorem 1a–1b gives consistency results for (9) and (10)  respectively  under certain con-
ditions on the parameters λcn > 0  λds > 0  and α∗ > 1. In particular  these conditions assume
knowledge of the noise variance ξ2 and spikiness ratio αsp(Θ∗). In practice  typically ξ and αsp(Θ∗)
are unknown and the parameters are tuned by validating on held out data.

Ω(y)(cid:105).

3 Main Results

We deﬁne the following “restricted” error cone and its subset:

(11)

TR = TR(Θ∗) = cone{∆ : R(Θ∗ + ∆) ≤ R(Θ∗)}  and ER = TR ∩ Sd1d2−1.

Let(cid:98)Θcn and(cid:98)Θds be the estimates from (9) and (10)  respectively. If λcn and λds are chosen such that
Θ∗ belongs to the feasible sets in (9) and (10)  respectively  then the error matrices (cid:98)∆cn = (cid:98)Θcn − Θ∗
and (cid:98)∆ds = (cid:98)Θds − Θ∗ are contained in TR.
Theorem 1a (Constrained Norm Minimizer). Under the problem setup in Section 2  let(cid:98)Θcn = Θ∗ +
(cid:98)∆cn be the estimate from (9) with λcn = 2ξ(cid:112)|Ω|. For large enough c0  if |Ω| > c2
(cid:17)
G(ER) log d 
then there exists an RSC parameter κc0 > 0 with κc0 ≈ 1 − o
  and constants c1 and c2
(cid:115)
(cid:41)
G(ER) log d) 
G(ER))−2 exp(−c2w2
such that  with probability greater than 1−exp(−c1w2
G(ER) log d
α∗2
d1d2

(cid:107)(cid:98)∆cn(cid:107)2

(cid:16) 1√

F ≤ 4 max

(cid:40)

c2
0w2

|Ω|

0w2

d1d2

log d

ξ2
κc0

1

.

 

Theorem 1b (Matrix Dantzig Selector). Under the problem setup in Section 2 

Θ∗ + (cid:98)∆ds be the estimate from (10) with λds ≥ 2ξ

G(ER) log d  then there exists an RSC parameter κc0 > 0 with κc0 ≈ 1 − o

|Ω| > c2
and a constant c1 such that  with probability greater than 1−exp(−c1w2

0w2

Ω(η). For large enough c0  if
 

log d

let (cid:98)Θds =
(cid:16) 1√
(cid:17)

d1d2

√
|Ω| R∗P ∗
(cid:115)

c2
0w2

α∗2
d1d2

 

(cid:41)
G(ER)) 
G(ER) log d

.

|Ω|

(cid:40)

dsΨ2R(TR)
λ2

κ2
c0

1

d1d2

F ≤ 16 max

(cid:107)(cid:98)∆ds(cid:107)2
(cid:113) d log d

d1d2

then w2

Recall Gaussian width wG and subspace compatibility constant ΨR from (1) and (4)  respectively.
Remarks:
G(ER) ≤ 3dr  ΨR(TR) ≤ 2r and
1. If R(Θ) = (cid:107)Θ(cid:107)∗ and rank(Θ∗) = r 
|Ω| w.h.p [10  14  26]. Using these bounds in Theorem 1b recovers

√
|Ω| (cid:107)P ∗
near–optimal results for low rank matrix completion under spikiness assumption [26].

Ω(η)(cid:107)2 ≤ 2

2. For both estimators  upper bound on sample complexity is dominated by the square of Gaussian
width which is often considered the effective dimension of a subset in high dimensional space
and plays a key role in high dimensional estimation under Gaussian measurement ensembles.
The results show that  independent of R(.)  the upper bound on sample complexity for consistent
matrix completion with highly localized measurements is within a log d factor of the known
sample complexity of ∼ w2

G(ER) for estimation from Gaussian measurements [3  10  37  5].

3. First term in estimation error bounds in Theorem 1a–1b scales with ξ2 which is the per observa-
tion noise variance (upto constant). The second term is an upper bound on error that arises due
to unidentiﬁability of Θ∗ within a certain radius under the spikiness constraints [26]; in contrast
[7] show exact recovery when ξ = 0 using more stringent matrix incoherence conditions.

4. Bound on (cid:98)∆cn from Theorem 1a is comparable to the result by Cand´es et al. [7] for low rank
matrix completion under non–low–noise regime  where the ﬁrst term dominates  and those of [10 
G(ER)  it
35] for high dimensional estimation under Gaussian measurements. With a bound on w2
is easy to specialize this result for new structural constraints. However  this bound is potentially
loose and asymptotically converges to a constant error proportional to the noise variance ξ2.
for speciﬁc structures  using application of Theorem 1b requires additional bounds on R∗P ∗
and ΨR(TR) besides w2

5. The estimation error bound in Theorem 1b is typically sharper than that in Theorem 1a. However 
Ω(η)

G(ER).

5

(cid:115) |Ω|
(cid:115) |Ω|

d1d2

(cid:114)EΩ sup

X Y ∈S

3.1 Partial Complexity Measures
Recall that for wG(S) = E supX∈S(cid:104)X  G(cid:105) and R|Ω| (cid:51) g ∼ N (0  I|Ω|) is a standard normal vector.
Deﬁnition 6 (Partial Complexity Measures). Given a randomly sampled Ω = {Ek ∈ Rd1×d2}  and
a centered random vector η ∈ R|Ω|  the partial η–complexity measure of S is given by:

wΩ η(S) = EΩ η

sup

(12)
X∈S−S
Special cases of η being a vector of standard Gaussian g  or standard Rademacher  (i.e. k ∈
{−1  1} w.p. 1/2) variables  are of particular interest.
Note: In the case of symmetric η  like g and   wΩ η(S) = 2EΩ η supX∈S(cid:104)X  P ∗
Ω(η)(cid:105)  and the later
(cid:3)
expression will be used interchangeably ignoring the constant term.
Theorem 2 (Partial Gaussian Complexity). Let S ⊆ Bd1d2 with non–empty interior  and let Ω be
sampled according to (6). ∃ universal constants k1  k2  K1 and K2 such that:

(cid:104)X  P ∗

Ω(η)(cid:105).

wΩ g(S) ≤ k1

wG(S) + k2

(cid:107)PΩ(X − Y )(cid:107)2

2

(13)

wΩ g(S) ≤ K1

wG(S) + K2 sup
X Y ∈S

(cid:107)X − Y (cid:107)∞.

d1d2

Also  for centered i.i.d. sub–Gaussian vector η ∈ R|Ω|  ∃ constant K3 s.t. wΩ η(S) ≤ K3wΩ g(S).
Note: For Ω (cid:40) [d1]× [d2]  the second term in (13) is a consequence of the localized measurements.

3.2 Spectral k–Support Norm

We introduced spectral k–support norm in Section 2.1. The estimators from (9) and (10) for spectral
k–support norm can be efﬁciently solved via proximal methods using the proximal operators derived
in [25]. We are interested in the statistical guarantees for matrix completion using spectral k–support
norm regularization. We extend the analysis for upper bounding the Gaussian width of the descent
cone for the vector k–support norm by [29] to the case of spectral k–support norm. WLOG let
d1 = d2 = ¯d. Let σ∗ ∈ R ¯d be the vector of singular values of Θ∗ sorted in non–ascending order.
Let r ∈ {0  1  2  . . .   k − 1} be the unique integer satisfying: σ∗
k−r.
Denote I2 = {1  2  . . .   k − r − 1} and I1 = {k − r  k − r + 1  . . .   s}. Finally  for I ⊆ [ ¯d] 
I )i = 0 ∀i ∈ I c  and (σ∗
(σ∗
Lemma 3. If rank of Θ∗ is s and ER is the error set for R(Θ) = (cid:107)Θ(cid:107)k–sp  then
(2 ¯d − s).

(cid:16) (r + 1)2(cid:107)σ∗

(cid:80)p
i=k−r σ∗

G(ER) ≤ s(2 ¯d − s) +
w2

+ |I1|(cid:17)

k−r−1 > 1
r+1

I )i = σ∗

i ∀i ∈ I.

i ≥ σ∗

(cid:107)2

2

I2

(cid:107)σ∗

I1

(cid:107)2

1

Proof of the above lemma is provided in the appendix. Lemma 3 can be combined with Theorem 1a
to obtain recovery guarantees for matrix completion under spectral k–support norm.

4 Discussions and Related Work

Sample Complexity: For consistent recovery in high dimensional convex estimation  it is desirable
that the descent cone at the target parameter Θ∗ is “small” relative to the feasible set (enforced by the
observations) of the estimator. Thus  it is not surprising that the sample complexity and estimation
error bounds of an estimator depends on some measure of complexity/size of the error cone at
Θ∗. Results in this paper are largely characterized in terms of a widely used complexity measure
of Gaussian width wG(.)  and can be compared with the literature on estimation from Gaussian
measurements.
Error Bounds: Theorem 1a provides estimation error bounds that depends only on the Gaussian
width of the descent cone. In non–low–noise regime  this result is comparable to analogous results
of constrained norm minimization [6  10  35]. However  this bound is potentially loose owing to
mismatched data–ﬁt term using squared loss  and asymptotically converges to a constant error pro-
portional to the noise variance ξ2.

6

A tighter analysis on the estimation error can be obtained for the matrix Dantzig selector (10) from
Theorem 1b. However  application of Theorem 1b requires computing high probability upper bound
on R∗P ∗
Ω(η). The literature on norms of random matrices [13  24  36  34] can be exploited in com-
puting such bounds. Beside  in special cases: if R(.) ≥ K(cid:107).(cid:107)∗  then KR∗(.) ≤ (cid:107).(cid:107)op can be used
to obtain asymptotically consistent results.
Finally  under near zero–noise  the second term in the results of Theorem 1 dominates  and bounds
are weaker than that of [6  19] owing to the relaxation of stronger incoherence assumption.
Related Work and Future Directions: The closest related work is the result on consistency of
matrix completion under decomposable norm regularization by [16]. Results in this paper are a strict
generalization to general norm regularized (not necessarily decomposable) matrix completion. We
provide non–trivial examples of application where structures enforced by such non–decomposable
norms are of interest. Further  in contrast to our results that are based on Gaussian width  the RSC
parameter in [16] depends on a modiﬁed complexity measure κR(d |Ω|) (see deﬁnition in [16]). An
advantage of results based on Gaussian width is that  application of Theorem 1 for special cases can
greatly beneﬁt from the numerous tools in the literature for the computation of wG(.).
Another closely related line of work is the non–asymptotic analysis of high dimensional estimation
under random Gaussian or sub–Gaussian measurements [10  1  35  3  37  5]. However  the analysis
from this literature rely on variants of RIP of the measurement ensemble [9]  which is not satisﬁed by
the the extremely localized measurements encountered in matrix completion[8]. In an intermediate
result  we establish a form of RSC for matrix completion under general norm regularization: a result
that was previously known only for nuclear norm and decomposable norm regularization.
In future work  it is of interest to derive matching lower bounds on estimation error for matrix
completion under general low dimensional structures  along the lines of [22  5] and explore special
case applications of the results in the paper. We also plan to derive explicit characterization of λds
in terms of Gaussian width of unit balls by exploiting generic chaining results for general Banach
spaces [33].

5 Proof Sketch

Proofs of the lemmas are provided in the Appendix.
5.1 Proof of Theorem 1
Deﬁne the following set of β–non–spiky matrices in Rd1×d2 for constant c0 from Theorem 1:

(cid:41)

< β

.

(14)

(cid:40)
(cid:115)

A(β) =

X : αsp(X) =

|Ω|

β2
c0

=

G(ER) log d

c2
0w2

√

d1d2(cid:107)X(cid:107)∞
(cid:107)X(cid:107)F

Deﬁne 

(15)

(cid:3)

|Ω|

√

= 4α∗2

d1d2 in (3).

G(ER) log d

0w2

F ≤ 4α∗2

β2
c0

(cid:113) c2

Case 1: Spiky Error Matrix When the error matrix from (9) or (10) has large spikiness ratio 

following bound on error is immediate using (cid:107)(cid:98)∆(cid:107)∞≤(cid:107)(cid:98)Θ(cid:107)∞ +(cid:107)Θ∗(cid:107)∞≤2α∗/
Proposition 4 (Spiky Error Matrix). For the constant c0 in Theorem 1a  if αsp((cid:98)∆cn) /∈ A(βc0)  then
(cid:107)(cid:98)∆cn(cid:107)2
. An analogous result also holds for (cid:98)∆ds.
Case 2: Non–Spiky Error Matrix Let (cid:98)∆ds (cid:98)∆cn ∈ A(β). Recall from (5)  that y − PΩ(Θ∗) = ξη 
where η ∈ R|Ω| consists of independent sub–Gaussian random variables with E[ηk] = 0  Var(ηk) =
1  and (cid:107)ηk(cid:107)Ψ2 ≤ b for a constant b.
5.1.1 Restricted Strong Convexity (RSC)
Recall TR and ER from (11). The most signiﬁcant step in the proof of Theorem 1 involves showing
that over a useful subset of TR  a form of RSC (2) is satisﬁed by a squared loss penalty.
Theorem 5 (Restricted Strong Convexity). Let |Ω| > c2
G(ER) log d  for large enough constant
0w2
c0. There exists a RSC parameter κc0 > 0 with κc0 ≈ 1 − o
  and a constant c1 such that 
the following holds w.p. greater that 1 − exp(−c1w2
G(ER)) 

(cid:16) 1√

(cid:17)

log d

7

∀X ∈ TR ∩ A(βc0 ) 

d1d2

|Ω| (cid:107)PΩ(X)(cid:107)2

2 ≥ κc0(cid:107)X(cid:107)2
F .

Proof in Appendix A combines empirical process tools along with Theorem 2.

(cid:3)

5.1.2 Constrained Norm Minimizer
exists a universal constant c2 such that  if λcn≥ 2ξ(cid:112)|Ω|  then w.p. greater than 1− 2 exp (−c2|Ω|) 
Lemma 6. Under the conditions of Theorem 1  let b be a constant such that ∀k  (cid:107)ηk(cid:107)Ψ2 ≤ b. There
(a) (cid:98)∆ds ∈ TR  and (b) (cid:107)PΩ((cid:98)∆cn)(cid:107)2≤ 2λcn.
Using λcn = 2ξ(cid:112)|Ω| in (9)  if (cid:98)∆cn∈A(βc0)  then using Theorem 5 and Lemma 6  w.h.p.
(cid:3)
(cid:107)(cid:98)∆cn(cid:107)2
Ω(η) ⇒ w.h.p. (a) (cid:98)∆ds∈TR; (b)

5.1.3 Matrix Dantzig Selector
Proposition 7. λds≥ ξ

Ω(PΩ((cid:98)∆ds))≤ 2λds.

(cid:107)PΩ((cid:98)∆cn)(cid:107)2

Above result follows from optimality of(cid:98)Θds and triangle inequality. Also 

√
|Ω| R∗P ∗

√
|Ω| R∗P ∗

≤ 4ξ2
κc0

≤ 1
κc0

|Ω|

d1d2

(16)

d1d2

d1d2

F

2

.

Ω(PΩ((cid:98)∆ds))R((cid:98)∆ds) ≤ 2λdsΨR(TR)(cid:107)(cid:98)∆ds(cid:107)F  

where recall norm compatibility constant ΨR(TR) from (4). Finally  using Theorem 5  w.h.p.

√

d1d2

|Ω| (cid:107)PΩ((cid:98)∆ds)(cid:107)2
2 ≤
(cid:107)(cid:98)∆ds(cid:107)2

F

d1d2

d1d2

√
|Ω| R∗P ∗
(cid:107)PΩ((cid:98)∆ds)(cid:107)2

2

≤ 1
|Ω|

κc0

≤ 4λdsΨR(TR)

κc0

(cid:107)(cid:98)∆ds(cid:107)F√

d1d2

.

(17)

jk

(XΩ g(X))X∈S  where XΩ g(X) = (cid:104)X  P ∗

5.2 Proof of Theorem 2
Let the entries of Ω = {Ek = eik e(cid:62)
: k = 1  2  . . .  |Ω|} be sampled as in (6). Recall that g ∈ R|Ω|
is a standard normal vector. For a compact S ⊆ Rd1×d2  it sufﬁces to prove Theorem 2 for a dense
countable subset of S. Overloading S to such a countable subset  deﬁne following random process:
(18)
We start with a key lemma in the proof of Theorem 2. Proof of this lemma  provided in Appendix B 
uses tools from the broad topic of generic chaining developed in recent works [31  33].
Lemma 8. For a compact subset S ⊆ Rd1×d2 with non–empty interior  ∃ constants k1  k2 such that:
(cid:3)
wΩ g(S) = E sup
X∈S
Lemma 9. There exists constants k3  k4  such that for compact S ⊆ Bd1d2 with non–empty interior

Ω(g)(cid:105) =(cid:80)
(cid:114)E sup

(cid:107)PΩ(X − Y )(cid:107)2
2.

(cid:115) |Ω|

XΩ g(X) ≤ k1

k(cid:104)X  Ek(cid:105)gk.

wG(S) + k2

X Y ∈S

d1d2

E sup
X Y ∈S

(cid:107)PΩ(X − Y )(cid:107)2

2 ≤ k3

w2

G(S) + k4( sup
X Y ∈S

(cid:107)X − Y (cid:107)∞)wΩ g(S)

|Ω|
d1d2

√
Theorem 2 follows by combining Lemma 8 and Lemma 9  and simple algebraic manipulations using
ab ≤ a/2 + b/2 and triangle inequality (See Appendix B.4).
The statement in Theorem 2 about partial sub–Gaussian complexity follows from a standard result
(cid:3)
in empirical process given in Lemma 11 in the appendix.
Acknowledgments We thank the anonymous reviewers for helpful comments and suggestions. S.
Gunasekar and J. Ghosh acknowledge funding from NSF grants IIS-1421729  IIS-1417697  and
IIS1116656. A. Banerjee acknowledges NSF grants IIS-1447566  IIS-1422557  CCF-1451986 
CNS-1314560  IIS-0953274  IIS-1029711  and NASA grant NNX12AQ39A.

8

References
[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: A geometric theory of phase

transitions in convex optimization. Inform. Inference  2014.

[2] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In NIPS  2012.
[3] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In NIPS  2014.
[4] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh. Clustering with bregman divergences. JMLR  2005.
[5] T. Cai  T. Liang  and A. Rakhlin. Geometrizing local rates of convergence for linear inverse problems.

arXiv preprint  2014.

[6] E. J. Cand´es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? ACM  2011.
[7] E. J. Cand´es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  2010.
[8] E. J. Cand´es and B. Recht. Exact matrix completion via convex optimization. FoCM  2009.
[9] Emmanuel J Candes and Terence Tao. Decoding by linear programming.

Information Theory  IEEE

Transactions on  2005.

[10] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear inverse

problems. Foundations of Computational Mathematics  2012.

[11] M. A. Davenport  Y. Plan  E. Berg  and M. Wootters. 1-bit matrix completion. Inform. Inference  2014.
[12] R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal

[13] A. Edelman. Eigenvalues and condition numbers of random matrices. Journal on Matrix Analysis and

of Functional Analysis  1967.

Applications  1988.

[14] M. Fazel  H Hindi  and S. P. Boyd. A rank minimization heuristic with application to minimum order

system approximation. In American Control Conference  2001.

[15] J. Forster and M. Warmuth. Relative expected instantaneous loss bounds. Journal of Computer and

System Sciences  2002.

straints. In ICML  2014.

[16] S. Gunasekar  P. Ravikumar  and J. Ghosh. Exponential family matrix completion under structural con-

[17] L. Jacob  J. P. Vert  and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS  2009.
[18] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. IEEE Trans. IT  2010.
[19] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. JMLR  2010.
[20] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli  2014.
[21] O. Klopp. Matrix completion by singular value thresholding: sharp bounds. arXiv preprint arXiv  2015.
[22] Vladimir Koltchinskii  Karim Lounici  Alexandre B Tsybakov  et al. Nuclear-norm penalization and

optimal rates for noisy low-rank matrix completion. The Annals of Statistics  2011.

[23] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer  1991.
[24] A. E. Litvak  A. Pajor  M. Rudelson  and N. Tomczak-Jaegermann. Smallest singular value of random

matrices and geometry of random polytopes. Advances in Mathematics  2005.

[25] A. M. McDonald  M. Pontil  and D. Stamos. New perspectives on k-support and cluster norms. arXiv

preprint  2014.

bounds with noise. JMLR  2012.

[26] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal

[27] S. Negahban  B. Yu  M. J. Wainwright  and P. Ravikumar. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. In NIPS  2009.

[28] B. Recht. A simpler approach to matrix completion. JMLR  2011.
[29] E. Richard  G. Obozinski  and J.-P. Vert. Tight convex relaxations for sparse matrix factorization.

In

ArXiv e-prints  2014.

[30] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. In Learning Theory. Springer  2005.
[31] M. Talagrand. Majorizing measures: the generic chaining. The Annals of Probability  1996.
[32] M. Talagrand. Majorizing measures without measures. Annals of probability  2001.
[33] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer  2014.
[34] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

[35] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. arXiv

Mathematics  2012.

preprint  2014.
[36] R. Vershynin.

pages 210–268  2012.

Introduction to the non-asymptotic analysis of random matrices. Compressed sensing 

[37] R. Vershynin. Estimation in high dimensions: a geometric perspective. ArXiv e-prints  2014.
[38] A. G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its

Applications  1992.

[39] E. Yang and P. Ravikumar. Dirty statistical models. In NIPS  2013.

9

,Suriya Gunasekar
Arindam Banerjee
Joydeep Ghosh