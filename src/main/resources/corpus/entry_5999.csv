2014,large scale canonical correlation analysis with iterative least squares,Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However  computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA  an iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.,Large Scale Canonical Correlation Analysis with

Iterative Least Squares

Yichao Lu

University of Pennsylvania

yichaolu@wharton.upenn.edu

Dean P. Foster
Yahoo Labs  NYC

dean@foster.net

Abstract

Canonical Correlation Analysis (CCA) is a widely used statistical tool with both
well established theory and favorable performance for a wide range of machine
learning problems. However  computing CCA for huge datasets can be very slow
since it involves implementing QR decomposition or singular value decomposi-
tion of huge matrices. In this paper we introduce L-CCA   a iterative algorithm
which can compute CCA fast on huge sparse datasets. Theory on both the asymp-
totic convergence and ﬁnite time accuracy of L-CCA are established. The experi-
ments also show that L-CCA outperform other fast CCA approximation schemes
on two real datasets.

1

Introduction

Canonical Correlation Analysis (CCA) is a widely used spectrum method for ﬁnding correlation
structures in multi-view datasets introduced by [15]. Recently  [3  9  17] proved that CCA is able
to ﬁnd the right latent structure under certain hidden state model. For modern machine learning
problems  CCA has already been successfully used as a dimensionality reduction technique for
the multi-view setting. For example  A CCA between the text description and image of the same
object will ﬁnd common structures between the two different views  which generates a natural
vector representation of the object. In [9]  CCA is performed on a large unlabeled dataset in order
to generate low dimensional features to a regression problem where the size of labeled dataset is
small. In [6  7] a CCA between words and its context is implemented on several large corpora to
generate low dimensional vector representations of words which captures useful semantic features.

When the data matrices are small  the classical algorithm for computing CCA involves ﬁrst a
QR decomposition of the data matrices which pre whitens the data and then a Singular Value
Decomposition (SVD) of the whitened covariance matrix as introduced in [11]. This is exactly how
Matlab computes CCA. But for huge datasets this procedure becomes extremely slow. For data
matrices with huge sample size [2] proposed a fast CCA approach based on a fast inner product
preserving random projection called Subsampled Randomized Hadamard Transform but it’s still
slow for datasets with a huge number of features. In this paper we introduce a fast algorithm for
ﬁnding the top kcca canonical variables from huge sparse data matrices (a single multiplication with
these sparse matrices is very fast) X 2 n ⇥ p1 and Y 2 n ⇥ p2 the rows of which are i.i.d samples
from a pair of random vectors. Here n  p1  p2  1 and kcca is relatively small number like 50
since the primary goal of CCA is to generate low dimensional features. Under this set up  QR
decomposition of a n ⇥ p matrix cost O(np2) which is extremely slow even if the matrix is sparse.
On the other hand since the data matrices are sparse  X>X and Y>Y can be computed very fast.
So another whitening strategy is to compute (X>X) 1
2 . But when p1  p2 are large this
takes O(max{p3

2   (Y>Y) 1
2}) which is both slow and numerically unstable.

1  p3

1

The main contribution of this paper is a fast iterative algorithm L-CCA consists of only QR de-
composition of relatively small matrices and a couple of matrix multiplications which only involves
huge sparse matrices or small dense matrices. This is achieved by reducing the computation of CCA
to a sequence of fast Least Square iterations. It is proved that L-CCA asymptotically converges to
the exact CCA solution and error analysis for ﬁnite iterations is also provided. As shown by the
experiments  L-CCA also has favorable performance on real datasets when compared with other
CCA approximations given a ﬁxed CPU time.

It’s worth pointing out that approximating CCA is much more challenging than SVD(or PCA).
As suggested by [12  13]  to approximate the top singular vectors of X  it sufﬁces to randomly
sample a small subspace in the span of X and some power iteration with this small subspace will
automatically converge to the directions with top singular values. On the other hand CCA has
to search through the whole X Y span in order to capture directions with large correlation. For
example  when the most correlated directions happen to live in the bottom singular vectors of the
data matrices  the random sample scheme will miss them completely. On the other hand  what
L-CCA algorithm doing intuitively is running an exact search of correlation structures on the top
singular vectors and an fast gradient based approximation on the remaining directions.

2 Background: Canonical Correlation Analysis

2.1 Deﬁnition

Canonical Correlation Analysis (CCA) can be deﬁned in many different ways. Here we use the def-
inition in [9  17] since this version naturally connects CCA with the Singular Value Decomposition
(SVD) of the whitened covariance matrix  which is the key to understanding our algorithm.
Deﬁnition 1. Let X 2 n ⇥ p1 and Y 2 n ⇥ p2 where the rows are i.i.d samples from a pair of
random vectors. Let x 2 p1 ⇥ p1  y 2 p2 ⇥ p2 and use x i  y j to denote the columns of
x  y respectively. Xx i  Yy j are called canonical variables if
i = j
i 6= j

>x iX>Yy j = ⇢di

if
if

0

>x iX>Xx j =⇢1

0

if
if

i = j
i 6= j

>y iY>Yy j =⇢1

0

if
if

i = j
i 6= j

Xx i  Yy i is the ith pair of canonical variables and di is the ith canonical correlation.

2.2 CCA and SVD

First introduce some notation. Let

For simplicity assume Cxx and Cyy are full rank and Let

Cxx = X>X Cyy = Y>Y Cxy = X>Y

˜Cxy = C 1

xx CxyC 1

2
yy

2

The following lemma provides a way to compute the canonical variables by SVD.
Lemma 1. Let ˜Cxy = UDV> be the SVD of ˜Cxy where ui  vj denote the left  right singular
vectors and di denotes the singular values. Then XC 1
yy vj are the canonical variables
of the X  Y space respectively.

xx ui  YC 1

2

2

Proof. Plug XC 1

xx ui  YC 1

2

2

yy vj into the equations in Deﬁnition 1 directly proves lemma 1

As mentioned before  we are interested in computing the top kcca canonical variables where kcca ⌧
p1  p2. Use U1  V1 to denote the ﬁrst kcca columns of U  V respectively and use U2  V2 for the
remaining columns. By lemma 1  the top kcca canonical variables can be represented by XC 1
xx U1
and YC 1

yy V1.

2

2

2

Algorithm 1 CCA via Iterative LS

Input : Data matrix X 2 n ⇥ p1  Y 2 n ⇥ p2. A target dimension kcca. Number of orthogonal
iterations t1
Output : Xkcca 2 n ⇥ kcca  Ykcca 2 n ⇥ kcca consist of top kcca canonical variables of X and Y.
1.Generate a p1 ⇥ kcca dimensional random matrix G with i.i.d standard normal entries.
2.Let X0 = XG
3.
for t = 1 to t1 do

Yt = HYXt1 where HY = Y(Y>Y)1Y>
Xt = HXYt where HX = X(X>X)1X>

end for
4.Xkcca = QR(Xt1)  Ykcca = QR(Yt1)
Function QR(Xt) extract an orthonormal basis of the column space of Xt with QR decomposition

3 Compute CCA by Iterative Least Squares

Since the top canonical variables are connected with the top singular vectors of ˜Cxy which can
be compute with orthogonal iteration [10] (it’s called simultaneous iteration in [21])  we can also
compute CCA iteratively. A detailed algorithm is presented in Algorithm1:
The convergence result of Algorithm 1 is stated in the following theorem:
Theorem 1. Assume |d1| > |d2| > |d3|... > |dkcca+1| and U>1 C
xxG is non singular (this will hold
with probability 1 if the elements of G are i.i.d Gaussian). The columns of Xkcca and Ykcca will
converge to the top kcca canonical variables of X and Y respectively if t1 ! 1.
Theorem 1 is proved by showing it’s essentially an orthogonal iteration [10  21] for computing the
top kcca eigenvectors of A = ˜Cxy ˜C>xy. A detailed proof is provided in the supplementary materials.

1
2

3.1 A Special Case

When X Y are sparse and Cxx  Cyy are diagonal (like the Penn Tree Bank dataset in the experi-
ments)  Algorithm 1 can be implemented extremely fast since we only need to multiply with sparse
matrices or inverting huge but diagonal matrices in every iteration. QR decomposition is performed
not only in the end but after every iteration for numerical stability issues (here we only need to QR
with matrices much smaller than X  Y). We call this fast version D-CCA in the following discus-
sions.
When Cxx  Cyy aren’t diagonal  computing matrix inverse becomes very slow. But we can still run
D-CCA by approximating (X>X)1  (Y>Y)1 with (diag(X>X))1  (diag(Y>Y))1 in algo-
rithm 1 when speed is a concern. But this leads to poor performance when Cxx  Cyy are far from
diagonal as shown by the URL dataset in the experiments.

3.2 General Case

Algorithm 1 reduces the problem of CCA to a sequence of iterative least square problems. When
X  Y are huge  solving LS exactly is still slow since it consists inverting a huge matrix but fast
LS methods are relatively well studied. There are many ways to approximate the LS solution by
optimization based methods like Gradient Descent [1  23]  Stochastic Gradient Descent [16  4] or
by random projection and subsampling based methods like [8  5]. A fast approximation to the top
kcca canonical variables can be obtained by replacing the exact LS solution in every iteration of Al-
gorithm 1 with a fast approximation. Here we choose LING [23] which works well for large sparse
design matrices for solving the LS problem in every CCA iteration.
The connection between CCA and LS has been developed under different setups for different pur-
poses. [20] shows that CCA in multi label classiﬁcation setting can be formulated as an LS problem.
[22] also formulates CCA as a recursive LS problem and builds an online version based on this
observation. The beneﬁt we take from this iterative LS formulation is that running a fast LS ap-

3

Algorithm 2 LING

Input : X 2 n ⇥ p  Y 2 n ⇥ 1. kpc  number of top left singular vectors selected. t2  number of
iterations in Gradient Descent.
Output : ˆY 2 n ⇥ 1  which is an approximation to X(X>X)1X>Y
1. Compute U1 2 n⇥ kpc  top kpc left singular vector of X by randomized SVD (See supplemen-
tary materials for detailed description).
2. Y1 = U1U>1 X.
3.Compute the residual. Yr = Y  Y1
4.Use gradient descent initial at the 0 vector (see supplementary materials for detailed description)
to approximately solve the LS problem minr2Rp kX r  Yrk2. Use r t2 to denote the solution
after t2 gradient iterations.
5. ˆY = Y1 + X r t2.

proximation in every iteration will give us a fast CCA approximation with both provable theoretical
guarantees and favorable experimental performance.

4 Algorithm

In this section we introduce L-CCA which is a fast CCA algorithm based on Algorithm 1.

4.1 LING: a Gradient Based Least Square Algorithm

First we need to introduce the fast LS algorithm LING as mentioned in section 3.2 which is used in
every orthogonal iteration of L-CCA .
Consider the LS problem:

⇤ = arg min

2Rp{kX  Y k2}

for X 2 n ⇥ p and Y 2 n ⇥ 1. For simplicity assume X is full rank. X ⇤ = X(X>X)1X>Y is
the projection of Y onto the column space of X. In this section we introduce a fast algorithm LING
to approximately compute X ⇤ without formulating (X>X)1 explicitly which is slow for large p.
The intuition of LING is as follows. Let U1 2 n ⇥ kpc (kpc ⌧ p) be the top kpc left singular vectors
of X and U2 2 n ⇥ (p  kpc) be the remaining singular vectors. In LING we decompose X ⇤ into
two orthogonal components 

X ⇤ = U1U>1 Y + U2U>2 Y

the projection of Y onto the span of U1 and the projection onto the span of U2. The ﬁrst term can
be computed fast given U1 since kpc is small. U1 can also be computed fast approximately with the
randomized SVD algorithm introduced in [12] which only requires a few fast matrix multiplication
and a QR decomposition of n ⇥ kpc matrix. The details for ﬁnding U1 are illustrated in the supple-
mentary materials. Let Yr = Y  U1U>1 Y be the residual of Y after projecting onto U1. For the
second term  we compute it by solving the optimization problem

min

r2Rp{kX r  Yrk2}

with Gradient Descent (GD) which is also described in detail in the supplementary materials. A
detailed description of LING are presented in Algorithm 2.
In the above discussion Y is a column vector. It is straightforward to generalize LING to ﬁt into
Algorithm 1 where Y have multiple columns by applying Algorithm 2 to every column of Y .
In the following discussions  we use LING (Y  X  kpc  t2) to denote the LING output with corre-
sponding inputs which is an approximation to X(X>X)1X>Y .
The following theorem gives error bound of LING .
Theorem 2. Use i to denote the ith singular value of X. Consider the LS problem

min

2Rp{kX  Y k2}

4

Algorithm 3 L-CCA

Input : X 2 n ⇥ p1  Y 2 n ⇥ p2: Data matrices.
kcca: Number of top canonical variables we want to extract.
t1: Number of orthogonal iterations.
kpc: Number of top singular vectors for LING
t2: Number of GD iterations for LING
Output : Xkcca 2 n ⇥ kcca  Ykcca 2 n ⇥ kcca: Top kcca canonical variables of X and Y.
1.Generate a p1 ⇥ kcca dimensional random matrix G with i.i.d standard normal entries.
2.Let X0 = XG  ˆX0 = QR(X0)
3.
for t = 1 to t1 do

Yt = LING( ˆXt1  Y  kpc  t2)  ˆYt = QR(Yt)
Xt = LING( ˆYt  X  kpc  t2)  ˆXt = QR(Xt)

end for
4.Xkcca = ˆXt1  Ykcca = ˆYt1

for X 2 n⇥ p and Y 2 n⇥ 1. Let Y ⇤ = X(X>X)1X>Y be the projection of Y onto the column
space of X and ˆYt2 = LING (Y  X  kpc  t2). Then

for some constant C > 0 and r =

kY ⇤  ˆYt2k2  Cr2t2
2
kpc+12
2
kpc+1+2

< 1

p

p

(1)

The proof is in the supplementary materials due to space limitation.
Remark 1. Theorem 2 gives some intuition of why LING decompose the projection into two com-
ponents. In an extreme case if we set kpc = 0 (i.e. don’t remove projection on the top principle
components and directly apply GD to the LS problem)  r in equation 1 becomes 2
12
. Usually
p
2
1+2
p
1 is much larger than p  so r is very close to 1 which makes the error decays slowly. Removing
projections on kpc top singular vector will accelerate error decay by making r smaller. The beneﬁt
of this trick is easily seen in the experiment section.

4.2 Fast Algorithm for CCA

Our fast CCA algorithm L-CCA are summarized in Algorithm 3:
There are two main differences between Algorithm 1 and 3. We use LING to solve Least squares
approximately for the sake of speed. We also apply QR decomposition on every LING output for
numerical stability issues mentioned in [21].

4.3 Error Analysis of L-CCA

This section provides mathematical results on how well the output of L-CCA algorithm approxi-
mates the subspace spanned by the top kcca true canonical variables for ﬁnite t1 and t2. Note that the
asymptotic convergence property of L-CCA when t1  t2 ! 1 has already been stated by theorem
1. First we need to deﬁne the distances between subspaces as introduced in section 2.6.3 of [10]:
Deﬁnition 2. Assume the matrices are full rank. The distance between the column space of matrix
W1 2 n ⇥ k and Z1 2 n ⇥ k is deﬁned by

dist(W1  Z1) = kHW1  HZ1k2

where HW1 = W1(W>1 W1)1W>1   HZ1 = Z1(Z>1 Z1)1Z>1 are projection matrices. Here
the matrix norm is the spectrum norm. Easy to see dist(W1  Z1) = dist(W1R1  Z1R2) for any
invertible k ⇥ k matrix R1  R2.
We continue to use the notation deﬁned in section 2. Recall that XC 1
ical variables from X. The following theorem bounds the distance between the truth XC 1
ˆXt1  the L-CCA output after ﬁnite iterations.

xx U1 gives the top kcca canon-
xx U1 and

2

2

5

Theorem 3. The distance between subspaces spanned top kcca canonical variables of X and the
subspace returned by L-CCA is bounded by

dist( ˆXt1  XC 1

2

xx U1)  C1✓ dkcca+1

dkcca ◆2t1

d2
kcca

r2t2

+ C2

kcca  d2
d2

kcca+1

where C1  C2 are constants. 0 < r < 1 is introduced in theorem 2. t1 is the number of power
iterations in L-CCA and t2 is the number of gradient iterations for solving every LS problem.

The proof of theorem 3 is in the supplementary materials.

5 Experiments

In this section we compare several fast algorithms for computing CCA on large datasets. First let’s
introduce the algorithms we compared in the experiments.

• RPCCA : Instead of running CCA directly on the high dimensional X Y  RPCCA com-
putes CCA only between the top krpcca principle components (left singular vector) of X
and Y where krpcca ⌧ p1  p2. For large n  p1  p2  we use randomized algorithm introduced
in [12] for computing the top principle components of X and Y (see supplementary ma-
terial for details). The tuning parameter that controls the tradeoff between computational
cost and accuracy is krpcca. When krpcca is small RPCCA is fast but fails to capture the
correlation structure on the bottom principle components of X and Y. When krpcca grows
larger the principle components captures more structure in X Y space but it takes longer
to compute the top principle components. In the experiments we vary krpcca.

• D-CCA : See section 3.1 for detailed descriptions. The advantage of D-CCA is it’s ex-
tremely fast.
In the experiments we iterate 30 times (t1 = 30) to make sure D-CCA
achieves convergence. As mentioned earlier  when Cxx and Cyy are far from diagonal
D-CCA becomes inaccurate.

• L-CCA : See Algorithm 3 for detailed description. We ﬁnd that the accuracy of LING
in every orthogonal iteration is crucial to ﬁnding directions with large correlation while a
small t1 sufﬁces. So in the experiments we ﬁx t1 = 5 and vary t2. In both experiments
we ﬁx kpc = 100 so the top kpc singular vectors of X  Y and every LING iteration can be
computed relatively fast.

• G-CCA : A special case of Algorithm 3 where kpc is set to 0. I.e.

the LS projection in
every iteration is computed directly by GD. G-CCA does not need to compute top singular
vectors of X and Y as L-CCA . But by equation 1 and remark 1 GD takes more iterations
to converge compared with LING . Comparing G-CCA and L-CCA in the experiments
illustrates the beneﬁt of removing the top singular vectors in LING and how this can affect
the performance of the CCA algorithm. Same as L-CCA we ﬁx the number of orthogonal
iterations t1 to be 5 and vary t2  the number of gradient iterations for solving LS.

RPCCA   L-CCA   G-CCA are all "asymptotically correct" algorithms in the sense that if we
spend inﬁnite CPU time all three algorithms will provide the exact CCA solution while D-CCA is
extremely fast but relies on the assumption that X Y both have orthogonal columns. Intuitively 
given a ﬁxed CPU time  RPCCA dose an exact search on krpcca top principle components of X
and Y. L-CCA does an exact search on the top kpc principle components (kpc < krpcca) and an
crude search over the other directions. G-CCA dose a crude search over all the directions. The
comparison is in fact testing which strategy is the most effective in ﬁnding large correlations over
huge datasets.
Remark 2. Both RPCCA and G-CCA can be regarded as special cases of L-CCA . When t1 is
large and t2 is 0  L-CCA becomes RPCCA and when kpc is 0 L-CCA becomes G-CCA .

In the following experiments we aims at extracting 20 most correlated directions from huge data
matrices X and Y. The output of the above four algorithms are two n⇥ 20 matrices Xkcca and Ykcca
the columns of which contains the most correlated directions. Then a CCA is performed between
Xkcca and Ykcca with matlab built-in CCA function. The canonical correlations between Xkcca and
Ykcca indicates the amount of correlations captured from the the huge X Y spaces by above four

6

algorithms. In all the experiments  we vary krpcca for RPCCA and t2 for L-CCA and G-CCA to
make sure these three algorithms spends almost the same CPU time ( D-CCA is alway fastest). The
20 canonical correlations between the subspaces returned by the four algorithms are plotted (larger
means better).

We want to make to additional comments here based on the reviewer’s feedback. First  for the two
datasets considered in the experiments  classical CCA algorithms like the matlab built in function
takes more than an hour while our algorithm is able to get an approximate answer in less than 10
minutes. Second  in the experiments we’ve been focusing on getting a good ﬁt on the training
datasets and the performance is evaluated by the magnitude of correlation captured in sample. To
achieve better generalization performance a common trick is to perform regularized CCA [14] which
easily ﬁts into our frame work since it’s equivalent to running iterative ridge regression instead of
OLS in Algorithm 1. Since our goal is to compute a fast and accurate ﬁt  we don’t pursue the
generalization performance here which is another statistical issue.

5.1 Penn Tree Bank Word Co-ocurrence

CCA has already been successfully applied to building a low dimensional word embedding in [6  7].
So the ﬁrst task is a CCA between words and their context. The dataset used is the full Wall Street
Journal Part of Penn Tree Bank which consists of 1.17 million tokens and a vocabulary size of 43k
[18]. The rows of X matrix consists the indicator vectors of the current word and the rows of Y
consists of indicators of the word after. To avoid sample sparsity for Y we only consider 3000 most
frequent words  i.e. we only consider the tokens followed by 3000 most frequent words which is
about 1 million. So X is of size 1000k ⇥ 43k and Y is of size 1000k ⇥ 3k where both X and
Y are very sparse. Note that every row of X and Y only has a single 1 since they are indicators
of words. So in this case Cxx  Cyy are diagonal and D-CCA can compute a very accurate CCA
in less than a minute as mentioned in section 3.1. On the other hand  even though this dataset can
be solved efﬁciently by D-CCA   it is interesting to look at the behavior of other three algorithms
which do not make use of the special structure of this problem and compare them with D-CCA
which can be regarded as the truth in this particular case. For RPCCA L-CCA G-CCA we try
three different parameter set ups shown in table 1 and the 20 correlations are shown in ﬁgure 1.
Among the three algorithms L-CCA performs best and gets pretty close to D-CCA as CPU time
increases. RPCCA doesn’t perform well since a lot correlation structure of word concurrence exist
in low frequency words which can’t be captured in the top principle components of X Y. Since the
most frequent word occurs 60k times and the least frequent words occurs only once  the spectral of
X drops quickly which makes GD converges very slowly. So G-CCA doesn’t perform well either.

Table 1: Parameter Setup for Two Real Datasets

PTB word co-occurrence

t2

t2

krpcca
RPCCA L-CCA G-CCA time
170
460
1180

300
500
800

17
51
127

7
38
115

CPU id

URL features

t2

t2

CPU
krpcca
RPCCA L-CCA G-CCA time
220
175
130

600
600
600

7
16
17

4
11
13

1
2
3

id

1
2
3

5.2 URL Features

The second dataset is the URL Reputation dataset from UCI machine learning repository. The
dataset contains 2.4 million URLs each represented by 3.2 million features. For simplicity we only
use ﬁrst 400k URLs. 38% of the features are host based features like WHOIS info  IP preﬁx and
62% are lexical based features like Hostname and Primary domain. See [19] for detailed information
about this dataset. Unfortunately the features are anonymous so we pick the ﬁrst 35% features as our
X and last 35% features as our Y. We remove the 64 continuous features and only use the Boolean
features. We sort the features according to their frequency (each feature is a column of 0s and 1s 
the column with most 1s are the most frequent feature). We run CCA on three different subsets of
X and Y. In the ﬁrst experiment we select the 20k most frequent features of X and Y respectively.

7

PTB Word Occurrence CPU time: 170 secs

PTB Word Occurrence CPU time: 460 secs

PTB Word Occurrence CPU time: 1180 secs

l

n
o
i
t
a
e
r
r
o
C

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

l

n
o
i
t
a
e
r
r
o
C

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

l

n
o
i
t
a
e
r
r
o
C

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

Figure 1: PTB word co-ocurrence: Canonical correlations of the 20 directions returned by four
algorithms. x axis are the indices and y axis are the correlations.

In the second experiment we select 20k most frequent features from X Y after removing the top
100 most frequent features of X and 200 most frequent features of Y. In the third experiment we
remove top 200 most frequent features from X and top 400 most frequent features of Y. So we are
doing CCA between two 400k ⇤ 20k data matrices in these experiments. In this dataset the features
within X and Y has huge correlations  so Cxx and Cyy aren’t diagonal anymore. But we still run
D-CCA since it’s extremely fast. The parameter set ups for the three subsets are shown in table 1
and the 20 correlations are shown in ﬁgure 2.
For this dataset the fast D-CCA doesn’t capture largest correlation since the correlation within X
and Y make Cxx  Cyy not diagonal. RPCCA has best performance in experiment 1 but not as good
in 2  3. On the other hand G-CCA has good performance in experiment 3 but performs poorly in 1 
2. The reason is as follows: In experiment 1 the data matrices are relatively dense since they includes
some frequent features. So every gradient iteration in L-CCA and G-CCA is slow. Moreover  since
there are some high frequency features and most features has very low frequency  the spectrum of
the data matrices in experiment 1 are very steep which makes GD in every iteration of G-CCA
converges very slowly. These lead to poor performance of G-CCA . In experiment 3 since the
frequent features are removed data matrices becomes more sparse and has a ﬂat spectrum which is
in favor of G-CCA . L-CCA has stable and close to best performance despite those variations in
the datasets.

n
o

i
t

l

a
e
r
r
o
C

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

 

URL1 CPU time: 220secs

URL2 CPU time: 175secs

URL3 CPU time: 130secs

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

n
o

i
t

l

a
e
r
r
o
C

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

 

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

n
o

i
t

l

a
e
r
r
o
C

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

 

 

L−CCA

D−CCA

RPCCA

G−CCA

5

10
Index

15

20

Figure 2: URL: Canonical correlations of the 20 directions returned by four algorithms. x axis are
the indices and y axis are the correlations.

6 Conclusion and Future Work

In this paper we introduce L-CCA   a fast CCA algorithm for huge sparse data matrices. We
construct theoretical bound for the approximation error of L-CCA comparing with the true CCA
solution and implement experiments on two real datasets in which L-CCA has favorable perfor-
mance. On the other hand  there are many interesting fast LS algorithms with provable guarantees
which can be plugged into the iterative LS formulation of CCA. Moreover  in the experiments we
focus on how much correlation is captured by L-CCA for simplicity. It’s also interesting to use
L-CCA for feature generation and evaluate it’s performance on speciﬁc learning tasks.

8

References
[1] Marina A.Epelman. Rate of convergence of steepest descent algorithm. 2007.
[2] Haim Avron  Christos Boutsidis  Sivan Toledo  and Anastasios Zouzias. Efﬁcient dimension-

ality reduction for canonical correlation analysis. In ICML (1)  pages 347–355  2013.

[3] Francis R. Bach and Michael I. Jordan. A probabilistic interpretation of canonical correlation

analysis. Technical report  University of California  Berkeley  2005.

[4] Léon Bottou. Large-Scale Machine Learning with Stochastic Gradient Descent.

In Yves
Lechevallier and Gilbert Saporta  editors  Proceedings of the 19th International Conference
on Computational Statistics (COMPSTAT’2010)  pages 177–187  Paris  France  August 2010.
Springer.

[5] Paramveer Dhillon  Yichao Lu  Dean P. Foster  and Lyle Ungar. New subsampling algorithms
for fast least squares regression. In Advances in Neural Information Processing Systems 26 
pages 360–368. 2013.

[6] Paramveer S. Dhillon  Dean Foster  and Lyle Ungar. Multi-view learning of word embeddings

via cca. In Advances in Neural Information Processing Systems (NIPS)  volume 24  2011.

[7] Paramveer S. Dhillon  Jordan Rodu  Dean P. Foster  and Lyle H. Ungar. Two step cca: A new
spectral method for estimating vector models of words. In Proceedings of the 29th Interna-
tional Conference on Machine learning  ICML’12  2012.

[8] Petros Drineas  Michael W. Mahoney  S. Muthukrishnan  and Tamás Sarlós. Faster least

squares approximation. CoRR  abs/0710.1435  2007.

[9] Dean P. Foster  Sham M. Kakade  and Tong Zhang. Multi-view dimensionality reduction via

canonical correlation analysis. Technical report  2008.

[10] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins

University Press  Baltimore  MD  USA  1996.

[11] Gene. H Golub and Hongyuan Zha. The canonical correlations of matrix pairs and their nu-
merical computation. Technical report  Computer Science Department  Stanford University 
1992.

[12] N. Halko  P. G. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev.  53(2):217–288 
May 2011.

[13] Nathan Halko  Per-Gunnar Martinsson  Yoel Shkolnisky  and Mark Tygert. An algorithm for
the principal component analysis of large data sets. SIAM J. Scientiﬁc Computing  33(5):2580–
2594  2011.

[14] David R. Hardoon  Sandor Szedmak  Or Szedmak  and John Shawe-taylor. Canonical correla-

tion analysis; an overview with application to learning methods. Technical report  2007.

[15] H Hotelling. Relations between two sets of variables. Biometrika  28:312–377  1936.
[16] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-

ance reduction. Advances in Neural Information Processing Systems (NIPS)  2013.

[17] Sham M. Kakade and Dean P. Foster. Multi-view regression via canonical correlation analysis.

In In Proc. of Conference on Learning Theory  2007.

[18] Michael Lamar  Yariv Maron  Mark Johnson  and Elie Bienenstock. SVD and Clustering for
Unsupervised POS Tagging. In Proceedings of the ACL 2010 Conference Short Papers  pages
215–219  Uppsala  Sweden  2010. Association for Computational Linguistics.

[19] Justin Ma  Lawrence K. Saul  Stefan Savage  and Geoffrey M. Voelker. Identifying suspicious
urls: An application of large-scale online learning. In In Proc. of the International Conference
on Machine Learning (ICML)  2009.

[20] Liang Sun  Shuiwang Ji  and Jieping Ye. A least squares formulation for canonical correlation
analysis. In Proceedings of the 25th International Conference on Machine Learning  ICML
’08  pages 1024–1031  New York  NY  USA  2008. ACM.

[21] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. SIAM  1997.

9

,Yichao Lu
Dean Foster
Tam Nguyen
Maximilian Dax
Chaithanya Kumar Mummadi
Nhung Ngo
Thi Hoai Phuong Nguyen
Zhongyu Lou
Thomas Brox