2019,Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses,In this paper  we study large-scale convex optimization algorithms based on the Newton method applied to regularized generalized self-concordant losses  which include logistic regression and softmax regression.  We first prove that our new simple scheme based on a sequence of problems with decreasing regularization parameters is provably globally convergent  that this convergence is linear with a constant factor which scales only logarithmically with the condition number. In the parametric setting  we obtain an algorithm with the same scaling than regular first-order methods but with an improved behavior  in particular in ill-conditioned problems. Second  in the non parametric machine learning setting  we provide an explicit algorithm combining  the previous scheme with Nystr\"om projections techniques  and prove that it achieves optimal generalization bounds with a time complexity of order O(n df)  a memory complexity of order O(df^2) and no dependence on the condition number  generalizing the results known for least squares regression. Here n is the number of observations and df is the associated degrees of freedom. In particular  this is the first large-scale algorithm to solve logistic and softmax regressions in the non-parametric setting with large condition numbers and theoretical guarantees.,Globally Convergent Newton Methods for

Ill-conditioned Generalized Self-concordant Losses

Ulysse Marteau-Ferey

Francis Bach

INRIA - École Normale Supérieure

PSL Reasearch University

ulysse.marteau-ferey@inria.fr

INRIA - École Normale Supérieure

PSL Reasearch University

francis.bach@inria.fr

Alessandro Rudi

INRIA - École Normale Supérieure

PSL Reasearch University

alessandro.rudi@inria.fr

Abstract

In this paper  we study large-scale convex optimization algorithms based on the
Newton method applied to regularized generalized self-concordant losses  which
include logistic regression and softmax regression. We ﬁrst prove that our new
simple scheme based on a sequence of problems with decreasing regularization
parameters is provably globally convergent  that this convergence is linear with a
constant factor which scales only logarithmically with the condition number. In
the parametric setting  we obtain an algorithm with the same scaling than regular
ﬁrst-order methods but with an improved behavior  in particular in ill-conditioned
problems. Second  in the non-parametric machine learning setting  we provide
an explicit algorithm combining the previous scheme with Nyström projection
techniques  and prove that it achieves optimal generalization bounds with a time
complexity of order O(ndfλ)  a memory complexity of order O(df2
λ) and no
dependence on the condition number  generalizing the results known for least-
squares regression. Here n is the number of observations and dfλ is the associated
degrees of freedom. In particular  this is the ﬁrst large-scale algorithm to solve
logistic and softmax regressions in the non-parametric setting with large condition
numbers and theoretical guarantees.

1

Introduction

Minimization algorithms constitute a crucial algorithmic part of many machine learning methods 
with algorithms available for a variety of situations [10]. In this paper  we focus on ﬁnite sum
problems of the form

min
x∈H fλ(x) = f (x) +

(cid:107)x(cid:107)2  with f (x) =

λ
2

1
n

fi(x) 

n(cid:88)

i=1

where H is a Euclidean or a Hilbert space  and each function is convex and smooth. The running-
time of minimization algorithms classically depends on the number of functions n  the explicit (for
Euclidean spaces) or implicit (for Hilbert spaces) dimension d of the search space  and the condition
number of the problem  which is upper bounded by κ = L/λ  where L characterizes the smoothness
of the functions fi  and λ the regularization parameter.
In the last few years  there has been a strong focus on problems with large n and d  leading to ﬁrst-
order (i.e.  gradient-based) stochastic algorithms  culminating in a sequence of linearly convergent

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

√

√

κ is not practical anymore (see examples in Sect. 5).

κ [15  22  14  4].
algorithms whose running time is favorable in n and d  but scale at best in
However  modern problems lead to objective functions with very large condition numbers  i.e.  in
many learning problems  the regularization parameter that is optimal for test predictive performance
may be so small that the scaling above in
These ill-conditioned problems are good candidates for second-order methods (i.e.  that use the
Hessians of the objective functions) such as Newton method. These methods are traditionally
discarded within machine learning for several reasons: (1) they are usually adapted to high precision
results which are not necessary for generalization to unseen data for machine learning problems [9] 
(2) computing the Newton step ∆λ(x) = ∇2fλ(x)−1∇fλ(x) requires to form the Hessian and solve
the associated linear system  leading to complexity which is at least quadratic in d  and thus prohibitive
for large d  and (3) the global convergence properties are not applicable  unless the function is very
special  i.e.  self-concordant [24] (which includes only few classical learning problems)  so they often
are only shown to converge in a small area around the optimal x.
In this paper  we argue that the three reasons above for not using Newton method can be circumvented
to obtain competitive algorithms: (1) high absolute precisions are indeed not needed for machine
learning  but faced with strongly ill-conditioned problems  even a low-precision solution requires
second-order schemes; (2) many approximate Newton steps have been designed for approximating
the solution of the associated large linear system [1  27  25  8]; (3) we propose a novel second-
order method which is globally convergent and which is based on performing approximate Newton
methods for a certain class of so-called generalized self-concordant functions which includes logistic
regression [6]. For these functions  the conditioning of the problem is also characterized by a more
local quantity: κ(cid:96) = R2/λ  where R characterizes the local evolution of Hessians. This leads
to second-order algorithms which are competitive with ﬁrst-order algorithms for well-conditioned
problems  while being superior for ill-conditioned problems which are common in practice.

Contributions. We make the following contributions:

(a) We build a global second-order method for the minimization of fλ  which relies only on
computing approximate Newton steps of the functions fµ  µ ≥ λ. The number of such
 ) where  is the desired precision  and c is an
steps will be of order O(c log κ(cid:96) + log 1
explicit constant. In the parametric setting (H = Rd)  c can be as bad as
κ(cid:96) in the
worst-case but much smaller in theory and practice. Moreover in the non-parametric/kernel
machine learning setting (H inﬁnite dimensional)  c does not depend on the local condition
number κ(cid:96).

√

(b) Together with the appropriate quadratic solver to compute approximate Newton steps 
we obtain an algorithm with the same scaling as regular ﬁrst-order methods but with an
improved behavior  in particular in ill-conditioned problems. Indeed  this algorithm matches
the performance of the best quadratic solvers but covers any generalized self-concordant
function  up to logarithmic terms.

(c) In the non-parametric/kernel machine learning setting we provide an explicit algorithm
combining the previous scheme with Nyström projections techniques. We prove that it
achieves optimal generalization bounds with O(ndfλ) in time and O(df2
λ) in memory 
where n is the number of observations and dfλ is the associated degrees of freedom. In
particular  this is the ﬁrst large-scale algorithm to solve logistic and softmax regression in
the non-parametric setting with large condition numbers and theoretical guarantees.

1.1 Comparison to related work
We consider two cases for H and the functions fi that are common in machine learning: H = Rd with
linear (in the parameter) models with explicit feature maps  and H inﬁnite-dimensional  corresponding
in machine learning to learning with kernels [32]. Moreover in this section we ﬁrst consider the
2 (x(cid:62)zi − yi)2 for
quadratic case  for example the squared loss in machine learning (i.e.  fi(x) = 1
some zi ∈ H  yi ∈ R). We ﬁrst need to introduce the Hessian of the problem  for any λ > 0  deﬁne

H(x) := ∇2f (x) 

Hλ(x) := ∇2fλ(x) = H(x) + λI 

in particular we denote by H (and analogously Hλ) the Hessian at optimum (which in case of squared
loss corresponds to the covariance matrix of the inputs).

2

Quadratic problems and H = Rd (ridge regression). The problem then consists in solving a
(ill-conditioned) positive semi-deﬁnite symmetric linear system of dimension d × d. Methods based
on randomized linear algebra  sketching and suitable subsampling [17  18  11] are able to ﬁnd the
solution with precision  in time that is O((nd+min(n  d)3) log(L/λ))  so essentially independently
of the condition number  because of the logarithmic complexity in λ.
Quadratic problems and H inﬁnite-dimensional (kernel ridge regression). Here the problem
corresponds to solving a (ill-conditioned) inﬁnite-dimensional linear system in a reproducing kernel
Hilbert space [32]. Since however the sum deﬁning f is ﬁnite  the problem can be projected on a
subspace of dimension at most n [5]  leading to a linear system of dimension n × n. Solving it
with the techniques above would lead to a complexity of the order O(n2)  which is not feasible on
massive learning problems (e.g.  n ≈ 107). Interestingly these problems are usually approximately
low-rank  with the rank represented by the so called effective-dimension dfλ [13]  counting essentially
the eigenvalues of the problem larger than λ 

dfλ = Tr(HH−1
λ ).

(1)
Note that dfλ is bounded by min{n  L/λ} and in many cases dfλ (cid:28) min(n  L/λ). Using suitable
projection techniques  like Nyström [34] or random features [26] it is possible to further reduce the
problem to dimension dfλ  for a total cost to ﬁnd the solution of O(ndf2
λ). Finally recent methods [29] 
combining suitable projection methods with reﬁned preconditioning techniques  are able to ﬁnd the
solution with precision compatible with the optimal statistical learning error [13] in time that is
O(ndfλ log(L/λ))  so being essentially independent of the condition number of the problem.

Convex problems and explicit features (logistic regression). When the loss function is self-
concordant it is possible to leverage the fast techniques for linear systems in approximate Newton
algorithms [25] (see more in Sec. 2)  to achieve the solution in essentially O(nd + min(n  d)3)
time  modulo logarithmic terms. However only few loss functions of interest are self-concordant 
in particular the widely used logistic and soft-max losses are not self-concordant  but generalized-
self-concordant [6]. In such cases we need to use (accelerated/stochastic) ﬁrst order optimization
methods to enter in the quadratic convergence region of Newton methods [2]  which leads to a

solution in O(dn + d(cid:112)nL/λ + min(n  d)3) time  which does not present any improvement on a

simple accelerated ﬁrst-order method. Globally convergent second-order methods have also been
proposed to solve such problems [21]  but the number of Newton steps needed being bounded only
by L/λ  they lead to a solution in O(L/λ (nd + min(n  d)3)). With λ that could be as small as
10−12 in modern machine learning problems  this makes both these kind of approaches expensive
from a computational viewpoint for ill-conditioned problems. For such problems  with our new
global second-order scheme  the algorithm we propose achieves instead a complexity of essentially
O((nd + min(n  d)3) log(R2/λ)) (see Thm. 1).
Convex problems and H inﬁnite-dimensional (kernel logistic regression). Analogously to the
case above  it is not possible to use Newton methods proﬁtably as global optimizers on losses that
are not self-concordant as we see in Sec. 3. In such cases by combining projecting techniques
developped in Sec. 4 and accelerated ﬁrst-order optimization methods  it is possible to ﬁnd a
solution in O(ndfλ + dfλ
scenario  since it strongly depends on the condition number L/λ. In Sec. 4 we suitably combine our
optimization algorithm with projection techniques achieving optimal statistical learning error [23] in
essentially O(ndfλ log(R2/λ)).

(cid:112)nL/λ) time. This can still be prohibitive in the very small regularization

First-order algorithms for ﬁnite sums.
In dimension d  accelerated algorithms for strongly-
convex smooth (not necessarily self-concordant) ﬁnite sums  such as K-SVRG [4]  have a running time

proportional O((n +(cid:112)nL/λ)d). This can be improved with preconditioning to O((n +(cid:112)dL/λ)d)

for large n [2]. Quasi-Newton methods can also be used [20]  but typically without the guarantees
that we provide in this paper (which are logarithmic in the condition number in natural scenarios).

2 Background: Newton methods and generalized self concordance

In this section we start by recalling the deﬁnition of generalized self concordant functions and motivate
it with examples. We then recall basic facts about Newton and approximate Newton methods  and

3

present existing techniques to efﬁciently compute approximate Newton steps. We start by introducing
the deﬁnition of generalized self-concordance  that here is an extension of the one in [6].
Deﬁnition 1 (generalized self-concordant (GSC) function). Let H be a Hilbert space. We say that f
is a generalized self-concordant function on G ⊂ H  when G is a bounded subset of H and f is a
convex and three times differentiable mapping on H such that

∀x ∈ H  ∀h  k ∈ H  ∇(3)f (x)[h  k  k] ≤ supg∈G |g · h| ∇2f (x)[k  k].

j wi)(cid:1) − x(cid:62)

yi

wi  where now x ∈ Rd×k and

i x) with ϕ(u) = log(eu + e−u).

i x))  where x  wi ∈ Rd and yi ∈ {−1  1}.

(b) Softmax regression: fi(x) = log(cid:0)(cid:80)k

We will usually denote by R the quantity supg∈G (cid:107)g(cid:107) < ∞ and often omit G when it is clear from
the context (for simplicity think of G as the ball in H centered in zero and with radius R > 0 
then supg∈G |g · h| = R(cid:107)h(cid:107)). The globally convergent second-order scheme we present in Sec. 3
is speciﬁc to losses which satisfy this generalized self-concordance property. The following loss
functions  which are widely used in machine learning  are generalized-self-concordant  and motivate
this work.
Example 1 (Application to ﬁnite-sum minimization). The following loss functions are generalized
self-concordant functions  but not self-concordant:
(a) Logistic regression: fi(x) = log(1 + exp(−yiw(cid:62)
j=1 exp(x(cid:62)
yi ∈ {1  . . .   k} and xj denotes the j-th column of x.
(c) Generalized linear models with bounded features (see details in [7  Sec. 2.1])  which include
conditional random ﬁelds [33].
(d) Robust regression: fi(x) = ϕ(yi − w(cid:62)
Note that these losses are not self-concordant in the sense of [25]. Moreover  even if the losses fi are
self-concordant  the objective function f is not necessarily self-concordant  making any attempt to
prove the self-concordance of the objective function f almost impossible.
Newton method (NM). Given x0 ∈ H  the Newton method consists in doing the following update:
(2)
λ (x)∇fλ(x) is called the Newton step at point x  and x − ∆λ(x) is the
The quantity ∆λ(x) := H−1
minimizer of the second order approximation of fλ around x. Newton methods enjoy the following
key property: if x0 is close enough to the optimum  the convergence to the optimum is quadratic and
the number of iterations required to a given precision is independent of the condition number of the
problem [12].
However Newton methods have two main limitations: (a) the region of quadratic convergence can be
quite small and reaching the region can be computationally expensive  since it is usually done via
ﬁrst order methods [2] that converge linearly depending on the condition number of the problem  (b)
the cost of computing the Hessian can be really expensive when n  d are large  and also (c) the cost
of computing ∆λ(xt) can be really prohibitive. In the rest of the section we recall some ways to deal
with (b) and (c). Our main result of Sec. 3 is to provide globalization scheme for the Newton method
to tackle problem (a)  which is easily integrable with approximate techniques to deal with (b) ans (c) 
to make second-order technique competitive.

∆λ(xt) := H−1

λ (xt)∇fλ(xt).

xt+1 = xt − ∆λ(xt) 

Approximate Newton methods (ANM) and approximate solutions to linear systems. Comput-
ing exactly the Newton increment ∆λ(xt)  which corresponds essentially to the solution of a linear
system  can be too expensive when n  d are large. A natural idea is to approximate the Newton
iteration  leading to approximate Newton methods 

xt+1 = xt −(cid:101)∆λ(xt) 

(cid:101)∆λ ≈ ∆λ(xt).

In this paper  more generally we consider any technique to compute (cid:101)∆λ(xt) that provides a relative

approximation [16] of ∆λ(xt) deﬁned as follows.
Deﬁnition 2 (relative approximation). Let ρ < 1  let A be an invertible positive deﬁnite Hermitian
operator on H and b in H. We denote by LinApprox(A  b  ρ) the set of all ρ-relative approximations
of z∗ = A−1b  i.e.  LinApprox(A  b  ρ) = {z ∈ H | (cid:107)z − z∗(cid:107)A ≤ ρ(cid:107)z∗(cid:107)A}.

(3)

4

Sketching and subsampling for approximate Newton methods. Many techniques for approxi-

mating linear systems have been used to compute (cid:101)∆λ  in particular sketching of the Hessian matrix

via fast transforms and subsampling (see [25  8  2] and references therein). Assuming for simplicity
that fi = (cid:96)i(w(cid:62)

i x)  with (cid:96)i : R → R and wi ∈ H  it holds:
i x)wiw(cid:62)

n(cid:88)

H(x) =

(w(cid:62)

(cid:96)(2)
i

i = V (cid:62)

x Vx 

(4)

1
n

i=1

i

(w(cid:62)

i x))1/2 and W ∈ Rn×d deﬁned as W = (w1  . . .   wn)(cid:62).

with Vx ∈ Rn×d = DxW   where Dx ∈ Rn×n is a diagonal matrix deﬁned as (Dx)ii =
((cid:96)(2)
Both sketching and subsampling methods approximate z∗ = Hλ(x)−1∇fλ(x) with ˜z =
ij where
j=1 are indices selected at random from

(cid:101)Hλ(x)−1∇fλ(x)  in particular  in the case of subsampling (cid:101)H(x) = (cid:80)Q
j=1 pjwij w(cid:62)
{1  . . .   n} with suitable probabilities. Sketching methods instead use (cid:101)H(x) = (cid:101)V (cid:62)
x (cid:101)Vx  with
(cid:101)Vx = ΩVx with Ω ∈ RQ×n a structured matrix such that computing (cid:101)Vx has a cost in the order

j=1 are suitable weights and (ij)Q

Q (cid:28) min(n  d)  (pj)n

of O(nd log n); to this end usually Ω is based on fast Fourier or Hadamard transforms [25]. Note that
essentially all the techniques used in approximate Newton methods guarantee relative approximation.
In particular the following results can be found in the literature (see Lemmas 28 and 29 in Appendix I
and [25]  Lemma 2 for more details).
Lemma 1. Let x  b ∈ H and assume that (cid:96)(2)
i ≤ a for a > 0. With probability 1 − δ the following
methods output an element in LinApprox(Hλ(x)  b  ρ)  in O(Q2d + Q3 + c) time  O(Q2 + d) space:
(a) Subsampling with uniform sampling (see [27  28])  where Q = O(ρ−2a/λ log 1
λδ ) and c = O(1).
(b) Subsampling with approximate leverage scores [27  3  28])  where Q = O(ρ−2 ¯dfλ log 1/λδ)  c =
2) and ¯dfλ = Tr(W (cid:62)W (W (cid:62)W + λ/aI)−1) [30]. Note that ¯dfλ ≤ min(n  d).
O(min(n  a/λ) ¯dfλ
(c) Sketching with fast Hadamard transform [25]  where Q = O(ρ−2 ¯dfλ log a/λδ)  c = O(nd log n).

3 Globally convergent scheme for ANM algorithms on GSC functions

The algorithm is based on the observation that when fλ is generalized self concordant  there exists
a region where t steps of ANM converge as fast as 2−t. Our idea is to start from a very large
regularization parameter λ0  such that we are sure that x0 is in the convergence region and perform
some steps of ANM such that the solution enters in the convergence region of fλ1  with λ1 = qλ0
with q < 1  and to iterate this procedure until we enter the convergence region of fλ. First we deﬁne
the region of interest and characterize the behavior of NM and ANM in the region  then we analyze
the globalization scheme.

Preliminary results: the Dikin ellipsoid. We consider the following region that we prove to be
contained in the region of quadratic convergence for the Newton method and that will be useful to
build the globalization scheme. Let c  R > 0 and fλ be generalized self-concordant with coefﬁcient R 
we call Dikin ellipsoid and denote by Dλ(c) the region

Dλ(c) :=(cid:8)x | νλ(x) ≤ c

√

λ/R(cid:9)  with νλ(x) := (cid:107)∇fλ(x)(cid:107)H

4 νλ(x)2 ≤ fλ(x) − fλ(x(cid:63)

λ (x) 
−1
where νλ(x) is usually called the Newton decrement and (cid:107)x(cid:107)A stands for (cid:107)A1/2x(cid:107).
Lemma 2. Let λ > 0  c ≤ 1/7  let fλ be generalized self-concordant and x ∈ Dλ(c). Then it
λ) ≤ νλ(x)2. Moreover Newton method starting from x0 has
holds: 1
quadratic convergence  i.e.  let xt be obtained via t ∈ N steps of Newton method in Eq. (2)  then
convergence rate  i.e.  let xt given by Eq. (3)  with (cid:101)∆t ∈ LinApprox(Hλ(xt) ∇fλ(xt)  ρ) and
νλ(xt) ≤ 2−(2t−1)νλ(x0). Finally  approximate Newton methods starting from x0 have a linear
ρ ≤ 1/7  then νλ(xt) ≤ 2−tνλ(x0).
This result is proved in Lemma 11 in Appendix B.3. The crucial aspect of the result above is that
when x0 ∈ Dλ(c)  the convergence of the approximate Newton method is linear and does not depend
√
on the condition number of the problem. However Dλ(c) itself can be very small depending on

λ/R. In the next subsection we see how to enter in Dλ(c) in an efﬁcient way.

5

√

Entering the Dikin ellipsoid using a second-order scheme. The lemma above shows that Dλ(c)
is a good region where to use the approximate Newton algorithm on GSC functions. However the
region itself is quite small  since it depends on
λ/R. Some other globalization schemes arrive to
regions of interest by ﬁrst-order methods or back-tracking schemes [2  1]. However such approaches

require a number of steps that is usually proportional to(cid:112)L/λ making them non-beneﬁcial in machine

learning contexts. Here instead we consider the following simple scheme where ANMρ(fλ  x  t) is the
result of a ρ-relative approximate Newton method performing t steps of optimization starting from x.
The main ingredient to guarantee the scheme to work is the following lemma (see Lemma 13 in
Appendix C.1 for a proof).
Lemma 3. Let µ > 0  c < 1 and x ∈ H. Let s = 1 + R(cid:107)x(cid:107)/c  then for q ∈ [1 − 2/(3s)  1)

Dµ(c/3) ⊆ Dqµ(c).

Now we are ready to show that we can guarantee the loop invariant xk ∈ Dµk (c). Indeed assume that
xk−1 ∈ Dµk−1 (c). Then νµk−1(xk−1) ≤ c
µk−1/R. By taking t = 2  ρ = 1/7  and performing
xk = ANMρ(fµk−1  xk−1  t)  by Lemma 2  νµk−1 (xk) ≤ 1/4νµk−1(xk−1) ≤ c/4
µk−1/R  i.e. 
xk ∈ Dµk−1 (c/4). If qk is large enough  this implies that xk ∈ Dqkµk−1 (c) = Dµk (c)  by Lemma 3.
Now we are ready to state our main theorem of this section.

√

√

Proposed Globalization Scheme

Start with x0 ∈ H  µ0 > 0  t  T ∈ N and (qk)k∈N ∈ (0  1].
For k ∈ N

Phase I: Getting in the Dikin ellispoid of fλ

Stop when µk+1 < λ and set xlast ← xk.

xk+1 ← ANMρ(fµk   xk  t)
µk+1 ← qk+1µk
Return(cid:98)x ← ANMρ(fλ  xlast  T )

Phase II: reach a certain precision starting from inside the Dikin ellipsoid

Fully adaptive method. The scheme presented above converges with the following parameters.
Theorem 1. Let  > 0. Set µ0 = 7R(cid:107)∇f (0)(cid:107)  x0 = 0  and perform the globalization scheme above
for ρ ≤ 1/7  t = 2  and qk = 1/3+7R(cid:107)xk(cid:107)
1+7R(cid:107)xk(cid:107)   T = (cid:100)log2
number of steps performed in the Phase I  it holds:

(cid:112)1 ∨ (λ−1/R2)(cid:101). Then denoting by K the

K ≤ (cid:98)(3 + 11R(cid:107)x(cid:63)

λ(cid:107)) log(7R(cid:107)∇f (0)(cid:107)/λ)(cid:99) .

fλ((cid:98)x) − fλ(x(cid:63)

λ) ≤  

(cid:16)

O

R(cid:107)x(cid:63)

(cid:17)(cid:17)

(nd log n + dQ2 + Q3)

Note that the theorem above (proven in Appendix C.3) guarantees a solution with error  with K steps
of ANM each performing 2 iterations of approximate linear system solving  plus a ﬁnal step of ANM
which performs T iterations of approximate linear system solving. In case of fi(x) = (cid:96)i(w(cid:62)
i x)  with
i ≤ a  for a > 0  the ﬁnal runtime cost of the proposed scheme to
(cid:96)i : R → R  wi ∈ H with (cid:96)(2)
(cid:16)
achieve precision   when combined with of the methods for approximate linear system solving from
Lemma 1 (i.e. sketching)  is O(Q2 + d) in memory and
λ
R

(cid:16) ¯dfλ log

where ¯dfλ  deﬁned in Lemma 1  measures the effective dimension of the correlation matrix W (cid:62)W
with W = (w1  . . .   wn)(cid:62) ∈ Rn×d  corresponding essentially to the number of eigenvalues of W (cid:62)W
larger than λ/a. In particular note that ¯dfλ ≤ min(n  d  rank(W )  ab2/λ)  with b := maxi (cid:107)wi(cid:107) 
and usually way smaller than such quantities.
Remark 1. The proposed method does not depend on the condition number of the problem L/λ  but
on the term R(cid:107)x(cid:63)
λ in the worst case  but usually way smaller.
For example  it is possible to prove that this term is bounded by an absolute constant not depending
on λ  if at least one minimum for f exists. In the appendix (see Proposition 7)  we show a variant of
this adaptive method which can leverage the regularity of the solution with respect to the Hessian 
i.e.  depending on the smaller quantity R

√
λ(cid:107) which can be in the order of R/

in time  Q = O

λ(cid:107) log

1
λδ

+ log

λ(cid:107)x(cid:63)

λ(cid:107)H

λ) instead of R(cid:107)x(cid:63)
λ(cid:107).

−1
λ (x(cid:63)

(cid:17)

 

R
λ

√

Finally note that it is possible to use qk = q ﬁxed for all the iterations and way smaller than the one
in Thm. 1  depending on some regularity properties of H (see Proposition 8 in Appendix C.2).

6

4 Application to the non-parametric setting: Kernel methods

In supervised learning the goal is to predict well on future data  given the observed training dataset.
Let X be the input space and Y ⊆ Rp be the output space. We consider a probability distribution P
over X × Y generating the data and the goal is to estimate g∗ : X → Y solving the problem

g∗ = arg min
g:X→Y

L(g)  L(g) = E[(cid:96)(g(x)  y)] 

n

1
n

w∈H

λ φ(x) 

(cid:80)n

(5)
for a given loss function (cid:96) : Y × Y → R. Note that P is not known  and accessible only via the
i=1  with n ∈ N  independently sampled from P . A prototypical estimator for g∗ is
dataset (xi  yi)n
i=1 (cid:96)(g(xi)  yi) over a suitable space of
functions G. Given φ : X → H a common choice is to select G as the set of linear functions of φ(x) 

the regularized minimizer of the empirical risk (cid:98)L(g) = 1
that is  G = {w(cid:62)φ(·) | w ∈ H}. Then the regularized minimizer of (cid:98)L  denoted by(cid:98)gλ  corresponds to
(cid:98)gλ(x) = (cid:98)w(cid:62)
Learning theory guarantees how fast(cid:98)gλ converges to the best possible estimator g∗ with respect
to the number of observed examples  in terms of the so called excess risk L((cid:98)gλ) − L(g∗). The

(cid:80)n
i=1 fi(w) + λ(cid:107)w(cid:107)2 

(cid:98)wλ = arg min

following theorem recovers the minimax optimal learning rates for squared loss and extend them to
any generalized self-concordant loss function.
Note on dfλ. In this section  we always denote with dfλ the effective dimension of the problem in
Eq. (5). When the loss belongs to the family of generalized linear models (see Example 1) and if the
model is well-speciﬁed  then dfλ is deﬁned exactly as in Eq. (1) otherwise we need a more reﬁned
deﬁnition (see [23] or Eq. (30) in Appendix D).
Theorem 2 (from [23]  Thm. 4). Let λ > 0  δ ∈ (0  1]. Let (cid:96) be generalized self-concordant with
parameter R > 0 and supx∈X (cid:107)φ(x)(cid:107) ≤ C < ∞. Assume that there exists g∗ minimizing L.
n ≥ C/λ log(δ−1C/λ) the following holds with probability 1 − δ:

Then there exists c0 not depending on n  λ  δ  dfλ  C  g∗  such that if(cid:112)dfλ/n  bλ ≤ λ1/2/R  and

fi(w) = (cid:96)(w(cid:62)φ(xi)  yi).

(6)

L((cid:98)gλ) − L(g∗) ≤ c0

(cid:16) dfλ

n

(cid:17)

+ b2
λ

log(1/δ) 

bλ := λ(cid:107)g∗(cid:107)H

λ (g∗).
−1

(7)

Under standard regularity assumptions of the learning problems [23]  i.e.  (a) the capacity condition
σj(H(g∗)) ≤ Cj−α  for α ≥ 1  C > 0 (i.e.  a decay of eigenvalues σj(H(g∗)) of the Hessian at the
optimum)  and (b) the source condition g∗ = H(g∗)rv  with v ∈ H and r > 0 (i.e.  the control of the
λ ≤ C(cid:48)(cid:48)λ1+2r  leading to
optimal g∗ for a speciﬁc Hessian-dependent norm)  dfλ ≤ C(cid:48)λ−1/α and b2
the following optimal learning rate 
Now we propose an algorithmic scheme to compute efﬁciently an approximation of(cid:98)gλ that achieves

L((cid:98)gλ) − L(g∗) ≤ c1n− 1+2rα

1+α+2rα log(1/δ)  when λ = n−

the same optimal learning rates. First we need to introduce the technique we are going to use.

1+α+2rα .

(8)

α

It consists in suitably selecting {¯x1  . . .   ¯xM} ⊂ {x1  . . .   xn}  with M (cid:28) n
Nyström projection.
and computing ¯gM λ  i.e.  the solution of Eq. (6) over HM = span{φ(¯x1)  . . .   φ(¯xM )} instead of H.
In this case the problem can be reformulated as a problem in RM as

1
n

¯fi(α) + λ(cid:107)α(cid:107)2 

¯gM λ = ¯α(cid:62)

M λT−1v(x) 

¯fλ(α) 

¯f (α) =

i=1

¯αM λ = arg min
α∈RM

(9)
where ¯fi(α) = (cid:96)(v(xi)(cid:62)T−1α  yi) and v(x) ∈ RM   v(x) = (k(x  ¯x1)  . . .   k(x  ¯xM )) with
k(x  x(cid:48)) = φ(x)(cid:62)φ(x(cid:48)) the associated positive-deﬁnite kernel [32]  while T is the upper trian-
gular matrix such that K = T(cid:62)T  with K ∈ RM×M with Kij = k(¯xi  ¯xj). In the next theorem
we characterize the sufﬁcient M to achieve minimax optimal rates  for two standard techniques of
choosing the Nyström points {¯x1  . . .   ¯xM}.
Theorem 3 (Optimal rates for learning with Nyström). Let λ > 0  δ ∈ (0  1]. Assume the conditions
of Thm. 2. Then the excess risk of ¯gM λ is bounded with prob. 1 − 2δ as in Eq. (7) (with c(cid:48)
1 ∝ c1) 
when

(1) Uniform Nyström method [28  29] is used and M ≥ C1/λ log(C2/λδ).
(2) Approximate leverage score method [3  28  29] is used and M ≥ C3 dfλ log(C4/λδ).

Here C  C1  C2  C4 do not depend on λ  n  M  dfλ  δ.

n(cid:88)

7

Thm. 3 generalizes results for learning with Nyström and squared loss [28]  to GSC losses. It is
proved in Thm. 6  in Appendix D.4. As in [28]  Thm. 3 shows that Nyström is a valid technique
for dimensionality reduction. Indeed it is essentially possible to project the learning problem on a
subspace HM of dimension M = O(c/λ) or even as small as M = O(dfλ) and still achieve the
optimal rates of Thm. 2. Now we are ready to introduce our algorithm.

Proposed algorithm. The algorithm conceptually consists in (a) performing a projection step with
Nyström  and (b) solving the resulting optimization problem with the globalization scheme proposed
in Sec. 3 based on ANM in Eq. (3). In particular  we want to avoid to apply explicitly T−1 to each
v(xi) in Eq. (9)  which would require O(nM 2) time. Then we will use the following approximation
technique based only on matrix vector products  so we can just apply T−1 to α at each iteration 
with a total cost proportional only to O(nM + M 2) per iteration. Given α ∇ ¯fλ(α)  we approximate
z∗ = ¯Hλ(α)−1∇ ¯fλ(α)  where ¯Hλ is the Hessian of ¯fλ(α)  with ˜z deﬁned as

˜z = prec-conj-gradt( ¯Hλ(α) ∇ ¯fλ(α)) 

where prec-conj-gradt corresponds to performing t steps of preconditioned conjugate gradi-
ent [19] with preconditioner computed using a subsampling approach for the Hessian among the ones
presented in Sec. 2  in the paragraph starting with Eq. (4). The pseudocode for the whole procedure
is presented in Alg. 1  Appendix E. This technique of approximate linear system solving has been
studied in [29] in the context of empirical risk minimization for squared loss.
Lemma 4 ([29]). Let λ > 0  α  b ∈ RM . The previous method  applied with t = O(log 1/ρ)  outputs
an element of LinApprox( ¯Hλ(α)  b  ρ)  with probability 1 − δ with complexity O((nM + M 2Q +
M 3 + c)t) in time and O(M 2 + n) in space  with Q = O(C1/λ log(C1/λδ))  c = O(1) if uniform
sub-sampling is used or Q = O(C2dfλ log(C1/λδ))  c = O(df2
λ )) if sub-sampling with
leverage scores is used [30].

λ min(n  1

A more complete version of this lemma is shown in Proposition 12 in Appendix D.5.1. We conclude
this section with a result proving the learning properties of the proposed algorithm.
Theorem 4 (Optimal rates for the proposed algorithms). Let λ > 0 and  < λ/R2. Under the
hypotheses of Thm. 3  if we set M as in Thm. 3  Q as in Lemma 4 and setting the globalization
scheme as in Thm. 1  then the proposed algorithm (Alg. 1  Appendix E) ﬁnishes in a ﬁnite number of
newton steps Nns = O(R(cid:107)g∗(cid:107) log(C/λ) + log(C/)) and returns a predictor gQ M λ of the form
gQ M λ = α(cid:62)T−1v(x). With probability at least 1 − δ  this predictor satisﬁes:

(cid:16) dfλ

n

(cid:17)

L(gQ M λ) − L(g∗) ≤ c0

+ b2

λ + 

log(1/δ) 

bλ := λ(cid:107)g∗(cid:107)H

λ (g∗).
−1

(10)

The theorem above (see Proposition 14  Appendix D.6 for exacts quantiﬁcations) shows that the
proposed algorithm is able to achieve the same learning rates of plain empirical risk minimization as
in Thm. 2. The total complexity of the procedure  including the cost of computing the preconditioner 
the selection of the Nyström points via approximate leverage scores and also the computation of the
leverage scores [30] is then

O(cid:0)R(cid:107)g∗(cid:107) log(R2/λ)(cid:0)n dfλ log(Cλ−1δ−1) cX + + df3

λ log3(Cλ−1δ−1) + min(n  C/λ) df2

(cid:1)(cid:1)

λ

λ log2(Cλ−1δ−1)) in space  where cX is the cost of computing the inner product
in time and O(df2
k(x  x(cid:48)) (in the kernel setting assumed when the input space X is X = Rp it is c = O(p)).
As noted in [30]  under the standard regularity assumptions on the learning problem seen above 
λ ≤ dfλ/λ ≤ n when the optimal λ is chosen. So the total computational complexity is
df2

λ·log2(Cλ−1δ−1)) in space.
First note  the fact that due to the statistical properties of the problem the complexity does not depend

O(cid:0)R log(R2/λ) log3(Cλ−1δ−1) (cid:107)g∗(cid:107) · n · dfλ · cX
even implicitly on(cid:112)C/λ  but only on log(C/λ)  so the algorithm runs in essentially O(ndfλ) 
(cid:112)nC/λ) of the accelerated ﬁrst-order methods we develop in Appendix F and
(cid:112)C/λ) of other Newton schemes (see Sec. 1.1). To our knowledge  this is the ﬁrst
complexity only (cid:101)O(ndfλ). This generalizes similar results for squared loss [29  30].

compared to O(dfλ
the O(ndfλ
algorithm to achieve optimal statistical learning rates for generalized self-concordant losses and with

(cid:1) in time  O(df2

8

Figure 1: Training loss and test error as as function of the number of passes on the data for our
algorithm vs. K-SVRG. on the (left) Susy and (right) Higgs data sets.

5 Experiments

The code necessary to reproduce the following experiments is available on GitHub at https:
//github.com/umarteau/Newton-Method-for-GSC-losses-.
We compared the performances of our algorithm for kernel logistic regression on two large scale
classiﬁcation datasets (n ≈ 107)  Higgs and Susy  pre-processed as in [29]. We implemented the
algorithm in pytorch and performed the computations on 1 Tesla P100-PCIE-16GB GPU. For Susy
(n = 5 × 106  p = 18): we used Gaussian kernel with k(x  x(cid:48)) = e−(cid:107)x−x(cid:48)(cid:107)2/(2σ2)  with σ = 5 
which we obtained through a grid search (in [29]  σ = 4 is taken for the ridge regression); M = 104
Nyström centers and a subsampling Q = M for the preconditioner  both obtained with uniform
sampling. Analogously for Higgs (n = 1.1 × 107  p = 28):   we used a Gaussian kernel with σ = 5
and M = 2.5 × 104 and Q = M  using again uniform sampling. To ﬁnd reasonable λ for supervised
learning applications  we cross-validated λ ﬁnding the minimum test error at λ = 10−10 for Susy
and λ = 10−9 for Higgs (see Figs. 2 and 3 in Appendix F) for such values our algorithm and the
competitor achieve an error of 19.5% on the test set for Susy  comparable to the state of the art (19.6%
[29]) and analogously for Higgs (see Appendix F). We then used such λ’s as regularization parameters
and compared our algorithm with a well known accelerated stochastic gradient technique Katyusha
SVRG (K-SVRG) [4]  tailored to our problem using mini batches. In Fig. 1 we show the convergence
of the training loss and classiﬁcation error with respect to the number of passes on the data  of our
algorithm compared to K-SVRG. It is possible to note our algorithm is order of magnitude faster in
achieving convergence  validating empirically the fact that the proposed algorithm scales as O(ndfλ)

in learning settings  while accelerated ﬁrst order methods go as O((n +(cid:112)nL/λ)dfλ). Moreover 

as mentioned in the introduction  this highlights the fact that precise optimization is necessary to
achieve a good performance in terms of test error. Finally  note that since a pass on the data is much
more expensive for K-SVRG than for our second order method (see Appendix F for details)  the
difference in computing time between the second order scheme and K-SVRG is even more in favour
of our second order method (see Figs. 4 and 5 in Appendix F).

Acknowledgments

We acknowledge support from the European Research Council (grant SEQUOIA 724063).

References
[1] Murat A. Erdogdu and Andrea Montanari. Convergence rates of sub-sampled Newton methods.

Technical Report 1508.02810  ArXiv  2015.

[2] Naman Agarwal  Brian Bullins  and Elad Hazan. Second-order stochastic optimization for

machine learning in linear time. J. Mach. Learn. Res.  18(1):4148–4187  January 2017.

[3] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical

guarantees. In Advances in Neural Information Processing Systems  pages 775–783  2015.

9

020406080100120passes over data19.419.619.820.020.220.420.620.821.0classification error104103102101distance to optimumsecond orderK-SVRG20406080100120passes over data27.828.028.228.428.628.829.0classification error106105104103102101distance to optimumsecond orderK-SVRG[4] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In

Proceedings of the Symposium on Theory of Computing  pages 1200–1205  2017.

[5] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathemati-

cal Society  68(3):337–404  1950.

[6] Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics 

4:384–414  2010.

[7] Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for

logistic regression. Journal of Machine Learning Research  15(1):595–627  2014.

[8] Raghu Bollapragada  Richard H. Byrd  and Jorge Nocedal. Exact and inexact subsampled
newton methods for optimization. IMA Journal of Numerical Analysis  39(2):545–578  2018.

[9] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural

Information Processing Systems  pages 161–168  2008.

[10] Léon Bottou  Frank E. Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. Siam Review  60(2):223–311  2018.

[11] Christos Boutsidis and Alex Gittens. Improved matrix algorithms via the subsampled random-
ized hadamard transform. SIAM Journal on Matrix Analysis and Applications  34(3):1301–1340 
2013.

[12] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[13] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Found.

Comput. Math.  7(3):331–368  July 2007.

[14] Aaron Defazio. A simple practical accelerated method for ﬁnite sums. In Advances in Neural

Information Processing Systems  pages 676–684  2016.

[15] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing systems  pages 1646–1654  2014.

[16] Peter Deuﬂhard. Newton Methods for Nonlinear Problems: Afﬁne Invariance and Adaptive

Algorithms. Springer  2011.

[17] Petros Drineas  Michael W Mahoney  Shan Muthukrishnan  and Tamás Sarlós. Faster least

squares approximation. Numerische mathematik  117(2):219–249  2011.

[18] Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney  and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine Learning
Research  13(Dec):3475–3506  2012.

[19] Gene H. Golub and Charles F. Van Loan. Matrix Computations  volume 3. JHU Press  2012.

[20] Robert Gower  Filip Hanzely  Peter Richtárik  and Sebastian U. Stich. Accelerated stochastic ma-
trix inversion: general theory and speeding up BFGS rules for faster second-order optimization.
In Advances in Neural Information Processing Systems  pages 1619–1629  2018.

[21] Sai Praneeth Karimireddy  Sebastian U. Stich  and Martin Jaggi. Global linear convergence of
newton’s method without strong-convexity or lipschitz gradients. CoRR  abs/1806.00413  2018.
URL http://arxiv.org/abs/1806.00413.

[22] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimiza-

tion. In Advances in Neural Information Processing Systems  pages 3384–3392  2015.

[23] Ulysse Marteau-Ferey  Dmitrii Ostrovskii  Francis Bach  and Alessandro Rudi. Beyond least-
squares: Fast rates for regularized empirical risk minimization through self-concordance. In
Proceedings of the Conference on Computational Learning Theory  2019.

10

[24] Arkadii Nemirovskii and Yurii Nesterov.

Interior-point polynomial algorithms in convex

programming. Society for Industrial and Applied Mathematics  1994.

[25] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization
algorithm with linear-quadratic convergence. SIAM Journal on Optimization  27(1):205–245 
2017.

[26] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in Neural Information Processing Systems  pages 1177–1184  2008.

[27] Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled Newton methods. Math.

Program.  174(1-2):293–326  2019.

[28] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström com-
putational regularization. In Advances in Neural Information Processing Systems 28  pages
1657–1665. 2015.

[29] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. FALKON: An optimal large scale
kernel method. In Advances in Neural Information Processing Systems 30  pages 3888–3898.
2017.

[30] Alessandro Rudi  Daniele Calandriello  Luigi Carratino  and Lorenzo Rosasco. On fast leverage
score sampling and optimal learning. In Advances in Neural Information Processing Systems 
pages 5672–5682  2018.

[31] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied

Mathematics  Philadelphia  PA  USA  2nd edition  2003.

[32] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge

University Press  2004.

[33] Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds. Founda-

tions and Trends R(cid:13) in Machine Learning  4(4):267–373  2012.

[34] Christopher K. I. Williams and Matthias Seeger. Using the Nyström method to speed up kernel

machines. In Advances in Neural Information Processing Systems  pages 682–688  2001.

11

,Ulysse Marteau-Ferey
Francis Bach
Alessandro Rudi