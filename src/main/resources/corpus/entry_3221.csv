2017,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls,We propose a rank-k variant of the classical Frank-Wolfe algorithm to solve convex optimization over a trace-norm ball. Our algorithm replaces the top singular-vector computation (1-SVD) in Frank-Wolfe with a top-k singular-vector computation (k-SVD)  which can be done by repeatedly applying 1-SVD k times. Alternatively  our algorithm can be viewed as a rank-k restricted version of projected gradient descent. We show that our algorithm has a linear convergence rate when the objective function is smooth and strongly convex  and the optimal solution has rank at most k. This improves the convergence rate and the total time complexity of the Frank-Wolfe method and its variants.,Linear Convergence of a Frank-Wolfe Type

Algorithm over Trace-Norm Balls∗

Zeyuan Allen-Zhu

Microsoft Research  Redmond

zeyuan@csail.mit.edu

Wei Hu

Princeton University

huwei@cs.princeton.edu

Elad Hazan

Princeton University

ehazan@cs.princeton.edu

Yuanzhi Li

Princeton University

yuanzhil@cs.princeton.edu

Abstract

We propose a rank-k variant of the classical Frank-Wolfe algorithm to solve convex
optimization over a trace-norm ball. Our algorithm replaces the top singular-vector
computation (1-SVD) in Frank-Wolfe with a top-k singular-vector computation
(k-SVD)  which can be done by repeatedly applying 1-SVD k times. Alternatively 
our algorithm can be viewed as a rank-k restricted version of projected gradient
descent. We show that our algorithm has a linear convergence rate when the
objective function is smooth and strongly convex  and the optimal solution has rank
at most k. This improves the convergence rate and the total time complexity of the
Frank-Wolfe method and its variants.

(cid:8)f (X) : (cid:107)X(cid:107)∗ ≤ θ(cid:9)  

Introduction

1
Minimizing a convex matrix function over a trace-norm ball  which is: (recall that the trace norm
(cid:107)X(cid:107)∗ of a matrix X equals the sum of its singular values)

minX∈Rm×n

(1.1)
is an important optimization problem that serves as a convex surrogate to many low-rank machine
learning tasks  including matrix completion [2  10  16]  multiclass classiﬁcation [4]  phase retrieval [3] 
polynomial neural nets [12]  and more. In this paper we assume without loss of generality that θ = 1.
One natural algorithm for Problem (1.1) is projected gradient descent (PGD). In each iteration 
PGD ﬁrst moves X in the direction of the gradient  and then projects it onto the trace-norm ball.
Unfortunately  computing this projection requires the full singular value decomposition (SVD) of the
matrix  which takes O(mn min{m  n}) time in general. This prevents PGD from being efﬁciently
applied to problems with large m and n.
Alternatively  one can use projection-free algorithms. As ﬁrst proposed by Frank and Wolfe [5] 
one can select a search direction (which is usually the gradient direction) and perform a linear
optimization over the constraint set in this direction. In the case of Problem (1.1)  performing linear
optimization over a trace-norm ball amounts to computing the top (left and right) singular vectors
of a matrix  which can be done much faster than full SVD. Therefore  projection-free algorithms
become attractive for convex minimization over trace-norm balls.
Unfortunately  despite its low per-iteration complexity  the Frank-Wolfe (FW) algorithm suffers from
slower convergence rate compared with PGD. When the objective f (X) is smooth  FW requires
O(1/ε) iterations to convergence to an ε-approximate minimizer  and this 1/ε rate is tight even if the
objective is also strongly convex [6]. In contrast  PGD achieves 1/
ε rate if f (X) is smooth (under
Nesterov’s acceleration [14])  and log(1/ε) rate if f (X) is both smooth and strongly convex.

√

∗The full version of this paper can be found on https://arxiv.org/abs/1708.02105.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√

Recently  there were several results to revise the FW method to improve its convergence rate for
strongly-convex functions. The log(1/ε) rate was obtained when the constraint set is a polyhe-
dron [7  11]  and the 1/
ε rate was obtained when the constraint set is strongly convex [8] or is a
spectrahedron [6].
Among these results  the spectrahedron constraint (i.e.  for all positive semideﬁnite matrices X with
Tr(X) = 1) studied by Garber [6] is almost identical to Problem (1.1)  but slightly weaker.2 When
stating the result of Garber [6]  we assume for simplicity that it also applies to Problem (1.1).
Our Question.

In this paper  we propose to study the following general question:

Can we design a “rank-k variant” of Frank-Wolfe to improve the convergence rate?

(That is  in each iteration it computes the top k singular vectors – i.e.  k-SVD – of some matrix.)

k (cid:28) min{n  m} such that a rank-k variant of FW can achieve the convergence rate of PGD?

Our motivation to study the above question can be summarized as follows:
• Since FW computes a 1-SVD and PGD computes a full SVD in each iteration  is there a value
• Since computing k-SVD costs roughly the same (sequential) time as “computing 1-SVD for k
times” (see recent work [1  13]) 3 if using a rank-k variant of FW  can the number of iterations be
reduced by a factor more than k? If so  then we can improve the sequential running time of FW.
• k-SVD can be computed in a more distributed manner than 1-SVD. For instance  using block
Krylov [13]  one can distribute the computation of k-SVD to k machines  each in charge of
independent matrix-vector multiplications. Therefore  it is beneﬁcial to study a rank-k variant of
FW in such settings.

1.1 Our Results
We propose blockFW  a rank-k variant of Frank-Wolfe. Given a convex function f (X) that is β-
smooth  in each iteration t  blockFW performs an update Xt+1 ← Xt + η(Vt − Xt)  where η > 0 is
a constant step size and Vt is a rank-k matrix computed from the k-SVD of (−∇f (Xt) + βηXt). If
k = min{n  m}  blockFW can be shown to coincide with PGD  so it can also be viewed as a rank-k
restricted version of PGD.
Convergence. Suppose f (X) is also α-strongly convex and suppose the optimal solution X∗
of Problem (1.1) has rank k  then we show that blockFW achieves linear convergence: it ﬁnds an
ε-approximate minimizer within O( β

(cid:18) kβ

(cid:19)

ε ) iterations  or equivalently  in
α log 1
1
ε

computations of 1-SVD.

log

α

T = O

We denote by T the number of 1-SVD computations throughout this paper. In contrast 

(cid:17)

(cid:16) β
(cid:16)

ε

(cid:110) β

TFW = O

(cid:0) β

(cid:1)1/4(cid:0) β

(cid:1)3/4√

(cid:0) β

(cid:1)1/2(cid:0) β

(cid:1)1/2

(cid:111)(cid:17)

for Frank-Wolfe

α

min

ε  

TGar = O

for Garber [6].
Above  σmin(X∗) is the minimum non-zero singular value of X∗. Note that σmin(X∗) ≤ (cid:107)X∗(cid:107)∗
rank(X∗) ≤
k .
We note that TGar is always outperformed by min{T  TFW}: ignoring the log(1/ε) factor  we have

σmin(X∗)

k  

α

1

1

ε

ε

• min(cid:8) β
• min(cid:8) β

α

ε   kβ
ε   kβ

α

(cid:9) ≤(cid:0) β
(cid:9) ≤(cid:0) β

α

α

(cid:1)1/4(cid:0) β
(cid:1)1/2(cid:0) β

ε

(cid:1)3/4
(cid:1)1/2

ε

k1/4 <(cid:0) β
k1/2 <(cid:0) β

α

α

(cid:1)1/4(cid:0) β
(cid:1)1/2(cid:0) β

ε

(cid:1)3/4√
(cid:1)1/2

ε

k  and
σmin(X∗).

1

2The the best of our knowledge  given an algorithm that works for spectrahedron  to solve Problem (1.1)  one
has to deﬁne a function g(Y ) over (n + m) × (n + m) matrices  by setting g(Y ) = f (2Y1:m m+1:m+n) [10].
After this transformation  the function g(Y ) is no longer strongly convex  even if f (X) is strongly convex. In
contrast  most algorithms for trace-norm balls  including FW and our later proposed algorithm  work as well for
spectrahedron after minor changes to the analysis.

3Using block Krylov [13]  Lanszos [1]  or SVRG [1]  at least when k is small  the time complexity of
(approximately) computing the top k singular vectors of a matrix is no more than k times the complexity of
(approximately) computing the top singular vector of the same matrix. We refer interested readers to [1] for
details.

2

# rank

algorithm
PGD [14] min{m  n}
accelerated
PGD [14] min{m  n}
Frank-
Wolfe [9]

1

Garber [6]

blockFW

1

k

# iterations

κ log(1/ε)
√
κ log(1/ε)

β
ε

4(cid:0) β
2(cid:0) β

ε

1

(cid:1) 3
(cid:1) 1

4

2

ε

κ
1

κ

√
k   or

1

σmin(X∗)

κ log(1/ε)

time complexity per iteration

O(cid:0)mn min{m  n}(cid:1)
O(cid:0)mn min{m  n}(cid:1)
˜O(cid:0)nnz(∇)(cid:1)
˜O(cid:0)nnz(∇) + (m + n)(cid:1)
k · ˜O(cid:0)nnz(∇) + k(m + n)κ(cid:1)

× min

× min

× min

 

2
ε1/2

(cid:26) (cid:107)∇(cid:107)1/2
(cid:26) (cid:107)∇(cid:107)1/2
(cid:110) ((cid:107)∇(cid:107)2+α)1/2

2
ε1/2

 

ε1/2

(cid:107)∇(cid:107)1/2

2

(σ1(∇)−σ2(∇))1/2

(cid:107)∇(cid:107)1/2

2

(σ1(∇)−σ2(∇))1/2

  κ((cid:107)∇(cid:107)2+α)1/2
α1/2σmin(X∗)

(cid:27)
(cid:27)
(cid:111)

Table 1: Comparison of ﬁrst-order methods to minimize a β-smooth  α-strongly convex function over the
unit-trace norm ball in Rm×n. In the table  k is the rank of X∗  κ = β
α is the condition number 
∇ = ∇f (Xt) is the gradient matrix  nnz(∇) is the complexity to multiply ∇ to a vector  σi(X) is the
i-th largest singular value of X  and σmin(X) is the minimum non-zero singular value of X.

REMARK. The low-rank assumption on X∗ should be reasonable: as we mentioned  in most
applications of Problem (1.1)  the ultimate reason for imposing a trace-norm constraint is to ensure
that the optimal solution is low-rank; otherwise the minimization problem may not be interesting to
solve in the ﬁrst place. Also  the immediate prior work [6] also assumes X∗ to have low rank.
k-SVD Complexity. For theoreticians who are concerned about the time complexity of k-SVD  we
also compare it with the 1-SVD complexity of FW and Garber. If one uses LazySVD [1]4 to compute
k-SVD in each iteration of blockFW  then the per-iteration k-SVD complexity can be bounded by

k · ˜O(cid:0)nnz(∇) + k(m + n)κ(cid:1) × min

(cid:26) ((cid:107)∇(cid:107)2 + α)1/2

(1.2)
α is the condition number of f  ∇ = ∇f (Xt) is the gradient matrix of the current
Above  κ = β
iteration t  nnz(∇) is the complexity to multiply ∇ to a vector  σmin(X∗) is the minimum non-zero
singular value of X∗  and ˜O hides poly-logarithmic factors.
In contrast  if using Lanczos  the 1-SVD complexity for FW and Garber can be bounded as (see [6])

ε1/2

 

.

κ((cid:107)∇(cid:107)2 + α)1/2
α1/2σmin(X∗)

(cid:27)

˜O(cid:0)nnz(∇)(cid:1) × min

(cid:110)(cid:107)∇(cid:107)1/2

(cid:111)

(cid:107)∇(cid:107)1/2

.

 

2

2
ε1/2

(σ1(∇) − σ2(∇))1/2

(1.3)
Above  σ1(∇) and σ2(∇) are the top two singular values of ∇  and the gap σ1(∇) − σ2(∇) can be
as small as zero.
We emphasize that our k-SVD complexity (1.2) can be upper bounded by a quantity that only
depends poly-logarithmically on 1/ε. In contrast  the worst-case 1-SVD complexity (1.3) of FW
and Garber depends on ε−1/2 because the gap σ1 − σ2 can be as small as zero. Therefore  if one
takes this additional ε dependency into consideration for the convergence rate  then blockFW has
rate polylog(1/ε)  but FW and Garber have rates ε−3/2 and ε−1 respectively. The convergence rates
and per-iteration running times of different algorithms for solving Problem (1.1) are summarized
in Table 1.

Practical Implementation. Besides our theoretical results above  we also provide practical sugges-
tions for implementing blockFW. Roughly speaking  one can automatically select a different “good”
rank k for each iteration. This can be done by iteratively ﬁnding the 1st  2nd  3rd  etc.  top singular
vectors of the underlying matrix  and then stop this process whenever the objective decrease is not
worth further increasing the value k. We discuss the details in Section 6.

4In fact  LazySVD is a general framework that says  with a meaningful theoretical support  one can apply
a reasonable 1-SVD algorithm k times in order to compute k-SVD. For simplicity  in this paper  whenever
referring to LazySVD  we mean to apply the Lanczos method k times.

3

2 Preliminaries and Notation
For a positive integer n  we deﬁne [n] := {1  2  . . .   n}. For a matrix A  we denote by (cid:107)A(cid:107)F   (cid:107)A(cid:107)2
and (cid:107)A(cid:107)∗ respectively the Frobenius norm  the spectral norm  and the trace norm of A. We use
(cid:104)· ·(cid:105) to denote the (Euclidean) inner products between vectors  or the (trace) inner products between
matrices (i.e.  (cid:104)A  B(cid:105) = Tr(AB(cid:62))). We denote by σi(A) the i-th largest singular value of a matrix
A  and by σmin(A) the minimum non-zero singular value of A. We use nnz(A) to denote the time
complexity of multiplying matrix A to a vector (which is at most the number of non-zero entries of
A). We deﬁne the (unit) trace-norm ball Bm n in Rm×n as Bm n := {X ∈ Rm×n : (cid:107)X(cid:107)∗ ≤ 1}.
Deﬁnition 2.1. For a differentiable convex function f : K → R over a convex set K ⊆ Rm×n  we
say
• f is β-smooth if f (Y ) ≤ f (X) + (cid:104)∇f (X)  Y − X(cid:105) + β
F for all X  Y ∈ K.
• f is α-strongly convex if f (Y ) ≥ f (X) + (cid:104)∇f (X)  Y − X(cid:105) + α
For Problem (1.1)  we assume f is differentiable  β-smooth  and α-strongly convex over Bm n. We
α the condition number of f  and by X∗ the minimizer of f (X) over the trace-norm
denote by κ = β
ball Bm n. The strong convexity of f (X) implies:
Fact 2.2. f (X) − f (X∗) ≥ α
Proof. The minimality of X∗ implies (cid:104)∇f (X∗)  X − X∗(cid:105) ≥ 0 for all X ∈ K. The fact follows then
(cid:3)
from the α-strong convexity of f.

F for all X  Y ∈ K;

F for all X ∈ K.

2 (cid:107)X − X∗(cid:107)2

2(cid:107)X − Y (cid:107)2

2 (cid:107)X − Y (cid:107)2

The Frank-Wolfe Algorithm. We now quickly review the Frank-Wolfe algorithm (see Algorithm 1)
and its relation to PGD.

Algorithm 1 Frank-Wolfe
Input: Step sizes {ηt}t≥1 (ηt ∈ [0  1])  starting point X1 ∈ Bm n
1: for t = 1  2  . . . do
2:
3: Xt+1 ← Xt + ηt(Vt − Xt)
4: end for

Vt ← argminV ∈Bm n(cid:104)∇f (Xt)  V (cid:105)

(cid:5) by ﬁnding the top left/right singular vectors ut  vt of −∇f (Xt)  and taking Vt = utv(cid:62)
t .

Let ht = f (Xt)− f (X∗) be the approximation error of Xt. The convergence analysis of Algorithm 1
is based on the following relation:
ht+1 = f (Xt + ηt(Vt − Xt)) − f (X∗)
y≤ ht + ηt(cid:104)∇f (Xt)  X∗ − Xt(cid:105) +

x≤ ht + ηt(cid:104)∇f (Xt)  Vt − Xt(cid:105) +
t (cid:107)Vt − Xt(cid:107)2
z≤ (1 − ηt)ht +
η2

F .
(2.1)
Above  inequality x uses the β-smoothness of f  inequality y is due to the choice of Vt in Line 2 
and inequality z follows from the convexity of f. Based on (2.1)  a suitable choice of the step size
ηt = Θ(1/t) gives the convergence rate O(β/ε) for the Frank-Wolfe algorithm.
If f is also α-strongly convex  a linear convergence rate can be achieved if we replace the linear
optimization step (Line 2) in Algorithm 1 with a constrained quadratic minimization:

t (cid:107)Vt − Xt(cid:107)2
η2
t (cid:107)Vt − Xt(cid:107)2
η2

β
2
β
2

β
2

F

F

Vt ← argmin
V ∈Bm n

(cid:104)∇f (Xt)  V − Xt(cid:105) +

ηt(cid:107)V − Xt(cid:107)2

F .

β
2

(2.2)

In fact  if Vt is deﬁned as above  we have the following relation similar to (2.1):

ht+1 ≤ ht + ηt(cid:104)∇f (Xt)  Vt − Xt(cid:105) +
β
2
≤ ht + ηt(cid:104)∇f (Xt)  X∗ − Xt(cid:105) +
β
2

t (cid:107)Vt − Xt(cid:107)2
η2
t (cid:107)X∗ − Xt(cid:107)2
η2

F

F ≤ (1 − ηt + κη2
where the last inequality follows from Fact 2.2. Given (2.3)  we can choose ηt = 1
2κ to obtain a linear
convergence rate because ht+1 ≤ (1 − 1/4κ)ht. This is the main idea behind the projected gradient

t )ht  

(2.3)

4

descent (PGD) method. Unfortunately  optimizing Vt from (2.2) requires a projection operation onto
Bm n  and this further requires a full singular value decomposition of the matrix ∇f (Xt) − βηtXt.
3 A Rank-k Variant of Frank-Wolfe
Our main idea comes from the following simple observation. Suppose we choose ηt = η = 1
all iterations  and suppose rank(X∗) ≤ k. Then we can add a low-rank constraint to Vt in (2.2):

2κ for

Vt ←

argmin

V ∈Bm n  rank(V )≤k

(cid:104)∇f (Xt)  V − Xt(cid:105) +

η(cid:107)V − Xt(cid:107)2

F .

β
2

(3.1)

Under this new choice of Vt  it is obvious that the same inequalities in (2.3) remain to hold  and thus
the linear convergence rate of PGD can be preserved. Let us now discuss how to solve (3.1).
3.1 Solving the Low-Rank Quadratic Minimization (3.1)
Although (3.1) is non-convex  we prove that it can be solved efﬁciently. To achieve this  we ﬁrst show
that Vt is in the span of the top k singular vectors of βηXt − ∇f (Xt).

Lemma 3.1. The minimizer Vt of (3.1) can be written as Vt = (cid:80)k
can perform k-SVD on At to compute {(ui  vi)}i∈[k]  plug the expression Vt =(cid:80)k
to minimizing −(cid:80)k
a1  . . .   ak ≥ 0  (cid:107)a(cid:107)1 ≤ 1(cid:9)  which is the same as projecting the vector 1

i   where a1  . . .   ak
are nonnegative scalars  and (ui  vi) is the pair of the left and right singular vectors of At :=
βηXt − ∇f (Xt) corresponding to its i-th largest singular value.
The proof of Lemma 3.1 is given in the full version of this paper. Now  owing to Lemma 3.1  we
into
the objective of (3.1)  and then search for the optimal values {ai}i∈[k]. The last step is equivalent

i Atvi) over the simplex ∆ :=(cid:8)a ∈ Rk :

i (where σi = u(cid:62)

2 η(cid:80)k

i=1 aiuiv(cid:62)

i=1 aiuiv(cid:62)

i=1 σiai + β

βη (σ1  . . .   σk) onto the

i=1 a2

i

simplex ∆. It can be easily solved in O(k log k) time (see for instance the applications in [15]).
3.2 Our Algorithm and Its Convergence
We summarize our algorithm in Algorithm 2 and call it blockFW.

Algorithm 2 blockFW
Input: Rank parameter k  starting point X1 = 0
1: η ← 1
2κ.
2: for t = 1  2  . . . do
3: At ← βηXt − ∇f (Xt)
4:

(u1  v1  . . .   uk  vk) ← k-SVD(At)
a ← argmina∈Rk a≥0 (cid:107)a(cid:107)1≤1 (cid:107)a − 1

βη σ(cid:107)2

Vt ←(cid:80)k

5:
6:
7: Xt+1 ← Xt + η(Vt − Xt)
8: end for

i=1 aiuiv(cid:62)

i

(cid:5) (ui  vi) is the i-th largest pair of left/right singular vectors of At
i Atvi)k

(cid:5) where σ := (u(cid:62)

i=1

F be the objective function in (3.1) 
t = gt(X∗). Given parameters γ ≥ 0 and ε ≥ 0  a feasible solution V to (3.1) is called

Since the state-of-the-art algorithms for k-SVD are iterative methods  which in theory can only give
approximate solutions  we now study the convergence of blockFW given approximate k-SVD solvers.
We introduce the following notion of an approximate solution to the low-rank quadratic minimization
problem (3.1).
Deﬁnition 3.2. Let gt(V ) = (cid:104)∇f (Xt)  V − Xt(cid:105) + β
and let g∗
(γ  ε)-approximate if it satisﬁes g(V ) ≤ (1 − γ)g∗
Note that the above multiplicative-additive deﬁnition makes sense because g∗
Fact 3.3.
−(1 − κη)ht = − ht
The next theorem gives the linear convergence of blockFW under the above approximate solutions to
(3.1). Its proof is simple and uses a variant of (2.3) (see the full version of this paper).

If rank(X∗) ≤ k  for our choice of step size η = 1

2 η(cid:107)V − Xt(cid:107)2
t + ε.

t ≤ 0:
2κ   we have g∗

2 ≤ 0 according to (2.3).

t = gt(X∗) ≤

5

Theorem 3.4. Suppose rank(X∗) ≤ k and ε > 0. If each Vt computed in blockFW is a ( 1
approximate solution to (3.1)  then for every t  the error ht = f (Xt) − f (X∗) satisﬁes

2   ε

8 )-

ht ≤(cid:0)1 − 1

8κ

(cid:1)t−1

h1 + ε

2 .

2   ε

ε ) iterations to achieve the target error ht ≤ ε.

8 )-approximate solution Vt to (3.1)  which we study in Section 4.

As a consequence  it takes O(κ log h1
Based on Theorem 3.4  the per-iteration running time of blockFW is dominated by the time necessary
to produce a ( 1
4 Per-Iteration Running Time Analysis
In this section  we study the running time necessary to produce a ( 1
2   ε)-approximate solution Vt
to (3.1). In particular  we wish to show a running time that depends only poly-logarithmically on 1/ε.
The reason is that  since we are concerning about the linear convergence rate (i.e.  log(1/ε)) in this
paper  it is not meaningful to have a per-iteration complexity that scales polynomially with 1/ε.
Remark 4.1. To the best of our knowledge  the Frank-Wolfe method and Garber’s method [6] have
their worst-case per-iteration complexities scaling polynomially with 1/ε. In theory  this also slows
down their overall performance in terms of the dependency on 1/ε.
4.1 Step 1: The Necessary k-SVD Accuracy
We ﬁrst show that if the k-SVD in Line 4 of blockFW is solved sufﬁciently accurate  then Vt obtained
in Line 6 will be a sufﬁciently good approximate solution to (3.1). For notational simplicity  in this
section we denote Gt := (cid:107)∇f (Xt)(cid:107)2 + α  and we let k∗ = rank(X∗) ≤ k.
Lemma 4.2. Suppose γ ∈ [0  1] and ε ≥ 0.
u1  v1  . . .   uk  vk returned by k-SVD in Line 4 satisfy u(cid:62)

In each iteration t of blockFW  if the vectors
i Atvi ≥ (1 − γ)σi(At) − ε for all i ∈ [k∗] 

+ 2(cid:1)γ  ε(cid:1)-approximate to (3.1).

i obtained in Line 6 is(cid:0)(cid:0) 6Gt

then Vt =(cid:80)k

i=1 aiuiv(cid:62)

ht

The proof of Lemma 4.2 is given in the full version of this paper  and is based on our earlier
characterization Lemma 3.1.
4.2 Step 2: The Time Complexity of k-SVD
We recall the following complexity statement for k-SVD:
Theorem 4.3 ([1]). The running time to compute the k-SVD of A ∈ Rm×n using LazySVD is5

(cid:16) k·nnz(A)+k2(m+n)

(cid:17)

√

gap

.

or

˜O

In the former case  we can have u(cid:62)

0  σk∗ (A)−σk∗+1(A)

σk∗ (A)

i Avi ≥ (1 − γ)σi(A) for all i ∈ [k]; in the latter case  if
i Avi ≥ σi(A) − ε for all

for some k∗ ∈ [k]  then we can guarantee u(cid:62)

1

i ∈ [k∗].
The First Attempt. Recall that we need a ( 1
it sufﬁces to obtain a (1 − γ)-multiplicative approximation to the k-SVD of At (i.e.  u(cid:62)
(1 − γ)σi(At) for all i ∈ [k])  as long as γ ≤

2   ε)-approximate solution to (3.1). Using Lemma 4.2 
i Atvi ≥
12Gt/ht+4. Therefore  we can directly apply the ﬁrst

(cid:1). However  when ht is very small  this running

running time in Theorem 4.3: ˜O(cid:0) k·nnz(At)+k2(m+n)
since (cid:107)At(cid:107)2 =(cid:13)(cid:13) α

2 Xt − ∇f (Xt)(cid:13)(cid:13)2 ≤ α

i Atvi ≥ σi(At) − ε

2 + (cid:107)∇f (Xt)(cid:107)2 ≤ Gt  from u(cid:62)

time can be unbounded. In that case  we observe that γ = ε
Gt

(independent of ht) also sufﬁces:
i Atvi ≥ (1 − ε/Gt)σi(At)
(cid:107)At(cid:107)2 ≥ σi(At) − ε; then according to
2   ε)-approximation.
) in Claim 4.5; the running time depends polynomially

we have u(cid:62)
Lemma 4.2 we can obtain (0  ε)-approximation to (3.1)  which is stronger than ( 1
We summarize this running time (using γ = ε
Gt
ε .
on 1
The Second Attempt. To make our linear convergence rate (i.e.  the log(1/ε) rate) meaningful  we
want the k-SVD running time to depend poly-logarithmically on 1/ε. Therefore  when ht is small 
we wish to instead apply the second running time in Theorem 4.3.

σi(At) ≥ σi(At) − ε

√

Gt

Gt

γ

(cid:17)

(cid:16) k·nnz(A)+k2(m+n)
(cid:105)

√

γ

˜O

gap ∈(cid:16)

5The ﬁrst is known as the gap-free result because it does not depend on the gap between any two singular
values. The second is known as the gap-dependent result  and it requires a k×k full SVD after the k approximate
singular vectors are computed one by one. The ˜O notation hides poly-log factors in 1/ε  1/γ  m  n  and 1/gap.

6

Recall that X∗ has rank k∗ so σk∗ (X∗) − σk∗+1(X∗) = σmin(X∗). We can show that this implies
2 X∗ − ∇f (X∗) also has a large gap σk∗ (A∗) − σk∗+1(A∗). Now  according to Fact 2.2 
A∗ := α
2 Xt − ∇f (Xt) is also close
when ht is small  Xt and X∗ are sufﬁciently close. This means At = α
to A∗  and thus has a large gap σk∗ (At) − σk∗+1(At). Then we can apply the second running time
in Theorem 4.3.
4.2.1 Formal Running Time Statements
Fact 4.4. We can store Xt as a decomposition into at most rank(Xt) ≤ kt rank-1 components.6
2 Xt − ∇f (Xt)  we have nnz(At) ≤ nnz(∇f (Xt)) + (m + n)rank(Xt) ≤
Therefore  for At = α
nnz(∇f (Xt)) + (m + n)kt.
If we always use the ﬁrst running time in Theorem 4.3  then Fact 4.4 implies:

Claim 4.5. The k-SVD computation in the t-th iteration of blockFW can be implemented in ˜O(cid:0)(cid:0)k ·
nnz(∇f (Xt)) + k2(m + n)t(cid:1)(cid:112)Gt/ε(cid:1) time.
becomes ˜O(cid:0)k · nnz(∇f (Xt))(cid:112)Gt/ε(cid:1)  which roughly equals k-times the 1-SVD running time
˜O(cid:0)nnz(∇)(cid:112)(cid:107)∇(cid:107)2/ε)(cid:1) of FW and Garber [6]. Since in practice  it sufﬁces to run blockFW and FW

Remark 4.6. As long as (m + n)kt ≤ nnz(∇f (Xt))  the k-SVD running time in Claim 4.5

for a few hundred 1-SVD computations  the relation (m + n)kt ≤ nnz(∇f (Xt)) is often satisﬁed.
If  as discussed above  we apply the ﬁrst running time in Theorem 4.3 only for large ht  and apply
the second running time in Theorem 4.3 for small ht  then we obtain the following theorem whose
proof is given in the full version of this paper.
Theorem 4.7. The k-SVD comuputation in the t-th iteration of blockFW can be implemented in
˜O

k · nnz(∇f (Xt)) + k2(m + n)t

(cid:17) κ

(cid:16)(cid:16)

time.

(cid:17)

√
Gt/α
σmin(X∗)

Remark 4.8. Since according to Theorem 3.4 we only need to run blockFW for O(κ log(1/ε))
iterations  we can plug t = O(κ log(1/ε)) into Claim 4.5 and Theorem 4.7  and obtain the running
time presented in (1.2). The per-iteration running time of blockFW depends poly-logarithmically on
1/ε. In contrast  the per-iteration running times of Garber [6] and FW depend polynomially on 1/ε 
making their total running times even worse in terms of dependency on 1/ε.
5 Maintaining Low-Rank Iterates
One of the main reasons to impose trace-norm constraints is to produce low-rank solutions. However 
the rank of iterate Xt in our algorithm blockFW can be as large as kt  which is much larger than k 
the rank of the optimal solution X∗. In this section  we show that by adding a simple modiﬁcation
to blockFW  we can make sure the rank of Xt is O(kκ log κ) in all iterations t  without hurting the
convergence rate much.
We modify blockFW as follows. Whenever t − 1 is a multiple of S = (cid:100)8κ(log κ + 1)(cid:101)  we compute
(note that this is the same as setting η = 1 in (3.1))

Wt ←

argmin

W∈Bm n  rank(W )≤k

(cid:104)∇f (Xt)  W − Xt(cid:105) +

(cid:107)W − Xt(cid:107)2

F  

β
2

and let the next iterate Xt+1 be Wt. In all other iterations the algorithm is unchanged. After this
change  the function value f (Xt+1) may be greater than f (Xt)  but can be bounded as follows:
Lemma 5.1. Suppose rank(X∗) ≤ k. Then we have f (Wt) − f (X∗) ≤ κht.

Proof. We have the following relation similar to (2.3):

f (Wt) − f (X∗) ≤ ht + (cid:104)∇f (Xt)  Wt − Xt(cid:105) +
≤ ht + (cid:104)∇f (Xt)  X∗ − Xt(cid:105) +
≤ ht − ht +

ht = κht .

β
2
β
2

β
2

· 2
α

(cid:107)Wt − Xt(cid:107)2
(cid:107)X∗ − Xt(cid:107)2

F

F

(cid:3)

6 In Section 5  we show how to ensure that rank(Xt) is always O(kκ log κ)  a quantity independent of t.

7

From Theorem 3.4 we know that hS+1 ≤ (1 − 1
2 ≤
eκ h1 + ε/2. Therefore  after setting XS+2 = WS+1  we still have hS+2 ≤
e−(log κ+1)h1 + ε
2 (according to Lemma 5.1). Continuing this analysis (letting the κε here be the “new
1
e h1 + κε
ε”)  we know that this modiﬁed version of blockFW converges to an ε-approximate minimizer in

8κ )8κ(log κ+1)h1 + ε

2 ≤ (1 − 1

8κ )Sh1 + ε

2 = 1

O(cid:0)κ log κ · log h1

(cid:1) iterations.

ε

.

2

Pk =

Remark 5.2. Since in each iteration the rank of Xt is increased by at most k  if we do the modiﬁed
step every S = O(κ log κ) iterations  we have that throughout the algorithm  rank(Xt) is never more
than O(kκ log κ). Furthermore we can always store Xt using O(kκ log κ) vectors  instead of storing
all the singular vectors obtained in previous iterations.
6 Preliminary Empirical Evaluation
We conclude this paper with some preliminary experiments to test the performance of blockFW. We
ﬁrst recall two machine learning tasks that fall into Problem (1.1).
Matrix Completion. Suppose there is an unknown matrix M ∈ Rm×n close to low-rank  and we
observe a subset Ω of its entries – that is  we observe Mi j for every (i  j) ∈ Ω. (Think of Mi j as
user i’s rating of movie j.) One can recover M by solving the following convex program:

(i j)∈Ω(Xi j − Mi j)2 | (cid:107)X(cid:107)∗ ≤ θ(cid:9) .
(cid:80)

minX∈Rm×n

(cid:8) 1

(cid:8) 1

j=1 aj(w(cid:62)

(cid:110)
x (cid:55)→(cid:80)k
If we write A =(cid:80)k

(6.1)
Although Problem (6.1) is not strongly convex  our experiments show the effectiveness of blockFW
on this problem.
Polynomial Neural Networks. Polynomial networks are neural networks with quadratic activation
function σ(a) = a2. Livni et al. [12] showed that such networks can express any function computed
by a Turing machine  similar to networks with ReLU or sigmoid activations. Following [12]  we
consider the class of 2-layer polynomial networks with inputs from Rd and k hidden neurons:

j x)2(cid:12)(cid:12)(cid:12)∀j ∈ [k]  wj ∈ Rd (cid:107)wj(cid:107)2 = 1(cid:86) a ∈ Rk(cid:111)

i=1 ajwjw(cid:62)

j   we have the following equivalent formulation:

Pk =(cid:8)x (cid:55)→ x(cid:62)Ax(cid:12)(cid:12) A ∈ Rd×d  rank(A) ≤ k(cid:9) .
i Axi − yi)2(cid:12)(cid:12)(cid:107)A(cid:107)∗ ≤ θ(cid:9) .

(cid:80)N
i=1(x(cid:62)

(cid:80)N
i=1(x(cid:62)

Therefore  if replace the hard rank constraint with trace norm (cid:107)A(cid:107)∗ ≤ θ  the task of empirical risk
minimization (ERM) given training data {(x1  y1)  . . .   (xN   yN )} ⊂ Rd × R can be formulated as7
(6.2)
minA∈Rd×d
i Axi − yi)2 is convex in A  the above problem falls into Problem (1.1).

Since f (A) = 1
2
Again  this objective f (A) might not be strongly convex  but we still perform experiments on it.
6.1 Preliminary Evaluation 1: Matrix Completion on Synthetic Data
We consider the following synthetic experiment for matrix completion. We generate a random
rank-10 matrix in dimension 1000 × 1000  plus some small noise. We include each entry into Ω with
probability 1/2. We scale M to (cid:107)M(cid:107)∗ = 10000  so we set θ = 10000 in (6.1).
We compare blockFW with FW and Garber [6]. When implementing the three algorithms  we use
exact line search. For Garber’s algorithm  we tune its parameter ηt = c
t with different constant values
c  and then exactly search for the optimum ˜ηt. When implementing blockFW  we use k = 10 and
η = 0.2. We use the MATLAB built-in solver for 1-SVD and k-SVD.
In Figure 1(a)  we compare the numbers of 1-SVD computations for the three algorithms. The plot
conﬁrms that it sufﬁces to apply a rank-k variant FW in order to achieve linear convergence.
6.2 Auto Selection of k
In practice  it is often unrealistic to know k in advance. Although one can simultaneously try
k = 1  2  4  8  . . . and output the best possible solution  this can be unpleasant to work with. We
propose the following modiﬁcation to blockFW which automatically chooses k.
In each iteration t  we ﬁrst run 1-SVD and compute the objective decrease  denoted by d1 ≥ 0. Now 
given any approximate k-SVD decomposition of the matrix At = βηXt − ∇f (Xt)  we can compute
its (k + 1)-SVD using one additional 1-SVD computation according to the LazySVD framework [1].

2

7We consider square loss for simplicity. It can be any loss function (cid:96)(x(cid:62)

i Axi  yi) convex in its ﬁrst argument.

8

(a) matrix completion on synthetic
data

(b) matrix completion
on MOVIELENS1M  θ = 10000

(c) polynomial neural network
on MNIST  θ = 0.03

Figure 1: Partial experimental results. The full 6 plots for MOVIELENS and 3 plots for MNIST are included in

the full version of this paper.

k+1 < dk

We compute the new objective decrease dk+1. We stop this process and move to the next iteration t+1
whenever dk+1
k . In other words  we stop whenever it “appears” not worth further increasing k.
We count this iteration t as using k + 1 computations of 1-SVD.
All the experiments on real-life datasets are performed using this above auto-k process.
6.3 Preliminary Evaluation 2: Matrix Completion on MOVIELENS
We study the same experiment in Garber [6]  the matrix completion Problem (6.1) on datasets
MOVIELENS100K (m = 943  n = 1862 and |Ω| = 105) and MOVIELENS1M (m = 6040  n =
3952 and |Ω| ≈ 106). In the second dataset  following [6]  we further subsample Ω so it contains
about half of the original entries. For each dataset  we run FW  Garber  and blockFW with three
different choices of θ.8 We present the six plots side-by-side in the full version of this paper.
We observe that when θ is large  there is no signiﬁcant advantage for using blockFW. This is because
the rank of the optimal solution X∗ is also high for large θ. In contrast  when θ is small (so X∗ is of
low rank)  as demonstrated for instance by Figure 1(b)  it is indeed beneﬁcial to apply blockFW.
6.4 Preliminary Evaluation 3: Polynomial Neural Network on MNIST
We use the 2-layer neural network Problem (6.2) to train a binary classiﬁer on the MNIST dataset of
handwritten digits  where the goal is to distinguish images of digit “0” from images of other digits.
The training set contains N = 60000 examples each of dimension d = 28× 28 = 784. We set yi = 1
if that example belongs to digit “0” and yi = 0 otherwise. We divide the original grey levels by 256
so xi ∈ [0  1]d. We again try three different values of θ  and compare FW  Garber  and blockFW.9
We present the three plots side-by-side in the full version of this paper.
The performance of our algorithm is comparable to FW and Garber for large θ  but as demonstrated
for instance by Figure 1(c)  when θ is small so rank(X∗) is small  it is beneﬁcial to use blockFW.
7 Conclusion
In this paper  we develop a rank-k variant of Frank-Wolfe for Problem (1.1) and show that: (1)
it converges in log(1/ε) rate for smooth and strongly convex functions  and (2) its per-iteration
complexity scales with polylog(1/ε). Preliminary experiments suggest that the value k can also be
automatically selected  and our algorithm outperforms FW and Garber [6] when X∗ is of relatively
smaller rank.
We hope more rank-k variants of Frank-Wolfe can be developed in the future.
Acknowledgments
Elad Hazan was supported by NSF grant 1523815 and a Google research award. The authors would
like to thank Dan Garber for sharing his code for [6].

8We perform exact line search for all algorithms. For Garber [6]  we tune the best ηt = c

t and exactly
search for the optimal ˜ηt. For blockFW  we let k be chosen automatically and choose η = 0.01 for all the six
experiments.

9We perform exact line search for all algorithms. For Garber [6]  we tune the best ηt = c

t and exactly search
for the optimal ˜ηt. For blockFW  we let k be chosen automatically and choose η = 0.0005 for all the three
experiments.

9

# 1-SVD computations020406080100Log(error)-2-1012345678FWGarberThis paper# 1-SVD computations050100150200Log(error)1234567FWGarberThis paper# 1-SVD computations0100200300400500Log(error)-1-0.500.511.522.533.54FWGarberThis paperReferences
[1] Zeyuan Allen-Zhu and Yuanzhi Li. LazySVD: Even faster SVD decomposition yet without

agonizing pain. In NIPS  pages 974–982  2016.

[2] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.

Communications of the ACM  55(6):111–119  2012.

[3] Emmanuel J Candes  Yonina C Eldar  Thomas Strohmer  and Vladislav Voroninski. Phase

retrieval via matrix completion. SIAM review  57(2):225–251  2015.

[4] Miroslav Dudik  Zaid Harchaoui  and Jérôme Malick. Lifted coordinate descent for learning

with trace-norm regularization. In AISTATS  pages 327–336  2012.

[5] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research

logistics quarterly  3(1-2):95–110  1956.

[6] Dan Garber. Faster projection-free convex optimization over the spectrahedron. In NIPS  pages

874–882  2016.

[7] Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with

applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666  2013.

[8] Dan Garber and Elad Hazan. Faster rates for the frank-wolfe method over strongly-convex sets.

In ICML  pages 541–549  2015.

[9] Elad Hazan. Sparse approximate solutions to semideﬁnite programs.
Symposium on Theoretical Informatics  pages 306–316. Springer  2008.

In Latin American

[10] Martin Jaggi and Marek Sulovský. A simple algorithm for nuclear norm regularized problems.

In ICML  pages 471–478  2010.

[11] Simon Lacoste-Julien and Martin Jaggi. An afﬁne invariant linear convergence analysis for

frank-wolfe algorithms. arXiv preprint arXiv:1312.7864  2013.

[12] Roi Livni  Shai Shalev-Shwartz  and Ohad Shamir. On the computational efﬁciency of training

neural networks. In NIPS  pages 855–863  2014.

[13] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and

faster approximate singular value decomposition. In NIPS  pages 1396–1404  2015.

[14] Yurii Nesterov.

Introductory Lectures on Convex Programming Volume: A Basic course 

volume I. Kluwer Academic Publishers  2004.

[15] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  December 2005.

[16] Shai Shalev-Shwartz  Alon Gonen  and Ohad Shamir. Large-scale convex minimization with a

low-rank constraint. arXiv preprint arXiv:1106.1622  2011.

10

,Zeyuan Allen-Zhu
Elad Hazan
Wei Hu
Yuanzhi Li
Marek Wydmuch
Kalina Jasinska
Mikhail Kuznetsov
Róbert Busa-Fekete
Krzysztof Dembczynski