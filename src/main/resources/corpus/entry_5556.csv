2013,Thompson Sampling for 1-Dimensional Exponential Family Bandits,Thompson Sampling has been demonstrated in many complex bandit models  however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for $1$-dimensional exponential family bandits. Our proof builds on previous work  but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed  including heavy-tailed exponential families.,Thompson Sampling for 1-Dimensional Exponential

Family Bandits

Nathaniel Korda

INRIA Lille - Nord Europe  Team SequeL

nathaniel.korda@inria.fr

Emilie Kaufmann

Institut Mines-Telecom; Telecom ParisTech
kaufmann@telecom-paristech.fr

Remi Munos INRIA Lille - Nord Europe  Team SequeL

remi.munos@inria.fr

Abstract

Thompson Sampling has been demonstrated in many complex bandit models 
however the theoretical guarantees available for the parametric multi-armed bandit
are still limited to the Bernoulli case. Here we extend them by proving asymptotic
optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential
family bandits. Our proof builds on previous work  but also makes extensive use
of closed forms for Kullback-Leibler divergence and Fisher information (through
the Jeffreys prior) available in an exponential family. This allow us to give a ﬁnite
time exponential concentration inequality for posterior distributions on exponen-
tial families that may be of interest in its own right. Moreover our analysis covers
some distributions for which no optimistic algorithm has yet been proposed  in-
cluding heavy-tailed exponential families.

1

Introduction

K-armed bandit problems provide an elementary model for exploration-exploitation tradeoffs found
at the heart of many online learning problems. In such problems  an agent is presented with K
distributions (also called arms  or actions) {pa}K
a=1  from which she draws samples interpreted as
rewards she wants to maximize. This objective induces a trade-off between choosing to sample a
distribution that has already yielded high rewards  and choosing to sample a relatively unexplored
distribution at the risk of loosing rewards in the short term. Here we make the assumption that
the distributions  pa  belong to a parametric family of distributions P = {p(· | θ)  θ ∈ Θ} where
Θ ⊂ R. The bandit model is described by a parameter θ0 = (θ1  . . .   θK) such that pa = p(· | θa).
We introduce the mean function µ(θ) = EX∼p(·|θ)[X]  and the optimal arm θ∗ = θa∗ where a∗ =
argmaxa µ(θa).
An algorithm  A  for a K-armed bandit problem is a (possibly randomised) method for choosing
which arm at to sample from at time t  given a history of previous arm choices and obtained rewards 
Ht−1 := ((as  xs))t−1
s=1: each reward xs is drawn from the distribution pas. The agent’s goal is to
design an algorithm with low regret:

(cid:34) t(cid:88)

(cid:35)

R(A  t) = R(A  t)(θ) := tµ(θ∗) − EA

xs

.

This quantity measures the expected performance of algorithm A compared to the expected perfor-
mance of an optimal algorithm given knowledge of the reward distributions  i.e. sampling always
from the distribution with the highest expectation.

s=1

1

Since the early 2000s the “optimisim in the face of uncertainty” heuristic has been a popular ap-
proach to this problem  providing both simplicity of implementation and ﬁnite-time upper bounds
on the regret (e.g. [4  7]). However in the last two years there has been renewed interest in the
Thompson Sampling heuristic (TS). While this heuristic was ﬁrst put forward to solve bandit prob-
lems eighty years ago in [15]  it was not until recently that theoretical analyses of its performance
were achieved [1  2  11  13]. In this paper we take a major step towards generalising these analyses
to the same level of generality already achieved for “optimistic” algorithms.

Thompson Sampling Unlike optimistic algorithms which are often based on conﬁdence intervals 
the Thompson Sampling algorithm  denoted by Aπ0 uses Bayesian tools and puts a prior distribution
πa 0 = π0 on each parameter θa. A posterior distribution  πa t  is then maintained according to the
rewards observed in Ht−1. At each time a sample θa t is drawn from each posterior πa t and then
the algorithm chooses to sample at = arg maxa∈{1 ... K}{µ(θa t)}. Note that actions are sampled
according to their posterior probabilities of being optimal.

Our contributions TS has proved to have impressive empirical performances  very close to those
of state of the art algorithms such as DMED and KL-UCB [11  9  7]. Furthermore recent works
[11  2] have shown that in the special case where each pa is a Bernoulli distribution B(θa)  TS using
a uniform prior over the arms is asymptotically optimal in the sense that it achieves the asymptotic
lower bound on the regret provided by Lai and Robbins in [12] (that holds for univariate parametric
bandits). As explained in [1  2]  Thompson Sampling with uniform prior for Bernoulli rewards
can be slightly adapted to deal with bounded rewards. However  there is no notion of asymptotic
optimality for this non-parametric family of rewards. In this paper  we extend the optimality property
that holds for Bernoulli distributions to more general families of parametric rewards  namely 1-
dimensional exponential families if the algorithm uses the Jeffreys prior:
Theorem 1. Suppose that the reward distributions belong to a 1-dimensional canonical exponential
family and let πJ denote the associated Jeffreys prior. Then 

R(AπJ   T )

µ(θa∗ ) − µ(θa)

 

(1)

K(cid:88)

a=1

=

ln T

K(θa  θa∗ )

lim
T→∞
θ) is the Kullback-Leibler divergence between pθ and p(cid:48)
θ.

where K(θ  θ(cid:48)) := KL(pθ  p(cid:48)
This theorem follows directly from Theorem 2. In the proof of this result we provide in Theorem
4 a ﬁnite-time  exponential concentration bound for posterior distributions of exponential family
random variables  something that to the best of our knowledge is new to the literature and of interest
in its own right. Our proof also exploits the connection between the Jeffreys prior  Fisher information
and the Kullback-Leibler divergence in exponential families.

Thompson Sampling. [2] establishes that R(AπU   T ) = O((cid:112)KT ln(T )) for Thompson Sampling

Related Work Another line of recent work has focused on distribution-independent bounds for

for bounded rewards (with the classic uniform prior πU on the underlying Bernoulli parameter). [14]
go beyond the Bernoulli model  and give an upper bound on the Bayes risk (i.e. the regret averaged
over the prior) independent of the prior distribution. For the parametric multi-armed bandit with K
arms described above  their result states that the regret of Thompson Sampling using a prior π0 is
not too big when averaged over this same prior:

[R(Aπ0   T )(θ)] ≤ 4 + K + 4(cid:112)KT log(T ).

E
θ∼π

⊗K
0

KT . In our paper  we rather
Building on the same ideas  [6] have improved this upper bound to 14
see the prior used by Thompson Sampling as a tool  and we want therefore to derive regret bounds
for any given problem parametrized by θ that depend on this parameter.
[14] also use Thompson Sampling in more general models  like the linear bandit model. Their result
is a bound on the Bayes risk that does not depend on the prior  whereas [3] gives a ﬁrst bound on
the regret in this model. Linear bandits consider a possibly inﬁnite number of arms whose mean
rewards are linearly related by a single  unknown coefﬁcient vector. Once again  the analysis in
[3] encounters the problem of describing the concentration of posterior distributions. However by
using a conjugate normal prior  they can employ explicit concentration bounds available for Normal
distributions to complete their argument.

√

2

Paper Structure
In Section 2 we describe important features of the one-dimensional canonical
exponential families we consider  including closed-form expression for KL-divergences and the
Jeffreys’ prior. Section 3 gives statements of the main results  and provides the proof of the regret
bound. Section 4 proves the posterior concentration result used in the proof of the regret bound.

2 Exponential Families and the Jeffreys Prior

A distribution is said to belong to a one-dimensional canonical exponential family if it has a density
with respect to some reference measure ν of the form:

(2)
where θ ∈ Θ ⊂ R. T and A are some ﬁxed functions that characterize the exponential family

and F (θ) = log(cid:0)(cid:82) A(x) exp [T (x)θ] dν(x)(cid:1). Θ is called the parameter space  T (x) the sufﬁcient

p(x | θ) = A(x) exp(T (x)θ − F (θ)) 

statistic  and F (θ) the normalisation function. We make the classic assumption that F is twice
differentiable with a continuous second derivative. It is well known [17] that:
VarX|θ[T (X)] = F (cid:48)(cid:48)(θ)

EX|θ(T (X)) = F (cid:48)(θ)

and

showing in particular that F is strictly convex. The mean function µ is differentiable and stricly
increasing  since we can show that

µ(cid:48)(θ) = CovX|θ(X  T (X)) > 0.

In particular  this shows that µ is one-to-one in θ.

KL-divergence in Exponential Families
In an exponential family  a direct computation shows
that the Kullback-Leibler divergence can be expressed as a Bregman divergence of the normalisation
function  F:

K(θ  θ(cid:48)) = DB

F (θ(cid:48)  θ) := F (θ(cid:48)) − [F (θ) + F (cid:48)(θ)(θ(cid:48) − θ)] .

(3)

Jeffreys prior in Exponential Families
In the Bayesian literature  a special “non-informative”
prior  introduced by Jeffreys in [10]  is sometimes considered. This prior  called the Jeffreys prior 
is invariant under re-parametrisation of the parameter space  and it can be shown to be proportional
to the square-root of the Fisher information I(θ). In the special case of the canonical exponential
family  the Fisher information takes the form I(θ) = F (cid:48)(cid:48)(θ)  hence the Jeffreys prior for the model
(2) is

Under the Jeffreys prior  the posterior on θ after n observations is given by

p(θ|y1  . . . yn) ∝(cid:112)F (cid:48)(cid:48)(θ) exp
When(cid:82)
are not proper: the prior is called improper if(cid:82)

(cid:32)
(cid:112)F (cid:48)(cid:48)(θ)dθ < +∞  the prior is called proper. However  stasticians often use priors which
(cid:112)F (cid:48)(cid:48)(θ)dθ = +∞ and any observation makes the

T (yi) − nF (θi)

(cid:33)

(4)

i=1

Θ

πJ (θ) ∝(cid:112)|F (cid:48)(cid:48)(θ)|.
n(cid:88)

θ

corresponding posterior (4) integrable.

Θ

Some Intuition for choosing the Jeffreys Prior
In the proof of our concentration result for
posterior distributions (Theorem 4) it will be crucial to lower bound the prior probability of
an -sized KL-divergence ball around each of the parameters θa. Since the Fisher information
F (cid:48)(cid:48)(θ) = limθ(cid:48)→θ K(θ  θ(cid:48))/|θ − θ(cid:48)|2  choosing a prior proportional to F (cid:48)(cid:48)(θ) ensures that the prior
√
measure of such balls are Ω(

).

Examples and Pseudocode Algorithm 1 presents pseudocode for Thompson Sampling with the
Jeffreys prior for distributions parametrized by their natural parameter θ. But as the Jeffreys prior
is invariant under reparametrization  if a distribution is parametrised by some parameter λ (cid:54)≡ θ 

the algorithm can use the Jeffreys prior ∝(cid:112)I(λ) on λ  drawing samples from the posterior on λ.

Note that the posterior sampling step (in bold) is always tractable using  for example  a Hastings-
Metropolis algorithm.

3

Algorithm 1 Thompson Sampling for Exponential Families with the Jeffreys prior
Require: F normalization function  T sufﬁcient statistic  µ mean function

for t = 1 . . . K do

Sample arm t and get rewards xt
Nt = 1  St = T (xt).

end for
for t = K + 1 . . . n do

Sample θa t from πa t ∝(cid:112)F (cid:48)(cid:48)(θ) exp (θSa − NaF (θ))

for a = 1 . . . K do

end for
Sample arm At = argmaxaµ(θa t) and get reward xt
SAt = SAt + T (xt) NAt = NAt + 1

end for

log

2

Distribution

Name
B(λ)

(cid:17)

(cid:16) λ

θ
1−λ
λ
σ2

Beta(cid:0) 1

Prior on λ
2   1

N (λ  σ2)
Γ(k  λ)
P(λ)

λx(1 − λ)1−xδ0 1
− (x−λ)2
1√
2πσ2 e
2σ2

(cid:1) Beta(cid:0) 1
2 + n − s(cid:1)
N(cid:16) s
(cid:17)
2 + s  n(cid:1)
Γ(cid:0) 1
Figure 1: The posterior distribution after observations y1  . . .   yn depends on n and s =(cid:80)n

Posterior on λ
2 + s  1
n   σ2
n
Γ(kn  s)

Γ (n + 1  s − n log xm)
αλ(n−1)k exp(−λks)

−λ
log(λ)
−λ − 1
−λk

Γ(k) xk−1e−λx1[0 +∞[(x)

λ

∝ 1
∝ 1
∝ 1√
∝ 1
∝ 1

λ

λxλ
xλ+1 1[xm +∞[(x)
m

kλ(xλ)k−1e−(λx)k

Pareto(xm  λ)
Weibull(k  λ)

λxe−λ

x!

δN(x)

λ

λk

1[0 +∞[

λk

i=1 T (yi)

Some examples of common exponential family models are given in Figure 1  together with the
posterior distributions on the parameter λ that is used by TS with the Jeffreys prior. In addition to
examples already studied in [7] for which T (x) = x  we also give two examples of more general
canonical exponential families  namely the Pareto distribution with known min value and unknown
tail index λ  Pareto(xm  λ)  for which T (x) = log(x)  and the Weibul distribution with known shape
and unknown rate parameter  Weibull(k  λ)  for which T (x) = xk. These last two distributions are
not covered even by the work in [8]  and belong to the family of heavy-tailed distributions.
For the Bernoulli model  we note futher that the use of the Jeffreys prior is not covered by the
previous analyses. These analyses make an extensive use of the uniform prior  through the fact that
the coefﬁcient of the Beta posteriors they consider have to be integers.

3 Results and Proof of Regret Bound

An exponential family K-armed bandit is a K-armed bandit for which the reward distributions pa
are known to be elements of an exponential family of distributions P(Θ). We denote by pθa the
distribution of arm a and its mean by µa = µ(θa).
Theorem 2 (Regret Bound). Assume that µ1 > µa for all a (cid:54)= 1  and that πa 0 is taken to be the
Jeffreys prior over Θ. Then for every  > 0 there exists a constant C( P) depending on  and on
the problem P such that the regret of Thompson Sampling using the Jeffreys prior satisﬁes

(cid:32) K(cid:88)

a=2

(cid:33)

(µ1 − µa)
K(θa  θ1)

R(AπJ   T ) ≤ 1 + 
1 − 

ln(T ) + C( P).

Proof: We give here the main argument of the proof of the regret bound  which proceed by bound-
ing the expected number of draws of any suboptimal arm. Along the way we shall state concentration
results whose proofs are postponed to later sections.

4

Step 0: Notation We denote by ya s the s-th observation of arm a and by Na t the number of times
arm a is chosen up to time t. (ya s)s≥1 is i.i.d. with distribution pθa. Let Y u
a := (ya s)1≤s≤u be
the vector of ﬁrst u observations from arm a. Ya t := Y Na t
is therefore the vector of observations
from arm a available at the beginning of round t. Recall that πa t  respectively πa 0  is the posterior 
respectively the prior  on θa at round t of the algorithm.
We deﬁne L(θ) to be such that PY ∼p(|θ)(p(Y |θ) ≥ L(θ)) ≥ 1
2. Observations from arm a such that
p(ya s|θ) ≥ L(θa) can therefore be seen as likely observations. For any δa > 0  we introduce the
event ˜Ea t = ˜Ea t(δa):

a

(cid:80)Na t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δa

− F (cid:48)(θa)

.

(5)

˜Ea t =

∃1 ≤ s(cid:48) ≤ Na t : p(ya s(cid:48)|θa) ≥ L(θa) 

Na t − 1
For all a (cid:54)= 1 and ∆a such that µa < µa + ∆a < µ1  we introduce

a t(∆a) :=(cid:0)µ (θa t) ≤ µa + ∆a

a t = Eθ

Eθ

s=1 s(cid:54)=s(cid:48) T (ya s)

(cid:1).

(cid:32)

On ˜Ea t  the empirical sufﬁcient statistic of arm a at round t is well concentrated around its mean
and a ’likely’ realization of arm a has been observed. On Eθ
a t  the mean of the distribution with
parameter θa t does not exceed by much the true mean  µa. δa and ∆a will be carefully chosen at
the end of the proof.

Step 1: Decomposition The idea of the proof is to decompose the probability of playing a subop-

timal arm using the events given in Step 0  and that E[Na T ] =(cid:80)T
a t)c(cid:17)
(cid:125)

at = a  ˜Ea t  Eθ
a t

at = a  ˜Ea t  (Eθ

T(cid:88)
(cid:124)

T(cid:88)
(cid:124)

E [Na T ] =

P(cid:16)

P(cid:16)

(cid:17)
(cid:125)

(cid:123)(cid:122)

(cid:123)(cid:122)

t=1

t=1

t=1

+

P (at = a):

P(cid:16)

T(cid:88)
(cid:124)

t=1

+

(cid:17)
(cid:125)

.

at = a  ˜Ec
a t

(cid:123)(cid:122)

(C)

(A)

(B)

where Ec denotes the complement of event E. Term (C) is controlled by the concentration of the
empirical sufﬁcient statistic  and (B) is controlled by the tail probabilities of the posterior distribu-
tion. We give the needed concentration results in Step 2. When conditioned on the event that the
optimal arm is played at least polynomially often  term (A) can be decomposed further  and then
controled by the results from Step 2. Step 3 proves that the optimal arm is played this many times.

Step 2: Concentration Results We state here the two concentration results that are necessary to
evaluate the probability of the above events.
Lemma 3. Let (ys) be an i.i.d sequence of distribution p(· | θ) and δ > 0. Then

(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

u

u(cid:88)

s=1

P

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ δ

[T (ys) − F (cid:48)(θ)]

≤ 2e−u ˜K(θ δ) 

where ˜K(θ  δ) = min(K(θ + g(δ)  θ)  K(θ − h(δ)  θ))  with g(δ) > 0 deﬁned by F (cid:48)(θ + g(δ)) =
F (cid:48)(θ) + δ and h(δ) > 0 deﬁned by F (cid:48)(θ − h(δ)) = F (cid:48)(θ) − δ.
The two following inequalities that will be useful in the sequel can easily be deduced from Lemma
3. Their proof is gathered in Appendix A with that of Lemma 3. For any arm a  for any b ∈]0  1[ 

T(cid:88)

P(at = a  ( ˜Ea t(δa))c) ≤

t=1

P(( ˜Ea t(δa))c ∩ Na t > tb) ≤

T(cid:88)

t=1

∞(cid:88)
∞(cid:88)

t=1

t=1

(cid:18) 1
(cid:19)t
(cid:18) 1
(cid:19)tb

2

+

t

2

∞(cid:88)
∞(cid:88)

t=1

+

t=1

2te−(t−1) ˜K(θa δa)

2t2e−(tb−1) ˜K(θa δa) 

(6)

(7)

The second result tells us that concentration of the empirical sufﬁcient statistic around its mean
implies concentration of the posterior distribution around the true parameter:
Theorem 4 (Posterior Concentration). Let πa 0 be the Jeffreys prior. There exists constants C1 a =
C1(F  θa) > 0  C2 a = C2(F  θa  ∆a) > 0  and N (θa  F ) s.t.  ∀Na t ≥ N (θa  F ) 

(cid:1) ≤ C1 ae−(Na t−1)(1−δaC2 a)K(θa µ−1(µa+∆a))+ln(Na t)

P(cid:0)µ(θa t) > µ(θa) + ∆a|Ya t

1 ˜Ea t

whenever δa < 1 and ∆a are such that 1 − δaC2 a(∆a) > 0.

5

Step 3: Lower Bound the Number of Optimal Arm Plays with High Probability The main
difﬁculty adressed in previous regret analyses for Thompson Sampling is the control of the number
of draws of the optimal arm. We provide this control in the form of Proposition 5 which is adapted
from Proposition 1 in [11]. The proof of this result  an outline of which is given in Appendix D 
explores in depth the randomised nature of Thompson Sampling. In particular  we show that the
proof in [11] can be signiﬁcantly simpliﬁed  but at the expense of no longer being able to describe
the constant Cb explicitly:

Proposition 5. ∀b ∈ (0  1)  ∃Cb(π  µ1  µ2  K) < ∞ such that(cid:80)∞

P(cid:0)N1 t ≤ tb(cid:1) ≤ Cb.

t=1

Step 4: Bounding the Terms of the Decomposition Now we bound the terms of the decomposi-
tion as discussed in Step 1: An upper bound on term (C) is given in (6)  whereas a bound on term
(B) follows from Lemma 6 below. Although the proof of this lemma is standard  and bears a strong
similarity to Lemma 3 of [3]  we provide it in Appendix C for the sake of completeness.
Lemma 6. For all actions a and for all  > 0  ∃ N = N(δa  ∆a  θa) > 0 such that

(B) ≤ [(1 − )(1 − δaC2 a)K(θa  µ−1(µa + ∆a))]−1 ln(T ) + max{N  N (θa  F )} + 1.

where N = N(δa  ∆a  θa) is the smallest integer such that for all n ≥ N

(n − 1)−1 ln(C1 an) < (1 − δaC2 a)K(θa  µ−1(µa + ∆a)) 

and N (θa  F ) is the constant from Theorem 4.

t=1

t=1

at = a  ˜Ea t  Eθ

P(cid:16)
P(cid:16)

When we have seen enough observations on the optimal arm  term (A) also becomes a result about
the concentration of the posterior and the empirical sufﬁcient statistic  but this time for the optimal
arm:

a t  N1 t > tb(cid:17)
+ Cb ≤ T(cid:88)
a  ˜E1 t(δ1)  N1 t > tb(cid:17)
(cid:123)(cid:122)
(cid:125)

(A) ≤ T(cid:88)
≤ T(cid:88)
(cid:124)
a = µ1 − µa − ∆a and δ1 > 0 remains to be chosen. The ﬁrst inequality comes from
where ∆(cid:48)
Proposition 5  and the second inequality comes from the following fact: if arm 1 is not chosen and
arm a is such that µ(θa t) ≤ µa + ∆a  then µ(θ1 t) ≤ µa + ∆a. A bound on term (C’) is given in
(7) for a = 1 and δ1. In Theorem 4  we bound the conditional probability that µ(θa t) exceed the
true mean. Following the same lines  we can also show that

P(cid:0)µ(θ1 t) ≤ µ1 − ∆(cid:48)
P(cid:16) ˜Ec
1 t(δ1) ∩ N1 t > tb(cid:17)
T(cid:88)
(cid:124)
(cid:123)(cid:122)
(cid:125)

a  N1 t > tb(cid:1) + Cb

µ(θ1 t) ≤ µ1 − ∆(cid:48)

P (µ(θ1 t) ≤ µ1 − ∆(cid:48)

For any ∆(cid:48)
function u (cid:55)→ e−(u−1)(1−δ1C2 1)K(θ1 µ−1(µ1−∆(cid:48)

a > 0  one can choose δ1 such that 1 − δ1C1 1 > 0. Then  with N = N (P) such that the
a))+ln u is decreasing for u ≥ N  (B(cid:48)) is bounded by

a|Y1 t) 1 ˜E1 t(δ1) ≤ C1 1e−(N1 t−1)(1−δ1C2 1)K(θ1 µ−1(µ1−∆(cid:48)
∞(cid:88)

a))+ln(N1 t).

+Cb (8)

C1 1e−(tb−1)(1−δ1C2 1)K(θ1 µ−1(µ1−∆(cid:48)

a))+ln(tb) < ∞.

N 1/b +

B(cid:48)

+

t=1

t=1

C(cid:48)

t=N 1/b+1

Step 4: Choosing the Values δa and a So far  we have shown that for any  > 0 and for any
choice of δa > 0 and 0 < ∆a < µ1 − µa such that 1 − δaC2 a > 0  there exists a constant
C(δa  ∆a   P) such that
E[Na T ] ≤

+ C(δa  ∆a   P)

(1 − δaC2 a)K(θa  µ−1(µa + ∆a))(1 − )

The constant is of course increasing (dramatically) when δa goes to zero  ∆a to µ1 − µa  or  to
zero. But one can choose ∆a close enough to µ1 − µa and δa small enough  such that

ln(T )

(1 − C2 a(∆a)δa)K(θa  µ−1(µa + ∆a)) ≥ K(θa  θ1)
(1 + )

 

and this choice leads to

Using that R(A  T ) =(cid:80)K

ln(T )

E[Na T ] ≤ 1 + 
1 − 
a=2(µ1 − µa)EA[Na T ] for any algorithm A concludes the proof.

+ C(δa  ∆a   P).

K(θa  θ1)

6

4 Posterior Concentration: Proof of Theorem 4

For ease of notation  we drop the subscript a and let (ys) be an i.i.d. sequence of distribution pθ 
with mean µ = µ(θ). Furthermore  by conditioning on the value of Ns  it is enough to bound
1 ˜Eu

P (µ(θu) ≥ µ + ∆|Y u) where Y u = (ys)1≤s≤u and

(cid:32)

˜Eu =

∃1 ≤ s(cid:48) ≤ u : p(ys(cid:48)|θ) ≥ L(θ) 

s=1 s(cid:54)=s(cid:48) T (ys)

u − 1

− F (cid:48)(θ)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ

.

(cid:80)u

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Step 1: Extracting a Kullback-Leibler Rate The argument rests on the following Lemma  whose
proof can be found in Appendix B
Lemma 7. Let ˜Eu be the event deﬁned by (5)  and introduce Θθ ∆ := {θ(cid:48) ∈ Θ : µ(θ(cid:48)) ≥ µ(θ)+∆}.
The following inequality holds:

(cid:82)

(cid:82)

e−(u−1)(K[θ θ(cid:48)]−δ|θ−θ(cid:48)|)π(θ(cid:48)|ys(cid:48))dθ(cid:48)
θ(cid:48)∈Θθ ∆
θ(cid:48)∈Θ e−(u−1)(K[θ θ(cid:48)]+δ|θ−θ(cid:48)|)π(θ(cid:48)|ys(cid:48))dθ(cid:48)

 

(9)

1 ˜Eu

P (µ(θu) ≥ µ + ∆|Y u) ≤
with s(cid:48) = inf{s ∈ N : p(ys|θ) ≥ L(θ)}.

Step 2: Upper bounding the numerator of (9) We ﬁrst note that on Θθ ∆ the leading term in the
exponential is K(θ  θ(cid:48)). Indeed  from (3) we know that

K(θ  θ(cid:48))/|θ − θ(cid:48)| = |F (cid:48)(θ) − (F (θ) − F (θ(cid:48)))/(θ − θ(cid:48))|

which  by strict convexity of F   is strictly increasing in |θ − θ(cid:48)| for any ﬁxed θ. Now since µ is
one-to-one and continuous  Θc

θ ∆ is an interval whose interior contains θ  and hence  on Θθ ∆ 

|θ − θ(cid:48)| ≥ F (µ−1(µ + ∆)) − F (θ)
K(θ  θ(cid:48))

µ−1(µ + ∆) − θ

− F (cid:48)(θ) := (C2(F  θ  ∆))−1 > 0.

So for δ such that 1 − δC2 > 0 we can bound the numerator of (9) by:

e−(u−1)(K(θ θ(cid:48))−δ|θ−θ(cid:48)|)π(θ(cid:48)|ys(cid:48))dθ(cid:48) ≤

e−(u−1)K(θ θ(cid:48))(1−δC2)π(θ(cid:48)|ys(cid:48))dθ(cid:48)

θ(cid:48)∈Θθ ∆
≤ e−(u−1)(1−δC2)K(θ µ−1(µ+∆))

(10)
where we have used that π(·|ys(cid:48)) is a probability distribution  and that  since µ is increasing 
K(θ  µ−1(µ + ∆)) = inf θ(cid:48)∈Θθ ∆ K(θ  θ(cid:48)).

π(θ(cid:48)|ys(cid:48))dθ(cid:48) ≤ e−(u−1)(1−δC2)K(θ µ−1(µ+∆))

Θθ ∆

(cid:90)

(cid:90)

θ(cid:48)∈Θθ ∆

(cid:90)

Step 3: Lower bounding the denominator of (9) To lower bound the denominator  we reduce
the integral on the whole space Θ to a KL-ball  and use the structure of the prior to lower bound
the measure of that KL-ball under the posterior obtained with the well-chosen observation ys(cid:48). We
introduce the following notation for KL balls: for any x ∈ Θ   > 0  we deﬁne

B(x) := {θ(cid:48) ∈ Θ : K(x  θ(cid:48)) ≤ } .

(θ−θ(cid:48))2 → F (cid:48)(cid:48)(θ) (cid:54)= 0 (since F is strictly convex). Therefore  there exists N1(θ  F ) such

We have K(θ θ(cid:48))
that for u ≥ N1(θ  F )  on B 1

(θ) 

u2

|θ − θ(cid:48)| ≤(cid:112)2K(θ  θ(cid:48))/F (cid:48)(cid:48)(θ).

(cid:90)

Using this inequality we can then bound the denominator of (9) whenever u ≥ N1(θ  F ) and δ < 1:
e−(u−1)(K(θ θ(cid:48))+δ|θ−θ(cid:48)|)π(θ(cid:48)|ys(cid:48))dθ(cid:48)
(cid:17)

e−(u−1)(K(θ θ(cid:48))+δ|θ−θ(cid:48)|)π(θ(cid:48)|ys(cid:48))dθ(cid:48) ≥
(cid:19)

(cid:90)
π(θ(cid:48)|ys(cid:48))dθ(cid:48) ≥ π(cid:0)B1/u2 (θ)|ys(cid:48)(cid:1) e

θ(cid:48)∈B1/u2 (θ)

(cid:113) 2

2K(θ θ(cid:48))
F(cid:48)(cid:48) (θ)

−(u−1)

K(θ θ(cid:48))+δ

1+

F (cid:48)(cid:48)(θ)

−(cid:16)

θ(cid:48)∈Θ

(cid:90)

(cid:114)

(cid:18)

≥

e

θ(cid:48)∈B1/u2 (θ)

.

(11)

7

Finally we turn our attention to the quantity

π(cid:0)B1/u2(θ)|ys(cid:48)(cid:1) =

(cid:82)
(cid:82)
B1/u2 (θ) p(y(cid:48)

Θ p(y(cid:48)

s|θ(cid:48))π0(θ(cid:48))dθ(cid:48)

s|θ(cid:48))π0(θ(cid:48))dθ(cid:48)

=

(cid:82)
(cid:82)
B1/u2 (θ) p(y(cid:48)

Θ p(y(cid:48)

s|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48))dθ(cid:48)
s|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48))dθ(cid:48)
(cid:20) F (b) − F (θ)

.

(12)

1

Now since the KL divergence is convex in the second argument  we can write B1/u2(θ) = (a  b).
So  from the convexity of F we deduce that

(cid:21)
u2 = K(θ  b) = F (b) − [F (θ) + (b − θ)F (cid:48)(θ)] = (b − θ)
Θ p(y|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48))dθ(cid:48) < ∞ is continuous on the compact C(θ). Thus  it follows that

≤ (b − θ) [F (cid:48)(b) − F (cid:48)(θ)] ≤ (b − a) [F (cid:48)(b) − F (cid:48)(θ)] ≤ (b − a) [F (cid:48)(b) − F (cid:48)(a)] .

As p(y | θ) → 0 as y → ±∞  the set C(θ) = {y : p(y | θ) ≥ L(θ)} is compact. The map

y (cid:55)→(cid:82)

− F (cid:48)(θ)

(b − θ)

(cid:26)(cid:90)

p(y|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48))dθ(cid:48)(cid:27)

< ∞

L(cid:48)(θ) = L(cid:48)(θ  F ) :=

sup

y:p(y|θ)>L(θ)

Θ

is an upper bound on the denominator of (12).
Now by the continuity of F (cid:48)(cid:48)  and the continuity of (y  θ) (cid:55)→ p(y|θ) in both coordinates  there exists
an N2(θ  F ) such that for all u ≥ N2(θ  F )
F (cid:48)(cid:48)(θ) ≥ 1
2
Finally  for u ≥ N2(θ  F )  we have a lower bound on the numerator of (12):

(cid:18)
p(y|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48)) ≥ L(θ)
s|θ(cid:48))(cid:112)F (cid:48)(cid:48)(θ(cid:48))dθ(cid:48) ≥ L(θ)
(cid:112)F (cid:48)(cid:48)(θ)

(cid:19)
(cid:112)F (cid:48)(cid:48)(θ)  ∀θ(cid:48) ∈ B1/u2 (θ)  y ∈ C(θ)
(cid:112)(F (cid:48)(b) − F (cid:48)(a)) (b − a) ≥ L(θ)

F (cid:48)(b) − F (cid:48)(a)

(cid:90) b

dθ(cid:48) =

b − a

p(y(cid:48)

(cid:90)

L(θ)

and

2

.

B1/u2 (θ)
Puting everything together  we get
max{N1  N2} such that for every δ < 1 satisfying 1 − δC2 > 0  and for every u ≥ N  one has

that there exist constants C2 = C2(F  θ  ∆) and N (θ  F ) =

a

2u

2

2

P(µ(θu) ≥ µ(θ) + ∆|Yu) ≤ 2e

1 ˜Eu

e−(u−1)(1−δC2)K(θ µ−1(µ+∆)).

Remark 8. Note that when the prior is proper we do not need to introduce the observation ys(cid:48) 
which signiﬁcantly simpliﬁes the argument. Indeed in this case  in (10) we can use π0 in place of
π(·|ys(cid:48)) which is already a probability distribution. In particular  the quantity (12) is replaced by
π0

(cid:0)B1/u2 (θ)(cid:1)  and so the constants L and L(cid:48) are not needed.

5 Conclusion

1+

(cid:113) 2
F(cid:48)(cid:48)(θ) L(cid:48)(θ)u
L(θ)

We have shown that choosing to use the Jeffreys prior in Thompson Sampling leads to an asymp-
totically optimal algorithm for bandit models whose rewards belong to a 1-dimensional canonical
exponential family. The cornerstone of our proof is a ﬁnite time concentration bound for posterior
distributions in exponential families  which  to the best of our knowledge  is new to the literature.
With this result we built on previous analyses and avoided Bernoulli-speciﬁc arguments. Thompson
Sampling with Jeffreys prior is now a provably competitive alternative to KL-UCB for exponential
family bandits. Moreover our proof holds for slightly more general problems than those for which
KL-UCB is provably optimal  including some heavy-tailed exponential family bandits.
Our arguments are potentially generalisable. Notably generalising to n-dimensional exponential
family bandits requires only generalising Lemma 3 and Step 3 in the proof of Theorem 4. Our result
is asymptotic  but the only stage where the constants are not explicitly derivable from knowledge of
F   T   and θ0 is in Lemma 9. Future work will investigate these open problems. Another possible
future direction lies the optimal choice of prior distribution. Our theoretical guarantees only hold for
Jeffreys’ prior  but a careful examination of our proof shows that the important property is to have 
for every θa 

(cid:33)

(cid:32)(cid:90)

− ln

(θ(cid:48):K(θa θ(cid:48))≤n−2)

π0(θ(cid:48))dθ(cid:48)

= o (n)  

which could hold for prior distributions other than the Jeffreys prior.

8

References
[1] S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem.

In Conference On Learning Theory (COLT)  2012.

[2] S. Agrawal and N. Goyal. Further optimal regret bounds for thompson sampling. In Sixteenth

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[3] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In

30th International Conference on Machine Learning (ICML)  2013.

[4] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit prob-

lem. Machine Learning  47(2):235–256  2002.

[5] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities. Oxford Univeristy Press 

2013.

[6] S. Bubeck and Che-Yu Liu. A note on the bayesian regret of thompson sampling with an

arbitrairy prior. arXiv:1304.5758  2013.

[7] O. Capp´e  A. Garivier  O-A. Maillard  R. Munos  and G. Stoltz. Kullback-Leibler upper con-

ﬁdence bounds for optimal sequential allocation. Annals of Statistics  41(3):516–541  2013.

[8] A. Garivier and O. Capp´e. The kl-ucb algorithm for bounded stochastic bandits and beyond.

In Conference On Learning Theory (COLT)  2011.

[9] J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support

models. In Conference On Learning Theory (COLT)  2010.

[10] H. Jeffreys. An invariant form for prior probability in estimation problem. Proceedings of the

Royal Society of London  186:453–461  1946.

[11] E. Kaufmann  N. Korda  and R. Munos. Thompson sampling: An asymptotically optimal
In Algorithmic Learning Theory  Lecture Notes in Computer Science 

ﬁnite-time analysis.
pages 199–213. Springer  2012.

[12] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[13] B.C. May  N. Korda  A. Lee  and D. Leslie. Optimistic bayesian sampling in contextual bandit

problems. Journal of Machine Learning Research  13:2069–2106  2012.

[14] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. arXiv:1301.2609 

2013.

[15] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25:285–294  1933.

[16] A.W Van der Vaart. Asymptotic Statistics. Cambridge University Press  1998.
[17] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer Publishing

Company  Incorporated  2010.

9

,Nathaniel Korda
Emilie Kaufmann
Remi Munos