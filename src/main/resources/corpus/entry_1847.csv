2010,Error Propagation for Approximate Policy and Value Iteration,We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover  we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum -- as opposed to what has been suggested by the previous results.  Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI  and the effect of an error term in the earlier iterations decays exponentially fast.,Error Propagation for Approximate Policy and

Value Iteration

Amir massoud Farahmand

Department of Computing Science

University of Alberta

Edmonton  Canada  T6G 2E8

amirf@ualberta.ca

Sequel Project  INRIA Lille

R´emi Munos

Lille  France

remi.munos@inria.fr

Csaba Szepesv´ari ∗

Department of Computing Science

University of Alberta

Edmonton  Canada  T6G 2E8
szepesva@ualberta.ca

Abstract

We address the question of how the approximation error/Bellman residual at each
iteration of the Approximate Policy/Value Iteration algorithms inﬂuences the qual-
ity of the resulted policy. We quantify the performance loss as the Lp norm of the
approximation error/Bellman residual at each iteration. Moreover  we show that
the performance loss depends on the expectation of the squared Radon-Nikodym
derivative of a certain distribution rather than its supremum – as opposed to what
has been suggested by the previous results. Also our results indicate that the
contribution of the approximation/Bellman error to the performance loss is more
prominent in the later iterations of API/AVI  and the effect of an error term in the
earlier iterations decays exponentially fast.

1

Introduction

The exact solution for the reinforcement learning (RL) and planning problems with large state space
is difﬁcult or impossible to obtain  so one usually has to aim for approximate solutions. Approximate
Policy Iteration (API) and Approximate Value Iteration (AVI) are two classes of iterative algorithms
to solve RL/Planning problems with large state spaces. They try to approximately ﬁnd the ﬁxed-
point solution of the Bellman optimality operator.
AVI starts from an initial value function V0 (or Q0)  and iteratively applies an approximation of
T ∗  the Bellman optimality operator  (or T π for the policy evaluation problem) to the previous
estimate  i.e.  Vk+1 ≈ T ∗Vk. In general  Vk+1 is not equal to T ∗Vk because (1) we do not have
direct access to the Bellman operator but only some samples from it  and (2) the function space
in which V belongs is not representative enough. Thus there would be an approximation error
εk = T ∗Vk − Vk+1 between the result of the exact VI and AVI.
Some examples of AVI-based approaches are tree-based Fitted Q-Iteration of Ernst et al. [1]  multi-
layer perceptron-based Fitted Q-Iteration of Riedmiller [2]  and regularized Fitted Q-Iteration of
Farahmand et al. [3]. See the work of Munos and Szepesv´ari [4] for more information about AVI.

∗Csaba Szepesv´ari is on leave from MTA SZTAKI. We would like to acknowledge the insightful comments
by the reviewers. This work was partly supported by AICML  AITF  NSERC  and PASCAL2 under no216886.

1

API is another iterative algorithm to ﬁnd an approximate solution to the ﬁxed point of the Bellman
optimality operator. It starts from a policy π0  and then approximately evaluates that policy π0  i.e. 
it ﬁnds a Q0 that satisﬁes T π0 Q0 ≈ Q0. Afterwards  it performs a policy improvement step  which
is to calculate the greedy policy with respect to (w.r.t.) the most recent action-value function  to get
a new policy π1  i.e.  π1(·) = arg maxa∈A Q0(·  a). The policy iteration algorithm continues by
approximately evaluating the newly obtained policy π1 to get Q1 and repeating the whole process
again  generating a sequence of policies and their corresponding approximate action-value functions
Q0 → π1 → Q1 → π2 → ··· . Same as AVI  we may encounter a difference between the ap-
proximate solution Qk (T πk Qk ≈ Qk) and the true value of the policy Qπk  which is the solution
of the ﬁxed-point equation T πk Qπk = Qπk. Two convenient ways to describe this error is either
by the Bellman residual of Qk (εk = Qk − T πk Qk) or the policy evaluation approximation error
(εk = Qk − Qπk).
API is a popular approach in RL literature. One well-known algorithm is LSPI of Lagoudakis and
Parr [5] that combines Least-Squares Temporal Difference (LSTD) algorithm (Bradtke and Barto
[6]) with a policy improvement step. Another API method is to use the Bellman Residual Mini-
mization (BRM) and its variants for policy evaluation and iteratively apply the policy improvement
step (Antos et al. [7]  Maillard et al. [8]). Both LSPI and BRM have many extensions: Farah-
mand et al. [9] introduced a nonparametric extension of LSPI and BRM and formulated them as
an optimization problem in a reproducing kernel Hilbert space and analyzed its statistical behavior.
Kolter and Ng [10] formulated an l1 regularization extension of LSTD. See Xu et al. [11] and Jung
and Polani [12] for other examples of kernel-based extension of LSTD/LSPI  and Taylor and Parr
[13] for a uniﬁed framework. Also see the proto-value function-based approach of Mahadevan and
Maggioni [14] and iLSTD of Geramifard et al. [15].
A crucial question in the applicability of API/AVI  which is the main topic of this work  is to un-
derstand how either the approximation error or the Bellman residual at each iteration of API or AVI
affects the quality of the resulted policy. Suppose we run API/AVI for K iterations to obtain a policy
πK. Does the knowledge that all εks are small (maybe because we have had a lot of samples and
used powerful function approximators) imply that V πK is close to the optimal value function V ∗
too? If so  how does the errors occurred at a certain iteration k propagate through iterations of
API/AVI and affect the ﬁnal performance loss?
There have already been some results that partially address this question. As an example  Propo-
sition 6.2 of Bertsekas and Tsitsiklis [16] shows that for API applied to a ﬁnite MDP  we have
lim supk→∞ (cid:107)V ∗ − V πk(cid:107)∞ ≤ 2γ
(1−γ)2 lim supk→∞ (cid:107)V πk − Vk(cid:107)∞ where γ is the discount facto.
Similarly for AVI  if the approximation errors are uniformly bounded ((cid:107)T ∗Vk − Vk+1(cid:107)∞ ≤ ε)  we
have lim supk→∞ (cid:107)V ∗ − V πk(cid:107)∞ ≤ 2γ
Nevertheless  most of these results are pessimistic in several ways. One reason is that they are
expressed as the supremum norm of the approximation errors (cid:107)V πk − Vk(cid:107)∞ or the Bellman error
(cid:107)Qk − T πk Qk(cid:107)∞. Compared to Lp norms  the supremum norm is conservative. It is quite possible
that the result of a learning algorithm has a small Lp norm but a very large L∞ norm. Therefore  it
is desirable to have a result expressed in Lp norm of the approximation/Bellman residual εk.
In the past couple of years  there have been attempts to extend L∞ norm results to Lp ones [18  17 
7]. As a typical example  we quote the following from Antos et al. [7]:
Proposition 1 (Error Propagation for API – [7]). Let p ≥ 1 be a real and K be a positive integer.
Then  for any sequence of functions {Q(k)} ⊂ B(X × A; Qmax)(0 ≤ k < K)  the space of Qmax-
bounded measurable functions  and their corresponding Bellman residuals εk = Qk − T πQk  the
following inequalities hold:

(1−γ)2 ε (Munos [17]).

where Rmax is an upper bound on the magnitude of the expected reward function and

(cid:16)

C 1/p

ρ ν max
0≤k<K

2γ

(1 − γ)2

(cid:107)Q∗ − QπK(cid:107)p ρ ≤

Cρ ν = (1 − γ)2(cid:88)

mγm−1

sup

π1 ... πm

m≥1

(cid:17)

 

K

p −1 Rmax

(cid:107)εk(cid:107)p ν + γ

(cid:13)(cid:13)(cid:13)(cid:13) d (ρP π1 ··· P πm)

dν

(cid:13)(cid:13)(cid:13)(cid:13)∞

.

This result indeed uses Lp norm of the Bellman residuals and is an improvement over results
like Bertsekas and Tsitsiklis [16  Proposition 6.2]  but still is pessimistic in some ways and does

2

dν

||∞ is intrinsic to the difﬁculty of the problem or can be relaxed.

not answer several important questions. For instance  this result implies that the uniform-over-all-
iterations upper bound max0≤k<K (cid:107)εk(cid:107)p ν is the quantity that determines the performance loss. One
may wonder if this condition is really necessary  and ask whether it is better to put more emphasis
on earlier/later iterations? Or another question is whether the appearance of terms in the form of
|| d(ρP π1···P πm )
The goal of this work is to answer these questions and to provide tighter upper bounds on the
performance loss of API/AVI algorithms. These bounds help one understand what factors contribute
to the difﬁculty of a learning problem. We base our analysis on the work of Munos [17]  Antos et al.
[7]  Munos [18] and provide upper bounds on the performance loss in the form of (cid:107)V ∗ − V πk(cid:107)1 ρ
(the expected loss weighted according to the evaluation probability distribution ρ – this is deﬁned
in Section 2) for API (Section 3) and AVI (Section 4). This performance loss depends on a certain
function of ν-weighted L2 norms of εks  in which ν is the data sampling distribution  and Cρ ν(K)
that depends on the MDP  two probability distributions ρ and ν  and the number of iterations K.
In addition to relating the performance loss to Lp norm of the Bellman residual/approximation er-
ror  this work has three main contributions that to our knowledge have not been considered before:
(1) We show that the performance loss depends on the expectation of the squared Radon-Nikodym
derivative of a certain distribution  to be speciﬁed in Section 3  rather than its supremum. The dif-
ference between this expectation and the supremum can be considerable. For instance  for a ﬁnite
state space with N states  the ratio can be of order O(N 1/2). (2) The contribution of the Bell-
man/approximation error to the performance loss is more prominent in later iterations of API/AVI.
and the effect of an error term in early iterations decays exponentially fast. (3) There are certain
structures in the deﬁnition of concentrability coefﬁcients that have not been explored before. We
thoroughly discuss these qualitative/structural improvements in Section 5.

2 Background

In this section  we provide a very brief summary of some of the concepts and deﬁnitions from
the theory of Markov Decision Processes (MDP) and reinforcement learning (RL) and a few other
notations. For further information about MDPs and RL the reader is referred to [19  16  20  21].
A ﬁnite-action discounted MDP is a 5-tuple (X  A  P R  γ)  where X is a measurable state space  A
is a ﬁnite set of actions  P is the probability transition kernel  R is the reward kernel  and 0 ≤ γ < 1
is the discount factor. The transition kernel P is a mapping with domain X × A evaluated at
(x  a) ∈ X × A that gives a distribution over X   which we shall denote by P (·|x  a). Likewise 
R is a mapping with domain X × A that gives a distribution of immediate reward over R  which
is denoted by R(·|x  a). We denote r(x  a) = E [R(·|x  a)]  and assume that its absolute value is
bounded by Rmax.
A mapping π : X → A is called a deterministic Markov stationary policy  or just a policy in
short. Following a policy π in an MDP means that at each time step At = π(Xt). Upon taking
action At at Xt  we receive reward Rt ∼ R(·|x  a)  and the Markov chain evolves according to
Xt+1 ∼ P (·|Xt  At). We denote the probability transition kernel of following a policy π by P π 
i.e.  P π(dy|x) = P (dy|x  π(x)).

The value function V π for a policy π is deﬁned as V π(x) (cid:44) E(cid:104)(cid:80)∞
action-value function is deﬁned as Qπ(x  a) (cid:44) E(cid:104)(cid:80)∞

(cid:12)(cid:12)(cid:12) X0 = x
(cid:12)(cid:12)(cid:12) X0 = x  A0 = a
(cid:105)

t=0 γtRt

. For a discounted
MDP  we deﬁne the optimal value and action-value functions by V ∗(x) = supπ V π(x) (∀x ∈ X )
and Q∗(x  a) = supπ Qπ(x  a) (∀x ∈ X  ∀a ∈ A). We say that a policy π∗ is optimal
if it achieves the best values in every state  i.e.  if V π∗ = V ∗. We say that a policy π is
greedy w.r.t. an action-value function Q and write π = ˆπ(·; Q)  if π(x) ∈ arg maxa∈A Q(x  a)
holds for all x ∈ X . Similarly  the policy π is greedy w.r.t. V   if for all x ∈ X   π(x) ∈
argmaxa∈A
is chosen in an arbitrary deterministic manner). Greedy policies are important because a greedy pol-
icy w.r.t. Q∗ (or V ∗) is an optimal policy. Hence  knowing Q∗ is sufﬁcient for behaving optimally
(cf. Proposition 4.3 of [19]).

(cid:82) P (dx(cid:48)|x  a)[r(x  a) + γV (x(cid:48))] (If there exist multiple maximizers  some maximizer

(cid:105)

t=0 γtRt

and the

3

erator is deﬁned as (T ∗V )(x) (cid:44) maxa

We deﬁne the Bellman operator for a policy π as (T πV )(x) (cid:44) r(x  π(x)) + γ(cid:82) V π(x(cid:48))P (dx(cid:48)|x  a)
and (T πQ)(x  a) (cid:44) r(x  a) + γ(cid:82) Q(x(cid:48)  π(x(cid:48)))P (dx(cid:48)|x  a). Similarly  the Bellman optimality op-
(cid:110)
(cid:111)
r(x  a) + γ(cid:82) V (x(cid:48))P (dx(cid:48)|x  a)
r(x  a) + γ(cid:82) maxa(cid:48) Q(x(cid:48)  a(cid:48))P (dx(cid:48)|x  a).
ρP π(dx(cid:48)) =(cid:82) P (dx(cid:48)|x  π(x))dρ(x). In words  ρ(P π)m ∈ M(X ) is an m-step-ahead probability
distribution of states if the starting state distribution is ρ and we follow P π for m steps. In what
follows we shall use (cid:107)V (cid:107)p ν to denote the Lp(ν)-norm of a measurable function V : X → R:
(cid:80)
(cid:107)V (cid:107)p
(cid:44)
1|A|

For a measurable space X   with a σ-algebra σX   we deﬁne M(X ) as the set of all probability
measures over σX . For a probability measure ρ ∈ M(X ) and the transition kernel P π  we deﬁne

X |V (x)|pdν(x). For a function Q : X × A (cid:55)→ R  we deﬁne (cid:107)Q(cid:107)p

(cid:44) ν|V |p (cid:44) (cid:82)
(cid:82)

X |Q(x  a)|pdν(x).

and (T ∗Q)(x  a) (cid:44)

a∈A

p ν

p ν

3 Approximate Policy Iteration
Consider the API procedure and the sequence Q0 → π1 → Q1 → π2 → ··· → QK−1 → πK 
where πk is the greedy policy w.r.t. Qk−1 and Qk is the approximate action-value function for policy
πk. For the sequence {Qk}K−1
k=0   denote the Bellman Residual (BR) and policy Approximation Error
(AE) at each iteration by

k = Qk − T πk Qk 
εBR
k = Qk − Qπk .
εAE

(1)
(2)

k }K−1

k }K−1

k=0 or the policy evaluation approximation error sequence {εAE

The goal of this section is to study the effect of ν-weighted L2p norm of the Bellman residual
sequence {εBR
k=0 on the per-
formance loss (cid:107)Q∗ − QπK(cid:107)p ρ of the outcome policy πK.
The choice of ρ and ν is arbitrary  however  a natural choice for ν is the sampling distribution of the
data  which is used by the policy evaluation module. On the other hand  the probability distribution
ρ reﬂects the importance of various regions of the state space and is selected by the practitioner. One
common choice  though not necessarily the best  is the stationary distribution of the optimal policy.
Because of the dynamical nature of MDP  the performance loss (cid:107)Q∗ − QπK(cid:107)p ρ depends on the
difference between the sampling distribution ν and the future-state distribution in the form of
ρP π1 P π2 ··· . The precise form of this dependence will be formalized in Theorems 3 and 4. Before
stating the results  we require to deﬁne the following concentrability coefﬁcients.
Deﬁnition 2 (Expected Concentrability of the Future-State Distribution). Given ρ  ν ∈ M(X ) 
ν (cid:28) λ1 (λ is the Lebesgue measure)  m ≥ 0  and an arbitrary sequence of stationary policies
{πm}m≥1  let ρP π1 P π2 . . . P πm ∈ M(X ) denote the future-state distribution obtained when the
ﬁrst state is distributed according to ρ and then we follow the sequence of policies {πk}m
Deﬁne the following concentrability coefﬁcients that is used in API analysis:

k=1.

cPI1 ρ ν(m1  m2; π) (cid:44)

cPI2 ρ ν(m1  m2; π1  π2) (cid:44)

cPI3 ρ ν (cid:44)

EX∼ν
EX∼ν
EX∼ν

dν

2

 

(X)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:0)ρ(P π∗)m1(P π)m2(cid:1)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:0)ρ(P π∗)m1(P π1)m2P π2(cid:1)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:0)ρP π∗(cid:1)

(X)

(X)

dν

dν

2

 

2

 

1For two measures ν1 and ν2 on the same measurable space  we say that ν1 is absolutely continuous with

respect to ν2 (or ν2 dominates ν1) and denote ν1 (cid:28) ν2 iff ν2(A) = 0 ⇒ ν1(A) = 0.

4

the

understanding

with
that
the
ρ(P π∗)m1(P π1)m2P π2 or ρP π∗
cPI1 ρ ν(m1  m2; π) = ∞ (similar for others).
Also deﬁne the following concentrability coefﬁcient that is used in AVI analysis:

if
is not absolutely continuous w.r.t.

distribution

future-state

)

ρ(P π∗)m1(P π)m2

(or
then we take

ν 

cVI ρ ν(m1  m2; π) (cid:44)

EX∼ν

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:0)ρ(P π)m1(P π∗)m2(cid:1)

dν

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 1

2

 

(X)

with the understanding that if the future-state distribution ρ(P π∗)m1(P π)m2 is not absolutely con-
tinuous w.r.t. ν  then we take cVI ρ ν(m1  m2; π) = ∞.
In order to compactly present our results  we deﬁne the following notation:

αk =

(1 − γ)γK−k−1

1 − γK+1

0 ≤ k < K.

1−γ . Then for any sequence {Qk}K−1

Theorem 3 (Error Propagation for API). Let p ≥ 1 be a real number  K be a positive integer 
and Qmax ≤ Rmax
k=0 ⊂ B(X × A  Qmax) (space of Qmax-bounded
measurable functions deﬁned on X × A) and the corresponding sequence {εk}K−1
k=0 deﬁned in (1)
or (2)   we have
(cid:21)

2p (ε0  . . .   εK−1; r) + γ

K

p −1Rmax

.

(cid:20)
where E(ε0  . . .   εK−1; r) =(cid:80)K−1

(cid:107)Q∗ − QπK(cid:107)p ρ ≤

(1 − γ)2

2γ

inf
r∈[0 1]

k=0 α2r

(a) If εk = εBR for all 0 ≤ k < K  we have

1
2p

2p ν.

PI(BR/AE) ρ ν(K; r)E 1
C
k (cid:107)εk(cid:107)2p
K−1(cid:88)

(cid:32)(cid:88)

α2(1−r)

γm(cid:16)

k

k=0

m≥0

CPI(BR) ρ ν(K; r) = (

1 − γ
2

)2

sup
π(cid:48)
0 ... π(cid:48)

K

cPI1 ρ ν(K − k − 1  m + 1; π(cid:48)

k+1)+

(cid:17)(cid:33)2

.

cPI1 ρ ν(K − k  m; π(cid:48)
k)

(b) If εk = εAE for all 0 ≤ k < K  we have

CPI(AE) ρ ν(K; r  s) = (

1 − γ
2

)2

sup
π(cid:48)
0 ... π(cid:48)

K

K−1(cid:88)

k=0

α2(1−r)

k

(cid:32)(cid:88)
(cid:88)

m≥0

m≥1

γmcPI1 ρ ν(K − k − 1  m + 1; π(cid:48)

k+1)+

γmcPI2 ρ ν(K − k − 1  m; π(cid:48)

k+1  π(cid:48)

k) + cPI3 ρ ν

(cid:33)2

.

4 Approximate Value Iteration
Consider the AVI procedure and the sequence V0 → V1 → ··· → VK−1  in which Vk+1 is the
result of approximately applying the Bellman optimality operator on the previous estimate Vk  i.e. 
Vk+1 ≈ T ∗Vk. Denote the approximation error caused at each iteration by

(3)
The goal of this section is to analyze AVI procedure and to relate the approximation error sequence
{εk}K−1
k=0 to the performance loss (cid:107)V ∗ − V πK(cid:107)p ρ of the obtained policy πK  which is the greedy
policy w.r.t. VK−1.

εk = T ∗Vk − Vk+1.

5

1−γ . Then for any sequence {Vk}K−1

Theorem 4 (Error Propagation for AVI). Let p ≥ 1 be a real number  K be a positive integer  and
Vmax ≤ Rmax
k=0 ⊂ B(X   Vmax)  and the corresponding sequence
(cid:21)
{εk}K−1

(cid:20)

k=0 deﬁned in (3)  we have
(cid:107)V ∗ − V πK(cid:107)p ρ ≤

2γ

(1 − γ)2

1
2p

VI ρ ν(K; r)E 1
C

inf
r∈[0 1]

2p (ε0  . . .   εK−1; r) +

K

γ

p Rmax

 

2
1 − γ

where

CVI ρ ν(K; r) = (

K−1(cid:88)
and E(ε0  . . .   εK−1; r) =(cid:80)K−1

1 − γ
2

)2 sup
π(cid:48)

k=0

k=0 α2r

k (cid:107)εk(cid:107)2p

2p ν.

(cid:88)

m≥0

α2(1−r)

k

γm (cVI ρ ν(m  K − k; π(cid:48)) + cVI ρ ν(m + 1  K − k − 1; π(cid:48)))

2

 

5 Discussion

In this section  we discuss signiﬁcant improvements of Theorems 3 and 4 over previous results such
as [16  18  17  7].

2γ

5.1 Lp norm instead of L∞ norm
As opposed to most error upper bounds  Theorems 3 and 4 relate (cid:107)V ∗ − V πK(cid:107)p ρ to the Lp norm
of the approximation or Bellman errors (cid:107)εk(cid:107)2p ν of iterations in API/AVI. This should be con-
trasted with the traditional  and more conservative  results such as lim supk→∞ (cid:107)V ∗ − V πk(cid:107)∞ ≤
(1−γ)2 lim supk→∞ (cid:107)V πk − Vk(cid:107)∞ for API (Proposition 6.2 of Bertsekas and Tsitsiklis [16]). The
use of Lp norm not only is a huge improvement over conservative supremum norm  but also allows
us to beneﬁt from the vast literature on supervised learning techniques  which usually provides error
upper bounds in the form of Lp norms  in the context of RL/Planning problems. This is especially
interesting for the case of p = 1 as the performance loss (cid:107)V ∗ − V πK(cid:107)1 ρ is the difference between
the expected return of the optimal policy and the resulted policy πK when the initial state distribu-
tion is ρ. Convenient enough  the errors appearing in the upper bound are in the form of (cid:107)εk(cid:107)2 ν
which is very common in the supervised learning literature. This type of improvement  however 
has been done in the past couple of years [18  17  7] - see Proposition 1 in Section 1.

5.2 Expected versus supremum concentrability of the future-state distribution

The concentrability coefﬁcients (Deﬁnition 2) reﬂect the effect of future-state distribution on the per-
formance loss (cid:107)V ∗ − V πK(cid:107)p ρ. Previously it was thought that the key contributing factor to the per-
formance loss is the supremum of the Radon-Nikodym derivative of these two distributions. This is
evident in the deﬁnition of Cρ ν in Proposition 1 where we have terms in the form of || d(ρ(P π)m)
||∞
instead of

(cid:104)| d(ρ(P π)m)

(X)|2(cid:105)(cid:17) 1

2 that we have in Deﬁnition 2.

(cid:16)EX∼ν

dν

dν

Nevertheless  it turns out that the key contributing factor that determines the performance loss is
the expectation of the squared Radon-Nikodym derivative instead of its supremum. Intuitively this
implies that even if for some subset of X (cid:48) ⊂ X the ratio d(ρ(P π)m)
is large but the probability ν(X (cid:48))
is very small  performance loss due to it is still small. This phenomenon has not been suggested by
previous results.
As an illustration of this difference  consider a Chain Walk with 1000 states with a single policy that
201 for x ∈ [400  600] and zero everywhere
drifts toward state 1 of the chain. We start with ρ(x) = 1
else. Then we evaluate both || d(ρ(P π)m)
) 1
2 for m = 1  2  . . . when ν
is the uniform distribution. The result is shown in Figure 1a. One sees that the ratio is constant in the
beginning  but increases when the distribution ρ(P π)m concentrates around state 1  until it reaches
steady-state. The growth and the ﬁnal value of the expectation-based concentrability coefﬁcient is
much smaller than that of supremum-based.

(cid:104)| d(ρ(P π)m)

||∞ and (EX∼ν

|2(cid:105)

dν

dν

dν

6

(a)

(cid:104)| d(ρ(P π)m)

|2(cid:105) 1

(cid:13)(cid:13)(cid:13) d(ρ(P π)m)

dν

(b)

(cid:13)(cid:13)(cid:13)∞

(a) Comparison of EX∼ν

(b) Comparison of
Figure 1:
(cid:107)Q∗ − Qk(cid:107)1 for uniform and exponential data sampling schedule. The total number of samples
is the same. [The Y -scale of both plots is logarithmic.]

2 and

dν

|2(cid:105)

dν

2 → √
) 1

||∞ → N  while (EX∼ν

It is easy to show that if the Chain Walk has N states and the policy has the same concentrating
behavior and ν is uniform  then || d(ρ(P π)m)
N when
√
m → ∞. The ratio  therefore  would be of order Θ(
N). This clearly shows the improvement of
this new analysis in a simple problem. One may anticipate that this sharper behavior happens in
many other problems too.
More generally  consider C∞ = || dµ
) 1
2 . For a ﬁnite state space
with N states and ν is the uniform distribution  C∞ ≤ N but CL2 ≤ √
N. Neglecting all
other differences between our results and the previous ones  we get a performance upper bound
in the form of (cid:107)Q∗ − QπK(cid:107)1 ρ ≤ c1(γ)O(N 1/4) supk (cid:107)εk(cid:107)2 ν  while Proposition 1 implies that
(cid:107)Q∗ − QπK(cid:107)1 ρ ≤ c2(γ)O(N 1/2) supk ||k||2 ν. This difference between O(N 1/4) and O(N 1/2)
shows a signiﬁcant improvement.

dν ||∞ and CL2 = (EX∼ν

dν

(cid:104)| d(ρ(P π)m)
(cid:104)| dµ
dν |2(cid:105)

5.3 Error decaying property

k=0 α2r

k (cid:107)εk(cid:107)2p

k=0 is in the form of E(ε0  . . .   εK−1; r) =(cid:80)K−1

the dependence of performance loss (cid:107)V ∗ − V πK(cid:107)p ρ (or
Theorems 3 and 4 show that
(cid:107)Q∗ − QπK(cid:107)p ρ) on {εk}K−1
2p ν. This has
a very special structure in that the approximation errors at later iterations have more contribution to
the ﬁnal performance loss. This behavior is obscure in previous results such as [17  7] that the depen-
dence of the ﬁnal performance loss is expressed as E(ε0  . . .   εK−1; r) = maxk=0 ... K−1 (cid:107)εk(cid:107)p ν
(see Proposition 1).
This property has practical and algorithmic implications too. It says that it is better to put more
effort on having a lower Bellman or approximation error at later iterations of API/AVI. This  for
instance  can be done by gradually increasing the number of samples throughout iterations  or to use
more powerful  and possibly computationally more expensive  function approximators for the later
iterations of API/AVI.
To illustrate this property  we compare two different sampling schedules on a simple MDP. The
MDP is a 100-state  2-action chain similar to Chain Walk problem in the work of Lagoudakis and
Parr [5]. We use AVI with a lookup-table function representation. In the ﬁrst sampling schedule 
every 20 iterations we generate a ﬁxed number of fresh samples by following a uniformly random
walk on the chain (this means that we throw away old samples). This is the ﬁxed strategy. In the
exponential strategy  we again generate new samples every 20 iterations but the number of samples
at the kth iteration is ckγ. The constant c is tuned such that the total number of both sampling
strategy is almost the same (we give a slight margin of about 0.1% of samples in favor of the ﬁxed
strategy). What we compare is (cid:107)Q∗ − Qk(cid:107)1 ν when ν is the uniform distribution. The result can be
seen in Figure 1b. The improvement of the exponential sampling schedule is evident. Of course  one

7

150010001500100101102103Step (m)Concentrability Coefficients  Infinity norm−based concentrabilityExpectation−base concentrability10204060801001201401601802000.20.30.40.50.60.811.52345IterationL1 error  UniformExponentialmay think of more sophisticated sampling schedules but this simple illustration should be sufﬁcient
to attract the attention of practitioners to this phenomenon.

5.4 Restricted search over policy space

k in cPI1 ρ ν(K − k  m; π(cid:48)

One interesting feature of our results is that it puts more structure and restriction on the way policies
may be selected. Comparing CPI ρ ν(K; r) (Theorem 3) and CVI ρ ν(K; r) (Theorem 4) with Cρ ν
(Proposition 1) we see that:
(1) Each concentrability coefﬁcient in the deﬁnition of CPI ρ ν(K; r) depends only on a single or
two policies (e.g.  π(cid:48)
k)). The same is true for CVI ρ ν(K; r). In contrast  the
mth term in Cρ ν has π1  . . .   πm as degrees of freedom  and this number is growing as m → ∞.
(2) The operator sup in CPI ρ ν and CVI ρ ν appears outside the summation. Because of that  we
only have K + 1 degrees of freedom π(cid:48)
K to choose from in API and remarkably only a
single degree of freedom in AVI. On the other other hand  sup appears inside the summation in the
deﬁnition of Cρ ν. One may construct an MDP that this difference in the ordering of sup leads to an
arbitrarily large ratio of two different ways of deﬁning the concentrability coefﬁcients.
(3) In API  the deﬁnitions of concentrability coefﬁcients cPI1 ρ ν  cPI2 ρ ν  and cPI3 ρ ν (Deﬁni-
tion 2) imply that if ρ = ρ∗  the stationary distribution induced by an optimal policy π∗  then
cPI1 ρ ν(m1  m2; π) = cPI1 ρ ν(·  m2; π) = (EX∼ν
) 1
2 (similar for the other two
coefﬁcients). This special structure is hidden in the deﬁnition of Cρ ν in Proposition 1  and instead
we have an extra m1 degrees of ﬂexibility.
Remark 1. For general MDPs  the computation of concentrability coefﬁcients in Deﬁnition 2 is
difﬁcult  as it is for similar coefﬁcients deﬁned in [18  17  7].

(cid:20)(cid:12)(cid:12)(cid:12) d(ρ∗(P π)m2 )

0  . . .   π(cid:48)

(cid:12)(cid:12)(cid:12)2(cid:21)

dν

6 Conclusion

To analyze an API/AVI algorithm and to study its statistical properties such as consistency or con-
vergence rate  we require to (1) analyze the statistical properties of the algorithm running at each
iteration  and (2) study the way the policy approximation/Bellman errors propagate and inﬂuence
the quality of the resulted policy.
The analysis in the ﬁrst step heavily uses tools from the Statistical Learning Theory (SLT) literature 
e.g.  Gy¨orﬁ et al. [22]. In some cases  such as AVI  the problem can be cast as a standard regression
with the twist that extra care should be taken to the temporal dependency of data in RL scenario.
The situation is a bit more complicated for API methods that directly aim for the ﬁxed-point solution
(such as LSTD and its variants)  but still the same kind of tools from SLT can be used too – see Antos
et al. [7]  Maillard et al. [8].
The analysis for the second step is what this work has been about. In our Theorems 3 and 4  we
have provided upper bounds that relate the errors at each iteration of API/AVI to the performance
loss of the whole procedure. These bounds are qualitatively tighter than the previous results such as
those reported by [18  17  7]  and provide a better understanding of what factors contribute to the
difﬁculty of the problem. In Section 5  we discussed the signiﬁcance of these new results and the
way they improve previous ones.
Finally  we should note that there are still some unaddressed issues. Perhaps the most important one
is to study the behavior of concentrability coefﬁcients cPI1 ρ ν(m1  m2; π)  cPI2 ρ ν(m1  m2; π1  π2) 
and cVI ρ ν(m1  m2; π) as a function of m1  m2  and of course the transition kernel P of MDP. A
better understanding of this question alongside a good understanding of the way each term εk in
E(ε0  . . .   εK−1; r) behaves  help us gain more insight about the error convergence behavior of the
RL/Planning algorithms.

References
[1] Damien Ernst  Pierre Geurts  and Louis Wehenkel. Tree-based batch mode reinforcement

learning. Journal of Machine Learning Research  6:503–556  2005.

8

[2] Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural
reinforcement learning method. In 16th European Conference on Machine Learning  pages
317–328  2005.

[3] Amir-massoud Farahmand  Mohammad Ghavamzadeh  Csaba Szepesv´ari  and Shie Mannor.
Regularized ﬁtted Q-iteration for planning in continuous-space markovian decision problems.
In Proceedings of American Control Conference (ACC)  pages 725–730  June 2009.

[4] R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of

Machine Learning Research  9:815–857  2008.

[5] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine

Learning Research  4:1107–1149  2003.

[6] Steven J. Bradtke and Andrew G. Barto. Linear least-squares algorithms for temporal differ-

ence learning. Machine Learning  22:33–57  1996.

[7] Andr´as Antos  Csaba Szepesv´ari  and R´emi Munos. Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine
Learning  71:89–129  2008.

[8] Odalric Maillard  R´emi Munos  Alessandro Lazaric  and Mohammad Ghavamzadeh. Finite-
sample analysis of bellman residual minimization. In Proceedings of the Second Asian Con-
ference on Machine Learning (ACML)  2010.

[9] Amir-massoud Farahmand  Mohammad Ghavamzadeh  Csaba Szepesv´ari  and Shie Mannor.
Regularized policy iteration. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors 
Advances in Neural Information Processing Systems 21  pages 441–448. MIT Press  2009.

[10] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares tempo-
ral difference learning. In ICML ’09: Proceedings of the 26th Annual International Conference
on Machine Learning  pages 521–528  New York  NY  USA  2009. ACM.

[11] Xin Xu  Dewen Hu  and Xicheng Lu. Kernel-based least squares policy iteration for reinforce-

ment learning. IEEE Trans. on Neural Networks  18:973–992  2007.

[12] Tobias Jung and Daniel Polani. Least squares SVM for least squares TD learning. In In Proc.

17th European Conference on Artiﬁcial Intelligence  pages 499–503  2006.

[13] Gavin Taylor and Ronald Parr. Kernelized value function approximation for reinforcement
learning. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine
Learning  pages 1017–1024  New York  NY  USA  2009. ACM.

[14] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A Laplacian framework
for learning representation and control in markov decision processes. Journal of Machine
Learning Research  8:2169–2231  2007.

[15] Alborz Geramifard  Michael Bowling  Michael Zinkevich  and Richard S. Sutton. iLSTD: El-
igibility traces and convergence analysis. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors 
Advances in Neural Information Processing Systems 19  pages 441–448. MIT Press  Cam-
bridge  MA  2007.

[16] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and

Neural Computation Series  3). Athena Scientiﬁc  1996.

[17] R´emi Munos. Performance bounds in lp norm for approximate value iteration. SIAM Journal

on Control and Optimization  2007.

[18] R´emi Munos. Error bounds for approximate policy iteration. In ICML 2003: Proceedings of

the 20th Annual International Conference on Machine Learning  2003.

[19] Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete-Time

Case. Academic Press  1978.

[20] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive

Computation and Machine Learning). The MIT Press  1998.

[21] Csaba Szepesv´ari. Algorithms for Reinforcement Learning. Morgan Claypool Publishers 

2010.

[22] L´aszl´o Gy¨orﬁ  Michael Kohler  Adam Krzy˙zak  and Harro Walk. A Distribution-Free Theory

of Nonparametric Regression. Springer Verlag  New York  2002.

9

,Shahin Shahrampour
Sasha Rakhlin
Ali Jadbabaie
Mehryar Mohri
Andres Munoz