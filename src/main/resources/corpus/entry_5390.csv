2018,Synthesized Policies for Transfer and Adaptation across Tasks and Environments,The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper  we consider the problem of learning to simultaneously transfer across both environments and tasks  probably more importantly  by learning from only sparse (environment  task) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from  environment and task embeddings. Notably  one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings  making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GridWorld and THOR  of which the agent takes as input an egocentric view  show that our approach gives rise to high success rates on all the (environment  task) pairs after learning from only 40% of them.,Synthesized Policies for Transfer and Adaptation

across Tasks and Environments

Hexiang Hu ∗

Liyu Chen ∗

University of Southern California

University of Southern California

Los Angeles  CA 90089
hexiangh@usc.edu

Los Angeles  CA 90089

liyuc@usc.edu

Boqing Gong
Tencent AI Lab

Bellevue  WA 98004

boqinggo@outlook.com

Fei Sha †
Netﬂix

Los Angeles  CA 90028
fsha@netflix.com

Abstract

The ability to transfer in reinforcement learning is key towards building an agent
of general artiﬁcial intelligence. In this paper  we consider the problem of learning
to simultaneously transfer across both environments (ε) and tasks (τ)  probably
more importantly  by learning from only sparse (ε  τ) pairs out of all the possible
combinations. We propose a novel compositional neural network architecture
which depicts a meta rule for composing policies from environment and task
embeddings. Notably  one of the main challenges is to learn the embeddings jointly
with the meta rule. We further propose new training methods to disentangle the
embeddings  making them both distinctive signatures of the environments and
tasks and effective building blocks for composing the policies. Experiments on
GRIDWORLD and THOR  of which the agent takes as input an egocentric view 
show that our approach gives rise to high success rates on all the (ε  τ) pairs after
learning from only 40% of them.

1

Introduction

Remarkable progress has been made in reinforcement learning in the last few years [16  21  26].
Among these  an agent learns to discover its best policy of actions to accomplish a task  by interacting
with the environment. However  the skills the agent learns are often tied for a speciﬁc pair of
the environment (ε) and the task (τ). Consequently  when the environment changes even slightly 
the agent’s performance deteriorates drastically [11  28]. Thus  being able to swiftly adapt to new
environments and transfer skills to new tasks is crucial for the agents to act in real-world settings.
How can we achieve swift adaptation and transfer? In this paper  we consider several progressively
difﬁcult settings. In the ﬁrst setting  the agent needs to adapt and transfer to a new pair of environ-
ment and task  when the agent has been exposed to the environment and the task before (but not
simultaneously). Our goal is to use as few as possible seen pairs (i.e.  a subset out of all possible (ε 
τ) combinations  as sparse as possible) to train the agent.
In the second setting  the agent needs to adapt and transfer across either environments or tasks  to
those previously unseen by the agent. For instance  a home service robot needs to adapt from one
home to another one but essentially accomplish the same sets of tasks  or the robot learns new tasks
in the same home. In the third setting  the agent has encountered neither the environment nor the task

∗Equal Contribution.
†On leave from University of Southern California (feisha@usc.edu).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: We consider a transfer learning scenario in reinforcement learning that considers transfer in both task
and environment. Three different settings are presented here (see text for details). The red dots denote SEEN
combinations  gray dots denote UNSEEN combinations  and arrows → denote transfer directions.

before. Intuitively  the second and the third settings are much more challenging than the ﬁrst one
and appear to be intractable. Thus  the agent is allowed to have a very limited amount of learning
data in the target environment and/or task  for instance  from one demonstration  in order to transfer
knowledge from its prior learning.
Figure 1 schematically illustrates the three settings. Several existing approaches have been proposed
to address some of those settings [1–3  14  17  24  25]; for a detailed discussion  see related works
in Section 2. A common strategy behind these works is to jointly learn through multi-task (rein-
forcement) learning [9  18  25]. Despite many progresses  however  adaptation and transfer remain a
challenging problem in reinforcement learning where a powerful learning agent easily overﬁts to the
environment or the task it has encountered  leading to poor generalization to new ones [11  28].
In this paper  we propose a new approach to tackle this challenge. Our main idea is to learn a meta
rule to synthesize policies whenever the agent encounters new environments or tasks. Concretely 
the meta rule uses the embeddings of the environment and the task to compose a policy  which is
parameterized as the linear combination of the policy basis. On the training data from seen pairs of
environments and tasks  our algorithm learns the embeddings as well as the policy basis. For new
environments or tasks  the agent learns the corresponding embeddings only while it holds the policy
basis ﬁxed. Since the embeddings are low-dimensional  a limited amount of training data in the new
environment or task is often adequate to learn well so as to compose the desired policy.
While deep reinforcement learning algorithms are capable of memorizing and thus entangling
representations of tasks and environments [28]  we propose a disentanglement objective such that
the embeddings for the tasks and the environments can be extracted to maximize the efﬁcacy of the
synthesized policy. Empirical studies demonstrate the importance of disentangling the representations.
We evaluated our approach on GRIDWORLD which we have created and the photo-realistic robotic
environment THOR [13]. We compare to several leading methods for transfer learning in a signiﬁcant
number of settings. The proposed approach outperforms most of them noticeably in improving the
effectiveness of transfer and adaptation.

2 Related Work

Multi-task [27] and transfer learning [24] for reinforcement learning (RL) have been long and
extensively studied. Teh et al. [25] presented a distillation based method that transfers the knowledge
from task speciﬁc agents to a multi-task learning agent. Andreas et al. [1] combined the option
framework [23] and modular network [2]  and presented an efﬁcient multi-task learning approach
which shares sub-policies across policy sketches of different tasks. Schaul et al. [19] encoded the goal
state into value functions and showed its generalization to new goals. More recently  Oh et al. [17]
proposed to learn a meta controller along with a set of parameterized policies to compose a policy
that generalizes to unseen instructions. In contrast  we jointly consider the tasks and environments
which can be both atomic  as we learn their embeddings without resorting to any external knowledge
(e.g.  text  attributes  etc.).
Several recent works [3  6  14  29] factorize Q value functions with an environment-agnostic state-
action feature encoding function and task-speciﬁc embeddings. Our model is related to this line of

2

UnseenSeenMenvsNtasks(c) Transfer Setting 3NtasksMenvs(b) Transfer Setting 2Menvs(a) Transfer Setting 1Ntaskswork in spirit. However  as opposed to learning the value functions  we directly learn a factorized
policy network with strengthened disentanglement between environments and tasks. This allows us
to easily generalize better to new environments or tasks  as shown in the empirical studies.

3 Approach

We begin by introducing notations and stating the research problem formally. We then describe the
main idea behind our approach  followed by the details of each component of the approach.

3.1 Problem Statement and Main Idea

Problem statement. We follow the standard framework for reinforcement learning [22]. An agent
interacts with an environment by sequentially choosing actions over time and aims to maximize its
cumulative rewards. This learning process is abstractly described by a Markov decision process with
the following components: a space of the agent’s state s ∈ S  a space of possible actions a ∈ A 
an initial distribution of states p0(s)  a stationary distribution characterizing how the state at time t
transitions to the next state at (t + 1): p(st+1|st  at)  and a reward function r := r(s  a).
lative reward: R = E[(cid:80)∞
The agent’s actions follow a policy π(a|s) : S × A → [0  1]  deﬁned as a conditional distribution
p(a|s). The goal of the learning is to identify the optimal policy that maximizes the discounted cumu-
t=0 γtr(st  at)]  where γ ∈ (0  1] is a discount factor and the expectation is
π. With it  we deﬁne the discounted state distribution as ρπ(s) =(cid:80)
s(cid:48)(cid:80)∞
taken with respect to the randomness in state transitions and taking actions. We denote by p(s|s(cid:48)  t  π)
the probability at state s after transitioning t time steps  starting from state s(cid:48) and following the policy
t=1 γt−1p0(s(cid:48))p(s|s(cid:48)  t  π).
In this paper  we study how an agent learns to accomplish a variety of tasks in different environments.
Let E and T denote the sets of the environments and the tasks  respectively. We assume the cases of
ﬁnite sets but it is possible to extend our approach to inﬁnite ones. While the most basic approach
is to learn an optimal policy under each pair (ε  τ ) of environment and task  we are interested in
generalizing to all combinations in (E T )  with interactive learning from a limited subset of (ε  τ )
pairs. Clearly  the smaller the subset is  the more desirable the agent’s generalization capability is.

Main idea.
In the rest of the paper  we refers to the limited subset of pairs as seen pairs or training
pairs and the rest ones as unseen pairs or testing pairs. We assume that the agent does not have
access to the unseen pairs to obtain any interaction data to learn the optimal policies directly. In
computer vision  such problems have been intensively studied in the frameworks of unsupervised
domain adaptation and zero-shot learning  for example  [4 5 8 15]. There are totally |E| × |T | pairs –
our goal is to learn from O(|E| + |T |) training pairs and generalize to all.
Our main idea is to synthesize policies for the unseen pairs of environments and tasks. In particular 
our agent learns two sets of embeddings: one for the environments and the other for the tasks.
Moreover  the agent also learns how to compose policies using such embeddings. Note that learning
both the embeddings and how to compose happens on the training pairs. For the unseen pairs  the
policies are constructed and used right away — if there is interaction data  the policies can be further
ﬁne-tuned. However  even without such interaction data  the synthesized policies still perform well.
To this end  we desire our approach to jointly supply two aspects: a compositional structure of
Synthesized Policies (SYNPO) from environment and task embeddings and a disentanglement
learning objective to learn the embeddings. We refer this entire framework as SYNPO and describe
its details in what follows.

3.2 Policy Factorization and Composition

Given a pair z = (ε  τ ) of an environment ε and a task τ  we denote by eε and eτ their embeddings 
respectively. The policy is synthesized with a bilinear mapping

πz(a|s) ∝ exp(ψT

s U (eε  eτ )φa + bπ)

(1)

where bπ is a scalar bias  and ψs and φa are featurized states and actions (for instances  image
pixels or the feature representations of an image). The bilinear mapping given by the matrix U is

3

Figure 2: Overview of our proposed model. Given a task and an environment  the corresponding embeddings
eε and eτ are retrieved to compose the policy coefﬁcients and reward coefﬁcients. Such coefﬁcients then linearly
combine the shared basis and synthesize a policy (and a reward prediction) for the agent.

parameterized as the linear combination of K basis matrices Θk 

U (eε  eτ ) =

αk(eε  eτ )Θk.

(2)

K(cid:88)

k=1

Note that the combination coefﬁcients depend on the speciﬁc pair of environment and task while the
basis is shared across all pairs. They enable knowledge transfer from the seen pairs to unseen ones.
Analogously  during learning (to be explained in detail in the later section)  we predict the rewards by
modeling them with the same set of basis but different combination coefﬁcients:

(cid:32)(cid:88)

(cid:33)

˜rz(s  a) = ψT

s V (eε  eτ )φa + br = ψT

s

βk(eε  eτ )Θk

φa + br

(3)

k

where br is a scalar bias. Note that similar strategies for learning to predict rewards along with
learning the policies have also been studied in recent works [3  12  29]. We ﬁnd this strategy helpful
too (cf. details in our empirical studies in Section 4).
Figure 2 illustrates the model architecture described above. In this paper  we consider agents that take
egocentric views of the environment  so a convolutional neural network is used to extract the state
features ψs (cf. the bottom left panel of Figure 2). The action features φa are learned as a look-up
table. Other model parameters include the basis Θ  the embeddings eε and eτ in the look-up tables
respectively for the environments and the tasks  and the coefﬁcient functions αk(· ·) and βk(· ·)
for respectively synthesizing the policy and reward predictor. The coefﬁcient functions αk(· ·) and
βk(· ·) are parameterized with one-hidden-layer MLPs with the inputs being the concatenation of eε
and eτ   respectfully.
3.3 Disentanglement of the Embeddings for Environments and Tasks

In SYNPO  both the embeddings and the bilinear mapping are to be learnt. In an alternative but
equivalent form  the policies are formulated as

πz(a|s) ∝ exp

αk(eε  eτ )ψT

s Θkφa + bπ

.

(4)

As the deﬁning coefﬁcients αk are parameterized by a neural network whose inputs and parameters are
both optimized  we need to impose additional structures such that the learned embeddings facilitate
the transfer across environments or tasks. Otherwise  the learning could overﬁt to the seen pairs and
consider each pair in unity  thus leading to poor generalization to unseen pairs.
To this end  we introduce discriminative losses to distinguish different environments or tasks through
s Θkφa} ∈ RK be the state-action representation. For the agent
the agent’s trajectories. Let x = {ψT
interacting with an environment-task pair z = (ε  τ )  we denote its trajectory as {x1  x2 ···   xt  . . .}.
We argue that a good embedding (either eε or eτ ) ought to be able to tell from which environment or

4

(cid:32)(cid:88)

k

(cid:33)

Task DescriptorTask EmbeddingEnvironment DescriptorEnvironmentEmbeddingStateFeature ExtractionAction EmbeddingRewardPredictionPolicyPredictionL2 NormalizeL2 Normalizee"<latexit sha1_base64="o0ScyitlsecfAWzDmpxR2anCJoc=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGKeUCyhNlJbzJkdmadmQ2EkO/w4kERr36MN//GSbIHTSxoKKq66e6KUsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jMo0wzpTQulWRA0KLrFuuRXYSjXSJBLYjIa3M785Qm24ko92nGKY0L7kMWfUOinEbmdENaaGCyW7pbJf8ecgqyTISRly1Lqlr05PsSxBaZmgxrQDP7XhhGrLmcBpsZMZTCkb0j62HZU0QRNO5kdPyblTeiRW2pW0ZK7+npjQxJhxErnOhNqBWfZm4n9eO7PxTTjhMs0sSrZYFGeCWEVmCZAe18isGDtCmebuVsIGVFNmXU5FF0Kw/PIqaVxWAr8S3F+Vqw95HAU4hTO4gACuoQp3UIM6MHiCZ3iFN2/kvXjv3seidc3LZ07gD7zPHzqQkm8=</latexit><latexit sha1_base64="o0ScyitlsecfAWzDmpxR2anCJoc=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGKeUCyhNlJbzJkdmadmQ2EkO/w4kERr36MN//GSbIHTSxoKKq66e6KUsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jMo0wzpTQulWRA0KLrFuuRXYSjXSJBLYjIa3M785Qm24ko92nGKY0L7kMWfUOinEbmdENaaGCyW7pbJf8ecgqyTISRly1Lqlr05PsSxBaZmgxrQDP7XhhGrLmcBpsZMZTCkb0j62HZU0QRNO5kdPyblTeiRW2pW0ZK7+npjQxJhxErnOhNqBWfZm4n9eO7PxTTjhMs0sSrZYFGeCWEVmCZAe18isGDtCmebuVsIGVFNmXU5FF0Kw/PIqaVxWAr8S3F+Vqw95HAU4hTO4gACuoQp3UIM6MHiCZ3iFN2/kvXjv3seidc3LZ07gD7zPHzqQkm8=</latexit><latexit sha1_base64="o0ScyitlsecfAWzDmpxR2anCJoc=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGKeUCyhNlJbzJkdmadmQ2EkO/w4kERr36MN//GSbIHTSxoKKq66e6KUsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jMo0wzpTQulWRA0KLrFuuRXYSjXSJBLYjIa3M785Qm24ko92nGKY0L7kMWfUOinEbmdENaaGCyW7pbJf8ecgqyTISRly1Lqlr05PsSxBaZmgxrQDP7XhhGrLmcBpsZMZTCkb0j62HZU0QRNO5kdPyblTeiRW2pW0ZK7+npjQxJhxErnOhNqBWfZm4n9eO7PxTTjhMs0sSrZYFGeCWEVmCZAe18isGDtCmebuVsIGVFNmXU5FF0Kw/PIqaVxWAr8S3F+Vqw95HAU4hTO4gACuoQp3UIM6MHiCZ3iFN2/kvXjv3seidc3LZ07gD7zPHzqQkm8=</latexit><latexit sha1_base64="o0ScyitlsecfAWzDmpxR2anCJoc=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Bjw4jGKeUCyhNlJbzJkdmadmQ2EkO/w4kERr36MN//GSbIHTSxoKKq66e6KUsGN9f1vb219Y3Nru7BT3N3bPzgsHR03jMo0wzpTQulWRA0KLrFuuRXYSjXSJBLYjIa3M785Qm24ko92nGKY0L7kMWfUOinEbmdENaaGCyW7pbJf8ecgqyTISRly1Lqlr05PsSxBaZmgxrQDP7XhhGrLmcBpsZMZTCkb0j62HZU0QRNO5kdPyblTeiRW2pW0ZK7+npjQxJhxErnOhNqBWfZm4n9eO7PxTTjhMs0sSrZYFGeCWEVmCZAe18isGDtCmebuVsIGVFNmXU5FF0Kw/PIqaVxWAr8S3F+Vqw95HAU4hTO4gACuoQp3UIM6MHiCZ3iFN2/kvXjv3seidc3LZ07gD7zPHzqQkm8=</latexit>e⌧<latexit sha1_base64="b17TtuIqqpVuQMgCnxZngNLs8a8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0M/NbT9xYodUDjlMeJnSgRCwYRSc1ea+LNOuVK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOPrcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6qBXw3uLiu1+zyOIpzAKZxDAFdQg1uoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBnGePMA==</latexit><latexit sha1_base64="b17TtuIqqpVuQMgCnxZngNLs8a8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0M/NbT9xYodUDjlMeJnSgRCwYRSc1ea+LNOuVK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOPrcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6qBXw3uLiu1+zyOIpzAKZxDAFdQg1uoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBnGePMA==</latexit><latexit sha1_base64="b17TtuIqqpVuQMgCnxZngNLs8a8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0M/NbT9xYodUDjlMeJnSgRCwYRSc1ea+LNOuVK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOPrcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6qBXw3uLiu1+zyOIpzAKZxDAFdQg1uoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBnGePMA==</latexit><latexit sha1_base64="b17TtuIqqpVuQMgCnxZngNLs8a8=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0M/NbT9xYodUDjlMeJnSgRCwYRSc1ea+LNOuVK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOPrcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6qBXw3uLiu1+zyOIpzAKZxDAFdQg1uoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBnGePMA==</latexit>↵(e" e⌧)<latexit sha1_base64="9UyHAbOkW8X/xPXTLrLzPRCcQKE=">AAACBnicbVDLSgNBEJyNrxhfUY8iDAYhgoRdEfQY8OIxinlAdgm9k04yZHZ2mZkNhJCTF3/FiwdFvPoN3vwbJ4+DJhY01FR1M90VJoJr47rfTmZldW19I7uZ29re2d3L7x/UdJwqhlUWi1g1QtAouMSq4UZgI1EIUSiwHvZvJn59gErzWD6YYYJBBF3JO5yBsVIrf+yDSHpQxJY/AIWJ5iKW59Q+DaRnrXzBLblT0GXizUmBzFFp5b/8dszSCKVhArRuem5ighEow5nAcc5PNSbA+tDFpqUSItTBaHrGmJ5apU07sbIlDZ2qvydGEGk9jELbGYHp6UVvIv7nNVPTuQ5GXCapQclmH3VSQU1MJ5nQNlfIjBhaAkxxuytlPVDAjE0uZ0PwFk9eJrWLkueWvLvLQvl+HkeWHJETUiQeuSJlcksqpEoYeSTP5JW8OU/Oi/PufMxaM8585pD8gfP5A7OBmKk=</latexit><latexit sha1_base64="9UyHAbOkW8X/xPXTLrLzPRCcQKE=">AAACBnicbVDLSgNBEJyNrxhfUY8iDAYhgoRdEfQY8OIxinlAdgm9k04yZHZ2mZkNhJCTF3/FiwdFvPoN3vwbJ4+DJhY01FR1M90VJoJr47rfTmZldW19I7uZ29re2d3L7x/UdJwqhlUWi1g1QtAouMSq4UZgI1EIUSiwHvZvJn59gErzWD6YYYJBBF3JO5yBsVIrf+yDSHpQxJY/AIWJ5iKW59Q+DaRnrXzBLblT0GXizUmBzFFp5b/8dszSCKVhArRuem5ighEow5nAcc5PNSbA+tDFpqUSItTBaHrGmJ5apU07sbIlDZ2qvydGEGk9jELbGYHp6UVvIv7nNVPTuQ5GXCapQclmH3VSQU1MJ5nQNlfIjBhaAkxxuytlPVDAjE0uZ0PwFk9eJrWLkueWvLvLQvl+HkeWHJETUiQeuSJlcksqpEoYeSTP5JW8OU/Oi/PufMxaM8585pD8gfP5A7OBmKk=</latexit><latexit sha1_base64="9UyHAbOkW8X/xPXTLrLzPRCcQKE=">AAACBnicbVDLSgNBEJyNrxhfUY8iDAYhgoRdEfQY8OIxinlAdgm9k04yZHZ2mZkNhJCTF3/FiwdFvPoN3vwbJ4+DJhY01FR1M90VJoJr47rfTmZldW19I7uZ29re2d3L7x/UdJwqhlUWi1g1QtAouMSq4UZgI1EIUSiwHvZvJn59gErzWD6YYYJBBF3JO5yBsVIrf+yDSHpQxJY/AIWJ5iKW59Q+DaRnrXzBLblT0GXizUmBzFFp5b/8dszSCKVhArRuem5ighEow5nAcc5PNSbA+tDFpqUSItTBaHrGmJ5apU07sbIlDZ2qvydGEGk9jELbGYHp6UVvIv7nNVPTuQ5GXCapQclmH3VSQU1MJ5nQNlfIjBhaAkxxuytlPVDAjE0uZ0PwFk9eJrWLkueWvLvLQvl+HkeWHJETUiQeuSJlcksqpEoYeSTP5JW8OU/Oi/PufMxaM8585pD8gfP5A7OBmKk=</latexit><latexit sha1_base64="9UyHAbOkW8X/xPXTLrLzPRCcQKE=">AAACBnicbVDLSgNBEJyNrxhfUY8iDAYhgoRdEfQY8OIxinlAdgm9k04yZHZ2mZkNhJCTF3/FiwdFvPoN3vwbJ4+DJhY01FR1M90VJoJr47rfTmZldW19I7uZ29re2d3L7x/UdJwqhlUWi1g1QtAouMSq4UZgI1EIUSiwHvZvJn59gErzWD6YYYJBBF3JO5yBsVIrf+yDSHpQxJY/AIWJ5iKW59Q+DaRnrXzBLblT0GXizUmBzFFp5b/8dszSCKVhArRuem5ighEow5nAcc5PNSbA+tDFpqUSItTBaHrGmJ5apU07sbIlDZ2qvydGEGk9jELbGYHp6UVvIv7nNVPTuQ5GXCapQclmH3VSQU1MJ5nQNlfIjBhaAkxxuytlPVDAjE0uZ0PwFk9eJrWLkueWvLvLQvl+HkeWHJETUiQeuSJlcksqpEoYeSTP5JW8OU/Oi/PufMxaM8585pD8gfP5A7OBmKk=</latexit>(e" e⌧)<latexit sha1_base64="R8vh5kuP0bbCsmePTJOyeZXopkg=">AAACBnicbVDLSgNBEJyN7/iKehRhMAgRJOyKoEfBi0cV84AkhN5JJxkyO7vM9AZCyMmLv+LFgyJe/QZv/o2TmIMmFjTUVHUz3RUmSlry/S8vs7C4tLyyupZd39jc2s7t7JZtnBqBJRGr2FRDsKikxhJJUlhNDEIUKqyEvauxX+mjsTLW9zRIsBFBR8u2FEBOauYO6iES8AI2630wmFipYn3C3ZMgPW7m8n7Rn4DPk2BK8myKm2bus96KRRqhJqHA2lrgJ9QYgiEpFI6y9dRiAqIHHaw5qiFC2xhOzhjxI6e0eDs2rjTxifp7YgiRtYModJ0RUNfOemPxP6+WUvuiMZQ6SQm1+PmonSpOMR9nwlvSoCA1cASEkW5XLrpgQJBLLutCCGZPnifl02LgF4Pbs/zl3TSOVbbPDlmBBeycXbJrdsNKTLAH9sRe2Kv36D17b977T2vGm87ssT/wPr4BPKOYXw==</latexit><latexit sha1_base64="R8vh5kuP0bbCsmePTJOyeZXopkg=">AAACBnicbVDLSgNBEJyN7/iKehRhMAgRJOyKoEfBi0cV84AkhN5JJxkyO7vM9AZCyMmLv+LFgyJe/QZv/o2TmIMmFjTUVHUz3RUmSlry/S8vs7C4tLyyupZd39jc2s7t7JZtnBqBJRGr2FRDsKikxhJJUlhNDEIUKqyEvauxX+mjsTLW9zRIsBFBR8u2FEBOauYO6iES8AI2630wmFipYn3C3ZMgPW7m8n7Rn4DPk2BK8myKm2bus96KRRqhJqHA2lrgJ9QYgiEpFI6y9dRiAqIHHaw5qiFC2xhOzhjxI6e0eDs2rjTxifp7YgiRtYModJ0RUNfOemPxP6+WUvuiMZQ6SQm1+PmonSpOMR9nwlvSoCA1cASEkW5XLrpgQJBLLutCCGZPnifl02LgF4Pbs/zl3TSOVbbPDlmBBeycXbJrdsNKTLAH9sRe2Kv36D17b977T2vGm87ssT/wPr4BPKOYXw==</latexit><latexit sha1_base64="R8vh5kuP0bbCsmePTJOyeZXopkg=">AAACBnicbVDLSgNBEJyN7/iKehRhMAgRJOyKoEfBi0cV84AkhN5JJxkyO7vM9AZCyMmLv+LFgyJe/QZv/o2TmIMmFjTUVHUz3RUmSlry/S8vs7C4tLyyupZd39jc2s7t7JZtnBqBJRGr2FRDsKikxhJJUlhNDEIUKqyEvauxX+mjsTLW9zRIsBFBR8u2FEBOauYO6iES8AI2630wmFipYn3C3ZMgPW7m8n7Rn4DPk2BK8myKm2bus96KRRqhJqHA2lrgJ9QYgiEpFI6y9dRiAqIHHaw5qiFC2xhOzhjxI6e0eDs2rjTxifp7YgiRtYModJ0RUNfOemPxP6+WUvuiMZQ6SQm1+PmonSpOMR9nwlvSoCA1cASEkW5XLrpgQJBLLutCCGZPnifl02LgF4Pbs/zl3TSOVbbPDlmBBeycXbJrdsNKTLAH9sRe2Kv36D17b977T2vGm87ssT/wPr4BPKOYXw==</latexit><latexit sha1_base64="R8vh5kuP0bbCsmePTJOyeZXopkg=">AAACBnicbVDLSgNBEJyN7/iKehRhMAgRJOyKoEfBi0cV84AkhN5JJxkyO7vM9AZCyMmLv+LFgyJe/QZv/o2TmIMmFjTUVHUz3RUmSlry/S8vs7C4tLyyupZd39jc2s7t7JZtnBqBJRGr2FRDsKikxhJJUlhNDEIUKqyEvauxX+mjsTLW9zRIsBFBR8u2FEBOauYO6iES8AI2630wmFipYn3C3ZMgPW7m8n7Rn4DPk2BK8myKm2bus96KRRqhJqHA2lrgJ9QYgiEpFI6y9dRiAqIHHaw5qiFC2xhOzhjxI6e0eDs2rjTxifp7YgiRtYModJ0RUNfOemPxP6+WUvuiMZQ6SQm1+PmonSpOMR9nwlvSoCA1cASEkW5XLrpgQJBLLutCCGZPnifl02LgF4Pbs/zl3TSOVbbPDlmBBeycXbJrdsNKTLAH9sRe2Kv36D17b977T2vGm87ssT/wPr4BPKOYXw==</latexit>a<latexit sha1_base64="el6Py6GIBQlOm5FdMp3Rgvsgg2g=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJfROJsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqyhpUCaXbERomuGQNy61g7UQzjCPBWtH4Zua3npg2XMkHO0lYGONQ8gGnaJ3U7CYj3sNeueJX/TnIKglyUoEc9V75q9tXNI2ZtFSgMZ3AT2yYobacCjYtdVPDEqRjHLKOoxJjZsJsfu2UnDmlTwZKu5KWzNXfExnGxkziyHXGaEdm2ZuJ/3md1A6uw4zLJLVM0sWiQSqIVWT2OulzzagVE0eQau5uJXSEGql1AZVcCMHyy6ukeVEN/Gpwd1mp3edxFOEETuEcAriCGtxCHRpA4RGe4RXePOW9eO/ex6K14OUzx/AH3ucPiNaPIw==</latexit><latexit sha1_base64="el6Py6GIBQlOm5FdMp3Rgvsgg2g=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJfROJsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqyhpUCaXbERomuGQNy61g7UQzjCPBWtH4Zua3npg2XMkHO0lYGONQ8gGnaJ3U7CYj3sNeueJX/TnIKglyUoEc9V75q9tXNI2ZtFSgMZ3AT2yYobacCjYtdVPDEqRjHLKOoxJjZsJsfu2UnDmlTwZKu5KWzNXfExnGxkziyHXGaEdm2ZuJ/3md1A6uw4zLJLVM0sWiQSqIVWT2OulzzagVE0eQau5uJXSEGql1AZVcCMHyy6ukeVEN/Gpwd1mp3edxFOEETuEcAriCGtxCHRpA4RGe4RXePOW9eO/ex6K14OUzx/AH3ucPiNaPIw==</latexit><latexit sha1_base64="el6Py6GIBQlOm5FdMp3Rgvsgg2g=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJfROJsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqyhpUCaXbERomuGQNy61g7UQzjCPBWtH4Zua3npg2XMkHO0lYGONQ8gGnaJ3U7CYj3sNeueJX/TnIKglyUoEc9V75q9tXNI2ZtFSgMZ3AT2yYobacCjYtdVPDEqRjHLKOoxJjZsJsfu2UnDmlTwZKu5KWzNXfExnGxkziyHXGaEdm2ZuJ/3md1A6uw4zLJLVM0sWiQSqIVWT2OulzzagVE0eQau5uJXSEGql1AZVcCMHyy6ukeVEN/Gpwd1mp3edxFOEETuEcAriCGtxCHRpA4RGe4RXePOW9eO/ex6K14OUzx/AH3ucPiNaPIw==</latexit><latexit sha1_base64="el6Py6GIBQlOm5FdMp3Rgvsgg2g=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJfROJsmY2ZllZlYIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgQ31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVqyhpUCaXbERomuGQNy61g7UQzjCPBWtH4Zua3npg2XMkHO0lYGONQ8gGnaJ3U7CYj3sNeueJX/TnIKglyUoEc9V75q9tXNI2ZtFSgMZ3AT2yYobacCjYtdVPDEqRjHLKOoxJjZsJsfu2UnDmlTwZKu5KWzNXfExnGxkziyHXGaEdm2ZuJ/3md1A6uw4zLJLVM0sWiQSqIVWT2OulzzagVE0eQau5uJXSEGql1AZVcCMHyy6ukeVEN/Gpwd1mp3edxFOEETuEcAriCGtxCHRpA4RGe4RXePOW9eO/ex6K14OUzx/AH3ucPiNaPIw==</latexit> s<latexit sha1_base64="vesXAMwppxL8jPVUL4wO79X4Crs=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOepMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVphg2mhNLtiBoUXGLDciuwnWqkSSSwFY1uZn7rCbXhSj7YcYphQgeSx5xR66RmNzW8Z3rlil/15yCrJMhJBXLUe+Wvbl+xLEFpmaDGdAI/teGEasuZwGmpmxlMKRvRAXYclTRBE07m107JmVP6JFbalbRkrv6emNDEmHESuc6E2qFZ9mbif14ns/F1OOEyzSxKtlgUZ4JYRWavkz7XyKwYO0KZ5u5WwoZUU2ZdQCUXQrD88ippXlQDvxrcXVZq93kcRTiBUziHAK6gBrdQhwYweIRneIU3T3kv3rv3sWgtePnMMfyB9/kDtOuPQA==</latexit><latexit sha1_base64="vesXAMwppxL8jPVUL4wO79X4Crs=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOepMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVphg2mhNLtiBoUXGLDciuwnWqkSSSwFY1uZn7rCbXhSj7YcYphQgeSx5xR66RmNzW8Z3rlil/15yCrJMhJBXLUe+Wvbl+xLEFpmaDGdAI/teGEasuZwGmpmxlMKRvRAXYclTRBE07m107JmVP6JFbalbRkrv6emNDEmHESuc6E2qFZ9mbif14ns/F1OOEyzSxKtlgUZ4JYRWavkz7XyKwYO0KZ5u5WwoZUU2ZdQCUXQrD88ippXlQDvxrcXVZq93kcRTiBUziHAK6gBrdQhwYweIRneIU3T3kv3rv3sWgtePnMMfyB9/kDtOuPQA==</latexit><latexit sha1_base64="vesXAMwppxL8jPVUL4wO79X4Crs=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOepMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVphg2mhNLtiBoUXGLDciuwnWqkSSSwFY1uZn7rCbXhSj7YcYphQgeSx5xR66RmNzW8Z3rlil/15yCrJMhJBXLUe+Wvbl+xLEFpmaDGdAI/teGEasuZwGmpmxlMKRvRAXYclTRBE07m107JmVP6JFbalbRkrv6emNDEmHESuc6E2qFZ9mbif14ns/F1OOEyzSxKtlgUZ4JYRWavkz7XyKwYO0KZ5u5WwoZUU2ZdQCUXQrD88ippXlQDvxrcXVZq93kcRTiBUziHAK6gBrdQhwYweIRneIU3T3kv3rv3sWgtePnMMfyB9/kDtOuPQA==</latexit><latexit sha1_base64="vesXAMwppxL8jPVUL4wO79X4Crs=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF49RzAOSJcxOepMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVphg2mhNLtiBoUXGLDciuwnWqkSSSwFY1uZn7rCbXhSj7YcYphQgeSx5xR66RmNzW8Z3rlil/15yCrJMhJBXLUe+Wvbl+xLEFpmaDGdAI/teGEasuZwGmpmxlMKRvRAXYclTRBE07m107JmVP6JFbalbRkrv6emNDEmHESuc6E2qFZ9mbif14ns/F1OOEyzSxKtlgUZ4JYRWavkz7XyKwYO0KZ5u5WwoZUU2ZdQCUXQrD88ippXlQDvxrcXVZq93kcRTiBUziHAK6gBrdQhwYweIRneIU3T3kv3rv3sWgtePnMMfyB9/kDtOuPQA==</latexit>˜rz(s a)<latexit sha1_base64="uMhnOPvpQd+Wwjt/9j7ViT7Q+GA=">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSxCBSmJCHosePFYxX5AG8JmM22XbjZhd6PU2J/ixYMiXv0l3vw3btsctPXBwOO9GWbmBQlnSjvOt1VYWV1b3yhulra2d3b37PJ+S8WppNCkMY9lJyAKOBPQ1Exz6CQSSBRwaAejq6nfvgepWCzu9DgBLyIDwfqMEm0k3y73NOMhZHLiP1bVKSYnvl1xas4MeJm4OamgHA3f/uqFMU0jEJpyolTXdRLtZURqRjlMSr1UQULoiAyga6ggESgvm50+wcdGCXE/lqaExjP190RGIqXGUWA6I6KHatGbiv953VT3L72MiSTVIOh8UT/lWMd4mgMOmQSq+dgQQiUzt2I6JJJQbdIqmRDcxZeXSeus5jo19+a8Ur/N4yiiQ3SEqshFF6iOrlEDNRFFD+gZvaI368l6sd6tj3lrwcpnDtAfWJ8/fzuThw==</latexit><latexit sha1_base64="uMhnOPvpQd+Wwjt/9j7ViT7Q+GA=">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSxCBSmJCHosePFYxX5AG8JmM22XbjZhd6PU2J/ixYMiXv0l3vw3btsctPXBwOO9GWbmBQlnSjvOt1VYWV1b3yhulra2d3b37PJ+S8WppNCkMY9lJyAKOBPQ1Exz6CQSSBRwaAejq6nfvgepWCzu9DgBLyIDwfqMEm0k3y73NOMhZHLiP1bVKSYnvl1xas4MeJm4OamgHA3f/uqFMU0jEJpyolTXdRLtZURqRjlMSr1UQULoiAyga6ggESgvm50+wcdGCXE/lqaExjP190RGIqXGUWA6I6KHatGbiv953VT3L72MiSTVIOh8UT/lWMd4mgMOmQSq+dgQQiUzt2I6JJJQbdIqmRDcxZeXSeus5jo19+a8Ur/N4yiiQ3SEqshFF6iOrlEDNRFFD+gZvaI368l6sd6tj3lrwcpnDtAfWJ8/fzuThw==</latexit><latexit sha1_base64="uMhnOPvpQd+Wwjt/9j7ViT7Q+GA=">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSxCBSmJCHosePFYxX5AG8JmM22XbjZhd6PU2J/ixYMiXv0l3vw3btsctPXBwOO9GWbmBQlnSjvOt1VYWV1b3yhulra2d3b37PJ+S8WppNCkMY9lJyAKOBPQ1Exz6CQSSBRwaAejq6nfvgepWCzu9DgBLyIDwfqMEm0k3y73NOMhZHLiP1bVKSYnvl1xas4MeJm4OamgHA3f/uqFMU0jEJpyolTXdRLtZURqRjlMSr1UQULoiAyga6ggESgvm50+wcdGCXE/lqaExjP190RGIqXGUWA6I6KHatGbiv953VT3L72MiSTVIOh8UT/lWMd4mgMOmQSq+dgQQiUzt2I6JJJQbdIqmRDcxZeXSeus5jo19+a8Ur/N4yiiQ3SEqshFF6iOrlEDNRFFD+gZvaI368l6sd6tj3lrwcpnDtAfWJ8/fzuThw==</latexit><latexit sha1_base64="uMhnOPvpQd+Wwjt/9j7ViT7Q+GA=">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSxCBSmJCHosePFYxX5AG8JmM22XbjZhd6PU2J/ixYMiXv0l3vw3btsctPXBwOO9GWbmBQlnSjvOt1VYWV1b3yhulra2d3b37PJ+S8WppNCkMY9lJyAKOBPQ1Exz6CQSSBRwaAejq6nfvgepWCzu9DgBLyIDwfqMEm0k3y73NOMhZHLiP1bVKSYnvl1xas4MeJm4OamgHA3f/uqFMU0jEJpyolTXdRLtZURqRjlMSr1UQULoiAyga6ggESgvm50+wcdGCXE/lqaExjP190RGIqXGUWA6I6KHatGbiv953VT3L72MiSTVIOh8UT/lWMd4mgMOmQSq+dgQQiUzt2I6JJJQbdIqmRDcxZeXSeus5jo19+a8Ur/N4yiiQ3SEqshFF6iOrlEDNRFFD+gZvaI368l6sd6tj3lrwcpnDtAfWJ8/fzuThw==</latexit>⇡z(a|s)<latexit sha1_base64="yCwp2OQiK/gaMRFA9L6BRPFQW2Q=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPBi8cq9gPSUDbbTbt0swm7E6HW/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmiTTjDdYIhPdDqnhUijeQIGSt1PNaRxK3gqH11O/9cC1EYm6x1HKg5j2lYgEo2glv5OK7iOp0Cdz1i2V3ao7A1kmXk7KkKPeLX11egnLYq6QSWqM77kpBmOqUTDJJ8VOZnhK2ZD2uW+pojE3wXh28oScWqVHokTbUkhm6u+JMY2NGcWh7YwpDsyiNxX/8/wMo6tgLFSaIVdsvijKJMGETP8nPaE5QzmyhDIt7K2EDaimDG1KRRuCt/jyMmmeVz236t1elGt3eRwFOIYTqIAHl1CDG6hDAxgk8Ayv8Oag8+K8Ox/z1hUnnzmCP3A+fwCAo5DH</latexit><latexit sha1_base64="yCwp2OQiK/gaMRFA9L6BRPFQW2Q=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPBi8cq9gPSUDbbTbt0swm7E6HW/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmiTTjDdYIhPdDqnhUijeQIGSt1PNaRxK3gqH11O/9cC1EYm6x1HKg5j2lYgEo2glv5OK7iOp0Cdz1i2V3ao7A1kmXk7KkKPeLX11egnLYq6QSWqM77kpBmOqUTDJJ8VOZnhK2ZD2uW+pojE3wXh28oScWqVHokTbUkhm6u+JMY2NGcWh7YwpDsyiNxX/8/wMo6tgLFSaIVdsvijKJMGETP8nPaE5QzmyhDIt7K2EDaimDG1KRRuCt/jyMmmeVz236t1elGt3eRwFOIYTqIAHl1CDG6hDAxgk8Ayv8Oag8+K8Ox/z1hUnnzmCP3A+fwCAo5DH</latexit><latexit sha1_base64="yCwp2OQiK/gaMRFA9L6BRPFQW2Q=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPBi8cq9gPSUDbbTbt0swm7E6HW/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmiTTjDdYIhPdDqnhUijeQIGSt1PNaRxK3gqH11O/9cC1EYm6x1HKg5j2lYgEo2glv5OK7iOp0Cdz1i2V3ao7A1kmXk7KkKPeLX11egnLYq6QSWqM77kpBmOqUTDJJ8VOZnhK2ZD2uW+pojE3wXh28oScWqVHokTbUkhm6u+JMY2NGcWh7YwpDsyiNxX/8/wMo6tgLFSaIVdsvijKJMGETP8nPaE5QzmyhDIt7K2EDaimDG1KRRuCt/jyMmmeVz236t1elGt3eRwFOIYTqIAHl1CDG6hDAxgk8Ayv8Oag8+K8Ox/z1hUnnzmCP3A+fwCAo5DH</latexit><latexit sha1_base64="yCwp2OQiK/gaMRFA9L6BRPFQW2Q=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSLUS0lE0GPBi8cq9gPSUDbbTbt0swm7E6HW/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmiTTjDdYIhPdDqnhUijeQIGSt1PNaRxK3gqH11O/9cC1EYm6x1HKg5j2lYgEo2glv5OK7iOp0Cdz1i2V3ao7A1kmXk7KkKPeLX11egnLYq6QSWqM77kpBmOqUTDJJ8VOZnhK2ZD2uW+pojE3wXh28oScWqVHokTbUkhm6u+JMY2NGcWh7YwpDsyiNxX/8/wMo6tgLFSaIVdsvijKJMGETP8nPaE5QzmyhDIt7K2EDaimDG1KRRuCt/jyMmmeVz236t1elGt3eRwFOIYTqIAHl1CDG6hDAxgk8Ayv8Oag8+K8Ox/z1hUnnzmCP3A+fwCAo5DH</latexit>task the trajectory is from. In particular  we formulate this as a multi-way classiﬁcation where we
desire xt (on average) is telltale of its environment ε or task τ:

(cid:88)
(cid:88)

t

t

log P (ε|xt) with P (ε|xt) ∝ exp(cid:0)g(xt)Teε
(cid:1)
log P (τ|xt) with P (τ|xt) ∝ exp(cid:0)h(xt)Teτ
(cid:1)

(cid:96)ε := −

(cid:96)τ := −

(5)

(6)

where we use two nonlinear mapping functions (g(·) and h(·)  parameterized by one-hidden-layer
MLPs) to transform the state-action representation xt  such that it retrieves eε and eτ . These two
functions are also learnt using the interaction data from the seen pairs.

3.4 Learning

Our approach (SYNPO) relies on the modeling assumption that the policies (and the reward predicting
functions) are factorized in the axes of the environment and the task. This is a generic assumption
and can be integrated with many reinforcement learning algorithms. In this paper  we study its
effectiveness on imitation learning (mostly) and also reinforcement learning.
In imitation learning  we denote by πe
z the expert policy of combination z and apply the simple
strategy of “behavior cloning” with random perturbations to learn our model from the expert demon-
stration [10]. We employ a cross-entropy loss for the policy as follows:

(cid:96)πz := −E

s∼ρπe

z  a∼πe

z

[log πz(a|s)]

A (cid:96)2 loss is used for learning the reward prediction function  (cid:96)rz := E
z  a∼πe
rz(s  a)(cid:107)2. Together with the disentanglement losses  they form the overall loss function

s∼ρπe

z(cid:107)˜rz(s  a) −

L := Ez[(cid:96)πz + λ1(cid:96)rz + λ2(cid:96)ε + λ3(cid:96)τ ]

which is then optimized through experience replay  as shown in Algorithm 1 in the supplemen-
tary materials (Suppl. Materials). We choose the value of those hyper-parameters λi so that the
contributions of the objectives are balanced. More details are presented in the Suppl. Materials.

3.5 Transfer to Unseen Environments and Tasks

Eq. 1 is used to synthesize a policy for any (ε  τ) pair  as long as the environment and the task — not
necessarily the pair of them — have appeared at least once in the training pairs. If  however  a new
environment and/or a new task appears (corresponding to the transfer setting 2 or 3 in Section 1) 
ﬁne-tuning is required to extract their embeddings. To do so  we keep all the components of our
model ﬁxed except the look-up tables (i.e.  embeddings) for the environment and/or the task. This
effectively re-uses the policy composition rule and enables fast learning of the environment and/or
the task embeddings  after seeing a few number of demonstrations. In the experiments  we ﬁnd it
works well even with only one shot of the demonstration.

4 Experiments

We validate our approach (SYNPO) with extensive experimental studies  comparing with several
baselines and state-of-the-art transfer learning methods.

4.1 Setup

We experiment with two simulated environments3: GRIDWORLD and THOR [13]  in both of which
the agent takes as input an egocentric view (cf. Figure 3). Please refer to the Suppl. Materials for
more details about the state feature function ψs used in these simulators.
GRIDWORLD and tasks. We design twenty 16 × 16 grid-aligned mazes  some of which are
visualized in Figure 3 (a). The mazes are similar in appearance but differ from each other in topology.
There are ﬁve colored blocks as “treasures” and the agent’s goal is to collect the treasures in pre-
speciﬁed orders  e.g.  “Pick up Red and then pick up Blue”. At a time step  the “egocentric” view

3The implementation of the two simulated environments are available on https://www.github.com/sha-lab/gridworld and

https://www.github.com/sha-lab/thor  respectfully.

5

Figure 3: From left to right: (a) Some sample mazes of our GRIDWORLD dataset. They are similar in appearance
but different in topology. Demonstrations of an agent’s egocentric views of (b) GRIDWORLD and (c) THOR.

observed by the agent consists of the agent’s surrounding within a 3 × 3 window and the treasures’
locations. At each run  the locations of the agent and treasures are randomized. We consider twenty
tasks in each environment  resulting |E| × |T | = 400 pairs of (ε  τ) in total. In the transfer setting
1 (cf. Figure 1(a))  we randomly choose 144 pairs as the training set under the constraint that each
of the environments appears at least once  so does any task. The remaining 256 pairs are used for
testing. For the transfer settings 2 and 3 (cf. Figure 1(b) and (c))  we postpone the detailed setups to
Section 4.2.2.

THOR [13] and tasks. We also test our method on THOR  a challenging 3D simulator where the
agent is placed in indoor photo-realistic scenes. The tasks are to search and act on objects  e.g.  “Put
the cabbage to the fridge”. Different from GRIDWORLD  the objects’ locations are unknown so the
agent has to search for the objects of interest by its understanding of the visual scene (cf. Figure 3(c)).
There are 7 actions in total (look up  look down  turn left  turn right  move forward  open/close 
pick up/put down). We run experiments with 19 scenes × 21 tasks in this simulator.
Evaluations. We evaluate the agent’s performance by the averaged success rate (AvgSR.) for
accomplishing the tasks  limiting the maximum trajectory length to 300 steps. For the results reported
in numbers (e.g.  Tables 1)  we run 100 rounds of experiments for each (ε  τ) pair by randomizing the
agent’s starting point and the treasures’ locations. To plot the convergence curves (e.g.  Figure 4)  we
sample 100 (ε  τ) combinations and run one round of experiment for each to save computation time.
We train our algorithms under 3 random seeds and report the mean and standard deviation (std).

Competing methods. We compare our approach (SYNPO) with the following baselines and com-
peting methods. Note that our problem setup is new  so we have to adapt the competing methods 
which were proposed for other scenarios  to ﬁt ours.
• MLP. The policy network is a multilayer perceptron whose input concatenates state features and
the environment and task embeddings. We train this baseline using the proposed losses for our
approach  including the disentanglement losses (cid:96)  (cid:96)τ ; it performs worse without (cid:96)  (cid:96)τ .
• Successor Feature (SF). We learn the successor feature model [3] by Q-imitation learning for fair
comparison. We strictly follow [14] to set up the learning objectives. The key difference of SF
from our approach is its lack of capability in capturing the environmental priors.
• Module Network (ModuleNet). We also implement a module network following [7]. Here we
train an environment speciﬁc module for each environment and a task speciﬁc module for each
task. The policy for a certain (ε  τ) pair is assembled by combining the corresponding environment
module and task module.
• Multi-Task Reinforcement Learning (MTL). This is a degenerated version of our method  where
we ignore the distinctions of environments. We simply replace the environment embeddings by
zeros for the coefﬁcient functions. The disentanglement loss on task embeddings is still used since
it leads to better performances than otherwise.

Please refer to the Suppl. Materials for more experimental details  including all the twenty GRID-
WORLD mazes  how we conﬁgure the rewards  optimization techniques  feature extraction for the
states  and our implementation of the baseline methods.

4.2 Experimental Results on GRIDWORLD

We ﬁrst report results on the adaptation and transfer learning setting 1  as described in Section 1 and
Figure 1(a). There  the agent acts upon a new pair of environment and task  both of which it has

6

(a) AvgSR. over Time on SEEN

(b) AvgSR. over Time on UNSEEN

Figure 4: On GRIDWORLD. Averaged success rate (AvgSR) on SEEN pairs and UNSEEN pairs  respectively.
Results are reported with |E| = 20 and |T | = 20. We report mean and std based on 3 training random seeds.

(a) Transfer learning performance curve

(b) AvgSR. over Time on UNSEEN

Figure 5: (a) Transfer learning performance (in AvgSR.) with respect to the ratio: # SEEN pairs / # TOTAL
pairs  with |E| = 10 and |T | = 10. (b) Reinforcement learning performance on unseen pairs of different
approaches (with PPO [20]). MLP overﬁts  MTL improves slightly  and SYNPO achieves 96.16% AvgSR.

encountered during training but not in the same (ε  τ) pair. The goal is to use as sparse (ε  τ) pairs
among all the combinations as possible to learn and yet still able to transfer successfully.

4.2.1 Transfer to Previously Encountered Environments and Tasks

Main results. Table 1 and Figure 4 show the success rates and convergence curves  respectively  of
our approach and the competing methods averaged over the seen and unseen (ε  τ) pairs. SYNPO
consistently outperforms the others in terms of both the convergence and ﬁnal performance  by a
signiﬁcant margin. On the seen split  MTL and MLP have similar performances  while MTL performs
worse comparing to MLP on the unseen split (i.e.
in terms of the generalization performance) 
possibly because it treats all the environments the same.
We design an extreme scenario to further challenge the environment-agnostic methods (e.g.  MTL).
We reduce the window size of the agent’s view to one  so the agent sees the cell it resides and the
treasures’ locations and nothing else. As a result  MTL suffers severely  MLP performs moderately
well  and SYNPO outperforms both signiﬁcantly (unseen AvgSR: MTL=6.1%  MLP=66.1%  SYNPO
= 76.8%). We conjecture that the environment information embodied in the states is crucial for the
agent to beware of and generalize across distinct environments. More discussions are deferred to the
Suppl. Materials.

How many seen (ε  τ) pairs do we need to transfer well? Figure 5(a) shows that  not surprisingly 
the transfer learning performance increases as the number of seen pairs increases. The acceleration
slows down after the seen/total ratio reaches 0.4. In other words  when there is a limited budget  our
approach enables the agent to learn from 40% of all possible (ε  τ) pairs and yet generalize well
across the tasks and environments.

Does reinforcement learning help transfer? Beyond imitation learning  we further study our
SYNPO for reinforcement learning (RL) under the same transfer learning setting. Speciﬁcally  we
use PPO [20] to ﬁne-tune the three top performing algorithms on GRIDWORLD. The results averaged
over 3 random seeds are shown in Figure 5(b). We ﬁnd that RL ﬁne-tuning improves the transfer

7

0250005000075000100000125000150000175000200000iteration0.00.20.40.60.8average success rateMLPMTLModuleNetSFSynPo0250005000075000100000125000150000175000200000iteration0.00.20.40.60.8average success rateMLPMTLModuleNetSFSynPo0.10.20.30.40.50.60.70.8# of seen / # of total0.20.40.60.81.0average success rateSeenUnseen0.000.250.500.751.001.251.501.752.00steps1e70.600.650.700.750.800.850.900.95success rate on test setMLPMTLSynPoMethod

Table 1: Performance (AvgSR.) of each method on GRIDWORLD (SEEN/UNSEEN = 144/256).
0.0 ± 0.0% 50.9 ± 33.8% 69.0 ± 2.0% 64.1 ± 1.2% 83.3 ± 0.5 %
0.0 ± 0.0% 30.4 ± 20.1% 66.1 ± 2.6% 41.5 ± 1.4% 82.1 ± 1.5%

AvgSR. (SEEN)

AvgSR. (UNSEEN)

SYNPO

SF

ModuleNet

MLP

MTL

Table 2: Performance of transfer learning in the settings 2 and 3 on GRIDWORLD

Setting Method Cross Pair (Q’s ε  P ’s τ) Cross Pair (P ’s ε  Q’s τ) Q Pairs
Setting 2 MLP
6.3%
13.5%
SYNPO
Setting 3 MLP
7.2%
12.9%
SYNPO

20.7%
21.5%
18.3%
19.4%

13.8%
50.5%
14.6%
42.7%

performance for all the three algorithms. In general  MLP suffers from over-ﬁtting  MTL is improved
moderately yet with a signiﬁcant gap to the best result  and SYNPO achieves the best AvgSR  96.16%.

Ablation studies. We refer readers to the Suppl. Materials for ablation studies of the learning
objectives.

4.2.2 Transfer to Previously Unseen Environments or Tasks

Now we investigate how effectively one can schedule transfer from seen environments and tasks to
unseen ones  i.e.  the settings 2 and 3 described in Section 1 and Figure 1(b) and (c). The seen pairs
(denoted by P ) are constructed from ten environments and ten tasks; the remaining ten environments
and ten tasks are unseen (denoted by Q). Then we have two settings of transfer learning.
One is to transfer to pairs which cross the seen set P and unseen set Q – this corresponds to the
setting 2 as the embeddings for either the unseen tasks or the unseen environments need to be learnt 
but not both. Once these embeddings are learnt  we use them to synthesize policies for the test (ε  τ)
pairs. This mimics the style “incremental learning of small pieces and integrating knowledge later”.
The other is the transfer setting 3. The agent learns policies via learning embeddings for the tasks
and environments of the unseen set Q and then composing  as described in section 3.5. Using the
embeddings from P and Q  we can synthesize policies for any (ε  τ) pair. This mimics the style of
“learning in giant jumps and connecting dots”.

Main results. Table 2 contrasts the results of the two transfer learning settings. Clearly  setting 2
attains stronger performance as it “incrementally learns” the embeddings of either the tasks or the
environments but not both  while setting 3 requires learning both simultaneously. It is interesting to
see this result aligns with how effective human learns.
Figure 6 visualizes the results whose rows are indexed by tasks and columns by environments. The
seen pairs in P are in the upper-left quadrant and the unseen set Q is on the bottom-right. We refer
readers to the Suppl. Materials for more details and discussions of the results.

4.3 Experimental Results on THOR

Main results. The results on the THOR simulator are shown in Table 3  where we report our
approach as well as the top performing ones on GRIDWORLD. Our SYNPO signiﬁcantly outperforms
three competing ones for both seen pairs and unseen pairs. Moreover  our approach also has the best
performance of success rate on seen to unseen  indicating that it is less prone to overﬁting than the
other methods. More details are included in the Suppl. Materials.

5 Conclusion

In this paper  we consider the problem of learning to simultaneously transfer across both environ-
ments (ε) and tasks (τ) under the reinforcement learning framework and  more importantly  by
learning from only sparse (ε  τ) pairs out of all the possible combinations. Speciﬁcally  we present a
novel approach that learns to synthesize policies from the disentangled embeddings of environments
and tasks. We evaluate our approach for the challenging transfer scenarios in two simulators  GRID-

8

(a) Transfer Setting 2

(b) Transfer Setting 3

Figure 6: Transfer results of settings 2 and 3. AvgSRs are marked in the grid (see Suppl. Materials for more
visually discernible plots). The tasks and environments in the purple cells are from the unseen Q set and the red
cells correspond to the rest. Darker color means better performance. It shows that cross-task transfer is easier
than cross-environment.

Table 3: Performance of each method on THOR (SEEN/UNSEEN=144/199)

Method

ModuleNet

AvgSR. (SEEN)

AvgSR. (UNSEEN)

51.5 %
14.4 %

MLP
47.5%
25.8%

MTL
52.2%
33.3%

SYNPO
55.6%
35.4%

WORLD and THOR. Empirical results verify that our method generalizes better across environments
and tasks than several competing baselines.
Acknowledgments We appreciate the feedback from the reviewers. This work is partially supported by DARPA#
FA8750-18-2-0117  NSF IIS-1065243  1451412  1513966/ 1632803/1833137  1208500  CCF-1139148  a
Google Research Award  an Alfred P. Sloan Research Fellowship  gifts from Facebook and Netﬂix  and ARO#
W911NF-12-1-0241 and W911NF-15-1-0484.

References

[1] J. Andreas  D. Klein  and S. Levine. Modular multitask reinforcement learning with policy sketches. In

ICML  2017.

[2] J. Andreas  M. Rohrbach  T. Darrell  and D. Klein. Neural module networks. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition  pages 39–48  2016.

[3] A. Barreto  W. Dabney  R. Munos  J. J. Hunt  T. Schaul  D. Silver  and H. P. van Hasselt. Successor features

for transfer in reinforcement learning. In NIPS  2017.

[4] S. Changpinyo  W.-L. Chao  B. Gong  and F. Sha. Synthesized classiﬁers for zero-shot learning. 2016

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 5327–5336  2016.

[5] W.-L. Chao  S. Changpinyo  B. Gong  and F. Sha. An empirical study and analysis of generalized zero-shot

learning for object recognition in the wild. In ECCV  2016.

[6] P. Dayan. Improving generalization for temporal difference learning: The successor representation. Neural

Computation  5:613–624  1993.

[7] C. Devin  A. Gupta  T. Darrell  P. Abbeel  and S. Levine. Learning modular neural network policies
for multi-task and multi-robot transfer. In Robotics and Automation (ICRA)  2017 IEEE International
Conference on  pages 2169–2176. IEEE  2017.

[8] B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. 2012

IEEE Conference on Computer Vision and Pattern Recognition  pages 2066–2073  2012.

[9] G. Hinton  O. Vinyals  and J. Dean. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531  2015.

[10] J. Ho and S. Ermon. Generative adversarial imitation learning.

Processing Systems  pages 4565–4573  2016.

In Advances in Neural Information

9

Env_0Env_1Env_2Env_3Env_4Env_5Env_6Env_7Env_8Env_9Env_10Env_11Env_12Env_13Env_14Env_15Env_16Env_17Env_18Env_19('R'  'B')('B'  'G')('G'  'O')('O'  'P')('P'  'R')('R'  'G')('B'  'O')('G'  'P')('O'  'R')('P'  'B')('R'  'O')('B'  'P')('G'  'R')('O'  'B')('P'  'G')('R'  'P')('B'  'R')('G'  'B')('O'  'G')('P'  'O')0.901.000.900.901.000.901.000.901.001.000.500.300.400.600.500.300.500.300.600.701.000.901.000.900.901.000.901.000.900.900.300.500.800.500.500.300.500.300.700.601.001.001.001.001.001.001.000.901.000.900.500.400.800.500.400.500.200.300.500.700.901.000.800.901.001.000.900.900.800.900.500.100.800.600.500.700.200.400.400.700.901.001.000.901.000.800.900.800.901.000.400.500.600.500.600.500.300.100.500.401.000.901.000.800.901.001.001.000.901.000.400.200.500.300.600.800.500.500.700.801.001.001.000.801.001.000.900.801.001.000.700.300.800.900.400.300.400.200.400.400.800.800.900.901.001.001.000.901.000.900.300.500.700.600.500.500.600.200.800.401.001.001.000.801.001.000.900.801.001.000.600.600.600.500.600.500.500.200.600.600.901.000.901.000.701.000.800.801.001.000.600.700.800.600.800.600.700.400.400.600.300.000.200.400.100.400.200.100.500.300.200.100.200.200.100.100.000.000.100.000.200.300.400.300.100.400.200.300.100.200.100.000.200.100.200.200.100.000.200.100.100.500.300.100.100.100.300.400.100.400.200.200.100.200.400.200.200.100.200.300.500.200.300.200.100.200.200.500.200.200.200.100.100.100.700.100.200.000.200.100.000.100.200.200.200.100.400.100.200.500.100.000.200.100.100.100.000.100.300.100.400.300.500.400.300.500.400.500.500.300.200.100.200.200.100.100.200.000.300.300.200.400.200.100.400.300.500.000.400.200.200.200.500.400.000.300.300.100.400.300.100.200.200.000.000.000.100.200.200.200.000.000.200.000.100.000.000.000.200.000.500.000.100.000.100.000.000.000.100.300.000.100.000.100.100.100.000.100.100.100.000.000.000.100.000.000.100.000.200.000.100.100.000.100.100.000.200.100.000.00Env_0Env_1Env_2Env_3Env_4Env_5Env_6Env_7Env_8Env_9Env_10Env_11Env_12Env_13Env_14Env_15Env_16Env_17Env_18Env_19('R'  'B')('B'  'G')('G'  'O')('O'  'P')('P'  'R')('R'  'G')('B'  'O')('G'  'P')('O'  'R')('P'  'B')('R'  'O')('B'  'P')('G'  'R')('O'  'B')('P'  'G')('R'  'P')('B'  'R')('G'  'B')('O'  'G')('P'  'O')1.001.001.001.000.900.900.900.900.901.000.400.500.800.200.500.400.500.300.800.001.001.000.900.900.901.000.901.000.901.000.400.300.200.400.600.200.300.100.900.101.000.800.901.000.700.900.901.001.001.000.300.300.700.500.500.400.500.300.700.301.000.800.901.000.800.900.800.901.000.900.500.300.500.400.400.400.400.300.800.300.900.900.801.001.000.800.901.000.901.000.400.800.600.200.200.200.200.400.600.501.001.000.900.700.900.900.800.800.901.000.500.300.600.600.900.200.400.500.600.401.001.001.000.700.900.800.900.900.900.900.400.500.500.300.800.400.300.100.400.400.900.800.900.700.800.800.800.901.000.900.300.400.700.400.200.400.300.300.700.100.800.800.801.001.001.000.800.801.001.000.000.400.700.400.800.400.500.300.700.400.801.001.000.900.900.800.901.000.901.000.300.300.700.400.700.400.300.200.900.200.400.200.300.200.200.500.500.300.200.100.200.100.000.200.200.200.100.100.000.100.200.300.200.300.100.200.100.100.400.200.300.000.100.100.000.100.000.100.400.000.300.200.100.400.100.200.300.100.300.300.100.200.200.000.100.200.100.000.100.100.200.300.200.300.200.000.200.100.100.300.100.200.000.200.100.100.000.000.400.200.100.100.200.200.000.100.100.000.200.200.000.200.000.100.100.100.300.000.000.100.400.200.300.400.500.100.300.100.300.500.400.400.200.100.100.200.100.100.700.400.100.500.200.200.100.200.200.300.300.200.300.300.400.000.300.200.100.000.300.000.400.000.200.000.000.300.100.100.200.000.200.000.200.000.100.200.000.000.000.000.100.000.100.000.100.300.100.000.100.300.200.000.000.100.100.100.000.100.000.100.000.100.000.200.000.200.100.100.300.200.100.000.000.000.400.200.100.000.100.40[11] S. Huang  N. Papernot  I. Goodfellow  Y. Duan  and P. Abbeel. Adversarial attacks on neural network

policies. arXiv preprint arXiv:1702.02284  2017.

[12] M. Jaderberg  V. Mnih  W. Czarnecki  T. Schaul  J. Z. Leibo  D. Silver  and K. Kavukcuoglu. Reinforcement

learning with unsupervised auxiliary tasks. CoRR  abs/1611.05397  2016.

[13] E. Kolve  R. Mottaghi  D. Gordon  Y. Zhu  A. Gupta  and A. Farhadi. Ai2-thor: An interactive 3d

environment for visual ai. CoRR  abs/1712.05474  2017.

[14] T. D. Kulkarni  A. Saeedi  S. Gautam  and S. Gershman. Deep successor reinforcement learning. CoRR 

abs/1606.02396  2016.

[15] I. Misra  A. Gupta  and M. Hebert. From red wine to red tomato: Composition with context. CVPR 2017 

pages 1160–1169  2017.

[16] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. A. Riedmiller 
A. Fidjeland  G. Ostrovski  S. Petersen  C. Beattie  A. Sadik  I. Antonoglou  H. King  D. Kumaran 
D. Wierstra  S. Legg  and D. Hassabis. Human-level control through deep reinforcement learning. Nature 
518:529–533  2015.

[17] J. Oh  S. Singh  H. Lee  and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement

learning. arXiv preprint arXiv:1706.05064  2017.

[18] E. Parisotto  J. L. Ba  and R. Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement

learning. arXiv preprint arXiv:1511.06342  2015.

[19] T. Schaul  D. Horgan  K. Gregor  and D. Silver. Universal value function approximators. In ICML  2015.
[20] J. Schulman  F. Wolski  P. Dhariwal  A. Radford  and O. Klimov. Proximal policy optimization algorithms.

arXiv preprint arXiv:1707.06347  2017.

[21] D. Silver  T. Hubert  J. Schrittwieser  I. Antonoglou  M. Lai  A. Guez  M. Lanctot  L. Sifre  D. Kumaran 
T. Graepel  T. P. Lillicrap  K. Simonyan  and D. Hassabis. Mastering chess and shogi by self-play with a
general reinforcement learning algorithm. CoRR  abs/1712.01815  2017.

[22] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural

Networks  16:285–286  1998.

[23] R. S. Sutton  D. Precup  and S. Singh. Between mdps and semi-mdps: A framework for temporal

abstraction in reinforcement learning. Artiﬁcial intelligence  112(1-2):181–211  1999.

[24] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of

Machine Learning Research  10:1633–1685  2009.

[25] Y. Teh  V. Bapst  W. M. Czarnecki  J. Quan  J. Kirkpatrick  R. Hadsell  N. Heess  and R. Pascanu. Distral:
Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems  pages
4499–4509  2017.

[26] O. Vinyals  T. Ewalds  S. Bartunov  P. Georgiev  A. S. Vezhnevets  M. Yeo  A. Makhzani  H. Küttler 
J. Agapiou  J. Schrittwieser  et al. Starcraft ii: a new challenge for reinforcement learning. arXiv preprint
arXiv:1708.04782  2017.

[27] A. Wilson  A. Fern  S. Ray  and P. Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian
approach. In Proceedings of the 24th international conference on Machine learning  pages 1015–1022.
ACM  2007.

[28] C. Zhang  O. Vinyals  R. Munos  and S. Bengio. A study on overﬁtting in deep reinforcement learning.

arXiv preprint arXiv:1804.06893  2018.

[29] Y. Zhu  D. Gordon  E. Kolve  D. Fox  L. Fei-Fei  A. Gupta  R. Mottaghi  and A. Farhadi. Visual semantic
planning using deep successor representations. In Proceedings of the IEEE International Conference on
Computer Vision  volume 2  page 7  2017.

10

,Hexiang Hu
Liyu Chen
Boqing Gong
Fei Sha
LU LIU
Tianyi Zhou
Guodong Long
Jing Jiang
Chengqi Zhang