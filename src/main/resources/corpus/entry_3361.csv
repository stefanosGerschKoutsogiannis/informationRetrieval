2013,Matrix factorization with binary components,Motivated by an application in computational biology  we consider constrained low-rank matrix factorization problems with $\{0 1\}$-constraints on one of the factors. In addition to the the non-convexity shared with more general matrix factorization schemes  our problem is further complicated by a combinatorial constraint set of size $2^{m \cdot r}$  where $m$ is the dimension of the data points and $r$ the rank of the factorization. Despite apparent intractability  we provide $-$in the line of recent work on non-negative matrix factorization by Arora et al.~(2012)$-$ an algorithm that provably recovers the underlying factorization in the exact case with operations of the order $O(m r 2^r + mnr)$ in the worst case. To obtain that result  we invoke theory centered around a fundamental result in combinatorics  the Littlewood-Offord lemma.,Matrix factorization with Binary Components

Martin Slawski  Matthias Hein and Pavlo Lutsik

Saarland University

{ms hein}@cs.uni-saarland.de  p.lutsik@mx.uni-saarland.de

Abstract

Motivated by an application in computational biology  we consider low-rank ma-
trix factorization with {0  1}-constraints on one of the factors and optionally con-
vex constraints on the second one. In addition to the non-convexity shared with
other matrix factorization schemes  our problem is further complicated by a com-
binatorial constraint set of size 2m·r  where m is the dimension of the data points
and r the rank of the factorization. Despite apparent intractability  we provide
− in the line of recent work on non-negative matrix factorization by Arora et
al. (2012)− an algorithm that provably recovers the underlying factorization in the
exact case with O(mr2r + mnr + r2n) operations for n datapoints. To obtain this
result  we use theory around the Littlewood-Offord lemma from combinatorics.

1 Introduction

Low-rank matrix factorization techniques like the singular value decomposition (SVD) constitute
an important tool in data analysis yielding a compact representation of data points as linear com-
binations of a comparatively small number of ’basis elements’ commonly referred to as factors 
components or latent variables. Depending on the speciﬁc application  the basis elements may be
required to fulﬁll additional properties  e.g. non-negativity [1  2]  smoothness [3] or sparsity [4  5].
In the present paper  we consider the case in which the basis elements are constrained to be binary 
i.e. we aim at factorizing a real-valued data matrix D into a product T A with T ∈ {0  1}m×r and
A ∈ Rr×n  r ≪ min{m  n}. Such decomposition arises e.g. in blind source separation in wire-
less communication with binary source signals [6]; in network inference from gene expression data
[7  8]  where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures
from DNA methylation signatures [9] in which case T represents presence/absence of methylation;
or in clustering with overlapping clusters with T as a matrix of cluster assignments [10  11].
Several other matrix factorizations involving binary matrices have been proposed in the literature. In
[12] and [13] matrix factorization for binary input data  but non-binary factors T and A is discussed 
whereas a factorization T W A with both T and A binary and real-valued W is proposed in [14] 
which is more restrictive than the model of the present paper. The model in [14] in turn encom-
passes binary matrix factorization as proposed in [15]  where all of D  T and A are constrained to
be binary. It is important to note that this ine of research is fundamentally different from Boolean
matrix factorization [16]  which is sometimes also referred to as binary matrix factorization.
A major drawback of matrix factorization schemes is non-convexity. As a result  there is in gen-
eral no algorithm that is guaranteed to compute the desired factorization. Algorithms such as block
coordinate descent  EM  MCMC  etc. commonly employed in practice lack theoretical guarantees
beyond convergence to a local minimum. Substantial progress in this regard has been achieved
recently for non-negative matrix factorization (NMF) by Arora et al. [17] and follow-up work in
[18]  where it is shown that under certain additional conditions  the NMF problem can be solved
globally optimal by means of linear programming. Apart from being a non-convex problem  the
matrix factorization studied in the present paper is further complicated by the {0  1}-constraints im-
posed on the left factor T   which yields a combinatorial optimization problem that appears to be
computationally intractable except for tiny dimensions m and r even in case the right factor A were

1

already known. Despite the obvious hardness of the problem  we present as our main contribution
an algorithm that provably provides an exact factorization D = T A whenever such factorization
exists. Our algorithm has exponential complexity only in the rank r of the factorization  but scales
linearly in m and n. In particular  the problem remains tractable even for large values of m as long
as r remains small. We extend the algorithm to the approximate case D ≈ T A and empirically
show superior performance relative to heuristic approaches to the problem. Moreover  we estab-
lish uniqueness of the exact factorization under the separability condition from the NMF literature
[17  19]  or alternatively with high probability for T drawn uniformly at random. As a corollary  we
obtain that at least for these two models  the suggested algorithm continues to be fully applicable
if additional constraints e.g. non-negativity  are imposed on the right factor A. We demonstrate the
practical usefulness of our approach in unmixing DNA methylation signatures of blood samples [9].
Notation. For a matrix M and index sets I  J   MI J denotes the submatrix corresponding to I and
J ; MI : and M: J denote the submatrices formed by the rows in I respectively columns in J . We
write [M ; M′] and [M  M′] for the row- respectively column-wise concatenation of M and M′. The
afﬁne hull generated by the columns of M is denoted by aff(M ). The symbols 1/0 denote vectors
or matrices of ones/zeroes and I denotes the identity matrix. We use | · | for the cardinality of a set.
Supplement. The supplement contains all proofs  additional comments and experimental results.

2 Exact case

We start by considering the exact case  i.e. we suppose that a factorization having the desired
properties exists. We ﬁrst discuss the geometric ideas underlying our basic approach for recovering
such factorization from the data matrix before presenting conditions under which the factorization
is unique. It is shown that the question of uniqueness as well as the computational performance of
our approach is intimately connected to the Littlewood-Offord problem in combinatorics [20].

2.1 Problem formulation. Given D ∈ Rm×n  we consider the following problem.

ﬁnd T ∈ {0  1}m×r and A ∈ Rr×n  A⊤1r = 1n such that D = T A.

(1)

k=1 of T   which are vertices of the hypercube [0  1]m  are referred to as compo-
The columns {T: k}r
nents. The requirement A⊤1r = 1n entails that the columns of D are afﬁne instead of linear combi-
nations of the columns of T . This additional constraint is not essential to our approach; it is imposed
for reasons of presentation  in order to avoid that the origin is treated differently from the other ver-
tices of [0  1]m  because otherwise the zero vector could be dropped from T   leaving the factorization
unchanged. We further assume w.l.o.g. that r is minimal  i.e. there is no factorization of the form (1)
with r′ < r  and in turn that the columns of T are afﬁnely independent  i.e. ∀λ ∈ Rr  λ⊤1r = 0 
T λ = 0 implies that λ = 0. Moreover  it is assumed that rank(A) = r. This ensures the existence
of a submatrix A: C of r linearly independent columns and of a corresponding submatrix of D: C of
afﬁnely independent columns  when combined with the afﬁne independence of the columns of T :

∀λ ∈ Rr  λ⊤1r = 0 : D: Cλ = 0 ⇐⇒ T (A: Cλ) = 0 =⇒ A: Cλ = 0 =⇒ λ = 0 

(2)

using at the second step that 1⊤r A: Cλ = 1⊤r λ = 0 and the afﬁne independence of the {T: k}r
k=1.
Note that the assumption rank(A) = r is natural; otherwise  the data would reside in an afﬁne
subspace of lower dimension so that D would not contain enough information to reconstruct T .

2.2 Approach. Property (2) already provides the entry point of our approach. From D = T A  it is
obvious that aff(T ) ⊇ aff(D). Since D contains the same number of afﬁnely independent columns
as T   it must also hold that aff(D) ⊇ aff(T )  in particular aff(D) ⊇ {T: k}r
k=1. Consequently  (1)
can in principle be solved by enumerating all vertices of [0  1]m contained in aff(D) and selecting a
maximal afﬁnely independent subset thereof (see Figure 1). This procedure  however  is exponential
in the dimension m  with 2m vertices to be checked for containment in aff(D) by solving a linear
system. Remarkably  the following observation along with its proof  which prompts Algorithm 1
below  shows that the number of elements to be checked can be reduced to 2r−1 irrespective of m.
Proposition 1. The afﬁne subspace aff(D) contains no more than 2r−1 vertices of [0  1]m. More-
over  Algorithm 1 provides all vertices contained in aff(D).

2

Algorithm 1 FINDVERTICES EXACT

1. Fix p ∈ aff(D) and compute P = [D: 1 − p  . . .   D: n − p].
2. Determine r − 1 linearly independent columns C of P   obtaining P: C and subsequently

r − 1 linearly independent rows R  obtaining PR C ∈ Rr−1×r−1.

Rm×2r−1

  where the columns of B(r−1) correspond to the elements of {0  1}r−1.

3. Form Z = P: C(PR C)−1 ∈ Rm×r−1 and bT = Z(B(r−1) − pR1⊤2r−1 ) + p1⊤2r−1 ∈
4. Set T = ∅. For u = 1  . . .   2r−1  if bT: u ∈ {0  1}m set T = T ∪ {bT: u}.

5. Return T = {0  1}m ∩ aff(D).

Algorithm 2 BINARYFACTORIZATION EXACT

1. Obtain T as output from FINDVERTICES EXACT(D)
2. Select r afﬁnely independent elements of T to be used as columns of T .
3. Obtain A as solution of the linear system [1⊤r ; T ]A = [1⊤n ; D].
4. Return (T  A) solving problem (1).

Figure 1: Illustration of the geometry underlying our ap-
proach in dimension m = 3. Dots represent data points
and the shaded areas their afﬁne hulls aff(D) ∩ [0  1]m.
Left: aff(D) intersects with r + 1 vertices of [0  1]m.
Right: aff(D) intersects with precisely r vertices.

Comments. In step 2 of Algorithm 1  determining the rank of P and an associated set of linearly
independent columns/rows can be done by means of a rank-revealing QR factorization [21  22].
The crucial step is the third one  which is a compact description of ﬁrst solving the linear systems
PR Cλ = b − pR for all b ∈ {0  1}r−1 and back-substituting the result to compute candidate vertices

P: Cλ + p stacked into the columns of bT ; the addition/subtraction of p is merely because we have to

deal with an afﬁne instead of a linear subspace  in which p serves as origin. In step 4  the pool of
2r−1 ’candidates’ is ﬁltered  yielding T = aff(D) ∩ {0  1}m.
Determining T is the hardest part in solving the matrix factorization problem (1). Given T   the
solution can be obtained after few inexpensive standard operations. Note that step 2 in Algorithm
2 is not necessary if one does not aim at ﬁnding a minimal factorization  i.e. if it sufﬁces to have
D = T A with T ∈ {0  1}m×r′
As detailed in the supplement  the case without sum-to-one constraints on A can be handled simi-
larly  as can be the model in [14] with binary left and right factor and real-valued middle factor.
Computational complexity. The dominating cost in Algorithm 1 is computation of the candidate

but r′ possibly being larger than r.

matrix bT and checking whether its columns are vertices of [0  1]m. Note that
bTR : = ZR :(B(r−1)−pR1⊤2r−1 )+pR1⊤2r−1 = Ir−1(B(r−1)−pR1⊤2r−1)+pR1⊤2r−1 = B(r−1)  (3)
i.e. the r − 1 rows of bT corresponding to R do not need to be taken into account. Forming the
matrix bT would hence require O((m − r + 1)(r − 1)2r−1) and the subsequent check for vertices in

the fourth step O((m − r + 1)2r−1) operations. All other operations are of lower order provided
e.g. (m − r + 1)2r−1 > n. The second most expensive operation is forming the matrix PR C in
step 2 with the help of a QR decomposition requiring O(mn(r − 1)) operations in typical cases
[21]. Computing the matrix factorization (1) after the vertices have been identiﬁed (steps 2 to 4 in
Algorithm 2) has complexity O(mnr + r3 + r2n). Here  the dominating part is the solution of a
linear system in r variables and n right hand sides. Altogether  our approach for solving (1) has
exponential complexity in r  but only linear complexity in m and n. Later on  we will argue that
under additional assumptions on T   the O((m−r+1)2r−1) terms can be reduced to O((r−1)2r−1).

2.3 Uniqueness.
In this section  we study uniqueness of the matrix factorization problem (1)
(modulo permutation of columns/rows). First note that in view of the afﬁne independence of the
columns of T   the factorization is unique iff T is  which holds iff

(4)
i.e. if the afﬁne subspace generated by {T: 1  . . .   T: r} contains no other vertices of [0  1]m than the
r given ones (cf. Figure 1). Uniqueness is of great importance in applications  where one aims at

aff(D) ∩ {0  1}m = aff(T ) ∩ {0  1}m = {T: 1  . . .   T: r} 

3

an interpretation in which the columns of T play the role of underlying data-generating elements.
Such an interpretation is not valid if (4) fails to hold  since it is then possible to replace one of the
columns of a speciﬁc choice of T by another vertex contained in the same afﬁne subspace.
Solution of a non-negative variant of our factorization. In the sequel  we argue that property (4)
plays an important role from a computational point of view when solving extensions of problem (1)
in which further constraints are imposed on A. One particularly important extension is the following.

ﬁnd T ∈ {0  1}m×r and A ∈ Rr×n

+   A⊤1r = 1n such that D = T A.

(5)

Problem (5) is a special instance of non-negative matrix factorization. Problem (5) is of particular
interest in the present paper  leading to a novel real world application of matrix factorization tech-
niques as presented in Section 4.2 below. It is natural to ask whether Algorithm 2 can be adapted
to solve problem (5). A change is obviously required for the second step when selecting r vertices
from T   since in (5) the columns D now have to be expressed as convex instead of only afﬁne com-
binations of columns of T : picking an afﬁnely independent collection from T does not take into
account the non-negativity constraint imposed on A. If  however  (4) holds  we have |T | = r and
Algorithm 2 must return a solution of (5) provided that there exists one.

Corollary 1. If problem (1) has a unique solution  i.e. if condition (4) holds and if there exists a
solution of (5)  then it is returned by Algorithm 2.

To appreciate that result  consider the converse case |T | > r. Since the aim is a minimal factoriza-
tion  one has to ﬁnd a subset of T of cardinality r such that (5) can be solved. In principle  this can

tionally feasible: the upper bound of Proposition 1 indicates that |T | = 2r−1 in the worst case. For
the example below  T consists of all 2r−1 vertices contained in an r − 1-dimensional face of [0  1]m:

be achieved by solving a linear program for(cid:0)|T |r (cid:1) subsets of T   but this is in general not computa-
T =

 with T =(T λ : λ1 ∈ {0  1}  . . .   λr−1 ∈ {0  1}  λr = 1 −

Ir−1 0r−1

λk) .

r−1Xk=1

(6)

0m−r×r

0⊤
r

Uniqueness under separability.
In view of the negative example (6)  one might ask whether
uniqueness according to (4) can be achieved under additional conditions on T . We prove uniqueness
under separability  a condition introduced in [19] and imposed recently in [17] to show solvability
of the NMF problem by linear programming. We say that T is separable if there exists a permutation
Π such that ΠT = [M ; Ir]  where M ∈ {0  1}m−r×r.
Proposition 2. If T is separable  condition (4) holds and thus problem (1) has a unique solution.

Uniqueness under generic random sampling. Both the negative example (6) as well as the posi-
tive result of Proposition 2 are associated with special matrices T . This raises the question whether
uniqueness holds respectively fails for broader classes of binary matrices. In order to gain insight
into this question  we consider random T with i.i.d. entries from a Bernoulli distribution with param-
eter 1
2 and study the probability of the event {aff(T ) ∩ {0  1}m = {T: 1  . . .   T: r}}. This question
has essentially been studied in combinatorics [23]  with further improvements in [24]. The results
therein rely crucially on Littlewood-Offord theory (see Section 2.4 below).
Theorem 1. Let T be a random m × r-matrix whose entries are drawn i.i.d. from {0  1} with
probability 1

2 . Then  there is a constant C so that if r ≤ m − C 

P(cid:16)aff(T )∩{0  1}m = {T: 1  . . .   T: r(cid:17) ≥ 1−(1+o(1)) 4(cid:18)r

3(cid:19)(cid:18) 3
4(cid:19)m

−(cid:18) 3

4

+ o(1)(cid:19)m

as m → ∞.

Theorem 1 suggests a positive answer to the question of uniqueness posed above. For m large
enough and r small compared to m (in fact  following [24] one may conjecture that Theorem 1
holds with C = 1)  the probability that the afﬁne hull of r vertices of [0  1]m selected uniformly at
random contains some other vertex is exponentially small in the dimension m. We have empirical
evidence that the result of Theorem 1 continues to hold if the entries of T are drawn from a Bernoulli
distribution with parameter in (0  1) sufﬁciently far away from the boundary points (cf. supplement).
As a byproduct  these results imply that also the NMF variant of our matrix factorization problem
(5) can in most cases be reduced to identifying a set of r vertices of [0  1]m (cf. Corollary 1).

4

2.4 Speeding up Algorithm 1.
formed (Step 3). We have discussed the case (6) where all candidates must indeed be vertices 
in which case it seems to be impossible to reduce the computational cost of O((m − r)r2r−1) 
which becomes signiﬁcant once m is in the thousands and r ≥ 25. On the positive side  Theorem
1 indicates that for many instances of T   only r out of 2r−1 candidates are in fact vertices.
In

In Algorithm 1  an m × 2r−1 matrix bT of potential vertices is

with coordinates not in {0  1}. We have observed empirically that this scheme rapidly reduces the

that case  noting that columns of bT cannot be vertices if a single coordinate is not in {0  1} (and
that the vast majority of columns of bT must have one such coordinate)  it is computationally more
favourable to incrementally compute subsets of rows of bT and then to discard already those columns
candidate set − already checking a single row of bT eliminates a substantial portion (see Figure 2).

Littlewood-Offord theory. Theoretical underpinning for the last observation can be obtained from
a result in combinatorics  the Littlewood-Offord (L-O)-lemma. Various extensions of that result have
been developed until recently  see the survey [25]. We here cite the L-O-lemma in its basic form.
Theorem 2. [20] Let a1  . . .   aℓ ∈ R \ {0} and y ∈ R.

i=1 aibi = y}(cid:12)(cid:12) ≤(cid:0) ℓ
(i) (cid:12)(cid:12){b ∈ {0  1}ℓ : Pℓ
⌊ℓ/2⌋(cid:1).
(ii) If |ai| ≥ 1  i = 1  . . .   ℓ  (cid:12)(cid:12){b ∈ {0  1}ℓ : Pℓ

i=1 aibi ∈ (y  y + 1)}(cid:12)(cid:12) ≤(cid:0) ℓ
⌊ℓ/2⌋(cid:1).

The two parts of Theorem 2 are referred to as discrete respectively continuous L-O lemma. The
discrete L-O lemma provides an upper bound on the number of {0  1}-vectors whose weighted
sum with given weights {ai}ℓ
i=1 is equal to some given number y  whereas the stronger continuous
version  under a more stringent condition on the weights  upper bounds the number of {0  1}-vectors
whose weighted sum is contained in some interval (y  y + 1). In order to see the relation of Theorem
2 to Algorithm 1  let us re-inspect the third step of that algorithm. To obtain a reduction of candidates

Zi :

k=1

B(r−1)

= Zi :pR − pi

in R do not need to be checked  cf. (3)) and u ∈ {1  . . .   2r−1} arbitrary. The u-th candidate can be

by checking a single row ofbT = Z(B(r−1)−pR1⊤2r−1 )+p1⊤2r−1   pick i /∈ R (recall that coordinates
a vertex only if bTi u ∈ {0  1}. The condition bTi u = 0 can be written as
: u| {z }=b
|
A similar reasoning applies when setting bTi u = 1. Provided none of the entries of Zi : = 0  the
discrete L-O lemma implies that there are at most 2(cid:0)
⌊(r−1)/2⌋(cid:1) out of 2r−1 candidates for which the
i-th coordinate is in {0  1}. This yields a reduction of the candidate set by 2(cid:0)
⌊(r−1)/2⌋(cid:1)/2r−1 =
O(cid:16) 1√r−1(cid:17). Admittedly  this reduction may appear insigniﬁcant given the total number of candi-

dates to be checked. The reduction achieved empirically (cf. Figure 2) is typically larger. Stronger
reductions have been proven under additional assumptions on the weights {ai}ℓ
i=1: e.g. for distinct
weights  one obtains a reduction of O((r − 1)−3/2) [25]. Furthermore  when picking successively d

|{z}

{ak}r

=y

{z

.

}

(7)

r−1

r−1

rows of bT and if one assumes that each row yields a reduction according to the discrete L-O lemma 

one would obtain the reduction (r − 1)−d/2 so that d = r − 1 would sufﬁce to identify all vertices
provided r ≥ 4. Evidence for the rate (r − 1)−d/2 can be found in [26]. This indicates a reduction
in complexity of Algorithm 1 from O((m − r)r2r−1) to O(r22r−1).
Achieving further speed-up with integer linear programming. The continuous L-O lemma (part
(ii) of Theorem 2) combined with the derivation leading to (7) allows us to tackle even the case
r = 80 (280 ≈ 1024). In view of the continuous L-O lemma  a reduction in the number of candidates

satisfying the relaxed constraint for the i-th coordinate can be obtained from the feasibility problem

can still be achieved if the requirement is weakened to bTi u ∈ [0  1]. According to (7) the candidates

ﬁnd b ∈ {0  1}r−1 subject to 0 ≤ Zi :(b − pR) + pi ≤ 1 

which is an integer linear program that can be solved e.g. by CPLEX. The L-O- theory suggests that
the branch-bound strategy employed therein is likely to be successful. With the help of CPLEX  it

(8)

is affordable to solve problem (8) with all m − r + 1 constraints (one for each of the rows of bT to

be checked) imposed simultaneously. We always recovered directly the underlying vertices in our
experiments and only these  without the need to prune the solution pool (which could be achieved
by Algorithm 1  replacing the 2r−1 candidates by a potentially much smaller solution pool).

5

Maximum number of remaining vertices (out of 2(r−1)) over 100 trials

Speed−up achieved by cplex (m = 1000)

 

2

)

g
o
l
(
 
s
e
c
i
t
r
e
V

 
f

o

 
r
e
b
m
u
N

25

20

15

10

5

 

0
0

1 2

5

 

r=8  p=0.02
r=8  p=0.1
r=8  p=0.5
r=16  p=0.02
r=16  p=0.1
r=16  p=0.5
r=24  p=0.02
r=24  p=0.1
r=24  p=0.5

3

2.5

2

1.5

1

0.5

0

s
d
n
o
c
e
s
 

n

i
 
)
e
m

i
t
 

U
P
C
(
0
1
g
o

l

alg1 p=0.5
alg1 p = 0.1
alg1 p = 0.9
cplex p = 0.5
cplex p = 0.1
cplex p = 0.9
80

70

10
Coordinates checked

20

50

100 200

500

−0.5

 
10

20

30

40

50

60

r

Figure 2: Left: Speeding up the algorithm by checking single coordinates  remaining number of
coordinates vs.# coordinates checked (m = 1000). Right: Speed up by CPLEX compared to
Algorithm 1. For both plots  T is drawn entry-wise from a Bernoulli distribution with parameter p.

3 Approximate case

In the sequel  we discuss an extension of our approach to handle the approximate case D ≈ T A
with T and A as in (1). In particular  we have in mind the case of additive noise i.e. D = T A + E
with kEkF small. While the basic concept of Algorithm 1 can be adopted  changes are necessary
because D may have full rank min{m  n} and second aff(D) ∩ {0  1}m = ∅  i.e. the distances of
aff(D) and the {T: k}r
k=1 may be strictly positive (but are at least assumed to be small). As dis-

Algorithm 3 FINDVERTICES APPROXIMATE

1. Let p = D1n/n and compute P = [D: 1 − p  . . .   D: n − p].
2. Compute U (r−1) ∈ Rm×r−1  the left singular vectors corresponding to the r − 1 largest
singular values of P . Select r − 1 linearly independent rows R of U (r−1)  obtaining
U (r−1)
R : ∈ Rr−1×r−1.

3. Form Z = U (r−1)(U (r−1)

R :

4. Compute bT 01 ∈ Rm×2r−1
5. For u = 1  . . .   2r−1  set δu = kbT: u −bT 01
: u1 . . .bT 01
6. Return T = [bT 01

: ur ]

)−1 and bT = Z(B(r−1) − pR1⊤2r−1 ) + p1⊤2r−1 .
: for u = 1  . . .   2r−1  i = 1  . . .   m  set bT 01

2 ).
: uk2. Order increasingly s.t. δu1 ≤ . . . ≤ δ2r−1 .

i u = I(bTi u > 1

tinguished from the exact case  Algorithm 3 requires the number of components r to be speciﬁed
in advance as it is typically the case in noisy matrix factorization problems. Moreover  the vector
p subtracted from all columns of D in step 1 is chosen as the mean of the data points  which is in
particular a reasonable choice if D is contaminated with additive noise distributed symmetrically
around zero. The truncated SVD of step 2 achieves the desired dimension reduction and potentially
reduces noise corresponding to small singular values that are discarded. The last change arises in

l=1  and subsequently using T = argmin{T (l)} minAkD − T (l)Ak2

step 5. While in the exact case  one identiﬁes all columns of bT that are in {0  1}m  one instead only

identiﬁes columns close to {0  1}m. Given the output of Algorithm 3  we solve the approximate
matrix factorization problem via least squares  obtaining the right factor from minAkD − T Ak2
F .
Reﬁnements. Improved performance for higher noise levels can be achieved by running Algorithm
3 multiple times with different sets of rows selected in step 2  which yields candidate matrices
{T (l)}s
F   i.e. one picks the candi-
date yielding the best ﬁt. Alternatively  we may form a candidate pool by merging the {T (l)}s
l=1 and
then use a backward elimination scheme  in which successively candidates are dropped that yield the
smallest improvement in ﬁtting D until r candidates are left. Apart from that  T returned by Algo-
rithm 3 can be used for initializing the block optimization scheme of Algorithm 4 below. Algorithm
4 is akin to standard block coordinate descent schemes proposed in the matrix factorization litera-
ture  e.g. [27]. An important observation (step 3) is that optimization of T is separable along the
rows of T   so that for small r  it is feasible to perform exhaustive search over all 2r possibilities
(or to use CPLEX). However  Algorithm 4 is impractical as a stand-alone scheme  because with-
out proper initialization  it may take many iterations to converge  with each single iteration being
more expensive than Algorithm 3. When initialized with the output of the latter  however  we have
observed convergence of the block scheme only after few steps.

6

Algorithm 4 Block optimization scheme for solving minT∈{0 1}m×r   A kD − T Ak2
1. Set k = 0 and set T (k) equal to a starting value.
2. A(k) ← argminAkD − T (k)Ak2
3. T (k) ← argminT∈{0 1}m×r kD−T A(k)k2
4. Alternate between steps 2 and 3.

F = argmin{Ti :∈{0 1}r}m

F and set k = k + 1.

F

i=1Pm

i=1kDi :−Ti :A(k)k2

2 (9)

4 Experiments

In Section 4.1 we demonstrate with the help of synthetic data that the approach of Section 3
performs well on noisy datasets. In the second part  we present an application to a real dataset.

4.1 Synthetic data.
Setup. We generate D = T ∗A∗ + αE  where the entries of T ∗ are drawn i.i.d. from {0  1} with
probability 0.5  the columns of A are drawn i.i.d. uniformly from the probability simplex and the
entries of E are i.i.d. standard Gaussian. We let m = 1000  r = 10 and n = 2r and let the noise
level α vary along a grid starting from 0. Small sample sizes n as considered here yield more
challenging problems and are motivated by the real world application of the next subsection.
Evaluation. Each setup is run 20 times and we report averages over the following perfor-
F /(m r) and the two RMSEs
mance measures:
kT ∗A∗ − T AkF /(m n)1/2 and kT A − DkF /(m n)1/2  where (T  A) denotes the output of one of
the following approaches that are compared. FindVertices: our approach in Section 3. oracle: we
solve problem (9) with A(k) = A∗. box: we run the block scheme of Algorithm 4  relaxing the
integer constraint into a box constraint. Five random initializations are used and we take the result
yielding the best ﬁt  subsequently rounding the entries of T to fulﬁll the {0  1}-constraints and

the normalized Hamming distance kT ∗ − T k2

reﬁtting A. quad pen: as box  but a (concave) quadratic penalty λPi k Ti k(1 − Ti k) is added to

push the entries of T towards {0  1}. D.C. programming [28] is used for the block updates of T .

Hamming(T T*) T0.5 r=10

RMSE(TA  T*A*) T0.5 r=10

RMSE(TA  D) T0.5 r=10

0.2

0.15

)
r
*

m

F2

(
/

|
*
T
−
T

|

0.1

0.05

 

0
0

0.2

0.15

)
r
*

m

F2

(
/

|
*
T
−
T

|

0.1

0.05

 

0
0

box
quad pen
oracle
FindVertices

0.05

alpha

0.1

Hamming(T T*) r=10

HotTopixx
FindVertices

 

 

box
quad pen
oracle
FindVertices

0.05

alpha

0.1

RMSE(TA  T*A*) r=10

HotTopixx
FindVertices

 

 

0.2

0.15

0.1

0.05

 

0
0

0.2

0.15

0.1

0.05

box
quad pen
oracle
FindVertices

0.05

alpha

0.1

RMSE(TA  D) r=10

HotTopixx
FindVertices

0.2

0.15

0.1

0.05

 

0
0

0.2

0.15

0.1

0.05

F

)
n
*
m

(
t
r
q
s
/

|

D
−
A
T

|

)
n
*
m

(
t
r
q
s
/

|

D
−
A
T

|

F

F

)
n
*
m

(
t
r
q
s
/

|
*
A
*
T
−
A
T

|

)
n
*
m

(
t
r
q
s
/

|
*
A
*
T
−
A
T

F

0.02

0.04

alpha

0.06

|

 

0
0

0.02

0.04

alpha

0.06

 

0
0

0.02

0.04

alpha

 

 

0.06

Figure 3: Top: comparison against block schemes. Bottom: comparison against HOTTOPIXX.
Left/Middle/Right: kT ∗ − T k2

F /(m r)  kT ∗A∗ − T AkF /(m n)1/2 and kT A − DkF /(m n)1/2.

Comparison to HOTTOPIXX [18]. HOTTOPIXX (HT) is a linear programming approach to
NMF equipped with guarantees such as correctness in the exact and robustness in the non-exact
case as long as T is (nearly) separable (cf. Section 2.3). HT does not require T to be binary  but
applies to the generic NMF problem D ≈ T A  T ∈ Rm×r
+ . Since separability is
crucial to the performance of HT  we restrict our comparison to separable T = [M ; Ir]  generating
the entries of M i.i.d. from a Bernoulli distribution with parameter 0.5. For runtime reasons 
we lower the dimension to m = 100. Apart from that  the experimental setup is as above. We

and A ∈ Rr×n

+

7

+

kD − T Ak2

use an implementation of HT from [29]. We ﬁrst pre-normalize D to have unit row sums as
required by HT  and obtain A as ﬁrst output. Given A  the non-negative least squares problem
F is solved. The entries of T are then re-scaled to match the original scale
minT∈Rm×r
of D  and thresholding at 0.5 is applied to obtain a binary matrix. Finally  A is re-optimized by
solving the above ﬁtting problem with respect to A in place of T . In the noisy case  HT needs a
tuning parameter to be speciﬁed that depends on the noise level  and we consider a grid of 12 values
for that parameter. The range of the grid is chosen based on knowledge of the noise matrix E. For
each run  we pick the parameter that yields best performance in favour of HT.
Results. From Figure 3  we ﬁnd that unlike the other approaches  box does not always recover
T ∗ even if the noise level α = 0. FindVertices outperforms box and quad pen throughout. For
α ≤ 0.06  its performance closely matches that of the oracle. In the separable case  our approach
performs favourably as compared to HT  a natural benchmark in this setting.

4.2 Analysis of DNA methylation data.
Background. Unmixing of DNA methylation proﬁles is a problem of high interest in cancer
research. DNA methylation is a chemical modiﬁcation of the DNA occurring at speciﬁc sites 
so-called CpGs. DNA methylation affects gene expression and in turn various processes such as
cellular differentiation. A site is either unmethylated (’0’) or methylated (’1’). DNA methylation
microarrays allow one to measure the methylation level for thousands of sites.
In the dataset
considered here  the measurements D (the rows corresponding to sites  the columns to samples)
result from a mixture of cell types. The methylation proﬁles of the latter are in {0  1}m  whereas 
depending on the mixture proportions associated with each sample  the entries of D take values in
[0  1]m. In other words  we have the model D ≈ T A  with T representing the methylation of the
cell types and the columns of A being elements of the probability simplex. It is often of interest
to recover the mixture proportions of the samples  because e.g. speciﬁc diseases  in particular
cancer  can be associated with shifts in these proportions. The matrix T is frequently unknown  and
determining it experimentally is costly. Without T   however  recovering the mixing matrix A is
challenging  in particular since the number of samples in typical studies is small.
Dataset. We consider the dataset studied in [9]  with m = 500 CpG sites and n = 12 samples of
blood cells composed of four major types (B-/T-cells  granulocytes  monocytes)  i.e. r = 4. Ground
truth is partially available: the proportions of the samples  denoted by A∗  are known.

A (ground truth)

estimated A

number of components vs error

 

1

t
n
e
n
o
p
m
o
c

1

2

3

4

 

1 2 3 4 5 6 7 8 9 101112

sample

t
n
e
n
o
p
m
o
c

1

2

3

4

 

0.5

0

 

1

0.2

)
n
 
*
 

m

 

FindVertices
ground truth

0.5

0

1 2 3 4 5 6 7 8 9 101112

sample

(
t
r
q
s
 
/
 

F

|

 

 

A
T
−
D

 

|

0.15

 

0.1
2

3
5
components used

4

6

Figure 4: Left: Mixture proportions of the ground truth. Middle: mixture proportions as estimated
by our method. Right: RMSEs kD − T AkF /(m n)1/2 in dependency of r.

Analysis. We apply our approach to obtain an approximate factorization D ≈ T A  T ∈ {0  1}m×r 
and A⊤1r = 1n. We ﬁrst obtained T as outlined in Section 3  replacing {0  1} by
A ∈ Rr×n
{0.1  0.9} in order to account for measurement noise in D that slightly pushes values towards 0.5.

+

This can be accomodated re-scaling bT 01 in step 4 of Algorithm 3 by 0.8 and then adding 0.1. Given

T   we solve the quadratic program A = argminA∈Rr×n
F and compare A to
the ground truth A∗. In order to judge the ﬁt as well as the matrix T returned by our method  we
compute T ∗ = argminT∈{0 1}m×r kD − T A∗k2
F as in (9). We obtain 0.025 as average mean squared
difference of T and T ∗  which corresponds to an agreement of 96 percent. Figure 4 indicates at
least a qualitative agreement of A∗ and A. In the rightmost plot  we compare the RMSEs of our
approach for different choices of r relative to the RMSE of (T ∗  A∗). The error curve ﬂattens after
r = 4  which suggests that with our approach  we can recover the correct number of cell types.

kD − T Ak2

+  A⊤ 1r=1n

8

References

[1] P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor model with optimal utiliza-

tion of error estimates of data values. Environmetrics  5:111–126  1994.

[2] D. Lee and H. Seung. Learning the parts of objects by nonnegative matrix factorization. Nature  401:788–

791  1999.

[3] J. Ramsay and B. Silverman. Functional Data Analysis. Springer  New York  2006.

[4] F. Bach  J. Mairal  and J. Ponce. Convex Sparse Matrix Factorization. Technical report  ENS  Paris  2008.

[5] D. Witten  R. Tibshirani  and T. Hastie. A penalized matrix decomposition  with applications to sparse

principal components and canonical correlation analysis. Biostatistics  10:515–534  2009.

[6] A-J. van der Veen. Analytical Method for Blind Binary Signal Separation.

IEEE Signal Processing 

45:1078–1082  1997.

[7] J. Liao  R. Boscolo  Y. Yang  L. Tran  C. Sabatti  and V. Roychowdhury. Network component analysis:

reconstruction of regulatory signals in biological systems. PNAS  100(26):15522–15527  2003.

[8] S. Tu  R. Chen  and L. Xu. Transcription Network Analysis by a Sparse Binary Factor Analysis Algorithm.

Journal of Integrative Bioinformatics  9:198  2012.

[9] E. Houseman et al. DNA methylation arrays as surrogate measures of cell mixture distribution. BMC

Bioinformatics  13:86  2012.

[10] A. Banerjee  C. Krumpelman  J. Ghosh  S. Basu  and R. Mooney. Model-based overlapping clustering.

In KDD  2005.

[11] E. Segal  A. Battle  and D. Koller. Decomposing gene expression into cellular processes. In Proceedings

of the 8th Paciﬁc Symposium on Biocomputing  2003.

[12] A. Schein  L. Saul  and L. Ungar. A generalized linear model for principal component analysis of binary

data. In AISTATS  2003.

[13] A. Kaban and E. Bingham. Factorisation and denoising of 0-1 data: a variational approach. Neurocom-

puting  71:2291–2308  2008.

[14] E. Meeds  Z. Gharamani  R. Neal  and S. Roweis. Modeling dyadic data with binary latent factors. In

NIPS  2007.

[15] Z. Zhang  C. Ding  T. Li  and X. Zhang. Binary matrix factorization with applications. In IEEE ICDM 

2007.

[16] P. Miettinen and T. Mielik¨ainen and A. Gionis and G. Das and H. Mannila. The discrete basis problem.

In PKDD  2006.

[17] S. Arora  R. Ge  R. Kannan  and A. Moitra. Computing a nonnegative matrix factorization – provably.

STOC  2012.

[18] V. Bittdorf  B. Recht  C. Re  and J. Tropp. Factoring nonnegative matrices with linear programs. In NIPS 

2012.

[19] D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition

into parts? In NIPS  2003.

[20] P. Erd¨os. On a lemma of Littlewood and Offord. Bull. Amer. Math. Soc  51:898–902  1951.

[21] M. Gu and S. Eisenstat. Efﬁcient algorithms for computing a strong rank-revealing QR factorization.

SIAM Journal on Scientiﬁc Computing  17:848–869  1996.

[22] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press  1996.

[23] A. Odlyzko. On Subspaces Spanned by Random Selections of ±1 vectors. Journal of Combinatorial

Theory A  47:124–133  1988.

[24] J. Kahn  J. Komlos  and E. Szemeredi. On the Probability that a ±1 matrix is singular. Journal of the

American Mathematical Society  8:223–240  1995.

[25] H. Nguyen and V. Vu. Small ball probability  Inverse theorems  and applications. arXiv:1301.0019.

[26] T. Tao and V. Vu. The Littlewoord-Offord problem in high-dimensions and a conjecture of Frankl and

F¨uredi. Combinatorica  32:363–372  2012.

[27] C.-J. Lin. Projected gradient methods for non-negative matrix factorization. Neural Computation 

19:2756–2779  2007.

[28] P. Tao and L. An. Convex analysis approach to D.C. programming: theory  algorithms and applications.

Acta Mathematica Vietnamica  pages 289–355  1997.

[29] https://sites.google.com/site/nicolasgillis/publications.

9

,Martin Slawski
Matthias Hein
Pavlo Lutsik
Julian Zimmert
Yevgeny Seldin