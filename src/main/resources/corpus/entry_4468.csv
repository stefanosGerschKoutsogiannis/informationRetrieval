2012,Provable ICA with Unknown Gaussian Noise  with Implications for Gaussian Mixtures and Autoencoders,We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular  suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1  -1\}^n$  $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this  we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm  one that has been overlooked in previous attempts  and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search.,Provable ICA with Unknown Gaussian Noise  with

Implications for Gaussian Mixtures and Autoencoders

Sanjeev Arora∗

Rong Ge∗

Ankur Moitra †

Sushant Sachdeva∗

Abstract

We present a new algorithm for Independent Component Analysis (ICA) which
has provable performance guarantees. In particular  suppose we are given samples
of the form y = Ax + η where A is an unknown n × n matrix and x is a random
variable whose components are independent and have a fourth moment strictly
less than that of a standard Gaussian random variable and η is an n-dimensional
Gaussian random variable with unknown covariance Σ: We give an algorithm that
provable recovers A and Σ up to an additive  and whose running time and sam-
ple complexity are polynomial in n and 1/. To accomplish this  we introduce
a novel “quasi-whitening” step that may be useful in other contexts in which the
covariance of Gaussian noise is not known in advance. We also give a general
framework for ﬁnding all local optima of a function (given an oracle for approx-
imately ﬁnding just one) and this is a crucial step in our algorithm  one that has
been overlooked in previous attempts  and allows us to control the accumulation
of error when we ﬁnd the columns of A one by one via local search.

Introduction

1
We present an algorithm (with rigorous performance guarantees) for a basic statistical problem.
Suppose η is an independent n-dimensional Gaussian random variable with an unknown covariance
matrix Σ and A is an unknown n × n matrix. We are given samples of the form y = Ax + η where
x is a random variable whose components are independent and have a fourth moment strictly less
than that of a standard Gaussian random variable. The most natural case is when x is chosen uni-
formly at random from {+1 −1}n  although our algorithms in even the more general case above.
Our goal is to reconstruct an additive approximation to the matrix A and the covariance matrix Σ
running in time and using a number of samples that is polynomial in n and 1
   where  is the target
precision (see Theorem 1.1) This problem arises in several research directions within machine learn-
ing: Independent Component Analysis (ICA)  Deep Learning  Gaussian Mixture Models (GMM) 
etc. We describe these connections next  and known results (focusing on algorithms with provable
performance guarantees  since that is our goal).
Most obviously  the above problem can be seen as an instance of Independent Component Analysis
(ICA) with unknown Gaussian noise. ICA has an illustrious history with applications ranging from
econometrics  to signal processing  to image segmentation. The goal generally involves ﬁnding a
linear transformation of the data so that the coordinates are as independent as possible [1  2  3]. This
is often accomplished by ﬁnding directions in which the projection is “non-Gaussian” [4]. Clearly 
if the datapoint y is generated as Ax (i.e.  with no noise η added) then applying linear transformation
A−1 to the data results in samples A−1y whose coordinates are independent. This restricted case
was considered by Comon [1] and Frieze  Jerrum and Kannan [5]  and their goal was to recover an
∗{arora  rongge  sachdeva}@cs.princeton.edu. Department of Computer Science  Princeton University 
Princeton NJ 08540. Research supported by the NSF grants CCF-0832797  CCF-1117309 and Simons Investi-
gator Grant
†moitra@ias.edu. School of Mathematics  Institute for Advanced Study  Princeton NJ 08540. Research

supported in part by NSF grant No. DMS-0835373 and by an NSF Computing and Innovation Fellowship.

1

additive approximation to A efﬁciently and using a polynomial number of samples. (We will later
note a gap in their reasoning  albeit ﬁxable by our methods. See also recent papers by Anandkumar
et al.  Hsu and Kakade[6  7]  that do not use local search and avoids this issue.) To the best of our
knowledge  there are currently no known algorithms with provable guarantees for the more general
case of ICA with Gaussian noise (this is especially true if the covariance matrix is unknown  as in
our problem)  although many empirical approaches are known. (eg. [8]  the issue of “empirical” vs
“rigorous” is elaborated upon after Theorem 1.1.)
The second view of our problem is as a concisely described Gaussian Mixture Model. Our data is
generated as a mixture of 2n identical Gaussian components (with an unknown covariance matrix)
whose centers are the points {Ax : x ∈ {−1  1}n}  and all mixing weights are equal. Notice  this
mixture of 2n Gaussians admits a concise description using O(n2) parameters. The problem of
learning Gaussian mixtures has a long history  and the popular approach in practice is to use the
EM algorithm [9]  though it has no worst-case guarantees (the method may take a very long time
to converge  and worse  may not always converge to the correct solution). An inﬂuential paper of
Dasgupta [10] initiated the program of designing algorithms with provable guarantees  which was
improved in a sequence of papers [11  12  13  14]. But in the current setting  it is unclear how to
apply any of the above algorithms (including EM) since the trivial application would keep track
of exponentially many parameters – one for each component. Thus  new ideas seem necessary to
achieve polynomial running time.
The third view of our problem is as a simple form of autoencoding [15]. This is a central notion in
Deep Learning  where the goal is to obtain a compact representation of a target distribution using a
multilayered architecture  where a complicated function (the target) can be built up by composing
layers of a simple function (called the autoencoder [16]). The main tenet is that there are interest-
ing functions which can be represented concisely using many layers  but would need a very large
representation if a “shallow” architecture is used instead). This is most useful for functions that
are “highly varying” (i.e. cannot be compactly described by piecewise linear functions or other
“simple” local representations). Formally  it is possible to represent using just (say) n2 parameters 
some distributions with 2n “varying parts” or “interesting regions.” The Restricted Boltzmann Ma-
chine (RBM) is an especially popular autoencoder in Deep Learning  though many others have been
proposed. However  to the best of our knowledge  there has been no successful attempt to give a
rigorous analysis of Deep Learning. Concretely  if the data is indeed generated using the distribu-
tion represented by an RBM  then do the popular algorithms for Deep Learning [17] learn the model
parameters correctly and in polynomial time? Clearly  if the running time were actually found to
be exponential in the number of parameters  then this would erode some of the advantages of the
compact representation.
How is Deep Learning related to our problem? As noted by Freund and Haussler [18] many years
ago  an RBM with real-valued visible units (the version that seems more amenable to theoretical
analysis) is precisely a mixture of exponentially many standard Gaussians. It is parametrized by an
n × m matrix A and a vector θ ∈ Rn. It encodes a mixture of n-dimensional standard Gaussians
centered at the points {Ax : x ∈ {−1  1}m}  where the mixing weight of the Gaussian centered at
2 + θ · x). This is of course reminiscent of our problem. Formally  our algorithm
Ax is exp((cid:107)Ax(cid:107)2
can be seen as a nonlinear autoencoding scheme analogous to an RBM but with uniform mixing
weights. Interestingly  the algorithm that we present here looks nothing like the approaches favored
traditionally in Deep Learning  and may provide an interesting new perspective.

1.1 Our results and techniques
We give a provable algorithm for ICA with unknown Gaussian noise. We have not made an attempt
to optimize the quoted running time of this model  but we emphasize that this is in fact the ﬁrst
algorithm with provable guarantees for this problem and moreover we believe that in practice our
algorithm will run almost as fast as the usual ICA algorithms  which are its close relatives.
Theorem 1.1 (Main  Informally). There is an algorithm that recovers the unknown A and Σ up to
additive error  in each entry in time that is polynomial in n (cid:107)A(cid:107)2 (cid:107)Σ(cid:107)2  1/  1/λmin(A) where (cid:107) · (cid:107)2
denotes the operator norm and λmin(·) denotes the smallest eigenvalue.

The classical approach for ICA initiated in Comon [1] and Frieze  Jerrum and Kannan [5]) is for
the noiseless case in which y = Ax. The ﬁrst step is whitening  which applies a suitable linear
transformation that makes the variance the same in all directions  thus reducing to the case where

2

A is a rotation matrix. Given samples y = Rx where R is a rotation matrix  the rows of R can be
found in principle by computing the vectors u that are local minima of E[(u · y)4]. Subsequently  a
number of works (see e.g. [19  20]) have focused on giving algorithms that are robust to noise. A
popular approach is to use the fourth order cumulant (as an alternative to the fourth order moment)
as a method for “denoising ” or any one of a number of other functionals whose local optima reveal
interesting directions. However  theoretical guarantees of these algorithms are not well understood.
The above procedures in the noise-free model can almost be made rigorous (i.e.  provably polyno-
mial running time and number of samples)  except for one subtlety: it is unclear how to use local
search to ﬁnd all optima in polynomial time. In practice  one ﬁnds a single local optimum  projects
to the subspace orthogonal to it and continues recursively on a lower-dimensional problem. How-
ever  a naive implementation of this idea is unstable since approximation errors can accumulate
badly  and to the best of our knowledge no rigorous analysis has been given prior to our work. (This
is not a technicality: in some similar settings the errors are known to blow up exponentially [21].)
One of our contributions is a modiﬁed local search that avoids this potential instability and ﬁnds all
local optima in this setting. (Section 4.2.)
Our major new contribution however is dealing with noise that is an unknown Gaussian. This is an
important generalization  since many methods used in ICA are quite unstable to noise (and a wrong
estimate for the covariance could lead to bad results). Here  we no longer need to assume we know
even rough estimates for the covariance. Moreover  in the context of Gaussian Mixture Models this
generalization corresponds to learning a mixture of many Gaussians where the covariance of the
components is not known in advance.
We design new tools for denoising and especially whitening in this setting. Denoising uses the fourth
order cumulant instead of the fourth moment used in [5] and whitening involves a novel use of the
Hessian of the cumulant. Even then  we cannot reduce to the simple case y = Rx as above  and are
left with a more complicated functional form (see “quasi-whitening” in Section 2.) Nevertheless 
we can reduce to an optimization problem that can be solved via local search  and which remains
amenable to a rigorous analysis. The results of the local optimization step can be then used to
simplify the complicated functional form and recover A as well as the noise Σ. We defer many of
our proofs to the supplementary material section  due to space constraints.
In order to avoid cluttered notation  we have focused on the case in which x is chosen uniformly at
random from {−1  +1}n  although our algorithm and analysis work under the more general con-
ditions that the coordinates of x are (i) independent and (ii) have a fourth moment that is less
than three (the fourth moment of a Gaussian random variable). In this case  the functional P (u)
(see Lemma 2.2) will take the same form but with weights depending on the exact value of the
fourth moment for each coordinate. Since we already carry through an unknown diagonal matrix D
throughout our analysis  this generalization only changes the entries on the diagonal and the same
algorithm and proof apply.
2 Denoising and quasi-whitening
As mentioned  our approach is based on the fourth order cumulant. The cumulants of a random
variable are the coefﬁcients of the Taylor expansion of the logarithm of the characteristic function
[22]. Let κr(X) be the rth cumulant of a random variable X. We make use of:
Fact 2.1. (i) If X has mean zero  then κ4(X) = E[X 4]− 3 E[X 2]2. (ii) If X is Gaussian with mean
µ and variance σ2  then κ1(X) = µ  κ2(X) = σ2 and κr(X) = 0 for all r > 2. (iii) If X and Y
are independent  then κr(X + Y ) = κr(X) + κr(Y ).

The crux of our technique is to look at the following functional  where y is the random variable
Ax + η whose samples are given to us. Let u ∈ Rn be any vector. Then P (u) = −κ4(uT y).
Note that for any u we can compute P (u) reasonably accurately by drawing sufﬁcient number of
samples of y and taking an empirical average. Furthermore  since x and η are independent  and η is
Gaussian  the next lemma is immediate. We call it “denoising” since it allows us empirical access
to some information about A that is uncorrupted by the noise η.

Lemma 2.2 (Denoising Lemma). P (u) = 2(cid:80)n

i .
i=1(uT A)4

The intuition is that P (u) = −κ4(uT Ax) since the fourth cumulant does not depend on the additive
Gaussian noise  and then the lemma follows from completing the square.

3

2.1 Quasi-whitening via the Hessian of P (u)
In prior works on ICA  whitening refers to reducing to the case where y = Rx for some some
rotation matrix R. Here we give a technique to reduce to the case where y = RDx + η(cid:48) where η(cid:48)
is some other Gaussian noise (still unknown)  R is a rotation matrix and D is a diagonal matrix that
depends upon A. We call this quasi-whitening. Quasi-whitening sufﬁces for us since local search
using the objective function κ4(uT y) will give us (approximations to) the rows of RD  from which
we will be able to recover A.
Quasi-whitening involves computing the Hessian of P (u)  which recall is the matrix of all 2nd order
partial derivatives of P (u). Throughout this section  we will denote the Hessian operator by H. In
matrix form  the Hessian of P (u) is

Ai kAj k(Ak · u)2; H(P (U )) = 24

(Ak · u)2AkAT

k = ADA(u)AT

∂2

∂ui∂uj

P (u) = 24

n(cid:88)

k=1

n(cid:88)

k=1

where Ak is the k-th column of the matrix A (we use subscripts to denote the columns of matrices
throught the paper). DA(u) is the following diagonal matrix:
Deﬁnition 2.3. Let DA(u) be a diagonal matrix in which the kth entry is 24(Ak · u)2.
Of course  the exact Hessian of P (u) is unavailable and we will instead compute an empirical

approximation (cid:98)P (u) to P (u) (given many samples from the distribution)  and we will show that the
Hessian of (cid:98)P (u) is a good approximation to the Hessian of P (u).

Deﬁnition 2.4. Given 2N samples y1  y(cid:48)

1  y2  y(cid:48)

2...  yN   y(cid:48)

N(cid:88)

(cid:98)P (u) =

−1
N

N(cid:88)

3
N

N of the random variable y  let
(uT yi)2(uT y(cid:48)

i=1

i=1

i)2.

(uT yi)4 +

Our ﬁrst step is to show that the expectation of the Hessian of (cid:98)P (u) is exactly the Hessian of P (u).
In fact  since the expectation of (cid:98)P (u) is exactly P (u) (and since (cid:98)P (u) is an analytic function of the

samples and of the vector u)  we can interchange the Hessian operator and the expectation operator.
Roughly  one can imagine the expectation operator as an integral over the possible values of the
random samples  and as is well-known in analysis  one can differentiate under the integral provided
that all functions are suitably smooth over the domain of integration.
Claim 2.5. Ey y(cid:48)[−(uT y)4 + 3(uT y)2(uT y(cid:48))2] = P (u)
This claim follows immediately from the deﬁnition of P (u)  and since y and y(cid:48) are independent.
Lemma 2.6. H(P (u)) = Ey y(cid:48)[H(−(uT y)4 + 3(uT y)2(uT y(cid:48))2)]
Next  we compute the two terms inside the expectation:
Claim 2.7. H((uT y)4) = 12(uT y)2yyT
Claim 2.8. H((uT y)2(uT y(cid:48))2) = 2(uT y(cid:48))2yyT + 2(uT y)2y(cid:48)(y(cid:48))T + 4(uT y)(uT y(cid:48))(y(y(cid:48))T +
(y(cid:48))yT )
Let λmin(A) denote the smallest eigenvalue of A. Our analysis also requires bounds on the entries
of DA(u0):
Claim 2.9. If u0 is chosen uniformly at random then with high probability for all i 

2

n

n

log n

min
i=1

(cid:107)Ai(cid:107)2

(cid:107)Ai(cid:107)2

Lemma 2.10.
If u0 is chosen uniformly at random and furthermore we are given 2N =
poly(n  1/  1/λmin(A) (cid:107)A(cid:107)2 (cid:107)Σ(cid:107)2) samples of y  then with high probability we will have that

2n−4 ≤ DA(u0))i i ≤ n
max
i=1
(1 − )ADA(u0)AT (cid:22) H((cid:98)P (u0)) (cid:22) (1 + )ADA(u0)AT .
Lemma 2.11. Suppose that (1− )ADA(u0)AT (cid:22) (cid:99)M (cid:22) (1 + )ADA(u0)AT   and let(cid:99)M = BBT .
Then there is a rotation matrix R∗ such that (cid:107)B−1ADA(u0)1/2 − R∗(cid:107)F ≤ √
we can ﬁnd a unit vector x where the quadratic forms xT ADA(u0)AT x and xT(cid:99)M x are too far apart
The intuition is: if any of the singular values of B−1ADA(u0)1/2 are outside the range [1−   1 + ] 
(which contradicts the condition of the lemma). Hence the singular values of B−1ADA(u0)1/2 can
all be set to one without changing the Froebenius norm of B−1ADA(u0)1/2 too much  and this
yields a rotation matrix.

n.

4

3 Our algorithm (and notation)
In this section we describe our overall algorithm. It uses as a blackbox the denoising and quasi-
whitening already described above  as well as a routine for computing all local maxima of some
“well-behaved” functions which is described later in Section 4.
Notation: Placing a hat over a function corresponds to an empirical approximation that we obtain
from random samples. This approximation introduces error  which we will keep track of.

Step 1: Pick a random u0 ∈ Rn and estimate the Hessian H((cid:98)P (u0)). Compute B such that
H((cid:98)P (u0)) = BBT . Let D = DA(u0) be the diagonal matrix deﬁned in Deﬁnition 2.3.
(cid:80)N
(cid:16)(cid:80)N
i=1(uT B−1yi)4 +
Step 3: Use the procedure ALLOPT((cid:98)P (cid:48)(u)  β  δ(cid:48)  β(cid:48)  δ(cid:48)) of Section 4 to compute all n local maxima
of the function (cid:98)P (cid:48)(u).

N   and let (cid:98)P (cid:48)(u) = − 1

which is an empirical estimation of P (cid:48)(u).

Step 2: Take 2N samples y1  y2  ...  yN   y(cid:48)

1  y(cid:48)

2  ...  y(cid:48)

N

3
N

i=1(uT B−1yi)2(uT B−1y(cid:48)

i)2(cid:17)

Step 4: Let R be the matrix whose rows are the n local optima recovered in the previous step. Use
procedure RECOVER of Section 5 to ﬁnd A and Σ.
Explanation: Step 1 uses the transformation B−1 computed in the previous Section to quasi-whiten
the data. Namely  we consider the sequence of samples z = B−1y  which are therefore of the form
R(cid:48)Dx+η(cid:48) where η = B−1η  D = DA(u0) and R(cid:48) is close to a rotation matrix R∗ (by Lemma 2.11).
In Step 2 we look at κ4((uT z))  which effectively denoises the new samples (see Lemma 2.2)  and
thus is the same as κ4(R(cid:48)D−1/2x). Let P (cid:48)(u) = κ4(uT z) = κ4(uT B−1y) which is easily seen to be
optima via local search. Ideally we would have liked access to the functional P ∗(u) = (uT R∗x)4
since the procedure for local optima works only for true rotations. But since R(cid:48) and R∗ are close

E[(uT R(cid:48)D−1/2x)4]. Step 2 estimates this function  obtaining (cid:98)P (cid:48)(u). Then Step 3 tries to ﬁnd local
we can make it work approximately with (cid:98)P (cid:48)(u)  and then in Step 4 use these local optima to ﬁnally

recover A.
Theorem 3.1. Suppose we are given samples of the form y = Ax + η where x is uniform on
{+1 −1}n  A is an n × n matrix  η is an n-dimensional Gaussian random variable independent
of x with unknown covariance matrix Σ. There is an algorithm that with high probability recovers

(cid:107)(cid:98)A − AΠdiag(ki)(cid:107)F ≤  where Π is some permutation matrix and each ki ∈ {+1 −1} and
also recovers (cid:107)(cid:98)Σ − Σ(cid:107)F ≤ . Furthermore the running time and number of samples needed are
poly(n  1/ (cid:107)A(cid:107)2  (cid:107)Σ(cid:107)2   1/λmin(A))
Note that here we recover A up to a permutation of the columns and sign-ﬂips. In general  this is
all we can hope for since the distribution of x is also invariant under these same operations. Also 
the dependence of our algorithm on the various norms (of A and Σ) seems inherent since our goal is
to recover an additive approximation  and as we scale up A and/or Σ  this goal becomes a stronger
relative guarantee on the error.

4 Framework for iteratively ﬁnding all local maxima
In this section  we ﬁrst describe a fairly standard procedure (based upon Newton’s method) for
ﬁnding a single local maximum of a function f∗ : Rn → R among all unit vectors and an analysis
of its rate of convergence. Such a procedure is a common tool in statistical algorithms  but here we
state it rather carefully since we later give a general method to convert any local search algorithm
(that meets certain criteria) into one that ﬁnds all local maxima (see Section 4.2).
Given that we can only ever hope for an additive approximation to a local maximum  one should
be concerned about how the error accumulates when our goal is to ﬁnd all local maxima. In fact  a
naive strategy is to project onto the subspace orthogonal to the directions found so far  and continue
in this subspace. However  such an approach seems to accumulate errors badly (the additive error
of the last local maxima found is exponentially larger than the error of the ﬁrst). Rather  the crux
of our analysis is a novel method for bounding how much the error can accumulate (by reﬁning old
estimates).

5

Algorithm 1. LOCALOPT  Input:f (u)  us  β  δ Output: vector v

1. Set u ← us.
2. Maximize (via Lagrangian methods) Proj⊥u(∇f (u))T ξ + 1

(cid:107)ξ(cid:107)2

2 Subject to (cid:107)ξ(cid:107)2 ≤ β(cid:48) and uT ξ = 0

2 ξT Proj⊥u(H(f (u)))ξ − 1

2

(cid:16) ∂

∂u

(cid:17)·

f (u)

3. Let ξ be the solution  ˜u = u+ξ
(cid:107)u+ξ(cid:107)
4. If f (˜u) ≥ f (u) + δ/2  set u ← ˜u and Repeat Step 2
5. Else return u

Our strategy is to ﬁrst ﬁnd a local maximum in the orthogonal subspace  then run the local optimiza-
tion algorithm again (in the original n-dimensional space) to “reﬁne” the local maximum we have
found. The intuition is that since we are already close to a particular local maxima  the local search
algorithm cannot jump to some other local maxima (since this would entail going through a valley).

4.1 Finding one local maximum
Throughout this section  we will assume that we are given oracle access to a function f (u) and its
gradient and Hessian. The procedure is also given a starting point us  a search range β  and a step
size δ. For simplicity in notation we deﬁne the following projection operator.
Deﬁnition 4.1. Proj⊥u(v) = v − (uT v)u  Proj⊥u(M ) = M − (uT M u)uuT .
The basic step the algorithm is a modiﬁcation of Newton’s method to ﬁnd a local improvement
that makes progress so long as the current point u is far from a local maxima. Notice that if we
add a small vector to u  we do not necessarily preserve the norm of u. In order to have control
over how the norm of u changes  during local optimization step the algorithm projects the gradient
∇f and Hessian H(f ) to the space perpendicular to u. There is also an additional correction term
−∂/∂uf (u) · (cid:107)ξ(cid:107)2/2. This correction term is necessary because the new vector we obtain is (u +
ξ)/(cid:107)(u + ξ)(cid:107)2 which is close to u − (cid:107)ξ(cid:107)2
2/2 · u + ξ + O(β3). Step 2 of the algorithm is just
maximizing a quadratic function and can be solved exactly using Lagrangian Multiplier method. To
increase efﬁciency it is also acceptable to perform an approximate maximization step by taking ξ to
be either aligned with the gradient Proj⊥u∇f (u) or the largest eigenvector of Proj⊥u(H(f (u))).
The algorithm is guaranteed to succeed in polynomial time when the function is Locally Improvable
and Locally Approximable:
Deﬁnition 4.2 ((γ  β  δ)-Locally Improvable). A function f (u) : Rn → R is (γ  β  δ)-Locally
Improvable  if for any u that is at least γ far from any local maxima  there is a u(cid:48) such that
(cid:107)u(cid:48) − u(cid:107)2 ≤ β and f (u(cid:48)) ≥ f (u) + δ.
Deﬁnition 4.3 ((β  δ)-Locally Approximable). A function f (u) is locally approximable  if its third
order derivatives exist and for any u and any direction v  the third order derivative of f at point u in
the direction of v is bounded by 0.01δ/β3.

The analysis of the running time of the procedure comes from local Taylor expansion. When a
function is Locally Approximable it is well approximated by the gradient and Hessian within a β
neighborhood. The following theorem from [5] showed that the two properties above are enough to
guarantee the success of a local search algorithm even when the function is only approximated.
Theorem 4.4 ([5]). If |f (u) − f∗(u)| ≤ δ/8  the function f∗(u) is (γ  β  δ)-Locally Improvable 
f (u) is (β  δ) Locally Approximable  then Algorithm 1 will ﬁnd a vector v that is γ close to some
local maximum. The running time is at most O((n2 + T ) max f∗/δ) where T is the time to evaluate
the function f and its gradient and Hessian.

4.2 Finding all local maxima
Now we consider how to ﬁnd all local maxima of a given function f∗(u). The crucial condition that
we need is that all local maxima are orthogonal (which is indeed true in our problem  and is morally
true when using local search more generally in ICA). Note that this condition implies that there are
at most n local maxima.1 In fact we will assume that there are exactly n local maxima. If we are
given an exact oracle for f∗ and can compute exact local maxima then we can ﬁnd all local maxima

6

Algorithm 2. ALLOPT  Input:f (u)  β  δ  β(cid:48)  δ(cid:48) Output: v1  v2  ...  vn  ∀i (cid:107)vi − v∗

i (cid:107) ≤ γ.

1. Let v1 = LOCALOPT(f  e1  β  δ)
2. FOR i = 2 TO n DO
3.
4.
5.
6. END FOR
7. Return v1  v2  ...  vn

Let gi be the projection of f to the orthogonal subspace of v1  v2  ...  vi−1.
Let u(cid:48) = LOCALOPT(g  e1  β(cid:48)  δ(cid:48)).
Let vi = LOCALOPT(f  u(cid:48)  β  δ).

easily: ﬁnd one local maximum  project the function into the orthogonal subspace  and continue to
ﬁnd more local maxima.
Deﬁnition 4.5. The projection of a function f to a linear subspace S is a function on that subspace
with value equal to f. More explicitly  if {v1  v2  ...  vd} is an orthonormal basis of S  the projection

of f to S is a function g : Rd → R such that g(w) = f ((cid:80)d

i=1 wivi).

The following theorem gives sufﬁcient conditions under which the above algorithm ﬁnds all local
maxima  making precise the intuition given at the beginning of this section.
Theorem 4.6. Suppose the function f∗(u) : Rn → R satisﬁes the following properties:

1. Orthogonal Local Maxima: The function has n local maxima v∗

i   and they are orthogonal

to each other.

of local maxima is (γ(cid:48)  β(cid:48)  δ(cid:48)) Locally Improvable. The step size δ(cid:48) ≥ 10δ.

2. Locally Improvable: f∗ is (γ  β  δ) Locally Improvable.
3. Improvable Projection: The projection of the function to any subspace spanned by a subset
4. Lipschitz: If (cid:107)u − u(cid:48)(cid:107)2 ≤ 3
√
5. Attraction Radius: Let Rad ≥ 3

nγ + γ(cid:48)  for any local maximum v∗
i (cid:107)2 ≤ Rad  then there exist a set U containing (cid:107)u − v∗

i   let T be min f∗(u)
√
i (cid:107)2 ≤ 3
for (cid:107)u − v∗
nγ + γ(cid:48) and
does not contain any other local maxima  such that for every u that is not in U but is β
close to U  f∗(u) < T .

nγ  then the function value |f∗(u) − f∗(u(cid:48))| ≤ δ(cid:48)/20.

√

If we are given function f such that |f (u) − f∗(u)| ≤ δ/8 and f is both (β  δ) and (β(cid:48)  δ(cid:48)) Locally
Approximable  then Algorithm 2 can ﬁnd all local maxima of f∗ within distance γ.

To prove this theorem  we ﬁrst notice the projection of the function f in Step 3 of the algorithm
should be close to the projection of f∗ to the remaining local maxima. This is implied by Lipschitz
condition and is formally shown in the following two lemmas. First we prove a “coupling” between
the orthogonal complement of two close subspaces:
Lemma 4.7. Given v1  v2  ...  vk  each γ-close respectively to local maxima v∗
k (this is
without loss of generality because we can permute the index of local maxima)  then there is an
orthonormal basis vk+1  vk+2  ...  vn for the orthogonal space of span{v1  v2  ...  vk} such that for

any unit vector w ∈ Rn−k (cid:80)n−k

nγ close to(cid:80)n−k

√
i=1 wkvk+i is 3

i=1 wkv∗

1  v∗

2  ...  v∗

k+i.

We prove this lemma using a modiﬁcation of the Gram-Schmidt orthonormalization procedure. Us-
ing this lemma we see that the projected function is close to the projection of f∗ in the span of the
rest of local maxima:
Lemma 4.8. Let g∗ be the projection of f∗ into the space spanned by the rest of local maxima  then
|g∗(w) − g(w)| ≤ δ/8 + δ(cid:48)/20 ≤ δ(cid:48)/8.
5 Local search on the fourth order cumulant
Next  we prove that the fourth order cumulant P ∗(u) satisﬁes the properties above. Then the algo-
rithm given in the previous section will ﬁnd all of the local maxima  which is the missing step in our
1Technically  there are 2n local maxima since for each direction u that is a local maxima  so too is −u but

this is an unimportant detail for our purposes.

7

Algorithm 3. RECOVER  Input:B  (cid:98)P (cid:48)(u)  (cid:98)R   Output: (cid:98)A (cid:98)Σ
(cid:16)(cid:98)P (cid:48)((cid:98)Ri)

1. Let (cid:98)DA(u) be a diagonal matrix whose ith entry is 1
2. Let (cid:98)A = B(cid:98)R(cid:98)DA(u)−1/2.
3. Estimate C = E[yyT ] by taking O(((cid:107)A(cid:107)2 + (cid:107)Σ(cid:107)2)4n2−2) samples and let (cid:98)C = 1
4. Let(cid:98)Σ = (cid:98)C − (cid:98)A(cid:98)AT
5. Return (cid:98)A (cid:98)Σ

(cid:17)−1/2

.

2

N

(cid:80)N

i .
i=1 yiyT

main goal: learning a noisy linear transformation Ax + η with unknown Gaussian noise. We ﬁrst
use a theorem from [5] to show that properties for ﬁnding one local maxima is satisﬁed.
Also  for notational convenience we set di = 2DA(u0)−2
i i and let dmin and dmax denote the min-
imum and maximum values (bounds on these and their ratio follow from Claim 2.9). Using this

notation P ∗(u) =(cid:80)n

i=1 di(uT R∗

i )4.

√

Theorem 5.1 ([5]). When β < dmin/10dmaxn2  the function P ∗(u) is (3
nβ  β  P ∗(u)β2/100)
Locally Improvable and (β  dminβ2/100n) Locally Approximable. Moreover  the local maxima of
the function is exactly {±R∗
i }.

We then observe that given enough samples  the empirical mean (cid:98)P (cid:48)(u) is close to P ∗(u). For

minλmin(A)8(cid:107)Σ(cid:107)4

concentration we require every degree four term zizjzkzl has variance at most Z.
Claim 5.2. Z = O(d2
suppose columns of R(cid:48) =
Lemma 5.3. Given 2N samples y1  y2  ...  yN   y(cid:48)
B−1ADA(u0)1/2 are  close to the corresponding columns of R∗  with high probability the function

(cid:98)P (cid:48)(u) is O(dmaxn1/2 + n2(N/Z log n)−1/2) close to the true function P ∗(u).

2  ...  y(cid:48)
N  

1  y(cid:48)

2 + d2

min).

The other properties required by Theorem 4.6 are also satisﬁed:
Lemma 5.4. For any (cid:107)u − u(cid:48)(cid:107)2 ≤ r  |P ∗(u) − P ∗(u(cid:48))| ≤ 5dmaxn1/2r. All local maxima of P ∗
has attraction radius Rad ≥ dmin/100dmax.
Applying Theorem 4.6 we obtain the following Lemma (the parameters are chosen so that all prop-
erties required are satisﬁed):
Lemma 5.5. Let β(cid:48) = Θ((dmin/dmax)2)  β = min{γn−1/2  Ω((dmin/dmax)4n−3.5)}  then the
procedure RECOVER(f  β  dminβ2/100n   β(cid:48)  dminβ(cid:48)2/100n) ﬁnds vectors v1  v2  ...  vn  so that
there is a permutation matrix Π and ki ∈ {±1} and for all i: (cid:107)vi − (RΠDiag(ki))∗

After obtaining (cid:98)R = [v1  v2  ...  vn] we can use Algorithm 3 to ﬁnd A and Σ:
Theorem 5.6. Given a matrix (cid:98)R such that there is permutation matrix Π and ki ∈ {±1} with
(cid:107)(cid:98)Ri − ki(R∗Π)i(cid:107)2 ≤ γ for all i  Algorithm 3 returns matrix (cid:98)A such that (cid:107)(cid:98)A − AΠDiag(ki)(cid:107)F ≤
(cid:107)(cid:98)Σ − Σ(cid:107)F ≤ .
2 n3/2λmin(A)) × min{1/(cid:107)A(cid:107)2   1}  we also have
R∗ (or an approximation) and since P ∗(u) =(cid:80)n

Recall that the diagonal matrix DA(u) is unknown (since it depends on A)  but if we are given
i )4  we can recover the matrix DA(u)
approximately from computing P ∗(R∗
i ). Then given DA(u)  we can recover A and Σ and this
completes the analysis of our algorithm.
Conclusions
ICA is a vast ﬁeld with many successful techniques. Most rely on heuristic nonlinear optimization.
An exciting question is: can we give a rigorous analysis of those techniques as well  just as we
did for local search on cumulants? A rigorous analysis of deep learning —say  an algorithm that
provably learns the parameters of an RBM—is another problem that is wide open  and a plausible
special case involves subtle variations on the problem we considered here.

2 n3/2/λmin(A)). If γ ≤ O(/(cid:107)A(cid:107)2

i=1 di(uT R∗

i (cid:107)2 ≤ γ.

O(γ (cid:107)A(cid:107)2

8

References

[1] P. Comon. Independent component analysis: a new concept? Signal Processing  pp. 287–314 

1994. 1  1.1

[2] A. Hyvarinen  J. Karhunen  E. Oja.

2001. 1

Independent Component Analysis. Wiley: New York 

[3] A. Hyvarinen  E. Oja. Independent component analysis: algorithms and applications. Neural

Networks  pp. 411–430  2000. 1

[4] P. J. Huber. Projection pursuit. Annals of Statistics pp. 435–475  1985. 1
[5] A. Frieze  M. Jerrum  R. Kannan. Learning linear transformations. FOCS  pp. 359–368  1996.

1  1.1  4.1  4.4  5  5.1

[6] A. Anandkumar  D. Foster  D. Hsu  S. Kakade  Y. Liu. Two SVDs sufﬁce: spectral decompo-
sitions for probabilistic topic modeling and latent Dirichlet allocation. Arxiv:abs/1203.0697 
2012. 1

[7] D. Hsu  S. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral

decompositions. Arxiv:abs/1206.5766  2012. 1

[8] L. De Lathauwer; J. Castaing; J.-F. Cardoso  Fourth-Order Cumulant-Based Blind Identiﬁ-
cation of Underdetermined Mixtures  Signal Processing  IEEE Transactions on  vol.55  no.6 
pp.2965-2973  June 2007 1

[9] A.P. Dempster  N.M. Laird  and D.B. Rubin. Maximum likelihood from incomplete data via

the EM Algorithm. Journal of the Royal Statistical Society Series B  pp. 1–38  1977. 1

[10] S. Dasgupta. Learning mixtures of Gaussians. FOCS pp. 634–644  1999. 1
[11] S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. Annals of

Applied Probability  pp. 69-92  2005. 1

[12] M. Belkin and K. Sinha. Polynomial learning of distribution families. FOCS pp. 103–112 

2010. 1

[13] A. T. Kalai  A. Moitra  and G. Valiant. Efﬁciently learning mixtures of two Gaussians. STOC

pp. 553-562  2010. 1

[14] A. Moitra and G. Valiant. Setting the polynomial learnability of mixtures of Gaussians. FOCS

pp. 93–102  2010. 1

[15] G. Hinton  R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Sci-

ence pp. 504–507  2006. 1

[16] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning 

pp. 1–127  2009. 1

[17] G. E. Hinton. A Practical Guide to Training Restricted Boltzmann Machines  Version 1  UTML

TR 2010-003  Department of Computer Science  University of Toronto  August 2010 1

[18] Y. Freund   D. Haussler. Unsupervised Learning of Distributions on Binary Vectors using Two

Layer Networks University of California at Santa Cruz  Santa Cruz  CA  1994 1

[19] S. Cruces  L. Castedo  A. Cichocki  Robust blind source separation algorithms using cumu-

lants  Neurocomputing  Volume 49  Issues 14  pp 87-118  2002. 1.1

[20] L.  De Lathauwer; B.  De Moor; J. Vandewalle. Independent component analysis based on
higher-order statistics only Proceedings of 8th IEEE Signal Processing Workshop on Statistical
Signal and Array Processing  1996. 1.1

[21] S. Vempala  Y. Xiao. Structure from local optima: learning subspace juntas via higher order

PCA. Arxiv:abs/1108.3329  2011. 1.1

[22] M. Kendall  A. Stuart. The Advanced Theory of Statistics Charles Grifﬁn and Company  1958.

2

9

,Volodymyr Mnih
Nicolas Heess
Alex Graves
koray kavukcuoglu