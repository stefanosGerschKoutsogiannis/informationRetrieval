2012,Distributed Probabilistic Learning for Camera Networks with Missing Data,Probabilistic approaches to computer vision typically assume a centralized setting  with the algorithm granted access to all observed data points.  However  many problems in wide-area surveillance can benefit from distributed modeling  either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular  we show how traditional centralized models  such as probabilistic PCA and missing-data PPCA  can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations.,Distributed Probabilistic Learning

for Camera Networks with Missing Data

Sejong Yoon

Department of Computer Science

Rutgers University

Vladimir Pavlovic

Department of Computer Science

Rutgers University

sjyoon@cs.rutgers.edu

vladimir@cs.rutgers.edu

Abstract

Probabilistic approaches to computer vision typically assume a centralized setting 
with the algorithm granted access to all observed data points. However  many
problems in wide-area surveillance can beneﬁt from distributed modeling  either
because of physical or computational constraints. Most distributed models to date
use algebraic approaches (such as distributed SVD) and as a result cannot explic-
itly deal with missing data. In this work we present an approach to estimation and
learning of generative probabilistic models in a distributed context where certain
sensor data can be missing. In particular  we show how traditional centralized
models  such as probabilistic PCA and missing-data PPCA  can be learned when
the data is distributed across a network of sensors. We demonstrate the utility
of this approach on the problem of distributed afﬁne structure from motion. Our
experiments suggest that the accuracy of the learned probabilistic structure and
motion models rivals that of traditional centralized factorization methods while
being able to handle challenging situations such as missing or noisy observations.

1

Introduction

Traditional computer vision algorithms  particularly those that exploit various probabilistic and
learning-based approaches  are often formulated in centralized settings. A scene or an object is
observed by a single camera with all acquired information centrally processed and stored in a single
knowledge base (e.g.  a classiﬁcation model). Even if the problem setting relies on multiple cameras 
as may be the case in multi-view or structure from motion (SfM) tasks  all collected information is
still processed and organized in a centralized fashion. Increasingly modern computational settings
are becoming characterized by networks of peer-to-peer connected devices  with local data process-
ing abilities. Nevertheless  the overall goal of such distributed device (camera) networks may still
be to exchange information and form a consensus interpretation of the visual scene. For instance 
even if a camera observes a limited set of object views  one would like its local computational model
to reﬂect a general 3D appearance of the object visible by other cameras in the network.
A number of distributed algorithms have been proposed to address the problems such as calibration 
pose estimation  tracking  object and activity recognition in large camera networks [1–3]. In order
to deal with high dimensionality of vision problems  distributed latent space search such as decen-
tralized variants of PCA have been studied in [4  5]. A more general framework using distributed
least squares [6] based on distributed averaging of sensor fusions [7] was introduced for PCA  tri-
angulation  pose estimation and SfM. Similar approaches have been extended to settings such as the
distributed object tracking and activity interpretation [8 9]. Even though the methods such as PCA or
Kalman ﬁltering have their well-known probabilistic counterparts  the aforementioned approaches
do not use probabilistic formulation when dealing with the distributed setting.
One critical challenge in distributed data analysis includes dealing with missing data. In camera
networks  different nodes will only have access to a partial set of data features because of varying
camera views or object movement. For instance  object points used for SfM may be visible only

1

in some cameras and only in particular object poses. As a consequence  different nodes will be
frequently exposed to missing data. However  most current distributed data analysis methods are
algebraic in nature and cannot seamlessly handle such missing data.
In this work we propose a distributed consensus learning approach for parametric probabilistic mod-
els with latent variables that can effectively deal with missing data. We assume that each node in
a network can observe only a fraction of the data (e.g.  object views in camera networks). Further-
more  we assume that some of the data features may be missing across different nodes. The goal of
the network of sensors is to learn a single consensus probabilistic model (e.g.  3D object structure)
without ever resorting to a centralized data pooling and centralized computation. We will demon-
strate that this task can be accomplished in a principled manner by local probabilistic models and
in-network information sharing  implemented as recursive distributed probabilistic learning.
In particular  we focus on probabilistic PCA (PPCA) as a prototypical example and derive its dis-
tributed version  the D-PPCA. We then suggest how missing data can be handled in this setting
using a missing-data PPCA and apply this model to solve the distributed SfM task in a camera net-
work. Our model is inspired by the consensus-based distributed Expectation-Maximization (EM)
algorithm for Gaussian mixtures [10]  which we extend to deal with generalized linear Gaussian
models [11]. Unlike other recently proposed decomposable Gaussian graphical models [4  12]  our
model does not depend on any speciﬁc type of graphs. Our network  of arbitrary topology  is as-
sumed to be static with a single connected component. These assumptions are reasonably applicable
to many real world camera network settings.
In Section 2  we ﬁrst explain the general distributed probabilistic model. Section 3 shows how D-
PPCA can be formulated as a special case of the probabilistic framework and propose the means for
handling missing data. We then explain how D-PPCA can be modiﬁed for the application in afﬁne
SfM. In Section 5  we report experimental results of our model using both synthetic and real data.
Finally  we discuss our approach including its limitations and possible solutions in Section 6.

2 Distributed Probabilistic Model

We start our discussion by ﬁrst considering a general parametric probabilistic model in a centralized
setting and then we show how to derive its distributed form.

2.1 Centralized Setting
Let X = {xn|xn ∈ RD} be a set of iid multivariate data points with the corresponding latent
variables Z = {zn|zn ∈ RM}  n = 1...N. Our model is a joint density deﬁned on (xn  zn) with a
global parameter θ

(xn  zn) ∼ p(xn  zn|θ) 

with p(X  Z|θ) = (cid:81)

n p(xn  zn|θ)  as depicted in Fig. 1a. In this general model  we can ﬁnd an
optimal global parameter ˆθ (in a MAP sense) by applying standard EM learning. The EM follows a
recursive two-step procedure: (a) E-step  where the posterior density p(zn|xn  θ) is estimated  and
(b) M-step: parametric optimization ˆθ = arg maxθ EZ|X [log p(X  Z|θ)]. It is important to point out
that each posterior density estimate at point n depends solely on the corresponding measurement xn
and does not depend on any other xk  k (cid:54)= n. This means that even if we partition independent mea-
surements into arbitrary subsets  posterior density estimation is accomplished locally  within each
subset. However  in the M-step all measurements X affect the choice of ˆθ because of the depen-
dence of each term in the completed log likelihood on the same ˆθ. This is a typical characteristic of
parametric models where the optimal parameters depend on summary data statistics.

2.2 Distributed Setting
Let G = (V  E) be an undirected connected graph with vertices i  j ∈ V and edges eij = (i  j) ∈ E
connecting the two vertices. Each i-th node is directly connected with 1-hop neighbors in Bi =
{j|eij ∈ E}. Suppose the set of data samples at i-th node is Xi = {xin|n = 1  ...  Ni}  where
xin ∈ RD is n-th measurement vector and Ni is the number of samples collected in i-th node.
Likewise  we deﬁne the latent variable set for node i as Zi = {zin|n = 1  ...  Ni}.

2

(a) Centralized

(b) Distributed

(c) Augmented

Figure 1: Centralized  distributed and augmented models for probabilistic PCA.

As observed previously  each posterior estimation is decentralized. Learning the model parameter
would be decentralized if each node had its own independent parameter θi. Still  the centralized
model can be equivalently deﬁned using the set of local parameters  with an additional constraint on
their consensus  θ1 = θ2 = ··· = θ|V |. This is illustrated in Fig. 1b where the local node models
are constrained using ties deﬁned on the underlying graph. The simple consensus tying can be more
conveniently deﬁned using a set of auxiliary variables ρij  one for each edge eij (Fig. 1c). This now
leads to the ﬁnal distributed consensus learning formulation  similar to [10]:

ˆθ = arg min

{θi:i∈V }

− log p(X|θ  G)

s.t.

θi = ρij  ρij = θj  i ∈ V  j ∈ Bi

(1)

where we marginalized on X. This is a constrained optimization task that can be solved in a prin-
cipal manner using the Alternating Direction Method of Multipliers (ADMM) [13–15]. ADMM
(cid:111)
iteratively  in a block-coordinate fashion  solves maxλ minθ L(·) on the augmented Lagrangian

L(θ  ρ  λ) = − log p(X|θ1  θ2  ...  θ|V |  G) +

ij1(θi − ρij) + λT
λT

ij2(ρij − θj)

(cid:88)

(cid:110)
(cid:88)
(cid:8)||θi − ρij||2 + ||ρij − θj||2(cid:9)

j∈Bi

i∈V

(cid:88)

(cid:88)

η
2

+

i∈V

j∈Bi

(2)
where λij1  λij2  i  j ∈ V are the Lagrange multipliers  η is some positive scalar parameter and
|| · || is induced norm. The last term (modulated by η) is not strictly necessary for consensus but
introduces additional regularization. Further discussions on this term and the parameter can be found
in [15] and [16]. The auxiliary ρij play a critical decoupling role and separate estimation of local
θi during block-coordinate ascent/descent. This classic (ﬁrst introduced in 1970s) meta decompose
algorithm can be used to devise a distributed counterpart for any centralized problem that attempts
to maximize a global log likehood function over a connected network.

3 Distributed Probabilistic PCA (D-PPCA)

We now apply the general distributed probabilistic learning explained above to the speciﬁc case of
distributed PPCA. Traditional centralized formulation of probabilistic PCA (PPCA) [17] assumes
that latent variable zin ∼ N (zin|0  I)  with a generative relation
xin = Wizin + µi + i 

(3)

where i ∼ N (i|0  a−1

i I) and ai is the noise precision. Inference then yields
p(zin|xin) = N (zin|L−1
(4)
) 
where Li = WT
i I. We can ﬁnd optimal parameters Wi  µi  ai by ﬁnding the maximum
likelihood estimates of the marginal data likelihood or by applying the EM algorithm on expected
complete data log likelihood with respect to the posterior density p(Zi|Xi).

i (xin − µi)  a−1

i Wi + a−1

i L−1

i WT

i

3.1 Distributed Formulation

The distributed algorithm developed in Section 2 can be directly applied to this PPCA model. The
basic idea is to assign each subset of samples as evidence for the local generative models with

3

n=1

where F (Θi) =

Ni(cid:80)
(cid:16)
(cid:88)
L(Φi) = −F (Θi)+(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

j∈Bi

j∈Bi

i∈V

+

i∈V
η
2

+

i∈V

j∈Bi

i

parameters Wi  µi  a−1
. The inference is accomplished locally in each node. The local parameter
estimates are then computed using the consensus updates that combine local summary data statistics
with the information about the model conveyed through neighboring network nodes. Below  we
outline speciﬁc details of this approach.
Let Θi = {Wi  µi  ai} be the set of parameters for each node i. The global constrained consensus
optimization now becomes

min{Wi µi ai:i∈V } −F (Θi)

s.t.

Wi = ρij  ρij = Wj 
µi = φij  φij = µj 
ai = ψij  ψij = aj 

i ∈ V  j ∈ Bi 
i ∈ V  j ∈ Bi 
i ∈ V  j ∈ Bi

(5)

log p(xin|Wi  µi  a−1

i

). The augmented Lagrangian is

ij1(Wi − ρij) + λT
λT

ij2(ρij − Wj)

(βij1(ai − ψij) + βij2(ψij − aj)) +

(cid:16)

ij1(µi − φij) + γT
γT

ij2(φij − µj)

(cid:17)

(||Wi − ρij||2 + ||ρij − Wj||2)

(cid:17)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

j∈Bi

i∈V

i∈V

j∈Bi

+

η
2

i∈V

j∈Bi

(||µi − φij||2 + ||φij − µj||2) +

η
2

((ai − ψij)2 + (ψij − aj)2)

(6)

where Φi = {Wi  µi  ai  ρij  φij  ψij; i ∈ V  j ∈ Bi} and {λijk} {γijk} {βijk} with k = 1  2
are the Lagrange multipliers. The scalar value η gives us control over the convergence speed of the
algorithm. With reasonably large positive η  the overall optimization converges fairly quickly [10].
We will explore the converging behaviour with respect to various η in synthetic data experiments.
Just like in a standard EM approach  we minimize the upper bound of L(Φi). Exploiting the pos-
terior density in (4)  we compute the expected mean and variance of latent variables in each node
as

i WT

i (xin − µi) 

E[zinzT

Maximization of the completed likelihood Lagrangian derived from (6) yields

in] = a−1
i L−1
i + E[zin]E[zin]T.
(cid:32)
(cid:19)(cid:27)
(cid:19)(cid:27)

Ni(cid:88)

E[zinzT

i + W(t)

n=1

ai

·

j

W(t)

(cid:18)

µ(t)

i + µ(t)
j )

· (Niai + 2η|Bi|)

−1  

(7)

(cid:33)−1

 

in] + 2η|Bi|I

(cid:18)
(cid:88)
(cid:88)

j∈Bi

j∈Bi

(8)

(9)

(10)

(11)

(12)

W(t+1)

i

=

µ(t+1)

i

=

E[zin] = L−1
(cid:26)
(cid:26)

Ni(cid:88)
Ni(cid:88)

n=1

ai

ai

(cid:18)
(cid:88)
(cid:88)
(cid:88)

j∈Bi

j∈Bi

j∈Bi

η
2

η
2

(cid:110)
(cid:110)
(cid:110)

λ(t+1)

i

= λ(t)

i +

n=1
η
2

γ(t+1)
i

= γ(t)

i +

β(t+1)
i

= β(t)

i +

(xin − µi)E[zin]T − 2λ(t)

i + η

xin − WiE[zin]

− 2γ(t)

i + η

(cid:111)

W(t+1)

i

− W(t+1)

 

µ(t+1)

i

− µ(t+1)

 

a(t+1)
i

− a(t+1)

(cid:19)

j

j

j

.

(cid:111)
(cid:111)
(cid:40)
(cid:104)E[zinzT

·

For ai  we solve the quadratic equation

0 = − NiD
2

Ni(cid:88)

n=1

+

1
2

i

2

+ 2η|Bi|a(t+1)

(cid:110)||xin − µi||2 + tr

+ a(t+1)

i

2β(t)

i − η

in]WT

i Wi

(cid:16)
(cid:88)
(cid:105)(cid:111)(cid:41)

j∈Bi

.

(cid:17) − Ni(cid:88)

n=1

a(t)
i + a(t)

j

E[zin]TWT

i (xin − µi)

(13)

The overall distributed EM algorithm for D-PPCA is summarized in Algorithm 1. Detailed deriva-
tion can be found in the supplementary material.

4

Algorithm 1 Distributed Probabilistic PCA (D-PPCA)
Require: For every node i initialize W(0)
i
for t = 0  1  2  ... until convergence do

  µ(0)

  a(0)

i

i

for all i ∈ V do

[E-step] Compute E[zin] and E[zinzT
[M-step] Compute W(t+1)

  µ(t+1)

in] via (7).
  a(t+1)

i

i

i

via (8 9 13).

randomly and set λ(0)

i = 0  γ(0)

i = 0  β(0)

i = 0.

  µ(t+1)

i

  and a(t+1)

i

to all neighbors of i ∈ Bi.

Compute λ(t+1)

i

  γ(t+1)

i

  and β(t+1)

i

via (10-12).

end for
for all i ∈ V do

Broadcast W(t+1)

i

end for
for all i ∈ V do

end for

end for

3.2 Missing Data D-PPCA

Traditional PPCA is an effective tool for dealing with data missing-at-random (MAR) in traditional
PCA [18]. While more sophisticated methods including variational approximations  c.f.  [18] are
possible direct use of PPCA is often sufﬁcient in practice. Hence  we adopt D-PPCA as a method to
deal with missing data in a distributed consensus setting.
Generalization to missing data D-PPCA from D-PPCA is straightforward and follows [18]. From
the perspective of ADMM-based learning the only modiﬁcations comes in the form of adjusted
n=1(xin−WiE[zin])

terms for local data summaries. For instance  in (9) the data summary term(cid:80)Ni

becomes

xi n f − wT

i f

E[zin] 

(14)

(cid:88)

n∈Oi f

where f = 1  . . .   D is the index of feature  Oi f is the set of samples in node i that have the feature
f present  xi n f is the value of the present feature  and wT
i f is the f-th row of matrix Wi. Similar
expressions can be derived for other local parameters. Note that (10-12) incur no changes.

4 D-PPCA for Structure from Motion (SfM)

In this section  we consider a speciﬁc formulation of the modiﬁed distributed probabilistic PCA
for application in afﬁne SfM. In SfM  our goal is to estimate the 3D location of N points on a
rigid object based on corresponding 2-D points observed from multiple cameras (or views). The
dimension D of our measurement matrix is thus twice the number of frames each camera observed.
A simple and effective way to solve this problem is the factorization method [19]. Given a 2D (image
coordinate) measurement matrix X  of size 2 · #f rames × #points  the matrix is factorized into a
2 · #f rames × 3 motion matrix M and the 3 × #points 3D structure matrix S. In the centralized
setting this can be easily computed using SVD on X. Equivalently  the estimates of M and S can
be found using inference and learning in a centralized PPCA  where M is treated as the PPCA
parameter and S is the latent structure. There we obtain additional estimates of the variance of
structure S  which are not immediately available from the factorization approach (although  they
can be found).
However  the above deﬁned (2 · #f rames × #points) data structure of X is not amenable to
distribution of different views (cameras  nodes)  as considered in Section 3 of D-PPCA. Namely 
D-PPCA assumes that the distribution is accomplished by splitting the data matrix X into sets of
non-overlapping columns  one for each node. Here  however  we seek to distribute the rows of
matrix X  i.e.  a set of (subsequent) frames is to be assigned to each node/camera.
Hence  to apply the D-PPCA framework to SfM we need to swap the role of rows and columns 
i.e.  consider modeling of XT. This  subsequently  means that the 3D scene structure (which is to
be shared across all nodes in the network) will be treated as the D-PPCA parameter. The latent
D-PPCA variables will model the unknown and uncertain motion of each camera (and/or object in
its view).

5

Speciﬁcally  we will consider the model

i = W · Zi + Ei
XT

(15)
i is the matrix of image coordinates of all points in node (camera) i of size #points × 2 ·
where XT
#f rames in node i  W is the #points × 3 3D structure (D-PPCA parameter) matrix and Zi is the
3 × 2 · #f rames motion matrix of node i.
One should note that we have implicitly assumed  in a standard D-PPCA manner  that each column
of Zi is iid and distributed as N (0  I). However  each pair of subsequent Zi columns represents
one 3 × 2 afﬁne motion matrix. While those columns are not truly independent our experiments (as
demonstrated in Section 5) show that this assumption is not detrimental in practice. Remaining task
is simply following the same process we did to derive D-PPCA.
Missing data in SfM will be handled using the formalism presented in Sec. 3.2. Strictly speaking 
the model of data missing-at-random is not always applicable to SfM. The reason is that occlusions 
the main source of missing data  cannot be treated as a random process. Instead  this setting corre-
sponds to data missing-not-at-random [18] (MNAR). If treated blindly  this may introduce bias in the
estimated models. However  as we demonstrate in experiments this assumption does not adversely
affect SfM when the number of missing points is within a reasonable range.

5 Experiments

In our experiments we ﬁrst study the general convergence properties of the D-PPCA algorithm in a
controlled synthetic setting. We then apply the D-PPCA to a set of SfM problems  both on synthetic
and on real data.

5.1 Empirical Convergence Analysis

Using synthetic data generated from Gaussian distribution  we observed that D-PPCA works well
regardless of the number of network nodes  topology  choice of the parameter η or even with missing
values in both MAR and MNAR cases. Detailed results for the syntehtic data is provided in the
supplementary materials.

5.2 Afﬁne Structure from Motion

We now show that the modiﬁed D-PPCA can be used as an effective framework for distributed afﬁne
SfM. We ﬁrst show results in a controlled environment with synthetic data and then report results
on data from real video sequences. We assume that correspondences across frames and cameras
are known. For the missing values of MNAR case  we either used the actual occlusions to induce
missing points or simulated consistently missing points over several frames.

5.2.1 Synthetic Data (Cube)

We ﬁrst generated synthetic data with a rotating unit cube and 5 cameras facing the cube in a 3D
space  similar to synthetic experiments in [6]. The cube is centered at the origin of the space and
rotates 30◦ counterclockwise. We extracted 8 cube points projected on each camera view every 6◦ 
i.e. each camera observed 5 frames. Cameras are placed on a skewed plane  making elevation along
z-axis as shown in Fig. 2a. For all synthetic and real SfM experiments  we picked η = 10 and
initialized Wi matrix with feature point coordinates of the ﬁrst frame visible in the i-th camera with
some small noise. The convergence criterion for D-PPCA for SfM was set as 10−3 relative error.
To measure the performance  we computed maximum subspace angle between the ground truth
3D coordinates and our estimated 3D structure matrix. For comparison  we conducted traditional
SVD-based SfM on the same data. In noise free case  D-PPCA for SfM always yielded the same
performance as SVD-based SfM with near 0◦.
We also tested D-PPCA for SfM with noisy and missing-value cases. First  we generated 20 inde-
pendent samples of all 25 frames with 10 different noise levels. Then we ran D-PPCA 20 times on
each of the independent sample and averaged the ﬁnal structure estimates. As Fig. 2b shows  we
found that D-PPCA for SfM is fairly robust to noise and tends to stabilize even as the noise level

6

(a) Camera Setting

(b) Subspace Angle vs. Ground Truth

Figure 2: Rotating unit cube with multiple cameras. Red circles are camera locations and blue
arrows indicate each camera’s facing direction. Green and red crosses in the right plot are outliers
for centralized SVD-based SfM and D-PPCA for SfM  respectively.

increases. The mean subspace angle tends to be slightly larger than that estimated by the central-
ized SVD SfM  however both reside within the overlapping conﬁdence intervals. Considering MAR
missing values  we obtained 1.66◦ for 20% missing points averaged over 10 different missing point
samples. In MNAR case with actual occlusions considered  D-PPCA yielded  relatively larger  20◦
error. Intuitively  this is because the missing points in the scene are naturally not random. However 
we argue that D-PPCA can still handle missing points given the evidence below.

5.2.2 Real Data

For real data experiement  we ﬁrst applied D-PPCA for SfM on the Caltech 3D Objects on Turntable
dataset [20]. The dataset provides various objects rotating on a turntable under different lighting
conditions. The views of most objects were taken every 5◦ which make it challenging to extract
feature points with correspondence across frames. Instead  we used a subset of the dataset which
provides views taken every degree. This subset contains images of 5 objects. To simulate multiple
cameras  we adopted a setting similar to that of [6]. We ﬁrst extracted ﬁrst 30◦ images of each
object. We then used KLT [21] implementation in Voodoo Camera Tracker1 to extract feature points
with correspondence. Lastly  we sequentially and equally partitioned the 30 images into 5 nodes to
simulate 5 cameras. Thus  each camera observes 6 frames. Table 1 shows the 5 objects and statistics
of feature points we extracted from the objects. We used η = 10 and convergence criterion 10−3.
Due to the lack of the ground truth 3D coordinates  we compared the subspace angles between the
structure inferred using the traditional centralized SVD-based SfM and the D-PPCA-based SfM.
Results are shown in Table 1 as the mean and variance of 20 independent runs. 10% MAR and
MNAR results are also provided in the table.
Experimenal results indicate existance of differences between the reconstructions obtained by cen-
tralized factorization approach and that of D-PPCA. However  the differences are small  depend on
the object in question  and almost always include  within their conﬁdence  the factorization result.
Qualitative examination reveals no noticable differences. Moreover  re-projecting back to the cam-
era coordinate space resulted in close matching with the tracked feature points  as shown in videos
provided in supplementary materials.
We also tested the utility of D-PPCA for SfM on the Hopkins155 dataset [22]. We adopted virtually
identical experimental setting as in [6]. We collected 135 single-object sequences containing image
coordinates of points and we simulated multi-camera setting by partitioning the frames sequentially
and almost equally for 5 nodes and the network was connected using ring topology. Again  we
computed maximum subspace angle between centralized SVD-based SfM and distributed D-PPCA
for SfM. We chose the convergence criterion as 10−3. Average maximum subspace angle between

1http://www.digilab.uni-hannover.de/docs/manual.html

7

01234501234−0.500.511.5200.20.40.60.811.21.41.6Noise level (%)Subspace angle (degree)012345678910D−PPCACentralizedSVD−basedSfMTable 1: Caltech 3D Objects on Turntable dataset statistics and quantitative results. Green dots indi-
cate feature points tracked with correspondance across all 30 frames. All results ran 20 independent
initializations. MAR results provide variances over both various initializations and missing value
settings.

Object

BallSander

BoxStuff

Rooster

Standing

StorageBin

# Points
# Frames

62
30

67
30

189
30

310
30

102
30

Mean

Subspace angle b/w centralized SVD SfM and D-PPCA (degree)
1.4848
0.4159

1.4397
0.4567

1.4767
0.9448

2.6221
1.6924

0.4463
1.2002

2.8358
1.3591
0.0444

Variance
Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MAR (degree)
Mean

Var.(init)
Var.(miss)
Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MNAR (degree)

6.2991
4.3562
0.5729

2.1556
0.1351
0.0161

7.6492
6.6424
0.7603

5.2506
3.8810
0.1755

Mean

Variance

3.1405
0.0124

6.4664
3.1955

5.8027
2.4333

9.2661
2.9720

3.7965
0.0089

D-PPCA for SfM and SVD-based SfM for all objects was 3.97◦ with variance 7.06. However 
looking into the result more carefully  we found that even with substantially larger subspace angle 
3D structure estimates were similar to that of SVD-based SfM only with orthogonal ambiguity issue.
Moreover  more than 53% of all objects yielded the subspace angle below 1◦  77% of them below
5◦ and more than 94% were less than 15◦. With 10% MAR  we obtained the mean 20.07◦ with
variance 27.94◦ with about 18% of them below 1◦  56% of them below 5◦ and more than 70% of
them less than the mean. We could not perform MNAR experiments on Hopkins as the ground truth
occlusion information is not provided with the dataset.

6 Discussion and Future Work

In this work we introduced a general approach for learning parameters of traditional centralized
probabilistic models  such as PPCA  in a distributed setting. Our synthetic data experiments showed
that the proposed algorithm is robust to choices of initial parameters and  more importantly  is not
adversely affected by variations in network size  topology or missing values. In the SfM problems 
the algorithm can be effectively used to distribute computation of 3D structure and motion in camera
networks  while retaining the probabilistic nature of the original model.
Despite its promising performance D-PPCA for SfM exhibits some limitations. In particular  we
assume the independence of the afﬁne motion matrix parameters in (15). The assumption is clearly
inconsistent with the modeling of motion on the SE(3) manifold. However  our experiments demon-
strate that  in practice  this violation is not crucial. This shortcoming can be amended in one of sev-
eral possible ways. One can reduce the iid assumption of individual samples to that of subsequent
columns (i.e.  full 3x2 motion matrices). Our additional experiments  not reported here  indicate
no discernable utility of this approach. A more principled approach would be to deﬁne priors for
motion matrices compatible with SE(3)  using e.g.  [23]. While appealing  the priors would render
the overall model non-linear and would require additional algorithmic considerations  perhaps in the
spirit of [1].

Acknowledgments

This work was supported in part by the National Science Foundation under Grant No. IIS 0916812.

8

References
[1] Roberto Tron and Rene Vidal. Distributed Computer Vision Algorithms. IEEE Signal Processing Maga-

zine  28:32–45  2011.

[2] A.Y. Yang  S. Maji  C.M. Christoudias  T. Darrell  J. Malik  and S.S. Sastry. Multiple-view Object Recog-
nition in Band-limited Distributed Camera Networks. In Distributed Smart Cameras  2009. ICDSC 2009.
Third ACM/IEEE International Conference on  30 2009-sept. 2 2009.

[3] Richard J. Radke. A Survey of Distributed Computer Vision Algorithms. In Hideyuki Nakashima  Hamid
Aghajan  and Juan Carlos Augusto  editors  Handbook of Ambient Intelligence and Smart Environments.
Springer Science+Business Media  LLC  2010.

[4] A. Wiesel and A.O. Hero. Decomposable Principal Component Analysis. Signal Processing  IEEE

Transactions on  57(11):4369–4377  2009.

[5] Sergio V. Macua  Pavle Belanovic  and Santiago Zazo. Consensus-based Distributed Principal Component
In Signal Processing Advances in Wireless Communications

Analysis in Wireless Sensor Networks.
(SPAWC)  2010 IEEE Eleventh International Workshop on  pages 1–5  June 2010.

[6] Roberto Tron and Rene Vidal. Distributed Computer Vision Algorithms Through Distributed Averaging.

In IEEE Conference on Computer Vision and Pattern Recognition  pages 57–63  2011.

[7] Lin Xiao  Stephen Boyd  and Sanjay Lall. A Scheme for Robust Distributed Sensor Fusion Based on
Average Consensus. In International Conference on Information Processing in Sensor Networks  pages
63–70  April 2005.

[8] R. Olfati-Saber. Distributed Kalman Filtering for Sensor Networks. In Decision and Control  2007 46th

IEEE Conference on  pages 5492 –5498  dec. 2007.

[9] Bi Song  A.T. Kamal  C. Soto  Chong Ding  J.A. Farrell  and A.K. Roy-Chowdhury. Tracking and Activity
Recognition Through Consensus in Distributed Camera Networks. Image Processing  IEEE Transactions
on  19(10):2564 –2579  oct. 2010.

[10] P.A. Forero  A. Cano  and G.B. Giannakis. Distributed Clustering Using Wireless Sensor Networks.

Selected Topics in Signal Processing  IEEE Journal of  5(4):707 –724  aug. 2011.

[11] Sam Roweis and Zoubin Ghahramani. A Unifying Review of Linear Gaussian Models. Neural Compu-

tation  11:305–345  1999.

[12] Ami Wiesel  Yonina C. Eldar  and Alfred O. Hero. Covariance Estimation in Decomposable Gaussian

Graphical Models. IEEE Transactions on Signal Processing  58(3):1482–1492  2010.

[13] Andrew R. Conn  Nicholas I. M. Gould  and Philippe L. Toint. A globally convergent augmented La-
grangian algorithm for optimization with general constraints and simple bounds. SIAM J. Numer. Anal. 
28:545–572  February 1991.

[14] Robert Michael Lewis and Virginia Torczon. A Globally Convergent Augmented Lagrangian Pattern
Search Algorithm for Optimization with General Constraints and Simple Bounds. SIAM J. on Optimiza-
tion  12:1075–1089  April 2002.

[15] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed Optimization
and Statistical Learning via the Alternating Direction Method of Multipliers. In Michael Jordan  editor 
Foundations and Trends in Machine Learning  volume 3  pages 1–122. Now Publishers  2011.

[16] Pedro A. Forero  Alfonso Cano  and Geogios B. Giannakis. Consensus-Based Distributed Support Vector

Machines. Journal of Machine Learning Research  11:1663–1707  2010.

[17] Michael E. Tipping and Chris M. Bishop. Probabilistic Principal Component Analysis. Journal of the

Royal Statistical Society  Series B  61:611–622  1999.

[18] Alexander Ilin and Tapani Raiko. Practical Approaches to Principal Component Analysis in the Presence

of Missing Values. Journal of Machine Learning Research  11:1957–2000  2010.

[19] Carlo Tomasi and Takeo Kanade. Shape and motion from image streams under orthography: a factoriza-

tion method. International Journal of Computer Vision  9:137–154  1992. 10.1007/BF00129684.

[20] Pierre Moreels and Pietro Perona. Evaluation of Features Detectors and Descriptors based on 3D Objects.

International Journal of Computer Vision  73(3):263–284  July 2007.

[21] Carlo Tomasi and Takeo Kanade. Detection and Tracking of Point Features. Technical Report CMU-CS-

91-132  Carnegie Mellon University  April 1991.

[22] Roberto Tron and Rene Vidal. A Benchmark for the Comparison of 3-D Motion Segmentation Algo-

rithms. In IEEE International Conference on Computer Vision and Pattern Recognition  2007.

[23] Yasuko Chikuse. Statistics on Special Manifolds  volume 174 of Lecture Notes in Statistics. Springer  1

edition  February 2003.

9

,Sanjoy Dasgupta
Samory Kpotufe
Jessa Bekker
Jesse Davis
Arthur Choi
Adnan Darwiche
Guy Van den Broeck
Hong Chen
Haifeng Xia
Heng Huang
Weidong Cai
Jen Ning Lim
Makoto Yamada
Bernhard Schölkopf
Wittawat Jitkrittum