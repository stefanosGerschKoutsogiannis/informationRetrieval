2019,Flexible Modeling of Diversity with Strongly Log-Concave Distributions,Strongly log-concave (SLC) distributions are a rich class of discrete probability distributions over subsets of some ground set. They are strictly more general than strongly Rayleigh (SR) distributions such as the  well-known determinantal point process. While SR distributions offer elegant models of diversity  they lack an easy control over how they express diversity. We propose SLC as the right extension of SR that enables easier  more intuitive control over diversity  illustrating this via examples of practical importance. We develop two fundamental tools needed to apply SLC distributions to learning and inference: sampling and mode finding.  For sampling we develop an MCMC sampler and give theoretical mixing time bounds. For mode finding  we establish a weak log-submodularity property for SLC functions and derive optimization guarantees for a distorted greedy algorithm.,Flexible Modeling of Diversity with Strongly

Log-Concave Distributions

Massachusetts Institute of Technology

Massachusetts Institute of Technology

Joshua Robinson

joshrob@mit.edu

Suvrit Sra

suvrit@mit.edu

Stefanie Jegelka

Massachusetts Institute of Technology

stefje@csail.mit.edu

Abstract

Strongly log-concave (SLC) distributions are a rich class of discrete probability
distributions over subsets of some ground set. They are strictly more general than
strongly Rayleigh (SR) distributions such as the well-known determinantal point
process. While SR distributions offer elegant models of diversity  they lack an easy
control over how they express diversity. We propose SLC as the right extension
of SR that enables easier  more intuitive control over diversity  illustrating this via
examples of practical importance. We develop two fundamental tools needed to
apply SLC distributions to learning and inference: sampling and mode ﬁnding.
For sampling we develop an MCMC sampler and give theoretical mixing time
bounds. For mode ﬁnding  we establish a weak log-submodularity property for
SLC functions and derive optimization guarantees for a distorted greedy algorithm.

Introduction

1
A variety of machine learning tasks involve selecting diverse subsets of items. How we model
diversity is  therefore  a key concern with possibly far-reaching consequences. Recently popular
probabilisitic models of diversity include determinantal point processes [32  39]  and more generally 
strongly Rayleigh (SR) distributions [8  35]. These models have been successfully deployed for subset
selection in applications such as video summarization [44]  fairness [13]  model compression [46] 
anomaly detection [49]  the Nyström method [41]  generative models [24  40]  and accelerated
coordinate descent [51]. While valuable and broadly applicable  SR distributions have one main
drawback: it is difﬁcult to control the strength and nature of diversity they model.
We counter this drawback by leveraging strongly log-concave (SLC) distributions [3–5]. These
distributions are strictly more general than SR measures  and possess key properties that enable easier 
more intuitive control over diversity. They derive their name from SLC polynomials introduced by
Gurvits already a decade ago [30]. More recently they have shot into prominence due to their key
role in developing deep connections between discrete and continuous convexity  with subsequent
applications in combinatorics [1  10  33]. In particular  they lie at the heart of recent breakthrough
results such as a proof of Mason’s conjecture [4] and obtaining a fully polynomial-time approximation
scheme for counting the number of bases of arbitrary matroids [3  5]. We remark that all these works
assume homogeneous SLC polynomials.
We build on this progress to develop fundamental tools for general SLC distributions  namely 
sampling and mode ﬁnding. We highlight the ﬂexibility of SLC distributions through two settings of
importance in practice: (i) raising any SLC distribution to a power α ∈ [0  1]; and (ii) incorporating a
constraint that allows sampling sets of any size up to a budget. In contrast to similar modiﬁcations

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

to SR measures (see e.g.  [49])  these settings retain the crucial SLC property. Setting (i) allows us
to conveniently tune the strength of diversity by varying a single parameter; while setting (ii) offers
greater ﬂexibility than ﬁxed cardinality distributions such as a k-determinantal point process [38].
This observation is simple yet important  especially since the “right” value of k is hard to ﬁx a priori.

Contributions. We brieﬂy summarize the main contributions of this work below.
(cid:4) We introduce the class of strongly log-concave distributions to the machine learning community 

showing how it can offer a ﬂexible discrete probabilistic model for distributions over subsets.

(cid:4) We prove various closure properties of SLC distributions (Theorems 2-5)  and show how to use

these properties for better controlling the distributions used for inference.

(cid:4) We derive sampling algorithms for SLC and related distributions  and analyze their corresponding

mixing times both theoretically and empirically (Algorithm 1  Theorem 8).

(cid:4) We study the negative dependence of SLC distributions by deriving a weak log-submodularity
property (Theorem 11). Optimization guarantees for a selection of greedy algorithms are obtained
as a consequence (Theorem 12).

As noted above  our results build on the remarkable recent progress in [3–5] and [10]. The biggest
difference between the previous work and this work is our focus on general non-homogeneous SLC
polynomials  corresponding to distributions over sets of varying cardinality  as opposed to purely
the homogeneous  i.e.  ﬁxed-cardinality  case. This broader focus necessitates development of some
new machinery  because unlike SR polynomials  the class of SLC polynomials is not closed under
homogenization. We summarize the related work below for additional context.

1.1 Related work

SR polynomials. Strongly Rayleigh distributions were introduced in [8] as a class of discrete
distributions possessing several strong negative dependence properties. It did not take long for their
potential in machine learning to be identiﬁed [39]. Particular attention has been paid to determinantal
point processes due to the intuitive way they capture negative dependence  and the fact that they
are parameterized by a single positive semi-deﬁnite kernel matrix. Convenient parameterization has
allowed an abundance of fast algorithms for learning the kernel matrix [23  26  45  47]  and sampling
[2  42  50]. SR distributions are a fascinating and elegant probabilistic family whose applicability in
machine learning is still an emerging topic [17  35  43  48].
SLC polynomials. Gurvits introduced SLC polynomials a decade ago [30] and studied their con-
nection to discrete convex geometry. Recently this connection was signiﬁcantly developed [10  5]
by establishing that matroids  and more generally M-convex sets  are characterized by the strong
log-concavity of their generating polynomial. This is in contrast to SR  for which it is known that
some matroids have generating polynomials that are not SR [9].
Log-submodular distributions. Distributions over subsets that are log-submodular (or supermod-
ular) are amenable to mode ﬁnding and variational inference with approximation guarantees  by
exploiting the optimization properties of submodular functions [20–22]. Theoretical bounds on
sampling time require additional assumptions [29]. Iyer and Bilmes [34] analyze inference for
submodular distributions  establishing polynomial approximation bounds.
MCMC samplers and mixing time. The seminal works [18  19] offer two tools for obtaining
mixing time bounds for Markov chains: lower bounding the spectral gap  or log-Sobolev constant.
These techniques have been successfully deployed to obtain mixing time bounds for homogenous SR
distributions [2]  general SR distributions [42]  and recently homogenous SLC distributions [5].

2 Background and setup
Notation. We write [n] = {1  . . .   n}  and denote by 2[n] the power set {S | S ⊆ [n]}. For any
deﬁne|α| =(cid:80)n
variable u  write ∂u to denote ∂
∂u; in case u = zi  we often abbreviate further by writing ∂i instead
of ∂zi. For S ⊆ [n] and α ∈ Nn let 1S ∈ {0  1}n denote the binary indicator vector of S  and
(cid:81)
i∈S zi and(cid:81)n
i where αi = 0
means we do not take any derivatives with respect to zi. We let zS and zα denote the monomials
respectively. For K = R or R+ we write K[z1  . . .   zn] to denote the set of
all polynomials in the variables z = (z1  . . .   zn) whose coefﬁcients belong to K. A polynomial is

i=1 αi. We also write variously ∂S

z =(cid:81)

z =(cid:81)

i∈S ∂i and ∂α

i∈[n] ∂αi

i=1 zαi

i

2

said to be d-homogeneous if it is the sum of monomials all of which are of degree d. Finally  for a set
X we shall minimize clutter by using X ∪ i and X \ i to denote X ∪ {i} and X \ {i} respectively.
SLC distributions. We consider distributions π : 2[n] → [0  1] on the subsets of a ground set [n].
There is a one-to-one correspondence between such distributions  and their generating polynomials
(1)

(cid:88)

(cid:88)

(cid:89)

π(S)zS.

fπ(z) :=

π(S)

S⊆[n]

zi =

i∈S

S⊆[n]

The central object of interest in this paper is the class of strongly log-concave distributions  which is
deﬁned by imposing certain log-concavity requirements on the corresponding generating polynomials.
Deﬁnition 1. A polynomial f ∈ R+[z1  . . .   zn] is strongly log-concave (SLC) if every derivative of
f is log-concave. That is  for any α ∈ Nn either ∂αf = 0  or the function log(∂αf (z)) is concave
at all z ∈ Rn
+. We say a distribution π is strongly log-concave if its generating polynomial fπ is
strongly log-concave; we also say π is d-homogeneous if fπ is d-homogeneous.

There are many examples of SLC distributions; we note a few important ones below.
– Determinantal point processes [39  27  38  41]  and more generally  Strongly Rayleigh (SR)

distributions [8  17  43  35].

– Exponentiated (for exponents in [0  1]) homogeneous SR distributions [49  5].
– The uniform distribution on the independent sets of a matroid [4].
SR distributions satisfy several strong negative dependence properties (e.g.  log-submodularity and
negative association). The fact that SLC is a strict superset of SR suggests that SLC distributions
possess some weaker negative dependence properties. These properties will play a crucial role in the
two fundamental tasks that we study in this paper: sampling and mode ﬁnding.

Sampling. Our ﬁrst task is to efﬁciently draw samples from an SLC distribution π. To that end  we
seek to develop Markov Chain Monte Carlo (MCMC) samplers whose mixing time (see Section 4 for
deﬁnition) can be well-controlled. For homogeneous π  the breakthrough work of Anari et al. [5]
provides the ﬁrst analysis of fast-mixing for a simple Markov chain called Base Exchange Walk;
this analysis is further reﬁned in [15]. Base Exchange Walk is deﬁned as follows: if currently
at state S ⊆ [n]  remove an element i ∈ S uniformly at random. Then move to R ⊃ S \ {i} with
probability proportional to π(R). This describes a transition kernel Q(S  R) for moving from S to R.
We build on these works to obtain the ﬁrst mixing time bounds for sampling from general (i.e.  not
necessarily homogeneous) SLC distributions (Section 4).

Mode ﬁnding. Our second main goal is optimization  where we consider the more general task of
ﬁnding a mode of an SLC distribution subject to a cardinality constraint. This task involves solving
max|S|≤d π(S). This task is known to be NP-hard even for SR distributions; indeed  the maximum
volume subdeterminant problem [14] is a special case. We consider a more practical approach based
on observing that SLC distributions satisfy a relaxed notion of log-submodularity  which enables us
to adapt simple greedy algorithms.
Before presenting the details about sampling and optimization  we need to ﬁrst establish some key
theoretical properties of general SLC distributions. This is the subject of the next section.
3 Theoretical tools for general SLC polynomials

In this technical section we develop the theory of strong log-concavity by detailing several transfor-
mations of an SLC polynomial f that preserve strong log-concavity. Such closure properties can be
essential for proving the SLC property  or for developing algorithmic results. Due to the correspon-
dence between distributions on 2[n] and their generating polynomials  each statement concerning
polynomials can be translated into a statement about probability distributions. The forthcoming
results assume polynomials that are supported on the independent sets of a matroid. This can be
viewed as a minor technical assumption since  to the best of our knowledge  all known SLC polyno-
mials are supported on the independent sets of a matroid. A fundamental correspondence between
homogenous SLC distributions and bases of a matroid was observed in [10]  however it remains an
open question to precisely understand this relationship for non-homogenous SLC polynomials. The
following theorem is a crucial stepping stone to sampling from non-homogeneous SLC distributions 
and to sampling with cardinality constraints.

3

Theorem 2. Let f =(cid:80)

S⊆[n] cSzS ∈ R+[z1  . . .   zn] be SLC  and suppose the support of the sum is
the collection of independent sets of a rank d matroid. Then for any k ≤ d the following polynomial
is SLC:

Hkf (z  y) =

cS

(k −|S|)!

zSyk−|S|.

(cid:88)

|S|≤k

(cid:88)

|S|≤k

Theorem 3. Let f =(cid:80)

The above operation is also referred to as scaled homogenization  since the resulting polynomial
is homogeneous and there is an added 1/(k −|S|)! factor. In fact  we may extend Theorem 2 to
allowing the user to add an additional exponentiating factor:
S⊆[n] cSzS ∈ R+[z1  . . .   zn] be SLC  and suppose the support of the sum
is the collection of independent sets of a rank d matroid. Then for 0 ≤ α ≤ 1 and any k ≤ d the
following polynomial is SLC:

Hk αf (z  y) =

cα
S

(k −|S|)!

zSyk−|S|.

Notably  Theorem 3 fails for all α > 1. For a proof of this see Appendix A.2.
Next  we show that polarization preserves strong log-concavity. Polarization essentially means to
replace a variable with a higher power by multiple “copies”  each occurring only with power one  in
a way that the resulting polynomial is symmetric (or permutation-invariant) in those copies. This is
achieved by averaging over elementary symmetric polynomials. Formally  the polarization of the

polynomial f =(cid:80)|S|≤d cSzSyd−|S| ∈ R[z1  . . .   zn  y] is deﬁned to be

Πf (z1  . . .   zn  y1  . . .   yd) =

cSzS

ed−|S|(y1  . . .   yd)

(cid:88)

|S|≤d

(cid:19)−1

(cid:18) d

|S|

where ek(y1  . . .   yd) is the kth elementary symmetric polynomial in d variables. The polarization
Πf has the following three properties:

1. It is symmetric in the variables y1  . . .   yd;
2. Setting y1 = . . . = yd = y recovers f;
3. Πf is multiafﬁne  and hence the generating polynomial of a distribution on 2[n+d].

Closure under polarization  combined with the homogenization results (Theorems 2 and 3) allows
non-homogeneous distributions to be transformed into homogenous ones. This allows general SLC
distributions to be transformed into homogenous SLC distributions for which fast mixing results are
known [5]. How to work backwards to obtain samples from the original distribution will be the topic
of the next section.
S⊆[n] cSzSyd−|S| ∈ R+[z1  . . .   zn  y] be SLC  and the support of the sum

Theorem 4. 1 Let f =(cid:80)

is the collection of independent sets of a rank d matroid. Then the polarization Πf is SLC.

Corollary 5. Let f =(cid:80)

Putting all of the preceding results together we obtain the following important corollary. It is this
observation that will allow us to do mode ﬁnding for SLC distributions and exponentiated  cardinality
constrained SLC distributions.
S⊆[n] cSzS ∈ R+[z1  . . .   zn] be SLC  and suppose the support of the sum
is the collection of independent sets of a rank d matroid. Then Π(Hk αf ) is SLC for any k ≤ d and
0 ≤ α ≤ 1.

In Appendix A.4 we also show that SLC distributions are closed under conditioning on a ﬁxed set
size. We mention those results since they may be of independent interest  but omit them from the
main text since we do not use them further in this paper.

1This result was independently discovered by Brändén and Huh [10].

4

4 Sampling from strongly log-concave distributions

In this section we outline how to use the SLC closure results from Section 3 to build a sampling
algorithm for general SLC distributions and prove mixing time bounds. Recall that we are considering
a probability distribution π : 2[n] → [0  1] that is strongly log-concave. The mixing time of a Markov

chain (Q  π) started at S0 is tS0(ε) = min{t ∈ N |(cid:13)(cid:13)Qt(S0 ·) − π(cid:13)(cid:13)1 ≤ ε} where Qt is the

t-step transition kernel. For the remainder of this section we consider the distribution ν where
ν(S) ∝ π(S)α1{|S| ≤ d} for 0 ≤ α ≤ 1  and d ∈ [n]. In particular  this includes π itself. The power
α allows to vary the degree of diversity induced by the distribution: α < 1 smooths ν  making it less
diverse. Indeed  as α → 0  ν converges to the uniform distribution  which promotes no diversity.
Meanwhile α > 1 (although outside the scope of our results) makes ν more pointy  with ν collapsing
to a point mass as α → ∞.
Our strategy is as follows: we ﬁrst “extend” ν to a distribution νsh over subsets of size |n| of [n + d]
to obtain a homogeneous distribution. If we can sample from νsh  then we can extract a sample
S ⊆ [n] of a scaled version of ν by simply restricting a sample T ∼ νsh to T ∩ [n]. If ν was SR  then
νsh would also be SR  and a fast sampler follows from this observation [42]. But  for general SLC
distributions (and their powers)  νsh is not SLC  and deriving a sampler is more challenging.
To still enable the homogenization strategy  we instead derive a carefully scaled version of a homoge-
neous version of ν that  as we prove  is homogeneneous and SLC and hence tractable. We use this
rescaled version as a proposal distribution in a sampler for νsh. To obtain an appropriately scaled
extended  homogeneous variant ν  we ﬁrst translate Corollary 5 into probabilistic language.
Theorem 6. Suppose that the support of the sum in the generating polynomial of ν is the collection
of independent sets of a rank d matroid. Then for any k ≤ d the following probability distribution on
2[n+k] is SLC:

(cid:0)
|S∩[n]|(cid:1)−1

k



0 

Hkν(S) ∝

ν(S∩[n])
(k−|S∩[n]|)!

 

for all S ⊆ [n + k] such that|S| = k
otherwise.

Proof. Observe that the generating polynomial of Hkν is Π(Hkf ) where f denotes the generating
polynomial of ν. The result follows immediately from Corollary 5.
The ultimate proposal that we use is not Hkν  but a modiﬁed version µ that better aligns with ν:

(cid:19)d−|S∩[n]|

(cid:18) d

e

µ(S) ∝

Hdν(S).

Proposition 7. If ν is SLC  then µ is SLC.

Proof. Lemma 39 in the Appendix says that strong log-concavity is preserved under linear transforma-
tions of the coordinates. This implies that µ is SLC since its generating polynomial is Π((Hdf ) ◦ T )
where f is the generating polynomial of ν and T is the linear transform deﬁned by: y (cid:55)→ d
e y and
zi (cid:55)→ zi for i = 1 . . .   n.
Importantly  since µ is homogeneous and SLC  the Base Exchange Walk for µ mixes rapidly.
Let Q denote the Markov transition kernel for Base Exchange Walk on 2[n+d] for µ. We use Q as
a proposal  and then compute the appropriate acceptance probability to obtain a chain that mixes to
the symmetric homogenization νsh of ν. The target νsh is a d-homogenous distribution on 2[n+d]:

νsh(S) ∝

ν(S ∩ [n])  for all S ⊆ [n + d] such that|S| = d.

(cid:80)
A crucial property of νsh is that its marginalization over the “dummy” variables yields ν  i.e. 
T :T∩[n]=S νsh(T ) = ν(S). Therefore  after obtaining a sample T ∼ νsh one then obtains a sample
from ν by computing T ∩ [n].
It is a simple computation to show that the acceptance probabilities in Algorithm 1 are indeed the
Metropolis-Hastings acceptance probabilities for sampling from νsh using the proposal Q. Therefore

(cid:18) d
(cid:12)(cid:12)S ∩ [n](cid:12)(cid:12)(cid:19)−1

5

Propose move T ∼ Q(S ·)

Algorithm 1 Metropolis-Hastings sampler for νsh with proposal Q
1: Initialize S ⊆ [n + d]
2: while not mixed do
3:
4:
5:
6:
7:
8:
9:
10:

Set k ←(cid:12)(cid:12)S ∩ [n](cid:12)(cid:12)
if(cid:12)(cid:12)T ∩ [n](cid:12)(cid:12) = k − 1 then
if(cid:12)(cid:12)T ∩ [n](cid:12)(cid:12) = k then
if(cid:12)(cid:12)T ∩ [n](cid:12)(cid:12) = k + 1 then

R ← T with probability min{1  e
R ← T
R ← T with probability min{1  d

(d−k)}  otherwise stay at S

1

e

d (d − k + 1)}  otherwise stay at S

(Q  νsh) is tS0(ε) = min{t ∈ N |(cid:13)(cid:13)Qt(S0 ·) − νsh

the chain mixes to νsh. We obtain the following mixing time bound  recalling that the mixing time of
Theorem 8. For d ≥ 8 the mixing time of the chain in Algorithm 1 started at S0 satisﬁes the bound

(cid:13)(cid:13)1 ≤ ε}.
(cid:19) 1
(cid:26)(cid:18) d

(cid:27)

√
tS0(ε) ≤ 1
2π
e

d5/22d

log log

|S0|

ν(S0)

+ log

1
2ε2

(cid:19)

.

(cid:18)

n and equal to Ω(2n/2) if d ≤ n/2 − √

A similar bound holds for d < 8. We note that although the mixing time bound scales poorly in d  the
bound has the interesting property of being independent of the ground set size n. Furthermore  the

bound is meaningful since the total number of subsets of n objects of size d is(cid:80)d
if d ≥ n/2 − √

n  [36]. So the mixing time bound is
exponentially better than brute force. Later we will detail experiments that suggest that this bound is
loose in d.
Efﬁcient implementation. It is sufﬁcient to only maintain R = S ∩ [n] since νsh is exchangeable
in the variables {n + 1  . . .   n + d}. Sampling T ∼ Q(S ·) involves dropping i ∈ S uniformly at
random  then computing the probability of µ((S\ i)∪ j) for each j not in S\ i. However again  by the
exchangeability of µ in {n+1  . . .   n+d} this probability is the same for each j in {n+1  . . .   n+d}
and so only needs to be performed for one such j.

(cid:1) = Ω(2n)

(cid:0)n

j=0

j

5 Maximization of weakly log-submodular functions

In this section we explore the negative dependence properties of SLC functions (unnormalized SLC
distributions) through the lens of submodularity: a well known negative dependence property [8].
In an earlier version of this paper we conjectured that SLC functions have the strong property of
log-submodularity. This conjecture has been disproved in a recent note [28].
Proposition 9 (Propositions 1 and 2 [28]). The the distribution with generating polynomial

(cid:0)4 + 3(x + y + z) + 3(xy + xz + yz)(cid:1)

f (x  y  z) =

1
22

is SLC but not log-submodular.

In response  we introduce a new notion of weak submodularity and show that any function ν such
that Hdν is SLC is weakly log-submodular. Finally  we prove that a distorted greedy optimization
procedure leads to optimization guarantees for weak (log-)submodular functions for the cardinality
constrained problem OPT ∈ arg max|S|≤k ν(S). Appendix C contains similar results for constrained
greedy optimization of increasing weak (log-)submodular functions and unconstrained double greedy
optimization of non-negative (log-)submodular functions.
Deﬁnition 10. We call a function ρ : 2[n] → R γ-weakly submodular if for any S ⊆ [n] and
i  j ∈ [n] \ S with i and j not equal  we have

ρ(S) + ρ(S ∪ {i  j}) ≤ γ + ρ(S ∪ i) + ρ(S ∪ j).

6

We say ν : 2[n] → R+ is γ-weakly log-submodular if log ν is (log γ)-weakly submodular.
When γ = 0 this reduces to the classic notion of submodularity. Note carefully that our notion of
weak submodularity differs from a notion of weak submodularity that already appears in the literature
[16  31  37]. Building on a result by Brändén and Huh [10]  we prove the following result.
Theorem 11. Any non-negative function ρ : 2[n] → R+ with support contained in {S ⊆ [n] :|S| ≤
d} and generating polynomial f such that Hdf is strongly log-concave is γ-weakly log-submodular

Next we show how weak log-submodularity gives a path to optimizing strongly log-concave functions.
Consider ρ : 2[n] → R  assumed to be γ-weakly submodular. Note in particular we do not assume
that ρ is non-negative. This is important since we are interested in applying this procedure to the
logarithm of a distribution  which need not be non-negative. Deﬁne ce = max{ρ([n]\ e)− ρ([n])  0} 
e∈S ce. We use the convention that c(∅) = 0. Then we may decompose ρ = η − c

where η = ρ + c. Note that η is γ-weakly submodular and c is a non-negative function.
We will extend the distorted greedy algorithm by [25  31] to our notion of weak submodularity. To
do so  we introduce the distorted objective Φi(S) = (1 − 1/k)k−iη(S) − c(S) for i = 0  . . . k.
The distorted greedy algorithm greedily builds a set R of size at most d by forming a sequence
∅ = S0  S1  . . .   Sk−1  Sk = R such that Si+1 is formed by adding the element ei ∈ [n] to Si that
maximizes Φi+1(Si ∪ ei) − Φi+1(Si) so long as the increment is positive.
Algorithm 2 Distorted greedy weak submodular constrained maximization of ν = η − c
1: Let S0 = ∅
2: for i = 0  . . .   k − 1 do
3:
4:
5:
6:
7: return R = Sk
Theorem 12. Suppose ρ : 2[n] → R is γ-weakly submodular and ρ(∅) = 0. Then the solution
R = Sk obtained by the distorted greedy algorithm satisﬁes

Set ei = arg maxe∈[n] Φi+1(Si ∪ e) − Φi+1(Si)
if Φi+1(Si ∪ ei) − Φi+1(Si) > 0 then
else Si+1 ← Si

Si+1 ← Si ∪ ei

(cid:18)

(cid:19)(cid:18)

(cid:19)

ρ(R) = η(R) − c(R) ≥

1 − 1
e

η(OPT) − 1
2

(cid:96)((cid:96) − 1)γ

− c(OPT) 

where OPT ∈ arg max|S|≤k ρ(S) and (cid:96) :=|OPT| ≤ k.
Note any weakly submodular function can be brought into the required form by subtracting ρ(∅) if it
is non-zero. If ν is weakly log-submodular  we can decompose ν = η/c such that log η and log c
perform the same role as η and c did in the weakly submodular setting. Then by applying Theorem
12 to log ν we obtain the following corollary.
Corollary 13. Suppose ν : 2[n] → R+ is γ-weakly log-submodular and ν(∅) = 1. Then the solution
R = Sk obtained by the distorted greedy algorithm satisﬁes

(cid:1).

for γ = 4(cid:0)1 − 1
and c(S) =(cid:80)

d

ν(R) =

η(R)
c(R)

6 Experiments

≥ γ− 1

2 (cid:96)((cid:96)−1)(1−1/e) η(OPT)1−1/e

.

c(OPT)

In this section we empirically evaluate the mixing time of Algorithm 1. We use the standard potential
scale reduction factor metric to measure convergence to the stationary distribution [11]. The method
involves running several chains in parallel and computing the average variance within each chain and
between the chains. The PSRF score is the ratio of the between variance over the within variance and
is usually above 1. When the PSRF score is close to 1 then the chains are considered to be mixed. In
all of our experiments we run three chains in parallel and declare them to be mixed once the PSRF
score drops below 1.05.
Figure 1 considers the results of running the Metropolis-Hastings algorithm on a sequence of
problems with different cardinality constraints d.
In each case we considered the distribution

7

(a)

(b)

Figure 1: Empirical mixing time analysis for sampling a ground set of size n = 250 and various
cardinality constraints d  (a) the PSRF score for each set of chains  (b) the approximate mixing time
obtained by thresholding at PSRF equal to 1.05.

(a)

(b)

(c)

Figure 2: (a b) Empirical mixing time analysis for sampling a set of size at most d = 40 for varying
ground set sizes  (a) the PSRF score for each set of chains  (b) the approximate mixing time obtained
by thresholding at PSRF equal to 1.05  (c) comparison of Algorithm 1 and a M-H algorithm where
the proposal is built using Hdν: d = 100 and n = 250.

ν(S) ∝(cid:112)det(LS)1{|S| ≤ d} where L is a randomly generated 250 × 250 PSD matrix. Here LS
(cid:112)det(LS)1{|S| ≤ 40} where L is a randomly generated PSD matrix where of appropriate size n.

denotes the|S| ×|S| submatrix of L whose indices belong to S. These simulations suggest that the
mixing time grows linearly in d for a ﬁxed n.
Figure 2 considers the results of running the Metropolis-Hastings algorithm on a sequence of
problems with different ground set sizes. In each case we considered the distribution ν(S) ∝

These simulations suggest that the mixing time grows sublinearly in n for a ﬁxed d.
It is important to know whether the mixing time is robust to different spectra σL of L. We consider
three cases  (i) smooth decay σL = [n]  (ii) a single large eigenvalue σL = {n  (n − 1)/2  (n −
2)/2  . . .   2/2  1/2}  and (iii) one ﬁfth of the eigenvalues are equal to n  the rest equal to 1/n. Note
that due to normalization  multiplying the spectrum by a constant does not affect the resulting
distribution. The results for (i) are the content of Figures 1 and 2 (a b). Figures 3 and 4 show the
results for (ii) and ﬁgures 5 and 6 show the results for (iii). Figures 3-6 can be found in Appendix D.
Finally  we address the question of why the proposal distribution was built using the particular choice
of µ we made. Indeed one may use Base Exchange Walk for any homogenous distribution on
2[n] to build a sampler  one simply needs to compute the appropriate acceptance probabilities. We
restrict our attention to SLC distributions so as to be able to build on the recent mixing time results
for homogenous SLC distributions. An obvious alternative to using µ to build the proposal is to use
Hdν. Figure 2(c) compares the empirical mixing time of these two chains. The strong empirical
improvement justiﬁes our choice of adding the extra rescaling factor d/e.

8

010002000300040005000# Iter1.01.11.21.31.41.5PSRFPotential Scale Reduction Factord = 30d = 60d = 90d = 120d = 150d = 180406080100120140160180cardinality constraint d10001250150017502000225025002750mixing timeApproximate Mixing Time010002000300040005000# Iter1.01.11.21.31.4PSRFPotential Scale Reduction Factorn = 30n = 60n = 90n = 120n = 150n = 1802004006008001000ground set size n6008001000120014001600mixing timeApproximate Mixing Time010002000300040005000# Iter1.11.21.31.41.5PSRFPotential Scale Reduction Factord/e rescalingno rescaling7 Discussion

In this paper we introduced strongly log-concave distributions as a promising class of models for
diversity. They have ﬂexibility beyond that of strongly Rayleigh distributions  e.g.  via exponentiated
and cardinality constrained distributions (which do not preserve the SR property). We derived a
suite of MCMC samplers for general SLC distributions and associated mixing time bounds. For
optimization  we showed that SLC distributions satisfy a weak submodularity property and used this
to prove mode ﬁnding guarantees.
Still  many open problems remain. Although the mixing time bound has the interesting property of not
directly depending on n  the O(2d) dependence seems quite conservative compared to the empirical
mixing time results. An important future direction would be to close this gap. More fundamentally 
the negative dependence properties of SLC distributions need to be explored in greater detail. Finally 
in order for SLC models to be deployed in practice the user needs a way to learn a good SLC model
from data  a non-trivial task in general since SLC distribution are non-parametric. However  both
exponentiation and cardinality constraint add a single parameter that must be learned. We leave the
question of how best to learn these parameters as an important topic for future work.

Acknowledgements

This work was supported by an NSF-BIGDATA award and the Defense Advanced Research Projects
Agency (grant number YFA17 N66001-17-1-4039). The views  opinions  and/or ﬁndings contained
in this article are those of the author and should not be interpreted as representing the ofﬁcial views
or policies  either expressed or implied  of the Defense Advanced Research Projects Agency or the
Department of Defense. We thank Matt Staib for helpful comments on the draft.

References
[1] Karim Adiprasito  June Huh  and Eric Katz. Hodge theory for combinatorial geometries. Annals

of Mathematics  188(2):381–452  2018.

[2] Nima Anari  Shayan Oveis Gharan  and Alireza Rezaei. Monte Carlo Markov chain algorithms
for sampling strongly Rayleigh distributions and determinantal point processes. In Conference
on Learning Theory  pages 103–115  2016.

[3] Nima Anari  Shayan Oveis Gharan  and Cynthia Vinzant. Log-concave polynomials  en-
tropy  and a deterministic approximation algorithm for counting bases of matroids. In Annual
Symposium on Foundations of Computer Science  pages 35–46. IEEE  2018.

[4] Nima Anari  Kuikui Liu  Shayan Oveis Gharan  and Cynthia Vinzant. Log-Concave Poly-
nomials III: Mason’s Ultra-Log-Concavity Conjecture for Independent Sets of Matroids.
arXiv:1811.01600  2018.

[5] Nima Anari  Kuikui Liu  Shayan Oveis Gharan  and Cynthia Vinzant. Log-Concave Polynomials
II: High-Dimensional Walks and an FPRAS for Counting Bases of a Matroid. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing. ACM  June 2019.

[6] Ravi B Bapat  Ravindra B Bapat  and Raghavan. Nonnegative matrices and applications 

volume 64. Cambridge University Press  1997.

[7] Christian Berg  Jens Peter Reus Christensen  and Paul Ressel. Harmonic analysis on semigroups:

theory of positive deﬁnite and related functions  volume 100. Springer  1984.

[8] Julius Borcea  Petter Brändén  and Thomas Liggett. Negative Dependence and the Geometry of

Polynomials. Journal of the American Mathematical Society  22(2):521–567  2009.

[9] Petter Brändén. Polynomials with the half-plane property and matroid theory. Advances in

Mathematics  216(1):302–320  2007.

[10] Petter Brändén and June Huh. Lorentzian polynomials. arXiv:1902.03719  2019.
[11] Stephen P Brooks and Andrew Gelman. General Methods for Monitoring Convergence of
Iterative Simulations. Journal of computational and graphical statistics  7(4):434–455  1998.
[12] Niv Buchbinder  Moran Feldman  Joseph Sefﬁ  and Roy Schwartz. A Tight Linear Time (1/2)-
Approximation for Unconstrained Submodular Maximization. SIAM Journal on Computing  44
(5):1384–1402  2015.

9

[13] L Elisa Celis  Vijay Keswani  Damian Straszak  Amit Deshpande  Tarun Kathuria  and
Nisheeth K Vishnoi. Fair and diverse DPP-based data summarization. arXiv:1802.04023 
2018.

[14] Ali Civril and Malik Magdon-Ismail. Exponential inapproximability of selecting a maximum

volume sub-matrix. Algorithmica  65(1):159–176  2013.

[15] Mary Cryan  Heng Guo  and Giorgos Mousa. Modiﬁed log-Sobolev inequalities for strongly

log-concave distributions. arXiv:1903.06081  2019.

[16] Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset
In International Conference on

selection  sparse approximation and dictionary selection.
Machine Learning  2011.

[17] Michal Derezinski and Manfred K Warmuth. Unbiased Estimates for Linear Regression via
Volume Sampling. In Advances in Neural Information Processing Systems  pages 3084–3093 
2017.

[18] Persi Diaconis  Daniel Stroock  et al. Geometric Bounds for Eigenvalues of Markov Chains.

The Annals of Applied Probability  1(1):36–61  1991.

[19] Persi Diaconis  Laurent Saloff-Coste  et al. Logarithmic Sobolev Inequalities for Finite Markov

Chains. The Annals of Applied Probability  6(3):695–750  1996.

[20] Josip Djolonga and Andreas Krause. From map to marginals: Variational inference in bayesian

submodular models. In Neural Information Processing Systems (NIPS)  2014.

[21] Josip Djolonga and Andreas Krause. Scalable variational inference in log-supermodular models.

In International Conference on Machine Learning (ICML)  2015.

[22] Josip Djolonga  Stefanie Jegelka  and Andreas Krause. Provable variational inference for
constrained log-submodular models. In Neural Information Processing Systems (NeurIPS) 
2018.

[23] Christophe Dupuy and Francis Bach. Learning Determinantal Point Processes in Sublinear
Time. Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics 
2018.

[24] Mohamed Elfeki  Camille Couprie  Morgane Riviere  and Mohamed Elhoseiny. GDPP: Learning

Diverse Generations Using Determinantal Point Process. arXiv:1812.00068  2018.

[25] Moran Feldman. Guess free maximization of submodular and linear sums. arXiv:1810.03813 

2018.

[26] Mike Gartrell  Ulrich Paquet  and Noam Koenigstein. Low-rank Factorization of Determinantal

Point Processes. In Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.

[27] Jennifer Gillenwater  Alex Kulesza  and Ben Taskar. Near-optimal map inference for determinan-
tal point processes. In Advances in Neural Information Processing Systems  pages 2735–2743 
2012.

[28] Alkis Gotovos. Strong log-concavity does not imply log-submodularity. arXiv  2019.
[29] Alkis Gotovos  S. Hamed Hassani  and Andreas Krause. Sampling from probabilistic submodu-

lar models. In Neural Information Processing Systems (NIPS)  2015.

[30] Leonid Gurvits. On multivariate Newton-like inequalities. In Advances in Combinatorial

Mathematics  pages 61–78. Springer  2009.

[31] Christopher Harshaw  Moran Feldman  Justin Ward  and Amin Karbasi. Submodular maximiza-
tion beyond non-negativity: Guarantees  fast algorithms  and applications. arXiv:1904.09354 
2019.

[32] J. Ben Hough  Manjunath Krishnapur  Yuval Peres  and Bálint Virág. Determinantal Processes

and Independence. Probab. Surveys  3:206–229  2006.

[33] June Huh. Combinatorial applications of the Hodge-Riemann relations. Proceedings of the

International Congress of Mathematicians  2018.

[34] Rishabh Iyer and Jeffrey Bilmes. Submodular point processes with applications to machine

learning. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

[35] Stefanie Jegelka and Suvrit Sra. Negative dependence  stable polynomials  and all that. NeurIPS

2018 Tutorial  2018.

10

[36] Emil Jeˇrábek. Dual weak pigeonhole principle  Boolean complexity  and derandomization.

Annals of Pure and Applied Logic  129(1-3):1–37  2004.

[37] Rajiv Khanna  Ethan Elenberg  Alexandros G Dimakis  Sahand Negahban  and Joydeep Ghosh.

Scalable greedy feature selection via weak submodularity. arXiv:1703.02723  2017.

[38] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-size determinantal point processes. In Proceedings

of the 28th International Conference on Machine Learning  pages 1193–1200  2011.

[39] Alex Kulesza  Ben Taskar  et al. Determinantal point processes for machine learning. Founda-

tions and Trends in Machine Learning  5(2–3):123–286  2012.

[40] James T Kwok and Ryan P Adams. Priors for diversity in generative latent variable models. In

Advances in Neural Information Processing Systems  pages 2996–3004  2012.

[41] Chengtao Li  Stefanie Jegelka  and Suvrit Sra. Fast DPP Sampling for Nyström with Application
to Kernel Methods. In International Conference on Machine Learning  pages 2061–2070  2016.
[42] Chengtao Li  Suvrit Sra  and Stefanie Jegelka. Fast mixing Markov chains for strongly Rayleigh
measures  DPPs  and constrained sampling. In Advances in Neural Information Processing
Systems  pages 4188–4196  2016.

[43] Chengtao Li  Stefanie Jegelka  and Suvrit Sra. Polynomial time algorithms for dual volume

sampling. In Advances in Neural Information Processing Systems  pages 5038–5047  2017.

[44] Hui Lin and Jeff Bilmes. Learning mixtures of submodular shells with application to document

summarization. In Uncertainty in Artiﬁcial Intelligence (UAI)  2012.

[45] Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes.

In International Conference on Machine Learning  pages 2389–2397  2015.

[46] Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression using determi-

nantal point processes. International Conference on Learning Representations  2016.

[47] Zelda Mariet and Suvrit Sra. Kronecker determinantal point processes. In Advances in Neural

Information Processing Systems  pages 2694–2702  2016.

[48] Zelda Mariet and Suvrit Sra. Elementary symmetric polynomials for optimal experimental

design sr measures. In Advances in Neural Information Processing Systems  2017.

[49] Zelda Mariet  Suvrit Sra  and Stefanie Jegelka. Exponentiated Strongly Rayleigh Distributions.

In Advances in Neural Information Processing Systems  pages 4459–4469  2018.

[50] Zelda Mariet  Yaniv Ovadia  and Jasper Snoek. DPPNet: Approximating Determinantal Point

Processes with Deep Networks. arXiv:1901.02051  2019.

[51] Anton Rodomanov and Dmitry Kropotov. A randomized coordinate descent method with

volume sampling. arXiv:1904.04587  2019.

11

,Joshua Robinson
Suvrit Sra
Stefanie Jegelka