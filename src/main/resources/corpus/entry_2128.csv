2015,Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial,We propose a novel distribution that generalizes the Multinomial distribution to enable dependencies between dimensions. Our novel distribution is based on the parametric form of the Poisson MRF model [Yang et al.  2012] but is fundamentally different because of the domain restriction to a fixed-length vector like in a Multinomial where the number of trials is fixed or known. Thus  we propose the Fixed-Length Poisson MRF (LPMRF) distribution. We develop methods to estimate the likelihood and log partition function (i.e. the log normalizing constant)  which was not developed for the Poisson MRF model. In addition  we propose novel mixture and topic models that use LPMRF as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed Admixture of Poisson MRFs [Inouye et al.  2014]. We show the effectiveness of our LPMRF distribution over Multinomial models by evaluating the test set perplexity on a dataset of abstracts and Wikipedia. Qualitatively  we show that the positive dependencies discovered by LPMRF are interesting and intuitive. Finally  we show that our algorithms are fast and have good scaling (code available online).,Fixed-Length Poisson MRF:

Adding Dependencies to the Multinomial

David I. Inouye

Pradeep Ravikumar

Inderjit S. Dhillon

Department of Computer Science

{dinouye pradeepr inderjit}@cs.utexas.edu

University of Texas at Austin

Abstract

We propose a novel distribution that generalizes the Multinomial distribution to
enable dependencies between dimensions. Our novel distribution is based on the
parametric form of the Poisson MRF model [1] but is fundamentally different be-
cause of the domain restriction to a ﬁxed-length vector like in a Multinomial where
the number of trials is ﬁxed or known. Thus  we propose the Fixed-Length Pois-
son MRF (LPMRF) distribution. We develop AIS sampling methods to estimate
the likelihood and log partition function (i.e. the log normalizing constant)  which
was not developed for the Poisson MRF model. In addition  we propose novel
mixture and topic models that use LPMRF as a base distribution and discuss the
similarities and differences with previous topic models such as the recently pro-
posed Admixture of Poisson MRFs [2]. We show the effectiveness of our LPMRF
distribution over Multinomial models by evaluating the test set perplexity on a
dataset of abstracts and Wikipedia. Qualitatively  we show that the positive de-
pendencies discovered by LPMRF are interesting and intuitive. Finally  we show
that our algorithms are fast and have good scaling (code available online).

1

Introduction & Related Work

The Multinomial distribution seems to be a natural distribution for modeling count-valued data
such as text documents.
Indeed  most topic models such as PLSA [3]  LDA [4] and numerous
extensions—see [5] for a survey of probabilistic topic models—use the Multinomial as the funda-
mental base distribution while adding complexity using other latent variables. This is most likely
due to the extreme simplicity of Multinomial parameter estimation—simple frequency counts—that
is usually smoothed by the simple Dirichlet conjugate prior. In addition  because the Multinomial
requires the length of a document to be ﬁxed or pre-speciﬁed  usually a Poisson distribution on doc-
ument length is assumed. This yields a Poisson-Multinomial distribution—which by well-known
results is merely an independent Poisson model.1 However  the Multinomial assumes independence
between the words because the Multinomial is merely the sum of independent categorical variables.
This restriction does not seem to ﬁt with real-world text. For example  words like “neural” and
“network” will tend to co-occur quite frequently together in NIPS papers. Thus  we seek to relax
the word independence assumption of the Multinomial.
The Poisson MRF distribution (PMRF) [1] seems to be a potential replacement for the Poisson-
Multinomial because it allows some dependencies between words. The Poisson MRF is developed
by assuming that every conditional distribution is 1D Poisson. However  the original formulation
in [1] only allowed for negative dependencies. Thus  several modiﬁcations were proposed in [6]
to allow for positive dependencies. One proposal  the Truncated Poisson MRF (TPMRF)  simply
truncated the PMRF by setting a max count for every word. While this formulation may provide

1The assumption of Poisson document length is not important for most topic models [4].

1

interesting parameter estimates  a TPMRF with positive dependencies may be almost entirely con-
centrated at the corners of the joint distribution because of the quadratic term in the log probability
(see the bottom left of Fig. 1). In addition  the log partition function of the TPMRF is intractable to
estimate even for a small number of dimensions because the sum is over an exponential number of
terms.
Thus  we seek a different distribution than a TPMRF that allows positive dependencies but is more
appropriately normalized. We observe that the Multinomial is proportional to an independent Pois-
son model with the domain restricted to a ﬁxed length L. Thus  in a similar way  we propose a
Fixed-Length Poisson MRF (LPMRF) that is proportional to a PMRF but is restricted to a domain
with a ﬁxed vector length—i.e. where (cid:107)x(cid:107)1 = L. This distribution is quite different from previous
PMRF variants because the normalization is very different as will be described in later sections. For
a motivating example  in Fig. 1  we show the marginal distributions of the empirical distribution
and ﬁtted models using only three words from the Classic3 dataset that contains documents regard-
ing library sciences and aerospace engineering (See Sec. 4). Clearly  real-world text has positive
dependencies as evidenced by the empirical marginals of “boundary” and “layer” (i.e. referring
to the boundary layer in ﬂuid dynamics) and LPMRF does the best at ﬁtting this empirical dis-
tribution. In addition  the log partition function—and hence the likelihood—for LPMRF can be
approximated using sampling as described in later sections. Under the PMRF or TPMRF models 
both the log partition function and likelihood were computationally intractable to compute exactly.2
Thus  approximating the log partition function of an LPMRF opens up the door for likelihood-based
hyperparameter estimation and model evaluation that was not possible with PMRF.

(a) Empirical Marginal Distributions

(b) Multinomial × Poisson (Ind. Poissons)

(c) Truncated Poisson MRF

(d) Fixed-Length PMRF × Poisson

Figure 1: Marginal Distributions from Classic3 Dataset (Top Left) Empirical Distribution  (Top
Right) Estimated Multinomial × Poisson joint distribution—i.e.
independent Poissons  (Bottom
Left) Truncated Poisson MRF  (Bottom Right) Fixed-Length PMRF × Poisson joint distribution.
The simple empirical distribution clearly shows a strong dependency between “boundary” and
“layer” but strong negative dependency of “boundary” with “library”. Clearly  the word-independent
Multinomial-Poisson distribution underﬁts the data. While the Truncated PMRF can model depen-
dencies  it obviously has normalization problems because the normalization is dominated by the
edge case. The LPMRF-Poisson distribution much more appropriately ﬁts the empirical data.

In the topic modeling literature  many researchers have realized the issue with using the Multino-
mial distribution as the base distribution. For example  the interpretability of a Multinomial can be
difﬁcult since it only gives an ordering of words. Thus  multiple metrics have been proposed to eval-
uate topic models based on the perceived dependencies between words within a topic [7  8  9  10].
In particular  [11] showed that the Multinomial assumption was often violated in real world data.
In another paper [12]  the LDA topic assignments for each word are used to train a separate Ising
model—i.e. a Bernoulli MRF—for each topic in a heuristic two-stage procedure. Instead of model-
ing dependencies a posteriori  we formulate a generalization of topic models that allows the LPMRF
distribution to directly replace the Multinomial. This allows us to compute a topic model and word
dependencies jointly under a uniﬁed model as opposed to the two-stage heuristic procedure in [12].

2The example in Fig. 1 was computed by exhaustively computing the log partition function.

2

boundarylibrarylayerlibrarylayerboundaryEmpirical Distributionboundarylibrarylayerlibrarylayerboundarymodels.LPMRF(nnz=0 nnzwords=0)boundarylibrarylayerlibrarylayerboundarymodels.TPMRF(nnz=6 nnzwords=0)boundarylibrarylayerlibrarylayerboundarymodels.LPMRF(nnz=6 nnzwords=0)This model has some connection to the Admixture of Poisson MRFs model (APM) [2]  which was
the ﬁrst topic model to consider word dependencies. However  the LPMRF topic model directly
relaxes the LDA word-independence assumption (i.e.
the independent case is the same as LDA)
whereas APM is only an indirect relaxation of LDA because APM mixes in the exponential family
canonical parameter space while LDA mixes in the standard Multinomial parameter space. Another
difference with APM is that our proposed LPMRF topic model can actually produce topic assign-
ments for each word similar to LDA with Gibbs sampling [13]. Finally  the LPMRF topic model
does not fall into the same generalization of topic models as APM because the instance-speciﬁc
distribution is not an LPMRF—as described more fully in later sections. The follow up APM paper
[14] gives a fast algorithm for estimating the PMRF parameters. We use this algorithm as the basis
for estimating the topic LPMRF parameters. For estimating the topic vectors for each document  we
give a simple coordinate descent algorithm for estimation of the LPMRF topic model. This estima-
tion of topic vectors can be seen as a direct relaxation of LDA and could even provide a different
estimation algorithm for LDA.

2 Fixed-Length Poisson MRF
Notation Let p  n and k denote the number of words  documents and topics respectively. We will
generally use uppercase letters for matrices (e.g. Φ  X)  boldface lowercase letters or indices of
matrices for column vectors (i.e xi  θ  Φs) and lowercase letters for scalar values (i.e. xi  θs).
Poisson MRF Deﬁnition First  we will brieﬂy describe the Poisson MRF distribution and refer
the reader to [1  6] for more details. A PMRF can be parameterized by a node vector θ and an
edge matrix Φ whose non-zeros encode the direct dependencies between words: PrPMRF(x| θ  Φ) =

s=1 log(xs!)−A (θ  Φ)(cid:1)  where A (θ  Φ) is the log partition function needed

exp(cid:0)θT x+xT Φx−(cid:80)p

for normalization. Note that without loss of generality  we can assume Φ is symmetric because it
only shows up in the symmetric quadratic term. The conditional distribution of one word given all
the others—i.e. Pr(xs|x−s)—is a 1D Poisson distribution with natural parameter ηs = θs + xT−sΦs
by construction. One primary issue with the PMRF is that the log partition function (i.e. A (θ  Φ))
is a log-sum over all vectors in Zp
+ and thus with even one positive dependency  the log partition
function is inﬁnite because of the quadratic term in the formulation. Yang et al. [6] tried to address
this issue but  as illustrated in the introduction  their proposed modiﬁcations to the PMRF can yield
unusual models for real-world data.

Figure 2: LPMRF distribution for L = 10 (left) and L = 20 (right) with negative  zero and pos-
itive dependencies. The distribution of LPMRF can be quite different than a Multinomial (zero
dependency) and thus provides a much more ﬂexible parametric distribution for count data.

LPMRF Deﬁnition The Fixed-Length Poisson MRF (LPMRF) distribution is a simple yet fun-
damentally different distribution than the PMRF. Letting L ≡ (cid:107)x(cid:107)1 be the length of document  we
deﬁne the LPMRF distribution as follows:

(x|θ  Φ  L) = exp(θT x + xT Φx −(cid:80)

s log(xs!) − AL(θ  Φ))

Pr

LPMRF

(1)

(2)

AL(θ  Φ) = log

s log(xs!))

exp(θT x + xT Φx −(cid:80)

(cid:88)

x∈XL

XL = {x : x ∈ Zp

+ (cid:107)x(cid:107)1 = L}.

(3)
The only difference from the PMRF parametric form is the log partition function AL(θ  Φ) which is
conditioned on the set XL (unlike the unbounded set for PMRF). This domain restriction is critical to
formulating a tractable and reasonable distribution. Combined with a Poisson distribution on vector
length L = (cid:107)x(cid:107)1  the LPMRF distribution can be a much more suitable distribution for documents
than a Multinomial. The LPMRF distribution reduces to the standard Multinomial if there are no

3

01234567891000.10.20.30.4Domain of Binomial at L = 10LPMRF with Different Parameters L = 10Probability Negative DependencyIndependent (Binomial)Positive Dependency0246810121416182000.050.10.150.20.250.30.35Domain of Binomial at L = 20LPMRF with Different Parameters L = 20Probability Negative DependencyIndependent (Binomial)Positive Dependencydependencies. However  if there are dependencies  then the distribution can be quite different than
a Multinomial as illustrated in Fig. 2 for an LPMRF with p = 2 and L ﬁxed at either 10 or 20
words. After the original submission  we realized that for p = 2 the LPMRF model is the same
as the multiplicative binomial generalization in [15]. Thus  the LPMRF model can be seen as a
multinomial generalization (p ≥ 2) of the multiplicative binomial in [15].

LPMRF Parameter Estimation Because the parametric form of the LPMRF model is the same
as the form of the PMRF model and we primarily care about ﬁnding the correct dependencies  we
decide to use the PMRF estimation algorithm described in [14] to estimate θ and Φ. The algorithm
in [14] uses an approximation to the likelihood by using the pseudo-likelihood and performing (cid:96)1
regularized nodewise Poisson regressions. The (cid:96)1 regularization is important both for the sparsity of
the dependencies and the computational efﬁciency of the algorithm. While the PMRF and LPMRF
are different distributions  the pseudo-likelihood approximation for estimation provides good results
as shown in the results section. We present timing results to show the scalability of this algorithm in
Sec. 5. Other parameter estimation methods would be an interesting area of future work.

2.1 Likelihood and Log Partition Estimation

Unlike previous work on the PMRF or TPMRF distributions  we develop a tractable approximation
to the LPMRF log partition function (Eq. 2) so that we can compute approximate likelihood values.
The likelihood of a model can be fundamentally important for hyperparameter optimization and
model evaluation.

LPMRF Annealed Importance Sampling First  we develop an LPMRF Gibbs sampler by con-
sidering the most common form of Multinomial sampling  namely by taking the sum of a sequence
of L Categorical variables. From this intuition  we sample one word at a time while holding all other
words ﬁxed. The probability of one word in the sequence w(cid:96) given all the other words is propor-
tional to exp(θs + 2Φsx−(cid:96)) where x−(cid:96) is the sum of all other words. See the Appendix for the
details of Gibbs sampling. Then  we derive an annealed importance sampler [16] using the Gibbs
sampling by scaling the Φ matrix for each successive distribution by the linear sequence starting
with 0 and ending with 1 (i.e. γ = 0  . . .   1). Thus  we start with a simple Multinomial sample
from Pr(x| θ  0 · Φ  L) = PrMult(x| θ  L) and then Gibbs sample from each successive distribution
PrLPMRF(x| θ  γΦ  L) updating the sample weight as deﬁned in [16] until we reach the ﬁnal distri-
bution when γ = 1. From these weighted samples  we can compute an estimate of the log partition
function [16].

Llog((cid:80)
the full derivation. We simplify this upper bound by subtracting log((cid:80)

Upper Bound Using H¨older’s inequality  a simple convex relaxation and the partition function of a
Multinomial  an upper bound for the log partition function can be computed: AL(θ  Φ) ≤ L2λΦ 1 +
s exp θs) − log(L!)  where λΦ 1 is the maximum eigenvalue of Φ. See the Appendix for
s exp θs) from θ (which
does not change the distribution) so that the second term becomes 0. Then  neglecting the constant
term −log(L!) that does not interact with the parameters (θ  Φ)  the log partition function is upper
bounded by a simple quadratic function w.r.t. L.

Weighting Φ for Different L For datasets in which L is observed for every sample but is not
uniform—such as document collections  the log partition function will grow quadratically in L if
there are any positive dependencies as suggested by the upper bound. This causes long documents
to have extremely small likelihood. Thus  we must modify Φ as L gets larger to conteract this
effect. We propose a simple modiﬁcation that scales the Φ for each L: ˜ΦL = ω(L)Φ. In particular 
we propose to use the sigmoidal function using the Log Logistic cumulative distribution function
(CDF): ω(L) = 1 − LogLogisticCDF(L| αLL  βLL). We set the βLL parameter to 2 so that the tail is
O(1/L2) which will eventually cause the upper bound to approach a constant. Letting ¯L = 1
i Li
n
be the mean instance length  we choose αLL = c ¯L for some small constant c. This choice of αLL
helps the weighting function to appropriately scale for corpuses of different average lengths.

(cid:80)

Final Approximation Method for All L For our experiments  we approximate the log partition
function value for all L in the range of the corpus. We use 100 AIS samples for 50 different test
values of L linearly spaced between the 0.5 ¯L and 3 ¯L so that we cover both small and large values

4

upper bounds all 50 estimates: amax = maxL[ω(L)L2]−1( ˆAL(θ  Φ)−Llog((cid:80)

of L. This gives a total of 5 000 annealed importance samples. We use the quadratic form of the
upper bound Ua(L) = ω(L)L2a (ignoring constants with respect to Φ) and ﬁnd a constant a that
s exp θs)+log(L!)) 
where ˆAL is an AIS estimate of the log partition function for the 50 test values of L. This gives a
smooth approximation for all L that are greater than or equal to all individual estimates (ﬁgure of
example approximation in Appendix).

Mixtures of LPMRF With an approximation to the likelihood  we can easily formulate an esti-
mation algorithm for a mixture of LPMRFs using a simple alternating  EM-like procedure. First 
given the cluster assignments  the LPMRF parameters can be estimated as explained above. Then 
the best cluster assignments can be computed by assigning each instance to the highest likelihood
cluster. Extending the LPMRF to topic models requires more careful analysis as described next.

3 Generalizing Topic Models using Fixed-Length Distributions

In standard topic models like LDA  the distribution contains a unique topic variable for every word in
the corpus. Essentially  this means that every word is actually drawn from a categorical distribution.
However  this does not allow us to capture dependencies between words because there is only one
word being drawn at a time. Therefore  we need to reformulate LDA in a way that the words from a
topic are sampled jointly from a Multinomial. From this reformulation  we can then simply replace
the Multinomial with an LPMRF to obtain a topic model with LPMRF as the base distribution. Our
reformulation of LDA groups the topic indicator variables for each word into k vectors correspond-
ing to the k different topics. These k “topic indicator” vectors zl are then assumed to be drawn
from a Multinomial with ﬁxed length L = (cid:107)zj(cid:107). This grouping of topic vectors yields an equivalent
distribution because the topic indicators are exchangeable and independent of one another given the
observed word and the document-topic distribution. This leads to the following generalization of
topic models in which an observation xi is the summation of k hidden variables zj
i :

Generic Topic Model
wi ∼ SimplexPrior(α)
Li ∼ LengthDistribution( ¯L)
mi ∼ PartitionDistribution(wi  Li)
i ∼ FixedLengthDist(φj ; (cid:107)zj
zj

xi =(cid:80)k

j=1 zj

i

i (cid:107) = mj
i )

Novel LPMRF Topic Model

wi ∼ Dirichlet(α)
Li ∼ Poisson(λ = ¯L)
mi ∼ Multinomial(p = wi; N = Li)
i ∼ LPMRF(θj  Φj; L = mj
zj
i )

xi =(cid:80)k

j=1 zj
i .

from a LPMRF( ¯θi = (cid:80)

Note that this generalization of topic models does not require the partition distribution and the ﬁxed-
length distribution to be the same. In addition  other distributions could be substituted for the Dirich-
let prior distribution on document-topic distributions like the logistic normal prior. Finally  this gen-
eralization allows for real-valued topic models for other types of data although exploration of this is
outside the scope of this paper.
This generalization is distinctive from the topic model generalization termed “admixtures” in [2].
Admixtures assume that each observation is drawn from an instance-speciﬁc base distribution whose
parameters are a convex combination of previous parameters. Thus an admixture of LPMRFs could
be formulated by assuming that each document  given the document-topic weights wi  is drawn
j wijθj  ¯Φi = wijΦj; L = (cid:107)xi(cid:107)1). Though this may be an interesting
model in its own right and useful for further exploration in future work  this is not the same as
the above proposed model because the distribution of xi is not an LPMRF but rather a sum of
independent LPMRFs. One case—possibly the only case—where these two generalizations of topic
models intersect is when the distribution is a Multinomial (i.e. a LPMRF with Φ = 0). As another
distinction from APM  the LPMRF topic model directly generalizes LDA because the LPMRF in the
above model reduces to a Multinomial if Φ = 0. Fully exploring the differences between this topic
model generalization and the admixture generalization are quite interesting but outside the scope of
this paper.
With this formulation of LPMRF topic models  we can create a joint optimization problem to solve
for the topic matrix Zi = [z1
i ] for each document and to solve for the shared LPMRF

i   . . .   zk

i   z2

5

parameters θ1...k  Φ1...k. The optimization is based on minimizing the negative log posterior:

n(cid:88)

k(cid:88)

i=1

j=1

arg min

Z1...n θ1...k Φ1...k

− 1
n

i |θj  Φj  mj
(zj

Pr

LPMRF

s.t. Zie = xi  Zi ∈ Zk×p
+  

i ) − n(cid:88)

i=1

)) − k(cid:88)

j=1

log( Pr
prior

(m1...k

i

log( Pr
prior

(θj  Φj))

where e is the all ones vector. Notice that the observations xi only show up in the constraints.
The prior distribution on m1...k
can be related to the Dirichlet distribution as in LDA by taking
i|α). Also  notice that the documents are all independent if the
Prprior(m1...k
LPMRF parameters are known so this optimization can be trivially parallelized.

) = PrDir(mj

(cid:96) m(cid:96)

i

i /(cid:80)

i

Connection to Collapsed Gibbs Sampling This optimization is very similar to the collapsed
Gibbs sampling for LDA [13]. Essentially  the key part to estimating the topic models is estimating
the topic indicators for each word in the corpus. The model parameters can then be estimated directly
from these topic indicators. In the case of LDA  the Multinomial parameters are trivial to estimate
by merely keeping track of counts and thus the parameters can be updated in constant time for every
topic resampled. This also suggests that an interesting area of future work would be to understand the
connections between collapsed Gibbs sampling and this optimization problem. It may be possible
to use this optimization problem to speed up Gibbs sampling convergence or provide a MAP phase
after Gibbs sampling to get non-random estimates.

Estimating Topic Matrices Z1...n For LPMRF topic models  the estimation of the LPMRF pa-
rameters given the topic assignments requires solving another complex optimization problem. Thus 
we pursue an alternating EM-like scheme as in LPMRF mixtures. First  we estimate LPMRF param-
eters with the PMRF algorithm from [14]  and then we optimize the topic matrix Zi ∈ Rp×k for each
document. Because of the constraints on Zi  we pursue a simple dual coordinate descent procedure.
We select two coordinates in row r of Zi and determine if the optimization problem can be improved
by moving a words from topic (cid:96) to topic q. Thus  we only need to solve a series of simple univariate
problems. Each univariate problem only has xis number of possible solutions and thus if the max
count of words in a document is bounded by a constant  the univariate subproblems can be solved
q gives
a better optimization value than Zi. If we remove constant terms w.r.t. a  we arrive at the following
univariate optimization problem (suppressing dependence on i because each of the n subproblems
are independent):

efﬁciently. More formally  we are seeking a step size a such that (cid:98)Zi = Zi + aereT

(cid:96) − aereT

arg min
−z(cid:96)
r≤a≤zq
r

−a[θ(cid:96)

r + 2zT

(cid:96) Φ(cid:96)

r − 2zT

r − θq
+ Am(cid:96)+a(θ(cid:96)  Φ(cid:96)) + Amq+a(θq  Φq) − log( Pr

r] + [log((z(cid:96)

q Φq

r +a)!) + log((zq

r−a)!)]

( ˜m1...k)) 

prior

where ˜m is the new distribution of length based on the step size a. The ﬁrst term is the linear and
quadratic term from the sufﬁcient statistics. The second term is the change in base measure of a
word is moved. The third term is the difference in log partition function if the length of the topic
vectors changes. Note that the log partition function can be precomputed so it merely costs a table
lookup. The prior also only requires a simple calculation to update. Thus the main computation
comes in the inner product zT
r. However  this inner product can be maintained very efﬁciently
and updated efﬁciently so that it does not signiﬁcantly affect the running time.

(cid:96) Φ(cid:96)

4 Perplexity Experiments

We evaluated our novel LPMRF model using perplexity on a held-out test set of documents from
a corpus composed of research paper abstracts3 denoted Classic3 and a collection of Wikipedia
documents. The Classic3 dataset has three distinct topic areas: medical (Medline  1033)  library
information sciences (CISI  1460) and aerospace engineering (CRAN  1400).

3http://ir.dcs.gla.ac.uk/resources/test_collections/

6

Experimental Setup We train all the models using a 90% training split of the documents
and compute the held-out perplexity on the remaining 10% where perplexity is equal
to
exp(−L(Xtest|θ1...k  Φ1...k)/Ntest)  where L is the log likelihood and Ntest is the total number of
words in the test set. We evaluate single  mixture and topic models with both the Multinomial as
the base distribution and LPMRF as the base distribution at k = {1  3  10  20}. The topic indicator
matrices Zi for the test set are estimated by ﬁtting a MAP-based estimate while holding the topic
parameters θ1...k  Φ1...k ﬁxed.4 For a single Multinomial or LPMRF  we set the smoothing param-
eter β to 10−4. 5 We select the LPMRF models using all combinations of 20 log spaced λ between
1 and 10−3  and 5 linearly spaced weighting function constants c between 1 and 2 for the weighting
function described in Sec. 2.1. In order to compare our algorithms with LDA  we also provide per-
plexity results using an LDA Gibbs sampler [13] for MATLAB 6 to estimate the model parameters.
For LDA  we used 2000 iterations and optimized the hyperparameters α and β using the likelihood
of a tuning set. We do not seek to compare with many topic models because many of them use the
Multinomial as a base distribution which could be replaced by a LPMRF but rather we simply focus
on simple representative models.7

Results The perplexity results for all models can be seen in Fig. 3. Clearly  a single LPMRF sig-
niﬁcantly outperforms a single Multinomial on the test dataset both for the Classic3 and Wikipedia
datasets. The LPMRF model outperforms the simple Multinomial mixtures and topic models in all
cases. This suggests that the LPMRF model could be an interesting replacement for the Multino-
mial in more complex models. For a small number of topics  LPMRF topic models also outperforms
Gibbs sampling LDA but does not perform as well for larger number of topics. This is likely due to
the well-developed sampling methods for learning LDA. Exploring the possibility of incorporating
sampling into the ﬁtting of the LPMRF topic model is an excellent area of future work. We believe
LPMRF shows signiﬁcant promise for replacing the Multinomial in various probabilistic models.

Figure 3: (Left) The LPMRF models quite signiﬁcantly outperforms the Multinomial for both
datasets. (Right) The LPMRF model outperforms the simple Multinomial model in all cases. For a
small number of topics  LPMRF topic models also outperforms Gibbs sampling LDA but does not
perform as well for larger number of topics.

Qualitative Analysis of LPMRF Parameters
In addition to perplexity analysis  we present the
top words  top positive dependencies and the top negative dependencies for the LPMRF topic
model in Table 1. Notice that in LDA  only the top words are available for analysis but an
LPMRF topic model can produce intuitive dependencies. For example  the positive dependency
“language+natural” is composed of two words that often co-occur in the library sciences but each
word independently does not occur very often in comparison to “information” and “library”. The
positive dependency “stress+reaction” suggests that some of the documents in the Medline dataset
likely refer inducing stress on a subject and measuring the reaction. Or in the aerospace topic  the
positive dependency “non+linear” suggests that non-linear equations are important in aerospace.
Notice that these concepts could not be discovered with a standard Multinomial-based topic model.

4For topic models  the likelihood computation is intractable if averaging over all possible Zi. Thus  we
use a MAP simpliﬁcation primarily for computational reasons to compare models without computationally
expensive likelihood estimation.

5For the LPMRF  this merely means adding 10−4 to y-values of the nodewise Poisson regressions.
6http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm
7We could not compare to APM [2  14] because it is not computationally tractable to calculate the likelihood

of a test instance in APM  and thus we cannot compute perplexity.

7

47 34 Mult LPMRF Perplexity Classic3 9.3 7.9 Mult LPMRF Perplexity Wikipedia 05101520253035k=3k=10k=20k=3k=10k=20MixtureTopic ModelTest Set Perplexity for Classic3 MultLPMRFGibbs LDATable 1: Top Words and Dependencies for LPMRF Topic Model

5 Timing and Scalability
Finally  we explore the practical performance of our algorithms. In C++  we implemented the three
core algorithms: ﬁtting p Poisson regressions  ﬁtting the n topic matrices for each document  and
sampling 5 000 AIS samples. The timing for each of these components respectively can be seen in
Fig. 4 for the Wikipedia dataset. We set λ = 1 in the ﬁrst two experiments which yields roughly
20 000 non-zeros and varied λ for the third experiment. Each of the components is trivially par-
allelized using OpenMP (http://openmp.org/). All timing experiments were conducted on
the TACC Maverick system with Intel Xeon E5-2680 v2 Ivy Bridge CPUs (2.80 GHz)  20 CPUs
per node  and 12.8 GB memory per CPU (https://www.tacc.utexas.edu/). The scaling
is generally linear in the parameters except for ﬁtting topic matrices which is O(k2). For the AIS
sampling  the scaling is linear in the number of non-zeros in Φ irrespective of p. Overall  we believe
our implementations provide both good scaling and practical performance (code available online).

Figure 4: (Left) The timing for ﬁtting p Poisson regressions shows an empirical scaling of O(np).
(Middle) The timing for ﬁtting topic matrices empirically shows scaling that is O(npk2). (Right)
The timing for AIS sampling shows that the sampling is approximately linearly scaled with the
number of non-zeros in Φ irrespective of p.
6 Conclusion
We motivated the need for a more ﬂexible distribution than the Multinomial such as the Poisson
MRF. However  the PMRF distribution has several complications due to its normalization that hin-
der it from being a general-purpose model for count data. We overcome these difﬁculties by re-
stricting the domain to a ﬁxed length as in a Multinomial while retaining the parametric form of
the Poisson MRF. By parameterizing by the length of the document  we can then efﬁciently com-
pute sampling-based estimates of the log partition function and hence the likelihood—which were
not tractable to compute under the PMRF model. We extend the LPMRF distribution to both mix-
tures and topic models by generalizing topic models using ﬁxed-length distributions and develop
parameter estimation methods using dual coordinate descent. We evaluate the perplexity of the pro-
posed LPMRF models on datasets and show that they offer good performance when compared to
Multinomial-based models. Finally  we show that our algorithms are fast and have good scaling.
Potential new areas could be explored such as the relation between the topic matrix optimization
method and Gibbs sampling. It may be possible to develop sampling-based methods for the LPMRF
topic model similar to Gibbs sampling for LDA. In general  we suggest that the LPMRF model could
open up new avenues of research where the Multinomial distribution is currently used.

Acknowledgments

This work was supported by NSF (DGE-1110007  IIS-1149803  IIS-1447574  DMS-1264033  CCF-
1320746) and ARO (W911NF-12-1-0390).

8

Top wordsTop Pos. EdgesTop Neg. EdgesTop wordsTop Pos. EdgesTop Neg. EdgesTop wordsTop Pos. EdgesTop Neg. Edgesinformationstates+unitedpaper-bookpatientsterm+longcells-patientflowsupported+simplyflow-shellslibrarypoint+viewlibraries-retrievalcasespositive+negativepatients-animalspressureaccount+takennumber-numbersresearchtest+testslibrary-chemicalnormalcooling+hypothermipatients-ratsboundaryagreement+goodflow-shellsystemprimary+secondarylibraries-languagecellssystem+centralhormone-proteinresultsmoment+pitchingwing-hypersoniclibrariesrecall+precisionsystem-publishedtreatmentatmosphere+heightgrowth-parathyroidtheorynon+linearsolutions-turbulentbookdissemination+sdiinformation-citationschildrenfunction+functionspatients-lensmethodlower+uppermach-reynoldssystemsdirect+accessinformation-citationfoundmethods+suitablepatients-micelayertunnel+windflow-stressesdatalanguage+naturalchemical-documentresultsstress+reactionpatients-dogsgiventime+dependenttheoretical-draguseyears+fivelibrary-scientistsbloodlow+rateshormone-tumornumberlevel+noisegeneral-bucklingscientificterm+longlibrary-scientificdiseasecase+reportpatients-childpresentedpurpose+notemade-conductedTopic 1Topic 2Topic 31 10 100 1000 10000 1000 10000 Time in Seconds (log scale) Number of Unique Words (p) (log scale) Timing for Poisson Regressions n=100k n=50k n=10k n=5k 0 200 400 600 800 1000 k=3 k=10 k=3 k=10 p=5000 p=10000 Time (s) Timing for Fitting Topic Matrices n=50000 n=100000 0 100 200 300 400 500 600 700 0 100000 200000 300000 400000 500000 Time (s) Number of Nonzeros in Edge Matrix Timing for AIS Sampling p=10000 p=5000 p=1000 References
[1] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu  “Graphical models via generalized linear models ” in

NIPS  pp. 1367–1375  2012.

[2] D. I. Inouye  P. Ravikumar  and I. S. Dhillon  “Admixture of Poisson MRFs: A Topic Model with Word

Dependencies ” in International Conference on Machine Learning (ICML)  pp. 683–691  2014.

[3] T. Hofmann  “Probabilistic latent semantic analysis ” in Uncertainty in Artiﬁcial Intelligence (UAI) 

pp. 289–296  Morgan Kaufmann Publishers Inc.  1999.

[4] D. Blei  A. Ng  and M. Jordan  “Latent dirichlet allocation ” JMLR  vol. 3  pp. 993–1022  2003.
[5] D. Blei  “Probabilistic topic models ” Communications of the ACM  vol. 55  pp. 77–84  Nov. 2012.
[6] E. Yang  P. Ravikumar  G. Allen  and Z. Liu.  “On poisson graphical models ” in NIPS  pp. 1718–1726 

2013.

[7] J. Chang  J. Boyd-Graber  S. Gerrish  C. Wang  and D. Blei  “Reading tea leaves: How humans interpret

topic models ” in NIPS  2009.

[8] D. Mimno  H. M. Wallach  E. Talley  M. Leenders  and A. McCallum  “Optimizing semantic coherence

in topic models ” in EMNLP  pp. 262–272  2011.

[9] D. Newman  Y. Noh  E. Talley  S. Karimi  and T. Baldwin  “Evaluating topic models for digital libraries ”

in ACM/IEEE Joint Conference on Digital Libraries (JCDL)  pp. 215–224  2010.

[10] N. Aletras and R. Court  “Evaluating Topic Coherence Using Distributional Semantics ” in International

Conference on Computational Semantics (IWCS 2013) - Long Papers  pp. 13–22  2013.

[11] D. Mimno and D. Blei  “Bayesian Checking for Topic Models ” in EMNLP  pp. 227–237  2011.
[12] R. Nallapati  A. Ahmed  W. Cohen  and E. Xing  “Sparse word graphs: A scalable algorithm for capturing

word correlations in topic models ” in ICDM  pp. 343–348  2007.

[13] M. Steyvers and T. Grifﬁths  “Probabilistic topic models ” in Latent Semantic Analysis: A Road to Mean-

ing  pp. 424–440  2007.

[14] D. I. Inouye  P. K. Ravikumar  and I. S. Dhillon  “Capturing Semantically Meaningful Word Dependencies

with an Admixture of Poisson MRFs ” in NIPS  pp. 3158–3166  2014.

[15] P. M. E. Altham  “Two Generalizations of the Binomial Distribution ” Journal of the Royal Statistical

Society. Series C (Applied Statistics)  vol. 27  no. 2  pp. 162–167  1978.

[16] R. M. Neal  “Annealed importance sampling ” Statistics and Computing  vol. 11  no. 2  pp. 125–139 

2001.

9

,David Inouye
Pradeep Ravikumar
Inderjit Dhillon
Anastasia Pentina
Ruth Urner
Gauthier Gidel
Francis Bach
Simon Lacoste-Julien