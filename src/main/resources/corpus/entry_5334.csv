2014,Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets,To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing  robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems  this becomes a daunting task  as the solution space (image labelings  sentence parses  etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically  we show via examples that when marginal gains of submodular diversity functions allow structured representations  this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits  tradeoffs  and show that our constructions lead to significantly better proposals.,Submodular meets Structured: Finding Diverse

Subsets in Exponentially-Large Structured Item Sets

Adarsh Prasad

UT Austin

adarsh@cs.utexas.edu

Stefanie Jegelka

UC Berkeley

stefje@eecs.berkeley.edu

Dhruv Batra
Virginia Tech

dbatra@vt.edu

Abstract

To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing  robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In structured
prediction problems  this becomes a daunting task  as the solution space (image
labelings  sentence parses  etc.) is exponentially large. We study greedy algo-
rithms for ﬁnding a diverse subset of solutions in structured-output spaces by
drawing new connections between submodular functions over combinatorial item
sets and High-Order Potentials (HOPs) studied for graphical models. Speciﬁcally 
we show via examples that when marginal gains of submodular diversity functions
allow structured representations  this enables efﬁcient (sub-linear time) approxi-
mate maximization by reducing the greedy augmentation step to inference in a
factor graph with appropriately constructed HOPs. We discuss beneﬁts  trade-
offs  and show that our constructions lead to signiﬁcantly better proposals.

Introduction

1
Many problems in Computer Vision  Natural Language Processing and Computational Biology in-
volve mappings from an input space X to an exponentially large space Y of structured outputs.
For instance  Y may be the space of all segmentations of an image with n pixels  each of which
may take L labels  so |Y| = Ln. Formulations such as Conditional Random Fields (CRFs) [24] 
Max-Margin Markov Networks (M3N) [31]  and Structured Support Vector Machines (SSVMs) [32]
have successfully provided principled ways of scoring all solutions y ∈ Y and predicting the single
highest scoring or maximum a posteriori (MAP) conﬁguration  by exploiting the factorization of a
structured output into its constituent “parts”.
In a number of scenarios  the posterior P(y|x) has several modes due to ambiguities  and we seek
not only a single best prediction but a set of good predictions:
(1) Interactive Machine Learning. Systems like Google Translate (for machine translation) or
Photoshop (for interactive image segmentation) solve structured prediction problems that are often
ambiguous ("what did the user really mean?"). Generating a small set of relevant candidate solutions
for the user to select from can greatly improve the results.
(2) M-Best hypotheses in cascades. Machine learning algorithms are often cascaded  with the
output of one model being fed into another [33]. Hence  at the initial stages it is not necessary to
make a single perfect prediction. We rather seek a set of plausible predictions that are subsequently
re-ranked  combined or processed by a more sophisticated mechanism.
In both scenarios  we ideally want a small set of M plausible (i.e.  high scoring) but non-redundant
(i.e.  diverse) structured-outputs to hedge our bets.
Submodular Maximization and Diversity. The task of searching for a diverse high-quality sub-
set of items from a ground set V has been well-studied in information retrieval [5]  sensor place-
ment [22]  document summarization [26]  viral marketing [17]  and robotics [10]. Across these
domains  submodularity has emerged as an a fundamental and practical concept – a property of
functions for measuring diversity of a subset of items. Speciﬁcally  a set function F : 2V → R is
submodular if its marginal gains  F (a|S) ≡ F (S∪a)−F (S) are decreasing  i.e. F (a|S) ≥ F (a|T )

1

(a) Image

(b) All segmentations: |V | = Ln

(c) Structured Representation.

Figure 1: (a) input image; (b) space of all possible object segmentations / labelings (each item is a segmenta-
tion); (c) we convert the problem of ﬁnding the item with the highest marginal gain F (a|S) to a MAP inference
problem in a factor graph over base variables y with an appropriately deﬁned HOP.
for all S ⊆ T and a /∈ T . In addition  if F is monotone  i.e.  F (S) ≤ F (T )  ∀S ⊆ T   then a simple
greedy algorithm (that in each iteration t adds to the current set St the item with the largest marginal
gain F (a|St)) achieves an approximation factor of (1 − 1
e ) [27]. This result has had signiﬁcant
practical impact [21]. Unfortunately  if the number of items |V | is exponentially large  then even a
single linear scan for greedy augmentation is infeasible.
In this work  we study conditions under which it is feasible to greedily maximize a submodular
function over an exponentially large ground set V = {v1  . . .   vN} whose elements are combinato-
rial objects  i.e.  labelings of a base set of n variables y = {y1  y2  . . .   yn}. For instance  in image
segmentation  the base variables yi are pixel labels  and each item a ∈ V is a particular labeling of
the pixels. Or  if each base variable ye indicates the presence or absence of an edge e in a graph 
then each item may represent a spanning tree or a maximal matching. Our goal is to ﬁnd a set of
M plausible and diverse conﬁgurations efﬁciently  i.e. in time sub-linear in |V | (ideally scaling as
a low-order polynomial in log |V |). We will assume F (·) to be monotone submodular  nonnegative
and normalized (F (∅) = 0)  and base our study on the greedy algorithm. As a running example  we
focus on pixel labeling  where each base variable takes values in a set [L] = {1  . . .   L} of labels.
Contributions. Our principal contribution is a conceptual one. We observe that marginal gains of a
number of submodular functions allow structured representations  and this enables efﬁcient greedy
maximization over exponentially large ground sets – by reducing the greedy augmentation step to
a MAP inference query in a discrete factor graph augmented with a suitably constructed High-
Order Potential (HOP). Thus  our work draws new connections between two seemingly disparate
but highly related areas in machine learning – submodular maximization and inference in graphical
models with structured HOPs. As speciﬁc examples  we construct submodular functions for three
different  task-dependent deﬁnitions of diversity  and provide reductions to three different HOPs for
which efﬁcient inference techniques have already been developed. Moreover  we present a generic
recipe for constructing such submodular functions  which may be “plugged” with efﬁcient HOPs
discovered in future work. Our empirical contribution is an efﬁcient algorithm for producing a set of
image segmentations with signiﬁcantly higher oracle accuracy1 than previous works. The algorithm
is general enough to transfer to other applications. Fig. 1 shows an overview of our approach.
Related work: generating multiple solutions. Determinental Point Processesare an elegant prob-
abilistic model over sets of items with a preference for diversity. Its generalization to a structured
setting [23] assumes a tree-structured model  an assumption that we do not make. Guzman-Rivera et
al. [14  15] learn a set of M models  each producing one solution  to form the set of solutions. Their
approach requires access to the learning sub-routine and repeated re-training of the models  which
is not always possible  as it may be expensive or proprietary. We assume to be given a single (pre-
trained) model from which we must generate multiple diverse  good solutions. Perhaps the closest
to our setting are recent techniques for ﬁnding diverse M-best solutions [2  28] or modes [7  8]
in graphical models. While [7] and [8] are inapplicable since they are restricted to chain and tree
graphs  we compare to other baselines in Section 3.2 and 4.
1.1 Preliminaries and Notation
We select from a ground set V of N items. Each item is a labeling y = {y1  y2  . . .   yn}
of n base variables. For clarity  we use non-bold letters a ∈ V for items  and boldface let-
ters y for base set conﬁgurations. Uppercase letters refer to functions over the ground set items
F (a|A)  R(a|A)  D(a|A)  and lowercase letters to functions over base variables f (y)  r(y)  d(y).

1The accuracy of the most accurate segmentation in the set.

2

+ argmaxa∈VF(a|S)≡r(y)d(y|S)F (S) = R(S) + λD(S) 

Formally  there is a bijection φ : V (cid:55)→ [L]m that maps items a ∈ V to their representation as base
variable labelings y = φ(a). For notational simplicity  we often use y ∈ S to mean φ−1(y) ∈ S 
i.e. the item corresponding to the labeling y is present in the set S ⊆ V . We write (cid:96) ∈ y if the label
(cid:96) is used in y  i.e. ∃j s.t. yj = (cid:96). For a set c ⊆ [n]  we use yc to denote the tuple {yi | i ∈ c}.
Our goal to ﬁnd an ordered set or list of items S ⊆ V that maximizes a scoring function F . Lists
generalize the notation of sets  and allow for reasoning of item order and repetitions. More details
about list vs set prediction can be found in [29  10].
Scoring Function. We trade off the relevance and diversity of list S ⊆ V via a scoring function
F : 2V → R of the form
where R(S) =(cid:80)
(1)
a∈S R(a) is a modular nonnegative relevance function that aggregates the quality
of all items in the list; D(S) is a monotone normalized submodular function that measure the diver-
sity of items in S; and λ ≥ 0 is a trade-off parameter. Similar objective functions were used e.g. in
[26]. They are reminiscent of the general paradigm in machine learning of combining a loss func-
tion that measures quality (e.g. training error) and a regularization term that encourages desirable
properties (e.g. smoothness  sparsity  or “diversity”).
Submodular Maximization. We aim to ﬁnd a list S that maximizes F (S) subject to a cardinality
constraint |S| ≤ M. For monotone submodular F   this may be done via a greedy algorithm that
starts out with S0 = ∅  and iteratively adds the next best item:
(2)
The ﬁnal solution SM is within a factor of (1 − 1
e ) of the optimal solution S∗: F (SM ) ≥ (1 −
e )F (S∗) [27]. The computational bottleneck is that in each iteration  we must ﬁnd the item with
the largest marginal gain. Clearly  if |V | has exponential size  we cannot touch each item even once.
Instead  we propose “augmentation sub-routines” that exploit the structure of V and maximize the
marginal gain by solving an optimization problem over the base variables.
2 Marginal Gains in Conﬁguration Space
To solve the greedy augmentation step via optimization over y  we transfer the marginal gain from
the world of items to the world of base variables and derive functions on y from F :

at ∈ argmaxa∈V F (a | St−1).

St = St−1 ∪ at 

1

.

(3)

is then given by R(a) = r(y) = (cid:80)
this quality becomes r(y) =(cid:80)
r(y) = (cid:80)

p∈V

(cid:124)

Maximizing F (a|S) now means maximizing f (y|S) for y = φ(a). This can be a hard combinatorial
optimization problem in general. However  as we will see  there is a broad class of useful functions
F for which f inherits exploitable structure  and argmaxy f (y|S) can be solved efﬁciently  exactly
or at least approximately.
Relevance Function. We use a structured relevance function R(a) that is the score of a factor graph
deﬁned over the base variables y. Let G = (V E) be a graph deﬁned over {y1  y2  . . .   yn}  i.e.
V = [n]  E ⊆
be the log-potential functions (or factors) for these cliques. The quality of an item a = φ−1(y)
C∈C θC(yC). For instance  with only node and edge factors 
θpq(yp  yq). In this model  ﬁnding the single

(cid:1). Let C = {C | C ⊆ V} be a set of cliques in the graph  and let θC : [L]|C| (cid:55)→ R

θp(yp) +(cid:80)

(cid:0)V2

C∈C θC(yC) = w

highest quality item corresponds to maximum a posteriori (MAP) inference in the factor graph.
Although we refer to terms with probabilistic interpretations such as “MAP”  we treat our relevance
function as output of an energy-based model [25] such as a Structured SVM [32]. For instance 
ψ(y) for parameters w and feature vector ψ(y). Moreover  we as-
sume that the relevance function r(y) is nonnegative2. This assumption ensures that F (·) is mono-
tone. If F is non-monotone  algorithms other than the greedy are needed [4  12]. We leave this
generalization for future work. In most application domains the relevance function is learned from
data and thus our positivity assumption is not restrictive – one can simply learn a positive relevance
function. For instance  in SSVMs  the relevance weights are learnt to maximize the margin between
the correct labeling and all incorrect ones. We show in the supplement that SSVM parameters that
assign nonnegative scores to all labelings achieve exactly the same hinge loss (and thus the same
generalization error) as without the nonnegativity constraint.

(p q)∈E

2Strictly speaking  this condition is sufﬁcient but not necessary. We only need nonnegative marginal gains.

3

(cid:125)
(cid:124)
F (φ−1(y) | S)

(cid:123)(cid:122)

f (y|S)

= R(φ−1(y))

(cid:124)

(cid:123)(cid:122)

r(y)

(cid:125)

(cid:124)
(cid:125)
+λ D(φ−1(y) | S)

(cid:123)(cid:122)

d(y|S)

Figure 2: Diversity via groups:
(a) groups deﬁned by the pres-
ence of labels (i.e. #groups
= L); (b) groups deﬁned by
Hamming balls around each
item/labeling (i.e. #groups =
Ln). In each case  diversity is
measured by how many groups
are covered by a new item. See
text for details.

(a) Label Groups

(b) Hamming Ball Groups

D(S) =

Our scheme relies on constructing groups Gi that cover the ground set  i.e. V = (cid:83)

3 Structured Diversity Functions
We next discuss a general recipe for constructing monotone submodular diversity functions D(S) 
and for reducing their marginal gains to structured representations over the base variables d(y|S).
i Gi. These
groups will be deﬁned by task-dependent characteristics – for instance  in image segmentation  G(cid:96)
can be the set of all segmentations that contain label (cid:96). The groups can be overlapping. For instance 
if a segmentation y contains pixels labeled “grass” and “cow”  then y ∈ Ggrass and y ∈ Gcow.
Group Coverage: Count Diversity. Given V and a set of groups {Gi}  we measure the diversity
of a list S in terms of its group coverage  i.e.  the number of groups covered jointly by items in S:
(4)
where we deﬁne Gi ∩ S as the intersection of Gi with the set of unique items in S. It is easy to show
that this function is monotone submodular. If G(cid:96) is the group of all segmentations that contain label
(cid:96)  then the diversity measure of a list of segmentations S is the number of object labels that appear
in any a ∈ S. The marginal gain is the number of new groups covered by a:

(cid:12)(cid:12)(cid:12)(cid:8)i | Gi ∩ S (cid:54)= ∅
(cid:9)(cid:12)(cid:12)(cid:12) 
(cid:12)(cid:12)(cid:12)(cid:8)i | a ∈ Gi and S ∩ Gi = ∅
(cid:9)(cid:12)(cid:12)(cid:12).
(cid:88)
h(cid:0)(cid:12)(cid:12)Gi ∩ S(cid:12)(cid:12)(cid:1).
(cid:20)
h(cid:0)1 +(cid:12)(cid:12)Gi ∩ S(cid:12)(cid:12)(cid:1)
Since h is concave  the gain h(cid:0)1 +(cid:12)(cid:12)Gi ∩ S(cid:12)(cid:12)(cid:1)
− h(cid:0)(cid:12)(cid:12)Gi ∩ S(cid:12)(cid:12)(cid:1) decreases as S becomes larger. Thus 

(6)
where h is any nonnegative nondecreasing concave scalar function. This is a sum of submodular
functions and hence submodular. Eqn. (4) is a special case of Eqn. (6) with h(y) = min{1  y}.
Other possibilities are √·  or log(1 + ·). For this general deﬁnition of diversity  the marginal gain is

D(a | S) =

(5)
Thus  the greedy algorithm will try to ﬁnd an item/segmentation that belongs to as many as yet
unused groups as possible.
Group Coverage: General Diversity. More generally  instead of simply counting the number of
groups covered by S  we can use a more reﬁned decay

− h(cid:0)(cid:12)(cid:12)Gi ∩ S(cid:12)(cid:12)(cid:1)(cid:21)

.

the marginal gain of an item a is proportional to how rare each group Gi (cid:51) a is in the list S.
In each step of the greedy algorithm  we maximize r(y) + λd(y|S). We already established a
structured representation of r(y) via a factor graph on y. In the next few subsections  we specify
three example deﬁnitions of groups Gi that instantiate three diversity functions D(S). For each
D(S)  we show how the marginal gains D(a|S) can be expressed as a speciﬁc High-Order Potential
(HOP) d(y|S) in the factor graph over y. These HOPs are known to be efﬁciently optimizable  and
hence we can solve the augmentation step efﬁciently. Table 1 summarizes these connections.
Diversity and Parsimony. If the groups Gi are overlapping  some y can belong to many groups
simultaneously. While such a y may offer an immediate large gain in diversity  in many applications
it is more natural to seek a small list of complementary labelings rather than having all labels occur
in the same y. For instance  in image segmentation with groups deﬁned by label presence (Sec. 3.1) 
natural scenes are unlikely to contain many labels at the same time.
Instead  the labels should
be spread across the selected labelings y ∈ S. Hence  we include a parsimony factor p(y) that
biases towards simpler labelings y. This term is a modular function and does not affect the diversity
functions directly. We next outline some example instantiations of the functions (4) and (6).

D(S) =

i

(cid:88)

D(a | S) =

i:Gi(cid:51)a

(7)

4

Section 3.1
Supplement
Section 3.2

Groups (Gi)
Labels
Label Transitions
Hamming Balls

Higher Order Potentials
Label Cost
Co-operative Cuts
Cardinality Potentials

Table 1: Different diversity functions and corresponding HOPs.

.

=

(cid:96)∈y

1.

(cid:96):G(cid:96)(cid:51)y

(cid:88)

(cid:88)

3.1 Diversity of Labels
For the ﬁrst example  let G(cid:96) be the set of all labelings y containing the label (cid:96)  i.e. y ∈ G(cid:96) if and only
if yj = (cid:96) for some j ∈ [n]. Such a diversity function arises in multi-class image segmentation – if the
highest scoring segmentation contains “sky” and “grass”  then we would like to add complementary
segmentations that contain an unused class label  say “sheep” or “cow”.
Structured Representation of Marginal Gains. The marginal gain for this diversity function turns
out to be a HOP called label cost [9]. It penalizes each label that occurs in a previous segmentation.
Let lcountS((cid:96)) be the number of segmentations in S that contain label (cid:96). In the simplest case of
coverage diversity (4)  the marginal gain provides a constant reward for every as yet unseen label (cid:96):
(8)

(cid:9)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:8)(cid:96) | y ∈ G(cid:96)  S ∩ G(cid:96) = ∅
d(y | S) =
h(cid:0)1 + lcountS((cid:96))(cid:1) − h(cid:0)lcountS((cid:96))(cid:1)(cid:21)
(cid:20)
(cid:20)
h(cid:0)1 +(cid:12)(cid:12)G(cid:96) ∩ S(cid:12)(cid:12)(cid:1) − h(cid:0)(cid:12)(cid:12)G(cid:96) ∩ S(cid:12)(cid:12)(cid:1)(cid:21)
(cid:88)

For the general group coverage diversity (6)  the gain becomes
d(y|S) =

(cid:96)∈y lcountS ((cid:96))=0

the segmentations already chosen in S. The parsimony factor in this setting is p(y) =(cid:80)

Thus  d(y|S) rewards the presence of a label (cid:96) in y by an amount proportional to how rare (cid:96) is in
(cid:96)∈y c((cid:96)).
In the simplest case  c((cid:96)) = −1  i.e. we are charged a constant for every label used in y.
With this type of diversity (and parsimony terms)  the greedy augmentation step is equivalent to
performing MAP inference in a factor graph augmented with label reward HOPs: argmaxy r(y) +
λ(d(y | S) + p(y)). Delong et al. [9] show how to perform approximate MAP inference with such
label costs via an extension to the standard α-expansion [3] algorithm.
Label Transitions. Label Diversity can be extended to reward not just the presence of previously
unseen labels  but also the presence of previously unseen label transitions (e.g.  a person in front
of a car or a person in front of a house). Formally  we deﬁne one group G(cid:96) (cid:96)(cid:48) per label pair (cid:96)  (cid:96)(cid:48) 
and y ∈ G(cid:96) (cid:96)(cid:48) if it contains two adjacent variables yi  yj with labels yi = (cid:96)  yj = (cid:96)(cid:48). This diversity
function rewards the presence of a label pair ((cid:96)  (cid:96)(cid:48)) by an amount proportional to how rare this pair
is in the segmentations that are part of S. For such functions  the marginal gain d(y|S) becomes
a HOP called cooperative cuts [16]. The inference algorithm in [19] gives a fully polynomial-time
approximation scheme for any nondecreasing  nonnegative h  and the exact gain maximizer for the
count function h(y) = min{1  y}. Further details may be found in the supplement.
3.2 Diversity via Hamming Balls
The label diversity function simply rewarded the presence of a label (cid:96)  irrespective of which or how
many variables yi were assigned that label. The next diversity function rewards a large Hamming dis-
i ]] between conﬁgurations (where [[·]] is the Iverson bracket.)
Let Bk(y) denote the k-radius Hamming ball centered at y  i.e. B(y) = {y(cid:48) | Ham(y(cid:48)  y) ≤ k}.
The previous section constructed one group per label (cid:96). Now  we construct one group Gy for each
conﬁguration y  which is the k-radius Hamming ball centered at y  i.e. Gy = Bk(y).
Structured Representation of Marginal Gains. For this diversity  the marginal gain d(y|S) be-
comes a HOP called cardinality potential [30]. For count group coverage  this becomes
(9a)

tance Ham(y1  y2) =(cid:80)n

i (cid:54)= y2

i=1[[y1

(cid:12)(cid:12)(cid:12)(cid:8)y(cid:48) | Gy(cid:48) ∩ (S ∪ y) (cid:54)= ∅
(cid:9)(cid:12)(cid:12)(cid:12) −
(cid:12)(cid:12)(cid:12) (cid:91)
(cid:12)(cid:12)(cid:12) (cid:91)

(cid:12)(cid:12)(cid:12) −

Bk(y(cid:48))

Bk(y(cid:48))

y(cid:48)∈S∪y

y(cid:48)∈S

d(y|S) =
=

i.e.  the marginal gain of adding y is the number of new conﬁgurations y(cid:48) covered by the Hamming
ball centered at y. Since the size of the intersection of Bk(y) with a union of Hamming balls does
not have a straightforward structured representation  we maximize a lower bound on d(y|S) instead:
(10)

d(y | S) ≥ dlb(y | S) ≡

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12) 

Bk(y(cid:48))

(9b)

(cid:104) (cid:91)

(cid:12)(cid:12)(cid:12)(cid:8)y(cid:48) | Gy(cid:48) ∩ S (cid:54)= ∅
(cid:9)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)Bk(y) ∩
(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)Bk(y)
(cid:12)(cid:12)(cid:12) −
(cid:88)
(cid:12)(cid:12)Bk(y) ∩ Bk(y(cid:48))(cid:12)(cid:12)
(cid:12)(cid:12)Bk(y)(cid:12)(cid:12) −

y(cid:48)∈S

y(cid:48)∈S

5

|S| − Iy(cid:48)(y) that saturates as y moves far away from y(cid:48).

involves maximizing a diversity-augmented score: argmaxy r(y)+λ(cid:80)
diversity function grows linearly with the Hamming distance  θy(cid:48)(y) = Ham(y(cid:48)  y) =(cid:80)n

This lower bound dlb(y|S) overcounts the intersection in Eqn. (9b) by summing the intersections
with each Bk(y(cid:48)) separately. We can also interpret this lower bound as clipping the series arising
from the inclusion-exclusion principle to the ﬁrst-order terms. Importantly  (10) depends on y only
via its Hamming distance to y(cid:48). This is a cardinality potential that depends only on the number of
(cid:80)
variables yi assigned to a particular label. Speciﬁcally  ignoring constant terms  the lower bound can
be written as a summation of cardinality factors (one for each previous solution y(cid:48) ∈ S): dlb(y|S) =
y(cid:48)∈S θy(cid:48)(y)  where θy(cid:48)(y) = b
|S| − Iy(cid:48)(y)  b is a constant (size of a k-radius Hamming ball)  and
Iy(cid:48)(y) is the number of points in the intersection of k-radius Hamming balls centered at y(cid:48) and y.
With this approximation  the greedy step means performing MAP inference in a factor graph aug-
mented with cardinality potentials: argmaxy r(y) + λdlb(y|S). This may be solved via message-
passing  and all outgoing messages from cardinality factors can be computed in O(n log n) time
[30]. While this algorithm does not offer any approximation guarantees  it performs well in prac-
tice. A subtle point to note is that dlb(y|S) is always decreasing w.r.t. |S| but may become negative
due to over-counting. We can ﬁx this by clamping dlb(y|S) to be greater than 0  but in our experi-
ments this was unnecessary – the greedy algorithm never chose a set where dlb(y|S) was negative.
Comparison to DivMBest. The greedy algorithm for Hamming diversity is similar in spirit to the
recent work of Batra et al. [2]  who also proposed a greedy algorithm (DivMBest) for ﬁnding diverse
MAP solutions in graphical models. They did not provide any justiﬁcation for greedy  and our
formulation sheds some light on their work. Similar to our approach  at each greedy step  DivMBest
y(cid:48)∈S θy(cid:48)(y). However  their
i=1[[y(cid:48)i (cid:54)=
yi]]. Linear diversity rewards are not robust  and tend to over-reward diversity. Our formulation uses
a robust diversity function θy(cid:48)(y) = b
In our experiments  we make the saturation behavior smoothly tunable via a parameter γ: Iy(cid:48)(y) =
e−γ Ham(y(cid:48)
 y). A larger γ corresponds to Hamming balls of smaller radius  and can be set to optimize
performance on validation data. We found this to work better than directly tuning the radius k.
4 Experiments
We apply our greedy maximization algorithms to two image segmentation problems: (1) interactive
binary segmentation (object cutout) (Section 4.1); (2) category-level object segmentation on the
PASCAL VOC 2012 dataset [11] (Section 4.2). We compare all methods by their respective oracle
accuracies  i.e. the accuracy of the most accurate segmentation in the set of M diverse segmentations
returned by that method. For a small value of M ≈ 5 to 10  a high oracle accuracy indicates that
the algorithm has achieved high recall and has identiﬁed a good pool of candidate solutions for
further processing in a cascaded pipeline. In both experiments  the label “background” is typically
expected to appear somewhere in the image  and thus does not play a role in the label cost/transition
diversity functions. Furthermore  in binary segmentation there is only one non-background label.
Thus  we report results with Hamming diversity only (label cost and label transition diversities are
not applicable). For the multi-class segmentation experiments  we report experiments with all three.
Baselines. We compare our proposed methods against DivMBest [2]  which greedily produces
diverse segmentation by explicitly adding a linear Hamming distance term to the factor graph. Each
Hamming term is decomposable along the variables yi and simply modiﬁes the node potentials
y(cid:48)∈S[[yi (cid:54)= y(cid:48)i]]. DivMBest has been shown to outperform techniques such as M-
Best-MAP [34  1]  which produce high scoring solutions without a focus on diversity  and sampling-
based techniques  which produce diverse solutions without a focus on the relevance term [2]. Hence 
we do not include those methods here. We also report results for combining different diversity
functions via two operators: (⊗)  where we generate the top M
k solutions for each of k diversity
functions and then concatenate these lists; and (⊕)  where we linearly combine diversity functions
(with coefﬁcients chosen by k-D grid search) and generate M solutions using the combined diversity.
4.1
In interactive foreground-background segmentation  the user provides partial labels via scribbles.
One way to minimize interactions is for the system to provide a set of candidate segmentations for
the user to choose from. We replicate the experimental setup of [2]  who curated 100 images from
the PASCAL VOC 2012 dataset  and manually provided scribbles on objects contained in them.
For each image  the relevance model r(y) is a 2-label pairwise CRF  with a node term for each

˜θ(yi) = θ(yi)+λ(cid:80)

Interactive segmentation

6

Label Cost (LC)

(cid:112)(·)

min{1  ·}
log(1 + ·)

MAP

42.35
42.35
42.35

M=5

45.43
45.72
46.28

M=15

45.58
50.01
50.39

Hamming Ball (HB)

MAP

43.43
43.43

M=5

51.21
51.71

M=15

52.90
55.32

DivMBest
HB

⊗ Combined Diversity

M=15

M=16

(cid:112)(·)

min{1  ·}
log(1 + ·)

⊕ Combined Diversity

M=15

Label Transition (LT)

MAP

42.35
42.35
42.35

M=5

44.26
45.43
45.92

M=15

44.78
46.21
46.89

HB ⊗ LC ⊗ LT
DivMBest ⊗ HB ⊗ LC ⊗ LT
Table 2: PASCAL VOC 2012 val oracle accuracies for different diversity functions.

DivMBest ⊕ HB
DivMBest ⊕ LC ⊕ LT

56.97
-

-
57.39

55.89
53.47

superpixel in the image and an edge term for each adjacent pair of superpixels. At each superpixel 
we extract colour and texture features. We train a Transductive SVM from the partial supervision
provided by the user scribbles. The node potentials are derived from the scores of these TSVMs. The
edge potentials are contrast-sensitive Potts. Fifty of the images were used for tuning the diversity
parameters λ  γ  and the other 50 for reporting oracle accuracies. The 2-label contrast-sensitive Potts
model results in a supermodular relevance function r(y)  which can be efﬁciently maximized via
graph cuts [20]. The Hamming ball diversity dlb(y|S) is a collection of cardinality factors  which
we optimize with the Cyborg implementation [30].
Results. For each of the 50 test images in our dataset we generated the single best y1 and 5 addi-
tional solutions {y2  . . .   y6} using each method. Table 3 shows the average oracle accuracies for
DivMBest  Hamming ball diversity  and their two combinations. We can see that the combinations
slightly outperform both approaches.

DivMBest
Hamming Ball
DivMBest⊗Hamming Ball
DivMBest⊕Hamming Ball

MAP

91.57
91.57
-
-

M=2

93.16
93.95
-
-

M=6

95.02
94.86
95.16
95.14

Table 3: Interactive segmentation: oracle pixel accuracies averaged over 50 test images

4.2 Category level Segmentation
In category-level object segmentation  we label each pixel with one of 20 object categories or back-
ground. We construct a multi-label pairwise CRF on superpixels. Our node potentials are outputs of
category-speciﬁc regressors trained by [6]  and our edge potentials are multi-label Potts. Inference
in the presence of diversity terms is performed with the implementations of Delong et al. [9] for
label costs  Tarlow et al. [30] for Hamming ball diversity  and Boykov et al. [3] for label transitions.

Figure 3: Qualitative Results:
each row shows the original im-
age  ground-truth segmentation
(GT) from PASCAL  the single-
best segmentation y1  and oracle
segmentation from the M = 15
segmentations (excluding y1) for
different deﬁnitions of diversity.
Hamming typically performs the
best. In certain situations (row3) 
label transitions help since the
single-best segmentation y1 in-
cluded a rare pair of labels (dog-
cat boundary).

Results. We evaluate all methods on the PASCAL VOC 2012 data [11]  consisting of train  val
and test partitions with about 1450 images each. We train the regressors of [6] on train  and
report oracle accuracies of different methods on val (we cannot report oracle results on test since
those annotations are not publicly available). Diversity parameters (γ  λ) are chosen by perform-
ing cross-val on val. The standard PASCAL accuracy is the corpus-level intersection-over-union
measure  averaged over all categories. For both label cost and transition  we try 3 different concave

7

functions h(·) = min{1 ·} (cid:112)

F (S∗)−Fmin ≥ (1 − 1

eα ) −

i i

(·) and log(1 + ·). Table 2 shows the results.3 Hamming ball diver-
sity performs the best  followed by DivMBest  and label cost/transitions are worse here. We found
that while worst on average  label transition diversity helps in an interesting scenario – when the
ﬁrst best segmentation y1 includes a pair of rare or mutually confusing labels (say dog-cat). Fig. 3
shows an example  and more illustrations are provided in the supplement. In these cases  searching
for a different label transition produces a better segmentation. Finally  we note that lists produced
with combined diversity signiﬁcantly outperform any single method (including DivMBest).
5 Discussion and Conclusion
In this paper  we study greedy algorithms for maximizing scoring functions that promote diverse
sets of combinatorial conﬁgurations. This problem arises naturally in domains such as Computer
Vision  Natural Language Processing  or Computational Biology  where we want to search for a set
of diverse high-quality solutions in a structured output space.
The diversity functions we propose are monotone submodular functions by construction. Thus  if
r(y) + p(y) ≥ 0 for all y  then the entire scoring function F is monotone submodular. We showed
that r(y) can simply be learned to be positive. The greedy algorithm for maximizing monotone
submodular functions has proved useful in moderately-sized unstructured spaces. To the best of our
knowledge  this is the ﬁrst generalization to exponentially large structured output spaces. In par-
ticular  our contribution lies in reducing the greedy augmentation step to inference with structured 
efﬁciently solvable HOPs. This insight makes new connections between submodular optimization
and work on inference in graphical models. We now address some questions.
Can we sample? One question that may be posed is how random sampling would perform for large
ground sets V . Unfortunately  the expected value of a random sample of M elements can be much
worse than the optimal value F (S∗)  especially if N is large. Lemma 1 is proved in the supplement.
Lemma 1. Let S ⊆ V be a sample of size M taken uniformly at random. There exist monotone
submodular functions where E[F (S)] ≤ M
Guarantees? If F is nonnegative  monotone submodular  then using an exact HOP inference algo-
rithm will clearly result in an approximation factor of 1− 1/e. But many HOP inference procedures
are approximate. Lemma 2 formalizes how approximate inference affects the approximation bounds.
Lemma 2. Let F ≥ 0 be monotone submodular.
If each step of the greedy algorithm uses an
approximate marginal gain maximizer bt+1 with F (bt+1 | St) ≥ α maxa∈V F (a | St) − t+1  then
i=1 t.
F (SM ) ≥ (1 − 1
Parts of Lemma 2 have been observed in previous work [13  29]; we show the combination in the
supplement. If F is monotone but not nonnegative  then Lemma 2 can be extended to a relative error
bound F (SM )−Fmin
that refers to Fmin = minS F (S) and the optimal
solution S∗. While stating these results  we add that further additive approximation losses occur if
the approximation bound for inference is computed on a shifted or reﬂected function (positive scores
vs positive energies). We pose theoretical improvements as an open question for future work. That
said  our experiments convincingly show that the algorithms perform very well in practice  even
when there are no guarantees (as with Hamming Ball diversity).
Generalization. In addition to the three speciﬁc examples in Section 3  our constructions generalize
to the broad HOP class of upper-envelope potentials [18]. The details are provided in the supplement.
Acknowledgements. We thank Xiao Lin for his help. The majority of this work was done while AP was an
intern at Virginia Tech. AP and DB were partially supported by the National Science Foundation under Grant
No. IIS-1353694 and IIS-1350553  the Army Research Ofﬁce YIP Award W911NF-14-1-0180  and the Ofﬁce
of Naval Research Award N00014-14-1-0679  awarded to DB. SJ was supported by gifts from Amazon Web
Services  Google  SAP  The Thomas and Stacey Siebel Foundation  Apple  C3Energy  Cisco  Cloudera  EMC 
Ericsson  Facebook  GameOnTalis  Guavus  HP  Huawei  Intel  Microsoft  NetApp  Pivotal  Splunk  Virdata 
VMware  WANdisco  and Yahoo!.
References
[1] D. Batra. An Efﬁcient Message-Passing Algorithm for the M-Best MAP Problem. In UAI  2012. 6

(cid:80)M
eα ) max|S|≤M F (S) −
(cid:80)
F (S∗)−Fmin

N max|S|=M F (S).

3MAP accuracies in Table 2 are different because of two different approximate MAP solvers: LabelCost/-

Transition use alpha-expansion and HammingBall/DivMBest use message-passing.

8

[2] D. Batra  P. Yadollahpour  A. Guzman-Rivera  and G. Shakhnarovich. Diverse M-Best Solutions in

Markov Random Fields. In ECCV  2012. 2  6

[3] Y. Boykov  O. Veksler  and R. Zabih. Efﬁcient approximate energy minimization via graph cuts. PAMI 

20(12):1222–1239  2001. 5  7

[4] N. Buchbinder  M. Feldman  J. Naor  and R. Schwartz. A tight (1/2) linear-time approximation to uncon-

strained submodular maximization. In FOCS  2012. 3

[5] J. Carbonell and J. Goldstein. The use of MMR  diversity-based reranking for reordering documents
and producing summaries. In Proc. 21st annual international ACM SIGIR conference on Research and
Development in Information Retrieval  SIGIR ’98  pages 335–336  1998. 1

[6] J. Carreira  R. Caseiro  J. Batista  and C. Sminchisescu. Semantic segmentation with second-order pool-

[7] C. Chen  V. Kolmogorov  Y. Zhu  D. Metaxas  and C. H. Lampert. Computing the m most probable modes

[8] C. Chen  H. Liu  D. Metaxas  and T. Zhao. Mode estimation for high dimensional discrete tree graphical

[9] A. Delong  A. Osokin  H. N. Isack  and Y. Boykov. Fast approximate energy minimization with label

ing. In ECCV  pages 430–443  2012. 7

of a graphical model. In AISTATS  2013. 2

models. In NIPS  2014. 2

costs. In CVPR  pages 2173–2180  2010. 5  7

[10] D. Dey  T. Liu  M. Hebert  and J. A. Bagnell. Contextual sequence prediction with application to control

library optimization. In Robotics Science and Systems (RSS)  2012. 1  3

[11] M. Everingham  L. V. Gool  C. K. I. Williams  J. Winn  and A. Zisserman. The PASCAL Visual Object

Classes Challenge 2012 (VOC2012). 6  7

[12] U. Feige  V. S. Mirrokni  and J. Vondrak. Maximizing non-monotone submodular functions. In FOCS 

[13] P. Goundan and A. Schulz. Revisiting the greedy approach to submodular set function maximization.

2007. ISBN 0-7695-3010-9. 3

Manuscript  2009. 8

Structured Outputs. In Proc. NIPS  2012. 2

structured prediction. In AISTATS  2014. 2

CVPR  2011. 5

[14] A. Guzman-Rivera  D. Batra  and P. Kohli. Multiple Choice Learning: Learning to Produce Multiple

[15] A. Guzman-Rivera  P. Kohli  D. Batra  and R. Rutenbar. Efﬁciently enforcing diversity in multi-output

[16] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: Coupling edges in graph cuts. In

[17] D. Kempe  J. Kleinberg  and E. Tardos. Maximizing the spread of inﬂuence through a social network. In

ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)  2003. 1

[18] P. Kohli and M. P. Kumar. Energy minimization for linear envelope MRFs. In CVPR  2010. 8
[19] P. Kohli  A. Osokin  and S. Jegelka. A principled deep random ﬁeld model for image segmentation. In

[20] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? PAMI  26(2):

CVPR  2013. 5

147–159  2004. 7

[21] A. Krause and S. Jegelka. Submodularity in machine learning: New directions. ICML Tutorial  2013. 2
[22] A. Krause  A. Singh  and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory 

efﬁcient algorithms and empirical studies. JMLR  9:235–284  2008. 1

[23] A. Kulesza and B. Taskar. Structured determinantal point processes. In Proc. NIPS  2010. 2
[24] J. D. Lafferty  A. McCallum  and F. C. N. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In ICML  2001. 1

[25] Y. LeCun  S. Chopra  R. Hadsell  M. Ranzato  and F. Huang. A tutorial on energy-based learning. In

Predicting Structured Data. MIT Press  2006. 3

[26] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In ACL  2011. 1  3
[27] G. Nemhauser  L. Wolsey  and M. Fisher. An analysis of approximations for maximizing submodular set

functions. Mathematical Programming  14(1):265–294  1978. 2  3

[28] D. Park and D. Ramanan. N-best maximal decoders for part models. In ICCV  2011. 2
[29] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In NIPS  2008.

3  8

[30] D. Tarlow  I. E. Givoni  and R. S. Zemel. HOP-MAP: Efﬁcient message passing with high order potentials.

In AISTATS  pages 812–819  2010. 5  6  7

[31] B. Taskar  C. Guestrin  and D. Koller. Max-Margin Markov networks. In NIPS  2003. 1
[32] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured and

interdependent output variables. JMLR  6:1453–1484  2005. 1  3

[33] P. Viola and M. J. Jones. Robust real-time face detection. Int. J. Comput. Vision  57(2):137–154  May

[34] C. Yanover and Y. Weiss. Finding the m most probable conﬁgurations using loopy belief propagation. In

2004. ISSN 0920-5691. 1

NIPS  2003. 6

9

,Adarsh Prasad
Stefanie Jegelka
Dhruv Batra