2012,Parametric Local Metric Learning for Nearest Neighbor Classification,We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this ''independence'' approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space.  We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems  tens of thousands of instances  and compared it with several state of the art metric learning methods  both global and local  as well as to SVM with automatic kernel selection  all of which it outperforms in a significant manner.,Parametric Local Metric Learning for Nearest

Neighbor Classiﬁcation

Jun Wang

Adam Woznica

Department of Computer Science

Department of Computer Science

University of Geneva

Switzerland

Jun.Wang@unige.ch

University of Geneva

Switzerland

Adam.Woznica@unige.ch

Alexandros Kalousis

Department of Business Informatics

University of Applied Sciences

Western Switzerland

Alexandros.Kalousis@hesge.ch

Abstract

We study the problem of learning local metrics for nearest neighbor classiﬁcation.
Most previous works on local metric learning learn a number of local unrelated
metrics. While this ”independence” approach delivers an increased ﬂexibility its
downside is the considerable risk of overﬁtting. We present a new parametric local
metric learning method in which we learn a smooth metric matrix function over the
data manifold. Using an approximation error bound of the metric matrix function
we learn local metrics as linear combinations of basis metrics deﬁned on anchor
points over different regions of the instance space. We constrain the metric matrix
function by imposing on the linear combinations manifold regularization which
makes the learned metric matrix function vary smoothly along the geodesics of
the data manifold. Our metric learning method has excellent performance both
in terms of predictive power and scalability. We experimented with several large-
scale classiﬁcation problems  tens of thousands of instances  and compared it with
several state of the art metric learning methods  both global and local  as well as to
SVM with automatic kernel selection  all of which it outperforms in a signiﬁcant
manner.

1

Introduction

The nearest neighbor (NN) classiﬁer is one of the simplest and most classical non-linear classiﬁca-
tion algorithms. It is guaranteed to yield an error no worse than twice the Bayes error as the number
of instances approaches inﬁnity. With ﬁnite learning instances  its performance strongly depends
on the use of an appropriate distance measure. Mahalanobis metric learning [4  15  9  10  17  14]
improves the performance of the NN classiﬁer if used instead of the Euclidean metric. It learns
a global distance metric which determines the importance of the different input features and their
correlations. However  since the discriminatory power of the input features might vary between dif-
ferent neighborhoods  learning a global metric cannot ﬁt well the distance over the data manifold.
Thus a more appropriate way is to learn a metric on each neighborhood and local metric learn-
ing [8  3  15  7] does exactly that. It increases the expressive power of standard Mahalanobis metric
learning by learning a number of local metrics (e.g. one per each instance).

1

Local metric learning has been shown to be effective for different learning scenarios. One of the
ﬁrst local metric learning works  Discriminant Adaptive Nearest Neighbor classiﬁcation [8]  DANN 
learns local metrics by shrinking neighborhoods in directions orthogonal to the local decision bound-
aries and enlarging the neighborhoods parallel to the boundaries. It learns the local metrics inde-
pendently with no regularization between them which makes it prone to overﬁtting. The authors of
LMNN-Multiple Metric (LMNN-MM) [15] signiﬁcantly limited the number of learned metrics and
constrained all instances in a given region to share the same metric in an effort to combat overﬁtting.
In the supervised setting they ﬁxed the number of metrics to the number of classes; a similar idea
has been also considered in [3]. However  they too learn the metrics independently for each region
making them also prone to overﬁtting since the local metrics will be overly speciﬁc to their respec-
tive regions. The authors of [16] learn local metrics using a least-squares approach by minimizing a
weighted sum of the distances of each instance to apriori deﬁned target positions and constraining the
instances in the projected space to preserve the original geometric structure of the data in an effort to
alleviate overﬁtting. However  the method learns the local metrics using a learning-order-sensitive
propagation strategy  and depends heavily on the appropriate deﬁnition of the target positions for
each instance  a task far from obvious. In another effort to overcome the overﬁtting problem of the
discriminative methods [8  15]  Generative Local Metric Learning  GLML  [11]  propose to learn
local metrics by minimizing the NN expected classiﬁcation error under strong model assumptions.
They use the Gaussian distribution to model the learning instances of each class. However  the
strong model assumptions might easily be very inﬂexible for many learning problems.
In this paper we propose the Parametric Local Metric Learning method (PLML) which learns a
smooth metric matrix function over the data manifold. More precisely  we parametrize the metric
matrix of each instance as a linear combination of basis metric matrices of a small set of anchor
points; this parametrization is naturally derived from an error bound on local metric approximation.
Additionally we incorporate a manifold regularization on the linear combinations  forcing the linear
combinations to vary smoothly over the data manifold. We develop an efﬁcient two stage algorithm
that ﬁrst learns the linear combinations of each instance and then the metric matrices of the anchor
points. To improve scalability and efﬁciency we employ a fast ﬁrst-order optimization algorithm 
FISTA [2]  to learn the linear combinations as well as the basis metrics of the anchor points. We
experiment with the PLML method on a number of large scale classiﬁcation problems with tens of
thousands of learning instances. The experimental results clearly demonstrate that PLML signiﬁ-
cantly improves the predictive performance over the current state-of-the-art metric learning methods 
as well as over multi-class SVM with automatic kernel selection.

2 Preliminaries
We denote by X the n×d matrix of learning instances  the i-th row of which is the xT
i ∈ Rd instance 
and by y = (y1  . . .   yn)T   yi ∈ {1  . . .   c} the vector of class labels. The squared Mahalanobis
distance between two instances in the input space is given by:

M(xi  xj) = (xi − xj)T M(xi − xj)
d2

where M is a PSD metric matrix (M (cid:23) 0). A linear metric learning method learns a Mahalanobis
metric M by optimizing some cost function under the PSD constraints for M and a set of additional
constraints on the pairwise instance distances. Depending on the actual metric learning method 
different kinds of constraints on pairwise distances are used. The most successful ones are the
large margin triplet constraints. A triplet constraint denoted by c(xi  xj  xk)  indicates that in the
projected space induced by M the distance between xi and xj should be smaller than the distance
between xi and xk.
Very often a single metric M can not model adequately the complexity of a given learning problem
in which discriminative features vary between different neighborhoods. To address this limitation
in local metric learning we learn a set of local metrics. In most cases we learn a local metric for
each learning instance [8  11]  however we can also learn a local metric for some part of the instance
space in which case the number of learned metrics can be considerably smaller than n  e.g. [15]. We
follow the former approach and learn one local metric per instance. In principle  distances should
then be deﬁned as geodesic distances using the local metric on a Riemannian manifold. However 
this is computationally difﬁcult  thus we deﬁne the distance between instances xi and xj as:

(xi  xj) = (xi − xj)T Mi(xi − xj)

d2
Mi

2

where Mi is the local metric of instance xi. Note that most often the local metric Mi of instance
xi is different from that of xj. As a result  the distance d2
(xi  xj) does not satisfy the symmetric
property  i.e. it is not a proper metric. Nevertheless  in accordance to the standard practice we will
continue to use the term local metric learning following [15  11].

Mi

3 Parametric Local Metric Learning

We assume that there exists a Lipschitz smooth vector-valued function f (x)  the output of which
is the vectorized local metric matrix of instance x. Learning the local metric of each instance is
essentially learning the value of this function at different points over the data manifold. In order to
signiﬁcantly reduce the computational complexity we will approximate the metric function instead
of directly learning it.

a

is

f (x)

function

smooth
to a vector norm (cid:107)·(cid:107) if (cid:107)f (x) − f (x(cid:48))(cid:107) ≤ α(cid:107)x − x(cid:48)(cid:107) and

(α  β  p)-Lipschitz

on Rd

(cid:13)(cid:13)f (x) − f (x(cid:48)) − ∇f (x(cid:48))T (x − x(cid:48))(cid:13)(cid:13) ≤ β (cid:107)x − x(cid:48)(cid:107)1+p  where ∇f (x(cid:48))T is the derivative of

Deﬁnition 1 A vector-valued
function with respect
the f function at x(cid:48). We assume α  β > 0 and p ∈ (0  1].
[18] have shown that any Lipschitz smooth real function f (x) deﬁned on a lower dimensional man-
ifold can be approximated by a linear combination of function values f (u)  u ∈ U  of a set U of
anchor points. Based on this result we have the following lemma that gives the respective error
bound for learning a Lipschitz smooth vector-valued function.
Lemma 1 Let (γ  U) be a nonnegative weighting on anchor points U in Rd. Let f be an (α  β  p)-
Lipschitz smooth vector function. We have for all x ∈ Rd:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)f (x) −(cid:88)

u∈U

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ α

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)x −(cid:88)

u∈U

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) + β

(cid:88)

u∈U

γu(x)f (u)

γu(x)u

γu(x)(cid:107)x − u(cid:107)1+p

(1)

The proof of the above Lemma 1 is similar to the proof of Lemma 2.1 in [18]; for lack of space
we omit its presentation. By the nonnegative weighting strategy (γ  U)  the PSD constraints on the
approximated local metric is automatically satisﬁed if the local metrics of anchor points are PSD
matrices.
Lemma 1 suggests a natural way to approximate the local metric function by parameterizing the
metric Mi of each instance xi as a weighted linear combination  Wi ∈ Rm  of a small set of
metric basis  {Mb1  . . .   Mbm}  each one associated with an anchor point deﬁned in some region
of the instance space. This parametrization will also provide us with a global way to regularize the
ﬂexibility of the metric function. We will ﬁrst learn the vector of weights Wi for each instance xi 
and then the basis metric matrices; these two together  will give us the Mi metric for the instance
xi.
More formally  we deﬁne a m × d matrix U of anchor points  the i-th row of which is the anchor
i ∈ Rd. We denote by Mbi the Mahalanobis metric matrix associated with ui.
point ui  where uT
The anchor points can be deﬁned using some clustering algorithm  we have chosen to deﬁne them
as the means of clusters constructed by the k-means algorithm. The local metric Mi of an instance
xi is parametrized by:

WibkMbk   Wibk ≥ 0 

(2)
where W is a n × m weight matrix  and its Wibk entry is the weight of the basis metric Mbk for
Wibk = 1 removes the scaling problem between different local
metrics. Using the parametrization of equation (2)  the squared distance of xi to xj under the metric
Mi is:

Wibk = 1

bk

bk

(cid:88)
the instance xi. The constraint(cid:80)

Mi =

bk

(cid:88)

d2
Mi

(xi  xj) =

Wibk d2

Mbk

(xi  xj)

(3)

Mbk

where d2
(xi  xj) is the squared Mahalanobis distance between xi and xj under the basis metric
Mbk. We will show in the next section how to learn the weights of the basis metrics for each instance
and in section 3.2 how to learn the basis metrics.

bk

3

(cid:88)

Algorithm 1 Smoothl Local Linear Weight Learning

Input: W0  X  U  G  L  λ1  and λ2
Output: matrix W

deﬁne(cid:101)gβ Y(W) = g(Y) + tr(∇g(Y)T (W − Y)) + β
while g(Wi) >(cid:101)gβ Yi(Wi) do

initialize: t1 = 1  β = 1 Y1 = W0  and i = 0
repeat
β∇g(Yi)))
β∇g(Yi)))

i = i + 1  Wi = P roj((Yi − 1

β = 2β  Wi = P roj((Yi − 1

2 (cid:107)W − Y(cid:107)2

F

√

end while
1+
ti+1 =

1+4t2
i
2
until converges;

  Yi+1 = Wi + ti−1

ti+1

(Wi − Wi−1)

3.1 Smooth Local Linear Weighting

Lemma 1 bounds the approximation error by two terms. The ﬁrst term states that x should be close
to its linear approximation  and the second that the weighting should be local. In addition we want
the local metrics to vary smoothly over the data manifold. To achieve this smoothness we rely
on manifold regularization and constrain the weight vectors of neighboring instances to be similar.
Following this reasoning we will learn Smooth Local Linear Weights for the basis metrics by mini-
mizing the error bound of (1) together with a regularization term that controls the weight variation

of similar instances. To simplify the objective function  we use the term(cid:13)(cid:13)x −(cid:80)
instead of(cid:13)(cid:13)x −(cid:80)

u∈U γu(x)u(cid:13)(cid:13)2
u∈U γu(x)u(cid:13)(cid:13). By including the constraints on the W weight matrix in (2)  the

optimization problem is given by:

g(W) = (cid:107)X − WU(cid:107)2

min
W

(cid:88)

s.t.

Wibk ≥ 0 

Wibk = 1 ∀i  bk

F + λ1tr(WG) + λ2tr(WTLW)

(4)

bk

where tr(·) and (cid:107)·(cid:107)F denote respectively the trace norm of a square matrix and the Frobenius norm
of a matrix. The m × n matrix G is the squared distance matrix between each anchor point ui and
each instance xj  obtained for p = 1 in (1)  i.e. its (i  j) entry is the squared Euclidean distance
(cid:80)
between ui and xj. L is the n × n Laplacian matrix constructed by D − S  where S is the n × n
symmetric pairwise similarity matrix of learning instances and D is a diagonal matrix with Dii =
k Sik. Thus the minimization of the tr(WTLW) term constrains similar instances to have similar
weight coefﬁcients. The minimization of the tr(WG) term forces the weights of the instances
to reﬂect their local properties. Most often the similarity matrix S is constructed using k-nearest
neighbors graph [19]. The λ1 and λ2 parameters control the importance of the different terms.
Since the cost function g(W) is convex quadratic with W and the constraint is simply linear  (4) is
a convex optimization problem with a unique optimal solution. The constraints on W in (4) can be
seen as n simplex constraints on each row of W; we will use the projected gradient method to solve
the optimization problem. At each iteration t  the learned weight matrix W is updated by:

Wt+1 = P roj(Wt − η∇g(Wt))

(5)
where η > 0 is the step size and ∇g(Wt) is the gradient of the cost function g(W) at Wt. The
P roj(·) denotes the simplex projection operator on each row of W. Such a projection operator can
be efﬁciently implemented with a complexity of O(nm log(m)) [6]. To speed up the optimization
procedure we employ a fast ﬁrst-order optimization method FISTA  [2]. The detailed algorithm is
described in Algorithm 1. The Lipschitz constant β required by this algorithm is estimated by using

the condition of g(Wi) ≤ (cid:101)gβ Yi(Wi) [1]. At each iteration  the main computations are in the

gradient and the objective value with complexity O(nmd + n2m).
To set the weights of the basis metrics for a testing instance we can optimize (4) given the weight of
the basis metrics for the training instances. Alternatively we can simply set them as the weights of
its nearest neighbor in the training instances. In the experiments we used the latter approach.

4

3.2 Large Margin Basis Metric Learning

In this section we deﬁne a large margin based algorithm to learn the basis metrics Mb1  . . .   Mbm.
Given the W weight matrix of basis metrics obtained using Algorithm 1  the local metric Mi of
an instance xi deﬁned in (2) is linear with respect to the basis metrics Mb1   . . .   Mbm. We deﬁne
the relative comparison distance of instances xi  xj and xk as: d2
(xi  xj). In
Mi
a large margin constraint c(xi  xj  xk)  the squared distance d2
(xi  xk) is required to be larger
(xi  xj) + 1  otherwise an error ξijk ≥ 0 is generated. Note that  this relative comparison
than d2
deﬁnition is different from that deﬁned in LMNN-MM [15]. In LMNN-MM to avoid over-ﬁtting 
different local metrics Mj and Mk are used to compute the squared distance d2
(xi  xj) and
(xi  xk) respectively  as no smoothness constraint is added between metrics of different local
d2
Mk
regions.
Given a set of triplet constraints  we learn the basis metrics Mb1  . . .   Mbm with the following
optimization problem:

(xi  xk) − d2

Mj

Mi

Mi

Mi

min

Mb1  ... Mbm  ξ

s.t.

α1

F +

(cid:88)

||Mbl||2

(cid:88)
(cid:88)
ξijk ≥ 0; ∀i  j  k Mbl (cid:23) 0; ∀bl

(xi  xk) − d2

bl
Wibl (d2

Mbl

Mbl

ijk

bl

ξijk + α2

(cid:88)

(cid:88)
(xi  xj)) ≥ 1 − ξijk ∀i  j  k

Wibl d2

(xi  xj)

Mbl

bl

ij

(6)

where α1 and α2 are parameters that balance the importance of the different terms. The large margin
triplet constraints for each instance are generated using its k1 same class nearest neighbors and k2
different class nearest neighbors by requiring its distances to the k2 different class instances to be
larger than those to its k1 same class instances. In the objective function of (6) the basis metrics are
learned by minimizing the sum of large margin errors and the sum of squared pairwise distances of
each instance to its k1 nearest neighbors computed using the local metric. Unlike LMNN we add the
squared Frobenius norm on each basis metrics in the objective function. We do this for two reasons.
First we exploit the connection between LMNN and SVM shown in [5] under which the squared
Frobenius norm of the metric matrix is related to the SVM margin. Second because adding this term
leads to an easy-to-optimize dual formulation of (6) [12].
Unlike many special solvers which optimize the primal form of the metric learning problem [15  13] 
we follow [12] and optimize the Lagrangian dual problem of (6). The dual formulation leads to an
efﬁcient basis metric learning algorithm. Introducing the Lagrangian dual multipliers γijk  pijk and
the PSD matrices Zbl to respectively associate with every large margin triplet constraints  ξijk ≥ 0
and the PSD constraints Mbl (cid:23) 0 in (6)  we can easily derive the following Lagrangian dual form

γijk −(cid:88)

(cid:88)
(cid:88)
1 ≥ γijk ≥ 0; ∀i j k Zbl (cid:23) 0; ∀bl
(Z∗
bl

Wibl Aij(cid:107)2
(cid:80)
ijkWibl Cijk−α2
and
ikxik−xT
ijxij respectively 

γijkWibl Cijk − α2
+(cid:80)
and the corresponding optimality conditions: M∗
1 ≥ γijk ≥ 0  where the matrices Aij and Cijk are given by xT
where xij = xi − xj.
Compared to the primal form  the main advantage of the dual formulation is that the second term
in the objective function of (7) has a closed-form solution for Zbl given a ﬁxed γ. To drive the
optimal solution of Zbl  let Kbl = α2
ijk γijkWibl Cijk. Then  given a ﬁxed γ 
the optimal solution of Zbl is Z∗
= (Kbl )+  where (Kbl )+ projects the matrix Kbl onto the PSD
cone  i.e. (Kbl )+ = U[max(diag(Σ))  0)]UT with Kbl = UΣUT.
Now  (7) is rewritten as:

(cid:80)
ij Wibl Aij −(cid:80)

ijxij and xT

· (cid:107)Zbl +

(cid:88)

Zb1  ... Zbm  γ

ij Wibl Aij )

1
4α1

ijk γ∗

max

(7)

s.t.

2α1

=

ijk

ijk

bl

bl

bl

ij

F

1
4α1

(cid:107)(Kbl )+ − Kbl(cid:107)2

F

(8)

g(γ) = −(cid:88)

γijk +
1 ≥ γijk ≥ 0;∀i  j  k

ijk

min

γ

s.t.

(cid:88)

bl

5

((K∗

bl

= 1
2α1

)+ − K∗

function in (8)  ∇g(γijk)  is given by: ∇g(γijk) = −1 +(cid:80)

And the optimal condition for Mbl is M∗
). The gradient of the objective
(cid:104)(Kbl )+ − Kbl   Wibl Cijk(cid:105). At
each iteration  γ is updated by: γi+1 = BoxP roj(γi − η∇g(γi)) where η > 0 is the step size.
The BoxP roj(·) denotes the simple box projection operator on γ as speciﬁed in the constraints
of (8). At each iteration  the main computational complexity lies in the computation of the eigen-
decomposition with a complexity of O(md3) and the computation of the gradient with a complexity
of O(m(nd2 + cd))  where m is the number of basis metrics and c is the number of large margin
triplet constraints. As in the weight learning problem the FISTA algorithm is employed to accelerate
the optimization process; for lack of space we omit the algorithm presentation.

bl

bl
1

bl

2α1

4 Experiments

In this section we will evaluate the performance of PLML and compare it with a number of rel-
evant baseline methods on six datasets with large number of instances  ranging from 5K to 70K
instances; these datasets are Letter  USPS  Pendigits  Optdigits  Isolet and MNIST. We want to de-
termine whether the addition of manifold regularization on the local metrics improves the predictive
performance of local metric learning  and whether the local metric learning improves over learning
with single global metric. We will compare PLML against six baseline methods. The ﬁrst  SML  is
a variant of PLML where a single global metric is learned  i.e. we set the number of basis in (6) to
one. The second  Cluster-Based LML (CBLML)  is also a variant of PLML without weight learn-
ing. Here we learn one local metric for each cluster and we assign a weight of one for a basis metric
Mbi if the corresponding cluster of Mbi contains the instance  and zero otherwise. Finally  we
also compare against four state of the art metric learning methods LMNN [15]  BoostMetric [13]1 
GLML [11] and LMNN-MM [15]2. The former two learn a single global metric and the latter two
a number of local metrics. In addition to the different metric learning methods  we also compare
PLML against multi-class SVMs in which we use the one-against-all strategy to determine the class
label for multi-class problems and select the best kernel with inner cross validation.
Since metric learning is computationally expensive for datasets with large number of features we
followed [15] and reduced the dimensionality of the USPS  Isolet and MINIST datasets by applying
PCA. In these datasets the retained PCA components explain 95% of their total variances. We
preprocessed all datasets by ﬁrst standardizing the input features  and then normalizing the instances
to so that their L2-norm is one.
PLML has a number of hyper-parameters. To reduce the computational time we do not tune λ1
and λ2 of the weight learning optimization problem (4)  and we set them to their default values of
λ1 = 1 and λ2 = 100. The Laplacian matrix L is constructed using the six nearest neighbors graph
following [19]. The anchor points U are the means of clusters constructed with k-means clustering.
The number m of anchor points  i.e.
the number of basis metrics  depends on the complexity of
the learning problem. More complex problems will often require a larger number of anchor points
to better model the complexity of the data. As the number of classes in the examined datasets is
10 or 26  we simply set m = 20 for all datasets. In the basis metric learning problem (6)  the
number of the dual parameters γ is the same as the number of triplet constraints. To speedup the
learning process  the triplet constraints are constructed only using the three same-class and the three
different-class nearest neighbors for each learning instance. The parameter α2 is set to 1  while
the parameter α1 is the only parameter that we select from the set {0.01  0.1  1  10  100} using
2-fold inner cross-validation. The above setting of basis metric learning for PLML is also used
with the SML and CBLML methods. For LMNN and LMNN-MM we use their default settings 
[15]  in which the triplet constraints are constructed by the three nearest same-class neighbors and
all different-class samples. As a result  the number of triplet constraints optimized in LMNN and
LMNN-MM is much larger than those of PLML  SML  BoostMetric and CBLML. The local metrics
are initialized by identity matrices. As in [11]  GLML uses the Gaussian distribution to model the
learning instances from the same class. Finally  we use the 1-NN rule to evaluate the performance
of the different metric learning methods. In addition as we already mentioned we also compare
against multi-class SVM. Since the performance of the latter depends heavily on the kernel with
which it is coupled we do automatic kernel selection with inner cross validation to select the best

1http://code.google.com/p/boosting
2http://www.cse.wustl.edu/∼kilian/code/code.html.

6

(a) LMNN-MM

(b) CBLML

(c) GLML

(d) PLML

Figure 1: The visualization of learned local metrics of LMNN-MM  CBLML  GLML and PLML.

Table 1: Accuracy results. The superscripts +−= next to the accuracies of PLML indicate the result
of the McNemar’s statistical test with LMNN  BoostMetric  SML  CBLML  LMNN-MM  GMLM
and SVM. They denote respectively a signiﬁcant win  loss or no difference for PLML. The number
in the parenthesis indicates the score of the respective algorithm for the given dataset based on the
pairwise comparisons of the McNemar’s statistical test.

Single Metric Learning Baselines
SML

LMNN

Datasets
Letter
Pendigits
Optdigits
Isolet
USPS
MNIST
Total Score

PLML

97.22+++|+++|+(7.0)
98.34+++|+++|+(7.0)
97.72===|+++|=(5.0)
95.25=+=|+++|=(5.5)
98.26+++|+++|=(6.5)
97.30=++|+++|=(6.0)

37

96.08(2.5)
97.43(2.0)
97.55(5.0)
95.51(5.5)
97.92(4.5)
97.30(6.0)

25.5

BoostMetric
96.49(4.5)
97.43(2.5)
97.61(5.0)
89.16(2.5)
97.65(2.5)
96.03(2.5)

19.5

Local Metric Learning Baselines

CBLML
95.82(2.5)
97.94(5.0)
95.94(1.5)
89.03(2.5)
96.22(0.5)
95.77(2.5)

LMNN-MM
95.02(1.0)
97.43(2.0)
95.94(1.5)
84.61(0.5)
97.90(4.0)
93.24(1.0)

14.5

10

GLML

93.86(0.0)
96.88(0.0)
94.82(0.0)
84.03(0.5)
96.05(0.5)
84.02(0.0)

1

SVM

96.64(5.0)
97.91(5.0)
97.33(5.0)
95.19(5.5)
98.19(5.5)
97.62(6.0)

32.5

96.71(5.5)
97.80(4.5)
97.22(5.0)
94.68(5.5)
97.94(4.0)
96.57(4.0)

28.5

kernel and parameter setting. The kernels were chosen from the set of linear  polynomial (degree 2 3
and 4)  and Gaussian kernels; the width of the Gaussian kernel was set to the average of all pairwise
distances. Its C parameter of the hinge loss term was selected from {0.1  1  10  100}.
To estimate the classiﬁcation accuracy for Pendigits  Optdigits  Isolet and MNIST we used the de-
fault train and test split  for the other datasets we used 10-fold cross-validation. The statistical
signiﬁcance of the differences were tested with McNemar’s test with a p-value of 0.05. In order to
get a better understanding of the relative performance of the different algorithms for a given dataset
we used a simple ranking schema in which an algorithm A was assigned one point if it was found
to have a statistically signiﬁcantly better accuracy than another algorithm B  0.5 points if the two
algorithms did not have a signiﬁcant difference  and zero points if A was found to be signiﬁcantly
worse than B.

4.1 Results

In Table 1 we report the experimental results. PLML consistently outperforms the single global
metric learning methods LMNN  BoostMetric and SML  for all datasets except Isolet on which
its accuracy is slightly lower than that of LMNN. Depending on the single global metric learning
method with which we compare it  it is signiﬁcantly better in three  four  and ﬁve datasets ( for
LMNN  SML  and BoostMetric respectively)  out of the six and never singiﬁcantly worse. When
we compare PLML with CBLML and LMNN-MM  the two baseline methods which learn one local
metric for each cluster and each class respectively with no smoothness constraints  we see that it is
statistically signiﬁcantly better in all the datasets. GLML fails to learn appropriate metrics on all
datasets because its fundamental generative model assumption is often not valid. Finally  we see
that PLML is signiﬁcantly better than SVM in two out of the six datasets and it is never signiﬁcantly
worse; remember here that with SVM we also do inner fold kernel selection to automatically select
the appropriate feature speace. Overall PLML is the best performing methods scoring 37 points over
the different datasets  followed by SVM with automatic kernel selection and SML which score 32.5
and 28.5 points respectively. The other metric learning methods perform rather poorly.
Examining more closely the performance of the baseline local metric learning methods CBLML and
LMNN-MM we observe that they tend to overﬁt the learning problems. This can be seen by their
considerably worse performance with respect to that of SML and LMNN which rely on a single
global model. On the other hand PLML even though it also learns local metrics it does not suffer
from the overﬁtting problem due to the manifold regularization. The poor performance of LMNN-

7

(a) Letter

(b) Pendigits

(c) Optdigits

(d) USPS

(e) Isolet

(f) MNIST

Figure 2: Accuracy results of PLML and CBLML with varying number of basis metrics.

MM is not in agreement with the results reported in [15]. The main reason for the difference is the
experimental setting. In [15]  30% of the training instance of each dataset were used as a validation
set to avoid overﬁtting.
To provide a better understanding of the behavior of the learned metrics  we applied PLML LMNN-
MM  CBLML and GLML  on an image dataset containing instances of four different handwritten
digits  zero  one  two  and four  from the MNIST dataset. As in [15]  we use the two main principal
components to learn. Figure 1 shows the learned local metrics by plotting the axis of their corre-
sponding ellipses(black line). The direction of the longer axis is the more discriminative. Clearly
PLML ﬁts the data much better than LMNN-MM and as expected its local metrics vary smoothly.
In terms of the predictive performance  PLML has the best with 82.76% accuracy. The CBLML 
LMNN-MM and GLML have an almost identical performance with respective accuracies of 82.59% 
82.56% and 82.51%.
Finally we investigated the sensitivity of PLML and CBLML to the number of basis metrics  we
experimented with m ∈ {5  10  15  20  25  30  35  40}. The results are given in Figure 2. We see
that the predictive performance of PLML often improves as we increase the number of the basis
metrics. Its performance saturates when the number of basis metrics becomes sufﬁcient to model the
underlying training data. As expected different learning problems require different number of basis
metrics. PLML does not overﬁt on any of the datasets. In contrast  the performance of CBLML gets
worse when the number of basis metrics is large which provides further evidence that CBLML does
indeed overﬁt the learning problems  demonstrating clearly the utility of the manifold regularization.

5 Conclusions

Local metric learning provides a more ﬂexible way to learn the distance function. However they are
prone to overﬁtting since the number of parameters they learn can be very large. In this paper we
presented PLML  a local metric learning method which regularizes local metrics to vary smoothly
over the data manifold. Using an approximation error bound of the metric matrix function  we
parametrize the local metrics by a weighted linear combinations of local metrics of anchor points.
Our method scales to learning problems with tens of thousands of instances and avoids the overﬁtting
problems that plague the other local metric learning methods. The experimental results show that
PLML outperforms signiﬁcantly the state of the art metric learning methods and it has a performance
which is signiﬁcantly better or equivalent to that of SVM with automatic kernel selection.

Acknowledgments

This work was funded by the Swiss NSF (Grant 200021-137949). The support of EU projects
DebugIT (FP7-217139) and e-LICO (FP7-231519)  as well as that of COST Action BM072 (’Urine
and Kidney Proteomics’) is also gratefully acknowledged.

8

References
[1] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing

norms. Optimization for Machine Learning.

[2] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal-recovery
problems. Convex Optimization in Signal Processing and Communications  pages 42–88 
2010.

[3] M. Bilenko  S. Basu  and R.J. Mooney. Integrating constraints and metric learning in semi-

supervised clustering. In ICML  page 11  2004.

[4] J.V. Davis  B. Kulis  P. Jain  S. Sra  and I.S. Dhillon. Information-theoretic metric learning. In

ICML  2007.

[5] H. Do  A. Kalousis  J. Wang  and A. Woznica. A metric learning perspective of svm: on the

relation of svm and lmnn. AISTATS  2012.

[6] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the l 1-ball

for learning in high dimensions. In ICML  2008.

[7] A. Frome  Y. Singer  and J. Malik.

Image retrieval and classiﬁcation using local distance
functions. In Advances in Neural Information Processing Systems  volume 19  pages 417–424.
MIT Press  2007.

[8] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Trans.

on PAMI  1996.

[9] P. Jain  B. Kulis  J.V. Davis  and I.S. Dhillon. Metric and kernel learning using a linear trans-

formation. JMLR  2012.

[10] R. Jin  S. Wang  and Y. Zhou. Regularized distance metric learning: Theory and algorithm. In

NIPS  2009.

[11] Y.K. Noh  B.T. Zhang  and D.D. Lee. Generative local metric learning for nearest neighbor

classiﬁcation. NIPS  2009.

[12] C. Shen  J. Kim  and L. Wang. A scalable dual approach to semideﬁnite metric learning. In

CVPR  2011.

[13] C. Shen  J. Kim  L. Wang  and A. Hengel. Positive semideﬁnite metric learning using boosting-

like algorithms. JMLR  2012.

[14] J. Wang  H. Do  A. Woznica  and A. Kalousis. Metric learning with multiple kernels. In NIPS 

2011.

[15] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. JMLR  2009.

[16] D.Y. Yeung and H. Chang. Locally smooth metric learning with application to image retrieval.

In ICCV  2007.

[17] Y. Ying  K. Huang  and C. Campbell. Sparse metric learning via smooth optimization. NIPS 

2009.

[18] K. Yu  T. Zhang  and Y. Gong. Nonlinear learning using local coordinate coding. NIPS  2009.
[19] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. NIPS  2004.

9

,Longquan Dai
Liang Tang
Yuan Xie
Jinhui Tang