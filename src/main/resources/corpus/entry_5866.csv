2018,Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition,The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge  but require expert demonstrations  which can be difficult or expensive to obtain in practice. We propose inverse event-based control  which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed  such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning  where an agent's goal is to maximize the probability that one or more events will happen at some point in the future  rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks  with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.,Variational Inverse Control with Events: A General

Framework for Data-Driven Reward Deﬁnition

Justin Fu∗ Avi Singh∗ Dibya Ghosh Larry Yang

Sergey Levine

{justinfu  avisingh  dibyaghosh  larrywyang  svlevine}@berkeley.edu

University of California  Berkeley

Abstract

The design of a reward function often poses a major practical challenge to real-
world applications of reinforcement learning. Approaches such as inverse rein-
forcement learning attempt to overcome this challenge  but require expert demon-
strations  which can be difﬁcult or expensive to obtain in practice. We propose
variational inverse control with events (VICE)  which generalizes inverse reinforce-
ment learning methods to cases where full demonstrations are not needed  such as
when only samples of desired goal states are available. Our method is grounded in
an alternative perspective on control and reinforcement learning  where an agent’s
goal is to maximize the probability that one or more events will happen at some
point in the future  rather than maximizing cumulative rewards. We demonstrate
the effectiveness of our methods on continuous control tasks  with a focus on high-
dimensional observations like images where rewards are hard or even impossible
to specify.

1

Introduction

Reinforcement learning (RL) has shown remarkable promise in recent years  with results on a range
of complex tasks such as robotic control (Levine et al.  2016) and playing video games (Mnih
et al.  2015) from raw sensory input. RL algorithms solve these problems by learning a policy
that maximizes a reward function that is considered as part of the problem formulation. There is
little practical guidance that is provided in the theory of RL about how these rewards should be
designed. However  the design of the reward function is in practice critical for good results  and
reward misspeciﬁcation can easily cause unintended behavior (Amodei et al.  2016). For example  a
vacuum cleaner robot rewarded to pick up dirt could exploit the reward by repeatedly dumping dirt on
the ground and picking it up again (Russell & Norvig  2003). Additionally  it is often difﬁcult to write
down a reward function at all. For example  when learning policies from high-dimensional visual
observations  practitioners often resort to using motion capture (Peng et al.  2017) or specialized
computer vision systems (Rusu et al.  2017) to obtain rewards.
As an alternative to reward speciﬁcation  imitation learning (Argall et al.  2009) and inverse rein-
forcement learning (Ng & Russell  2000) instead seek to mimic expert behavior. However  such
approaches require an expert to show how to solve a task. We instead propose a novel problem
formulation  variational inverse control with events (VICE)  which generalizes inverse reinforcement
learning to alternative forms of expert supervision. In particular  we consider cases when we have
examples of a desired ﬁnal outcome  rather than full demonstrations  so the expert only needs to
show what the desired outcome of a task is (see Figure 1). A straightforward way to make use of
these desired outcomes is to train a classiﬁer (Pinto & Gupta  2016; Tung et al.  2018) to distinguish
desired and undesired states. However  for these approaches it is unclear how to correctly sample
negatives and whether using this classiﬁer as a reward will result in the intended behavior  since an

∗equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Standard IRL requires full expert demon-
strations and aims to produce an agent that mimics the
expert. VICE generalizes IRL to cases where we only
observe ﬁnal desired outcomes  which does not require
the expert to actually know how to perform the task.

RL agent can learn to exploit the classiﬁer  in the same way it can exploit human-designed rewards.
Our framework provides a more principled approach  where classiﬁer training corresponds to learning
probabilistic graphical model parameters (see Figure 2)  and policy optimization corresponds to
inferring the optimal actions. By selecting an inference query which corresponds to our intentions 
we can mitigate reward hacking scenarios similar to those previously described  and also specify the
task with examples rather than manual engineering.
Our inverse formulation is based on a corre-
sponding forward control framework which re-
frames control as inference in a graphical model.
Our framework resembles prior work (Kappen
et al.  2009; Toussaint  2009; Rawlik et al. 
2012)  but we extend this connection by re-
placing the conventional notion of rewards with
event occurence variables. Rewards correspond
to log-probabilities of events  and value func-
tions can be interpreted as backward messages
that represent log-probabilities of those events
occurring. This framework retains the full ex-
pressivity of RL  since any rewards can be ex-
pressed as log-probabilities  while providing
more intuitive guidance on task speciﬁcation.
It further allows us to express various intentions 
such as for an event to happen at least once  ex-
actly once at any time step  or once at a speciﬁc timestep. Crucially  our framework does not require
the agent to observe the event happening  but only to know the probability that it occurred. While
this may seem unusual  it is more practical in the real world  where success may be determined
by probabilistic models that themselves carry uncertainty. For example  the previously mentioned
vacuum cleaner robot needs to estimate from its observations whether its task has been accomplished
and would never receive direct feedback from the real world whether a room is clean.
Our contributions are as follows. We ﬁrst in-
troduce the event-based control framework by
extending previous control as inference work to
alternative queries which we believe to be use-
ful in practice. This view on control can ease
the process of reward engineering by mapping
a user’s intention to a corresponding inference
query in a probabilistic graphical model. Our
experiments demonstrate how different queries
Figure 2: Our framework learns event probabilities
can result in different behaviors which align with
from data. We use neural networks as function approx-
the corresponding intentions. We then propose
imators to model this distribution  which allows us to
methods to learn event probabilities from data 
work with high dimensional observations like images.
in a manner analogous to inverse reinforcement
learning. This corresponds to the use case where designing event probabilities by hand is difﬁcult 
but observations (e.g.  images) of successful task completion are easier to provide. This approach is
substantially easier to apply in practical situations  since full demonstrations are not required. Our
experiments demonstrate that our framework can be used in this fashion for policy learning from
high dimensional visual observations where rewards are hard to specify. Moreover  our method
substantially outperforms baselines such as sparse reward RL  indicating that our framework provides
an automated shaping effect when learning events  making it feasible to solve otherwise hard tasks.

2 Related work

Our reformulation of RL is based on the connection between control and inference (Kappen et al. 
2009; Ziebart  2010; Rawlik et al.  2012). The resulting problem is sometimes referred to as maximum
entropy reinforcement learning  or KL control. Duality between control and inference in the case of
linear dynamical systems has been studied in Kalman (1960); Todorov (2008). Maximum entropy
objectives can be optimized efﬁciently and exactly in linearly solvable MDPs (Todorov  2007) and

2

conv layersfc layersenvironments with discrete states. In linear-quadratic systems  control as inference techniques have
been applied to solve path planning problems for robotics (Toussaint  2009). In the context of deep
RL  maximum entropy objectives have been used to derive soft variants of Q-learning and policy
gradient algorithms (Haarnoja et al.  2017; Schulman et al.  2017; O’Donoghue et al.  2016; Nachum
et al.  2017). These methods embed the standard RL objective  formulated in terms of rewards  into
the framework of probabilistic inference. In contrast  we aim speciﬁcally to reformulate RL in a way
that does not require specifying arbitrary scalar-valued reward functions.
In addition to studying inference problems in a control setting  we also study the problem of learning
event probabilities in these models. This is related to prior work on inverse reinforcement learning
(IRL)  which has also sought to cast learning of objectives into the framework of probabilistic
models (Ziebart et al.  2008; Ziebart  2010). As explained in Section 5  our work generalizes IRL to
cases where we only provide examples of a desired outcome or goal  which is signiﬁcantly easier to
provide in practice since we do not need to know how to achieve the goal.
Reward design is crucial for obtaining the desired behavior from RL agents (Amodei et al.  2016).
Ng & Russell (2000) showed that rewards can be modiﬁed  or shaped  to speed up learning without
changing the optimal policy. Singh et al. (2010) study the problem of optimal reward design  and
introduce the concept of a ﬁtness function. They observe that a proxy reward that is distinct from the
ﬁtness function might be optimal under certain settings  and Sorg et al. (2010) study the problem
of how this optimal proxy reward can be selected. Hadﬁeld-Menell et al. (2017) introduce the
problem of inferring the true objective based on the given reward and MDP. Our framework aids task
speciﬁcation by introducing two decisions: the selection of the inference query that is of interest (i.e. 
when and how many times should the agent cause the event?)  and the speciﬁcation of the event of
interest. Moreover  as discussed in Section 6  we observe that our method automatically provides a
reward shaping effect  allowing us to solve otherwise hard tasks.

3 Preliminaries

In this section we introduce our notation and summarize how control can be framed as infer-
ence. Reinforcement learning operates on Markov decision processes (MDP)  deﬁned by the tuple
(S A T   r  γ  ρ0). S A are the state and action spaces  respectively  r is a reward function  which is
typically taken to be a scalar ﬁeld on S × A  and γ ∈ (0  1) is the discount factor. T and ρ0 represent
the dynamics and initial state distributions  respectively.

3.1 Control as inference

s2

a2

e2

. . .

s1

a1

s3

a3

In order to cast control as an inference problem 
we begin with the standard graphical model for
an MDP  which consists of states and actions.
We incorporate the notion of a goal with an ad-
ditional variable et that depends on the state
(and possibly also the action) at time step t 
according to p(et|st  at).
If the goal is spec-
iﬁed with a reward function  we can deﬁne
p(et = 1|st  at) = er(s a) which  as we dis-
cuss below  leads to a maximum entropy version
of the standard RL framework. This requires the
rewards to be negative  which is not restrictive
in practice  since if the rewards are bounded we
can re-center them so that the maximum value
is 0. The structure of this model is presented in
Figure 3  and is also considered in prior work  as discussed in the previous section.
The maximum entropy reinforcement learning objective emerges when we condition on e1:T = 1.
Consider computing a backward message β(st  at) = p(et:T = 1|st  at). Letting Q(st  at) =
log β(st  at)  notice that the backward messages encode the backup equations

Figure 3: A graphical model framework for control. In
maximum entropy reinforcement learning  we observe
e1:T = 1 and can perform inference on the trajectory to
obtain a policy.

. . .

. . .

sT

aT

eT

e1

e3

Q(st  at) = r(st  at) + log Est+1[eV (st+1)]

V (st) = log

3

(cid:90)

eQ(st a)da .

a∈A

We include the full derivation in Appendix A  which resembles derivations discussed in prior
work (Ziebart et al.  2008). This backup equation corresponds to maximum entropy RL  and is
equivalent to soft Q-learning and causal entropy RL formulations in the special case of deterministic
dynamics (Haarnoja et al.  2017; Schulman et al.  2017). For the case of stochastic dynamics 
maximum-entropy RL is optimistic with respect to the dynamics and produces risk-seeking behavior 
and we refer the reader to Appendix B  which covers a variational derivation of the policy objective
which properly handles stochastic dynamics.

4 Event-based control
In control as inference  we chose log p(et = 1|st  at) = r(s  a) so that the resulting inference problem
matches the maximum entropy reinforcement learning objective. However  we might also ask: what
does the variable et  and its probability  represent? The connection to graphical models lets us
interpret rewards as the log-probability that an event occurs  and the standard approach to reward
design can also be viewed as specifying the probability of some binary event  that we might call an
optimality event. This provides us with an alternative way to think about task speciﬁcation: rather
than using arbitrary scalar ﬁelds as rewards  we can specify the events for which we would like to
maximize the probability of occurrence.
We now outline inference procedures for different types of problems of interest in the graphical
model depicted in Figure 3. In Section 5  we will discuss learning procedures in this graphical
model which allow us to specify objectives from data. The strength of the events framework for
task speciﬁcation lies in both its intuitive interpretation and ﬂexibility: though we can obtain similar
behavior in standard reinforcement learning  it may require considerable reward tuning and changes
to the overall problem statement  including the dynamics. In contrast  events provides a single uniﬁed
framework where the problem parameters remain unchanged  and we simply ask the appropriate
queries. We will discuss:

• ALL query: p(τ|e1:T = 1)  meaning the event should happen at each time step.
• AT query: p(τ|et∗ = 1)  meaning the event should happen at a speciﬁc time t∗.
• ANY query: p(τ|e1 = 1 or e2 = 1 or ... eT = 1) meaning the event should happen on at

least one time step during each trial.

We present two derivations for each query: a conceptually simple one based on maximum entropy
and message passing (see Section 3.1)  and one based on variational inference  (see Appendix B) 
which is more appropriate for stochastic dynamics. The resulting variational objective is of the form:

J(π) = −DKL(π(τ )||p(τ|evidence)) = Es1:T  a1:T ∼q[ ˆQ(s1:T   a1:T ) + H π(·|s1:T )] 

where ˆQ is an empirical Q-value estimator for a trajectory and H π(·|s1:T ) = −(cid:80)T

t=0 log π(at|st)
represents the entropy of the policy. This form of the objective can be used in policy gradient
algorithms  and in special cases can also be written as a recursive backup equation for dynamic
programming algorithms. We directly present our results here  and present more detailed derivations
(including extensions to discounted cases) in Appendices C and D.

4.1 ALL and AT queries

We begin by reviewing the ALL query  when we wish for an agent to trigger an event at every timestep.
This can be useful  for example  when expressing some continuous task such as maintaining some
sort of conﬁguration (such as balancing on a unicycle) or avoiding an adverse outcome  such as not
causing an autonomous car to collide. As covered in Section 3.1  conditioning on the event at all time
steps mathematically corresponds to the same problem as entropy maximizing RL  with the reward
given by log p(et = 1|st  at).
Theorem 4.1 (ALL query). In the ALL query  the message passing update for the Q-value can be
written as:
where Q(st  at) represents the log-message log p(et:T = 1|st  at). The corresponding empirical
Q-value can be written recursively as:

Q(st  at) = log p(et = 1|st  at) + log Est+1 [eV (st+1)] 

ˆQ(st:T   at:T ) = log p(et = 1|st  at) + ˆQ(st+1:T   at+1:T ).

4

Proof. See Appendices C.1 and D.1

The AT query  or querying for the event at a speciﬁc time step  results in the same equations  except
log p(e = 1|st  at)  is only given at the speciﬁed time t∗. While we generally believe that the ANY
query presented in the following section will be more broadly applicable  there may be scenarios
where an agent needs to be in a particular conﬁguration or location at the end of an episode. In these
cases  the AT query would be the most appropriate.

4.2 ANY query

The ANY query speciﬁes that an event should happen at least once before the end of an episode 
without regard for when in particular it takes place. Unlike the ALL and AT queries  the ANY query
does not correspond to entropy maximizing RL and requires a new backup equation. It is also in
many cases more appropriate: if we would like an agent to accomplish some goal  we might not care
when in particular that goal is accomplished  and we likely don’t need it to accomplish it more than
once. This query can be useful for specifying behaviors such as reaching a goal state  completion of a
task  etc. Let the stopping time t∗ = min{t ≥ 0|et = 1} denote the ﬁrst time that the event occurs.
Theorem 4.2 (ANY query). In the ANY query  the message passing update for the Q-value can be
written as:

(cid:16)
(cid:17)
p(et = 1|st  at) + p(et = 0|st  at)Est+1[eV (st+1)]
p(et = 1|st  at) + p(et = 0|st  at)e ˆQ(st+1:T  at+1:T )(cid:17)
(cid:16)

.

Q(st  at) = log

ˆQ(st:T   at:T ) = log

where Q(st  at) represents the log-message log p(t ≤ t∗ ≤ T|st  at). The corresponding empirical
Q-value can be written recursively as:

Proof. See Appendices C.2 and D.2

This query is related to ﬁrst-exit RL problems  where an agent receives a reward of 1 when a speciﬁed
goal is reached and is immediately moved to an absorbing state but it does not require the event
to actually be observed  which makes it applicable to a variety of real-world situations that have
uncertainty over the goal. The backup equations of the ANY query are equivalent to the ﬁrst-exit
problem when p(e|s  a) is deterministic. This can be seen by setting p(e = 1|s  a) = rF (s  a)  where
rF (s  a) is an goal indicator function that denotes the reward of the ﬁrst-exit problem. In this case 
we have Q(s  a) = 0 if the goal is reachable  and Q(s  a) = −∞ if not. In the ﬁrst-exit case  we have
Q(s  a) = 1 if the goal is reachable and Q(s  a) = 0 if not - both cases result in the same policy.

4.3 Sample-based optimization using policy gradients

In small  discrete settings with known dynamics  we can use the backup equations in the previous
section to solve for optimal policies with dynamic programming. For large problems with unknown
dynamics  we can also derive model-free analogues to these methods  and apply them to complex
tasks with high-dimensional function approximators. One commonly used method is the policy
gradient  and which we can derive via logarithmic differentiation as:

∇θJ(θ) = −∇θDKL(πθ(τ )||p(τ|evidence))

(cid:34) T(cid:88)

= Es1:T  a1:T ∼πθ

∇ log πθ(at|st)( ˆQ(s1:T   a1:T ) + H π(·|st:T ))

t=1

Under certain assumptions we can replace ˆQ(s1:T   a1:T ) with ˆQ(st:T   at:T ) to obtain an estimator
which only depends on future returns. See Appendix E for further explanation.
This estimator can be integrated into standard policy gradient algorithms  such as TRPO Schulman
et al. (2015)  to train expressive inference models using neural networks. Extensions of our approach
to other RL methods with function approximation  such as Q-learning and approximate dynamic
programming  can also be derived from the backup equations  though this is outside the scope of the
present work.

5

(cid:35)

Algorithm 1 VICE: Variational Inverse Control with Events
1: Obtain examples of expert states and actions sE
2: Initialize policy π and binary discriminator Dθ.
3: for step n in {1  . . .   N} do
4:
5:
6:
7: end for

Collect states and actions si = (s1  ...  sT )  ai = (a1  ...  aT ) by executing π.
Train Dθ via logistic regression to classify expert data sE
Update π with respect to pθ using the appropriate inference objective.

i   aE
i

i   aE

i from samples si  ai.

5 Learning event probabilities from data

In the previous section  we presented a control framework that operates on events rather than reward
functions  and discussed how the user can choose from among a variety of inference queries to obtain
a desired outcome. However  the event probabilities must still be obtained in some way  and may be
difﬁcult to hand-engineer in many practical situations - for example  an image-based deep RL system
may need an image classiﬁer to determine if it has accomplished its goal. In such situations  we can
ask the user to instead supply examples of states or observations where the event has happened  and
learn the event probabilities pθ(e = 1|s  a). Inverse reinforcement learning corresponds to the case
when we assume the expert triggers an event at all timesteps (the ALL query)  in which case we
require full demonstrations. However  if we assume the expert is optimal under an ANY or AT query 
full demonstrations are not required because the event is not assumed to be triggered at each timestep.
This means our supervision can be of the form of a desired set of states rather than full trajectories.
For example  in the vision-based robotics case  this means that we can specify goals using images of
a desired goal state  which are much easier to obtain than full demonstrations.
Formally  for each query  we assume our dataset of states and actions (s  a) ∼ pdata(s  a|e = 1)
when the event has happened  assuming the data-generating policy follows one of our inference
queries. Our objective is imitation: we wish to train a model which produces samples that match the
data. To that end  we learn the parameters of the model pθ(s  a|e = 1)  trained with the maximum
likelihood objective:

L(θ) = −Epdata [log pθ(s  a|e = 1)]

The gradient of this model is:

∇θL(θ) = −Epdata [∇θ log pθ(s  a|e = 1)] + Epθ [∇θ log pθ(s  a|e = 1)]

(1)
Where the second term corresponds to the gradient of the partition function of pθ(s  a|e = 1). Thus 
this implies an algorithm where we sample states and actions from the model pθ and use them to
compute the gradient update.

5.1 Sample-based optimization with discriminators

In high-dimensional settings  a convenient method to perform the gradient update in Eqn. 1 is to
embed the model pθ(s  a|evidence) within a discriminator between samples pθ and data pdata and
take the gradient of the cross-entropy loss. Second  in order to draw samples from the model we
instead train a "generator" policy via variational inference to draw samples from pθ. The variational
inference procedure corresponds to those outlined in Section 4.
Speciﬁcally  we adapt the method of Fu et al. (2018)  which alternates between training a discriminator
with the ﬁxed form

Dθ(s  a) = pθ(s  a)/(pθ(s  a) + π(a|s))

to distinguish between policy samples and success states  and a policy that minimizes the KL
divergence between DKL(π(s  a)||pθ(s  a| = 1)). As shown in previous work (Finn et al.  2016b; Fu
et al.  2018)  the gradient of the cross entropy loss of the discriminator is equivalent to the gradient
of Eqn. 1  and using the reward log Dθ(s  a) − log(1 − Dθ(s  a)) with the appropriate inference
objective is equivalent to minimizing KL between the sampler and generator. We show the latter
equivalence in Appendix F  and pseudocode for our algorithm is presented in Algorithm 1

6

6 Experimental evaluation

Our experimental evaluation aims to answer the following questions: (1) How does the behavior of an
agent change depending on the choice of query? We study this question in the case where the event
probabilities are already speciﬁed. (2) Does our event learning framework (VICE) outperform simple
alternatives  such as ofﬂine classiﬁer training  when learning event probabilities from data? We study
this question in settings where it is difﬁcult to manually specify a reward function  such as when the
agent receives raw image observations. (3) Does learning event probabilities provide better shaped
rewards than the ground truth event occurrence indicators? Additional videos and supplementary
material are available at https://sites.google.com/view/inverse-event.

6.1

Inference with pre-speciﬁed event probabilities

We ﬁrst demonstrate how the ANY and ALL queries in
our framework result in different behaviors. We adapt
TRPO (Schulman et al.  2015)  a natural policy gradient
algorithm  to train policies using our query procedures
derived in Section 4. Our examples involve two goal-
reaching domains  HalfCheetah and Lobber  shown in
Figure 4: HalfCheetah and Lobber tasks.
Figure 4. The goal of HalfCheetah is to navigate a 6-DoF
agent to a goal position  and in Lobber  a robotic arm must throw an block to a goal position. To study
the inference process in isolation  we manually design the event probabilities as e−||xagent−xtarget||2
for the HalfCheetah and e−(cid:107)xblock−xgoal(cid:107)2 for the Lobber.
The experimental results are shown in Table 1.
While the average distance to the goal for both
queries was roughly the same  the ANY query
results in a much closer minimum distance. This
makes sense  since in the ALL query the agent
is punished for every time step it is not near the
goal. The ANY query can afford to receive lower
cumulative returns and instead has max-seeking
behavior which more accurately reaches the tar-
get. Here  the ANY query better expresses our
intention of reaching a target.

Avg. Dist
1.35 (0.20)
1.33 (0.16)
HalfCheetah-Random 8.95 (5.37)
0.61 (0.12)
0.59 (0.11)
0.93 (0.01)

Table 1: Results on HalfCheetah and Lobber tasks
(5 trials). The ALL query generally results in superior
returns  but the ANY query results in the agent reaching
the target more accurately. Random refers to a random
gaussian policy.

Min. Dist
0.97 (0.46)
2.01 (0.48)
5.41 (2.67)
0.25 (0.20)
0.36 (0.21)
0.91 (0.01)

HalfCheetah-ANY
HalfCheetah-ALL

Lobber-ANY
Lobber-ALL

Lobber-Random

Query

6.2 Learning event probabilities

Query type

Table 2: Results on Maze  Ant and Pusher environments (5 trials).
The metric reported is the ﬁnal distance to the goal state (lower is
better). VICE performs better than the classiﬁer-based setup on all
the tasks  and the performance is substantially better for the Ant
and Pusher task. Detailed learning curves are provided in Appendix
G.

We now compare our event proba-
bility learning framework  which we
call variational inverse control with
events (VICE)  against an ofﬂine clas-
siﬁer training baseline. We also com-
pare our method to learning from
true binary event indicators  to see if
our method can provide some reward
shaping beneﬁts to speed up the learn-
ing process. The data for learning
event probabilities comes from suc-
cess states. That is  we have access to
a set of states {sE
i }i=1...n  which may
have been provided by the user  for which we know the event took place. This setting generalizes
IRL  where instead of entire expert demonstrations  we simply have examples of successful states.
The ofﬂine classiﬁer baseline trains a neural network to distinguish success state ("positives") from
states collected by a random policy. The number of positives and negatives in this procedure is kept
balanced. This baseline is a reasonable and straightforward method to specify rewards in the standard
RL framework  and provides a natural point of comparison to our approach  which can also be viewed
as learning a classiﬁer  but within the principled framework of control as inference. We evaluate these
methods on the following tasks:

VICE (ours)
0.20 (0.19)
0.23 (0.15)
0.64 (0.32)
0.62 (0.55)
0.09 (0.01)
0.11 (0.01)

e ALL
ANY
M
t ALL
n
ANY
A
h ALL
ANY

Classiﬁer
0.35 (0.29)
0.37 (0.21)
2.71 (0.75)
3.93 (1.56)
0.25 (0.01)
0.25 (0.01)

True Binary
0.11 (0.01)

1.61 (1.35)

0.17 (0.03)

z
a

s
u
P

7

Figure 5: Visualizations of the Pusher  Maze  and Ant
tasks. In the Maze and Ant tasks  the agent seeks to
reach a pre-speciﬁed goal position. In the Pusher task 
the agent seeks to place a block at the goal position.

Maze from pixels. In this task  a point mass needs to navigate to a goal location through a small
maze  depicted in Figure 5. The observations consist of 64x64 RGB images that correspond to an
overhead view of the maze. The action space consists of X and Y forces on the robot. We use CNNs
to represent the policy and the event distributions  training with 1000 success states as supervision.
Ant. In this task  a quadrupedal “ant” (shown in Figure 5) needs to crawl to a goal location  placed
3m away from its starting position. The state space contains joint angles and XYZ-coordinates of the
ant. The action space corresponds to joint torques. We use 500 success states as supervision.
Pusher from pixels. In this task  a 7-DoF robotic arm (shown in Figure 5) must push a cylinder
object to a goal location. The state space contains joint angles  joint velocities and 64x64 RGB
images  and the action space corresponds to joint torques. We use 10K success states as supervision.
Training details and neural net architectures can
be found in Appendix G. We also compare our
method against a reinforcement learning base-
line that has access to the true binary event indi-
cator. For all the tasks  we deﬁne a “goal region” 
and give the agent a +1 reward when it is in the
goal region  and 0 otherwise. Note that this RL
baseline  which is similar to vanilla RL from
sparse rewards  “observes” the event  providing
it with additional information  while our model
only uses the event probabilities learned from the success examples and receives no other supervision.
It is included to provide a reference point on the difﬁculty of the tasks. Results are summarized in
Table 2  and detailed learning curves can be seen in Figure 6 and Appendix G. We note the following
salient points from these experiments.
VICE outperforms naïve classiﬁer. We ob-
serve that for Maze  both the simple classiﬁer
and our method (VICE) perform well  though
VICE achieves lower ﬁnal distance. In the Ant
environment  VICE is crucial for obtaining good
performance  and the simple classiﬁer fails to
solve the task. Similarly  for the Pusher task 
VICE signiﬁcantly outperforms the classiﬁer
(which fails to solve the task). Unlike the naïve
classiﬁer approach  VICE actively integrates
negative examples from the current policy into
the learning process  and appropriately models
the event probabilities together with the dynam-
ical properties of the task  analogously to IRL.
Shaping effect of VICE. For the more difﬁcult
ant and pusher domains  VICE actually outper-
forms RL with the true event indicators. We
analyze this shaping effect further in Figure 6: our framework obtains performance that is supe-
rior to learning with true event indicators  while requiring much weaker supervision. This indi-
cates that the event probability distribution learned by our method has a reward-shaping effect 
which greatly simpliﬁes the policy search process. We further compare our method against a hand-
engineered shaped reward  depicted in dashed lines in Figure 6. The engineered reward is given by
−0.2∗(cid:107)xblock − xarm(cid:107)−(cid:107)xblock − xgoal(cid:107)  and is impossible to compute when we don’t have access
to xblock  which is usually the case when learning in the real world. We observe that our method
achieves performance that is comparable to this engineered reward  indicating that our automated
shaping effect is comparable to hand-engineered shaped rewards.

Figure 6: Results on the Pusher task (lower is better) 
averaged across ﬁve random seeds. VICE signiﬁcantly
outperforms the naive classiﬁer and true binary event
indicators. Further  the performance is comparable to
learning from an oracle hand-engineered reward (de-
noted in dashed lines). Curves for the Ant and Maze
tasks can be seen in Appendix G.

7 Conclusion

In this paper  we described how the connection between control and inference can be extended to
derive a reinforcement learning framework that dispenses with the conventional notion of rewards 
and replaces them with events. Events have associated probabilities. which can either be provided

8

0200400600800Iterations0.080.100.120.140.160.180.200.220.240.26Final Distance from GoalBinary IndicatorCLS-ALLCLS-ANYOracleVICE-ALLVICE-ANYby the user  or learned from data. Recasting reinforcement learning into the event-based framework
allows us to express various goals as different inference queries in the corresponding graphical model.
The case where we learn event probabilities corresponds to a generalization of IRL where rather than
assuming access to expert demonstrations  we assume access to states and actions where an event
occurs. IRL corresponds to the case where we assume the event happens at every timestep  and we
extend this notion to alternate graphical model queries where events may happen at a single timestep.

Acknowledgements

This research was supported by an ONR Young Investigator Program award  the National Science
Foundation through IIS-1651843  IIS-1614653  and IIS-1700696  Berkeley DeepDrive  and donations
from Google  Amazon  and NVIDIA.

References
Amodei  Dario  Olah  Chris  Steinhardt  Jacob  Christiano  Paul  Schulman  John  and Mané  Dan.

Concrete problems in AI safety. ArXiv Preprint  abs/1606.06565  2016.

Argall  Brenna D.  Chernova  Sonia  Veloso  Manuela  and Browning  Brett. A survey of robot

learning from demonstration. Robotics and autonomous systems  57(5):469–483  2009.

Finn  C.  Tan  X.  Duan  Y.  Darrell  T.  Levine  S.  and Abbeel  P. Deep spatial autoencoders for

visuomotor learning. In ICRA  2016a.

Finn  Chelsea  Christiano  Paul  Abbeel  Pieter  and Levine  Sergey. A connection between generative
adversarial networks  inverse reinforcement learning  and energy-based models. abs/1611.03852 
2016b.

Fu  Justin  Luo  Katie  and Levine  Sergey. Learning robust rewards with adversarial inverse
reinforcement learning. In International Conference on Learning Representations (ICLR)  2018.

Haarnoja  Tuomas  Tang  Haoran  Abbeel  Pieter  and Levine  Sergey. Reinforcement learning with

deep energy-based policies. In International Conference on Machine Learning (ICML)  2017.

Hadﬁeld-Menell  Dylan  Milli  Smitha  Abbeel  Pieter  Russell  Stuart J.  and Dragan  Anca D. Inverse

reward design. In NIPS  2017.

Ho  Jonathan and Ermon  Stefano. Generative adversarial imitation learning. In Advances in Neural

Information Processing Systems (NIPS)  2016.

Kalman  Rudolf. A new approach to linear ﬁltering and prediction problems. 82:35–45  1960.

Kappen  Hilbert J.  Gomez  Vicenc  and Opper  Manfred. Optimal control as a graphical model

inference problem. 2009.

Levine  Sergey  Finn  Chelsea  Darrell  Trevor  and Abbeel  Pieter. End-to-end training of deep

visuomotor policies. Journal of Machine Learning (JMLR)  2016.

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David  Rusu  Andrei A  Veness  Joel  Bellemare 
Marc G  Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas K  Ostrovski  Georg  Petersen  Stig 
Beattie  Charles  Sadik  Amir  Antonoglou  Ioannis  King  Helen  Kumaran  Dharshan  Wierstra 
Daan  Legg  Shane  and Hassabis  Demis. Human-level control through deep reinforcement
learning. Nature  518(7540):529–533  feb 2015. ISSN 0028-0836.

Nachum  Oﬁr  Norouzi  Mohammad  Xu  Kelvin  and Schuurmans  Dale. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems (NIPS)  2017.

Ng  Andrew and Russell  Stuart. Algorithms for inverse reinforcement learning. In International

Conference on Machine Learning (ICML)  2000.

O’Donoghue  Brendan  Munos  Remi  Kavukcuoglu  Koray  and Mnih  Volodymyr. Combining

policy gradient and q-learning. 2016.

9

Peng  Xue Bin  Andrychowicz  Marcin  Zaremba  Wojciech  and Abbeel  Pieter. Sim-to-real transfer

of robotic control with dynamics randomization. CoRR  abs/1710.06537  2017.

Pinto  Lerrel and Gupta  Abhinav. Supersizing self-supervision: Learning to grasp from 50k tries and

700 robot hours. In IEEE International Conference on Robotics and Automation (ICRA)  2016.

Rawlik  Konrad  Toussaint  Marc  and Vijayakumar  Sethu. On stochastic optimal control and
reinforcement learning by approximate inference. In Robotics: Science and Systems (RSS)  2012.

Russell  Stuart J. and Norvig  Peter. Artiﬁcial Intelligence: A Modern Approach. Pearson Education 

2 edition  2003. ISBN 0137903952.

Rusu  Andrei A.  Vecerik  Matej  Rothörl  Thomas  Heess  Nicolas  Pascanu  Razvan  and Hadsell 
Raia. Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot
Learning (CoRL)  2017.

Schulman  John  Levine  Sergey  Moritz  Philipp  Jordan  Michael I.  and Abbeel  Pieter. Trust Region

Policy Optimization. In International Conference on Machine Learning (ICML)  2015.

Schulman  John  Chen  Xi  and Abbeel  Pieter. Equivalence between policy gradients and soft

q-learning. 2017.

Singh  S.  Lewis  R.  and Barto  A. Where do rewards come from? In Proceedings of the International

Symposium on AI Inspired Biology - A Symposium at the AISB 2010 Convention  2010.

Sorg  Jonathan  Singh  Satinder P.  and Lewis  Richard L. Reward design via online gradient ascent.

In NIPS  2010.

Todorov  Emo. Linearly-solvable markov decision problems. In Advances in Neural Information

Processing Systems (NIPS)  2007.

Todorov  Emo. General duality between optimal control and estimation. In IEEE Conference on

Decision and Control (CDC)  2008.

Toussaint  Marc. Robot trajectory optimization using approximate inference.

Conference on Machine Learning (ICML)  2009.

In International

Tung  Hsiao-Yu Fish  Harley  Adam W.  Huang  Liang-Kang  and Fragkiadaki  Katerina. Reward
learning from narrated demonstrations. In Conference on Computer Vision and Pattern Recognition
(CVPR)  2018.

Ziebart  Brian. Modeling purposeful adaptive behavior with the principle of maximum causal entropy.

PhD thesis  Carnegie Mellon University  2010.

Ziebart  Brian  Maas  Andrew  Bagnell  Andrew  and Dey  Anind. Maximum entropy inverse

reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2008.

10

,Justin Fu
Avi Singh
Dibya Ghosh
Larry Yang
Sergey Levine