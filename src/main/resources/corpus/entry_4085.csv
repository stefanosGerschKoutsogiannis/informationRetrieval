2014,Online combinatorial optimization with stochastic decision sets and adversarial losses,Most work on sequential learning assumes a fixed set of actions that are available all the time. However  in practice  actions can consist of picking subsets of readings from sensors that may break from time to time  road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings  as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally  we evaluate our algorithms empirically and show their improvement over the known approaches.,Online combinatorial optimization with stochastic

decision sets and adversarial losses

Gergely Neu

Michal Valko

SequeL team  INRIA Lille – Nord Europe  France
{gergely.neu michal.valko}@inria.fr

Abstract

Most work on sequential learning assumes a ﬁxed set of actions that are available
all the time. However  in practice  actions can consist of picking subsets of read-
ings from sensors that may break from time to time  road segments that can be
blocked or goods that are out of stock. In this paper we study learning algorithms
that are able to deal with stochastic availability of such unreliable composite ac-
tions. We propose and analyze algorithms based on the Follow-The-Perturbed-
Leader prediction method for several learning settings differing in the feedback
provided to the learner. Our algorithms rely on a novel loss estimation technique
that we call Counting Asleep Times. We deliver regret bounds for our algorithms
for the previously studied full information and (semi-)bandit settings  as well as a
natural middle point between the two that we call the restricted information set-
ting. A special consequence of our results is a signiﬁcant improvement of the best
known performance guarantees achieved by an efﬁcient algorithm for the sleeping
bandit problem with stochastic availability. Finally  we evaluate our algorithms
empirically and show their improvement over the known approaches.

1

Introduction

In online learning problems [4] we aim to sequentially select actions from a given set in order to
optimize some performance measure. However  in many sequential learning problems we have to
deal with situations when some of the actions are not available to be taken. A simple and well-
studied problem where such situations arise is that of sequential routing [8]  where we have to select
every day an itinerary for commuting from home to work so as to minimize the total time spent
driving (or even worse  stuck in a trafﬁc jam). In this scenario  some road segments may be blocked
for maintenance  forcing us to work with the rest of the road network. This problem is isomorphic to
packet routing in ad-hoc computer networks where some links might not be always available because
of a faulty transmitter or a depleted battery. Another important class of sequential decision-making
problems where the decision space might change over time is recommender systems [11]. Here 
some items may be out of stock or some service may not be applicable at some time (e.g.  a movie
not shown that day  bandwidth issues in video streaming services). In these cases  the advertiser may
refrain from recommending unavailable items. Other reasons include a distributor being overloaded
with commands or facing shipment problems.
Learning problems with such partial-availability restrictions have been previously studied in the
framework of prediction with expert advice. Freund et al. [7] considered the problem of online
prediction with specialist experts  where some experts’ predictions might not be available from time
to time  and the goal of the learner is to minimize regret against the best mixture of experts. Kleinberg
et al. [15] proposed a stronger notion of regret measured against the best ranking of experts and gave
efﬁcient algorithms that work under stochastic assumptions on the losses  referring to this setting as
prediction with sleeping experts. They have also introduced the notion of sleeping bandit problems
where the learner only gets partial feedback about its decisions. They gave an inefﬁcient algorithm

1

for the non-stochastic case  with some hints that it might be difﬁcult to learn efﬁciently in this
general setting. This was later reafﬁrmed by Kanade and Steinke [14]  who reduce the problem of
PAC learning of DNF formulas to a non-stochastic sleeping experts problem  proving the hardness
of learning in this setup. Despite these negative results  Kanade et al. [13] have shown that there
is still hope to obtain efﬁcient algorithms in adversarial environments  if one introduces a certain
stochastic a assumption on the decision set.
In this paper  we extend the work of Kanade et al. [13] to combinatorial settings where the action
set of the learner is possibly huge  but has a compact representation. We also assume stochastic
action availability: in each decision period  the decision space is drawn from a ﬁxed but unknown
probability distribution independently of the history of interaction between the learner and the envi-
ronment. The goal of the learner is to minimize the sum of losses associated with its decisions. As
usual in online settings  we measure the performance of the learning algorithm by its regret deﬁned
as the gap between the total loss of the best ﬁxed decision-making policy from a pool of policies
and the total loss of the learner. The choice of this pool  however  is a rather delicate question in our
problem: the usual choice of measuring regret against the best ﬁxed action is meaningless  since not
all actions are available in all time steps. Following Kanade et al. [13] (see also [15])  we consider
the policy space composed of all mappings from decision sets to actions within the respective sets.
We study the above online combinatorial optimization setting under three feedback assumptions.
Besides the full-information and bandit settings considered by Kanade et al. [13]  we also consider a
restricted feedback scheme as a natural middle ground between the two by assuming that the learner
gets to know the losses associated only with available actions. This extension (also studied by [15])
is crucially important in practice  since in most cases it is unrealistic to expect that an unavailable
expert would report its loss. Finally  we also consider a generalization of bandit feedback to the
combinatorial case known as semi-bandit feedback.
Our main contributions in this paper are two algorithms called SLEEPINGCAT and SLEEPINGCAT-
BANDIT that work in the restricted and semi-bandit information schemes  respectively. The best
known competitor of our algorithms is the BSFPL algorithm of Kanade et al. [13] that works in
two phases. First  an initial phase is dedicated to the estimation of the distribution of the available
actions. Then  in the main phase  BSFPL randomly alternates between exploration and exploitation.
Our technique improves over the FPL-based method of Kanade et al. [13] by removing the costly ex-
ploration phase dedicated to estimate the availability probabilities  and also the explicit exploration
steps in their main phase. This is achieved by a cheap alternative loss estimation procedure called
Counting Asleep Times (or CAT) that does not require estimating the distribution of the action sets.
This technique improves the regret bound of [13] after T steps from O(T 4/5) to O(T 2/3) in their
√
setting  and also provides a regret guarantee of O(

T ) in the restricted setting.1

2 Background

We now give the formal deﬁnition of the learning problem. We consider a sequential interaction
scheme between a learner and an environment where in each round t ∈ [T ] = {1  2  . . .   T}  the
learner has to choose an action Vt from a subset St of a known decision set S ⊆ {0  1}d with
(cid:107)v(cid:107)1 ≤ m for all v ∈ S. We assume that the environment selects St according to some ﬁxed (but
unknown) distribution P  independently of the interaction history. Unaware of the learner’s decision 
the environment also decides on a loss vector (cid:96)t ∈ [0  1]d that will determine the loss suffered by the
learner  which is of the form V (cid:62)
t (cid:96)t. We make no assumptions on how the environment generates
the sequence of loss vectors  that is  we are interested in algorithms that work in non-oblivious (or
adaptive) environments. At the end of each round  the learner receives some feedback based on the
loss vector and the action of the learner. The goal of the learner is pick its actions so as to minimize
the losses it accumulates by the end of the T ’th round. This setup generalizes the setting of online
combinatorial optimization considered by Cesa-Bianchi and Lugosi [5]  Audibert et al. [1]  where
the decision set is assumed to be ﬁxed throughout the learning procedure. The interaction protocol
is summarized on Figure 1 for reference.

1While not explicitly proved by Kanade et al. [13]  their technique can be extended to work in the restricted

setting  where it can be shown to guarantee a regret of O(T 3/4).

2

Parameters:
full set of decision vectors S = {0  1}d  number of rounds T   unknown distribution P ∈ ∆2S
For all t = 1  2  . . .   T repeat
1. The environment draws a set of available actions St ∼ P and picks a loss vector
2. The set St is revealed to the learner.
3. Based on its previous observations (and possibly some source of randomness)  the

(cid:96)t ∈ [0  1]d.

learner picks an action Vt ∈ St.

4. The learner suffers loss V (cid:62)

t (cid:96)t and gets some feedback:

(a) in the full information setting  the learner observes (cid:96)t 
(b) in the restricted setting  the learner observes (cid:96)t i for all i ∈ Dt 
(c) in the semi-bandit setting  the learner observes (cid:96)t i for all i such that Vt i = 1.

Figure 1: The protocol of online combinatorial optimization with stochastic action availability.

We distinguish between three different feedback schemes  the simplest one being the full information
scheme where the loss vectors are completely revealed to the learner at the end of each round. In
the restricted-information scheme  we make a much milder assumption that the learner is informed
about the losses of the available actions. Precisely  we deﬁne the set of available components as

Dt = {i ∈ [d] : ∃v ∈ St : vi = 1}

and assume that the learner can observe the i-th component of the loss vector (cid:96)t if and only if i ∈ Dt.
This is a sensible assumption in a number of practical applications  e.g.  in sequential routing prob-
lems where components are associated with links in a network. Finally  in the semi-bandit scheme 
we assume that the learner only observes losses associated with the components of its own decision 
that is  the feedback is (cid:96)t i for all i such that Vt i = 1. This is the case in in online advertising set-
tings where components of the decision vectors represent customer-ad allocations. The observation
history Ft is deﬁned as the sigma-algebra generated by the actions chosen by the learner and the
decision sets handed out by the environment by the end of round t: Ft = σ(Vt St  . . .   V1 S1).
The performance of the learner is measured with respect to the best ﬁxed policy (otherwise known
as a choice function in discrete choice theory [16]) of the form π : 2S → S. In words  a policy
π will pick action π( ¯S) ∈ ¯S whenever the environment selects action set ¯S. The (total expected)
regret of the learner is deﬁned as

T(cid:88)

E(cid:104)

t=1

RT = max

π

(cid:105)

(Vt − π(St))

(cid:62)

(cid:96)t

.

(1)

Note that the above expectation integrates over both the randomness injected by the learner and the
stochastic process generating the decision sets. The attentive reader might notice that this regret
criterion is very similar to that of Kanade et al. [13]  who study the setting of prediction with expert
advice (where m = 1) and measure regret against the best ﬁxed ranking of experts. It is actually
easy to show that the optimal policy in their setting belongs to the set of ranking policies  making
our regret deﬁnition equivalent to theirs.

3 Loss estimation by Counting Asleep Times

In this section  we describe our method used for estimating unobserved losses that works without
having to explicitly learn the availability distribution P. To explain the concept on a high level  let
us now consider our simpler partial-observability setting  the restricted-information setting. For the
formal treatment of the problem  let us ﬁx any component i ∈ [d] and deﬁne At i = 1{i∈Dt} and
ai = E [At i |Ft−1 ]. Had we known the observation probability ai  we would be able to estimate
the i’th component of the loss vector (cid:96)t by ˆ(cid:96)∗
t i = ((cid:96)t iAt i)/ai  as the quantity (cid:96)t iAt i is observable.
It is easy to see that the estimate ˆ(cid:96)∗
t i is unbiased by deﬁnition – but  unfortunately  we do not
know ai  so we have no hope to compute it. A simple idea used by Kanade et al. [13] is to devote

3

((cid:80)T0

the ﬁrst T0 rounds of interaction solely to the purpose of estimating ai by the sample mean ˆai =
t=1 At i)/T0. While this trick gets the job done  it is obviously wasteful as we have to throw

away all loss observations before the estimates are sufﬁciently concentrated. 2
We take a much simpler approach based on the observation that the “asleep-time” of component i
is a geometrically distributed random variable with parameter ai. The asleep-time of component i
starting from time t is formally deﬁned as

Nt i = min{n > 0 : i ∈ Dt+n}  

which is the number of rounds until the next observation of the loss associated with component i.
Using the above deﬁnition  we construct our loss estimates as the vector ˆ(cid:96)t whose i-th component is
(2)

ˆ(cid:96)t i = (cid:96)t iAt iNt i.
It is easy to see that the above loss estimates are unbiased as

E [(cid:96)t iAt iNt i |Ft−1 ] = (cid:96)t iE [At i |Ft−1 ] E [Nt i |Ft−1 ] = (cid:96)t iai · 1
ai

= (cid:96)t i

for any i. We will refer to this loss-estimation method as Counting Asleep Times (CAT).
Looking at the deﬁnition (2)  the attentive reader might worry that the vector ˆ(cid:96)t depends on future
realizations of the random decision sets and thus could be useless for practical use. However  ob-
serve that there is no reason that the learner should use the estimate ˆ(cid:96)t i before component i wakes
up in round t + Nt i – which is precisely the time when the estimate becomes well-deﬁned. This
suggests a very simple implementation of CAT: whenever a component is not available  estimate its
loss by the last observation from that component! More formally  set

(cid:40)

ˆ(cid:96)t i =

if i ∈ Dt
(cid:96)t i 
ˆ(cid:96)t−1 i  otherwise.

It is easy to see that at the beginning of any round t  the two alternative deﬁnitions match for all
components i ∈ Dt. In the next section  we conﬁrm that this property is sufﬁcient for running our
algorithm.

4 Algorithms & their analyses

For all information settings  we base our learning algorithms on the Follow-the-Perturbed-Leader
(FPL) prediction method of Hannan [9]  as popularized by Kalai and Vempala [12]. This algorithm
works by additively perturbing the total estimated loss of each component  and then running an op-
timization oracle over the perturbed losses to choose the next action. More precisely  our algorithms

maintain the cumulative sum of their loss estimates (cid:98)Lt =(cid:80)t
η(cid:98)Lt−1 − Zt

ˆ(cid:96)t and pick the action

vT(cid:16)

(cid:17)

s=1

 

Vt = arg min

v∈St

where Zt is a perturbation vector with independent exponentially distributed components with unit
expectation  generated independently of the history  and η > 0 is a parameter of the algorithm. Our
algorithms for the different information settings will be instances of FPL that employ different loss
estimates suitable for the respective settings. In the ﬁrst part of this section  we present the main
tools of analysis that will be used for each resulting method.
As usual for analyzing FPL-based methods [12  10  18]  we start by deﬁning a hypothetical fore-

caster that uses a time-independent perturbation vector (cid:101)Z with standard exponential components
the decision set: we introduce the time-independent decision set (cid:101)S ∼ P (drawn independently of

and peeks one step into the future. However  we need an extra trick to deal with the randomness of
the ﬁltration (Ft)t) and deﬁne

(cid:101)Vt = arg min
v∈(cid:101)S

vT(cid:16)

η(cid:98)Lt − (cid:101)Z

(cid:17)

.

2Notice that we require “sufﬁcient concentration” from 1/ˆai and not only from ˆai! The deviation of such

quantities is rather difﬁcult to control  as demonstrated by the complicated analysis of Kanade et al. [13].

4

Clearly  this forecaster is infeasible as it uses observations from the future. Also observe that

(cid:101)Vt−1 ∼ Vt given Ft−1. The following two lemmas show how analyzing this forecaster can help in

establishing the performance of our actual algorithms.
Lemma 1. For any sequence of loss estimates  the expected regret of the hypothetical forecaster
against any ﬁxed policy π : 2S → S satisﬁes

E

(cid:34) T(cid:88)

(cid:35)
(cid:16)(cid:101)Vt − π((cid:101)S)
(cid:17)T ˆ(cid:96)t
Lemma 3.1]) and using the upper bound E(cid:2)(cid:13)(cid:13)(cid:101)Z(cid:13)(cid:13)∞
(cid:3) ≤ log d + 1.
(cid:20)(cid:16)(cid:101)V T
(cid:12)(cid:12)(cid:12)Ft−1
(cid:105) ≤ η E

((cid:101)Vt−1 − (cid:101)Vt)T ˆ(cid:96)t

E(cid:104)

t−1

t=1

η

≤ m (log d + 1)

.

(cid:21)

(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12)Ft−1

.

ˆ(cid:96)t

The statement is easily proved by applying the follow-the-leader/be-the-leader lemma3 (see  e.g.  [4 

The following result can be extracted from the proof of Theorem 1 of Neu and Bart´ok [18].
Lemma 2. For any sequence of nonnegative loss estimates 

In the next subsections  we apply these results to obtain bounds for the three information settings.

4.1 Algorithm for full information

In the simplest setting  we can use ˆ(cid:96)t = (cid:96)t  which yields the following theorem:
Theorem 1. Deﬁne

(cid:35)

(cid:41)

(cid:40)

π(St)T(cid:96)t

  4(log d + 1)

.

Setting η =(cid:112)(log d + 1)/L∗

L∗
T = max

E

min

π

(cid:34) T(cid:88)
(cid:113)

t=1

T   the regret of FPL in the full information scheme satisﬁes

RT ≤ 2m

2L∗

T (log d + 1).

As this result is comparable to the best available bounds for FPL [10  18] in the full information
setting and a ﬁxed decision set  it reinforces the observation of Kanade et al. [13]  who show that the
sleeping experts problem with full information and stochastic availability is no more difﬁcult than
the standard experts problem. The proof of Theorem 1 follows directly from combining Lemmas 1
and 2 with some standard tricks. For completeness  details are provided in Appendix A.

4.2 Algorithm for restricted feedback

T(cid:88)

In this section  we use the CAT loss estimate deﬁned in Equation (2) as ˆ(cid:96)t in FPL  and call the
resulting method SLEEPINGCAT. The following theorem gives the performance guarantee for this
algorithm.
E [ Vt i| i ∈ Dt]. The total expected regret of SLEEPINGCAT

Theorem 2. Deﬁne Qt = (cid:80)d

i=1

against the best ﬁxed policy is upper bounded as

RT ≤ m(log d + 1)

+ 2ηm

Qt.

η

t=1

(cid:3) = E [π(St)T(cid:96)t]  where we used that ˆ(cid:96)t is independent of
Proof. We start by observing E(cid:2)π((cid:101)S)T ˆ(cid:96)t
(cid:101)S and is an unbiased estimate of (cid:96)t  and also that St ∼ (cid:101)S. The proof is completed by combining
(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:20)(cid:16)(cid:101)V T

this with Lemmas 1 and 2  and the bound

≤ 2mQt.

The proof of this last statement follows from a tedious calculation that we defer to Appendix B.

3This lemma can be proved in the current case by virtue of the ﬁxed decision set (cid:101)S  allowing the necessary

(cid:21)

t−1

ˆ(cid:96)t

E

recursion steps to go through.

5

Below  we provide two ways of further bounding the regret under various assumptions. The ﬁrst one
provides a universal upper bound that holds without any further assumptions.

Corollary 1. Setting η =(cid:112)(log d + 1)/(2dT )  the regret of SLEEPINGCAT against the best ﬁxed

policy is bounded as

RT ≤ 2m(cid:112)2dT (log d + 1).

√

The proof follows from the fact that Qt ≤ d no matter what P is. A somewhat surprising feature
of our bound is its scaling with
d log d  which is much worse than the logarithmic dependence
exhibited in the full information case. It is easy to see  however  that this bound is not improvable in
general – see Appendix D for a simple example. The next bound  however  shows that it is possible
to improve this bound by assuming that most components are reliable in some sense  which is the
case in many practical settings.

Corollary 2. Assuming ai ≥ β for all i  we have Qt ≤ 1/β  and setting η =(cid:112)β(log d + 1)/(2T )

guarantees that the regret of SLEEPINGCAT against the best ﬁxed policy is bounded as

(cid:115)

RT ≤ 2m

2T (log d + 1)

β

.

4.3 Algorithm for semi-bandit feedback

We now turn our attention to the problem of learning with semi-bandit feedback where the learner
only gets to observe the losses associated with its own decision. Speciﬁcally  we assume that the
learner observes all components i of the loss vector such that Vt i = 1. The extra difﬁculty in this
setting is that our actions inﬂuence the feedback that we receive  so we have to be more careful when
deﬁning our loss estimates. Ideally  we would like to work with unbiased estimates of the form

P( ¯S)E(cid:2)Vt i

(cid:12)(cid:12)Ft−1 St = ¯S(cid:3) .

(3)

(cid:88)

¯S∈2S

ˆ(cid:96)∗
t i =

(cid:96)t i
q∗

t i

Vt i 

where

t i = E [ Vt i|Ft−1] =
q∗

for all i ∈ [d]. Unfortunately though  we are in no position to compute these estimates  as this would
require perfect knowledge of the availability distribution P! Thus we have to look for another way
to compute reliable loss estimates. A possible idea is to use

qt i · ai = E [ Vt i|Ft−1 St] · P [i ∈ Dt] .

t i in Equation 3 to normalize the observed losses. This choice yields another unbiased

instead of q∗
loss estimate as

(cid:20) (cid:96)t iVt i

E

qt iai

(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12)Ft−1

(cid:20)

(cid:20) Vt i

qt i

(cid:12)(cid:12)(cid:12)(cid:12)Ft−1 St

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)Ft−1

(cid:21)

=

E

(cid:96)t i
ai

E

=

(cid:96)t i
ai

E [ At i|Ft−1] = (cid:96)t i 

(4)

which leaves us with the problem of computing qt i and ai. While this also seems to be a tough
challenge  we now show to estimate this quantity by generalizing the CAT technique presented in
Section 3.
Besides our trick used for estimating the 1/ai’s by the random variables Nt i  we now also have to
face the problem of not being able to ﬁnd a closed-form expression for the qt i’s. Hence  we follow
the geometric resampling approach of Neu and Bart´ok [18] and draw an additional sequence of M
perturbation vectors Z(cid:48)

t(M ) and use them to compute

t(1)  . . .   Z(cid:48)
V (cid:48)
t (k) = arg min

(cid:110)
η(cid:98)Lt−1 − Z(cid:48)
Kt i = min(cid:0)(cid:8)k ∈ [M ] : V (cid:48)

t i(k) = Vt i

v∈St

(cid:111)
(cid:9) ∪ {M}(cid:1) .

t(k)

for all k ∈ [M ]. Using these simulated actions  we deﬁne

and

(5)
for all i. Setting M = ∞ makes this expression equivalent to (cid:96)t iVt i
in expectation  yielding yet
another unbiased estimator for the losses. Our analysis  however  crucially relies on setting M to

ˆ(cid:96)t i = (cid:96)t iKt iNt iVt i

qt iai

6

T(cid:88)

t=1

(cid:12)(cid:12)(cid:12)Ft−1
e ·(cid:16)

(cid:16)√

(cid:17)2/3

(cid:17)1/3

a ﬁnite value so as to control the variance of the loss estimates. We are not aware of any other
work that achieves a similar variance-reduction effect without explicitly exploring the action space
[17  6  5  3]  making this alternative bias-variance tradeoff a unique feature of our analysis. We call
the algorithm resulting from using the loss estimates above SLEEPINGCATBANDIT. The following
theorem gives the performance guarantee for this algorithm.

Theorem 3. Deﬁne Qt =(cid:80)d

i=1

DIT against the best ﬁxed policy is bounded as

E [ Vt i| i ∈ Dt]. The total expected regret of SLEEPINGCATBAN-

RT ≤ m(log d + 1)

+ 2ηM m

Qt +

dT
eM

.

η

lar argument that we used in the proof of Theorem 2. After yet another long and tedious calculation
(see Appendix C)  we can prove

(cid:12)(cid:12)Ft−1
Proof. First  observe that E(cid:2)ˆ(cid:96)t i
(cid:3) ≤ (cid:96)t i as E [ Kt iVt i|Ft−1 St] ≤ At i and
E [ At iNt i|Ft−1] = 1 by deﬁnition. Thus  we can get E(cid:2)π((cid:101)S)T ˆ(cid:96)t
(cid:3) ≤ E [π(St)T(cid:96)t] by a simi-
(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:20)(cid:16)(cid:101)V T
t (cid:96)t|Ft−1] ≤ E(cid:104)(cid:101)V T

The proof is concluded by combining this bound with Lemmas 1 and 2 and the upper bound

≤ 2M mQt.

E [ V T

(cid:21)

(cid:105)

t−1

t−1

ˆ(cid:96)t

(6)

ˆ(cid:96)t

E

+

d
eM

 

which can be proved by following the proof of Theorem 1 in Neu and Bart´ok [18].

Corollary 3. Setting η =
2m(log d+1)
regret of SLEEPINGCATBANDIT against the best ﬁxed policy is bounded as

and M = 1√

m(log d+1)

2dT

√

dT

guarantees that the

RT ≤ (2mdT )2/3 · (log d + 1)1/3.

The proof of the corollary follows from bounding Qt ≤ d and plugging the parameters into the
bound of Theorem 3. Similarly to the improvement of Corollary 2  it is possible to replace the factor
d2/3 by (d/β)1/3 if we assume that ai ≥ β for all i and some β > 0.
This corollary implies that SLEEPINGCATBANDIT achieves a regret of (2KT )2/3 · (log K + 1)1/3
in the case when S = [K]  that is  in the K-armed sleeping bandit problem considered by Kanade
et al. [13]. This improves their bound of O((KT )4/5 log T ) by a large margin  thanks to the fact
that we did not have to explicitly learn the distribution P.

5 Experiments

In this section we present the empirical evaluation of our algorithms for bandit and semi-bandit
settings  and compare them to its counterparts [13]. We demonstrate that the wasteful exploration of
BSFPL does not only result in worse regret bounds but also degrades its empirical performance.
For the bandit case  we evaluate SLEEPINGCATBANDIT using the same setting as Kanade et al. [13].
We consider an experiment with T = 10  000 and 5 arms  each of which are available independenly
of each other with probability p. Losses for each arm are constructed as random walks with Gaussian
increments of standard deviation 0.002  initialized uniformly on [0  1]. Losses outside [0  1] are trun-
cated. In our ﬁrst experiment (Figure 2  left)  we study the effect of changing p on the performance
of BSFPL and SLEEPINGCATBANDIT. Notice that when p is very low  there are few or no arms to
choose from. In this case the problems are easy by design and all algorithms suffer low regret. As
p increases  the policy space starts to blow up and the problem becomes more difﬁcult. When p ap-
proaches one  it collapses into the set of single arms and the problem gets easier again. Observe that
the behavior of SLEEPINGCATBANDIT follows this trend. On the other hand  the performance of
BSFPL steadily decreases with increasing availability. This is due to the explicit exploration rounds
in the main phase of BSFPL  that suffers the loss of the uniform policy scaled by the exploration
probability. The performance of the uniform policy is plotted for reference.

7

Figure 2: Left: Multi-arm bandits - varying availabilities. Middle: Shortest paths on a 3 × 3 grid.
Right: Shortest paths on a 10 × 10 grid.

To evaluate SLEEPINGCATBANDIT in the semi-bandit setting  we consider the shortest path prob-
lem on grids of 3 × 3 and 10 × 10 nodes  which amounts to 12 and 180 edges respectively. For
each edge  we generate a random-walk loss sequence in the same way as in our ﬁrst experiment.
In each round t  the learner has to choose a path from the lower left corner to the upper right one
composed from available edges. The individual availability of each edge is sampled with probability
0.9  independently of the others. Whenever an edge gets disconnected from the source  it becomes
unavailable itself  resuling in a quite complicated action-availability distribution. Once a learner
chooses a path  the losses of chosen road segments are revealed and the learner suffers their sum.
Since [13] does not provide a combinatorial version of their approach  we compare against COMBB-
SFPL  a straightforward extension of BSFPL. As in BSFPL  we dedicate an initial phase to esti-
mate the availabilities of each component  requiring d oracle calls per step. In the main phase  we
follow BSFPL and alternate between exploration and exploitation. In exploration rounds  we test
for the reachability of a randomly sampled edge and update the reward estimates as in BSFPL.
Figure 2 (middle and right) shows the performance of COMBBSFPL and SLEEPINGCATBANDIT
for a ﬁxed loss sequence  averaged over 20 samples of the component availabilities. We also plot the
performance of a random policy that follows the perturbed leader with all-zero loss estimates. First
observe that the initial exploration phase sets back the performance of COMBBSFPL signiﬁcantly.
The second drawback of COMBBSFPL is the explicit separation of exploration and the exploitation
rounds. This drawback is far more apparent when the number of components increases  as it is the
case for the 10 × 10 grid graph with 180 components. As COMBBSFPL only estimates the loss of
one edge per exploration step  sampling each edge as few as 50 times eats up 9  000 rounds from
the available 10  000. SLEEPINGCATBANDIT does not suffer from this problem as it uses all its
observations in constructing the loss estimates.

6 Conclusions & future work

In this paper  we studied the problem of online combinatorial optimization with changing decision
sets. Our main contribution is a novel loss-estimation technique that enabled us to prove strong
regret bounds under various partial-feedback schemes. In particular  our results largely improve on
the best known results for the sleeping bandit problem [13]  which suffers large losses from both
from an initial exploration phase and from explicit exploration rounds in the main phase. These
ﬁndings are also supported by our experiments.

Still  one might ask if it is possible to efﬁciently achieve a regret of order
T under semi-bandit
feedback. While the EXP4 algorithm of Auer et al. [2] can be used to obtain such regret guarantee 
running this algorithm is out of question as its time and space complexity can be double-exponential
in d (see also the comments in [15]). Had we had access to the loss estimates (3)  we would be able
√
to control the regret of FPL as the term on the right hand side of Equation (6) could be replaced
by md  which is sufﬁcient for obtaining a regret bound of O(m
dT log d). In fact  it seems that
learning in the bandit setting requires signiﬁcantly more knowledge about P than the knowledge of
the ai’s. The question if we can extend the CAT technique to estimate all the relevant quantities of
P is an interesting problem left for future investigation.

√

Acknowledgements The research presented in this paper was supported by French Ministry
of Higher Education and Research  by European Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no270327 (CompLACS)  and by FUI project Herm`es.

8

availabitycumulative regret at time T = 10000sleeping bandits  5 arms  varying availabity  average over 20 runs0.10.20.30.40.50.60.70.80.9100.050.10.150.20.25BSFPLSleepingCatRandomGuessReferences
[1] Audibert  J. Y.  Bubeck  S.  and Lugosi  G. (2014). Regret in online combinatorial optimization.

Mathematics of Operations Research. to appear.

[2] Auer  P.  Cesa-Bianchi  N.  Freund  Y.  and Schapire  R. E. (2002). The nonstochastic multi-

armed bandit problem. SIAM J. Comput.  32(1):48–77.

[3] Bubeck  S.  Cesa-Bianchi  N.  and Kakade  S. M. (2012). Towards minimax policies for online

linear optimization with bandit feedback. In COLT 2012  pages 1–14.

[4] Cesa-Bianchi  N. and Lugosi  G. (2006). Prediction  Learning  and Games. Cambridge Univer-

sity Press  New York  NY  USA.

[5] Cesa-Bianchi  N. and Lugosi  G. (2012). Combinatorial bandits. Journal of Computer and

System Sciences  78:1404–1422.

[6] Dani  V.  Hayes  T. P.  and Kakade  S. (2008). The price of bandit information for online

optimization. In NIPS-20  pages 345–352.

[7] Freund  Y.  Schapire  R.  Singer  Y.  and Warmuth  M. (1997). Using and combining predictors
that specialize. In Proceedings of the 29th Annual ACM Symposium on the Theory of Computing 
pages 334–343. ACM Press.

[8] Gy¨orgy  A.  Linder  T.  Lugosi  G.  and Ottucs´ak  Gy.. (2007). The on-line shortest path problem

under partial monitoring. Journal of Machine Learning Research  8:2369–2403.

[9] Hannan  J. (1957). Approximation to bayes risk in repeated play. Contributions to the theory of

games  3:97–139.

[10] Hutter  M. and Poland  J. (2004). Prediction with expert advice by following the perturbed

leader for general weights. In ALT  pages 279–293.

[11] Jannach  D.  Zanker  M.  Felfernig  A.  and Friedrich  G. (2010). Recommender Systems: An

Introduction. Cambridge University Press.

[12] Kalai  A. and Vempala  S. (2005). Efﬁcient algorithms for online decision problems. Journal

of Computer and System Sciences  71:291–307.

[13] Kanade  V.  McMahan  H. B.  and Bryan  B. (2009). Sleeping experts and bandits with stochas-

tic action availability and adversarial rewards. In AISTATS 2009  pages 272–279.

[14] Kanade  V. and Steinke  T. (2012). Learning hurdles for sleeping experts. In Proceedings of
the 3rd Innovations in Theoretical Computer Science Conference (ITCS 12)  pages 11–18. ACM.
[15] Kleinberg  R. D.  Niculescu-Mizil  A.  and Sharma  Y. (2008). Regret bounds for sleeping

experts and bandits. In COLT 2008  pages 425–436.

[16] Koshevoy  G. A. (1999). Choice functions and abstract convex geometries. Mathematical

Social Sciences  38(1):35–44.

[17] McMahan  H. B. and Blum  A. (2004). Online geometric optimization in the bandit setting

against an adaptive adversary. In COLT 2004  pages 109–123.

[18] Neu  G. and Bart´ok  G. (2013). An efﬁcient algorithm for learning with semi-bandit feedback.

In ALT 2013  pages 234–248.

9

,Gergely Neu
Michal Valko
Kohei Hayashi
Yuichi Yoshida