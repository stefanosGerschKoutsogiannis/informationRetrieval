2013,Online PCA for Contaminated Data,We consider the online Principal Component Analysis (PCA) for contaminated samples (containing outliers) which are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers  previous online PCA algorithms fail in this case and their results can be arbitrarily bad. Here we propose the online robust PCA algorithm  which is able to improve the PCs estimation upon an initial one steadily  even when faced with a constant fraction of outliers. We show that the final result of the proposed online RPCA has an acceptable degradation from the optimum. Actually  under mild conditions  online RPCA achieves the maximal robustness with a $50\%$ breakdown point. Moreover  online RPCA is shown to be efficient for both storage and computation  since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data.,Online PCA for Contaminated Data

Jiashi Feng

ECE Department

National University of Singapore

jiashi@nus.edu.sg

Huan Xu

ME Department

National University of Singapore

mpexuh@nus.edu.sg

Shie Mannor
EE Department

Technion

shie@ee.technion.ac.il

Shuicheng Yan
ECE Department

National University of Singapore

eleyans@nus.edu.sg

Abstract

We consider the online Principal Component Analysis (PCA) where contaminated
samples (containing outliers) are revealed sequentially to the Principal Compo-
nents (PCs) estimator. Due to their sensitiveness to outliers  previous online PCA
algorithms fail in this case and their results can be arbitrarily skewed by the out-
liers. Here we propose the online robust PCA algorithm  which is able to im-
prove the PCs estimation upon an initial one steadily  even when faced with a
constant fraction of outliers. We show that the ﬁnal result of the proposed online
RPCA has an acceptable degradation from the optimum. Actually  under mild
conditions  online RPCA achieves the maximal robustness with a 50% breakdown
point. Moreover  online RPCA is shown to be efﬁcient for both storage and com-
putation  since it need not re-explore the previous samples as in traditional robust
PCA algorithms. This endows online RPCA with scalability for large scale data.

1

Introduction

In this paper  we investigate the problem of robust Principal Component Analysis (PCA) in an online
fashion. PCA aims to construct a low-dimensional subspace based on a set of principal components
(PCs) to approximate all the observed samples in the least-square sense [19]. Conventionally  it
computes PCs as the eigenvectors of the sample covariance matrix in batch mode  which is both
computationally expensive and in particular memory exhausting  when dealing with large scale data.
To address this problem  several online PCA algorithms have been developed in literature [15  23 
10]. For online PCA  at each time instance  a new sample is revealed  and the PCs estimation is
updated accordingly without having to re-explore all previous samples. Signiﬁcant advantages of
online PCA algorithms include independence of their storage space requirement of the number of
samples  and handling newly revealed samples quite efﬁciently.
Due to the quadratic loss used  PCA is notoriously sensitive to corrupted observations (outliers) 
and the quality of its output can suffer severely in the face of even a few outliers. Therefore  much
work has been dedicated to robustifying PCA [12  2  24  6]. However  all of these methods work
in batch mode and cannot handle sequentially revealed samples in the online learning framework.
For instance  [24] proposed a high-dimensional robust PCA (HR-PCA) algorithm that is based on
iterative performing PCA and randomized removal. Notice that the random removal process involves
calculating the order statistics over all the samples to obtain the removal probability. Therefore  all
samples must be stored in memory throughout the process. This hinders its application to large scale
data  for which storing all data is impractical.

1

In this work  we propose a novel online Robust PCA algorithm to handle contaminated sample set 
i.e.  sample set that comprises both authentic samples (non-corrupted samples) and outliers (cor-
rupted samples)  which are revealed sequentially to the algorithm. Previous online PCA algorithms
generally fail in this case  since they update the PCs estimation through minimizing the quadratic
error w.r.t. every new sample and are thus sensitive to outliers. The outliers may manipulate the PCs
estimation severely and the result can be arbitrarily bad. In contrast  the proposed online RPCA is
shown to be robust to the outliers. This is achieved by a probabilistic admiting/rejection procedure
when a new sample comes. This is different from previous online PCA methods  where each and
every new sample is admitted. The probabilistic admittion/rejection procedure endows online RPCA
with the ability to reject more outliers than authentic samples and thus alleviates the affect of outliers
and robustiﬁes the PCs estimation. Indeed  we show that given a proper initial estimation  online
RPCA is able to steadily improve its output until convergence. We further bound the deviation of the
ﬁnal output from the optimal solution. In fact  under mild conditions  online RPCA can be resistent
to 50% outliers  namely having a 50% breakdown point. This is the maximal robustness that can be
achieved by any method.
Compared with previous robust PCA methods (typically works in batch mode)  online RPCA only
needs to maintain a covariance matrix whose size is independent of the number of data points. Upon
accepting a newly revealed sample  online RPCA updates the PCs estimation accordingly without
re-exploring the previous samples. Thus  online RPCA can deal with large amounts of data with
low storage expense. This is in stark contrast with previous robust PCA methods which typically
requires to remember all samples. To the best of our knowledge  this is the ﬁrst attempt to make
online PCA work for outlier-corrupted data  with theoretical performance guarantees.

2 Related Work

Standard PCA is performed in batch mode  and its high computational complexity may become
cumbersome for the large datasets. To address this issue  different online learning techniques have
been proposed  for example [1  8]  and many others.
Most of current online PCA methods perform the PCs estimation in an incremental manner [8  18 
25]. They maintain a covariance matrix or current PCs estimation  and update it according to the
new sample incrementally. Those methods provide similar PCs estimation accuracy. Recently  a
randomized online PCA algorithm was proposed by [23]  whose objective is to minimize the total
expected quadratic error minus the total error of the batch algorithm (i.e.  the regret). However  none
of these online PCA algorithms is robust to the outliers.
To overcome the sensitiveness of PCA to outliers  many robust PCA algorithms have been pro-
posed [21  4  12]  which can be roughly categorized into two groups. They either pursue robust
estimation of the covariance matrix  e.g.  M-estimator [17]  S-estimator [22]  and Minimum Co-
variance Determinant (MCD) estimator [21]  or directly maximize certain robust estimation of uni-
variate variance for the projected observations [14  3  4  13]. These algorithms inherit the robustness
characteristics of the adopted estimators and are qualitatively robust. However  none of them can
be directly applied in online learning setting. Recently  [24] and the following work [6] propose
high-dimensional robust PCA  which can achieve maximum 50% breakdown point. However  these
methods iteratively remove the observations or tunes the observations weights based on statistics
obtained from the whole data set. Thus  when a new data point is revealed  these methods need to
re-explore all of the data and become quite expensive in computation and in storage.
The most related works to ours are the following two works. In [15]  an incremental and robust
subspace learning method is proposed. The method proposes to integrate the M-estimation into the
standard incremental PCA calculation. Speciﬁcally  each newly coming data point is re-weighted by
a pre-deﬁned inﬂuence function [11] of its residual to the current estimated subspace. However  no
performance guarantee is provided in this work. Moreover  the performance of the proposed algo-
rithm relies on the accuracy of PCs obtained previously. And the error will be cumulated inevitably.
Recently  a compressive sensing based recursive robust PCA algorithm was proposed in [20]. In this
work  the authors focused on the case where the outliers can be modeled as sparse vectors. In con-
trast  we do not impose any structural assumption on the outliers. Moreover  the proposed method
in [20] essentially solves compressive sensing optimization over a small batch of data to update the
PCs estimation instead of using a single sample  and it is not clear how to extend the method to the

2

latter case. Recently  He et al. propose an incremental gradient descent method on Grassmannian
manifold for solving the robust PCA problem  named GRASTA [9]. However  they also focus on a
different case from ours where the outliers are sparse vectors.

3 The Algorithm

3.1 Problem Setup
Given a set of observations {y1 ···   yT} (here T can be ﬁnite or inﬁnite) which are revealed se-
quentially  the goal of online PCA is to estimate and update the principal components (PCs) based on
the newly revealed sample yt at time instance t. Here  the observations are the mixture of authentic
samples (non-corrupted samples) and outliers (corrupted samples). The authentic samples zi ∈ Rp
are generated through a linear mapping: zi = Axi + ni. Noise ni is sampled from normal distribu-
tion N (0  Ip); and the signal xi ∈ Rd are i.i.d. samples of a random variable x with mean zero and
variance Id. Let µ denote the distribution of x. The matrix A ∈ Rp×d and the distribution µ are un-
√
known. We assume µ is absolutely continuous w.r.t. the Borel measure and spherically symmetric.
And µ has light tails  i.e.  there exist constants C > 0 such that Pr((cid:107)x(cid:107) ≥ x) ≤ d exp(1−Cx/α
d)
for all x ≥ 0. The outliers are denoted as oi ∈ Rp and in particular they are deﬁned as follows.
Deﬁnition 1 (Outlier). A sample oi ∈ Rp is an outlier w.r.t. the subspace spanned by {wj}d

deviates from the subspace  i.e. (cid:80)d

j=1 if it

j=1 |wT

j oi|2 ≤ Γo.

In the above deﬁnition  we assume that the basis wj and outliers o are both normalized (see Al-
gorithm 1 step 1)-a) where all the samples are (cid:96)2-normalized). Thus  we directly use inner product
to deﬁne Γo. Namely a sample is called outlier if it is distant from the underlying subspace of the
signal. Apart from this assumption  the outliers are arbitrary. In this work  we are interested in the
case where the outliers are mixed with authentic samples uniformly in the data stream  i.e.  taking
any subset of the dataset  the outlier fraction is identical when the size of the subset is large enough.
to the proposed online RPCA algorithm is the sequence of observations Y =
The input
{y1  y2 ···   yT}  which is the union of authentic samples Z = {zi} generated by the aforemen-
tioned linear model and outliers O = {oi}. The outlier fraction in the observations is denoted as
λ. Online RPCA aims at learning the PCs robustly and the learning process proceeds in time in-
stances. At the time instance t  online RPCA chooses a set of principal components {w(t)
j }d
j=1. The
performance of the estimation is measured by the Expressed Variance (E.V.) [24]:

(cid:80)d
(cid:80)d

T

.

E.V. (cid:44)

j

j=1 w(t)
j=1 wT

AAT w(t)
j
j AAT wj

Here  wj denotes the true principal components of matrix A. The E.V. represents the portion of
signal Ax being expressed by {w(t)
j=1. Thus  1 − E.V. is the reconstruction error of the signal.
j }d
The E.V. is a commonly used evaluation metric for the PCA algorithms [24  5]. It is always less than
or equal to one  with equality achieved by a perfect recovery.

3.2 Online Robust PCA Algorithm

The details of the proposed online RPCA algorithm are shown in Algorithm 1. In the algorithm 
the observation sequence Y = {y1  y2 ···   yT} is sequentially partitioned into (T (cid:48) + 1) batches
{B0  B1  B2  . . .   BT (cid:48)}. Each batch consists of b observations. Since the authentic samples and
outliers are mixed uniformly  the outlier fraction in each batch is also λ. Namely  in each batch Bi 
there are (1 − λ)b authentic samples and λb outliers.
Note that such small batch partition is only for the ease of illustration and analysis. Since the
algorithm only involves standard PCA computation  we can employ any incremental or online PCA
method [8  15] to update the PCs estimation upon accepting a new sample. The maintained sample
covariance matrix  can be set to zero every b time instances. Thus the batch partition is by no means
necessary in practical implementation. In the algorithm  the initial PC estimation can be obtained
through standard PCA or robust PCA [24] on a mini batch of the samples.

3

Algorithm 1 Online Robust PCA Algorithm

Input: Data sequence {y1  . . .   yT}  buffer size b.
Initialization: Partition the data sequence into small batches {B0  B1  . . .   BT (cid:48)}. Each patch
contains b data points. Perform PCA on the ﬁrst batch B0 and obtain the initial principal compo-
j }d
nent {w(0)
j=1.
t = 1. w∗
j = w(0)
while t ≤ T (cid:48) do

 ∀j = 1  . . .   d.

j

1) Initialize the sample covariance matrix: C (t) = 0.
for i = 1 to b do

a) Normalize the data point by its (cid:96)2-norm: y(t)
i
b) Calculate the variance of y(t)

along the direction w(t−1): δi =(cid:80)d

i /(cid:107)y(t)

i (cid:107)(cid:96)2.

:= y(t)

i

j=1

(cid:12)(cid:12)(cid:12)(cid:12)w(t−1)

j

T

y(t)
i

(cid:12)(cid:12)(cid:12)(cid:12)2

.

c) Accept y(t)
d) Scale y(t)
e) If y(t)

i

i

i with probability δi.
√
as y(t)
δi.
i /b

i ← y(t)

is accepted  update C (t) ← C (t) + y(t)

i y(t)

i

T

.

end for
2) Perform eigen-decomposition on Ct and obtain the leading d eigenvector {w(t)
j }d
3) Update the PC as w∗
4) t := t + 1.

 ∀j = 1  . . .   d.

j = w(t)

j

j=1.

end while
Return w∗.

We now explain the intuition of the proposed online RPCA algorithm. Given an initial solution
w(0) which is “closer” to the true PC directions than to the outlier direction 1  the authentic samples
will have larger variance along the current PC direction than outliers. Thus in the probabilistic data
selection process (as shown in Algorithm 1 step b) to step d))  authentic samples are more likely
to be accepted than outliers. Here the step d) of scaling the samples is important for obtaining
an unbiased estimator (see details in the proof of Lemma 4 in supplementary material and [16]).
Therefore  in the following PC updating based on standard PCA on the accepted data  authentic
samples will contribute more than the outliers. The estimated PCs will be “moved” towards to the
true PCs gradually. Such process is repeated until convergence.

4 Main Results

In this section we present the theoretical performance guarantee of the proposed online RPCA al-
gorithm (Algorithm 1). In the sequel  w(t)
is the solution at the t-th time instance. Here without
j
loss of generality we assume the matrix A is normalized  such that the E.V. of the true princi-
j AT Awj = 1. The following theorem provides the performance
guarantee of Algorithm 1 under the noisy case. The performance of w(t) can be measured by

j=1 wT
j A(cid:107)2. Let s = (cid:107)x(cid:107)2/(cid:107)n(cid:107)2 be the signal noise ratio.

pal component wj is(cid:80)d
H(w(t)) (cid:44)(cid:80)d

Theorem 1 (Noisy Case Performance). There exist constants c(cid:48)
2 which depend on the signal
noise ratio s and 1  2 > 0 which approximate zero when s → ∞ or b → ∞  such that if the initial
solution w(0)

in Algorithm 1 satisﬁes:

j=1 (cid:107)w(t)T

1  c(cid:48)

j

λb(cid:88)

d(cid:88)

i=1

j=1

(cid:12)(cid:12)(cid:12)(cid:12)w(0)

j

T

oi

1(1−2)−1)−
(c(cid:48)

(cid:18) 1

(cid:12)(cid:12)(cid:12)(cid:12)2 ≤ (1 − λ)b(1 − 2)
(cid:118)(cid:117)(cid:117)(cid:116) (c(cid:48)

2(1 − Γo)
c(cid:48)

1(1 − ) + 1)2 − 42

4

4

and

H(w(0)) ≥ 1
2

(cid:19)

 

T

1(1 − ) − 1)2 − 2
(c(cid:48)
(cid:80)λb

(cid:80)d

− c(cid:48)

2

i=1

j=1(w(0)

j

(1 − λ)b(1 − 2)

oi)2(1 − Γo)

 

1In the following section  we will provide a precise description of the required closeness.

4

then the performance of the solution from Algorithm 1 will be improved in each iteration  and even-
tually converges to:

t→∞ H(w(t))
lim

≥ 1
2

1(1 − 2) − 1) +
(c(cid:48)

(cid:118)(cid:117)(cid:117)(cid:116) (c(cid:48)

1(1 − 2) − 1)2 − 42

− c(cid:48)

2

4

Here 1 and 2 decay as ˜O(d 1
(1 + 1/s)4.
Remark 1. From Theorem 1  we can observe followings:

2 s−1)   decays as ˜O(d 1

2 b− 1

2 b− 1

2 )  and c(cid:48)

(cid:80)λb

i=1

(cid:80)d

j=1(w(0)

j

T

oi)2(1 − Γo)

(1 − λ)b(1 − 2)
1 = (s − 1)2/(s + 1)2  c(cid:48)

2 =

.

1(1 − 2) − 1)/2 +(cid:112)(c(cid:48)

1. When the outliers vanish  the second term in the square root of performance H(w(t)) is
1(1 − 2) − 1)2 − 42/2 <
1 < 1. Namely  the ﬁnal performance is smaller than but approximates

zero. H(w(t)) will converge to (c(cid:48)
1(1 − 2) − 1 < c(cid:48)
c(cid:48)
1. Here c(cid:48)

1  1  2 explain the affect of noise.

2. When s → ∞  the affect of noise is eliminated  1  2 → 0  c(cid:48)

1 → 1. H(w(t)) converges
to 1 − 2. Here  depends on the ratio of intrinsic dimension over the sample size  and 
accounts for the statistical bias due to performing PCA on a small portion of the data.

3. When the batch size increases to inﬁnity   → 0  H(w(t)) converges to 1  meaning perfect

recovery.

To further investigate the behavior of the proposed online RPCA in presence of outliers  we consider
the following noiseless case. For the noiseless case  the signal noise ratio s → ∞  and thus c(cid:48)
2 →
1 and 1  2 → 0. Then we can immediately obtain the performance bound of Algorithm 1 for the
noiseless case from Theorem 1.
Theorem 2 (Noiseless Case Performance). Suppose there is no noise. If the initial solution w(0) in
Algorithm 1 satisﬁes:

1  c(cid:48)

λb(cid:88)

j=1

d(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1
(cid:118)(cid:117)(cid:117)(cid:116) 1

4

+

−

4

and

i=1

H(w(0)) ≥ 1
2

−

(w(0)T

 

j oi)2 ≤ (1 − λ)b
4(1 − Γo)
(cid:80)λb

(cid:80)d

j=1(w(0)

i=1

T

j

(1 − λ)b

oi)2(1 − Γo)

 

then the performance of the solution from Algorithm 1 will be improved in each updating and even-
tually converges to:

t→∞ H(w(t)) ≥ 1

lim

2

(cid:80)λb

i=1

(cid:80)d

−

j=1(w(0)

j

(1 − λ)b

T

oi)2(1 − Γo)

.

Remark 2. Observe from Theorem 2 the followings:

(cid:80)d

1. When the outliers are distributed on the groundtruth subspace  i.e. (cid:80)d
conditions become(cid:80)λb
2. When the outliers are orthogonal to the groundtruth subspace  i.e. (cid:80)d
the conditions for the initial solution becomes(cid:80)λb

initial solution  the ﬁnal performance will converge to 1.

(cid:80)d
j=1 |w(0)T

j=1(w(0)T

i=1

i=1

j oi|2 = 1  the
oi)2 < ∞ and H(w(0)) ≥ 0. Namely  for whatever

j=1 |wT

j oi|2 = 0 
j oi|2 ≤ b(1 − λ)/4  and
oi)2/(1 − λ)b. Hence  when the outlier fraction

j=1 |wT

(cid:114)
1/4 −(cid:80)λb

(cid:80)d

H0 ≥ 1/2 −
λ increases  the initial solution should be further away from outliers.

j=1(w(0)

i=1

T

j

5

3. When 0 < (cid:80)d
1/4 −(cid:80)λb

(cid:114)

(cid:80)d
j oi|2 < 1  the performance of online RPCA is improved by at
j=1 |wT
oi)2(1 − Γo)/(1 − λ)b from its initial solution. Hence 
j=1(w(0)
least 2
when the initial solution is further away from the outliers  the outlier fraction is smaller  or
the outliers are closer to groundtruth subspace  the improvement is more signiﬁcant. More-
over  observe that given a proper initial solution  even if λ = 0.5  the performance of online
RPCA still has a positive lower bound. Therefore  the breakdown point of online RPCA is
50%  the highest that any algorithm can achieve.

i=1

T

j

Discussion on the initial condition In Theorem 1 and Theorem 2  a mild condition is imposed on
the initial estimate. In practice  the initial estimate can be obtained by applying batch RPCA [6] or
HRPCA [24] on a small subset of the data. These batch methods are able to provide initial estimate
with performance guarantee  which may satisfy the initial condition.

5 Proof of The Results

We brieﬂy explain the proof of Theorem 1: we ﬁrst show that when the PCs estimation is being
improved  the variance of outliers along the PCs will keep decreasing. Then we demonstrate that
each PCs updating conducted by Algorithm 1 produces a better PCs estimation and decreases the
impact of outliers. Such improvement will continue until convergence  and the ﬁnal performance
has bounded deviation from the optimum.
We provide here some concentration lemmas which are used in the proof of Theorem 1. The proof
of these lemmas is provided in the supplementary material. We ﬁrst show that with high probability 
both the largest and smallest eigenvalues of the signals xi in the original space converge to 1. This
result is adopted from [24].
Lemma 1. There exists a constant c that only depends on µ and d  such that for all γ > 0 and b
signals {xi}b

i=1  the following holds with high probability:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

b

b(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤  

(wT xi)2 − 1

(cid:113)

d log3 b/b.

where  = cα

sup
w∈Sd

Next lemma is about the sampling process in the Algorithm 1 from step b) to step d). Though the
sampling process is without replacement and the sampled observations are not i.i.d.  the following
lemma provides the concentration of the sampled observations.
Lemma 2 (Operator-Bernstein inequality [7]). Let {z(cid:48)
i=1  which is
formed by randomly sampling without replacement from Z  as in Algorithm 1. Then the following
statement holds

i}m
i=1 be a subset of Z = {zi}t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

i=1

(cid:32) m(cid:88)

i=1

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ

wT z(cid:48)

i − E

wT z(cid:48)

i

with probability larger than 1 − 2 exp(−δ2/4m).

Given the result in Lemma 1   we obtain that the authentic samples concentration properties as stated
in the following lemma [24].
Lemma 3. If there exists  such that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤  

sup
w∈Sd

|wT xi|2 − 1

t(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

t

i=1

6

and the observations zi are normalized by (cid:96)2-norm  then for any w1 ···   wd ∈ Sp  the following
holds:

(1 − )H(w) − 2(cid:112)(1 + )H(w)/s
t(cid:88)
d(cid:88)
≤ 1
t
j A(cid:107)2 and s is the signal noise ratio.
j=1 (cid:107)wT

j zi)2 ≤ (1 + )H(w) + 2(cid:112)(1 + )H(w)/s + 1/s2

(1/s − 1)2

(1/s + 1)2

(wT

j=1

i=1

 

where H(w) =(cid:80)d

Based on Lemma 2 and Lemma 3  we obtain the following concentration results for the selected
observations in the Algorithm 1.
Lemma 4. If there exists  such that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

t

t(cid:88)

i=1

sup
w∈Sd

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤  

|wT xi|2 − 1

− δ

≤ 1
t

then for any

i=1 as in Algorithm 1 

and the observations {z(cid:48)
i}m
i=1 are sampled from {zi}d
w1  . . .   wd ∈ Sp  with large probability  the following holds:

(1 − )H(w) − 2(cid:112)(1 + )H(w)/s
d(cid:88)
t(cid:88)
where H(w) (cid:44)(cid:80)d
j A(cid:107)2  s is the signal noise ratio and m is the number of sampled obser-
j=1 (cid:107)wT

i)2 ≤ (1 + )H(w) + 2(cid:112)(1 + )H(w)/s + 1/s2

(1/s + 1)2b/m
j z(cid:48)
(wT

(1/s − 1)2b/m

vations in each batch and δ > 0 is a small constant.
We denote the set of accepted authentic samples as Zt and the set of accepted outliers as Ot from the
t-th small batch. In the following lemma  we provide the estimation of number of accepted authentic
samples |Zt| and outliers |Ot|.
Lemma 5. For the current obtained principal components {w(t−1)
authentic samples |Zt| and outliers |Ot| satisfy

}d
j=1  the number of the accepted

+ δ 

j=1

i=1

j

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|Zt|

b

(1−λ)b(cid:88)

d(cid:88)

i=1

j=1

− 1
b

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ and

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|Ot|

b

λb(cid:88)

d(cid:88)

i=1

j=1

− 1
b

(w(t−1)

j

T

zi)2

(w(t−1)

j

T

oi)2

with probability at least 1 − e−2δ2b. Here δ > 0 is a small constant  λ is the outlier fraction and b
is the size of the small batch.

From the above lemma  we can see that when the batch size b is sufﬁciently large  the above estima-
tion for |Zt| and |Ot| holds with large probability. In the following lemma  we show that when the
algorithm improves the PCs estimation  the impact of outliers will be decreased accordingly.
Lemma 6. For an outlier oi  an arbitrary orthogonal basis {wj}d
{wj}d

j oi and(cid:80)d
j oi is a monotonically decreasing function of(cid:80)d

j=1 and the groundtruth basis
j oi  the
j=1 wT
j=1 wT

j=1 which satisfy that(cid:80)d

j wj ≥ (cid:80)d

j oi ≥ (cid:80)d

value of(cid:80)d

j=1 wT

j=1 wT

j=1 wT

j=1 wT

j wj.

Being equipped by the above lemmas  we can proceed to prove Theorem 1. The details of the proof
is deferred to the supplementary material due to the space limit.

6 Simulations

The numerical study is aimed to illustrate the performance of online robust PCA algorithm. We
follow the data generation method in [24] to randomly generate a p × d matrix A and then scale its

7

leading singular value to s  which is the signal noise ratio. A λ fraction of outliers are generated.
Since it is hard to determine the most adversarial outlier distribution  in simulations  we generate
the outliers concentrate on several directions deviating from the groundtruth subspace. This makes a
rather adversarial case and is suitable for investigating the robustness of the proposed online RPCA
algorithm. In the simulations  in total T = 10  000 samples are generated to form the sample se-
quence. For each parameter setting  we report the average result of 20 tests and standard deviation.
The initial solution is obtained by performing batch HRPCA [24] on the ﬁrst batch. Simulation
results for p = 100  d = 1  s = 2 and outliers distributed on one direction are shown in Figure 1. It
takes around 0.5 seconds for the proposed online RPCA to process 10  000 samples of 100 dimen-
sional  on a PC with Quad CPU with 2.83GHz and RAM of 8GB. In contrast  HRPCA costs 237
seconds to achieve E.V. = 0.99. More simulation results for the d > 1 case are provided in the
supplementary material due to the space limit.
From the results  we can make the following observations. Firstly  online RPCA can improve the PC
estimation steadily. With more samples being revealed  the E.V. of the online RPCA outputs keep
increasing. Secondly  the performance of online RPCA is rather robust to outliers. For example  the
ﬁnal result converges to E.V. ≈ 0.95 (HRPCA achieves 0.99) even with λ = 0.3 for relatively low
signal noise ratio s = 2 as shown in Figure 1. To more clearly demonstrate the robustness of online
RPCA to outliers  we implement the online PCA proposed in [23] as baseline for the σ = 2 case.
The results are presented in Figure 1  from which we can observe that the performance of online
PCA drops due to the sensitiveness to newly coming outliers. When the outlier fraction λ ≥ 0.1  the
online PCA cannot recover the true PC directions and the performance is as low as 0.

Figure 1: Performance comparison of online RPCA (blue line) with online PCA (red line). Here
s = 2  p = 100  T = 10  000  d = 1. The outliers are distributed on a single direction.
7 Conclusions

In this work  we proposed an online robust PCA (online RPCA) algorithm for samples corrupted
by outliers. The online RPCA alternates between standard PCA for updating PCs and probabilistic
selection of the new samples which alleviates the impact of outliers. Theoretical analysis showed
that the online RPCA could improve the PC estimation steadily and provided results with bounded
deviation from the optimum. To the best of our knowledge  this is the ﬁrst work to investigate such
online robust PCA problem with theoretical performance guarantee. The proposed online robust
PCA algorithm can be applied to handle challenges imposed by the modern big data analysis.

Acknowledgement

J. Feng and S. Yan are supported by the Singapore National Research Foundation under its Inter-
national Research Centre @Singapore Funding Initiative and administered by the IDM Programme
Ofﬁce. H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier
Two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133. S. Mannor is partially
supported by the Israel Science Foundation (under grant 920/12) and by the Intel Collaborative
Research Institute for Computational Intelligence (ICRI-CI).

8

0102030405000.20.40.60.81 λ= 0.01# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.03# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.05# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.08# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.10# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.15# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.20# batchesE.V. Online RPCAOnline PCA0102030405000.20.40.60.81 λ= 0.30# batchesE.V. Online RPCAOnline PCAReferences
[1] J.R. Bunch and C.P. Nielsen. Updating the singular value decomposition. Numerische Mathe-

matik  1978.

[2] E.J. Candes  X. Li  Y. Ma  and J. Wright.

ArXiv:0912.3599  2009.

Robust principal component analysis?

[3] C. Croux and A. Ruiz-Gazen. A fast algorithm for robust principal components based on

projection pursuit. In COMPSTAT  1996.

[4] C. Croux and A. Ruiz-Gazen. High breakdown estimators for principal components:

projection-pursuit approach revisited. Journal of Multivariate Analysis  2005.

the

[5] A. d’Aspremont  F. Bach  and L. Ghaoui. Optimal solutions for sparse principal component

analysis. JMLR  2008.

[6] J. Feng  H. Xu  and S. Yan. Robust PCA in high-dimension: A deterministic approach. In

ICML  2012.

[7] David Gross and Vincent Nesme. Note on sampling without replacing from a ﬁnite collection

of matrices. arXiv preprint arXiv:1001.2738  2010.

[8] P. Hall  D. Marshall  and R. Martin. Merging and splitting eigenspace models. TPAMI  2000.
[9] Jun He  Laura Balzano  and John Lui. Online robust subspace tracking from partial informa-

tion. arXiv preprint arXiv:1109.3827  2011.

[10] P. Honeine. Online kernel principal component analysis: a reduced-order model. TPAMI 

2012.

[11] P.J. Huber  E. Ronchetti  and MyiLibrary. Robust statistics. John Wiley & Sons  New York 

1981.

[12] M. Hubert  P.J. Rousseeuw  and K.V. Branden. Robpca: a new approach to robust principal

component analysis. Technometrics  2005.

[13] M. Hubert  P.J. Rousseeuw  and S. Verboven. A fast method for robust principal components
with applications to chemometrics. Chemometrics and Intelligent Laboratory Systems  2002.
[14] G. Li and Z. Chen. Projection-pursuit approach to robust dispersion matrices and principal
components: primary theory and monte carlo. Journal of the American Statistical Association 
1985.

[15] Y. Li. On incremental and robust subspace learning. Pattern recognition  2004.
[16] Michael W Mahoney. Randomized algorithms for matrices and data.

arXiv preprint

arXiv:1104.5557  2011.

[17] R.A. Maronna. Robust m-estimators of multivariate location and scatter. The annals of statis-

tics  1976.

[18] S. Ozawa  S. Pang  and N. Kasabov. A modiﬁed incremental principal component analysis for

on-line learning of feature space and classiﬁer. PRICAI  2004.

[19] K. Pearson. On lines and planes of closest ﬁt to systems of points in space. Philosophical

Magazine  1901.

[20] C. Qiu  N. Vaswani  and L. Hogben. Recursive robust pca or recursive sparse recovery in large

but structured noise. arXiv preprint arXiv:1211.3754  2012.

[21] P.J. Rousseeuw. Least median of squares regression. Journal of the American statistical asso-

ciation  1984.

[22] P.J. Rousseeuw and A.M. Leroy. Robust regression and outlier detection. John Wiley & Sons

Inc  1987.

[23] M.K. Warmuth and D. Kuzmin. Randomized online pca algorithms with regret bounds that are

logarithmic in the dimension. JMLR  2008.

[24] H. Xu  C. Caramanis  and S. Mannor. Principal component analysis with contaminated data:

The high dimensional case. In COLT  2010.

[25] H. Zhao  P.C. Yuen  and J.T. Kwok. A novel incremental principal component analysis and its

application for face recognition. TSMC-B  2006.

9

,Jiashi Feng
Huan Xu
Shie Mannor
Shuicheng Yan
Robert Nishihara
Stefanie Jegelka
Michael Jordan