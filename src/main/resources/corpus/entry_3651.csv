2010,Multitask Learning without Label Correspondences,We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels  and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces for the purpose of classification  such as integrating Yahoo! and DMOZ web directories.,Multitask Learning without Label Correspondences

Novi Quadrianto1  Alex Smola2  Tib´erio Caetano1  S.V.N. Vishwanathan3  James Petterson1

1 SML-NICTA & RSISE-ANU  Canberra  ACT  Australia

2 Yahoo! Research  Santa Clara  CA  USA

3 Purdue University  West Lafayette  IN  USA

Abstract

We propose an algorithm to perform multitask learning where each task has poten-
tially distinct label sets and label correspondences are not readily available. This is
in contrast with existing methods which either assume that the label sets shared by
different tasks are the same or that there exists a label mapping oracle. Our method
directly maximizes the mutual information among the labels  and we show that the
resulting objective function can be efﬁciently optimized using existing algorithms.
Our proposed approach has a direct application for data integration with different
label spaces  such as integrating Yahoo! and DMOZ web directories.

1

Introduction

In machine learning it is widely known that if several tasks are related  then learning them simulta-
neously can improve performance [1–4]. For instance  a personalized spam classiﬁer trained with
data from several different users is likely to be more accurate than one that is trained with data from
a single user. If one views learning as the task of inferring a function f from the input space X to the
output space Y  then multitask learning is the problem of inferring several functions fi : Xi (cid:55)→ Yi
simultaneously. Traditionally  one either assumes that the set of labels Yi for all the tasks are the
same (that is  Yi = Y for all i)  or that we have access to an oracle mapping function gi j : Yi (cid:55)→ Yj.
However  as we argue below  in many natural settings these assumptions are not satisﬁed.
Our motivating example is the problem of learning to automatically categorize objects on the web
into an ontology or directory. It is well established that many web-related objects such as web direc-
tories and RSS directories admit a (hierarchical) categorization  and web directories aim to do this
in a semi-automated fashion. For instance  it is desirable  when building a categorizer for the Yahoo!
directory1  to take into account other web directories such as DMOZ2. Although the tasks are clearly
related  their label sets are not identical. For instance  some section heading and sub-headings may
be named differently in the two directories. Furthermore  different editors may have made differ-
ent decisions about the ontology depth and structure  leading to incompatibilities. To make matters
worse  these ontologies evolve with time and certain topic labels may die naturally due to lack of
interest or expertise while other new topic labels may be added to the directory. Given the large label
space  it is unrealistic to expect that a label mapping function is readily available. However  the two
tasks are clearly related and learning them simultaneously is likely to improve performance.
This paper presents a method to learn classiﬁers from a collection of related tasks or data sets  in
which each task has its own label dictionary  without constructing an explicit label mapping among
them. We formulate the problem as that of maximizing mutual information among the labels sets.
We then show that this maximization problem yields an objective function which can be written
as a difference of concave functions. By exploiting convex duality [5]  we can solve the resulting
optimization problem efﬁciently in the dual space using existing DC programming algorithms [6].

1http://dir.yahoo.com/
2http://www.dmoz.org/

1

Related Work As described earlier  our work is closely related to the research efforts on multitask
learning  where the problem of simultaneously learning multiple related tasks is addressed. Several
papers have empirically and theoretically highlighted the beneﬁts of multitask learning over single-
task learning when the tasks are related. There are several approaches to deﬁne task relatedness.
The works of [2  7  8] consider the setting when the tasks to be learned jointly share a common
subset of features. This can be achieved by adding a mixed-norm regularization term that favors a
common sparsity proﬁle in features shared by all tasks. Task relatedness can also be modeled as
learning functions that are close to each other in some sense [3  9]. Crammer et al. [10] consider the
setting where  in addition to multiple sources of data  estimates of the dissimilarities between these
sources are also available. There is also work on data integration via multitask learning where each
data source has the same binary label space  whereas the attributes of the inputs can admit different
orderings as well as be linearly transformed [11].
The remainder of the paper is organized as follows. We brieﬂy develop background on the maximum
entropy estimation problem and its dual in Section 2. We introduce in Section 3 the novel multi-
task formulation in terms of a mutual information maximization criterion. Section 4 presents the
algorithm to solve the optimization problem posed by the multitask problem. We then present the
experimental results  including applications on news articles and web directories data integration  in
Section 5. Finally  in Section 6 we conclude the paper.

2 Maximum Entropy Duality for Conditional Distributions

in Section 4. Recall the deﬁnition of the Shannon entropy  H(y|x) := −(cid:80)
Here we brieﬂy summarize the well known duality relation between approximate conditional maxi-
mum entropy estimation and maximum a posteriori estimation (MAP) [5  12]. We will exploit this
y p(y|x) log p(y|x) 
where p(y|x) is a conditional distribution on the space of labels Y. Let x ∈ X and assume the
existence of φ(x  y) : X × Y (cid:55)→ H  a feature map into a Hilbert space H. Given a data set
(X  Y ) := {(x1  y1)   . . .   (xm  ym)}  where X := {x1  . . .   xm}  deﬁne
m(cid:88)

m(cid:88)

Ey∼p(y|X) [φ(X  y)] :=

Ey∼p(y|xi) [φ(xi  y)]   and µ =

1
m

i=1

φ(xi  yi).

(1)

1
m

i=1

Lemma 1 ([5]  Lemma 6) With the above notation we have

−H(y|xi) s.t. (cid:13)(cid:13)Ey∼p(y|X) [φ(X  y)] − µ(cid:13)(cid:13)H ≤  and (cid:88)
m(cid:88)
(cid:104)θ  µ(cid:105)H − m(cid:88)

exp((cid:104)θ  φ(xi  y)(cid:105)) − (cid:107)θ(cid:107)H .

log(cid:88)

y∈Y

i=1

min
p(y|x)

= max

θ

i=1

y

p(y|xi) = 1

(2a)

(2b)

Although we presented a version of the above theorem using Hilbert spaces  it can also be extended
to Banach spaces. Choosing different Banach space norms recovers well known algorithms such as
(cid:96)1 or (cid:96)2 regularized logistic regression. Also note that by enforcing the moment matching constraint
exactly  that is  setting  = 0  we recover the well-known duality between maximum (Shannon)
entropy and maximum likelihood (ML) estimation.

3 Multitask Learning via Mutual Information

For the purpose of explaining our basic idea  we focus on the case when we want to integrate two
data sources such as Yahoo! directory and DMOZ. Associated with each data source are labels
Y = {y1  . . .   yc} ⊆ Y and observations X = {x1  . . .   xm} ⊆ X (resp. Y (cid:48) = {y(cid:48)
c(cid:48)} ⊆ Y(cid:48)
and X(cid:48) = {x(cid:48)
m(cid:48)} ⊆ X(cid:48)). The observations are disjoint but we assume that they are drawn
from the same domain  i.e.  X = X(cid:48) (in our running example they are webpages).
If we are interested to solve each of the categorization tasks independently  a maximum entropy
estimator described in Section 2 can be readily employed [13]. Here we would like to learn the

1  . . .   x(cid:48)

1  . . .   y(cid:48)

2

two tasks simultaneously in order to improve classiﬁcation accuracy. Assuming that the labels are
different yet correlated we should assume that the joint distribution p(y  y(cid:48)) displays high mutual
information between y and y(cid:48). Recall that the mutual information between random variables y and
y(cid:48) is deﬁned as I(y  y(cid:48)) = H(y) + H(y(cid:48)) − H(y  y(cid:48))  and that this quantity is high when the two
variables are mutually dependent. To illustrate this  consider in our running example of integrating
Yahoo! and DMOZ web directories  we would expect there is a high mutual dependency between
section heading ‘Computer & Internet’ at Yahoo! directory and ‘Computers’ at DMOZ directory
although they are named somewhat slightly different. Since the marginal distributions over the
labels  p(y) and p(y(cid:48)) are ﬁxed  maximizing mutual information can then be viewed as minimizing
the joint entropy

p(y  y(cid:48)) log p(y  y(cid:48)).

(3)

H(y  y(cid:48)) = −(cid:88)

y y(cid:48)

This reasoning leads us to adding the joint entropy as an additional term for the objective function
of the multitask problem. If we deﬁne

m(cid:88)

i=1

m(cid:48)(cid:88)

i=1

then we have the following objective function

maximize

p(y|x)

i) − λH(y  y(cid:48)) for some λ > 0

µ =

1
m

φ(xi  yi) and µ(cid:48) =

1
m(cid:48)

φ(x(cid:48)

i  y(cid:48)
i) 

m(cid:48)(cid:88)

H(y(cid:48)|x(cid:48)

H(y|xi) +

m(cid:88)
s.t. (cid:13)(cid:13)Ey∼p(y|X) [φ(X  y)] − µ(cid:13)(cid:13) ≤  and (cid:88)
(cid:13)(cid:13)Ey(cid:48)∼p(y(cid:48)|X(cid:48)) [φ(cid:48)(X(cid:48)  y(cid:48))] − µ(cid:48)(cid:13)(cid:13) ≤ (cid:48) and (cid:88)

y∈Y

i=1

i=1

p(y|xi) = 1

p(y(cid:48)|x(cid:48)

i) = 1.

y(cid:48)∈Y(cid:48)

(4)

(5a)

(5b)

(5c)

Intuitively  the above objective function tries to ﬁnd a ‘simple’ distribution p which is consistent
with the observed samples via moment matching constraints while also taking into account task
relatedness. We can recover the single task maximum entropy estimator by removing the joint
entropy term (by setting λ = 0)  since the optimization problem (the objective functions as well
as the constraints) in (5) will be decoupled in terms of p(y|x) and p(y(cid:48)|x(cid:48)). There are two main
challenges in solving (5):

• The joint entropy term H(y  y(cid:48)) is concave  hence the above objective of the optimization
problem is not concave in general (it is the difference of two concave functions). We there-
fore propose to solve this non-concave problem using DC programming [6]  in particular
the concave convex procedure (CCCP) [14  15].
• The joint distribution between labels p(y  y(cid:48)) is unknown. We will estimate this quan-
tity (therefore the joint entropy quantity) from the observations x and x(cid:48). Further  we
assume that y and y(cid:48) are conditionally independent given an arbitrary input x ∈ X  that is
p(y  y(cid:48)|x) = p(y|x)p(y(cid:48)|x). For instance  in our example  annotations made by an editor
at Yahoo! and an editor at DMOZ on the set of webpages are assumed conditionally in-
dependent given the set of webpages. This assumption essentially means that the labeling
process depends entirely on the set of webpages  i.e.  any other latent factors that might
connect the two editors are ignored.

In the following section we discuss in further detail how to address these two challenges  as well
as the resulting optimization problem obtained  which can be solved efﬁciently by existing convex
solvers.

4 Optimization
The concave convex procedure (CCCP) works as follow: for a given function f(x) = g(x) − h(x) 
where g is concave and −h is convex  a lower bound can be found by
f(x) ≥ g(x) − h(x0) − (cid:104)∂h(x0)  x − x0(cid:105) .

(6)

3

This lower bound is concave and can be maximized effectively over a convex domain. Subsequently
one ﬁnds a new location x0 and the entire procedure is repeated. This procedure is guaranteed to
converge to a local optimum or saddle point [16].
Therefore  one potential approach to solve the optimization problem in (5) is to use successive linear
lower bounds on H(y  y(cid:48)) and to solve the resulting decoupled problems in p(y|x) and p(y(cid:48)|x(cid:48))
separately. We estimate the joint entropy term H(y  y(cid:48)) by its empirical quantity on x and x(cid:48) with
the conditional independence assumption (in the sequel  we make the dependency of p(y|x) on a
parameter θ explicit and similarly for the dependency of p(y(cid:48)|x(cid:48)) on θ(cid:48))  that is

H(y  y(cid:48)|X) = −(cid:88)

y y(cid:48)

(cid:34)

m(cid:88)

i=1

1
m

p(y|xi  θ)p(y(cid:48)|xi  θ(cid:48))

log

p(y|xj  θ)p(y(cid:48)|xj  θ(cid:48))

(7)

(cid:35)

 1

m

m(cid:88)

j=1

  

1 + log

 p(y(cid:48)|xi  θ(cid:48)

and similarly for H(y  y(cid:48)|X(cid:48)). Each iteration of CCCP approximates the convex part (negative joint
entropy) by its tangent  that is (cid:104)∂h(x0)  x(cid:105) in (6). Therefore  taking derivatives of the joint entropy
with respect to p(y|xi) and evaluating at parameters at iteration t − 1  denoted as θt−1 and θ(cid:48)
t−1 
yields

=

j=1

(cid:34)

1
m

t−1)

t−1).

min
p(y|x)

(8)

(9)

(cid:88)

y

(cid:88)

y(cid:48)

−H(y|xi) + λ

p(y|xj  θt−1)p(y(cid:48)|xj  θ(cid:48)

1
m
Deﬁne similarly gy(x(cid:48)
i)  gy(cid:48)(xi)  and gy(cid:48)(x(cid:48)
p(y(cid:48)|x(cid:48)
optimization problems in p(y|xi) and an analogous problem in p(y(cid:48)|x(cid:48)
i):

gy(xi) := −∂p(y|xi)H(y  y(cid:48)|X)
m(cid:88)
i) for the derivative with respect to p(y|x(cid:48)
(cid:34)
(cid:35)
m(cid:88)
m(cid:48)(cid:88)
subject to (cid:13)(cid:13)Ey∼p(y|X)[φ(X  y)] − µ(cid:13)(cid:13) ≤ .

i)  p(y(cid:48)|xi) and
i)  respectively. This leads  by optimizing the lower bound in (6)  to the following decoupled

(10b)
The above objective function is still in the form of maximum entropy estimation  with the lineariza-
tion of the joint entropy quantities acting like additional evidence terms. Furthermore  we also
impose an additional maximum entropy requirement on the ‘off-set’ observations p(y|x(cid:48)
i)  as after
all we also want the ‘simplicity’ requirement of the distribution p on the input x(cid:48)
i. We can of course
weigh the requirement on ‘off-set’ observations differently.
While we succeed in reducing the non-concave objective function in (5) to a decoupled concave ob-
jective function in (10)  it might be desirable to solve the problem in the dual space due to difﬁculty
in handling the constraint in (10b). The following lemma shows the duality of the objective function
in (10). The proof is given in the supplementary material.

i) + λ(cid:48)(cid:88)

gy(x(cid:48)

i)p(y|x(cid:48)
i)

gy(xi)p(y|xi)

−H(y|x(cid:48)

+

i=1

(10a)

(cid:35)

i=1

y

Lemma 2 The corresponding Fenchel’s dual of (10) is

min

θ

m(cid:88)

i=1

− 1
m

log(cid:88)
m(cid:88)

y

i=1

exp((cid:104)θ  φ(xi  y)(cid:105) − λgy(xi)) +

(cid:104)θ  φ(xi  yi)(cid:105) + (cid:107)θ(cid:107)(cid:96)2

m(cid:48)(cid:88)

log(cid:88)

exp((cid:104)θ  φ(x(cid:48)

i  y)(cid:105) − λ(cid:48)gy(x(cid:48)
i))

i=1

y

(11)

The above dual problem still has the form of logistic regression with the additional evidence terms
from task relatedness appearing in the log-partition function. Several existing convex solvers can be
used to solve the optimization problem in (11) efﬁciently. Refer to Algorithm 1 for a pseudocode of
our proposed method.

Initialization For each iteration of CCCP  the linearization part of the joint entropy function re-
quires the value of θ and θ(cid:48) at the previous iteration (refer to (9)). At the beginning of the iteration 
we can start the algorithm with a uniform prior  i.e. set p(y) = 1/|Y| and p(y(cid:48)) = 1/|Y(cid:48)|.

4

Input: Datasets (X  Y ) and (X(cid:48)  Y (cid:48)) with Y (cid:54)= Y(cid:48)  number of iterations N
Output: θ  θ(cid:48)
Initialize p(y) = 1/|Y| and p(y(cid:48)) = 1/|Y(cid:48)|
for t = 1 to N do

Solve the dual problem in (11) w.r.t. p(y|x  θ) and obtain θt
Solve the dual problem in (11) w.r.t. p(y(cid:48)|x(cid:48)  θ(cid:48)) and obtain θ(cid:48)

t

Algorithm 1 Multitask Mutual Information

end for
return θ ← θN   θ(cid:48) ← θ(cid:48)

N

5 Experiments
To assess the performance of our proposed multitask algorithm  we perform binary n-task (n ∈
{3  5  7  10}) experiments on MNIST digit dataset and a multiclass 2-task experiment on the
Reuters1-v2 dataset plus an application on integrating Yahoo! and DMOZ web directory. We detail
those experiments in turn in the following sections.

5.1 MNIST
Datasets MNIST data set3 consists of 28 × 28-size images of hand-written digits from 0 through
9. We use a small sample of the available training set to simulate the situation when we only have
limited number of labeled examples and test the performance on the entire available test set. In this
experiment  we look at a binary n-task (n ∈ {3  5  7  10}) problem. We consider digits {8  9  0} 
{6  7  8  9  0}  {4  5  6  7  8  9  0} and {1  2  3  4  5  6  7  8  9  0} for the 3-task  5-task  7-task and 10-
task  respectively. To simulate the problem that we have distinct label dictionaries for each task 
we consider the following setting: in the 3-task problem  the ﬁrst task has binary labels {+1 −1} 
where label +1 means digit 8 and label −1 means digit 9 and 0; in the second task  label +1 means
digit 9 and label −1 means digit 8 and 0; lastly in the third task  label +1 means digit 0 and label
−1 means digit 8 and 9. Similar one-against-rest grouping is also used for 5-task  7-task and 10-task
problems. Each of the tasks has its own input x.

Algorithms We couldn’t ﬁnd in the literature of multitask learning methods addressing the same
problem as the one we study: learn multiple tasks when there is no correspondence between the
output spaces. Therefore we compared the performance of our multitask method against the baseline
given by the maximum entropy estimator applied to each of the tasks independently. Note that
we focus on the setting in which data sources have disjoint sets of covariate observations (vide
Section 3) and thus a simple strategy of multilabel prediction with union of label sets corresponds
to our baseline. For both ours and the baseline method  we use a Gaussian kernel to deﬁne the
implicit feature map on the inputs. The width of the kernel was set to the median between pairs
of observations  as suggested in [17]. The regularization parameter was tuned for the single task
estimator and the same value was used for the multitask. The weight on the joint entropy term was
set to be equal to 1.

we can then deﬁne the joint entropy as H(y  y(cid:48)  y(cid:48)(cid:48)) = −(cid:80)

Pairwise Label Correlation Section 3 describes the multitask objective function for the case of
the 2-task problem. For the case when the number of tasks to be learned jointly is greater than 2  we
experiment in two different ways: in one approach we can deﬁne the joint entropy term on the full
joint distribution  that is when we want to learn jointly 3 different tasks having label y  y(cid:48) and y(cid:48)(cid:48) 
y y(cid:48) y(cid:48)(cid:48) p(y  y(cid:48)  y(cid:48)(cid:48)) log p(y  y(cid:48)  y(cid:48)(cid:48)). As
more computationally efﬁcient way  we can consider the joint entropy on the pairwise distribution
instead. We found that the performance of our method is quite similar for the two cases and we
report results only on the pairwise case.

Results The experiments are repeated for 10 times and the results are summarized in Table 1. We
ﬁnd that  on average  jointly learning the multiple related tasks always improves the classiﬁcation

3http://yann.lecun.com/exdb/mnist

5

Table 1: Performance assessment  Accuracy ± STD. m(m(cid:48)) denotes the number of training data
points (number of test points). STL: single task learning; MTL: multi task learning and Upper
Bound: multi class learning. Boldface indicates a signiﬁcance difference between STL and MTL
(one-sided paired Welch t-test with 99.95% conﬁdence level).

Tasks
8 \-8
9 \-9
0 \-0
Average
6 \-6
7 \-7
8 \-8
9 \-9
0 \-0
Average
4 \-4
5 \-5
6 \-6
7 \-7
8 \-8
9 \-9
0 \-0
Average
1 \-1
2 \-2
3 \-3
4 \-4
5 \-5
6 \-6
7 \-7
8 \-8
9 \-9
0 \-0
Average

m (m’)
15 (2963)
15 (2963)
120 (2963)

25 (4949)
25 (4949)
25 (4949)
25 (4949)
150 (4949)

70 (6823)
70 (6823)
70 (6823)
70 (6823)
70 (6823)
70 (6823)
210 (6823)

100 (10000)
100 (10000)
100 (10000)
100 (10000)
100 (10000)
100 (10000)
100 (10000)
100 (10000)
100 (10000)
300 (10000)

STL
77.39±5.23
91.12±5.94
98.66±0.67
89.06
81.79±10.18
70.73±16.58
62.52±10.15
63.80±13.70
97.35±1.33
75.84
71.69±6.83
67.55±4.70
86.31±2.93
83.34±3.54
75.61±6.00
63.69±11.42
97.20±1.49
77.91
96.59±2.11
67.77±3.49
72.59±5.90
69.91±5.82
53.78±2.78
79.22±5.21
76.57±10.2
63.57±2.65
63.28±6.69
98.43±0.84
74.17

80.03±4.83
91.96±5.42
98.21±0.92
90.07
83.86±9.51
72.84±15.77
66.77±9.43
67.26±12.65
96.60±1.64
77.47
73.49±6.77
70.10±4.61
87.21±2.77
84.02±3.69
76.97±5.12
65.74±10.15
96.56±1.67
79.16
96.80±1.91
69.95±2.68
74.18±5.54
71.76±5.47
57.26±2.72
80.54±4.53
77.18±9.43
65.85±2.50
65.38±6.09
97.81±1.01
75.67

MTL Upper Bound
93.42±0.87
95.99±0.75
98.79±0.25
96.07
96.37±1.06
91.99±2.23
92.05±1.76
92.53±1.65
97.59±0.62
94.10
91.20±1.55
89.30±0.34
94.03±0.95
91.94±0.90
87.46±1.69
86.89±1.79
97.24±0.73
91.15
96.89±0.59
88.74±1.94
87.59±2.95
92.87±0.94
85.71±1.38
92.93±0.98
89.83±1.24
83.51±0.63
84.94±1.45
98.49±0.40
90.82

accuracy. When assessing the performance on each of the tasks  we notice that the advantage of
learning jointly is particularly signiﬁcant for those tasks with smaller number of observations.

5.2 Ontology

News Ontologies
In this experiment  we consider multiclass learning in a 2-task problem. We
use the Reuters1-v2 news article dataset [18] which has been pre-processed4. In the pre-processing
stage  the label hierarchy is reorganized by mapping the data set to the second level of topic hier-
archy. The documents that only have labels of the third or fourth levels are mapped to their parent
category of the second level. The documents that only have labels of the ﬁrst level are not mapped
onto any category. Lastly any multi-labelled instances are removed. The second level hierarchy
consists of 53 categories and we perform experiments on the top 10 categories. TF-IDF features are
used  and the dictionary size (feature dimension) is 47236. For this experiment  we use 12500 news
articles to form one set of data and another 12500 news article to form the second set of data. In the
ﬁrst set  we group the news articles having the label {1  2}  {3  4}  {5  6}  {7  8} and {9  10} and
re-label it as {1  2  3  4  5}. For the second set of data  it also has 5 labels but this time the labels are

4http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/multiclass.html

6

Table 2: Yahoo! Top Level Categorization Results. STL: single task learning accuracy; MTL:
multi task learning accuracy; % Imp.: relative performance improvement. The highest relative
improvement at Yahoo! is for the topic of ‘Computer & Internet’  i.e. there is an increase in accuracy
from 48.12% to 52.57%. Interestingly  DMOZ has a similar topic but was called ‘Computers’ and it
achieves accuracy of 75.72%.
Topic

MTL/STL (% Imp.) Topic

MTL/STL (% Imp.)

Arts
Business & Economy
Computer & Internet
Education
Entertainment
Government
Health

56.27/55.11
66.52/66.88
52.57/48.12
62.48/63.02
63.30/61.37
24.44/22.88
85.42/85.27

(2.10)
(-0.53)
(9.25)
(-0.85)
(3.14)
(6.82)
(1.76)

News & Media
Recreation
Reference
Regional
Science
Social Science
Society & Culture

15.23/14.83
68.81/67.00
26.65/24.81
62.85/61.86
78.58/79.75
31.55/30.68
49.51/49.05

(1.03)
(2.70)
(7.42)
(1.60)
(-1.46)
(2.84)
(0.94)

Table 3: DMOZ Top Level Categorization Results. STL: single task learning accuracy; MTL:
multi task learning accuracy; % Imp.: relative performance improvement. The improvement of
multitask to single task on each topic is negligible for DMOZ web directories. Arguably  this can be
partly explained as DMOZ has higher average topic categorization accuracy than Yahoo! and there
might be more knowledge to be shared from DMOZ to Yahoo! than vice versa.

Topic

MTL/STL (% Imp.) Topic

MTL/STL (% Imp.)

Arts
Business
Computers
Games
Health
Home
News
Recreation

57.52/57.84
54.02/53.05
75.08/75.72
78.58/78.58
82.34/82.55
67.47/67.47
61.70/62.01
58.04/58.25

(-0.5)
(1.83)
(-0.8)
(0)
(-0.14)
(0)
(-0.49)
(-0.36)

Reference
Regional
Science
Shopping
Society
Sports
World

67.42/67.42
28.59/28.56
42.67/42.09
75.20/74.62
57.68/58.20
83.49/83.53
87.80/87.57

(0)
(0.10)
(1.38)
(0.54)
(-0.89)
(-0.05)
(0.26)

generated by {1  6}  {2  7}  {3  8}  {4  9} and {5  10} grouping. We split equally the news articles
on each set to form training and test sets. We run a maximum entropy estimator independently 
p(y|x  θ) and p(y(cid:48)|x(cid:48)  θ(cid:48))   on the two sets achieving accuracy of 92.59% for the ﬁrst set and 91.53%
for the second set. We then learn the two sets of the news articles jointly and in the ﬁrst test set 
we achieve accuracy of 93.81%. For the second test set  we achieve an accuracy of 93.31%. This
experiment further emphasizes that it is possible to learn several related tasks simultaneously even
though they have different label sets and it is beneﬁcial to do so.

Web Ontologies We also perform an experiment on the data integration of Yahoo! and DMOZ
web directories. We consider the top level of the Yahoo!’s topic tree and sample web links listed in
the directory. Similarly we also consider the top level of the DMOZ topic tree and retrieve sampled
web links. We consider the content of the ﬁrst page of each web link as our input data. It is possible
that the ﬁrst page that is being linked from the web directory contain mostly images (for the purpose
of attracting visitors)  thus we only consider those webpages that have enough texts to be a valid
input. This gives us 19186 webpages for Yahoo! and 35270 for DMOZ. For the sake of getting
enough texts associated with each link  we can actually crawl many more pages associated with the
link. However  we ﬁnd that it is quite damaging to do so because as we crawl deeper the topic of the
texts are rapidly changing. We use the standard bag-of-words representation with TF-IDF weighting
as our features. The dictionary size (feature dimension) is 27075. We then use 2000 web pages from
Yahoo! and 2000 pages from DMOZ as training sets and the remainder as test sets. Table 2 and 3
summarize the experimental results.

7

From the experimental results on web directories integration  we observe the following:

• Similarly to the experiments on MNIST digits and Reuters1-v2 news articles  multitask
learning always helps on average  i.e. the average relative improvements are positive for
both Yahoo! and DMOZ web directories;
• The improvement of multitask to single task on each topic is more prominent for Yahoo!
web directories and is negligible for DMOZ web directories (2.62% and 0.07%  respec-
tively). Arguably  this can be partly explained as Yahoo! has lower average topic catego-
rization accuracy than DMOZ (c.f. 60.22% and 64.68 %  respectively). It seems that there
is much more knowledge to be shared from DMOZ to Yahoo! in the hope to increase the
latter’s classiﬁcation accuracies;
• Looking closely at accuracy at each topic  the highest relative improvement at Yahoo! is
for the topic of ‘Computer & Internet’  i.e. there is an increase in accuracy from 48.12%
to 52.57%. Interestingly  DMOZ has a similar topic but was called ‘Computers’ and it
achieves accuracy of 75.72%. The improvement might be partly because our proposed
method is able to discover the implicit label correlations despite the two topics being named
differently;
• Regarding the worst classiﬁed categories  we have ‘News & Media’ for Yahoo! and ‘Re-
gional’ for DMOZ. This is intuitive since those two topics can indeed cover a wide range
of subjects. The easiest category to be classiﬁed is ‘Health’ for Yahoo! and ‘World’ for
DMOZ. As well  this is quite intuitive as the world of health contains mostly speciﬁc jargon
and the world of world has much language-speciﬁc webpage content.

6 Discussion and Conclusion

We presented a method to learn classiﬁers from a collection of related tasks or data sets  in which
each task has its own label set. Our method works without the need of an explicit mapping between
the label spaces of the different tasks. We formulate the problem as one of maximizing the mutual
information among the label sets. Our experiments on binary n-task (n ∈ {3  5  7  10}) and mul-
ticlass 2-task problems revealed that  on average  jointly learning the multiple related tasks  albeit
with different label sets  always improves the classiﬁcation accuracy. We also provided experiments
on a prototypical application of our method: classifying in Yahoo! and DMOZ web directories.
Here we deliberately used small amounts of data–a common situation in commercial tagging and
classiﬁcation. This shows that classiﬁcation accuracy of Yahoo! signiﬁcantly increased. Given that
DMOZ classiﬁcation was already 4.5% better prior to the application of our method  this shows
the method was able to transfer classiﬁcation accuracy from the DMOZ task to the Yahoo! task.
Furthermore  the experiments seem to suggest that our proposed method is able to discover implicit
label correlations despite the lack of label correspondences.
Although the experiments on web directories integration is encouraging  we have clearly only
touched the surface of possibilities to be explored. While we focused on the categorization at the
top level of the topic tree  it might be beneﬁcial (and further highlight the usefulness of multitask
learning  as observed in [2–4  9]) to consider categorization at deeper levels (take for example the
second level of the tree)  where we have much fewer observations for each category. In the extreme
case  we might consider the labels as corresponding to a directed acyclic graph (DAG) and encode
the feature map associated with the label hierarchy accordingly. One instance as considered in [19]
is to use a feature map φ(y) ∈ Rk for k nodes in the DAG (excluding the root node) and associate
with every label y the vector describing the path from the root node to y  ignoring the root node
itself.
Furthermore  the application of data integration which admit a hierarchical categorization goes be-
yond web related objects. With our method  it is also now possible to learn classiﬁers from a collec-
tion of related gene-ontology graphs [20] or patent hierarchies [19].

Acknowledgments NICTA is funded by the Australian Government as represented by the Depart-
ment of Broadband  Communications and the Digital Economy and the Australian Research Council
through the ICT Centre of Excellence program. N. Quadrianto is partly supported by Microsoft Re-
search Asia Fellowship.

8

References
[1] R. Caruana. Multitask learning. Machine Learning  28:41–75  1997.
[2] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Mach. Learn.  73(3):243–272  2008.

[3] Kai Yu  Volker Tresp  and Anton Schwaighofer. Learning gaussian processes from multiple
tasks. In ICML ’05: Proceedings of the 22nd international conference on Machine learning 
pages 1012–1019  New York  NY  USA  2005. ACM.

[4] Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from mul-

tiple tasks and unlabeled data. Journal of Machine Learning Research  6:1817–1853  2005.

[5] Y. Altun and A.J. Smola. Unifying divergence minimization and statistical inference via convex
duality. In H.U. Simon and G. Lugosi  editors  Proc. Annual Conf. Computational Learning
Theory  LNCS  pages 139–153. Springer  2006.

[6] T. Pham Dinh and L. Hoai An. A D.C. optimization algorithm for solving the trust-region

subproblem. SIAM Journal on Optimization  8(2):476–505  1988.

[7] G. Obozinski  B. Taskar  and M. I. Jordan. Multi-task feature selection. Technical report  U.C.

Berkeley  2007.

[8] Remi Flamary  Alain Rakotomamonjy  Gilles Gasso  and Stephane Canu. Svm multi-task

learning and non convex sparsity measure. In The Learning Workshop  2009.

[9] Theodoros Evgeniou  Charles A. Micchelli  and Massimiliano Pontil. Learning multiple tasks

with kernel methods. J. Mach. Learn. Res.  6:615–637  2005.

[10] K. Crammer  M. Kearns  and J. Wortman. Learning from multiple sources. In NIPS 19  pages

321–328. MIT Press  2007.

[11] Shai Ben-David  Johannes Gehrke  and Reba Schuller. A theoretical framework for learning
from a pool of disparate data sources. In KDD ’02: Proceedings of the 8th ACM international
conference on Knowledge discovery and data mining  pages 443–449. ACM  2002.

[12] M. Dud´ık and R. E. Schapire. Maximum entropy distribution estimation with generalized reg-
ularization. In G´abor Lugosi and Hans U. Simon  editors  Proc. Annual Conf. Computational
Learning Theory. Springer Verlag  June 2006.

[13] Nadia Ghamrawi and Andrew McCallum. Collective multi-label classiﬁcation. In CIKM ’05:
Proceedings of the 14th ACM international conference on Information and knowledge man-
agement  pages 195–200  New York  NY  USA  2005. ACM.

[14] A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation  15:915–

936  2003.

[15] A. J. Smola  S. V. N. Vishwanathan  and T. Hofmann. Kernel methods for missing variables. In
R.G. Cowell and Z. Ghahramani  editors  Proceedings of International Workshop on Artiﬁcial
Intelligence and Statistics  pages 325–332  2005.

[16] Bharath Sriperumbudur and Gert Lanckriet. On the convergence of the concave-convex pro-
cedure. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors 
Advances in Neural Information Processing Systems 22  pages 1759–1767. MIT Press  2009.
[17] B. Sch¨olkopf. Support Vector Learning. R. Oldenbourg Verlag  Munich  1997. Download:

http://www.kernel-machines.org.

[18] David D. Lewis  Yiming Yang  Tony G. Rose  and Fan Li. RCV1: A new benchmark collection
for text categorization research. The Journal of Machine Learning Research  5:361–397  2004.
[19] Lijuan Cai and T. Hofmann. Hierarchical document categorization with support vector ma-
chines. In Proceedings of the Thirteenth ACM conference on Information and knowledge man-
agement  pages 78–87  New York  NY  USA  2004. ACM Press.

[20] M. Ashburner  C. A. Ball  J. A. Blake  D. Botstein  H. Butler  J. M. Cherry  A. P. Davis 
K. Dolinski  S. S. Dwight  J. T. Eppig  M. A. Harris  D. P. Hill  L. Issel-Tarver  A. Kasarskis 
S. Lewis  J. C. Matese  J. E. Richardson  M. Ringwald  G. M. Rubin  and G. Sherlock. Gene
ontology: tool for the uniﬁcation of biology. the gene ontology consortium. Nat Genet  25:25–
29  2000.

[21] J. M. Borwein and Q. J. Zhu. Techniques of Variational Analysis. CMS books in Mathematics.

Canadian Mathematical Society  2005.

9

,Changbo Zhu
Huan Xu
Chenlei Leng
Shuicheng Yan
Ricardo Silva
Edouard Pauwels
Francis Bach
Jean-Philippe Vert