2013,Integrated Non-Factorized Variational Inference,We present a non-factorized variational method for full posterior inference in Bayesian hierarchical models   with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation. Our approach unifies the integrated nested Laplace approximation (INLA) under the variational framework. The proposed method is applicable in more challenging scenarios than typically assumed by INLA   such as Bayesian Lasso   which is characterized by the non-differentiability of the $\ell_{1}$ norm arising from independent Laplace priors. We derive an upper bound for the Kullback-Leibler divergence   which yields a fast closed-form solution via decoupled optimization. Our method is a reliable analytic alternative to Markov chain Monte Carlo (MCMC)  and it results in a tighter evidence lower bound than that of mean-field variational Bayes (VB) method.,Integrated Non-Factorized Variational Inference

Shaobo Han

shaobo.han@duke.edu

Duke University

Durham  NC 27708

Xuejun Liao
Duke University

Durham  NC 27708
xjliao@duke.edu

Lawrence Carin
Duke University

Durham  NC 27708
lcarin@duke.edu

Abstract

We present a non-factorized variational method for full posterior inference in
Bayesian hierarchical models  with the goal of capturing the posterior variable de-
pendencies via efﬁcient and possibly parallel computation. Our approach uniﬁes
the integrated nested Laplace approximation (INLA) under the variational frame-
work. The proposed method is applicable in more challenging scenarios than typ-
ically assumed by INLA  such as Bayesian Lasso  which is characterized by the
non-differentiability of the (cid:96)1 norm arising from independent Laplace priors. We
derive an upper bound for the Kullback-Leibler divergence  which yields a fast
closed-form solution via decoupled optimization. Our method is a reliable ana-
lytic alternative to Markov chain Monte Carlo (MCMC)  and it results in a tighter
evidence lower bound than that of mean-ﬁeld variational Bayes (VB) method.

1

Introduction

Markov chain Monte Carlo (MCMC) methods [1] have been dominant tools for posterior analysis in
Bayesian inference. Although MCMC can provide numerical representations of the exact posterior 
they usually require intensive runs and are therefore time consuming. Moreover  assessment of
a chain’s convergence is a well-known challenge [2]. There have been many efforts dedicated to
developing deterministic alternatives  including the Laplace approximation [3]  variational methods
[4]  and expectation propagation (EP) [5]. These methods each have their merits and drawbacks [6].
More recently  the integrated nested Laplace approximation (INLA) [7] has emerged as an encourag-
ing method for full posterior inference  which achieves computational accuracy and speed by taking
advantage of a (typically) low-dimensional hyper-parameter space  to perform efﬁcient numerical
integration and parallel computation on a discrete grid. However  the Gaussian assumption for the
latent process prevents INLA from being applied to more general models outside of the family of
latent Gaussian models (LGMs).
In the machine learning community  variational inference has received signiﬁcant use as an efﬁcient
alternative to MCMC. It is also attractive because it provides a closed-form lower bound to the
model evidence. An active area of research has been focused on developing more efﬁcient and
accurate variational inference algorithms  for example  collapsed inference [8  9]  non-conjugate
models [10  11]  multimodal posteriors [12]  and fast convergent methods [13  14].
The goal of this paper is to develop a reliable and efﬁcient deterministic inference method  to both
achieve the accuracy of MCMC and retain its inferential ﬂexibility. We present a promising varia-
tional inference method without requiring the widely used factorized approximation to the posterior.
Inspired by INLA  we propose a hybrid continuous-discrete variational approximation  which en-
ables us to preserve full posterior dependencies and is therefore more accurate than the mean-ﬁeld
variational Bayes (VB) method [15]. The continuous variational approximation is ﬂexible enough
for various kinds of latent ﬁelds  which makes our method applicable to more general settings than
assumed by INLA. The discretization of the low-dimensional hyper-parameter space can overcome
the potential non-conjugacy and multimodal posterior problems in variational inference.

1

2

Integrated Non-Factorized Variational Bayesian Inference

Consider a general Bayesian hierarchical model with observation y  latent variables x  and hyperpa-
rameters θ. The exact joint posterior p(x  θ|y) = p(y  x  θ)/p(y) can be difﬁcult to evaluate  since

usually the normalization p(y) =(cid:82)(cid:82) p(y  x  θ)dxdθ is intractable and numerical integration of x

is too expensive.
To address this problem  we ﬁnd a variational approximation to the exact posterior by minimizing
the Kullback-Leibler (KL) divergence KL (q(x  θ|y)||p(x  θ|y)). Applying Jensen’s inequality to
the log-marginal data likelihood  one obtains

ln p(y) = ln(cid:82)(cid:82) q(x  θ|y) p(y x θ)

q(x θ|y) dxdθ ≥(cid:82)(cid:82) q(x  θ|y) ln p(y x θ)

(1)
which holds for any proposed approximating distributions q(x  θ|y). L is termed the evidence
lower bound (ELBO)[4]. The gap in the Jensen’s inequality is exactly the KL divergence. Therefore
minimizing the Kullback-Leibler (KL) divergence is equivalent to maximizing the ELBO.
To make the variational problem tractable  the variational distribution q(x  θ|y) is commonly re-
quired to take a restricted form. For example  mean-ﬁeld variational Bayes (VB) method assumes
the distribution factorizes into a product of marginals [15]  q(x  θ|y) = q(x)q(θ)  which ignores the
posterior dependencies among different latent variables (including hyperparameters) and therefore
impairs the accuracy of the approximate posterior distribution.

q(x θ|y) dxdθ := L 

2.1 Hybrid Continuous and Discrete Variational Approximations
We consider a non-factorized approximation to the posterior q(x  θ|y) = q(x|y  θ)q(θ|y)  to pre-
serve the posterior dependency structure. Unfortunately  this generally leads to a nontrivial opti-
mization problem 
q(cid:63)(x  θ|y) = argmin{q(x θ|y)} KL (q(x  θ|y)||p(x  θ|y))  

= argmin{q(x θ|y)}(cid:82)(cid:82) q(x  θ|y) ln q(x θ|y)
= argmin{q(x|y θ)  q(θ|y)}(cid:82) q(θ|y)
is a ﬁnite mixture of Dirac-delta distributions  qd(θ|y) = (cid:80)
(2)
(cid:80)
We propose a hybrid continuous-discrete variational distribution q(x|y  θ)qd(θ|y)  where qd(θ|y)
k ωkδθk (θ) with ωk = qd(θk|y) and
k ωk = 1. Clearly  qd(θ|y) is an approximation of q(θ|y) by discretizing the continuous (typi-
cally) low-dimensional parameter space of θ using a grid G with ﬁnite grid points1. One can always
reduce the discretization error by increasing the number of points in G. To obtain a useful discretiza-
tion at a manageable number of grid points  the dimension of θ cannot be too large; this is also the
same assumption in INLA [7]  but we remove here the Gaussian prior assumption of INLA on latent
effects x.
The hybrid variational approximation is found by minimizing the KL divergence  i.e. 

(cid:105)
p(x θ|y) dx + ln q(θ|y)

(cid:104)(cid:82) q(x|θ  y) ln q(x|θ y)

p(x θ|y) dxdθ 

dθ.

(cid:104)(cid:82) q(x|θk  y) ln q(x|y θk)

p(x θk|y) dx + ln qd(θk|y)

KL (q(x  θ|y)||p(x  θ|y)) =(cid:80)

k qd(θk|y)
which leads to the approximate marginal posterior 

q(x|y) =(cid:80)

k q(x|y  θk)qd(θk|y)

(cid:105)

(3)

(4)

As will be clearer shortly  the problem in (3) can be much easier to solve than that in (2).
We give the name integrated non-factorized variational Bayes (INF-VB) to the method of approx-
imating p(x  θ|y) with q(x|y  θ)qd(θ|y) by solving the optimization problem in (3). The use of
qd(θ) is equivalent to numerical integration  which is a key idea of INLA [7]  see Section 2.3 for
details. It has also been used in sampling methods when samples are not easy to obtain directly
[16]. Here we use this idea in variational inference to overcome the potential non-conjugacy and
multimodal posterior problems in θ.

2.2 Variational Optimization

The proposed INF-VB method consists of two algorithmic steps:

1The grid points need not to be uniformly spaced  one may put more grid points to potentially high mass

regions if credible prior information is available.

2

(cid:105)

p(x θk|y) dx + ln qd(θk|y)

• Step 1: Solving multiple independent optimization problems  each for a grid point in G  to obtain

the optimal q(x|y  θk)  ∀θk ∈ G  i.e. 

q(cid:63)(x|y  θk) = argmin{q(x|y θk)}(cid:80)

= argmin{q(x|y θk)}(cid:82) q(x|θk  y) ln q(x|y θk)

p(x|y θk) dx
= argmin{q(x|y θk)} KL(q(x|y  θk)||p(x|y  θk))

k qd(θk|y)

(cid:104)(cid:82) q(x|θk  y) ln q(x|y θk)

(5)
The optimal variational distribution q(cid:63)(x|y  θk) is the exact posterior p(x|y  θk). In case it is
not available  we may further constrain q(x|y  θk) to a parametric form  examples including: (i)
multivariate Gaussian [17]  if the posterior asymptotic normality holds; (ii) skew-normal densities
[6  18]; or (iii) an inducing factorization assumption (see Ch.10.2.5 in [19])  if the latent variables
x are conditionally independent or their dependencies are negligible.
• Step 2: Given {q(cid:63)(x|y  θk) : θk ∈ G} obtained in Step 1  one solves
{q(cid:63)

d(θk|y)} = argmin{qd(θk|y)}(cid:80)

dx + ln qd(θk|y)

(cid:124)
k qd(θk|y)

q(cid:63)(x|θk  y) ln

(cid:20)(cid:90)
(cid:123)(cid:122)
(cid:16)(cid:82) q(cid:63)(x|y  θk) ln p(x θk|y)

l(qd(θk|y))=l(ωk)
k > 0)  which is solved to give
.

q(cid:63)(x|y  θk)
p(x  θk|y)

(cid:17)

(cid:21)
(cid:125)

Setting ∂l(ωk)/∂ωk = 0 (also ∂2l(ωk)/∂ω2

d(θk|y) ∝ exp
q(cid:63)

multiplicative constant  which can be identiﬁed from the normalization constraint(cid:80)

(6)
Note that qd(θ|y) is evaluated at a grid of points θk ∈ G  it needs to be known only up to a
d(θk|y) =

1. The integral in (6) can be analytically evaluated in the application considered in Section 3.

q(cid:63)(x|y θk) dx

k q(cid:63)

2.3 Links between INF-VB and INLA

The INF-VB is a variational extension of the integrated nested Laplace approximations (INLA)
[7]  a deterministic Bayesian inference method for latent Gaussian models (LGMs)  to the case
when p(x|θ) exhibits strong non-Gaussianity and hence p(θ|y) may not be approximated accurately
by the Laplace’s method of integration [20]. To see the connection  we review brieﬂy the three
computation steps of INLA and compare them with INF-VB in below:
1. Based on the Laplace approximation [3]  INLA seeks a Gaussian distribution qG (x|y  θk) =
N (x; x∗(θk)  H(x∗(θk))−1)  ∀θk ∈ G that captures most of the probabilistic mass locally 
where x∗(θk) = argmaxx p(x|y  θk) is the posterior mode  and H(x∗(θk)) is the Hessian ma-
trix of the log posterior evaluated at the mode. By contrast  INF-VB with the Gaussian parametric
constraint on q(cid:63)(x|y  θk) provides a global variational Gaussian approximation qV G(x|y  θk) in
the sense that the conditions of the Laplace approximation hold on average [17]. As we will
see next  the averaging operator plays a crucial role in handling the non-differentiable (cid:96)1 norm
arising from the double-exponential priors.

qLA (θ|y) = p(x θ|y)
q(x|y θ)

2. INLA computes the marginal posteriors of θ based on the Laplace’s method of integration [20] 
(7)
The quality of this approximation depends on the accuracy of q(x|y  θ). When q(x|y  θ) =
p(x|y  θ)  one has qLA (θ|y) equal to p(θ|y)  according to the Bayes rule. It has been shown
in [7] that (7) is accurate enough for latent Gaussian models with qG (x|y  θ). Alternatively  the
d(θ|y) by INF-VB (6) can be derived as a lower bound of the true
variational optimal posterior q(cid:63)
posterior p(θ|y) by Jensen’s inequality.

(cid:104)(cid:82) p(x θ|y)
q(x|y θ) q(x|y  θ)dx

(cid:105) ≥(cid:82) ln

(cid:104) p(x θ|y)
(cid:105)
q(x|y θ) q(x|y  θ)

(8)
Its optimality justiﬁcations in Section 2.2 also explain the often observed empirical successes of
hyperparameter selection based on the ELBO of ln p(y|θ) [13]  when the ﬁrst level of Bayesian
inference is performed  i.e. only the conditional posterior q(x|y  θ) with ﬁxed θ is of interest. In
Section 4 we compare the accuracies of both (6) and (7) for hyperparameter learning.
3. INLA obtains the marginal distributions of interest  e.g.  q(x|y) via numerically integrating out
k q(x|y  θk)q(θk|y)∆k with area weights ∆k. In INF-VB  we have qd(θ|y) =

θ: q(x|y) =(cid:80)
(cid:80)
k ωkδθk (θ). Let ωk = q(θk|y)∆k  we immediately have

dx = ln q(cid:63)

ln p(θ|y) = ln

d(θ|y)

(cid:12)(cid:12)(cid:12)x=x∗(θ)

3

q(x|y) =(cid:82) q(x|y  θ)qd(θ|y)dθ =(cid:80)

k q(x|y  θk)qd(θk|y) =(cid:80)

k q(x|y  θk)q(θk|y)∆k (9)
This Dirac-delta mixture interpretation of numerical integration also enables us to quantitize
the accuracy of INLA approximation qG(x|y  θ)qLA(θ|y) using the KL divergence to p(x  θ|y)
under the variational framework.

In contrast to INLA  INF-VB provides q(x|y  θ) and qd(θ|y)  both are optimal in a sense of the min-
imum Kullback-Leibler divergence  within the proposed hybrid distribution family. In this paper we
focus on the full posterior inference of Bayesian Lasso [21] where the local Laplace approximation
in INLA cannot be applied  as the non-differentiability of the (cid:96)1 norm prevents one from computing
the Hessian matrix. Besides  if we do not exploit the scale mixture of normals representation [22] of
Laplace priors (i.e.  no data-augmentation)  we are actually dealing with a non-conjugate variational
inference problem in Bayesian Lasso.

(cid:16)− λ√

(cid:17)

3 Application to Bayesian Lasso
Consider the Bayesian Lasso regression model [21]  y = Φx + e  where Φ ∈ Rn×p is the design
matrix containing predictors  y ∈ Rn are responses2  and e ∈ Rn contain independent zero-mean
Gaussian noise e ∼ N (e; 0  σ2In). Following [21] we assume3 

xj|σ2  λ2 ∼ λ
√

σ2 exp

2

σ2(cid:107)xj(cid:107)1

  σ2 ∼ InvGamma(σ2; a  b)  λ2 ∼ Gamma(λ2; r  s)

While the Lasso estimates [23] provide only the posterior modes of the regression parameters x ∈
Rp  Bayesian Lasso [21] provides the complete posterior distribution p(x  θ|y)  from which one
may obtain whatever statistical properties are desired of x and θ  including the posterior mode 
mean  median  and credible intervals.
Since in our approach variational Gaussian approximation is performed separately (see Section 3.1)
for each hyperparameter {λ  σ2} considered  the efﬁciency of approximating p(x|y  θ) is particu-
larly important. The upper bound of the KL divergence derived in Section 3.2 provides an approxi-
mate closed-form solution  that is often accurate enough or requires a small number of gradient iter-
ations to converge to optimality. The tightness of the upper bound is analyzed using spectral-norm
bounds (See Section 3.3)  which also provide insights on the connection between the deterministic
Lasso [23] and the Bayesian Lasso [21].

3.1 Variational Gaussian Approximation

(cid:110)−(cid:107)y−Φx(cid:107)2

2σ2 − λ

σ(cid:107)x(cid:107)1

.

(cid:111)

The conditional distribution of y and x given θ is

p(y  x|θ) = λp/(2σ)p

√

Def.

g(µ  D)

(10)
The postulated approximation  q(x|θ  y) = N (x; µ  D)  is a multivariate Gaussian density (drop-
ping dependencies of variational parameters (µ  D) on (θ  y) for brevity)  whose parameters (µ  D)
are found by minimizing the KL divergence to p(x|θ  y) 

(2πσ2)n

exp

= KL(q(x; µ  D)(cid:107)p(x|y  θ)) =(cid:82) q(x; µ  D) ln q(x;µ D)
=(cid:82) q(x; µ  D) ln q(x;µ D)
(cid:112)dj 

(cid:2)µj − 2µjΨ(hj) + 2(cid:112)djψ(hj)(cid:3)  

p(y x|θ) dx + ln p(y|θ) 

(cid:107)y−Φµ(cid:107)2+tr(Φ(cid:48)ΦD)

2 ln|D|+

p(x|y θ) dx

Eq((cid:107)x(cid:107)1) + ln p(y|θ) − ln ψ(σ2  λ)
dj = Djj

hj = −µj

j=1

(11)
where ψ(σ2  λ) = (4πeλ2σ−2)p/2(2πσ2)−n/2  Ψ(·) and ψ(·) corresponds to the standard normal
cumulative distribution function and probability density function  respectively. Expectation is taken
with respect to q(x; µ  D). Deﬁne D = CCT   where C is the Cholesky factorization of the covari-
ance matrix D. Since g(µ  D) is convex in the parameter space (µ  C)  a global optimal variational
Gaussian approximation q(cid:63)(x|y  θ) is guaranteed  which achieves the minimum KL divergence to
p(x|θ  y) within the family of multivariate Gaussian densities speciﬁed [13]4.

Eq((cid:107)x(cid:107)1) =(cid:80)p

= − 1

+ λ
σ

2σ2

2We assume that both y and the columns of Φ have been mean-centered to remove the intercept term.
3[21] suggested using scaled double-exponential priors under which they showed that p(x  σ2|y  λ) is uni-
modal  further  the unimodality helps to accelerate convergence of the data-augmentation Gibbs sampler and
makes the posterior mode more meaningful. Gamma prior is put on λ2 for conjugacy.

4Code for variational Gaussian approximation is available at mloss.org/software/view/308

4

As a ﬁrst step  one ﬁnds q(cid:63)(x|y  θ) using gradient based procedures independently for each hyper-
parameter combinations {λ  σ2}. Second  q(cid:63)(θ|y) can be evaluated analytically using either (6) or
(7); both will yield a ﬁnite mixture of Gaussian distribution for the marginal posterior q(x|y) via nu-
merical integration  which is highly efﬁcient since we only have two hyperparameters in Bayesian
Lasso. Finally  the evidence lower bound (ELBO) in (1) can also be evaluated analytically after
simple algebra. We will show in Section 4.3 a comparison with the mean-ﬁeld variational Bayesian
(VB) approach  derived based on a scale normal mixture representation [22] of the Laplace prior.

3.2 Upper Bounds of KL divergence

We provide an approximate solution ( ˆµ  ˆD) via minimizing an upper bound of KL divergence (11).
This solution solves a Lasso problem in µ  and has a closed-form expression for D  making this
computationally efﬁcient. In practice  it could serve as an initialization for gradient procedures.
Lemma 3.1. (Triangle Inequality) Eq(cid:107)x(cid:107)1 ≤ Eq(cid:107)x − µ(cid:107)1 + (cid:107)µ(cid:107)1  where Eq(cid:107)x − µ(cid:107)1 =

(cid:112)2/π(cid:80)p

(cid:112)dj  with the expectation taken with respect to q(x; µ  D).
j=1 dj ≤(cid:113)

j ≤(cid:80)p
j=1  it holds
++  tr(A2) ≤ tr(A) ≤ √

(cid:113)(cid:80)p

Lemma 3.2. For any {dj ≥ 0}p
Lemma 3.3. [24] For any A ∈ Sp
Theorem 3.1. (Upper and Lower bound) For any A  D ∈ Sp
1√

(cid:112)dj ≤ √

++  A =

p tr(A2).

p tr(A).

j=1 d2

√

D5  dj = Djj holds

p(cid:80)p

j.
j=1 d2

j=1

j=1

p tr(A) ≤(cid:80)p
(cid:124)

f (µ  D) =

(cid:114) 2p

π

λ
σ

Applying Lemma 3.1 and Theorem 3.1 in (11)  one obtains an upper bound for KL divergence 
+ ln p(y|θ)

(cid:107)y − Φµ(cid:107)2

tr(Φ(cid:48)ΦD)

ln|D| +

√
tr(

D)

+

+

(cid:107)µ(cid:107)1

ψ(σ2 λ)

2

(cid:123)(cid:122)

λ
σ

(cid:125)

+− 1
2

(cid:124)

2σ2

(cid:125)

(cid:123)(cid:122)

2σ2

f2(D)

≥ g(µ  D) = KL(q(x; µ  D)(cid:107)p(x|y  θ))

f1(µ)

(12)

In the problem of minimizing the KL divergence g(µ  CCT )  one needs to iteratively update µ and
C  since they are coupled. However  the upper bound f (µ  D) decouples into two additive terms:
f1 is a function of µ while f2 is a function of D  which greatly simpliﬁes the minimization.
• The minimization of f1(µ) is a convex Lasso problem. Using path-following algorithms (e.g.  a
modiﬁed least angle regression algorithm (LARS) [25])  one can efﬁciently compute the entire
solution path of Lasso estimates as a function of λ0 = 2λσ in one shot. Global optimal solutions
for ˆµ(θk) on each grid point θk ∈ G can be recovered using the piece-wise linear property.

• The function f2(D) is convex in the parameter space A =

D  whose minimizer is in closed-

form and can be found by setting the gradient to zero and solving the resulting equation 

√

(cid:18)(cid:113) λ2p

(cid:113) λ2p

(cid:19)−1

∇Af2 = −A−1 + Φ(cid:48)ΦA

σ2 + λ

π I = 0 

ˆA =

2πσ2 I +

2πσ2 I + Φ(cid:48)Φ

σ2

  (13)

(cid:113) 2p

We have ˆD = ˆA2  which is guaranteed to be a positive deﬁnite matrix. Note that the global
optimum ˆD(θk) for each grid point θk ∈ G have the same eigenvectors as the Gram matrix Φ(cid:48)Φ
and differ only in eigenvalues. For j = 1  . . .   p  denote the eigenvalues of D and Φ(cid:48)Φ as αj and

βj  respectively. By (13)  we have αj = λ(cid:112)p/(2πσ2) +(cid:112)λ2p/(2πσ2) + βj/σ2. Therefore 

one can pre-compute the eigenvectors once  and only update the eigenvalues as a function of θk.
This will make the computation efﬁcient both in time and memory.

The solutions ( ˆµ  ˆD) which minimize the KL upper bound f ( ˆµ  ˆD) in (12) achieves its global
optimum. Meanwhile  it is also accurate in the sense of the KL divergence g( ˆµ  ˆD) in (11)  as we
will show next. Tightness analysis of the upper bound is also provided  using trace norm bounds.

5Since D is positive deﬁnite  it has a unique symmetric square root A =

D by taking square root of the eigenvalues.

√
D  which can be obtained from

5

3.3 Theoretical Anlaysis

(cid:0)(cid:107)y − Φx(cid:107)2

Eq(x|y θ)

(cid:0)(cid:107)y − Φµ(cid:107)2

(cid:1)  

(cid:1)

(cid:16)(cid:107)y−Φµ(cid:107)2

Theorem 3.2. (KL Divergence Upper Bound) Let ( ˆµ  ˆD) be the minimizer of the KL upper
bound(12)  i.e.  ˆµ solves the Lasso and ˆD is given in (13). Then

  f2( ˆD) =(cid:80)

g( ˆµ  ˆD) ≤ minµ D f (µ  D) = f1( ˆµ) + f2( ˆD) + ln p(y|θ)

(14)
−1.
where f1( ˆµ) = minµ
Thus the KL divergence for ( ˆµ  ˆD) is upper bounded by the minimum achievable (cid:96)1-penalized least
square error 1 = f1( ˆµ) and terms in f2( ˆD) which are ultimately related to the eigenvalues {βj}
(j = 1  . . .   p) of the Gram matrix Φ(cid:48)Φ.
Let (µ∗  D∗) be the minimizer of the original KL divergence g(µ  D)  and g1(µ|D) collect the
terms of g(µ  D) that are related to µ. Then the Bayesian posterior mean obtained via VG  i.e. 

j ln αj +(cid:80)

2σ2 +(cid:80)

(cid:113) 2λ2n

2σ2 + λ

σ(cid:107)µ(cid:107)1

−2
βj α
j

(cid:17)

ψ(σ2 λ)

(αj)

π

j

j

2

µ∗ = argminµ g1(µ|D∗) = argminµ

2 + 2λσ(cid:107)x(cid:107)1

(15)

2 + 2λσ(cid:107)µ(cid:107)1

is a counterpart of the deterministic Lasso [23]  which appears naturally in the upper bound 

ˆµ = argminµ f1(µ) = argminµ

(16)
Note that the Lasso solution cannot be found by gradient methods due to non-differentiability. By
taking the expectation  the objective function is smoothed around 0 and thus differentiable. This
connection indicates that in VG for Bayesian Lasso  the conditions of deterministic Lasso hold on
average  with respect to the variational distribution q(x|y  θ)  in the parameter space of µ.
The following theorem (proof sketches are in the Supplementary Material) provides quantitative
measures of the closeness of the upper bounds  f1(µ) and f (µ  D)  to their respective true counter-
parts.
Theorem 3.3. The tightness of f1(µ) and f (µ  D) is given by

g1(µ|D) − f1(µ) ≤ tr(Φ(cid:48)ΦD)

2σ2 + λ
which holds for any (µ  D) ∈ Rp × Sp
KL divergence  or information gap)  we have

f1(µ∗) ≤ g1(µ∗) ≤ g1( ˆµ) ≤ 1 + tr(Φ(cid:48)ΦD)/(2σ2) + λ(cid:112)2p/(σ2π)tr(
g( ˆµ  ˆD) ≤ f ( ˆµ  ˆD) ≤ f (µ∗  D∗) ≤ 2 + 2λ(cid:112)2p/(σ2π)tr(

D∗)

D)  f (µ  D) − g(µ  D) ≤ 2λ

D)(17)
++. Further assume g(µ∗  D∗) = 2 (minimum achievable

(18a)
(18b)

√

σ

σ

(cid:113) 2p
(cid:112) ˆD)

√
π tr(

√
π tr(

(cid:113) 2p

4 Experiments

We consider long runs of MCMC 6 as reference solutions  and consider two types of INF-VB: INF-
VB-1 calculates hyperparameter posteriors using (6); while INF-VB-2 uses (7) and evaluates it at
the posterior mode of p(x|y  θ). We also compare INF-VB-1 and INF-VB-2 to VB  a mean-ﬁeld
variational Bayes (VB) solution (See Supplementary Material for update equations). The results
show that the INF-VB method is more accurate than VB  and is a promising alternative to MCMC
for Bayesian Lasso.

4.1 Synthetic Dataset

We compare the proposed INF-VB methods with VB and intensive MCMC runs  in terms of the joint
posterior q(λ2  σ2|y)   the marginal posteriors of hyper-parameters q(σ2|y) and q(λ2|y)  and the
marginal posteriors of regression coefﬁcients q(xj|y) (see Figure 1). The observations are generated
i x + i  i = 1  . . .   600  where φij are drawn from an i.i.d. normal distribution7 
from yi = φT
where the pairwise correlation between the jth and the kth columns of Φ is 0.5|j−k|; we draw
i ∼ N (0  σ2)  xj|λ  σ ∼ Laplace(λ/σ)  j = 1  . . .   300  and set σ2 = 0.5  λ = 0.5.

6In all experiments shown here  we take intensive MCMC runs as the gold standard (with 5 × 103 burn-ins
and 5× 105 samples collected). We use data-augmentation Gibbs sampler introduced in [21]. Ground truth for
latent variables and hyper-parameter are also compared to whenever possible. The hyperparameters for Gamma
distributions are set to a = b = r = s = 0.001 through all these experiments. If not mentioned  the grid size
is 50 × 50  which is uniformly created around the ordinary least square (OLS) estimates of hyper-parameters.
7The responses y and the columns of Φ are centered; the columns of Φ are also scaled to have unit variance

6

(a)

(b)

(c)

(d)

(e)

(f )

(g)

(h)

Figure 1: Contour plots for joint posteriors of hyperparameters q(σ2  λ2|y): (a)-(d); Marginal pos-
terior of hyperparameters and coefﬁcients: (e) q(σ2|y)  (f)q(λ2|y); (g) q(x1|y)  (h)q(x2|y)
See Figure 1(a)-(d)  both MCMC and INF-VB preserve the strong posterior dependence among
hyperparameters  while mean-ﬁeld VB cannot. While mean-ﬁeld VB approximates the posterior
mode well  the posterior variance can be (sometimes severely) underestimated  see Figure 1(e)  (f ).
Since we have analytically approximated p(x|y) by a ﬁnite mixture of normal distribution q(x|y  θ)
with mixing weights q(θ|y)  the posterior marginals for the latent variables: q(xj|y) are easily
accessible from this analytical representation. Perhaps surprisingly  both INF-VB and mean-ﬁeld
VB provide quite accurate marginal distributions q(xj|y)  see Figure 1(j)-(h) for examples. The
differences in the tails of q(θ|y) between INF-VB and mean-ﬁeld VB yield negligible differences
in the marginal distributions q(xj|y)  when θ is integrated out.

4.2 Diabetes Dataset

We consider the benchmark diabetes dataset [25] frequently used in previous studies of Bayesian
Lasso; see [21  26]  for example. The goal of this diagnostic study  as suggested in [25]  is to
construct a linear regression model (n = 442  p = 10) to reveal the important determinants of
the response  and to provide interpretable results to guide disease progression.
In Figure 2  we
show accurate marginal posteriors of hyperparameters q(σ2|y) and q(λ2|y) as well as marginals
of coefﬁcients q(xj|y)  j = 1  . . .   10  which indicate the relevance of each predictor. We also
compared them to the ordinary least square (OLS) estimates.

(a)

(b)

(c)

(d)

(e)

(f )

(g)

(h)

(i)

Figure 2: Posterior marginals of hyperparameters: (a) q(σ2|y) and (b)q(λ2|y); posterior marginals
of coefﬁcients: (c)-(l) q(xj|y) (j = 1  . . .   10)

(k)

(j)

(l)

7

λ2σ2  0.150.20.250.30.350.350.40.450.50.550.6MCMCGround Truthλ2σ2  0.150.20.250.30.350.350.40.450.50.550.6VBGround Truthλ2σ2  0.150.20.250.30.350.350.40.450.50.550.6INF−VB−1Ground Truthλ2σ2  0.150.20.250.30.350.350.40.450.50.550.6INF−VB−2Ground Truth0.30.40.50.60.705101520σ2q(σ2|y)  MCMCINF−VB−1INF−VB−2VBGround Truth0.10.150.20.250.30.350.4051015202530λ2q(λ2|y)  MCMCINF−VB−1INF−VB−2VBGround Truth−1.2−1.1−1−0.9−0.802468x1q(x1|y)  MCMCINF−VB−1INF−VB−2VBGround Truth−0.2−0.100.10246810x2q(x2|y)  MCMCINF−VB−1INF−VB−2VBGround Truth0.811.21.40123456σ2q(σ2|y)  MCMCINF−VB−1INF−VB−2VBOLS0100020003000400000.511.522.5x 10−3λ2q(λ2|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.15−0.1−0.0500.05051015x1 (age)q(x1|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.15−0.1−0.0500.05051015x2 (sex)q(x2|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x3 (bmi)q(x3|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x4 (bp)q(x4|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x5 (tc)q(x5|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.0500.050.10.15051015x6 (ldl)q(x6|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x7 (hdl)q(x7|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.0500.050.10.15051015x8 (tch)q(x8|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x9 (ltg)q(x9|y)  MCMCINF−VB−1INF−VB−2VBOLS−0.1−0.0500.050.105101520x10 (glu)q(x10|y)  MCMCINF−VB−1INF−VB−2VBOLS4.3 Comparison: Accuracy and Speed
We quantitatively measure the quality of the approximate joint probability q(x  θ|y) provided by our
non-factorized variational methods  and compare them to VB under factorization assumptions. The
KL divergence KL(q(x  θ|y)|p(x  θ|y)) is not directly available; instead  we compare the negative
evidence lower bound (1)  which can be evaluated analytically in our case and differs from the KL
divergence only up to a constant. We also measure the computational time of different algorithms
In INF-VB  different grids of sizes m × m are considered  where
by elapsed times (seconds).
m = 1  5  10  30  50. We consider two real world datasets: the above Diabetes dataset  and the
Prostate cancer dataset [27]. Here  INF-VB-3 and INF-VB-4 refer to the methods that use the
approximate solution in Section 3.2 with no gradient steps for q(x|y  θ)  and use (6) or (7) for
q(θ|y).

(c)

(d)

(a)

(b)

Figure 3: Negative evidence lower bound (ELBO) and elapsed time v.s. grid size; (a)  (b) for the
Diabetes dataset (n = 442  p = 10). (c)  (d) for the Prostate cancer dataset (n = 97  p = 8)
The quality of variational methods depends on the ﬂexibility of variational distributions. In INF-VB
for Bayesian Lasso  we constrain q(x|y  θ) to be parametric and q(θ|y) to be still in free form. See
from Figure 3  the accuracy of INF-VB method with a 1×1 grid is worse than mean-ﬁeld VB  which
corresponds to the partial Bayesian learning of q(x|y  θ) with a ﬁxed θ. As the grid size increases 
the accuracies of INF-VB (even those without gradient steps) also increase and are in general of
better quality than mean-ﬁeld VB  in the sense of negative ELBO (KL divergence up to a constant).
The computational complexities of INF-VB  mean-ﬁeld VB  and MCMC methods are proportional
to the grid size  number of iterations toward local optimum  and the number of runs  respectively.
Since the computations on the grid are independent  INF-VB is highly parallelizable  which is an
important feature as more multiprocessor computational power becomes available. Besides  one
may further reduce its computational load by choosing grid points more economically  which will
be pursued in our next step. Even the small datasets we show here for illustration enjoy good speed-
ups. A signiﬁcant speed-up for INF-VB can be achieved via parallel computing.

5 Discussion
We have provided a ﬂexible framework for approximate inference of the full posterior p(x  θ|y)
based on a hybrid continuous-discrete variational distribution  which is optimal in the sense of the
KL divergence. As a reliable and efﬁcient alternative to MCMC  our method generalizes INLA to
non-Gaussian priors and VB to non-factorization settings. While we have used Bayesian Lasso as
an example  our inference method is generically applicable. One can also approximate p(x|y  θ)
using other methods  such as scalable variational methods [28]  or improved EP [29].
The posterior p(θ|y)  which is analyzed based on a grid approximation  enables users to do both
model averaging and model selection  depending on speciﬁc purposes. The discretized approxi-
mation of p(θ|y) overcomes the potential non-conjugacy or multimodal issues in the θ space in
variational inference  and it also allows parallel implementation of the hybrid continuous-discrete
variational approximation with the dominant computational load (approximating the continuous
high dimensional q(x|y  θ)) distributed on each grid point  which is particularly important when
applying INF-VB to large-scale Bayesian inference. INF-VB has limitations. The number of hyper-
parameters θ should be no more than 5 to 6  which is the same fundamental limitation of INLA.

Acknowledgments

The work reported here was supported in part by grants from ARO  DARPA  DOE  NGA and ONR.

8

01020304050630635640645650655660665mNegative ELBO  INF−VB−1INF−VB−2INF−VB−3INF−VB−4VB0102030405005101520mElapsed Time (seconds)  MCMCINF−VB−1INF−VB−2INF−VB−3INF−VB−4VB01020304050115120125130135140145150mNegative ELBO  INF−VB−1INF−VB−2INF−VB−3INF−VB−4VB01020304050051015mElapsed Time (seconds)  MCMCINF−VB−1INF−VB−2INF−VB−3INF−VB−4VBReferences
[1] D. Gamerman and H. F. Lopes. Markov chain Monte Carlo: stochastic simulation for Bayesian inference.

Chapman & Hall Texts in Statistical Science Series. Taylor & Francis  2006.

[2] C. P. Robert and G. Casella. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-

Verlag New York  Inc.  Secaucus  NJ  USA  2005.

[3] R. E. Kass and D. Steffey. Approximate Bayesian inference in conditionally independent hierarchical

models (parametric empirical Bayes models). J. Am. Statist. Assoc.  84(407):717–726  1989.

[4] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul. An introduction to variational methods for
graphical models. In Learning in graphical models  pages 105–161  Cambridge  MA  1999. MIT Press.
[5] T. P. Minka. Expectation propagation for approximate Bayesian inference. In J. S. Breese and D. Koller 
editors  Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence  pages 362–369  2001.
[6] J. T. Ormerod. Skew-normal variational approximations for Bayesian inference. Technical Report CRG-

TR-93-1  School of Mathematics and Statistics  Univeristy of Sydney  2011.

[7] H. Rue  S. Martino  and N. Chopin. Approximate Bayesian inference for latent Gaussian models by using
integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B  71(2):319–
392  2009.

[8] J. Hensman  M. Rattray  and N. D. Lawrence. Fast variational inference in the conjugate exponential

family. In Advances in Neural Information Processing Systems  2012.

[9] J. Foulds  L. Boyles  C. Dubois  P. Smyth  and M. Welling. Stochastic collapsed variational Bayesian
inference for latent Dirichlet allocation. In 19th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD)  2013.

[10] J. W. Paisley  D. M. Blei  and M. I. Jordan. Variational Bayesian inference with stochastic search. In

International Conference on Machine Learning  2012.

[11] C. Wang and D. M. Blei. Truncation-free online variational inference for Bayesian nonparametric models.

In Advances in Neural Information Processing Systems  2012.

[12] S. J. Gershman  M. D. Hoffman  and D. M. Blei. Nonparametric variational inference. In International

Conference on Machine Learning  2012.

[13] E. Challis and D. Barber. Concave Gaussian variational approximations for inference in large-scale
Bayesian linear models. Journal of Machine Learning Research - Proceedings Track  15:199–207  2011.
[14] M. E. Khan  S. Mohamed  and K. P. Muprhy. Fast Bayesian inference for non-conjugate Gaussian process

regression. In Advances in Neural Information Processing Systems  2012.

[15] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  Gatsby Computa-

tional Neuroscience Unit  University College London  2003.

[16] C. Ritter and M. A. Tanner. Facilitating the Gibbs sampler: The Gibbs stopper and the griddy-Gibbs

sampler. J. Am. Statist. Assoc.  87(419):pp. 861–868  1992.

[17] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural Comput. 

21(3):786–792  2009.

[18] E. Challis and D. Barber. Afﬁne independence variational inference. In Advances in Neural Information

Processing Systems  2012.

[19] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-

Verlag New York  Inc.  Secaucus  NJ  USA  2006.

[20] L. Tierney and J. B. Kadane. Accurate approximations for posterior moments and marginal densities. J.

Am. Statist. Assoc.  81:82–86  1986.

[21] T. Park and G. Casella. The Bayesian Lasso. J. Am. Statist. Assoc.  103(482):681–686  2008.
[22] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal Statistical

Society. Series B  36(1):pp. 99–102  1974.

[23] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society 

Series B  58:267–288  1996.

[24] G. H. Golub and C. V. Loan. Matrix Computations(Third Edition). Johns Hopkins University Press  1996.
[25] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics  32:407–

499  2004.

[26] C. Hans. Bayesian Lasso regression. Biometrika  96(4):835–845  2009.
[27] T. Stamey  J. Kabalin  J. McNeal  I. Johnstone  F. Freha  E. Redwine  and N. Yang. Prostate speciﬁc
antigen in the diagnosis and treatment of adenocarcinoma of the prostate. ii. radical prostatectomy treated
patients. Journal of Urology  16:pp. 1076–1083  1989.

[28] M. W. Seeger and H. Nickisch. Large scale Bayesian inference and experimental design for sparse linear

models. SIAM J. Imaging Sciences  4(1):166–199  2011.

[29] B. Cseke and T. Heskes. Approximate marginals in latent Gaussian models. J. Mach. Learn. Res.  12:417–

454  2011.

9

,Shaobo Han
Xuejun Liao
Lawrence Carin