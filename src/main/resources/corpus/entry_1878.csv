2009,Evaluating multi-class learning strategies in a generative hierarchical framework for object detection,Multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance  while joint representations optimize sharing and reduce inference time --- but are complex to train. Conveniently  sequential learning of categories cuts down training time by transferring existing knowledge to novel classes  but cannot fully exploit the richness of shareability and might depend on ordering in learning. In hierarchical frameworks these issues have been little explored. In this paper  we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows. Specifically  we propose  evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories  2.) joint training of classes  3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets.,Evaluating multi-class learning strategies in a
hierarchical framework for object detection

Sanja Fidler

Marko Boben

Aleˇs Leonardis

Faculty of Computer and Information Science

University of Ljubljana  Slovenia

{sanja.fidler  marko.boben  ales.leonardis}@fri.uni-lj.si

Abstract

Multi-class object learning and detection is a challenging problem due to the
large number of object classes and their high visual variability. Specialized de-
tectors usually excel in performance  while joint representations optimize sharing
and reduce inference time — but are complex to train. Conveniently  sequential
class learning cuts down training time by transferring existing knowledge to novel
classes  but cannot fully exploit the shareability of features among object classes
and might depend on ordering of classes during learning. In hierarchical frame-
works these issues have been little explored. In this paper  we provide a rigorous
experimental analysis of various multiple object class learning strategies within a
generative hierarchical framework. Speciﬁcally  we propose  evaluate and com-
pare three important types of multi-class learning: 1.) independent training of
individual categories  2.) joint training of classes  and 3.) sequential learning of
classes. We explore and compare their computational behavior (space and time)
and detection performance as a function of the number of learned object classes
on several recognition datasets. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection perfor-
mance and could thus be used to learn the classes on a larger scale.

1 Introduction

Object class detection has been one of the mainstream research areas in computer vision. In recent
years we have seen a signiﬁcant trend towards larger recognition datasets with an increasing number
of object classes [1]. This necessitates representing  learning and detecting multiple object classes 
which is a challenging problem due to the large number and the high visual variability of objects.
To learn and represent multiple object classes there have mainly been two strategies: the detectors
for each class have either been trained in isolation  or trained on all classes simultaneously. Both
exert certain advantages and disadvantages. Training independently allows us to apply complex
probabilistic models that use a signiﬁcant amount of class speciﬁc features and allows us to tune
the parameters for each class separately. For object class detection  these approaches had notable
success [2]. However  representing multiple classes in this way  means stacking together speciﬁc
class representations. This  on the one hand  implies that each novel class can be added in constant
time  however  the representation grows clearly linearly with the number of classes and is thus also
linear in inference. On the other hand  joint representations enlarge sublinearly by virtue of sharing
the features among several object classes [3  4]. This means sharing common computations and
increasing the speed of the joint detector. Training  however  is usually quadratic in the number of
classes. Furthermore  adding just one more class forces us to re-train the representation altogether.
Receiving somewhat less attention  the strategy to learn the classes sequentially (but not indepen-
dently) potentially enjoys the traits of both learning types [4  5  6]. By learning one class after

1

another  we can transfer the knowledge acquired so far to novel classes and thus likely achieve both 
sublinearity in inference and cut down training time. In order to scale to a higher number of object
classes  learning them sequentially lends itself as the best choice.
In literature  the approaches have mainly used one of these three learning strategies in isolation.
To the best of our knowledge  little research has been done on analyzing and comparing them with
respect to one another. This is important because it allows us to point to losses and gains of each
particular learning setting  which could focus further research and improve the performance. This is
exactly what this paper is set to do — we present a hierarchical framework within which all of the
aforementioned learning strategies can be unbiasedly evaluated and put into perspective.
Prominent work on these issues has been done in the domain of ﬂat representations [4  3]  where
each class is modeled as an immediate aggregate of local features. However  there is an increasing
literature consensus  that hierarchies provide a more suitable form of multi-class representation [7 
8  9  10  11  12]. Hierarchies not only share complex object parts among similar classes  but can
re-use features at several levels of granularity also for dissimilar objects.
In this paper  we provide a rigorous experimental evaluation of several important multi-class learning
strategies for object detection within a generative hierarchical framework. We make use of the
hierarchical learning approach by [13]. Here we propose and evaluate three types of multi-class
learning: 1.) independent training of individual categories  2.) joint training  3.) sequential training
of classes. Several issues were evaluated on multiple object classes: 1.) growth of representation 
2.) training and 3.) inference time  4.) degree of feature sharing and re-use at each level of the
hierarchy  5.) inﬂuence of class ordering in sequential learning  and 6.) detection performance  all
as a function of the number of classes learned. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection performance and could
thus be used to learn the classes on a larger scale.
Related work. Prior work on multi-class learning in generative hierarchies either learns separate
hierarchies for each class [14  15  16  10  17]  trains jointly [7  18  9  19  20  11]  whereas work
on sequential learning of classes has been particularly scarce [6  13]. However  to the best of our
knowledge  no work has dealt with  evaluated and compared multiple important learning concepts
under one hierarchical framework.

2 The hierarchical model and inference

The hierarchical model. We use the hierarchical model of [13  21]  which we summarize here. Ob-
jects are represented with a recursive compositional shape vocabulary which is learned from images.
The vocabulary contains a set of shape models or compositions at each layer. Each shape model in
the hierarchy is modeled as a conjunction of a small number of parts (shapes from the previous
layer). Each part is spatially constrained on the parent shape model via a spatial relation which is
modeled with a two-dimensional Gaussian distribution. The number and the type of parts can differ
across the shape models and is learned from the data without supervision. At the lowest layer  the
vocabulary consists of a small number of short oriented contour fragments  while the vocabulary at
the top-most layer contains models that code the shapes of the whole objects. For training  we need
a positive and a validation set of class images  while the structure of the representation is learned in
an unsupervised way (no labels on object parts or smaller subparts need to be given).
The hierarchical vocabulary V = (V  E) is represented with a directed graph  where multiple edges
between two vertices are allowed. The vertices V of the graph represent the shape models and
the edges E represent the composition relations between them. The graph V has a hierarchical
structure  where the set of vertices V is partitioned into subsets V 1  . . .   V O  each containing the
shapes at a particular layer. The vertices {v1
i }6
i=1 at the lowest layer V 1 represent 6 oriented contour
fragments. The vertices at the top-most layer V O  referred to as the object layer represent the whole
C ⊆ V O that code the
shapes of the objects. Each object class C is assigned a subset of vertices V O
object layer shapes of that particular class. We denote the set of edges between the vertex layers
V (cid:96) and V (cid:96)−1 with E(cid:96). Each edge e(cid:96)
in E(cid:96) is associated with the Gaussian parameters
R and its part v(cid:96)−1
Ri := θ(e(cid:96)
.
θ(cid:96)
We will use θ(cid:96)
R. The pair
V (cid:96) := (V (cid:96)  E(cid:96)) will be referred to as the vocabulary at layer (cid:96).

Ri  Σ(cid:96)
Ri)i to denote the vector of all the parameters of a shape model v(cid:96)

Ri) of the spatial relation between the parent shape v(cid:96)

Ri) = (µ(cid:96)
R = (θ(cid:96)

i

Ri = v(cid:96)

Rv(cid:96)−1

i

2

Inference. We infer object class instances in a query image I in the following way. We follow the
contour extraction of [13]  which ﬁnds local maxima in oriented Gabor energy. This gives us the
contour fragments F and their positions X. In the process of inference we build a (directed acyclic)
inference graph G = (Z  Q). The vertices Z are partitioned into vertex layers 1 to O (object layer) 
Z = Z 1∪···∪ZO  and similarly also the edges  Q = Q1∪···∪QO. Each vertex z(cid:96) = (v(cid:96)  x(cid:96)) ∈ Z (cid:96)
represents a hypothesis that a particular shape v(cid:96) ∈ V (cid:96) from the vocabulary is present at location x(cid:96).
The edges in Q(cid:96) connect each parent hypothesis z(cid:96)
. The edges in
the bottom layer Q1 connect the hypotheses in the ﬁrst layer Z 1 with the observations. With S(z)
we denote the subgraph of G that contains the vertices and edges of all descendants of vertex z.
Since our deﬁnition of each vocabulary shape model assumes that its parts are conditionally inde-
pendent  we can calculate the likelihood of the part hypotheses z(cid:96)−1
) under a parent
hypothesis z(cid:96)

R) by taking a product over the individual likelihoods of the parts:

R to all of its part hypotheses z(cid:96)−1

i = (v(cid:96)−1

  x(cid:96)−1

i

i

i

R  x(cid:96)

R = (v(cid:96)
p(v(cid:96)−1  x(cid:96)−1 | v(cid:96)

R  x(cid:96)

R  θ(cid:96)

R) = (cid:89)

e(cid:96)
Ri=v(cid:96)

Rv(cid:96)−1

i

p(x(cid:96)−1

i

| x(cid:96)

R  v(cid:96)−1

i

  v(cid:96)

R  θ(cid:96)

Ri)

(1)

i

i

| x(cid:96)

The term pRi := p(x(cid:96)−1
R  v(cid:96)−1
Ri) stands for the spatial constraint imposed by a vocab-
R  θ(cid:96)
. It is modeled by a
ulary edge e(cid:96)
Ri between a parent hypothesis z(cid:96)
R | θ(cid:96)
normal distribution  pRi = N (x(cid:96)−1
Ri). If the likelihood in (1) is
R and its most likely part hypotheses. The log-likelihood
above a threshold  we add edges between z(cid:96)
R is then calculated recursively over the subgraph S(z(cid:96)
R):
of the observations under a hypothesis z(cid:96)
(2)

R and its part hypothesis z(cid:96)−1
Ri)  where θ(cid:96)

log pR(cid:48)i(cid:48) + (cid:88)

R;V) = (cid:88)

log p(F  X  z1:(cid:96)−1 | z(cid:96)

log p(F  X | z1
i(cid:48)) 

  v(cid:96)
i − x(cid:96)

Ri = (µ(cid:96)

Ri  Σ(cid:96)

i

where E(S(z(cid:96)
R)  respectively.
The last term is the likelihood of the Gabor features under a particular contour fragment hypothesis.

R)) denote the edges and vertices of the subgraph S(z(cid:96)

R)) and V (S(z(cid:96)

zR(cid:48) zi(cid:48)∈E(S(z(cid:96)

R))

i(cid:48)∈V (S(z(cid:96)
z1

R))

3 Multi-class learning strategies

N(cid:88)

We ﬁrst deﬁne the objective function for multi-class learning and show how different learning strate-
gies can be used with it in the following subsections. Our goal is to ﬁnd a hierarchical vocabulary
V that well represents the distribution p(I | C) ≈ p(F  X | C;V) at minimal complexity of the
representation (C denotes the class variable). Speciﬁcally  we seek for a vocabulary V = ∪(cid:96)V (cid:96) that
optimizes the function f over the data D = {(Fn  Xn  Cn)}N

n=1 (N training images):

f(V) = L(D | V) − λ · T (V)

(3)

f(V)  where
The ﬁrst term in (3) represents the log-likelihood:

V∗ = arg max

V

p(Fn  Xn  z | C;V) 

L(D | V) =

log p(Fn  Xn | C;V) =

n=1

n=1

(4)
while T (V) penalizes the complexity of the model [21] and λ controls the amount of penalization.
Several approximations are made to learn the vocabulary; namely  the vocabulary is learned layer by
layer (in a bottom-up way) by ﬁnding frequent spatial layouts of parts from the previous layer [13]
and then using f to select a minimal set of models at each layer that still produce a good whole-
object shape representation at the ﬁnal  object layer [21]. The top layer models are validated on a
set of validation images and those yielding a high rate of false-positives are removed from V.
We next show how different training strategies are performed to learn a joint multi-class vocabulary.

z

N(cid:88)

log(cid:88)

Independent training of individual classes

3.1
In independent training  a class speciﬁc vocabulary Vc is learned using the training images of each
particular class C = c. We learn Vc by maximizing f over the data D = {(Fn  Xn  C = c)}. For
the negative images in the validation step  we randomly sample images from other classes. The joint
multi-class representation V is then obtained by stacking the class speciﬁc vocabularies Vc together 
V (cid:96) = ∪cV (cid:96)
c is the only layer common to all
classes (6 oriented contour fragments)  thus V 1 = V 1
c .

c (the edges E are added accordingly). Note that V 1

3

3.2 Joint training of classes

In joint training  the learning phase has two steps.
In the ﬁrst step  the training data D for all
the classes is presented to the algorithm simultaneously  and is treated as unlabeled. The spatial
parameters θ of the models at each layer are then inferred from images of all classes  and will code
“average” spatial part dispositions. The joint statistics also inﬂuences the structure of the models by
preferring those that are most repeatable over the classes. This way  the jointly learned vocabulary
V will be the best trade-off between the likelihood L and the complexity T over all the classes in the
dataset. However  the ﬁnal  top-level likelihood for each particular class could be low because the
more discriminative class-speciﬁc information has been lost. Thus  we employ a second step which
revisits each class separately. Here  we use the joint vocabulary V and add new models v(cid:96)
R to each
layer (cid:96) if they further increase the score f for each particular class. This procedure is similar to that
used in sequential training and will be explained in more detail in the following subsection. Object
layer VO is consequently learned and added to V for each class. We validate the object models after
all classes have been trained. A similarity measure is used to compare every two classes based on the
degree of feature sharing between them. In validation  we choose the negative images by sampling
the images of the classes according to the distribution deﬁned by the similarity measure. This way 
we discard the models that poorly discriminate between the similar classes.

3.3 Sequential training of classes

When training the classes sequentially  we train on each class separately  however  our aim is to
1.) maximize the re-use of compositions learned for the previous classes  and 2.) add those missing
(class-speciﬁc) compositions that are needed to represent class k sufﬁciently well. Let V1:k−1 denote
the vocabulary learned for classes 1 to k − 1. To learn a novel class k  for each layer (cid:96) we seek a
new set of shape models that maximizes f over the data D = {(Fn  Xn  C = k)} conditionally on
the already learned vocabulary V (cid:96)
1:k−1. This is done by treating the hypotheses inferred with respect
to V (cid:96)
1:k−1 as ﬁxed  which gives us a starting value of the score function f. Each new model v(cid:96)
is then evaluated and selected conditionally on this value  i.e such that the difference f(V (cid:96)
1:k−1 ∪
R) − f(V (cid:96)
1:k−1) is maximized. Since according to the deﬁnition in (4) the likelihood L increases
v(cid:96)
the most when the hypotheses have largely disjoint supports  we can greatly speed up the learning
process: the models need to be learned only with respect to those (F  X) in an image that have a
low likelihood under the vocabulary V (cid:96)

1:k−1  which can be determined prior to training.

R

4 Experimental results

We have evaluated the hierarchical multi-class learning strategies on several object classes. Specif-
ically  we used: UIUC multi-scale cars [22]  GRAZ [4] cows and persons  Weizmann multi-scale
horses (adapted by Shotton et al. [23])  all ﬁve classes from the ETH dataset [24]  and all ten classes
from TUD shape2 [25]. Basic information is given in Table 1. A 6-layer vocabulary was learned. 1
The bounding box information was used during training.
When evaluating detection performance  a detection will be counted as correct  if the predicted
bounding box coincides with groundtruth more than 50%. On the ETH dataset alone  this threshold
is lowered to 0.3 to enable a fair comparison with the related work [24]. The performance will be
given either with recall at equal error rate (EER)  positive detection rate at low FPPI  or as classif.-
by-detection (on TUD shape2)  depending on the type of results reported on that dataset thus-far.
To evaluate the shareability of compositions between the classes  we will use the following measure:

deﬁned for each layer (cid:96) separately. By “v(cid:96)
connecting any of the class speciﬁc shapes V O

R∈V (cid:96)
v(cid:96)
R used by class C” it is meant that there is a path of edges
R. To give some intuition behind the measure:
1The number of layers depends on the objects’ size in the training images (it is logarithmic with the number
of non-overlapping contour fragments in an image). To enable a consistent evaluation of feature sharing  etc 
we have scaled the training images in a way which produced the whole-shape models at layer 6 for each class.

C and v(cid:96)

(cid:88)

deg share((cid:96)) =

1
|V (cid:96)|

(# of classes that use v(cid:96)
# of all classes − 1

R) − 1

 

4

deg share = 0 if no shape from layer (cid:96) is shared (each class uses its own set of shapes)  and it is 1
if each shape is used by all the classes. Beside the mean (which deﬁnes deg share)  the plots will
also show the standard deviation. In sequential training  we can additionally evaluate the degree of
re-use when learning each novel class. Higher re-use means lower training time and a more compact
representation. We expect a tendency of higher re-use as the number k of classes grows  thus we
deﬁne it with respect to the number of learned classes:
# of v(cid:96)
# of all v(cid:96)

1:k−1 used by ck
1:k used by ck

deg transfer(k  (cid:96)) =

R ∈ V (cid:96)

R ∈ V (cid:96)

(5)

Evaluation was performed by progressively increasing the number of object classes (from 2 to 10).
The individual training will be denoted by I  joint by J  and sequential by S.
Table 2 relates the detection performances of I to those of the related work. On the left side  we
report detection accuracy at low FPPI rate for the ETH dataset  averaged over 5 random splits of
training/test images as in [24]. On the right side  recall at EER is given for a number of classes.
Two classes. We performed evaluation on two visually very similar classes (cow  horse)  and two
dissimilar classes (person  car). Table 3 gives information on 1.) size (the number of compositions
at each layer)  2.) training and 3.) inference times  4.) recall at EER. In sequential training  both
possible orders were used (denoted with S1 and S2) to see whether different learning orders (of
classes) affect the performance. The ﬁrst two rows show the results for each class individually 
while the last row contains information with respect to the conjoined representations. Already for
two classes  the cumulative training time is slightly lower for S than I  while both being much
smaller than that of J.
Degree of sharing. The hierarchies learned in I  J  and S on cows and horses  and J for car-person
are shown in Fig. 2 in a respective order from left to right. The red nodes depict cow/car and blue
horse/person compositions. The green nodes depict the shared compositions. We can observe a
slightly lower number of shareable nodes for S compared to J  yet still the lower layers for cow-
horse are almost completely re-used. Even for the visually dissimilar classes (car-person) sharing is
present at lower layers. Numerically  the degrees of sharing and transfer are plotted in Fig. 1.
Detection rate. The recall values for each class are reported in Table 3. Interestingly  “knowing”
horses improved the performance for cows. For car-person  individual training produced the best
result  while training person before car turned out to be a better strategy for S. Fig. 1 shows the
detection rates for cows and horses on the joint test set (the strongest class hypothesis is evaluated) 
which allows for a much higher false-positive rate. We evaluate it with F-measure (to account for
FP). A higher performance for all joint representations over the independent one can be observed.
This is due to the high degree of sharing in J and S  which puts similar hypotheses in perspective
and thus discriminates between them better.
Five classes. The results for ETH-5 are reported in Table 4. We used half of the images for training 
and the other half for testing. The split was random  but the same for I  J  and S. We also test
whether different orders in S affect performance (we report an average over 3 random S runs).
Ordering does slightly affect performance  which means we may try to ﬁnd an optimal order of
classes in training. We can also observe that the number of compositions at each layer is higher for
S as for J (both being much smaller than I)  but this only slightly showed in inference times.
Ten classes. The results on TUD-10 are presented in Table 5. A few examples of the learned shapes
for S are shown in Fig. 3. Due to the high training complexity of J  we have only ran J for 2  5 and
10 classes. We report classif.-by-detection (the strongest class hypothesis in an image must overlap
with groundtruth more than 50%). To demonstrate the strength of our representation  we have also
ran (linear) SVM on top of hypotheses from Layers 1− 3  and compared the performances. Already
here  Layer 3 + SVM outperforms prior work [25] by 10%. Fig. 4-(11.) shows classiﬁcation as a
number of learned classes. Our approach consistently outperforms SVM  which is likely due to the
high scale- and rotation- variability of images with which our approach copes well. Fig. 4 shows:
inference time  cumulative training time  degree of sharing (for the ﬁnal 10-class repr.)  transfer  and
classiﬁcation rates as a function of the number of learned classes.
Vocabulary size. The top row in Fig 4 shows representation size for I  J and S as a function
of learned classes. With respect to worst case (I)  both J and S have a highly sublinear growth.
Moreover  in layers 2 and 3  where the burden on inference is the highest (the highest number of

5

inferred hypotheses)  an almost constant tendency can be seen. We also compare the curves with
those reported for a ﬂat approach by Opelt et al. [4] in Fig 4-(5). We plot the number of models at
Layer 5 which are approximately of the same granularity as the learned boundary parts in [4]. Both 
J and S hierarchical learning types show a signiﬁcantly better logarithmic tendency as in [4].
Fig 4-(6) shows the size of the hierarchy ﬁle stored on disk. It is worth emphasizing that the hierarchy
subsuming 10 classes uses only 0.5Mb on disk and could ﬁt on an average mobile device.
50 classes. To increase the scale of the experiments we show the performance of sequential training
on 50 classes from LabelMe [1]. The results are presented in Fig. 5. For I in the inference time plot
we used the inference time for the ﬁrst class linearly extrapolated with the number of classes. We
can observe that S achieves much lower inference times than I  although it is clear that for a higher
number of classes more research is needed to cut down the inference times to a practical value.

Figure 1: From left to right: 1.) detection rate (F measure) on the joint cow-horse test set. 2.) degree of
sharing for cow-horse  3.) car-person vocabularies  4.) an example detection of a person and horse.

Figure 2: Learned 2-class vocabularies for different learning types (the nodes depict the compositions v(cid:96)
links represent the edges e(cid:96)
hierarchy for 1.) I  2.) J  3.) S1  and 4.) car-person J. Green nodes denoted shared compositions.

R  the
Ri between them — the parameters θ(cid:96) are not shown). From left to right: cow-horse

5 Conclusions and discussion

independent  2).

We evaluated three types of multi-class learning strategies in a hierarchical compositional frame-
work  namely 1.)
joint  and 3.) sequential training. A comparison was made
through several important computational aspects as well as by detection performance. We conclude
that: 1.) Both joint and sequential training strategies exert sublinear growth in vocabulary size (more
evidently so in the lower layers) and  consequently  sublinear inference time. This is due to a high
degree of sharing and transfer within the resulting vocabularies. The hierarchy obtained by sequen-
tial training grows somewhat faster  but not signiﬁcantly so. 2.) Training time was expectedly worst
for joint training  while training time even reduced with each additional class during sequential
training. 3.) Different training orders of classes did perform somewhat differently — this means we
might try to ﬁnd an “optimal” order of learning. 4.) Training independently has mostly yielded the
best detection rates  but the discrepancy with the other two strategies was low. For similar classes
(cow-horse)  sequential learning even improved the detection performance  and was in most cases
above the joint’s performance. By training sequentially  we can learn class speciﬁc features (yet
still have a high degree of sharing) which boost performance. Most importantly  sequential training
has achieved the best trade-off between detection performance  re-usability  inference and training
time. The observed computational properties of all the strategies in general  and sequential learning
in particular  go well beyond the reported behavior of ﬂat approaches [4]. This makes sequential
learning of compositional hierarchies suitable for representing the classes on a larger scale.

Acknowledgments

This research has been supported in part by the following funds: EU FP7-215843 project POETICON  EU
FP7-215181 project CogX  Research program Computer Vision P2-0214 and Project J2-2221 (ARRS).

6

IJS1S20.70.750.80.850.90.951Detection rate: JOINT cow+horse datasetlearning typeF−measure1234500.10.20.30.40.50.60.70.80.91Degree of sharing / re−use: cow−horselayerdegree of sharing  Joint trainingSequential 1: cow + horseSequential 2: horse + car1234500.10.20.30.40.50.60.70.80.91Degree of sharing / re−use: car−personlayerdegree of sharing  Joint trainingSequential 1: car + personSequential 2: person + carlayer 1 layer 2

layer 3

layer 4

layer 5

layer 6

hammer

pliers

saucepan

scissors
Figure 3: A few examples from the learned hierarchical shape vocabulary for S on TUD-10. Each shape in
the hierarchy is a composition of shapes from the layer below. Only the mean of each shape is shown.

method
Stark et al.[25]
Level 1 + SVM
Level 2 + SVM
Level 3 + SVM
Independent
Joint
Sequential

size on disk
/
206 Kb
3  913 Kb
34  508 Kb
1  249 Kb
408 Kb
490 Kb

classf. rate
44%
32%
44% train. time
54%
207 min
71%
752 min
69%
151 min
71%

L4
159
39
49
Table 5: Results on the TUD-10. Classiﬁcation obtained as classiﬁcation-by-detection.

L2
74
14
9

L3
96
23
21

infer. time
12.2 sec
2.0 sec
2.4 sec

size of representation

L5
181
59
76

Figure 4: Results on TUD-10. Top: (1-4) repr. size as a function of the number of learned classes. Middle:
5.) repr. size compared to [4]  6.) size of hierarchy on disk  7.) avg. inference time per image  8.) cumulative
train. time. Bottom: degree of 9.) sharing and 10.) transfer  11.) classif. rates  12.) example detection of cup.

Figure 5: Results on 50 object classes from LabelMe [1]. From left to right: Size of representation (number of
compositions per layer)  inference times  and deg transfer  all as a function of the number of learned classes.

7

12345678910010203040506070number of classessize of representationGrowth of representation  independentjointsequentialLayer 212345678910020406080100number of classessize of representationGrowth of representation  independentjointsequentialLayer 312345678910050100150number of classessize of representationGrowth of representation  independentjointsequentialLayer 412345678910050100150number of classessize of representationGrowth of representation  independentjointsequentialLayer 5123456789100100200300400500number of classessize of representationGrowth of representation  Opelt et al.independentjointsequential246810024681012number of classesinference time (sec)Inference time per image  independentjointsequential2468100200400600800100012001400number of classescumulative size on disk (Kb)Size on disk  independentjointsequentialLayer 2 + SVMLayer 3 + SVM246810050100150200number of classescumulative training time (min)Cumulative training time  independentsequential1234500.10.20.30.40.50.60.70.80.91Degree of sharing: TUDlayerdegree of sharing  Joint trainingSequential: alphabetical order1234567891000.10.20.30.40.50.60.70.80.91Degree of transfer per layernumber of learnt classesdegree of transfer  Layer 1Layer 2Layer 3Layer 4Layer 52468405060708090100number of classesClassification rate (%)Classification rate  sequentialSVM on Layer 2SVM on Layer 31020304050050100150200250300number of classesnumber of compositionsSize of representation (# classes)  Layer 1Layer 2Layer 3Layer 4Layer 51020304050050100150200250300350number of classesinference time per image (sec)Inference time per image (# classes)  independentsequential510152025303540455000.10.20.30.40.50.60.70.80.91Degree of transfer per layernumber of learnt classesdegree of transfer  Layer 1Layer 2Layer 3Layer 4Layer 52
S

1
S

J

I

5
.
8
9

3
.
4
9

.

4
6
9

4
.
2
9

4
.
0
6

4
.
6
7

9
.
6
9

4
.
3
9

6
.
5
9

5
.
3
9

3
.
6
5

9
.
4
7

9
.
6
9

4
.
3
9

6
.
5
9

7
.
1
9

3
.
8
5

0
.
5
7

9
.
6
9

3
.
4
9

6
.
5
9

5
.
3
9

4
.
0
6

.

0
7
7

2
S

0
.
2

7
.
2

5
.
2

4
.
5

0
.
3

0
.
5

1
S

1
.
2

7
.
2

6
.
2

3
.
5

8
.
2

9
.
4

J

0
.
2

6
.
2

5
.
2

2
.
5

6
.
2

8
.
4

I

9
.
1

3
.
2

3
.
4

4
.
3

3
.
2

3
.
6

2
S

9
1

0
2

9
3

3
3

7
1

0
5

1
S

5
2

7
1

2
4

5
3

5
1

0
5

J

/

/

5
6

/

/

5
8

I

5
2

0
2

5
4

5
3

7
1

2
5

5
L

0
2

4
2

8
3

8
1

1
2

8
3

I
P
P
F

)

%

(

e
t
a
r

.
t
e
d

.
t
e
s
a
t
a
d

e
p
a
h
s
H
T
E
s
s
a
l
c
−
5
e
h
t

)
c
e
s
(
e
m

i
t
.
r
f
n
i

)
n
i
m
(
e
m

i
t
.

n
r
t

S

J

I

S

J

I

S

J

8
2
.
0

2
3
.
0

8
1
.
0

2
2
.
0

1
2
.
0

7
2
.
0

4
3
.
0

6
1
.
0

2
2
.
0

2
2
.
0

4
2
0

.

4
2
0

.

4
3
.
0

4
.
0

9
1
.
0

1
3
.
0

8
2
.
0

0
3
.
0

4
.
6
8

0
.
0
8

6
.
4
8

3
.
3
8

7
.
2
7

4
.
1
8

4
.
6
8

0
.
0
8

3
.
1
8

3
.
3
8

7
.
9
6

1
.
0
8

6
.
8
8

5
.
5
8

4
.
2
8

9
.
4
8

8
.
5
7

.

4
3
8

1
.
2
1

1
.
2
1

1
.
2
1

1
.
2
1

1
.
2
1

1
.
2
1

1
.
1
1

1
.
1
1

1
.
1
1

1
.
1
1

1
.
1
1

1
.
1
1

I

6
.
3

4
.
3

2
.
3

6
.
3

8
.
2

S

3
2

1
2

6
2

8
1

2
1

J

/

/

/

/

/

I

3
2

5
2

1
3

1
3

7
1

6
.
6
1

0
0
1

5
3
2

7
2
1

)
7
.
3
1
(
9
7

)
1
+
2
(

2
S

4
L

3
L

0
2

8
1

9
2

8
1

9
1

1
3

7
1

2
1

9
1

9
1

6
1

5
2

2
L

4
1

2
1

4
1

1
1

9

1
1

5
L

5
2

1
2

0
4

6
1

3
2

9
3

)
2
+
1
(

1
S

4
L

3
L

3
2

4
2

3
3

3
1

5
1

7
2

7
1

8
1

1
2

0
1

2
1

9
1

f
o

r
e
b
m
u
n
(

n
o
i
t
a
t
n
e
s
e
r
p
e
r

f
o

e
z
i
s

2
L

7
1

8
1

8
1

6

1
1

2
1

5
L

5
2

7
2

6
3

0
2

2
2

2
4

4
L

7
2

6
2

0
3

0
2

4
1

5
2

J

3
L

5
2

6
2

6
2

6
1

2
1

8
1

2
L

7
1

7
1

7
1

7

7

7

5
L

5
2

4
2

9
4

6
1

1
2

7
3

4
L

3
2

8
1

1
4

3
1

9
1

2
3

I

3
L

7
1

2
1

9
2

0
1

6
1

6
2

2
L

7
1

2
1

9
2

6

9

5
1

n
o

s
e
p
y
t

g
n
i
n
r
a
e
l

t
n
e
r
e
f
f
i
d

r
o
f

s
t
l
u
s
e
R

:
4
e
l
b
a
T

n
o
i
t
a
t
n
e
s
e
r
p
e
r

f
o

e
z
i
s

)
5
.
7
(
3
2

)
6
.
3
(
2
2

)
2
.
1
(
0
3

)
9
.
4
(
9
2

)
5
.
1
(
7
2

)
7
.
2
1
(
7
2

)
2
(
8
2

)
7
.
1
(
5
3

)
7
.
4
(
0
3

)
0
.
4
(
0
3

)
5
.
9
(
1
6

)
7
.
1
(
5
2

)
1
.
8
(
2
2

)
9
.
5
(
8
2

)
9
.
7
(
5
2

)
4
.
6
(
3
2

)
5
.
2
(
3
3

5
L

s
n
u
r
3
r
e
v
o

4
L

-

)
d
t
s
(
n
a
e
m

3
L

 

S

2
L

)
6
.
0
(
0
1

)
6
.
0
(
9

)
6
.
0
(
0
1

)
6
.
0
(
0
1

)
0
(
0
1

)
6
.
0
(
1
1

5
L

8
2

2
2

6
2

4
3

7
2

5
5

4
L

1
2

1
2

6
2

5
2

6
2

2
3

J

3
L

5
1

6
1

9
1

9
1

0
2

2
2

2
L

4
1

3
1

5
1

6
1

6
1

6
1

5
L

8
2

2
2

7
3

3
2

6
2

4
L

7
2

2
2

2
2

5
2

9
2

I

6
3
1

5
2
1

3
L

0
3

1
1

3
1

6
1

8
1

8
8

2
L

1
1

7

5

9

1
1

3
4

)

%

(

R
E
E

t
a

.
c
e
r

)
c
e
s
(

e
m

i
t

.
r
e
f
n
i

)
n
i
m

(

e
m

i
t

.

n
i
a
r
t

)
r
e
y
a
l

r
e
p

s
n
o
i
t
i
s
o
p
m
o
c

.
s
e
s
s
a
l
c
n
o
s
r
e
p
-
r
a
c

d
n
a

 
e
s
r
o
h
-
w
o
c

e
h
t

n
o

s
e
p
y
t

g
n
i
n
r
a
e
l

t
n
e
r
e
f
f
i
d

r
o
f

s
t
l
u
s
e
R

:
3
e
l
b
a
T

.

n
o
i
t
a
m
r
o
f
n
i

t
e
s
a
t
a
D

:
1

e
l
b
a
T

0
2

8
2
2

0
4

8
0
1

9
1

9
1

e
s
r
o
h

.

m
z
i
e

W

r
a
c

C
U
U

I

n
o
s
r
e
p

z
a
r
G

0
2

5
6

w
o
c

0
2

0
1

0
2

0
1

s
r
o
s
s
i
c
s

n
a
p
e
c
u
a
s

0
2

0
1

0
2

0
1

0
2

0
1

0
2

0
1

0
2

0
1

0
2

0
1

)
t
s
e
t
(
2
e
p
a
h
s
D
U
T
+

)
n
i
a
r
t
(
1
e
p
a
h
s
D
U
T

t
o
p

s
r
e
i
l
p

n
a
p

g
u
m

e
f
i
n
k

r
e
m
m
a
h

0
2

0
1

k
r
o
f

p
u
c

0
2

0
1

5
1

7
1

4
2

4
2

0
3

7
5

n
a
w
s

g
u
m

e
f
f
a
r
i
g

e
p
a
h
s
H
T
E

1
2

7
2

9
1

1
2

e
l
t
t
o
b

e
l
p
p
a

t
e
s
a
t
a
d

s
s
a
l
c

n
i
a
r
t

t
s
e
t

#

#

l
a
u
t
c
a

e
h
t

t
r
o
p
e
r

o
s
l
a

e
w
e
l
i
h
w

 
k
r
o
w
d
e
t
a
l
e
r

y
l
t
c
e
r
i
d

t
o
n

s
u
h
t

e
r
a

d
n
a
(

n
o
i
t
a
m
r
o
f
n
i

r
u
o
t
n
o
c

e
h
t

t
s
u
j

r
o
f

I
P
P
F

4
.
0

t
a

)

%
n
i
(

e
t
a
r
-
n
o
i
t
c
e
t
e
d

e
g
a
r
e
v
A

:
t
f
e
L

.

k
r
o
w
d
e
t
a
l
e
r

h
t
i

w
s
e
t
a
r

n
a
h
t

e
r
o
m
e
s
u

t
a
h
t

s
e
h
c
a
o
r
p
p
a

e
h
T

.
s
e
s
s
a
l
c

s
u
o
i
r
a
v

r
o
f

R
E
E

t
a

l
l
a
c
e
R

:
t
h
g
i
R

.

y
a
r
g

d
e
d
a
h
s

e
r
a

)
s
r
u
o

o
t

e
l
b
a
r
a
p
m
o
c

n
o
i
t
c
e
t
e
d

.
e
s
a
b
a
t
a
d

f
o

n
o
s
i
r
a
p
m
o
C

:
2

e
p
a
h
s
H
T
E
e
h
t

r
o
f

e
l
b
a
T

 
I
P
P
F

.

4
0
6

.

5
3
9

.

3
4
9

5
.
8
9

n
o
s
r
e
p

]
4
[
6
.
2
5

]
3
2
[
4
.
2
5

]
8
2
[
0
.
0
5

e
l
a
c
s

r
a
c

]
7
2
[
6
.
0
9

]
9
2
[
5
.
3
9

]
3
1
[
1
.
2
9

/

e
s
r
o
h

]
3
2
[
0
.
9
8

]
8
2
[
0
.
3
9

w
o
c

]
4
[

]
7
[

.
0
0
1

.
0
0
1

]
3
2
[
5
.
8
9

k
r
o
w
d
e
t
a
l
e
R

.

n
i
a
r
t

.

d
n
i

e
g
a
r
e
v
a

n
a
w
s

8
.
6
7

8
.
4
8

7
.
3
8

)
4
.
3
1
(
4
.
5
7

)
4
.
8
(
0
.
4
8

)
4
.
5
(
2
.
8
7

I
P
P
F
6
2
.
0

g
u
m

)
6
.
8
(
6
.
3
8

)
1
.
5
(
7
.
2
8

.

)
3
2
(
6
4
8

I
P
P
F
7
2
0

.

.

e
f
f
a
r
i
g

)
6
.
4
1
(
6
.
8
5

)
4
.
5
(
5
.
0
9

)
3
.
4
(
3
.
3
8

I
P
P
F
1
2
.
0

e
l
t
t
o
b

)
5
.
7
(
2
.
3
8

)
1
.
6
(
8
.
6
7

.

)
8
2
(
2
6
8

I
P
P
F
6
3
0

.

.

o
g
o
l
e
l
p
p
a

)
7
.
1
(
2
.
3
8

)
5
.
4
(
9
.
9
8

)
6
.
2
(
3
.
7
8

I
P
P
F
2
3
.
0

.

]
4
2
[

]
6
2
[

n
i
a
r
t

.

d
n
i

)
2
(

e
s
r
o
h

)
1
(
w
o
c

)
2
(

)
1
(

r
a
c

n
o
s
r
e
p

.
s
r
h
+
w
o
c

.

n
s
r
p
+
r
a
c

s
s
a
l
c

s
s
a
l
c

8

o
g
o
l
e
l
p
p
a

e
l
t
t
o
b

e
f
f
a
r
i
g

g
u
m

n
a
w
s

l
l
a

References
[1] Russell  B.  Torralba  A.  Murphy  K.  and Freeman  W. T. (2008) Labelme: a database and web-based

tool for image annotation. IJCV  77  157–173.

[2] Leibe  B.  Leonardis  A.  and Schiele  B. (2008) Robust object detection with interleaved categorization

and segmentation. IJCV  77  259–289.

[3] Torralba  A.  Murphy  K. P.  and Freeman  W. T. (2007) Sharing visual features for multiclass and multi-

view object detection. IEEE PAMI  29  854–869.

[4] Opelt  A.  Pinz  A.  and Zisserman  A. (2008) Learning an alphabet of shape and appearance for multi-

class object detection. IJCV  80  16–44.

[5] Fei-Fei  L.  Fergus  R.  and Perona  P. (2004) Learning generative visual models from few training ex-
amples: an incremental bayesian approach tested on 101 object categories. IEEE CVPR’04 Workshop on
Generative-Model Based Vision.

[6] Krempp  S.  Geman  D.  and Amit  Y. (2002) Sequential learning of reusable parts for object detection.

Tech. rep.

[7] Todorovic  S. and Ahuja  N. (2007) Unsupervised category modeling  recognition  and segmentation in

images. IEEE PAMI.

[8] Zhu  S. and Mumford  D. (2006) A stochastic grammar of images. Found. and Trends in Comp. Graphics

and Vision  2  259–362.

[9] Ranzato  M. A.  Huang  F.-J.  Boureau  Y.-L.  and LeCun  Y. (2007) Unsupervised learning of invariant

feature hierarchies with applications to object recognition. CVPR.

[10] Ullman  S. and Epshtein  B. (2006) Visual Classiﬁcation by a Hierarchy of Extended Features.. Towards

Category-Level Object Recognition  Springer-Verlag.

[11] Sivic  J.  Russell  B. C.  Zisserman  A.  Freeman  W. T.  and Efros  A. A. (2008) Unsupervised discovery

of visual object class hierarchies. CVPR.

[12] Bart  I.  Porteous  E.  Perona  P.  and Wellings  M. (2008) Unsupervised learning of visual taxonomies.

CVPR.

[13] Fidler  S. and Leonardis  A. (2007) Towards scalable representations of visual categories: Learning a

hierarchy of parts. CVPR.

[14] Scalzo  F. and Piater  J. H. (2005) Statistical learning of visual feature hierarchies. W. on Learning  CVPR.
[15] Zhu  L.  Lin  C.  Huang  H.  Chen  Y.  and Yuille  A. (2008) Unsupervised structure learning: Hierarchical

recursive composition  suspicious coincidence and competitive exclusion. ECCV  vol. 2  pp. 759–773.

[16] Fleuret  F. and Geman  D. (2001) Coarse-to-ﬁne face detection. IJCV  41  85–107.
[17] Schwartz  J. and Felzenszwalb  P. (2007) Hierarchical matching of deformable shapes. CVPR.
[18] Ommer  B. and Buhmann  J. M. (2007) Learning the compositional nature of visual objects. CVPR.
[19] Serre  T.  Wolf  L.  Bileschi  S.  Riesenhuber  M.  and Poggio  T. (2007) Object recognition with cortex-

like mechanisms. IEEE PAMI  29  411–426.

[20] Sudderth  E.  Torralba  A.  Freeman  W. T.  and Willsky  A. (2008) Describing visual scenes using trans-

formed objects and parts. IJCV  pp. 291–330.

[21] Fidler  S.  Boben  M.  and Leonardis  A. (2009) Optimization framework for learning a hierarchical shape

vocabulary for object class detection. BMVC.

[22] Agarwal  S.  Awan  A.  and Roth  D. (2004) Learning to detect objects in images via a sparse  part-based

representation. IEEE PAMI  26  1475–1490.

[23] Shotton  J.  Blake  A.  and Cipolla  R. (2008) Multi-scale categorical object recognition using contour

fragments. PAMI  30  1270–1281.

[24] Ferrari  V.  Fevrier  L.  Jurie  F.  and Schmid  C. (2007) Accurate object detection with deformable shape

models learnt from images. CVPR.

[25] Stark  M. and Schiele  B. (2007) How good are local features for classes of geometric objects? ICCV.
[26] Fritz  M. and Schiele  B. (2008) Decomposition  discovery and detection of visual categories using topic

models. CVPR.

[27] Mutch  J. and Lowe  D. G. (2006) Multiclass object recognition with sparse  localized features. CVPR 

pp. 11–18.

[28] Shotton  J.  Blake  A.  and Cipolla  R. (2008) Efﬁciently combining contour and texture cues for object

recognition. BMVC.

[29] Ahuja  N. and Todorovic  S. (2008) Connected segmentation tree – a joint representation of region layout

and hierarchy. CVPR.

9

,Giorgio Patrini
Tiberio Caetano
Zelun Luo
Yuliang Zou
Judy Hoffman
Li Fei-Fei
Wenye Li
Jingwei Mao
Yin Zhang
Shuguang Cui