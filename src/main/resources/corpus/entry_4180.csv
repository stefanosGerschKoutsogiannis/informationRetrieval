2017,Overcoming Catastrophic Forgetting by Incremental Moment Matching,Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here  we propose a method  i.e. incremental moment matching (IMM)  to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task  respectively. To make the search space of posterior parameter smooth  the IMM procedure is complemented by various transfer learning techniques including weight transfer  L2-norm of the old and the new parameter  and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST  CIFAR-10  Caltech-UCSD-Birds  and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.,Overcoming Catastrophic Forgetting by

Incremental Moment Matching

Sang-Woo Lee1  Jin-Hwa Kim1  Jaehyun Jun1  Jung-Woo Ha2  and Byoung-Tak Zhang1 3

Seoul National University1

Clova AI Research  NAVER Corp2

Surromind Robotics3

{slee jhkim jhjun}@bi.snu.ac.kr jungwoo.ha@navercorp.com

btzhang@bi.snu.ac.kr

Abstract

Catastrophic forgetting is a problem of neural networks that loses the information
of the ﬁrst task after training the second task. Here  we propose a method  i.e. in-
cremental moment matching (IMM)  to resolve this problem. IMM incrementally
matches the moment of the posterior distribution of the neural network which is
trained on the ﬁrst and the second task  respectively. To make the search space
of posterior parameter smooth  the IMM procedure is complemented by various
transfer learning techniques including weight transfer  L2-norm of the old and the
new parameter  and a variant of dropout with the old parameter. We analyze our ap-
proach on a variety of datasets including the MNIST  CIFAR-10  Caltech-UCSD-
Birds  and Lifelog datasets. The experimental results show that IMM achieves
state-of-the-art performance by balancing the information between an old and a
new network.

1

Introduction

Catastrophic forgetting is a fundamental challenge for artiﬁcial general intelligence based on neural
networks. The models that use stochastic gradient descent often forget the information of previous
tasks after being trained on a new task [1  2]. Online multi-task learning that handles such problems
is described as continual learning. This classic problem has resurfaced with the renaissance of deep
learning research [3  4].
Recently  the concept of applying a regularization function to a network trained by the old task to
learning a new task has received much attention. This approach can be interpreted as an approxima-
tion of sequential Bayesian [5  6]. Representative examples of this regularization approach include
learning without forgetting [7] and elastic weight consolidation [8]. These algorithms succeeded in
some experiments where their own assumption of the regularization function ﬁts the problem.
Here  we propose incremental moment matching (IMM) to resolve the catastrophic forgetting prob-
lem. IMM uses the framework of Bayesian neural networks  which implies that uncertainty is intro-
duced on the parameters in neural networks  and that the posterior distribution is calculated [9  10].
The dimension of the random variable in the posterior distribution is the number of the parameters
in the neural networks. IMM approximates the mixture of Gaussian posterior with each component
representing parameters for a single task to one Gaussian distribution for a combined task. To merge
the posteriors  we introduce two novel methods of moment matching. One is mean-IMM  which
simply averages the parameters of two networks for old and new tasks as the minimization of the
average of KL-divergence between one approximated posterior distribution for the combined task

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Geometric illustration of incremental moment matching (IMM). Mean-IMM simply av-
erages the parameters of two neural networks  whereas mode-IMM tries to ﬁnd a maximum of the
mixture of Gaussian posteriors. To make IMM be reasonable  the search space of the loss function
between the posterior means µ1 and µ2 should be reasonably smooth and convex-like. To ﬁnd a
µ2 which satisﬁes this condition of a smooth and convex-like path from µ1  we propose applying
various transfer techniques for the IMM procedure.

and each Gaussian posterior for the single task [11]. The other is mode-IMM  which merges the pa-
rameters of two networks using a Laplacian approximation [9] to approximate a mode of the mixture
of two Gaussian posteriors  which represent the parameters of the two networks.
In general  it is too naïve to assume that the ﬁnal posterior distribution for the whole task is Gaussian.
To make our IMM work  the search space of the loss function between the posterior means needs to
be smooth and convex-like. In other words  there should not be high cost barriers between the means
of the two networks for an old and a new task. To make our assumption of Gaussian distribution for
neural network reasonable  we applied three main transfer learning techniques on the IMM proce-
dure: weight transfer  L2-norm of the old and the new parameters  and our newly proposed variant
of dropout using the old parameters. The whole procedure of IMM is illustrated in Figure 1.

2 Previous Works on Catastrophic Forgetting

One of the major approaches preventing catastrophic forgetting is to use an ensemble of neural net-
works. When a new task arrives  the algorithm makes a new network  and shares the representation
between the tasks [12  13]. However  this approach has a complexity issue  especially in inference 
because the number of networks increases as the number of new tasks that need to be learned in-
creases.
Another approach studies the methods using implicit distributed storage of information  in typical
stochastic gradient descent (SGD) learning. These methods use the idea of dropout  maxout  or neu-
ral module to distributively store the information for each task by making use of the large capacity of
the neural network [4]. Unfortunately  most studies following this approach had limited success and
failed to preserve performance on the old task when an extreme change to the environment occurred
[3]. Alternatively  Fernando et al. [14] proposed PathNet  which extends the idea of the ensemble
approach for parameter reuse [13] within a single network. In PathNet  a neural network has ten or
twenty modules in each layer  and three or four modules are picked for one task in each layer by
an evolutionary approach. This method alleviates the complexity issue of the ensemble approach to
continual learning in a plausible way.
The approach with a regularization term also has received attention. Learning without forgetting
(LwF) is one example of this approach  which uses the pseudo-training data from the old task [7].
Before learning the new task  LwF puts the training data of the new task into the old network 
and uses the output as pseudo-labels of the pseudo-training data. By optimizing both the pseudo-
training data of the old task and the real data of the new task  LwF attempts to prevent catastrophic
forgetting. This framework is promising where the properties of the pseudo training set is similar to
the ideal training set. Elastic weight consolidation (EWC)  another example of this approach  uses
sequential Bayesian estimation to update neural networks for continual learning [8]. In EWC  the
posterior distribution trained by the previous task is used to update the new prior distribution. This
new prior is used for learning the new posterior distribution of the new task in a Bayesian manner.

2

1µ2µ1:2Modeµ1:2Meanµ111111:2121122()()Modeµµµ-----=S+SS+S1:212()/2Meanµµµ=+12µµ®2212µµ-1212()dropoutµµµ+×-Find !2  which makes !1:2	perform better drop-transferweight-transferL2-transferEWC assumes that the covariance matrix of the posterior is diagonal and there are no correlations
between the nodes. Though this assumption is fragile  EWC performs well in some domains.
EWC is a monumental recent work that uses sequential Bayesian for continual learning of neural
networks. However  updating the parameter of complex hierarchical models by sequential Bayesian
estimation is not new [5]. Sequential Bayes was used to learn topic models from stream data by
Broderick et al. [6]. Huang et al. applied sequential Bayesian to adapt a deep neural network to
the speciﬁc user in the speech recognition domain [15  16]. They assigned the layer for the user
adaptation and applied MAP estimation to this single layer. Similar to our IMM method  Bayesian
moment matching is used for sum-product networks  a kind of deep hierarchical probabilistic model
[17]. Though sum-product networks are usually not scalable to large datasets  their online learning
method is useful  and it achieves similar performance to the batch learner. Our method using moment
matching focuses on continual learning and deals with signiﬁcantly different statistics between tasks 
unlike the previous method.

3

Incremental Moment Matching

In incremental moment matching (IMM)  the moments of posterior distributions are matched in an
incremental way. In our work  we use a Gaussian distribution to approximate the posterior distri-
bution of parameters. Given K sequential tasks  we want to ﬁnd the optimal parameter µ∗
1:K and
Σ∗
1:K of the Gaussian approximation function q1:K from the posterior parameter for each kth task 
(µk  Σk).

p1:K ≡ p(θ|X1 ···   XK  y1 ···   yK) ≈ q1:K ≡ q(θ|µ1:K  Σ1:K)

pk ≡ p(θ|Xk  yk) ≈ qk ≡ q(θ|µk  Σk)

(1)
(2)

q1:K denotes an approximation of the true posterior distribution p1:K for the whole task  and qk
denotes an approximation of the true posterior distribution pk over the training dataset (Xk  yk) for
the kth task. θ denotes the vectorized parameter of the neural network. The dimension of µk and
µ1:k is D  and the dimension of Σk and Σ1:k is D × D  respectively  where D is the dimension of θ.
For example  a multi-layer perceptrons (MLP) with [784-800-800-800-10] has the number of nodes 
D = 1917610 including bias terms.
Next  we explain two proposed moment matching algorithms for the continual learning of modern
deep neural networks. The two algorithms generate two different moments of Gaussian with different
objective functions for the same dataset.

(cid:80)K

3.1 Mean-based Incremental Moment Matching (mean-IMM)

Mean-IMM averages the parameters of two networks in each layer  using mixing ratios αk with
k αk = 1. The objective function of mean-IMM is to minimize the following local KL-distance

or the weighted sum of KL-divergence between each qk and q1:K [11  18]:

(cid:80)K
k αkKL(qk||q1:K)
1:K =(cid:80)K

µ∗

1:K = argmin
µ1:K  Σ1:K

µ∗
1:K  Σ∗

1:K =(cid:80)K

Σ∗

k αkµk
k αk(Σk + (µk − µ∗

1:K)(µk − µ∗

1:K)T )

(3)

(4)
(5)

µ∗
1:K and Σ∗
1:K are the optimal solution of the local KL-distance. Notice that covariance information
is not needed for mean-IMM  since calculating µ∗
1:K does not require any Σk. A series of µk is
sufﬁcient to perform the task. The idea of mean-IMM is commonly used in shallow networks [19 
20]. However  the contribution of this paper is to discover when and how mean-IMM can be applied
in modern deep neural networks and to show it can performs better with other transfer techniques.
Future works may include other measures to merge the networks  including the KL-divergence be-

tween q1:K and the mixture of each qk (i.e. KL(q1:K||(cid:80)K

k αkqk)) [18].

3

3.2 Mode-based Incremental Moment Matching (mode-IMM)

Mode-IMM is a variant of mean-IMM which uses the covariance information of the posterior of
Gaussian distribution. In general  a weighted average of two mean vectors of Gaussian distributions
is not a mode of MoG. In discriminative learning  the maximum of the distribution is of primary
interest. According to Ray and Lindsay [21]  all the modes of MoG with K clusters lie on (K − 1)-
k ak = 1}.

dimensional hypersurface {θ|θ = ((cid:80)K

k µk)  0 < ak < 1 and(cid:80)

k )−1((cid:80)K

See Appendix A for more details.
Motivated by the above description  a mode-IMM approximate MoG with Laplacian approximation 
in which the logarithm of the function is expressed by the Taylor expansion [9]. Using Laplacian
approximation  the MoG is approximated as follows:

k akΣ−1

k akΣ−1

log q1:K ≈(cid:80)K

θT ((cid:80)K
1:K · ((cid:80)K
1:K = ((cid:80)K

k αk log qk + C = − 1
2
1:K = Σ∗
µ∗
Σ∗

k αkΣ−1
k )−1

k αkΣ−1

k µk)

k )θ + ((cid:80)K

k αkΣ−1

k αkΣ−1

k µk)θ + C(cid:48)

(6)

(7)
(8)

(cid:21)

(cid:20) ∂

For Equation 8  we add I to the term to be inverted in practice  with an identity matrix I and a small
constant .
Here  we assume diagonal covariance matrices  which means that there is no correlation among
parameters. This diagonal assumption is useful  since it decreases the number of parameters for
each covariance matrix from O(D2) to O(D) for the dimension of the parameters D.
For covariance  we use the inverse of a Fisher information matrix  following [8  22]. The main
idea of this approximation is that the square of gradients for parameters is a good indicator of their
precision  which is the inverse of the variance. The Fisher information matrix for the kth task  Fk is
deﬁned as:

Fk = E

ln p(˜y|x  µk) · ∂
∂µk

ln p(˜y|x  µk)T

∂µk

 

(9)

where the probability of the expectation follows x ∼ πk and ˜y ∼ p(y|x  µk)  where πk denotes an
empirical distribution of Xk.

4 Transfer Techniques for Incremental Moment Matching

In general  the loss function of neural networks is not convex. Consider that shufﬂing nodes and
their weights in a neural network preserves the original performance. If the parameters of two neural
networks initialized independently are averaged  it might perform poorly because of the high cost
barriers between the parameters of the two neural networks [23]. However  we will show that various
transfer learning techniques can be used to ease this problem  and make the assumption of Gaussian
distribution for neural networks reasonable. In this section  we introduce three practical techniques
for IMM  including weight-transfer  L2-transfer  and drop-transfer.

4.1 Weight-Transfer

Weight-transfer initialize the parameters for the new task µk with the parameters of the previous
task µk−1 [24]. In our experiments  the use of weight-transfer was critical to the continual learn-
ing performance. For this reason  the experiments on IMM in this paper use the weight-transfer
technique by default.
The weight-transfer technique is motivated by the geometrical property of neural networks discov-
ered in the previous work [23]. They found that there is a straight path from the initial point to the
solution without any high cost barrier  in various types of neural networks and datasets. This dis-
covery suggests that the weight-transfer from the previous task to the new task makes a smooth loss

4

Figure 2: Experimental results on visualizing the effect of weight-transfer. The geometric property
of the parameter space of the neural network is analyzed. Brighter is better. θ1  θ2  and θ3 are the
vectorized parameters of trained networks from randomly selected subsets of the CIFAR-10 dataset.
This ﬁgure shows that there are better solutions between the three locally optimized parameters.

surface between two solutions for the tasks  so that the optimal solution for both tasks lies on the
interpolated point of the two solutions.
To empirically validate the concept of weight-transfer  we use the linear path analysis proposed by
Goodfellow et al. [23] (Figure 2). We randomly chose 18 000 instances from the training dataset
of CIFAR-10  and divided them into three subsets of 6 000 instances each. These three subsets are
used for sequential training by CNN models  parameterized by θ1  θ2  and θ3  respectively. Here  θ2
is initialized from θ1  and then θ3 is initialized from θ2  in the same way as weight-transfer. In this
analysis  each loss and accuracy is evaluated at a series of points θ = θ1 + α(θ2 − θ1) + β(θ3 −
θ2)  varying α and β. In Figure 2  the loss surface of the model on each online subset is nearly
3 (θ1 + θ2 + θ3)  which is the same as the solution
convex. The ﬁgure shows that the parameter at 1
of mean-IMM  performs better than any other reference points θ1  θ2  or θ3. However  when θ2 is
not initialized by θ1  the convex-like shape disappears  since there is a high cost barrier between the
loss function of θ1 and θ2.

4.2 L2-transfer

L2-transfer is a variant of L2-regularization. L2-transfer can be interpreted as a special case of
EWC where the prior distribution is Gaussian with λI as a covariance matrix. In L2-transfer  a
regularization term of the distance between µk−1 and µk is added to the following objective function
for ﬁnding µk  where λ is a hyperparameter:

log p(yk|Xk  µk) − λ · ||µk − µk−1||2

2

(10)

The concept of L2-transfer is commonly used in transfer learning [25  26] and continual learning
[7  8] with large λ. Unlike the previous usage of large λ  we use small λ for the IMM procedure.
In other words  µk is ﬁrst trained by Equation 10 with small λ  and then merged to µ1:k in our
IMM. Since we want to make the loss surface between µk−1 and µk smooth  and not to minimize
the distance between µk−1 and µk. In convex optimization  the L2-regularizer makes the convex
function strictly convex. Similarly  we hope L2-transfer with small λ help to ﬁnd a µk with a convex-
like loss space between µk−1 and µk.

4.3 Drop-transfer

Drop-transfer is a novel method devised in this paper. Drop-transfer is a variant of dropout where
µk−1 is the zero point of the dropout procedure. In the training phase  the following ˆµk i is used for
the weight vector corresponding to the ith node µk i:

(cid:40)

ˆµk i =

µk−1 i 
1−p · µk i − p

1

if ith node is turned off

1−p · µk−1 i  otherwise

(11)

where p is the dropout ratio. Notice that the expectation of ˆµk i is µk i.

5

Table 1: The averaged accuracies on the disjoint MNIST for two sequential tasks (Top) and the
shufﬂed MNIST for three sequential tasks (Bottom). The untuned setting refers to the most natural
hyperparameter in the equation of each algorithm  whereas the tuned setting refers to using heuristic
hand-tuned hyperparameters. Hyperparam denotes the main hyperparameter of each algorithm. For
IMM with transfer  only α is tuned. The numbers in the parentheses refer to standard deviation.
Every IMM uses weight-transfer.

Untuned

Hyperparam

Tuned

Hyperparam

Disjoint MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2  Drop-transfer + Mean-IMM
L2  Drop-transfer + Mode-IMM

Shufﬂed MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2  Drop-transfer + Mean-IMM
L2  Drop-transfer + Mode-IMM

Explanation of
Hyperparam

epoch per dataset

λ in (10)
p in (11)
λ in (20)
α2 in (4)
α2 in (7)
λ / α2
λ / α2
p / α2
p / α2

λ / p / α2
λ / p / α2

epoch per dataset

λ in (10)
p in (11)
λ in (20)
α3 in (4)
α3 in (7)
λ / α3
λ / α3
p / α3
p / α3

λ / p / α3
λ / p / α3

10
-
0.5
1.0
0.50
0.50

60
-
0.5
-

0.33
0.33

0.001 / 0.50
0.001 / 0.50
0.5 / 0.50
0.5 / 0.50

0.001 / 0.5 / 0.50
0.001 / 0.5 / 0.50

Hyperparam

1e-4 / 0.33
1e-4 / 0.33
0.5 / 0.33
0.5 / 0.33

1e-4 / 0.5 / 0.33
1e-4 / 0.5 / 0.33

-

Accuracy

47.72 (± 0.11)
51.72 (± 0.79)
47.84 (± 0.04)
90.45 (± 2.24)
91.49 (± 0.98)
78.34 (± 1.82)
92.52 (± 0.41)
80.75 (± 1.28)
93.35 (± 0.49)
66.10 (± 3.19)
93.97 (± 0.32)

-

-

Accuracy

89.15 (± 2.34)
94.75 (± 0.62)
93.23 (± 1.37)
98.02 (± 0.05)
90.38 (± 1.74)
98.16 (± 0.08)
90.79 (± 1.30)
97.80 (± 0.07)
89.51 (± 2.85)
97.83 (± 0.10)

0.001 / 0.60
0.001 / 0.45
0.5 / 0.60
0.5 / 0.50

0.001 / 0.5 / 0.75
0.001 / 0.5 / 0.45

Hyperparam

0.05
0.05
0.5
600M
0.55
0.45

-

1e-3
0.2
-

0.55
0.60

1e-4 / 0.65
1e-4 / 0.60
0.5 / 0.65
0.5 / 0.55

1e-4 / 0.5 / 0.90
1e-4 / 0.5 / 0.50

Accuracy

71.32 (± 1.54)
85.81 (± 0.52)
51.72 (± 0.79)
52.72 (± 1.36)
91.92 (± 0.98)
92.02 (± 0.73)
92.62 (± 0.95)
92.73 (± 0.35)
92.64 (± 0.60)
93.35 (± 0.49)
93.97 (± 0.23)
94.12 (± 0.27)

Accuracy
∼95.5 [8]

∼98.2 [8]

96.37 (± 0.62)
96.86 (± 0.21)
95.02 (± 0.42)
98.08 (± 0.08)
95.93 (± 0.31)
98.30 (± 0.08)
96.49 (± 0.44)
97.95 (± 0.08)
97.36 (± 0.19)
97.92 (± 0.05)

There are studies [27  20] that have interpreted dropout as an exponential ensemble of weak learners.
By this perspective  since the marginalization of output distribution over the whole weak learner is
intractable  the parameters multiplied by the inverse of the dropout rate are used for the test phase
in the procedure. In other words  the parameters of the weak learners are  in effect  simply averaged
oversampled learners by dropout. At the process of drop-transfer in our continual learning setting 
we hypothesize that the dropout process makes the averaged point of two arbitrary sampled points
using Equation 11 a good estimator.
We investigated the search space of the loss function of the MLP trained from the MNIST handwrit-
ten digit recognition dataset for with and without dropout regularization  to supplement the evidence
of the described hypothesis. Dropout regularization makes the accuracy of a sampled point from
dropout distribution and an average point of two sampled parameters  from 0.450 (± 0.084) to 0.950
(± 0.009) and 0.757 (± 0.065) to 0.974 (± 0.003)  respectively. For the case of both with and without
dropout  the space between two arbitrary samples is empirically convex  and ﬁts to the second-order
equation. Based on this experiment  we expect not only that the search space of the loss function
between modern neural networks can be easily nearly convex [23]  but also that regularizers  such
as dropout  make the search space smooth and the point in the search space have a good accuracy in
continual learning.

5 Experimental Results

We evaluate our approach on four experiments  whose settings are intensively used in the previous
works [4  8  7  12]. For more details and experimental results  see Appendix D. The source code for
the experiments is available in Github repository1.
Disjoint MNIST Experiment. The ﬁrst experiment is the disjoint MNIST experiment [4]. In this
experiment  the MNIST dataset is divided into two datasets: the ﬁrst dataset consists of only digits
{0  1  2  3  4} and the second dataset consists of the remaining digits {5  6  7  8  9}. Our task is 10-

1https://github.com/btjhjeon/IMM_tensorﬂow

6

Figure 3: Test accuracies of two IMM models with weight-transfer on the disjoint MNIST (Left) 
the shufﬂed MNIST (Middle)  and the ImageNet2CUB experiment (Right). α is a hyperparameter
that balances the information between the old and the new task.

Figure 4: Test accuracies of IMM with various transfer techniques on the disjoint MNIST. Both L2-
transfer and drop-transfer boost the performance of IMM and make the optimal value of α larger
than 1/2. However  drop-transfer tends to make the accuracy curve more smooth than L2-transfer
does.

class joint categorization  unlike the setting in the previous work  which considers two independent
tasks of 5-class categorization. Because the inference should decide whether a new instance comes
from the ﬁrst or the second task  our task is more difﬁcult than the task of the previous work.
We evaluate the models both on the untuned setting and the tuned setting. The untuned setting refers
to the most natural hyperparameter in the equation of each algorithm. The tuned setting refers to
using heuristic hand-tuned hyperparameters. Consider that tuned hyperparameter setting is often
used in previous works of continual learning as it is difﬁcult to deﬁne a validation set in their setting.
For example  when the model needs to learn from the new task after learning from the old task  a low
learning rate or early stopping without a validation set  or arbitrary hyperparameter for balancing is
used [3  8]. We discover hyperparameters in the tuned setting not only to ﬁnd the oracle performance
of each algorithm  but also to show that there exist some paths consisting of the point that performs
reasonably for both tasks. Hyperparam in Table 1 denotes hyperparameter mainly searched in the
tuned setting. Table 1 (Top) and Figure 3 (Left) shows the experimental results from the disjoint
MNIST experiment.
In our experimental setting  the usual SGD-based optimizers always perform less than 50%  because
the biases of the output layer for the old task are always pushed to large negative values  which
implies that our task is difﬁcult. Figure 4 also shows that mode-IMM is robust with α and the
optimal α of mean-IMM is larger than 1/2 in the disjoint MNIST experiment.
Shufﬂed MNIST Experiment. The second experiment is the shufﬂed MNIST experiment [3  8] of
three sequential tasks. In this experiment  the ﬁrst dataset is the same as the original MNIST dataset.
However  in the second dataset  the input pixels of all images are shufﬂed with a ﬁxed  random per-
mutation. In previous work  EWC reaches the performance level of the batch learner  and it is argued
that EWC overcomes catastrophic forgetting in some domains. The experimental details are similar
to the disjoint MNIST experiment  except all models are allowed to use dropout regularization. In
the experiment  the ﬁrst dataset is the same as the original MNIST dataset. However  in the second
and the third dataset  the input pixels of all images are shufﬂed with a ﬁxed  random permutation 

7

00.20.40.60.8100.20.40.60.811.2alpha  for weighing two networksTest AccuracyThe disjoint MNIST experiment First Task  Mean−IMMSecond Task  Mean−IMMFirst Task  Mode−IMMSecond Task  Mode−IMM00.20.40.60.810.950.9550.960.9650.970.9750.980.9850.990.9951alpha  for weighing two networksTest AccuracyThe shuffled MNIST experiment First Task  Mean−IMMSecond Task  Mean−IMMFirst Task  Mode−IMMSecond Task  Mode−IMM00.20.40.60.810.520.540.560.580.60.62alpha  for weighing two networksTest AccuracyThe ImageNet2CUB experiment First Task  Mean−IMMSecond Task  Mean−IMMFirst Task  Mode−IMMSecond Task  Mode−IMM00.20.40.60.810.50.550.60.650.70.750.80.850.90.95alpha  for weighing two networksTest AccuracyThe disjoint MNIST experiment Mean−IMMMode−IMML2−transfer + Mean−IMML2−transfer + Mode−IMM00.20.40.60.810.50.550.60.650.70.750.80.850.90.95alpha  for weighing two networksTest AccuracyThe disjoint MNIST experiment Mean−IMMMode−IMMDrop−transfer + Mean−IMMDrop−transfer + Mode−IMML2  Drop−transfer + Mean−IMML2  Drop−transfer + Mode−IMMTable 2: Experimental results on the Lifelog dataset among different classes (location  sub-location 
and activity) and different subjects (A  B  C). Every IMM uses weight-transfer.

Dual memory architecture [12]
Mean-IMM
Mode-IMM

Location

78.11
77.60
77.14

Sub-location Activity
52.92
52.74
54.07

72.36
73.78
75.76

A

67.02
67.03
67.97

B

58.80
57.73
60.12

C

77.57
79.35
78.89

respectively. Therefore  the difﬁculty of the three datasets is the same  though a different solution is
required for each dataset.
Table 1 (Bottom) and Figure 3 (Middle) shows the experimental results from the shufﬂed MNIST
experiment. Notice that accuracy of drop-transfer (p = 0.2) alone is 96.86 (± 0.21) and L2-transfer
(λ = 1e-4) + drop-transfer (p = 0.4) alone is 97.61 (± 0.15). These results are competitive to EWC
without dropout  whose performance is around 97.0.
ImageNet to CUB Dataset. The third experiment is the ImageNet2CUB experiment [7]  the con-
tinual learning problem from the ImageNet dataset to the Caltech-UCSD Birds-200-2011 ﬁne-
grained classiﬁcation (CUB) dataset [28]. The numbers of classes of ImageNet and CUB dataset
are around 1K and 200  and the numbers of training instances are 1M and 5K  respectively. In the
ImageNet2CUB experiment  the last-layer is separated for the ImageNet and the CUB task. The
structure of AlexNet is used for the trained model of ImageNet [29]. In our experiment  we match
the moments of the last-layer ﬁne-tuning model and the LwF model  with mean-IMM and mode-
IMM.
Figure 3 (Right) shows that mean-IMM moderately balances the performance of two tasks between
two networks. However  the balanced hyperparameter of mode-IMM is far from α = 0.5. We think
that it is because the scale of the Fisher matrix F is different between the ImageNet and the CUB
task. Since the number of training data of the two tasks is different  the mean of the square of the
gradient  which is the deﬁnition of F   tends to be different. This implies that the assumption of
mode-IMM does not always hold for heterogeneous tasks. See Appendix D.3 for more information
including the learning methods of IMM where a different class output layer or a different scale of
the dataset is used.
Our results of IMM with LwF exceed the previous state-of-the-art performance  whose model is
also LwF. This is because  in the previous works  the LwF model is initialized by the last-layer ﬁne-
tuning model  not directly by the original AlexNet. In this case  the performance loss of the old task
is not only decreased  but also the performance gain of the new task is decreased. The accuracies of
our mean-IMM (α = 0.5) are 56.20 and 56.73 for the ImageNet task and the CUB task  respectively.
The gains compared to the previous state-of-the-art are +1.13 and -1.14. In the case of mean-IMM
(α = 0.8) and mode-IMM (α = 0.99)  the accuracies are 55.08 and 59.08 (+0.01  +1.12)  and 55.10
and 59.12 (+0.02  +1.35)  respectively.
Lifelog Dataset. Lastly  we evaluate the proposed methods on the Lifelog dataset [12]. The Lifelog
dataset consists of 660 000 instances of egocentric video stream data  collected over 46 days from
three participants using Google Glass [30]. Three class categories  location  sub-location  and activ-
ity  are labeled on each frame of video. In the Lifelog dataset  the class distribution changes con-
tinuously and new classes appear as the day passes. Table 2 shows that mean-IMM and mode-IMM
are competitive to the dual-memory architecture  the previous state-of-the-art ensemble model  even
though IMM uses single network.

6 Discussion

A Shift of Optimal Hyperparameter of IMM. The tuned setting shows there often exists some α
which makes the performance of the mean-IMM close to the mode-IMM. However  in the untuned
hyperparameter setting  mean-IMM performs worse when more transfer techniques are applied. Our
Bayesian interpretation in IMM assumes that the SGD training of the k-th network µk is mainly
affected by the k-th task and is rarely affected by the information of the previous tasks. However 
transfer techniques break this assumption; thus the optimal α is shifted to larger than 1/k. For-
tunately  mode-IMM works more robustly than mean-IMM where transfer techniques are applied.

8

Figure 4 illustrates the change of the test accuracy curve corresponding to the applied transfer tech-
niques and the following shift of the optimal α in mean-IMM and mode-IMM.
Bayesian Approach on Continual Learning. Kirkpatrick et al. [8] interpreted that the Fisher
matrix F as weight importance in explaining their EWC model. In the shufﬂed MNIST experiment 
since a large number of pixels always have a value of zero  the corresponding elements of the Fisher
matrix are also zero. Therefore  EWC does work by allowing weights to change  which are not used
in the previous tasks. On the other hand  mode-IMM also works by selectively balancing between
two weights using variance information. However  these assumptions on weight importance do not
always hold  especially in the disjoint MNIST experiment. The most important weight in the disjoint
MNIST experiment is the bias term in the output layer. Nevertheless  these bias parts of the Fisher
matrix are not guaranteed to be the highest value nor can they be used to balance the class distribution
between the ﬁrst and second task. We believe that using only the diagonal of the covariance matrix
in Bayesian neural networks is too naïve in general and that this is why EWC failed in the disjoint
MNIST experiment. We think it could be alleviated in future work by using a more complex prior 
such as a matrix Gaussian distribution considering the correlations between nodes in the network
[31].
Balancing the Information of an Old and a New Task. The IMM procedure produces a neural
network without a performance loss for kth task µk  which is better than the ﬁnal solution µ1:k in
terms of the performance of the kth task. Furthermore  IMM can easily weigh the importance of
tasks in IMM models in real time. For example  αt can be easily changed for the solution of mean-
t αtµt . In actual service situations of IT companies  the importance of the old
and the new task frequently changes in real time  and IMM can handle this problem. This property
differentiates IMM from the other continual learning methods using the regularization approach 
including LwF and EWC.

IMM µ1:k = (cid:80)k

7 Conclusion

Our contributions are four folds. First  we applied mean-IMM to the continual learning of modern
deep neural networks. Mean-IMM makes competitive results to comparative models and balances
the information between an old and a new network. We also interpreted the success of IMM by the
Bayesian framework with Gaussian posterior. Second  we extended mean-IMM to mode-IMM with
the interpretation of mode-ﬁnding in the mixture of Gaussian posterior. Mode-IMM outperforms
mean-IMM and comparative models in various datasets. Third  we introduced drop-transfer  a novel
method proposed in the paper. Experimental results showed that drop-transfer alone performs well
and is similar to the EWC without dropout  in the domain where EWC rarely forgets. Fourth  We
applied various transfer techniques in the IMM procedure to make our assumption of Gaussian
distribution reasonable. We argued that not only the search space of the loss function among neural
networks can easily be nearly convex  but also regularizers  such as dropout  make the search space
smooth  and the point in the search space have good accuracy. Experimental results showed that
applying transfer techniques often boost the performance of IMM. Overall  we made state-of-the-
art performance in various datasets of continual learning and explored geometrical properties and a
Bayesian perspective of deep neural networks.

Acknowledgments

The authors would like to thank Jiseob Kim  Min-Oh Heo  Donghyun Kwak  Insu Jeon  Christina
Baek  and Heidi Tessmer for helpful comments and editing. This work was supported by the
Naver Corp. and partly by the Korean government (IITP-R0126-16-1072-SW.StarLab  IITP-2017-0-
01772-VTT  KEIT-10044009-HRI.MESSI  KEIT-10060086-RISF). Byoung-Tak Zhang is the cor-
responding author.

References
[1] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. Psychology of learning and motivation  24:109–165  1989.

9

[2] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive

sciences  3(4):128–135  1999.

[3] Ian J Goodfellow  Mehdi Mirza  Da Xiao  Aaron Courville  and Yoshua Bengio. An empiri-
cal investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint
arXiv:1312.6211  2013.

[4] Rupesh K Srivastava  Jonathan Masci  Sohrob Kazerounian  Faustino Gomez  and Jürgen
Schmidhuber. Compete to compute. In Advances in neural information processing systems 
pages 2310–2318  2013.

[5] Zoubin Ghahramani. Online variational bayesian learning. In NIPS workshop on Online Learn-

ing  2000.

[6] Tamara Broderick  Nicholas Boyd  Andre Wibisono  Ashia C Wilson  and Michael I Jordan.
Streaming variational bayes. In Advances in Neural Information Processing Systems  pages
1727–1735  2013.

[7] Zhizhong Li and Derek Hoiem. Learning without forgetting.

Computer Vision  pages 614–629. Springer  2016.

In European Conference on

[8] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins  An-
drei A Rusu  Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy
of Sciences  2017.

[9] David JC MacKay. A practical bayesian framework for backpropagation networks. Neural

computation  4(3):448–472  1992.

[10] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncer-
tainty in neural network. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15)  pages 1613–1622  2015.

[11] Jacob Goldberger and Sam T Roweis. Hierarchical clustering of a mixture model. In Advances

in Neural Information Processing Systems  pages 505–512  2005.

[12] Sang-Woo Lee  Chung-Yeon Lee  Dong Hyun Kwak  Jiwon Kim  Jeonghee Kim  and Byoung-
Tak Zhang. Dual-memory deep learning architectures for lifelong learning of everyday human
In Twenty-Fifth International Joint Conference on Artiﬁcial Intelligencee  pages
behaviors.
1669–1675  2016.

[13] Andrei A Rusu  Neil C Rabinowitz  Guillaume Desjardins  Hubert Soyer  James Kirkpatrick 
Koray Kavukcuoglu  Razvan Pascanu  and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671  2016.

[14] Chrisantha Fernando  Dylan Banarse  Charles Blundell  Yori Zwols  David Ha  Andrei A Rusu 
Alexander Pritzel  and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734  2017.

[15] Zhen Huang  Jinyu Li  Sabato Marco Siniscalchi  I-Fan Chen  Chao Weng  and Chin-Hui Lee.
Feature space maximum a posteriori linear regression for adaptation of deep neural networks.
In Fifteenth Annual Conference of the International Speech Communication Association  2014.

[16] Zhen Huang  Sabato Marco Siniscalchi  I-Fan Chen  Jinyu Li  Jiadong Wu  and Chin-Hui Lee.
Maximum a posteriori adaptation of network parameters in deep models. In Sixteenth Annual
Conference of the International Speech Communication Association  2015.

[17] Abdullah Rashwan  Han Zhao  and Pascal Poupart. Online and distributed bayesian moment
matching for parameter learning in sum-product networks. In Proceedings of the 19th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics  pages 1469–1477  2016.

[18] Kai Zhang and James T Kwok. Simplifying mixture models through function approximation.

Neural Networks  IEEE Transactions on  21(4):644–658  2010.

10

[19] Manas Pathak  Shantanu Rane  and Bhiksha Raj. Multiparty differential privacy via aggre-
gation of locally trained classiﬁers. In Advances in Neural Information Processing Systems 
pages 1876–1884  2010.

[20] Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information

Processing Systems  pages 2814–2822  2013.

[21] Surajit Ray and Bruce G Lindsay. The topography of multivariate normal mixtures. Annals of

Statistics  pages 2042–2065  2005.

[22] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv

preprint arXiv:1301.3584  2013.

[23] Ian J Goodfellow  Oriol Vinyals  and Andrew M Saxe. Qualitatively characterizing neural

network optimization problems. arXiv preprint arXiv:1412.6544  2014.

[24] Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features
in deep neural networks? In Advances in neural information processing systems  pages 3320–
3328  2014.

[25] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Proceed-
ings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining  pages 109–117. ACM  2004.

[26] Wolf Kienzle and Kumar Chellapilla. Personalized handwriting recognition via biased regu-
larization. In Proceedings of the 23rd international conference on Machine learning  pages
457–464. ACM  2006.

[27] Nitish Srivastava  Geoffrey E Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhut-
dinov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research  15(1):1929–1958  2014.

[28] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. Tech. Rep. CNS-TR-2011-001  2011.

[29] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[30] Sang-Woo Lee  Chung-Yeon Lee  Dong-Hyun Kwak  Jung-Woo Ha  Jeonghee Kim  and
Byoung-Tak Zhang. Dual-memory neural networks for modeling cognitive activities of hu-
mans via wearable sensors. Neural Networks  2017.

[31] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with

matrix gaussian posteriors. arXiv preprint arXiv:1603.04733  2016.

[32] Surajit Ray and Dan Ren. On the upper bound of the number of modes of a multivariate normal

mixture. Journal of Multivariate Analysis  108:41–52  2012.

[33] Carlos Améndola  Alexander Engström  and Christian Haase. Maximum number of modes of

gaussian mixtures. arXiv preprint arXiv:1702.05066  2017.

[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

11

,Sang-Woo Lee
Jin-Hwa Kim
Jaehyun Jun
Jung-Woo Ha
Byoung-Tak Zhang
Ludwig Schmidt
Shibani Santurkar
Dimitris Tsipras
Kunal Talwar
Aleksander Madry
Christopher Beckham
Sina Honari
Vikas Verma
Alex Lamb
Farnoosh Ghadiri
R Devon Hjelm
Yoshua Bengio
Chris Pal