2008,On the Complexity of Linear Prediction: Risk Bounds  Margin Bounds  and Regularization,We provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes. These bounds make short work of providing a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either $L_2$ or $L_1$ constraints)  margin bounds (including both $L_2$ and $L_1$ margins  along with more general notions based on relative entropy)  a proof of the PAC-Bayes theorem  and $L_2$ covering numbers (with $L_p$ norm constraints and relative entropy constraints). In addition to providing a unified analysis  the results herein provide some of the sharpest risk and margin bounds (improving upon a number of previous results). Interestingly  our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction (up to a constant factor of 2).,On the Complexity of Linear Prediction:

Risk Bounds  Margin Bounds  and Regularization

Sham M. Kakade

TTI Chicago

Chicago  IL 60637
sham@tti-c.org

Karthik Sridharan

TTI Chicago

Chicago  IL 60637

Ambuj Tewari

TTI Chicago

Chicago  IL 60637

karthik@tti-c.org

tewari@tti-c.org

Abstract

This work characterizes the generalization ability of algorithms whose predic-
tions are linear in the input vector. To this end  we provide sharp bounds for
Rademacher and Gaussian complexities of (constrained) linear classes  which di-
rectly lead to a number of generalization bounds. This derivation provides simpli-
ﬁed proofs of a number of corollaries including: risk bounds for linear prediction
(including settings where the weight vectors are constrained by either L2 or L1
constraints)  margin bounds (including both L2 and L1 margins  along with more
general notions based on relative entropy)  a proof of the PAC-Bayes theorem 
and upper bounds on L2 covering numbers (with Lp norm constraints and rela-
tive entropy constraints). In addition to providing a uniﬁed analysis  the results
herein provide some of the sharpest risk and margin bounds. Interestingly  our
results show that the uniform convergence rates of empirical risk minimization
algorithms tightly match the regret bounds of online learning algorithms for linear
prediction  up to a constant factor of 2.

1

Introduction

Linear prediction is the cornerstone of an extensive number of machine learning algorithms  in-
cluding SVM’s  logistic and linear regression  the lasso  boosting  etc. A paramount question is to
understand the generalization ability of these algorithms in terms of the attendant complexity re-
strictions imposed by the algorithm. For example  for the sparse methods (e.g. regularizing based
on L1 norm of the weight vector) we seek generalization bounds in terms of the sparsity level. For
margin based methods (e.g. SVMs or boosting)  we seek generalization bounds in terms of either
the L2 or L1 margins. The focus of this paper is to provide a more uniﬁed analysis for methods
which use linear prediction.

Given a training set {(xi  yi)}n
the F -regularized ℓ-risk. More speciﬁcally 

i=1  the paradigm is to compute a weight vector ˆw which minimizes

ˆw = argmin

w

1
n

ℓ(hw  xii   yi) + λF (w)

(1)

n

Xi=1

where ℓ is the loss function  F is the regularizer  and hw  xi is the inner product between vectors x
and w. In a formulation closely related to the dual problem  we have:

ˆw = argmin
w:F (w)≤c

1
n

n

Xi=1

ℓ(hw  xii   yi)

(2)

where  instead of regularizing  a hard restriction over the parameter space is imposed (by the constant
c). This works provides generalization bounds for an extensive family of regularization functions F .

Rademacher complexities (a measure of the complexity of a function class) provide a direct route
to obtaining such generalization bounds  and this is the route we take. Such bounds are analogous
to VC dimensions bounds  but they are typically much sharper and allow for distribution dependent
bounds. There are a number of methods in the literature to use Rademacher complexities to obtain
either generalization bounds or margin bounds. Bartlett and Mendelson [2002] provide a general-
ization bound for Lipschitz loss functions. For binary prediction  the results in Koltchinskii and
Panchenko [2002] provide means to obtain margin bounds through Rademacher complexities.

In this work  we provide sharp bounds for Rademacher and Gaussian complexities of linear classes 
with respect to a strongly convex complexity function F (as in Equation 1). These bounds provide
simpliﬁed proofs of a number of corollaries: generalization bounds for the regularization algorithm
in Equation 2 (including settings where the weight vectors are constrained by either L2 or L1 con-
straints)  margin bounds (including L2 and L1 margins  and  more generally  for Lp margins)  a
proof of the PAC-Bayes theorem  and L2 covering numbers (with Lp norm constraints and relative
entropy constraints). Our bounds are often tighter than previous results and our proofs are all under
this more uniﬁed methodology.

Our proof techniques — reminiscent of those techniques for deriving regret bounds for online learn-
ing algorithms — are rooted in convex duality (following Meir and Zhang [2003]) and use a more
general notion of strong convexity (as in Shalev-Shwartz and Singer [2006]). Interestingly  the risk
bounds we provide closely match the regret bounds for online learning algorithms (up to a constant
factor of 2)  thus showing that the uniform converge rates of empirical risk minimization algorithms
tightly match the regret bounds of online learning algorithms (for linear prediction). The Discussion
provides this more detailed comparison.

1.1 Related Work

A staggering number of results have focused on this problem in varied special cases. Perhaps the
most extensively studied are margin bounds for the 0-1 loss. For L2-margins (relevant for SVM’s 
perceptron based algorithms  etc.)  the sharpest bounds are those provided by Bartlett and Mendel-
son [2002] (using Rademacher complexities) and Langford and Shawe-Taylor [2003]  McAllester
[2003] (using the PAC-Bayes theorem). For L1-margins (relevant for Boosting  winnow  etc) 
bounds are provided by Schapire et al. [1998] (using a self-contained analysis) and Langford et al.
[2001] (using PAC-Bayes  with a different analysis). Another active line of work is on sparse meth-
ods — particularly methods which impose sparsity via L1 regularization (in lieu of the non-convex
L0 norm). For L1 regularization  Ng [2004] provides generalization bounds for this case  which
follow from the covering number bounds of Zhang [2002]. However  these bounds are only stated
as polynomial in the relevant quantities (dependencies are not provided).

Previous to this work  the most uniﬁed framework for providing generalization bounds for linear
prediction stem from the covering number bounds in Zhang [2002]. Using these covering number
bounds  Zhang [2002] derives margin bounds in a variety of cases. However  providing sharp gen-
eralization bounds for problems with L1 regularization (or L1 constraints in the dual) requires more
delicate arguments. As mentioned  Ng [2004] provides bounds for this case  but the techniques used
by Ng [2004] would result in rather loose dependencies (the dependence on the sample size n would
be n−1/4 rather than n−1/2). We discuss this later in Section 4.

2 Preliminaries

Our input space  X   is a subset of a vector space  and our output space is Y. Our samples (X  Y ) ∈
X × Y are distributed according to some unknown distribution P . The inner product between
vectors x and w is denoted by hw  xi  where w ∈ S (here  S is a subset of the dual space to
our input vector space). A norm of a vector x is denoted by kxk  and the dual norm is deﬁned as
kwk⋆ = sup{hw  xi : kxk ≤ 1}. We further assume that for all x ∈ X   kxk ≤ X.
Let ℓ : R×Y → R+ be our loss function of interest. Throughout we shall consider linear predictors
of form hw  xi. The expected of loss of w is denoted by L(w) = E[ℓ(hw  xi   y)]. As usual  we are
provided with a sequence of i.i.d. samples {(xi  yi)}n
i=1  and our goal is to minimize our expected
nPn
loss. We denote the empirical loss as ˆL(w) = 1
i=1 ℓ(hw  xii   yi).

The restriction we make on our complexity function F is that it is a strongly convex function. In
particular  we assume it is strongly convex with respect to our dual norm: a function F : S → R is
said to be σ-strongly convex w.r.t. to k · k∗ iff ∀u  v ∈ S  ∀α ∈ [0  1]  we have

F (αu + (1 − α)v) ≤ αF (u) + (1 − α)F (v) −

σ
2

α(1 − α)ku − vk2
∗ .

See Shalev-Shwartz and Singer [2006] for more discussion on this generalized deﬁnition of strong
convexity.
Recall the deﬁnition of the Rademacher and Gaussian complexity of a function class F  
f (xi)ǫi#

Rn(F) = E"sup

Gn(F) = E"sup

f (xi)ǫi#

1
n

1
n

f ∈F

f ∈F

n

n

where  in the former  ǫi independently takes values in {−1  +1} with equal probability  and  in the
latter  ǫi are independent  standard normal random variables. In both expectations  (x1  . . .   xn) are
i.i.d.

Xi=1

Xi=1

nPn

i=1 ℓ(f (xi)  yi) is the empirical loss.

As mentioned in the Introduction  there are number of methods in the literature to use Rademacher
complexities to obtain either generalization bounds or margin bounds. Two results are particularly
useful to us. First  Bartlett and Mendelson [2002] provides the following generalization bound for
Lipschitz loss functions. Here  L(f ) = E[ℓ(f (x)  y)] is the expected of loss of f : X → R  and
ˆL(f ) = 1
Theorem 1. (Bartlett and Mendelson [2002]) Assume the loss ℓ is Lipschitz (with respect to its
ﬁrst argument) with Lipschitz constant Lℓ and that ℓ is bounded by c. For any δ > 0 and with
probability at least 1 − δ simultaneously for all f ∈ F   we have that
L(f ) ≤ ˆL(f ) + 2LℓRn(F) + cr log(1/δ)

where Rn(F) is the Rademacher complexity of a function class F   and n is the sample size.
The second result  for binary prediction  from Koltchinskii and Panchenko [2002] provides a mar-
gin bound in terms of the Rademacher complexity. The following is a variant of Theorem 2 in
Koltchinskii and Panchenko [2002]:
(Koltchinskii and Panchenko [2002]) The zero-one loss function is given by
Theorem 2.
ℓ(f (x)  y) = 1[yf (x) ≤ 0]  where y ∈ {+1 −1}. Denote the fraction of the data having γ-
margin mistakes by Kγ(f ) := |{i:yif (xi)<γ}|
. Assume that ∀f ∈ F we have supx∈X |f (x)| ≤ C.
Then  with probability at least 1 − δ over the sample  for all margins γ > 0 and all f ∈ F we have 
+s log(log2

L(f ) ≤ Kγ(f ) + 4Rn(F)

4C
γ )

+r log(1/δ)

2n

2n

n

γ

n

.

(We provide a proof in the appendix.) The above results show that if we provide sharp bounds on the
Rademacher complexities then we obtain sharp generalization bounds. Typically  we desire upper
bounds on the Rademacher complexity that decrease with n.

3 Complexities of Linear Function Classes

Given a subset W ⊆ S  deﬁne the associated class of linear functions FW as FW := {x 7→ hw  xi :
w ∈ W}. Our main theorem bounds the complexity of FW for certain sets W.
Theorem 3. (Complexity Bounds) Let S be a closed convex set and let F : S → R be σ-strongly
convex w.r.t. k · k∗ s.t. inf w∈S F (w) = 0. Further  let X = {x : kxk ≤ X}. Deﬁne W = {w ∈
S : F (w) ≤ W 2

∗ }. Then  we have
Rn(FW ) ≤ XW∗r 2

σn

 

Gn(FW ) ≤ XW∗r 2

σn

.

The restriction inf w∈S F (w) = 0 is not a signiﬁcant one since adding a constant to F still keeps it
strongly convex. Interestingly  the complexity bounds above precisely match the regret bounds for
online learning algorithms (for linear prediction)  a point which we return to in the Discussion. We
ﬁrst provide a few examples  before proving this result.

3.1 Examples

(1) Lp/Lq norms. Let S = Rd. Take k·k  k·k∗ to be the Lp  Lq norms for p ∈ [2 ∞)  1/p+1/q = 1 
where kxkp :=(cid:16)Pd
q and note that it is 2(q−1)-strongly convex
on Rd w.r.t. itself. Set X   W as in Theorem 3. Then  we have
Rn(FW ) ≤ XW∗r p − 1

j=1 |xi|p(cid:17)1/p

. Choose F (w) = k·k2

(3)

.

n

(2) L∞/L1 norms. Let S = {w ∈ Rd : kwk1 = W1   wj ≥ 0} be the W1-scaled probability
simplex. Take k · k  k · k∗ to be the L∞  L1 norms  kxk∞ = max1≤j≤d |xj|. Fix a probability
distribution µ > 0 and let F (w) = entroµ(w) := Pj(wj/W1) log(wj/(W1µj)). For any µ 
entroµ(w) is 1/W 2
1 -strongly convex on S w.r.t. k · k1. Set X as in Theorem 3 and let W(E) =
{w ∈ S : entroµ(w) ≤ E}. Then  we have

Rn(FW(E)) ≤ XW1r 2E

n

.

(4)

Note that if we take µ to be the uniform distribution then for any w ∈ S we have that trivial upper
bound of entroµ(w) ≤ log d. Hence if we let W := W(log d) with uniform µ and note that it is the
entire scaled probability simplex. Then

Rn(FW ) ≤ XW1r 2 log d

n

.

(5)

The restriction wj ≥ 0 can be removed in the deﬁnition of S by the standard trick of doubling the
dimension of x to include negated copies of each coordinate. So  if we have S = {w ∈ Rd :
kwk1 ≤ W1} and we set X as above and W = S  then we get Rn(FW ) ≤ XW1p2 log(2d)/n.

In this way  even though the L1 norm is not strongly convex (so our previous Theorem does not
directly apply to it)  the class of functions imposed by this L1 norm restriction is equivalent to that
imposed by the above entropy restriction. Hence  we are able to analyze the generalization properties
of the optimization problem in Equation 2.
(3) Smooth norms. A norm is (2  D)-smooth on S if for any x  y ∈ S 

d2
dt2kx + tyk2 ≤ 2D2kyk2 .

Let k·k be a (2  D)-smooth norm and k·k∗ be its dual. Lemma 11 in the appendix proves that k·k∗
is 2/D2-strongly convex w.r.t. itself. Set X   W as in Theorem 3. Then  we have

Rn(FW ) ≤

XW∗D
√n

.

(6)

(4) Bregman divergences. For a strongly convex F   deﬁne the Bregman divergence ∆F (wkv) :=
F (w) − F (v) − h∇F (v)  w − vi. It is interesting to note that Theorem 3 is still valid if we choose
W∗ = {w ∈ S : ∆F (wkv) ≤ W 2
∗ } for some ﬁxed v ∈ S. This is because the Bregman divergence
∆F (·kv) inherits the strong convexity of F .
Except for (5)  none of the above bounds depend explicitly on the dimension of the underlying space
and hence can be easily extended to inﬁnite dimensional spaces under appropriate assumptions.

3.2 The Proof

First  some background on convex duality is in order. The Fenchel conjugate of F : S → R is
deﬁned as:

F ∗(θ) := sup

w∈S hw  θi − F (w) .

A simple consequence of this deﬁnition is Fenchel-Young inequality 

∀θ  w ∈ S  hw  θi ≤ F (w) + F ∗(θ) .

If F is σ-strongly convex  then F ∗ is differentiable and

∀θ  η  F ∗(θ + η) ≤ F ∗(θ) + h∇F ∗(θ)  ηi +

1
2σkηk2
∗ .

(7)

See the Appendix in Shalev-Shwartz [2007] for proof. Using this inequality we can control the
expectation of F ∗ applied to a sum of independent random variables.
Lemma 4. Let S be a closed convex set and let F : S → R be σ-strongly convex w.r.t. k·k∗. Let Zi
be mean zero independent random vectors such that E[kZik2] ≤ V 2. Deﬁne Si := Pj≤i Zi. Then
F ∗(Si) − iV 2/2σ is a supermartingale. Furthermore  if inf w∈S F (w) = 0  then E[F ∗(Sn)] ≤
nV 2/2σ.

Proof. Note that inf w∈S F (w) = 0 implies F ∗(0) = 0. Inequality (7) gives 

F ∗(Si−1 + Zi) ≤ F ∗(Si) + h∇F ∗(Si−1)  Zii +

1
2σkZik2
∗ .

Taking conditional expectation w.r.t. Z1  . . .   Zi−1 and noting that Ei−1[Zi] = 0 and Ei−1[kZik2
V 2  we get

∗] ≤

Ei−1[F ∗(Si)] ≤ F ∗(Si−1) + 0 +

V 2
2σ

where Ei−1[·] abbreviates E[·| Z1  . . .   Zi−1]. To end the proof  note that inf w∈S F (w) = 0 implies
F ∗(0) = 0.

Like Meir and Zhang [2003] (see Section 5 therein)  we begin by using conjugate duality to bound
the Rademacher complexity. To ﬁnish the proof  we exploit the strong convexity of F by applying
the above lemma.

Proof. Fix x1  . . .   xn such that kxik ≤ X. Let θ = 1
Gaussian random variables (our proof only requires that E[ǫi] = 0 and E[ǫ2
λ > 0. By Fenchel’s inequality  we have hw  λθi ≤ F (w) + F ∗(λθ) which implies

nPi ǫixi where ǫi’s are i.i.d. Rademacher or

i ] = 1). Choose arbitrary

Since  F (w) ≤ W 2

hw  θi ≤
∗ for all w ∈ W  we have

F (w)

λ

+

F ∗(λθ)

λ

.

w∈W hw  θi ≤
sup

W 2
∗
λ

+

F ∗(λθ)

λ

.

Taking expectation (w.r.t. ǫi’s)  we get

E(cid:20) sup
w∈W hw  θi(cid:21) ≤

W 2
∗
λ

+

1
λ

E [F ∗(λθ)] .

(so that Sn = λθ) and note that the conditions of Lemma 4 are satisﬁed with

Now set Zi = λǫi xi
V 2 = λ2B2/n2 and hence E[F ∗(λθ)] ≤ λ2X 2

n

2σn . Plugging this above  we have

Setting λ =q 2σnW 2

X 2

∗

E(cid:20) sup
w∈W hw  θi(cid:21) ≤

W 2
∗
λ

+

λX 2
2σn

.

gives

w∈W hw  θi(cid:21) ≤ XW∗r 2
E(cid:20) sup

σn

.

which completes the proof.

4 Corollaries

4.1 Risk Bounds

We now provide generalization error bounds for any Lipschitz loss function ℓ  with Lipschitz con-
stant Lℓ. Based on the Rademacher generalization bound provided in the Introduction (see Theo-
rem 1) and the bounds on Rademacher complexity proved in previous section  we obtain the follow-
ing corollaries.
Corollary 5. Each of the following statements holds with probability at least 1− δ over the sample:

• Let W be as in the Lp/Lq norms example. For all w ∈ W 

L(w) ≤ ˆL(w) + 2LℓXW∗r p − 1

n

+ LℓXW∗r log(1/δ)

2n

• Let W be as in the L∞/L1 norms example. For all w ∈ W 

L( ˆw) ≤ ˆL(w) + 2LℓXW1r 2 log(d)

n

+ LℓXW1r log(1/δ)

2n

Ng [2004] provides bounds for methods which use L1 regularization. These bounds are only stated
as polynomial bounds  and  the methods used (covering number techniques from Pollard [1984] and
covering number bounds from Zhang [2002]) would provide rather loose bounds (the n dependence
would be n−1/4).
In fact  even a more careful analysis via Dudley’s entropy integral using the
covering numbers from Zhang [2002] would result in a worse bound (with additional log n factors).
The above argument is sharp and rather direct.

4.2 Margin Bounds

In this section we restrict ourselves to binary classiﬁcation where Y = {+1 −1}. Our prediction
is given by sign(hw  xi). The zero-one loss function is given by ℓ(hw  xi   y) = 1[y hw  xi ≤
0]. Denote the fraction of the data having γ-margin mistakes by Kγ(f ) := |{i:yif (xi)<γ}|
. We
now demonstrate how to get improved margin bounds using the upper bounds for the Rademacher
complexity derived in Section 3.

n

Based on the Rademacher margin bound provided in the Introduction (see Theorem 2)  we get the
following corollary which will directly imply the margin bounds we are aiming for. The bound for
the p = 2 case has been used to explain the performance of SVMs. Our bound essentially matches
the best known bound [Bartlett and Mendelson  2002] which was an improvement over previous
bounds [Bartlett and Shawe-Taylor  1999] proved using fat-shattering dimension estimates. For the
L∞/L1 case  our bound improves the best known bound [Schapire et al.  1998] by removing a factor
of √log n.
Corollary 6. (Lp Margins) Each of the following statements holds with probability at least 1 − δ
over the sample:

• Let W be as in the Lp/Lq norms example. For all γ > 0  w ∈ W 
)

4XW∗

XW∗

γ r p − 1

n

+s log(log2

n

γ

+r log(1/δ)

2n

L(w) ≤ Kγ(w) + 4

• Let W be as in the L∞/L1 norms example. For all γ > 0  w ∈ W 
)

4XW1

XW1

γ r 2 log(d)

n

+s log(log2

n

γ

L(w) ≤ Kγ(w) + 4

+r log(1/δ)

2n

The following result improves the best known results of the same kind  [Langford et al.  2001  The-
orem 5] and [Zhang  2002  Theorem 7]  by removing a factor of √log n. These results themselves
were an improvement over previous results obtained using fat-shattering dimension estimates.

Corollary 7. (Entropy Based Margins) Let X be such that for all x ∈ X   kxk∞ ≤ X. Consider
the class W = {w ∈ Rd : kwk1 ≤ W1}. Fix an arbitrary prior µ. We have that with probability
at least 1 − δ over the sample  for all margins γ > 0 and all weight vector w ∈ W 

L(w) ≤ Kγ(w) + 8.5

XW1

γ r entroµ(w) + 2.5

n

where entroµ(w) :=Pi

Proof. Proof is provided in the appendix.

|wi|
kwk1

log( |wi|
µikwk1

)

+s log(log2

n

4XW1

γ

)

+r log(1/δ)

2n

4.3 PAC-Bayes Theorem

We now show that (a form of) the PAC Bayesian theorem [McAllester  1999] is a consequence of
Theorem 3. In the PAC Bayesian theorem  we have a set of hypothesis (possibly inﬁnite) C. We
choose some prior distribution over this hypothesis set say µ  and after observing the training data 
we choose any arbitrary posterior ν and the loss we are interested in is ℓν(x  y) = Ec∼νℓ(c  x  y)
that is basically the expectation of the loss when hypothesis c ∈ C are drawn i.i.d. using distribution
ν. Note that in this section we are considering a more general form of the loss.
The key observation as that we can view ℓν(x) as the inner product hdν(·)  ℓ(·  x  y)i between the
measure dν(·) and the loss ℓ(·  x). This leads to the following straightforward corollary.
Corollary 8. (PAC-Bayes) For a ﬁxed prior µ over the hypothesis set C  and any loss bounded by 1 
with probability at least 1 − δ over the sample  simultaneously for all choice of posteriors ν over C
we have that 

Lν ≤ ˆLν + 4.5r max{KL(νkµ)  2}

n

+r log(1/δ)

2n

(8)

Proof. Proof is provided in the appendix.

Interestingly  this result is an improvement over the original statement  in which the last term was

plog(n/δ)/n. Our bound removes this extra log(n) factor  so  in the regime where we ﬁx ν and

examine large n  this bound is sharper. We note that our goal was not to prove the PAC-Bayes
theorem  and we have made little attempt to optimize the constants.

4.4 Covering Number Bounds

It is worth noting that using Sudakov’s minoration results we can obtain upper bound on the L2
(and hence also L1) covering numbers using the Gaussian complexities. The following is a direct
corollary of the Sudakov minoration theorem for Gaussian complexities (Theorem 3.18  Page 80 of
Ledoux and Talagrand [1991]).
Corollary 9. Let FW be the function class from Theorem 3. There exists a universal constant K > 0
such that its L2 covering number is bounded as follows:

∀ǫ > 0

log(N2(FW   ǫ  n)) ≤

2K 2X 2W 2
∗

σǫ2

This bound is sharper than those that could be derived from the N∞ covering number bounds of
Zhang [2002].

5 Discussion: Relations to Online  Regret Minimizing  Algorithms

In this section  we make a further assumption that loss ℓ(hw  xi   y) is convex in its ﬁrst argument.
We now show that in the online setting that the regret bounds for linear prediction closely match our
risk bounds. The algorithm we consider performs the update 

wt+1 = ∇F −1(∇F (wt) − η∇wℓ(hwt  xti   yt))

(9)

This algorithm captures both gradient updates  multiplicative updates  and updates based on the Lp
norms  through appropriate choices of F . See Shalev-Shwartz [2007] for discussion.

For the algorithm given by the above update  the following theorem is a bound on the cumulative
regret. It is a corollary of Theorem 1 in Shalev-Shwartz and Singer [2006] (and also of Corollary 1
in Shalev-Shwartz [2007])  applied to our linear case.
Corollary 10. (Shalev-Shwartz and Singer [2006]) Let S be a closed convex set and let F : S → R
be σ-strongly convex w.r.t. k · k∗. Further  let X = {x : kxk ≤ X} and W = {w ∈ S : F (w) ≤
W 2
∗ }. Then for the update given by Equation 9 if we start with w1 = argmin F (w)  we have that
for all sequences {(xt  yt)}n

t=1 

n

Xt=1

ℓ(hwt  xti   yt) − argmin

w∈W

Xt=1

n

ℓ(hw  xti   yt) ≤ LℓXW∗r 2n

σ

For completeness  we provide a direct proof in the Appendix.
Interestingly  the regret above is
precisely our complexity bounds (when Lℓ = 1). Also  our risk bounds are a factor of 2 worse 
essentially due to the symmetrization step used in proving Theorem 1.

References

P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results.

Journal of Machine Learning Research  3:463–482  2002.

P. L. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machines and other pattern
classiﬁers. In B. Sch¨olkopf  C. J. C. Burges  and A. J. Smola  editors  Advances in Kernel Methods – Support
Vector Learning  pages 43–54. MIT Press  1999.

N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.

V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of

combined classiﬁers. Annals of Statistics  30(1):1–50  2002.

J. Langford and J. Shawe-Taylor. PAC-Bayes & margins.

In Advances in Neural Information Processing

Systems 15  pages 423–430  2003.

J. Langford  M. Seeger  and Nimrod Megiddo. An improved predictive accuracy bound for averaging classiﬁers.

In Proceedings of the Eighteenth International Conference on Machine Learning  pages 290–297  2001.

M. Ledoux and M. Talagrand. Probability in Banach spaces: Isoperimetry and processes  volume 23 of Ergeb-

nisse der Mathematik und ihrer Grenzgebiete (3). Springer-Verlag  1991.

David A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In Proceedings of the Sixteenth Annual Con-

ference on Computational Learning Theory  pages 203–215  2003.

David A. McAllester. PAC-Bayesian model averaging. In Proceedings of the Twelfth Annual Conference on

Computational Learning Theory  pages 164–170  1999.

Ron Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine

Learning Research  4:839–860  2003.

A.Y. Ng. Feature selection  l1 vs. l2 regularization  and rotational invariance. In Proceedings of the Twenty-First

International Conference on Machine Learning  2004.

David Pollard. Convergence of Stochastic Processes. Springer-Verlag  1984.

R.E. Schapire  Y. Freund  P. Bartlett  and W.S. Lee. Boosting the margin: A new explanation for the effective-

ness of voting methods. The Annals of Statistics  26(5):1651–1686  October 1998.

S. Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis  The Hebrew Univer-

sity  2007.

S. Shalev-Shwartz and Y. Singer. Convex repeated games and Fenchel duality. In Advances in Neural Informa-

tion Processing Systems 20  2006.

M. Warmuth and A. K. Jagota. Continuous versus discrete-time non-linear gradient descent: Relative loss
In Fifth International Symposium on Artiﬁcial Intelligence and Mathematics 

bounds and convergence.
1997.

T. Zhang. Covering number bounds of certain regularized linear function classes. Journal of Machine Learning

Research  2:527–550  2002.

,Ryosuke Matsushita
Toshiyuki Tanaka
Di He
Yingce Xia
Tao Qin
Liwei Wang
Nenghai Yu
Tie-Yan Liu
Wei-Ying Ma