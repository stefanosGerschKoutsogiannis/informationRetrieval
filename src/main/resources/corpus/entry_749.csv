2018,Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC,We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems  arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites global non-convex optimization on the spherical manifold of quaternions  with posterior sampling  in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of solutions. We devise theoretical convergence guarantees and extensively evaluate our method on synthetic and real benchmarks. Besides its elegance in formulation and theory  we show that our method is robust to missing data  noise and the estimated uncertainties capture intuitive properties of the data.,Bayesian Pose Graph Optimization via Bingham
Distributions and Tempered Geodesic MCMC

Tolga Birdal1 2

Umut ¸Sim¸sekli3

M. Onur Eken1 2

Slobodan Ilic1 2

1 CAMP Chair  Technische Universität München  85748  München  Germany

2 Siemens AG  81739  München  Germany

3 LTCI  Télécom ParisTech  Université Paris-Saclay  75013  Paris  France

Abstract

We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algo-
rithm for initializing pose graph optimization problems  arising in various scenarios
such as SFM (structure from motion) or SLAM (simultaneous localization and
mapping). TG-MCMC is ﬁrst of its kind as it unites global non-convex optimiza-
tion on the spherical manifold of quaternions with posterior sampling  in order to
provide both reliable initial poses and uncertainty estimates that are informative
about the quality of solutions. We devise theoretical convergence guarantees and
extensively evaluate our method on synthetic and real benchmarks. Besides its
elegance in formulation and theory  we show that our method is robust to missing
data  noise and the estimated uncertainties capture intuitive properties of the data.

1

Introduction

The ability to navigate autonomously is now a key technology in self driving cars  unmanned aerial
vehicles (UAV)  robot guidance  augmented reality  3D digitization  sensory network localization and
more. This ubiquitous appliance is due to the fact that vision sensors can provide cues to directly
solve 6DoF pose estimation problem and do not necessitate external tracking input  such as imprecise
GPS  to ego-localize. Many of the problems in these domains can now be addressed by tailor-made
pipelines such as SLAM (Simultaneous Localization and Mapping)  SfM (Structure From Motion)
or multi robot localization (MRL) [1  2]. Nowadays  thanks to the resulting reliable estimates of
rotations and translations  many of these pipelines exploit some form of an optimization  such as
bundle adjustment (BA) [3] or 3D global registration [4  5]  that can globally consider the acquired
measurements [6]. Holistically  these methods belong to the family of pose graph optimization
(PGO) [7]. Unfortunately  many of PGO post-processing stages  which take in to account both camera
poses and 3D structure  are too costly for online or even soft-realtime operation. This bottleneck
demands good solutions for PGO initialization  that can relieve the burden of the joint optimization.
In this paper  we address the particular problem of initializing PGO  in which multiple local mea-
surements are fused into a globally consistent estimate  without resorting to the costly bundle
adjustment or optimization that uses structure. In speciﬁcs  let us consider a ﬁnite simple directed
graph G = (V  E)  where vertices correspond to reference frames and edges to the available rela-
tive measurements as shown in Figures 1(a)  1(b). Both vertices and edges are labeled with rigid
motions representing absolute and relative poses  respectively. Each absolute pose is described by
a homogeneous transformation matrix {Mi ∈ SE(3)}n
i=1. Similarly  each relative orientation is
expressed as the transformation between frames i and j  Mij  where (i  j) ∈ E ⊂ [n] × [n]. The
labeling of the edges is such that if (i  j) ∈ E  then (j  i) ∈ E and Mij = M−1
ji . Hence  we consider
G to be undirected. With a convention as shown in Figure 1(c)  the link between absolute and relative
transformations is encoded by the compatibility constraint:

Mij ≈ MjM−1

  ∀i (cid:54)= j

i

(1)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: From left to right: (a) Initial pose graph of relative poses. (b) Absolute poses w.r.t. common
reference frame. (c) Convention used to describe the pairwise relationships. (d) A sample Bingham
distribution and the rotational components.

Primarily motivated by Govindu et. al. [8]  rigid-motion synchronization initializes PGO by comput-
ing an estimate of the vertex labels Mi (absolute poses) given enough measurements of the ratios
Mij. In other words  it tries to ﬁnd the absolute poses that best ﬁt the relative pairwise measurements.
Typically  in order to remove the gauge freedom  one of the poses is set to identity M0 = I and the
problem reduces to recovering n − 1 absolute poses. The solution is the state of the art method to
initialize  say SfM [1  9  10] thanks to the good quality of the estimates.
The PGO problem is often formed as non-convex optimization problems  opening up room for
different formulations and approaches. Direct methods try to compute a good initial solution [11  9 
12  13]  which are then reﬁned by iterative techniques [14  15]. Robustness to outlier relative pose
estimates is also crucial for a better solution [16  17  10  18  2]. The structure of our peculiar problem
allows for global optimization  when isotropic noise is assumed and under reasonable noise levels
as well as well connected graph structures [11  19  20  21  22  23]. It is also noteworthy that  even
though the problem has been previously handled with statistical approaches [24]  up until now  to the
best of our knowledge  estimation of uncertainties in PGO initialization are never truly considered.
In this paper  we look at the graph optimization problem from a probabilistic point of view. We begin
by representing the problem on the Cartesian product of the true Riemannian manifold of quaternions
and Euclidean manifold of translations. We model rotations with Bingham distributions [25] and
translation with Gaussians. The probabilistic framework provides two important features: (i) we can
align the modes of the data (relative motions) with the posterior parameters  (ii) we can quantify the
uncertainty of our estimates by using the posterior predictive distributions. In order to achieve these
goals  we come up with efﬁcient algorithms both for maximum a-posteriori (MAP) estimation and
posterior sampling: ‘tempered’ geodesic Markov Chain Monte Carlo (TG-MCMC). Controlled by
a single parameter  TG-MCMC can either work as a standard MCMC algorithm that can generate
samples from a Bayesian posterior  whose entropy  or covariance  as well as the samples themselves 
provide necessary cues for uncertainty estimation - both on camera poses and possibly on the 3D
structure  or it can work as an optimization algorithm that is able to generate samples around the
global optimum of the MAP estimation problem. In this perspective  TG-MCMC bridges the gap
between geodesic MCMC (gMCMC) [26] and non-convex optimization  as we will theoretically
present. In a nutshell  our contributions are as follows:
• Novel probabilistic model using Bingham distributions in pose averaging for the ﬁrst time 
• Tempered gMCMC: Novel tempered Hamiltonian Monte Carlo (HMC) [27  28  29] algorithm for
global optimization and sampling on the manifolds using the known geodesic ﬂow 
• Theoretical understanding and convergence guarantees for the devised algorithm 
• Strong experimental results justifying the validity of the approach.

2 Preliminaries and Technical Background
Notation and deﬁnitions: x ∈ R is a scalar. We denote vectors by lower case bold letters x =
(x1 ··· xN ) ∈ RN . A square matrix X = (Xij) ∈ RN×N . IN×N is the identity matrix. Rotations
belong to the special orthogonal group R ∈ SO(3). With translations t ∈ R3  they form the
3D special Euclidean group SE(3). We also deﬁne an m-dimensional Riemannian manifold M 
endowed with a Riemannian metric G to be a smooth curved space  equipped with the inner product
(cid:104)u  v(cid:105)x = uT Gv in the tangent space TxM  embedded in an ambient higher-dimensional Euclidean

2

𝐗𝐂1𝐂2𝐑𝐂1+𝐭𝜃𝐪𝑗𝐪𝑖−1𝐕1𝐪𝑖𝑗𝐕2(a) Initial pose graph of relative orientations.(c) Convention used to describe the pairwise relationships.(d) A sample Bingham distributionand the rotational components.𝐂𝒊𝐂𝒋𝑬𝒊𝒋𝐖𝐨𝐫𝐥𝐝𝐅𝐫𝐚𝐦𝐞…𝐀𝐛𝐬𝐨𝐥𝐮𝐭𝐞𝐩𝐨𝐬𝐞𝐬(b) Resulting poses w.r.t. a common frame.space Rn. One such manifold is the unit hypersphere in Rd: Sd−1 = {x ∈ Rd : (cid:107)x(cid:107) = 1} ⊂ Rd. A
vector v is said to be tangent to a point x ∈ M if xT v = 0. A tangent space is the set Tx of all such
vectors: Tx = {v ∈ Rd : xT v = 0}. We deﬁne the geodesic on the manifold to be a constant speed 
length minimizing curve between x  y ∈ M  γ : [0  1] → M  with γ(0) = x and γ(1) = y.
Quaternions: A quaternion q is an element of Hamilton algebra H  extending the complex numbers
with three imaginary units i  j  k in the form q = q11 + q2i + q3j + q4k = (q1  q2  q3  q4)T  with
(q1  q2  q3  q4)T ∈ R4 and i2 = j2 = k2 = ijk = −1. We also write q := [a  v] with the scalar part
a = q1 ∈ R and the vector part v = (q2  q3  q4)T ∈ R3. The conjugate ¯q of the quaternion q is
given by ¯q := q1 − q2i − q3j − q4k. A versor or unit quaternion q ∈ H1 with 1
= (cid:107)q(cid:107) := q · ¯q
and q−1 = ¯q  gives a compact and numerically stable parametrization to represent orientation of
objects in S3  avoiding gimbal lock and singularities [30  31]. Identifying antipodal points q and
−q with the same element  the unit quaternions form a double covering group of SO (3). The
non-commutative multiplication of two quaternions p := [p1  vp] and r := [r1  vr] is deﬁned to be
p ⊗ r = [p1r1 − vp · vr  p1vr + r1vp + vp × vr]. For simplicity we use p ⊗ r := p · r := pr.
Manifold of quaternions: Unit quaternions form a hyperspherical manifold  S3  that is an embedded
Riemannian submanifold of R4. This forms a Hausdorff space  where each point has an open
neighborhood homeomorphic to the open N-dimensional disc  called an N-manifold. Due to the
topology of the sphere  there is no unique way ﬁnd a globally covering coordinate patch. It is hence
common to use local exponential and logarithmic maps that can be sphere-speciﬁcally deﬁned as:
Exp(x  v) = x cos(θ) + v sin(θ)/θ  where v denotes a tangent vector to x. This property decorates
quaternions with a known analytic geodesic ﬂow  given by [26]:

!

(cid:21)(cid:20)cos(αt) − sin(αt)

(cid:21)(cid:20)1

0

(cid:21)

(cid:20)1

0

0
0 α

1/α

sin(αt)

cos(αt)

[x(t) v(t)] = [x(0) v(0)]

(2)
where α (cid:44) (cid:107)v(0)(cid:107). It is also useful to think about a quaternion as the normal vector to itself  due to
the unitness of the hypersphere. By this property  projection onto Tx reads P (x) = I − xxT [26].
The Bingham Distribution: Derived from a zero-mean Gaussian  the Bingham distribution [25] is
an antipodally symmetric probability distribution conditioned to lie on Sd−1 with probability density
function (PDF) B : Sd−1 → R:

B(x; Λ  V) = (1/F ) exp(xT VΛVT x) = (1/F ) exp(cid:0)(cid:88)d

(3)
where V ∈ Rd×d is an orthogonal matrix (VVT = VT V = Id×d) describing the orientation 
Λ = diag(0  λ1 ···   λd−1) ∈ Rd×d with 0 ≥ λ1 ≥ ··· ≥ λd−1 is the concentration matrix  and F
is a normalization constant. With this formulation  the mode of the distribution is obtained as the
ﬁrst column of V. The antipodal symmetry of the PDF makes it amenable to explain the topology of
quaternions  i. e.  B(x;·) = B(−x;·) holds for all x ∈ Sd−1. When d = 4 and λ1 = λ2 = λ3  it is
safe to write Λ = diag([1  0  0  0]). In this case  the logarithm of the Bingham density reduces to the
dot product of two quaternions q1 (cid:44) x and the mode of the distribution  say ¯q2. For rotations  this
induces a metric  dbingham = (q1 · ¯q2)2 = cos(θ/2)2  that is closely related to the true Riemannian
distance driemann = (cid:107)log(R1RT
have been extensively used to represent distributions on quaternions [32  33  34]; however  to the best
of our knowledge  never for the problem at hand.

2 )(cid:107) (cid:44) 2arccos(|q1¯q2|) (cid:44) 2arccos((cid:112)dbingham). Bingham distributions

i x)2(cid:1)

λi(vT

i=1

3 The Proposed Model
We now describe our proposed model for PGO initialization. We consider the situation where we
observe a set of noisy pairwise poses Mij  represented by augmented quaternions as {qij ∈ S3 ⊂
R4  tij ∈ R3}. The indices (i  j) ∈ E run over the edges the graph. We assume that the observations
{qij  tij}(i j)∈E are generated by a probabilistic model that has the following hierarchical structure:
(4)
where the latent variables {qi ∈ S3}n
i=1 denote the true values of the absolute
poses and absolute translations with respect to a common origin  corresponding to Mi of Eq. 1. Here 
p(qi) and p(ti) denote the prior distributions of the latent variables  and the product of the densities
p(qij|·) and p(tij|·) form the likelihood function.

qij|· ∼ p(qij|qi  qj) 
i=1 and {ti ∈ R3}n

tij|· ∼ p(tij|qi  qj  ti  tj) 

qi ∼ p(qi) 

ti ∼ p(ti) 

3

By respecting the natural manifolds of the latent variables  we choose the following prior model:qi ∼
B(Λp  Vp)  ti ∼ N (0  σ2
p are the prior model parameters  which are
assumed to be known. We then choose the following model for the observed variables:

pI) where Λp  Vp  and σ2

qij|qi  qj ∼ B(Λ  V(qj ¯qi)) 

tij|qi  qj  ti  tj ∼ N (µij  σ2I) 

(5)

where Λ is a ﬁxed  V is a matrix-valued function that will be deﬁned in the sequel; µij denotes the
expected value of tij provided that the values of the relevant latent variables qi qj  ti  tj are known 
and has the form: µij (cid:44) tj − (qj ¯qi)ti(qi ¯qj). With this modeling strategy  we are expecting that tij
would be close to the true translation µij that is a deterministic function of the absolute poses. Our
strategy also lets tij differ from µij and the level of this ﬂexibility is determined by σ2.
Constructing Bingham distribution on any given mode q ∈ S3 requires ﬁnding a frame bundle
S3 → FS3 composed of the unit vector (the mode) and its orthonormals. Being parallelizable
(d = 1  2  4 or 8)  manifold of unit quaternions enjoys an injective homomorphism to the orthonormal
matrix ring composed of the orthonormal basis [35]. Thus  we deﬁne V : S3 (cid:55)→ R4×4 as follows:
It is easy to verify that V(q) is orthonormal for every q ∈ S3.
V(q) further gives a convenient notation for representing
quaternions as matrices paving the way to linear operations 
such as quaternion multiplication or orthonormalization with-
out pesky Gram-Schmidt processes. By using the deﬁnition of V(q) and assuming that the diagonal
entries of Λ are sorted in decreasing order  we have the following property:

.
(cid:8)p(qij|qi  qj) = B(Λ  V(qj ¯qi))(cid:9) = qj ¯qi.

q1 −q2 −q3

q4
q1
q2
q3
q4
q3 −q4
q1 −q2
q3 −q2 −q1
q4

V(q) (cid:44)

(6)

arg max

qij

Similar to the proposed observation model for the relative translations  given the true poses qi  qj 
this modeling strategy sets the most likely value of the relative pose to the deterministic value qj ¯qi 
and also lets qij differ from this value up to the extent determined by Λ. This conﬁguration is
illustrated in Fig 1(d).
Representing SE(3) in the form of a quaternion-translation parameterization  we can now formulate
the motion-synchronization problem as a probabilistic inference problem. In particular we are
interested in the following two quantities:
(cid:88)
1. The maximum a-posteriori (MAP) estimate: (Q(cid:63)  T(cid:63)) = arg maxQ T p(Q  T|D) =

(cid:8) log p(qij|Q  T) + log p(tij|Q  T)(cid:9) +

(cid:16) (cid:88)

(cid:88)

log p(qi) +

log p(ti)

arg max

  (7)

(cid:17)

Q T

(i j)∈E

i

i

where D ≡ {qij  tij}(i j)∈E denotes the observations  Q ≡ {qi}n

i=1 and T ≡ {ti}n

i=1.

2. The full posterior distribution: p(Q  T|D) ∝ p(D|Q  T) × p(Q) × p(T).
Both of these problems are very challenging and cannot be directly addressed by standard methods
such as gradient descent (problem 1) or standard MCMC methods (problem 2). The difﬁculty in
these problems is mainly originated by the fact that the posterior density is non-log-concave (i.e. the
negative log-posterior is non-convex) and any algorithm that aims at solving one of these problems
should be able to operate in the particular manifold of this problem  that is (S3)n × R3n ⊂ R7n.

4 Tempered Geodesic Monte Carlo for Pose Graph Optimization
Connection between sampling and optimization: In a recent study [36]  Liu et al. proposed the
stochastic gradient geodesic Monte Carlo (SG-GMC) as an extension to [26] and provided a practical
posterior sampling algorithm for the problems that are deﬁned on manifolds whose geodesic ﬂows
are analytically available. Since our augmented quaternions form such a manifold1  we can use this
algorithm for generating (approximate) samples from the posterior distribution  which would address
the second problem deﬁned in Section 3.

1The manifold (S3)n × R3n can be expressed as a product of the manifolds S3 (n times) and R3n. Therefore 
its geodesic ﬂow is the combination of the geodesic ﬂows of individual manifolds. Since the geodesic ﬂows in
Sd−1 and Rd are analytically available  so is the ﬂow of the product manifold [26].

4

Recent studies have shown that SG-MCMC techniques [37  38  39  40  41] are closely related to
optimization [42  43  44  45  28  29] and they indeed have a strong potential in non-convex problems
due to their randomized nature. In particular  it has been recently shown that  a simple variant of
SG-MCMC is guaranteed to converge to a point near a local optimum in polynomial time [46  47]
and eventually converge to a point near the global optimum [43]  even in non-convex settings. Even
though these recent results illustrated the advantages of SG-MCMC in optimization  it is not clear
how to develop an SG-MCMC-based optimization algorithm that can operate on manifolds. In this
section  we will extend the SG-GMC algorithm in this vein to obtain a parametric algorithm  which is
able to both sample from the posterior distribution and perform optimization for obtaining the MAP
estimates depending on the choice of the practitioner. In other words  the algorithm should be able to
address both problems that we deﬁned in Section 3 with theoretical guarantees.
We start by deﬁning a more compact notation that will facilitate the presentation of the algorithm.
We deﬁne the variable x ∈ X   such that x (cid:44) [q(cid:62)
n ](cid:62) and X (cid:44) (S3)n ×
R3n. The posterior density of interest then has the form πH(x) (cid:44) p(x|D) ∝ exp(−U (x)) with
respect to the Hausdorff measure  where U is called the potential energy has the following form:
U (x) (cid:44) −(log p(D|x) + log p(x)) = −(log p(D|Q  T) + log p(Q) + log p(T)). We deﬁne a
smooth embedding ξ : R6n (cid:55)→ X such that ξ(˜x) = x. If we consider the embedded posterior density
πλ(˜x) (cid:44) p(˜x|D) with respect to the Lebesgue measure  then by the area formula (cf. Theorem 1

in [48])  we have the following key property: πH(x) = πλ(˜x)/(cid:112)|G(˜x)|  where |G| denotes the
determinant of the Riemann metric tensor [G(˜x)]i j (cid:44)(cid:80)7n

for all i  j ∈ {1  . . .   6n}.

1   . . .   q(cid:62)

1   . . .   t(cid:62)

n   t(cid:62)

The main idea in our approach is to introduce an inverse temperature variable β ∈ R+ and consider
the tempered posterior distributions whose density is proportional to exp(−βU (x)). When β = 1 
this density coincides with the original posterior; however  as β goes to inﬁnity  the tempered density
concentrates near the global minimum of the potential U [49  50]. This important property implies
that  for large enough β  a random sample that is drawn from the tempered posterior would be close
to the global optimum and can therefore be used as a MAP estimate.
Construction of the algorithm: We will now construct the proposed algorithm. In particular  we
will ﬁrst extend the continuous-time Markov process proposed in [36] and develop a process whose
marginal stationary distribution has a density proportional to exp(−βU (x)) for any given β > 0.
Then we will develop practical algorithms for generating samples from this tempered posterior.
We propose the following stochastic differential equation (SDE) in the Euclidean space by making
use of the embedding ξ:
d˜xt = G(˜xt)−1ptdt

∂xl
∂ ˜xi

∂xl
∂ ˜xj

l=1

1
2

∇˜x log |G| + cpt +

∇˜x(p(cid:62)

t G−1pt)dt +

(2c/β)M(cid:62)M dWt 

(8)
where ∇˜xUλ (cid:44) −∇˜x log πλ  G and M are short-hand notations for G(˜xt) and [M(˜xt)]ij (cid:44)
∂xi/∂ ˜xj  respectively  pt ∈ R6n is called the momentum variable  c > 0 is called the friction  and
Wt denotes the standard Brownian motion in R6n.
We will ﬁrst analyze the invariant measure of the SDE (8).
t ](cid:62) ∈ R12n and (ϕt)t≥0 be a Markov process that is a solution of
Proposition 1. Let ϕt = [˜xt  p(cid:62)
the SDE (8). Then (ϕt)t≥0 has an invariant measure µϕ  whose density with respect to the Lebesgue
measure is proportional to exp(−Eλ(ϕ))   where Eλ is is deﬁned as follows:
p(cid:62)G(˜x)−1p.

Eλ(ϕ) (cid:44) βUλ(˜x) +

log |G(˜x)| +

(9)

dpt = −(cid:16)∇˜xUλ(˜xt) +

1
2

(cid:113)

β
2

β
2

All the proofs are given in the supplementary document. By using the area formula and the deﬁnitions
of G and M  one can show that the density of µϕ can also be written with respect to the Hausdorff
measure  as follows: (see Section 3.2 in [26] for details) EH(x  v) (cid:44) βU + β
2 v(cid:62)v  where v =
M(M(cid:62)M)−1p. This result shows that  if we could exactly simulate the SDE (8)  then the marginal
distribution of the sample paths would converge to a measure πx on X whose density is proportional
to exp(−βU (x)). Therefore  for β = 1 we would be sampling from πH (i.e. we recover SG-GMC) 
and for large β  we would be sampling near the global optimum of U. An illustration of the behavior
of β on a toy example is provided in the supplementary material.

5

(a)

(b)

(c)

(d)

(e)

Figure 2: Synthetic Evaluations. (a) Mean Riemannian error vs noise variance. (b) Mean Euclidean
(translational) error vs noise variance. (c) Riemannian error vs e for N = 50. e = |E|/N 2 refers to
graph completeness and N to the node count. (d) Euclidean error for N = 50 vs e. (e) Monitoring
the absolute error w.r.t. ground truth  during optimization and respective posterior sampling.

Numerical integration: We will now develop an algorithm for simulating (8) in discrete-time. We
follow the approach given in [26  36]  where we split (8) into three disjoint parts and solve those
parts analytically in an iterative fashion. The split SDE is given as follows:

(cid:26)d˜xt = G−1ptdt

A:

dpt = − 1

2∇(p(cid:62)

t G−1pt)dt

B:

(cid:26)d˜xt = 0

d˜xt = 0

+

O:

dpt = −cptdt

dpt = −(∇Uλ(˜xt) + 1

(cid:113) 2c
2∇ log |G|)dt
β M(cid:62)MdWt.
The nice property of these (stochastic) differential equations is that  each of them can be analytically
simulated directly on the manifold X   by using the identity x = ξ(˜x) and the deﬁnitions of G  M 
and v. In practice  one ﬁrst needs to determine a sequence for the A  B  O steps  set a step-size h for
integration along the time-axis t  and solve those steps one by one in an iterative fashion [51  39]. In
our applications  we have emprically observed that the sequence BOA provides better results among
several other combinations  including the ABOBA scheme that was used in [36]. We provide the
solutions of the A  B  O steps  as well as the required gradients in the supplementary material.
Theoretical analysis: In this section  we will provide non-asymptotic results for the proposed
algorithm. Let us denote the output of the algorithm {xk}N
k=1  where k denotes the iterations and N
denotes the number of iterations. In the MAP estimation problem  we are interested in ﬁnding x(cid:63) (cid:44)
arg minx U (x)  whereas for full Bayesian inference  we are interested in approximating posterior
k=1 φ(xk) 

expectations with ﬁnite sample averages  i.e. ¯φ (cid:44)(cid:82)

X φ(x)πH(x) dx ≈ ˆφ (cid:44) (1/N )(cid:80)N

where ˆUN (cid:44) (1/N )(cid:80)N

where φ is a test function.
As brieﬂy discussed in [36]  the convergence behavior of the SG-GMC algorithm can be directly
analyzed within the theoretical framework presented in [39]. In a nutshell  the theory in [39] suggests
that  with the BOA integration scheme  the bias |E ˆφ − φ| is of order O(N−1/2).
In this study  we focus on the MAP estimation problem and analyze the ergodic error E[ ˆUN − U (cid:63)] 
k=1 U (xk) and U (cid:63) (cid:44) U (x(cid:63)). This error resembles the bias where the test
function φ is chosen as the potential U; however  on the contrary  it directly relates the sample average
to the global optimum. Similar ergodic error notions have already been considered in non-convex
optimization [52  53  28]. We present our main result in the following theorem. Due to space
limitations and for avoiding obscuring the results  we present the required assumptions and the
explicit forms of constants in the supplementary document.
Theorem 1. Assume that the conditions given in the supp. doc. hold. If the iterates are obtained by
using the BOA the scheme  then the following bound holds for β small enough and X = (S3)n×R3n:

(cid:12)(cid:12)E ˆUN − U (cid:63)(cid:12)(cid:12) = O(cid:0)β/(N h) + h/β + 1/β(cid:1) 

(10)

A1 (cid:44) E[ ˆUN − ¯Uβ] and A2 (cid:44) [ ¯Uβ − U (cid:63)] ≥ 0  and ¯Uβ (cid:44)(cid:82)

Sketch of the proof. We decompose the error into two terms: E[ ˆUN − U (cid:63)] = A1 + A2  where
X U (x)πx(dx). The term A1 is the bias
term  which we can bounded by using existing results. The rest of the proof deals with bounding A2 
where we incorporate ideas from [43]. The full proof resides in the supplementary.

6

0.040.070.10.20.5100.340.681.041.41.752.092.44chordalminspangovinduarrigonitorsellotg-mcmc0.040.070.10.20.5100.20.40.60.811.2minspangovindutorsellotg-mcmc0.160.20.240.320.40.71100101102minspanchordalarrigonigovindutorsellotg-mcmc0.160.20.240.320.4110-210-1100minspangovindutorsellotg-mcmc010020030001234Error (Rotation + Translation)0.30.40.50.60.70.80.91Consistency2002503000.1060.1080.112002503000.966810.966820.966830.966840.96685Table 1: Evaluations on EPFL Benchmark.

Torsello

Govindu

EIG-SE(3) TG-MCMC
Ozyesil et. al. R-GODEC
MRE MTE MRE MTE MRE MTE MRE MTE MRE MTE MRE MTE
0.007 0.040 0.009 0.106 0.015 0.106 0.015 0.040 0.004 0.106 0.015
HerzJesus-P8
0.060
0.065 0.130 0.038 0.081 0.020 0.081 0.020 0.070 0.010 0.081 0.020
HerzJesus-P25 0.140
Fountain-P11
0.030
0.004 0.030 0.006 0.071 0.004 0.071 0.004 0.030 0.004 0.071 0.004
0.203 0.440 0.433 0.101 0.035 0.101 0.035 0.040 0.009 0.090 0.035
0.560
Entry-P10
1.769 1.570 1.493 0.393 0.147 0.393 0.147 1.480 0.709 0.393 0.148
3.690
Castle-P19
1.393 0.780 1.123 0.631 0.323 0.629 0.321 0.530 0.212 0.622 0.285
1.970
Castle-P30
0.574 0.498 0.517 0.230 0.091 0.230 0.090 0.365 0.158 0.227 0.085
Average
1.075

Theorem 1 shows that the proposed algorithm will eventually provide samples that are close to the
global optimizer x(cid:63) even when U is non-convex. This result is fundamentally different from the
guarantees for the existing convex optimization algorithms on manifolds [54  55]  and is mainly due
to the stochasticity of the algorithm that is introduced by the Brownian motion. However  despite
this nice theoretical property  in practice our algorithm will still be affected by the meta-stability
phenomenon  where it will converge near a local minimum and stay there for an exponential amount
of time [47].
We also note that our proof covers only the case where X = (S3)n × R3n; however  we believe that
it can be easily extended to more general setting. We also note that our gradient computations can be
replaced with stochastic gradients in the case of large-scale applications where the number of data
points can be prohibitively large  so that computing the gradients at each iteration becomes practically
infeasible. The same theoretical results hold as long as the stochastic gradients are unbiased.

5 Experiments
In a sequel of evaluations  we will be benchmarking our TG-MCMC against the state of the art
methods including subsets of: convex programming of Ozyesil et. al. [56]  Lie algebraic method
of Govindu [15]  dual quaternions linearization of Torsello et. al. [15]  direct EIG-SE3 method of
Arrigoni [12] and R-GODEC [57]. We also include two baseline methods: 1. propagating the pose
information along one possible minimum spanning tree  2. the chordal averaging [58].
Synthetic Evaluations: We ﬁrst synthesize random problems by drawing quaternions from Bingham
and translations from Gaussian distributions  and randomly dropping (100|E|/N 2)% edges from a
fully connected pose graph. On these problems  we run a series of tests including monitoring the
gradient steps  noise robustness  tolerance to graph completeness (sparsity) and ﬁdelity w.r.t. ground
truth. For each test  we distort the graph for the entity we test  i.e. add noise on nodes if we test the
noise resilience. The rotational errors are evaluated by the true Riemannian distance  (cid:107)log(RT ˆR)(cid:107) 
the translations by Euclidean [59]. Fig. 2 plots our ﬁndings. It is noticeable that our accuracy is
always on par with or better than the state of the art for moderate problems. In presence of increased
noise (Figures 2(a)  2(b)) or sparsiﬁed graph structure leading to missing data (Figures 2(c)  2(d)) 
our method shows clear advantage in both rotational and transnational components of the error. This
is thanks to our probabilistic formulation and theoretically grounded inference scheme.
Results in Real Data: We now evaluate our framework by running SFM on the EPFL Bench-
mark [60]  that provide 8 to 30 images per dataset  along with ground-truth camera transformations.
Similar to [12]  we use the ground truth scale to circumvent its ambiguity. The mean rotation and
translation errors (MRE  MTE) are depicted in Tab. 1. Notice that when rotations and translations
are combined  our optimization results in superior minimum for both  not to mention the uncertainty
information computed as a by-product. While many methods can perform similarly on easy sets 
a clear advantage is visible on Castle sequences where severe noise and missing connections are
present. There  for instance  EIG-SE(3) also fails to ﬁnd a good closed form solution.
Next  we qualitatively demonstrate the unique capability of our method  uncertainty estimation on
various SFM problems and datasets [61  62  60]. To do so  we ﬁrst run our optimizer setting β to
inﬁnity2 for > 400 iterations. After that point  depending on the dataset  we set β to a smaller value
2Note that the case β → ∞ renders the SDE degenerate and hence  cannot be analyzed by using our tools.

However  due to meta-stability  the algorithm performs similarly either for large β or for β → ∞.

7

Figure 3: Uncertainty estimation in the Dante Square. From left to right: the colored reconstruction
(bundle adjustment used in 3D structure only)  a sample image from the dataset  reconstructed points
colored w.r.t. uncertainty value  a close-up to the center of the square  Dante statue.

Figure 4: Visualization of uncertainty in Notre Dame  Angel  Dinasour and Fountain datasets.

(∼ 1000)  allowing the sampling of posterior for 40 times. This behaviour is shown in Fig. 2(e). For
each sample  that is a solution of the problem in Eq. 1  we perform a 3D reconstruction  similar to
[16]: We ﬁrst estimate 2D keypoints and relative rotations by running 1) VSFM [63] 2) two-frame
bundle adjustment [64  65] (BA) on image pairs  resulting in pairwise poses  as well as a rough
two-view 3D structure. We run our method on these relative poses  computing the absolute estimates.
Fixing the estimated poses  a second BA then optimizes for the optimal 3D structure. At the end 
we obtain 40 3D scenes per dataset. For each point of each scene  we record the mean and variance
across different reconstructions  transferring the uncertainty estimation to the 3D cloud of points.
In Figures 3 and 4  we colorize each point by mapping the uncertainty value to RGB space using a
jet-colormap  with a scale proportional to the diameter of reconstruction. It is consistently visible
that our uncertainty estimates could capture regions of space where there are more and reliable data:
Outlying points  noise or distant structures can be identiﬁed by interpreting the uncertainty.

6 Conclusion

We have proposed TG-MCMC  a manifold-aware  tempered rigid motion synchronization algorithm
with a novel probabilistic formulation. TG-MCMC enjoys unique properties of trading-off approx-
imately globally optimal solutions with non-asymptotic guarantees  to drawing samples from the
posterior distribution  providing uncertainty estimates for the PGO-initialization problem.
Our algorithm paves the way to a diverse potential future research: First  stochastic gradients can
be employed to handle large problems  scaling up to hundreds of thousands of nodes. Next  the
uncertainty estimates can be plugged into existing pipelines such as BA or PGO to further improve
their quality. We also leave it as a future work to investigate different simulation schemes by altering
the order of and combining differently the A  B  and O steps. Finally  TG-MCMC can be extended to
different problems  still maintaining its nice theoretical properties.

8

Acknowledgements

We would like to thank Robert M. Gower and François Portier for fruitful discussions and Hans
Peschke for his feedback and efforts in verifying the correctness of our descriptions. We thank
Antonio Vargas of the Mathematics-StackExchange for providing the reference on inequalities for
generalized hypergeometric functions. This work is partly supported by the French National Research
Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014) and by the industrial
chair Machine Learning for Big Data from Télécom ParisTech.

References
[1] Arno Knapitsch  Jaesik Park  Qian-Yi Zhou  and Vladlen Koltun. Tanks and temples: Bench-
marking large-scale scene reconstruction. ACM Transactions on Graphics (TOG)  36(4):78 
2017.

[2] Luca Carlone and Giuseppe Carlo Calaﬁore. Convex relaxations for pose graph optimization

with outliers. IEEE Robotics and Automation Letters  3:1160–1167  2018.

[3] Bill Triggs  Philip F McLauchlan  Richard I Hartley  and Andrew W Fitzgibbon. Bundle
In International workshop on vision algorithms  pages

adjustment—a modern synthesis.
298–372. Springer  1999.

[4] Tolga Birdal and Slobodan Ilic. Cad priors for accurate and ﬂexible instance reconstruction. In

The IEEE International Conference on Computer Vision (ICCV)  Oct 2017.

[5] Daniel F Huber and Martial Hebert. Fully automatic registration of multiple 3d data sets. Image

and Vision Computing  21(7):637–650  2003.

[6] Tolga Birdal  Emrah Bala  Tolga Eren  and Slobodan Ilic. Online inspection of 3d parts via a
locally overlapping camera network. In Applications of Computer Vision (WACV)  2016 IEEE
Winter Conference on  pages 1–10. IEEE  2016.

[7] Rainer Kümmerle  Giorgio Grisetti  Hauke Strasdat  Kurt Konolige  and Wolfram Burgard. g
2 o: A general framework for graph optimization. In Robotics and Automation (ICRA)  2011
IEEE International Conference on  pages 3607–3613. IEEE  2011.

[8] Venu Madhav Govindu. Combining two-view constraints for motion estimation. In Computer
Vision and Pattern Recognition  2001. CVPR 2001. Proceedings of the 2001 IEEE Computer
Society Conference on  volume 2  pages II–II. IEEE  2001.

[9] Luca Carlone  Roberto Tron  Kostas Daniilidis  and Frank Dellaert. Initialization techniques for
3d slam: a survey on rotation estimation and its use in pose graph optimization. In Robotics and
Automation (ICRA)  2015 IEEE International Conference on  pages 4597–4604. IEEE  2015.

[10] Roberto Tron  Xiaowei Zhou  and Kostas Daniilidis. A survey on rotation optimization in
structure from motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops  pages 77–85  2016.

[11] Johan Fredriksson and Carl Olsson. Simultaneous multiple rotation averaging using lagrangian

duality. In Asian Conference on Computer Vision  pages 245–258. Springer  2012.

[12] Federica Arrigoni  Andrea Fusiello  and Beatrice Rossi. Spectral motion synchronization in se

(3). arXiv preprint arXiv:1506.08765  2015.

[13] Federica Arrigoni  Andrea Fusiello  and Beatrice Rossi. Camera motion from group synchro-
nization. In 3D Vision (3DV)  2016 Fourth International Conference on  pages 546–555. IEEE 
2016.

[14] Andrea Torsello  Emanuele Rodola  and Andrea Albarelli. Multiview registration via graph
diffusion of dual quaternions. In Computer Vision and Pattern Recognition (CVPR)  2011 IEEE
Conference on  pages 2441–2448. IEEE  2011.

9

[15] Venu Madhav Govindu. Lie-algebraic averaging for globally consistent motion estimation. In
Computer Vision and Pattern Recognition  2004. CVPR 2004. Proceedings of the 2004 IEEE
Computer Society Conference on  volume 1  pages I–I. IEEE  2004.

[16] Avishek Chatterjee and Venu Madhav Govindu. Efﬁcient and robust large-scale rotation
averaging. In Computer Vision (ICCV)  2013 IEEE International Conference on  pages 521–528.
IEEE  2013.

[17] Richard Hartley  Khurrum Aftab  and Jochen Trumpf. L1 rotation averaging using the weiszfeld
algorithm. In Computer Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on 
pages 3041–3048. IEEE  2011.

[18] Avishek Chatterjee and Venu Madhav Govindu. Robust relative rotation averaging. IEEE

transactions on pattern analysis and machine intelligence  40(4):958–972  2018.

[19] Kyle Wilson  David Bindel  and Noah Snavely. When is rotations averaging hard? In European

Conference on Computer Vision  pages 255–270. Springer  2016.

[20] D.M. Rosen  L. Carlone  A.S. Bandeira  and J.J. Leonard. SE-Sync: A certiﬁably correct
algorithm for synchronization over the special Euclidean group. Technical Report MIT-CSAIL-
TR-2017-002  Computer Science and Artiﬁcial Intelligence Laboratory  Massachusetts Institute
of Technology  Cambridge  MA  February 2017.

[21] Jesus Briales and Javier Gonzalez-Jimenez. Fast global optimality veriﬁcation in 3d slam. In
Intelligent Robots and Systems (IROS)  2016 IEEE/RSJ International Conference on  pages
4630–4636. IEEE  2016.

[22] Anders Eriksson  Carl Olsson  Fredrik Kahl  and Tat-Jun Chin. Rotation averaging and strong
duality. In The IEEE Conference on Comptuter Vision and Pattern Recognition (CVPR)  June
2018.

[23] Jesus Briales and Javier Gonzalez-Jimenez. Initialization of 3d pose graph optimization using
lagrangian duality. In Robotics and Automation (ICRA)  2017 IEEE International Conference
on  pages 5134–5139. IEEE  2017.

[24] Roberto Tron and Kostas Daniilidis. Statistical pose averaging with non-isotropic and incomplete
relative measurements. In European Conference on Computer Vision  pages 804–819. Springer 
2014.

[25] Christopher Bingham. An antipodally symmetric distribution on the sphere. The Annals of

Statistics  pages 1201–1225  1974.

[26] Simon Byrne and Mark Girolami. Geodesic monte carlo on embedded manifolds. Scandinavian

Journal of Statistics  40(4):825–845  2013.

[27] R. M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo 

2(11):2  2011.

[28] Umut Simsekli  Cagatay Yildiz  Thanh Huy Nguyen  Ali Taylan Cemgil  and Gaël Richard.
Asynchronous stochastic quasi-Newton MCMC for non-convex optimization. In ICML 2018 
2018.

[29] X. Gao  M. Gürbüzbalaban  and L. Zhu. Global convergence of stochastic gradient Hamiltonian
Monte Carlo for non-convex stochastic optimization: Non-asymptotic performance bounds and
momentum-based acceleration. arXiv preprint arXiv:1809.04618  2018.

[30] Vincent Lepetit  Pascal Fua  et al. Monocular model-based 3d tracking of rigid objects: A

survey. Foundations and Trends R(cid:13) in Computer Graphics and Vision  1(1):1–89  2005.

[31] Benjamin Busam  Tolga Birdal  and Nassir Navab. Camera pose ﬁltering with local regression
geodesics on the riemannian manifold of dual quaternions. In IEEE International Conference
on Computer Vision Workshop (ICCVW)  October 2017.

10

[32] Jared Glover  Gary Bradski  and Radu Bogdan Rusu. Monte carlo pose estimation with
quaternion kernels and the bingham distribution. In Robotics: science and systems  volume 7 
page 97  2012.

[33] Gerhard Kurz  Igor Gilitschenski  Simon Julier  and Uwe D Hanebeck. Recursive estimation of
orientation based on the bingham distribution. In Information Fusion (FUSION)  2013 16th
International Conference on  pages 1487–1494. IEEE  2013.

[34] J. Glover and L. P. Kaelbling. Tracking the spin on a ping pong ball with the quaternion
bingham ﬁlter. In 2014 IEEE International Conference on Robotics and Automation (ICRA) 
pages 4133–4140  May 2014.

[35] Norman Earl Steenrod. The topology of ﬁbre bundles  volume 14. Princeton University Press 

1951.

[36] Chang Liu  Jun Zhu  and Yang Song. Stochastic gradient geodesic mcmc methods. In Advances

in Neural Information Processing Systems  pages 3009–3017  2016.

[37] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  pages
681–688  2011.

[38] Y. A. Ma  T. Chen  and E. Fox. A complete recipe for stochastic gradient MCMC. In Advances

in Neural Information Processing Systems  pages 2899–2907  2015.

[39] C. Chen  N. Ding  and L. Carin. On the convergence of stochastic gradient MCMC algorithms
with high-order integrators. In Advances in Neural Information Processing Systems  pages
2269–2277  2015.

[40] A. Durmus  U. Simsekli  E. Moulines  R. Badeau  and G. Richard. Stochastic gradient
Richardson-Romberg Markov Chain Monte Carlo. In Advances in Neural Information Process-
ing Systems  pages 2047–2055  2016.

[41] Umut Simsekli. Fractional Langevin Monte Carlo: Exploring Lévy driven stochastic differential
equations for Markov Chain Monte Carlo. In International Conference on Machine Learning 
2017.

[42] Arnak S Dalalyan. Further and stronger analogy between sampling and optimization: Langevin

monte carlo and gradient descent. arXiv preprint arXiv:1704.04752  2017.

[43] M. Raginsky  A. Rakhlin  and M. Telgarsky. Non-convex learning via stochastic gradient
Langevin dynamics: a nonasymptotic analysis. In Proceedings of the 2017 Conference on
Learning Theory  volume 65  pages 1674–1703  2017.

[44] U. Simsekli  R. Badeau  T. Cemgil  and G. Richard. Stochastic quasi-Newton Langevin Monte

Carlo. In International Conference on Machine Learning  pages 642–651  2016.

[45] N. Ye and Z. Zhu. Stochastic fractional Hamiltonian Monte Carlo. In International Joint Con-
ference on Artiﬁcial Intelligence  IJCAI-18  pages 3019–3025. International Joint Conferences
on Artiﬁcial Intelligence Organization  7 2018.

[46] Y. Zhang  P. Liang  and M. Charikar. A hitting time analysis of stochastic gradient langevin
dynamics. In Proceedings of the 2017 Conference on Learning Theory  volume 65  pages
1980–2022  2017.

[47] Belinda Tzen  Tengyuan Liang  and Maxim Raginsky. Local optimality and generalization
guarantees for the langevin algorithm via empirical metastability. In Conference on Learning
Theory  2018.

[48] Persi Diaconis  Susan Holmes  Mehrdad Shahshahani  et al. Sampling from a manifold. In
Advances in Modern Statistical Theory and Applications: A Festschrift in honor of Morris L.
Eaton  pages 102–125. Institute of Mathematical Statistics  2013.

[49] C. Hwang. Laplace’s method revisited: weak convergence of probability measures. The Annals

of Probability  pages 1177–1182  1980.

11

[50] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in Rˆd.

SIAM Journal on Control and Optimization  29(5):999–1018  1991.

[51] Ben Leimkuhler and Charles Matthews. Molecular Dynamics: With Deterministic and Stochas-

tic Numerical Methods  volume 39. Springer  2015.

[52] X. Lian  Y. Huang  Y. Li  and J. Liu. Asynchronous parallel stochastic gradient for nonconvex
optimization. In Advances in Neural Information Processing Systems  pages 2737–2745  2015.

[53] C. Chen  D. Carlson  Z. Gan  C. Li  and L. Carin. Bridging the gap between stochastic gradient

MCMC and stochastic optimization. In AISTATS  2016.

[54] H. Zhang and S. Sra. First-order methods for geodesically convex optimization. In Conference

on Learning Theory  pages 1617–1638  2016.

[55] Y. Liu  F. Shang  J. Cheng  H. Cheng  and L. Jiao. Accelerated ﬁrst-order methods for
geodesically convex optimization on Riemannian manifolds. In Advances in Neural Information
Processing Systems  pages 4875–4884  2017.

[56] Onur Özye¸sil  Amit Singer  and Ronen Basri. Stable camera motion estimation using convex

programming. SIAM Journal on Imaging Sciences  8(2):1220–1262  2015.

[57] Federica Arrigoni  Luca Magri  Beatrice Rossi  Pasqualina Fragneto  and Andrea Fusiello.
Robust absolute rotation estimation via low-rank and sparse matrix decomposition. In 3D Vision
(3DV)  2014 2nd International Conference on  volume 1  pages 491–498. IEEE  2014.

[58] Richard Hartley  Jochen Trumpf  Yuchao Dai  and Hongdong Li. Rotation averaging. Interna-

tional journal of computer vision  103(3):267–305  2013.

[59] Adrian Haarbach  Tolga Birdal  and Slobodan Ilic. Survey of higher order rigid body motion
interpolation methods for keyframe animation and continuous-time trajectory estimation. In 3D
Vision (3DV)  2018 Sixth International Conference on  pages 381–389. IEEE  2018.

[60] Christoph Strecha  Wolfgang Von Hansen  Luc Van Gool  Pascal Fua  and Ulrich Thoennessen.
On benchmarking camera calibration and multi-view stereo for high resolution imagery. In
Computer Vision and Pattern Recognition  2008. CVPR 2008. IEEE Conference on  pages 1–8.
Ieee  2008.

[61] 3df

zephyr

reconstruction

3df-zephyr-reconstruction-showcase/. Accessed: 2018-05-15.

showcase.

https://www.3dflow.net/

[62] Kyle Wilson and Noah Snavely. Robust global translations with 1dsfm. In Proceedings of the

European Conference on Computer Vision (ECCV)  2014.

[63] Changchang Wu et al. Visualsfm: A visual structure from motion system. 2011.

[64] Tolga Birdal  Ievgeniia Dobryden  and Slobodan Ilic. X-tag: A ﬁducial tag for ﬂexible and

accurate bundle adjustment. In IEEE International Conference on 3DVision  October 2016.

[65] Sameer Agarwal  Keir Mierle  and Others. Ceres solver. http://ceres-solver.org.

12

,Tolga Birdal
Umut Simsekli
Mustafa Onur Eken
Slobodan Ilic
Tatjana Chavdarova
Gauthier Gidel
François Fleuret
Simon Lacoste-Julien