2019,Learning to Perform Local Rewriting for Combinatorial Optimization,Search-based methods for hard combinatorial optimization are often guided by heuristics. Tuning heuristics in various conditions and situations is often time-consuming. In this paper  we propose NeuRewriter that learns a policy to pick heuristics and rewrite the local components of the current solution to iteratively improve it until convergence. The policy factorizes into a region-picking and a rule-picking component  each parameterized by a neural network trained with actor-critic methods in reinforcement learning. NeuRewriter captures the general structure of combinatorial problems and shows strong performance in three versatile tasks: expression simplification  online job scheduling and vehicle routing problems. NeuRewriter outperforms the expression simplification component in Z3; outperforms DeepRM and Google OR-tools in online job scheduling; and outperforms recent neural baselines and Google OR-tools in vehicle routing problems.,LearningtoPerformLocalRewritingforCombinatorialOptimizationXinyunChen∗UCBerkeleyxinyun.chen@berkeley.eduYuandongTianFacebookAIResearchyuandong@fb.comAbstractSearch-basedmethodsforhardcombinatorialoptimizationareoftenguidedbyheuristics.Tuningheuristicsinvariousconditionsandsituationsisoftentime-consuming.Inthispaper weproposeNeuRewriterthatlearnsapolicytopickheuristicsandrewritethelocalcomponentsofthecurrentsolutiontoitera-tivelyimproveituntilconvergence.Thepolicyfactorizesintoaregion-pickingandarule-pickingcomponent eachparameterizedbyaneuralnetworktrainedwithactor-criticmethodsinreinforcementlearning.NeuRewritercapturesthegeneralstructureofcombinatorialproblemsandshowsstrongperformanceinthreeversatiletasks:expressionsimpliﬁcation onlinejobschedulingandvehi-cleroutingproblems.NeuRewriteroutperformstheexpressionsimpliﬁcationcomponentinZ3[15];outperformsDeepRM[33]andGoogleOR-tools[19]inonlinejobscheduling;andoutperformsrecentneuralbaselines[35 29]andGoogleOR-tools[19]invehicleroutingproblems.21IntroductionSolvingcombinatorialproblemsisalong-standingchallengeandhasalotofpracticalapplications(e.g. jobscheduling theoremproving planning decisionmaking).Whileproblemswithspeciﬁcstructures(e.g. shortestpath)canbesolvedefﬁcientlywithprovenalgorithms(e.g dynamicprogram-ming greedyapproach search) manycombinatorialproblemsareNP-hardandrelyonmanuallydesignedheuristicstoimprovethequalityofsolutions[1 40 27].Althoughitisusuallyeasytocomeupwithmanyheuristics determiningwhenandwheresuchheuristicsshouldbeapplied andhowtheyshouldbeprioritized istime-consuming.Ittakescommercialsolversdecadestotunetostrongperformanceinpracticalproblems[15 44 19].Toaddressthisissue previousworksuseneuralnetworkstopredictacompletesolutionfromscratch givenacompletedescriptionoftheproblem[50 33 29 21].Whilethisavoidssearchandtuning adirectpredictioncouldbedifﬁcultwhenthenumberofvariablesgrows.Improvingiterativelyfromanexistingsolutionisacommonapproachforcontinuoussolutionspaces e.g trajectoryoptimizationinrobotics[34 47 31].However suchmethodsrelyingongradientinformationtoguidethesearch isnotapplicablefordiscretesolutionspacesduetoindifferentiablity.Toaddressthisproblem wedirectlylearnaneural-basedpolicythatimprovesthecurrentsolutionbyiterativelyrewritingalocalpartofituntilconvergence.Inspiredbytheproblemstructures thepolicyisfactorizedintotwoparts:theregion-pickingandtherule-pickingpolicy andistrainedend-to-endwithreinforcementlearning rewardingcumulativeimprovementofthesolution.Weapplyourapproach NeuRewriter tothreedifferentdomains:expressionsimpliﬁcation onlinejobscheduling andvehicleroutingproblems.WeshowthatNeuRewriterisbetterthanstrong∗WorkpartiallydonewheninterningatFacebookAIResearch.2Thecodeisavailableathttps://github.com/facebookresearch/neural-rewriter.33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.heuristicsusingmultiplemetrics.Forexpressionsimpliﬁcation NeuRewriteroutperformstheexpressionsimpliﬁcationcomponentinZ3[15].Foronlinejobscheduling underacontrolledsetting NeuRewriteroutperformsGoogleOR-tools[19]intermsofbothspeedandqualityofthesolution andDeepRM[33] aneural-basedapproachthatpredictsaholisticschedulingplan bylargemarginsespeciallyinmorecomplicatedsetting(e.g. withmoreheterogeneousresources).Forvehicleroutingproblems NeuRewriteroutperformstworecentneuralnetworkapproaches[35 29]andGoogleOR-tools[19].Furthermore extensiveablationstudiesshowthatourapproachworkswellindifferentsituations(e.g. differentexpressionlengths non-uniformjob/resourcedistribution) andtransferswellwhendistributionshifts(e.g. testonlongerexpressionsthanthoseusedfortraining).!"#"∼%#⋅|!"("∼%(⋅|!"[#"]!"+ =.(!" #" (")CurrentState(i.e.Solution)Region-PickerRule-Picker!"#"!"[#"]("!"+ Figure1:Theframeworkofourneuralrewriter.Giventhecurrentstate(i.e. solutiontotheoptimizationproblem)st weﬁrstpickaregionωtbytheregion-pickingpolicyπω(ωt|st) andthenpickarewritingruleutusingtherule-pickingpolicyπu(ut|st[ωt]) whereπu(ut|st[ωt])givestheprobabilitydistributionofapplyingeachrewritingruleu∈Utothepartialsolution.Oncethepartialsolutionisupdated weobtainanimprovedsolutionst+1andrepeattheprocessuntilconvergence.2RelatedWorkMethods.Usingneuralnetworkmodelsforcombinatorialoptimizationhasbeenexploredinthelastfewyears.Astraightforwardideaistoconstructasolutiondirectly(e.g. withaSeq2Seqmodel)fromtheproblemspeciﬁcation[50 6 33 28].However suchapproachesmightmeetwithdifﬁcultiesiftheproblemhascomplexconﬁgurations asourevaluationindicates.Incontrast ourpaperfocusesoniterativeimprovementofacompletesolution.Trajectoryoptimizationwithlocalgradientinformationhasbeenwidelystudiedinroboticswithmanyeffectivetechniques[34 9 51 47 32 31].Fordiscreteproblems itispossibletoapplycontinuousrelaxationandapplygradientdescent[10].Incontrast welearnthegradientfrompreviousexperiencetooptimizeacompletesolution similartodata-drivendescent[49]andsyntheticgradient[26].Atahighlevel ourframeworkiscloselyconnectedwiththelocalsearchpipeline.Speciﬁcally wecanleverageourlearnedRLpolicytoguidethelocalsearch i.e. todecidewhichneighborsolutiontomoveto.Wewilldemonstratethatinourevaluatedtasks ourapproachoutperformsseverallocalsearchalgorithmsguidedbymanuallydesignedheuristics andsoftwaressupportingmoreadvancedlocalsearchalgorithms i.e. Z3[15]andOR-tools[19].Applications.Forexpressionsimpliﬁcation somerecentworkusedeepneuralnetworkstodiscoverequivalentexpressions[11 2 52].Inparticular [11]trainsadeepneuralnetworktorewritealgebraicexpressionswithsupervisedlearning whichrequiresacollectionofgroundtruthrewritingpaths andmaynotﬁndnovelrewritingroutines.Wemitigatetheselimitationsusingreinforcementlearning.Jobschedulingandresourcemanagementproblemsareubiquitousandfundamentalincomputersystems.Variousworkhavestudiedtheseproblemsfromboththeoreticalandempiricalsides[8 20 3 42 48 33 13].Inparticular arecentlineofworkstudiesdeepreinforcementlearningforjobscheduling[33 13]andvehicleroutingproblems[29 35].Ourapproachistestedonmultipledomainswithextensiveablationstudies andcouldalsobeextendedtoothercloselyrelatedtaskssuchascodeoptimization[41 12] theoremproving[25 30 4 24] textsimpliﬁcation[14 37 18] andclassicalcombinatorialoptimizationproblemsbeyondroutingproblems[16 28 7 50 27] e.g. VertexCoverProblem[5].3ProblemSetupLetSbethespaceofallfeasiblesolutionsintheproblemdomain andc:S→Rbethecostfunction.Thegoalofoptimizationistoﬁndargmins∈Sc(s).Inthiswork insteadofﬁndingasolutionfromscratch weﬁrstconstructafeasibleone thenmakeincrementalimprovementbyiterativelyapplyinglocalrewritingrulestotheexistingsolutionuntilconvergence.Ourrewritingformulationisespeciallysuitableforproblemswiththefollowingproperties:(1)afeasiblesolution2iseasytoﬁnd;(2)thesearchspacehaswell-behavedlocalstructures whichcouldbeutilizedtoincrementallyimprovethesolution.Forsuchproblems acompletesolutionprovidesafullcontextfortheimprovementusingarewriting-basedapproach allowingadditionalfeaturestobecomputed whichishardtoobtainifthesolutionisgeneratedfromscratch;meanwhile differentsolutionsmightshareacommonroutinetowardstheoptimum whichcouldberepresentedaslocalrewritingrules.Forexample itismucheasiertodecidewhethertopostponejobswithlargeresourcerequirementswhenanexistingjobscheduleisprovided.Furthermore simpleruleslikeswappingtwojobscouldimprovetheperformance.Formally eachsolutionisastate andeachlocalregionandtheassociatedrewritingruleisanaction.Optimizationasarewritingproblem.LetUbetherewritingruleset.Supposestisthecurrentsolution(orstate)atiterationt.Weﬁrstcomputeastate-dependentregionsetΩ(st) thenpickaregionωt∈Ω(st)usingtheregion-pickingpolicyπω(ωt|st).Wethenpickarewritingruleutapplicabletothatregionωtusingtherule-pickingpolicyπu(ut|st[ωt]) wherest[ωt]isasubsetofstatest.Wethenapplythisrewritingruleut∈Utost[ωt] andobtainthenextstatest+1=f(st ωt ut).Givenaninitialsolution(orstate)s0 ourgoalistoﬁndasequenceofrewritingsteps(s0 (ω0 u0)) (s1 (ω1 u1)) ... (sT−1 (ωT−1 uT−1)) sTsothattheﬁnalcostc(sT)isminimized.Totacklearewritingproblem rule-basedrewriterswithmanually-designedrewritingroutineshavebeenproposed[23].However manuallydesigningsuchroutinesisnotatrivialtask.Anincompletesetofroutinesoftenleadstoaninefﬁcientexhaustivesearch whileasetofkaleidoscopicroutinesisoftencumbersometodesign hardtomaintainandlacksﬂexibility.Inthispaper weproposetotrainaneuralnetworkinstead usingreinforcementlearning.Recentadvanceindeepreinforcementlearningsuggeststhepotentialofwell-trainedmodelstodiscovernoveleffectivepolicies suchasdemonstratedinComputerGo[43]andvideogames[36].Moreover byleveragingreinforcementlearning ourapproachcouldbeextendedtoabroaderrangeofproblemsthatcouldbehardforrule-basedrewritersandclassicsearchalgorithms.Forexample wecandesigntherewardtotakethevalidityofthesolutionintoaccount sothatwecanstartwithaninfeasiblesolutionandthenmovetowardsafeasibleone.Ontheotherhand wecanalsotraintheneuralnetworktoexploretheconnectionsbetweendifferentsolutionsinthesearchspace.Inourevaluation wedemonstratethatourapproach(1)mitigateslaborioushumanefforts (2)discoversnovelrewritingpathsfromitsownexploration and(3)ﬁndsbettersolutiontooptimizationproblemthanthecurrentstate-of-the-artandtraditionalheuristic-basedsoftwarepackagestunedfordecades.4NeuralRewriterModelInthefollowing wepresentthedesignofourrewritingmodel i.e. NeuRewriter.Weﬁrstprovideanoverviewofourmodelframework thenpresentthedesigndetailsfordifferentapplications.4.1ModelOverviewFigure1illustratestheoverallframeworkofourneuralrewriter andwedescribethetwokeycomponentsforrewritingasfollows.MoredetailscanbefoundinAppendixC.Scorepredictor.Giventhestatest thescorepredictorcomputesascoreQ(st ωt)foreveryωt∈Ω(st) whichmeasuresthebeneﬁtofrewritingst[ωt].Ahighscoreindicatesthatrewritingst[ωt]couldbedesirable.NotethatΩ(st)isaproblem-dependentregionset.Forexpressionsimpliﬁcation Ω(st)includesallsub-treesoftheexpressionparsetrees;forjobscheduling Ω(st)coversalljobnodesforscheduling;andforvehiclerouting itincludesallnodesintheroute.Ruleselector.Givenst[ωt]toberewritten therule-pickingpolicypredictsaprobabilitydistributionπu(st[ωt])overtheentirerulesetU andselectsaruleut∈Utoapplyaccordingly.4.2TrainingDetailsLet(s0 (ω0 u0)) ... (sT−1 (ωT−1 uT−1)) sTbetherewritingsequenceintheforwardpass.Rewardfunction.Wedeﬁner(st (ωt ut))asr(st (ωt ut))=c(st)−c(st+1) wherec(·)isthetask-speciﬁccostfunctioninSection3.Q-Actor-Critictraining.Wetraintheregion-pickingpolicyπωandrule-pickingpolicyπusimulta-neously.Forπω(ωt|st;θ) weparameterizeitasasoftmaxoftheunderlyingQ(st ωt;θ)function:πω(ωt|st;θ)=exp(Q(st ωt;θ))Pωtexp(Q(st ωt;θ))(1)3min-v0v2v1v1!"#"∗=argmax+#⋅ !"ConstantReduction0./≤(a)(b)(c)015342#"∗!"./43swap!"3210Route:0→1→2→0→3→0#"∗2swap./0Figure2:TheinstantiationofNeuRewriterfordifferentdomains:(a)expressionsimpliﬁcation;(b)jobscheduling;and(c)vehiclerouting.In(a) stistheexpressionparsetree whereeachsquarerepresentsanodeinthetree.ThesetΩ(st)includeseverysub-treerootedatanon-terminalnode fromwhichtheregion-pickingpolicyselectsωt∼πω(ωt|st))torewrite.Afterwards therule-pickingpolicypredictsarewritingruleut∈U thenrewritesthesub-treeωttogetthenewtreest+1.In(b) stisthedependencygraphrepresentationofthejobschedule.Eachcirclewithindexgreaterthan0representsajobnode andnode0isanadditionalonerepresentingthemachine.Edgesinthegraphreﬂectjobdependencies.Theregion-pickingpolicyselectsajobωttore-schedulefromalljobnodes thentherule-pickingpolicychoosesamovingactionutforωt thenmodiﬁessttogetanewdependencygraphst+1.In(c) stisthecurrentroute andωtisthenodeselectedtochangethevisitorder.Node0isthedepot andothernodesarecustomerswithcertainresourcedemands.Theregion-pickingpolicyandtherule-pickingpolicyworksimilarlytothejobschedulingones.andinsteadlearnQ(st ωt;θ)byﬁttingittothecumulativerewardsampledfromthecurrentpoliciesπωandπu:Lω(θ)=1TT−1Xt=0(T−1Xt0=tγt0−tr(s0t (ω0t u0t))−Q(st ωt;θ))2(2)WhereTisthelengthoftheepisode(i.e. thenumberofrewritingsteps) andγisthedecayfactor.Forrule-pickingpolicyπu(ut|st[ωt];φ) weemploytheAdvantageActor-Criticalgorithm[45]withthelearnedQ(st ωt;θ)asthecritic andthusavoidboot-strappingwhichcouldcausesampleinsufﬁciencyandinstabilityintraining.Thisformulationissimilarinspirittosoft-Qlearning[22].Denoting∆(st (ωt ut))≡PT−1t0=tγt0−tr(s0t (ω0t u0t))−Q(st ωt;θ)astheadvantagefunction thelossfunctionoftheruleselectoris:Lu(φ)=−T−1Xt=0∆(st (ωt ut))logπu(ut|st[ωt];φ)(3)TheoveralllossfunctionisL(θ φ)=Lu(φ)+αLω(θ) whereαisahyper-parameter.MoretrainingdetailscanbefoundinAppendixD.5ApplicationsInthefollowingsections wediscusstheapplicationofourrewritingapproachtothreedifferentdomains:expressionsimpliﬁcation onlinejobscheduling andvehiclerouting.Inexpressionsimpliﬁcation weminimizetheexpressionlengthusingawell-deﬁnedsemantics-preservingrewritingruleset.Inonlinejobscheduling weaimtoreducetheoverallwaitingtimeofjobs.Invehiclerouting weaimtominimizethetotaltourlength.5.1ExpressionSimpliﬁcationWeﬁrstapplyourapproachtoexpressionsimpliﬁcationdomain.Inparticular weconsiderexpressionsinHalide adomain-speciﬁclanguageforhigh-performanceimageprocessing[39] whichiswidelyusedatscaleinmultipleproductsofGoogle(e.g. YouTube)andAdobePhotoshop.SimplifyingHalideexpressionsisanimportantsteptowardstheoptimizationoftheentirecode.Tothisend arule-basedrewriterisimplementedfortheexpressions whichiscarefullytunedwithmanually-designedheuristics.ThegrammaroftheexpressionsconsideredintherewriterisspeciﬁedinAppendixA.1.Noticethatthegrammarincludesamorecomprehensiveoperatorsetthanpreviousworksonﬁndingequivalentexpressions whichconsideronlybooleanexpressions[2 17]orasubsetofalgorithmicoperations[2].Therewriterincludeshundredsofmanually-designedrewritingtemplates.Givenanexpression therewriterchecksthetemplatesinapre-designedorder andappliesthoserewritingtemplatesthatmatchanysub-expressionoftheinput.Afterinvestigatingtherewritingtemplatesintherule-basedrewriter weﬁndthatalargenumberofrewritingtemplatesenumeratespeciﬁccasesforanuphillrule whichlengthenstheexpressionﬁrst4andshortensitlater(e.g. “min/max”expansion).Similartomomentumtermsingradientdescentforcontinuousoptimization suchrulesareusedtoescapealocaloptimum.However theyshouldonlybeappliedwhentheinitialexpressionsatisﬁescertainpre-conditions whichistraditionallyspeciﬁedbymanualdesign acumbersomeprocessthatishardtogeneralize.Observingtheselimitations wehypothesizethataneuralnetworkmodelhasthepotentialofdoingabetterjobthantherule-basedrewriter.Inparticular weproposetoonlykeepthecorerewritingrulesintheruleset removeallunnecessarypre-conditions andlettheneuralnetworkdecidewhichandwhentoapplyeachrewritingrule.Inthisway theneuralrewriterhasabetterﬂexibilitythantherule-basedrewriter becauseitcanlearnsuchrewritingdecisionsfromdata andhastheabilityofdiscoveringnovelrewritingpatternsthatarenotincludedintherule-basedrewriter.Ruleset.WeincorporatetwokindsoftemplatesfromHaliderewritingruleset.Theﬁrstkindissimplerules(e.g. v−v→0) whilethesecondoneistheuphillrulesafterremovingtheirmanuallydesignedpre-conditionsthatdonotaffectthevalidityoftherewriting.Inthisway arulesetwith|U|=19categoriesisbuilt.SeeAppendixB.1formoredetails.Modelspeciﬁcation.Weuseexpressionparsetreesastheinput andemploytheN-aryTree-LSTMdesignedin[46]astheinputencodertocomputetheembeddingforeachnodeinthetree.Boththescorepredictorandtheruleselectorarefullyconnectedneuralnetworks takentheLSTMembeddingsastheinput.MoredetailscanbefoundinAppendixC.1.5.2JobSchedulingProblemWealsostudythejobschedulingproblem usingtheproblemsetupin[33].Notation.SupposewehaveamachinewithDtypesofresources.Eachjobjisspeciﬁedasvj=(ρj Aj Tj) wheretheD-dimensionalvectorρj=[ρjd]denotestherequiredportion0≤ρjd≤1oftheresourcetyped Ajisthearrivaltimestep andTjistheduration.Inaddition wedeﬁneBjasthescheduledbeginningtime andCj=Bj+Tjasthecompletiontime.Weassumethattheresourcerequirementisﬁxedduringtheentirejobexecution eachjobmustruncontinuouslyuntilﬁnishing andnopreemptionisallowed.Weadoptanonlinesetting:thereisapendingjobqueuethatcanholdatmostWjobs.Whenanewjobarrives itcaneitherbeallocatedimmediately orbeaddedtothequeue.Ifthequeueisalreadyfull tomakespaceforthenewjob atleastonejobinthequeueneedstobescheduledimmediately.Thegoalistoﬁndatimescheduleforeveryjob sothattheaveragewaitingtimeisasshortaspossible.Ruleset.Thesetofrewritingrulesistore-scheduleajobvjandallocateitafteranotherjobvj0ﬁnishesoratitsarrivaltimeAj.SeeAppendixB.2fordetailsofarewritingstep.Thesizeoftherewritingrulesetis|U|=2W sinceeachjobcouldonlyswitchitsschedulingorderwithatmostWofitsformerandlatterjobsrespectively.Representation.Werepresenteachscheduleasadirectedacyclicgraph(DAG) whichdescribesthedependencyamongthescheduletimeofdifferentjobs.Speciﬁcally wedenoteeachjobvjasanodeinthegraph andweaddanadditionalnodev0torepresentthemachine.IfajobvjisscheduledatitsarrivaltimeAj(i.e. Bj=Aj) thenweaddadirectededgehv0 vjiinthegraph.Otherwise theremustexistatleastonejobvj0suchthatCj0=Bj(i.e. jobjstartsrightafterjobj0).Weaddanedgehvj0 vjiforeverysuchjobvj0tothegraph.Figure2(b)showsthesetting andwedefertheembeddingandgraphconstructiondetailstoAppendixC.2.Modelspeciﬁcation.Toencodethegraphs weextendtheChild-SumTree-LSTMarchitecturein[46] whichissimilartotheDAG-structuredLSTMin[53].Similartotheexpressionsimpliﬁcationmodel boththescorepredictorandtheruleselectorarefullyconnectedneuralnetworks andwedeferthemodeldetailstoAppendixC.2.5.3VehicleRoutingProblemInaddition weevaluateourapproachonvehicleroutingproblemsstudiedin[29 35].Speciﬁcally wefocusontheCapacitatedVRP(CVRP) whereasinglevehiclewithlimitedcapacityneedstosatisfytheresourcedemandsofasetofcustomernodes.Todoso weconstructmultipleroutesstartingandendingatthedepot i.e. node0inFigure2(c) sothattheresourcesdeliveredineachroutedonotexceedthevehiclecapacity whilethetotalroutelengthisminimized.5Werepresenteachvehicleroutingproblemasasequenceofthenodesvisitedinthetour anduseabi-directionalLSTMtoembedtheroutes.Therulesetissimilartothejobscheduling whereeachnodecanswapwithanothernodeintheroute.Thearchitecturesofthescorepredictorandruleselectoraresimilartojobscheduling.MoredetailscanbefoundinAppendixC.3.6ExperimentsWepresenttheevaluationresultsinthissection.Tocalculatetheinferencetime werunallalgorithmsonthesameserverequippedwith2QuadroGP100GPUsand80CPUcores.Only1GPUisusedwhenevaluatingneuralnetworks and4CPUcoresareusedforsearchalgorithms.Wesetthetimeoutofsearchalgorithmstobe10secondsperinstance.AllneuralnetworksinourevaluationareimplementedinPyTorch[38].6.1ExpressionSimpliﬁcationSetup.Toconstructthedataset weﬁrstgeneraterandompipelinesusingthegeneratorinHalide thenextractexpressionsfromthem.Weﬁlteroutthoseirreducibleexpressions thensplittherestinto8/1/1fortraining/validation/testsetsrespectively.SeeAppendixA.1formoredetails.Metrics.Weevaluatethefollowingtwometrics:(1)Averageexpressionlengthreduction whichisthelengthreducedfromtheinitialexpressiontotherewrittenone andthelengthisdeﬁnedasthenumberofcharactersintheexpression;(2)Averagetreesizereduction whichisthenumberofnodesdecreasedfromtheinitialexpressionparsetreetotherewrittenone.Baselines.WeexaminetheeffectivenessofNeuRewriteragainsttwokindsofbaselines.Theﬁrstkindofbaselinesareheuristic-basedrewritingapproaches includingHalide-rule(therule-basedHaliderewriterinSection3)andHeuristic-search whichappliesbeamsearchtoﬁndtheshortestrewritingwithourrulesetateachstep.NotethatNeuRewriterdoesnotusebeamsearch.Inaddition wealsocompareourapproachwithZ3 ahigh-performancetheoremproverdevelopedbyMicrosoftResearch[15].Z3providestwotacticstosimplifytheexpressions:Z3-simplifyperformssomelocaltransformationusingitspre-deﬁnedrules andZ3-ctx-solver-simplifytraverseseachsub-formulaintheinputexpressionandinvokesthesolvertoﬁndasimplerequivalentonetoreplaceit.Thissearch-basedtacticisabletoperformsimpliﬁcationnotincludedintheHalideruleset andisgenerallybetterthantherule-basedcounterpartbutwithmorecomputation.ForZ3-ctx-solver-simplify wesetthetimeouttobe10secondsforeachinputexpression.Results.Figure3apresentsthemainresults.WecannoticethattheperformanceofZ3-simplifyisworsethanHalide-rule becausetherulesetincludedinthissimpliﬁerismorerestrictedthantheHalideone andinparticular itcannothandleexpressionswith“max/min/select”operators.Ontheotherhand NeuRewriteroutperformsboththerule-basedrewritersandtheheuristicsearchbyalargemargin.Inparticular NeuRewritercouldreducetheexpressionlengthandparsetreesizebyaround52%and59%onaverage;comparedtotherule-basedrewriters ourmodelfurtherreducestheaverageexpressionlengthandtreesizebyaround20%and15%respectively.Weobservethatthemainperformancegaincomesfromlearningtoapplyuphillrulesappropriatelyinwaysthatarenotincludedinthemanually-designedtemplates.Forexample considertheexpression5≤max(max(v0 3)+3 max(v1 v2)) whichcouldbereducedtoTruebyexpandingmax(max(v0 3)+3 max(v1 v2))andmax(v0 3).Usingarule-basedrewriterwouldrequiretheneedofspecifyingthepre-conditionsrecursively whichbecomesprohibitivewhentheexpressionsbecomemorecomplex.Ontheotherhand heuristicsearchmaynotbeabletoﬁndthecorrectorderofexpandingtherighthandsizeoftheexpressionwhenmore“min/max”areincluded whichwouldmakethesearchlessefﬁcient.Furthermore NeuRewriteralsooutperformsZ3-ctx-solver-simplifyintermsofboththeresultqualityandthetimeefﬁciency asshowninFigure3aandTable1a.NotethattheimplementationofZ3isinC++andhighlyoptimized whileNeuRewriterisimplementedinPython;meanwhile Z3-ctx-solver-simplifycouldperformrewritingstepsthatarenotincludedintheHalideruleset.MoreresultscanbefoundinAppendixG.Generalizationtolongerexpressions.Tomeasurethegeneralizabilityofourapproach weconstruct4subsetsofthetrainingset:Train≤20 Train≤30 Train≤50andTrain≤100 whichonlyincludeexpressionsoflengthatmost20 30 50and100inthefulltrainingset.WealsobuildTest>100 asubsetofthefulltestsetthatonlyincludesexpressionsoflengthlargerthan100.ThestatisticsofthesedatasetscanbefoundinAppendixA.1.6Average expression length reductionAverage tree size reduction010203040506070Average expression length reduction17.7436.1347.0850.8157.2805101520Average tree size reduction7.399.6813.7615.8216.71Z3-simplifyHalide-ruleHeuristic SearchZ3-ctx-solver-simplifyNeuRewriter(a)TestTest>100020406080100Average expression length reduction36.1345.2550.8169.7957.2879.0854.3572.9551.4969.9350.7465.0950.5564.44Halide-ruleZ3-ctx-solver-simplifyNeuRewriter (Train)NeuRewriter (Train100)NeuRewriter (Train50)NeuRewriter (Train30)NeuRewriter (Train20)(b)Figure3:Experimentalresultsoftheexpressionsimpliﬁcationproblem.In(b) wetrainNeuRewriteronexpressionsofdifferentlengths(describedinthebrackets).(a)(b) (c)(d)Figure4:Experimentalresultsofthejobschedulingproblemvaryingthefollowingaspects:(a)thenumberofresourcetypesD;(b)jobfrequency;(c)resourcedistribution;(d)joblength.ForNeuRewriter wedescribetrainingjobdistributionsinthebrackets.Workloadsin(a)arewithsteadyjobfrequency non-uniformresourcedistribution andnon-uniformjoblength.In(b) (c)and(d) D=20.In(b)and(c) weomitthecomparisonwithsomeapproachesbecausetheirresultsaresigniﬁcantlyworse;forexample theaverageslowdownofEJFis14.53onthedynamicjobfrequency and11.06ontheuniformresourcedistribution.MoreresultscanbefoundinAppendixE.VRP20  Cap30VRP50  Cap40VRP100  Cap500510152025Average tour length7.0812.9620.336.8112.2518.966.4311.3117.166.4011.1516.966.2510.6216.236.1610.5116.10Random SweepRandom CWOr-toolsNazari et al. (RL beam 10)AM (sampling)NeuRewriter(a)VRP20  Cap30VRP50  Cap40VRP100  Cap500510152025Average tour length6.1611.5118.866.3810.5117.336.6511.6316.10NeuRewriter (VRP20  Cap30)NeuRewriter (VRP50  Cap40)NeuRewriter (VRP100  Cap50)(b)Figure5:Experimentalresultsofthevehicleroutingproblemwithdifferentnumberofcustomernodesandvehiclecapacity;e.g. VRP100 Cap50meansthereare100customernodesandthevehiclecapacityis50.(a)NeuRewriteroutperformsmultiplebaselinesandpreviousworks[29 35].MoreresultscanbefoundinAppendixF.(b)WeevaluatethegeneralizationperformanceofNeuRewriteronproblemsfromdifferentdistributions andwedescribethetrainingproblemdistributionsinthebrackets.7WepresenttheresultsoftrainingourmodelondifferentdatasetsaboveinFigure3b.Eventrainedonshortexpressions NeuRewriterisstillcomparablewiththeZ3solver.Thankstolocalrewritingrules ourapproachcangeneralizewellevenwhenoperatingonverydifferentdatadistributions.6.2JobSchedulingProblemSetup.Werandomlygenerate100Kjobsequences anduse80K/10K/10Kfortraining validationandtesting.Typicallyeachjobsequenceincludes∼50jobs.WeuseanonlinesettingwherejobsarriveontheﬂywithapendingjobqueueoflengthW=10.Unlessstatedotherwise wegenerateinitialschedulesusingEarliestJobFirst(EJF) whichcanbeconstructedwithnegligibleoverhead.WhenthenumberofresourcetypesD=2 wefollowthesamesetupasin[33].ThemaximaljobdurationTmax=15 andthelatestjobarrivaltimeAmax=50.WithlargerD exceptchangingtheresourcerequirementofeachjobtoincludemoreresourcetypes otherconﬁgurationsstaythesame.Metric.FollowingDeepRM[33] weusetheaveragejobslowdownηj≡(Cj−Aj)/Tj≥1asourevaluationmetric.Notethatηj=1meansnoslowdown.Jobproperties.TotestthestabilityandgeneralizabilityofNeuRewriter wechangejobproperties(andtheirdistributions):(1)NumberofresourcetypesD:largerDleadstomorecomplicatedscheduling;(2)Averagejobarrivalrate:theprobabilitythatanewjobwillarrive Steadyjobfrequencysetsittobe70% andDynamicjobfrequencymeansthejobarrivalratechangesrandomlyateachtimestep;(3)Resourcedistribution:jobsmightrequiredifferentresources wheresomeareuniform(e.g. half-halfforresource1and2)whileothersarenon-uniform(seeAppendixA.2forthedetaileddescription);(4)Joblengths:Uniformjoblength:lengthofeachjobintheworkloadiseither[10 15](long)or[1 3](short) andNon-uniformjoblength:workloadhasbothshortandlongjobs.WeshowthatNeuRewriterisfairlyrobustunderdifferentdistributions.Whentrainedononedistribution itcangeneralizetootherswithoutperformancecollapse.WecompareNeuRewriterwiththreekindsofbaselines.BaselinesonManuallydesignedheuristics:EarliestJobFirst(EJF)scheduleseachjobintheincreasingorderoftheirarrivaltime.ShortestJobFirst(SJF)alwaysallocatestheshortestjobinthependingjobqueueateachtimestep whichisalsousedasabaselinein[33].ShortestFirstSearch(SJFS)searchesovertheshortestkjobstoscheduleateachtimestep andreturnstheoptimalone.Weﬁndthatotherheuristic-basedbaselinesusedin[33]generallyperformworsethanSJF especiallywithlargeD.Thus weomitthecomparison.BaselinesonNeuralnetwork.WecomparewithDeepRM[33] aneuralnetworkalsotrainedwithRLtoconstructasolutionfromscratch.BaselinesonOfﬂineplanning.Tomeasuretheoptimalityofthesealgorithms wealsotakeanofﬂinesetting wheretheentirejobsequenceisavailablebeforescheduling.Notethatthisisequivalenttoassuminganunboundedlengthofthependingjobqueue.Withsuchadditionalknowledge thissettingprovidesastrongbaseline.Wetriedtwoofﬂinealgorithms:(1)SJF-offline whichisasimpleheuristicthatscheduleseachjobintheincreasingorderofitsduration;and(2)GoogleOR-tools[19] whichisagenerictoolboxforcombinatorialoptimization.ForOR-tools wesetthetimeouttobe10secondsperworkload butweﬁndthatitcannotachieveagoodperformanceevenwithalargertimeout andwedeferthediscussiontoAppendixE.ResultsonScalability.AsshowninFigure4a NeuRewriteroutperformsbothheuristicalgo-rithmsandthebaselineneuralnetworkDeepRM.Inparticular whiletheperformanceofDeepRMandNeuRewriteraresimilarwhenD=2 withlargerD DeepRMstartstoperformworsethanheuristic-basedalgorithms whichisconsistentwithourhypothesisthatitbecomeschallengingtodesignaschedulefromscratchwhentheenvironmentbecomesmorecomplex.Ontheotherhand NeuRewritercouldcapturethebottleneckofanexistingschedulethatlimitsitsefﬁciency thenprogressivelyreﬁneittoobtainabetterone.Inparticular ourresultsareevenbetterthanofﬂinealgorithmsthatassumetheknowledgeoftheentirejobsequence whichfurtherdemonstratestheeffectivenessofNeuRewriter.Meanwhile wepresenttherunningtimeofOR-tools DeepRMandNeuRewriterinTable1b.WecanobservethatbothDeepRMandNeuRewriteraremuchmoretime-efﬁcientthanOR-tools;ontheotherhand therunningtimeofNeuRewriteriscomparabletoDeepRM whileachievingmuchbetterresults.MorediscussioncanbefoundinAppendixE.ResultsonRobustness.AsshowninFigure4 NeuRewriterexcelsinalmostalldifferentjobdistributions exceptwhenthejoblengthsareuniform(shortorlong Figure4d) inwhichcase8Time(s)Z3-solver1.375NeuRewriter0.159(a)Time(s)OR-tools10.0DeepRM0.020NeuRewriter0.037(b)VRP20VRP50VRP100OR-tools0.0100.0530.231Nazarietal.0.1620.2320.445AM0.0360.1680.720NeuRewriter0.1330.2110.398(c)Table1:Averageruntime(perinstance)ofdifferentsolvers(OR-tools[19]andthetacticZ3-ctx-solver-simplifyofZ3[15])andRL-basedapproaches(NeuRewriter DeepRM[33] Nazarietal.[35]andAM[29])overthetestsetof:(a)expressionsimpliﬁcation;(b)jobscheduling;(c)vehiclerouting.existingmethods/heuristicsaresufﬁcient.ThisshowsthatNeuRewritercandealwithcomplicatedscenariosandisadaptivetodifferentdistributions.ResultsonGeneralization.Furthermore NeuRewritercanalsogeneralizetodifferentdistribu-tionsthanthoseusedintraining withoutsubstantialperformancedrop.Thisshowsthepoweroflocalrewritingrules:usinglocalcontextcouldyieldmoregeneralizablesolutions.6.3VehicleRoutingProblemSetupandBaselines.Wefollowthesametrainingsetupas[29 35]byrandomlygeneratingvehicleroutingproblemswithdifferentnumberofcustomernodesandvehiclecapacity.Wecomparewithtwoneuralnetworkapproaches i.e. AM[29]andNazarietal.[35] andbothofthemtrainaneuralnetworkpolicyusingreinforcementlearningtoconstructtheroutefromscratch.WealsocomparewithOR-toolsandseveralclassicheuristicsstudiedin[35].Results.WeﬁrstdemonstrateourmainresultsinFigure5a whereweincludethevariantofeachbaselinethatperformsthebest anddefermoreresultstoAppendixF.NotethattheinitialroutesgeneratedforNeuRewriterareevenworsethantheclassicheuristics;however startingfromsuchsub-optimalsolutions NeuRewriterisstillabletoiterativelyimprovethesolutionsandoutperformsallthebaselineapproachesondifferentproblemdistributions.Inaddition forVRP20problems wecancomputetheexactoptimalsolutions whichprovidesanaveragetourlengthof6.10.WeobservethattheresultofNeuRewriter(i.e. 6.16)istheclosesttothislowerbound whichalsodemonstratesthatNeuRewriterisabletoﬁndsolutionswithbetterquality.WealsocomparetheruntimeofthemostcompetitiveapproachesinTable1c.NotethattheOR-ToolssolverforvehicleroutingproblemsishighlytunedandimplementedinC++ whiletheRL-basedapproachesincomparisonareimplementedinPython.Meanwhile following[35] toreporttheruntimeofRLmodels wedecodeasingleinstanceatatime thusthereispotentialroomforspeedimprovementbydecodingmultipleinstancesperbatch.Nevertheless wecanstillobservethatNeuRewriterachievesabetterbalancebetweentheresultqualityandthetimeefﬁciency especiallywithalargerproblemscale.ResultsonGeneralization.Furthermore inFigure5b weshowthatNeuRewritercangeneralizetodifferentproblemdistributionsthantrainingones.Inparticular theystillexceedtheperformanceoftheclassicheuristics andaresometimescomparableorevenbetterthantheOR-tools.MorediscussioncanbefoundinAppendixF.7ConclusionInthiswork weproposetoformulateoptimizationasarewritingproblem andsolvetheproblembyiterativelyrewritinganexistingsolutiontowardstheoptimum.Weutilizedeepreinforcementlearningtotrainourneuralrewriter.Inourevaluation wedemonstratetheeffectivenessofourneuralrewriteronmultipledomains whereourmodeloutperformsbothheuristic-basedalgorithmsandbaselinedeepneuralnetworksthatgenerateanentiresolutiondirectly.Meanwhile weobservethatsinceourapproachisbasedonlocalrewriting itcouldbecometime-consumingwhenlargechangesareneededineachiterationofrewriting.Inextremecaseswhereeachrewritingstepneedstochangetheglobalstructure startingfromscratchbecomespreferrable.Weconsiderimprovingtheefﬁciencyofourrewritingapproachandextendingittomorecomplicatedscenariosasfuturework.9References[1]M.AffenzellerandR.Mayrhofer.Genericheuristicsforcombinatorialoptimizationproblems.InProc.ofthe9thInternationalConferenceonOperationalResearch pages83–92 2002.[2]M.Allamanis P.Chanthirasegaran P.Kohli andC.Sutton.Learningcontinuoussemanticrepresentationsofsymbolicexpressions.InInternationalConferenceonMachineLearning pages80–88 2017.[3]M.Armbrust A.Fox R.Grifﬁth A.D.Joseph R.Katz A.Konwinski G.Lee D.Patterson A.Rabkin I.Stoica etal.Aviewofcloudcomputing.CommunicationsoftheACM 53(4):50–58 2010.[4]L.BachmairandH.Ganzinger.Rewrite-basedequationaltheoremprovingwithselectionandsimpliﬁcation.JournalofLogicandComputation 4(3):217–247 1994.[5]R.Bar-YehudaandS.Even.Alinear-timeapproximationalgorithmfortheweightedvertexcoverproblem.JournalofAlgorithms 2(2):198–203 1981.[6]A.BayandB.Sengupta.Approximatingmeta-heuristicswithhomotopicrecurrentneuralnetworks.arXivpreprintarXiv:1709.02194 2017.[7]I.Bello H.Pham Q.V.Le M.Norouzi andS.Bengio.Neuralcombinatorialoptimizationwithreinforcementlearning.arXivpreprintarXiv:1611.09940 2016.[8]J.Bła˙zewicz W.Domschke andE.Pesch.Thejobshopschedulingproblem:Conventionalandnewsolutiontechniques.Europeanjournalofoperationalresearch 93(1):1–33 1996.[9]S.J.Bradtke B.E.Ydstie andA.G.Barto.Adaptivelinearquadraticcontrolusingpolicyiteration.InProceedingsoftheAmericancontrolconference volume3 pages3475–3475.Citeseer 1994.[10]R.R.Bunel A.Desmaison P.K.Mudigonda P.Kohli andP.Torr.Adaptiveneuralcompilation.InAdvancesinNeuralInformationProcessingSystems pages1444–1452 2016.[11]C.-H.Cai Y.Xu D.Ke andK.Su.Learningofhuman-likealgebraicreasoningusingdeepfeedforwardneuralnetworks.BiologicallyInspiredCognitiveArchitectures 25:43–50 2018.[12]T.Chen L.Zheng E.Yan Z.Jiang T.Moreau L.Ceze C.Guestrin andA.Krishnamurthy.Learningtooptimizetensorprograms.NIPS 2018.[13]W.Chen Y.Xu andX.Wu.Deepreinforcementlearningformulti-resourcemulti-machinejobscheduling.arXivpreprintarXiv:1711.07440 2017.[14]T.A.CohnandM.Lapata.Sentencecompressionastreetransduction.JournalofArtiﬁcialIntelligenceResearch 34:637–674 2009.[15]L.DeMouraandN.Bjørner.Z3:Anefﬁcientsmtsolver.InInternationalconferenceonToolsandAlgorithmsfortheConstructionandAnalysisofSystems pages337–340.Springer 2008.[16]M.Deudon P.Cournut A.Lacoste Y.Adulyasak andL.-M.Rousseau.Learningheuristicsforthetspbypolicygradient.InInternationalConferenceontheIntegrationofConstraintProgramming ArtiﬁcialIntelligence andOperationsResearch pages170–181.Springer 2018.[17]R.Evans D.Saxton D.Amos P.Kohli andE.Grefenstette.Canneuralnetworksunderstandlogicalentailment?ICLR 2018.[18]D.FeblowitzandD.Kauchak.Sentencesimpliﬁcationastreetransduction.InProceedingsoftheSecondWorkshoponPredictingandImprovingTextReadabilityforTargetReaderPopulations pages1–10 2013.[19]Google.Googleor-tools.https://developers.google.com/optimization/ 2019.[20]R.Grandl G.Ananthanarayanan S.Kandula S.Rao andA.Akella.Multi-resourcepackingforclusterschedulers.ACMSIGCOMMComputerCommunicationReview 44(4):455–466 2015.10[21]A.Graves G.Wayne andI.Danihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401 2014.[22]T.Haarnoja H.Tang P.Abbeel andS.Levine.Reinforcementlearningwithdeepenergy-basedpolicies.InICML pages1352–1361.JMLR.org 2017.[23]Halide.Halidesimpliﬁer.https://github.com/halide/Halide 2018.[24]J.Hsiang H.Kirchner P.Lescanne andM.Rusinowitch.Thetermrewritingapproachtoautomatedtheoremproving.TheJournalofLogicProgramming 14(1-2):71–99 1992.[25]D.Huang P.Dhariwal D.Song andI.Sutskever.Gamepad:Alearningenvironmentfortheoremproving.arXivpreprintarXiv:1806.00608 2018.[26]M.Jaderberg W.M.Czarnecki S.Osindero O.Vinyals A.Graves D.Silver andK.Kavukcuoglu.Decoupledneuralinterfacesusingsyntheticgradients.InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70 pages1627–1635.JMLR.org 2017.[27]R.M.Karp.Reducibilityamongcombinatorialproblems.InComplexityofcomputercomputa-tions pages85–103.Springer 1972.[28]E.Khalil H.Dai Y.Zhang B.Dilkina andL.Song.Learningcombinatorialoptimizationalgorithmsovergraphs.InAdvancesinNeuralInformationProcessingSystems pages6348–6358 2017.[29]W.Kool H.vanHoof andM.Welling.Attention learntosolveroutingproblems!InInternationalConferenceonLearningRepresentations 2019.[30]G.Lederman M.N.Rabe andS.A.Seshia.Learningheuristicsforautomatedreasoningthroughdeepreinforcementlearning.arXivpreprintarXiv:1807.08058 2018.[31]S.LevineandP.Abbeel.Learningneuralnetworkpolicieswithguidedpolicysearchunderunknowndynamics.InAdvancesinNeuralInformationProcessingSystems pages1071–1079 2014.[32]S.LevineandV.Koltun.Guidedpolicysearch.InInternationalConferenceonMachineLearning pages1–9 2013.[33]H.Mao M.Alizadeh I.Menache andS.Kandula.Resourcemanagementwithdeepreinforce-mentlearning.InProceedingsofthe15thACMWorkshoponHotTopicsinNetworks pages50–56.ACM 2016.[34]D.Q.MAYNE.Differentialdynamicprogramming–auniﬁedapproachtotheoptimizationofdynamicsystems.InControlandDynamicSystems volume10 pages179–254.Elsevier 1973.[35]M.Nazari A.Oroojlooy L.Snyder andM.Takac.Reinforcementlearningforsolvingthevehicleroutingproblem.InAdvancesinNeuralInformationProcessingSystems pages9861–9871 2018.[36]OpenAI.Openaidota2bot.https://openai.com/the-international/ 2018.[37]G.H.PaetzoldandL.Specia.Textsimpliﬁcationastreetransduction.InProceedingsofthe9thBrazilianSymposiuminInformationandHumanLanguageTechnology 2013.[38]A.Paszke S.Gross S.Chintala G.Chanan E.Yang Z.DeVito Z.Lin A.Desmaison L.Antiga andA.Lerer.Automaticdifferentiationinpytorch.InNIPS-W 2017.[39]J.Ragan-Kelley C.Barnes A.Adams S.Paris F.Durand andS.Amarasinghe.Halide:alanguageandcompilerforoptimizingparallelism locality andrecomputationinimageprocessingpipelines.ACMSIGPLANNotices 48(6):519–530 2013.[40]C.R.Reeves.Modernheuristictechniquesforcombinatorialproblems.Advancedtopicsincomputerscience volume15.McGraw-Hill 1995.11[41]E.Schkufza R.Sharma andA.Aiken.Stochasticsuperoptimization.InACMSIGARCHComputerArchitectureNews volume41 pages305–316.ACM 2013.[42]Z.Scully G.Blelloch M.Harchol-Balter andA.Scheller-Wolf.Optimallyschedulingjobswithmultipletasks.ACMSIGMETRICSPerformanceEvaluationReview 45(2):36–38 2017.[43]D.Silver J.Schrittwieser K.Simonyan I.Antonoglou A.Huang A.Guez T.Hubert L.Baker M.Lai A.Bolton etal.Masteringthegameofgowithouthumanknowledge.Nature 550(7676):354 2017.[44]N.SorenssonandN.Een.Minisatv1.13-asatsolverwithconﬂict-clauseminimization.SAT 2005(53):1–2 2005.[45]R.S.Sutton A.G.Barto etal.Reinforcementlearning:Anintroduction.1998.[46]K.S.Tai R.Socher andC.D.Manning.Improvedsemanticrepresentationsfromtree-structuredlongshort-termmemorynetworks.InProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics 2015.[47]Y.Tassa T.Erez andE.Todorov.Synthesisandstabilizationofcomplexbehaviorsthroughonlinetrajectoryoptimization.InIntelligentRobotsandSystems(IROS) 2012IEEE/RSJInternationalConferenceon pages4906–4913.IEEE 2012.[48]D.Terekhov D.G.Down andJ.C.Beck.Queueing-theoreticapproachesfordynamicscheduling:asurvey.SurveysinOperationsResearchandManagementScience 19(2):105–129 2014.[49]Y.TianandS.G.Narasimhan.Hierarchicaldata-drivendescentforefﬁcientoptimaldeformationestimation.InProceedingsoftheIEEEInternationalConferenceonComputerVision pages2288–2295 2013.[50]O.Vinyals M.Fortunato andN.Jaitly.Pointernetworks.InAdvancesinNeuralInformationProcessingSystems pages2692–2700 2015.[51]D.Vrabie O.Pastravanu M.Abu-Khalaf andF.L.Lewis.Adaptiveoptimalcontrolforcontinuous-timelinearsystemsbasedonpolicyiteration.Automatica 45(2):477–484 2009.[52]W.Zaremba K.Kurach andR.Fergus.Learningtodiscoverefﬁcientmathematicalidentities.InAdvancesinNeuralInformationProcessingSystems pages1278–1286 2014.[53]X.Zhu P.Sobhani andH.Guo.Dag-structuredlongshort-termmemoryforsemanticcom-positionality.InProceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies pages917–926 2016.12,Xinyun Chen
Yuandong Tian