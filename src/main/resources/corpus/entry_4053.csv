2013,Online Learning with Switching Costs and Other Adaptive Adversaries,We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice  under both full-information and bandit feedback. We measure the player's performance using a new notion of regret  also known as policy regret  which better captures the adversary's adaptiveness to the player's behavior. In a setting where losses are allowed to drift  we characterize ---in a nearly complete manner--- the power of adaptive adversaries with bounded memories and switching costs. In particular  we show that with switching costs  the attainable rate with bandit feedback is $T^{2/3}$. Interestingly  this rate is significantly worse than the $\sqrt{T}$ rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits  we also show that a bounded memory adversary can force $T^{2/3}$ regret even in the full information case  proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.,Online Learning with Switching Costs and Other

Adaptive Adversaries

Nicol`o Cesa-Bianchi

Universit`a degli Studi di Milano

Italy

Ofer Dekel

Microsoft Research

USA

Ohad Shamir

Microsoft Research

and the Weizmann Institute

Abstract

We study the power of different types of adaptive (nonoblivious) adversaries in
the setting of prediction with expert advice  under both full-information and ban-
dit feedback. We measure the player’s performance using a new notion of regret 
also known as policy regret  which better captures the adversary’s adaptiveness
to the player’s behavior. In a setting where losses are allowed to drift  we char-
acterize —in a nearly complete manner— the power of adaptive adversaries with
bounded memories and switching costs. In particular  we show that with switch-
ing costs  the attainable rate with bandit feedback is �Θ(T 2/3). Interestingly  this
rate is signiﬁcantly worse than the Θ(√T ) rate attainable with switching costs in
the full-information case. Via a novel reduction from experts to bandits  we also
show that a bounded memory adversary can force�Θ(T 2/3) regret even in the full

information case  proving that switching costs are easier to control than bounded
memory adversaries. Our lower bounds rely on a new stochastic adversary strat-
egy that generates loss processes with strong dependencies.

1

Introduction

An important instance of the framework of prediction with expert advice —see  e.g.  [8]— is deﬁned
as the following repeated game  between a randomized player with a ﬁnite and ﬁxed set of available
actions and an adversary. At the beginning of each round of the game  the adversary assigns a loss to
each action. Next  the player deﬁnes a probability distribution over the actions  draws an action from
this distribution  and suffers the loss associated with that action. The player’s goal is to accumulate
loss at the smallest possible rate  as the game progresses. Two versions of this game are typically
considered: in the full-information feedback version  at the end of each round  the player observes
the adversary’s assignment of loss values to each action. In the bandit feedback version  the player
only observes the loss associated with his chosen action  but not the loss values of other actions.
We assume that the adversary is adaptive (also called nonoblivious by [8] or reactive by [16])  which
means that the adversary chooses the loss values on round t based on the player’s actions on rounds
1 . . . t − 1. We also assume that the adversary is deterministic and has unlimited computational
power. These assumptions imply that the adversary can specify his entire strategy before the game
begins.
In other words  the adversary can perform all of the calculations needed to specify  in
advance  how he plans to react on each round to any sequence of actions chosen by the player.
More formally  let A denote the ﬁnite set of actions and let Xt denote the player’s random action on
round t. We adopt the notation X1:t as shorthand for the sequence X1 . . . Xt. We assume that the
adversary deﬁnes  in advance  a sequence of history-dependent loss functions f1  f2  . . .. The input
to each loss function ft is the entire history of the player’s actions so far  therefore the player’s loss
on round t is ft(X1:t). Note that the player doesn’t observe the functions ft  only the losses that
result from his past actions. Speciﬁcally  in the bandit feedback model  the player observes ft(X1:t)
on round t  whereas in the full-information model  the player observes ft(X1:t−1  x) for all x ∈ A.

1

On any round T   we evaluate the player’s performance so far using the notion of regret  which
compares his cumulative loss on the ﬁrst T rounds to the cumulative loss of the best ﬁxed action in
hindsight. Formally  the player’s regret on round T is deﬁned as

RT =

ft(X1:t) − min
x∈A

T�t=1

T�t=1

ft(x . . . x) .

(1)

RT is a random variable  as it depends on the randomized action sequence X1:t. Therefore  we also
consider the expected regret E[RT ]. This deﬁnition is the same as the one used in [18] and [3] (in
the latter  it is called policy regret)  but differs from the more common deﬁnition of expected regret

E� T�t=1

ft(X1:t) − min
x∈A

ft(X1:t−1  x)� .

T�t=1

(2)

The deﬁnition in Eq. (2) is more common in the literature (e.g.  [4  17  10  16])  but is clearly inade-
quate for measuring a player’s performance against an adaptive adversary. Indeed  if the adversary is
adaptive  the quantity ft(X1:t−1  x)is hardly interpretable —see [3] for a more detailed discussion.
In general  we seek algorithms for which E[RT ] can be bounded by a sublinear function of T  
implying that the per-round expected regret  E[RT ]/T   tends to zero. Unfortunately  [3] shows that
arbitrary adaptive adversaries can easily force the regret to grow linearly. Thus  we need to focus on
(reasonably) weaker adversaries  which have constraints on the loss functions they can generate.
The weakest adversary we discuss is the oblivious adversary  which determines the loss on round t
based only on the current action Xt. In other words  this adversary is oblivious to the player’s past
actions. Formally  the oblivious adversary is constrained to choose a sequence of loss functions that
satisﬁes ∀t  ∀x1:t ∈ At  and ∀x�1:t−1 ∈ At−1 

ft(x1:t) = ft(x�1:t−1  xt) .

(3)
The majority of previous work in online learning focuses on oblivious adversaries. When dealing
with oblivious adversaries  we denote the loss function by �t and omit the ﬁrst t− 1 arguments. With
this notation  the loss at time t is simply written as �t(Xt).
For example  imagine an investor that invests in a single stock at a time. On each trading day he
invests in one stock and suffers losses accordingly. In this example  the investor is the player and the
stock market is the adversary. If the investment amount is small  the investor’s actions will have no
measurable effect on the market  so the market is oblivious to the investor’s actions. Also note that
this example relates to the full-information feedback version of the game  as the investor can see the
performance of each stock at the end of each trading day.
A stronger adversary is the oblivious adversary with switching costs. This adversary is similar to the
oblivious adversary deﬁned above  but charges the player an additional switching cost of 1 whenever
Xt �= Xt−1. More formally  this adversary deﬁnes his sequence of loss functions in two steps: ﬁrst
he chooses an oblivious sequence of loss functions  �1  �2 . . .  which satisﬁes the constraint in Eq. (3).
Then  he sets f1(x) = �1(x)  and

∀ t ≥ 2  ft(x1:t) = �t(xt) + I{xt�=xt−1} .

(4)
This is a very natural setting. For example  let us consider again the single-stock investor  but now
assume that each trade has a ﬁxed commission cost. If the investor keeps his position in a stock for
multiple trading days  he is exempt from any additional fees  but when he sells one stock and buys
another  he incurs a ﬁxed commission. More generally  this setting (or simple generalizations of it)
allows us to capture any situation where choosing a different action involves a costly change of state.
In the paper  we will also discuss a special case of this adversary  where the loss function �t(x) for
each action is sampled i.i.d. from a ﬁxed distribution.
The switching costs adversary deﬁnes ft to be a function of Xt and Xt−1  and is therefore a special
case of a more general adversary called an adaptive adversary with a memory of 1. This adversary
is constrained to choose loss functions that satisfy ∀t  ∀x1:t ∈ At  and ∀x�1:t−2 ∈ At−2 

(5)
This adversary is more general than the switching costs adversary because his loss functions can
depend on the previous action in an arbitrary way. We can further strengthen this adversary and

ft(x1:t) = ft(x�1:t−2  xt−1  xt) .

2

deﬁne the bounded memory adaptive adversary  which has a bounded memory of an arbitrary size.
In other words  this adversary is allowed to set his loss function based on the player’s m most recent
past actions  where m is a predeﬁned parameter. Formally  the bounded memory adversary must
choose loss functions that satisfy  ∀t  ∀x1:t ∈ At  and ∀x�1:t−m−1 ∈ At−m−1 

ft(x1:t) = ft(x�1:t−m−1  xt−m:t) .

In the information theory literature  this setting is called individual sequence prediction against loss
functions with memory [18].
In addition to the adversary types described above  the bounded memory adaptive adversary has
additional interesting special cases. One of them is the delayed feedback oblivious adversary of
[19]  which deﬁnes an oblivious loss sequence  but reveals each loss value with a delay of m rounds.
Since the loss at time t depends on the player’s action at time t − m  this adversary is a special case
of a bounded memory adversary with a memory of size m. The delayed feedback adversary is not a
focus of our work  and we present it merely as an interesting special case.
So far  we have deﬁned a succession of adversaries of different strengths. This paper’s goal is
to understand the upper and lower bounds on the player’s regret when he faces these adversaries.
Speciﬁcally  we focus on how the expected regret depends on the number of rounds  T   with either
full-information or bandit feedback.

1.1 The Current State of the Art

Different aspects of this problem have been previously studied and the known results are surveyed
below and summarized in Table 1. Most of these previous results rely on the additional assumption
that the range of the loss functions is bounded in a ﬁxed interval  say [0  C]. We explicitly make note
of this because our new results require us to slightly generalize this assumption.
As mentioned above  the oblivious adversary has been studied extensively and is the best under-
stood of all the adversaries discussed in this paper. With full-information feedback  both the Hedge
algorithm [15  11] and the follow the perturbed leader (FPL) algorithm [14] guarantee a regret of

O(√T )  with a matching lower bound of Ω(√T ) —see  e.g.  [8]. Analyses of Hedge in settings

where the loss range may vary over time have also been considered —see  e.g.  [9]. The oblivious
setting with bandit feedback  where the player only observes the incurred loss ft(X1:t)  is called the
nonstochastic (or adversarial) multi-armed bandit problem. In this setting  the Exp3 algorithm of [4]
guarantees the same regret O(√T ) as the full-information setting  and clearly the full-information
lower bound Ω(√T ) still applies.
The follow the lazy leader (FLL) algorithm of [14] is designed for the switching costs setting with
full-information feedback. The analysis of FLL guarantees that the oblivious component of the
player’s expected regret (without counting the switching costs)  as well as the expected number of

switches  is upper bounded by O(√T )  implying an expected regret of O(√T ).
The work in [3] focuses on the bounded memory adversary with bandit feedback and guarantees
an expected regret of O(T 2/3). This bound naturally extends to the full-information setting. We
note that [18  12] study this problem in a different feedback model  which we call counterfactual
feedback  where the player receives a full description of the history-dependent function ft at the end
of round t. In this setting  the algorithm presented in [12] guarantees an expected regret of O(√T ).

Learning with bandit feedback and switching costs has mostly been considered in the economics
literature  using a different setting than ours and with prior knowledge assumptions (see [13] for
an overview). The setting of stochastic oblivious adversaries (i.e.  oblivious loss functions sampled
i.i.d. from a ﬁxed distribution) was ﬁrst studied by [2]  where they show that O(log T ) switches are
sufﬁcient to asymptotically guarantee logarithmic regret. The paper [20] achieves logarithmic regret
nonasymptotically with O(log T ) switches.
Several other papers discuss online learning against “adaptive” adversaries [4  10  16  17]  but these
results are not relevant to our work and can be easily misunderstood. For example  several bandit

algorithms have extensions to the “adaptive” adversary case  with a regret upper bound of O(√T )

[1]. This bound doesn’t contradict the Ω(T ) lower bound for general adaptive adversaries mentioned

3

oblivious

switching cost memory of size 1

bounded memory

adaptive

√T
√T

√T
√T

√T
√T

Full-Information Feedback

T 2/3
√T

Bandit Feedback

T 2/3

√T → T 2/3

T 2/3

√T → T 2/3

T 2/3

√T → T 2/3

T 2/3

√T → T 2/3

T
T

T
T

Ω

Ω

�O
�O

Table 1: State-of-the-art upper and lower bounds on regret (as a function of T ) against different
adversary types. Our contribution to this table is presented in bold face.

earlier  since these papers use the regret deﬁned in Eq. (2) rather than the regret used in our work 
deﬁned in Eq. (1).
Another related body of work lies in the ﬁeld of competitive analysis —see [5]  which also deals
with loss functions that depend on the player’s past actions  and the adversary’s memory may even
be unbounded. However  obtaining sublinear regret is generally impossible in this case. Therefore 
competitive analysis studies much weaker performance metrics such as the competitive ratio  making
it orthogonal to our work.

1.2 Our Contribution

In this paper  we make the following contributions (see Table 1):

• Our main technical contribution is a new lower bound on regret that matches the existing
upper bounds in several of the settings discussed above. Speciﬁcally  our lower bound
applies to the switching costs adversary with bandit feedback and to all strictly stronger
adversaries.

bounds up to logarithmic factors.

setting with full-information feedback  again matching the known upper bound.

• Building on this lower bound  we prove another regret lower bound in the bounded memory
• We conﬁrm that existing upper bounds on regret hold in our setting and match the lower
• Despite the lower bound  we show that for switching costs and bandit feedback  if we
also assume stochastic i.i.d. losses  then one can get a distribution-free regret bound of
O(√T log log log T ) for ﬁnite action sets  with only O(log log T ) switches. This result

uses ideas from [7]  and is deferred to the supplementary material.

Our new lower bound is a signiﬁcant step towards a complete understanding of adaptive adversaries;
observe that the upper and lower bounds in Table 1 essentially match in all but one of the settings.
Our results have two important consequences. First  observe that the optimal regret against the

costs and bandit feedback. This demonstrates that dependencies in the loss process must play a

feedback. To the best of our knowledge  this is the ﬁrst theoretical conﬁrmation that learning with
bandit feedback is strictly harder than learning with full-information  even on a small ﬁnite action set
and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the
number of actions [4]  or required large or continuous action spaces —see  e.g.  [6  21]). Moreover 

switching costs adversary is Θ�√T� with full-information feedback  versus Θ�T 2/3� with bandit
recall the regret bound of O�√T log log log T� against the stochastic i.i.d. adversary with switching
crucial role in controlling the power of the switching costs adversary. Indeed  the Ω�T 2/3� lower
bound proven in the next section heavily relies on such dependencies.
Second  observe that in the full-information feedback case  the optimal regret against a switching
costs adversary is Θ(√T )  whereas the optimal regret against the more general bounded memory
adversary is Ω(T 2/3). This is somewhat surprising given the ideas presented in [18] and later ex-
tended in [3]: The main technique used in these papers is to take an algorithm originally designed
for oblivious adversaries  forcefully prevent it from switching actions very often  and obtain a new
algorithm that guarantees a regret of O(T 2/3) against bounded memory adversaries. This would

4

seem to imply that a small number of switches is the key to dealing with general bounded memory
adversaries. Our result contradicts this intuition by showing that controlling the number of switches
is easier then dealing with a general bounded memory adversary.
As noted above  our lower bounds require us to slightly weaken the standard technical assumption
that loss values lie in a ﬁxed interval [0  C]. We replace it with the following two assumptions:

1. Bounded range. We assume that the loss values on each individual round are bounded
in an interval of constant size C  but we allow this interval to drift from round to round.
Formally  ∀t  ∀x1:t ∈ At and ∀x�1:t ∈ At 

2. Bounded drift. We also assume that the drift of each individual action from round to round

is contained in a bounded interval of size Dt  where Dt may grow slowly  as O��log(t)�.

Formally  ∀t and ∀x1:t ∈ At 

��ft(x1:t) − ft(x�1:t)�� ≤ C .
��ft(x1:t) − ft+1(x1:t  xt)�� ≤ Dt .

(6)

(7)

Since these assumptions are a relaxation of the standard assumption  all of the known lower bounds
on regret automatically extend to our relaxed setting. For our results to be consistent with the current
state of the art  we must also prove that all of the known upper bounds continue to hold after the
relaxation  up to logarithmic factors.

2 Lower Bounds

In this section  we prove lower bounds on the player’s expected regret in various settings.

2.1 Ω(T 2/3) with Switching Costs and Bandit Feedback

We begin with a Ω(T 2/3) regret lower bound against an oblivious adversary with switching costs 
when the player receives bandit feedback. It is enough to consider a very simple setting  with only
two actions  labeled 1 and 2. Using the notation introduced earlier  we use �1  �2  . . . to denote the
oblivious sequence of loss functions chosen by the adversary before adding the switching cost.
Theorem 1. For any player strategy that relies on bandit feedback and for any number of rounds T  
there exist loss functions f1  . . .   fT that are oblivious with switching costs  with a range bounded

by C = 2  and a drift bounded by Dt =�3 log(t) + 16  such that E[RT ] ≥ 1

The full proof is given in the supplementary material  and here we give an informal proof sketch.
We begin by constructing a randomized adversarial strategy  where the loss functions �1  . . .   �T are
an instantiation of random variables Lt  . . .   LT deﬁned as follows. Let ξ1  . . .   ξT be i.i.d. standard
Gaussian random variables (with zero mean and unit variance) and let Z be a random variable that
equals −1 or 1 with equal probability. Using these random variables  deﬁne for all t = 1 . . . T

40 T 2/3.

Lt(1) =

ξs  

Lt(2) = Lt(1) + ZT −1/3 .

(8)

t�s=1

t=1 is simply a Gaussian random walk and {Lt(2)}T

In words  {Lt(1)}T
t=1 is the same random walk 
slightly shifted up or down —see ﬁgure 1 for an illustration. It is straightforward to conﬁrm that this
loss sequence has a bounded range  as required by the theorem: by construction we have |�t(1) −
�t(2)| = T −1/3 ≤ 1 for all t  and since the switching cost can add at most 1 to the loss on each
round  we conclude that |ft(1) − ft(2)| ≤ 2 for all t. Next  we show that the expected regret
of any player against this random loss sequence is Ω(T 2/3)  where expectation is taken over the
randomization of both the adversary and the player. The intuition is that the player can only gain
information about which action is better by switching between them. Otherwise  if he stays on
the same action  he only observes a random walk  and gets no further information. Since the gap
between the two losses on each round is T −1/3  the player must perform Ω(T 2/3) switches before
he can identify the better action. If the player performs that many switches  the total regret incurred
due to the switching costs is Ω(T 2/3). Alternatively  if the player performs o(T 2/3) switches  he

5

2

0

−2

5

10

15
t

20

25

30

�t(1)
�t(2)

Figure 1: A particular realization of the random loss sequence deﬁned in Eq. (8). The sequence of
losses for action 1 follows a Gaussian random walk  whereas the sequence of losses for action 2
follows the same random walk  but slightly shifted either up or down.

can’t identify the better action; as a result he suffers an expected regret of Ω(T −1/3) on each round
and a total regret of Ω(T 2/3).
Since the randomized loss sequence deﬁned in Eq. (8)  plus a switching cost  achieves an expected
regret of Ω(T 2/3)  there must exist at least one deterministic loss sequence �1 . . . �T with a regret of
Ω(T 2/3). In our proof  we show that there exists such �1 . . . �T with bounded drift.

2.2 Ω(T 2/3) with Bounded Memory and Full-Information Feedback

We build on Thm. 1 and prove a Ω(T 2/3) regret lower bound in the full-information setting  where
we get to see the entire loss vector on every round. To get this strong result  we need to give the
adversary a little bit of extra power: memory of size 2 instead of size 1 as in the case of switching
costs. To show this result  we again consider a simple setting with two actions.
Theorem 2. For any player strategy that relies on full-information feedback and for any number of
rounds T ≥ 2  there exist loss functions f1  . . .   fT   each with a memory of size m = 2  a range
bounded by C = 2  and a drift bounded by Dt =�3 log(t) + 18  such that E[RT ] ≥ 1
40 (T − 1)2/3.
The formal proof is deferred to the supplementary material and a proof sketch is given here. The
proof is based on a reduction from full-information to bandit feedback that might be of independent
interest. We construct the adversarial loss sequence as follows: on each round  the adversary assigns
the same loss to both actions. Namely  the value of the loss depends only on the player’s previous two
actions  and not on his action on the current round. Recall that even in the full-information version of
the game  the player doesn’t know what the losses would have been had he chosen different actions
in the past. Therefore  we have made the full-information game as difﬁcult as the bandit game.
Speciﬁcally  we construct an oblivious loss sequence �1 . . . �T as in Thm. 1 and deﬁne

ft(x1:t) = �t−1(xt−1) + I{xt−1�=xt−2} .

(9)
In words  we deﬁne the loss on round t of the full-information game to be equal to the loss on round
t − 1 of a bandits-with-switching-costs game in which the player chooses the same sequence of
actions. This can be done with a memory of size 2  since the loss in Eq. (9) is fully speciﬁed by the
player’s choices on rounds t  t − 1  t − 2. Therefore  the Ω(T 2/3) lower bound for switching costs
and bandit feedback extends to the full-information setting with a memory of size at least 2.

3 Upper Bounds

In this section  we show that the known upper bounds on regret  originally proved for bounded
losses  can be extended to the case of losses with bounded range and bounded drift. Speciﬁcally  of
the upper bounds that appear in Table 1  we prove the following:

• O(√T ) for an oblivious adversary with switching costs  with full-information feedback.
• �O(√T ) for an oblivious adversary with bandit feedback (where �O hides logarithmic factors).
• �O(T 2/3) for a bounded memory adversary with bandit feedback.

6

The remaining upper bounds in Table 1 are either trivial or follow from the principle that an upper
bound still holds if we weaken the adversary or provide a more informative feedback.

3.1 O(√T ) with Switching Costs and Full-Information Feedback
In this setting  ft(x1:t) = �t(xt) + I{xt�=xt−1}. If the oblivious losses �1 . . . �T (without the addi-
tional switching costs) were all bounded in [0  1]  the Follow the Lazy Leader (FLL) algorithm of
[14] would guarantee a regret of O(√T ) with respect to these losses (again  without the additional
switching costs). Additionally  FLL guarantees that its expected number of switches is O(√T ).

We use a simple reduction to extend these guarantees to loss functions with a range bounded in an
interval of size C and with an arbitrary drift.
On round t  after choosing an action and receiving the loss function �t  the player deﬁnes the modi-
ﬁed loss ��t(x) = 1
then chooses the next action.
Theorem 3. If each of the loss functions f1  f2  . . . is oblivious with switching costs and has a range
bounded by C then the player strategy described above attains O(C√T ) expected regret.

C−1��t(x) − miny �t(y)� and feeds it to the FLL algorithm. The FLL algorithm

The full proof is given in the supplementary material but the proof technique is straightforward. We
ﬁrst show that each ��t is bounded in [0  1] and therefore the standard regret bound for FLL holds
with respect to the sequence of modiﬁed loss functions ��1  ��2  . . .. Then we show that the guarantees
provided for FLL imply a regret of O(√T ) with respect to the original loss sequence f1  f2  . . ..
3.2

�O(√T ) with an Oblivious Adversary and Bandit Feedback

In this setting  ft(x1:t) simply equals �t(xt). The reduction described in the previous subsection
cannot be used in the bandit setting  since minx �t(x) is unknown to the player  and a different
reduction is needed. The player sets a ﬁxed horizon T and focuses on controlling his regret at time
T ; he can then use a standard doubling trick [8] to handle an inﬁnite horizon. The player uses the
fact that each ft has a range bounded by C. Additionally  he deﬁnes D = maxt≤T Dt and on each
round he deﬁnes the modiﬁed loss

f�t(x1:t) =

1

2(C + D)��t(xt) − �t−1(xt−1)� +

1
2

.

(10)

expected regret.

Theorem 4. If each of the loss functions f1 . . . fT is oblivious with a range bounded by C and

a ﬁxed action. The Exp3 algorithm  due to [4]  is such an algorithm. The player chooses his actions
according to the choices made by Exp3. The following theorem states that this reduction results in

Note that f�t(X1:t) can be computed by the player using only bandit feedback. The player then feeds
f�t(X1:t) to an algorithm that guarantees a O(√T ) standard regret (see deﬁnition in Eq. (2)) against
a bandit algorithm that guarantees a regret of �O(√T ) against oblivious adversaries.
a drift bounded by Dt = O��log(t)� then the player strategy described above attains �O(C√T )
The full proof is given in the supplementary material. In a nutshell  we show that each f�t is a loss
function bounded in [0  1] and that the analysis of Exp3 guarantees a regret of O(√T ) with respect to
the loss sequence f�1 . . . f�T . Then  we show that this guarantee implies a regret of (C +D)O(√T ) =
�O(C√T ) with respect to the original loss sequence f1 . . . fT .
�O(T 2/3) with Bounded Memory and Bandit Feedback

Proving an upper bound against an adversary with a memory of size m  with bandit feedback 
requires a more delicate reduction. As in the previous section  we assume a ﬁnite horizon T and we
let D = maxt Dt. Let K = |A| be the number of actions available to the player.
Since fT (x1:t) depends only on the last m + 1 actions in x1:t  we slightly overload our notation
and deﬁne ft(xt−m:t) to mean the same as ft(x1:t). To deﬁne the reduction  the player ﬁxes a base

3.3

7

�ft(xt−m:t) =

1

2�C + (m + 1)D��ft(xt−m:t) − ft−m−1(x0 . . . x0)� +

1
2

.

action x0 ∈ A and for each t > m he deﬁnes the loss function

Next  he divides the T rounds into J consecutive epochs of equal length  where J = Θ(T 2/3). We
assume that the epoch length T /J is at least 2K(m + 1)  which is true when T is sufﬁciently large.
At the beginning of each epoch  the player plans his action sequence for the entire epoch. He uses
some of the rounds in the epoch for exploration and the rest for exploitation. For each action in A 
the player chooses an exploration interval of 2(m + 1) consecutive rounds within the epoch. These
K intervals are chosen randomly  but they are not allowed to overlap  giving a total of 2K(m + 1)
exploration rounds in the epoch. The details of how these intervals are drawn appears in our analysis 
in the supplementary material. The remaining T /J − 2K(m + 1) rounds are used for exploitation.
The player runs the Hedge algorithm [11] in the background  invoking it only at the beginning of
each epoch and using it to choose one exploitation action that will be played consistently on all of the
exploitation rounds in the epoch. In the exploration interval for action x  the player ﬁrst plays m + 1
rounds of the base action x0 followed by m + 1 rounds of the action x. Letting tx denote the ﬁrst
round in this interval  the player uses the observed losses ftx+m(x0 . . . x0) and ftx+2m+1(x . . . x)

as feedback to the Hedge algorithm.
We prove the following regret bound  with the proof deferred to the supplementary material.
Theorem 5. If each of the loss functions f1 . . . fT is has a memory of size m  a range bounded

to compute �ftx+2m+1(x . . . x). In our analysis  we show that the latter is an unbiased estimate of
the average value of �ft(x . . . x) over t in the epoch. At the end of the epoch  the K estimates are fed
by C  and a drift bounded by Dt = O��log(t)� then the player strategy described above attains
�O(T 2/3) expected regret.

4 Discussion

In this paper  we studied the problem of prediction with expert advice against different types of
adversaries  ranging from the oblivious adversary to the general adaptive adversary. We proved
upper and lower bounds on the player’s regret against each of these adversary types  in both the
full-information and the bandit feedback models. Our lower bounds essentially matched our up-
per bounds in all but one case: the adaptive adversary with a unit memory in the full-information

setting  where we only know that regret is Ω(√T ) and O(T 2/3). Our bounds have two important

consequences. First  we characterize the regret attainable with switching costs  and show a setting
where predicting with bandit feedback is strictly more difﬁcult than predicting with full-information
feedback —even in terms of the dependence on T   and even on small ﬁnite action sets. Second  in
the full-information setting  we show that predicting against a switching costs adversary is strictly
easier than predicting against an arbitrary adversary with a bounded memory. To obtain our re-
sults  we had to relax the standard assumption that loss values are bounded in [0  1]. Re-introducing
this assumption and proving similar lower bounds remains an elusive open problem. Many other
questions remain unanswered. Can we characterize the dependence of the regret on the number of
actions? Can we prove regret bounds that hold with high probability? Can our results be generalized
to more sophisticated notions of regret  as in [3]?
In addition to the adversaries discussed in this paper  there are other interesting classes of adversaries
that lie between the oblivious and the adaptive. A notable example is the family of deterministically
adaptive adversaries  which includes adversaries that adapt to the player’s actions in a known de-
terministic way  rather than in a secret malicious way. For example  imagine playing a multi-armed
bandit game where the loss values are initially oblivious  but whenever the player chooses an arm
with zero loss  the loss of the same arm on the next round is deterministically changed to zero. Many
real-world online prediction scenarios are deterministically adaptive  but we lack a characterization
of the expected regret in this setting.

Acknowledgments

Part of this work was done while NCB was visiting OD at Microsoft Research  whose support is
gratefully acknowledged.

8

References
[1] J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In COLT 

2009.

[2] R. Agrawal  M.V. Hedge  and D. Teneketzis. Asymptotically efﬁcient adaptive allocation rules
IEEE Transactions on Automatic

for the multiarmed bandit problem with switching cost.
Control  33(10):899–906  1988.

[3] R. Arora  O. Dekel  and A. Tewari. Online bandit learning against an adaptive adversary:
from regret to policy regret. In Proceedings of the Twenty-Ninth International Conference on
Machine Learning  2012.

[4] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[5] A. Borodin and R. El-Yaniv. Online computation and competitive analysis. Cambridge Uni-

versity Press  1998.

[6] S. Bubeck  R. Munos  G. Stoltz  and C. Szepesv´ari. X-armed bandits. Journal of Machine

Learning Research  12:1655–1695  2011.

[7] N. Cesa-Bianchi  C. Gentile  and Y. Mansour. Regret minimization for reserve prices in
second-price auctions. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms
(SODA13)  2013.

[8] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[9] N. Cesa-Bianchi  Y. Mansour  and G. Stoltz.

Improved second-order bounds for prediction

with expert advice. Machine Learning  66(2/3):321–352  2007.

[10] V. Dani and T. P. Hayes. Robbing the bandit: Less regret in online geometric optimization
against an adaptive adversary. In Proceedings of the Seventeenth Annual ACM-SIAM Sympo-
sium on Discrete Algorithms  2006.

[11] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of computer and System Sciences  55(1):119–139  1997.

[12] A. Gyorgy and G. Neu. Near-optimal rates for limited-delay universal lossy source coding. In

IEEE International Symposium on Information Theory  pages 2218–2222  2011.

[13] T. Jun. A survey on the bandit problem with switching costs. De Economist  152:513–541 

2004.

[14] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of Com-

puter and System Sciences  71:291–307  2005.

[15] N. Littlestone and M.K. Warmuth. The weighted majority algorithm. Information and Com-

putation  108:212–261  1994.

[16] O. Maillard and R. Munos. Adaptive bandits: Towards the best history-dependent strategy. In
Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics 
2010.

[17] H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an
adaptive adversary. In Proceedings of the Seventeenth Annual Conference on Learning Theory 
2004.

[18] N. Merhav  E. Ordentlich  G. Seroussi  and M.J. Weinberger. Sequential strategies for loss
functions with memory. IEEE Transactions on Information Theory  48(7):1947–1958  2002.
[19] C. Mesterharm. Online learning with delayed label feedback. In Proceedings of the Sixteenth

International Conference on Algorithmic Learning Theory  2005.

[20] R. Ortner. Online regret bounds for Markov decision processes with deterministic transitions.

Theoretical Computer Science  411(29–30):2684–2695  2010.

[21] O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.

CoRR  abs/1209.2388  2012.

9

,Nicolò Cesa-Bianchi
Ofer Dekel
Ohad Shamir
Ming Liang
Xiaolin Hu
Bo Zhang
Luca Ambrogioni
Max Hinne
Marcel Van Gerven
Eric Maris