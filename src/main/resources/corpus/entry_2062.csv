2010,Fast global convergence rates of gradient methods for high-dimensional statistical recovery,Many statistical $M$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.  We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.  This high-dimensional structure precludes the usual global assumptions---namely  strong convexity and smoothness conditions---that underlie classical optimization analysis.  We define appropriately restricted versions of these conditions  and show that they are satisfied with high probability for various statistical models.  Under these conditions  our theory guarantees that Nesterov's first-order method~\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model  meaning the typical Euclidean distance between the true unknown parameter $\theta^*$ and the optimal solution $\widehat{\theta}$.  This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.  Our analysis applies to a wide range of $M$-estimators and statistical models  including sparse linear regression using Lasso ($\ell_1$-regularized regression)  group Lasso  block sparsity  and low-rank matrix recovery using nuclear norm regularization.  Overall  this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.,Fast global convergence of gradient methods

for high-dimensional statistical recovery

Alekh Agarwal1

Department of Electrical Engineering and Computer Science1 and Department of Statistics2

Sahand N. Negahban1

Martin J. Wainwright1 2

University of California  Berkeley

Berkeley  CA 94720-1776

{alekh sahand n wainwrig}@eecs.berkeley.edu

Abstract

Many statistical M -estimators are based on convex optimization problems
formed by the weighted sum of a loss function with a norm-based regular-
izer. We analyze the convergence rates of ﬁrst-order gradient methods for
solving such problems within a high-dimensional framework that allows the
data dimension d to grow with (and possibly exceed) the sample size n.
This high-dimensional structure precludes the usual global assumptions—
namely  strong convexity and smoothness conditions—that underlie clas-
sical optimization analysis. We deﬁne appropriately restricted versions of
these conditions  and show that they are satisﬁed with high probability
for various statistical models. Under these conditions  our theory guaran-
tees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate
of convergence up to the statistical precision of the model  meaning the
typical Euclidean distance between the true unknown parameter θ∗ and

the optimal solution bθ. This globally linear rate is substantially faster than

previous analyses of global convergence for speciﬁc methods that yielded
only sublinear rates. Our analysis applies to a wide range of M -estimators
and statistical models  including sparse linear regression using Lasso (`1-
regularized regression)  group Lasso  block sparsity  and low-rank matrix
recovery using nuclear norm regularization. Overall  this result reveals an
interesting connection between statistical precision and computational eﬃ-
ciency in high-dimensional estimation.

1

Introduction

High-dimensional data sets present challenges that are both statistical and computational in
nature. On the statistical side  recent years have witnessed a ﬂurry of results on consistency
and rates for various estimators under high-dimensional scaling  meaning that the data
dimension d and other structural parameters are allowed to grow with the sample size
n. These results typically involve some assumption regarding the underlying structure of
the parameter space  including sparse vectors  low-rank matrices  or structured regression
functions  as well as some regularity conditions on the data-generating process. On the
computational side  many estimators for statistical recovery are based on solving convex
programs. Examples of such M -estimators include `1-regularized quadratic programming
(Lasso)  second-order cone programs for sparse non-parametric regression  and semideﬁnite
programming relaxations for low-rank matrix recovery.

In parallel  a line of recent work (e.g.  [3  7  6  5  12  18]) focuses on polynomial-time
algorithms for solving these types of convex programs. Several authors [2  6  1] have used
variants of Nesterov’s accelerated gradient method [12] to obtain algorithms with a global

1

sublinear rate of convergence. For the special case of compressed sensing (sparse regression
with incoherent design)  some authors have established fast convergence rates in a local
sense–once the iterates are close enough to the optimum [3  5]. Other authors have studied
ﬁnite convergence of greedy algorithms (e.g.  [18]). If an algorithm identiﬁes the support
set of the optimal solution  the problem is then eﬀectively reduced to the lower-dimensional
subspace  and thus fast convergence can be guaranteed in a local sense. Also in application to
compressed sensing  Garg and Khandekar [4] showed that a thresholded gradient algorithm
converges rapidly up to some tolerance; we discuss this result in more detail following our
Corollary 2 on this special case of sparse linear models.

Unfortunately  for general convex programs with only Lipschitz conditions  the best conver-
gence rates in a global sense using ﬁrst-order methods are sub-linear. Much faster global
rates—in particular  at a linear or geometric rate—can be achieved if global regularity condi-
tions like strong convexity and smoothness are imposed [11]. However  a challenging aspect
of statistical estimation in high dimensions is that the underlying optimization problems
can never be globally strongly convex when d > n in typical cases (since the d × d Hessian
matrix is rank-deﬁcient)  and global smoothness conditions cannot hold when d/n → +∞.
In this paper  we analyze a simple variant of the composite gradient method due to
Nesterov [12] in application to the optimization problems that underlie regularized M -
estimators. Our main contribution is to establish a form of global geometric convergence for
this algorithm that holds for a broad class of high-dimensional statistical problems. We do
so by leveraging the notion of restricted strong convexity  used in recent work by Negahban
et al. [8] to derive various bounds on the statistical error in high-dimensional estimation.
Our analysis consists of two parts. We ﬁrst establish that for optimization problems un-
derlying such M -estimators  appropriately modiﬁed notions of restricted strong convexity
(RSC) and smoothness (RSM) suﬃce to establish global linear convergence of a ﬁrst-order
method. Our second contribution is to prove that for the iterates generated by our ﬁrst-
order method  these RSC/RSM assumptions do indeed hold with high probability for a
broad class of statistical models  among them sparse linear regression  group-sparse regres-
sion  matrix completion  and estimation in generalized linear models. We note in passing
that our notion of RSC is related to but slightly diﬀerent than its previous use for bounding
statistical error [8]  and hence we cannot use these existing results directly.

An interesting aspect of our results is that we establish global geometric convergence only up

problem. Note that this is very natural from the statistical perspective  since it is the true

to the statistical precision of the problem  meaning the typical Euclidean distance kbθ − θ∗k
between the true parameter θ∗ and the estimate bθ obtained by solving the optimization
parameter θ∗ itself (as opposed to the solution bθ of the M -estimator) that is of primary

interest  and our analysis allows us to approach it as close as is statistically possible. Over-
all  our results reveal an interesting connection between the statistical and computational
properties of M -estimators—that is  the properties of the underlying statistical model that
make it favorable for estimation also render it more amenable to optimization procedures.

The remainder of the paper is organized as follows. In the following section  we give a precise
description of the M -estimators considered here  provide deﬁnitions of restricted strong
convexity and smoothness  and their link to the notion of statistical precision. Section 3
gives a statement of our main result  as well as its corollaries when specialized to various
statistical models. Section 4 provides some simulation results that conﬁrm the accuracy of
our theoretical predictions. Due to space constraints  we refer the reader to the full-length
version of our paper for technical details.

2 Problem formulation and optimization algorithm

In this section  we begin by describing the class of regularized M -estimators to which our
analysis applies  as well as the optimization algorithms that we analyze. Finally  we describe
the assumptions that underlie our main result.

2

A class of regularized M -estimators: Given a random variable Z ∼ P taking values
in some set Z  let Z n
from
P. Assuming that P lies within some indexed family {Pθ  θ ∈ Ω}  the goal is to recover an
estimate of the unknown true parameter θ∗ ∈ Ω generating the data. In order to do so  we
consider the regularized M -estimator

1 = {Z1  . . .   Zn} be a collection of n observations drawn i.i.d.

bθλn ∈ arg min

θ∈Ω(cid:8)L(θ; Z n

1 ) + λnR(θ)(cid:9) 

where L : Ω×Z n 7→ R is a loss function  and R : Ω 7→ R+ is a non-negative regularizer on
the parameter space. Throughout this paper  we assume that the loss function L is convex
and diﬀerentiable  and that the regularizer R is a norm. In order to assess the quality of
an estimate  we measure the error kbθλn − θ∗k in some norm induced by an inner product
h·  ·i on the parameter space. Typical choices are the standard Euclidean inner product and
`2-norm for vectors; the trace inner product and the Frobenius norm for matrices; and the
L2(P) inner product and norm for non-parametric regression. As described in more detail
in Section 3.2  a variety of estimators—among them the Lasso  structured non-parametric
regression in RKHS  and low-rank matrix recovery—can be cast in this form (1). When the
data Z n

1 are clear from the context  we frequently use the shorthand L(·) for L(·; Z n
1 ).

In general  we expect the loss function L to be
Composite objective minimization:
diﬀerentiable  while the regularizer R can be non-diﬀerentiable. Nesterov [12] proposed a
simple ﬁrst-order method to exploit this type of structure  and our focus is a slight variant
of this procedure. In particular  given some initialization θ0 ∈ Ω  consider the update
for t = 0  1  2  . . . 

θt+1 = arg min

(2)

θ∈BR(ρ)(cid:8)h∇L(θt)  θi + λnR(θ) +

2(cid:9) 
γu
2 kθ − θtk2

where γu > 0 is a parameter related to the smoothness of the loss function  and

(1)

(3)

BR(ρ) :=(cid:8)θ ∈ Ω | R(θ) ≤ ρ(cid:9)

is the ball of radius ρ in the norm deﬁned by the regularizer. The only diﬀerence from
Nesterov’s method is the additional constraint θ ∈ BR(ρ)  which is required for control
of early iterates in the high-dimensional setting. Parts of our theory apply to arbitrary
choices of the radius ρ; for obtaining results that are statistically order-optimal  a setting
ρ = Θ(R(θ∗)) with θ∗ ∈ BR(ρ) is suﬃcient  so that fairly conservative upper bounds on
R(θ∗) are adequate.
Structural conditions in high dimensions:
It is known that under global smoothness
and strong convexity assumptions  the procedure (2) enjoys a globally geometric convergence

rate  meaning that there is some α ∈ (0  1) such that kθt −bθk = O(αt) for all iterations
t = 0  1  2  . . . (e.g.  see Theorem 5 in Nesterov [12]). Unfortunately  in the high-dimensional
setting (d > n)  it is usually impossible to guarantee strong convexity of the problem (1) in
a global sense. For instance  when the data is drawn i.i.d.  the loss function consists of a
sum of n terms. The resulting d × d Hessian matrix ∇2L(θ; Z n
1 ) is often a sum of n rank-1
terms and hence rank-degenerate whenever n < d. However  as we show in this paper  in
order to obtain fast convergence rates for an optimization method  it is suﬃcient that (a)
the objective is strongly convex and smooth in a restricted set of directions  and (b) the

algorithm approaches the optimum bθ only along these directions.

Let us now formalize this intuition. Consider the ﬁrst-order Taylor series expansion of the
loss function around the point θ0 in the direction of θ:

TL(θ; θ0) := L(θ) − L(θ0) − h∇L(θ0)  θ − θ0i.

(4)

Deﬁnition 1 (Restricted strong convexity (RSC)). We say the loss function L
satisﬁes the RSC condition with strictly positive parameters (γ`  κ`  δ) if

TL(θ; θ0) ≥

γ`
2 kθ − θ0k2 − κ`δ2

for all θ  θ0 ∈ BR(ρ).

(5)

3

In order to gain intuition for this deﬁnition  ﬁrst consider the degenerate setting δ = κ` = 0.
In this case  imposing the condition (5) for all θ ∈ Ω is equivalent to the usual deﬁnition
of strong convexity on the optimization set. In contrast  when the pair (δ  κ`) are strictly
positive  the condition (5) only applies to a limited set of vectors. In particular  when θ0 is

set equal to the optimum bθ  and we assume that θ belongs to the set
δ2(cid:9) 

C := BR(ρ) ∩(cid:8)θ ∈ Ω | kθ −bθk2 ≥

4κ`
γ`

then condition (5) implies that TL(θ;bθ) ≥ γ`
4 kθ −bθk2 for all θ ∈ C. Thus  for any feasible θ
that is not too close to the optimum bθ  we are guaranteed strong convexity in the direction
θ −bθ.

We now specify an analogous notion of restricted smoothness:

Deﬁnition 2 (Restricted smoothness (RSM)). We say the loss function L satisﬁes
the RSM condition with strictly positive parameters (γu  κu  δ) if

TL(θ;bθ) ≤

γu

2 kθ −bθk2 + κuδ2

for all θ ∈ BR(ρ).

(6)

Note that the tolerance parameter δ is the same as that in the deﬁnition (5). The additional
term κuδ2 is not present in analogous smoothness conditions in the optimization literature 
but it is essential in our set-up.

Loss functions and statistical precision:
In order for these deﬁnitions to be sensi-
ble and of practical interest  it remains to clarify two issues. First  for what types of loss
function and regularization pairs can we expect RSC/RSM to hold? Second  what is the
smallest tolerance δ with which they can hold? Past work by Negahban et al. [8] has intro-
duced the class of decomposable regularizers; it includes various regularizers frequently used
in M -estimation  among them `1-norm regularization  block-sparse regularization  nuclear
norm regularization  and various combinations of such norms. Negahban et al. [8] showed
that versions of RSC with respect to θ∗ hold for suitable loss functions combined with a
decomposable regularizer. The deﬁnition of RSC given here is related but slightly diﬀerent:
instead of control in a neighborhood of the true parameter θ∗  we need control over the iter-

ates of an algorithm approaching the optimum bθ. Nonetheless  it can be also be shown that

our form of RSC (and also RSM) holds with high probability for decomposable regularizers 
and this fact underlies the corollaries stated in Section 3.2.

With regards to the choice of tolerance parameter δ  as our results will clarify  it makes little
sense to be concerned with choices that are substantially smaller than the statistical precision
of the model. There are various ways in which statistical precision can be deﬁned; one
natural one is 2
in the data-dependent loss function.1 The statistical precision of various M -estimators
under high-dimensional scaling are now relatively well understood  and in the sequel  we
will encounter various models for which the RSM/RSC conditions hold with tolerance equal
to the statistical precision.

stat := E[kbθλn − θ∗k2]  where the expectation is taken over the randomness

3 Global geometric convergence and its consequences

In this section  we ﬁrst state the main result of our paper  and discuss some of its conse-
quences. We illustrate its application to several statistical models in Section 3.2.

1As written  statistical precision also depends on the choice of λn  but our theory will involve

speciﬁc choices of λn that are order-optimal.

4

3.1 Guarantee of geometric convergence

Recall that bθλn denotes any optimal solution to the problem (1). Our main theorem guaran-
tees that if the RSC/RSM conditions hold with tolerance δ  then Algorithm (2) is guaranteed
to have a geometric rate of convergence up to this tolerance. The theorem statement involves
the objective function φ(θ) = L(θ) + λnR(θ).
Theorem 1 (Geometric convergence). Suppose that the loss function satisﬁes conditions
(RSC) and (RSM) with a tolerance δ and parameters (γ`  γu  κ`  κu). Then the sequence
{θt}∞

t=0 generated by the updates (2) satisﬁes

+ c1δ2

for all t = 0  1  2  . . .

(7)

kθt −bθk2 ≤ c0(cid:18)1 −

where c0 := 2 (φ(0)−φ(bθ))

γ`

  and c1 := 8γu
γ 2

γ`

4γu(cid:19)t
` (cid:0) 4γ`κ`

γu

+ κu(cid:1).

4γu

Remarks: Note that the bound (7) consists of two terms: the ﬁrst term decays exponen-
tially fast with the contraction coeﬃcient α := 1 − γ`
. The second term is an additive
oﬀset  which becomes relevant only for t large enough such that kθt −bθk2 = O(δ2). Thus 
the result guarantees a globally geometric rate of convergence up to the tolerance parameter
δ. Previous work has focused primarily on the case of sparse linear regression. For this spe-
cial case  certain methods are known to be globally convergent at sublinear rates (e.g.  [2]) 
meaning of the type O(1/t2). The geometric rate of convergence guaranteed by Theorem 1
is exponentially faster. Other work on sparse regression [3  5] has provided geometric rates
of convergence that hold once the iterates are close to the optimum. In contrast  Theorem 1
guarantees geometric convergence if the iterates are not too close to the optimum bθ.
In Section 3.2  we describe a number of concrete models for which the (RSC) and (RSM)
conditions hold with δ (cid:16) stat  which leads to the following result.
Corollary 1. Suppose that the loss function satisﬁes conditions (RSC) and (RSM) with
tolerance δ = O(stat) and parameters (γ`  γu  κ`  κu). Then
log(4γu/(4γu − γ`))(cid:1)

T = O(cid:0)

log(1/stat)

steps of the updates (2) ensures that kθT − θ∗k2 = O(2
In the setting of statistical recovery  since the true parameter θ∗ is of primary interest  there
is little point to optimizing to a tolerance beyond the statistical precision. To the best of
our knowledge  this result—where fast convergence happens when the optimization error is
larger than statistical precision—is the ﬁrst of its type  and makes for an interesting contrast
with other local convergence results.

stat).

(8)

3.2 Consequences for speciﬁc statistical models

We now consider the consequences of Theorem 1 for some speciﬁc statistical models. In
contrast to the previous deterministic results  these corollaries hold with high probability.

Sparse linear regression: First  we consider the case of sparse least-squares regression.
Given an unknown regression vector θ∗ ∈ Rd  suppose that we make n i.i.d. observations of
the form yi = hxi  θ∗i + wi  where wi is zero-mean noise. For this model  each observation
is of the form Zi = (xi  yi) ∈ Rd × R. In a variety of applications  it is natural to assume
that θ∗ is sparse. For a parameter q ∈ [0  1] and radius Rq > 0  let us deﬁne the `q “ball”

Bq(Rq) :=(cid:8)θ ∈ Rd |

dXj=1

|βj|q ≤ Rq(cid:9).

(9)

Note that q = 0 corresponds to the case of “hard sparsity”  for which any vector β ∈ B0(R0)
is supported on a set of cardinality at most R0. For q ∈ (0  1]  membership in Bq(Rq)
enforces a decay rate on the ordered coeﬃcients  thereby modelling approximate sparsity.

5

In order to estimate the unknown regression vector θ∗ ∈ Bq(Rq)  we consider the usual
Lasso program  with the quadratic loss function L(θ; Z n
i=1(yi − hxi  θi)2 and the
`1-norm regularizer R(θ) := kθk1. We consider the Lasso in application to a random design
model  in which each predictor vector xi ∼ N (0  Σ); we assume that maxj=1 ... d Σjj ≤ 1 for
standardization  and that the condition number κ(Σ) is ﬁnite.

2nPn

1 ) := 1

c2

for all t = 0  1  2  . . ..

(10)

Corollary 2 (Sparse vector recovery). Suppose that the observation noise wi is zero-mean
and sub-Gaussian with parameter σ  and θ∗ ∈ Bq(Rq)  and we use the Lasso program with
λn = 2σq log d
n . Then there are universal positive constants ci  i = 0  1  2  3 such that with
log d )q/2(cid:1) satisfy
probability at least 1 − exp(−c3nλ2
κ(Σ)(cid:19)t

n)  the iterates (2) with ρ2 = Θ(cid:0)σ2Rq( n
+ c1 σ2Rq(cid:18) log d
n (cid:19)1−q/2
{z
}

2 ≤ c0(cid:18)1 −

kθt −bθk2

It is worth noting that the form of statistical error stat given in bound (10) is known to
be minimax optimal up to constant factors [13]. In related work  Garg and Khandekar [4]
showed that for the special case of design matrices that satisfy the restricted isometry prop-
erty (RIP)  a thresholded gradient method has geometric convergence up to the tolerance

kwk2/√n ≈ σ. However  this tolerance is independent of sample size  and far larger the sta-

tistical error stat if n > log d; moreover  severe conditions like RIP are not needed to ensure
fast convergence. In particular  Corollary 2 guarantees guarantees geometric convergence
up to stat for many random matrices that violate RIP. The proof of Corollary 2 involves
exploiting some random matrix theory results [14] in order to verify that the RSC/RSM
conditions hold with high probability (see the full-length version for details).

|

2
stat

Matrix regression with rank constraints: For a pair of matrices A  B ∈ Rm×m  we use
hhA  Bii = trace(AT B) to denote the trace inner product. Suppose that we are given n i.i.d.
observations of the form yi = hhXi  Θ∗ii + wi  where wi is zero-mean noise with variance σ2 
and Xi ∈ Rm×m is an observation matrix. The parameter space is Ω = Rm×m and each ob-
servation is of the form Zi = (Xi  yi) ∈ Rm×m× R. In many contexts  it is natural to assume
that Θ∗ is exactly or approximately low rank; applications include collaborative ﬁltering and
matrix completion [7  15]  compressed sensing [16]  and multitask learning [19  10  17]. In
order to model such behavior  we let σ(Θ∗) ∈ Rm denote the vector of singular values of
Θ∗ (padded with zeros as necessary)  and impose the constraint σ(Θ∗) ∈ Bq(Rq). We then
1 ) = 1
consider the M -estimator based on the quadratic loss L(Θ; Z n
i=1(yi − hhXi  Θii)2
combined with the nuclear norm R(Θ) = kσ(Θ)k1 as the regularizer.
Various problems can be cast within this framework of matrix regression:
• Matrix completion: In this case  observation yi is a noisy version of a randomly selected
entry Θ∗
a(i) b(i) of the unknown matrix. It is a special case with Xi = Ea(i)b(i)  the matrix
with one in position (a(i)  b(i)) and zeros elsewhere.
• Compressed sensing: In this case  the observation matrices Xi are dense  drawn from
some random ensemble  with the simplest being Xi ∈ Rm×m with i.i.d. N (0  1) entries.
• Multitask regression: In this case  the matrix Θ∗ is likely to be non-square  with the
column size m2 corresponding to the dimension of the response variable  and m1 to the
number of predictors. Imposing a low-rank constraint on Θ∗ is equivalent to requiring
that the regression vectors (or columns of the matrix) lie close to a lower-dimensional
subspace. See the papers [10  17] for more details on reformulating this problem as an
instance of matrix regression.

2nPn

For each of these problems  it is possible to show that suitable forms of the RSC/RSM
conditions will hold with high probability. For the case of matrix completion  the paper [9]
establishes a form of RSC useful for controlling statistical error; this argument can be suit-
ably modiﬁed to establish related notions of RSC/RSM required for ensuring fast algorithmic
convergence. Similar statements apply to the settings of compressed sensing and multi-task

6

2
mat (cid:16)

(cid:17)1−q/2

n

Rq(cid:16) m log m
Rq(cid:16) m
n(cid:17)1−q/2




for matrix completion

otherwise 

regression. For these matrix regression problems  consider the statistical precision

rates that (up to logarithmic factors) are known to be minimax-optimal [9  17]. As dictated

by this statistical theory  the regularization parameter should be chosen as λn = cσq m log m
for matrix completion  and λn = cσp m
n otherwise  where c > 0 is a universal positive con-
stant. The following result applies to matrix regression problems for which the RSC/RSM
conditions hold with tolerance δ = stat.
Corollary 3 (Low-rank matrix recovery). Suppose that σ(Θ∗) ∈ Bq(Rq)  and the observa-
tion noise is zero-mean σ-sub-Gaussian. Then there are universal positive constants c1  c2  c3
λn (cid:17) satisfy
such that with probability at least 1−exp(−c3nλ2
Here the contraction coeﬃcient ν ∈ (0  1) is a universal constant  independent of (n  m  Rq) 
depending on the parameters (γ`  γu). We refer the reader to the full-length version for
speciﬁc form taken for diﬀerent variants of matrix regression.

n)  the iterates (2) with ρ = Θ(cid:16) mat

for all t = 0  1  2  . . ..

|||Θt − Θ∗|||2

F ≤ c0νt + c12

mat

n

4 Simulations

In this section  we provide some experimental results that conﬁrm the accuracy of our theo-
retical predictions. In particular  these results verify the predicted linear rates of convergence
under the conditions of Corollaries 2 and 3.

Sparse regression: We consider a random ensemble of problems  in which each de-
sign vector xi ∈ Rd is generated i.i.d.
according to the recursion x(1) = z1 and
x(j) = zj + υxi(j − 1) for j = 2  . . .   d  where the zj are N (0  1)  and υ ∈ [0  1) is a correla-
tion parameter. The singular values of the resulting covariance matrix Σ satisfy the bounds
σmin(Σ) ≥ 1/(1 + υ)2 and σmax(Σ) ≤
(1−υ)2(1+υ) . Note that Σ has a ﬁnite condition number
for all υ ∈ [0  1); for υ = 0  it is the identity  but it becomes ill-conditioned as υ → 1. We
recall that in this setting yi = hxi  θ∗i + wi where wi ∼ N (0  1) and θ∗ ∈ Bq(Rq). We study
the convergence properties for sample sizes n = αs log d using diﬀerent values of α. We note
that the per iteration cost of our algorithm is n × d. All our results are averaged over 10
random trials.

2

Our ﬁrst experiment is based on taking the correlation parameter υ = 0  and the `q-ball
parameter q = 0  corresponding to exact sparsity. We then measure convergence rates for
α ∈ {1  1.25  5  25} with d = 40000 and s = (log d)2. As shown in Figure 1(a)  the procedure
fails to converge for α = 1: with this setting  the sample size n is too small for conditions
(RSC) and (RSM) to hold  so that a constant step size leads to oscillations without these
conditions. For α suﬃciently large to ensure RSC/RSM  we observe a geometric convergence

of the error kθt − bθk2  and the convergence rate is faster for α = 25 compared to α = 5 

since the RSC/RSM constants are better with larger sample size.

On the other hand  we expect the convergence rates to be slower when the condition number
of Σ is worse; in addition to address this issue  we ran the same set of experiments with the
correlation parameter υ = 0.5. As shown in Figure 1(b)  in sharp contrast to the case υ = 0 
we no longer observe geometric convergence for α = 1.25  since the conditioning of Σ with
υ = 0.5 is much poorer than with the identity matrix. Finally  we also expect optimization
to be harder as the sparsity parameter q ∈ [0  1] is increase away from zero. For larger q 
larger sample sizes are required to verify the RSC/RSM conditions. Figure 1(c) shows that
even with υ = 0  setting α = 5 is required for geometric convergence.

Low-rank matrices: We also performed experiments with two diﬀerent versions of low-
rank matrix regression  each time with m2 = 1602. The ﬁrst setting is a version of com-
pressed sensing with matrices Xi ∈ R160×160 with i.i.d. N (0  1) entries  and we set q = 0 

7

2

0

−2

−4

−6

)
2
k
ˆβ
−

t
β
k
(
g
o
l

log error vs. iterations

 

α = 1
α =1.25
α = 5
α = 25

2

0

−2

−4

−6

−8

)
2
k
ˆβ
−

t
β
k
(
g
o
l

 

−8
0

50

100

Iterations
(a)

150

200

−10
0

 

α = 1
α =1.25
α = 5
α = 25

50

100

Iterations
(b)

150

200

 

−8
0

50

log error vs. iterations

 

log error vs. iterations

 

2

0

−2

−4

−6

)
2
k
ˆβ
−

t
β
k
(
g
o
l

α = 1
α =1.25
α = 5
α = 25

150

200

100

Iterations
(c)

Figure 1. Plot of the log of the optimization error log(kθt − bθk2) in the sparse linear
regression problem. In this problem  d = 40000  s = (log d)2  n = αs log d. Plot (a) shows
convergence for the exact sparse case with q = 0 and Σ = I (i.e. υ = 0). In panel (b)  we
observe how convergence rates change for a non-identity covariance with υ = 0.5. Finally
plot (c) shows the convergence rates when υ = 0  q = 1.

and formed a matrix Θ∗ with rank R0 = dlog me. We then performed a series of trials with
sample size n = αR0 m  with the parameter α ∈ {1  5  25}. The per iteration cost in this
case is n × m2. As seen in Figure 2(a)  the general behavior of convergence rates in this
problem stays the same as for the sparse linear regression problem: it fails to converge when
α is too small  and converges geometrically (with a progressively faster rate) as α increases.
Figure 2(b) shows matrix completion also enjoys geometric convergence  for both exactly
low-rank (q = 0) and approximately low-rank matrices.

log error vs. iterations

 

α = 1
α = 5
α =25

2

0

−2

−4

−6

)
F
k
ˆΘ
−

t

Θ
k
(
g
o
l

1

0

−1

−2

−3

−4

)
F
k
ˆΘ
−

t

Θ
k
(
g
o
l

log error vs. iterations

 

q = 0
q =0.5
q = 1

−8
 
0

50

100

Iterations
(a)

150

200

−5
 
0

10

20

40

50

60

30

Iterations
(b)

(a) Plot of log Frobenius error log(|||Θt − bΘ|||F ) versus number of iterations
Figure 2.
in matrix compressed sensing for a matrix size m = 160 with rank R0 = dlog(160)e  and
sample sizes n = αR0m. For α = 1  the algorithm oscillates whereas geometric convergence
is obtained for α ∈ {5  25}  consistent with the theoretical prediction.
(b) Plot of log
Frobenius error log(|||Θt − bΘ|||F ) versus number of iterations in matrix completion with
approximately low rank matrices (q ∈ {0  0.5  1})  showing geometric convergence.

5 Discussion

We have shown that even though high-dimensional M -estimators in statistics are neither
strongly convex nor smooth  simple ﬁrst-order methods can still enjoy global guarantees of
geometric convergence. The key insight is that strong convexity and smoothness need only
hold in restricted senses  and moreover  these conditions are satisﬁed with high probabil-
ity for many statistical models and decomposable regularizers used in practice. Examples
include sparse linear regression and `1-regularization  various statistical models with group-
sparse regularization  and matrix regression with nuclear norm constraints. Overall  our
results highlight that the properties of M -estimators favorable for fast rates in a statistical
sense can also be used to establish fast rates for optimization algorithms.

Acknowledgements: AA  SN and MJW were partially supported by grants AFOSR-
09NL184; SN and MJW acknowledge additional funding from NSF-CDI-0941742.

8

References

[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[2] S. Becker  J. Bobin  and E. J. Candes. Nesta: a fast and accurate ﬁrst-order method

for sparse recovery. Technical report  Stanford University  2009.

[3] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal

of Fourier Analysis and Applications  14:813–837  2008.

[4] R. Garg and R. Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm
for sparse recovery with restricted isometry property. In ICML  New York  NY  USA 
2009. ACM.

[5] E. T. Hale  Y. Wotao  and Y. Zhang. Fixed-point continuation for `1-minimization:

Methodology and convergence. SIAM J. on Optimization  19(3):1107–1130  2008.

[6] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization.
International Conference on Machine Learning  New York  NY  USA  2009. ACM.

In

[7] Z. Lin  A. Ganesh  J. Wright  L. Wu  M. Chen  and Y. Ma. Fast convex optimization
algorithms for exact recovery of a corrupted low-rank matrix. Technical Report UILU-
ENG-09-2214  Univ. Illinois  Urbana-Champaign  July 2009.

[8] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for
high-dimensional analysis of M-estimators with decomposable regularizers. In NIPS
Conference  Vancouver  Canada  December 2009. Full length version arxiv:1010.2731v1.

[9] S. Negahban and M. J. Wainwright. Restricted strong convexity and (weighted) matrix
completion: Optimal bounds with noise. Technical report  UC Berkeley  August 2010.
arxiv:1009.2118.

[10] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise
and high-dimensional scaling. Annals of Statistics  To appear. Originally posted as
arxiv:0912.5100.

[11] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Pub-

lishers  New York  2004.

[12] Y. Nesterov. Gradient methods for minimizing composite objective function. Techni-
cal Report 76  Center for Operations Research and Econometrics (CORE)  Catholic
University of Louvain (UCL)  2007.

[13] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax rates of estimation for high-
dimensional linear regression over `q-balls. Technical Report arXiv:0910.2042  UC
Berkeley  Department of Statistics  2009.

[14] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue conditions for corre-
lated Gaussian designs. Journal of Machine Learning Research  11:2241–2259  August
2010.

[15] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning

Research  2010. Posted as arXiv:0910.0651v2.

[16] B. Recht  M. Fazel  and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix

equations via nuclear norm minimization. SIAM Review  Vol 52(3):471–501  2010.

[17] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. Tech-

nical Report arXiv:0912.5338v2  Universite de Paris  January 2010.

[18] J. A. Tropp and A. C. Gilbert. Signal recovery from random measurements via orthog-
onal matching pursuit. IEEE Transactions on Information Theory  53(12):4655–4666 
December 2007.

[19] M. Yuan  A. Ekici  Z. Lu  and R. Monteiro. Dimension reduction and coeﬃcient
estimation in multivariate linear regression. Journal Of The Royal Statistical Society
Series B  69(3):329–346  2007.

9

,Andrea Frome
Greg Corrado
Jon Shlens
Samy Bengio
Jeff Dean
Marc'Aurelio Ranzato
Tomas Mikolov
Uygar Sümbül
Douglas Roossien
Dawen Cai
Fei Chen
Nicholas Barry
John Cunningham
Edward Boyden
Liam Paninski
Ryuichi Kiryo
Gang Niu
Marthinus du Plessis
Masashi Sugiyama