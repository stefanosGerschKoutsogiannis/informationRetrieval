2010,Structured sparsity-inducing norms through submodular functions,Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible  i.e.  with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound)  in this case the L1-norm. In this paper  we investigate more general set-functions than the cardinality  that may incorporate prior knowledge or structural constraints which are common in many applications: namely  we show that for nondecreasing submodular set-functions  the corresponding convex envelope can be obtained from its Lovasz extension  a common tool in submodular analysis. This defines a family of polyhedral norms  for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions  we can give a new interpretation to known norms  such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms  in particular ones that can be used as non-factorial priors for supervised learning.,Structured sparsity-inducing norms

through submodular functions

Francis Bach

INRIA - Willow project-team

Laboratoire d’Informatique de l’Ecole Normale Sup´erieure

Paris  France

francis.bach@ens.fr

Abstract

Sparse methods for supervised learning aim at ﬁnding good linear predictors from
as few variables as possible  i.e.  with small cardinality of their supports. This
combinatorial selection problem is often turned into a convex optimization prob-
lem by replacing the cardinality function by its convex envelope (tightest convex
lower bound)  in this case the ℓ1-norm. In this paper  we investigate more gen-
eral set-functions than the cardinality  that may incorporate prior knowledge or
structural constraints which are common in many applications: namely  we show
that for nondecreasing submodular set-functions  the corresponding convex en-
velope can be obtained from its Lov´asz extension  a common tool in submodu-
lar analysis. This deﬁnes a family of polyhedral norms  for which we provide
generic algorithmic tools (subgradients and proximal operators) and theoretical
results (conditions for support recovery or high-dimensional inference). By se-
lecting speciﬁc submodular functions  we can give a new interpretation to known
norms  such as those based on rank-statistics or grouped norms with potentially
overlapping groups; we also deﬁne new norms  in particular ones that can be used
as non-factorial priors for supervised learning.

1 Introduction

The concept of parsimony is central in many scientiﬁc domains. In the context of statistics  signal
processing or machine learning  it takes the form of variable or feature selection problems  and is
commonly used in two situations: First  to make the model or the prediction more interpretable or
cheaper to use  i.e.  even if the underlying problem does not admit sparse solutions  one looks for the
best sparse approximation. Second  sparsity can also be used given prior knowledge that the model
should be sparse. In these two situations  reducing parsimony to ﬁnding models with low cardinality
turns out to be limiting  and structured parsimony has emerged as a fruitful practical extension  with
applications to image processing  text processing or bioinformatics (see  e.g.  [1  2  3  4  5  6  7]
and Section 4). For example  in [4]  structured sparsity is used to encode prior knowledge regarding
network relationship between genes  while in [6]  it is used as an alternative to structured non-
parametric Bayesian process based priors for topic models.

Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms
has focused mainly on the speciﬁc allowed set of sparsity patterns [1  2  4  6]: if w ∈ Rp denotes the
predictor we aim to estimate  and Supp(w) denotes its support  then these norms are designed so that
penalizing with these norms only leads to supports from a given family of allowed patterns. In this
paper  we instead follow the approach of [8  3] and consider speciﬁc penalty functions F (Supp(w))
of the support set  which go beyond the cardinality function  but are not limited or designed to only
forbid certain sparsity patterns. As shown in Section 6.2  these may also lead to restricted sets of
supports but their interpretation in terms of an explicit penalty on the support leads to additional

1

insights into the behavior of structured sparsity-inducing norms (see  e.g.  Section 4.1). While direct
greedy approaches (i.e.  forward selection) to the problem are considered in [8  3]  we provide
convex relaxations to the function w 7→ F (Supp(w))  which extend the traditional link between the
ℓ1-norm and the cardinality function.
This is done for a particular ensemble of set-functions F   namely nondecreasing submodular func-
tions. Submodular functions may be seen as the set-function equivalent of convex functions  and
exhibit many interesting properties that we review in Section 2—see [9] for a tutorial on submodu-
lar analysis and [10  11] for other applications to machine learning. This paper makes the following
contributions:

− We make explicit links between submodularity and sparsity by showing that the convex enve-
lope of the function w 7→ F (Supp(w)) on the ℓ∞-ball may be readily obtained from the Lov´asz
extension of the submodular function (Section 3).
− We provide generic algorithmic tools  i.e.  subgradients and proximal operators (Section 5)  as
well as theoretical guarantees  i.e.  conditions for support recovery or high-dimensional inference
(Section 6)  that extend classical results for the ℓ1-norm and show that many norms may be tackled
by the exact same analysis and algorithms.

− By selecting speciﬁc submodular functions in Section 4  we recover and give a new interpre-
tation to known norms  such as those based on rank-statistics or grouped norms with potentially
overlapping groups [1  2  7]  and we deﬁne new norms  in particular ones that can be used as non-
factorial priors for supervised learning (Section 4). These are illustrated on simulation experiments
in Section 7  where they outperform related greedy approaches [3].
Notation.
For w ∈ Rp  Supp(w) ⊂ V = {1  . . .   p} denotes the support of w  deﬁned as
Supp(w) = {j ∈ V  wj 6= 0}. For w ∈ Rp and q ∈ [1 ∞]  we denote by kwkq the ℓq-norm of w.
We denote by |w| ∈ Rp the vector of absolute values of the components of w. Moreover  given a
vector w and a matrix Q  wA and QAA are the corresponding subvector and submatrix of w and Q.
Finally  for w ∈ Rp and A ⊂ V   w(A) = Pk∈A wk (this deﬁnes a modular set-function).
2 Review of submodular function theory

Throughout this paper  we consider a nondecreasing submodular function F deﬁned on the power
set 2V of V = {1  . . .   p}  i.e.  such that:

∀A  B ⊂ V 
∀A  B ⊂ V 

F (A) + F (B) > F (A ∪ B) + F (A ∩ B) 
A ⊂ B ⇒ F (A) 6 F (B).

(submodularity)
(monotonicity)

Moreover  we assume that F (∅) = 0. These set-functions are often referred to as polymatroid
set-functions [12  13]. Also  without loss of generality  we may assume that F is strictly positive
on singletons  i.e.  for all k ∈ V   F ({k}) > 0. Indeed  if F ({k}) = 0  then by submodularity and
monotonicity  if A ∋ k  F (A) = F (A\{k}) and thus we can simply consider V \{k} instead of V .
Classical examples are the cardinality function (which will lead to the ℓ1-norm) and  given a partition
of V into B1 ∪ ··· ∪ Bk = V   the set-function A 7→ F (A) which is equal to the number of groups
B1  . . .   Bk with non empty intersection with A (which will lead to the grouped ℓ1/ℓ∞-norm [1  14]).
Lov´asz extension. Given any set-function F   one can deﬁne its Lov´asz extensionf : Rp
+ → R  as
follows; given w ∈ Rp
+  we can order the components of w in decreasing order wj1 > ··· > wjp >
0  the value f (w) is then deﬁned as:
f (w) = Pp

k=1 wjk [F ({j1  . . .   jk}) − F ({j1  . . .   jk−1})].

The Lov´asz extension f is always piecewise-linear  and when F is submodular  it is also convex
(see  e.g.  [12  9]). Moreover  for all δ ∈ {0  1}p  f (δ) = F (Supp(δ)): f is indeed an extension
from vectors in {0  1}p (which can be identiﬁed with indicator vectors of sets) to all vectors in Rp
+.
Moreover  it turns out that minimizing F over subsets  i.e.  minimizing f over {0  1}p is equivalent
to minimizing f over [0  1]p [13].
Submodular polyhedron and greedy algorithm.
We denote by P the submodular poly-
hedron [12]  deﬁned as the set of s ∈ Rp
+ such that for all A ⊂ V   s(A) 6 F (A)  i.e. 
P = {s ∈ Rp
+  ∀A ⊂ V  s(A) 6 F (A)}  where we use the notation s(A) = Pk∈A sk. One

(1)

2

(0 1)/F({2})

(1 1)/F({1 2})

(1 0)/F({1})

Figure 1: Polyhedral unit ball  for 4 different submodular functions (two variables)  with different
stable inseparable sets leading to different sets of extreme points; changing values of F may make
some of the extreme points disappear. From left to right: F (A) = |A|1/2 (all possible extreme
points)  F (A) = |A| (leading to the ℓ1-norm)  F (A) = min{|A|  1} (leading to the ℓ∞-norm) 
F (A) = 1

2 1{A∩{2}6=∅} + 1{A6=∅} (leading to the structured norm Ω(w) = 1

2|w2| + kwk∞).

important result in submodular analysis is that if F is a nondecreasing submodular function  then
we have a representation of f as a maximum of linear functions [12  9]  i.e.  for all w ∈ Rp
+ 

(2)

f (w) = max
s∈P

w⊤s.

Instead of solving a linear program with p + 2p contraints  a solution s may then be obtained by the
following “greedy algorithm”: order the components of w in decreasing order wj1 > ··· > wjp  
and then take for all k ∈ {1  . . .   p}  sjk = F ({j1  . . .   jk}) − F ({j1  . . .   jk−1}).
Stable sets. A set A is said stable if it cannot be augmented without increasing F   i.e.  if for all
sets B ⊃ A  B 6= A ⇒ F (B) > F (A). If F is strictly increasing (such as for the cardinality)  then
all sets are stable. The set of stable sets is closed by intersection [13]  and will correspond to the set
of allowed sparsity patterns (see Section 6.2).
Separable sets. A set A is separable if we can ﬁnd a partition of A into A = B1∪···∪Bk such that
F (A) = F (B1) + ··· + F (Bk). A set A is inseparable if it is not separable. As shown in [13]  the
submodular polytope P has full dimension p as soon as F is strictly positive on all singletons  and its
faces are exactly the sets {sk = 0} for k ∈ V and {s(A) = F (A)} for stable and inseparable sets A.
We denote by T the set of such sets. This implies that P = {s ∈ Rp
+  ∀A ∈ T   s(A) 6 F (A)}.
These stable inseparable sets will play a role when describing extreme points of unit balls of our
new norms (Section 3) and for deriving concentration inequalities in Section 6.3. For the cardinality
function  stable and inseparable sets are singletons.

3 Deﬁnition and properties of structured norms
We deﬁne the function Ω(w) = f (|w|)  where |w| is the vector in Rp composed of absolute values
of w and f the Lov´asz extension of F . We have the following properties (see proof in [15])  which
show that we indeed deﬁne a norm and that it is the desired convex envelope:

Proposition 1 (Convex envelope  dual norm) Assume that the set-function F is submodular  non-
decreasing  and strictly positive for all singletons. Deﬁne Ω : w 7→ f (|w|). Then:
(i) Ω is a norm on Rp 
(ii) Ω is the convex envelope of the function g : w 7→ F (Supp(w)) on the unit ℓ∞-ball 
(iii) the dual norm (see  e.g.  [16]) of Ω is equal to Ω∗(s) = maxA⊂V ksAk1
F (A) = maxA∈T
We provide examples of submodular set-functions and norms in Section 4  where we go from set-
functions to norms  and vice-versa. From the deﬁnition of the Lov´asz extension in Eq. (1)  we see
that Ω is a polyhedral norm (i.e.  its unit ball is a polyhedron). The following proposition gives the
set of extreme points of the unit ball (see proof in [15] and examples in Figure 1):

ksAk1
F (A) .

Proposition 2 (Extreme points of unit ball) The extreme points of the unit ball of Ω are the vec-
tors

F (A) s  with s ∈ {−1  0  1}p  Supp(s) = A and A a stable inseparable set.

1

This proposition shows  that depending on the number and cardinality of the inseparable stable sets 
we can go from 2p (only singletons) to 3p − 1 extreme points (all possible sign vectors). We show
in Figure 1 examples of balls for p = 2  as well as sets of extreme points. These extreme points will
play a role in concentration inequalities derived in Section 6.

3

Figure 2: Sequence and groups: (left) groups for contiguous patterns  (right) groups for penalizing
the number of jumps in the indicator vector sequence.

1
1
1
1

0.5
0.5
0.5
0.5

s
s
s
s
t
t
t
t
h
h
h
h
g
g
g
g
e
e
e
e
w
w
w
w

i
i
i
i

0
0
0
0
−6
−6
−6
−6

−4
−4
−4
−4

−2
−2
−2
−2
log(λ)
log(λ)
log(λ)
log(λ)

0
0
0
0

1
1
1
1

0.5
0.5
0.5
0.5

s
s
s
s
t
t
t
t
h
h
h
h
g
g
g
g
e
e
e
e
w
w
w
w

i
i
i
i

0
0
0
0
−6
−6
−6
−6

0.2

0.1

s
t
h
g
e
w

i

0

−4
−4
−4
−4

−2
−2
−2
−2
log(λ)
log(λ)
log(λ)
log(λ)

0
0
0
0

−2

−1

log(λ)

1
1
1
1

0.5
0.5
0.5
0.5

s
s
s
s
t
t
t
t
h
h
h
h
g
g
g
g
e
e
e
e
w
w
w
w

i
i
i
i

0
0
0
0
−6
−6
−6
−6

−4
−4
−4
−4

−2
−2
−2
−2
log(λ)
log(λ)
log(λ)
log(λ)

0
0
0
0

Figure 3: Regularization path for a penalized least-squares problem (black: variables that should
be active  red: variables that should be left out). From left to right: ℓ1-norm penalization (a wrong
variable is included with the correct ones)  polyhedral norm for rectangles in 2D  with zoom (all
variables come in together)  mix of the two norms (correct behavior).

4 Examples of nondecreasing submodular functions

We consider three main types of submodular functions with potential applications to regularization
for supervised learning. Some existing norms are shown to be examples of our frameworks (Sec-
tion 4.1  Section 4.3)  while other novel norms are designed from speciﬁc submodular functions
(Section 4.2). Other examples of submodular functions  in particular in terms of matroids and en-
tropies  may be found in [12  10  11] and could also lead to interesting new norms. Note that set
covers  which are common examples of submodular functions are subcases of set-functions deﬁned
in Section 4.1 (see  e.g.  [9]).

4.1 Norms deﬁned with non-overlapping or overlapping groups
We consider grouped norms deﬁned with potentially overlapping groups [1  2]  i.e.  Ω(w) =
PG⊂V d(G)kwGk∞ where d is a nonnegative set-function (with potentially d(G) = 0 when G
should not be considered in the norm). It is a norm as soon as ∪G d(G)>0G = V and it corresponds
to the nondecreasing submodular function F (A) = PG∩A6=∅ d(G). In the case where ℓ∞-norms
are replaced by ℓ2-norms  [2] has shown that the set of allowed sparsity patterns are intersections of
complements of groups G with strictly positive weights. These sets happen to be the set of stable
sets for the corresponding submodular function; thus the analysis provided in Section 6.2 extends the
result of [2] to the new case of ℓ∞-norms. However  in our situation  we can give a reinterpretation
through a submodular function that counts the number of times the support A intersects groups G
with non zero weights. This goes beyond restricting the set of allowed sparsity patterns to stable
sets. We show later in this section some insights gained by this reinterpretation. We now give some
examples of norms  with various topologies of groups.
Hierarchical norms. Hierarchical norms deﬁned on directed acyclic graphs [1  5  6] correspond
to the set-function F (A) which is the cardinality of the union of ancestors of elements in A. These
have been applied to bioinformatics [5]  computer vision and topic models [6].
Norms deﬁned on grids.
If we assume that the p variables are organized in a 1D  2D or 3D
grid  [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or
convex shapes  with applications in computer vision [17]. For example  for the groups deﬁned in
the left side of Figure 2 (with unit weights)  we have F (A) = p − 2 + range(A) if A 6= ∅ and
F (∅) = 0 (the range of A is equal to max(A) − min(A) + 1). From empty sets to non-empty sets 
there is a gap of p − 1  which is larger than differences among non-empty sets. This leads to the
undesired result  which has been already observed by [2]  of adding all variables in one step  rather
than gradually  when the regularization parameter decreases in a regularized optimization problem.
In order to counterbalance this effect  adding a constant times the cardinality function has the effect
of making the ﬁrst gap relatively smaller. This corresponds to adding a constant times the ℓ1-norm
and  as shown in Figure 3  solves the problem of having all variables coming together. All patterns
are then allowed  but contiguous ones are encouraged rather than forced.

4

Another interesting new norm may be deﬁned from the groups in the right side of Figure 2. Indeed  it
corresponds to the function F (A) equal to |A| plus the number of intervals of A. Note that this also
favors contiguous patterns but is not limited to selecting a single interval (like the norm obtained
from groups in the left side of Figure 2). Note that it is to be contrasted with the total variation
(a.k.a. fused Lasso penalty [18])  which is a relaxation of the number of jumps in a vector w rather
than in its support. In 2D or 3D  this extends to the notion of perimeter and area  but we do not
pursue such extensions here.

π

0

R ∞

4.2 Spectral functions of submatrices
Given a positive semideﬁnite matrix Q ∈ Rp×p and a real-valued function h from R+ → R  one may
deﬁne tr[h(Q)] as Pp
i=1 h(λi) where λ1  . . .   λp are the (nonnegative) eigenvalues of Q [19]. We
can thus deﬁne the set-function F (A) = tr h(QAA) for A ⊂ V . The functions h(λ) = log(λ+t) for
t > 0 lead to submodular functions  as they correspond to entropies of Gaussian random variables
(see  e.g.  [12  9]). Thus  since for q ∈ (0  1)  λq = q sin qπ
log(1 + λ/t)tq−1dt (see  e.g.  [20]) 
h(λ) = λq for q ∈ (0  1] are positive linear combinations of functions that lead to nondecreasing
submodular functions. Thus  they are also nondecreasing submodular functions  and  to the best of
our knowledge  provide novel examples of such functions.
In the context of supervised learning from a design matrix X ∈ Rn×p  we naturally use Q = X⊤X.
If h is linear  then F (A) = tr X⊤A XA = Pk∈A X⊤k Xk (where XA denotes the submatrix of X with
columns in A) and we obtain a weighted cardinality function and hence and a weighted ℓ1-norm 
which is a factorial prior  i.e.  it is a sum of terms depending on each variable independently.
In a frequentist setting  the Mallows CL penalty [21] depends on the degrees of freedom  of the
form tr X⊤A XA(X⊤A XA + λI)−1. This is a non-factorial prior but unfortunately it does not lead to
a submodular function. In a Bayesian context however  it is shown by [22] that penalties of the form
log det(X⊤A XA + λI) (which lead to submodular functions) correspond to marginal likelihoods
associated to the set A and have good behavior when used within a non-convex framework. This
highlights the need for non-factorial priors which are sub-linear functions of the eigenvalues of
X⊤A XA  which is exactly what nondecreasing submodular function of submatrices are. We do not
pursue the extensive evaluation of non-factorial convex priors in this paper but provide in simulations
examples with F (A) = tr(X⊤A XA)1/2 (which is equal to the trace norm of XA [16]).

4.3 Functions of cardinality
For F (A) = h(|A|) where h is nondecreasing  such that h(0) = 0 and concave  then  from Eq. (1) 
Ω(w) is deﬁned from the rank statistics of |w| ∈ Rp
+  i.e.  if |w(1)| > |w(2)| > ··· > |w(p)| 
then Ω(w) = Pp
k=1[h(k) − h(k − 1)]|w(k)|. This includes the sum of the q largest elements  and
might lead to interesting new norms for unstructured variable selection but this is not pursued here.
However  the algorithms and analysis presented in Section 5 and Section 6 apply to this case.

5 Convex analysis and optimization

In this section we provide algorithmic tools related to optimization problems based on the regular-
ization by our novel sparsity-inducing norms. Note that since these norms are polyhedral norms with
unit balls having potentially an exponential number of vertices or faces  regular linear programming
toolboxes may not be used.
From Ω(w) = maxs∈P s⊤|w| and the greedy algorithm1 presented in Section 2 
Subgradient.
one can easily get in polynomial time one subgradient as one of the maximizers s. This allows to use
subgradient descent  with  as shown in Figure 4  slow convergence compared to proximal methods.
Proximal operator. Given regularized problems of the form minw∈Rp L(w) + λΩ(w)  where
L is differentiable with Lipschitz-continuous gradient  proximal methods have been shown to be
particularly efﬁcient ﬁrst-order methods (see  e.g.  [23]). In this paper  we consider the methods
“ISTA” and its accelerated variants “FISTA” [23]  which are compared in Figure 4.

1The greedy algorithm to ﬁnd extreme points of the submodular polyhedron should not be confused with

the greedy algorithm (e.g.  forward selection) that we consider in Section 7.

5

1

2kw − zk2

it sufﬁces to be able to solve efﬁciently problems of the form:
To apply these methods 
2 + λΩ(w). In the case of the ℓ1-norm  this reduces to soft thresholding of z 
minw∈Rp
the following proposition (see proof in [15]) shows that this is equivalent to a particular algorithm
for submodular function minimization  namely the minimum-norm-point algorithm  which has no
complexity bound but is empirically faster than algorithms with such bounds [12]:

Proposition 3 (Proximal operator) Let z ∈ Rp and λ > 0  minimizing 1
2 + λΩ(w)
is equivalent to ﬁnding the minimum of the submodular function A 7→ λF (A) − |z|(A) with the
minimum-norm-point algorithm.

2kw − zk2

In [15]  it is shown how a solution for one problem may be obtained from a solution to the other
problem. Moreover  any algorithm for minimizing submodular functions allows to get directly the
support of the unique solution of the proximal problem and that with a sequence of submodular
function minimizations  the full solution may also be obtained. Similar links between convex opti-
mization and minimization of submodular functions have been considered (see  e.g.  [24]). However 
these are dedicated to symmetric submodular functions (such as the ones obtained from graph cuts)
and are thus not directly applicable to our situation of non-increasing submodular functions.

Finally  note that using the minimum-norm-point algorithm leads to a generic algorithm that can be
applied to any submodular functions F   and that it may be rather inefﬁcient for simpler subcases
(e.g.  the ℓ1/ℓ∞-norm  tree-structured groups [6]  or general overlapping groups [7]).

6 Sparsity-inducing properties

In this section  we consider a ﬁxed design matrix X ∈ Rn×p and y ∈ Rn a vector of random
responses. Given λ > 0  we deﬁne ˆw as a minimizer of the regularized least-squares cost:

minw∈Rp

1

2nky − Xwk2

2 + λΩ(w).

(3)

We study the sparsity-inducing properties of solutions of Eq. (3)  i.e.  we determine in Section 6.2
which patterns are allowed and in Section 6.3 which sufﬁcient conditions lead to correct estimation.
Like recent analysis of sparsity-inducing norms [25]  the analysis provided in this section relies
heavily on decomposability properties of our norm Ω.

6.1 Decomposability
For a subset J of V   we denote by FJ : 2J → R the restriction of F to J  deﬁned for A ⊂ J
by FJ (A) = F (A)  and by F J : 2J c
→ R the contraction of F by J  deﬁned for A ⊂ J c by
F J (A) = F (A ∪ J) − F (A). These two functions are submodular and nondecreasing as soon as F
is (see  e.g.  [12]).
We denote by ΩJ the norm on RJ deﬁned through the submodular function FJ   and ΩJ the pseudo-
norm deﬁned on RJ c deﬁned through F J (as shown in Proposition 4  it is a norm only when J is
a stable set). Note that ΩJ c (a norm on J c) is in general different from ΩJ . Moreover  ΩJ (wJ ) is
actually equal to Ω( ˜w) where ˜wJ = wJ and ˜wJ c = 0  i.e.  it is the restriction of Ω to J.
We can now prove the following decomposition properties  which show that under certain circum-
stances  we can decompose the norm Ω on subsets J and their complements:

Proposition 4 (Decomposition) Given J ⊂ V and ΩJ and ΩJ deﬁned as above  we have:
(i) ∀w ∈ Rp  Ω(w) > ΩJ (wJ ) + ΩJ (wJ c ) 
(ii) ∀w ∈ Rp  if minj∈J |wj| > maxj∈J c |wj|   then Ω(w) = ΩJ (wJ ) + ΩJ (wJ c ) 
(iii) ΩJ is a norm on RJ c if and only if J is a stable set.

6.2 Sparsity patterns
In this section  we do not make any assumptions regarding the correct speciﬁcation of the linear
model. We show that with probability one  only stable support sets may be obtained (see proof in
[15]). For simplicity  we assume invertibility of X⊤X  which forbids the high-dimensional situation
p > n we consider in Section 6.3  but we could consider assumptions similar to the ones used in [2].

6

Proposition 5 (Stable sparsity patterns) Assume y ∈ Rn has an absolutely continuous density
with respect to the Lebesgue measure and that X⊤X is invertible. Then the minimizer ˆw of Eq. (3)
is unique and  with probability one  its support Supp( ˆw) is a stable set.

F (B)

F (B∪J)−F (J)

6.3 High-dimensional inference
We now assume that the linear model is well-speciﬁed and extend results from [26] for sufﬁcient
support recovery conditions and from [25] for estimation consistency. As seen in Proposition 4 
the norm Ω is decomposable and we use this property extensively in this section. We denote by
; by submodularity and monotonicity of F   ρ(J) is always between
ρ(J) = minB⊂J c
zero and one  and  as soon as J is stable it is strictly positive (for the ℓ1-norm  ρ(J) = 1). Moreover 
we denote by c(J) = supw∈Rp ΩJ (wJ )/kwJk2  the equivalence constant between the norm ΩJ and
the ℓ2-norm. We always have c(J) 6 |J|1/2 maxk∈V F ({k}) (with equality for the ℓ1-norm).
The following propositions allow us to get back and extend well-known results for the ℓ1-norm  i.e. 
Propositions 6 and 8 extend results based on support recovery conditions [26]; while Propositions 7
and 8 extend results based on restricted eigenvalue conditions (see  e.g.  [25]). We can also get back
results for the ℓ1/ℓ∞-norm [14]. As shown in [15]  proof techniques are similar and are adapted
through the decomposition properties from Proposition 4.

Proposition 6 (Support recovery) Assume that y = Xw∗ + σε  where ε is a standard multivariate
normal vector. Let Q = 1
n X⊤X ∈ Rp×p. Denote by J the smallest stable set containing the
support Supp(w∗) of w∗. Deﬁne ν = minj w∗
j 6=0 |w∗j| > 0  assume κ = λmin(QJJ ) > 0 and that
for η > 0  (ΩJ )∗[(ΩJ (Q−1
JJ QJj))j∈J c ] 6 1 − η. Then  if λ 6 κν
2c(J)  the minimizer ˆw is unique
and has support equal to J  with probability larger than 1 − 3P (cid:0)Ω∗(z) > ληρ(J)√n
(cid:1)  where z is a

multivariate normal with covariance matrix Q.

2σ

Proposition 7 (Consistency) Assume that y = Xw∗ + σε  where ε is a standard multivariate
normal vector. Let Q = 1
n X⊤X ∈ Rp×p. Denote by J the smallest stable set containing the support
Supp(w∗) of w∗. Assume that for all ∆ such that ΩJ (∆J c ) 6 3ΩJ(∆J )  ∆⊤Q∆ > κk∆Jk2
2. Then
24c(J)2λ
κρ(J)2 and 1
we have Ω( ˆw − w∗) 6
  with probability larger than
1 − P (cid:0)Ω∗(z) > λρ(J)√n
(cid:1) where z is a multivariate normal with covariance matrix Q.

nkX ˆw − Xw∗k2

36c(J)2λ2

2σ

2 6

κρ(J)2

Proposition 8 (Concentration inequalities) Let z be a normal variable with covariance matrix Q.
2|A| exp (cid:0) − t2F (A)2/2
Let T be the set of stable inseparable sets. Then P (Ω∗(z) > t) 6 PA∈T
1⊤QAA1 (cid:1).
7 Experiments

We provide illustrations on toy examples of some of the results presented in the paper. We consider
the regularized least-squares problem of Eq. (3)  with data generated as follows: given p  n  k  the
design matrix X ∈ Rn×p is a matrix of i.i.d. Gaussian components  normalized to have unit ℓ2-
norm columns. A set J of cardinality k is chosen at random and the weights w∗J are sampled from a
standard multivariate Gaussian distribution and w∗J c = 0. We then take y = Xw∗+n−1/2kXw∗k2 ε
where ε is a standard Gaussian vector (this corresponds to a unit signal-to-noise ratio).

Proximal methods vs. subgradient descent.
For the submodular function F (A) = |A|1/2 (a
simple submodular function beyond the cardinality) we compare three optimization algorithms de-
scribed in Section 5  subgradient descent and two proximal methods  ISTA and its accelerated ver-
sion FISTA [23]  for p = n = 1000  k = 100 and λ = 0.1. Other settings and other set-functions
would lead to similar results than the ones presented in Figure 4: FISTA is faster than ISTA  and
much faster than subgradient descent.
Relaxation of combinatorial optimization problem. We compare three strategies for solving
the combinatorial optimization problem minw∈Rp
2 + λF (Supp(w)) with F (A) =
tr(X⊤A XA)1/2  the approach based on our sparsity-inducing norms  the simpler greedy (forward
selection) approach proposed in [8  3]  and by thresholding the ordinary least-squares estimate. For
all methods  we try all possible regularization parameters. We see in the right plots of Figure 4 that

2nky − Xwk2

1

7

i

)
f
(
n
m
−
)
w

(
f

100

10−5

 
0

 

fista
ista
subgradient

r
r
r
o
o
o
r
r
r
r
r
r
e
e
e

 
 
 
l
l
l

a
a
a
u
u
u
d
d
d
s
s
s
e
e
e
r
r
r

i
i
i

20

40

time (seconds)

60

1
1
1

0.5
0.5
0.5

0
0
0
 
 
 
0
0
0

thresholded OLS
thresholded OLS
thresholded OLS
greedy
greedy
greedy
submodular
submodular
submodular

 
 
 

1

r
o
r
r
e

0.5

 
l

a
u
d
s
e
r

i

20
20
20

penalty
penalty
penalty

40
40
40

0
0

20

penalty

40

Figure 4: (Left) Comparison of iterative optimization algorithms (value of objective function vs. run-
ning time). (Middle/Right) Relaxation of combinatorial optimization problem  showing residual er-
ror 1
2 vs. penalty F (Supp( ˆw)): (middle) high-dimensional case (p = 120  n = 20 
k = 40)  (right) lower-dimensional case (p = 120  n = 120  k = 40).

nky − X ˆwk2

n
120
120
120
120
120
120
20
20
20
20
20
20

k
80
40
20
10
6
4
80
40
20
10
6
4

p
120
120
120
120
120
120
120
120
120
120
120
120

submodular
40.8 ± 0.8
35.9 ± 0.8
29.0 ± 1.0
20.4 ± 1.0
15.4 ± 0.9
11.7 ± 0.9
46.8 ± 2.1
47.9 ± 1.9
49.4 ± 2.0
49.2 ± 2.0
43.5 ± 2.0
41.0 ± 2.1

ℓ2 vs. submod.
-2.6 ± 0.5
2.4 ± 0.4
9.4 ± 0.5
17.5 ± 0.5
22.7 ± 0.5
26.3 ± 0.5
-0.6 ± 0.5
-0.3 ± 0.5
0.4 ± 0.5
0.0 ± 0.6
3.5 ± 0.8
4.8 ± 0.7

ℓ1 vs. submod.
0.6 ± 0.0
0.3 ± 0.0
-0.1 ± 0.0
-0.2 ± 0.0
-0.2 ± 0.0
-0.1 ± 0.0
3.0 ± 0.9
3.5 ± 0.9
2.2 ± 0.8
1.0 ± 0.8
0.9 ± 0.6
-1.3 ± 0.5
2/n (multiplied by 100) with
Table 1: Normalized mean-square prediction errors kX ˆw − Xw∗k2
optimal regularization parameters (averaged over 50 replications  with standard deviations divided
by √50). The performance of the submodular method is shown  then differences from all methods to
this particular one are computed  and shown in bold when they are signiﬁcantly greater than zero  as
measured by a paired t-test with level 5% (i.e.  when the submodular method is signiﬁcantly better).

greedy vs. submod.
21.8 ± 0.9
15.8 ± 1.0
6.7 ± 0.9
-2.8 ± 0.8
-5.3 ± 0.8
-6.0 ± 0.8
22.9 ± 2.3
23.7 ± 2.0
23.5 ± 2.1
20.3 ± 2.6
24.4 ± 3.0
25.1 ± 3.5

for hard cases (middle plot) convex optimization techniques perform better than other approaches 
while for easier cases with more observations (right plot)  it does as well as greedy approaches.
Non factorial priors for variable selection. We now focus on the predictive performance and
compare our new norm with F (A) = tr(X⊤A XA)1/2  with greedy approaches [3] and to regulariza-
tion by ℓ1 or ℓ2 norms. As shown in Table 1  the new norm based on non-factorial priors is more
robust than the ℓ1-norm to lower number of observations n and to larger cardinality of support k.

8 Conclusions

We have presented a family of sparsity-inducing norms dedicated to incorporating prior knowl-
edge or structural constraints on the support of linear predictors. We have provided a set of com-
mon algorithms and theoretical results  as well as simulations on synthetic examples illustrating the
good behavior of these norms. Several avenues are worth investigating: ﬁrst  we could follow cur-
rent practice in sparse methods  e.g.  by considering related adapted concave penalties to enhance
sparsity-inducing norms  or by extending some of the concepts for norms of matrices  with potential
applications in matrix factorization or multi-task learning (see  e.g.  [27] for application of submod-
ular functions to dictionary learning). Second  links between submodularity and sparsity could be
studied further  in particular by considering submodular relaxations of other combinatorial func-
tions  or studying links with other polyhedral norms such as the total variation  which are known to
be similarly associated with symmetric submodular set-functions such as graph cuts [24].
Acknowledgements. This paper was partially supported by the Agence Nationale de la Recherche
(MGA Project) and the European Research Council (SIERRA Project). The author would like to
thank Edouard Grave  Rodolphe Jenatton  Armand Joulin  Julien Mairal and Guillaume Obozinski
for discussions related to this work.

8

References

[1] P. Zhao  G. Rocha  and B. Yu. Grouped and hierarchical model selection through composite

absolute penalties. Annals of Statistics  37(6A):3468–3497  2009.

[2] R. Jenatton  J.Y. Audibert  and F. Bach. Structured variable selection with sparsity-inducing

norms. Technical report  arXiv:0904.3523  2009.

[3] J. Huang  T. Zhang  and D. Metaxas. Learning with structured sparsity. In Proc. ICML  2009.
[4] L. Jacob  G. Obozinski  and J.-P. Vert. Group Lasso with overlaps and graph Lasso. In Proc.

ICML  2009.

[5] S. Kim and E. Xing. Tree-guided group Lasso for multi-task regression with structured sparsity.

In Proc. ICML  2010.

[6] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for sparse hierarchical

dictionary learning. In Proc. ICML  2010.

[7] J. Mairal  R. Jenatton  G. Obozinski  and F. Bach. Network ﬂow algorithms for structured

sparsity. In Adv. NIPS  2010.

[8] J. Haupt and R. Nowak. Signal reconstruction from noisy random projections. IEEE Transac-

tions on Information Theory  52(9):4036–4048  2006.

[9] F Bach. Convex analysis and optimization with submodular functions: a tutorial. Technical

Report 00527714  HAL  2010.

[10] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models.

In Proc. UAI  2005.

[11] Y. Kawahara  K. Nagano  K. Tsuda  and J.A. Bilmes. Submodularity cuts and applications. In

Adv. NIPS  2009.

[12] S. Fujishige. Submodular Functions and Optimization. Elsevier  2005.
[13] J. Edmonds. Submodular functions  matroids  and certain polyhedra. In Combinatorial opti-

mization - Eureka  you shrink!  pages 11–26. Springer  2003.

[14] S. Negahban and M. J. Wainwright. Joint support recovery under high-dimensional scaling:

[15] F. Bach. Structured sparsity-inducing norms through submodular functions. Technical Report

Beneﬁts and perils of ℓ1-ℓ∞-regularization. In Adv. NIPS  2008.
00511310  HAL  2010.

[16] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[17] R. Jenatton  G. Obozinski  and F. Bach. Structured sparse principal component analysis. In

Proc. AISTATS  2009.

[18] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the

fused Lasso. J. Roy. Stat. Soc. B  67(1):91–108  2005.

[19] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge Univ. Press  1990.
[20] T. Ando. Concavity of certain maps on positive deﬁnite matrices and applications to hadamard

products. Linear Algebra and its Applications  26:203–241  1979.

[21] C. L. Mallows. Some comments on Cp. Technometrics  15(4):661–675  1973.
[22] D. Wipf and S. Nagarajan. Sparse estimation using general likelihoods and non-factorial priors.

In Adv. NIPS  2009.

[23] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[24] A. Chambolle and J. Darbon. On total variation minimization and surface evolution using
parametric maximum ﬂows. International Journal of Computer Vision  84(3):288–307  2009.
[25] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-

dimensional analysis of M-estimators with decomposable regularizers. In Adv. NIPS  2009.

[26] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2563  2006.

[27] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proc.

ICML  2010.

9

,Ryan Giordano
Tamara Broderick
Michael Jordan