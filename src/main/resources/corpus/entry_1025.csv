2012,Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem  and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups  while when all pairs of nodes communicate (as in a complete graph)  there is an optimal number of processors that depends on $r$. Surprisingly  a speedup can be obtained  in terms of the time to reach a fixed level of accuracy  by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.,Communication/Computation Tradeoffs in
Consensus-Based Distributed Optimization

Konstantinos I. Tsianos  Sean Lawlor  and Michael G. Rabbat

Department of Electrical and Computer Engineering

{konstantinos.tsianos  sean.lawlor}@mail.mcgill.ca

McGill University  Montr´eal  Canada

michael.rabbat@mcgill.ca

Abstract

We study the scalability of consensus-based distributed optimization algorithms
by considering two questions: How many processors should we use for a given
problem  and how often should they communicate when communication is not
free? Central to our analysis is a problem-speciﬁc value r which quantiﬁes the
communication/computation tradeoff. We show that organizing the communica-
tion among nodes as a k-regular expander graph [1] yields speedups  while when
all pairs of nodes communicate (as in a complete graph)  there is an optimal num-
ber of processors that depends on r. Surprisingly  a speedup can be obtained 
in terms of the time to reach a ﬁxed level of accuracy  by communicating less
and less frequently as the computation progresses. Experiments on a real cluster
solving metric learning and non-smooth convex minimization tasks demonstrate
strong agreement between theory and practice.

1

Introduction

How many processors should we use and how often should they communicate for large-scale dis-
tributed optimization? We address these questions by studying the performance and limitations of a
class of distributed algorithms that solve the general optimization problem

m(cid:88)

j=1

minimize

x∈X

F (x) =

1
m

lj(x)

(1)

where each function lj(x) is convex over a convex set X ⊆ Rd. This formulation applies widely in
machine learning scenarios  where lj(x) measures the loss of model x with respect to data point j 
and F (x) is the cumulative loss over all m data points.
Although efﬁcient serial algorithms exist [2]  the increasing size of available data and problem di-
mensionality are pushing computers to their limits and the need for parallelization arises [3]. Among
many proposed distributed approaches for solving (1)  we focus on consensus-based distributed op-
timization [4  5  6  7] where each component function in (1) is assigned to a different node in a
network (i.e.  the data is partitioned among the nodes)  and the nodes interleave local gradient-based
optimization updates with communication using a consensus protocol to collectively converge to a
minimizer of F (x).
Consensus-based algorithms are attractive because they make distributed optimization possible with-
out requiring centralized coordination or signiﬁcant network infrastructure (as opposed to  e.g.  hi-
erarchical schemes [8]). In addition  they combine simplicity of implementation with robustness to
node failures and are resilient to communication delays [9]. These qualities are important in clusters 
which are typically shared among many users  and algorithms need to be immune to slow nodes that

1

use part of their computation and communication resources for unrelated tasks. The main drawback
of consensus-based optimization algorithms comes from the potentially high communication cost
associated with distributed consensus. At the same time  existing convergence bounds in terms of it-
erations (e.g.  (7) below) suggest that increasing the number of processors slows down convergence 
which contradicts the intuition that more computing resources are better.
This paper focuses on understanding the limitations and potential for scalability of consensus-based
optimization. We build on the distributed dual averaging framework [4]. The key to our analysis is
to attach to each iteration a cost that involves two competing terms: a computation cost per itera-
tion which decreases as we add more processors  and a communication cost which depends on the
network. Our cost expression quantiﬁes the communication/computation tradeoff by a parameter r
that is easy to estimate for a given problem and platform. The role of r is essential; for example 
when nodes communicate at every iteration  we show that in complete graph topologies  there exists
an optimal number of processors nopt = 1√
r   while for k-regular expander graphs [1]  increasing
the network size yields a diminishing speedup. Similar results are obtained when nodes commu-
nicate every h > 1 iterations and even when h increases with time. We validate our analysis with
experiments on a cluster. Our results show a remarkable agreement between theory and practice.
In Section 2 we formalize the distributed optimization problem and summarize the distributed dual
averaging algorithm. Section 3 introduces the communication/computation tradeoff and contains the
basic analysis where nodes communicate at every iteration. The general case of sparsifying commu-
nication is treated in Section 4. Section 5 tests our theorical results on a real cluster implementation
and Section 6 discusses some future extensions.

2 Distributed Convex Optimization

Assume we have at our disposal a cluster with n processors to solve (1)  and suppose without loss
of generality that m is divisible by n. In the absence of any other information  we partition the data
evenly among the processors and our objective becomes to solve the optimization problem 

 n

m

n(cid:88)

i=1

n(cid:88)

m

j=1

 =

n(cid:88)

i=1

1
n

lj|i(x)

fi(x)

(2)

minimize

x∈X

F (x) =

1
m

lj(x) =

1
n

m(cid:88)

j=1

where we use the notation lj|i to denote loss associated with the jth local data point at processor
i (i.e.  j|i = (i − 1) m
n + j). The local objective functions fi(x) at each node are assumed to be
L-Lipschitz and convex. The recent distributed optimization literature contains multiple consensus-
based algorithms with similar rates of convergence for solving this type of problem. We adopt
the distributed dual averaging (DDA) framework [4] because its analysis admits a clear separation
between the standard (centralized) optimization error and the error due to distributing computation
over a network  facilitating our investigation of the communication/computation tradeoff.

2.1 Distributed Dual Averaging (DDA)

In DDA  nodes iteratively communicate and update optimization variables to solve (2). Nodes only
communicate if they are neighbors in a communication graph G = (V  E)  with the |V | = n vertices
being the processors. The communication graph is user-deﬁned (application layer) and does not
necessarily correspond to the physical interconnections between processors. DDA requires three
additional quantities: a 1-strongly convex proximal function ψ : Rd → R satisfying ψ(x) ≥ 0 and
); and a n × n doubly
ψ(0) = 0 (e.g.  ψ(x) = 1
stochastic consensus matrix P with entries pij > 0 only if either i = j or (j  i) ∈ E and pij = 0
otherwise. The algorithm repeats for each node i in discrete steps t  the following updates:

2 xT x); a positive step size sequence a(t) = O( 1√

t

n(cid:88)

j=1

(cid:26)

zi(t) =

pijzj(t − 1) + gi(t − 1)

xi(t) =argmin

1

x∈X

(cid:104)zi(t)  x(cid:105) +

(cid:0)(t − 1) · ˆxi(t − 1) + xi(t)(cid:1)

a(t)

ψ(x)

ˆxi(t) =

1
t

(cid:27)

(3)

(4)

(5)

2

where gi(t− 1) ∈ ∂fi(xi(t− 1)) is a subgradient of fi(x) evaluated at xi(t− 1). In (3)  the variable
zi(t) ∈ Rd maintains an accumulated subgradient up to time t and represents node i’s belief of
the direction of the optimum. To update zi(t) in (3)  each node must communicate to exchange the
variables zj(t) with its neighbors in G. If ψ(x∗) ≤ R2  for the local running averages ˆxi(t) deﬁned
in (5)  the error from a minimizer x∗ of F (x) after T iterations is bounded by (Theorem 1  [4])

Erri(T ) = F (ˆxi(T )) − F (x∗) ≤ R2
T a(T )

L2
2T

a(t − 1)

T(cid:88)
 2

t=1

n

n(cid:88)

j=1

+

T(cid:88)

t=1

+

L
T

a(t)

 (6)

(cid:107)¯z(t) − zj(t)(cid:107)∗ + (cid:107)¯z(t) − zi(t)(cid:107)∗

(cid:80)n

where L is the Lipschitz constant  (cid:107)·(cid:107)∗ indicates the dual norm  ¯z(t) = 1
i=1 zi(t)  and
(cid:107)¯z(t) − zi(t)(cid:107)∗ quantiﬁes the network error as a disagreement between the direction to the opti-
mum at node i and the consensus direction ¯z(t) at time t. Furthermore  from Theorem 2 in [4]  with
a(t) = A√
t

  after optimizing for A we have a bound on the error 

n

√

Erri(T ) ≤ C1

√
log (T
T

n)

 

C1 = 2LR

19 +

12

1 − √

 

λ2

(7)

(cid:115)

where λ2 is the second largest eigenvalue of P . The dependence on the communication topology
is reﬂected through λ2  since the sparsity structure of P is determined by G. According to (7) 
increasing n slows down the rate of convergence even if λ2 does not depend on n.

3 Communication/Computation Tradeoff

In consensus-based distributed optimization algorithms such as DDA  the communication graph G
and the cost of transmitting a message have an important inﬂuence on convergence speed  especially
when communicating one message requires a non-trivial amount of time (e.g.  if the dimension d of
the problem is very high).
We are interested in the shortest time to obtain an -accurate solution (i.e.  Erri(T ) ≤ ). From (7) 
1 − √
convergence is faster for topologies with good expansion properties; i.e.  when the spectral gap
λ2 does not shrink too quickly as n grows. In addition  it is preferable to have a balanced
network  where each node has the same number of neighbors so that all nodes spend roughly the
same amount of time communicating per iteration. Below we focus on two particular cases and take
G to be either a complete graph (i.e.  all pairs of nodes communicate) or a k-regular expander [1].
By using more processors  the total amount of communication inevitably increases. At the same
time  more data can be processed in parallel in the same amount of time. We focus on the scenario
where the size m of the dataset is ﬁxed but possibly very large. To understand whether there is room
for speedup  we move away from measuring iterations and employ a time model that explicitly ac-
counts for communication cost. This will allow us to study the communication/computation tradeoff
and draw conclusions based on the total amount of time to reach an  accuracy solution.

3.1 Time model

At each iteration  in step (3)  processor i computes a local subgradient on its subset of the data:

gi(x) =

∂fi(x)

∂x

=

n
m

∂lj|i(x)

∂x

.

(8)

n(cid:88)

m

j=1

The cost of this computation increases linearly with the subset size. Let us normalize time so that
one processor compute a subgradient on the full dataset of size m in 1 time unit. Then  using n cpus 
each local gradient will take 1
n time units to compute. We ignore the time required to compute the
projection in step (4); often this can be done very efﬁciently and requires negligible time when m is
large compared to n and d.

3

We account for the cost of communication as follows. In the consensus update (3)  each pair of
neighbors in G transmits and receives one variable zj(t − 1). Since the message size depends only
on the problem dimension d and does not change with m or n  we denote by r the time required to
transmit and receive one message  relative to the 1 time unit required to compute the full gradient
on all the data. If every node has k neighbors  the cost of one iteration in a network of n nodes is

(9)
Using this time model  we study the convergence rate bound (7) after attaching an appropriate time
unit cost per iteration. To obtain a speedup by increasing the number of processors n for a given
problem  we must ensure that -accuracy is achieved in fewer time units.

+ kr time units / iteration.

1
n

3.2 Simple Case: Communicate at every Iteration

In the original DDA description (3)-(5)  nodes communicate at every iteration. According to our
time model  T iterations will cost τ = T ( 1
n + kr) time units. From (7)  the time τ () to reach error
 is found by substituting for T and solving for τ (). Ignoring the log factor in (7)  we get

=  =⇒ τ () =

C 2
1
2

+ kr

time units.

(10)

(cid:17)

(cid:16) 1

n

1(cid:113) τ ()

1
n +kr

C1

This simple manipulation reveals some important facts. If communication is free  then r = 0. If in
addition the network G is a k-regular expander  then λ2 is ﬁxed [10]  C1 is independent of n and
1 /(2n). Thus  in the ideal situation  we obtain a linear speedup by increasing the number
τ () = C 2
of processors  as one would expect. In reality  of course  communication is not free.
Complete graph. Suppose that G is the complete graph  where k = n − 1 and λ2 = 0. In this
scenario we cannot keep increasing the network size without eventually harming performance due
to the excessive communication cost. For a problem with a communication/computation tradeoff r 
the optimal number of processors is calculated by minimizing τ () for n:

1√
r

∂τ ()

= 0 =⇒ nopt =

.

∂n

(11)
Again  in accordance with intuition  if the communication cost is too high (i.e.  r ≥ 1) and it takes
more time to transmit and receive a gradient than it takes to compute it  using a complete graph
cannot speedup the optimization. We reiterate that r is a quantity that can be easily measured for
a given hardware and a given optimization problem. As we report in Section 5  the optimal value
predicted by our theory agrees very well with experimental performance on a real cluster.
Expander. For the case where G is a k-regular expander  the communication cost per node remains
constant as n increases. From (10) and the expression for C1 in (7)  we see that n can be increased
without losing performance  although the beneﬁt diminishes (relative to kr) as n grows.

4 General Case: Sparse Communication

The previous section analyzes the case where processors communicate at every iteration. Next we
investigate the more general situation where we adjust the frequency of communication.

4.1 Bounded Intercommunication Intervals

Suppose that a consensus step takes place once every h + 1 iterations. That is  the algorithm repeats
h ≥ 1 cheap iterations (no communication) of cost 1
n time units followed by an expensive iteration
(with communication) with cost 1
n + kr. This strategy clearly reduces the overall average cost per
iteration. The caveat is that the network error (cid:107)¯z(t) − zi(t)(cid:107)∗ is higher because of having executed
fewer consensus steps.
In a cheap iteration we replace the update (3) by zi(t) = zi(t − 1) + gi(t − 1). After some straight-
forward algebra we can show that [for (12)  (16) please consult the supplementary material]:

Ht−1(cid:88)

h−1(cid:88)

n(cid:88)

(cid:2)P Ht−w(cid:3)

zi(t) =

Qt−1(cid:88)

ij gj(wh + k) +

gi(t − Qt + k).

(12)

w=0

k=0

j=1

k=0

4

where Ht = (cid:98) t−1
if mod(t  h) > 0 and Qt = h otherwise. Using the fact that P 1 = 1  we obtain

h (cid:99) counts the number of communication steps in t iterations  and Qt = mod(t  h)

¯z(t) − zi(t) =

1
n

zs(t) − zi(t) =

n(cid:88)

s=1

Ht−1(cid:88)

w=0

+

1
n

ij

n

j=1

k=0

gj(wh + k)

(cid:17) h−1(cid:88)

(cid:16) 1
n(cid:88)
−(cid:2)P Ht−w(cid:3)
n(cid:88)
Qt−1(cid:88)
(cid:0)gs(t − Qt + k) − gi(t − Qt + k)(cid:1).
(cid:13)(cid:13)(cid:13)(cid:13) 1
1T −(cid:2)P Ht−w(cid:3)

hL + 2hL

(cid:13)(cid:13)(cid:13)(cid:13)1

k=0

s=1

n

i :

(13)

(14)

(15)

Taking norms  recalling that the fi are convex and Lipschitz  and since Qt ≤ h  we arrive at

(cid:107)¯z(t) − zi(t)(cid:107)∗ ≤ Ht−1(cid:88)

w=0

λ2

12h

1 − √
(cid:33)−1

(cid:18) 1
(cid:23)

kr

(cid:19)

(cid:115)

Using a technique similar to that in [4] to bound the (cid:96)1 distance of row i of P Ht−w to its stationary
distribution as t grows  we can show that

(cid:107)¯z(t) − zi(t)(cid:107)∗ ≤ 2hL

(16)
for all t ≤ T . Comparing (16) to equation (29) in [4]  the network error within t iterations is no more
than h times larger when a consensus step is only performed once every h + 1 iterations. Finally 
we substitute the network error in (6). For a(t) = A√

+ 3hL

√

t=1 a(t) ≤ 2A
√
√

= Ch

n)

T   and
√

n)

√
log (T
T

.

(17)

Erri(T ) ≤

+ AL2

1 +

+ 18h

We minimize the leading term Ch over A to obtain

(cid:18)

(cid:18) R2
(cid:32)(cid:115)

A

log(T

n)
λ2

√
1 − √
  we have(cid:80)T
(cid:19)(cid:19) log (T
(cid:115)

T

t

12h

R
L

A =

1 + 18h +

1 − √
1 − √
λ2
Of the T iterations  only HT = (cid:98) T−1
h (cid:99) involve communication. So  T iterations will take
1
n

τ = (T − HT )

+ HT kr time units.

and Ch = 2RL

1 + 18h +

(cid:19)

+ HT

+ kr

12h

T
n

λ2

=

n

.

To achieve -accuracy  ignoring again the logarithmic factor  we need T = C2

h

2 iterations  or

(cid:18) T

(cid:22) T − 1

τ () =

+

n

h

(cid:19)

(cid:18) 1

n

≤ C 2
h
2

+

kr
h

time units.

(18)

(19)

(20)

From the last expression  for a ﬁxed number of processors n  there exists an optimal value for h that
depends on the network size and communication graph G:

hopt =

nkr
1−√
18 + 12

λ2

.

(21)

If the network is a complete graph  using hopt yields τ () = O(n); i.e.  using more processors
hurts performance when not communicating every iteration. On the other hand  if the network is a
k-regular expander then τ () = c1√
n + c2 for constants c1  c2  and we obtain a diminishing speedup.

4.2

Increasingly Sparse Communication

Next  we consider progressively increasing the intercommunication intervals. This captures the
intuition that as the optimization moves closer to the solution  progress slows down and a processor
should have “something signiﬁcantly new to say” before it communicates. Let hj − 1 denote the
number of cheap iterations performed between the (j − 1)st and jth expensive iteration; i.e.  the ﬁrst
communication is at iteration h1  the second at iteration h1 + h2  and so on. We consider schemes

5

where hj = jp for p ≥ 0. The number of iterations that nodes communicate out of the ﬁrst T total

iterations is given by HT = max{H : (cid:80)H
(cid:90) HT
j=1 hj ≤ T}. We have
T − 1
ypdy =⇒ H p+1
p + 1

(cid:90) HT

jp ≤ 1 +

y=1

 

(22)

1

1T −(cid:2)P Ht−w(cid:3)

p+1 ) as T → ∞. Similar to (15)  the network error is bounded as
(cid:107)·(cid:107)1 hw + 2htL.

L + 2htL = L

hw−1(cid:88)

i :

(23)

(cid:13)(cid:13)(cid:13)(cid:13)1

≤ T ≤ H p+1
T + p
p + 1
Ht−1(cid:88)

ypdy ≤ HT(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:107)¯z(t) − zi(t)(cid:107)∗ ≤ Ht−1(cid:88)

which means that HT = Θ(T

y=1

j=1

n

w=0
We split the sum into two terms based on whether or not the powers of P have converged. Using the
√
1−√
split point ˆt = log(T
(cid:107)¯z(t) − zi(t)(cid:107)∗ ≤L

  the (cid:96)1 term is bounded by 2 when w is large and by 1

(cid:107)·(cid:107)1 hw + 2htL

T when w is small:

(24)

w=0

k=0

λ2

n)

Ht−1−ˆt(cid:88)
Ht−1−ˆt(cid:88)

Ht−1(cid:88)
(cid:107)·(cid:107)1 hw + L
Ht−1(cid:88)

w=0

w=Ht−ˆt

wp + 2L

≤ L
T

≤ L
T

≤ L
p + 1

w=0

(Ht − ˆt − 1)
p + 1
Lp

wp + 2tpL

w=Ht−ˆt

1
p+1 + p

+ 2Lˆt(Ht − 1)p + 2tpL

(25)

(26)

AT 1−q +
6L2ˆtA

+

T

T(cid:88)
T(cid:88)

t=1

(27)
since T > Ht − ˆt − 1. Substituting this bound into (6) and taking the step size sequence to be
a(t) = A

t + 2tpL

T (p + 1)

+ 2LˆtH p

+

tq with A and q to be determined  we get
Erri(T ) ≤ R2

L2A

2(1 − q)T q +

3L2A

(p + 1)(1 − q)T q +

3L2pA

(p + 1)(1 − q)T 1+q

(28)

H p
t
tq +

6L2A

T

tp−q.

(cid:33)

1

1
T

O(t

T(cid:88)

H p
tq ≤ 1
t

The ﬁrst four summands converge to zero when 0 < q < 1. Since Ht = Θ(t

p+1−q(cid:17)
(cid:80)T
(29)
t=1 tp−q ≤ T p−q
which converges to zero if
p−q+1 
so the term goes to zero as T → ∞ if p < q. In conclusion  Erri(T ) converges no slower than
O( log (T
2 to balance the ﬁrst three summands  for
)  while nodes communicate

p+1 < q. To bound the last term  note that 1

T q−p . If we choose q = 1

p+1 )p
tq

√
T q−p

p+1−q+1

) since

≤ O

< 1

q− p
p+1

p+1 ) 

(cid:16)

= O

t=1

t=1

√

T

T

T

T

n)

n)

1

p

T

T

p

1

p

√
small p > 0  the rate of convergence is arbitrarily close to O( log (T
increasingly infrequently as T → ∞.
T
Out of T total iterations  DDA executes HT = Θ(T
cation and T − HT cheap iterations without communication  so

p

(cid:19)

(cid:18)

(cid:18) 1

τ () = O

+ T

p
p+1 kr

= O

T

(cid:18) T

n

+

n

kr
1

p+1

T

(cid:19)(cid:19)

p+1 ) expensive iterations involving communi-

.

(30)

In this case  the communication cost kr becomes a less and less signiﬁcant proportion of τ () as T
n ). To
increases. So for any 0 < p < 1
get Erri(T ) ≤   ignoring the logarithmic factor  we need

2  if k is ﬁxed  we approach a linear speedup behaviour Θ( T

(cid:18) Cp

(cid:19) 2

1−2p

T =



iterations  with Cp = 2LR

7 +

12p + 12

(3p + 1)(1 − √

+

λ2)

12

2p + 1

.

(31)

From this last equation we see that for 0 < p < 1
communication should  in fact  be faster than communicating at every iteration.

2 we have Cp < C1  so using increasingly sparse

6

T(cid:88)
(cid:32)

t=1

(cid:115)

5 Experimental Evaluation

To verify our theoretical ﬁndings  we implement DDA on a cluster of 14 nodes with 3.2 GHz Pen-
tium 4HT processors and 1 GB of memory each  connected via ethernet that allows for roughly
11 MB/sec throughput per node. Our implementation is in C++ using the send and receive functions
of OpenMPI v1.4.4 for communication. The Armadillo v2.3.91 library  linked to LAPACK and
BLAS  is used for efﬁcient numerical computations.

5.1 Application to Metric Learning

Metric learning [11  12  13] is a computationally intensive problem where the goal is to ﬁnd a
distance metric D(u  v) such that points that are related have a very small distance under D while
for unrelated points D is large. Following the formulation in [14]  we have a data set {uj  vj  sj}m
j=1
with uj  vj ∈ Rd and sj = {−1  1} signifying whether or not uj is similar to vj (e.g.  similar if
they are from the same class). Our goal is to ﬁnd a symmetric positive semi-deﬁnite matrix A (cid:23) 0
hinge-type loss function lj(A  b) = max{0  sj
that determines whether two points are dissimilar according to DA(· ·). In the batch setting  we
formulate the convex optimization problem

to deﬁne a pseudo-metric of the form DA(u  v) = (cid:112)(u − v)T A(u − v). To that end  we use a
(cid:0)DA(uj  vj)2 − b(cid:1) + 1} where b ≥ 1 is a threshold

m(cid:88)

j=1

minimize

A b

F (A  b) =

lj(A  b)

subject to A (cid:23) 0  b ≥ 1.

The subgradient of lj at (A  b) is zero if sj(DA(uj  vj)2 − b) ≤ −1. Otherwise

∂lj(A  b)

∂A

= sj(uj − vj)T (uj − vj) 

and

∂lj(A  b)

∂b

= −sj.

(32)

(33)

Since DDA uses vectors xi(t) and zi(t)  we represent each pair (Ai(t)  bi(t)) as a d2+1 dimensional
vector. The communication cost is thus quadratic in the dimension. In step (3) of DDA  we use the
2 xT x  in which case (4) simpliﬁes to taking xi(t) = −a(t − 1)zi(t) 
proximal function ψ(x) = 1
followed by projecting xi(t) to the constraint set by setting bi(t) ← max{1  bi(t)} and projecting
Ai(t) to the set of positive semi-deﬁnite matrices by ﬁrst taking its eigenvalue decomposition and
reconstructing Ai(t) after forcing any negative eigenvalues to zero.
We use the MNIST digits dataset which consists of 28 × 28 pixel images of handwritten digits 0
through 9. Representing images as vectors  we have d = 282 = 784 and a problem with d2 + 1 =
614657 dimensions trying to learn a 784 × 784 matrix A. With double precision arithmetic  each
DDA message has a size approximately 4.7 MB. We construct a dataset by randomly selecting 5000
pairs from the full MNIST data. One node needs 29 seconds to compute a gradient on this dataset 
and sending and receiving 4.7 MB takes 0.85 seconds. The communication/computation tradeoff
29 ≈ 0.0293. According to (11)  when G is a complete graph  we
value is estimated as r = 0.85
expect to have optimal performance when using nopt = 1√
r = 5.8 nodes. Figure 1(left) shows the
evolution of the average function value ¯F (t) = 1
i F (ˆxi(t)) for 1 to 14 processors connected as
n
a complete graph  where ˆxi(t) is as deﬁned in (5). There is a very good match between theory and
practice since the fastest convergence is achieved with n = 6 nodes.
In the second experiment  to make r closer to 0  we apply PCA to the original data and keep the top
87 principal components  containing 90% of the energy. The dimension of the problem is reduced
dramatically to 87 · 87 + 1 = 7570 and the message size to 59 KB. Using 60000 random pairs of
MNIST data  the time to compute one gradient on the entire dataset with one node is 2.1 seconds 
while the time to transmit and receive 59 KB is only 0.0104 seconds. Again  for a complete graph 
Figure 1(right) illustrates the evolution of ¯F (t) for 1 to 14 nodes. As we see  increasing n speeds up
the computation. The speedup we get is close to linear at ﬁrst  but diminishes since communication
is not entirely free. In this case r = 0.0104

2.1 = 0.005 and nopt = 14.15.

(cid:80)

5.2 Nonsmooth Convex Minimization

Next we create an artiﬁcial problem where the minima of the components fi(x) at each node are
very different  so that communication is essential in order to obtain an accurate optimizer of F (x).

7

Figure 1: (Left) In a subset of the Full MNIST data for our speciﬁc hardware  nopt = 1√
r = 5.8. The
fastest convergence is achieved on a complete graph of 6 nodes. (Right) In the reduced MNIST data
using PCA  the communication cost drops and a speedup is achieved by scaling up to 14 processors.

(cid:16)

M(cid:88)

j=1

(cid:17)

We deﬁne fi(x) as a sum of high dimensional quadratics 

j|i(x) = (x − cξ
lξ

j|i)T (x − cξ

j|i) 

ξ ∈ {1  2} 

(34)

l1
j|i(x)  l2

 

max

j|i  c2

fi(x) =

j|i(x)
where x ∈ R10 000  M = 15  000 and c1
j|i are the centers of the quadratics. Figure 2 illustrates
again the average function value ¯F (t) for 10 nodes in a complete graph topology. The baseline per-
formance is when nodes communicate at every iteration (h = 1). For this problem r = 0.00089 and 
from (21)  hopt = 1. Naturally communicating every 2 iterations (h = 2) slows down convergence.
Over the duration of the experiment  with h = 2  each node communicates with its peers 55 times.
We selected p = 0.3 for increasingly sparse communication  and got HT = 53 communications
per node. As we see  even though nodes communicate as much as the h = 2 case  convergence is
even faster than communicating at every iteration. This veriﬁes our intuition that communication is
more important in the beginning. Finally  the case where p = 1 is shown. This value is out of the
permissible range  and as expected DDA does not converge to the right solution.

Figure 2: Sparsifying communication to minimize (34) with 10 nodes in a complete graph topology.
When waiting t0.3 iterations between consensus steps  convergence is faster than communicating
at every iteration (h = 1)  even though the total number of consensus steps performed over the
duration of the experiment is equal to communicating every 2 iterations (h = 2). When waiting a
linear number of iterations between consensus steps (h = t) DDA does not converge to the right
solution. Note: all methods are initialized from the same value; the x-axis starts at 5 sec.

6 Conclusions and Future Work

The analysis and experimental evaluation in this paper focus on distributed dual averaging and re-
veal the capability of distributed dual averaging to scale with the network size. We expect that
similar results hold for other consensus-based algorithms such as [5] as well as various distributed
averaging-type algorithms (e.g.  [15  16  17]). In the future we will extend the analysis to the case of
stochastic optimization  where ht = tp could correspond to using increasingly larger mini-batches.

8

5010015020025030035040045000.511.522.533.54Time (sec)¯F(t)  n = 1n = 2n = 4n = 6n = 8n = 10n = 12n = 1410203040506000.20.40.60.811.21.41.6Time (sec)¯F(t)  n = 1n = 2n = 4n = 6n = 8n = 10n = 12n = 14204060801001201401601.21.41.61.822.22.4x 105Time (sec)¯F(t)  h = 1h = 2h = t0.3h = tReferences
[1] O. Reingold  S. Vadhan  and A. Wigderson  “Entropy waves  the zig-zag graph product  and new constant-

degree expanders ” Annals of Mathematics  vol. 155  no. 2  pp. 157–187  2002.

[2] Y. Nesterov  “Primal-dual subgradient methods for convex problems ” Mathematical Programming Series

B  vol. 120  pp. 221–259  2009.

[3] R. Bekkerman  M. Bilenko  and J. Langford  Scaling up Machine Learning  Parallel and Distributed

Approaches. Cambridge University Press  2011.

[4] J. Duchi  A. Agarwal  and M. Wainwright  “Dual averaging for distributed optimization: Convergence
analysis and network scaling ” IEEE Transactions on Automatic Control  vol. 57  no. 3  pp. 592–606 
2011.

[5] A. Nedic and A. Ozdaglar  “Distributed subgradient methods for multi-agent optimization ” IEEE Trans-

actions on Automatic Control  vol. 54  no. 1  January 2009.

[6] B. Johansson  M. Rabi  and M. Johansson  “A randomized incremental subgradient method for distributed

optimization in networked systems ” SIAM Journal on Control and Optimization  vol. 20  no. 3  2009.

[7] S. S. Ram  A. Nedic  and V. V. Veeravalli  “Distributed stochastic subgradient projection algorithms for
convex optimization ” Journal of Optimization Theory and Applications  vol. 147  no. 3  pp. 516–545 
2011.

[8] A. Agarwal and J. C. Duchi  “Distributed delayed stochastic optimization ” in Neural Information Pro-

cessing Systems  2011.

[9] K. I. Tsianos and M. G. Rabbat  “Distributed dual averaging for convex optimization under communica-

tion delays ” in American Control Conference (ACC)  2012.

[10] F. Chung  Spectral Graph Theory. AMS  1998.
[11] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. Russell  “Distance metric learning  with application to clustering

with side-information ” in Neural Information Processing Systems  2003.

[12] K. Q. Weinberger and L. K. Saul  “Distance metric learning for large margin nearest neighbor classiﬁca-

tion ” Journal of Optimization Theory and Applications  vol. 10  pp. 207–244  2009.

[13] K. Q. Weinberger  F. Sha  and L. K. Saul  “Convex optimizations for distance metric learning and pattern

classiﬁcation ” IEEE Signal Processing Magazine  2010.

[14] S. Shalev-Shwartz  Y. Singer  and A. Y. Ng  “Online and batch learning of pseudo-metrics ” in ICML 

2004  pp. 743–750.

[15] M. A. Zinkevich  M. Weimer  A. Smola  and L. Li  “Parallelized stochastic gradient descent ” in Neural

Information Processing Systems  2010.

[16] R. McDonald  K. Hall  and G. Mann  “Distributed training strategies for the structured perceptron ” in
Annual Conference of the North American Chapter of the Association for Computational Linguistics 
2012  pp. 456–464.

[17] G. Mann  R. McDonald  M. Mohri  N. Silberman  and D. D. Walker  “Efﬁcient large-scale distributed
training of conditional maximum entropy models ” in Neural Information Processing Systems  2009  pp.
1231–1239.

9

,Yasutoshi Ida
Yasuhiro Fujiwara
Hisashi Kashima