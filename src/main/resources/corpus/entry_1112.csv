2018,Constrained Cross-Entropy Method for Safe Reinforcement Learning,We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well-suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation to guarantee the convergence of the proposed algorithm. At last  we show with simulation experiments that the proposed algorithm can effectively learn feasible policies without assumptions on the feasibility of initial policies  even with non-Markovian objective functions and constraint functions.,Constrained Cross-Entropy Method for Safe

Reinforcement Learning

Department of Electrical and Systems Engineering

Min Wen

University of Pennsylvania

wenm@seas.upenn.edu

Ufuk Topcu

Department of Aerospace Engineering and Engineering Mechanics

University of Texas at Austin

utopcu@utexas.edu

Abstract

We study a safe reinforcement learning problem in which the constraints are de-
ﬁned as the expected cost over ﬁnite-length trajectories. We propose a constrained
cross-entropy-based method to solve this problem. The method explicitly tracks
its performance with respect to constraint satisfaction and thus is well-suited for
safety-critical applications. We show that the asymptotic behavior of the pro-
posed algorithm can be almost-surely described by that of an ordinary differential
equation. Then we give sufﬁcient conditions on the properties of this differential
equation for the convergence of the proposed algorithm. At last  we show with
simulation experiments that the proposed algorithm can effectively learn feasi-
ble policies without assumptions on the feasibility of initial policies  even with
non-Markovian objective functions and constraint functions.

1

Introduction

This paper studies the following constrained optimal control problem: given a dynamical system
model with continuous states and actions  a objective function and a constraint function  ﬁnd a
controller that maximizes the objective function while satisfying the constraint. Although this topic
has been studied for decades within the control community [3]  it is still challenging for practical
problems. To illustrate some major difﬁculties  consider the synthesis of a policy for a nonholonomic
mobile robot to reach a goal while avoiding obstacles (which introduces constraints) in a cost-efﬁcient
way (which induces an objective). The obstacle-free state space is usually nonconvex. The equations
of the dynamical system model are typically highly nonlinear. Constraint functions and cost functions
may not be convex or differentiable in the state and action variables. There may even be hidden
variables that are not observable and make transitions and costs non-Markovian. Given all these
difﬁculties  we still need to compute a policy that is at least feasible and improve the cost objective as
much as possible.
Reinforcement learning (RL) methods have been widely used to learn optimal policies for agents with
complicated or even unknown dynamics. For problems with continuous state and action spaces  the
agent’s policy is usually modeled as a parameterized function of states such as deep neural networks
and later trained using policy gradient methods [35; 30; 27; 28; 21; 8; 29]. By encoding control tasks
as reward or cost functions  RL has successfully solved a wide range of tasks such as Atari games
[19; 20]  the game of Go [31; 32]  controlling simulated robots [36; 24] and real robots [15; 37; 22].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Most of the existing methods for RL solve only unconstrained problems. However  it is generally
non-trivial to transform a constrained optimal control problem into an unconstrained one  due to
the asymmetry between the goals of objective optimization and constraint satisfaction. On the one
hand  it is usually acceptable to output a policy that is only locally optimal with respect to the
optimization objective. On the other hand  in many application scenarios where constraints encode
safety requirements or the amount of available resources  violating the constraint even by a small
amount may have signiﬁcant consequences.
Existing methods for safe reinforcement learning that are based on policy gradient methods cannot
guarantee strict feasibility of the policies they output  even when initialized with feasible initial
policies. When initialized with an infeasible policy  they usually are not be able to ﬁnd even a single
feasible policy until their convergence (with an example in Section 5). These limitations motivate the
following question: Can we develop a reinforcement learning algorithm that explicitly addresses the
priority of constraint satisfaction? Rather than assuming that the initial policy is feasible and that one
can always ﬁnd a feasible policy in the estimated gradient direction  we need to deal with cases in
which the initial policy is not feasible  or we have never seen a feasible policy before.
Inspired by stochastic optimization methods based on the cross-entropy (CE) concept [11]  we
propose a new safe reinforcement learning algorithm  which we call the constrained cross-entropy
(CCE) method. The basic framework is the same with standard CE methods: In each iteration  we
sample from a distribution of policies  select a set of elite sample policies and use them to update the
policy distribution. Rather than treating the constraints as an extra term in the objective function as
what policy gradient method do  we use constraint values to sort sample policies. If there are not
enough feasible sample policies  we select only those with the best constraint performance as elite
sample policies. If a given proportion of the sample policies are feasible  we select the feasible sample
policies with the best objective values as elite sample policies. Instead of initializing the optimization
with a feasible policy  the method improves both the objective function and the constraint function
with the constraint as a prioritized concern.
Our algorithm can be used as a black-box optimizer. It does not even assume that there is an
underlying reward or cost function encoding the optimization objective and constraint functions.
In fact  the algorithm can be applied to any ﬁnite-horizon problem (say  with horizon N) whose
objective and constraint functions are deﬁned as the average performance over some distribution of
trajectories. For example  a constraint function can be the probability that the agent satisﬁes a given
task speciﬁcation (which may be Markovian or non-Markovian) with policy πθ  if the satisfaction
of the given task can be decided with any N-step trajectory. An optimization objective may be the
expected number of steps before the agent reaches a goal state  or the expected maximum distance the
agent has left from its origin  or the expected minimum distance between the agent and any obstacle
over the whole trajectory.
Our contributions are as follows. First  we present a model-free constrained RL algorithm that works
with continuous state and action spaces. Second  we prove that the asymptotic behavior of our
algorithm can be almost-surely described by that of an ordinary differential equation (ODE)  which is
easily interpretable with respect to the objectives. Third  we give sufﬁcient conditions on the properties
of this ODE to guarantee the convergence of our algorithm. We show with numerical experiments
that  our algorithm converges to feasible policies in all our experiments with all combinations of
feasible or infeasible initial policies  Markovian or non-Markovian objectives and constraints  while
other policy-gradient-based algorithms fail to ﬁnd strictly feasible solutions.

2 Related Work

Safety has long been concerned in RL literature and is formulated as various criteria [7]. We choose
to take the so-called constrained criterion [7] to encode our safety requirement  which is the same as
in the literature of constrained Markov decision processes (CMDP) [2]. Approaches are still limited
for safe RL with continuous state and action spaces. Uchibe and Doya [34] proposed a constrained
policy gradient reinforcement learning algorithm  which relies on projected gradients to maintain
feasibility. The computation of projection restricts the types of constraints it can deal with  and there
is no known guarantee on convergence. Chow et al. [4] came up with a trajectory-based primal-dual
subgradient algorithm for a risk-constrained RL problem with ﬁnite state and action spaces. The
algorithm is proved to converge almost-surely to a local saddle point. However  the constraints are

2

just implicitly considered by updating dual variables and the output policy may not actually satisfy the
constraints. Recently  Achiam et al. [1] proposed a trust region method for CMDP called constrained
policy optimization (CPO)  which can deal with high-dimensional policy classes such as neural
networks and claim to maintain feasibility if started with a feasible solution. However  we found in
Section 5 that feasibility is rarely guaranteed during learning in practice  possibly due to errors in
gradient and Hessian matrix estimation.
Cross-entropy-based stochastic optimization techniques have been applied to a series of RL and
optimal control problems. Mannor et al. [18] used cross-entropy methods to solve a stochastic
shortest-path problem on ﬁnite Markov decision processes  which is essentially an unconstrained
problem. Szita and Lörincz [33] took a noisy variant to learn how to play Tetris. Kobilarov [14]
introduced a similar technique to motion planning in constrained continuous-state environments
by considering distributions over collision-free trajectories. Livingston et al. [17] generalized this
method to deal with a broader class of trajectory-based constraints called linear temporal logic
speciﬁcations. Both methods simply discard all sample trajectories that violate the given constraints 
and thus their work can be considered as a special case of our work when the constraint function has
binary outputs. Similar applications in approximate optimal control with constraints can be found in
[23; 6; 16].

3 Preliminaries
For a set B  let D(B) be the set of all probability distributions over B  int(B) be the interior of B
and Bk := {s0  s1  . . .   sk−1 | st ∈ B  ∀t = 0  . . .   k − 1} be the set of all sequences composed by
elements in B of length k for any k ∈ N+.
A (reward-free) Markov decision process (MDP) is deﬁned as a tuple (S A  T  P0)  where S is a set
of states  A is a set of actions  T : S ×A → D(S) is a transition distribution function and P0 ∈ D(S)
is an initial state distribution. Let Π : S → D(A) be the set of all stationary policies. Given a
ﬁnite horizon N  an N-step trajectory is a sequence of N state-action pairs. Each stationary policy
π ∈ Π decides a distribution over N-step trajectories such that the probability to draw a trajectory
t=0 π(at|st). Without
loss of generality  we assume that N is ﬁxed and use Pπ to represent Pπ N .
An objective function J : (S × A)N → R is a mapping from each N-step trajectory to a scalar value.
For each π ∈ Π  let
be the expected value of J with the N-step trajectory distribution decided by π. A policy π ∈ Π is an
optimal policy in Π with respect to J if GJ (π) = maxπ(cid:48)∈Π GJ (π(cid:48)).
A cost function Z : (S × A)N → R is also a function deﬁned on N-step trajectories. Let

τ = s0  a0  . . .   sN−1  aN−1 is Pπ N (τ ) = P0(s0)(cid:81)N−2

t=0 T (st+1|st  at)(cid:81)N−1

GJ (π) := Eτ∼Pπ [J(τ )]

HZ(π) := Eτ∼Pπ [Z(τ )]

be the expected cost over trajectory distribution Pπ. A policy π ∈ Π is feasible for a constrained
optimization problem with cost function Z and constraint upper bound d if HZ(π) ≤ d. Let ΠZ d be
the set of all feasible policies.
For notational simplicity  we omit J and Z in GJ and HZ whenever there is no ambiguity. For any
policy π ∈ Π  we refer to G(π) and H(π) as the G-value and H-value of π.

4 Constrained Cross-Entropy Framework

4.1 Problem Formulation

In this paper  we consider a ﬁnite-horizon RL problem with a strictly positive objective function
J : (S × A)N → R+  a cost function Z : (S × A)N → R and a constraint upper bound d. For
MDPs with continuous state and action spaces  it is usually intractable to exactly solve an optimal
stationary policy due to the curse of dimensionality. An alternative is to use function approximators 
such as neural networks  to parameterize a subset of policies. Given a parameterized class of policies
ΠΘ with a parameter space Θ ⊆ Rdθ  we aim to solve the following problem:

(cid:84) ΠZ d
π∗ = arg max

π∈ΠΘ

GJ (π).

3

The proposed algorithm  which we call the constrained cross-entropy method  generalizes the well-
known cross-entropy method [18] for unconstrained optimization. The basic idea is to generate a
sequence of policy distributions that eventually concentrates on a feasible (locally) optimal policy.
Given a distribution over ΠΘ  we randomly generate a set of sample policies  sort them with a ranking
function that depends on their G-values and H-values and then update the policy distribution with a
subset of high-ranking sample policies.
Given the policy parameterization ΠΘ  we use distributions over the parameter space Θ to represent
distributions over the policy space ΠΘ. We focus ourselves on a speciﬁc family of distributions
over Θ called natural exponential family (NEF)  which includes many useful distributions such as
Gaussian distribution and Gamma distribution. A formal deﬁnition of NEF is as follows.
Deﬁnition 4.1. A parameterized family FV = {fv ∈ D(Θ)  v ∈ V ⊆ Rdv} is called a natural
exponential family if there exist continuous mappings Γ : Rdθ → Rdv and K : Rdθ → R such that

Γ(θ) − K(v)(cid:1)  where V ⊆ {v ∈ Rdv : |K(v)| < ∞} is the natural parameter

fv(θ) = exp(cid:0)v
space and K(v) = log(cid:82)

Γ(θ)(cid:1)dθ.

(cid:124)

Θ exp(cid:0)v

(cid:124)

As with other CE-based algorithms  we replace the original objective G(πθ) = Eτ∼Pπθ
[J(τ )] with
a surrogate function. For the unconstrained CE method  the surrogate function is the conditional
expectation over policies whose G-values are highly ranked with the current sampling distribution
fv. The ranking function is deﬁned using the concept of ρ-quantiles for random variables  which is
formally deﬁned as below.
Deﬁnition 4.2. [10] Given a distribution P ∈ D(R)  ρ ∈ (0  1) and a random variable X ∼ P   the
ρ-quantile of X is deﬁned as a scalar γ such that P r(X ≤ γ) ≥ ρ and P r(X ≥ γ) ≥ 1 − ρ.
For ρ ∈ (0  1)  v ∈ V and any function X : Θ → R  we denote the ρ-quantile of X for θ ∼ fv by
ξX (ρ  v). We also deﬁne δ : R × {≥ ≤  >  <  =} × R → {0  1} as an indicator function such that
for ◦ ∈ {≥ ≤  >  <  =}  δ(x ◦ y) = 1 if and only if x ◦ y holds. The surrogate objective function for
the unconstrained CE method is Eθ∼fv [G(πθ)δ(G(πθ) ≥ ξG(1 − ρ  v))]. In other words  a policy
πθ is considered as highly ranked if G(πθ) ≥ ξG(1 − ρ  v). When there is a constraint H(π) ≤ d 
we deﬁne U : ΠΘ → R such that U (πθ) := G(πθ)δ(H(πθ) ≤ d) for any θ ∈ Θ and extend the
surrogate function as follows:

(cid:26)Eθ∼fv [G(πθ)δ(H(πθ) ≤ ξH (ρ  v))] 

L(v; ρ) :=

if ξH (ρ  v) > d;
otherwise.
We can combine the two cases. Deﬁne S : ΠΘ × V × (0  1) → {0  1} such that

Eθ∼fv [U (πθ)δ(U (πθ) ≥ ξU (1 − ρ  v))] 

S(πθ  v  ρ) :=δ(ξH (ρ  v) > d)δ(H(πθ) ≤ ξH (ρ  v))+

δ(ξH (ρ  v) ≤ d)δ(H(πθ) ≤ d)δ(U (πθ) ≥ ξU (1 − ρ  v)) 

then (1) can be rewritten as

L(v; ρ) = Eθ∼fv [G(πθ)S(πθ  v  ρ)].

(1)

(2)

The interpretation of L is as follows: If the ρ-quantile of H for the current policy distribution fv is
greater than the constraint threshold d  we select policies in ΠΘ by their H-values in order to increase
the probability of drawing feasible policies. Consequently  πθ is highly ranked if H(πθ) ≤ ξH (ρ  v).
If the proportion of feasible policies is higher than ρ  we select policies that are both feasible and with
large objective values  i.e.  πθ is highly ranked if H(πθ) ≤ d and U (πθ) ≥ ξU (1 − ρ  v). Intuitively 
S can be considered as the indicator function of the highly-ranked or elite samples.
Remark 1. By maximizing U  we implicitly prioritizes feasibility over the G objective: For any
feasible policy π and infeasible policy π(cid:48)  it can be easily veriﬁed that U (π) ≥ U (π(cid:48))  as G and U
are non-negative by deﬁnition.
Remark 2. If ξH (ρ  v) ≤ d  then G(πθ)δ(G(πθ) ≥ ξG(1 − ρ  v)) ≥ U (πθ)δ(U (πθ) ≥ ξU (1 −
ρ  v)) ≥ G(πθ)δ(H(πθ) ≤ ξH (ρ  v)). Intuitively  if at least 100ρ% of all policies are feasible 
L(v; ρ) is less than the objective value for the unconstrained CE method and greater than the expected
G-value over the 100ρ% policies of the highest H-values.

The main problem we solve in this paper can be then stated as follows.

4

Simulate πθi and estimate G(πθi)  H(πθi).

parameterized policies ΠΘ  an NEF family FV.

Sample θ1  . . .   θnl ∼ fvl i.i.d..
for i = 1  . . .   nl do

Algorithm 1 Constrained Cross-Entropy Method
Require: An objective function G  a constraint function H  a constraint upper bound d  a class of
1: l ← 1. Initialize nl  vl  ρ  λl  αl. kl ← (cid:100)ρnl(cid:101). ˆηl ← 0.
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:

(cid:12)(cid:12) H(πθi ) ≥ d} in descending order of G. Let Λl be the ﬁrst kl elements.
(cid:1).
(cid:80)

end for
Sort {θi}nl
i=1 in ascending order of H. Let Λl be the ﬁrst kl elements.
) ≤ d then
if H(πθkl
Sort {θi
(cid:80)
end if
ˆηl+1 ← αl
vl+1 ← m−1(ˆηl+1).
Update nl  λl  αl. l ← l + 1. kl ← (cid:100)ρnl(cid:101).

G(πθ) Γ(θ) + (1 − αl)(cid:0) λl

(cid:80)nl
i=1 Γ(θi) + (1 − λl)ˆηl

12:
13:
14: until Stopping rule is satisﬁed.

G(πθ)
θ∈Λl

θ∈Λl

nl

Problem 1. Given a set Π = {πθ : θ ∈ Θ} of policies with parameter space Θ  an NEF FV =
{fv ∈ D(Θ) : v ∈ V} of distributions over Θ  two functions G : Π → R+ and H : Π → R  a
constraint upper bound d and ρ ∈ (0  1)  compute v∗ ∈ V such that

v∈V
where L : V × (0  1) → R is deﬁned in (1) or (2).

v∗ = arg max

L(v; ρ) 

4.2 The Constrained Cross-Entropy Algorithm

The pseudocode of the constrained cross-entropy algorithm is given in Algorithm 1. We ﬁrst explain
the basic ideas behind the updates in Algorithm 1  and provide a proof of convergence in Section 4.3.
We ﬁrst describe the key idea behind the (idealized) CE-based stochastic optimization method as in
[12]. For notational simplicity  we use Ev[·] to represent Eθ∼fv [·] in the rest of this paper. Deﬁne
m(v) := Ev[Γ(θ)] ∈ Rdv for v ∈ V  which is continuously differentiable in v and ∂
∂v m(v) =
Covv[Γ(θ)] where Covv[Γ(θ)] denotes the covariance matrix of Γ(θ) with θ ∼ fv. We take
Assumption 1 to guarantee that m−1 exists and is continuously differentiable over {η : ∃ v ∈
int(V) s.t. η = m(v)} (see Lemma 1 in supplemental material).
Assumption 1. Covv[Γ(θ)] is positive deﬁnite for any v ∈ V ⊆ int({v ∈ Rdv : |K(v)| < ∞}).
By deﬁnition of ρ-quantiles  it is a rare event to sample the highly ranked policies for small ρ. Thus
we apply importance sampling to estimate L(v; ρ) using any sampling distribution g that shares the
same support Θ as fv  among which the optimal distribution g∗
G(πθ)S(πθ  v  ρ)fv(θ)

(3)
In practice we smoothen the updates by including a learning rate α ∈ (0  1) so the goal distribution is
v +(1−α)fv. We can project ˜gv to fv(cid:48) ∈ FV by minimizing the Kullback-Leibler (KL) diver-
˜gv = αg∗
gence of fv(cid:48)(cid:48) ∈ FV from ˜gv  which is equivalent to minimizing the cross entropy between ˜gv and fv(cid:48)(cid:48).
If FV is an NEF  log fv(cid:48)(cid:48) (θ) = (v(cid:48)(cid:48))
Θ ˜gv(θ) log fv(cid:48)(cid:48) (θ)dθ
is convex in v(cid:48)(cid:48). As a result  v(cid:48) can be found by setting ∂
= 0  which
∂v(cid:48)(cid:48)
induces

Γ(θ) − K(v(cid:48)(cid:48)) is concave in v(cid:48)(cid:48); thus −(cid:82)

v [25] with minimal variance is

Θ ˜gv(θ) log fv(cid:48)(cid:48) (θ)dθ

g∗
v(θ) =

L(v; ρ)

(cid:17)

(cid:124)

.

m(v(cid:48)) − m(v) =α(cid:0)Eg∗
(cid:16) ∂
As a property of NEF  the KL-divergence of fv from g satisﬁes ∂
Therefore
∂v(cid:48)(cid:48) DKL(g∗

m(v(cid:48)) − m(v) = −α

v

(cid:16) −(cid:82)
[Γ(θ)] − m(v)(cid:1).
(cid:17)(cid:12)(cid:12)(cid:12)v(cid:48)(cid:48)=v

v  fv(cid:48)(cid:48))

(4)
∂v DKL(g  fv) = −Eg[Γ(θ)]+m(v).

 

(5)

5

− m(v)

L(v; ρ)
fv(θ)(Γ(θ) − m(v))dθ

˜L(v; ρ) =

(cid:90)
(cid:90)

=

(∗)
=

=

Θ

L(v; ρ)

G(πθ)S(πθ  v  ρ)

(cid:0) ∂

G(πθ)S(πθ  v  ρ)

fv(θ)(cid:1)dθ
(cid:12)(cid:12)(cid:12)v(cid:48)(cid:48)=v
∂v(cid:48)(cid:48) log Ev(cid:48)(cid:48) [G(πθ)S(πθ  v  ρ)]

L(v; ρ)

Θ
∂

∂v

 

(∗∗)
=

∂
∂v(cid:48)(cid:48)

Ev(cid:48)(cid:48) [G(πθ)S(πθ  v  ρ)]

L(v; ρ)

(cid:12)(cid:12)(cid:12)v(cid:48)(cid:48)=v

(6)

v  fv) where g∗

v is the optimal sampling distribution from importance sampling.

which conﬁrms that m(v) is always updated in the negative gradient direction of the objective
function DKL(g∗
Remark 3. The equality in (5) holds not just for the optimal distribution g∗
distribution.
Deﬁne ˜L(v; ρ) := Eg∗

[Γ(θ)] − m(v). If G has a strictly positive lower bound and is bounded  then
Ev[G(πθ)S(πθ  v  ρ)Γ(θ)]

v but for any reference

v

(cid:12)(cid:12)(cid:12)v(cid:48)(cid:48)=v

∂v fv(θ) = fv(θ)(Γ(θ) − m(v)) and the (∗∗) step holds by the

where the (∗) step holds by noticing ∂
dominated convergence theorem. Combining (4) and (6)  we get
∂v(cid:48)(cid:48) log Ev(cid:48)(cid:48) [G(πθ)S(πθ  v  ρ)]

m(v(cid:48)) − m(v) = α

∂

= α ˜L(v; ρ) 

(7)
which leads to the second interpretation of the updates: The update from v to v(cid:48) approximately
follows the gradient direction of log L(v(cid:48)(cid:48); ρ)  while the quantiles are estimated using the previous
distribution fv.
Algorithm 1 essentially takes the above updates in (3) and (4) in each iteration  with all expectations
and quantiles estimated by Monte Carlo simulation. Given fvl ∈ D(Θ) in the lth iteration  we sample
over policies (Step 3)  evaluate their G-values and H-values (Step 5)  estimate S(·  v  ρ) (Step 7 to
10) and estimate m(vl+1) with ˆηl+1 (Step 11) and ﬁnally update the sampling distribution to vl+1
(Step 12).

4.3 Convergence Analysis
We prove the convergence of Algorithm 1 by comparing the asymptotic behavior of {ˆηl}l≥0 with the
ﬂow induced by the following ordinary differential equation (ODE):

∂η(t)

∂t

= ˜L(m−1(η(t)); ρ) 

(8)

where we deﬁne η := m(v) or equivalently  v = m−1(η).
We need a series of assumptions for technical reasons.
Assumption 2.

(2a) ˜L(v; ρ) is continuous in v ∈ int(V) and (8) has a unique integral curve

for any given initial condition.

is positive and decreasing with liml→∞ αl = 0 (cid:80)∞

(2b) The number of samples in the lth iteration is nl = Θ(lβ)  β > 0. The gain sequence {αl}
l=1 αl = ∞. {λl} satisﬁes λl = O( 1
lλ )
(2c) For any ρ ∈ (0  1) and fv for any v ∈ V  the ρ-quantile of {H(πθ) : θ ∼ fv} and the

for some λ > 0 such that β + 2λ > 1.

(1 − ρ)-quantile of {U (πθ) : θ ∼ fv} are both unique.

(2d) Both Θ and V are compact.
(2e) The function G deﬁned in Problem 1 is bounded and has a positive lower bound:

inf π∈Π G(π) > 0. The function H in Problem 1 is bounded.

(2f) vl ∈ int(V) for any iteration l.

Assumption (2a) ensures that (8) is well-posed and has a unique solution. Assumption (2b) addresses
some requirements on the number of sampled policies in each iteration and other hyperparameters

6

(cid:80)

∂v
∂t

=

.

(9)

θ∈Λl

in Algorithm 1. Assumptions (2c) to (2e) are used in the proof of the convergence of Algorithm 1.
Assumption (2c) is required to show that 1
G(πθ) in Step 11 of Algorithm 1 is an unbiased
nl
estimate of Evl [G(πθ)S(πθ  vl  ρ)]. Assumption (2d) and (2e) are compactness and boundedness
constraints for the sets and functions involved in Algorithm 1  which are unlikely to be restrictive in
practice. Assumption (2f) states that V is large enough such that the learned v lies within its interior.
The main result that connects the asymptotic behavior of Algorithm 1 with that of an ODE is stated
in Theorem 4.1. The main idea behind the proof of Theorem 4.1 is similar to that of Theorem 3.1 in
[12]  although the details are tailored to our problem. There are two major parts in the convergence
proof: The ﬁrst part shows that all the sampling-based estimates converge to the true values almost
surely  including sample quantiles and sample estimates of G  H and L. The second part shows that
the asymptotic behavior of the idealized updates in (4) can be described by the ODE (8). A detailed
proof of Theorem 4.1 is shown in the supplemental material.
Theorem 4.1. If Assumptions 1 and 2 hold  the sequence {ˆηl}l≥0 in Step 11 of Algorithm 1 converges
to a connected internally chain recurrent set of (8) as l → ∞ with probability 1.
By deﬁnition of η in (8)  we know ∂η(t)
∂t = ∂v
Assumption 1  (8) can be rewritten with variable v

∂t · Covv[Γ(θ)]. Since Covv[Γ(θ)] is invertible by

(cid:16) ˜L(v; ρ)

(cid:17)(cid:124)(cid:0)Covv[Γ(θ)](cid:1)−1

The conclusion of Theorem 4.1 can be equivalently stated in terms of the variable v: the sequence
{vl}l≥0 of Algorithm 1 converges to a connected internally chain recurrent set of (9) as l → ∞ with
probability 1.
Intuitively  a point v0 ∈ V is chain recurrent for (9) if the solution v(t) of (9) with initial condition
v(0) = v0 can return to v0 within some ﬁnite time t(cid:48) > 0 itself or just with ﬁnitely many arbitrarily
small perturbations. An internally chain recurrent set is a nonempty compact invariant set of
chain-recurrent points  i.e.  v can never leave an internally chain recurrent set if v0 belongs to it.
Theorem 4.1 implies that with probability 1  the set of points that occur inﬁnitely often in {vl}l≥0
are internally chain recurrent for (9). Since fv belongs to NEF  Covv[Γ(θ)] is the Fisher information
matrix at v and the right hand side of (9) is an estimate of the natural gradient of log L(v; p) with
a ﬁxed indicator function S. This suggests that v evolves to increase L(v; ρ)  which is consistent
with the optimization problem (1) and our motivation to solve a constrained RL problem. Note that
internally chain-recurrent sets are generally not unique and our algorithm can still converge to a local
optimum.
To further interpret Theorem 4.1  we ﬁrst note that any equilibrium of (8) forms an internally chain
recurrent set by itself. The following result shows a sufﬁcient condition for an equilibrium point ¯v∗
of (8) to be locally asymptotically stable  i.e.  there is a small neighborhood of this equilibrium of ¯v∗
such that once entered  (9) will converge to ¯v∗.
Theorem 4.2. Let ϕ : V → R be any function such that ∂
¯v∗ ∈ int(V) of (9) that is an isolated local maximum of ϕ(v) is locally asympototically stable.
The proof of Theorem 4.2 is done by constructing a local Lyapunov function and is given in the
supplemental material. It also shows that ϕ(v) always decreases in the interior of V unless it hits a
stationary point of (9)  which suggests a stronger property of our algorithm as stated in Theorem 4.3.
Theorem 4.3. If all equilibria of (9) are isolated  the sequence {vl}l≥0 derived by Algorithm 1
converges toward an equilibrium of (9) as l → ∞ with probability 1.

∂v ϕ(v) = ˜L(v; ρ). Any equilibrium

5 Experimental Results
We consider a mobile robot navigation task with only local sensors. There is a compact goal region G
and a non-overlapping compact bad region B in the robot’s environment. The transition function is
deterministic. The robot uses a local sensing model to observe if B or G is in its neighborhood and
the direction of the center of G in its local coordinate. Details of this experiment and the local sensing
model can be found in the supplemental material.
We compare the performance of CCE to trust region policy optimization (TRPO) [27]  a state-of-
the-art unconstrained RL algorithm  and its variant for constrained problems  i.e.  CPO [1]. For all

7

Objective value GJi(πθ)

Constraint value HZi(πθ) (Feasible regions are below dashed lines)

(a) i = 1

(b) i = 2

(c) i = 3

(d) i = 4

(e) i = 1

Figure 1: Learning curves of CCE  CPO and TRPO with different objectives GJi and constraints
HZi. The x-axes show the total number of sample trajectories for CCE and the total number of
equivalent sample trajectories for TRPO and CPO. The y-axes show the sample mean of the objective
and constraint values of the learned policy (for TRPO and CPO) or the learned policy distribution
(for CCE). Each experiment is repeated for 5 times. More details can be found in the supplemental
material.

Table 1: Ji(τ )  Zi(τ ) and constraint upper bound di for i = 1  2  3  4  τ ∈ (S × A)N .

i

1

2

3

4

Ji(τ )

1 for each state in G; 2|y| for
y ∈ [−2 −0.2]; 0 otherwise.

each state with

30 times the minimum
signed distance from any

state in τ to B.

Same as J2(τ ).

Same as J1(τ ).

Zi(τ )

di

Ji Markovian Zi Markovian

-1 if the robot arrives
G which is absorbing;

0 otherwise.

-1 if the robot visited
G in τ; 0 otherwise.
-1 for each state in G;

0 otherwise.

-1 if the robot visits
G and never visits B;

0 otherwise.

-0.5

Yes

-0.5

-5

No

No

-0.5

Yes

Yes

No

Yes

No

experiments  the agent’s policy is modeled as a fully connected neural network with two hidden layers
with 30 nodes in each layer. Trajectory length for all experiments is set to N = 30. All experiments
are implemented in rllab [5].
Figure 1 shows the learning curves of CCE 
TRPO and CPO for four different objectives
and constraints (i = 1  2  3  4). The objective
functions and constraint functions used in each
experiment are interpreted in Table 1. For exper-
iments in which Ji is not strictly positive  we use
exp(Ji) instead of Ji for the CCE. TRPO results
show that the constraints cannot be satisﬁed by
merely optimizing the corresponding objectives.
We ﬁrst initialize each experiment with a ran-
domly generated infeasible policy. We ﬁnd that
CCE successfully outputs feasible policies in all

Figure 2: Average performance of CCE  CPO and
TRPO for Experiment 4 with initial feasible policy.

(b) HZ4 (πθ).

(a) GJ4 (πθ).

8

experiments. On the other hand  CPO needs signiﬁcantly more samples to ﬁnd a single feasible
policy  or simply converges to an infeasible policy especially if the constraint is non-Markovian.
We repeat the ﬁrst experiment (i = 1) with feasible initial policies and obtain the result in the last
column of Figure 1. In this case  CPO leaves the feasible region rapidly and then follows generally the
same path as if it is initialized with an infeasible policy. This behavior suggests that its incapability to
enforce constraint satisfaction is not due to the lack of initial feasibility. Although CCE also leaves
the feasible region at an early stage of iterations  it regains feasibility much faster than the previous
case with infeasible initial polices. These results suggest that CCE is more reliable than CPO for
applications where the strict constraint satisfaction is critical.
In Figure 2  we compare the performance of CPO and CCE in Experiment 4 to that of TRPO with
objective GJ4 − 100HZ4. Due to the non-Markovian nature of Z4  HZ4(π) is not sensitive to local
changes in π(s) at any state s. It therefore makes it more difﬁcult for standard RL algorithms to
improve its HZ4-value. The ﬁxed penalty coefﬁcient 100 is chosen to be neither too large nor too
small so it can show a large variety of locally optimal behaviors with very different GJ4-values and
HZ4-values. Figure 2 clearly shows the trade-off between GJ4-values and HZ4-values  which partially
explains the gap between GJ4-value outputs of CCE and CPO. With a ﬁxed penalty coefﬁcient  the
policies learned by TRPO are either infeasible or with very small constraint values. The policy output
by CCE has higher GJ4-value than all feasible policies found by TRPO and CPO.

6 Conclusions and Future Work

In this work  we studied a safe reinforcement learning problem with the constraints that are deﬁned
as the expected cost over ﬁnite-length trajectories. We proposed a constrained cross-entropy-based
method to solve this problem  analyzed its asymptotic performance using an ODE and proved
its convergence. We showed with simulation experiments that our method can effectively learn
feasible policies without assumptions on the feasibility of initial policies with both Markovian and
non-Markovian objective functions and constraint functions.
CCE is expected to be less sample-efﬁcient than gradient-based methods especially for high-
dimensional systems. Unlike gradient-based methods such as TRPO  CCE does not infer the
performances of unseen policies from previous experience. As a result  it has to repetitively sample
good policies in order to make steady improvement. Meanwhile  CCE can be easily parallelized
as each sampled policy is evaluated independently. This may mitigate the problem of high sample
complexity as other evolutionary methods [26].
Given all these limitations  we ﬁnd the CCE method to be particularly useful in learning hierarchical
policies. With a high-level policy that speciﬁes intermediate goals and thus reduces the state space
for low-level policies  we can use CCE to train a (locally) optimal low-level policy while satisfying
local constraints. As shown in the experiment of our paper  CCE converges with reasonable sample
complexity and outperforms CPO on its constraint performance. Since the satisfaction of low-
level constraints is of critical signiﬁcance to the performance of the overall policy  CCE seems to
be especially well-suited for this application. In future work  we will combine this method with
off-policy policy evaluation techniques such as [13; 9] to improve sample complexity.

Acknowledgement

This work was supported in part by ONR N000141712623  ARO W911NF-15-1-0592 and DARPA
W911NF-16-1-0001.

References
[1] J. Achiam  D. Held  A. Tamar  and P. Abbeel. Constrained policy optimization. In International

Conference on Machine Learning  pages 22–31  2017.

[2] E. Altman. Asymptotic properties of constrained markov decision processes. Mathematical

Methods of Operations Research  37(2):151–170  1993.

[3] D. P. Bertsekas. Dynamic Programming and Optimal Control  Vol. II. Athena Scientiﬁc  3rd

edition  2007. ISBN 1886529302  9781886529304.

9

[4] Y. Chow  M. Ghavamzadeh  L. Janson  and M. Pavone. Risk-constrained reinforcement learning

with percentile risk criteria. arXiv preprint arXiv:1512.01629  2015.

[5] Y. Duan  X. Chen  R. Houthooft  J. Schulman  and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In International Conference on Machine Learning  pages 1329–
1338  2016.

[6] J. Fu  I. Papusha  and U. Topcu. Sampling-based approximate optimal control under temporal
logic constraints. In Proceedings of the 20th International Conference on Hybrid Systems:
Computation and Control  pages 227–235. ACM  2017.

[7] J. Garcıa and F. Fernández. A comprehensive survey on safe reinforcement learning. Journal of

Machine Learning Research  16(1):1437–1480  2015.

[8] S. Gu  T. Lillicrap  I. Sutskever  and S. Levine. Continuous deep q-learning with model-
In M. F. Balcan and K. Q. Weinberger  editors  Proceedings of The
based acceleration.
33rd International Conference on Machine Learning  volume 48 of Proceedings of Machine
Learning Research  pages 2829–2838  New York  New York  USA  20–22 Jun 2016. PMLR.
URL http://proceedings.mlr.press/v48/gu16.html.

[9] J. P. Hanna  P. Stone  and S. Niekum. Bootstrapping with models: Conﬁdence intervals for
In Proceedings of the 16th Conference on Autonomous Agents and
off-policy evaluation.
MultiAgent Systems  pages 538–546. International Foundation for Autonomous Agents and
Multiagent Systems  2017.

[10] T. Homem-de Mello. A study on the cross-entropy method for rare-event probability estimation.

INFORMS Journal on Computing  19(3):381–394  2007.

[11] J. Hu  M. C. Fu  S. I. Marcus  et al. A model reference adaptive search method for stochastic

global optimization. Communications in Information and Systems  8(3):245–276  2008.

[12] J. Hu  P. Hu  and H. S. Chang. A stochastic approximation framework for a class of randomized

optimization algorithms. IEEE Transactions on Automatic Control  57(1):165–178  2012.

[13] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In

International Conference on Machine Learning  pages 652–661  2016.

[14] M. Kobilarov. Cross-entropy randomized motion planning. In Robotics: Science and Systems 

2011.

[15] S. Levine  C. Finn  T. Darrell  and P. Abbeel. End-to-end training of deep visuomotor policies.

The Journal of Machine Learning Research  17(1):1334–1373  2016.

[16] L. Li and J. Fu. Sampling-based approximate optimal temporal logic planning. In Robotics and
Automation (ICRA)  2017 IEEE International Conference on  pages 1328–1335. IEEE  2017.

[17] S. C. Livingston  E. M. Wolff  and R. M. Murray. Cross-entropy temporal logic motion planning.
In Proceedings of the 18th International Conference on Hybrid Systems: Computation and
Control  pages 269–278. ACM  2015.

[18] S. Mannor  R. Rubinstein  and Y. Gat. The cross entropy method for fast policy search. In In

International Conference on Machine Learning  pages 512–519. Morgan Kaufmann  2003.

[19] V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Riedmiller.

Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602  2013.

[20] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves 
M. Riedmiller  A. K. Fidjeland  G. Ostrovski  et al. Human-level control through deep rein-
forcement learning. Nature  518(7540):529–533  2015.

[21] V. Mnih  A. P. Badia  M. Mirza  A. Graves  T. Lillicrap  T. Harley  D. Silver  and K. Kavukcuoglu.
In International Conference on

Asynchronous methods for deep reinforcement learning.
Machine Learning  pages 1928–1937  2016.

10

[22] W. Montgomery  A. Ajay  C. Finn  P. Abbeel  and S. Levine. Reset-free guided policy search:
Efﬁcient deep reinforcement learning with stochastic initial states. In Robotics and Automation
(ICRA)  2017 IEEE International Conference on  pages 3373–3380. IEEE  2017.

[23] I. Papusha  J. Fu  U. Topcu  and R. M. Murray. Automata theory meets approximate dynamic
programming: Optimal control with temporal logic constraints. In Decision and Control (CDC) 
2016 IEEE 55th Conference on  pages 434–440. IEEE  2016.

[24] I. Popov  N. Heess  T. Lillicrap  R. Hafner  G. Barth-Maron  M. Vecerik  T. Lampe  Y. Tassa 
T. Erez  and M. Riedmiller. Data-efﬁcient deep reinforcement learning for dexterous manipula-
tion. arXiv preprint arXiv:1704.03073  2017.

[25] R. Y. Rubinstein and B. Melamed. Modern simulation and modeling  volume 7. Wiley New

York  1998.

[26] T. Salimans  J. Ho  X. Chen  S. Sidor  and I. Sutskever. Evolution strategies as a scalable

alternative to reinforcement learning. arXiv preprint arXiv:1703.03864  2017.

[27] J. Schulman  S. Levine  P. Abbeel  M. Jordan  and P. Moritz. Trust region policy optimization.
In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages
1889–1897  2015.

[28] J. Schulman  P. Moritz  S. Levine  M. Jordan  and P. Abbeel. High-dimensional continuous

control using generalized advantage estimation. arXiv preprint arXiv:1506.02438  2015.

[29] J. Schulman  F. Wolski  P. Dhariwal  A. Radford  and O. Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347  2017.

[30] D. Silver  G. Lever  N. Heess  T. Degris  D. Wierstra  and M. Riedmiller. Deterministic policy
gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14)  pages 387–395  2014.

[31] D. Silver  A. Huang  C. J. Maddison  A. Guez  L. Sifre  G. Van Den Driessche  J. Schrittwieser 
I. Antonoglou  V. Panneershelvam  M. Lanctot  et al. Mastering the game of go with deep neural
networks and tree search. nature  529(7587):484–489  2016.

[32] D. Silver  J. Schrittwieser  K. Simonyan  I. Antonoglou  A. Huang  A. Guez  T. Hubert  L. Baker 
M. Lai  A. Bolton  et al. Mastering the game of go without human knowledge. Nature  550
(7676):354  2017.

[33] I. Szita and A. Lörincz. Learning tetris using the noisy cross-entropy method. Learning  18(12) 

2006.

[34] E. Uchibe and K. Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards.
In Development and Learning  2007. ICDL 2007. IEEE 6th International Conference on  pages
163–168. IEEE  2007.

[35] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. In Reinforcement Learning  pages 5–32. Springer  1992.

[36] I. Zamora  N. G. Lopez  V. M. Vilches  and A. H. Cordero. Extending the openai gym
for robotics: a toolkit for reinforcement learning using ros and gazebo. arXiv preprint
arXiv:1608.05742  2016.

[37] T. Zhang  G. Kahn  S. Levine  and P. Abbeel. Learning deep control policies for autonomous
aerial vehicles with mpc-guided policy search. In Robotics and Automation (ICRA)  2016 IEEE
International Conference on  pages 528–535. IEEE  2016.

11

,Min Wen
Ufuk Topcu