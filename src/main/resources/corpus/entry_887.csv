2016,CRF-CNN: Modeling Structured Information in Human Pose Estimation,Deep convolutional neural networks (CNN) have achieved great success. On the other hand  modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network  there is no message passing between neurons in the same layer. In this paper  we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way  and it is applied to human pose estimation. A message passing scheme is proposed  so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer  and it is also integrated with feedforward propagation in neural networks. Finally  a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.,CRF-CNN: Modeling Structured Information in

Human Pose Estimation

Xiao Chu

The Chinese University of Hong Kong

xchu@ee.cuhk.edu.hk

Hongsheng Li

The Chinese University of Hong Kong

hsli@ee.cuhk.edu.hk

Wanli Ouyang

The Chinese University of Hong Kong

wlouyang@ee.cuhk.edu.hk

Xiaogang Wang

The Chinese University of Hong Kong

xgwang@ee.cuhk.edu.hk

Abstract

Deep convolutional neural networks (CNN) have achieved great success. On
the other hand  modeling structural information has been proved critical in many
vision problems. It is of great interest to integrate them effectively. In a classical
neural network  there is no message passing between neurons in the same layer. In
this paper  we propose a CRF-CNN framework which can simultaneously model
structural information in both output and hidden feature layers in a probabilistic way 
and it is applied to human pose estimation. A message passing scheme is proposed 
so that in various layers each body joint receives messages from all the others
in an efﬁcient way. Such message passing can be implemented with convolution
between features maps in the same layer  and it is also integrated with feedforward
propagation in neural networks. Finally  a neural network implementation of end-
to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through
experiments on two benchmark datasets.

1

Introduction

A lot of efforts have been devoted to structure design of convolutional neural network (CNN). They
can be divided into two groups. One is to achieve higher expressive power by making CNN deeper
[19  10  20]. The other is to model structures among features and outputs  either as post processing
[6  2] or as extra information to guide the learning of CNN [29  22  24]. They are complementary.
Human pose estimation is to estimate body joint locations from 2D images  which could be applied to
assist other tasks such as [4  14  26] The very ﬁrst attempt adopting CNN for human pose estimation
is DeepPose [23]. It used CNN to regress joint locations repeatedly without directly modeling the
output structure. However  the prediction of body joint locations relies both on their own appearance
scores and the prediction of other joints. Hence  the output space for human pose estimation is
structured. Later  Chen and Yuille [2] used a graphical model for the spatial relationship between
body joints and used it as post processing after CNN. Learning CNN features and structured output
together was proposed in [22  21  24]. Researchers were also aware of the importance of introducing
structures at the feature level [3]. However  the design of CNN for structured output and structured
features was heuristic  without principled guidance on how information should be passed. As deep
models are shown effective for many practical applications  researchers on statistical learning and
deep learning try to use probabilistic models to illustrate the ideas behind deep models [9  7  29].
Motivated by these works  we provide a CRF framework that models structures in both output and
hidden feature layers in CNN  called CRF-CNN. It provides us with a principled illustration on how
to model structured information at various levels in a probabilistic way and what are the assumptions

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

made when incorporating different CRF into CNN. Existing works can be illustrated as special
implementations of CRF-CNN. DeepPose [23] only considered the feature-output relationship  and
the approaches in [2  22] considered feature-output and output-output relationships. In contrast  our
proposed full CRF-CNN model takes feature-output  output-output  and feature-feature relationships
into consideration  which is novel in pose estimation.
It also facilitates us in borrowing the idea behind the sum-product algorithm and developing a message
passing scheme so that each body joint receives messages from all the others in an efﬁcient way by
saving intermediate messages. Given a set of body joints as vertices on a graph  there is no conclusion
on whether a tree structured model [28  8] or a loopy structured model [25  16] is the best choice.
A tree structure has exact inference while a loopy structure can model more complex relationship
among vertices. Our proposed message passing scheme is applicable to both.
Our contributions can be summarized as follows. (1) A CRF is proposed to simultaneously model
structured features and structured body part spatial relationship. We show step by step how ap-
proximations are made to use an end-to-end learning CNN for implementing such CRF model. (2)
Motivated by the efﬁcient algorithm for marginalization on tree structures  we provide a message
passing scheme for our CRF-CNN so that every vertex receives messages from all the others in an
efﬁcient way. Message passing can be implemented with convolution between feature maps in the
same layer. Because of the approximation used  this message passing can be used for both tree and
loopy structures. (3) CRF-CNN is applied to two human pose estimation benchmark datasets and
achieve better performance on both dataset compared with previous methods.

2 CRF-CNN

The power of combing powerful statistical models with CNN has been proved [6  3]. In this section
we start with a brief review of CRF and study how the pose estimation problem can be formulated
under the proposed CRF-CNN framework. It includes estimating body joints independently from
CNN features  modeling the spatial relationship of body joints in the output layer of CNN  and
modeling the spatial relationship of features in the hidden layers of CNN.
Let I denote an image  and z = {z1  ...  zN} denote locations of N body joints. We are interested in
modeling the conditional probability p(z|I  Θ) parameterized by Θ  expressed in a Gibbs distribution:

p(z|I  Θ) =

e−En(z I Θ)

Z

=

(cid:80)

e−En(z I Θ)
z∈Z e−En(z I Θ)

 

(1)

where En(Z  I  Θ) is the energy function. The conditional distribution by introducing latent variables
h = {h1  h2  . . .   hK} can be modeled as follows:

p(z|I  Θ) =

p(z  h|I  Θ)  where p(z  h|I  Θ) =

(cid:80)

e−En(z h I Θ)

z∈Z h∈H e−En(z h I Θ)

(2)

(cid:88)

h

En(z  h  I  Θ) is the energy function to be deﬁned later. The latent variables correspond to features
obtained from a neural network in our implementation. We deﬁne an undirected graph G = (V E) 
where V = z ∪ h  E = Ez ∪ Eh ∪ Ezh. Ez  Eh  and Ezh denote sets of edges connecting body joints 
connecting latent variables  and connecting latent variables with body joints  respectively.

2.1 Model 1
Denote ∅ as an empty set. If we suppose there is no edge connecting joints and no edge connecting
latent variables in the graphical model  i.e. Ez = ∅  Eh = ∅  then

p(z  h|I  Θ) =

p(zi|h  I  Θ)

p(hk|I  Θ) 

En(z  h  I  Θ) =

(i k)∈Ezh

k

ψzh(zi  hk) +

φh(hk  I) 

(cid:89)

(cid:88)

k

where φh(∗) denotes the unary/data term for image I  ψzh(∗ ∗) denotes the terms for the correlations
between latent variables h and body joint conﬁgurations z. It corresponds to the model in Fig. 1(a)
and it is a typical feedforward neural network.

(cid:89)
(cid:88)

i

2

(3)

(4)

Figure 1: Different implementations of the CRF-CNN framework.

Example. In DeepPose [23]  CNN features h in the top hidden layer were obtained from images  and
could be treated as latent variables and illustrated by term φh(hk  I) in (4). There is no connection
between neurons in hidden layers. Body joint locations were estimated from CNN features in [23] 
which could be illustrated by the term ψzh(zi  hk). The body joints are independently estimated
without considering their correlations  which means Ez = ∅.

2.2 Model 2
If we suppose Eh = ∅ in the graphical model  p(z  h|I  Θ) becomes

(cid:89)
(cid:88)

k

(i k)∈Ezh

p(z  h|I  Θ) = p(z|h  I  Θ)

p(hk|I  Θ).

(5)

Compared with (3)  joint locations are no longer independent. The energy function for this model is

En(z  h  I  Θ) =

ψz(zi  zj) +

ψzh(zi  hk) +

φh(hk  I).

(6)

(cid:88)

k

(cid:88)

(i j)∈Ez

i<j

It corresponds to the model in Fig. 1(b). Compared with (4)  ψz(zi  zj) in (6) is added to model the
pairwise relationship between joints.
Example. To model the spatial relationship among body joints  the approaches in Yang et al. [22]
built up pairwise terms and spatial models. They are different implementations of ψz(zi  zj) in (6).

2.3 Our model

In our model  h is considered as a set of discrete latent variables and each hk is represented as a
1-of-L L dimensional vector. p(z  h|Θ) and En(z  h  I  Θ) for this model are:

p(z  h|I  Θ) = p(z|h  I  Θ)p(h|I  Θ).

En(z  h  I  Θ) =

ψh(hk  hl) +

ψz(zi  zj) +

(cid:88)

(k l)∈Eh

k<l

(cid:88)

(i j)∈Ez

i<j

(cid:88)

(i k)∈Ezh

(7)

φh(hk  I).

(cid:88)

k

ψzh(zi  hk) +

(8)
It is the model in Fig. 1(c) and exhibits the largest expressive power compared with the models in (4)
and (6). ψh(hk  hl) is added to model the pairwise relationship among features/latent variables in (8).
Details on the set of edges E. Body joints have structures and it may not be suitable to use a fully
connected graph. The tree structure in Fig. 2(b) is widely used since it ﬁts human knowledge on
the skeleton of body joints and how body parts articulate. A further beneﬁt for a tree structure
with N vertices is that all vertices can receive messages from others with 2N message passing
operations. To better deﬁne the structure of latent variables h  we group the latent variables so
that a joint zi corresponds to a particular group of latent variables denoted by hi  and h = ∪ihi.
i=1 ψzh(zi  hi)  i.e. zi is only connected to latent
variables in hi. We further constrain connections among feature groups: (hi  hj) ∈ Eh ⇐⇒

ψzh(zi  hk) in (8) is simpliﬁed into(cid:80)N

(i k)∈Ezh

(cid:80)

3

!"#(a) Multi-layer neural network(c)Structured hidden layer(b) Structured output space(d)Our implementation……vvv……vvv……vvv……vvv$%&$%$&(zi  zj) ∈ Ez. It means that feature groups are connected if and only if their corresponding body
joints are connected. Fig. 1(d) shows an example of this model. Our implementation is as follows:

En(z  h  I  Θ) =

(cid:88)

(i j)∈Eh

i<j

ψh(hi  hj) +

(cid:88)

(i j)∈Ez

i<j

N(cid:88)

i=1

ψz(zi  zj) +

K(cid:88)

k=1

φh(hk  I)  (9)

ψzh(zi  hi) +

Implementation with neural networks

3
In order to marginalize latent variables h and obtain p(z|I  Θ)  the computational complexity of
marginalization in (2) is high  exponentially proportional to the cardinality of h. In order to infer
p(z|I  Θ) in a more efﬁcient way  we use the following approximations:

(cid:88)

(cid:88)

p(z|I  Θ) =

p(z  h|I  Θ) =

p(z|h  I  Θ)p(h|I  Θ) ≈ p(z|˜h  I  Θ) 

h

h

where ˜h = [˜h1  ˜h2  . . .   ˜hN ] = E[h] =

hp(h|I  Θ) 

(10)

(11)

(cid:88)

h

In (10) and (11)  we replace h by its average conﬁguration ˜h = E[h] and this approximation was
also used in greedy layer-wise learning for deep belief net in [11].

p(h|I  Θ) ≈(cid:89)

Q(hi|I  Θ) =

i

1

Zh i

exp

Q(hi|I  Θ) 

− (cid:88)

hk∈hi

φh(hk  I) − (cid:88)

(i j)∈Eh

i<j

 .

ψh(hi  Q(hj|I  Θ))

(12)

(13)

(cid:88)

The target is to marginalize the distribution of h  as shown in 12. We adopt the classical mean-ﬁeld
approximation approach for message passing[15]. p(h|I  Θ) in (11) is approximated by a product of
independent Q(hi|I  Θ) in (12) and (13).
We ﬁrst ignore the pairwise term ψh(hi  hj) which will be addressed later in Section 3.1. Suppose
φh(hk  I) = hkwT
kf  where f is the feature representation of image I. For a binary latent variable hk 

˜hk = E[hk] =

hkQ(hk|I  Θ) = sigm(φh(hk  I)) = sigm(wT

kf ) 

(14)

hk

where sigm(x) = 1/(1 + e−x) is the sigmoid function. Therefore  the mapping from f to ˜h can be
implemented with one-layer transformation in a neural network and sigmoid is the activation function.
˜h is a new feature vector derived from f and f can be obtained from lower layers in a network.

3.1 Message passing on tree structured latent variables
In order to infer p(z|I  Θ)  the key challenge in our framework is to obtain the marginalized distribu-
tion of hidden units  i.e.   Q(hi|I  Θ) in (12). One can obtain Q(hi|I  Θ) through message passing
and further estimate ˜h Then p(z|˜h  I  Θ) in (10) can be estimated with existing works such as [2  28].
According to the sum-product algorithm for a tree structure  every node can receive the messages
from other nodes through two message passing routes  ﬁrst from leaves to a root and then from the
root to the leaves [13]. The key is to have a planned route and to store the intermediate messages.
Our proposed messaging passing algorithm is summarized in Algorithm 1. An example of message
passing for a tree structure with 4 nodes as shown in Fig. 2(c). For detailed illustrations of 2  please
refer to the supplementary material.
We drop I and Θ to be concise.

4

(cid:46) Initialization
(cid:46) Passing messages M times

Uk ← f ⊗ wk  for k = 1 to K
for m = 1 to M do

Select a predeﬁned message passing route Sm
for e = 1 to |Eh| do

Algorithm 1 Message passing among features on factor graph.
1: procedure BELIEF PROPAGATION(Θ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end procedure

Fj→fk ← Uj +(cid:80)
Ffj→k ←(cid:80)
Q(hk) ← τ (Uk +(cid:80)

Qj→fk ← τ (−Fj→k)
Denote the factor node j by fj

p∈par(j)\k Qp→fj ⊗ wp→k

end for
for k = 1 to K do

fp∈ne(k) Ffp→k)

end for

else

end if

end for

Choose an edge (j → k) from Eh according to the route Sm
Denote ne(j) as the set of neighboring nodes for node j on the graphical model
if k is a factor node denoted by fk then
fp∈ne(j)\k Ffp→j

(cid:46) Pass message from factors to variable
(cid:46) Normalize

(cid:46) Pass message to the factor

Figure 2: Message passing. (a) is the annotation of a person with its tree structure. (b) is the tree
structured model employed on the LSP dataset. In (b)  the pink colored nodes are linearly interpolated.
(c d) show message passing on a factored graph with different routes. (e) is a loopy model. In (e)  the
edges in green color are extra edges added on the tree structured model in(b).

According to the mean-ﬁeld approximation algorithm  the above message passing process should
be conducted for multiple times with share parameter to converge. To implement ψh(hi  hj)  we
use matrix multiplication for easier illustration but convolution (which is a special form of matrix
multiplication) for implementation in Algorithm 1. Then message passing is implemented with
convolution between feature maps.
The proposed method is extensible to loopy structured graphs  as shown in Fig. 2(e). The underlying
concept of building up probabilistic model at feature level is the same. However  for loopy structures 
the key challenge is to deﬁne the rule in message passing. Either a sequence of asymmetric message
passing order is predeﬁned  which seems not reasonable for symmetric structure of human poses  or
use the ﬂooding scheme to repeated collect information for neighborhood joints. We compared tree
structure with loop structure with ﬂooding scheme in the experimental section.

3.2 Overall picture of CRF-CNN for human pose estimation

An overview of the approach is shown in Fig. 3. In this pipeline  the prediction on ith body part
∈ [0  1]
conﬁguration zi is represented by a score map p(zi|h) = {˜z(1 1)
denotes the predicted conﬁdence on the existence of the ith body joint at the location (x  y). Similarly 
the group of features ˜hi used for estimating p(z|h) is represented by ˜hi = {˜h(1 1)
  . . .} 

  . . .}  where ˜z(x y)

  ˜h(1 2)

  ˜z(1 2)

i

i

i

i

i

5

ℎ"ℎ#ℎ$ℎ%&'&(&)(a)(b)(c)(d)(e)ℎ"ℎ#ℎ$ℎ%&'&(&)Figure 3: CNN implementation of our model. (1) We use the fc6 layer of VGG to obtain features f
from an image. (2) The features f are then used for passing messages among latent variables h. (3)
Then the estimated latent variables ˜h are used for estimating the predicted body part score maps ˜z.
We only show the message passing process between two joints to be concise. Best viewed in color.

i

i

contains L channels of features at location (x  y).

is a length-L vector. Therefore  the feature group ˜hi is represented by a feature

i = 1  . . .   N. ˜h(x y)
map of L channels  where h(x y)
1) It comprises a fully convolutional network stage  which takes an image as input and outputs
features f. We use the fully convolutional implementation of VGG and the output of fc6 in VGG is
used as the feature map f.
2) Messages are passed among features h with Algorithm 1. Initially  data term Uk for the kth feature
group is obtained from feature map f by convolution  which is our implementation of term φh(hk  I)
in (13) and corresponds to Algorithm 1 line 2. Then CNN is used for passing messages among h
using lines 3-19 in Algorithm 1  which implements term ψh(hi  Q(hj|I  Θ)) in (13) by convolution.
After message passing  the ˜hi for i = 1  . . .   N is obtained and treated as feature maps to be used.
3) Then the feature maps ˜hi for i = 1  . . .   N are used to obtain the score map for in-
ferring p(z|h  I  Θ) with (10). As a simple example for illustration  we can use ˜z(x y)
=
˜h(x y)
wT
for the ith part at
p
i
i
location (x  y). In this case  ˜h(x y)
is the feature with L channels at location (x  y) and wi can be
treated as the classiﬁer. Our implementation uses the approach in [2] to infer p(z|h  I  Θ)  which also
models the spatial relationship among zi.
During training  a whole image (or many of them) can be used as the mini-batch and the error at each
output location of the network can be computed using an appropriate loss function with respect to the
ground truth of the body joints. We use softmax loss with respect to the estimated part conﬁguration
z as the approximate loss function. Since we have used CNN from input to features f  ˜hi and ˜z   a
single CNN is used for obtaining the score map of body joints from the image. End-to-end learning
with softmax loss and standard BP is used.

to obtain the predicted score ˜z(x y)

(cid:16)

i

z(x y)
i

= 1|hi  I

= sigm

i

i

(cid:16)

(cid:17)

(cid:17)

4 Experiment

We conduct experiments on two benchmark datasets: the LSP dataset [12] and the FLIC dataset [18].
LSP contains 2  000 images. 1  000 images for training and 1  000 for test. Each person is annotated
with 14 joints. FLIC contains 3  987 training images and 1  016 testing images from Hollywood
movies with upper body annotated. On both datasets  we use observer centric annotation for training
and evaluation. We also use negative samples  i.e. images not containing any person  from the INRIA
dataset [5]. In summary  we are consistent with Chen et al. [2] in training data preparation.

6

VGGUntil	fc6\{pooling4 5}!"#$%&$%&$'()(%&$ "#$)')("#$ "#-)'((%&$ %&-)…(2)(1)(3).ℎ0$(1 2)(3 4)Channel"#-4.1 Results on the LSP dataset

The experimental results for our and previous approaches on LSP are shown in Table 1. For evaluation
metric  we choose the prevailing evaluation method: strict Percentage of Correct Parts (PCP). Under
this metric  a limb is considered to be detected only if both ends of an limb lie in 50% of the length
w.r.t. the ground-truth location. For pose estimation  it is well known that the accuracy of CNN
features is higher than handcrafted features. Therefore  we only compare with methods that use CNN
features to be concise. Pishchulin et al. [17] use extra training data  so we do not compare with
it. Yang et al. [27] learned features and structured body part conﬁgurations simultaneously. Our
performance is better than them because we model structure among features. Chu et al. [3] learned
structured features and heuristically deﬁned a message passing scheme. Using only the LSP training
data  these two approaches have the highest PCP (Observer-Centric) reported in [1]. The model in
[3] has no probabilistic interpretation and cannot be modeled as CRF. Most vertices in their CNN
can only receive information from half of the vertices  while in our message passing scheme each
node could receive information from all vertices  since it is developed from CRF and the sum-product
algorithm. The approaches in [27  3] are all based on the VGG structure as ours. By using a more
effective message passing scheme  our method reduces the mean error rate by 10%.

Table 1: Quantitative results on LSP dataset (PCP)

Experiment
Chen&Yuille [2]
Yang et al. [27]
Chu et al. [3]
Ours

Torso Head U.arms L.arms U.legs L.legs Mean
75.0
92.7
96.5
81.1
81.1
95.4
83.1
96.0

87.8
83.1
89.6
91.3

69.2
78.8
76.9
80.0

55.4
66.7
65.2
67.1

82.9
88.7
87.6
89.5

77.0
81.7
83.2
85.0

4.2 Results on the FLIC dataset

We use Percentage of Correct Keypoints (PCK) as the evaluation metric. Because it is widely adopted
by previous works on FLIC  it provides convenience for comparison. These published works only
reported results on elbow and wrist and we follow the same practice. PCK reports the percentage of
predictions that lay in the normalized distance of annotation. Toshev et al. [23]  Chen and Yuille [2]
and Tompson et al. [21] also used CNN features. When compared with previous state of the art  our
method improves the performance of elbow and wrist by 2.7% and 1.7% respectively.

Table 2: Quantitative results on FLIC dataset (PCK@0.2)

Experiment
Toshev et al. [23]
Tompson et al. [21]
Chen and Yuille [2]
Ours

Elbow Wrist
82.0
92.3
89.0
93.1
92.4
95.3
98.0
94.1

4.3 Diagnostic Experiments

In this subsection  we conduct experiments to compare different message passing schemes  structures 
and noniliear functions. The experimental results in Table 3 use the same VGG for feature extraction.
Flooding is a message passing schedule  in which all vertices pass the messages to their neighboring
vertices simultaneously and locally as follows:

φ(hi) +

(cid:88)

Qt+1(hi) = τ

  

Qt(hi(cid:48)) ⊗ wi(cid:48)→i

i(cid:48)∈VN (i)\i

(15)

where VN (i) denotes the neighboring vertices of the ith vertex in the graphical model. We adopt the
iterative updating scheme in the work of Zheng et al. [29].
In Table 3  Flooding-1itr-tree denotes the result of using ﬂooding to perform message passing once
using CNN as in [29]. The tree structure in Fig. 2 (b) is adopted. Flooding-2iter-tree indicates

7

Table 3: Diagnostic Experiments (PCP)

Experiment
Flooding-1iter-tree
Flooding-2iter-tree
Flooding-2iter-loopy
Serial-tree(ReLU)
Serial-tree(Softmax)

Torso Head U.arms L.arms U.legs L.legs Mean
76.6
93.0
77.1
93.5
94.0
78.4
80.1
95.5
96.0
83.1

73.0
73.0
74.4
75.9
80.0

87.5
86.7
88.2
88.9
91.3

58.9
59.8
62.1
63.8
67.1

84.3
83.7
84.3
87.1
89.5

76.4
79.0
80.0
81.4
85.0

the result of using ﬂooding to pass messages twice. The weights across the two message passing
iterations are shared. Experimental results show slight improvement of passing twice than once.
The result for the loopy structured graph in Fig. 2 (e) is denoted by Flooding-2iter-loopy. The
connection of a pair of joints is decided by the following protocol: if 90% of the training sample’s
distance is within 48 pixels  which is the receptive ﬁeld size of our ﬁlters  we connect these two joints.
Improvement of 1.3% is introduced by these extra connections.
These approaches share the same drawbacks: lack of information for making predictions. With one
iteration of message passing  each body part could only receive information from neighborhood parts 
while with two iterations a part can only receive information from parts of depth 2. However  the
largest depth in our graph is 10. Flooding is inefﬁcient for a node to receive the messages from the
other nodes. This problem is solved with the serial scheme.
Serial scheme passes messages following a predeﬁned order and update information sequentially.
For a tree structured graph with N vertices  each vertex can be marginalized by passing the messages
within 2N operations using the efﬁcient sum-product algorithm [13]. The result of using serial
message passing is denoted by Serial-tree(Softmax) in Table 3. In can be shown that the serial scheme
performs better than the ﬂooding scheme.
It is well known that softmax leads to vanishing of gradients which make the network training
z e{αx} to accelerate the training process. We
inefﬁcient. In experiment  we replace 1
set α ← 0.5 and β ← Nc  where Nc is the number of feature channels. With this slight change  the
network can converge much faster than softmax without using α and β. The performance of using
this softmax  which is derived from our CRF in (13)  is 3% higher than Serial-tree(ReLU)  which
uses ReLU as the non-linear function for passing messages among features  a scheme used in [3].

z e{x} with β 1

5 Conclusion

We propose to use CRF for modeling structured features and structured human body part conﬁgura-
tions. This CRF is implemented by an end-to-end learning CNN. The efﬁcient sum-product algorithm
in the probabilistic model guides us in using an efﬁcient message passing approach so that each
vertex receives messages from other nodes in a more efﬁcient way. And the use of CRF also helps
us to choose non-linear functions and to know what are the assumptions and approximations made
in order to use CNN to implement such CRF. The gain in performance on two benchmark human
pose estimation datasets proves the effectiveness of this attempt  which shows a new direction for the
structure design of deep neural networks.
Acknowledgment: This work is supported by SenseTime Group Limited  Research Grants Council of
Hong Kong (Project Number CUHK14206114  CUHK14205615  CUHK14207814  CUHK14203015 
and CUHK417011) and National Natural Science Foundation of China (Number 61371192 and
61301269). W. Ouyang and X. Wang are the corresponding authors.

References
[1] Mpii human pose dataset. http://human-pose.mpi-inf.mpg.de/#related_benchmarks. Accessed:

2016-05-20.

[2] X. Chen and A. L. Yuille. Articulated pose estimation by a graphical model with image dependent pairwise

relations. In NIPS  2014.

[3] X. Chu  W. Ouyang  H. Li  and X. Wang. Structured feature learning for pose estimation. In CVPR  2016.

8

[4] X Chu  W Ouyang  W Yang  and X Wang. Multi-task recurrent neural network for immediacy prediction.

In ICCV  2015.

[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR  2005.

[6] J. Deng  N. Ding  Y. Jia  A. Frome  K. Murphy  S. Bengio  Y. Li  H. Neven  and H. Adam. Large-scale

object classiﬁcation using label relation graphs. In ECCV. 2014.

[7] SM Eslami  N. Heess  T. Weber  Y. Tassa  K. Kavukcuoglu  and G. E. Hinton. Attend  infer  repeat: Fast

scene understanding with generative models. arXiv preprint arXiv:1603.08575  2016.

[8] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. IJCV  61(1):55–79 

2005.

[9] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep

learning. arXiv preprint arXiv:1506.02142  2015.

[10] K. He  X. Zhang  Ss Ren  and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on

imagenet classiﬁcation. arXiv preprint arXiv:1502.01852  2015.

[11] G. E. Hinton  S. Osindero  and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation 

18:1527–1554  2006.

[12] S. Johnson and M. Everingham. Clustered pose and nonlinear appearance models for human pose

estimation. In BMVC  2010.

[13] F. Kschischang  B. Frey  and H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE Transac-

tions on Information Theory  47(2):498–519  2001.

[14] Wei Li  Rui Zhao  Tong Xiao  and Xiaogang Wang. Deepreid: Deep ﬁlter pairing neural network for

person re-identiﬁcation. In CVPR  2014.

[15] G. Lin  C. Shen  I. Reid  and A. van den Hengel. Deeply learning the messages in message passing

inference. In NIPS  2015.

[16] W. Ouyang  X. Chu  and X. Wang. Multi-source deep learning for human pose estimation. In CVPR  2014.

[17] L. Pishchulin  E. Insafutdinov  S. Tang  B. Andres  M. Andriluka  P. Gehler  and B.b Schiele. Deepcut:

Joint subset partition and labeling for multi person pose estimation. [arXiv]  November 2015.

[18] B. Sapp and B. Taskar. Modec: Multimodal decomposable models for human pose estimation. In CVPR 

2013.

[19] K. Simonyan and A.s Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

[20] C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  and A. Rabinovich.

Going deeper with convolutions. In CVPR  2015.

[21] J. Tompson  R. Goroshin  A. Jain  Y. LeCun  and C. Bregler. Efﬁcient object localization using convolutional

networks. In CVPR  2015.

[22] J. Tompson  A. Jain  Y. LeCun  and C. Bregler. Joint training of a convolutional network and a graphical

model for human pose estimation. In NIPS  2014.

[23] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In CVPR  2014.

[24] L. Wan  D. Eigen  and R. Fergus. End-to-end integration of a convolutional network  deformable parts

model and non-maximum suppression. arXiv preprint arXiv:1411.5309  2014.

[25] Y. Wang  D. Tran  and Z. Liao. Learning hierarchical poselets for human parsing. In CVPR  2011.

[26] Tong Xiao  Hongsheng Li  Wanli Ouyang  and Xiaogang Wang. Learning deep feature representations

with domain guided dropout for person re-identiﬁcation. In CVPR  2016.

[27] W. Yang  W. Ouyang  H. Li  and X. Wang. End-to-end learning of deformable mixture of parts and deep

convolutional neural networks for human pose estimation. In CVPR  2016.

[28] Y. Yang and D. Ramanan. Articulated human detection with ﬂexible mixtures of parts. PAMI  35(12):2878–

2890  2013.

[29] S. Zheng  S. Jayasumana  B. Romera-Paredes  V. Vineet  Z. Su  D. Du  C. Huang  and P. Torr. Conditional

random ﬁelds as recurrent neural networks. In ICCV  2015.

9

,Zhenwen Dai
Georgios Exarchakis
Jörg Lücke
Xiao Chu
Wanli Ouyang
hongsheng Li
Xiaogang Wang
Yi Wang
Xin Tao
Xiaojuan Qi
Xiaoyong Shen
Jiaya Jia