2019,Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function,We present an algorithm based on the \emph{Optimism in the Face of Uncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. 
By evaluating the state-pair difference of the optimal bias function $h^{*}$  the proposed algorithm achieves a regret bound of $\tilde{O}(\sqrt{SATH})$\footnote{The symbol $\tilde{O}$ means $O$ with log factors ignored. } for MDP with S states and A actions  in the case that an upper bound $H$ on the span of $h^{*}$  i.e.  $sp(h^{*})$ is known. 
This result outperforms the best previous regret bounds $\tilde{O}(HS\sqrt{AT})$\cite{bartlett2009regal} by a factor of $\sqrt{SH}$. 
Furthermore  this regret bound matches the lower bound of $\Omega(\sqrt{SATH})$\cite{jaksch2010near} up to a logarithmic factor. As a consequence   we show that there is a near optimal regret bound of $\tilde{O}(\sqrt{DSAT})$ for MDPs with finite diameter $D$ compared to the lower bound of $\Omega(\sqrt{DSAT})$\cite{jaksch2010near}.,Regret Minimization for Reinforcement Learning by

Evaluating the Optimal Bias Function

Zihan Zhang

Tsinghua University

zihan-zh17@mails.tsinghua.edu.cn

Xiangyang Ji

Tsinghua University

xyji@tsinghua.edu.cn

Abstract

We present an algorithm based on the Optimism in the Face of Uncertainty (OFU)
principle which is able to learn Reinforcement Learning (RL) modeled by Markov
decision process (MDP) with ﬁnite state-action space efﬁciently. By evaluating
the state-pair difference of the optimal bias function h⇤  the proposed algorithm
achieves a regret bound of ˜O(pSAHT )1 for MDP with S states and A actions 
in the case that an upper bound H on the span of h⇤  i.e.  sp(h⇤) is known. This
result outperforms the best previous regret bounds ˜O(SpAHT )[Fruit et al.  2019]
by a factor of pS. Furthermore  this regret bound matches the lower bound of
⌦(pSAHT )[Jaksch et al.  2010] up to a logarithmic factor. As a consequence 
we show that there is a near optimal regret bound of ˜O(pSADT ) for MDPs with
a ﬁnite diameter D compared to the lower bound of ⌦(pSADT )[Jaksch et al. 
2010].

1

Introduction

In this work we consider the Reinforcement Learning (RL) problem [Burnetas and Katehakis  1997 
Sutton and Barto  2018] of an agent interacting with an environment. The problem is generally
modelled as a discrete Markov Decision Process (MDP)[Puterman  1994]. The RL agent needs to
learn the underlying dynamics of the environment in order to make sequential decisions. At step t 
the agent observes current state st and chooses an action at based on the policy learned from the past.
Then the agent receives a reward rt from the environment  and the environment transits to state st+1
according to the states transition model. Particularly  both rt and st+1 are independent of previous
trajectories  and are only conditioned on st and at. In the online framework of reinforcement learning 
we aim to maximize cumulative reward. Therefore  there is a trade-off between exploration and
exploitation  i.e.  taking actions we have not learned accurately enough and taking actions which
seem to be optimal currently.
The solutions to exploration-exploitation dilemma can mainly be divided into two groups. In the ﬁrst
group  the approaches utilize the Optimism in the Face of Uncertainty (OFU) principle [Auer et al. 
2002]. Under OFU principle  the agent maintains a conﬁdent set of MDPs and the underlying MDP
is contained in this set with high probability. The agent executes the optimal policy of the best MDP
in the conﬁdence set [Bartlett and Tewari  2009  Jaksch et al.  2010  Maillard et al.  2011  Fruit et al. 
2018a]. In the second group  the approaches utilize posterior sampling [Thompson  1933]. The agent
maintains a posterior distribution over reward functions and transition models. It samples an MDP
and executes corresponding optimal policy in each epoch. Because of simplicity and scalability  as
well as provably optimal regret bound  posterior sampling has been getting popular in related research
ﬁeld [Osband et al.  2013  Osband and Van Roy  2016  Agrawal and Jia  2017  Abbasi-Yadkori  2015].

1The symbol ˜O means O with log factors ignored.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Related Work

In the research ﬁeld of regret minimization for reinforcement learning  Jaksch et al. [2010] showed a
regret bound of ˜O(DSpAT ) for MDPs with a ﬁnite diameter D  and proved that it is impossible
to reach a regret bound smaller than ⌦(pSADT ). Agrawal and Jia [2017] established a better
regret bound of ˜O(DpSAT ) by posterior sampling method. Bartlett and Tewari [2009] achieved
a regret bound of ˜O(HSpAT ) where H is an input as an upper bound of sp(h⇤) . Fruit et al.
[2018b] designed a practical algorithm for the constrained optimization problem in REGAL.C [Bartlett
and Tewari  2009]  and obtained a regret bound of ˜O(HpSAT ) where   S is the number
of possible next states. On the other hand  Ouyang et al. [2017] and Theocharous et al. [2017]
designed posterior sampling algorithms with Bayesian regret bound of ˜O(HSpAT )  with the
assumption that elements of support of the prior distribution have a consistent upper bound H for
their optimal bias spans.Talebi and Maillard [2018] showed a problem-dependent regret bound of

˜O(qPs a V (Ps a  h⇤)ST ). Recently  Fruit et al. [2019] presented improved analysis of UCRL2B

algorithm and obtained a regret bound of ˜O(SpDAT ).
There are also considerable work devoted to studying ﬁnite-horizon MDP. Osband and Van Roy
[2016] presented PRSL to establish a Bayesian regret bound of ˜O(HpSAT ) using posterior sampling
method. And later Azar et al. [2017] reached a better regret bound of ˜O(pSAHT ). Recently  Kakade
et al. [2018] and Zanette and Brunskill [2019] achieved the same regret bound of ˜O(pSAHT ) by
learning a precise value function to predict the best future reward of current state.
We notice a mistake about concentration of average of independent multinoulli trials in the proof of
[Agrawal and Jia  2017] (see Appendix.A for further details). This mistake suggests that they may
not reduce a factor of pS in their regret bounds.

1.2 Main Contribution
In this paper  we design an OFU based algorithm  and achieve a regret bound of ˜O(pSAHT ) given
an upper bound H on sp(h⇤). As a corollary  we establish a regret bound of ˜O(pSADT ) for the
MDPs with ﬁnite diameter D. Meanwhile the corresponding lower bounds for the above two upper
bounds are ⌦(pSAHT ) and ⌦(pSADT ) respectively. In a nutshell  our algorithm improves the
regret bound by a factor of pS compared to the best previous known results.
Our Approach: we consider regret minimization for RL by evaluating state-pair difference of the
optimal bias function. Firstly  we observe that we can achieve a near-optimal regret bound with guide
of the optimal bias function. Considering the fact that it is hard to estimate the optimal bias function
directly [Ortner  2008]  we design a conﬁdence set Hk of the optimal bias function. Based on Hk
we obtain a tighter conﬁdence set of MDPs and a better regret bound. It is notable that the order
of samples in the trajectory is crucial when computing Hk in our algorithm  while it is ignored in
previous methods. In this way  we utilize more information about the trajectory when computing the
conﬁdence set  which enables us to achieve a better regret bound.

2 Preliminaries

We consider the MDP learning problem where the MDP M = hS A  r  P  s1i. S = {1  2  ...  S} is
the state space  A = {1  2  ...  A} is the action space  P : S⇥A! S 2 is the transition model 
r : S⇥A! [0 1] is the reward function  and s1 is the initial state. The agent executes action a at
state s and receives a reward r(s  a)  and then the system transits to the next state s0 according to
P(·|s  a) = Ps a. In this paper  we assume that E[r(s  a)] is known for each (s  a) pair  and denote
E[r(s  a)] as rs a. It is not difﬁcult to extend the proof to the original case.
In the following sections  we mainly focus on weak-communicating (see deﬁnition [Bartlett and
Tewari  2009]) MDPs.
Assumption 1. The underlying MDP is weak-communicating .

2In this paper  we use X to denote all distributions on X. Particularly  we use m to denote the m-simplex.

2

We ﬁrst summarize several useful known results for MDPs and RL.
Deﬁnition 1 (Policy). A policy ⇡ : S! A is a mapping from the state space to all distributions on
the action space. In the case the support of ⇡(s) is a single action  we also denote this action as ⇡(s).

Given a policy ⇡  transition model P and reward function r  we use P⇡ to denote the transition
probability matrix and r⇡ to denote the reward vector under ⇡. Speciﬁcally  when ⇡ is a deterministic
policy  P⇡ = [P1 ⇡(1)  ...  Ps ⇡(s)] and r⇡ = [r1 ⇡(1)  ...  rS ⇡(S)]T .
Deﬁnition 2 (Average reward). Given a policy ⇡  when starting from s1 = s  the average reward is
deﬁned as:

⇢⇡(s) = lim
T!1

1
T Eat⇠⇡(st) 1tT [

TXt=1

rst at|s1 = s].

The optimal average reward and the optimal policy are deﬁned as ⇢⇤(s) = max⇡ ⇢⇡(s) and ⇧⇤(s) =
arg max⇡ ⇢⇡(s) respectively. It is well known that  under Assumption 1  ⇢⇤(s) is state independent 
so that we write it as ⇢⇤ in the rest of the paper for simplicity.
Deﬁnition 3 (Diameter). Diameter of an MDP M is deﬁned as:
T ⇡
s!s0 

D(M ) = max

min

s s02S s6=s0

⇡:S!A

where T ⇡

s!s0 denotes the expected number of steps to reach s0 from s under policy ⇡.

Under Assumption 1  it is known the optimal bias function h⇤ satisﬁes that

h⇤ + ⇢⇤1 = max
a2A

(rs a + P T

s ah⇤)

(1)

where 1 = [1  1  ...  1]T . It is obvious that if h satisﬁes (1)  then so is h⇤+1 for any  2 R. Assuming
h is a solution to (1)  we set3  =  mins hs and h⇤ = h + 1  then the optimal bias function h⇤ is
uniquely deﬁned. Besides  the span operator sp : RS ! R is deﬁned as sp(v) = max
s s02[S]|vs  vs0|.
The reinforcement learning problem. In reinforcement learning  the agent starts at s1 = sstart 
and proceeds to make decisions in rounds t = 1  2  ...  T . The S  A and {rs a}s2S a2A are known
to the agent  while the transition model P is unknown to agent. Therefore  the ﬁnal performance is
measured by the cumulative regret deﬁned as

R(T  sstart) := T⇢ ⇤ 

TXt=1
The upper bound for R(T  sstart) we provide is always consistent with that of sstart. In the following
sections  we use R(T  sstart) to denote R(T ) for simplicity.
3 Algorithm Description

rst at.

3.1 Framework of UCRL2
We ﬁrst revisit the classical framework of UCRL2 [Jaksch et al.  2010] brieﬂy. As described in
Algorithm 1 (EBF)  there are mainly three components in the UCRL2 framework: doubling episodes 
building the conﬁdence set and solving the optimization problem.
Doubling episodes: The algorithm proceeds through episodes k = 1  2  .... In the k-th episode  the
agent makes decisions according to ⇡k. The episode ends whenever 9(s  a)  such that the visit count
of (s  a) in the k-th episode is larger than or equal to the visit count of (s  a) before the k-th episode.
Let K be the number of episodes. Therefore  we can get that K  SA(log2( T
SA ) + 1)  3SA log(T )
when SA  2 [Jaksch et al.  2010].
Building the conﬁdence set: At the beginning of an episode  the algorithm computes a collection of
plausible MDPs  i.e.  the conﬁdence set Mk based on previous trajectory. Mk should be designed
properly such that the underlying MDP M is contained by Mk with high probability  and the elements

3In this paper  we use [v1  v2  ...  vS]T to indicate a vector v 2 RS

3

in Mk are closed to M. In our algorithm  the conﬁdence set is not a collection of MDPs. Instead  we
design a 4-tuple (⇡  P 0(⇡)  h0(⇡) ⇢ (⇡)) to describe a plausible MDP and its optimal policy.
Solving the optimization problem: Given a conﬁdence set M  the algorithm selects an element
from M according to some criteria. Generally  to keep the optimality of the chosen MDP  the
algorithm needs to maximize the average reward with respect to certain constraints. Then the
corresponding optimal policy will be executed in current episode.

3.2 Tighter Conﬁdence Set by Evaluating the Optimal Bias Function
REGAL.C [Bartlett and Tewari  2009] utilizes H to compute Mk  thus avoiding the issues brought by
the diameter D. Similar to REGAL.C  we assume that H  an upper bound of sp(h⇤) is known. We
design a novel method to compute the conﬁdence set  which is able to utilize the knowledge of the
history trajectory more efﬁciently. We ﬁrst compute a well-designed conﬁdence set Hk of the optimal
bias function  and obtain a tighter conﬁdence set Mk based on Hk.
On the basis of above discussion  we summarize high-level intuitions as below:
Exploration guided by the optimal bias function: Once the true optimal bias function h⇤ is given 
we could get a better regret bound. In this case we regard the regret minimization problem as S
independent multi-armed bandit problems. UCB algorithm with Bernstein bound [Lattimore and
Hutter  2012] provides a near optimal regret bound. However  we can not get h⇤ exactly. Instead  a
tight conﬁdence set of h⇤ also helps to guide exploration.
Conﬁdence set of the optimal bias function: We ﬁrst study what could be learned about h⇤ if we
always choose optimal actions. For two different states s  s0  suppose we start from s at t1  and reach
s0 the ﬁrst time at t2 (t2 is a stopping time)  then we have E[Pt21
(rt  ⇢⇤)]4= ⇤s s0 := h⇤s  h⇤s0 by
the deﬁnition of optimal bias function. As a result Pt21
(rt  ⇢⇤) could be regarded as an unbiased
estimator for ⇤s s0. Based on concentration inequalities for martingales  we have the following formal
deﬁnitions and lemma.
Deﬁnition 4. Given a trajectory L = {(st  at  st+1  rt)}1tN   for s  s0 2S and s 6= s0  let
ts1(L) := min{min{t|st = s}  N + 2}. We deﬁne {tsk(L)}k2 and {tek(L)}k1 recursively by
following rules 

t=t1

t=t1

tek(L) := min min{t|st = s0  t > tsk(L)}  N + 2  
tsk(L) := min min{t|st = s  t > tek1(L)}  N + 2 .

The count of arrivals c(s  s0 L) from s to s0 is deﬁned as

c(s  s0 L) := max{k|tek(L)  N + 1}.

Here we deﬁne min ? = +1 and max ? = 0 respectively.
Lemma 1 (Main Lemma). We say an MDP is ﬂat if all its actions are optimal. Suppose M is a
ﬂat MDP (without the constraint rs a 2 [0  1]). We run N steps following an algorithm G under M.
Let L = {(st  at  st+1  rt)}1tN be the ﬁnal trajectory. For any two states s  s0 2S and s 6= s0 
let c(s  s0 L)  {tek(L)}k1 and {tsk(L)}k1 be deﬁned as in Deﬁnition 4. Then we have  for any
algorithm G  with probability at least 1  N   for any 1  c  c(s  s0 L) it holds that



cXk=1⇣h⇤s0  h⇤s +

Xtsk(L)ttek(L)1

(rt  ⇢⇤)⌘  (p2N + 1)sp(h⇤).

 )5

where  = log( 2
To use Lemma 1 to compute Hk  we have to overcome two problems: (i) M may not be ﬂat; (ii) we
do not have the value of ⇢⇤. Under the assumption the total regret is ˜O(HSpAT )  we can solve the
problems subtly.
Let regs a = h⇤s +⇢⇤P T
and could be regarded as the single step regret of (s  a). Let r0s a = h⇤s + ⇢⇤  P T

s ah⇤rs a  which is also called optimal gap [Burnetas and Katehakis  1997]
s ah⇤ = rs a + regs a

(2)

4To explain the high-level idea  we assume this expectaion is well-deﬁned.
5In this paper  always denotes log( 2

 ).

4

Algorithm 1 EBF: Estimate the Bias Function
Input: H    T .
Initialize: t 1 tk 0.
1: for episodes k = 1  2  ... do
tk current time;
2:
Ltk1 { (si  ai  si+1  ri)}1itk1;
3:
4: Mk BuildCS(H  log( 2
Choose (⇡  P 0(⇡)  h0(⇡) ⇢ (⇡)) 2M k to maximize ⇢(⇡) over Mk;
5:
⇡k ⇡;
6:
Follow ⇡k until the visit count of some (s  a) pair doubles.
7:
8: end for

 ) Ltk1);



Ns a s0|(h⇤s0  h⇤s)  (h0s0  h0s)| 2

and M0 = hS A  r0  P  s1i. It is easy to prove that M0 is ﬂat and has the same optimal bias function
and optimal average reward as M. We attain by Lemma 1 that with high probability  it holds that
regst at + (p2N + 1)sp(h⇤). (3)
Let h0 2 [0  H]S be a vector such that (3) still holds with h⇤ replaced by h0  then we can derive that

c(s s0 L)Xk=1 ⇣h⇤s0  h⇤s +

Xtsk(L)ttek(L)1

(rst at  ⇢⇤)⌘ 
NXt=1

NXt=1
regst at + 2(p2N + 1)H

with high probability it holds

ˆNs a s0|(h⇤s0  h⇤s)  (h0s0  h0s)| = ˜O(HSpAN ).

(4)
As for the problem we have no knowledge about ⇢⇤  we can replace ⇢⇤ with the empirical average

where Ns a s0 :=PN
t=1 I[st = s  at = a  st+1 = s0]  c(s  s0 L). Because it is not hard to bound
t=1 regst at ⇡R (N ) up to ˜O(HSpAN ) by REGAL.C [Bartlett and Tewari  2009]  we obtain that
PN
reward ˆ⇢. Our claim about (4) still holds as long as N (⇢⇤  ˆ⇢) = ˜O(HSpAN )  which is equivalent
to R(N ) = ˜O(HSpAN ).
Although it seems that (4) is not tight enough  it helps to bound the error term due to the difference
between hk and h⇤ up to o(pT ) by setting N = T . (refer to Appendix.C.5.)
Based on the discussion above  we deﬁne Hk as:
Hk := {h 2 [0  H]S||L1(h  s  s0 Ltk1)| 48SpAT sp(h) + (p2T + 1)sp(h) 8s  s0  s 6= s0}

where

Together with constraints on the transition model (5)-(7) and constraint on optimality (8)  we propose
Algorithm 2 to build the conﬁdence set  where

L1(h  s  s0 L) =

Xtsk(L)itek(L)1

(ri  ˆ⇢)⌘.

c(s s0 L)Xk=1 ⇣(hs0  hs) +
V (x  h) =Xs

xsh2

s  (xT h)2.

4 Main Results

In this section  we summarize the results obtained by using Algorithm 1 on weak-communicating
MDPs. In the case there is an available upper bound H for sp(h⇤)  we have following theorem.
Theorem 1 (Regret bound (H known)). With probability 1    for any weak-communicating MDP
 )) and S  A  H  20 where p1 is a
M and any initial state sstart 2S   when T  p1(S  A  H  log( 1
polynomial function  the regret of EBF algorithm is bounded by

R(T )  490rSAHT log(

40S2A2T log(T )



) 

5

t=1 I[st = s  at = a]  1}  8(s  a);

Algorithm 2 BuildCS(H   L)
Input: H    L = {(si  ai  si+1  ri)}1iN
1: H { h 2 [0  H]S| |L1(h  s  s0 L)| 48SpAT sp(h) + (p2T + 1)sp(h) 8s  s0  s 6= s0};
2: Ns a max{PN
3: ˆPs a s0 PN
4: O {
R  such that

⇡|⇡ is a deterministic policy  and 9P 0(⇡) 2 RS⇥A⇥S  h0(⇡) 2H and ⇢(⇡) 2
|P 0s a s0(⇡)  ˆPs a s0| 2q ˆPs a s0/Ns a + 3/Ns a + 4

t=1 I[st=s at=a st+1=s0]

  8(s  a  s0);

(5)

3
4

3

4 /N

s a 

Ns a

|P 0s a(⇡)  ˆPs a|1 q14S/N s a

(6)

|(P 0s a(⇡) ˆPs a)T h0(⇡)| 2qV ( ˆPs a  h0(⇡))/Ns a + 12H/N s a + 10H 3/4/N 3/4

k s a  (7)

P 0s ⇡(s)(⇡)T h0(⇡) + rs ⇡(s) = max
a2A

holds for any s  a  s0};

5: Return:{(⇡  P 0(⇡)  h0(⇡) ⇢ (⇡))|⇡ 2O }.

P 0s a(⇡)T h0(⇡) + rs a = h0(⇡) + ⇢(⇡)1

(8)

T   we get

whenever an upper bound of the span of optimal bias function H is known. By setting  = 1

that E[R(T )] = ˜O(pSAHT )
Theorem 1 generalizes the ˜O(pSAHT ) regret bound from the ﬁnite-horizon setting [Azar et al. 
2017] to general weak-communicating MDPs  and improves the best previous known regret bound
˜O(HpSAT )[Fruit et al.  2019] by an pS factor. More importantly  this upper bound matches the
⌦(pSAHT ) lower bound up to a logarithmic factor.
Based on Theorem 1  in the case the diameter D is ﬁnite but unknown  we can reach a regret bound
of ˜O(pSADT ).
Corollary 1. For weak-communicating MDP M with a ﬁnite unknown diameter D and any initial
 )) and S  A  D  20 where p2
state sstart 2S   with probability 1    when T  p2(S  A  D  log( 1
is a polynomial function  the regret can be bounded by

R(T )  491rSADT (log(
T   we get that E[R(T )] = ˜O(pSADT ).

By setting  = 1

S3A2T log(T )

).



We postpone the proof of Corollary 1 to Appendix.D.
Although EBF is proved to be near optimal  it is hard to implement the algorithm efﬁciently. The
optimization problem in line 5 Algorithm 1 is well-posed because of the optimality equation (8).
However  the constraint (7) is non-convex in h0(⇡)  which makes the optimization problem hard
to solve. Recently  Fruit et al. [2018b] proposed a practical algorithm SCAL  which solves the
optimization problem in REGAL.C efﬁciently. We try to expand the span truncation operator Tc to our
framework  but fail to make substantial progress. We have to leave this to future work.

5 Analysis of EBF (Proof Sketch of Theorem 1)

Our proof mainly contains two parts. In the ﬁrst part  we bound the probabilites of the bad events. In
the second part  we manage to bound the regret when the good event occurs.

6

5.1 Probability of Bad Events

s a =Pt

s a s0

3

+ 2

+

We ﬁrst present the explicit deﬁnition of the bad events. Let N (t)
i=1 I[si = s  ai = a]. We
denote Nk s a = N (tk1)
as the visit count of (s  a) before the k-th episode  and vk s a as the visit
count of (s  a) in the k-th episode respectively. We also denote ˆP (k) as the empirical transition model
before the k-th episode.
Deﬁnition 5 (Bad event). For the k-th episode  deﬁne

s a

max{Nk s a  1}

max{Nk s a  1}

max{Nk s a  1}

sp(h⇤)

max{Nk s a  1} 

vk0 s aregs a > 22HSpAT o

s a )T h⇤| > 2s V (Ps a  h⇤))
s a s0  Ps a s0| > 2vuut ˆP (k)
(⇢⇤  rst at)| > 26HSpAT  Xk0<kXs a

B1 k :=⇢9(s  a)  s.t.|(Ps a  ˆP (k)
B2 k =⇢9(s  a  s0)  s.t.| ˆP (k)
B3 k =n| X1t<tk
B4 k ={(⇡⇤  P ⇤  h⇤ ⇢ ⇤)|⇡⇤is a deterministic optimal policy}\M k = ? .
The bad event in the k-th episode therefore is deﬁned as Bk = B1 k [ B2 k [ B3 k [ B4 k  and the
total bad event B is deﬁned as B := [1kK+1Bk. At the same time  we have the deﬁnition of the
good event as G = BC.
Lemma 2 (Bound of P(B)). Suppose we run Algorithm 1 for T steps  then P(B)  (6AT +
12S2A)SA log(T ) when T  A log(T ) and SA  4.
5.2 Regret when the Good Event Occurs
In this section we assume that the good event G occurs. We use Rk to denote the regret in the
k-th episode. We use P 0k  Pk  ˆPk  rk  ⇢k and hk to denote P 0⇡k (⇡k)  P⇡k  ˆP (k)
⇡k   r⇡k  ⇢(⇡k) and
h0(⇡k) respectively. We deﬁne vk as the vector such that vk s = vk s ⇡k(s) 8s  and introduce
k s s0 = hk s  hk s0 8s  s0.
Noting that for ↵> 0 PkPs a vk s a

2↵) 
which could be ignored when T is sufﬁciently large. Therefore  we can omit such terms without
changing the regret bound.
According to BC
Rk = vT
= vT
|

k (⇢k1  rk) = vT
k (⇢⇤1  rk)  vT
k (P 0k  I)T hk
k ( ˆPk  Pk)T h⇤
+ vT
+ vT
k (Pk  I)T hk
{z
|
}
|
}
3k
We bound the four terms in the right side of (9) separately.
Term 1k : The expectation of 1k never exceeds [H  H]. However  we can not directly utilize this
to bound 1k. By observing that 1k has a martingale difference structure  we have following lemma
based on concentration inequality for martingales.
Lemma 3. When T  S2AH 2  with probability 1  3  it holds that
1k  KH + (4H + 2p12T H).

k ( ˆPk  Pk)T (hk  h⇤)
}

k (P 0k  ˆPk)T hk
}

+↵ could be roughly bounded by O(T 1

4 k and the optimality of ⇢k we have

{z
4k

1

max{Nk a s 1}

1
2

{z
2k

+ vT

|

{z
1k

. (9)

+

4

4 3

4 
max{Nk s a  1} 3

Xk
vk s a✓2s V (Ps a  h⇤)

Term 2k : Recalling the deﬁnition of V (x  h) in Section 3  BC
2k Xs a

max{Nk s a  1}◆ ⇡ O✓Xs a

max{Nk s a  1}

H

+2

1 k implies that

vk s as V (Ps a  h⇤)
max{Nk s a  1}◆ 

(10)

7

terms. We bound RHS of (10) by bounding

s a V (Ps a  h⇤) by O(T H). Formally  we have following lemma.

the insigniﬁcant

where ⇡ means we omit
Ps a N (T )
Lemma 4. When T  S2AH 2  with probability 1  
vk s as V (Ps a  h⇤)

Xk s a

Term 3k : According to (7) we have

3k Xs a

vk s aL2(max{Nk s a  1}  ˆP (k)

max{Nk s a  1}  21pSAHT .
s a   hk) ⇡ O✓Xs a

vk s as V ( ˆP (k)

max{Nk s a  1}◆

s a   hk)

(11)

of (11)  because hk varies in different episodes  we have to bound the static part and the dynamic part
separately. Noting that

where L2(N  p  h) = 2pV (p  h)/N + 12H/N + 10H 3/4/N 3/4. When dealing with the RHS
qV ( ˆP (k)

s a   h⇤) qV (Ps a  h⇤))

ˆP (k)

s a   hk) qV ( ˆP (k)
s a   h⇤)) + (qV ( ˆP (k)
s a   hk) qV (Ps a  h⇤)  (qV ( ˆP (k)
q|V ( ˆP (k)
s a   h⇤)| +q|V ( ˆP (k)
s a   hk)  V ( ˆP (k)
s4HXs0
s a s0|k s s0  ⇤s s0| +q4H 2| ˆP (k)
s a s0|k s s0  ⇤s s0| +vuut4H 2s
Xs0 q4H ˆP (k)
⇡ O⇣Xs0 q4H ˆP (k)
s a s0|k s s0  ⇤s s0|⌘ 
vuut ˆP (k)

s a s0|k s s0  ⇤s s0|
max{Nk s a  1}

vk s aXs0

pH Xk s a

According to the bound of the second term  it sufﬁces to bound

s a   h⇤)  V (Ps a  h⇤)|
s a  Ps a|1

14S

max{Nk s a  1}

s a s0  Ps a s)(⇤s s0  k s s0)

( ˆP (k)

s a s0

vk s a( ˆP (k)

vk s aXs0

2 k the fourth term can be bounded by:

Surprisingly  we ﬁnd that this term is an upper bound for the fourth term.
Term 4k : Recalling that ⇤s s0 = h⇤s  h⇤s0  according to BC
4k =Xs a
s a  Ps a)T (hk  hk s1  h⇤ + h⇤s1) =Xs a
vuut ˆP (k)
⇡ O✓Xs a
max{Nk s a  1}|k s s0  ⇤s s0|◆
vk s aXs0
vuut ˆP (k)
◆.
= O✓pHXs a
vk s aXs0
(14)
according to (4) and the fact vk s a  max{Nk s a  1} we have
vk s ar ˆP (k)
max{Nk s a 1}  qmax{Nk s a  1} ˆP (k)
s a s0|k s s0⇤s s0|
4 ). To be rigorous 
we have following lemma.
Lemma 5. With probability 1  S2T   it holds that

s a s0|k s s0  ⇤s s0|
max{Nk s a  1}

s a s0|k s s0  ⇤s s0| = ˜O(T 1

To bound (13 

(12)

(13)

Xk Xs a

vk s aXs0

s a s0|(k s s0  ⇤s s0)|
max{Nk s a  1}

vuut ˆP (k)

8

 11KS

5

2 A

1

4 H

1

2 T

1

4 

1

4 .

(15)

Due to the lack of space  the proofs are delayed to the appendix.
Putting (9)-(12)  (14)  Lemma 3  Lemma 4 and Lemma 5 together  we conclude that R(T ) =
˜O(pSAHT ).

6 Conclusion

In this paper we answer the open problems proposed by Jiang and Agarwal [2018] partly by designing
an OFU based algorithm EBF and proving a regret bound of ˜O(pHSAT ) whenever H  an upper
bound on sp(h⇤) is known. We evaluate state-pair difference of the optimal bias function during
learning process. Based on this evaluation  we design a delicate conﬁdence set to guide the agent to
explore in the right direction. We also prove a regret bound of ˜O(pDSAT ) without prior knowledge
about sp(h⇤). Both two regret bounds match the corresponding lower bound up to a logarithmic
factor and outperform the best previous known bound by an pS factor.

Acknowledgments

The authors would like to thank the anonymous reviewers for valuable comments and advice.

References
Yasin Abbasi-Yadkori. Bayesian optimal control of smoothly parameterized systems. In Conference

on Uncertainty in Artiﬁcial Intelligence  2015.

Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning  worst-case
regret bounds. In Advances in Neural Information Processing Systems  pages 1184–1194  2017.

Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning  47(2-3):235–256  2002.

Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for reinforce-

ment learning. arXiv preprint arXiv:1703.05449  2017.

Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

A. N. Burnetas and M. N. Katehakis. Optimal Adaptive Policies for Markov Decision Processes.

1997.

Ronan Fruit  Matteo Pirotta  and Alessandro Lazaric. Near optimal exploration-exploitation in
non-communicating markov decision processes. In Advances in Neural Information Processing
Systems  pages 2998–3008  2018a.

Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. Efﬁcient bias-span-constrained

exploration-exploitation in reinforcement learning. arXiv preprint arXiv:1802.04020  2018b.

Ronan Fruit  Matteo Pirotta  and Alessandro Lazaric. Improved analysis of ucrl2b. 2019.

Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11(Apr):1563–1600  2010.

Nan Jiang and Alekh Agarwal. Open problem: The dependence of sample complexity lower bounds

on planning horizon. In Conference On Learning Theory  pages 3395–3398  2018.

Sham Kakade  Mengdi Wang  and Lin F Yang. Variance reduction methods for sublinear reinforce-

ment learning. arXiv preprint arXiv:1802.09184  2018.

Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International Conference on

Algorithmic Learning Theory  pages 320–334. Springer  2012.

9

Odalric-Ambrym Maillard  Rémi Munos  and Gilles Stoltz. A ﬁnite-time analysis of multi-armed
bandits problems with kullback-leibler divergences. In Proceedings of the 24th annual Conference
On Learning Theory  pages 497–514  2011.

Ronald Ortner. Online regret bounds for markov decision processes with deterministic transitions. In

International Conference on Algorithmic Learning Theory  pages 123–137. Springer  2008.

Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement

learning? arXiv preprint arXiv:1607.00215  2016.

Ian Osband  Daniel Russo  and Benjamin Van Roy. (more) efﬁcient reinforcement learning via
posterior sampling. Advances in Neural Information Processing Systems  pages 3003–3011  2013.
Yi Ouyang  Mukul Gagrani  Ashutosh Nayyar  and Rahul Jain. Learning unknown markov decision
processes: A thompson sampling approach. In Advances in Neural Information Processing Systems 
pages 1333–1342  2017.

M L Puterman. Markov decision processes: Discrete stochastic dynamic programming. 1994.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press  2018.
Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undis-

counted reinforcement learning in mdps. arXiv preprint arXiv:1803.01626  2018.

Georgios Theocharous  Zheng Wen  Yasin Abbasi-Yadkori  and Nikos Vlassis. Posterior sampling

for large scale reinforcement learning. arXiv preprint arXiv:1711.07979  2017.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. arXiv preprint arXiv:1901.00210 
2019.

10

,Zihan Zhang
Xiangyang Ji