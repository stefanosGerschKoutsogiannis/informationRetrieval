2019,Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes,We implement gradient-based variational inference routines for Wishart and inverse Wishart processes  which we apply as Bayesian models for the dynamic  heteroskedastic covariance matrix of a multivariate time series. The Wishart and inverse Wishart processes are constructed from i.i.d. Gaussian processes  existing variational inference algorithms for which form the basis of our approach. These methods are easy to implement as a black-box and scale favorably with the length of the time series  however  they fail in the case of the Wishart process  an issue we resolve with a simple modification into an additive white noise parameterization of the model. This modification is also key to implementing a factored variant of the construction  allowing inference to additionally scale to high-dimensional covariance matrices. Through experimentation  we demonstrate that some (but not all) model variants outperform multivariate GARCH when forecasting the covariances of returns on financial instruments.,Scalable Bayesian dynamic covariance modeling with
variational Wishart and inverse Wishart processes

Creighton Heaukulani

No Afﬁliation

Bangkok  Thailand

c.k.heaukulani@gmail.com

Mark van der Wilk

PROWLER.io

Cambridge  United Kingdom

mark@prowler.io

Abstract

We implement gradient-based variational inference routines for Wishart and in-
verse Wishart processes  which we apply as Bayesian models for the dynamic 
heteroskedastic covariance matrix of a multivariate time series. The Wishart and
inverse Wishart processes are constructed from i.i.d. Gaussian processes  existing
variational inference algorithms for which form the basis of our approach. These
methods are easy to implement as a black-box and scale favorably with the length
of the time series  however  they fail in the case of the Wishart process  an issue we
resolve with a simple modiﬁcation into an additive white noise parameterization
of the model. This modiﬁcation is also key to implementing a factored variant
of the construction  allowing inference to additionally scale to high-dimensional
covariance matrices. Through experimentation  we demonstrate that some (but
not all) model variants outperform multivariate GARCH when forecasting the
covariances of returns on ﬁnancial instruments.

1

Introduction

Estimating the (time series of) covariance matrices between the variables in a multivariate time
series is a principal problem of interest in many domains  including the construction of ﬁnancial
trading portfolios [15] and the study of brain activity measurements in neurological studies [7].
Estimating the entries of the covariance matrices is a challenging problem  however  because there are
O(N D2) parameters to estimate for a time series of length N with D variables  yet we only record
a single observation of the time series consisting of O(N D) data points. Bayesian models (and
their corresponding inference procedures) often perform well in these overparameterized problems;
indeed  Fox and West [7]  Wilson and Ghahramani [24] and Fox and Dunson [6] show that Bayesian
approaches based on the Wishart and inverse Wishart processes produce better estimates of dynamic
covariance matrices than the venerable multivariate GARCH approaches [3  4].
The Wishart and inverse Wishart processes are two related stochastic processes in the state space of
symmetric  positive deﬁnite matrices  making them appropriate models for (heteroskedastic) time
series of covariance matrices. They are themselves constructed from i.i.d. Gaussian processes  in
analogy to the construction of Wishart or inverse Wishart random variables from i.i.d. Gaussian
random variables. Exact posterior inference for these models is intractable  and so previous authors
have suggested approximate inference routines based on Markov chain Monte Carlo (MCMC) algo-
rithms. We instead propose a gradient-based variational inference routine  derived from approaches
to approximate inference with (sparse and/or multi-output) Gaussian process models. Taking the
variational approach has several advantages including a simple  black-box implementation and the
ability to scale down the computational cost of inference with respect to N  the length of the time
series  if required. Furthermore  we derive a factored variant of the model that may additionally scale
inference to large numbers of variables D  i.e.  the dimensionality of the covariance matrix. In our

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

experiments  we will see that our black-box  scalable  gradient-based variational inference routines
have predictive performance that is competitive with multivariate GARCH.
We start by considering variational inference routines for the model presented by Wilson and Ghahra-
mani [24]; our approach is a gradient-based analogue of the coordinate ascent algorithms for varia-
tional inference on Wishart processes presented by van der Wilk et al. [23]. The factored variants of
the model that we build toward in Section 5 end up being a reparameterization of the construction by
Fox and Dunson [6]  and so our work provides gradient-based variational inference routines for their
model class as well. Alternatively  Fox and West [7] construct inverse Wishart processes that are
autoregressive (as opposed to the full process dependence assumed by the Gaussian processes); Wu
et al. [26] model time series of (univariate) variances with Gaussian process models; and Wu et al.
[25] consider generalizing multivariate GARCH by modeling the transition matrices of the process
with autoregressive structures. All of these references elect MCMC-based inference and emphasize
that Bayesian inference of (co)variances dominate non-Bayesian approaches.

2 Wishart and inverse Wishart processes
Let Y := (Yn  n ≥ 1) denote a sequence of measurements in RD  which will be regressed upon a
corresponding sequence of input locations (i.e.  covariates) in Rp denoted by X := (Xn  n ≥ 1).
In our applications  we will take Xn to be a univariate (so p = 1)  real-valued representation of the
“time” at which the measurement Yn was taken. For example  in a dataset of daily stock returns 
the vector Yn can record the returns for D stocks on day n  for n ≤ N  where the points in X can
be linearly spaced in some ﬁxed interval like (0  1)  and individual points of X may be altered to
account for any irregular spacing  such as weekends or the removal of special trading days.
We let the conditional likelihood of Yn be given by the multivariate Gaussian density

n ≥ 1 

Yn | µn  Σn ∼ N (µn  Σn) 

(1)
for a sequence µ1  µ2  . . . of elements in RD and a sequence Σ1  Σ2  . . . of (random) positive deﬁnite
matrices  which we note may depend on X. In modern portfolio theory [15]  where Yn is a sequence of
ﬁnancial returns  predictions for the mean process µ1  µ2  . . . are used in conjunction with predictions
for the covariances of the residuals Σ1  Σ2  . . . to construct a portfolio that maximizes expected
return while minimizing risk. In this article  we will focus on modeling the process Σ1  Σ2  . . .   and
we henceforth assume that Yn is mean zero (i.e.  µn = 0  the zero vector)  for n ≤ N.
Bayesian models for the sequence Σ := (Σ1  Σ2  . . . ) include the Wishart and inverse Wishart
processes. In analogy to the construction of Wishart and inverse Wishart random variables from i.i.d.
collections of Gaussian random variables  we may construct Wishart and inverse Wishart processes
from i.i.d. collections of Gaussian processes as follows. Let

fd k ∼ GP(0  κ(·   · ; θ)) 

(2)
be i.i.d. Gaussian processes with zero mean function and (shared) kernel function κ(·   · ; θ)  where θ
denotes any parameters of the kernel function  and the positive integer-valued ν ≥ D will be called
the degrees of freedom parameter. Let Fn d k := fd k(Xn)  and let Fn := (Fn d k  d ≤ D  k ≤ ν)
denote the D × ν matrix of collected function values  for every n ≥ 1. Construct

d ≤ D  k ≤ ν 

Σn = AFnF T

(3)
where A ∈ RD×D satisﬁes the condition that the symmetric matrix AAT is positive deﬁnite.1 So
constructed  Σn is (marginally) Wishart distributed  and Σ := (Σ1  Σ2  . . . ) is correspondingly called
a Wishart process with degrees of freedom ν and scale matrix AAT . Alternatively  if we instead
construct the precision matrix

n AT  

n ≥ 1 

Σ−1
n = AFnF T

n AT  

n ≥ 1 

(4)

then Σn is inverse Wishart distributed  and Σ is called an inverse Wishart process (with degrees of
freedom ν and scale matrix AAT ). The dynamics of the process of covariance matrices Σ are inherited
by the Gaussian processes  which are perhaps best controlled by the kernel function κ(·  · ; θ).

1Alternatively  we may take A to be the (triangular) cholesky factor of a positive deﬁnite matrix AAT .

2

The posterior distribution for Σ is difﬁcult to evaluate  and so previous MCMC-based approaches to
approximate inference typically utilize conjugacy results between the (inverse) Wishart distribution
and the likelihood function in Eq. (1). In contrast  the “black-box” variational inference routines
that we suggest only require evaluations of the log conditional likelihood function  dramatically
simplifying their implementation. For the Wishart process case  we have

log p(Yn | Fn) = − D
2

log(2π) − 1
2

log |AFnF T

n AT| − 1
2

Y T
n (AFnF T

n AT )−1Yn 

and for the inverse Wishart case  we have

log p(Yn | Fn) = − D
2

log(2π) +

1
2

log |AFnF T

n AT| − 1
2

Y T
n AFnF T

n AT Yn.

(5)

(6)

Changing our implementation between these two likelihood models only requires changing the
line(s) of code computing these expressions  highlighting the ease of the black-box approach. Other
likelihoods may be considered; for example  Eq. (5) and Eq. (6) may be replaced by the likelihood
function for a multivariate t-distribution (as done by Wu et al. [25])  a popular heavy-tailed model.

3

Inducing points and variational inference

A popular approach to variational inference with Gaussian processes is based on the introduction
of M inducing points Z := (Z1  . . .   ZM )  taking values in the same space as the inputs X  upon
which we assume the dependence of the function values Fn decouple during inference [1  9  18]. In
particular  for every d ≤ D and k ≤ ν  let Um d k := fd k(Zm)  for m ≤ M  denote the evaluations
of the Gaussian process at the inducing points  and collectively denote Ud k := (Um d k  m ≤ M )
and Fd k := (Fn d k  n ≤ N ). By independence  and with well-known properties of the Gaussian
distribution  we may write

p(Y  F  U ) =

p(Yn | Fn)

p(Fd k | Ud k)p(Ud k)

 

(7)

N(cid:89)

(cid:104)

(cid:105) D(cid:89)

ν(cid:89)

(cid:104)

n=1

d=1

k=1

(cid:105)

where

xz) 

zz K T

zz Ud k  Kxx − KxzK−1

p(Fd k | Ud k) = N (Fd k; KxzK−1
p(Ud k) = N (Ud k; 0  Kzz) 

(8)
(9)
and where the N × N matrix Kxx has (n  n(cid:48))-th element κ(Xn  Xn(cid:48); θ)  the N × M matrix Kxz has
(n  m)-th element κ(Xn  Zm; θ)  and the M × M matrix Kzz has (m  m(cid:48))-th element κ(Zm  Zm(cid:48); θ).
Following Hensman et al. [10]  we introduce a variational approximation to the posterior distribution
of the latent variables that takes the following form: Independently for every d ≤ D and k ≤ ν  let
(10)
for some variational parameters µd k ∈ RM and Sd k ∈ RM×M a real  symmetric  positive deﬁnite
matrix. It follows that

q(Fd k  Ud k) = p(Fd k | Ud k)q(Ud k)  where q(Ud k) = N (Ud k; µd k  Sd k) 

p(Fd k | Ud k)q(Ud k)dUd k = N (Fd k; ˜Kµd k  Kxx + ˜K(Sd k − Kzz) ˜K T ) 

q(Fd k) =
where ˜K := KxzK−1
q(Fd k). We may then lower bound the log marginal likelihood of the data as follows:

zz . That is  the variational approximation q(Ud k) induces the approximation

(11)

(cid:90)

log p(Y ) ≥ N(cid:88)

Eq(Fn)[log p(Yn | Fn)] − D(cid:88)

ν(cid:88)

KL[q(Ud k)|| p(Ud k)] 

(12)

n=1

d=1

k=1

where KL[q || p] denotes the Kullback–Leibler divergence from q to p. To perform inference  we
maximize the evidence lower bound on the right hand side of Eq. (12)—which we note depends on
the parameters to be optimized Θ := {Z  µ  S  θ} through the variational distribution q—via gradient
ascent. The terms KL[q(Ud k)|| p(Ud k)] may be analytically evaluated and so their gradients (w.r.t.
the optimization parameters) are straightforward to compute. Because q(F ) is not conjugate to the
likelihood p(Y | F )  we cannot analytically evaluate the term Eq(Fn)[log p(Yn | Fn)]. We therefore

3

follow Salimans and Knowles [21] and Kingma and Welling [12] to approximate the gradients of
(Monte Carlo estimates of) this expression by “differentiating through” random samples from q as
follows. Independently for every d ≤ D and k ≤ ν  produce the R ≥ 1 Monte Carlo samples

F (r)
d k = ψd k(w(r)

d k; Θ)  w(r)

d k ∼ N (0  IN ) 

r = 1  . . .   R 

(13)

where ψd k(w; Θ) := Bd kw + ˜Kµd k  for the matrix Bd k ∈ RN×N that satisﬁes Bd kBT
d k =
Kxx + ˜K(Sd k − Kzz) ˜K T   as given by the Cholesky factor. Note then that the samples generated
according to Eq. (13) have distribution q(Fd k). Form the Monte Carlo approximations

∇(µd k Sd k)Eq(Fn)[log p(Yn | Fn)] ≈ 1
R
for every d ≤ D and k ≤ ν  where ◦ denotes the element-wise product  and
R(cid:88)

(cid:104)∇Fd k log p(Yn | F (r)
(cid:104)∇Fd k log p(Yn | F (r)
ν(cid:88)

∇(Z θ)Eq(Fn)[log p(Yn | Fn)] ≈ 1
R

R(cid:88)
D(cid:88)

r=1

r=1

d=1

k=1

(cid:105)

(cid:105)

n ) ◦ ∇(µd k Sd k)ψd k(w(r)

d k; Θ)

 

n ) ◦ ∇(Z θ)ψd k(w(r)

d k; Θ)

.

(cid:88)

These unbiased estimates often have low enough variance that a single Monte Carlo sample sufﬁces
for the approximation [21]  however  we will see in Section 4 that this is not the case with the
Wishart process  where a numerical instability renders these estimates useless. Finally  the gradients
of the lower bound on the right hand side of Eq. (12) with respect to µd k and Sd k may then be
approximated by the unbiased estimator

(cid:104)∇(µd k Sd k)Eq(Fn)[log p(Yn | Fn)]
(cid:104)∇(Z θ)Eq(Fn)[log p(Yn | Fn)]
(cid:88)

(cid:105) − ∇(µd k Sd k)KL[q(Ud k)|| p(Ud k)] 
(cid:105) − D(cid:88)

∇(Z θ)KL[q(Ud k)|| p(Ud k)].

where B ⊆ {(Xn  Yn) : n ≤ N} is a minibatch of the datapoints. Likewise  the gradients with
respect to Z and θ may be approximated by

ν(cid:88)

N
|B|

(14)

(15)

n∈B

N
|B|

n∈B

d=1

k=1

With the gradient approximations in Eqs. (14) and (15)  gradient ascent may now be carried out with
a Robbins–Monro stochastic approximation routine.
We can see that an immediate beneﬁt of taking this black-box variational approach is the ease of
switching between the Wishart and inverse Wishart processes  requiring only a switch between the
appropriate log conditional likelihood function log p(Yn | Fn)  given by Eq. (5) or Eq. (6)  in the
subroutine computing Eqs. (14) and (15). This implementation is particularly easy with GPﬂow [16] 
a Gaussian process toolbox built on Tensorﬂow  as demonstrated with a code snippet in the Appendix.
Note that choosing M = N and ﬁxing the locations of the inducing points Z at the inputs X results
in a Wishart or inverse Wishart process model that captures full temporal dependence among the N
measurements. In this case  the evaluations of log p(Yn | Fn) in Eqs. (5) and (6) have computational
complexity and memory requirements with respect to N scaling in O(N 3) and O(N 2)  respectively.
However  another (equally important) advantage of the inducing point formulation and the variational
approach to inference is the ability to reduce this computational burden with respect to N  if needed.
In particular  by selecting M (cid:28) N  we end up with sparse approximations to the Gaussian processes.
For simplicity  assume that ν = D. In this case  producing a Monte Carlo sample from q(Fd k) for the
minibatch B scales in O(N 3
b +NbM +M 2) space  where Nb := |B|.
Producing this for every d ≤ D  k ≤ ν  together with the computation of log p(Yn | Fn)  results in
an overall computation in O(NbD3 + D2(N 3
b + NbM + M 2))
space. Note that all of these complexities scale linearly with the number of samples R used for the
Monte Carlo approximations in Eqs. (14) and (15).

b + NbM 2 + M 3)) time and O(D2(N 2

b +NbM 2 +M 3) time and O(N 2

4 The additive white noise model

In our initial experiments  we found that the inverse Wishart parameterization successfully moved
the parameters into a good region of the state space  whereas the Wishart process failed to move

4

(a) Wishart process (horizontal axis on a log-scale)

(b) Additive white noise Wishart process

Figure 1: Histograms of 1 000 Monte Carlo samples of the gradient with respect to the variable F1 1 1
in a univariate model. Fig. 1(a) shows an extremely skewed distribution in the case of the Wishart
process  and Fig. 1(b) shows its correction under the additive white noise reparameterization. The
mean of each distribution is shown as a red horizontal line.

n (AFnF T

the parameters in the correct direction (based on traceplots of parameters and validation metrics). It
appears that this failure is due to extremely high variance of the Monte Carlo gradient approximation
routine. By studying the log-likelihood function for the Wishart process in Eq. (5)  we hypothesize
that evaluating the inverse in the ﬁnal term  − 1
n AT )−1Yn  on Monte Carlo samples
of Fn (as required by the procedure described in Section 3) is problematic because those samples
can often be close to the origin  resulting in this quantity being extremely large in magnitude. For
example  in the case of a univariate output  i.e.  D = 1  and corresponding unit scale A = 1  the
likelihood involves computation of the scalar term − 1
n  which is large in magnitude for samples
when fn is closer to zero than the data point yn  a problem that is exacerbated by the quadratic scales.
To visualize this issue  consider a bivariate output Yn with constant covariance matrix Σn =
[[2.0  1.9]  [1.9  2.0]] and A = [[1  0]  [0  1]]. We let ν = D = 2 and simulated a dataset Yn at
input locations Xn  for n ≤ 30  which together with some inducing points Zm  m ≤ 10  are sampled
uniformly in (0  1). As described in Section 3  we compute the following 1 000 samples:
d ≤ 2  k ≤ 2  n ≤ 30  r ≤ 1000 

∇Fd k log p(Yn | F (r)

d k ∼ q(Fd k) 

n )  F (r)

n/f 2

(16)

2 Y T

2 y2

and we display a histogram of the samples corresponding to the variable F1 1 1 in Fig. 1(a)  where
the horizontal axis is on a log scale. The distribution is extremely skewed; the mean of these samples 
at around 2.5 × 109  is plotted as a red vertical line  and the standard deviation is 7.9 × 1010!
To resolve this issue  consider once again the case when D = 1 with unit scale A = 1. We can modify
the previously problematic scalar term to be − 1
n + λ)  where the denominator is shifted away
from zero by a parameter λ > 0. More generally  we can accomplish this with a slightly generalized
construction to that studied by van der Wilk et al. [23]: Construct the covariance matrix of yn as

n/(f 2

2 y2

n AT + Λ  n ≥ 1 

Σn := AFnF T

(17)
where Λ is a diagonal D × D matrix with positive (diagonal) entries. To interpret this modiﬁcation 
note that the model in Section 2 may be alternatively written as yn = AFnzn  where zn ∼ N (0  Iν) 
for n ≥ 1   so that Cov(yn|Fn) = AFnF T
n AT . The modiﬁed construction may be instead written as
(18)

zn ∼ N (0  Iν) 

εn ∼ N (0  Λ) 

yn = AFnzn + εn 

n ≥ 1 

and so Cov(yn|Fn) = AFnF T
n AT +Λ. This modiﬁcation may therefore be interpreted as introducing
white (or observational) noise to the model. The log conditional likelihood in Eq. (5) is replaced by

log p(Yn | Fn) =

N D

2

log(2π) − log |AFnF T

n AT + Λ| − 1
2

Y T
n (AFnF T

n AT + Λ)−1Yn.

(19)

The approximated gradients may now be stably computed: In Fig. 1(b)  we plot a histogram of the
samples of the gradients in Eq. (16) for this modiﬁed model  where Λ = [[0.01  0.0]  [0.0  0.01]].

5

e10e5e0e5e10e15e20e25e300102030405060700.030.020.010.000.010.020.03050100150200250n AT + Λ−1 

Σ−1
n := AFnF T

While the inverse Wishart case does not suffer such computational issues  we will see in Section 5 that
this additive white noise modiﬁcation is the key to a factored variant of both the Wishart and inverse
Wishart processes  inference for which is tractable for high-dimensional covariance matrices. In the
inverse Wishart case  however  a useful additive white noise modiﬁcation is not easy to implement.
We consider instead the following construction for the precision matrix
n ≥ 1 

(20)
where  as a diagonal matrix  Λ−1 contains the inverted elements on the diagonal of Λ. If the variables
in yn are independent  then the elements of Λ retain their interpretation as (the variances of) additive
white noise. More generally  they have an interpretation as additive terms to the partial variances of
the variables in yn. The log conditional likelihood in Eq. (6) is now replaced by
log p(Yn | Fn) =
n AT + Λ−1| − 1
n AT + Λ−1)Yn. (21)
2
d d ∼ gamma(a  b)  d ≤ D  for some
The elements of Λ share an inverse gamma prior distribution Λ−1
a  b > 0. We ﬁt a mean-ﬁeld variational approximation with an analogous approach to the methods
in Section 3 for gamma random variables described by Figurnov et al. [5]. (Alternative approaches
were described by Knowles [13] and Ruiz et al. [20].) We ﬁt a and b by maximum likelihood.

log(2π) + log |AFnF T

Y T
n (AFnF T

N D

2

5 Factored covariance models

The computational and memory requirements of inference in the models so far presented scale with
respect to D in O(D3) and O(D2)  respectively  since we must invert (or take the determinant of) a
D × D matrix. This will become intractable for even moderate values of D  which is particularly
troublesome in applications like ﬁnance where D could  for example  represent the number of ﬁnancial
instruments in a large stock market index like the S&P 500. To reduce this complexity  consider
ﬁxing some K (cid:28) D and reducing Fn to be of size K × ν  for some ν ≥ K. The matrix FnF T
n is
a K × K Wishart-distributed matrix. Let A now be of size D × K. Then by a scaling property of
the Wishart distribution [19  p. 535]  the D × D matrix AFnF T
n AT is also Wishart-distributed. This
factor-like  low-rank model has signiﬁcantly fewer parameters than those in Sections 2 and 4.
Consider applying this construction to the additive white noise model for the Wishart process
described in Section 4  where Σn = AFnF T
n AT +Λ. Recalling that Λ is diagonal  the log conditional
likelihood function in Eq. (19) may be computed efﬁciently with the Woodbury matrix identities as

log p(Yn | Fn) =

N D

log Λd d +

log |Iν + F T

n AT Λ−1AFn|

1
2

(22)

Y T
n AFn(Iν + F T

n AT Λ−1AFn)−1F T

n AT Yn.

n AT +Λ−1  which we note is a reparameterization
In the inverse Wishart case  we have Σ−1
of the construction by Fox and Dunson [6]. The log conditional likelihood function in this case is

n = AFnF T

D(cid:88)

d=1
1
2

log(2π) − 1
2
n Λ−1Yn +
Y T

2
− 1
2

log(2π) +

D(cid:88)
n Λ−1Yn − 1
Y T
2

1
2

d=1

2
− 1
2

log p(Yn | Fn) =

N D

log Λd d − 1
2

log |Iν + F T

n AT ΛAFn|

(23)

Y T
n AFnF T

n AT Yn.

For simplicity  assume ν = K. Then these log conditional likelihood functions may be computed
(with respect to D and K) in O(DK 2) time and O(DK) space. With the black-box approach to
variational inference  we need only drop the expressions in Eqs. (22) and (23) into the subroutines
computing the gradient estimates in Eqs. (14) and (15). The overall complexity then reduces to
computations in O(NbDK 2 + K 2(N 3
b + NbM + M 2))
space. This model and inference procedure is therefore scalable to both large N and D regimes.

b + NbM 2 + M 3)) time and O(DK + K 2(N 2

6 Experiments on ﬁnancial returns

We implement our variational inference routines on the model variants applied to three datasets of
ﬁnancial returns  which are denoted as follows (note that we take the log returns  which are deﬁned
at time t + 1 as log(1 + Pt+1/Pt)  where Pt is the price of the instrument at time t):

6

Dow 30: Intraday returns on the components of the Dow 30 Industrial Average (as of the changes on
Jun. 8  2009)  taken at the close of every ﬁve-minute interval from Nov. 17  2017 through Dec. 6 
2017. The resulting dataset size is N = 978  D = 30. The raw data was from Marjanovic [14].
FX: Daily foreign exchange rates for 20 currency pairs taken from Wu et al. [26]. The dataset size is
N = 1  565  D = 20.
S&P 500: Daily returns on the closing prices of the components of the S&P 500 index between
Feb. 8  2013 through Feb. 7  2018  taken from Nugent [17]. Missing prices are forward-ﬁlled. The
resulting dataset size is N = 1  258  D = 505 (there are 505 names in the index).
The simplest baseline is univariate ARCH (applied to each variable independently)  implemented
through the Python package arch [22]. The MGARCH variants we compare to are the dynamic
conditional correlation model (DCC) [3] with Gaussian and multivariate-t likelihoods  and the gener-
alized orthogonal garch model (GO-GARCH) [2]  a competitive variant of the BEKK MGARCH
speciﬁcation. These baselines are among the dominant MGARCH modeling approaches and were
implemented through the R package rmgarch [8]. The MGARCH baselines do not scale to the S&P
500 dataset  and there are no ubiquitous baselines in this large covariance regime.
We used a diagonal matrix A for the full-rank (non-factored) covariance models. The parameters
in A and Λ are inferred by maximum likelihood. The values of Λ were initialized to Λd d = 0.001 
d ≤ D. The degrees of freedom parameter ν is set to the number of variables D  or the number of
factors K in the factored covariance cases. We did not ﬁnd performance to be sensitive to this choice.
We used M = 300 inducing points  R = 2 variational samples for the Monte Carlo approximations 
and a minibatch size of 300. The gradient ascent step sizes were scheduled according to Adam [11].
We selected the stopping times and an exponential learning rate decay schedule via cross validation 
choosing the setting that maximized the test loglikelihood metric (see below) on a validation set. The
validation sets were the ﬁnal 2%  5%  and 5% of the measurements in just one of the training sets for
the Dow 30  FX  and S&P 500 datasets  respectively.
For each dataset  we created 10 evenly-sized training and testing sets with a sliding window  where
the test set comprises 10 consecutive measurements following the training set (we may therefore
consider a 10-step-ahead forecasting task)  and no testing sets overlap. To evaluate the models  we
t at horizon t  for t ≤ 10—and compute the log-likelihood
forecast the covariance matrix—say  Σ∗
of the corresponding test measurement Yt under a mean-zero Gaussian distribution with covariance
Σ∗
t . The prediction is formed by Monte Carlo estimation with 300 samples from the ﬁtted variational
distribution. The parameter settings producing the prediction with the highest training log-likelihood
from among a window of 300 steps following the stopping time is kept for testing.
In order to visualize our experimental setup  we display the results for the FX dataset in Fig. 2 as a
series of grouped histograms. The horizontal axis represents the forecast horizon; at each horizon 
the boxplot of test-loglikelihoods (over the 10 training/testing sets) are displayed for each model.
The Wishart and inverse Wishart process variants are denoted by ‘wp’ and ‘iwp’  respectively. If the
additive white noise parameterization described in Section 4 is used (with a non-factored covariance
model)  we prepend the model name with ‘n-’. The factored model variants  described in Section 5 
have model names prepended with ‘f[K]-’  where [K] is the number of factors. We used a Gaussian
process covariance kernel composed as the sum of a Matern 3/2 kernel  a rational quadratic kernel  a
radial basis function kernel  and a periodic component  which is itself composed as the product of a
periodic kernel and a radial basis function kernel (see the code snippet in the Appendix). The ARCH
baseline is denoted ‘arch’  the DCC baselines with a multivariate normal and multivariate-t likelihood
are denoted ‘dcc’ and ‘dcc-t’  respectively  and the GO-GARCH baseline is denoted ‘go-garch’.
We compare the collections of the log-likelihood scores for each of the 10 forecast horizons in each
of the 10 test sets (resulting in populations of 100 scores each). In Table 1  we report the mean score
± one standard deviation for each model and dataset. For each of our model variants  we provide in
brackets the p-value of a Wilcoxon signed-rank test comparing the performance of the model against
the highest performing MGARCH baseline (which was always either go-garch or dcc-t) or the ARCH
baseline in the case of the S&P 500 dataset. We bold the highest performing model on each dataset 
and we highlight any improvements with a ∗ if signiﬁcant at a 0.05 level.
The Wishart process variants score highest on each dataset; it is notable that they consistently
outperform their inverse Wishart process counterparts. In fact  the inverse Wishart process appears to
have unreliable performance; while the iwp variants outperform MGARCH on the Dow 30 dataset 

7

Figure 2: Example display of the results for the FX dataset. A set of boxplots reporting the test
loglikelihoods of the predictions is displayed for each step of the 10-step forecast horizon (indicated
on the horizontal axis). Each boxplot contains the scores from the 10 training/testing splits.

Table 1: Test loglikelihood metrics across 10-step forecast horizons in 10 test splits. We display the
mean over the 100 scores  along with ± one std. dev. The p-value of a Wilcoxon signed-rank test
comparing our models to the highest performing MGARCH/ARCH baseline is displayed in brackets.
The highest score is bolded. Signiﬁcant improvements at a 0.05 level are highlighted with a ∗.

Dow 30
142.47 ± 17.97
162.70 ± 42.98
162.64 ± 42.86
163.59 ± 52.65
164.09 ± 26.47∗ (1.71e-8)
164.49 ± 19.82∗ (1.13e-9)
165.98 ± 23.23∗ (1.03e-6)
162.28 ± 22.91∗ (4.67e-11)
165.39 ± 30.89∗ (2.31e-5)
–
–

FX
68.24 ± 7.55
82.52 ± 4.55
82.54 ± 4.56
82.43 ± 4.85
81.42 ± 4.12 (8.15e-8)
82.10 ± 3.72 (1.62e-3)
82.69 ± 4.15 (5.11e-2)
77.76 ± 3.94 (5.99e-17)
81.12 ± 3.59 (2.62e-10)
–
–

S&P 500
1358.23 ± 355.12
–
–
–
–
–
–
1275.27 ± 264.48 (4.14e-18)
1423.31 ± 132.19∗ (1.48e-13)
1047.73 ± 1436.16 (3.90e-18)
1438.40 ± 130.14∗ (1.54e-15)

arch
dcc
dcc-t
go-garch
iwp
n-iwp
n-wp
f10-iwp
f10-wp
f30-iwp
f30-wp

they perform poorly on the FX and S&P 500 datasets. On the Dow 30 dataset  not only does the
(additive noise  full covariance) Wishart process (denoted n-wp) signiﬁcantly outperform the best
performing MGARCH baseline (go-garch  in this case)  but every other one of our full covariance
models and f10-wp does as well. The f10-iwp model variant is the only one of our models that
underperforms go-garch  further emphasizing that the inverse Wishart process should be avoided.
While n-wp attains the highest score on the FX dataset  it is not deemed signiﬁcant over the scores for
the highest performing MGARCH baseline (dcc-t in this case)  according to the Wilcoxon signed-rank
test. However  we may take some comfort in the fact that the p-value of 5.11e-2 (comparing the
scores of n-wp and dcc-t) is very close to signiﬁcance at this level. We note  however  that every other
one of our models under-performs compared to dcc-t. The MGARCH baselines cannot scale to the
S&P 500 dataset  and so we may only compare our factored covariance models against the diagonal
ARCH baseline. The f30-wp and f10-wp models both signiﬁcantly outperform the ARCH baseline 
however  worryingly  the f30-iwp and f10-iwp model variants signiﬁcantly under-perform the ARCH
baseline  giving yet another example of the unreliable performance of the inverse Wishart process.
With this evidence  we recommend a practitioner to always implement the Wishart process instead of
the inverse Wishart process. Further study should be undertaken to understand why the performance
of these two model variants differ. Unsurprisingly  the additive noise parameterization always
improves performance (n-iwp always outperforms iwp)  which we may attribute to the additional
noise parameters afforded to the model in this case. While we see the factored Wishart process
perform competitively with its full covariance counterpart on the Dow 30 dataset  this was the not the
case on the FX dataset  and so (unsurprisingly) a full covariance model should be preferred if enough
computational resources are available.

8

12345678910horizon5060708090test_llarchdccdcc-tgo-garchiwpn-iwpn-wpf10-iwpf10-wp7 Conclusion

We conclude that the black-box variational approach to inference signiﬁcantly eases the implementa-
tion of the various Wishart and inverse Wishart process models that we have presented. If needed 
the computational burden of inference with respect to both the length of the time series and the
dimensionality of the covariance matrix may be reduced. We hope that the initial failure of the
black-box variational approach in the case of the Wishart process provides a warning to practitioners
that these methods cannot always be trusted to work out of the box. We recommend that practitioners
always implement the (additive noise) Wishart process instead of the inverse Wishart process. When
the dimensionality of the covariance matrix is large  one may use the factored model  however  a full
covariance model should be preferred if computational resources will allow it.

Appendix: Implementation in GPﬂow

To demonstrate the ease of implementing our methods  we provide the following 25 lines of Python
code implementing the inverse Wishart process in GPﬂow [16] (version 1.3.0):
i m p o r t numpy a s np
i m p o r t
from gpflow i m p o r t models  

l i k e l i h o o d s   k e r n e l s   params  

t r a n s f o r m s   d e c o r s

t e n s o r f l o w a s

t f

c l a s s

I n v W i s h a r t P r o c e s s L i k e l i h o o d ( l i k e l i h o o d s . L i k e l i h o o d ) :

d e f _ _ i n i t _ _ ( s e l f   D  R=1) :

s u p e r ( ) . _ _ i n i t _ _ ( )
s e l f . R  
s e l f . A_diag = params . P a r a m e t e r ( np . ones (D)  

s e l f .D = R   D

t r a n s f o r m = t r a n s f o r m s . p o s i t i v e )

@decors . p a r a m s _ a s _ t e n s o r s
d e f v a r i a t i o n a l _ e x p e c t a t i o n s ( s e l f   mu   S   Y) :

# d e c o r a t o r

t r a n s l a t i n g TF t e n s o r s

f o r GPflow

N  D = t f . s ha pe (Y)
W = t f . random_normal ( [ s e l f . R   N 
F = W ∗ ( S ∗∗ 0 . 5 ) + mu

# s a m p l e s

t f . sh ap e (mu) [ 1 ] ] )

t h r o u g h which TF a u t o m a t i c a l l y d i f f e r e n t i a t e s

t h e )

l i k e l i h o o d

# compute t h e ( mean o f
[ s e l f . R   N  D  −1])
AF = s e l f . A_diag [ :   None ] ∗ t f . r e s h a p e ( F  
y f f y = t f . reduce_sum ( t f . einsum ( ’ jk   i j k l −> i j l ’   Y  AF) ∗∗ 2 . 0  
c h o l s = t f . c h o l e s k y ( t f . matmul (AF   AF  
l o g p = t f . reduce_sum ( t f . l o g ( t f . m a t r i x _ d i a g _ p a r t ( c h o l s ) )   a x i s =2) − 0 . 5 ∗ y f f y
r e t u r n t f . reduce_mean ( logp  

t r a n s p o s e _ b = True ) )

a x i s =−1)

a x i s =0)

# c h o l e s k y o f p r e c i s i o n

c l a s s

I n v W i s h a r t P r o c e s s ( models . svgp . SVGP) :

d e f _ _ i n i t _ _ ( s e l f   X  Y  Z   m i n i b a t c h _ s i z e =None   nu=None ) :

D = Y. sh ap e [ 1 ]
nu = D i f nu i s None e l s e nu

# d e g r e e s o f

freedom

# c r e a t e a c o m p o s i t i o n a l k e r n e l
k e r n = k e r n e l s . Matern32 ( 1 ) + k e r n e l s . R a t i o n a l Q u a d r a t i c ( 1 ) + k e r n e l s . RBF ( 1 )

f u n c t i o n

+ k e r n e l s . P e r i o d i c K e r n e l ( 1 ) ∗ k e r n e l s . RBF ( 1 )

\ \

# a l m o s t
s u p e r ( ) . _ _ i n i t _ _ (X  Y  Z = Z   k e r n = kern  

a l l work i s done by SVGP!

# n o t a t i o n a s

l i k e l i h o o d = I n v W i s h a r t P r o c e s s L i k e l i h o o d (D  R=10)  
n u m _ l a t e n t = D ∗ nu  
m i n i b a t c h _ s i z e = m i n i b a t c h _ s i z e )

# number o f o u t p u t s

i n t h e p a p e r
( m u l t i−o u t p u t GP)

# 10 MCMC s a m p l e s

GPﬂow’s abstract class gpflow.models.svgp.SVGP is designed to automate gradient-based vari-
ational inference with (sparse) Gaussian process models. Our only model-speciﬁc computation is
for the Monte Carlo approximations of the log-likelihood expression in Eq. (6)  which is carried out
by the method InvWishartProcessLikelihood.variational_expectations. The Tensorﬂow
backend automatically differentiates through these expressions to obtain the gradients described in
Section 3. Finally  note that a kernel function is being deﬁned from a composition of several simpler
kernel functions  demonstrating one of the many utilities of GPﬂow; this is the particular composition
used in our experiments in Section 6.

Acknowledgements

We thank anonymous reviewers for feedback. All funding for the experiments were personally
provided by CH  who does not have an afﬁliation for this work.

9

References
[1] M. Bauer  M. van der Wilk  and C. E. Rasmussen. Understanding probabilistic sparse Gaussian

process approximations. In NIPS  2016.

[2] R. Van der Weide. GO-GARCH: a multivariate generalized orthogonal GARCH model. Journal

of Applied Econometrics  17(5):549–564  2002.

[3] R. Engle. Dynamic conditional correlation: A simple class of multivariate generalized autore-
gressive conditional heteroskedasticity models. Journal of Business & Economic Statistics  20
(3):339–350  2002.

[4] R. F. Engle and K. F. Kroner. Multivariate simultaneous generalized ARCH. Econometric

Theory  11(1):122–150  1995.

[5] M. Figurnov  S. Mohamed  and A. Mnih. Implicit reparameterization gradients. In NIPS  2018.

[6] E. B. Fox and D. B. Dunson. Bayesian nonparametric covariance regression. Journal of Machine

Learning Research  16:2501–2542  2015.

[7] E. B. Fox and M. West. Autoregressive models for variance matrices: Stationary inverse Wishart

processes. arXiv preprint arXiv:1107.5239  2011.

[8] A. Ghalanos. rmgarch: Multivariate GARCH models  2014. URL https://cran.r-project.

org/web/packages/rmgarch. R package version 1.2-8.

[9] J. Hensman  N. Fusi  and N. D. Lawrence. Gaussian processes for big data. In UAI  2013.

[10] J. Hensman  A. G. de G. Matthews  and Z. Ghahramani. Scalable variational Gaussian process

classiﬁcation. In AISTATS  2015.

[11] D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. In ICLR  2015.

[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR  2014.

[13] D. A. Knowles. Stochastic gradient variational Bayes for gamma approximating distributions.

arXiv preprint arXiv:1509.01631  2015.

[14] Boris Marjanovic. Huge stock market dataset. Kaggle.com  Nov. 2017. URL https://www.
kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs. Ver-
sion 3. Last updated 11/10/2017.

[15] H. Markowitz. Portfolio selection. The Journal of Finance  7(1):77–91  1952.

[16] A. G. de G. Matthews  M. van der Wilk  T. Nickson  K. Fujii  A. Boukouvalas  P. León-Villagrá 
Z. Ghahramani  and J. Hensman. GPﬂow: A Gaussian process library using TensorFlow.
Journal of Machine Learning Research  18(40):1–6  2017.

[17] Cam Nugent. S&p 500 stock data. Kaggle.com  Feb. 2018. URL https://www.kaggle.com/

camnugent/sandp500. Version 4.

[18] J. Quiñonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian

process regression. Journal of Machine Learning Research  6:1939–1959  2005.

[19] C. R. Rao. Linear statistical inference and its applications  volume 2. Wiley New York  1973.

[20] F. R. Ruiz  M. Titsias  and D. Blei. The generalized reparameterization gradient. In NIPS  2016.

[21] T. Salimans and D. A. Knowles. Fixed-form variational posterior approximation through

stochastic linear regression. Bayesian Analysis  8(4):837–882  2013.

[22] K. Sheppard. Arch  2014. URL https://github.com/bashtage/arch. Python package

version 4.3.1.

[23] M. van der Wilk  A. G. Wilson  and C. E. Rasmussen. Variational inference for latent variable
modelling of correlation structure. In NIPS 2014 Workshop on Advances in Variational Inference 
2014.

10

[24] A. G. Wilson and Z. Ghahramani. Generalised Wishart processes. In UAI  2010.

[25] Y. Wu  J. M. Hernández-Lobato  and Z. Ghahramani. Dynamic covariance models for multivari-

ate ﬁnancial time series. In ICML  2013.

[26] Y. Wu  J. M. Hernández-Lobato  and Z. Ghahramani. Gaussian process volatility model. In

NIPS  2014.

11

,Creighton Heaukulani
Mark van der Wilk