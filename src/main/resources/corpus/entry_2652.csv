2019,Information Competing Process for Learning Diversified Representations,Learning representations with diversified information remains as an open problem. Towards learning diversified representations  a new approach  termed Information Competing Process (ICP)  is proposed in this paper. Aiming to enrich the information carried by feature representations  ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task. By fusing representation parts learned competitively under different conditions  ICP facilitates obtaining diversified representations which contain rich information. Experiments on image classification and image reconstruction tasks demonstrate the great potential of ICP to learn discriminative and disentangled representations in both supervised and self-supervised learning settings.,Information Competing Process for Learning

Diversiﬁed Representations

Jie Hu12  Rongrong Ji123∗  ShengChuan Zhang1  Xiaoshuai Sun1 

Qixiang Ye4  Chia-Wen Lin5  Qi Tian6.

1Media Analytics and Computing Lab  Department of Artiﬁcial Intelligence 

School of Informatics  Xiamen University.

2National Institute for Data Science in Health and Medicine  Xiamen University.

3Peng Cheng Laboratory. 4University of Chinese Academy of Sciences.

5National Tsing Hua University. 6Noah’s Ark Lab  Huawei.

Abstract

Learning representations with diversiﬁed information remains as an open problem.
Towards learning diversiﬁed representations  a new approach  termed Information
Competing Process (ICP)  is proposed in this paper. Aiming to enrich the informa-
tion carried by feature representations  ICP separates a representation into two parts
with different mutual information constraints. The separated parts are forced to
accomplish the downstream task independently in a competitive environment which
prevents the two parts from learning what each other learned for the downstream
task. Such competing parts are then combined synergistically to complete the task.
By fusing representation parts learned competitively under different conditions 
ICP facilitates obtaining diversiﬁed representations which contain rich information.
Experiments on image classiﬁcation and image reconstruction tasks demonstrate
the great potential of ICP to learn discriminative and disentangled representations
in both supervised and self-supervised learning settings. 1

1

Introduction

Representation learning aims to make the learned feature representations more effective on extracting
useful information from input for downstream tasks [4]  which has been an active research topic
in recent years and has become the foundation for many tasks [28  8  11  15  40  20  6]. Notably 
a majority of works about representation learning have been studied from the viewpoint of mutual
information constraint. For instance  the Information Bottleneck (IB) theory [38  1] minimizes the
information carried by representations to ﬁt the target outputs  and the generative models such as
β-VAE [13  5] also rely on such information constraint to learn disentangled representations. Some
other works [22  3  26  14] reveal the advantages of maximizing the mutual information for learning
discriminative representations. Despite the exciting progresses  learning diversiﬁed representations
remains as an open problem. Diversiﬁed representations are learned with different constraints
encouraging representation parts to extract various information from inputs  which results in powerful
features to represent the inputs. In principle  a good representation learning approach is supposed to
discriminate and disentangle the underlying explanatory factors hidden in the input [4]. However 
this goal is hard to realize as the existing methods typically resort to only one type of information
constraint. As a consequence  the information diversity of the learned representations is deteriorated.

∗Corresponding Author.
1Codes  models and experimental results are all available at https://github.com/hujiecpp/

InformationCompetingProcess/

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper we present a diversiﬁed representation learning scheme  termed Information Competing
Process (ICP)  which handles the above issues through a new information diversifying objective.
First  the separated representation parts learned with different constraints are forced to accomplish
the downstream task competitively. Then  the rival representation parts are combined to solve the
downstream task synergistically. A novel solution is further proposed to optimize the new objective
in both supervised and self-supervised learning settings.
We verify the effectiveness of the proposed ICP on both image classiﬁcation and image reconstruction
tasks  where neural networks are used as the feature extractors. In the supervised image classiﬁcation
task  we integrate ICP with four different network architectures (i.e.  VGG [34]  GoogLeNet [35] 
ResNet [12]  and DenseNet [16]) to demonstrate how the diversiﬁed representations boost classiﬁca-
tion accuracy. In the self-supervised image reconstruction task  we implement ICP with β-VAE [13]
to investigate its ability of learning disentangled representations to reconstruct and manipulate the
inputs. Empirical evaluations suggest that ICP ﬁts ﬁner labeled dataset and disentangles ﬁne-grained
semantic information for representations.

2 Related Work

Representation Learning with Mutual Information. Mutual information has been a powerful
tool in representation learning for a long time. In the unsupervised setting  mutual information
maximization is typically studied  which targets at adding speciﬁc information to the representation
and forces the representation to be discriminative. For instance  the InfoMax principle [22  3]
advocates maximizing mutual information between the inputs and the representations  which forms
the basis of independent component analysis [17]. Contrastive Predictive Coding [26] and Deep
InfoMax [14] maximize mutual information between global and local representation pairs  or the
input and global/local representation pairs.
In the supervised or self-supervised settings  mutual information minimization is commonly utilized.
For instance  the Information Bottleneck (IB) theory [38] uses the information theoretic objective to
constrain the mutual information between the input and the representation. IB was then introduced
to deep neural networks [37  33  31]  and Deep Variational Information Bottleneck (VIB) [1] was
recently proposed to reﬁne IB with a variational approximation. Another group of works in self-
supervised setting adopt generative models to learn representations [19  30]  in which the mutual
information plays an important role in learning disentangled representations. For instance  β-VAE
[13] is a variant of Variation Auto-Encoder [19] that attempts to learn a disentangled representation
by optimizing a heavily penalized objective with mutual information minimization. Recent works
in [5  18  7] revise the objective of β-VAE by applying various constraints. One special case is
InfoGAN [8]  which maximizes the mutual information between representation and a factored
Gaussian distribution. Besides  Mutual Information Neural Estimation [2] estimates the mutual
information of continuous variables. Differing from the above schemes  the proposed ICP leverages
both mutual information maximization and minimization to create competitive environment for
learning diversiﬁed representations.
Representation Collaboration. The idea of collaborating neural representations can be found
in Neural Expectation Maximization [10] and Tagger [9]  which uses different representations to
group and represent individual entities. The Competitive Collaboration [29] method is the most
relevant to our work. It deﬁnes a three-player game with two competitors and a moderator  where
the moderator takes the role of a critic and the two competitors collaborate to train the moderator.
Unlike Competitive Collaboration  the proposed ICP enforces two (or more) representation parts to
be complementary through different mutual information constraints for the same downstream task
by a competitive environment  which endows the capability of learning more discriminative and
disentangled representations.

3

Information Competing Process

The key idea of ICP is depicted in Fig. 1  in which different representation parts compete and
collaborate with each other to diversify the information. In this section  we ﬁrst unify supervised and
self-supervised objectives for acheving the target tasks. Then  the information competing objective
for learning diversiﬁed representations is proposed.

2

Figure 1: The proposed Information Competing Process. In the competitive step  the rival repre-
sentation parts are forced to accomplish the downstream task solely by preventing both parts from
knowing what each other learned under different constraints for the task. In the synergetic step 
these representation parts are combined to complete the downstream task synthetically. ICP can be
generalized to arbitrary number of constrained parts  and in this paper we make an example of two.

3.1 Unifying Supervised and Self-Supervised Objectives

The information constraining objective in supervised setting has the same form as that of self-
supervised setting except the target outputs. We therefore unify these two objectives by using t
as the output of the downstream tasks. In supervised setting  t represents the label of input x. In
self-supervised setting  t represents the input x itself. This leads to the uniﬁed objective function
linking the representation r of input x and target t as:

(1)
where I(· ·) stands for the mutual information. This uniﬁed objective describes a constraint with the
goal of maximizing the mutual information between the representation r and the target t.

max(cid:2)I(r  t)(cid:3) 

3.2 Separating and Diversifying Representations

To explicitly diversify the information on representations  we directly separate the representation
r into two parts [z  y] with different constraints  and encourage representations to learn discrepant
information from the input x. Speciﬁcally  we constrain the information capacity of representation
part z while increasing the information capacity of representation part y. To that effect  we have the
following objective function:

max(cid:2)I(r  t) + αI(y  x) − βI(z  x)(cid:3) 

(2)

where α > 0 and β > 0 are the regularization factors.

3.3 Competition of Representation Parts

To prevent any one of the representation parts from dominating the downstream task  we let z and
y to accomplish the downstream task t solely by utlizing the mutual information constraints I(z  t)
and I(y  t). Additionally  for ensuring the representations catch diversiﬁed information through
different constraints  ICP prevents z and y from knowing what each other learned for the downstream
task  which is realized by enforcing z and y independent of each other. These constraints result in a
competitive environment to enrich the information carried by representations. Correspondingly  the
objective of ICP is concluded as:

max(cid:2) I(r  t)
(cid:124) (cid:123)(cid:122) (cid:125)

+ αI(y  x)
2(cid:13) Maximization
where γ > 0 is the regularization factor.

1(cid:13) Synergy

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

− βI(z  x)
3(cid:13) Minimization

(cid:124)

+I(z  t) + I(y  t) − γI(z  y)

(cid:123)(cid:122)

4(cid:13) Competition

(cid:3) 

(cid:125)

(3)

4 Optimizing the Objective of ICP

In this section  we derive a solution to optimize the objective of ICP. Although all terms of this
objective have the same formulation that calculates the mutual information between two variables  they

3

xzyttCompetitionConstraint AConstraint BxzytSynergy(cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90) (cid:90)

I(z  x) =

=

=

(cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90)

P (z  x) log

P (z  x)

P (z)P (x)

P (z  x) log P (z|x)dxdz −

P (z  x) log P (z|x)dxdz −

dxdz =

P (z  x) log

dxdz

P (z|x)
P (z)

P (x|z)P (z) log P (z)dxdz

(4)

need to be optimized using different methods due to their different aims. We therefore classify these
terms as the mutual information minimization term I(z  x)  the mutual information maximization
term I(y  x)  the inference terms I(z  t) I(y  t) I(r  t) and the predictability minimization term
I(z  y) to ﬁnd the solution.

4.1 Mutual Information Minimization Term

To minimize the mutual information between x and z  we can ﬁnd out a tractable upper bound
for the intractable I(z  x). In the existing works [19  1]  I(z  x) is usually deﬁned under the joint
distribution of inputs and their encoding distribution  as it is the constraint between the inputs and the
representations. Concretely  the formulation is derived as:

P (z) log P (z)dz.

Let Q(z) be a variational approximation of P (z)  we have:

(cid:90)

KL(cid:2)P (z)||Q(z)(cid:3) ≥ 0 ⇒
(cid:90) (cid:90)

P (z|x)P (x) log

I(z  x) ≤

(cid:90)

P (z) log Q(z)dz.

(5)

(cid:104)

KL(cid:2)P (z|x)||Q(z)(cid:3)(cid:105)

 

(6)

P (z) log P (z)dz ≥

P (z|x)
Q(x)

dxdz = Ex∼P (x)

According to Eq. 5  the trackable upper bound after applying the variational approximation is:

which enforces the extracted z conditioned on x to a predeﬁned distribution Q(z) such as a standard
Gaussian distribution.

4.2 Mutual Information Maximization Term

To maximize the mutual information between x and y  we deduce a tractable alternate for the
intractable I(y  x). Speciﬁcally  like the above minimization term  the mutual information should
also be deﬁned as the joint distribution of inputs and their encoding distribution. As it is hard to
derive a tractable lower bound for this term  we expand the mutual information as:

dxdy = KL(cid:2)P (y|x)P (x)||P (y)P (x)(cid:3).

(7)

I(y  x) =

P (y  x) log

P (y  x)

P (y)P (x)

(cid:90) (cid:90)

Since Eq. 7 means that maximizing the mutual information is equal to enlarging the Kullback-Leibler
(KL) divergence between distributions P (y|x)P (x) and P (y)P (x)  and the maximization of KL
divergence is divergent. We instead maximize the Jensen-Shannon (JS) divergence as an alternative
which approximates the maximization of KL divergence but is convergent. As [25]  a tractable
variational estimation of JS divergence can be deﬁned as:

JS(cid:2)P (y|x)P (x)||P (y)P (x)(cid:3) = max

(cid:104)E(y x)∼P (y|x)P (x)

+E(ˆy x)∼P (y)P (x)

(cid:2) log D(y  x)(cid:3)
(cid:2) log(cid:0)1 − D(ˆy  x)(cid:1)(cid:3)(cid:105)

(8)

 

where D is a discriminator that estimates the probability of the input pair  (y  x) is the positive pair
sampled from P (y|x)P (x)  and (ˆy  x) is the negative pair sampled from P (y)P (x). As ˆy shoule be
the representation conditioned on x  we disorganize y in the positive pair (x  y) to obtain the negative
pair (x  ˆy).

4.3

Inference Term

The inference terms in Eq. 3 should be deﬁned as the joint distribution of representation and the
output distribution of downstream task solver. We take I(r  t) as an example  and I(z  t) I(y  t)

4

Algorithm 1: Optimization of Information Competing Process
Input: The source input x with the downstream task target t  the prior distribution Q(z)  Q(t|r) 

Q(t|z) and Q(t|y) for variational approximation  and the hyperparameters α  β  γ.

Output: The learned representation extractor and downstream solver.

1 while not Convergence do
2
3
4
5
6
7
8
9
10
11
12 end

Optimize Eq. 8 and Eq. 16 for discriminator D and predictor H;
// Mutual Information Minimization Term:
Replace I(z  x) in Eq. 3 with the tractable upper bound in Eq. 6;
// Mutual Information Maximization Term:
Replace I(y  x) in Eq. 3 with the tractable alternative in Eq. 8;
// Inference Term:
Replace I(z  t) I(y  t) I(r  t) in Eq. 3 with the tractable lower bound in Eq. 14;
// Predictability Minimization Term:
Replace I(z  y) in Eq. 3 with Eq. 16;
Optimize Eq. 3 while ﬁxing the parameters of D and H;

have the same formulation with I(r  t). We expand this mutual information term as:

(cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90) (cid:90)

I(r  t) =

=

=

≥

P (r  t) log

P (t|r)
P (t)

drdt

P (r  t) log P (t|r)dtdr −

(cid:90)

P (t) log P (t)dt

P (r  t) log P (t|r)dtdr + H(t)

P (t|r)p(r) log P (t|r)dtdr 

(9)

where H(t) ≥ 0 is the information entropy of t. Let Q(t|r) be a variational approximation of P (t|r) 
we have:

KL(cid:2)P (t|r)||Q(t|r)(cid:3) ≥ 0 ⇒

(cid:90)

P (t|r) log P (t|r)dt ≥

P (t|r) log Q(t|r)dt.

(10)

(cid:90)

By applying the variational approximation  the trackable lower bound of the mutual information
between r and t is:

I(r  t) ≥

P (r  t) log Q(t|r)dtdr.

(11)

(cid:90) (cid:90)

Based on the above formulation  we derive different objectives for the supervised and self-supervised
settings in what follows.
Supervised Setting. In the supervised setting  t represents the known target labels. By assuming that
the representation r is not dependent on the label t  i.e.  P (r|x  t) = P (r|x)  we have:

P (x  r  t) = P (r|x  t)P (t|x)P (x) = P (r|x)P (t|x)P (x).

Accordingly  the joint distribution of r and t can be written as:

P (r  t) =

P (x  r  t)dx =

P (r|x)P (t|x)P (x)dx.

(cid:90)

(12)

(13)

(cid:90)

(cid:90) (cid:90) (cid:90)

Combining Eq. 11 with Eq. 13  we get the lower bound of the inference term in the supervised setting:

I(r  t) ≥

= Ex∼P (x)

(cid:104)Er∼P (r|x)

(cid:2)(cid:90)

P (x)P (r|x)P (t|x) log Q(t|r)dtdrdx

P (t|x) log Q(t|r)dt(cid:3)(cid:105)

(14)

.

5

Table 1: Classiﬁcation error rates (%) on CIFAR-10 test set.

Baseline
VIB [1]

DIM* [14]

VIB×2
DIM*×2
ICP-ALL
ICP-COM

ICP

7.63

4.92

6.67

VGG16 [34] GoogLeNet [35] ResNet20 [12] DenseNet40 [16]
6.81↑0.14
6.54↓0.13
6.86↑0.19
7.24↑0.57
6.97↑0.30
6.59↓0.08
6.10↓0.57

5.72↓0.11
6.15↑0.32
6.36↑0.53
5.60↓0.23
6.13↑0.30
5.63↓0.20
4.99↓0.84

5.09↑0.17
4.65↓0.27
4.88↓0.04
4.95↑0.03
4.76↓0.16
4.67↓0.25
4.26↓0.66

6.95↓0.68
7.61↓0.02
6.85↓0.78
7.46↓0.17
6.47↓1.16
7.33↓0.30
6.01↓1.62

5.83

Table 2: Classiﬁcation error rates (%) on CIFAR-100 test set.

Baseline
VIB [1]

DIM* [14]

VIB×2
DIM*×2
ICP-ALL
ICP-COM

ICP

31.91

20.68

26.41

VGG16 [34] GoogLeNet [35] ResNet20 [12] DenseNet40 [16]
26.56↑0.15
26.74↑0.33
26.08↓0.33
25.72↓0.69
26.73↑0.32
26.37↓0.04
24.54↓1.87

20.93↑0.25
20.94↑0.26
22.09↑1.41
21.74↑1.06
20.90↑0.22
20.81↑0.13
18.55↓2.13

26.37↓1.18
27.51↓0.04
29.33↑1.78
27.15↓0.40
27.51↓0.04
26.85↓0.70
24.52↓3.03

30.84↓1.07
32.62↑0.71
29.74↓2.17
30.16↓1.75
28.35↓3.56
32.76↑0.85
28.13↓3.78

27.55

Since the conditional probability P (t|x) represents the distribution of labels in the supervised setting 
Eq. 14 is actually the cross entropy loss for classiﬁcation.
Self-supervised Setting. In the self-supervised setting  t is the input x itself. Therefore  Eq 11 can
be directly derived as:

(cid:104)Er∼P (r|x)

(cid:2) log Q(x|r)(cid:3)(cid:105)

.

(15)

(cid:90) (cid:90)

I(r  x) ≥

P (r|x)P (x) log Q(x|r)dxdt = Ex∼P (x)

Assuming Q(t|r) as a Gaussian distribution  Eq. 15 can be expanded as the L2 reconstruction loss
for the input x.

4.4 Predictability Minimization Term

To diversify the information and prevent the dominance of one representation part  we constrain
the mutual information between z and y  which equals to make z and y be independent with each
other. Inspired by [32]  we introduce a predictor H to fulﬁll this goal. Concretely  we let H predict
y conditioned on z  and prevent the extractor from producing z which can predict y. The same
operation is conducted on y to z. The corresponding objective is:

(cid:104)Ez∼P (z|x)

(cid:2)H(y|z)(cid:3) + Ey∼P (y|x)

(cid:2)H(z|y)(cid:3)(cid:105)

min max

.

(16)

So far  we have all the tractable bounds and alternatives for optimizing the information diversifying
objective of ICP. The optimization process is summarized in Alg. 1.

5 Experiments

In experiments  all the probabilistic feature extractors  task solvers  predictor and discriminator are
implemented by neural networks. We suppose Q(z)  Q(t|r)  Q(t|z)  Q(t|y) are standard Gaussian
distributions and use reparameterization trick by following VAE [19]. The objectives are differentiable
and trained using backpropagation.
In the classiﬁcation task (supervised setting)  we use one
fully-connected layer as classiﬁer. In the reconstruction task (self-supervised setting)  multiple
deconvolution layers are used as the decoder to reconstruct the inputs. The implementation details
and the experimental logs are all avaliable at our source code page.

6

(a) Correlation heatmap of ICP-ALL.

(b) Correlation heatmap of ICP.

Figure 2: Heatmaps of the correlation between categories and the dimension of representations of
VGGNet on CIFAR-10. The horizontal axis denotes the dimension of representations  and the vertical
axis denotes the categories. Darker color denotes higher correlation.

5.1 Supervised Setting: Classiﬁcation Tasks

5.1.1 Datasets

CIFAR-10 and CIFAR-100 [21] are used to evaluate the performance of ICP in the image classiﬁcation
task. These datasets contain natural images belonging to 10 and 100 classes respectively. CIFAR-100
comes with ﬁner labels than CIFAR-10. The raw images are with 32×32 pixels and we normalize
them using the channel means and standard deviations. Standard data augmentation by random
cropping and mirroring is applied to the training set.

5.1.2 Classiﬁcation Performance and Ablation Study

We utilize four architectures including VGGNet [34]  GoogLeNet [35  36]  ResNet [12]  and
DenseNet [16] to test the general applicability of ICP and to study the diversiﬁed representations
learned by ICP. We use the classiﬁcation results of original network architectures as our baselines.
The deep Variational Information Bottleneck (VIB) [1] and global version of Deep InfoMax with one
additional mutual maximization term (DIM*) [14] are used as references  in which VIB is optimized
by maximizing I(z  t) − βI(z  x)   and DIM* is optimized by maximizing I(y  t) + αI(y  x). To
make a fair comparison  we expand the representation dimension of both methods to the same size of
ICP’s (denoted as VIB×2  and DIM*×2). The VIB  DIM*  VIB×2 and DIM*×2 are the methods that
only use one type of representation constraints in ICP  which can also be regarded as ablation study
for ICP with single information constraint and without the information diversifying objective.
For further ablation study  we optimize ICP without all the information diversifying and competing
constraints (i.e.  optimize Eq. 1)  which is denoted as ICP-ALL. We also optimize ICP with the
information diversifying objective but without the information competing objective (i.e.  optimize
Eq. 2)  which is denoted as ICP-COM.
The classiﬁcation results on CIFAR-10 and CIFAR-100 are shown in Tables 1 and 2. We ﬁnd that
VIB  DIM*  VIB×2 and DIM*×2 achieve sub-optimal results due to the limited diversiﬁcation of
representations. ICP-ALL do not work well as the large model capacity overﬁts the training set  and
ICP-COM fails because of the dominance of one type of representations. These results show that
expanding models with sole constraint or removing one constraint from the objective decreases the
performance. Only ICP generalizes to all these architectures and reports the best performance. In
addition  the results on different datasets (i.e.  CIFAR-10 and CIFAR-100) suggest that ICP works
better on the ﬁner labeled dataset (i.e.  CIFAR-100). We attribute the success to the diversiﬁed
representations that capture more detailed information of inputs.

7

Categories①②③DifferentDimensionsCategoriesGeneralSpecificDifferentDimensions(a) dSprites

(b) 3D Faces

Figure 3: Qualitative disentanglement results of ICP on (a) dSprites and (b) 3D Faces datasets.

5.1.3 Interpretability of The Diversiﬁed Representations

To explain the intuitive idea and the superior results of ICP  we study the learned classiﬁcation
models to explore why ICP works and provide some insights about the interpretability of the learned
representations. In the following  we make an example of VGGNet on CIFAR-10 and visualize
the normalized absolute value of the classiﬁer’s weights. As shown in Fig. 2(a)  the classiﬁcation
dependency is fused in ICP-ALL  which means combining two representations directly without any
constraints does not diversify the representation. The ﬁrst green bounding box shows that the
classiﬁcation relies on both parts. The second and the third green bounding boxes show that the
classiﬁcation relies more on the ﬁrst part or the second part. On the contrary  as shown in Fig. 2(b)  the
classiﬁcation dependency can be separated into two parts. As the mutual information minimization
makes the representation carry more general information of input while the maximization makes the
representation carry more speciﬁc information of input  a small number of dimensions are sufﬁcient
for inference (i.e.  the left bounding box of Fig. 2(b))  while a large number of dimensions are required
for inference (i.e.  the right bounding box of Fig. 2(b)). This suggests that ICP learns diversiﬁed
representations for classiﬁcation.

5.2 Self-supervised Setting: Reconstruction

5.2.1 Datasets

We perform quantitative and qualitative disentanglement evaluations with the dataset of 2D shapes
(dSprites) [24] and the dataset of synthetic 3D Faces [27]. The ground truth factors of dSprites are
scale(6)  rotation(40)  posX(32) and posY(32). The ground truth factors of 3D Faces are azimuth(21) 
elevation(11) and lighting(11). Parentheses contain number of quantized values for each factor. The
dSprites and 3D Faces contain 3 types of shapes and 50 identities  respectively  which are treated as
noise during evaluation. The images of both datasets are reshaped to 64×64 pixels to compare with
the baseline methods. We also evaluate the reconstruction and manipulation performance on more
challenging CelebA [23] dataset which contains a large number of celebrity faces. The images are
reshaped to 128×128 pixels for more detialed reconstruction instead of 64×64 pixels.

5.2.2 Quantitative Evaluation

We evaluate the disentanglement performance quantitatively by the Mutual Information Gap (MIG)
score [7] with the 2D shapes (dSprites) [24] dataset and 3D Faces [27] dataset. MIG is a classiﬁer-free
information-theoretic disentanglement metric and is meaningful for any factorized latent distribution.
As shown in Table 3  ICP achieves the state-of-the-art performance on the quantitative evaluation
of disentanglement. We also conduct ablation studies as what we do in the supervised setting.

8

PosXPosYScaleRotationElevationLightingFace WidthAzimuth(a) Smile

(b) Goatee

(c) Eyeglasses

(d) Hair Color

Figure 4: Qualitative disentanglement results of β-VAE and ICP on CelebA. Each row represents a
different seed image used to infer the representation.

From the results of ICP-ALL and ICP-COM  we ﬁnd
disentanglement performance decreases without
the information diversifying and competing pro-
cess. For the challenging CelebA [23] dataset  we
evaluate the reconstruction performance via the
average Mean Square Error (MSE) and the Struc-
tural Similarity Index (SSIM) [39]. The MSE of
ICP is 8.5 ∗ 10−3 compared with 9.2 ∗ 10−3 of β-
VAE [13] and the SSIM of ICP is 0.62 compared
with 0.60 of β-VAE [13]  which show ICP retains more information of input for reconstruction.

Table 3: MIG score of disentanglement.
3D Faces [27]

β-VAE [13]
β-TCVAE [7]

ICP-ALL
ICP-COM

ICP

dSprites [24]

0.22
0.38
0.33
0.20
0.48

0.54
0.62
0.26
0.57
0.73

5.2.3 Qualitative Evaluation

For qualitative evaluation  we conduct the latent space traverse by traversing a single dimension of the
learned representation over the range of [-3  3] while keeping other dimensions ﬁxed. We manually
pick the dimensions which have semantic meaning related to human concepts from the reconstruction
results. The qualitative disentanglement results are shown in Figs. 3 and 4. It can be seen that many
ﬁne-grained semantic attributes such as rotation on dSprites dataset  face width on 3D Face dataset
and goatee on CelebA dataset are disentangled clearly by ICP with details.

6 Conclusion

We proposed a new approach named Information Competing Process (ICP) for learning diversiﬁed
representations. To enrich the information carried by representations  ICP separates a representation
into two parts with different mutual information constraints  and prevents both parts from knowing
what each other learned for the downstream task. Such rival representations are then combined
to accomplish the downstream task synthetically. Experiments demonstrated the great potential of
ICP in both supervised and self-supervised settings. The nature behind the performance gain lies
in that ICP has the ability to learn diversiﬁed representations  which provides fresh insights for the
representation learning problem.

9

!-VAEICP!-VAEICP!-VAEICP!-VAEICP!-VAEICP!-VAEICP!-VAEICP!-VAEICPAcknowledgments

This work is supported by the National Key R&D Program (No.2017YFC0113000  and
No.2016YFB1001503)  Nature Science Foundation of China (No.U1705262  No.61772443  No.
61802324  No.61572410 and No.61702136)  and Nature Science Foundation of Fujian Province 
China (No. 2017J01125 and No. 2018J01106).

References
[1] Alexander A Alemi  Ian Fischer  Joshua V Dillon  and Kevin Murphy. Deep variational

information bottleneck. In International Conference on Learning Representations  2017.

[2] Mohamed Ishmael Belghazi  Aristide Baratin  Sai Rajeswar  Sherjil Ozair  Yoshua Bengio 
In

Aaron Courville  and R Devon Hjelm. Mine: mutual information neural estimation.
International Conference on Machine Learning  2018.

[3] Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind

separation and blind deconvolution. Neural Computation  1995.

[4] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and

new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence  2013.

[5] Christopher P Burgess  Irina Higgins  Arka Pal  Loic Matthey  Nick Watters  Guillaume Des-
jardins  and Alexander Lerchner. Understanding disentangling in beta-vae. In Advances in
Neural Information Processing Systems  2018.

[6] Fuhai Chen  Rongrong Ji  Jiayi Ji  Xiaoshuai Sun  Ge Xuri Zhang  Baochang  Yongjian Wu 
Feiyue Huang  and Yan Wang. Variational structured semantic inference for diverse image
captioning. In Advances in Neural Information Processing Systems  2019.

[7] Tian Qi Chen  Xuechen Li  Roger B Grosse  and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In Advances in Neural Information Processing
Systems  2018.

[8] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems  2016.

[9] Klaus Greff  Antti Rasmus  Mathias Berglund  Tele Hao  Harri Valpola  and Jürgen Schmidhuber.
Tagger: Deep unsupervised perceptual grouping. In Advances in Neural Information Processing
Systems  2016.

[10] Klaus Greff  Sjoerd van Steenkiste  and Jürgen Schmidhuber. Neural expectation maximization.

In Advances in Neural Information Processing Systems  2017.

[11] Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large

graphs. In Advances in Neural Information Processing Systems  2017.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition 
2016.

[13] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick 
Shakir Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations 
2017.

[14] R Devon Hjelm  Alex Fedorov  Samuel Lavoie-Marchildon  Karan Grewal  Adam Trischler 
and Yoshua Bengio. Learning deep representations by mutual information estimation and
maximization. In International Conference on Learning Representations  2019.

[15] Jie Hu  Rongrong Ji  Hong Liu  Shengchuan Zhang  Cheng Deng  and Qi Tian. Towards visual
feature translation. In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition  2019.

10

[16] Gao Huang  Zhuang Liu  Laurens Van Der Maaten  and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition  2017.

[17] Aapo Hyvärinen and Erkki Oja. Independent component analysis: algorithms and applications.

Neural Networks  2000.

[18] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on

Machine Learning  2018.

[19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International

Conference on Learning Representations  2013.

[20] Alexander Kolesnikov  Xiaohua Zhai  and Lucas Beyer. Revisiting self-supervised visual
representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition  2019.

[21] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[22] Ralph Linsker. Self-organization in a perceptual network. Computer  1988.

[23] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of the IEEE International Conference on Computer Vision  2015.

[24] Loic Matthey  Irina Higgins  Demis Hassabis  and Alexander Lerchner. dsprites: Disentangle-

ment testing sprites dataset. https://github.com/deepmind/dsprites-dataset/  2017.

[25] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  2016.

[26] Aaron van den Oord  Yazhe Li  and Oriol Vinyals. Representation learning with contrastive

predictive coding. arXiv preprint arXiv:1807.03748  2018.

[27] Pascal Paysan  Reinhard Knothe  Brian Amberg  Sami Romdhani  and Thomas Vetter. A 3d face
model for pose and illumination invariant face recognition. In IEEE International Conference
on Advanced Video and Signal Based Surveillance  2009.

[28] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[29] Anurag Ranjan  Varun Jampani  Kihwan Kim  Deqing Sun  Jonas Wulff  and Michael J Black.
Adversarial collaboration: Joint unsupervised learning of depth  camera motion  optical ﬂow
and motion segmentation. In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition  2019.

[30] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International Conference on Machine
Learning  2014.

[31] Andrew Michael Saxe  Yamini Bansal  Joel Dapello  Madhu Advani  Artemy Kolchinsky 
Brendan Daniel Tracey  and David Daniel Cox. On the information bottleneck theory of deep
learning. 2018.

[32] Jürgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computa-

tion  1992.

[33] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via

information. arXiv preprint arXiv:1703.00810  2017.

[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

11

[35] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov 
Dumitru Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions.
In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition  2015.

[36] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition  2016.

[37] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In

IEEE Information Theory Workshop  2015.

[38] Naftali Tishby  Fernando C Pereira  and William Bialek. The information bottleneck method.

arXiv preprint physics/0004057  2000.

[39] Zhou Wang  Alan C Bovik  Hamid R Sheikh  Eero P Simoncelli  et al. Image quality assessment:

from error visibility to structural similarity. IEEE Transactions on Image Processing  2004.

[40] Xiaosong Zhang  Fang Wan  Chang Liu  Rongrong Ji  and Qixiang Ye. FreeAnchor: Learning
to match anchors for visual object detection. In Neural Information Processing Systems  2019.

12

,Pierre Baldi
Peter Sadowski
Jie Hu
Rongrong Ji
ShengChuan Zhang
Xiaoshuai Sun
Qixiang Ye
Chia-Wen Lin
Qi Tian