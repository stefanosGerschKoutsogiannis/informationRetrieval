2019,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution,We study the multi-channel sparse blind deconvolution (MCS-BD) problem  whose task is to simultaneously recover a kernel $\mathbf a$ and multiple sparse inputs $\{\mathbf x_i\}_{i=1}^p$ from their circulant convolution $\mathbf y_i = \mb a \circledast \mb x_i $ ($i=1 \cdots p$). We formulate the task as a nonconvex optimization problem over the sphere. Under mild statistical assumptions of the data  we prove that the vanilla Riemannian gradient descent (RGD) method  with random initializations  provably recovers both the kernel $\mathbf a$ and the signals $\{\mathbf x_i\}_{i=1}^p$ up to a signed shift ambiguity. In comparison with state-of-the-art results  our work shows significant improvements in terms of sample complexity and computational efficiency. Our theoretical results are corroborated by numerical experiments  which demonstrate superior performance of the proposed approach over the previous methods on both synthetic and real datasets.,A Nonconvex Approach for Exact and Efﬁcient

Multichannel Sparse Blind Deconvolution

Qing Qu

New York University

qq213@nyu.edu

Xiao Li

Chinese University of Hong Kong

xli@ee.cuhk.edu.hk

Zhihui Zhu(cid:6)

Johns Hopkins University

zzhu29@jhu.edu

Abstract

We study the multi-channel sparse blind deconvolution (MCS-BD) problem 
whose task is to simultaneously recover a kernel a and multiple sparse inputs
i(cid:16)1 from their circulant convolution yi (cid:16) a f xi (i (cid:16) 1;(cid:4)(cid:4)(cid:4) ; p). We formu-
txiup
late the task as a nonconvex optimization problem over the sphere. Under mild
statistical assumptions of the data  we prove that the vanilla Riemannian gradient
descent (RGD) method  with random initializations  provably recovers both the
kernel a and the signals txiup
i(cid:16)1 up to a signed shift ambiguity. In comparison
with state-of-the-art results  our work shows signiﬁcant improvements in terms
of sample complexity and computational efﬁciency. Our theoretical results are
corroborated by numerical experiments  which demonstrate superior performance
of the proposed approach over the previous methods on both synthetic and real
datasets.

1

Introduction

yi (cid:16) a f xi P Rn;

i P rps :(cid:16) t1; : : : ; pu;

We study the blind deconvolution problem with multiple inputs: given circulant convolutions

(1)
we aim to recover both the kernel a P Rn and the signals txiup
i(cid:16)1 P Rn using efﬁcient methods.
Blind deconvolution is an ill-posed problem in its most general form. Nonetheless  problems in prac-
tice often exhibits intrinsic low-dimensional structures  showing promises for efﬁcient optimization.
One such useful structure is the sparsity of the signals txiup
i(cid:16)1 [1]. The multichannel sparse blind
deconvolution (MCS-BD) broadly appears in the context of communications [2  3]  computational
imaging [4  5]  seismic imaging [6–8]  neuroscience [9–13]  computer vision [14–16]  and more.
(cid:15) Neuroscience. Detections of neuronal spiking activity is a prerequisite for understanding the
mechanism of brain function. Calcium imaging [12 13] and functional MRI [9 11] are two widely
used techniques  which record the convolution of unknown neuronal transient response and sparse
spike trains. The spike detection problem can be naturally cast as a MCS-BD problem.
(cid:15) Computational (microscopy) imaging. Super-resolution ﬂuorescent microscopy imaging [4  17 
18] conquers the resolution limit by solving sparse deconvolution problems. Its basic principle
is using photoswitchable ﬂuorophores that stochastically activate ﬂuorescent molecular  creating
a video sequence of sparse superpositions of point spread function (PSF). In many scenarios
(especially in 3D imaging)  as it is often difﬁcult to obtain the PSF due to defocus and unknown
aberrations [19]  it is preferred to estimate the point-sources and PSF jointly by solving MCS-BD.
(cid:15) Image deblurring. Sparse blind deconvolution problems also arise in natural image processing:
when a blurry image is taken due to the resolution limit or malfunction of imaging procedure  it
(cid:6)ZZ is also with the Department of Electrical & Computer Engineering  University of Denver.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Comparison with existing methods for solving MCS-BD2

Methods

Assumptions

Formulation
Algorithm

Recovery
Condition

Time Complexity

Wang et al. [20]
a spiky & invertible 
xi (cid:18)i:i:d: BGp(cid:18)q
min}q}8(cid:16)1 }CqY }1
r
interior point
(cid:18) P Op1{?
nq 
rOpp4n5 logp1{"qq
Ωpnq
p ě

Li et al. [21]
a invertible 
xi (cid:18)i:i:d: BRp(cid:18)q
maxqPSn(cid:1)1 }CqP Y }4
␣
(
r
noisy RGD
(cid:18) P Op1q 
rOppn13{"8q
Ωpmax
n; (cid:20)8

n8

4

p ě

"8 q

Ours

a invertible 
xi (cid:18)i:i:d: BGp(cid:18)q
minqPSn(cid:1)1 H(cid:22) pCqP Y q
)
r
rOppn5 (cid:0) pn logp1{"qq
Ωpmax

!
vanilla RGD
(cid:18) P Op1q 
n; (cid:20)8
(cid:22)2

p ě

n4q

can be modeled as a blur pattern convolved with visually plausible sharp images (whose gradient
are sparse) [15  16].

Prior arts. Recently  there have been a few attempts to solve MCS-BD with guaranteed perfor-
mance. Wang et al. [20] formulated the task as ﬁnding the sparsest vector in a subspace problem [22].
p ě Ωpn log nq and the sparsity level (cid:18) P Op1{?
They considered a convex objective  showing that the problem can be solved to exact solutions when
nq. A similar approach has also been investigated
by [23]. Li et al. [21] consider an ℓ4-maximization problem over the sphere  revealing benign global
geometric structures of the problem. Correspondingly  they introduced a noisy Riemannian gradient
descent (RGD) that solves the problem to approximate solutions in polynomial time.
These results are very inspiring but still suffer from quite a few limitations. The theory and method
in [20] only applies to cases when a is approximately a delta function (which excludes most prob-
lems of interest) and txiup
i(cid:16)1 are very sparse. Li et al. [21] suggests that more generic kernels a can
be handled via preconditioning of the data. However  due to the heavy-tailed behavior of ℓ4-loss 
the sample complexity provided in [21] is quite pessimistic3. Moreover  noisy RGD is proved to
converge with huge amounts of iterations [21]  and it requires additional efforts to tune the noise
parameters which is often unrealistic in practice. As mentioned in [21]  one may use vanilla RGD
which almost surely converges to a global minimum  but without guarantee on the number of itera-
tions. On the other hand  Li et al. [21] only considered the Bernoulli-Rademacher model4 which is
quite restrictive.
Contributions.
In this work  we introduce an efﬁcient optimization method for solving MCS-BD.
We consider a natural nonconvex formulation based on a smooth relaxation of ℓ1-loss. Under mild
assumptions of the data  we prove the following result.

With random initializations  a vanilla RGD efﬁciently ﬁnds an approximate solution  which can
then be reﬁned by a subgradient method that converges to the target solution in a linear rate.

We summarize our main result in Table 1. By comparison5 with [21]  our approach demonstrates sub-
stantial improvements for solving MCS-BD in terms of both sample and time complexity. Moreover 
our experimental results imply that our analysis is still far from tight – the phase transitions suggest
that p ě Ωppoly logpnqq samples might be sufﬁcient for exact recovery  which is favorable for ap-
plications (as real data in form of images can have millions of pixels  resulting in huge dimension
n). Our analysis is inspired by recent results on orthogonal dictionary learning [24–26]  but much of
our theoretical analysis is tailored for MCS-BD with a few extra new ingredients. Our work is the
ﬁrst result provably showing that vanilla gradient descent type methods solve MCS-BD efﬁciently.
r
2Here  (i) BGp(cid:18)q and BRp(cid:18)q denote Bernoulli-Gaussian and Bernoulli-Rademacher distribution  respec-
tively; (ii) (cid:18) P r0; 1s is the Bernoulli parameter controlling the sparsity level of xi; (iii) " denotes the recovery
precision of global solution a(cid:14)  i.e.  minℓ }a (cid:1) sℓ ra(cid:14)s} ď "; (iv)
Ω hides logpnq  (cid:18) and other factors.
3As the tail of BGp(cid:18)q distribution is heavier than that of BRp(cid:18)q  their sample complexity would be even
worse if BGp(cid:18)q model was considered.
4We say x obeys a Bernoulli-Rademacher distribution when x (cid:16) b d r where d denotes point-wise

rO and

product  b follows i.i.d. Bernoulli distribution and r follows i.i.d. Rademacher distribution.

5We do not ﬁnd a direct comparison with [20] meaningful  mainly due to its limitations of the kernel

assumption and sparsity level (cid:18) P Op1{?

nq discussed above.

2

Moreover  our ideas could potentially lead to new algorithmic guarantees for other nonconvex prob-
lems such as blind gain and phase calibration [27 28] and convolutional dictionary learning [29 30].
The full version [31] of this work can be found at https://arxiv.org/abs/1908.10776.
2 Problem Formulation
To begin  we list our assumptions on the unknown kernel a P Rn and sparse inputs txiup
(cid:15) Invertible kernel. We assume the kernel a to be invertible in the sense that its spectrum

p
p
i(cid:16)1 P Rn:
a (cid:16) F a
a (cid:16) F a is the discrete Fourier transform (DFT) of a with
does not have zero entries  where
F P Cn(cid:2)n being the DFT matrix. Let Ca P Rn(cid:2)n be an n (cid:2) n circulant matrix whose ﬁrst
aq F [32]  it

column is a. Since this circulant matrix Ca can be decomposed as Ca (cid:16) F (cid:6) diagpp
ai|{ mini |p
is also invertible and we deﬁne its condition number (cid:20)paq :(cid:16) maxi |p

ai|.

(cid:15) Random sparse signal. The input signals txiup
(BGp(cid:18)q) distribution:

xi (cid:16) bi d gi;

bi (cid:18)i:i:d: Bp(cid:18)q;

gi (cid:18)i:i:d: Np0; Iq;

where (cid:18) P r0; 1s is the Bernoulli-parameter which controls the sparsity level of xi.

i(cid:16)1 are sparse  and follow i.i.d. Bernoulli-Gaussian

(cid:24)

(cid:16)

yi (cid:16) s(cid:1)ℓ r(cid:8)(cid:11)as f sℓ

As aforementioned  this assumption generalizes those used in [20 21]. In particular  the ﬁrst assump-
tion on kernel a is much more practical than that of [20]  in which a is assumed to be spiky. The
second assumption is a generalization of the Bernoulli-Rademacher model adopted in [21].
Note that the MCS-BD problem exhibits intrinsic signed scaling-shift symmetry  i.e.  for any (cid:11) ­(cid:16) 0 
(2)
where sℓ r(cid:4)s denotes a cyclic shift operator of length ℓ. Without loss of generality  for the rest of the
paper we assume that the kernel a is normalized with }a} (cid:16) 1. Thus  we only hope to recover a
and txiup
(cid:4)(cid:4)(cid:4) xps. We
A nonconvex formulation. Let Y (cid:16) ry1 y2
can rewrite the measurement (1) in a matrix-vector form via circulant matrices 
i P rps øæ Y (cid:16) CaX;

i(cid:16)1 up to a signed shift ambiguity 

Since Ca is assumed to be invertible  we can deﬁne its corresponding inverse kernel h P Rn by

(cid:4)(cid:4)(cid:4) yps and X (cid:16) rx1 x2
(cid:8)

ad(cid:1)1 whose corresponding circulant matrix satisﬁes

(cid:8)(cid:11)(cid:1)1xi

i P rps;

;

h :(cid:16) F (cid:1)1p

yi (cid:16) a f xi (cid:16) Caxi;

(cid:0)p
Ch :(cid:16) F (cid:6) diag
ad(cid:1)1
looomooon
where p(cid:4)qd(cid:1)1 denotes entrywise inversion. Observing
Ch (cid:4) Y (cid:16) Ch (cid:4) Ca
‚

it leads us to consider the following objective
}CqY }0 (cid:16) 1

min

(cid:16) I

p

1
np

q

np

i(cid:16)1

F (cid:16) C(cid:1)1
loomoon
a ;
(cid:4)X (cid:16) X

;

sparse

s.t. q (cid:24) 0:

(3)

}Cyi q}0 ;

Obviously  when the solution of (3) is unique  the only minimizer is the inverse kernel h up to signed
scaling-shift (i.e.  q(cid:14) (cid:16) (cid:8)(cid:11)sℓ rhs)  producing ChY (cid:16) X with the highest sparsity. The nonzero
constraint q (cid:24) 0 is enforced simply to prevent the trivial solution q (cid:16) 0. Ideally  if we could solve
(3) to obtain one of the target solutions q(cid:14) (cid:16) sℓ rhs up to a signed scaling  the kernel a and sparse
signals txiup

i(cid:16)1 can be exactly recovered up to signed shift via

(cid:25)

a(cid:14) (cid:16) F (cid:1)1

;

i (cid:16) Cyiq(cid:14); p1 ď i ď pq:
x(cid:14)

However  it has been known for decades that optimizing the basic ℓ0-formulation (3) is an NP-hard
problem [33  34]. Instead  we consider the following nonconvex6 relaxation of the original problem
(3):

(cid:17)
pF q(cid:14)qd(cid:1)1
‚

p

φhpqq :(cid:16) 1
np

q

min

(4)
where H(cid:22)p(cid:4)q is the Huber loss [35] and P is a preconditioning matrix  both of which will be deﬁned
and discussed as follows.

6It is nonconvex because of the spherical constraint q P Sn(cid:1)1.

i(cid:16)1

H(cid:22) pCyiP qq ;

s.t. q P Sn(cid:1)1;

3

Smooth sparsity surrogate.
It is well-known that ℓ1-norm serves as a natural sparsity surrogate
for ℓ0-norm  but its nonsmoothness often makes it difﬁcult for analysis7. Here  we consider the
Huber loss8 H(cid:22) p(cid:4)q which is widely used in robust optimization [35]. It acts as a smooth sparsity
surrogate of ℓ1 penalty and is deﬁned as:

#

h(cid:22)pZijq;

h(cid:22) pzq :(cid:16)

|z|
2(cid:22) (cid:0) (cid:22)

z2

2

|z| ě (cid:22)
|z| ă (cid:22)

;

(5)

‚
H(cid:22)pZq :(cid:16) n
i(cid:16)1

‚

p

j(cid:16)1

where (cid:22) ą 0 is a smoothing scalar. Our choice h(cid:22) pzq is ﬁrst-order smooth  and behaves exactly
same as the ℓ1-norm for |z| ě (cid:22). In contrast  although the the ℓ4 objective in [21] is smooth  it only
promotes sparsity in special cases. Moreover  it results in a heavy-tailed process  producing ﬂat land-
scape around target solutions  and requiring substantially more samples for measure concentration.
Figure 1 shows a comparison of optimization landscapes in low dimension: the Huber-loss behaves
very similar to the ℓ1-loss  while optimizing the ℓ4-loss could result in large approximation error.
Preconditioning. An
ill-conditioned
kernel a can result
in poor optimiza-
tion landscapes (see Figure 1 for an
illustration).
To alleviate this effect 
we introduce a preconditioning matrix
P P Rn(cid:2)n [21 36 37]  deﬁned as follows9

(b) Huber-loss  7

(a) ℓ1-loss  7

(c) ℓ4-loss  7

(cid:3)

‚

p

(cid:11)

(cid:1)1{2

P (cid:16)

1

(cid:18)np

i(cid:16)1

CJ

yi

Cyi

;

(6)

(d) ℓ1-loss  ✓ (e) Huber-loss  ✓ (f) ℓ4-loss  ✓

which reﬁnes the function landscapes by
orthogonalizing the circulant matrix Ca:

loomoon

CaP

(cid:0)
(cid:8)
looooooooomooooooooon
(cid:19) Ca

CJ
a Ca

(cid:1)1{2

:

(7)

R

Q orthogonal

Figure 1: Comparison of optimization landscapes for dif-
ferent loss functions. Here 7 and ✓ mean without and with
the preconditioning matrix P   respectively. Each ﬁgure plots
the function values of the loss over S2  where the function val-
ues are all normalized between 0 and 1 (darker color means
smaller value  and vice versa). The small red dots on the
landscapes denote shifts of the ground truths.

Since P (cid:19) pCaCaq(cid:1)1{2  R can be
proved to be very close to the orthogonal
matrix Q. Thus  R is much more well-
conditioned than Ca. As illustrated in Fig-
ure 1  a comparison of optimization land-
scapes without and with preconditioning
shows that preconditioning symmetriﬁes
the optimization landscapes and eliminates spurious local minimizers. Therefore  it makes the prob-
lem more amendable for optimization.
Constrain over the sphere Sn(cid:1)1. We relax the nonconvex constraint q ­(cid:16) 0 in (3) by a unit
norm constraint on q. The norm constraint removes the scaling ambiguity as well as prevents the
trivial solution q (cid:16) 0. Note that the choice of the norm has strong implication for computation.
When q is constrained over ℓ8-norm  the ℓ1{ℓ8 optimization problem breaks beyond sparsity level
(cid:18) ě Ωp1{?
It
has been shown recently that optimizing over the sphere often leads to optimal sparsity (cid:18) P Op1q
[21  22  36  38]. Therefore  we choose to work with q P Sn(cid:1)1 and we also show similar recovery
results for MCS-BD.
Next  we develop efﬁcient ﬁrst-order methods and provide guarantees for exact recovery.

In contrast  the sphere Sn(cid:1)1 is a homogeneous Riemannian manifold.

nq [20].

of random process and perturbation analysis for preconditioning.

7The subgradient of ℓ1-loss is non-Lipschitz  which introduces tremendous difﬁculty in controlling suprema
(cid:22) pzq  with h(cid:22) pzq (cid:16)
8Actually  h(cid:22)p(cid:4)q is a scaled and elevated version of standard Huber function hs
(cid:22) pzq (cid:0) (cid:22)
9Here  the sparsity (cid:18) serves as a normalization purpose. It is often not known ahead of time  but the scaling

2 . Hence in our framework minimizing with h(cid:22) pzq is equivalent to minimizing with hs

(cid:22) pzq.

(cid:22) hs

1

here does not change the optimization landscape.

4

3 Main Results and Analysis
In this section  we show that the underlying benign ﬁrst-order geometry of the optimization land-
scapes of (4) enables efﬁcient and exact recovery using vanilla gradient descent methods  even with
random initialization. Our main result can be captured by the following theorem.
Theorem 3.1 We assume that the kernel a is invertible with condition number (cid:20)  and txiup
i(cid:16)1 (cid:18)
BGp(cid:18)q. Suppose (cid:18) P

. Whenever

(cid:18); 1?

!

(cid:0)

(cid:8)

1

"
and (cid:22) ď c min

)
*

n ; 1

3

(cid:2)

(cid:10)

n

(cid:18)(cid:1)2n4 log3pnq log
(cid:2)
(cid:10)

p ě C max

n;

(cid:20)8
(cid:18)(cid:22)2(cid:27)2

min

log4 n

(cid:18)n
(cid:22)

;

(8)

the function (4) satisﬁes certain regularity conditions (see Theorem 3.2)  allowing us to
w.h.p.
design an efﬁcient vanilla ﬁrst-order method. In particular  with probability at least 1
2   by using a
random initialization  the algorithms provably recover the target solution up to a signed shift with
"-precision in a linear rate

(cid:10)(cid:10)

(cid:2)

(cid:2)

#Iter ď C1

1
"
In the following  we explain our results in several aspects.

(cid:0) logpnpq log

(cid:18)(cid:1)1n4 log

1
(cid:22)

:

␣

r
Ωpmax

r
Ωpmax

␣
n; (cid:20)8{(cid:22)2

i(cid:16)1 are very sparse (cid:18) P Op1{?
(

(cid:15) Sample Complexity. As shown in Table 1  our sample complexity p ě

Remark 1.
(cid:15) Conditions and Assumptions. Here  as the MCS-BD problem becomes trivial10 when (cid:18) ď 1{n 
we only focus on the regime (cid:18) ą 1{n. Similar to [21]  our result only requires the kernel a to be
(
invertible and sparsity level (cid:18) to be constant. In contrast  the method in [20] only works when the
kernel a is spiky and txiup
nq  excluding most problems of interest.
n4q
n8{"8q in [21]. As aforementioned  this
in (8) improves upon the result p ě
improvement partly owes to the similarity of the Huber-loss to ℓ1-loss  so that the Huber-loss
is much less heavy-tailed than the ℓ4-loss studied in [21]  requiring fewer samples for measure
concentration. Still  our result leaves much room for improvement – we believe the sample de-
(cid:0)
pendency on (cid:18)(cid:1)1 is an artifact of our analysis11  and the phase transition in Figure 5 suggests that
r
p ě Ωppoly logpnqq samples might be sufﬁcient for exact recovery.
(cid:15) Algorithmic Convergence.
it should be noted that
O
RGD in [21  Theorem IV.2]. This has been achieved via a two-stage approach: (i) we ﬁrst run
Opn4q iterations of vanilla RGD to obtain an approximate solution; (ii) then perform a subgradi-
ent method with linear convergence to the ground-truth. Moreover  without any noise parameters
to tune  vanilla RGD is more practical than the noisy RGD in [21].

rOpn12{"2q of the noisy

(cid:8)
n4 (cid:0) logp1{"q

for our algorithm substantially improves upon that

the number of

iteration

Finally 

n; (cid:20)8

3.1 A glimpse of high dimensional geometry
To study the optimization landscape of (5)  we simplify the problem by a change of variable q (cid:16) Qq 
which rotates the space by the orthogonal matrix Q in (7). Since the rotation Q does not change the
optimization landscape  by an abuse of notation of q and q  we can rewrite the problem (5) as

‚

p

(cid:0)

(cid:8)

min

(9)
where we also used the fact that CyiP (cid:16) CxiR in (7). Moreover  since R (cid:19) Q is near orthogonal 
by assuming RQ(cid:1)1 (cid:16) I  for pure analysis purposes we can further reduce (9) to

i(cid:16)1

H(cid:22)

s.t.

q

;

CxiRQ(cid:1)1q

}q} (cid:16) 1;

H(cid:22) pCxiqq ;

s.t.

}q} (cid:16) 1:

(10)

fpqq :(cid:16) 1
np
r
fpqq (cid:16) 1
np

min

q

‚

p

i(cid:16)1

The reduction in (10) is simpler and much easier for analysis. By a similar analysis as [24  36]  it
can be shown that asymptotically the landscape is highly symmetric and the standard basis vectors
t(cid:8)eiun
i(cid:16)1 are approximately12 the only global minimizers. Hence  as RQ(cid:1)1 (cid:19) I  we can study the
10The problem becomes trivial when (cid:18) ď 1{n because (cid:18)n (cid:16) 1 so that each xi tends to be an one sparse
11The same (cid:18)(cid:1)1 dependency also appears in [21  24  25  36  37].
12The standard basis t(cid:8)eiun

i(cid:16)1 are exact global solutions for ℓ1-loss. The Huber loss we considered here

(cid:14)-function.

introduces small approximation errors due to its smoothing effects.

5

r
r
landscape of fpqq via studying the landscape of
fpqq followed by a perturbation analysis. As illus-
*
a
fpqq  we partition the sphere into 2n symmetric
trated in Figure 2  based on the target solutions of
regions  and consider 2n (disjoint) subsets of each region13 [24  25]
1 (cid:0) (cid:24); qi ż 0

"
q P Sn(cid:1)1 |

(cid:24) P r0;8q;

S i(cid:8)

:(cid:16)

ě

;

(cid:24)

where q(cid:1)i is a subvector of q with i-th entry removed. For every i P rns  S i(cid:0)
(cid:24) ) contains
exactly one of the target solution ei (or (cid:1)ei)  and all points in this set have one unique largest entry
with index i  so that they are closer to ei (or (cid:1)ei) in ℓ8 distance than all the other standard basis
vectors. As shown in Figure 2  the union of these sets form a full partition of the sphere only when
(cid:24) (cid:16) 0. While for small (cid:24) ą 0  each disjoint set excludes all the saddle points and maximizers  but
their union covers most measure of the sphere: when (cid:24) (cid:16) p5 log nq(cid:1)1  their union covers at least half
of the sphere  and hence a random initialization falls into one of the regions S i(cid:8)
(cid:24) with probability at
least 1{2 [25]. Therefore  we can only consider the optimization landscapes on the sets S i(cid:8)
(cid:24)   where
we show the Riemannian gradient of fpqq

(or S i(cid:1)

(cid:8)

(cid:24)

|qi|
}q(cid:1)i}8

grad fpqq :(cid:16) PqK∇fpqq (cid:16)

∇f pqq

(cid:24)   but they also hold for S i(cid:1)
(cid:24) .

satisﬁes the following properties in each set S i(cid:8)
in terms of S i(cid:0)
Proposition 3.2 (Regularity Condition) Suppose (cid:18) P
satisﬁes (8)  w.h.p. over the randomness of txiup

)
i(cid:16)1  the Riemannian gradient of fpqq satisﬁes

and (cid:22) ď c min

(cid:24) . For convenience  we will simply present the results

. When p

(cid:18); 1?

!

n ; 1

n

1

3

(cid:0)
I (cid:1) qqJ
(cid:8)
(cid:0)

(11)

(cid:24)

Figure 2: Illustration of the set S 1(cid:0)
in 3-
dimension. Region 1 (purple region) denotes
the interior of S 1(cid:0)
(cid:24) when (cid:24) (cid:16) 0  where it in-
cludes one unique target solution. We show
the regularity condition (11) within S 1(cid:0)
  ex-
cluding a green region of order Op(cid:22)q (i.e. 
Region 2) due to Huber smoothing.

(cid:24)

is compact and ei is also contained

for any q P S i(cid:0)

(cid:24) with

a
1 (cid:1) q2
(cid:11)pqq (cid:16)

xgrad fpqq; qiq (cid:1) eiy ě (cid:11)pqq}q (cid:1) ei} ;
i ě (cid:22)  where the regularity parameter is

"
c1(cid:18)p1 (cid:1) (cid:18)qqi
c1(cid:18)p1 (cid:1) (cid:18)qn(cid:1)1qi

a
a
1 (cid:1) q2
1 (cid:1) q2

i P r(cid:22); (cid:13)s
i ě (cid:13)

(cid:24) with

a
1 (cid:1) q2

which increases as q gets closer to ei. Here (cid:13) P r(cid:22); 1q is some constant.
Remark 2. Here  our result is stated with respect
to ei for the sake of simplicity. It should be noted
that asymptotically the global minimizer of (9) is
(cid:12)pRQ(cid:1)1q(cid:1)1ei rather than ei  where (cid:12) is a normal-
ization factor. Nonetheless  as RQ(cid:1)1 (cid:19) I  the
global optimizer (cid:12)pRQ(cid:1)1q(cid:1)1ei of (9) is very close
to ei  so that we can state a similar result with re-
spect to (cid:12)pRQ(cid:1)1q(cid:1)1ei. The regularity condition
(11) shows that any q P S i(cid:0)
i ě (cid:22)
is not a stationary point. Similar regularity condition
has been proved for phase retrieval [39]  dictionary
learning [25]  etc. Such condition implies that the
negative gradient direction coincides with the direc-
tion to the target solution. The lower bound on Rie-
mannian gradient ensures that the iterate still makes
sufﬁcient progress to the target solution  even when
it is close to the target.
To ensure convergence of RGD  we also need to
show the following property  so that once initialized
in S i(cid:0)
the iterates of the RGD method implicitly reg-
ularize themselves staying in the set S i(cid:0)
(cid:24) . This en-
sures that the regularity condition (11) holds through
the solution path of the RGD method.
13Here  we deﬁne }q(cid:1)i}(cid:1)18 (cid:16) (cid:0)8 when }q(cid:1)i}8 (cid:16) 0  so that the set S i(cid:0)
in the set.

(cid:24)

(cid:24)

6

e1e2−e2e3−e3ξ=0ξ=5log(n)1
qj

F

ě c4

(cid:18)p1 (cid:1) (cid:18)q

i(cid:16)1  the Riemannian gradient of fpqq satisﬁes

B
Proposition 3.3 (Implicit Regularization) Under the same condition of Proposition 3.2  w.h.p.
over the randomness of txiup
grad fpqq;

ej (cid:1) 1
ei
qi
for all q P S i(cid:0)
j ě 1
(cid:24) and any qj such that j (cid:24) i and q2
i .
3 q2
Remark 3.
In a nutshell  (12) guarantees that the negative gradient direction points towards ei
j ě 1
i ; @j (cid:24) i). With this  we can
component-wisely for relatively large components (i.e.  q2
3 q2
prove that those components will not increase after gradient update  ensuring the iterates stay within
the region S i(cid:0)
(cid:24) . This type of implicit regularizations for the gradient has also been discovered
for many nonconvex optimization problems  such as low-rank matrix factorizations [40–43]  phase
retrieval [44]  and neural network training [45].
3.2 From geometry to efﬁcient optimization
Phase 1: Finding an approximate solution via RGD. Starting from a random initialization qp0q
uniformly drawn from Sn(cid:1)1  we solve the problem (4) via vanilla RGD

(cid:24)
1 (cid:0) (cid:24)

(12)

n

;

(cid:1)
qpkq (cid:1) (cid:28) (cid:4) grad fpqpkqq

(cid:9)

qpk(cid:0)1q (cid:16) PSn(cid:1)1

;

(13)

where (cid:28) ą 0 is the stepsize  and PSn(cid:1)1 p(cid:4)q is a projection operator onto the sphere Sn(cid:1)1.
Proposition 3.4 (Linear convergence of gradient descent) Suppose Proposition 3.2 and Proposi-
tion 3.3 hold. With probability at least 1{2  the random initialization qp0q falls into one of the regions
S i(cid:8)

for some i P rns. Choosing a ﬁxed step size (cid:28) ď c

in (13)  we have

(cid:22); n(cid:1)3{2

␣

(
(cid:10)

(cid:2)

(cid:24)

(cid:15)(cid:15)(cid:15)

qpkq (cid:1) ei

(cid:15)(cid:15)(cid:15) ď 2(cid:22); @k ě N :(cid:16) C

n min

n4 log

1
(cid:22)

:

(cid:18)

Because of the preconditioning and smoothing via Huber loss (5)  the geometry structure in Propo-
sition 3.2 implies that the gradient descent method can only produce an approximate solution qs up
to a precision Op(cid:22)q. Moreover  as we can show that }ei (cid:1) (cid:12)pRQ(cid:1)1q(cid:1)1ei} ď (cid:22){2  it does not make
much difference of stating the result in terms of either ei or (cid:12)pRQ(cid:1)1q(cid:1)1ei. Next  we show that 
by using qs as a warm start  an extra linear program (LP) rounding procedure produces an exact
solution pRQ(cid:1)1q(cid:1)1ei up to a scaling factor in a few iterations.
Phase 2: Exact solution via LP rounding. Let r (cid:16) qs be the solution obtain from solving RGD.
We recover the exact solution by solving the following LP problem14

min

q

(cid:16)pqq :(cid:16) 1
np

CxiRQ(cid:1)1q

1

s.t.

xr; qy (cid:16) 1:

(14)

(cid:15)(cid:15)

‚

p

(cid:15)(cid:15)

i(cid:16)1

p

J

(cid:0)

(cid:0)

(cid:9)

‚

RQ(cid:1)1

(cid:8)
(cid:0)

(cid:8)
gpkq (cid:16) 1
np
(cid:1)J

(cid:1)
CxiRQ(cid:1)1qpkq

Since the feasible set xr; qy (cid:16) 1 is essentially the tangent space of the sphere Sn(cid:1)1 at r  and r (cid:16) qs
is pretty close to the target solution  one should expect that the optimizer q(cid:14) of (14) exactly recovers
the inverse kernel h up to a scaled-shift. The problem (14) is convex and can be directly solved
using standard tools such as CVX [46]  but it will be time consuming for large dataset. Instead  we
introduce an efﬁcient projected subgradient method for solving (14) 
qpk(cid:0)1q (cid:16) qpkq (cid:1) (cid:28)pkqPrKgpkq;
CJ
RQ(cid:1)1
For convenience  let

(cid:1)1 enr
25 and let r (cid:16) qs which satisﬁes }r (cid:1) ei} ď 2(cid:22). Choose (cid:28)pkq (cid:16)
Proposition 3.5 Suppose (cid:22) ď 1
(cid:17)k(cid:28)p0q with (cid:28)p0q (cid:16) c1 log(cid:1)2pnpq and (cid:17) P r
; 1q. Under the same condition
of Theorem 3.1  w.h.p. the sequence tqpkqu produced by (15) with qp0q (cid:16) r converges to the target
solution in a linear rate  i.e. 

:
r  and deﬁne the distance dpqq between q and the truth
(cid:0)

r
r :(cid:16)
distpqq :(cid:16) }dpqq} ; dpqq :(cid:16) q (cid:1)

(cid:8)
1 (cid:1) c2 log(cid:1)2pnpq
ř
i(cid:16)1 }Cyi P q}1 ; s.t. xr; qy (cid:16) 1  with r (cid:16) QJr.

distpqpkqq ď C(cid:17)k;
an equivalent problem of (14) as minq (cid:16)pqq :(cid:16) 1

14For convenience  we state this problem in the rotated space. For the original problem (5)  we should solve

@ k (cid:16) 0; 1; 2;(cid:4)(cid:4)(cid:4) :

RQ(cid:1)1

(15)

i(cid:16)1

sign

:

rn

(cid:8)

1{2

xi

np

p

7

Figure 3: Comparison of iterate
convergence. p (cid:16) 50  n (cid:16) 200 
(cid:18) (cid:16) 0:25.

Figure 4: Comparison of recovery
probability with varying (cid:18). p (cid:16) 50 
n (cid:16) 500.

Remark 4. Unlike smooth problems  in general  subgradient methods for nonsmooth problem
have to use geometrically diminishing stepsize to achieve linear convergence15 [48–51]. The under-
(cid:8)
lying geometry that supports the use of geometric diminishing step size and linear convergence is
the so-called sharpness property [52] of the problem (14). In particular  for some constant (cid:11) ą 0 
we prove (cid:16)pqq is sharp in the sense that
(cid:1)1

(cid:1)(cid:0)

(cid:9)

ě (cid:11) (cid:4) distpqq;

@ xr; qy (cid:16) 1:

(cid:16)pqq (cid:1) (cid:16)

RQ(cid:1)1

en{r

rn

Finally  we end this section by noting that although we use matrix-vector form of convolutions in
(13) and (15)  all the matrix-vector multiplications can be efﬁciently implemented by FFT  including
the preconditioning matrix in (6) which is also a circulant matrix. With FFT  the complexities of
implementing one gradient update in (13) and subgradient in (15) are both Oppn log nq.
4 Experiment

Experiments on 1D synthetic dataset. First  we conduct a series of experiments on synthetic
dataset to demonstrate the superior performance of the vanilla RGD method (13). For all synthetic
experiments  we generate the measurements yi (cid:16) a f xi (1 ď i ď p)  with the ground truth kernel
a P Rn drawn uniformly random from the sphere Sn(cid:1)1 (i.e.  a (cid:18) UpSn(cid:1)1q)  and with sparse signals
xi P Rn; i (cid:16) rps drawn from i.i.d. Bernoulli-Gaussian distribution xi (cid:18)i:i:d: BGp(cid:18)q.
We compare the performances of RGD16 with random initialization on ℓ1-loss  Huber-loss  and ℓ4-
loss considered in [21]. We use line-search for adaptively choosing stepsize. For a fair comparison
of optimizing all losses  we reﬁne all solutions with the LP rounding procedure (14) optimized by
subgradient descent (15)  and use the same random initialization uniformly drawn from the sphere.
For judging the success of recovery  let q(cid:14) be a solution produced by the algorithm and we deﬁne

(cid:26)accpq(cid:14)q :(cid:16) }CaP q(cid:14)}8 {}CaP q(cid:14)} P r0; 1s:

If q(cid:14) achieves the target solution  it should satisfy P q(cid:14) (cid:16) h  with h being the inverse kernel of a and
thus (cid:26)accpq(cid:14)q (cid:16) 1. Therefore  we should expect (cid:26)accpq(cid:14)q (cid:19) 1 when an algorithm produces a correct
solution. For the following simulations  we assume successful recovery whenever (cid:26)accpq(cid:14)q ě 0:95.

(a) Comparison of iterate convergence. We ﬁrst compare the convergence in terms of the distance
from the iterate to the target solution for all losses using RGD. As shown in Figure 3  in Phase 1
optimizing ℓ4-loss can only produce an approximate solution up to precision 10(cid:1)2. In contrast 
optimizing Huber-loss converges much faster  and producing much more accurate solutions as
(cid:22) decreases. In Phase 2  subgradient descent converges linearly to the exact solution.
(b) Recovery with varying sparsity. Fix n (cid:16) 500 and p (cid:16) 50  we compare the recovery prob-
ability with varying sparsity level (cid:18). For each (cid:18)  we repeat the simulation for 15 times. As
illustrated in Figure 4  optimizing Huber-loss enables successful recovery for much larger (cid:18) in
comparison with that of ℓ4-loss. The performances of optimizing ℓ1-loss and Huber-loss are
quite similar.

15Typical choices such as (cid:28)pkq (cid:16) Op1{kq and (cid:28)pkq (cid:16) Op1{?

16For ℓ1-loss  we use Riemannian subgradient method  similar to (15).

kq lead to sublinear convergence [47–51].

8

(a) ℓ1-loss

(b) Huber-loss

(c) ℓ4-loss

Figure 5: Comparison of phase transition on pp; nq with ﬁxed (cid:18) (cid:16) 0:25.

(c) Phase transition on pp; nq. Finally  we ﬁx (cid:18) (cid:16) 0:25  and test the dependency of sample
number p on the dimension n via phase transition plots. For each individual pp; nq  we repeat
the simulation for 15 times. Whiter pixels in Figure 5 indicates higher success probability  and
vice versa. As shown in Figure 5  for a given n  optimizing Huber-loss requires much fewer
samples p for recovery in comparison with that of ℓ4-loss. The performance of optimizing
ℓ1-loss and Huber-loss is comparable; we conjecture sample dependency for optimizing both
losses is p ě Ωppoly logpnqq. In contrast  optimizing ℓ4-loss might need p ě Ωpnq samples.
(a) Observation

(b) Ground truth

(c) Huber-loss

(d) ℓ4-loss

(e) Ground truth

(f) Huber-loss

(g) ℓ4-loss

Figure 6: STORM imaging via solving MCS-BD. The ﬁrst line shows (a) observed image 
(b) ground truth  (c) recovered image by optimizing Huber-loss  and (d) by ℓ4-loss. The second
line  (e) ground truth kernel  (f) recovered by optimizing Huber-loss  and (g) by ℓ4-loss.

Real experiment on 2D super-resolution microscopy imaging. As introduced in Section 1 
stochastic optical reconstruction microscopy (STORM) is a new computation based imaging tech-
nique which breaks the resolution limits of optical ﬂuorescence microscopy [4  17  18]. The basic
principle is using photoswitchable ﬂorescent probes to create multiple images Yi (cid:16) A f Xi  where
f denotes 2D circular convolution  A is PSF  and tXiup
i(cid:16)1 are sparse point-sources. In 3D imaging 
the PSF A is hard to estimate due to defocus and unknown aberrations [19]  so that we want to jointly
estimate the PSF A and point sources tXiup
i(cid:16)1 are recovered  we can obtain a high
resolution image by aggregating all Xi. We test our algorithms on this task  by using p (cid:16) 1000
frames obtained from a standard dataset17. As demonstrated in Figure 6  optimizing Huber-loss
using vanilla RGD can near perfectly recover both the underlying Bessel PSF and point-sources 
producing accurate high resolution image. In contrast  optimizing ℓ4-loss [21] fails to recover the
PSF  resulting in some aliasing effects of the recovered image.
Discussion & Acknowledgement
Due to space limitation  we refer readers to Section 5 of our full paper [31] for a comprehensive dis-
cussion. QQ also would like to acknowledge the support of Microsoft PhD fellowship  and Moore-
Sloan foundation fellowship. XL would like to acknowledge the support by Grant CUHK14210617
from the Hong Kong Research Grants Council. ZZ was partly supported by NSF Grant 1704458.

i(cid:16)1. Once tXiup

17Available at http://bigwww.epfl.ch/smlm/datasets/index.html?p=tubulin-conjal647.

9

References
[1] Yenson Lau  Qing Qu  Han-Wen Kuo  Pengcheng Zhou  Yuqian Zhang  and John Wright. Short-and-

sparse deconvolution – a geometric approach. Preprint  2019.

[2] Shun-ichi Amari  Scott C Douglas  Andrzej Cichocki  and Howard H Yang. Multichannel blind deconvo-
lution and equalization using the natural gradient. In First IEEE Signal Processing Workshop on Signal
Processing Advances in Wireless Communications  pages 101–104. IEEE  1997.

[3] Ning Tian  Sung-Hoon Byun  Karim Sabra  and Justin Romberg. Multichannel myopic deconvolution in
underwater acoustic channels via low-rank recovery. The Journal of the Acoustical Society of America 
141(5):3337–3348  2017.

[4] Eric Betzig  George H Patterson  Rachid Sougrat  O Wolf Lindwasser  Scott Olenych  Juan S Bonifacino 
Michael W Davidson  Jennifer Lippincott-Schwartz  and Harald F Hess. Imaging intracellular ﬂuorescent
proteins at nanometer resolution. Science  313(5793):1642–1645  2006.

[5] Huajun She  Rong-Rong Chen  Dong Liang  Yuchou Chang  and Leslie Ying. Image reconstruction from
phased-array data based on multichannel blind deconvolution. Magnetic resonance imaging  33(9):1106–
1113  2015.

[6] Kjetil F Kaaresen and Toﬁnn Taxt. Multichannel blind deconvolution of seismic signals. Geophysics 

63(6):2093–2107  1998.

[7] Kenji Nose-Filho  André K Takahata  Renato Lopes  and João MT Romano. A fast algorithm for sparse

multichannel blind deconvolution. Geophysics  81(1):V7–V16  2015.
[8] Audrey Repetti  Mai Quyen Pham  Laurent Duval  Emilie Chouzenoux  and Jean-Christophe Pesquet. Eu-
clid in a taxicab: Sparse blind deconvolution with smoothed ℓ1{ℓ2 regularization. IEEE signal processing
letters  22(5):539–543  2015.

[9] Darren R Gitelman  William D Penny  John Ashburner  and Karl J Friston. Modeling regional and
psychophysiologic interactions in fmri: the importance of hemodynamic deconvolution. Neuroimage 
19(1):200–207  2003.

[10] Chaitanya Ekanadham  Daniel Tranchina  and Eero P Simoncelli. A blind sparse deconvolution method
for neural spike identiﬁcation. In Advances in Neural Information Processing Systems  pages 1440–1448 
2011.

[11] Guo-Rong Wu  Wei Liao  Sebastiano Stramaglia  Ju-Rong Ding  Huafu Chen  and Daniele Marinazzo.
A blind deconvolution approach to recover effective connectivity brain networks from resting state fmri
data. Medical image analysis  17(3):365–374  2013.

[12] Johannes Friedrich  Pengcheng Zhou  and Liam Paninski. Fast online deconvolution of calcium imaging

data. PLoS computational biology  13(3):e1005423  2017.

[13] Eftychios A Pnevmatikakis  Daniel Soudry  Yuanjun Gao  Timothy A Machado  Josh Merel  David Pfau 
Thomas Reardon  Yu Mu  Clay Laceﬁeld  Weijian Yang  et al. Simultaneous denoising  deconvolution 
and demixing of calcium imaging data. Neuron  89(2):285–299  2016.

[14] Anat Levin  Yair Weiss  Fredo Durand  and William T Freeman. Understanding blind deconvolution
algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence  33(12):2354–2367  2011.
[15] Haichao Zhang  David Wipf  and Yanning Zhang. Multi-image blind deblurring using a coupled adaptive
sparse prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
1051–1058  2013.

[16] Filip Sroubek and Peyman Milanfar. Robust multichannel blind deconvolution via fast alternating mini-

mization. IEEE Transactions on Image processing  21(4):1687–1700  2012.

[17] Samuel T Hess  Thanu PK Girirajan  and Michael D Mason. Ultra-high resolution imaging by ﬂuores-

cence photoactivation localization microscopy. Biophysical journal  91(11):4258–4272  2006.

[18] Michael J Rust  Mark Bates  and Xiaowei Zhuang. Sub-diffraction-limit imaging by stochastic optical

reconstruction microscopy (storm). Nature methods  3(10):793  2006.

[19] Pinaki Sarder and Arye Nehorai. Deconvolution methods for 3-d ﬂuorescence microscopy images. IEEE

Signal Processing Magazine  23(3):32–45  2006.

[20] Liming Wang and Yuejie Chi. Blind deconvolution from multiple sparse inputs. IEEE Signal Processing

Letters  23(10):1384–1388  2016.

[21] Yanjun Li and Yoram Bresler. Global geometry of multichannel sparse blind deconvolution on the sphere.

arXiv preprint arXiv:1805.10437  2018.

[22] Qing Qu  Ju Sun  and John Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating

directions. In Advances in Neural Information Processing Systems  pages 3401–3409  2014.

10

[23] Augustin Cosse. A note on the blind deconvolution of multiple sparse signals from unknown subspaces. In
Wavelets and Sparsity XVII  volume 10394  page 103941N. International Society for Optics and Photonics 
2017.

[24] Dar Gilboa  Sam Buchanan  and John Wright. Efﬁcient dictionary learning with gradient descent. arXiv

preprint arXiv:1809.10313  2018.

[25] Yu Bai  Qijia Jiang  and Ju Sun. Subgradient descent learns orthogonal dictionaries. arXiv preprint

arXiv:1810.10702  2018.

[26] Yuexiang Zhai  Zitong Yang  Zhenyu Liao  John Wright  and Yi Ma. Complete dictionary learning via

ℓ4-norm maximization over the orthogonal group. arXiv preprint arXiv:1906.02435  2019.

[27] Yanjun Li  Kiryung Lee  and Yoram Bresler. Identiﬁability in bilinear inverse problems with applications
to subspace or sparsity-constrained blind gain and phase calibration. IEEE Transactions on Information
Theory  63(2):822–842  2017.

[28] Shuyang Ling and Thomas Strohmer. Self-calibration and bilinear inverse problems via linear least

squares. SIAM Journal on Imaging Sciences  11(1):252–292  2018.

[29] Hilton Bristow  Anders Eriksson  and Simon Lucey. Fast convolutional sparse coding. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition  pages 391–398  2013.

[30] Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative review

and new algorithms. IEEE Transactions on Computational Imaging  4(3):366–381  2018.

[31] Qing Qu  Xiao Li  and Zhihui Zhu. A nonconvex approach for exact and efﬁcient multichannel sparse
[32] Robert M Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends R⃝ in Communi-

blind deconvolution. in preparation.

cations and Information Theory  2(3):155–239  2006.

[33] Thomas F Coleman and Alex Pothen. The null space problem i. complexity. SIAM Journal on Algebraic

Discrete Methods  7(4):527–537  1986.

[34] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing 

24(2):227–234  1995.

[35] Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics  pages 492–518.

Springer  1992.

[36] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere i: Overview and the

geometric picture. IEEE Transactions on Information Theory  63(2):853–884  2016.

[37] Yuqian Zhang  Han-wen Kuo  and John Wright. Structured local minima in sparse blind deconvolution.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances
in Neural Information Processing Systems 31  pages 2328–2337. Curran Associates  Inc.  2018.

[38] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere ii: Recovery by rieman-

nian trust-region method. IEEE Transactions on Information Theory  63(2):885–914  2017.

[39] Emmanuel J. Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory

and algorithms. Information Theory  IEEE Transactions on  61(4):1985–2007  April 2015.

[40] Suriya Gunasekar  Blake E Woodworth  Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro. Im-
plicit regularization in matrix factorization. In Advances in Neural Information Processing Systems  pages
6151–6159  2017.

[41] Cong Ma  Kaizheng Wang  Yuejie Chi  and Yuxin Chen. Implicit regularization in nonconvex statistical
estimation: Gradient descent converges linearly for phase retrieval  matrix completion and blind deconvo-
lution. arXiv preprint arXiv:1711.10467  2017.

[42] Yuejie Chi  Yue M Lu  and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization:

An overview. arXiv preprint arXiv:1809.09573  2018.

[43] Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-rank matrix estimation:
IEEE Signal Processing

Recent theory and fast algorithms via convex and nonconvex optimization.
Magazine  35(4)  2018.

[44] Yuxin Chen  Yuejie Chi  Jianqing Fan  and Cong Ma. Gradient descent with random initialization: fast

global convergence for nonconvex phase retrieval. Mathematical Programming  176(1-2):5–37  2019.

[45] Behnam Neyshabur  Ryota Tomioka  Ruslan Salakhutdinov  and Nathan Srebro. Geometry of optimiza-

tion and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071  2017.

[46] Michael Grant  Stephen Boyd  and Yinyu Ye. Cvx: Matlab software for disciplined convex programming 

2008.

[47] Stephen Boyd  Lin Xiao  and Almir Mutapcic. Subgradient methods. lecture notes of EE392o  Stanford

University  Autumn Quarter  2004:2004–2005  2003.

11

[48] Jean-Louis Gofﬁn. On convergence rates of subgradient optimization methods. Mathematical program-

ming  13(1):329–347  1977.

[49] Xiao Li  Zhihui Zhu  Anthony Man-Cho So  and Rene Vidal. Nonconvex robust low-rank matrix recovery.

arXiv preprint arXiv:1809.09237  2018.

[50] Damek Davis  Dmitriy Drusvyatskiy  Kellie J MacPhee  and Courtney Paquette. Subgradient methods
for sharp weakly convex functions. Journal of Optimization Theory and Applications  179(3):962–982 
2018.

[51] Xiao Li  Zhihui Zhu  Anthony Man-Cho So  and Jason D Lee. Incremental methods for weakly convex

optimization. arXiv preprint arxiv.org:1907.11687  2019.

[52] James V Burke and Michael C Ferris. Weak sharp minima in mathematical programming. SIAM Journal

on Control and Optimization  31(5):1340–1359  1993.

12

,Qing Qu
Xiao Li
Zhihui Zhu