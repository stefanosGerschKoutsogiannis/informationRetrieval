2016,Stochastic Gradient Geodesic MCMC Methods,We propose two stochastic gradient MCMC methods for sampling from Bayesian posterior distributions defined on Riemann manifolds with a known geodesic flow  e.g. hyperspheres. Our methods are the first scalable sampling methods on these manifolds  with the aid of stochastic gradients. Novel dynamics are conceived and 2nd-order integrators are developed. By adopting embedding techniques and the geodesic integrator  the methods do not require a global coordinate system of the manifold and do not involve inner iterations. Synthetic experiments show the validity of the method  and its application to the challenging inference for spherical topic models indicate practical usability and efficiency.,Stochastic Gradient Geodesic MCMC Methods

â€  Dept. of Comp. Sci. & Tech.  TNList Lab; Center for Bio-Inspired Computing Research

â€  State Key Lab for Intell. Tech. & Systems  Tsinghua University  Beijing  China

Chang Liuâ€   Jun Zhuâ€   Yang Songâ€¡âˆ—

â€¡ Dept. of Physics  Tsinghua University  Beijing  China

{chang-li14@mails  dcszj@}.tsinghua.edu.cn; songyang@stanford.edu

Abstract

We propose two stochastic gradient MCMC methods for sampling from Bayesian
posterior distributions deï¬ned on Riemann manifolds with a known geodesic ï¬‚ow 
e.g. hyperspheres. Our methods are the ï¬rst scalable sampling methods on these
manifolds  with the aid of stochastic gradients. Novel dynamics are conceived
and 2nd-order integrators are developed. By adopting embedding techniques and
the geodesic integrator  the methods do not require a global coordinate system of
the manifold and do not involve inner iterations. Synthetic experiments show the
validity of the method  and its application to the challenging inference for spherical
topic models indicate practical usability and efï¬ciency.

1

Introduction

Dynamics-based Markov Chain Monte Carlo methods (D-MCMCs) are sampling methods using
dynamics simulation for state transition in a Markov chain. They have become a workhorse for
Bayesian inference  with well-known examples like Hamiltonian Monte Carlo (HMC) [22] and
stochastic gradient Langevin dynamics (SGLD) [29]. Here we consider variants for sampling from
distributions deï¬ned on Riemann manifolds. Overall  geodesic Monte Carlo (GMC) [7] stands out
for its notable performance on manifolds with known geodesic ï¬‚ow  such as simplex  hypersphere
and Stiefel manifold [26  16]. Its applicability to manifolds with no global coordinate systems (e.g.
hyperspheres) is enabled by the embedding technique  and its geodesic integrator eliminates inner
(within one step in dynamics simulation) iteration to ensure efï¬ciency. It is also used for efï¬cient
sampling from constraint distributions [17]. Constrained HMC (CHMC) [6] aims at manifolds deï¬ned
by a constraint in some Rn. It covers all common manifolds  but inner iteration makes it less appealing.
Other D-MCMCs involving Riemann manifold  e.g. Riemann manifold Langevin dynamics (RMLD)
and Riemann manifold HMC (RMHMC) [13]  are invented for better performance but still on the task
of sampling in Euclidean space  where the target variable is treated as the global coordinates of some
distribution manifold. Although they can be used to sample in non-Euclidean Riemann manifolds by
replacing the distribution manifold with the target manifold  a global coordinate system of the target
manifold is required. Moreover  RMHMC suffers from expensive inner iteration.
However  GMC scales undesirably to large datasets  which are becoming common. An effective
strategy to scale up D-MCMCs is by randomly sampling a subset to estimate a noisy but unbiased
stochastic gradient  with stochastic gradient MCMC methods (SG-MCMCs). Welling et al. [29]
pioneered in this direction by developing stochastic gradient Langevin dynamics (SGLD). Chen
et al. [9] apply the idea to HMC with stochastic gradient HMC (SGHMC)  where a non-trivial
dynamics with friction has to be conceived. Ding et at. [10] propose stochastic gradient NosÃ©-Hoover
thermostats (SGNHT) to automatically adapt the friction to the noise by a thermostats. To unify
dynamics used for SG-MCMCs  Ma et al. [19] develop a complete recipe to formulate the dynamics.

âˆ—JZ is the corresponding author; YS is with Department of Computer Science  Stanford University  CA.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Table 1: A summary of some D-MCMCs. â€“: sampling on manifold not supported; â€ : The integrators
are not in the SSI scheme (It is unclear whether the claimed â€œ2nd-orderâ€ is equivalent to ours); â€¡:
2nd-order integrators for SGHMC and mSGNHT are developed by [8] and [18]  respectively.

methods

GMC [7]
RMLD [13]
RMHMC [13]
CHMC [6]
SGLD [29]
SGHMC [9] / SGNHT [10]
SGRLD [23] / SGRHMC [19]
SGGMC / gSGNHT (proposed)

stochastic
gradient

Ã—
Ã—
Ã—
Ã—
âˆš
âˆš
âˆš
âˆš

no inner
iteration

âˆš
âˆš
Ã—
Ã—
âˆš
âˆš
âˆš
âˆš

no global
coordinates

âˆš
Ã—
Ã—
âˆš
â€“
â€“
Ã—
âˆš

order of
integrator

2nd
1st
2ndâ€ 
2ndâ€ 
1st
1stâ€¡
1st
2nd

In this paper  we present two SG-MCMCs for manifolds with known geodesic ï¬‚ow: stochastic
gradient geodesic Monte Carlo (SGGMC) and geodesic stochastic gradient NosÃ©-Hoover thermostats
(gSGNHT). They are the ï¬rst scalable sampling methods on manifolds with known geodesic ï¬‚ow and
no global coordinate systems. We use the recipe [19] to tackle the non-trivial dynamics conceiving
task. Our novel dynamics are also suitable for developing 2nd-order integrators by adopting the
symmetric splitting integrator (SSI) [8] scheme. A key property of a Kth-order integrator is the
bias of the expected sample average at iteration L can be upper bounded by Lâˆ’K/(K+1) and the
mean square error by Lâˆ’2K/(2K+1) [8]  so a higher order integrator basically performs better. Our
integrators also incorporate the geodesic integrator to avoid inner iteration. Our methods can also be
used to scalably sample from constraint distributions [17] like GMC.
There exist other SG-MCMCs on Riemann manifold  e.g. SGRLD [23] and SGRHMC [19]  stochastic
gradient versions of RMLD and RMHMC respectively. But they also require the Riemann man-
ifold to have a global coordinate system  like their original versions as is mentioned above. So
basically they cannot draw samples from hyperspheres  while our methods are capable. Technically 
SGRLD/SGRHMC (and RMLD/RMHMC) samples in the coordinate space  so we need a global one
to make it valid. The explicit use of the Riemann metric tensor also makes the methods more difï¬cult
to implement. Our methods (and GMC) sample in the isometrically embedded space  where the
whole manifold is represented and the Riemann metric tensor is implicitly embodied by the isometric
embedding. Moreover  our integrators are of a higher order. Tab. 1 summarizes the key properties of
aforementioned D-MCMCs  where our advantages are clearly shown.
Finally  we apply our samplers to perform inference for spherical admixture models (SAM) [24].
SAM deï¬nes a hierarchical generative process to describe the data that are expressed as unit vectors
(i.e.  elements on the hypersphere). The task of posterior inference is to identify a set of latent topics 
which are also unit vectors. This process is highly challenging due to a non-conjugate structure and
the strict manifold constraints. None of the existing MCMC methods is both applicable to the task
and scalable. We demonstrate that our methods are the most efï¬cient methods to learn SAM on large
datasets  with a good performance on testing data perplexity.

âˆ’âˆ‡ log Ï€0(q) âˆ’(cid:80)D

2 Preliminaries
We brieï¬‚y review the basics of SG-MCMCs. Consider a Bayesian model with latent variable q  prior
Ï€0(q) and likelihood Ï€(x|q). Given a dataset D = {xd}D
d=1  sampling from the posterior Ï€(q|D)
by D-MCMCs requires computing the gradient of potential energy âˆ‡U (q) (cid:44) âˆ’âˆ‡ log Ï€(q|D) =
d=1 âˆ‡ log Ï€(xd|q)  which is linear to data size D thus not scalable. SG-MCMCs
(cid:80)
address this challenge by randomly drawing a subset S of D to build the stochastic gradient âˆ‡q ËœU (q) (cid:44)
âˆ’âˆ‡q log Ï€0(q)âˆ’ D|S|
xâˆˆS âˆ‡q log Ï€(x|q)  a noisy but unbiased estimate.Under the i.i.d. assumption
of D  the central limit theorem holds: in the sense of convergence in distribution for large D 

âˆ‡q ËœU (q) = âˆ‡qU (q) + N (0  V (q)) 

(1)

where we use N (Â· Â·) to denote a Gaussian random variable and V (q) is some covariance matrix.
The gradient noise raises challenging restrictions to the SG-MCMC dynamics. Ma et al. [19]
then provide a recipe to construct correct dynamics. It claims that for a random variable z  given
a Hamiltonian H(z)  a skew-symmetric matrix (curl matrix) Q(z) and a positive deï¬nite matrix
(diffusion matrix) D(z)  the dynamics deï¬ned by the following stochastic differential equation (SDE)

2

(2)
has the unique stationary distribution Ï€(z) âˆ exp{âˆ’H(z)}  where W (t) is the Wiener process and
(3)

f (z) = âˆ’ [D(z) + Q(z)]âˆ‡zH(z) + Î“(z)  Î“i(z) =

(Dij(z) + Qij(z)) .

dz = f (z)dt +(cid:112)2D(z)dW (t)
(cid:88)

âˆ‚
âˆ‚zj

j

The above dynamics is compatible with stochastic gradient. For SG-MCMCs  z is usually an
augmentation of the target variable q  and the Hamiltonian usually follows the form H(z) = T (z) +
U (q). Referring to Eqn. (1)  âˆ‡q ËœH(z) = âˆ‡qH(z) + N (0  V (q)) and Ëœf (z) = f (z) + N (0  B(z)) 
where B(z) is the covariance matrix of the Gaussian noise passed from âˆ‡z ËœH(z) to Ëœf (z) through
Eqn. (3). We informally rewrite dW (t) as N (0  dt) and express dynamics Eqn. (2) as

dz =f (z)dt + N (0  2D(z)dt) = f (z)dt + N (0  B(z)dt2) + N(cid:0)0  2D(z)dt âˆ’ B(z)dt2(cid:1)
= Ëœf (z)dt + N(cid:0)0  2D(z)dt âˆ’ B(z)dt2(cid:1).

(4)
This tells us that the same dynamics can be exactly expressed by stochastic gradient. Moreover  the
recipe is complete: for any continuous Markov process deï¬ned by Eqn. (2) with a unique stationary
distribution Ï€(z) âˆ exp{âˆ’H(z)}  there exists a skew-symmetric matrix Q(z) so that Eqn. (3) holds.
3 Stochastic Gradient Geodesic MCMC Methods
We now formally develop our SGGMC and gSGNHT. We will describe the task settings  develop the
dynamics  and show how to simulate by 2nd-order integrators and stochastic gradient.
3.1 Technical Descriptions of the Settings
We ï¬rst describe a Riemann manifold. Main concepts
are depicted in Fig. 1. Let M be an m-dim Riemann
manifold  which is covered by a set of local coordi-
nate systems. Denote one of them by (N   Î¦)  where
N âŠ† M is an open subset  and Î¦ : N â†’ â„¦  Q (cid:55)â†’ q
with â„¦ (cid:44) Î¦(N ) âŠ† Rm  Q âˆˆ N and q âˆˆ â„¦ is a
homeomorphism. Additionally  transition mappings
between any two intersecting local coordinate systems
are required to be smooth. Denote the Riemann metric
tensor under (N   Î¦) by G(q)  an m Ã— m symmetric
positive-deï¬nite matrix. Another way to describe M is through embedding â€” a diffeomorphism
Î : M â†’ Î(M) âŠ† Rn (n â‰¥ m). In (N   Î¦)  Î can be embodied by a more sensible mapping
Î¾ (cid:44) Î â—¦ Î¦âˆ’1 : Rm â†’ Rn  q (cid:55)â†’ x  which links the coordinate space and the embedded space. For
convenience  we only consider isometric embeddings (whose existence is guaranteed [21]): Î such
  1 â‰¤ i  j â‰¤ m holds for any local coordinate system. Common
manifolds are subsets of some Rn  in which case the identity mapping (as Î) from Rn (where M is
deï¬ned) to Rn (the embedded space) is isometric.
To deï¬ne a distribution on a Riemann manifold  from which we want to sample  we need a measure.
In the coordinate space Rm  â„¦ naturally possesses the Lebesgue measure Î»m(dq)  and the probability
density can be deï¬ned in â„¦  which we denote as Ï€(q). In the embedded space Rn  Î(N ) naturally
possesses the Hausdorff measure Hm(dx)  and we denote the probability density w.r.t this measure

as Ï€H(x). The relation between them can be found by Ï€H(Î¾(q)) = Ï€(q)/(cid:112)|G(q)|.

Figure 1: An illustration of manifold M
with local coordinate system (N   Î¦) and
embedding Î. See text for details.

that G(q)ij =(cid:80)n

âˆ‚Î¾l(q)

âˆ‚Î¾l(q)

l=1

âˆ‚qi

âˆ‚qj

3.2 The Dynamics
We now construct our dynamics using the recipe [19] so that our dynamics naturally have the desired
stationary distribution  leading to correct samples. It is important to note that the recipe only suits for
dynamics in a Euclidean space. So we can only develop the dynamics in the coordinate space but
not in the embedded space Î(M)  which is generally not Euclidean. However it is advantageous to
simulate the dynamics in the embedded space (See Sec. 3.3).
Dynamics for SGGMC Deï¬ne the momentum in the coordinate space p âˆˆ Rm and the augmented
variable z = (q  p) âˆˆ R2m. Deï¬ne the Hamiltonian 2 H(z) = U (q) + 1
2 p(cid:62)G(q)âˆ’1p 
2Another derivation of the momentum and the Hamiltonian originated from physics in both coordinate and

2 log |G(q)| + 1

embedded spaces is provided in Appendix C.

3

â„ğ‘šğ‘š=2â„ğ‘›ğ‘›=3ğ’©ğ‘„ğ‘Î¦Î©Îğ’©ğœ‰Îğ‘¥â„³ï£±ï£²ï£³ dq =Gâˆ’1pdt

where U (q) (cid:44) âˆ’ log Ï€(q). We deï¬ne the Hamiltonian so that the canonical distribution Ï€(z) âˆ
exp{âˆ’H(z)} marginalized w.r.t p recovers the target distribution Ï€(q). For a symmetric positive
deï¬nite n Ã— n matrix C  deï¬ne the diffusion matrix D(z) and the curl matrix Q(z) as

D(z) =

0

0 M (q)(cid:62)CM (q)

  Q(z) =

(cid:19)

(cid:18) 0 âˆ’I

I

0

(cid:19)

 

(cid:18) 0

where we deï¬ne M (q)nÃ—m : M (q)ij = âˆ‚Î¾i(q)/âˆ‚qj. So from Eqn. (2  3)  the dynamics

dp = âˆ’ âˆ‡qU dt âˆ’ 1
2

âˆ‡q log |G|dt âˆ’ M(cid:62)CM Gâˆ’1p dt âˆ’ 1
2

âˆ‡q

has a unique stationary distribution Ï€(z) âˆ exp{âˆ’H(z)}.
(5)
Dynamics for gSGNHT Deï¬ne z = (q  p  Î¾) âˆˆ R2m+1  where Î¾ âˆˆ R is the thermostats. For a
positive C âˆˆ R  deï¬ne the Hamiltonian H(z) = U (q) + 1
2 (Î¾ âˆ’ C)2 
whose marginalized canonical distribution is Ï€(q) as desired. Deï¬ne D(z) and Q(z) as

2 p(cid:62)G(q)âˆ’1p + m

(cid:2)p(cid:62)Gâˆ’1p(cid:3)dt + N (0  2M(cid:62)CM dt)

(cid:32) 0

(cid:33)

0
0
0

0

D(z) =

0 CG(q)
0

0

  Q(z) =

Then by Eqn. (2  3) the proper dynamics of gSGNHT is

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

dq =Gâˆ’1pdt
dp = âˆ’ âˆ‡qU dt âˆ’ 1
2

dÎ¾ =(

1
m

p(cid:62)Gâˆ’1p âˆ’ 1)dt

âˆ’I
0

ï£¶ï£¸  

2 log |G(q)| + 1

0
I
p/m
0 âˆ’p(cid:62)/m 0

ï£«ï£­ 0
(cid:2)p(cid:62)Gâˆ’1p(cid:3)dt + N (0  2CGdt)

âˆ‡q

âˆ‡q log |G|dt âˆ’ Î¾p dt âˆ’ 1
2

.

(6)

These two dynamics are novel. They are extensions of the dynamics of SGHMC and SGNHT to
Riemann manifolds  respectively. Conceiving the dynamics in this form is also intended for the
convenience to develop 2nd-order geodesic integrators  which differs from SGRHMC.

3.3 Simulation with 2nd-order Geodesic Integrators
In this part we develop our integrators by following the symmetric splitting integrator (SSI) scheme [8] 
which is guaranteed to be of 2nd-order. The idea of SSI is to ï¬rst split the dynamics into parts with
each analytically solvable  then alternately simulate each exactly with the analytic solutions. Although
also SSI  the integrator of GMC does not ï¬t our dynamics where diffusion arises. But we adopt its
embedding technique to get rid of any local coordinate system thus release the global coordinate
system requirement. So we will solve and simulate the split dynamics in the isometrically embedded
space  where everything is expressed by the position x = Î¾(q) and the velocity v = Ë™x (which is
actually the momentum in the isometrically embedded space  see Appendix C; the overhead dot
means time derivative)  instead of q and p.
Integrator for SGGMC We ï¬rst split dynamics (5) into sub-SDEs with each analytically solvable:

ï£±ï£²ï£³dq =Gâˆ’1pdt

dp =âˆ’ 1
2

âˆ‡q

A :

(cid:2)p(cid:62)Gâˆ’1p(cid:3)dt

(cid:40)dq =0

  B :

dp =âˆ’M(cid:62)CMGâˆ’1pdt

  O :

ï£±ï£´ï£´ï£²ï£´ï£´ï£³

dq =0
dp =âˆ’âˆ‡qU (q)dtâˆ’ 1
2

+ N (0  2M(cid:62)CM dt)

âˆ‡qlog|G(q)|dt

.

As noted in GMC  the solution of dynamics A is the geodesic ï¬‚ow of the manifold [1]. Intuitively 
dynamics A describes motion with no force so a particle moves freely on the manifold  e.g. the
uniform motion in Euclidean space  and motion along great circles (velocity rotating with varying
tangents along the trajectory) on hypersphere Sdâˆ’1 (cid:44) {x âˆˆ Rd|(cid:107)x(cid:107) = 1} ((cid:107) Â· (cid:107) denotes (cid:96)2-norm).
The evolution of the position and velocity of this kind is the geodesic ï¬‚ow. We require an explicit
form of the geodesic ï¬‚ow in the embedded space. For Sdâˆ’1 

(cid:40)

x(t) = x(0) cos(Î±t) +(cid:0)v(0)/Î±(cid:1) sin(Î±t)

v(t) = âˆ’Î±x(0) sin(Î±t) + v(0) cos(Î±t)

(7)

4

is the geodesic ï¬‚ow expressed by the embedded variables x and v  where Î± = (cid:107)v(0)(cid:107).
By details in [7] or Appendix A  dynamics B and O are solved as

(cid:40)x(t) =x(0)
v(t) =v(0)+Î›(cid:0)x(0)(cid:1)(cid:2)âˆ’âˆ‡xUH(cid:0)x(0)(cid:1)t+N (0  2Ct)(cid:3)  

(cid:40)x(t) =x(0)
v(t) =expm(cid:8)âˆ’Î›(cid:0)x(0)(cid:1)Ct(cid:9)v(0)

  O :

B :

where UH(x) (cid:44) âˆ’ log Ï€H(x)  expm{Â·} is the matrix exponent  and Î›(x) is the projection onto the
tangent space at x in the embedded manifold. For Rn  Î›(x) = In (the identity mapping in Rn) and
for Snâˆ’1 embedded in Rn  Î›(x) = In âˆ’ xx(cid:62) (see Appendix A.3).
We further reduce dynamics B for scalar C: v(t) = Î›(x(0)) exp{âˆ’Ct}v(0) = exp{âˆ’Ct}v(0)  by
noting that exp{âˆ’Ct} is a scalar and v(0) already lies on the tangent space at x(0). To illustrate this
form  we expand the exponent for small t and get v(t) = (1 âˆ’ Ct)v(0)  which is exactly the action
of a friction dissipating energy to control injected noise  as proposed in SGHMC. Our investigation
reveals that this form holds generally for v as the momentum in the isometrically embedded space 
but not the usual momentum p in the coordinate space. In SGHMC  v and p are undistinguishable 
but in our case v can only lie in the tangent space and p is arbitrary in Rm.
Integrator for gSGNHT We split dynamics (6) in a similar way:
dq =0
dp =âˆ’âˆ‡qU dtâˆ’ 1
2

âˆ‡q log |G| dt

  B :

A :

.

dq =Gâˆ’1pdt
dp =âˆ’ 1
âˆ‡q
2
p(cid:62)Gâˆ’1pâˆ’1

(cid:2)p(cid:62)Gâˆ’1p(cid:3)dt
(cid:17)

(cid:16) 1

dÎ¾ =

dt

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

dp =âˆ’Î¾p dt
dÎ¾ =0

ï£±ï£²ï£³ dq =0
(cid:2)p(cid:62)G(q)âˆ’1p(cid:3) = âˆ‡q

  O :

+N (0  2CGdt)

m

(cid:2)p(cid:62)G(q)âˆ’1p(cid:3)(cid:62)
by motion with no force. Now the evolution of Î¾ can be solved as Î¾(t) = Î¾(0) +(cid:0) 1

For dynamics A  the solution of q and p is again the geodesic ï¬‚ow. To solve Î¾  we ï¬rst ï¬gure out that
for dynamics A  p(cid:62)Gâˆ’1p is constant: d
Ë™p =
âˆ’2 Ë™p(cid:62) Ë™q + 2 Ë™q(cid:62) Ë™p = 0. Alternatively we note that 1
2 v(cid:62)v is the kinetic energy 3 conserved

Ë™q+2(cid:2)G(q)âˆ’1p(cid:3)(cid:62)
m v(0)(cid:62)v(0) âˆ’ 1(cid:1) t.

2 p(cid:62)Gâˆ’1p = 1

dt

dÎ¾ =0

Dynamics O is identical to the one of SGGMC. Dynamics B can be solved similarly with only
v updated: v(t) = exp{âˆ’Î¾(0)t}v(0). Expansion of this recovers the dissipation of energy by an
adaptive friction proposed by SGNHT  and we extend it to an embedded space.
Now we consider incorporating stochastic gradient. Only the common dynamics O is affected.
Similar to Eqn. (1)  we express the stochastic gradient as âˆ‡x ËœUH(x) = âˆ‡xUH(x) + N (0  V (x)) 
then reformulate the solution of dynamics O as

v(t) = v(0) + Î›(cid:0)x(0)(cid:1) Â·(cid:104)âˆ’âˆ‡x ËœUH(cid:0)x(0)(cid:1)t + N(cid:16)

0  2Ctâˆ’V(cid:0)x(0)(cid:1)t2(cid:17)(cid:105)

.

(8)

To estimate the usually unknown V (x)  a simple way is just to take it as zero  in the sense that V (x)t2
is a higher order inï¬nitesimal of 2Ct for t as a small simulation step size. Another way to estimate
V (x) is by the empirical Fisher information  as is done in [2].
Finally  as SSI suggests  we simulate the complete dynamics by exactly simulating these solutions
alternately in an â€œABOBAâ€ pattern. For a time step size of Îµ  dynamics A and B advance by Îµ/2 for
once and dynamics O by Îµ. As other SG-MCMCs  we omit the unscalable Metropolis-Hastings test.
But the consistency is still guaranteed [8] of e.g. the estimation by averaging over samples drawn
from SG-MCMCs. Algorithms of SGGMC and gSGNHT are listed in Appendix E.
4 Application to Spherical Admixture Model
We now apply SGGMC/gSGNHT to solve the challenging task of posterior inference in Spherical
Admixture Model (SAM) [24]. SAM is a Bayesian topic model for spherical data (each datum is in
some Sdâˆ’1)  such as the tf-idf representation of text data. It enables more feature representations for
hierarchical Bayesian models  and have the beneï¬t over Latent Dirichlet Allocation (LDA) [5] to
directly model the absence of words. The structure of SAM is shown in Fig. 2. Each document vd 
each topic Î²k  the corpus mean Âµ and the hyper-parameter m are all in SV âˆ’1 with V the vocabulary
size. Each topic proportion Î¸d is in (K âˆ’ 1)-dim simplex with K the number of topics.

3p(cid:62)Gâˆ’1p = (Gâˆ’1p)(cid:62)G(Gâˆ’1p) = Ë™q(cid:62)(M(cid:62)M ) Ë™q = (M Ë™q)(cid:62)(M Ë™q) = v(cid:62)v for an isometric embedding.

5

SAM uses the von Mises-Fisher distribution (vMF) (see e.g.
on hyperspheres.
eter Îº âˆˆ R+ has pdf (w.r.t

where cd(Îº) = Îºd/2âˆ’1/(cid:0)(2Ï€)d/2Id/2âˆ’1(Îº)(cid:1) and Ir(Â·) denotes the modiï¬ed Bessel func-

[20]) to model variables
The vMF on Sdâˆ’1 with mean Âµ âˆˆ Sdâˆ’1 and concentration param-
the Hausdorff measure) vMF(x|Âµ  Îº) = cd(Îº) exp{ÎºÂµ(cid:62)x} 

tion of

the ï¬rst kind and order r.

Then the generating process of SAM is

â€¢ Draw Âµ âˆ¼ vMF(Âµ|m  Îº0);
â€¢ For k = 1  . . .   K  draw topic Î²k âˆ¼ vMF(Î²k|Âµ  Ïƒ);
â€¢ For d = 1  . . .   D  draw Î¸d âˆ¼ Dir(Î¸d|Î±) and vd âˆ¼
where Â¯v(Î²  Î¸d)(cid:44) Î²Î¸d
(cid:107)Î²Î¸d(cid:107) with Î² (cid:44) (Î²1  . . .   Î²K) is an approxi-
mate spherical weighted mean of topics. The joint distribution

of(cid:0)v (cid:44) (v1  . . .   vD)  Î²  Î¸ (cid:44) (Î¸1  . . .   Î¸K)  Âµ(cid:1) can be known.

vMF(vd|Â¯v(Î²  Î¸d)  Îº) 

Figure 2: An illustration of SAM
model structure.

(cid:90)

(cid:90) Ï€(Î²  Î¸|v)

Ï€(Î²|v)

âˆ‡Î²Ï€(Î²  Î¸|v)
Ï€(Î²  Î¸|v)

The inference task is to estimate the topic posterior Ï€(Î²|v). As it is intractable  [24] provides a mean-
ï¬eld variational inference method and solves an optimization problem under spherical constraint 
which is tackled by repeatedly normalizing. However  this treatment is not applicable to most
sampling methods since it may corrupt the distribution of the samples. [24] tries a simple adaptive
Metropolis-Hastings sampler with undesirable results  and no more attempt of sampling methods
appears. Due to the deï¬ciency of global coordinate system of hypersphere  most Riemann manifold
samplers including SGRLD and SGRHMC fail. To our knowledge  only CHMC and GMC are
suitable  yet not scalable. Our samplers are appropriate for the task  with the advantage of scalability.
Now we present our inference method that uses SGGMC/gSGNHT to directly sample from Ï€(Î²|v).
First we note that Âµ can be collapsed analytically and the marginalized distribution of (v  Î²  Î¸) is:

Ï€(v  Î²  Î¸) = cV (Îº0)cV (Ïƒ)KcV ((cid:107) Â¯m(Î²)(cid:107))âˆ’1

where Â¯m(Î²) (cid:44) Îº0m + Ïƒ(cid:80)K

(9)
k=1 Î²k. To sample from Ï€(Î²|v) using our samplers  we only need to
know a stochastic estimate of the gradient of potential energy âˆ‡Î²U (Î²|v) (cid:44) âˆ’âˆ‡Î² log Ï€(Î²|v)  which
can be estimated by adopting the technique used in [11]: âˆ‡Î² log Ï€(Î²|v) =

Dir(Î¸d|Î±)vMF(vd|Â¯v(Î²  Î¸d)  Îº) 

d=1

D(cid:89)

dÎ¸ = EÏ€(Î¸|Î² v) [âˆ‡Î² log Ï€(Î²  Î¸|v)]  

1

Ï€(Î²|v)

âˆ‡Î²

Ï€(Î²  Î¸|v)dÎ¸ =

(cid:80)N
where âˆ‡Î² log Ï€(Î²  Î¸|v) = âˆ‡Î² log Ï€(v  Î²  Î¸) is known  and the expectation can be estimated by aver-
aging over a set of samples {Î¸(n)}N
n=1 âˆ‡Î² log Ï€(v  Î²  Î¸(n)).
To draw {Î¸(n)}N
n=1  noting the simplex constraint and that the target distribution Ï€(Î¸|v  Î²) is known
up to a constant multiplier  we use GMC to do the task.
To scale up  we use a subset {d(s)}S
to get a stochastic estimate for each âˆ‡Î² log Ï€(v  Î²  Î¸(n)). The ï¬nal stochastic gradient is:

s=1 of indices of randomly chosen items from the whole data set

n=1 from Ï€(Î¸|v  Î²): âˆ‡Î²U (Î²|v) â‰ˆ 1

N

âˆ‡Î² ËœU (Î²|v) â‰ˆ âˆ‡Î² log cV ((cid:107) Â¯m(Î²)(cid:107)) âˆ’ Îº

D
N S

v(cid:62)
d(s)Â¯v(Î²  Î¸(n)

d(s)).

(10)

The inference algorithm for SAM by SGGMC/gSGNHT is summarized in Alg. 3 in Appendix E.
5 Experiments
We present empirical results on both synthetic and real datasets to prove the accuracy and efï¬ciency
of our methods. All target densities are expressed in the embedded space w.r.t the Hausdorff measure
so we omit the subscript â€œHâ€. Synthetic experiments are only for SGGMC since the advantage to use
thermostats has been shown by [10] and the effectiveness of gSGNHT is presented on real datasets.
Detailed settings of the experiments are provided in Appendix F.

the target distribution such that the potential energy is U (x) = âˆ’ log(cid:0)exp{5Âµ(cid:62)

5.1 Toy Experiment
We ï¬rst present the utility and check the correctness of SGGMC by a greenhouse experiment with
known stochastic gradient noise. Consider sampling from a circle (S1) for easy visualization. We set
where x  Âµ1  Âµ2 âˆˆ S1 and Âµ1 = âˆ’Âµ2 = Ï€

3 (angle from +x direction). The stochastic gradient is

1 x} + 2 exp{5Âµ(cid:62)

2 x}(cid:1) 

6

N(cid:88)

S(cid:88)

n=1

s=1

 ğ· ğ¾ ğ‘š ğœ…0 ğœ… ğœ‡ ğ›½ğ‘˜ ğœƒğ‘‘ ğ‘£ğ‘‘ ğ›¼ ğœ (a) Ï€(v1|D)

(c) Ï€(v1  v2|D)

(b) Ï€(v2|D)
Figure 4: (a-b): True and empirical densities for Ï€(v1|D) and Ï€(v2|D). (c) True (left) and empirical
by SGGMC (right) densities for Ï€(v1  v2|D).
produced by corrupting with N (0  1000I)  whose variance is used as V (x) in Eqn. (8) for sampling.
Fig. 3(a) shows 100 samples from SGGMC and empirical distribution of 10 000 samples in the
embedded space R2. True and empirical distributions are compared in Fig. 3(b) in angle space (local
coordinate space). We see no obvious corruption of the result when using stochastic gradient.
It should be stressed that although it is possi-
ble to apply scalable methods like SGRLD in
spherical coordinate systems (almost global
ones)  it is too troublesome to work out the
form of e.g. Riemann metric tensor  and spe-
cial treatments like reï¬‚ection at boundaries
have to be considered. Numerical instability
at boundaries also tends to appear. All these
will get even worse in higher dimensions. Our
methods work in embedded spaces  so all
these issues are bypassed and can be elegantly
extended to high dimensions.

(a) samples by SGGMC in
the embedded space
Figure 3: Toy experiment results: (a) samples and
empirical distribution of SGGMC; (b) comparison
of true and empirical distributions.

(b) distribution comparison in
angle space

5.2 Synthetic Experiment
We then test SGGMC on a simple Bayesian posterior estimation task. We adopt a model with similar
structure as the one used in [29]. Consider a mixture model of two vMFs on S1 with equal weights:
Ï€(v1)=vMF(v1|e1 Îº1)  Ï€(v2)=vMF(v2|e1 Îº2)  Ï€(xi|v1 v2)âˆvMF(xi|v1 Îºx) + vMF(xi|Âµ Îºx) 
where e1 = (1  0) and Âµ (cid:44) (v1 + v2)/(cid:107)v1 + v2(cid:107). The task is to infer the posterior Ï€(v1  v2|D)  where
D = {xi}D=100
24   v2 = Ï€
8
and Îº1 = Îº2 = Îºx = 20 by GMC. SGGMC uses empirical Fisher information in the way of [2]
for V (x) in Eqn. (8)  and uses 10 for batch size. Fig. 4(a-b) show the true and empirical marginal
posteriors of v1 and v2  and Fig. 4(c) presents empirical joint posterior by samples from SGGMC
and its true density. We see that samples from SGGMC exhibit no observable corruption when a
mini-batch is used  and fully explore the two modes and the strong correlation of v1 and v2. 4

is our synthetic data that is generated from the likelihood with v1 = âˆ’ Ï€

i=1

5.3 Spherical Admixture Models
Setups For baselines  we compare with the mean-ï¬eld variational inference (VI) by [24] and its
stochastic version (StoVI) based on [15]  as well as GMC methods. It is problematic for GMC to
directly sample from the target distribution Ï€(Î²|v) since the potential energy is hard to estimate  which
is required for Metropolis-Hastings (MH) test in GMC. An approximate Monte Carlo estimation is
provided in Appendix B and the corresponding method for SAM is GMC-apprMH. An alternative
is GMC-bGibbs  which adopts blockwise Gibbs sampling to alternately sample from Ï€(Î²|Î¸  v) and
Ï€(Î¸|Î²  v) (both known up to a constant multiplier) using GMC.
(cid:80)
We evaluate the methods by log-perplexity â€” the average of negative log-likelihood on a held-out
test set Dtest. Variational methods produce a single point estimate Ë†Î² and the log-perplexity is
log-perp = âˆ’ 1|Dtest|
and log-perp = âˆ’ 1|Dtest|

needs to be estimated. By noting that Ï€(vd|Î²) = (cid:82) Ï€(vd  Î¸d|Î²)dÎ¸d = EÏ€(Î¸d|Î²)[Ï€(vd|Î²  Î¸d)]  we

(cid:80)M
m=1 Ï€(vd|Î²(m))). In both cases the intractable Ï€(vd|Î²)

log Ï€(vd| Ë†Î²). Sampling methods draw a set of samples {Î²(m)}M

(cid:80)

log( 1
M

dâˆˆDtest

dâˆˆDtest

m=1

4Appendix D provides a rationale on the shape of the joint posterior.

7

âˆ’0.6âˆ’0.4âˆ’0.200.20.40.600.511.522.533.544.55 trueGMCSGGMCâˆ’0.6âˆ’0.4âˆ’0.200.20.40.600.511.522.533.544.55 trueGMCSGGMCâˆ’0.500.5âˆ’0.6âˆ’0.4âˆ’0.200.20.40.6âˆ’0.4âˆ’0.200.20.4âˆ’0.6âˆ’0.4âˆ’0.200.20.40.6 0.5 1 1.5 23021060240902701203001503301800 empirical distributionsamples from SGGMCâˆ’4âˆ’3âˆ’2âˆ’10123400.10.20.30.40.50.60.7Ï†Ï€(Ï†) trueGMCSGGMCd ) (exactly known from the generating process) over samples
n=1 drawn from Ï€(Î¸d|Î²) = Ï€(Î¸d) = Dir(Î±)  the prior of Î¸d. The log-perplexity is not

estimate it by averaging Ï€(vd|Î²  Î¸(n)
{Î¸(n)
d }N
comparable among different models so we exclude LDA from our baseline.
We show the performance of all methods on a small and a large dataset. Hyper-parameters of
SAM are ï¬xed while training and set the same for all methods. V (x) in Eqn. (8) is taken zero
for SGGMC/gSGNHT. All sampling methods are implemented 5 in C++ and fairly parallelized
by OpenMP. VI/StoVI are run in MATLAB codes by [24] and we only use their ï¬nal scores for
comparison. Appendix F gives further implementation details  including techniques to avoid overï¬‚ow.

small dataset
On the
The small dataset
is the
20News-different dataset
used by [24]  which con-
sists of 3 categories from
20Newsgroups dataset.
It
is small (1 666 training and
1 107 test documents) so
we have the chance to see
the eventual results of all
methods. We use 20 topics
and 50 as the batch size.
Fig. 5(a) shows the perfor-
mance of all methods. We
can see that our SGGMC and gSGNHT perform better than others. VI converges swiftly but cannot
go any lower due to the intrinsic gap between the mean-ï¬eld variational distribution and the true
posterior. StoVI converges slower than VI in this small scale case  and exhibits the same limit.
All sampling methods eventually go below variational methods  and ours go the lowest. gSGNHT
shows its beneï¬t to outperform SGGMC under the same setting. For our methods  an appropriately
smaller batch size achieves a better result due to the speed-up by subsampling. Note that even the
full-batch SGGMC and gSGNHT outperform GMC variants. This may be due to the randomness in
the dynamics helps jumping out of one local mode to another for a better exploration.

Figure 5: Evolution of log-perplexity along wall time of all methods
on (a) 20News-different dataset and (b) 150K Wikipedia subset.

(a) 20News-different

(b) 150K Wikipedia

On the large dataset For the large dataset  we use a subset of the Wikipedia dataset with 150K
training and 1K test documents  to challenge the scalability of all the methods. We use 50 topics and
100 as the batch size. Fig. 5(b) shows the outcome. We see that the gap between our methods and
other baselines gets larger  indicating our scalability. Bounded curves of VI/StoVI  the advantage of
using thermostats and subsampling speed-up appear again. Our full-batch versions are still better than
GMC variants. GMC-apprMH and GMC-bGibbs scale badly; they converge slowly in this case.

6 Conclusions and Discussions
We propose SGGMC and gSGNHT  SG-MCMCs for scalable sampling from manifolds with known
geodesic ï¬‚ow. They are saliently efï¬cient on their applications. Novel dynamics are constructed and
2nd-order geodesic integrators are developed. We apply the methods to SAM topic model for more
accurate and scalable inference. Synthetic experiments verify the validity and experiments for SAM
on real-world data shows an obvious advantage in accuracy over variational inference methods and
in scalability over other applicable sampling methods. There remains possible broad applications
of our methods  including models involving vMF (e.g. mixture of vMF [4  14  28]  DP mixture of
vMF [12  3  27])  constraint distributions [17] (e.g. truncated Gaussian)  and distributions on Stiefel
manifold (e.g. Bayesian matrix completion [25])  where the ability of scale-up will be appealing.

Acknowledgments
The work was supported by the National Basic Research Program (973 Program) of China (No.
2013CB329403)  National NSF of China Projects (Nos. 61620106010  61322308  61332007)  the
Youth Top-notch Talent Support Program  and Tsinghua Initiative Scientiï¬c Research Program (No.
20141080934).

5All the codes and data can be found at http://ml.cs.tsinghua.edu.cn/~changliu/sggmcmc-sam/.

8

1021031041051000150020002500300035004000450050005500wall time in seconds (log scale)logâˆ’perplexity VIStoVIGMCâˆ’apprMHGMCâˆ’bGibbsSGGMCâˆ’batchSGGMCâˆ’fullgSGNHTâˆ’batchgSGNHTâˆ’full1031041052000250030003500400045005000wall time in seconds (log scale)logâˆ’perplexity VIStoVIGMCâˆ’apprMHGMCâˆ’bGibbsSGGMCâˆ’batchSGGMCâˆ’fullgSGNHTâˆ’batchgSGNHTâˆ’fullReferences
[1] Ralph Abraham  Jerrold E Marsden  and Jerrold E Marsden. Foundations of mechanics. Benjamin/Cummings Publishing Company

Reading  Massachusetts  1978.

[2] Sungjin Ahn  Anoop Korattikara  and Max Welling. Bayesian posterior sampling via stochastic gradient ï¬sher scoring. arXiv preprint

arXiv:1206.6380  2012.

[3] Nguyen Kim Anh  Nguyen The Tam  and Ngo Van Linh. Document clustering using dirichlet process mixture model of von mises-ï¬sher

distributions. In The 4th International Symposium on Information and Communication Technology  SoICT 2013  page 131â€“138  2013.

[4] Arindam Banerjee  Inderjit S Dhillon  Joydeep Ghosh  and Suvrit Sra. Clustering on the unit hypersphere using von mises-ï¬sher

distributions. Journal of Machine Learning Research  6:1345â€“1382  2005.

[5] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning Research  3:993â€“

1022  2003.

[6] Marcus A. Brubaker  Mathieu Salzmann  and Raquel Urtasun. A family of mcmc methods on implicitly deï¬ned manifolds. In Proceed-

ings of the 15th International Conference on Artiï¬cial Intelligence and Statistics (AISTATS)  pages 161â€“172  2012.

[7] Simon Byrne and Mark Girolami. Geodesic monte carlo on embedded manifolds. Scandinavian Journal of Statistics  40(4):825â€“845 

2013.

[8] Changyou Chen  Nan Ding  and Lawrence Carin. On the convergence of stochastic gradient mcmc algorithms with high-order integrators.

In Advances in Neural Information Processing Systems  pages 2269â€“2277  2015.

[9] Tianqi Chen  Emily Fox  and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In Proceedings of the 31st International

Conference on Machine Learning (ICML-14)  pages 1683â€“1691  2014.

[10] Nan Ding  Youhan Fang  Ryan Babbush  Changyou Chen  Robert D. Skeel  and Hartmut Neven. Bayesian sampling using stochastic

gradient thermostats. In Advances in Neural Information Processing Systems  pages 3203â€“3211  2014.

[11] Chao Du  Jun Zhu  and Bo Zhang. Learning deep generative models with doubly stochastic mcmc. arXiv preprint arXiv:1506.04557 

2015.

[12] Kaushik Ghosh  Rao Jammalamadaka  and Ram C. Tiwari. Semiparametric bayesian techniques for problems in circular data. Journal

of Applied Statistics  30(2):145â€“161  2003.

[13] Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical

Society: Series B (Statistical Methodology)  73(2):123â€“214  2011.

[14] Siddharth Gopal and Yiming Yang. Von mises-ï¬sher clustering models. In Proceedings of the 31st International Conference on Machine

Learning (ICML-14)  2014.

[15] Matthew D. Hoffman  David M. Blei  Chong Wang  and John Paisley. Stochastic variational inference. The Journal of Machine Learning

Research  14(1):1303â€“1347  2013.

[16]

I. M. James. The topology of Stiefel manifolds  volume 24. Cambridge University Press  1976.

[17] Shiwei Lan  Bo Zhou  and Babak Shahbaba. Spherical hamiltonian monte carlo for constrained target distributions. In Proceedings of

the 31st International Conference on Machine Learning (ICML-14)  pages 629â€“637  2014.

[18] Chunyuan Li  Changyou Chen  Kai Fan  and Lawrence Carin. High-order stochastic gradient thermostats for bayesian learning of deep

models. arXiv preprint arXiv:1512.07662  2015.

[19] Yi-An Ma  Tianqi Chen  and Emily Fox. A complete recipe for stochastic gradient mcmc. In Advances in Neural Information Processing

Systems  pages 2899â€“2907  2015.

[20] Kanti V. Mardia and Peter E. Jupp. Distributions on spheres. Directional Statistics  pages 159â€“192  2000.

[21] John Nash. The imbedding problem for riemannian manifolds. Annals of Mathematics  pages 20â€“63  1956.

[22] Radford M. Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo  2  2011.

[23] Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the probability simplex. In Advances in Neural

Information Processing Systems  pages 3102â€“3110  2013.

[24] Joseph Reisinger  Austin Waters  Bryan Silverthorn  and Raymond J. Mooney. Spherical topic models.

International Conference on Machine Learning (ICML-10)  pages 903â€“910  2010.

In Proceedings of the 27th

[25] Yang Song and Jun Zhu. Bayesian matrix completion via adaptive relaxed spectral regularization. In The 30th AAAI Conference on

Artiï¬cial Intelligence (AAAI-16)  2016.

[26] Eduard L. Stiefel. Richtungsfelder und fernparallelismus in n-dimensionalen mannigfaltigkeiten. Commentarii Mathematici Helvetici 

8(1):305â€“353  1935.

[27] Julian Straub  Jason Chang  Oren Freifeld  and John W. Fisher III. A dirichlet process mixture model for spherical data. In Proceedings

of the 18th International Conference on Artiï¬cial Intelligence and Statistics (AISTATS)  pages 930â€“938  2015.

[28] Jalil Taghia  Zhanyu Ma  and Arne Leijon. Bayesian estimation of the von mises-ï¬sher mixture model with variational inference. IEEE

Transactions on Pattern Analysis and Machine Intelligence  36(9):1701â€“1715  2014.

[29] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International

Conference on Machine Learning (ICML-11)  pages 681â€“688  2011.

9

,Chang Liu
Jun Zhu
Yang Song