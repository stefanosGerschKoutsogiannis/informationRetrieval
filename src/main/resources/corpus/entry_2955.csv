2017,Learning Mixture of Gaussians with Streaming Data,In this paper  we study the problem of learning a mixture of Gaussians with streaming data: given a stream of $N$ points in $d$ dimensions generated by an unknown mixture of $k$ spherical Gaussians  the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians  accurately if they are sufficiently separated. Assuming each pair of centers are $C\sigma$ distant with $C=\Omega((k\log k)^{1/4}\sigma)$ and where $\sigma^2$ is the maximum variance of any Gaussian component  we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians \citep{vempalawang}. For finite samples  we show that a bias term based on the initial estimate decreases at $O(1/{\rm poly}(N))$ rate while variance decreases at nearly optimal rate of $\sigma^2 d/N$. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed  the asymptotic per-step time complexity of our algorithm is the optimal $d\cdot k$ while space complexity of our algorithm is $O(dk\log k)$.  In addition to the bias and variance terms which tend to $0$  the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an \emph{approximation error} that cannot be avoided. However  by using a streaming version of the classical \emph{(soft-thresholding-based)} EM method that exploits the Gaussian distribution explicitly  we show that for a mixture of two Gaussians the true means can be estimated consistently  with estimation error decreasing at nearly optimal rate  and tending to $0$ for $N\rightarrow \infty$.,Learning Mixture of Gaussians with Streaming Data

Aditi Raghunathan
Stanford University

aditir@stanford.edu

Prateek Jain

Microsoft Research  India
prajain@microsoft.com

Ravishankar Krishnaswamy

Microsoft Research  India
rakri@microsoft.com

Abstract

In this paper  we study the problem of learning a mixture of Gaussians with stream-
ing data: given a stream of N points in d dimensions generated by an unknown
mixture of k spherical Gaussians  the goal is to estimate the model parameters using
a single pass over the data stream. We analyze a streaming version of the popular
Lloyd’s heuristic and show that the algorithm estimates all the unknown centers of
the component Gaussians accurately if they are sufﬁciently separated. Assuming
each pair of centers are Cσ distant with C = Ω((k log k)1/4σ) and where σ2 is
the maximum variance of any Gaussian component  we show that asymptotically
the algorithm estimates the centers optimally (up to certain constants); our center
separation requirement matches the best known result for spherical Gaussians [18].
For ﬁnite samples  we show that a bias term based on the initial estimate decreases
at O(1/poly(N )) rate while variance decreases at nearly optimal rate of σ2d/N.
Our analysis requires seeding the algorithm with a good initial estimate of the true
cluster centers for which we provide an online PCA based clustering algorithm.
Indeed  the asymptotic per-step time complexity of our algorithm is the optimal
d · k while space complexity of our algorithm is O(dk log k).
In addition to the bias and variance terms which tend to 0  the hard-thresholding
based updates of streaming Lloyd’s algorithm is agnostic to the data distribution
and hence incurs an approximation error that cannot be avoided. However  by
using a streaming version of the classical (soft-thresholding-based) EM method
that exploits the Gaussian distribution explicitly  we show that for a mixture of
two Gaussians the true means can be estimated consistently  with estimation error
decreasing at nearly optimal rate  and tending to 0 for N → ∞.

1

Introduction

Clustering data into homogeneous clusters is a critical ﬁrst step in any data analysis/exploration task
and is used extensively to pre-process data  form features  remove outliers and visualize data. Due
to the explosion in amount of data collected and processed  designing clustering algorithms that
can handle large datasets that do not ﬁt in RAM is paramount to any big-data system. A common
approach in such scenarios is to treat the entire dataset as a stream of data  and then design algorithms
which update the model after every few points from the data stream. In addition  there are several
practical applications where the data itself is not available beforehand and is streaming in  for example
in any typical online system like web-search.
For such a model  the algorithm of choice in practice is the so-called streaming k-means heuristic.
It is essentially a streaming version of the celebrated k-means algorithm or Lloyd’s heuristic [8].
The basic k-means algorithm is designed for ofﬂine/batch data where each data point is assigned to
the nearest centroid and the centroids are then updated based on the assigned points; this process is
iterated till the solution is locally optimal. The streaming version of the k-means algorithm assigns
the new point from the stream to the closest centroid and updates this centroid immediately. That is 
unlike ofﬂine k-means which ﬁrst assigns all the points to the respective centroids and then updates

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the centroids  the streaming algorithm updates the centroids after each point  making it much more
space efﬁcient. While streaming k-means and its several variants are used heavily in practice  their
properties such as solution quality  time complexity of convergence have not been studied widely. In
this paper  we attempt to provide such a theoretical study of the streaming k-means heuristic. One
of the big challenges is that even the (ofﬂine) k-means algorithm attempts to solve a non-convex
NP-hard problem. Streaming data poses additional challenges because of the large noise in each
point that can deviate the solution signiﬁcantly.
In the ofﬂine setting  clustering algorithms are typically studied under certain simplifying assumptions
that help bypass the worst-case NP-hardness of these problems. One of the most widely studied
setting is when the data is sampled from a mixture of well-separated Gaussians [5  18  1]  which is
also the generative assumption that we impose on the data in this work. However  the online/streaming
version of the k-means algorithm has not been studied in such settings. In this work  we design and
study a variant of the popular online k-means algorithm where the data is streaming-in  we cannot
store more than logarithmically many data points  and each data point is sampled from a mixture of
well-separated spherical Gaussians. The goal of the algorithm is then to learn the means of each of
the Gaussians; note that estimating other parameters like variance  and weight of each Gaussian in
the mixture becomes simple once the true means are estimated accurately.
Our Results. Our main contribution is the ﬁrst bias-variance bound for the problem of learning
√
Gaussian mixtures with streaming data. Assuming that the centers are separated by Cσ where
log k) and if we seed the algorithm with initial cluster centers that are ≤ Cσ/20 distance
C = Ω(
away from the true centers  then we show that the error in estimating the true centers can be
decomposed into three terms and bound each one of them: (a) the bias term  i.e.  the term dependent
on distance of true means to initial centers decreases at a 1/poly(N ) rate  where N is the number

of data points observed so far  (b) the variance term is bounded by σ2(cid:0) d log N

(cid:1) where σ is the

N

√

standard deviation of each of the Gaussians  and d is the dimensionality of the data  and (c) an ofﬂine
approximation error: indeed  note that even the ofﬂine Lloyd’s heuristic will have an approximation
error due to its hard-thresholding nature. For example  even when k = 2  and the centers are separated
by Cσ  around exp(− C2
8 ) fraction of points from the ﬁrst Gaussian will be closer to the second
center  and so the k-means heuristic will converge to centers that are at a squared distance of roughly
O(C 2) exp(− C2
8 )σ2 from the true means. We essentially inherit this ofﬂine error up to constants.
Note that the above result holds at a center separation of Ω(
log kσ) distance  which is substantially
weaker than the currently best-known result of Ω(σk1/4) for even the ofﬂine problem [18]. However 
as mentioned before  this only holds provided we have a good initialization. To this end  we show
that when C = Ω(σ(k log k)1/4)  we can combine an online PCA algorithm [9  11] with the batch
k-means algorithm on a small seed sample of around O(k log k) points  to get such an initialization.
Note that this separation requirement nearly matches the best-known result ofﬂine results [18].
Finally  we also study a soft-version of streaming k-means algorithm  which can also be viewed
as the streaming version of the popular Expectation Maximization (EM) algorithm. We show that
for mixture of two well-separated Gaussians  a variant of streaming EM algorithm recovers the
above mentioned bias-variance bound but without the approximation error. That is  after observing
inﬁnite many samples  streaming EM converges to the true means and matches the corresponding
ofﬂine results in [3  6]; to the best of our knowledge this is also ﬁrst such consistency result for the
streaming mixture problem. However  the EM updates require that the data is sampled from mixture
of Gaussians  while the updates of streaming Lloyd’s algorithm are agnostic of the data distribution
and hence same updates can be used to solve arbitrary mixture of sub-Gaussians as well.
Technical Challenges. One key technical challenge in analyzing streaming k-means algorithm in
comparison to the standard streaming regression style problems is that the ofﬂine problem itself is
non-convex and moreover can only be solved approximately. Hence  a careful analysis is required to
separate out the error we get in each iteration in terms of the bias  variance  and inherent approximation
error terms. Moreover  due to the non-convexity  we are able to guarantee decrease in error only if
each of our iterates lies in a small ball around the true mean. While this is initially true due to the
initialization algorithm  our intermediate centers might escape these balls during our update. However 
we show using a delicate martingale based argument that with high probability  our estimates stay
within slightly larger balls around the true means  which turns out to be sufﬁcient for us.

2

Related Work. A closely related work to ours is an independent work by [17] which studies a
stochastic version of k-means for data points that satisfy a spectral variance condition which can be
seen as a deterministic version of the mixture of distributions assumption. However  their method
requires multiple passes over the data  thus doesn’t ﬁt directly in the streaming k-means setting.
In particular  the above mentioned paper analyzes the stochastic k-means method only for highly
accurate initial set of iterates which requires a large burn-in period of t = O(N 2) and hence needs
O(N ) passes over the data  where N is the number of data points. Tensor methods [1  10] can also
be extended to cluster streaming data points sampled from a mixture distribution but these methods
suffer from large sample/time complexity and might not provide reasonable results when the data
distribution deviates from the assumed generative model.
In addition to the gaussian mixture model  clustering problems are also studied under other models
such as data with small spectral variance [12]  stability of data [4]  etc. It would be interesting to
study the streaming versions in such models as well.
Paper Outline. We describe our models and problem setup in Section 2. We then present our
streaming k-means algorithm and its proof overview in Sections 3 and 4. We then discuss the
initialization procedure in Section 5. Finally we describe our streaming-EM algorithm in Section 6.

2 Setup and Notation

We assume that the data is drawn from a mixture of k spherical Gaussians distributions  i.e. 

xt i.i.d∼ (cid:88)

wiN (µ(cid:63)

i   σ2I)  µ(cid:63)

i ∈ Rd ∀i = 1  2  . . . k

(1)

i

i ∈ Rd is the mean of the i-th mixture component  mixture weights wi ≥ 0  and(cid:80)

i wi = 1.
where µ(cid:63)
All the problem parameters (i.e.  the true means  the variance σ2 and the mixture weights) are
unknown to the algorithm. Using the standard streaming setup  where the tth sample xt ∈ Rd is
drawn from the data distribution  our goal is to produce an estimate ˆµi of µ(cid:63)
i for i = 1  2  . . . k in a
single pass over the data using bounded space.
Center Separation. A suitable notion of signal to noise ratio for our problem turns out to be the ratio
of minimum separation between the true centers and the maximum variance along any direction. We
denote this ratio by C = mini j
by Cij. Here
and in the rest of the paper  (cid:107)y(cid:107) is the Euclidean norm of a vector y. We use η to denote the learning
rate of the streaming updates and µt

. For convenience  we also denote

i to denote the estimate of µ(cid:63)

i at time t.

(cid:107)µ(cid:63)

i −µ(cid:63)
j (cid:107)
σ

(cid:107)µ(cid:63)

i −µ(cid:63)
j (cid:107)
σ

Remarks. For a cleaner presentation  we assume that all the mixture weights are 1/k  but our results
hold with general weights as long as an appropriate center separation condition is satisﬁed. Secondly 
our proofs also go through when the Gaussians have different variances σi
2  as long as the separation
conditions are satisﬁed with σ = maxi σi. We furnish details in the full version of this paper [14].

3 Algorithm and Main Result

In this section  we describe our proposed streaming clustering algorithm and present our analysis of
the algorithm. At a high level  we follow the approach of various recent results for (ofﬂine) mixture
recovery algorithms [18  12]. That is  we initialize the algorithm with an SVD style operation which
de-noises the data signiﬁcantly in Algorithm 1 and then apply our streaming version of Lloyd’s
heuristic in Algorithm 2. Note that the Lloyd’s algorithm is agnostic to the underlying distribution
and does not include distribution speciﬁc terms like variance etc.
Intuitively  the initialization algorithm ﬁrst computes an online batch PCA in the for-loop. After this
step  we perform an ofﬂine distance-based clustering on the projected subspace (akin to Vempala-
Wang for the ofﬂine algorithm). Note that since we only need estimates for centers within a suitable
proximity from the true centers  this step only uses few (roughly k log k) samples. These centers are
fed as the initial centers for the streaming update algorithm. The streaming algorithm then  for each
new sample  updates the current center which is closest to the sample  and iterates.

3

Figure 1: Illustration of optimal K-means error

Algorithm 1 InitAlg(N0)

U ← random orthonormal matrix ∈ Rd×k
B = Θ(d log d)  S = 0
for t = 1 to N0 − k log k do

if mod(t  B) = 0 then
U ← QR(S · U ) 

S ← 0

end if
Receive xt as generated by the input stream
S = S + xt(xt)T

end for
X0 = [xN0−k log k+1  . . .   xN0]
Form nearest neighbor graph using U T X0 and
ﬁnd connected components
k] ← mean of points in each compo-
[ν0
nent
Return: [µ0
1  . . .   µ0

1   . . .   U ν0
k]

k] = [U ν0

1   . . .   ν0

.

N

k} ← InitAlgo(N0).

Algorithm 2 StreamKmeans(N  N0)
1: Set η ← 3k log 3N
2: Set {µ0
1  . . .   µ0
3: for t = 1 to N do
Receive xt+N0 given by the input stream
4:
x = xt+N0
5:
Let it = arg mini (cid:107)x − µt−1
6:
Set µt
7:
+ ηx
it
Set µt
8:
9: end for
10: Output: µN

= (1 − η)µt−1
for i (cid:54)= it
i = µt−1
1   . . .   µN
k

(cid:107).

it

i

i

We now present our main result for the streaming clustering problem.
Theorem 1. Let xt  1 ≤ t ≤ N + N0 be generated using a mixture of Gaussians (1) with wi = 1/k 
∀i. Let N0  N ≥ O(1)k3d3 log d and C ≥ Ω((k log k)1/4). Then  the mean estimates (µN
1   . . .   µN
k )
output by Algorithm 2 satisﬁes the following error bound:

(cid:34)(cid:88)

i

E

(cid:35)

(cid:107)µN

i − µ(cid:63)
i (cid:107)2

i (cid:107)2
≤ maxi (cid:107)µ(cid:63)
(cid:125)
(cid:123)(cid:122)
(cid:124)

NΩ(1)

bias

+O(k3)

σ2 d log N
(cid:125)
(cid:123)(cid:122)
(cid:124)

N

variance

(cid:124)

+ exp(−C 2/8)(C 2 + k)σ2

≈oﬄine k−means error

(cid:123)(cid:122)

 .
(cid:125)

Our error bound consists of three key terms: bias  variance  and ofﬂine k-means error  with bias and
variance being standard statistical error terms: (i) bias is dependent on the initial estimation error
and goes down at N ζ rate where ζ > 1 is a large constant; (ii) variance error is the error due to
noise in each observation xt and goes down at nearly optimal rate of ≈ σ2 d
N albeit with an extra
log N term as well as worse dependence on k; and (iii) an ofﬂine k-means error  which is the error
that even the ofﬂine Lloyds’ algorithm would incur for a given center separation C. Note that while
sampling from the mixture distribution  ≈ exp(−C 2/8) fraction of data-points can be closer to the
true means of other clusters rather than their own mean  because the tails of the distributions overlap.
Hence  in general it is not possible to assign back these points to the correct cluster  without any
modeling assumptions. These misclassiﬁed points will shift the estimated centers along the line
joining the means. See Figure 3 for an illustration. This error can however be avoided by performing
soft updates  which is discussed in Section 6.
Time  space  and sample complexity: Our algorithm has nearly optimal time complexity of O(d· k)
per iteration; the initialization algorithm requires about O(d4k3) time. Space complexity of our
algorithm is O(dk · log k) which is also nearly optimal. Finally  the sample complexity is O(d3k3) 
which is a loose upper bound and can be signiﬁcantly improved by a more careful analysis. To
compare  the best known sample complexity for the ofﬂine setting is ˜O(kd) [2]  which is better by a
factor of (dk)2.

4

√

Analysis Overview. The proof of Theorem 1 essentially follows from the two theorems stated below:
a) update analysis given a good initialization; b) InitAlg analysis for showing such an initialization.
Theorem 2 (Streaming Update). Let xt  N0 + 1 ≤ t ≤ N + N0 be generated using a mixture of
Gaussians (1) with wi = 1/k  ∀i  and N = Ω(k3d3 log kd). Also  let the center-separation C ≥
i (cid:107) ≤ Cσ
20 .
Ω(
(cid:19)
Then  the streaming update of StreamKmeans(N  N0)   i.e  Steps 3-8 of Algorithm 2 satisﬁes:

i are such that for all 1 ≤ i ≤ k  (cid:107)µ0
(cid:18)

log k)  and also suppose our initial centers µ0

i − µ(cid:63)

(cid:35)

+ O(k3)

exp(−C 2/8)(C 2 + k)σ2 +

log N

dσ2

.

(cid:107)µN

i − µ(cid:63)
i (cid:107)2

≤ maxi (cid:107)µ(cid:63)
i (cid:107)2

NΩ(1)

(cid:34)(cid:88)

i

E

N

√

Note that our streaming update analysis requires only C = Ω(
log k) separation but needs appropri-
ate initialization that is guaranteed by the below result.
Theorem 3 (Initialization). Let xt  1 ≤ t ≤ N0 be generated using a mixture of Gaussians (1)
with wi = 1/k  ∀i. Let µ0
and
N0 = Ω

  then w.p. ≥ 1 − 1/poly(k)  we have maxi (cid:107)µ0

k be the output of Algorithm 1. If C = Ω

(k log k)1/4(cid:17)

(cid:16)
i (cid:107) ≤ C

i − µ(cid:63)

d3k3 log dk

2  . . . µ0

1  µ0

(cid:16)

(cid:17)

20 σ.

4 Streaming Update Analysis

At a high level our analysis shows that at each step of the streaming updates  the error decreases on
average. However  due to the non-convexity of the objective function we can show such a decrease
only if the current estimates of our centers lie in a small ball around the true centers of the gaussians.
Indeed  while the initialization provides us with such centers  due to the added noise in each step 
our iterates may occasionally fall outside these balls  and we need to bound the probability that this
happens. To overcome this  we start with initial centers that are within slightly smaller balls around
the true means  and use a careful Martingale argument to show that even if the iterates go a little
farther from the true centers (due to noise)  with high probability  the iterates are still within the
slightly larger ball that we require to show decrease in error.
We therefore divide our proof in two parts: a) ﬁrst we show in Section 4.1 that the error decreases in
expectation  assuming that the current estimates lie in a reasonable neighborhood around the true
centers; and b) in Section 4.2) we show using a martingale analysis that with high probability  each
iterate satisﬁes the required neighborhood condition if the initialization is good enough.
We formalize the required condition for our per-iteration error analysis below. For the remainder of
this section  we ﬁx the initialization and only focus on Steps 3-8 of Algorithm 2.
Deﬁnition 1. For a ﬁxed initialization  and given a sequence of points ωt = (xt(cid:48)+N0+1 : 0 ≤ t(cid:48) <
t)  we say that condition It is satisﬁed at time t if maxi (cid:107)µt(cid:48)
10 holds for all 0 ≤ t(cid:48) ≤ t.
Note that given a sequence of points and a ﬁxed initialization  Algorithm 2 is deterministic.

i (cid:107) ≤ Cσ

i − µ(cid:63)

We now deﬁne the following quantities which will be useful in the upcoming analysis. At any
time t ≥ 1  let ωt = (xt(cid:48)+N0+1 : 0 ≤ t(cid:48) < t) denote the sequence of points received by our
i (cid:107)2 denote the random variable measuring the current
t to be the maximum cluster error at time t. Now  let

(cid:3) be the expected error of the ith cluster center after receiving
(cid:3) be the expected error

algorithm. For all t ≥ 0  let (cid:101)Ei
error for cluster i  and let (cid:101)Vt = maxi (cid:101)Ei
t = (cid:107)µt
i − µ(cid:63)
(cid:98)Ei
i (cid:107)2 |ωt
i − µ(cid:63)
conditioned on It  and let Et =(cid:80)

t+1 = E
the (t + 1)th  conditioned on ωt. Finally  let Ei

t = E(cid:2)(cid:107)µt

(cid:2)(cid:107)µt+1

i (cid:107)2 |It

i − µ(cid:63)

xt+N0+1

t.
i Ei

4.1 Error Reduction in Single Iteration

Our main tool toward showing Theorem 2 is the following theorem which bounds the expected error
after updating the means on arrival of the next sample.
√
Theorem 4. If It holds and C ≥ Ω(

log k)  then for all i  we have

k5(cid:101)Vt + O(1)η2dσ2 + O(k)η(1 − η) exp(−C 2/8)(C 2 + k)σ2 .

(cid:98)Ei
t+1 ≤(1 − η
2k

)(cid:101)Ei

t +

η

5

Proof sketch of Theorem 4. In all calculations in this proof  we ﬁrst assume that the candidate centers
satisfy It  and all expectations and probabilities are only over the new sample xt+N0+1  which we
denote by x after omitting the superscript. Now recall our update rule: µt+1
i + ηx if µt
i
is the closest center for the new sample x; the other centers are unchanged. To simplify notations  let:

i = (1 − η)µt

i (x) = 1 iff i = arg min
gt

j

(cid:107)x − µt

j(cid:107)  gt

i (x) = 0 otherwise.

(2)

By deﬁnition  we have for all i 
i = (1 − η)µt
µt+1

i + η(cid:0)gt

i (x)x + (1 − gt

i (x))µt
i

i + ηgt

i (x)(x − µt
i).

(cid:1) = µt

Our proof relies on the following simple yet crucial lemmas. The ﬁrst bounds the failure probability
of a sample being closest to an incorrect cluster center among our candidates. The second shows
that if the candidate centers are sufﬁciently close to the true centers  then the failure probability of
mis-classifying a point to a wrong center is (upto constant factors) the probability of mis-classiﬁcation
even in the optimal solution (with true centers). Finally the third lemma shows that the probability of
i (x) = 1 for each i is lower-bounded. Complete details and proofs appear in [14].
gt
Lemma 1. Suppose condition It holds. For any i  j (cid:54)= i  let x ∼ Cl(j) denote a random point from

j(cid:107)(cid:3) ≤ exp(−Ω(C 2

ij)).

i (cid:107)) ≤ σ/Cij. For any i  j (cid:54)= i  let x ∼ Cl(j) denote
i(cid:107) ≤ (cid:107)x − µt

j(cid:107)(cid:3) ≤ O(1) exp(−C 2

ij/8).
i (x) = 1] ≥ 1
2k .

log k)  then for all i  then Pr [gt

And so  equipped with the above notations and lemmas  we have

cluster j. Then Pr(cid:2)(cid:107)x − µt
a random point from cluster j. Then Pr(cid:2)(cid:107)x − µt

i(cid:107) ≤ (cid:107)x − µt
i − µ(cid:63)
i (cid:107) (cid:107)µt
√
Lemma 3. If It holds and C = Ω(

Lemma 2. Suppose max((cid:107)µt

i − µ(cid:63)

(cid:98)Ei

t+1 = Ex

= (1 − η)2(cid:107)µt

(cid:2)(cid:107)µt+1
i (cid:107)2(cid:3)
i (cid:107)2 + η2E(cid:2)(cid:107)gt
i − µ(cid:63)
+ 2η(1 − η)E(cid:104)(cid:68)
i  (cid:0)gt
i − µ(cid:63)
t + η2 E(cid:2)(cid:107)gt
i − µ(cid:63)
i (x)(x − µ(cid:63)
)(cid:101)Ei
µt
(cid:124)
(cid:123)(cid:122)
η2E(cid:2)(1 − gt
i (cid:107)2(cid:3) ≤ η2(cid:101)Ei
2η(1 − η)(cid:101)Ei
i (x) = 0] ≤ 2η(1 − η)(cid:101)Ei
i (x))(cid:107)µt
i − µ(cid:63)
terms with coefﬁcient (cid:101)Ei
t Pr [gt

≤ (1 − η
2k

i (x)(x − µ(cid:63)

T1

t.

i (x)(x − µ(cid:63)
i )(cid:107)2(cid:3)
i ) + (1 − gt
(cid:125)

i ) + (1 − gt
+2η(1 − η) E(cid:104)(cid:68)
(cid:124)

i (x))(µt
i − µ(cid:63)
i − µ(cid:63)
µt

i (x))(µt

i − µ(cid:63)

i )(cid:107)2(cid:3)
i )(cid:1)(cid:69)(cid:105)
i  (cid:0)gt
(cid:123)(cid:122)
i (x)(x − µ(cid:63)

T2

i )(cid:1)(cid:69)(cid:105)
(cid:125)

The last inequality holds because of the following line of reasoning:
term in the second squared norm evaluates to 0 due to the product gt
i   (1 − gt

the cross
i (x))  (ii)
i )(cid:105)] ≤
i − µ(cid:63)
t(1 − 1/2k) by Lemma 3  and ﬁnally (iv) by collecting

(i) ﬁrstly 
i (x)(1 − gt
i (x))(µt

t  (iii) 2η(1 − η)E [(cid:104)µt

i − µ(cid:63)

The proof then roughly proceeds as follows: suppose in an ideal case  gt
i (x) is 1 for all points x
generated from cluster i  and 0 otherwise. Then  if x is a random sample from cluster i  T1 would be
dσ2  and T2 would be 0. Of course  the difﬁculty is that gt
i (x) is not always as well-behaved  and so
the bulk of the analysis is in carefully using Lemmas 1and 2  and appropriately “charging” the various

error terms we get to the current error (cid:101)Ei

t  the variance  and the ofﬂine approximation error.

4.2 Ensuring Proximity Condition Via Super-Martingales
In the previous section  we saw that condition It = 1 is sufﬁcient to ensure that expected one-step
error reduces at time step t + 1. Our next result shows that IN = 1 is satisﬁed with high probability.
Theorem 5. Suppose maxi (cid:107)µ0

Our argument proceeds as follows. Suppose we track the behaviour of the actual error terms (cid:101)Ei

t
over time  and stop the process (call it a failure) when any of these error terms exceeds C 2σ2/100
(recall that they are all initially smaller than C 2σ2/400). Assuming that the process has not stopped 
we show that each of these error terms has a super-martingale behaviour using Theorem 4  which

20 σ  then IN = 1 w.p ≥ 1 − (

i (cid:107) ≤ C

i − µ(cid:63)

poly(N ) ).

1

6

says that on average  the expected one-step error drops. Moreover  we also show that the actual
one-step difference  while not bounded  has a sub-gaussian tail. Our theorem now follows by using
Azuma-Hoeffding type inequality for super-martingale sequences.

4.3 Wrapping Up

Now  using Theorems 4 and 5  we can get the following theorem.
Theorem 6. Let γ = O(k)η2dσ2 + O(k2)η(1 − η)exp(−C 2/8)(C 2 + k)σ2. Then if C ≥
η γ.
Ω(

log k)  for all t  we have Et+1 ≤ (1 − η

4k )N E0 + 4k

√

t+1 = E(cid:104)(cid:107)µt+1

i

i − µ(cid:63)

(cid:105)

i (cid:107)2(cid:12)(cid:12)(cid:12)It

4k )Et + γ. It follows that EN ≤ (1 − η
to be the average over all sample paths of (cid:101)Ei

Proof. Let E
t+1 condi-
tioned on It. Recall that Et+1 is very similar  except the conditioning is on It+1. With this notation 
let us take expectation over all sample paths where It is satisﬁed  and use Theorem 4 to get
k5 Et + O(1)η2dσ2 + O(k)η(1 − η) exp(−C 2/8)(C 2 + k)σ2 .

t+1 ≤(1 − η
2k

)Ei

t +

E

η

i

And so  summing over all i we will get

Et+1 ≤(1 − η
3k

)Et + O(k)η2dσ2 + O(k2)η(1 − η) exp(−C 2/8)(C 2 + k)σ2 .

N 2 ) since Pr [It+1] ≥ 1 − 1/N 5 by Theorem 5.

Finally note that Et+1 and Et+1 are related as Et+1 Pr [It+1] ≤ Et+1 Pr [It]  and so Et+1 ≤
Et+1(1 + 1
Proof of Theorem 2. From Theorem 5 we know that the probability of IN being satisﬁed is 1−1/N 5 
and in this case  we can use Theorem 6 to get the desired error bound. In case IN fails  then the
maximum possible error is roughly maxi j (cid:107)µ(cid:63)
j(cid:107)2 · N (when all our samples are sent to the
same cluster)  which contributes a negligible amount to the bias term.

i − µ(cid:63)

5

Initialization for streaming k-means

log k)σ if we can initialize all centers such that (cid:107)µ0

√
In Section 4 we saw that our proposed streaming algorithm can lead to a good solution for any
separation Cσ ≥ O(
20 σ. We now
show that InitAlg (Algorithm 1) is one such procedure. We ﬁrst approximately compute the top-k
eigenvectors U of the data covariance using a streaming PCA algorithm [9  13] on O(k3d3 log d)
samples. We next store k log k points and project them onto the subspace spanned by U. We then
perform a simple distance based clustering [18] that correctly clusters the stored points (assuming
reasonable center separation)  and ﬁnally we output these cluster centers.

i (cid:107) ≤ C

i − µ(cid:63)

Proof of Theorem 3. Using an argument similar to [9] (Theorem 3)  we get that U obtained by the
online PCA algorithm (Steps 1:4 of Algorithm 1) satisﬁes (w.p. ≥ 1 − 1/poly(d)):

Now  let(cid:98)µ

∗
i = U T µ(cid:63)
Hence  if U T xt  U T xt(cid:48)

i − µ(cid:63)

(cid:107)U U T µ(cid:63)

i (cid:107)2 ≤ .01σ2  ∀1 ≤ i ≤ k.

i . For any x sampled from mixture distribution (1)  U T x ∼(cid:80)
2 ≤ (k + 8α(cid:112)k log k)σ2 

both belong to cluster i  then (w.p. ≥ 1 − 1/kα):

(cid:107)U T xt(cid:48) − U T xt(cid:48)(cid:107)2 = (cid:107)U T (zt − zt(cid:48)

)(cid:107)2

i wiN ((cid:98)µ

(3)
∗
i   σ2I).

(4)

where xt = µ(cid:63)
random variable tail bound. Similarly if U T xt  U T xt(cid:48)
and xt(cid:48)

then (w.p. ≥ 1 − 1/kα):

i + zt and xt(cid:48)
j + zt(cid:48)

i + zt(cid:48)

= µ(cid:63)

= µ(cid:63)

(cid:107)U T xt(cid:48) − U T xt(cid:48)(cid:107)2 = (cid:107)(cid:98)µ

i −(cid:98)µ

∗

. The last inequality above follows by using standard χ2
i + zt

belong to cluster i and j  i.e.  xt = µ(cid:63)

∗
j(cid:107)2 + (cid:107)U T (zt − zt(cid:48)

≥ (C 2 − .2C + 8α(cid:112)k log k − 16αC(cid:112)log k)σ2 

∗
j )T U T (zt − zt(cid:48)

)(cid:107)2

∗

)

2 + 2((cid:98)µ

i −(cid:98)µ

(5)

where the above equation follows by using (3)  setting α = C/32 and using C = Ω((k log k)1/4).

7

Using (4)  (5)  w.h.p. all the points from the same cluster are closer to each other than points from
other clusters. Hence  connected components of nearest neighbor graph recover clusters accurately.
t∈Cluster(i) U T xt for each i. Since  our clustering is com-

Now  we estimate (cid:98)µi =

|Cluster(i)|

1

pletely accurate  we have w.p. ≥ 1 − 2m2/kC/32 
∗
i (cid:107)2 ≤ σ

√

(cid:112)|Cluster(i)| .

log k

(6)

(cid:80)
(cid:107)(cid:98)µi −(cid:98)µ

k − C(cid:112) m

As wi = 1/k for all i  |Cluster(i)| ≥ m
setting m = O(k log k) and by using (3)  (6) along with C = Ω((k log k)1/4).
Remark 1. We would like to emphasize that our analysis for the convergence of streaming algo-
rithms works even for smaller separations C = O(
log k)  as long as we can get a good enough
initialization. Hence  a better initialization algorithm with weaker dependence of C on k would lead
to an improvement in the overall algorithm.

k w.p. ≥ 1 − 1/kC/32. Theorem now follows by

√

6 Soft thresholding EM based algorithm

In this section  we study a streaming version of the Expectation Maximization (EM) algorithm [7]
which is also used extensively in practice. While the standard k-means or Lloyd’s heuristic is known
to be agnostic to the distribution  and the same procedure can solve the mixture problem for a variety
of distributions [12]  EM algorithms are designed speciﬁcally for the input mixture distribution. In
this section  we consider a streaming version of the EM algorithm when applied to the problem of
mixture of two spherical Gaussians with known variances. In this case  the EM algorithm reduces to
a softer version of the Lloyd’s algorithm where a point can be partially assigned to the two clusters.
Recent results by [6  3  19] show convergence of the EM algorithm in the ofﬂine setting for this
2 = −µ(cid:63) and the center separation
simple setup. In keeping with earlier notation  let µ(cid:63)
C = 2(cid:107)µ(cid:63)(cid:107)

. Hence  xt i.i.d∼ 1

1 = µ(cid:63) and µ(cid:63)

2N (µ(cid:63)  σ2I) + 1

2N (−µ(cid:63)  σ2I).

σ

Algorithm 3 StreamSoftUpdate(N  N0)

Set η = 3 log N
N .
Set µ0
for t = 1 to N do

i ← InitAlgo(N0).

Receive xt+N0 as generated by the input stream.
x = xt+N0
Let wt =

(cid:1)+exp(cid:0) −(cid:107)x+µt(cid:107)2

exp(cid:0) −(cid:107)x−µt(cid:107)2

exp(cid:0) −(cid:107)x−µt(cid:107)2

(cid:1)

(cid:1)

σ2

σ2

σ2

Set µt+1 = (1 − η)µt + η[2wt − 1]x.

end for

In our algorithm  wt(x) is an estimate of the probability that x belongs to the cluster with µt  given
that it is drawn from a balanced mixture of gaussians at µt and −µt. Calculating wt(x) corresponds
to the E step and updating the estimate of the centers corresponds to the M step of the EM algorithm.
Similar to the streaming Lloyd’s algorithm presented in Section 3  our analysis of streaming soft
updates can be separated into streaming update analysis and analysis InitAlg (which is already
presented in Section 5). We now provide our main theorem  and the proof is presented in Appendix C.
Theorem 7 (Streaming Update). Let xt  1 ≤ t ≤ N + N0 be generated using a mixture two
balanced spherical Gaussians with variance σ2. Also  let the center-separation C ≥ 4  and also
suppose our initial estimate µ0 is such that (cid:107)µ0 − µ(cid:63)(cid:107) ≤ Cσ
20 . Then  the streaming update of
StreamSoftUpdate(N  N0)   i.e  Steps 3-8 of Algorithm 3 satisﬁes:

E(cid:2)(cid:107)µN − µ(cid:63)(cid:107)2(cid:3) ≤ (cid:107)µ(cid:63)(cid:107)2
(cid:124) (cid:123)(cid:122) (cid:125)

N Ω(1)

(cid:124)

log N

(cid:123)(cid:122)

N

dσ2

(cid:125)

.

+ O(1)

bias

variance

8

Remark 2. Our bias and variance terms are similar to the ones in Theorem 1 but the above bound
does not have the additional approximation error term. Hence  in this case we can estimate µ(cid:63)
consistently but the algorithm applies only to a mixture of Gaussians while our algorithm and result
in Section 3 can potentially be applied to arbitrary sub-Gaussian distributions.
Remark 3. We note that for our streaming soft update algorithm  it is not critical to know the
variance σ2 beforehand. One could get a good estimate of σ by taking the mean of a random
projection of a small number of points. We provide the details in the full version of this paper [14].

7 Conclusions
In this paper  we studied the problem of clustering with streaming data where each data point is
sampled from a mixture of spherical Gaussians. For this problem  we study two algorithms that use
appropriate initialization: a) a streaming version of Lloyd’s method  b) a streaming EM method. For
both the methods we show that we can accurately initialize the cluster centers using an online PCA
based method. We then show that assuming Ω((k log k)1/4σ) separation between the cluster centers 
the updates by both the methods lead to decrease in both the bias as well as the variance error terms.
For Lloyd’s method there is an additional estimation error term  which even the ofﬂine algorithm
incurs  and which is avoided by the EM method. However  the streaming Lloyd’s method is agnostic
to the data distribution and can in fact be applied to any mixture of sub-Gaussians problem. For future
work  it would be interesting to study the streaming data clustering problem under deterministic
assumptions like [12  16]. Also  it is an important question to understand the optimal separation
assumptions needed for even the ofﬂine gaussian mixture clustering problem.

References
[1] Anima Anandkumar  Rong Ge  Daniel J. Hsu  Sham M. Kakade  and Matus Telgarsky. Tensor
decompositions for learning latent variable models (A survey for ALT). In Proceedings of ALT 
pages 19–38  2015.

[2] Hassan Ashtiani  Shai Ben-David  and Abbas Mehrabian. Sample-efﬁcient learning of mixtures.

arXiv preprint arXiv:1706.01596  2017.

[3] Sivaraman Balakrishnan  Martin J Wainwright  and Bin Yu. St atistical guarantees for the em
algorithm: From population to sample-based analysis. Annals of Stats. 45 (1)  77-120  2014.

[4] Maria-Florina Balcan  Avrim Blum  and Anupam Gupta. Clustering under approximation

stability. J. ACM  60(2):8:1–8:34  2013.

[5] Anirban Dasgupta  John Hopcroft  Ravi Kannan  and Pradipta Mitra. Spectral clustering with

limited independence. In Proceedings of SODA  pages 1036–1045  2007.

[6] Constantinos Daskalakis  Christos Tzamos  and Manolis Zampetakis. Ten steps of em sufﬁce

for mixtures of two gaussians. arXiv preprint arXiv:1609.00368  2016.

[7] Arthur P Dempster  Nan M Laird  and Donald B Rubin. Maximum likelihood from incomplete

data via the em algorithm. Journal of the royal statistical society  pages 1–38  1977.

[8] R. O. Duda  P. E. Hart  and D. G. Stork. Pattern Classiﬁcation. John Wiley and Sons  2000.

[9] Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In

Proceedings of NIPS  pages 2861–2869  2014.

[10] Daniel J. Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: moment methods

and spectral decompositions. In Proceedings of ITCS ’13  pages 11–20  2013.

[11] Prateek Jain  Chi Jin  Sham M. Kakade  Praneeth Netrapalli  and Aaron Sidford. Streaming
PCA: matching matrix bernstein and near-optimal ﬁnite sample guarantees for oja’s algorithm.
In Proceedings of COLT  pages 1147–1164  2016.

[12] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm.

In Proceedings of FOCS  pages 299–308  2010.

9

[13] Ioannis Mitliagkas  Constantine Caramanis  and Prateek Jain. Memory limited  streaming PCA.

In Proceedings of 27th NIPS  pages 2886–2894  2013.

[14] Aditi Raghunathan  Ravishankar Krishnaswamy  and Prateek Jain. Learning mixture of gaus-

sians with streaming data. CoRR  abs/1707.02391  2017.

[15] Ohad Shamir. A variant of azuma’s inequality for martingales with subgaussian tails. arXiv

preprint arXiv:1110.2392  2011.

[16] Cheng Tang and Claire Monteleoni. On lloyd’s algorithm: New theoretical insights for clustering

in practice. In Proceedings of AISTATS  pages 1280–1289  2016.

[17] Cheng Tang and Claire Monteleoni. Convergence rate of stochastic k-means. Proceedings of

AISTATS  2017.

[18] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J. Comput.

Syst. Sci.  68(4):841–860  2004.

[19] Ji Xu  Daniel J Hsu  and Arian Maleki. Global analysis of expectation maximization for
mixtures of two gaussians. In Advances in Neural Information Processing Systems  pages
2676–2684  2016.

10

,Aditi Raghunathan
Prateek Jain
Ravishankar Krishnawamy
Zhao Song
David Woodruff
Peilin Zhong