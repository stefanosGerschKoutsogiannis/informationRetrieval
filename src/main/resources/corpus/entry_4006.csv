2015,On the consistency theory of high dimensional variable screening,Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method  it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension $p$ is substantially larger than the sample size $n$  variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold.This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular  we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples  we show two screening methods $SIS$ and $HOLP$ are both strong screening consistent (subject to additional constraints) with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under random designs. In addition  we relate the RDD condition to the irrepresentable condition  and highlight limitations of $SIS$.,On the consistency theory of high dimensional

variable screening

Xiangyu Wang

Dept. of Statistical Science

Duke University  USA

xw56@stat.duke.edu

Chenlei Leng

Dept. of Statistics

University of Warwick  UK
C.Leng@warwick.ac.uk

David B. Dunson

Dept. of Statistical Science

Duke University  USA

dunson@stat.duke.edu

Abstract

Variable screening is a fast dimension reduction technique for assisting high di-
mensional feature selection. As a preselection method  it selects a moderate size
subset of candidate variables for further reﬁning via feature selection to produce
the ﬁnal model. The performance of variable screening depends on both compu-
tational efﬁciency and the ability to dramatically reduce the number of variables
without discarding the important ones. When the data dimension p is substantially
larger than the sample size n  variable screening becomes crucial as 1) Faster fea-
ture selection algorithms are needed; 2) Conditions guaranteeing selection consis-
tency might fail to hold. This article studies a class of linear screening methods
and establishes consistency theory for this special class. In particular  we prove
the restricted diagonally dominant (RDD) condition is a necessary and sufﬁcient
condition for strong screening consistency. As concrete examples  we show two
screening methods SIS and HOLP are both strong screening consistent (subject
to additional constraints) with large probability if n > O((ρs+σ/τ )2 log p) under
random designs. In addition  we relate the RDD condition to the irrepresentable
condition  and highlight limitations of SIS.

1

Introduction

The rapidly growing data dimension has brought new challenges to statistical variable selection  a
crucial technique for identifying important variables to facilitate interpretation and improve predic-
tion accuracy. Recent decades have witnessed an explosion of research in variable selection and
related ﬁelds such as compressed sensing [1  2]  with a core focus on regularized methods [3–7].
Regularized methods can consistently recover the support of coefﬁcients  i.e.  the non-zero signals 
via optimizing regularized loss functions under certain conditions [8–10]. However  in the big data
era when p far exceeds n  such regularized methods might fail due to two reasons. First  the con-
ditions that guarantee variable selection consistency for convex regularized methods such as lasso
might fail to hold when p >> n; Second  the computational expense of both convex and non-convex
regularized methods increases dramatically with large p.
Bearing these concerns in mind  [11] propose the concept of “variable screening”  a fast technique
that reduces data dimensionality from p to a size comparable to n  with all predictors having non-
zero coefﬁcients preserved. They propose a marginal correlation based fast screening technique
“Sure Independence Screening” (SIS) that can preserve signals with large probability. However 
this method relies on a strong assumption that the marginal correlations between the response and
the important predictors are high [11]  which is easily violated in the practice. [12] extends the
marginal correlation to the Spearman’s rank correlation  which is shown to gain certain robustness
but is still limited by the same strong assumption. [13] and [14] take a different approach to attack
the screening problem. They both adopt variants of a forward selection type algorithm that includes
one variable at a time for constructing a candidate variable set for further reﬁning. These methods

1

eliminate the strong marginal assumption in [11] and have been shown to achieve better empirical
performance. However  such improvement is limited by the extra computational burden caused
by their iterative framework  which is reported to be high when p is large [15]. To ameliorate
concerns in both screening performance and computational efﬁciency  [15] develop a new type of
screening method termed “High-dimensional ordinary least-square projection” (HOLP ). This new
screener relaxes the strong marginal assumption required by SIS and can be computed efﬁciently
(complexity is O(n2p))  thus scalable to ultra-high dimensionality.
This article focuses on linear models for tractability. As computation is one vital concern for design-
ing a good screening method  we primarily focus on a class of linear screeners that can be efﬁciently
computed  and study their theoretical properties. The main contributions of this article lie in three
aspects.

1. We deﬁne the notion of strong screening consistency to provide a uniﬁed framework for
analyzing screening methods.
In particular  we show a necessary and sufﬁcient condi-
tion for a screening method to be strong screening consistent is that the screening matrix
is restricted diagonally dominant (RDD). This condition gives insights into the design of
screening matrices  while providing a framework to assess the effectiveness of screening
methods.

2. We relate RDD to other existing conditions. The irrepresentable condition (IC) [8] is nec-
essary and sufﬁcient for sign consistency of lasso [3]. In contrast to IC that is speciﬁc to the
design matrix  RDD involves another ancillary matrix that can be chosen arbitrarily. Such
ﬂexibility allows RDD to hold even when IC fails if the ancillary matrix is carefully chosen
(as in HOLP ). When the ancillary matrix is chosen as the design matrix  certain equiva-
lence is shown between RDD and IC  revealing the difﬁculty for SIS to achieve screening
consistency. We also comment on the relationship between RDD and the restricted eigen-
value condition (REC) [6] which is commonly seen in the high dimensional literature. We
illustrate via a simple example that RDD might not be necessarily stronger than REC.

size of n = O(cid:0)(ρs + σ/τ )2 log p(cid:1) is sufﬁcient for SIS and HOLP to be screening con-

3. We study the behavior of SIS and HOLP under random designs  and prove that a sample

sistent  where s is the sparsity  ρ measures the diversity of signals and τ /σ evaluates the
signal-to-noise ratio. This is to be compared to the sign consistency results in [9] where the
design matrix is ﬁxed and assumed to follow the IC.

The article is organized as follows. In Section 1  we set up the basic problem and describe the
framework of variable screening. In Section 2  we provide a deterministic necessary and sufﬁcient
condition for consistent screening. Its relationship with the irrepresentable condition is discussed
in Section 3. In Section 4  we prove the consistency of SIS and HOLP under random designs by
showing the RDD condition is satisﬁed with large probability  although the requirement on SIS is
much more restictive.

2 Linear screening

Consider the usual linear regression

Y = Xβ +  

where Y is the n× 1 response vector  X is the n× p design matrix and  is the noise. The regression
task is to learn the coefﬁcient vector β. In the high dimensional setting where p >> n  a sparsity
assumption is often imposed on β so that only a small portion of the coordinates are non-zero. Such
an assumption splits the task of learning β into two phases. The ﬁrst is to recover the support of
β  i.e.  the location of non-zero coefﬁcients; The second is to estimate the value of these non-zero
signals. This article mainly focuses on the ﬁrst phase.
As pointed out in the introduction  when the dimensionality is too high  using regularization methods
methods raises concerns both computationally and theoretically. To reduce the dimensionality  [11]
suggest a variable screening framework by ﬁnding a submodel

Md = {i : | ˆβi| is among the largest d coordinates of | ˆβ|} or Mγ = {i : | ˆβi| > γ}.

2

(1)

Let Q = {1  2 ···   p} and deﬁne S as the true model with s = |S| being its cardinarlity. The
hope is that the submodel size |Md| or |Mγ| will be smaller or comparable to n  while S ⊆ Md
or S ⊆ Mγ. To achieve this goal two steps are usually involved in the screening analysis. The
ﬁrst is to show there exists some γ such that mini∈S | ˆβi| > γ and the second step is to bound the
size of |Mγ| such that |Mγ| = O(n). To unify these steps for a more comprehensive theoretical
framework  we put forward a slightly stronger deﬁnition of screening consistency in this article.
Deﬁnition 2.1. (Strong screening consistency) An estimator ˆβ (of β) is strong screening consistent
if it satisﬁes that

and

min
i∈S

| ˆβi| > max
i(cid:54)∈S

| ˆβi|

∀i ∈ S.

sign( ˆβi) = sign(βi) 

(2)
Remark 2.1. This deﬁnition does not differ much from the usual screening property studied in the
literature  which requires mini∈S | ˆβi| > max(n−s)
| ˆβi|  where max(k) denotes the kth largest item.
i(cid:54)∈S
The key of strong screening consistency is the property (1) that requires the estimator to preserve
consistent ordering of the zero and non-zero coefﬁcients. It is weaker than variable selection consis-
tency in [8]. The requirement in (2) can be seen as a relaxation of the sign consistency deﬁned in [8] 
as no requirement for ˆβi  i (cid:54)∈ S is needed. As shown later  such relaxation tremendously reduces the
restriction on the design matrix  and allows screening methods to work for a broader choice of X.
The focus of this article is to study the theoretical properties of a special class of screeners that take
the linear form as

ˆβ = AY

for some p×n ancillary matrix A. Examples include sure independence screening (SIS) where A =
X T /n and high-dimensional ordinary least-square projection (HOLP ) where A = X T (XX T )−1.
We choose to study the class of linear estimators because linear screening is computationally efﬁ-
cient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a
special case of linear estimators although it is not well deﬁned for p > n.

3 Deterministic guarantees

In this section  we derive the necessary and sufﬁcient condition that guarantees ˆβ = AY to be strong
screening consistent. The design matrix X and the error  are treated as ﬁxed in this section and we
will investigate random designs later. We consider the set of sparse coefﬁcient vectors deﬁned by

(cid:26)

(cid:26)(cid:88)

(cid:27)

.

maxi∈supp(β) |βi|
mini∈supp(β) |βi| ≤ ρ

B(s  ρ) =

β ∈ Rp : |supp(β)| ≤ s 

The set B(s  ρ) contains vectors having at most s non-zero coordinates with the ratio of the largest
and smallest coordinate bounded by ρ. Before proceeding to the main result of this section  we
introduce some terminology that helps to establish the theory.
Deﬁnition 3.1. (restricted diagonally dominant matrix) A p × p symmetric matrix Φ is restricted
diagonally dominant with sparsity s if for any I ⊆ Q  |I| ≤ s − 1 and i ∈ Q \ I

Φii > C0 max

|Φij + Φkj| 

|Φij − Φkj|

+ |Φik| ∀k (cid:54)= i  k ∈ Q \ I 

(cid:88)

(cid:27)

j∈I
where C0 ≥ 1 is a constant.
Notice this deﬁnition implies that for i ∈ Q \ I
|Φij + Φkj| +

(cid:18)(cid:88)

Φii ≥ C0

j∈I

j∈I

(cid:88)

j∈I

(cid:19)

/2 ≥ C0

(cid:88)

j∈I

|Φij| 

|Φij − Φkj|

(3)

which is related to the usual diagonally dominant matrix. The restricted diagonally dominant ma-
trix provides a necessary and sufﬁcient condition for any linear estimators ˆβ = AY to be strong
screening consistent. More precisely  we have the following result.

3

Theorem 1. For the noiseless case where  = 0  a linear estimator ˆβ = AY is strong screening
consistent for every β ∈ B(s  ρ)  if and only if the screening matrix Φ = AX is restricted diagonally
dominant with sparsity s and C0 ≥ ρ.
Proof. Assume Φ is restricted diagonally dominant with sparsity s and C0 ≥ ρ. Recall ˆβ = Φβ.
Suppose S is the index set of non-zero predictors. For any i ∈ S  k (cid:54)∈ S  if we let I = S \ {i}  then
we have
| ˆβi| = |βi|

(Φij + Φkj) + Φki −(cid:88)

Φkj − Φki

(cid:88)

= |βi|

(cid:26)

(cid:27)

(cid:19)

(cid:18)

Φii +

Φij

> −|βi|

Φkj + Φki

βj
βi

j∈I
βj
βi

(cid:88)

j∈I

Φii +

(cid:18)(cid:88)

j∈I

Φii +

(cid:18)
(cid:18)(cid:88)

j∈I

βj
βi

(cid:19)

(cid:19)
(cid:19)

and
| ˆβi| = |βi|

> |βi|

βj
βi

Φij

Φkj + Φki

= |βi|

Φii +

j∈I
= sign(βi) · ˆβk.

βj
βi

j∈I

(cid:88)
(cid:18)(cid:88)
(cid:88)

j∈I

βi

= −|βi|
(cid:26)

(cid:19)

βj
βi

j∈I

βjΦkj + βiΦki

= −sign(βi) · ˆβk 

(Φij − Φkj) − Φki +

βj
βi

(cid:88)

j∈I

βj
βi

Φkj + Φki

(cid:27)

Therefore  whatever value sign(βi) is  it always holds that | ˆβi| > | ˆβk| and thus mini∈S | ˆβi| >
maxk(cid:54)∈S | ˆβk|.
To prove the sign consistency for non-zero coefﬁcients  we notice that for i ∈ S 

(cid:88)

j∈I

(cid:18)

(cid:19)

(cid:88)

j∈I

βj
βi

ˆβiβi = Φiiβ2

i +

Φijβjβi = β2
i

Φii +

Φij

> 0.

The proof of necessity is left to the supplementary materials.

The noiseless case is a good starting point to analyze ˆβ. Intuitively  in order to preserve the correct
order of the coefﬁcients in ˆβ = AXβ  one needs AX to be close to a diagonally dominant matrix 
so that ˆβi  i ∈ MS will take advantage of the large diagonal terms in AX to dominate ˆβi  i (cid:54)∈ MS
that is just linear combinations of off-diagonal terms.
When noise is considered  the condition in Theorem 1 needs to be changed slightly to accommodate
extra discrepancies. In addition  the smallest non-zero coefﬁcient has to be lower bounded to ensure
a certain level of signal-to-noise ratio. Thus  we augment our previous deﬁnition of B(s  ρ) to have
a signal strength control

Bτ (s  ρ) = {β ∈ B(s  ρ)| min

|βi| ≥ τ}.

i∈supp(β)

Then we can obtain the following modiﬁed Theorem.
Theorem 2. With noise  the linear estimator ˆβ = AY is strong screening consistent for every
β ∈ Bτ (s  ρ) if Φ = AX − 2τ−1(cid:107)A(cid:107)∞Ip is restricted diagonally dominant with sparsity s and
C0 ≥ ρ.
The proof of Theorem 2 is essentially the same as Theorem 1 and is thus left to the supplementary
materials. The condition in Theorem 2 can be further tailored to a necessary and sufﬁcient version
with extra manipulation on the noise term. Nevertheless  this might not be useful in practice due to
the randomness in noise. In addition  the current version of Theorem 2 is already tight in the sense
that there exists some noise vector  such that the condition in Theorem 2 is also necessary for strong
screening consistency.
Theorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide
practical guidance for screening design. In Section 4  we consider some concrete examples of an-
cillary matrix A and prove that conditions in Theorems 1 and 2 are satisﬁed by the corresponding
screeners with large probability under random designs.

4

4 Relationship with other conditions

For some special cases such sure independence screening (”SIS”)  the restricted diagonally dominant
(RDD) condition is related to the strong irrepresentable condition (IC) proposed in [8]. Assume each
column of X is standardized to have mean zero. Letting C = X T X/n and β be a given coefﬁcient
vector  the IC is expressed as

(cid:107)CSc SC−1

S S · sign(βS)(cid:107)∞ ≤ 1 − θ

(4)
for some θ > 0  where CA B represents the sub-matrix of C with row indices in A and column
indices in B. The authors enumerate several scenarios of C such that IC is satisﬁed. We verify some
of these scenarios for screening matrix Φ.
Corollary 1. If Φii = 1  ∀i and |Φij| < c/(2s)  ∀i (cid:54)= j for some 0 ≤ c < 1 as deﬁned in Corollary
1 and 2 in [8]  then Φ is a restricted diagonally dominant matrix with sparsity s and C0 ≥ 1/c.
If |Φij| < r|i−j|  ∀i  j for some 0 < r < 1 as deﬁned in Corollary 3 in [8]  then Φ is a restricted
diagonally dominant matrix with sparsity s and C0 ≥ (1 − r)2/(4r).
A more explicit but nontrivial relationship between IC and RDD is illustrated below when |S| = 2.
Theorem 3. Assume Φii = 1  ∀i and |Φij| < r  ∀i (cid:54)= j. If Φ is restricted diagonally dominant with
sparsity 2 and C0 ≥ ρ  then Φ satisﬁes

(cid:107)ΦSc SΦ−1

S S · sign(βS)(cid:107)∞ ≤ ρ−1
1 − r

for all β ∈ B(2  ρ). On the other hand  if Φ satisﬁes the IC for all β ∈ B(2  ρ) for some θ  then Φ is
a restricted diagonally dominant matrix with sparsity 2 and
1 − r
1 + r

C0 ≥ 1
1 − θ

.

Theorem 3 demonstrates certain equivalence between IC and RDD. However  it does not mean
that RDD is also a strong requirement. Notice that IC is directly imposed on the covariance matrix
X T X/n. This makes IC a strong assumption that is easily violated; for example  when the predictors
are highly correlated. In contrast to IC  RDD is imposed on matrix AX where there is ﬂexibility in
choosing A. Only when A is chose to be X/n  RDD is equivalently strong as IC  as shown in next
theorem. For other choices of A  such as HOLP deﬁned in next section  the estimator satisﬁes RDD
even when predictors are highly correlated. Therefore  RDD is considered as weak requirement.
For ”SIS”  the screening matrix Φ = X T X/n coincides with the covariance matrix  making RDD
and IC effectively equivalent. The following theorem formalizes this.
Theorem 4. Let A = X T /n and standardize columns of X to have sample variance one. Assume
X satisﬁes the sparse Riesz condition [16]  i.e 

min

π⊆Q  |π|≤s

λmin(X T

π Xπ/n) ≥ µ 

√

√

s/µ  then X satisﬁes the IC for any β ∈ B(s  ρ).

s/µ  the strong screening consistency of SIS for B(s +

for some µ > 0. Now if AX is restricted diagonally dominant with sparsity s + 1 and C0 ≥ ρ with
ρ >
In other words  under the condition ρ >
1  ρ) implies the model selection consistency of lasso for B(s  ρ).
Theorem 4 illustrates the difﬁculty of SIS. The necessary condition that guarantees good screening
performance of SIS also guarantees the model selection consistency of lasso. However  such a
strong necessary condition does not mean that SIS should be avoided in practice given its substantial
advantages in terms of simplicity and computational efﬁciency. The strong screening consistency
deﬁned in this article is stronger than conditions commonly used in justifying screening procedures
as in [11].
Another common assumption in the high dimensional literature is the restricted eigenvalue condition
(REC). Compared to REC  RDD is not necessarily stronger due to its ﬂexibility in choosing the
ancillary matrix A. [17  18] prove that the REC is satisﬁed when the design matrix is sub-Gaussian.
However  REC might not be guaranteed when the row of X follows heavy-tailed distribution. In
contrast  as the example shown in next section and in [15]  by choosing A = X T (XX T )−1  the
resulting estimator satisﬁes RDD even when the rows of X follow heavy-tailed distributions.

5

5 Screening under random designs

In this section  we consider linear screening under random designs when X and  are Gaussian.
The theory developed in this section can be easily extended to a broader family of distributions  for
example  where  follows a sub-Gaussian distribution [19] and X follows an elliptical distribution
[11  15]. We focus on the Gaussian case for conciseness. Let  ∼ N (0  σ2) and X ∼ N (0  Σ).
We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2.
Recall the ancillary matrices for SIS and HOLP are deﬁned respectively as

ASIS = X/n 

AHOLP = X T (XX T )−1.

For simplicity  we assume Σii = 1 for i = 1  2 ···   p. To verify the RDD condition  it is essential
to quantify the magnitude of the entries of AX and A.
Lemma 1. Let Φ = ASISX  then for any t > 0 and i (cid:54)= j ∈ Q  we have
tn
2eK

|Φii − Σii| ≥ t

≤ 2 exp

− min

8e2K

P

 

(cid:19)
(cid:19)

(cid:26)
(cid:26)

(cid:18) t2n
(cid:18) t2n

(cid:19)(cid:27)
(cid:19)(cid:27)

 

 

and

P

|Φij − Σij| ≥ t

≤ 6 exp

− min

 

tn
6eK

72e2K

(cid:18)
(cid:18)

where K = (cid:107)X 2(1) − 1(cid:107)ψ1 is a constant  X 2(1) is a chi-square random variable with one degree
of freedom and the norm (cid:107) · (cid:107)ψ1 is deﬁned in [19].
Lemma 1 states that the screening matrix Φ = ASISX for SIS will eventually converge to the co-
variance matrix Σ in l∞ when n tends to inﬁnity and log p = o(n). Thus  the screening performance
of SIS strongly relies on the structure of Σ. In particular  the (asymptotically) necessary and sufﬁ-
cient condition for SIS being strong screening consistent is Σ satisfying the RDD condition. For
the noise term  we have the following lemma.
Lemma 2. Let η = ASIS. For any t > 0 and i ∈ Q  we have

(cid:26)

(cid:18) t2n

− min

 

tn
6eK

72e2K

(cid:19)(cid:27)

 

P (|ηi| ≥ σt) ≤ 6 exp

where K is deﬁned the same as in Lemma 1.

The proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and
is thus omitted. As indicated before  the necessary and sufﬁcient condition for SIS to be strong
screening consistent is that Σ follows RDD. As RDD is usually hard to verify  we consider a stronger
sufﬁcient condition inspired by Corollary 1.
Theorem 5. Let r = maxi(cid:54)=j |Σij|. If r < 1

2ρs   then for any δ > 0  if the sample size satisﬁes

(cid:18) 1 + 2ρs + 2σ/τ

(cid:19)2

n > 144K

(5)
least 1 − δ  Φ = ASISX −
where K is deﬁned in Lemma 1 
2τ−1(cid:107)ASIS(cid:107)∞Ip is restricted diagonally dominant with sparsity s and C0 ≥ ρ. In other words 
SIS is screening consistent for any β ∈ Bτ (s  ρ).

then with probability at

1 − 2ρsr

log(3p/δ) 

(cid:18)

Proof. Taking union bound on the results from Lemma 1 and 2  we have for any t > 0 and p > 2 

min
i∈Q

Φii ≤ 1 − t or max
i(cid:54)=j

t
P
6e
In other words  for any δ > 0  when n ≥ K log(7p2/δ)  with probability at least 1 − δ  we have

|Φij| ≥ r + t or (cid:107)η(cid:107)∞ ≥ σt

≤ 7p2 exp

− n
K

72e2  

min

(cid:18) t2

(cid:19)(cid:27)

.

(cid:19)

(cid:26)

(cid:114)

(cid:114)

Φii ≥ 1 − 6

min
i∈Q

√

2e

K log(7p2/δ)

n

  max
i(cid:54)=j

|Φij| ≤ r + 6

√

2e

6

K log(7p2/δ)

n

 

(cid:114)

√
|ηi| ≤ 6

2eσ

max
i∈Q

K log(7p2/δ)

n

.

A sufﬁcient condition for Φ to be restricted diagonally dominant is that
|ηi|.

|Φij| + 2τ−1 max

min

Φii > 2ρs max
i(cid:54)=j

i

i
Plugging in the values we have
√
1 − 6

K log(7p2/δ)

(cid:114)

2e

(cid:114)

√

> 2ρs(r + 6

2e

K log(7p2/δ)

) + 12

2eτ−1σ

n

n

n
Solving the above inequality (notice that 7p2/δ < 9p2/δ2 and ρ > 1) completes the proof.
The requirement that maxi(cid:54)=j |Σij| < 1/(ρsr) or the necessary and sufﬁcient condition that Σ is
RDD strictly constrains the correlation structure of X  causing the difﬁculty for SIS to be strong
screening consistent. For HOLP we instead have the following result.
Lemma 3. Let Φ = AHOLP X. Assume p > c0n for some c0 > 1  then for any C > 0 there exists
some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i ∈ Q  j (cid:54)= i  we have

(cid:19)

(cid:18)

(cid:18)

|Φii| < c1κ−1 n
p

≤ 2e−Cn  P

|Φii| > c2κ

n
p

≤ 2e−Cn

(cid:114)

K log(7p2/δ)

.

√

(cid:19)

P

and

P

where c4 =

√
c2(c0−c1)
√
c3(c0−1)

.

(cid:18)

(cid:19)

√

n
p

|Φij| > c4κt

≤ 5e−Cn + 2e−t2/2 

Proof. The proof of Lemma 3 relies heavily on previous results for the Stiefel Manifold provided in
the supplementary materials. We only sketch the basic idea here and leave the complete proof to the
supplementary materials. Deﬁning H = X T (XX T )−1/2  then we have Φ = HH T and H follows
the Matrix Angular Central Gaussian (MACG) with covariance Σ. The diagonal terms of HH T
can be bounded similarly via the Johnson-Lindenstrauss lemma  by using the fact that HH T =
Σ1/2U (U T ΣU )−1U Σ  where U is a p × n random projection matrix. Now for off-diagonal terms 
we decompose the Stiefel manifold as H = (G(H2)H1 H2)  where H1 is a (p − n + 1) × 1 vector 
H2 is a p × (n − 1) matrix and G(H2) is chosen so that (G(H2) H2) ∈ O(p)  and show that H1
follows Angular Central Gaussian (ACG) distribution with covariance G(H2)T ΣG(H2) conditional
on H2. It can be shown that e2HH T e1
1 HH T e1  then
1 G(H2)H1 = t1  and we obtain the desired coupling distribution as
1 H2 = 0 is equivalent to eT
eT
2 G(H2)H1|eT
1 G(H2)H1 = t1. Using the normal representation of ACG(Σ)  i.e. 
2 HH T e1
eT
if x = (x1 ···   xp) ∼ N (0  Σ)  then x/(cid:107)x(cid:107) ∼ ACG(Σ)  we can write G(H2)H1 in terms of
normal variables and then bound all terms using concentration inequalities.

= e2G(H2)H1|eT

1 H2 = 0. Let t2

1 = eT

(d)
= eT

(d)

n

Lemma 3 quantiﬁes the entries of the screening matrix for HOLP . As illustrated in the lemma 
p ) and the off-diagonal terms are
regardless of the covariance Σ  diagonal terms of Φ are always O( n
√
p ). Thus  with n ≥ O(s2)  Φ is likely to satisfy the RDD condition with large probability. For

O(
the noise vector we have the following result.
Lemma 4. Let η = AHOLP . Assume p > c0n for some c0 > 1  then for any C > 0 there exist the
same c1  c2  c3 as in Lemma 3 such that for any t > 0 and i ∈ Q 

(cid:18)

|ηi| ≥ 2σ

P

√
c2κt
1 − c−1

0

√

n
p

< 4e−Cn + 2e−t2/2 

if n ≥ 8C/(c0 − 1)2.
The proof is almost identical to Lemma 2 and is provided in the supplementary materials. The
following theorem results after combining Lemma 3 and 4.

(cid:19)

7

(cid:26)

Theorem 6. Assume p > c0n for some c0 > 1. For any δ > 0  if the sample size satisﬁes

2C(cid:48)κ4(ρs + σ/τ )2 log(3p/δ) 

n > max

(6)
where C(cid:48) = max{ 4c2
0 )2} and c1  c2  c3  c4  C are the same constants deﬁned in Lemma 3 
then with probability at least 1 − δ  Φ = AHOLP X − 2τ−1(cid:107)AHOLP (cid:107)∞Ip is restricted diagonally
dominant with sparsity s and C0 ≥ ρ. This implies HOLP is screening consistent for any β ∈
Bτ (s  ρ).

(c0 − 1)2

4c2
1(1−c
c2

4
c2
1

−1

8C

 

 

(cid:27)

Proof. Notice that if

i

min

|Φii| > 2sρ max

|Φij| + 2τ−1(cid:107)X T (XX T )−1(cid:107)∞ 

(7)
then the proof is complete because Φ − 2τ−1(cid:107)X T (XX T )−1(cid:107)∞ is already a restricted diagonally
dominant matrix. Let t =
c1κ−1 n
p

Cn/ν. The above equation then requires
− 2σ

=(cid:0)c1κ−1 − 2c4

Cκsρ
ν

Cκsρ
ν

− 2σ

(cid:1) n

− 2c4

(1 − c−1

(1 − c−1

> 0 

√

√

√

√

√

n
p

p

ij

c2Cκ
0 )τ ν

which implies that

n
p

√

c2Cκt
0 )τ ν
√

2c4

ν >
√

Cκ2ρs
c1

√

+

= C1κ2ρs + C2κ2τ−1σ > 1 

c2Cκ2
2σ
c1(1 − c−1
0 )τ
. Therefore  taking union bounds on all matrix entries  we

where C1 = 2c4
c1
have

C

  C2 = 2

(cid:18)(cid:8)(7) does not hold(cid:9)(cid:19)

P

c2C
−1
c1(1−c
0 )

which is satisﬁed provided (noticing
the precise condition we need.

C log 3p

δ . Now pushing ν to the limit gives (6) 

There are several interesting observations on equation (5) and (6). First  (ρs + σ/τ )2 appears in
both expressions. We note that ρs evaluates the sparsity and the diversity of the signal β while σ/τ
is closely related to the signal-to-noise ratio. Furthermore  HOLP relaxes the correlation constraint
r < 1/(2ρs) or the covariance constraint (Σ is RDD) with the conditional number constraint. Thus
for any Σ  as long as the sample size is large enough  strong screening consistency is assured.
Finally  HOLP provides an example to satisfy the RDD condition in answer to the question raised
in Section 4.

6 Concluding remarks

This article studies and establishes a necessary and sufﬁcient condition in the form of restricted
diagonally dominant screening matrices for strong screening consistency of a linear screener. We
verify the condition for both SIS and HOLP under random designs.
In addition  we show a
close relationship between RDD and the IC  highlighting the difﬁculty of using SIS in screening for
arbitrarily correlated predictors. For future work  it is of interest to see how linear screening can be
adapted to compressed sensing [20] and how techniques such as preconditioning [21] can improve
the performance of marginal screening and variable selection.

Acknowledgments This research was partly support by grant NIH R01-ES017436 from the Na-
tional Institute of Environmental Health Sciences.

8

< (p + 5p2)e−Cn + 2p2e−Cn/ν < (7 +

)p2e−Cn/ν2

 

1
n

where the second inequality is due to the fact that p > n and ν > 1. Now for any δ > 0  (7) holds
with probability at least 1 − δ if

(cid:18)

n ≥ ν2
C

log(7 + 1/n) + 2 log p − log δ
√

8 < 3) n ≥ 2ν2

(cid:19)

 

References
[1] David L Donoho.

52(4):1289–1306  2006.

Compressed sensing.

IEEE Transactions on Information Theory 

[2] Richard Baraniuk. Compressive sensing. IEEE Signal Processing Magazine  24(4)  2007.
[3] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Statistical Methodology)  58(1):267–288  1996.

[4] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American Statistical Association  96(456):1348–1360  2001.
[5] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is

much larger than n. The Annals of Statistics  35(6):2313–2351  2007.

[6] Peter J Bickel  Ya’acov Ritov  and Alexandre B Tsybakov. Simultaneous analysis of lasso and

dantzig selector. The Annals of Statistics  37(4):1705–1732  2009.

[7] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The

Annals of Statistics  38(2):894–942  2010.

[8] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine

Learning Research  7:2541–2563  2006.

[9] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity
using l1-constrained quadratic programming. IEEE Transactions on Information Theory  2009.
[10] Jason D Lee  Yuekai Sun  and Jonathan E Taylor. On model selection consistency of m-
estimators with geometrically decomposable penalties. Advances in Neural Processing Infor-
mation Systems  2013.

[11] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature
space. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  70(5):849–
911  2008.

[12] Gaorong Li  Heng Peng  Jun Zhang  Lixing Zhu  et al. Robust rank correlation based screening.

The Annals of Statistics  40(3):1846–1877  2012.

[13] Hansheng Wang. Forward regression for ultra-high dimensional variable screening. Journal

of the American Statistical Association  104(488):1512–1524  2009.

[14] Haeran Cho and Piotr Fryzlewicz. High dimensional variable selection via tilting. Journal of

the Royal Statistical Society: Series B (Statistical Methodology)  74(3):593–622  2012.

[15] Xiangyu Wang and Chenlei Leng. High-dimensional ordinary least-squares projection for

screening variables. https://stat.duke.edu/˜xw56/holp-paper.pdf  2015.

[16] Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in high-

dimensional linear regression. The Annals of Statistics  36(4):1567–1594  2008.

[17] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Restricted eigenvalue properties for
correlated gaussian designs. The Journal of Machine Learning Research  11:2241–2259  2010.
[18] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv

preprint arXiv:0912.4045  2009.

[19] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[20] Lingzhou Xue and Hui Zou. Sure independence screening and compressed random sensing.

Biometrika  98(2):371–380  2011.

[21] Jinzhu Jia and Karl Rohe. Preconditioning to comply with the irrepresentable condition. arXiv

preprint arXiv:1208.5584  2012.

9

,Xiangyu Wang
David Dunson
Huaian Diao
Zhao Song
David Woodruff
Xin Yang