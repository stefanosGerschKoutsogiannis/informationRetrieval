2017,Optimal Shrinkage of Singular Values Under Random Data Contamination,A low rank matrix X has been contaminated by uniformly distributed noise  missing values  outliers and corrupt entries. Reconstruction of X from the singular values and singular vectors of the  contaminated matrix Y is a key problem in machine learning  computer vision and data science.  In this paper we show that common contamination models   (including arbitrary combinations of uniform noise  missing values  outliers and corrupt entries) can be described efficiently using a single framework. We develop an asymptotically optimal algorithm that estimates X by manipulation of the singular values of Y  which applies to any of the contamination models considered.  Finally  we find an explicit signal-to-noise cutoff  below which estimation of X from the singular value decomposition of Y must fail  in a well-defined sense.,Optimal Shrinkage of Singular Values Under

Random Data Contamination

School of Computer Science and Engineering

School of Computer Science and Engineering

Danny Barash

Hebrew University
Jerusalem  Israel

Matan Gavish

Hebrew University
Jerusalem  Israel

danny.barash@mail.huji.ac.il

gavish@cs.huji.ac.il

Abstract

A low rank matrix X has been contaminated by uniformly distributed noise  missing
values  outliers and corrupt entries. Reconstruction of X from the singular values
and singular vectors of the contaminated matrix Y is a key problem in machine
learning  computer vision and data science. In this paper  we show that common
contamination models (including arbitrary combinations of uniform noise  missing
values  outliers and corrupt entries) can be described efﬁciently using a single
framework. We develop an asymptotically optimal algorithm that estimates X by
manipulation of the singular values of Y   which applies to any of the contamination
models considered. Finally  we ﬁnd an explicit signal-to-noise cutoff  below which
estimation of X from the singular value decomposition of Y must fail  in a well-
deﬁned sense.

1

Introduction

Reconstruction of low-rank matrices from noisy and otherwise contaminated data is a key problem in
machine learning  computer vision and data science. Well-studied problems such as dimension reduc-
tion [3]  collaborative ﬁltering [24  28]  topic models [13]  video processing [21]  face recognition
[35]  predicting preferences [26]  analytical chemistry [29] and background-foreground separation
[4] all reduce  under popular approaches  to low-rank matrix reconstruction. A signiﬁcant part of the
literature on these problems is based on the singular value decomposition (SVD) as the underlying
algorithmic component  see e.g. [7  19  23].
Understanding and improving the behavior of SVD in the presence of random data contamination
therefore arises as a crucially important problem in machine learning. While this is certainly a
classical problem [14  17  20]  it remains of signiﬁcant interest  owing in part to the emergence of
low-rank matrix models for matrix completion and collaborative ﬁltering [9  34].
Let X be an m-by-n unknown low-rank matrix of interest (m ≤ n)  and assume that we only observe
the data matrix Y   which is a contaminated or noisy version of X. Let

Y =

yiuiv(cid:48)

i

(1)

m(cid:88)

i=1

r(cid:88)

be the SVD of the data matrix Y . Any algorithm based on the SVD essentially aims to obtain an
estimate for the target matrix X from (1). Most practitioners simply form the Truncated SVD (TSVD)
estimate [18]

ˆXr =

yiuiv(cid:48)

i

(2)

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

i=1

where r is an estimate of rank(X)  whose choice in practice tends to be ad hoc [15].
Recently  [10  16  32] have shown that under white additive noise  it is useful to apply a carefully
designed shrinkage function η : R → R to the data singular values  and proposed estimators of the
form

ˆXη =

η(yi)uiv(cid:48)
i .

(3)

n(cid:88)

i=1

n(cid:88)

Such estimators are extremely simple to use  as they involve only simple manipulation of the data
singular values. Interestingly  in the additive white noise case  it was shown that a unique optimal
shrinkage function η(y) exists  which asymptotically delivers the same performance as the best
possible rotation-invariant estimator based on the data Y [16]. Singular value shrinkage thus emerged
as a simple yet highly effective method for improving the SVD in the presence of white additive
noise  with the unique optimal shrinker as a natural choice for the shrinkage function. A typical form
of optimal singular value shrinker is shown in Figure 1 below  left panel.
Shrinkage of singular values  an idea that can be traced back to Stein’s groundbreaking work on
covariance estimation from the 1970’s [33]  is a natural generalization of the classical TSVD. Indeed 
ˆXr is equivalent to shrinkage with the hard thresholding shrinker η(y) = 1y≥λ  as (2) is equivalent
to

ˆXλ =

1yi≥λuiv(cid:48)

i

(4)

i=1

with a speciﬁc choice of the so-called hard threshold λ. While the choice of the rank r for truncation
point TSVD is often ad hoc and based on gut feeling methods such as the Scree Plot method [11]  its
equivalent formulation  namely hard thresholding of singular values  allows formal and systematic
analysis. In fact  restricting attention to hard thresholds alone [15] has shown that under white
additive noise there exists a unique asymptotically optimal choice of hard threshold for singular
values. The optimal hard threshold is a systematic  rational choice for the number of singular values
that should be included in a truncated SVD of noisy data. [27] has proposed an algorithm that ﬁnds
η∗ in presence of additive noise and missing values  but has not derived an explicit shrinker.

1.1 Overview of main results

In this paper  we extend this analysis to common data contaminations that go well beyond additive
white noise  including an arbitrary combination of additive noise  multiplicative noise  missing-at-
random entries  uniformly distributed outliers and uniformly distributed corrupt entries.
The primary contribution of this paper is formal proof that there exists a unique asymptotically
optimal shrinker for singular values under uniformly random data contaminations  as well a unique
asymptotically optimal hard threshold. Our results are based on a novel  asymptotically precise
description of the effect of these data contaminations on the singular values and the singular vectors of
the data matrix  extending the technical contribution of [16  27  32] to the setting of general uniform
data contamination.

General contamination model. We introduce the model

Y = A (cid:12) X + B

(5)

where X is the target matrix to be recovered  and A  B are random matrices with i.i.d entries. Here 
(A (cid:12) B)i j = Ai jBi j is the Hadamard (entrywise) product of A and B.
iid∼ (µA  σ2
A)  meaning that the entries of A are i.i.d drawn from a distribution
Assume that Ai j
with mean µA and variance σ2
B). In Section 2 we show that for various
choices of the matrix A and B  this model represents a broad range of uniformly distributed random
contaminations  including an arbitrary combination of additive noise  multiplicative noise  missing-at-
random entries  uniformly distributed outliers and uniformly distributed corrupt entries. As a simple
example  if B ≡ 0 and P (Ai j = 1) = κ  then the Y simply has missing-at-random entries.

A  and that Bi j

iid∼ (0  σ2

2

To quantify what makes a “good” singular value shrinker η for use in (3)  we use the standard Mean
Square Error (MSE) metric and

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆXη(Y ) − X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

F

.

L(η|X) =

Using the methods of [16]  our results can easily be extended to other error metrics  such as the
nuclear norm or operator norm losses. Roughly speaking  an optimal shrinker η∗ has the property
that  asymptotically as the matrix size grows 

L(η∗|X) ≤ L(η|X)
for any other shrinker η and any low-rank target matrix X.
The design of optimal shrinkers requires a subtle understanding of the random ﬂuctuations of the data
singular values y1  . . .   yn  which are caused by the random contamination. Such results in random
matrix theory are generally hard to prove  as there are nontrivial correlations between yi and yj 
i (cid:54)= j. Fortunately  in most applications it is very reasonable to assume that the target matrix X is
low rank. This allows us to overcome this difﬁculty by following [15  27  32] and considering an
asymptotic model for low-rank X  inspired by Johnstone’s Spiked Covariance Model [22]  in which
the correlation between yi and yj  for i (cid:54)= j vanish asymptotically.
We state our main results informally at ﬁrst. The ﬁrst main result of this paper is the existence of a
unique asymptotically optimal hard threshold λ∗ in (4).
Importantly  as E(Y ) = µAX  to apply hard thresholding to Y = A (cid:12) X + B we must from now on
deﬁne

n(cid:88)

i=1

ˆXλ =

1
µA

1yi>λuiv

(cid:48)
i .

Theorem 1. (Informal.) Let X be an m-by-n low-rank matrix and assume that we observe the
contaminated data matrix Y given by the general contamination model (5). Then there exists a
unique optimal (def. 3) hard threshold λ∗ for the singular values of Y   given by

(cid:19)

β
c

c +
√

1
c

c +

∗

λ

= σB

(cid:115)(cid:18)

(cid:19)(cid:18)
(cid:113)
1 + β +(cid:112)1 + 14β + β2/


(cid:19)2 − β − 1

(cid:118)(cid:117)(cid:117)(cid:116)(cid:32)(cid:18) y

σ2
B
yµA

σB

0

(cid:33)2

∗

η

(y) =

where β = m/n and c =
Our second main result is the existence of a unique asymptotically optimal shrinkage function η∗ in
(equation (3)). We calculate this shrinker explicitly:
Theorem 2. (Informal.) Assume everything as in Theorem 1. Then there exists a unique optimal (def.
3) shrinker η∗ for the singular values of Y given by

2.

− 4β y ≥ σB(1 +(cid:112)β)
y < σB(1 +(cid:112)β)

We also discover that for each contamination model  there is a critical signal-to-noise cutoff  below
which X cannot be reconstructed from the singular values and vectors of Y . Speciﬁcally  let η0 be
the zero singular value shrinker  η0(y) ≡ 0  so that ˆXη0 (Y ) ≡ 0. Deﬁne the critical signal level for a
shrinker η by

xcritical(η) = inf
x

{x : L(η|X) < L(η0|X)}

where X = x˜u˜v(cid:48) is an arbitrary rank-1 matrix with singular value x. In other words  xcritical(η)
is the smallest singular value of the target matrix  for which η still outperforms the trivial zero
shrinker η0. As we show in Section 4  a target matrix X with a singular value below xcritical(η)
cannot be reliably reconstructed using η. The critical signal level for the optimal shrinker η∗ is
of special importance  since a target matrix X with a singular value below xcritical(η∗) cannot be
reliably reconstructed using any shrinker η. Restricting attention to hard thresholds only  we deﬁne
xcritical(λ)  the critical level for a hard threshold  similarly. Again  singular values of X that fall
below xcritical(λ∗) cannot be reliably reconstructed using any hard threshold.
Our third main result is the explicit calculation of these critical signal levels:

3

Theorem 3. (Informal.) Assume everything as in Theorem 1 and let c be as in Theorem 1. Let η∗ be
the optimal shrinker from Theorem 2 and let λ∗ be the optimal hard threshold from Theorem 1. The
critical signal levels for η∗ and λ∗ are given by:

xcritical(η∗) = (σB/µA) · β
xcritical(λ∗) = (σB/µA) · c .

1
4

Finally  one might ask what the improvement is in terms of the mean square error that is guaranteed
by using the optimal shrinker and optimal threshold. As discussed below  existing methods are either
infeasible in terms of running time on medium and large matrices  or lack a theory that can predict
the reconstruction mean square error. For lack of a better candidate  we compare the optimal shrinker
and optimal threshold to the default method  namely  TSVD.
Theorem 4. (Informal.) Consider β = 1  and denote the worst-case mean square error of TSVD  η∗
and λ∗ by MT SV D  Mη∗ and Mλ∗  respectively  over a target matrix of low rank r. Then

(cid:19)2
(cid:19)2
(cid:19)2

(cid:18) σB
(cid:18) σB
(cid:18) σB

µA

µA

µA

5r

2r

3r .

MT SV D =

Mη∗ =

Mλ∗ =

Indeed  the optimal shrinker offers a signiﬁcant performance improvement (speciﬁcally  an improve-
ment of 3r(σB/µA)2  over the TSVD baseline.

Figure 1: Left: Optimal shrinker for additive noise and missing-at-random contamination. Right:
Phase plane for critical signal levels  see Section 6  Simulation 2.

Our main results allow easy calculation of the optimal threshold  optimal shrinkage and signal-to-noise
cutoffs for various speciﬁc contamination models. For example:

1. Additive noise and missing-at-random. Let X be an m-by-n low-rank matrix. Assume
that some entries are completely missing and the rest suffer white additive noise. Formally 
we observe the contaminated matrix

(cid:26)Xi j + Zi j w.p. κ

Yi j =

0

w.p. 1 − κ

 

iid∼ (0  σ2)  namely  follows an unknown distribution with mean 0 and variance
where Zi j
σ2. Let β = m/n. Theorem 1 implies that in this case  the optimal hard threshold for the
singular values of Y is
(cid:113)
1 + β +(cid:112)1 + 14β + β2/

λ∗ =(cid:112)σ2κ (c + 1/c) (c + β/c)

where c =
2. In other words  the optimal location (w.r.t mean
square error) to truncate the singular values of Y   in order to recover X  is given by λ∗. The

√

4

0123456y0123456η(y)β=0.3β=0.6β=10.150.250.350.450.550.650.750.850.95κ00.20.40.60.811.21.41.61.822.22.42.62.83xThreshold CriticalShrinker Criticaloptimal shrinker from Theorem 2 for this contamination mode may be calculated similarly 
and is shown in Figure 1  left panel. By Theorem 4  the improvement in mean square
error obtained by using the optimal shrinker  over the TSVD baseline  is 3rσ2/κ  quite a
signiﬁcant improvement.

2. Additive noise and corrupt-at-random. Let X be an m-by-n low-rank matrix. Assume
that some entries are irrecoverably corrupt (replaced by random entries)  and the rest suffer
white additive noise. Formally 

(cid:26)Xi j + Zi j w.p. κ

.

Where Zi j
The optimal shrinker  which should be applied to the singular values of Y   is given by:

iid∼ (0  σ2)  Wi j

Yi j =

Wi j

w.p. 1 − κ

iid∼ (0  τ 2)  and τ is typically large. Let ˜σ =(cid:112)κσ2 + (1 − κ)τ 2.
˜σ2/(yκ)
(cid:113)(cid:0)(y/˜σ)2 − β − 1(cid:1)2 − 4 y ≥ ˜σ(1 +(cid:112)β)
y < ˜σ(1 +(cid:112)β)

0

∗

η

(y) =

.

By Theorem 4  the improvement in mean square error  obtained by using the optimal
shrinker  over the TSVD baseline  is 3r(κσ2 + (1 − κ)τ 2)/κ2.

1.2 Related Work

The general data contamination model we propose includes as special cases several modes extensively
studied in the literature  including missing-at-random and outliers. While it is impossible to propose a
complete list of algorithms to handle such data  we offer a few pointers  organized around the notions
of robust principal component analysis (PCA) and matrix completion. To the best of our knowledge 
the precise effect of general data contamination on the SVD (or the closely related PCA) has not been
documented thus far. The approach we propose  based on careful manipulation of the data singular
values  enjoys three distinct advantages. One  its running time is not prohibitive; indeed  it involves a
small yet important modiﬁcation on top of the SVD or TSVD  so that it is available whenever the
SVD is available. Two  it is well understood and its performance (say  in mean square error) can be
reliably predicted by the available theory. Three  to the best of our knowledge  none of the approaches
below have become mainstream  and most practitioners still turn to the SVD  even in the presence of
data contamination. Our approach can easily be used in practice  as it relies on the well-known and
very widely used SVD  and can be implemented as a simple modiﬁcation on top of the existing SVD
implementations.
Robust Principle Component Analysis (RPCA). In RPCA  one assumes Y = X + W where
X is the low rank target matrix and W is a sparse outliers matrix. Classical approaches such as
inﬂuence functions [20]  multivariate trimming [17] and random sampling techniques [14] lack a
formal theoretical framework and are not well understood. More modern approaches based on convex
optimization [9  34] proposed reconstructing X from Y via the nuclear norm minimization

||X||∗ + λ||Y − X||1  

min

X

whose runtime and memory requirements are both prohibitively large in medium and large matrices.
Matrix Completion. There are numerous heuristic approaches for data analysis in the presence of
missing values [5  30  31]. To the best of our knowledge  there are no formal guarantees of their
performance. When the target matrix is known to be low rank  the reconstruction problem is known
as matrix completion. [7–9] and numerous other authors have shown that a semi-deﬁnite program
may be used to stably recover the target matrix  even in the presence of additive noise. Here too  the
runtime and memory requirements are both prohibitively large in medium and large matrices  making
these algorithms infeasible in practice.

2 A Uniﬁed Model for Uniformly Distributed Contamination

Contamination modes encountered in practice are best described by a combination of primitive modes 
shown in Table 1 below. These primitive contamination modes ﬁt into a single template:

5

Deﬁnition 1. Let A and B be two random variables  and assume that all moments of A and B are
bounded. Deﬁne the contamination link function

Given a matrix X  deﬁne the corresponding contaminated matrix Y with entries

fA B(x) = Ax + B .

indep.∼ fA B(Xi j) .

Yi j

(6)

Now observe that each of the primitive modes above corresponds to a different choice of random
variables A and B  as shown in Table 1. Speciﬁcally  each of the primitive modes is described by a
different assignment to A and B. We employ three different random variables in these assignments:
iid∼ (0  τ 2/n)  a
Z
iid∼ Bernoulli(κ) describing a
random variable describing a large “outlier” measurement; and M
random choice of “defective” entries  such as a missing value  an outlier and so on.

iid∼ (0  σ2/n)  a random variable describing multiplicative or additive noise; W

Table 1: Primitive modes ﬁt into the model (6). By convention  Y is m-by-n  Z
iid∼ (0  τ 2/n) denotes an outlier random variable and M
noise random variable  W
contaminated entry random variable.

iid∼ (0  σ2/n) denotes a
iid∼ Bernoulli(κ) is a

mode
i.i.d additive noise
i.i.d multiplicative noise
missing-at-random
outliers-at-random
corruption-at-random

model
Yi j = Xi j + Zi j
σ
Yi j = Xi j Zi j
σ
Yi j = Mi j Xi j
κ
κ τ
Yi j = Xi j + Mi jWi j
Yi j = Mi jXi j + (1 − Mi j)Wi j M (1 − M )W κ τ

A
1
Z
M
1

B
Z
0
0

M W

levels

Actual datasets rarely demonstrate a single primitive contamination mode. To adequately describe
contamination observed in practice  one usually needs to combine two or more of the primitive
contamination modes into a composite mode. While there is no point in enumerating all possible
combinations  Table 2 offers a few notable composite examples  using the framework (6). Many other
examples are possible of course.

3 Signal Model

Following [32] and [15]  as we move toward our formal results we are considering an asymptotic
model inspired by Johnstone’s Spiked Model [22]. Speciﬁcally  we are considering a sequence of
iid∼ fAn Bn (Xn). We
increasingly larger data target matrices Xn  and corresponding data matrices Yn
make the following assumptions regarding the matrix sequence {Xn}:

A1 Limiting aspect ratio: The matrix dimension mn × n sequence converges: mn/n → β as
A2 Fixed signal column span: Let the rank r > 0 be ﬁxed and choose a vector x ∈ Rr with

n → ∞. To simplify the results  we assume 0 < β ≤ 1.

coordinates x = (x1  . . . xr) such that x1 > . . . > xr > 0. Assume that for all n

is an arbitrary singular value decomposition of Xn 

Xn = ˜Un diag(x1  . . .   xr) ˜Vn

Table 2: Some examples of composite contamination modes and how they ﬁt into the model (6). Z W  M are
the same as in Table 1.

mode

Additive noise and missing-at-random
Additive noise and corrupt-at-random

multiplicative noise and corrupt-at-random ZM
1

Additive noise and outliers

6

A
M
M ZM + W (1 − M )

B
ZM

W (1 − M )

Z + W (1 − M )

levels
σ κ
σ κ τ
σ κ τ
σ κ τ

A3 Incoherence of the singular vectors of Xn: We make one of the following two assumptions

regarding the singular vectors of Xn:
A3.1 Xn is random with an orthogonally invariant distribution. Speciﬁcally  ˜Un and ˜Vn 
which follow the Haar distribution on orthogonal matrices of size mn and n  respec-
tively.

A3.2 The singular vectors of Xn are non-concentrated. Speciﬁcally  each left singular vector
˜un i of Xn (the i-th column of ˜Un) and each right singular vector ˜vn j of Xn (the j-th
column of ˜Vn) satisfy1
||˜un i||∞ ≤ C

||˜vn j||∞ ≤ C

logD(n)√

logD(mn)

and

√

n

mn

for any i  j and ﬁxed constants C  D.

Deﬁnition 2. (Signal model.) Let An
B/n) have bounded
moments. Let Xn follow assumptions [A1]–[A3] above. We say that the matrix sequence
Yn = fAn Bn (Xn) follows our signal model  where fA B(X) is as in Deﬁnition 1. We further denote
i=1 yn iun ivn i

i=1 xi˜un i˜vn i for the singular value decomposition of Xn and Yn =(cid:80)m

Xn =(cid:80)r

A/n) and Bn

iid∼ (µA  σ2

iid∼ (0  σ2

for the singular value decomposition of Yn.

4 Main Results

Having described the contamination and the signal model  we can now formulate our main results.
All proofs are deferred to the Supporting Information. Let Xn and Yn follow our signal model 
Deﬁnition 2  and write x = (x1  . . .   xr) for the non-zero singular values of Xn. For a shrinker η 
we write

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆXn(Yn) − Xn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

.

F

L∞(η|x) a.s.= lim
n→∞

assuming the limit exists almost surely. The special case of hard thresholding at λ is denoted as
L∞(η|x).
Deﬁnition 3. Optimal shrinker and optimal threshold. A shrinker η∗ is called optimal if

L∞(η|x) ≤ L∞(η|x)

for any shrinker η  any r ≥ 1 and any x = (x1  . . .   xr). Similarly  a threshold λ is called optimal if
L∞(λ∗|x) ≤ L∞(λ|x) for any threshold λ  any r ≥ 1 and any x = (x1  . . .   xr).
With these deﬁnitions  our main results Theorem 2 and Theorem 1 become formal. To make Theorem
3 formal  we need the following lemma and deﬁnition.
Lemma 1. Decomposition of the asymptotic mean square error. Let Xn and Yn follow our signal
model (Deﬁnition 2) and write x = (x1  . . .   xr) for the non-zero singular values of Xn  and let η be

the optimal shrinker. Then the limit L∞(η|x) a.s. exists  and L∞(η|x) a.s.= (cid:80)r

i=1 L1(η|x)  where

(cid:19)

(t4 − β)2

(cid:18)

1 −

x2
(cid:19)2(cid:18)(cid:18)

x2

L1(η|x) =

where t = (µA · x)/σB. Similarly  for a threshold λ we have L∞(λ|x) =(cid:80)r

t < β

1
4

(t4 + βt2)(t4 + t2)

(cid:19)(cid:18)

(cid:19)

(cid:18)

t +

1
t

t +

β
t

−

t2 − 2β
t2

t ≥ β

1
4

(cid:19)(cid:19)

i=1 L1(λ|x) with

µAx ≥ x(λ)

µAx < x(λ)


(cid:18) σB

µA

x2

(cid:114)

L1(λ|x) =

Where

x(y) =

(σB/

0

√

2µA)

(y/σB)2 − β − 1 +

(cid:113)(cid:0)1 + β − (y/σB)2(cid:1)2 − 4β t ≥ β

t < β

1
4

1
4

(7)

1The incoherence assumption is widely used in related literature [6  12  27]  and asserts that the singular

vectors are spread out so X is not sparse and does not share singular subspaces with the noise.

7

Deﬁnition 4. Let η0 be the zero singular value shrinker  η0(y) ≡ 0  so that ˆXη0(Y ) ≡ 0. Let η be a
singular value shrinker. The critical signal level for η is

xcritical(η) = inf
x

{L1(η|X) < L1(η0|X)}

As we can see  the asymptotic mean square error decomposes over the singular values of the target
matrix  x1  . . .   xr. Each value xi that falls below xcritical(η) is better estimated with the zero
shrinker η0 than with η. It follows that any xi that falls below xcritical(η∗)  where η∗ is the optimal
shrinker  cannot be reliably estimated by any shrinker η  and its corresponding data singular value yi
should simply be set to zero. This makes Theorem 2 formal.

5 Estimating the model parameters

In practice  using the optimal shrinker we propose requires an estimate of the model parameters. In
general  σB is easy to estimate from the data via a median-matching method [15]  namely

ˆσB =

ymed√
nµβ

 

where ymed is the median singular value of Y  and µβ is the median of the Mar˘cenko-Pastur distribu-
tion. However  estimation of µA and σA must be considered on a case-by-case basis. For example  in
the “Additive noise and missing at random” mode (table 2)  σA ≡ 1 is known  and µA is estimated
by dividing the amount of missing values by the matrix size.

6 Simulation

Simulations were performed to verify the correctness of our main results2. For more details  see
Supporting Information.

1. Critical signal level xcritical(λ∗) under increasing noise. Figure 2  left panel  shows
the amount of data singular values yi above xcritical(λ∗)  as a function of the fraction of
missing values κ. Theorem 3 correctly predicts the exact values of κ at which the “next”
data singular value falls below xcritical(λ∗).

2. Phase plane for critical signal levels xcritical(η∗) and xcritical(λ∗). Figure 1  right panel 
shows the x  κ plane  where x is the signal level and κ is the fraction of missing values. At
each point in the plane  several independent data matrices were generated. Heatmap shows
the fraction of the experiments at which the data singular value y1 was above xcritical(η∗)
and xcritical(λ∗). The overlaid graphs are theoretical predictions of the critical points.

3. Brute-force veriﬁcation of the optimal shrinker shape. Figure 2  right panel  shows the
shape of the optimal shrinker (Theorem 1). We performed a brute-force search for the value
of η(y) that produces the minimal mean square error. A brute force search  performed with
a relatively small matrix size  matches the asymptotic shape of the optimal shrinker.

7 Conclusions

Singular value shrinkage emerges as an effective method to reconstruct low-rank matrices from
contaminated data that is both practical and well understood. Through simple  carefully designed
manipulation of the data singular values  we obtain an appealing improvement in the reconstruction
mean square error. While beyond our present scope  following [16]  it is highly likely that the
optimal shrinker we have developed offers the same mean square error  asymptotically  as the best
rotation-invariant estimator based on the data  making it asymptotically the best SVD-based estimator
for the target matrix.

2The full Matlab code that generated the ﬁgures in this paper and in the Supporting Information is permanently

available at https://purl.stanford.edu/kp113fq0838.

8

Figure 2: Left: empirical validation of the predicted critical signal level (Simulation 1). Right:
Empirical validation of the optimal shrinker shape (Simulation 3).

Acknowledgements

DB was supported by Israeli Science Foundation grant no. 1523/16 and German-Israeli Foundation
for scientiﬁc research and development program no. I-1100-407.1-2015.

References
[1] Benaych-Georges  Florent and Nadakuditi  Raj Rao. The singular values and vectors of low
rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis  111:
120–135  2012. ISSN 0047259X.

[2] Bloemendal  Alex  Erdos  Laszlo  Knowles  Antti  Yau  Horng Tzer  and Yin  Jun. Isotropic
local laws for sample covariance and generalized Wigner matrices. Electronic Journal of
Probability  19(33):1–53  2014. ISSN 10836489.

[3] Boutsidis  Christos  Zouzias  Anastasios  Mahoney  Michael W  and Drineas  Petros. Ran-
domized dimensionality reduction for k-means clustering. IEEE Transactions on Information
Theory  61(2):1045–1062  2015.

[4] Bouwmans  Thierry  Sobral  Andrews  Javed  Sajid  Ki  Soon  and Zahzah  El-hadi. Decomposi-
tion into low-rank plus additive matrices for background / foreground separation : A review for
a comparative evaluation with a large-scale dataset. Computer Science Review  2016. ISSN
1574-0137.

[5] Buuren  Stef and Groothuis-Oudshoorn  Karin. mice: Multivariate imputation by chained

equations in r. Journal of statistical software  45(3)  2011.

[6] Cai  Jian-Feng  Candes  Emmanuel J.  and Zuowei  Shen. A singular value thresholding
algorithm for matrix completion. 2010 Society for Industrial and Applied Mathematics  20(4):
1956–1982  2010.

[7] Candes  Emmanuel J. and Plan  Yaniv. Matrix completion with noise. Proceedings of the IEEE 

98(6):925–936  2010. ISSN 00189219.

[8] Candes  Emmanuel J and Plan  Yaniv. Matrix completion with noise. Proceedings of the IEEE 

98(6):925–936  2010.

[9] Candès  Emmanuel J.  Li  Xiaodong  Ma  Yi  and Wright  John. Robust principal component

analysis? Journal of the ACM  58(3):1–37  may 2011. ISSN 00045411.

[10] Candes  Emmanuel J  Sing-Long  Carlos A  and Trzasko  Joshua D. Unbiased risk estimates for
singular value thresholding and spectral estimators. IEEE transactions on signal processing  61
(19):4643–4657  2013.

9

00.20.40.60.81κ00.511.522.533.544.55Number of estimable singular values11.522.533.544.5y-10123456η(y)TheoreticalEmpirical[11] Cattell  Raymond B. The scree test for the number of factors. Multivariate Behavioral Research 

1(2):245–276  1966.

[12] Chandrasekaran  Venkat  Sanghavi  Sujay  Parrilo  Pablo a.  and Willsky  Alan S. Rank-Sparsity
Incoherence for Matrix Decomposition. SIAM Journal on Optimization  21(2):572–596  2011.
ISSN 1052-6234.

[13] Das  Rajarshi  Zaheer  Manzil  and Dyer  Chris. Gaussian lda for topic models with word

embeddings. In ACL (1)  pp. 795–804  2015.

[14] Fischler  Martin A and Bolles  Robert C. Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Communications of the
ACM  24(6):381–395  1981.

[15] Gavish  Matan and Donoho  David L. The optimal hard threshold for singular values is 4/sqrt(3).

IEEE Transactions on Information Theory  60(8):5040–5053  2014. ISSN 00189448.

[16] Gavish  Matan and Donoho  David L. Optimal shrinkage of singular values. IEEE Transactions

on Information Theory  63(4):2137–2152  2017.

[17] Gnanadesikan  Ramanathan and Kettenring  John R. Robust estimates  residuals  and outlier

detection with multiresponse data. Biometrics  pp. 81–124  1972.

[18] Golub  Gene and Kahan  William. Calculating the singular values and pseudo-inverse of a
matrix. Journal of the Society for Industrial and Applied Mathematics  Series B: Numerical
Analysis  2(2):205–224  1965.

[19] Hastie  Trevor  Tibshirani  Robert  Sherlock  Gavin  Brown  Patrick  Botstein  David  and
Eisen  Michael. Imputing Missing Data for Gene Expression Arrays Imputation using the SVD.
Technical Report  pp. 1–9  1999.

[20] Huber  Peter J. Robust statistics. Springer  2011.

[21] Ji  Hui  Liu  Chaoqiang  Shen  Zuowei  and Xu  Yuhong. Robust video denoising using low
rank matrix completion. 2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition  pp. 1791–1798  2010. ISSN 1063-6919.

[22] Johnstone  Iain M. On the distribution of the largest eigenvalue in principal components analysis.

The Annals of Statistics  29(2):295–327  2001.

[23] Lin  Zhouchen  Chen  Minming  and Ma  Yi. The Augmented Lagrange Multiplier Method for

Exact Recovery of Corrupted Low-Rank Matrices. 2013.

[24] Luo  Xin  Zhou  Mengchu  Xia  Yunni  and Zhu  Qingsheng. An efﬁcient non-negative
matrix-factorization-based approach to collaborative ﬁltering for recommender systems. IEEE
Transactions on Industrial Informatics  10(2):1273–1284  2014.

[25] Marcenko  V. A. and Pastur  L. A. Distribution of eigenvalues for some sets of random matrices.

Math. USSR-Sbornik  1(4):457–483  1967.

[26] Meloun  Milan  Capek  Jindrich  Miksk  Petr  and Brereton  Richard G. Critical comparison of
methods predicting the number of components in spectroscopic data. Analytica Chimica Acta 
423(1):51–68  2000.

[27] Nadakuditi  Raj Rao. OptShrink: An algorithm for improved low-rank signal matrix Denoising
by optimal  data-driven singular value shrinkage. IEEE Transactions on Information Theory  60
(5):3002–3018  2014. ISSN 00189448.

[28] Rao  Nikhil  Yu  Hsiang-Fu  Ravikumar  Pradeep K  and Dhillon  Inderjit S. Collaborative
ﬁltering with graph information: Consistency and scalable methods. In Advances in neural
information processing systems  pp. 2107–2115  2015.

10

[29] Rennie  Jasson Dm M and Srebro  Nathan. Fast Maximum Margin Matrix Factorization for
Collaborative Prediction. Proceedings of the 22Nd International Conference on Machine
Learning  pp. 713–719  2005. ISSN 1595931805. doi: 10.1145/1102351.1102441. URL
http://doi.acm.org/10.1145/1102351.1102441.

[30] Rubin  Donald B. Multiple imputation after 18+ years. Journal of the American statistical

Association  91(434):473–489  1996.

[31] Schafer  Joseph L. Analysis of incomplete multivariate data. CRC press  1997.

[32] Shabalin  Andrey A and Nobel  Andrew B. Reconstruction of a low-rank matrix in the presence

of Gaussian noise. Journal of Multivariate Analysis  118:67–76  2013. ISSN 0047-259X.

[33] Stein  Charles M. Lectures on the theory of estimation of many parameters. Journal of
Soviet Mathematics  74(5)  1986. URL http://link.springer.com/article/10.1007/
BF01085007.

[34] Wright  John  Peng  Yigang  Ma  Yi  Ganesh  Arvind  and Rao  Shankar. Robust Principal
Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices. Advances in Neural
Information Processing Systems (NIPS)  pp. 2080—-2088  2009. ISSN 0010-3640.

[35] Yang  Jian  Qian  Jianjun  Luo  Lei  Zhang  Fanlong  and Gao  Yicheng. Nuclear norm based
matrix regression with applications to face recognition with occlusion and illumination changes.
IEEE Transactions on Pattern Analysis and Machine Intelligence Machine Intelligence  pp(99):
1–1  2016. ISSN 0162-8828.

11

,Danny Barash
Matan Gavish