2011,Active Classification based on Value of Classifier,Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers  which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time  where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations  using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance  resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets  and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.,Active Classiﬁcation based on Value of Classiﬁer

Department of Electrical Engineering

Department of Computer Science

Daphne Koller

Stanford University
Stanford  CA 94305

Tianshi Gao

Stanford University
Stanford  CA 94305

tianshig@stanford.edu

koller@cs.stanford.edu

Abstract

Modern classiﬁcation tasks usually involve many class labels and can be informed
by a broad range of features. Many of these tasks are tackled by constructing a
set of classiﬁers  which are then applied at test time and then pieced together in a
ﬁxed procedure determined in advance or at training time. We present an active
classiﬁcation process at the test time  where each classiﬁer in a large ensemble
is viewed as a potential observation that might inform our classiﬁcation process.
Observations are then selected dynamically based on previous observations  using
a value-theoretic computation that balances an estimate of the expected classiﬁca-
tion gain from each observation as well as its computational cost. The expected
classiﬁcation gain is computed using a probabilistic model that uses the outcome
from previous observations. This active classiﬁcation process is applied at test
time for each individual test instance  resulting in an efﬁcient instance-speciﬁc de-
cision path. We demonstrate the beneﬁt of the active scheme on various real-world
datasets  and show that it can achieve comparable or even higher classiﬁcation ac-
curacy at a fraction of the computational costs of traditional methods.

1 Introduction

As the scope of machine learning applications has increased  the complexity of the classiﬁcation
tasks that are commonly tackled has grown dramatically. On one dimension  many classiﬁcation
problems involve hundreds or even thousands of possible classes [8]. On another dimension  re-
searchers have spent considerable effort developing new feature sets for particular applications  or
new types of kernels. For example  in an image labeling task  we have the option of using GIST
feature [26]  SIFT feature [23]  spatial HOG feature [33]  Object Bank [21] and more. The beneﬁts
of combining information from different types of features can be very signiﬁcant [12  33].

To solve a complex classiﬁcation problem  many researchers have resorted to ensemble methods  in
which multiple classiﬁers are combined to achieve an accurate classiﬁcation decision. For example 
the Viola-Jones classiﬁer [32] uses a cascade of classiﬁers  each of which focuses on different spatial
and appearance patterns. Boosting [10] constructs a committee of weak classiﬁers  each of which
focuses on different input distributions. Multiclass classiﬁcation problems are very often reduced
to a set of simpler (often binary) decisions  including one-vs-one [11]  one-vs-all  error-correcting
output codes [9  1]  or tree-based approaches [27  13  3]. Intuitively  different classiﬁers provide
different “expertise” in making certain distinctions that can inform the classiﬁcation task. However 
as we discuss in Section 2  most of these methods use a ﬁxed procedure determined at training time
to apply the classiﬁers without adapting to each individual test instance.

In this paper  we take an active and adaptive approach to combine multiple classiﬁers/features at
test time  based on the idea of value of information [16  17  24  22]. At training time  we construct
a rich family of classiﬁers  which may vary in the features that they use or the set of distinctions
that they make (i.e.  the subset of classes that they try to distinguish). Each of these classiﬁers is
trained on all of the relevant training data. At test time  we dynamically select an instance-speciﬁc

1

subset of classiﬁers. We view each our pre-trained classiﬁer as a possible observation we can make
about an instance; each one adds a potential value towards our ability to classify the instance  but
also has a cost. Starting from an empty set of observations  at each stage  we use a myopic value-of-
information computation to select the next classiﬁer to apply to the instance in a way that attempts to
increase the accuracy of our classiﬁcation state (e.g.  decrease the uncertainty about the class label)
at a low computational cost. This process stops when one of the suitable criteria is met (e.g.  if
we are sufﬁciently conﬁdent about the prediction). We provide an efﬁcient probabilistic method for
estimating the uncertainty of the class variable and about the expected gain from each classiﬁer. We
show that this approach provides a natural trajectory  in which simple  cheap classiﬁers are applied
initially  and used to provide guidance on which of our more expensive classiﬁers is likely to be
more informative. In particular  we show that we can get comparable (or even better) performance
to a method that uses a large range of expensive classiﬁers  at a fraction of the computational cost.

2 Related Work

Our classiﬁcation model is based on multiple classiﬁers  so it resembles ensemble methods like
boosting [10]  random forests [4] and output-coding based multiclass classiﬁcation [9  1  29  14].
However  these methods use a static decision process  where all classiﬁers have to be evaluated
before any decision can be made. Moreover  they often consider a homogeneous set of classiﬁers 
but we consider a variety of heterogeneous classiﬁers with different features and function forms.

Some existing methods can make classiﬁcation decisions based on partial observations. One exam-
ple is a cascade of classiﬁers [32  28]  where an instance goes through a chain of classiﬁers and the
decision can be made at any point if the classiﬁer response passes some threshold. Another type of
method focuses on designing the stopping criteria. Schwing et al. [30] proposed a stopping criterion
for random forests such that decisions can be made based on a subset of the trees. However  these
methods have a ﬁxed evaluation sequence for any instance  so there is no adaptive selection of which
classiﬁers to use based on what we have already observed.

Instance-speciﬁc decision paths based on previous observations can be found in decision tree style
models  e.g.  DAGSVM [27] and tree-based methods [15  13  3]. Instead of making hard decisions
based on individual observations like these methods  we use a probabilistic model to fuse informa-
tion from multiple observations and only make decisions when it is sufﬁciently conﬁdent.

When observations are associated with different features  our method also performs feature selec-
tion. Instead of selecting a ﬁxed set of features in the learning stage [34]  we actively select instance-
speciﬁc features in the test stage. Furthermore  our method also considers computational properties
of the observations. Our selection criterion trades off between the statistical gain and the compu-
tational cost of the classiﬁer  resulting in a computationally efﬁcient cheap-to-expensive evaluation
process. Similar ideas are hard-coded by Vedaldi et al. [31] without adaptive decisions about when to
switch to which classiﬁer with what cost. Angelova et al. [2] performed feature selection to achieve
certain accuracy under some computational budget  but the selection is at training time without adap-
tation to individual test instances. Chai et al. [5] considered test-time feature value acquisition with
a strong assumption that observations are conditionally independent given the class variable.

Finally  our work is inspired by decision-making under uncertainty based on value of informa-
tion [16  17  24  22]. For classiﬁcation  Krause and Guestrin [19] used it to compute a conditional
plan for asking the expert  trying to optimize classiﬁcation accuracy while requiring as little expert
interaction as possible. In machine learning  Cohn et al. [7] used active learning to select training
instances to reduce the labeling cost and speedup the learning  while our work focuses on inference.

3 Model

We denote the instance and label pair as (X  Y ). Furthermore  we assume that we have been pro-
vided a set of trained classiﬁers H  where hi ∈ H : X → R can be any real-valued classiﬁers
(functions) from existing methods. For example  for multiclass classiﬁcation  hi can be one-vs-all
classiﬁers  one-vs-one classiﬁers and weak learners from the boosting algorithms. Note that hi’s
do not have to be homogeneous meaning that they can have different function forms  e.g.  linear
or nonlinear  and more importantly they can be trained on different types of features with various
computational costs. Given an instance x  our goal is to infer Y by sequentially selecting one hi to
evaluate at a time  based on what has already been observed  until we are sufﬁciently conﬁdent about

2

Y or some other stopping criterion is met  e.g.  the computational constraint. The key in this process
is how valuable we think a classiﬁer hi is  so we introduce the value of a classiﬁer as follows.
Value of Classiﬁer. Let O be the set of classiﬁers that have already been evaluated (empty at the
beginning). Denote the random variable Mi = hi(X) as the response/margin of the i-th classiﬁer
in H and denote the random vector for the observed classiﬁers as MO = [Mo1   Mo2   . . .   Mo|O|]T  
where ∀oi ∈ O. Given the actual observed values mO of MO  we have a posterior P (Y |mO)
over Y . For now  suppose we are given a reward R : P → R which takes in a distribution P and
returns a real value indicating how preferable P is. Furthermore  we use C(hi|O) to denote the
computational cost of evaluating classiﬁer hi conditioned on the set of evaluated classiﬁers O. This
is because if hi shares the same feature with some oi ∈ O  we do not need to compute the feature
again. With some chosen reward R and a computational model C(hi|O)  we deﬁne the value of an
unobserved classiﬁer as follows.

Deﬁnition 1 The value of classiﬁer V (hi|mO) for a classiﬁer hi given the observed classiﬁer re-
sponses mO is the combination of the expected reward of the state informed by hi and the compu-
tational cost of hi. Formally 

V (hi|mO) ∆=Z P (mi|mO)R(P (Y |mi  mO))dmi −
=Emi∼P (Mi|mO)(cid:2)R(P (Y |mi  mO))(cid:3) −

1
τ

1
τ

C(hi|O)

C(hi|O)

(1)

The value of classiﬁer has two parts corresponding to the statistical and computational properties

of the classiﬁer respectively. The ﬁrst part VR(hi|mO) ∆= E(cid:2)R(P (Y |mi  mO))(cid:3) is the expected
reward of P (Y |mi  mO)  where the expectation is with respect to the posterior of Mi given mO.
The second part VC (hi|mO) ∆= − 1
τ C(hi|O) is a computational penalty incurred by evaluating the
classiﬁer hi. The constant τ controls the tradeoff between the reward and the cost.
Given the deﬁnition of the value of classiﬁer  at each step of our sequential evaluations  our goal is
to pick hi with the highest value:

h∗ = argmax
hi∈H\O

V (hi|mO) = argmax
hi∈H\O

VR(hi|mO) + VC (hi|mO)

(2)

We introduce the building blocks of the value of classiﬁer  i.e.  the reward  the cost and the proba-
bilistic model in the following  and then explain how to compute it.
Reward Deﬁnition. We propose two ways to deﬁne the reward R : P → R.
Residual Entropy. From the information-theoretical point of view  we want to reduce the uncertainty
of the class variable Y by observing classiﬁer responses. Therefore  a natural way to deﬁne the
reward is to consider the negative residual entropy  that is the lower the entropy the higher the
reward. Formally  given some posterior distribution P (Y |mO)   we deﬁne

R(P (Y |mO)) = −H(Y |mO) = Xy

P (y|mO) log P (y|mO)

(3)

The value of classiﬁer under this reward deﬁnition is closely related to information gain. Speciﬁcally 

VR(hi|mO) =Emi∼P (Mi|mO)(cid:2) − H(Y |mi  mO)(cid:3) + H(Y |mO) − H(Y |mO)

=I(Y ; Mi|mO) − H(Y |mO)

Since H(Y |mO) is a constant w.r.t. hi  we have

h∗ = argmax
hi∈H/O

VR(hi|mO) + VC (hi|mO) = argmax
hi∈H/O

I(Y ; Mi|mO) + VC (hi|mO)

(4)

(5)

Therefore  at each step  we want to pick the classiﬁer with the highest mutual information with the
class variable Y given the observed classiﬁer responses mO with a computational constraint.
Classiﬁcation Loss. From the classiﬁcation loss point of view  we want to minimize the expected
loss when choosing classiﬁers to evaluate. Therefore  given a loss function ∆(y  y′) specifying the

3

penalty of classifying an instance of class y to y′  we can deﬁne the reward as the negative of the
minimum expected loss:

R(P (Y |mO)) = − min

y′ Xy

P (y|mO)∆(y  y′) = − min
y′

Ey∼P (Y |mO)(cid:2)∆(y  y′)(cid:3)

(6)

To gain some intuition about this deﬁnition  consider a 0-1 loss function  i.e.  ∆(y  y′) = 1{y 6= y′} 
then R(P (Y |mO)) = −1 + maxy′ P (y′|mO). To maximize R  we want the peak of P (Y |mO) to
be as high as possible. In our experiment  these two reward deﬁnitions give similar results.
Classiﬁcation Cost. The cost of evaluating a classiﬁer h on an instance x can be broken down into
two parts. The ﬁrst part is the cost of computing the feature φ : X → Rn on which h is built  and
the second is the cost of computing the function value of h given the input φ(x). If h shares the
same feature as some evaluated classiﬁers in O  then C(h|O) only consists of the cost of evaluating
the function h  otherwise it will also include the cost of computing the feature input φ. Note that
computing φ is usually much more expensive than evaluating the function value of h.
Probabilistic Model. Given a test instance x  we construct an instance-speciﬁc joint distribution
over Y and the selected observations MO. Our probabilistic model is a mixture model  where each
component corresponds to a class Y = y  and we use a uniform prior P (Y ). Starting from an empty
O  we model P (Mi  Y ) as a mixture of Gaussian distributions. At each step  given the selected MO 
we model the new joint distribution P (Mi  MO  Y ) = P (Mi|MO  Y )P (MO  Y ) by modeling the
new P (Mi|MO  Y = y) as a linear Gaussian  i.e.  P (Mi|MO  Y = y) = N (θT
y). As we
y
show in Section 5  this choice of probabilistic model works well empirically. We discuss how to
learn the distribution and do inference in the next section.

MO  σ2

4 Learning and Inference

Learning P (Mi|mO  y). Given the subset of the training set {(x(j)  y(j) = y)}Ny
to the instances from class y  we denote m(j)
from {(m(j)  y(j) = y)}Ny
P (Mi|y) = N (µy  σ2
and σ2
sian  i.e.  µy = θT
y
we know mO at test time  we estimate θy and σ2

j=1 corresponding
i = hi(x(j))  then our goal is to learn P (Mi|mO  y)
j=1. If O = ∅  then P (Mi|mO  y) reduces to the marginal distribution
Ny Pj m(j)
 
If O 6= ∅  we assume that P (Mi|mO  y) is a linear Gaus-
mO. Note that we also append a constant 1 to mO as the bias term. Since
y by maximizing the local likelihood with a Gaus-
k2

y)  and based on maximum likelihood estimation  we have µy = 1
i − µy)2.

Ny Pj(m(j)

y = 1

i

sian prior on θy. Speciﬁcally  for each training instance j from class y  let wj = e−
where β is a bandwidth parameter  then the regularized local log likelihood is

kmO −m

(j)
O

β

 

L(θy  σy; mO) = −λ k θy k2

2 +

Ny

Xj=1

wj log N (m(j)

i

; θT
y

m

(j)

O   σ2
y)

(7)

where we overload the notation N (x; µy  σ2
variance σ2
with ℓ2 regularization. Maximizing (7) results in:

y) to mean the value of a Gaussian PDF with mean µy and
y evaluated at x. Note that maximizing (7) is equivalent to locally weighted regression [6]

ˆθy = argmin

θy

λ k θy k2

2 +

Ny

Xj=1

wj k m(j)

i − θT
y

m

(j)
O k2

2= ( ¯MT

O

W ¯MO + λI)−1 ¯MT
O

W ¯Mi

(8)

where ¯MO is a matrix whose j-th row is m
wj’s   ¯Mi is an column vector whose j-th element is m(j)
noting that ( ¯MT
O
shared for different classiﬁers hi’s. Finally  the estimated σ2

(j)T
O   W is a diagonal matrix whose diagonal entries are
  and I is an identity matrix. It is worth
W ¯MO + λI)−1W in (8) does not depend on i  so it can be computed once and

i

y is

ˆσy

2 =

Ny

Xj=1

1

j=1 wj

PNy

wj k m(j)

i − ˆθy

T

m

(j)

O k2

(9)

4

Computing V (fi|mO). Given the learned distribution  we can easily compute the two CPDs
in (1)  i.e.  P (Mi|mO) and P (Y |mi  mO). P (Mi|mO) can be obtained as P (Mi|mO) =
Py P (Mi|mO  y)P (y|mO)  where P (Y |mO) is the posterior over Y given some observation
mO which is tracked over iterations. Speciﬁcally  P (Y |mi  mO) ∝ P (mi  mO|Y )P (Y ) =
P (mi|mO  Y )P (mO|Y )P (Y )  where all terms are available by caching previous computations.
Finally  to compute V (fi|mO)  the computational part VC (fi|mO) is just a lookup in a cost table 
and the expected reward part VR(fi|mO) can be rewritten as:

VR(hi|mO) = Xy

P (y|mO)Emi∼P (Mi|mO y)(cid:2)R(P (Y |mi  mO))(cid:3)

(10)

Therefore  each component Emi∼P (Mi|mO y)(cid:2)R(P (Y |mi  mO))(cid:3) is the expectation of a function
of a scalar Gaussian variable. We use Gaussian quadrature [18] 1 to approximate each component
expectation  and then do the weighted average to get VR(hi|mO).
Dynamic Inference. Given the building blocks introduced before  one can execute the classiﬁcation
process in |H| steps  where at each step  the values of all the remaining classiﬁers are computed.
However  this will incur a large scheduling cost. This is due to the fact that usually |H| is large. For
example  in multiclass classiﬁcation  if we include all one-vs-one classiﬁers into H  |H| is quadratic
in the number of classes. Since we are maintaining a belief over Y as observations are accumulated 
we can use it to make the inference process more adaptive resulting in small scheduling cost.
Early Stopping. Based on the posterior P (Y |mO)  we can make dynamic and adaptive decision
about whether to continue observing new classiﬁers or stop the process. We propose two stop-
ping criteria. We stop the inference process whenever either of them is met  and use the pos-
terior over Y at that point to make classiﬁcation decision. The ﬁrst criterion is based on the
information-theoretic point of view. Given the current posterior estimation P (Y |mi  mO) and
the previous posterior estimation P (Y |mO)  the relative entropy (KL-divergence) between them
is D(cid:16)P (Y |mO) k P (Y |mi  mO)(cid:17). We stop the inference procedure when this divergence is below

some threshold t. The second criterion is based on the classiﬁcation point of view. We consider the
gap between the probability of the current best class and that of the runner-up. Speciﬁcally  we deﬁne
the margin given a posterior P (Y |mO) as δm(P (Y |mO)) = P (y∗|mO) − maxy6=y∗ P (Y |mO) 
where y∗ = argmaxy P (y|mO). If δm(P (Y |mO)) ≥ t′  then the inference stops.
Dynamic Pruning of Class Space. In many cases  a class is mainly confused with a small number of
other classes (the confusion matrix is often close to sparse). This implies that after observing a few
classiﬁers  the posterior P (Y |mO) is very likely to be dominated by a few modes leaving the rest
with very small probability. For those classes y with very small P (y|mO)  their contributions to the
value of classiﬁer (10) are negligible. Therefore  when computing (10)  we ignore the components
whose P (y|mO) is below some small threshold (equivalent to setting the contribution from this
component to 0). Furthermore  when P (y|mO) falls below some very small threshold for a class y 
we will not estimate the likelihood related to y  i.e.  P (Mi|mO  y)  but use a small constant.
Dynamic Classiﬁer Space. To avoid computing the values of all the remaining classiﬁers  we can
dynamically restrict the search space of classiﬁers to those having high expected mutual informa-
tion with Y with respect to the current posterior P (Y |mO). Speciﬁcally  during the training  for
each classiﬁer hi we can compute the mutual information I(Mi; By) between its response Mi and
a class y  where By is a binary variable indicating whether an instance is from class y or not.
Given our current posterior P (Y |mO)  we tried two ways to rank the unobserved classiﬁers. First 
we simply select the top L classiﬁers with the highest I(Mi; Bˆy)  where ˆy is the most probable
class based on current posterior. Since we can sort classiﬁers in the training stage  this step is con-
stant time. Another way is that for each classiﬁer  we can compute a weighted mutual information

score  i.e.  Py P (y|mO)I(Mi; By)  and we restrict the classiﬁer space to those with the top L
scores. Note that computing the scores is very efﬁcient  since it is just an inner product between two
vectors  where I(Y ; By)’s have been computed and cached before testing. Our experiments showed
that these two scores have similar performances  and we used the ﬁrst method to report the results.
Analysis of Time Complexity. At each iteration t  the scheduling overhead includes selecting
the top L candidate observations  and for each candidate i  learning P (Mi|mO  y) and computing

1We found that 3 or 5 points provide an accurate approximation.

5

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

 
0

results on satimage dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree

5

10

number of evaluated classifiers

 

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
a
n
o
i
t
a
c

 

i
f
i

s
s
a

l

c

 
t
s
e
t

15

 
0

5

results on pendigits dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
20

25

40

15

10
35
number of evaluated classifiers

30

 

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

45

0.2

 
0

5

10

results on vowel dataset

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
25

30

45

35

40

50

15

20

number of evaluated classifiers

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

y
c
a
r
u
c
c
a
n
o
i
t
a
c

 

i
f
i

s
s
a

l

c

 
t
s
e
t

55

0.1

 
100

results on letter dataset

 

selection by value of classifier
random selection
one−vs−all
dagsvm
one−vs−one
tree
101

102
number of evaluated classifiers

Figure 1: (Best viewed magniﬁed and in colors) Performance comparisons on UCI datasets. From
the left to right are the results on satimage  pendigits  vowel and letter (in log-scale) datasets. Note
that the error bars for pendigits and letter datasets are very small (around 0.5% on average).

V (fi|mO). First  selecting the top L candidate observations is a constant time  since we can sort
the observations based on I(Mi; By) before the test process. Second  estimating P (Mi|mO  y)
requires computing (8) and (9) for different y’s. Given our dynamic pruning of class space  suppose
there are only Nt Y promising classes to consider instead of the total number of classes K. Since
( ¯MT
W ¯MO + λI)−1W in (8) does not depend on i  we compute it for each promising class  which
O
y + t2Ny + t3) ﬂoating point operations  and share it for different i’s. After computing
takes O(tN 2
this shared component  for each pair of i and a promising class  computing (8) and (9) both take
t Y ). Putting everything together  the overall cost at
O(tNy). Finally  computing (10) takes O(N 2
t Y ). The key to have a low cost is to
iteration t is O(Nt Y (tN 2
effectively prune the class space (small Nt Y ) and reach a decision quickly (small t).

y + t2Ny + t3) + LNt Y tNy + LN 2

5 Experimental Results

We performed experiments on a collection of four UCI datasets [25] and on a scene recognition
dataset [20]. All tasks are multiclass classiﬁcation problems. The ﬁrst set of experiments focuses on
a single feature type and aims to show that (i) our probabilistic model is able to combine multiple
binary classiﬁers to achieve comparable or higher classiﬁcation accuracy than traditional methods;
(ii) our active evaluation strategy successfully selects a signiﬁcantly fewer number of classiﬁers. The
second set of experiments considers multiple features  with varying computational complexities.
This experiment shows the real power of our active scheme. Speciﬁcally  it dynamically selects an
instance-speciﬁc subset of features  resulting in higher classiﬁcation accuracy of using all features
but with a signiﬁcant reduction in the computational cost.
Basic Setup. Given a feature φ  our set of classiﬁers Hφ consists of all one-vs-one classiﬁers  all
one-vs-all classiﬁers  and all node classiﬁers from a tree-based method [13]  where a node classiﬁer
can be trained to distinguish two arbitrary clusters of classes. Therefore  for a K-class problem  the
number of classiﬁers given a single feature is |Hφ| = (K−1)K
+ K + Nφ tree  where Nφ tree is the
i=1  our pool of classiﬁers is
number of nodes in the tree model. If there are multiple features {φi}F
i=1Hφi. The form of all classiﬁers is linear SVM for the ﬁrst set of experiments and nonlinear
H = ∪F
SVM with various kernels for the second set of experiments. During training  in addition to learning
the classiﬁers  we also need to compute the response m(j)
i of each classiﬁer hi ∈ H for each training
instance x(j). In order to make the training distribution of the classiﬁer responses better match the
test distribution  when evaluating classiﬁer hi on x(j)  we do not want hi to be trained on x(j). To
achieve this  we use a procedure similar to cross validation. Speciﬁcally  we split the training set
into 10 folds  and for each fold  instances from this fold are tested using the classiﬁers trained on the
other 9 folds. After this procedure  each training instance x(j) will be evaluated by all hi’s. Note
that the classiﬁers used in the test stage are trained on the entire training set. Although for different
training instances x(j) and x(k) from different folds and a test instance x  m(j)
and mi are
obtained using different hi’s  our experimental results conﬁrmed that their empirical distributions
are close enough to achieve good performance.
Standard Multiclass Problems from UCI Repository. The ﬁrst set of experiments are done on
four standard multiclass problems from the UCI machine learning repository [25]: vowel (speech
recognition  11 classes)  letter (optical character recognition  26 classes)  satimage (pixel-based clas-
siﬁcation/segmentation on satellite images  6 classes) and pendigits (hand written digits recognition 

  m(k)

2

i

i

6

10 classes). We used the same training/test split as speciﬁed in the UCI repository. For each dataset 
there is only one type of feature  so it will be computed at the ﬁrst step no matter which classiﬁer
is selected. After that  all classiﬁers have the same complexity  so the results will be independent
of the τ parameter in the deﬁnition of value of classiﬁer (1). For the baselines  we have one-vs-one
with max win  one-vs-all  DAGSVM [27] and a tree-based method [13]. These methods vary both
in terms of what set of classiﬁers they use and how those classiﬁers are evaluated and combined.
To evaluate the effectiveness of our classiﬁer selection scheme  we introduce another baseline that
selects classiﬁers randomly  for which we repeated the experiments for 10 times and the average and
one standard deviation are reported. We compare different methods in terms of both the classiﬁca-
tion accuracy and the number of evaluated classiﬁers. For our algorithm and the random selection
baseline  we show the accuracy over iterations as well. Since in our framework the number of it-
erations (classiﬁers) needed varies over instances due to early stopping  the maximum number of
iterations shown is deﬁned as the mean plus one standard derivation of the number of classiﬁer
evaluations of all test instances. In addition  for the tree-based method  the number of evaluated
classiﬁers is the mean over all test instances.

Figure 1 shows a set of results. As can be seen  our method can achieve comparable or higher
accuracy than traditional methods. In fact  we achieved the best accuracy on three datasets and
the gains over the runner-up methods are 0.2%  5.2%  8.2% for satimage  vowel  and letter datasets
respectively. We think the statistical gain might come from two facts: (i) we are performing instance-
speciﬁc “feature selection” to only consider those most informative classiﬁers; (ii) another layer of
probabilistic model is used to combine the classiﬁers instead of the uniform voting of classiﬁers used
by many traditional methods. In terms of the number of evaluated classiﬁers  our active scheme is
very effective: the mean number of classiﬁer evaluations for 6-class  10-class  11-class and 26-class
problems are 4.50  3.22  6.15 and 7.72. Although the tree-based method can also use a few number
of classiﬁers  sometimes it suffers from a signiﬁcant drop in accuracy like on the vowel and letter
datasets. Furthermore  compared to the random selection scheme  our method can effectively select
more informative classiﬁers resulting in faster convergence to a certain classiﬁcation accuracy.

The performance gain of our method is not free. To maintain a belief over the class variable Y
and to dynamically select classiﬁers with high value  we have introduced additional computational
costs  i.e.  estimating conditional distributions and computing the value of classiﬁers. For example 
this additional cost is around 10ms for satimage  however  evaluating a linear classiﬁer only takes
less than 1ms due to very low feature dimension  so the actual running time of the active scheme
is higher than one-vs-one. Therefore  our method will have a real computational advantage only
if the cost of evaluating the classiﬁers is higher than the cost of our probabilistic inference. We
demonstrate such beneﬁt of our method in the context of multiple high dimensional features below.
Scene Recognition. We test our active classiﬁcation on a benchmark scene recognition dataset
Scene15 [20]. It has 15 scene classes and 4485 images in total. Following the protocol used in
[20  21]  100 images per class are randomly sampled for training and the remaining 2985 for test.

model

all features
best feature OB [21]
fastest feature GIST [26]
ours τ = 25
ours τ = 100
ours τ = 600

accuracy

feature cost
(# of features)
86.40% 52.645s (184)
83.38% 6.20s
72.70% 0.399s
86.26% 1.718s (5.62)
86.77% 6.573s (4.71)
88.11% 19.821s (4.46)

classiﬁer
cost
0.426s
0.024s
0.0002s
0.010s
0.014s
0.031s

scheduling
cost
0
0
0
0.141s
0.116s
0.094s

total
running time
53.071s
6.224s
0.3992s
1.869s (28.4x)
6.703s (7.9x)
19.946s (2.7x)

Table 1: Detailed performance comparisons on Scene15 dataset with various feature types. For our
methods  we show the speedup factors with respective to using all the features in a static way.
We consider various types of features  since as shown in [33]  the classiﬁcation accuracy can be
signiﬁcantly improved by combining multiple features but at a high computational cost. Our feature
set includes 7 features from [33]  including GIST  spatial HOG  dense SIFT  Local Binary Pattern 
self-similarity  texton histogram  geometry speciﬁc histograms (please refer to [33] for details)  and
another recently proposed high-level image feature Object Bank [21]. The basic idea of Object
Bank is to use the responses of various object detectors as the feature. The current release of the
code from the authors selected 177 object detectors  each of which outputs a feature vector φi with

7

dimension 252. These individual vectors are concatenated together to form the ﬁnal feature vector
Instead of treating Φ as an undecomposable single feature
Φ = [φ1; φ2; . . . ; φ177] ∈ R44 604.
vector  we can think of it as a collection of 177 different features {φi}177
i=1. Therefore  our feature
pool consists of 184 features in total. Their computational costs vary from 0.035 to 13.796 seconds 
with the accuracy from 54% to 83%. One traditional way to combine these features is through
multiple kernel learning. Speciﬁcally  we take the average of individual kernels constructed based
on individual features  and train a one-vs-all SVM using the joint average kernel. Surprisingly  this
simple average kernel performs comparably with learning the weights to combine them [12].

For our active classiﬁcation  we will not compute all features at the beginning of the evaluation
process  but will only compute a component φi when a classiﬁer h based on it is selected. We
will cache all evaluated φi’s  so different classiﬁers sharing the same φi will not induce repeated
computation of the common φi. We decompose the computational costs per instance into three
parts: (1) the feature cost  which is the time spent on computing the features; (2) the classiﬁer cost 
which is the time spent on evaluating the function value of the classiﬁers; (3) the scheduling cost 
which is the time spent on selecting the classiﬁers using our method. To demonstrate the trade-off
between the accuracy and computational cost in the deﬁnition of value of classiﬁer  we run multiple
experiments with various τ’s.

0.9

0.85

0.8

 

0.75

results on scene15 dataset

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
 
t
s
e
t

The results are shown in Table 1. We also report
comparisons to the best individual features in terms
of either accuracy or speed (the reported accuracy
is the best of one-vs-one and one-vs-all). As can
be seen  combining all features using the traditional
method indeed improves the accuracy signiﬁcantly
over those individual features  but at an expensive
computational cost. However  using active classiﬁ-
cation  to achieve similar accuracy as the baseline
of all features  we can get 28.4x speedup (τ = 25).
Note that at this conﬁguration  our method is faster
than the state-of-the-art individual feature [21]  and
is also 2.8% better in accuracy. Furthermore  if we
put more emphasis on the accuracy  we can get the
best accuracy 88.11% when τ = 600.
To further test the effectiveness of our active selec-
tion scheme  we compare with another baseline that
sequentially adds one feature at a time from a ﬁltered
pool of features. Speciﬁcally  we ﬁrst rank the individual features based on their classiﬁcation accu-
racy  and only consider the top 80 features (using 80 features achieves essentially the same accuracy
as using 184 features). Given this selected pool  we arrange the features in order of increasing com-
putational complexity  and then train a classiﬁer based on the top N features for all values of N from
1 to 80. As shown in Figure 2  our active scheme is one order of magnitude faster than the baseline
given the same level of accuracy.

Figure 2: Classiﬁcation accuracy versus run-
ning time for the baseline  active classiﬁca-
tion  and various individual features.

sequentially adding features
active classification
GIST
LBP
spatial HOG
Object Bank
dense SIFT

20

15
35
running time (seconds)

25

30

0.65

 
0

40

45

50

5

10

0.7

6 Conclusion and Future Work

In this paper  we presented an active classiﬁcation process based on the value of classiﬁer. We ap-
plied this active scheme in the context of multiclass classiﬁcation  and achieved comparable and
even higher classiﬁcation accuracy with signiﬁcant computational savings compared to traditional
static methods. One interesting future direction is to estimate the value of features instead of individ-
ual classiﬁers. This is particularly important when computing the feature is much more expensive
than evaluating the function value of classiﬁers  which is often the case. Once a feature has been
computed  a set of classiﬁers that are built on it will be cheap to evaluate. Therefore  predicting the
value of the feature (equivalent to the joint value of multiple classiﬁers sharing the same feature) can
potentially lead to more computationally efﬁcient classiﬁcation process.
Acknowledgment. This work was supported by the NSF under grant No. RI-0917151  the Ofﬁce of
Naval Research MURI grant N00014-10-10933  and the Boeing company. We thank Pawan Kumar
and the reviewers for helpful feedbacks.

8

References
[1] E. L. Allwein  R. E. Schapire  and Y. Singer. Reducing multiclass to binary: a unifying approach for

margin classiﬁers. J. Mach. Learn. Res.  1:113–141  2001.

[2] A. Angelova  L. Matthies  D. Helmick  and P. Perona. Fast terrain classiﬁcation using variable-length

representation for autonomous navigation. CVPR  2007.

[3] S. Bengio  J. Weston  and D. Grangier. Label embedding trees for large multiclass task. In NIPS  2010.
[4] L. Breiman. Random forests. In Machine Learning  pages 5–32  2001.
[5] X. Chai  L. Deng  and Q. Yang. Test-cost sensitive naive bayes classiﬁcation. In ICDM  2004.
[6] W. S. Cleveland and S. J. Devlin. Locally weighted regression: An approach to regression analysis by

local ﬁtting. Journal of the American Statistical Association  83:596–610  1988.

[7] D.A. Cohn  Zoubin Ghahramani  and M.I. Jordan. Active learning with statistical models. CoRR 

[8] J. Deng  A.C. Berg  K. Li  and L. Fei-Fei. What does classifying more than 10 000 image categories tell

[9] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. J.

cs.AI/9603104  1996.

us? In ECCV10  pages V: 71–84  2010.

of A. I. Res.  2:263–286  1995.

[10] Y. Freud. Boosting a weak learning algorithm by majority. In Computational Learning Theory  1995.
[11] Jerome H. Friedman. Another approach to polychotomous classiﬁcation. Technical report  Department

of Statistics  Stanford University  1996.

[12] P.V. Gehler and S. Nowozin. On feature combination for multiclass object classiﬁcation. In ICCV  2009.
[13] G. Grifﬁn and P. Perona. Learning and using taxonomies for fast visual categorization. In CVPR  2008.
[14] V. Guruswami and A. Sahai. Multiclass learning  boosting  and error-correcting codes. In Proc. of the

Twelfth Annual Conf. on Computational Learning Theory  1999.

[15] T. Hastie  R. Tibshirani  and J. H. Friedman. The elements of statistical learning: data mining  inference 

and prediction. 2009.

2009.

[16] R. A. Howard. Information value theory. IEEE Trans. on Systems Science and Cybernetics  1966.
[17] R. A. Howard. Decision analysis: Practice and promise. Management Science  1988.
[18] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press 

[19] A. Krause and C. Guestrin. Optimal value of information in graphical models. Journal of Artiﬁcial

[20] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing

Intelligence Research (JAIR)  35:557–591  2009.

natural scene categories. In CVPR  2006.

[21] L.-J. Li  H. Su  E.P. Xing  and L. Fei-Fei. Object bank: A high-level image representation for scene

classiﬁcation and semantic feature sparsiﬁcation. In NIPS  2010.

[22] D. V. Lindley. On a Measure of the Information Provided by an Experiment. The Annals of Mathematical

Statistics  27(4):986–1005  1956.

[23] D.G. Lowe. Object recognition from local scale-invariant features. In ICCV  1999.
[24] V.S. Mookerjee and M.V. Mannino. Sequential decision models for expert system optimization. IEEE

Trans. on Knowledge & Data Engineering  (5):675.

[25] D.J. Newman  S. Hettich  C.L. Blake  and C.J. Merz. Uci repository of machine learning databases  1998.
[26] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the

spatial envelope. IJCV  2001.

[27] J.C. Platt  N. Cristianini  and J. Shawe-taylor. Large margin dags for multiclass classiﬁcation. In NIPS 

[28] M.J. Saberian and N. Vasconcelos. Boosting classiﬁer cascades. In NIPS  2010.
[29] Robert E. Schapire. Using output codes to boost multiclass learing problems. In ICML  1997.
[30] G. A. Schwing  C. Zach  Zheng Y.  and M. Pollefeys. Adaptive random forest - how many “experts” to

ask before making a decision? In CVPR  2011.

[31] A. Vedaldi  V. Gulshan  M. Varma  and A. Zisserman. Multiple kernels for object detection. In ICCV 

2000.

2009.

[32] P. Viola and M. Jones. Robust Real-time Object Detection. IJCV  2002.
[33] J.X. Xiao  J. Hays  K.A. Ehinger  A. Oliva  and A.B. Torralba. Sun database: Large-scale scene recogni-

[34] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML 

tion from abbey to zoo. In CVPR  2010.

pages 412–420  1997.

9

,Qiang Liu
Alexander Ihler
Kristof Schütt
Pieter-Jan Kindermans
Huziel Enoc Sauceda Felix
Stefan Chmiela
Alexandre Tkatchenko
Klaus-Robert Müller