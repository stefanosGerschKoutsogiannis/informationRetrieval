2012,Majorization for CRFs and Latent Likelihoods,The partition function plays a key role in probabilistic modeling including conditional random fields  graphical models  and maximum likelihood estimation. To optimize partition functions  this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Such bounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems  low-rank versions of the bound are provided and outperform LBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods.,Majorization for CRFs and Latent Likelihoods

Tony Jebara

Department of Computer Science

Columbia University

Anna Choromanska

Department of Electrical Engineering

Columbia University

jebara@cs.columbia.edu

aec2163@columbia.edu

Abstract

The partition function plays a key role in probabilistic modeling including condi-
tional random ﬁelds  graphical models  and maximum likelihood estimation. To
optimize partition functions  this article introduces a quadratic variational upper
bound. This inequality facilitates majorization methods: optimization of com-
plicated functions through the iterative solution of simpler sub-problems. Such
bounds remain efﬁcient to compute even when the partition function involves
a graphical model (with small tree-width) or in latent likelihood settings. For
large-scale problems  low-rank versions of the bound are provided and outper-
form LBFGS as well as ﬁrst-order methods. Several learning applications are
shown and reduce to fast and convergent update rules. Experimental results show
advantages over state-of-the-art optimization methods.

1 Introduction
The estimation of probability density functions over sets of random variables is a central problem
in learning. Estimation often requires minimizing the partition function as is the case in conditional
random ﬁelds (CRFs) and log-linear models [1  2]. Training these models was traditionally done
via iterative scaling and bound-majorization methods [3  4  5  6  1] which achieved monotonic con-
vergence. These approaches were later surpassed by faster ﬁrst-order methods [7  8  9] and then
second-order methods such as LBFGS [10  11  12]. This article revisits majorization and repairs
its slow convergence by proposing a tighter bound on the log-partition function. The improved ma-
jorization outperforms state-of-the-art optimization tools and admits multiple versatile extensions.
Many decomposition methods for conditional random ﬁelds and structured prediction have sought
to render the learning and prediction problems more manageable [13  14  15]. Our decomposi-
tion  however  hinges on bounding and majorization: decomposing an optimization of complicated
functions through the iterative solution of simpler sub-problems [16  17]. A tighter bound provides
convergent monotonic minimization while outperforming ﬁrst- and second-order methods in prac-
tice1. The bound applies to graphical models [18]  latent variable situations [17  19  20  21] as well
as high-dimensional settings [10]. It also accommodates convex constraints on the parameter space.
This article is organized as follows. Section 2 presents the bound and Section 3 uses it for ma-
jorization in CRFs. Extensions to latent likelihood are shown in Section 4. The bound is extended
to graphical models in Section 5 and high dimensional problems in Section 6. Section 7 provides
experiments and Section 8 concludes. The Supplement contains proofs and additional results.
2 Partition Function Bound
Consider a log-linear density model over discrete y ∈ Ω

p(y|θ) =

h(y) exp

θ

⊤

f (y)

1

Z(θ)

(

)

1Recall that some second-order methods like Newton-Raphson are not monotonic and may even fail to

converge for convex cost functions [4] unless  of course  line searches are used.

1

∑

which is parametrized by a vector θ ∈ Rd of dimensionality d ∈ N. Here  f : Ω 7→ Rd is
any vector-valued function mapping an input y to some arbitrary vector. The prior h : Ω 7→ R+
is a ﬁxed non-negative measure. The partition function Z(θ) is a scalar that ensures that p(y|θ)
f (y)). Assume that the number of conﬁgurations of y is
normalizes  i.e. Z(θ) =
|Ω| = n and is ﬁnite2. The partition function is clearly log-convex in θ and a linear lower-bound
is given via Jensen’s inequality. This article contributes an analogous quadratic upper-bound on the
partition function. Algorithm 1 computes3 the bound’s parameters and Theorem 1 shows the precise
guarantee it provides.

y h(y) exp(θ

⊤

Algorithm 1 ComputeBound
Input Parameters ˜θ  f (y)  h(y) ∀y ∈ Ω
Init z → 0+  µ = 0  (cid:6) = zI
For each y ∈ Ω {
⊤
α = h(y) exp( ˜θ
l = f (y) − µ
(cid:6) + = tanh( 1
µ + = α
z+α l
}
z + = α
Output z  µ  (cid:6)

2 log(α/z))

2 log(α/z)

f (y))

⊤

ll

∑

2 (θ − ˜θ)

⊤

(cid:6)(θ − ˜θ) + (θ − ˜θ)

⊤
µ) upper-

Theorem 1 Algorithm 1 ﬁnds z  µ  (cid:6) such that z exp( 1
bounds Z(θ) =

f (y)) for any θ  ˜θ  f (y) ∈ Rd and h(y) ∈ R+ for all y ∈ Ω.

⊤

y h(y) exp(θ

Proof 1 (Sketch  See Supplement for Formal Proof) Recall the bound log(eθ + e
1). Tilt the bound to handle log(h1eθ
Obtain a multivariate variant log(eθ
Add an additional exponential term to get log(h1eθ
to extend to n elements in the summation.

f1 + h2eθ

f2 + h3eθ

1 + e

−θ

⊤

⊤

⊤

⊤

⊤

−θ) ≤ cθ2 [22].
f2 ).
f1 + h2eθ
f3 ). Iterate the last step

⊤

⊤

∑

⊤
y h(y) exp( ˜θ

The bound improves previous inequalities and its proof is in the Supplement. It tightens [4  19] since
it avoids wasteful curvature tests (it uses duality theory to compare the bound and the optimized
function rather than compare their Hessians). It generalizes [22] which only holds for n = 2 and
h(y) constant; it generalizes [23] which only handles a simpliﬁed one-dimensional case. The bound
is computed using Algorithm 1 by iterating over the y variables (“for each y ∈ Ω”) according to an
arbitrary ordering via the bijective function π : Ω 7→ {1  . . .   n} which deﬁnes i = π(y). The order
in which we enumerate over Ω slightly varies the (cid:6) in the bound (but not the µ and z) when |Ω| >
2. However  we empirically investigated the inﬂuence of various orderings on bound performance
(in all the experiments presented in Section 7) and noticed no signiﬁcant effect across ordering
⊤ with µ and z
schemes. Recall that choosing (cid:6) =
as in Algorithm 1 yields the second-order Taylor approximation (the Hessian) of the log-partition
function. Algorithm 1 replaces a sum of log-linear models with a single log-quadratic model which
makes monotonic majorization straightforward. The ﬁgure inside Algorithm 1 depicts the bound on
log Z(θ) for various choices of ˜θ. If there are no constraints on the parameters (i.e. any θ ∈ Rd
is admissible)  a simple closed-form iterative update rule emerges: ˜θ ← ˜θ − (cid:6)
−1µ. Alternatively 
if θ must satisfy linear (convex) constraints it is straightforward to compute an update by solving a
quadratic (convex) program. This update rule is interleaved with the bound computation.
3 Conditional Random Fields and Log-Linear Models
The partition function arises naturally in maximum entropy estimation or minimum relative entropy
estimation (cf. Supplement) as well as in conditional extensions of the maximum entropy paradigm
where the model is conditioned on an observed input. Such models are known as conditional random
ﬁelds and have been useful for structured prediction problems [1  24]. CRFs are given a data-set
{(x1  y1)  . . .   (xt  yt)} of independent identically-distributed (iid) input-output pairs where yj is

f (y))(f (y) − µ)(f (y) − µ)

2 Here  assume n is enumerable. Later  for larger spaces use O(n) to denote the time to compute Z.
3By continuity  take tanh( 1

2 log(1))/(2 log(1)) = 1

4 and limz!0+ tanh( 1

2 log(α/z))/(2 log(α/z)) = 0.

2

−50500.050.10.150.20.250.30.35θlog(Z) and Boundsthe observed sample in a (discrete) space Ωj conditioned on the observed input xj. A CRF deﬁnes
a distribution over all y ∈ Ωj (of which yj is a single element) as the log-linear model

∑

p(y|xj  θ) =

1

hxj (y) exp(θ

fxj (y))

⊤

⊤

y∈Ωj

hxj (y) exp(θ

Zxj (θ)
fxj (y)). For the j’th training pair  we are given a non-
where Zxj (θ) =
negative function hxj (y) ∈ R+ and a vector-valued function fxj (y) ∈ Rd deﬁned over the domain
y ∈ Ωj. In this section  for simplicity  assume n = maxt
|. Each partition function Zxj (θ) is
|Ωyj
a function of θ. The parameter θ for CRFs is estimated by maximizing the regularized conditional
∥θ∥2 where λ ∈ R+ is a regularizer set
log-likelihood4 or log-posterior:
using prior knowledge or cross-validation. Rewriting gives the objective of interest

∑
j=1 log p(yj|xj  θ) − tλ
t∑

j=1

2

t

⊤

fxj (yj) − tλ

2

+ θ

∥θ∥2.

(1)

J(θ) =

log

j=1

hxj (yj)
Zxj (θ)

If prior knowledge (or constraints) restrict the solution vector to a convex hull (cid:3)  the maximization
problem becomes arg maxθ∈(cid:3) J(θ).
Algorithm 2 proposes a method for maximizing the regularized conditional likelihood J(θ) or 
equivalently minimizing the partition function Z(θ). It solves the problem in Equation 1 subject
to convex constraints by interleaving the quadratic bound with a quadratic programming procedure.
Theorem 2 establishes the convergence of the algorithm and the proof is in the Supplement.

∑

Algorithm 2 ConstrainedMaximization
0: Input xj  yj and functions hxj   fxj for j = 1  . . .   t  regularizer λ ∈ R+ and convex hull (cid:3) ⊆ Rd
1: Initialize θ0 anywhere inside (cid:3) and set ˜θ = θ0

While not converged

2: For j = 1  . . .   t

∑

j

∑

n−1
i=1

n−1
i=1

∑

)

i−1

Get µj  (cid:6)j from hxj   fxj   ˜θ via Algorithm 1

2 (θ − ˜θ)
((cid:6)j +λI)(θ − ˜θ) +
⊤

1

3: Set ˜θ =arg minθ∈(cid:3)
4: Output ˆθ = ˜θ
⌈
|Ωj| ≤ n  Algorithm 2
Theorem 2 For any θ0 ∈ (cid:3)  all ∥fxj (y)∥ ≤ r and all
outputs a ˆθ such that J( ˆθ) − J(θ0) ≥ (1 − ϵ) maxθ∈(cid:3)(J(θ) − J(θ0)) in more than
)

iterations.

)⌉

∑

tanh(log(i)/2)

n−1
i=1

1 + λ

(

(

2r2 (

/ log

(

)

j θ

−1

log(i)

log

⊤

(µj − fxj (yj) + λ ˜θ)

1
ϵ

n

=

tanh(log(i)/2)

log n

log(i)

(i+1) log(i) is the logarithmic integral which is O

The series
asymptotically [26]. The next sections show how to handle hidden variables in the learning problem 
exploit graphical modeling  and further accelerate the underlying algorithms.
4 Latent Conditional Likelihood
Section 3 showed how the partition function is useful for maximum conditional likelihood problems
involving CRFs. In this section  maximum conditional likelihood is extended to the setting where
some variables are latent. Latent models may provide more ﬂexibility than fully observable models
[21  27  28]. For instance  hidden conditional random ﬁelds were shown to outperform generative
hidden-state and discriminative fully-observable models [21].
Consider the latent setting where we are given t iid samples x1  . . .   xt from some unknown distri-
bution ¯p(x) and t corresponding samples y1  . . .   yt drawn from identical conditional distributions
¯p(y|x1)  . . .   ¯p(y|xt) respectively. Assume that the true generating distributions ¯p(x) and ¯p(y|x)
are unknown. Therefore  we aim to estimate a conditional distribution p(y|x) from some set of hy-
potheses that achieves high conditional likelihood given the data-set D = {(x1  y1)  . . .   (xt  yt)}.
4Alternatively  variational Bayesian approaches can be used instead of maximum likelihood via expectation
propagation (EP) or power EP [25]. These  however  assume Gaussian posterior distributions over parameters 
require approximations  are computationally expensive and are not necessarily more efﬁcient than BFGS.

3

.

(2)

∑
We will select this conditional distribution by assuming it emerges from a conditioned joint distri-
bution over x and y as well as a hidden variable m which is being marginalized as p(y|x  Θ) =
∑
m p(x y m|Θ)
y;m p(x y m|Θ) . Here m ∈ Ωm represents a discrete hidden variable  x ∈ Ωx is an input and
y ∈ Ωy is a discrete output variable. The parameter Θ contains all parameters that explore the
function class of such conditional distributions. The latent likelihood of the data L(Θ) = p(D|Θ)
subsumes Equation 1 and is the new objective of interest

t∏

p(yj|xj  Θ) =

L(Θ) =

j=1

t∏

j=1

(

∑
∑
m p(xj  yj  m|Θ)
y m p(xj  y  m|Θ)
)

A good choice of the parameters is one that achieves a large conditional likelihood value (or poste-
rior) on the data set D. Next  assume that each p(x|y  m  Θ) is an exponential family distribution

p(x|y  m  Θ) = h(x) exp

y mϕ(x) − a(θy m)
⊤

θ

(

πy;m∑

where each conditional is speciﬁed by a function h : Ωx 7→ R+ and a feature mapping ϕ : Ωx 7→ Rd
which are then used to derive the normalizer a : Rd 7→ R+. A parameter θy m ∈ Rd se-
lects a speciﬁc distribution. Multiply each exponential family term by an unknown marginal dis-
tribution called the mixing proportions p(y  m|π) =
. This is parametrized by an un-
known parameter π = {πy m} ∀y  m where πy m ∈ [0 ∞). Finally  the collection of all pa-
rameters is Θ = {θy m  πy m} ∀y  m. Thus  we have the complete likelihood p(x  y  m|Θ) =
∑
Insert this expression into Equation 2 and remove con-
stant factors that appear in both denominator and numerator. Apply the change of variables
)
∑
exp(νy m) = πy m exp(−a(θy m)) and rewrite the objective as a function5 of a vector θ:
∑
m exp
fj yj  m
y m exp (θ⊤fj y m)

y mϕ(x) − a(θy m)
⊤
(
t∏
(

⊤
yj  mϕ(xj) + νyj  m
θ⊤
y mϕ(xj) + νy m

)
) =

m exp
y m exp

∑
∑

πy;mh(x)
y;m πy;m

t∏

L(Θ) =

y;m πy;m

)

(

exp

⊤

θ

θ

θ

.

.

j=1

j=1

··· θ

⊤
|Ωy| |Ωm| ν|Ωy| |Ωm|]

⊤
1 2 ν1 2
1]δ[(ˆy  ˆm)=(1 1)] ··· [ϕ(xj )
⊤

The last equality emerges by rearranging all Θ parameters as a vector θ ∈ R|Ωy||Ωm|(d+1) equal to
⊤ and introducing fj ˆy  ˆm ∈ R|Ωy||Ωm|(d+1) deﬁned
⊤
1 1 ν1 1 θ
[θ
⊤ is
⊤
as [[ϕ(xj )
1]
positioned appropriately in the longer fj ˆy  ˆm vector which is elsewhere zero). We will now ﬁnd a
variational lower bound on L(θ) ≥ Q(θ  ˜θ) which is tight when θ = ˜θ such that L( ˜θ) = Q( ˜θ  ˜θ).
We proceed by bounding each numerator and each denominator in the product over j = 1  . . .   t.
Apply Jensen’s inequality to lower bound each numerator term as

⊤
⊤ (thus the feature vector [ϕ(xj)

1]δ[(ˆy  ˆm)=(|Ωy| |Ωm|)]]

where ηj m = (e

˜θ

fj;yj ;m)/(

). Algorithm 1 then bounds the denominator

exp

θ

fj yj  m

m ηj;m log ηj;m

(

∑
∑

m
⊤

exp

y m

⊤

∑
(

θ

m′ e
⊤

˜θ

⊤

fj;yj ;m′

) ≥ eθ
⊤∑
m ηj;mfj;yj ;m−∑
) ≤ zje
j=1(µj −∑
∑

2 (θ− ˜θ)
⊤

1

t

fj y m

(cid:6)j (θ− ˜θ)+(θ− ˜θ)
⊤

µj .

The overall lower bound on the likelihood is then
− 1
2 (θ− ˜θ)

Q(θ  ˜θ) = L( ˜θ)e

⊤ ˜(cid:6)(θ− ˜θ)−(θ− ˜θ)
⊤

˜µ

∑

t

j=1 (cid:6)j and ˜µ =

where ˜(cid:6) =
m ηj mfj yj  m). The right hand side is simply an
exponentiated quadratic function in θ which is easy to maximize. This yields an iterative scheme
similar to Algorithm 2 for monotonically maximizing latent conditional likelihood.
5 Graphical Models for Large n
The bounds in the previous sections are straightforward to compute when Ω is small. However 
for graphical models  enumerating over Ω can be daunting. This section provides faster algorithms

5It is now easy to regularize L((cid:18)) by adding − t(cid:21)

2

∥(cid:18)∥2.

4

that recover the bound efﬁciently for graphical models of bounded tree-width. A graphical model
represents the factorization of a probability density function. This article will consider the factor
graph notation of a graphical model. A factor graph is a bipartite graph G = (V  W  E) with variable
vertices V = {1  . . .   k}  factor vertices W = {1  . . .   m} and a set of edges E between V and W .
In addition  deﬁne a set of random variables Y = {y1  . . .   yk} each associated with the elements of
V and a set of non-negative scalar functions Ψ = {ψ1  . . .   ψm} each associated with the elements
of W . The factor graph implies that p(Y ) factorizes as p(y1  . . .   yk) = 1
c∈W ψc(Yc) where
Z
Z is a normalizing partition function (the dependence on parameters is suppressed here) and Yc is
a subset of the random variables that are associated with the neighbors of node c. In other words 
Yc = {yi|i ∈ Ne(c)} where Ne(c) is the set of vertices that are neighbors of c.
Inference in
graphical models requires the evaluation and the optimization of Z. These computations can be
NP-hard in general yet are efﬁcient when G satisﬁes certain properties (low tree-width). Consider
a log-linear model (a function class) indexed by a parameter θ ∈ (cid:3) in a convex hull (cid:3) ⊆ Rd as
follows

∏

p(Y |θ) =

∑

∏

(

)

hc(Yc) exp

⊤

θ

fc(Yc)

∏

c∈W
⊤

Z(θ)

1

(

)

Y

θ

fc(Yc)

c∈W hc(Yc) exp

where Z(θ) =
. The model is deﬁned by a set of vector-valued
functions fc(Yc) ∈ Rd and scalar-valued functions hc(Yc) ∈ R+. Choosing a function from the
function class hinges on estimating θ by optimizing Z(θ). However  Algorithm 1 may be inappli-
cable due to the large number of conﬁgurations in Y . Instead  consider a more efﬁcient surrogate
algorithm which computes the same bound parameters by efﬁciently exploiting the factorization of
the graphical model. This is possible since exponentiated quadratics are closed under multiplication
and the required bound computations distribute nicely across decomposable graphical models.

Algorithm 3 JunctionTreeBound
Input Reverse-topological tree T with c = 1  . . .   m factors hc(Yc) exp( ˜θ
⊤
For c = 1  . . .   m

fc(Yc)) and ˜θ ∈ Rd

If (c < m) {Yboth = Yc ∩ Ypa(c)  Ysolo = Yc \ Ypa(c)}
Else {Yboth ={}  Ysolo = Yc}
For each u ∈ Yboth

{ Initialize zc|x ← 0+  µc|x = 0  (cid:6)c|x = zc|xI

For each v ∈ Ysolo
w = u ⊗ v;
{
∑
⊤
αw = hc(w)e ˜θ

∏

fc(w)

(cid:6)c|u =

b∈ch(c)(cid:6)b|w+

∑

b∈ch(c)zb|w;
tanh( 1
2 log(
αw
zc|u

2 log(

αw
zc|u
)

lw = fc(w) − µc|u +
⊤
w; µc|u = αw

lwl

))

zc|u+αw

lw;

b∈ch(c) µb|w;

zc|u = αw; }}

Output Bound as z = zm  µ = µm  (cid:6) = (cid:6)m

Begin by assuming that the graphical model in question is a junction tree and satisﬁes the running
intersection property [18]. In Algorithm 3 (the Supplement provides a proof of its correctness)  take
ch(c) to be the set of children-cliques of clique c and pa(c) to be the parent of c. Note that the
algorithm enumerates over u ∈ Ypa(c) ∩ Yc and v ∈ Yc \ Ypa(c). The algorithm stores a quadratic
bound for each conﬁguration of u (where u is the set of variables in common across both clique c
and its parent). It then forms the bound by summing over v ∈ Yc \ Ypa(c)  each conﬁguration of
each variable a clique c has that is not shared with its parent clique. The algorithm also collects
precomputed bounds from children of c. Also deﬁne w = u ⊗ v ∈ Yc as the conjunction of both
indexing variables u and v. Thus  the two inner for loops enumerate over all conﬁgurations w ∈ Yc
of each clique. Note that w is used to query the children b ∈ ch(c) of a clique c to report their bound
parameters zb|w  µb|w  (cid:6)b|w. This is done for each conﬁguration w of the clique c. Note  however 
that not every variable in clique c is present in each child b so only the variables in w that intersect Yb
are relevant in indexing the parameters zb|w  µb|w  (cid:6)b|w and the remaining variables do not change
the values of zb|w  µb|w  (cid:6)b|w.
Algorithm 3 is efﬁcient in the sense that computations involve enumerating over all conﬁgurations
of each clique in the junction tree rather than over all conﬁgurations of Y . This shows that the

5

∑

∑

c

c

(cid:12)(cid:12)(cid:12)

|Yc|) rather than O(|Ω|) as in Algorithm 1. Thus  for estimating the
computation involved is O(
|Yc| for the graphical
computational efﬁciency of various algorithms in this article  take n =
model case rather than n = |Ω|. Algorithm 3 is a simple extension of the known recursions that
are used to compute the partition function and its gradient vector. Thus  in addition to the (cid:6) matrix
which represents the curvature of the bound  Algorithm 3 is recovering the partition function value
z and the gradient since µ = ∂ log Z(θ)

θ= ˜θ
6 Low-Rank Bounds for Large d
In many realistic situations  the dimensionality d is large and this prevents the storage and inver-
sion of the matrix (cid:6). We next present a low-rank extension that can be applied to any of the
algorithms presented so far. As an example  consider Algorithm 4 which is a low-rank incar-
nation of Algorithm 2. Each iteration of Algorithm 2 requires O(tnd2 + d3) time since step 2
computes several (cid:6)j ∈ Rd×d matrices and 3 performs inversion.
Instead  the new algorithm
provides a low-rank version of the bound which still majorizes the log-partition function but re-
quires only ˜O(tnd) complexity (putting it on par with LBFGS). First  note that step 3 in Algo-

∂θ

.

Algorithm 4 LowRankBound
Input Parameter ˜θ  regularizer λ ∈ R+  model ft(y) ∈ Rd and ht(y) ∈ R+ and rank k ∈ N
Initialize S = 0 ∈ Rk×k  V = orthonormal ∈ Rk×d  D = tλI ∈ diag(Rd×d)
For each t { Set z → 0+; µ = 0;

For each y{

√

tanh( 1

⊤

⊤

} }

ft(y); r =

α
z ))

√
V(i ·); r = r − p(i)V(i ·);

(ft(y) − µ) ;

2 log(
α
z )

2 log(

else
µ + = α

{ D = D + 1

⊤|r|diag(|r|); } }

−1u where u = tλ ˜θ +

z+α (ft(y) − µ); z + = α;

AQ = svd(S); S ← A; V ← QV;

⊤
α = ht(y)e ˜θ
For i = 1  . . .   k : p(i) = r
For i = 1  . . .   k : For j = 1  . . .   k : S(i  j) = S(i  j) + p(i)p(j);
⊤
Q
s = [S(1  1)  . . .   S(k  k) ∥r∥2]
⊤
; ˜k = arg mini=1 ... k+1 s(i);
if (˜k ≤ k) { D = D + S(˜k  ˜k)1
⊤|V(j ·)| diag(|V(k ·)|);
S(˜k  ˜k) = ∥r∥2; r = ∥r∥−1r; V(k ·) = r; }
∑
Output S ∈ diag(Rk×k)  V ∈ Rk×d  D ∈ diag(Rd×d)
j=1 µj − fxj (yj). Clearly  Al-
rithm 2 can be written as ˜θ = ˜θ − (cid:6)
gorithm 1 can recover u by only computing µj for j = 1  . . .   t and skipping all steps involving
matrices. This merely requires O(tnd) work. Second  we store (cid:6) using a low-rank represen-
SV + D where V ∈ Rk×d is orthonormal  S ∈ Rk×k is positive semi-deﬁnite  and
tation V
D ∈ Rd×d is non-negative diagonal. Rather than increment the matrix by a rank one update of the
(fi − µi)  simply project ri onto each
form (cid:6)i = (cid:6)i−1 + rir
eigenvector in V and update matrix S and V via a singular value decomposition (O(k3) work).
After removing k such projections  the remaining residual from ri forms a new eigenvector ek+1
and its magnitude forms a new singular value. The resulting rank (k + 1) system is orthonormal
with (k + 1) singular values. We discard its smallest singular value and corresponding eigenvector
to revert back to an order k eigensystem. However  instead of merely discarding we can absorb
the smallest singular value and eigenvector into the D component by bounding the remaining outer-
product with a diagonal term. This provides a guaranteed overall upper bound in ˜O(tnd) (k is
assumed to be logarithmic with dimension d). Finally  to invert (cid:6)  we apply the Woodbury formula:
−1 which only requires O(k3) work. A proof of
−1 = D
(cid:6)
correctness for Algorithm 4 can be found in the Supplement.
7 Experiments
We ﬁrst focus on the logistic regression task and compare the performance of the bound (using
the low-rank Algorithm 2) with ﬁrst-order and second order methods such as LBFGS  conjugate
gradient (CG) and steepest descent (SD). We use 4 benchmark data-sets: the SRBCT and Tumors

⊤
i where ri =

−1 + VD

−1VD
)

−1 + D

−1V

⊤

−1V

⊤

√

tanh( 1

2 log(α/z))

2 log(α/z)

(S

t

6

data-sets from [29] as well as the Text and SecStr data-sets from http://olivier.chapelle.cc/ssl-
book/benchmarks.html. For all experiments in this section  the setup is as follows. Each data-set is
split into training (90%) and testing (10%) parts. All implementations are run on the same hardware
with C++ code. The termination criterion for all algorithms is a change in estimated parameter or
−6 (with a ceiling on the number of iterations of 106). Results are
function values smaller than 10
averaged over 10 random initializations close to 0. The regularization parameter λ  when used  was
chosen through crossvalidation. In Table 1 we report times in seconds and the number of iterations
for each algorithm (including LBFGS) to achieve the LBFGS termination solution modulo a small
−4). Table 1 also provides data-set sizes and regularization values. The ﬁrst 4
constant ϵ (set to 10
columns in Table 1 provide results for this experiment.

Data-set

Size

SRBCT
n = 4
t = 83

d = 9236
λ = 101
iter
42
43

Algorithm time
6.10
LBFGS
3246.83
7.27
18749.15
40.61 100 14840.66
3.67
1639.93

SD
CG

Bound

8

iter
8
53
42
4

Tumors
n = 26
t = 308

d = 390260

λ = 101
time

Text
n = 2

t = 1500
d = 23922
λ = 102
time
15.54
153.10
57.30
6.18

iter
7
69
23
3

SecStr
n = 2

t = 83679
d = 632
λ = 101
time
881.31
1490.51
667.67
27.97

iter
47
79
36
9

CoNLL
m = 9

t = 1000
d = 33615
λ = 101
time

25661.54
93821.72
88973.93
16445.93

iter
17
12
23
4

PennTree
m = 45
t = 1000
d = 14175
λ = 101
time

62848.08
156319.31
76332.39
27073.42

iter
7
12
18
2

Table 1: Time in seconds and iterations required to obtain within ϵ of the LBFGS solution (where
−4) for logistic regression problems (on SRBCT  Tumors  Text and SecStr data-sets where
ϵ = 10
n is the number of classes) and Markov CRF problems (on CoNLL and PennTree data-sets  where
m is the number of classes). Here  t is the total number of samples (training and testing)  d is the
dimensionality of the feature vector and λ is the cross-validated regularization setting.

Structured prediction problems are explored using two popular data-sets. The ﬁrst one contains
Spanish news wire articles from the a session of the CoNLL 2002 conference. This corpus involves
a named entity recognition problem and consists of sentences where each word is annotated with
one of m = 9 possible labels. The second task is from the PennTree Bank. This corpus involves a
tagging problem and consists of sentences where each word is labeled with one of m = 45 possible
parts-of-speech. A conditional random ﬁeld is estimated with a Markov chain structure to give
word labels a sequential dependence. The features describing the words are constructed as in [30].
Two last columns of Table 1 provide results for this experiment. We used the low-rank version of
Algorithm 3. In both experiments  the bound always remained fastest as indicated in bold.

Figure 1: Classiﬁcation boundaries using the bound and EM for a toy latent likelihood problem.

We next performed experiments with maximum latent conditional likelihood problems. We denote
by m the number of hidden variables. Due to the non-concavity of this objective  we are most in-
terested in ﬁnding good local maxima. We start with a simple toy experiment from [19] comparing
the bound to the expectation-maximization (EM) algorithm in the binary classiﬁcation problem pre-
sented on the left image of Figure 1. The model incorrectly uses only 2 Gaussians per class while
the data is generated using 8 Gaussians total. On Figure 1 we show the decision boundary obtained
using the bound (with m = 2) and EM. EM performs as well as random chance guessing while the
bound classiﬁes the data very well. The average test log-likelihood obtained by EM was -1.5e+06
while the bound obtained -21.8.

7

−50510152025−25−20−15−10−505Data−set−50510152025−25−20−15−10−505Bound−50510152025−25−20−15−10−505EMWe next compared the algorithms (the bound  Newton-Raphson  BFGS  CG and SD) in maximum
latent conditional likelihood problems on ﬁve benchmark data-sets. These included four UCI data-
sets6 (ion  bupa  hepatitis and wine) and the previously used SRBCT data-set. The feature mapping
used was ϕ(x) = x ∈ Rd which corresponds to a mixture Gaussian-gated logistic regressions
(obtained by conditioning a mixture of m Gaussians per class). We used a value of λ = 0 throughout
the latent experiments. We explored setting m ∈ {1  2  3  4}. Table 2 shows the testing latent log-
likelihood at convergence for m chosen through cross-validation (the Supplement contains a more
complete table). In bold  we show the algorithm that obtained the highest testing log-likelihood.
The bound is the best performer overall and ﬁnds better solutions in less time. Figure 2 depicts the
convergence on ion  hepatitis and SRBCT data sets.

Data-set
hepatitis wine SRBCT
Algorithm m = 3 m = 2 m = 2 m = 3 m = 4
-6.06
-5.61
-5.76
-5.54
-0.11

-21.78
-21.74
-21.81
-21.85
-19.95

-5.88
-5.56
-5.57
-5.95
-4.18

-5.28
-5.14
-4.84
-5.50
-4.40

-1.79
-1.37
-0.95
-0.71
-0.48

ion

bupa

BFGS

SD
CG

Newton
Bound

Table 2: Test log-likelihood at convergence for ion  bupa  hepatitis  wine and SRBCT data-sets.

Figure 2: Convergence of test latent log-likelihood on ion  hepatitis and SRBCT data-sets.

8 Discussion
A simple quadratic upper bound for the partition function of log-linear models was proposed and
makes majorization approaches competitive with state-of-the-art ﬁrst- and second-order optimiza-
tion methods. The bound is efﬁciently recoverable for graphical models and admits low-rank vari-
ants for high-dimensional data. It allows faster and monotonically convergent majorization in CRF
learning and maximum latent conditional likelihood problems (where it also ﬁnds better local max-
ima). Future work will explore intractable partition functions where likelihood evaluation is hard but
bound maximization may remain feasible. Furthermore  the majorization approach will be applied
in stochastic [31] and distributed optimization settings.
Acknowledgments
The authors thank A. Smola  M. Collins  D. Kanevsky and the referees for valuable feedback.
References
[1] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. In ICML  2001.

[2] A. Globerson  T. Koo  X. Carreras  and M. Collins. Exponentiated gradient algorithms for log-linear

structured prediction. In ICML  2007.

[3] J. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Annals of Math. Stat. 

43:1470–1480  1972.

6Downloaded from http://archive.ics.uci.edu/ml/

8

−50510−25−20−15−10−50ionlog(Time) [sec]−log(J(θ)) BoundNewtonBFGSConjugate gradientSteepest descent−6−4−2024−11−10−9−8−7−6−5−4hepatitislog(Time) [sec]−log(J(θ))−4−2024−12−10−8−6−4−20SRBCTlog(Time) [sec]−log(J(θ))[4] D. Bohning and B. Lindsay. Monotonicity of quadratic approximation algorithms. Ann. Inst. Statist.

Math.  40:641–663  1988.

[5] A. Berger. The improved iterative scaling algorithm: A gentle introduction. Technical report  1997.

[6] S. Della Pietra  V. Della Pietra  and J. Lafferty. Inducing features of random ﬁelds. IEEE PAMI  19(4) 

1997.

[7] R. Malouf. A comparison of algorithms for maximum entropy parameter estimation. In CoNLL  2002.

[8] H. Wallach. Efﬁcient training of conditional random ﬁelds. Master’s thesis  University of Edinburgh 

2002.

[9] F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds. In NAACL  2003.

[10] C. Zhu  R. Byrd  P Lu  and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale

bound-constrained optimization. TOMS  23(4)  1997.

[11] S. Benson and J. More. A limited memory variable metric method for bound constrained optimization.

Technical report  Argonne National Laboratory  2001.

[12] G. Andrew and J. Gao. Scalable training of ℓ1-regularized log-linear models. In ICML  2007.

[13] D. Roth. Integer linear programming inference for conditional random ﬁelds. In ICML  2005.

[14] Y. Mao and G. Lebanon. Generalized isotonic conditional random ﬁelds. Machine Learning  77:225–248 

2009.

[15] C. Sutton and A. McCallum. Piecewise training for structured prediction. Machine Learning  77:165–194 

2009.

[16] J. De Leeuw and W. Heiser. Convergence of correction matrix algorithms for multidimensional scaling 

chapter Geometric representations of relational data. 1977.

[17] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.

J. of the Royal Stat. Soc.  B-39  1977.

[18] M. Wainwright and M Jordan. Graphical models  exponential families and variational inference. Foun-

dations and Trends in Machine Learning  1(1-2):1–305  2008.

[19] T. Jebara and A. Pentland. On reversing Jensen’s inequality. In NIPS  2000.

[20] J. Salojarvi  K Puolamaki  and S. Kaski. Expectation maximization algorithms for conditional likelihoods.

In ICML  2005.

[21] A. Quattoni  S. Wang  L. P. Morency  M. Collins  and T. Darrell. Hidden conditional random ﬁelds. IEEE

PAMI  29(10):1848–1852  October 2007.

[22] T. Jaakkola and M. Jordan. Bayesian parameter estimation via variational methods. Statistics and Com-

puting  10:25–37  2000.

[23] G. Bouchard. Efﬁcient bounds for the softmax and applications to approximate inference in hybrid mod-

els. In NIPS AIHM Workshop  2007.

[24] B. Taskar  C. Guestrin  and D. Koller. Max margin Markov networks. In NIPS  2004.

[25] Y. Qi  M. Szummer  and T. P. Minka. Bayesian conditional random ﬁelds. In AISTATS  2005.

[26] T. Bromwich and T. MacRobert. An Introduction to the Theory of Inﬁnite Series. Chelsea  1991.

[27] S. B. Wang  A. Quattoni  L.-P. Morency  and D. Demirdjian. Hidden conditional random ﬁelds for gesture

recognition. In CVPR  2006.

[28] Y. Wang and G. Mori. Max-margin hidden conditional random ﬁelds for human action recognition. In

CVPR  pages 872–879. IEEE  2009.

[29] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization for Machine Learning  chapter Convex

optimization with sparsity-inducing norms. MIT Press  2011.

[30] Y. Altun  I. Tsochantaridis  and T. Hofmann. Hidden Markov support vector machines. In ICML  2003.

[31] SVN. Vishwanathan  N. Schraudolph  M. Schmidt  and K. Murphy. Accelerated training of conditional

random ﬁelds with stochastic gradient methods. In ICML  2006.

[32] T. Jebara. Multitask sparsity via maximum entropy discrimination. JMLR  12:75–110  2011.

9

Majorization for CRFs and Latent Likelihoods

(Supplementary Material)

Tony Jebara

Department of Computer Science

Columbia University

Anna Choromanska

Department of Electrical Engineering

Columbia University

jebara@cs.columbia.edu

aec2163@columbia.edu

Abstract

This supplement presents additional details in support of the full article. These in-
clude the application of the majorization method to maximum entropy problems.
It also contains proofs of the various theorems  in particular  a guarantee that the
bound majorizes the partition function. In addition  a proof is provided guarantee-
ing convergence on (non-latent) maximum conditional likelihood problems. The
supplement also contains supporting lemmas that show the bound remains ap-
plicable in constrained optimization problems. The supplement then proves the
soundness of the junction tree implementation of the bound for graphical mod-
els with large n. It also proves the soundness of the low-rank implementation of
the bound for problems with large d. Finally  the supplement contains additional
experiments and ﬁgures to provide further empirical support for the majorization
methodology.

∑

∑

Supplement for Section 2
Proof of Theorem 1 Rewrite the partition function as a sum over the integer index j = 1  . . .   n
under the random ordering π : Ω 7→ {1  . . .   n}. This deﬁnes j = π(y) and associates h and f with
−1(j)). Next  write Z(θ) =
−1(j)) and fj = f (π
fj) by introducing
hj = h(π
λ = θ − ˜θ and αj = hj exp( ˜θ
⊤
fj). Deﬁne the partition function over only the ﬁrst i components
⊤
fj). When i = 0  a trivial quadratic upper bound holds
as Zi(θ) =
Z0(θ) ≤ z0 exp

with the parameters z0 → 0+  µ0 = 0  and (cid:6)0 = z0I. Next  add one term to the current partition
function Z1(θ) = Z0(θ) + α1 exp(λ

⊤
f1). Apply the current bound Z0(θ) to obtain
Z1(θ) ≤ z0 exp( 1
⊤
2 λ
Consider the following change of variables

i
j=1 αj exp(λ

n
j=1 αj exp(λ

µ0) + α1 exp(λ

(cid:6)0λ + λ

(cid:6)0λ + λ

⊤
1
2 λ

)

(

f1).

⊤

⊤

µ0

⊤

⊤

u = (cid:6)1/2
γ = α1
z0

0 λ − (cid:6)
exp( 1

(f1 − µ0))
0 (f1 − µ0))
−1

(cid:6)

2 (f1 − µ0)
⊤

−1/2
0

(

)

.

Apply Lemma 1 (cf. [32] p. 100) to the last term to get

⊤

log Z1(θ) ≤ log z0 − 1

and rewrite the logarithm of the bound as
2 (f1 − µ0)
log Z1(θ) ≤ log z0 − 1
2 (f1 − µ0)
⊤
(u − v)
⊤
1+γ exp(− 1
∥v∥2)

+

v

2

(cid:6)

0 (f1 − µ0) + λ
−1
⊤
⊤(
0 (f1 − µ0) + λ
−1

(u − v)

⊤

+

(cid:6)

1
2

I + Γvv

∥u∥2) + γ
)
)

∥v∥2

+γ

f1 + log

exp( 1
2

(
(
exp
(u − v)

1
2

⊤)

f1 + log

10

. The bound in [32] is tight when u = v. To achieve tightness

where Γ =

tanh(

1

2 log(γ exp(− 1
2 log(γ exp(− 1
2

2
∥v∥2))

∥v∥2)))

(

)

−1/2
when θ = ˜θ or  equivalently  λ = 0  we choose v = (cid:6)
0
⊤
1
2 λ

Z1(θ) ≤ z1 exp

⊤

(cid:6)1λ + λ

µ1

(µ0 − f1) yielding

where we have

z1 = z0 + α1

µ1 =

z0

z0 + α1

µ0 +

α1

z0 + α1

f1

(cid:6)1 = (cid:6)0 +

tanh( 1

2 log(α1/z0))

2 log(α1/z0)

(µ0 − f1)(µ0 − f1)

⊤

.

This rule updates the bound parameters z0  µ0  (cid:6)0 to incorporate an extra term in the sum over i in
Z(θ). The process is iterated n times (replacing 1 with i and 0 with i − 1) to produce an overall
bound on all terms.

)

∥u∥2

+ γ

(
(
⊤(
exp
(u − v)

1
2

) ≤
⊤)

I + Γvv

(u − v)

. Equality is achieved when u = v.

)

)

(

(

Lemma 1 (See [32] p. 100)
For all u ∈ Rd  any v ∈ Rd and any γ ≥ 0  the bound log
1
2
2 log(γ exp(−∥v∥2/2)))

(u − v)
1 + γ exp(− 1

holds when the scalar term Γ = tanh( 1

∥v∥2)

2 log(γ exp(−∥v∥2/2))

∥v∥2

exp

+ γ

log

+

+

⊤

v

1
2

2

Proof of Lemma 1 The proof is provided in [32].
Supplement for Section 3
Maximum entropy problem We show here that partition functions arise naturally in maximum
entropy estimation or minimum relative entropy RE(p∥h) =
∑
h(y) estimation. Consider
the following problem:

y p(y) log p(y)

∑

∑

RE(p∥h) s.t.

min

p

p(y)f (y) = 0 

y

y

p(y)g(y) ≥ 0.

(

)

Here  assume that f : Ω 7→ Rd and g : Ω 7→ Rd
∑
over the sample space. The solution distribution p(y) = h(y) exp
recovered by the dual optimization

′

(

θ  ϑ =

arg
max
ϑ≥0 θ

− log

h(y) exp

θ

y

⊤

f (y) + ϑ

⊤

g(y)

)

are arbitrary (non-constant) vector-valued functions
/Z(θ  ϑ) is

f (y) + ϑ

g(y)

⊤

⊤

θ

where θ ∈ Rd and ϑ ∈ Rd
. These are obtained by minimizing Z(θ  ϑ) or equivalently by max-
imizing its negative logarithm. Algorithm 1 permits variational maximization of the dual via the
quadratic program

′

2 (β − ˜β)
⊤

1

(cid:6)(β − ˜β) + β

⊤

µ

min
ϑ≥0 θ

⊤

⊤

⊤

]. Note that any general convex hull of constraints β ∈ (cid:3) ⊆ Rd+d

′

could be

ϑ

= [θ

where β
imposed without loss of generality.
Proof of Theorem 2 We begin by proving a lemma that will be useful later.
Lemma 2 If κ(cid:9) ≽ (cid:8) ≻ 0 for (cid:8)  (cid:9) ∈ Rd×d  then

2 (θ − ˜θ)
⊤
2 (θ − ˜θ)
⊤

L(θ) = − 1
U (θ) = − 1
κ supθ∈(cid:3) U (θ) for any convex (cid:3) ⊆ Rd  ˜θ ∈ (cid:3)  µ ∈ Rd and κ ∈ R+.

(cid:8)(θ − ˜θ) − (θ − ˜θ)
⊤
(cid:9)(θ − ˜θ) − (θ − ˜θ)
⊤

µ

µ

satisfy supθ∈(cid:3) L(θ) ≥ 1

11

Proof of Lemma 2 Deﬁne the primal problems of interest as PL = supθ∈(cid:3) L(θ) and PU =
supθ∈(cid:3) U (θ). The constraints θ ∈ (cid:3) can be summarized by a set of linear inequalities Aθ ≤ b
where A ∈ Rk×d and b ∈ Rk for some (possibly inﬁnite) k ∈ Z. Apply the change of variables
z = θ− ˜θ. The constraint A(z+ ˜θ) ≤ b simpliﬁes into Az ≤ ˜b where ˜b = b−A ˜θ. Since ˜θ ∈ (cid:3)  it
(cid:8)z −
is easy to show that ˜b ≥ 0. We obtain the equivalent primal problems PL = supAz≤˜b
⊤
z

− 1
⊤
2 z

µ and PU = supAz≤˜b

− 1
⊤
2 z

(cid:9)z − z
⊤
µ. The corresponding dual problems are
⊤
⊤
−1A
A(cid:8)
y
2
−1A
⊤
A(cid:9)
2

−1µ
⊤
(cid:8)
2
−1µ
⊤
(cid:9)
2

−1µ+y

−1µ+y

⊤
A(cid:9)

⊤
A(cid:8)

⊤˜b+

⊤˜b+

⊤
y

+y

+y

µ

µ

.

y

y

DL = inf
y≥0

DU = inf
y≥0

Due to strong duality  PL = DL and PU = DU . Apply the inequalities (cid:8) ≼ κ(cid:9) and y
⊤ ˜b > 0 as
−1µ
⊤
(cid:9)
µ
2κ

⊤
−1A
y
2κ

⊤
A(cid:9)
y
κ

(cid:9)z − z
⊤

µ = inf
y≥0

⊤
A(cid:9)
y

⊤˜b +

− κ
2

−1µ

PL ≥ sup
Az≤˜b

⊤
z

+ y

+

≥ 1
κ

DU =

PU .

1
κ

This proves that PL ≥ 1
We will use the above to prove Theorem 2. First  we will upper-bound (in the Loewner ordering
sense) the matrices (cid:6)j in Algorithm 2. Since ∥fxj (y)∥2 ≤ r for all y ∈ Ωj and since µj in
Algorithm 1 is a convex combination of fxj (y)  the outer-product terms in the update for (cid:6)j satisfy

κ PU .

(fxj (y) − µ)(fxj (y) − µ)

Thus  (cid:6)j ≼ F(α1  . . .   αn)4r2I holds where
F(α1  . . .   αn) =

⊤ ≼ 4r2I.
αi∑
αi∑

2 log(

2 log(

i−1
k=1 αk

)

i−1
k=1 αk

))

tanh( 1

n∑

i=2

∑

n∑

tanh( 1

2 log(

π

i=2

2 log(

∑

∑

α(cid:25)(i)
i−1
k=1 α(cid:25)(k)

))

.

α(cid:25)(i)
i−1
k=1 α(cid:25)(k)

)

(cid:6)j ≼

2r2

tanh(log(i)/2)

I = ωI.

log(i)

i=1

∑
∑
∥θ − ˜θ∥2−
(µj−fxj (yj))
(θ − ˜θ)
⊤
(θ − ˜θ)
∥θ − ˜θ∥2−

(µj−fxj (yj))
⊤

j

J(θ) ≥ J( ˜θ)−tω+tλ
J(θ) ≤ J( ˜θ)−tλ

2

2

j

12

using the deﬁnition of α1  . . .   αn in the proof of Theorem 1. The formula for F starts at i = 2 since
z0 → 0+. Assume permutation π is sampled uniformly at random. The expected value of F is then

Eπ[F(α1  . . .   αn)] =

1
n!

at the setting αi = 1 ∀i. Due to the expectation over π  we have ∂E

We claim that the expectation is maximized when all αi = 1 or any positive constant. Also  F
is invariant under uniform scaling of its arguments. Write the expected value of F as E for short.
Next  consider ∂E
= ∂E
for any l  o. Therefore  the gradient vector is constant when all αi = 1. Since F(α1  . . .   αn)
∂αo
∂αl
is invariant to scaling  the gradient vector must therefore be the all zeros vector. Thus  the point
when all αi = 1 is an extremum or a saddle. Next  consider
for any l  o. At the setting
= c(n)/(n − 1) for some non-negative constant function
αi = 1  ∂2E
∂α2
l
c(n). Thus  the αi = 1 extremum is locally concave and is a maximum. This establishes that
n−1∑
Eπ[F(α1  . . .   αn)] ≤ Eπ[F(1  . . .   1)] and yields the Loewner bound

= −c(n) and 

(

)

∂E
∂αl

∂E
∂αl

∂αo

∂αo

∂αl

∂

∂

Apply this bound to each (cid:6)j in the lower bound on J(θ) and also note a corresponding upper bound

which follows from Jensen’s inequality. Deﬁne the current ˜θ at time τ as θτ and denote by Lτ (θ) the
above lower bound and by Uτ (θ) the above upper bound at time τ. Clearly  Lτ (θ) ≤ J(θ) ≤ Uτ (θ)
with equality when θ = θτ . Algorithm 2 maximizes J(θ) after initializing at θ0 and performing
an update by maximizing a lower bound based on (cid:6)j. Since Lτ (θ) replaces the deﬁnition of (cid:6)j
with ωI ≽ (cid:6)j  Lτ (θ) is a looser bound than the one used by Algorithm 2. Thus  performing
θτ +1 = arg maxθ∈(cid:3) Lτ (θ) makes less progress than a step of Algorithm 1. Consider computing the
slower update at each iteration τ and returning θτ +1 = arg maxθ∈(cid:3) Lτ (θ). Setting (cid:8) = (tω +tλ)I 
(cid:9) = tλI and κ = ω+λ

λ allows us to apply Lemma 2 as follows

Since Lτ (θτ ) = J(θτ ) = Uτ (θτ )  J(θτ +1) ≥ supθ∈(cid:3) Lτ (θ) and supθ∈(cid:3) Uτ (θ) ≥ J(θ
obtain

∗

)  we

1
κ

sup
θ∈(cid:3)

Lτ (θ) − Lτ (θτ ) =
(
(
Iterate the above inequality starting at t = 0 to obtain
1 − 1
κ

J(θτ +1) − J(θ

J(θτ ) − J(θ

) ≥

) ≥

∗

∗

1 − 1
κ

sup
θ∈(cid:3)

)
)τ

Uτ (θ) − Uτ (θτ ).

(J(θτ ) − J(θ

∗

)) .

(

(J(θ0) − J(θ
1 − 1

∗

)) .

)τ or log(1/ϵ) = τ log κ
⌉

⌈

κ−1 .
or

log(1/ϵ)
log (cid:20)
(cid:20)−1

A solution within a multiplicative factor of ϵ implies that ϵ =
Inserting the deﬁnition for κ shows that the number of iterations τ is at most

κ

⌈

⌉

log(1/ϵ)

log(1+λ/ω)

. Inserting the deﬁnition for ω gives the bound.

Y 2 0
1

Y 1 1
1

Y 1 1
2

Y 1 1
3

···

Y 1 1
m1;1

Figure 3: Junction tree of depth 2.

1

Algorithm 5 SmallJunctionTree
Input Parameters ˜θ and h(u)  f (u) ∀u ∈ Y 2 0
∏m1;1
Initialize z → 0+  µ = 0  (cid:6) = zI
∑m1;1
For each conﬁguration u ∈ Y 2 0
{
i=1 zi exp(− ˜θ
⊤
⊤
µi)) exp( ˜θ
α = h(u)(
i=1 µi − µ
l = f (u) +
(cid:6) + =
µ + = α
}
z + = α
Output z  µ  (cid:6)

i=1 (cid:6)i + tanh( 1

∑m1;1

2 log(α/z))

z+α l

2 log(α/z)

⊤

ll

1

and zi  (cid:6)i  µi ∀i = 1  . . .   m1 1

∑m1;1

(f (u) +

⊤
i=1 µi)) = h(u) exp( ˜θ

∏m1;1

i=1 zi

f (u))

c

∑

Supplement for Section 5
Proof of correctness for Algorithm 3 Consider a simple junction tree of depth 2 shown on Figure 3.
The notation Y a b
refers to the cth tree node located at tree level a (ﬁrst level is considered as the one
with tree leaves) whose parent is the bth from the higher tree level (the root has no parent so b = 0).
Let
\Y a2 ;b2
c1
c1
refers to the sum over all conﬁgurations of variables that are in Y a1 b1
but not in Y a2 b2
. Let ma b
denote the number of children of the bth node located at tree level a + 1. For short-hand  use
ψ(Y ) = h(Y ) exp(θ

refer to the sum over all conﬁgurations of variables in Y a1 b1

f (Y )). The partition function can be expressed as:

∑

Y a1 ;b1

Y a1 ;b1

and

⊤

c2

c1

c1

c2

13

Y 3;0
1

Y 2;1
1

Y 2;1
2

···

Y 2;1
m2;1

Y 1;1
1

Y 1;1
2

··· Y 1;1

m1;1

Y 1;2
1

Y 1;2
2

··· Y 1;2

m1;2

Y 1;m2;1
1

Y 1;m2;1
2

···

Y 1;m2;1
m1;m2;1

Figure 4: Junction tree of depth 3.

ψ(u)
[
[

ψ(u)

m1;1∏
m1;1∏

i=1

i=1

h(u) exp(θ



ψ(v)

 ∑
(
\Y 2;0
θ − ˜θ)
m1;1∏

v∈Y 1;1

zi exp(

1
2

1

i

⊤

f (u))

zi exp

i=1

u∈Y 2;0

∑
∑
∑

1

1

u∈Y 2;0

u∈Y 2;0

1

Z(θ) =

≤

=

[

∑

(
m1;1∏

h(u)

i=1

u∈Y 2;0

1

Z(θ) ≤

zi exp(− ˜θ
⊤

µi)

exp

)]

µi

)]

⊤

µi

⊤

(cid:6)i(θ − ˜θ) + (θ − ˜θ)
⊤
(

)

1
2

(θ − ˜θ)
⊤
(
(
m1;1∑

(

⊤

θ

∑

(cid:6)i(θ − ˜θ) + (θ − ˜θ)
))
)]

m1;1∑

v∈Y 1;1

)

f (u) +

µi

i=1

i

(θ − ˜θ)

.

(θ − ˜θ)

⊤

1
2

(cid:6)i

i=1

(

where the upper-bound is obtained by applying Theorem 1 to each of the terms
By simply rearranging terms we get:

\Y 2;0

1

ψ(v).

(

exp

)

∑

Z(θ) =

u∈Y 3;0

1

1

that

this
prove
can
2 (θ − ˆθ)
(cid:6)(θ − ˆθ) + (θ − ˆθ)
⊤
⊤

expression
by
One
where z  (cid:6) and µ can be computed using Algo-
z exp
rithm 5 (a simpliﬁcation of Algorithm 3). We will call this result Lemma A. The proof is similar to
the proof of Theorem 1 so is not repeated here.
Consider enlarging the tree to a depth 3 as shown on Figure 4. The partition function is now

upper-bounded

can

be

µ

m2;1∏

i=1

ψ(u)
(
[

1

ψ(v)

 ∑
m1;i∏
(
∑
ψ(v)
(
(cid:6)i(θ − ˆθ) + (θ − ˆθ)
⊤

v∈Y 2;1

v∈Y 2;1

 ∑
∏m1;i
)

w∈Y 1;i

\Y 3;0

\Y 3;0

µi

j=1

⊤

1

1

j

i

i

ψ(w)

\Y 2;1

(∑

i

w∈Y 1;i

j=1
. This yields

j

 .




)]

\Y 2;1

i

(θ − ˜θ)
⊤

(cid:6)i(θ − ˜θ) + (θ − ˜θ)
⊤

µi

.

1
2

By Lemma A we can upper bound each

by the expression zi exp

∑

2 (θ − ˆθ)
m2;1∏

ψ(u)

zi exp

i=1

u∈Y 3;0

1

Z(θ) ≤

))

term

ψ(w)

This process can be viewed as collapsing the sub-trees S2 1
m2;1 to super-nodes that
are represented by bound parameters  zi  (cid:6)i and µi  i = {1  2 ···   m2 1}  where the sub-trees are

2   . . .  S2 1

1   S2 1

14

deﬁned as:

S2 1
1
S2 1
2
...

S2 1
m2;1

= {Y 2 1
= {Y 2 1

1

2

1

  Y 1 1
  Y 1 2

1

2

  Y 1 1
  Y 1 2

2

3

  Y 1 1
  Y 1 2

3

  . . .   Y 1 1
m1;1
  . . .   Y 1 2
m1;2

}
}

= {Y 2 1
(

m2;1

  Y 1 m2;1

1

  Y 1 m2;1

2

  Y 1 m2;1

3

  . . .   Y 1 m2;1
m1;m2;1

}.

)

Notice that the obtained expression can be further upper bounded using again Lemma A (induction)
yielding a bound of the form: z exp

(cid:6)(θ − ˆθ) + (θ − ˆθ)

⊤

µ

.

1

2 (θ − ˆθ)
⊤

Finally  for a general tree  follow the same steps described above  starting from leaves and collapsing
nodes to super-nodes  each represented by bound parameters. This procedure effectively yields
Algorithm 3 for the junction tree under consideration.
Supplement for Section 6
Proof of correctness for Algorithm 4 We begin by proving a lemma that will be useful later.
Lemma 3 For all x ∈ Rd and for all l ∈ Rd 

d∑

i=1

x(i)2l(i)2 ≥

 d∑
)2

i=1

x(i)

√∑
⇐⇒ d∑

i=1

2

l(i)2

d
j=1 l(j)2

x(i)2l(i)2 ≥

.

 d∑

i=1

2

.

√∑

x(i)l(i)2

d
j=1 l(j)2

Proof of Lemma 3 By Jensen’s inequality 

d∑

i=1

l(i)2∑

d
j=1 l(j)2

x(i)2

(
d∑

i=1

∑

≥

x(i)l(i)2
d
j=1 l(j)2

Now we prove the correctness of Algorithm 4. At the ith iteration  the algorithm stores (cid:6)i using
i SiVi + Di where Vi ∈ Rk×d is orthonormal  Si ∈ Rk×k positive
⊤
a low-rank representation V
semi-deﬁnite and Di ∈ Rd×d is non-negative diagonal. The diagonal terms D are initialized to tλI
∑
where λ is the regularization term. To mimic Algorithm 1 we must increment the (cid:6) matrix by a
⊤
rank one update of the form (cid:6)i = (cid:6)i−1 + rir
i . By projecting ri onto each eigenvector in V  we
j=1 Vi−1(j ·)riVi−1(j ·)
⊤
⊤
i−1Vi−1ri + g where g is the
can decompose it as ri =
remaining residue. Thus the update rule can be rewritten as:
⊤
⊤
i−1Vi−1ri + g)(V
(cid:6)i = (cid:6)i−1 + rir
i = V

⊤
i−1Si−1Vi−1 + Di−1 + (V

⊤
⊤
i−1Vi−1ri + g)

+ g = V

k

⊤
i−1(Si−1 + Vi−1rir

⊤
i V

⊤
i−1)Vi−1 + Di−1 + gg

⊤

= V

′⊤
′
i−1S
i−1V

′
i−1 + gg

⊤

+ Di−1

= V

⊤
i V

⊤
i−1S

′
i−1Qi−1 = svd(Si−1 + Vi−1rir

′
i−1 = Qi−1Vi−1 and deﬁned Qi−1 in terms of the singular value decomposition 
where we deﬁne V
′
i−1 is diagonal and nonnegative by
Q
construction. The current formula for (cid:6)i shows that we have a rank (k + 1) system (plus diagonal
term) which needs to be converted back to a rank k system (plus diagonal term) which we denote by
′
i. We have two options as follows.
(cid:6)
Case 1) Remove g from (cid:6)i to obtain
′

⊤
i−1). Note that S

i−1 + Di−1 = (cid:6)i − gg

′⊤
′
i−1S
i−1V
where c = ∥g∥2 and v = 1∥g∥ g.
′
Case 2) Remove the mth (smallest) eigenvalue in S
i−1 and its corresponding eigenvector:
⊤ − S
′

= (cid:6)i − cvv

(m ·) = (cid:6)i − cvv

′
i−1 + Di−1 + gg

′
i = V

′
(cid:6)
i = V

(m ·)

′
i−1V

′⊤
i−1S

(m  m)V

V

(cid:6)

⊤

⊤

⊤

⊤

′

′

where c = S

′

(m  m) and v = V(m ·)

′

.

15

⊤

′
i = (cid:6)i + cvv

⊤ where c ≥ 0 and
Clearly  both cases can be written as an update of the form (cid:6)
v = 1. We choose the case with smaller c value to minimize the change as we drop from a system
v
of order (k + 1) to order k. Discarding the smallest singular value and its corresponding eigenvector
would violate the bound.
Instead  consider absorbing this term into the diagonal component to
′′
′
i + F which also
preserve the bound. Formally  we look for a diagonal matrix F such that (cid:6)
i = (cid:6)
i x ≥ x
maintains x
(cid:6)ix ⇐⇒ x

(cid:6)ix for all x ∈ Rd. Thus  we want to satisfy:

Fx ⇐⇒ c

x ≤ x
⊤

d∑

)2

x(i)2F(i)

x(i)v(i)

(

i x ≥ x

′′
(cid:6)

⊤
x

′′
(cid:6)

cvv

⊤

⊤

⊤

⊤

⊤

where  for ease of notation  we take F(i) = F(i  i).

= 1

Deﬁne v
this assumption. We need an F such that

w v where w = v

⊤

1. Consider the case where v ≥ 0 though we will soon get rid of
. Equivalently  we

d
i=1 x(i)v(i)

′

∑

≤ d∑
)2

i=1

i=1

(∑

(∑

∑
)2
i=1 x(i)2F(i) ≥ c
. Deﬁne F(i)

d

′

)2

(∑

′
d
i=1 x(i)v(i)
′
d
i=1 x(i)v(i)

′ ≥

. Thus  we obtain F(i) = cw2F(i)

(

d∑

d

j=1 v(j) we have

x(i)2F(i) ≥ c

)2

x(i)v(i)

.

i=1

= F(i)

cw2 for all i = 1  . . .   d. So  we need
. Using Lemma 3 it is easy to show that we
= cwv(i). Therefore  for all x ∈ Rd 

′

(3)

d

need

′
(i) = v(i)

∑
cw2 ≥
i=1 x(i)2 F(i)
′
∑
d
such that
an F
i=1 x(i)2F(i)
′
may choose F
all v ≥ 0  and for F(i) = cv(i)
d∑
sufﬁcient to set F(i) = c|v(i)|∑
for F(i) = c|v(i)|∑

i=1

d
j=1

To generalize the inequality to hold for all vectors v ∈ Rd with potentially negative entries  it is
|v(j)|. To verify this  consider ﬂipping the sign of any v(i).
The left side of the Inequality 3 does not change. For the right side of this inequality  ﬂipping the
sign of v(i) is equivalent to ﬂipping the sign of x(i) and not changing the sign of v(i). However  in
this case the inequality holds as shown before (it holds for any x ∈ Rd). Thus for all x  v ∈ Rd and

d
j=1

|v(j)|  Inequality 3 holds.

Supplement for Section 7
Small scale experiments In additional small-scale experiments  we compared Algorithm 2 with
steepest descent (SD)  conjugate gradient (CG)  BFGS and Newton-Raphson. Small-scale problems
may be interesting in real-time learning settings  for example  when a website has to learn from a
user’s uploaded labeled data in a split second to perform real-time retrieval. We considered logistic
regression on ﬁve UCI data sets where missing values were handled via mean-imputation. A range of
regularization settings λ ∈ {100  102  104} was explored and all algorithms were initialized from the
same ten random start-points. Table 3 shows the average number of seconds each algorithm needed
to achieve the same solution that BFGS converged to (all algorithms achieve the same solution due
to concavity). The bound is the fastest algorithm as indicated in bold.
data|λ a|100 a|102 a|104 b|100 b|102 b|104 c|100 c|102 c|104 d|100 d|102 d|104 e|100 e|102 e|104
3.28 2.63 2.01 1.49
BFGS
1.94 2.68 2.49 1.54
1.23 0.48 0.55 0.43
0.60 0.35 0.26 0.20
0.07 0.03 0.03 0.03

2.45 3.14 2.00 1.60 4.09 1.03 1.90 5.62
2.88
1.60 2.18 6.17 5.83 1.92 0.64 0.56 12.04 1.27
0.85 0.70 0.67 0.83 0.65 0.64 0.72 1.36
1.21
0.63
0.22 0.43 0.37 0.35 0.39 0.34 0.32 0.92
0.01 0.07 0.04 0.04 0.07 0.02 0.02 0.16
0.09

1.90
1.74
0.78
Newton 0.31
0.01
Bound

0.89
0.92
0.83
0.25
0.01

SD
CG

Table 3:
Convergence time in seconds under various regularization levels for a) Bupa (t =
345  dim = 7)  b) Wine (t = 178  dim = 14)  c) Heart (t = 187  dim = 23)  d) Ion
(t = 351  dim = 34)  and e) Hepatitis (t = 155  dim = 20) data sets.

Inﬂuence of rank k on bound performance in large scale experiments We also examined the
inﬂuence of k on bound performance and compared it with LBFGS  SD and CG. Several choices

16

of k were explored. Table 4 shows results for the SRBCT data-set. In general  the bound performs
best but slows down for superﬂuously large values of k. Steepest descent and conjugate gradient
are slow yet obviously do not vary with k. Note that each iteration takes less time with smaller k
for the bound. However  we are reporting overall runtime which is also a function of the number of
iterations. Therefore  total runtime (a function of both) may not always decrease/increase with k.

k

1

2

8

16

4

64
LBFGS 1.37 1.32 1.39 1.35 1.46 1.40 1.54
8.80 8.80 8.80 8.80 8.80 8.80 8.80
4.39 4.39 4.39 4.39 4.39 4.39 4.39
0.56 0.56 0.67 0.96 1.34 2.11 4.57

SD
CG

Bound

32

Table 4: Convergence time in seconds as a function of k.

Additional latent-likelihood results For completeness  Figure 5 depicts two additional data-sets
to complement Figure 2. Similarly  Table 5 shows all experimental settings explored in order to
provide the summary Table 2 in the main article.

Figure 5: Convergence of test latent log-likelihood for bupa and wine data-sets.

ion

bupa

BFGS

Data-set
Algorithm m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4
-4.96 -5.55 -5.88 -5.79 -22.07 -21.78 -21.92 -21.87 -4.42 -5.28 -4.95 -4.93
-11.80 -9.92 -5.56 -8.59 -21.76 -21.74 -21.73 -21.83 -4.93 -5.14 -5.01 -5.20
-5.47 -5.81 -5.57 -5.22 -21.81 -21.81 -21.81 -21.81 -4.84 -4.84 -4.84 -4.84
-5.95 -5.95 -5.95 -5.95 -21.85 -21.85 -21.85 -21.85 -5.50 -5.50 -5.50 -4.50
-6.08 -4.84 -4.18 -5.17 -21.85 -19.95 -20.01 -19.97 -5.47 -4.40 -4.75 -4.92

Newton
Bound

hepatitis

SD
CG

wine

BFGS

SRBCT

Data-set
Algorithm m = 1 m = 2 m = 3 m = 4 m = 1 m = 2 m = 3 m = 4
-0.90 -0.91 -1.79 -1.35 -5.99 -6.17 -6.09 -6.06
-1.61 -1.60 -1.37 -1.63 -5.61 -5.62 -5.62 -5.61
-0.51 -0.78 -0.95 -0.51 -5.62 -5.49 -5.36 -5.76
-0.71 -0.71 -0.71 -0.71 -5.54 -5.54 -5.54 -5.54
-0.51 -0.51 -0.48 -0.51 -5.31 -5.31 -4.90 -0.11

Newton
Bound

SD
CG

Table 5: Test latent log-likelihood at convergence for different values of m ∈ {1  2  3  4} on ion 
bupa  hepatitis  wine and SRBCT data-sets.

17

−50510−24−23−22−21−20−19bupalog(Time) [sec]−log(J(θ))−4−202468−20−15−10−50winelog(Time) [sec]−log(J(θ)) BoundNewtonBFGSConjugate gradientSteepest descent,Saurabh Paul
Malik Magdon-Ismail
Petros Drineas