2018,Differentially Private Bayesian Inference for Exponential Families,The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods  it gives properly calibrated posterior beliefs in the non-asymptotic data regime.,Differentially Private Bayesian Inference for

Exponential Families

Garrett Bernstein

College of Information and Computer Sciences

University of Massachusetts Amherst

Amherst  MA 01002

gbernstein@cs.umass.edu

Daniel Sheldon

College of Information and Computer Sciences

University of Massachusetts Amherst

Amherst  MA 01002

sheldon@cs.umass.edu

Abstract

The study of private inference has been sparked by growing concern regarding the
analysis of data when it stems from sensitive sources. We present the Ô¨Årst method
for private Bayesian inference in exponential families that properly accounts for
noise introduced by the privacy mechanism. It is efÔ¨Åcient because it works only
with sufÔ¨Åcient statistics and not individual data. Unlike other methods  it gives
properly calibrated posterior beliefs in the non-asymptotic data regime.

1

Introduction

Differential privacy is the dominant standard for privacy [1]. A randomized algorithm that satisÔ¨Åes
differential privacy offers protection to individuals by guaranteeing that its output is insensitive to
changes caused by the data of any single individual entering or leaving the data set. An algorithm can
be made differentially private by applying one of several general-purpose mechanisms to randomize
the computation in an appropriate way  for example  by adding noise calibrated to the sensitivity
of the quantity being computed  where sensitivity captures how much the quantity depends on any
individual‚Äôs data [1]. Due to the obvious importance of protecting individual privacy while drawing
population level inferences from data  differentially private algorithms have been developed for a
broad range of machine learning tasks [2‚Äì9].
There is a growing interest in private methods for Bayesian inference [10‚Äì14]. In Bayesian inference 
a modeler selects a prior distribution p(Œ∏) over some parameter  observes data x that depends
probabilistically on Œ∏ through a model p(x | Œ∏)  and then reasons about Œ∏ through the posterior
distribution p(Œ∏ | x)  which quantiÔ¨Åes updated beliefs and uncertainty about Œ∏ after observing x.
Bayesian inference is a core machine learning task and there is an obvious need to be able to conduct
it in a way that protects privacy when x is sensitive. Additionally  recent work has identiÔ¨Åed surprising
connections between sampling from posterior distributions and differential privacy‚Äîfor example 
a single perfect sample from p(Œ∏ | x) satisÔ¨Åes differential privacy for some setting of the privacy
parameter [10‚Äì13].
An ‚Äúobvious‚Äù way to conduct private Bayesian inference is to privatize the computation of the
posterior  that is  to design a differentially private algorithm A that outputs y = A(x) with the goal
that y ‚âà p(Œ∏ | x) is a privatized representation of the posterior. However  using y directly as ‚Äúthe

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr√©al  Canada.

posterior‚Äù will not correctly quantify beliefs  because the Bayesian modeler never observes x  she
observes y; her posterior beliefs are now quantiÔ¨Åed by p(Œ∏ | y).
This paper will take a different approach to private Bayesian inference by designing a pair of
algorithms: The release mechanism A computes a private statistic y = A(x) of the input data; the
inference algorithm P computes p(Œ∏ | y). These algorithms should satisfy the following criteria:
‚Ä¢ Privacy. The release mechanism A is differentially private. By the post-processing property of
‚Ä¢ Calibration. The inference algorithm P can efÔ¨Åciently compute or approximate the correct
‚Ä¢ Utility. Informally  the statistic y should capture ‚Äúas much information as possible‚Äù about x so

differential privacy [15]  all further computations are also private.
posterior  p(Œ∏ | y) (see Section 4 for our process to measure calibration).
that p(Œ∏ | y) is ‚Äúclose‚Äù to p(Œ∏ | x) (see Section 4 for our process to measure utility).

One challenge is computational efÔ¨Åciency. The exact posterior p(Œ∏ | y) ‚àù (cid:82) p(Œ∏)p(x1:n |

Importantly  the release mechanism A is public  so the distribution p(y | x) is known. Williams and
McSherry Ô¨Årst suggested conducting inference on the output of a differentially private algorithm and
showed how to do this for the factored exponential mechanism [16]; see also [17‚Äì20].
Our work focuses speciÔ¨Åcally on Bayesian inference when the private data X = x1:n is an iid sample
of (publicly known) size n from an exponential family model p(xi | Œ∏). Exponential families include
many of the most familiar parametric probability models. We will adopt a straightforward release
mechanism where the Laplace mechanism [1] is used to release noisy sufÔ¨Åcient statistics y [12  19] 
which are a Ô¨Ånite-dimensional quantity that capture all the information about Œ∏ [21].
The technical challenge is then to develop an efÔ¨Åcient general-purpose inference algorithm P.
Œ∏)p(y|x1:n)dx1:n integrates over all possible data sets [16]  which is intractable to do directly
for large n. We integrate instead over the sufÔ¨Åcient statistics s  which have Ô¨Åxed dimension and
completely characterize the posterior; furthermore  since they are a sum over individuals  p(s | Œ∏)
is asymptotically normal. We develop an efÔ¨Åcient Gibbs sampler that uses a normal approximation
for s together with variable augmentation to model the Laplace noise in a way that yields simple
updates [22].
A second challenge is that the sufÔ¨Åcient statistics may be unbounded  which makes their release
incompatible with the Laplace mechanism. We address this by imposing truncation bounds and
only computing statistics from data that fall within the bounds. We show how to use automatic
differentiation and a ‚Äúrandom sum‚Äù central limit theorem to compute the parameters of the normal
approximation p(s | Œ∏) for a truncated exponential family when the number of individuals that fall
within the truncation bounds is unknown.
Our overall contribution is the pairing of an existing simple release mechanism A with a novel 
efÔ¨Åcient  and general-purpose Gibbs sampler P that meets the criteria outlined above for private
Bayesian inference in any univariate exponential family or multivariate exponential family with
bounded sufÔ¨Åcient statistics.1 We show empirically that when compared with competing methods 
ours is the only one that provides properly calibrated beliefs about Œ∏ in the non-asymptotic regime 
and that it provides good utility compared with other private Bayesian inference approaches.

2 Differential Privacy

Differential privacy requires that an individual‚Äôs data has a limited effect on the algorithm‚Äôs behavior.
In our setting  a data set X = x1:n := (x1  . . .   xn) consists of records from n individuals  where
xi ‚àà Rd is the data of the ith individual. We will assume n is known. Differential privacy reasons
about the hypothesis that one individual chooses not to remove their data from the data set  and their
record is replaced by another one.2 Let nbrs(X) denote the set of data sets that differ from X by
exactly one record‚Äîi.e.  if X(cid:48) ‚àà nbrs(X)  then X(cid:48) = (x1:i  x(cid:48)

i  xi+1:n) for some i.

1There are remaining technical challenges for multivariate models with unbounded sufÔ¨Åcient statistics that

we leave for future work.

2This variant assumes n remains Ô¨Åxed  which is sometimes called bounded differential privacy [23].

2

DeÔ¨Ånition 1 (Differential Privacy; Dwork et al. [1]). A randomized algorithm A satisÔ¨Åes -differential
privacy if for any input X  any X(cid:48) ‚àà nbrs(X) and any subset of outputs O ‚äÜ Range(A) 

Pr[A(X) ‚àà O] ‚â§ exp() Pr[A(X

(cid:48)) ‚àà O].

the Laplace mechanism outputs the random variable L(X) ‚àº Lap(cid:0)f (X)  ‚àÜf /(cid:1) from the Laplace

We achieve differential privacy by injecting noise into statistics that are computed on the data. Let f
be any function that maps datasets to Rd. The amount of noise depends on the sensitivity of f.
DeÔ¨Ånition 2 (Sensitivity). The sensitivity of a function f is ‚àÜf = maxX X(cid:48)‚àànbrs(X) (cid:107)f (X) ‚àí
f (X(cid:48))(cid:107)1.
We drop the subscript f when it is clear from context. Our approach achieves differential privacy
through the application of the Laplace mechanism.
DeÔ¨Ånition 3 (Laplace Mechanism; Dwork et al. [1]). Given a function f that maps data sets to Rm 
distribution  which has density Lap(z; u  b) = (2b)‚àím exp (‚àí(cid:107)z ‚àí u(cid:107)1/b). This corresponds to
adding zero-mean independent noise ui ‚àº Lap(0  ‚àÜf /) to each component of f (X).
A Ô¨Ånal important property of differential privacy is post-processing [15]; if an algorithm A is -
differentially private  then any algorithm that takes as input only the output of A  and does not use
the original data set X  is also -differentially private.

3 Private Bayesian Inference in Exponential Families

(cid:62)

t(x) ‚àí A(Œ∑)(cid:1)  
t(x1:n) ‚àí nA(Œ∑)(cid:1)  

(cid:62)

the log-partition function  and h(x) is the base measure. The density of the full data is

We consider the canonical setting of Bayesian inference in an exponential family. The modeler
posits a prior distribution p(Œ∏)  assumes the data x1:n is an iid sample from an exponential family
model p(x | Œ∏)  and wishes to compute the posterior p(Œ∏ | x1:n). An exponential family in natural
parameterization has density

where Œ∑ are the natural parameters  t(x) is the sufÔ¨Åcient statistic  A(Œ∑) =(cid:82) h(x) exp(cid:0)Œ∑(cid:62)t(x)(cid:1) dx is
where h(x1:n) =(cid:81)n

p(x | Œ∑) = h(x) exp(cid:0)Œ∑
p(x1:n | Œ∑) = h(x1:n) exp(cid:0)Œ∑
i=1 h(xi) and t(x1:n) =(cid:80)n

i=1 t(xi). Notice that once normalizing constants
are dropped  this density is dependent on the data only directly through the sufÔ¨Åcient statistics 
s = t(x1:n).
We will write exponential families more generally as p(x | Œ∏) to indicate the case when the natural
parameters Œ∑ = Œ∑(Œ∏) depend on a different parameter vector Œ∏.
Every exponential family distribution has a conjugate prior distribution p(Œ∏; Œª)[24] with hyperparam-
eters Œª. A conjugate prior has the property that  if it is used as the prior  then the posterior belongs
to the same family  i.e.  p(Œ∏ | x1:n; Œª) = p(Œ∏; Œª(cid:48)) for some Œª(cid:48) that depends only on Œª  n  and the
sufÔ¨Åcient statistics s. We write this function as Œª(cid:48) = Conjugate-Update(Œª  s  n); our methods are not
tied to the speciÔ¨Åc choice of conjugate prior  only that the posterior parameters can be calculated in
this form. See supplementary material for a general form of Conjugate-Update.

3.1 Release Algorithm: Noisy SufÔ¨Åcient Statistics

If privacy were not a concern  the Bayesian modeler would simply compute the sufÔ¨Åcient statistics
s = t(x1:n) and use them to update the posterior beliefs. However  to maintain privacy  the modeler
must access the sensitive data only through a randomized release mechanism A. As a result  in order
to obtain proper posterior beliefs the modeler must account for the randomization of the release
mechanism by performing inference.
We take the simple approach of releasing noisy sufÔ¨Åcient statistics via the Laplace mechanism  as
in [12  13  19]. SufÔ¨Åcient statistics are a natural quantity to release. They are an ‚Äúinformation
bottleneck‚Äù‚Äîa Ô¨Ånite-dimensional quantity that captures all the relevant information about Œ∏. The
i=1 t(xi) is a sum
over individuals  the sensitivity is ‚àÜs = maxx x(cid:48)‚ààRd (cid:107)t(x) ‚àí t(x(cid:48))(cid:107)1. When t(¬∑) is unbounded this
quantity becomes inÔ¨Ånite; we will modify the release mechanism so the sensitivity is Ô¨Ånite (Sec. 3.3).

released value is y = A(x1:n) ‚àº Lap(s  ‚àÜs/). Because s = t(x1:n) = (cid:80)n

3

3.2 Basic Inference Approach: Bounded SufÔ¨Åcient Statistics
The goal of the inference algorithm P is to compute p(Œ∏ | y). We Ô¨Årst develop the basic approach for
the simpler case when t(x) is bounded  and then extend both A and P to handle the unbounded case.
The full joint distribution of the probability model can be expressed as:

p(Œ∏  s  y) = p(Œ∏) p(s | Œ∏) p(y | s) 

(cid:82)

where p(Œ∏) = p(Œ∏; Œª) is a conjugate prior and the goal is to compute a representation of p(Œ∏ | y) ‚àù
s p(Œ∏  s  y)ds by integrating over the sufÔ¨Åcient statistics.
We will develop a Gibbs sampler to sample from this distribution. There are two main challenges.
First  the distribution p(s | Œ∏) is obtained by marginalizing over the data sample x1:n  and is usually
not known in closed form. We will address this with an asymptotically correct normal approximation.
Second  when resampling s within the Gibbs algorithm  we require the full conditional distribution of
s given the other variables  which is proportional to p(s|Œ∏)p(y | s). Care must be taken to make it
easy to sample from this conditional distribution. We address this via variable augmentation. We
discuss our approach to both challenges in detail below.
Normal approximation of p(s | Œ∏). The exact form of the sufÔ¨Åcient statistic distribution p(s | Œ∏)
is obtained by marginalizing over the data:

(cid:90)

p(s | Œ∏) =

p(x1:n | Œ∏)dx1:n 

t‚àí1(s)

‚àí1(s) :=(cid:8)x1:n : t(x1:n) = s(cid:9).

t

tractable full conditional. By the central limit theorem (CLT)  because s =(cid:80)

In general  the exact form of this distribution is not available. In some cases  it is‚Äîfor example
if x ‚àº Bernoulli(Œ∏) then s ‚àº Binomial(n  Œ∏)‚Äîbut even then it may not lead to a tractable full
conditional for s.
Properties of exponential families pave the way toward a general approach that always leads to a
i t(xi) is a sum of iid
random variables  it is asymptotically normal. It can be approximated as p(s | Œ∏) ‚âà N (s; n¬µ  nŒ£) 
where ¬µ = E[t(x)] and Œ£ = Var[t(x)] are the mean and variance of the sufÔ¨Åcient statistic of a
n (s ‚àí n¬µ) D‚àí‚Üí N (0  Œ£) [25]. The
1‚àö
single individual. This approximation is asymptotically correct:
quantities ¬µ and Œ£ can be computed using well-known properties of exponential families [25]:

¬µ = E[t(x)] =

‚àÇ
‚àÇŒ∑(cid:62) A(Œ∑) 

Œ£ = Var[t(x)] =

‚àÇ2

‚àÇŒ∑‚àÇŒ∑(cid:62) A(Œ∑) 

(1)

where Œ∑ = Œ∑(Œ∏) is the natural parameter.
Note that we will not use this approximation for Gibbs updates of Œ∏. Instead  we will compute the
conditional p(Œ∏ | s) using standard conjugacy formulas. In this sense  we maintain two views of the
joint distribution p(Œ∏  s)‚Äîwhen updating Œ∏  it is the standard exponential family model  which leads
to conjugate updates; when updating s  it is approximated as p(Œ∏)N (s; n¬µ  sŒ£)  which will lead to
simple updates when combined with a variable augmentation technique.
Variable augmentation for p(y | s). We seek a tractable form for the full conditional of s under
the normal approximation  which is the product of a normal density and a Laplace density:

p(s | Œ∏  y) ‚àù N (s; n¬µ  nŒ£) Lap(y; s  ‚àÜs/).

A similar situation arises in the Bayesian Lasso [22]  and we will employ the same variable aug-
mentation trick. A Laplace random variable z ‚àº Lap(u  b) can be written as a scale mixture of
normals by introducing a latent variable œÉ2 ‚àº Exp(1/(2b2))  i.e.  the distribution with density

1/(2b2) exp(cid:0)‚àíœÉ2/(2b2)(cid:1) and letting z ‚àº N (u  œÉ2). We apply this separately to each dimension of

the vector y so that:

(cid:32)

(cid:33)

j ‚àº Exp
œÉ2

2
2‚àÜ2
s

y ‚àº N(cid:0)s  diag(œÉ2)(cid:1).

 

4

Algorithm 1 Gibbs Sampler  Bounded ‚àÜs
1: Initialize Œ∏  s  œÉ2
2: repeat
3:
4:
5:

s ‚àº NormProduct(cid:0)n¬µ  nŒ£  y  diag(œÉ2)(cid:1)
(cid:17)

Œ∏ ‚àº p(Œ∏; Œª(cid:48)) where Œª(cid:48) = Conjugate-Update(Œª  s  n)
Calculate ¬µ = E[s] and Œ£ = Var[s] (e.g.  use Eq. (1))

(cid:16)

1/œÉ2

j ‚àº InverseGaussian



‚àÜs|y‚àís|   2

‚àÜ2
s

6:

Subroutine NormProduct
1: input: ¬µ1  Œ£1  ¬µ2  Œ£2
1 + Œ£‚àí1

2: Œ£3 =(cid:0)Œ£‚àí1
(cid:0)Œ£‚àí1

(cid:1)‚àí1
1 ¬µ1 + Œ£‚àí1

3: ¬µ3 = Œ£3
4: return: N (¬µ3  Œ£3)

2

2 ¬µ2

(cid:1)

The Gibbs Sampler. After the normal approxi-
mation and variable augmentation  the generative
process is as shown to the right. The Ô¨Ånal Gibbs
sampling algorithm is shown in Algorithm 1. Note
that the update for Œ∏ is based on conjugacy in the ex-
act distribution p(Œ∏  s)  while the update for s uses
the density of the generative process to the right  so
that p(s | Œ∏  œÉ2  y) ‚àù p(s | Œ∏) p(y | œÉ2  s)  which
is a product of two normal densities

(cid:19)
y ‚àº N(cid:0)s  diag(œÉ2)(cid:1)
N (s; n¬µ  nŒ£)N(cid:0)y; s  diag(œÉ2)(cid:1) ‚àù N (s; ¬µs  Œ£s) 

Œ∏ ‚àº p(Œ∏; Œª)
s ‚àº N (n¬µ  nŒ£)
j ‚àº Exp
œÉ2

(cid:18) 2

2‚àÜ2
s

for all j

(cid:112)

where ¬µs and Œ£s are are deÔ¨Åned in Algorithm 1 [26].
lows Park and Casella [22];

v/(2œÄx3) exp(cid:0)‚àív(x ‚àí m)2/(2m2x)(cid:1). Full derivations are given in the supplement.

fol-
the inverse Gaussian density is InverseGaussian(x; m  v) =

The update for œÉ2

3.3 Unbounded SufÔ¨Åcient Statistics and Truncated Exponential Families

The Laplace mechanism does not apply when the sufÔ¨Åcient statistics are unbounded  because ‚àÜs =
maxx y (cid:107)t(x) ‚àí t(y)(cid:107)1 = ‚àû. Thus  we need a new release mechanism A and inference algorithm P.
We present a solution for the case when x is univariate. All elements of the solution can generalize
to higher dimensions  except that one step will have running time that is exponential in d; we leave
improvement of this to future work and focus on the simpler univariate case.
Release mechanism. Our solution is to truncate the support of the (now univariate) p(x | Œ∏)
to x ‚àà [a  b]  where a and b are Ô¨Ånite bounds provided by the modeler. If the modeler cannot
select bounds a priori  they may be selected privately as a preliminary step using a variant of the
exponential mechanism (see PrivateQuantile in Smith [27]).3 Then  given truncation bounds 
the data owner redacts individuals where xi /‚àà [a  b] and reports the truncated sufÔ¨Åcient statistics
i=1 1[a b](xi) ¬∑ t(xi) where 1S(x) is the indicator function of the set S. The sensitivity of ÀÜs
is now ‚àÜÀÜs = maxx y‚ààR (cid:107)ÀÜt(x) ‚àí ÀÜt(y)(cid:107)1 where ÀÜt(x) = 1[a b](x) t(x). An easy upper bound for this
quantity (see supplement) is:

ÀÜs =(cid:80)n

(cid:12)(cid:12)tj(x) ‚àí tj(y)(cid:12)(cid:12)(cid:111)

 

‚àÜÀÜs ‚â§ d(cid:88)

j=1

(cid:110)

max

max
x‚àà[a b]

|tj(x)|  max
x y‚àà[a b]

where tj(x) is the jth component of the sufÔ¨Åcient statistics. The bounds [a  b] will be selected so this
quantity is bounded. The released value is y ‚àº Lap(ÀÜs  ‚àÜÀÜs/).

Inference: Truncated Exponential Family. Several new challenges arise for inference. The
quantity ÀÜs is no longer a sufÔ¨Åcient statistic for the model p(x | Œ∏)  and we will need new insights to
understand p(ÀÜs | Œ∏) and p(Œ∏ | ÀÜs). Since ÀÜs is a sum over individuals where xi ‚àà [a  b]  it will be useful
to examine the probability of the event x ‚àà [a  b] as well as the conditional distribution of x given this
event. To facilitate a general development  assume a generic truncation interval [v  w]  not necessarily
3Selecting truncation bounds will consume some of the privacy budget and modify the release mechanism A.

We do not consider inference with respect to this part of the release mechanism.

5

‚úì<latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit>ùë†ùë¶ùúé$equal to [a  b]. Let F (x; Œ∏) =(cid:82) x

(cid:16)

(cid:17)

(cid:90) w

h(x) exp(cid:0)Œ∑T t(x)(cid:1) dx.

(2)

‚àí‚àû p(x | Œ∏)dx be the CDF of the original (univariate) exponential
family model. It is clear that Pr(x ‚àà [v  w]) = F (w; Œ∏) ‚àí F (v; Œ∏). The conditional distribution of x
given x ‚àà [v  w] is a truncated exponential family  which  in its natural parameterization is:

ÀÜp(x | Œ∑) = 1[v w](x) h(x) exp

Œ∑T t(x) ‚àí ÀÜA(Œ∑)

 

ÀÜA =

v

Note that this is still an exponential family model (with a modiÔ¨Åed base measure)  and all of the
standard results apply  such as the existence of a conjugate prior and the formulas in Eq. (1) for the
mean and variance of t(x) under the truncated distribution.
Random sum CLT for p(ÀÜs | Œ∏). We would like to again apply an asymptotic normal approximation
for ÀÜs  but we do not know how many individuals fall within the truncation bounds. The ‚Äúrandom sum
CLT‚Äù of Robbins [28] applies to the setting where the number of terms in the sum is itself a random
k=1 t(xik )  where {i1  . . .   iN} is the set of indices
of individuals with data inside the truncation bounds  i.e.  the indices such that xik ‚àà [v  w]. The
number N is now a random variable distributed as N ‚àº Binom(n  q)  where q = F (w; Œ∏) ‚àí F (v; Œ∏).
Proposition 1. Let ÀÜ¬µ = E ÀÜp[t(x)] and ÀÜŒ£ = Var ÀÜp[t(x)] be the mean and variance of t(x) in the
k=1 t(xik ) is asymptotically normal with mean and

variable. The sum can be rewritten as ÀÜs =(cid:80)N
truncated exponential family. Then ÀÜs = (cid:80)N

variance:

m := E[ÀÜs] = E[N ]ÀÜ¬µ = nq ÀÜ¬µ 
V := Var(ÀÜs) = E[N ] ÀÜŒ£ + Var[N ]ÀÜ¬µÀÜ¬µ

(cid:0)ÀÜs ‚àí m(cid:1) D‚àí‚Üí N (0  ¬ØŒ£) as n ‚Üí ‚àû  where ¬ØŒ£ = V/n = q ÀÜŒ£ + q(1 ‚àí q)ÀÜ¬µÀÜ¬µ(cid:62).

(cid:62) = nq ÀÜŒ£ + nq(1 ‚àí q)ÀÜ¬µÀÜ¬µ
(cid:62)

.

SpeciÔ¨Åcally  1‚àö
n

Proof. Each term of the sum has mean ÀÜ¬µ and variance ÀÜŒ£  and the number of terms is N ‚àº
Binom(n  q). The result follows from Robbins [28].

Computing ÀÜ¬µ and ÀÜŒ£ by automatic differentiation (autodiff). To use the normal approximation
we need to compute ÀÜ¬µ and ÀÜŒ£.
Lemma 1. Let p(x | Œ∏) be a univariate exponential family model and let ÀÜp(x | Œ∏) be the correspond-
ing exponential family model truncated to generic interval [v  w]. Then

ÀÜ¬µ = E ÀÜp[t(x)] = Ep[t(x)] +

(3)

‚àÇ

‚àÇŒ∑T log(cid:0)F (w; Œ∑) ‚àí F (v; Œ∑)(cid:1)
‚àÇŒ∑‚àÇŒ∑T log(cid:0)F (w; Œ∑) ‚àí F (v; Œ∑)(cid:1)

‚àÇ2

ÀÜŒ£ = Var ÀÜp[t(x)] = Varp[t(x)] +

Proof. It is straightforward to derive from Eq. (2) that ÀÜA(Œ∑) = A(Œ∑) + log(cid:0)F (w; Œ∑)‚àí F (v; Œ∑)(cid:1). The

(4)

result follows from applying Eq. (1) to this expression for ÀÜA(Œ∑). See the supplement for derivation of
ÀÜA(Œ∑) and proof of this lemma.

We will use Equations (3) and (4) to compute ÀÜ¬µ and ÀÜŒ£ by using autodiff to compute the desired
derivatives. If the mean and variance Ep[t(x)] and Varp[t(x)] of the untruncated distribution are not
known  we can apply autodiff to compute them as well using Eq. (1).
When x is multivariate  analogous expressions can be derived for ÀÜ¬µ and ÀÜŒ£. The adjustment factors will
include multivariate CDFs  with a number of terms that grow exponentially in d. This is currently the
main limitation in applying our methods to multivariate models with unbounded sufÔ¨Åcient statistics.
Conjugate updates for p(Œ∏ | ÀÜs). The Ô¨Ånal issue is the distribution p(Œ∏ | ÀÜs)  which is no longer
characterized by conjugacy because ÀÜs are not the full sufÔ¨Åcient statistics. We again turn to variable
i=1 1[b ‚àû]t(xi) be the sufÔ¨Åcient statistics
for the individuals that fall in the lower portion [‚àí‚àû  a] and upper portion [b ‚àû] of the support of x 
respectively. We will instantiate ÀÜs(cid:96) and ÀÜsu as latent variables and model their distributions using the

i=1 1[‚àí‚àû a]t(xi) and ÀÜsu =(cid:80)n

augmentation. Let ÀÜs(cid:96) =(cid:80)n

6

Algorithm 2 Gibbs Sampler  Unbounded ‚àÜs
1: Initialize Œ∏  ÀÜs  œÉ2  a  b
2: [v(cid:96)  w(cid:96)] ‚Üê [‚àí‚àû  a]
3: [vc  wc] ‚Üê [a  b]
4: [vu  wu] ‚Üê [b ‚àû]
5: repeat
6:
7:
8:
9:
10:

mr  Vr ‚Üê RS-CLT(Œ∏  vr  wr) for r ‚àà {(cid:96)  c  u}
m(cid:48)
s ‚àº N (m(cid:96) + m(cid:48)
Œ∏ ‚àº p(Œ∏; Œª(cid:48)) where Œª(cid:48) = Conjugate-Update(Œª  s  n)
Recalculate mc and Vc  then draw ÀÜsc ‚àº N (mc  Vc)
1/œÉ2

c ‚Üê NormProduct(cid:0)mc  Vc  y  diag(cid:0)œÉ2(cid:1)(cid:1)

c + mu  V(cid:96) + V(cid:48)

j ‚àº InverseGaussian

c + Vu)

c  V(cid:48)

(cid:16)

(cid:17)

11:



‚àÜÀÜs|y‚àíÀÜsc|   2

‚àÜ2
ÀÜs

Algorithm 3 RS-CLT
1: input: Œ∏  v  w
2: q ‚Üê F (b; w) ‚àí F (a; v)
3: ÀÜ¬µ  ÀÜŒ£ ‚Üê autodiff of Eqns. 3  4
4: m ‚Üê nq
5: V ‚Üê nq ÀÜŒ£ + nq(1 ‚àí q)ÀÜ¬µÀÜ¬µ(cid:62)
6: return: m  V

12: until

random sum CLT approximation from Prop. 1 and Lemma 1 (but with different truncation bounds).
Let ÀÜsc = ÀÜs be the sufÔ¨Åcient statistics for the ‚Äúcenter‚Äù portion  and deÔ¨Åne the three truncation intervals
as [v(cid:96)  w(cid:96)] = [‚àí‚àû  a]  [vc  wc] = [a  b] and [vu  wu] = [b ‚àû]. The full sufÔ¨Åcient statistics are equal
to s = ÀÜs(cid:96) + ÀÜsc + ÀÜsu. Conditioned on all other variables  each component is multivariate normal  so
the sum s is also multivariate normal. We can therefore sample s and then sample from p(Œ∏ | s) using
conjugacy. We will also need to draw ÀÜsc separately to be used to update œÉ2.

The Gibbs Sampler. The (approximate) generative process in the unbounded case is:

Œ∏ ‚àº p(Œ∏; Œª) 

ÀÜsr ‚àº N(cid:0)mr  Vr)  for r ‚àà {(cid:96)  c  u} where mr  Vr = RS-CLT(Œ∏  vr  wr)
(cid:19)
y ‚àº N(cid:0)ÀÜsc  diag(œÉ2)(cid:1).

(cid:18) 2

j ‚àº Exp
œÉ2

for all j  

2‚àÜ2
ÀÜs

The Gibbs sampler to sample from this distribution is given in Algorithm 2. Note that in Line 8
we employ rejection sampling in which sufÔ¨Åcient statistics are sampled until the values drawn are
valid for the given data model  e.g.  s must be positive for the binomial distribution. The RS-CLT
algorithm to compute parameters of the random sum CLT is shown in Algorithm 3.

4 Experiments

We design experiments to measure the calibration and utility of our method for posterior inference.
We conduct experiments for the binomial model with beta prior  the multinomial model with Dirichlet
prior  and the exponential model with gamma prior. The last model is unbounded and requires
truncation; we set the bounds to keep the middle 95% of individuals  which is reasonable to assume
known a priori for some cases  such as modeling human height.

Methods. We run our Gibbs sampler for 5000 iterations after 2000 burnin iterations (see supple-
mentary material for convergence results)  which we compare to two baselines. The Ô¨Årst method
uses the same release mechanism as our Gibbs sampler and performs conjugate updates using the
noisy sufÔ¨Åcient statistics [12  13]. This method converges to the true posterior as n ‚Üí ‚àû because
the Laplace noise will eventually become negligible compared to sampling variability [12]. However 
the noise is not negligible for moderate n; we refer to this method as ‚Äúnaive‚Äù. For truncated models
we allow the naive method to ‚Äúcheat‚Äù by accessing the noisy untruncated sufÔ¨Åcient statistics s. Thus
the method is not private  and receives strictly more information than our Gibbs sampler  but with
the same magnitude noise. This allows us to demonstrate miscalibration without highly technical
modiÔ¨Åcations to the baseline method to be able to deal with truncated sufÔ¨Åcient statistics.

7

The second baseline is a version of the one-posterior sampling (OPS) mechanism [11‚Äì13]  which
employs the exponential mechanism [29] to release samples from a privatized posterior. We release
100 samples using the method of [12]  each with ops = /100  such that the entire algorithm
achieves -differential privacy. Private MCMC sampling [11] is a more sophisticated method to
release multiple samples from a privatized posterior and could potentially make better use of the
privacy budget; however  private MCMC will also necessarily be miscalibrated  and only achieves the
weaker privacy guarantee of (  Œ¥)-differential privacy for Œ¥ > 0  so would not be direct comparable
to our method. OPS serves as a suitable baseline that achieves -differential privacy. We include OPS
only for experiments on the binomial model  for which it requires the support of Œ∏ to be truncated to
[a0  1 ‚àí a0] where a0 > 0. We set a0 = 0.1.
We also include a non-private posterior for comparison  which performs conjugate updates using the
non-noisy sufÔ¨Åcient statistics.

Evaluation. We evaluate both the calibration and utility of the posterior. For calibration we adapt a
method of Cook et al. [30]: the idea is to draw iid samples (Œ∏i  xi) from the joint model p(Œ∏)p(x | Œ∏) 
and conduct posterior inference in each trial. Let Fi(Œ∏) be the CDF of the true posterior p(Œ∏ | xi)
in trial i. Then we know that Ui = Fi(Œ∏i) is uniformly distributed  because Œ∏i ‚àº p(Œ∏ | xi) (see
supplementary material). In other words  the actual parameter Œ∏i is equally likely to land at any
quantile of the posterior. To test the posterior inference procedure  we instead compute Ui as the
quantile at which Œ∏i lands within a set of samples from the approximate posterior. After M trials
of the whole procedure we test for uniformity of U1:M using the Kolmogorov-Smirnov goodness-
of-Ô¨Åt test [31]  which measures the maximum distance between the empirical CDF of U1:M and the
uniform CDF; lower values are better and zero corresponds to perfect uniformity. We also visualize
the empirical CDFs to assess calibration qualitatively.
Higher utility of a private posterior is indicated by closeness to the non-private posterior  which we
measure with maximum mean discrepancy (MMD)  a kernel-based statistical test to determine if two
sets of samples are drawn from different distributions [32]. Given m i.i.d. samples (p  q) ‚àº P √ó Q 
an unbiased estimate of the MMD is

(cid:88)m

i(cid:54)=j

MMD2(P  Q) =

1

m(m ‚àí 1)

(k(pi  pj) + k(qi  qj) ‚àí k(pi  qj) ‚àí k(pj  qi))  

where k is a continuous kernel function; we use a standard normal kernel. The higher the value the
more likely the two samples are drawn from different distributions.

Results. Figure 1a shows the results for three models and varying n and . Our method (Gibbs)
achieves the same calibration level as non-private posterior inference for all settings. The naive
method ignores noise and is too conÔ¨Ådent about parameter values implied by treating the noisy
sufÔ¨Åcient statistics as true ones; it is only well-calibrated with increasing n and  when noise becomes
negligible relative to population size. OPS is not calibrated because it samples from an over-dispersed
version of p(Œ∏ | x).
Figure 1b shows the empirical CDF plots for n = 1000 and  = 0.01. Our method and the non-private
method are both perfectly calibrated. The naive method‚Äôs over-conÔ¨Ådence in the wrong sufÔ¨Åcient
statistics causes its posterior to usually be too tight at the wrong value; thus the true parameter always
lies in a tail of the approximate posterior  so too much mass is placed near 0 and 1. OPS shows the
opposite behavior: its posterior is always too diffuse  so the true parameter lies close to the middle.
For multinomial we show measures only for the parameter of the Ô¨Årst category  but results hold for
all categories.
Figure 1c shows the MMD test statistic between each method and the non-private posterior  used as
a measure of utility. Our method consistently achieves utility at least as good as the naive method
for binomial and multinomial models. We omit OPS  which is never calibrated. For the exponential
model (not shown) we did not obtain conclusive utility comparisons due to the lack of a naive baseline
that properly handles truncation; the ‚Äúcheating‚Äù naive method from our calibration experiments
sometimes attains higher utility than our method  and sometimes lower  but this comparison is not
meaningful because it receives strictly more information.

8

(a)

(b)

(c)

Figure 1: (a) Calibration as Kolmogorov-Smirnov statistic vs. number of individuals at  =
[0.01  0.10] for binomial  multinomial  and exponential models.
(b) Empirical CDF plots at
(n = 1000;  = 0.01) for binomial  multinomial  and exponential models. (c) Utility as MMD
with non-private posterior vs. number of individuals at  = [0.01  0.10] for binomial and multinomial
models.

5 Conclusion

We presented a Gibbs sampling approach for private posterior inference in exponential family models.
Rather than trying to approximate the posterior of p(Œ∏ | x1:n)  we divide our procedure into a private
release mechanism y = A(x1:n) and an inference algorithm P that computes p(Œ∏ | y). The release
mechanism is designed to facilitate inference. We develop a general-purpose Gibbs sampler that
applies to any exponential family model that has bounded sufÔ¨Åcient statistics; a truncated version
applies to univariate models with unbounded sufÔ¨Åcient statistics. The Gibbs sampler uses general
properties of exponential families to approximate the distribution of the sufÔ¨Åcient statistics  and
therefore avoids the need to reason about individuals. Promising lines of future work are to develop
efÔ¨Åcient methods for multivariate exponential families with unbounded sufÔ¨Åcient statistics  and to
develop methods for conditional models based on exponential families  such as generalized linear
models.

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant Nos.
1522054 and 1617533.

References
[1] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference  pages 265‚Äì284.
Springer  2006.

9

0.000.250.50KS stat.binomialmultinomialŒµ=0.01exponential102103104n0.000.250.50KS stat.102103104n102103104nŒµ=0.10Non-Priv.GibbsNaiveOPS0.00.51.0u0.00.51.0Pr(U‚â§u)binomial0.00.51.0umultinomial0.00.51.0uexponentialNon-Priv.GibbsNaiveOPS0.00.20.40.6MMDbinomial102103104n0.00.20.40.6MMDŒµ=0.01Œµ=0.100.00.20.40.6MMDmultinomial102103104n0.00.20.40.6MMDŒµ=0.01Œµ=0.10[2] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances

in Neural Information Processing Systems  pages 289‚Äì296  2009.

[3] Benjamin I.P. Rubinstein  Peter L. Bartlett  Ling Huang  and Nina Taft. Learning in a large func-
tion space: Privacy-preserving mechanisms for SVM learning. arXiv preprint arXiv:0911.5708 
2009.

[4] Shiva Prasad Kasiviswanathan  Homin K. Lee  Kobbi Nissim  Sofya Raskhodnikova  and Adam

Smith. What can we learn privately? SIAM Journal on Computing  40(3):793‚Äì826  2011.

[5] Mart√≠n Abadi  Andy Chu  Ian Goodfellow  H. Brendan McMahan  Ilya Mironov  Kunal Talwar 
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security  pages 308‚Äì318. ACM  2016.

[6] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empirical

risk minimization. Journal of Machine Learning Research  12(Mar):1069‚Äì1109  2011.

[7] Daniel Kifer  Adam Smith  and Abhradeep Thakurta. Private convex empirical risk minimization

and high-dimensional regression. Journal of Machine Learning Research  1(41):3‚Äì1  2012.

[8] Prateek Jain and Abhradeep Thakurta. Differentially private learning with kernels. ICML (3) 

28:118‚Äì126  2013.

[9] Raef Bassily  Adam Smith  and Abhradeep Thakurta. Private empirical risk minimization:
EfÔ¨Åcient algorithms and tight error bounds. In Foundations of Computer Science (FOCS)  2014
IEEE 55th Annual Symposium on  pages 464‚Äì473. IEEE  2014.

[10] Christos Dimitrakakis  Blaine Nelson  Aikaterini Mitrokotsa  and Benjamin I.P. Rubinstein.
Robust and private Bayesian inference. In International Conference on Algorithmic Learning
Theory  pages 291‚Äì305. Springer  2014.

[11] Yu-Xiang Wang  Stephen Fienberg  and Alex Smola. Privacy for free: Posterior sampling
and stochastic gradient Monte Carlo. In Proceedings of the 32nd International Conference on
Machine Learning (ICML-15)  pages 2493‚Äì2502  2015.

[12] James Foulds  Joseph Geumlek  Max Welling  and Kamalika Chaudhuri. On the theory and
practice of privacy-preserving Bayesian data analysis. In Proceedings of the Thirty-Second
Conference on Uncertainty in ArtiÔ¨Åcial Intelligence  pages 192‚Äì201  2016.

[13] Zuhe Zhang  Benjamin I.P. Rubinstein  and Christos Dimitrakakis. On the differential privacy

of Bayesian inference. In Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence  2016.

[14] Joseph Geumlek  Shuang Song  and Kamalika Chaudhuri. Renyi differential privacy mecha-
nisms for posterior sampling. In Advances in Neural Information Processing Systems  pages
5295‚Äì5304  2017.

[15] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Found.

and Trends in Theoretical Computer Science  2014.

[16] Oliver Williams and Frank McSherry. Probabilistic inference and differential privacy.

Advances in Neural Information Processing Systems  pages 2451‚Äì2459  2010.

In

[17] Vishesh Karwa  Aleksandra B. Slavkovi¬¥c  and Pavel Krivitsky. Differentially private exponential
random graphs. In International Conference on Privacy in Statistical Databases  pages 143‚Äì155.
Springer  2014.

[18] Vishesh Karwa and Aleksandra B. Slavkovi¬¥c. Inference using noisy degrees: Differentially

private beta-model and synthetic graphs. The Annals of Statistics  44(1):87‚Äì112  2016.

[19] Garrett Bernstein  Ryan McKenna  Tao Sun  Daniel Sheldon  Michael Hay  and Gerome Miklau.
Differentially private learning of undirected graphical models using collective graphical models.
In International Conference on Machine Learning  pages 478‚Äì487  2017.

10

[20] Aaron Schein  Zhiwei Steven Wu  Mingyuan Zhou  and Hanna Wallach. Locally private
Bayesian inference for count models. NIPS 2017 Workshop: Advances in Approximate Bayesian
Inference  2018.

[21] R.A. Fisher. On the mathematical foundations of theoretical statistics. Phil. Trans. R. Soc. Lond.

A  222(594-604):309‚Äì368  1922.

[22] Trevor Park and George Casella. The Bayesian lasso. Journal of the American Statistical

Association  103(482):681‚Äì686  2008.

[23] Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. In Proceedings of the
2011 ACM SIGMOD International Conference on Management of data  pages 193‚Äì204. ACM 
2011.

[24] Persi Diaconis and Donald Ylvisaker. Conjugate priors for exponential families. The Annals of

statistics  pages 269‚Äì281  1979.

[25] Peter J. Bickel and Kjell A. Doksum. Mathematical statistics: basic ideas and selected topics 

volume I  volume 117. CRC Press  2015.

[26] Kaare Brandt Petersen and Michael Syskind Pedersen. The matrix cookbook. Technical

University of Denmark  7(15):510  2008.

[27] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In
Proceedings of the Forty-third Annual ACM Symposium on Theory of Computing  pages 813‚Äì
822  2011.

[28] Herbert Robbins. The asymptotic distribution of the sum of a random number of random

variables. Bulletin of the American Mathematical Society  54(12):1151‚Äì1161  1948.

[29] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Foundations
of Computer Science  2007. FOCS‚Äô07. 48th Annual IEEE Symposium on  pages 94‚Äì103. IEEE 
2007.

[30] Samantha R. Cook  Andrew Gelman  and Donald B. Rubin. Validation of software for Bayesian
models using posterior quantiles. Journal of Computational and Graphical Statistics  15(3):
675‚Äì692  2006.

[31] Frank J. Massey Jr. The Kolmogorov-Smirnov test for goodness of Ô¨Åt. Journal of the American

Statistical Association  46(253):68‚Äì78  1951.

[32] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Sch√∂lkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723‚Äì773 
2012.

11

,Garrett Bernstein
Daniel Sheldon