2018,Differentially Private Bayesian Inference for Exponential Families,The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods  it gives properly calibrated posterior beliefs in the non-asymptotic data regime.,Differentially Private Bayesian Inference for

Exponential Families

Garrett Bernstein

College of Information and Computer Sciences

University of Massachusetts Amherst

Amherst  MA 01002

gbernstein@cs.umass.edu

Daniel Sheldon

College of Information and Computer Sciences

University of Massachusetts Amherst

Amherst  MA 01002

sheldon@cs.umass.edu

Abstract

The study of private inference has been sparked by growing concern regarding the
analysis of data when it stems from sensitive sources. We present the ﬁrst method
for private Bayesian inference in exponential families that properly accounts for
noise introduced by the privacy mechanism. It is efﬁcient because it works only
with sufﬁcient statistics and not individual data. Unlike other methods  it gives
properly calibrated posterior beliefs in the non-asymptotic data regime.

1

Introduction

Differential privacy is the dominant standard for privacy [1]. A randomized algorithm that satisﬁes
differential privacy offers protection to individuals by guaranteeing that its output is insensitive to
changes caused by the data of any single individual entering or leaving the data set. An algorithm can
be made differentially private by applying one of several general-purpose mechanisms to randomize
the computation in an appropriate way  for example  by adding noise calibrated to the sensitivity
of the quantity being computed  where sensitivity captures how much the quantity depends on any
individual’s data [1]. Due to the obvious importance of protecting individual privacy while drawing
population level inferences from data  differentially private algorithms have been developed for a
broad range of machine learning tasks [2–9].
There is a growing interest in private methods for Bayesian inference [10–14]. In Bayesian inference 
a modeler selects a prior distribution p(θ) over some parameter  observes data x that depends
probabilistically on θ through a model p(x | θ)  and then reasons about θ through the posterior
distribution p(θ | x)  which quantiﬁes updated beliefs and uncertainty about θ after observing x.
Bayesian inference is a core machine learning task and there is an obvious need to be able to conduct
it in a way that protects privacy when x is sensitive. Additionally  recent work has identiﬁed surprising
connections between sampling from posterior distributions and differential privacy—for example 
a single perfect sample from p(θ | x) satisﬁes differential privacy for some setting of the privacy
parameter [10–13].
An “obvious” way to conduct private Bayesian inference is to privatize the computation of the
posterior  that is  to design a differentially private algorithm A that outputs y = A(x) with the goal
that y ≈ p(θ | x) is a privatized representation of the posterior. However  using y directly as “the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

posterior” will not correctly quantify beliefs  because the Bayesian modeler never observes x  she
observes y; her posterior beliefs are now quantiﬁed by p(θ | y).
This paper will take a different approach to private Bayesian inference by designing a pair of
algorithms: The release mechanism A computes a private statistic y = A(x) of the input data; the
inference algorithm P computes p(θ | y). These algorithms should satisfy the following criteria:
• Privacy. The release mechanism A is differentially private. By the post-processing property of
• Calibration. The inference algorithm P can efﬁciently compute or approximate the correct
• Utility. Informally  the statistic y should capture “as much information as possible” about x so

differential privacy [15]  all further computations are also private.
posterior  p(θ | y) (see Section 4 for our process to measure calibration).
that p(θ | y) is “close” to p(θ | x) (see Section 4 for our process to measure utility).

One challenge is computational efﬁciency. The exact posterior p(θ | y) ∝ (cid:82) p(θ)p(x1:n |

Importantly  the release mechanism A is public  so the distribution p(y | x) is known. Williams and
McSherry ﬁrst suggested conducting inference on the output of a differentially private algorithm and
showed how to do this for the factored exponential mechanism [16]; see also [17–20].
Our work focuses speciﬁcally on Bayesian inference when the private data X = x1:n is an iid sample
of (publicly known) size n from an exponential family model p(xi | θ). Exponential families include
many of the most familiar parametric probability models. We will adopt a straightforward release
mechanism where the Laplace mechanism [1] is used to release noisy sufﬁcient statistics y [12  19] 
which are a ﬁnite-dimensional quantity that capture all the information about θ [21].
The technical challenge is then to develop an efﬁcient general-purpose inference algorithm P.
θ)p(y|x1:n)dx1:n integrates over all possible data sets [16]  which is intractable to do directly
for large n. We integrate instead over the sufﬁcient statistics s  which have ﬁxed dimension and
completely characterize the posterior; furthermore  since they are a sum over individuals  p(s | θ)
is asymptotically normal. We develop an efﬁcient Gibbs sampler that uses a normal approximation
for s together with variable augmentation to model the Laplace noise in a way that yields simple
updates [22].
A second challenge is that the sufﬁcient statistics may be unbounded  which makes their release
incompatible with the Laplace mechanism. We address this by imposing truncation bounds and
only computing statistics from data that fall within the bounds. We show how to use automatic
differentiation and a “random sum” central limit theorem to compute the parameters of the normal
approximation p(s | θ) for a truncated exponential family when the number of individuals that fall
within the truncation bounds is unknown.
Our overall contribution is the pairing of an existing simple release mechanism A with a novel 
efﬁcient  and general-purpose Gibbs sampler P that meets the criteria outlined above for private
Bayesian inference in any univariate exponential family or multivariate exponential family with
bounded sufﬁcient statistics.1 We show empirically that when compared with competing methods 
ours is the only one that provides properly calibrated beliefs about θ in the non-asymptotic regime 
and that it provides good utility compared with other private Bayesian inference approaches.

2 Differential Privacy

Differential privacy requires that an individual’s data has a limited effect on the algorithm’s behavior.
In our setting  a data set X = x1:n := (x1  . . .   xn) consists of records from n individuals  where
xi ∈ Rd is the data of the ith individual. We will assume n is known. Differential privacy reasons
about the hypothesis that one individual chooses not to remove their data from the data set  and their
record is replaced by another one.2 Let nbrs(X) denote the set of data sets that differ from X by
exactly one record—i.e.  if X(cid:48) ∈ nbrs(X)  then X(cid:48) = (x1:i  x(cid:48)

i  xi+1:n) for some i.

1There are remaining technical challenges for multivariate models with unbounded sufﬁcient statistics that

we leave for future work.

2This variant assumes n remains ﬁxed  which is sometimes called bounded differential privacy [23].

2

Deﬁnition 1 (Differential Privacy; Dwork et al. [1]). A randomized algorithm A satisﬁes -differential
privacy if for any input X  any X(cid:48) ∈ nbrs(X) and any subset of outputs O ⊆ Range(A) 

Pr[A(X) ∈ O] ≤ exp() Pr[A(X

(cid:48)) ∈ O].

the Laplace mechanism outputs the random variable L(X) ∼ Lap(cid:0)f (X)  ∆f /(cid:1) from the Laplace

We achieve differential privacy by injecting noise into statistics that are computed on the data. Let f
be any function that maps datasets to Rd. The amount of noise depends on the sensitivity of f.
Deﬁnition 2 (Sensitivity). The sensitivity of a function f is ∆f = maxX X(cid:48)∈nbrs(X) (cid:107)f (X) −
f (X(cid:48))(cid:107)1.
We drop the subscript f when it is clear from context. Our approach achieves differential privacy
through the application of the Laplace mechanism.
Deﬁnition 3 (Laplace Mechanism; Dwork et al. [1]). Given a function f that maps data sets to Rm 
distribution  which has density Lap(z; u  b) = (2b)−m exp (−(cid:107)z − u(cid:107)1/b). This corresponds to
adding zero-mean independent noise ui ∼ Lap(0  ∆f /) to each component of f (X).
A ﬁnal important property of differential privacy is post-processing [15]; if an algorithm A is -
differentially private  then any algorithm that takes as input only the output of A  and does not use
the original data set X  is also -differentially private.

3 Private Bayesian Inference in Exponential Families

(cid:62)

t(x) − A(η)(cid:1)  
t(x1:n) − nA(η)(cid:1)  

(cid:62)

the log-partition function  and h(x) is the base measure. The density of the full data is

We consider the canonical setting of Bayesian inference in an exponential family. The modeler
posits a prior distribution p(θ)  assumes the data x1:n is an iid sample from an exponential family
model p(x | θ)  and wishes to compute the posterior p(θ | x1:n). An exponential family in natural
parameterization has density

where η are the natural parameters  t(x) is the sufﬁcient statistic  A(η) =(cid:82) h(x) exp(cid:0)η(cid:62)t(x)(cid:1) dx is
where h(x1:n) =(cid:81)n

p(x | η) = h(x) exp(cid:0)η
p(x1:n | η) = h(x1:n) exp(cid:0)η
i=1 h(xi) and t(x1:n) =(cid:80)n

i=1 t(xi). Notice that once normalizing constants
are dropped  this density is dependent on the data only directly through the sufﬁcient statistics 
s = t(x1:n).
We will write exponential families more generally as p(x | θ) to indicate the case when the natural
parameters η = η(θ) depend on a different parameter vector θ.
Every exponential family distribution has a conjugate prior distribution p(θ; λ)[24] with hyperparam-
eters λ. A conjugate prior has the property that  if it is used as the prior  then the posterior belongs
to the same family  i.e.  p(θ | x1:n; λ) = p(θ; λ(cid:48)) for some λ(cid:48) that depends only on λ  n  and the
sufﬁcient statistics s. We write this function as λ(cid:48) = Conjugate-Update(λ  s  n); our methods are not
tied to the speciﬁc choice of conjugate prior  only that the posterior parameters can be calculated in
this form. See supplementary material for a general form of Conjugate-Update.

3.1 Release Algorithm: Noisy Sufﬁcient Statistics

If privacy were not a concern  the Bayesian modeler would simply compute the sufﬁcient statistics
s = t(x1:n) and use them to update the posterior beliefs. However  to maintain privacy  the modeler
must access the sensitive data only through a randomized release mechanism A. As a result  in order
to obtain proper posterior beliefs the modeler must account for the randomization of the release
mechanism by performing inference.
We take the simple approach of releasing noisy sufﬁcient statistics via the Laplace mechanism  as
in [12  13  19]. Sufﬁcient statistics are a natural quantity to release. They are an “information
bottleneck”—a ﬁnite-dimensional quantity that captures all the relevant information about θ. The
i=1 t(xi) is a sum
over individuals  the sensitivity is ∆s = maxx x(cid:48)∈Rd (cid:107)t(x) − t(x(cid:48))(cid:107)1. When t(·) is unbounded this
quantity becomes inﬁnite; we will modify the release mechanism so the sensitivity is ﬁnite (Sec. 3.3).

released value is y = A(x1:n) ∼ Lap(s  ∆s/). Because s = t(x1:n) = (cid:80)n

3

3.2 Basic Inference Approach: Bounded Sufﬁcient Statistics
The goal of the inference algorithm P is to compute p(θ | y). We ﬁrst develop the basic approach for
the simpler case when t(x) is bounded  and then extend both A and P to handle the unbounded case.
The full joint distribution of the probability model can be expressed as:

p(θ  s  y) = p(θ) p(s | θ) p(y | s) 

(cid:82)

where p(θ) = p(θ; λ) is a conjugate prior and the goal is to compute a representation of p(θ | y) ∝
s p(θ  s  y)ds by integrating over the sufﬁcient statistics.
We will develop a Gibbs sampler to sample from this distribution. There are two main challenges.
First  the distribution p(s | θ) is obtained by marginalizing over the data sample x1:n  and is usually
not known in closed form. We will address this with an asymptotically correct normal approximation.
Second  when resampling s within the Gibbs algorithm  we require the full conditional distribution of
s given the other variables  which is proportional to p(s|θ)p(y | s). Care must be taken to make it
easy to sample from this conditional distribution. We address this via variable augmentation. We
discuss our approach to both challenges in detail below.
Normal approximation of p(s | θ). The exact form of the sufﬁcient statistic distribution p(s | θ)
is obtained by marginalizing over the data:

(cid:90)

p(s | θ) =

p(x1:n | θ)dx1:n 

t−1(s)

−1(s) :=(cid:8)x1:n : t(x1:n) = s(cid:9).

t

tractable full conditional. By the central limit theorem (CLT)  because s =(cid:80)

In general  the exact form of this distribution is not available. In some cases  it is—for example
if x ∼ Bernoulli(θ) then s ∼ Binomial(n  θ)—but even then it may not lead to a tractable full
conditional for s.
Properties of exponential families pave the way toward a general approach that always leads to a
i t(xi) is a sum of iid
random variables  it is asymptotically normal. It can be approximated as p(s | θ) ≈ N (s; nµ  nΣ) 
where µ = E[t(x)] and Σ = Var[t(x)] are the mean and variance of the sufﬁcient statistic of a
n (s − nµ) D−→ N (0  Σ) [25]. The
1√
single individual. This approximation is asymptotically correct:
quantities µ and Σ can be computed using well-known properties of exponential families [25]:

µ = E[t(x)] =

∂
∂η(cid:62) A(η) 

Σ = Var[t(x)] =

∂2

∂η∂η(cid:62) A(η) 

(1)

where η = η(θ) is the natural parameter.
Note that we will not use this approximation for Gibbs updates of θ. Instead  we will compute the
conditional p(θ | s) using standard conjugacy formulas. In this sense  we maintain two views of the
joint distribution p(θ  s)—when updating θ  it is the standard exponential family model  which leads
to conjugate updates; when updating s  it is approximated as p(θ)N (s; nµ  sΣ)  which will lead to
simple updates when combined with a variable augmentation technique.
Variable augmentation for p(y | s). We seek a tractable form for the full conditional of s under
the normal approximation  which is the product of a normal density and a Laplace density:

p(s | θ  y) ∝ N (s; nµ  nΣ) Lap(y; s  ∆s/).

A similar situation arises in the Bayesian Lasso [22]  and we will employ the same variable aug-
mentation trick. A Laplace random variable z ∼ Lap(u  b) can be written as a scale mixture of
normals by introducing a latent variable σ2 ∼ Exp(1/(2b2))  i.e.  the distribution with density

1/(2b2) exp(cid:0)−σ2/(2b2)(cid:1) and letting z ∼ N (u  σ2). We apply this separately to each dimension of

the vector y so that:

(cid:32)

(cid:33)

j ∼ Exp
σ2

2
2∆2
s

y ∼ N(cid:0)s  diag(σ2)(cid:1).

 

4

Algorithm 1 Gibbs Sampler  Bounded ∆s
1: Initialize θ  s  σ2
2: repeat
3:
4:
5:

s ∼ NormProduct(cid:0)nµ  nΣ  y  diag(σ2)(cid:1)
(cid:17)

θ ∼ p(θ; λ(cid:48)) where λ(cid:48) = Conjugate-Update(λ  s  n)
Calculate µ = E[s] and Σ = Var[s] (e.g.  use Eq. (1))

(cid:16)

1/σ2

j ∼ InverseGaussian



∆s|y−s|   2

∆2
s

6:

Subroutine NormProduct
1: input: µ1  Σ1  µ2  Σ2
1 + Σ−1

2: Σ3 =(cid:0)Σ−1
(cid:0)Σ−1

(cid:1)−1
1 µ1 + Σ−1

3: µ3 = Σ3
4: return: N (µ3  Σ3)

2

2 µ2

(cid:1)

The Gibbs Sampler. After the normal approxi-
mation and variable augmentation  the generative
process is as shown to the right. The ﬁnal Gibbs
sampling algorithm is shown in Algorithm 1. Note
that the update for θ is based on conjugacy in the ex-
act distribution p(θ  s)  while the update for s uses
the density of the generative process to the right  so
that p(s | θ  σ2  y) ∝ p(s | θ) p(y | σ2  s)  which
is a product of two normal densities

(cid:19)
y ∼ N(cid:0)s  diag(σ2)(cid:1)
N (s; nµ  nΣ)N(cid:0)y; s  diag(σ2)(cid:1) ∝ N (s; µs  Σs) 

θ ∼ p(θ; λ)
s ∼ N (nµ  nΣ)
j ∼ Exp
σ2

(cid:18) 2

2∆2
s

for all j

(cid:112)

where µs and Σs are are deﬁned in Algorithm 1 [26].
lows Park and Casella [22];

v/(2πx3) exp(cid:0)−v(x − m)2/(2m2x)(cid:1). Full derivations are given in the supplement.

fol-
the inverse Gaussian density is InverseGaussian(x; m  v) =

The update for σ2

3.3 Unbounded Sufﬁcient Statistics and Truncated Exponential Families

The Laplace mechanism does not apply when the sufﬁcient statistics are unbounded  because ∆s =
maxx y (cid:107)t(x) − t(y)(cid:107)1 = ∞. Thus  we need a new release mechanism A and inference algorithm P.
We present a solution for the case when x is univariate. All elements of the solution can generalize
to higher dimensions  except that one step will have running time that is exponential in d; we leave
improvement of this to future work and focus on the simpler univariate case.
Release mechanism. Our solution is to truncate the support of the (now univariate) p(x | θ)
to x ∈ [a  b]  where a and b are ﬁnite bounds provided by the modeler. If the modeler cannot
select bounds a priori  they may be selected privately as a preliminary step using a variant of the
exponential mechanism (see PrivateQuantile in Smith [27]).3 Then  given truncation bounds 
the data owner redacts individuals where xi /∈ [a  b] and reports the truncated sufﬁcient statistics
i=1 1[a b](xi) · t(xi) where 1S(x) is the indicator function of the set S. The sensitivity of ˆs
is now ∆ˆs = maxx y∈R (cid:107)ˆt(x) − ˆt(y)(cid:107)1 where ˆt(x) = 1[a b](x) t(x). An easy upper bound for this
quantity (see supplement) is:

ˆs =(cid:80)n

(cid:12)(cid:12)tj(x) − tj(y)(cid:12)(cid:12)(cid:111)

 

∆ˆs ≤ d(cid:88)

j=1

(cid:110)

max

max
x∈[a b]

|tj(x)|  max
x y∈[a b]

where tj(x) is the jth component of the sufﬁcient statistics. The bounds [a  b] will be selected so this
quantity is bounded. The released value is y ∼ Lap(ˆs  ∆ˆs/).

Inference: Truncated Exponential Family. Several new challenges arise for inference. The
quantity ˆs is no longer a sufﬁcient statistic for the model p(x | θ)  and we will need new insights to
understand p(ˆs | θ) and p(θ | ˆs). Since ˆs is a sum over individuals where xi ∈ [a  b]  it will be useful
to examine the probability of the event x ∈ [a  b] as well as the conditional distribution of x given this
event. To facilitate a general development  assume a generic truncation interval [v  w]  not necessarily
3Selecting truncation bounds will consume some of the privacy budget and modify the release mechanism A.

We do not consider inference with respect to this part of the release mechanism.

5

✓<latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit><latexit sha1_base64="JqEnYvV6PtsKBJYmBVwEpjIMANw=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOOsmY2ZllplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41rU4NhwbXUpt2xCxIoaCBAiW0EwMsjiS0ovHtzG89gbFCqwecJBDGbKjEQHCGTmp2cQTIeuWKX/XnoKskyEmF5Kj3yl/dvuZpDAq5ZNZ2Aj/BMGMGBZcwLXVTCwnjYzaEjqOKxWDDbH7tlJ45pU8H2rhSSOfq74mMxdZO4sh1xgxHdtmbif95nRQH12EmVJIiKL5YNEglRU1nr9O+MMBRThxh3Ah3K+UjZhhHF1DJhRAsv7xKmhfVwK8G95eV2k0eR5GckFNyTgJyRWrkjtRJg3DySJ7JK3nztPfivXsfi9aCl88ckz/wPn8Ao/ePKA==</latexit>𝑠𝑦𝜎$equal to [a  b]. Let F (x; θ) =(cid:82) x

(cid:16)

(cid:17)

(cid:90) w

h(x) exp(cid:0)ηT t(x)(cid:1) dx.

(2)

−∞ p(x | θ)dx be the CDF of the original (univariate) exponential
family model. It is clear that Pr(x ∈ [v  w]) = F (w; θ) − F (v; θ). The conditional distribution of x
given x ∈ [v  w] is a truncated exponential family  which  in its natural parameterization is:

ˆp(x | η) = 1[v w](x) h(x) exp

ηT t(x) − ˆA(η)

 

ˆA =

v

Note that this is still an exponential family model (with a modiﬁed base measure)  and all of the
standard results apply  such as the existence of a conjugate prior and the formulas in Eq. (1) for the
mean and variance of t(x) under the truncated distribution.
Random sum CLT for p(ˆs | θ). We would like to again apply an asymptotic normal approximation
for ˆs  but we do not know how many individuals fall within the truncation bounds. The “random sum
CLT” of Robbins [28] applies to the setting where the number of terms in the sum is itself a random
k=1 t(xik )  where {i1  . . .   iN} is the set of indices
of individuals with data inside the truncation bounds  i.e.  the indices such that xik ∈ [v  w]. The
number N is now a random variable distributed as N ∼ Binom(n  q)  where q = F (w; θ) − F (v; θ).
Proposition 1. Let ˆµ = E ˆp[t(x)] and ˆΣ = Var ˆp[t(x)] be the mean and variance of t(x) in the
k=1 t(xik ) is asymptotically normal with mean and

variable. The sum can be rewritten as ˆs =(cid:80)N
truncated exponential family. Then ˆs = (cid:80)N

variance:

m := E[ˆs] = E[N ]ˆµ = nq ˆµ 
V := Var(ˆs) = E[N ] ˆΣ + Var[N ]ˆµˆµ

(cid:0)ˆs − m(cid:1) D−→ N (0  ¯Σ) as n → ∞  where ¯Σ = V/n = q ˆΣ + q(1 − q)ˆµˆµ(cid:62).

(cid:62) = nq ˆΣ + nq(1 − q)ˆµˆµ
(cid:62)

.

Speciﬁcally  1√
n

Proof. Each term of the sum has mean ˆµ and variance ˆΣ  and the number of terms is N ∼
Binom(n  q). The result follows from Robbins [28].

Computing ˆµ and ˆΣ by automatic differentiation (autodiff). To use the normal approximation
we need to compute ˆµ and ˆΣ.
Lemma 1. Let p(x | θ) be a univariate exponential family model and let ˆp(x | θ) be the correspond-
ing exponential family model truncated to generic interval [v  w]. Then

ˆµ = E ˆp[t(x)] = Ep[t(x)] +

(3)

∂

∂ηT log(cid:0)F (w; η) − F (v; η)(cid:1)
∂η∂ηT log(cid:0)F (w; η) − F (v; η)(cid:1)

∂2

ˆΣ = Var ˆp[t(x)] = Varp[t(x)] +

Proof. It is straightforward to derive from Eq. (2) that ˆA(η) = A(η) + log(cid:0)F (w; η)− F (v; η)(cid:1). The

(4)

result follows from applying Eq. (1) to this expression for ˆA(η). See the supplement for derivation of
ˆA(η) and proof of this lemma.

We will use Equations (3) and (4) to compute ˆµ and ˆΣ by using autodiff to compute the desired
derivatives. If the mean and variance Ep[t(x)] and Varp[t(x)] of the untruncated distribution are not
known  we can apply autodiff to compute them as well using Eq. (1).
When x is multivariate  analogous expressions can be derived for ˆµ and ˆΣ. The adjustment factors will
include multivariate CDFs  with a number of terms that grow exponentially in d. This is currently the
main limitation in applying our methods to multivariate models with unbounded sufﬁcient statistics.
Conjugate updates for p(θ | ˆs). The ﬁnal issue is the distribution p(θ | ˆs)  which is no longer
characterized by conjugacy because ˆs are not the full sufﬁcient statistics. We again turn to variable
i=1 1[b ∞]t(xi) be the sufﬁcient statistics
for the individuals that fall in the lower portion [−∞  a] and upper portion [b ∞] of the support of x 
respectively. We will instantiate ˆs(cid:96) and ˆsu as latent variables and model their distributions using the

i=1 1[−∞ a]t(xi) and ˆsu =(cid:80)n

augmentation. Let ˆs(cid:96) =(cid:80)n

6

Algorithm 2 Gibbs Sampler  Unbounded ∆s
1: Initialize θ  ˆs  σ2  a  b
2: [v(cid:96)  w(cid:96)] ← [−∞  a]
3: [vc  wc] ← [a  b]
4: [vu  wu] ← [b ∞]
5: repeat
6:
7:
8:
9:
10:

mr  Vr ← RS-CLT(θ  vr  wr) for r ∈ {(cid:96)  c  u}
m(cid:48)
s ∼ N (m(cid:96) + m(cid:48)
θ ∼ p(θ; λ(cid:48)) where λ(cid:48) = Conjugate-Update(λ  s  n)
Recalculate mc and Vc  then draw ˆsc ∼ N (mc  Vc)
1/σ2

c ← NormProduct(cid:0)mc  Vc  y  diag(cid:0)σ2(cid:1)(cid:1)

c + mu  V(cid:96) + V(cid:48)

j ∼ InverseGaussian

c + Vu)

c  V(cid:48)

(cid:16)

(cid:17)

11:



∆ˆs|y−ˆsc|   2

∆2
ˆs

Algorithm 3 RS-CLT
1: input: θ  v  w
2: q ← F (b; w) − F (a; v)
3: ˆµ  ˆΣ ← autodiff of Eqns. 3  4
4: m ← nq
5: V ← nq ˆΣ + nq(1 − q)ˆµˆµ(cid:62)
6: return: m  V

12: until

random sum CLT approximation from Prop. 1 and Lemma 1 (but with different truncation bounds).
Let ˆsc = ˆs be the sufﬁcient statistics for the “center” portion  and deﬁne the three truncation intervals
as [v(cid:96)  w(cid:96)] = [−∞  a]  [vc  wc] = [a  b] and [vu  wu] = [b ∞]. The full sufﬁcient statistics are equal
to s = ˆs(cid:96) + ˆsc + ˆsu. Conditioned on all other variables  each component is multivariate normal  so
the sum s is also multivariate normal. We can therefore sample s and then sample from p(θ | s) using
conjugacy. We will also need to draw ˆsc separately to be used to update σ2.

The Gibbs Sampler. The (approximate) generative process in the unbounded case is:

θ ∼ p(θ; λ) 

ˆsr ∼ N(cid:0)mr  Vr)  for r ∈ {(cid:96)  c  u} where mr  Vr = RS-CLT(θ  vr  wr)
(cid:19)
y ∼ N(cid:0)ˆsc  diag(σ2)(cid:1).

(cid:18) 2

j ∼ Exp
σ2

for all j  

2∆2
ˆs

The Gibbs sampler to sample from this distribution is given in Algorithm 2. Note that in Line 8
we employ rejection sampling in which sufﬁcient statistics are sampled until the values drawn are
valid for the given data model  e.g.  s must be positive for the binomial distribution. The RS-CLT
algorithm to compute parameters of the random sum CLT is shown in Algorithm 3.

4 Experiments

We design experiments to measure the calibration and utility of our method for posterior inference.
We conduct experiments for the binomial model with beta prior  the multinomial model with Dirichlet
prior  and the exponential model with gamma prior. The last model is unbounded and requires
truncation; we set the bounds to keep the middle 95% of individuals  which is reasonable to assume
known a priori for some cases  such as modeling human height.

Methods. We run our Gibbs sampler for 5000 iterations after 2000 burnin iterations (see supple-
mentary material for convergence results)  which we compare to two baselines. The ﬁrst method
uses the same release mechanism as our Gibbs sampler and performs conjugate updates using the
noisy sufﬁcient statistics [12  13]. This method converges to the true posterior as n → ∞ because
the Laplace noise will eventually become negligible compared to sampling variability [12]. However 
the noise is not negligible for moderate n; we refer to this method as “naive”. For truncated models
we allow the naive method to “cheat” by accessing the noisy untruncated sufﬁcient statistics s. Thus
the method is not private  and receives strictly more information than our Gibbs sampler  but with
the same magnitude noise. This allows us to demonstrate miscalibration without highly technical
modiﬁcations to the baseline method to be able to deal with truncated sufﬁcient statistics.

7

The second baseline is a version of the one-posterior sampling (OPS) mechanism [11–13]  which
employs the exponential mechanism [29] to release samples from a privatized posterior. We release
100 samples using the method of [12]  each with ops = /100  such that the entire algorithm
achieves -differential privacy. Private MCMC sampling [11] is a more sophisticated method to
release multiple samples from a privatized posterior and could potentially make better use of the
privacy budget; however  private MCMC will also necessarily be miscalibrated  and only achieves the
weaker privacy guarantee of (  δ)-differential privacy for δ > 0  so would not be direct comparable
to our method. OPS serves as a suitable baseline that achieves -differential privacy. We include OPS
only for experiments on the binomial model  for which it requires the support of θ to be truncated to
[a0  1 − a0] where a0 > 0. We set a0 = 0.1.
We also include a non-private posterior for comparison  which performs conjugate updates using the
non-noisy sufﬁcient statistics.

Evaluation. We evaluate both the calibration and utility of the posterior. For calibration we adapt a
method of Cook et al. [30]: the idea is to draw iid samples (θi  xi) from the joint model p(θ)p(x | θ) 
and conduct posterior inference in each trial. Let Fi(θ) be the CDF of the true posterior p(θ | xi)
in trial i. Then we know that Ui = Fi(θi) is uniformly distributed  because θi ∼ p(θ | xi) (see
supplementary material). In other words  the actual parameter θi is equally likely to land at any
quantile of the posterior. To test the posterior inference procedure  we instead compute Ui as the
quantile at which θi lands within a set of samples from the approximate posterior. After M trials
of the whole procedure we test for uniformity of U1:M using the Kolmogorov-Smirnov goodness-
of-ﬁt test [31]  which measures the maximum distance between the empirical CDF of U1:M and the
uniform CDF; lower values are better and zero corresponds to perfect uniformity. We also visualize
the empirical CDFs to assess calibration qualitatively.
Higher utility of a private posterior is indicated by closeness to the non-private posterior  which we
measure with maximum mean discrepancy (MMD)  a kernel-based statistical test to determine if two
sets of samples are drawn from different distributions [32]. Given m i.i.d. samples (p  q) ∼ P × Q 
an unbiased estimate of the MMD is

(cid:88)m

i(cid:54)=j

MMD2(P  Q) =

1

m(m − 1)

(k(pi  pj) + k(qi  qj) − k(pi  qj) − k(pj  qi))  

where k is a continuous kernel function; we use a standard normal kernel. The higher the value the
more likely the two samples are drawn from different distributions.

Results. Figure 1a shows the results for three models and varying n and . Our method (Gibbs)
achieves the same calibration level as non-private posterior inference for all settings. The naive
method ignores noise and is too conﬁdent about parameter values implied by treating the noisy
sufﬁcient statistics as true ones; it is only well-calibrated with increasing n and  when noise becomes
negligible relative to population size. OPS is not calibrated because it samples from an over-dispersed
version of p(θ | x).
Figure 1b shows the empirical CDF plots for n = 1000 and  = 0.01. Our method and the non-private
method are both perfectly calibrated. The naive method’s over-conﬁdence in the wrong sufﬁcient
statistics causes its posterior to usually be too tight at the wrong value; thus the true parameter always
lies in a tail of the approximate posterior  so too much mass is placed near 0 and 1. OPS shows the
opposite behavior: its posterior is always too diffuse  so the true parameter lies close to the middle.
For multinomial we show measures only for the parameter of the ﬁrst category  but results hold for
all categories.
Figure 1c shows the MMD test statistic between each method and the non-private posterior  used as
a measure of utility. Our method consistently achieves utility at least as good as the naive method
for binomial and multinomial models. We omit OPS  which is never calibrated. For the exponential
model (not shown) we did not obtain conclusive utility comparisons due to the lack of a naive baseline
that properly handles truncation; the “cheating” naive method from our calibration experiments
sometimes attains higher utility than our method  and sometimes lower  but this comparison is not
meaningful because it receives strictly more information.

8

(a)

(b)

(c)

Figure 1: (a) Calibration as Kolmogorov-Smirnov statistic vs. number of individuals at  =
[0.01  0.10] for binomial  multinomial  and exponential models.
(b) Empirical CDF plots at
(n = 1000;  = 0.01) for binomial  multinomial  and exponential models. (c) Utility as MMD
with non-private posterior vs. number of individuals at  = [0.01  0.10] for binomial and multinomial
models.

5 Conclusion

We presented a Gibbs sampling approach for private posterior inference in exponential family models.
Rather than trying to approximate the posterior of p(θ | x1:n)  we divide our procedure into a private
release mechanism y = A(x1:n) and an inference algorithm P that computes p(θ | y). The release
mechanism is designed to facilitate inference. We develop a general-purpose Gibbs sampler that
applies to any exponential family model that has bounded sufﬁcient statistics; a truncated version
applies to univariate models with unbounded sufﬁcient statistics. The Gibbs sampler uses general
properties of exponential families to approximate the distribution of the sufﬁcient statistics  and
therefore avoids the need to reason about individuals. Promising lines of future work are to develop
efﬁcient methods for multivariate exponential families with unbounded sufﬁcient statistics  and to
develop methods for conditional models based on exponential families  such as generalized linear
models.

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant Nos.
1522054 and 1617533.

References
[1] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference  pages 265–284.
Springer  2006.

9

0.000.250.50KS stat.binomialmultinomialε=0.01exponential102103104n0.000.250.50KS stat.102103104n102103104nε=0.10Non-Priv.GibbsNaiveOPS0.00.51.0u0.00.51.0Pr(U≤u)binomial0.00.51.0umultinomial0.00.51.0uexponentialNon-Priv.GibbsNaiveOPS0.00.20.40.6MMDbinomial102103104n0.00.20.40.6MMDε=0.01ε=0.100.00.20.40.6MMDmultinomial102103104n0.00.20.40.6MMDε=0.01ε=0.10[2] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances

in Neural Information Processing Systems  pages 289–296  2009.

[3] Benjamin I.P. Rubinstein  Peter L. Bartlett  Ling Huang  and Nina Taft. Learning in a large func-
tion space: Privacy-preserving mechanisms for SVM learning. arXiv preprint arXiv:0911.5708 
2009.

[4] Shiva Prasad Kasiviswanathan  Homin K. Lee  Kobbi Nissim  Sofya Raskhodnikova  and Adam

Smith. What can we learn privately? SIAM Journal on Computing  40(3):793–826  2011.

[5] Martín Abadi  Andy Chu  Ian Goodfellow  H. Brendan McMahan  Ilya Mironov  Kunal Talwar 
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security  pages 308–318. ACM  2016.

[6] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empirical

risk minimization. Journal of Machine Learning Research  12(Mar):1069–1109  2011.

[7] Daniel Kifer  Adam Smith  and Abhradeep Thakurta. Private convex empirical risk minimization

and high-dimensional regression. Journal of Machine Learning Research  1(41):3–1  2012.

[8] Prateek Jain and Abhradeep Thakurta. Differentially private learning with kernels. ICML (3) 

28:118–126  2013.

[9] Raef Bassily  Adam Smith  and Abhradeep Thakurta. Private empirical risk minimization:
Efﬁcient algorithms and tight error bounds. In Foundations of Computer Science (FOCS)  2014
IEEE 55th Annual Symposium on  pages 464–473. IEEE  2014.

[10] Christos Dimitrakakis  Blaine Nelson  Aikaterini Mitrokotsa  and Benjamin I.P. Rubinstein.
Robust and private Bayesian inference. In International Conference on Algorithmic Learning
Theory  pages 291–305. Springer  2014.

[11] Yu-Xiang Wang  Stephen Fienberg  and Alex Smola. Privacy for free: Posterior sampling
and stochastic gradient Monte Carlo. In Proceedings of the 32nd International Conference on
Machine Learning (ICML-15)  pages 2493–2502  2015.

[12] James Foulds  Joseph Geumlek  Max Welling  and Kamalika Chaudhuri. On the theory and
practice of privacy-preserving Bayesian data analysis. In Proceedings of the Thirty-Second
Conference on Uncertainty in Artiﬁcial Intelligence  pages 192–201  2016.

[13] Zuhe Zhang  Benjamin I.P. Rubinstein  and Christos Dimitrakakis. On the differential privacy

of Bayesian inference. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

[14] Joseph Geumlek  Shuang Song  and Kamalika Chaudhuri. Renyi differential privacy mecha-
nisms for posterior sampling. In Advances in Neural Information Processing Systems  pages
5295–5304  2017.

[15] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Found.

and Trends in Theoretical Computer Science  2014.

[16] Oliver Williams and Frank McSherry. Probabilistic inference and differential privacy.

Advances in Neural Information Processing Systems  pages 2451–2459  2010.

In

[17] Vishesh Karwa  Aleksandra B. Slavkovi´c  and Pavel Krivitsky. Differentially private exponential
random graphs. In International Conference on Privacy in Statistical Databases  pages 143–155.
Springer  2014.

[18] Vishesh Karwa and Aleksandra B. Slavkovi´c. Inference using noisy degrees: Differentially

private beta-model and synthetic graphs. The Annals of Statistics  44(1):87–112  2016.

[19] Garrett Bernstein  Ryan McKenna  Tao Sun  Daniel Sheldon  Michael Hay  and Gerome Miklau.
Differentially private learning of undirected graphical models using collective graphical models.
In International Conference on Machine Learning  pages 478–487  2017.

10

[20] Aaron Schein  Zhiwei Steven Wu  Mingyuan Zhou  and Hanna Wallach. Locally private
Bayesian inference for count models. NIPS 2017 Workshop: Advances in Approximate Bayesian
Inference  2018.

[21] R.A. Fisher. On the mathematical foundations of theoretical statistics. Phil. Trans. R. Soc. Lond.

A  222(594-604):309–368  1922.

[22] Trevor Park and George Casella. The Bayesian lasso. Journal of the American Statistical

Association  103(482):681–686  2008.

[23] Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. In Proceedings of the
2011 ACM SIGMOD International Conference on Management of data  pages 193–204. ACM 
2011.

[24] Persi Diaconis and Donald Ylvisaker. Conjugate priors for exponential families. The Annals of

statistics  pages 269–281  1979.

[25] Peter J. Bickel and Kjell A. Doksum. Mathematical statistics: basic ideas and selected topics 

volume I  volume 117. CRC Press  2015.

[26] Kaare Brandt Petersen and Michael Syskind Pedersen. The matrix cookbook. Technical

University of Denmark  7(15):510  2008.

[27] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In
Proceedings of the Forty-third Annual ACM Symposium on Theory of Computing  pages 813–
822  2011.

[28] Herbert Robbins. The asymptotic distribution of the sum of a random number of random

variables. Bulletin of the American Mathematical Society  54(12):1151–1161  1948.

[29] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Foundations
of Computer Science  2007. FOCS’07. 48th Annual IEEE Symposium on  pages 94–103. IEEE 
2007.

[30] Samantha R. Cook  Andrew Gelman  and Donald B. Rubin. Validation of software for Bayesian
models using posterior quantiles. Journal of Computational and Graphical Statistics  15(3):
675–692  2006.

[31] Frank J. Massey Jr. The Kolmogorov-Smirnov test for goodness of ﬁt. Journal of the American

Statistical Association  46(253):68–78  1951.

[32] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

11

,Garrett Bernstein
Daniel Sheldon