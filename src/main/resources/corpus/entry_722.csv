2013,Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits,We develop an inference and optimal design procedure for recovering synaptic weights in neural microcircuits. We base our procedure on data from an experiment in which populations of putative presynaptic neurons can be stimulated while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for large amounts of information about the biological system to be incorporated if available. We then present a simpler model to facilitate online experimental design which entails the use of efficient Bayesian inference. The optimized approach results in equal quality posterior estimates of the synaptic weights in roughly half the number of experimental trials under experimentally realistic conditions  tested on synthetic data generated from the full model.,Bayesian Inference and Online Experimental Design

for Mapping Neural Microcircuits

Ben Shababo ∗

Department of Biological Sciences

Columbia University  New York  NY 10027

bms2156@columbia.edu

Ari Pakman

Department of Statistics 

Center for Theoretical Neuroscience 

& Grossman Center for the Statistics of Mind
Columbia University  New York  NY 10027

ap3053@columbia.edu

Brooks Paige ∗

Department of Engineering Science

University of Oxford  Oxford OX1 3PJ  UK

brooks@robots.ox.ac.uk

Liam Paninski

Department of Statistics 

Center for Theoretical Neuroscience 

& Grossman Center for the Statistics of Mind
Columbia University  New York  NY 10027

liam@stat.columbia.edu

Abstract

With the advent of modern stimulation techniques in neuroscience  the oppor-
tunity arises to map neuron to neuron connectivity.
In this work  we develop
a method for efﬁciently inferring posterior distributions over synaptic strengths
in neural microcircuits. The input to our algorithm is data from experiments in
which action potentials from putative presynaptic neurons can be evoked while
a subthreshold recording is made from a single postsynaptic neuron. We present
a realistic statistical model which accounts for the main sources of variability in
this experiment and allows for signiﬁcant prior information about the connectivity
and neuronal cell types to be incorporated if available. Due to the technical chal-
lenges and sparsity of these systems  it is important to focus experimental time
stimulating the neurons whose synaptic strength is most ambiguous  therefore we
also develop an online optimal design algorithm for choosing which neurons to
stimulate at each trial.

1

Introduction

A major goal of neuroscience is the mapping of neural microcircuits at the scale of hundreds to
thousands of neurons [1]. By mapping  we speciﬁcally mean determining which neurons synapse
onto each other and with what weight. One approach to achieving this goal involves the simultaneous
stimulation and observation of populations of neurons. In this paper  we speciﬁcally address the
mapping experiment in which a set of putative presynaptic neurons are optically stimulated while
an electrophysiological trace is recorded from a designated postsynaptic neuron. It should be noted
that the methods we present are general enough that most stimulation and subthreshold monitoring
technology would be well ﬁt by our model with only minor changes. These types of experiments
have been implemented with some success [2  3  6]  yet there are several issues which prevent
efﬁcient  large scale mapping of neural microcircuitry. For example  while it has been shown that
multiple neurons can be stimulated simultaneously [4  5]  successful mapping experiments have thus
far only stimulated a single neuron per trial which increases experimental time [2  3  6]. Stimulating
multiple neurons simultaneously and with high accuracy requires well-tuned hardware  and even
then some level of stimulus uncertainty may remain.
In addition  a large portion of connection

∗These authors contributed equally to this work.

1

weights are small which has meant that determining these weights is difﬁcult and that many trials
must be performed. Due to the sparsity of neural connectivity  potentially useful trials are spent
on unconnected pairs instead of reﬁning weight estimates for connected pairs when the stimuli
are chosen non-adaptively. In this paper  we address these issues by developing a procedure for
sparse Bayesian inference and information-based experimental design which can reconstruct neural
microcircuits accurately and quickly despite the issues listed above.

2 A realistic model of neural microcircuits

In this section we propose a novel and thorough statistical model which is speciﬁc enough to capture
most of the relevant variability in these types of experiments while being ﬂexible enough to be used
with many different hardware setups and biological preparations.

2.1 Stimulation

In our experimental setup  at each trial  n = 1  . . .   N  the experimenter stimulates R of K possible
presynaptic neurons. We represent the chosen set of neurons for each trial with the binary vector
zn ∈ {0  1}K  which has a one in each of the the R entries corresponding to the stimulated neurons
on that trial. One of the difﬁculties of optical stimulation lies in the experimenter’s inability to
stimulate a speciﬁc neuron without possibly failing to stimulate the target neuron or engaging other
nearby neurons. In general  this is a result of the fact that optical excitation does not stimulate a
single point in space but rather has a point spread function that is dependent on the hardware and the
biological tissue. To complicate matters further  each neuron has a different rheobase (a measure of
how much current is needed to generate an action potential) and expression level of the optogenetic
protein. While some work has shown that it may be possible to stimulate exact sets of neurons 
this setup requires very speciﬁc hardware and ﬁne tuning [4  5].
In addition  even if a neuron
ﬁres  there is some probability that synaptic transmission will not occur. Because these events are
difﬁcult or impossible to observe  we model this uncertainty by introducing a second binary vector
xn ∈ {0  1}K denoting the neurons that actually release neurotransmitter in trial n. The conditional
distribution of xn given zn can be chosen by the experimenter to match their hardware settings and
understanding of synaptic transmission rates in their preparation.

2.2 Sparse connectivity

Numerous studies have collected data to estimate both connection probabilities and synaptic weight
distributions as a function of distance and cell identity [2  3  6  7  8  9  10  11  12]. Generally  the
data show that connectivity is sparse and that most synaptic weights are small with a heavy tail of
strong connections. To capture the sparsity of neural connectivity  we place a “spike-and-slab” prior
on the synaptic weights wk [13  14  15]  for each presynaptic neuron k = 1  . . .   K; these priors are
designed to place non-zero probability on the event that a given weight wk is exactly zero. Note that
we do not need to restrict the “slab” distributions (the conditional distributions of wk given that wk
is nonzero) to the traditional Gaussian choice  and in fact each weight can have its own parameters.
For example  log-normal [12] or exponential [8  10] distributions may be used in conjunction with
information about cell type and location to assign highly informative priors 1.

2.3 Postsynaptic response

In our model a subthreshold response is measured from a designated postsynaptic neuron. Here we
assume the measurement is a one-dimensional trace yn ∈ RT   where T is the number of samples in
the trace. The postsynaptic response for each synaptic event in a given trial can be modeled using an
appropriate template function fk(·) for each presynaptic neuron k. For this paper we use an alpha
function to model the shape of each neuron’s contribution to the postsynaptic current  parameterized
by time constants τk which deﬁne the rise and decay time. As with the synaptic weight priors  the
template functions could be designed based on the cells’ identities. The onset of each postsynaptic

1A cell’s identity can be general such as excitatory or inhibitory  or more speciﬁc such as VIP- or PV-
interneurons. These identities can be identiﬁed by driving the optogenetic channel with a particular promotor
unique to that cell type or by coexpressing markers for various cell types along with the optogenetic channel.

2

Figure 1: A schematic of the model experiment. The left ﬁgure shows the relative location of
100 presynaptic neurons; inhibitory neurons are shown in yellow  and excitatory neurons in purple.
Neurons marked with a black outline have a nonzero connectivity to the postsynaptic neuron (shown
as a blue star  in the center). The blue circles show the diffusion of the stimulus through the tissue.
The true connectivity weights are shown on the upper right  with blue vertical lines marking the ﬁve
neurons which were actually ﬁred as a result of this stimulus. The resulting time series postsynaptic
current trace is shown in the bottom right. The connected neurons which ﬁred are circled in red  the
triangle and star marking their weights and corresponding postsynaptic events in the plots at right.

response may be jittered such that each event starts at some time dnk after t = 0  where the delays
could be conditionally distributed on the parameters of the stimulation and cells. Finally  at each time
step the signal is corrupted by zero mean Gaussian noise with variance ν2. This noise distribution is
chosen for simplicity; however  the model could easily handle time-correlated noise.

2.4 Full deﬁnition of model

The full model can be summarized by the likelihood

p(Y|w  X  D) =

N

ynt

n=1
with the general spike-and-slab prior

t=1

N(cid:89)

T(cid:89)

(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

k

wkxnkfk(t − dnk  τk)  ν2

(cid:19)

(1)

p(γk) = Bernoulli(ak) 

p(wk|γk) = γkp(wk|γk = 1) + (1 − γk)δ0(wk)

(2a  2b)
where Y ∈ RN×T   X ∈ {0  1}N×K  and D ∈ RN×K are composed of the responses  latent
neural activity  and delays  respectively; γk is a binary variable indicating whether or not neuron k
is connected.
We restate that the key to this model is that it captures the main sources of uncertainty in the exper-
iment while providing room for particulars regarding hardware and the anatomy and physiology of
the system to be incorporated. To infer the marginal distribution of the synaptic weights  one can
use standard Bayesian methods such as Gibbs sampling or variational inference  both of which are
discussed below. An example set of neurons and connectivity weights  along with the set of stimuli
and postsynaptic current trace for a single trial  is shown in Figure 1.

3

Inference

Throughout the remainder of the paper  all simulated data is generated from the model presented
above. As mentioned  any free hyperparameters or distribution choices can be chosen intelligently
from empirical evidence. Biological parameters may be speciﬁc and chosen on a cell by cell basis
or left general for the whole system. We show in our results that inference and optimal design still
perform well when general priors are used. Details regarding data simulation as well as speciﬁc
choices we make in our experiments are presented in Appendix A.

3

WeightCurrent [pA]Postsynaptic current tracePresynaptic weightsLocation of presynaptic neurons and stimuli050100150200−30−20−10010020406080100−101Time [samples]Neuron k3.1 Charge as synaptic strength

single variable ck =(cid:80)
yn =(cid:80)T

To reduce the space over which we perform inference  we collapse the variables wk and τk into a
t wkfk(t − dnk  τk) which quantiﬁes the charge transfer during the synaptic
event and can be used to deﬁne the strength of a connection. Integrating over time also eliminates
any dependence on the delays dnk. In this context  we reparameterize the likelihood as a function of

t=0 ynt and σ = νT 1/2 and the resulting likelihood is

(cid:89)

p(y|X  c) =

N (yn|x(cid:62)

n c  σ2).

(3)

n

We found that na¨ıve MCMC sampling over the posterior of w  τ   γ  X  and D insufﬁciently ex-
plored the support and inference was unsuccessful. In this effort to make the inference procedure
computationally tractable  we discard potentially useful temporal information in the responses. An
important direction for future work is to experiment with samplers that can more efﬁciently explore
the full posterior (e.g.  using Wang-Landau or simulated tempering methods).

3.2 Gibbs sampling
The reparameterized posterior p(c  γ  X|Z  y) can be inferred using a simple Gibbs sampler. We
approximate the prior over c as a spike-and-slab with Gaussian slabs where the slabs could be
truncated if the cells’ excitatory or inhibitory identity is known. Each xnk can be sampled by
computing the odds ratio  and following [15] we draw each ck  γk from the joint distribution
p(ck  γk|Z  y  X {cj  γj|j (cid:54)= k}) by sampling ﬁrst γk from p(γk|Z  y  X {cj|j (cid:54)= k})  then
p(ck|Z  y  X {cj |j (cid:54)= k}  γk).

3.3 Variational Bayes

As stated earlier we do not only want to recover the parameters of the system  but want to perform
optimal experimental design  which is a closed-loop process. One essential aspect of the design
procedure is that decisions must be returned to the experimenter quickly  on the order of a few
seconds. This means that we must be able to perform inference of the posterior as well as choose
the next stimulus extremely quickly. For realistically sized systems with hundred to thousands of
neurons  Gibbs sampling will be too slow  and we have to explore other options for speeding up
inference.
To achieve this decrease in runtime  we approximate the posterior distribution of c and γ using a
variational approach [16]. The use of variational inference for spike-and-slab regression models has
been explored in [17  18]  and we follow their methods with some minor changes. If we  for now 
assume that X is known and let the spike-and-slab prior on c have untruncated Gaussian slabs  then
this variational approach ﬁnds the best fully-factorized approximation to the true posterior

p(c  γ|x1:n  y1:n) ≈(cid:89)
(cid:26)αkN (ck|µk  s2

k

k)
(1 − αk)δ0(ck)

q(ck  γk) =

q(ck  γk)

if γk = 1
otherwise.

where the functional form of q(ck  γk) is itself restricted to a spike-and-slab distribution

The variational parameters αk  µk  sk for k = 1  . . .   K are found by minimizing the KL-divergence
KL(q||p) between the left and right hand sides of Eq. 4 with respect to these values. As is the case
with fully-factorized variational distributions  updating the posterior involves an iterative algorithm
which cycles through the parameters for each factor.
The factorized variational approximation is reasonable when the number of simultaneous stimuli 
R  is small. Note that if we examine the posterior distributions of the weights

(cid:89)

(cid:2)akN (ck|ηk  σ2

k) + (1 − ak)δ0(ck)(cid:3)

p(c|y  X) ∝(cid:89)

N (yn|x(cid:62)

n c  σ2)

n

k

we see that if each xn contains only one nonzero value then each factor in the likelihood is depen-
dent on only one of the K weights and can be multiplied into the corresponding kth spike-and-slab.

4

(4)

(5)

(6)

Therefore  since the product of a spike-and-slab and a Gaussian is still a spike-and-slab  if we stim-
ulate only one neuron at each trial then this posterior is also spike-and-slab  and the variational
approximation becomes exact in this limit.
Since we do not directly observe X  we must take the expectation of the variational parameters
αk  µk  sk with respect to the distribution p(X|Z  y). We Monte Carlo approximate this integral
in a manner similar to the approach used for integrating over the hyperparameters in [17]; how-
ever  here we further approximate by sampling over potential stimuli xnk from p(xnk = 1|zn). In
practice we will see this approximation sufﬁces for experimental design  with the overall variational
approach performing nearly as well for posterior weight reconstruction as Gibbs sampling from the
true posterior.

4 Optimal experimental design

The preparations needed to perform these type of experiments tend to be short-lived  and indeed  the
very act of collecting data — that is  stimulating and probing cells — can compromise the health of
the preparation further. Also  one may want to use the connectivity information to perform additional
experiments. Therefore it becomes critical to complete the mapping phase of the experiment as
quickly as possible. We are thus strongly motivated to optimize the experimental design: to choose
the optimal subset of neurons zn to stimulate at each trial to minimize N  the overall number of
trials required for good inference.
The Bayesian approach to the optimization of experimental design has been explored in [19  20 
21]. In this paper  we maximize the mutual information I(θ;D) between the model parameters θ
and the data D; however  other objective functions could be explored. Mutual information can be
decomposed into a difference of entropies  one of which does not depend on the data. Therefore
the optimization reduces to the intuitive objective of minimizing the posterior entropy with respect
to the data. Because the previous data Dn−1 = {(z1  y1)  . . .   (zn−1  yn−1)} are ﬁxed and yn is
dependent on the stimulus zn  our problem is reduced to choosing the optimal next stimulus  denoted
n  in expectation over yn 
z(cid:63)

z(cid:63)
n = arg max

zn

Eyn|zn [I(θ;D)] = arg min

Eyn|zn [H(θ|D)] .

zn

(7)

5 Experimental design procedure

The optimization described in Section 4 entails performing a combinatorial optimization over zn 
where for each zn we consider an expectation over all possible yn. In order to be useful to experi-
menters in an online setting  we must be able to choose the next stimulus in only one or two seconds.
For any realistically sized system  an exact optimization is computationally infeasible; therefore in
the following section we derive a fast method for approximating the objective function.

5.1 Computing the objective function

The variational posterior distribution of ck  γk can be used to characterize our general objective
function described in Section 4. We deﬁne the cost function J to be the right-hand side of Equation 7 
J ≡ Eyn|zn [H(c  γ|D)]
(8)
n can be found by minimizing J. We beneﬁt immediately from

such that the optimal next stimulus z(cid:63)
the factorized approximation of the variational posterior  since we can rewrite the joint entropy as

H[c  γ|D] ≈(cid:88)

H[ck  γk|D]

(9)

allowing us to optimize over the sum of the marginal entropies instead of having to compute the
(intractable) entropy over the full posterior. Using the conditional entropy identity H[ck  γk|D] =
H[ck|γk D] + H[γk|D]  we see that the entropy of each spike-and-slab is the sum of a weighted
Gaussian entropy and a Bernoulli entropy and we can write out the approximate objective function
as

(cid:105)
k n)) − αk n log αk n − (1 − αk n) log(1 − αk n)

J ≈(cid:88)

(1 + log(2πs2

.

(10)

(cid:104) αk n

2

Eyn|zn

k

k

5

Here  we have introduced additional notation  using αk n  µk n  and sk n to refer to the parameters of
the variational posterior distribution given the data through trial n. Intuitively  we see that equation
10 represents a balance between minimizing the sparsity pattern entropy H[γk] of each neuron and
minimizing the weight entropy H[ck|γk = 1] proportional to the probability αk that the presynaptic
neuron is connected. As p(γk = 1) → 1  the entropy of the Gaussian slab distribution grows to
dominate. In algorithm behavior  we see when the probability that a neuron is connected increases 
we spend time stimulating it to reduce the uncertainty in the corresponding nonzero slab distribution.
To perform this optimization we must compute the expected joint entropy with respect to p(yn|zn).
For any particular candidate zn  this can be Monte Carlo approximated by ﬁrst sampling yn from the
posterior distribution p(yn|zn  c Dn−1)  where c is drawn from the variational posterior inferred at
trial n − 1. Each sampled yn may be used to estimate the variational parameters αk n and sk n with
which we evaluate H[ck  γk]; we average over these evaluations of the entropy from each sample to
compute an estimate of J in Eq. 10.
Once we have chosen z(cid:63)
n  we execute the actual trial and run the variational inference procedure on
the full data to obtain the updated variational posterior parameters αk n  µk n  and sk n which are
needed for optimization. Once the experiment has concluded  Gibbs sampling can be run  though
we found only a limited gain when comparing Gibbs sampling to variational inference.

5.2 Fast optimization

1:n−1 ˜x(cid:62)

nk = 1 for the R neurons corresponding to the ˜zk

The major cost to the algorithm is in the stimulus selection phase. It is not feasible to evaluate the
right-hand side of equation 10 for every zn because as K grows there is a combinatorial explosion
of possible stimuli. To avoid an exhaustive search over possible zn  we adopt a greedy approach
for choosing which R of the K locations to stimulate. First we rank the K neurons based on an
approximation of the objective function. To do this  we propose K hypothetical stimuli  ˜zk
n  each
all zeros except the kth entry equal to 1 — that is  we examine only the K stimuli which represent
stimulating a single location. We then set z∗
n which
give the smallest values for the objective function and all other entries of z∗
n to zero. We found that
the neurons selected by a brute force approach are most likely to be the neurons that the greedy
selection process chooses (see Figure 1 in the Appendix).
For large systems of neurons  even the above is too slow to perform in an online setting. For each of
the K proposed stimuli ˜zk
n  to approximate the expected entropy we must compute the variational
posterior for M samples of [X(cid:62)
n ](cid:62) and L samples of yn (where ˜xn is the random variable
corresponding to p(˜xn|˜zn)). Therefore we run the variational inference procedure on the full data
on the order of O(M KL) times at each trial. As the system size grows  running the variational
inference procedure this many times becomes intractable because the number of iterations needed
to converge the coordinate ascent algorithm is dependent on the correlations between the rows of
X. This is implicitly dependent on both N  the number of trials  and R  the number of stimulus
locations (see Figure 2 in the Appendix). Note that the stronger dependence here is on R; when
R = 1 the variational parameter updates become exact and independent across the neurons  and
therefore no coordinate ascent is necessary and the runtime becomes linear in K.
We therefore take one last measure to speed up the optimization process by implementing an online
Bayesian approach to updating the variational posterior (in the stimulus selection phase only). Since
the variational posterior of ck and γk takes the same form as the prior distribution  we can use the
posterior from trial n − 1 as the prior at trial n  allowing us to effectively summarize the previous
data. In this online setting  when we stimulate only one neuron  only the parameters of that speciﬁc
neuron change. If during optimization we temporarily assume that ˜xk
n  this results in explicit
updates for each variational parameter  with no coordinate ascent iterations required.
In total  the resulting optimization algorithm has a runtime O(KL) with no coordinate ascent al-
gorithms needed. The combined accelerations described in this section result in a speed up of
several orders of magnitude which allows the full inference and optimization procedure to be run
in real time  running at approximately one second per trial in our computing environment for
K = 500  R = 8. It is worth mentioning here that there are several points at which parallelization
could be implemented in the full algorithm. We chose to parallelize over M which distributes the
sampling of X and the running of variational inference for each sample. (Formulae and step-by-step
implementation details are found in Appendix B.)

n = ˜zk

6

Figure 2: A comparison of normalized reconstruction error (NRE) over 800 trials in a system with
500 neurons  between random stimulus selection (red  magenta) and our optimal experimental de-
sign approach (blue  cyan). The heavy red and blue lines indicate the results when running the
Gibbs sampler at that point in the experiment  and the thinner magenta and cyan lines indicate the
results from variational inference. Results are shown over three noise levels ν = 1  2.5  5  and for
multiple numbers of stimulus locations per trial  R = 2  4  8  16. Each plot shows the median and
quartiles over 50 experiments. The error decreases much faster in the optimal design case  over a
wide parameter range.

6 Experiments and results

We ran our inference and optimal experimental design algorithm on data sets generated from the
model described in Section 2. We benchmarked our optimal design algorithm against a sequence
of randomly chosen stimuli  measuring performance by normalized reconstruction error  deﬁned as
(cid:107)E[c] − c(cid:107)2/(cid:107)c(cid:107)2; we report the variation in our experiments by plotting the median and quartiles.
Baseline results are shown in Figure 2  over a range of values for stimulations per trial R and
baseline postsynaptic noise levels ν. The results here use an informative prior  where we assume the
excitatory or inhibitory identity is known  and we set individual prior connectivity probabilities for
each neuron based on that neuron’s identity and distance from the postsynaptic cell. We choose to
let X be unobserved and let the stimuli Z produce Gaussian ellipsoids which excite neurons that are
located nearby. All model parameters are given in Appendix A.
We see that inference in general performs well. The optimal procedure was able to achieve equiva-
lent reconstruction quality as a random stimulation paradigm in signiﬁcantly fewer trials when the
number of stimuli per trial and response noise were in an experimentally realistic range (R = 4
and ν = 2.5 being reasonable values). Interestingly  the approximate variational inference methods
performed about as well as the full Gibbs sampler here (at much less computational cost)  although
Gibbs sampling seems to break down when R grows too large and the noise level is small  which
may be a consequence of strong  local peaks in the posterior.
As the the number of stimuli per trial R increases  we start to see improved weight estimates and
faster convergence but a decrease in the relative beneﬁt of optimal design; the random approach
“catches up” to the optimal approach as R becomes large. This is consistent with the results of [22] 
who argue that optimal design can provide only modest gains in performing sparse reconstructions 

7

0.20.40.60.811.2ν=1.0NREofE[c]R=20.20.40.60.811.21.4ν=2.5NREofE[c]02004006008000.40.60.811.21.4ν=5.0NREofE[c]trial nR=40200400600800trial nR=80200400600800trial nR=160200400600800trial nFigure 3: The results of in-
ference and optimal design
(A) with a single spike-and-
slab prior for all connections
(prior connection probability
of .1  and each slab Gaus-
sian with mean 0 and stan-
dard deviation 31.4); and (B)
with X observed. Both ex-
periments show median and
quartiles range with R = 4
and ν = 2.5.

if the design vectors x are unconstrained. (Note that these results do not apply directly in our setting
if R is small  since in this case x is constrained to be highly sparse — and this is exactly where we
see major gains from optimal online designs.)
Finally  we see that we are still able to recover the synaptic strengths when we use a more general
prior as in Figure 3A where we placed a single spike-and-slab prior across all the connections. Since
we assumed the cells’ identities were unknown  we used a zero-centered Gaussian for the slab and
a prior connection probability of .1. While we allow for stimulus uncertainty  it will likely soon be
possible to stimulate multiple neurons with high accuracy. In Figure 3B we see that - as expected -
performance improves.
It is helpful to place this observation in the context of [23]  which proposed a compressed-sensing
algorithm to infer microcircuitry in experiments like those modeled here. The algorithms proposed
by [23] are based on computing a maximum a posteriori (MAP) estimate of the weights w; note
that to pursue the optimal Bayesian experimental design methods proposed here  it is necessary
to compute (or approximate) the full posterior distribution  not just the MAP estimate. (See  e.g. 
[24] for a related discussion.)
In the simulated experiments of [23]  stimulating roughly 30 of
500 neurons per trial is found to be optimal; extrapolating from Fig. 2  we would expect a limited
difference between optimal and random designs in this range of R. That said  large values of R
lead to some experimental difﬁculties: ﬁrst  stimulating large populations of neurons with high
spatial resolution requires very ﬁned tuned hardware (note that the approach of [23] has not yet
been applied to experimental data  to our knowledge); second  if R is sufﬁciently large then the
postsynaptic neuron can be easily driven out of a physiologically realistic regime  which in turn
means that the basic linear-Gaussian modeling assumptions used here and in [23] would need to be
modiﬁed. We plan to address these issues in more depth in our future work.

7 Future Work

There are several improvements we would like to explore in developing this model and algorithm
further. First  the implementation of an inference algorithm which performs well on the full model
such that we can recover the synaptic weights  the time constants  and the delays would allow us to
avoid compressing the responses to scalar values and recover more information about the system.
Also  it may be necessary to improve the noise model as we currently assume that there are no
spontaneous synaptic events which will confound the determination of each connection’s strength.
Finally  in a recent paper  [25]  a simple adaptive compressive sensing algorithm was presented
which challenges the results of [22]. It would be worth exploring whether their algorithm would be
applicable to our problem.

Acknowledgements

This material is based upon work supported by  or in part by  the U. S. Army Research Laboratory
and the U. S. Army Research Ofﬁce under contract number W911NF-12-1-0594 and an NSF CA-
REER grant. We would also like to thank Rafael Yuste and Jan Hirtz for helpful discussions  and
our anonymous reviewers.

8

02004006008000.40.60.811.2trial nXObserved02004006008000.40.50.60.70.80.911.1NREofE[c]trial nGeneralPriorReferences

[1] R. Reid  “From Functional Architecture to Functional Connectomics ” Neuron  vol. 75  pp. 209–217  July

2012.

[2] M. Ashby and J. Isaac  “Maturation of a recurrent excitatory neocortical circuit by experience-dependent

unsilencing of newly formed dendritic spines ” Neuron  vol. 70  no. 3  pp. 510 – 521  2011.

[3] E. Fino and R. Yuste  “Dense Inhibitory Connectivity in Neocortex ” Neuron  vol. 69  pp. 1188–1203 

Mar. 2011.

[4] V. Nikolenko  K. E. Poskanzer  and R. Yuste  “Two-photon photostimulation and imaging of neural cir-

cuits ” Nat Meth  vol. 4  pp. 943–950  Nov. 2007.

[5] A. M. Packer  D. S. Peterka  J. J. Hirtz  R. Prakash  K. Deisseroth  and R. Yuste  “Two-photon optogenet-

ics of dendritic spines and neural circuits ” Nat Meth  vol. 9  pp. 1202–1205  Dec. 2012.

[6] A. M. Packer and R. Yuste  “Dense  unspeciﬁc connectivity of neocortical parvalbumin-positive interneu-
rons: A canonical microcircuit for inhibition? ” The Journal of Neuroscience  vol. 31  no. 37  pp. 13260–
13271  2011.

[7] B. Barbour  N. Brunel  V. Hakim  and J.-P. Nadal  “What can we learn from synaptic weight distribu-

tions? ” Trends in neurosciences  vol. 30  pp. 622–629  Dec. 2007.

[8] C. Holmgren  T. Harkany  B. Svennenfors  and Y. Zilberter  “Pyramidal cell communication within local

networks in layer 2/3 of rat neocortex ” The Journal of Physiology  vol. 551  no. 1  pp. 139–153  2003.

[9] J. Kozloski  F. Hamzei-Sichani  and R. Yuste  “Stereotyped position of local synaptic targets in neocor-

tex ” Science  vol. 293  no. 5531  pp. 868–872  2001.

[10] R. B. Levy and A. D. Reyes  “Spatial proﬁle of excitatory and inhibitory synaptic connectivity in mouse

primary auditory cortex ” The Journal of Neuroscience  vol. 32  no. 16  pp. 5609–5619  2012.

[11] R. Perin  T. K. Berger  and H. Markram  “A synaptic organizing principle for cortical neuronal groups ”

Proceedings of the National Academy of Sciences  vol. 108  no. 13  pp. 5419–5424  2011.

[12] S. Song  P. J. Sj¨ostr¨om  M. Reigl  S. Nelson  and D. B. Chklovskii  “Highly nonrandom features of

synaptic connectivity in local cortical circuits. ” PLoS biology  vol. 3  p. e68  Mar. 2005.

[13] E. I. George and R. E. McCulloch  “Variable selection via gibbs sampling ” Journal of the American

Statistical Association  vol. 88  no. 423  pp. 881–889  1993.

[14] T. J. Mitchell and J. J. Beauchamp  “Bayesian variable selection in linear regression ” Journal of the

American Statistical Association  vol. 83  no. 404  pp. 1023–1032  1988.

[15] S. Mohamed  K. A. Heller  and Z. Ghahramani  “Bayesian and l1 approaches to sparse unsupervised

learning ” CoRR  vol. abs/1106.1157  2011.

[16] C. M. Bishop  Pattern Recognition and Machine Learning. Springer  2007.
[17] P. Carbonetto and M. Stephens  “Scalable variational inference for bayesian variable selection in regres-
sion  and its accuracy in genetic association studies ” Bayesian Analysis  vol. 7  no. 1  pp. 73–108  2012.
[18] M. Titsias and M. Lzaro-Gredilla  “Spike and Slab Variational Inference for Multi-Task and Multiple

Kernel Learning ” in Advances in Neural Information Processing Systems 24  pp. 2339–2347  2011.

[19] Y. Dodge  V. Fedorov  and H. Wynn  eds.  Optimal Design and Analysis of Experiments. North Holland 

1988.

[20] D. J. C. MacKay  “Information-based objective functions for active data selection ” Neural Comput. 

vol. 4  pp. 590–604  July 1992.

[21] L. Paninski  “Asymptotic Theory of Information-Theoretic Experimental Design ” Neural Comput. 

vol. 17  pp. 1480–1507  July 2005.

[22] E. Arias-Castro  E. J. Cand`es  and M. A. Davenport  “On the fundamental limits of adaptive sensing ”

IEEE Transactions on Information Theory  vol. 59  no. 1  pp. 472–481  2013.

[23] T. Hu  A. Leonardo  and D. Chklovskii  “Reconstruction of Sparse Circuits Using Multi-neuronal Excita-

tion (RESCUME) ” in Advances in Neural Information Processing Systems 22  pp. 790–798  2009.

[24] S. Ji and L. Carin  “Bayesian compressive sensing and projection optimization ” in Proceedings of the
24th international conference on Machine learning  ICML ’07  (New York  NY  USA)  pp. 377–384 
ACM  2007.

[25] M. Malloy and R. D. Nowak  “Near-optimal adaptive compressed sensing ” CoRR  vol. abs/1306.6239 

2013.

9

,Ben Shababo
Brooks Paige
Ari Pakman
Liam Paninski
Qilong Wang
Zilin Gao
Jiangtao Xie
Wangmeng Zuo
Peihua Li