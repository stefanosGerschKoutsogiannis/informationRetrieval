2018,Regularizing by the Variance of the Activations' Sample-Variances,Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations  we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove  this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians  the new loss would either join the two together  or separate between them optimally in the LDA sense  depending on the prior probabilities. Finally  we are able to link the new regularization term to the batchnorm method  which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks.,Regularizing by the Variance of the

Activations’ Sample-Variances

Etai Littwin1 Lior Wolf 12

1Tel Aviv University 2Facebook AI Research

Abstract

Normalization techniques play an important role in supporting efﬁcient and often
more effective training of deep neural networks. While conventional methods ex-
plicitly normalize the activations  we suggest to add a loss term instead. This new
loss term encourages the variance of the activations to be stable and not vary from
one random mini-batch to the next. As we prove  this encourages the activations
to be distributed around a few distinct modes. We also show that if the inputs
are from a mixture of two Gaussians  the new loss would either join the two to-
gether  or separate between them optimally in the LDA sense  depending on the
prior probabilities. Finally  we are able to link the new regularization term to the
batchnorm method  which provides it with a regularization perspective. Our ex-
periments demonstrate an improvement in accuracy over the batchnorm technique
for both CNNs and fully connected networks.

1

Introduction

We propose a novel regularization technique that is applied before the activation of all neurons in the
neural network. The new regularization term encourages the distribution of the individual activations
to have a few distinct modes. This property is achieved implicitly by computing the variance of the
activation of each neuron in each minibatch and by penalizing for variations of this variance  i.e. 
we encourage the variances to be the same across the mini-batches.
We provide a theoretical link between the variance-based regularization term and the resulting
peaked activation distributions  which we also observe experimentally  see Fig. 1. In addition  we
also provide experimental evidence that the new term leads to improved accuracy and can replace 
during training  normalization techniques such as the batch-norm technique.
The link between the new regularization term and batch-norm is further explored theoretically. A
distribution with few modes would lead to more stable batches and  for example  the representation
of a given sample would not vary along different batches.
In other words  it is desirable that a
sample  if repeated twice in multiple batches  would produce the same network activations post-
normalization. This is an indirect way in which batchnorm beneﬁts from few-modes. In our method
it is encouraged more explicitly.
The new regularization term is adaptive  in the sense that it can lead to a few distinct outcomes.
When applied to a mixture of two Gaussians  the regularization leads  in an unsupervised way  to
one of two possible projections: either the LDA projection that maximally separates between the
two Gaussians  or the orthogonal projection that is least sensitive to their differences.
Interestingly  the amount of variance in each activation is controlled by a parameter β. In order to
avoid searching over a wide range of hyper-parameters  we optimize for this term during training
and allow each neuron to adapt to a different level of variance.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

(a)

(b)

Figure 1: Histograms of activations in a network trained on the UCI adult dataset. (a) Random
neurons trained with batchnorm. (b) Random neurons trained with our VCL method. Each row
corresponds to a different hidden layer.

2 The Variance Constancy Loss

The distribution of the activations of each neuron depends on both the distribution of network inputs
and the weight of the network upstream from that neuron. Let ρ be a random variable denoting the
activations of a single neuron and denote the underlying distribution as p. The variance of ρ is given
by σ2 = E[(ρ − µρ)2]  where µρ = E[ρ]. For a ﬁnite sample s = {ρ1...ρn} randomly drawn from
p  the unbiased sample variance of p over s is given by σ2
i=1 ρsi)2. The
variance of the sample variances is given by:

(cid:80)n

n

s = 1
n−1

(cid:80)n
i=1(ρsi − 1
− σ4(n − 3)
n(n − 1)

E[(σ2 − σ2

s )2] =

m4
n

where m4 = E[(ρ − µρ)4] is the fourth moment of ρ [4].
From Eq. 1  given that the distribution has a given variance  the variance of variance is controlled by
n and the fourth moment of the distribution. We would like to show that this variance of measured
variances is low for distributions with few modes.
Intuitively  a distribution with a few distinct
modes would have a low variance of sample variance  since there is a relatively small number of
possibilities to sample from. Consider  for example  a distribution of 2 modes and a sample size of
n. There are only n possible patterns to select from the two modes. For n = 3 there are aaa  aab 
abb  and bbb  where a and b represent selecting from the ﬁrst mode or from the second mode. For a

distribution with k modes  this would be(cid:0)n+k−1

(cid:1)  which can be considerably larger.

k

In the following analysis we characterize distributions with low variance of sample variance. Specif-
ically  we are interested in distributions pρ such that the quantity E[(σ2 − σ2
s )2] is minimized under
the constraint that the variance is ﬁxed  i.e.  σ2 = α. Formally  we are interested in the following
minimization problem:

p∗ = arg min

E[(σ2 − σ2

s )2] s.t σ2 = α

p
Note that we can reformulate Eq. 2 as:

p(cid:63) = arg min

p

E[(1 − σ2

s

σ2 )2] s.t σ2 = α

The next result shows that minimizing Eq. 2 over the space of distributions will result in a distribu-
tion p(cid:63) with two modes.
Theorem 1. Any minimizing distribution of Eq. 2 is of the form ρ(cid:63) = az +b such that z is distributed
according to the Bernoulli distribution with parameter 1

2   and a  b ∈ R  a (cid:54)= 0.

2

(1)

(2)

(3)

Proof. From Eq. 3 and Eq. 1 we have:

p(cid:63)
ρ = arg min

p

(cid:18) m4

α2n

− (n − 3)
n(n − 1)

(cid:19)

(4)

and so we are left with the problem of minimizing the fourth moment of p under the constraint
σ2 = α.
Note that for any distribution  the variance squared is a lower bound for the fourth moment. To see
this  we denote the slack variable y = (ρ − µρ)2  and we have:

var(y) = E[y2] − (E[y])2 = m4 − σ4 ≥ 0

(5)
where equality is attained when var(y) = 0  i.e  when y is constant. Therefore  m4 is minimal when
|ρ − µρ| is constant  which means  since ρ is not constant (σ2 = α > 0)  that p has two values with
equal probability.

The term m4
σ4 in Eq. 4 is called kurtosis and is denoted by κ(ρ). Distributions with high kurtosis tend
to exhibit heavy tails  while distributions with low kurtosis are light tailed  with few outliers. For
the two peak distribution of Thm. 1  there is no tail.

2.1 A Phase Shift Behavior

The condition on the variance in Eq. 3 is redundant  since neurons with ﬁxed activations do not
contribute to learning. We therefore deﬁne the variance constancy loss for a distribution p as:

Ls(p) = E[(1 − σ2

s

σ2 )2]

(6)
This regularization can be seen as an additional unsupervised clustering loss per unit  since it is
minimized by clustering its input to two modes. The driving force for the weights of each unit has a
surprising quality  encouraging separation between clusters if they are prominent enough  or uniting
the clusters if they are not  as demonstrated in the next theorem:
Theorem 2. Consider the random input distributed as a GMM with two components x ∈ Rd ∼
pN (µ1  Σ2) + (1 − p)N (µ2  Σ2). We denote the within and between covariance matrices as Σw =
pΣ1 + (1 − p)Σ2  Σb = (µ1 − µ2)(µ1 − µ2)(cid:62). Given a vector of weights θ ∈ Rd  we denote
ρ = x(cid:62)θ  it holds that:

 arg minθ

arg minθ

θ(cid:62)Σwθ
θ(cid:62)Σbθ
θ(cid:62)Σbθ
θ(cid:62)Σwθ

arg min

κ(ρ) =

θ

1−

√
2 ≤ p ≤ 1+

1
3

√

2

else

1
3

(7)

Proof. Note that ρ ∼ pN (µ(cid:62)
with mean µ and variance σ2  the non-centered fourth and second moments are given by:

1 θ  θ(cid:62)Σ2θ) + (1 − p)N (µ(cid:62)

2 θ  θ(cid:62)Σ2θ). For a Gaussian distribution

m4 = µ4 + 6µ2σ2 + 3σ4  m2 = σ2 + µ2

(8)
Due to the linearity of integration  the moments for a GMM distribution follows naturally. The
mean of rho is given by µ = pµ1 + (1 − p)µ2. Noticing that µ1 − µ = (1 − p)(µ1 − µ2)  and
µ2 − µ = p(µ2 − µ1)  and denoting p(1 − p) = α  the fourth and second moments of ρ are given
by: m4 = α(1 − 3α)(θ(cid:62)Σbθ)2 + 6α(θ(cid:62)Σwθ)(θ(cid:62)Σbθ) + 3(θ(cid:62)Σwθ)2  σ2 = (αθ(cid:62)Σbθ + θ(cid:62)Σwθ).
and so:

κ(ρ) =

α(1 − 3α)(θ(cid:62)Σbθ)2 + 6α(θ(cid:62)Σwθ)(θ(cid:62)Σbθ) + 3(θ(cid:62)Σwθ)2

((α)θ(cid:62)Σbθ + θ(cid:62)Σwθ))2

= 3 +

α(1 − 6α)(θ(cid:62)Σbθ)2
(αθ(cid:62)Σbθ + θ(cid:62)Σwθ)2

(9)

α(1 − 6α) ≤ 0
else

(10)

(cid:18)

(cid:19)

θ

3 +

arg min

α(1 − 6α)(θ(cid:62)Σbθ)2
(αθ(cid:62)Σbθ + θ(cid:62)Σwθ)2
θ(cid:62)Σwθ

αθ(cid:62)Σbθ + θ(cid:62)Σwθ
α(1 − 6α)(θ(cid:62)Σbθ)
θ(cid:62)Σwθ
arg minθ
θ(cid:62)Σbθ
=
θ(cid:62)Σbθ
arg minθ
√
√
θ(cid:62)Σwθ
Note that in the regime where α(1 − 6α) ≤ 0  1−
2 ≤ p ≤ 1+

α(1 − 6α)(θ(cid:62)Σbθ)

= arg max

= arg max

(cid:40)

1
3

1
3

.

2

θ

θ

3

(a)

(b)

(c)

(d)

Figure 2: A single linear unit trained with VCL and no other loss on 2D inputs. (a) a GMM with
p = 0.1. (b) a GMM wioth p = 0.25. (c) the activations of the learned neuron on the input in (a).
(d) similarly for (b). The red lines in (a) and (c) represent the learned projection. For case (a)  since
for p = 0.1 < 0.2113  the projection is such that the two clusters unite. In cases (b)  the projection
provides a perfect discrimination between the clusters.

This can be interpreted as follows: if both clusters have a relatively high prior probabilities  then the
weights of the unit will encourage a separation in the LDA sense. If one cluster has a small prior
probability  then the weights will encourage to merge the clusters together by increasing θ(cid:62)Σwθ 
and decreasing θ(cid:62)Σbθ. See Fig. 2. This might be beneﬁcial for preventing overﬁtting on outliers in
the training set  since artifacts that are speciﬁc to a small number of training examples have a small
prior probability  and will be discouraged from propagating forward.

2.2 A Loss for Stochastic Gradient Descent

We now deﬁne an alternative regularization based on two mini-batches  and prove a minimum upper
bound. Given two sets of iid samples s1 = {ρ1...ρn}  s2 = {ρ(cid:48)

n}  we deﬁne loss variant:

1...ρ(cid:48)

(cid:19)2

(cid:18)

1 − σ2
s1
σ2
s2

Ls1 s2(p) =

The following theorem shows an upper bound on the deviation of the ratio σ2
Theorem 3. It holds that for every 1 >  > 0:
)2 ≤ 42

(cid:19)

(cid:18)

κ(ρ)

≥

P r

s1
σ2
s2

1 − 1
2 (

n

− (n − 3)
n(n − 1)

)

(1 − )2

from 1.

(cid:19)2

Proof. From Chebyshev’s inequality  it holds that for any set of iid samples s = {x1...xn}:

(cid:18) 42
(1 + )2 ≤ (1 − σ2
(cid:18)

s1
σ2
s2

|1 − σ2
sE[σ2
s ]
and so with probability of at least 1 − var(σ2
s ])2 it holds that 1 −  ≤ σ2
s )
sE[σ2
2(E[σ2
] = E[σ2
] = σ2 we have that:
s1  s2 with var(σ2
s1

≤ var(σ2
s )
2(E[σ2
s ])2

) = var(σ2
s2

) and E[σ2

| > 

P r

s1

s2

s ] ≤ 1 + . for two iid sets

(cid:18)

(cid:19)

(cid:19)

≥

(cid:19)
(cid:18)
(cid:19)

(cid:18)

(cid:18)

(11)

(12)

(13)

(14)

(15)

(16)

P r

1 −  ≤ σ2
s1E[σ2

s1

σ2
s2E[σ2

s2

]

 

]

≤ 1 + 

≥

1 − var(σ2
s1
2σ4

)

The bound for the ratio follows naturally:

(cid:18) 1 − 

P r

1 + 

(cid:18) 42
(1 + )2 ≤ (1 − σ2

s1
σ2
s2

≤ σ2
s1
σ2
s2

≤ 1 + 
1 − 

1 − var(σ2
s1
2σ4

)

)2 ≤ 42

(1 − )2

≥

1 − var(σ2
s1
2σ4

and:

P r

Replacing var(σ2
s1

) with Eq. 1 completes the proof.

4

(cid:19)2

(cid:19)2

(cid:19)2

)

Note that the RHS of Eq. 12 is maximized when κ(ρ) is minimized  similarly to Thm. 1.
In practice  the regularization used during training must be robust to instances where σ2
s2
so the variance constancy loss (VCL) we advocate for is
(p) = (1 − σ2
σ2
s2

s1
+ β

Lβ

s1 s2

)2

≈ 0  and

(17)

for some β > 0. This modiﬁcation has a two-fold effect. It both stabalizes the loss by preventing
exploding gradients  and it encourages the variance for each neuron output to grow. The latter is
due to the fact that  by multiplying the activations by a constant scale larger than one  β becomes
more insigniﬁcant. In other words  for β = 0 the distance between the peaks of the distribution is
non-consequential. As β grows  there is a stronger driving force that separates the two modes. In
our experiments  in order to avoid searching for global optimal values of β  and since the optimal β
can vary between layers and neurons  we optimize for this value per-neuron. This is reminiscent to
the per-neuron ﬁtting of the additive and multiplicative values in batchnorm.
Note that optimizing m4 directly is not advisable  since estimating higher moments from small
batches is prone to large estimation errors.

2.3 Batchnorm as a Minimizer of Kurtosis

The use of batchnorm during training of neural networks has been shown to improve test perfor-
mance  as well as speed up training time. In batchnorm  sample statistics of each mini-batch are
calculated  and used for normalization of the activations (either before or after the application of
non-linearity). Speciﬁcally  each activation is normalized to have zero mean and unit variance. This
scheme introduces additional randomness in the network  since the output of a unit depends on the
particular mini-batch statistics  as well as the particular input sample. Since the sample mean is
a much more reliable statistic than the sample variance  most of the randomness is caused by the
variance of the sample variance.
Consider a single unit ρx that undergoes batch-norm during training. The output of that unit given
input x and batch s is given by ρ(x)−µs
. We expect the batch statistics σs  µs to be reliable approx-
imations of the actual statistics  otherwise performance would vary wildly between test and train
splits  as well as between mini-batches during training. We therefore expect for each sample x:

σs

(cid:12)(cid:12)(cid:12)(cid:12) << 1

− 1

ρ(x) − µs
ρ(x) − µ
ρ(x)−µ ≈ 1. Since this applies to all

(18)

We note that µs is a more reliable statistic than σs  and so ρ(x)−µs
inputs x  we have:

(cid:12)(cid:12)(cid:12)(cid:12) ρ(x) − µs

σs

− ρ(x) − µ

σ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) σ

σs

(cid:12)(cid:12)(cid:12)(cid:12) =

σ

(cid:12)(cid:12)(cid:12)(cid:12) ρ(x) − µ
(cid:12)(cid:12)(cid:12)(cid:12) << 1 

− 1

≈ 1

σ
σs

(cid:12)(cid:12)(cid:12)(cid:12) σ
(cid:19)

σs

(cid:18) 1√

From Chebyshev’s inequality  it holds for 1 >  > 0:
≥ 1 − var(σ2
s )
2(E[σ2

1√
1 − 

≤ σ
σs

1 + 

≤

P r

s ])2 = 1 − 1

2

(cid:18) κ(ρ)

n

− (n − 3)
n(n − 1)

(cid:19)

(19)

(20)

Therefore  under mild assumptions  a low value for the Kurtosis leads to a stable application of
batchnorm. Note that in batchnorm  Eq. 18 is not forced  and so kurtosis is not explicitly minimized.

2.4 The Loss in Action

According to Thm. 3  when the sample size n is large  the bound on the probability in the RHS of
Eq.12 is high regardless of κ. Therefore  the ratio of the sample variance and the true variance is
close regardless of the shape of the distribution. This favors small n for the VCL method. Empiri-
cally  we notice that VCL tends to work better as n is lower  where the best results for CNN models
are achieved when setting n = 2.
We opt for the simplest way to sample minibatches of size n for the loss  without changing the
mini-batches that are used for the SGD procedure. Assume that the size of the SGD minibatches

5

is N. Typically n < 2N  and we take out of the N samples of the SGD minibatch the ﬁrst two
consecutive subsets of size n. The variance constancy loss (VCL) is computed based on these two
arbitrary subsets. In all of our experiments β is set to an initial value of 1.0  and then updated for
each unit through backpropagation. In our experiments  the VCL terms are averaged in each layer 
and then summed up across layers. A weight γ is applied to this loss.
When n is very small  training becomes unstable due to increasing random variations in sample
statistics. This instability is minimized by VCL  which increases its overall inﬂuence. In order to
support such small n  training is stabilized by performing gradient clipping. Speciﬁcally  the L2
norm of the gradient of each layer is clipped  with a clipping value of 1.

3 Experiments

Comparing different activation functions or different normalization schemes and their combinations 
is a notorious task: every choice beneﬁts the most from a different set of hyperparameters  leading to
large search space and high computational demands and  often  reproducibility issues. The authors
of [11]  for example  provided an exemplary set of experiments to demonstrate that their SeLU
activation function outperforms other activation functions. For the UCI datasets  the authors provide
detail experimental protocols  some code  and all the train/test splits. Despite all these  we were not
able to completely replicate their UCI experiments for various reasons. First  our resources allowed
us to test less architectures by the deadline. Second  we were uncertain regarding  for example  the
amount and location of dropout used. In another example  we were able to replicate the CIFAR
experimental result for the ELU activation function [2]. However  unlike the published results  in
our experiments  batchnorm improves the accuracy. This highlights the challenges of comparative
experiments  but is in no way a criticism on the previous work. Indeed  both ELU and SeLU have
provided a great deal of performance gain in a wide variety of follow-up work.
We demonstrate the effectiveness of VCL regularization on several benchmark datasets  comparing
with competitive baselines. We conduct two sets of experiments. In the ﬁrst set of experiments  we
test CNNs on the CIFAR-10  CIFAR-100 and tiny Imagenet datasets. In the second  we evaluate
fully connected networks on all of the UCI datasets with more than 1000 samples. To support
reproducibility  the entire code of all of our experiments is to be promptly released.

CIFAR The two CIFAR datasets (Krizhevsky Hinton  2009) consist of colored natural images
sized at 32×32 pixels. CIFAR-10 (C10) and CIFAR-100 (C100) images are drawn from 10 and 100
classes  respectively. For each dataset  there are 50 000 training images and 10 000 images reserved
for testing. We use a standard data augmentation scheme (Lin et al.  2013; Romero et al.  2014;
Lee et al.  2015; Springenberg et al.  2014; Srivastava et al.  2015; Huang et al.  2016b; Larsson et
al.  2016)  in which the images are zero-padded with 4 pixels on each side  randomly cropped to
produce 32×32 images  and horizontally mirrored with probability 0.5.

For the CIFAR datasets  we employ the 11-layer architecture that was used by [2] to compare ac-
tivation functions. The 18-layer architecture was trained with a dedicated dropout scheduling that
makes it more speciﬁc to a certain choice of activation function  and is slower to train. We do not
employ ZCA whitening on the data since it seems to decrease the overall accuracy for ReLU and
Learky ReLU. For all experiments  500 epochs are used and a batch size N of 250. We employ a
learning rate of 0.05  which was reduced at epoch 180 to 0.02  and further reduced by a factor of
10 every 100 epochs. A momentum of 0.9 was used and the L2 regularization term was weighed
by 0.0001. The hyperparameters of VCL are ﬁxed: the weight of the VCL regularization is set to
γ = 0.01.
The results are presented in Tab. 2  with running time per training iterations comparisons presented
in Tab. 1. We compare ReLU to Leaky ReLU with a constant of 0.2 and to ELU  with different
normalization techniques. Experiments with VCL are performed with n = 2  3  5  7  9. Our result
for CIFAR-100 of the ELU activation matches the reported result in [2] (CIFAR-10 result is not
provided for this architecture). As can be seen  batchnorm contributes to ReLU and ELU but not
to Leaky ReLU. The best results are obtained with a combination of ELU and our VCL method for
both datasets. The only experiment in which VCL does not contribute more than batchnorm is the
ReLU experiment on CIFAR-100. The largest contribution of VCL is to ELU.

6

Table 1: Time in Seconds per 100 iterations (CIFAR-100).

Method
Without normalization
Batchnorm
VCL

Intel i7 CPU Volta GPU

367.1
702.3
400.1

29.2
31.6
30.3

Table 2: Test error w/o normalization  with bathnorm (bn)  layer normalization (ln)  group normal-
ization (gn) or vcl.

CIFAR-10 CIFAR-100

CIFAR-10 CIFAR-100

ReLU
ReLU+bn
ReLU+ln
ReLU+gn
ReLU+vcl (n = 9)
ReLU+vcl (n = 7)
ReLU+vcl (n = 5)
ReLU+vcl (n = 3)
ReLU+vcl (n = 2)
LReLU
LReLU+bn
LReLU+ln
LReLU+gn

0.0836
0.0778
0.0792
0.0871
0.0780
0.0810
0.0785
0.0790
0.0780
0.0670
0.0708
0.0700
0.0707

(Continued to the right)

0.328
0.291
0.307
0.319
0.308
0.305
0.304
0.306
0.303
0.268
0.272
0.270
0.283

LReLU+vcl (n = 9)
LReLU+vcl (n = 7)
LReLU+vcl (n = 5)
LReLU+vcl (n = 3)
LReLU+vcl (n = 2)
ELU
ELU+bn
ELU+ln
ELU+gn
ELU+vcl (n = 9)
ELU+vcl (n = 7)
ELU+vcl (n = 5)
ELU+vcl (n = 3)
ELU+vcl (n = 2)

0.0660
0.0665
0.0648
0.0657
0.0645
0.0698
0.0663
0.0675
0.0671
0.0670
0.0633
0.0615
0.0622
0.0615

0.267
0.264
0.264
0.262
0.263
0.287
0.269
0.267
0.282
0.276
0.271
0.258
0.261
0.256

Tiny Imagenet The Tiny ImageNet dataset consists of a subset of ImageNet [16]  with 200 differ-
ent classes  each of which has 500 training images and 50 validation images  downscaled to 64×64.
For augmentation  the images are zero padded with 8 pixels on each side  and randomly cropped to
produce 64 × 64 images  and then horizontally mirrored with probability 0.5.
For this set  we employ a similar architecture used for the CIFAR experiments  with twice as many
convolutional kernels per layer.
In order to account for the higher resolution images  we apply
average pooling at the end of the 5’th convolutional block. We also use the same hyper parameters
as in the CIFAR experiments  namely γ = 0.01  and n = 5. A learning rate of 0.05 is employed 
which is reduced to 0.02 after 50 epochs  and further reduced by 10 at 100 and 180 epochs. We
report the validation accuracy after 250 epochs. The results are reported in Tab. 3. Results for
Resnet-110  WRN-32  DenseNet-40 are as reported in [6].

UCI We also apply VCL to the 44 UCI datasets with more than 1000 samples. The train/test splits
were provided by the authors of [11]. In each experiment  we three ﬁxed architectures with 256
hidden neurons per layer and depth of either 4  8  or 16. For ReLU and ELU the last layer had a
dropout rate of 0.5. For SeLU  we employ the prescribed α−dropout rate of 0.05 for all hidden
layers. A learning rate of 0.01 was used for the ﬁrst 200 epochs and then a learning rate of 10−3 was
used. All runs were terminated after 500 epochs. Following [11]  an averaging operator with a mask
size of 10 was applied to the validation error  and the epoch and architecture with the best smoothed

Validation error

Validation error

Deep ELU network
Deep ELU network + bn
Deep ELU network + vcl n=2

0.392
0.402
0.373

ResNet-110
Wide-ResNet-32
DenseNet-40

0.465
0.365
0.390

Table 3: Validation error on tiny imagenet. We ran the three Deep ELU experiments. The baseline
results are from [6].

7

Figure 3: An accuracy based Dolan-More proﬁle for the UCI experiments of Tab. 5. There are 9
plots  one for each combination of activation and normalization. The x-axis is the threshold (τ).
Since for accuracy scores  higher is better  whereas typical Dolan-More plots show cost (such as
run-time)  the axis is ordered in reverse. The y-axis is  for a given combination out of the 9  the ratio
of datasets in which the obtained accuracy is above τ times the maximal accuracy over all 9 options.

Table 4: Number of “wins” for each normalization method  per activation function.

ReLU ELU SELU

No normalization
Batchnorm
VCL

9
15
27

14
16
23

11
15
28

validation error was selected. Batches were of size N = 20  γ = 0.01  and  for these experiments 
n = 10.
The results are shown in Fig. 3 and fully reported in the appendix (Tab. 5). As expected  no method
wins across all experiments. However  the results show that the method that wins the most (out of
the 9 options) is either the combination of SeLU and VCL or that of ELU and VCL. A breakdown
per each activation unit separately is presented in Tab. 4. A win is counted if the method reaches
the minimal value among the three normalization options and if performance is not constant. For all
three activation functions  VCL provides more wins than batchnorm  and batchnorm outperforms
the no normalization option. The gap between VCL and batchnorm is larger for SELU and the
lowest for ReLU  which is also consistent with the results in Tab. 2.

4 Related Work

The seminal batchnorm method [8] has enabled a markable increased in performance for a great
number of machine learning tasks  ranging from computer vision [5] to playing board games [20].
In practice  the method is said to suffer from a few limitations [17  7  24]. One of these limitations is
the reliance on the batch statistics during the forward step  including at test time  which is performed
one sample at a time. The training statistics are therefore used as surrogates at test time  which is
detrimental as there is a shift between the training and the test distributions [14]. Our method  as a
loss-based method  does not employ batch statistics at test time.
Another limitation of batchnorm is the reliance on batch statistics  which are unreliable for small
batches. This leads to the need to employ larger batches  which tend to result in worse generaliza-
tion [24]. This disadvantage turns into an advantage in our method  since this instability is what our
method aims to reduce. Indeed  we perform our experiments with only a few samples for the VCL
loss.

8

0.950.9550.960.9650.970.9750.980.9850.990.995100.10.20.30.40.50.60.70.80.91ReLUReLU+batchnormReLU+vclELUELU+batchnormELU+vclSELUSELU+batchnormSELU+vclOther normalization techniques  which do not rely on batch statistics include classical methods  such
as local response normalization [13  9  12]  layer normalization [1]  instance normalization [22] 
weight normalization [18]  and the very recent group normalization [24].
Since our regularization term encourages bimodal activation distributions  it is somewhat related
to the study of networks with binary activation functions [3]. However  our goal is to increase the
classiﬁcation accuracy and not to achieve the efﬁciency beneﬁts of binary activations.
Considering one of the modes as a baseline activation  our work can be viewed as related to sparsity
regularization methods  including L1 regularization [21] and its local or selective application [19  25]
and structural sparsiﬁcation methods [23] that also modify the architecture by pruning some of the
connections. Such methods lead to more efﬁcient networks as well as to an improvement in accuracy.
Our method is also related to variational methods such as the variational autoencoder [10]  which
employs a regularization term that enforces a certain distribution on some of the activations. The
target distribution is often taken to be Gaussian in contract to our loss term that encourages multiple
modes. In this sense  our work is more related to discrete variational autoencoders [15]. In contrast
to such work  our method employs the regularization term everywhere  the multi-modal structure is
soft  and the number of modes is not enforced (Thm. 2  and the fact that multi-peak distributions
with more than 2 peaks also have low Kurtosis).

5 Conclusions

The batchnorm method plays a pivotal role in many of the recent successes of deep learning. With
the growing dependency on this method  some researchers have voiced concerns about the required
batch sizes. VCL employs small subsets of the mini-batch and seems to perform as well or better
than batchnorm on the standard benchmarks tested. It therefore holds the promise of improving
conditioning without imposing constraints on the optimization process. Since VCL is a regulariza-
tion term and not a normalization mechanism  and since the statistics of sample moments is well
understood  the new method could be compatible with a wider variety of optimization methods in
comparison to bachnorm. Compared to other loss terms  VCL shapes the activation distribution in
one of several phases  according to the input statistics.
As future work  we would like to address some limitations that were observed during the experi-
ments. The ﬁrst is the observation that while VCL shows good results with the ReLU activations on
the UCI experiences  in image experiments the combination of the two underperforms when com-
pared to ReLU with batchnorm. The second limitation is that so far we were not able to replace
batchnorm with VCL for ResNets.

Acknowledgements

This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant ERC CoG 725974).

References
[1] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450  2016.

[2] Djork-Arn´e Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep network

learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289  2015.

[3] Matthieu Courbariaux  Itay Hubara  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Bina-
rized neural networks: Training deep neural networks with weights and activations constrained
to+ 1 or-1. arXiv preprint arXiv:1602.02830  2016.

[4] Donald Estep  Axel Malqvist  and Simon Tavener. Error estimation and adaptive computation

for elliptic problems with randomly perturbed data. 2006.

[5] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion  pages 770–778  2016.

9

[6] Gao Huang  Yixuan Li  Geoff Pleiss  Zhuang Liu  John E. Hopcroft  and Kilian Q. Weinberger.

Snapshot ensembles: Train 1  get M for free. arXiv preprint arXiv:1704.00109  2017.

[7] Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-
normalized models. In Advances in Neural Information Processing Systems  pages 1942–1950 
2017.

[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[9] Kevin Jarrett  Koray Kavukcuoglu  Yann LeCun  et al. What is the best multi-stage architecture
In Computer Vision  2009 IEEE 12th International Conference on 

for object recognition?
pages 2146–2153. IEEE  2009.

[10] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[11] G¨unter Klambauer  Thomas Unterthiner  Andreas Mayr  and Sepp Hochreiter.

Self-
normalizing neural networks. In Advances in Neural Information Processing Systems  pages
972–981  2017.

[12] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[13] Siwei Lyu and Eero P Simoncelli. Nonlinear image representation using divisive normaliza-
tion. In Computer Vision and Pattern Recognition  2008. CVPR 2008. IEEE Conference on 
pages 1–8. IEEE  2008.

[14] Sylvestre-Alvise Rebufﬁ  Hakan Bilen  and Andrea Vedaldi. Learning multiple visual domains
with residual adapters. In Advances in Neural Information Processing Systems  pages 506–516 
2017.

[15] Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200  2016.
[16] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael S. Bernstein  Alexander C. Berg  and Fei-Fei
Li. Imagenet large scale visual recognition challenge. CoRR  2014.

[17] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems 
pages 2234–2242  2016.

[18] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization
to accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems  pages 901–909  2016.

[19] Simone Scardapane  Danilo Comminiello  Amir Hussain  and Aurelio Uncini. Group sparse

regularization for deep neural networks. Neurocomputing  241:81–89  2017.

[20] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  et al. Mastering the game
of go without human knowledge. Nature  550(7676):354  2017.

[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[22] D Ulyanov  A Vedaldi  and V Lempitsky. Instance normalization: The missing ingredient for

fast stylization. arXiv preprint arXiv:1607.08022  2016.

[23] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity
in deep neural networks. In Advances in Neural Information Processing Systems  pages 2074–
2082  2016.

[24] Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494  2018.
[25] Jaehong Yoon and Sung Ju Hwang. Combined group and exclusive sparsity for deep neural

networks. In International Conference on Machine Learning  pages 3958–3966  2017.

10

A More results

Table 5: The results of the UCI experiments

ELU
bn
0.342
0.148
0.106
0.018
0.223
0.108
0.307
0.009
0.139
0.506
0.268
0.006
0.319
0.037
0.131
0.070
0.192
0
0.004
0.194
0.09
0.026
0.036
0.042
0.040
0.314
0.462
0.279
0.025
0.103
0.063
0.289
0.038
0.110
0.001
0.276
0.021
0.208
0.029
0.105
0.164
0.164
0.424
0.482
8

vcl
0.330
0.155
0.099
0.036
0.204
0.104
0.207
0.016
0.143
0.490
0.301
0.012
0.295
0.044
0.130
0.073
0.189
0
0
0.202
0.085
0.032
0.029
0.032
0.038
0.296
0.387
0.282
0.018
0.107
0.068
0.228
0.04
0.106
0.001
0.274
0.017
0.208
0.029
0.093
0.163
0.161
0.431
0.478
11

0.343
0.150
0.112
0.031
0.219
0.103
0.226
0.010
0.143
0.475
0.276
0.012
0.299
0.043
0.130
0.081
0.172
0
0
0.199
0.097
0.040
0.029
0.033
0.041
0.305
0.419
0.278
0.025
0.112
0.066
0.245
0.041
0.095
0.001
0.281
0.021
0.214
0.026
0.103
0.162
0.151
0.432
0.469
2

SeLU
bn
0.335
0.148
0.110
0.032
0.211
0.109
0.435
0.010
0.144
0.501
0.349
0.012
0.290
0.037
0.125
0.068
0.163
0
0.006
0.181
0.088
0.030
0.047
0.033
0.035
0.321
0.463
0.283
0.035
0.115
0.069
0.243
0.044
0.104
0.004
0.276
0.024
0.215
0.027
0.104
0.163
0.160
0.413
0.491
7

vcl
0.339
0.147
0.107
0.025
0.202
0.097
0.218
0.010
0.139
0.454
0.187
0.012
0.305
0.045
0.126
0.080
0.194
0
0
0.184
0.093
0.032
0.031
0.036
0.037
0.281
0.403
0.268
0.021
0.115
0.070
0.242
0.040
0.100
0.001
0.272
0.019
0.208
0.025
0.102
0.153
0.147
0.417
0.485
11

0.325
0.152
0.103
0.039
0.214
0.108
0.217
0.020
0.153
0.490
0.272
0.006
0.295
0.054
0.138
0.075
0.18
0
0.001
0.205
0.090
0.034
0.031
0.039
0.044
0.280
0.393
0.273
0.021
0.105
0.070
0.252
0.051
0.108
0.001
0.285
0.020
0.208
0.028
0.106
0.164
0.149
0.397
0.461
5

abalone
adult
bank
car
cardio.-10clases
cardio.-3clases
chess-krvk
chess-krvkp
connect-4
contrac
hill-valley
image-segmentation
led-display
letter
magic
miniboone
molec-biol-splice
mushroom
nursery
oocytes-m.-nucleus-4d
oocytes-m.-states-2f
optical
ozone
page-blocks
pendigits
plant-margin
plant-shape
plant-texture
ringnorm
semeion
spambase
statlog-german-credit
statlog-image
statlog-landsat
statlog-shuttle
steel-plates
thyroid
titanic
twonorm
wall-following
waveform-noise
waveform
wine-quality-red
wine-quality-white
Number of wins
out of 9 options

0.334
0.156
0.112
0.054
0.221
0.106
0.250
0.027
0.146
0.502
0.530
0.006
0.309
0.061
0.138
0.084
0.214
0
0.007
0.228
0.091
0.039
0.028
0.039
0.043
0.291
0.433
0.297
0.022
0.116
0.075
0.296
0.045
0.114
0.001
0.299
0.026
0.208
0.030
0.126
0.173
0.164
0.413
0.461
3

ReLU
bn
0.342
0.148
0.108
0.029
0.238
0.110
0.301
0.015
0.153
0.546
0.399
0
0.326
0.038
0.135
0.090
0.223
0
0.005
0.209
0.096
0.025
0.033
0.037
0.041
0.305
0.442
0.281
0.026
0.110
0.075
0.273
0.041
0.113
0.001
0.280
0.024
0.208
0.040
0.115
0.197
0.166
0.414
0.483
3

vcl
0.331
0.155
0.109
0.041
0.224
0.096
0.233
0.023
0.150
0.480
0.270
0.006
0.307
0.051
0.133
0.083
0.205
0
0.005
0.196
0.093
0.033
0.031
0.039
0.037
0.282
0.420
0.297
0.021
0.111
0.068
0.248
0.047
0.106
0.001
0.270
0.021
0.208
0.028
0.112
0.172
0.167
0.404
0.468
3

11

,Jingwei Liang
Jalal Fadili
Gabriel Peyré
Etai Littwin
Lior Wolf