2018,PAC-Bayes bounds for stable algorithms with instance-dependent priors,PAC-Bayes bounds have been proposed to get risk estimates based on a training sample. In this paper the PAC-Bayes approach is combined with stability of the hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is used with a Gaussian prior centered at the expected output. Thus a novelty of our paper is using priors defined in terms of the data-generating distribution. Our main result estimates the risk of the randomized algorithm in terms of the hypothesis stability coefficients. We also provide a new bound for the SVM classifier  which is compared to other known bounds experimentally. Ours appears to be the first uniform hypothesis stability-based bound that evaluates to non-trivial values.,PAC-Bayes bounds for stable algorithms with

instance-dependent priors

Omar Rivasplata

UCL

Emilio Parrado-Hern´andez
University Carlos III of Madrid

John Shawe-Taylor

UCL

Shiliang Sun

East China Normal University

Csaba Szepesv´ari

DeepMind

Abstract

PAC-Bayes bounds have been proposed to get risk estimates based on a training
sample. In this paper the PAC-Bayes approach is combined with stability of the
hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is
used with a Gaussian prior centered at the expected output. Thus a novelty of our
paper is using priors deﬁned in terms of the data-generating distribution. Our main
result estimates the risk of the randomized algorithm in terms of the hypothesis
stability coefﬁcients. We also provide a new bound for the SVM classiﬁer  which
is compared to other known bounds experimentally. Ours appears to be the ﬁrst
uniform hypothesis stability-based bound that evaluates to non-trivial values.

1

Introduction

This paper combines two directions of research: stability of learning algorithms  and PAC-Bayes
bounds for algorithms that randomize with a data-dependent distribution. The combination of these
ideas enables the development of risk bounds that exploit stability of the learned hypothesis but are
independent of the complexity of the hypothesis class. The PAC-Bayes framework (Shawe-Taylor
and Williamson [1997]  McAllester [1999a b]) is used here with ‘priors’ deﬁned in terms of the
data-generating distribution  as introduced by Catoni [2007] and developed further e.g. by Lever
et al. [2010]  Parrado-Hern´andez et al. [2012] and Dziugaite and Roy [2018]. Speciﬁcally  our work
derives PAC-Bayes bounds for hypothesis stable Hilbert space valued algorithms.
The analysis introduced by Bousquet and Elisseeff [2002]  which followed and extended Lugosi
and Pawlak [1994] and was further developed by Celisse and Guedj [2016]  Abou-Moustafa and
Szepesv´ari [2017] and Liu et al. [2017] among others  shows that stability of learning algorithms
can be used to give bounds on the generalisation of the learned functions. Intuitively  this is because
stable learning should ensure that slightly different training sets give similar solutions. In this paper
stability is measured by the sensitivity coefﬁcients (see our Deﬁnition 1) of the hypothesis learned
by a Hilbert space valued algorithm. We provide an analysis leading to a PAC-Bayes bound for
randomized classiﬁers under Gaussian randomization. As a by-product of the stability analysis we
derive a concentration inequality for the learned hypothesis. Applying it to Support Vector Machines
(Shawe-Taylor and Cristianini [2004]  Steinwart and Christmann [2008]) we deduce a concentration
bound for the SVM weight vector  and a PAC-Bayes performance bound for SVM with Gaussian
randomization. Experimental results compare our new bound with other stability-based bounds  and
with a more standard PAC-Bayes bound. We also experiment with their use in model selection.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

2 Deﬁnitions and Main Result(s)

We consider a learning problem where the learner observes pairs (Xi  Yi) of patterns (inputs) Xi
from the space1 X and labels Yi in the space Y. A training set (or sample) is a ﬁnite sequence
Sn = ((X1  Y1)  . . .   (Xn  Yn)) of such observations. Each pair (Xi  Yi) is a random element of
X⇥Y whose (joint) probability law is2 P 2 M1(X⇥Y ). We think of P as the underlying ‘true’ (but
unknown) data-generating distribution. Examples are i.i.d. (independent and identically distributed)
in the sense that the joint distribution of Sn is the n-fold product measure P n = P ⌦···⌦ P .
A learning algorithm is a function A : [n(X⇥Y )n !Y X that maps training samples (of any size)
to predictor functions. Given Sn  the algorithm produces a learned hypothesis A(Sn) : X!Y that
will be used to predict the label of unseen input patterns X 2X . Typically X⇢ Rd and Y⇢ R.
For instance  Y = {1  1} for binary classiﬁcation  and Y = R for regression. A loss function
` : R ⇥Y! [0 1) is used to assess the quality of hypotheses h : X!Y . Say if a pair (X  Y ) is
sampled  then `(h(X)  Y ) quantiﬁes the dissimilarity between the label h(X) predicted by h  and
the actual label Y . We may write `h(X  Y ) = `(h(X)  Y ) to express the losses (of h) as function of
the training examples. The (theoretical) risk of hypothesis h under data-generating distribution P is
R(h  P ) = h`h  Pi.3 It is also called the error of h under P . The empirical risk of h on a sample Sn
nPn
i=1 (Xi Yi) is the empirical measure4 on X⇥Y associated
is R(h  Pn) = h`h  Pni where Pn = 1
to the sample. Notice that the risk (empirical or theoretical) is tied to the choice of a loss function.
For instance  consider binary classiﬁcation with the 0-1 loss `01(y0  y) = 1[y0 6= y]  where 1[·] is an
indicator function equal to 1 when the argument is true and equal to 0 when the argument is false.
In this case the risk is R01(c  P ) = P [c(X) 6= Y ]  i.e.  the probability of misclassifying the random
nPn
example (X  Y ) ⇠ P when using c; and the empirical risk is R01(c  Pn) = 1
i=1 1[c(Xi) 6= Yi] 
i.e.  the in-sample proportion of misclassiﬁed examples.
Our main theorem concerns Hilbert space valued algorithms  in the sense that the learned hypotheses
live in a Hilbert space H. In this case we may use the Hilbert space norm kwkH =phw  wiH to
measure the difference between the hypotheses learned from two slightly different samples.
To shorten the notation we will write Z = X⇥Y . A generic element of this space is z = (x  y) 
the observed examples are Zi = (Xi  Yi) and the sample of size n is Sn = (Z1  . . .   Zn).
Deﬁnition 1. Consider a learning algorithm A : [nZ n !H where H is a separable Hilbert space.
We deﬁne5 the hypothesis sensitivity coefﬁcient of A at sample size n as follows:
kA(z1:i1  zi  zi+1:n)  A(z1:i1  z0i  zi+1:n)kH .

n = sup
i2[n]

sup
zi z0i

This is close in spirit to what is called “uniform stability” in the literature  except that our deﬁnition
concerns stability of the learned hypothesis itself (measured by a distance on the hypothesis space) 
while e.g. Bousquet and Elisseeff [2002] deal with stability of the loss functional. The latter could be
called “loss stability” (in terms of “loss sensitivity coefﬁcients”) for the sake of informative names.
Writing z1:n ⇡ z01:n when these n-tuples differ at one entry (at most)  an equivalent formulation to
the above is n = supz1:n⇡z01:n kA(z1:n)  A(z01:n)kH. In particular  if two samples Sn and S0n
differ only on one example  then kA(Sn)  A(S0n)kH  n. Thus our deﬁnition implies stability
with respect to replacing one example with an independent copy. Alternatively  one could deﬁne
n = ess supSn⇡S0n kA(Sn)  A(S0n)kH  which corresponds to the “uniform argument stability”
of Liu et al. [2017]. We avoid the ‘almost-sure’ technicalities by deﬁning our n’s as the maximal
difference (in norm) with respect to all n-tuples z1:n ⇡ z01:n. The extension to sensitivity when
changing several examples is natural: kA(z1:n)  A(z01:n)kH  nPn
i=1 1[zi 6= z0i]. Note that
n is a Lipschitz factor with respect to the Hamming distance. The “total Lipschitz stability” of
Kontorovich [2014] is a similar notion for stability of the loss functional. The “collective stability”
of London et al. [2013] is not comparable to ours (different setting) despite the similar look.

1All spaces where random variables take values are assumed to be measurable spaces.
2M1(Z) denotes the set of all probability measures over the space Z.
3 Mathematicians write hf  ⌫i def= RX⇥Y
with respect to a (not necessarily probability) measure ⌫ on X⇥Y .
4 Integrals with respect to Pn evaluate as follows:RX⇥Y

f (x  y) d⌫(x  y) for the integral of a function f : X⇥Y! R
i=1 `(h(Xi)  Yi).
5 For a list ⇠1 ⇠ 2 ⇠ 3  . . . and indexes i < j  we write ⇠i:j = (⇠i  . . .  ⇠ j)  i.e.  the segment from ⇠i to ⇠j.

`(h(x)  y) dPn(x  y) = 1

nPn

2

We will consider randomized classiﬁers that operate as follows. Let C be the classiﬁer space  and let
Q 2 M1(C) be a probability distribution over the classiﬁers. To make a prediction the randomized
classiﬁer picks c 2C according to Q and predicts a label with the chosen c. Each prediction is
made with a fresh c draw. For simplicity we use the same label Q for the probability distribution and
for the corresponding randomized classiﬁer. The risk measures R(c  P ) and R(c  Pn) are extended
to randomized classiﬁers: R(Q  P ) ⌘ RC
R(c  P ) dQ(c) is the average theoretical risk of Q  and
R(Q  Pn) ⌘RC
R(c  Pn) dQ(c) its average empirical risk. Given two distributions Q0  Q 2 M1(C) 

the Kullback-Leibler divergence (a.k.a. relative entropy) of Q with respect to Q0 is

KL(QkQ0) =ZC
) + (1  q) log( 1q
1q0

log dQ
dQ0 dQ .
)  and kl+(qkq0) = kl(qkq0)1[q < q0].

Of course this makes sense when Q is absolutely continuous with respect to Q0  which ensures that
the Radon-Nikodym derivative dQ/dQ0 exists. For Bernoulli distributions with parameters q and q0
we write kl(qkq0) = q log( q
2.1 Main theorem: a PAC-Bayes bound for stable algorithms with Gaussian randomization
Theorem 2. Let A be a Hilbert space valued algorithm. Suppose that (once trained) the algorithm
will randomize according to Gaussian distributions Q = N (A(Sn)  2I).
If A has hypothesis
stability coefﬁcient n  then for any randomization variance 2 > 0  for any  2 (0  1)  with
probability  1  2 we have

q0

kl+(R(Q  Pn)kR(Q  P )) 

n2
n

22 ⇣1 +q 1

⌘2
2 log 1

n

+ log( n+1
 )

.

The proof  see Section 4 below  combines stability of the learned hypothesis (as in our Deﬁnition 1)
and a PAC-Bayes theorem quoted there for reference. Note that the randomizing distribution Q
is data-dependent. Literature on the PAC-Bayes framework for learning linear classiﬁers includes
Germain et al. [2015] and Parrado-Hern´andez et al. [2012] with references. Applications of this
framework to neural networks are given  e.g.  by London [2017]  Dziugaite and Roy [2017]  and
Dziugaite and Roy [2018]. The latter combines PAC-Bayes and a substantially different stability
notion: they use distributional stability for choosing a prior in a data-dependent manner.

2.2 Application: a PAC-Bayes bound for SVM with Gaussian randomization
For a Support Vector Machine (SVM) with feature map ' : X!H into a separable Hilbert space
H  we may identify6 a linear classiﬁer cw(·) = sign(hw  '(·)i) with a vector w 2H . With this
identiﬁcation we can regard an SVM as a Hilbert space7 valued mapping that based on a training
sample Sn learns a weight vector Wn = SVM(Sn) 2H . In this context  stability of the SVM’s
solution then reduces to stability of the learned weight vector.
To be speciﬁc  let SVM(Sn) be the SVM that regularizes the empirical risk over the sample Sn by
solving the following optimization problem:

w ✓ 

`(cw(Xi)  Yi)◆ .

nXi=1
n (Example 2 of Bousquet and Elisseeff [2002] 
Our stability coefﬁcients in this case satisfy n  2
adapted to our setting). Then a direct application of our Theorem 2 together with a concentration
argument for the SVM weight vector (see our Corollary 9 below) gives the following:
Corollary 3. Let Wn = SVM(Sn). Suppose that (once trained) the algorithm will randomize
according to Gaussian8 distributions Q = N (Wn  2I). For any randomization variance 2 > 0 
for any  2 (0  1)  with probability  1  2 we have

2kwk2 +

arg min

(svm)

1
n

kl+(R(Q  Pn)kR(Q  P )) 

2

22n⇣1 +q 1

⌘2
2 log 1

n

+ log( n+1
 )

.

6Riesz representation theorem is behind this identiﬁcation.
7H may be inﬁnite-dimensional (e.g. Gaussian kernel).
8See Section 5 about the interpretation of Gaussian randomization for a Hilbert space valued algorithm.

3

In closing this section we mention that our main theorem is general in that it may be specialized to
any Hilbert space valued algorithm. This covers any regularized ERM algorithm [Liu et al.  2017].
We applied it to SVM’s whose hypothesis sensitivity coefﬁcients (as in our Deﬁnition 1) are known.
It can be argued that neural networks (NN’s) fall under this framework as well. Then an appealing
future research direction  with deep learning in view  is to ﬁgure out the sensitivity coefﬁcients of
NN’s trained by Stochastic Gradient Descent. Then our main theorem could be applied to provide
non-vacuous bounds for the performance of NN’s  which we believe is very much needed.

3 Comparison to other bounds

For reference we list several risk bounds (including ours). They are in the context of binary clas-
siﬁcation (Y = {1  +1}). For clarity  risks under the 0-1 loss are denoted by R01 and risks with
respect to the (clipped) hinge loss are denoted by Rhi. Bounds requiring a Lipschitz loss function
do not apply to the 0-1 loss. However  the 0-1 loss is upper bounded by the hinge loss  allowing us
to upper bound the risk with respect to the former in therms of the risk with respect to the latter. On
the other hand  results requiring a bounded loss function do not apply to the regular hinge loss. In
those cases the clipped hinge loss is used  which enjoys boundedness and Lipschitz continuity.

3.1 P@EW: Our new instance-dependent PAC-Bayes bound
Our Corollary 3  with Q = N (Wn  2I)  a Gaussian centered at Wn = SVM(Sn) with random-
ization variance 2  gives the following risk bound which holds with probability  1  2:
 .
log n + 1

22n2✓1 +r 1

kl+(R01(Q  Pn)kR01(Q  P )) 

◆2
log 1

As will be clear from the proof (see Section 4 below)  this bound is obtained from the PAC-Bayes
bound (Theorem 4) using a prior centered at the expected weight.

1
n

+

2

2



3.2 P@O: Prior at the origin PAC-Bayes bound
The PAC-Bayes bound Theorem 4 again with Q = N (Wn  2I)  gives the following risk bound
which holds with probability  1  :

82  kl+(R01(Q  Pn)kR01(Q  P )) 

1
22nkWnk2 +

1
n

log n + 1



 .

3.3 Bound of Liu et al. [2017]
From Corollary 1 of Liu et al. [2017] (but with  as in formulation (svm)) we get the following risk
bound which holds with probability  1  2:

R01(Wn  P )  Rhi(Wn  P )  Rhi(Wn  Pn) +

8

nr2 log 2

 +r 1

2n

log 1
 .

We use Corollary 1 of Liu et al. [2017] with B = 1  L = 1 and M = 1 (clipped hinge loss).

3.4 Bound of Bousquet and Elisseeff [2002]
From Example 2 of Bousquet and Elisseeff [2002] (but with  as in formulation (svm)) we get the
following risk bound which holds with probability  1  :
2
R01(Wn  P )  Rhi(Wn  P )  Rhi(Wn  Pn) +
n

2n

+⇣1 +

4

⌘r 1

We use Example 2 and Theorem 17 (based on Theorem 12) of Bousquet and Elisseeff [2002] with
 = 1 (normalized kernel) and M = 1 (clipped hinge loss).
In Appendix C below there is a list of different SVM formulations  and how to convert between
them. We found it useful when implementing code for experiments.

log 1
 .

4

There are obvious differences in the nature of these bounds: the last two (Liu et al. [2017] and
Bousquet and Elisseeff [2002]) are risk bounds for the (un-randomized) classiﬁers  while the ﬁrst
two (P@EW  P@O) give an upper bound on the binary KL-divergence between the average risks
(empirical to theoretical) of the randomized classiﬁers. Of course inverting the KL-divergence we
get a bound for the average theoretical risk. Also  the ﬁrst two bounds have an extra parameter  the
randomization variance (2)  that can be optimized. Note that P@O bound is not based on stability 
while the other three bounds are based on stability notions. Next let us comment on how these
bounds compare quantitatively.
Our P@EW bound and the P@O bound are similar except for the ﬁrst term on the right hand side.
This term comes from the KL-divergence between the Gaussian distributions. Our P@EW bound’s
ﬁrst term improves with larger values of   which in turn penalize the norm of the weight vector
of the corresponding SVM  resulting in a small ﬁrst term in P@O bound. Note that P@O bound
is equivalent to the setting of Q = N (µWn/kWnk  2I)  a Gaussian with center in the direction
of Wn  at distance µ > 0 from the origin (as discussed by Langford [2005] and implemented by
Parrado-Hern´andez et al. [2012]).
The ﬁrst term on the right hand side of our P@EW bound comes from the concentration of the weight
(see our Corollary 9). Lemma 1 of Liu et al. [2017] implies a similar concentration inequality for
the weight vector  but it is not hard to see that our concentration bound is slightly better.
Finally  in the experiments we compare our P@EW bound with Bousquet and Elisseeff [2002].

4 Proofs

As we said before  the proof of our Theorem 2 combines stability of the learned hypothesis (in the
sense of our Deﬁnition 1) and a well-known PAC-Bayes bound  quoted next for reference:
Theorem 4. (PAC-Bayes bound) Consider a learning algorithm A : [n(X⇥Y )n !C . For any
Q0 2 M1(C)  and for any  2 (0  1)  with probability  1   we have

KL(QkQ0) + log( n+1
 )

8Q 2 M1(C)  kl+(R(Q  Pn)kR(Q  P )) 

n
The probability is over the generation of the training sample Sn ⇠ P n.
This is borrowed from Langford [2005]  originally Theorem 1 of Seeger [2002]. See also Theorem
2.1 of Germain et al. [2009]. To use the PAC-Bayes bound  we will use Q0 = N (E[A(Sn)]  2I)
and Q = N (A(Sn)  2I)  a Gaussian distribution centered at the expected output and a Gaussian
(posterior) distribution centered at the random output A(Sn)  both with covariance operator 2I.
The KL-divergence between those Gaussians scales with kA(Sn)  E[A(Sn)]k2. More precisely:

.

KL(QkQ0) =

1
22kA(Sn)  E[A(Sn)]k2 .

Therefore  bounding kA(Sn)E[A(Sn)]k will give (via the PAC-Bayes bound of Theorem 4 above)
a corresponding bound on the binary divergence between the average empirical risk R(Q  Pn) and
the average theoretical risk R(Q  P ) of the randomized classiﬁer Q. Hypothesis stability (in the form
of our Deﬁnition 1) implies a concentration inequality for kA(Sn) E[A(Sn)]k. This is done in our
Corollary 8 (see Section 4.3 below) and completes the circle of ideas to prove our main theorem. The
proof of our concentration inequality is based on an extension of the bounded differences theorem
of McDiarmid to vector-valued functions discussed next.

4.1 McDiarmid’s inequality for real-valued functions of the sample
To shorten the notation let’s present the training sample as Sn = (Z1  . . .   Zn) where each example
Zi is a random variable taking values in the (measurable) space Z. We quote a well-known theorem:
Theorem 5. (McDiarmid inequality) Let Z1  . . .   Zn be independent Z-valued random variables 
and f : Z n ! R a real-valued function such that for each i and for each list of ‘complementary’
arguments z1  . . .   zi1  zi+1  . . .   zn we have

sup
zi z0i

Then for every ✏> 0  Pr{f (Z1:n)  E[f (Z1:n)] >✏ } exp⇣ 2✏2
Pn

|f (z1:i1  zi  zi+1:n)  f (z1:i1  z0i  zi+1:n)| ci .
i⌘.

i=1 c2

5

McDiarmid’s inequality applies to a real-valued function of independent random variables. Next we
present an extension to vector-valued functions of independent random variables. The proof follows
the steps of the proof of the classic result above  but we have not found this result in the literature 
hence we include the details.

4.2 McDiarmid’s inequality for vector-valued functions of the sample
Let Z1  . . .   Zn be independent Z-valued random variables and f : Z n !H a function into a
separable Hilbert space. We will prove that bounded differences in norm9 implies concentration of
f (Z1:n) around its mean in norm  i.e.  that kf (Z1:n)  Ef (Z1:n)k is small with high probability.
Notice that McDiarmid’s theorem can’t be applied directly to f (Z1:n)Ef (Z1:n) when f is vector-
valued. We will apply McDiarmid to the real-valued kf (Z1:n)  Ef (Z1:n)k  which will give an
upper bound for kf  Efk in terms of Ekf  Efk. The next lemma upper bounds Ekf  Efk for
a function f with bounded differences in norm. Its proof is in Appendix A.
Lemma 6. Let Z1  . . .   Zn be independent Z-valued random variables  and f : Z n !H a function
into a Hilbert space H satisfying the bounded differences property: for each i and for each list of
‘complementary’ arguments z1  . . .   zi1  zi+1  . . .   zn we have

sup
zi z0i

kf (z1:i1  zi  zi+1:n)  f (z1:i1  z0i  zi+1:n)k  ci .

Then Ekf (Z1:n)  E[f (Z1:n)]k pPn

i .
i=1 c2

If the vector-valued function f (z1:n) has bounded differences in norm (as in Lemma 6) and C 2 R is
any constant  then the real-valued function kf (z1:n)  Ck has the bounded differences property (as
in McDiarmid’s theorem). In particular this is true for kf (z1:n)  Ef (Z1:n)k (notice that Ef (Z1:n)
is constant over replacing Zi by an independent copy Z0i) so applying McDiarmid’s inequality to it 
combining with Lemma 6  we get the following theorem:
Theorem 7. Under the assumptions of Lemma 6  for any  2 (0  1)  with probability  1   we
have

kf (Z1:n)  E[f (Z1:n)]k vuut
through its Euclidean norm kc1:nk2 =pPn

i .
i=1 c2

i +rPn

c2

i=1 c2
i
2

nXi=1

log 1
 .

Notice that the vector c1:n = (c1  . . .   cn) of difference bounds appears in the above inequality only

4.3 Stability implies concentration
The hypothesis sensitivity coefﬁcients give concentration of the learned hypothesis:
Corollary 8. Let A be a Hilbert space valued algorithm. Suppose A has hypothesis sensitivity
coefﬁcient n at sample size n. Then for any  2 (0  1)  with probability  1   we have

kA(Sn)  E[A(Sn)]k  pn n 1 +r 1

2

! .
log 1

This is a consequence of Theorem 7 since ci  n for i = 1  . . .   n  hence kc1:nk  pn n.
Last (not least) we deduce concentration of the weight vector Wn = SVM(Sn).
Corollary 9. Let Wn = SVM(Sn). Suppose that the kernel used by SVM is bounded by B. For
any > 0  for any  2 (0  1)  with probability  1   we have
! .
log 1

pn 1 +r 1

kWn  E[Wn]k 

2B

2

Under these conditions we have hypothesis sensitivity coefﬁcients n  2B
and Elisseeff [2002]  Example 2 and Lemma 16  adapted to our setting). Then apply Corollary 8.

n (we follow Bousquet

9The Hilbert space norm  induced by the inner product of H.

6

5 Gaussian distributions over the Hilbert space of classiﬁers?

This section aims to provide a rigorous explanation for Gaussian randomization in Hilbert spaces 
which has been used here and in several previous machine learning works. For instance in the
setting of SVM classiﬁers with feature map  : X!H   the output is a weight vector that lives
in the Hilbert space H. With the Gaussian kernel in mind  we are facing an inﬁnite-dimensional
separable H  which upon the choice of an orthonormal basis {e1  e2  . . .} can be identiﬁed with the
space10 `2 ⇢ RN of square summable sequences of real numbers  via the isometric isomorphism
H! `2 that maps the vector w = P1i=1 wiei 2H to the sequence (w1  w2  . . .) 2 `2. Thus
without loss of generality we may regard the feature map as  : X! `2 ⇢ RN.
The PAC-Bayes approach applied to SVMs says that instead of committing to the weight vector
Wn = SVM(Sn) we will randomize by choosing a fresh W 2H according to some probability
distribution on H for each prediction. Suppose the randomized classiﬁer is to be chosen according
to a Gaussian distribution. Although it commonly appears in the literature  it is worth wondering
just what is a Gaussian distribution over the space H = `2.
Two possibilities come to mind for the Gaussian random classiﬁer W : (1) according to a Gaussian
measure on `2  say W ⇠N (µ  ⌃) with mean µ 2 `2 and covariance operator ⌃ meeting the
requirements (positive  trace-class) for this to be a Gaussian measure on `2; or (2) according to a
Gaussian measure on the bigger RN  e.g. W ⇠N (µ  I) by which we mean the measure constructed
as the product of a sequence N (µi  1) of independent real-valued Gaussians with unit variance.
These two possibilities are mutually exclusive since the ﬁrst choice gives a measure on RN whose
mass is supported on `2  while the second choice leads to a measure on RN supported outside of `2.
A good reference for this topic is Bogachev [1998].

Let us go with the second choice: N (0  I) =N1i= N (0  1)  a ‘standard Gaussian’ on RN. This is
a legitimate probability measure on RN (by Kolmogorov’s Extension theorem). But it is supported
outside of `2  so when sampling a W 2 RN according to this measure  with probability one such W
will be outside of our feature space `2. Then we have to wonder about the meaning of hW ·i when
W is not in the Hilbert space carrying this inner product.
Let us write W = (⇠1 ⇠ 2  . . .) a sequence of i.i.d. standard (real-valued) Gaussian random variables.
Let x = (x1  x2  . . .) 2 `2  and consider the formal expression hx  Wi =P1i=1 xi⇠i. Notice that
Then (see e.g. Bogachev [1998]  Theorem 1.1.4) our formal object hx  Wi =P1i=1 xi⇠i is actually

well-deﬁned in the sense that the series is convergent almost surely (i.e. with probability one) 
although as we pointed out such W is outside `2.

E[|xi⇠i|2] =

|xi|2 < 1 .

1Xi=1

1Xi=1

5.1 Predicting with the Gaussian random classiﬁer
Let Wn = SVM(Sn) be the weight vector found by running SVM on the sample Sn. We write it as

i=1 ↵iYi(Xi). Let (· ·) be the kernel doing the “kernel trick.”

Wn =Pn
Also as above let W be a Gaussian random vector in RN  and write it as W = P1j=1 ⇠jej with
⇠1 ⇠ 2  . . . i.i.d. standard Gaussians. As usual ej stands for the canonical unit vectors having a 1 in
the jth coordinate and zeros elsewhere.
For an input x 2X with corresponding feature vector (x) 2H   we predict with

↵iYi(Xi  x) +

⇠jhej  (x)i .

1Xj=1

(hej  (x)i)2 = k(x)k2  

1Xi=1

7

This is well-deﬁned since

hWn + W  (x)i =

nXi=1
E[(⇠jhej  (x)i)2] =

1Xi=1

so the seriesP1j=1 ⇠jhej  (x)i converges almost surely (Bogachev [1998]  Theorem 1.1.4).

10Just to be sure: RN stands for the set of all inﬁnite sequences of real numbers.

Figure 1: Tightness of P@O bound on PIM (left) and RIN (right) shown as the difference between
the bound and the test error of the underlying randomized classiﬁer. Smaller values are preferred.

Figure 2: Tightness of P@EW bound (the bound derived here) on PIM (left) and RIN (right) shown as
the difference between the bound and the test error of the underlying randomized classiﬁer. Smaller
values are preferred.

6 Experiments
The purpose of the experiments was to explore the strengths and potential weaknesses of our new
bound in relation to the previous alternatives  as well as to explore the bound’s ability to help model
selection. For this  to facilitate comparisons  taking the setup of Parrado-Hern´andez et al. [2012]  we
experimented with the ﬁve UCI datasets described there. However  we present results for PIM and
RIN only  as the results on the other datasets mostly followed the results on these and in a way these
two datasets are the most extreme. In particular  they are the smallest and largest with dimensions
768 ⇥ 8 (768 examples  and 8 dimensional feature space)  and 7200 ⇥ 20  respectively.
Model and data preparation We used an offset-free SVM classiﬁer with a Gaussian RBF kernel
2/(22)) with RBF width parameter > 0. The SVM used the so-called
(x  y) = exp(kx  yk2
standard SVM-C formulation which multiplies the total (hinge) loss by C > 0; the conversion to
our formulation (svm) is given by C = 1
n where n is the number of training examples and > 0 is
the penalty factor. The datasets were split into a training and a test set using the train test split
method of scikit  keeping 80% of the data for training and 20% for testing.
Model parameters Following the procedure suggested in Section 2.3.1 of Chapelle and Zien [2005] 
we set up a geometric 7 ⇥ 7 grid over the (C  )-parameter space where C ranges between 28C0
and 22C0 and  ranges between 230 and 230  where 0 is the median of the Euclidean distance
between pairs of data points of the training set  and given 0  C0 is obtained as the reciprocal value
of the empirical variance of data in feature space underlying the RBF kernel with width 0. The
grid size was selected for economy of computation. The grid lower and upper bounds for  were
ad-hoc  though they were inspired by the literature  while for the same for C  we enlarged the lower
range to focus on the region of the parameter space where the stability-based bounds have a better
chance to be effective: In particular  the stability-based bounds grow with C in a linear fashion  with
a coefﬁcient that was empirically observed to be close to one.

8

Computations For each of the (C  ) pairs on the said grid  we trained an SVM-model using a
Python implementation of the SMO algorithm of Platt [1999]  adjusted to SVMs with no offset
(Steinwart and Christmann [2008] argue that “the offset term has neither a known theoretical nor an
empirical advantage” for the Gaussian RBF kernel). We then calculated various bounds using the
obtained model  as well as the corresponding test error rates (recall that the randomized classiﬁers’
test error is different than the test error of the SVM model that uses no randomization). The bounds
compared were the two mentioned hinge-loss based bounds: The bound by Liu et al. [2017] and
that of Bousquet and Elisseeff [2002]. In addition we calculated the P@O and (our) P@EW bound.
When these latter were calculated we optimized the randomization variance parameter 2
noise by
minimizing error estimate obtained from the respective bound (the KL divergence was inverted
numerically). Further details of this can be found in Appendix D.
Results As explained earlier our primary interest is to explore the various bounds strengths and
weaknesses. In particular  we are interested in their tightness  as well as their ability to support
model selection. As the qualitative results were insensitive to the split  results for a single “random”
(arbitrary) split are shown only.
Tightness The hinge loss based bounds gave trivial bounds over almost all pairs of (C  ). Upon
investigating this we found that this is because the hinge loss takes much larger values than the
training error rate unless C takes large values (cf. Fig. 3 in Appendix D). However  for large values of
C  both of the bounds are vacuous. In general  the stability based bounds (Liu et al. [2017]  Bousquet
and Elisseeff [2002] and our bound) are sensitive to large values of C. Fig. 1 show the difference
between the P@O bound and the test error of the underlying respective randomized classiﬁers as a
function of (C  ) while Fig. 2 shows the difference between the P@EW bound and the test error
of the underlying randomized classiﬁer. (Figs. 7 and 9 in the appendix show the test errors for these
classiﬁers  while Figs. 6 and 8 shows the bound.) The meticulous reader may worry about that it
appears that on the smaller dataset  PIM  the difference shown for P@O is sometimes negative. As
it turns out this is due to the randomness of the test error: Once we add a conﬁdence correction that
accounts for the randomness of the small test set (ntest = 154) this difference disappears once we
correct the test error for this. From the ﬁgures the most obvious difference between the bounds is
that the P@EW bound is sensitive to the value of C and it becomes loose for larger values of C. This
is expected: As noted earlier  stability based bounds  which P@EW is an instance of  are sensitive
to C. The P@O bound shows a weaker dependence on C if any. In the appendix we show the
advantage (or disadvantage) of the P@EW bound over the P@O bound on Fig. 10. From this ﬁgure
we can see that on PIM  P@EW is to be preferred almost uniformly for small values of C (C  0.5) 
while on RIN  the advantage of P@EW is limited both for smaller values of C and a certain range
of the RBF width. Two comments are in order in connection to this: (i) We ﬁnd it remarkable that
a stability-based bound can be competitive with the P@O bound  which is known as one of the best
bounds available. (ii) While comparing bounds is interesting for learning about their qualities  the
bounds can be used together (e.g.  at the price of an extra union bound).
Model selection To evaluate a bound’s capability in helping model selection it is worth comparing
the correlation between the bound and test error of the underlying classiﬁers. By comparing Figs. 6
and 7 with Figs. 8 and 9 it appears that perhaps the behavior of the P@EW bound (at least for
small values of C) follows more closely the behavior of the corresponding test error surface. This is
particularly visible on RIN  where the P@EW bound seems to be able to pick better values both for
C and   which lead to a much smaller test error (around 0.12) than what one can obtain by using
the P@O bound.

7 Discussion

We have developed a PAC-Bayes bound for randomized classiﬁers. We proceeded by investigating
the stability of the hypothesis learned by a Hilbert space valued algorithm. A special case being
SVMs. We applied our main theorem to SVMs  leading to our P@EW bound  and we compared it
to other stability-based bounds and to a previously known PAC-Bayes bound. The main ﬁnding is
that perhaps P@EW is the ﬁrst nontrivial bound that uses (uniform) hypothesis stability. Our work
can be viewed as contributing to a line of research that aims to develop ‘self-bounding algorithms’
(Freund [1998]  Langford and Blum [2003]) in the sense that besides producing a predictor the
algorithm also creates a performance certiﬁcate based on the available data.

9

Acknowledgements

Omar Rivasplata is sponsored by DeepMind via an Overseas Impact Studentship to undertake grad
studies at UCL Department of Computer Science. Csaba Szepesv´ari gratefully acknowledges the
Alberta machine intelligence institute (Amii)  with funding from Alberta Innovates – Technology
Futures and from the Natural Sciences and Engineering Research Council of Canada. Shiliang Sun
is supported by the NSFC Project 61673179  and Shanghai Knowledge Service Platform Project
ZF1213. John Shawe-Taylor acknowledges support of the UK Defence Science and Technology
Laboratory (Dstl) and Engineering and Physical Sciences Research Council (EPSRC) under grant
EP/R018693/1. This is part of the collaboration between US DOD  UK MOD and UK EPSRC under
the Multidisciplinary University Research Initiative (MURI). This work was done in part while John
Shawe-Taylor was visiting the Simons Institute for the Theory of Computing at UC Berkeley.

References
Karim Abou-Moustafa and Csaba Szepesv´ari. An a priori exponential tail bound for k-folds cross-

validation. ArXiv e-prints  2017.

Vladimir I. Bogachev. Gaussian Measures. American Mathematical Society  1998.
Olivier Bousquet and Andr´e Elisseeff. Stability and generalisation. Journal of Machine Learning

Research  2:499–526  2002.

Olivier Catoni. PAC-Bayesian supervised classiﬁcation: the thermodynamics of statistical learning.

Technical report  Institute of Mathematical Statistics  Beachwood  Ohio  USA  2007.

Alain Celisse and Benjamin Guedj. Stability revisited: new generalisation bounds for the leave-one-

out. arXiv preprint arXiv:1608.06412  2016.

Olivier Chapelle and Alexander Zien. Semi-supervised classiﬁcation by low density separation. In

AISTATS  2005.

Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In UAI  2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential

privacy. arXiv preprint arXiv:1802.09583  2018.

Yoav Freund. Self bounding learning algorithms. In Proc. of the 11th annual conference on Com-

putational Learning Theory  pages 247–258. ACM  1998.

Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  and Mario Marchand. PAC-Bayesian
learning of linear classiﬁers. In Proc. of the 26th International Conference on Machine Learning 
pages 353–360. ACM  2009.

Pascal Germain  Alexandre Lacasse  Francois Laviolette  Mario Marchand  and Jean-Francis Roy.
Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning algorithm. Journal
of Machine Learning Research  16:787–860  2015.

Aryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Proc.

of the 31st International Conference on Machine Learning  pages 28–36  2014.

John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learn-

ing Research  6(Mar):273–306  2005.

John Langford and Avrim Blum. Microchoice bounds and self bounding learning algorithms. Ma-

chine Learning  51(2):165–179  2003.

Guy Lever  Franc¸ois Laviolette  and John Shawe-Taylor. Distribution-dependent PAC-Bayes priors.

In International Conference on Algorithmic Learning Theory  pages 119–133. Springer  2010.

Tongliang Liu  G´abor Lugosi  Gergely Neu  and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In Proc. of the 34th International Conference on Machine Learning  pages 2159–
2167  2017.

10

Ben London. A PAC-Bayesian analysis of randomized learning with application to stochastic gra-
dient descent. In Advances in Neural Information Processing Systems  pages 2931–2940  2017.
Ben London  Bert Huang  Ben Taskar  and Lise Getoor. Collective stability in structured prediction:
In Proc. of the 30th International Conference on Machine

Generalization from one example.
Learning  pages 828–836  2013.

G´abor Lugosi and Miroslaw Pawlak. On the posterior-probability estimate of the error rate of non-
parametric classiﬁcation rules. IEEE Transactions on Information Theory  40(2):475–481  1994.

David A McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999a.
David A McAllester. PAC-Bayesian model averaging. In Proc. of the 12th annual conference on

Computational Learning Theory  pages 164–170. ACM  1999b.

Emilio Parrado-Hern´andez  Amiran Ambroladze  John Shawe-Taylor  and Shiliang Sun. PAC-Bayes
bounds with data dependent priors. Journal of Machine Learning Research  13:3507–3531  2012.
John C. Platt. Fast training of support vector machines using sequential minimal optimization. In
B. Sch¨olkopf  C. J. C. Burges  and A. J. Smola  editors  Advances in Kernel Methods – Support
Vector Learning  pages 185–208. MIT Press  Cambridge MA  1999.

Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation.

Journal of Machine Learning Research  3:233–269  2002.

John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press  Cambridge  UK  2004.

John Shawe-Taylor and Robert C Williamson. A PAC analysis of a Bayesian estimator. In Proc. of

the 10th annual conference on Computational Learning Theory  pages 2–9. ACM  1997.

Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Science & Business

Media  2008.

11

,Omar Rivasplata
Emilio Parrado-Hernandez
John Shawe-Taylor
Shiliang Sun
Csaba Szepesvari