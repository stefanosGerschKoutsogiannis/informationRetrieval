2019,Correlation clustering with local objectives,Correlation Clustering is a powerful graph partitioning model that aims to cluster items based on the notion of similarity between items. An instance of the Correlation Clustering problem consists of a graph G (not necessarily complete) whose edges are labeled by a binary classifier as
similar and dissimilar. Classically  we are tasked with producing a clustering that minimizes the number of disagreements: an edge is in disagreement if it is a similar edge and is present across clusters or if it is a dissimilar edge and is present within a cluster. Define the disagreements vector to be an n dimensional vector indexed by the vertices  where the v-th index is the number of disagreements at vertex v.

Recently  Puleo and Milenkovic (ICML '16) initiated the study of the Correlation Clustering framework in which the objectives were more general functions of the disagreements vector. In this paper  we study algorithms for minimizing \ell_q norms (q >= 1) of the disagreements vector for both arbitrary and complete graphs. We present the first known algorithm for minimizing the \ell_q norm of the disagreements vector on arbitrary graphs and also provide an improved algorithm for minimizing the \ell_q norm (q >= 1) of the disagreements vector on complete graphs. We also study an alternate cluster-wise local objective introduced by Ahmadi  Khuller and Saha (IPCO '19)  which aims to minimize the maximum number of disagreements associated with a cluster. We present an improved (2 + \eps) approximation algorithm for this objective.,Correlation Clustering with Local Objectives

Sanchit Kalhan

Konstantin Makarychev

Timothy Zhou

Abstract

Correlation Clustering is a powerful graph partitioning model that aims to clus-
ter items based on the notion of similarity between items. An instance of the
Correlation Clustering problem consists of a graph G (not necessarily complete)
whose edges are labeled by a binary classiﬁer as “similar” and “dissimilar”. An
objective which has received a lot of attention in literature is that of minimizing
the number of disagreements: an edge is in disagreement if it is a “similar” edge
and is present across clusters or if it is a “dissimilar” edge and is present within
a cluster. Deﬁne the disagreements vector to be an n dimensional vector indexed
by the vertices  where the v-th index is the number of disagreements at vertex v.
Recently  Puleo and Milenkovic (ICML ’16) initiated the study of the Correlation
Clustering framework in which the objectives were more general functions of the
disagreements vector. In this paper  we study algorithms for minimizing `q norms
(q  1) of the disagreements vector for both arbitrary and complete graphs. We
present the ﬁrst known algorithm for minimizing the `q norm of the disagreements
vector on arbitrary graphs and also provide an improved algorithm for minimizing
the `q norm (q  1) of the disagreements vector on complete graphs. We also
study an alternate cluster-wise local objective introduced by Ahmadi  Khuller and
Saha (IPCO ’19)  which aims to minimize the maximum number of disagreements
associated with a cluster. We also present an improved (2 + ")-approximation
algorithm for this objective. Finally  we compliment our algorithmic results for
minimizing the `q norm of the disagreements vector with some hardness results.

1

Introduction

A basic task in machine learning is that of clustering items based on the similarity between them.
This task can be elegantly captured by Correlation Clustering  a clustering framework ﬁrst introduced
by Bansal et al. [2004]. In this model  we are given access to items and the similarity/dissimilarity
between them in the form of a graph G on n vertices. The edges of G represent whether the items
are similar or dissimilar and are labelled as (“+”) and (“”) respectively. The goal is to produce
a clustering that agrees with the labeling of the edges as much as possible  i.e.  to group positive
edges in the same cluster and place negative edges across different clusters (a positive edge that
is present across clusters or a negative edge that is present within the same cluster is said to be in
disagreement). The Correlation Clustering problem can be viewed as an agnostic learning problem 
where we are given noisy examples and the task is to ﬁt a hypothesis as best as possible to these
examples. Co-reference resolution (see e.g.  Cohen and Richman [2001  2002])  spam detection
(see e.g.  Ramachandran et al. [2007]  Bonchi et al. [2014]) and image segmentation (see e.g.  Wirth
[2017]) are some of the applications to which Correlation Clustering has been applied to in practice.
This task is made trivial if the labeling given is consistent (transitive): if (u  v) and (v  w) are similar 
then (u  w) is similar for all vertices u  v  w in G (the connected components on similar edges would
give an optimal clustering). Instead  it is assumed that the given labeling is inconsistent  i.e.  it is
possible that (u  w) are dissimilar even though (u  v) and (v  w) are similar. For such a triplet u  v  w 
every possible clustering incurs a disagreement on at least one edge and thus  no perfect clustering

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

exists. The optimal clustering is the one which minimizes the disagreements. Moreover  as the
number of clusters is not predeﬁned  the optimal clustering can use anywhere from 1 to n clusters.
Minimizing the total weight of edges in disagreement is the objective that has received the most
consideration in literature. Deﬁne the disagreements vector be an n dimensional vector indexed by
the vertices where the v-th coordinate equals the number of disagreements at v. Thus  minimizing the
total number of disagreements is equivalent to minimizing the `1 norm of the disagreements vector.
Puleo and Milenkovic [2016] initiated the study of local objectives in the Correlation Clustering
framework. They focus on complete graphs and study the minimization of `q norms (q  1) of the
disagreements vector – for which they provided a 48approximation algorithm. Charikar  Gupta 
and Schwartz [2017] gave an improved 7approximation algorithm for minimizing `q disagreements
on complete graphs. They also studied the problem of minimizing the `1 norm of the disagreements
vector (also known as Min Max Correlation Clustering) for arbitrary graphs  for which they provided
a O(pn)approximation.
For higher values of q (particularly q = 1)  a clustering optimized for minimizing the `q norm
prioritizes reducing the disagreements at vertices that are worst off. Thus  such metrics are very
unforgiving in most cases as it is possible that in the optimal clustering there is only one vertex with
high disagreements while every other vertex has low disagreements. Hence  one is forced to infer
the most pessimistic picture about the overall clustering. The `2 norm is a solution to this tension
between the `1 and `1 objectives. The `2 norm of the disagreements vector takes into account the
disagreements at each vertex while also penalizing the vertices with high disagreements more heavily.
Thus  a clustering optimized for the minimum `2 norm gives a more balanced clustering as it takes
into consideration both the global and local picture.
Recently  Ahmadi  Khuller  and Saha [2019b] introduced an alternative min max objective for
correlation clustering (which we call AKS min max objective). For a cluster C ✓ V   let us refer
to similar edges with exactly one endpoint in C and dissimilar edges with both endpoints in C as
edges in disagreements with respect to C. We call the weight of all edges in disagreement with C the
cost of C. Then  the AKS min max objective asks to ﬁnd a clustering C1  . . .   CT that minimizes
the maximum cost Ci. Ahmadi et al. [2019b] gave an O(log n)approximation algorithm for this
objective. Ahmadi  Galhotra  Khuller  Saha  and Schwartz [2019a] improved the approximation
factor to O(plog n · max{log |E|  log(k)}).
Our contributions. In this paper  we provide positive and negative results for Correlation Clustering
with the `q objective. We ﬁrst study the problem of minimizing disagreements on arbitrary graphs.
We present the ﬁrst approximation algorithm minimizing any `q norm (q  1) of the disagreements
vector.
Theorem 1.1. There exists a polynomial time O(n
2q n)approximation algorithm for
the minimum `q disagreements problem on general weighted graphs.

2q · log

2 1

1
2 + 1

1

For the `2 objective  the above algorithm leads to an approximation ratio of ˜O(n1/4)  thus providing the
ﬁrst known approximation ratio for optimizing the clustering for this version of the objective. Note that
the above algorithm matches the best approximation guarantee of O(log n) for the classical objective
of minimizing the `1 norm of the disagreements vector. For the `1 norm  our algorithm matches the
guarantee of the algorithm by Charikar  Gupta  and Schwartz [2017] up to log factors. Fundamental
combinatorial optimization problems like Multicut  Multiway Cut and s-t Cut can be framed as special
cases of Correlation Clustering. Thus  Theorem 1.1 leads to the ﬁrst known algorithms for Multicut 
Multiway Cut and s-t Cut with the `q objective when q 6= 1 and q 6= 1. We can also use the algorithm
2q n) bi-criteria approximation for Min k-Balanced
from Theorem 1.1 to obtain O(n
Partitioning with the `q objective (we omit details here).
Next  we study the case of complete graphs. For this case  we present an improved 5approximation
algorithm for minimizing any `q norm (q  1) of the disagreements vector.
Theorem 1.2. There exists a polynomial time 5approximation algorithm for the minimum `q
disagreements problem on complete graphs.

2q · log

2 1

1
2 + 1

1

We also study the case of complete bipartite graphs where disagreements need to be bounded for
only one side of the bipartition  and not the whole vertex set. We give an improved 5approximation
algorithm for minimizing any `q norm (q  1) of the disagreements vector.

2

Theorem 1.3. There exists a polynomial time 5approximation algorithm for the minimum `q
disagreements problem on complete bipartite graphs where disagreements are measured for only one
side of the bipartition.

In this paper  we also consider the AKS min max objective.
For this objective  we
give a (2 + ")approximation algorithm  which improves the approximation ratio of
O(plog n · max{log |E|  log(k)}) given by Ahmadi  Galhotra  Khuller  Saha  and Schwartz
[2019a].
Theorem 1.4. There exists a polynomial time (2 + ")approximation algorithm for the AKS min
max problem on arbitrary graphs.

1

Finally  in the full version of this paper (see supplemental materials)  we present an integrality gap of
2 1
2q ) for minimum `q s  t cut and prove a hardness of approximation of 2 for minimum `1
⌦(n
s  t cut.
Previous work. Bansal  Blum  and Chawla [2004] showed that it is NP-hard to ﬁnd a clustering that
minimizes the total disagreements  even on complete graphs. They give a constant-factor approxima-
tion algorithm to minimize disagreements and a PTAS to maximize agreements on complete graphs.
For complete graphs  Ailon  Charikar  and Newman [2008] presented a randomized algorithm with an
approximation guarantee of 3 to minimize total disagreements. They also gave a 2.5 approximation
algorithm based on LP rounding. This factor was improved to slightly less than 2.06 by Chawla 
Makarychev  Schramm  and Yaroslavtsev [2015]. Since  the natural LP is known to have an integrality
gap of 2  the problem of optimizing the classical objective is almost settled with respect to the natural
LP. For arbitrary graphs  the best known approximation ratio is O(log n) (see Charikar  Guruswami 
and Wirth [2003]  Demaine  Emanuel  Fiat  and Immorlica [2006]). Assuming the Unique Games
Conjecture  there is no constant-factor approximation algorithm for minimizing `1 disagreements on
arbitrary graphs (see Chawla et al. [2006]). Puleo and Milenkovic [2016] ﬁrst studied Correlation
Clustering with more local objectives. For minimizing `q (q  1) norms of the disagreements vector
on complete graphs  their algorithm achieves an approximation guarantee of 48. This was improved
to 7 by Charikar  Gupta  and Schwartz [2017]. Charikar et al. [2017] also studied the problem of
minimizing the `1 norm of the disagreements vector on general graphs. They showed that the
natural LP/SDP has an integrality gap of n/2 for this problem and provided a O(pn)approximation
algorithm for minimum `1 disagreements. Puleo and Milenkovic [2016] also initiated the study of
minimizing the `q norm of the disagreements vector (for one side of the bipartition) on complete
bipartite graphs. The presented a 10approximation algorithm for this problem  which was improved
to 7 by Charikar  Gupta  and Schwartz [2017]. Recently  Ahmadi et al. [2019b] studied an alternative
objective for the correlation clustering problem. Motivated by creating balanced communities for
problems such as image segmentation and community detection in social networks  they propose
a new cluster-wise min-max objective. This objective minimizes the maximum weight of edges in
disagreement associated with a cluster  where an edge is in disagreement with respect to a cluster if
it is a similar edge and has exactly one end point in the cluster or if it is a dissimilar edge and has

both its endpoints in the cluster. They gave an O(plog n · max{log |E|  log(k)})approximation
algorithm for this objective. Moreover  they give a O(r2)approximation algorithm for graphs that
exclude a Kr r minor  and a 14approximation algorithm for complete graphs.

2 Preliminaries

We now formally deﬁne the Correlation Clustering with `q objective problem. We will need the
following deﬁnition. Consider a set of points V and two disjoint sets of edges on V : positive edges
E+ and negative edges E. We assume that every edge has a weight wuv. For every partition P of
V   we say that a positive edge is in disagreement with P if the endpoints u and v belongs to different
parts of P; and a negative edge is in disagreement with P if the endpoints u and v belongs to the
same part of P. The vector of disagreements  denoted by disagree(P  E+  E)  is a |V | dimensional
vector indexed by elements of V . Its coordinate u equals

disagreeu(P  E+  E) =Xv:(u v)2E+[E

wuv1((u  v) is in disagreement with P).

3

minimize max⇣kykq Xu2V
subject to yu = Xv:(u v)2E+
zu = Xv:(u v)2E+

q⌘
zu 1
wuvxuv + Xv:(u v)2E
uvxuv + Xv:(u v)2E

wq

wuv(1  xuv)
wq
uv(1  xuv)

for all u 2 V
for all u 2 V

xv1v2 + xv2v3  xv1v3
xuv = xvu
xuv 2 [0  1]

for all v1  v2  v3 2 V
for all u  v 2 V
for all u  v 2 V

(P)

(P1)

(P2)

(P3)
(P4)
(P5)

Figure 3.1: Convex relaxation for Correlation Clustering with min `q objective for q < 1.

That is  disagreeu(P  E+  E) is the weight of disagreeing edges incident to u. We similarly deﬁne
a cut vector for a set of edges E:

cutu(P  E) =Xv:(u v)2E

wuv1(u and v are separated by P).

1

u)

We use the standard deﬁnition for the `q norm of a vector x: kxkq = (Pu xq
q and kxk1 =
maxu xu. For a partition P  we denote by P(u) the piece that contains vertex u.
Deﬁnition 1. In the Correlation Clustering problem with `q objective  we are given a graph G on
a set V with two disjoint sets of edges E+ and E and a set of weights wuv. The goal is ﬁnd a
partition P that minimizes the `q norm of the disagreements vector  k disagree(P  E+  E)kq.
In our algorithm for Correlation Clustering on arbitrary graphs  we will use a powerful technique
of padded metric space decompositions (see e.g.  Bartal [1996]  Rao [1999]  Fakcharoenphol and
Talwar [2003]  Gupta  Krauthgamer  and Lee [2003]).
Deﬁnition 2 (Padded Decomposition). Let (X  d) be a metric space on n points  and let  > 0. A
probabilistic distribution of partitions P of X is called a padded decomposition if it satisﬁes the
following properties:

• Each cluster C 2P has diameter at most .
• For every u 2 X and "> 0  Pr(Ball(u  ) 6⇢ P(u))  D · 

 where Ball(u  ) = {v 2
Theorem 2.1 (Fakcharoenphol  Rao  and Talwar [2004]). Every metric space (X  d) on n points
admits a D = O(log n) separating padded decomposition. Moreover  there is a polynomial-time
algorithm that samples a partition from this distribution.

X : d(u  v)  }

3 Convex Relaxation

In our algorithms for minimizing `q disagreements in arbitrary and complete graphs  we use a convex
relaxation given in Figure 3.1. Our convex relaxation for Correlation Clustering is fairly standard.
It is similar to relaxations used in the papers by Garg  Vazirani  and Yannakakis [1996]  Demaine 
Emanuel  Fiat  and Immorlica [2006]  Charikar  Guruswami  and Wirth [2003]. For every pair of
vertices u and v  we have a variable xuv that is equal to the distance between u and v in the “multicut
metric”. Variables xuv satisfy the triangle inequality constraints (P3). They are also symmetric (P4)
and xuv 2 [0  1] (P5). Thus  the set of vertices V equipped with the distance function d(u  v) = xuv
is a metric space.
Additionally  for every vertex u 2 V   we have variables yu and zu (see constraints (P1) and (P2))
that lower bound the number of disagreeing edges incident to u. The objective of our convex
program is to minimize max(kykq  (Pu zu)
q ). Note that all constraints in the program (P) are linear;
however  the objective function of (P) is not convex as is. So in order to ﬁnd the optimal solution  we

1

4

raise the objective function to the power of q and ﬁnd feasible x  y  z that minimizes the objective
q Pu zu).
max(kykq
This program has a polynomial number of linear constraints  and its objective function is convex:
q Pu zu)  is the maximum of two convex functions.
This is because the objective function  max(kykq
The ﬁrst function  kykq
q is the sum of q-th powers of the variables yu which are positive. Thus  kykq
is convex and differentiable. The second function Pu zu is a linear function. Therefore  we can use
off-the-shelf convex solvers (quadratic solvers for `2) to get an optimal solution to (P ).
Let us verify that program (P) is a relaxation for Correlation Clustering. Consider an arbitrary
partitioning P of V . In the integral solution corresponding to P  we set xuv = 0 if u and v are
in the same cluster in P; and xuv = 1 if u and v are in different clusters in P. In this solution 
distances xuv satisfy triangle inequality constraints (P3) and xuv = xvu (P4). Observe that a positive
edge (u  v) 2 E+ is in disagreement with P if xuv = 1; and a negative edge (u  v) 2 E is in
disagreement if xuv = 0. Thus  in this integral solution  yu = disagreeu(P  E+  E) and moreover 
u. Therefore  in the integral solution corresponding to P  the objective function of (P) equals
zu  yq
k disagreeu(P  E+  E)kq. Of course  the cost of the optimal fractional solution to the problem may
be less than the cost of the optimal integral solution. Thus  (P) is a relaxation for our problem. Below 
we denote the cost of the optimal fraction solution to (P) by LP .
We remark that we can get a simpler relaxation by removing variables z and changing the objective
function to kykq. This relaxation also works for `1 norm. We use it in our 5-approximation
algorithm.

q

4 Overview of Algorithms

We note that some proofs from Subsections 4.1  4.2 and 4.3 have been deferred to Sections A  B and
C respectively (in the supplementary material). These Lemmas and their proofs have been referrenced
appropriately.

4.1 Correlation Clustering on arbitrary graphs

q+1

q1
2q log

In this section  we describe our algorithm for minimizing `q disagreements on arbitrary graphs. We
will prove the following main theorem.
Theorem 4.1. There exists a randomized polynomial-time O(n
2q n)approximation algo-
rithm for Correlation Clustering with the `q objective (q  1).
We remark that the same algorithm gives O(pn log n)approximation for the `1 norm. We omit

the details in the conference version of the paper.
Our algorithm relies on a procedure for partitioning arbitrary metric spaces into pieces of small
diameter. In particular  we prove the following theorem 
Theorem 4.2. There exists a polynomial-time randomized algorithm that given a metric space (X  d)
on n points and parameter  returns a random partition P of X such that the diameter of every set
P in P is at most  and for every q  1 (q 6= 1) and every weighted graph G = (X  E  w)  we
have
Ehk cut(P  E)kqi  Cn

2q n ·h⇣Xu2X Xv:(u v)2E

q1
2q log

q+1

wq
uv

d(u  v)

 ⌘1/q
+⇣Xu2X⇣ Xv:(u v)2E

+

wuv

d(u  v)

 ⌘q⌘1/qi 

(1)

for some absolute constant C.

We defer the proof of the above theorem to Section A.
We now show how to use the above metric space partitioning scheme to obtain an approximation
algorithm for Correlation Clustering. Note that this proves Theorem 4.1.

5

Proof of Theorem 4.1. Our algorithm ﬁrst ﬁnds the optimal solution x  y  z to the convex relaxation
(P) presented in Section 3. Then  it deﬁnes a metric d(u  v) = xuv on the vertices of the graph. Finally 
it runs the metric space partitioning algorithm with = 1 /2 from Section A (see Theorem 4.2) and
outputs the obtained partitioning P.
Let us analyze the performance of this algorithm. Denote the cost of the optimal solution x  y  z by
LP . We know that the cost of the optimal solution OP T is lower bounded by LP (see Section 3
for details). By Theorem 4.2  applied to the graph G = (V  E+) (note: we ignore negative edges for
now) 

C


q+1

yq

q1
2q log

q+1

q1
2q log

n

q +Xu2V
u 1

2q n ·⇣Xu2V

q⌘  4Cn
zu 1

Ehk cut(P  E+)kqi 
2q n · LP.
(2)
Recall that a positive edge is not in agreement if and only if it is cut. Hence  disagree(P  E+  ?) =
cut(P  E+)  and the bound above holds for Ek disagree(P  E+  ?)kq. By the triangle inequality 
Ek disagree(P  E+  E)kq  Ek disagree(P  E+  ?)kq + Ek disagree(P  ?  E)kq. Hence  to
ﬁnish the proof  it remains to upper bound Ek disagree(P  ?  E)kq.
Observe that the diameter of every cluster returned by the algorithm is at most = 1 /2. For
all disagreeing negative edges (u  v) 2 E  we have xuv  1/2 and 1  xuv  1/2. Thus 
disagreeu(P  ?  E)  2yu for every u  and Ek disagree(P  ?  E)kq  2kykq  2LP . This
completes the proof.

4.2 Correlation Clustering on complete graphs
In this section  we present our algorithm for Correlation Clustering on complete graphs and its
analysis. Our algorithm achieves an approximation ratio of 5 and is an improvement over the
approximation ratio of 7 by Charikar  Gupta  and Schwartz [2017].

4.2.1 Summary of the algorithm
Our algorithm is based on rounding an optimal solution to the convex relaxation (P). Recall that
for complete graphs  we can get a simpler relaxation by removing the variables z in our convex
programming formulation. We start with considering the entire vertex set of unclustered vertices. At
each step t of the algorithm  we select a subset of vertices as a cluster Ct and remove it from the set
of unclustered vertices. Thus  each vertex is assigned to a cluster exactly once and is never removed
from a cluster once it is assigned.
For each vertex w 2 V   let Ball(w  ⇢) = {u 2 V : xuw  ⇢} be the set of vertices within a distance
of ⇢ from w. For r = 1/5 the quantity r  xuw where u 2 Ball(w  r) represents the distance from u
to the boundary of the ball of radius 1/5 around w. Let Vt ✓ V be the set of unclustered vertices at
step t  and deﬁne

Lt(w) = Xu2Ball(w r)\Vt

r  xuw.

At each step t  we select the vertex wt that maximizes the quantity Lt(w) over all unclustered vertices
w 2 Vt and select the set Ball(wt  2r) as a cluster. We repeat this step until all the nodes have
been clustered. A complete description of the algorithm can be found in Figure B.1 (supplementary
material).

4.2.2 Overview of the analysis
Our main result for complete graphs is the following  which proves Theorem 1.3.
Theorem 4.3. Algorithm 2 is a 5approximation algorithm for Correlation Clustering on complete
graphs.
For an edge (u  v) 2 E  let LP (u  v) be the LP cost of the edge (u  v): LP (u  v) = xuv if (u  v) 2
E+ and LP (u  v) = 1  xuv if (u  v) 2 E. Let ALG(u  v) = 1((u  v) is in disagreement ).
Deﬁne

proﬁt(u) = X(u v)2E

LP (u  v)  r X(u v)2E

ALG(u  v) 

6

where r = 1/5. We show that for each vertex u 2 V   we have proﬁt(u)  0 (see Lemma 4.4) and 
therefore  the number of disagreeing edges incident to u is upper bounded by 5y(u):

ALG(u) = Xv:(u v)2E

ALG(u  v) 

1

r Xv:(u v)2E

LP (u  v) = 5y(u).

Thus  kALGkq  5kykq for any q  1. Consequently  the approximation ratio of the algorithm is at
most 5 for any norm `q.
Lemma 4.4. For every u 2 V   we have proﬁt(u)  0.
At each step t of the algorithm  we create a new cluster Ct and remove it from the graph. We also
remove all edges with at least one endpoint in Ct. Denote this set of edges by

ALG(u  v).

(3)

Now let

Et = {(u  v) : u 2 Ct or v 2 Ct}.

proﬁtt(u  v) =⇢LP (u  v)  rALG(u  v) 

0 

proﬁtt(u) = Xv2Vt

proﬁtt(u  v) = X(u v)2Et

.

if (u  v) 2 E
otherwise
LP (u  v)  r X(u v)2Et

As all sets Et are disjoint  proﬁt(u) =Pt proﬁtt(u). Thus  to prove Lemma 4.4  it is sufﬁcient to
show that proﬁtt(u)  0 for all t. Note that we only need to consider u 2 Vt as proﬁtt(u) = 0 for
u /2 Vt.
Consider a step t of the algorithm and vertex u 2 Vt. Let w = wt be the center of the cluster
chosen at this step. First  we show that since the diameter of the cluster Ct is 4r  for all negative
edges (u  v) 2 E with u  v 2 Ct  we can charge the cost of disagreement to the edge itself  that
is  proﬁtt(u  v) is nonnegative for (u  v) 2 E (see Lemma B.3). We then consider two cases:
xuw 2 [0  r] [ [3r  1] and xuw 2 (r  3r].
The former case is fairly simple since disagreeing positive edges (u  v) 2 E+ (with xuw 2 [0  r] [
[3r  1]) have a “large” LP cost. In Lemma B.4 and Lemma B.5  we prove that the cost of disagreement
can be charged to the edge itself and hence proﬁtt(u)  0.
We then consider the latter case. For vertices u with xuw 2 (r  3r]  proﬁtt(u  v) for some disagreeing
positive edges (u  v) might be negative. Thus  we split the proﬁt at step t for such vertices u into the
proﬁt they get from edges (u  v) with v in Ball(w  r) \ Vt and from edges with v in Vt \ Ball(w  r).
That is 

proﬁtt(u  v)

proﬁtt(u  v)

.

proﬁtt(u) = Xv2Ball(w r)
{z

|

Phigh(u)

+ Xv2Vt\Ball(w r)
{z
|

}

Plow(u)

}

Denote the ﬁrst term by Phigh(u) and the second term by Plow(u). We show that Plow(u)  Lt(u)
(see Claim B.6 and Lemma B.7) and Phigh  Lt(w) (see Claim B.8 and Lemma B.9) and conclude
that proﬁtt(u) = Phigh(u) + Plow(u)  Lt(w)  Lt(u)  0 since Lt(w) = maxw02Vt Lt(w0) 
Lt(u).
This ﬁnishes the proof of Lemma 4.4.

4.3 Correlation Clustering with AKS Min Max Objective
In this section  we present our improved algorithm for Correlation Clustering with AKS Min Max
Objective. Our algorithm produces a clustering of cost at most (2 + ")OP T   which improves upon

the bound of O(plog n · max{log |E|  log(k)})approximation algorithm studied by Ahmadi 
Galhotra  Khuller  Saha  and Schwartz [2019a].
For a subset S ✓ V of vertices  we use cost+(S) to refer to the weight of positive edges “associated”
with S that are in disagreement. These are the edges with exactly one end point in S. Thus 
cost+(S) =P(u v)2E+ u2S v62S wuv. Similarly  we use cost(S) to refer to the weight of dissimilar

edges “associated” with S that are in disagreement. These are the edges with both endpoints in

7

S. Thus  cost(S) =P(u v)2E u v2S wuv. The total cost of the set S is cost(S) = cost+(S) +

cost(S).
Similar to the algorithm of Ahmadi et al. [2019b]  our algorithm works in two phases. In the ﬁrst phase 
the algorithm covers all vertices of the graph with (possibly overlapping) sets S1  . . .   Sk such that the
cost of each set Si is at most 2OP T (i.e.  cost(Si)  2OP T for each i 2{ 1  . . .   k}). In the second
phase  the algorithm ﬁnds sets P1  . . .   Pk such that: (1) P1  . . .   Pk are disjoint and cover the vertex
set; (2) Pi ✓ Si (and  consequently  cost(Pi)  cost(Si)); (3) cost+(Pi)  (1 + ") cost+(Si).
The sets P1  . . .   Pk are obtained from S1  . . .   Sk using an uncrossing procedure of Bansal et al.
[2011]. Hence the clustering that is output is P = (P1  . . .   Pk). The improvement in the approxima-
tion factor comes from the ﬁrst phase of the algorithm.

4.3.1 Summary of the algorithm
At the core of our algorithm is a simple subproblem: For a given vertex z 2 V   ﬁnd a subset S ✓ V
containing z such that cost(S) is minimized. We solve this subproblem using a linear programming
relaxation  which is formulated as follows: The LP has a variable xu for each vertex u 2 V . In the
intended integral solution  we have xu = 1 if u is in the set S  and xu = 0  otherwise. That is  xu is
the indicator of the event “u 2 S”. The LP has only one constraint: xz = 1. A complete description
of the LP can be found in Figure C.1. In Claim C.1 we show that this LP is indeed a valid relaxation
for our subproblem.
Moreover we prove that this LP is half-integral  please see section C.1 for details. We now present
our algorithm which gives a 2-approximation to the subproblem.
Rounding algorithm for subproblem. We present a simple rounding algorithm. Let x⇤ be an
optimal half-integral LP solution to the problem. We obtain an integral solution x by rouding down
x⇤  that is xu = bx⇤uc for all u. Thus  µuv  2 · µ⇤uv and ⌘uv  ⌘⇤uv for all positive and negative
edges respectively. Thus  the cost of the rounded solution x is at most 2OPT.
Rounding algorithm for AKS Min Max Correlation Clustering. To obtain a cover of all the
vertices  we pick yet uncovered vertices z 2 V one by one and for each z  ﬁnd a set S(z) as described
above. Then  we remove those sets S(z) that are completely covered by other sets. The obtained
family of sets S = {S(z)} satisﬁes the following properties: (1) Sets in S cover the entire set V ; (2)
cost(S)  2OP T for each S 2S ; (3) Each set S 2S is not covered by the other sets in S (that is 
for each S 2S   S 6⇢ [S02(S\{S})S0). However  sets S in S are not necessarily disjoint.
Following Ahmadi et al. [2019b]  we then apply an uncrossing procedure developed by Bansal et al.
[2011] to the sets Si in S and obtain disjoint sets Pi such that (1) Pi ⇢ Si and (2) cost+(Pi) 
cost+(Si) + "OP T for each i (see Lemma C.3 in Section C.2). We have cost+(Pi)  cost+(Si) +
"OP T and cost(Pi)  cost(Si)  since Pi is a subset of Si. Thus  cost(Pi)  cost(Si) + "OP T
and  consequently  P1  . . .   Pk is a 2(1 + ")-approximation for Correlation Clustering with the
AKS Min Max objective. We note that by slightly modifying our algorithm we can obtain a 2-
approximation.
Finally  we show that AKS Min-Max Correlation Clustering is at least as hard as Vertex Cover (see
C.3 for details). Vertex Cover is NP-hard to approximate within any constant factor better than 2
assuming the Unique Games conjecture (UGC) (see Khot and Regev [2008]). Thus  our algorithm
gives the best possible approximation if UGC holds.

References
Saba Ahmadi  Sainyam Galhotra  Samir Khuller  Barna Saha  and Roy Schwartz. Min-max correlation

clustering via multicut  2019a.

Saba Ahmadi  Samir Khuller  and Barna Saha. Min-max correlation clustering via multicut. In
International Conference on Integer Programming and Combinatorial Optimization  pages 13–26.
Springer  2019b.

Nir Ailon  Moses Charikar  and Alantha Newman. Aggregating inconsistent information: ranking

and clustering. Journal of the ACM (JACM)  55(5):23  2008.

8

Nikhil Bansal  Avrim Blum  and Shuchi Chawla. Correlation clustering. Machine learning  56(1-3):

89–113  2004.

Nikhil Bansal  Uriel Feige  Robert Krauthgamer  Konstantin Makarychev  Viswanath Nagarajan 
Joseph Naor  and Roy Schwartz. Min-max graph partitioning and small set expansion. In 2011
IEEE 52nd Annual Symposium on Foundations of Computer Science  pages 17–26. IEEE  2011.
In
Foundations of Computer Science  1996. Proceedings.  37th Annual Symposium on  pages 184–193.
IEEE  1996.

Yair Bartal. Probabilistic approximation of metric spaces and its algorithmic applications.

Francesco Bonchi  David Garcia-Soriano  and Edo Liberty. Correlation clustering: from theory to

practice. In KDD  page 1972. Citeseer  2014.

Moses Charikar  Venkatesan Guruswami  and Anthony Wirth. Clustering with qualitative information.

In IEEE Symposium on Foundations of Computer Science (FOCS). Citeseer  2003.

Moses Charikar  Neha Gupta  and Roy Schwartz. Local guarantees in graph cuts and clustering.
In International Conference on Integer Programming and Combinatorial Optimization  pages
136–147. Springer  2017.

Shuchi Chawla  Robert Krauthgamer  Ravi Kumar  Yuval Rabani  and D Sivakumar. On the hardness

of approximating multicut and sparsest-cut. computational complexity  15(2):94–114  2006.

Shuchi Chawla  Konstantin Makarychev  Tselil Schramm  and Grigory Yaroslavtsev. Near optimal
lp rounding algorithm for correlationclustering on complete and complete k-partite graphs. In
Proceedings of the forty-seventh annual ACM symposium on Theory of computing  pages 219–228.
ACM  2015.

William Cohen and Jacob Richman. Learning to match and cluster entity names. In ACM SIGIR-2001

Workshop on Mathematical/Formal Methods in Information Retrieval  2001.

William W Cohen and Jacob Richman. Learning to match and cluster large high-dimensional data
sets for data integration. In Proceedings of the eighth ACM SIGKDD international conference on
Knowledge discovery and data mining  pages 475–480. ACM  2002.

Erik D Demaine  Dotan Emanuel  Amos Fiat  and Nicole Immorlica. Correlation clustering in general

weighted graphs. Theoretical Computer Science  361(2-3):172–187  2006.

Jittat Fakcharoenphol and Kunal Talwar. Improved decompositions of graphs with forbidden minors.
In 6th International workshop on Approximation algorithms for combinatorial optimization  pages
36–46  2003.

Jittat Fakcharoenphol  Satish Rao  and Kunal Talwar. A tight bound on approximating arbitrary

metrics by tree metrics. Journal of Computer and System Sciences  69(3):485–497  2004.

Naveen Garg  Vijay V Vazirani  and Mihalis Yannakakis. Approximate max-ﬂow min-(multi) cut

theorems and their applications. SIAM Journal on Computing  25(2):235–251  1996.

Anupam Gupta  Robert Krauthgamer  and James R Lee. Bounded geometries  fractals  and low-

distortion embeddings. In null  page 534. IEEE  2003.

Subhash Khot and Oded Regev. Vertex cover might be hard to approximate to within 2- ". Journal of

Computer and System Sciences  74(3):335–349  2008.

Gregory Puleo and Olgica Milenkovic. Correlation clustering and biclustering with locally bounded

errors. In International Conference on Machine Learning  pages 869–877  2016.

Anirudh Ramachandran  Nick Feamster  and Santosh Vempala. Filtering spam with behavioral
blacklisting. In Proceedings of the 14th ACM conference on Computer and communications
security  pages 342–351. ACM  2007.

Satish Rao. Small distortion and volume preserving embeddings for planar and euclidean metrics. In
Proceedings of the ﬁfteenth annual symposium on Computational geometry  pages 300–306. ACM 
1999.

9

Anthony Wirth. Correlation Clustering  pages 280–284. Springer US  Boston  MA  2017. ISBN

978-1-4899-7687-1. doi: 10.1007/978-1-4899-7687-1_176. URL

.

10

,Sanchit Kalhan
Konstantin Makarychev
Timothy Zhou