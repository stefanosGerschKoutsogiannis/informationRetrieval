2018,Exploration in Structured Reinforcement Learning,We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state  action) pairs. For any arbitrary structure  we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs  the bounds are shown not to scale with the sizes S and A of the state and action spaces  i.e.  they are smaller than c log T where T is the time horizon and the constant c only depends on the Lipschitz structure  the span of the bias function  and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as SA log T. We devise DEL (Directed Exploration Learning)  an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs  and show that the simplified version is still able to efficiently exploit the structure.,Exploration in Structured Reinforcement Learning

Jungseul Ok
KTH  EECS

Stockholm  Sweden

ockjs@illinois.edu

Alexandre Proutiere

KTH  EECS

Stockholm  Sweden
alepro@kth.se

Damianos Tranos

KTH  EECS

Stockholm  Sweden
tranos@kth.se

Abstract

We address reinforcement learning problems with ﬁnite state and action spaces
where the underlying MDP has some known structure that could be potentially
exploited to minimize the exploration rates of suboptimal (state  action) pairs. For
any arbitrary structure  we derive problem-speciﬁc regret lower bounds satisﬁed
by any learning algorithm. These lower bounds are made explicit for unstructured
MDPs and for those whose transition probabilities and average reward functions are
Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs  the bounds
are shown not to scale with the sizes S and A of the state and action spaces  i.e. 
they are smaller than c log T where T is the time horizon and the constant c only
depends on the Lipschitz structure  the span of the bias function  and the minimal
action sub-optimality gap. This contrasts with unstructured MDPs where the regret
lower bound typically scales as SA log T . We devise DEL (Directed Exploration
Learning)  an algorithm that matches our regret lower bounds. We further simplify
the algorithm for Lipschitz MDPs  and show that the simpliﬁed version is still able
to efﬁciently exploit the structure.

1

Introduction

√

Real-world Reinforcement Learning (RL) problems often concern dynamical systems with large state
and action spaces  which make the design of efﬁcient algorithms extremely challenging. This difﬁculty
is well illustrated by the known regret fundamental limits. The regret compares the accumulated
reward of an optimal policy (aware of the system dynamics and reward function) to that of the
algorithm considered  and it quantiﬁes the loss incurred by the need of exploring sub-optimal (state 
action) pairs to learn the system dynamics and rewards. In online RL problems with undiscounted
SAT 1  where S  A  and T denote the
reward  regret lower bounds typically scale as SA log T or
sizes of the state and action spaces and the time horizon  respectively. Hence  with large state and
action spaces  it is essential to identify and exploit any possible structure existing in the system
dynamics and reward function so as to minimize exploration phases and in turn reduce regret to
reasonable values. Modern RL algorithms actually implicitly impose some structural properties
either in the model parameters (transition probabilities and reward function  see e.g. [Ortner and
Ryabko  2012]) or directly in the Q-function (for discounted RL problems  see e.g. [Mnih et al. 
2015]. Despite the successes of these recent algorithms  our understanding of structured RL problems
remains limited.
In this paper  we explore structured RL problems with ﬁnite state and action spaces. We ﬁrst
derive problem-speciﬁc regret lower bounds satisﬁed by any algorithm for RL problems with any
arbitrary structure. These lower bounds are instrumental to devise algorithms optimally balancing
exploration and exploitation  i.e.  achieving the regret fundamental limits. A similar approach has

1The ﬁrst lower bound is asymptotic in T and problem-speciﬁc  the second is minimax. We ignore here for

simplicity the dependence of these bounds in the diameter  bias span  and action sub-optimality gap.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

δmin

δ2
min

been recently applied with success to stochastic bandit problems  where the average reward of arms
exhibits structural properties  e.g. unimodality [Combes and Proutiere  2014]  Lipschitz continuity
[Magureanu et al.  2014]  or more general properties [Combes et al.  2017]. Extending these results
to RL problems is highly non trivial  and to our knowledge  this paper is the ﬁrst to provide problem-
speciﬁc regret lower bounds for structured RL problems. Although the results presented here concern
ergodic RL problems with undiscounted reward  they could be easily generalized to discounted
problems (under an appropriate deﬁnition of regret).
Our contributions are as follows:
1. For ergodic structured RL problems  we derive problem-speciﬁc regret lower bounds. The latter
are valid for any structure (but are structure-speciﬁc)  and for unknown system dynamics and reward
function.
2. We analyze the lower bounds for unstructured MDPs  and show that they scale at most as
(H+1)2
SA log T   where H and δmin represent the span of the bias function and the minimal state-
action sub-optimality gap  respectively. These results extend previously known regret lower bounds
derived in the seminal paper [Burnetas and Katehakis  1997] to the case where the reward function is
unknown.
3. We further study the regret lower bounds in the case of Lipschitz MDPs. Interestingly  these
bounds are shown to scale at most as (H+1)3
SlipAlip log T where Slip and Alip only depend on the
Lipschitz properties of the transition probabilities and reward function. This indicates that when H
and δmin do not scale with the sizes of the state and action spaces  we can hope for a regret growing
logarithmically with the time horizon  and independent of S and A.
4. We propose DEL  an algorithm that achieves our regret fundamental limits for any structured
MDP. DEL is rather complex to implement since it requires in each round to solve an optimization
problem similar to that providing the regret lower bounds. Fortunately  we were able to devise
simpliﬁed versions of DEL  with regret scaling at most as (H+1)2
SlipAlip log T
δmin
for unstructured and Lipschitz MDPs  respectively. In absence of structure  DEL  in its simpliﬁed
version  does not require to compute action indexes as done in OLP [Tewari and Bartlett  2008]  and
yet achieves similar regret guarantees without the knowledge of the reward function. DEL  simpliﬁed
for Lipschitz MDPs  only needs  in each step  to compute the optimal policy of the estimated MDP 
as well as to solve a simple linear program.
5. Preliminary numerical experiments (presented in the supplementary material) illustrate our
theoretical ﬁndings. In particular  we provide examples of Lipschitz MDPs  for which the regret
under DEL does not seem to scale with S and A  and signiﬁcantly outperforms algorithms that do
not exploit the structure.

SA log T and (H+1)3

δ2
min

2 Related Work

Regret lower bounds have been extensively investigated for unstructured ergodic RL problems.
[Burnetas and Katehakis  1997] provided a problem-speciﬁc lower bound similar to ours  but only
valid when the reward function is known. Minimax regret lower bounds have been studied e.g. in
√
[Auer et al.  2009] and [Bartlett and Tewari  2009]: in the worst case  the regret has to scale as
DSAT where D is the diameter of the MDP. In spite of these results  regret lower bounds for
unstructured RL problems are still attracting some attention  see e.g. [Osband and Van Roy  2016]
for insightful discussions. To our knowledge  this paper constitutes the ﬁrst attempt to derive regret
lower bounds in the case of structured RL problems. Our bounds are asymptotic in the time horizon
T   but we hope to extend them to ﬁnite time horizons using similar techniques as those recently
used to provide such bounds for bandit problems [Garivier et al.  Jun. 2018]. These techniques
address problem-speciﬁc and minimax lower bounds in a uniﬁed manner  and can be leveraged to
derive minimax lower bounds for structured RL problems. However we do not expect minimax lower
bounds to be very informative about the regret gains that one may achieve by exploiting a structure
(indeed  the MDPs leading to worst-case regret in unstructured RL comply to many structures).
√
There have been a plethora of algorithms developed for ergodic unstructured RL problems. We
may classify these algorithms depending on their regret guarantees  either scaling as log T or
T .
In absence of structure  [Burnetas and Katehakis  1997] developed an asymptotically optimal  but
involved  algorithm. This algorithm has been simpliﬁed in [Tewari and Bartlett  2008]  but remains

2

√

√

T include UCRL2 [Auer et al.  2009]  KL-UCRL with regret guarantees ˜O(DS

more complex than our proposed algorithm. Some algorithms have ﬁnite-time regret guarantees
scaling as log T [Auer and Ortner  2007]  [Auer et al.  2009]  [Filippi et al.  2010]. For example  the
authors of [Filippi et al.  2010] propose KL-UCRL an extension of UCRL [Auer and Ortner  2007]
with regret bounded by D2S2A
log T . Having ﬁnite-time regret guarantees is arguably desirable  but
δmin
so far this comes at the expense of a much larger constant in front of log T . Algorithms with regret
scaling as
AT ) 
REGAL.C [Bartlett and Tewari  2009] with guarantees ˜O(HS
AT ). Recently  the authors of
[Agrawal and Jia  2017] managed to achieve a regret guarantee of ˜O(D
SAT )  but only valid when
T ≥ S5A.
Algorithms devised to exploit some known structure are most often applicable to RL problems with
continuous state or action spaces. Typically  the transition probabilities and reward function are
assumed to be smooth in the state and action  typically Lipschitz continuous [Ortner and Ryabko 
2012]  [Lakshmanan et al.  2015]. The regret then needs to scale as a power of T   e.g. T 2/3 in
[Lakshmanan et al.  2015] for 1-dimensional state spaces. An original approach to RL problems for
which the transition probabilities belong to some known class of functions was proposed in [Osband
and Van Roy  2014]. The regret upper bounds derived there depend on the so-called Kolmogorov and
eluder dimensions  which in turn depend on the chosen class of functions. Our approach to design
learning algorithms exploiting the structure is different from all aforementioned methods  as we aim
at matching the problem-speciﬁc minimal exploration rates of sub-optimal (state  action) pairs.

√

√

t -measurable. We denote by Π the set of all such policies.

3 Models and Objectives
We consider an MDP φ = (pφ  qφ) with ﬁnite state and action spaces S and A of respective
cardinalities S and A. pφ and qφ are the transition and reward kernels of φ. Speciﬁcally  when in state
x  taking action a  the system moves to state y with probability pφ(y|x  a)  and a reward drawn from
distribution qφ(·|x  a) of average rφ(x  a) is collected. The rewards are bounded  w.l.o.g.  in [0  1].
We assume that for any (x  a)  qφ(·|x  a) is absolutely continuous w.r.t. some measure λ on [0  1]2.
The random vector Zt := (Xt  At  Rt) represents the state  the action  and the collected reward at
step t. A policy π selects an action  denoted by πt(x)  in step t when the system is in state x based
on the history captured through Hπ
t   the σ-algebra generated by (Z1  . . .   Zt−1  Xt) observed under
π: πt(x) is Hπ
Structured MDPs. The MDP φ is initially unknown. However we assume that φ belongs to some
well speciﬁed set Φ which may encode a known structure of the MDP. The knowledge of Φ can be
exploited to devise (more) efﬁcient policies. The results derived in this paper are valid under any
structure  but we give a particular attention to the cases of
(i) Unstructured MDPs: φ ∈ Φ if for all (x  a)  pφ(· | x  a) ∈ P(S) and qφ(· | x  a) ∈ P([0  1])3;
(ii) Lipschitz MDPs: φ ∈ Φ if pφ(·|x  a) and rφ(x  a) are Lipschitz-continuous w.r.t. x and a in some
metric space (we provide a precise deﬁnition in the next section).
The learning problem. The expected cumulative reward up to step T of a policy π ∈ Π when
x[·] denotes the expectation under
the system starts in state x is V π
policy π given that X1 = x. Now assume that the system starts in state x and evolves according
to the initially unknown MDP φ ∈ Φ for given structure Φ  the objective is to devise a policy
π ∈ Π maximizing V π
T (x) up to step T deﬁned as the
difference between the cumulative reward of an optimal policy and that obtained under π:

T (x) or equivalently  minimizing the regret Rπ

t=1 Rt]  where Eπ

x[(cid:80)T

T (x) := Eπ

T (x) := V ∗
Rπ

T (x) − V π

T (x)

T (x).

T (x) := supπ∈Π V π

where V ∗
Preliminaries and notations. Let ΠD be the set of stationary (deterministic) policies  i.e. when
in state Xt = x  f ∈ ΠD selects an action f (x) independent of t. φ is communicating if each
pair of states are connected by some policy. Further  φ is ergodic if under any stationary policy 
2λ can be the Lebesgue measure; alternatively  if rewards take values in {0  1}  λ can be the sum of Dirac
3P(S) is the set of distributions on S and P([0  1]) is the set of distributions on [0  1]  absolutely continuous

measures at 0 and 1.

w.r.t. λ.

3

T V π

φ(x) := limT→∞ 1

the resulting Markov chain (Xt)t≥1 is irreducible. For any communicating φ and any policy π ∈
φ(x) the gain of π (or long-term average reward) started from initial state
ΠD  we denote by gπ
T (x). We denote by Π∗(φ) the set of stationary policies with maximal
x: gπ
gain: Π∗(φ) := {f ∈ ΠD : gf
φ(x). If φ is
φ of f ∈ ΠD is
communicating  the maximal gain is constant and denoted by g∗
deﬁned by hf
φ(Xt))]  and quantiﬁes the advantage of starting
φ  respectively  the Bellman operator under action a and the
in state x. We denote by Ba
optimal Bellman operator under φ. They are deﬁned by: for any h : S (cid:55)→ R and x ∈ S 

φ(x) := C- limT→∞ Ef
φ and B∗

φ(x) ∀x ∈ S}  where g∗

φ. The bias function hf

φ(x) := maxπ∈Π gπ

t=1(Rt − gf

φ(x) = g∗

x[(cid:80)∞
(cid:88)

(Ba

φh)(x) := rφ(x  a) +

pφ(y|x  a)h(y)

and

y∈S

a∈A (Ba
φ satisfy the evaluation equation: for all state x ∈ S  gf

φh)(x) := max

(B∗

φh)(x) .

φ(x)+hf

φ(x) =

φ and hf

φ verify the optimality equation:

Then for any f ∈ ΠD  gf
(Bf (x)

φ hf

φ and hf

φ)(x). Furthermore  f ∈ Π∗(φ) if and only if gf
φ(x) = (B∗

gf
φ(x) + hf

φhf

φ)(x) .

φh)(x)}. For ergodic φ  h∗

φ the bias function of an optimal stationary policy4  and by H its span H :=
φ(y). For x ∈ S  h : S (cid:55)→ R  and φ ∈ Φ  let O(x; h  φ) = {a ∈ A : (B∗
φh)(x) =
φ is unique up to an additive constant. Hence  for ergodic φ  the set
φ  φ)  and Π∗(φ) = {f ∈ ΠD : f (x) ∈

We denote by h∗
φ(x) − h∗
maxx y h∗
(Ba
of optimal actions in state x under φ is O(x; φ) := O(x; h∗
O(x; φ) ∀x ∈ S}. Finally  we deﬁne for any state x and action a 
φh∗

δ∗(x  a; φ) := (B∗

φ)(x) − (Ba

φ)(x) .

φh∗

This can be interpreted as the long-term regret obtained by initially selecting action a in state x (and
then applying an optimal stationary policy) rather than following an optimal policy. The minimum
gap is deﬁned as δmin := min(x a):δ∗(x a;φ)>0 δ∗(x  a; φ).
We denote by ¯R+ = R+ ∪ {∞}. The set of MDPs is equipped with the following (cid:96)∞-norm:
(cid:107)φ−ψ(cid:107) := max(x a)∈S×A (cid:107)φ(x  a)−ψ(x  a)(cid:107) where (cid:107)φ(x  a)−ψ(x  a)(cid:107) := |rφ(x  a)−rψ(x  a)|+
maxy∈S |pφ(y | x  a) − pψ(y | x  a)|.
The proofs of all results are presented in the supplementary material.

4 Regret Lower Bounds

T (x) = o(T α) .

In this section  we present an (asymptotic) regret lower bound satisﬁed by any uniformly good learning
algorithm. An algorithm π ∈ Π is uniformly good if for all ergodic φ ∈ Φ  any initial state x and any
constant α > 0  the regret of π satisﬁes Rπ
To state our lower bound  we introduce the following notations. For φ and ψ  we denote φ (cid:28) ψ if the
kernel of φ is absolutely continuous w.r.t. that of ψ  i.e.  ∀E  Pφ[E] = 0 if Pψ[E] = 0. For φ and ψ
such that φ (cid:28) ψ and (x  a)  we deﬁne the KL-divergence between φ and ψ in state-action pair (x  a)
KLφ|ψ(x  a) as the KL-divergence between the distributions of the next state and collected reward if
the state is x and a is selected under these two MDPs:
pφ(y|x  a)
pψ(y|x  a)

qφ(r|x  a)
qψ(r|x  a)

pφ(y|x  a) log

qφ(r|x  a) log

KLφ|ψ(x  a) =

(cid:88)

(cid:90) 1

λ(dr) .

+

0

y∈S

We further deﬁne the set of confusing MDPs as:
∆Φ(φ) = {ψ ∈ Φ : φ (cid:28) ψ  (i) KLφ|ψ(x  a) = 0 ∀x ∀a ∈ O(x; φ); (ii) Π∗(φ) ∩ Π∗(ψ) = ∅} .
This set consists of MDP ψ’s that (i) coincide with φ for state-action pairs where the actions are
optimal (the kernels of φ and ψ cannot be statistically distinguished under an optimal policy); and
such that (ii) the optimal policies under ψ are not optimal under φ.

4In case of h∗

φ is not unique  we arbitrarily select an optimal stationary policy and deﬁne h∗
φ.

4

Theorem 1. Let φ ∈ Φ be ergodic. For any uniformly good algorithm π ∈ Π and for any x ∈ S 

lim inf
T→∞

Rπ
T (x)
log T

≥ KΦ(φ) 

η(x  a)δ∗(x  a; φ) 

(1)

(2)

where KΦ(φ) is the value of the following optimization problem:

(cid:88)

(x a)∈S×A

inf

η∈FΦ(φ)

:(cid:80)

where FΦ(φ) := {η ∈ ¯RS×A

+

(x a)∈S×A η(x  a)KLφ|ψ(x  a) ≥ 1 ∀ψ ∈ ∆Φ(φ)}.

The above theorem can be interpreted as follows. When selecting a sub-optimal action a in state x 
one has to pay a regret of δ∗(x  a; φ). Then the minimal number of times any sub-optimal action a in
state x has to be explored scales as η∗(x  a) log T where η∗(x  a) solves the optimization problem
(2). It is worth mentioning that our lower bound is tight  as we present in Section 5 an algorithm
achieving this fundamental limit of regret.
The regret lower bound stated in Theorem 1 extends the problem-speciﬁc regret lower bound derived
in [Burnetas and Katehakis  1997] for unstructured ergodic MDPs with known reward function. Our
lower bound is valid for unknown reward function  but also applies to any structure Φ. Note however
that at this point  it is only implicitly deﬁned through the solution of (2)  which seems difﬁcult to solve.
The optimization problem can actually be simpliﬁed  as shown later in this section  by providing
useful structural properties of the feasibility set FΦ(φ) depending on the structure considered. The
simpliﬁcation will be instrumental to quantify the gain that can be achieved when optimally exploiting
the structure  as well as to design efﬁcient algorithms.

In the following  the optimization problem: inf η∈F(cid:80)

(x a)∈S×A η(x  a)δ∗(x  a; φ) is referred to as

P (φ F); so that P (φ FΦ(φ)) corresponds to (2).
The proof of Theorem 1 combines a characterization of the regret as a function of the number of times
NT (x  a) up to step T (state  action) pair (x  a) is visited  and of the δ∗(x  a; φ)’s  and change-of-
measure arguments as those recently used to prove in a very direct manner regret lower bounds in ban-
dit optimization problems [Kaufmann et al.  2016]. More precisely  for any uniformly good algorithm
π  and for any confusing MDP ψ ∈ ∆Φ(φ)  we show that the exploration rates required to statistically
[NT (x  a)]KLφ|ψ(x  a) ≥ 1 where
distinguish ψ from φ satisfy lim inf T→∞ 1
log T
the expectation is taken w.r.t. φ given any initial state x1. The theorem is then obtained by considering
(hence optimizing the lower bound) all possible confusing MDPs.

(x a)∈S×A Eπ

(cid:80)

x1

4.1 Decoupled exploration in unstructured MDPs
In the absence of structure  Φ = {ψ : pψ(·|x  a) ∈ P(S)  qψ(·|x  a) ∈ P([0  1]) ∀(x  a)}  and we
have:
Theorem 2. Consider the unstructured model Φ  and let φ ∈ Φ be ergodic. We have:

: ∀(x  a) s.t. a /∈ O(x; φ)  η(x  a)KLφ|ψ(x  a) ≥ 1  ∀ψ ∈ ∆Φ(x  a; φ)(cid:9)

FΦ(φ) =(cid:8)η ∈ ¯RS×A

+

ψh∗

φ + h∗

φ)(x) > g∗

φ(x)}.
where ∆Φ(x  a; φ) := {ψ ∈ Φ : KLφ|ψ(y  b) = 0 ∀(y  b) (cid:54)= (x  a) and (Ba
The theorem states that in the constraints of the optimization problem (2)  we can restrict our attention
to confusing MDPs ψ that are different than the original MDP φ only for a single state-action
pair (x  a). Further note that the condition (Ba
φ(x) is equivalent to saying that
action a becomes optimal in state x under ψ (see Lemma 1(i) in [Burnetas and Katehakis  1997]).
Hence to obtain the lower bound in unstructured MDPs  we may just consider confusing MDPs ψ
which make an initially sub-optimal action a in state x optimal by locally changing the kernels and
rewards of φ at (x  a) only. Importantly  this observation implies that an optimal algorithm π must
[NT (x  a)] ∼ log T / inf ψ∈∆Φ(x a;φ) KLφ|ψ(x  a). In other words  the required level of
satisfy Eπ
exploration of the various sub-optimal state-action pairs are decoupled  which signiﬁcantly simpliﬁes
the design of optimal algorithms.
To get an idea on how the regret lower bound scales as the sizes of both state and action spaces 
we can further provide an upper bound of the regret lower bound. One may easily observe that

φ)(x) > g∗

φ + h∗

ψh∗

x1

5

Fun(φ) ⊂ FΦ(φ) where

(cid:40)

Fun(φ) =

η ∈ ¯RS×A

+

(cid:18) δ∗(x  a; φ)

H + 1

(cid:41)
(cid:19)2 ≥ 2  ∀(x  a) s.t. a /∈ O(x; φ)

.

: η(x  a)

From this result  an upper bound of the regret lower bound is Kun(φ) := 2 (H+1)2
δmin
can devise algorithms achieving this regret scaling (see Section 5).
Theorem 2 relies on the following decoupling lemma  actually valid under any structure Φ.
Lemma 1. Let U1 U2 be two non-overlapping subsets of the (state  action) pairs such that for all
(x  a) ∈ U0 := U1 ∪U2  a /∈ O(x; φ). Deﬁne the following three MDPs in Φ obtained starting from φ
and changing the kernels for (state  action) pairs in U1 ∪U2. Speciﬁcally  let (p  q) be some transition
and reward kernels. For all (x  a)  deﬁne ψj  j ∈ {0  1  2} as

SA log T   and we

(cid:26)(p(·|x  a)  q(·|x  a))

(pφ(·|x  a)  qφ(·|x  a))

if (x  a) ∈ Uj 
otherwise.

(pψj (·|x  a)  qψj (·|x  a)) =

Then  if Π∗(φ) ∩ Π∗(ψ0) = ∅  then Π∗(φ) ∩ Π∗(ψ1) = ∅ or Π∗(φ) ∩ Π∗(ψ2) = ∅.

4.2 Lipschitz structure

Lipschitz structures have been widely studied in the bandit and reinforcement learning literature.
We ﬁnd it convenient to use the following structure  although one could imagine other variants in
more general metric spaces. We assume that the state (resp. action) space can be embedded in the d
(resp. d(cid:48)) dimensional Euclidian space: S ⊂ [0  D]d and A ⊂ [0  D(cid:48)]d(cid:48)
. We consider MDPs whose
transition kernels and average rewards are Lipschitz w.r.t. the states and actions. Speciﬁcally  let
L  L(cid:48) > 0  α  α(cid:48) > 0  and

Φ = {ψ : pψ(·|x  a) ∈ P(S)  qψ(·|x  a) ∈ P([0  1]) : (L1)-(L2) hold ∀(x  a)} 

where

(L1)

+

(L2)

(cid:88)

(cid:107)p1 − p2(cid:107)1 =(cid:80)
y∈S |p1(y) − p2(y)|.
(cid:88)


(cid:16) δmin

(cid:17)1/α

Slip := min

a /∈O(x φ)

η(x  a)

x∈S

D

d

(cid:32)(cid:20) δ∗(x(cid:48)  a(cid:48); φ)

H + 1

(cid:107)pψ(·|x  a) − pψ(·|x(cid:48)  a(cid:48))(cid:107)1 ≤ Ld(x  x(cid:48))α + L(cid:48)d(a  a(cid:48))α(cid:48)
|rψ(x  a) − rψ(x(cid:48)  a(cid:48))| ≤ Ld(x  x(cid:48))α + L(cid:48)d(a  a(cid:48))α(cid:48)

.

 

Here d(· ·) is the Euclidean distance  and for two distributions p1 and p2 on S we denote by
Theorem 3. For the model Φ with Lipschitz structure (L1)-(L2)  we have Flip(φ) ⊂ FΦ(φ) where
Flip(φ) is the set of η ∈ ¯RS×A

satisfying for all (x(cid:48)  a(cid:48)) such that a(cid:48) /∈ O(x(cid:48)  φ) 

(cid:16)

Ld(x  x(cid:48))α + L(cid:48)d(a  a(cid:48))α(cid:48)(cid:17)(cid:21)

− 2

(cid:33)2

+

≥ 2

(3)

where we use the notation [u]+ := max{0  u} for u ∈ R. Furthermore  the optimal values KΦ(φ)
and Klip(φ) of P (φ FΦ(φ)) and P (φ Flip(φ)) are upper bounded by 8 (H+1)3
SlipAlip where
D(cid:48)√
(cid:16) δmin

d   and Alip := min

d(cid:48) .

(cid:17)1/α(cid:48) + 1

A 

S 



8L(cid:48)(H+1)

8L(H+1)

+ 1

δ2
min

√

d(cid:48)

The above theorem has important consequences. First  it states that exploiting the Lipschitz structure
optimally  one may achieve a regret at most scaling as (H+1)3
SlipAlip log T . This scaling is inde-
pendent of the sizes of the state and action spaces provided that the minimal gap δmin is ﬁxed  and
provided that the span H does not scale with S. The latter condition typically holds for fast mixing
models or for MDPs with diameter not scaling with S (refer to [Bartlett and Tewari  2009] for a
precise connection between H and the diameter). Hence  exploiting the structure can really yield
signiﬁcant regret improvements. As shown in the next section  leveraging the simpliﬁed structure
in Flip(φ)  we may devise a simple algorithm achieving these improvements  i.e.  having a regret
scaling at most as Klip(φ) log T .

δ2
min

6

Algorithm 1 DEL(γ)
input Model structure Φ

Initialize N1(x) ← 1[x = X1]  N1(x  a) ← 0  s1(x) ← 0  p1(y | x  a) ← 1/|S|  r1(x  a) ← 0
for each x  y ∈ S  a ∈ A  and φ1 accordingly.
for t = 1  ...  T do

t := φt(Ct)  h(cid:48)

t(x) :=

t

1

For each x ∈ S  let Ct(x) := {a ∈ A : Nt(x  a) ≥ log2 Nt(x)}  φ(cid:48)
h∗
(x)  ζt :=
φ(cid:48)
if ∀a ∈ O(x; φ(cid:48)
Monotonize: At ← Amnt
else if ∃a ∈ A s.t. Nt(Xt  a) < log Nt(Xt)

1+log log t and γt := (1 + γ)(1 + log t)
t)  Nt(Xt  a) < log2 Nt(Xt) + 1 then

:= arg mina∈O(x;φ(cid:48)

t) Nt(Xt  a).

t

(cid:16) Nt(x a)

γt

1+log log Nt(Xt) then
:= arg mina∈A Nt(Xt  a).

: (x  a) ∈ S × A(cid:17) ∈ FΦ(φt;Ct  ζt). then

t

Estimate: At ← Aest

t

t) Nt(Xt  a).

:= arg mina∈O(x;φ(cid:48)

Exploit: At ← Axpt
For each (x  a) ∈ S × A  let δt(x  a) := δ∗(x  a; φt Ct  ζt).
if Ft := FΦ(φt;Ct  ζt) ∩ {η : η(x  a) = ∞ if δt(x  a) = 0} = ∅ then
else

Let ηt(x  a) = ∞ if δt(x  a) = 0 and ηt(x  a) = 0 otherwise.
Obtain a solution ηt of P(δt Ft): inf η∈Ft

(cid:80)

(x a)∈S×A η(x  a)δt(x  a)

else if

else

end if
Explore: At ← Axpr

t

:= arg mina∈A:Nt(Xt a)≤ηt(Xt a)γt Nt(Xt  a).

end if
Select action At  and observe the next state Xt+1 and the instantaneous reward Rt.
Update φt+1  Nt+1(x) and Nt+1(x  a) for each (x  a) ∈ S × A.

end for

5 Algorithms

In this section  we present DEL (Directed Exploration Learning)  an algorithm that achieves the regret
limits identiﬁed in the previous section. Asymptotically optimal algorithms for generic controlled
Markov chains have already been proposed in [Graves and Lai  1997]  and could be adapted to our
setting. By presenting DEL  we aim at providing simpliﬁed  yet optimal algorithms. Moreover  DEL
can be adapted so that the exploration rates of sub-optimal actions are directed towards the solution of
an optimization problem P (φ F(φ)) provided that F(φ) ⊂ FΦ(φ) (it sufﬁces to use F(φt) instead
of FΦ(φt) in DEL). For example  in the case of Lipschitz structure Φ  running DEL on Flip(φ) yields
a regret scaling at most as (H+1)3

SlipAlip log T .

δ2
min

The pseudo-code of DEL with input parameter γ > 0 is given in Algorithm 1. There  for notational
convenience  we abuse the notations and redeﬁne log t as 1[t ≥ 1] log t  and let ∞ · 0 = 0. φt refers
to the estimated MDP at time t (using empirical transition rates and rewards). For any non-empty
correspondence C : S (cid:16) A (i.e.  for any x  C(x) is a non-empty subset of A)  let φ(C) denote the
restricted MDP where the set of actions available at state x is C(x). Then  g∗
φ(C) are the
(optimal) gain and bias functions corresponding to the restricted MDP φ(C). Given a restriction
deﬁned by C  for each (x  a) ∈ S × A  let δ∗(x  a; φ C) := (B∗
φ(C))(x) and
φ(C)(y). For ζ ≥ 0  let δ∗(x  a; φ C  ζ) := 0 if δ∗(x  a; φ C) ≤ ζ 
Hφ(C) := maxx y∈S h∗
and let δ∗(x  a; φ C  ζ) := δ∗(x  a; φ C) otherwise. For ζ ≥ 0  we further deﬁne the set of confusing
MDPs ∆Φ(φ;C  ζ)  and the set of feasible solutions FΦ(φ;C  ζ) as:

φ(C) and h∗
φh∗

φ(C))(x) − (Ba

φ(C)h∗

∆Φ(φ;C  ζ) :=

ψ ∈ Φ ∪ {φ} : φ (cid:28) ψ;

(i) KLφ|ψ(x  a) = 0 ∀x ∀a ∈ O(x; φ(C));
(ii) ∃(x  a) ∈ S × A s.t.

a /∈ O(x; φ(C)) and δ∗(x  a; ψ C  ζ) = 0

(cid:41)

(cid:41)

FΦ(φ;C  ζ) :=

η(x  a)KLφ|ψ(x  a) ≥ 1  ∀ψ ∈ ∆Φ(φ;C  ζ)

.

φ(C)(x) − h∗
(cid:40)
(cid:40)

η ∈ ¯RS×A

+

(cid:88)

(cid:88)

x∈S

a∈A

:

7

Similar sets Fun(φ;C  ζ) and Flip(φ;C  ζ) can be deﬁned for the cases of unstructured and Lipschitz
MDPs (refer to the supplementary material)  and DEL can be simpliﬁed in these cases by replac-
ing FΦ(φ;C  ζ) by Fun(φ;C  ζ) or Flip(φ;C  ζ) in the pseudo-code. Finally  P(δ F) refers to the

optimization problem inf η∈F(cid:80)

(x a)∈S×A η(x  a)δ(x  a).

DEL combines the ideas behind OSSB [Combes et al.  2017]  an asymptotically optimal algorithm for
structured bandits  and the asymptotically optimal algorithm presented in [Burnetas and Katehakis 
1997] for RL problems without structure. DEL design aims at exploring sub-optimal actions no more
than what the regret lower bound prescribes. To this aim  it essentially solves in each iteration t an
optimization problem close to P (φt FΦ(φt)) where φt is an estimate of the true MDP φ. Depending
on the solution and the number of times apparently sub-optimal actions have been played  DEL
decides to explore or exploit. The estimation phase ensures that certainty equivalence holds. The
"monotonization" phase together with the restriction to relatively well selected actions were already
proposed in [Burnetas and Katehakis  1997] to make sure that accurately estimated actions only are
selected in the exploitation phase. The various details and complications introduced in DEL ensure
that its regret analysis can be conducted. In practice (see the supplementary material)  our initial
experiments suggest that many details can be removed without large regret penalties.
Theorem 4. For a structure Φ with Bernoulli rewards and for any ergodic MDP φ ∈ Φ  assume
that: (i) φ is in the interior of Φ (i.e.  there exists a constant ζ0 > 0 such that for any ζ ∈ (0  ζ0) 
ψ ∈ Φ if (cid:107)φ − ψ(cid:107) ≤ ζ and ψ (cid:28) φ)  (ii) the solution η∗(φ) is uniquely deﬁned for each (x  a) such
that a /∈ O(x; φ)  (iii) continuous at φ (i.e.  for any given ε > 0  there exists ζ(ε) > 0 such that
for all ζ ∈ (0  ζ(ε))  if (cid:107)ψ − φ(cid:107) ≤ ζ  max(x a):a /∈O(x;φ) |η∗(x  a; ψ  ζ) − η∗(x  a; φ)| ≤ ε where
η∗(ψ  ζ) is solution of P(δ∗(ψ A  ζ) FΦ(ψ;A  ζ))  and η∗(x  a; φ) that of P (φ FΦ(φ))). Then  for
π = DEL(γ) with any γ > 0  we have:

lim sup
T→∞

Rπ
T (φ)
log T

≤ (1 + γ)KΦ(φ) .

(4)

For Lipschitz Φ with (L1)-(L2) (resp.
unstructured Φ)  if π = DEL uses in each step t 
Flip(φt;Ct  ζt) (resp. Fun(φt;Ct  ζt)) instead of FΦ(φt;Ct  ζt)  its regret is asymptotically smaller
than (1 + γ)Klip(φ) log T (resp. (1 + γ)Kun(φ) log T ).
In the above theorem  the assumptions about the uniqueness and continuity of the solution η∗(φ)
could be veriﬁed for particular structures. In particular  we believe that they generally hold in the
case of unstructured and Lipschitz MDPs. Also note that similar assumptions have been made in
[Graves and Lai  1997].

6 Extensions and Future Work

It is worth extending the approach developed in this paper to the case of structured discounted RL
problems (although for such problems  there is no ideal way of deﬁning the regret of an algorithm).
There are other extensions worth investigating. For example  since our framework allows any kind of
structure  we may specify our regret lower bounds for structures stronger than that corresponding
to Lipschitz continuity  e.g.  the reward may exhibit some kind of unimodality or convexity. Under
such structures  the regret improvements might become even more signiﬁcant. Another interesting
direction consists in generalizing the results to the case of communicating MDPs. This would allow
us for example to consider deterministic system dynamics and unknown probabilistic rewards.

7 Acknowledgements

This work was partially supported by the Wallenberg AI  Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation. Jungseul Ok is now with UIUC in
Prof. Sewoong Oh’s group. He would like to thank UIUC for ﬁnancially supporting his participation
to NIPS 2018 conference.

8

References
Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. In

Advances in Neural Information Processing Systems 31  2017.

Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. In

Advances in Neural Information Processing Systems 19  2007.

Peter Auer  Thomas Jaksch  and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In

Advances in Neural Information Processing Systems 22  2009.

Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning in
weakly communicating MDPs. In Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence 
2009.

Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision processes.

Mathematics of Operations Research  22(1):222–255  1997.

Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In

Proceedings of the 31st International Conference on Machine Learning  2014.

Richard Combes  Stefan Magureanu  and Alexandre Proutiere. Minimal exploration in structured stochastic

bandits. In Advances in Neural Information Processing Systems 30  2017.

Sarah Filippi  Olivier Cappé  and Aurélien Garivier. Optimism in reinforcement learning and Kullback-Leibler

divergence. In 48th Annual Allerton Conference on Communication  Control  and Computing  2010.

Aurélien Garivier  Pierre Ménard  and Gilles Stoltz. Explore ﬁrst  exploit next: The true shape of regret in bandit

problems. Mathematics of Operations Research  Jun. 2018.

Todd L. Graves and Tze Leung Lai. Asymptotically efﬁcient adaptive choice of control laws in controlled

Markov chains. SIAM J. Control and Optimization  35(3):715–743  1997.

Emilie Kaufmann  Olivier Cappé  and Aurélien Garivier. On the complexity of best-arm identiﬁcation in

multi-armed bandit models. The Journal of Machine Learning Research  17(1):1–42  2016.

Kailasam Lakshmanan  Ronald Ortner  and Daniil Ryabko. Improved regret bounds for undiscounted continuous

reinforcement learning. In 32nd International Conference on Machine Learning  2015.

Stefan Magureanu  Richard Combes  and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and

optimal algorithms. In Conference on Learning Theory  2014.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G. Bellemare  Alex
Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Petersen  Charles Beattie  Amir Sadik 
Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra  Shane Legg  and Demis Hassabis.
Human-level control through deep reinforcement learning. Nature  518:529–533  2015.

Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning. In

Advances in Neural Information Processing Systems 25  2012.

Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder dimension. In Advances

in Neural Information Processing Systems 27  2014.

Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv preprint

arXiv:1608.02732  2016.

Ambuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible

MDPs. In Advances in Neural Information Processing Systems 20  2008.

9

,Jungseul Ok
Alexandre Proutiere
Damianos Tranos