2009,Optimal Scoring for Unsupervised Learning,We are often interested in casting classification and clustering problems in a regression framework  because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria. In this paper we illustrate optimal scoring  which was originally proposed for performing Fisher linear discriminant analysis by regression  in the application of unsupervised learning. In particular  we devise a novel clustering algorithm that we call optimal discriminant clustering (ODC). We associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering  discriminative clustering and sparse principal component analysis. Thus  our work shows that optimal scoring  provides a new approach to the implementation of  unsupervised learning. This approach facilitates the development of new unsupervised learning algorithms.,Optimal Scoring for Unsupervised Learning

Zhihua Zhang and Guang Dai

College of Computer Science & Technology

Zhejiang University

Hangzhou  Zhejiang  310027 China

Abstract

We are often interested in casting classiﬁcation and clustering problems as a re-
gression framework  because it is feasible to achieve some statistical properties
in this framework by imposing some penalty criteria. In this paper we illustrate
optimal scoring  which was originally proposed for performing the Fisher linear
discriminant analysis by regression  in the application of unsupervised learning. In
particular  we devise a novel clustering algorithm that we call optimal discriminant
clustering. We associate our algorithm with the existing unsupervised learning al-
gorithms such as spectral clustering  discriminative clustering and sparse principal
component analysis. Experimental results on a collection of benchmark datasets
validate the effectiveness of the optimal discriminant clustering algorithm.

1 Introduction
The Fisher linear discriminant analysis (LDA) is a classical method that considers dimensionality re-
duction and classiﬁcation jointly. LDA estimates a low-dimensional discriminative space deﬁned by
linear transformations through maximizing the ratio of between-class scatter to within-class scatter.
It is well known that LDA is equivalent to a least mean squared error procedure in the binary classi-
ﬁcation problem [4]. It is of great interest to obtain a similar relationship in multi-class problems. A
signiﬁcant literature has emerged to address this issue [6  8  12  14]. This provides another approach
to performing LDA by regression  in which penalty criteria are tractably introduced to achieve some
statistical properties such as regularized LDA [5] and sparse discriminant analysis [2].
It is also desirable to explore unsupervised learning problems in a regression framework. Recently 
Zou et al. [17] reformulated principal component analysis (PCA) as a regression problem and then
devised a sparse PCA by imposing the lasso (the elastic net) penalty [10  16] on the regression
vector. In this paper we consider unsupervised learning problems by optimal scoring  which was
originally proposed to perform LDA by regression [6]. In particular  we devise a novel unsupervised
framework by using the optimal scoring and the ridge penalty.
This framework can be used for dimensionality reduction and clustering simultaneously. We are
mainly concerned with the application in clustering. In particular  we propose a clustering algorithm
that we called optimal discriminant clustering (ODC). Moreover  we establish a connection of our
clustering algorithm with discriminative clustering algorithms [3  13] and spectral clustering algo-
rithms [7  15]. This implies that we can cast these clustering algorithms as regression-type problems.
In turn  this facilitates the introduction of penalty terms such as the lasso and elastic net so that we
have sparse unsupervised learning algorithms.
Throughout this paper  Im denotes the m×m identity matrix  1m the m×1 vector of ones  0 the zero
vector or matrix with appropriate size  and Hm = Im − 1
m the m×m centering matrix. For
an m×1 vector a = (a1  . . .   am)(cid:48)  diag(a) represents the m×m diagonal matrix with a1  . . .   am
as its diagonal entries. For an m×m matrix A = [aij]  we let A+ be the Moore-Penrose inverse
of A  tr(A) be the trace of A  rk(A) be the rank of A and (cid:107)A(cid:107)F =
tr(A(cid:48)A) be the Frobenius
norm of A.

m 1m1(cid:48)

(cid:112)

1

2 Problem Formulation

(cid:80)c

We are concerned with a multi-class classiﬁcation problem. Given a set of n p-dimensional data
points  {x1  . . .   xn} ∈ X ⊂ Rp  we assume that the xi are grouped into c disjoint classes and that
each xi belongs to one class. Let V = {1  2  . . .   n} denote the index set of the data points xi and
partition V into c disjoint subsets Vj; i.e.  Vi ∩ Vj = ∅ for i (cid:54)= j and ∪c
j=1Vj = V   where the
cardinality of Vj is nj so that
We also make use of a matrix representation for the problem in question. In particular  we let X =
[x1  . . .   xn](cid:48) be an n×p data matrix  and E = [eij] be an n×c indicator matrix with eij = 1 if input
xi is in class j and eij = 0 otherwise. Let Π = diag(n1  . . .   nc)  Π 1
nc) 
cΠ = π(cid:48)  E1c = 1n 
π = (n1  . . .   nc)(cid:48) and
cπ = n  E(cid:48)E = Π and Π−1π = 1c.
1(cid:48)

nc)(cid:48). It follows that 1(cid:48)

√
2 = diag(

j=1 nj = n.

nE = 1(cid:48)

n1  . . .  

n1  . . .  

π = (

√

√

√

√

2.1 Scoring Matrices

Hastie et al. [6] deﬁned a scoring matrix for the c-class classiﬁcation problem. That is  it is such
a c×(c−1) matrix Θ ∈ Rc×(c−1) that Θ(cid:48)(E(cid:48)E)Θ = Θ(cid:48)ΠΘ = Ic−1. The jth row of Θ deﬁnes a
scoring or scaling for the jth class. Here we reﬁne this deﬁnition as:

(cid:181)

(cid:179)√

Deﬁnition 1 Given a c-class classiﬁcation problem with the cardinality of the jth class being nj  a
c×(c−1) matrix Θ is referred to as the class scoring matrix if it satisﬁes

Θ(cid:48)ΠΘ = Ic−1

and π(cid:48)Θ = 0.
It follows from this deﬁnition that ΘΘ(cid:48) = Π−1− 1
n 1c1(cid:48)
c. In the literature [15]  the authors presented
(cid:48)
a speciﬁc example for Θ = (θ1  . . .   θc−1)(cid:48). That is  θ
1 =

(cid:113)(cid:80)c
j=l nj
j=l nj
 − √
for l = 2  . . .   c−1. Especially  when c = 2  Θ = (
n1√
nn2
Let Y = EΘ (n×(c−1)). We then have Y(cid:48)Y = Ic−1 and 1(cid:48)
nY = 0. To address an unsupervised
clustering problem with c classes  we relax the setting of Y = EΘ and give the following deﬁnition.
Deﬁnition 2 An n×(c−1) matrix Y is referred to as the sample scoring matrix if it satisﬁes

j=l+1 nj
)(cid:48) is a 2-dimensional vector.

(cid:113)(cid:80)c
(cid:113)
(cid:80)c

 − √
n1√
n(n−n1)

n−n1√
√

(cid:80)c

√
n2√
nn1

j=l+1 nj

0 ∗ 1(cid:48)

1(cid:48)
c−1

1(cid:48)
c−l

(cid:182)

(cid:180)

(cid:48)
l =

θ

nn1

nl

l−1 

and

nl

 

Y(cid:48)Y = Ic−1

and

1(cid:48)
nY = 0.

Note that c does not necessarily represent the number of classes in this deﬁnition. For example  we
view c−1 as the dimension of a reduced dimensional space in the dimensionality reduction problem.

2.2 Optimal Scoring for LDA

To devise a classiﬁer for the c-class classiﬁcation problem  we consider a penalized optimal scoring
model  which is deﬁned by

(cid:111)

(cid:110)
f(Θ  W) (cid:44) 1
2

(cid:107)EΘ − HnXW(cid:107)2

F + σ2
2

tr(W(cid:48)W)

min
Θ  W

(1)
under the constraints Θ(cid:48)ΠΘ = Ic−1 and π(cid:48)Θ = 0 where Θ ∈ Rc×(c−1) and W ∈ Rp×(c−1).
Compared with the setting in [6]  we add the constraint π(cid:48)Θ = 0. The reason is due to
1(cid:48)
nHnXW = 0. We thus impose 1(cid:48)
Denote

nEΘ = π(cid:48)Θ = 0 for consistency.

R = Π− 1

2 E(cid:48)HnX(X(cid:48)HnX + σ2Ip)−1X(cid:48)HnEΠ− 1
2 .

1

2 = 0  there exists a c×(c−1) orthogonal matrix ∆  the columns of which are the eigen-

Since Rπ
vectors of R. That is  ∆ satisﬁes ∆(cid:48)∆ = Ic−1 and ∆(cid:48)π

1

2 = 0.

2

Theorem 1 A minimizer of Problem (1) is ˆΘ = Π− 1
Here [∆  1√

2 ] is the c×c matrix of the orthonormal eigenvectors of R.

2 ∆ and ˆW = (X(cid:48)HnX + σ2Ip)−1X(cid:48)HnE ˆΘ.

1

n π

Since for an arbitrary class scoring matrix Θ  its rank is c−1  we have Θ = ˆΘΥ where Υ is
some (c−1)×(c−1) orthonormal matrix. Moreover  it follows from ΘΘ(cid:48) = Π−1 − 1
c that the
between-class scatter matrix is given by

n 1c1(cid:48)

Σb = X(cid:48)HnEΘΘ(cid:48)E(cid:48)HnX = X(cid:48)HnE ˆΘ ˆΘ

(cid:48)

E(cid:48)HnX.

Accordingly  we can also write the generalized eigenproblem for the penalized LDA as

X(cid:48)HnE ˆΘ ˆΘ

(cid:48)

E(cid:48)HnXA = (X(cid:48)HnX + σ2Ip)AΛ 

because the total scatter matrix Σ is Σ = X(cid:48)HnX. We now obtain

It is well known that ˆW ˆΘ
over  ˆΘ
between A in the penalized LDA and W in the penalized optimal scoring model (1).

E(cid:48)HnXA is the eigenvector matrix of ˆΘ

E(cid:48)HnXA = AΛ.
(cid:48)
E(cid:48)HnX ˆW have the same nonzero eigenvalues. More-
E(cid:48)HnX ˆW. We thus establish the relationship

ˆW ˆΘ
E(cid:48)HnX and ˆΘ

(cid:48)

(cid:48)

(cid:48)

(cid:48)

3 Optimal Scoring for Unsupervised Learning

In this section we extend the notion of optimal scoring to unsupervised learning problems  leading
to a new framework for dimensionality reduction and clustering analysis simultaneously.

3.1 Framework

(cid:111)

(cid:110)
f(Y  W) (cid:44) 1
2

tr(W(cid:48)W)

In particular  we relax EΘ in (1) as a sample scoring matrix Y and deﬁne the following penalized
model:

(cid:107)Y − HnXW(cid:107)2

F + σ2
2

(2)
nY = 0 and Y(cid:48)Y = Ic−1. The following theorem provides a solution for

min
Y  W
under the constraints 1(cid:48)
this problem.
Theorem 2 A minimizer of Problem (2) is ˆY and ˆW = (X(cid:48)HnX + σ2Ip)−1X(cid:48)Hn ˆY  where ˆY is
the n×(c−1) orthogonal matrix of the top eigenvectors of HnX(X(cid:48)HnX + σ2Ip)−1X(cid:48)Hn.
The proof is given in Appendix A. Note that all the eigenvalues of HnX(X(cid:48)HnX + σ2Ip)−1X(cid:48)Hn
are between 0 and 1. Especially  when σ2 = 0  the eigenvalues are either 1 or 0.
In this
case  if rk(HnX) ≥ c−1  f( ˆY  ˆW) achieves its minimum 0  otherwise the minimum value is
c−1−rk(HnX)

.

2

With the estimates of Y and W  we can develop an unsupervised learning procedure. It is clear that
W can be treated as a non-orthogonal projection matrix and HnXW is then the low-dimensional
conﬁguration of X. Using this treatment  we obtain a new alternative to the regression formulation of
PCA by Zou et al. [17]. In this paper  however  we concentrate on the application of the framework
in clustering analysis.

3.2 Optimal Discriminant Clustering

Our clustering procedure is given in Algorithm 1. We refer to this procedure as optimal discriminant
clustering due to its relationship with LDA  which is shown by the connection between (1) and (2).
Assume that ˜X = [˜x1  . . .   ˜xn](cid:48) (n×r) is a feature matrix corresponding to the data matrix X. In
this case  we have

S = Hn ˜X( ˜X(cid:48)Hn ˜X + σ2Ir)−1 ˜X(cid:48)Hn = C(C + σ2In)−1 

3

where C = Hn ˜X ˜X(cid:48)Hn is the n×n centered kernel matrix. This implies that we can obtain ˆY
without the explicit use of the feature matrix ˜X. Moreover  we can compute Z by

Z = Hn ˜X( ˜X(cid:48)Hn ˜X + σ2Ir)−1 ˜X(cid:48)HnY = SY.

We are thus able to devise this clustering algorithm by using the reproducing kernel k(· ·) :
X×X → R such that K(xi  xj) = ˜x(cid:48)

i˜xj and K = ˜X ˜X(cid:48).

Algorithm 1 Optimal Discriminant Clustering Algorithm
1: procedure ODC(HnX  c  σ2)
2:
3:
4:
5:
6: end procedure

Estimate ˆY and ˆW according to Theorem 2;
Calculate Z = [z1  . . .   zn](cid:48) = HnX ˆW;
Perform K-means on the zi;
Return the partition of the zi as the partition of the xi.

3.3 Related Work

We now explore the connection of the optimal discriminant clustering with the discriminative clus-
tering algorithm [3] and spectral clustering [7]. Recall that ˆY is the matrix of the c−1 top eigenvec-
tors of C(C + σ2In)−1. Consider that if λ (cid:54)= 0 is an eigenvalue of C with associated eigenvector
u  then λ/(λ + σ2) ((cid:54)= 0) is an eigenvalue of C(C + σ2In)−1 with associated eigenvector u. More-
over  λ/(λ + σ2) is increasing as λ increases. This implies that ˆY is also the matrix of the c−1 top
eigenvectors of C. As we know  the spectral clustering applies a rounding scheme such as K-means
directly on ˆY. We thus have a relationship between the spectral clustering and optimal discriminant
clustering.
We study the relationship between the discriminative clustering algorithm and the spectral cluster-
ing algorithm. Let M be a linear transformation from the r-dimensional ˜X to an s-dimensional
transformed feature space F  namely

where M is an r×s matrix of rank s (s < r). The corresponding scatter matrices in the F-space
are thus given by M(cid:48)ΣM and M(cid:48)ΣbM. The discriminative clustering algorithm [3  13] in the
reproducing kernel Hilbert space (RKHS) tries to solve the problem of

F = ˜XM 

argmax

E  M

f(E  M) (cid:44) tr((M(cid:48)(Σ+σ2Ir)M)−1M(cid:48)ΣbM)

= tr

(M(cid:48)( ˜X(cid:48)Hn ˜X+σ2Ir)M)−1M(cid:48) ˜X(cid:48)HnE

(cid:161)

(cid:161)

(cid:162)−1E(cid:48)Hn ˜XM
(cid:162)

E(cid:48)E

Applying the discussion in [15] to Hn ˜XM(M(cid:48)( ˜X(cid:48)Hn ˜X+σ2Ir)M)−1M(cid:48) ˜X(cid:48)Hn  we have the fol-
lowing relaxation problem

max Y∈Rn×(c−1) M∈Rr×s tr(Y(cid:48)Hn ˜XM(M(cid:48)( ˜X(cid:48)Hn ˜X+σ2Ir)M)−1M(cid:48) ˜X(cid:48)HnY) 
s.t. Y(cid:48)Y = Ic−1 and Y(cid:48)1n = 0.

(3)

Express M = ˜X(cid:48)HnB + N where N satisﬁes N(cid:48) ˜X(cid:48)Hn = 0 (i.e.  N ∈ span{ ˜X(cid:48)Hn}⊥) and B is
some n×s matrix. Under the condition of either σ2 = 0 or N = 0 (i.e.  M ∈ span{ ˜X(cid:48)Hn})  we
can obtain that

Hn ˜XM(M(cid:48)( ˜X(cid:48)Hn ˜X+σ2Ir)M)−1M(cid:48) ˜X(cid:48)Hn = CB(B(cid:48)(CC + σ2C)B)−1B(cid:48)C.

Again consider that if λ (cid:54)= 0 is an eigenvalue of C with associated eigenvector u  then λ/(λ+σ2) (cid:54)=
0 is an eigenvalue of C(CC + σ2C)+C with associated eigenvector u. Moreover  λ/(λ + σ2) is
increasing in λ. We now directly obtain the following theorem from Theorem 3.1 in [13].
Theorem 3 Let Y∗ and M∗ be the solution of Problem (3). Then

4

Table 1: Summary of the benchmark datasets  where c is the number of classes  p is the dimension
of the input vector  and n is the number of samples in the dataset.

Types

Face

Gene

UCI

Dataset
ORL
Yale
PIE

SRBCT

Iris
Yeast

Image segmentation

Statlog landsat satellite

c
40
15
68
4
4
10
7
7

p

1024
1024
1024
2308

4
8
19
36

n
400
165
6800
63
150
1484
2100
2000

(i) If σ2 = 0  Y∗ is the solution of the following problem

argmaxY∈Rn×(c−1) tr(Y(cid:48)CC+Y) 
s.t. Y(cid:48)Y = Ic−1 and Y(cid:48)1n = 0.

(ii) If M ∈ span{ ˜X(cid:48)Hn}  Y∗ is the solution of the following problem:

argmaxY ∈Rn×(c−1) tr(Y(cid:48)CY) 
s.t. Y(cid:48)Y = Ic−1 and Y(cid:48)1n = 0.

Theorem 3 shows that discriminative clustering is essentially equivalent to spectral clustering. This
further leads us to a relationship between the discriminative clustering and optimal discriminant
clustering from the relationship between the spectral clustering and optimal discriminant clustering.
In summary  we are able to unify the discriminative clustering as well as spectral clustering into the
optimal scoring framework in (2).

4 Experimental Study

To evaluate the performance of our optimal discriminant clustering (ODC) algorithm  we conducted
experimental comparisons with other related clustering algorithms on several real-world datasets. In
particular  the comparison was implemented on three face datasets  the “SRBCT” gene dataset  and
four UCI datasets. Further details of these datasets are summarized in Table 1.
To effectively evaluate the performance  we employed two typical measurements: the Normalized
Mutual Information (NMI) and the Clustering Error (CE). It should be mentioned that for NMI  the
larger this value  the better the performance. For CE  the smaller the value  the better the perfor-
mance. More details and the corresponding implementations for both can be found in [11].
In the experiments  we compared our ODC with four different clustering algorithms  i.e.  the
conventional K-means [1]  normalized cut (NC) [9]  DisCluster [3] and DisKmeans [13].
It is
worth noting that two discriminative clustering algorithms: DisCluster [3] and DisKmeans [13] 
are very closely related to our ODC  because they are derived from the discriminant anal-
ysis criteria in essence (also see the analysis in Section 3.3).
the implemen-
tation code for NC is available at http://www.cis.upenn.edu/∼jshi/software/.
the parameter σ2 in ODC is sought from the range σ2 ∈
For the sake of simplicity 
{10−3  10−2.5  10−2  10−1.5  10−1  10−0.5  100  100.5  101  101.5  102  102.5  103}. Similarly  the
parameters in other clustering algorithms compared here are also searched in a wide range.
For simplicity  we just reported the best results of clustering algorithms with respect to different
parameters on each dataset. Table 2 summaries the NMI and CE on all datasets. According to the
NMI values in Table 2  our ODC outperforms other clustering algorithms on ﬁve datasets: ORL 
SRBCT  iris  yeast and image segmentation. According to the CE values in Table 2  it
is obvious that the performance of our ODC is best in comparison with other algorithms on all the
datasets  and NC and DisKmeans algorithms can achieve the almost same performance with ODC
on the SRBCT and iris datasets respectively. Also  it is seen that the DisCluster algorithm has
dramatically different performance based on the NMI and CE. The main reason is that the ﬁnal
solution in DisCluster is very sensitive to the initial variables and numerical computation.

In addition 

5

(a)

(e)

(b)

(f)

(c)

(g)

(d)

(h)

Figure 1: The NMI versus the parameter σ tuning in ODC on all datasets  where the NMI of K-
means is used as the baseline: (a) ORL; (b) Yale; (c) PIE; (d) SRBCT; (e) iris; (f) yeast; (g)
image segmentation; (h) statlog landsat satellite.

In order to reveal the effect of the parameter σ on ODC  Figures 1 and 2 depict the NMI and CE
results of ODC with respect to different parameters σ on all datasets. Similar to [11  13]  we used the
results of K-means as a baseline. From Figures 1 and 2  we can see that similar to the conventional
clustering algorithms (including the compared algorithms)  the parameter σ has a signiﬁcant impact
on the performance of ODC  especially when the evaluation results are measured by NMI. In contrast
to the result in Figure 1  the effect of the parameter σ becomes less pronounced in Figure 2.

Table 2: Clustering results: the Normalized Mutual Information (NMI) and the Clustering Error
(CE) (%) of all clustering algorithms are calculated on different datasets.

Measure

NMI

CE (%)

Dataset
ORL
Yale
PIE

SRBCT

Iris
Yeast

Image segmentation

Statlog landsat satellite

ORL
Yale
PIE

SRBCT

Iris
Yeast

Image segmentation

Statlog landsat satellite

K-means
0.7971
0.6237
0.1140
0.2509
0.6595
0.2968
0.5830
0.6126
38.25
45.45
79.82
55.55
16.66
59.43
45.14
32.30

NC

0.8015
0.6203
0.2232
0.3722
0.6876
0.2915
0.5500
0.6316
34.50
46.06
79.82
47.61
15.33
59.90
49.47
32.65

DisCluster DisKmeans

0.7978
0.5974
0.1940
0.3216
0.7248
0.2993
0.5700
0.6152
38.75
45.45
77.35
50.79
12.66
59.43
45.95
32.25

0.8531
0.5641
0.3360
0.2683
0.7353
0.3020
0.5934
0.6009
29.00
45.45
66.23
53.96
11.33
57.07
41.66
31.20

ODC
0.8567
0.5766
0.3035
0.3966
0.7353
0.3041
0.5942
0.6166
28.50
44.84
65.52
47.61
11.33
56.73
40.23
30.50

5 Concluding Remarks

In this paper we have proposed a regression framework to deal with unsupervised dimensionality
reduction and clustering simultaneously. The framework is based on the optimal scoring and ridge
penalty. In particular  we have developed a new clustering algorithm which is called optimal discrim-
inant clustering (ODC). ODC can efﬁciently identify the optimal solution and it has an underlying
relationship with the discriminative clustering and spectral clustering.

6

−6−4−202460.680.70.720.740.760.780.80.820.840.860.882log(σ)NMI  K−meansODC−6−4−202460.50.520.540.560.580.60.620.642log(σ)NMI  K−meansODC−6−4−202460.050.10.150.20.250.30.350.42log(σ)NMI  K−meansODC−6−4−202460.20.250.30.350.42log(σ)NMI  K−meansODC−6−4−202460.60.620.640.660.680.70.720.742log(σ)NMI  K−meansODC−6−4−202460.2550.260.2650.270.2750.280.2850.290.2950.30.3052log(σ)NMI  K−meansODC−6−4−202460.540.550.560.570.580.590.62log(σ)NMI  K−meansODC−6−4−202460.570.580.590.60.610.620.632log(σ)NMI  K−meansODC(a)

(e)

(b)

(f)

(c)

(g)

(d)

(h)

Figure 2: The CE (%) versus the parameter σ tuning in ODC on all datasets  where the CE (%) of
K-means is used as the baseline: (a) ORL; (b) Yale; (c) PIE; (d) SRBCT; (e) iris; (f) yeast;
(g) image segmentation; (h) statlog landsat satellite.

This framework allows us for developing a sparse unsupervised learning algorithm; that is  we alter-
natively consider the following optimization problem:
(cid:107)Y − HnXW(cid:107)2

tr(W(cid:48)W) + λ2(cid:107)W(cid:107)1

min
Y  W

under the constraints 1(cid:48)

1
2

f(Y  W) =
nY = 0 and Y(cid:48)Y = Ic−1. We will study this further.

F + λ1
2

Acknowledgement

This work has been supported in part by program for Changjiang Scholars and Innovative Research
Team in University (IRT0652  PCSIRT)  China.

A Proof of Theorem 2
For simplicity  we replace HnX by X and let q = c−1 in the following derivation. Consider the
Lagrange function:

L(Y  W  B  b)

=

1
2

tr(Y(cid:48)Y) − tr(Y(cid:48)XW) +

tr(B(Y(cid:48)Y−Iq)) − tr(b(cid:48)Y(cid:48)1n) 
where B is a q×q symmetric matrix of Lagrange multipliers and b is a q×1 vector of Lagrange
multipliers. By direct differentiation  it can be shown that

tr(W(cid:48)(X(cid:48)X+σ2Ip)W) − 1
2

1
2

∂L
∂Y
∂L
∂W

= Y − XW − YB − 1nb(cid:48) 
= (X(cid:48)X + σ2Ip)W − X(cid:48)Y.

Letting ∂L

∂Y = 0  we have

Y − XW − YB − 1nb(cid:48) = 0.

Pre-multiplying both sides of the above equation by 1(cid:48)
∂Y = 0 and ∂L

∂W = 0 that

∂L

(cid:189)

n  we obtain b = 0. Thus  it follows from

Y − XW − YB = 0 
W = (X(cid:48)X + σ2Ip)−1X(cid:48)Y.

7

−6−4−20246283032343638402log(σ)CE (%)  K−meansODC−6−4−2024644.844.94545.145.245.345.445.545.6CE (%)2log(σ)  K−meansODC−6−4−20246646668707274767880822log(σ)CE (%)  K−meansODC−6−4−20246474849505152535455562log(σ)CE (%)  K−meansODC−6−4−20246111213141516172log(σ)CE (%)  K−meansODC−6−4−2024656.55757.55858.55959.52log(σ)CE (%)  K−meansODC−6−4−20246404142434445462log(σ)CE (%)  K−meansODC−6−4−2024630.53131.53232.52log(σ)CE (%)  K−meansODCSubstituting the second equation into the ﬁrst equation  we further have
(In − X(X(cid:48)X + σ2Ip)−1X(cid:48))Y = YB.

B where UB is a q×q orthonormal
Now we take the spectral decomposition of B as B = UBΛBU(cid:48)
matrix and ΛB is a q×q diagonal matrix. We thus have (In − X(X(cid:48)X + σ2Ip)−1X(cid:48))YUB =
YUBΛB. This shows that the diagonal entries of ΛB and the columns of YUB are the eigenvalues
and the associated eigenvectors of In − X(X(cid:48)X + σ2Ip)−1X(cid:48).
We consider the case that n ≥ p. Let the SVD of X be X = UΓV(cid:48) where U (n×p) and V (p×p) are
orthogonal  and Γ = diag(γ1  . . .   γp) (p×p) is a diagonal matrix with γ1 ≥ γ2 ≥ ··· ≥ γp ≥ 0. We
then have X(X(cid:48)X + σ2Ip)−1X(cid:48) = UΛU(cid:48)  where Λ = diag(λ1  . . .   λp) with λi = γ2
i + σ2).
There exists such an n×(n−p) orthogonal matrix U3 that its last column is
1√
n 1n and [U  U3]
is an n×n orthonormal matrix. That is  U3 is the eigenvector matrix of X(X(cid:48)X + σ2Ip)−1X(cid:48)
corresponding to the eigenvalue 0. Let U1 be the n×q matrix of the ﬁrst q columns of [U  U3].
We now deﬁne ˆY = U1  ˆW = (X(cid:48)X + σ2Ip)−1X(cid:48)U1  UB = Iq and ΛB = diag(1− λ1  . . .   1−
λq) where λi = 0 whenever i > p. It is easily seen that such a ˆY satisﬁes ˆY(cid:48) ˆY = Iq and ˆY(cid:48)1n = 0
due to U(cid:48)

i /(γ2

1U(cid:48)

1 = Iq and X(cid:48)1n = 0. Moreover  we have
λi = q
2

f( ˆY  ˆW) = q
2

− 1
2

− 1
2

q(cid:88)

i=1

q(cid:88)

i=1

γ2
i
i + σ2
γ2

where γi = 0 whenever i > p. Note that all the eigenvalues of X(X(cid:48)X + σ2Ip)−1X(cid:48) are between
0 and 1. Especially  when σ2 = 0  the eigenvalues are either 1 or 0. In this case  if rk(X) ≥ q 
f( ˆY  ˆW) achieves its minimum 0  otherwise the minimum value is q−rk(X)
To verify that ( ˆY  ˆW) is a minimizer of problem (2)  we consider the Hessian matrix of
L with respect to (Y  W). Let vec(Y(cid:48)) = (y11  . . .   y1q  y21  . . .   ynq)(cid:48) and vec(W(cid:48)) =
(cid:184)
(w11  . . .   w1q  w21  . . .   wpq)(cid:48). The Hessian matrix is then given by

(cid:34)

(cid:35)

(cid:183)

.

2

∂2L

∂2L

∂vec(Y(cid:48))∂vec(Y(cid:48))(cid:48)
∂vec(W(cid:48))∂vec(Y(cid:48))(cid:48)
2]  where C1 and C2 are n×q and p×q  be an arbitrary nonzero (n+p)×q matrix

∂vec(Y(cid:48))∂vec(W(cid:48))(cid:48)
∂vec(W(cid:48))∂vec(W(cid:48))(cid:48)

Iq⊗(X(cid:48)X + σ2Ip)

∂2L

=

.

(Iq−B)⊗In
−Iq⊗X(cid:48)

−Iq⊗X

1  C(cid:48)
1[1n  ˆY] = 0  which is equivalent to C(cid:48)

∂2L

H(Y  W) =
Let C(cid:48) = [C(cid:48)
such that C(cid:48)
If rk(X) ≤ q  we have C(cid:48)
vec(C(cid:48))(cid:48)H( ˆY  ˆW)vec(C(cid:48)) = tr(C(cid:48)
= tr(C(cid:48)

1X = 0. Hence 

1C1(Iq − B)) − 2tr(C(cid:48)
1C1(Iq − B)) + tr(C(cid:48)

1XC2) + tr(C(cid:48)
2(X(cid:48)X + σ2Ip)C2) ≥ 0.

2(X(cid:48)X + σ2Ip)C2)

11n = 0 and C(cid:48)

1U1 = 0.

This implies that ( ˆY  ˆW) is a minimizer of problem (2).
In the case that rk(X) = m > q  we have p > q. Thus we can partition U and V into U = [U1  U2]
and V = [V1  V2] where V1 and V2 are p×q and p×(p−q). Thus 
vec(C(cid:48))(cid:48)H( ˆY  ˆW)vec(C(cid:48)) = tr(C(cid:48)
≥ tr(C(cid:48)
+tr(C(cid:48)
(Λ1/2
+tr(C(cid:48)

1C1(Iq − B)) − 2tr(C(cid:48)
2C1)−2tr(C(cid:48)
1U2Λ2U(cid:48)
3C1Λ1) + tr(C(cid:48)
1U3U(cid:48)
2C1 − D1/2
2 U(cid:48)
1U3U(cid:48)
3C1Λ1) + tr(C(cid:48)

1XC2) + tr(C(cid:48)
1U2Γ2V(cid:48)
2V1D1V(cid:48)
2 U(cid:48)
2V1D1V(cid:48)

2(X(cid:48)X + σ2Ip)C2)
(cid:164)
2C2)

2C2)+tr(C(cid:48)
1C2)
2C1 − D1/2
1C2) ≥ 0.

2C2)(cid:48)(Λ1/2

2V2D2V(cid:48)

2 V(cid:48)

2 V(cid:48)

= tr

2C2)

(cid:163)

Here Λ1 = diag(λ1  . . .   λq)  Λ2 = diag(λq+1  . . .   λp)  Γ1 = diag(γ1  . . .   γq)  Γ2 =
2 Λ1/2
diag(γq+1  . . .   γp)  D1 = Γ2
.
Moreover  we use the fact that
tr(C(cid:48)

2 + σ2Ip−q  so we have Γ2 = D1/2
1U2Λ2U(cid:48)

1 + σ2Iq and D2 = Γ2
2C1Λ1) ≥ tr(C(cid:48)

because λiIq − Λ2 for i = 1  . . .   q are positive semideﬁnite.
If n < p  we also make the SVD of X as X = UΓV(cid:48). But  right now  U is n×n  V is n×p  and Λ
is n×n. Using this SVD  we have the same result as the case of n ≥ p.

1U2U(cid:48)

2C1)

2

8

References
[1] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  ﬁrst edition  2007.
[2] L. Clemmensen  T. Hastie  and B. Erbøll. Sparse discriminant analysis. Technical report  June

2008.

[3] F. De la Torre and T. Kanade. Discriminative cluster analysis.

Conference on Machine Learning  2006.

In The 23rd International

[4] R. O. Duda  P. E. Hart  and D. G. Stork. Pattern Classiﬁcation. John Wiley and Sons  New

York  second edition  2001.

[5] T. Hastie  A. Buja  and R. Tibshirani. Penalized discriminant analysis. The Annals of Statistics 

23(1):73–102  1995.

[6] T. Hastie  R. Tibshirani  and A. Buja. Flexible discriminant analysis by optimal scoring. Jour-

nal of the American Statistical Association  89(428):1255–1270  1994.

[7] A. Y. Ng  M. I. Jordan  and Y. Weiss. On spectral clustering: analysis and an algorithm. In

Advances in Neural Information Processing Systems 14  volume 14  2002.

[8] C. H. Park and H. Park. A relationship between linear discriminant analysis and the gener-
alized minimum squared error solution. SIAM Journal on Matrix Analysis and Applications 
27(2):474–492  2005.

[9] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern

Analysis and Machine Intelligence  22(8):888–905  2000.

[10] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society  Series B  58:267–288  1996.

[11] M. Wu and B. Sch¨olkopf. A local learning approach for clustering. In Advances in Neural

Information Processing Systems 19  2007.

[12] J. Ye. Least squares linear discriminant analysis. In The Twenty-Fourth International Confer-

ence on Machine Learning  2007.

[13] J. Ye  Z. Zhao  and M. Wu. Discriminative k-means for clustering. In Advances in Neural

Information Processing Systems 20  2008.

[14] Z. Zhang  G. Dai  and M. I. Jordan. A ﬂexible and efﬁcient algorithm for regularized Fisher
discriminant analysis. In The European Conference on Machine Learning and Principles and
Practice of Knowledge Discovery in Databases (ECML PKDD)  2009.

[15] Z. Zhang and M. I. Jordan. Multiway spectral clustering: A margin-based perspective. Statis-

tical Science  23(3):383–403  2008.

[16] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society  Series B  67:301–320  2005.

[17] H. Zou  T. Hastie  and R. Tibshirani. Sparse principal component analysis. Journal of Compu-

tational and Graphical Statistics  15:265–286  2006.

9

,Sebastian Tschiatschek
Rishabh Iyer
Jeff Bilmes
Alaa Saade
Florent Krzakala
Lenka Zdeborová
Qinshi Wang
Wei Chen