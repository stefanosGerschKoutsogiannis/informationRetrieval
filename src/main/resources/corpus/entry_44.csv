2016,A Consistent Regularization Approach for Structured Prediction,We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space.  We exploit this fact to  design learning  algorithms using a surrogate loss approach and regularization techniques.   We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed method. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.,A Consistent Regularization Approach for Structured

Prediction

Carlo Ciliberto ∗ 1
cciliber@mit.edu

Alessandro Rudi ∗ 1 2
ale_rudi@mit.edu

Lorenzo Rosasco 1 2
lrosasco@mit.edu

1 Laboratory for Computational and Statistical Learning - Istituto Italiano di Tecnologia  Genova  Italy &

Massachusetts Institute of Technology  Cambridge  MA 02139  USA.

2 Università degli Studi di Genova  Genova  Italy.

∗Equal contribution

Abstract

We propose and analyze a regularization approach for structured prediction prob-
lems. We characterize a large class of loss functions that allows to naturally
embed structured outputs in a linear space. We exploit this fact to design learning
algorithms using a surrogate loss approach and regularization techniques. We
prove universal consistency and ﬁnite sample bounds characterizing the general-
ization properties of the proposed method. Experimental results are provided to
demonstrate the practical usefulness of the proposed approach.

1

Introduction

Many machine learning applications require dealing with data-sets having complex structures  e.g.
natural language processing  image segmentation  reconstruction or captioning  pose estimation 
protein folding prediction to name a few [1  2  3]. Structured prediction problems pose a challenge
for classic off-the-shelf learning algorithms for regression or binary classiﬁcation. This has motivated
the extension of methods such as support vector machines to structured problems [4]. Dealing
with structured prediction problems is also a challenge for learning theory. While the theory of
empirical risk minimization provides a very general statistical framework  in practice it needs to be
complemented with an ad-hoc analysis for each speciﬁc setting. Indeed  in the last few years  an
effort has been made to analyze speciﬁc structured problems  such as multiclass classiﬁcation [5] 
multi-labeling [6]  ranking [7] or quantile estimation [8]. A natural question is whether a unifying
learning framework can be developed to address a wide range of problems from theory to algorithms.
This paper takes a step in this direction  proposing and analyzing a general regularization approach
to structured prediction. Our starting observation is that for a large class of these problems  we can
deﬁne a natural embedding of the associated loss functions into a linear space. This allows to deﬁne
a (least squares) surrogate problem of the original structured one  that is cast within a multi-output
regularized learning framework [9  10  11]. We prove that by solving the surrogate  we are able to
recover the exact solution of the original structured problem. The corresponding algorithm essentially
generalizes approaches considered in [12  13  14  15  16]. We study the generalization properties of
the proposed approach  establishing universal consistency as well as ﬁnite sample bounds.
The rest of this paper is organized as follows: in Sec. 2 we introduce the structured prediction problem
in its generality and present our algorithm to approach it. In Sec. 3 we introduce and discuss a
surrogate framework for structured prediction  from which we derive our algorithm. In Sec. 4  we
analyze the theoretical properties of the proposed algorithm. In Sec. 5 we draw connections with
previous work in structured prediction. Sec. 6 reports promising experimental results on a variety of
structured prediction problems. Sec. 7 concludes the paper outlining relevant directions for future
research.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

2 A Regularization Approach to Structured prediction
The goal of supervised learning is to learn functional relations f : X → Y between two sets X  Y 
given a ﬁnite number of examples. In particular in this work we are interested to structured prediction 
namely the case where Y is a set of structured outputs (such as histograms  graphs  time sequences 
points on a manifold  etc.). Moreover  structure on Y can be implicitly induced by a suitable loss
(cid:52) : Y × Y → R (such as edit distance  ranking error  geodesic distance  indicator function of a
subset  etc.). Then  the problem of structured prediction becomes

minimize
f :X→Y

E(f ) 

with

E(f ) =

X×Y

(cid:52)(f (x)  y) dρ(x  y)

(1)

and the goal is to ﬁnd a good estimator for the minimizer of the above equation  given a ﬁnite number
of (training) points {(xi  yi)}n
i=1 sampled from a unknown probability distribution ρ on X × Y. In
the following we introduce an estimator ˆf : X → Y to approach Eq. (1). The rest of this paper is
devoted to prove that ˆf it a consistent estimator for a minimizer of Eq. (1).
Our Algorithm for Structured Prediction. In this paper we propose and analyze the following
estimator

(cid:90)

αi(x) (cid:52) (y  yi) with α(x) = (K + nλI)−1Kx ∈ Rn

(Alg. 1)

n(cid:88)

i=1

ˆf (x) = argmin

y∈Y

given a positive deﬁnite kernel k : X ×X → R and training set {(xi  yi)}n
i=1. In the above expression 
αi(x) is i-th entry in α(x)  K ∈ Rn×n is the kernel matrix Ki j = k(xi  xj)  Kx ∈ Rn the vector
with entires (Kx)i = k(x  xi)  λ > 0 a regularization parameter and I the identity matrix.
From a computational perspective  the procedure in Alg. 1 is divided in two steps: a learning step
where input-dependents weights αi(·) are computed (which essentially consists in solving a kernel
ridge regression problem) and a prediction step where the αi(x)-weighted linear combination in
Alg. 1 is optimized  leading to a prediction ˆf (x) given an input x. The idea of a similar two-steps
strategy goes back to standard approaches for structured prediction and was originally proposed
in [17]  where a “score” function F (x  y) was learned to estimate the “likelihood” of a pair (x  y)
sampled from ρ  and then used in ˆf (x) = argminy∈Y −F (x  y)  to predict the best ˆf (x) ∈ Y given
x ∈ X . This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of
approaches for structured prediction [1  12  14].
Intuition. While providing a principled derivation of Alg. 1 for a large class of loss functions is a
main contribution of this work  it is useful to ﬁrst consider the special case where (cid:52) is induced by a
reproducing kernel h : Y × Y → R on the output set  such that

(cid:52)(y  y(cid:48)) = h(y  y) − 2h(y  y(cid:48)) + h(y(cid:48)  y(cid:48)).

(2)
This choice of (cid:52) was originally considered in Kernel Dependency Estimation (KDE) [18]. In
particular  for the special case of normalized kernels (i.e. h(y  y) = 1 ∀y ∈ Y)  Alg. 1 essentially
reduces to [12  13  14] and recalling their derivation is insightful. Note that  since a kernel can be
written as h(y  y(cid:48)) = (cid:104)ψ(y)  ψ(y(cid:48))(cid:105)HY   with ψ : Y → HY a non-linear map into a feature space
HY [19]  then Eq. (2) can be rewritten as

(3)
Directly minimizing the equation above with respect to f is generally challenging due to the non
linearity ψ. A possibility is to replace ψ ◦ f by a function g : X → HY that is easier to optimize. We
can then consider the regularized problem

(cid:52)(f (x)  y(cid:48)) = (cid:107)ψ(f (x)) − ψ(y(cid:48))(cid:107)2HY .

with G a space of functions1 g : X → HY of the form g(x) =(cid:80)

(cid:107)g(xi) − ψ(yi)(cid:107)2HY + λ(cid:107)g(cid:107)2G

minimize

(4)
i=1 k(x  xi)ci with ci ∈ HY and k

g∈G

1
n

a reproducing kernel. Indeed  in this case the solution to Eq. (4) is

n(cid:88)

i=1

ˆg(x) =

αi(x)ψ(yi) with α(x) = (K + nλI)−1Kx ∈ Rn

(5)

i=1

1G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product

(cid:104)k(xi ·)ci  k(xj ·)cj(cid:105)G = k(xi  xj)(cid:104)ci  cj(cid:105)HY

2

n(cid:88)

n(cid:88)

i=1

where the αi are the same as in Alg. 1. Since we replaced (cid:52)(f (x)  y) by (cid:107)g(x)− ψ(y)(cid:107)2HY   a natural
question is how to recover an estimator ˆf from ˆg. In [12] it was proposed to consider

ˆf (x) = argmin

y∈Y

(cid:107)ψ(y) − ˆg(x)(cid:107)2HY = argmin
y∈Y

h(y  y) − 2

αi(x)h(y  yi) 

(6)

which corresponds to Alg. 1 when h is a normalized kernel.
The discussion above provides an intuition on how Alg. 1 is derived but raises also a few questions.
First  it is not clear if and how the same strategy could be generalized to loss functions that do not
satisfy Eq. (2). Second  the above reasoning hinges on the idea of replacing ˆf with ˆg (and then
recovering ˆf by Eq. (6))  however it is not clear whether this approach can be justiﬁed theoretically.
Finally  we can ask what are the statistical properties of the resulting algorithm. We address the
ﬁrst two questions in the next section  while the rest of the paper is devoted to establish universal
consistency and generalization bounds for algorithm Alg. 1.

3 Surrogate Framework and Derivation

(cid:90)

To derive Alg. 1 we consider ideas from surrogate approaches [20  21  7] and in particular [5]. The
idea is to tackle Eq. (1) by substituting (cid:52)(f (x)  y) with a “relaxation” L(g(x)  y) on a space HY 
that is easy to optimize. The corresponding surrogate problem is

R(g) 

with

R(g) =

X×Y

minimize
g:X→HY

L(g(x)  y) dρ(x  y) 

Fisher Consistency: E(d ◦ g∗) = E(f∗) 
Comparison Inequality: E(d ◦ g) − E(f∗) ≤ ϕ(R(g) − R(g∗)) 

(7)
and the question is how a solution g∗ for the above problem can be related to a minimizer f∗ of
Eq. (1). This is made possible by the requirement that there exists a decoding d : HY → Y  such that
(8)
(9)
hold for all g : X → HY  where ϕ : R → R is such that ϕ(s) → 0 for s → 0. Indeed  given an
estimator ˆg for g∗  we can “decode” it considering ˆf = d ◦ ˆg and use the excess risk R(ˆg) − R(g∗)
to control E( ˆf ) − E(f∗) via the comparison inequality in Eq. (9). In particular  if ˆg is a data-
dependent predictor trained on n points and R(ˆg) → R(g∗) when n → +∞  we automatically
have E( ˆf ) → E(f∗). Moreover  if ϕ in Eq. (9) is known explicitly  generalization bounds for ˆg are
automatically extended to ˆf.
Provided with this perspective on surrogate approaches  here we revisit the discussion of Sec. 2 for
the case of a loss function induced by a kernel h. Indeed  by assuming the surrogate L(g(x)  y) =
(cid:107)g(x) − ψ(y)(cid:107)2HY   Eq. (4) becomes the empirical version of the surrogate problem at Eq. (7)
and leads to an estimator ˆg of g∗ as in Eq. (5). Therefore  the approach in [12  14] to recover
ˆf (x) = argminy L(g(x)  y) can be interpreted as the result ˆf (x) = d ◦ ˆg(x) of a suitable decoding
of ˆg(x). An immediate question is whether the above framework satisﬁes Eq. (8) and (9). Moreover 
we can ask if the same idea could be applied to more general loss functions.
In this work we identify conditions on (cid:52) that are satisﬁed by a large family of functions and moreover
allow to design a surrogate framework for which we prove Eq. (8) and (9). The ﬁrst step in this
direction is to introduce the following assumption.
Assumption 1. There exists a separable Hilbert space HY with inner product (cid:104)· ·(cid:105)HY   a continuous
embedding ψ : Y → HY and a bounded linear operator V : HY → HY  such that

(cid:52)(y  y(cid:48)) = (cid:104)ψ(y)  V ψ(y(cid:48))(cid:105)HY

∀y  y(cid:48) ∈ Y

(10)

Asm. 1 is similar to Eq. (3) and in particular to the deﬁnition of a reproducing kernel. Note however
that by not requiring V to be positive semideﬁnite (or even symmetric)  we allow for a surprisingly
wide range of functions beyond kernel functions. Indeed  below we give some examples of functions
that satisfy Asm. 1 (see supplementary material Sec. C for more details):
Example 1. The following functions of the form (cid:52) : Y × Y → R satisfy Asm. 1:

3

1. Any loss on Y of ﬁnite cardinality. Several problems belong to this setting  such as Multi-

Class Classiﬁcation  Multi-labeling  Ranking  predicting Graphs (e.g. protein foldings).

2. Regression and Classiﬁcation Loss Functions. Least-squares  Logistic  Hinge  -insensitive 

τ-Pinball.

3. Robust Loss Functions. Most loss functions used for robust estimation [22] such as the
absolute value  Huber  Cauchy  German-McLure  “Fair” and L2 − L1. See [22] or the
supplementary material for their explicit formulation.

4. KDE. Loss functions (cid:52) induced by a kernel such as in Eq. (2).
5. Distances on Histograms/Probabilities. The χ2 and the Hellinger distances.
6. Diffusion distances on Manifolds. The squared diffusion distance induced by the heat kernel

(at time t > 0) on a compact Reimannian manifold without boundary [23].

The Least Squares Loss Surrogate Framework. Asm. 1 implicitly deﬁnes the space HY similarly
to Eq. (3). The following result motivates the choice of the least squares surrogate and moreover
suggests a possible choice for the decoding.
Lemma 1. Let (cid:52) : Y × Y → R satisfy Asm. 1 with ψ : Y → HY bounded. Then the expected risk
in Eq. (1) can be written as

for all f : X → Y  where g∗ : X → HY minimizes

E(f ) =

(cid:104)ψ(f (x))  V g∗(x)(cid:105)HY dρX (x)

R(g) =

(cid:107)g(x) − ψ(y)(cid:107)2HY dρ(x  y).

X×Y

(cid:90)
(cid:90)

X

(11)

(12)

(14)
(15)

Lemma 1 shows how Eq. (12) arises naturally as surrogate problem. In particular  Eq. (11) suggests
to choose the decoding

(cid:104) ψ(y)   V h (cid:105)HY

∀h ∈ HY  

y∈Y

d(h) = argmin

(13)
since d ◦ g∗(x) = arg miny∈Y(cid:104)ψ(y)  V g∗(x)(cid:105) and therefore E(d ◦ g∗) ≤ E(f ) for any measurable
f : X → Y  leading to Fisher Consistency. We formalize this in the following result.
Theorem 2. Let (cid:52) : Y × Y → R satisfy Asm. 1 with Y a compact set. Then  for every measurable
g : X → HY and d : HY → Y satisfying Eq. (13)  the following holds

E(d ◦ g∗) = E(f∗)

E(d ◦ g) − E(f∗) ≤ c(cid:52)(cid:112)R(g) − R(g∗).

with c(cid:52) = (cid:107)V (cid:107) maxy∈Y (cid:107)ψ(y)(cid:107)HY .
Thm. 2 shows that for all (cid:52) satisfying Asm. 1  the corresponding surrogate framework identiﬁed
by the surrogate in Eq. (12) and decoding Eq. (13) satisﬁes Fisher consistency Eq. (14) and the
comparison inequality in Eq. (15). We recall that a ﬁnite set Y is always compact  and moreover 
assuming the discrete topology on Y  we have that any ψ : Y → HY is continuous. Therefore 
Thm. 2 applies in particular to any structured prediction problem on Y with ﬁnite cardinality.
Thm. 2 suggest to approach structured prediction by ﬁrst learning ˆg and then decoding it to recover
ˆf = d ◦ ˆg. A natural question is how to choose ˆg in order to compute ˆf in practice. In the rest of this
section we propose an approach to this problem.
Derivation for Alg. 1. Minimizing R in Eq. (12) corresponds to a vector-valued regression problem
[9  10  11]. In this work we adopt an empirical risk minimization approach to learn ˆg as in Eq. (4).
The following result shows that combining ˆg with the decoding in Eq. (13) leads to the ˆf in Alg. 1.
Lemma 3. Let (cid:52) : Y × Y → R satisfy Asm. 1 with Y a compact set. Let ˆg : X → HY be the
minimizer of Eq. (4). Then  for all x ∈ X

d ◦ ˆg(x) = argmin
y∈Y

αi(x) (cid:52) (y  yi)

α(x) = (K + nλI)−1Kx ∈ Rn

(16)

n(cid:88)

i=1

4

Lemma 3 concludes the derivation of Alg. 1. An interesting observation is that computing ˆf does not
require explicit knowledge of the embedding ψ and the operator V   which are implicitly encoded
within the loss (cid:52) by Asm. 1. In analogy to the kernel trick [24] we informally refer to such assumption
as the “loss trick”. We illustrate this effect with an example.
Example 2 (Ranking). In ranking problems the goal is to predict ordered sequences of a ﬁxed number
(cid:96) of labels. For these problems  Y corresponds to the set of all ordered sequences of (cid:96) labels and has
cardinality |Y| = (cid:96)!  which is typically dramatically larger than the number n of training examples
(e.g. for (cid:96) = 15  (cid:96)! (cid:39) 1012). Therefore  given an input x ∈ X   directly computing ˆg(x) ∈ R|Y| is
impractical. On the opposite  the loss trick allows to express d ◦ ˆg(x) only in terms of the n weights
αi(x) in Alg. 1  making the computation of the argmin easier to approach in general. For details on
the rank loss (cid:52)rank and the corresponding optimization over Y  we refer to the empirical analysis of
Sec. 6.

In this section we have shown a derivation for the structured prediction algorithm proposed in this
work. In Thm. 2 we have shown how the expected risk of the proposed estimator ˆf is related to an
estimator ˆg via a comparison inequality. In the following we will make use of these results to prove
consistency and generalization bounds for Alg. 1.

4 Statistical Analysis

In this section we study the statistical properties of Alg. 1 exploiting of the relation between the
structured and surrogate problems characterized be the comparison inequality in Thm. 2. We begin
our analysis by proving that Alg. 1 is universally consistent.
Theorem 4 (Universal Consistency). Let (cid:52) : Y × Y → R satisfy Asm. 1  X and Y be compact sets
and k : X ×X → R a continuous universal reproducing kernel2. For any n ∈ N and any distribution
ρ on X × Y let ˆfn : X → Y be obtained by Alg. 1 with {(xi  yi)}n
i=1 training points independently
sampled from ρ and λn = n−1/4. Then 

n→+∞E( ˆfn) = E(f∗)

lim

with probability 1

(17)

Thm. 4 shows that  when the (cid:52) satisﬁes Asm. 1  Alg. 1 approximates a solution f∗ to Eq. (1)
arbitrarily well  given a sufﬁcient number of training examples. To the best of our knowledge this is
the ﬁrst consistency result for structured prediction in the general setting considered in this work and
characterized by Asm. 1  in particular for the case of Y with inﬁnite cardinality (dense or discrete).
The No Free Lunch Theorem [25] states that it is not possible to prove uniform convergence rates for
Eq. (17). However  by imposing suitable assumptions on the regularity of g∗ it is possible to prove
generalization bounds for ˆg and then  using Thm. 2  extend them to ˆf. To show this  it is sufﬁcient
to require that g∗ belongs to G the reproducing kernel Hilbert space used in the ridge regression of
Eq. (4). Note that in the proofs of Thm. 4 and Thm. 5  our analysis on ˆg borrows ideas from [10] and
extends their result to our setting for the case of HY inﬁnite dimensional (i.e. when Y has inﬁnite
cardinality). Indeed  note that in this case [10] cannot be applied to the estimator ˆg considered in this
work (see supplementary material Sec. B.3  Lemma 18 for details).
Theorem 5 (Generalization Bound). Let (cid:52) : Y × Y → R satisfy Asm. 1  Y be a compact set and
k : X × X → R a bounded continuous reproducing kernel. Let ˆfn denote the solution of Alg. 1 with
n training points and λ = n−1/2. If the surrogate risk R deﬁned in Eq. (12) admits a minimizer
g∗ ∈ G  then

E( ˆfn) − E(f∗) ≤ cτ 2 n− 1

4

(18)

holds with probability 1 − 8e−τ for any τ > 0  with c a constant not depending on n and τ.
The bound in Thm. 5 is of the same order of the generalization bounds available for the least squares
binary classiﬁer [26]. Indeed  in Sec. 5 we show that in classiﬁcation settings Alg. 1 reduces to least
squares classiﬁcation. This opens the way to possible improvements  as we discuss in the following.

2This is a standard assumption for universal consistency (see [21]). An example of continuous universal

kernel is the Gaussian k(x  x(cid:48)) = exp(−γ(cid:107)x − x(cid:48)(cid:107)2)  with γ > 0.

5

Remark 1 (Better Comparison Inequality). The generalization bounds for the least squares classiﬁer
can be improved by imposing regularity conditions on ρ via the Tsybakov condition [26]. This was
observed in [26] for binary classiﬁcation with the least squares surrogate  where a tighter comparison
inequality than the one in Thm. 2 was proved. Therefore  a natural question is whether the inequality
of Thm. 2 could be similarly improved  consequently leading to better rates for Thm. 5. Promising
results in this direction can be found in [5]  where the Tsybakov condition was generalized to the
multi-class setting and led to a tight comparison inequality analogous to the one for the binary setting.
However  this question deserves further investigation. Indeed  it is not clear how the approach in [5]
could be further generalized to the case where Y has inﬁnite cardinality.
Remark 2 (Other Surrogate Frameworks). In this paper we focused on a least squares surrogate
loss function and corresponding framework. A natural question is to ask whether other loss functions
could be considered to approach the structured prediction problem  sharing the same or possibly even
better properties. This question is related also to Remark 1  since different surrogate frameworks
could lead to sharper comparison inequalities. This seems an interesting direction for future work.

5 Connection with Previous Work

Binary and Multi-class Classiﬁcation. It is interesting to note that in classiﬁcation settings  Alg. 1
corresponds to the least squares classiﬁer [26]. Indeed  let Y = {1  . . .   (cid:96)} be a set of labels and
consider the misclassiﬁcation loss (cid:52)(y  y(cid:48)) = 1 for y (cid:54)= y(cid:48) and 0 otherwise. Then (cid:52)(y  y(cid:48)) =
y V ey(cid:48) with ei ∈ R(cid:96) the i-the element of the canonical basis of R(cid:96) and V = 1 − I  where I is the
e(cid:62)
(cid:96) × (cid:96) identity matrix and 1 the matrix with all entries equal to 1. In the notation of surrogate methods
adopted in this work  HY = R(cid:96) and ψ(y) = ey. Note that both Least squares classiﬁcation and our
approach solve the surrogate problem at Eq. (4)

n(cid:88)

i=1

1
n

(cid:107)g(xi) − eyi(cid:107)2R(cid:96) + λ (cid:107)g(cid:107)2G

(19)

to obtain a vector-valued predictor ˆg : X → R(cid:96) as in Eq. (5). Then  the least squares classiﬁer ˆc and
the decoding ˆf = d ◦ ˆg are respectively obtained by

ˆc(x) = argmax
i=1 ... (cid:96)

ˆg(x)

ˆf (x) = argmin
i=1 ... (cid:96)

V ˆg(x).

(20)

However  since V = 1 − I  it is easy to see that ˆc(x) = ˆf (x) for all x ∈ X .
Kernel Dependency Estimation. In Sec. 2 we discussed the relation between KDE [18  12] and
Alg. 1. In particular  we have observed that if (cid:52) is induced by a kernel h : Y × Y → R as in Eq. (2)
and h is normalized  i.e. h(y  y) = κ ∀y ∈ Y  with κ > 0  then algorithm Eq. (6) proposed in [12]
leads to the same predictor as Alg. 1. Therefore  we can apply Thm. 4 and 5 to prove universal
consistency and generalization bounds for methods such as [12  14]. Some theoretical properties of
KDE have been previously studied in [15] from a PAC Bayesian perspective. However  the obtained
bounds do not allow to control the excess risk or establish consistency of the method. Moreover  note
that when the kernel h is not normalized  the “decoding” in Eq. (6) is not equivalent to Alg. 1. In
particular  given the surrogate solution g∗  applying Eq. (6) leads to predictors that do not minimize
Eq. (1). As a consequence  approaches in [12  13  14] are not consistent in the general case.
Support Vector Machines for Structured Output. A popular approach to structured prediction
is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the
well-known SVM algorithm to the structured setting. One of the main advantages of SVMstruct is
that it can be applied to a variety of problems since it does not impose strong assumptions on the loss.
In this view  our approach shares similar properties  and in particular allows to consider Y of inﬁnite
cardinality. Moreover  we note that generalization studies for SVMstruct are available [3] (Ch. 11).
However  it seems that these latter results do not allow to derive universal consistency of the method.

6 Experiments

In this section we report on preliminary experiments showing the performance of the proposed
approach on simulated as well as real structured prediction problems.

6

Linear [7]
Hinge [27]
Logistic [28]
SVM Struct [4]
Alg. 1

Rank Loss
0.430 ± 0.004
0.432 ± 0.008
0.432 ± 0.012
0.451 ± 0.008
0.396 ± 0.003

Table 1: Normalized (cid:52)rank for ranking
methods on the MovieLens dataset [29].

KDE [18]
(Gaussian)

0.149 ± 0.013
0.736 ± 0.032
0.294 ± 0.012

Alg. 1

(Hellinger)
0.172 ± 0.011
0.647 ± 0.017
0.193 ± 0.015

Loss
(cid:52)G
(cid:52)H
(cid:52)R

Table 2:
(KDE [18]) and Hellinger loss.

Digit

reconstruction using Gaussian

Ranking Movies. We considered the problem of ranking movies in the MovieLens dataset [29]
(ratings (from 1 to 5) of 1682 movies by 943 users). The goal was to predict preferences of a given
user  i.e. an ordering of the 1682 movies  according to the user’s partial ratings. We applied Alg. 1 to
the ranking problem using the rank loss [7]

(cid:52)rank(y  y(cid:48)) =

1
2

γ(y(cid:48))ij (1 − sign(yi − yj)) 

(21)

M(cid:88)

i j=1

where M is the number of movies  y is a re-ordering of the sequence 1  . . .   M. The scalar γ(y)ij
denotes the costs (or reward) of having movie j ranked higher than movie i. Similarly to [7]  we set
γ(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5). We chose as k
in Alg. 1  a linear kernel on features similar to those proposed in [7]  which were computed based on
users’ profession  age  similarity of previous ratings  etc. Since solving Alg. 1 for (cid:52)rank is NP-hard
(see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [30] to approximate the
ˆf (x) of Alg. 1. Results are reported in Tab. 1 comparing Alg. 1 (Ours) with surrogate ranking methods
using a Linear [7]  Hinge [27] or Logistic [28] loss and Struct SVM [4]. We randomly sampled
n = 643 users for training and tested on the remaining 300. We performed 5-fold cross-validation for
model selection. We report the normalized (cid:52)rank  averaged over 10 trials to account for statistical
variability. Interestingly  our approach appears to outperform all competitors  suggesting that Alg. 1
is a viable approach to ranking.
Image Reconstruction with Hellinger Distance. We considered the USPS digits reconstruction
experiment originally proposed in [18]. The goal is to predict the lower half of an image depicting a
digit  given the upper half of the same image in input. The standard approach is to use a Gaussian
kernel kG on images in input and adopt KDE methods such as [18  12  14] with loss (cid:52)G(y  y(cid:48)) =
1− kG(y  y(cid:48)). Here we take a different approach and  following [31]  we interpret an image depicting
a digit as an histogram and normalize it to sum up to 1. Therefore  Y is the unit simplex in R128
(16 × 16 images) and we adopt the Hellinger distance (cid:52)H

(cid:52)H (y  y(cid:48)) =

|(yi)1/2 − (y(cid:48)

i)1/2|

for

y = (yi)M
i=1

(22)

i=1

to measure distances on Y. We used the kernel kG on the input space and compared Alg. 1 using
respectively (cid:52)H and (cid:52)G. For (cid:52)G Alg. 1 correpsponds to [12]. We performed digit reconstruction
experiments by training on 1000 examples evenly distributed among the 10 digits of USPS and
tested on 5000 images. We performed 5-fold cross-validation for model selection. Tab. 2 reports
the performance of Alg. 1 and the KDE methods averaged over 10 runs. Performance are reported
according to the Gaussian loss (cid:52)G and Hellinger loss (cid:52)H. Unsurprisingly  methods trained with
respect to a speciﬁc loss perform better than the competitor with respect to such loss. Therefore  as a
further measure of performance we also introduced the “Recognition” loss (cid:52)R. This loss has to be
intended as a measure of how “well” a predictor was able to correctly reconstruct an image for digit
recognition purposes. To this end  we trained an automatic digit classiﬁer and deﬁned (cid:52)R to be the
misclassiﬁcation error of such classiﬁer when tested on images reconstructed by the two prediction
algorithms. This automatic classiﬁer was trained using a standard SVM [24] on a separate subset of
USPS images and achieved an average 0.04% error rate on the true 5000 test sets. In this case a clear
difference in performance can be observed between using two different loss functions  suggesting
that (cid:52)H is more suited for the reconstruction problem.

7

M(cid:88)

n
50
100
200
500
1000

Alg. 1

0.39 ± 0.17
0.21 ± 0.04
0.12 ± 0.02
0.08 ± 0.01
0.07 ± 0.01

RNW

0.45 ± 0.18
0.29 ± 0.04
0.24 ± 0.03
0.22 ± 0.02
0.21 ± 0.02

KRR

0.62 ± 0.13
0.47 ± 0.09
0.33 ± 0.04
0.31 ± 0.03
0.19 ± 0.02

Figure 1: Robust estimation on the regression problem in Sec. 6 by minimizing the Cauchy loss with
Alg. 1 (Ours) or Nadaraya-Watson (Nad). KRLS as a baseline predictor. Left. Example of one run of
the algorithms. Right. Average distance of the predictors to the actual function (without noise and
outliers) over 100 runs with respect to training sets of increasing dimension.

Robust Estimation. We considered a regression problem with many outliers and evaluated Alg. 1
using the Cauchy loss (see Example 1 - (3)) for robust estimation. Indeed  in this setting  Y =
[−M  M ] ⊂ R is not structured  but the non-convexity of (cid:52) can be an obstacle to the learning
process. We generated a dataset according to the model y = sin(6πx) +  + ζ  where x was sampled
uniformly on [−1  1] and  according to a zero-mean Gaussian with variance 0.1. ζ modeled the
outliers and was sampled according to a zero-mean random variable that was 0 with probability
0.90 and a value uniformly at random in [−3  3] with probability 0.1. We compared Alg. 1 with the
Nadaraya-Watson robust estimator (RNW) [32] and kernel ridge regression (KRR) with a Gaussian
kernel as baseline. To train Alg. 1 we used a Gaussian kernel on the input and performed predictions
(i.e. solved Eq. (16)) using Matlab FMINUNC function for unconstrained minimization. Experiments
were performed with training sets of increasing dimension (100 repetitions each) and test set of 1000
examples. 5-fold cross-validation for model selection. Results are reported in Fig. 1  showing that
our estimator signiﬁcantly outperforms the others. Moreover  our method appears to greatly beneﬁt
from training sets of increasing size.

7 Conclusions and Future Work

In this work we considered the problem of structured prediction from a Statistical Learning Theory
perspective. We proposed a learning algorithm for structured prediction that is split into a learning
and prediction step similarly to previous methods in the literature. We studied the statistical properties
of the proposed algorithm by adopting a strategy inspired to surrogate methods. In particular  we
identiﬁed a large family of loss functions for which it is natural to identify a corresponding surrogate
problem. This perspective allows to prove a derivation of the algorithm proposed in this work.
Moreover  by exploiting a comparison inequality relating the original and surrogate problems we
were able to prove universal consistency and generalization bounds under mild assumption. In
particular  the bounds proved in this work recover those already known for least squares classiﬁcation 
of which our approach can be seen as a generalization. We supported our theoretical analysis with
experiments showing promising results on a variety of structured prediction problems.
A few questions were left opened. First  we ask whether the comparison inequality can be improved
(under suitable hypotheses) to obtain faster generalization bounds for our algorithm. Second  the
surrogate problem in our work consists of a vector-valued regression (in a possibly inﬁnite dimensional
Hilbert space)  we solved this problem by plain kernel ridge regression but it is natural to ask whether
approaches from the multi-task learning literature could lead to substantial improvements in this
setting. Finally  an interesting question is whether alternative surrogate frameworks could be derived
for the setting considered in this work  possibly leading to tighter comparison inequalities. We will
investigate these questions in the future.

References
[1] Pedro F Felzenszwalb  Ross B Girshick  David McAllester  and Deva Ramanan. Object detection with

discriminatively trained part-based models. PAMI  IEEE Transactions on  32(9):1627–1645  2010.

[2] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In

Proceedings of the IEEE Conference on CVPR  pages 3128–3137  2015.

[3] Thomas Hofmann Bernhard Schölkopf Alexander J. Smola Ben Taskar Bakir  Gökhan and S.V.N Vish-

wanathan. Predicting structured data. MIT press  2007.

8

−1−0.8−0.6−0.4−0.200.20.40.60.81−2024Alg. 1RNWKRLS[4] Ioannis Tsochantaridis  Thorsten Joachims  Thomas Hofmann  and Yasemin Altun. Large margin methods

for structured and interdependent output variables. In JMLR  pages 1453–1484  2005.

[5] Youssef Mroueh  Tomaso Poggio  Lorenzo Rosasco  and Jean-Jacques Slotine. Multiclass learning with

simplex coding. In NIPS  pages 2798–2806  2012.

[6] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. Artiﬁcial Intelligence  2013.
[7] John C Duchi  Lester W Mackey  and Michael I Jordan. On the consistency of ranking algorithms. In
Proceedings of the 27th International Conference on Machine Learning (ICML-10)  pages 327–334  2010.
[8] Ingo Steinwart  Andreas Christmann  et al. Estimating conditional quantiles with the help of the pinball

loss. Bernoulli  17(1):211–225  2011.

[9] Charles A Micchelli and Massimiliano Pontil. Kernels for multi–task learning. In Advances in Neural

Information Processing Systems  pages 921–928  2004.

[10] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.

Foundations of Computational Mathematics  7(3):331–368  2007.

[11] M. Álvarez  N. Lawrence  and L. Rosasco. Kernels for vector-valued functions: a review. Foundations and

Trends in Machine Learning  4(3):195–266  2012. see also http://arxiv.org/abs/1106.6251.

[12] Corinna Cortes  Mehryar Mohri  and Jason Weston. A general regression technique for learning transduc-

tions. In Proceedings of the 22nd international conference on Machine learning  2005.

[13] P. Geurts  L. Wehenkel  and F. d’Alché Buc. Kernelizing the output of tree-based methods. In ICML  2006.
[14] H. Kadri  M. Ghavamzadeh  and P. Preux. A generalized kernel approach to structured output learning.

Proc. International Conference on Machine Learning (ICML)  2013.

[15] S. Giguère  M. M.  K. Sylla  and F. Laviolette. Risk bounds and learning algorithms for the regression
approach to structured output prediction. In ICML. JMLR Workshop and Conference Proceedings  2013.
[16] C. Brouard  M. Szafranski  and F. d’Alché Buc. Input output kernel regression: Supervised and semi-

supervised structured output prediction with operator-valued kernels. JMLR  17(176):1–48  2016.

[17] Michael Collins. Discriminative training methods for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural
language processing-Volume 10  pages 1–8. Association for Computational Linguistics  2002.

[18] Jason Weston  Olivier Chapelle  Vladimir Vapnik  André Elisseeff  and Bernhard Schölkopf. Kernel

dependency estimation. In Advances in neural information processing systems  pages 873–880  2002.

[19] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and

statistics. Springer Science & Business Media  2011.

[20] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[21] Ingo Steinwart and Andreas Christmann. Support Vector Machines. Information Science and Statistics.

Springer New York  2008.

[22] Peter J Huber and Elvezio M Ronchetti. Robust statistics. Springer  2011.
[23] Richard Schoen and Shing-Tung Yau. Lectures on differential geometry  volume 2. International press

Boston  1994.

[24] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines  regulariza-

tion  optimization  and beyond. MIT press  2002.

[25] David H Wolpert. The lack of a priori distinctions between learning algorithms. Neural computation 

8(7):1341–1390  1996.

[26] Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent learning.

Constructive Approximation  26(2):289–315  2007.

[27] Ralf Herbrich  Thore Graepel  and Klaus Obermayer. Large margin rank boundaries for ordinal regression.

Advances in neural information processing systems  pages 115–132  1999.

[28] Ofer Dekel  Yoram Singer  and Christopher D Manning. Log-linear models for label ranking. In Advances

in neural information processing systems  page None  2004.

[29] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. ACM Transactions

on Interactive Intelligent Systems (TiiS)  5(4):19  2015.

[30] Peter Eades  Xuemin Lin  and William F Smyth. A fast and effective heuristic for the feedback arc set

problem. Information Processing Letters  47(6):319–323  1993.

[31] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems  pages 2292–2300  2013.

[32] Wolfgang Härdle. Robust regression function estimation. Journal of Multivariate Analysis  14(2):169–180 

1984.

9

,Carlo Ciliberto
Lorenzo Rosasco
Alessandro Rudi