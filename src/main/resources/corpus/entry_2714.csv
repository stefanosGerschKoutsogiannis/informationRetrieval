2015,Inverse Reinforcement Learning with Locally Consistent Reward Functions,Existing inverse reinforcement learning (IRL) algorithms have assumed each expert’s demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions  hence catering to more realistic and complex experts’ behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model  an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert’s demonstrated trajectories. As a result  the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL  which is  interestingly  a reduced variant of our approach.,Inverse Reinforcement Learning with Locally

Consistent Reward Functions

Dept. of Computer Science  National University of Singapore  Republic of Singapore†

Dept. of Electrical Engineering and Computer Science  Massachusetts Institute of Technology  USA§

Quoc Phong Nguyen†  Kian Hsiang Low†  and Patrick Jaillet§

{qphong lowkh}@comp.nus.edu.sg†  jaillet@mit.edu§

Abstract

Existing inverse reinforcement learning (IRL) algorithms have assumed each ex-
pert’s demonstrated trajectory to be produced by only a single reward function.
This paper presents a novel generalization of the IRL problem that allows each
trajectory to be generated by multiple locally consistent reward functions  hence
catering to more realistic and complex experts’ behaviors. Solving our gener-
alized IRL problem thus involves not only learning these reward functions but
also the stochastic transitions between them at any state (including unvisited
states). By representing our IRL problem with a probabilistic graphical model 
an expectation-maximization (EM) algorithm can be devised to iteratively learn
the different reward functions and the stochastic transitions between them in order
to jointly improve the likelihood of the expert’s demonstrated trajectories. As a
result  the most likely partition of a trajectory into segments that are generated
from different locally consistent reward functions selected by EM can be derived.
Empirical evaluation on synthetic and real-world datasets shows that our IRL al-
gorithm outperforms the state-of-the-art EM clustering with maximum likelihood
IRL  which is  interestingly  a reduced variant of our approach.

Introduction

1
The reinforcement learning problem in Markov decision processes (MDPs) involves an agent using
its observed rewards to learn an optimal policy that maximizes its expected total reward for a given
task. However  such observed rewards or the reward function deﬁning them are often not available
nor known in many real-world tasks. The agent can therefore learn its reward function from an
expert associated with the given task by observing the expert’s behavior or demonstration  and this
approach constitutes the inverse reinforcement learning (IRL) problem.
Unfortunately  the IRL problem is ill-posed because inﬁnitely many reward functions are consistent
with the expert’s observed behavior. To resolve this issue  existing IRL algorithms have proposed
alternative choices of the agent’s reward function that minimize different dissimilarity measures de-
ﬁned using various forms of abstractions of the agent’s generated optimal behavior vs. the expert’s
observed behavior  as brieﬂy discussed below (see [17] for a detailed review): (a) The projection
algorithm [1] selects a reward function that minimizes the squared Euclidean distance between the
feature expectations obtained by following the agent’s generated optimal policy and the empirical
feature expectations observed from the expert’s demonstrated state-action trajectories; (b) the multi-
plicative weights algorithm for apprentice learning [24] adopts a robust minimax approach to deriv-
ing the agent’s behavior  which is guaranteed to perform no worse than the expert and is equivalent
to choosing a reward function that minimizes the difference between the expected average reward
under the agent’s generated optimal policy and the expert’s empirical average reward approximated
using the agent’s reward weights; (c) the linear programming apprentice learning algorithm [23]
picks its reward function by minimizing the same dissimilarity measure but incurs much less time
empirically; (d) the policy matching algorithm [16] aims to match the agent’s generated optimal
behavior to the expert’s observed behavior by choosing a reward function that minimizes the sum of

1

squared Euclidean distances between the agent’s generated optimal policy and the expert’s estimated
policy (i.e.  from its demonstrated trajectories) over every possible state weighted by its empirical
state visitation frequency; (e) the maximum entropy IRL [27] and maximum likelihood IRL (MLIRL)
[2] algorithms select reward functions that minimize an empirical approximation of the Kullback-
Leibler divergence between the distributions of the agent’s and expert’s generated state-action tra-
jectories  which is equivalent to maximizing the average log-likelihood of the expert’s demonstrated
trajectories. The log-likelihood formulations of the maximum entropy IRL and MLIRL algorithms
differ in the use of smoothing at the trajectory and action levels  respectively. As a result  the for-
mer’s log-likelihood or dissimilarity measure does not utilize the agent’s generated optimal policy 
which is consequently questioned by [17] as to whether it is considered an IRL algorithm. Bayesian
IRL [21] extends IRL to the Bayesian setting by maintaining a distribution over all possible reward
functions and updating it using Bayes rule given the expert’s demonstrated trajectories. The work
of [5] extends the projection algorithm [1] to handle partially observable environments given the
expert’s policy (i.e.  represented as a ﬁnite state controller) or observation-action trajectories.
All the IRL algorithms described above have assumed that the expert’s demonstrated trajectories
are only generated by a single reward function. To relax this restrictive assumption  the recent
works of [2  6] have  respectively  generalized MLIRL (combining it with expectation-maximization
(EM) clustering) and Bayesian IRL (integrating it with a Dirichlet process mixture model) to handle
trajectories generated by multiple reward functions (e.g.  due to many intentions) in observable
environments. But  each trajectory is assumed to be produced by a single reward function.
In this paper  we propose a new generalization of the IRL problem in observable environments 
which is inspired by an open question posed in the seminal works of IRL [19  22]: If behavior
is strongly inconsistent with optimality  can we identify “locally consistent” reward functions for
speciﬁc regions in state space? Such a question implies that no single reward function is globally
consistent with the expert’s behavior  hence invalidating the use of all the above-mentioned IRL
algorithms. More importantly  multiple reward functions may be locally consistent with the expert’s
behavior in different segments along its state-action trajectory and the expert has to switch/transition
between these locally consistent reward functions during its demonstration. This can be observed
in the following real-world example [26] where every possible intention of the expert is uniquely
represented by a different reward function: A driver intends to take the highway to a food center for
lunch. An electronic toll coming into effect on the highway may change his intention to switch to
another route. Learning of the driver’s intentions to use different routes and his transitions between
them allows the transport authority to analyze  understand  and predict the trafﬁc route patterns and
behavior for regulating the toll collection. This example  among others (e.g.  commuters’ intentions
to use different transport modes  tourists’ intentions to visit different attractions  Section 4)  motivate
the practical need to formalize and solve our proposed generalized IRL problem.
This paper presents a novel generalization of the IRL problem that  in particular  allows each ex-
pert’s state-action trajectory to be generated by multiple locally consistent reward functions  hence
catering to more realistic and complex experts’ behaviors than that afforded by existing variants of
the IRL problem (which all assume that each trajectory is produced by a single reward function)
discussed earlier. At ﬁrst glance  one may straightaway perceive our generalization as an IRL prob-
lem in a partially observable environment by representing the choice of locally consistent reward
function in a segment as a latent state component. However  the observation model cannot be easily
speciﬁed nor learned from the expert’s state-action trajectories  which invalidates the use of IRL
for POMDP [5]. Instead  we develop a probabilistic graphical model for representing our gener-
alized IRL problem (Section 2)  from which an EM algorithm can be devised to iteratively select
the locally consistent reward functions as well as learn the stochastic transitions between them in
order to jointly improve the likelihood of the expert’s demonstrated trajectories (Section 3). As a
result  the most likely partition of an expert’s demonstrated trajectory into segments that are gener-
ated from different locally consistent reward functions selected by EM can be derived (Section 3) 
thus enabling practitioners to identify states in which the expert transitions between locally consis-
tent reward functions and investigate the resulting causes. To extend such a partitioning to work
for trajectories traversing through any (possibly unvisited) region of the state space  we propose
using a generalized linear model to represent and predict the stochastic transitions between reward
functions at any state (i.e.  including states not visited in the expert’s demonstrated trajectories) by
exploiting features that inﬂuence these transitions (Section 2). Finally  our proposed IRL algorithm
is empirically evaluated using both synthetic and real-world datasets (Section 4).

2

2 Problem Formulation
A Markov decision process (MDP) for an agent is deﬁned as a tuple (S A  t  r✓  ) consisting of a
ﬁnite set S of its possible states such that each state s 2S is associated with a column vector s
of realized feature measurements  a ﬁnite set A of its possible actions  a state transition function
[0  1] denoting the probability t(s  a  s0)   P (s0|s  a) of moving to state s0
t : S⇥A⇥S !
by performing action a in state s  a reward function r✓ : S! R mapping each state s 2S
to its reward r✓(s)   ✓>s where ✓ is a column vector of reward weights  and constant factor
 2 (0  1) discounting its future rewards. When ✓ is known  the agent can compute its policy
⇡✓ : S⇥A! [0  1] specifying the probability ⇡✓(s  a)   P (a|s  r✓) of performing action a in state
s. However  ✓ is not known in IRL and to be learned from an expert (Section 3).
Let R denote a ﬁnite set of locally consistent reward functions of the agent and re✓ be a reward func-
tion chosen arbitrarily from R prior to learning. Deﬁne a transition function ⌧! : R⇥S⇥R ! [0  1]
for switching between these reward functions as the probability ⌧!(r✓  s  r✓0)   P (r✓0|s  r✓ ! )
in state s where the set !  
of switching from reward function r✓ to reward function r✓0
{!r✓r✓0}r✓2R r✓02R\{re✓} contains column vectors of transition weights !r✓r✓0 for all r✓ 2R and
r✓0 2R \ { re✓} if the features inﬂuencing the stochastic transitions between reward functions can
be additionally observed by the agent during the expert’s demonstration  and !   ; otherwise.
In our generalized IRL problem  ⌧! is not known and to be learned from the expert (Section 3).
Speciﬁcally  in the former case  we propose using a generalized linear model to represent ⌧!:

⌧!(r✓  s  r✓0)  ⇢ exp(!>r✓r✓0

's)/(1 +Pr ¯✓2R\{re✓} exp(!>r✓r ¯✓

1/(1 +Pr ¯✓2R\{re✓} exp(!>r✓r ¯✓

's))

's))

if r✓0 6= re✓ 

otherwise;

(1)

where 's is a column vector of random feature measurements inﬂuencing the stochastic transitions
between reward functions (i.e.  ⌧!) in state s.
Remark 1. Different from s whose feature measurements are typically assumed in IRL algorithms
to be realized/known to the agent for all s 2S and remain static over time  the feature measurements
of 's are  in practice  often not known to the agent a priori and can only be observed when the ex-
pert (agent) visits the corresponding state s 2S during its demonstration (execution)  and may vary
over time according to some unknown distribution  as motivated by the real-world examples given
in Section 1. Without prior observation of the feature measurements of 's for all s 2S (or knowl-
edge of their distributions) necessary for computing ⌧! (1)  the agent cannot consider exploiting ⌧!
for switching between reward functions within MDP or POMDP planning  even after learning its
weights !; this eliminates the possibility of reducing our generalized IRL problem to an equivalent
conventional IRL problem (Section 1) with only a single reward function (i.e.  comprising a mixture
of locally consistent reward functions). Furthermore  the observation model cannot be easily speci-
ﬁed nor learned from the expert’s trajectories of states  actions  and 's  which invalidates the use of
IRL for POMDP [5]. Instead of exploiting ⌧! within planning  during the agent’s execution  when
it visits some state s and observes the feature measurements of 's  it can then use and compute ⌧!
for state s to switch between reward functions  each of which has generated a separate MDP policy
prior to execution  as illustrated in a simple example in Fig. 1 below.
⌧!(r✓0   s  r✓0 )
Remark 2. Using a generalized linear model to represent ⌧! (1) al-
lows learning of the stochastic transitions between reward functions
(speciﬁcally  by learning ! (Section 3)) to be generalized across dif-
ferent states. After learning  (1) can then be exploited for predicting
the stochastic transitions between reward functions at any state (i.e. 
including states not visited in the expert’s demonstrated state-action
trajectories). Consequently  the agent can choose to traverse a trajec-
tory through any region (i.e.  possibly not visited by the expert) of the
state space during its execution and the most likely partition of its tra-
jectory into segments that are generated from different locally consis-
tent reward functions selected by EM can still be derived (Section 3).
In contrast  if the feature measurements of 's cannot be observed by
the agent during the expert’s demonstration (i.e.  ! = ;  as deﬁned above)  then such a generaliza-
tion is not possible; only the transition probabilities of switching between reward functions at states
visited in the expert’s demonstrated trajectories can be estimated (Section 3). In practice  since the
number |S| of visited states is expected to be much larger than the length L of any feature vector 's 

Figure 1: Transition func-
tion ⌧! of an agent in state s
for switching between two
reward functions r✓ and r✓0
with their respective poli-
cies ⇡✓ and ⇡✓0 generated
prior to execution.

⌧!(r✓  s  r✓0 )

⌧!(r✓0   s  r✓)

⌧!(r✓  s  r✓)

r✓

r✓0

3

An
2

Sn
2

Sn
1

t

R✓n

0

R✓n

1

R✓n

2

An
1

···

···

···

Sn
Tn

An
Tn

t )Tn

t )Tn

R✓n
Tn

t1  sn

t where R✓n

t 2R   an

t=1  and sn   (sn

n=1  and s1:N   (sn)N

t 2A   and sn
t   An

t   an
t and Sn
t=0  an   (an

Figure 2: Probabilistic graphical model
of the expert’s n-th demonstrated tra-
jectory encoding its stochastic transi-
tions between reward functions with
solid edges (i.e.  ⌧!(r✓n
t ) =
t1 ! ) for t = 1  . . .   Tn) 
t |sn
P (r✓n
state transitions with dashed edges
(i.e.  t(sn
t   an
t   sn
t )
for t = 1  . . .   Tn  1)  and policy
with dotted edges (i.e.  ⇡✓n
t ) =
P (an

the number O(|S||R|2) of transition probabilities to be estimated is bigger than |!| = O(L|R|2) in
(1). So  observing 's offers a further advantage of reducing the number of parameters to be learned.
Fig. 2 shows the probabilistic graphical model for rep-
resenting our generalized IRL problem. To describe our
model  some notations are necessary: Let N be the num-
ber of the expert’s demonstrated trajectories and Tn be the
length (i.e.  number of time steps) of its n-th trajectory for
n = 1  . . .   N. Let r✓n
t 2S de-
note its reward function  action  and state at time step t
in its n-th trajectory  respectively. Let R✓n
t   and Sn
t
be random variables corresponding to their respective re-
t   and sn
alizations r✓n
is a latent variable 
t are observable variables. Deﬁne r✓n  
and An
t )Tn
t=1 as sequences
(r✓n
of all its reward functions  actions  and states in its n-th
trajectory  respectively. Finally  deﬁne r✓1:N   (r✓n)N
n=1 
a1:N   (an)N
n=1 as tuples of all
its reward function sequences  action sequences  and state
sequences in its N trajectories  respectively.
It can be observed from Fig. 2 that our probabilistic graph-
ical model of the expert’s n-th demonstrated trajectory en-
codes its stochastic transitions between reward functions  state transitions  and policy. Through our
model  the Viterbi algorithm [20] can be applied to derive the most likely partition of the expert’s
trajectory into segments that are generated from different locally consistent reward functions se-
lected by EM  as shown in Section 3. Given the state transition function t(· · ·) and the number
|R| of reward functions  our model allows tractable learning of the unknown parameters using EM
(Section 3)  which include the reward weights vector ✓ for all reward functions r✓ 2R   transition
function ⌧! for switching between reward functions  initial state probabilities ⌫(s)   P (Sn
1 = s)
for all s 2S   and initial reward function probabilities (r✓)   P (R✓n
0 = r✓) for all r✓ 2R .
3 EM Algorithm for Parameter Learning
A straightforward approach to learning the unknown parameters ⇤   (⌫    {✓|r✓ 2R}  ⌧ !) is to
select the value of ⇤ that directly maximizes the log-likelihood of the expert’s demonstrated trajec-
tories. Computationally  such an approach is prohibitively expensive due to a large joint parameter
space to be searched for the optimal value of ⇤. To ease this computational burden  our key idea is
to devise an EM algorithm that iteratively reﬁnes the estimate for ⇤ to improve the expected log-
likelihood instead  which is guaranteed to improve the original log-likelihood by at least as much:
P (r✓1:N|s1:N   a1:N   ⇤i) log P (r✓1:N   s1:N   a1:N|⇤).

t+1|sn
t (sn
t   an
t ) for t = 1  . . .   Tn).

t+1) = P (sn

t |sn

t   r✓n

t   r✓n

t   r✓n

t   an

n=1 log ⌫(sn

n=1Pr✓2R P (R✓n

1 ) +PN
t=1Pr✓2R P (R✓n
t=1Pr✓ r✓02R P (R✓n

Maximization (M) step. ⇤i+1 = argmax⇤ Q(⇤  ⇤i)
where ⇤i denotes an estimate for ⇤ at iteration i. The Q function of EM can be reduced to the
following sum of ﬁve terms  as shown in Appendix A:

Expectation (E) step. Q(⇤  ⇤i)  Pr✓1:N
Q(⇤  ⇤i) =PN
(2)
+PN
n=1PTn
(3)
+PN
n=1PTn
(4)
n=1PTn1
+PN
(5)
Interestingly  each of the ﬁrst four terms in (2)  (3)  and (4) contains a unique unknown parameter
type (respectively  ⌫    {✓|r✓ 2R}   and ⌧!) and can therefore be maximized separately in the
M step to be discussed below. As a result  the parameter space to be searched can be greatly re-
duced. Note that the third term (3) generalizes the log-likelihood in MLIRL [2] (i.e.  assuming all
trajectories to be produced by a single reward function) to that allowing each expert’s trajectory to
be generated by multiple locally consistent reward functions. The last term (5)  which contains the
known state transition function t  is independent of unknown parameters ⇤.1

t = r✓0|sn  an  ⇤i) ⇥ log ⌧!(r✓  sn

t = r✓|sn  an  ⇤i) log ⇡✓(sn

0 = r✓|sn  an  ⇤i) log (r✓)

t1 = r✓  sn
t+1) .

log t(sn

t   an
t )

t   R✓n

t   r✓0)

t   an

t   sn

t=1

1If the state transition function is unknown  then it can be learned by optimizing the last term (5).

4

g1(✓)  

t   an
t )

n=1 I n

P (R✓n

t   an

NXn=1

n=1 P (R✓n

1 = s  and

0 = r✓|sn  an  ⇤i)

1 for all s 2S where I n

t = r✓|sn  an  ⇤i)
⇡✓(sn

1 is an indicator variable of value 1 if sn

Learning initial state probabilities. To maximize the ﬁrst term in the Q function (2) of EM  we

time  it does not have to be reﬁned.
Learning initial reward function probabilities. To maximize the second term in Q function (2) of

use the method of Lagrange multipliers with the constraintPs2S ⌫(s) = 1 to obtain the estimate
b⌫(s) = (1/N )PN
0 otherwise. Sinceb⌫ can be computed directly from the expert’s demonstrated trajectories in O(N )
EM  we utilize the method of Lagrange multipliers with the constraintPr✓2R (r✓) = 1 to derive
(6)
for all r✓i 2R where i+1 denotes an estimate for  at iteration i+1  ✓i denotes an estimate for ✓ at
iteration i  and P (R✓n
n=1 |R|2Tn)
time using a procedure inspired by Baum-Welch algorithm [3]  as shown in Appendix B.
Learning reward functions. The third term in the Q function (3) of EM is maximized using
gradient ascent and its gradient g1(✓) with respect to ✓ is derived to be
d⇡✓(sn
t   an
t )
d✓

i+1(r✓i) = (1/N )PN
t = r✓|sn  an  ⇤i) (in this case  t = 0) can be computed in O(PN

TnXt=1
for all ✓ 2{ ✓0|r✓0 2R} . For ⇡✓(sn
t ) to be differentiable in ✓  we deﬁne the Q✓ function
of MDP using an operator that blends the Q✓ values via Boltzmann exploration [2]: Q✓(s  a)  
✓>s + Ps02S t(s  a  s0) ⌦a0 Q✓(s0  a0) where ⌦aQ✓(s  a)   Pa2A Q✓(s  a) ⇥ ⇡✓(s  a) such
that ⇡✓(s  a)   exp(Q✓(s  a))/Pa02A exp(Q✓(s  a0)) is deﬁned as a Boltzmann exploration
policy  and > 0 is a temperature parameter. Then  we update ✓i+1 ✓i + g1(✓i) where  is the
learning step size. We use backtracking line search method to improve the performance of gradient
ascent. Similar to MLIRL  the time incurred in each iteration of gradient ascent depends mostly on
that of value iteration  which increases with the size of the MDP’s state and action space.
Learning transition function for switching between reward functions. To maximize the fourth
term in the Q function (4) of EM  if the feature measurements of 's cannot be observed by the agent
during the expert’s demonstration (i.e.  ! = ;)  then we utilize the method of Lagrange multipliers
with the constraintsPr✓02R ⌧!(r✓  s  r✓0) = 1 for all r✓ 2R and s 2 S to obtain
(8)
for r✓i  r✓0i 2R and s 2 S where S is the set of states visited by the expert  ⌧!i+1 is an estimate
for ⌧! at iteration i + 1  and n t r✓i  s r ¯✓i   P (R✓n
t = r¯✓|sn  an  ⇤i) can be
computed efﬁciently by exploiting the intermediate results from evaluating P (R✓n
t = r✓|sn  an  ⇤i)
described previously  as detailed in Appendix B.
On the other hand  if the feature measurements of 's can be observed by the agent during the expert’s
demonstration  then recall that we use a generalized linear model to represent ⌧! (1) (Section 2) and
! is the unknown parameter to be estimated. Similar to learning the reward weights vector ✓ for
reward function r✓  we maximize the fourth term (4) in the Q function of EM by using gradient
ascent and its gradient g2(!r✓r✓0 ) with respect to !r✓r✓0 is derived to be

t=1 n t r✓i  s r✓0i )/(Pr ¯✓i2RPN

⌧!i+1(r✓i  s  r✓0i) = (PN

n=1PTn

n=1PTn

t1 = r✓  Sn

t=1 n t r✓i  s r ¯✓i )

t = s  R✓n

(7)

g2(!r✓r✓0 )  

NXn=1

TnXt=1 Xr ¯✓2R

n t r✓ sn
⌧!(r✓  sn

t  r ¯✓
t   r¯✓)

t   r¯✓)

d⌧!(r✓  sn
d!r✓r✓0

(9)

r✓r✓0

r✓r✓0

+ g2(!i

r✓r✓0 !i

n=1 |R|2|S|Tn) time.

denote an estimate for !r✓r✓0 at iteration i. Then  it is updated
for all !r✓r✓0 2 !. Let !i
using !i+1
) where  is the learning step size. Backtracking line search
r✓r✓0
method is also used to improve the performance of gradient ascent here. In both cases  the time
incurred in each iteration i is proportional to the number of n t r✓i  s r ¯✓i to be computed  which is

Viterbi algorithm for partitioning a trajectory into segments with different locally consistent

O(PN
reward functions. Given the ﬁnal estimate b⇤= ( b⌫ b  {b✓|rb✓ 2R}  ⌧b!) for the unknown pa-
rameters ⇤ produced by EM  the most likely partition of the expert’s n-th demonstrated trajectory
t=0  
)Tn
into segments generated by different locally consistent reward functions is r⇤✓n = (r⇤✓n
argmaxr✓n P (r✓n|sn  an b⇤) = argmaxr✓n P (r✓n  sn  an|b⇤)  which can be derived using the
Viterbi algorithm [20]. Speciﬁcally  deﬁne vrb✓ T for T = 1  . . .   Tn as the probability of the most

t

5

0

0   R✓n

0

Tn

P ((r✓n

)T1
t=0

t

= t(sn

)  r⇤✓n

T

1

t )T
t )T

t=1 and (an

t )T

t=1:

t )T

t=1  (an

t )T1

t=0   R✓n

T1  sn
P (r✓n

T   an
1 = r✓  sn

likely reward function sequence (r✓n

t )T1
t=0 from time steps 0 to T  1 ending with reward function
T = r✓  (sn
T ) maxrb✓0
1|b⇤) =b⌫(sn
1   an
1   rb✓) .
1   r⇤✓n
vrb✓ Tn. The above Viterbi algorithm can be applied in the

rb✓ at time step T that produce state and action sequences (sn
t=1|b⇤)
vrb✓ T   max(r✓n
vrb✓0  T1 ⌧b!(rb✓0  sn
T1  an
T ) ⇡b✓(sn
T   rb✓)  
vrb✓ 1   maxr✓n
1 ) ⇡b✓(sn
1 ) maxrb✓0b(rb✓0) ⌧b!(rb✓0  sn
1   an
Then  r⇤✓n
vrb✓0  T ⌧b!(rb✓0  sn
= argmaxrb✓0b(rb✓0) ⌧b!(rb✓0  sn
= argmaxrb✓0
T = 1  . . .   Tn  1  and r⇤✓n
= argmaxrb✓
same way to partition an agent’s trajectory traversing through any region (i.e.  possibly not visited
by the expert) of the state space during its execution in O(|R|2T ) time.
4 Experiments and Discussion
This section evaluates the empirical performance of our IRL algorithm using 3 datasets featuring
experts’ demonstrated trajectories in two simulated grid worlds and real-world taxi trajectories. The
average log-likelihood of the expert’s demonstrated trajectories is used as the performance metric
because it inherently accounts for the ﬁdelity of our IRL algorithm in learning the locally consistent
reward functions (i.e.  R) and the stochastic transitions between them (i.e.  ⌧!):
(10)
where Ntot is the total number of the expert’s demonstrated trajectories available in the dataset.
As proven in [17]  maximizing L(⇤) with respect to ⇤ is equivalent to minimizing an empirical ap-
proximation of the Kullback-Leibler divergence between the distributions of the agent’s and expert’s

L(⇤)   (1/Ntot)PNtot

n=1 log P (sn  an|⇤)

T +1  r⇤✓n

) for

T +1

B

A

generated state-action trajectories. Note that when the ﬁnal estimateb⇤ produced by EM (Section 3)
is plugged into (10)  the resulting P (sn  an|b⇤) in (10) can be computed efﬁciently using a procedure

similar to that in Section 3  as detailed in Appendix C. To avoid local maxima in gradient ascent 
we initialize our EM algorithm with 20 random ⇤0 values and report the best result based on the Q
value of EM (Section 3).
To demonstrate the importance of modeling and learning
stochastic transitions between locally consistent reward func-
tions  the performance of our IRL algorithm is compared with
that of its reduced variant assuming no change/switching of
reward function within each trajectory  which is implemented
by initializing ⌧!(r✓  s  r✓) = 1 for all r✓ 2R and s 2S
and deactivating the learning of ⌧!. In fact  it can be shown
(Appendix D) that such a reduction  interestingly  is equiva-
lent to EM clustering with MLIRL [2]. So  our IRL algorithm
generalizes EM clustering with MLIRL  the latter of which
has been empirically demonstrated in [2] to outperform many
existing IRL algorithms  as discussed in Section 1.
Simulated grid world A. The environment (Fig. 3) is modeled as a 5 ⇥ 5 grid of states  each of
which is either land  water  water and destination  or obstacle associated with the respective feature
vectors (i.e.  s) (0  1  0)>  (1  0  0)>  (1  0  1)>  and (0  0  0)>. The expert starts at origin (0  2)
and any of its actions can achieve the desired state with 0.85 probability. It has two possible reward
functions  one of which prefers land to water and going to destination (i.e.  ✓ = (0  20  30)>)  and
the other of which prefers water to land and going to destination (i.e.  ✓0 = (20  0  30)>). The expert
will only consider switching its reward function at states (2  0) and (2  4) from r✓0 to r✓ with 0.5
probability and from r✓ to r✓0 with 0.7 probability; its reward function remains unchanged at all
other states. The feature measurements of 's cannot be observed by the agent during the expert’s
demonstration. So  ! = ; and ⌧! is estimated using (8). We set  to 0.95 and the number |R| of
reward functions of the agent to 2.
Fig. 4a shows results of the average log-likelihood L (10) achieved by our IRL algorithm  EM
clustering with MLIRL  and the expert averaged over 4 random instances with varying number
N of expert’s demonstrated trajectories. It can be observed that our IRL algorithm signiﬁcantly
outperforms EM clustering with MLIRL and achieves a L performance close to that of the expert 
especially when N increases. This can be explained by its modeling of ⌧! and its high ﬁdelity in
learning and predicting ⌧!: While our IRL algorithm allows switching of reward function within
each trajectory  EM clustering with MLIRL does not.

Figure 3: Grid worlds A (states
(0  0)  (1  1)  and (2  2) are  respec-
tively  examples of water  land  and
obstacle)  and B (state (2  2) is an
example of barrier). ‘O’ and ‘D’ de-
note origin and destination.

6

0123401234OD0123401234OD−24

−27

−28

−29

0

−15

−17

−22

−23

−25

−26

400

500

600

−19

100

200

300

−21

0

200

400

600

800

1000

1200

1400

1600

−23

−25

(a)

(b)

d
o
o
h
i
l
e
k
i
l

−
g
o
l
 
e
g
a
r
e
v
A

d
o
o
h
i
l
e
k
i
l

−
g
o
l
 
e
g
a
r
e
v
A

No. of demonstrated trajectories

No. of demonstrated trajectories

t1 = r✓ (R✓n

Our IRL algorithm
EM clustering with MLIRL
Expert

Our IRL algorithm
EM clustering with MLIRL
Expert

Figure 4: Graphs of average log-likelihood L achieved by our
IRL algorithm  EM clustering with MLIRL  and the expert vs.
number N of expert’s demonstrated trajectories in simulated
grid worlds (a) A (Ntot = 1500) and (b) B (Ntot = 500).

We also observe that the accuracy of
estimating the transition probabili-
ties ⌧!(r✓  s  .) (⌧!(r✓0  s  .)) using
(8) depends on the frequency and
distribution of trajectories demon-
strated by the expert with its reward
t1 = r✓0)
function R✓n
at time step t1 and its state sn
t = s
at time step t  which is expected.
Those transition probabilities that
are poorly estimated due to few rel-
evant expert’s demonstrated trajec-
tories  however  do not hurt the L
performance of our IRL algorithm by much because such trajectories tend to have very low prob-
ability of being demonstrated by the expert. In any case  this issue can be mitigated by using the
generalized linear model (1) to represent ⌧! and observing the feature measurements of 's necessary
for learning and computing ⌧!  as shown next.
Simulated grid world B. The environment (Fig. 3) is also modeled as a 5 ⇥ 5 grid of states  each
of which is either the origin  destination  or land associated with the respective feature vectors (i.e.
s) (0  1)>  (1  0)>  and (0  0)>. The expert starts at origin (4  0) and any of its actions can achieve
the desired state with 0.85 probability. It has two possible reward functions  one of which prefers
going to destination (i.e.  ✓ = (30  0)>)  and the other of which prefers returning to origin (i.e.  ✓0 =
(0  30)>). While moving to the destination  the expert will encounter barriers at some states with
corresponding feature vectors 's = (1  1)> and no barriers at all other states with 's = (0  1)>; the
second component of 's is used as an offset value in the generalized linear model (1). The expert’s
behavior of switching between reward functions is governed by a generalized linear model ⌧! (1)

with re✓ = r✓0 and transition weights !r✓r✓ = (11  12)> and !r✓0 r✓ = (13 12)>. As a result 
it will  for example  consider switching its reward function at states with barriers from r✓ to r✓0
with 0.269 probability. We estimate ⌧! using (9) and set  to 0.95 and the number |R| of reward
functions of the agent to 2. To assess the ﬁdelity of learning and predicting the stochastic transitions
between reward functions at unvisited states  we intentionally remove all demonstrated trajectories
that visit state (2  0) with a barrier.
Fig. 4b shows results of L (10) performance achieved by our IRL algorithm  EM clustering with
MLIRL  and the expert averaged over 4 random instances with varying N. It can again be observed
that our IRL algorithm outperforms EM clustering with MLIRL and achieves an L performance
comparable to that of the expert due to its modeling of ⌧! and its high ﬁdelity in learning and
predicting ⌧!: While our IRL algorithm allows switching of reward function within each trajectory 

EM clustering with MLIRL does not. Besides  the estimated transition function ⌧b! using (9) is very

close to that of the expert  even at unvisited state (2  0). So  unlike using (8)  the learning of ⌧! with
(9) can be generalized well across different states  thus allowing ⌧! to be predicted accurately at any
state. Hence  we will model ⌧! with (1) and learn it using (9) in the next experiment.
Real-world taxi trajectories. The Comfort taxi company in Singapore has provided GPS traces of
59 taxis with the same origin and destination that are map-matched [18] onto a network (i.e.  com-
prising highway  arterials  slip roads  etc) of 193 road segments (i.e.  states). Each road segment/state
is speciﬁed by a 7-dimensional feature vector s: Each of the ﬁrst six components of s is an indi-
cator describing whether it belongs to Alexandra Road (AR)  Ayer Rajah Expressway (AYE)  Depot
Road (DR)  Henderson Road (HR)  Jalan Bukit Merah (JBM)  or Lower Delta Road (LDR)  while
the last component of s is the normalized shortest path distance from the road segment to desti-
nation. We assume that the 59 map-matched trajectories are demonstrated by taxi drivers with a
common set R of 2 reward functions and the same transition function ⌧! (1) for switching between
reward functions  the latter of which is inﬂuenced by the normalized taxi speed constituting the ﬁrst
component of 2-dimensional feature vector 's; the second component of 's is used as an offset of
value 1 in the generalized linear model (1). The number |R| of reward functions is set to 2 because
when we experiment with |R| = 3  two of the learned reward functions are similar. Every driver can
deterministically move its taxi from its current road segment to the desired adjacent road segment.

7

Fig. 5a shows results of L (10)
performance achieved by our IRL
algorithm and EM clustering with
MLIRL averaged over 3 random in-
stances with varying N. Our IRL
algorithm outperforms EM cluster-
ing with MLIRL due to its modeling
of ⌧! and its high ﬁdelity in learning
and predicting ⌧!.
To see this  our IRL algorithm is
able to learn that a taxi driver is
likely to switch between reward
functions representing different in-

d
o
o
h
i
l
e
k
i
l

−
g
o
l
 
e
g
a
r
e
v
A

−4

−4.5

−5

−5.5

−6

−6.5

−7

−7.5

−8

Our IRL algorithm
EM clustering with MLIRL

10

20

30

40

50

60

No. of demonstrated trajectories

(a)

1

0.8

0.6

0.4

0.2

y
t
i
l
i
b
a
b
o
r
P

0
 
0

0.2

Normalized taxi speed

0.6

0.4
(b)

Figure 5: Graphs of (a) average log-likelihood L achieved by
our IRL algorithm and EM clustering with MLIRL vs. no. N
of taxi trajectories (Ntot = 59) and (b) transition probabilities
of switching between reward functions vs. taxi speed.

 

)
)

⌧b!(rb✓  s  rb✓0
⌧b!(rb✓0
  s  rb✓0

0.8

1

(Fig. 6b) due to large rewards for traveling on them (respectively  reward weights 30.5 and 23.7).
As an example  Fig. 6c shows the most likely partition of a demonstrated trajectory into segments

along DR  HR  and JBM to destination. On the other hand  the reward functions learned by EM
clustering with MLIRL are both associated with his intention of driving directly to destination (i.e. 

directly to the destination (Fig. 6a) due to a huge penalty (i.e.  reward weight -49) on being far
from destination and a large reward (i.e.  reward weight 35.7) for taking the shortest path from ori-

tentions within its demonstrated trajectory: Reward function rb✓ denotes his intention of driving
gin to destination  which is via JBM  while rb✓0 denotes his intention of detouring to DR or JBM
generated from locally consistent reward functions rb✓ and rb✓0  which is derived using our Viterbi
algorithm (Section 3). It can be observed that the driver is initially in rb✓0 on the slip road exiting
AYE  switches from rb✓0 to rb✓ upon turning into AR to detour to DR  and remains in rb✓ while driving
similar to rb✓); it is not able to learn his intention of detouring to DR or JBM.
timated transition function ⌧b! using (9).
rb✓ (i.e.  driving directly to destination)  he is
less of taxi speed. But  when he is in rb✓0 (i.e. 

Fig. 5b shows the inﬂuence of normalized taxi
speed (i.e.  ﬁrst component of 's) on the es-
It
can be observed that when the driver is in

very unlikely to change his intention regard-

LDR

(a)

AYE

DR

JBM

O

O

AR

AR

HR

D

detouring to DR or JBM)  he is likely (un-
likely) to remain in this intention if taxi speed
is low (high). The demonstrated trajectory in
Fig. 6c in fact supports this observation: The

DR

HR

D

LDR

drive at relatively high speed on ﬂat terrain.

slip road exiting AYE  which causes the low
taxi speed. Upon turning into AR to detour to

driver initially remains in rb✓0 on the upslope
DR  he switches from rb✓0 to rb✓ because he can
5 Conclusion
This paper describes an EM-based IRL al-
gorithm that can learn the multiple reward
functions being locally consistent in differ-
ent segments along a trajectory as well as the
stochastic transitions between them.
It gen-
eralizes EM-clustering with MLIRL and has
been empirically demonstrated to outperform
it on both synthetic and real-world datasets.
For our future work  we plan to extend our IRL algorithm to cater to an unknown number of reward
functions [6]  nonlinear reward functions [12] modeled by Gaussian processes [4  8  13  14  15  25] 
other dissimilarity measures described in Section 1  linearly-solvable MDPs [7]  active learning with
Gaussian processes [11]  and interactions with self-interested agents [9  10].
Acknowledgments. This work was partially supported by Singapore-MIT Alliance for Research
and Technology Subaward Agreement No. 52 R-252-000-550-592.

Figure 6: Reward (a) rb✓(s) and (b) rb✓0(s) for each
road segment s withb✓ = (7.4  3.9  16.3  20.3  35.7 
21.5  49.0)> andb✓0 = (5.2  9.2  30.5  15.0  23.7 
21.5  9.2)> such that more red road segments
give higher rewards. (c) Most likely partition of a
demonstrated trajectory from origin ‘O’ to destina-
tion ‘D’ into red and green segments generated by
rb✓ and rb✓0  respectively.

JBM

AYE

(b)

8

(c)OARDRAYEJBMHRLDRDReferences
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML 

2004.

[2] M. Babes¸-Vroman  V. Marivate  K. Subramanian  and M. Littman. Apprenticeship learning about multiple

intentions. In Proc. ICML  pages 897–904  2011.

[3] J. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaus-
sian mixture and Hidden Markov models. Technical Report ICSI-TR-97-02  University of California 
Berkeley  1998.

[4] J. Chen  N. Cao  K. H. Low  R. Ouyang  C. K.-Y. Tan  and P. Jaillet. Parallel Gaussian process regression

with low-rank covariance matrix approximations. In Proc. UAI  pages 152–161  2013.

[5] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. JMLR  12:691–

730  2011.

[6] J. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward func-

tions. In Proc. NIPS  pages 314–322  2012.

[7] K. Dvijotham and E. Todorov.

pages 335–342  2010.

Inverse optimal control with linearly-solvable MDPs.

In Proc. ICML 

[8] T. N. Hoang  Q. M. Hoang  and K. H. Low. A unifying framework of anytime sparse Gaussian process
regression models with stochastic variational inference for big data. In Proc. ICML  pages 569–578  2015.
[9] T. N. Hoang and K. H. Low. A general framework for interacting Bayes-optimally with self-interested

agents using arbitrary parametric model and model prior. In Proc. IJCAI  pages 1394–1400  2013.

[10] T. N. Hoang and K. H. Low. Interactive POMDP Lite: Towards practical planning to predict and exploit

intentions for interacting with self-interested agents. In Proc. IJCAI  pages 2298–2305  2013.

[11] T. N. Hoang  K. H. Low  P. Jaillet  and M. Kankanhalli. Nonmyopic ✏-Bayes-optimal active learning of

Gaussian processes. In Proc. ICML  pages 739–747  2014.

[12] S. Levine  Z. Popovi´c  and V. Koltun. Nonlinear inverse reinforcement learning with Gaussian processes.

In Proc. NIPS  pages 19–27  2011.

[13] K. H. Low  J. Chen  T. N. Hoang  N. Xu  and P. Jaillet. Recent advances in scaling up Gaussian process
In S. Ravela and A. Sandu  editors  Proc. Dynamic

predictive models for large spatiotemporal data.
Data-driven Environmental Systems Science Conference (DyDESS’14). LNCS 8964  Springer  2015.

[14] K. H. Low  N. Xu  J. Chen  K. K. Lim  and E. B. ¨Ozg¨ul. Generalized online sparse Gaussian processes

with application to persistent mobile robot localization. In Proc. ECML/PKDD Nectar Track  2014.

[15] K. H. Low  J. Yu  J. Chen  and P. Jaillet. Parallel Gaussian process regression for big data: Low-rank

representation meets Markov approximation. In Proc. AAAI  pages 2821–2827  2015.

[16] G. Neu and C. Szepesv´ari. Apprenticeship learning using inverse reinforcement learning and gradient

methods. In Proc. UAI  pages 295–302  2007.

[17] G. Neu and C. Szepesv´ari. Training parsers by inverse reinforcement learning. Machine Learning  77(2–

3):303–337  2009.

[18] P. Newson and J. Krumm. Hidden Markov map matching through noise and sparseness. In Proc. 17th
ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems  pages
336–343  2009.

[19] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proc. ICML  2000.
[20] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc.

IEEE  77(2):257–286  1989.

[21] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proc. IJCAI  pages 2586–

2591  2007.

[22] S. Russell. Learning agents for uncertain environments. In Proc. COLT  pages 101–103  1998.
[23] U. Syed  M. Bowling  and R. E. Schapire. Apprenticeship learning using linear programming. In Proc.

ICML  pages 1032–1039  2008.

[24] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proc. NIPS  pages

1449–1456  2007.

[25] N. Xu  K. H. Low  J. Chen  K. K. Lim  and E. B. ¨Ozg¨ul. GP-Localize: Persistent mobile robot localization

using online sparse Gaussian process observation model. In Proc. AAAI  pages 2585–2592  2014.

[26] J. Yu  K. H. Low  A. Oran  and P. Jaillet. Hierarchical Bayesian nonparametric approach to modeling and
learning the wisdom of crowds of urban trafﬁc route planning agents. In Proc. IAT  pages 478–485  2012.
[27] B. D. Ziebart  A. Maas  J. A. Bagnell  and A. K. Dey. Maximum entropy inverse reinforcement learning.

In Proc. AAAI  pages 1433–1438  2008.

9

,Martin Azizyan
Aarti Singh
Larry Wasserman
Quoc Phong Nguyen
Bryan Kian Hsiang Low
Patrick Jaillet