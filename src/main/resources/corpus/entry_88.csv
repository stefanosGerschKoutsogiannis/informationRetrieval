2019,A General Theory of Equivariant CNNs on Homogeneous Spaces,We present a general theory of Group equivariant Convolutional Neural Networks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature maps in these networks represent fields on a homogeneous base space  and layers are equivariant maps between spaces of fields. The theory enables a systematic classification of all existing G-CNNs in terms of their symmetry group  base space  and field type. We also answer a fundamental question: what is the most general kind of equivariant linear map between feature spaces (fields) of given types? We show that such maps correspond one-to-one with generalized convolutions with an equivariant kernel  and characterize the space of such kernels.,A General Theory of Equivariant CNNs on

Homogeneous Spaces

Taco S. Cohen

Qualcomm AI Research∗

Mario Geiger

PCSL Research Group

Qualcomm Technologies Netherlands B.V.

EPFL

tacos@qti.qualcomm.com

mario.geiger@epfl.ch

Maurice Weiler

QUVA Lab

U. of Amsterdam
m.weiler@uva.nl

Abstract

We present a general theory of Group equivariant Convolutional Neural Networks
(G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature
maps in these networks represent ﬁelds on a homogeneous base space  and layers
are equivariant maps between spaces of ﬁelds. The theory enables a systematic
classiﬁcation of all existing G-CNNs in terms of their symmetry group  base
space  and ﬁeld type. We also consider a fundamental question: what is the most
general kind of equivariant linear map between feature spaces (ﬁelds) of given
types? Following Mackey  we show that such maps correspond one-to-one with
convolutions using equivariant kernels  and characterize the space of such kernels.

1

Introduction

Through the use of convolution layers  Convolutional Neural Networks (CNNs) have a built-in
understanding of locality and translational symmetry that is inherent in many learning problems.
Because convolutions are translation equivariant (a shift of the input leads to a shift of the output) 
convolution layers preserve the translation symmetry. This is important  because it means that further
layers of the network can also exploit the symmetry.

Motivated by the success of CNNs  many researchers have worked on generalizations  leading to a
growing body of work on Group equivariant CNNs (G-CNNs) for signals on Euclidean space and
the sphere [1–7] as well as graphs [8  9]. With the proliferation of equivariant network layers  it has
become difﬁcult to see the relations between the various approaches. Furthermore  when faced with
a new modality (diffusion tensor MRI  say)  it may not be immediately obvious how to create an
equivariant network for it  or whether a given kind of equivariant layer is the most general one.

In this paper we present a general theory of homogeneous G-CNNs. Feature spaces are modelled
as spaces of ﬁelds on a homogeneous space. They are characterized by a group of symmetries
G  a subgroup H ≤ G that together with G determines a homogeneous space B ≃ G/H  and a
representation ρ of H that determines the type of ﬁeld (vector  tensor  etc.). Related work is classiﬁed
by (G  H  ρ). The main theorems say that equivariant linear maps between ﬁelds over B can be
written as convolutions with an equivariant kernel  and that the space of equivariant kernels can be
realized in three equivalent ways. We will assume some familiarity with groups  cosets  quotients 
representations and related notions (see Appendix A).

This paper does not contain truly new mathematics (in the sense that a professional mathematician
with expertise in the relevant subjects would not be surprised by our results)  but instead provides
a new formalism for the study of equivariant convolutional networks. This formalism turns out to
be a remarkably good ﬁt for describing real-world G-CNNs. Moreover  by describing G-CNNs in a
language used throughout modern physics and mathematics (ﬁelds  ﬁber bundles  etc.)  it becomes
possible to apply knowledge gained over many decades in those domains to machine learning.

*Qualcomm AI Research is an initiative of Qualcomm Technologies  Inc.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Overview of the Theory

This paper has two main parts. First  in Sec. 2  we introduce a mathematical model for convolutional
feature spaces. The basic idea is that feature maps represent ﬁelds over a homogeneous space. As it
turns out  deﬁning the notion of a ﬁeld is quite a bit of work. So in order to motivate the introduction
of each of the required concepts  we will in this section provide an overview of the relevant concepts
and their relations  using the example of a Spherical CNN with vector ﬁeld feature maps.

The second part of this paper (Section 3) is about maps between the feature spaces. We require these
to be equivariant  and focus in particular on the linear layers. The main theorems (3.1–3.4) show that
linear equivariant maps between the feature spaces are in one-to-one correspondence with equivariant
convolution kernels (i.e. convolution is all you need)  and that the space of equivariant kernels can be
realized as a space of matrix-valued functions on a group  coset space  or double coset space  subject
to linear constraints.

In order to specify a convolutional feature space  we need to specify two things: a homogeneous
space B over which the ﬁeld is deﬁned  and the type of ﬁeld (e.g. vector ﬁeld  tensor ﬁeld  etc.). A
homogeneous space for a group G is a space B where for any two x  y ∈ B there is a transformation
g ∈ G that relates them via gx = y. Here we consider the example of a vector ﬁeld on the sphere
B = S2 with symmetry group G = SO(3)  the group of 3D rotations. The sphere is a homogeneous
space for SO(3) because we can map any point on the sphere to any other via a rotation.

Formally  a ﬁeld is deﬁned as a section of a vector bundle associated to a principal bundle. In order
to understand what this means  we must ﬁrst know what a ﬁber bundle is (Sec. 2.1)  and understand
how the group G can be viewed as a principal bundle (Sec. 2.2). Brieﬂy  a ﬁber bundle formalizes the
idea of parameterizing a set of identical spaces called ﬁbers by another space called the base space.

The ﬁrst way in which ﬁber bundles play a role in the theory is that the action
of G on B allows us to think of G as a “bundle of groups” or principal bundle.
Roughly speaking  this works as follows: if we ﬁx an origin o ∈ B  we
can consider the stabilizer subgroup H ≤ G of transformations that leave o
unchanged: H = {g ∈ G | go = o}. For example  on the sphere the stabilizer
is SO(2)  the group of rotations around the axis through o (e.g. the north pole).
As we will see in Section 2.2  this allows us to view G as a bundle with base
space B ≃ G/H and a ﬁber H. This is shown for the sphere in Fig. 1 (cartoon).
In this case  we can think of SO(3) as a bundle of circles (H = SO(2)) over
the sphere  which itself is the quotient S2 ≃ SO(3)/ SO(2).

Figure 1: SO(3) as
a principal SO(2)
bundle over S2.

To deﬁne the associated bundle (Sec. 2.3) we take the principal bundle G and
replace the ﬁber H by a vector space V on which H acts linearly via a group
representation ρ. This yields a vector bundle with the same base space B and
a new ﬁber V . For example  the tangent bundle of S2 (Fig. 2) is obtained by
replacing the circular SO(2) ﬁbers in Fig. 1 by 2D planes. Under the action
of H = SO(2)  a tangent vector at the north pole is rotated (even though
the north pole itself is ﬁxed by SO(2))  so we let ρ(h) be a 2 × 2 rotation
matrix. In a general convolutional feature space with n channels  V would be
an n-dimensional vector space. Finally  ﬁelds are deﬁned as sections of this

Figure 2: Tangent
bundle of S2.

bundle  i.e. an assignment to each point x of an element in the ﬁber over x (see Fig. 3).

Having deﬁned the feature space  we need to
specify how it transforms (e.g. say how a vector
ﬁeld on S2 is rotated). The natural way to trans-
form a ρ-ﬁeld is via the induced representation
π = IndG
H ρ of G (Section 2.4)  which combines
the action of G on the base space B and the ac-
tion of ρ on the ﬁber V to produce an action on
sections of the associated bundle (See Figure 3).
Finally  having deﬁned the feature spaces and
their transformation laws  we can study equiv-
ariant linear maps between them (Section 3). In
Sec. 4–6 we cover implementation aspects  re-
lated work  and concrete examples  respectively.

Figure 3: Φ maps scalar ﬁelds to vector ﬁelds 
and is equivariant to the induced representation
πi = IndSO(3)

SO(2) ρi.

2

2 Convolutional Feature Spaces

2.1 Fiber Bundles

Intuitively  a ﬁber bundle is a parameterization of a set of isomorphic spaces (the ﬁbers) by another
space (the base). For example  we can think of a feature space in a classical CNN as a set of vector
spaces Vx ≃ Rn (n being the number of channels)  one per position x in the plane [2]. This is an
example of a trivial bundle  because it is simply the Cartesian product of the plane and Rn. General
ﬁber bundles are only locally trivial  meaning that they locally look like a product while having a
different global topological structure.

The simplest example of a non-trivial bundle is the Mobius strip  which
locally looks like a product of the circle (the base) with a line segment
(the ﬁber)  but is globally distinct from a cylinder (see Fig. 4). A more
practically relevant example is given by the tangent bundle of the sphere
(Fig. 2)  which has as base space S2 and ﬁbers that look like R2  but is
topologically distinct from S2 × R2 as a bundle.

Figure 4: Cylinder and
Möbius strip

Formally  a bundle consists of topological spaces E (total space)  B (base space)  F (canonical
ﬁber)  and a projection map p : E → B  satisfying a local triviality condition. Basically  this
condition says that locally  the bundle looks like a product U × F of a piece U ⊆ B of the base
space  and F the canonical ﬁber. Formally  the condition is that for every a ∈ E  there is an
open neighbourhood U ⊆ B of p(a) and a homeomorphism ϕ : p−1(U ) → U × F so that the
proj1−−−→ U agrees with p : p−1(U ) → U (where proj1(u  f ) = u). The
map p−1(U )
homeomorphism ϕ is said to locally trivialize the bundle above the trivializing neighbourhood U .

ϕ
−→ U × F

−1(x) is F   and ϕ is a homeomorphism  we
Considering that for any x ∈ U the preimage proj1
see that the preimage Fx = p−1(x) for x ∈ B is also homeomorphic to F . Thus  we call Fx the
ﬁber over x  and see that all ﬁbers are homeomorphic. Knowing this  we can denote a bundle by its
projection map p : E → B  leaving the canonical ﬁber F implicit.

Various more reﬁned notions of ﬁber bundle exist  each corresponding to a different kind of ﬁber. In
this paper we will work with principal bundles (bundles of groups) and vector bundles (bundles of
vector spaces).

A section s of a ﬁber bundle is an assignment to each x ∈ B of an element s(x) ∈ Fx. Formally  it is
a map s : B → E that satisﬁes p◦s = idB. If the bundle is trivial  a section is equivalent to a function
f : B → F   but for a non-trivial bundle we cannot continuously align all the ﬁbers simultaneously 
and so we must keep each s(x) in its own ﬁber Fx. Nevertheless  on a trivializing neighbourhood
U ⊆ B  we can describe the section as a function sU : U → F   by setting ϕ(s(x)) = (x  sU (x)).

2.2 G as a Principal H-Bundle

Recall (Sec. 1.1) that with every feature space of a G-CNN is associated a homogeneous space
B (e.g. the sphere  projective space  hyperbolic space  Grassmann & Stiefel manifolds  etc.)  and
recall further that such a space has a stabilizer subgroup H = {g ∈ G | go = o} (this group being
independent of origin o up to isomorphism). As discussed in Appendix A  the cosets gH of H (e.g.
the circles in Fig. 1) partition G  and the set of cosets  denoted G/H (e.g. the sphere in Fig. 1)  can
be identiﬁed with B (up to a choice of origin).

It is this partitioning of G into cosets that induces a special kind of bundle structure on G. The
projection map that deﬁnes the bundle structure sends an element g ∈ G to the coset gH it belongs
to. Thus  it is a map p : G → G/H  and we have a bundle with total space G  base space G/H and
canonical ﬁber H. Intuitively  this allows us to think of G as a base space G/H with a copy of H
attached at each point x ∈ G/H. The copies of H are glued together in a potentially twisted manner.

This bundle is called a principal H-bundle  because we have a transitive and ﬁxed-point free group
action G × H → G that preserves the ﬁbers. This action is given by right multiplication  g 7→ gh 
which preserves ﬁbers because p(gh) = ghH = gH = p(g). That is  by right-multiplying an
element g ∈ G by h ∈ H  we get an element gh that is in general different from g but is still within
the same coset (i.e. ﬁber). That the action is transitive and free on cosets follows immediately from
the group axioms.

3

One can think of a principal bundle as a bundle of generalized frames or gauges relative to which
geometrical quantities can be expressed numerically. Under this interpretation the ﬁber at x is a space
of generalized frames  and the action by H is a change of frame. For instance  each point on the
circles in Fig. 1 can be identiﬁed with a right-handed orthogonal frame  and the action of SO(2)
corresponds to a rotation of this frame. The group H may also include internal symmetries  such as
color space rotations  which do not relate in any way to the spatial dimensions of B.

In order to numerically represent a ﬁeld on some neighbourhood U ⊆ G/H  we need to choose a
frame for each x ∈ U in a continuous manner. This is formalized as a section of the principal bundle.
Recall that a section of p : G → G/H is a map s : G/H → G that satisﬁes p ◦ s = idG/H . Since
p projects g to its coset gH  the section chooses a representative s(gH) ∈ gH for each coset gH.
Non-trivial principal bundles do not have continuous global sections  but we can always use a local
section on U ⊆ G/H  and represent a ﬁeld on overlapping local patches covering G/H.

Aside from the right action of H  which turns G into a principal H-bundle  we also have a left
action of G on itself  as well as an action of G on the base space G/H. In general  the action of
G on G/H does not agree with the action on G  in that gs(x) 6= s(gx)  because the action on G
includes a twist of the ﬁber. This twist is described by the function h : G/H × G → H deﬁned by
gs(x) = s(gx)h(x  g) (whenever both s(x) and s(gx) are deﬁned). This function will be used in
various calculations below. We note for the interested reader that h satisﬁes the cocycle condition
h(x  g1g2) = h(g2x  g1)h(x  g2).

2.3 The Associated Vector Bundle

Feature spaces are deﬁned as spaces of sections of the associated vector bundle  which we will now
deﬁne. In physics  a section of an associated bundle is simply called a ﬁeld.

p
To deﬁne the associated vector bundle  we start with the principal H-bundle G
−→ G/H  and
essentially replace the ﬁbers (cosets) by vector spaces V . The space V ≃ Rn carries a group
representation ρ of H that describes the transformation behaviour of the feature vectors in V under a
change of frame. These features could for instance transform as a scalar  a vector  a tensor  or some
other geometrical quantity [2  6  8]. Figure 3 shows an example of a vector ﬁeld (ρ(h) being a 2 × 2
rotation matrix in this case) and a scalar ﬁeld (ρ(h) = 1).

The ﬁrst step in constructing the associated vector bundle is to take the product G × V . In the context
of representation learning  we can think of an element (g  v) of G × V as a feature vector v ∈ V and
an associated pose variable g ∈ G that describes how the feature detector was steered to obtain v.
For instance  in a Spherical CNN [10] one would rotate a ﬁlter bank by g ∈ SO(3) and match it with
the input to obtain v. If we apply a transformation h ∈ H to g and simultaneously apply its inverse
to v  we get an equivalent element (gh  ρ(h−1)v). In a Spherical CNN  this would correspond to a
change in orientation of the ﬁlters by h ∈ SO(2).

So in order to create the associated bundle  we take the quotient of the product G × V by this
action: A = G ×ρ V = (G × V )/H. In other words  the elements of A are orbits  deﬁned as
[g  v] = {(gh  ρ(h−1)v) | h ∈ H}. The projection pA : A → G/H is deﬁned as pA([g  v]) = gH.
One may check that this is well deﬁned  i.e. independent of the orbit representative g of [g  v] =
[gh  ρ(h−1)v]. Thus  the associated bundle has base G/H and ﬁber V   meaning that locally it looks
like G/H × V . We note that the associated bundle construction works for any principal H-bundle 
nog just p : G → G/H  which suggests a direction for further generalization [11].

A ﬁeld (“stack of feature maps”) is a section of the associated bundle  meaning that it is a map
s : G/H → A such that πρ ◦ s = idG/H . We will refer to the space of sections of the associated
vector bundle as I. Concretely  we have two ways to encode a section: as functions f : G → V
subject to a constraint  and as local functions from U ⊆ G/H to V . We will now deﬁne both.

2.3.1 Sections as Mackey Functions

The construction of the associated bundle as a product G × V subject to an equivalence relation
suggests a way to describe sections concretely: a section can be represented by a function f : G → V
subject to the equivariance condition

f (gh) = ρ(h−1)f (g).

(1)

4

Such functions are called Mackey functions. They provide a redundant encoding of a section of A 
by encoding the value of the section relative to any choice of frame / section of the principal bundle
simultaneously  with the equivariance constraint ensuring consistency.

A linear combination of Mackey functions is a Mackey function  so they form a vector space  which we
will refer to as IG. Mackey functions are easy to work with because they allow a concrete and global
description of a ﬁeld  but their redundancy makes them unsuitable for computer implementation.

2.3.2 Local Sections as Functions on G/H

The associated bundle has base G/H and ﬁber V   so locally  we can describe a section as an
unconstrained function f : U → V where U ⊆ G/H is a trivializing neighbourhood (see Sec. 2.1).
We refer to the space of such sections as IC . Given a local section f ∈ IC   we can encode it as a
Mackey function through the following lifting isomorphism Λ : IC → IG:

[Λf ](g) = ρ(h(g)−1)f (gH) 

[Λ−1f ′](x) = f ′(s(x)) 

(2)

where h(g) = h(H  g) = s(gH)−1g ∈ H and s(x) ∈ G is a coset representative for x ∈ G/H.
This map is analogous to the lifting deﬁned by [12] for scalar ﬁelds (i.e. ρ(h) = I)  and can be
deﬁned more generally for any principal / associated bundle [13].

2.4 The Induced Representation

The induced representation π = IndG

H ρ describes the action of G on ﬁelds. In IG  it is deﬁned as:
[πG(g)f ](k) = f (g−1k).
(3)

In IC   we can deﬁne the induced representation πC on a local neighbourhood U as

[πC(g)f ](x) = ρ(h(g−1  x)−1)f (g−1x).

(4)

Here we have assumed that h is deﬁned at (g−1  x).
is not  one would need to
change to a different section of G → G/H. One may verify  using the composition
4 does indeed deﬁne a representation of G. Moreover 
law for h (Sec.
one may verify that πG(g) ◦ Λ = Λ ◦ πC(g)  i.e.
they deﬁne isomorphic representations.

that Eq.

2.2) 

If it

We can interpret Eq. 4 as follows. To transform a ﬁeld 
we move the ﬁber at g−1x to x  and we apply a trans-
formation to the ﬁber itself using ρ. This is visualized
in Fig. 5 for a planar vector ﬁeld. Some other exam-
ples include an RGB image (ρ(h) = I3)  a ﬁeld of wind
directions on earth (ρ(h) a 2 × 2 rotation matrix)  a diffu-
sion tensor MRI image (ρ(h) a representation of SO(3)
acting on 2-tensors)  a regular G-CNN on Z3 [14  15]
(ρ a regular representation of H).

3 Equivariant Maps and Convolutions

Figure 5: The rotation of a planar vector
ﬁeld in two steps: moving each vector to
its new position without changing its ori-
entation  and then rotating the vectors.

Each feature space in a G-CNN is deﬁned as the space of sections of some associated vector
bundle  deﬁned by a choice of base G/H and representation ρ of H that describes how the ﬁbers
transform. A layer in a G-CNN is a map between these feature spaces that is equivariant to the
induced representations acting on them. In this section we will show that equivariant linear maps can
always be written as a convolution-like operation using an equivariant kernel. We will ﬁrst derive this
result for the induced representation realized in the space IG of Mackey functions  and then convert
the result to local sections of the associated vector bundle in Section 3.2. We will assume that G is
locally compact and unimodular.
Consider adjacent feature spaces i = 1  2 with a representation (ρi  Vi) of Hi ≤ G. Let πi = IndG
be the representation acting on I i

Hi ρi

G can be written as

(5)

G. A bounded linear operator I 1
[κ · f ](g) = ZG
κ(g  g′)f (g′)dg′ 

G → I 2

5

f(x)f(g−1x)ρ(g)f(g−1x)using a two-argument linear operator-valued kernel κ : G × G → Hom(V1  V2)  where Hom(V1  V2)
denotes the space of linear maps V1 → V2. Choosing bases  we get a matrix-valued kernel.

We are interested in the space of equivariant linear maps between induced representations  deﬁned
as H = HomG(I 1  I 2) = {Φ ∈ Hom(I 1  I 2) | Φπ1(g) = π2(g)Φ  ∀g ∈ G}. In order for Eq. 5 to
deﬁne an equivariant map Φ ∈ H  the kernel κ must satisfy a constraint. By (partially) resolving this
constraint  we will show that Eq. 5 can always be written as a cross-correlation1
Theorem 3.1. (convolution is all you need) An equivariant map Φ ∈ H can always be written as a
convolution-like integral.

Proof. Since we are only interested in equivariant maps  we get a constraint on κ. For all u  g ∈ G:

[κ · [π1(u)f ]](g) = [π2(u)[κ · f ]](g)

κ(g  g′)f (u−1g′)dg′ = ZG
ZG
ZG
κ(g  ug′)f (g′)dg′ = ZG

κ(u−1g  g′)f (g′)dg′

κ(u−1g  g′)f (g′)dg′

κ(g  ug′) = κ(u−1g  g′)
κ(ug  ug′) = κ(g  g′)

⇔

⇔

⇔

⇔

(6)

Hence  without loss of generality  we can deﬁne the two-argument kernel κ(·  ·) in terms of a
one-argument kernel: κ(g−1g′) ≡ κ(e  g−1g′) = κ(ge  gg−1g′) = κ(g  g′).

The application of κ to f thus reduces to a cross-correlation:

[κ · f ](g) = ZG

κ(g  g′)f (g′)dg′ = ZG

κ(g−1g′)f (g′)dg′ = [κ ⋆ f ](g).

(7)

3.1 The Space of Equivariant Kernels

The constraint Eq. 6 implies a constraint on the one-argument kernel κ. The space of admissible
kernels is in one-to-one correspondence with the space of equivariant maps. Here we give three
different characterizations of this space of kernels. Detailed proofs can be found in Appendix B.

Theorem 3.2. H is isomorphic to the space of bi-equivariant kernels on G  deﬁned as:

KG = {κ : G → Hom(V1  V2) | κ(h2gh1) = ρ2(h2)κ(g)ρ1(h1) 

∀g ∈ G  h1 ∈ H1  h2 ∈ H2}.

(8)

Proof. It is easily veriﬁed (see supp. mat.) that right equivariance follows from the fact that f ∈ I 1
G
is a Mackey function  and left equivariance follows from the requirement that κ ⋆ f ∈ I 2
G should be a
Mackey function. The isomorphism is given by ΓG : KG → H deﬁned as [ΓGκ]f = κ ⋆ f .

The analogous result for the two argument kernel is that κ(gh2  g′h1) should be equal to
ρ2(h−1
2 )κ(g  g′)ρ1(h1) for g  g′ ∈ G  h1 ∈ H1  h2 ∈ H2. This has the following interesting in-
terpretation: κ is a section of a certain associated bundle. We deﬁne a right-action of H1 × H2 on
G×G by setting (g  g′)·(h1  h2) = (gh1  g′h2) and a representation ρ12 of H1 ×H2 on Hom(V1  V2)
by setting ρ12(h1  h2)Ψ = ρ2(h2)Ψρ1(h−1
1 ) for Ψ ∈ Hom(V1  V2). Then the constraint on κ(·  ·)
can be written as κ((g  g′)·(h1  h2)) = ρ12((h1  h2)−1)κ((g  g′)). We recognize this as the condition
of being a Mackey function (Eq. 1) for the bundle (G × G) ×ρ12 Hom(V1  V2).

There is another another way to characterize the space of equivariant kernels:
Theorem 3.3. H is isomorphic to the space of left-equivariant kernels on G/H1  deﬁned as:

KC = {←−κ : G/H1 → Hom(V1  V2) | ←−κ (h2x) = ρ2(h2)←−κ (x)ρ1(h1(x  h2)−1) 

∀h2 ∈ H2  x ∈ G/H1}

(9)

1As in most of the CNN literature  we will not be precise about distinguishing convolution and correlation.

6

Proof. using the decomposition g = s(gH1)h1(g) (see Appendix A)  we can deﬁne

κ(g) = κ(s(gH1)h1(g)) = κ(s(gH1)) ρ1(h1(g)) ≡ ←−κ (gH1)ρ1(h1(g)) 

(10)

This deﬁnes the lifting isomorphism for kernels  ΛK : KC → KG. It is easy to verify that when
deﬁned in this way  κ satisﬁes right H1-equivariance.
We still have the left H2-equivariance constraint from Eq. 8  which translates to ←−κ as follows (details
in supp. mat.). For g ∈ G  h2 ∈ H2 and x ∈ G/H1 

κ(h2g) = ρ2(h2)κ(g) ⇔ ←−κ (h2x) = ρ2(h2)←−κ (x)ρ1(h1(x  h2)−1).

(11)

Theorem 3.4. H is isomorphic to the space of H γ(x)H1

2

-equivariant kernels on H2\G/H1:

KD = {¯κ : H2\G/H1 → Hom(V1  V2) | ¯κ(x) = ρ2(h)¯κ(x)ρx

1 (h)−1 

∀x ∈ H2\G/H1  h ∈ H γ(x)H1

2

} 

(12)

Where γ : H2\G/H1 → G is a choice of double coset representatives  and ρx
the stabilizer H γ(x)H1
= {h ∈ H2 | hγ(x)H1 = γ(x)H1} ≤ H1  deﬁned as

2

1 is a representation of

ρx
1 (h) = ρ1(h1(γ(x)H1  h)) = ρ1(γ(x)−1hγ(x)) 

(13)

Proof. In supplementary material. For examples  see Section 6.

3.2 Local Sections on G/H

We have seen that an equivariant map between spaces of Mackey functions can always be realized as
a cross-correlation on G  and we have studied the properties of the kernel  which can be encoded as
a kernel on G or G/H1 or H2\G/H1  subject to the appropriate constraints. When implementing
a G-CNN  it would be wasteful to use a Mackey function on G  so we need to understand what it
means for ﬁelds realized by local functions f : U → V for U ⊆ G/H1. This is done by sandwiching
the cross-correlation κ⋆ : I 1

G with the lifting isomorphisms Λi : I i

C → I i
G.

G → I 2

[Λ−1

2 [κ ⋆ [Λ1f ]]](x) = ZG

κ(s2(x)−1s1(y))f (y)dy

= ZG/H1

←−κ (s2(x)−1y)ρ1(h1(s2(x)−1s1(y)))f (y)dy

(14)

Which we refer to as the ρ1-twisted cross-correlation on G/H1. We note that for semidirect product
groups  the ρ1 factor disappears and we are left with a standard cross-correlation on G/H1 with
an equivariant kernel ←−κ ∈ KC . We note the similarity of this expression to gauge equivariant
convolution as deﬁned in [11].

3.3 Equivariant Nonlinearities

The network as a whole is equivariant if all of its layers are equivariant. So our theory would not be
complete without a discussion of equivariant nonlinearities and other kinds of layers. In a regular
G-CNN [1]  ρ is the regular representation of H  which means that it can be realized by permutation
matrices. Since permutations and pointwise nonlinearities commute  any such nonlinearity can be
used. For other kinds of representations ρ  special equivariant nonlinearities must be used. Some
choices include norm nonlinearities [3] for unitary representations  tensor product nonlinearities [8] 
or gated nonlinearities where a scalar ﬁeld is normalized by a sigmoid and then multiplied by another
ﬁeld [6]. Other constructions  such as batchnorm and ResNets  can also be made equivariant [1  2].
A comprehensive overview and comparison over equivariant nonlinearities can be found in [7].

7

4

Implementation

Several different approaches to implementing group equivariant CNNs have been proposed in the
literature. The implementation details thereby depend on the speciﬁc choice of symmetry group
G  the homogeneous space G/H  its discretization and the representation ρ. In any case  since the
equivariance constraints on convolution kernels are linear  the space of H-equivariant kernels is a
linear subspace of the unrestricted kernel space. This implies that it is sufﬁcient to solve for a basis
of H-equivariant kernels  in terms of which any equivariant kernel can be expanded using learned
weights.
A case of high practical importance are equivariant CNNs on Euclidean spaces Rd. Implementations
mostly operate on discrete pixel grids. In this case  the steerable kernel basis is typically pre-sampled
on a small grid  linearly combined during the forward pass  and then used in a standard convolution
routine. The sampling procedure requires particular attention since it might introduce aliasing
artifacts [4  6]. A more in depth discussion of an implementation of equivariant CNNs  operating on
Euclidean pixel grids  is provided in [7]. Alternatively to processing signals on a pixel grid  signals on
Euclidean spaces might be sampled on an irregular point cloud. In this case the steerable kernel space
is typically implemented as an analytical function  which is subsequently sampled on the cloud [5].

Implementations of spherical CNNs depend on the choice of signal representation as well. In [10] 
the authors choose a spectral approach to represent the signal and kernels in Fourier space. The
equivariant convolution is performed by exploiting the Fourier theorem. Other approaches deﬁne
the convolution spatially. In these cases  some grid on the sphere is chosen on which the signal
is sampled. As in the Euclidean case  the convolution is performed by matching the signal with a
H-equivariant kernel  which is being expanded in terms of a pre-computed basis.

5 Related Work

In Appendix D  we provide a systematic classiﬁcation of equivariant CNNs on homogeneous spaces 
according to the theory presented in this paper. Besides these references  several papers deserve
special mention. Most closely related is the work of [12]  whose theory is analogous to ours  but only
covers scalar ﬁelds (corresponding to using a trivial representation ρ(h) = I in our theory). A proper
treatment of general ﬁelds as we do here is more difﬁcult  as it requires the use of ﬁber bundles and
induced representations. The ﬁrst use of induced representations and ﬁelds in CNNs is [2]  and the
ﬁrst CNN on a non-trivial homogeneous space (the Sphere) is [16].

A framework for (non-convolutional) networks equivariant to ﬁnite groups was presented by [17]  and
equivariant set and graph networks are analyzed by [18–21]. Our use of ﬁelds (with ρ block-diagonal)
can be viewed as a formalization of convolutional capsules [22  23]. Other related work includes
[24–31]. A preliminary version of this paper appeared as [32].

For mathematical background  we recommend [13  33–37]. The study of induced representations and
equivariant maps between them was pioneered by Mackey [38–41]  who rigorously proved results
essentially similar to the ones in this paper  though presented in a more abstract form that may not be
easy to recognize as having relevance to the theory of equivariant CNNs.

6 Concrete Examples

6.1 The rotation group SO(3) and spherical CNNs

The group of 3D rotations SO(3) is a three-dimensional manifold that can be parameterized by ZYZ
Euler angles α ∈ [0  2π)  β ∈ [0  π] and γ ∈ [0  2π)  i.e. g = Z(α)Y (β)Z(γ)  (where Z and Y
denote rotations around the Z and Y axes). For this example we choose H = H1 = H2 = SO(2) =
{Z(α) | α ∈ [0  2π)} as the group of rotations around the Z-axis  i.e. the stabilizer subgroup of the
north pole of the sphere. A left H-coset is then a subset of SO(3) of the form

gH = {Z(α)Y (β)Z(γ)Z(α′) | α′ ∈ [0  2π)} = {Z(α)Y (β)Z(α′) | α′ ∈ [0  2π)}.

Thus  the coset space G/H is the sphere S2  parameterized by spherical coordinates α and β. As
expected  the stabilizer Hx of a point x ∈ S2 is the set of rotations around the axis through x  which
is isomorphic to H = SO(2).

8

Figure 6: Quotients of SO(3) and SE(3).

What about the double coset space (Appendix A.1)? The orbit of a point x(α  β) ∈ S2 under H is a
circle around the Z axis at lattitude β  so the double coset space H\G/H  which indexes these orbits 
is the segment [0  π) (see Fig. 6).

The section s : G/H → G may be deﬁned (almost everywhere) as s(α  β) = Z(α)Y (β) ∈ SO(3) 
and γ(β) = Y (β) ∈ SO(3). Then the stabilizer H γ(β)H1
for β ∈ H\G/H is the set of Z-axis
rotations that leave the point γ(β)H1 = (0  β) ∈ S2 invariant. For the north and south pole (β = 0
or β = π)  this stabilizer is all of H = SO(2)  but for other points it is the trivial subgroup {e}.

2

Thus  according to Theorem 3.4  the equivariant kernels are matrix-valued functions on the segment
[0  π)  that are mostly unconstrained (except at the poles). As functions on G/H1 (Theorem 3.3) 
they are matrix-valued functions satisfying ←−κ (rx) = ρ2(r)←−κ (x)ρ1(h1(x  r)−1) for r ∈ SO(2) and
x ∈ S2. This says that as a function on the sphere ←−κ is determined on SO(2)-orbits {rx | r ∈ SO(2)}
(lattitudinal circles around the Z axis) by its value on one point of the orbit. Indeed  if ρ(h) = 1 is
the trivial representation  we see that ←−κ is constant on these orbits  in agreement with [42] who use
isotropic ﬁlters. For ρ2 a regular representation of SO(2)  we recover the non-isotropic method of
[10]. For segmentation tasks  one can use a trivial representation for ρ2 in the output layer to obtain a
scalar feature map on S2  analogous to [43]. Other choices  such as ρ the standard 2D representation
of SO(2)  would make it possible to build spherical CNNs that can process vector ﬁelds  but this has
not been done yet.

6.2 The roto-translation group SE(3) and 3D Steerable CNNs

The group of rigid body motions SE(3) is a 6D manifold R3 ⋊ SO(3). We choose H = H1 = H2 =
SO(3) (rotations around the origin). A left H-coset is a set of the form gH = trH = {trr′ | r′ ∈
SO(3)} = {tr | r ∈ SO(3)} where t is the translation component of g. Thus  the coset space G/H
is R3. The stabilizer Hx of a point x ∈ R3 is the set of rotations around x  which is isomorphic
to SO(3). The orbit of a point x ∈ R3 is a spherical shell of radius kxk  so the double coset space
H\G/H  which indexes these orbits  is the set of radii [0  ∞).

Since SE(3) is a trivial principal SO(3) bundle  we can choose a global section s : G/H → G by
taking s(x) to be the translation by x. As double coset representatives we can choose γ(kxk) to
be the translation by (0  0  kxk). Then the stabilizer H γ(kxk)H1
for kxk ∈ H\G/H is the set of
rotations around Z  i.e. SO(2)  except for kxk = 0  where it is SO(3).

2

For any representations ρ1  ρ2  the equivariant maps between sections of the associated vector bundle
are given by convolutions with matrix-valued kernels on R3 that satisfy ←−κ (rx) = ρ2(r)←−κ (x)ρ1(r−1)
for r ∈ SO(3) and x ∈ R3. This follows from Theorem 3.3 with the simpliﬁcation h1(x  r) = r
for all r ∈ H  because SE(3) is a semidirect product (Appendix A.2). Alternatively  we can deﬁne
←−κ in terms of ¯κ  which is a kernel on H\G/H = [0  ∞) satisfying ¯κ(x) = ρ2(r)¯κ(x)ρ1(r) for
r ∈ SO(2) and x ∈ [0  ∞). This is in agreement with the results obtained by [6].

7 Conclusion

In this paper we have developed a general theory of equivariant convolutional networks on homoge-
neous spaces using the formalism of ﬁber bundles and ﬁelds. Field theories are the de facto standard
formalism for modern physical theories  and this paper shows that the same formalism can elegantly
describe the de facto standard learning machine: the convolutional network and its generalizations.
By connecting this very successful class of networks to modern theories in mathematics and physics 
our theory provides many opportunities for the development of new theoretical insights about deep
learning  and the development of new equivariant network architectures.

9

References

[1] Taco S Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of
The 33rd International Conference on Machine Learning (ICML)  volume 48  pages 2990–2999 
2016.

[2] Taco S Cohen and Max Welling. Steerable CNNs. In ICLR  2017.

[3] Daniel E Worrall  Stephan J Garbin  Daniyar Turmukhambetov  and Gabriel J Brostow. Har-
monic networks: Deep translation and rotation equivariance. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  July 2017.

[4] Maurice Weiler  Fred A Hamprecht  and Martin Storath. Learning steerable ﬁlters for rotation
equivariant CNNs. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  June 2018.

[5] Nathaniel Thomas  Tess Smidt  Steven Kearnes  Lusann Yang  Li Li  Kai Kohlhoff  and Patrick
Riley. Tensor ﬁeld networks: Rotation- and Translation-Equivariant neural networks for 3D
point clouds. arXiv:1802.08219 [cs.LG]  2018.

[6] Maurice Weiler  Mario Geiger  Max Welling  Wouter Boomsma  and Taco Cohen. 3D steerable
CNNs: Learning rotationally equivariant features in volumetric data. In Advances in Neural
Information Processing Systems (NeurIPS)  2018.

[7] Maurice Weiler and Gabriele Cesa. General E(2)-Equivariant Steerable CNNs. In Advances in

Neural Information Processing Systems (NeurIPS)  2019.

[8] Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning

atomic potentials. arXiv:1803.01588 [cs.LG]  2018.

[9] Risi Kondor  Hy Truong Son  Horace Pan  Brandon Anderson  and Shubhendu Trivedi. Covari-

ant compositional networks for learning graphs. arXiv:1801.02144 [cs.LG]  January 2018.

[10] Taco S Cohen  Mario Geiger  Jonas Koehler  and Max Welling. Spherical CNNs. In International

Conference on Learning Representations (ICLR)  2018.

[11] Taco S. Cohen  Maurice Weiler  Berkay Kicanaoglu  and Max Welling. Gauge Equivariant
Convolutional Networks and the Icosahedral CNN. In International Conference on Machine
Learning (ICML)  2019.

[12] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution
in neural networks to the action of compact groups. In International Conference on Machine
Learning (ICML)  2018.

[13] Mark Hamilton. Mathematical Gauge Theory: With Applications to the Standard Model of
Particle Physics. Universitext. Springer International Publishing  2017. ISBN 978-3-319-68438-
3. doi: 10.1007/978-3-319-68439-0.

[14] Marysia Winkels and Taco S Cohen. 3D G-CNNs for pulmonary nodule detection. In Interna-

tional Conference on Medical Imaging with Deep Learning (MIDL)  2018.

[15] Daniel Worrall and Gabriel Brostow. CubeNet: Equivariance to 3D rotation and translation. In

European Conference on Computer Vision (ECCV)  2018.

[16] Taco S Cohen  Mario Geiger  Jonas Koehler  and Max Welling. Convolutional Networks for

Spherical Signals. In ICML Workshop on Principled Approaches to Deep Learning  2017.

[17] Siamak Ravanbakhsh  Jeff Schneider  and Barnabas Poczos. Equivariance through Parameter-

Sharing. In International Conference on Machine Learning (ICML)  2017.

[18] Haggai Maron  Heli Ben-Hamu  Nadav Shamir  and Yaron Lipman. Invariant and Equivariant

Graph Networks. In International Conference on Learning Representations (ICLR)  2019.

[19] Haggai Maron  Ethan Fetaya  Nimrod Segol  and Yaron Lipman. On the Universality of

Invariant Networks. In International Conference on Machine Learning (ICML)  2019.

10

[20] Nimrod Segol and Yaron Lipman. On Universal Equivariant Set Networks. arXiv:1910.02421

[cs  stat]  October 2019.

[21] Nicolas Keriven and Gabriel Peyré. Universal Invariant and Equivariant Graph Neural Networks.

In Neural Information Processing Systems (NeurIPS)  2019.

[22] Sara Sabour  Nicholas Frosst  and Geoffrey E Hinton. Dynamic routing between capsules. In
I Guyon  U V Luxburg  S Bengio  H Wallach  R Fergus  S Vishwanathan  and R Garnett  editors 
Advances in Neural Information Processing Systems 30  pages 3856–3866. Curran Associates 
Inc.  2017.

[23] Geoffrey Hinton  Nicholas Frosst  and Sara Sabour. Matrix capsules with EM routing. In

International Conference on Learning Representations (ICLR)  2018.

[24] Chris Olah. Groups and group convolutions.

https://colah.github.io/posts/

2014-12-Groups-Convolution/  2014.

[25] R Gens and P Domingos. Deep symmetry networks. In Advances in Neural Information

Processing Systems (NIPS)  2014.

[26] Laurent Sifre and Stephane Mallat. Rotation  scaling and deformation invariant scattering for
texture discrimination. IEEE conference on Computer Vision and Pattern Recognition (CVPR) 
2013.

[27] E Oyallon and S Mallat. Deep Roto-Translation scattering for object classiﬁcation. In IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2865–2873  2015.

[28] Stéphane Mallat. Understanding deep convolutional networks. Philos. Trans. A Math. Phys.

Eng. Sci.  374(2065):20150203  April 2016.

[29] Jan J Koenderink. The brain a geometry engine. Psychol. Res.  52(2-3):122–127  1990.

[30] Jan Koenderink and Andrea van Doorn. The structure of visual spaces. J. Math. Imaging Vis. 

31(2):171  April 2008.

[31] Jean Petitot. The neurogeometry of pinwheels as a sub-riemannian contact structure. J. Physiol.

Paris  97(2-3):265–309  2003.

[32] Taco S Cohen  Mario Geiger  and Maurice Weiler. Intertwiners between induced representations
(with applications to the theory of equivariant neural networks). arXiv:1803.10743 [cs.LG] 
March 2018.

[33] R W Sharpe. Differential Geometry: Cartan’s Generalization of Klein’s Erlangen Program.

1997.

[34] Adam Marsh. Gauge theories and ﬁber bundles: Deﬁnitions  pictures  and results. July 2016.

[35] G B Folland. A Course in Abstract Harmonic Analysis. CRC Press  1995.

[36] T Ceccherini-Silberstein  A Machí  F Scarabotti  and F Tolli. Induced representations and

mackey theory. J. Math. Sci.  156(1):11–28  January 2009.

[37] David Gurarie. Symmetries and Laplacians: Introduction to Harmonic Analysis  Group Repre-

sentations and Applications. Elsevier B.V.  1992.

[38] George W Mackey. On induced representations of groups. Amer. J. Math.  73(3):576–592  July

1951.

[39] George W Mackey. Induced representations of locally compact groups I. Ann. Math.  55(1):

101–139  1952.

[40] George W Mackey.

Induced representations of locally compact groups II. the frobenius

reciprocity theorem. Ann. Math.  58(2):193–221  1953.

[41] George W Mackey.

Induced Representations of Groups and Quantum Mechanics. W.A.

Benjamin Inc.  New York-Amsterdam  1968.

11

[42] Carlos Esteves  Christine Allen-Blanchette  Ameesh Makadia  and Kostas Daniilidis. 3D object
classiﬁcation and retrieval with spherical CNNs. In European Conference on Computer Vision
(ECCV)  2018.

[43] Jim Winkens  Jasper Linmans  Bastiaan S Veeling  Taco S. Cohen  and Max Welling. Improved
Semantic Segmentation for Histopathology using Rotation Equivariant Convolutional Networks.
In International Conference on Medical Imaging with Deep Learning (MIDL workshop)  2018.

[44] Michael M Bronstein  Joan Bruna  Yann LeCun  Arthur Szlam  and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 34 
2017.

[45] Yann LeCun  Bernhard E Boser  John S Denker  Donnie Henderson  R E Howard  Wayne E
Hubbard  and Lawrence D Jackel. Handwritten digit recognition with a Back-Propagation
network. In D S Touretzky  editor  Advances in Neural Information Processing Systems 2  pages
396–404. Morgan-Kaufmann  1990.

[46] S Dieleman  J De Fauw  and K Kavukcuoglu. Exploiting cyclic symmetry in convolutional

neural networks. In International Conference on Machine Learning (ICML)  2016.

[47] Emiel Hoogeboom  Jorn W T Peters  Taco S Cohen  and Max Welling. HexaConv.

In

International Conference on Learning Representations (ICLR)  2018.

[48] Yanzhao Zhou  Qixiang Ye  Qiang Qiu  and Jianbin Jiao. Oriented response networks. In CVPR 

2017.

[49] Erik J Bekkers  Maxime W Lafarge  Mitko Veta  Koen A J Eppenhof  and Josien P W Pluim.
Roto-Translation covariant convolutional networks for medical image analysis. In Medical
Image Computing and Computer Assisted Intervention (MICCAI)  2018.

[50] Diego Marcos  Michele Volpi  Nikos Komodakis  and Devis Tuia. Rotation equivariant vector

ﬁeld networks. In International Conference on Computer Vision (ICCV)  2017.

[51] Rohan Ghosh and Anupam K Gupta. Scale steerable ﬁlters for locally scale-invariant convolu-

tional neural networks. arXiv preprint arXiv:1906.03861  2019.

[52] Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In International

Conference on Machine Learning (ICML)  2019.

[53] Ivan Sosnovik  Michał Szmaja  and Arnold Smeulders. Scale-equivariant steerable networks 

2019.

[54] Risi Kondor  Zhen Lin  and Shubhendu Trivedi. Clebsch–Gordan Nets: A Fully Fourier Space
Spherical Convolutional Neural Network. In Conference on Neural Information Processing
Systems (NeurIPS)  2018.

[55] Brandon Anderson  Truong-Son Hy  and Risi Kondor. Cormorant: Covariant molecular neural

networks. arXiv preprint arXiv:1906.04015  2019.

[56] Nathanaël Perraudin  Michaël Defferrard  Tomasz Kacprzak  and Raphael Sgier. DeepSphere:
Efﬁcient spherical Convolutional Neural Network with HEALPix sampling for cosmological
applications. Astronomy and Computing 27  2018.

[57] Chiyu Jiang  Jingwei Huang  Karthik Kashinath  Prabhat  Philip Marcus  and Matthias Niessner.
Spherical CNNs on unstructured grids. In International Conference on Learning Representations
(ICLR)  2019.

12

,Taco Cohen
Mario Geiger
Maurice Weiler