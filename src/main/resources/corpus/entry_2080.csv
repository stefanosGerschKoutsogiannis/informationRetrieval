2019,Faster width-dependent algorithm for mixed packing and covering LPs,In this paper  we give a faster width-dependent algorithm for mixed packing-covering LPs. Mixed packing-covering LPs are fundamental to combinatorial optimization in computer science and operations research. Our algorithm finds a $1+\eps$ approximate solution in time $O(Nw/ \varepsilon)$  where $N$ is number of nonzero entries in the constraint matrix  and $w$ is the maximum number of nonzeros in any constraint. This algorithm is faster than Nesterov's smoothing algorithm which requires $O(N\sqrt{n}w/ \eps)$ time  where $n$ is the dimension of the problem. Our work utilizes the framework of area convexity introduced in [Sherman-FOCS'17] to obtain the best dependence on $\varepsilon$ while breaking the infamous $\ell_{\infty}$ barrier to eliminate the factor of $\sqrt{n}$. The current best width-independent algorithm for this problem runs in time $O(N/\eps^2)$ [Young-arXiv-14] and hence has worse running time dependence on $\varepsilon$. Many real life instances of mixed packing-covering problems exhibit small width and for such cases  our algorithm can report higher precision results when compared to width-independent algorithms. As a special case of our result  we report a $1+\varepsilon$ approximation algorithm for the densest subgraph problem which runs in time $O(md/ \varepsilon)$  where $m$ is the number of edges in the graph and $d$ is the maximum graph degree.,Faster Width-dependent Algorithm for Mixed

Packing and Covering LPs

Digvijay Boob
Georgia Tech
Atlanta  GA

digvijaybb40@gatech.edu

Saurabh Sawlani

Georgia Tech
Atlanta  GA

sawlani@gatech.edu

Di Wang˚
Google AI
Atlanta  GA

wadi@google.com

Abstract

?

In this paper  we give a faster width-dependent algorithm for mixed packing-
covering LPs. Mixed packing-covering LPs are fundamental to combinatorial
optimization in computer science and operations research. Our algorithm ﬁnds
a 1 ` ε approximate solution in time OpN w{εq  where N is number of nonzero
entries in the constraint matrix  and w is the maximum number of nonzeros in any
constraint. This algorithm is faster than Nesterov’s smoothing algorithm which
requires OpN
nw{εq time  where n is the dimension of the problem. Our work
utilizes the framework of area convexity introduced in [Sherman-FOCS’17] to ob-
?
tain the best dependence on ε while breaking the infamous (cid:96)8 barrier to eliminate
the factor of
n. The current best width-independent algorithm for this problem
runs in time OpN{ε2q [Young-arXiv-14] and hence has worse running time depen-
dence on ε. Many real life instances of mixed packing-covering problems exhibit
small width and for such cases  our algorithm can report higher precision results
when compared to width-independent algorithms. As a special case of our result 
we report a 1` ε approximation algorithm for the densest subgraph problem which
runs in time Opmd{εq  where m is the number of edges in the graph and d is the
maximum graph degree.

1

Introduction

Mixed packing and covering linear programs (LPs) are a natural class of LPs where coefﬁcients 
variables  and constraints are non-negative. They model a wide range of important problems in
combinatorial optimization and operations research. In general  they model any problem which
contains a limited set of available resources (packing constraints) and a set of demands to fulﬁll
(covering constraints).
Two special cases of the problem have been widely studied in literature: pure packing  formulated as
maxxtbT x | P x ď pu; and pure covering  formulated as minxtbT x | Cx ě cu where P  p  C  c  b
are all non-negative. These are known to model fundamental problems such as maximum bipartite
graph matching  minimum set cover  etc. [LN93]. Algorithms to solve packing and covering LPs have
also been applied to great effect in designing ﬂow control systems [BBR04]  scheduling problems
[PST95]  zero-sum matrix games [Nes05] and in mechanism design [ZN01]. In this paper  we study
the mixed packing and covering (MPC) problem  formulated as checking the feasibility of the set:
tx | P x ď p  Cx ě cu  where P  C  p  c are non-negative. We say that x is an ε-approximate solution
to MPC if it belongs to the relaxed set tx | P x ď p1` εqp  Cx ě p1´ εqcu. MPC is a generalization
of pure packing and pure covering  hence it is applicable to a wider range of problems such as
multi-commodity ﬂow on graphs [You01  She17]  non-negative linear systems and X-ray tomography
[You01].

˚Work done when author was at Georgia Tech.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

General LP solving techniques such as the interior point method can approximate solutions to MPC in
as few as Oplogp1{εqq iterations - however  they incur a large per-iteration cost. In contrast  iterative
approximation algorithms based on ﬁrst-order optimization methods require polyp1{εq iterations  but
the iterations are fast and in most cases are conducive to efﬁcient parallelization. This property is of
utmost importance in the context of ever-growing datasets and the availability of powerful parallel
computers  resulting in much faster algorithms in relatively low-precision regimes.

1.1 Previous work

In literature  algorithms for the MPC problem can be grouped into two broad categories: width-
dependent and width-independent. Here  width is an intrinsic property of a linear program which
typically depends on the dimensions and the largest entry of the constraint matrix  and is an indication
of the range of values any constraint can take. In the context of this paper and the MPC problem  we
deﬁne wP and wC as the maximum number of non-zeros in any constraint in P and C respectively.
We deﬁne the width of the LP as w def“ maxpwP   wCq.
One of the ﬁrst approaches used to solve LPs was Langrangian-relaxation: replacing hard constraints
with loss functions which enforce the same constraints indirectly. Using this approach  Plotkin 
Schmoys and Tardos [PST95]  and Grigoriadis and Khachiyan [GK96] obtained width-dependent
polynomial-time approximation algorithms for MPC. Luby and Nisan [LN93] gave the ﬁrst width-

dependent parallelizable algorithm for pure packing and pure covering  which ran in rOpε´4q parallel
time  and rOpN ε´4q total work. Here  parallel time (sometimes termed as depth) refers to the longest
in rOpε´4q parallel time  and rOpmdε´2q total work2. Young [You14] later improved his algorithm
parallel run-time of rOpε´3q.

chain of dependent operations  and work refers to the total number of operations in the algorithm.
Young [You01] extended this technique to give the ﬁrst width-independent parallel algorithm for MPC
to run using total work OpN ε´2q. Mahoney et al. [MRWZ16] later gave an algorithm with a faster

The other most prominent approach in literature towards solving an LP is by converting it into a
smooth function [Nes05]  and then applying general ﬁrst-order optimization techniques [Nes05 
Nes12]. Although the dependence on ε from using ﬁrst-order techniques is much improved  it
usually comes at the cost of sub-optimal dependence on the input size and width. For the MPC
problem  Nesterov’s accelerated method [Nes12]  as well as Bienstock and Iyengar’s adaptation
[BI06] of Nesterov’s smoothing [Nes05]  give rise to algorithms with runtime linearly depending
on ε´1  but with far from optimal dependence on input size and width. For pure packing and pure
covering problems  however  Allen-Zhu and Orrechia [AO19] were the ﬁrst to incorporate Nesterov-
like acceleration while still being able to obtain near-linear width-independent runtimes  giving a

rOpN ε´1q time algorithm for the packing problem. For the covering problem  they gave a rOpN ε´1.5q
time algorithm  which was then improved to rOpN ε´1q by [WRM16]. Importantly  however  the

above algorithms do not generalize to MPC.

1.2 Our contributions

We give the best parallel width-dependent algorithm for MPC  while only incurring a linear depen-
dence on ε´1 in the parallel runtime and total work. Additionally  the total work has near-linear
dependence on the input-size. Formally  we state our main theorem as follows.
Theorem 1.1. There exists a parallel ε-approximation algorithm for the mixed packing covering

problem  which runs in rOpw ¨ ε´1q parallel time  while performing rOpw ¨ N ¨ ε´1q total work  where

N is the total number of non-zeros in the constraint matrices  and w is the width of the given LP.

Table 1 compares the running time of our algorithm to previous works solving this problem.
Sacriﬁcing width independence for faster convergence with respect to precision proves to be a
valuable trade-off for several combinatorial optimization problems which naturally have a low width.
Prominent examples of such problems which are not pure packing or covering problems include
multicommodity ﬂow and densest subgraph  where the width is bounded by the degree of a vertex.
In a large number of real-world graphs  the maximum vertex degree is usually small  hence our

2d here is the maximum number of constraints that any variable appears in.

2

Table 1: Comparison of runtimes of ε-approximation algorithms for the mixed packing covering
problem.

Total Work

rOpmdε´2q
rOpn2.5w1.5
rOpw ¨ N
rOpN ε´2q
rOpN ε´3q
rOpwN ε´1q

P wε´1q
?
nε´1q

Comments

d is column-width
width-dependent
width-dependent

width-dependent

Young [You01]

Bienstock and Iyengar [BI06]

Nesterov [Nes12]
Young [You14]

Mahoney et al. [MRWZ16]

This paper

Parallel Runtime

rOpε´4q
rOpw
rOpε´4q
rOpε´3q
rOpwε´1q

?

nε´1q

algorithm proves to be much faster when we want high-precision solutions. We explicitly show that
this result directly gives the fastest algorithm for the densest subgraph problem on low-degree graphs
in Appendix C.

2 Notation and Deﬁnitions
For any integer q  we represent using (cid:107)¨(cid:107)q the q-norm of any vector. We represent the inﬁnity-norm as
(cid:107)¨(cid:107)8. We denote the inﬁnity-norm ball (sometimes called the (cid:96)8 ball) as the set Bn8prq def“ tx P Rn :
ř
(cid:107)x(cid:107)8 ď ru. The nonnegative part of this ball is denoted as Bn` 8prq “ tx P Rn : x ě 0n (cid:107)x(cid:107)8 ď
ru. For radius r “ 1  we drop the radius speciﬁcation and use the short notation Bn8 and Bn` 8. We
i“1 xi ď 1u. For any y ě 0k 
denote the extended simplex of dimension k as ∆`
pyq “ y{(cid:107)y(cid:107)1 if (cid:107)y(cid:107)1 ě 1. Further  for any set K  we represent its interior  relative interior
proj∆`
and closure as intpKq  relintpKq and clpKq  respectively. The function exp is applied to a vector
element wise. The division of two vectors of same dimension is also performed element wise.
For any matrix A  we use nnzpAq to denote the number of nonzero entries in it. We use Ai : and A: j
to refer to the ith row and jth column of A respectively. We use notation Aij (or Ai j alternatively)
to denote an element in the i-th row and j-th column of matrix A. (cid:107)A(cid:107)8 denotes the operator norm
(cid:107)A(cid:107)8Ñ8 def“ supx‰0
(cid:107)Ax(cid:107)8
(cid:107)x(cid:107)8 . For a symmetric matrix A and an antisymmetric matrix B  we deﬁne
an operator ľi as A ľi B ô

is positive semi-deﬁnite.

def“ tx P Rk :



„

k

k

k

A ´B
B A

We formally deﬁne an ε-approximate solution to the mixed packing-covering (MPC) problem as
follows.
Deﬁnition 2.1. We say that x is an ε-approximate solution of the mixed packing-covering problem
if x satisﬁes x P Bn` 8  P x ď p1 ` εq1p and Cx ě p1 ´ εq1c.
Here  1k denotes a vectors of 1’s of dimension k for any integer k.
The saddle point problem on two sets x P X and y P Y can be deﬁned as follows:

min
xPX

max
yPY

(1)
where Lpx  yq is some bilinear form between x and y. For this problem  we deﬁne the primal-dual

gap function as suppsx syqPXˆY Lpx syq ´ Lpsx  yq. This gap function can be used as measure of
Deﬁnition 2.2. We say that px  yq P XˆY is an ε-optimal solution for (1) if suppsx syqPXˆY Lpx syq´
Lpsx  yq ď ε.

accuracy of the above saddle point solution.

Lpx  yq 

3 Technical overview

The mixed packing-covering (MPC) problem is formally deﬁned as follows.
Given two nonnegative matrices P P Rpˆn  C P Rcˆn  ﬁnd an x P Rn  x ě 0 (cid:107)x(cid:107)8 ď 1 such that
P x ď 1p and Cx ě 1c if it exists  otherwise report infeasibility.

3

Note that the vector of 1’s on the right hand side of the packing and covering constraints can be
obtained by simply scaling each constraint appropriately. We also assume that each entry in the
matrices P and C is at most 1. This assumption  and subsequently the (cid:96)8 constraints on x also cause
no loss of generality3.
We reformulate MPC as a saddle point problem  as deﬁned in Section 2;

yP∆`

max
c   zP∆`

p

Lpx  y  zq 

(2)

„

λ˚ def“ min
„
xPBn` 8

P ´1p
´C
1c



p

x
1

c  zP∆`

. The relation between the two formulations is shown

where Lpx  y  zq :“ ryT zTs
in Section 4. For the rest of the paper  we focus on the saddle point formulation (2).
ηpxq def“ maxyP∆`
Lpx  y  zq is a piecewise linear convex function. Assuming oracle access
to this “inner" maximization problem  the “outer" problem of minimizing ηpxq can be performed
using ﬁrst order methods like mirror descent  which are suitable when the underlying problem space
is the unit (cid:96)8 ball. One drawback of this class of methods is that their rate of convergence  which is
standard for non-accelerated ﬁrst order methods on non-differentiable objectives  is Op 1
ε2q to obtain
an ε-approximate minimizer x of η which satisﬁes ηpxq ď η˚ ` ε  where η˚ is the optimal value.
This means that the algorithm needs to access the inner maximization oracle Op 1
ε2q times  which can
become prohibitively large in the high precision regime.
Note that even though η is a piecewise linear non-differentiable function  it is not a black box function 
but a maximization linear functions in x. This structure can be exploited using Nesterov’s smoothing
technique [Nes05]. In particular  ηpxq can be approximated by choosing a strongly convex3 function
φ : ∆`

c Ñ R and considering

p ˆ ∆`

yP∆`

c  zP∆`

Lpx  y  zq ´ φpy  zq.

This strongly convex regularization yields thatrη is a Lipschitz-smooth4 convex function. If L is the
constant of Lipschitz smoothness ofrη then application of any of the accelerated gradient methods
construct a smooth ε-approximationrη of η  the Lipschitz smoothness constant L can be chosen to

rηpxq “ max
a
in literature will converge in Op
L{εq iterations. Moreover  it can also be shown that in order to
be of the order Op1{εq  which in turn implies an overall convergence rate of Op1{εq. In particular 
Nesterov’s smoothing achieves an oracle complexity of Opp(cid:107)P(cid:107)8 ` (cid:107)C(cid:107)8qDx maxtDy  Dzuε´1q 
where where Dx  Dy and Dz denote the sizes of the ranges of their respective regularizers which
are strongly convex functions. Dy and Dz can be made of the order of log p and log c  respectively.
However  Dx can be problematic since x belongs to an (cid:96)8 ball. More on this will soon follow.
Nesterov’s dual extrapolation algorithm[Nes07] gives a very similar complexity but is a different
algorithm in that it directly addresses the saddle point formulation (2) rather than viewing the problem
as optimizing a non-smooth function η. The ﬁnal convergence for the dual extrapolation algorithm
is given in terms of the primal-dual gap function of the saddle point problem (2). This algorithms
views the saddle point problem as solving variational inequality for an appropriate monotone operator
in joint domain px  y  zq. Moreover  as opposed to smoothing techniques which only regularize the
dual  this algorithm regularizes both primal and dual parts (joint regularization)  hence is a different
scheme altogether.
Note that for both schemes mentioned above  the maximization oracle itself has an analytical
expression which involves matrix-vector multiplication. Hence each call to the oracle incurs a
sequential run-time of nnzpPq ` nnzpCq. Then  overall complexity for both schemes is of order
OppnnzpPq ` nnzpCqqp(cid:107)P(cid:107)8 ` (cid:107)C(cid:107)8qDx maxtDy  Dzuε´1q.

p

3This transformation can be achieved by adapting techniques from [WRM16] while increasing dimension of
the problem up to a logarithmic factor. Details of this fact are in Appendix B in the full version of this paper. For
the purpose of the main text  we work with this assumption.
4Deﬁnitions of Lipschitz-smoothness and strong convexity can be found in many texts in nonlinear program-
ming and machine learning. e.g. [Bub14]. Intuitively  f is Lipschitz-smooth if the rate of change of ∇f can be
bounded by a quantity known as the “constant of Lipschitz smoothness”.

4

The (cid:96)8 barrier
Note that the ﬁrst method  i.e.  Nesterov’s smoothing technique has known lower bounds due to
[GN15] (see Corollary 1 in their paper). According to their result  the framework of Nesterov’s
smoothing has a known limitation since it only regularizes the dual variables. As opposed to this 
Nesterov’s dual extrapolation regularizes both primal and dual variables  and has potential to skip the
earlier mentioned lower bounds of [GN15]. However  the complexity result of this method involves a
Dx term  which denotes the range of a convex function over the domain of x. The following lemma
states a lower bound for this range in case of (cid:96)8 balls.
Lemma 3.1. Any strongly convex function has a range of at least Ωp?
Since Dx “ Ωp?
?

nq for each member function of this wide class  there is no hope of eliminating this

nq on any (cid:96)8 ball.

n factor using techniques involving explicit use of strong convexity.

So  the goal now is to ﬁnd a joint regularization function with a small range over (cid:96)8 balls  but still
act as good enough regularizers to enable accelerated convergence of the descent algorithm. In
pursuit of breaking this (cid:96)8 barrier  we draw inspiration from the notion of area convexity introduced
by Sherman [She17]. Area convexity is a weaker notion than strong convexity  however  it is still
strong enough to ensure that accelerated ﬁrst order methods still go through when using area convex
regularizers. Since this is a weaker notion than strong convexity  we can construct area convex
functions which have range of Opnop1qq on (cid:96)8 ball.
First  we deﬁne area convexity  and then go on to mention its relevance to the saddle point problem
(2).
Area convexity is a notion deﬁned in context of a matrix A P Raˆb and a convex set K Ď Ra`b. Let
MA

„



def“

.

0bˆb ´AT
0aˆa
A

Deﬁnition 3.2 ([She17]). A function φ is area convex with respect to a matrix A on a convex set K iff
pv ´ uqT MApu´ tq.
for any t  u  v P K  φ satisﬁes φ

φptq` φpuq` φpvq

` t ` u ` v

˘

3

`

ď 1
3

˘

?
´ 1
3

3

`

„


0 ´1
0
1

2pφptq ` φpuqq exceeds φp 1

To understand the deﬁnition above  let us ﬁrst look at the notion of strong convexity. φ is said to
2pt ` uqq by an amount
be strongly convex if for any two points t  u  1
proportional to (cid:107)t ´ u(cid:107)2
2. Deﬁnition 3.2 generalizes this notion in context of matrix A for any three
points x  y  z. φ is area-convex on set K if for any three points t  u  v P K  we have 1
3pφptq` φpuq`
3pt ` u ` vqq by an amount proportional to the area of the triangle deﬁned by the
φpvqq exceeds φp 1
convex hull of t  u  v.
Consider the case that points t  u  v are collinear. For this case  the area term (i.e.  the term involving
MA) in Deﬁnition 3.2 is 0 since matrix MA is antisymmetric. In this sense  area convexity is even
weaker than strict convexity. Moreover  the notion of area is parameterized by matrix A. To see
a speciﬁc example of this notion of area  consider A “
and t  u  v P R2. Then  for all
possible permutations of t  u  v  the area term takes a value equal to ˘pt1pu2 ´ v2q ` u1pv2 ´
t2q ` v1pt2 ´ u2qq. Since the condition holds irrespective of the permutation so we must have that
|t1pu2 ´ v2q ` u1pv2 ´ t2q ` v1pt2 ´ u2q|. But note
φp t`u`v
2|t1pu2 ´ v2q ` u1pv2 ´ t2q ` v1pt2 ´ u2q|.
that area of triangle formed by points t  u  v is equal to 1
Hence the area term is just a high dimensional matrix based generalization of the area of a triangle.
Coming back to the saddle point problem (2)  we need to pick a suitable area convex function φ
on the set Bn` 8 ˆ ∆`
c . Since φ is deﬁned on the joint space  it has the property of joint
regularization vis a vis (2). However  we need an additional parameter: a suitable matrix MA. The
choice of this matrix is related to the bilinear form of the primal-dual gap function of (2). We delve
into the technical details of this in Section 4  however  we state that the matrix is composed of P  C
and some additional constants. The algorithm we state exactly follows Nesterov’s dual extrapolation
method described earlier. One notable difference is that in [Nes07]  they consider joint regularization
by a strongly convex function which does not depend on the problem matrices P  C but only on the
constraint set Bn` 8 ˆ ∆`
c . Our area convex regularizer  on the other hand  is tailor made for
the particular problem matrices P  C as well as the constraint set.

˘
φptq ` φpuq ` φpvq

p ˆ ∆`

p ˆ ∆`

´ 1
?
3

q ď 1

3

3

3

5

4 Area Convexity for Mixed Packing Covering LPs

In this section  we present our technical results and algorithm for the MPC problem  with the end goal
of proving Theorem 1.1. First  we relate an p1 ` εq-approximate solution to the saddle point problem
to an ε-approximate solution to MPC. Next  we present some theoretical background towards the
goal of choosing and analyzing an appropriate area-convex regularizer in the context of the saddle
point formulation  where the key requirement of the area convex function is to obtain a provable and
efﬁcient convergence result. Finally  we explicitly show an area convex function which is generated
using a simple “gadget" function. We show that this area convex function satisﬁes all key requirements
and hence achieves the desired accelerated rate of convergence. This section closely follows [She17] 
in which the author chooses an area convex function speciﬁc to the undirected multicommodity ﬂow
problem. Due to space constraints  we relegate almost all proofs to Appendix A (in the full version)
and simply include pointers to proofs in [She17] when it is directly applicable.

4.1 Saddle Point Formulation for MPC

pair px  y  zq and psx sy szq for (2)  we denote w “ px  u  y  zq and sw “ psx su sy szq where u su P R.

Consider the saddle point formulation in (2) for MPC. Given a feasible primal-dual feasible solution
Then  we deﬁne a function Q : Rn`1`p`c ˆ Rn`1`p`c Ñ R as
´ ryT zTs

„

„



.

P ´1p
´C
1c

x
u

„


„sxsu
Lpx sy szq ´ Lpsx  y  zq

P ´1p
´C
1c

Qpw swq def“ rsyT szTs
Qpw swq “

Note that if u “su “ 1  then
supswPW

sxPBn` 8 syP∆`

sup

p  szP∆`

c

is precisely the primal-dual gap function deﬁned in Section 2. Notice that if px˚  y˚  z˚q is a saddle
point of (2)  then we have

Lpx˚  y  zq ď Lpx˚  y˚  z˚q ď Lpx  y˚  z˚q

p   z P ∆`
for all x P Bn` 8  y P ∆`
where W def“ Bn` 8 ˆ t1u ˆ ∆`
This motivates the following accuracy measure of the candidate approximate solution w.
Deﬁnition 4.1. We say that w P W is an ε-optimal solution of (2) iff

c . From above equation  it is clear that Qpw  w˚q ě 0 for all w P W
p ˆ ∆`
c and w˚ “ px˚  1  y˚  z˚q P W. Moreover  Qpw˚  w˚q “ 0.

supswPW

Remark 4.2. Recall the deﬁnition of MA for a matrix A in Section 3. We can rewrite Qpw swq “
swT Jw where J “ MH and
„



Qpw swq ď ε.
»—–0nˆn 0nˆ1 ´P T

01ˆn
P
´C

0
´1p
1c

C T
´1T
1T
p
c
0pˆp 0pˆc
0cˆc
0cˆp

ﬁﬃﬂ .

Thus  the gap function in Deﬁnition 4.1 can be written in the bilinear form supswPW swT Jw.
Lemma 4.3. Let px  y  zq satisfy suppsx sy szqPBn` 8ˆ∆`
2. y  z satisfy yTpPsx ´ 1pq ` zTp´Csx ` 1cq ą 0 for allsx P Bn` 8.

Lemma 4.3 relates the ε-optimal solution of (2) to the ε-approximate solution to MPC.

Lpx sy szq ´ Lpsx  y  zq ď ε. Then either

1. x is an ε-approximate solution of MPC  or

p ˆ∆`

c

This lemma states that in order to ﬁnd an ε-approximate solution of MPC  it sufﬁces to ﬁnd ε-optimal
solution of (2). Henceforth  we will focus on ε-optimality of the saddle point formulation (2).

6

H “

P ´1p
´C
1c

ñ J :“

4.2 Area Convexity with Saddle Point Framework

Here we state some useful lemmas which help in determining whether a differentiable function is
area convex. We start with the following remark which follows from the deﬁnition of area convexity
(Deﬁnition 3.2).

Remark 4.4. If φ is area convex with respect to A on a convex set K  and sK Ď K is a convex set 
then φ is area convex with respect to A on sK.

„


0 ´1
0
1

The following two lemmas from [She17] provide the key characterization of area convexity.
Lemma 4.5. Let A P R2ˆ2 symmetric matrix. A ľi
ô A ľ 0 and detpAq ě 1.
Lemma 4.6. Let φ be twice differentiable on the interior of convex set K  i.e.  intpKq.

1. If φ is area convex with respect to A on intpKq  then d2φpxq ľi MA for all x P intpKq.
2. If d2φpxq ľi MA for all x P intpKq  then φ is area convex with respect to 1
Moreover  if φ is continuous on clpKq  then φ is area convex with respect to 1

3 A on intpKq.
3 A on clpKq.
In order to handle the operator ľi (recall from Section 2)  we state some basic but important properties
of this operator  which will come in handy in later proofs.
Remark 4.7. For symmetric matrices A and C and antisymmetric matrices B and D 

1. If A ľi B then A ľip´Bq.
2. If A ľi B and λ ě 0 then λA ľi λB.
3. If A ľi B and C ľi D then A ` C ľipB ` Dq.

Having laid a basic foundation for area convexity  we now focus on its relevance to solving the saddle
point problem (2). Considering Remark 4.2  we can write the gap function criterion of optimality
in terms of bilinear form of the matrix J. Suppose we have a function φ which is area convex with
respect to H on set W. Then  consider the following jointly-regularized version of the bilinear form:
(3)
Similar to Nesterov’s dual extrapolation  one can attain Op1{εq convergence of accelerated gradient

descent for functionrηpwq in (3) over variable w. In order to obtain gradients ofrηpwq  we need access
to argmaxswPW swT Jw ´ φpswq. However  it may not be possible to ﬁnd an exact maximizer in all

rηpwq :“ supswPW

swT Jw ´ φpswq.

cases. Again  one can get around this difﬁculty by instead using an approximate optimization oracle
of the problem in (3).
Deﬁnition 4.8. A δ-optimal solution oracle (OSO) for φ : W Ñ R takes input a and outputs w P W
such that

aTsw ´ φpswq ´ δ.

aT w ´ φpwq ě supswPW

Given Φ as a δ-OSO for a function φ  consider the following algorithm (Algorithm 4.2):

Algorithm 1 Area Convex Mixed Packing Covering (AC-MPC)

Initialize w0 “ p0n  1  0p`cq
for t “ 0  . . .   T do
end for

wt`1 Ð wt ` ΦpJwt ` 2JΦpJwtqq

For Algorithm 4.2  [She17] shows the following:
?
Lemma 4.9. Let φ : W Ñ r´ρ  0s. Suppose φ is area convex with respect to 2
for J “ MH and for all t ě 1 we have wt{t P W and 

3H on W. Then

In particular  in ρ

ε iterations  Algorithm 4.2 obtain pδ ` εq-solution of the saddle point problem (2).

swJ wt
t ď δ ` ρ
t .

supswPW

7

The analysis of this lemma closely follows the analysis of Nesterov’s dual extrapolation.
Note that  each iteration consists of Op1q matrix-vector multiplications  Op1q vector additions  and
Op1q calls to the approximate oracle. Since the former two are parallelizable to Oplog nq depth 
the same remains to be shown for the oracle computation to complete the proof of the run-time in
Theorem 1.1.
Recall from the discussion in Section 3 that the critical bottleneck of Nesterov’s method is that
nq  which is achieved even in the Euclidean (cid:96)2 norm. This makes ρ in
nq  which can be a major bottleneck for high dimensional LPs  which are

diameter of the (cid:96)8 ball is Ωp?
Lemma 4.9 to also be Ωp?

commonplace among real-world applications.
Although  on the face of it  area convexity applied to the saddle point formulation (2) has a similar
framework to Nesterov’s dual extrapolation  the challenge is to construct a φ for which we can
overcome the above bottleneck. Particularly  there are three key challenges to tackle:
1. We need to show that existence of a function φ that is area convex with respect to H on W.
2. φ : W Ñ r´ρ  0s should be such that ρ is not too large.
3. There should exist an efﬁcient δ-OSO for φ.
In the next subsection  we focus on these three aspects in order to complete our analysis.

4.3 Choosing an area convex function

First  we consider a simple 2-D gadget function and prove a “nice" property of this gadget. Using
this gadget  we construct a function which can be shown to be area convex using the aforementioned
property of the gadget.
Let γβ : R2` Ñ R be a function parameterized by β deﬁned as
γβpa  bq “ ba log a ` βb log b.


„

Lemma 4.10. Suppose β ě 2. Then d2γβpa  bq ľ

for all a P p0  1s and b ą 0.

0 ´1
0
1

Theorem 4.11. Let w “ px  u  y  zq and deﬁne

Now  using the function γβ  we construct a function φ and use the sufﬁciency criterion provided in
Lemma 4.6 to show that φ is area convex with respect to J on W. Note that our set of interest W is
not full-dimensional  whereas Lemma (4.6) is only stated for int and not for relint. To get around

this difﬁculty  we consider a larger setĎW Ą W such thatĎW is full dimensional and φ is area convex
onĎW. Then we use Remark 4.4 to obtain the ﬁnal result  i.e.  area convexity of φ.
Cijγcipxj  ziq ` cř
φpwq def“ př
setĎW :“ Bn`1` 8p1q ˆ ∆`
„

Pijγpipxj  yiq ` př
where pi “ 2 ˚ (cid:107)P(cid:107)8

(cid:107)Pi :(cid:107)1

„
γ2pu  ziq 
P ´1p
´C
1c

and ci “ 2 ˚ (cid:107)C(cid:107)8
(cid:107)Ci :(cid:107)1
p ˆ ∆`

?
c . In particular  it also implies 6

γ2pu  yiq ` cř

  then φ is area convex with respect to 1
3

3φ is area convex with respect to

i“1

j“1



on

i“1

i“1

j“1

nř

nř

i“1

?
2

3

P ´1p
´C
1c

on set W.

Theorem 4.11 addresses the ﬁrst part of the key three challenges. Next  Lemma 4.12 shows an upper
bound on the range of φ.
Lemma 4.12. Function φ : W Ñ r´ρ  0s then ρ “ Op(cid:107)P(cid:107)8 log p ` (cid:107)C(cid:107)8 log cq.

Finally  we need an efﬁcient δ-OSO. Consider the following alternating minimization algorithm.

8

Algorithm 2 δ-OSO for φ

end for

Input a P Rn`1  a1 P Rp  a2 P Rc  δ ą 0
Initialize px0  u0q P Bn` 8 ˆ t1u arbitrarily.
for k “ 1  . . .   K do
pyk  zkq Ð argmax
yP∆`
c   zP∆`
pxk  ukq Ð argmax

p

yT a1 ` zT a2 ´ φpxk´1  uk´1  y  zq
rxT usa ´ φpx  u  yk  zkq

px uqPBn` 8ˆt1u

δq iterations.

[Bec15] shows the following convergence result.
Lemma 4.13. For δ ą 0  Algorithm 2 is a δ-OSO for φ which converges in Oplog 1
We show that for our chosen φ  we can perform the two argmax computations in each iteration of
Algorithm 2 analytically in time OpnnzpPq ` nnzpCqq  and hence we obtain a δ-OSO which takes
OppnnzpPq ` nnzpCq log 1
δq total work. Parallelizing matrix-vector multiplications eliminates the
(cid:32)
dependence on nnzpPq and nnzpCq  at the cost of another logpNq term.
Lemma 4.14. Each argmax in Algorithm 2 can be computed as follows:
xk “ min
yk “ proj∆`
zk “ proj∆`
In particular  we can compute xk  yk  zk in OpnnzpPq ` nnzpCqq work and Oplog Nq parallel time.

(˘
(˘
for all j P rns.
2p(cid:107)P(cid:107)8`1qpa1 ´ P xk´1 log xk´1q
2p(cid:107)C(cid:107)8`1qpa2 ´ Cxk´1 log xk´1q

exp
exp

˘
(cid:32)
(cid:32)
P T yk`CT zk ´ 1

a

1

`
`

(

`

exp

p

c

  1n

1

Acknowledgements

We thank Richard Peng for many important pointers and discussions.

References

[AO19] Zeyuan Allen-Zhu and Lorenzo Orecchia. Nearly linear-time packing and covering
LP solvers - achieving width-independence and -convergence. Math. Program.  175(1-
2):307–353  2019.

[BBR04] Yair Bartal  John W. Byers  and Danny Raz. Fast  distributed approximation algorithms
for positive linear programming with applications to ﬂow control. SIAM J. Comput. 
33(6):1261–1279  2004.

[Bec15] Amir Beck. On the convergence of alternating minimization for convex programming
with applications to iteratively reweighted least squares and decomposition schemes.
SIAM Journal on Optimization  25(1):185–209  2015.

[BGM14] Bahman Bahmani  Ashish Goel  and Kamesh Munagala. Efﬁcient primal-dual graph
algorithms for mapreduce. In Algorithms and Models for the Web Graph - 11th Inter-
national Workshop  WAW 2014  Beijing  China  December 17-18  2014  Proceedings 
pages 59–78  2014.

[BI06] Daniel Bienstock and Garud Iyengar. Approximating fractional packings and coverings

in o(1/epsilon) iterations. SIAM J. Comput.  35(4):825–854  2006.

[Bub14] Sébastien Bubeck. Theory of convex optimization for machine learning. arXiv preprint

arXiv:1405.4980  15  2014.

[Cha00] Moses Charikar. Greedy approximation algorithms for ﬁnding dense components in a
graph. In Proceedings of the Third International Workshop on Approximation Algorithms
for Combinatorial Optimization  APPROX ’00  pages 84–95  Berlin  Heidelberg  2000.
[GK96] Michael D. Grigoriadis and Leonid G. Khachiyan. Approximate minimum-cost multi-

commodity ﬂows in õ(epsilon-2knm) time. Math. Program.  75:477–482  1996.

[GN15] Cristóbal Guzmán and Arkadi Nemirovski. On lower complexity bounds for large-scale

smooth convex optimization. Journal of Complexity  31(1):1 – 14  2015.

9

[LN93] Michael Luby and Noam Nisan. A parallel approximation algorithm for positive linear
programming. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory
of Computing  May 16-18  1993  San Diego  CA  USA  pages 448–457  1993.

[MRWZ16] Michael W. Mahoney  Satish Rao  Di Wang  and Peng Zhang. Approximating the
solution to mixed packing and covering lps in parallel o(epsilonˆ{-3}) time. In 43rd
International Colloquium on Automata  Languages  and Programming  ICALP 2016 
July 11-15  2016  Rome  Italy  pages 52:1–52:14  2016.

[Nes05] Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program. 

103(1):127–152  2005.

[Nes07] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities

and related problems. Math. Program.  109(2-3):319–344  2007.

[Nes12] Yurii Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization

problems. SIAM Journal on Optimization  22(2):341–362  2012.

fractional packing and covering problems. Math. Oper. Res.  20(2):257–301  1995.

[PST95] Serge A. Plotkin  David B. Shmoys  and Éva Tardos. Fast approximation algorithms for
[She17] Jonah Sherman. Area-convexity  l8 regularization  and undirected multicommodity
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
ﬂow.
Computing  STOC 2017  Montreal  QC  Canada  June 19-23  2017  pages 452–460 
2017.

[WRM16] Di Wang  Satish Rao  and Michael W. Mahoney. Uniﬁed acceleration method for packing
and covering problems via diameter reduction. In 43rd International Colloquium on
Automata  Languages  and Programming  ICALP 2016  July 11-15  2016  Rome  Italy 
pages 50:1–50:13  2016.

[You01] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering.
In 42nd Annual Symposium on Foundations of Computer Science  FOCS 2001  14-17
October 2001  Las Vegas  Nevada  USA  pages 538–546  2001.

[You14] Neal E. Young. Nearly linear-time approximation schemes for mixed packing/covering

and facility-location linear programs. CoRR  abs/1407.3015  2014.

[ZN01] Edo Zurel and Noam Nisan. An efﬁcient approximate allocation algorithm for com-
binatorial auctions. In Proceedings 3rd ACM Conference on Electronic Commerce
(EC-2001)  Tampa  Florida  USA  October 14-17  2001  pages 125–136  2001.

10

,Digvijay Boob
Saurabh Sawlani
Di Wang