2018,Bilinear Attention Networks,Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively.  However  the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem  co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper  we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels  while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore  we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets  showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.,Bilinear Attention Networks

Jin-Hwa Kim1⇤  Jaehyun Jun2  Byoung-Tak Zhang2 3

1SK T-Brain  2Seoul National University  3Surromind Robotics
jnhwkim@sktbrain.com  {jhjun btzhang}@bi.snu.ac.kr

Abstract

Attention networks in multimodal learning provide an efﬁcient way to utilize given
visual information selectively. However  the computational cost to learn attention
distributions for every pair of multimodal input channels is prohibitively expensive.
To solve this problem  co-attention builds two separate attention distributions for
each modality neglecting the interaction between multimodal inputs. In this paper 
we propose bilinear attention networks (BAN) that ﬁnd bilinear attention distri-
butions to utilize given vision-language information seamlessly. BAN considers
bilinear interactions among two groups of input channels  while low-rank bilinear
pooling extracts the joint representations for each pair of channels. Furthermore 
we propose a variant of multimodal residual networks to exploit eight-attention
maps of the BAN efﬁciently. We quantitatively and qualitatively evaluate our
model on visual question answering (VQA 2.0) and Flickr30k Entities datasets 
showing that BAN signiﬁcantly outperforms previous methods and achieves new
state-of-the-arts on both datasets.

1

Introduction

Machine learning for computer vision and natural language processing accelerates the advancement of
artiﬁcial intelligence. Since vision and natural language are the major modalities of human interaction 
understanding and reasoning of vision and natural language information become a key challenge. For
instance  visual question answering involves a vision-language cross-grounding problem. A machine
is expected to answer given questions like "who is wearing glasses?"  "is the umbrella upside down?" 
or "how many children are in the bed?" exploiting visually-grounded information.
For this reason  visual attention based models have succeeded in multimodal learning tasks  identifying
selective regions in a spatial map of an image deﬁned by the model. Also  textual attention can be
considered along with visual attention. The attention mechanism of co-attention networks [36  18  20 
39] concurrently infers visual and textual attention distributions for each modality. The co-attention
networks selectively attend to question words in addition to a part of image regions. However  the co-
attention neglects the interaction between words and visual regions to avoid increasing computational
complexity.
In this paper  we extend the idea of co-attention into bilinear attention which considers every pair
of multimodal channels  e.g.  the pairs of question words and image regions. If the given question
involves multiple visual concepts represented by multiple words  the inference using visual attention
distributions for each word can exploit relevant information better than that using single compressed
attention distribution.
From this background  we propose bilinear attention networks (BAN) to use a bilinear attention
distribution  on top of low-rank bilinear pooling [15]. Notice that the BAN exploits bilinear inter-
actions between two groups of input channels  while low-rank bilinear pooling extracts the joint
representations for each pair of channels. Furthermore  we propose a variant of multimodal residual

⇤This work was done while at Seoul National University.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Overview

• After getting bilinear attention maps  we can stack multiple BANs.

What is the mustache 
made of ?

XTU

ρ

GRU

All hidden 
X

states

K

p

Y

φ

Object Detection

∙

VTY

K

= ρ

φ

att_1

att_2

x

o ft m a

S

K

Step 1. Bilinear Attention Maps

Residual Learning

K

U’TX
ρ

φ

1

ρ

Att_1

K
YTV’
1

=

φ

X’

X
+
K=N
repeat 1→ρ

Residual Learning
φ

ρ

1

K
YTV’’

U’’TX’
ρ

X’
+
K
repeat 1→ρ
Step 2. Bilinear Attention Networks

=φ

Att_2

1

Sum 
Pooling

MLP  
classiﬁer

Figure 1: Overview of the two-glimpse BAN. Two multi-channel inputs  -object detection features
and ⇢-length GRU hidden vectors  are used to get bilinear attention maps and joint representations to
be used by a classiﬁer. For the deﬁnition of the BAN  see the text in Section 3.

networks (MRN) to efﬁciently utilize the multiple bilinear attention maps of the BAN  unlike the
previous works [6  15] where multiple attention maps are used by concatenating the attended features.
Since the proposed residual learning method for BAN exploits residual summations instead of con-
catenation  which leads to parameter-efﬁciently and performance-effectively learn up to eight-glimpse
BAN. For the overview of the two-glimpse BAN  please refer to Figure 1.
Our main contributions are:

on top of the low-rank bilinear pooling technique.

• We propose the bilinear attention networks (BAN) to learn and use bilinear attention distributions 
• We propose a variant of multimodal residual networks (MRN) to efﬁciently utilize the multiple
bilinear attention maps generated by our model. Unlike previous works  our method successfully
utilizes up to 8 attention maps.

• Finally  we validate our proposed method on a large and highly-competitive dataset  VQA
2.0 [8]. Our model achieves a new state-of-the-art maintaining simplicity of model structure.
Moreover  we evaluate the visual grounding of bilinear attention map on Flickr30k Entities [23]
outperforming previous methods  along with 25.37% improvement of inference speed taking
advantage of the processing of multi-channel inputs.

2 Low-rank bilinear pooling

We ﬁrst review the low-rank bilinear pooling and its application to attention networks [15]  which
uses single-channel input (question vector) to combine the other multi-channel input (image features)
as single-channel intermediate representation (attended feature).
Low-rank bilinear model. The previous works [35  22] proposed a low-rank bilinear model to
reduce the rank of bilinear weight matrix Wi to give regularity. For this  Wi is replaced with the
i   where Ui 2 RN⇥d and Vi 2 RM⇥d. As a result 
multiplication of two smaller matrices UiVT
this replacement makes the rank of Wi to be at most d  min(N  M ). For the scalar output fi (bias
terms are omitted without loss of generality):
(1)
where 1 2 Rd is a vector of ones and  denotes Hadamard product (element-wise multiplication).
Low-rank bilinear pooling. For a vector output f  a pooling matrix P is introduced:

fi = xT Wiy ⇡ xT UiVT

i y = 1T (UT

i x  VT

i y)

f = PT (UT x  VT y)

(2)
where P 2 Rd⇥c  U 2 RN⇥d  and V 2 RM⇥d. It allows U and V to be two-dimensional tensors
by introducing P for a vector output f 2 Rc  signiﬁcantly reducing the number of parameters.
Unitary attention networks. Attention provides an efﬁcient mechanism to reduce input channel
by selectively utilizing given information. Assuming that a multi-channel input Y consisting of
 = |{yi}| column vectors  we want to get single channel ˆy from Y using the weights {↵i}:

(3)

ˆy =Xi

↵iyi

2

where ↵ represents an attention distribution to selectively combine  input channels. Using the
low-rank bilinear pooling  the ↵ is deﬁned by the output of softmax function as:

↵ := softmax⇣PT(UT x · 1T )  (VT Y)⌘

(4)
where ↵ 2 RG⇥  P 2 Rd⇥G  U 2 RN⇥d  x 2 RN  1 2 R  V 2 RM⇥d  and Y 2 RM⇥. If
G > 1  multiple glimpses (a.k.a. attention heads) are used [13  6  15]  then ˆy =fG
g=1Pi ↵g iyi  the
concatenation of attended outputs. Finally  two single channel inputs x and ˆy can be used to get the
joint representation using the other low-rank bilinear pooling for a classiﬁer.

3 Bilinear attention networks
We generalize a bilinear model for two multi-channel inputs  X 2 RN⇥⇢ and Y 2 RM⇥  where
⇢ = |{xi}| and  = |{yj}|  the numbers of two input channels  respectively. To reduce both input
channel simultaneously  we introduce bilinear attention map A2 R⇢⇥ as follows:
(5)
where U0 2 RN⇥K  V0 2 RM⇥K  (XT U0)k 2 R⇢  (YT V0)k 2 R  and f0k denotes the k-th
element of intermediate representation. The subscript k for the matrices indicates the index of column.
Notice that Equation 5 is a bilinear model for the two groups of input channels where A in the middle
is a bilinear weight matrix. Interestingly  Equation 5 can be rewritten as:

k A(YT V0)k

f0k = (XT U0)T

f0k =

⇢Xi=1

Xj=1

⇢Xi=1

Xj=1

Ai j(XT

i U0k)(V0T

k Yj) =

Ai jXT

i (U0kV0T

k )Yj

(6)

where Xi and Yj denotes the i-th channel (column) of input X and the j-th channel (channel) of
input Y  respectively  U0k and V0k denotes the k-th column of U0 and V0 matrices  respectively 
and Ai j denotes an element in the i-th row and the j-th column of A. Notice that  for each pair of
channels  the 1-rank bilinear representation of two feature vectors is modeled in XT
k )Yj
of Equation 6 (eventually at most K-rank bilinear pooling for f0 2 RK). Then  the bilinear joint
representation is f = PT f0 where f 2 RC and P 2 RK⇥C. For the convenience  we deﬁne the
bilinear attention networks as a function of two multi-channel inputs parameterized by a bilinear
attention map as follows:

i (U0kV0T

(7)
Bilinear attention map. Now  we want to get the attention map similarly to Equation 4. Using
Hadamard product and matrix-matrix multiplication  the attention map A is deﬁned as:

f = BAN(X  Y;A).

(8)
where 1 2 R⇢  p 2 RK0  and remind that A2 R⇢⇥. The softmax function is applied element-
wisely. Notice that each logit Ai j of the softmax is the output of low-rank bilinear pooling as:
(9)

A := softmax⇣(1 · pT )  XT UVT Y⌘
Ai j = pT(UT Xi)  (VT Yj).
Ag := softmax⇣(1 · pT
g )  XT UVT Y⌘

(10)
where the parameters of U and V are shared  but not for pg where g denotes the index of glimpses.
Residual learning of attention. Inspired by multimodal residual networks (MRN) from Kim et al.
[14]  we propose a variant of MRN to integrate the joint representations from the multiple bilinear
attention maps. The i + 1-th output is deﬁned as:

The multiple bilinear attention maps can be extended as follows:

fi+1 = BANi(fi  Y;Ai) · 1T + fi

(11)
where f0 = X (if N = K) and 1 2 R⇢. Here  the size of fi is the same with the size of X as
successive attention maps are processed. To get the logits for a classiﬁer  e.g.  two-layer MLP  we
sum over the channel dimension of the last output fG  where G is the number of glimpses.
Time complexity. When we assume that the number of input channels is smaller than feature
sizes  M  N  K    ⇢  the time complexity of the BAN is the same with the case of one
multi-channel input as O(KM) for single glimpse model. Since the BAN consists of matrix chain
multiplication and exploits the property of low-rank factorization in the low-rank bilinear pooling.

3

4 Related works

Multimodal factorized bilinear pooling. Yu et al. [39] extends low-rank bilinear pooling [15]
using the rank > 1. They remove a projection matrix P  instead  d in Equation 2 is replaced with
much smaller k while U and V are three-dimensional tensors. However  this generalization was
not effective for BAN  at least in our experimental setting. Please see BAN-1+MFB in Figure 2b
where the performance is not signiﬁcantly improved from that of BAN-1. Furthermore  the peak GPU
memory consumption is larger due to its model structure which hinders to use multiple-glimpse BAN.
Co-attention networks. Xu and Saenko [36] proposed the spatial memory network model estimating
the correlation among every image patches and tokens in a sentence. The estimated correlation C
is deﬁned as (UX)T Y in our notation. Unlike our method  they get an attention distribution

↵ = softmax maxi=1 ... ⇢(Ci) 2 R⇢ where the logits to softmax are the maximum values in each

row vector of C. The attention distribution for the other input can be calculated similarly. There are
variants of co-attention networks [18  20]  especially  Lu et al. [18] sequentially get two attention
distributions conditioning on the other modality. Recently  Yu et al. [39] reduce the co-attention
method into two steps  self-attention for a question embedding and the question-conditioned attention
for a visual embedding. However  these co-attention approaches use separate attention distributions
for each modality  neglecting the interaction between the modalities what we consider and model.

5 Experiments

5.1 Datasets
Visual Question Answering (VQA). We evaluate on the VQA 2.0 dataset [1  8]  which is improved
from the previous version to emphasize visual understanding by reducing the answer bias in the
dataset. This improvement pushes the model to have the more effective joint representation of question
and image  which ﬁts the motivation of our bilinear attention approach. The VQA evaluation metric
considers inter-human variability deﬁned as Accuracy(ans) = min(#humans that said ans/3  1).
Note that reporting accuracies are averaged over all ten choose nine sets of ground-truths. The test
set is split into test-dev  test-standard  test-challenge  and test-reserve. The annotations for the test set
are unavailable except the remote evaluation servers.
Flickr30k Entities. For the evaluation of visual grounding by the bilinear attention maps  we use
Flickr30k Entities [23] consisting of 31 783 images [38] and 244 035 annotations that multiple
entities (phrases) in a sentence for an image are mapped to the boxes on the image to indicate the
correspondences between them. The task is to localize a corresponding box for each entity. In this
way  visual grounding of textual information is quantitatively measured. Following the evaluation
metric [23]  if a predicted box has the intersection over union (IoU) of overlapping area with one
of the ground-truth boxes which are greater than or equal to 0.5  the prediction for a given entity is
correct. This metric is called Recall@1. If K predictions are permitted to ﬁnd at least one correction 
it is called Recall@K. We report Recall@1  5  and 10 to compare state-of-the-arts (R@K in Table 4).
The upper bound of performance depends on the performance of object detection if the detector
proposes candidate boxes for the prediction.

5.2 Preprocessing
Question embedding. For VQA  we get a question embedding XT 2 R14⇥N using GloVe word
embeddings [21] and the outputs of Gated Recurrent Unit (GRU) [5] for every time-steps up to the
ﬁrst 14 tokens following the previous work [29]. The questions shorter than 14 words are end-padded
with zero vectors. For Flickr30k Entities  we use a full length of sentences (82 is maximum) to get all
entities. We mark the token positions which are at the end of each annotated phrase. Then  we select
a subset of the output channels of GRU using these positions  which makes the number of channels is
the number of entities in a sentence. The word embeddings and GRU are ﬁne-tuned in training.
Image features. We use the image features extracted from bottom-up attention [2]. These features are
the output of Faster R-CNN [25]  pre-trained using Visual Genome [17]. We set a threshold for object
detection to get  = 10 to 100 objects per image. The features are represented as YT 2 R⇥2 048 
which is ﬁxed while training. To deal with variable-channel inputs  we mask the padding logits with
minus inﬁnite to get zero probability from softmax avoiding underﬂow.

4

5.3 Nonlinearity and classiﬁer
Nonlinearity. We use ReLU [19] to give nonlinearity to BAN:

f0k = (XT U0)T

k · A · (YT V0)k
A :=(1 · pT )  (XT U) · (VT Y).

(12)

(13)

where  denotes ReLU(x) := max(x  0). For the attention maps  the logits are deﬁned as:

Classiﬁer. For VQA  we use a two-layer multi-layer perceptron as a classiﬁer for the ﬁnal joint
representation fG. The activation function is ReLU. The number of outputs is determined by the
minimum occurrence of the answer in unique questions as nine times in the dataset  which is 3 129.
Binary cross entropy is used for the loss function following the previous work [29]. For Flickr30k
Entities  we take the output of bilinear attention map  and binary cross entropy is used for this output.

5.4 Hyperparameters and regularization
Hyperparameters. The size of image features and question embeddings are M = 2  048 and
N = 1  024  respectively. The size of joint representation C is the same with the rank K in low-
rank bilinear pooling  C = K = 1  024  but K0 = K ⇥ 3 is used in the bilinear attention maps
to increase a representational capacity for residual learning of attention. Every linear mapping is
regularized by Weight Normalization [27] and Dropout [28] (p = .2  except for the classiﬁer with
.5). Adamax optimizer [16]  a variant of Adam based on inﬁnite norm  is used. The learning rate is
min(ie3  4e3) where i is the number of epochs starting from 1  then after 10 epochs  the learning
rate is decayed by 1/4 for every 2 epochs up to 13 epochs (i.e. 1e3 for 11-th and 2.5e4 for 13-th
epoch). We clip the 2-norm of vectorized gradients to .25. The batch size is 512.
Regularization. For the test split of VQA  both train and validation splits are used for training. We
augment a subset of Visual Genome [17] dataset following the procedure of the previous works [29].
Accordingly  we adjust the model capacity by increasing all of N  C  and K to 1 280. And  G = 8
glimpses are used. For Flickr30k Entities  we use the same test split of the previous methods [23] 
without additional hyperparameter tuning from VQA experiments.

6 VQA results and discussions

6.1 Quantitative results
Comparison with state-of-the-arts. The ﬁrst row in Table 1 shows 2017 VQA Challenge winner
architecture [2  29]. BAN signiﬁcantly outperforms this baseline and successfully utilize up to eight
bilinear attention maps to improve its performance taking advantage of residual learning of attention.
As shown in Table 3  BAN outperforms the latest model [39] which uses the same bottom-up attention
feature [2] by a substantial margin. BAN-Glove uses the concatenation of 300-dimensional Glove
word embeddings and the semantically-closed mixture of these embeddings (see Appendix A.1).
Notice that similar approaches can be found in the competitive models [6  39] in Table 3 with a
different initialization strategy for the same 600-dimensional word embedding. BAN-Glove-Counter
uses both the previous 600-dimensional word embeddings and counting module [41]  which exploits
spatial information of detected object boxes from the feature extractor [2]. The learned representation
c 2 R+1 for the counting mechanism is linearly projected and added to the joint representation after
applying ReLU (see Equation 15 in Appendix A.2). In Table 5 (Appendix)  we compare with the
entries in the leaderboard of both VQA Challenge 2017 and 2018 achieving the 1st place at the time
of submission (our entry is not shown in the leaderboard since challenge entries are not visible).
Comparison with other attention methods. Unitary attention has a similar architecture with Kim
et al. [15] where a question embedding vector is used to calculate the attentional weights for multiple
image features of an image. Co-attention has the same mechanism of Yu et al. [39]  similar to Lu et al.
[18]  Xu and Saenko [36]  where multiple question embeddings are combined as single embedding
vector using a self-attention mechanism  then unitary visual attention is applied. Table 2 conﬁrms that
bilinear attention is signiﬁcantly better than any other attention methods. The co-attention is slightly
better than simple unitary attention. In Figure 2a  co-attention suffers overﬁtting more severely
(green) than any other methods  while bilinear attention (blue) is more regularized compared with the

5

y
p
o
r
t
n
E

4.2

3.9

3.6

3.3

3.0

 

o
c
S
n
o
i
t
a
d

i
l

a
V

55

50

45

40

35

Att_1
Att_2
Att_3
Att_4

2 Glimpses
4 Glimpses
8 Glimpses

6

4

2

0

14

12

8
10
Epoch

Table 1: Validation scores on VQA 2.0
dataset for the number of glimpses of the
BAN. The standard deviations are reported
after ± using three random initialization.
(a)
(b)

18

85

16

e
r
o
c
s
 
n
o
i
t
a
d

i
l

a
V

75
Model
65
Bottom-Up [29]
BAN-1
55
BAN-2
45
BAN-4
BAN-8
35
BAN-12
3

7

1

5

VQA Score
63.37 ±0.21
bi-att train
co-att train
65.36 ±0.14
uni-att train
65.61 ±0.10
bi-att val
65.81 ±0.09
co-att val
uni-att val
66.00 ±0.11
66.04 ±0.08
11 13 15 17 19

9
Epoch

8

4

5

6

7

3

2

1

0

(c)

The number of used glimpses

Table 2: Validation scores on VQA 2.0 dataset for
attention and integration mechanisms. The nParams
indicates the number of parameters. Note that the
hidden sizes of unitary attention and co-attention are
70
6.0
1 280  while 1 024 for the BAN.
65
5.0
60
nParams
VQA Score
4.0
55
31.9M 64.59 ±0.04
50
32.5M 64.79 ±0.06
32.2M 65.36 ±0.14
45
40
44.8M 65.81 ±0.09
35
44.8M 64.78 ±0.08
51.1M 64.71 ±0.21
Epoch

Model
Unitary attention
BAN-1
Co-attention
BAN-2
Bilinear attention
BAN-4
BAN-8
BAN-4 (residual)
BAN-12
BAN-4 (sum)
8 10 12
2
BAN-4 (concat)
Used glimpses

1 3 5 7 9 11 1315 1719

y
p
o
r
t
n
E

Att_2
Att_4

Att_1
Att_3

1.0

0.0

2.0

3.0

4

0

6

e
r
o
c
s
 
n
o
i
t
a
d

i
l

a
V

(a)

e
r
o
c
s
 

n
o
i
t
a
d

i
l

a
V

85

74

63

52

41

30

Bi-Att train
Co-Att train
Uni-Att train
Bi-Att val
Co-Att val
Uni-Att val

1

4

7 10 13 16 19
Epoch

(b)

e
r
o
c
s
 

n
o
i
t
a
d

i
l

a
V

67.0

65.5

64.0

62.5

61.0
59.5

58.0

(c)

e
r
o
c
s
 

n
o
i
t
a
d

i
l

a
V

Uni-Att
Co-Att
BAN-1
BAN-4
BAN-1+MFB

70
65
60
55
50
45
40
35

0

(d)

y
p
o
r
t
n
E

8.0
6.7

5.3

4.0

2.7

1.3

0.0

Att_1
Att_2
Att_3
Att_4

1 4 7 10 13 16 19

Epoch

BAN-1
BAN-2
BAN-4
BAN-8
BAN-12

8 10 12

6

4

2
Used glimpses

0M 15M 30M 45M 60M
The number of parameters

Figure 2: (a) learning curves. Bilinear attention (bi-att) is more robust to overﬁtting than unitary
attention (uni-att) and co-attention (co-att). (b) validation scores for the number of parameters. The
error bar indicates the standard deviation among three random initialized models  although it is too
small to be noticed for over-15M parameters. (c) ablation study for the ﬁrst-N-glimpses (x-axis) used
in the evaluation. (d) the information entropy (y-axis) for each attention map in the four-glimpse
BAN. The entropy of multiple attention maps is converged to certain levels.

others. In Figure 2b  BAN is the most parameter-efﬁcient among various attention methods. Notice
that four-glimpse BAN more parsimoniously utilizes its parameters than one-glimpse BAN does.

6.2 Residual learning of attention

Comparison with other approaches. In the second section of Table 2  the residual learning of
attention signiﬁcantly outperforms the other methods  sum  i.e.  fG =Pi BANi(X  Y;Ai)  and
concatenation (concat)  i.e.  fG = kiBANi(X  Y;Ai). Whereas  the difference between sum and
concat is not signiﬁcantly different. Notice that the number of parameters of concat is larger than the
others since the input size of the classiﬁer is increased.
Ablation study. An interesting property of residual learning is robustness toward arbitrary abla-
tions [31]. To see the relative contributions  we observe the learning curve of validation scores
when incremental ablation is performed. First  we train {1 2 4 8 12}-glimpse models using training
split. Then  we evaluate the model on validation split using the ﬁrst N attention maps. Hence  the
intermediate representation fN is directly fed into the classiﬁer instead of fG. As shown in Figure 2c 
the accuracy gain of the ﬁrst glimpse is the highest  then the gain is smoothly decreased as the number
of used glimpses is increased.
Entropy of attention. We analyze the information entropy of attention distributions in a four-glimpse
BAN. As shown in Figure 2d  the mean entropy of each attention for validation split is converged
to a different level of values. This result is repeatably observed in the other number of glimpse
models. Our speculation is the multi-attention maps do not equally contribute similarly to voting
by committees  but the residual learning by the multi-step attention. We argue that this is a novel
observation where the residual learning [9] is used for stacked attention networks.

1

6

Figure 3: Visualization of the bilinear attention maps for two-glimpse BAN. The left and right
groups indicate the ﬁrst and second bilinear attention maps (right in each group  log-scaled) and the
visualized image (left in each group). The most salient six boxes (1-6 numbered in the images and
x-axis of the grids) in the ﬁrst attention map determined by marginalization are visualized on both
images to compare. The model gives the correct answer  brown.

(a) A girl in a yellow tennis suit  green 
visor and white tennis shoes holding a 
tennis racket in a position where she is 
going to hit the tennis ball.

(b) A man in a denim shirt and 
pants is smoking a cigarette while 
playing a cello for money.

(c) A male conductor wearing all black 
leading an orchestra and choir on a 
brown stage playing and singing a 
musical number.

Figure 4: Visualization examples from the test split of Flickr30k Entities are shown. Solid-lined
boxes indicate predicted phrase localizations and dashed-line boxes indicate the ground-truth. If there
are multiple ground-truth boxes  the closest box is shown to investigate. Each color of a phrase is
matched with the corresponding color of predicted and ground-truth boxes. Best view in color.

6.3 Qualitative analysis

The visualization for a two-glimpse BAN is shown in Figure 3. The question is “what color are the
pants of the guy skateboarding”. The question and content words  what  pants  guy  and skateboarding
and skateboarder’s pants in the image are attended. Notice that the box 2 (orange) captured the sitting
man’s pants in the bottom.

7 Flickr30k entities results and discussions

To examine the capability of bilinear attention map to capture vision-language interactions  we
conduct experiments on Flickr30k Entities [23]. Our experiments show that BAN outperforms the
previous state-of-the-art on the phrase localization task with a large margin of 4.48% at a high speed
of inference.
Performance. In Table 4  we compare with other previous approaches. Our bilinear attention map
to predict the boxes for the phrase entities in a sentence achieves new state-of-the-art with 69.69%
for Recall@1. This result is remarkable considering that BAN does not use any additional features
like box size  color  segmentation  or pose-estimation [23  37]. Note that both Query-Adaptive
RCNN [10] and our off-the-shelf object detector [2] are based on Faster RCNN [25] and pre-trained
on Visual Genome [17]. Compared to Query-Adaptive RCNN  the parameters of our object detector
are ﬁxed and only used to extract 10-100 visual features and the corresponding box proposals.
Type. In Table 6 (included in Appendix)  we report the results for each type of Flickr30k Entities.
Notice that clothing and body parts are signiﬁcantly improved to 74.95% and 47.23%  respectively.
Speed. The faster inference is achieved taking advantage of multi-channel inputs in our BAN. Unlike
previous methods  BAN ables to infer multiple entities in a sentence which can be prepared as a

7

Table 3: Test-dev and test-standard scores of single-model on VQA 2.0 dataset to compare state-of-
the-arts  trained on training and validation splits  and Visual Genome for feature extraction or data
augmentation. † This model can be found in https://github.com/yuzcccc/vqa-mfb  which is
not published in the paper.

Model
Bottom-Up [2  29]
MFH [39]
Counter [41]
MFH+Bottom-Up [39]†
BAN (ours)
BAN+Glove (ours)
BAN+Glove+Counter (ours)

-

-

-

56.05

81.82

44.21

Overall Yes/no Number Other Test-std
65.32
65.67
66.12
68.09
68.76
69.52
69.66
70.04

51.62
49.56
50.93
50.66
54.04

58.97
59.89
60.26
60.50
60.52

83.14
84.27
85.31
85.46
85.42

70.35

68.41

-

-
-
-

Table 4: Test split results for Flickr30k Entities. We report the average performance of our three
randomly-initialized models (the standard deviation of R@1 is 0.17). Upper Bound of performance
asserted by object detector is shown. † box size and color information are used as additional features.
‡ semantic segmentation  object detection  and pose-estimation is used as additional features. Notice
that the detectors of Hinami and Satoh [10] and ours [2] are based on Faster RCNN [25]  pre-trained
using Visual Genome dataset [17].

Detector
MCG [3]

Model
Zhang et al. [40]
Hu et al. [11]
Rohrbach et al. [26]
Wang et al. [33]
Wang et al. [32]
Rohrbach et al. [26]
Fukui et al. [6]
Plummer et al. [23]
Yeh et al. [37]
Hinami and Satoh [10] Query-Adaptive RCNN [10]
BAN (ours)

Edge Boxes [42]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]†
YOLOv2 [24]‡
Bottom-Up [2]

R@1
28.5
27.8
42.43
42.08
43.89
48.38
48.69
50.89
53.97
65.21
69.69

R@5 R@10 Upper Bound
52.7
-
-
-

61.3
62.9
-
-

64.46

68.66

-
76.9
77.90
76.91
76.91
77.90

85.12

-

-
-

-
-

-
-

-
-

-
-

71.09

75.73

84.22

86.35

87.45

multi-channel input. Therefore  the number of forwardings to infer is signiﬁcantly decreased. In our
experiment  BAN takes 0.67 ms/entity whereas the setting that single entity as an example takes 0.84
ms/entity  achieving 25.37% improvement. We emphasize that this property is a novel in our model
that considers every interaction among vision-language multi-channel inputs.
Visualization. Figure 4 shows the examples from the test split of Flickr30k Entities. The entities
which have visual properties  i.e.  a yellow tennis suit and white tennis shoes in Figure 4a  and a
denim shirt in Figure 4b  are correct. However  a relatively small object (e.g.  a cigarette in Figure 4b)
and the entity that requires semantic inference (e.g.  a male conductor in Figure 4c) are incorrect.

8 Conclusions

BAN gracefully extends unitary attention networks exploiting bilinear attention maps  where the joint
representations of multimodal multi-channel inputs are extracted using low-rank bilinear pooling.
Although BAN considers every pair of multimodal input channels  the computational cost remains in
the same magnitude  since BAN consists of matrix chain multiplication for efﬁcient computation. The
proposed residual learning of attention efﬁciently uses up to eight bilinear attention maps  keeping
the size of intermediate features constant. We believe our BAN gives a new opportunity to learn the
richer joint representation for multimodal multi-channel inputs  which appear in many real-world
problems.

8

Acknowledgments
We would like to thank Kyoung-Woon On  Bohyung Han  Hyeonwoo Noh  Sungeun Hong  Jaesun
Park  and Yongseok Choi for helpful comments and discussion. Jin-Hwa Kim was supported by 2017
Google Ph.D. Fellowship in Machine Learning and Ph.D. Completion Scholarship from College of
Humanities  Seoul National University. This work was funded by the Korea government (IITP-2017-
0-01772-VTT  IITP-R0126-16-1072-SW.StarLab  2018-0-00622-RMI  KEIT-10060086-RISF). The
part of computing resources used in this study was generously shared by Standigm Inc.

References
[1] Aishwarya Agrawal  Jiasen Lu  Stanislaw Antol  Margaret Mitchell  C Lawrence Zitnick  Devi
Parikh  and Dhruv Batra. Vqa: Visual question answering. International Journal of Computer
Vision  123(1):4–31  2017.

[2] Peter Anderson  Xiaodong He  Chris Buehler  Damien Teney  Mark Johnson  Stephen Gould 
and Lei Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
Answering. arXiv preprint arXiv:1707.07998  2017.

[3] Pablo Arbeláez  Jordi Pont-Tuset  Jonathan T Barron  Ferran Marques  and Jitendra Malik.
In IEEE conference on computer vision and pattern

Multiscale combinatorial grouping.
recognition  pages 328–335  2014.

[4] Hedi Ben-younes  Rémi Cadene  Matthieu Cord  and Nicolas Thome. MUTAN: Multimodal
Tucker Fusion for Visual Question Answering. In IEEE International Conference on Computer
Vision  pages 2612–2620  2017.

[5] Kyunghyun Cho  Bart Van Merriënboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares 
Holger Schwenk  and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. In 2014 Conference on Empirical Methods in
Natural Language Processing  pages 1724–1734  2014.

[6] Akira Fukui  Dong Huk Park  Daylen Yang  Anna Rohrbach  Trevor Darrell  and Marcus
Rohrbach. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
Grounding. arXiv preprint arXiv:1606.01847  2016.

[7] Ross Girshick. Fast r-cnn. In IEEE International Conference on Computer Vision  pages

1440–1448  2015.

[8] Yash Goyal  Tejas Khot  Douglas Summers-Stay  Dhruv Batra  and Devi Parikh. Making the V
in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In
IEEE Conference on Computer Vision and Pattern Recognition  2017.

[9] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep Residual Learning for Image

Recognition. In IEEE Conference on Computer Vision and Pattern Recognition  2016.

[10] Ryota Hinami and Shin’ichi Satoh. Query-Adaptive R-CNN for Open-Vocabulary Object

Detection and Retrieval. arXiv preprint arXiv:1711.09509  2017.

[11] Ronghang Hu  Huazhe Xu  Marcus Rohrbach  Jiashi Feng  Kate Saenko  and Trevor Darrell.
Natural language object retrieval. In IEEE Computer Vision and Pattern Recognition  pages
4555–4564  2016.

[12] Ilija Ilievski and Jiashi Feng. A Simple Loss Function for Improving the Convergence and

Accuracy of Visual Question Answering Models. 2017.

[13] Max Jaderberg  Karen Simonyan  Andrew Zisserman  and Koray Kavukcuoglu. Spatial Trans-
former Networks. In Advances in Neural Information Processing Systems 28  pages 2008–2016 
2015.

[14] Jin-Hwa Kim  Sang-Woo Lee  Donghyun Kwak  Min-Oh Heo  Jeonghee Kim  Jung-Woo Ha 
and Byoung-Tak Zhang. Multimodal Residual Learning for Visual QA. In Advances in Neural
Information Processing Systems 29  pages 361–369  2016.

9

[15] Jin-Hwa Kim  Kyoung Woon On  Woosang Lim  Jeonghee Kim  Jung-Woo Ha  and Byoung-Tak
Zhang. Hadamard Product for Low-rank Bilinear Pooling. In The 5th International Conference
on Learning Representations  2017.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization.

International Conference on Learning Representations  2015.

In

[17] Ranjay Krishna  Yuke Zhu  Oliver Groth  Justin Johnson  Kenji Hata  Joshua Kravitz  Stephanie
Chen  Yannis Kalantidis  Li-Jia Li  David A Shamma  Michael Bernstein  and Li Fei-Fei. Visual
genome: Connecting language and vision using crowdsourced dense image annotations. arXiv
preprint arXiv:1602.07332  2016.

[18] Jiasen Lu  Jianwei Yang  Dhruv Batra  and Devi Parikh. Hierarchical Question-Image Co-

Attention for Visual Question Answering. arXiv preprint arXiv:1606.00061  2016.

[19] Vinod Nair and Geoffrey E Hinton. Rectiﬁed Linear Units Improve Restricted Boltzmann

Machines. 27th International Conference on Machine Learning  pages 807–814  2010.

[20] Hyeonseob Nam  Jung-Woo Ha  and Jeonghee Kim. Dual Attention Networks for Multimodal
Reasoning and Matching. In IEEE Conference on Computer Vision and Pattern Recognition 
2016.

[21] Jeffrey Pennington  Richard Socher  and Christopher D Manning. GloVe: Global Vectors for
Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing  2014.

[22] Hamed Pirsiavash  Deva Ramanan  and Charless C. Fowlkes. Bilinear classiﬁers for visual
recognition. In Advances in Neural Information Processing Systems 22  pages 1482–1490 
2009.

[23] Bryan A. Plummer  Liwei Wang  Chris M. Cervantes  Juan C. Caicedo  Julia Hockenmaier 
and Svetlana Lazebnik. Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
Richer Image-to-Sentence Models. International Journal of Computer Vision  123:74–93  2017.

[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better  Faster  Stronger. In IEEE Computer

Vision and Pattern Recognition  2017.

[25] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster R-CNN: Towards Real-Time
Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence  39(6)  2017.

[26] Anna Rohrbach  Marcus Rohrbach  Ronghang Hu  Trevor Darrell  and Bernt Schiele. Grounding
of textual phrases in images by reconstruction. In European Conference on Computer Vision 
pages 817–834  2016.

[27] Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to

Accelerate Training of Deep Neural Networks. arXiv preprint arXiv:1602.07868  2016.

[28] Nitish Srivastava  Geoffrey E. Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhut-
dinov. Dropout : A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of
Machine Learning Research  15(1):1929–1958  2014.

[29] Damien Teney  Peter Anderson  Xiaodong He  and Anton van den Hengel. Tips and Tricks for Vi-
sual Question Answering: Learnings from the 2017 Challenge. arXiv preprint arXiv:1708.02711 
2017.

[30] Alexander Trott  Caiming Xiong  and Richard Socher.

Interpretable Counting for Visual

Question Answering. In International Conference on Learning Representations  2018.

[31] Andreas Veit  Michael J Wilber  and Serge Belongie. Residual Networks are Exponential
Ensembles of Relatively Shallow Networks. In Advances in Neural Information Processing
Systems 29  pages 550–558  2016.

10

[32] Liwei Wang  Yin Li  and Svetlana Lazebnik. Learning Deep Structure-Preserving Image-
Text Embeddings. In IEEE Conference on Computer Vision and Pattern Recognition  pages
5005–5013  2016.

[33] Mingzhe Wang  Mahmoud Azab  Noriyuki Kojima  Rada Mihalcea  and Jia Deng. Structured
Matching for Phrase Localization. In European Conference on Computer Vision  volume 9908 
pages 696–711  2016.

[34] Peng Wang  Qi Wu  Chunhua Shen  and Anton van den Hengel. The VQA-Machine: Learning
How to Use Existing Vision Algorithms to Answer New Questions. In Computer Vision and
Pattern Recognition (CVPR)  pages 1173–1182  2017.

[35] Lior Wolf  Hueihan Jhuang  and Tamir Hazan. Modeling appearances with low-rank SVM.

IEEE Conference on Computer Vision and Pattern Recognition  2007.

[36] Huijuan Xu and Kate Saenko. Ask  Attend and Answer: Exploring Question-Guided Spatial
Attention for Visual Question Answering. In European Conference on Computer Vision  2016.
[37] Raymond A Yeh  Jinjun Xiong  Wen-Mei W Hwu  Minh N Do  and Alexander G Schwing.
Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts. In
Advances in Neural Information Processing Systems 30  2017.

[38] Peter Young  Alice Lai  Micah Hodosh  and Julia Hockenmaier. From image descriptions
to visual denotations: New similarity metrics for semantic inference over event descriptions.
Transactions of the Association for Computational Linguistics  2:67–78  2014.

[39] Zhou Yu  Jun Yu  Chenchao Xiang  Jianping Fan  and Dacheng Tao. Beyond Bilinear: Gen-
eralized Multi-modal Factorized High-order Pooling for Visual Question Answering. IEEE
Transactions on Neural Networks and Learning Systems  2018.

[40] Jianming Zhang  Zhe Lin  Jonathan Brandt  Xiaohui Shen  and Stan Sclaroff. Top-Down Neural
Attention by Excitation Backprop. In European Conference on Computer Vision  volume 9908 
pages 543–559  2016.

[41] Yan Zhang  Jonathon Hare  and Adam Prügel-Bennett. Learning to Count Objects in Natural Im-
ages for Visual Question Answering. In International Conference on Learning Representations 
2018.

[42] C Lawrence Zitnick and Piotr Dollár. Edge boxes: Locating object proposals from edges. In

European Conference on Computer Vision  pages 391–405  2014.

11

,Sebastian Stober
Jin-Hwa Kim
Jaehyun Jun
Byoung-Tak Zhang