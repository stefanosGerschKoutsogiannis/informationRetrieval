2019,On the Convergence Rate of Training Recurrent Neural Networks,How can local-search methods such as stochastic gradient descent (SGD) avoid bad local minima in training multi-layer neural networks? Why can they fit random labels even given non-convex and non-smooth architectures? Most existing theory only covers networks with one hidden layer  so can we go deeper?

In this paper  we focus on recurrent neural networks (RNNs) which are multi-layer networks widely used in natural language processing.
They are harder to analyze than feedforward neural networks  because the \emph{same} recurrent unit is repeatedly applied across the entire time horizon of length $L$  which is analogous to feedforward networks of depth $L$. We show when the number of neurons is sufficiently large  meaning polynomial in the training data size and in $L$  then SGD is capable of minimizing the regression loss in the linear convergence rate. This gives theoretical evidence of how RNNs can memorize data.

More importantly  in this paper we build general toolkits to analyze multi-layer networks with ReLU activations. For instance  we prove why ReLU activations can prevent exponential gradient explosion or vanishing  and build a perturbation theory to analyze first-order approximation of multi-layer networks.,On the Convergence Rate of

Training Recurrent Neural Networks∗

Zeyuan Allen-Zhu
Microsoft Research AI

zeyuan@csail.mit.edu

Yuanzhi Li

Carnegie Mellon University
yuanzhil@andrew.cmu.edu

Zhao Song
UT-Austin

zhaos@utexas.edu

Abstract

How can local-search methods such as stochastic gradient descent (SGD) avoid
bad local minima in training multi-layer neural networks? Why can they ﬁt ran-
dom labels even given non-convex and non-smooth architectures? Most existing
theory only covers networks with one hidden layer  so can we go deeper?
In this paper  we focus on recurrent neural networks (RNNs) which are multi-layer
networks widely used in natural language processing. They are harder to analyze
than feedforward neural networks  because the same recurrent unit is repeatedly
applied across the entire time horizon of length L  which is analogous to feedfor-
ward networks of depth L. We show when the number of neurons is sufﬁciently
large  meaning polynomial in the training data size and in L  then SGD is capa-
ble of minimizing the regression loss in the linear convergence rate. This gives
theoretical evidence of how RNNs can memorize data.
More importantly  in this paper we build general toolkits to analyze multi-layer
networks with ReLU activations. For instance  we prove why ReLU activations
can prevent exponential gradient explosion or vanishing  and build a perturbation
theory to analyze ﬁrst-order approximation of multi-layer networks.

1

Introduction

Neural networks have been one of the most powerful tools in machine learning over the past a few
decades. The multi-layer structure of neural network gives it supreme power in expressibility and
learning performance. However  it raises complexity concerns: the training objective is generally
non-convex and non-smooth. In practice  local-search algorithms such as stochastic gradient descent
(SGD) are capable of ﬁnding global optima  at least on the training data [19  59]. How SGD avoids
local minima for such objectives remains an open theoretical question since Goodfellow et al. [19].
In recent years  there have been a number of theoretical results aiming at a better understanding of
this phenomenon. Many of them focus on two-layer (thus one-hidden-layer) neural networks and
assume that the inputs are random Gaussian or sufﬁciently close to Gaussian [7  15  18  32  37  50 
55  60  61]. Some study deep neural networks but assuming the activation function is linear [5  6  22].
Some study the convex task of training essentially only the last layer of the network [13].
More recently  Safran and Shamir [43] provided evidence that  even when inputs are standard Gaus-
sians  two-layer neural networks can indeed have spurious local minima  and suggested that over-
parameterization (i.e.  increasing the number of neurons) may be the key in avoiding spurious local
minima. Li and Liang [31] showed that  for two-layer networks with the cross-entropy loss  in
the over-parametrization regime  gradient descent (GD) is capable of ﬁnding nearly-global optimal
solutions on the training data. This result was later extended to the (cid:96)2 loss by Du et al. [16].
In this paper  we show GD and SGD are capable of training multi-layer neural networks (with ReLU
activation) to global minima on any non-degenerate training data set. Furthermore  the running time

∗Full version and future updates can be found on https://arxiv.org/abs/1810.12065.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

is polynomial in the number of layers and the number of data points. Since there are many different
types of multi-layer networks (convolutional  feedforward  recurrent  etc.)  in this present paper  we
focus on recurrent neural networks (RNN) as our choice of multi-layer networks  and feedforward
networks are only its “special case” (see our follow-up work [3]).
Recurrent Neural Networks. Among different architectures of neural networks  one of the least
theoretically-understood structure is the recurrent one [17]. A recurrent neural network recurrently
applies the same network unit to a sequence of input tokens  such as a sequence of words in a
language sentence. RNN is particularly useful when there are long-term  non-linear interactions
between input tokens in the same sequence. These networks are widely used in practice for natural
language processing  language generation  machine translation  speech recognition  video and music
processing  and many other tasks [11  12  27  35  36  44  52  54]. On the theory side  while there
are some attempts to show that an RNN is more expressive than a feedforward neural network [28] 
when and how an RNN can be efﬁciently learned has nearly-zero theoretical explanation.
In practice  RNN is usually trained by simple local-search algorithms such as SGD. However  unlike
shallow networks  the training process of RNN often runs into the trouble of vanishing or exploding
gradient [53]. That is  the value of the gradient becomes exponentially small or large in the time
horizon  even when the training objective is still constant. In practice  one of the popular ways to
resolve this is by the long short term memory (LSTM) structure [24]. However  one can also use
rectiﬁed linear units (ReLUs) as activation functions to avoid vanishing or exploding gradient [45].
In fact  one of the earliest adoptions of ReLUs was on applications of RNNs for this purpose twenty
years ago [21  46]. For a detailed survey on RNN  we refer the readers to Salehinejad et al. [45].

1.1 Our Question

In this paper  we study the following general question

• Can ReLU provably stabilize the training process and avoid vanishing/exploding gradient?
• Can RNN be trained close to zero training error efﬁciently under mild assumptions?

(When there is no activation function  RNN is known as linear dynamical system and Hardt et al.
[23] proved the convergence for such linear dynamical systems.)
Motivations. One may also want to study whether RNN can be trained close to zero test error.
However  unlike feedforward networks  the training error  or the ability to memorize examples  may
actually be desirable for RNN. After all  many tasks involving RNN are related to memories  and
certain RNN units are even referred to memory cells. Since RNN applies the same network unit to
all input tokens in a sequence  the following question can possibly of its own interest:

• How does RNN learn mappings (say from token 3 to token 7) without destroying others?

Another motivation is the following. An RNN can be viewed as a space constraint  differentiable
Turing machine  except that the input is only allowed to be read in a ﬁxed order. It was shown
in Siegelmann and Sontag [49] that all Turing machines can be simulated by recurrent networks
built of neurons with non-linear activations. In practice  RNN is also used as a tool to build neural
Turing machines [20]  equipped with a grand goal of automatically learning an algorithm based on
the observation of the inputs and outputs. To this extent  we believe the task of understanding the
trainability as a ﬁrst step towards understanding RNN can be meaningful on its own.
Our Result. To present the simplest result  we focus on the classical Elman network with ReLU
activation:
where W ∈ Rm×m  A ∈ Rm×dx
where B ∈ Rd×m
We denote by φ the ReLU activation function: φ(x) = max(x  0). We note that (fully-connected)
feedforward networks are only “special cases” to this by replacing W with W(cid:96) for each layer.2
We consider a regression task where each sequence of inputs consists of vectors x1  . . .   xL ∈ Rdx
L ∈ Rd. We assume there are n
and we perform least-square regression with respect to y∗
training sequences  each of length L. We assume the training sequences are δ-separable (say vectors

h(cid:96) = φ(W · h(cid:96)−1 + Ax(cid:96)) ∈ Rm
y(cid:96) = B · h(cid:96) ∈ Rd

1  . . .   y∗

2Most of the technical lemmas of this paper remain to hold (and become much simpler) once W is replaced

with W(cid:96). This is carefully treated by [3].

2

x1 are different by relative distance δ > 0 for every pairs of training sequences). Our main theorem
can be stated as follows
Theorem. If the number of neurons m ≥ poly(n  d  L  δ−1  log ε−1) is polynomially large  we can
ﬁnd weight matrices W  A  B where the RNN gives ε training error

• if gradient descent (GD) is applied for T = Ω(cid:0) poly(n d L)
Ω(cid:0) poly(n d L)

(cid:1) iterations  starting from random Gaussian initializations.3

(cid:1) iterations  starting from ran-

(mini-batch or regular) stochastic gradient descent

(SGD)

is applied for T =

dom Gaussian initializations; or

δ2

log 1
ε

• if

δ2

log 1
ε

Our Contribution. We summarize our contributions as follows.
• We believe this is the ﬁrst proof of convergence of GD/SGD for training the hidden layers of
recurrent neural networks (or even for any multi-layer networks of more than two layers) when
activation functions are present.4

• Our results provide arguably the ﬁrst theoretical evidence towards the empirical ﬁnding of Good-
fellow et al. [19] on multi-layer networks  regarding the ability of SGD to avoid (spurious) local
minima. Our theorem does not exclude the existence of bad local minima

• We build new technical toolkits to analyze multi-layer networks with ReLU activation  which
have now found many applications [1–3  9]. For instance  combining this paper with new tech-
niques  one can derive guarantees on testing error for RNN in the PAC-learning language [1].

Extension: DNN. A feedforward neural network of depth L is similar to Elman RNN with the
main difference being that the weights across layers are separately trained. As one shall see  this
only makes our proofs simpler because we have more independence in randomness. Our theorems
also apply to feedforward neural networks  and we have written a separate follow-up paper [3] to
address feedforward (fully-connected  residual  and convolutional) neural networks.
EXTENSION: DEEP RNN. Elman RNN is also referred to as three-layer RNN  and one may also
study the convergence of RNNs with more hidden layers. This is referred to as deep RNN [45]. Our
theorem also applies to deep RNNs (by combining this paper together with [3]).
EXTENSION: LOSS FUNCTIONS. For simplicity  in this paper we have adopted the (cid:96)2 regression
loss. Our results generalize to other Lipschitz smooth (but possibly nonconvex) loss functions  by
combining with the techniques of [3].

1.2 Other Related Works

Another relevant work is Brutzkus et al. [8] where the authors studied over-paramterization in the
case of two-layer neural network under a linear-separable assumption.
Instead of using randomly initialized weights like this paper  there is a line of work proposing algo-
rithms using weights generated from some “tensor initialization” process [4  26  48  55  61].
There is huge literature on using the mean-ﬁeld theory to study neural networks [10  14  25  30  34 
38–40  42  47  56–58]. At a high level  they study the network dynamics at random initialization
when the number of hidden neurons grow to inﬁnity  and use such initialization theory to predict per-
formance after training. However  they do not provide theoretical convergence rate for the training
process (at least when the number of neurons is ﬁnite).

3At a ﬁrst glance  one may question how it is possible for SGD to enjoy a logarithmic time dependency in
ε−1; after all  even when minimizing strongly-convex and Lipschitz-smooth functions  the typical convergence
rate of SGD is T ∝ 1/ε as opposed to T ∝ log(1/ε). We quickly point out there is no contradiction here if
the stochastic pieces of the objective enjoy a common global minimizer.

4Our theorem holds even when A  B are at random initialization and only the hidden weight matrix W
is trained. This is much more difﬁcult to analyze than the convex task of training only the last layer B [13].
Training only the last layer can signiﬁcantly reduce the learning power of (recurrent or not) neural networks in
practice.

3

2

i∈[m] (cid:107)wi(cid:107)p

2 Notations and Preliminaries
We denote by (cid:107)·(cid:107)2 (or sometimes (cid:107)·(cid:107)) the Euclidean norm of vectors  and by (cid:107)·(cid:107)2 the spectral norm
of matrices. We denote by (cid:107)·(cid:107)∞ the inﬁnite norm of vectors  (cid:107)·(cid:107)0 the sparsity of vectors or diagonal
matrices  and (cid:107)·(cid:107)F the Frobenius norm of matrices. Given matrix W   we denote by Wk or wk the k-

th row vector of W . We denote the row (cid:96)p norm for W ∈ Rm×d as (cid:107)W(cid:107)2 p :=(cid:0)(cid:80)

(cid:1)1/p.

(cid:81)i−1
j=1(I−(cid:98)vj(cid:98)v(cid:62)
(cid:107)(cid:81)i−1
j=1(I−(cid:98)vj(cid:98)v(cid:62)

By deﬁnition  (cid:107)W(cid:107)2 2 = (cid:107)W(cid:107)F .
We use N (µ  σ) to denote Gaussian distribution with mean µ and variance σ; or N (µ  Σ) to denote
Gaussian vector with mean µ and covariance Σ. We use 1event to denote the indicator function of
whether event is true. We denote by ek the k-th standard basis vector. We use φ(·) to denote the
ReLU function  namely φ(x) = max{x  0} = 1x≥0 · x. Given univariate function f : R → R  we
also use f to denote the same function over vectors: f (x) = (f (x1)  . . .   f (xm)) if x ∈ Rm.
Given vectors v1  . . .   vn ∈ Rm  we deﬁne U = GS(v1  . . .   vn) as their Gram-Schmidt orthonor-

malization. Namely  U = [(cid:98)v1  . . .  (cid:98)vn] ∈ Rm×n where
Note that in the occasion that(cid:81)i−1
v1(cid:107)v1(cid:107)
j )vi is the zero vector  we let(cid:98)vi be an arbitrary unit
vector that is orthogonal to(cid:98)v1  . . .  (cid:98)vi−1.
We assume n training inputs are given: (xi 1  xi 2  . . .   xi L) ∈(cid:0)Rdx(cid:1)L for each input i ∈ [n]. We
i L) ∈(cid:0)Rd(cid:1)L for each input i ∈ [n]. Without

assume n training labels are given: (y∗
loss of generality  we assume (cid:107)xi (cid:96)(cid:107) ≤ 1 for every i ∈ [n] and (cid:96) ∈ [L]. Also without loss of
for every i ∈ [n].5
generality  we assume (cid:107)xi 1(cid:107) = 1 and its last coordinate [xi 1]dx = 1√
We make the following assumption on the input data (see Footnote 9 for how to relax it):
Assumption 2.1. (cid:107)xi 1 − xj 1(cid:107) ≥ δ for some parameter δ ∈ (0  1] and every pair of i (cid:54)= j ∈ [n].
Given weight matrices W ∈ Rm×m  A ∈ Rm×dx  B ∈ Rd×m  we introduce the following notations
to describe the evaluation of RNN on the input sequences. For each i ∈ [n] and j ∈ [L]:

2.1 Elman Recurrent Neural Network

j=1(I −(cid:98)vj(cid:98)v(cid:62)

i 2  . . .   y∗

(cid:98)v1 =

(cid:98)vi =

j )vi

j )vi(cid:107).

and

for i ≥ 2:

i 1  y∗

2

hi 0 = 0 ∈ Rm
yi (cid:96) = B · hi (cid:96) ∈ Rd

gi (cid:96) = W · hi (cid:96)−1 + Axi (cid:96) ∈ Rm
hi (cid:96) = φ(W · hi (cid:96)−1 + Axi (cid:96)) ∈ Rm

A very important notion that this entire paper relies on is the following:
Deﬁnition 2.2. For each i ∈ [n] and (cid:96) ∈ [L]  let Di (cid:96) ∈ Rm×m be the diagonal matrix where

As a result  we can write hi (cid:96) = Di (cid:96)W hi (cid:96)−1.

(Di (cid:96))k k = 1(W·hi (cid:96)−1+Axi (cid:96))k≥0 = 1(gi (cid:96))k≥0 .

m )  and the entries of Bi j are i.i.d. generated from N (0  1
d ).

We consider the following random initialization distributions for W   A and B.
Deﬁnition 2.3. We say that W  A  B are at random initialization  if the entries of W and A are i.i.d.
generated from N (0  2
Throughout this paper  for notational simplicity  we refer to index (cid:96) as the (cid:96)-th layer of RNN  and
hi (cid:96)  xi (cid:96)  yi (cid:96) respectively as the hidden neurons  input  output on the (cid:96)-th layer. We acknowledge
that in certain literatures  one may regard Elman network as a three-layer RNN.
Assumption 2.4. We assume m ≥ poly(n  d  L  1
Without loss of generality  we assume δ ≤

is not satisﬁed one can decrease δ). Throughout the paper except the detailed appendix  we use (cid:101)O 
(cid:101)Ω and(cid:101)Θ notions to hide polylogarithmic dependency in m. To simplify notations  we denote by

CL2 log3 m for some sufﬁciently large constant C (if this

ε ) for some sufﬁciently large polynomial.

δ   log 1

1

ρ := nLd log m and  := nLdδ−1 log(m/ε) .

5If it only satisﬁes (cid:107)xi 1(cid:107) ≤ 1 one can pad it with an additional coordinate to make (cid:107)xi 1(cid:107) = 1 hold. As

  this is equivalent to adding a bias term N (0  1

m ) for the ﬁrst layer.

for the assumption [xi 1]dx = 1√

2

4

f (W ) :=(cid:80)n
∇kf (W ) =(cid:80)n
∇f (W ) =(cid:80)n

(cid:80)L
(cid:80)L

i=1

a=2

i=1

a=2

(cid:80)a−1
(cid:80)a−1

2.2 Objective and Gradient
For simplicity  we only optimize over the weight matrix W ∈ Rm×m and let A and B be at random
initialization. As a result  our (cid:96)2-regression objective is a function over W :6

i=1 fi(W )

and fi(W ) := 1
2

2 where

lossi (cid:96) := Bhi (cid:96) − y∗

i (cid:96) .

(cid:80)L
(cid:96)=2 (cid:107) lossi (cid:96) (cid:107)2

Using chain rule  one can write down a closed form of the (sub-)gradient:
Fact 2.5. For k ∈ [m]  the gradient with respect to Wk (denoted by ∇k) and the full gradient are

i (cid:96)+1→a · lossi a)k · hi (cid:96) · 1(cid:104)Wk hi (cid:96)(cid:105)+(cid:104)Ak xi (cid:96)+1(cid:105)≥0
(cid:62)
(cid:96)=1 (Back
(cid:96)=1 Di (cid:96)+1

i (cid:96)+1→a · lossi a
(cid:62)

i (cid:96)

(cid:0) Back

(cid:1) · h(cid:62)

where for every i ∈ [n]  (cid:96) ∈ [L]  and a = (cid:96) + 1  (cid:96) + 2  . . .   L:

Backi (cid:96)→(cid:96) := B ∈ Rd×m and Backi (cid:96)→a := BDi aW ··· Di (cid:96)+1W ∈ Rd×m .

3 Our Results

Our main results can be formally stated as follows.

Theorem 1 (GD). Suppose η = (cid:101)Θ(cid:0) δ

m poly(n  d  L)(cid:1) and m ≥ poly(n  d  L  δ−1  log ε−1). Let

W (0)  A  B be at random initialization. With high probability over the randomness of W (0)  A  B 
if we apply gradient descent for T steps W (t+1) = W (t) − η∇f (W (t))  then it satisﬁes

Theorem 2 (SGD). Suppose η = (cid:101)Θ(cid:0) δ

f (W (T )) ≤ ε

for

δ2

1
ε

log

(cid:1).

T =(cid:101)Ω(cid:0) poly(n  d  L)
m poly(n  d  L)(cid:1) and m ≥ poly(n  d  L  δ−1  log ε−1).
T =(cid:101)Ω(cid:0) poly(n  d  L)

(cid:1).

log

δ2

1
ε

Let W (0)  A  B be at random initialization.
If we apply stochastic gradient descent for T steps
W (t+1) = W (t)− η∇fi(W (t)) for a random index i ∈ [n] per step  then with high probability (over
W (0)  A  B and the randomness of SGD)  it satisﬁes

f (W (T )) ≤ ε

for

In both cases  we essentially have linear convergence rates. Notably  our results show that the
dependency of the number of layers L  is polynomial. Thus  even when RNN is applied to sequences
of long input data  it does not suffer from exponential gradient explosion or vanishing (e.g.  2Ω(L)
or 2−Ω(L)) through the entire training process.
Main Technical Theorems. Our main Theorem 1 and Theorem 2 are in fact natural consequences
of the following two technical theorems. They both talk about the ﬁrst-order behavior of RNNs
when the weight matrix W is sufﬁciently close to some random initialization.
The ﬁrst theorem is similar to the classical Polyak-Łojasiewicz condition [33  41]  and says that
(cid:107)∇f (W )(cid:107)2

Theorem 3. With high probability over random initialization(cid:102)W   A  B  it satisﬁes
∀W ∈ Rm×m with (cid:107)W −(cid:102)W(cid:107)2 ≤ poly()√

F is at least as large as the objective value.

(cid:107)∇f (W )(cid:107)2
F  (cid:107)∇fi(W )(cid:107)2

F ≥
× m × f (W )  
F ≤ poly(ρ) × m × f (W ) .
(Only the ﬁrst statement is the Polyak-Łojasiewicz condition; the second is a simple-to-proof gradi-
Theorem 4. With high probability over random initialization(cid:102)W   A  B  it satisﬁes for every ˘W ∈
ent upper bound.) The second theorem shows a special “semi-smoothness” property of the objective.
Rm×m with (cid:107) ˘W −(cid:102)W(cid:107) ≤ poly()√
f ( ˘W + W (cid:48)) ≤ f ( ˘W ) + (cid:104)∇f ( ˘W )  W (cid:48)(cid:105) + poly()m1/3 ·(cid:112)f (W ) · (cid:107)W (cid:48)(cid:107)2 + poly(ρ)m(cid:107)W (cid:48)(cid:107)2

m   and for every W (cid:48) ∈ Rm×m with (cid:107)W (cid:48)(cid:107) ≤ τ0√
m  

2 .
6The index (cid:96) starts from 2  because Bhi 1 = Bφ(Axi 1) remains constant if we are not optimizing over A

(cid:107)∇f (W )(cid:107)2

poly(ρ)

m

δ

:

and B.

5

At a high level  the convergence of GD and SGD are careful applications of the two technical the-
orems above: indeed  Theorem 3 shows that as long as the objective value is high  the gradient is
large; and Theorem 4 shows that if one moves in the (negative) gradient direction  then the objective
value can be sufﬁciently decreased. These two technical theorems together ensure that GD/SGD
does not hit any saddle point or (bad) local minima along its training trajectory. This was practically
observed by Goodfellow et al. [19] and a theoretical justiﬁcation was open since then.
An Open Question. We did not try to tighten the polynomial dependencies of (n  d  L) in the
proofs. When m is sufﬁciently large  we make use of the randomness at initialization to argue that 
for all the points within a certain radius from initialization  for instance Theorem 3 holds. In practice 
however  the SGD can create additional randomness as time goes; also  in practice  it sufﬁces for
those points on the SGD trajectory to satisfy Theorem 3. Unfortunately  such randomness can — in
principle — be correlated with the SGD trajectory  so we do not know how to use that in the proofs.
Analyzing such correlated randomness is certainly beyond the scope of this paper  but can possibly
explain why in practice  the size of m needed is not that large.

3.1 Conclusion

Overall  we provide the ﬁrst proof of convergence of GD/SGD for non-linear neural networks that
have more two layers. We show with overparameterization GD/SGD can avoid hitting any (bad)
local minima along its training trajectory. This was practically observed by Goodfellow et al. [19]
and a theoretical justiﬁcation was open since then. We present our result using recurrent neural
networks (as opposed to the simpler feedforward networks [3]) in this very ﬁrst paper  because
memorization in RNN could be of independent interest. Also  our result proves that RNN can
learn mappings from different input tokens to different output tokens simultaneously using the same
recurrent unit.
Last but not least  we build new tools to analyze multi-layer networks with ReLU activations that
could facilitate many new research on deep learning. For instance  our techniques in Section 4
provide a general theory for why ReLU activations avoid exponential exploding (see e.g.
(4.1) 
(4.4)) or exponential vanishing (see e.g. (4.1)  (4.3)); and our techniques in Section 5 give a general
theory for the stability of multi-layer networks against adversarial weight perturbations  which is at
the heart of showing the semi-smoothness Theorem 4  and used by all the follow-up works [1–3  9].

PROOF SKETCH

The main difﬁculty of this paper is to prove Theorem 3 and 4  and we shall sketch the proof ideas in
Section 4 through 8. In this main body  we only include Section 4 and 5 because they already given
some insights into how the proof proceeds. We shall put our emphasize on
• how to avoid exponential blow up in L  and
• how to deal with the issue of randomness dependence across layers.
We genuinely hope that this high-level sketch can (1) give readers a clear overview of the proof
without the necessity of going to the appendix  and (2) appreciate our proof and understand why it
is necessarily long.7

4 Basic Properties at Random Initialization
In this section we derive basic properties of the RNN when the weight matrices W  A  B are all at
random initialization. The corresponding precise statements and proofs are in Appendix B.
The ﬁrst one says that the forward propagation neither explodes or vanishes  that is 

≤ (cid:107)hi (cid:96)(cid:107)2 (cid:107)gi (cid:96)(cid:107)2 ≤ O(L) .

(4.1)
7For instance  proving gradient norm lower bound in Theorem 3 for a single neuron k ∈ [m] is easy  but
how to apply concentration across neurons? Crucially  due to the recurrent structure these quantities are never
independent  so we have to build necessary probabilistic tools to tackle this. If one is willing to ignore such
subtleties  then our sketched proof is sufﬁciently short and gives a good overview.

1
2

6

√

i.i.d. from N(cid:0)0  2

(cid:1)  the norm (cid:107)W z(cid:107)2 is around

m

Intuitively  (4.1) very reasonable. Since the weight matrix W is randomly initialized with entries
2 for any ﬁxed vector z. Equipped with ReLU
activation  it “shuts down” roughly half of the coordinates of W z and reduces the norm (cid:107)φ(W z)(cid:107) to
one. Since in each layer (cid:96)  there is an additional unit-norm signal xi (cid:96) coming in  we should expect
the ﬁnal norm of hidden neurons to be at most O(L).
Unfortunately  the above argument cannot be directly applied since the weight matrix W is reused
for L times so there is no fresh new randomness across layers. Let us explain how we deal with this
issue carefully  because it is at the heart of all of our proofs in this paper. Recall  each time W is
applied to some vector hi (cid:96)  it only uses “one column of randomness” of W . Mathematically  letting
U(cid:96) ∈ Rm×n(cid:96) denote the column orthonormal matrix using Gram-Schmidt

we have W hi (cid:96) = W U(cid:96)−1U(cid:62)
• The term W (I − U(cid:96)−1U(cid:62)
• The term W U(cid:96)−1U(cid:62)

U(cid:96) := GS (h1 1  . . .   hn 1  h1 2  . . .   hn 2  . . .   h1 (cid:96)  . . .   hn (cid:96))  

(cid:96)−1hi (cid:96) + W (I − U(cid:96)−1U(cid:62)
(cid:96)−1)hi (cid:96) has new randomness independent of the previous layers.8

(cid:96)−1)hi (cid:96).

(cid:96)−1hi (cid:96) relies on the randomness of W in the directions of hi a for a < (cid:96)
of the previous layers. We cannot rely on the randomness of this term  because when applying
inductive argument till layer (cid:96)  the randomness of W U(cid:96)−1 is already used.
Fortunately  W U(cid:96)−1 ∈ Rm×n((cid:96)−1) is a rectangular matrix with m (cid:29) n((cid:96) − 1) (thanks to
√
overparameterization!) so one can bound its spectral norm by roughly
2. This ensures that
no matter how hi (cid:96) behaves (even arbitrarily correlated with W U(cid:96)−1)  the norm of the ﬁrst term
cannot be too large. It is crucial here that W U(cid:96)−1 is a rectangular matrix  because for a square
random matrix such as W   its spectral norm is 2 and using that  the forward propagation bound
will exponentially blow up.

(cid:96)−1)hi (cid:96)(cid:107)2 ≥(cid:101)Ω(

1
L2 ) .

This summarizes the main idea for proving (cid:107)hi (cid:96)(cid:107) ≤ O(L) in (4.1); the lower bound 1
Our next property says in each layer  the amount of “fresh new randomness” is non-negligible:

2 is similar.

(cid:107)(I − U(cid:96)−1U(cid:62)

(I − U(cid:96)U(cid:62)

(4.2)
This relies on a more involved inductive argument than (4.1). At high level  one needs to show that
in each layer  the amount of “fresh new randomness” reduces only by a factor at most 1 − 1
10L.
Using (4.1) and (4.2)  we obtain the following property about the data separability:

Here  we say two vectors x and y are δ-separable if (cid:13)(cid:13)(I − yy(cid:62)/(cid:107)y(cid:107)2

(cid:96) )hi (cid:96)+1 and (I − U(cid:96)U(cid:62)

(cid:96) )hj (cid:96)+1 are (δ/2)-separable  ∀i  j ∈ [n] with i (cid:54)= j

2)x(cid:13)(cid:13) ≥ δ and vice versa.

(4.3)

Property (4.3) shows that the separability information (say on input token 1) does not diminish by
more than a polynomial factor even if the information is propagated for L layers.
We prove (4.3) by induction. In the ﬁrst layer (cid:96) = 1 we have hi 1 and hj 1 are δ-separable which is a
consequence of Assumption 2.1. If having fresh new randomness  given two δ separable vectors x  y 
one can show that φ(W x) and φ(W y) are also δ(1 − o( 1
L ))-separable. Again  in RNN  we do not
have fresh new randomness  so we rely on (4.2) to give us reasonably large fresh new randomness.
Applying a careful induction helps us to derive that (4.3) holds for all layers.9
Intermediate Layers and Backward Propagation. Training neural network is not only about
forward propagation. We also have to bound intermediate layers and backward propagation.
The ﬁrst two results we derive are the following. For every (cid:96)1 ≥ (cid:96)2 and diagonal matrices D(cid:48) of
sparsity s ∈ [ρ2  m0.49]:

(cid:107)D(cid:48)W Di (cid:96)1 ··· Di (cid:96)2W D(cid:48)(cid:107)2 ≤ (cid:101)O(
(cid:107)W Di (cid:96)1 ··· Di (cid:96)2W(cid:107)2 ≤ O(L3)
√

√

m)
(cid:96)−1)hi (cid:96)  we have W (I − U(cid:96)−1U(cid:62)
m I) and is independent of all {hi a | i ∈ [n]  a < (cid:96)}.

8More precisely  letting v = (I − U(cid:96)−1U(cid:62)
W v(cid:107)v(cid:107) is a random Gaussian vector in N (0  2
9This is the only place that we rely on Assumption 2.1. This assumption is somewhat necessary in the
following sense. If xi (cid:96) = xj (cid:96) for some pair i (cid:54)= j for all the ﬁrst ten layers (cid:96) = 1  2  . . .   10  and if y∗
i (cid:96) (cid:54)= y∗
i (cid:96)
for even just one of these layers  then there is no hope in having the training objective decrease to zero. Of
course  one can make more relaxed assumption on the input data  involving both xi (cid:96) and y∗
i (cid:96). While this is
possible  it complicates the statements so we do not present such results in this paper.

s/

(cid:96)−1)hi (cid:96) =(cid:0)W v(cid:107)v(cid:107)

(4.4)
(4.5)

(cid:1)(cid:107)v(cid:107). Here 

7

Intuitively  one cannot use spectral bound argument to derive (4.4) or (4.5): the spectral norm of W
is 2  and even if ReLU activations cancel half of its mass  the spectral norm (cid:107)DW(cid:107)2 remains to be
√
2. When stacked together  this grows exponential in L.

Instead  we use an analogous argument to (4.1) to show that  for each ﬁxed vector z  the norm of
(cid:107)W Di (cid:96)1 ··· Di (cid:96)2W z(cid:107)2 is at most O(1) with extremely high probability 1 − e−Ω(m/L2). By stan-
dard ε-net argument  (cid:107)W Di (cid:96)1 ··· Di (cid:96)2W z(cid:107)2 is at most O(1) for all m
L3 -sparse vectors z. Finally 
L3 . Finally  we apply
for a possible dense vector z  we can divide it into L3 chunks each of sparsity m
the upper bound for L3 times. This proves (4.4). One can use similar argument to prove (4.5).
Remark 4.1. We did not try to tighten the polynomial factor here in L. We conjecture that proving
an O(1) bound may be possible  but that question itself may be a sufﬁciently interesting random
matrix theory problem on its own.
The next result is for back propagation. For every (cid:96)1 ≥ (cid:96)2 and diagonal matrices D(cid:48) of sparsity
s ∈ [ρ2  m0.49]:

(4.6)
Its proof is in the same spirit as (4.5)  with the only difference being the spectral norm of B is around

s)

(cid:112)m/d as opposed to O(1).

(cid:107)BDi (cid:96)1 ··· Di (cid:96)2W D(cid:48)(cid:107)2 ≤ (cid:101)O(

√

5 Stability After Adversarial Perturbation

In this section we study the behavior of RNN after adversarial perturbation. The corresponding
precise statements and proofs are in Appendix C.

Letting(cid:102)W   A  B be at random initialization  we consider some matrix W =(cid:102)W + W (cid:48) for (cid:107)W (cid:48)(cid:107)2 ≤
m . Here  W (cid:48) may depend on the randomness of(cid:102)W   A and B  so we say it can be adversarially

poly()√
chosen. The results of this section will later be applied essentially twice:
• Once for those updates generated by GD or SGD  where W (cid:48) is how much the algorithm has
• The other time (see Section 7.3) for a technique that we call “randomness decomposition” where

we decompose the true random initialization W into W =(cid:102)W +W (cid:48)  where(cid:102)W is a “fake” random

moved away from the random initialization.

initialization but identically distributed as W . Such technique comes from smooth analysis [51].

To illustrate our high-level idea  from this section on (so in Section 5  7 and 8)

we ignore the polynomial dependency in  and hide it in the big-O notion.

We denote by (cid:101)Di (cid:96) (cid:101)gi (cid:96) (cid:101)hi (cid:96) respectively the values of Di (cid:96)  gi (cid:96) and hi (cid:96) determined by(cid:102)W and A at
random initialization; and by Di (cid:96) = (cid:101)Di (cid:96) +D(cid:48)
those determined by W =(cid:102)W + W (cid:48) after the adversarial perturbation.
i (cid:96)(cid:107)2 (cid:107)h(cid:48)

Forward Stability. Our ﬁrst  and most technical result is the following:
(cid:107)g(cid:48)
Intuitively  one may hope to prove (5.1) by induction  because we have (ignoring subscripts in i)

i (cid:96) and hi (cid:96) =(cid:101)hi (cid:96) +h(cid:48)

i (cid:96)  gi (cid:96) =(cid:101)gi (cid:96) +g(cid:48)

i (cid:96)(cid:107)2 ≤ O(m−1/2)  

i (cid:96) respectively

(cid:107)D(cid:48)

(5.1)

and

(cid:107)D(cid:48)

i (cid:96)(cid:107)0 ≤ O(m2/3)
+(cid:102)W D(cid:48)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:96)(cid:48) = W (cid:48)D(cid:96)(cid:48)−1g(cid:96)(cid:48)−1
g(cid:48)

(cid:123)(cid:122)

(cid:124)

(cid:96)(cid:48)−1g(cid:96)(cid:48)−1

(cid:125)

i (cid:96)gi (cid:96)(cid:107)2 ≤ O(m−1/2) .
+(cid:102)W(cid:101)D(cid:96)(cid:48)−1g(cid:48)
(cid:124)
(cid:123)(cid:122)

(cid:96)(cid:48)−1

(cid:125)

.

x

The main issue here is that  the spectral norm of(cid:102)W(cid:101)D(cid:96)(cid:48)−1 in z is greater than 1  so we cannot apply

naive induction due to exponential blow up in L. Neither can we apply techniques from Section 4 
because the changes such as g(cid:96)(cid:48)−1 can be adversarial.
In our actual proof of (5.1)  instead of applying induction on z  we recursively expand z by the
above formula. This results in a total of L terms of x type and L terms of y type. The main
difﬁculty is to bound a term of y type  that is:

y

z

(cid:13)(cid:13)(cid:102)W(cid:101)D(cid:96)1 ···(cid:101)D(cid:96)2+1(cid:102)W D(cid:48)

g(cid:96)2

(cid:96)2

(cid:13)(cid:13)2

Our argument consists of two conceptual steps.

8

(1) Suppose g(cid:96)2 =(cid:101)g(cid:96)2 + g(cid:48)
(cid:96)2 2(cid:107)∞ ≤ m−1 
(cid:102)W(cid:101)D(cid:96)1 ···(cid:101)D(cid:96)2+1(cid:102)W x can be written as y = y1 + y2 with (cid:107)y1(cid:107)2 ≤ O(m−2/3) and (cid:107)y2(cid:107)∞ ≤

=(cid:101)g(cid:96)2 + g(cid:48)
g(cid:96)2(cid:107)2 ≤ O(m−1/2) and (cid:107)D(cid:48)

(2) Suppose x ∈ Rm with (cid:107)x(cid:107)2 ≤ m−1/2 and (cid:107)x(cid:107)0 ≤ m2/3 

(cid:96)2 1(cid:107)2 ≤ m−1/2 and (cid:107)g(cid:48)
g(cid:96)2(cid:107)0 ≤ O(m2/3).

(cid:96)2 1 + g(cid:48)

(cid:96)2 2 where (cid:107)g(cid:48)

then we show that y =

then we argue that (cid:107)D(cid:48)

(cid:96)2

(cid:96)2

(cid:96)2

O(m−1).

The two steps above enable us to perform induction without exponential blow up. Indeed  they
together enable us to go through the following logic chain:

(cid:107) · (cid:107)2 ≤ m−1/2 and (cid:107) · (cid:107)∞ ≤ m−1
=⇒
(cid:107) · (cid:107)2 ≤ m−2/3 and (cid:107) · (cid:107)∞ ≤ m−1 ⇐=

(1)

(2)

(cid:107) · (cid:107)2 ≤ m−1/2 and (cid:107) · (cid:107)0 ≤ m2/3

Since there is a gap between m−1/2 and m−2/3  we can make sure that all blow-up factors are
absorbed into this gap  using the property that m is polynomially large. This enables us to perform
induction to prove (5.1) without exponential blow-up.
Intermediate Layers and Backward Stability. Using (5.1)  and especially using the sparsity
(cid:107)D(cid:48)(cid:107)0 ≤ m2/3 from (5.1)  one can apply the results in Section 4 to derive the following stability
bounds for intermediate layers and backward propagation:

(cid:13)(cid:13)Di (cid:96)1W ··· Di (cid:96)2W − (cid:101)Di (cid:96)1(cid:102)W ···(cid:101)Di (cid:96)2(cid:102)W(cid:13)(cid:13)2 ≤ O(L7)
(cid:13)(cid:13)BDi (cid:96)1 W ··· Di (cid:96)2 W − B(cid:101)Di (cid:96)1(cid:102)W ···(cid:101)Di (cid:96)2(cid:102)W(cid:13)(cid:13)2 ≤ O(cid:0)m1/3(cid:1) .
i (cid:96))k| ≤ O(cid:0)m−2/3(cid:1)
(cid:13)(cid:13)2 ≤ O(cid:0)m−1/6(cid:1)

|(((cid:102)W + W (cid:48))h(cid:48)
(cid:13)(cid:13)BDi (cid:96)1W ··· Di (cid:96)2 W ek − B(cid:101)Di (cid:96)1(cid:102)W ···(cid:101)Di (cid:96)2(cid:102)W ek

Special Rank-1 Perturbation. For technical reasons  we also need two bounds in the special case
of W (cid:48) = yz(cid:62) for some unit vector z and sparse y with (cid:107)y(cid:107)0 ≤ poly(). We prove that  for this type
of rank-one adversarial perturbation  it satisﬁes for every k ∈ [m]:

(5.2)
(5.3)

(5.4)
(5.5)

6 Conclusion and What’s After Page 8

We conclude the paper here because Section 4 and 5 have already given some insights into how
the proof proceeds and how to avoid exponential blow up in L.
In the supplementary material 
within another 3 pages we also sketch the proof ideas for Theorem 3 and 4 (see Section 7 and 8).
We genuinely hope that this high-level sketch can (1) give readers a clear overview of the proof
without the necessity of going to the appendix  and (2) appreciate our proof and understand why it
is necessarily long.10 /
Overall  we provide the ﬁrst proof of convergence of GD/SGD for non-linear neural networks that
have more two layers. We show with overparameterization GD/SGD can avoid hitting any (bad)
local minima along its training trajectory. This was practically observed by Goodfellow et al. [19]
and a theoretical justiﬁcation was open since then. We present our result using recurrent neural
networks (as opposed to the simpler feedforward networks [3]) in this very ﬁrst paper  because
memorization in RNN could be of its own independent interest. Also  our result proves that RNN
can indeed learn mappings from different input tokens to different input tokens simultaneously.
Last but not least  we build new tools to analyze multi-layer networks with ReLU activations that
could facilitate many new research on deep learning. For instance  our techniques in Section 4
provide a general theory for why ReLU activations avoid exponential exploding (see e.g.
(4.1) 
(4.4)) or exponential vanishing (see e.g. (4.1)  (4.3)); and our techniques in Section 5 give a general
theory for the stability of multi-layer networks against adversarial weight perturbations  which is at
the heart of showing the semi-smoothness Theorem 4  and used by all the follow-up works [1–3  9].

10For instance  proving gradient norm lower bound in Theorem 3 for a single neuron k ∈ [m] is easy  but
how to apply concentration across neurons? Crucially  due to the recurrent structure these quantities are never
independent  so we have to build necessary probabilistic tools to tackle this. If one is willing to ignore such
subtleties  then our sketched proof is sufﬁciently short and gives a good overview.

9

References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable
Generalization? In NeurIPS  2019. Full version available at http://arxiv.org/abs/1902.
01028.

[2] Zeyuan Allen-Zhu  Yuanzhi Li  and Yingyu Liang. Learning and Generalization in Overpa-
rameterized Neural Networks  Going Beyond Two Layers. In NeurIPS  2019. Full version
available at http://arxiv.org/abs/1811.04918.

[3] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML  2019. Full version available at http://arxiv.org/abs/
1811.03962.

[4] Sanjeev Arora  Aditya Bhaskara  Rong Ge  and Tengyu Ma. Provable bounds for learning
some deep representations. In International Conference on Machine Learning (ICML)  pages
584–592  2014.

[5] Sanjeev Arora  Nadav Cohen  Noah Golowich  and Wei Hu. A convergence analysis of gradi-

ent descent for deep linear neural networks. arXiv preprint arXiv:1810.02281  2018.

[6] Peter Bartlett  Dave Helmbold  and Phil Long. Gradient descent with identity initialization efﬁ-
ciently learns positive deﬁnite linear transformations. In International Conference on Machine
Learning (ICML)  pages 520–529  2018.

[7] Alon Brutzkus and Amir Globerson.

vnet with gaussian inputs.
http://arxiv.org/abs/1702.07966  2017.

Globally optimal gradient descent for a con-
In International Conference on Machine Learning (ICML).

[8] Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In ICLR  2018.
URL https://arxiv.org/abs/1710.10174.

[9] Yuan Cao and Quanquan Gu. A Generalization Theory of Gradient Descent for Learning

Over-parameterized Deep ReLU Networks. arXiv preprint arXiv:1902.01384  2019.

[10] Minmin Chen  Jeffrey Pennington  and Samuel S. Schoenholz. Dynamical isometry and a
mean ﬁeld theory of RNNs: Gating enables signal propagation in recurrent neural networks.
arXiv:1806.05394  2018. URL http://arxiv.org/abs/1806.05394.

[11] Kyunghyun Cho  Bart Van Merri¨enboer  Dzmitry Bahdanau  and Yoshua Bengio. On
the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259  2014.

[12] Junyoung Chung  Caglar Gulcehre  KyungHyun Cho  and Yoshua Bengio.

evaluation of gated recurrent neural networks on sequence modeling.
arXiv:1412.3555  2014.

Empirical
arXiv preprint

[13] Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems (NeurIPS)  pages 2422–2430  2017.

[14] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural net-
works: The power of initialization and a dual view on expressivity. In Advances in Neural
Information Processing Systems (NeurIPS)  pages 2253–2261  2016.

[15] Simon S. Du  Jason D. Lee  Yuandong Tian  Barnab´as P´oczos  and Aarti Singh. Gradient
In ICML 

descent learns one-hidden-layer CNN: don’t be afraid of spurious local minima.
2018.

[16] Simon S. Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient Descent Provably Opti-

mizes Over-parameterized Neural Networks. ArXiv e-prints  2018.

[17] Jeffrey L Elman. Finding structure in time. Cognitive science  14(2):179–211  1990.

10

[18] Rong Ge  Jason D. Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. In ICLR  2017. URL http://arxiv.org/abs/1711.00501.

[19] Ian J Goodfellow  Oriol Vinyals  and Andrew M Saxe. Qualitatively characterizing neural

network optimization problems. In ICLR  2015.

[20] Alex Graves  Greg Wayne  and Ivo Danihelka. Neural turing machines. CoRR  abs/1410.5401 

2014. URL http://arxiv.org/abs/1410.5401.

[21] Richard LT Hahnloser. On the piecewise analysis of networks of linear threshold neurons.

Neural Networks  11(4):691–697  1998.

[22] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In ICLR  2017. URL http:

//arxiv.org/abs/1611.04231.

[23] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynamical

systems. Journal of Machine Learning Research (JMLR)  19(29):1–44  2018.

[24] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation  9

(8):1735–1780  1997.

[25] Arthur Jacot  Franck Gabriel  and Cl´ement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in neural information processing systems 
pages 8571–8580  2018.

[26] Majid Janzamin  Hanie Sedghi  and Anima Anandkumar. Beating the perils of non-
convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint
arXiv:1506.08473  2015.

[27] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing  pages 1700–
1709  2013.

[28] Valentin Khrulkov  Alexander Novikov  and Ivan Oseledets. Expressive power of recurrent
In International Conference on Learning Representations  2018. URL

neural networks.
https://openreview.net/forum?id=S1WRibb0Z.

[29] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. Annals of Statistics  pages 1302–1338  2000.

[30] Jaehoon Lee  Yasaman Bahri  Roman Novak  Samuel S. Schoenholz  Jeffrey Pennington  and
Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. arXiv:1711.00165  2017.
URL http://arxiv.org/abs/1711.00165.

[31] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems
(NeurIPS)  2018.

[32] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with
In Advances in Neural Information Processing Systems (NeurIPS).

ReLU activation.
http://arxiv.org/abs/1705.09886  2017.

[33] S Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS  Les ´equations

aux d´eriv´ees partielles  117:87–89  1963.

[34] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences  115(33):
E7665–E7671  2018.

[35] Tom´aˇs Mikolov  Martin Karaﬁ´at  Luk´aˇs Burget  Jan ˇCernock`y  and Sanjeev Khudanpur. Recur-
rent neural network based language model. In Eleventh Annual Conference of the International
Speech Communication Association  2010.

11

[36] Tom´aˇs Mikolov  Stefan Kombrink  Luk´aˇs Burget  Jan ˇCernock`y  and Sanjeev Khudanpur. Ex-
tensions of recurrent neural network language model. In Acoustics  Speech and Signal Pro-
cessing (ICASSP)  2011 IEEE International Conference on  pages 5528–5531. IEEE  2011.

[37] Rina Panigrahy  Ali Rahimi  Sushant Sachdeva  and Qiuyi Zhang. Convergence results for

neural networks via electrodynamics. In ITCS  2018.

[38] Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via ran-
dom matrix theory. In International Conference on Machine Learning (ICML)  International
Convention Centre  Sydney  Australia  2017.

[39] Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In

Advances in Neural Information Processing Systems (NeurIPS)  2017.

[40] Jeffrey Pennington  Samuel S. Schoenholz  and Surya Ganguli. Resurrecting the sigmoid in
deep learning through dynamical isometry: theory and practice. arXiv:1711.04735  November
2017. URL http://arxiv.org/abs/1711.04735.

[41] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-

tel’noi Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[42] Ben Poole  Subhaneil Lahiri  Maithreyi Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
In Advances In

Exponential expressivity in deep neural networks through transient chaos.
Neural Information Processing Systems  pages 3360–3368  2016. 00047.

[43] Itay Safran and Ohad Shamir.

ReLU neural networks.
http://arxiv.org/abs/1712.08968  2018.

Spurious local minima are common in two-layer
In International Conference on Machine Learning (ICML).

[44] Has¸im Sak  Andrew Senior  and Franc¸oise Beaufays. Long short-term memory recurrent neural
network architectures for large scale acoustic modeling. In Fifteenth annual conference of the
international speech communication association  2014.

[45] Hojjat Salehinejad  Julianne Baarbe  Sharan Sankar  Joseph Barfett  Errol Colak  and Shahrokh
Valaee. Recent advances in recurrent neural networks. arXiv preprint arXiv:1801.01078  2017.

[46] Emilio Salinas and Laurence F. Abbott. A model of multiplicative neural responses in parietal

cortex. Proceedings of the National Academy of Sciences  93(21):11956–11961  1996.

[47] Samuel S. Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep infor-
mation propagation. In ICLR  2017. URL https://openreview.net/pdf?id=H1W1UN9gg.

[48] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with

sparse connectivity. In ICLR. arXiv preprint arXiv:1412.2693  2015.

[49] Hava T Siegelmann and Eduardo D Sontag. Turing computability with neural nets. Applied

Mathematics Letters  4(6):77–80  1991.

[50] Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. CoRR  abs/1705.04591  2017.

URL http://arxiv.org/abs/1705.04591.

[51] Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex

algorithm usually takes polynomial time. Journal of the ACM  51(3):385–463  2004.

[52] Martin Sundermeyer  Ralf Schl¨uter  and Hermann Ney. Lstm neural networks for language
modeling. In Thirteenth annual conference of the international speech communication associ-
ation  2012.

[53] Ilya Sutskever  James Martens  and Geoffrey Hinton. Generating text with recurrent neural
networks. In International Conference on Machine Learning (ICML)  pages 1017–1024  2011.

[54] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems (NeurIPS)  pages 3104–3112 
2014.

12

[55] Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network
and its applications in convergence and critical point analysis. In International Conference on
Machine Learning (ICML). http://arxiv.org/abs/1703.00560  2017.

[56] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel S. Schoenholz  and Jeffrey Pen-
nington. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10 000-layer
In International Conference on Machine Learning
vanilla convolutional neural networks.
(ICML)  2018.

[57] Greg Yang and Sam S. Schoenholz. Deep mean ﬁeld theory: Layerwise variance and width
variation as methods to control gradient explosion. ICLR open review  2018. URL https:
//openreview.net/forum?id=rJGY8GbR-.

[58] Greg Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In

Advances in Neural Information Processing Systems (NeurIPS)  pages 7103–7114  2017.

[59] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR)  2017.

[60] Kai Zhong  Zhao Song  and Inderjit S Dhillon. Learning non-overlapping convolutional neural

networks with multiple kernels. arXiv preprint arXiv:1711.03440  2017.

[61] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery guaran-
tees for one-hidden-layer neural networks. In International Conference on Machine Learning
(ICML). arXiv preprint arXiv:1706.03175  2017.

13

,Zeyuan Allen-Zhu
Yuanzhi Li
Zhao Song