2018,Learning Attractor Dynamics for Generative Memory,A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics  which iteratively cleans up patterns during recall. However  incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients  which are known to make training neural networks difficult.  In this work  we exploit recent advances in variational inference and avoid the vanishing gradient problem by training a generative distributed memory with a variational lower-bound-based Lyapunov function. The model is minimalistic with surprisingly few parameters. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.,Learning Attractor Dynamics

for Generative Memory

Yan Wu  Greg Wayne  Karol Gregor  Timothy Lillicrap

DeepMind

{yanwu gregwayne karolg countzero}@google.com

Abstract

A central challenge faced by memory systems is the robust retrieval of a stored
pattern in the presence of interference due to other stored patterns and noise. A the-
oretically well-founded solution to robust retrieval is given by attractor dynamics 
which iteratively clean up patterns during recall. However  incorporating attractor
dynamics into modern deep learning systems poses difﬁculties: attractor basins are
characterised by vanishing gradients  which are known to make training neural net-
works difﬁcult. In this work  we avoid the vanishing gradient problem by training a
generative distributed memory without simulating the attractor dynamics. Based
on the idea of memory writing as inference  as proposed in the Kanerva Machine 
we show that a likelihood-based Lyapunov function emerges from maximising the
variational lower-bound of a generative memory. Experiments shows it converges
to correct patterns upon iterative retrieval and achieves competitive performance as
both a memory model and a generative model.

1

Introduction

Memory plays an important role in both artiﬁcial and biological learning systems [4]. Various forms
of external memory have been used to augment neural networks [5  14  25  29  31  32]. Most of these
approaches use attention-based reading mechanisms that compute a weighted average of memory
contents. These mechanisms typically retrieve items in a single step and are ﬁxed after training.
While external-memory offers the potential of quickly adapting to new data after training  it is unclear
whether these previously proposed attention-based mechanisms can fully exploit this potential. For
example  when inputs are corrupted by noise that is unseen during training  are such one-step attention
processes always optimal?
In contrast  experimental and theoretical studies of neural systems suggest memory retrieval is a
dynamic and iterative process: memories are retrieved through a potentially varying period of time 
rather than a single step  during which information can be continuously integrated [3  7  20]. In
particular  attractor dynamics are hypothesised to support the robust performance of various forms of
memory via their self-stabilising property [8  12  16  28  33]. For example  point attractors eventually
converge to a set of ﬁxed points even from noisy initial states. Memories stored at such ﬁxed points
can thus be retrieved robustly. To our knowledge  only the Kanerva Machine (KM) incorporates
iterative reconstruction of a retrieved pattern within a modern deep learning model  but it does not
have any guarantee of convergence [32].
Incorporating attractor dynamics into modern neural networks is not straightforward. Although
recurrent neural networks can in principle learn any dynamics  they face the problem of vanishing
gradients. This problem is aggravated when directly training for attractor dynamics  which by
deﬁnition imply vanishing gradients [23] (see also Section 2.2). In this work  we avoid vanishing
gradients by constructing our model to dynamically optimise a variational lower-bound. After
training  the stored patterns serve as attractive ﬁxed-points to which even random patterns will

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

converge. Thanks to the underlying probabilistic model  we do not need to simulate the attractor
dynamics during training  thus avoiding the vanishing gradient problem. We applied our approach
to a generative distributed memory. In this context we focus on demonstrating high capacity and
robustness  though the framework may be used for any other memory model with a well-deﬁned
likelihood.
To conﬁrm that the emerging attractor dynamics help memory retrieval  we experiment with the
Omniglot dataset [22] and images from DMLab [6]  showing that the attractor dynamics consistently
improve images corrupted by noise unseen during training  as well as low-quality prior samples. The
improvement of sampling quality tracks the decrease of an energy which we deﬁned based on the
variational lower-bound.

2 Background and Notation

All vectors are assumed to be column vectors. Samples from a dataset D  as well as other variables 
are indexed with the subscript t when the temporal order is speciﬁed. We use the short-hand subscript
<t and (cid:54)t to indicate all elements with indexes “less than” and “less than or equally to” t  respectively.
(cid:104)f (x)(cid:105)p(x) is used to denotes the expectation of function f (x) over the distribution p(x).

2.1 Kanerva Machines

Our model shares the same essential structure as the Kanerva Machine (ﬁgure 1  left) [32]  which
views memory as a global latent variable in a generative model. Underlying the inference process is
the assumption of exchangeability of the observations: i.e.  an episode of observations x1  x2  . . .   xT
is exchangeable if shufﬂing the indices within the episode does not affect its probability [2]. This
ensures that a pattern xt can be retrieved regardless of the order it was stored in the memory — there
is no forgetting of earlier patterns. Formally  exchangeability implies all the patterns in an episode

are conditionally independent: p(x1  x2  . . . xT|M) =(cid:81)T

More speciﬁcally  p (M; R  U) deﬁnes the distribution over the K × C memory matrix M  where
K is the number of rows and C is the code size used by the memory. The statistical structure of the
memory is summarised in its mean and covariance through parameters R and U. Intuitively  while
the mean provides materials for the memory to synthesise observations  the covariance coordinates
memory reads and writes. R is the mean matrix of M  which has same K × C shape. M’s columns
are independent  with the same variance for all elements in a given row. The covariance between rows
of M is encoded in the K × K covariance matrix U. The vectorised form of M has the multivariate
distribution p (vec (M)) = N (vec (M)| vec (R)   I ⊗ U)  where vec (·) is the vectorisation operator
and ⊗ denotes the Kronecker product. Equivalently  the memory can be summarised as the matrix
variate normal distribution p(M) = MN (R  U  I)  Reading from memory is achieved via a weighted
sum over rows of M  weighted by addressing weights w:

t=1 p(xt|M).

K(cid:88)

k=1

z =

w(k) · M(k) + ξ

(1)

where k indexes the elements of w and the rows of M. ξ is observation noise with ﬁxed variance to
ensure the model’s likelihood is well deﬁned (Appendix A). The memory interfaces with data inputs
x via neural network encoders and decoders.
Since the memory is a linear Gaussian model  its posterior distribution p(M|z(cid:54)t  w(cid:54)t) is analytically
tractable and online Bayesian inference can be performed efﬁciently. [32] interpreted inferring the
posterior of memory as a writing process that optimally balances previously stored patterns and new
patterns. To infer w  however  the KM uses an amortised inference model q(w|x)  similar to the
encoder of a variational autoencoder (VAE) [19  27]  which does not access the memory. Although
it can distil information about the memory into its parameters during training  such parameterised
information cannot easily by adapted to test-time data. This can damage performance during testing 
for example  when the memory is loaded with different numbers of patterns  as we shall demonstrated
in experiments.

2

Figure 1: Variables: M – the memory  x – inputs (e.g.  images)  w – addressing weigths  z –
embedding of x. Left: The probabilistic graphical model shared by the Kanerva Machine and
our model. The memory is a latent variable shared by all patterns in an episode  and provides
exchangeability within the episode. z is omitted since the deterministic embedding of x does not
affect the graphical model. Right: Schematic structure of our model. The memory is a Gaussian
random matrix.

2.2 Attractor Dynamics

A theoretically well-founded approach for robust memory retrieval is to employ attractor dynamics
[3  15  18  33]. In this paper  we focus on point attractors  although other types of attractor may
also support memory systems [12]. For a discrete-time dynamical system with state x and dynamics
speciﬁed by the function f (·)  its states evolve as: xn+1 = f (xn). A ﬁxed-point x∗ = xn satisﬁes
= 0  so that xn+1 = xn = x∗. A ﬁxed point x∗ is attractive
the condition ∂xn+1
∂xn
if  for any point near x∗  iterative application of f (·) converges to x∗. A more formal deﬁnition of
a point attractor is given in Appendix E  along with a proof of attractor dynamics for our model.
Gradient-based training of attractors with parametrised models f (·; θ)  such as neural networks  is
difﬁcult: for any loss function L that depends on the n’th state xn  the gradient

(cid:12)(cid:12)x=xn

= ∂f (x)
∂x

(2)
t=1
xu → 0 when xu → x∗ according
vanishes when xt approaches a ﬁxed point  since xn
xt
to the ﬁxed-point condition. This is the “vanishing gradients” problem  which makes backpropagating
gradients through the attractor settling dynamics difﬁcult [23  24].

=(cid:81)n−1

xu+1

u=t

∂xn
∂xt ·

∂xt
∂θ

n(cid:88)

∂L
∂θ

=

∂L
∂xn ·

3 Dynamic Kanerva Machines

We call our model the Dynamic Kanerva Machine (DKM)  because it optimises weights w at each
step via dynamic addressing. We depart from both Kanerva’s original sparse distributed memory [18]
and the KM by removing the static addresses that are ﬁxed after training. The DKM is illustrated
in ﬁgure 1 (right). Following the KM  we use a Gaussian random matrix M for the memory  and
approximate samples of M using its mean R. We use subscripts t for Mt  Rt and Ut to distinguish
the memory or parameters after the online update at the t’th step when necessary. Therefore 
p (Mt|x(cid:54)t) = p (M|x(cid:54)t).
We use a neural network encoder e(x) → z to deterministically map an external input x to embedding
z. To obtain a valid likelihood function  the decoder is a parametrised distribution d(z) → p (x|z)
that transforms an embedding z to a distribution in the input space  similar to the decoder in the VAE.
Together the pair forms an autoencoder.

Similar to eq. 1  we construct z from the memory and addressing weights via z =(cid:80)K
(cid:0)µwt  σ2

k=1 w(k) ·
M(k). Since both mappings e(·) and d(·) are deterministic  we hereafter omit all dependencies of
distributions on z for brevity. For a Bayesian treatment of the addressing weights  we assume they
have the Gaussian prior p(wt) = N (0  1). The posterior distribution q (wt) = N
variance that is trained as a parameter and a mean that is optimised analytically at each step (Section
3.1). All parameters of the model and their initialisations are summarised in Appendix B.
To train the model in a maximum-likelihood setting  we update the model parameters to maximise
the log-likelihood of episodes x(cid:54)T sampled from the training set (summarised in Algorithm 1). As

(cid:1) has a

w

3

…x1<latexit sha1_base64="XKRpkkbtZtFMa4xQyLBNO+caSI4=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKmKRwA==</latexit><latexit sha1_base64="XKRpkkbtZtFMa4xQyLBNO+caSI4=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKmKRwA==</latexit><latexit sha1_base64="XKRpkkbtZtFMa4xQyLBNO+caSI4=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKmKRwA==</latexit><latexit sha1_base64="XKRpkkbtZtFMa4xQyLBNO+caSI4=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKmKRwA==</latexit>x2<latexit sha1_base64="+fmTm2ZVR20F4bY6bvBdYsdn+/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzI5bQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBK+aRwQ==</latexit><latexit sha1_base64="+fmTm2ZVR20F4bY6bvBdYsdn+/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzI5bQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBK+aRwQ==</latexit><latexit sha1_base64="+fmTm2ZVR20F4bY6bvBdYsdn+/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzI5bQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBK+aRwQ==</latexit><latexit sha1_base64="+fmTm2ZVR20F4bY6bvBdYsdn+/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzI5bQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBK+aRwQ==</latexit>xT<latexit sha1_base64="Lv0eXnBKX961VeUHLOca/6YomRA=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZuxBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9fbpHj</latexit><latexit sha1_base64="Lv0eXnBKX961VeUHLOca/6YomRA=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZuxBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9fbpHj</latexit><latexit sha1_base64="Lv0eXnBKX961VeUHLOca/6YomRA=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZuxBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9fbpHj</latexit><latexit sha1_base64="Lv0eXnBKX961VeUHLOca/6YomRA=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZuxBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9fbpHj</latexit>wT<latexit sha1_base64="RLT7ZikDAglq0wJwzAf2M9NDR/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZulBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9d55Hi</latexit><latexit sha1_base64="RLT7ZikDAglq0wJwzAf2M9NDR/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZulBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9d55Hi</latexit><latexit sha1_base64="RLT7ZikDAglq0wJwzAf2M9NDR/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZulBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9d55Hi</latexit><latexit sha1_base64="RLT7ZikDAglq0wJwzAf2M9NDR/4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclUQEXRbduKzQFzShTKaTduhkEmZulBL6G25cKOLWn3Hn3zhts9DWAwOHc+7lnjlhKoVB1/121tY3Nre2Szvl3b39g8PK0XHbJJlmvMUSmehuSA2XQvEWCpS8m2pO41DyTji+m/mdR66NSFQTJykPYjpUIhKMopV8P6Y4CqP8adpv9itVt+bOQVaJV5AqFGj0K1/+IGFZzBUySY3peW6KQU41Cib5tOxnhqeUjemQ9yxVNOYmyOeZp+TcKgMSJdo+hWSu/t7IaWzMJA7t5CyjWfZm4n9eL8PoJsiFSjPkii0ORZkkmJBZAWQgNGcoJ5ZQpoXNStiIasrQ1lS2JXjLX14l7cuaZ/nDVbV+W9RRglM4gwvw4BrqcA8NaAGDFJ7hFd6czHlx3p2PxeiaU+ycwB84nz9d55Hi</latexit>w2<latexit sha1_base64="t6lFtd8cCHTCg5TWlOjAY0C6ISs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzo5TQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBKl+RwA==</latexit><latexit sha1_base64="t6lFtd8cCHTCg5TWlOjAY0C6ISs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzo5TQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBKl+RwA==</latexit><latexit sha1_base64="t6lFtd8cCHTCg5TWlOjAY0C6ISs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzo5TQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBKl+RwA==</latexit><latexit sha1_base64="t6lFtd8cCHTCg5TWlOjAY0C6ISs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTKdtEMnkzBzo5TQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk414y0Wy1h3A2q4FIq3UKDk3URzGgWSd4LJbe53Hrk2IlYPOE14P6IjJULBKFrJ9yOK4yDMnmaD+qBSdWvuHGSVeAWpQoHmoPLlD2OWRlwhk9SYnucm2M+oRsEkn5X91PCEsgkd8Z6likbc9LN55hk5t8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU1lW4K3/OVV0q7XPMvvL6uNm6KOEpzCGVyAB1fQgDtoQgsYJPAMr/DmpM6L8+58LEbXnGLnBP7A+fwBKl+RwA==</latexit>w1<latexit sha1_base64="r74evW3svIyiOgkGXGS8tPTvtKg=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKNuRvw==</latexit><latexit sha1_base64="r74evW3svIyiOgkGXGS8tPTvtKg=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKNuRvw==</latexit><latexit sha1_base64="r74evW3svIyiOgkGXGS8tPTvtKg=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKNuRvw==</latexit><latexit sha1_base64="r74evW3svIyiOgkGXGS8tPTvtKg=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe5bfX9YaN0UdZTiBUzgHD66gAXfQhBYwSOAZXuHNSZ0X5935WIyWnGLnGP7A+fwBKNuRvw==</latexit>M<latexit sha1_base64="qSAOqPjcFyl69u3u5Tz7jN6qjRo=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiFy9CBVuLbSib7Uu7dLMJuxuhhP4LLx4U8eq/8ea/cdPmoK0DC8PMe+y8CRLBtXHdb6e0srq2vlHerGxt7+zuVfcP2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvs79hydUmsfy3kwS9CM6lDzkjBorPfYiakZBmN1O+9WaW3dnIMvEK0gNCjT71a/eIGZphNIwQbXuem5i/Iwqw5nAaaWXakwoG9Mhdi2VNELtZ7PEU3JilQEJY2WfNGSm/t7IaKT1JArsZJ5QL3q5+J/XTU146WdcJqlByeYfhakgJib5+WTAFTIjJpZQprjNStiIKsqMLaliS/AWT14m7bO6Z/ndea1xVdRRhiM4hlPw4AIacANNaAEDCc/wCm+Odl6cd+djPlpyip1D+APn8we75JDx</latexit><latexit sha1_base64="qSAOqPjcFyl69u3u5Tz7jN6qjRo=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiFy9CBVuLbSib7Uu7dLMJuxuhhP4LLx4U8eq/8ea/cdPmoK0DC8PMe+y8CRLBtXHdb6e0srq2vlHerGxt7+zuVfcP2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvs79hydUmsfy3kwS9CM6lDzkjBorPfYiakZBmN1O+9WaW3dnIMvEK0gNCjT71a/eIGZphNIwQbXuem5i/Iwqw5nAaaWXakwoG9Mhdi2VNELtZ7PEU3JilQEJY2WfNGSm/t7IaKT1JArsZJ5QL3q5+J/XTU146WdcJqlByeYfhakgJib5+WTAFTIjJpZQprjNStiIKsqMLaliS/AWT14m7bO6Z/ndea1xVdRRhiM4hlPw4AIacANNaAEDCc/wCm+Odl6cd+djPlpyip1D+APn8we75JDx</latexit><latexit sha1_base64="qSAOqPjcFyl69u3u5Tz7jN6qjRo=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiFy9CBVuLbSib7Uu7dLMJuxuhhP4LLx4U8eq/8ea/cdPmoK0DC8PMe+y8CRLBtXHdb6e0srq2vlHerGxt7+zuVfcP2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvs79hydUmsfy3kwS9CM6lDzkjBorPfYiakZBmN1O+9WaW3dnIMvEK0gNCjT71a/eIGZphNIwQbXuem5i/Iwqw5nAaaWXakwoG9Mhdi2VNELtZ7PEU3JilQEJY2WfNGSm/t7IaKT1JArsZJ5QL3q5+J/XTU146WdcJqlByeYfhakgJib5+WTAFTIjJpZQprjNStiIKsqMLaliS/AWT14m7bO6Z/ndea1xVdRRhiM4hlPw4AIacANNaAEDCc/wCm+Odl6cd+djPlpyip1D+APn8we75JDx</latexit><latexit sha1_base64="qSAOqPjcFyl69u3u5Tz7jN6qjRo=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiFy9CBVuLbSib7Uu7dLMJuxuhhP4LLx4U8eq/8ea/cdPmoK0DC8PMe+y8CRLBtXHdb6e0srq2vlHerGxt7+zuVfcP2jpOFcMWi0WsOgHVKLjEluFGYCdRSKNA4EMwvs79hydUmsfy3kwS9CM6lDzkjBorPfYiakZBmN1O+9WaW3dnIMvEK0gNCjT71a/eIGZphNIwQbXuem5i/Iwqw5nAaaWXakwoG9Mhdi2VNELtZ7PEU3JilQEJY2WfNGSm/t7IaKT1JArsZJ5QL3q5+J/XTU146WdcJqlByeYfhakgJib5+WTAFTIjJpZQprjNStiIKsqMLaliS/AWT14m7bO6Z/ndea1xVdRRhiM4hlPw4AIacANNaAEDCc/wCm+Odl6cd+djPlpyip1D+APn8we75JDx</latexit>RUneural networkencoder/decoderlinear Gaussianmodeldynamicaddressingwt<latexit sha1_base64="S8icjISybvxDgv0M5fbtxGwAO3M=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AjmeSAg==</latexit><latexit sha1_base64="S8icjISybvxDgv0M5fbtxGwAO3M=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AjmeSAg==</latexit><latexit sha1_base64="S8icjISybvxDgv0M5fbtxGwAO3M=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AjmeSAg==</latexit><latexit sha1_base64="S8icjISybvxDgv0M5fbtxGwAO3M=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AjmeSAg==</latexit>zt<latexit sha1_base64="TjI0qHTFYc+sRP+O2BXd2hFW9NE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVBDf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AkvySBQ==</latexit><latexit sha1_base64="TjI0qHTFYc+sRP+O2BXd2hFW9NE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVBDf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AkvySBQ==</latexit><latexit sha1_base64="TjI0qHTFYc+sRP+O2BXd2hFW9NE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVBDf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AkvySBQ==</latexit><latexit sha1_base64="TjI0qHTFYc+sRP+O2BXd2hFW9NE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVBDf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8AkvySBQ==</latexit>xt<latexit sha1_base64="IvidLzRsEhl3vGMao18Zl4TfqwU=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8Aj+6SAw==</latexit><latexit sha1_base64="IvidLzRsEhl3vGMao18Zl4TfqwU=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8Aj+6SAw==</latexit><latexit sha1_base64="IvidLzRsEhl3vGMao18Zl4TfqwU=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8Aj+6SAw==</latexit><latexit sha1_base64="IvidLzRsEhl3vGMao18Zl4TfqwU=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0GOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnuX3l7XGTVFHGU7gFM7BgytowB00oQUMEniGV3hzUufFeXc+FqMlp9g5hj9wPn8Aj+6SAw==</latexit>Algorithm 1 Training the Dynamic Kanerva Machine (Single training step)

sample an episode x1  x2  . . .   xT from D
Initialise memory q (M0) = p (M0; R0  U0)
for t = 1 : T (in arbitrary order) do
compute embedding zt = e(xt)
compute weights distribution q (wt) by solving µwt from eq. 6 using q (Mt−1)
update memory (Appendix A): q (Mt; Rt  Ut) ← q (Mt−1|µwt  zt; Rt−1  Ut−1)
(optional) set q (Mt−1) = q (Mt) and repeat the previous 2 steps

// begin writing

end for
for t = 1 : T (in arbitrary order) do
compute embedding zt = e(xt)
compute weights distribution q (wt) by solving µwt from eq. 6 using q (MT )
compute read-out embedding: ˆzt ←

(cid:80)K
k=1 wt(k) · RT (k) using sample wt ∼ q (wt)

end for
compute the the objective O ← LT + LAE (eq. 4 and eq. 11) using previously obtained terms
update parameters via gradient ascent to maximise O

// end of reading

// end of writing
// begin reading

is common for latent variable models  we achieve this by maximising a variational lower-bound of
the likelihood. To avoid cluttered notation we assume all training episodes have the same length T ;
nothing in our algorithm depends on this assumption. Given an approximated memory distribution
q (M)  the log-likelihood of an episode can be decomposed as (see full derivation in Appendix C):

ln p (x(cid:54)T ) = LT +

(cid:104)DKL (q (wt)(cid:107)p (wt|xt  M))(cid:105)q(M) + DKL (q (M)(cid:107)p (M|x(cid:54)T ))

(3)

T(cid:88)

t=1

(cid:17)

with its variational lower-bound:

T(cid:88)

(cid:16)

t=1

LT =

(cid:104)ln p (xt|wt  M)(cid:105)q(wt) q(M) − DKL (q (wt)(cid:107)p (wt))

− DKL (q (M)(cid:107)p (M))

(4)

For consistency  we write p (wt) = p (w) = N (0  1). From the perspective of the EM algorithm
[11]  the lower-bound can be maximised in two ways: 1. By tightening the the bound while keeping
the likelihood unchanged. This can be achieved by minimising the KL-divergences in eq. 3  so that
q (wt) approximates the posterior distribution p (wt|xt  M) and q (M) approximates the posterior
distribution p (M|x(cid:54)T ). 2. By directly maximising the lower-bound LT as an evidence lower-bound
1. This may both improve the
objective (ELBO) by  for example  gradient ascent on parameters of LT
quality of posterior approximation by squeezing the bound  and maximising the likelihood of the
generative model.
We develop an algorithm analogous to the two step-EM algorithm: it ﬁrst analytically tighten the
lower-bound by minimising the KL-divergence terms in eq. 3 via inference of tractable parameters 
and then maximises the lower-bound by slow updating of the remaining model parameters via
backpropagation. The analytic inference in the ﬁrst step is quick and does not require training 
allowing the model to adapt to new data at test time.

3.1 Dynamic Addressing
Recall that the approximate posterior distribution of wt has the form: q (wt) = N
the variance parameter is trained using gradient-based updates  dynamic addressing is used to ﬁnd the
µwt that minimises DKL (q (w)(cid:107)p (w|x  M)). Dropping the subscript when it applies to any given
x and M  it can be shown that the KL-divergence can be approximated by the following quadratic
form (see Appendix D for derivation):

w

(cid:0)µwt  σ2

(cid:1). While

DKL (q (w)(cid:107)p (w|x  M)) ≈ −(cid:107)e(x) − M

2σ2
ξ

(cid:124)

µw(cid:107)2

1
2 (cid:107)µw(cid:107)2 + . . .

−

(5)

1This differs from the original EM algorithm  which ﬁxes the approximated posterior in the M step.

4

where the terms that are independent of µw are omitted. Then  the optimal µw can be found by
solving the (regularised) least-squares problem:

(6)
This operation can be implemented efﬁciently via an off-the-shelve least-square solver  such as
TensorFlow’s matrix_solve_ls function which we used in experiments. Intuitively  dynamic
addressing ﬁnds the combination of memory rows that minimises the square error between the read
(cid:124)
out M

µw and the embedding z = e(x)  subject to the constraint from the prior p (w).

µw ←

+ σ2

(cid:0)M M

(cid:124)

ξ · I(cid:1)−1

(cid:124)
M

e(x)

3.2 Bayesian Memory Update
We now turn to the more challenging problem of minimising DKL (q (M)(cid:107)p (M|x(cid:54)T )). We tackle
this minimisation via a sequential update algorithm. To motivate this algorithm we begin by consider-
ing T = 1. In this case  eq. 3 can be simpliﬁed to:

ln p (x1) = L1 + (cid:104)DKL (q (w1)(cid:107)p (w1|x1  M))(cid:105)q(M) + DKL (q (M1)(cid:107)p (M1|x1))

(7)
While it is still unclear how to minimise DKL (q (M1)(cid:107)p (M|x1))  if a suitable weight distribution
q (w1) were given  a slightly different term DKL (q (M1|w1)(cid:107)p (M1|w1  x1)) can be minimised to
0. To achieve this  we can set q (M1|w1) = p (M1|x1  w1) by updating the parameters of q(M)
using the same Bayesian update rule as in the KM (Appendix A): R1  U1 ← R0  U0. We may then
marginalise out w1 to obtain

(cid:90)

q (M1) =

q (M1|w1) q (w1) dw1

A reasonable guess of w1 can be obtained by be solving

q (w1) ← argmax
q(cid:48)(w1)

DKL (q

(w1)(cid:107)p (w1|x1  M0))

(cid:48)

(8)

(9)

as in section 3.1  but using the prior memory M0. To continue  we treat the current posterior q (M1)
as next prior  and compute q (M2) using x2 following the same procedure until we obtain q (MT )
using all x(cid:54)T .
More formally  Appendix C shows this heuristic online update procedure maximises another lower-
bound of the log-likelihood. In addition  the marginalisation in eq. 8 can be approximated by using
µwt instead of sampling wt for each memory update:

q (Mt) ≈ p (Mt|xt  µwt )

(10)
Although this lower-bound is looser than LT (eq. 4)  Appendix C suggests it can be tighten by
iteratively using the updated memory for addressing (e.g.  replacing M0 in eq. 9 by the updated
M1  the “optional” step in Algorithm 1) and update the memory with the reﬁned q (wt). We found
that extra iterations yielded only marginal improvement in our setting  so we did not use it in our
experiments.

3.3 Gradient-Based Training

Having inferred q (wt) and q (M) = q (MT )  we now focus on gradient-based optimisation of the
lower-bound LT (eq. 4). To ensure the likelihood in eq. 4 ln p (x|w  M) can be produced from
the likelihood given by the memory ln p (z|w  M)  we ideally need a bijective pair of encoder and
decoder x ⇐⇒ z (see Appendix D for more discussion). This is difﬁcult to guarantee  but we can
approximate this condition by maximising the autoencoder log-likelihood:
(11)

LAE = (cid:104)ln d (e(x))(cid:105)x∼D

Taken together  we maximise the following joint objective using backpropagation:

O = LT + LAE

(12)

We note that dynamic addressing during online memory updates introduces order dependence since
q (wt) always depends on the previous memory. This violates the model’s exchangeable structure
(order-independence). Nevertheless  gradient-ascend on LT mitigates this effect by adjusting the
model so that DKL (q (w)(cid:107)p (w|x  M)) remains close to a minimum even for previous q (wt).
Appendix C explains this in more details.

5

3.4 Prediction / Reading

memory M: p (x|xq  M) = (cid:82) p (x|w  M) p (w|xq  M) dw This posterior distribution does not

The predictive distribution of our model is the posterior distribution of x given a query xq and
have an analytic form in general (unless d(x|z) is Gaussian). We therefore approximate the integral
using the maximum a posteriori (MAP) estimator of w∗:

(13)
Thus  w∗ can be computed by solving the same least-square problem as in eq. 6 and choosing
w∗ = µw (see Appendix D for details).

p (ˆx|xq  M) = p (x|w

  M)

(cid:12)(cid:12)(cid:12)w∗=argmaxw p(w|xq M)

∗

3.5 Attractor Dynamics

To understand the model’s attractor dynamics  we deﬁne the energy of a conﬁguration (x  w) with a
given memory M as:

E(x  w) = −(cid:104)ln p (x|w  M)(cid:105)q(M) + DKL (qt(w)(cid:107)p (w))

(14)
For a well trained model  with x ﬁxed  E(x  w) is at minimum with respect to w after minimising
DKL (q (w)(cid:107)p (w|x  M)) (eq. 6). To see this  note that the negative of E(x  w) consist of just terms
in LT in eq. 4 that depend on a speciﬁc x and w  which are maximised during training. Now we
can minimise E(x  w) further by ﬁxing w and optimising x. Since only the ﬁrst term depends on x 
E(x  w) is further minimised by choosing the mode of the likelihood function (cid:104)ln p (x|w  M)(cid:105)q(M).
For example  we take the mean for the Gaussian likelihood  and round the sigmoid outputs for the
Bernoulli likelihood. Each step can be viewed as coordinate descent over the energy E(x  w)  as
illustrated in ﬁgure 2 (left).
The step of optimising w following by taking the mode of x is exactly the same as taking the
mode of the predictive distribution xmode = argmaxˆx p (ˆx|xq  M) (eq. 13). Therefore  we can
simulate the attractor dynamics by repeatedly feeding-back the predictive mode as the next query:
x1 = argmaxˆx p(ˆx|x0  M)  x2 = argmaxˆx p(ˆx|x1  M)  . . .   xn = argmaxˆx p(ˆx|xn−1  M). This
sequence converges to a stored pattern in the memory  because each iteration minimises the energy
E(x  w)  so that E(xn  wn) < E(xn−1  wn−1)  unless it has already converged at E(xn  wn) =
E(xn−1  wn−1). Therefore  the sequence will converge to some x∗  a local minimum in the energy
landscape  which in a well trained memory model corresponds to a stored pattern.
Viewing xn = argmaxˆx p(ˆx|xn−1  M) as a dynamical system  the stored patterns correspond to
point attractors in this system. See Appendix C for a formal treatment. In this work we employed
deterministic dynamics in our experiments and to simplify analysis. Alternatively  sampling from
q (w) and the predictive distribution would give stochastic dynamics that simulate Markov-Chain
Monte Carlo (MCMC). We leave this direction for future investigation.

4 Experiments

We tested our model on Ominglot [22] and frames from DMLab tasks [6]. Both datasets have images
from a large number of classes  well suited to testing fast adapting external memory: 1200 different
characters in Omniglot  and inﬁnitely many procedurally generated 3-D maze environments from
DMLab. We treat Omniglot as binary data  while DMLab has larger real-valued colour images. We
demonstrate that the same model structure with identical hyperparameters (except for number of
ﬁlters  the predictive distribution  and memory size) can readily handle these different types of data.
To compare with the KM  we followed [32] to prepare the Ominglot dataset  and employed the same
convolutional encoder and decoder structure. We trained all models using the Adam optimiser [19]
with learning rate 1 × 10−4. We used 16 ﬁlters in the convnet and 32 × 100 memory for Omniglot 
and 256 ﬁlters and 64 × 200 memory for DMLab. We used the Bernoulli likelihood function for
Omniglot  and the Gaussian likelihood function for DMLab data. Uniform noise U(0  1
128 ) was added
to the labyrinth data to prevent the Gaussian likelihood from collapsing.
Following [32]  we report the lower-bound on the conditional log-likelihood ln p (x(cid:54)T|M) by
removing DKL (q (M)(cid:107)p (M)) from LT (eq. 4). This is the negative energy −E  and we obtained

6

the per-image bound (i.e.  conditional ELBO) by dividing it by the episode size. We trained the model
for Omniglot for approximately 3×105 steps; the test conditional ELBO reached 77.2  which is worse
than the 68.3 reported from the KM [32]. However  we show that the DKM generalises much better to
unseen long episodes. We trained the model for DMLab for 1.1×105 steps; the test conditional ELBO
reached −9046.5  which corresponds to 2.75 bits per pixel. After training  we used the same testing
protocol as [32]  ﬁrst computing the posterior distribution of memory (writing) given an episode  and
then performing tasks using the memory’s posterior mean. For reference  our implementation of the
memory module is provided at https://github.com/deepmind/dynamic-kanerva-machines.

Capacity

We investigated memory capacity using the Omniglot dataset  and compared our model with the
KM and DNC. To account for the additional O(K 3) cost in the proposed dynamic addressing 
our model in Omniglot experiments used a signiﬁcantly smaller number of memory parameters
(32 × 100 + 32 × 32) than the DNC (64 × 100)  and less than half of that used for the KM in [32].
Moreover  our model does not have additional parametrised structure  like the memory controllers in
DNC or the amortised addressing module in the KM. As in [32]  we train our model using episodes
with 32 patterns randomly sampled from all classes  and test it using episodes with lengths ranging
from 10 to 200  drawn from 2  4  or 8 classes of characters (i.e. varying the redundancy of the
observed data). We report retrieval error as the negative of the conditional ELBO. The results are
shown in ﬁgure 2 (right)  with results for the KM and DNC adapted from [32].

Figure 2: Left: Illustration of the attractor dynamics that converge to a local minimum of the energy
E(x  w). The circles shows contours of the energy. Black arrows shows the results from optimising
w by solving the least-square problem; blue arrows depict optimisation of x by taking the mode of
the predictive distribution. Right: Comparing the capacity of our model (diamond lines) with the KM
(solid lines) and the DNC (dashed lines). Our model compresses and generalises signiﬁcantly better
for long episodes.

The capacity curves for our model are strikingly ﬂat compared with both the DNC and the KM; we
believe that this is because the parameter-free addressing (section 3.1) generalises to longer episodes
much better than the parametrised addressing modules in the DNC or the KM. The errors are larger
than the KM for small numbers of patterns (approximately <60)  possibly because the KM over-ﬁts
to shorter episodes that were more similar to training episodes.

Attractor Dynamics: Denoising and Sampling

We next veriﬁed the attractor dynamics through denoising and sampling tasks. These task demonstrate
how low-quality patterns  either from noise-corruption or imperfect priors  can be corrected using the
attractor dynamics. Figure 3 (a) and Figure 4 (a) show the result of denoising. We added salt-and-
pepper noise to Omniglot images by randomly ﬂipping 15% of the bits  and independent Gaussian
noise N (0  0.15) to all pixels in DMLab images. Such noise is never presented during training. We
ran the attractor dynamics (section 3.5) for 15 iterations from the noise corrupted images. Despite the
signiﬁcant corruption of images via different types of noise  the image quality improved steadily for
both datasets. Interestingly  the denoised Omniglot patterns are even cleaner and smoother than the

7

w<latexit sha1_base64="z/nekUz73YPC7FPrXIbLtsxjNL4=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbpQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/u2kRs=</latexit><latexit sha1_base64="z/nekUz73YPC7FPrXIbLtsxjNL4=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbpQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/u2kRs=</latexit><latexit sha1_base64="z/nekUz73YPC7FPrXIbLtsxjNL4=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbpQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/u2kRs=</latexit><latexit sha1_base64="z/nekUz73YPC7FPrXIbLtsxjNL4=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbpQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/u2kRs=</latexit>x<latexit sha1_base64="5FH1f5q1iZ8UGbO/bkoJpmknF4s=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbsQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/07kRw=</latexit><latexit sha1_base64="5FH1f5q1iZ8UGbO/bkoJpmknF4s=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbsQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/07kRw=</latexit><latexit sha1_base64="5FH1f5q1iZ8UGbO/bkoJpmknF4s=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbsQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/07kRw=</latexit><latexit sha1_base64="5FH1f5q1iZ8UGbO/bkoJpmknF4s=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2L+3SzSbsbsQS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ndLK6tr6RnmzsrW9s7tX3T9o6zhVDFssFrG6D6hGwSW2DDcC7xOFNAoEdoLxde53HlFpHss7M0nQj+hQ8pAzaqz00IuoGQVh9jTtV2tu3Z2BLBOvIDUo0OxXv3qDmKURSsME1brruYnxM6oMZwKnlV6qMaFsTIfYtVTSCLWfzRJPyYlVBiSMlX3SkJn6eyOjkdaTKLCTeUK96OXif143NeGln3GZpAYlm38UpoKYmOTnkwFXyIyYWEKZ4jYrYSOqKDO2pIotwVs8eZm0z+qe5bfntcZVUUcZjuAYTsGDC2jADTShBQwkPMMrvDnaeXHenY/5aMkpdg7hD5zPH/07kRw=</latexit>E(x w)<latexit sha1_base64="/FInCnFwHf1qSgJbbgXOiO2T0kc=">AAACDHicbZDLSsNAFIZP6q3WW9Wlm8EiVJCSiKDLogguK9gLtKFMppN26OTCzEQtIQ/gxldx40IRtz6AO9/GSRpBW38Y+PjPOcw5vxNyJpVpfhmFhcWl5ZXiamltfWNzq7y905JBJAhtkoAHouNgSTnzaVMxxWknFBR7DqdtZ3yR1tu3VEgW+DdqElLbw0OfuYxgpa1+udLzsBoRzOPLpJqx48b3yRH64bvkUHeZNTMTmgcrhwrkavTLn71BQCKP+opwLGXXMkNlx1goRjhNSr1I0hCTMR7SrkYfe1TacXZMgg60M0BuIPTzFcrc3xMx9qSceI7uTFeUs7XU/K/WjZR7ZsfMDyNFfTL9yI04UgFKk0EDJihRfKIBE8H0roiMsMBE6fxKOgRr9uR5aB3XLM3XJ5X6eR5HEfZgH6pgwSnU4Qoa0AQCD/AEL/BqPBrPxpvxPm0tGPnMLvyR8fENBQKblA==</latexit><latexit sha1_base64="/FInCnFwHf1qSgJbbgXOiO2T0kc=">AAACDHicbZDLSsNAFIZP6q3WW9Wlm8EiVJCSiKDLogguK9gLtKFMppN26OTCzEQtIQ/gxldx40IRtz6AO9/GSRpBW38Y+PjPOcw5vxNyJpVpfhmFhcWl5ZXiamltfWNzq7y905JBJAhtkoAHouNgSTnzaVMxxWknFBR7DqdtZ3yR1tu3VEgW+DdqElLbw0OfuYxgpa1+udLzsBoRzOPLpJqx48b3yRH64bvkUHeZNTMTmgcrhwrkavTLn71BQCKP+opwLGXXMkNlx1goRjhNSr1I0hCTMR7SrkYfe1TacXZMgg60M0BuIPTzFcrc3xMx9qSceI7uTFeUs7XU/K/WjZR7ZsfMDyNFfTL9yI04UgFKk0EDJihRfKIBE8H0roiMsMBE6fxKOgRr9uR5aB3XLM3XJ5X6eR5HEfZgH6pgwSnU4Qoa0AQCD/AEL/BqPBrPxpvxPm0tGPnMLvyR8fENBQKblA==</latexit><latexit sha1_base64="/FInCnFwHf1qSgJbbgXOiO2T0kc=">AAACDHicbZDLSsNAFIZP6q3WW9Wlm8EiVJCSiKDLogguK9gLtKFMppN26OTCzEQtIQ/gxldx40IRtz6AO9/GSRpBW38Y+PjPOcw5vxNyJpVpfhmFhcWl5ZXiamltfWNzq7y905JBJAhtkoAHouNgSTnzaVMxxWknFBR7DqdtZ3yR1tu3VEgW+DdqElLbw0OfuYxgpa1+udLzsBoRzOPLpJqx48b3yRH64bvkUHeZNTMTmgcrhwrkavTLn71BQCKP+opwLGXXMkNlx1goRjhNSr1I0hCTMR7SrkYfe1TacXZMgg60M0BuIPTzFcrc3xMx9qSceI7uTFeUs7XU/K/WjZR7ZsfMDyNFfTL9yI04UgFKk0EDJihRfKIBE8H0roiMsMBE6fxKOgRr9uR5aB3XLM3XJ5X6eR5HEfZgH6pgwSnU4Qoa0AQCD/AEL/BqPBrPxpvxPm0tGPnMLvyR8fENBQKblA==</latexit><latexit sha1_base64="/FInCnFwHf1qSgJbbgXOiO2T0kc=">AAACDHicbZDLSsNAFIZP6q3WW9Wlm8EiVJCSiKDLogguK9gLtKFMppN26OTCzEQtIQ/gxldx40IRtz6AO9/GSRpBW38Y+PjPOcw5vxNyJpVpfhmFhcWl5ZXiamltfWNzq7y905JBJAhtkoAHouNgSTnzaVMxxWknFBR7DqdtZ3yR1tu3VEgW+DdqElLbw0OfuYxgpa1+udLzsBoRzOPLpJqx48b3yRH64bvkUHeZNTMTmgcrhwrkavTLn71BQCKP+opwLGXXMkNlx1goRjhNSr1I0hCTMR7SrkYfe1TacXZMgg60M0BuIPTzFcrc3xMx9qSceI7uTFeUs7XU/K/WjZR7ZsfMDyNFfTL9yI04UgFKk0EDJihRfKIBE8H0roiMsMBE6fxKOgRr9uR5aB3XLM3XJ5X6eR5HEfZgH6pgwSnU4Qoa0AQCD/AEL/BqPBrPxpvxPm0tGPnMLvyR8fENBQKblA==</latexit>Figure 3: a: Denoising of Omniglot patterns. Patterns in the second column are obtained by adding
15% salt-and-pepper noise to the patterns in the ﬁrst column. The following columns shows samples
from consecutive iterations. b: Sampling of Omniglot patterns. Patterns inside the top box were
written into memory. Patterns in the ﬁrst column are reconstructed using w ∼ p (w)  which are then
improved through iterations in the following columns. c: Energy as a function of iterations during
denoising. d: Energy as a function of iterations during sampling.

original patterns. The trajectories of the energy during denoising for 20 examples (including those
we plotted as images) are shown in Figure 3 (c) and Figure 4 (c)  demonstrating that the system states
were attracted to points with lower energy.

Figure 4: Experiment results for DMLab. Description of each panel is matched to those in Figure 3.

Sampling from the models’ prior distributions provides another application of the attractor dynamics.
Generative models trained with stochastic variational inference usually suffer from the problem of low
sample quality  because the asymmetric KL-divergence they minimise usually results in priors broader
than the posterior that is used to train the decoders. While different approaches exist to improve
sample quality  including using more elaborated posteriors [26] and different training objectives
[13]  our model solves this problem by moving to regions with higher likelihoods via the attractor
dynamics. As illustrated in Figure 3 (c) and Figure 4 (c)  the initial samples have relatively low
quality  but they were improved steadily through iterations. This improvement is correlated with the
decrease of energy. We do observe ﬂuctuations in energy in all experiments  especially for DMLab.
This may be caused by the saddle-points that are more common in larger models [9]. While the
observation of saddle-points violates our assumption of local minima (section 3.5)  our model still
worked well and the energy generally dropped after temporarily rising.

8

5 Discussion

Here we have presented a novel approach to robust attractor dynamics inside a generative distributed
memory. Other than the neural network encoder and decoder  our model has only a small number of
statistically well-deﬁned parameters. Despite its simplicity  we have demonstrated its high capacity by
efﬁciently compressing episodes online and have shown its robustness in retrieving patterns corrupted
by unseen noise.
Our model can trade increased computation for higher precision retrieval by running attractor
dynamics for more iterations. This idea of using attractors for memory retrieval and cleanup dates
to Hopﬁeld nets [15] and Kanerva’s sparse distributed memory [18]. Zemel and Mozer proposed
a generative model for memory [33] that pioneered the use of variational free energy to construct
attractors for memory. By restricting themselves to a localist representation  their model is easy
to train without backpropagation  though this choice constrains its capacity. On the other hand 
Boltzmann Machines [1] are high-capacity generative models with distributed representations which
obey stochastic attractor dynamics. However  writing memories into the weights of Boltzmann
machines is typically slow and difﬁcult. In comparison  the DKM trains quickly via a low-variance
gradient estimator and allows fast memory writing as inference. Notably  Saul and Jordan [30]
discussed the limit of undirected graphical models compared with directed models in learning  and
proposed mean-ﬁeld-based attractor dynamics for iterative inference in feed-forward belief networks.
Our method can also be seen as an extension along this line.
As a principled probabilistic model  the linear Gaussian memory of the DKM can be seen as a
special case of the Kalman Filter (KF) [17] without the drift-diffusion dynamics of the latent state.
This more stable structure captures the statistics of entire episodes during sequential updates with
minimal interference. The idea of using the latent state of the KF as memory is closely related to the
hetero-associative novelty ﬁlter suggested in [10]. The DKM can be also contrasted with recently
proposed nonlinear generalisations of the KF such as [21] in that we preserve the higher-level linearity
for efﬁcient analytic inference over a very large latent state (M). By combining deep neural networks
and variational inference this allows our model to store associations between a large number of
patterns  and generalise to large scale non-Gaussian datasets .

References
[1] David H Ackley  Geoffrey E Hinton  and Terrence J Sejnowski. A learning algorithm for boltzmann

machines. In Readings in Computer Vision  pages 522–533. Elsevier  1987.

[2] David J Aldous. Exchangeability and related topics.

XIII—1983  pages 1–198. Springer  1985.

In École d’Été de Probabilités de Saint-Flour

[3] Daniel J Amit. Modeling brain function: The world of attractor neural networks. Cambridge university

press  1992.

[4] John Robert Anderson. Learning and memory: An integrated approach. John Wiley & Sons Inc  2000.

[5] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. arXiv preprint arXiv:1409.0473  2014.

[6] Charles Beattie  Joel Z Leibo  Denis Teplyashin  Tom Ward  Marcus Wainwright  Heinrich Küttler  Andrew
Lefrancq  Simon Green  Víctor Valdés  Amir Sadik  et al. Deepmind lab. arXiv preprint arXiv:1612.03801 
2016.

[7] Jonathan D Cohen  William M Perlstein  Todd S Braver  Leigh E Nystrom  Douglas C Noll  John Jonides 
and Edward E Smith. Temporal dynamics of brain activation during a working memory task. Nature 
386(6625):604  1997.

[8] John Conklin and Chris Eliasmith. A controlled attractor network model of path integration in the rat.

Journal of computational neuroscience  18(2):183–203  2005.

[9] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In
Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors  Advances in Neural
Information Processing Systems 27  pages 2933–2941. Curran Associates  Inc.  2014.

9

[10] Peter Dayan and Sham Kakade. Explaining away in weight space. In Advances in neural information

processing systems  pages 451–457  2001.

[11] Arthur P Dempster  Nan M Laird  and Donald B Rubin. Maximum likelihood from incomplete data via the

em algorithm. Journal of the royal statistical society. Series B (methodological)  pages 1–38  1977.

[12] Surya Ganguli  Dongsung Huh  and Haim Sompolinsky. Memory traces in dynamical systems. Proceedings

of the National Academy of Sciences  105(48):18970–18975  2008.

[13] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani  M. Welling  C. Cortes 
N. D. Lawrence  and K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 27 
pages 2672–2680. Curran Associates  Inc.  2014.

[14] Alex Graves  Greg Wayne  Malcolm Reynolds  Tim Harley  Ivo Danihelka  Agnieszka Grabska-Barwi´nska 
Sergio Gómez Colmenarejo  Edward Grefenstette  Tiago Ramalho  John Agapiou  et al. Hybrid computing
using a neural network with dynamic external memory. Nature  538(7626):471  2016.

[15] John J Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.

Proceedings of the national academy of sciences  79(8):2554–2558  1982.

[16] Auke Jan Ijspeert  Jun Nakanishi  Heiko Hoffmann  Peter Pastor  and Stefan Schaal. Dynamical movement

primitives: learning attractor models for motor behaviors. Neural computation  25(2):328–373  2013.

[17] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME–Journal of Basic Engineering  82(Series D):35–45  1960.

[18] Pentti Kanerva. Sparse distributed memory. MIT press  1988.
[19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

International Conference on Learning Representations (ICLR)  2013.

In Proceedings of the 2nd

[20] Konrad P Kording  Joshua B Tenenbaum  and Reza Shadmehr. The dynamics of memory as a consequence

of optimal adaptation to a changing body. Nature neuroscience  10(6):779  2007.

[21] Rahul G Krishnan  Uri Shalit  and David Sontag. Deep kalman ﬁlters. arXiv preprint arXiv:1511.05121 

2015.

[22] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept learning through

probabilistic program induction. Science  350(6266):1332–1338  2015.

[23] Razvan Pascanu  Tomas Mikolov  and Yoshua Bengio. On the difﬁculty of training recurrent neural

networks. In International Conference on Machine Learning  pages 1310–1318  2013.

[24] Barak A Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural Computation 

1(2):263–269  1989.

[25] Alexander Pritzel  Benigno Uria  Sriram Srinivasan  Adrià Puigdomènech  Oriol Vinyals  Demis Hassabis 

Daan Wierstra  and Charles Blundell. Neural episodic control. arXiv preprint arXiv:1703.01988  2017.

[26] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. arXiv

preprint arXiv:1505.05770  2015.

[27] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. In The 31st International Conference on Machine Learning
(ICML)  2014.

[28] Alexei Samsonovich and Bruce L McNaughton. Path integration and cognitive mapping in a continuous

attractor neural network model. Journal of Neuroscience  17(15):5900–5920  1997.

[29] Adam Santoro  Sergey Bartunov  Matthew Botvinick  Daan Wierstra  and Timothy Lillicrap. One-shot

learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065  2016.

[30] Lawrence K Saul and Michael I Jordan. Attractor dynamics in feedforward neural networks. Neural

Computation  12(6):1313–1335  2000.

[31] Jason Weston  Sumit Chopra  and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916 

2014.

[32] Yan Wu  Greg Wayne  Alex Graves  and Timothy Lillicrap. The kanerva machine: A generative distributed

memory. In International Conference on Learning Representations  2018.

[33] Richard S Zemel and Michael C Mozer. A generative model for attractor dynamics. In Advances in neural

information processing systems  pages 80–88  2000.

10

,Yan Wu
Gregory Wayne
Karol Gregor
Timothy Lillicrap