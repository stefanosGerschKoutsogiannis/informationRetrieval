2011,Better Mini-Batch Algorithms via Accelerated Gradient Methods,Mini-batch algorithms have recently received significant attention as a way to speed-up stochastic convex optimization problems. In this paper  we study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis  which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up. We propose a novel accelerated gradient algorithm  which deals with this deficiency  and enjoys a uniformly superior guarantee. We conclude our paper with experiments  on real-world datasets  which validates our algorithm and  substantiates our theoretical insights.,Better Mini-Batch Algorithms

via Accelerated Gradient Methods

Andrew Cotter

Toyota Technological Institute at Chicago

cotter@ttic.edu

Ohad Shamir

Microsoft Research  NE
ohadsh@microsoft.com

Nathan Srebro

Karthik Sridharan

Toyota Technological Institute at Chicago

Toyota Technological Institute at Chicago

nati@ttic.edu

karthik@ttic.edu

Abstract

Mini-batch algorithms have been proposed as a way to speed-up stochastic
convex optimization problems. We study how such algorithms can be im-
proved using accelerated gradient methods. We provide a novel analysis 
which shows how standard gradient methods may sometimes be insuﬃcient
to obtain a signiﬁcant speed-up and propose a novel accelerated gradient
algorithm  which deals with this deﬁciency  enjoys a uniformly superior
guarantee and works well in practice.

1

Introduction

We consider a stochastic convex optimization problem of the form minw∈W L(w)  where
L(w) = Ez [￿(w  z)]  based on an empirical sample of instances z1  . . .   zm. We assume that
W is a convex subset of some Hilbert space (which in this paper  we will take to be Euclidean
space)  and ￿ is non-negative  convex and smooth in its ﬁrst argument (i.e. has a Lipschitz-
constinuous gradient). The classical learning application is when z = (x  y) and ￿(w  (x  y))
is a prediction loss. In recent years  there has been much interest in developing eﬃcient
ﬁrst-order stochastic optimization methods for these problems  such as mirror descent [2  6]
and dual averaging [9  16]. These methods are characterized by incremental updates based
on subgradients ∂￿(w  zi) of individual instances  and enjoy the advantages of being highly
scalable and simple to implement.

An important limitation of these methods is that they are inherently sequential  and so
problematic to parallelize. A popular way to speed-up these algorithms  especially in a
parallel setting  is via mini-batching  where the incremental update is performed on an
average of the subgradients with respect to several instances at a time  rather than a single
instance (i.e.  1
j=1 ∂￿(w  zi+j)). The gradient computations for each mini-batch can
be parallelized  allowing these methods to perform faster in a distributed framework (see
for instance [11]). Recently  [10] has shown that a mini-batching distributed framework is
capable of attaining asymptotically optimal speed-up in general (see also [1]).

b￿b

A parallel development has been the popularization of accelerated gradient descent methods
[7  8  15  5]. In a deterministic optimization setting and for general smooth convex functions 
these methods enjoy a rate of O(1/n2) (where n is the number of iterations) as opposed
to O(1/n) using standard methods. However  in a stochastic setting (which is the relevant
one for learning problems)  the rate of both approaches have an O(1/√n) dominant term
in general  so the beneﬁt of using accelerated methods for learning problems is not obvious.

1

Algorithm 1 Stochastic Gradient Descent with Mini-Batching (SGD)

Parameters: Step size η  mini-batch size b.
Input: Sample z1  . . .   zm
w1 = 0
for i = 1 to n = m/b do

t=b(i−1)+1 ￿(wi  zt)

Let ￿i(wi) = 1
w￿i+1 := wi − η∇￿i(wi))
wi+1 := PW (w￿i+1)
i=1 wi

end for
Return ¯w = 1

b￿bi
n￿n

Algorithm 2 Accelerated Gradient Method (AG)
Parameters: Step sizes (γi β i)  mini-batch size b
Input: Sample z1  . . .   zm
w = 0
for i = 1 to n = m/b do

t=b(i−1)+1 ￿(w  zt)

i

b￿bi

Let ￿i(wi) := 1
:= β−1
wmd
w￿i+1 := wmd
wi+1 := PW (w￿i+1)
wag
i+1 ← β−1

i wi + (1 − β−1
i − γi∇￿i(wmd
i wi+1 + (1 − β−1

i

end for
Return wag
n

)wag
i
)

i

)wag
i

i

In this paper  we study the application of accelerated methods for mini-batch algorithms  and
provide theoretical results  a novel algorithm  and empirical experiments. The main resulting
message is that by using an appropriate accelerated method  we obtain signiﬁcantly better
stochastic optimization algorithms in terms of convergence speed. Moreover  in certain
regimes acceleration is actually necessary in order to allow a signiﬁcant speedups. The
potential beneﬁt of acceleration to mini-batching has been brieﬂy noted in [4]  but here we
study this issue in much more depth. In particular  we make the following contributions:

• We develop novel convergence bounds for the standard gradient method  which
reﬁnes the result of [10  4] by being dependent on L(w￿)  the expected loss of
the best predictor in our class. For example  we show that in the regime where the
desired suboptimality is comparable or larger than L(w￿)  including in the separable
case L(w￿) = 0  mini-batching does not lead to signiﬁcant speed-ups with standard
gradient methods.

is optimized for a mini-batch framework and implicitly adaptive to L(w￿).

• We develop a novel variant of the stochastic accelerated gradient method [5]  which
• We provide an analysis of our accelerated algorithm  reﬁning the analysis of [5] by
being dependent on L(w￿)  and show how it always allows for signiﬁcant speed-
ups via mini-batching  in contrast to standard gradient methods. Moreover  its
performance is uniformly superior  at least in terms of theoretical upper bounds.
• We provide an empirical study  validating our theoretical observations and the eﬃ-

cacy of our new method.

2 Preliminaries

As discussed in the introduction  we focus on a stochastic convex optimization problem 
where we wish to minimize L(w) = Ez [￿(w  z)] over some convex domain W  using an i.i.d.
sample z1  . . .   zm. Throughout this paper we assume that the instantaneous loss ￿(·  z) is
convex  non-negative and H-smooth for each z ∈Z . Also in this paper  we take W to be
the set W = {w : ￿w￿ ≤ D}  although our results can be generalized.

2

We discuss two stochastic optimization approaches to deal with this problem: stochastic
gradient descent (SGD)  and accelerated gradient methods (AG). In a mini-batch setting 
both approaches iteratively average sub-gradients with respect to several instances  and use
this average to update the predictor. However  the update is done in diﬀerent ways.

The stochastic gradient descent algorithm (which in more general settings is known as
mirror descent  e.g. [6]) is summarized as Algorithm 1. In the pseudocode  PW refers to the
projection on to the ball W  which amounts to rescaling w to have norm at most D.
The accelerated gradient method (e.g.  [5]) is summarized as Algorithm 2.

In terms of existing results  for the SGD algorithm we have [4  Section 5.1]

whereas for an accelerated gradient algorithm  we have [5]

E [L( ¯w)] − L(w￿) ≤O ￿￿ 1
n )] − L(w￿) ≤O ￿￿ 1

m

m

E [L(wag

b

m￿  

+

b2

m2￿ .

+

Thus  as long as b = o(√m)  both methods allow us to use a large mini-batch size b
without signiﬁcantly degrading the performance of either method. This allows the number of
iterations n = m/b to be smaller  potentially resulting in faster convergence speed. However 
these bounds do not show that accelerated methods have a signiﬁcant advantage over the
SGD algorithm  at least when b = o(√m)  since both have the same ﬁrst-order term 1/√m.
To understand the diﬀerences between these two methods better  we will need a more reﬁned
analysis  to which we now turn.

3 Convergence Guarantees

The following theorems provide a reﬁned convergence guarantee for the SGD algorithm and
the AG algorithm  which improves on the analysis of [10  4  5] by being explicitly dependent
on L(w￿)  the expected loss of the best predictor w￿ in W. Due to lack of space  the proofs
are only sketched. The full proofs are deferred to the supplementary material.
=
Theorem 1. For

Stochastic Gradient Descent

algorithm with

the

η

min￿ 1

L(w￿)Hn

2H   ￿ bD2
1+￿ HD2

L(w￿)bn￿  assuming L(0) ≤ HD2  we get that
E [L( ¯w)] − L(w￿) ≤￿ HD2 L(w￿)

2bn

+

n

2HD2

+

9HD2

bn

Theorem 2. For the Accelerated Descent algorithm with βi = i+1

2   γi = γip where

γ = min￿ 1

4H   ￿

bD2

412HL(w￿)(n−1)2p+1   ￿

2p+1￿
1044H(n−1)2p￿ p+1

b

2p+1￿
4HD2+√4HD2L(w￿)￿ p

D2

(1)

(2)

and

as long as n ≥ 904  we have that

 

2 log(n − 1)

p = min￿max￿ log(b)
n )] − L(w￿) ≤ 358￿ HD2L(w￿)

b(n − 1)

E [L(wag

2 (log(b(n − 1)) − log log(n))￿   1￿  

log log(n)

+

1545HD2

√b(n − 1)

+

1428HD2√log n

b(n − 1)

+

4HD2
(n − 1)2

3

We emphasize that Theorem 2 gives more than a theoretical bound: it actually speciﬁes a
novel accelerated gradient strategy  where the step size γi scales polynomially in i  in a way
dependent on the minibatch size b and L(w￿). While L(w￿) may not be known in advance 
it does have the practical implication that choosing γi ∝ ip for some p < 1  as opposed to
just choosing γi ∝ i as in [5])  might yield superior results.
The key observation used for analyzing the dependence on L(w￿) is that for any non-negative
H-smooth convex function f : W ￿→ R  we have [13]:
￿∇f (w)￿ ≤￿4Hf (w)

This self-bounding property tells us that the norm of the gradient is small at a point if the
loss is itself small at that point. This self-bounding property has been used in [14] in the
online setting and in [13] in the stochastic setting to get better (faster) rates of convergence
for non-negative smooth losses. The implication of this observation are that for any w ∈ W 

￿∇L(w)￿ ≤￿4HL(w) and ∀z ∈Z  ￿￿(w  z)￿ ≤￿4H￿(w  z).

Proof sketch for Theorem 1. The proof for the stochastic gradient descent bound is
mainly based on the proof techniques in [5] and its extension to the mini-batch case in [10].
Following the line of analysis in [5]  one can show that

(3)

E￿ 1

n

n￿i=1

L(wi)￿ − L(w￿) ≤ η

n−1

n−1￿i=1

E￿￿∇L(wi) − ∇￿i(wi)￿2￿ + D2

2η(n−1)

In the case of [5]  E [￿∇L(wi) − ∇￿i(wi)￿] is bounded by the variance  and that leads to the
ﬁnal bound provided in [5] (by setting η appropriately). As noticed in [10]  in the minibatch
b￿bi
setting we have ∇￿i(wi) = 1
L(wi)￿ − L(w￿) ≤
E￿ 1

t=b(i−1)+1 ￿(wi  zt) and so one can further show that

E￿∇L(wi) − ∇￿(wi  zt)￿2 + D2
2η(n−1)

b2(n−1)

(4)

n

η

n−1￿i=1

ib￿t=

(i−1)b+1

n￿i=1

In [10]  each of ￿∇L(wi) − ∇￿(wi  zt)￿ is bounded by σ0 and so setting η  the mini-batch
bound provided there is obtained. In our analysis we further use the self-bounding property
to (4) and get that

E￿ 1

n

n￿i=1

L(wi)￿ − L(w￿) ≤ 16Hη

b(n−1)

n−1￿i=1

E [L(wi)] + D2

2η(n−1)

rearranging and setting η appropriately gives the ﬁnal bound.

Proof sketch for Theorem 2. The proof of the accelerated method starts in a similar
way as in [5]. For the γi’a and βi’s mentioned in the theorem  following similar lines of
analysis as in [5] we get the preliminary bound

E [L(wag

n )] − L(w￿) ≤

2γ

(n − 1)p+1

n−1￿i=1

i2p E￿￿￿∇L(wmd

i

) − ∇￿i(wmd

i

)￿￿2￿ +

D2

γ(n − 1)p+1

In [5] the step size γi = γ(i + 1)/2 and βi = (i + 1)/2 which eﬀectively amounts to
p = 1 and further similar to the stochastic gradient descent analysis. Furthermore  each

to the ﬁnal bound provided in [5] by setting γ appropriately. On the other hand  we ﬁrst
notice that due to the mini-batch setting  just like in the proof of stochastic gradient descent 

)￿￿2￿ is assumed to be bounded by some constant  and thus leads

E￿￿￿∇L(wmd

) − ∇￿i(wmd

i

i

E [L(wag

n )] − L(w￿) ≤

2γ

b2(n−1)p+1

i2p

n−1￿i=1

ib￿t=

b(i−1)+1

E￿￿￿∇L(wmd

i

) − ∇￿(wmd

i

  zt)￿￿2￿ +

D2

γ(n−1)p+1

4

Using smoothness  the self bounding property some manipulations  we can further get the
bound

E [L(wag

n )] − L(w￿) ≤ 64Hγ
b(n−1)1−p
+

(E [L(wag

n−1￿i=1
γ(n−1)p+1 + 32HD2
b(n−1)

D2

i )] − L(w￿)) + 64HγL (w￿)(n−1)p

b

that

above

Notice

i=1 (E [L(wag

￿n−1

recursively bounds E [L(wag

the
of
i )] − L(w￿)). While unrolling the recursion all the way down to 2
does not help  we notice that for any w ∈W   L(w)− L(w￿) ≤ 12HD2 + 3L(w￿). Hence we
unroll the recursion to M steps and use this inequality for the remaining sum. Optimizing
over number of steps up to which we unroll and also optimizing over the choice of γ  we get
the bound 

n )] − L(w￿)

in terms

E [L(wag

n )] − L(w￿) ≤￿ 1648HD2L(w￿)

+ 348(6HD2+2L(w￿))

(b(n − 1))

p

p+1 + 32HD2
b(n−1)

b(n−1)
(n−1)p+1 + 36HD2
b(n−1)

+ 4HD2

b(n−1)
log(n)
(b(n−1))

p

2p+1

Using the p as given in the theorem statement  and few simple manipulations  gives the ﬁnal
bound.

4 Optimizing with Mini-Batches

To compare our two theorems and understand their implications  it will be convenient to
treat H and D as constants  and focus on the more interesting parameters of sample size
m  minibatch size b  and optimal expected loss L(w￿). Also  we will ignore the logarithmic
factor in Theorem 2  since we will mostly be interested in signiﬁcant (i.e. polynomial)
diﬀerences between the two algorithms  and it is quite possible that this logarithmic factor
is merely an artifact of our analysis. Using m = nb  we get that the bound for the SGD
algorithm is

and the bound for the accelerated gradient method we propose is

E [L( ¯w)] − L(w￿) ≤ ˜O￿￿ L(w￿)
n )]− L(w￿) ≤ ˜O￿￿ L(w￿)

1
√bn

bn

bn

+

+

1

m

n￿ = ˜O￿￿ L(w￿)
n2￿ = ˜O￿￿ L(w￿)

m

1

b

m￿  

+

(5)

√b
m

+

b2

m2￿ . (6)

+

+

E [L(wag

To understand the implication these bounds  we follow the approach described in [3  12] to
analyze large-scale learning algorithms. First  we ﬁx a desired suboptimality parameter ￿ 
which measures how close to L(w￿) we want to get. Then  we assume that both algorithms
are ran till the suboptimality of their outputs is at most ￿. Our goal would be to understand
the runtime each algorithm needs  till attaining suboptimality ￿  as a function of L(w￿) ￿  b .

To measure this runtime  we need to discern two settings here: a parallel setting  where we
assume that the mini-batch gradient computations are performed in parallel  and a serial
setting  where the gradient computations are performed one after the other. In a parallel
setting  we can take the number of iterations n as a rough measure of the runtime (note
that in both algorithms  the runtime of a single iteration is comparable). In a serial setting 
the relevant parameter is m  the number of data accesses.

To analyze the dependence on m and n  we upper bound (5) and (6) by ￿  and invert them
to get the bounds on m and n. Ignoring logarithmic factors  for the SGD algorithm we get

1

￿￿ L(w￿)

￿

n ≤

1
b

·

+ 1￿

m ≤

5

1

￿￿ L(w￿)

￿

+ b￿  

(7)

1

￿￿ L(w￿)

￿

n ≤

1
b

·

+

1
√b

+ √￿￿

1

￿￿ L(w￿)

￿

+ √b + b√￿￿ .

m ≤

(8)

and for the AG algorithm we get

First  let us compare the performance of these two algorithms in the parallel setting  where
the relevant parameter to measure runtime is n. Analyzing which of the terms in each
bound dominates  we get that for the SGD algorithm  there are 2 regimes  while for the AG
algorithm  there are 2-3 regimes depending on the relationship between L(w￿) and ￿. The
following two tables summarize the situation (again  ignoring constants):

SGD Algorithm
Regime

n

b ≤￿L(w￿)m L(w￿)
b ≥￿L(w￿)m

￿2b
1
￿

￿ ≤ L(w￿)2

￿ ≥ L(w￿)2

AG Algorithm
Regime

b ≤ L(w￿)1/4m3/4
b ≥ L(w￿)1/4m3/4

b ≤ L(w￿)m

L(w￿)m ≤ b ≤ m2/3

b ≥ m2/3

n

L(w￿)

￿2b
1√￿
L(w￿)

￿2b
1
￿√b
1√￿

From the tables  we see that for both methods  there is an initial linear speedup as a function
of the minibatch size b. However  in the AG algorithm  this linear speedup regime holds for
much larger minibatch sizes1. Even beyond the linear speedup regime  the AG algorithm

still maintains a √b speedup  for the reasonable case where ￿ ≥ L(w￿)2. Finally  in all

regimes  the runtime bound of the AG algorithm is equal or signiﬁcantly smaller than that
of the SGD algorithm.

We now turn to discuss the serial setting  where the runtime is measured in terms of m.
Inspecting (7) and (8)  we see that a larger size of b actually requires m to increase for both
algorithms. This is to be expected  since mini-batching does not lead to large gains in a
serial setting. However  using mini-batching in a serial setting might still be beneﬁcial for
implementation reasons  resulting in constant-factor improvements in runtime (e.g. saving
overhead and loop control  and via pipelining  concurrent memory accesses etc.). In that
case  we can at least ask what is the largest mini-batch size that won’t degrade the runtime
guarantee by more than a constant. Using our bounds  the mini-batch size b for the SGD
algorithm can scale as much as L/￿  vs. a larger value of L/￿3/2 for the AG algorithm.

Finally  an interesting point is that the AG algorithm is sometimes actually necessary to
obtain signiﬁcant speed-ups via a mini-batch framework (according to our bounds). Based
on the table above  this happens when the desired suboptimality ￿ is not much bigger then
L(w￿)  i.e. ￿ =Ω( L(w￿)). This includes the “separable” case  L(w￿) = 0  and in general
a regime where the “estimation error” ￿ and “approximation error” L(w￿) are roughly the
same—an arguably very relevant one in machine learning. For the SGD algorithm  the

critical mini-batch value￿L(w￿)m can be shown to equal L(w￿)/￿  which is O(1) in our
case. So with SGD we get no non-constant parallel speedup. However  with AG  we still
enjoy a speedup of at least Θ(√b)  all the way up to mini-batch size b = m2/3.

5 Experiments

We implemented both the SGD algorithm (Algorithm 1) and the AG algorithm (Algorithm
2  using step-sizes of the form γi = γip as suggested by Theorem 2) on two publicly-available
binary classiﬁcation problems  astro-physics and CCAT. We used the smoothed hinge loss
￿(w; x  y)  deﬁned as 0.5−yw￿x if yw￿x ≤ 0; 0 if yw￿x > 1  and 0.5(1−yw￿x)2 otherwise.
While both datasets are relatively easy to classify  we also wished to understand the algo-
rithms’ performance in the “separable” case L(w￿) = 0  to see if the theory in Section 4

1Since it is easily veriﬁed that ￿L(w￿)m is generally smaller than both L(w￿)1/4m3/4 and

L(w￿)m

6

astro-physics

CCAT

●

0
2
0
0
.
0

8
1
0
0
.
0

6
1
0
0
.
0

4
1
0
0
.
0

s
s
o
L

t
s
e
T

●

●

b=4
b=16
b=64

2
2
0
0
.
0

0
2
0
0
.
0

8
1
0
0
.
0

6
1
0
0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

p

b=16
b=64
b=256

●

●

●

0.2

0.4

0.6

0.8

1.0

p

Figure 1: Left: Test smoothed hinge loss  as a function of p  after training using the AG algorithm
on 6361 examples from astro-physics  for various batch sizes. Right: the same  for 18578 examples
from CCAT. In both datasets  margin violations were removed before training so that L(w￿) = 0.
The circled points are the theoretically-derived values p = ln b/(2 ln(n − 1)) (see Theorem 2).

holds in practice. To this end  we created an additional version of each dataset  where
L(w￿) = 0  by training a classiﬁer on the entire dataset and removing margin violations.

In all of our experiments  we used up to half of the data for training  and one-quarter
each for validation and testing. The validation set was used to determine the step sizes
η and γi. We justify this by noting that our goal is to compare the performance of the
SGD and AG algorithms  independently of the diﬃculties in choosing their stepsizes. In
the implementation  we neglected the projection step  as we found it does not signiﬁcantly
aﬀect performance when the stepsizes are properly selected.

In our ﬁrst set of experiments  we attempted to determine the relationship between the
performance of the AG algorithm and the p parameter  which determines the rate of in-
crease of the step sizes γi. Our experiments are summarized in Figure 5. Perhaps the most
important conclusion to draw from these plots is that neither the “traditional” choice p = 1 
nor the constant-step-size choice p = 0  give the best performance in all circumstances. In-
stead  there is a complicated data-dependent relationship between p  and the ﬁnal classiﬁer’s
performance. Furthermore  there appears to be a weak trend towards higher p performing
better for larger minibatch sizes b  which corresponds neatly with our theoretical predictions.

In our next experiment  we directly compared the performance of the SGD and AG. To do
so  we varied the minibatch size b while holding the total amount of training data (m = nb)
ﬁxed. When L(w￿) > 0 (top row of Figure 5)  the total sample size m is high and the
suboptimality ￿ is low (red and black plots)  we see that for small minibatch size  both
methods do not degrade as we increase b  corresponding to a linear parallel speedup. In
fact  SGD is actually overall better  but as b increases  its performance degrades more
quickly  eventually performing worse than AG. That is  even in the least favorable scenario
for AG (high L(w￿) and small ￿  see the tables in Sec. 4)  it does give beneﬁts with large
enough minibatch sizes. Further  we see that once the suboptimality ￿ is roughly equal to
L∗  AG signiﬁcantly outperforms SGD  even with small minibatches  agreeing with theory.
Turning to the case L(w￿) = 0 (bottom two rows of Figure 5)  which is theoretically more
favorable to AG  we see it is indeed mostly better  in terms of retaining linear parallel
speedups for larger minibatch sizes  even for large data set sizes corresponding to small
suboptimality values  and might even be advantageous with small minibatch sizes.

6 Summary

In this paper  we presented novel contributions to the theory of ﬁrst order stochastic convex
optimization (Theorems 1 and 2  generalizing results of [4] and [5] to be sensitive to L (w￿)) 

7

5
7
0

.

0

0
7
0

.

0

5
6
0

.

0

0
6
0

.

0

5
5
0

.

0

0
5
0

.

0

2
1
0

.

0

8
0
0

.

0

4
0
0
0

.

0
0
0
0

.

8
0
0
0

.

6
0
0
0

.

4
0
0
0

.

2
0
0
0

.

0
0
0
0

.

s
s
o
L

t
s
e
T

s
s
o
L

t
s
e
T

n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

t
s
e
T

astro-physics

CCAT

T=402207
T=25137
T=1571

T=31884
T=7796
T=1949

4
1

.

0

3
1

.

0

2
1

.

0

1
1

.

0

0
1

.

0

9
0

.

0

8
0

.

0

1

5

10

50 100

500

5000

1

10

100

1000

10000

1

5

10

50 100

500

T=402207
T=25137
T=1571

1

10

100

1000

10000

T=402207
T=25137
T=1571

T=31884
T=7796
T=1949

T=31884
T=7796
T=1949

0
3
0

.

0

0
2
0

.

0

0
1
0
0

.

0
0
0
0

.

0
2
0
0

.

5
1
0
0

.

0
1
0
0

.

5
0
0
0

.

0
0
0
0

.

1

5

10

50 100

500

1

10

b

1000

10000

100

b

Figure 2: Test loss on astro-physics and CCAT as a function of mini-batch size b (in log-scale) 
where the total amount of training data m = nb is held ﬁxed. Solid lines and dashed lines are for
SGD and AG respectively (for AG  we used p = ln b/(2 ln(n− 1)) as in Theorem 2). The upper row
shows the smoothed hinge loss on the test set  using the original (uncensored) data. The bottom
rows show the smoothed hinge loss and misclassiﬁcation rate on the test set  using the modiﬁed
data where L(w￿) = 0. All curves are averaged over three runs.

developed a novel step size strategy for the accelerated method that we used in order to
obtain our results and we saw works well in practice  and provided a more reﬁned analysis
of the eﬀects of minibatching which paints a diﬀerent picture then previous analyses [4  1]
and highlights the beneﬁt of accelerated methods.

A remaining open practical and theoretical question is whether the bound of Theorem 2
is tight. Following [5]  the bound is tight for b = 1 and b → ∞  i.e. the ﬁrst and third
terms are tight  but it is not clear whether the 1/(√bn) dependence is indeed necessary.
It would be interesting to understand whether with a more reﬁned analysis  or perhaps
diﬀerent step-sizes  we can avoid this term  whether an altogether diﬀerent algorithm is
needed  or whether this term does represent the optimal behavior for any method based on
b-aggregated stochastic gradient estimates.

8

References
[1] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. Technical report 

arXiv  2011.

[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods

for convex optimization. Operations Research Letters  31(3):167 – 175  2003.

[3] L. Bottou and O. Bousquet. The tradeoﬀs of large scale learning. In NIPS  2007.

[4] O. Dekel  R. Gilad Bachrach  O. Shamir  and L. Xiao. Optimal distributed online

prediction using mini-batches. Technical report  arXiv  2010.

[5] G. Lan. An optimal method for stochastic convex optimization. Technical report 

Georgia Institute of Technology  2009.

[6] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609 
2009.

[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate

of convergence o(1/k2). Doklady AN SSSR  269:543–547  1983.

[8] Y. Nesterov.

Smooth minimization of non-smooth functions. Math. Program. 

103(1):127–152  2005.

[9] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical

Programming  120(1):221–259  August 2009.

[10] O. Shamir O. Dekel  R. Gilad-Bachrach and L. Xiao. Optimal distributed online pre-

diction. In ICML  2011.

[11] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: primal estimated

sub-gradient solver for SVM. Math. Program.  127(1):3–30  2011.

[12] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training

set size. In ICML  2008.

[13] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In NIPS 

2010.

[14] S.Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis 

Hebrew University of Jerusalem  2007.

[15] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization.

Submitted to SIAM Journal on Optimization  2008.

[16] L. Xiao. Dual averaging methods for regularized stochastic learning and online opti-

mization. Journal of Machine Learning Research  11:2543–2596  2010.

9

,Nicholas Ruozzi
Tony Jebara
Cheng Li
Felix Wong
Zhenming Liu
Varun Kanade