2019,More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation,Current state-of-the-art models for video action recognition are mostly based on expensive 3D ConvNets. This results in a need for large GPU clusters to train and evaluate such architectures. To address this problem  we present an lightweight and memory-friendly architecture for action recognition that performs on par with or better than current architectures by using only a fraction of resources.
The proposed architecture is based on a combination of a deep subnet operating on low-resolution frames with a compact subnet operating on high-resolution frames  allowing for high efficiency and accuracy at the same time. We demonstrate that our approach achieves a reduction by 3~4 times in FLOPs and ~2 times in memory usage compared to the baseline. This enables training deeper models with more input frames under the same computational budget.  To further obviate the need for large-scale 3D convolutions  a temporal aggregation module is proposed to model temporal dependencies in a video at very small additional computational costs. Our models achieve strong performance on several action recognition benchmarks including Kinetics  Something-Something and Moments-in-time. The code and models are available at \url{https://github.com/IBM/bLVNet-TAM}.,More Is Less: Learning Efﬁcient Video

Representations by Big-Little Network and Depthwise

Temporal Aggregation

Quanfu Fan† 1  Chun-Fu (Richard) Chen† 2  Hilde Kuehne1  Marco Pistoia2  David Cox1

†: Equal contribution

1MIT-IBM Waston AI Lab  Cambridge  MA 02142  USA

2IBM T.J. Waston Research Center  Yorktown Heights  NY 10598  USA

{qfan  chenrich  pistoia}@us.ibm.com  {kuehne  david.d.cox}@ibm.com

Abstract

Current state-of-the-art models for video action recognition are mostly based on
expensive 3D ConvNets. This results in a need for large GPU clusters to train and
evaluate such architectures. To address this problem  we present an lightweight
and memory-friendly architecture for action recognition that performs on par with
or better than current architectures by using only a fraction of resources. The
proposed architecture is based on a combination of a deep subnet operating on
low-resolution frames with a compact subnet operating on high-resolution frames 
allowing for high efﬁciency and accuracy at the same time. We demonstrate that our
approach achieves a reduction by 3 ∼ 4 times in FLOPs and ∼ 2 times in memory
usage compared to the baseline. This enables training deeper models with more
input frames under the same computational budget. To further obviate the need for
large-scale 3D convolutions  a temporal aggregation module is proposed to model
temporal dependencies in a video at very small additional computational costs.
Our models achieve strong performance on several action recognition benchmarks
including Kinetics  Something-Something and Moments-in-time. The code and
models are available at https://github.com/IBM/bLVNet-TAM.

1

Introduction

Current state-of-the-art approaches for video action recognition are based on convolutional neural
networks (CNNs). These include the best performing 3D models  such as I3D [1] and ResNet3D [2] 
and some effective 2D models  such as Temporal Relation Networks (TRN) [3] and Temporal Shift
Modules (TSM) [4]. A CNN-based model usually considers a sequence of frames as input  obtained
through either uniform or dense sampling from a video [1  5]. In general  Longer input sequences
yield better recognition results. However  one problem arising for a model requesting more input
frames is that the GPU resources required for training and inference also signiﬁcantly increase in
both memory and time. For example  the top-performing I3D models [1] on the Kinetics [6] dataset
were trained with 64 frames on a cluster of 32 GPUs  and the non-local network [7] even uses 128
frames as input. Another problem for action recognition is the lack of effective methods for temporal
modeling when moving away from 3D spatiotemporal convolutions. While 2D convolutional models
are more resource-friendly than their 3D counterparts  they lack expressiveness over time and thus
cannot take much beneﬁt from richer input data.
In this paper  we present an efﬁcient and memory-friendly spatio-temporal representation for action
recognition  which enables training of deeper models while allowing for more input frames. The ﬁrst
part of our approach is inspired by the Big-Little-Net architecture (bLNet [8]). We propose a new

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

video architecture that has two network branches with different complexities: one branch processing
low-resolution frames in a very deep subnet  and another branch processing high-resolution frames
in a compact subnet. The two branches complement each other through merging at the end of each
network layer. With such a design  our approach can process twice as many frames as the baseline
model without compromising efﬁciency. We refer to this architecture as “Big-Little-Video-Net”
(bLVNet).
In light of the limited ability of capturing temporal dependencies in bLVNet  we further develop an
effective method to exploit temporal relations across frames by a so called “Depthwise Temporal Ag-
gregation Module” (TAM). The method enables the exchange of temporal information between frames
by weighted channel-wise aggregation. This aggregation is made learnable with 1×1 depthwise
convolution  and implemented as an independent network module. The temporal aggregation module
can be easily integrated into the proposed network architecture to progressively learn spatio-temporal
patterns in a hierarchical way. Moreover  the module is extremely compact and adds only negligible
computational costs and parameters to bLVNet.
Our main contributions lie in the following two interconnected aspects: (1) We propose a lightweight
video architecture based on dual-path network to learn video features  and (2) we develop a temporal
aggregation module to enable effective temporal modeling without the need for computationally
expensive 3D convolutions.
We evaluate our approach on the Kinetics-400 [6]  Something-Something [9] and Moments-in-
time [10] datasets. The evaluation shows that bLVNet-TAM successfully allows us to train action-
classiﬁcation models with deeper backbones (i.e.  ResNet-101) as well as more (up to 64) input
frames  using a single compute node with 8 Tesla V100 GPUs. Our comprehensive experiments
demonstrate that our approach achieves highly competitive results on all datasets while maintaining
efﬁciency. Especially  it establishes a new state-of-the-art result on Something-Something and
Moments-in-time by outperforming previous approaches in the literature by a large margin.

2 Related Work

Activity classiﬁcation has always been a challenging research topic  with ﬁrst attempts reaching back
by almost two decades [11]; deep-learning architectures nowadays achieve tremendous recognition
rates on various challenging tasks  such as Kinetics [1]  ActivityNet [12]  or Thumos [13].
Most successful architectures in the ﬁeld are usually based on the so-called two-stream model [14] 
processing a single RGB frame and optical-ﬂow input in two separate CNNs with a late fusion in the
upper layers. Over the last years  many approaches extend this idea by processing a stack of input
frames in both streams  thus extending the temporal window of the architecture form 1 to up to 128
input frames per stream. To further capture the temporal correlation in the input over time  those
architectures usually make use of 3D convolutions as  e.g.  in I3D [1]  S3D [15]  and ResNet3D [2] 
usually leading to a large-scale parameter space to train.
Another way to capture temporal relations has been proposed by [5]  [3]  and [4]. Those architectures
mainly build on the idea of processing videos in the form of multiple segments  and then fusing them
at the higher layers of the networks. The ﬁrst approach with this pattern was the so-called Temporal
Segment Networks (TSN) proposed by Wang et al. [5]. The idea of TSN has been extended by
Temporal Relation Networks (TRN) [3]  which apply the idea of relational networks to the modeling
of temporal relations between observations in videos. Another approach for capturing temporal
contexts has been proposed by Temporal Shift Modules (TSM) [4]. This approach shifts part of the
channels along the temporal dimension  thereby allowing for information to be exchanged among
neighboring frames. More complex approaches have been tried as well  e.g. in the context of non-
local neural networks [7]. Our temporal aggregation module is based on depthwise 1×1 convolutions
to capture temporal dependencies across frames effectively.
Separate convolutions are considered in approaches such as [15  16] to reduce costly computation in
3D convolutional models. More recently  SlowFast Network [17] uses a dual-pathway network to
process a video at both slow and fast frame rates. The fast pathway is made lightweight  similar to
Little Net in our proposed architecture. However  our approach reduces computation based on both a
lightweight architecture and low image resolution. Furthermore  the recent work Timeception [18]
applies the concept of “Inception" to temporal domain for capturing long-range temporal dependencies

2

in a video. The Timeception layers involve group convolutions at different time scales while our
TAM layers only use depthwise convolution. As a result  the Timeception has signiﬁcantly more
parameters than the TAM (10% vs. 0.1% of the total model parameters).

3 Our Approach

We aim at developing efﬁcient and effective video representations for video understanding. To address
the computational challenge imposed by the desired long input to a model  we propose a new video
architecture based on the Big-Little network (bLNet) [8] for learning video features.We ﬁrst give a
brief recap of bLNet in Section 3.1. We then show  in Section 3.2  how to extend bLNet to an efﬁcient
video architecture that allows for seeing more frames with less computation and memory. An example
of the proposed network architecture can be found in the supplementary material (Section A).
To make temporal modeling more effective in our approach  we further develop a temporal aggregation
module (TAM) to capture short-term as well as long-term temporal dependencies across frames. Our
method is implemented as a separate network module and integrated with the proposed architecture
seamlessly to learn a hierarchical temporal representation for action recognition. We detail this
method in Section 3.3.

3.1 Recap of Big-Little Network

The Big-Little Net  abbreviated as bLNet in [8]  is a CNN architecture for learning strong feature
representations by combining multi-scale image information. The bLNet processes an image at
different resolutions using a dual-path network  but with low computational loads based on a clever
design. The key idea is to have a high-complexity subnet (Big-Net) along with a low-cost one
(Little-Net) operate on the low-scale and high-scale parts of an image in parallel. By such a design 
the two subnets learn features complementary to each other while using less computation. The two
branches are merged at the end of each network layer to fuse the low-scale and high-scale information
so as to form a stronger image representation. The bLNet approach demonstrates improvement of
model efﬁciency and performance on both object and speech recognition  using popular architectures
such as ResNet  ResNeXt and SEResNeXt. More details on bLNet can be found in the original
paper. In this work  we mainly adopt bLResNet-50 and bLResNet-101 as backbone for our proposed
architecture.

3.2 Big-Little Video Network as Video Representation

We describe our architecture in the context of 2D convolutions. However our approach is not speciﬁc
to 2D convolutions and potentially extendable to any architecture based on 3D convolutions.

Figure 1: Different architectures for action recognition. a) TSN [5] uses a shared CNN to process
each frame independently  so there is no temporal interaction between frames. b) TSN-bLNet is a
variant of TSN that uses bLNet [8] as backbone. It is efﬁcient  but still lacks temporal modeling. c)
bLVNet feeds odd and even frames separately into different branches in bLNet. The branch merging
at each layer (local fusion) captures short-term temporal dependencies between adjacent frames. d)
bLVNet-TAM includes the proposed aggregation module  represented as a red box  which further
empowers bLVNet to model long-term temporal dependencies across frames (global fusion).

3

!!!"#$%&$%’%()(*(+()(*!!(+( !!(-(.!!!!!"#$%&$%’%!()!!!(*!!!(+!!"#$%&$%’%()(*!!(-(.!!(+( !!!!!"#$%&$%’%"#$%&’(#$%&’)(*’+ -#$(*.’+ /#$(*.’+ )%01!!/&0’1234$#5&6#5&42(7&341#8214(’%9#$6#5&42(7&3401#:214(’%9#$;90<6&7=9771&<6&7;32$8>4?&309$0!3&59879#$(9@32?&The approach of Temporal Segment Networks (TSN) [5] provides a generic framework for learning
video representations. With a shared 2D ConvNet as backbone  TSN performs frame-level predictions
and then aggregates the results into a ﬁnal video-level prediction (Fig. 1a)). The framework of TSN is
efﬁcient and has been successfully adopted by some recent approaches for action recognition such as
TRN [3] and TSM [4]. Given its efﬁciency  we also choose TSN as the underlying video framework
for our work.
Let F = {ft|t = 1··· n} be a set of sampled input frames from a video. We divide F into two
groups  namely odd frames Fodd = {fk ∈ F| mod (k  2) (cid:54)= 0} at half of the input image resolution 
and even frames Feven = {fk ∈ F| mod (k  2) = 0} at the input image resolution. For convenience 
from now on  Fodd is referred to as big frames and Feven as little frames. Note that big branch can
take either of a pair of frames as input and the other frame goes to the little branch.
In TSN  all input frames are ordered as a batch of size n  where the tth element corresponds to the
tth frame. We denote the input and output feature maps of the tth frame at the kth layer of the model
by xk
The bLNet can be directly plugged into TSN as the backbone network for learning video-level
representation. We refer to this architecture as TSN-bLNet to differentiate it from the vanilla TSN
(Fig. 1b)). This network fully enjoys the efﬁciency of bLNet  cutting the computational costs down
by 1.6 ∼ 2 times according to [8]. Mathematically  the output yt can be written as

t ∈ RC×W×H  respectively. Whenever possible  we omit k for clarity.

t ∈ RC×W×H and yk

yt = F(netB([xt]1/2) + netL(xt)  θt).

(1)
Here [·]s is an operator scaling a tensor up or down by a factor of s in the spatial domain; netB and
netL are the Big-Net and Little-Net in the bLNet aforementioned; and θt are the model parameters.
Following [8]  F indicates an additional residual block applied after merging the big and little
branches to stabilize and enhance the combined feature representation.
The architecture described above only learns features from a single frame  so there are no interactions
between frames. Alternatively  we can feed the odd and even frames separately into the big and little
branches so that each branch obtains complementary information from different frames. This idea is
illustrated in Fig. 1c) and the output yt in this case can be expressed by

(cid:26)F(netB((cid:98)xt(cid:99)1/2) + netL(xt+1)  θt) 

yt =

yt−1 

if mod (t  2) (cid:54)= 0
otherwise

(2)

While the modiﬁcation proposed above is simple  it leads to a new video architecture  which is
called Big-Little-Video-Net  or bLVNet for short. The bLVNet makes two distinct differences from
TSN-bLNet. Firstly  without increasing any computation  it can take input frames two times as many
as TSN-bLNet. We shall demonstrate the beneﬁt of leveraging more frames for temporal modeling in
Section 4. Furthermore  the bLVNet has 1.5 ∼ 2.0× fewer FLOPs than TSN while seeing frames
twice as many as TSN  thanks to the efﬁciency of the dual-path network. Secondly  the merging of
the two branches in bLVNet now happens on two different frames carrying temporal information.
We call this type of temporal interaction by local fusion  since it only captures temporal relations
between two adjacent frames. In spite of that  local fusion gives rise to a signiﬁcant performance
boost for recognition  as shown later in Section 4.3.

3.3 Temporal Aggregation Module

Temporal modeling is a challenging problem for video understanding. Theoretically  adding a
recurrent layer such as LSTM [19] on top of a 2D ConvNet seems like a promising means to capture
temporal ordering and long-term dependencies in actions. Nonetheless  such approaches are not
practically competent with 3D ConvNets [1]  which use spatio-temporal ﬁlters to learn hierarchical
feature representations. One issue with 3D models is that they are heavy in parameters and costly
in computation  making them hard to train. Even though some approaches like S3D [15] and
R(2+1)D [16] alleviates this issue by separating a 3D convolution ﬁlter into a 2D spatial component
followed by a 1D temporal component  they are in general still more expensive than 2D ConvNet
models.
With the efﬁcient bLVNet architecture described above  our goal is to further improve its spatio-
temporal representation by effective temporal modeling. The local fusion in bLVNet only exploits
temporal relations between neighbored frames. To address this limitation  we develop a method

4

to capture short-term as well as long-term dependencies across frames. Our basic idea is to fuse
temporal information at each time instance by weighted channel-wise aggregation. As detailed below 
this idea can be efﬁciently implemented as a network module to progressively learn spatio-temporal
patterns in a hierarchical way.
Let yt be the output (i.e. neural activation) of the tth frame ft at a layer of the network (see Eq. 2).
To model the temporal dependencies between ft and its neighbors  we aggregate the activations of
all the frames within a temporal range r around ft. A weight is learned for each channel of the
activations to indicate its relevance. Speciﬁcally  the aggregation results can be written as

j=(cid:98)r/2(cid:99)(cid:88)

j=−(cid:98)r/2(cid:99)

ˆyt = ReLU (

wj ⊗ yt+j) 

(3)

where ⊗ indicates the channel-wise multiplication and wj ∈ RC is the weights. The ⊗ is deﬁned
as: for a vector v = [v1 v2 ··· vC] and a tensor M = [m1 m2 ··· mC] with C feature channels 
v ⊗ M = [v1 ∗ m1 v2 ∗ m2 ··· vC ∗ mC].
We implement the temporal aggregation as a network module (Fig. 2). It involves three steps as
follows 

r × n;

1. apply 1×1 depthwise convolution r times to n input tensors to form an output matrix of size
2. shift the ith row left (or right) by |i − (cid:98)r/2(cid:99)| positions if i > (cid:98)r/2(cid:99) (or i ≤ (cid:98)r/2(cid:99)) and if

needed  pad leading or trailing zero tensors in the front or at the end;

3. perform temporal aggregation along the column to generate the output.

The aggregation module(TAM)  highlighted as a red box in Fig. 1d)  is inserted as a separate layer after
the local temporal fusion in the bLVNet  resulting in the ﬁnal bLVNet-TAM architecture. Obviously
none of the steps in the implementation above involve costly computation  so the module is fairly
fast. A node in the network initially only sees r − 1 neighbors. As the network goes deeper  the
amount of context that the node involves in the input grows quickly  similar to how the receptive
ﬁeld of a neuron is enlarged in a CNN. In such a manner  long-range temporal dependencies are thus
potentially captured. For this reason  the temporal aggregation is also called global temporal fusion
here  as opposed to the local temporal fusion discussed above.
The work of TSM [4] has also applied temporal shifting to swap feature channels between neighboring
frames. In such a case  TSM can be treated as a special case of our method where the weights are
empirically set rather than learned from data. In Section 4.3  we demonstrate that the proposed TAM
is more effective than TSM for temporal modeling under different video architectures. TAM is also
related to S3D [15] and R(2+1)D [16] in that TAM is independent of spatial convolutions. However 
TAM is based on depthwise convolution  thus has fewer parameters and less computation than S3D
and R(2+1)D.
The TAM can also be integrated into 3D convolutions such as C3D [20] and I3D [1] to further enhance
the temporal modeling capability that already exists in these models. Due to the difference in how
temporal data is presented between 2D-based and 3D-based models  the temporal shifting now needs
to operate on feature channels within a tensor instead of on tensors themselves.

Figure 2: Temporal aggregation module (TAM). The TAM takes as input a batch of tensors  each of
which is the activation of a frame  and produces a batch of tensors with the same order and dimension.
The module consists of three operations: 1) 1×1 depthwise convolutions to learn a weight for each
feature channel; 2) temporal shifts (left or right direction indicated by the smaller arrows; the white
cubes are padded zero tensors.); and 3) aggregation by summing up the weighted activations from 1).

5

!"#!"#$%&’&()*#%+ -.*/0"123*4#0567(8+-9%-":3*4#0567(;::5*:6%-0"4 Experiments

4.1 Experimental Setup

Datasets. We evaluate our approach on three large-scale datasets for video recognition  including
the widely used Something-Something (Version 1 and Version 2) [9]  Kinetics-400 [6] and the
recent Moments-in-time dataset [10]. They are herein referred to as SS-V1  SS-V2  Kinetics-400 and
Moments  respectively.
Something-Something is a dataset containing videos of 174 types of predeﬁned human-object
interactions with everyday objects. The version 1 and 2 include 108k and 220k videos  respectively.
This dataset focuses on human-object interactions in a rather simple setup with no scene contexts to be
exploited for recognition. Instead temporal relationships are as important as appearance for reasoning
about the interactions. Because of this  the dataset serves as a good benchmark for evaluating the
efﬁcacy of temporal modeling  such as proposed in our approach.
Kinetics-400 [6] has emerged as a standard benchmark for action recognition after UCF101 [21] and
HMDB [22]  but on a signiﬁcantly larger scale. The dataset consists of 240k training videos and 20k
validation videos  with each video trimmed to around 10 seconds. It has a total of 400 human action
categories.
Moments-in-time [10] is a recent collection of one million labeled videos  involving actions from
people  animals  objects or natural phenomena. It has 339 classes and each video clip is trimmed to 3
seconds long.
Data Augmentation. During training  we follow the data augmentation used in TSN [5] to augment
the video with different sizes spatially and ﬂip the video horizontally with 50% probability. Further-
more  since our models are ﬁnetuned on pretrained ImageNet  we normalize the data with the mean
and standard deviation of the ImageNet images. The model input is formed by uniform sampling 
which ﬁrst divides a video into n uniform segments and then selects one random frame from each
segment as the input.
During inference  we resize the smaller side of an image to 256 and then crop a centered 224×224
region. The center frame of each segment in uniform sampling is picked as the input. On Something-
Something and Moments  our results are based on the single-crop and single-clip setting. On
Kinetics-400  we use the common practice of multi-crop and multi-clip for evaluation.
Training Details. Since all the three datasets are large-scale  we train the models in a progressive
way. For each type of backbone (for example  bLResNet-50)  we ﬁrst ﬁnetune a base model on
ImageNet with a minimum input length (i.e. 8×2 in our case) using 50 epochs. We adopt the Nesterov
momentum optimizer with an initial weight of 0.01  a weight decay of 0.0005 and a momentum of
0.9. We then ﬁnetune a new model with longer input (for example  16×2) on top of the corresponding
base model  but with 25 epochs only. In this case  the initial learning rate is set to 0.01 on Something-
Something and 0.005 on Kinetics and Moments. The learning rate is decreased by a factor of 10 at
the 10-th and 20-th epoch  respectively.
This strategy allows to signiﬁcantly reduce the training time needed for all the models evaluated in
our experiments. All our models were trained on a server with 8 GPU cards and a total of 128G GPU
memory. We set the total batch size to 64 whenever possible. For models that require more memory
to train  we adjust the batch size accordingly to the maximum number allowed.

4.2 Main Results

Something-Something. We ﬁrst report our results on the validation set of the Something-Something
datasets in Table 1 and Table 2. With a moderately deep backbone bLResNet-50  our approach outper-
forms all 3D models on SS-V1 while using much fewer input frames (8×2) and being substantially
more efﬁcient. TSM [4] was the previously best approach on Something-Something. Under the same
backbone (i.e. ResNet-50)  our approach is better than TSM on both SS-V1 and SS-V2 while being
more efﬁcient (i.e our 8x2 model has 1.4 times fewer FLOPs than a 8-frame TSM model).
When empowered with a stronger backbone bLResNet-101  our approach achieves even better results
at 32×2 frames (53.1% top-1 accuracy on SS-V1  and 65.2% on SS-V2)  establishing a new state-
of-the-art on Something-Something. Notably  these results while based on RGB information only 

6

Table 1: Recognition Accuracy of Various Models on Something-Something-V1 (SS-V1).

Model

Backbone

Pretrain

Frames Modality

Param (106)

FLOPs (109)

Val

Test

Top-1 (%) Top-5 (%) Top-1 (%)

I3D [1]

NL I3D + GCN [23]

S3D [15]

ECO-LiteEn [24]

TSN [5]
TRN [3]

TSM [4]

bLVNet-TAM

Inception
ResNet-50
Inception

BNInception+ResNet18

BNInception
BNInception
BNInception
ResNet-50
ResNet-50
ResNet-50
bLResNet-50
bLResNet-50
bLResNet-101
bLResNet-101
bLResNet-101
bLResNet-101

ImageNet
ImageNet
ImageNet
ImageNet
ImageNet
ImageNet
ImageNet
Kinetics
Kinetics
Kinetics
ImageNet

SS-V1

ImageNet

SS-V1
SS-V1
SS-V1

64

32+32

64
92
8
8
8+8
8
16

16+16
8×2
16×2
8×2
16×2
24×2
32×2

RGB
RGB
RGB
RGB
RGB
RGB

RGB+Flow

RGB
RGB

RGB+Flow

RGB
RGB
RGB
RGB
RGB
RGB

12.7
303
8.77
150
10.7
18.3
−
24.3
24.3
−
25.0
25.0
40.2
40.2
40.2
40.2

111
62.2
66
267
16
16
−
33
65
−
23.8
47.7
32.1
64.3
96.4
128.6

45.8
46.1
47.3
46.4
19.5
34.4
42.0
45.6
47.2
52.6
46.4
48.4
47.8
49.6
52.2
53.1

76.5
76.8
78.1
−
−
−
−
74.2
77.1
81.9
76.6
78.8
78.0
79.8
81.8
82.9

27.2
−
−
42.3
−
33.6
40.7

46.0
50.7
−
−
−
−
−
48.9

Table 2: Recognition Accuracy of Various Models on Something-Something-V2 (SS-V2).

Model

Backbone

Pretrain

Frames Modality

Param (106)

FLOPs (109)

Val

Test

Top-1 (%) Top-5 (%) Top-1 (%) Top-5 (%)

TRN [3]

TSM [4]

bLVNet-TAM

BNInception
BNInception
ResNet-50†
ResNet-50†
ResNet-50
bLResNet-50
bLResNet-50
bLResNet-101
bLResNet-101
bLResNet-101
bLResNet-101
bLResNet-101∗

ImageNet
ImageNet
Kinetics
Kinetics
Kinetics
ImageNet

SS-V2

ImageNet

SS-V2
SS-V2
SS-V2
SS-V2

8
8
8
16
−
8×2
16×2
8×2
16×2
24×2
32×2
32×2

RGB

RGB+Flow

RGB
RGB

RGB+Flow

RGB
RGB
RGB
RGB
RGB
RGB

RGB+Flow

18.3
36.6
24.3
24.3
−
25.0
25.0
40.2
40.2
40.2
40.2
−

16
32
33
65
−
23.8
47.7
32.1
64.3
96.4
128.6
−

48.8
55.5
58.9
61.4
66.0
59.1
61.7
60.2
61.9
64.0
65.2
68.5

77.6
83.1
85.5
87.0
90.5
86.0
88.1
87.1
88.4
89.8
90.3
91.4

50.9
56.2
−
−
66.6
−
−
−
−
−
−
67.1

79.3
83.2
−
−
91.3
−
−
−
−
−
−
91.4

† : using their pretrained models and code to evaluate under the 1-crop and 1-clip setting for fair comparison
∗ : model ensemble of RGB and Flow model  each is evaluated with 3 crops and 10 clips and uses 256 as the shorter side.

are superior to those obtained from the best two-stream models at no more computational cost. This
strongly demonstrates the effectiveness of our approach for temporal modeling. We further evaluated
our models on the test set of Something-Something. Our results are consistently better than the best
results reported by the other approaches in comparison including 2-stream models.
Kinetics-400. Kinetics-400 is one of the most popular benchmarks for action recognition. Currently
the best-performed models on this dataset are all based on 3D Convolutions. However  it has been
shown in the literature that temporal ordering in this dataset does not seem to be as crucial as RGB
information for recognition. For example  as experimented in S3D [15]  the model trained on normal
time-order data performs well on the time-reversed data on Kinetics. In accordance to this  our
approach (3 crops and 3 clips) mainly performs on par with or better than the current large-scale
architectures  but without outperforming them as clearly as on the Something-Something datasets 
where the temporal relations are more essential for an overall understanding of the video content.

Table 3: Recognition Accuracy of Various Models on Kinetics-400 (RGB-only).

Net

Backbone

FLOPs (109) Top-1 (%) Top-5 (%)

STC [25]

ARTNet [26]

C3D [26]
I3D [1]
S3D [15]

R(2+1)D [16]

SlowFast-4×16 [17]

TSN [5]

TSM-8 [4]
TSM-16 [4]

bLVNet-TAM-8×2
bLVNet-TAM-16×2
bLVNet-TAM-24×2

ResNeXt-101

ResNet-18
ResNet-18
Inception
Inception
ResNet-34
ResNet-50
InceptionV3

ResNet-50
ResNet-50
bLResNet-50
bLResNet-50
bLResNet-50

ECO-LiteEn [24]

BNInception+ResNet18

−

−
−

23.5×250
19.6×250
108×N/A

267

36.1×30
142.8×10
42.7×30
85.4×30
31.1×9
62.3×9
93.4×9

68.7
69.2
65.6
71.1
72.2
72.0
75.6
72.5
70.7
74.1
74.7
71.0
72.0
73.5

88.5
88.3
85.7
89.3
90.6
90.0
92.1
−
-
91.2
−
89.8
90.6
91.2

Pretrain
None
None
None

ImageNet
ImageNet

None
None

ImageNet
ImageNet
ImageNet
ImageNet
ImageNet
Kinetics
Kinetics

7

Table 4: Recognition Accuracy of Various Models on Moments-in-time.

Net

Backbone

Pretrain

Frames Modality
Audio
RGB

−
16

RGB+Flow

16+16

16
16
−
8×2
16×2

RGB
RGB
−
RGB
RGB

Top-1 (%) Top-5 (%)

7.60
24.1
25.3
28.3
29.5
31.2
31.2
31.4

18.0
49.1
50.1
53.9
56.1
57.7
58.3
59.3

SoundNet [10]

TSN [10]
TSN [10]
TRN [10]
I3D [10]

BNInception
BNInception

Inception
ResNet-50

−

−

ImageNet

ImageNet

−
−
−
−

Ensemble [10]
bLVNet-TAM bLResNet-50

ImageNet
bLResNet-50 Moments

Figure 3: Number of input frames v.s. model accuracy and memory usage. (a) A longer input sequence
yields better recognition in our proposed bLVNet-TAM on the Something-Something dataset [9]  but
not in TSN [5] due to limited temporal modeling ability. (b) Compared to TSN  bLVNet-TAM reduces
memory usage by ∼2 times under the same number of input frames.

Moments. We ﬁnally evaluate the proposed architecture on the Moments dataset [10]  a large-scale
action dataset with about three times more training samples than Kinetics-400. Since Moments
is relatively new and results reported on it are limited  we only compare our results with those
reported in the Moments paper [10]. As can been seen from Table 4  our approach outperforms all
the single-stream models as well as the ensemble one. We hope our models provide stronger baseline
results for future reference on this challenging dataset.
It is also noted that our model trained with 16 × 2 frames only produces slightly better top-1 accuracy
than the model trained with 8 × 2 frames. We speculate that this has to do with the fact that the
Moments clips are only as short as 3 seconds and that there is only a limited impact in choosing a
ﬁner temporal granularity on this dataset.

4.3 Ablation Studies

In this section  we conduct ablation studies to provide more insights about our main ideas.
Is temporal aggregation effective?. We validate the efﬁcacy of the proposed temporal aggregation
module (TAM)  which is considered as a global fusion method (Section 3.3). Local fusion here is
referred to the branch merging in the dual path network (Section 3.2). We compare TAM with the
temporal shift module used in TSM [4] in Table 5 under two different video architectures: TSN and
bLVNet proposed in this work. TAM demonstrates clear advantages over TSM  outperforming TSM
by over 2% under both architectures. Interestingly  with the here proposed bLVNet baseline with
local temporal fusion almost doubles the performance of a TSN baseline  improving the accuracy
from 17.4% to 33.6%. On top of that  TAM boosts the performance by another 13% in both cases 
suggesting that TAM is complementary to local fusion. This further conﬁrms the signiﬁcance of
temporal reasoning on the Something-Something dataset.
Does seeing more frames help?. One of the main contribution of this work is an efﬁcient video
architecture that makes it possible to train deeper models with more input frames using moderate
GPU resources. Fig. 3a) shows consistent improvement of our approach on SS-V1 as the number of
input frames increases. A similar trend in our results can be observed on Kinetics-400 in Table 3. On
the other hand  the almost ﬂattened line from TSN suggests that a model without effective temporal
modeling cannot take much of the beneﬁt from longer input frames.

8

a)b)Memory Usage. We compare the memory usage between our approach based on bLResNet-50 and
TSN based on ResNet-50. As shown in Fig. 3b)  our approach is more memory friendly than TSN 
achieving a saving of ∼2 times at the same number of input frames. The larger batch size allowed for
training under the same computational budget is critical for our approach to obtain better models and
reduce training time.

5 Conclusion

We presented an efﬁcient and memory-friendly
video architecture for learning video represen-
tations. The proposed architecture allows for
twice as many input frames as the baseline while
using less computation and memory. This en-
ables training of deeper models with richer input
under the same GPU resources. We further de-
veloped a temporal aggregation method to cap-
ture temporal dependencies effectively across
frames. Our models achieve strong performance on several action recognition benchmarks  and
establish a state-of-the-art on the Something-Something dataset.

Backbone
ResNet-50
ResNet-50
ResNet-50
bLResNet-50
bLResNet-50
bLResNet-50

Table 5: Temporal Modeling on SS-V1.

Local Fusion Global Fusion Top-1 (%)

Net

TSN

bLVNet

None
None
None
(cid:88)
(cid:88)
(cid:88)

None
TSM
TAM
None
TSM
TAM

17.4
43.4
46.1
33.6
44.2
46.4

References
[1] Joao Carreira and Andrew Zisserman. Quo vadis  action recognition? a new model and the kinetics dataset.
In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 6299–6308 
2017.

[2] Kensho Hara  Hirokatsu Kataoka  and Yutaka Satoh. Learning spatio-temporal features with 3d residual
networks for action recognition. In Proceedings of the IEEE International Conference on Computer Vision 
pages 3154–3160  2017.

[3] Bolei Zhou  Alex Andonian  Aude Oliva  and Antonio Torralba. Temporal relational reasoning in videos.

In Proceedings of the European Conference on Computer Vision (ECCV)  pages 803–818  2018.

[4] Ji Lin  Chuang Gan  and Song Han. Temporal shift module for efﬁcient video understanding. arXiv

preprint arXiv:1811.08383  2018.

[5] Limin Wang  Yuanjun Xiong  Zhe Wang  Yu Qiao  Dahua Lin  Xiaoou Tang  and Luc Van Gool. Temporal
segment networks: Towards good practices for deep action recognition. In Proceedings of the European
Conference on Computer Vision (ECCV). Springer  2016.

[6] Will Kay  Joao Carreira  Karen Simonyan  Brian Zhang  Chloe Hillier  Sudheendra Vijayanarasimhan 
Fabio Viola  Tim Green  Trevor Back  Paul Natsev  et al. The kinetics human action video dataset. arXiv
preprint arXiv:1705.06950  2017.

[7] Xiaolong Wang  Ross Girshick  Abhinav Gupta  and Kaiming He. Non-local neural networks. In The

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2018.

[8] Chun-Fu (Richard) Chen  Quanfu Fan  Neil Mallinar  Tom Sercu  and Rogerio Feris. Big-little net: An
efﬁcient multi-scale feature representation for visual and speech recognition. In International Conference
on Learning Representations  2019.

[9] Raghav Goyal  Samira Ebrahimi Kahou  Vincent Michalski  Joanna Materzynska  Susanne Westphal 
Heuna Kim  Valentin Haenel  Ingo Fruend  Peter Yianilos  Moritz Mueller-Freitag  et al. The" something
something" video database for learning and evaluating visual common sense. In ICCV  2017.

[10] Mathew Monfort  Alex Andonian  Bolei Zhou  Kandan Ramakrishnan  Sarah Adel Bargal  Yan Yan  Lisa
Brown  Quanfu Fan  Dan Gutfreund  Carl Vondrick  et al. Moments in time dataset: one million videos for
event understanding. IEEE transactions on pattern analysis and machine intelligence  2019.

[11] J.K. Aggarwal and M.S. Ryoo. Human activity analysis: A review. ACM Comput. Surv.  43(3)  2011.

[12] Bernard Ghanem Fabian Caba Heilbron  Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-
scale video benchmark for human activity understanding. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)  2015.

9

[13] Y.-G. Jiang  J. Liu  A. Roshan Zamir  G. Toderici  I. Laptev  M. Shah  and R. Sukthankar. THUMOS
challenge: Action recognition with a large number of classes. http://crcv.ucf.edu/THUMOS14/ 
2014.

[14] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in

videos. In Neural Information Processing System (NIPS)  2014.

[15] Saining Xie  Chen Sun  Jonathan Huang  Zhuowen Tu  and Kevin Murphy. Rethinking spatiotemporal
In Proceedings of the European

feature learning: Speed-accuracy trade-offs in video classiﬁcation.
Conference on Computer Vision (ECCV)  pages 305–321  2018.

[16] Du Tran  Heng Wang  Lorenzo Torresani  Jamie Ray  Yann LeCun  and Manohar Paluri. A closer look at
spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition  pages 6450–6459  2018.

[17] Christoph Feichtenhofer  Haoqi Fan  Jitendra Malik  and Kaiming He. Slowfast networks for video

recognition. arXiv preprint arXiv:1812.03982  2018.

[18] Noureldien Hussein  Efstratios Gavves  and Arnold W.M. Smeulders. Timeception for complex action
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2019.

[19] Jeffrey Donahue  Lisa Anne Hendricks  Sergio Guadarrama  Marcus Rohrbach  Subhashini Venugopalan 
Kate Saenko  and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and
description. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2015.

[20] Du Tran  Lubomir Bourdev  Rob Fergus  Lorenzo Torresani  and Manohar Paluri. Learning Spatiotemporal
Features With 3D Convolutional Networks. In The IEEE International Conference on Computer Vision
(ICCV)  December 2015.

[21] Khurram Soomro  Amir Roshan Zamir  Mubarak Shah  Khurram Soomro  Amir Roshan Zamir  and

Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv  2012.

[22] H. Kuehne  H. Jhuang  E. Garrote  T. Poggio  and T. Serre. HMDB: a large video database for human

motion recognition. In Proceedings of the International Conference on Computer Vision (ICCV)  2011.

[23] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In The European Conference on

Computer Vision (ECCV)  September 2018.

[24] Mohammadreza Zolfaghari  Kamaljeet Singh  and Thomas Brox. Eco: Efﬁcient convolutional network for
online video understanding. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 695–712  2018.

[25] Ali Diba  Mohsen Fayyaz  Vivek Sharma  M. Mahdi Arzani  Rahman Yousefzadeh  Juergen Gall  and
Luc Van Gool. Spatio-temporal channel correlation networks for action classiﬁcation. In The European
Conference on Computer Vision (ECCV)  September 2018.

[26] Limin Wang  Wei Li  Wen Li  and Luc Van Gool. Appearance-and-relation networks for video classiﬁcation.

In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2018.

10

,Hyokun Yun
Parameswaran Raman
S. Vishwanathan
Raef Bassily
Kobbi Nissim
Uri Stemmer
Abhradeep Guha Thakurta
Quanfu Fan
Chun-Fu (Richard) Chen
Hilde Kuehne
Marco Pistoia
David Cox