2012,On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes,We consider infinite-horizon stationary $\gamma$-discounted Markov   Decision Processes  for which it is known that there exists a   stationary optimal policy. Using Value and Policy Iteration with   some error $\epsilon$ at each iteration  it is well-known that one   can compute stationary policies that are $\frac{2\gamma{(1-\gamma)^2}\epsilon$-optimal. After arguing that this   guarantee is tight  we develop variations of Value and Policy   Iteration for computing non-stationary policies that can be up to   $\frac{2\gamma}{1-\gamma}\epsilon$-optimal  which constitutes a significant   improvement in the usual situation when $\gamma$ is close to   $1$. Surprisingly  this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that   of ``computing near-optimal stationary policies''.,On the Use of Non-Stationary Policies for Stationary

Inﬁnite-Horizon Markov Decision Processes

Bruno Scherrer

Inria  Villers-l`es-Nancy  F-54600  France

bruno.scherrer@inria.fr

Boris Lesner

Inria  Villers-l`es-Nancy  F-54600  France

boris.lesner@inria.fr

Abstract

We consider inﬁnite-horizon stationary γ-discounted Markov Decision Processes 
for which it is known that there exists a stationary optimal policy. Using Value
and Policy Iteration with some error  at each iteration  it is well-known that one
can compute stationary policies that are
(1−γ)2 -optimal. After arguing that this
guarantee is tight  we develop variations of Value and Policy Iteration for com-
puting non-stationary policies that can be up to 2γ
1−γ -optimal  which constitutes a
signiﬁcant improvement in the usual situation when γ is close to 1. Surprisingly 
this shows that the problem of “computing near-optimal non-stationary policies”
is much simpler than that of “computing near-optimal stationary policies”.

2γ

1

Introduction

Given an inﬁnite-horizon stationary γ-discounted Markov Decision Process [24  4]  we consider
approximate versions of the standard Dynamic Programming algorithms  Policy and Value Iteration 
that build sequences of value functions vk and policies πk as follows

Approximate Value Iteration (AVI):

Approximate Policy Iteration (API):

(cid:26)

vk+1 ← T vk + k+1
vk ← vπk + k
πk+1 ← any element of G(vk)

(1)

(2)

where v0 and π0 are arbitrary  T is the Bellman optimality operator  vπk is the value of policy πk
and G(vk) is the set of policies that are greedy with respect to vk. At each iteration k  the term k
accounts for a possible approximation of the Bellman operator (for AVI) or the evaluation of vπk
(for API). Throughout the paper  we will assume that error terms k satisfy for all k  (cid:107)k(cid:107)∞ ≤ 
for some  ≥ 0. Under this assumption  it is well-known that both algorithms share the following
performance bound (see [25  11  4] for AVI and [4] for API):
Theorem 1. For API (resp. AVI)  the loss due to running policy πk (resp. any policy πk in G(vk−1))
instead of the optimal policy π∗ satisﬁes

lim sup
k→∞

(cid:107)v∗ − vπk(cid:107)∞ ≤

2γ

(1 − γ)2 .

2γ

2γ

The constant
(1−γ)2 can be very big  in particular when γ is close to 1  and consequently the above
bound is commonly believed to be conservative for practical applications. Interestingly  this very
constant
(1−γ)2 appears in many works analyzing AVI algorithms [25  11  27  12  13  23  7  6  20  21 
22  9]  API algorithms [15  19  16  1  8  18  5  17  10  3  9  2] and in one of their generalization [26] 
suggesting that it cannot be improved. Indeed  the bound (and the
(1−γ)2 constant) are tight for
API [4  Example 6.4]  and we will show in Section 3 – to our knowledge  this has never been argued
in the literature – that it is also tight for AVI.

2γ

1

Even though the theory of optimal control states that there exists a stationary policy that is optimal 
the main contribution of our paper is to show that looking for a non-stationary policy (instead of a
stationary one) may lead to a much better performance bound. In Section 4  we will show how to
deduce such a non-stationary policy from a run of AVI. In Section 5  we will describe two original
policy iteration variations that compute non-stationary policies. For all these algorithms  we will
prove that we have a performance bound that can be reduced down to 2γ
1
1−γ
better than the standard bound of Theorem 1  which is signiﬁcant when γ is close to 1. Surprisingly 
this will show that the problem of “computing near-optimal non-stationary policies” is much simpler
than that of “computing near-optimal stationary policies”. Before we present these contributions  the
next section begins by precisely describing our setting.

1−γ . This is a factor

2 Background
We consider an inﬁnite-horizon discounted Markov Decision Process [24  4] (S A  P  r  γ)  where S
is a possibly inﬁnite state space  A is a ﬁnite action space  P (ds(cid:48)|s  a)  for all (s  a)  is a probability
kernel on S  r : S × A → R is a reward function bounded in max-norm by Rmax  and γ ∈ (0  1)
is a discount factor. A stationary deterministic policy π : S → A maps states to actions. We write
rπ(s) = r(s  π(s)) and Pπ(ds(cid:48)|s) = P (ds(cid:48)|s  π(s)) for the immediate reward and the stochastic
kernel associated to policy π. The value vπ of a policy π is a function mapping states to the expected
discounted sum of rewards received when following π from any state: for all s ∈ S 

(cid:34) ∞(cid:88)

t=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)s0 = s  st+1 ∼ Pπ(·|st)

(cid:35)

.

vπ(s) = E

γtrπ(st)

The value vπ is clearly bounded by Vmax = Rmax/(1 − γ).
It is well-known that vπ can be
characterized as the unique ﬁxed point of the linear Bellman operator associated to a policy π:
Tπ : v (cid:55)→ rπ + γPπv. Similarly  the Bellman optimality operator T : v (cid:55)→ maxπ Tπv has as
unique ﬁxed point the optimal value v∗ = maxπ vπ. A policy π is greedy w.r.t. a value function v
if Tπv = T v  the set of such greedy policies is written G(v). Finally  a policy π∗ is optimal  with
value vπ∗ = v∗  iff π∗ ∈ G(v∗)  or equivalently Tπ∗ v∗ = v∗.
Though it is known [24  4] that there always exists a deterministic stationary policy that is optimal 
we will  in this article  consider non-stationary policies and now introduce related notations. Given
a sequence π1  π2  . . .   πk of k stationary policies (this sequence will be clear in the context we
describe later)  and for any 1 ≤ m ≤ k  we will denote πk m the periodic non-stationary policy
that takes the ﬁrst action according to πk  the second according to πk−1  . . .   the mth according to
πk−m+1 and then starts again. Formally  this can be written as

πk m = πk πk−1 ··· πk−m+1 πk πk−1 ··· πk−m+1 ···

It is straightforward to show that the value vπk m of this periodic non-stationary policy πk m is the
unique ﬁxed point of the following operator:

Tk m = Tπk Tπk−1 ··· Tπk−m+1.

Finally  it will be convenient to introduce the following discounted kernel:

Γk m = (γPπk )(γPπk−1)··· (γPπk−m+1).

In particular  for any pair of values v and v(cid:48)  it can easily be seen that Tk mv−Tk mv(cid:48) = Γk m(v−v(cid:48)).

3 Tightness of the performance bound of Theorem 1

The bound of Theorem 1 is tight for API in the sense that there exists an MDP [4  Example 6.4]
for which the bound is reached. To the best of our knowledge  a similar argument has never been
provided for AVI in the literature. It turns out that the MDP that is used for showing the tightness
for API also applies to AVI. This is what we show in this section.
Example 1. Consider the γ-discounted deterministic MDP from [4  Example 6.4] depicted on Fig-
ure 1. It involves states 1  2  . . . . In state 1 there is only one self-loop action with zero reward  for
each state i > 1 there are two possible choices: either move to state i − 1 with zero reward or stay

2

0

1

−2γ

−2(γ + γ2)

−2 γ−γk
1−γ 

0

2

0

3

0

. . .

k

0

0

. . .

Figure 1: The determinisitic MDP for which the bound of Theorem 1 is tight for Value and Policy
Iteration.

with reward ri = −2 γ−γi
1−γ  with  ≥ 0. Clearly the optimal policy in all states i > 1 is to move to
i − 1 and the optimal value function v∗ is 0 in all states.
Starting with v0 = v∗  we are going to show that for all iterations k ≥ 1 it is possible to have a
policy πk+1 ∈ G(vk) which moves in every state but k + 1 and thus is such that vπk+1(k + 1) =
1−γ = −2 γ−γk+1
To do so  we assume that the following approximation errors are made at each iteration k > 0:

(1−γ)2   which meets the bound of Theorem 1 when k tends to inﬁnity.

rk+1

With this error  we are now going to prove by induction on k that for all k ≥ 1 

(cid:40) −


0

if i = k
if i = k + 1
otherwise

.

k(i) =



vk(i) =

−γk−1
rk/2 − 
−(rk/2 − )
0

if i < k
if i = k
if i = k + 1
otherwise

.

Since v0 = 0 the best action is clearly to move in every state i ≥ 2 which gives v1 = v0 + 1 = 1
which establishes the claim for k = 1.
Assuming that our induction claim holds for k  we now show that it also holds for k + 1.
For the move action  write qm
1)  hence

k its action-value function. For all i > 1 we have qm

k (i) = 0 + γvk(i−

 γ(−γk−1)

= −γk
γ(rk/2 − )
= rk+1/2
−γ(rk/2 − ) = −rk+1/2
0

qm
k (i) =

if i = 2  . . .   k
if i = k + 1
if i = k + 2
otherwise

.

For the stay action  write qs
hence



qs
k(i) =

k its action-value function. For all i > 0 we have qs
ri + γ(−γk−1) = ri − γk
rk + γ(rk/2 − ) = rk + rk+1/2
rk+1 − rk+1/2
rk+2 + γ0
0

if i = 1  . . .   k − 1
if i = k
if i = k + 1
if i = k + 2
otherwise

= rk+1/2
= rk+2

.

k(i) = ri + γvk(i) 

k   qs

k (k + 1) = qs

k) + k+1 gives the result for vk+1.

k(i) for all these states but k + 1 where qm

First  only the stay action is available in state 1  hence  since r0 = 0 and k+1(1) = 0  we have
k(1) + k+1(1) = −γk  as desired. Second  since ri < 0 for all i > 1 we have
vk+1(1) = qs
k(k + 1) = rk+1/2. Using the fact
qm
k (i) > qs
that vk+1 = max(qm
The fact that for i > 1 we have qm
k(i) with equality only at i = k +1 implies that there exists
a policy πk+1 greedy for vk which takes the optimal move action in all states but k + 1 where the
stay action has the same value  leaving the algorithm the possibility of choosing the suboptimal stay
action in this state  yielding a value vπk+1(k + 1)  matching the upper bound as k goes to inﬁnity.
Since Example 1 shows that the bound of Theorem 1 is tight  improving performance bounds imply
to modify the algorithms. The following sections of the paper shows that considering non-stationary
policies instead of stationary policies is an interesting path to follow.

k (i) ≥ qs

3

4 Deducing a non-stationary policy from AVI

While AVI (Equation (1)) is usually considered as generating a sequence of values v0  v1  . . .   vk−1 
it also implicitely produces a sequence1 of policies π1  π2  . . .   πk  where for i = 0  . . .   k − 1 
πi+1 ∈ G(vi). Instead of outputing only the last policy πk  we here simply propose to output the
periodic non-stationary policy πk m that loops over the last m generated policies. The following
theorem shows that it is indeed a good idea.
Theorem 2. For all iteration k and m such that 1 ≤ m ≤ k  the loss of running the non-stationary
policy πk m instead of the optimal policy π∗ satisﬁes:

(cid:107)v∗ − vπk m(cid:107)∞ ≤

2

1 − γm

 + γk(cid:107)v∗ − v0(cid:107)∞

.

(cid:18) γ − γk

1 − γ

(cid:19)

When m = 1 and k tends to inﬁnity  one exactly recovers the result of Theorem 1. For general
m  this new bound is a factor 1−γm
1−γ better than the standard bound of Theorem 1. The choice that
optimizes the bound  m = k  and which consists in looping over all the policies generated from the
very start  leads to the following bound:

(cid:18) γ

(cid:19)

1 − γ

− γk
1 − γk

 +

2γk

1 − γk (cid:107)v∗ − v0(cid:107)∞ 

(cid:107)v∗ − vπk k(cid:107)∞ ≤ 2
1−γ  when k tends to ∞.

that tends to 2γ

The rest of the section is devoted to the proof of Theorem 2. An important step of our proof lies
in the following lemma  that implies that for sufﬁciently big m  vk = T vk−1 + k is a rather good
approximation (of the order
1−γ ) of the value vπk m of the non-stationary policy πk m (whereas in
general  it is a much poorer approximation of the value vπk of the last stationary policy πk).
Lemma 1. For all m and k such that 1 ≤ m ≤ k 



(cid:107)T vk−1 − vπk m(cid:107)∞ ≤ γm(cid:107)vk−m − vπk m(cid:107)∞ +

γ − γm
1 − γ

.

Proof of Lemma 1. The value of πk m satisﬁes:

By induction  it can be shown that the sequence of values generated by AVI satisﬁes:

vπk m = Tπk Tπk−1 ··· Tπk−m+1vπk m .
m−1(cid:88)

Tπk vk−1 = Tπk Tπk−1 ··· Tπk−m+1vk−m +

Γk ik−i.

(3)

(4)

By substracting Equations (4) and (3)  one obtains:

i=1

T vk−1 − vπk m = Tπk vk−1 − vπk m = Γk m(vk−m − vπk m ) +

m−1(cid:88)

i=1

Γk ik−i

and the result follows by taking the norm and using the fact that for all i  (cid:107)Γk i(cid:107)∞ = γi.

We are now ready to prove the main result of this section.

Proof of Theorem 2. Using the fact that T is a contraction in max-norm  we have:

(cid:107)v∗ − vk(cid:107)∞ = (cid:107)v∗ − T vk−1 + k(cid:107)∞
≤ (cid:107)T v∗ − T vk−1(cid:107)∞ + 
≤ γ(cid:107)v∗ − vk−1(cid:107)∞ + .

1A given sequence of value functions may induce many sequences of policies since more than one greedy
policy may exist for one particular value function. Our results holds for all such possible choices of greedy
policies.

4

Then  by induction on k  we have that for all k ≥ 1 

(cid:107)v∗ − vk(cid:107)∞ ≤ γk(cid:107)v∗ − v0(cid:107)∞ +

1 − γk
1 − γ

.

(5)

Using Lemma 1 and Equation (5) twice  we can conclude by observing that
(cid:107)v∗ − vπk m(cid:107)∞ ≤ (cid:107)T v∗ − T vk−1(cid:107)∞ + (cid:107)T vk−1 − vπk m(cid:107)∞
(cid:19)

(cid:18)
≤ γ(cid:107)v∗ − vk−1(cid:107)∞ + γm(cid:107)vk−m − vπk m(cid:107)∞ +
≤ γ
+ γm(cid:0)(cid:107)vk−m − v∗(cid:107)∞ + (cid:107)v∗ − vπk m(cid:107)∞(cid:1) +

γk−1(cid:107)v∗ − v0(cid:107)∞ +

1 − γk−1
1 − γ

γ − γm
1 − γ





γ − γm
1 − γ



≤ γk(cid:107)v∗ − v0(cid:107)∞ +

γ − γk
1 − γ



(cid:18)
(cid:18) γ − γk

1 − γ

1 − γk−m

+ γm

γk−m(cid:107)v∗ − v0(cid:107)∞ +

1 − γ
(cid:19)
= γm(cid:107)v∗ − vπk m(cid:107)∞ + 2γk(cid:107)v∗ − v0(cid:107)∞ +
≤

 + γk(cid:107)v∗ − v0(cid:107)∞

2

.

1 − γm

(cid:19)

γ − γm
1 − γ



+

 + (cid:107)v∗ − vπk m(cid:107)∞
2(γ − γk)

1 − γ



5 API algorithms for computing non-stationary policies

We now present similar results that have a Policy Iteration ﬂavour. Unlike in the previous section
where only the output of AVI needed to be changed  improving the bound for an API-like algorithm
is slightly more involved. In this section  we describe and analyze two API algorithms that output
non-stationary policies with improved performance bounds.

API with a non-stationary policy of growing period Following our ﬁndings on non-stationary
policies AVI  we consider the following variation of API  where at each iteration  instead of comput-
ing the value of the last stationary policy πk  we compute that of the periodic non-stationary policy
πk k that loops over all the policies π1  . . .   πk generated from the very start:

vk ← vπk k + k

πk+1 ← any element of G(vk)

where the initial (stationary) policy π1 1 is chosen arbitrarily. Thus  iteration after iteration  the non-
stationary policy πk k is made of more and more stationary policies  and this is why we refer to it as
having a growing period. We can prove the following performance bound for this algorithm:
Theorem 3. After k iterations  the loss of running the non-stationary policy πk k instead of the
optimal policy π∗ satisﬁes:

(cid:107)v∗ − vπk k(cid:107)∞ ≤ 2(γ − γk)
1 − γ

 + γk−1(cid:107)v∗ − vπ1 1(cid:107)∞ + 2(k − 1)γkVmax.

When k tends to inﬁnity  this bound tends to 2γ
original API bound.

1−γ   and is thus again a factor

1

1−γ better than the

5

Proof of Theorem 3. Using the facts that Tk+1 k+1vπk k = Tπk+1Tk kvπk k = Tπk+1 vπk k and
Tπk+1vk ≥ Tπ∗ vk (since πk+1 ∈ G(vk))  we have:

v∗ − vπk+1 k+1
= Tπ∗ v∗ − Tk+1 k+1vπk+1 k+1
= Tπ∗ v∗ − Tπ∗ vπk k + Tπ∗ vπk k − Tk+1 k+1vπk k + Tk+1 k+1vπk k − Tk+1 k+1vπk+1 k+1
= γPπ∗ (v∗ − vπk k ) + Tπ∗ vπk k − Tπk+1vπk k + Γk+1 k+1(vπk k − vπk+1 k+1)
= γPπ∗ (v∗ − vπk k ) + Tπ∗ vk − Tπk+1vk + γ(Pπk+1 − Pπ∗ )k + Γk+1 k+1(vπk k − vπk+1 k+1)
≤ γPπ∗ (v∗ − vπk k ) + γ(Pπk+1 − Pπ∗ )k + Γk+1 k+1(vπk k − vπk+1 k+1 ).
By taking the norm  and using the facts that (cid:107)vπk k(cid:107)∞ ≤ Vmax  (cid:107)vπk+1 k+1(cid:107)∞ ≤ Vmax  and
(cid:107)Γk+1 k+1(cid:107)∞ = γk+1  we get:

(cid:107)v∗ − vπk+1 k+1(cid:107)∞ ≤ γ(cid:107)v∗ − vπk k(cid:107)∞ + 2γ + 2γk+1Vmax.

Finally  by induction on k  we obtain:

(cid:107)v∗ − vπk k(cid:107)∞ ≤ 2(γ − γk)
1 − γ

 + γk−1(cid:107)v∗ − vπ1 1(cid:107)∞ + 2(k − 1)γkVmax.

Though it has an improved asymptotic performance bound  the API algorithm we have just described
has two (related) drawbacks: 1) its ﬁnite iteration bound has a somewhat unsatisfactory term of the
form 2(k − 1)γkVmax  and 2) even when there is no error (when  = 0)  we cannot guarantee that 
similarly to standard Policy Iteration  it generates a sequence of policies of increasing values (it
is easy to see that in general  we do not have vπk+1 k+1 ≥ vπk k). These two points motivate the
introduction of another API algorithm.

API with a non-stationary policy of ﬁxed period We consider now another variation of API
parameterized by m ≥ 1  that iterates as follows for k ≥ m:

vk ← vπk m + k

πk+1 ← any element of G(vk)

where the initial non-stationary policy πm m is built from a sequence of m arbitrary stationary
policies π1  π2 ···   πm. Unlike the previous API algorithm  the non-stationary policy πk m here
only involves the last m greedy stationary policies instead of all of them  and is thus of ﬁxed period.
This is a strict generalization of the standard API algorithm  with which it coincides when m = 1.
For this algorithm  we can prove the following performance bound:
Theorem 4. For all m  for all k ≥ m  the loss of running the non-stationary policy πk m instead of
the optimal policy π∗ satisﬁes:

(cid:107)v∗ − vπk m(cid:107)∞ ≤ γk−m(cid:107)v∗ − vπm m(cid:107)∞ +

2(γ − γk+1−m)
(1 − γ)(1 − γm)

.

1−γ better than the standard bound of Theorem 1.

When m = 1 and k tends to inﬁnity  we recover exactly the bound of Theorem 1. When m > 1
and k tends to inﬁnity  this bound coincides with that of Theorem 2 for our non-stationary version
of AVI: it is a factor 1−γm
The rest of this section develops the proof of this performance bound. A central argument of our
proof is the following lemma  which shows that similarly to the standard API  our new algorithm
has an (approximate) policy improvement property.
Lemma 2. At each iteration of the algorithm  the value vπk+1 m of the non-stationary policy

πk+1 m = πk+1 πk . . . πk+2−m πk+1 πk . . . πk−m+2 . . .

cannot be much worse than the value vπ(cid:48)

of the non-stationary policy

k m

π(cid:48)
k m = πk−m+1 πk . . . πk+2−m πk−m+1 πk . . . πk−m+2 . . .

in the precise following sense:

vπk+1 m ≥ vπ(cid:48)

k m

− 2γ

1 − γm .

6

k m is related to πk m as follows: π(cid:48)

The policy π(cid:48)
k m differs from πk+1 m in that every m steps  it chooses the oldest policy πk−m+1
instead of the newest one πk+1. Also π(cid:48)
k m takes the ﬁrst action
according to πk−m+1 and then runs πk m; equivalently  since πk m loops over πkπk−1 . . . πk−m+1 
π(cid:48)
k m = πk−m+1πk m can be seen as a 1-step right rotation of πk m. When there is no error (when
 = 0)  this shows that the new policy πk+1 m is better than a “rotation” of πk m. When m =
1  πk+1 m = πk+1 and π(cid:48)
k m = πk and we thus recover the well-known (approximate) policy
improvement theorem for standard API (see for instance [4  Lemma 6.1]).
Proof of Lemma 2. Since π(cid:48)
have vπ(cid:48)

= Tπk−m+1vπk m. Now  since πk+1 ∈ G(vk)  we have Tπk+1vk ≥ Tπk−m+1vk and
− vπk+1 m = Tπk−m+1 vπk m − vπk+1 m

k m takes the ﬁrst action with respect to πk−m+1 and then runs πk m  we

k m

vπ(cid:48)

k m

= Tπk−m+1 vk − γPπk−m+1k − vπk+1 m
≤ Tπk+1vk − γPπk−m+1 k − vπk+1 m
= Tπk+1vπk m + γ(Pπk+1 − Pπk−m+1)k − vπk+1 m
= Tπk+1Tk mvπk m − Tk+1 mvπk+1 m + γ(Pπk+1 − Pπk−m+1 )k
= Tk+1 mTπk−m+1vπk m − Tk+1 mvπk+1 m + γ(Pπk+1 − Pπk−m+1)k
= Γk+1 m(Tπk−m+1vπk m − vπk+1 m ) + γ(Pπk+1 − Pπk−m+1)k
= Γk+1 m(vπ(cid:48)

− vπk+1 m ) + γ(Pπk+1 − Pπk−m+1)k.

k m

from which we deduce that:

− vπk+1 m ≤ (I − Γk+1 m)−1γ(Pπk+1 − Pπk−m+1)k

vπ(cid:48)

k m

and the result follows by using the facts that (cid:107)k(cid:107)∞ ≤  and (cid:107)(I − Γk+1 m)−1(cid:107)∞ = 1

1−γm .

We are now ready to prove the main result of this section.

Proof of Theorem 4. Using the facts that 1) Tk+1 m+1vπk m = Tπk+1Tk mvπk m = Tπk+1vπk m and
2) Tπk+1vk ≥ Tπ∗ vk (since πk+1 ∈ G(vk))  we have for k ≥ m 
v∗ − vπk+1 m
= Tπ∗ v∗ − Tk+1 mvπk+1 m
= Tπ∗ v∗ − Tπ∗ vπk m + Tπ∗ vπk m − Tk+1 m+1vπk m + Tk+1 m+1vπk m − Tk+1 mvπk+1 m
= γPπ∗ (v∗ − vπk m ) + Tπ∗ vπk m − Tπk+1vπk m + Γk+1 m(Tπk−m+1vπk m − vπk+1 m )
≤ γPπ∗ (v∗ − vπk m ) + Tπ∗ vk − Tπk+1vk + γ(Pπk+1 − Pπ∗ )k + Γk+1 m(Tπk−m+1vπk m − vπk+1 m )
≤ γPπ∗ (v∗ − vπk m ) + γ(Pπk+1 − Pπ∗ )k + Γk+1 m(Tπk−m+1vπk m − vπk+1 m).
Consider the policy π(cid:48)
Lemma 2 that Tπk−m+1vπk m = vπ(cid:48)

k m deﬁned in Lemma 2. Observing as in the beginning of the proof of

  Equation (6) can be rewritten as follows:
v∗ − vπk+1 m ≤ γPπ∗ (v∗ − vπk m ) + γ(Pπk+1 − Pπ∗ )k + Γk+1 m(vπ(cid:48)

− vπk+1 m).

(6)

k m

k m

By using the facts that v∗ ≥ vπk m  v∗ ≥ vπk+1 m and Lemma 2  we get

(cid:107)v∗ − vπk+1 m(cid:107)∞ ≤ γ(cid:107)v∗ − vπk m(cid:107)∞ + 2γ +
2γ

= γ(cid:107)v∗ − vπk m(cid:107)∞ +

γm(2γ)
1 − γm

Finally  we obtain by induction that for all k ≥ m 

1 − γm .

(cid:107)v∗ − vπk m(cid:107)∞ ≤ γk−m(cid:107)v∗ − vπm m(cid:107)∞ +

2(γ − γk+1−m)
(1 − γ)(1 − γm)

.

7

6 Discussion  conclusion and future work

We recalled in Theorem 1 the standard performance bound when computing an approximately op-
timal stationary policy with the standard AVI and API algorithms. After arguing that this bound is
tight – in particular by providing an original argument for AVI – we proposed three new dynamic
programming algorithms (one based on AVI and two on API) that output non-stationary policies for
which the performance bound can be signiﬁcantly reduced (by a factor

1

1−γ ).



(1−γ)2 ). Using the informal equivalence of the horizons T (cid:39) 1

From a bibliographical point of view  it is the work of [14] that made us think that non-stationary
policies may lead to better performance bounds. In that work  the author considers problems with
a ﬁnite-horizon T for which one computes non-stationary policies with performance bounds in
O(T )  and inﬁnite-horizon problems for which one computes stationary policies with performance
bounds in O(
1−γ one sees that
non-stationary policies look better than stationary policies. In [14]  non-stationary policies are only
computed in the context of ﬁnite-horizon (and thus non-stationary) problems; the fact that non-
stationary policies can also be useful in an inﬁnite-horizon stationary context is to our knowledge
completely new.
The best performance improvements are obtained when our algorithms consider periodic non-
stationary policies of which the period grows to inﬁnity  and thus require an inﬁnite memory  which
may look like a practical limitation. However  in two of the proposed algorithm  a parameter m
allows to make a trade-off between the quality of approximation
(1−γm)(1−γ)  and the amount of
memory O(m) required.
  that is a
memory that scales linearly with the horizon (and thus the difﬁculty) of the problem  one can get a
performance bound of2

(1−e−1)(1−γ)  ≤ 3.164γ
1−γ .
We conjecture that our asymptotic bound of 2γ
1−γ   and the non-asymptotic bounds of Theorems 2
and 4 are tight. The actual proof of this conjecture is left for future work. Important recent works
of the literature involve studying performance bounds when the errors are controlled in Lp norms
instead of max-norm [19  20  21  1  8  18  17] which is natural when supervised learning algorithms
are used to approximate the evaluation steps of AVI and API. Since our proof are based on compo-
nentwise bounds like those of the pioneer works in this topic [19  20]  we believe that the extension
of our analysis to Lp norm analysis is straightforward. Last but not least  an important research
direction that we plan to follow consists in revisiting the many implementations of AVI and API for
building stationary policies (see the list in the introduction)  turn them into algorithms that look for
non-stationary policies and study them precisely analytically as well as empirically.

In practice  it is easy to see that by choosing m =

(cid:109)

2γ

2γ

(cid:108) 1

1−γ

References

[1] A. Antos  Cs. Szepesv´ari  and R. Munos. Learning near-optimal policies with Bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning 
71(1):89–129  2008.

[2] M. Gheshlaghi Azar  V. Gmez  and H.J. Kappen. Dynamic Policy Programming with Func-
tion Approximation. In 14th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS)  volume 15  Fort Lauderdale  FL  USA  2011.

[3] D.P. Bertsekas. Approximate policy iteration: a survey and some new methods. Journal of

Control Theory and Applications  9:310–335  2011.

[4] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc  1996.
[5] L. Busoniu  A. Lazaric  M. Ghavamzadeh  R. Munos  R. Babuska  and B. De Schutter. Least-
squares methods for Policy Iteration. In M. Wiering and M. van Otterlo  editors  Reinforcement
Learning: State of the Art. Springer  2011.

[6] D. Ernst  P. Geurts  and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal

of Machine Learning Research (JMLR)  6  2005.

2With this choice of m  we have m ≥ 1

log 1/γ and thus

8

1−γm ≤ 2

1−e−1 ≤ 3.164.

2

[7] E. Even-dar. Planning in pomdps using multiplicity automata.

Intelligence (UAI  pages 185–192  2005.

In Uncertainty in Artiﬁcial

[8] A.M. Farahmand  M. Ghavamzadeh  Cs. Szepesv´ari  and S. Mannor. Regularized policy itera-

tion. Advances in Neural Information Processing Systems  21:441–448  2009.

[9] A.M. Farahmand  R. Munos  and Cs. Szepesv´ari. Error propagation for approximate policy

and value iteration (extended version). In NIPS  December 2010.

[10] V. Gabillon  A. Lazaric  M. Ghavamzadeh  and B. Scherrer. Classiﬁcation-based Policy Iter-
ation with a Critic. In International Conference on Machine Learning (ICML)  pages 1049–
1056  Seattle  ´Etats-Unis  June 2011.

[11] G.J. Gordon. Stable Function Approximation in Dynamic Programming.

261–268  1995.

In ICML  pages

[12] C. Guestrin  D. Koller  and R. Parr. Max-norm projections for factored MDPs. In International

Joint Conference on Artiﬁcial Intelligence  volume 17-1  pages 673–682  2001.

[13] C. Guestrin  D. Koller  R. Parr  and S. Venkataraman. Efﬁcient Solution Algorithms for Fac-

tored MDPs. Journal of Artiﬁcial Intelligence Research (JAIR)  19:399–468  2003.

[14] S.M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis  University

College London  2003.

[15] S.M. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning.

In International Conference on Machine Learning (ICML)  pages 267–274  2002.

[16] M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning

Research (JMLR)  4:1107–1149  2003.

[17] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-Sample Analysis of Least-Squares Policy

Iteration. To appear in Journal of Machine learning Research (JMLR)  2011.

[18] O.A. Maillard  R. Munos  A. Lazaric  and M. Ghavamzadeh. Finite Sample Analysis of Bell-
man Residual Minimization. In Masashi Sugiyama and Qiang Yang  editors  Asian Conference
on Machine Learpning. JMLR: Workshop and Conference Proceedings  volume 13  pages 309–
324  2010.

[19] R. Munos. Error Bounds for Approximate Policy Iteration. In International Conference on

Machine Learning (ICML)  pages 560–567  2003.

[20] R. Munos. Performance Bounds in Lp norm for Approximate Value Iteration. SIAM J. Control

and Optimization  2007.

[21] R. Munos and Cs. Szepesv´ari. Finite time bounds for sampling based ﬁtted value iteration.

Journal of Machine Learning Research (JMLR)  9:815–857  2008.

[22] M. Petrik and B. Scherrer. Biasing Approximate Dynamic Programming with a Lower Dis-
count Factor. In Twenty-Second Annual Conference on Neural Information Processing Systems
-NIPS 2008  Vancouver  Canada  2008.

[23] J. Pineau  G.J. Gordon  and S. Thrun. Point-based value iteration: An anytime algorithm
for POMDPs. In International Joint Conference on Artiﬁcial Intelligence  volume 18  pages
1025–1032  2003.

[24] M. Puterman. Markov Decision Processes. Wiley  New York  1994.
[25] S. Singh and R. Yee. An Upper Bound on the Loss from Approximate Optimal-Value Func-

tions. Machine Learning  16-3:227–233  1994.

[26] C. Thiery and B. Scherrer. Least-Squares λ Policy Iteration: Bias-Variance Trade-off in Con-

trol Problems. In International Conference on Machine Learning  Haifa  Israel  2010.

[27] J.N. Tsitsiklis and B. Van Roy. Feature-Based Methods for Large Scale Dynamic Program-

ming. Machine Learning  22(1-3):59–94  1996.

9

,Xianjie Chen
Alan Yuille
Yuan Deng
Jon Schneider
Balasubramanian Sivan