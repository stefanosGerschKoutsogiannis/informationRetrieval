2008,Sparse probabilistic projections,We present a generative model for performing sparse probabilistic projections  which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions  such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a pre-processing tool for the construction of template attacks.,Sparse probabilistic projections

C´edric Archambeau

Department of Computer Science

Francis R. Bach

INRIA - Willow Project

University College London  United Kingdom

Ecole Normale Sup´erieure  Paris  France

c.archambeau@cs.ucl.ac.uk

francis.bach@mines.org

Abstract

We present a generative model for performing sparse probabilistic projections 
which includes sparse principal component analysis and sparse canonical corre-
lation analysis as special cases. Sparsity is enforced by means of automatic rele-
vance determination or by imposing appropriate prior distributions  such as gener-
alised hyperbolic distributions. We derive a variational Expectation-Maximisation
algorithm for the estimation of the hyperparameters and show that our novel prob-
abilistic approach compares favourably to existing techniques. We illustrate how
the proposed method can be applied in the context of cryptoanalysis as a pre-
processing tool for the construction of template attacks.

1 Introduction

Principal component analysis (PCA) is widely used for data pre-processing  data compression and
dimensionality reduction. However  PCA suffers from the fact that each principal component is a
linear combination of all the original variables. It is thus often difﬁcult to interpret the results. In
recent years  several methods for sparse PCA have been designed to ﬁnd projections which retain
maximal variance  while enforcing many entries of the projection matrix to be zero [20  6]. While
most of these methods are based on convex or partially convex relaxations of the sparse PCA prob-
lem  [16] has looked at using the probabilistic PCA framework of [18] along with (cid:96)1-regularisation.
Canonical correlation analysis (CCA) is also commonly used in the context for dimensionality re-
duction.The goal is here to capture features that are common to several views of the same data.
Recent attempts for constructing sparse CCA include [10  19].
In this paper  we build on the probabilistic interpretation of CCA outlined by [2] and further extended
by [13]. We introduce a general probabilistic model  which allows us to infer from an arbitrary
number of views of the data  both a shared latent representation and individual low-dimensional
representations of each one of them. Hence  the probabilistic reformulations of PCA and CCA ﬁt
this probabilistic framework. Moreover  we are interested in sparse solutions  as these are important
for interpretation purposes  denoising or feature extraction. We consider a Bayesian approach to
the problem. A proper probabilistic approach allows us to treat the trade-off between the modelling
accuracy (of the high-dimensional observations by low-dimensional latent variables) and the degree
of sparsity of the projection directions in principled way. For example  we do not need to estimate
the sparse components successively  using  e.g.  deﬂation  but we can estimate all sparse directions
jointly as we are taking the uncertainty of the latent variable into account.
In order to ensure sparse solutions we propose two strategies. The ﬁrst one  discussed in Ap-
pendix A  is based on automatic relevance determination (ARD) [14]. No parameter needs to be
set in advance. The entries in the projection matrix which are not well determined by the data are
automatically driven to zero. The second approach uses priors from the generalised hyperbolic fam-
ily [3]  and more speciﬁcally the inverse Gamma. In this case  the degree of sparsity can be adjusted 
eventually leading to very sparse solutions if desired. For both approaches we derive a variational
EM algorithm [15].

1

(a)

(b)

Figure 1: (a) Graphical model (see text for details). Arrows denote conditional dependencies.
Shaded and unshaded nodes are respectively observed and unobserved random variables. Plates
indicate repetitions. (b) Marginal prior on the individual matrix entries (b = 1).

2 Generative model

We consider the graphical model shown in Figure 1(a). For each observation  we have P independent
measurements x1  . . .   xP in different measurement spaces or views. The measurement xp ∈ RDp
is modelled as a mix of a common (or view independent) continuous latent vector y0 ∈ RQ0 and a
view dependent continuous latent vector yp ∈ RQp  such that

xp = Wpy0 + Vpyp + µp + p 
p=1 are the view dependent offsets and p ∼ N (0  τ−1

(1)
where {µp}P
p IDp) is the residual error in view
p.
We are interested in the case where y0 and yp are low-dimensional vectors  i.e.  Q0  Qp (cid:28) Dp for
all p. We impose Gaussian priors on the latent vectors:

Wp ∈ RDp×Q0  Vp ∈ RDp×Qp  

y0 ∼ N (0  Φ−1
0 ) 

yp ∼ N (0  Φ−1

p )  p ∈ {1  . . .   P}.

(2)
The resulting generative model comprises a number of popular probabilistic projection techniques as
special cases. If there is a single view (and a single latent cause) and the prior covariance is diagonal 
we recover probabilistic factor analysis [9]. If the prior is also isotropic  then we get probabilistic
PCA [18]. If there are two views  we recover probabilistic CCA [2].
We seek a solution for which the matrices {Wp}P
p=1 are sparse  i.e. most of their en-
tries are zero. One way to achieve sparsity is by means of ARD-type priors [14]. In this framework 
a zero-mean Gaussian prior is imposed on the entries of the weight matrices:

p=1 and {Vp}P

wipj ∼ N (0  1/αipj) 
vipkp ∼ N (0  1/βipkp) 

ip ∈ {1  . . .   Dp}  j ∈ {1  . . .   Q0} 
ip ∈ {1  . . .   Dp}  kp ∈ {1  . . .   Qp}.

(3)
(4)
Type II maximum likelihood leads then to a sparse solution when considering independent hyper-
parameters. The updates arising in the context of probabilistic projections are given in Appendix A.
Since marginalisation with respect to both the latent vectors and the weights is intractable  we apply
variational EM [15]. Unfortunately  following this route does not allow us to adjust the degree of
sparsity  which is important e.g. for interpretation purposes or for feature extraction.
Hence  we seek a more ﬂexible approach. In the remaining of this paper  we will assume that the
marginal prior on each weight λij  which is either an entry of {Wp}P
p=1 and will be
deﬁned shortly  has the form of an (inﬁnite) weighted sum of scaled Gaussians:

p=1 or {Vp}P

(cid:90)

p(λij) =

N (0  γ−1

ij ) p(γij) dγij.

(5)

We will chose the prior over γij in such a way that the resulting marginal prior over the correspond-
ing λij induces sparsity. A similar approach was followed in the context of sparse nonparametric
Bayesian regression in [4  5].

2

−10−5051000.10.20.30.40.50.60.70.80.91!p(! )  a/DQ=0.1a/DQ=1a/DQ=102.1 Compact reformulation of the generative model

Before discussing the approximate inference scheme  we rewrite the model in a more compact way.
Let us denote the nth observation  the corresponding latent vector and the means respectively by

xn =(cid:0)x(cid:62)

n1  . . .   x(cid:62)

nP

(cid:1)(cid:62)

 

zn =(cid:0)y(cid:62)

The generative model can be reformulated as follows:

(cid:1)(cid:62)

 

µ =(cid:0)µ(cid:62)

1   . . .   µ(cid:62)

P

(cid:1)(cid:62)

.

n0  y(cid:62)

n1  . . .   y(cid:62)

nP

pQp 

Φ ∈ RQ×Q  Q = Q0 +(cid:80)
i ∈ {1  . . .   D}  j ∈ {1  . . .   Q}  D =(cid:80)
 τ1ID1
  

Λ ∈ RD×Q  Ψ ∈ RD×D 

Ψ =

0
...

. . .
...
. . .

...
0

τP IDP

pDp 

 .

0
. . .
...
...
. . . VP

(6)
(7)
(8)

zn ∼ N (0  Φ−1) 
λij|γij ∼ N (0  γ−1
ij ) 

where

Λ =

xn|zn  Λ ∼ N (Λzn + µ  Ψ−1) 

 Λ1

...
ΛP

 =

 W1 V1

...
WP

...
0

Note that we do not assume that the latent spaces are correlated as Φ = diag{Φ0  Φ1  . . .   ΦP}.
This is consistent with the fact the common latent space is modelled independently through y0.
Subsequently  we will also denote the matrix of the hyperparameters by Γ ∈ RD×Q  where we set
(and ﬁx) γij = ∞ for all λij = 0.

2.2 Sparsity inducing prior over the individual scale variables

We impose an inverse Gamma prior on the scale variable γij:
γij ∼ IG(a/DQ  b) 

(9)
for all i and j. The shape parameter a and the scale parameter b are non-negative. The marginal prior
on the weight λij is then in the class of the generalised hyperbolic distributions [3] and is deﬁned in
terms of the modiﬁed Bessel function of the third kind Kω(·):

p(λij) =

for λij (cid:54)= 0  and

(cid:33) a

2DQ− 1

4

(cid:16)(cid:113)

(cid:17)

K a

DQ− 1

2

2bλ2
ij

(cid:114) 2

b
Γ( a

π

a

DQ

(cid:32)
(cid:40) (cid:113) b

DQ)

λ2
ij
2b

lim
λij→0

p(λij) =

2π

Γ( a

DQ− 1
2 )
Γ( a
DQ )
∞

a

DQ > 1
2  
otherwise.

(10)

(11)

The function Γ(·) is the (complete) Gamma function.
The effective prior on the individual weights is shown in Figure 1(b). Intuitively  the joint distribu-
tion over the weights is sparsity inducing as it is sharply peaked around zero (and in fact inﬁnite for
sufﬁciently small a). It favours only a small number of weights to be non-zero if the scale variable
b is sufﬁciently large. For a more formal discussion in the context of regression we refer to [7].
It is interesting to note that for a/DQ = 1 we recover the popular Laplace prior  which is equivalent
to the (cid:96)1-regulariser or the LASSO [17]  and for a/DQ → 0 and b → 0 the resulting prior is
the Normal-Jeffreys prior.
In fact  the automatic thresholding method described in Appendix A
ﬁts also into the framework deﬁned by (5). However  it corresponds to imposing a ﬂat prior on
the scale variables over the log-scale  which is a limiting case of the Gamma distribution. When
imposing independent Gamma priors on the scale variables  the effective joint marginal is a product
of Student-t distributions  which again is sharply peaked around zero and sparsity inducing.

3 Variational approximation
We view {zn}N
n=1 and matrix Γ as latent variables  and optimise the parameters θ = {µ  Φ  Λ  Ψ}
by EM. In other words  we view the weight matrix Λ as a matrix of parameter and estimate the

3

N(cid:88)

Fq(x1  . . .   xN   θ) = − N(cid:88)

entries by maximum a posteriori (MAP) learning. The other parameters are estimated by maximum
likelihood (ML).
The variational free energy is given by

(12)
where (cid:104)·(cid:105)q denotes the expectation with respect to the variational distribution q and H[·] is the differ-
ential entropy. Since the Kullback-Leibler divergence (KL) is non-negative  the negative free energy
is a lower bound to log-marginal likelihood:

(cid:104)ln p(xn  zn  Γ|θ)(cid:105)q − H[q(z1  . . .   zN   Γ)] 

n=1

ln p(xn|θ) = −Fq({xn}  θ) + KL [q({zn}  Γ)(cid:107)p({zn}  Γ)|{xn}  θ)] (cid:62) −Fq({xn}  θ).

n=1

(13)
Interestingly it is not required to make a factorised approximation of the the joint posterior q to ﬁnd
a tractable solution. Indeed  the posterior q factorises naturally given the data and the weights  such
that the posteriors we will obtain in the E-step are exact.
The variational EM ﬁnds maximum likelihood estimates for the parameters by cycling through the
following two steps until convergence:

1. The posterior over the latent variables are computed for ﬁxed parameters by minimising

the KL in (13). It can be shown that the variational posteriors are given by

q(z1  . . .   zN ) ∝ N(cid:89)

n=1

N(cid:88)

θ

n=1

e(cid:104)ln p(xn zn Γ|θ)(cid:105)q(Γ)  

(14)

q(Γ) ∝ e(cid:104)ln p(xn zn|Γ θ)(cid:105)q(z1 ... zN )p(Γ).

(15)
2. The variational free energy (12) is minimised wrt the parameters for ﬁxed q. This leads
in effect to type II ML estimates for the paramteres and is equivalent to maximising the
expected complete log-likelihood:

θ ← argmax

(cid:104)ln p(xn  zn  Γ|θ)(cid:105)q .

(16)

Depending on the initialisation  the variational EM algorithm converges to a local maximum of
the log-marginal likelihood. The convergence can be checked by monitoring the variational lower
bound  which monotonically increases during the optimisation. The explicit expression of the vari-
ational bound is here omitted due to a lack of space

3.1 Posterior of the latent vectors

The joint posterior of the latent vectors factorises into N posteriors due to the fact the observations
are independent. Hence  the posterior of each low-dimenstional latent vector is given by

where ¯zn = ¯SnΛ(cid:62)Ψ(xn − µ) is the mean and ¯Sn =(cid:0)Λ(cid:62)ΨΛ + Φ(cid:1)−1 is the covariance.

q(zn) = N (¯zn  ¯Sn) 

(17)

3.2 Posterior of the scale variables

The inverse Gamma distribution is not conjugate to the exponential family. However  the posterior
over matrix Γ is tractable. It has the form of a product of generalised inverse Gaussian distributions
(see Appendix B for a formal deﬁnition):

q(Γ) =

p(γij|λij) =

N −(¯ωij  ¯ϕij  ¯χij)

(18)

D(cid:89)

Q(cid:89)

i=1

j=1

where ¯ωij = − a
factorised form arises from the scale variable being independent conditioned on the weights.

ij and ¯χij = 2b are the shape parameters. The

2 is the index and ¯ϕij = λ2

DQ + 1

D(cid:89)

Q(cid:89)

i=1

j=1

4

3.3 Update for the parameters

Based on the properties of the Gaussian and the generalised inverse Gaussian  we can compute the
variational lower bound  which can then be maximised. This leads to the following updates:

N(cid:88)

N(cid:88)
N(cid:88)

n=1

Φ−1 ← 1
N

diag{¯zn¯z(cid:62)

n + ¯Sn} 

µ ← 1
N

(xn − Λ¯zn) 

n=1

λi ←(cid:16)¯Γi + Ψ(i  i)
N(cid:88)

n (cid:105)(cid:17)−1

N(cid:88)
(cid:8)(xnp − µp)(cid:62)(xnp − µp) − 2(xnp − µp)(cid:62)Λp¯zn +(cid:10)(Λpzn)(cid:62)Λpzn

(xn(i) − µ(i))¯zn 

(cid:104)znz(cid:62)

Ψ(i  i)

n=1

n=1

(19)

(20)

(cid:11)(cid:9)  (21)

(22)

(23)

p ← 1
τ−1
N Dp

n=1

where the required expectations are given by

(cid:104)znz(cid:62)

n (cid:105) = ¯Sn + ¯zn¯z(cid:62)
n  

¯Γi = diag(cid:8)(cid:104)γi1(cid:105)  . . .  (cid:104)γiQ(cid:105)(cid:9) 

(cid:10)(Λpzn)(cid:62)Λpzn
(cid:115) ¯χij

(cid:104)γij(cid:105) =

(cid:11) = tr{(cid:104)¯zn¯z(cid:62)
(cid:0)√
(cid:1)
n (cid:105)Λ(cid:62)
p Λp} 
(cid:1) .
(cid:0)√

¯χij ¯ϕij
¯χij ¯ϕij

Kω+1
Kω

¯ϕij

Note that diag{·} denotes a block-diagonal operation in (19). More importantly  since we are seek-
ing a sparse projection matrix  we do not suffer from the rotational ambiguity problem as is for
example the case standard probabilistic PCA.

4 Experiments

4.1 Synthetic denoising experiments

Because of identiﬁability issues which are subject of ongoing work  we prefer to compare various
methods for sparse PCA in a denoising experiment. That is  we assume that the data were generated
from sparse components plus some noise and we compare the various sparse PCA on the denoising
task  i.e.  on the task of recovering the original data. We generated the data as follows: select
uniformly at random M = 4 unit norm sparse vectors in P = 10 dimensions with known number
S = 4 of non zero entries  then generate i.i.d. values of the random variables Z from three possible
distributions (Gaussian  Laplacian  uniform)  then add isotropic noise of relative standard deviation
1/2. When the latent variables are Gaussian  our model exactly matches the data and our method
should provide a better ﬁt; however  we consider also situations where the model is misspeciﬁed in
order to study the robustness of our probabilistic model.
We consider our two models: SCA-1 (which uses automatic relevance determination type of sparsity
priors) and SCA-2 (which uses generalised hyperbolic distribution)  where we used 6 latent dimen-
sions (larger than 4) and ﬁxed hyperparameters that lead to vague priors. Those two models thus
have no free parameters and we compare them to the following methods  which all have two regu-
larisation parameters (rank and regularisation): DSPCA [6]  the method of Zou [20] and the recent
method of [16] which essentially considers a probabilistic PCA with (cid:96)1-penalty on the weights.
In Table 1 we report mean-squared reconstruction error averaged over 10 replications. It can be seen
that two proposed probabilistic approaches perform similarly and signiﬁcantly outperform other
sparse PCA methods  even when the model is misspeciﬁed.

4.2 Template attacks

Power consumption and electromagnetic radiation are among the most extensively used side-
channels for analysing physically observable cryptographic devices. A common belief is that the
useful information for attacking a device is hidden at times where the traces (or time series) have
large variance. Once the relevant samples have been identiﬁed they can be used to construct tem-
plates  which can then be used to assess if a device is secure. A simple  yet very powerful approach 
recently proposed by [1]  is to select time samples based on PCA. Figure 2(a) shows the weight

5

N SCA-1
39.9
100
36.5
200
400
35.5
N SCA-1
39.9
100
36.8
200
400
36.4
N SCA-1
39.3
100
36.5
200
300
35.8

SCA-2
40.8
36.8
35.5
SCA-2
40.9
37.0
36.4
SCA-2
40.3
36.7
35.8

50.8
50.4
42.5

Zou DSPCA L1-PCA
42.2
40.8
39.8
Zou DSPCA L1-PCA
42.6
40.9
40.5
Zou DSPCA L1-PCA
42.7
40.2
40.6

49.8
48.1
46.8

48.5
46.2
41.0

42.9
41.4
40.3

43.6
41.1
40.7

43.4
40.8
40.9

Table 1: Denoising experiment with sparse PCA (we report mean squared errors): (top) Gaussian
distributed latent vectors  (middle) latent vectors generated from the uniform distribution  (bottom)
latent vectors generated from the Laplace distribution.

(a) Probabilistic PCA.

(b) Sparse probabilistic PCA (SCA-2).

Figure 2: Power traces and ﬁrst three principal directions.

associated to each time sample by the ﬁrst three principal directions found by PCA. The problem
with this approach is that all time samples get a non-zero weights. As a result  the user has to deﬁne
a threshold manually in order to decide whether the information leakage at time t is relevant or not.
Figure 2(b) shows the weight associated to the time samples by SCA-2 when using a Laplace prior
(i.e. for a/DQ = 1). It can be observed that one gets a much better picture of where the relevant
information is. Clearly  sparse probabilitic PCA can be viewed as being more robust to spurious
noise and provides a more reliable and amenable solution.

5 Conclusion

In this paper we introduced a general probabilistic model for inferring sparse probabilistic projection
matrices. Sparsity was enforced by either imposing an ARD-type prior or by means of the a Normal-
Inverse Gamma prior. Although the inverse Gamma is not conjugate to the exponential family  the
posterior is tractable as it is a special case of the generalised inverse Gaussian [12]  which in turn
is a conjugate prior to this family. Future work will include the validation of the method on a wide
range of applications and in particular as a feature extracting tool.

Acknowledgments

We are grateful to the PASCAL European network of excellence for partially supporting this work.

6

02040608010012014016018020000.10.2Power020406080100120140160180200−101!1020406080100120140160180200−101!2020406080100120140160180200−101!3t02040608010012014016018020000.10.2Power020406080100120140160180200−101!1020406080100120140160180200−101!2020406080100120140160180200−101!3tA Automatic thresholding the weights by ARD

In this section  we provide the updates for achieving automatic thresholding of projection matrix
entries in a probabilistic setting. We apply Tipping’s sparse Bayesian theory [8]  which is closely
related to ARD [14]. More speciﬁcally  we assume the prior over the scale variables is uniform over
a log-scale  which is a limiting case of the Gamma distribution.
Let us view {zn}N
variational EM. The variational free energy is given by

n=1 and Λ as latent variables and optimise the parameters θ = {µ  Φ  Ψ  Γ} by

Fq(x1  . . .   xN   θ) = − N(cid:88)

n=1

(cid:104)ln p(xn  zn  Λ|θ)(cid:105)q − H[q(z1  . . .   zN   Λ)].

(24)

In order to ﬁnd a tractable solution  we further have to assume that the approximate posterior q has
a factorised form. We can then compute the posterior of the low-dimenstional latent vectors:

q(zn) = N (¯zn  ¯Sn) 

Ψ(xn − µ) and ¯Sn =(cid:0) ¯Λ
D(cid:89)

(cid:62)

Ψ ¯Λ +(cid:80)
D(cid:89)

iΨ(i  i) ¯Σi + Φ(cid:1)−1. And the posterior of

(25)

(cid:62)
where ¯zn = ¯Sn ¯Λ
the weights is given by

where ¯λi = ¯ΣiΨ(i  i)(cid:80)
partially factorised form(cid:81)

q(Λ) =

q(λi) =

n(xn(i) − µ(i))¯zn and ¯Σi =(cid:0)Γi + Ψ(i  i)(cid:80)

i=1

i=1

N (¯λi  ¯Σi) 

(26)

n }(cid:1)−1. The

n{¯Sn + ¯zn¯z(cid:62)

i q(λi) arises naturally. Note also that the update for the mean weights
has the same form as in (20). Finally  the updates for the parameters are found by maximising the
negative free energy  which corresponds to performing type II ML for the scaling variables. This
yields

n=1

N(cid:88)

diag(cid:8)¯zn¯z(cid:62)

N(cid:88)
(xn − ¯Λ¯zn)  Φ−1 ← 1
N(cid:88)
(cid:8)(xnp − µp)(cid:62)(xnp − µp) − 2(xnp − µp)(cid:62) ¯Λp¯zn +(cid:10)(Λpzn)(cid:62)Λpzn
N
ij + ¯Σi(j  j) and(cid:10)(Λpzn)(cid:62)Λpzn

(cid:11) = tr{(¯zn¯z(cid:62)

n + ¯Sn)(cid:80)

γij ← (cid:104)λ2

n + ¯Sn

ij(cid:105)−1 

(¯λip

(cid:62)
¯λ
ip

(cid:9) 

n=1

n=1

ip

+ ¯Σip)}.

(27)

(cid:11)(cid:9)  (28)

µ ← 1
N

p ← 1
τ−1
N Dp
ij(cid:105) = ¯λ2

where (cid:104)λ2

B Generalised inverse Gaussian distribution

The Generalised inverse Gaussian distribution is in the class of generalised hyperbolic distributions.
It is deﬁned as follows [12  11]:

√
y ∼ N −(ω  χ  φ) = χ−ω(
χφ)ω
√
χφ)
2Kω(

yω−1e− 1

2 (χy−1+φy) 

(29)

where y > 0 and Kω(·) is the modiﬁed Bessel function of the third kind1 with index ω.
The following expectations are useful [12]:

(cid:114) χ

φ

Rω((cid:112)χφ) 

(cid:104)y(cid:105) =

(cid:115)

φ
χ

R−ω((cid:112)χφ) 

(cid:104)y−1(cid:105) =

(cid:104)ln y(cid:105) = ln ω + d ln Kω(
dω

√
χφ)

 

(30)

where Rω(·) ≡ Kω+1(·)/Kω(·).

1The modiﬁed Bessel function of the third kind is known under various names. In particular  it is also known
as the modiﬁed Bessel function of the second kind (cf. E. W. Weisstein: ”Modiﬁed Bessel Function of the Sec-
ond Kind.” From MathWorld: http://mathworld.wolfram.com/ModiﬁedBesselFunctionoftheSecondKind.html).

7

Inverse Gamma distribution

When φ = 0 and ω < 0  the generalised inverse Gaussian distribution reduces to the inverse Gamma
distribution:

IG(a  b) = ba

It is straightforward to verify this result by posing a = −ω and b = χ/2  and noting that

a  b > 0.

x  

Γ(a) x−a−1e− b
Kω(y) = Γ(−ω)2−ω−1yω

(31)

(32)

for ω < 0.

lim
y→0

References
[1] C. Archambeau  E. Peeters  F.-X. Standaert  and J.-J. Quisquater. Template attacks in principal subspaces.
In L. Goubin and M. Matsui  editors  8th International Workshop on Cryptographic Hardware and Em-
bedded Systems(CHES)  volume 4249 of Lecture Notes in Computer Science  pages 1–14. Springer  2006.
[2] F. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical

Report 688  Department of Statistics  University of California  Berkeley  2005.

[3] O. Barndorff-Nielsen and R. Stelzer. Absolute moments of generalized hyperbolic distributions and
approximate scaling of normal inverse Gaussian L´evy processes. Scandinavian Journal of Statistics 
32(4):617–637  2005.

[4] P. J. Brown and J. E. Grifﬁn. Bayesian adaptive lassos with non-convex penalization. Technical Report

CRiSM 07-02  Department of Statistics  University of Warwick  2007.

[5] F. Caron and A. Doucet. Sparse bayesian nonparametric regression. In 25th International Conference on

Machine Learning (ICML). ACM  2008.

[6] A. d’Aspremont  E. L. Ghaoui  M. I. Jordan  and G. R. G. Lanckriet. A direct formulation for sparse PCA

using semideﬁnite programming. SIAM Review  49(3):434–48  2007.

[7] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal

of the American Statistical Association  96:1348–1360  2001.

[8] A. C. Faul and M. E. Tipping. Analysis of sparse Bayesian learning. In T. G. Dietterich  S. Becker  and
Z. Ghahramani  editors  Advances in Neural Information Processing Systems 14 (NIPS)  pages 383–389.
The MIT Press  2002.

[9] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report

CRG-TR-96-1  Department of Computer Science  University of Toronto  1996.

[10] D. Hardoon and J. Shawe-Taylor. Sparse canonical correlation analysis. Technical report  PASCAL

EPrints  2007.

[11] Wenbo Hu. Calibration of multivariate generalized hyperbolic distributions using the EM algorithm  with
applications in risk management  portfolio optimization and portfolio credit risk. PhD thesis  Florida State
University  United States of America  2005.

[12] B. Jørgensen. Statistical Properties of the Generalized Inverse Gaussian Distribution. Springer-Verlag 

1982.

[13] A. Klami and S. Kaski. Local dependent components.

In Z. Ghahramani  editor  24th International

Conference on Machine Learning (ICML)  pages 425–432. Omnipress  2007.

[14] D. J. C. MacKay. Bayesian methods for backprop networks.

In E. Domany  J.L. van Hemmen  and

K. Schulten  editors  Models of Neural Networks  III  pages 211–254. 1994.

[15] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and other

variants. In M. I. Jordan  editor  Learning in Graphical Models  pages 355–368. The MIT press  1998.

[16] C. D. Sigg and J. M. Buhmann. Expectation-maximization for sparse and non-negative PCA. In 25th

International Conference on Machine Learning (ICML). ACM  2008.

[17] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society

B  58:267–288  1996.

[18] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal

Statistical Society B  61:611–622  1999.

[19] D. Torres  D. Turnbull  B. K. Sriperumbudur  L. Barrington  and G.Lanckriet. Finding musically mean-

ingful words using sparse CCA. In NIPS workshop on Music  Brain and Cognition  2007.

[20] H. Zou  T. Hastie  and R. Tibshirani. Sparse principal component analysis. Journal of Computational and

Graphical Statistics  15(2):265–286  2006.

8

,Jasper Snoek
Richard Zemel
Ryan Adams