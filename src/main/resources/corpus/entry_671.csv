2013,BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables,The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However  it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20 000 variables. In this paper  we develop an algorithm BigQUIC  which can solve 1 million dimensional l1-regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine  with bounded memory. In order to do so  we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components. In spite of these modifications   we are able to theoretically analyze our procedure and show that BigQUIC can achieve super-linear or even quadratic convergence rates.,BIG & QUIC: Sparse Inverse Covariance Estimation

for a Million Variables

Cho-Jui Hsieh  M´aty´as A. Sustik  Inderjit S. Dhillon  Pradeep Ravikumar

Department of Computer Science

University of Texas at Austin

{cjhsieh sustik inderjit pradeepr}@cs.utexas.edu

Russell A. Poldrack

Department of Psychology and Neurobiology

University of Texas at Austin

poldrack@mail.utexas.edu

Abstract

The (cid:96)1-regularized Gaussian maximum likelihood estimator (MLE) has been
shown to have strong statistical guarantees in recovering a sparse inverse covari-
ance matrix even under high-dimensional settings. However  it requires solving
a difﬁcult non-smooth log-determinant program with number of parameters scal-
ing quadratically with the number of Gaussian variables. State-of-the-art methods
thus do not scale to problems with more than 20  000 variables. In this paper 
we develop an algorithm BIGQUIC  which can solve 1 million dimensional (cid:96)1-
regularized Gaussian MLE problems (which would thus have 1000 billion pa-
rameters) using a single machine  with bounded memory. In order to do so  we
carefully exploit the underlying structure of the problem. Our innovations include
a novel block-coordinate descent method with the blocks chosen via a clustering
scheme to minimize repeated computations; and allowing for inexact computation
of speciﬁc components. In spite of these modiﬁcations  we are able to theoreti-
cally analyze our procedure and show that BIGQUIC can achieve super-linear or
even quadratic convergence rates.

f(Θ) 

Introduction

1
Let {y1  y2  . . .   yn} be n samples drawn from a p-dimensional Gaussian distribution N (µ  Σ)  also
known as a Gaussian Markov Random Field (GMRF). An important problem is that of recovering
the covariance matrix (or its inverse) of this distribution  given the n samples  in a high-dimensional
regime where n (cid:28) p. A popular approach involves leveraging the structure of sparsity in the inverse
covariance matrix  and solving the following (cid:96)1-regularized maximum likelihood problem:

(cid:80)n
arg min
Θ(cid:31)0
i=1(yi − ˜µ)(yi − ˜µ)T is the sample covariance matrix and ˜µ = 1

{− log det Θ + tr(SΘ) + λ(cid:107)Θ(cid:107)1} = arg min
Θ(cid:31)0

where S = 1
i=1 yi is
n
the sample mean. While the non-smooth log-determinant program in (1) is usually considered a
difﬁcult optimization problem to solve  due in part to its importance  there has been a long line
of recent work on algorithms to solve (1): see [7  6  3  16  17  18  15  11] and references therein.
The state-of-the-art seems to be a second order method QUIC [9] that has been shown to achieve
super-linear convergence rates. Complementary techniques such as exact covariance thresholding
[13  19]  and the divide and conquer approach of [8]  have also been proposed to speed up the
solvers. However  as noted in [8]  the above methods do not scale to problems with more than 20  000
variables  and typically require several hours even for smaller dimensional problems involving ten
thousand variables. There has been some interest in statistical estimators other than (1) that are
more amenable to optimization: including solving node-wise Lasso regression problems [14] and
the separable linear program based CLIME estimator [2]. However the caveat with these estimators
is that they are not guaranteed to yield a positive-deﬁnite covariance matrix  and typically yield less
accurate parameters.
What if we want to solve the M-estimator in (1) with a million variables? Note that the number of
parameters in (1) is quadratic in the number of variables  so that for a million variables  we would

(cid:80)n

(1)

n

1

have a trillion parameters. There has been considerable recent interest in such “Big Data” problems
involving large-scale optimization: these however are either targeted to “big-n” problems with a lot
of samples  unlike the constraint of “big-p” with a large number of variables in our problem  or are
based on large-scale distributed and parallel frameworks  which require a cluster of processors  as
well as software infrastructure to run the programs over such clusters. At least one caveat with such
large-scale distributed frameworks is that they would be less amenable to exploratory data analysis
by “lay users” of such GMRFs. Here we ask the following ambitious but simple question: can we
solve the M-estimator in (1) with a million variables using a single machine with bounded memory?
This might not seem like a viable task at all in general  but note that the optimization problem in (1)
arises from a very structured statistical estimation problem: can we leverage the underlying structure
to be able to solve such an ultra-large-scale problem?
In this paper  we propose a new solver  BIGQUIC  to solve the (cid:96)1-regularized Gaussian MLE prob-
lem with extremely high dimensional data. Our method can solve one million dimensional problems
with 1000 billion variables using a single machine with 32 cores and 32G memory. Our proposed
method is based on the state-of-the-art framework of QUIC [9  8]. The key bottleneck with QUIC
stems from the memory required to store the gradient W = X−1 of the iterates X  which is a dense
p× p matrix  and the computation of the log-determinant function of a p× p matrix. A starting point
to reduce the memory footprint is to use sparse representations for the iterates X and compute the
elements of the empirical covariance matrix S on demand from the sample data points. In addition
we also have to avoid the storage of the dense matrix X−1 and perform intermediate computa-
tions involving functions of such dense matrices on demand. These naive approaches to reduce the
memory however would considerably increase the computational complexity  among other caveats 
which would make the algorithm highly impractical.
To address this  we present three key innovations. Our ﬁrst is to carry out the coordinate descent
computations in a blockwise manner  and by selecting the blocks very carefully using an automated
clustering scheme  we not only leverage sparsity of the iterates  but help cache computations suit-
ably. Secondly  we reduce the computation of the log-determinant function to linear equation solving
using the Schur decomposition that also exploits the symmetry of the matrices in question. Lastly 
since the Hessian computation is a key bottleneck in the second-order method  we compute it inex-
actly. We show that even with these modiﬁcations and inexact computations  we can still guarantee
not only convergence of our overall procedure  but can easily control the degree of approximation
of Hessian to achieve super-linear or even quadratic convergence rates. Inspite of our low-memory
footprint  these innovations allow us to beat the state of the art DC-QUIC algorithm (which has
no memory limits) in computational complexity even on medium-size problems of a few thousand
variables. Finally  we show how to parallelize our method in a multicore shared memory system.
The paper is organized as follows. In Section 2  we brieﬂy review the QUIC algorithm and outline
the difﬁculties of scaling QUIC to million dimensional data. Our algorithm is proposed in Section 3.
We theoretically analyze our algorithm in Section 4  and present experimental results in Section 5.
2 Difﬁculties in scaling QUIC to million dimensional data
Our proposed algorithm is based on the framework of QUIC [9]; which is a state of the art procedure
for solving (1)  based on a second-order optimization method. We present a brief review of the
algorithm  and then explain the key bottlenecks that arise when scaling it to million dimensions.
Since the objective function of (1) is non-smooth  we can separate the smooth and non-smooth part
by f(X) = g(X) + h(X)  where g(X) = − log det X + tr(SX) and h(X) = λ(cid:107)X(cid:107)1.
QUIC is a second-order method that iteratively solves for a generalized Newton direction using
coordinate descent; and then descends using this generalized Newton direction and line-search. To
leverage the sparsity of the solution  the variables are partitioned into Sf ixed and Sf ree sets:

Xij ∈ Sf ixed if |∇ijg(X)| ≤ λij  and Xij = 0  Xij ∈ Sf ree otherwise.

(2)
Only the free set Sf ree is updated at each Newton iteration  reducing the number of variables to be
updated to m = |Sf ree|  which is comparable to (cid:107)X∗(cid:107)0  the sparsity of the solution.
Difﬁculty in Approximating the Newton Direction. Let us ﬁrst consider the generalized Newton
direction for (1):
(3)

{¯gXt(D) + h(Xt + D)} 

Dt = arg min
D

¯gXt(D) = g(Xt) + tr(∇g(Xt)T D) +

where
(4)
In our problem ∇g(Xt) = S − X−1
  where ⊗ denotes the Kronecker
product of two matrices. When Xt is sparse  the Newton direction computation (3) can be solved

1
vec(D)T∇2g(Xt) vec(D).
2
t ⊗ X−1

and ∇2g(X) = X−1

t

t

2

; using this to compute a = W 2

t

Dij ← Dij − c + S(c − b/a  λij/a) 

ij + WiiWjj  b = Sij − Wij + wT

efﬁciently by coordinate descent [9]. The obvious implementation calls for the computation and
storage of Wt = X−1
i Dwj  and
c = Xij + Dij. Armed with these quantities  the coordinate descent update for variable Dij takes
the form:
(5)
where S(z  r) = sign(z) max{|z| − r  0} is the soft-thresholding function.
The key computational bottleneck here is in computing the terms wT
i Dwj  which take O(p2)
time when implemented naively. To address this  [9] proposed to store and maintain U = DW  
which reduced the cost to O(p) ﬂops per update. However  this is not a strategy we can use when
dealing with very large data sets: storing the p by p dense matrices U and W in memory would
be prohibitive. The straightforward approach is to compute (and recompute when necessary) the
elements of W on demand  resulting in O(p2) time complexity.
Our key innovation to address this is a novel block coordinate descent scheme  detailed in Section
3.1  that also uses clustering to strike a balance between memory use and computational cost while
exploiting sparsity. The result is a procedure with comparable wall-time to that of QUIC on mid-
sized problems and can scale up to very large problem instances that the original QUIC could not.
Difﬁculty in the Line Search Procedure. After ﬁnding the generalized Newton direction Dt 
QUIC then descends using this direction after a line-search via Armijo’s rule. Speciﬁcally  it selects
the largest step size α ∈ {β0  β1  . . .} such that X + αDt is (a) positive deﬁnite  and (b) satisﬁes
the following sufﬁcient decrease condition:

f(X + αD∗) ≤ f(X) + ασδ  δ = tr(∇g(X)T D∗) + (cid:107)X + D∗(cid:107)1 − (cid:107)X(cid:107)1.

(6)
The key computational bottleneck is checking positive deﬁniteness (typically by computing the
smallest eigenvalue)  and the computation of the determinant of a sparse matrix with dimension
that can reach a million. As we show in Appendix 6.4  the time and space complexity of classical
sparse Cholesky decomposition generally grows quadratically to dimensionality even when ﬁxing
the number of nonzero elements in the matrix  so it is nontrivial to address this problem. Our key
innovation  detailed in Section 3.2  is an efﬁcient procedure that checks both conditions (a) and (b)
above using Schur complements and sparse linear equation solving. The computation only uses
memory proportional to the number of nonzeros in the iterate.
Many other difﬁculties arise when dealing with large sparse matrices in the sparse inverse covairance
problem. We present some of them in Appendix 6.5.
3 Our proposed algorithm
In this section  we describe our proposed algorithm  BIGQUIC  with the key innovations mentioned
in the previous section. We assume that the iterates Xt have m nonzero elements  and that each
iterate is stored in memory using a sparse format. We denote the size of the free set by s and observe
that it is usually very small and just a constant factor larger than m∗  the number of nonzeros in the
ﬁnal solution [9]. Also  the sample covariance matrix is stored in its factor form S = Y Y T   where
Y is the normalized sample matrix. We now discuss a crucial element of BIGQUIC  our novel block
coordinate descent scheme for solving each subproblem (3).

3.1 Block Coordinate Descent method

The most expensive step during the coordinate descent update for Dij is the computation of
i Dwj  where wi is the i-th column of W = X−1; see (5). It is not possible to compute W = X−1
wT
with Cholesky factorization as was done in [9]  nor can it be stored in memory. Note that wi is the
solution of the linear system Xwi = ei. We thus use the conjugate gradient method (CG) to com-
pute wi  leveraging the fact that X is a positive deﬁnite matrix. This solver requires only matrix
vector products  which can be efﬁciently implemented for the sparse matrix X. CG has time com-
plexity O(mT )  where T is the number of iterations required to achieve the desired accuracy.
Vanilla Coordinate Descent. A single step of coordinate descent requires the solution of two
linear systems Xwi = ei and Xwj = ej which yield the vectors wi  wj  and we can then compute
i Dwj. The time complexity for each update would require O(mT +s) operations  and the overall
wT
complexity will be O(msT +s2) for one full sweep through the entire matrix. Even when the matrix
is sparse  the quadratic dependence on nonzero elements is expensive.
Our Approach: Block Coordinate Descent with memory cache scheme.
In the following we
present a block coordinate descent scheme that can accelerate the update procedure by storing and

3

S¯z ¯q

i uj = Pij + wT
Sz

uSz + wT
Sq

reusing more results of the intermediate computations. The resulting increased memory use and
speedup is controlled by the number of blocks employed  that we denote by k.
Assume that only some columns of W are stored in memory. In order to update Dij  we need both
wi and wj; if either one is not directly available  we have to recompute it by CG and we call this
a “cache miss”. A good update sequence can minimize the cache miss rate. While it is hard to ﬁnd
the optimal sequence in general  we successfully applied a block by block update sequence with a
careful clustering scheme  where the number of cache misses is sufﬁciently small.
Assume we pick k such that we can store p/k columns of W (p2/k elements) in memory. Suppose
we are given a partition of N = {1  . . .   p} into k blocks  S1  . . .   Sk. We divide matrix D into
k × k blocks accordingly. Within each block we run Tinner sweeps over variables within that block 
and in the outer iteration we sweep through all the blocks Touter times. We use the notation WSq to
denote a p by |Sq| matrix containing columns of W that corresponds to the subset Sq.
Coordinate descent within a block. To update the variables in the block (Sz  Sq) of D  we ﬁrst
compute WSz and WSq by CG and store it in memory  meaning that there is no cache miss during
the within-block coordinate updates. With Usq = DWSq maintained  the update for Dij can be
i uj when i ∈ Sz and j ∈ Sq. After updating each Dij to Dij + µ  we can maintain
computed by wT
USq by

Uit ← Uit + µWjt  Ujt ← Ujt + µWit  ∀t ∈ Sq.
The above coordinate update computations cost only O(p/k) operations because we only update a
subset of the columns. Observe that Urt never changes when r /∈ {Sz ∪ Sq}.
Therefore  we can use the following arrangement to further reduce the time complexity. Before
(uj)S¯z ¯q for all (i  j)
running coordinate descent for the block we compute and store Pij = (wi)T
in the free set of the current block  where S¯z ¯q = {i | i /∈ Sz and i /∈ Sq}. The term wT
i uj for
updating Dij can then be computed by wT
uSq. With this trick  each
coordinate descent step within the block only takes O(p/k) time  and we only need to store USz Sq 
which only requires O(p2/k2) memory. Computing Pij takes O(p) time for each i  j  so if we
update each coordinate Tinner times within a block  the time complexity is O(p + Tinnerp/k) and the
amortized cost per coordinate update is only O(p/Tinner + p/k). This time complexity suggests that
we should run more iterations within each block.
To go through all the blocks  each time we select a z ∈
Sweeping through all the blocks.
{1  . . .   k} and updates blocks (Sz  S1)  . . .   (Sz  Sk). Since all of them share {wi | i ∈ Sz}  we
ﬁrst compute them and store in memory. When updating an off-diagonal block (Sz  Sq)  if the free
sets are dense  we need to compute and store {wi | i ∈ Sq}. So totally each block of W will be
computed k times. The total time complexity becomes O(kpmT )  where m is number of nonzeros
in X and T is number of conjugate gradient iterations. Assume the nonzeros in X is close to the
size of free set (m ≈ s)  then each coordinate update costs O(kpT ) ﬂops.
Selecting the blocks using clustering. We now show that a careful selection of the blocks using
a clustering scheme can lead to dramatic speedup for block coordinate descent. When updating
variables in the block (Sz  Sq)  we would need the column wj only if some variable in {Dij | i ∈
Sz} lies in the free set. Leveraging this key observation  given two partitions Sz and Sq  we deﬁne
the set of boundary nodes as: B(Sz  Sq) ≡ {j | j ∈ Sq and ∃i ∈ Sz s.t. Fij = 1}  where the matrix
F is an indicator of the free set.
z(cid:54)=q |B(Sz  Sq)|.
z(cid:54)=q |B(Sz  Sq)| is mini-
mal. It appears to be hard to ﬁnd the partitioning that minimizes the number of boundary nodes.
However  we note that the number in question is bounded by the number of cross cluster edges:
Fij. This suggests the use of graph clustering algorithms  such as METIS
[10] or Graclus [5] which minimize the right hand side. Assuming that the ratio of between-cluster
edges to the number of total edges is r  we observe a reduced time complexity of O((p+rm)T ) when
computing elements of W   and r is very small in real datasets. In real datasets  when we converge
to very sparse solutions  more than 95% of edges are in the diagonal blocks. In case of the fMRI
dataset with p = 228483  we used 20 blocks  and the total number of boundary nodes were only
|B| = 8697. Compared to block coordinate descent with random partition  which generally needs
to compute 228483 × 20 columns  the clustering resulted in the computation of 228483 + 8697
columns  thus achieved an almost 20 times speedup. In Appendix 6.6 we also discuss additional
beneﬁts of the graph clustering algorithm that results in accelerated convergence.

The number of columns to be computed in one sweep is then given by p +(cid:80)
Therefore  we would like to ﬁnd a partition {S1  . . .   Sk} for which (cid:80)
B(Sz  Sq) <(cid:80)

i∈Sz j∈Sq

4

p(cid:88)

3.2 Line Search

The line search step requires an efﬁcient and scalable procedure that computes log det(A) and
checks the positive deﬁniteness of a sparse matrix A. We present a procedure that has complex-
ity of at most O(mpT ) where T is the number of iterations used by the sparse linear solver. We
note that computing log det(A) for a large sparse matrix A for which we only have a matrix-vector
multiplication subroutine available is an interesting subproblem on its own and we expect that nu-
merous other applications may beneﬁt from the approach presented below. The following lemma
can be proved by induction on p:

is a partitioning of an arbitrary p × p matrix  where a is a scalar
Lemma 1. If A =
and b is a p − 1 dimensional vector then det(A) = det(C)(a − bT C−1b). Moreover  A is positive
deﬁnite if and only if C is positive deﬁnite and (a − bT C−1b) > 0.

b C 

(cid:18) a bT

(cid:19)

The above lemma allows us to compute the determinant by reducing it to solving linear systems;
and also allows us to check positive-deﬁniteness. Applying Lemma 1 recursively  we get

log det A =

log(Aii − AT

(i+1):p iA−1

(i+1):p (i+1):pA(i+1):p i) 

(7)

i=1

where each Ai1:i2 j1:j2 denotes a submatrix of A with row indexes i1  . . .   i2 and column indexes
j1  . . .   j2. Each A−1
(i+1):p (i+1):pA(i+1):p i in the above formula can be computed as the solution of
a linear system and hence we can avoid the storage of the (dense) inverse matrix. By Lemma 1  we
can check the positive deﬁniteness by verifying that all the terms in (7) are positive deﬁnite. Notice
that we have to compute (7) in a reverse order (i = p  . . .   1) to avoid the case that A(i+1):p (i+1):p
is non positive deﬁnite.

3.3 Summary of the algorithm
In this section we present BIGQUIC as Algorithm 1. The detailed time complexity analysis are
presented in Appendix 6.7. In summary  the time needed to compute the columns of W in block
coordinate descent  O((p + |B|)mT Touter)  dominates the time complexity  which underscores the
importance of minimizing the number of boundary nodes |B| via our clustering scheme.

Algorithm 1: BIGQUIC algorithm

Input
Output: Sequence {Xt} that converges to X∗.

: Samples Y   regularization parameter λ  initial iterate X0

Compute Wt = X−1
Run graph clustering algorithm based on absolute values on free set.
for sweep = 1  . . .   Touter do

column by column  partition the variables into free and ﬁxed sets.

t

1 for t = 0  1  . . . do
2
3
4
5
6
7
8
9
10
11

for s = 1  . . .   k do

Compute WSs by CG.
for q = 1  . . .   k do

Identify boundary nodes Bsq := B(Ss  Sq) ⊂ Sq (only need if s (cid:54)= q)
Compute WBsq for boundary nodes (only need if s (cid:54)= q).
Compute UBsq  and Pij for all (i  j) the current block.
Conduct coordinate updates.

12

Find the step size α by the method proposed in Section 3.2.

Parallelization. While our method can run well on a single machine with a single core  here
we point out components of our algorithm that can be “embarrassingly” parallelized on any single
machine with multiple cores (with shared memory). We ﬁrst note that we can obtain a good starting
point for our algorithm by applying the divide-and-conquer framework proposed in [8]: this divides
the problem into k subproblems  which can then be independently solved in parallel. Consider the
steps of our Algorithm 1 in BIGQUIC. In step 2  instead of computing columns of W one by one 
we can compute t rows of W at a time  and parallelize these t jobs. A similar trick can be used in
step 6 and 9. In step 3  we use the multi-core version of METIS (ParMETIS) for graph clustering.

5

In step 8 and 10  the computations are naturally independent. In step 15  we compute each term
in (7) independently and abort if any of the processes report non-positive deﬁniteness. The only
sequential part is the coordinate update in step 11  but note  (see Section 3.1)  that we have reduced
the complexity of this step from O(p) in QUIC to O(p/k).
4 Convergence Analysis
In this section  we present two main theoretical results. First  we show that our algorithm converges
to the global optimum even with inaccurate Hessian computation. Second  we show that by a careful
control of the error in the Hessian computation  BIGQUIC can still achieve a quadratic rate of
convergence in terms of Newton iterations. Our analysis differs from that in QUIC [9]  where the
computations are all assumed to be accurate. [11] also provides a convergence analysis for general
proximal Newton methods  but our algorithm with modiﬁcations such as ﬁxed/free set selection
does not exactly fall into their framework; moreover our analysis shows a quadratic convergence
rate  while they only show a super-linear convergence rate.
In the BIGQUIC algorithm  we compute wi in two places. The ﬁrst place is the gradient compu-
tation in the second term of (4)  where ∇g(X) = S − W . The second place is in the third term
of (4)  where ∇2g(X) = W ⊗ W . At the ﬁrst glance they are equivalent and can be computed
simultaneously  but it turns out that by carefully analysing the difference between two types of wi 
we can achieve much faster convergence  as discussed below.
The key observation is that we only require the gradient Wij for all (i  j) ∈ Sf ree to conduct
coordinate descent updates. Since the free set is very sparse and can ﬁt in memory  those Wij only
need to be computed once and stored in memory. On the other hand  the computation of wT
i Dwj
corresponds to the Hessian computation  and we need two columns for each coordinate update 
which has to be computed repeatedly.
It is easy to produce an example where the algorithm converges to a wrong point when the gradient
computation is not accurate  as shown in Figure 5(b) (in Appendix 6.5). Luckily  based on the above
analysis the gradient only needs to be computed once per Newton iteration  so we can compute it
with high precision. On the other hand  wi for the Hessian has to be computed repeatedly  so we do
not want to spend too much time to compute each of them accurately. We deﬁne ˆHt = ˆWt⊗ ˆWt to be
the approximated Hessian matrix  and derive the following theorem to show that even if Hessian is
inaccurate  BIGQUIC still converges to the global optimum. Notice that our proof covers BIGQUIC
algorithm with ﬁxed/free set selection  and the only assumption is that subproblem (3) is solved
exactly for each Newton iteration; it is the future work to consider the case where subproblems are
solved approximately.
Theorem 1. For solving (1)  if ∇g(X) is computed exactly and ¯ηI (cid:23) ˆHt (cid:23) ηI for some constant
¯η  η > 0 at every Newton iteration  then BIGQUIC converges to the global optimum.

The proof is in Appendix 6.1. Theorem 1 suggests that we do not need very accurate Hessian compu-
tation for convergence. To have super-linear convergence rate  we require the Hessian computation
to be more and more accurate as Xt approaches X∗. We ﬁrst introduce the following notion of
minimum norm subgradient to measure the optimality of X:

(cid:26)∇ijg(X) + sign(Xij)λij

gradS

ij f(X) =

sign(∇ijg(X)) max(|∇ijg(X)| − λij  0)

if Xij (cid:54)= 0 
if Xij = 0.

The following theorem then shows that if we compute Hessian more and more accurately  BIGQUIC
will have a super-linear or even quadratic convergence rate.
Theorem 2. When applying BIGQUIC to solve (1)  assume ∇g(Xt) is exactly computed and
∇2g(Xt) is approximated by Ht  and the following condition holds:

(cid:64)(i  j) such that X∗

ij = 0 and |∇ijg(X∗)| = λ.

Then (cid:107)Xt+1 − X∗(cid:107) = O((cid:107)Xt − X∗(cid:107)1+p) as k → ∞ for 0 < p ≤ 1 if and only if

(cid:107) ˆHt − ∇2g(Xt)(cid:107) = O((cid:107) gradS(Xt)(cid:107)p) as k → ∞.

(8)

(9)

The proof is in Appendix 6.2. The assumption in (8) can be shown to be satisﬁed with very high
probability (and was also satisﬁed in our experiments). Theorem 2 suggests that we can achieve
super-linear  or even quadratic convergence rate by a careful control of the approximated Hessian
ˆHt. In the BIGQUIC algorithm  we can further control (cid:107) ˆHt−∇2g(Xt)(cid:107) by the residual of conjugate

6

(a) Comparison on chain graph.

(b) Comparison on random graph.

(c) Comparison on fmri data.

Figure 1: The comparison of scalability on three types of graph structures. In all the experiments  BIGQUIC
can solve larger problems than QUIC even with a single core  and using 32 cores BIGQUIC can solve million
dimensional data in one day.
gradient solvers to achieve desired convergence rate. Suppose the residual is bi = X ˆwi − ei for
each i = 1  . . .   p  and Bt = [b1b2 . . . bp] is a collection of the residuals at the t-th iteration. The
following theorem shows that we can control the convergence rate by controlling the norm of Bt.
Theorem 3. In the BIGQUIC algorithm  if the residual matrix (cid:107)Bt(cid:107) = O((cid:107) gradS(Xt)(cid:107)p) for
some 0 < p ≤ 1 as t → ∞  then (cid:107)Xt+1 − X∗(cid:107) = O((cid:107)Xt − X∗(cid:107)1+p) as t → ∞.

i i = 1.25.

i i−1 = −0.5 and Σ−1

The proof is in Appendix 6.3. Since gradS(Xt) can be easily computed without additional cost  and
residuals B can be naturally controlled when running conjugate gradient  we can easily control the
asymptotic convergence rate in practice.
5 Experimental Results
In this section  we show that our proposed method BIGQUIC can scale to high-dimensional datasets
on both synthetic data and real data. All the experiments are run on a single computing node with 4
Intel Xeon E5-4650 2.7GHz CPUs  each with 8 cores and 32G memory.
Scalability of BIGQUIC on high-dimensional datasets.
In the ﬁrst set of experiments  we show
BIGQUIC can scale to extremely high dimensional datasets. We conduct experiments on the fol-
lowing synthetic and real datasets:
(1) Chain graphs: the ground truth precision matrix is set to be Σ−1
(2) Graphs with random pattern: we use the procedure mentioned in Example 1 in [12] to generate
random pattern. When generating the graph  we assume there are 500 clusters  and 90% of the edges
are within clusters. We ﬁx the average degree to be 10.
(3) FMRI data: The original dataset has dimensionality p = 228  483 and n = 518. For scalability
experiments  we subsample various number of random variables from the whole dataset.
We use λ = 0.5 for chain and random Graph so that number of recovered edges is close to the ground
truth  and set number of samples n = 100. We use λ = 0.6 for the fMRI dataset  which recovers
a graph with average degree 20. We set the stopping condition to be gradS(Xt) < 0.01(cid:107)Xt(cid:107)1. In
all of our experiments  number of nonzeros during the optimization phase do not exceed 5(cid:107)X∗(cid:107)0 in
intermediate steps  therefore we can always store the sparse representation of Xt in memory. For
BIGQUIC  we set blocks k to be the smallest number such that p/k columns of W can ﬁt into
32G memory. For both QUIC and BIGQUIC  we apply the divide and conquer method proposed
in [8] with 10-clusters to get a better initial point. The results are shown in Figure 1. We can see
that BIGQUIC can solve one million dimensional chain graphs and random graphs in one day  and
handle the full fMRI dataset in about 5 hours.
More interestingly  even for dataset with size less than 30000  where p2 size matrices can ﬁt in
memory  BIGQUIC is faster than QUIC by exploiting the sparsity. Figure 2 shows an example on
a sampled fMRI dataset with p = 20000  and we can see BIGQUIC outperforms QUIC even when
using a single core. Also  BIGQUIC is much faster than other solvers  including Glasso [7] and
ALM [17]. Figure 3 shows the speedup under a multicore shared memory machine. BIGQUIC can
achieve about 14 times speedup using 16 cores  and 20 times speedup when using 32 cores.
FMRI dataset. An extensive resting state fMRI dataset from a single individual was analyzed in or-
der to test BIGQUIC on real-world data. The data (collected as part of the MyConnectome project:
http://www.myconnectome.org) comprised 36 resting fMRI sessions collected across dif-
ferent days using whole-brain multiband EPI acquisition  each lasting 10 minutes (TR=1.16 secs 
multiband factor=4  TE = 30 ms  voxel size = 2.4 mm isotropic  68 slices  518 time points). The

7

Figure 3: The speedup of BIGQUIC when using
multiple cores.

Figure 2: Comparison on fMRI data with p =
20000 (the maximum dimension that state-of-the-
art softwares can handle).
data were preprocessed using FSL 5.0.2  including motion correction  scrubbing of motion frames 
registration of EPI images to a common high-resolution structural image using boundary-based reg-
istration  and afﬁne transformation to MNI space. The full brain mask included 228 483 voxels.
After motion scrubbing  the dataset included a total 18 435 time points across all sessions.
BIGQUIC was applied to the full dataset: for the ﬁrst time  we can learn a GMRF over the entire
set of voxels  instead of over a smaller set of curated regions or supervoxels. Exploratory analyses
over a range of λ values suggested that λ = 0.5 offered a reasonable level of sparsity. The result-
ing graph was analyzed to determine whether it identiﬁed neuroscientiﬁcally plausible networks.
Degree was computed for each vertex; high-degree regions were primarily found in gray matter re-
gions  suggesting that the method successfully identiﬁed plausible functional connections (see left
panel of Figure 4). The structure of the graph was further examined in order to determine whether
the method identiﬁed plausible network modules. Modularity-based clustering [1] was applied to
the graph  resulting in 60 modules that exceeded the threshold size of 100 vertices. A number of
neurobiologically plausible resting-state networks were identiﬁed  including “default mode” and
sensorimotor networks (right panel of Figure 4). In addition  the method identiﬁed a number of
structured coherent noise sources (i.e. MRI artifacts) in the dataset. For both neurally plausible and
artifactual modules  the modules detected by BIGQUIC are similar to those identiﬁed using inde-
pendent components analysis on the same dataset  without the need for the extensive dimensionality
reduction (without statistical guarantees) inherent in such techniques.

Figure 4: (Best viewed in color) Results from BIGQUIC analyses of resting-state fMRI data. Left panel: Map
of degree distribution across voxels  thresholded at degree=20. Regions showing high degree were generally
found in the gray matter (as expected for truly connected functional regions)  with very few high-degree voxels
found in the white matter. Right panel: Left-hemisphere surface renderings of two network modules obtained
through graph clustering. Top panel shows a sensorimotor network  bottom panel shows medial prefrontal 
posterior cingulate  and lateral temporoparietal regions characteristic of the “default mode” generally observed
during the resting state. Both of these are commonly observed in analyses of resting state fMRI data.
Acknowledgments
This research was supported by NSF grant CCF-1320746 and NSF grant CCF-1117055. C.-J.H.
also acknowledges the support of IBM PhD fellowship. P.R. acknowledges the support of ARO via
W911NF-12-1-0390 and NSF via IIS-1149803  DMS-1264033. R.P. acknowledges the support of
ONR via N000140710116 and the James S. McDonnell Foundation.

8

References
[1] V. D. Blondel  J.-L. Guillaume  R. Lambiotte  and E. Lefebvre. Fast unfolding of community

hierarchies in large networks. J. Stat Mech  2008.

[2] T. Cai  W. Liu  and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix

estimation. Journal of American Statistical Association  106:594–607  2011.

[3] A. d’Aspremont  O. Banerjee  and L. E. Ghaoui. First-order methods for sparse covariance

selection. SIAM Journal on Matrix Analysis and its Applications  30(1):56–66  2008.

[4] R. S. Dembo  S. C. Eisenstat  and T. Steihaug. Inexact Newton methods. SIAM J. Numerical

Anal.  19(2):400–408  1982.

[5] I. S. Dhillon  Y. Guan  and B. Kulis. Weighted graph cuts without eigenvectors: A multi-
level approach. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 
29:11:1944–1957  2007.

[6] J. Duchi  S. Gould  and D. Koller. Projected subgradient methods for learning sparse Gaus-

sians. UAI  2008.

[7] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graph-

ical lasso. Biostatistics  9(3):432–441  July 2008.

[8] C.-J. Hsieh  I. S. Dhillon  P. Ravikumar  and A. Banerjee. A divide-and-conquer method for

sparse inverse covariance estimation. In NIPS  2012.

[9] C.-J. Hsieh  M. A. Sustik  I. S. Dhillon  and P. Ravikumar. Sparse inverse covariance estimation

using quadratic approximation. 2013.

[10] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular

graphs. SIAM J. Sci. Comput.  20(1):359–392  1999.

[11] J. D. Lee  Y. Sun  and M. A. Saunders. Proximal newton-type methods for minimizing com-

posite functions. In NIPS  2012.

[12] L. Li and K.-C. Toh. An inexact interior point method for (cid:96)1-reguarlized sparse covariance

selection. Mathematical Programming Computation  2:291–315  2010.

[13] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for

large-scale graphical lasso. Journal of Machine Learning Research  13:723–736  2012.

[14] N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with the

lasso. Annals of Statistics  34:1436–1462  2006.

[15] P. Olsen  F. Oztoprak  J. Nocedal  and S. Rennie. Newton-like methods for sparse inverse
covariance estimation. Technical report  Optimization Center  Northwestern University  2012.
[16] B. Rolfs  B. Rajaratnam  D. Guillot  A. Maleki  and I. Wong. Iterative thresholding algorithm

for sparse inverse covariance estimation. In NIPS  2012.

[17] K. Scheinberg  S. Ma  and D. Goldfarb. Sparse inverse covariance selection via alternating

linearization methods. NIPS  2010.

[18] K. Scheinberg and I. Rish. Learning sparse Gaussian Markov networks using a greedy coor-
dinate ascent approach. In J. Balczar  F. Bonchi  A. Gionis  and M. Sebag  editors  Machine
Learning and Knowledge Discovery in Databases  volume 6323 of Lecture Notes in Computer
Science  pages 196–212. Springer Berlin / Heidelberg  2010.

[19] D. M. Witten  J. H. Friedman  and N. Simon. New insights and faster computations for the

graphical lasso. Journal of Computational and Graphical Statistics  20(4):892–900  2011.

9

,Cho-Jui Hsieh
Matyas Sustik
Inderjit Dhillon
Pradeep Ravikumar
Russell Poldrack
Koosha Khalvati
Seongmin Park
Jean-Claude Dreher
Rajesh Rao
Amirhossein Taghvaei
Jin Kim
Prashant Mehta