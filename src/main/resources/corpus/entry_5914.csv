2018,Wavelet regression and additive models for irregularly spaced data,We present a novel approach for nonparametric regression using wavelet basis functions. Our proposal  waveMesh  can be applied to non-equispaced data with sample size not necessarily a power of 2. We develop an efficient proximal gradient descent algorithm for computing the estimator and establish adaptive minimax convergence rates. The main appeal of our approach is that it naturally extends to additive and sparse additive models for a potentially large number of covariates. We prove minimax optimal convergence rates under a weak compatibility condition for sparse additive models. The compatibility condition holds when we have a small number of covariates. Additionally  we establish convergence rates for when the condition is not met. We complement our theoretical results with empirical studies comparing waveMesh to existing methods.,Wavelet regression and additive models for

irregularly spaced data

Asad Haris∗

Department of Biostatistics
University of Washington

Seattle  WA 98195
aharis@uw.edu

Noah Simon

Department of Biostatistics
University of Washington

Seattle  WA 98195
nrsimon@uw.edu

Ali Shojaie

Department of Biostatistics
University of Washington

Seattle  WA 98195
ashojaie@uw.edu

Abstract

We present a novel approach for nonparametric regression using wavelet basis
functions. Our proposal  waveMesh  can be applied to non-equispaced data with
sample size not necessarily a power of 2. We develop an efﬁcient proximal gradient
descent algorithm for computing the estimator and establish adaptive minimax
convergence rates. The main appeal of our approach is that it naturally extends to
additive and sparse additive models for a potentially large number of covariates.
We prove minimax optimal convergence rates under a weak compatibility condition
for sparse additive models. The compatibility condition holds when we have a
small number of covariates. Additionally  we establish convergence rates for when
the condition is not met. We complement our theoretical results with empirical
studies comparing waveMesh to existing methods.

Introduction

1
We consider the canonical task of estimating a regression function  f  from observations {(xi  yi) :
i = 1  . . .   n}  with xi ∈ [0  1]p  yi ∈ R and yi = f (xi) + εi (i = 1  . . .   n)  where εi are
independent  mean 0  sub-Gaussian random variables. A popular approach for estimating f is to use
linear combinations of a pre-speciﬁed set of basis functions  e.g.  polynomials  splines [Wahba  1990] 
wavelets [Daubechies  1992]  or other systems [ ˇCencov  1962]. The weights  or coefﬁcients  in such
a linear combination are often determined using some form of penalized regression. In this paper 
we focus on estimators that use wavelets. Wavelet-based estimators have compelling theoretical
properties. However  a number of issues have limited their adaptation in many non-parametric
applications. The approach proposed in this paper overcomes these issues. Throughout the paper  we
assume basic knowledge of wavelet methods though some key points will be reviewed. For a detailed
introduction to wavelets  see books by Daubechies [1992]  Percival and Walden [2006]  Vidakovic
[2009]  Nason [2010]  Ogden [2012].
Wavelets are a system of orthonormal basis functions for L2([0  1]). Wavelets are popular for
representing functions because they allow time and frequency localization [Daubechies  1990] as
opposed to  say  Fourier bases  which allow only frequency localization. Additionally  wavelet-based

∗Mailing address: Box 357232  University of Washington  Seattle  WA 98195-7232

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

methods are computationally efﬁcient. The main ingredient of wavelet regression is the discrete
wavelet transform (DWT) and its inverse (IDWT) which can be computed in O(n) operations [Mallat 
1989]. Unfortunately  traditional wavelet methods require stringent conditions on the data  speciﬁcally
that xi = i/n with n = 2J for some integer J. This is not a problem in many signal processing
applications with regularly sampled signals; however  in general non-parametric regression  this
condition will rarely be satisﬁed. A simple solution for general data types is to ignore irregular
spacing of data [Cai and Brown  1999  Sardy et al.  1999] and/or artiﬁcially extend the signal such
that n = 2J [Strang and Nguyen  1996  Ch. 8]. Other solutions include transformations [Cai and
Brown  1998  Pensky and Vidakovic  2001] or interpolation [Hall and Turlach  1997  Kovac and
Silverman  2000  Antoniadis and Fan  2001] of the data to a regular grid of size 2J. The literature
on univariate wavelet methods is quite extensive and cannot be adequately discussed within this
manuscript. In contrast  the literature on wavelet methods for multiple covariates is rather limited 
particularly when the number of covariates is large.
For the multivariate settings with xi ∈ [0  1]p for p ≥ 2  we consider estimating an additive model 

j (cid:98)fj (xij). Additive models naturally extend linear models to capture non-linear

i.e.  (cid:98)f (xi) =(cid:80)

conditional relationships  while retaining some interpretability; they also do not suffer from the
curse of dimensionality. Despite these beneﬁts  wavelet-based additive models have received limited
attention. This is most likely because data with multiple covariates are rarely available on a regular
grid of size n = 2J. Sardy and Tseng [2004] ﬁt additive wavelet models by treating the data as if
regularly spaced; however  they do not discuss the case when n is not a power of 2. A number of
proposals transform the data to a regular grid [Amato and Antoniadis  2001  Zhang and Wong  2003 
Grez and Vidakovic  2018]. However  to do this  the density of the covariates must be estimated 
which unnecessarily invokes the curse of dimensionality. In addition  to the best of our knowledge 
there are no wavelet-based methods for ﬁtting additive models in high dimensions (when p > n) that

induce sparsity  i.e.  for many j  give a solution with (cid:98)fj ≡ 0.

In this paper  we give a simple proposal that effectively extends wavelet-based methods to non-
parametric modeling with a potentially large number of covariates. We present an interpolation-based
approach for dealing with irregularly spaced data when n is not necessarily a power of 2. However 
unlike existing interpolation methods  we do not transform the raw data (xi  yi). As a result  our
method naturally extends to additive and sparse additive models. We also propose a penalized
estimation framework to induce sparsity in high dimensions. We develop a proximal gradient descent
method for computation of our estimator  which leverages fast algorithms for DWT and sparse matrix
multiplication. Furthermore  we establish adaptive minimax convergence rates (up to a log n factor)
similar to that of existing wavelet methods for regularly spaced data. We also establish convergence
rates for our (sparse) additive proposal for a potentially large number of covariates. We discuss an
extension of our proposal to general convex loss functions  and a weighted variation of our penalty
which exhibits improved performance.
In Section 2 we present our univariate  additive and sparse additive proposals. The univariate case
(p = 1) is mainly presented to motivate our proposal. We also present our main algorithm for
computing the estimator. We establish convergence rates of our estimators in Section 3  and present
empirical studies in Section 4. Concluding remarks are given in Section 5.

2 Methodology

2.1 Short background on wavelets

We begin with a quick review of wavelet methods for nonparametric regression covering 3 main
ingredients: (1) wavelet basis functions  (2) the discrete wavelet transform (DWT) and  (3) shrinkage.
First  wavelets are a system of orthonormal basis functions for L2([0  1]) or L2(R). The bases are
generated by translations and dilations of special functions φ(·) and ψ(·) called the father and mother
wavelet  respectively. In greater detail  for any j0 ≥ 0  a function f ∈ L2([0  1]) can be written as

2j0−1(cid:88)

∞(cid:88)

2j−1(cid:88)

f (x) =

αj0kφj0k(x) +

βjkψjk(x) 

(1)

where

k=0

j=j0

k=0

φjk(x) = 2j/2φ(2jx − k)  ψjk(x) = 2j/2ψ(2jx − k).

2

j=j0

(cid:80)2j−1

k=0 αj0kφj0k(x) +(cid:80)J

are considered  i.e.  functions of the form f (x) =(cid:80)2j0−1

The coefﬁcients αj0k and βjk are called the father and mother wavelet coefﬁcients  respectively. The
index j is called the resolution level and j0 is the minimum resolution level. Different choices of
φ and ψ generate various wavelet families. Popular choices are Daubechies [Daubechies  1988] 
Coiﬂets [Daubechies  1993]  Meyer wavelets [Meyer  1985]  and Spline wavelets [Chui  1992]; for
an overview of wavelet families  see Ogden [2012]. Often functions with a truncated basis expansion
k=0 βjkψjk(x) 
for some J. For regular data with xi = i/n (i = 1  . . .   n) and n = 2J for some J  we can calculate
the vector f = [f (1/n)  f (2/n)  . . .   f (n/n)](cid:62) efﬁciently via our second ingredient described next.
f = W (cid:62)d  where d =(cid:0)αj00  . . .   αj02j0−1  βj00  βj01  . . .   βJ2J−1
Any vector f = [f (1/n)  f (2/n)  . . .   f (n/n)](cid:62)  for function f with truncated wavelet basis expan-
sion of order J  can be written as a linear combination of that truncated wavelet basis. In particular 
is the vector of wavelet coef-
Speciﬁcally  W is an orthogonal matrix with Wli ≈ √
ﬁcients  and the rows of W contain the corresponding wavelet basis functions evaluated at xi = i/n.
nφjk(i/n)  for some
l; the
n factor is due to convention in the literature and software implementation. By orthogonality 
d = W f; this transformation from f to its wavelet coefﬁcients via multiplication by W is known as
the discrete wavelet transform (DWT). The transformation from wavelet coefﬁcients to ﬁtted values 
via multiplication by W (cid:62) is known as the inverse discrete wavelet transform (IDWT). The DWT and
IDWT can be computed in O(n) operations via Mallat’s pyramid algorithm [Mallat  1989]. However 
this is only possible for n = 2J.

Finally  shrinkage is employed to obtain estimates of the form (cid:98)f = W (cid:62)(cid:98)d; for ease of exposition  we

will assume j0 = 0; i.e.  all except the ﬁrst element of d correspond to mother wavelet coefﬁcients.
Our methodology and theoretical results do not depend on the choice of j0. The wavelet shrinkage
estimator is given by

nψjk(i/n)  or Wli ≈ √

(cid:1)(cid:62)

√

(cid:107)y − W (cid:62)d(cid:107)2

2 + λ

|di| 

(2)

i=2

(cid:80)n
for a positive tuning parameter λ  and given data {(i/n  yi) ∈ R2 : i = 1  . . .   n}. The (cid:96)1 penalty 
i=2 |di| ≡ (cid:107)d−1(cid:107)1  shrinks the wavelet coefﬁcients and also induces sparsity; the sparsity is
motivated by the desirable parsimony property of wavelets: many functions in L2([0  1]) are sparse
deﬁne (cid:101)d = W y  the DWT of y. Then  (cid:98)d1 = (cid:101)d1 and (cid:98)di = sgn((cid:101)di)(|(cid:101)di| − 2λ)+ (i = 2  . . .   n) where
linear combinations of wavelet bases. The optimization problem (2) can be solved exactly as follows:

(x)+ = max(x  0). Thus  for regularly spaced data with n = 2J  wavelet bases provide an efﬁcient
nonparametric estimator. In the following subsection  we discuss some existing methods for dealing
with irregularly spaced data and present our novel proposal  waveMesh.

(cid:98)d ← arg min

d∈Rn

1
2

n(cid:88)

2.2 A novel interpolation scheme

The common approach to dealing with irregularly spaced data is to map the observed outcomes
{(xi  yi) ∈ [0  1] × R : i = 1  . . .   n} to approximate outcomes on the regular grid {(i/n  y(cid:48)
i) ∈ R2 :
i = 1  . . .   K} for K = 2J for some integer J  via either interpolation or transformation of the data.
The novelty of our approach is a reversal of the direction of interpolation  i.e.  interpolation from ﬁtted
values on the regular grid i/K (i = 1  . . .   K)  to approximated ﬁts on the raw data xi (i = 1  . . .   n).
For our proposal  we require an interpolation scheme which can be written as a linear map. In greater
detail  for any function f evaluated at a regular grid  f = [f (1/K)  . . .   f (K/K)](cid:62) we require
R ∈ Rn×K. Linear interpolation is a natural choice where

an interpolation scheme (cid:101)f (·) such that [(cid:101)f (x1)  . . .  (cid:101)f (xn)](cid:62) = Rf for some interpolation matrix

for x ∈ (i/K  (i + 1)/K] and (cid:101)f (x) = f (1/K) for x ≤ 1/K; and the interpolation matrix is

+ f ((i + 1)/K)

(i + 1) − Kx
(i + 1) − i

Kx − i
(i + 1) − i

 

(cid:101)f (x) = f (i/K)
 1

Rij =

(j + 1) − Kxi
Kxi − (j − 1)
0

(3)

(4)

j = 1  xi ≤ 1/K
j = (cid:98)Kxi(cid:99)  xi ∈ (1/K  1]
j = (cid:100)Kxi(cid:101)  xi ∈ (1/K  1]
otherwise

.

3

(cid:98)d ← arg min

d∈RK

1
2

Our proposal  waveMesh  solves the following convex optimization problem

(cid:107)y − RW (cid:62)d(cid:107)2

2 + λ(cid:107)d−1(cid:107)1 

(5)

where K = 2(cid:100)log2 n(cid:101)  d−1 = [d2  . . .   dn](cid:62) ∈ RK−1  and W ∈ RK×K is the usual DWT matrix. To

evaluate the waveMesh estimate at a new point x ∈ R  one can use r(x)(cid:62)W (cid:62)(cid:98)d  where r is given by
Rn×K be the interpolation matrix corresponding to covariate j  i.e.  Rjf = [(cid:101)f (x1j)  . . .  (cid:101)f (xnj)](cid:62).

the chosen interpolation scheme. The advantage of waveMesh  over existing methods  is that it can
naturally be extended to additive models. Given data {(xi  yi) ∈ Rp+1 : i = 1  . . .   n}  let Rj ∈

Then  waveMesh can be extended to ﬁtting additive models by the following optimization problem:

d1 ... dp∈RK

(cid:98)d1  . . .   (cid:98)dp ← arg min
and (cid:98)f = [(cid:98)f (x1)  . . .  (cid:98)f (xn)](cid:62) = (cid:80)p
(cid:13)(cid:13)(cid:13)y − p(cid:88)
(cid:98)d1  . . .   (cid:98)dp ← arg min

d1 ... dp∈RK

1
2

j=1

j=1

1
2

(cid:13)(cid:13)(cid:13)2

2

+ λ

p(cid:88)

RjW (cid:62)dj

(cid:107)dj −1(cid:107)1 

(cid:13)(cid:13)(cid:13)y − p(cid:88)
j=1 (cid:98)fj = (cid:80)p
j=1 RjW (cid:62)(cid:98)dj. Finally  we can extend additive
(cid:13)(cid:13)(cid:13)2
(cid:3) .

(cid:2)λ1(cid:107)dj −1(cid:107)1 + λ2(cid:107)RjW (cid:62)dj(cid:107)2

RjW (cid:62)dj

p(cid:88)

(7)

(6)

j=1

+

2

waveMesh to ﬁtting sparse additive models for a potentially large number of covariates. This can be
achieved by adding a sparsity inducing penalty for each component fj as follows:

j=1

2.3 Algorithm for waveMesh and sparse additive waveMesh

+ tlP (d) 

2

(cid:13)(cid:13)(cid:13)2
(cid:17) − d
(cid:111) − W (cid:62)d

(cid:13)(cid:13)(cid:13)2

2

We now present a proximal gradient descent algorithm [Parikh and Boyd  2014] for solving the
optimization problem (5). For convex loss (cid:96) and penalty P   the proximal gradient descent algorithm
iteratively ﬁnds the minimizer of {(cid:96)(d) + P (d)} via the iteration:

(cid:13)(cid:13)(cid:13)(cid:16)

d(l+1) ← arg min
d∈RK

1
2

d(l) − tl∇(cid:96)(d(l))

(cid:13)(cid:13)(cid:13)(cid:110)

for a step-size tl > 0. The algorithm is guaranteed to converge as long as tl ≤ L−1 where L is the
Lipschitz constant of ∇(cid:96)(·). The step-size can be ﬁxed or selected via a line search algorithm. For
(5)  we obtain the following iterative scheme:

d(l+1) ← arg min
d∈RK

1
2

(IK − tlR(cid:62)R)W (cid:62)d(l) + tlR(cid:62)y

+ tlλ(cid:107)d(l)−1(cid:107)1.

(8)

Our algorithm has a number of desirable features which make it computationally efﬁcient. Firstly 
(8) is the traditional wavelet problem for regularly spaced data (2)  with response vector r =
{(IK − tlR(cid:62)R)W (cid:62)d(l) + tlR(cid:62)y}. The vector r can be efﬁciently calculated via the sparsity of
R and Mallat’s algorithm for DWT [Mallat  1989]. Secondly  we can use a ﬁxed step size with
tl = L−1
max where Lmax is the maximum eigenvalue of R(cid:62)R. Again  the maximum eigenvalue can
be efﬁciently computed for sparse matrices  e.g.  if R is the linear interpolation matrix then R(cid:62)R
is tridiagonal  and its eigenvalues can be calculated in O(K log K) operations. The matrix R for
linear interpolation matrix needs to be computed once and requires a sorting of the observations 
i.e. O(n log n). Finally  by taking advantage of Nesterov-style acceleration [Nesterov  2007]  the
worst-case convergence rate of the algorithm after k steps can be improved from O(k−1) to O(k−2).
The procedure (8) can also be used to solve the additive (6) and sparse additive (7) extensions via a
block coordinate descent algorithm. Speciﬁcally  given a set of estimates dj (j = 1  . . .   p) we can
ﬁx all but one of the vectors dj and optimize over the non-ﬁxed vector  by solving
2 + λ1(cid:107)d−1(cid:107)1 + λ2(cid:107)RjW (cid:62)d(cid:107)2 

(9)
for some vector rj ∈ Rn. For additive waveMesh (λ2 = 0)  this reduces to the univariate problem
which can be solved via the algorithm (8). For sparse additive waveMesh (λ2 (cid:54)= 0)  the problem can
be solved by solving (9) with λ2 = 0 following by a soft-scaling operation [Petersen et al.  2016 
Lemma 7.1]. We detail our algorithm for sparse additive waveMesh in the supplementary material.

(cid:107)rj − RjW (cid:62)d(cid:107)2

minimize

d∈RK

1
2

4

2.4 Some extensions and variations

In this subsection  we discuss some variations and extensions of waveMesh  namely (1) using a
conservative order for the wavelet basis expansion  (2) extending waveMesh for more general loss
functions and  (3) using a weighted (cid:96)1 penalty for shrinkage of wavelet coefﬁcients.
While in (5) we set K = 2(cid:100)log2 n(cid:101)  we could  instead  set K to be any power of 2. Since the main
computational step in our algorithm is the DWT and IDWT which requires O(K) operations  a
smaller value of K can greatly reduce the computation time. Furthermore  using a smaller K can
lead to superior predictive performance in some settings; this is formalized in our theoretical results
of Section 3 and observed in the simulation studies of Section 4. In the supplementary material we
present additional simulation studies comparing the prediction performance and computation time of
waveMesh for various values of K.
Secondly  waveMesh can be extended to other loss functions appropriate for various data types. For
example  we can extend our methodology to the setting of binary classiﬁcation via a logistic loss
function. Let yi ∈ {−1  1} (i = 1  . . .   n) be the observed response. For the univariate case  we get

(cid:98)d ← arg min

d∈RK

1
2

n(cid:88)

i=1

log(cid:0)1 + exp(cid:2)−yi(RW (cid:62)d)i

(cid:3)(cid:1) + λ(cid:107)d−1(cid:107)1.

Like the least squares loss  (10) naturally extends to (sparse) additive models. The problem can be
efﬁciently solved via a proximal gradient descent algorithm described in the supplementary material.
Finally  we consider a variation of our (cid:96)1 penalty motivated by the SURESHRINK procedure of Donoho
and Johnstone [1995]. For a vector d ∈ RK of discrete father and mother wavelet coefﬁcients  denote
by d[j] the discrete mother wavelet coefﬁcients at resolution level j. For this particular variation  we
require that the minimum resolution level j0 > 1. We then propose to solve

(cid:98)d ← arg min

d∈RK

(cid:107)y − RW (cid:62)d(cid:107)2

2 + λ

1
2

(cid:112)2 log(j)(cid:107)d[j](cid:107)1.

log2 K(cid:88)

j=j0

In the supplementary material we show that the above estimator outperforms the usual waveMesh
estimator (5) in terms of prediction error.

3 Theoretical results

(10)

(11)

In this section  we study ﬁnite sample properties of our univariate estimator (5)  and sparse additive
estimator (7). We begin with a quick introduction to Besov spaces and their connection to wavelet
bases. We establish minimax convergence rates (up to a log n factor) for our univariate proposal. We
note that our estimator (5) can be seen as a lasso estimator [Tibshirani  1996] with design matrix
RW (cid:62); this allows us to use well-known results for the lasso estimator to easily establish minimax
rates which we present below. Additionally  the lasso formulation allows us to establish sufﬁcient

conditions for the uniqueness of our estimator. Speciﬁcally  ﬁtted values(cid:98)f = RW (cid:62)(cid:98)d are unique
whereas uniqueness of(cid:98)d depends on the matrix RW (cid:62). In the interest of brevity  we omit derivation of
sufﬁcient conditions for uniqueness of (cid:98)d and refer the interested reader to Tibshirani [2013]. Finally 

we also establish rates for the sparse additive waveMesh proposal for a speciﬁc penalty.
q1 q2  are function spaces with speciﬁc degrees of smoothness
Besov spaces on the unit interval  Bs
in their derivative: for the Besov norm (cid:107) · (cid:107)Bs
< C}.
The constants (s  q1  q2) are the parameters of Besov spaces; for a function g ∈ L2([0  1]) with the
wavelet bases expansion (1)  the Besov norm is deﬁned as

= {g ∈ L2([0  1]) : (cid:107)g(cid:107)Bs

  Bs

q1 q2

q1 q2

q1 q2

(cid:107)g(cid:107)Bs

q1 q2

= (cid:107)αj0(cid:107)q1 +

2j(s+1/2−1/q1)(cid:107)βj(cid:107)q1

 

(12)

j=j0

where αj0 ∈ R2j0 is the vector of father wavelet coefﬁcients with minimum resolution level j0 and
βj ∈ R2j is the vector of mother wavelet coefﬁcients at resolution level j. For completeness  we
also deﬁne (cid:107)g(cid:107)Bs

(cid:9). We consider Besov spaces

(cid:8)2j(s+1/2−1/q1)(cid:107)βj(cid:107)q1

q1 ∞ = (cid:107)αj0(cid:107)q1 + supj≥j0

5

(cid:20) ∞(cid:88)

(cid:26)

(cid:27)q2(cid:21)1/q2

because they generalize well-known classes such as the Sobolev (Bs
2 2  s = 1  2  . . .)  and Hölder
(Bs∞ ∞  s > 0) spaces and the class of bounded total variation functions (sandwiched between B1
1 1
and B1
1 ∞). Our ﬁrst result below establishes near minimax convergence rates for the prediction
error of our estimator. An attractive feature of our estimator is that it achieves this rate without any
information about the parameters (s  q1  q2). We recover the usual wavelet rates of Donoho [1995]
under the special case when xi = i/n and R = In. Additionally  the theorem justiﬁes the use of
K < n basis functions: if the true function is sufﬁciently smooth  we recover the usual rates with an
additional log K factor instead of log n.

Theorem 1 Suppose yi = f 0(xi) + εi (i = 1  . . .   n) for mean zero  sub-Gaussian noise εi. Deﬁne

the estimator (cid:98)f = RW (cid:62)(cid:98)d = [(cid:98)f (x1)  . . .  (cid:98)f (xn)]T for linear interpolation matrix R (4) where

(cid:98)d ← arg min

d∈RK

1
2

(cid:107)y − RW (cid:62)d(cid:107)2

2 + λ(cid:107)d−1(cid:107)1 

j=1

2j0−1(cid:88)

q1 q2

2

n

1
n

≤ C

(cid:19) 2s

2s+1

(cid:18) log K

(cid:13)(cid:13)(cid:13)f 0 − (cid:98)f

and the mother wavelet ψ  has r null moments and r continuous derivatives where

where the constant c1 depends on R and the distribution of εi  and the constant C depends on R.

for the usual DWT transform matrix W ∈ RK×K associated with some orthogonal wavelet family.
f 0 ∈ Bs
r > max{1  s}. Suppose λ ≥ c1
(speciﬁcally K ≥ c1n1/(2s+1) for some constant c1)  with probability at least 1 − 2 exp(−t2/2)  we
have

Further  deﬁne f 0 = [f 0(x1)  . . .   f 0(xn)](cid:62) and (cid:101)f 0 = [f 0(1/K)  . . .   f 0(K/K)](cid:62). Assume that
(cid:112)t2 + 2 log K for some t > 0. Then  for sufﬁciently large K
(cid:13)(cid:13)(cid:13)2

(cid:107)f 0 − R(cid:101)f 0(cid:107)2
The above theorem includes an approximation error term (cid:107)f 0 − R(cid:101)f 0(cid:107)2

2 which depends on the type
of interpolation matrix R. For example  for linear interpolation of a twice continuously differentiable
function  the approximation error scales as O(K−2). Thus  for a sufﬁciently large K (particularly
K = n)  the approximation error will disappear. In fact  as long as the approximation error is of the
order (log K/n)2s/(2s+1)  we obtain the usual near-minimax rate.
For the sparse additive model  we consider a different model motivated by the Besov norm

(12). Our next theorem provides convergence rates for the estimated function (cid:98)f = (cid:80)p
j=1 (cid:98)fj =
(cid:80)p
j=1 RjW (cid:62)(cid:98)dj  where
(cid:3)  
(cid:2)λ1Ps(dj) + λ2(cid:107)RjW (cid:62)dj(cid:107)2
(cid:98)d1  . . .   (cid:98)dp ← arg min

RjW (cid:62)dj

(cid:13)(cid:13)(cid:13)y − p(cid:88)

+

2
n

(13)

2 

(cid:13)(cid:13)(cid:13)2

2

+

p(cid:88)

j=1

d1 ... dp∈RK

1
2

and the penalty Ps is the discrete version of the Besov norm for Bs
1 1. Speciﬁcally  for d as a
vector of father coefﬁcients  αj0k (k = 0  . . .   2j0 − 1)  and mother wavelet coefﬁcients βjk (j =
j0  . . .   J; k = 0  . . .   2j − 1) the penalty is

Ps(d) =

|αj0k| +

2j(s−1/2)

.

(14)

k=0

j=j0

k=0

j∈S (cid:107)fj(cid:107)2 ≤(cid:112)|S|(cid:107)(cid:80)p

j∈S (cid:107)fj(cid:107)2 and (cid:107)(cid:80)p
(cid:80)
inequality(cid:80)

Before presenting our next result  we state and discuss the so called compatibility condition. This
condition is common in the high-dimensional literature [van de Geer and Bühlmann  2009] and crucial
for proving minimax rates for sparse additive models. Brieﬂy  our proof requires the semi-norms
j=1 fj(cid:107)2 to be somehow ‘compatible’  for an index set S ⊆ {1  . . .   p}. In
the low-dimensional/non-sparse case  i.e.  S = {1  . . .   p}  the semi-norms are compatible by the
j=1 fj(cid:107)2. The compatibility condition ensures such an inequality
holds for proper subsets S. Furthermore  the compatibility condition can be relaxed at the cost of
proving a slower rate; this is similar to the lasso slow rate [Dalalyan et al.  2017].
Deﬁnition 1 The compatibility condition is said to hold for an index set S ⊂ {1  2  . . .   p}  with
compatibility constant ϑ(S) > 0  if for all γ > 0 and any set of discrete wavelet coefﬁcients vector

6

J(cid:88)

(cid:16)

2j−1(cid:88)

|βjk|(cid:17)

/ϑ(S).

j∈S (cid:107)RjW (cid:62)dj(cid:107)  it

Theorem 2 Assume the model yi = f 0(xi)+εi (i = 1  . . .   n) with mean zero  sub-Gaussian εi. Let
j be an arbitrary
sparse additive function with S∗ ⊂ {1  2  . . .   p}. Let ρ = κ max{n−2s/(2s+1)  (log p/n)1/2} for a
constant κ that depends on the distribution of εi and s. Suppose λ ≥ 4ρ. Then  with probability at
least 1 − 2 exp(−c1nρ2) − c2 exp(−c3nρ2)  we have

j∈S∗ RjW (cid:62)d∗

j∈S∗ f∗

j=1 RjW (cid:62)dj

j∈Sc n−1(cid:107)RjW (cid:62)dj(cid:107)2 + γ(cid:80)p
j=1 Ps(dj) ≤ 3(cid:80)
(cid:13)(cid:13)(cid:13)2
j =(cid:80)
(cid:17)1/2(cid:111)
2s+1  |S∗|(cid:16) log p

(d1  . . .   dp)  that satisfy(cid:80)
holds that(cid:80)
(cid:98)f =(cid:80)p

j∈S (cid:107)RjW (cid:62)dj(cid:107)2 ≤(cid:112)|S|(cid:13)(cid:13)(cid:13)(cid:80)p
j=1 (cid:98)fj be as deﬁned in (13)  and let f∗ =(cid:80)
(cid:13)(cid:13)(cid:13)2
n−1(cid:13)(cid:13)(cid:13)f 0 − (cid:98)f
(cid:110)|S∗|n− s
n−1(cid:13)(cid:13)(cid:13)f 0 − (cid:98)f

|S∗|−1(cid:80)
(cid:110)|S∗|n− 2s
where the constant C2 depends on ϑ(S∗) and |S∗|−1(cid:80)

(cid:111)
j∈S∗ Ps(d∗
j ).

where constants c1  c2 depend on the distribution of εi and s  and C1 depends on κ and
j ). Furthermore  if the compatibility condition holds for S∗ with constant
ϑ(S∗) we have

j∈S∗ Ps(d∗

+ n−1(cid:13)(cid:13)f 0 − f∗(cid:13)(cid:13)2

2  

+ 4n−1(cid:13)(cid:13)f 0 − f∗(cid:13)(cid:13)2

2  

≤ C2 max

2s+1  |S∗| log p
n

≤ C1 max

2

n

(cid:13)(cid:13)(cid:13)2

2

4 Numerical experiments

4.1 Experiments for univariate regression

MSE = n−1(cid:13)(cid:13)f 0 − (cid:98)f(cid:13)(cid:13)2

We begin with a simulation to compare the performance of univariate waveMesh to the traditional
interpolation method of Kovac and Silverman [2000]  isometric wavelet method of Sardy et al.
[1999]—which treats the data as if it were regularly spaced—and adaptive lifting method of Nunes
et al. [2006]. The former two methods are implemented in the R package wavethres [Nason  2016]
and the latter is implemented in the adlift package [Nunes and Knight  2017].
We generate the data as yi = f 0(xi) + εi (i = 1  . . .   n) for different choices of function f 0 and
n. The errors are distributed as εi ∼ N (0  σ2) with σ2 chosen such that SNR = 5  where SNR =
var(f 0)/σ2. We consider two different choices of the covariate  xi ∼ U[0  1] and xi ∼ N (0  1)
scaled to lie in [0  1]. We consider 6 different choices for the function f 0: 1. polynomial  2. sine 
3. piecewise polynomial  4. heavy sine  5. bumps and  6. doppler. These functions are shown in
Figure 1 of the supplementary material. We apply our proposal  waveMesh  the interpolation proposal
of Kovac and Silverman [2000] and isometric wavelet proposal of Sardy et al. [1999]  for a sequence
of 50 λ values linear on the log scale and select the λ value that minimizes the mean square error 
2. For adaptive lifting  the R implementation automatically selects a tuning
parameter. We implement waveMesh using the linear interpolation matrix (4). We also implement
waveMesh using a small grid  i.e.  we ﬁt (5) with K = 25 and 26. The R implementation of isometric
wavelets requires sample sizes to be a power of two; if not  we pad the response vector with zeros.
We also analyze the motorcycle data studied by Silverman [1985] consisting of 133 head acceleration
measurements in a simulated motorcycle accident taken at 94 unequally spaced time points. To
avoid the issue of repeated measurements  we average acceleration measurements at the same time
leading to a sample size of n = 94. Selection of tuning parameter for waveMesh is done via 5-fold
cross validation. For interpolation [Sardy et al.  1999] and isometric [Kovac and Silverman  2000]
wavelet proposals  we use the universal thresholding rule for tuning parameter selection [Donoho
and Johnstone  1994]; this rule leads to near minimax convergence rates like that of Theorem 1.
Table 1 shows the ratio of MSE between our proposal with K = 2(cid:100)log2 n(cid:101) and other proposals
for uniformly distributed xi. We observe that our proposal has the smallest MSE for all functions
except the Bumps function. Even for the Bumps function  waveMesh exhibits superior prediction
performance over other methods for n = 512. We also observe that waveMesh with smaller values
of K often outperforms the full waveMesh (K = 2(cid:100)log2 n(cid:101)) method in terms of MSE. Results for
normally distributed xi are given in the supplementary material. In that case  we again observe that
waveMesh outperforms existing methods for a number of simulation scenarios  except for a few
cases with polynomial and bumps functions. Results for sample sizes that are not powers of two

7

Table 1: Results for xi ∼ U[0  1] averaged over 100 replicates; the ratio MSE / MSEF G is shown
along with 100× the standard error  where MSEF G is the MSE of waveMesh with K = 2(cid:100)log2 n(cid:101).
Boldface values represent the method with the smallest MSE within each row of the table.

Polynomial

Sine

Piecewise
Polynomial

Heavy Sine

Bumps

Doppler

waveMesh
K = 25

1.19 (5.51)
0.92 (5.57)
1.00 (6.20)
0.78 (3.18)
0.97 (3.14)
0.76 (3.18)
0.66 (2.50)
0.57 (2.34)
0.85 (1.97)
0.77 (2.00)
0.82 (1.92)
1.01 (2.43)
0.84 (2.44)
0.75 (2.66)
0.66 (1.64)
0.58 (1.59)
2.11 (2.30)
2.86 (2.77)
4.81 (6.82)
7.45 (9.13)
0.98 (1.69)
1.24 (2.02)
1.71 (3.92)
2.58 (4.85)

waveMesh
K = 26
1.00 (0.00)
0.77 (3.07)
0.85 (3.15)
0.72 (2.58)
1.00 (0.00)
0.76 (1.96)
0.70 (2.22)
0.56 (2.22)
1.00 (0.00)
0.82 (1.52)
0.79 (1.59)
0.86 (1.70)
1.00 (0.00)
0.82 (1.16)
0.72 (1.14)
0.60 (1.18)
1.00 (0.00)
2.11 (1.62)
3.47 (4.39)
5.69 (6.77)
1.00 (0.00)
0.89 (1.04)
0.94 (1.38)
1.26 (2.01)

n = 64
n = 128
n = 256
n = 512
n = 64
n = 128
n = 256
n = 512
n = 64
n = 128
n = 256
n = 512
n = 64
n = 128
n = 256
n = 512
n = 64
n = 128
n = 256
n = 512
n = 64
n = 128
n = 256
n = 512

Interpolation

Isometric

Adaptive Lifting

1.24 (4.11)
1.12 (6.00)
1.61 (9.04)
1.76 (6.11)
1.47 (5.81)
1.29 (6.08)
1.93 (9.49)
2.13 (7.78)
1.18 (3.12)
1.26 (2.75)
1.42 (3.18)
1.71 (3.56)
1.12 (3.04)
1.17 (3.32)
1.37 (2.98)
1.58 (3.05)
1.70 (1.75)
1.40 (1.59)
1.43 (1.89)
1.32 (1.35)
1.15 (3.45)
1.07 (2.13)
1.20 (2.11)
1.21 (1.48)

1.78 (7.56)
1.33 (7.18)
1.50 (7.67)
1.13 (2.64)
1.59 (6.72)
1.46 (5.24)
1.34 (4.23)
1.24 (3.66)
1.31 (3.62)
1.22 (2.61)
1.14 (2.11)
1.15 (1.99)
1.41 (3.17)
1.50 (4.75)
1.33 (2.58)
1.29 (1.60)
0.72 (1.34)
0.63 (0.83)
0.88 (0.99)
1.19 (1.03)
1.33 (3.20)
1.44 (2.57)
1.29 (1.99)
1.10 (1.31)

4.28 (29.86)
3.57 (31.27)
4.29 (31.29)
3.61 (26.47)
3.62 (33.65)
2.98 (19.78)
3.41 (18.80)
3.63 (28.42)
1.63 (9.07)
1.40 (7.36)
1.15 (6.04)
1.25 (7.24)
1.70 (8.35)
1.56 (8.26)
1.53 (6.74)
1.50 (9.21)
1.07 (5.12)
0.85 (2.43)
0.97 (2.00)
1.23 (2.34)
1.30 (3.65)
1.18 (3.22)
1.30 (3.44)
1.23 (3.36)

were similar to the results provided here. In the interest of brevity  these results are presented in the
supplementary material.
In Figure 1  we plot the motorcycle data and ﬁtted functions for each method. Here  waveMesh
reasonably models the data via a smooth function; the interpolation method has a similar but slightly
more biased result around 10 to 25 ms. Adaptive lifting and isometric wavelets lead to highly variable
estimates.

4.2 Experiments for multivariate additive regression

We proceed with a simulation study to illustrate the performance of additive waveMesh compared
to the proposal of Sardy and Tseng [2004]  AMlet. We use the author-provided R implementation
for the AMlet proposal; due to a lack of R packages for other proposals  we defer the comparison to
future work. We consider the following simulation setting: we generate data with yi = f1(xi1) +
f2(xi2) + f3(xi3) + f4(xi4) + εi (i = 1  . . .   210)  where εi ∼ N (0  σ2)  xi ∼ U[0  1]  and σ2 such
that SNR = 10. The four functions f1  . . .   f4 are the polynomial  sine  piecewise polynomial and
heavy sine functions presented in Figure 1 of the supplementary material. We consider sample sizes
n = 64  100  256  500  512 and results were averaged over 100 data sets. For sample sizes not a
power of 2  the response vector was padded with zeros for the R implementation of AMlet. The
universal threshold rule was used for AMlet as detailed in Sardy and Tseng [2004]; 5-fold cross
validation was used for additive waveMesh for selection of λ.
For a real world data analysis  we consider the Boston housing data analyzed by Ravikumar et al.
[2009]. The goal is to predict the median value of homes based on 10 predictors. The data consists of
n = 506 observations; we use 256 observations for training and calculate the test error on the rest.
Tuning parameters are selected in the same way as the simulation study above.
Table 2 shows the MSE of both proposals for various choices of n for the simulation study. The
results clearly indicate that additive waveMesh offers substantial improvement over AMlet  especially

8

Figure 1: Fitted functions to the motorcycle accident dataset for each of the 4 methods.

for smaller values of n. We observe similar results for the Boston housing data: the average test
error is 21.2 for waveMesh (standard error 0.34) and 25.1 for AMlet (standard error 0.42). These
results support our theoretical analysis and underscore the advantages of waveMesh in sparse high-
dimensional additive models.

5 Conclusion

In this paper  we introduced waveMesh  a novel method for non-parametric regression using wavelets.
Unlike traditional methods  waveMesh does not require that covariates are uniformly spaced on the
unit interval  nor does it require that the sample size is a power of 2. We achieve this using a novel
interpolation approach for wavelets. The main appeal of our proposal is that it naturally extends to
multivariate additive models for a potentially large number of covariates.
To compute the estimator  we proposed an efﬁcient proximal gradient descent algorithm  which
leverages existing techniques for fast computation of the DWT. We established minimax convergence
rates for our univariate proposal over a large class of Besov spaces. For a particular Besov space 
we also established minimax convergence rates for our (sparse) additive framework. The R package
waveMesh  which implements our methodology  will soon be publicly available on GitHub.

Table 2: MSE and standard error of waveMesh and AMlet averaged over 100 data sets.

waveMesh
AMlet

n = 64

10.76 (0.31)
100.48 (1.83)

n = 100

11.35 (0.33)
34.58 (1.05)

n = 128
8.82 (0.24)
45.49 (1.09)

n = 256
5.45 (0.11)
19.57 (0.33)

n = 500
4.34 (0.08)
10.67 (0.12)

n = 512
4.08 (0.07)
8.90 (0.11)

9

1020304050−100−50050waveMeshTime (ms)Acceleration (g)1020304050−100−50050InterpolationTime (ms)Acceleration (g)1020304050−100−50050IsometricTime (ms)Acceleration (g)1020304050−100−50050Adaptive LiftingTime (ms)Acceleration (g)Acknowledgments

We thank three anonymous referees for insightful comments that substantially improved the
manuscript. We thank Professor Sylvain Sardy for providing software. This work was partially
supported by National Institutes of Health grants to A.S. and N.S.  and National Science Foundation
grants to A.S.

References
Umberto Amato and Anestis Antoniadis. Adaptive wavelet series estimation in separable nonpara-

metric regression models. Statistics and Computing  11(4):373–394  2001.

Anestis Antoniadis and Jianqing Fan. Regularization of wavelet approximations. Journal of the

American Statistical Association  96(455):939–967  2001.

Tony Cai and Lawrence D Brown. Wavelet shrinkage for nonequispaced samples. The Annals of

Statistics  26(5):1783–1799  1998.

Tony Cai and Lawrence D Brown. Wavelet estimation for samples with random uniform design.

Statistics & Probability Letters  42(3):313–321  1999.

Charles K Chui. An introduction to wavelets. Philadelphia  SIAM  38  1992.

Arnak S. Dalalyan  Mohamed Hebiri  and Johannes Lederer. On the prediction performance of the

lasso. Bernoulli  23(1):552–581  2017.

Ingrid Daubechies. Orthonormal bases of compactly supported wavelets. Communications on Pure

and Applied Mathematics  41(7):909–996  1988.

Ingrid Daubechies. The wavelet transform  time-frequency localization and signal analysis. IEEE

Transactions on Information Theory  36(5):961–1005  1990.

Ingrid Daubechies. Ten lectures on wavelets  volume 61. SIAM  1992.

Ingrid Daubechies. Orthonormal bases of compactly supported wavelets II. variations on a theme.

SIAM Journal on Mathematical Analysis  24(2):499–519  1993.

David L Donoho. De-noising by soft-thresholding. IEEE Transactions on Information Theory  41(3):

613–627  1995.

David L Donoho and Iain M Johnstone. Adapting to unknown smoothness via wavelet shrinkage.

Journal of the American Statistical Association  90(432):1200–1224  1995.

David L Donoho and Jain M Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika 

81(3):425–455  1994.

German A Schnaidt Grez and Brani Vidakovic. Empirical wavelet-based estimation for non-linear

additive regression models. ArXiv e-prints  March 2018.

Peter Hall and Berwin A Turlach. Interpolation methods for nonlinear wavelet regression with

irregularly spaced design. The Annals of Statistics  25(5):1912–1925  1997.

Arne Kovac and Bernard W Silverman. Extending the scope of wavelet regression methods by
coefﬁcient-dependent thresholding. Journal of the American Statistical Association  95(449):
172–183  2000.

Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation.

IEEE Transactions on Pattern Analysis and Machine Intelligence  11(7):674–693  1989.

Yves Meyer. Principe d’incertitude  bases hilbertiennes et algebres d’operateurs. Séminaire Bourbaki 

662:1985–1986  1985.

Guy Nason. Wavelet methods in statistics with R. Springer Science & Business Media  2010.

10

Guy Nason. wavethresh: Wavelets Statistics and Transforms  2016. URL https://CRAN.

R-project.org/package=wavethresh. R package version 4.6.8.

Yurii Nesterov. Gradient methods for minimizing composite objective function. Technical report 

UCL  2007.

Matthew A Nunes and Marina I Knight. adlift: An Adaptive Lifting Scheme Algorithm  2017. URL

https://CRAN.R-project.org/package=adlift. R package version 1.3-3.

Matthew A Nunes  Marina I Knight  and Guy P Nason. Adaptive lifting for nonparametric regression.

Statistics and Computing  16(2):143–159  2006.

Todd Ogden. Essential wavelets for statistical applications and data analysis. Springer Science &

Business Media  2012.

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization  1(3):

127–239  2014.

Marianna Pensky and Brani Vidakovic. On non-equally spaced wavelet regression. Annals of the

Institute of Statistical Mathematics  53(4):681–690  2001.

Donald B Percival and Andrew T Walden. Wavelet methods for time series analysis  volume 4.

Cambridge University Press  2006.

Ashley Petersen  Daniela Witten  and Noah Simon. Fused lasso additive model. Journal of Computa-

tional and Graphical Statistics  25(4):1005–1025  2016.

Pradeep Ravikumar  John Lafferty  Han Liu  and Larry Wasserman. Sparse additive models. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  71(5):1009–1030  2009.

Sylvain Sardy and Paul Tseng. AMlet  RAMlet  and GAMlet: automatic nonlinear ﬁtting of additive
models  robust and generalized  with wavelets. Journal of Computational and Graphical Statistics 
13(2):283–309  2004.

Sylvain Sardy  Donald B Percival  Andrew G Bruce  Hong-Ye Gao  and Werner Stuetzle. Wavelet

shrinkage for unequally spaced data. Statistics and Computing  9(1):65–75  1999.

Bernhard W Silverman. Some aspects of the spline smoothing approach to non-parametric regression
curve ﬁtting. Journal of the Royal Statistical Society. Series B (Methodological)  pages 1–52  1985.

Gilbert Strang and Truong Nguyen. Wavelets and ﬁlter banks. SIAM  1996.

Robert J Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 267–288  1996.

Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics  7:1456–1490 

2013.

Sara van de Geer and Peter Bühlmann. On the conditions used to prove oracle results for the lasso.

Electronic Journal of Statistics  3:1360–1392  2009.

Nikolai N ˇCencov. Evaluation of an unknown distribution density from observations. Doklady  3:

1559–1562  1962.

Brani Vidakovic. Statistical modeling by wavelets  volume 503. John Wiley & Sons  2009.

Grace Wahba. Spline Models for Observational Data. SIAM  1990.

Shuanglin Zhang and Man-Yu Wong. Wavelet threshold estimation for additive regression models.

The Annals of Statistics  31(1):152–173  2003.

11

,Asad Haris
Noah Simon