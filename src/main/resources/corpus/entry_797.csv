2013,Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search,We go beyond the notion of pairwise similarity and look into  search problems with $k$-way similarity functions. In this paper  we focus on problems related to  \emph{3-way Jaccard} similarity: $\mathcal{R}^{3way}= \frac{|S_1 \cap S_2 \cap S_3|}{|S_1 \cup S_2 \cup S_3|}$  $S_1  S_2  S_3 \in \mathcal{C}$  where $\mathcal{C}$ is a size $n$ collection of sets (or binary vectors).  We show that approximate $\mathcal{R}^{3way}$ similarity search problems admit  fast algorithms with  provable guarantees  analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to $k$-way resemblance. In the process  we extend traditional framework of \emph{locality sensitive hashing (LSH)} to handle higher order similarities  which could be of independent theoretical interest. The applicability of $\mathcal{R}^{3way}$ search is shown on the Google sets" application. In addition  we demonstrate the advantage of $\mathcal{R}^{3way}$ resemblance over the pairwise case in improving retrieval quality.",Beyond Pairwise: Provably Fast Algorithms for

Approximate k-Way Similarity Search

Anshumali Shrivastava

Department of Computer Science
Computing and Information Science

Cornell University

Ithaca  NY 14853  USA

Ping Li

Department of Statistics & Biostatistics

Department of Computer Science

Rutgers University

Piscataway  NJ 08854  USA

anshu@cs.cornell.edu

pingli@stat.rutgers.edu

Abstract

We go beyond the notion of pairwise similarity and look into search problems
with k-way similarity functions. In this paper  we focus on problems related to
3-way Jaccard similarity: R3way =
|S1∪S2∪S3|  S1; S2; S3 ∈ C  where C is a
|S1∩S2∩S3|
size n collection of sets (or binary vectors). We show that approximate R3way
similarity search problems admit fast algorithms with provable guarantees  analo-
gous to the pairwise case. Our analysis and speedup guarantees naturally extend
to k-way resemblance. In the process  we extend traditional framework of locality
sensitive hashing (LSH) to handle higher-order similarities  which could be of in-
dependent theoretical interest. The applicability of R3way search is shown on the
“Google Sets” application. In addition  we demonstrate the advantage of R3way
resemblance over the pairwise case in improving retrieval quality.

1 Introduction and Motivation
Similarity search (near neighbor search) is one of the fundamental problems in Computer Science.
The task is to identify a small set of data points which are “most similar” to a given input query.
Similarity search algorithms have been one of the basic building blocks in numerous applications
including search  databases  learning  recommendation systems  computer vision  etc.
One widely used notion of similarity on sets is the Jaccard similarity or resemblance [5  10  18  20].
Given two sets S1; S2 ⊆ Ω = {0; 1; 2; :::; D − 1}  the resemblance R2way between S1 and S2 is
deﬁned as: R2way =
|S1∩S2|
|S1∪S2| . Existing notions of similarity in search problems mainly work with
pairwise similarity functions. In this paper  we go beyond this notion and look at the problem of
k-way similarity search  where the similarity function of interest involves k sets (k ≥ 2). Our work
exploits the fact that resemblance can be naturally extended to k-way resemblance similarity [18 
21]  deﬁned over k sets {S1; S2; :::; Sk} as Rk−way =
Binary high-dimensional data
The current web datasets are typically binary  sparse  and ex-
tremely high-dimensional  largely due to the wide adoption of the “Bag of Words” (BoW) represen-
tations for documents and images. It is often the case  in BoW representations  that just the presence
or absence (0/1) of speciﬁc feature words captures sufﬁcient information [7  16  20]  especially
with (e.g. ) 3-grams or higher-order models. And so  the web can be imagined as a giant storehouse
of ultra high-dimensional sparse binary vectors. Of course  binary vectors can also be equivalently
viewed as sets (containing locations of the nonzero features).
We list four practical scenarios where k-way resemblance search would be a natural choice.
(i) Google Sets:
(http://googlesystem.blogspot.com/2012/11/google-sets-still-available.html)
Google Sets is among the earliest google projects  which allows users to generate list of similar
words by typing only few related keywords. For example  if the user types “mazda” and “honda”
the application will automatically generate related words like “bmw”  “ford”  “toyota”  etc. This
application is currently available in google spreadsheet. If we assume the term document binary
|w1∩w2∩w|
representation of each word w in the database  then given query w1 and w2  we show that
|w1∪w2∪w|
turns out to be a very good similarity measure for this application (see Section 7.1).

|S1∩S2∩:::∩Sk|
|S1∪S2∪:::∪Sk|.

1

(ii) Joint recommendations: Users A and B would like to watch a movie together. The proﬁle of
each person can be represented as a sparse vector over a giant universe of attributes. For example 
a user proﬁle may be the set of actors  actresses  genres  directors  etc  which she/he likes. On the
other hand  we can represent a movie M in the database over the same universe based on attributes
associated with the movie. If we have to recommend movie M  jointly to users A and B  then a
|A∩B∩M|
natural measure to maximize is
|A∪B∪M|. The problem of group recommendation [3] is applicable
in many more settings such as recommending people to join circles  etc.
(iii) Improving retrieval quality: We are interested in ﬁnding images of a particular type of ob-
ject  and we have two or three (possibly noisy) representative images. In such a scenario  a natural
expectation is that retrieving images simultaneously similar to all the representative images should
be more reﬁned than just retrieving images similar to any one of them. In Section 7.2  we demon-
strate that in cases where we have more than one element to search for  we can reﬁne our search
quality using k-way resemblance search. In a dynamic feedback environment [4]  we can improve
subsequent search quality by using k-way similarity search on the pages already clicked by the user.
(iv) Beyond pairwise clustering: While machine learning algorithms often utilize the data
through pairwise similarities (e.g.  inner product or resemblance)  there are natural scenarios where
the afﬁnity relations are not pairwise  but rather triadic  tetradic or higher [2  30]. The computational
cost  of course  will increase exponentially if we go beyond pairwise similarity.
Efﬁciency is crucial With the data explosion in modern applications  the brute force way of scan-
ning all the data for searching is prohibitively expensive  specially in user-facing applications like
search. The need for k-way similarity search can only be fulﬁlled if it admits efﬁcient algorithms.
This paper fulﬁlls this requirement for k-way resemblance and its derived similarities. In particular 
we show fast algorithms with provable query time guarantees for approximate k-way resemblance
search. Our algorithms and analysis naturally provide a framework to extend classical LSH frame-
work [14  13] to handle higher-order similarities  which could be of independent theoretical interest.
Organization
In Section 2  we review approximate near neighbor search and classical Locality
Sensitive Hashing (LSH). In Section 3  we formulate the 3-way similarity search problems. Sec-
tions 4  5  and 6 describe provable fast algorithms for several search problems. Section 7 demon-
strates the applicability of 3-way resemblance search in real applications.
2 Classical c-NN and Locality Sensitive Hashing (LSH)
Initial attempts of ﬁnding efﬁcient (sub-linear time) algorithms for exact near neighbor search  based
on space partitioning  turned out to be a disappointment with the massive dimensionality of current
datasets [11  28]. Approximate versions of the problem were proposed [14  13] to break the linear
query time bottleneck. One widely adopted formalism is the c-approximate near neighbor (c-NN).

Deﬁnition 1 (c-Approximate Near Neighbor or c-NN). Consider a set of points  denoted by P  in a
D-dimensional space RD  and parameters R0 > 0  (cid:14) > 0. The task is to construct a data structure
which  given any query point q  if there exist an R0-near neighbor of q in P  it reports some cR0-near
neighbor of q in P with probability 1 − (cid:14).
The usual notion of c-NN is for distance. Since we deal with similarities  we deﬁne R0-near neighbor
of point q as a point p with Sim(q; p) ≥ R0  where Sim is the similarity function of interest.
Locality sensitive hashing (LSH) [14  13] is a popular framework for c-NN problems. LSH is a
family of functions  with the property that similar input objects in the domain of these functions
have a higher probability of colliding in the range space than non-similar ones. In formal terms 
consider H a family of hash functions mapping RD to some set S
Deﬁnition 2 (Locality Sensitive Hashing (LSH)). A family H is called (R0; cR0; p1; p2)-sensitive if
for any two points x; y ∈ RD and h chosen uniformly from H satisﬁes the following:

• if Sim(x; y) ≥ R0 then P rH(h(x) = h(y)) ≥ p1
• if Sim(x; y) ≤ cR0 then P rH(h(x) = h(y)) ≤ p2

For approximate nearest neighbor search typically  p1 > p2 and c < 1 is needed. Note  c < 1 as
we are deﬁning neighbors in terms of similarity. Basically  LSH trades off query time with extra
preprocessing time and space which can be accomplished off-line.

2

Fact 1 Given a family of (R0; cR0; p1; p2) -sensitive hash functions  one can construct a data struc-
ture for c-NN with O(n(cid:26) log1=p2 n) query time and space O(n1+(cid:26))  where (cid:26) = log 1=p1
log 1=p2

.

Minwise Hashing for Pairwise Resemblance One popular choice of LSH family of functions
associated with resemblance similarity is  Minwise Hashing family [5  6  13]. Minwise Hashing
family applies an independent random permutation (cid:25) : Ω → Ω  on the given set S ⊆ Ω  and looks
at the minimum element under (cid:25)  i.e. min((cid:25)(S)). Given two sets S1; S2 ⊆ Ω = {0; 1; 2; :::; D − 1} 
it can be shown by elementary probability argument that

P r (min((cid:25)(S1)) = min((cid:25)(S2))) =

(1)

|S1 ∩ S2|
|S1 ∪ S2| = R2way:

The recent work on b-bit minwise hashing [20  23] provides an improvement by storing only the
lowest b bits of the hashed values: min((cid:25)(S1))  min((cid:25)(S2)). [26] implemented the idea of building
hash tables for near neighbor search  by directly using the bits from b-bit minwise hashing.
3 3-way Similarity Search Formulation
Our focus will remain on binary vectors which can also be viewed as sets. We illustrate our method
|S1∩S2∩S3|
|S1∪S2∪S3|. The algorithm and
using 3-way resemblance similarity function Sim(S1; S2; S3) =
guarantees naturally extend to k-way resemblance. Given a size n collection C ⊆ 2Ω of sets (or
binary vectors)  we are particularly interested in the following three problems:

1. Given two query sets S1 and S2  ﬁnd S3 ∈ C that maximizes Sim(S1; S2; S3).
2. Given a query set S1  ﬁnd two sets S2; S3 ∈ C maximizing Sim(S1; S2; S3).
3. Find three sets S1; S2; S3 ∈ C maximizing Sim(S1; S2; S3).

The brute force way of enumerating all possibilities leads to the worst case query time of O(n) 
O(n2) and O(n3) for problem 1  2 and 3  respectively. In a hope to break this barrier  just like the
case of pairwise near neighbor search  we deﬁne the c-approximate (c < 1) versions of the above
three problems. As in the case of c-NN  we are given two parameters R0 > 0 and (cid:14) > 0. For each
of the following three problems  the guarantee is with probability at least 1 − (cid:14):

′
3

′
2; S

′
2; S

′
3

3) ≥ cR0.
′

3) ≥ cR0.
′

1. (3-way c-Near Neighbor or 3-way c-NN) Given two query sets S1 and S2  if there
∈ C so that
2. (3-way c-Close Pair or 3-way c-CP) Given a query set S1  if there exists a pair of
∈ C so that
3. (3-way c-Best Cluster or 3-way c-BC) If there exist sets S1; S2; S3 ∈ C with
3) ≥ cR0.
′

exists S3 ∈ C with Sim(S1; S2; S3) ≥ R0  then we report some S
Sim(S1; S2; S
set S2; S3 ∈ C with Sim(S1; S2; S3) ≥ R0  then we report sets S
Sim(S1; S
Sim(S1; S2; S3) ≥ R0  then we report sets S
4 Sub-linear Algorithm for 3-way c-NN
The basic philosophy behind sub-linear search is bucketing  which allows us to preprocess dataset
in a fashion so that we can ﬁlter many bad candidates without scanning all of them. LSH-based
techniques rely on randomized hash functions to create buckets that probabilistically ﬁlter bad can-
didates. This philosophy is not restricted for binary similarity functions and is much more general.
Here  we ﬁrst focus on 3-way c-NN problem for binary data.
Theorem 1 For R3way c-NN one can construct a data structure with O(n(cid:26) log1=cR0 n) query time
and O(n1+(cid:26)) space  where (cid:26) = 1 −
(cid:3)

∈ C so that Sim(S

′
1; S

′
′
2; S
3

′
1; S

′
2; S

log 1=c

log 1=c+log 1=R0

.

The argument for 2-way resemblance can be naturally extended to k-way resemblance. Speciﬁcally 
given three sets S1  S2  S3 ⊆ Ω and an independent random permutation (cid:25) : Ω → Ω  we have:

P r (min((cid:25)(S1)) = min((cid:25)(S2)) = min((cid:25)(S3))) = R3way:

(2)
Eq.( 2) shows that minwise hashing  although it operates on sets individually  preserves all 3-way
(in fact k-way) similarity structure of the data. The existence of such a hash function is the key
requirement behind the existence of efﬁcient approximate search. For the pairwise case  the proba-
bility event was a simple hash collision  and the min-hash itself serves as the bucket index. In case

3

of 3-way (and higher) c-NN problem  we have to take care of a more complicated event to create an
indexing scheme. In particular  during preprocessing we need to create buckets for each individual
S3  and while querying we need to associate the query sets S1 and S2 to the appropriate bucket. We
need extra mechanisms to manipulate these minwise hashes to obtain a bucketing scheme.
Proof of Theorem 1: We use two additional functions: f1 : Ω → N for manipulating min((cid:25)(S3))
and f2 : Ω × Ω → N for manipulating both min((cid:25)(S1)) and min((cid:25)(S2)). Let a ∈ N+ such that
|Ω| = D < 10a. We deﬁne f1(x) = (10a + 1) × x and f2(x; y) = 10ax + y. This choice ensures
that given query S1 and S2  for any S3 ∈ C  f1(min((cid:25)(S3))) = f2(min((cid:25)(S1)); min((cid:25)(S2))) holds
if and only if min((cid:25)(S1)) = min((cid:25)(S2)) = min((cid:25)(S2))  and thus we get a bucketing scheme.
To complete the proof  we introduce two integer parameters K and L. Deﬁne a new hash function
by concatenating K events. To be more precise  while preprocessing  for every element S3 ∈ C
create buckets g1(S3) = [f1(h1(S3)); :::; f1(hK(S3))] where hi is chosen uniformly from minwise
hashing family. For given query points S1 and S2  retrieve only points in the bucket g2(S1; S2) =
[f2(h1(S1); h1(S2)); :::; f2(hK(S1); hK(S2))]. Repeat this process L times independently. For any
S3 ∈ C  with Sim(S1; S2; S3) ≥ R0  is retrieved with probability at least 1 − (1 − RK
0 )L. Using
K = ⌈ log n
  the proof can be obtained
using standard concentration arguments used to prove Fact 1  see [14  13]. It is worth noting that
(cid:14) ). Note  the process is
the probability guarantee parameter (cid:14) gets absorbed in the constants as log( 1
stopped as soon as we ﬁnd some element with R3way ≥ cR0.
(cid:3)

(cid:14) )⌉  where (cid:26) = 1 −

⌉ and L = ⌈n(cid:26) log( 1

log 1=c+log 1=R0

log 1
cR0

log 1=c

∗

∗-way c-NN for any k

∗-way similarity search so long as k

+1) identical query sets in k-way c-NN  and it reduces to k

Theorem 1 can be easily extended to k-way resemblance with same query time and space guarantees.
∗ ≤ k  because we can always
Note that k-way c-NN is at least as hard as k
choose (k−k
∗-way c-NN problem. So 
any improvements in R3way c-NN implies improvement in the classical min-hash LSH for Jaccard
similarity. The proposed analysis is thus tight in this sense.
The above observation makes it possible to also perform the traditional pairwise c-NN search using
the same hash tables deployed for 3-way c-NN. In the query phase we have an option  if we have
two different queries S1; S2  then we retrieve from bucket g2(S1; S2) and that is usual 3-way c-NN
search. If we are just interested in pairwise near neighbor search given one query S1  then we will
look into bucket g2(S1; S1)  and we know that the 3-way resemblance between S1; S1; S3 boils
down to the pairwise resemblance between S1 and S3. So  the same hash tables can be used for
both the purposes. This property generalizes  and hash tables created for k-way c-NN can be used
∗ ≤ k. The approximation guarantees still holds. This
for any k
ﬂexibility makes k-way c-NN bucketing scheme more advantageous over the pairwise scheme.
One of the peculiarity of LSH based techniques is that the
query complexity exponent (cid:26) < 1 is dependent on the choice
of the threshold R0 we are interested in and the value of c
which is the approximation ratio that we will tolerate. Figure 1
plots (cid:26) = 1−
with respect to c  for selected R0
values from 0.01 to 0.99. For instance  if we are interested in
highly similar pairs  i.e. R0 ≈ 1  then we are looking at near
O(log n) query complexity for c-NN problem as (cid:26) ≈ 0. On
the other hand  for very lower threshold R0  there is no much
of hope of time-saving because (cid:26) is close to 1.
5 Other Efﬁcient k-way Similarities
We refer to the k-way similarities for which there exist sub-linear algorithms for c-NN search with
query and space complexity exactly as given in Theorem 1 as efﬁcient . We have demonstrated
existence of one such example of efﬁcient similarities  which is the k-way resemblance. This leads
to a natural question: “Are there more of them?”.
∑∞
[9] analyzed all the transformations on similarities that preserve existence of efﬁcient LSH search. In
particular  they showed that if S is a similarity for which there exists an LSH family  then there also
exists an LSH family for any similarity which is a probability generating function (PGF) transfor-
i=1 piS i  where S ∈ [0; 1] and
mation on S. PGF transformation on S is deﬁned as P GF (S) =
pi ≥ 0 satisﬁes
i=1 pi = 1. Similar theorem can also be shown in the case of 3-way resemblance.

Figure 1: (cid:26) = 1 −

∑∞

log 1=c+log 1=R0

log 1=c

log 1=c

log 1=c+log 1=R0

.

4

00.20.40.60.8100.20.40.60.810.050.10.20.30.40.50.60.70.80.90.95cρR0=0.99R0=0.01(cid:3)

Theorem 2 Any PGF transformation on 3-way resemblance R3way is efﬁcient.
Recall
in the proof of Theorem 1  we created hash assignments f1(min((cid:25)(S3))) and
f2(min((cid:25)(S1)); min((cid:25)(S2)))  which lead to a bucketing scheme for the 3-way resemblance search 
∑∞
where the collision event E = {f1(min((cid:25)(S3)) = f2(min((cid:25)(S1)); min((cid:25)(S2)))} happens with
probability P r(E) = R3way. To prove the above Theorem 2  we will need to create hash events
i=1 pi(R3way)i. Note that 0 ≤ P GF (R3way) ≤ 1. We will
having probability P GF (R3way) =
make use of the following simple lemma.
Lemma 1 (R3way)n is efﬁcient for all n ∈ N.
Proof: Deﬁne new hash assignments gn
[f2(h1(S1); h1(S2)); :::; f2(hn(S1); hn(S2))]. The collision event gn
probability (R3way)n. We now use the pair < gn
guarantees  as in Theorem 1  for (R3way)n as well.

2 (S1; S2) =
2 (S1; S2) has
2 > instead of < f1  f2 > and obtain same
(cid:3)

1 (S3) = [f1(h1(S3)); :::; f1(hn(S3))] and gn
1 (S3) = gn

1   gn

1; gi

∑∞

Proof of Theorem 2: From Lemma 1  let < gi
as used in above lemma. We sample one hash pair from the set {< gi
the probability of sampling < gi

2 > be the hash pair corresponding to (R3way)i
2 >: i ∈ N}  where
∑∞
2 > is proportional to pi. Note that pi ≥ 0  and satisﬁes
i=1 pi = 1  and so the above sampling is valid. It is not difﬁcult to see that the collision of the
i=1 pi(R3way)i.
(cid:3)

sampled hash pair has probability exactly
Theorem 2 can be naturally extended to k-way similarity for any k ≥ 2. Thus  we now have
inﬁnitely many k-way similarity functions admitting efﬁcient sub-linear search. One  that might be
interesting  because of its radial basis kernel like nature  is shown in the following corollary.

1; gi

1; gi

Corollary 1 e

Rk(cid:0)way−1 is efﬁcient.

log 1=c

.

log 1=c+log 1=R0

Rk(cid:0)way−1 is a PGF on Rk−way.(cid:3)

Rk(cid:0)way normalized by e to see that e

Proof: Use the expansion of e
6 Fast Algorithms for 3-way c-CP and 3-way c-BC Problems
For 3-way c-CP and 3-way c-BC problems  using bucketing scheme with minwise hashing family
will save even more computations.
Theorem 3 For R3way c-Close Pair Problem (or c-CP) one can construct a data structure with
O(n2(cid:26) log1=cR0 n) query time and O(n1+2(cid:26)) space  where (cid:26) = 1 −
(cid:3)
Note that we can switch the role of f1 and f2 in the proof of Theorem 1. We are thus left with a c-NN
problem with search space O(n2) (all pairs) instead of n. A bit of analysis  similar to Theorem 1 
will show that this procedure achieves the required query time O(n2(cid:26) log1=cR0 n)  but uses a lot
more space  O(n2(1+(cid:26)))  than shown in the above theorem. It turns out that there is a better way of
doing c-CP that saves us space.
Proof of Theorem 3: We again start with constructing hash tables. For every element Sc ∈ C  we
create a hash-table and store Sc in bucket B(Sc) = [h1(Sc); h2(Sc); :::; hK(Sc)]  where hi is chosen
uniformly from minwise independent family of hash functions H. We create L such hash-tables. For
a query element Sq we look for all pairs in bucket B(Sq) = [h1(Sq); h2(Sq); :::; hK(Sq)] and repeat
this for each of the L tables. Note  we do not form pairs of elements retrieved from different tables
as they do not satisfy Eq. (2). If there exists a pair S1  S2 ∈ C with Sim(Sq; S1; S2) ≥ R0  using
Eq. (2)  we can see that we will ﬁnd that pair in bucket B(Sq) with probability 1 − (1 − RK
0 )L.
Here  we cannot use traditional choice of K and L  similar to what we did in Theorem 1  as there
⌉ and L = ⌈n2(cid:26) log( 1
(cid:14) )⌉ 
are O(n2) instead of O(n) possible pairs. We instead use K = ⌈ 2 log n
with (cid:26) = 1 −
. With this choice of K and L  the result follows. Note  the process
is stopped as soon as we ﬁnd pairs S1 and S2 with Sim(Sq; S1; S2) ≥ cR0. The key argument that
saves space from O(n2(1+(cid:26))) to O(n1+2(cid:26)) is that we hash n points individually. Eq. (2) makes it
clear that hashing all possible pairs is not needed when every point can be processed individually 
(cid:3)
and pairs formed within each bucket itself ﬁlter out most of the unnecessary combinations.

log 1=c+log 1=R0

log 1
cR0

log 1=c

5

log 1=c

log 1=c+log 1=R0

.

Theorem 4 For R3way c-Best Cluster Problem (or c-BC) there exist an algorithm with running time
O(n1+2(cid:26) log1=cR0 n)  where (cid:26) = 1 −
(cid:3)
The argument similar to one used in proof of Theorem 3 leads to the running time of
O(n1+3(cid:26) log1=cR0 n) as we need L = O(n3(cid:26))  and we have to processes all points at least once.
Proof of Theorem 4: Repeat c-CP problem n times for every element in collection C acting
as query once. We use the same set of hash tables and hash functions every time. The prepro-
cessing time is O(n1+2(cid:26) log1=cR0 n) evaluations of hash functions and the total querying time is
O(n × n2(cid:26) log1=cR0 n)  which makes the total running time O(n1+2(cid:26) log1=cR0 n).
(cid:3)
For k-way c-BC Problem  we can achieve O(n1+(k−1)(cid:26) log1=cR0 n) running time. If we are inter-
ested in very high similarity cluster  with R0 ≈ 1  then (cid:26) ≈ 0  and the running time is around
O(n log n). This is a huge saving over the brute force O(nk). In most practical cases  specially in
big data regime where we have enormous amount of data  we can expect the k-way similarity of
good clusters to be high and ﬁnding them should be efﬁcient. We can see that with increasing k 
hashing techniques save more computations.
7 Experiments
In this section  we demonstrate the usability of 3-way and higher-order similarity search using (i)
Google Sets  and (ii) Improving retrieval quality.
7.1 Google Sets: Generating Semantically Similar Words
Here  the task is to retrieve words which are “semantically” similar to the given set of query words.
We collected 1.2 million random documents from Wikipedia and created a standard term-doc bi-
nary vector representation of each term present in the collected documents after removing standard
stop words and punctuation marks. More speciﬁcally  every word is represented as a 1.2 million di-
mension binary vector indicating its presence or absence in the corresponding document. The total
number of terms (or words) was around 60 000 in this experiment.
Since there is no standard benchmark available for this task  we show qualitative evaluations. For
querying  we used the following four pairs of semantically related words: (i) “jaguar” and “tiger”;
(ii) “artiﬁcial” and “intelligence”; (iii) “milky” and “way” ; (iv) “ﬁnger” and “lakes”. Given the
query words w1 and w2  we compare the results obtained by the following four methods.
• Google Sets: We use Google’s algorithm and report 5 words from Google spreadsheets [1].
• 3-way Resemblance (3-way): We use 3-way resemblance
|w1∩w2∩w|
|w1∪w2∪w| to rank every word
• Sum Resemblance (SR): Another intuitive method is to use the sum of pairwise resem-
• Pairwise Intersection (PI): We ﬁrst retrieve top 100 words based on pairwise resemblance
for each w1 and w2 independently. We then report the words common in both. If there is
no word in common we do not report anything.

|w2∩w|
|w2∪w| and report top 5 words based on this ranking.

This is Google’s algorithm which uses its own data.

w and report top 5 words based on this ranking.

blance

|w1∩w|
|w1∪w| +

The results in Table 1 demonstrate that using 3-way resemblance retrieves reasonable candidates
for these four queries. An interesting query is “ﬁnger” and “lakes”. Finger Lakes is a region in
upstate New York. Google could only relate it to New York  while 3-way resemblance could even
retrieve the names of cities and lakes in the region. Also  for query “milky” and “way”  we can
see some (perhaps) unrelated words like “dance” returned by Google. We do not see such random
behavior with 3-way resemblance. Although we are not aware of the algorithm and the dataset used
by Google  we can see that 3-way resemblance appears to be a right measure for this application.
The above results also illustrate the problem with using the sum of pairwise similarity method. The
similarity value with one of the words dominates the sum and hence we see for queries “artiﬁcial”
and “intelligence” that all the retrieved words are mostly related to the word “intelligence”. Same is
the case with query “ﬁnger” and “lakes” as well as “jaguar” and “tiger”. Note that “jaguar” is also a
car brand. In addition  for all 4 queries  there was no common word in the top 100 words similar to
the each query word individually and so PI method never returns anything.

6

Table 1: Top ﬁve words retrieved using various methods for different queries.

“JAGUAR” AND “ TIGER”

GOOGLE

LION

LEOPARD
CHEETAH

CAT
DOG

3-WAY
LEOPARD
CHEETAH

LION

PANTHER

CAT

SR
CAT

PI
—
LEOPARD —
—
LITRE
—
BMW
CHASIS —

GOOGLE
DANCE
STARS
SPACE
THE

UNIVERSE

STARS
EARTH
LIGHT
SPACE

“MILKY” AND “ WAY”
SR
EVEN

3-WAY
GALAXY

PI
—
ANOTHER —
—
—
—

STILL
BACK
TIME

“ARTIFICIAL” AND “INTELLIGENCE”
GOOGLE
COMPUTER

COMPUTER

3-WAY

PROGRAMMING

SCIENCE

SCIENCE
ROBOT

ROBOTICS

INTELLIGENT

HUMAN

TECHNOLOGY

SR

PI
SECURITY —
WEAPONS —
SECRET —
ATTACKS —
HUMAN —

“FINGER” AND “LAKES”

GOOGLE

NEW
YORK

NY

PARK
CITY

3-WAY
SENECA
CAYUGA

ERIE

ROCHESTER
IROQUOIS

SR

RIVERS

PI
—
FRESHWATER —
—
—
STREAMS
FORESTED —

FISH

Improving Retrieval Quality in Similarity Search

We should note the importance of the denominator term in 3-way resemblance  without which fre-
quent words will be blindly favored. The exciting contribution of this paper is that 3-way resem-
blance similarity search admits provable sub-linear guarantees  making it an ideal choice. On the
other hand  no such provable guarantees are known for SR and other heuristic based search methods.
7.2
We also demonstrate how the retrieval quality of traditional similarity search can be boosted by uti-
lizing more query candidates instead of just one. For the evaluations we choose two public datasets:
MNIST and WEBSPAM  which were used in a recent related paper [26] for near neighbor search
with binary data using b-bit minwise hashing [20  23].
The two datasets reﬂect diversity both in terms of task and scale that is encountered in practice.
The MNIST dataset consists of handwritten digit samples. Each sample is an image of 28 × 28
pixel yielding a 784 dimension vector with the associated class label (digit 0 − 9). We binarize the
data by settings all non zeros to be 1. We used the standard partition of MNIST  which consists
of 10 000 samples in one set and 60 000 in the other. The WEBSPAM dataset  with 16 609 143
features  consists of sparse vector representation of emails labeled as spam or not. We randomly
sample 70 000 data points and partitioned them into two independent sets of size 35 000 each.

Table 2: Percentage of top candidates with the same labels as that of query retrieved using various
similarity criteria. More indicates better retrieval quality (Best marked in bold).

TOP
Pairwise
3-way NNbor
4-way NNbor

1
94.20
96.90
97.70

MNIST

10

92.33
96.13
96.89

20

91.10
95.36
96.28

50

89.06
93.78
95.10

1
98.45
99.75
99.90

WEBSPAM
10
20

96.94
98.68
98.87

96.46
97.80
98.15

50

95.12
96.11
96.45

For evaluation  we need to generate potential similar search query candidates for k-way search. It
makes no sense in trying to search for object simultaneously similar to two very different objects. To
generate such query candidates  we took one independent set of the data and partition it according
to the class labels. We then run a cheap k-mean clustering on each class  and randomly sample
triplets < x1; x2; x3 > from each cluster for evaluating 2-way  3-way and 4-way similarity search.
For MNIST dataset  the standard 10 000 test set was partitioned according to the labels into 10 sets 
each partition was then clustered into 10 clusters  and we choose 10 triplets randomly from each
cluster. In all we had 100 such triplets for each class  and thus 1000 overall query triplets. For
WEBSPAM  which consists only of 2 classes  we choose one of the independent set and performed
the same procedure. We selected 100 triplets from each cluster. We thus have 1000 triplets from
each class making the total number of 2000 query candidates.
The above procedures ensure that the elements in each triplets < x1; x2; x3 > are not very far from
each other and are of the same class label. For each triplet < x1; x2; x3 >  we sort all the points x
in the other independent set based on the following:

• Pairwise: We only use the information in x1 and rank x based on resemblance

|x1∩x|
|x1∪x|.

7

• 3-way NN: We rank x based on 3-way resemblance
• 4-way NN: We rank x based on 4-way resemblance

|x1∩x2∩x|
|x1∪x2∪x| .
|x1∩x2∩x3∩x|
|x1∪x2∪x3∪x|.

We look at the top 1  10  20 and 50 points based on orderings described above. Since  all the
query triplets are of the same label  The percentage of top retrieved candidates having same label as
that of the query items is a natural metric to evaluate the retrieval quality. This percentage values
accumulated over all the triplets are summarized in Table 2.
We can see that top candidates retrieved by 3-way resemblance similarity  using 2 query points 
are of better quality than vanilla pairwise similarity search. Also 4-way resemblance  with 3 query
points  further improves the results compared to 3-way resemblance similarity search. This clearly
demonstrates that multi-way resemblance similarity search is more desirable whenever we have
more than one representative query in mind. Note that  for MNIST  which contains 10 classes  the
boost compared to pairwise retrieval is substantial. The results follow a consistent trend.
8 Future Work
While the work presented in this paper is promising for efﬁcient 3-way and k-way similarity search
in binary high-dimensional data  there are numerous interesting and practical research problems we
can study as future work. In this section  we mention a few such examples.
One-permutation hashing. Traditionally  building hash tables for near neighbor search required
many (e.g.  1000) independent hashes. This is both time- and energy-consuming  not only for build-
ing tables but also for processing un-seen queries which have not been processed. One permutation
hashing [22] provides the hope of reducing many permutations to merely one. The version in [22] 
however  was not applicable to near neighbor search due to the existence of many empty bins (which
offer no indexing capability). The most recent work [27] is able to ﬁll the empty bins and works
well for pairwise near neighbor search. It will be interesting to extend [27] to k-way search.
Non-binary sparse data. This paper focuses on minwise hashing for binary data. Various extensions
to real-valued data are possible. For example  our results naturally apply to consistent weighted
sampling [25  15]  which is one way to handle non-binary sparse data. The problem  however  is not
solved if we are interested in similarities such as (normalized) k-way inner products  although the
line of work on Conditional Random Sampling (CRS) [19  18] may be promising. CRS works on
non-binary sparse data by storing a bottom subset of nonzero entries after applying one permutation
to (real-valued) sparse data matrix. CRS performs very well for certain applications but it does not
work in our context because the bottom (nonzero) subsets are not properly aligned.
Building hash tables by directly using bits from minwise hashing. This will be a different approach
from the way how the hash tables are constructed in this paper. For example  [26] directly used
the bits from b-bit minwise hashing [20  23] to build hash tables and demonstrated the signiﬁcant
advantages compared to sim-hash [8  12] and spectral hashing [29]. It would be interesting to see
the performance of this approach in k-way similarity search.
k-Way sign random projections. It would be very useful to develop theory for k-way sign random
projections. For usual (real-valued) random projections  it is known that the volume (which is related
to the determinant) is approximately preserved [24  17]. We speculate that the collision probability
of k-way sign random projections might be also a (monotonic) function of the determinant.
9 Conclusions
We formulate a new framework for k-way similarity search and obtain fast algorithms in the case of
k-way resemblance with provable worst-case approximation guarantees. We show some applications
of k-way resemblance search in practice and demonstrate the advantages over traditional search. Our
analysis involves the idea of probabilistic hashing and extends the well-known LSH family beyond
the pairwise case. We believe the idea of probabilistic hashing still has a long way to go.

Acknowledgement
The work is supported by NSF-III-1360971  NSF-Bigdata-1419210  ONR-N00014-13-1-0764  and
AFOSR-FA9550-13-1-0137. Ping Li thanks Kenneth Church for introducing Google Sets to him in
the summer of 2004 at Microsoft Research.

8

References
[1] http://www.howtogeek.com/howto/15799/how-to-use-autoﬁll-on-a-google-docs-spreadsheet-quick-tips/.
[2] S. Agarwal  Jongwoo Lim  L. Zelnik-Manor  P. Perona  D. Kriegman  and S. Belongie. Beyond pairwise

clustering. In CVPR  2005.

[3] Sihem Amer-Yahia  Senjuti Basu Roy  Ashish Chawlat  Gautam Das  and Cong Yu. Group recommenda-

tion: semantics and efﬁciency. Proc. VLDB Endow.  2(1):754–765  2009.

[4] Christina Brandt  Thorsten Joachims  Yisong Yue  and Jacob Bank. Dynamic ranked retrieval. In WSDM 

pages 247–256  2011.

[5] Andrei Z. Broder. On the resemblance and containment of documents. In the Compression and Complexity

of Sequences  pages 21–29  Positano  Italy  1997.

[6] Andrei Z. Broder  Moses Charikar  Alan M. Frieze  and Michael Mitzenmacher. Min-wise independent

permutations (extended abstract). In STOC  pages 327–336  Dallas  TX  1998.

[7] Olivier Chapelle  Patrick Haffner  and Vladimir N. Vapnik. Support vector machines for histogram-based

image classiﬁcation. IEEE Trans. Neural Networks  10(5):1055–1064  1999.

[8] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  2002.
[9] Flavio Chierichetti and Ravi Kumar. LSH-preserving functions and their applications. In SODA  2012.
[10] Dennis Fetterly  Mark Manasse  Marc Najork  and Janet L. Wiener. A large-scale study of the evolution

of web pages. In WWW  pages 669–678  Budapest  Hungary  2003.

[11] Jerome H. Friedman  F. Baskett  and L. Shustek. An algorithm for ﬁnding nearest neighbors.

Transactions on Computers  24:1000–1006  1975.

IEEE

[12] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and

satisﬁability problems using semideﬁnite programming. Journal of ACM  42(6):1115–1145  1995.

[13] Sariel Har-Peled  Piotr Indyk  and Rajeev Motwani. Approximate nearest neighbor: Towards removing

the curse of dimensionality. Theory of Computing  8(14):321–350  2012.

[14] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimen-

sionality. In STOC  pages 604–613  Dallas  TX  1998.

[15] Sergey Ioffe. Improved consistent sampling  weighted minhash and l1 sketching. In ICDM  2010.
[16] Yugang Jiang  Chongwah Ngo  and Jun Yang. Towards optimal bag-of-features for object categorization

and semantic video retrieval. In CIVR  pages 494–501  Amsterdam  Netherlands  2007.

[17] Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Technical report 

arXiv:1207.6083  2013.

[18] Ping Li and Kenneth W. Church. A sketch algorithm for estimating two-way and multi-way associations.

Computational Linguistics (Preliminary results appeared in HLT/EMNLP 2005)  33(3):305–354  2007.

[19] Ping Li  Kenneth W. Church  and Trevor J. Hastie. Conditional random sampling: A sketch-based sam-

pling technique for sparse data. In NIPS  pages 873–880  Vancouver  Canada  2006.

[20] Ping Li and Arnd Christian K¨onig. b-bit minwise hashing.

In Proceedings of the 19th International

Conference on World Wide Web  pages 671–680  Raleigh  NC  2010.

[21] Ping Li  Arnd Christian K¨onig  and Wenhao Gui. b-bit minwise hashing for estimating three-way simi-

larities. In NIPS  Vancouver  Canada  2010.

[22] Ping Li  Art B Owen  and Cun-Hui Zhang. One permutation hashing. In NIPS  Lake Tahoe  NV  2012.
[23] Ping Li  Anshumali Shrivastava  and Arnd Christian K¨onig. b-bit minwise hashing in practice. In Inter-

netware  Changsha  China  2013.

[24] Avner Magen and Anastasios Zouzias. Near optimal dimensionality reductions that preserve volumes. In

APPROX / RANDOM  pages 523–534  2008.

[25] Mark Manasse  Frank McSherry  and Kunal Talwar. Consistent weighted sampling. Technical Report

MSR-TR-2010-73  Microsoft Research  2010.

[26] Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data. In ECML 

Bristol  UK  2012.

[27] Anshumali Shrivastava and Ping Li. Densifying one permutation hashing via rotation for fast near neigh-

bor search. In ICML  Beijing  China  2014.

[28] Roger Weber  Hans-J¨org Schek  and Stephen Blott. A quantitative analysis and performance study for

similarity-search methods in high-dimensional spaces. In VLDB  pages 194–205  1998.

[29] Yair Weiss  Antonio Torralba  and Robert Fergus. Spectral hashing. In NIPS  Vancouver  Canada  2008.
[30] D. Zhou  J. Huang  and B. Sch¨olkopf. Beyond pairwise classiﬁcation and clustering using hypergraphs.

In NIPS  Vancouver  Canada  2006.

9

,Anshumali Shrivastava
Ping Li