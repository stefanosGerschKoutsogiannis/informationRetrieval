2014,Efficient Minimax Strategies for Square Loss Games,We consider online prediction problems where the loss between the prediction and the outcome is measured by the squared Euclidean distance and its generalization  the squared Mahalanobis distance. We derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the Brier game) and the $\ell_2$ ball (this setup is related to Gaussian density estimation). We show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state  with coefficients that can be efficiently computed using an explicit recurrence relation. The resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.,Efﬁcient Minimax Strategies for Square Loss Games

Wouter M. Koolen

Queensland University of Technology and UC Berkeley

wouter.koolen@qut.edu.au

Alan Malek

University of California  Berkeley
malek@eecs.berkeley.edu

University of California  Berkeley and Queensland University of Technology

Peter L. Bartlett

peter@berkeley.edu

Abstract

We consider online prediction problems where the loss between the prediction and
the outcome is measured by the squared Euclidean distance and its generalization 
the squared Mahalanobis distance. We derive the minimax solutions for the case
where the prediction and action spaces are the simplex (this setup is sometimes
called the Brier game) and the (cid:96)2 ball (this setup is related to Gaussian density
estimation). We show that in both cases the value of each sub-game is a quadratic
function of a simple statistic of the state  with coefﬁcients that can be efﬁciently
computed using an explicit recurrence relation. The resulting deterministic mini-
max strategy and randomized maximin strategy are linear functions of the statistic.

1

Introduction

We are interested in general strategies for sequential prediction and decision making (a.k.a. online
learning) that improve their performance with experience. Since the early days of online learning 
people have formalized such learning tasks as regret games. The learner interacts with an adver-
sarial environment with the goal of performing almost as well as the best strategy from some ﬁxed
reference set. In many cases  we have efﬁcient algorithms with an upper bound on the regret that
meets the game-theoretic lower bound (up to a small constant factor). In a few special cases  we
have the exact minimax strategy  meaning that we understand the learning problem at all levels of
detail. In even fewer cases we can also efﬁciently execute the minimax strategy. These cases serve
as exemplars to guide our thinking about learning algorithms.
In this paper we add two interesting examples to the canon of efﬁciently computable minimax strate-
gies. Our setup  as described in Figure 1  is as follows. The Learner and the Adversary play vectors
a ∈ A and x ∈ X   upon which the Learner is penalized using the squared Euclidean distance
(cid:107)a − x(cid:107)2 or its generalization  the squared Mahalanobis distance 

(cid:107)a − x(cid:107)2

(cid:124)
W = (a − x)

W −1(a − x) 

parametrized by a symmetric matrix W (cid:31) 0. After a sequence of T such interactions  we compare
the loss of the Learner to the loss of the best ﬁxed prediction a∗ ∈ A. In all our examples  this best
ﬁxed action in hindsight is the mean outcome a∗ = 1
t=1 xt  regardless of W . We use regret  the
difference between the loss of the learner and the loss of a∗  to evaluate performance. The minimax
regret for the T -round game  also known as the value of the game  is given by

(cid:80)T

T

V := inf
a1

sup
x1

··· inf

aT

sup
xT

T(cid:88)

t=1

T(cid:88)

t=1

1
2

(cid:107)at − xt(cid:107)2

W − inf

a

1

(cid:107)a − xt(cid:107)2

W

1
2

(1)

where the at range over actions A and the xt range over outcomes X . The minimax strategy chooses
the at  given all past outcomes x1  . . .   xt−1  to achieve this regret. Intuitively  the minimax regret
is the regret if both players play optimally while assuming the other player is doing the same.
Our ﬁrst example is the Brier game  where the action and outcome spaces are the probability simplex
with K outcomes. The Brier game is traditionally popular in meteorology [Bri50].
Our second example is the ball game  where the
action and outcome spaces are the Euclidean norm
ball  i.e. A = X = {x ∈ RK | (cid:107)x(cid:107)2 = 1}. (Even
though we measure loss by the squared Mahalanobis
distance  we play on the standard Euclidean norm
ball.) The ball game is related to Gaussian density
estimation [TW00].
In each case we exhibit a strategy that can play a
T -round game in O(T K 2) time.
(The algorithm
spends O(T K + K 3) time pre-processing the game 
and then plays in O(K 2) time per round.)

• Learner chooses prediction at ∈ A
• Adversary chooses outcome xt ∈ X
• Learner incurs loss 1
2(cid:107)at − xt(cid:107)2
W .

Given: T   W   A  X .
For t = 1  2  . . .   T

Figure 1: Protocol

2 Outline

We deﬁne our loss using the squared Mahalanobis distance  parametrized by a symmetric matrix
W (cid:31) 0. We recover the squared Euclidean distance by choosing W = I. Our games will always
last T rounds. For some observed data x1  . . .   xn  the value-to-go for the remaining T − n rounds
is given by

V (x1  . . .   xn) := inf
an+1

sup
xn+1

··· inf

aT

sup
xT

1
2

(cid:107)at − xt(cid:107)2

W − inf

a

(cid:107)a − xt(cid:107)2
W .

1
2

By deﬁnition  the minimax regret (1) is V = V () where  is the empty sequence  and the value-to-
go satisﬁes the recurrence

T(cid:88)

t=n+1

T(cid:88)

t=1

(cid:40)− inf a

(cid:80)T

1

2(cid:107)a − xt(cid:107)2

t=1

W

1

2(cid:107)an+1 − xn+1(cid:107)2

if n = T  
if n < T .

(2)

V (x1  . . .   xn) =

inf an+1 supxn+1

W + V (x1  . . .   xn+1)

t=1 xt and σ2 =(cid:80)n

(x1  . . .   xn) of length n  we summarize the state by s =(cid:80)n

Our analysis for the two games proceeds in a similar manner. For some past history of plays
(cid:124)
t W −1xt. As
t=1 x
we will see  the value-to-go after n of T rounds can be written as V (s  σ2  n); i.e. it only depends
on the past plays through s and σ2. More surprisingly  for each n  the value-to-go V (s  σ2  n) is
a quadratic function of s and a linear function of σ2 (under certain conditions on W ). While it
is straightforward to see that the terminal value V (s  σ2  T ) is quadratic in the state (this is easily
checked by computing the loss of the best expert and using the ﬁrst case of Equation (2))  it is not at
W −1x  n + 1) to V (s  σ2  n)  using the second
all obvious that propagating from V (s + x  σ2 + x
case of (2)  preserves this structure.
This compact representation of the value-function is an essential ingredient for a computationally
feasible algorithm. Many minimax approaches  such as normalized maximum likelihood [Sht87] 
have computational complexities that scale exponentially with the time horizon. We derive a strategy
that can play in constant amortized time.
Why is this interesting? We go beyond previous work in a few directions. First  we exhibit two new
games that belong to the tiny class admitting computationally feasible minimax algorithms. Second 
we consider the setting with squared Mahalanobis loss which allows the user intricate control over
the penalization of different prediction errors. Our results clearly show how the learner should
exploit this prioritization.

(cid:124)

2.1 Related work

Repeated games with minimax strategies are frequently studied ([CBL06]) and  in online learning 
minimax analysis has been applied to a variety of losses and repeated games; however  computa-

2

tionally feasible algorithms are the exception  not the rule. For example  consider log loss  ﬁrst
discussed in [Sht87]. Whiile the minimax algorithm  Normalized Maximum Likelihood  is well
known [CBL06]  it generally requires computation that is exponential in the time horizon as one
needs to aggregate over all data sequences. To our knowledge  there are two exceptions where
efﬁcient NML forecasters are possible: the multinomial case where fast Fourier transforms may
be exploited [KM05]  and very particular exponential families that cause NML to be a Bayesian
strategy [HB12]  [BGH+13]. The minimax optimal strategy is known also for: (i) the ball game
with W = I [TW00] (our generalization to Mahalanobis W (cid:54)= I results in fundamentally differ-
ent strategies)  (ii) the ball game with W = I and a constraint on the player’s deviation from the
current empirical minimizer [ABRT08] (for which the optimal strategy is Follow-the-Leader)  (iii)
Lipschitz-bounded convex loss functions [ABRT08]  (iv) experts with an L∗ bound [AWY08]  and
(v) static experts with absolute loss [CBS11]. While not guaranteed to be an exhaustive list  the
previous paragraph demonstrates the rarity of tractable minimax algorithms.

3 The Ofﬂine Problem

The regret is deﬁned as the difference between the loss of the algorithm and the loss of the best
action in hindsight. Here we calculate that action and its loss.
Lemma 3.1. Suppose A ⊇ conv(X ) (this will always hold in the settings we study). For data
x1  . . .   xT ∈ X   the loss of the best action in hindsight equals

T(cid:88)

t=1

inf
a∈A

(cid:107)a − xt(cid:107)2

W =

1
2

1
2

(cid:32) T(cid:88)

t=1

(cid:124)
t W −1xt − 1
(cid:80)T
x
T

t=1 xt.

T

and the minimizer is the mean outcome a∗ = 1

(cid:32) T(cid:88)

(cid:33)(cid:124)

xt

W −1

(cid:32) T(cid:88)

(cid:33)(cid:33)

t=1

t=1

xt

 

(3)

Proof. The unconstrained minimizer and value are obtained by equating the derivative to zero and
plugging in the solution. The assumption A ⊇ conv(X ) ensures that the constraint a ∈ A is
inactive.
(cid:80)t−1
The best action in hindsight is curiously independent of W   A and X . This also shows that the
s=1 xs is independent of W and A as well. As we
follow the leader strategy that plays at = 1
t−1
shall see  the minimax strategy does not have this property.

4 Simplex (Brier) Game

(cid:124)

+ | 1

2(cid:107)a − x(cid:107)2

In this section we analyze the Brier game. The action and outcome spaces are the probability simplex
x = 1}. The loss is given by half the squared
on K outcomes; A = X = (cid:52) := {x ∈ RK
Mahalanobis distance  1
W . We present a full minimax analysis of the T -round game: we
calculate the game value  derive the maximin and minimax strategies  and discuss their efﬁcient
implementation.
The structure of this section is as follows.
In Lemmas 4.1 and 4.2  the conclusions (value and
optimizers) are obtained under the proviso that the given optimizer lies in the simplex. In our main
result  Theorem 4.3  we apply these auxiliary results to our minimax analysis and argue that the
maximizer indeed lies in the simplex. We immediately work from a general symmetric W (cid:31) 0 with
the following lemma.
Lemma 4.1. Fix a symmetric matrix C (cid:31) 0 and vector d. The optimization problem

(cid:16)

(cid:124)
d

Cd − (1

has value 1
2

(cid:124)
Cd−1)2
= 1
1(cid:124)C1
2
Cd − 1
(cid:124)
d − 1
1(cid:124)C1
provided that p∗ is in the simplex.

p∗ = C

(cid:18)

(cid:17)

C−1p + d

(cid:124)

2

p

max

p∈(cid:52) − 1
(cid:0)d
(cid:124)(cid:0)C − C11
(cid:18)
(cid:19)

(cid:124)

p

(cid:1) d + 21

(cid:124)
C
1(cid:124)C1
C − C11

(cid:124)
C
1(cid:124)C1

(cid:124)
Cd−1
1(cid:124)C1

(cid:19)

d +

C1
1(cid:124)C1

(cid:1) attained at optimizer

1

=

3

Proof. We solve for the optimal p∗. Introducing Lagrange multiplier λ for the constraint(cid:80)
p∗ = C(cid:0)d − 1
C(cid:0)d − 1

k pk =
(cid:124)
1  we need to have p = C (d − λ1) which results in λ = 1
Cd−1
1(cid:124)C1. Thus  the maximizer equals
(cid:124)

1(cid:124)C1 1(cid:1) which produces objective value 1

1(cid:124)C1 1(cid:1)(cid:124)

1(cid:124)C1 1(cid:1) .

(cid:0)d + 1

Cd−1

Cd−1

Cd−1

(cid:124)

(cid:124)

2

The statement follows from simpliﬁcation.

This lemma allows us to compute the value and saddle point whenever the future payoff is quadratic.
Lemma 4.2. Fix symmetric matrices W (cid:23) 0 and A such that W −1 + A (cid:23) 0  and a vector b. The
optimization problem

a∈(cid:52) max
min
x∈(cid:52)

1
2

(cid:107)a − x(cid:107)2

W +

(cid:124)

Ax + b

x

(cid:124)

1
2

x

achieves its value

1
2

(cid:124)

c

W c − 1
2

(cid:124)

(1

W c − 1)2
1(cid:124)W 1

where

c =

diag(cid:0)W −1 + A(cid:1) + b

at saddle point (the maximin strategy randomizes  playing x = ei with probability p∗
i )

(cid:18)

1
2

(cid:19)

a∗ = p∗ =

W − W 11

(cid:124)
W
1(cid:124)W 1

c +

W 1
1(cid:124)W 1

provided p∗ (cid:23) 0.
Proof. The objective is convex in x for each a as W −1 + A (cid:23) 0  so it is maximized at a corner
x = ek. We apply min-max swap (see e.g. [Sio58])  properness of the loss (which implies that
a∗ = p∗) and expand:

a∈(cid:52) max
min
x∈(cid:52)

1
2

= min

a∈(cid:52) max

k

(cid:107)a − x(cid:107)2

1
W +
2
(cid:107)a − ek(cid:107)2

(cid:20) 1

2

1
2
E
k∼p

(cid:20) 1

1
W +
2
(cid:107)a − ek(cid:107)2

(cid:124)

(cid:124)
Ax + b

x

x

(cid:124)
kAek + b

(cid:124)

e

ek

W +

(cid:124)
(cid:124)
kAek + b
e

1
2

(cid:21)

ek

(cid:21)

W +

(cid:124)

e

1
2

(cid:124)
kAek + b

diag(cid:0)W −1 + A(cid:1)(cid:124)

ek

p + b

(cid:124)

p

= max

p∈(cid:52) min
a∈(cid:52)
E
= max
p∈(cid:52)
k∼p
p∈(cid:52) − 1

= max

(cid:107)p − ek(cid:107)2
2
(cid:124)
W −1p +
1
2
The proof is completed by applying Lemma 4.1.

p

2

4.1 Minimax Analysis of the Brier Game

(cid:19)

(cid:18)

Next  we turn to computing V (s  σ2  n) as a recursion and specifying the minimax and maximin
strategies. However  for the value-to-go function to retain its quadratic form  we need an alignment
condition on W . We say that W is aligned with the simplex if

W − W 11

(cid:124)
W
1(cid:124)W 1

diag(W −1) (cid:23) − 2

W 1
1(cid:124)W 1

(4)
where (cid:23) denotes an entry-wise inequality between vectors. Note that many matrices besides I
satisfy this condition: for example  all symmetric 2 × 2 matrices. We can now fully specify the
value and strategies for the Brier game.
Theorem 4.3. Consider the T -round Brier game with Mahalanobis loss 1

satisfying the alignment condition (4). After n outcomes (x1  . . .   xn) with statistics s =(cid:80)n
and σ2 =(cid:80)n

W with W
t=1 xt

2(cid:107)a − x(cid:107)2

 

t=1 x

(cid:124)
t W −1xt the value-to-go is
W −1s − 1
2

(cid:124)
αns

1
2

V (s  σ2  n) =

(1 − nαn) diag(W −1)

(cid:124)

s + γn 

σ2 +

1
2

4

(cid:18)

(cid:19)

(cid:18)

and the minimax and maximin strategies are given by

a∗(s  σ2  n) = p∗(s  σ2  n) =

W 1
1(cid:124)W 1

+ αn+1

s − nW 1
1(cid:124)W 1

1
2
where the coefﬁcients are deﬁned recursively by

+

(1 − nαn+1)

(cid:19)

diag(W −1)

W − W 11

(cid:124)
W
1(cid:124)W 1

αT =

1
T
αn = α2
n+1 + αn+1
(1 − nαn+1)2

γn =

2

+ γn+1.

(cid:32)

γT = 0

(cid:124)
diag(W −1)

W diag(W −1) −

1
4

(cid:0) 1

(cid:124)
2 1

W diag(W −1) − 1(cid:1)2

1(cid:124)W 1

(cid:33)

(cid:80)T

Proof. We prove this by induction  beginning at the end of the game and working backwards in
time. Assume that V (s  σ2  T ) has the given form. Recall that the value at the end of the game is
V (s  σ2  T ) = − inf a
2(cid:107)a − xt(cid:107)2
W and is given by Lemma 3.1. Matching coefﬁcients  we
ﬁnd V (s  σ2  T ) corresponds to αT = 1
T and γT = 0.
Now assume that V has the assumed form after n rounds. Using s and σ2 to denote the state after
n − 1 rounds  we can write

t=1

1

V (s  σ2  n − 1) = min

1
2

(cid:107)a − x(cid:107)2
(cid:124)
(σ2 + x

W +
W −1x) +

a∈(cid:52) max
x∈(cid:52)
− 1
2

W −1(s + x)

(cid:124)
αn(s + x)
(1 − nαn) diag(W −1)

(cid:124)

(s + x) + γn.

1
2
1
2

Using Lemma 4.2 to evaluate the right hand side produces a quadratic function in the state  and we
can then match terms to ﬁnd αn−1 and γn−1 and the minimax and maximin strategy. The ﬁnal step
is checking the p∗ (cid:23) 0 condition necessary to apply Lemma 4.2  which is equivalent to W being
aligned with the simplex. See the appendix for a complete proof.

This full characterization of the game allows us to derive the following minimax regret bound.
Theorem 4.4. Let W satisfy the alignment condition (4). The minimax regret of the T -round simplex
game satisﬁes

(cid:32)

(cid:0) 1

(cid:124)

2 1

W diag(W −1) − 1(cid:1)2

(cid:33)

.

1(cid:124)W 1

V ≤ 1 + ln(T )

2

diag(W −1)

(cid:124)

W diag(W −1) −

1
4

Proof. The regret is equal to the value of the game  V = V (0  0  0) = γ0. First observe that

(1 − nαn+1)2 = 1 − 2nαn+1 + n2α2

n+1

= 1 − 2nαn+1 + n2(αn − αn+1)
= αn+1 + 1 − (n + 1)2αn+1 + n2αn.

After summing over n the last two terms telescope  and we ﬁnd

γ0 ∝ T−1(cid:88)

(1 − nαn+1)2 = − T 2αT +

(1 + αn+1) =

n=0

n=0

T−1(cid:88)

T(cid:88)

n=1

αn.

Each αn can be bounded by 1/n  as observed in [TW00  proof of Lemma 2]. In the base case n = T
this holds with equality  and for n < T we have
1

n(n + 2)

1

It follows that γ0 ∝(cid:80)T

αn = α2

n+1 + αn+1 ≤

n=1 αn ≤(cid:80)T

n=1

(n + 1)2 +
n ≤ 1 + ln(T ) as desired.

n + 1

=

1

1
n

(n + 1)2 ≤ 1

n

.

5

5 Norm Ball Game

This section parallels the previous. Here  we consider the online game with Mahalanobis loss and
A = X = (cid:13) := {x ∈ RK | (cid:107)x(cid:107) ≤ 1}  the 2-norm Euclidian ball (not the Mahalanobis ball). We
show that the value-to-go function is always quadratic in s and linear in σ2 and derive the minimax
and maximin strategies.
Lemma 5.1. Fix a symmetric matrix A and vector b and assume A + W −1 (cid:31) 0. Let λmax be the
−2 b ≤
largest eigenvalue of W −1 +A and vmax the corresponding eigenvector. If b
1  then the optimization problem

(λmaxI − A)

(cid:124)

(cid:107)a − x(cid:107)2

W +

1
2

1
2

(cid:124)

(cid:124)
Ax + x

b

x

inf
sup
x∈(cid:13)
a∈(cid:13)
−1 b + 1

2 λmax  minimax strategy a∗ = (λmaxI − A)−1b and a ran-

(cid:124)

(λmaxI − A)

(cid:18)

has value 1
2 b
domized maximin strategy that plays two unit length vectors  with
±

(cid:124)
(cid:107)vmax
a
(cid:124)
 
⊥a⊥
where a⊥ and a(cid:107) are the components of a∗ perpendicular and parallel to vmax.

x = a⊥ ±(cid:113)

2(cid:112)1 − a

(cid:124)
1 − a
⊥a⊥vmax

1
2

Pr

=

(cid:19)

Proof. As the objective is convex  the inner optimum must be on the boundary and hence will be at
a unit vector x. Introduce a Lagrange multiplier λ for x

x ≤ 1 to get the Lagrangian

(cid:124)

inf
a∈(cid:13)

inf
λ≥0

sup
x

(cid:107)a − x(cid:107)2

W +

1
2

1
2

(cid:124)

x

Ax + x

(cid:124)

b + λ

(1 − x

(cid:124)

x).

1
2

This is concave in x if W −1 + A − λI (cid:22) 0  that is  λmax ≤ λ. Differentiating yields the optimizer
x∗ = (W −1 + A − λI)−1(W −1a − b)  which leaves us with an optimization in only a and λ:

inf
a∈(cid:13)

inf

λ≥λmax

1
2

(cid:124)
a

W −1a − 1
2

(cid:124)
(W −1a − b)

(W −1 + A − λI)−1(W −1a − b) +

1
2

λ.

Since the inﬁmums are over closed sets  we can exchange their order. Unconstrained optimization
−1 b. Evaluating the objective at a∗ and using W −1a∗ − b =
of a results in a∗ = (λI − A)
W −1 (λI − A)

−1 b − b =(cid:0)W −1 + A − λI(cid:1) (λI − A)

i

 

(cid:124)

(cid:124)

b

inf

1
2

1
2

+ λ

−1 b +

(λI − A)

λ≥λmax

i λiuiu

λ = inf

(λmaxI − A)

using the spectral decomposition A = (cid:80)

λ≥λmax
(cid:124)
i . For λ ≥ λmax  we have λ ≥ λi. Taking
−2 b ≤ 1  this function is increasing in λ ≥ λmax  and so
derivatives  provided b
obtains its inﬁmum at λmax. Thus  when the assumed inequality is satisﬁed  the a∗ is minimax for
the given x∗.
To obtain the maximin strategy  we can take the usual convexiﬁcation where the Adversary plays
distributions P over the unit sphere. This allows us to swap the inﬁmum and supremum (see e.g.
Sion’s minimax theorem[Sio58]) and obtain an equivalent optimization problem. We then see that
(cid:124) of the distribution
the objective only depends on the mean µ = E x and second moment D = E xx
P . The characterization in [KNW13  Theorem 2.1] tells us that µ  D are the ﬁrst two moments of a
distribution on units iff tr(D) = 1 and D (cid:23) µµ
(cid:124)
W −1a − a

(cid:124). Then  our usual min-max swap yields
(cid:124)
W −1x +

W −1x +

(cid:20) 1

V = sup

Ax + b

(cid:21)

(cid:124)
a

x

(cid:124)

(cid:124)

(cid:32)(cid:88)

−1 b results in
(cid:124)
i b)2
1
(u
λ − λi
2

(cid:33)

2
(cid:124)
W −1a − a

inf
a∈(cid:13)

P

E
x∼P
(cid:124)
1
a
2
(cid:124)
µ

= sup
µ D

= sup
µ D

= − 1
2

inf
a∈(cid:13)
− 1
2
a∗(cid:124)
W −1a∗ + b

W −1µ +

(cid:124)

1
2

1
2

x

x

tr(cid:0)(W −1 + A)D(cid:1) + b
tr(cid:0)(W −1 + A)D(cid:1)

µ

(cid:124)

1
2

W −1µ +

tr(cid:0)(W −1 + A)D(cid:1) + b

1
2
a∗ + sup

1
2

D(cid:23)a∗a∗(cid:124)
tr(D)=1

(cid:124)

µ

6

µ

vmax

Figure 2: Illustration of the maximin distribution from Lemma 5.1. The mixture of red unit vectors
(cid:124)
with mean µ has second moment D = µµ

+ (1 − µ
(cid:124)

µ)vmaxv

(cid:124)
max.

where the second equality uses a = µ and the third used the saddle point condition µ∗ = a∗. The
matrix D with constraint tr(D) = 1 now seeks to align with the largest eigenvector of W −1 + A
but it also has to respect the constraint D (cid:23) a∗a∗(cid:124)
. We now re-parameterize by C = D − a∗a∗(cid:124)
.
We then need to ﬁnd

tr(cid:0)(W −1 + A)C(cid:1) .

sup
C(cid:23)0

tr(C)=1−a∗(cid:124)

a∗

1
2

By linearity of the objective the maximizer is of rank 1  and hence this is a (scaled) maximum
eigenvalue problem  with solution given by C∗ = (1 − a∗(cid:124)
+
(1 − a∗(cid:124)
(cid:124)
max. This essentially reduces ﬁnding P to a 2-dimensional problem  which can
be solved in closed form [KNW13  Lemma 4.1]. It is easy to verify that the mixture in the theorem
has the desired mean a∗ and second moment D∗. See Figure 2 for the geometrical intuition.

max  so that D∗ = a∗a∗(cid:124)
(cid:124)

a∗)vmaxv

a∗)vmaxv

Notice that both the minimax and maximin strategies only depend on W through λmax and vmax.

5.1 Minimax Analysis of the Ball Game

With the above lemma  we can compute the value and strategies for the ball game in an analogous
way to Theorem 4.3. Again  we ﬁnd that the value function at the end of the game is quadratic in
the state  and  surprisingly  remains quadratic under the backwards induction.
Theorem 5.2. Consider the T -round ball game with loss 1

to-go for a state with statistics s =(cid:80)n

t=1 xt and σ2 =(cid:80)n

2(cid:107)a − x(cid:107)2
(cid:124)
t W −1xt is
t=1 x

W . After n rounds  the value-

The minimax strategy plays

V (s  σ2  n) =

1
2

(cid:124)
s

Ans − 1
2

σ2 + γn.

a∗(s  σ2  n) = (cid:0)λmaxI − (An+1 − W −1)(cid:1)−1
(cid:18)
x = a⊥ ±(cid:113)
2(cid:112)1 − a

(cid:124)
1 − a
⊥a⊥vmax

(cid:19)

±

1
2

=

Pr

(cid:124)
(cid:107)vmax
a
(cid:124)
⊥a⊥

 

An+1s

and the maximin strategy plays two unit length vectors with

where λmax and vmax correspond to the largest eigenvalue of An+1 and a⊥ and a(cid:107) are the com-
ponents of a∗ perpendicular and parallel to vmax. The coefﬁcients An and γn are determined
recursively by base case AT = 1

(cid:1)−1
T W −1 and γT = 0 and recursion

(cid:0)W −1 + λmaxI − An+1

An+1 + An+1

An = An+1

γn =

1
2

λmax + γn+1.

7

Proof outline. The proof is by induction on the number n of rounds played. In the base case n = T
we ﬁnd (see (3)) AT = 1

T W −1 and γT = 0. For the the induction step  we need to calculate

V (s  σ2  n) = inf
a∈(cid:13)

sup
x∈(cid:13)

1
2

(cid:107)a − x(cid:107)2

(cid:124)
W + V (s + x  σ2 + x

W −1x  n + 1).

Using the induction hypothesis  we expand the right-hand-side to
An+1(s + x) − 1
2

(cid:107)a − x(cid:107)2

(cid:124)
(s + x)

W +

1
2

1
2

sup
x∈(cid:13)

inf
a∈(cid:13)

(σ2 + x

(cid:124)

W −1x) + γn+1.

which we can evaluate by applying Lemma 5.1 with A = An+1−W −1 and b = An+1s. Collecting
terms and matching with V (s  σ2  n) = 1
2 σ2 + γn yields the recursion for An and γn as
well as the given minimax and maximin strategies. As before  much of the algebra has been moved
to the appendix.

Ans− 1

(cid:124)
2 s

Understanding the eigenvalues of An As we have seen from the An recursion  the eigensystem
is always the same as that of W −1. Thus  we can characterize the minimax strategy completely by
its effect on the eigenvalues of W −1. Denote the eigenvalues of An and W −1 to be λi
n and νi 
respectively  with λ1

n corresponding to the largest eigenvalue. The eigenvalues follow:

+ λi

λi
n =

n+1)2
(λi
λi
n+1(νi + λ1
n+1 − λi
n+1 − λi
(cid:1)2
νi + λ1
νi + λ1
n unchanged. The largest eigenvalue λ1
which leaves the order of the λi
T /ν1 = 1/T and λ1
+ λ1
λ1
n+1/ν1
n = αnνmax.
the αn parameter in the Brier game  i.e. λmax
This observation is the key to analyzing the minimax regret.
Theorem 5.3. The minimax regret of the T -round ball game satisﬁes

n/ν1 =(cid:0)λ1

n+1 =

n+1

n+1)

 

n+1
n satisﬁes the recurrence
n+1/ν1  which  remarkably  is the same recurrence for

V ≤ 1 + ln(T )

λmax(W −1).

Proof. We have V = V (0  0  0) = γ0 = (1/2)(cid:80)T
proof of Theorem 4.4 gives the bound on(cid:80)T

2

n=1 αn.

n = (1/2)λmax(W −1)(cid:80)T

n=1 λmax

n=1 αn. The

Taking stock  we ﬁnd that the minimax regrets of the Brier game (Theorems 4.3) and ball game
(Theorems 5.2) have identical dependence on the horizon T but differ in a complexity factor arising
from the interaction of the action space and the loss matrix W .

6 Conclusion

In this paper  we have presented two games that  unexpectedly  have computationally efﬁcient min-
imax strategies. While the structure of the squared Mahalanobis distance is important  it is the
interplay between the loss and the constraint set that allows efﬁcient calculation of the backwards
induction  value-to-go  and achieving strategies. For example  the squared Mahalanobis game with
(cid:96)1 ball action spaces does not admit a quadratic value-to-go unless W = I.
We emphasize the low computational cost of this method despite the exponential blow-up in state
space size. In the Brier game  the αn coefﬁcients need to be precomputed  which can be done in
O(T ) time. Similarly  computation of the eigenvalues of the An coefﬁcients for the ball game can be
done in O(T K + K 3) time. Then  at each iteration of the algorithm  only matrix-vector multiplica-
tions between the current state and the precomputed parameters are required. Hence  playing either
T round game requires O(T K 2) time. Unfortunately  as is the case with most minimax algorithms 
the time horizon must be known in advance.
There are many different future directions. We are currently pursuing a characterization of action
spaces that permit quadratic value functions under squared Mahalanobis loss  and investigating con-
nections between losses and families of value functions closed under backwards induction. There
is some notion of conjugacy between losses  value-to-go functions  and action spaces  but a gen-
eralization seems difﬁcult: the Brier game and ball game worked out for seemingly very different
reasons.

8

References
[ABRT08] Jacob Abernethy  Peter L. Bartlett  Alexander Rakhlin  and Ambuj Tewari. Optimal
strategies and minimax lower bounds for online convex games. In Servedio and Zhang
[SZ08]  pages 415–423.
Jacob Abernethy  Manfred K. Warmuth  and Joel Yellin. When random play is optimal
against an adversary. In Servedio and Zhang [SZ08]  pages 437–446.

[AWY08]

[BGH+13] Peter L. Bartlett  Peter Grunwald  Peter Harremo¨es  Fares Hedayati  and Wojciech
Kotłowski. Horizon-independent optimal prediction with log-loss in exponential fami-
lies. CoRR  abs/1305.4324  2013.
Glenn W Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly
weather review  78(1):1–3  1950.

[Bri50]

[CBL06] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  learning  and games. Cambridge

University Press  2006.

[HB12]

[CBS11] Nicol`o Cesa-Bianchi and Ohad Shamir. Efﬁcient online learning via randomized round-
ing. In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger 
editors  Advances in Neural Information Processing Systems 24  pages 343–351  2011.
Fares Hedayati and Peter L. Bartlett. Exchangeability characterizes optimality of se-
quential normalized maximum likelihood and bayesian prediction with jeffreys prior.
In International Conference on Artiﬁcial Intelligence and Statistics  pages 504–510 
2012.
Petri Kontkanen and Petri Myllym¨aki. A fast normalized maximum likelihood algo-
rithm for multinomial data. In Proceedings of the Nineteenth International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI-05)  pages 1613–1616  2005.

[KM05]

[KNW13] Wouter M. Koolen  Jiazhong Nie  and Manfred K. Warmuth. Learning a set of direc-
In Shai Shalev-Shwartz and Ingo Steinwart  editors  Proceedings of the 26th

tions.
Annual Conference on Learning Theory (COLT)  June 2013.
Yurii Mikhailovich Shtar’kov. Universal sequential coding of single messages. Prob-
lemy Peredachi Informatsii  23(3):3–17  1987.
Maurice Sion. On general minimax theorems. Paciﬁc J. Math.  8(1):171–176  1958.
Rocco A. Servedio and Tong Zhang  editors. 21st Annual Conference on Learning
Theory - COLT 2008  Helsinki  Finland  July 9-12  2008. Omnipress  2008.
Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density
estimation. In 13th COLT  pages 100–106  2000.

[Sht87]

[Sio58]
[SZ08]

[TW00]

9

,Wouter Koolen
Alan Malek
Peter Bartlett
Sung-Soo Ahn
Michael Chertkov
Jinwoo Shin