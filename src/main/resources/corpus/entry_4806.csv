2019,SIC-MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits,Motivated by cognitive radio networks  we consider the stochastic multiplayer multi-armed bandit problem  where several players pull arms simultaneously and collisions occur if one of them is pulled by several players at the same stage.  We present a decentralized algorithm that achieves the same performance as a centralized one   contradicting the existing lower bounds for that problem. This is possible by ``hacking'' the standard model by constructing a communication protocol between players that deliberately enforces collisions  allowing them to share their information at a negligible cost. 
This motivates the introduction of a more appropriate dynamic setting without sensing  where similar communication protocols are no longer possible. However  we show that the logarithmic growth of the regret is still achievable for this model with a new algorithm.,SIC - MMAB: Synchronisation Involves

Communication in Multiplayer Multi-Armed Bandits

Etienne Boursier

CMLA  ENS Paris-Saclay

etienne.boursier@ens-paris-saclay.fr

Vianney Perchet

CMLA  ENS Paris-Saclay

Criteo AI Lab  Paris

vianney.perchet@normalesup.org

Abstract

Motivated by cognitive radio networks  we consider the stochastic multiplayer
multi-armed bandit problem  where several players pull arms simultaneously and
collisions occur if one of them is pulled by several players at the same stage.
We present a decentralized algorithm that achieves the same performance as a
centralized one  contradicting the existing lower bounds for that problem. This
is possible by “hacking” the standard model by constructing a communication
protocol between players that deliberately enforces collisions  allowing them to
share their information at a negligible cost. This motivates the introduction of a
more appropriate dynamic setting without sensing  where similar communication
protocols are no longer possible. However  we show that the logarithmic growth of
the regret is still achievable for this model with a new algorithm.

1

Introduction

In the stochastic Multi Armed Bandit problem (MAB)  a single player sequentially takes a decision
(or “pulls an arm”) amongst a ﬁnite set of possibilities [K] := {1  . . .   K}. After pulling arm k ∈ [K]
at stage t ∈ N∗  the player receives a random reward Xk(t) ∈ [0  1]  drawn i.i.d. according to some
unknown distribution νk of expectation µk := E[Xk(t)]. Her objective is to maximize her cumulative
reward up to stage T ∈ N∗. This sequential decision problem  ﬁrst introduced for clinical trials
[27  25]  involves an “exploration/exploitation dilemma” where the player must trade-off acquiring
vs. using information. The performance of an algorithm is controlled in term of regret  the difference
of the cumulated reward of an optimal algorithm knowing the distributions (νk)k∈[K] beforehand and
the cumulated reward of the player. It is known that any “reasonable” algorithm must incur at least a
logarithmic regret [19]  which is attained by some existing algorithms such as UCB [1  4].
MAB has been recently popularized thanks to its applications to online recommendation systems.
Many different variants of MAB and classes of algorithms have thus emerged in the recent years [see
11]. In particular  they have been considered for cognitive radios [16]  where the problem gets more
intricate as multiple users are involved and they collide if they pull the same arm k at the same time
t  i.e.  they transmit on the same channel. If this happens  they all receive 0 as a reward instead of
Xk(t)  meaning that no message is transmitted.
If a central agent controls simultaneously all players’ behavior then a tight lower bound is known
[3  18]. Yet this centralized problem is not adapted to cognitive radios  as it allows communication
between players at each time step; in practice  this induces signiﬁcant costs in both energy and
time. As a consequence  most of the current interest lies in the decentralized case [20  2  5]  which
presents another complication due to the feedback. Besides the received reward  an additional piece of
information may be observed at each time step. When this extra observation is the collision indicator 
Rosenski et al. [26] provided two algorithms for both a ﬁxed and a varying number of players. They
are based on a Musical Chairs procedure that quickly assigns players to different arms. Besson and

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Kaufmann [8] provided an efﬁcient UCB-based algorithm if Xk(t) is observed instead1. Lugosi and
Mehrabian [21] recently proposed an algorithm using no additional information. The performances
of these algorithms and the underlying model differences are summarized in Table 1  Section 1.1.
The ﬁrst non trivial lower bound for this problem has been recently improved [20  8]. These lower
bounds suggest that decentralization adds to the regret a multiplicative factor M  the number of
players  compared to the centralized case [3]. Interestingly  these lower bounds scale linearly with
the inverse of the gaps between the µk whereas this scaling is quadratic for most of the existing
algorithms. This is due to the fact that although collisions account for most of the regret  lower
bounds are proved without considering them.
Although it is out of our scope  the heterogeneous model introduced by Kalathil et al. [17] is worth
mentioning. In this case  the reward distribution depends on each user [6  7]. An algorithm reaching
the optimal allocation without explicit communication between the players was recently proposed
[9].
Our main contributions are the following:
Section 2: When collisions are observed  we introduce a new decentralized algorithm that is
“hacking” the setting and induces communication between players through deliberate collisions. The
regret of this algorithm reaches asymptotically (up to some universal constant) the lower bound of
the centralized problem  meaning that the aforementioned lower bounds are unfortunately incorrect.
This algorithm relies on the unrealistic assumption that all users start transmitting at the very same
time. It also explains why the current literature fails to provide near optimal results for the multiplayer
bandits. It therefore appears that the assumption of synchronization has to be removed for practical
application of the multiplayer bandits problem. On the other hand  this technique also shows that
exhibiting lower bounds in multi-player MAB is more complex than in stochastic standard MAB.

Section 3: Without synchronization or collision observations  we propose the ﬁrst algorithm with a
logarithmic regret. The dependencies in the gaps between rewards yet become quadratic.

1.1 Models

In this section  we introduce different models of multiplayer MAB with a known number of arms K
but an unknown number of players M ≤ K. The horizon T is assumed known to the players (for
simplicity of exposure  as the anytime generalization of results is now well understood [14]). At each
time step t ∈ [T ]  given their (private) information  all players j ∈ [M ] simultaneously pull the arms
πj(t) and receive the reward rj(t) ∈ [0  1] such that

rj(t) := Xπj (t)(t)(1 − ηπj (t)(t))  where ηπj (t)(t) is the collision indicator deﬁned by
Ck(t) := {j ∈ [M ] | πj(t) = k}.

ηk(t) := 1#Ck(t)>1

with

The problem is centralized if players can communicate any information to each other. In that case 
they can easily avoid collisions and share their statistics. In opposition  the problem is decentralized
when players have only access to their own rewards and actions. The crucial concept we introduce is
(a)synchronization between players. With synchronization  the model is called static.
Assumption 1 (Synchronization). Player i enters the bandit game at the time τi = 0 and stays until
the ﬁnal horizon T . This is common knowledge to all players.
Assumption 2 (Quasi-Asynchronization). Players enter at different times τi ∈ {0  . . .   T − 1} and
stay until the ﬁnal horizon T . The τi are unknown to all players (including i).

With quasi-asynchronicity2  the model is dynamic and several variants already exist [26]. Denote by
M(t) the set of players in the game at time t (unknown but not random) and by µ(n) the n-th order
statistics of µ  i.e.  µ(1) ≥ µ(2) ≥ . . . ≥ µ(K). The total regret is then deﬁned for both static and
dynamic models by:

T(cid:88)

#M(t)(cid:88)

 T(cid:88)

(cid:88)

 .

rj(t)

RT :=

µ(k) − Eµ

t=1

k=1

t=1

j∈M(t)

1We stress that Xk(t) does not necessarily correspond to the received reward in case of collision.
2We prefer not to mention asynchronicity as players still use shared discrete time slots.

2

As mentioned in the introduction  different observation settings are considered.

Collision Sensing: Player j observes ηπj (t)(t) and rj(t) at each time step.
No sensing: Player j only observes rj(t)  i.e.  a reward of 0 can indistinguishably come from a

collision with another player or a null statistic Xπj (t)(t).

Notice that as soon as P(Xk = 0) = 0  the No Sensing and Collision Sensing settings are equivalent.
The setting where both Xπj (t) and rj(t) are observed is also considered in the literature and is called
Statistic Sensing [8]. The No Sensing setting is the most difﬁcult one as there is no extra observation.
Table 1 below compares the performances of the major algorithms  specifying the precise setting
considered for each of them. The second algorithm of Lugosi and Mehrabian [21] and our algorithms
also have problem independent bounds that are not mentioned in Table 1 for the sake of clarity. Due
to space constraints  ADAPTED SIC-MMAB  SIC-MMAB2 and their related results are presented in
Appendix C. Note that the two dynamic algorithms in Table 1 rely on different speciﬁc assumptions.

Model

Algorithm’s Reference

Prior knowledge

Centralized Multiplayer

Theorem 1 [18]

Decentralized  Stat. Sensing

Theorem 11 [8]

M

M

Decentralized  Col. Sensing

Theorem 1 [26]

µ(M )−µ(M +1)

Decentralized  Col. Sensing

SIC-MMAB (Thm 1)

Decentralized  No Sensing

Theorem 1.1 [21]

-

M

Decentralized  No Sensing

Theorem 1.2 [21]

M  µ(M )

Decentralized  No Sensing

ADAPT. SIC-MMAB (Eq (13))

Decentralized  No Sensing

SIC-MMAB2 (Thm 3)

Dec.  Col. Sensing   Dynamic

Theorem 2 [26]

Dec.  No Sensing  Dynamic

DYN-MMAB (Thm 2)

µ(K)

µ(K)

¯∆(M )

-

Asymptotic Upper bound (up to constant factor)

k>M

log(T )

(cid:88)
M 3 (cid:88)
(cid:16)
µ(M )−µ(M +1)

µ(M )−µ(k)
(cid:16)
µ(i)−µ(k)
(cid:17)2

1≤i<k≤K

MK log(T )

log(T )

(cid:17)2

+ M K log(T )

log(T )

µ(M )−µ(k)
(cid:16)
µ(M )−µ(M +1)

MK log(T )

(cid:17)2

(cid:88)

k>M

MK2
µ(M )

log2(T ) + M K log(T )

log(T )

µ(M )−µ(k)

+ M 3K log(T )

µ(K)

∆(cid:48)

log2(cid:0) log(T )(cid:1)

(cid:88)

k>M

(cid:88)

k>M

M

log(T )

µ(M )−µ(k)

+ MK2
µ(K)

log(T )

M

√

K log(T )T
¯∆2

(M )

MK log(T )

¯∆2

(M )

+ M 2K log(T )

µ(M )

Table 1: Performances of different algorithms. Our algorithms and results are highlighted in
red. ¯∆(M ) := mini=1 ... M (µ(i) − µ(i+1)) is the smallest gap among the top-M + 1 arms and
∆(cid:48) := min{µ(M ) − µi | µ(M ) − µi > 0} is the positive sub-optimality gap.

2 Collision Sensing: achieving centralized performances by communicating

through collisions

In this section  we consider the Collision Sensing static model and prove that the decentralized
problem is almost as complex  in terms of regret growth  as the centralized one. When players are
synchronized  we provide an algorithm with an exploration regret similar to the known centralized
lower bound [3]. This algorithm strongly relies on the synchronization assumption  which we leverage
to allow communication between players through observed collisions. The communication protocol
is detailed and explained in Section 2.2.3. This result also implies that the two lower bounds provided
in the literature [8  20] are unfortunately not correct. Indeed  the factor M that was supposed to be
the cost of the decentralization in the regret should not appear.
Let us now describe our algorithm SIC-MMAB. It consists of several phases.

1. The initialization phase ﬁrst estimates the number of players and assigns ranks among them.
2. Players then alternate between exploration phases and communication phases.

3

(a) During the p-th exploration phase  each arm is pulled 2p times and its performance is

estimated in a Successive Accepts and Rejects fashion [22  12].

(b) During the communication phases  players communicate their statistics to each other
using collisions. Afterwards  the updated common statistics are known to all players.
3. The last phase  the exploitation one  is triggered for a player as soon as an arm is detected as

optimal and assigned to her. This player then pulls this arm until the ﬁnal horizon T .

2.1 Some preliminary notations

Players that are not in the exploitation phase are called active. We denote  with a slight abuse of
notation  by [Mp] the set of active players during the p-th phase of exploration-communication
and by Mp ≤ M its cardinality. Notice that Mp is non increasing because players never leave the
exploitation phase.
Any arm among the top-M ones is called optimal and any other arm is sub-optimal. Arms that
still need to be explored (players cannot determine whether they are optimal or sub-optimal yet) are
active. We denote  with the same abuse of notation  the set of active arms by [Kp] of cardinality
Kp ≤ K. By construction of our algorithm  this set is common to all active players at each stage.
Our algorithm is based on a protocol called sequential hopping [15]. It consists of incrementing
the index of the arm pulled by a speciﬁc player:
t at time t  she will play
πk
t+1 = πk

if she plays arm πk
t + 1 (mod [Kp]) at time t + 1 during the p-th exploration phase.

2.2 Description of our protocol

As mentioned above  the SIC-MMAB algorithm consists of several phases. During the communication
phase  players communicate with each other. At the end of this phase  each player thus knows
the statistics of all players on all arms  so that this decentralized problem becomes similar to the
centralized one. After alternating enough times between exploration and communication phases 
sub-optimal arms are eliminated and players are ﬁxed to different optimal arms and will exploit them
until stage T . The complete pseudocode of SIC-MMAB is given in Algorithm 1  Appendix A.1.

2.2.1 Initialization phase

The objective of the ﬁrst phase is to estimate the number of players M and to assign internal
ranks to players. First  players follow the Musical Chairs algorithm [26]  described in Pseudocode 4 
Appendix A.1  during T0 := (cid:100)K log(T )(cid:101) steps in order to reach an orthogonal setting  i.e.  a position
where they are all pulling different arms. The index of the arm pulled by a player at stage T0 will
then be her external rank.
The second procedure  given by Pseudocode 5 in Appendix A.1  determines M and assigns a unique
internal rank in [M ] to each player. For example  if there are three players on arms 5  7 and 2 at
t = T0  their external ranks are 5  7 and 2 respectively  while their internal ranks are 2  3 and 1.
Roughly speaking  the players follow each other sequentially hopping through all the arms so that
players with external ranks k and k(cid:48) collide exactly after a time k + k(cid:48). Each player then deduces M
and her internal rank from observed collisions during this procedure that lasts 2K steps.
In the next phases  active players will always know the set of active players [Mp]. This is how
the initial symmetry among players is broken and it allows the decentralized algorithm to establish
communication protocols.

2.2.2 Exploration phase

During the p-th exploration phase  active players sequentially hop among the active arms for Kp2p
steps. Any active arm is thus pulled 2p times by each active player. Using their internal rank  players
start and remain in an orthogonal setting during the exploration phase  which is collision-free.

Tk(p) =(cid:80)M

We denote by Bs = 3
the error bound after s pulls and by Tk(p) (resp. Sk(p)) the centralized
number of pulls (resp. sum of rewards) for the arm k during the p ﬁrst exploration phases  i.e. 
k (p) is the number of pulls for the arm k by player j during the p ﬁrst

2s

j=1 T j

k (p) where T j

(cid:113) log(T )

4

exploration phases. During the communication phase  quantized rewards(cid:101)Sj

between active players as described in Section 2.2.3.
After a succession of two phases (exploration and communication)  an arm k is accepted if

(cid:110)
i ∈ [Kp](cid:12)(cid:12)(cid:101)µk(p) − BTk(p) ≥(cid:101)µi(p) + BTi(p)
(cid:80)M
m=1 (cid:101)Sj

k(p)

#

(cid:111) ≥ Kp − Mp 

where(cid:101)µk(p) =

k(p) will be communicated

is the centralized quantized empirical mean of the arm k3  which is an
Tk(p) . This inequality implies that k is among the top-Mp active arms

approximation of ˆµk(p) = Sk(p)
with high probability. In the same way  k is rejected if

i ∈ [Kp](cid:12)(cid:12)(cid:101)µi(p) − BTi(p) ≥(cid:101)µk(p) + BTk(p)

(cid:111) ≥ Mp 

Tk(p)

(cid:110)

#

meaning that there are at least Mp active arms better than k with high probability. Notice that each
k(p) to accept/reject an arm instead of the exact ones

player j uses her own quantized statistics (cid:101)Sj
k(p). Otherwise  the estimations(cid:101)µk(p) would indeed differ between the players as well as the sets
and the conﬁdence bound can be chosen as Bs =(cid:112)2 log(T )/s.

Sj
of accepted and rejected arms. With Bernoulli distributions  the quantization becomes unnecessary

2.2.3 Communication phase

In this phase  each active player communicates  one at a time  her statistics of the active arms to all
other active players. Each player has her own communicating arm  corresponding to her internal
rank. When the player j is communicating  she sends a bit at a time step to the player l by deciding
which arm to pull: a 1 bit is sent by pulling the communicating arm of player l (a collision occurs)
and a 0 bit by pulling her own arm. The main originality of SIC-MMAB comes from this trick which
allows implicit communication through collisions and is used in subsequent papers [13  10  24]. In
an independent work  Tibrewal et al. [28] also proposed an algorithm using similar communication
protocols for the heterogeneous case.
As an arm is pulled 2n times by a single player during the n-th exploration phase  it has been
pulled 2p+1 − 1 times in total at the end of the p-th phase and the statistic Sj
k(p) is a real number in
k(p) ∈ [2p+1− 1] to each other in p + 1
k(p) − n be the integer and decimal parts of Sj
k(p) 
k(p)] = Sj
k(p).

[0  2p+1− 1]. Players then send a quantized integer statistic(cid:101)Sj
the quantized statistic is then n + 1 with probability d and n otherwise  so that E[(cid:101)Sj

bits  i.e.  collisions. Let n = (cid:98)Sj

k(p)(cid:99) and d = Sj

An active player can have three possible statuses during the communication phase:

1. either she is receiving some other players’ statistics about the arm k. In that case  she

proceeds to Receive Protocol (see Pseudocode 1).

2. Or she is sending her quantized statistics about arm k to player l (who is then receiving). In
that case  she proceeds to Send Protocol (see Pseudocode 2) to send them in a time p + 1.
3. Or she is pulling her communicating arm  while waiting for other players to ﬁnish communi-

cating statistics among them.

Communicated statistics are all of length p + 1  even if they could be sent with shorter messages  in
order to maintain synchronization among players. Using their internal ranks  the players can com-
municate in turn without interfering with each other. The general protocol for each communication
phase is described in Pseudocode 3 below.

At the end of the communication phase  all active players know the statistics (cid:101)Sj

k(p) and so which
arms to accept or reject. Rejected arms are removed right away from the set of active arms. Thanks
to the assigned ranks  accepted arms are assigned to one player each. The remaining active players
then update both sets of active players and arms as described in Algorithm 1  line 21.
This communication protocol uses the fact that a bit can be sent with a single collision. Without
sensing  this can not be done in a single time step  but communication is still somehow possible. A
bit can then be sent in log(T )
T . Using this trick  two different algorithms
µ(K)
relying on communication protocols are proposed in Appendix C for the No Sensing setting.

3For a player j already exploiting since the pj-th phase  we instead use the last statistic (cid:101)Sj

steps with probability 1 − 1

k(p) = (cid:101)Sj

k(pj).

5

Receive Protocol

Input: p (phase number)  l (own internal rank) 

[Kp] (set of active arms)

Output: s (statistic sent by the sending
player)
1: s ← 0 and π ← index of the l-th active arm
2: for n = 0  . . .   p do
3:
4:
5:
6: end for
7: return s
Pseudocode 1: receive statistics of length
p + 1.

Pull π
if ηπ(t) = 1 then # other player sends 1
s ← s + 2n end if

# sent statistics

Send Protocol

Input:

l (player receiving)  s (statistics to
send)  p (phase number)  j (own internal rank) 
[Kp] (set of active arms)
1: m ← binary writing of s of length p + 1  i.e. 

s =(cid:80)p

n=0 mn2n
2: for n = 0  . . .   p do
if mn = 1 then
3:
Pull the l-th active arm
4:
else Pull the j-th active arm
5:
end if
6:
7: end for
Pseudocode 2: send statistics s of length
p + 1 to player l.

# send 1
# send 0

Input: s (personal statistics of previous phases)  p (phase number)  j (own internal rank)  [Kp] (set of active

Communication Protocol

arms)  [Mp] (set of active players)

Output:(cid:101)S (quantized statistics of all active players)
(cid:40)(cid:98)s[k](cid:99) + 1 with probability s[k] − (cid:98)s[k](cid:99)
1: For all k  sample(cid:101)s[k] =
2: Deﬁne Ep := {(i  l  k) ∈ [Mp] × [Mp] × [Kp] | i (cid:54)= l} and set(cid:101)Sj ←(cid:101)s

(cid:98)s[k](cid:99) otherwise

if i = j then Send (l (cid:101)s[k]  p  j  [Kp])
else if l = j then (cid:101)Si[k] ← Receive(p  j  [Kp])

else for p + 1 time steps do Pull the j-th active arm end for
end if

3: for (i  l  k) ∈ Ep do
4:
5:
6:
7:
8: end for

9: return (cid:101)S

# quantization

# Player i sends stats of arm k to player l
# player communicating
# player receiving
# wait while others communicate

Pseudocode 3: player with rank j proceeds to the p-th communication phase.

2.2.4 Regret bound of SIC-MMAB

Theorem 1 bounds the expected regret incurred by SIC-MMAB. Due to space constraints  its proof is
delayed to Appendix A.2.
Theorem 1. With the choice T0 = (cid:100)K log(T )(cid:101)  for any given set of parameters K  M and µµµ:

(cid:88)
(cid:3) ≤ c1

E(cid:2)RT

min

k>M

(cid:26) log(T )
(cid:26)
(cid:18)

µ(M ) − µ(k)

(cid:27)
 (cid:112)T log(T )

+ c3KM 3 log2

min

log(T )

(µ(M ) − µ(M +1))2   T

+ c2KM log(T )

(cid:27)(cid:19)

where c1  c2 and c3 are universal constants.

The ﬁrst  second and third terms respectively correspond to the regret incurred by the exploration  ini-
tialization and communication phases  which dominate the regret due to low probability events of bad

initialization or incorrect estimations. Notice that the minmax regret scales with O(K(cid:112)T log(T )).

Experiments on synthetic data are described in Appendix A.3. They empirically conﬁrm that SIC-
MMAB scales better than MCTopM [8] with the gaps ∆  besides having a smaller minmax regret.

2.3

In contradiction with existing lower bounds?

(cid:17)

(cid:16)

M(cid:80)

(cid:16)(cid:80)

Theorem 1 is in contradiction with the two existing lower bounds [8  20]  however SIC-MMAB
respects the conditions required for both.
It was thought that the decentralized lower bound
  while the centralized lower bound was already known to be
was Ω

log(T )

k>M

µ(M )−µ(k)

log(T )

k>M

µ(M )−µ(k)

[3]. However  it appears that the asymptotic regret of the decentralized
Ω
case is not that much different from the latter  at least if players are synchronized. Indeed  SIC-MMAB
takes advantage of this synchronization to establish communication protocols as players are able to

(cid:17)

6

communicate through collisions. Subsequent papers [10  24] recently improved the communication
protocols of SIC-MMAB to obtain both initialization and communication costs constant in T   conﬁrm-
ing that the lower bound of the centralized case is also tight for the decentralized model considered
so far.
Liu and Zhao [20] proved the lower bound “by considering the best case that they do not collide”.
This is only true if colliding does not provide valuable information and the policies just maximize
the losses at each round  disregarding the information gathered for the future. Our algorithm is built
upon the idea that the value of the information provided by collisions can exceed in the long run the
immediate loss in rewards (which is standard in dynamic programming or reinforcement learning
for instance). The mistake of Besson and Kaufmann [8] is found in the proof of Lemma 12 after the
sentence “We now show that second term in (25) is zero”. The conditional expectation cannot be
put inside/outside of the expectation as written and the considered term  which corresponds to the
difference of information given by collisions for two different distributions  is therefore not zero.
These two lower bounds disregarded the amount of information that can be deduced from collisions 
while SIC-MMAB obviously takes advantage of this information.
Our exploration regret reaches  up to a constant factor  the lower bound of the centralized problem
[3]. Although it is sub-logarithmic in time  the communication cost scales with KM 3 and can thus
be predominant in practice. Indeed for large networks  M 3 can easily be greater than log(T ) and the
communication cost would then prevail over the other terms. This highlights the importance of the
parameter M in multiplayer MAB and future work should focus on the dependency in both M and T
instead of only considering asymptotic results in T .
Synchronization is not a reasonable assumption for practical purposes and it also leads to undesirable
algorithms relying on communication protocols such as SIC-MMAB. We thus claim that this assump-
tion should be removed in the multiplayer MAB and the dynamic model should be considered instead.
However  this problem seems complex to model formally. Indeed  if players stay in the game only for
a very short period  learning is not possible. The difﬁculty to formalize an interesting and nontrivial
dynamic model may explain why most of the literature focused on the static model so far.
3 Without synchronization  the dynamic setting
From now on  we no longer assume that players can communicate using synchronization. In the
previous section  it was crucial that all exploration/communication phases start and end at the same
time. This assumption is clearly unrealistic and should be alleviated  as radios do not start and end
transmitting simultaneously. We also consider the more difﬁcult No Sensing setting in this section.
We assume in the following that players do not leave the game once they have started. Yet  we
mention that our results can also be adapted to the cases when players can leave the game during
speciﬁc intervals or share an internal synchronized clock [26]. If the time is divided in several
intervals  DYN-MMAB can be run independently on each of these intervals as suggested by Rosenski
et al. [26]. In some cases  players will be leaving in the middle of these intervals  leading to a large
regret. But for any other interval  every player stays until its end  thus satisfying Assumption 2.
In this section  Assumption 2 holds. At each stage t = tj + τj  player j does not know t but only tj
(duration since joining). We denote by T j = T − τj the (known) time horizon of player j.
3.1 A logarithmic regret algorithm

As synchronization no longer holds  we propose the DYN-MMAB algorithm  relying on different tools
than SIC-MMAB. The main ideas of DYN-MMAB are given in Section 3.2. Its thorough description as
well as the proof of the regret bound are delayed to Appendix B due to space constraints.
The regret incurred by DYN-MMAB in the dynamic No Sensing model is given by Theorem 2 and
its proof is delayed to Appendix B.2. We also mention that DYN-MMAB leads to a Pareto optimal
conﬁguration in the more general problem where users’ reward distributions differ [17  6  7  9].
Theorem 2. In the dynamic setting  the regret incurred by DYN-MMAB is upper bounded as follows:

(cid:32)

(cid:33)

 

E[RT ] ≤ O

M 2K log(T )

µ(M )

+

M K log(T )

¯∆2

(M )

where M = #M(T ) is the total number of players in the game and ¯∆(M ) = min

i=1 ... M

7

(µ(i) − µ(i+1)).

3.2 A communication-less protocol

DYN-MMAB’s ideas are easy to understand but the upper bound proof is quite technical. This section
gives some intuitions about DYN-MMAB and its performance guarantees stated in Theorem 2.
A player will only follow two different sampling strategies: either she samples uniformly at random
in [K] during the exploration phase; or she exploits an arm and pulls it until the ﬁnal horizon. In the
ﬁrst case  the exploration of the other players is not too disturbed by collisions as they only change
the mean reward of all arms by a common multiplicative term. In the second case  the exploited arm
will appear as sub-optimal to the other players  which is actually convenient for them as this arm is
now exploited.
During the exploration phase  a player will update a set of arms called Occupied ⊂ [K] and an
ordered list of arms called Preferences ∈ [K](cid:63). As soon as an arm is detected as occupied (by
another player)  it is then added to Occupied (which is the empty set at the beginning). If an arm is
discovered to be the best one amongst those that are neither in Occupied nor in Preferences  it
is then added to Preferences (at the last position). An arm is active for player j if it was neither
added to Occupied nor to Preferences by this player yet.
To handle the fact that players can enter the game at anytime  we introduce the quantity γj(t)  the
expected multiplicative factor of the means deﬁned by

t+τj(cid:88)

E(cid:104)

t(cid:48)=1+τj

(1 − 1
K

)mt(cid:48)−1(cid:105)

 

γj(t) =

1
t

where mt is the number of players in their exploration phase at time t. The value of γj(t) is unknown
to the player and random but it only affects the analysis of DYN-MMAB and not how it runs.
The objective of the algorithm is still to form estimates and conﬁdence intervals of the performances
of arms. However  it might happen that the true mean µk does not belong to this conﬁdence interval.
Indeed  this is only true for γj(t)µk  if the arm k is still free (not exploited). This is the ﬁrst point of
Lemma 1 below. Notice that as soon as the conﬁdence interval for the arm i dominates the conﬁdence
interval for the arm k  then it must hold that γj(t)µi ≥ γj(t)µk and thus arm i is better than k.
The second crucial point is to detect when an arm k is exploited by another player. This detection will
happen if a player receives too many 0 rewards successively (so that it is statistically very unlikely
that this arm is not occupied). The number of zero rewards needed for player j to disregard arm k is
denoted by Lj
k  which is sequentially updated during the process (following the rule of Equation (4)
k ≥ 2e log(T j)/µk. As the probability of observing a 0 reward on a free
in Appendix B.1)  so that Lj
arm k is smaller than 1 − µk/e  no matter the current number of players  observing Lj
k successive 0
rewards on an unexploited arm happens with probability smaller than

1

(T j )2 .

The second point of Lemma 1 then states that an exploited arm will either be quickly detected as
occupied after observing Lj
k is small enough) or its average reward will quickly drop
because it now gives zero rewards (and it will be dominated by another arm after a relatively small
number of pulls). The proof of Lemma 1 is delayed to Appendix B.2.
Lemma 1. We denote by ˆrj

k(t) the empirical average reward of arm k for player j at stage t + τj.

k zeros (if Lj

1. For any player j and arm k  if k is still free at stage t + τj  then

P(cid:104)|ˆrj

(cid:114)
k(t) − γj(t)µk| > 2

(cid:105) ≤ 4

6 K log(T j)

t

t

holds as long as k is free.

2. On the other hand  if k is exploited by some player j(cid:48) (cid:54)= j at stage t0 +τj  then  conditionally

(T j)2 .
We then say that the arm k is correctly estimated by player j if |ˆrj
2

(cid:113) 6 K log(T j )
on the correct estimation of all the arms by player j  with probability 1 − O(cid:0) 1
• either k is added to Occupied at a stage at most t0 + τj + O(cid:16) K log(T )
(cid:17)
O(cid:16) K log(T )

T j
by player j 
• or k is dominated by another unoccupied arm i (for player j) at stage at most

k(t) − γj(t)µk| ≤

(cid:1):

(cid:17)

µk

+ τj.

µ2
i

8

It remains to describe how players start exploiting arms. After some time (upper-bounded by
Lemma 10 in Appendix B.2)  an arm which is still free and such that all better arms are occupied
will be detected as the best remaining one. The player will try to occupy it  and this happens as soon
as she gets a positive reward from it: either she succeeds and starts exploiting it  or she fails and
assumes it is occupied by another player (this only takes a few number of steps  see Lemma 1). In the
latter case  she resumes exploring until she detects the next available best arm. With high probability 
the player will necessarily end up exploiting an arm while all the better arms are already exploited by
other players.
4 Conclusion
We have presented algorithms for different multiplayer bandits models. The ﬁrst one illustrates
why the assumption of synchronization between the players is basically equivalent to allowing
communication. Since communication through collisions is possible with other players at a sub-
logarithmic cost  the decentralized multiplayer bandits is almost equivalent to the centralized one for
the considered model. However  this communication cost has a large dependency in the number of
agents in the network. Future work should then focus on considering both the dependency in time
and the number of players as well as developing efﬁcient communication protocols.
Our major claim is that synchronization should not be considered anymore  unless communication is
allowed. We thus introduced a dynamic model and proposed the ﬁrst algorithm with a logarithmic
regret.

Acknowledgments

This work was supported in part by a public grant as part of the Investissement d’avenir project 
reference ANR-11-LABX-0056-LMH  LabEx LMH  in a joint call with Gaspard Monge Program for
optimization  operations research and their interactions with data sciences.

References
[1] R. Agrawal. Sample mean based index policies with o(log n) regret for the multi-armed bandit

problem. Advances in Applied Probability  27(4):1054–1078  1995.

[2] A. Anandkumar  N. Michael  A. K. Tang  and A. Swami. Distributed algorithms for learning
and cognitive medium access with logarithmic regret. IEEE Journal on Selected Areas in
Communications  29(4):731–745  2011.

[3] V. Anantharam  P. Varaiya  and J. Walrand. Asymptotically efﬁcient allocation rules for the
multiarmed bandit problem with multiple plays-part i: I.i.d. rewards. IEEE Transactions on
Automatic Control  32(11):968–976  1987.

[4] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

[5] O. Avner and S. Mannor. Concurrent bandits and cognitive radio networks. In Joint Euro-
pean Conference on Machine Learning and Knowledge Discovery in Databases  pages 66–81.
Springer  2014.

[6] O. Avner and S. Mannor. Learning to coordinate without communication in multi-user multi-

armed bandit problems. arXiv preprint arXiv:1504.08167  2015.

[7] O. Avner and S. Mannor. Multi-user communication networks: A coordinated multi-armed

bandit approach. arXiv preprint arXiv:1808.04875  2018.

[8] L. Besson and E. Kaufmann. Multi-Player Bandits Revisited. In Algorithmic Learning Theory 

Lanzarote  Spain  2018.

[9] I. Bistritz and A. Leshem. Distributed multi-player bandits-a game of thrones approach. In

Advances in Neural Information Processing Systems  pages 7222–7232. 2018.

[10] E. Boursier  E. Kaufmann  A. Mehrabian  and V. Perchet. A practical algorithm for multiplayer

bandits when arm means vary among players. arXiv preprint arXiv:1902.01239  2019.

9

[11] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.

[12] S. Bubeck  T. Wang  and N. Viswanathan. Multiple identiﬁcations in multi-armed bandits. In

International Conference on Machine Learning  pages 258–265  2013.

[13] S. Bubeck  Y. Li  Y. Peres  and M. Sellke. Non-stochastic multi-player multi-armed bandits:
Optimal rate with collision information  sublinear without. arXiv preprint arXiv:1904.12233 
2019.

[14] R. Degenne and V. Perchet. Anytime optimal algorithms in stochastic multi-armed bandits. In

International Conference on Machine Learning  pages 1587–1595  2016.

[15] H. Joshi  R. Kumar  A. Yadav  and S. J. Darak. Distributed algorithm for dynamic spectrum
access in infrastructure-less cognitive radio network. In 2018 IEEE Wireless Communications
and Networking Conference (WCNC)  pages 1–6  2018.

[16] W. Jouini  D. Ernst  C. Moy  and J. Palicot. Multi-armed bandit based policies for cognitive
radio’s decision making issues. In 2009 3rd International Conference on Signals  Circuits and
Systems (SCS)  2009.

[17] D. Kalathil  N. Nayyar  and R. Jain. Decentralized learning for multiplayer multiarmed bandits.

IEEE Transactions on Information Theory  60(4):2331–2345  2014.

[18] J. Komiyama  J. Honda  and H. Nakagawa. Optimal regret analysis of thompson sampling in
stochastic multi-armed bandit problem with multiple plays. In International Conference on
Machine Learning  pages 1152–1161  2015.

[19] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

applied mathematics  6(1):4–22  1985.

[20] K. Liu and Q. Zhao. Distributed learning in multi-armed bandit with multiple players. IEEE

Transactions on Signal Processing  58(11):5667–5681  2010.

[21] G. Lugosi and A. Mehrabian. Multiplayer bandits without observing collision information.

arXiv preprint arXiv:1808.08416  2018.

[22] V. Perchet and P. Rigollet. The multi-armed bandit problem with covariates. The Annals of

Statistics  41(2):693–721  2013.

[23] V. Perchet  P. Rigollet  S. Chassang  and E. Snowberg. Batched bandit problems. In Proceedings

of The 28th Conference on Learning Theory  pages 1456–1456  2015.

[24] A. Proutiere and P. Wang. An optimal algorithm in multiplayer multi-armed bandits  2019.

[25] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  58(5):527–535  1952.

[26] J. Rosenski  O. Shamir  and L. Szlak. Multi-player bandits–a musical chairs approach. In

International Conference on Machine Learning  pages 155–163  2016.

[27] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3-4):285–294  1933.

[28] H. Tibrewal  S. Patchala  M.K. Hanawal  and S.J. Darak. Distributed learning and optimal
assignment in multiplayer heterogeneous networks. In IEEE INFOCOM  pages 1693–1701 
2019.

10

,Mitchell Stern
Noam Shazeer
Jakob Uszkoreit
Etienne Boursier
Vianney Perchet