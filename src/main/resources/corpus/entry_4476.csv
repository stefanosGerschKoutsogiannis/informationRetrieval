2019,Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle,Q-learning with function approximation is one of the most popular methods in reinforcement learning. Though the idea of using function approximation was proposed at least 60 years ago  even in the simplest setup  i.e  approximating Q-functions with linear functions  it is still an open problem how to design a provably efficient algorithm that learns a near-optimal policy. The key challenges are how to efficiently explore the state space and how to decide when to stop exploring in conjunction with the function approximation scheme.

The current paper presents a provably efficient algorithm for Q-learning with linear function approximation. Under certain regularity assumptions  our algorithm  Difference Maximization Q-learning  combined with linear function approximation  returns a near-optimal policy using polynomial number of trajectories. Our algorithm introduces a new notion  the Distribution Shift Error Checking (DSEC) oracle. This oracle tests whether there exists a function in the function class that predicts well on a distribution $\mathcal{D}_1$  but predicts poorly on another distribution $\mathcal{D}_2$  where $\mathcal{D}_1$ and $\mathcal{D}_2$ are distributions over states induced by two different exploration policies. For the linear function class  this oracle is equivalent to solving a top eigenvalue problem. We believe our algorithmic insights  especially the DSEC oracle  are also useful in designing and analyzing reinforcement learning algorithms with general function approximation.,Provably Efﬁcient Q-learning with Function

Approximation via Distribution Shift Error Checking

Oracle

Simon S. Du

Institute for Advanced Study

ssdu@ias.edu

Yuping Luo

Princeton University

yupingl@cs.princeton.edu

Ruosong Wang

Carnegie Mellon University
ruosongw@andrew.cmu.edu

Hanrui Zhang
Duke University

hrzhang@cs.duke.edu

Abstract

Q-learning with function approximation is one of the most popular methods in
reinforcement learning. Though the idea of using function approximation was
proposed at least 60 years ago [27]  even in the simplest setup  i.e  approximating
Q-functions with linear functions  it is still an open problem how to design a
provably efﬁcient algorithm that learns a near-optimal policy. The key challenges
are how to efﬁciently explore the state space and how to decide when to stop
exploring in conjunction with the function approximation scheme.
The current paper presents a provably efﬁcient algorithm for Q-learning with lin-
ear function approximation. Under certain regularity assumptions  our algorithm 
Difference Maximization Q-learning (DMQ)  combined with linear function
approximation  returns a near-optimal policy using polynomial number of trajecto-
ries. Our algorithm introduces a new notion  the Distribution Shift Error Checking
(DSEC) oracle. This oracle tests whether there exists a function in the function
class that predicts well on a distribution D1  but predicts poorly on another distri-
bution D2  where D1 and D2 are distributions over states induced by two different
exploration policies. For the linear function class  this oracle is equivalent to
solving a top eigenvalue problem. We believe our algorithmic insights  especially
the DSEC oracle  are also useful in designing and analyzing reinforcement learning
algorithms with general function approximation.

1

Introduction

Q-learning is a foundational method in reinforcement learning [35] and has been successfully applied
in various domains. Q-learning aims at learning the optimal state-action value function (Q-function).
Once we have learned the Q-function  at every state  we can just greedily choose the action with the
largest Q value  which is guaranteed to be an optimal policy.
Although being a fundamental method  theoretically  we only have a good understanding of Q-
learning in the tabular setting. Strehl et al. [30] and Jin et al. [18] showed with proper exploration
techniques  one can obtain a near-optimal Q-function (and so a near-optimal policy) using polynomial
number of trajectories  in terms of number of states  actions and planning horizon. While these
analyses provide valuable insights  they are of limited practical importance because the number of
states in most applications is enormous. Even worse  it has been proved that in the tabular setting  the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

number of trajectories needed to learn a near-optimal policy scales at least linearly with the number
of states [16].
To resolve this problem  we need reinforcement learning methods that generalize  which  for Q-
learning methods  is to constrain Q-function to a pre-speciﬁed function class  e.g.  linear functions
or neural networks. The basic assumption of this function approximation scheme is that the true
Q-function lies in the function class. A natural problem is:

Can we design provably efﬁcient Q-learning algorithms with function approximation?

Indeed  this is one of the major open problems in reinforcement learning [32]. The idea of using
function approximation was proposed at least 60 years ago [27]  where linear functions are used
to approximate the value functions in playing checkers. However  even in the most basic setting 
Q-learning with linear function approximation  there is no provably efﬁcient algorithm in the general
stochastic setting.
The key challenges are how to 1) efﬁciently explore the state space to learn a good predictor that
generalizes across states and 2) decide when to stop exploring. In order to deal with these challenges 
we need to exploit the fact that the true Q-function belongs to a pre-speciﬁed function class.

Our Contributions Our main theoretical contribution is a provably efﬁcient algorithm for Q-
learning with linear function approximation in the episodic Markov decision process (MDP) setting.
Theorem 1.1 (Main Theorem (informal)). Suppose the Q-function is linear. Then under certain
regularity assumptions  Algorithm 1  Difference Maximization Q-learning (DMQ) returns an
✏-suboptimal policy ⇡ using poly(1/✏) number of trajectories.

Our algorithm works for episodic MDPs with general stochastic transitions. In contrast  previous
algorithms only work for deterministic systems  or rely on strong assumptions  e.g.  a sufﬁciently
good exploration policy is given. See Section 2 for more discussion. Our main assumption is that the
Q-function is linear. Note this is somehow a necessary assumption because otherwise one should not
use linear function approximation in the ﬁrst place.
Before getting into details  we ﬁrst give an overview of our main techniques. As we have discussed 
the main technical challenge is to design an efﬁcient exploration algorithm  and decide when to stop
exploring. Our main algorithmic contribution is to introduce a new notion  the Distribution Shift
Error Checking (DSEC) oracle (cf. Oracle 4.1 and Oracle 4.2). Given two distributions D1 and D2 
this oracle returns True if there exists a function in the pre-speciﬁed function class which predicts
well on D1 but predicts poorly on D2. We will show that this is an extremely useful notion. If the
oracle returns False  then our learned predictor performs well on both distributions. If the oracle
returns True  we know D2 contains information that we can explore  which implies the policy that
generates D2 is a valuable exploration policy. We will discuss the DSEC oracle in more detail in
Section 4.
With this oracle at hand  a natural question is how many times this oracle will return True  as we will
not stop exploring if it always returns True. A technical contribution of this paper is to show for the
linear function class  this oracle will only return True at most polynomial number of times. At a high
level  whenever the oracle returns True  it means we will learn something new from D2. However 
since the complexity of the function class is bounded  we cannot learn new things too many times.
Formally  we use a potential function argument to make this intuition rigorous (cf. Lemma A.5).

1.1 Organization

This paper is organized as follows. In Section 2  we review related work. In Section 3  we introduce
necessary notations  deﬁnitions and assumptions. In Section 4  we describe the DSEC oracle in
detail. In Section 5  we present our general algorithm for Q-learning with function approximation. In
Section 6  we instantiate the general algorithm to the linear function approximation case  and present
our main theorem. We conclude and discuss future works in Section 7. All technical proofs are
deferred to the supplementary material.

2

2 Related Work

Classical theoretical reinforcement learning literature studies asymptotic behavior of concrete algo-
rithms. The most related work is [24]  which studies an online Q-learning algorithm with a ﬁxed
exploration policy. They showed that the estimated Q-function converges to the true Q-function
asymptotically. Recently  Zou et al. [39] derived ﬁnite sample bounds for the same setting. The major
drawback of these works is that they put strong assumptions on the ﬁxed exploration policy. For
example  Zou et al. [39] require that the covariance matrix induced by the exploration policy has
lower bounded least eigenvalue. In general  it is hard to verify whether a policy has such benign
properties.
While it is challenging to design efﬁcient algorithms for Q-learning with function approximation 
in the tabular setting  exploration becomes much easier  as one can ﬁrst estimate the transition
probabilities and then design exploration policies accordingly. There is a substantial body of work on
tabular reinforcement learning [2  16  19  5  21  10]. For Q-learning  Strehl et al. [30] introduced the
delayed Q-learning algorithm which has O(T 4/5) regret bound. A recent work by Jin et al. [18] gave

a UCB-based algorithm which enjoys O⇣pT⌘ regret bound. More recent papers provided reﬁned

analyses that exploit benign properties of the MDP  e.g.  the gap between the optimal action and the
rest [28  38]  which our algorithm also utilizes. However  it is hard to generalize the exploration
techniques in these previous works  since they all rely on the fact that the total number of states is
ﬁnite.
Recently  exploration algorithms are proposed for Q-learning with function approximation. Osband
et al. [25] proposed a Thompson-sampling based method for the linear function class. Later works
further generalized sampling-based algorithms to Q-functions with neural network parameteriza-
tion [6  23  13]. However  none of these works have polynomial sample complexity guarantees. Pazis
and Parr [26] gave a nearest-neighbor-based algorithm for exploration in continuous state space.
However  in general this type of algorithms has exponential dependence on the state dimension.
The seminal work by Wen and Van Roy [36] proposed an algorithm  optimistic constraint propagation
(OCP)  which enjoys polynomial sample complexity bounds for a family of Q-function classes 
including the linear function class as a special case. However  their algorithm can only deal with
deterministic systems  i.e.  both transition dynamics and rewards are deterministic. A line of recent
papers study Q-learning in the general state-action metric space [37  29]. However  due to the
generality  the sample complexity has exponential dependence on the dimension.
Finally  a recent series of work introduced contextual decision processes (CDPs) [22  17  9  31  11]
and developed algorithms with polynomial sample complexity guarantees. Our paper is not directly
comparable with these results  since they can deal with general function classes. In some cases  the
function approximation is even not for the Q-function  but for modeling the map from the observed
state to hidden states [11]. The result in [17] also applies to our setting. However  their bound depends
on both the function class complexity and a quantity called the Bellman rank. Conceptually  since
our bound does not depend on the Bellman rank  our result thus demonstrates that the function class
complexity alone is enough for efﬁcient learning.

3 Preliminaries

Notations We begin by introducing necessary notations. We write [h] to denote the set {1  . . .   h}.
For any ﬁnite set S  we write unif (S) to denote the uniform distribution over S and 4 (S) to denote
the probability simplex. Let k·k2 denote the Euclidean norm of a ﬁnite-dimensional vector in Rd.
For a symmetric matrix A  let kAkop denote its operator norm and i (A) denote its i-th eigenvalue.
Throughout the paper  all sets are multisets  i.e.  a single element can appear multiple times.

Markov Decision Processes (MDPs) Let M = (S A  H  P  R) be an MDP  where S is the
(possibly uncountable) state space  A is the ﬁnite action space with |A| = K  H 2 Z+ is the
planning horizon  P : S⇥A! 4 (S) is the transition function and R : S⇥A! 4 (R) is the
reward distribution.

3

Es⇠D⇡

hh|V ⇡(s)  V ⇤(s)|2i  CEs⇠D⇡

h

A (stochastic) policy ⇡ : S! 4 (A) prescribes a distribution over actions for each state. Without
loss of generality  we assume a ﬁxed start state s1.1 The policy ⇡ induces a random trajectory
s1  a1  r1  s2  a2  r2  . . .   sH  aH  rH where r1 ⇠ R(s1  a1)  s2 ⇠ P (s1  a1)  a2 ⇠ ⇡(s2)  etc. For a
given policy ⇡  we use D⇡
h to denote the distribution over Sh induced by executing policy ⇡.
To streamline our analysis  we denote Sh ✓S to be the set of states at level h. Similar to previous
theoretical reinforcement learning results  we also assume rh  0 for all h 2 [H] andPH
h=1 rh 
1 [17]. Our goal is to ﬁnd a policy ⇡ that maximizes the expected reward EhPH
h=1 rh | ⇡i. We use
⇡⇤ to denote the optimal policy.
Given a policy ⇡  a level h 2 [H] and a state-action pair (s  a) 2S h ⇥A   the Q-function is
deﬁned as Q⇡(s  a) = EhPH
h0=h rh0 | sh = s  ah = a  ⇡i. It will also be useful to deﬁne the value
h0=h rh0 | sh = s  ⇡i. For simplicity  we denote
function of a given state s 2S h as V ⇡(s) = EhPH

Q⇤(s  a) = Q⇡⇤(s  a) and V ⇤ = V ⇡⇤(s). Recall that if we know Q⇤  we can just choose the action
greedily: ⇡⇤(s) = argmaxa2AQ⇤(s  a). In this paper  we make the following assumption about the
variation of the suboptimality of policies [12].
Assumption 3.1 (Bounded Coefﬁcient of Variation of Policy Sub-optimality). There exists a constant
1  C < 1  such that for any ﬁxed level h 2 [H] and deterministic policy ⇡ 
[|V ⇡(s)  V ⇤(s)|]2 .

Intuitively  this assumption says the variation due to the randomness over states is not too large
comparing to the mean. For example  if the transition is deterministic  then this assumption holds
with C = 1.
Our paper also relies on the following ﬁne-grained characterization of the MDP.
Deﬁnition 3.1 (Suboptimality Gaps). Given s 2S and a 2A   the gap is deﬁned as gap(s  a) =
V ⇤(s)  Q⇤(s  a). The minimum gap is deﬁned as    mins2S a2A {gap(s  a) : gap(s  a) > 0}.
This notion has been extensively studied in the bandit literature to obtain ﬁne-grained bounds [4].
Recently  Simchowitz et al. [28] derived regret bounds in tabular MDPs based on this notion. In this
paper we assume > 0  and the sample complexity of our algorithm depends polynomially on 1/.
Notice that assuming  is strictly positive is not a restrictive assumption for the ﬁnite action setting
considered in this paper. First  in the contextual linear bandit literature  this assumption is widely
discussed. See  e.g.  [1  8]. The notion  context  in the bandit literature is essentially (s) in our
paper and the number of contexts can also be inﬁnite. Second  there are many natural environments
in RL which satisfy this assumption. For example  in many environments  states can be classiﬁed
as good states and bad states. In these environments  an agent can obtain a reward only if it is in
a good state. There are also two kinds of actions: good actions and bad actions. If the agent is in
a good state and chooses a good action  the agent will transit to a good state. If the agent chooses
a bad action  the agent will transit to a bad state. If the agent is in a bad state  whatever action the
agent chooses  the agent will transit to a bad state. Note that for this kind of environments  there is
a strictly positive gap between good actions and bad actions when the agent is in good states and
there is no difference between good actions and bad actions when the agent is in bad states. In this
case   is strictly positive  since by Deﬁnition 3.1  we take the minimum over all state-action pairs
with strictly positive gap. These environments are natural generalizations of the combination lock
environment [20]. Some Atari games  e.g. Freeway  have a similar ﬂavor as these environments.

Function Approximation When the state space is large  we need structures on the state space
so that reinforcement learning methods can generalize. We constrain the optimal Q-function to a
pre-speciﬁed function class Q [7]  e.g.  the class of linear functions. In this paper we associate each
h 2 [H] and a 2A with a Q-function f a
Assumption 3.2. For every (h  a) 2 [H] ⇥A   its associated optimal Q-function is in Q.

h 2Q . We make the following assumption.

1Some papers assume the starting state is sampled from a distribution P1. Note this is equivalent to assuming
a ﬁxed state s1  by setting P (s1  a) = P1 for all a 2A and now our s2 is equivalent to the starting state in their
assumption.

4

This is a widely used assumption in the theoretical reinforcement learning literature [17]. Note
that without this assumption  we cannot hope to obtain optimal policy using functions in Q as the
Q-function.
The focus of this paper is about linear function class which is one of the most popular function
classes used in practice. This function class depends on a feature extractor  : S! Rd which
can be a hand-crafted feature extractor or a pre-trained neural network that transforms a state to a
d-dimension embedding. For sh 2S h and a 2A   our estimated optimal Q-function admits the form
h (s) = (s)> ˆ✓a
h 2 Rd only depends on the level h 2 [H] and a 2A . Therefore  we only
f a
need to learn K · H d-dimension vectors (linear coefﬁcients)  since by Assumption 3.2  for each
h 2 [H] and a 2A   there exists ✓a
The aim of this paper is to obtain polynomial sample complexity bounds. To this end  we also need
some regularity conditions.
Assumption 3.3. For all s 2S   its feature is bounded k(s)k2  1. For all h 2 [H]  a 2A   the
true linear predictor is bounded k✓a
4 Distribution Shift Error Checking Oracle

h 2 Rd such that for all sh 2S h  Q⇤(sh  a) = (sh)>✓a
h.

hk2  1.

h where ˆ✓a

As we have discussed in Section 1  in reinforcement learning  we often need to know whether a
predictor learned from samples generated from one distribution D1 can predict well on another
distribution D2. This is related to off-policy learning for which one often needs to bound the
probability density ratio between D1 and D2 on all state-action pair. When function approximation
scheme is used  we naturally arrive at the following oracle.
Oracle 4.1 (Distribution Shift Error Checking Oracle (D1 D2 ✏ 1 ✏ 2  ⇤)). For two given distributions
D1 D2 over S  two real numbers ✏1 and ✏2  and a regularizer ⇤: Q⇥Q! R  deﬁne

v = max
f1 f22Q

Es⇠D2h(f1(s)  f2(s))2i

s.t. Es⇠D1h(f1(s)  f2(s))2i +⇤( f1  f2)  ✏1.

The oracle returns True if v  ✏2  and False otherwise.
To motivate this oracle  let f2 be the optimal Q-function and f1 is a predictor we learned using
samples generated from distribution D1. In this scenario  we know f1 has a small expected error ✏1
on distribution D1. Note since we maximize over the entire function class Q  v is an upper bound on
the expected error of f1 on distribution D2. If v is large enough  say larger than ✏2  then it could be
the case that we cannot predict well on distribution D2. On the other hand  if v is small  we are certain
that f1 has small error on D2. Here we add a regularization term ⇤(f1  f2) to prevent pathological
cases. The concrete choice of ⇤ will be given later.
In practice  it is impossible to get access to the underlying distributions D1 and D2. Thus  we use
samples generated from these two distributions instead.
Oracle 4.2 (Sample-based Distribution Shift Error Checking Oracle (D1  D2 ✏ 1 ✏ 2  ⇤)). For two
set of states D1  D2 ✓S   two real numbers ✏1 and ✏2  and a regularizer ⇤: Q⇥Q! R  deﬁne

v = max
f1 f22Q
1

s.t.

1

|D2| Xti2D2h(f1(ti)  f2(ti))2i

|D1| Xsi2D1h(f1(si)  f2(si))2i +⇤( f1  f2)  ✏1.

The oracle returns True if v  ✏2 and False otherwise. If D1 = ;  the oracle simply returns True.
An interesting property of Oracle 4.2 is that it only depends on the states and does not rely on the
reward values.

5 Difference Maximization Q-learning

Now we describe our algorithm. We maintain three sets of global variables.

5

Algorithm 1 Difference Maximization Q-learning (DMQ)

Output: A near-optimal policy ⇡.

1: for h = H  H  1  . . .   1 do
2:
3: Return ˆ⇡  the greedy policy with respect to {f a

Run Algorithm 2 on input h.

h}a2A h2[H].

i=1 is a set of states in Sh.

1. {f a
h}a2A h2[H]. These are our estimated Q-functions for all actions a 2A and all levels h 2 [H].
2. {⇧h}h2[H]. For each level h 2 [H]  ⇧h is a set of exploration policies for level h  which we use
to collect data.
3. {Dh}h2[H]. For each h 2 [H]  Dh = {sh i}N
We initialize these global variables in the following manner. For {f a
h}a2A h2[H]  we initialize them
arbitrarily. For each h 2 [H]  we initialize ⇧h to be a single purely random exploration policy  i.e. 
⇧h = {⇡}  where ⇡(s) = unif (A) for all s 2S . We initialize {Dh}h2[H] to be empty sets.
Algorithm 1 uses Algorithm 2 to learn predictors for each level h 2 [H]. Algorithm 2 takes h 2 [H]
as input  tries to learn predictors {f a
at level h. Algorithm 3 takes h 2 [H] and a 2A as
inputs  and checks whether the predictors learned for later levels h0 > h are accurate enough under
the current policy.
Now we explain Algorithm 2 and Algorithm 3 in more detail. Algorithm 2 iterates all actions  and for
each action a 2A   it uses Algorithm 3 to check whether we can learn the Q-function that corresponds
to a well. After executing Algorithm 3  we are certain that we can learn f a
h well (we will explain this
in the next paragraph)  and thus construct a set of new policies ⇧a
h}⇡h2⇧h  in the following
h = {⇡a
way. For each policy ⇡h 2 ⇧h  we deﬁne ⇡a
⇡h(sh0)
a
argmaxa02Af a0

if h0 < h
if h0 = h
if h0 > h

h}a2A

h0 (sh0)

h as

(1)

.

This policy uses ⇡h as the roll-in policy till level h  chooses action a at level h and uses greedy policy
with respect to {f a
  the current estimates of Q-functions at level h + 1  . . .   H as the
roll-out policy. In each iteration  we sample one policy ⇡ uniformly at random from ⇧a
h  and use it to
collect (s  y)  where s 2S h and y 2 R is the on-the-go reward. In total we collect a dataset Da
h with
size N · |⇧h|  and we use regression to learn a predictor on these data. Formally  we calculate

h0}h0>h a2A

⇡a

h(sh0) =8<:

f a

h = argminf2Q24

1

N · |⇧h| X(s y)2Da

h

(f (s)  y)2 + (f )35 .

(2)

Here  (f ) represents a regularization term on f. Finally  we update Dh by using each ⇡h 2 ⇧h to
collect N states in Sh.
h deﬁned in (1) to collect N trajectories.
Now we explain Algorithm 3. For each ⇡h 2 ⇧h  we use ⇡a
For each h0 = h + 1  . . .   H  we set eD⇡a
i=1  where sh0 i is the state at level h0 in the
i-th trajectory. Next  for each h0 = H  . . .   h + 1  we invoke Oracle 4.2 on input Dh0 and eD⇡a
h h0.
Note that Dh0 was collected when we execute Algorithm 2 to learn the predictors at level h0 . The
oracle will return whether our current predictors at level h0 can still predict well on the distribution
that generates eD⇡a
h to our policy set ⇧h0  and we execute Algorithm 2 to
learn the predictors at level h0 once again. Note it is crucial to iterate h0 from H to h + 1  so that we
will always make sure the predictors at later levels are correct.

h h0. If not  then we add ⇡a

h h0 = {sh0 i}N

6 Provably Efﬁcient Q-learning with Linear Function Approximation

Now we instantiate our algorithm to the linear function class. For the regression problem in (2)  we
set (✓) = ridge k✓k2
2. The concrete choice of the parameter ridge will be given later. In this case 

6

Algorithm 2

Input: h 2 [H]  a target level.

1: for a 2A do
2:
3:
4:
5:
6:
7:
8:
9:
10: Set Dh = ;.
11: for ⇡h 2 ⇧h do
12:
13:

h = ;.

h according to (1).

Execute Algorithm 3 on input (h  a).
Initialize Da
Construct a policy set ⇧a
h| do
for i = 1  . . .   N · |⇧a
Sample ⇡ ⇠ unif (⇧a
h).
Use ⇡ to collect (si  yi)  where si 2S h and yi is the on-the-go reward.
Add (si  yi) into Da
h.
(f (s)  y)2 + (f )i.

1

Learn a predictor f a

h = argminf2Qh
Use ⇡h to collect a set of states {s⇡h i}N
Add all states {s⇡h i}N

i=1 into Dh.

h

N·|⇧h|P(s y)2Da
i=1  where s⇡h i 2S h.

Algorithm 3

h deﬁned in (1).

Input: target level h 2 [H] and an action a 2A .

1: for ⇡h 2 ⇧h do
Collect N trajectories using policy ⇡a
2:
for h0 = H  H  1  . . .   h + 1 do
3:
Let eD⇡a
4:
Invoke Oracle 4.2 on input⇣Dh0 eD⇡a

if Oracle 4.2 returns True then

h h0 = {sh0 i}N

h}.
⇧h0 =⇧ h0 [{ ⇡a
Execute Algorithm 2 on input h0.

5:
6:
7:
8:

h h0 

✏s
|⇧h0|

 ✏ t  ⇤⇧h0⌘.

i=1 be the states at level h0 on the N trajectories collected using ⇡a
h.

the regression program represents the ridge regression estimator

ˆ✓a

h =0@ 1

|Da
h (sh) = (sh)> ˆ✓a

h| X(s y)2Da
h for sh 2S h.

h

(s)(s)> + ridge · I1A

10@ 1

|Da

h| X(s y)2Da

h

y · (s)1A  

and f a
For Oracle 4.2  we choose ⇤⇧h0 (✓1 ✓ 2) = r/|⇧h0| ·k ✓1  ✓2k2
2. The concrete choice of the
parameter r will be given later. Since Q is the linear function class  the optimization problem is
equivalent to the following program

max
✓1 ✓2

1

|D2| Xti2D2(✓1  ✓2)>(ti)2
|D1| Xsi2D1(✓1  ✓2)>(si)2

1

2  ✏1.

s.t.

+ r/|⇧h0| ·k ✓1  ✓2k2
|D2|Pti2D2
|D1|Psi2D1
(✓1  ✓2)  then the optimization problem can be further reduced to
s.t. ˜✓2  1 

(si)(si)> + r/|⇧h0| · I  M2 = 1
˜✓>⇣✏1M 1

1 ⌘ ˜✓

1 M2M 1

max

˜✓

2

2

We let M1 = 1
˜✓   1p✏1

M 1/2

1

which is equivalent to compute the top eigenvalue of ✏1M 1
. Therefore  the regression
problem in (2) and Oracle 4.2 can be efﬁciently implemented. Our main result is the following
theorem.

1 M2M 1

1

2

2

(ti)(ti)>  and let

7

Theorem 6.1 (Provably Efﬁcient Q-Learning with Linear Function Approximation). Let ✏ 
poly(  1/C  1/d  1/H  1/K) be the target accuracy parameter. Under Assumption 3.1  3.2 and 3.3 
then using at most poly(1/✏) trajectories  with high probability  Algorithm 1 returns a policy ˆ⇡ that
satisﬁes V ˆ⇡(s1)  V ⇤(s1)  ✏.
This theorem demonstrates that if the true Q-function is linear  then it is actually possible to learn
a near-optimal policy with polynomial number of samples. We refer readers to the Proof of Theo-
rem 6.1 for the speciﬁc values of ✏t ✏ s ✏ N   ridge  r  N. Furthermore  our algorithm also runs in
polynomial time. Therefore  this is the ﬁrst provably efﬁcient algorithm for Q-learning with function
approximation in the stochastic setting.
Now we brieﬂy sketch the proof of Theorem 6.1. The full proof is deferred to Section A. Our
proof follows directly from the design of our algorithm. First  through classical analysis of linear
regression  we know the learned predictor ˆ✓a
h can predict well on the distribution induced by ⇡a
h.
Second  Oracle 4.2 guarantees that if it returns False  then the learned predictors at level h0 can
h. Therefore  the labels we used to
predict well on the distribution over Sh induced by the policy ⇡a
learn ✓h
a well. Now the trickiest part of the proof
is to show Oracle 4.2 returns True at most polynomial number of times. To establish this  for each
h 2 [H]  we construct a potential function in terms of covariance matrices induced by the policies
in ⇧h. We show whenever a new policy is added to ⇧h  this potential function must be increased
by a multiplicative factor. We further show this potential function is at always polynomially upper
bounded by the size of the policy set. Therefore  we can conclude the size of ⇧h is polynomially
upper bounded. See Lemma A.5 for details.

a have only small bias  and thus  we can learn ✓h

7 Discussion

By giving a provably efﬁcient algorithm for Q-learning with function approximation  this paper paves
the way for rigorous studies of modern model-free reinforcement learning methods with function
approximation. Now we list some future directions.

Regret Bound This paper presents a PAC bound but no regret bound. Note that we assume the
gap between the on-the-go reward of the best action and the rest is strictly positive. In the tabular
setting  previous work showed that under this assumption  one can obtain log T regret bound [28  38].
We believe it would be a very strong result to prove (or disprove) log T regret bound in the setting
considered in this paper.

Q-learning with General Function Class While the main theorem in this paper is about the linear
function class  the DSEC oracle and the general algorithmic framework applies to any function
classes. From an information-theoretic point of view  given Oracle 4.2  can we use it to design
algorithms for general function class with polynomial sample complexity guarantees? For example 
if the Q-function class has a bounded VC-dimension  can Algorithm 1 give a polynomial sample
complexity guarantee? We believe a generalization of Lemma A.5 is required to resolve this question.
Another interesting problem is to generalize our algorithm to the case that the Q-function is not
exactly linear but can only be approximated by a linear function.
From the computational point of view  can we characterize the function classes for which we have an
efﬁcient solver for Oracle 4.2? For those we do not have such exact solvers  can we develop a relaxed
version of Oracle 4.2 which  possibly sacriﬁcing the sample efﬁciency  makes the optimization
problem tractable. This idea was used in the sparse learning literature [34]. Another interesting
problem is to improve the computational efﬁciency of our algorithm to make it fast enough to be used
in practice.

Toward a Rigorous Theory for DQN Deep Q-learning (DQN) is one of the most popular model-
free methods in modern reinforcement learning. Recent studies established that over-parameterized
neural networks are equivalent to kernel predictors [15  3] with multi-layer kernel functions. Since
kernel predictors can be viewed as linear predictors in inﬁnite dimensional feature spaces  can
we adapt our algorithm to over-parameterized neural networks and multi-layer kernels  and prove
polynomial sample complexity guarantees when  e.g.  the true Q-function has a small reproducing
Hilbert space norm?

8

Acknowledgements

The authors would like to thank Nan Jiang  Akshay Krishnamurthy  Wen Sun  Yining Wang and Lin
F. Yang for useful discussions. The paper was initiated while S. S. Du was an intern at MSR NYC
and a Ph.D. student at Carnegie Mellon University. Part of this work was done while S. S. Du and R.
Wang were visiting Simons Institute.

References
[1] Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári.

Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems  pages 2312–2320 
2011.

[2] Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret

bounds. In NIPS  2017.

[3] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 
2019.

[4] Jean-Yves Audibert and Sébastien Bubeck. Best arm identiﬁcation in multi-armed bandits. In

COLT-23th Conference on learning theory-2010  pages 13–p  2010.

[5] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for

reinforcement learning. arXiv preprint arXiv:1703.05449  2017.

[6] K. Azizzadenesheli  E. Brunskill  and A. Anandkumar. Efﬁcient exploration through bayesian
deep Q-networks. In 2018 Information Theory and Applications Workshop (ITA)  pages 1–9 
Feb 2018.

[7] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming  volume 5. Athena

Scientiﬁc Belmont  MA  1996.

[8] Varsha Dani  Thomas P Hayes  and Sham M Kakade. Stochastic linear optimization under

bandit feedback. In Conference on Learning Theory  2008.

[9] Christoph Dann  Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and
Robert E Schapire. On polynomial time PAC reinforcement learning with rich observations.
arXiv preprint arXiv:1803.00606  2018.

[10] Christoph Dann  Tor Lattimore  and Emma Brunskill. Unifying PAC and regret: Uniform PAC
bounds for episodic reinforcement learning. In Proceedings of the 31st International Conference
on Neural Information Processing Systems  NIPS’17  pages 5717–5727  USA  2017. Curran
Associates Inc.

[11] Simon S Du  Akshay Krishnamurthy  Nan Jiang  Alekh Agarwal  Miroslav Dudík  and John
Langford. Provably efﬁcient RL with rich observations via latent state decoding. arXiv preprint
arXiv:1901.09018  2019.

[12] Brian Everitt. The Cambridge dictionary of statistics. Cambridge University Press  Cambridge 

UK; New York  2002.

[13] Meire Fortunato  Mohammad Gheshlaghi Azar  Bilal Piot  Jacob Menick  Matteo Hessel  Ian
Osband  Alex Graves  Volodymyr Mnih  Remi Munos  Demis Hassabis  Olivier Pietquin 
Charles Blundell  and Shane Legg. Noisy networks for exploration. In International Conference
on Learning Representations  2018.

[14] Daniel Hsu  Sham M Kakade  and Tong Zhang. Random design analysis of ridge regression. In

Conference on learning theory  pages 9–1  2012.

[15] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8571–8580  2018.

9

[16] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11(Apr):1563–1600  2010.

[17] Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and Robert E Schapire.
Contextual decision processes with low bellman rank are PAC-learnable. In Proceedings of the
34th International Conference on Machine Learning-Volume 70  pages 1704–1713. JMLR. org 
2017.

[18] Chi Jin  Zeyuan Allen-Zhu  Sebastien Bubeck  and Michael I Jordan. Is Q-learning provably

efﬁcient? In Advances in Neural Information Processing Systems  pages 4863–4873  2018.

[19] Sham Kakade  Mengdi Wang  and Lin Yang. Variance reduction methods for sublinear rein-

forcement learning. 02 2018.

[20] Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD

thesis  University of London London  England  2003.

[21] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Mach. Learn.  49(2-3):209–232  November 2002.

[22] Akshay Krishnamurthy  Alekh Agarwal  and John Langford. PAC reinforcement learning with
rich observations. In Advances in Neural Information Processing Systems  pages 1840–1848 
2016.

[23] Zachary Chase Lipton  Xiujun Li  Jianfeng Gao  Lihong Li  Faisal Ahmed  and Li Deng.
BBQ-networks: Efﬁcient exploration in deep reinforcement learning for task-oriented dialogue
systems. In AAAI  2018.

[24] Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In
International Conference on Computational Learning Theory  pages 308–322. Springer  2007.

[25] Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via random-
ized value functions. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning - Volume 48  ICML’16  pages 2377–2386. JMLR.org  2016.

[26] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space markov decision
processes. In Proceedings of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence 
AAAI’13  pages 774–781. AAAI Press  2013.

[27] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of

Research and Development  3(3):210–229  July 1959.

[28] Max Simchowitz and Kevin Jamieson. Non-asymptotic gap-dependent regret bounds for tabular

MDPs. 05 2019.

[29] Zhao Song and Wen Sun. Efﬁcient model-free reinforcement learning in metric spaces. arXiv

preprint arXiv:1905.00475  2019.

[30] Alexander L Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L Littman. PAC
model-free reinforcement learning. In Proceedings of the 23rd international conference on
Machine learning  pages 881–888. ACM  2006.

[31] Wen Sun  Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  and John Langford. Model-based
reinforcement learning in contextual decision processes. arXiv preprint arXiv:1811.08540 
2018.

[32] Richard S. Sutton. Open theoretical questions in reinforcement learning. In EuroCOLT  1999.

[33] Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and

Trends R in Machine Learning  8(1-2):1–230  2015.

[34] Vincent Q Vu  Juhee Cho  Jing Lei  and Karl Rohe. Fantope projection and selection: A
near-optimal convex relaxation of sparse PCA. In Advances in neural information processing
systems  pages 2670–2678  2013.

10

[35] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292 

1992.

[36] Zheng Wen and Benjamin Van Roy. Efﬁcient exploration and value function generalization in
deterministic systems. In Advances in Neural Information Processing Systems  pages 3021–
3029  2013.

[37] Lin F Yang  Chengzhuo Ni  and Mengdi Wang. Learning to control in metric space with optimal

regret. arXiv preprint arXiv:1905.01576  2019.

[38] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in rein-
forcement learning without domain knowledge using value function bounds. arXiv preprint
arXiv:1901.00210  2019.

[39] Shaofeng Zou  Tengyu Xu  and Yingbin Liang. Finite-sample analysis for SARSA and Q-

learning with linear function approximation. arXiv preprint arXiv:1902.02234  2019.

11

,Simon Du
Yuping Luo
Ruosong Wang
Hanrui Zhang