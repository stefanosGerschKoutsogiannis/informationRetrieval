2018,Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation,We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting  where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of $O(1/\varepsilon)$ to attain an error of $\varepsilon$ under a variant of restricted eigenvalue condition  and the rate has better dependency on the problem dimension than existing methods. Particularly  if the smallest magnitude of the non-zero components of the optimal solution is not too small  the rate of our proposed {\it Hybrid} algorithm can be boosted to near the minimax optimal sample complexity of {\it full information} algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods.,Sample Efﬁcient Stochastic Gradient Iterative Hard
Thresholding Method for Stochastic Sparse Linear

Regression with Limited Attribute Observation

Tomoya Murata

NTT DATA Mathematical Systems Inc.   Tokyo  Japan

murata@msi.co.jp

Department of Mathematical Informatics 

Graduate School of Information Science and Technology 

The University of Tokyo  Tokyo  Japan

Center for Advanced Intelligence Project  RIKEN  Tokyo  Japan

Taiji Suzuki

taiji@mist.i.u-tokyo.ac.jp

Abstract

We develop new stochastic gradient methods for efﬁciently solving sparse linear
regression in a partial attribute observation setting  where learners are only allowed
to observe a ﬁxed number of actively chosen attributes per example at training
and prediction times. It is shown that the methods achieve essentially a sample
complexity of O(1/ε) to attain an error of ε under a variant of restricted eigenvalue
condition  and the rate has better dependency on the problem dimension than exist-
ing methods. Particularly  if the smallest magnitude of the non-zero components of
the optimal solution is not too small  the rate of our proposed Hybrid algorithm
can be boosted to near the minimax optimal sample complexity of full information
algorithms. The core ideas are (i) efﬁcient construction of an unbiased gradient
estimator by the iterative usage of the hard thresholding operator for conﬁguring
an exploration algorithm; and (ii) an adaptive combination of the exploration and
an exploitation algorithms for quickly identifying the support of the optimum and
efﬁciently searching the optimal parameter in its support. Experimental results are
presented to validate our theoretical ﬁndings and the superiority of our proposed
methods.

1

Introduction

In real-world sequential prediction scenarios  the features (or attributes) of examples are typically high-
dimensional and construction of the all features for each example may be expensive or impossible.
One of the example of these scenarios arises in the context of medical diagnosis of a disease  where
each attribute is the result of a medical test on a patient [4]. In this scenarios  observations of the all
features for each patient may be impossible because it is undesirable to conduct the all medical tests
on each patient due to its physical and mental burden.
In limited attribute observation settings [1  4]   learners are only allowed to observe a given number
of attributes per example at training time. Hence learners need to update their predictor based on the
actively chosen attributes which possibly differ from example to example.
Several methods have been proposed to deal with this setting in linear regression problems. Cesa-
Bianchi et al. [4] have proposed a generalized stochastic gradient descent algorithm [16  5  14] based

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

on the ideas of picking observed attributes randomly and constructing a noisy version of all attributes
using them. Hazan and Koren [7] have proposed an algorithm combining a stochastic variant of EG
algorithm [11] with the idea in [4]  which improves the dependency of the problem dimension of the
convergence rate proven in [4].
In these work  limited attribute observation settings only at training time have been considered.
However  it is natural to assume that the observable number of attributes at prediction time is same as
the one at training time. This assumption naturally requires the sparsity of output predictors.
Despite the importance of the requirement of the sparsity of predictors  a hardness result in this
setting is known. Foster et al. [6] have considered online (agnostic) sparse linear regression in the
limited attribute observation setting. They have shown that no algorithm running in polynomial time
per example can achieve any sub-linear regret unless NP ⊂ BPP. Also it has been shown that this
hardness result holds in stochastic i.i.d. (non-agnostic) settings in [8]. These hardness results suggest
that some additional assumptions are needed.
More recently  Kale and Karnin [10] have proposed an algorithm based on Dantzig Selector [3] 
which run in polynomial time per example and achieve sub-linear regrets under restricted isometry
condition [2]  which is well-known in sparse recovery literature. Particularly in non-agnostic settings 

the proposed algorithm achieves a sample complexity of (cid:101)O(1/ε)1  but the rate has bad dependency on

the problem dimension. Additionally  this algorithm requires large memory cost since it needs to store
the all observed samples due to the applications of Dantzig Selector to the updated design matrices.
Independently  Ito et al. [8] have also proposed three efﬁcient runtime algorithms based on regularized
dual averaging [15] with their proposed exploration-exploitation strategies in non-agnostic settings
under linear independence of features or compatibility[2]. The one of the three algorithms achieves
a sample complexity of O(1/ε2) under linear independence of features  which is worse than the
one in [10]  but has better dependency on the problem dimension. The other two algorithms also
achieve a sample complexity of O(1/ε2)  but the additional term independent to ε has unacceptable
dependency on the problem dimension.
As mentioned above  there exist several efﬁcient runtime algorithms which solve sparse linear
regression problem with limited attribute observations under suitable conditions. However   the
convergence rates of these algorithms have bad dependency on the problem dimension or on desired
accuracy. Whether more efﬁcient algorithms exist is a quite important and interesting question.

Main contribution In this paper  we focus on stochastic i.i.d. (non-agnostic) sparse linear regres-
sion in the limited attribute observation setting and propose new sample efﬁcient algorithms in this
setting. The main feature of proposed algorithms is summarized as follows:

Our algorithms achieve a sample complexity of (cid:101)O(1/ε) with much better dependency on the

problem dimension than the ones in existing work. Particularly  if the smallest magnitude of the
non-zero components of the optimal solution is not too small  the rate can be boosted to near the
minimax optimal sample complexity of full information algorithms.

Additionally  our algorithms also possess run-time efﬁciency and memory efﬁciency  since the average
run-time cost per example and the memory cost of the proposed algorithms are in order of the number
of observed attributes per example and of the problem dimension respectively  that are better or
comparable to the ones of existing methods.
We list the comparisons of our methods with several preceding methods in our setting in Table 1.

2 Notation and Problem Setting

In this section  we formally describe the problem to be considered in this paper and the assumptions
for our theory.

1(cid:101)O hides extra log-factors.

†Note that the necessary number of observed attributes per example at prediction time is s∗  that is nearly

same as the other algorithms in table 1.

2

Table 1: Comparisons of our methods with existing methods in our problem setting. Sample
complexity means the necessary number of samples to attain an error ε. "# of observed attrs per
ex." indicates the necessary number of observed attributes per example at training time which the
algorithm requires at least. s(cid:48) is the number of observed attributes per example  s∗ is the size of the
support of the optimal solution  d is the problem dimension  ε is the desired accuracy and r2
min is the
smallest magnitude of the non-zero components of the optimal solution. We regard the smoothness
and strong convexity parameters of the objectives derived from the additional assumptions and the

boundedness parameter of the input data distribution as constants. (cid:101)O hides extra log-factors for

simplifying the notation.

(cid:101)O

Sample complexity

ε

s(cid:48)

s(cid:48)

O

(cid:17)

d2 σ2
ε2

(cid:16) ds2∗
(cid:0)σ + d
(cid:1)2 1
(cid:17)
(cid:16)
(cid:17)
(cid:16) d16
(cid:16) d16
(cid:17)
(cid:16) ds2∗
(cid:17)
(cid:17) σ2
(cid:16) s∗

s(cid:48)16 + d σ2
s(cid:48)16 + d σ2
s(cid:48) σ2
s(cid:48) + ds∗
∧ ds∗
s(cid:48)

O

O

ε2

ε2

ε

ε

s(cid:48) +

r2
min

(cid:101)O
(cid:16) ds2∗

# of observed
attrs per ex.

†

1

s∗ + 2

s∗

s∗

O(s∗)

O(s∗)

(cid:17)

Additional
assumptions

restricted isometry

condition

linear independence

of features

linear independence

of features
compatibility

Objective type

Regret

Regret

Regret

Regret

restricted smoothness &
restricted strong convexity
restricted smoothness &
restricted strong convexity

Expected risk

Expected risk

Dantzig [10]

RDA1 [8]

RDA2 [8]

RDA3 [8]

Exploration

Hybrid

(cid:101)O

2.1 Notation

We use the following notation in this paper.

• (cid:107) · (cid:107) denotes the Euclidean L2 norm (cid:107) · (cid:107)2: (cid:107)x(cid:107) = (cid:107)x(cid:107)2 =(cid:112)(cid:80)

i .
i x2

abbreviated as [n].

• For natural number m  n  [m  n] denotes the set {m  m + 1  . . .   n}. If m = 1  [1  n] is
• Hs denotes the projection onto s-sparse vectors  i.e.  Hs(θ(cid:48)) = argminθ∈Rd (cid:107)θ(cid:107)0≤s(cid:107)θ − θ(cid:48)(cid:107)
• For x ∈ Rd  x|j denotes the j-th element of x. For S ⊂ [d]  we use x|S ∈ R|S| to denote

for s ∈ N  where (cid:107)θ(cid:107)0 denotes the number of non-zero elements of θ.
the restriction of x to S: (x|S)|j = xj for j ∈ S.

2.2 Problem deﬁnition

In this paper  we consider the following sparse linear regression model:
∗ x + ξ  where (cid:107)θ∗(cid:107)0 = s∗  x ∼ DX  

(1)
where ξ is a mean zero sub-Gaussian random variable with parameter σ2  which is independent to
x ∼ DX. We denote D as the joint distribution of x and y.
For ﬁnding true parameter θ∗ of model (1)  we focus on the following optimization problem:

y = θ(cid:62)

(2)
where (cid:96)y(a) is the standard squared loss (a − y)2 and s ≥ s∗ is some integer. We can easily see that
true parameter θ∗ is an optimal solution of the problem (2).

(cid:107)θ(cid:107)0≤s θ∈Rd

min

{L(θ) def= E(x y)∼D[(cid:96)y(θ(cid:62)x)]} 

Limited attribute observation We assume that only a small subset of the attributes which we
actively choose per example rather than all attributes can be observed at both training and prediction
time. In this paper  we aim to construct algorithms which solve problem (2) with only observing
s(cid:48)(≥ s ≥ s∗) ∈ [d] attributes per example. Typically  the situation s(cid:48) (cid:28) d is considered.

2.3 Assumptions

We make the following assumptions for our analysis.

3

Assumption 1 (Boundedness of data). For x ∼ DX  (cid:107)x(cid:107)∞ ≤ R∞ with probability one.
Assumption 2 (Restricted smoothness of L). Objective function L satisﬁes the following restricted
smoothness condition:
∀s ∈ [d] ∃Ls > 0 ∀θ1  θ2 ∈ Rd : (cid:107)θ1(cid:107)0 (cid:107)θ2(cid:107)0 ≤ s ⇒ L(θ1) ≤ L(θ2)+(cid:104)∇L(θ2)  θ1−θ2(cid:105)+
Assumption 3 (Restricted strong convexity of L). Objective function L satisﬁes the following
restricted strong convexity condition:
∀s ∈ [d] ∃µs > 0 ∀θ1  θ2 ∈ Rd : (cid:107)θ1(cid:107)0 (cid:107)θ2(cid:107)0 ≤ s ⇒ L(θ2)+(cid:104)∇L(θ2)  θ1−θ2(cid:105)+

Ls
2

(cid:107)θ1−θ2(cid:107)2.

(cid:107)θ1−θ2(cid:107)2 ≤ L(θ1).

µs
2

By the restricted strong convexity of L  we can easily see that the true parameter of model (1) is the
unique optimal solution of optimization problem (2). We denote the condition number Ls/µs by κs.
Remark. In linear regression settings  Assumptions 2 and 3 are equivalent to assuming

and

∀s ∈ [d] ∃Ls > 0 :

sup

θ∈Rd\{0} (cid:107)θ(cid:107)0≤2s

θ(cid:62)Ex∼DX [xx(cid:62)]θ

(cid:107)θ(cid:107)2

≤ Ls

∀s ∈ [d] ∃µs > 0 :

inf

θ∈Rd\{0} (cid:107)θ(cid:107)0≤2s

θ(cid:62)Ex∼DX [xx(cid:62)]θ

(cid:107)θ(cid:107)2

≥ µs

respectively. Note that these conditions are stronger than restricted eigenvalue condition  but are
weaker than restricted isometry condition.

3 Approach and Algorithm Description

In this section  we illustrate our main ideas and describe the proposed algorithms in detail.

3.1 Exploration algorithm

One of the difﬁculties in partial information settings is that the standard stochastic gradient is
no more available. In linear regression settings  the gradient what we want to estimate is given
y(θ(cid:62)x)x] = E(x y)∼D[2(θ(cid:62)x − y)x]. In general  we need to construct unbiased
by E(x y)∼D[(cid:96)(cid:48)
estimators of E(x y)∼D[yx] and Ex∼DX [xx(cid:62)]. A standard technique is an usage of ˆx  which is
deﬁned as ˆx|j = x|j (j ∈ S) and ˆx|j = 0 (j (cid:54)∈ S)  where S ⊂ [d] is randomly observed with
|S| = s(cid:48) and x ∼ DX. Then we obtain an unbiased estimator of E(x y)∼D[yx] as y d
s(cid:48) ˆx. Similarly  an
unbiased estimator of Ex∼DX [xx(cid:62)] is given by ˆxˆx(cid:62) with adequate element-wise scaling. Note that
particularly the latter estimator has a quite large variance because the probability that the (i  j)- entry
of ˆxˆx(cid:62) becomes non-zero is O(s(cid:48)2/d2) when i (cid:54)= j  which is very small when s(cid:48) (cid:28) d.
If the updated solution θ is sparse  computing θ(cid:62)x requires only observing the attributes of x which
correspond to the support of θ and there exists no need to estimate Ex∼DX [xx(cid:62)]  which has a
potentially large variance. However  this idea is not applied to existing methods because they do not
ensure the sparsity of the updated solutions at training time and generate sparse output solutions only
at prediction time by using the hard thresholding operator.
Iterative applications of the hard thresholding to the updated solutions at training time ensure the
sparsity of them and an efﬁcient construction of unbiased gradient estimators is enabled. Also we can
fully utilize the restricted smoothness and restricted strong convexity of the objective (Assumption 2
and 3) due to the sparsity of the updated solutions if the optimal solution of the objective is sufﬁciently
sparse.
Now we present our proposed estimator. Motivated by the above discussion  we adopt the iterative
usage of the hard thresholding at training time. Thanks to the usage of the hard thresholding operator
that projects dense vectors to s-sparse ones  we are guaranteed that the updated solutions are s(< s(cid:48))-
sparse  where s(cid:48) is the number of observable attributes per example. Hence we can efﬁciently estimate
Ex∼DX [θ(cid:62)xx] as θ(cid:62)xˆx with adequate scaling. As described above  computing θ(cid:62)x can be efﬁciently

4

executed and only requires observing s attributes of x. Thus an naive algorithm based on this idea
becomes as follows:

Sample (xt  yt) ∼ D.
Observe xt|supp(θt−1)∪S  where S is a random subset of [d] with |S| = s(cid:48) − s.
t−1xt − yt)
Compute gt = 2(θ(cid:62)
Update θt = Hs(θt−1 − ηtgt).

s(cid:48) − s

ˆxt.

d

for t = 1  2  . . .   T . Unfortunately  this algorithm has no theoretical guarantee due to the use of
the hard thresholding. Generally  stochastic gradient methods need to decrease the learning rate ηt
as t → ∞ for reducing the noise effect caused by the randomness in the construction of gradient
estimators. Then a large amount of stochastic gradients with small step sizes are cumulated for
proper updates of solutions. However  the hard thresholding operator clears the cumulated effect on
the outside of the support of the current solution at every update and thus the convergence of the
above algorithm is not ensured if decreasing learning rate is used. For overcoming this problem  we
adopt the standard mini-batch strategy for reducing the variance of the gradient estimator without
decreasing the learning rate.
s(cid:48)−s(cid:101) × Bt
We provide the concrete procedure based on the above ideas in Algorithm 1. We sample (cid:100) d
examples per one update. The support of the current solution and deterministically selected s(cid:48) − s
attributes are observed for each example. For constructing unbiased gradient estimator gt  we average
the Bt unbiased gradient estimators  where each estimator is the concatenation of block-wise unbiased
gradient estimators of (cid:100) d
s(cid:48)−s(cid:101) examples. Note that a constant step size is adopted. We call Algorithm
1 as Exploration since each coordinate is equally treated with respect to the construction of the
gradient estimator.

3.2 Reﬁnement of Algorithm 1 using exploitation and its adaptation

As we will state in Theorem 4.1 of Section 4  Exploration (Algorithm 1) achieves a linear convergence
when adequate leaning rate  support size s and mini-batch sizes {Bt}∞
t=1 are chosen. Using this fact 
we can show that Algorithm 1 identiﬁes the optimal support in ﬁnite iterations with high probability.
When once we ﬁnd the optimal support  it is much efﬁcient to optimize the parameter on it rather
than globally. We call this algorithm as Exploitation and describe the detail in Algorithm 2. Ideally  it
is desirable that ﬁrst we run Exploration (Algorithm 1) and if we ﬁnd the optimal support  then we
switch from Exploration to Exploitation (Algorithm 2). However  whether the optimal support has
been found is uncheckable in practice and the theoretical number of updates for ﬁnding it depends
on the smallest magnitude of the non-zero components of the optimal solution  which is unknown.
Therefore  we need to construct an algorithm which combines Exploration and Exploitation  and
is adaptive to the unknown value. We give this adaptive algorithm in Algorithm 3. This algorithm
alternately uses Exploration and Exploitation. We can show that Algorithm 3 achieves at least the
same convergence rate as Exploration  and thanks to the usage of Exploitation  its rate can be much
boosted when the smallest magnitude of the non-zero components of the optimal solution is not too
small. We call this algorithm as Hybrid.

4 Convergence Analysis

In this section  we provide the convergence analysis of our proposed algorithms. We use (cid:101)O nota-
O(cid:0)log(cid:0) κsd

(cid:1)(cid:1)  where δ is a conﬁdence parameter used in the statements.

tion to hide extra log-factors for simplifying the statements. Here  the log-factors have the form

δ

4.1 Analysis of Algorithm 1
The following theorem implies that Algorithm 1 with sufﬁciently large mini-batch sizes {Bt}∞
achieves a linear convergence.
Theorem 4.1 (Exploration). Let T ∈ N and θ0 ∈ Rd. For Algorithm 1  if we adequately choose
  then for any s(cid:48)(> s) ∈ [d]  δ ∈ (0  1) and ∆ > 0

s = O(cid:0)κ2

ss∗(cid:1)  η = Θ

and ˇα = Θ

(cid:16) 1

(cid:16) 1

(cid:17)

t=1

Ls

(cid:17)

κs

5

Algorithm 1: Exploration(θ0 ∈ Rd  η > 0  s(cid:48)  s ∈ [d] (s(cid:48) > s)  {Bt}  T ∈ N)

and Ji = [(s(cid:48) − s)(i − 1) + 1  (s(cid:48) − s)i ∧ d] for i ∈ [d(cid:48)].

(cid:109)

s(cid:48)−s

(cid:16)

Set θ0 = Hs(θ0)  d(cid:48) =
for t = 1 to T do

(cid:108) d
(cid:17) ∼ D for i ∈ [d(cid:48)] and b ∈ [Bt].
Set St−1 = supp(θt−1).
  y(b)
x(b)
Sample
(cid:80)Bt
i
|St−1 and y(b)
|Ji  x(b)
Observe x(b)
Compute gt|Ji = 1
b=1 (cid:96)(cid:48)
Update θt = Hs(θt−1 − ηgt).

(θt−1|(cid:62)

x(b)
i

St−1

y(b)
i

Bt

i

i

i

i

end for
return θT .

for i ∈ [d(cid:48)] and b ∈ [Bt].

|St−1 )x(b)

i

|Ji for i ∈ [d(cid:48)].

Algorithm 2: Exploitation(θ0 ∈ Rd  η > 0  {Bt}  T ∈ N)

Set S0 = supp(θ0).
for t = 1 to T do

Sample(cid:0)x(b)  y(b)(cid:1) ∼ D for b ∈ [Bt].

(cid:80)Bt
b=1 (cid:96)(cid:48)

Observe x(b)|S0 and y(b) for b ∈ [Bt].
Compute gt|S0 = 1
Set gt|S
Update θt = θt−1 − ηgt.

y(b) (θt−1|(cid:62)

= 0.

Bt

(cid:123)
0

S0

x(b)|S0 )x(b)|S0.

end for
return θT .

there exists Bt = (cid:101)O

(cid:16)
(cid:16)L(θT ) − L(θ∗) ≤ (1 − ˇα)T (L(θ0) − L(θ∗) + ∆)

(t = 1  . . .  ∞) such that

s2 ∨ σ2

(1− ˇα)T

R4∞
L2
s

R2∞
Ls

(cid:17)

κ2
s

T s

∆

(cid:17) ≥ 1 − δ.

P

The proof of Theorem 4.1 is found in Section A.1 of the supplementary material.
From Theorem 4.1  we obtain the following corollary  which gives a sample complexity of the
algorithm.
Corollary 4.2 (Exploration). For Algorithm 1  under the settings of Theorem 4.1 with ∆ = L(θ0) −
L(θ∗)  the necessary number of observed samples to achieve P (L(θT ) − L(θ∗) ≤ ε) ≥ 1 − δ is

(cid:18) κsR4∞

µ2
s

(cid:101)O

(cid:19)

.

ds2
s(cid:48) − s

+

κsR2∞

µs

ds
s(cid:48) − s

σ2
ε

The proof of Corollary 4.2 is given in Section A.2 of the supplementary material.
sample complexity of (cid:101)O(ds∗ + dσ2/ε).
Remark. If we set s(cid:48) − s = Θ(s) and assume that κs  R2∞ and µs are Θ(1)  Corollary 4.2 gives the
achieves a sample complexity of (cid:101)O(s2∗ + s∗σ2/ε)  if κs  R2∞ and µs are regard as Θ(1). This rate is
Remark. Corollary 4.2 implies that in full information settings  i.e.  s(cid:48) − s = Θ(d)  Algorithm 4.2
near the minimax optimal sample complexity of (cid:101)O(s∗σ2/ε) in full information settings [13].

Remark. The estimator θT is guaranteed to be asymptotically consistent  because it can be easily seen
that (cid:107)θT − θ∗(cid:107)2 converges to 0 as T → ∞ by using the restricted strong convexity of the objective L
and its convergence rate is nearly same as the one of the objective gap L(θT ) − L(θ∗).

4.2 Analysis of Algorithm 2

Generally  Algorithm 2 does not ensure its convergence. However  the following theorem shows that
running Algorithm 2 with sufﬁciently large batch sizes will not increase the objective values too

6

for k = 1 to K do

Algorithm 3: Hybrid((cid:101)θ0 ∈ Rd  η > 0  s(cid:48)  s ∈ [d] (s(cid:48) > s)  {B−
t=1  T −
k ).

k = Exploration((cid:101)θk−1  η  s(cid:48)  s {B−
t k}∞
k   η {Bt k}∞
t=1  Tk).

Update(cid:101)θ−
Update(cid:101)θk = Exploitation((cid:101)θ−
return (cid:101)θK.

end for

t k}  {Bt k}  {T −

k }  {Tk}  K ∈ N)

much. Moreover  if the support of the optimal solution is included in the one of a initial point  then
Algorithm 2 also achieves a linear convergence.
Theorem 4.3 (Exploitation). Let T ∈ N  θ0 ∈ Rd and s ≥ |supp(θ0)| ∨ |supp(θ∗)| ∈ N. For
  then for any δ ∈ (0  1) and
Algorithm 2  if we adequately choose η = Θ
R2∞
Ls

(cid:17)
(cid:16) 1
(cid:17)
∆ > 0  there exists Bt = (cid:101)O
P
(cid:17) ≥ 1 − δ
(cid:16)L(θT ) − L(θ∗) ≤ 1
(cid:16)L(θT ) − L(θ∗) ≤ (1 − ˇα)T (L(θ0) − L(θ∗) + ∆)
(cid:17) ≥ 1 − δ

(cid:16) R4∞
(1− ˇα)T
1− ˇα (L(θ0) − L(θ∗)) + ∆

(Generally) 
(If supp(θ∗) ⊂ supp(θ0)) .

(t = 1  . . .  ∞) such that

T s2 ∨ σ2

and ˇα = Θ

(cid:16) 1

(cid:17)

µ2
s

Ls

T s

P

κs

∆

The proof of Theorem 4.3 is found in Section B of the supplementary material.

4.3 Analysis of Algorithm 3

σ2

Combining Theorem 4.1 and Theorem 4.3  we obtain the following theorem and corollary. These
imply that using the adequate numbers of inner loops {T −
k }  {Tk} and mini-batch sizes {B−
t k} 
{Bt k} of Algorithm 1 and Algorithm 2 respectively  Algorithm 3 is guaranteed to achieve the same
sample complexity as the one of Algorithm 1 at least. Furthermore  if the smallest magnitude of the
non-zero components of the optimal solution is not too small  its sample complexity can be much
reduced.
(1− ˇα)T . Let K ∈ N and(cid:101)θ0 ∈ Rd. If we adequately choose s = O(κ2
Theorem 4.4 (Hybrid). We denote rmin = minj∈supp(θ∗)|θ∗|j| and Bk(T  s  ˇα) = κ2
T s2 ∨
(cid:17)
(cid:16) 1
(cid:16) 1
(cid:17)
ˇα(1− ˇα)k(L((cid:101)θ0)−L(θ∗))
ss∗) 
(cid:108)
  for any s(cid:48)(> s) ∈ [d] and δ ∈ (0  1
3 )  Algorithm 3 with T −
and adequate Tk = (cid:101)T =
t k = (cid:101)O(Bk(T −
k = 3 
Bt k = (cid:101)O(Bk(Tk  s  ˇα)) satisﬁes
  B−
k   s  ˇα)) and
P
(cid:16)L((cid:101)θK) − L(θ∗) ≤ 2(1 − ˇα)K(L((cid:101)θ0) − L(θ∗)
(cid:16)L((cid:101)θK) − L(θ∗) ≤ 2(1 − ˇα)K+(cid:101)T (L((cid:101)θ0) − L(θ∗))
(cid:108)

(cid:17)(cid:109)
s)(s(cid:48)−s) ∨ 1
(cid:17) ≥ 1 − δ
(cid:17) ≥ 1 − 2δ

(Generally) 
(if K ≥ ˇk + 1) 

R2∞
Ls
and ˇα = Θ

(cid:16) 4(L((cid:101)θ0)−L(θ∗))

log((1− ˇαs)−1) log

(cid:17)(cid:109)

η = O

R4∞
L2
s

(cid:16)

Θ(κ2

Ls

T s

P

κs

d

1

s

where ˇk =

1

log((1− ˇα)−1) log

r2
minµs

.

The proof of Theorem 4.4 is found in Section C.1 of the supplementary material.
Corollary 4.5 (Hybrid). Under the settings of Theorem 4.5  the necessary number of observed

samples to achieve P (L((cid:101)θK) − L(θ∗) ≤ ε) ≥ 1 − δ for Algorithm 3 is

(cid:18) κ3

(cid:101)O

sR4∞
µ2
s

s2 +

κsR4∞

µ2
s

ds2
s(cid:48) − s

+

κsR2∞

µs

ss
µsr2

min

∧ ds
s(cid:48) − s

(cid:18) κ2

(cid:19) σ2

(cid:19)

.

ε

The proof of Corollary 4.5 is given in Section C.2 of the supplementary material.
min) (cid:28) d/(s(cid:48) − s)  the sample complexity of Hybrid can be
Remark. From Corollary 4.5  if κ2
are Θ(1) and s(cid:48) − s = Θ(s)  Algorithm 3 achieves a sample complexity of (cid:101)O(ds∗ + s∗σ2/ε)  which
much better than the one of Exploration only. Particularly  if we assume that κs  R∞/µs and µsr2
partial information settings. In this case  the complexity is signiﬁcantly smaller than (cid:101)O(ds∗ + dσ2/ε)

is asymptotically near the minimax optimal sample complexity of full information algorithms even in

s/(µsr2

min

of Algorithm 1 in this situation.

7

5 Relation to Existing Work

In this section  we describe the relation between our methods and the most relevant existing methods.
The methods of [4] and [7] solve the stochastic linear regression with limited attribute observation 
but the limited information setting is only assumed at training time and not at prediction time  which
is different from ours. Also their theoretical sample complexities are O(1/ε2) which is worse than
ours. The method of [10] solve the sparse linear regression with limited information based on Dantzig
Selector. It has been shown that the method achieves sub-linear regret in both agnostic (online)
and non-agnostic (stochastic) settings under an online variant of restricted isometry condition. The
convergence rate in non-agnostic cases is much worse than the ones of ours in terms of the dependency
on the problem dimension d  but the method has high versatility since it has theoretical guarantees
also in agnostic settings  which have not been focused in our work. The methods of [8] are based
on regularized dual averaging with their exploration-exploitation strategies and achieve a sample
complexity of O(1/ε2) under linear independence of features or compatibility  which is worse than

(cid:101)O(1/ε) of ours. Also the rate of Algorithm 1 in [8] has worse dependency on the dimension d than

the ones of ours. Additionally theoretical analysis of the method assumes linear independence of
features  which is much stronger than restricted isometry condition or our restricted smoothness and
strong convexity conditions. The rate of Algorithm 2  3 in [8] has an additional term which has
quite terrible dependency on d  though it is independent to ε. Their exploration-exploitation idea is
different from ours. Roughly speaking  these methods observe s∗ attributes which correspond to the
coordinates that have large magnitude of the updated solution  and s(cid:48) − s∗ attributes uniformly at
random. This means that exploration and exploitation are combined in single updates. In contrast  our
proposed Hybrid updates a predictor alternatively using Exploration and Exploitation. This is a big
difference: if their scheme is adopted  the variance of the gradient estimator on the coordinates that
have large magnitude of the updated solution becomes small  however the variance reduction effect
is buried in the large noise derived from the other coordinates  and this makes efﬁcient exploitation
impossible. In [9] and [12]  (stochastic) gradient iterative hard thresholding methods for solving
empirical risk minimization with sparse constraints in full information settings have been proposed.
Our Exploration algorithm can be regard as generalization of these methods to limited information
settings.

6 Numerical Experiments

In this section  we provide numerical experiments to demonstrate the performance of the proposed
algorithms through synthetic data and real data.
We compare our proposed Exploration and Hybrid with state-of-the-art Dantzig [10] and RDA
(Algorithm 13in [8]) in our limited attribute observation setting on a synthetic and real dataset.
We randomly split the dataset into training (90%) and testing (10%) set and then we trained each
algorithm on the training set and executed the mean squared error on the test set. We independently
repeated the experiments 5 times and averaged the mean squared error. For each algorithm  we
appropriately tuned the hyper-parameters and selected the ones with the lowest mean squared error.

Synthetic dataset Here we compare the performances in synthetic data. We generated n = 105
samples with dimension d = 500. Each feature was generated from an i.i.d. standard normal. The
optimal predictor was constructed as follows: θ∗|j = 1 for j ∈ [13]  θ∗|j = −1 for j ∈ [14  25] and
θ∗|j = 0 for the other j. The optimal predictor has only 25 non-zero components and thus s∗ = 25.
The output was generated as y = θ(cid:62)
∗ x + ξ  where ξ was generated from an i.i.d. standard normal.
We set the number of observed attributes per example s(cid:48) as 50. Figure 1 shows the averaged mean
squared error as a function of the number of observed samples. The error bars depict two standard
deviation of the measurements. Our proposed Hybrid and Exploration outperformed the other two
methods. RDA initially performed well  but its convergence slowed down. Dantzig showed worse
performance than all the other methods. Hybrid performed better than Exploration and showed rapid
convergence.

3In [8]  three algorithms have been proposed (Algorithm 1  2 and 3). We did not implement the latter two
ones because the theoretical sample complexity of these algorithms makes no sense unless d/s(cid:48) is quite small
due to the existence of the additional term d16/s(cid:48)16 in it.

8

Real dataset Finally  we show the experimental results on a real dataset CT-slice4. CT-slice dataset
consists of n = 53  500 CT images with d = 383 features. The target variable of each image denotes
the relative location of the image on the axial axis. We set the number of observable attributes per
example s(cid:48) as 20. In ﬁgure 2  the mean squared error is depicted against the number of observed
examples. The error bars show two standard deviation of the measurements. Again  our proposed
methods surpasses the performances of the existing methods. Particularly  the convergence of Hybrid
was signiﬁcantly fast and stable. In this dataset  Dantzig showed nice convergence and comparable to
our Exploration. The convergence of RDA was quite slow and a bit unstable.

Figure 1: Comparison on synthetic data.

Figure 2: Comparison on CT-slice data.

7 Conclusion

We presented sample efﬁcient algorithms for stochastic sparse linear regression problem with limited
attribute observation. We developed Exploration algorithm based on an efﬁcient construction of an
unbiased gradient estimator by taking advantage of the iterative usage of hard thresholding in the
updates of predictors . Also we reﬁned Exploration by adaptively combining it with Exploitation
and proposed Hybrid algorithm. We have shown that Exploration and Hybrid achieve a sample

complexity of (cid:101)O(1/ε) with much better dependency on the problem dimension than the ones in

existing work. Particularly  if the smallest magnitude of the non-zero components of the optimal
solution is not too small  the rate of Hybrid can be boosted to near the minimax optimal sample
complexity of full information algorithms. In numerical experiments  our methods showed superior
convergence behaviors compared to preceding methods on synthetic and real data sets.

Acknowledgement

TS was partially supported by MEXT Kakenhi (25730013  25120012  26280009  15H05707 and
18H03201)  Japan Digital Design  and JST-CREST.

References
[1] S. Ben-David and E. Dichterman. Learning with restricted focus of attention. In Proceedings of

the sixth annual conference on Computational learning theory  pages 287–296. ACM  1993.

[2] P. Bühlmann and S. Van De Geer. Statistics for high-dimensional data: methods  theory and

applications. Springer Science & Business Media  2011.

[3] E. Candes  T. Tao  et al. The dantzig selector: Statistical estimation when p is much larger than

n. The Annals of Statistics  35(6):2313–2351  2007.

[4] N. Cesa-Bianchi  S. Shalev-Shwartz  and O. Shamir. Efﬁcient learning with partially observed

attributes. Journal of Machine Learning Research  12(Oct):2857–2878  2011.

4This dataset

is publicly available on https://archive.ics.uci.edu/ml/datasets/

Relative+location+of+CT+slices+on+axial+axis.

9

[5] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting.

Journal of Machine Learning Research  10(Dec):2899–2934  2009.

[6] D. Foster  S. Kale  and H. Karloff. Online sparse linear regression. In Conference on Learning

Theory  pages 960–970  2016.

[7] E. Hazan and T. Koren. Linear regression with limited observation.

arXiv:1206.4678  2012.

arXiv preprint

[8] S. Ito  D. Hatano  H. Sumita  A. Yabe  T. Fukunaga  N. Kakimura  and K.-I. Kawarabayashi.
Efﬁcient sublinear-regret algorithms for online sparse linear regression with limited observation.
In Advances in Neural Information Processing Systems  pages 4102–4111  2017.

[9] P. Jain  A. Tewari  and P. Kar. On iterative hard thresholding methods for high-dimensional
m-estimation. In Advances in Neural Information Processing Systems  pages 685–693  2014.

[10] S. Kale  Z. Karnin  T. Liang  and D. Pál. Adaptive feature selection: Computationally efﬁcient
online sparse linear regression under RIP. In Proceedings of the 34th International Conference
on Machine Learning  volume 70 of Proceedings of Machine Learning Research  pages 1780–
1788  2017.

[11] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear

predictors. Information and Computation  132(1):1–63  1997.

[12] N. Nguyen  D. Needell  and T. Woolf. Linear convergence of stochastic iterative greedy
algorithms with sparse constraints. IEEE Transactions on Information Theory  63(11):6869–
6895  2017.

[13] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax rates of estimation for high-dimensional
linear regression over (cid:96)q -balls. IEEE transactions on information theory  57(10):6976–6994 
2011.

[14] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient

solver for svm. Mathematical programming  127(1):3–30  2011.

[15] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.

Journal of Machine Learning Research  11(Oct):2543–2596  2010.

[16] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages
928–936  2003.

10

,Gauri Jagatap
Chinmay Hegde
Tomoya Murata
Taiji Suzuki