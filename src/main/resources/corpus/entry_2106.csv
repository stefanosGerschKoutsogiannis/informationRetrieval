2016,Finite Sample Prediction and Recovery Bounds for Ordinal Embedding,The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints like ``item $i$ is closer to item $j$ than item $k$''.  Ordinal   constraints like this often come from human judgments.  The classic approach to solving this problem is known as non-metric multidimensional scaling.  To account for errors and variation in judgments  we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. The ordinal embedding problem has been studied for decades  but most past work pays little attention to the question of whether accurate embedding is possible  apart from empirical studies.  This paper shows that under a generative data model it is possible to learn the correct embedding from noisy distance comparisons.  In establishing this fundamental result  the paper makes several new contributions. First  we derive prediction error bounds for embedding from noisy distance comparisons by exploiting the fact that the rank of a distance matrix of points in $\R^d$ is at most $d+2$. These bounds characterize how well a learned embedding predicts new comparative judgments. Second  we show that the underlying embedding can be recovered by solving a simple convex optimization.  This result is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible  but there exists a nonlinear map that is invertible. Third  two new algorithms for ordinal embedding are proposed and evaluated in experiments.,Finite Sample Prediction and Recovery Bounds

for Ordinal Embedding

Lalit Jain

University of Michigan
Ann Arbor  MI 48109
lalitj@umich.edu

Kevin Jamieson

University of California  Berkeley

Berkeley  CA 94720

kjamieson@berkeley.edu

Robert Nowak

University of Wisconsin

Madison  WI 53706
rdnowak@wisc.edu

Abstract

The goal of ordinal embedding is to represent items as points in a low-dimensional
Euclidean space given a set of constraints like “item i is closer to item j than
item k”. Ordinal constraints like this often come from human judgments. The
classic approach to solving this problem is known as non-metric multidimensional
scaling. To account for errors and variation in judgments  we consider the noisy
situation in which the given constraints are independently corrupted by reversing
the correct constraint with some probability. The ordinal embedding problem has
been studied for decades  but most past work pays little attention to the question
of whether accurate embedding is possible  apart from empirical studies. This
paper shows that under a generative data model it is possible to learn the correct
embedding from noisy distance comparisons. In establishing this fundamental
result  the paper makes several new contributions. First  we derive prediction error
bounds for embedding from noisy distance comparisons by exploiting the fact
that the rank of a distance matrix of points in Rd is at most d + 2. These bounds
characterize how well a learned embedding predicts new comparative judgments.
Second  we show that the underlying embedding can be recovered by solving a
simple convex optimization. This result is highly non-trivial since we show that
the linear map corresponding to distance comparisons is non-invertible  but there
exists a nonlinear map that is invertible. Third  two new algorithms for ordinal
embedding are proposed and evaluated in experiments.

1 Ordinal Embedding

Ordinal embedding aims to represent items as points in Rd so that the distances between items agree
as well as possible with a given set of ordinal comparisons such as item i is closer to item j than
to item k. In other words  the goal is to ﬁnd a geometric representation of data that is faithful to
comparative similarity judgments. This problem has been studied and applied for more than 50 years 
dating back to the classic non-metric multidimensional scaling (NMDS) [1  2] approach  and it is
widely used to gauge and visualize how people perceive similarities.
Despite the widespread application of NMDS and recent algorithmic developments [3  4  5  6  7] 
the fundamental question of whether an embedding can be learned from noisy distance/similarity
comparisons had not been answered. This paper shows that if the data are generated according to
a known probabilistic model  then accurate recovery of the underlying embedding is possible by
solving a simple convex optimization  settling this long-standing open question. In the process of
answering this question  the paper also characterizes how well a learned embedding predicts new
distance comparisons and presents two new computationally efﬁcient algorithms for solving the
optimization problem.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

1.1 Related Work
The classic approach to ordinal embedding is NMDS [1  2]. Recently  several authors have proposed
new approaches based on more modern techniques. Generalized NMDS [3] and Stochastic Triplet
Embedding (STE) [6] employ hinge or logistic loss measures and convex relaxations of the low-
dimensionality (i.e.  rank) constraint based on the nuclear norm. These works are most closely related
to the theory and methods in this paper. The Linear partial order embedding (LPOE) method is
similar  but starts with a known Euclidean embedding and learns a kernel/metric in this space based
distance comparison data [7]. The Crowd Kernel [4] and t-STE [6] propose alternative non-convex
loss measures based on probabilistic generative models. The main contributions in these papers are
new optimization methods and experimental studies  but did not address the fundamental question
of whether an embedding can be recovered under an assumed generative model. Other recent work
has looked at the asymptotics of ordinal embedding  showing that embeddings can be learned as the
number of items grows and the items densely populate the embedding space [8  9  10]. In contrast 
this paper focuses on the practical setting involving a ﬁnite set items. Finally  it is known that at least
2dn log n distance comparisons are necessary to learn an embedding of n points in Rd [5].

1.2 Ordinal Embedding from Noisy Data
Consider n points x1  x2  . . .   xn 2 Rd. Let X = [x1 ··· xn] 2 Rd⇥n. The Euclidean distance
matrix D? is deﬁned to have elements D?
2. Ordinal embedding is the problem of
recovering X given ordinal constraints on distances. This paper focuses on “triplet” constraints of
the form D?
ik  where 1  i 6= j 6= k  n. Furthermore  we only observe noisy indications of
these constraints  as follows. Each triplet t = (i  j  k) has an associated probability pt satisfying

ij = kxi  xjk2

ij < D?

pt > 1/2 () kxi  xjk2 < kxi  xkk2 .

Let S denote a collection of triplets drawn independently and uniformly at random. And for
each t 2S we observe an independent random variable yt = 1 with probability pt  and yt = 1
otherwise. The goal is to recover the embedding X from these data. Exact recovery of D? from such
data requires a known link between pt and D?. To this end  our main focus is the following problem.

Ordinal Embedding from Noisy Data
Consider n points x1  x2 ···   xn in d-dimensional Euclidean space. Let S denote a collection of
triplets and for each t 2S observe an independent random variable

yt = 8<:

1 w.p. f (D?
1

ij  D?
ik)

w.p. 1  f (D?

ij  D?
ik)

.

where the link function f : R ! [0  1] is known. Estimate X from S  {yt}  and f.
For example  if f is the logistic function  then for triplet t = (i  j  k)

ij  D?

ik) =

1
1 + exp(D?

ij  D?
ik)

pt = P(yt = 1) = f (D?
ik = log 1pt

ij  D?

pt . However  we stress that we only require the existence of a link

then D?
function for exact recovery of D?. Indeed  if one just wishes to predict the answers to unobserved
triplets  then the results of Section 2 hold for arbitrary pt probabilities. Aspects of the statistical
analysis are related to one-bit matrix completion and rank aggregation [11  12  13]. However  we
use novel methods for the recovery of the embedding based on geometric properties of Euclidean
distance matrices.

 

(1)

1.3 Organization of Paper
This paper takes the following approach to ordinal embedding.

1. Our samples are assumed to be independently generated according to a probabilistic model based
on an underlying low-rank distance matrix. We use relatively standard statistically learning theory

2

techniques to analyze the minimizer of a bounded  Lipschitz loss with a nuclear norm constraint 
and show that an embedding can be learned from the data that predicts nearly as well as the true
embedding with O(dn log n) samples (Theorem 1).
2. Next  assuming the form of the probabilistic generative model is known (e.g.  logistic)  we show
that if the learned embedding is a good predictor of the ordinal comparisons  then it must also be a
good estimator of the true differences of distances between the embedding points (Theorem 2). This
result hinges on the fact that the (linear) observation model acts approximately like an isometry on
differences of distances.
3. While the true differences of distances can be estimated  the observation process is “blind” to the
mean distance between embedding points. Despite this  we show that the mean is determined by the
differences of distances  due to the special properties of Euclidean distance matrices. Speciﬁcally 
the second eigenvalue of the “mean-centered” distance matrix (well-estimated by the data from the
estimate of the differences of distances  Theorem 3) is proportional to the mean distance (Theorem 4).
This allows us to show that the minimizer of the loss with a nuclear norm constraint indeed recovers
an accurate estimate of the underlying true distance matrix.

1.4 Notation and Assumptions
We will use (D?  G?) to denote the distance and Gram matrices of the latent embedding  and (D  G)
to denote an arbitrary distance matrix and its corresponding Gram matrix. The observations {yt} carry
information about D?  but distance matrices are invariant to rotation and translation  and therefore
it may only be possible to recover X up to a rigid transformation. Without loss of generality  we
assume assume the points x1  . . . xn 2 Rd are centered at the origin (i.e. Pn
n 11T . If X is centered  XV = X. Note that D? is
Deﬁne the centering matrix V := I  1
determined by the Gram matrix G? = X T X. In addition  X can be determined from G? up
to a unitary transformation. Note that if X is centered  the Gram matrix is “centered” so that
V G?V = G?. It will be convenient in the paper to work with both the distance and Gram matrix
representations  and the following identities will be useful to keep in mind. For any distance matrix
D and its centered Gram matrix G

i=1 xi = 0).

1
2

V DV  

G = 
D = diag(G)1T  2G + 1diag(G)T  

(3)
where diag(G) is the column vector composed of the diagonal of G. In particular this establishes a
bijection between centered Gram matrices and distance matrices. We refer the reader to [14] for an
insightful and thorough treatment of the properties of distance matrices. We also deﬁne the set of all
unique triplets

(2)

T := (i  j  k) : 1  i 6= j 6= k  n  j < k .

Assumption 1. The observed triplets in S are drawn independently and unifomly from T .
2 Prediction Error Bounds
For t 2T with t = (i  j  k) we deﬁne Lt to be the linear operator satisfying Lt(X T X) =
kxi  xjk2  kxi  xkk2 for all t 2T . In general  for any Gram matrix G

Lt(G) := Gjj  2Gij  Gkk + 2Gik.

We can naturally view Lt as a linear operator on Sn
+  the space of n⇥n symmetric positive semideﬁnite
matrices. We can also represent Lt as a symmetric n ⇥ n matrix that is zero everywhere except on
the submatrix corresponding to i  j  k which has the form

"

and so we will write

0 1
1
1
1

1
0

0 1 #

Lt(G) := hLt  Gi

3

where hA  Bi = vec(A)T vec(B) for any compatible matrices A  B. Ordering the elements of T
2 -dimensional vector
lexicographically  we arrange all the Lt(G) together to deﬁne the nn1
(4)
L(G) = [L123(G) L124(G) ···  Lijk(G) ··· ]T .
Let `(ythLt  Gi) denote a loss function. For example we can consider the 0  1 loss `(ythLt  Gi) =
{sign{ythLt Gi}6=1}  the hinge-loss `(ythLt  Gi) = max{0  1  ythLt  Gi}  or the logistic loss
1

(5)
Let pt := P(yt = 1) and take the expectation of the loss with respect to both the uniformly random
selection of the triple t and the observation yt  we have the risk of G

`(ythLt  Gi) = log(1 + exp(ythLt  Gi)).

1

|T |Xt2T
|S|Xt2S

1

bRS(G) =

R(G)

:= E[`(ythLt  Gi)] =

pt`(hLt  Gi) + (1  pt)`(hLt  Gi).

Given a set of observations S under the model deﬁned in the problem statement  the empirical risk is 
(6)

`(ythLt  Gi)

+  let kGk⇤
(7)

which is an unbiased estimator of the true risk: E[bRS(G)] = R(G). For any G 2 Sn
denote the nuclear norm and kGk1 := maxij |Gij|. Deﬁne the constraint set
We estimate G? by bG  the solution of the program 
bG := argmin

G2G bRS(G) .

+ : kGk⇤   kGk1  } .

G  := {G 2 Sn

Since G? is positive semideﬁnite  we expect the diagonal entries of G? to bound the off-diagonal
entries. So an inﬁnity norm constraint on the diagonal guarantees that the points x1  . . .   xn corre-
sponding to G? live inside a bounded `2 ball. The `1 constraint in (7) plays two roles: 1) if our loss
function is Lipschitz  large magnitude values of hLt  Gi can lead to large deviations of bRS(G) from
R(G); bounding ||G||1 bounds |hLt  Gi|. 2) Later we will deﬁne ` in terms of the link function
f and as the magnitude of hLt  Gi increases the magnitude of the derivative of the link function f
typically becomes very small  making it difﬁcult to “invert”; bounding ||G||1 tends to keep hLt  Gi
within an invertible regime of f.
Theorem 1. Fix    and assume G? 2G  . If the loss function `(·) is L-Lipschitz (or | supy `(y)|
L max{1  12}) then with probability at least 1   

(8)

R(bG)  R(G?) 

4L

|S| r 18|S| log(n)

n

+

p3
3

log n! + Ls 288 log 2/

|S|

Proof. The proof follows from standard statistical learning theory techniques  see for instance [15].
By the bounded difference inequality  with probability 1  
R(bG)  R(G?) = R(bG)  bRS(bG) + bRS(bG)  bRS(G?) + bRS(G?)  R(G?)
where supG2G  `(ythLt  Gi) `(yt0hLt0  Gi)  supG2G  L|hytLt  yt0Lt0  Gi|  12L =: B
using the facts that Lt has 6 non-zeros of magnitude 1 and ||G||1  .
Using standard symmetrization and contraction lemmas  we can introduce Rademacher random
variables ✏t 2 {1  1} for all t 2S so that

G2G  |bRS(G)  R(G)|] +s 2B2 log 2/

G2G  |bRS(G)  R(G)| 2E[ sup

 2 sup

|S|

E sup

G2G  |bRS(G)  R(G)| E sup

G2G 

4

2L

|S|Xt2S

✏thLt  Gi

.

The right hand side is just the Rademacher complexity of G . By deﬁnition 

{G : kGk⇤  } =  · conv({uuT : |u| = 1}).

E sup

where conv(U ) is the convex hull of a set U. Since the Rademacher complexity of a set is the same
as the Rademacher complexity of it’s closed convex hull 

G2G Xt2S

✏tLt! u
✏thLt  Gi  E sup
|u|=1Xt2S
✏thLt  uuTi
which we recognize is just EkPt2S ✏tLtk. By [16  6.6.1] we can bound the operator norm
kPt2S ✏tLtk in terms of the variance ofPt2S L2
t and the maximal eigenvalue of maxt Lt. These
|S| r 18|S| log(n)
EkXt2S

|u|=1
log n! .

are computed in Lemma 1 given in the supplemental materials. Combining these results gives 

uT Xt2S

✏tLtk 

= E sup

2L
|S|

p3
3

2L

+

n

We remark that if G is a rank d < n matrix then

kGk⇤ 

pdkGkF 

pdnkGk1

so if G? is low rank  we really only need a bound on the inﬁnity norm of our constraint set. Under

the assumption that G? is rank d with ||G?||1   and we set  = pdn  then Theorem 1 implies
that for |S| > n log n/161

R(bG)  R(G?)  8Ls 18dn log(n)

|S|

+ Ls 288 log 2/

|S|

with probability at least 1  . The above display says that |S| must scale like dn log(n) which is
consistent with known ﬁnite sample bounds [5].

3 Maximum Likelihood Embedding
We now turn our attention to recovering metric information about G?. Let S be a collection of
triplets sampled uniformly at random with replacement and let f : R ! (0  1) be a known probability
function governing the observations. Any link function f induces a natural loss function `f   namely 
the negative log-likelihood of a solution G given an observation yt deﬁned as
1

1

`f (ythLt  Gi) = 1yt=1 log(

f (hLt Gi) ) + 1yt=1 log(

1f (hLt Gi) )

For example  the logistic link function of (1) induces the logistic loss of (5). Recalling that P(yt =
1) = f (hLt  Gi) we have

E[`f (ythLt  Gi)] = f (hLt  G?i) log(

1

f (hLt Gi) ) + (1  f (hLt  G?i) log(

1

1f (hLt Gi) )

= H(f (hLt  G?i)) + KL(f (hLt  G?i)|f (hLt  Gi))
p ) + (1  p) log( 1

1p ) and KL(p  q) = p log( p

where H(p) = p log( 1
1q ) are the
entropy and KL divergence of Bernoulli RVs with means p  q. Recall that ||G||1   controls the
magnitude of hLt  Gi so for the moment  assume this is small. Then by a Taylor series f (hLt  Gi) ⇡
2 + f0(0)hLt  Gi using the fact that f (0) = 1

2  and by another Taylor series we have

q ) + (1  p) log( 1p

1

KL(f (hLt  G?i)|f (hLt  Gi)) ⇡ KL( 1

2 + f0(0)hLt  G?i| 1
⇡ 2f0(0)2(hLt  G?  Gi)2.

2 + f0(0)hLt  Gi)

R(G) = 1

Thus  recalling the deﬁnition of L(G) from (4) we conclude that if eG 2 arg minG R(G) with
|T |Pt2T E[`f (ythLt  Gi)] then one would expect L(eG) ⇡L (G?). Moreover  since
bRS(G) is an unbiased estimator of R(G)  one expects L(bG) to approximate L(G?). The next

theorem  combined with Theorem 1  formalizes this observation; its proof is found in the appendix.

5

Theorem 2. Let Cf = mint2T inf G2G  |f0hLt  Gi| where f0 denotes the derivative of f. Then

for any G

2C2
f

|T | kL(G) L (G?)k2

F  R(G)  R(G?) .

1

4 exp(6).

t(D) := Dij  Dik ⌘L t(G) .

4 exp(6||G||1) for any t  G so it sufﬁces to take Cf = 1

2 -dimensional vector
(D) = [D12  D13 ···   Dij  Dik ··· ]T .

Note that if f is the logistic link function of (1) then its straightforward to show that |f0hLt  Gi|
4 exp(|hLt  Gi|)  1
It remains to see that we can recover G? even given L(G?)  much less L(bG). To do this  it is more
convenient to work with distance matrices instead of Gram matrices. Analogous to the operators
Lt(G) deﬁned above  we deﬁne the operators t for t 2T satisfying 
We will view the t as linear operators on the space of symmetric hollow n ⇥ n matrices Sn
h  which
includes distance matrices as special cases. As with L  we can arrange all the t together  ordering
the t 2T lexicographically  to deﬁne the nn1
We will use the fact that L(G) ⌘ (D) heavily. Because (D) consists of differences of matrix
entries   has a non-trivial kernel. However  it is easy to see that D can be recovered given (D) and
any one off-diagonal element of D  so the kernel is 1-dimensional. Also  the kernel is easy to identify
by example. Consider the regular simplex in d dimensions. The distances between all n = d + 1
vertices are equal and the distance matrix can easily be seen to be 11T  I. Thus (D) = 0 in this
case. This gives us the following simple result.
Lemma 2. Let Sn
h denote the space of symmetric hollow matrices  which includes all distance
2  1
matrices. For any D 2 Sn
dimensional subspace of Sn
So we see that the operator  is not invertible on Sn
h. Deﬁne J := 11T  I. For any D  let C  the
centered distance matrix  be the component of D orthogonal to the kernel of L (i.e.  tr(CJ) = 0).
Then we have the orthogonal decomposition

h  the set of linear functionals {t(D)  t 2T } spans ann
h  and the 1-dimensional kernel is given by the span of 11T  I.

where D = trace(DJ)/kJk2
interpretation:
2 X1ijn
2n

D =

1

D = C + D J  

F . Since G is assumed to be centered  the value of D has a simple

Dij =

2

n  1 X1in

hxi  xii =

2kGk⇤
n  1

 

(9)

the average of the squared distances or alternatively a scaled version of the nuclear norm of G.

close to C?. The next theorem quantiﬁes this.

solution to 8. Though  is not invertible on all Sn

Let bD and bC be the corresponding distance and centered distance matrices corresponding to bG the
the kernel  namely J?. So if (bD) ⇡ (D?)  or equivalently L(bG) ⇡L (G?)  we expect bC to be
Theorem 3. Consider the setting of Theorems 1 and 2 and let bC  C? be deﬁned as above. Then
fs 288 log 2/

h  it is invertible on the subspace orthogonal to

log n! +

L
4C2

p3
3

L
4C2

F 

n

1

|S|

Proof. By combining Theorem 2 with the prediction error bounds obtainined in 1 we see that

2kbC  C?k2
2n

2C2
f

2 kL(bG) L (G?)k2
nn1

F 

+

f|S| r 18|S| log(n)
|S| r 18|S| log(n)

4L

n

p3
3

+

log n! + Ls 288 log 2/

.

|S|

Next we employ the following restricted isometry property of  on the subspace J? whose proof is
in the supplementary materials.

6

Lemma 3. Let D and D0 be two different distance matrices of n points in Rd and Rd0. Let C and
C0 be the components of D and D0 orthogonal to J. Then

nkC  C0k2

F  k(C)  (C0)k2 = k(D)  (D0)k2  2(n  1)kC  C0k2
F .

The result then follows.

F  O⇣ L

C2

This implies that by collecting enough samples  we can recover the centered distance matrix. By
applying the discussion following Theorem 1 when G? is rank d  we can state an upperbound of

2)kbC  C?k2
1
2(n
recover D? or G?. Remarkably  despite this unknown component being in the kernel  we show next
that it can be recovered.
Theorem 4. Let D be a distance matrix of n points in Rd  let C be the component of D orthogonal
to the kernel of L  and let 2(C) denote the second largest eigenvalue of C. If n > d + 2  then

⌘. However  it is still not clear that this is enough to

fq dn log(n)+log(1/)

|S|

D = C + 2(C) J .

(10)

This shows that D is uniquely determined as a function of C. Therefore  since (D) =( C) and
because C is orthogonal to the kernel of   the distance matrix D can be recovered from (D) 
even though the linear operator  is non-invertible.
We now provide a proof of Theorem 4 in the case where n > d + 3. The result is true in the case
when n > d + 2 but requires a more detailed analysis. This includes the construction of a vector x
such that Dx = 1 and 1T x  0 for any distance matrix a result in [17].
Proof. To prove Theorem 4 we need the following lemma  proved in the supplementary materials.
Lemma 4. Let D be a Euclidean distance matrix on n points. Then D is negative semideﬁnite on
the subspace

1? := {x 2 Rn|1T x = 0}.

Furthermore  ker(D) ⇢ 1?.
For any matrix M  let i(M ) denote its ith largest eigenvalue. Under the conditions of the theorem 
we show that for > 0  2(D  J) = . Since C = D  DJ  this proves the theorem.
Note that  i(D  J) = i(D  11T ) +  for 1  i  n and  arbitrary. So it sufﬁces to show
that 2(D  11T ) = 0.
By Weyl’s Theorem

Since 1(11T ) = 0  we have 2(D  11T )  2(D) = 0. By the Courant-Fischer Theorem

2(D  11T )  2(D) + 1(11T ) .

2(D) =

min

U :dim(U )=n1

max

x2U x6=0

xT Dx
xT x  min

U =1?

max

x2U x6=0

xT Dx
xT x  0

since D negative semideﬁnite on 1?. Now let vi denote the ith eigenvector of D with eigenvalue
i = 0. Then

(D  11T )vi = Dvi = 0  

since vT
i 1 = 0 by 4. So D  11T has at least n  d  2 zero eigenvalues  since rankD  d + 2.
In particular  if n > d + 3  then D  11T must have at least two eigenvalues equal to 0. Therefore 
2(D  11T ) = 0.
The previous theorem along with Theorem 3 guarantees that we can recover G? as we increase
the number of triplets sampled. The ﬁnal theorem  which follows directly from Theorems 3 and 4 
summarizes this.
Theorem 5. Assume n > d + 2 and consider the setting of Theorems 1 and 2. As |S| ! 1 
bD ! D⇤ where bD is the distance matrix corresponding to bG (the solution to 8).
Proof. Recall bD = bC + 2(bC)J  so as bC ! C⇤  bD ! D⇤.

7

Figure 1: G? generated with n = 64 points in d = 2 and d = 8 dimensions on the left and right.

4 Experimental Study

line search  projecting onto the subspace spanned by the top d eigenvalues at each step (i.e. setting the

for any triplet t = (i  j  k).We report the prediction error on a holdout set of 10  000 triplets and
the error in Frobenius norm of the estimated Gram matrix over 36 random trials. We minimize the

2⇤  E⇥kxi  xjk2
2 || xi  xk||2
|S|Pt2S log(1 + exp(ythLt  Gi)).

The section empirically studies the properties of estimators suggested by our theory. It is not an
attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see
2d Id) 2 Rd 
[18  4  6  3]. In what follows each of the n points is generated randomly: xi ⇠N (0  1
i = 1  . . .   n  motivated by the observation that
2⇤ = 2E⇥kxik2
2⇤ = 1

E[|hLt  G?i|] = E⇥kxi  xjk2
logistic MLE objective bRS(G) = 1
For each algorithm considered  the domain of the objective variable G is the space of symmetric
positive semi-deﬁnite matrices. None of the methods impose the constraint maxij |Gij|  (as
done above)  since this was used to simplify the analysis and does not have a large impact in practice.
Rank-d Projected Gradient Descent (PGD) performs gradient descent on the objective bRS(G) with
smallest n d eigenvalues to 0). Nuclear Norm PGD performs gradient descent on bRS(G) projecting
onto the nuclear norm ball with radius kG?k⇤  where G? is the Gram matrix of the latent embedding.
The nuclear norm projection can have the undesirable effect of shrinking the non-zero eigenvalues
toward the origin. To compensate for this potential bias  we employ Nuclear Norm PGD Debiased 
which takes the biased output of Nuclear Norm PGD  decomposes it into U EU T where U 2 Rn⇥d
are the top d eigenvectors  and outputs Udiag(bs)U T wherebs = arg mins2Rd bRS(Udiag(s)U T ).
This last algorithm is motivated by the observation that methods for minimizing k·k 1 or k·k ⇤ are
good at identifying the true support of a signal  but output biased magnitudes [19]. Rank-d PGD and
Nuclear Norm PGD Debiased are novel ordinal embedding algorithms.
Figure 1 presents how the algorithms behave for n = 64 and d = 2  8. We observe that the unbiased
nuclear norm solution behaves near-identically to the rank-d solution and remark that this was
observed in all of our experiments (see the supplementary materials for other values of n  d  and
scalings of G?). A popular technique for recovering rank d embeddings is to perform (stochastic)

In all of our experiments this method produced Gram matrices nearly identical to those produced by
our Rank-d-PGD method  but Rank-d-PGD was an order of magnitude faster in our implementation.

gradient descent on bRS(U T U ) with objective variable U 2 Rn⇥d taken as the embedding [18  4  6].
Also  in light of our isometry theorem  we can show that the Hessian of E[bRS(G)] is nearly a scaled

identity  leading us to hypothesize that a globally optimal linear convergence result for this non-
convex optimization may be possible using the techniques of [20  21]. Finally  we note that previous
literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less
accurate embeddings than those of non-convex methods [4  6]. The results imply that Nuclear Norm
PGD Debiased appears to close the performance gap between the convex and non-convex solutions.
Acknowledgments This work was partially supported by the NSF grants CCF-1218189 and IIS-
1447449  the NIH grant 1 U54 AI117924-01  the AFOSR grant FA9550-13-1-0138  and by ONR
awards N00014-15-1-2620  and N00014-13-1-0129. We would also like to thank Amazon Web
Services for providing the computational resources used for running our simulations.

8

References
[1] Roger N Shepard. The analysis of proximities: Multidimensional scaling with an unknown

distance function. i. Psychometrika  27(2):125–140  1962.

[2] Joseph B Kruskal. Nonmetric multidimensional scaling: a numerical method. Psychometrika 

29(2):115–129  1964.

[3] Sameer Agarwal  Josh Wills  Lawrence Cayton  Gert Lanckriet  David J Kriegman  and Serge
Belongie. Generalized non-metric multidimensional scaling. In International Conference on
Artiﬁcial Intelligence and Statistics  pages 11–18  2007.

[4] Omer Tamuz  Ce Liu  Ohad Shamir  Adam Kalai  and Serge J Belongie. Adaptively learning
the crowd kernel. In Proceedings of the 28th International Conference on Machine Learning
(ICML-11)  pages 673–680  2011.

[5] Kevin G Jamieson and Robert D Nowak. Low-dimensional embedding using adaptively selected
ordinal data. In Communication  Control  and Computing (Allerton)  2011 49th Annual Allerton
Conference on  pages 1077–1084. IEEE  2011.

[6] Laurens Van Der Maaten and Kilian Weinberger. Stochastic triplet embedding. In Machine
Learning for Signal Processing (MLSP)  2012 IEEE International Workshop on  pages 1–6.
IEEE  2012.

[7] Brian McFee and Gert Lanckriet. Learning multi-modal similarity. The Journal of Machine

Learning Research  12:491–523  2011.

[8] Matthäus Kleindessner and Ulrike von Luxburg. Uniqueness of ordinal embedding. In COLT 

pages 40–67  2014.

[9] Yoshikazu Terada and Ulrike V Luxburg. Local ordinal embedding. In Proceedings of the 31st

International Conference on Machine Learning (ICML-14)  pages 847–855  2014.

[10] Ery Arias-Castro. Some theory for ordinal embedding. arXiv preprint arXiv:1501.02861  2015.
[11] Mark A Davenport  Yaniv Plan  Ewout van den Berg  and Mary Wootters. 1-bit matrix

completion. Information and Inference  3(3)  2014.

[12] Yu Lu and Sahand N Negahban. Individualized rank aggregation using nuclear norm regulariza-
tion. In 2015 53rd Annual Allerton Conference on Communication  Control  and Computing
(Allerton)  pages 1473–1479. IEEE  2015.

[13] D. Park  J   Neeman  J. Zhang  S. Sanghavi  and I. Dhillon. Preference completion: Large-scale
collaborative ranking from pairwise comparisons. Proc. Int. Conf. Machine Learning (ICML) 
2015.

[14] Jon Dattorro. Convex Optimization & Euclidean Distance Geometry. Meboo Publishing USA 

2011.

[15] Stéphane Boucheron  Olivier Bousquet  and Gábor Lugosi. Theory of classiﬁcation: A survey

of some recent advances. ESAIM: probability and statistics  9:323–375  2005.

[16] Joel A. Tropp. An introduction to matrix concentration inequalities  2015.
[17] Pablo Tarazaga and Juan E. Gallardo. Euclidean distance matrices: new characterization and

boundary properties. Linear and Multilinear Algebra  57(7):651–658  2009.

[18] Kevin G Jamieson  Lalit Jain  Chris Fernandez  Nicholas J Glattard  and Rob Nowak. Next: A
system for real-world development  evaluation  and application of active learning. In Advances
in Neural Information Processing Systems  pages 2638–2646  2015.

[19] Nikhil Rao  Parikshit Shah  and Stephen Wright. Conditional gradient with enhancement and

truncation for atomic norm regularization. In NIPS workshop on Greedy Algorithms  2013.

[20] Samet Oymak  Benjamin Recht  and Mahdi Soltanolkotabi. Sharp time–data tradeoffs for linear

inverse problems. arXiv preprint arXiv:1507.04793  2015.

[21] Jie Shen and Ping Li. A tight bound of hard thresholding. arXiv preprint arXiv:1605.01656 

2016.

9

,Lalit Jain
Kevin Jamieson
Rob Nowak