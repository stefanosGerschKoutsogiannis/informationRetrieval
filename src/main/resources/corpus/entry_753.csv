2009,Bayesian Source Localization with the Multivariate Laplace Prior,We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the specification of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior.,Bayesian Source Localization with the

Multivariate Laplace Prior

Marcel van Gerven1 2 Botond Cseke1 Robert Oostenveld2 Tom Heskes1 2

Radboud University Nijmegen
Nijmegen  The Netherlands

1Institute for Computing and Information Sciences

2Donders Institute for Brain  Cognition and Behaviour

Abstract

We introduce a novel multivariate Laplace (MVL) distribution as a sparsity pro-
moting prior for Bayesian source localization that allows the speciﬁcation of con-
straints between and within sources. We represent the MVL distribution as a scale
mixture that induces a coupling between source variances instead of their means.
Approximation of the posterior marginals using expectation propagation is shown
to be very efﬁcient due to properties of the scale mixture representation. The com-
putational bottleneck amounts to computing the diagonal elements of a sparse ma-
trix inverse. Our approach is illustrated using a mismatch negativity paradigm for
which MEG data and a structural MRI have been acquired. We show that spatial
coupling leads to sources which are active over larger cortical areas as compared
with an uncoupled prior.

1 Introduction

Y = XS + E

Electroencephalography (EEG) and magnetoencephalography (MEG) provide an instantaneous and
non-invasive measure of brain activity. Let q  p  and t denote the number of sensors  sources and
time points  respectively. Sensor readings Y ∈ Rq×t and source currents S ∈ Rp×t are related by
(1)
where X ∈ Rq×p is a lead ﬁeld matrix that represents how sources project onto the sensors and
E ∈ Rq×t represents sensor noise.
Unfortunately  localizing distributed sources is an ill-posed inverse problem that only admits a
unique solution when additional constraints are deﬁned. In a Bayesian setting  these constraints
take the form of a prior on the sources [3  19]. Popular choices of prior source amplitude distri-
butions are Gaussian or Laplace priors  whose MAP estimates correspond to minimum norm and
minimum current estimates  respectively [18]. Minimum norm estimates produce spatially smooth
solutions but are known to suffer from depth bias and smearing of nearby sources. In contrast  mini-
mum current estimates lead to focal source estimates that may be scattered too much throughout the
brain volume [9].
In this paper  we take the Laplace prior as our point of departure for Bayesian source localization
(instead of using just the MAP estimate). The obvious approach is to assume univariate Laplace
priors on individual sources. Here  in contrast  we assume a multivariate Laplace distribution over
all sources  which allows sources to be coupled. We show that such a distribution can be represented
as a scale mixture [2] that differs substantially from the one presented in [5].
Our representation allows the speciﬁcation of both spatio-temporal as well as sparsity constraints.
Since the posterior cannot be computed exactly  we formulate an efﬁcient expectation propagation

1

algorithm [12] which allows us to approximate the posterior of interest for very large models. Ef-
ﬁciency arises from the block diagonal form of the approximate posterior covariance matrix due to
properties of the scale mixture representation. The computational bottleneck then reduces to com-
putation of the diagonal elements of a sparse matrix inverse  which can be solved through Cholesky
decomposition of a sparse matrix and application of the Takahashi equation [17]. Furthermore  mo-
ment matching is achieved by one-dimensional numerical integrations. Our approach is evaluated
on MEG data that was recorded during an oddball task.

2 Bayesian source localization

In a Bayesian setting  the goal of source localization is to estimate the posterior

p(S | Y  X  Σ  Θ) ∝ p(Y | S  X  Σ)p(S | Θ)

where the likelihood term p(Y | S) =(cid:81)

(2)
t N (yt | Xst  Σ) factorizes over time and Σ represents
sensor noise. The prior p(S | Θ)  with Θ acting as a proxy for the hyper-parameters  can be used
to incorporate (neuroscientiﬁc) constraints. For simplicity  we assume independent Gaussian noise
with a ﬁxed variance σ2  i.e.  Σ = σ2I. Without loss of generality  we will focus on one time-point
(yt  st) only and drop the subscript when clear from context.1
The source localization problem can be formulated as a (Bayesian) linear regression problem where
the source currents s play the role of the regression coefﬁcients and rows of the lead ﬁeld matrix
X can be interpreted as covariates. In the following  we deﬁne a multivariate Laplace distribution 
represented in terms of a scale mixture  as a convenient prior that incorporates both spatio-temporal
and sparsity constraints.
The univariate Laplace distribution

L (s | λ) ≡ λ
2

exp (−λ|s|)

(3)

can be represented as a scale mixture of Gaussians [2]  the scaling function being an exponential
distribution with parameter λ2/2. The scale parameter λ controls the width of the distribution and
thus the regularizing behavior towards zero. Since the univariate exponential distribution is a χ2
2
distribution  one can alternatively write

dudv N(cid:0)s | 0  u2 + v2(cid:1)N(cid:0)u | 0  1/λ2(cid:1)N(cid:0)v | 0  1/λ2(cid:1) .

L (s | λ) =

(cid:90)

(4)

Eltoft et al [5] deﬁned the multivariate Laplace distribution as a scale mixture of a multivariate
√
zΣ1/2s where s is a standard normal multivariate Gaussian  Σ is a positive
Gaussian given by
deﬁnite matrix  and z is drawn from a univariate exponential distribution. The work presented
in [11] is based on similar ideas but replaces the distribution on z with a multivariate log-normal
distribution.
In contrast  we use an alternative formulation of the multivariate Laplace distribution that couples
the variances of the sources rather than the source currents themselves. This is achieved by gener-
alizing the representation in Eq. (4) to the multivariate case. For an uncoupled multivariate Laplace
distribution  this generalization reads

dudv(cid:89)

N(cid:0)si | 0  u2

i + v2

i

(cid:1)N(cid:0)vi | 0  1/λ2(cid:1)N(cid:0)ui | 0  1/λ2(cid:1)

L (s | λ) =

(cid:90)

(5)

i

such that each source current si gets assigned scale variables ui and vi. We can interpret the scale
variables corresponding to source i as indicators of its relevance: the larger (the posterior estimate
of) u2
i   the more relevant the corresponding source. In order to introduce correlations between
sources  we deﬁne our multivariate Laplace (MVL) distribution as the following scale mixture:

i + v2

(cid:90)

(cid:32)(cid:89)

N(cid:0)si | 0  u2

i + v2

i

(cid:1)(cid:33)

L (s | λ  J) ≡

dudv

N(cid:0)v | 0  J−1/λ2(cid:1)N(cid:0)u | 0  J−1/λ2(cid:1)  

(6)

1Multiple time-points can be incorporated by vectorizing Y and S  and augmenting X.

i

2

prior. The factor f represents the likelihood term N(cid:0)y | Xs  σ2I(cid:1). Factors gi correspond to the

Figure 1: Factor graph representation of Bayesian source localization with a multivariate Laplace

coupling between sources and scales. Factors h1 and h2 represent the (identical) multivariate Gaus-
sians on u and v with prior precision matrix J. The gi are the only non-Gaussian terms and need to
be approximated.

where J−1 is a normalized covariance matrix. This deﬁnition yields a coupling in the magnitudes
of the source currents through their variances. The normalized covariance matrix J−1 speciﬁes the
correlation strengths  while λ acts as a regularization parameter. Note that this approach is deﬁning
the multivariate Laplace with the help of a multivariate exponential distribution [10]. As will be
shown in the next section  apart from having a semantics that differs from [5]  our scale mixture rep-
resentation has some desirable characteristics that allow for efﬁcient approximate inference. Based
on the above formulation  we deﬁne the sparse linear model as

p(y  s | X  σ2  λ  J) = N(cid:0)y | Xs  σ2I(cid:1)L (s | λ  J) .

(7)

The factor graph in Fig. 1 depicts the interactions between the variables in our model.

3 Approximate inference

Our goal is to compute posterior marginals for sources si as well as scale variables ui and vi in order
to determine source relevance. These marginals are intractable and we need to resort to approximate
inference methods. In this paper we use a deterministic approximate inference method called expec-
tation propagation (EP) [12]. For a detailed analysis of the use of EP in case of the decoupled prior 
which is a special case of our MVL prior  we refer to [16]. EP works by iterative minimizations of
the Kullback–Leibler (KL) divergence between appropriately chosen distributions in the following
way.
We introduce the vector of all latent variables z = (sT   uT   vT )T . The posterior distribution on z
given the data y (which is considered ﬁxed and given and therefore omitted in our notation) can be
written in the factorized form

where t0(z) ∝ N(cid:0)y | Xs  σ2I(cid:1)N(cid:0)v | 0  J−1/λ2(cid:1)N(cid:0)u | 0  J−1/λ2(cid:1) and ti(z) = ti(si  ui  vi) =
(cid:1). The term t0(z) is a Gaussian function  i.e.  it can be written in the form
N(cid:0)si | 0  u2
block-diagonal structure. Using EP  we will approximate p(z) with q (z) ∝ t0(z)(cid:81)

exp(zT h0 − zT K0z/2). It factorizes into Gaussian functions of s  u  and v such that K0 has a
¯ti (z)  where
the ¯ti(z) are Gaussian functions as well.
Our deﬁnition of the MVL distribution leads to several computational beneﬁts. Equation (6) intro-
duces 2p auxiliary Gaussian variables (u  v) that are coupled to the si’s by p non-Gaussian factors 
thus  we have to approximate p terms. The multivariate Laplace distribution deﬁned in [5] introduces
one auxiliary variable and couples all the sisj terms to it  therefore  it would lead to p2 non-Gaussian
terms to be approximated. Moreover  as we will see below  the a priori independence of u and v and

p(z) ∝ t0(z)(cid:89)

i + v2

i

i

ti(z)  

i

(8)

3

fs1s2···spg1g2···gpu1v1u2v2···upvph1h2i by deﬁning q\i ∝ t0(z)(cid:81)\i

¯tj  minimizing KL(cid:0)tiq\i (cid:107) q∗(cid:1) with
gence then boils down to the minimization of KL(cid:0)ti(zi)q\i(zi) (cid:107) q∗(zi)(cid:1) with respect to q∗(zi)

the form of the terms ti(z) results in an approximation of the posterior with the same block-diagonal
structure as that of t0(z).
In each step  EP updates ¯ti with ¯t∗
i ∝ q∗/q\i. It can be shown that when ti depends only on a subset of
respect to q∗ and setting ¯t∗
variables zi (in our case on zi = (si  ui  vi)) then so does ¯ti. The minimization of the KL diver-
i (zi) ∝ q∗(zi)/q\i(zi). Minimization of the KL divergence corresponds to
and ¯ti is updated to ¯t∗
moment matching  i.e.  q∗(si  ui  vi) is a Gaussian with the same mean and covariance matrix as
qi(zi) ∝ ti(zi)q\i(zi). So  to update the i-th term in a standard application of EP  we would have
to compute q\i(zi) and could then use a three-dimensional (numerical) integration to compute all
ﬁrst and second moments of qi(zi). Below we will explain how we can exploit the speciﬁc charac-
teristics of the MVL to do this more efﬁciently. For stability  we use a variant of EP  called power
EP [13]  where q\i ∝ ¯t(1−α)
explanation of standard EP corresponds to α = 1. In the following we will give the formulas for
general α.
We will now work out the EP update for the i-th term approximation in more detail to show by
induction that ¯ti(si  ui  vi) factorizes into independent terms for si  ui  and vi. Since ui and vi play
exactly the same role  it is also easy to see that the term approximation is always symmetric in ui
and vi. Let us suppose that q (si  ui  vi) and consequently q\i (si  ui  vi) factorizes into independent
terms for si  ui  and vi  e.g.  we can write

i q\i (cid:107) q∗(cid:1) with α ∈ (0  1] is minimized. The above

¯tj and KL(cid:0)tα

(cid:81)\i

i

(10)

(cid:19)

(cid:1)

i

i

ασ2

i

i

i

q\i (si  ui  vi) = N (si | mi  σ2

i )N (ui | 0  ν2

i )N (vi | 0  ν2
i ).

i + v2

i

i + v2

qi(si | ui  vi) ∝ N

(9)
By initializing ¯ti(si  ui  vi) = 1  we have q(z) ∝ t0(z) and the factorization of q\i (si  ui  vi)
follows directly from the factorization of t0(z) into independent terms for s  u  and v. That is  for
the ﬁrst EP step  the factorization can be guaranteed. To obtain the new term approximation  we
have to compute the moments of the distribution qi(si  ui  vi) ∝ N (si | 0  u2
i )αq\i(si  ui  vi) 
which  by regrouping terms  can be written in the form qi(si  ui  vi) = qi(si | ui  vi)qi(ui  vi) with

(cid:18)
qi(ui  vi) ∝ (cid:0)u2
(cid:3))/2. Since qi(ui  vi) can be expressed as a function of u2
(cid:3) + E(cid:2)v2
(cid:3) = (E(cid:2)u2
(cid:3) can be computed with one-dimensional Gauss-Laguerre integration. Summa-

(11)
Since qi(ui  vi) only depends on u2
i and is thus invariant under sign changes of ui and vi 
we must have E [ui] = E [vi] = 0  as well as E [uivi] = 0. Because of symmetry  we further have
i + v2
i
only  this variance can be computed from (11) using one-dimensional Gauss-Laguerre numerical
quadrature [15]. The ﬁrst and second moments of si conditioned upon ui and vi follow directly
from (10). Because both (10) and (11) are invariant under sign changes of ui and vi  we must have
E [siui] = E [sivi] = 0. Furthermore  since the conditional moments again depend only on u2
i + v2
i  

(cid:3) = E(cid:2)v2
E(cid:2)u2
also E [si] and E(cid:2)s2

i + v2
×N (ui | 0  ν2
i and v2

(cid:1)(1−α)/2 N(cid:0)√

i (u2
σ2
 
i + u2
ασ2
αmi | 0  ασ2

i + v2
i )
i + v2
i + u2

si | mi(u2
i + u2

i )N (vi | 0  ν2
i ) .

i + v2
i )
i + v2

rizing  we have shown that if the old term approximations factorize into independent terms for si  ui 
i (si  ui  vi) ∝ q∗(si  ui  vi)/q\i(si  ui  vi) 
and vi  the new term approximation after an EP update  ¯t∗
must do the same. Furthermore  given the cavity distribution q\i(si  ui  vi)  all required moments
can be computed using one-dimensional numerical integration.
The crucial observation here is that the terms ti(si  ui  vi) introduce dependencies between si and
(ui  vi)  as expressed in Eqs. (10) and (11)  but do not lead to correlations that we have to keep track
of in a Gaussian approximation. This is not speciﬁc to EP  but a consequence of the symmetries and
invariances of the exact distribution p(s  u  v). That is  also when the expectations are taken with
respect to the exact p(s  u  v) we have E [ui] = E [vi] = E [uivi] = E [siui] = E [sivi] = 0 and

(cid:3) determines the amount of regularization

(cid:3). The variance of the scales E(cid:2)u2

(cid:3) = E(cid:2)v2

E(cid:2)u2

on the source parameter si such that large variance implies little regularization.
Last but not least  contrary to conventional sequential updating  we choose to update the terms ¯ti in
parallel. That is  we compute all q\is and update all terms simultaneously. Calculating q\i(si  ui  vi)

i + v2

i

i

i

i

i

i

4

requires the computation of the marginal moments q(si)  q(ui) and q(vi). For this  we need the
diagonal elements of the inverse of the precision matrix K of q(z). This precision matrix has the
block-diagonal form

 XT X/σ2 + Ks

0
0

K =



λ2J + Ku

0

0

0
0

λ2J + Kv

(12)

where J is a sparse precision matrix which determines the coupling  and Ks  Ku  and Kv = Ku
are diagonal matrices that contain the contributions of the term approximations. We can exploit the
low-rank representation of XT X/σ2 + Ks to compute its inverse using the Woodbury formula [7].
The diagonal elements of the inverse of λ2J + Ku can be computed efﬁciently via sparse Cholesky
decomposition and the Takahashi equation [17]. By updating the term approximations in parallel 
we only need to perform these operations once per parallel update.

4 Experiments

Returning to the source localization problem  we will show that the MVL prior can be used to induce
constraints on the source estimates. To this end  we use a dataset obtained for a mismatch negativity
experiment (MMN) [6]. The MMN is the negative component of the difference between responses
to normal and deviant stimuli within an oddball paradigm that peaks around 150 ms after stimulus
onset. In our experiment  the subject had to listen to normal (500 Hz) and deviant (550 Hz) tones 
presented for 70 ms. Normal tones occurred 80% of the time  whereas deviants occurred 20% of the
time. A total of 600 trials was acquired.
Data was acquired with a CTF MEG System (VSM MedTech Ltd.  Coquitlam  British Columbia 
Canada)  which provides whole-head coverage using 275 DC SQUID axial gradiometers. A re-
alistically shaped volume conduction model was constructed based on the individual’s structural
MRI [14]. The brain volume was discretized to a grid with a 0.75 cm resolution and the lead ﬁeld
matrix was calculated for each of the 3863 grid points according to the head position in the system
and the forward model. The lead ﬁeld matrix is deﬁned for the three x  y  and z orientations in each
of the source locations and was normalized to correct for depth bias. Consequently  the lead ﬁeld
matrix X is of size 275 × 11589. The 275 × 1 observation vector y was rescaled to prevent issues
with numerical precision.
In the next section  we compare source estimates for the MMN difference wave that have been
obtained when using either a decoupled or a coupled MVL prior. For ease of exposition  we focus
on a spatial prior induced by the coupling of neighboring sources. In order to demonstrate the effect
of the spatial prior  we assume a ﬁxed regularization parameter λ and ﬁxed noise variance σ2  as
estimated by means of the L curve criterion [8]. Differences in the source estimates will therefore
arise only from the form of the 11589 × 11589 sparse precision matrix J. The ﬁrst estimate is
obtained by assuming that there is no coupling between elements of the lead ﬁeld matrix  such that
J = I. This gives a Bayesian formulation of the minimum current estimate [18]. The second
estimate is obtained by assuming a coupling between neighboring sources i and j within the brain
volume with ﬁxed strength c. This coupling is speciﬁed through the unnormalized precision matrix
ˆJij.2
This prior dictates that the magnitude of the variances of the source currents are coupled between
sources.
For the coupling strength c  we use correlation as a guiding principle. Recall that the unnormal-
ized precision matrix ˆJ in the end determines the correlations (of the variances) between sources.
Speciﬁcally  correlation between sources si and sj is given by

ˆJ by assuming ˆJix jx = ˆJiy jy = ˆJiz jz = −c while diagonal elements ˆJii are set to 1 −(cid:80)

j(cid:54)=i

(cid:16)ˆJ−1(cid:17)

(cid:16)ˆJ−1(cid:17) 1

2

(cid:16)ˆJ−1(cid:17) 1

2

/

ij

ii

jj

rij =

.

(13)

For example  using c = 10  we would obtain a correlation coefﬁcient of ri i+1 = 0.78. Note that this
also leads to more distant sources having non-zero correlations. The positive correlation between

2The normalized precision matrix is obtained through J = diag(ˆJ−1)

1

2 ˆJ diag(ˆJ−1)

1
2 .

5

Figure 2: Spatial coupling leads to the normalized precision matrix J with coupling of neighboring
source orientations in the x  y  and z directions. The (reordered) matrix L is obtained from the
Cholesky decomposition of J. The correlation matrix C shows the correlations between the source
orientations. For the purpose of demonstration  we show matrices using a very coarse discretization
of the brain volume.

neighboring sources is motivated by the notion that we expect neighboring sources to be similarly
though not equivalently involved for a given task. Evidently  the desired correlation coefﬁcient also
depends on the resolution of the discretized brain volume.
Figure 2 demonstrates how a chosen coupling leads to a particular structure of J  where irregularities
in J are caused by the structure of the imaged brain volume. The ﬁgure also shows the computational
bottleneck of our algorithm  which is to compute diagonal elements of J−1. This can be solved by
means of the Takahashi equation which operates on the matrix L that results from a sparse Cholesky
decomposition. The block diagonal structure of L arises from a reordering of rows and columns
using  for instance  the amd algorithm [1]. The correlation matrix C shows the correlations between
the sources induced by the structure of J. Zeros in the correlation matrix arise from the independence
between source orientations x  y  and z.

5 Results

Figure 3 depicts the difference wave that was obtained by subtracting the trial average for standard
tones from the trial average for deviant tones. A negative deﬂection after 100 ms is clearly visible.
The event-related ﬁeld indicates patterns of activity at central channels in both hemispheres. These

Figure 3: Evolution of the difference wave at right central sensors and event-related ﬁeld of the
difference wave 125 ms after cue onset.

6

JLC00.050.10.150.20.250.3−10−8−6−4−2024x10−14time(s)standarddeviantdifferenceFigure 4: Source estimates using a decoupled prior (top) or a coupled prior (bottom). Plots are
centered on the left temporal source.

Figure 5: Relative variance using a decoupled prior (top) or a coupled prior (bottom). Plots are
centered on the right temporal source.

ﬁndings are consistent with the mismatch negativity literature [6]. We now proceed to localizing the
sources of the activation induced by mismatch negativity.
Figure 4 depicts the localized sources when using either a decoupled MVL prior or a coupled MVL
prior. The coupled spatial prior leads to stronger source currents that are spread over a larger brain
volume. MVL source localization has correctly identiﬁed the source over left temporal cortex but
does not capture the source over right temporal cortex that is also hypothesized to be present (cf.
Fig. 3). Note however that the source estimates in Fig. 4 represent estimated mean power and thus
do not capture the full posterior over the sources.
Differences between the decoupled and the coupled prior become more salient when we look at the
relative variance of the auxiliary variables as shown in Fig. 5. Relative variance is deﬁned here as
posterior variance minus prior variance of the auxiliary variables  normalized to be between zero
and one. This measure indicates the change in magnitude of the variance of the auxiliary variables 
and thus indirectly that of the sources via Eq. (6). Since only sources with non-zero contributions
should have high variance  this measure can be used to indicate the relevance of a source. Figure 5

7

shows that temporal sources in both left and right hemispheres are relevant. The relevance of the
temporal source in the right hemisphere becomes more pronounced when using the coupled prior.

6 Discussion

In this paper  we introduced a multivariate Laplace prior as the basis for Bayesian source localiza-
tion. By formulating this prior as a scale mixture we were able to approximate posteriors of interest
using expectation propagation in an efﬁcient manner. Computation time is mainly inﬂuenced by the
sparsity structure of the precision matrix J which is used to specify interactions between sources by
coupling their variances. We have demonstrated the feasibility of our approach using a mismatch
negativity dataset. It was shown that coupling of neighboring sources leads to source estimates that
are somewhat more spatially smeared as compared with a decoupled prior. Furthermore  visualiza-
tion of the relative variance of the auxiliary variables gave additional insight into the relevance of
sources.
Contrary to the MAP estimate (i.e.  the minimum current estimate)  our Bayesian estimate does not
exactly lead to sparse posteriors given a ﬁnite amount of data. However  posterior marginals can
still be used to exclude irrelevant sources since these will typically have a mean activation close to
zero with small variance. In principle  we could force our posteriors to become more MAP-like by

replacing the likelihood term with N(cid:0)y | Xs  σ2I(cid:1)1/T in the limit T → 0. From the Bayesian point

of view  one may argue whether taking this limit is fair. In any case  given the inherent uncertainty
in our estimates we favor the representation in terms of (non-sparse) posterior marginals.
Note that it is straightforward to impose other constraints since this only requires the speciﬁcation
of suitable interactions between sources through J. For instance  the spatial prior could be made
more realistic by taking anatomical constraints into account or by the inclusion of coupling between
sources over time. Other constraints that can be implemented with our approach are the coupling of
individual orientations within a source  or even the coupling of source estimates between different
subjects. Coupling of source orientations has been realized before in [9] through an (cid:96)1/(cid:96)2 norm 
although not using a fully Bayesian approach. In future work  we aim to examine the effect of the
proposed priors and optimize the regularization and coupling parameters via empirical Bayes [4].
Other directions for further research are inclusion of the noise variance in the optimization procedure
and dealing with the depth bias that often arises in distributed source models in a more principled
way.
In [11]  ﬁelds of Gaussian scale mixtures were used for modeling the statistics of wavelet coefﬁcients
of photographics images. Our approach differs in two important aspects. To obtain a generalization
of the univariate Laplace distribution  we used a multivariate exponential distribution of the scales 
to be compared with the multivariate log-normal distribution in [11]. The Laplace distribution has
the advantage that it is the most sparsifying prior that  in combination with a linear model  still leads
to a unimodal posterior [16]. Furthermore  we described an efﬁcient method for approximating
marginals of interest whereas in [11] an iterative coordinate-ascent method was used to compute the
MAP solution. Since (the efﬁciency of) our method for approximate inference only depends on the
sparsity of the multivariate scale distribution  and not on its precise form  it should be feasible to
compute approximate marginals for the model presented in [11] as well.
Concluding  we believe the scale mixture representation of the multivariate Laplace distribution
to be a promising approach to Bayesian distributed source localization. It allows a wide range of
constraints to be included and  due to the characteristics of the scale mixture  posteriors can be
approximated efﬁciently even for very large models.

Acknowledgments

The authors gratefully acknowledge the support of the Dutch technology foundation STW (project
number 07050) and the BrainGain Smart Mix Programme of the Netherlands Ministry of Economic
Affairs and the Netherlands Ministry of Education  Culture and Science. Tom Heskes is supported
by Vici grant 639.023.604.

8

References
[1] P. R. Amestoy  T. A. Davis  and I. S. Duff. Algorithm 837: Amd  an approximate minimum
degree ordering algorithm. ACM Transactions on Mathematical Software  30(3):381–388 
2004.

[2] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal

Statistical Society  Series B  36(1):99–102  1974.

[3] S. Baillet and L. Garnero. A Bayesian approach to introducing anatomo-functional priors in the
EEG/MEG inverse problem. IEEE Transactions on Biomedical Engineering  44(5):374–385 
1997.

[4] J. M. Bernardo and J. F. M. Smith. Bayesian Theory. Wiley  1994.
[5] T. Eltoft  T. Kim  and T. Lee. On the multivariate Laplace distribution. IEEE Signal Processing

Letters  13(5):300–303  2006.

[6] M. I. Garrido  J. M. Kilner  K. E. Stephan  and K. J. Friston. The mismatch negativity: A

review of underlying mechanisms. Clinical Neurophysiology  120:453–463  2009.

[7] G. Golub and C. van Loan. Matrix Computations. John Hopkins University Press  Baltimore 

MD  3rd edition  1996.

[8] P. C. Hansen. Rank-Deﬁcient and Discrete Ill-Posed Problems: Numerical Aspects of Linear
Inversion. Monographs on Mathematical Modeling and Computation. Society for Industrial
Mathematics  1987.

[9] S. Haufe  V. V. Nikulin  A. Ziehe  K.-R. M¨uller  and G. Nolte. Combining sparsity and rota-

tional invariance in EEG/MEG source reconstruction. NeuroImage  42(2):726–738  2008.

[10] N. T. Longford. Classes of multivariate exponential and multivariate geometric distributions
derived from Markov processes. In H. W. Block  A. R. Sampson  and T. H. Savits  editors 
Topics in statistical dependence  volume 16 of IMS Lecture Notes Monograph Series  pages
359–369. IMS Business Ofﬁce  Hayward  CA  1990.

[11] S. Lyu and E. P. Simoncelli. Statistical modeling of images with ﬁelds of Gaussian scale
mixtures. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information
Processing Systems 19  pages 945–952. MIT Press  Cambridge  MA  2007.
[12] T. Minka. Expectation propagation for approximate Bayesian inference.

In J. Breese and
D. Koller  editors  Proceedings of the Seventeenth Conference on Uncertainty in Artiﬁcial In-
telligence  pages 362–369. Morgan Kaufmann  2001.

[13] T. Minka. Power EP. Technical report  Microsoft Research  Cambridge  2004.
[14] G. Nolte. The magnetic lead ﬁeld theorem in the quasi-static approximation and its use
for magnetoencephalography forward calculation in realistic volume conductors. Physics in
Medicine & Biology  48(22):3637–3652  2003.

[15] W. H. Press  S. A. Teukolsky  W. T. Vetterling  and B. P. Flannery. Numerical Recipes in C.

Cambridge University Press  3rd edition  2007.

[16] M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of

Machine Learning Research  9:759–813  2008.

[17] K. Takahashi  J. Fagan  and M. S. Chen. Formation of a sparse bus-impedance matrix and its
application to short circuit study. In 8th IEEE PICA Conference  pages 63–69  Minneapolis 
MN  1973.

[18] K. Uutela  M. H¨am¨al¨ainen  and E. Somersalo. Visualization of magnetoencephalographic data

using minimum current estimates. NeuroImage  10:173–180  1999.

[19] D. Wipf and S. Nagarajan. A uniﬁed Bayesian framework for MEG/EEG source imaging.

NeuroImage  44(3):947–966  2009.

9

,Deepti Pachauri
Risi Kondor
Gautam Sargur
Vikas Singh
Sylvestre-Alvise Rebuffi
Hakan Bilen
Andrea Vedaldi
Zijun Zhang
Yining Zhang
Zongpeng Li