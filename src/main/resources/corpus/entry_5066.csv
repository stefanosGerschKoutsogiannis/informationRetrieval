2010,Smoothness  Low Noise and Fast Rates,We establish an excess risk bound of O(H R_n^2 + sqrt{H L*} R_n) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n  where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = sqrt{R/n}  this translates to a learning rate of ̃ O(RH/n) in the separable (L* = 0) case and O(RH/n + sqrt{L* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.,Smoothness  Low-Noise and Fast Rates

Nathan Srebro
nati@ttic.edu

Karthik Sridharan
karthik@ttic.edu

Ambuj Tewari

ambuj@cs.utexas.edu

Toyota Technological Institute at Chicago

Computer Science Dept.  University of Texas at Austin

(cid:16)

Abstract
√
HR2

n +

(cid:17)

We establish an excess risk bound of ˜O
for ERM with an H-smooth loss
function and a hypothesis class with Rademacher complexity Rn  where L∗ is the best risk achiev-

able by the hypothesis class. For typical hypothesis classes where Rn =(cid:112)R/n  this translates to

a learning rate of ˜O (RH/n) in the separable (L∗ = 0) case and ˜O
more
generally. We also provide similar guarantees for online and stochastic convex optimization of a
smooth non-negative objective.

RH/n +(cid:112)L∗RH/n

(cid:17)

(cid:16)

HL∗Rn

(cid:80)n

Introduction

i=1 φ(h(xi)  yi) of an i.i.d. sample (x1  y1)  . . .   (xn  yn).

1
Consider empirical risk minimization for a hypothesis class H = {h : X → R} w.r.t. some non-negative loss function
φ(t  y). That is  we would like to learn a predictor h with small risk L (h) = E [φ(h(X)  Y )] by minimizing the
empirical risk ˆL(h) = 1
n
Statistical guarantees on the excess risk are well understood for parametric (i.e. ﬁnite dimensional) hypothesis classes.
More formally  these are hypothesis classes with ﬁnite VC-subgraph dimension [23] (aka pseudo-dimension). For
such classes learning guarantees can be obtained for any bounded loss function (i.e. s.t. |φ| ≤ b < ∞) and the relevant
measure of complexity is the VC-subgraph dimension.
Alternatively  even for some non-parametric hypothesis classes (i.e. those with inﬁnite VC-subgraph dimension) 
e.g. the class of low-norm linear predictors HB = {hw : x (cid:55)→ (cid:104)w  x(cid:105)|(cid:107)w(cid:107) ≤ B}   guarantees can be obtained in terms
of scale-sensitive measures of complexity such as fat-shattering dimensions [1]  covering numbers [23] or Rademacher
complexity [2]. The classical statistical learning theory approach for obtaining learning guarantees for such scale-
sensitive classes is to rely on the Lipschitz constant D of φ(t  y) w.r.t. t (i.e. bound on its derivative w.r.t. t). The
excess risk can then be bounded as (in expectation over the sample):

L

Rn(H) is the Rademacher complexity  which typically scales as Rn(H) = (cid:112)R/n. E.g. for (cid:96)2-bounded linear

(1)
where ˆh = arg min ˆL(h) is the empirical risk minimizer (ERM)  L∗ = inf h L (h) is the approximation error  and
predictors  R = B2 sup(cid:107)X(cid:107)2
2.
In this paper we address two deﬁciencies of the guarantee (1). First  the bound applies only to loss functions with
bounded derivative  like the hinge loss and logistic loss popular for classiﬁcation  or the absolute-value ((cid:96)1) loss for
2 (t − y)2  for which the second derivative is
regression. It is not directly applicable to the squared loss φ(t  y) = 1
bounded  but not the ﬁrst. We could try to simply bound the derivative of the squared loss in terms of a bound on
the magnitude of h(x)  but e.g. for norm-bounded linear predictors HB this results in a very disappointing excess risk

bound of the form O((cid:112)B4(max(cid:107)X(cid:107))4/n). One aim of this paper is to provide clean bounds on the excess risk for

smooth loss functions such as the squared loss with a bounded second  rather then ﬁrst  derivative.

(cid:16)ˆh
(cid:17) ≤ L∗ + 2DRn(H) = L∗ + 2

(cid:114)

D2 R
n

1

√
n might be unavoidable in general.
The second deﬁciency of (1) is the dependence on 1/
But at least for ﬁnite dimensional (parametric) classes  we know it can be improved to a 1/n rate when the distribution
is separable  i.e. when there exists h ∈ H with L (h) = 0 and so L∗ = 0. In particular  if H is a class of bounded
functions with VC-subgraph-dimension d (e.g. d-dimensional linear predictors)  then in expectation over sample [22]:

√
n. The dependence on 1/

(cid:16)ˆh
(cid:17) ≤ L∗ + O

(cid:32)

(cid:114)

dDL∗ log n

(cid:33)

(cid:17)

(cid:18) HR

(cid:19)

(cid:16)ˆh
(cid:17) ≤ L

∗

(cid:16)

(cid:32)

(cid:114)

(cid:33)

dD log n

L

The(cid:112)1/n term disappears in the separable case  and we get a graceful degradation between the(cid:112)1/n rate to the 1/n
such as the hinge-loss  the excess risk might scale as(cid:112)1/n and not 1/n  even in the separable case. However  for

rate for separable case. Could we get a 1/n separable rate  and such a graceful degradation  in non-parametric case?
As we will show  the two deﬁciencies are actually related. For non-parametric classes  and non-smooth Lipschitz loss 

H-smooth non-negative loss functions  where the second derivative of φ(t  y) w.r.t. t is bounded by H  a 1/n separable
rate is possible. In Section 2 we obtain the following bound on the excess risk (up to logarithmic factors):

(2)

+

n

n

HR2

n(H) +

√
HL∗Rn(H)

∗
= L

+ ˜O

HR

HRL∗

≤ 2L
∗

+ ˜O

+ ˜O

.

n

n

n

L

+

(3)
2 ≤ 1  the excess risk is bounded by

In particular  for (cid:96)2-norm-bounded linear predictors HB with sup(cid:107)X(cid:107)2

˜O(HB2/n +(cid:112)HB2L∗/n). Another interesting distinction between parametric and non-parametric classes  is that

√
even for the squared-loss  the bound (3) is tight and the non-separable rate of 1/
n is unavoidable. This is in con-
trast to the parametric (ﬁne dimensional) case  where a rate of 1/n is always possible for the squared loss  regardless
of the approximation error L∗ [16]. The differences between parametric and scale-sensitive classes  and between
non-smooth  smooth and strongly convex loss functions are discussed in Section 4 and summarized in Table 1.
The guarantees discussed thus far are general learning guarantees for the stochastic setting that rely only on the
Rademacher complexity of the hypothesis class  and are phrased in terms of minimizing some scalar loss function. In
Section 3 we consider also the online setting  in addition to the stochastic setting  and present similar guarantees for
online and stochastic convex optimization [32  24]. The guarantees of Section 3 match equation (3) for the special
case of a convex loss function and norm-bounded linear predictors  but Section 3 capture a more general setting of
optimizing an arbitrary non-negative convex objective  which we require to be smooth (there is no separate discussion
of a “predictor” and a scalar loss function in Section 3). Results in Section 3 are expressed in terms of properties of
the norm  rather then a measure of concentration like the Radamacher complexity as in (3) and Section 2. However 
the online and stochastic convex optimization setting of Section 3 is also more restrictive  as we require the objective
be convex (in Section 2 we make no assumption about the convexity of hypothesis class H nor the loss function φ).
Speciﬁcally  for a non-negative H-smooth convex objective  over a domain bounded by B  we prove that the average

online regret (and excess risk of stochastic optimization) is bounded by O(HB2/n +(cid:112)HB2L∗/n). Comparing with
the bound of O((cid:112)D2B2/n) when the loss is D-Lipschitz rather then H-smooth [32  21]  we see the same relationship

discussed above for ERM. Unlike the bound (3) for the ERM  the convex optimization bound avoids polylogarithmic
factors. The results in Section 3 also generalize to smoothness and boundedness with respect to non-Euclidean norms.
Studying the online and stochastic convex optimization setting (Section 3)  in addition to ERM (Section 2)  has several
advantages. First  it allows us to obtain a learning guarantee for an efﬁcient single-pass learning methods  namely
stochastic gradient descent (or mirror descent)  as well as for the non-stochastic regret. Second  the bound we obtain
in the convex optimization setting (Section 3) is actually better then the bound for the ERM (Section 2) as it avoids all
polylogarithmic and large constant factors. Third  the bound is applicable to other non-negative online or stochastic
optimization problems beyond classiﬁcation  including problems for which ERM is not applicable (see  e.g.  [24]).
The detailed proofs of the statements claimed in this paper can be found in the supplementary material corresponding
to the paper.

2 Empirical Risk Minimization with Smooth Loss
Recall that the Rademacher complexity of H for any n ∈ N given by [2]:
1
n

Eσ∼Unif({±1}n)

Rn(H) =

(cid:34)

sup

x1 ... xn∈X

sup
h∈H

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

h(xi)σi

(4)

2

Throughout we shall consider the “worst case” Rademacher complexity.
Our starting point is the learning bound (1) that applies to D-Lipschitz loss functions  i.e. such that |φ(cid:48)(t  y)| ≤ D
(we always take derivatives w.r.t. the ﬁrst argument). What type of bound can we obtain if we instead bound the
second derivative φ(cid:48)(cid:48)(t  y)? We will actually avoid talking about the second derivative explicitly  and instead say that
a function is H-smooth iff its derivative is H-Lipschitz. For twice differentiable φ  this just means that |φ(cid:48)(cid:48)| ≤ H.
The central observation  which allows us to obtain guarantees for smooth loss functions  is that for a smooth loss  the
derivative can be bounded in terms of the function value:

Lemma 2.1. For an H-smooth non-negative function f : R (cid:55)→ R  we have: |f(cid:48)(t)| ≤(cid:112)4Hf (t)
φ(cid:48)( ˆw  x) at the ERM ˆw. Applying Lemma 2.1 to L(ˆh)  we can bound |E [φ(cid:48)( ˆw  X)]| ≤(cid:113)

This Lemma allows us to argue that close to the optimum value  where the value of the loss is small  then so is its
derivative. Looking at the dependence of (1) on the derivative bound D  we are guided by the following heuristic
intuition: Since we should be concerned only with the behavior around the ERM  perhaps it is enough to bound
4HL(ˆh). What we would
actually want is to bound each |φ(cid:48)( ˆw  x)| separately  or at least have the absolute value inside the expectation—this
is where the non-negativity of the loss plays an important role. Ignoring this important issue for the moment and
plugging this instead of D into (1) yields L(ˆh) ≤ L∗ + 4
HL(ˆh)Rn(H). Solving for L(ˆh) yields the desired bound
(3).
This rough intuition is captured by the following Theorem:
Theorem 1. For an H-smooth non-negative loss φ s.t.∀x y h |φ(h(x)  y)| ≤ b  for any δ > 0 we have that with
probability at least 1 − δ over a random sample of size n  for any h ∈ H 

(cid:113)

L (h) ≤ ˆL(h) + K

(cid:32)(cid:113)
(cid:32)√
(cid:16)ˆh
(cid:17) ≤ L∗ + K

and so:

L

(cid:32)√
(cid:32)√

(cid:114)
(cid:114)

(cid:33)
(cid:33)

ˆL(h)

H log1.5n Rn(H) +

b log(1/δ)

n

+ H log3n R2

n(H) +

b log(1/δ)

n

L∗

H log1.5n Rn(H) +

b log(1/δ)

n

+ H log3n R2

n(H) +

b log(1/δ)

n

(cid:33)
(cid:33)

where K < 105 is a numeric constant derived from [20] and [6].
Note that only the “conﬁdence” terms depended on b = sup|φ|  and this is typically not the dominant term—we
believe it is possible to also obtain a bound that holds in expectation over the sample (rather than with high probability)
and that avoids a direct dependence on sup|φ|.
To prove Theorem 1 we use the notion of Local Rademacher Complexity [3]  which allows us to focus on the behavior
close to the ERM. To this end  consider the following empirically restricted loss class
(x  y) (cid:55)→ φ(h(x)  y) : h ∈ H  ˆL(h) ≤ r

Lφ(r) :=

(cid:111)

(cid:110)

√

Lemma 2.2  presented below  solidiﬁes the heuristic intuition discussed above  by showing that the Rademacher com-
plexity of Lφ(r) scales with
Hr. The Lemma can be seen as a higher-order version of the Lipschitz Composition
Lemma [2]  which states that the Rademacher complexity of the unrestricted loss class is bounded by DRn(H). Here 
we use the second  rather then ﬁrst  derivative  and obtain a bound that depends on the empirical restriction:
Lemma 2.2. For a non-negative H-smooth loss φ bounded by b and any function class H bounded by B:

Rn(Lφ(r)) ≤

√

12Hr Rn(H)

16 log3/2

(cid:32)

(cid:19)

(cid:18) nB

Rn(H)

(cid:32)

√

n

(cid:33)(cid:33)

− 14 log3/2

12HB√
b

Applying Lemma 2.2  Theorem 1 follows using standard Local Rademacher argument [3].

2.1 Related Results
√

Rates faster than 1/

n have been previously explored under various conditions  including when L∗ is small.

3

The Finite Dimensional Case : Lee et al [16] showed faster rates for squared loss  exploiting the strong convexity
of this loss function  even when L∗ > 0  but only with ﬁnite VC-subgraph-dimension. Panchenko [22] provides fast
rate results for general Lipschitz bounded loss functions  still in the ﬁnite VC-subgraph-dimension case. Bousquet [6]
provided similar guarantees for linear predictors in Hilbert spaces when the spectrum of the kernel matrix (covariance
of X) is exponentially decaying  making the situation almost ﬁnite dimensional. All these methods rely on ﬁniteness
of effective dimension to provide fast rates. In this case  smoothness is not necessary. Our method  on the other hand 
establishes fast rates  when L∗ = 0  for function classes that do not have ﬁnite VC-subgraph-dimension. We show
how in this non-parametric case  smoothness is necessary and plays an important role (see also Table 1).
Aggregation : Tsybakov [29] studied learning rates for aggregation  where a predictor is chosen from the convex
hull of a ﬁnite set of base predictors. This is equivalent to an (cid:96)1 constraint where each base predictor is viewed
as a “feature”. As with (cid:96)1-based analysis  since the bounds depend only logarithmically on the number of base
predictors (i.e. dimensionality)  and rely on the scale of change of the loss function  they are of “scale sensitive”
nature. For such an aggregate classiﬁer  Tsybakov obtained a rate of 1/n when zero (or small) risk is achieve by
one of the base classiﬁers. Using Tsybakov’s result  it is not enough for zero risk to be achieved by an aggregate
(i.e. bounded ell1) classiﬁer in order to obtain the faster rate. Tsybakov’s core result is thus in a sense more similar
to the ﬁnite dimensional results  since it allows for a rate of 1/n when zero error is achieved by a ﬁnite cardinality
(and hence ﬁnite dimension) class. Tsybakov then used the approximation error of a small class of base predictors
w.r.t. a large hypothesis class (i.e. a covering) to obtain learning rates for the large hypothesis class by considering
√
aggregation within the small class. However these results only imply fast learning rates for hypothesis classes with
very low complexity. Speciﬁcally to get learning rates better than 1/
n using these results  the covering number of
the hypothesis class at scale  needs to behave as 1/p for some p < 2. But typical classes  including the class of
linear predictors with bounded norm  have covering numbers that scale as 1/2 and so these methods do not imply
fast rates for such function classes. In fact  to get rates of 1/n with these techniques  even when L∗ = 0  requires
covering numbers that do not increase with  at all  and so actually ﬁnite VC-subgraph-dimension. Chesneau et al
√
[10] extend Tsybakov’s work also to general losses  deriving similar results for Lipschitz loss function. The same
caveats hold: even when L∗ = 0  rates faster when 1/
n require covering numbers that grow slower than 1/2  and
rates of 1/n essentially require ﬁnite VC-subgraph-dimension. Our work  on the other hand  is applicable whenever the
Rademacher complexity (equivalently covering numbers) can be controlled. Although it uses some similar techniques 
it is also rather different from the work of Tsybakov and Chesneau et al  in that it points out the importance of
smoothness for obtaining fast rates in the non-parametric case: Chesneau et al relied only on the Lipschitz constant 
which we show  in Section 4  is not enough for obtaining fast rates in the non-parametric case  even when L∗ = 0.
Local Rademacher Complexities : Bartlett et al [3] developed a general machinery for proving possible fast rates
based on local Rademacher complexities. However  it is important to note that the localized complexity term typically
dominates the rate and still needs to be controlled. For example  Steinwart [27] used Local Rademacher Complexity to
provide fast rate on the 0/1 loss of Support Vector Machines (SVMs) ((cid:96)2-regularized hinge-loss minimization) based on
the so called “geometric margin condition” and Tsybakov’s margin condition. Steinwart’s analysis is speciﬁc to SVMs.
We also use Local Rademacher Complexities in order to obtain fast rates  but do so for general hypothesis classes 
based only on the standard Rademacher complexity Rn(H) of the hypothesis classes  as well as the smoothness of the
loss function and the magnitude of L∗  but without any further assumptions on the hypothesis classes itself.
Non-Lipschitz Loss : Beyond the strong connections between smoothness and fast rates which we highlight  we are
also not aware of prior work providing an explicit and easy-to-use result for controlling a generic non-Lipschitz loss
(such as the squared loss) solely in terms of the Rademacher complexity.

3 Online and Stochastic Optimization of Smooth Convex Objectives
We now turn to online and stochastic convex optimization. In these settings a learner chooses w ∈ W  where W is a
closed convex set in a normed vector space  attempting to minimize an objective (cid:96)(w  z) on instances z ∈ Z  where
(cid:96) : W ×Z → R is an objective function which is convex in w. This captures learning linear predictors w.r.t. a convex
loss function φ(t  z)  where Z = X × Y and (cid:96)(w  (x  y)) = φ((cid:104)w  x(cid:105)  y)  and extends beyond supervised learning.
We consider the case where the objective (cid:96)(w  z) is H-smooth w.r.t. some norm (cid:107)w(cid:107) (the reader may choose to think
of W as a subset of a Euclidean or Hilbert space  and (cid:107)w(cid:107) as the (cid:96)2-norm): By this we mean that for any z ∈ Z  and
all w  w(cid:48) ∈ W

(cid:107)∇(cid:96)(w  z) − ∇(cid:96)(w(cid:48)  z)(cid:107)∗ ≤ H (cid:107)w − w(cid:48)(cid:107)

4

where (cid:107) · (cid:107)∗ is the dual norm. The key here is to generalize Lemma 2.1 to smoothness w.r.t. a vector w  rather than
scalar smoothness:

Lemma 3.1. For an H-smooth non-negative f : W → R  for all w ∈ W: (cid:107)∇f (w)(cid:107)∗ ≤(cid:112)4Hf (w)

In order to consider general norms  we will also need to rely on a non-negative regularizer F : W (cid:55)→ R that is a
1-strongly convex (see Deﬁnition in e.g. [31]) w.r.t. to the norm (cid:107)w(cid:107) for all w ∈ W. For the Euclidean norm we can
use the squared Euclidean norm regularizer: F (w) = 1

2 (cid:107)w(cid:107)2.

3.1 Online Optimization Setting

In the online convex optimization setting we consider an n round game played between a learner and an adversary
(Nature) where at each round i  the player chooses a wi ∈ W and then the adversary picks a zi ∈ Z. The player’s
choice wi may only depend on the adversary’s choices in previous rounds. The goal of the player is to have low
average objective value 1
n
A classic algorithm for this setting is Mirror Descent [4]  which starts at some arbitrary w1 ∈ W and updates wi+1
according to zi and a stepsize η (to be discussed later) as follows:

i=1 (cid:96)(wi  zi) compared to the best single choice in hind sight [9].

(cid:80)n

wi+1 ← arg min
w∈W

(cid:104)η∇(cid:96)(wi  zi) − ∇F (wi)  w(cid:105) + F (w)

(5)

For the Euclidean norm with F (w) = 1
wi+1 ← ΠW(wi − η∇(cid:96)(wi  zi)) where ΠW(w) = arg minw(cid:48)∈W (cid:107)w − w(cid:48)(cid:107) is the projection onto W.
Theorem 2. For any B ∈ R and L∗ if we use stepsize η =
H 2B4+HB2nL∗ for the Mirror Descent algorithm
then for any instance sequence z1  . . .   zn ∈ Z  the average regret w.r.t. any w∗ ∈ W s.t. F (w∗) ≤ B2 and

2(cid:107)w(cid:107)2  the update (5) becomes projected online gradient descent [32]:

HB2+

√

1

(cid:80)n
j=1 (cid:96)(w∗  zi) ≤ L∗ is bounded by:

1
n

n(cid:88)

i=1

1
n

(cid:96)(wi  zi) − 1
n

n(cid:88)

i=1

(cid:96)(w∗  zi) ≤ 4HB2

n

+ 2

(cid:115)

HB2L∗

n

Note that the stepsize depends on the bound L∗ on the loss in hindsight. The above theorem can be proved using
Lemma 3.1 and Theorem 1 of [26].

3.2 Stochastic Optimization

An online algorithm can also serve as an efﬁcient one-pass learning algorithm in the stochastic setting. Here  we again
consider an i.i.d. sample z1  . . .   zn from some unknown distribution (as in Section 2)  and we would like to ﬁnd w
with low risk L(w) = E [(cid:96)(w  Z)]. When z = (x  y) and (cid:96)(w  z) = φ((cid:104)w  x(cid:105)  y) this agrees with the supervised
learning risk discussed in the Introduction and analyzed in Section 2. But instead of focusing on the ERM  we run
Mirror Descent on the sample  and then take ˜w = 1
i=1 wi. Standard arguments [8] allow us to convert the online
regret bound of Theorem 2 to a bound on the excess risk:
Corollary 3. For any B ∈ R and L∗  if we run Mirror Descent on the sample with η =
for any w∗ ∈ W with F (w∗) ≤ B2 and L(w∗) ≤ L∗  with expectation over the sample:

H 2B4+HB2nL∗   then

(cid:80)n

HB2+

√

1

n

(cid:115)

L ( ˜wn) − L (w(cid:63)) ≤ 4HB2

n

+ 2

HB2L∗

n

.

It is instructive to contrast this guarantee with similar looking guarantees derived recently in the stochastic convex
optimization literature [14]. There  the model is stochastic ﬁrst-order optimization  i.e.
the learner gets to see an
unbiased estimate ∇l(w  zi) of the gradient of L(w). The variance of the estimate is assumed to be bounded by σ2.
√
The expected accuracy after n gradient evaluations then has two terms: a “accelerated” term that is O(H/n2) and a
slow O(σ/
n) term. While this result is applicable more generally (since it doesn’t require non-negativity of (cid:96))  it is
not immediately clear if our guarantees can be derived using it. The main difﬁculty is that σ depends on the norm of
the gradient estimates. Thus  it cannot be bounded in advance even if we know that L(w(cid:63)) is small. That said  it is

5

intuitively clear that towards the end of the optimization process  the gradient norms will typically be small if L(w(cid:63))
is small because of the self bounding property (Lemma 3.1).
It is interesting to note that using stability arguments  a guarantee very similar to Corollary 3  avoiding the polyloga-
rithmic factors of Theorem 1 as well as the dependence on the bound on the loss  can be obtained also for a “batch”
learning rule similar to ERM  but incorporating regularization. For given regularization parameter λ > 0 deﬁne the
regularized empirical loss as ˆLλ(w) := ˆL(w) + λF (w) and consider the Regularized Empirical Risk Minimizer

ˆwλ = arg min
w∈W

ˆLλ(w)

(6)

The following theorem provides a bound on excess risk similar to Corollary 3:
Theorem 4. For any B ∈ R and L∗ if we set λ = 128H
n2 + 128HL∗
and L(w(cid:63)) ≤ L∗  we have that in expectation over sample of size n:

n +

1282H 2

nB2

then for all w(cid:63) ∈ W with F (w(cid:63)) ≤ B2

(cid:113)

(cid:115)

L ( ˆwλ) − L (w(cid:63)) ≤ 256HB2

n

+

2048HB2L∗

n

.

To prove Theorem 4 we use stability arguments similar to the ones used by Shalev-Shwartz et al [24]  which are in turn
based on Bousquet and Elisseeff [7]. However  while Shalev-Shwartz et al [24] use the notion of uniform stability 
here it is necessary to look at stability in expectation to get the faster rates.

4 Tightness

In this Section we return to the learning rates for the ERM for parametric and for scale-sensitive hypothesis classes
(i.e. in terms of the dimensionality and in terms of scale sensitive complexity measures)  discussed in the Introduction
and analyzed in Section 2. We compare the guarantees on the learning rates in different situations  identify differences
between the parametric and scale-sensitive cases and between the smooth and non-smooth cases  and argue that these
differences are real by showing that the corresponding guarantees are tight. Although we discuss the tightness of the
learning guarantees for ERM in the stochastic setting  similar arguments can also be made for online learning.
Table 1 summarizes the bounds on the excess risk of the ERM implied by Theorem 1 as well previous bounds for Lips-
chitz loss on ﬁnite-dimensional [22] and scale-sensitive [2] classes  and a bound for squared-loss on ﬁnite-dimensional
classes [9  Theorem 11.7] that can be generalized to any smooth strongly convex loss. We shall now show that the

Loss function is:
D-Lipschitz

H-smooth

Parametric

dim(H) ≤ d  

(cid:113) dDL∗
(cid:113) dHL∗

n

Scale-Sensitive

|h| ≤ 1 Rn(H) ≤(cid:112)R/n
(cid:113) D2R
(cid:113) HRL∗
(cid:113) HRL∗

n

n

n

dD
n +
dH
n +
H
λ

H-smooth and λ-strongly Convex

HR
n +
HR
n +
Table 1: Bounds on the excess risk  up to polylogarithmic factors.

dH
n

n

√

n dependencies in Table 1 are unavoidable. To do so  we will consider the class H = {x (cid:55)→ (cid:104)w  x(cid:105) : (cid:107)w(cid:107) ≤ 1} of
1/
(cid:96)2-bounded linear predictors (all norms in this Section are Euclidean)  with different loss functions  and various spe-

ciﬁc distributions over X ×Y  where X =(cid:8)x ∈ Rd : (cid:107)x(cid:107) ≤ 1(cid:9) and Y = [0  1]. For the non-parametric lower-bounds 

we will allow the dimensionality d to grow with the sample size n.
Inﬁnite dimensional  Lipschitz (non-smooth)  separable
Consider the absolute difference loss φ(h(x)  y) = |h(x) − y|  take d = 2n and consider the following distribution: X
(cid:80)n
n ri  where r1  . . .   rd ∈ {±1} is
is uniformly distributed over the d standard basis vectors ei and if X = ei  then Y = 1√
i=1 riei  (cid:107)w(cid:63)(cid:107) = 1 and L∗ = L (w(cid:63)) = 0.
an arbitrary sequence of signs unknown to the learner. Taking w(cid:63) = 1√
However any sample (x1  y1)  . . .   (xn  yn) reveals at most n of 2n signs ri  and no information on the remaining
signs. This means that for any learning algorithm  there exists a choice of ri’s such that on at least n of the remaining
points not seen by the learner  he/she has to suffer a loss of at least 1/

√
n  yielding an overall risk of at least 1/

4n.

√

n

6

1
d

1
d

(cid:107)w − w(cid:63)(cid:107)2
√

of the expected risk is w(cid:63) =(cid:80)d

d

2

√

Inﬁnite dimensional  smooth  non-separable  even if strongly convex
Consider the squared loss φ(h(x)  y) = (h(x) − y)2 which is 2-smooth and 2-strongly convex. For any σ ≥ 0 let
n/σ and consider the following distribution: X is uniform over ei as before  but this time Y |X is random  with
d =
Y |(X = ei) ∼ N ( ri
√
  σ)  where again ri are pre-determined  unknown to the learner  random signs. The minimizer

ei  with (cid:107)w(cid:63)(cid:107) = 1
L (w) − L (w(cid:63)) = E [(cid:104)w − w(cid:63)  x(cid:105)]2 =

√
ri
d
2

i=1

d(cid:88)
2 and L∗ = L(w(cid:63)) = σ2. Furthermore  for any w ∈ W 

(w[i] − w(cid:63)[i])2 =

√
If the norm constraint becomes tight  i.e. (cid:107) ˆw(cid:107) = 1  then L( ˆw)− L(w(cid:63)) ≥ 1/(4d) = σ/(4
n). Oth-
erwise  each coordinate is a separate mean estimation problem  with ni samples  where ni is the number of appearances

of ei in the sample. We have E(cid:2)( ˆw[i] − w(cid:63)[i])2(cid:3) = σ2/ni and so L( ˆw)−L∗ = 1

≥(cid:113) L∗

d (cid:107) ˆw − w(cid:63)(cid:107)2 = 1

(cid:80)d

L∗/(4

n) =

√

i=1

i=1

n

d

σ2
ni

Finite dimensional  smooth  not strongly convex  non-separable:
Take d = 1  with X = 1 with probability q and X = 0 with probability 1 − q. Conditioned on X = 0 let Y = 0
qn and Y = −1 with
deterministically and while conditioned on X = 1 let Y = +1 with probability p = 1
probability 1 − p. Consider the following 1-smooth loss :
(h(x) − y)2
if |h(x) − y| ≤ 1/2
|h(x) − y| − 1/4 if |h(x) − y| ≥ 1/2

2 + 0.2√

First  irrespective of choice of w  when x = 0  we always have h(x) = 0 and so suffer no loss. This happens with
probability 1 − q. Next observe that for p > 0.5  the optimal predictor is w(cid:63) ≥ 1/2. However  for n > 20  with

probability at least 0.25 (cid:80)n
However for p > 0.5 and n > 20  L∗ > q/2 and so with probability 0.25  L( ˆw) − L∗ >(cid:112)0.32L∗/n.

i=1 yi < 0  and so ˆw ≤ −1/2. Hence  L( ˆw) − L∗ > L(−1/2) − L(1/2) =(cid:112)0.16 q/n.

φ(h(x)  y) =

(cid:40)

5

Implications

5.1

Improved Margin Bounds

“Margin bounds” provide a bound on the expected zero-one loss of a classiﬁers based on the margin 0/1 error on
the training sample. Koltchinskii and Panchenko [13] provides margin bounds for a generic class H based on the
Rademacher complexity of the class. This is done by using a non-smooth Lipschitz “ramp” loss that upper bounds the
zero-one loss and is upper-bounded by the margin zero-one loss. However  such an analysis unavoidably leads to a
1/

n rate even in the separable case. Following the same idea we use the following smooth “ramp”:

√

 1

0

φ(t) =

1+cos(πt/γ)

2

t ≤ 0
t ≥ γ

0 < t < γ

This loss function is π2
4γ2 -smooth and is lower bounded by the zero-one loss and upper bounded by the γ mar-
gin loss. Using Theorem 1 we can now provide improved margin bounds for the zero-one loss of any classiﬁer

based on empirical margin error. Denote err(h) = E(cid:2) 11{h(x)(cid:54)=y}(cid:3) the zero-one risk and for any γ > 0 and sample
(x1  y1)  . . .   (xn  yn) ∈ X × {±1} deﬁne the γ-margin empirical zero one loss as(cid:99)errγ(h) := 1
(cid:19)

i=1 11{yih(xi)<γ}.
Theorem 5. For any hypothesis class H  with |h| ≤ b  and any δ > 0  with probability at least 1 − δ  simultaneously
for all margins γ > 0 and all h ∈ H:

(cid:80)n

(cid:113) log(log( 4b

log(log( 4b

(cid:18)

(cid:19)

n

log1.5 n

γ Rn(H) +

γ )/δ)

n

+ log3 n

γ2 R2

γ )/δ)

n

err(h) ≤ (cid:99)errγ(h) + K

(cid:18)(cid:113)(cid:99)errγ(h)

where K is a numeric constant from Theorem 1.

In particular  for appropriate numeric constant K :

err(h) ≤ 1.01(cid:99)errγ(h) + K

(cid:32)

2 log3 n

γ2 R2

n(H) +

2 log(log( 4b

γ )/δ)

n

n(H) +
(cid:33)

Improved margin bounds of the above form have been previously shown speciﬁcally for linear prediction in a Hilbert
space based on the PAC Bayes theorem [19  15]. However PAC-Bayes based results are speciﬁc to certain linear
function class. Theorem 5  in contrast  is a generic concentration-based result that can be applied to any function class.

7

Interaction of Norm and Dimension

5.2
Consider the problem of learning a low-norm linear predictor with respect to the squared loss φ(t  z) = (t − z)2 
where X ∈ Rd  for ﬁnite but very large d  and where the expected norm of X is low. Speciﬁcally  let X be Gaussian
with E(cid:107)X(cid:107)2 = B  Y = (cid:104)w∗  X(cid:105) + N (0  σ2) with (cid:107)w∗(cid:107) = 1  and consider learning a linear predictor using (cid:96)2
regularization. What determines the sample complexity? How does the error decrease as the sample size increases?
From a scale-sensitive statistical learning perspective  we expect that the sample complexity  and the decrease of the
error  should depend on the norm B  especially if d (cid:29) B2. However  for any ﬁxed d and B  even if d (cid:29) B2 
asymptotically as the number of samples increase  the excess risk of norm-constrained or norm-regularized regression
actually behaves as L( ˆw) − L∗ ≈ d
n σ2  and depends (to ﬁrst order) only on the dimensionality d and not on B [17].
√
The asymptotic dependence on the dimensionality alone can be understood through Table 1. In this non-separable
situation  parametric complexity controls can lead to a 1/n rate  ultimately dominating the 1/
n rate resulting from
L∗ > 0 when considering the scale-sensitive  non-parametric complexity control B. Combining Theorem 4 with the
asymptotic d
n σ2 behavior  and noting that at the worst case we can predict using a zero vector  yields the following
overall picture on the expected excess risk of ridge regression with an optimally chosen λ:

L( ˆwλ) − L∗ ≤ O(cid:0)min(cid:0)B2  B2/n + Bσ/

n  dσ2/n(cid:1)(cid:1)

√

Roughly speaking  each term above describes the behavior in a different regime of the sample size. The ﬁrst regime
has excess risk of order B2 which occurs until n = Θ(B2). The second (“low-noise”) regime is one where the excess
risk is dominated by the norm and behaves as B2/n  until n = Θ(B2/σ2) and L( ˆw) = Θ(L∗). The third (“slow”)
regime  where the excess risk is controlled by the norm and the approximation error and behaves as Bσ/
n  until
n = Θ(d2σ2/B2) and L( ˆw) = L∗ + Θ(B2/d). The fourth (“asymptotic”) regime is where excess risk behaves as
d/n. This sheds further light on recent work by Liang and Srebro [18] based on exact asymptotics.

√

5.3 Sparse Prediction

The use of the (cid:96)1 norm has become popular for learning sparse predictors in high dimensions  as in the LASSO. The
LASSO estimator [28] ˆw is obtained by considering the squared loss φ(z  y) = (z− y)2 and minimizing ˆL(w) subject
to (cid:107)w(cid:107)1 ≤ B. Let us assume there is some (unknown) sparse reference predictor w0 that has low expected loss and
sparsity (number of non-zeros) (cid:107)w0(cid:107)0 = k  and that (cid:107)x(cid:107)∞ ≤ 1  y ≤ 1. In order to choose B and apply Theorem 1 in
this setting  we need to bound (cid:107)w0(cid:107)1. This can be done by  e.g.  assuming that the features x[i] in the support of w0
are mutually uncorrelated. Under such an assumption  we have: (cid:107)w0(cid:107)2
Thus  Theorem 1 along with Rademacher complexity bounds from [11] gives us 

1 ≤ kE(cid:10)w0  x(cid:11)2 ≤ 2k(L(w0) + Ey2) ≤ 4k.

L( ˆw) ≤ L(w0) + ˜O

(cid:16)

k log(d)/n +(cid:112)k L(w0) log(d)/n

(cid:17)

.

(7)

It is possible to relax the no-correlation assumption to a bound on the correlations  as in mutual incoherence  or to other
weaker conditions [25]. But in any case  unlike typical analysis for compressed sensing  where the goal is recovering
w0 itself  here we are only concerned with correlations inside the support of w0. Furthermore  we do not require that
the optimal predictor is sparse or that the model is well speciﬁed: only that there exists a low risk predictor using a
small number of fairly uncorrelated features.
Bounds similar to (7) have been derived using specialized arguments [12  30  5]—here we demonstrate that bounds of
these forms can be obtained under simple conditions  using the generic framework we suggest. It is also interesting to
note that the methods and results of Section 3 can also be applied to this setting. We use the entropy regularizer

(cid:88)
which is non-negative and 1-strongly convex with respect to (cid:107)w(cid:107)1 on W = (cid:8)w ∈ Rd(cid:12)(cid:12)w[i] ≥ 0 (cid:107)w(cid:107)1 ≤ B(cid:9)  with
each feature’s negation). Recalling that(cid:13)(cid:13)w0(cid:13)(cid:13)1 ≤ 2
(cid:16)

F (w) ≤ B2(1 + log d) (we consider here only non-negative weights—in order to allow w[i] < 0 we can include also
k in the entropy regularizer (8)  we have
from Theorem 4 we that L( ˆwλ) ≤ L(w0) + O
where ˆwλ is the regularized
empirical minimizer (6) using the entropy regularizer (8) with λ as in Theorem 4. The advantage here is that using
Theorem 4 instead of Theorem 1 avoids the extra logarithmic factors.

k log(d)/n +(cid:112)k L(w0) log(d)/n

k and using B = 2

(cid:18) x[i]

(cid:19)

i

√

F (w) = B

x[i] log

B2
e

(cid:17)

1/d

(8)

+

√

8

References
[1] N. Alon  S. Ben-David  N. Cesa-Bianchi  and D. Haussler. Scale-sensitive dimensions  uniform convergence  and learnability.

FOCS  0:292–301  1993.

[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. JMLR  3:463–

482  2002.

[3] P.L. Bartlett  O. Bousquet  and S. Mendelson. Local rademacher complexities. Annals of Statistics  33(4):1497–1537  2005.
[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations

Research Letters  31:167–175  2003.

[5] P.J. Bickel  Y. Ritov  and A.B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics 

37(4):1705–1732  2009.

[6] O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning Algorithms.

PhD thesis  Ecole Polytechnique  2002.

[7] Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. J. Mach. Learn. Res.  2:499–526  2002.
[8] N. Cesa-Bianchi  A. Conconi  and C.Gentile. On the generalization ability of on-line learning algorithms. In NIPS  pages

359–366  2002.

[9] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
[10] Christophe Chesneau and Guillaume Lecu. Adapting to unknown smoothness by aggregation of thresholded wavelet estima-

tors. 2006.

[11] S.M. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk bounds  margin bounds  and

regularization. In NIPS  2008.

[12] V. Koltchinskii. Sparsity in penalized empirical risk minimization. Ann. Inst  H. Poincar´e Probab. Statist.  45(1):7–57  2009.
[13] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classi-

ﬁers. Ann. of Stats.  30(1):1–50  2002.

[14] G. Lan. Convex Optimization Under Inexact First-order Information. PhD thesis  Georgia Institute of Technology  2009.
[15] J. Langford and J. Shawe-Taylor. PAC-Bayes & margins. In Advances in Neural Information Processing Systems 15  pages

423–430  2003.

[16] Wee Sun Lee  Peter L. Bartlett  and Robert C. Williamson. The importance of convexity in learning with squared loss. IEEE

Trans. on Information Theory  1998.

[17] P. Liang  F. Bach  G. Bouchard  and M. I. Jordan. Asymptotically optimal regularization in smooth parametric models. In

NIPS  2010.

[18] P. Liang and N. Srebro. On the interaction between norm and dimensionality: Multiple regimes in learning. In ICML  2010.
[19] D. A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In COLT  pages 203–215  2003.
[20] Shahar Mendelson. Rademacher averages and phase transitions in glivenko-cantelli classes. IEEE Trans. On Information

Theory  48(1):251–263  2002.

[21] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. Nauka Publishers  Moscow  1978.
[22] D. Panchenko. Some extensions of an inequality of vapnik and chervonenkis. Electronic Communications in Probability 

7:55–65  2002.

[23] David Pollard. Convergence of Stochastic Processes. Springer-Verlag  1984.
[24] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Stochastic convex optimization. In COLT  2009.
[25] S. Shalev-Shwartz  N. Srebro  and T. Zhang. Trading accuracy for sparsity. Technical report  TTI-C  2009. Available at

ttic.uchicago.edu/∼shai.

[26] S.Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis  Hebrew University of Jerusalem 

2007.

[27] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels. ANNALS OF STATISTICS  35:575 

2007.

[28] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B.  58(1):267–288  1996.
[29] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics  32:135–166  2004.
[30] S. A. van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics  36(2):614–645  2008.
[31] C. Zalinescu. Convex analysis in general vector spaces. World Scientiﬁc Publishing Co. Inc.  River Edge  NJ  2002.
[32] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML  2003.

9

,Ioannis Mitliagkas
Constantine Caramanis
Prateek Jain