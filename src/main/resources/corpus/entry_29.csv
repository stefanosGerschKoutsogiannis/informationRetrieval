2016,A Bio-inspired Redundant Sensing Architecture,Sensing is the process of deriving signals from the environment that allows artificial systems to interact with the physical world. The Shannon theorem specifies the maximum rate at which information can be acquired. However  this upper bound is hard to achieve in many man-made systems. The biological visual systems  on the other hand  have highly efficient signal representation and processing mechanisms that allow precise sensing. In this work  we argue that redundancy is one of the critical characteristics for such superior performance. We show architectural advantages by utilizing redundant sensing  including correction of mismatch error and significant precision enhancement. For a proof-of-concept demonstration  we have designed a heuristic-based analog-to-digital converter - a zero-dimensional quantizer. Through Monte Carlo simulation with the error probabilistic distribution as a priori  the performance approaching the Shannon limit is feasible. In actual measurements without knowing the error distribution  we observe at least 2-bit extra precision. The results may also help explain biological processes including the dominance of binocular vision  the functional roles of the fixational eye movements  and the structural mechanisms allowing hyperacuity.,A Bio-inspired Redundant Sensing Architecture

Anh Tuan Nguyen  Jian Xu and Zhi Yang∗

Department of Biomedical Engineering

University of Minnesota
Minneapolis  MN 55455
∗yang5029@umn.edu

Abstract

Sensing is the process of deriving signals from the environment that allows artiﬁ-
cial systems to interact with the physical world. The Shannon theorem speciﬁes
the maximum rate at which information can be acquired [1]. However  this up-
per bound is hard to achieve in many man-made systems. The biological visual
systems  on the other hand  have highly efﬁcient signal representation and pro-
cessing mechanisms that allow precise sensing. In this work  we argue that re-
dundancy is one of the critical characteristics for such superior performance. We
show architectural advantages by utilizing redundant sensing  including correction
of mismatch error and signiﬁcant precision enhancement. For a proof-of-concept
demonstration  we have designed a heuristic-based analog-to-digital converter - a
zero-dimensional quantizer. Through Monte Carlo simulation with the error prob-
abilistic distribution as a priori  the performance approaching the Shannon limit
is feasible. In actual measurements without knowing the error distribution  we
observe at least 2-bit extra precision. The results may also help explain biological
processes including the dominance of binocular vision  the functional roles of the
ﬁxational eye movements  and the structural mechanisms allowing hyperacuity.

1

Introduction

Visual systems have perfected the art of sensing through billions of years of evolution. As an exam-
ple  with roughly 100 million photoreceptors absorbing light and 1.5 million retinal ganglion cells
transmitting information [2  3  4]  a human can see images in three-dimensional space with great
details and unparalleled resolution. Anatomical studies determine the spatial density of the photore-
ceptors on the retina  which limits the peak foveal angular resolution to 20-30 arcseconds according
to Shannon theory [1  2]. There are also other imperfections due to nonuniform distribution of cells’
shape  size  location  and sensitivity that further constrain the precision. However  experiment data
have shown that human can achieve an angular separation close to 1 arcminute in a two-point acu-
ity test [5]. In certain conditions  it is even possible to detect an angular misalignment of only 2-5
arcseconds [6]  which surpasses the virtually impossible physical barrier. This ability  known as
hyperacuity  has bafﬂed scientists for decades: what kind of mechanism allows human to read an
undistorted image with such a blunt instrument?
Among the approaches to explain this astonishing feat of human vision  redundant sensing is a
promising candidate. It is well-known that redundancy is an important characteristic of many bio-
logical systems  from DNA coding to neural network [7]. Previous studies [8  9] suggest there is
a connection between hyperacuity and binocular vision - the ability to see images using two eyes
with overlapping ﬁeld of vision. Also known as stereopsis  it presents a passive form of redun-
dant sensing. In addition to the obvious advantage of seeing objects in three-dimensional space 
the binocular vision has been proven to increase visual dynamic range  contrast  and signal-to-noise
ratio [10]. It is evident that seeing with two eyes enables us to sense a higher level of information

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: Illustration of n-dimensional quantizers without (ideal) and with mismatch error. (a) Two-
dimensional quantizers for image sensing.
(b) Zero-dimensional quantizers for analog-to-digital
data conversion.

as well as to correct many intrinsic errors and imperfections. Furthermore  the eyes continuously
and involuntarily engage in a complex micro-ﬁxational movement known as microsaccade  which
suggests an active form of redundant sensing [11]. During microsaccade  the image projected on the
retina is shifted across a few photoreceptors in a pseudo-random manner. Empirical studies [12] and
computational models [13] suggest that the redundancy created by these micro-movements allows
efﬁcient sampling of spatial information that can surpass the static diffraction limitation.
Both biological and artiﬁcial systems encounter similar challenges to achieve precise sensing in the
presence of non-ideal imperfections. One of those is mismatch error. At a high resolution  even a
small degree of mismatch error can degrade the performance of many man-made sensors [14  15].
For example  it is not uncommon for a 24-bit analog-to-digital converter (ADC) to have 18-20 bits
effective resolution [16]. Inspired by the human visual system  we explore a new computational
framework to remedy mismatch error based on the principle of redundant sensing. The proposed
mechanism resembles the visual systems’ binocular architecture and is designed to increase the
precision of a zero-dimensional data quantization process. By assuming the error probabilistic dis-
tribution as a priori  we show that precise data conversion approaching the Shannon limit can be
accomplished.
As a proof-of-concept demonstration  we have designed and validated a high-resolution ADC in-
tegrated circuit. The device utilizes a heuristic approach that allows unsupervised estimation and
calibration of mismatch error. Simulation and measurement results have demonstrated the efﬁcacy
of the proposed technique  which can increase the effective resolution by 2-5 bits and linearity by
4-6 times without penalties in chip area and power consumption.

2 Mismatch Error

2.1 Quantization & Shannon Limit

Data quantization is the partition of a continuous n-dimensional vector space into M subspaces 
∆0  ...  ∆M−1  called quantization regions as illustrated in Figure 1. For example  an eye is a two-
dimensional biological quantizer while an ADC is a zero-dimensional artiﬁcial quantizer  where the
partition occurs in a spatial  temporal and scalar domain. Each quantization region is assigned a
representative value  d0  ...  dM−1  which uniquely encodes the quantized information. While the
representative values are well-deﬁned in the abstract domain  the actual partition often depends on
the physical properties of the quantization device and has a limited degree of freedom for adjustment.
An optimal data conversation is achieved with a set of uniformly distributed quantization regions. In
practice  it is difﬁcult to achieve due to the physical constraints in the partition process. For example 
individual pixel cells can deviate from the ideal morphology  location  and sensitivity. These relative
differences  referred to as mismatch error  contribute to the data conversion error.
In this paper  we consider a zero-dimensional (scalar) quantizer  which is the mathematical equiv-
alence of an ADC device. A N-bit quantizer divides the continuous conversion full-range (FR =
[0  2N ]) into 2N quantization regions  ∆0  ...  ∆2N−1  with nominal unity length E(|∆i|) = ∆ = 1

2

Figure 2: (a) Degeneration of entropy  i.e. maximum effective resolution  due to mismatch er-
ror versus quantizer’s intrinsic resolution. (b) The proportion of data conversion error measured
by mismatch-to-quantization ratio (MQR). With a conventional architecture  mismatch error is the
dominant source  especially in a high-resolution domain. The proposed method allows suppressing
mismatch error below quantization noise and approaching the Shannon limit.

least-signiﬁcant-bit (LSB). The quantization regions are deﬁned by a set of discrete references1 
SR = {θ0  ...  θ2N}  where 0 = θ0 < θ1 < ... < θ2N = 2N . An input signal x is assigned the
digital code d(x) = i ∈ SD = {0  1  2  ...  2N − 1}  if it falls into region ∆i deﬁned by

x ← d(x) = i ⇔ x ∈ ∆i ⇔ θi ≤ x < θi+1.

(1)
The Shannon entropy of a N-bit quantizer [17  18] quantiﬁes the maximum amount of information
that can be acquired by the data conversion process
√
H = − log2
(cid:90) 2N
2N−1(cid:88)

where M is the normalized total mean square error integrated over each digital code

[x − d(x) − 1/2]2dx

(cid:90) θi+1

12 · M  

(2)

(3)

(x − i − 1/2)2dx.

M =

1
23N

1
23N

=

0

i=0

θi

In this work  we consider both quantization noise and mismatch error. The Shannon limit is generally
preferred as the maximum rate at which information can be acquired without any mismatch error 
where θi = i ∀i or SR\{2N} = SD  M is equal to the total quantization noise Q = 2−2N /12 
and the entropy is equal to the quantizer’s intrinsic resolution H = N. The differences between
SR\{2N} and SD are caused by mismatch error and result in the degeneration of entropy. Figure
2(a) shows the entropy  i.e. maximum effective resolution  versus the quantizer’s intrinsic resolution
with ﬁxed mismatch ratios σ0 = 1% and σ0 = 10%. Figure 2(b) describes the proportion of error
contributed by each source  as measured by mismatch-to-quantization ratio (MQR)

MQR =

.

(4)

M − Q

Q

It is evident that at a high resolution  mismatch error is the dominant source causing data conver-
sion error. The Shannon theory implies that mismatch error is the fundamental problem relating to
the physical distribution of the reference set. [19  20] have proposed post-conversion calibration
methods  which are ineffective in removing mismatch error without altering the reference set itself.
A standard workaround solution is using larger components thus better matching characteristics;
however  this incurs penalties concerning cost and power consumption. As a rule of thumb  1-bit
increase in resolution requires a 4-time increase of resources [14]. To further advance the system
performance  a design solution that is robust to mismatch error must be realized.

1θ2N = 2N is a dummy reference to deﬁne the conversion full-range.

3

Figure 3: Simulated distribution of mismatch error in terms of (a) expected absolute error |PE(i)|
(c  d)
and (b) expected differential error PD(i) in a 16-bit quantizer with 10% mismatch ratio.
Optimal mismatch error distribution in the proposed strategy. At the maximum redundancy 16 ·
(15  1)  mismatch error becomes negligible.

2.2 Mismatch Error Model

For artiﬁcial systems  binary coding is popularly used to encode the reference set. It involves parti-
tioning the array of unit cells into a set of binary-weighted components SC  and assembling different
components in SC to form the needed references. The precision of the data conversion is related
to the precise matching of these unit cells  which can be in forms of comparators  capacitors  re-
sistors  or transistors  etc. Due to fabrication variations  undesirable parasitics  and environmental
interference  each unit cell follows a probabilistic distribution which is the basis of mismatch error.
We consider the situation where the distribution of mismatch error is known as a priori. Each unit
cell  cu  is assumed to be normally distributed with mismatch ratio σ0: cu ∼ N(1  σ2
0). SC is then a
collection of the binary-weighted components ci  each has 2i independent and identically distributed
unit cells

SC = {ci|ci ∼ N(2i  2iσ2

0)} 

∀i ∈ [0  N − 1].

(5)

Each reference θi is associated with a unique assembly Xi of the components2

SR\{2N} = {θi =

|Xi ∈ P(SC)} 

∀i ∈ [0  2N − 1] 

(6)

(cid:80)

(cid:80)N−1

ck∈Xi

ck

1

2N−1

j=0 cj

where P(SC) is the power set of SC. Binary coding allows the shortest data length to encode the
references: N control signals are required to generate 2N elements of SR. However  because each
reference is bijectively associated with an assembly of components  it is not possible to rectify the
mismatch error due to the random distribution of the components’ weight without physically altering
the components themselves.
The error density function deﬁned as PE(i) = θi − i quantiﬁes the mismatch error at each digital
code. Figure 3(a) shows the distribution of |PE(i)| at 10% mismatch ratio through Monte Carlo
2The dummy reference θ2N = 2N is exempted. Other references are normalized over the total weight to

deﬁne the conversion full-range of FR = [0  2N ]

4

Figure 4: Associating and exchanging the information between individual pixels in the same ﬁeld of
vision generate an exponential number of combinations and allow efﬁcient spatial data acquisition
beyond physical constraints. Inspired by this process  we propose a redundant sensing strategy that
involves blending components between two imperfect sets to gain extra precision.

simulations  where there is noticeably larger error associating with middle-range codes. In fact  it
can be shown that if unit cells are independent  identically distributed  PE(i) approximates a normal
distribution as follows

PE(i) = θi − i ∼ N(0 

2j−1

i ∈ [0  2N − 1] 

(7)

N−1(cid:88)

j=0

(cid:12)(cid:12)(cid:12)(cid:12)Dj −

(cid:12)(cid:12)(cid:12)(cid:12) σ2

0) 

i

2N − 1

where i = DN−1...D1D0 (Dj ∈ {0  1} ∀j) is the binary representation of i.
Another drawback of binary coding is that it can create differential “gap” between the references.
Figure 3(b) presents the estimated distribution of differential gap PD(i) = θi+1 − θi at 10% mis-
match ratio. When the gap exceeds two unit-length  signals that should be mapped to two or multiple
codes collapse into a single code  resulting in a loss of information. This phenomenon is commonly
known as wide code  an unrecoverable situation by any post-conversion calibration methods. Also 
wide gaps tend to appear at two adjacent codes that have large Hamming distance  e.g. 01111 and
10000. Subsequently  the amount of information loss can be signal dependent and ampliﬁed at
certain parts of data conversation range.

3 Proposed Strategy

The proposed general strategy is to incorporate redundancy into the quantization process such that
one reference θi can be generated by a large number of distinct component assemblies Xi  each
yields a different amount of mismatch. Among numerous options that lead to the same goal  the
optimal reference set is the collection of assemblies with the least mismatch error over every digital
code.
Furthermore  we propose that such redundant characteristic can be achieved by resembling the visual
systems’ binocular structure. It involves a secondary component set that has overlapping weights
with the primary component set. By exchanging the components with similar weights between the
two sets  excessive redundant component assemblies can be realized. We hypothesize that a simi-
lar mechanism may have been employed in the brain that allows associating information between
individual pixels on the same ﬁeld of vision in each eye as illustrated in Figure 4. Because such
association creates an exponential number of combinations  even a small percentage of 100 million
photoreceptors and 1.5 million retinal ganglion cells that are “interchangeable” could result in a
signiﬁcant degree of redundancy.
The design of the primary and secondary component set  SC 0 and SC 1  speciﬁes the level and
distribution of redundancy. Speciﬁcally  SC 1 is derived by subtracting from the conventional binary-
weighted set SC  while the remainders form the primary component set SC 0. The total nominal
ci j∈(SC 0∪SC 1) ci j = 2N0 − 1  where N0 is the resolution of the

weight remains unchanged as(cid:80)

5

Figure 5: The distribution of the number of assemblies NA(i) with different geometrical identity
in (a) 2-component-set design and (b) 3-component-set design. Higher assembly count  i.e.  larger
level of redundancy  is allocated for digital codes with larger mismatch error.

(cid:26)2i 

quantizer as well as the primary component set. It is worth mentioning that mismatch error is mostly
contributed by the most-signiﬁcant-bit (MSB) rather than the least-signiﬁcant-bit (LSB) as implied
by Equation (5). Subsequently  to optimize the level and distribution of redundancy  the secondary
set should advantageously consist of binary-weighted components that are derived from the MSB.
SC 0 and SC 1 can be described as follows

Primary: SC 0 = {c0 i|c0 i =

if i < N0 − N1
otherwise
Secondary: SC 1 = {c1 i|c1 i = 2N0−N1+i−s1  ∀i ∈ [0  N1 − 1]} 

2i − c1 i−N0+N1 

 ∀i ∈ [0  N0 − 1]} 

(8)

where N1 is the resolution of SC 1 and s1 is a scaling factor satisfying 1 ≤ N1 ≤ N0 − 1 and
1 ≤ s1 ≤ N0 − N1. Different values of N1 and s1 result in different degree and distribution
of redundancy. Any design within this framework can be represented by its unique geometrical
identity: N0 · (N1  s1). The total number of components assemblies is |P(SC 0 ∪ SC 1)| = 2N0+N1 
which is much greater than the cardinality of the reference-set |SR| = 2N0  thus implies the high
level of intrinsic redundancy.
NA(i) is deﬁned as the number of assemblies that represent the same reference θi and is an essential
indicator that speciﬁes the redundancy distribution

NA(i) = |{X|X ∈ P(SC 0 ∪ SC 1) ∧ (cid:88)

cj k = i}| 

i ∈ [0  2N0 − 1].

(9)

cj k∈X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)i − (cid:88)

cj k∈X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  

Figure 5(a) shows NA(i) versus digital codes with N0 = 8 and multiple combinations of
(N1  s1). The design of SC 1 should generate more options for middle-range codes  which suf-
fer from larger mismatch error. Simulations suggest N1 decides the total number of assemblies 
NA(i) = |P(SC 0 ∪ SC 1)| = 2N0+N1; s1 deﬁnes the morphology of the redundancy dis-

(cid:80)2N0−1

tribution. A larger value of s1 gives a more spreading distribution.
Removing mismatch error is equivalent to searching for the optimal component assembly Xop i that
generates the reference θi with the least amount of mismatch

i=0

Xop i =

argmin

X∈P(SC 0∪SC 1)

cj k

i ∈ [0  2N0 − 1].

(10)

The optimal reference set SR op is then the collection of all references generated by Xop i. In this
work  we do not attempt to ﬁnd Xop i as it is an NP-optimization problem with the complexity of
O(2N0+N1) that may not have a solution in the polynomial space.
Instead  this section focuses
on showing the achievable precision with the proposed architecture while section 4 will describe a
heuristic approach. The simulation results in Figure 2(b) demonstrate our technique can suppress

6

mismatch error below quantization noise  thus approaching the Shannon limit even at high resolution
In this simulation  the secondary set is chosen as N1 = N0 − 1 for
and large mismatch ratio.
maximum redundancy. Figure 3(c  d) shows the distribution of mismatch error after correction.
Even at the minimum redundancy (N1 = 1)  a signiﬁcant degree of mismatch is rectiﬁed. At the
maximum redundancy (N1 = N0 − 1)  the mismatch error becomes negligible compared with
quantization noise.
Based on the same principles  a n-set components design (n = 3  4  ...) can be realized  which gives
an increased level redundancy and more complex distribution as shown in Figure 5(b)  where n = 3
and the geometrical identity is N0 · (N1  s1) · (N2  s2). With different combinations of Nk and
sk (k = 1  2  ...)  NA(i) can be catered to a known mismatch error distribution and yield a better
performance. However  adding more component set(s) can increase the computational burden as
the complexity increases rapidly with every additional set(s): O(2N0+N1+N2+...). Given mismatch
error can be well rectiﬁed with a two-set implementation over a wide range of resolution  n > 2
might be unnecessary.
Similarly  three or more eyes may give better vision. However  the brain circuits and control network
would become much more complicated to integrate signals and information. In fact  stereopsis is an
advanced feature to human and animals with well-developed neural capacity [7]. Despite possessing
two eyes  many reptiles  ﬁshes and other mammals  have their eyes located on the opposite sides of
the head  which limits the overlapping region thus stereopsis  in exchange for a wider ﬁeld of vision.
Certain species of insect such as Arachnids can possess from six to eight eyes. However  studies have
pointed out that their eyes do not function in synchronous to resolve the ﬁne resolution details [21].
It is not a coincidence that at least 30% of the human brain cortex is directly or indirectly involved
in processing visual data [7]. We conjecture that the computational limitation is a major reason that
many higher-order animals are evolved to have two eyes  thus keep the cyclops and triclops remain
in the realm of mythology. No less as it would sacriﬁce visual processing precision  yet no more as
it would overload the brain’s circuit complexity.

4 Practical Implementation & Results

A mixed-signal ADC integrated circuit has been designed and fabricated to demonstrate the feasi-
bility of the proposed architecture. The nature of hardware implementation limits the deployment
of sophisticated learning algorithms. Instead  the circuit relies on a heuristic approach to efﬁciently
estimate the mismatch error and adaptively reconﬁgure its components in an unsupervised manner.
The detailed hardware algorithm and circuits implementation are presented seperately. In this paper 
we only brieﬂy summarize the techniques and results.
The ADC design is based on successive-approximation register (SAR) architecture and features
redundant sensing with a geometrical identity 14 · (13  1). The component set SC is a binary-
weighted capacitor array. We have chosen the smallest capacitance available in the CMOS process to
implement the unit cell for reducing circuits power and area. However  it introduces large capacitor
mismatch ratios up to 5% which limits the effective resolution to 10-bit or below for previous works
reported in the literature [14  19  20].
The resolution of the secondary array is chosen as N1 = N0 − 1 to maximize the exchange capacity
between two component sets

i ∈ [1  N − 2].

c0 i = c1 i−1 = 1/2c0 i+1 

(11)
In the auto-calibration mode  the mismatch error of each component is estimated by comparing the
capacitors with similar nominal values implied by Equation (11). The procedure is unsupervised
and fully automatic. The result is a reduced dimensional set of parameters that characterize the
distribution of mismatch error. In the data conversion mode  a heuristic algorithm is employed that
utilizes the estimated parameters to generate the component assembly with near-minimal mismatch
error for each reference. A key technique is to shift the capacitor utilization towards the MSB by
exchanging the components with similar weight  then to compensate the left-over error using the
LSB. Although the algorithm has the complexity of O(N0 + N1)  parallel implementation allows
the computation to ﬁnish within a single clock cycle.
By assuming the LSB components contribute an insigniﬁcant level of mismatch error as implied by
Equation (5)  this heuristic approach trades accuracy for speed. However  the excessive amount of

7

Figure 6: High-resolution ADC implementation. (a) Monte Carlo simulations of the unsupervised
error estimation and calibration technique. (b) The chip micrograph. (c) Differential nonlinearity
(DNL) and (d) integral nonlinearity (INL) measurement results.

redundancy guarantees the convergence of an adequate near-optimal solution. Figure 6(a) shows
simulated plots of effective-number-of-bits (ENOB) versus unit-capacitor mismatch ratio  σ0(Cu).
With the proposed method  the effective resolution is shown to approach the Shannon limit even with
large mismatch ratios. It is worth mentioning that we also take the mismatch error associated with
the bridge-capacitor  σ0(Cb)  into consideration. Figure 6(b) shows the chip micrograph. Figure
6(c  d) gives the measurement results of standard ADC performance merit in terms of differential
nonlinearity (DNL) and integral nonlinearity (INL). The results demonstrate that a 4-6 fold increase
of linearity is feasible.

5 Conclusion

This work presents a redundant sensing architecture inspired by the binocular structure of the hu-
man visual system. We show architectural advantages of using redundant sensing in removing mis-
match error and enhancing sensing precision. A high resolution  zero-dimensional data quantizer
is presented as a proof-of-concept demonstration. Through Monte Carlo simulation with the error
probabilistic distribution as a priori  we ﬁnd the precision can approach the Shannon limit. In actual
measurements without knowing the error probabilistic distribution  the gain of extra 2-bit precision
and 4-6 times linearity is observed. We envision that the framework can be generalized to handle
higher dimensional data and apply to a variety of applications such as digital imaging  functional
magnetic resonance imaging (fMRI)  3D data acquisition  etc. Moreover  engineering such bio-
inspired artiﬁcial systems may help better understand the biological processes such as stereopsis 
microsaccade  and hyperacuity.

Acknowledgment

The authors would like to thank Phan Minh Nguyen for his valuable comments.

8

References
[1] Shannon  C.E. (1948) A Mathematical Theory of Communication. Bell System Technical Journal  vol.

27(3)  pp. 379423.

[2] Curcio  C.A.  Sloan  K.R.  Kalina  R.E.  Hendrickson  A.E. (1990) Human photoreceptor topography.

Journal of Comparative Neurology  vol. 292(4)  pp. 497-523.

[3] Curcio  C. A.  Allen  K. A. (1990) Topography of ganglion cells in human retina. Journal of Comparative

Neurology  vol. 300(1)  pp. 5-25.

[4] Read  J.C. (2015) What is stereoscopic vision good for? Proc. SPIE 9391  Stereoscopic Displays and

Applications XXVI  pp. 93910N.

[5] Westheimer  G. (1977) Spatial frequency and light-spread descriptions of visual acuity and hyperacuity.

Journal of the Optical Society of America  vol. 67(2)  pp. 207-212.

[6] Beck  J.  Schwartz  T. (1979) Vernier acuity with dot test objects. Vision Research  vol. 19(3)  pp. 313-

319.

[7] Reece  J.B.  Urry  L.A  Cain  M.L.  Wasserman  S.A  Minorsky  P.V.  Jackson R.B.  Campbell  N.A.

(2010) Campbell biology  9th Ed. Boston: Benjamin Cummings/Pearson.

[8] Westheimer  G.  McKee  S.P. (1978) Stereoscopic acuity for moving retinal images. Journal of the Optical

Society of America  vol. 68(4)  pp. 450-455.

[9] Crick  F.H.  Marr  D.C.  Poggio  T. (1980) An information processing approach to understanding the

visual cortex. The Organization of the Cerebral Cortex  MIT Press  pp. 505-533.

[10] Cagenello  R.  Arditi  A.  Halpern  D. L. (1993) Binocular enhancement of visual acuity. Journal of the

Optical Society of America A  vol. 10(8)  pp. 1841-1848.

[11] Martinez-Conde  S.  Otero-Millan  J.  Macknik  S.L. (2013) The impact of microsaccades on vision:

towards a uniﬁed theory of saccadic function. Nature Reviews Neuroscience  vol. 14(2)  pp. 83-96.

[12] Hicheur  H.  Zozor  S.  Campagne  A.  Chauvin  A. (2013) Microsaccades are modulated by both atten-
tional demands of a visual discrimination task and background noise. Journal of vision  vol. 13(13)  pp.
18-18.

[13] Hennig  M.H.  W¨org¨otter  F. (2004) Eye micro-movements improve stimulus detection beyond the

Nyquist limit in the peripheral retina. Advances in Neural Information Processing Systems.

[14] Murmann  B. (2008) A/D converter trends: Power dissipation  scaling and digitally assisted architectures.

Custom Integrated Circuits Conference  2008. CICC 2008. IEEE  pp. 105-112.

[15] Nguyen  A.T.  Xu  J.  Yang  Z. (2015) A 14-bit 0.17 mm2 SAR ADC in 0.13µm CMOS for high precision

nerve recording. Custom Integrated Circuits Conference (CICC)  2015 IEEE  pp. 1-4.

[16] Analog Devices (2016) 24-Bit Delta-Sigma ADC with Low Noise PGA. AD1555/1556 datasheet.
[17] Frey  M.  Loeliger.  H.A. (2007) On the static resolution of digitally corrected analog-to-digital and
digital-to-analog converters with low-precision components. Circuits and Systems I: Regular Papers 
IEEE Transactions on  vol. 54(1)  pp. 229-237.

[18] Biveroni  J.  Loeliger  H.A. (2008) On sequential analog-to-digital conversion with low-precision com-

ponents. Information Theory and Applications Workshop  2008. IEEE  pp. 185-187.

[19] Um  J.Y.  Kim  Y.J.  Song  E.W.  Sim  J.Y.  Park  H.J. (2013) A digital-domain calibration of split-
capacitor DAC for a differential SAR ADC without additional analog circuits. Circuits and Systems I:
Regular Papers  IEEE Transactions on  vol. 60(11)  pp. 2845-2856.

[20] Xu  R.  Liu  B.  Yuan  J. (2012) Digitally calibrated 768-kS/s 10-b minimum-size SAR ADC array with

dithering. Solid-State Circuits  IEEE Journal of  vol. 47(9)  pp. 2129-2140.

[21] Land  M.F. (1985) The morphology and optics of spider eyes. Neurobiology of arachnids  pp. 53-78 

Springer Berlin Heidelberg.

9

,Anh Tuan Nguyen
Jian Xu
Zhi Yang