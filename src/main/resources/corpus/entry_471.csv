2019,Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors,Deep neural networks as image priors have been recently introduced for problems
such as denoising  super-resolution and inpainting with promising performance
gains over hand-crafted image priors such as sparsity. Unlike learned generative
priors they do not require any training over large datasets. However  few theoretical guarantees exist in the scope of using untrained network priors for inverse imaging problems. We explore new applications and theory for untrained neural network priors. Specifically  we consider the problem of solving linear inverse problems  such as compressive sensing  as well as non-linear problems  such as compressive phase retrieval. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present a projected gradient descent scheme that can be used for both compressive sensing and phase retrieval and provide rigorous theoretical guarantees for its convergence. We also show both theoretically as well as empirically that with deep neural network priors  one can achieve better compression rates for the same image quality as compared to when hand crafted priors are used.,Algorithmic Guarantees for Inverse Imaging

with Untrained Network Priors

Gauri Jagatap

New York University

gauri.jagatap@nyu.edu

Chinmay Hegde

New York University
chinmay.h@nyu.edu

Abstract

Deep neural networks as image priors have been recently introduced for problems
such as denoising  super-resolution and inpainting with promising performance
gains over hand-crafted image priors such as sparsity. Unlike learned generative
priors they do not require any training over large datasets. However  few theoretical
guarantees exist in the scope of using untrained network priors for inverse imaging
problems. We explore new applications and theory for untrained neural network
priors. Speciﬁcally  we consider the problem of solving linear inverse problems 
such as compressive sensing  as well as non-linear problems  such as compressive
phase retrieval. We model images to lie in the range of an untrained deep generative
network with a ﬁxed seed. We further present a projected gradient descent scheme
that can be used for both compressive sensing and phase retrieval and provide
rigorous theoretical guarantees for its convergence. We also show both theoretically
as well as empirically that with deep neural network priors  one can achieve better
compression rates for the same image quality as compared to when hand crafted
priors are used.

1

Introduction

1.1 Motivation

Deep neural networks have led to unprecedented success in solving several problems  speciﬁcally in
the domain of inverse imaging. Image denoising [1]  super-resolution [2]  inpainting and compressed
sensing [3]  and phase retrieval [4] are among the many imaging applications that have beneﬁted
from the usage of deep convolutional networks (CNNs) trained with thousands of images.
Apart from supervised learning  deep CNN models have also been used in unsupervised setups  such
as Generative Adversarial Networks (GANs). Here  image priors based on a generative model [5] are
learned from training data. In this context  neural networks emulate the probability distribution of the
data inputs. GANs have been used to model signal prior by learning the distribution of training data.
Such learned priors have replaced hand-crafted priors with high success rates [3  6  7  8]. However 
the main challenge with these approaches is the requirement of massive amounts of training data. For
instance  super-resolution CNN [2] uses ImageNet which contains millions of images. Moreover 
convergence guarantees for training such networks are limited [7].
In contrast  there has been recent interest in using untrained neural networks as an image prior. Deep
Image Prior [9] and variants such as Deep Decoder [10] are capable of solving linear inverse imaging
problems with no training data whatsover  while merely imposing an auto-encoder [9] and decoder
[10] architecture as a structural prior. For denoising  inpainting and super-resolution  deep image
priors have shown superior reconstruction performance as compared to conventional methodologies
such as basis pursuit denoising (BPDN) [11]  BM3D [12] as well as convolutional sparse coding
[13]. Similar emperical results have been claimed very recently in the context of time-series data

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

for audio applications [14  15]. The theme in all of these approaches is the same: to design a prior
that exploits local image correlation  instead of global statistics  and ﬁnd a good low-dimensional
neural representation of natural images. However  most of these works have very limited [16  10] or
no theoretical guarantees.
Neural networks priors for compressive imaging has only recently been explored. In the context
of compressive sensing (CS)  [17] uses Deep Image Prior along with learned regularization for
reconstructing images from compressive measurements [18]. However  the model described still relies
on training data for learning appropriate regularization parameters. For the problem of compressive
sensing  priors such as sparsity [19] and structured sparsity [20] have been traditionally used.
Phase retrieval is another inverse imaging problem in several Fourier imaging applications  which
involves reconstructing images from magnitude-only measurements. Compressive phase retrieval
(CPR) models use sparse priors for reducing sample requirements; however  standard techniques from
recent literature [21] suggest a quadratic dependence of number of measurements on the sparsity level
for recovering sparse images from magnitude-only Gaussian measurements and the design of a smart
initialization scheme [22  21]. If a prior is learned via a GAN [7]  [23]  then this requirement can be
brought down; however one requires sufﬁcient training data  which can be prohibitively expensive to
obtain in domains such as medical or astronomical imaging.

1.2 Our contributions

In this paper  we explore  in depth  the use of untrained deep neural networks as an image prior for
inverting images from under-sampled linear and non-linear measurements. Speciﬁcally  we assume
that the image  x∗d×1 has d pixels. We further assume that the image x∗ belongs to the range spanned
by the weights of a deep under-parameterized untrained neural network G(w; z)  which we denote
by S  where w is a set of the weights of the deep network and z is the latent code. The compressive
measurements are stored in vector y = f (x∗)  where f embeds either compressive linear (deﬁned
by operator A(·)) or compressive magnitude-only (deﬁned by operator |A(·)|) measurements. The
task is to reconstruct image ˆx which corresponds to small measurement error minx∈S (cid:107)f (x) − y(cid:107)2
2.
With this setup  we establish theoretical guarantees for successful image reconstruction from both
measurement schemes under untrained network priors.
Our speciﬁc contributions are as follows:
• We ﬁrst present a new variant of the Restricted Isometry Property (RIP) [18] via a covering number
argument for the range of images S spanned by a deep untrained neural network. We use this result
to guarantee unique image reconstruction for two different compressive imaging schemes.

• We propose a projected gradient descent (PGD) algorithm for solving the problem of compressive
sensing with a deep untrained network prior. To our knowledge this is the ﬁrst paper to use deep
neural network priors for compressive sensing 1  which relies on no training data2. We analyze the
conditions under which PGD provably converges and report the sample complexity requirements
corresponding to it. We also show superior performance of this framework via empirical results.
• We are the ﬁrst to use deep network priors in the context of phase retrieval. We introduce a
novel formulation  to solve compressive phase retrieval with fewer measurements as compared
to state-of-art. We further provide preliminary guarantees for the convergence of a projected
gradient descent scheme to solve the problem of compressive phase retrieval. We empirically show
signiﬁcant improvements in image reconstruction quality as compared to prior works.

We note that our sample complexity results rely on the number of parameters of the assumed deep
network prior. Therefore  to get meaningful bounds  our network priors are under-parameterized 
in that the total number of unknown parameters of the deep network is smaller than the dimension
of the image. To ensure this  we build upon the formulation of the deep decoder [10]  which is a
special network architecture resembling the decoder of an autoencoder (or generator of a GAN). The
requirement of under-parameterization of deep network priors is natural; the goal is to design priors
that concisely represent natural images. Moreover  this also ensures that the network does not ﬁt noise
[10]. Due to these merits  we use select the deep decoder architecture for all analyses in this paper.

1We note recent concurrent work in [24] which explores a similar approach for compressive sensing; however

our paper focuses theoretical guarantees rooted in an algorithmic procedure.

2[17] requires training data for learning a regularization function.

2

1.3 Prior work

Sparsifying transforms have long been used to constrain the solutions of inverse imaging problems
in the context of denoising or inpainting. Conventional approaches to solve these problems include
Basis Pursuit Denoising (BPDN) or Lasso [11]  TVAL3 [25]  which rely on using (cid:96)0  (cid:96)1 and total
variation (TV) regularizations on the image to be recovered. Sparsity based priors are highly effective
and dataset independent  however it heavily relies on choosing a good sparsifying basis [26].
Instead of hand-picking the sparsifying transform  in dictionary learning one learns both the sparsify-
ing transform and the sparse code [27]. The dictionary captures global statistics of a given dataset 3.
Multi-layer convolutional sparse coding [16] is an extension of sparse coding which models a given
dataset in the form of a product of several linear dictionaries  all of which are convolutional in nature
and this problem is challenging.
Generative adversarial networks (GAN) [5] have been used to generate photo-realistic images in
an unsupervised fashion. The generator consists of stacked convolutions and maps random low-
dimensional noise vectors to full sized images. GAN priors have been successfully used for inverse
imaging problems [6  7  28  29  8]. The shortcomings of this approach are two-fold: test images are
strictly restricted to the range of a trained generator  and the requirement of sufﬁcient training data.
Sparse signal recovery from linear compressive measurements [18] as well as magnitude-only
compressive measurements [21] has been extensively studied  with several algorithmic approaches
[19  21]. In all of these approaches  modeling the low-dimensional embedding is challenging and
may not be captured correctly using simple hand-crafted priors such as structured sparsity [20]. Since
it is hard to estimate these hyper-parameters accurately  the number of samples required to reconstruct
the image is often much higher than information theoretic limits [30  6].
The problem of compressive phase retrieval speciﬁcally  is even more challenging because it is non-
convex. Several papers in recent literature [31  32  21] rely on the design of a spectral initialization
scheme which ensures that one can subsequently optimize over a convex ball of the problem. However
this initialization requirement results in high sample requirements and is a bottleneck in achieving
information theoretically optimal sample complexity.
Deep image prior [9] (DIP) uses primarily an encoder-decoder as a prior on the image  alongside
an early stopping condition  for inverse imaging problems such as denoising  super-resolution and
inpainting. Deep decoder [10] (DD) improves upon DIP  providing a much simpler  underparameter-
ized architecture  to learn a low-dimensional manifold (latent code) and a decoding operation from
this latent code to the full image. Because it is under parameterized  deep decoder does not ﬁt noise 
and therefore does not require early stopping.
Deep network priors in the context of compressive imaging have only recently been explored [17] 
and only in the context of compressive sensing. In contrast with [17] which extends the idea of a Deep
Image Prior to incorporate learned regularizations  in this paper we focus more on theoretical aspects
of the problem and also explore applications in compressive phase retrieval. To our knowledge the
application of deep network priors to compressive phase retrieval is novel.

2 Notation

Throughout the paper  lower case letters denote vectors  such as v and upper case letters for matrices 
such as M. A set of variables subscripted with different indices is represented with bold-faced
shorthand of the following form: w := {W1  W2  . . . WL}. The neural network consists of L layers 
each layer denoted as Wl  with l ∈ {1  . . . L} and are 1 × 1 convolutional. Up-sampling operators
are denoted by Ul. Vectorization of a matrix is written as vec(·). The activation function considered
is Rectiﬁed Linear Unit (ReLU)  denoted as σ(·). Hadamard or element-wise product is denoted by ◦.
Element-wise absolute valued vector is denoted by |v|. Unless mentioned otherwise  (cid:107)v(cid:107) denotes
vector (cid:96)2-norm and (cid:107)M(cid:107) denotes spectral norm (cid:107)M(cid:107)2.

3Local structural information from a single image can also be used to learn dictionaries  by constructing

several overlapping crops or patches of a single image.

3

3 Problem setup

3.1 Deep neural network priors
In this paper we discuss the problem of inverting a mapping x → y of the form:

y = f (x)

where x = vec(X)dk is a d-dimensional signal X d×k (vectorized image)  with k channels and
f : x → y ∈ Rn captures a compressive measurement procedure  such as a linear operator A(·) or
magnitude only measurements |A(·)| and n < dk. We elaborate further on the exact structure of f in
the next subsection (Section 3.2). The task of reconstructing image x from measurements y can be
formulated as an optimization problem of the form:

(cid:107)y − f (x)(cid:107)2

(1)

min
x∈S

where we have chosen the (cid:96)2-squared loss function and where S captures the prior on the image.
If the image x can be represented as the action of a deep generative network G(w; z) with weights w
on some latent code z  such that x = G(w; z)  then the set S captures the characteristics of G(w; z).
The latent code z := vec(Z1) with Z1 ∈ Rd1×k1 is a low-dimensional embedding with dimension
d1k1 (cid:28) dk and its elements are generated from uniform random distribution.
When the network G(·) and its weights w := {W1  . . . WL} are known (from pre-training a generative
network over large datasets) and ﬁxed  the task is to obtain an estimate ˆx = G(w; ˆz)  which indirectly
translates to ﬁnding the optimal latent space encoding ˆz . This problem has been studied in [6  7] in
the form of using learned GAN priors for inverse imaging.
In this paper however  the weights of the generator w are not pre-trained; rather  the task is to
estimate image ˆx = G( ˆw; z) ≈ G(w∗; z) = x∗ and corresponding weights ˆw  for a ﬁxed seed z 
where x∗ is assumed to be the true image and the true weights w∗ (possibly non-unique) satisfy
w∗ = minw (cid:107)x∗ − G(w; z)(cid:107)2
2. Note that the optimization in Eq. 1 is equivalent to substituting the
surjective mapping G : w → x  and optimizing over w 

(cid:107)y − f (G(w; z))(cid:107)2 

min

w

(2)

and estimate weights ˆw and corresponding image ˆx.
Speciﬁcally  the untrained network G(w; z) takes the form of an expansive neural network; a decoder
architecture similar to the one in [10] 4. The neural network is composed of L weight layers Wl 
indexed by l ∈ {1  . . .   L} and are 1× 1 convolutions  upsampling operators Ul for l ∈ {1  . . . L− 1}
and ReLU activation σ(·) and is expressed as follows

x = G(w; z) = UL−1σ(ZL−1WL−1)WL = ZLWL 

(3)

where σ(·) represents the action of ReLU operation  Z di×ki
z = vec(Z1)  dL = d and WL ∈ RkL×k.
To capture the range of images spanned by the deep neural network architecture described above  we
formally introduce the main assumption in our paper through Deﬁnition 1. Without loss in generality 
we set k = 1 for the rest of this paper  while noting that the techniques carry over to general k.
Deﬁnition 1. A given image x ∈ Rd is said to obey an untrained neural network prior if it belongs
to a set S deﬁned as:

= Ui−1σ(Zi−1Wi−1)  for i = 2  . . . L 

i

S := {x|x = G(w; z)}

where z is a (randomly chosen  ﬁxed) latent code vector and G(w; z) has the form in Eq. 3.

3.2 Observation models and assumptions

We now discuss the compressive measurement setup in more detail. Compressive measurement
schemes were developed in [18] for efﬁcient imaging and storage of images and work only as long as
certain structural assumptions on the signal (or image) are met. The optimization problem in Eq.1 is

4Alternatively  one may assume the architecture of the generator of a DCGAN [33  17].

4

non-convex in general  partly dictated by the non-convexity of set S. Moreover  in the case of phase
retrieval  the loss function is itself non-convex. Therefore unique signal recovery for either problems
is not guaranteed without making speciﬁc assumptions on the measurement setup.
In this paper  we assume that the measurement operation can be represented by the action of a Gaussian
matrix A which is rank-deﬁcient (n < d). The entries of this matrix are such that Aij ∼ N (0  1/n).
Linear compressive measurements take the form y = Ax and magnitude-only measurements take the
form y = |Ax|. We formally discuss the two different imaging schemes in the next two sections. We
also present algorithms and theoretical guarantees for their convergence. For both algorithms  we
require that a special (S  γ  β)-RIP holds for measurement matrix A  which is deﬁned below.
Deﬁnition 2. (S  γ  β)-RIP: Set-Restricted Isometry Property with parameters γ  β:
For parameters γ  β > 0  a matrix A ∈ Rn×d satisﬁes (S  γ  β)-RIP  if for all x ∈ S 

γ(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ β(cid:107)x(cid:107)2.

We refer to the left (lower) inequality as (S  γ)-RIP and right (upper) inequality as (S  β)-RIP.
The (S  1 − α  1 + α) RIP is achieved by Gaussian matrix A under certain assumptions  which we
state and prove via Lemma 1 as follows.
Lemma 1. If an image x ∈ Rd has a decoder prior (captured in set S)  where the decoder consists
of weights w and piece-wise linear activation (ReLU)  a random Gaussian matrix A ∈ Rn×d with
elements from N (0  1/n)  satisﬁes (S  1 − α  1 + α)-RIP  with probability 1 − e−cα2n  as long as

(cid:19)

n = O

k1
α2

kl log d

  for small constant c and 0 < α < 1.

(cid:18)

L(cid:80)

l=2

Proof sketch: We use a union of sub-spaces model  similar to that developed in [6] which was
developed for GAN priors  to capture the range of a deep untrained network.
Our method uses a linearization principle. If the output sign of any ReLU activation σ(·) on its
inputs were known a priori  then the mapping x = G(w; z) becomes a product of linear weight
matrices and linear upsampling operators acting on the latent code z. The bulk of the proof relies on
constructing a counting argument for the number of such linearized networks; call that number N.
For a ﬁxed linear subspace  the image x has a representation of the form x = U Zw  where U absorbs
all upsampling operations  Z is latent code which is ﬁxed and known and w is the direct product of
all weight matrices with w ∈ Rk1. An oblivious subspace embedding (OSE) of x takes the form

(1 − α)(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ (1 + α)(cid:107)x(cid:107)2 

where A is a Gaussian matrix  and holds for all k1-dimensional vectors w  with high probability as
long as n = O(k1/α2). We further require to take a union bound over all possible such linearized
networks  which is given by N. The sample complexity corresponding to this bound is then computed
to complete the set-restricted RIP result. The complete proof can be found in Appendix D and a
discussion on the sample complexity is presented in Appendix B.

4 Linear compressive sensing with deep network prior

(cid:107)y − Ax(cid:107)2

We now analyze linear compressed Gaussian measurements of a vectorized image x  with a deep
network prior. The reconstruction problem assumes the following form:

min

x

s.t. x = G(w; z) 

(4)
where A ∈ Rn×d is Gaussian matrix with n < d  unknown weight matrices w and latent code z which
is ﬁxed. We solve this problem via Algorithm 1  Network Projected Gradient Descent (Net-PGD) for
compressed sensing recovery.
Speciﬁcally  we break down the minimization into two parts; we ﬁrst solve an unconstrained loss
minimization of the objective function in Eq. 4 by implementing one step of gradient descent in Step
3 of Algorithm 1. The update vt typically does not adhere to the deep network prior constraint vt (cid:54)∈ S.
To ensure that this happens  we solve a projection step in Line 4 of Algorithm 1  which happens to be
the same as ﬁtting a deep network prior to a noisy image. We iterate through this procedure in an
alternating fashion until the estimates xt converge to x∗ within error factor .
We further establish convergence guarantees for Algorithm 1 in Theorem 1.

5

Algorithm 1 Net-PGD for compressed sensing recovery.
1: Input: y  A  z = vec(Z1)  η  T = log 1
2: for t = 1 ···   T do

3:
4:
5:
6: end for
7: Output ˆx ← xT .

vt ← xt − ηA(cid:62)(Axt − y)
wt ← arg min
w
xt+1 ← G(wt; z)

(cid:107)vt − G(w; z)(cid:107)

{gradient step for least squares}
{projection to range of deep network}

Theorem 1. Suppose the sampling matrix An×d satisﬁes (S  1− α  1 + α)-RIP with high probability
then  Algorithm 1  with η small enough  produces ˆx such that (cid:107)ˆx − x∗(cid:107) ≤  and requires T ∝ log 1
iterations.



Proof sketch: The proof of this theorem predominantly relies on our new set-restricted RIP result
and uses standard techniques from compressed sensing theory. Indicating the loss function in Eq.
4 as L(xt) = (cid:107)y − Axt(cid:107)2  we aim to establish a contraction of the form L(xt+1) < νL(xt)  with
ν < 1. To achieve this  we combine the projection criterion in Step 4 of Algorithm 1  which strictly
implies that

(cid:107)xt+1 − vt(cid:107) ≤ (cid:107)x∗ − vt(cid:107)

and vt = xt−ηA(cid:62)(Axt−y) from Step 3 of Algorithm 1  where η is chosen appropriately. Therefore 

(cid:107)xt+1 − xt + ηA(cid:62)A(xt − x∗)(cid:107)2 ≤ (cid:107)x∗ − xt + ηA(cid:62)A(xt − x∗)(cid:107)2.

Furthermore  we utilize (S  1 − α  1 + α)-RIP and its Corollary 1 (refer Appendix D) which apply to
x∗  xt  xt+1 ∈ S  to show that

L(xt+1) ≤ νL(xt)

and subsequently the error contraction (cid:107)xt+1 − x∗(cid:107) ≤ νo(cid:107)xt − x∗(cid:107)  with ν  νo < 1 to guarantee
linear convergence of Net-PGD for compressed sensing recovery. This convergence result implies
that Net-PGD requires T ∝ log 1/ iterations to produce ˆx within -accuracy of x∗. The complete
proof of Theorem 1 can be found in Appendix D. In Appendix A we provide some exposition on the
projection step (line 4 of Algorithm 1).

5 Compressive phase retrieval under deep image prior
In compressive phase retrieval  one wants to reconstruct a signal x ≈ x∗ ∈ S from measurements of
the form y = |Ax∗| and therefore the objective is to minimize the following

(cid:107)y − |Ax|(cid:107)2

s.t. x = G(w; z) 

min

x

(5)

where n < d and A is Gaussian  z is a ﬁxed seed and weights w need to be estimated. We propose a
Network Projected Gradient Descent (Net-PGD) for compressive phase retrieval to solve this problem 
which is presented in Algorithm 2.
Algorithm 2 broadly consists of two parts. For the ﬁrst part  in Line 3 we estimate the phase of the
current estimate and in Line 4 we use this to compute the Wirtinger gradient [31] and execute one
step for solving an unconstrained phase retrieval problem with gradient descent. The second part of
the algorithm is (Line 5)  estimating the weights of the deep network prior with noisy input vt. This is
the projection step and ensures that the output wt and subsequently the image estimate xt = G(wt; z)
lies in the range of the decoder G(·) outlined by set S.
We highlight that the problem in Eq. 5 is signiﬁcantly more challenging than the one in Eq. 4.
The difﬁculty hinges on estimating the missing phase information accurately. For a real-valued
vectors  there are 2n different phase vectors p = sign(Ax) for a ﬁxed choice of x  which satisfy
y = |Ax|  moreover the entries of p are restricted to {1 −1}. Hence  phase estimation is a non-
convex problem. Therefore  with Algorithm 2 the problem in Eq.5 can only be solved to convergence
locally; an initialization scheme is required to establish global convergence guarantees. We highlight
the guarantees of Algorithm 2 in Theorem 2.

6

Algorithm 2 Net-PGD for compressive phase retrieval.
1: Input: A  z = vec(Z1)  η  T = log 1
2: for t = 1 ···   T do
pt ← sign(Axt)
3:
vt ← xt − ηA(cid:62)(Axt − y ◦ pt)
4:
wt ← arg min
(cid:107)vt − G(w; z)(cid:107)
5:
w
xt+1 ← G(wt; z)
6:
7: end for
8: Output ˆx ← xT .

   x0 s.t. (cid:107)x0 − x∗(cid:107) ≤ δi(cid:107)x∗(cid:107).

{phase estimation}
{gradient step for phase retrieval}
{projection to range of deep network}

Theorem 2. Suppose the sampling matrix An×d with Gaussian entries satisﬁes (S  1−α  1+α)-RIP
with high probability  Algorithm 2 solves Eq. 5 with η small enough  such that (cid:107)ˆx−x∗(cid:107) ≤   as long as
.
the weights are initialized appropriately and the number of measurements is n = O

L(cid:80)

(cid:19)

(cid:18)

k1

kl log d

l=2

Proof sketch: The proof for Theorem 2 relies on two important results; (S  1 − α  1 + α)-RIP and
Lemma 2 which establishes a bound on the phase estimation error. Formally  the update in Step 4 of
Algorithm 2 can be re-written as

vt+1 = xt − ηA(cid:62)(cid:0)Axt − Ax∗ ◦ sign(Ax∗) ◦ sign(Axt)(cid:1) = xt − ηA(cid:62)(cid:0)Axt − Ax∗(cid:1) − ηεt

p

p := A(cid:62)Ax∗ ◦ (1 − sign(Ax∗) ◦ sign(Axt)) is phase estimation error.

where εt
If sign(Ax∗) ≈ sign(Axt)  then the above resembles the gradient step from the linear compressive
sensing formulation. Thus  if x0 is initialized well  the error due to phase mis-match εt
p can be
bounded  and subsequently  a convergence result can be formulated.
Next  Step 5 of Algorithm 2 learns weights wt that produce xt = G(wt; z)  such that

for t = {1  2  . . . T}. Then  the above projection rule yields:

(cid:107)xt+1 − vt(cid:107) ≤ (cid:107)xt − vt(cid:107)

(cid:107)xt+1 − vt+1 + vt+1 − x∗(cid:107) ≤ (cid:107)xt+1 − vt+1(cid:107) + (cid:107)x∗ − vt+1(cid:107) ≤ 2(cid:107)x∗ − vt+1(cid:107) 

Using the update rule from Eq. 12 and plugging in for vt+1:

(cid:107)xt+1 − x∗(cid:107) ≤ (cid:107)(1 − ηA(cid:62)A)ht(cid:107) + (cid:107)εt
p(cid:107)

1
2

where η is chosen appropriately. The rest of the proof relies on bounding the ﬁrst term via matrix norm
inequalities using Corollary 2 (in Appendix D) of (S  1−α  1+α)-RIP as (cid:107)(1−ηA(cid:62)A)ht(cid:107) ≤ ρo(cid:107)ht(cid:107)
and the second term is bounded via Lemma 2 as (cid:107)εt
p(cid:107) ≤ δo(cid:107)xt − x∗(cid:107) as long as (cid:107)x0 − x∗(cid:107) ≤ δi(cid:107)x∗(cid:107).
Hence we obtain a convergence criterion of the form

(cid:107)xt+1 − x∗(cid:107) ≤ 2(ρo + ηδo)(cid:107)xt − x∗(cid:107) := ρ(cid:107)xt − x∗(cid:107).
where ρ < 1. Note that this proof relies on a bound on the phase error (cid:107)εt
p(cid:107) which is established
via Lemma 2. The complete proof for Theorem 2 can be found in Appendix D. In Appendix A we
provide some exposition on the projection step (line 5 of Algorithm 2). In our experiments (Section
6) we note that a uniform random initialization of the weights w0 (which is common in training
neural networks)  to yield x0 = G(w0; z) is sufﬁcient for Net-PGD to succeed for compressive phase
retrieval. In Appendix C we show experimental evidence to support this claim.

6 Experimental results

Dataset: We use images from the MNIST database and CelebA database to test our algorithms and
reconstruct 6 grayscale (MNIST  28 × 28 pixels (d = 784)) and 5 RGB (CelebA) images. The
CelebA dataset images are center cropped to size 64 × 64 × 3 (d = 12288). The pixel values of all
images are scaled to lie between 0 and 1.

7

Original Compressed Net-GD Net-PGD Lasso

TVAL3

Original Compressed Net-GD Net-PGD Lasso

Net-GD
Lasso

Net-PGD
TVAL3

1.5

1

E
S
M
n

0.5

0

0.1

0.2

0.15

0.25
compression ratio f

0.3

(a)

(b)

(c)

Figure 1: (CS) Reconstructed images from linear measurements (at compression rate n/d = 0.1)
with (a) n = 78 measurements for examples from MNIST  (b) n = 1228 measurements for examples
from CelebA  and (c) nMSE at different compression rates f = n/d for MNIST.

Original Compressed Net-GD Net-PGD Sparta

n/d = 0.5

Net-GD
Net-PGD

Sparta

n/d = 0.1

0.5

E
S
M
n

0

0

1

2

compression ratio f

3

(a)

(c)

(c)

Figure 2: (CPR) Reconstructed images from magnitude-only measurements (a) at compression rate
of n/d = 0.3 for MNIST  (b) at compression rates of n/d = 0.1  0.5 for CelebA with (row 1 3)
Net-GD and (row 2 4) Net-PGD  (c) nMSE at different compression rates f = n/d for MNIST.

Deep network architecture: We ﬁrst optimize the deep network architecture which ﬁt our example
images such that x∗ ≈ G(w∗; z) (referred as “compressed” image). For MNIST images  the
architecture was ﬁxed to a 2 layer conﬁguration k1 = 15  k2 = 15  k3 = 10  and for CelebA images 
a 3 layer conﬁguration with k1 = 120  k2 = 15  k3 = 15  k4 = 10. Both architectures use bilinear
upsampling operations. Further details on this setup can be found in Appendix C.
Measurement setup: We use a Gaussian measurement matrix of size n × d with n varied such that
(i) n/d = 0.08  0.1  0.15  0.2  0.25  0.3 for compressive sensing and (ii) n/d = 0.1  0.2  0.3  0.5  1  3
for compressive phase retrieval. The elements of A are picked such that Ai j ∼ N (0  1/n) and we
report averaged reconstruction error values over 10 different instantiations of A for a ﬁxed image
(image of digit ‘0’ from MNIST)  network conﬁguration and compression ratio n/d .

6.1 Compressive sensing

Algorithms and baselines: We implement 4 schemes based on untrained priors for solving CS  (i)
gradient descent with deep network prior which solves Eq.2 (we call this Net-GD)  similar to [17] but
without learned regularization (ii) Net-PGD  (iii) Lasso ((cid:96)1 regularization) with sparse prior in DCT
basis and ﬁnally (iv) TVAL3 [25] (Total Variation regularization). The TVAL3 code only works for
grayscale images  therefore we do not use it for CelebA examples. The reconstructions are shown in
Figure 1 for images from (a) MNIST and (b) CelebA datasets. The implementation details can be
found in Appendix C.

8

Performance metrics: We compare reconstruction quality using normalized Mean-Squared Error
(nMSE)  which is calculated as (cid:107)ˆx − x∗(cid:107)2/(cid:107)x∗(cid:107)2. We plot the variation of the nMSE with different
compression rates f = n/d for all the algorithms tested averaged over all trials for MNIST in Figure
1 (c). We note that both Net-GD and Net-PGD produce superior reconstructions as compared to state
of art. Running time performance is reported in Appendix C.

6.2 Compressive phase retrieval

Algorithms and baselines: We implement 3 schemes based on untrained priors for solving CPR   (i)
Net-GD (ii) Net-PGD and ﬁnally (iii) Sparse Truncated Amplitude Flow (Sparta) [22]  with sparse
prior in DCT basis for both datasets. The reconstructions are shown in Figure 2 for (a) MNIST and
(b) CelebA datasets. We plot nMSE at varying compression rates for all algorithms averaged over all
trials for MNIST in Figure 2(c) and note that both Net-GD and Net-PGD outperform Sparta. Running
term performance as well as goodness of random initialization scheme are discussed in Appendix C.

7 Acknowledgments

This work was supported in part by NSF grants CAREER CCF-2005804  CCF-1815101  and a faculty
fellowship from the Black and Veatch Foundation.

9

References
[1] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P. Manzagol. Stacked denoising autoen-
coders: Learning useful representations in a deep network with a local denoising criterion.
Journal of machine learning research  11(Dec):3371–3408  2010.

[2] C. Dong  C. Loy  K. He  and X. Tang. Image super-resolution using deep convolutional networks.

IEEE transactions on pattern analysis and machine intelligence  38(2):295–307  2016.

[3] J. Chang  C. Li  B. Póczos  and B. Kumar. One network to solve them all—solving linear
inverse problems using deep projection models. In 2017 IEEE International Conference on
Computer Vision (ICCV)  pages 5889–5898. IEEE  2017.

[4] C. Metzler  P. Schniter  A. Veeraraghavan  and R. Baraniuk. prdeep: Robust phase retrieval with
a ﬂexible deep network. In International Conference on Machine Learning  pages 3498–3507 
2018.

[5] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems 
pages 2672–2680  2014.

[6] A. Bora  A. Jalal  E. Price  and A. Dimakis. Compressed sensing using generative models.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
537–546. JMLR. org  2017.

[7] P. Hand  O. Leong  and V. Voroninski. Phase retrieval under a generative prior. In Advances in

Neural Information Processing Systems  pages 9136–9146  2018.

[8] T. Lillicrap Y. Wu  M. Rosca. Deep compressed sensing. arXiv preprint arXiv:1905.06723 

2019.

[9] D. Ulyanov  A. Vedaldi  and V. Lempitsky. Deep image prior. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition  pages 9446–9454  2018.

[10] R. Heckel and P. Hand. Deep decoder: Concise image representations from untrained non-

convolutional networks. In International Conference on Learning Representations  2018.

[11] S. Chen  D. Donoho  and M. Saunders. Atomic decomposition by basis pursuit. SIAM review 

43(1):129–159  2001.

[12] K. Dabov  A. Foi  V. Katkovnik  and K. Egiazarian. Image denoising with block-matching and
3d ﬁltering. In Image Processing: Algorithms and Systems  Neural Networks  and Machine
Learning  volume 6064  page 606414. International Society for Optics and Photonics  2006.

[13] V. Papyan  Y. Romano  J. Sulam  and M. Elad. Convolutional dictionary learning via local
processing. In Proceedings of the IEEE International Conference on Computer Vision  pages
5296–5304  2017.

[14] A. Dimakis S. Ravula. One-dimensional deep image prior for time series inverse problems.

arXiv preprint arXiv:1904.08594  2019.

[15] L. Wolf M. Michelashvili. Audio denoising with deep network priors. arXiv preprint arXiv:

arXiv:1904.07612  2019.

[16] J. Sulam  V. Papyan  Y. Romano  and M. Elad. Multilayer convolutional sparse modeling:
Pursuit and dictionary learning. IEEE Transactions on Signal Processing  66(15):4090–4104 
2018.

[17] D. Van Veen  A. Jalal  E. Price  S. Vishwanath  and A. Dimakis. Compressed sensing with deep

image prior and learned regularization. arXiv preprint arXiv:1806.06438  2018.

[18] D. Donoho. Compressed sensing. IEEE Transactions on information theory  52(4):1289–1306 

2006.

10

[19] D. Needell and J. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate

samples. Applied and computational harmonic analysis  26(3):301–321  2009.

[20] R. Baraniuk  V. Cevher  M. Duarte  and C. Hegde. Model-based compressive sensing. IEEE

Transactions on Information Theory  56:1982–2001  2010.

[21] G. Jagatap and C. Hegde. Fast  sample-efﬁcient algorithms for structured phase retrieval. In

Advances in Neural Information Processing Systems  pages 4917–4927  2017.

[22] G. Wang  L. Zhang  G. Giannakis  M. Akçakaya  and J. Chen. Sparse phase retrieval via

truncated amplitude ﬂow. IEEE Transactions on Signal Processing  66(2):479–491  2017.

[23] F. Shamshad  F. Abbas  and A. Ahmed. Deep ptych: Subsampled fourier ptychography using
generative priors. In IEEE International Conference on Acoustics  Speech and Signal Processing
(ICASSP)  pages 7720–7724. IEEE  2019.

[24] R. Heckel. Regularizing linear inverse problems with convolutional neural networks. arXiv

preprint arXiv:1907.03100  2019.

[25] C. Li  W. Yin  and Y. Zhang. User’s guide for tval3: Tv minimization by augmented lagrangian

and alternating direction algorithms.

[26] S. Mallat. A wavelet tour of signal processing. Elsevier  1999.

[27] M. Aharon  M. Elad  and A. Bruckstein. K-svd: An algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Transactions on signal processing  54(11):4311 
2006.

[28] R. Hyder  V. Shah  C. Hegde  and S. Asif. Alternating phase projected gradient descent with
generative priors for solving compressive phase retrieval. arXiv preprint arXiv:1903.02707 
2019.

[29] V. Shah and C. Hegde. Solving linear inverse problems using gan priors: An algorithm with
provable guarantees. In 2018 IEEE International Conference on Acoustics  Speech and Signal
Processing (ICASSP)  pages 4609–4613. IEEE  2018.

[30] G. Jagatap and C. Hegde. Sample-efﬁcient algorithms for recovering structured signals from

magnitude-only measurements. IEEE Transactions on Information Theory  2019.

[31] Y. Chen and E. Candes. Solving random quadratic systems of equations is nearly as easy as
solving linear systems. In Advances in Neural Information Processing Systems  pages 739–747 
2015.

[32] T. Cai  X. Li  and Z. Ma. Optimal rates of convergence for noisy sparse phase retrieval via

thresholded wirtinger ﬂow. The Annals of Statistics  44(5):2221–2251  2016.

[33] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolu-

tional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[34] S. Oymak and M. Soltanolkotabi. Towards moderate overparameterization: global convergence

guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674  2019.

[35] S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations 
2018.

[36] H. Zhang and Y. Liang. Reshaped wirtinger ﬂow for solving quadratic system of equations. In

Advances in Neural Information Processing Systems  pages 2622–2630  2016.

[37] Huishuai Zhang and Yingbin Liang. Reshaped wirtinger ﬂow for solving quadratic system of

equations. In Advances in Neural Information Processing Systems  pages 2622–2630  2016.

[38] Tamás S. Improved approximation algorithms for large matrices via random projections. 2006
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06)  pages 143–152 
2006.

11

,Gauri Jagatap
Chinmay Hegde