2011,Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC,Multi-structure model fitting has traditionally taken a two-stage approach: First  sample a (large) number of model hypotheses  then select the subset of hypotheses that optimise a joint fitting and model selection criterion. This disjoint two-stage approach is arguably suboptimal and inefficient - if the random sampling did not retrieve a good set of hypotheses  the optimised outcome will not represent a good fit. To overcome this weakness we propose a new multi-structure fitting approach based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our method is an adaptive hypothesis generator  whose proposal distribution is learned incrementally and online. We prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in MCMC. Our method effectively conducts hypothesis sampling and optimisation simultaneously  and gives superior computational efficiency over other methods.,Simultaneous Sampling and Multi-Structure Fitting

with Adaptive Reversible Jump MCMC

Trung Thanh Pham  Tat-Jun Chin  Jin Yu and David Suter

School of Computer Science  The University of Adelaide  South Australia
{trung tjchin jin.yu dsuter}@cs.adelaide.edu.au

Abstract

Multi-structure model ﬁtting has traditionally taken a two-stage approach: First 
sample a (large) number of model hypotheses  then select the subset of hypotheses
that optimise a joint ﬁtting and model selection criterion. This disjoint two-stage
approach is arguably suboptimal and inefﬁcient — if the random sampling did not
retrieve a good set of hypotheses  the optimised outcome will not represent a good
ﬁt. To overcome this weakness we propose a new multi-structure ﬁtting approach
based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our
method is an adaptive hypothesis generator  whose proposal distribution is learned
incrementally and online. We prove that this adaptive proposal satisﬁes the dimin-
ishing adaptation property crucial for ensuring ergodicity in MCMC. Our method
effectively conducts hypothesis sampling and optimisation simultaneously  and
yields superior computational efﬁciency over previous two-stage methods.

1

Introduction

Multi-structure model ﬁtting is concerned with estimating the multiple instances (or structures) of
a geometric model embedded in the input data. The task manifests in applications such as mixture
regression [21]  motion segmentation [27  10]  and multi-projective estimation [29]. Such a prob-
lem is known for its “chicken-and-egg” nature: Both data-to-structure assignments and structure
parameters are unavailable  but given the solution of one subproblem  the solution of the other can
be easily derived. In practical settings the number of structures is usually unknown beforehand  thus
model selection is required in conjunction to ﬁtting. This makes the problem very challenging.
A common framework is to optimise a robust goodness-of-ﬁt function jointly with a model selection
criterion. For tractability most methods [25  19  17  26  18  7  31] take a “hypothesise-then-select”
approach: First  randomly sample from the parameter space a large number of putative model hy-
potheses  then select a subset of the hypotheses (structures) that optimise the combined objective
function. The hypotheses are typically ﬁtted on minimal subsets [9] of the input data. Depending on
the speciﬁc deﬁnition of the cost functions  a myriad of strategies have been proposed to select the
best structures  namely tabu search [25]  branch-and-bound [26]  linear programming [19]  dirichlet
mixture clustering [17]  message passing [18]  graph cut [7]  and quadratic programming [31].
While sampling is crucial for tractability  a disjoint two-stage approach raises an awkward situa-
tion: If the sampled hypotheses are inaccurate  or worse  if not all valid structures are sampled  the
selection or optimisation step will be affected. The concern is palpable especially for higher-order
geometric models (e.g.  fundamental matrices in motion segmentation [27]) where enormous sam-
pling effort is required before hitting good hypotheses (those ﬁtted on all-inlier minimal subsets).
Thus two-stage approaches are highly vulnerable to sampling inadequacies  even with theoretical
assurances on the optimisation step (e.g.  globally optimal over the sampled hypotheses [19  7  31]).
The issue above can be viewed as the lack of a stopping criterion for the sampling stage. If there
is only one structure  we can easily evaluate the sample quality (e.g.  consensus size) on-the-ﬂy

1

and stop as soon as the prospect of obtaining a better sample becomes insigniﬁcant [9]. Under
multi-structure data  it is unknown what a suitable stopping criterion is (apart from solving the
overall ﬁtting and model selection problem itself). One can consider iterative local reﬁnement of the
structures or re-sampling after data assignment [7]  but the fact remains that if the initial hypotheses
are inaccurate  the results of the subsequent ﬁtting and reﬁnement will be affected.
Clearly  an approach that simultaneously samples and optimises is more appropriate. To this end
we propose a new method for multi-structure ﬁtting and model selection based on Reversible Jump
Markov Chain Monte Carlo (RJMCMC) [12]. By design MCMC techniques directly optimise via
sampling. Despite their popular use [3] the method has not been fully explored in multi-structure
ﬁtting (a few authors have applied Monte Carlo techniques for robust estimation [28  8]  but mostly
to enhance hypothesis sampling on single-structure data). We show how to exploit the reversible
jump mechanism to provide a simple and effective framework for multi-structure model selection.
The bane of MCMC  however  is the difﬁculty in designing efﬁcient proposal distributions. Adaptive
MCMC techniques [4  24] promise to alleviate this difﬁculty by learning the proposal distribution
on-the-ﬂy. Instrumental in raising the efﬁciency of our RJMCMC approach is a recently proposed
hypothesis generator [6] that progressively updates the proposal distribution using generated hy-
potheses. Care must be taken in introducing such adaptive schemes  since a chain propagated based
on a non-stationary proposal is non-Markovian  and unless the proposal satisﬁes certain proper-
ties [4  24]  this generally means a loss of asymptotic convergence to the target distribution.
Clearing these technical hurdles is one of our major contributions: Using emerging theory from
adaptive MCMC [23  4  24  11]  we prove that the adaptive proposal  despite its origins in robust es-
timation [6]  satisﬁes the properties required for convergence  most notably diminishing adaptation.
The rest of the paper is organised as follows: Sec. 2 formulates our goal within a clear optimisation
framework  and outlines our RJMCMC approach. Sec. 3 describes the adaptive hypothesis proposal
used in our method  and develops proof that it is a valid adaptive MCMC sampler. We present our
experimental results in Sec. 4 and draw conclusions in Sec. 5.

2 Multi-Structure Fitting and Model Selection
Give input data X = {xi}N
i=1  usually with outliers  our goal is to recover the instances or structures
θk = {θc}k
c=1 of a geometric model M embedded in X. The number of valid structures k is
unknown beforehand and must also be estimated from the data. The problem domain is therefore
the joint space of structure quantity and parameters {k  θk}. Such a problem is typically solved by
jointly minimising ﬁtting error and model complexity. Similar to [25  19  26]  we use the AIC [1]

{k∗  θ∗

k∗} = arg min
{k θk}

−2 log L(θk) + 2αn(θk).

Here L(θk) is the robust data likelihood and n(θk) the number of parameters to deﬁne θk. We
include a positive constant α to allow reweighting of the two components. Assuming i.i.d. Gaussian
noise with known variance σ  the above problem is equivalent to minimising the function

(cid:18) minc ric

(cid:19)

1.96σ

N(cid:88)

i=1

f (k  θk) =

ρ

+ αn(θk) 

(1)

where ric = g(xi  θc) is the absolute residual of xi to the c-th structure θc in θk. The residuals
are subjected to a robust loss function ρ(·) to limit the inﬂuence of outliers; we use the biweight
function [16]. Minimising a function like (1) over a vast domain {k  θk} is a formidable task.

2.1 A reversible jump simulated annealing approach

Simulated annealing has proven to be effective for difﬁcult model selection problems [2  5]. The
idea is to propagate a Markov chain for the Boltzmann distribution encapsulating (1)

bT (k  θk) ∝ exp(−f (k  θk)/T )

(2)
where temperature T is progressively lowered until the samples from bT (k  θk) converge to the
global minima of f (k  θk). Algorithm 1 shows the main body of the algorithm. Under weak regu-
larity assumptions  there exist cooling schedules [5] that will guarantee that as T tends to zero the
samples from the chain will concentrate around the global minima.

2

To simulate bT (k  θk) we adopt a mixture of kernels MCMC approach [2]. This involves in each
iteration the execution of a randomly chosen type of move to update {k  θk}. Algorithm 2 sum-
marises the idea. We make available 3 types of moves: birth  death and local update. Birth and
death moves change the number of structures k. These moves effectively cause the chain to jump
across parameter spaces θk of different dimensions. It is crucial that these trans-dimensional jumps
are reversible to produce correct limiting behaviour of the chain. The following subsections explain.

Algorithm 1 Simulated annealing for multi-structure ﬁtting and model selection
1: Initialise temperature T and state {k  θk}.
2: Simulate Markov chain for bT (k  θk) until convergence.
3: Lower temperature T and repeat from Step 2 until T ≈ 0.
Algorithm 2 Reversible jump mixture of kernels MCMC to simulate bT (k  θk)
Require: Last visited state {k  θk} of previous chain  probability β (Sec. 4 describes setting β).
1: Sample a ∼ U[0 1].
2: if a ≤ β then
3: With probability rB(k)  attempt birth move  else attempt death move.
4: else
5:
6: end if
7: Repeat from Step 1 until convergence (e.g.  last V moves all rejected).

Attempt local update.

2.1.1 Birth and death moves
The birth move propagates {k  θk} to {k(cid:48)  θ(cid:48)
k(cid:48)}  with k(cid:48) = k + 1. Applying Green’s [12  22] seminal
theorems on RJMCMC  the move is reversible if it is accepted with probability min{1  A}  where

bT (k(cid:48)  θ(cid:48)

k(cid:48))[1 − rB(k(cid:48))]/k(cid:48)

A =

bT (k  θk)rB(k)q(u)

∂(θk  u)

(cid:12)(cid:12)(cid:12)(cid:12) ∂θ(cid:48)

k(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12) .

(3)

The probability of proposing the birth move is rB(k)  where rB(k) = 1 for k = 1  rB(k) = 0.5
for k = 2  . . .   kmax − 1  and rB(kmax) = 0. In other words  any move that attempts to move k
beyond the range [1  kmax] is disallowed in Step 3 of Algorithm 2. The death move is proposed with
probability 1 − rB(k). An existing structure is chosen randomly and deleted from θk. The death
move is accepted with probability min{1  A−1}  with obvious changes to the notations in A−1.
In the birth move  the extra degrees of freedom required to specify the new item in θ(cid:48)
k(cid:48) are given
by auxiliary variables u  which are in turn proposed by q(u). Following [18  7  31]  we estimate
parameters of the new item by ﬁtting the geometric model M onto a minimal subset of the data. Thus
u is a minimal subset of X. The size p of u is the minimum number of data required to instantiate
M  e.g.  p = 4 for planar homographies  and p = 7 or 8 for fundamental matrices [15]. Our
approach is equivalently minimising (1) over collections {k  θk} of minimal subsets of X  where
now θk ≡ {uc}k
Considering only minimal subsets somewhat simpliﬁes the problem  but there are still a colossal
number of possible minimal subsets. Obtaining good overall performance thus hinges on the ability
of proposal q(u) to propose minimal subsets that are relevant  i.e.  those ﬁtted purely on inliers of
valid structures in the data. One way is to learn q(u) incrementally using generated hypotheses. We
describe such a scheme [6] in Sec. 3 and prove that the adaptive proposal preserves ergodicity.

c=1. Taking this view the Jacobian ∂θ(cid:48)

k(cid:48)/∂(θk  u) is simply the identity matrix.

2.1.2 Local update

A local update does not change the model complexity k. The move involves randomly choosing a
structure θc in θk to update  making only local adjustments to its minimal subset uc. The outcome
is a revised minimal subset u(cid:48)

c  and the move is accepted with probability min{1  A}  where

A =

bT (k  θ(cid:48)
bT (k  θk)q(u(cid:48)

k)q(uc|θ(cid:48)
c)
c|θc)

.

(4)

As shown in the above our local update is also accomplished with the adaptive proposal q(u|θ)  but
this time conditioned on the selected structure θc. Sec. 3 describes and anlyses q(u|θ).

3

3 Adaptive MCMC for Multi-Structure Fitting

Our work capitalises on the hypothesis generation scheme of Chin et al. called Multi-GS [6] origi-
nally proposed for robust geometric ﬁtting. The algorithm maintains a series of sampling weights
which are revised incrementally as new hypotheses are generated. This bears similarity to the pio-
neering Adaptive Metropolis (AM) method of Haario et al. [13]. Here  we prove that our adaptive
proposals q(u) and q(u|θ) based on Multi-GS satisfy conditions required to preserve ergodicity.

3.1 The Multi-GS algorithm
Let {θm}M
m=1 aggregate the set of hypotheses ﬁtted on the minimal subsets proposed thus far in all
birth and local update moves in Algorithm 1. To build the sampling weights  ﬁrst for each xi ∈ X
we compute its absolute residuals as measured to the M hypotheses  yielding the residual vector

We then ﬁnd the permutation

r(i) := [ r(i)

1 r(i)

2

a(i) := [ a(i)

1 a(i)

2

··· r(i)
M ].
··· a(i)
M ]

that sorts the elements in r(i) in non-descending order. The permutation a(i) essentially ranks the
M hypotheses according to the preference of xi; The higher a hypothesis is ranked the more likely
xi is an inlier to it. The weight wi j between the pair xi and xj is obtained as

wi j = Ih(xi  xj) :=

(5)
where |a(i)
h | is the number of identical elements shared by the ﬁrst-h elements of a(i) and a(j).
Clearly wi j is symmetric with respect to the input pair xi and xj  and wi i = 1 for all i. To ensure
technical consistency in our later proofs  we add a small positive offset γ to the weight1  or

h ∩ a(j)

wi j = max(Ih(xi  xj)  γ) 

(6)
hence γ ≤ wi j ≤ 1. The weight wi j measures the correlation of the top h preferences of xi and
xj  and this value is typically high iff xi and xj are inliers from the same structure; Figs. 1 (c)–(g)
illustrate. Parameter h controls the discriminative power of wi j  and is typically set as a ﬁxed ratio k
of M  i.e.  h = (cid:100)kM(cid:101). Experiments suggest that k = 0.1 provides generally good performance [6].
Multi-GS exploits the preference correlations to sample the next minimal subset u = {xst}p
t=1 
where xst ∈ X and st ∈ {1  . . .   N} indexes the particular datum from X; henceforth we regard
u ≡ {st}p
t=1. The ﬁrst datum s1 is chosen purely randomly. Beginning from t = 2  the selection of
the t-th member st considers the weights related to the data s1  . . .   st−1 already present in u. More
speciﬁcally  the index st is sampled according to the probabilities

(cid:12)(cid:12)(cid:12)a(i)

1
h

h ∩ a(j)

h

(cid:12)(cid:12)(cid:12)  

Pt(i) ∝ t−1(cid:89)

wsz i 

for i = 1  . . .   N 

(7)

z=1

i.e.  if Pt(i) > Pt(j) then i is more likely than j to be chosen as st. A new hypothesis θM +1 is then
ﬁtted on u and the weights are updated in consideration of θM +1. Experiments comparing sampling
efﬁciency (e.g.  all-inlier minimal subsets produced per unit time) show that Multi-GS is superior
over previous guided sampling schemes  especially on multi-structure data; See [6] for details.

3.2

Is Multi-GS a valid adaptive MCMC proposal?

Our RJMCMC scheme in Algorithm 2 depends on the Multi-GS-inspired adaptive proposals qM (u)
and qM (u|θ)  where we now add the subscript M to make explicit their dependency on the set of
aggregated hypotheses {θm}M
i j=1 they induce. The probability of
proposing a minimal subset u = {st}p

t=1 from qM (u) can be calculated as

m=1 as well as the weights {wi j}N
d(cid:75)

(cid:34)p−1(cid:89)

(cid:89)

(cid:35)−1

qM (u) =

wsa sb

1T

wse

 

(8)

1
N

a<b
b≤p

d=1

e=1

1It can be shown if both xi and xj are uniformly distributed outliers  the expected value of wi j is h/M 

i.e.  a given pair xi and xj will likely have non-zero preference correlation.

4

where wi is the column vector [ wi 1 . . . wi N ]T and(cid:74) is the sequential Hadamard product over

the given multiplicands. The term with the inverse in Eq. (8) relates to the normalising constants for
Eq. (7). As an example  the probability of selecting the minimal subset u = {s1  s2  s3  s4} is

qM (u) =

1
N

ws1 s2ws1 s3 ws2 s3ws1 s4 ws2 s4ws3 s4

1T ws11T (ws1 (cid:12) ws2 )1T (ws1 (cid:12) ws2 (cid:12) ws3)
(cid:18)
−O(g(xi  θ))

(cid:19)

.

The local update proposal qM (u|θ) differs only in the manner in which the ﬁrst datum xs1 is se-
lected. Instead of chosen purely randomly  the ﬁrst index s1 is sampled according to

Ps1(i) ∝ exp

 

n

for i = 1  . . .   N 

(9)
where O(g(xi  θ)) is the order statistic of the absolute residual g(xi  θ) as measured to θ; to deﬁne
qM (u|θ) the 1/N term in Eq. (8) is simply replaced with the appropriate probability from Eq. (9).
For local updates an index i is more likely to be chosen as s1 if xi is close to θ. Parameter n relates
to our prior belief of the minimum number of inliers per structure; we ﬁx this to n = 0.1N.
Since our proposal distributions are updated with the arrival of new hypotheses  the corresponding
transition probabilities are inhomogeneous (they change with time) and the chain non-Markovian
(the transition to a future state depends on all previous states). We aim to show that such contin-
ual adaptations with Multi-GS will still lead to the correct target distribution (2). First we restate
Theorem 1 in [11] which is distilled from other work on Adaptive MCMC [23  4  24].
Theorem 1. Let Z = {Zn : n > 0} be a stochastic process on a compact state space Ξ evolving
according to a collection of transition kernels

Tn(z  z(cid:48)) = pr(Zn+1|Zn = z  Zn−1 = zn−1  . . .   Z0 = z0) 

and let p(z) be the distribution of Zn. Suppose for every n and z0  . . .   zn−1 ∈ Ξ and for some
distribution π(z) on Ξ 

π(zn)Tn(zn  zn+1) = π(zn+1) 

|Tn+k(z  z(cid:48)) − Tn(z  z(cid:48))| ≤ anck  an = O(n−r1)  ck = O(k−r2)  r1  r2 > 0 

Tn(z  z(cid:48)) ≥ π(z(cid:48))   > 0 

where  does not depend on n  z0  . . .   zn−1. Then  for any initial distribution p(z0) for Z0 

|p(zn) − π(zn)| → 0 for n → ∞.

sup
zn

(10)

(11)
(12)

(cid:88)

zn

Diminishing adaptation. Eq. (11) dictates that the transition kernel  and thus the proposal distri-
bution in the Metropolis-Hastings updates in Eqs. (3) and (4)  must converge to a ﬁxed distribution 
i.e.  the adaptation must diminish. To see that this occurs naturally in qM (u)  ﬁrst we show that wi j
for all i  j converges as M increases. Without loss of generality assume that b new hypotheses are
generated between successive weight updates wi j and w(cid:48)
kM ∩ a(j)
kM|

kM| ± b(k + 1)

(cid:48)(i)
k(M +b) ∩ a

i j. Then 

kM ∩ a(j)

(cid:48)(j)
k(M +b)|

(cid:12)(cid:12)(cid:12)w

lim

M→∞

(cid:48)
i j − wi j

k(M + b)

kM

k(M + b)

− |a(i)

− |a(i)

kM ∩ a(j)
kM|

kM

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) = lim

M→∞

= lim

M→∞

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|a
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|a(i)

kM ∩ a(j)

kM|/M ± b(k + 1)/M

k + kb/M

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ lim

M→∞
kM ∩ a(j)
kM|/M

− |a(i)

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)|a(i)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0 

where a(cid:48)(i) is the revised preference of xi in consideration of the b new hypotheses. The result is
based on the fact that the extension of b hypotheses will only perturb the overlap between the top-k
percentile of any two preference vectors by at most b(k + 1) items. It should also be noted that the
result is not due to w(cid:48)

i j and wi j simultaneously vanishing with increasing M; in general

since a(i) and a(j) are extended and revised as M increases and this may increase their mutual
overlap. Figs. 1 (c)–(g) illustrate the convergence of wi j as M increases. Using the above result  it
can be shown that the product of any two weights also converges

(cid:12)(cid:12)w(cid:48)

lim
M→∞

i jw(cid:48)

p q − wi jwp q

i j(w(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12)w(cid:48)

p q − wp q) + wp q(w(cid:48)
p q − wp q

(cid:12)(cid:12) +(cid:12)(cid:12)wp q

i j

i j − wi j)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)w(cid:48)

i j − wi j

(cid:12)(cid:12) = 0.

kM ∩ a(j)

kM|/M (cid:54)= 0

lim

M→∞|a(i)
(cid:12)(cid:12)w(cid:48)
(cid:12)(cid:12) = lim
(cid:12)(cid:12)w(cid:48)

M→∞
≤ lim
M→∞

5

This result is readily extended to the product of any number of weights. To show the convergence
of the normalisation terms in (8)  we ﬁrst observe that the sum of weights is bounded away from 0

∀i 

1T wi ≥ L 

L > 0 

due to the offsetting (6) and the constant element wi i = 1 in wi (although wi i will be set to zero to
enforce sampling without replacement [6]). It can thus be established that

(cid:12)(cid:12)(cid:12)(cid:12) = lim

M→∞

(cid:12)(cid:12)(cid:12)(cid:12) 1T w(cid:48)

(1T w(cid:48)

i − 1T wi
i)(1T wi)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ lim

M→∞

(cid:12)(cid:12)(cid:12)(cid:12) 1T w(cid:48)

i − 1T wi
L2

(cid:12)(cid:12)(cid:12)(cid:12) = 0

(cid:12)(cid:12)(cid:12)(cid:12)

lim
M→∞

1

1T w(cid:48)

i

− 1

1T wi

since the sum of the weights also converges. The result is readily extended to the inverse of the sum
of any number of Hadamard products of weights  since we have also previously established that the
product of any number of weights converges. Finally  since Eq. (8) involves only multiplications of
convergent quantities  qM (u) will converge to a ﬁxed distribution as the update progresses.
Invariance. Eq. (10) requires that transition probabilities based on qM (u) permits an invariant dis-
tribution individually for all M. Since we propose and accept based on the Metropolis-Hastings
algorithm  detailed balance is satisﬁed by construction [3]  which means that a Markov chain prop-
agated based on qM (u) will asymptotically sample from the target distribution.
Uniform ergodicity. Eq. (12) requires that qM (u) for all M be individually ergodic  i.e.  the re-
sulting chain using qM (u) is aperiodic and irreducible. Again  since we simulate the target using
Metropolis-Hastings  every proposal has a chance of being rejected  thus implying aperiodicity [3].
Irreducibility is satisﬁed by the offsetting in (6) and renormalising [20]  since this implies that there
is always a non-zero probability of reaching any state (minimal subset) from the current state.
The above results apply for the local update proposal qM (u|θ) which differs from qM (u) only in the
(stationary) probability to select the ﬁrst index s1. Hence qM (u|θ) is also a valid adaptive proposal.

4 Experiments

We compare our approach (ARJMC) against state-of-the-art methods: message passing [18]
(FLOSS)  energy minimisation with graph cut [7] (ENERGY)  and quadratic programming based on
a novel preference feature [31] (QP-MF). We exclude older methods with known weaknesses  e.g. 
computational inefﬁciency [19  17  26]  low accuracy due to greedy search [25]  or vulnerability to
outliers [17]. All methods are run in MATLAB except ENERGY which is available in C++2.
For ARJMC  standard deviation σ in (1) is set as t/1.96  where t is the inlier threshold [9] obtained
using ground truth model ﬁtting results— The same t is provided to the competitors. In Algorithm 1
temperature T is initialiased as 1 and we apply the geometric cooling schedule Tnext = 0.99T .
In Algorithm 2  probability β is set as equal to current temperature T   thus allowing more global
explorations in the parameter space initially before concentrating on local reﬁnement subsequently.
Such a helpful strategy is not naturally practicable in disjoint two-stage approaches.

4.1 Two-view motion segmentation

The goal is to segment point trajectories X matched across two views into distinct motions [27].
Trajectories of a particular motion can be related by a distinct fundamental matrix F ∈ R3×3 [15].
Our task is thus to estimate the number of motions k and the fundamental matrices {Fc}k
c=1 corre-
sponding to the motions embedded in data X. Note that X may contain false trajectories (outliers).
We estimate fundamental matrix hypotheses from minimal subsets of size p = 8 using the 8-point
method [14]. The residual g(xi  F) is computed as the Sampson distance [15].
We test the methods on publicly available two-view motion segmentation datasets [30]. In particular
we test on the 3- and 4-motion datasets provided  namely breadtoycar  carchipscube  toycubecar 
breadcubechips  biscuitbookbox  cubebreadtoychips and breadcartoychips; see the dataset home-
page for more details. Correspondences were established via SIFT matching and manual ﬁltering
was done to obtain ground truth segmentation. Examples are shown in Figs. 1(a) and 1(b).

2http://vision.csd.uwo.ca/code/#Multi-label optimization

6

(a) breadtoycar dataset with 3 motions (37 
39 and 34 inliers  56 outliers)

(b) cubebreadtoychips dataset with 4 mo-
tions (71  49  38 and 81 inliers  88 outliers)

(c) M = 50

(d) M = 100

(e) M = 1000

(f) M = 5000

(g) M = 10000

(h) Value of function f (k  θk) (best viewed in colour)

(i) Segmentation error (best viewed in colour)

(j) M = 100

(k) M = 200

(l) M = 500

(m) M = 1000

Figure 1: (a) and (b) show respectively a 3- and 4-motion dataset (colours show ground truth la-
belling). To minimise clutter  lines joining false matches are not drawn. (c)–(g) show the evolution
of the matrix of pairwise weights (5) computed from (b) as the number of hypotheses M is increased.
For presentation the data are arranged according to their structure membership  which gives rise to
a 4-block pattern. Observe that the block pattern  hence weights  converge as M increases. (h) and
(i) respectively show performance measures (see text) of four methods on the dataset in (b). (j)–(m)
show the evolution of the labelling result of ARJMC as M increases (only one view is shown).

Figs. 1(c)–(g) show the evolution of the pairwise weights (5) as M increases until 10 000 for the data
in Fig. 1(b). The matrices exhibit a a four-block pattern  indicating strong mutual preference among
inliers from the same structure. This phenomenon allows accurate selection of minimal subsets in
Multi-GS [6]. More pertinently  as we predicted in Sec. 3.2  the weights converge as M increases 
as evidenced by the stabilising block pattern. Note that only a small number of weights are actually
computed in Multi-GS [6]; the full matrix of weights are calculated here for illustration only.
We run ARJMC and record the following performance measures: Value of the objective function
f (k  θk) in Eq. (1)  and segmentation error. The latter involves assigning each datum xi ∈ X
to the nearest structure in θk if the residual is less than the threshold t; else xi is labelled as an
outlier. The overall labelling error is then obtained. The measures are recorded at time intervals
corresponding to the instances when M = 100  200  . . .   1000 number of hypotheses generated so
far in Algorithm 1. Median results over 20 repetitions on the data in Fig. 1(b) are shown in Figs. 1(h)
and 1(i). Figs. 1(j)–1(m) depict the evolution of the segmentation result of ARJMC as M increases.

7

5010015020025030050100150200250300501001502002503005010015020025030050100150200250300501001502002503005010015020025030050100150200250300501001502002503005010015020025030005101520253015202530354045Time (s)Objective function value f(k θk)  QP−MF (random)ENERGY (random)FLOSS (random)ARJMCQP−MF (Multi−GS)ENERGY (Multi−GS)FLOSS (Multi−GS)051015202530010203040506070Time (s)Segmentation error (%)  QP(cid:239)MF (random)ENERGY (random)FLOSS (random)ARJMCQP(cid:239)MF (Multi(cid:239)GS)ENERGY (Multi(cid:239)GS)FLOSS (Multi(cid:239)GS)For objective comparisons the competing two-stage methods were tested as follows: First  M =
100  200  . . .   1000 hypotheses are accumulatively generated (using both uniform random sam-
pling [9] and Multi-GS [6]). A new instance of each method is invoked on each set of M hypotheses.
We ensure that each method returns the true number of structures for all M; this represents an ad-
vantage over ARJMC  since the “online learning” nature of ARJMC means the number of structures
is not discovered until closer to convergence. Results are also shown in Figs. 1(h) and 1(i).
Firstly  it is clear that the performance of the two-stage methods on both measures are improved
dramatically with the application of Multi-GS for hypothesis generation. From Fig. 1(h) ARJMC is
the most efﬁcient in minimising the function f (k  θk); it converges to a low value in signiﬁcantly
less time. It should be noted however that the other methods are not directly minimising AIC or
f (k  θk). The segmentation error (which no method here is directly minimising) thus represents a
more objective performance measure. From Fig. 1(i)  it can be seen that the initial error of ARJMC
is much higher than all other methods  a direct consequence of not having yet estimated the true
number of structures. The error is eventually minimised as ARJMC converges. Table 1 which
summarises the results on the other datasets (all using Multi-GS) conveys a similar picture. Further
results on multi-homography detection also yield similar outcomes (see supplementary material).

Dataset

# inliers  outliers

breadtoycar (3 structures)

37  39 and 34 inliers  56 outliers

carchipscube (3 structures)

19  33 and 53 inliers  60 outliers

toycubecar (3 structures)

45  69 and 14 inliers  72 outliers

M
100
200
300
400
500
600
700
800
900
1000

Time (seconds)

Dataset

# inliers  outliers

M
100
200
300
400
500
600
700
800
900
1000

Time (seconds)

S
S
O
L
F

25.22
14.13
10.43
9.57
9.57
8.70
8.91
7.83
7.39
7.17
12.88

Y
G
R
E
N
E

31.74
26.74
33.48
27.83
27.39
25.87
30.43
21.09
25.22
20.43
9.40

F
M
-
P
Q
24.78
18.91
18.70
18.26
26.30
20.43
21.30
22.17
26.74
25.22
21.57

C
M
J
R
A
68.70
61.96
54.13
48.48
10.87
8.48
7.17
6.52
6.52
6.52
5.44

S
S
O
L
F

21.82
15.76
12.73
10.30
10.30
9.09
8.48
10.30
8.48
9.09
9.57

Y
G
R
E
N
E

29.70
36.97
24.24
32.73
30.91
28.48
22.42
26.67
36.36
28.48
7.02

F
M
-
P
Q
23.64
30.30
26.67
28.48
27.27
23.03
27.88
25.45
26.06
23.64
16.23

C
M
J
R
A
52.73
58.18
49.09
24.24
13.33
9.70
9.70
9.70
9.70
9.70
5.16

S
S
O
L
F

31.75
23.00
22.75
22.00
22.50
21.75
17.50
21.50
18.75
15.50
11.73

Y
G
R
E
N
E

26.25
27.25
25.25
26.25
22.50
26.50
26.50
26.50
20.75
23.00
8.14

F
M
-
P
Q
29.00
19.25
18.00
22.50
23.00
20.75
23.00
20.00
15.75
18.25
18.94

C
M
J
R
A
81.50
75.75
65.00
52.75
45.75
37.75
23.50
18.50
19.75
19.50
4.95

breadcubechip (3 structures)

34  57 and 58 inliers  81 outliers

breadcartoychip (4 structures)

33  23  41 and 58 inliers  82 outliers

biscuitbookbox (3 structures)

67  41 and 54 inliers  97 outliers

S
S
O
L
F

23.49
16.27
12.65
13.86
12.05
12.05
10.84
10.84
10.84
10.84
9.57

Y
G
R
E
N
E

21.08
13.25
10.84
11.45
13.25
12.05
11.45
12.05
10.24
10.84
6.96

F
M
-
P
Q
24.10
15.06
18.07
14.46
13.25
12.05
9.04
11.45
10.24
10.84
16.38

C
M
J
R
A
81.93
78.92
70.48
48.80
37.95
11.45
9.64
9.64
7.83
8.43
4.47

S
S
O
L
F

36.92
28.90
19.41
17.51
13.92
11.81
10.76
10.55
10.34
9.70
13.40

Y
G
R
E
N
E

35.86
27.00
21.30
20.88
18.56
19.83
15.18
18.56
14.55
15.18
9.86

F
M
-
P
Q
32.07
20.04
17.09
15.19
13.50
13.92
12.66
12.24
11.39
11.60
22.46

C
M
J
R
A
54.01
61.60
61.18
56.54
21.94
18.99
18.14
10.97
9.70
9.70
5.39

S
S
O
L
F

17.57
11.00
7.92
8.49
7.92
5.79
5.79
5.79
5.79
5.79
15.46

Y
G
R
E
N
E

25.87
17.95
17.95
14.86
18.73
17.18
18.92
16.60
18.53
13.71
10.66

F
M
-
P
Q
18.15
17.76
9.27
13.51
10.04
11.39
14.67
13.51
12.36
13.13
24.36

C
M
J
R
A
49.03
31.85
6.95
6.37
4.44
5.21
4.83
5.21
5.21
5.79
5.47

Table 1: Median segmentation error (%) at different number of hypotheses M. Time elapsed at
M = 1000 is shown at the bottom. The lowest error and time achieved on each dataset is boldfaced.

5 Conclusions

By design  since our algorithm conducts hypothesis sampling  geometric ﬁtting and model selection
simultaneously  it minimises wastage in the sampling process and converges faster than previous
two-stage approaches. This is evident from the experimental results. Underpinning our novel Re-
versible Jump MCMC method is an efﬁcient hypothesis generator whose proposal distribution is
learned online. Drawing from new theory on Adaptive MCMC  we prove that our efﬁcient hypoth-
esis generator satisﬁes the properties crucial to ensure convergence to the correct target distribution.
Our work thus links the latest developments from MCMC optimisation and geometric model ﬁtting.
Acknowledgements. The authors would like to thank Anders Eriksson his insightful comments.
This work was partly supported by the Australian Research Council grant DP0878801.

8

References
[1] H. Akaike. A new look at the statistical model identiﬁcation.

19(6):716–723  1974.

IEEE Trans. on Automatic Control 

[2] C. Andrieu  N. de Freitas  and A. Doucet. Robust full Bayesian learning for radial basis networks. Neural

Computation  13:2359–2407  2001.

[3] C. Andrieu  N. de Freitas  A. Doucet  and M. I. Jordan. An introduction to MCMC for machine learning.

Machine Learning  50:5–43  2003.

[4] C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statistics and Computing  18(4)  2008.
[5] S. P. Brooks  N. Friel  and R. King. Classical model selection via simulated annealing. J. R. Statist. Soc.

B  65(2):503–520  2003.

[6] T.-J. Chin  J. Yu  and D. Suter. Accelerated hypothesis generation for multi-structure robust ﬁtting. In

European Conf. on Computer Vision  2010.

[7] A. Delong  A. Osokin  H. Isack  and Y. Boykov. Fast approximate energy minimization with label costs.

In Computer Vision and Pattern Recognition  2010.

[8] L. Fan and T. Pyln¨an¨ainen. Adaptive sample consensus for efﬁcient random optimisation. In Int. Sympo-

sium on Visual Computing  2009.

[9] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model ﬁtting with applica-

tions to image analysis and automated cartography. Comm. of the ACM  24:381–395  1981.

[10] S. Gaffney and P. Smyth. Trajectory clustering with mixtures of regression models.

Knowledge Discovery and Data Mining  1999.

In ACM SIG on

[11] P. Giordani and R. Kohn. Adaptive independent Metropolis-Hastings by fast estimation of mixtures of

normals. Journal of Computational and Graphical Statistics  19(2):243–259  2010.

[12] P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.

Biometrika  82(4):711–732  1995.

[13] H. Haario  E. Saksman  and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli  7(2):223–242 

2001.

[14] R. Hartley.

In defense of the eight-point algorithm.

Intelligence  19(6):580–593  1997.

IEEE Trans. on Pattern Analysis and Machine

[15] R. Hartley and A. Zisserman. Multiple View Geometry. Cambridge University Press  2004.
[16] P. J. Huber. Robust Statistics. John Wiley & Sons Inc.  2009.
[17] Y.-D. Jian and C.-S. Chen. Two-view motion segmentation by mixtures of dirichlet process with model

selection and outlier removal. In International Conference on Computer Vision  2007.

[18] N. Lazic  I. Givoni  B. Frey  and P. Aarabi. FLoSS: Facility location for subspace segmentation. In IEEE

Int. Conf. on Computer Vision  2009.

[19] H. Li. Two-view motion segmentation from linear programming relaxation.

Pattern Recognition  2007.

In Computer Vision and

[20] D. Nott and R. Kohn. Adaptive sampling for Bayesian variable selection. Biometrika  92:747–763  2005.
[21] N. Quadrianto  T. S. Caetano  J. Lim  and D. Schuurmans. Convex relaxation of mixture regression with

efﬁcient algorithms. In Advances in Neural Information Processing Systems  2010.

[22] S. Richardson and P. J. Green. On Bayesian analysis on mixtures with an unknown number of components.

J. R. Statist. Soc. B  59(4):731–792  1997.

[23] G. O. Roberts and J. S. Rosenthal. Coupling and ergodicity of adaptive Markov chain Monte Carlo

algorithms. Journal of Applied Probability  44:458–475  2007.

[24] G. O. Roberts and J. S. Rosenthal. Examples of adaptive MCMC. Journal of Computational and Graph-

ical Statistics  18(2):349–367  2009.

[25] K. Schinder and D. Suter. Two-view multibody structure-and-motion with outliers through model selec-

tion. IEEE Trans. on Pattern Analysis and Machine Intelligence  28(6):983–995  2006.

[26] N. Thakoor and J. Gao. Branch-and-bound hypothesis selection for two-view multiple structure and

motion segmentation. In Computer Vision and Pattern Recognition  2008.

[27] P. H. S. Torr. Motion segmentation and outlier detection. PhD thesis  Dept. of Engineering Science 

University of Oxford  1995.

[28] P. H. S. Torr and C. H. Davidson.

IMPSAC: Synthesis of importance sampling and random sample

consensus. IEEE Trans. on Pattern Analysis and Machine Intelligence  25(3):354–364  2003.

[29] E. Vincent and R. Lagani`ere. Detecting planar homographies in an image pair. In International Sympo-

sium on Image and Signal Processing and Analysis  2001.

[30] H. S. Wong  T.-J. Chin  J. Yu  and D. Suter. Dynamic and hierarchical multi-structure geometric model

ﬁtting. In International Conference on Computer Vision  2011.

[31] J. Yu  T.-J. Chin  and D. Suter. A global optimization approach to robust multi-model ﬁtting. In Computer

Vision and Pattern Recognition  2011.

9

,Christof Seiler
Simon Rubinstein-Salzedo
Susan Holmes
Yong Guo
Yin Zheng
Mingkui Tan
Qi Chen
Jian Chen
Peilin Zhao
Junzhou Huang