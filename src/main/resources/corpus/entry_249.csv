2019,A Universally Optimal Multistage Accelerated Stochastic Gradient Method,We study the problem of minimizing a strongly convex  smooth function when we have noisy estimates of its gradient. We propose a novel multistage accelerated algorithm that is universally optimal in the sense that it achieves the optimal rate both in the deterministic and stochastic case and operates without knowledge of noise characteristics. The algorithm consists of stages that use a stochastic version of Nesterov's method with a specific restart and parameters selected to achieve the fastest reduction in the bias-variance terms in the convergence rate bounds.,A Universally Optimal Multistage Accelerated

Stochastic Gradient Method

Necdet Serhat Aybat∗

Pennsylvania State University

University Park  PA  USA

nsa10@psu.edu

Mert Gürbüzbalaban∗

Rutgers University
Piscataway  NJ  USA
mg1366@rutgers.edu

Alireza Fallah∗

Massachusetts Institute of Technology

Cambridge  MA  USA
afallah@mit.edu

Asuman Ozdaglar∗

Massachusetts Institute of Technology

Cambridge  MA  USA

asuman@mit.edu

Abstract

We study the problem of minimizing a strongly convex  smooth function when we
have noisy estimates of its gradient. We propose a novel multistage accelerated
algorithm that is universally optimal in the sense that it achieves the optimal rate
both in the deterministic and stochastic case and operates without knowledge of
noise characteristics. The algorithm consists of stages that use a stochastic version
of Nesterov’s method with a speciﬁc restart and parameters selected to achieve the
fastest reduction in the bias-variance terms in the convergence rate bounds.

1

Introduction

First order optimization methods play a key role in solving large scale machine learning problems due
to their low iteration complexity and scalability with large data sets. In several cases  these methods
operate with noisy ﬁrst order information either because the gradient is estimated from draws or
subset of components of the underlying objective function [3  8  13  16  17  21  36  9  11] or noise is
injected intentionally due to privacy or algorithmic considerations [4  25  30  14  15]. A fundamental
question in this setting is to design fast algorithms with optimal convergence rate  matching the lower
bounds on the oracle complexity in terms of target accuracy and other important parameters both for
the deterministic and stochastic case (i.e.  with or without gradient errors).
In this paper  we design an optimal ﬁrst order method to solve the problem

f∗ (cid:44) min
x∈Rd

(1)
where  for scalars 0 < µ ≤ L  Sµ L(Rd) is the set of continuously differentiable functions f : Rd →
R that are strongly convex with modulus µ and have Lipschitz-continuous gradients with constant L 
which imply that for every x  y ∈ Rd  f satisﬁes (see e.g. [27])

f (x)

such that f ∈ Sµ L(Rd) 

µ
2

(cid:107)x − y(cid:107)2 ≤ f (x) − f (y) − ∇f (y)(cid:62)(x − y) ≤ L
2

(cid:107)x − y(cid:107)2.

(2)

For f ∈ Sµ L(Rd)  the ratio κ (cid:44) L
denote the solution of problem (1) by f∗ which is achieved at the unique optimal point x∗.

µ is called the condition number of f. Throughout the paper  we

∗The authors are in alphabetical order.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We assume that the gradient information is available through a stochastic oracle  which at each
iteration n  given the current iterate xn ∈ Rd  provides the noisy gradient ˜∇f (xn  wn) where {wn}n
is a sequence of independent random variables such that for all n ≥ 0 

E[ ˜∇f (xn  wn)|xn] = ∇f (xn)  E(cid:104)(cid:107) ˜∇f (xn  wn) − ∇f (xn)(cid:107)2(cid:12)(cid:12)(cid:12)xn

(cid:105) ≤ σ2.

(3)

This oracle model is commonly considered in the literature (see e.g. [16  17  6]). In Appendix K  we
show how our analysis can be extended to the following more general noise setting  same as the one
studied in [3]  where the variance of the noise is allowed to grow linearly with the squared distance to
the optimal solution:

E[ ˜∇f (xn  wn)|xn] = ∇f (xn)  E(cid:104)(cid:107) ˜∇f (xn  wn) − ∇f (xn)(cid:107)2(cid:12)(cid:12)(cid:12)xn

(cid:105) ≤ σ2 + η2(cid:107)xn − x∗(cid:107)2. (4)

for some constant η ≥ 0.
Under noise setting in (3)  the performance of many algorithms is characterized by the expected error
of the iterates (in terms of the suboptimality in function values) which admits a bound as a sum of two
terms: a bias term that shows the decay of initialization error f (x0) − f∗ and is independent of the
noise parameter σ2  and a variance term that depends on σ2 and is independent of the initial point x0.
A lower bound on the bias term follows from the seminal work of Nemirovsky and Yudin [26]  which
showed that without noise (σ = 0) and after n iterations  E [f (xn)] − f∗ cannot be smaller than2

L(cid:107)x0 − x∗(cid:107)2

2 exp(−O(1)

n√
κ

).

(5)

With noise  Raginsky and Rakhlin [31] provided the following (much larger) lower bound3 on
function suboptimality which also provides a lower bound on the variance term:

for n sufﬁciently large.

(6)

(cid:18) σ2

(cid:19)

µn

Ω

(cid:107)x0−x∗(cid:107)2

n2

n +

Several algorithms have been proposed in the recent literature attempting to achieve these lower
bounds.4 Xiao [38] obtains O(log(n)/n) performance guarantees in expected suboptimality for an
accelerated version of the dual averaging method. Dieuleveut et al. [12] consider quadratic objective
function and develop an algorithm with averaging to achieve the error bound O( σ2
). Hu
et al. [20] consider general strongly convex and smooth functions and achieve an error bound with
similar dependence under the assumption of bounded noise. Ghadimi and Lan [16] and Chen et al.
[7] extend this result to the noise model in (3) by introducing the accelerated stochastic approximation
algorithm (AC-SA) and optimal regularized dual averaging algorithm (ORDA)  respectively. Both
AC-SA and ORDA have multistage versions presented in [17] and [7] where authors improve the
bias term of their single stage methods to the optimal exp(−O(1)n/
κ) by exploiting knowledge of
σ and the optimality gap ∆  i.e.  an upper bound for f (x0) − f∗  in the operation of the algorithm.
Another closely related paper is [8] which proposed µAGD+ and showed under additive noise model
(cid:107)x0−x∗(cid:107)2
that it admits the error bound O( σ2
) for any p ≥ 1 where the constants grow with p  and
in particular  they achieve the bound O( σ2 log n
In this paper  we introduce the class of Multistage Accelerated Stochastic Gradient (M-ASG) methods
that are universally optimal  achieving the lower bound both in the noiseless deterministic case
and the noisy stochastic case up to some constants independent of µ and L. M-ASG proceeds in
stages that use a stochastic version of Nesterov’s accelerated method [27] with a speciﬁc restart and
parameterization. Given an arbitrary length and constant stepsize for the ﬁrst stage together with
geometrically growing lengths and shrinking stepsizes for the following stages  we ﬁrst provide a
general convergence rate result for M-ASG (see Theorem 3.4). Given the computational budget
n  a speciﬁc choice for the length of the ﬁrst stage is shown to achieve the optimal error bound
without requiring knowledge of the noise bound σ2 and the initial optimality gap (See Corollary 3.8).

np
n +

) for p = log n.

(cid:107)x0−x∗(cid:107)2 log n

n +

√

nlog n

2This lower bound is shown with the additional assumption n ≤ d
3The authors show this result for µ = 1. Nonetheless  it can be generalized to any µ > 0 by scaling the

4Here we review their error bounds after n iterations highlighting dependence on σ2  n  and initial point x0 

problem parameters properly.

suppressing µ and L dependence.

2

Algorithm

AC-SA

Multi. AC-SA

ORDA

Multi. ORDA
Cohen et al.

M-ASG (With parameters in Corollary 3.7)
M-ASG (With parameters in Corollary 3.8)
M-ASG (With parameters in Corollary 3.9)

Table 1: Comparison of algorithms
Requires

σ ∆ n or 


 


 






 








[n]
[]

Opt. Opt.
Bias
Var.

















To the best of our knowledge  this is the ﬁrst algorithm that achieves such a lower bound under
such informational assumptions. In Table 1  we provide a comparison of our algorithm with other
algorithms in terms of required assumptions and optimality of their results in both bias and variance
terms. In particular  we consider ACSA [16]  Multistage AC-SA [17]  ORDA and Multistage ORDA
[7]  and the algorithm proposed in [8].
Our paper builds on an analysis of Nesterov’s accelerated stochastic method with a speciﬁc momentum
parameter presented in Section 2 which may be of independent interest. This analysis follows from
a dynamical system representation and study of ﬁrst order methods which has gained attention in
the literature recently [24  19  2]. In Section 3  we present the M-ASG algorithm  and characterize
its behavior under different assumptions as summarized in Table 1. In particular  we show that it
achieves the optimal convergence rate with the given budget of iterations n. In Section 4  we show
how additional information such as σ and ∆ can be leveraged in our framework to improve practical
performance. Finally  in Section 5  we provide numerical results on the comparison of our algorithm
with some of the other most recent methods in the literature.
Preliminaries and notation: Let Id and 0d represent the d × d identity and zero matrices. For
matrix A ∈ Rd×d  Tr(A) and det(A) denote the trace and determinant of A  respectively. Also  for
scalars 1 ≤ i ≤ j ≤ d and 1 ≤ k ≤ l ≤ d  we use A[i:j] [k:l] to show the submatrix formed by rows
i to j and columns k to l. We use the superscript (cid:62) to denote the transpose of a vector or a matrix
depending on the context. Throughout this paper  all vectors are represented as column vectors. Let
+ denote the set of all symmetric and positive semi-deﬁnite m × m matrices. For two matrices
Sm
A ∈ Rm×n and B ∈ Rp×q  their Kronecker product is denoted by A ⊗ B. For scalars 0 < µ ≤ L 
Sµ L(Rd) is the set of continuously differentiable functions f : Rd → R that are strongly convex
with modulus µ and have Lipschitz-continuous gradients with constant L. All logarithms throughout
the paper are in natural basis.

2 Modeling Accelerated Gradient method as a dynamical system

αµ

√

In this section we study Nesterov’s Accelerated Stochastic Gradient method (ASG) [27] with the
stochastic ﬁrst-order oracle in (3):

yk = (1 + β)xk − βxk−1 
L ] is the stepsize and β = 1−√

xk+1 = yk − α ˜∇f (yk  wk)
(7)
where α ∈ (0  1
αµ is the momentum parameter. This choice of
momentum parameter has already been studied in the literature  e.g.  [28  37  33]. In the next lemma 
we provide a new motivation for this choice by showing that for quadratic functions and in the
noiseless setting  this momentum parameter achieves the fastest asymptotic convergence rate for a
given ﬁxed stepsize α ∈ (0  1
Lemma 2.1. Let f ∈ Sµ L(Rd) be a strongly convex quadratic function such that f (x) = 1
2 x(cid:62)Qx−
p(cid:62)x + r where Q is a d by d symmetric positive deﬁnite matrix with all its eigenvalues in the interval
[µ  L]. Consider the deterministic ASG iterations  i.e.  σ = 0  as shown in (7)  with constant stepsize
α ∈ (0  1/L]. Then  the fastest asymptotic convergence rate  i.e. the smallest ρ ∈ (0  1) that satisﬁes
the inequality

L ]. The proof of this lemma is provided in Appendix A.

1+

(cid:107)xk − x∗(cid:107)2 ≤ (ρ + k)2k(cid:107)x0 − x∗(cid:107)2  ∀x0 ∈ Rd 

3

for some non-negative sequence {k}k that goes to zero is ρ = 1 − √
β = 1−√
√

αµ5 and it is achieved by
αµ . As a consequence  for this choice of β  there exists {k} such that limk→∞ k = 0 and

1+

αµ

f (xk) − f∗ ≤ L(1 − √

αµ + k)2k(cid:107)x0 − x∗(cid:107)2.

where ξk :=(cid:2)x(cid:62)

Our analysis builds on the reformulation of a ﬁrst-order optimization algorithm as a linear dynamical
system. Following [24  19]  we write ASG iterations as
ξk+1 = Aξk + B ˜∇f (yk  wk) 

(cid:3)(cid:62) ∈ R2d is the state vector and A  B and C are matrices with appropriate
(cid:20)1 + β −β

dimensions deﬁned as the Kronecker products A = ˜A ⊗ Id  B = ˜B ⊗ Id and C = ˜C ⊗ Id with

(cid:20)−α

yk = Cξk 

x(cid:62)
k−1

(cid:21)

(cid:21)

(8)

k

˜C = [1 + β −β] .

˜A =

1

0

 

˜B =

 

0

(9)

We can also relate the state ξk to the iterate xk in a linear fashion through the identity xk =
T ξk  T (cid:44) [Id
0d]. We study the evolution of the ASG method through the following Lyapunov
function which also arises in the study of deterministic accelerated gradient methods:

VP (ξ) = (ξ − ξ∗)(cid:62)P (ξ − ξ∗) + f (T ξ) − f∗

(10)
where P is a symmetric positive semi-deﬁnite matrix. We ﬁrst state the following lemma which can
be derived by adapting the proof of Proposition 4.6 in [2] to our setting with less restrictive noise
assumption compared to the additive noise model of [2]. Its proof can be found in Appendix B.
Lemma 2.2. Let f ∈ Sµ L(Rd) . Consider the ASG iterations given by (7). Assume there exist
ρ ∈ (0  1) and ˜P ∈ S2

+  possibly depending on ρ  such that

ρ2 ˜X1 + (1 − ρ2) ˜X2 (cid:23)

(cid:21)

(cid:20) ˜A(cid:62) ˜P ˜A − ρ2 ˜P
 (1 + β)2µ −β(1 + β)µ −(1 + β)

˜A(cid:62) ˜P ˜B
˜B(cid:62) ˜P ˜B

˜B(cid:62) ˜P ˜A

β

1
2

−β(1 + β)µ
−(1 + β)

α(2 − Lα)

β2µ
β

˜X2 =

(11)

 .

(12)

where

 β2µ −β2µ

−β2µ
−β

β2µ
β

  

−β
β

˜X1 =

1
2

α(2 − Lα)
Let P = ˜P ⊗ Id. Then  for every k ≥ 0 

E [VP (ξk+1)] ≤ ρ2E [VP (ξk)] + σ2α2( ˜P1 1 +

L
2

).

We use this lemma and derive the following theorem which characterize the behavior of ASG method
for when α ∈ (0  1/L] and β = 1−√
√
αµ (see the proof in Appendix C).
Theorem 2.3. Let f ∈ Sµ L(Rd) . Consider the ASG iterations given in (7) with α ∈ (0  1
β = 1−√
√

L ] and

1+

αµ

αµ

αµ . Then 

1+

E [VPα(ξk+1)] ≤ (1 − √

for every k ≥ 0  where Pα = ˜Pα ⊗ Id with ˜Pα =

αµ)E [VPα (ξk)] +

(cid:113) 1
2 −(cid:113) 1
2α(cid:112) µ



2α

σ2α

2

(cid:20)(cid:113) 1

2α

(1 + αL)

2 −(cid:113) 1
(cid:112) µ

2α

(13)

(cid:21)

.

This result relies on the special structure of Pα which will also be key for our analysis in Section 3.

3 A class of multistage ASG algorithms

In this section  we introduce a class of multistage ASG algorithms  represented in Algorithm 1 which
we denote by M-ASG. The main idea is to run ASG with properly chosen parameters (αk  βk) at

4

Algorithm 1: Multistage Accelerated Stochastic Gradient Algorithm (M-ASG)
1 Set n0 = −1;
2 for k = 1; k ≤ K; k = k + 1 do
3
4

Set xk
for m = 1; m ≤ nk; m = m + 1 do

nk−1+1;

1 = xk−1
0 = xk
Set βk = 1−√
√
Set yk
Set xk

1+

µαk
µαk

;

m = (1 + βk)xk
m+1 = yk

m − βkxk
m − αk ˜∇f (yk

m−1;
m  wk
m)

5

6
7
8
9 end

end

each stage k ∈ 1  . . .   K for K ≥ 2 stages. In addition  each new stage is dependent on the previous
stage as the ﬁrst two initial iterates of the new stage are set to the last iterate of the previous stage.
To analyze Algorithm 1  we ﬁrst characterize the evolution of iterates in one speciﬁc stage through
the Lyapunov function in (10). The details of the proof is provided in Appendix D.
Theorem 3.1. Let f ∈ Sµ L(Rd) . Consider running the ASG method given in (7) for n iterations
with α = c2

αµ for some 0 < c ≤ 1. Then  for Pα given in Theorem 2.3 

L and β = 1−√

√

1+

αµ

E [VPα (ξn+1)] ≤ exp(−n

c√
κ

)E [VPα (ξ1)] +

κc

.

(14)

σ2√

L

Given a computational budget of n iterations  we use this result to choose a stepsize that help us
achieve an approximately optimal decay in the variance term which yields the following corollary for
M-ASG algorithm with K = 1 stage  and its proof can be found in Appendix E.
Corollary 3.2. Let f ∈ Sµ L(Rd) . Consider running M-ASG  i.e.  Algorithm 1  for only one stage
with n1 = n iterations and stepsize α1 =

(cid:17)2

κ log n

1

L for some scalar p ≥ 1. Then 
0) − f∗) +

pσ2 log n

nµ

np (f (x0

(15)

√

(cid:16) p
n+1)(cid:3) − f∗ ≤ 2

n

E(cid:2)f (x1

√

√
κ max{2 log(p

(cid:62)

i−1

  xk

(cid:104)

κ)  e}.

(cid:62)(cid:105)(cid:62)

for 1 ≤ i ≤ nk + 1 –recall that xk

provided that n ≥ p
For subsequent analysis  given K ≥ 1  for all 1 ≤ k ≤ K  we deﬁne the state vector ξk
i =
nk−1+1  where K is the number
xk
i
of stages. We analyze the performance of each stage with respect to a stage-dependent Lyapunov
function VPαk
. The following lemma relates the performance bounds with respect to consecutive
choice of Lyapunov functions  building on our speciﬁc restarting mechanism (The proof can be found
in Appendix F).
Lemma 3.3. Let f ∈ Sµ L(Rd) . Consider M-ASG  i.e.  Algorithm 1. Then  for every 1 ≤ k ≤ K−1 

1 = xk−1

0 = xk

E(cid:104)

(cid:105) ≤ 2E(cid:104)

(cid:105)

VPαk+1

(ξk+1

1

)

VPαk

(ξk

nk+1)

.

(16)

Now  we are ready to state and prove the main result of the paper (see proof in Appendix G):
Theorem 3.4. Let f ∈ Sµ L(Rd) . Consider running M-ASG   i.e.  Algorithm 1  with some n1 ≥ 1
22kL for any k ≥ 2 and p ≥ 1. The last
and α1 = 1
iterate of each stage  i.e.  xk
√

nk+1  satisﬁes the following bound for all k ≥ 1:

κ log(2p+2)(cid:101) and αk = 1

L and ﬁxing nk = 2k(cid:100)√
nk+1)(cid:3) − f∗ ≤
E(cid:2)f (xk

0) − f∗)(cid:1) +

(cid:0)exp(−n1/

κ)(f (x0

(17)

2

σ2√
κ
L2k−1 .

general strongly convex functions in Theorem 2.3  as there ρ =(cid:112)1 − √

2(p+1)(k−1)

αµ.

5Note that although this rate is asymptotic  its smaller than the non-asymptotic rate that we provide for

5

NK(p  n1) (cid:44)(cid:80)K

κ log(2p+2)(cid:101).

m where k = (cid:100)log2

We next deﬁne NK(p  n1) as the number of iterations needed to run M-ASG for K ≥ 1 stages  i.e. 

k=1 nk. Note for K ≥ 2 and with parameters given in Theorem 3.4 

NK(p  n1) = n1 + (2K+1 − 4)(cid:100)√
(cid:16)

(cid:17) − 1(cid:101) and m = n − Nk−1(p  n1).

(18)
We deﬁne {xn}n∈Z+ sequence such that xn is the iterate generated by M-ASG algorithm at the end
of n gradient steps for n ≥ 0  i.e.  x0 = x0
n+1 for 1 ≤ n ≤ n1  and for n > n1 we set
xn = xk
Remark 3.5. In the absence of noise  i.e.  σ = 0  the result of Theorem 3.4 recovers the linear
convergence rate of deterministic gradient methods as its special case. Indeed  running M-ASG
only for one stage with n iterations  i.e.  K = 1 and n1 = n guarantees that E [f (xn)] − f∗ ≤
2 exp(−n/
The next theorem remarks the behavior of M-ASG after running it for n iterations with the parameters
in the preceding theorem  and its proof is provided in Appendix H.
Theorem 3.6. Let f ∈ Sµ L(Rd) . Consider running Algorithm 1 for n iterations and with parame-
ters given in Theorem 3.4 and n1 < n. Then the error is bounded by

0) − f∗) for all n ≥ 1.

n−n1
κ log(2p+2)(cid:101) + 4

0  xn = x1

κ)(f (x0

(cid:100)√

√

E [f (xn)] − f∗

(cid:32)

√

≤ O(1)

(8(p + 1)

κ log(2))p+1

(n − n1)p+1

0) − f∗)(cid:1) +

√

κ)(f (x0

(cid:33)

.

(19)

(p + 1)σ2
(n − n1)µ
√
κ log (12(p + 1)κ)(cid:101) 

(cid:0)exp(−n1/
(cid:18) 1

Corollary 3.7. Under the premise of Theorem 3.6  choosing n1 = (cid:100)(p + 1)
the suboptimality error of M-ASG after n ≥ 2n1 admits
np+1 (f (x0

E [f (xn)] − f∗ ≤ O(1)

0) − f∗) +

(p + 1)σ2

nµ

(cid:19)

.

Theorem 3.6 immediately yields the result in Corollary 3.7  (suboptimal with respect to dependence
on initial optimality gap); see Appendix I for the proof. Similar rate results have also been obtained
by AC-SA [16] and ORDA [7] algorithms.
We continue this section by pointing out some important special cases of our result. We ﬁrst show
in the next corollary how our algorithm is universally optimal and capable of achieving the lower
bounds (5) and (6) simultaneously. The proof follows from (19) and n − n1 ≥ n
√
Corollary 3.8. Under the premise of Theorem 3.6  consider a computational budget of n ≥ 2
C for some positive constant C ≥ 2  we obtain a bound matching the
iterations. By setting n1 = n
lower bounds in (5) and (6)  i.e. 

κ.

κ

2 ≥ √
(cid:19)

(cid:18)

E [f (xn)] − f∗ ≤ O(1)

exp(− n
√

C

κ )(f (x0

0) − f∗) +

σ2
nµ

.

We note that achieving the lower bound through the M-ASG algorithm requires the knowledge or
estimation of the strong convexity constant µ. In some applications  µ may not be known a priori.
However  for regularized risk minimization problems  the regularization parameter is known and it
determines the strong convexity constant. It is also worth noting that  even for the deterministic case 
[1] has shown that for a wide class of algorithms including ASG  it is not possible to obtain the lower
bound (5) without knowing the strong convexity parameter. In addition  in Appendix L  we show
how our framework can be extended to obtain nearly optimal results in the merely convex setting; i.e.
when µ = 0. Finally  note that the Lipschitz constant L can be estimated from data using standard
line search techniques in practice  see [5] and [32  Alg. 2].
The lower bound can also be stated as the minimum number of iterations needed to ﬁnd an −solution 
i.e  to ﬁnd x such that E[f (x)] − f∗ ≤   for any given  > 0. In the following corollary  and with
0) − f∗  we state
the additional assumption of knowing the bound ∆ on the initial optimality gap f (x0
this version of lower bound. The proof is provided in Appendix J.
0) − f∗  for any  ∈ (0  ∆)  running M-ASG 
Corollary 3.9. Let f ∈ Sµ L(Rd) . Given ∆ ≥ f (x0
Algorithm 1  with parameters given in Theorem 3.4  p = 1  and n1 = (cid:100)√
compute −solution within n iterations in total  where

(cid:1)(cid:101)  one can

κ log(cid:0) 4∆



n = (cid:100)√

κ log

(cid:18) 4∆

(cid:19)



(cid:101) + (cid:100)16(1 + log(8))

(cid:101).

σ2
µ

(20)

6

Recall that we presented a comparison with other state-of-the-art algorithms in Table 1. In particular 
this table shows that Multistage AC-SA [17] and Multistage ORDA [7] also achieve the lower bounds
provided that noise parameters are known – note we do not make this extra assumption for M-ASG. It
is also worth noting that the idea of restart  which plays a key role in achieving the lower bounds 
has been studied before in the context of deterministic accelerated methods [29  39]. However  a
naive extension of these restart methods to the stochastic setting leads to a two-stage algorithm which
switches from constant step-size to diminishing step-size when the variance term dominates the
bias term. Nevertheless  implementing this technique requires the knowledge of σ2 and optimality
gap to tune algorithms for achieving optimal rates in both bias and variance terms. M-ASG  on the
other hand  achieves the optimal rates using a speciﬁc multistage scheme that does not require the
knowledge of the parameter σ2. In the supplementary material  we also discuss how M-ASG is
related to AC-SA and Multistage AC-SA algorithms proposed in [16  17].
4 M-ASG∗: An improved bias-variance trade-off

In section 3  we described a universal algorithm that do not require the knowledge of neither initial
suboptimality gap ∆ nor the noise magnitude σ2 to operate. However  as we will argue in this section 
our framework is ﬂexible in the sense that additional information about the magnitude of ∆ or σ2 can
be leveraged to improve practical performance. We ﬁrst note that several algorithms in the literature
assume that an upper bound on ∆ is known or can be estimated  as summarized in Table 1. This
assumption is reasonable in a variety of applications when there is a natural lower bound on f. For
example  in supervised learning scenarios such as support vector machines  regression or logistic
regression problems  the loss function f has non-negative values [35]. Similarly  the noise level σ2
may be known or estimated  e.g.  in private risk minimization [4]  the noise is added by the user to
ensure privacy; therefore  it is a known quantity.
There is a natural well-known trade-off between constant and decaying stepsizes (decaying with
the number of iterations n) in stochastic gradient algorithms. Since the noise is multiplied with the
stepsize  a stepsize that is decaying with the number of iterations n leads to a decay in the variance
term; however  this will slow down the decay of the bias term  which is controlled essentially by
the behavior of the underlying deterministic accelerated gradient algorithm (AG) that will give the
best performance with the constant stepsize (note that when σ = 0  the bias term gives the known
performance bounds for the AG algorithm). The main idea behind the M-ASG algorithm (which
allows it to achieve the lower bounds) is to exploit this trade-off to decide on the right time  n1  to
switch to decaying stepsizes  i.e.  when the bias term is sufﬁciently small so that the variance term
dominates and should be handled with the decaying stepsize. This insight is visible from the results
of Theorem 3.4 which gives further insights on the choice of the stepsize at every stage to achieve
L in the
the lower bounds. Theorem 3.4 shows that if M-ASG is run with a constant stepsize α1 = 1
ﬁrst stage  then the variance term admits the bound σ2√
L which does not decay with the number of
iterations n1 in the ﬁrst stage. However  in later stages  when n > n1  the stepsize αk is decreased as
the number of iterations grows and this results in a decay of the variance term. Overall  the choice
of the length of the ﬁrst stage n1  has a major impact in practice which we will highlight in our
numerical experiments.
If an estimate of ∆ or σ2 is known  it is desirable to choose n1 as small as possible such that it
κ )E(cid:2)VPα1
ensures the bias term becomes smaller than the variance term at the end of the ﬁrst stage. More
speciﬁcally  applying Theorem 3.1 for c = 1  one can choose n1 to balance the variance σ2√
3.3  can be bounded by E(cid:2)VPα1
and
the bias exp(−n1
κ ) ≤ σ2√

0) − f∗). Therefore 
by having an estimate of an upper bound for ∆  n1 can be set to be the smallest number such that
2∆ exp(−n1

1)(cid:3)  as shown in the proof of Lemma

1)(cid:3) terms. The term E(cid:2)VPα1
1)(cid:3) = µ(cid:107)x0

(ξ1

(ξ1
0) − f∗ ≤ 2(f (x0
2 + f (x0

κ

L   i.e. 

κ

L

(ξ1

1√

1√

κ

0 − x∗(cid:107)2
(cid:16) 2L∆

κ log

√

σ2

κ

(cid:17)(cid:101).

n1 = (cid:100)√

This result allows one to ﬁne-tune the switching point to start using the decaying stepsizes within
our framework as a function of σ2 and ∆. In scenarios  when the noise level σ is small or the initial
gap ∆ is large  n1 is chosen large enough to guarantee a fast decay in the bias term. We would like to
emphasize that this modiﬁed M-ASG algorithm only requires the knowledge of σ and ∆ for selecting

7

(21)

ﬁg1: σ2

n = 10−2

ﬁg2: σ2

n = 10−4

ﬁg3: σ2

n = 10−6

Figure 1: Comparison on a quadratic function for n = 1000 iterations with different level of noise.

ﬁg1: σ2

n = 10−2

ﬁg2: σ2

n = 10−4

ﬁg3: σ2

n = 10−6

Figure 2: Comparison on a quadratic function for n = 10000 iterations with different level of noise.

n1 and the rest of the parameters can be chosen as in Theorem 3.4 which are independent of both
σ and ∆. Finally  the following theorem provides theoretical guarantees of our framework for this
choice of n1. The proof is omitted as it is similar to the proofs of Theorems 3.4 and 3.6.
Theorem 4.1. Let f ∈ Sµ L(Rd) . Consider running Algorithm 1 for n iterations and with parame-
ters given in Theorem 3.4  p = 1  and n1 set as (21). Then  the expected suboptimality in function
values admits the bound E [f (xn)] − f∗ ≤ 36(1 + log(8))

(n−n1)µ for all n ≥ n1.

σ2

5 Numerical experiments

n) where σ2

In this section  we demonstrate the numerical performance of Algorithm 1 with parameters speciﬁed
by Corollary 3.7 (M-ASG) and Theorem 4.1 (M-ASG∗) and compare with other methods from
the literature. In our ﬁrst experiment  we consider the strongly convex quadratic objective f (x) =
2 x(cid:62)Qx− bx + λ(cid:107)x(cid:107)2 where Q is the Laplacian of a cycle graph6  b is a random vector and λ = 0.01
1
is a regularization parameter. We assume the gradients ∇f (x) are corrupted by additive noise with a
Gaussian distribution N (0  σ2
n ∈ {10−6  10−4  10−2}. We note that this example has been
previously considered in the literature as a problem instance where Standard ASG (ASG iterations
√
κ−1√
κ+1 ) perform badly compared to Standard
with standard choice of parameters α = 1
GD (Gradient Descent with standard choice of the stepsize α = 1/L) [18]. In Figures 1 and 2 
we compare M-ASG and M-ASG∗ with Standard GD  Standard AG  µAGD+ [8]  and Multistage
AC-SA [17]. We consider dimension d = 100 and initialize all the methods from x0
0 = 0. We run the
algorithms Multistage AC-SA  and M-ASG∗  having access to the same estimate of ∆. Figures 1-
2 show the average performance of all the algorithms along with the 95% conﬁdence interval over
50 sample runs while the total number of iterations n = 1000 and n = 10000 respectively as the
noise level σ2 is varied. The simulation results reveal that both M-ASG and M-ASG∗ have typically
a faster decay of the error in the beginning and outperforms the other algorithms in general when

L and β =

6All diagonal entries of Q are 2  Qi j = −1 if |i − j| ≡ 1 (mod d)  and the remaining entries are zero.

8

100101102103Iteration count10-2100102f-f*Standard GDStandard AGMultistage AC-SA AGD+M-ASGM-ASG*100101102103Iteration count10-410-2100102f-f*Standard GDStandard AGMultistage AC-SA AGD+M-ASGM-ASG*100101102103Iteration count10-610-410-2100102104f-f*Standard GDStandard AGMultistage AC-SA AGD+M-ASGM-ASG*ﬁg1: b = 50

ﬁg2: b = 100

ﬁg3: b = 500

Figure 3: Comparison on logistic regression with n = 10000 iterations and with different batch sizes.

the number of iterations is small to moderate. In this case  the speed-up obtained by M-ASG and
M-ASG∗ is more prominent if the noise level σ2 is smaller. However  as the number of iterations
grows  the performance of the algorithms become similar as the variance term dominates. In addition 
we would like to highlight that when the noise is small  using n1 as suggested in (21)  M-ASG∗ runs
stage one longer than M-ASG; hence  enjoys the linear rate of decay for more iterations before the
variance term becomes the dominant term.
For the second set of experiments  we consider a regularized logistic regression problem for binary
classiﬁcation. In particular  we read 10000 images from the M-NIST [23] data-set  and our goal is to
distinguish the image of digit zero from that of digit eight.7 The number of samples is N = 1945  and
the size of each image is 20 by 20 after removing the margins (hence d = 400 after vectorizing the
images). At each iteration  we randomly choose a batch size b of images to compute an estimate of the
gradient.8 We choose the regularization parameter equal to 1√
following the standard practice (see
e.g. [34]). In Figure 3 we compare M-ASG with Standard GD  Standard AG  µAGD+ [8]  and AC-SA
[17] for b ∈ {50  100  500}. The batch size controls the noise level  with larger batches leading to
smaller σ. We run each of these algorithms for 50 times  and plot their average performance and 95%
conﬁdence intervals. It can be seen that M-ASG usually start faster  and achieves the asymptotic rate
of other algorithms for all different batch sizes.

N

6 Conclusion

In this work  we consider strongly convex smooth optimization problems where we have access to
noisy estimates of the gradients. We proposed a multistage method that adapts the choice of the
parameters of the Nesterov’s accelerated gradient at each stage to achieve the optimal rate. Our
method is universal in the sense that it does not require the knowledge of the noise characteristics
to operate and can achieve the optimal rate both in the deterministic and stochastic settings. We
provided numerical experiments that compare our method with existing approaches in the literature 
illustrating that our method performs well in practice.

Acknowledgements

The work of Necdet Serhat Aybat is partially supported by NSF Grant CMMI-1635106. Alireza
Fallah is partially supported by Siebel Scholarship. Mert Gürbüzbalaban acknowledges support from
the grants NSF DMS-1723085 and NSF CCF-1814888.

7We provide an experiment with synthetic data for logistic loss in Appendix N.
8This is an unbiased estimate of the gradient with ﬁnite but unknown variance  and therefore we do not use

M-ASG∗ or other algorithms that need the knowledge of variance.

9

References
[1] Yossi Arjevani and Ohad Shamir. On the iteration complexity of oblivious ﬁrst-order optimiza-
tion algorithms. In Proceedings of The 33rd International Conference on Machine Learning 
volume 48 of Proceedings of Machine Learning Research  pages 908–916  New York  New
York  USA  20–22 Jun 2016. PMLR.

[2] Necdet Serhat Aybat  Alireza Fallah  Mert Gurbuzbalaban  and Asuman Ozdaglar. Robust accel-
erated gradient methods for smooth strongly convex functions. arXiv preprint arXiv:1805.10579 
2018.

[3] Francis Bach and Eric Moulines. Non-Asymptotic Analysis of Stochastic Approximation
Algorithms for Machine Learning. In Neural Information Processing Systems (NIPS)  Spain 
2011.

[4] R. Bassily  A. Smith  and A. Thakurta. Private empirical risk minimization: Efﬁcient algorithms
and tight error bounds. In Foundations of Computer Science (FOCS)  2014 IEEE 55th Annual
Symposium on  pages 464–473. IEEE  2014.

[5] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM journal on imaging sciences  2(1):183–202  2009.

[6] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and

Trends R(cid:13) in Machine Learning  8(3-4):231–357  2015.

[7] Xi Chen  Qihang Lin  and Javier Pena. Optimal regularized dual averaging methods for
stochastic optimization. In F. Pereira  C. J. C. Burges  L. Bottou  and K. Q. Weinberger  editors 
Advances in Neural Information Processing Systems 25  pages 395–403. Curran Associates 
Inc.  2012.

[8] Michael Cohen  Jelena Diakonikolas  and Lorenzo Orecchia. On acceleration with noise-
corrupted gradients. In Proceedings of the 35th International Conference on Machine Learning 
volume 80 of Proceedings of Machine Learning Research  pages 1019–1028  Stockholmsmäs-
san  Stockholm Sweden  2018. PMLR.

[9] A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimiza-

tion  19(3):1171–1183  2008.

[10] E de Klerk. Aspects of Semideﬁnite Programming: Interior Point Algorithms and Selected

Applications  volume 65. Springer Science & Business Media  2002.

[11] O. Devolder  F. Glineur  and Y. Nesterov. First-order methods of smooth convex optimization

with inexact oracle. Mathematical Programming  146(1-2):37–75  2014.

[12] Aymeric Dieuleveut  Nicolas Flammarion  and Francis Bach. Harder  better  faster  stronger
convergence rates for least-squares regression. The Journal of Machine Learning Research 
18(1):3520–3570  2017.

[13] N. Flammarion and F. Bach. From averaging to acceleration  there is only a step-size. In

Conference on Learning Theory  pages 658–695  2015.

[14] X. Gao  M. Gürbüzbalaban  and L. Zhu. Global Convergence of Stochastic Gradient Hamiltonian
Monte Carlo for Non-Convex Stochastic Optimization: Non-Asymptotic Performance Bounds
and Momentum-Based Acceleration. ArXiv e-prints  September 2018.

[15] Xuefeng Gao  Mert Gurbuzbalaban  and Lingjiong Zhu. Breaking Reversibility Accel-
arXiv e-prints  page

erates Langevin Dynamics for Global Non-Convex Optimization.
arXiv:1812.07725  December 2018.

[16] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal
on Optimization  22(4):1469–1492  2012.

10

[17] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization  ii: shrinking procedures and optimal algorithms.
SIAM Journal on Optimization  23(4):2061–2089  2013.

[18] M. Hardt. Robustness versus acceleration. August 18th  2014. http://blog.mrtz.org/

2014/08/18/robustness-versus-acceleration.html  August 2014.

[19] Bin Hu and Laurent Lessard. Dissipativity theory for Nesterov’s accelerated method. In Pro-
ceedings of the 34th International Conference on Machine Learning  volume 70 of Proceedings
of Machine Learning Research  pages 1549–1557  International Convention Centre  Sydney 
Australia  2017. PMLR.

[20] Chonghai Hu  Weike Pan  and James T. Kwok. Accelerated gradient methods for stochastic
optimization and online learning. In Advances in Neural Information Processing Systems 22 
pages 781–789. Curran Associates  Inc.  2009.

[21] Prateek Jain  Sham M. Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford.
Accelerating stochastic gradient descent for least squares regression. In Proceedings of the 31st
Conference On Learning Theory  volume 75 of Proceedings of Machine Learning Research 
pages 545–604. PMLR  2018.

[22] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical

Programming  133(1):365–397  Jun 2012.

[23] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ 

1998.

[24] Laurent Lessard  Benjamin Recht  and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization  26(1):57–95 
2016.

[25] Arvind Neelakantan  Luke Vilnis  Quoc V Le  Ilya Sutskever  Lukasz Kaiser  Karol Kurach 
and James Martens. Adding gradient noise improves learning for very deep networks. arXiv
preprint arXiv:1511.06807  2015.

[26] Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

efﬁciency in optimization. Wiley  1983.

[27] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87.

Springer  2004.

[28] Atsushi Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Advances

in Neural Information Processing Systems  pages 1574–1582  2014.

[29] B. O’Donoghue and E. Candès. Adaptive restart for accelerated gradient schemes. Foundations

of Computational Mathematics  15(3):715–732  Jun 2015.

[30] M. Raginsky  A. Rakhlin  and M. Telgarsky. Non-convex learning via stochastic gradient

langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849  2017.

[31] Maxim Raginsky and Alexander Rakhlin. Information-based complexity  feedback and dynam-
ics in convex programming. IEEE Transactions on Information Theory  57(10):7036–7056 
2011.

[32] Mark Schmidt  Reza Babanezhad  Mohamed Ahmed  Aaron Defazio  Ann Clifton  and Anoop
Sarkar. Non-uniform stochastic average gradient method for training conditional random ﬁelds.
In artiﬁcial intelligence and statistics  pages 819–828  2015.

[33] Bin Shi  Simon S Du  Michael I Jordan  and Weijie J Su. Understanding the acceleration
phenomenon via high-resolution differential equations. arXiv preprint arXiv:1810.08907  2018.

[34] Karthik Sridharan  Shai Shalev-Shwartz  and Nathan Srebro. Fast rates for regularized objectives.

In Advances in Neural Information Processing Systems  pages 1545–1552  2009.

11

[35] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media 

2013.

[36] Sharan Vaswani  Francis Bach  and Mark Schmidt. Fast and faster convergence of sgd for
over-parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288 
2018.

[37] Hoi-To Wai  Wei Shi  Cesar A Uribe  Angelia Nedich  and Anna Scaglione. On curvature-aided

incremental aggregated gradient methods. arXiv preprint arXiv:1806.00125  2018.

[38] Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.

Journal of Machine Learning Research  11(Oct):2543–2596  2010.

[39] Zeyuan Allen Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient

and mirror descent. In ITCS  2017.

12

,Necdet Serhat Aybat
Alireza Fallah
Mert Gurbuzbalaban
Asuman Ozdaglar