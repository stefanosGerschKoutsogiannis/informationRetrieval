2013,A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data,Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials.  However  the most common neural point process models  the Poisson process and the gamma renewal process  do not capture interactions and correlations that are critical to modeling populations of neurons.  We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction.  We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons.   The model is extended to incorporate gain control or divisive normalization  and the modulation of neural spiking based on periodic phenomena.  Applied to neural spike recordings from the rat hippocampus  we see that the model captures inhibitory relationships  a dichotomy of classes of neurons  and a periodic modulation by the theta rhythm known to be present in the data.,A Determinantal Point Process Latent Variable
Model for Inhibition in Neural Spiking Data

Jasper Snoek∗
Harvard University

jsnoek@seas.harvard.edu

Ryan P. Adams
Harvard University

rpa@seas.harvard.edu

Richard S. Zemel
University of Toronto

zemel@cs.toronto.edu

Abstract

Point processes are popular models of neural spiking behavior as they provide a
statistical distribution over temporal sequences of spikes and help to reveal the
complexities underlying a series of recorded action potentials. However  the most
common neural point process models  the Poisson process and the gamma renewal
process  do not capture interactions and correlations that are critical to modeling
populations of neurons. We develop a novel model based on a determinantal point
process over latent embeddings of neurons that effectively captures and helps vi-
sualize complex inhibitory and competitive interaction. We show that this model
is a natural extension of the popular generalized linear model to sets of interacting
neurons. The model is extended to incorporate gain control or divisive normaliza-
tion  and the modulation of neural spiking based on periodic phenomena. Applied
to neural spike recordings from the rat hippocampus  we see that the model cap-
tures inhibitory relationships  a dichotomy of classes of neurons  and a periodic
modulation by the theta rhythm known to be present in the data.

Introduction

1
Statistical models of neural spike recordings have greatly facilitated the study of both intra-neuron
spiking behavior and the interaction between populations of neurons. Although these models are
often not mechanistic by design  the analysis of their parameters ﬁt to physiological data can help
elucidate the underlying biological structure and causes behind neural activity. Point processes in
particular are popular for modeling neural spiking behavior as they provide statistical distributions
over temporal sequences of spikes and help to reveal the complexities underlying a series of noisy
measured action potentials (see  e.g.  Brown (2005)). Signiﬁcant effort has been focused on address-
ing the inadequacies of the standard homogenous Poisson process to model the highly non-stationary
stimulus-dependent spiking behavior of neurons. The generalized linear model (GLM) is a widely
accepted extension for which the instantaneous spiking probability can be conditioned on spiking
history or some external covariate. These models in general  however  do not incorporate the known
complex instantaneous interactions between pairs or sets of neurons. Pillow et al. (2008) demon-
strated how the incorporation of simple pairwise connections into the GLM can capture correlated
spiking activity and result in a superior model of physiological data. Indeed  Schneidman et al.
(2006) observe that even weak pairwise correlations are sufﬁcient to explain much of the collective
behavior of neural populations. In this paper  we develop a point process over spikes from col-
lections of neurons that explicitly models anti-correlation to capture the inhibitive and competitive
relationships known to exist between neurons throughout the brain.

∗Research was performed while at the University of Toronto.

1

Although the incorporation of pairwise inhibition in statistical models is challenging  we demon-
strate how complex nonlinear pairwise inhibition between neurons can be modeled explicitly and
tractably using a determinantal point process (DPP). As a starting point  we show how a collection
of independent Poisson processes  which is easily extended to a collection of GLMs  can be jointly
modeled in the context of a DPP. This is naturally extended to include dependencies between the in-
dividual processes and the resulting model is particularly well suited to capturing anti-correlation or
inhibition. The Poisson spike rate of each neuron is used to model individual spiking behavior  while
pairwise inhibition is introduced to model competition between neurons. The reader familiar with
Markov random ﬁelds can consider the output of each generalized linear model in our approach to
be analogous to a unary potential while the DPP captures pairwise interaction. Although inhibitory 
negative pairwise potentials render the use of Markov random ﬁelds intractable in general; in con-
trast  the DPP provides a more tractable and elegant model of pairwise inhibition. Given neural
spiking data from a collection of neurons and corresponding stimuli  we learn a latent embedding
of neurons such that nearby neurons in the latent space inhibit one another as enforced by a DPP
over the kernel between latent embeddings. Not only does this overcome a modeling shortcoming of
standard point processes applied to spiking data but it provides an interpretable model for studying
the inhibitive and competitive properties of sets of neurons. We demonstrate how divisive normal-
ization is easily incorporated into our model and a learned periodic modulation of individual neuron
spiking is added to model the inﬂuence on individual neurons of periodic phenomena such as theta
or gamma rhythms.
The model is empirically validated in Section 4  ﬁrst on three simulated examples to show the in-
ﬂuence of its various components and then using spike recordings from a collection of neurons in
the hippocampus of an awake behaving rat. We show that the model learns a latent embedding of
neurons that is consistent with the previously observed inhibitory relationship between interneurons
and pyramidal cells. The inferred periodic component of approximately 4 Hz is precisely the fre-
quency of the theta rhythm observed in these data and its learned inﬂuence on individual neurons is
again consistent with the dichotomy of neurons.
2 Background
2.1 Generalized Linear Models for Neuron Spiking
A standard starting point for modeling single neuron spiking data is the homogenous Poisson pro-
cess  for which the instantaneous probability of spiking is determined by a scalar rate or intensity
parameter. The generalized linear model (Brillinger  1988; Chornoboy et al.  1988; Paninski  2004;
Truccolo et al.  2005) is a framework that extends this to allow inhomogeneity by conditioning the
spike rate on a time varying external input or stimulus. Speciﬁcally  in the GLM the rate parameter
results from applying a nonlinear warping (such as the exponential function) to a linear weighting
of the inputs. Paninski (2004) showed that one can analyze recorded spike data by ﬁnding the max-
imum likelihood estimate of the parameters of the GLM  and thereby study the dependence of the
spiking on external input. Truccolo et al. (2005) extended this to analyze the dependence of a neu-
ron’s spiking behavior on its past spiking history  ensemble activity and stimuli. Pillow et al. (2008)
demonstrated that the model of individual neuron spiking activity was signiﬁcantly improved by
including coupling ﬁlters from other neurons with correlated spiking activity in the GLM. Although
it is prevalent in the literature  there are fundamental limitations to the GLM’s ability to model real
neural spiking patterns. The GLM can not model the joint probability of multiple neurons spiking
simultaneously and thus lacks a direct dependence between the spiking of multiple neurons. Instead 
the coupled GLM relies on an assumption that pairs of neurons are conditionally independent given
the previous time step. However  empirical evidence  from for example neural recordings from the
rat hippocampus (Harris et al.  2003)  suggests that one can better predict the spiking of an individ-
ual neuron by taking into account the simultaneous spiking of other neurons. In the following  we
show how to express multiple GLMs as a determinantal point process  enabling complex inhibitory
interactions between neurons. This new model enables a rich set of interactions between neurons
and enables them to be embedded in an easily-visualized latent space.
2.2 Determinantal Point Processes
The determinantal point process is an elegant distribution over conﬁgurations of points in space that
tractably models repulsive interactions. Many natural phenomena are DPP distributed including
fermions in quantum mechanics and the eigenvalues of random matrices. For an in-depth survey 

2

see Hough et al. (2006); see Kulesza and Taskar (2012) for an overview of their development within
machine learning. A point process provides a distribution over subsets of a space S. A determi-
nantal point process models the probability density (or mass function  as appropriate) for a subset
of points  S ⊆ S as being proportional to the determinant of a corresponding positive semi-deﬁnite
gram matrix KS  i.e.  p(S) ∝ |KS|. In the L-ensemble construction that we limit ourselves to here 
this gram matrix arises from the application of a positive semi-deﬁnite kernel function to the set S.
Kernel functions typically capture a notion of similarity and so the determinant is maximized when
the similarity between points  represented as the entries in KS is minimized. As the joint probability
is higher when the points in S are distant from one another  this encourages repulsion or inhibition
between points. Intuitively  if one point i is observed  then another point j with high similarity  as
captured by a large entry [KS]ij of KS  will become less likely to be observed under the model. It
is important to clarify here that KS can be any positive semi-deﬁnite matrix over some set of in-
puts corresponding to the points in the set  but it is not the empirical covariance between the points
themselves. Conversely  KS encodes a measure of anti-correlation between points in the process.
Therefore  we refer hereafter to KS as the kernel or gram matrix.
3 Methods
3.1 Modeling inter-Neuron Inhibition with Determinantal Point Processes
We are interested in modelling the spikes on N neurons during an interval of time T . We will
assume that time has been discretized into T bins of duration δ. In our formulation here  we assume
that all interaction across time occurs due to the GLM and that the determinantal point process
only modulates the inter-neuron inhibition within a single time slice. This corresponds to a Poisson
assumption for the marginal of each neuron taken by itself.
In our formulation  we associate each neuron  n  with a D-dimensional latent vector yn ∈ RD and
take our space to be the set of these vectors  i.e.  S = {y1  y2 ···   yN}. At a high level  we use an
L-ensemble determinantal point process to model which neurons spike in time t via a subset St ⊂ S:
(1)
Here the entries of the matrix KS arise from a kernel function kθ(· ·) applied to the values {yn}N
n=1
so that [KS]n n(cid:48) = kθ(yn  yn(cid:48)). The kernel function  governed by hyperparameters θ  measures the
degree of dependence between two neurons as a function of their latent vectors. In our empirical
analysis we choose a kernel function that measures this dependence based on the Euclidean distance
between latent vectors such that neurons that are closer in the latent space will inhibit each other
more. In the remainder of this section  we will expand this to add stimulus dependence.
As the determinant of a diagonal matrix is simply the product of the diagonal entries  when KS
is diagonal the DPP has the property that it is simply the joint probability of N independent (dis-
cretized) Poisson processes. Thus in the case of independent neurons with Poisson spiking we can
write KS as a diagonal matrix where the diagonal entries are the individual Poisson intensity param-
eters  KS = diag(λ1  λ2 ···   λN ). Through conditioning the diagonal elements on some external
input  this elegant property allows us to express the joint probability of N independent GLMs in
the context of the DPP. This is the starting point of our model  which we will combine with a full
covariance matrix over the latent variables to include interaction between neurons.
Following Zou and Adams (2012)  we express the marginal preference for a neuron ﬁring over
others  thus including the neuron in the subset S  with a “prior kernel” that modulates the covariance.
Assuming that kθ(y  y) = 1  this kernel has the form

n=1) = |KSt|

Pr(St |{yn}N

|KS + I N|

.

[KS]n n(cid:48) = kθ(yn  yn(cid:48))δ

(2)
where n  n(cid:48)
∈ S and λn is the intensity measure of the Poisson process for the individual spiking
behavior of neuron n. We can use these intensities to modulate the DPP with a GLM by allowing
the λn to depend on a weighted time-varying stimulus. We denote the stimulus at time t by a
vector xt ∈ RK and neuron-speciﬁc weights as wn ∈ RK  leading to instantaneous rates:
This leads to a stimulus dependent kernel for the DPP L-ensemble:

n = exp{xT
λ(t)

t wn}.

λn(cid:48) 

(3)

λn

(cid:112)

(cid:112)

[K(t)

S ]n n(cid:48) = kθ(yn  yn(cid:48)) δ exp

3

(cid:26) 1

2

(cid:27)

xT

t (wn + wn(cid:48))

.

(4)

(cid:113)

(cid:113)

(cid:113)
T(cid:89)

t=1

It is convenient to denote the diagonal matrix Π(t) = diag(
the St-restricted submatrix Π(t)
St
neurons that spiked at time t. We can now write the joint probability of the spike history as

λ(t)
N )  as well as
  where St indexes the rows of Π corresponding to the subset of

λ(t)
2  ···  

λ(t)
1  

Pr({St}T

t=1 |{wn  yn}N

n=1 {xt}T

t=1  θ) =

St

KStΠ(t)
St |

|δΠ(t)
S KSΠ(t)

S + IN|

|δΠ(t)

.

(5)

The generalized linear model now modulates the marginal rates  while the determinantal point pro-
cess induces inhibition. This is similar to unary versus pairwise potentials in a Markov random ﬁeld.
Note also that as the inﬂuence of the DPP goes to zero  KS tends toward the identity matrix and
the probability of neuron n ﬁring becomes (for δ (cid:28) 1) δλ(t)
n   which recovers the basic GLM. The
latent embeddings yn and weights wn can now be learned so that the appropriate balance is found
between stimulus dependence and inhibition due to  e.g.  overlapping receptive ﬁelds.
3.2 Learning
n=1 from data by maximizing the likelihood in Equation 5.
We learn the model parameters {wn  yn}N
This optimization is performed using stochastic gradient descent on mini-batches of time slices.
The computational complexity of learning the model is asymptotically dominated by the cost of
computing the determinants in the likelihood  which are O(N 3) in this model. This was not a
limiting factor in this work  as we model a population of 31 neurons. Fitting this model for 31
neurons in Section 4.3 with approximately eighty thousand time bins requires approximately three
hours using a single core of a typical desktop computer. The cubic scaling of determinants in this
model will not be a realistic limiting factor until it is possible to simultaneously record from tens of
thousands of neurons simultaneously. Nevertheless  at these extremes there are promising methods
for scaling the DPP using low rank approximations of KS (Affandi et al.  2013) or expressing them
in the dual representation when using a linear covariance (Kulesza and Taskar  2011).
3.3 Gain and Contrast Normalization
There is increasing evidence that neural responses are normalized or scaled by a common factor such
as the summed activations across a pool of neurons (Carandini and Heeger  2012). Many compu-
tational models of neural activity include divisive normalization as an important component (Wain-
wright et al.  2002). Such normalization can be captured in our model through scaling the individual
neuron spiking rates by a stimulus-dependent multiplicative constant νt > 0:
KStΠ(t)
St |

Pr(St |{wn  yn}N
t wν}. We learn these parameters wν jointly with the other model parameters.

where νt = exp{xT
3.4 Modeling the Inﬂuence of Periodic Phenomena
Neuronal spiking is known to be heavily inﬂuenced by periodic phenomena. For example  in our
empirical analysis in Section 4.3 we apply the model to the spiking of neurons in the hippocampus
of behaving rats. Csicsvari et al. (1999) observe that the theta rhythm plays a signiﬁcant role in
determining the spiking behavior of the neurons in these data  with neurons spiking in phase with
the 4 Hz periodic signal. Thus  the ﬁring patterns of neurons that ﬁre in phase can be expected to
be highly correlated while those which ﬁre out of phase will be strongly anti-correlated. In order to
incorporate the dependence on a periodic signal into our model  we add to λ(t)
n a periodic term that
modulates the individual neuron spiking rates with a frequency f  a phase ϕ  and a neuron-speciﬁc
amplitude or scaling factor ρn 

n=1  xt  θ  νt) = |νtδΠ(t)
|νtδΠ(t)

S KSΠ(t)

S + IN|

(6)

St

 

λ(t)

(7)
where t is the time at which the spikes occurred. Note that if desired one can easily manipulate
Equation 7 to have each of the neurons modulated by an individual frequency  ai  and offset bi.
Alternatively  we can create a mixture of J periodic components  modeling for example the inﬂuence
of the theta and gamma rhythms  by adding a sum over components 

t wn + ρn sin(f t + ϕ)(cid:9)

n = exp(cid:8)xT
xT

J(cid:88)

j=1

4



λ(t)
n = exp

t wn +

ρjn sin(fj t + ϕj)

(8)

(a) Sliding Bar

(b) Random Spiking

(c) Gain Control

Figure 1: Results of the simulated moving bar experiment (1a) compared to independent spiking behavior (1b).
Note that in 1a the model puts neighboring neurons within the unit length scale while it puts others at least one
length scale apart. 1c demonstrates the weights  wν  of the gain component learned if up to 5x random gain is
added to the stimulus at retina locations 6-12.

4 Experiments
In this section we present an empirical analysis of the model developed in this paper. We ﬁrst
evaluate the model on a set of simulated experiments to examine its ability to capture inhibition in
the latent variables while learning the stimulus weights and gain normalization. We then train the
model on recorded rat hippocampal data and evaluate its ability to capture the properties of groups of
interacting neurons. In all experiments we compute KS with the Mat´ern 5/2 kernel (see Rasmussen
and Williams (2006) for an overview) with a ﬁxed unit length scale (which determines the overall
scaling of the latent space).
4.1 Simulated Moving Bar
We ﬁrst consider an example simulated problem where twelve neurons are conﬁgured in order along
a one dimensional retinotopic map and evaluate the ability of the DPP to learn latent representations
that reﬂect their inhibitive properties. Each neuron has a receptive ﬁeld of a single pixel and the
neurons are stimulated by a three pixel wide moving bar. The bar is slid one pixel at each time step
from the ﬁrst to last neuron  and this is repeated twenty times. Of the three neighboring neurons
exposed to the bar  all receive high spike intensity but due to neural inhibition  only the middle one
spikes. A small amount of random background stimulus is added as well  causing some neurons to
spike without being stimulated by the moving bar. We train the DPP speciﬁed above on the resulting
spike trains  using the stimulus of each neuron as the Poisson intensity measure and visualize the
one-dimensional latent representation  y  for each neuron. This is compared to the case where all
neurons receive random stimulus and spike randomly and independently when the stimulus is above
a threshold. The resulting learned latent values for the neurons are displayed in Figure 1. We see
in Figure 1a that the DPP prefers neighboring neurons to be close in the latent space  because they
compete when the moving bar stimulates them. To demonstrate the effect of the gain and contrast
normalization we now add random gain of up to 5x to the stimulus only at retina locations 6-12 and
retrain the model while learning the gain component. In Figure 1c we see that the model learns to
use the gain component to normalize these inputs.
4.2 Digits Data
Now we use a second simulated experiment to examine the ability of the model to capture structure
encoding inhibitory interactions in the latent representation while learning the stimulus dependent
probability of spiking from data. This experiment includes thirty simulated neurons  each with a
two dimensional latent representation  i.e.  N = 30  yn ∈ R2. The stimuli are 16×16 images of
handwritten digits from the MNIST data set  presented sequentially  one per “time slice”. In the
data  each of the thirty neurons is specialized to one digit class  with three neurons per digit. When
a digit is presented  two neurons ﬁre among the three: one that ﬁres with probability one  and one
of the remaining two ﬁres with uniform probability. Thus  we expect three neurons to have strong
probability of ﬁring when the stimulus contains their preferred digit; however  one of the neurons
does not spike due to competition with another neuron. We expect the model to learn this inhibition
by moving the neurons close together in the latent space. Examining the learned stimulus weights
and latent embeddings  shown in Figures 2a and 2b respectively  we see that this is indeed the
case. This scenario highlights a major shortcoming of the coupled GLM. For each of the inhibitory

5

024681012−1.5−1−0.500.511.52Latent ValueOrder in 1D Retina024681012−2−1012Latent ValueOrder in 1D Retina02468101200.20.40.60.811.21.4Gain WeightOrder in 1D Retina(a) Stimulus Weights

(b) 2D Latent Embedding

Figure 2: Results of the digits experiment. A visualization of the neuron speciﬁc weights wn (2a) and latent
embedding (2b) learned by the DPP. In (2b) each blue number indicates the position of the neuron that always
ﬁres for that speciﬁc digit  and the red and green numbers indicate the neurons that respond to that digit but
inhibit each other. We observe in (2b) that inhibitory pairs of neurons  the red and green pairs  are placed
extremely close to each other in the DPP’s learned latent space while neurons that spike simultaneously (the blue
and either red or green) are distant. This scenario emphasizes the beneﬁt of having an inhibitory dependence
between neurons. The coupled GLM can not model this scenario well because both neurons of the inhibitory
pair receive strong stimulus but there is no indication from past spiking behavior which neuron will spike.

(a) Kernel Matrix  KS

(b) Stimulus Weights  wn

(c) wν

(d) wn=3

Figure 3: Visualizations of the parameters learned by the DPP on the Hippocampal data. Figure 3a shows a
visualization of the kernel matrix KS. Dark colored entries of KS indicate a strong pairwise inhibition while
lighter ones indicate no inhibition. The low frequency neurons  pyramidal cells  are strongly anti-correlated
which is consistent with the notion that they are inhibited by a common source such as an interneuron. Figure 3b
shows the (normalized) weights  wn learned from the stimulus feature vectors  which consist of concatenated
location and orientation bins  to each neuron’s Poisson spike rate λ(t)
n . An interesting observation is that the
two highest frequency neurons  interneurons  have little dependence on any particular stimulus and are strongly
anti-correlated with a large group of low frequency pyramidal cells. 3c shows the weights  wν to the gain
control  ν  and 3d shows a visualization of the stimulus weights for a single neuron n = 3 organized by
location and orientation bins. In 3a and 3b the neurons are ordered by their ﬁring rates. In 3d we see that the
neuron is stimulated heavily by a speciﬁc location and orientation.

pairs of neurons  both will simultaneously receive strong stimulus but the conditional independence
assumption will not hold; past spiking behavior can not indicate that only one can spike.
4.3 Hippocampus Data
As a ﬁnal experiment  we empirically evaluate the proposed model on multichannel recordings from
layer CA1 of the right dorsal hippocampus of awake behaving rats (Mizuseki et al.  2009; Csicsvari
et al.  1999). The data consist of spikes recorded from 31 neurons across four shanks during open
ﬁeld tasks as well as the syncronized positions of two LEDs on the rat’s head. The extracted positions
and orientations of the rat’s head are binned into twenty-ﬁve discrete location and twelve orientation
bins which are input to the model as the stimuli. Approximately twenty seven minutes of spike
recording data was divided into time slices of 20ms. The data are hypothesized to consist of spiking

6

051015202530Neuron Index0510152025300.00.51.0051015202530Neuron Index05101520Stimulus Index05101520Stimulus Index0123401234Location GridOrientations(a) Latent embedding of neurons

(b) Latent embedding of neurons (zoomed)

Figure 4: A visualization of the two dimensional latent embeddings  yn  learned for each neuron. Figure 4b
shows 4a zoomed in on the middle of the ﬁgure. Each dot indicates the latent value of a neuron. The color
of the dots represents the empirical spiking rate of the neuron  the number indicates the depth of the neuron
according to its position along the shank - from 0 (shallow) to 7 (deep) - and the letter denotes which of four
distinct shanks the neurons spiking was read from. We observe that the higher frequency interneurons are
placed distant from each other but in a conﬁguration such that they inhibit the low frequency pyramidal cells.

(a) Single periodic component

(b) Two component mixture

(c) (Csicsvari et al.  1999)

Figure 5: A visualization of the periodic component learned by our model. In 5a  the neurons share a single
learned periodic frequency and offset but each learn an individual scaling factor ρn and 5b shows the average
inﬂuence of the two component mixture on the high and low spike rate neurons. In 5c we provide a reproduction
from (Csicsvari et al.  1999) for comparison. In 5a the neurons are colored by ﬁring rate from light (high) to
dark (low). Note that the model learns a frequency that is consistent with the approximately 4 Hz theta rhythm
and there is a dichotomy in the learned amplitudes  ρ  that is consistent with the inﬂuence of the theta rhythm
on pyramidal cells and interneurons.

originating from two classes of neurons  pyramidal cells and interneurons (Csicsvari et al.  1999) 
which are largely separable by their ﬁring rates. Csicsvari et al. (1999) found that interneurons ﬁre
at a rate of 14 ± 1.43 Hz and pyramidal cells at 1.4 ± 0.01 Hz. Interneurons are known to inhibit
pyramidal cells  so we expect interesting inhibitory interactions and anti-correlated spiking between
the pyramidal cells. In our qualitative analysis we visualize the the data by the ﬁring rates of the
neurons to see if the model learns this dichotomy.
Figures 3  4 and 5a show visualizations of the parameters learned by the model with a single periodic
component according to Equation 7. Figure 3 shows the kernel matrix KS corresponding to the
latent embeddings in Figure 4 and the stimulus and gain control weights learned by the model. In
Figure 4 we see the two dimensional embeddings  yn  learned for each neuron by the same model.
In Figure 5 we see the periodic components learned for individual neurons on the hippocampal
data according to Equation 7 when the frequency term f and offset ϕ are shared across neurons.
However  the scaling terms ρn are learned for each neuron  so the neurons can each determine the
inﬂuence of the periodic component on their spiking behavior. Although the parameters are all
randomly initialized at the start of learning  the single frequency signal learned is of approximately
4 Hz which is consistent with the theta rhyhtm that Mizuseki et al. (2009) empirically observed in
these data. In Figures 5a and 5b we see that each neuron’s amplitude component depends strongly

7

−1.0−0.50.00.51.0−0.6−0.4−0.20.00.20.40.60.84a4b5c6c4c6d110SpikeRate(Hz)−0.2−0.10.00.10.2−0.20−0.15−0.10−0.050.000.050.100.150.205a6a0a6b1b3b7c2c3c5d110SpikeRate(Hz)0.00.20.40.60.81.0Time (seconds)0.00.51.01.52.002¼4Hz Phase0.20.40.60.81.01.21.41.61.8Low Spike Rate (Pyr)High Spike Rate (Int)Model
Only Latent
Only Stimulus
Stimulus + Periodic + Latent
Stimulus + Gain + Periodic
Stimulus + Gain
Stimulus + Periodic + Gain + Latent
Stimulus + 2×Periodic + Gain + Latent

Valid Log Likelihood Train Log Likelihood

−3.79
−3.17
−3.07
−3.04
−2.95
−2.74
−2.07

−3.68
−3.29
−2.91
−2.92
−2.84
−2.63
−1.96

Table 1: Model log likelihood on the held out validation set and training set for various combinations of
components. We found the algorithm to be extremely stable. Each model conﬁguration was run 5 times with
different random initializations and the variance of the results was within 10−8.

on the neuron’s ﬁring rate. This is also consistent with the observations of Csicsvari et al. (1999)
that interneurons and pyramidal cells are modulated by the theta rhythm at different amplitudes. We
ﬁnd a strong similarity between the periodic inﬂuence learned by our two component model (5b) to
that in the reproduced ﬁgure (5c) from Csicsvari et al. (1999).
In Table 1 we present the log likelihood of the training data and withheld validation data under
variants of our model after learning the model parameters. The validation data consists of the last
full minute of recording which is 3 000 consecutive 20ms time slices. We see that the likelihood of
the validation data under our model increases as each additional component is added. Interestingly 
adding a second component to the periodic mixture greatly increases the model log likelihood.
Finally  we conduct a leave-one-neuron out prediction experiment on the validation data to compare
the proposed model to the coupled GLM. A spike is predicted if it increases the likelihood under
the model and the accuracy is averaged over all neurons and time slices in the validation set. We
compare GLMs with the periodic component  gain  stimulus and coupling ﬁlters to our DPP with the
latent component. The models did not differ signiﬁcantly in the correct prediction of when neurons
would not spike - i.e. both were 99% correct. However  the DPP predicted 21% of spikes correctly
while the GLM predicted only 5.5% correctly. This may be counterintuitive  as one may not expect a
model for inhibitory interactions to improve prediction of when spikes do occur. However  the GLM
predicts almost no spikes (483 spikes of a possible 92 969)  possibly due to its inability to capture
higher order inhibitory structure. As an example scenario  in a one-of-N neuron ﬁring case the GLM
may prefer to predict that nothing ﬁres (rather than incorrectly predict multiple spikes) whereas the
DPP can actually condition on the behavior of the other neurons to determine which neuron ﬁred.
5 Conclusion
In this paper we presented a novel model for neural spiking data from populations of neurons that is
designed to capture the inhibitory interactions between neurons. The model is empirically validated
on simulated experiments and rat hippocampal neural spike recordings. In analysis of the model
parameters ﬁt to the hippocampus data  we see that it indeed learns known structure and interac-
tions between neurons. The model is able to accurately capture the known interaction between a
dichotomy of neurons and the learned frequency component reﬂects the true modulation of these
neurons by the theta rhythm.
There are numerous possible extensions that would be interesting to explore. A deﬁning feature of
the DPP is an ability to model inhibitory relationships in a neural population; excitatory connections
between neurons are modeled as through the lack of inhibition. Excitatory relationships could be
modeled by incorporating an additional process  such as a Gaussian process  but integrating the
two processes would require some care. Also  a limitation of the current approach is that time
slices are modeled independently. Thus  neurons are not inﬂuenced by their own or others’ spiking
history. The DPP could be extended to include not only spikes from the current time slice but also
neighboring time slices. This will present computational challenges  however  as the DPP scales with
respect to the number of spikes. Finally  we see from Table 1 that the gain modulation and periodic
component are essential to model the hippocampal data. An interesting alternative to the periodic
modulation of individual neuron spiking probabilities would be to have the latent representation
of neurons itself be modulated by a periodic component. This would thus change the inhibitory
relationships to be a function of the theta rhythm  for example  rather than static in time.

8

References
Emery N. Brown. Theory of point processes for neural systems. In Methods and Models in Neuro-

physics  chapter 14  pages 691–726. 2005.

J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  E. J. Chichilnisky  and E. P. Simoncelli.
Spatio-temporal correlations and visual signaling in a complete neuronal population. Nature  454
(7206):995–999  Aug 2008.

Elad Schneidman  Michael J. Berry  Ronen Segev  and William Bialek. Weak pairwise correlations
imply strongly correlated network states in a neural population. Nature  440(7087):1007–1012 
April 2006.

David R. Brillinger. Maximum likelihood analysis of spike trains of interacting nerve cells. Biolog-

ical Cybernetics  59(3):189–200  August 1988.

E.S. Chornoboy  L.P. Schramm  and A.F. Karr. Maximum likelihood identiﬁcation of neural point

process systems. Biological Cybernetics  59(3):265–275  1988.

Liam Paninski. Maximum likelihood estimation of cascade point-process neural encoding models.

Network: Computation in Neural Systems  15(4):243–262  2004.

W. Truccolo  U. T. Eden  M. R. Fellows  J. P. Donoghue  and E. N. Brown. A point process frame-
work for relating neural spiking activity to spiking history  neural ensemble  and extrinsic covari-
ate effects. Journal of Neurophysiology  93(2):1074  2005.

K. D. Harris  J. Csicsvari  H. Hirase  G. Dragoi  and G. Buzsaki. Organization of cell assemblies in

the hippocampus. Nature  424:552–555  2003.

J. Ben Hough  Manjunath Krishnapur  Yuval Peres  and Blint Vir´ag. Determinantal processes and

independence. Probability Surveys  3:206–229  2006.

Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Foundations

and Trends in Machine Learning  5(2–3)  2012.

James Zou and Ryan P. Adams. Priors for diversity in generative latent variable models. In Advances

in Neural Information Processing Systems  2012.

Raja H. Affandi  Alex Kulesza  Emily Fox  and Ben Taskar. Nystr¨om Approximation for Large-

Scale Determinantal Processes. In Artiﬁcial Intelligence and Statistics  2013.

Alex Kulesza and Ben Taskar. Structured determinantal point processes. In Advances in Neural

Information Processing Systems  2011.

Matteo Carandini and David J. Heeger. Normalization as a canonical neural computation. Nature

reviews. Neuroscience  13(1):51–62  January 2012.

Martin J. Wainwright  Odelia Schwartz  and Eero P. Simoncelli. Natural image statistics and divisive
normalization: Modeling nonlinearity and adaptation in cortical neurons. In R Rao  B Olshausen 
and M Lewicki  editors  Probabilistic Models of the Brain: Perception and Neural Function 
chapter 10  pages 203–222. MIT Press  February 2002.

J. Csicsvari  H. Hirase  A. Czurk´o  A. Mamiya  and G. Buzs´aki. Oscillatory coupling of hippocampal
pyramidal cells and interneurons in the behaving rat. The Journal of Neuroscience  19(1):274–
287  jan 1999.

Carl E. Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT

Press  2006.

Kenji Mizuseki  Anton Sirota  Eva Pastalkova  and Gy¨orgy Buzs´aki. Theta oscillations provide
temporal windows for local circuit computation in the entorhinal-hippocampal loop. Neuron  64
(2):267–280  October 2009.

9

,Jasper Snoek
Richard Zemel
Ryan Adams