2017,Stochastic Mirror Descent in Variationally Coherent Optimization Problems,In this paper  we examine a class of non-convex stochastic optimization problems which we call variationally coherent  and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems  we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case)  and we show that the last iterate of SMD converges to the problem’s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather  variational coherence  a much weaker requirement  suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.,Stochastic Mirror Descent in

Variationally Coherent Optimization Problems

Zhengyuan Zhou
Stanford University

zyzhou@stanford.edu

Panayotis Mertikopoulos

Univ. Grenoble Alpes  CNRS  Inria  LIG
panayotis.mertikopoulos@imag.fr

Nicholas Bambos
Stanford University

bambos@stanford.edu

Stephen Boyd

Stanford University
boyd@stanford.edu

Peter Glynn

Stanford University

glynn@stanford.edu

Abstract

In this paper  we examine a class of non-convex stochastic optimization problems
which we call variationally coherent  and which properly includes pseudo-/quasi-
convex and star-convex optimization problems. To solve such problems  we focus
on the widely used stochastic mirror descent (SMD) family of algorithms (which
contains stochastic gradient descent as a special case)  and we show that the
last iterate of SMD converges to the problem’s solution set with probability 1.
This result contributes to the landscape of non-convex stochastic optimization by
clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for
(almost sure) global convergence; rather  variational coherence  a much weaker
requirement  sufﬁces. Characterization of convergence rates for the subclass of
strongly variationally coherent optimization problems as well as simulation results
are also presented.

1

Introduction

The stochastic mirror descent (SMD) method and its variants[1  7  8] is arguably one of the most
widely used family of algorithms in stochastic optimization – convex and non-convex alike. Starting
with the orginal work of [16]  the convergence of SMD has been studied extensively in the context of
convex programming (both stochastic and deterministic)  saddle-point problems  and monotone varia-
tional inequalities. Some of the most important contributions in this domain are due to Nemirovski et
al. [15]  Nesterov [18] and Xiao [23]  who provided tight convergence bounds for the ergodic average
of SMD in stochastic/online convex programs  variational inequalities  and saddle-point problems.
These results were further boosted by recent work on extra-gradient variants of the algorithm [11  17] 
and the ergodic relaxation of [8] where the independence assumption on the gradient samples is
relaxed and is replaced by a mixing distribution that converges in probability to a well-deﬁned limit.
However  all these works focus exclusively on the algorithm’s ergodic average (also known as time-
average)  a mode of convergence which is strictly weaker than the convergence of the algorithm’s last
iterate. In addition  most of the analysis focuses on establishing convergence "in expectation" and
then leveraging sophisticated martingale concentration inequalities to derive "large deviations" results
that hold true with high probability. Last (but certainly not least)  the convexity of the objective plays
a crucial role: thanks to the monotonicity of the gradient  it is possible to exploit regret-like bounds
and transform them to explicit convergence rates.1

1For the role of variational monotonicity in the context of convex programming  see also [22].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

By contrast  the gradient operator of the non-convex programs studied in this paper does not satisfy
any reasonable monotonicity property (such as quasi-/pseudo-monotonicity  monotonicity-plus  or
any of the standard variants encountered in the theory of variational inequalities [9]. Furthermore 
given that there is no inherent averaging in the algorithm’s last iterate  it is not possible to employ a
regret-based analysis such as the one yielding convergence in convex programs. Instead  to establish
convergence  we use the stochastic approximation method of Benaïm and Hirsch [2  3] to compare the
evolution of the SMD iterates to the ﬂow of a mean  underlying dynamical system.2 By a judicious
application of martingale limit theory  we then exploit variational coherence to show that the last
iterate of SMD converges with probability 1  recovering in the process a large part of the convergence
analysis of the works mentioned above.

Our Contributions. We consider a class of non-convex optimization problems  which we call
variationally coherent and which strictly includes convex  pseudo/quasi-convex and star-convex
optimization problems. For this class of optimization problems  we show that the last iterate of
SMD with probability 1 to a global minimum under i.i.d. gradient samples. To the best of our
knowledge  this strong convergence guarantee (almost sure of the last iterate of SMD) is not known
even for stochastic convex problems. As such  this results contributes to the landscape of non-convex
stochastic optimization by making clear that neither pseudo-/quasi-convexity nor star-convexity is
essential for global convergence; rather  variational coherence  a much weaker requirement  sufﬁces.
Our analysis leverages the Lyapunov properties of the Fenchel coupling [14]  a primal-dual divergence
measure that quantiﬁes the distance between primal (decision) variables and dual (gradient) variables 
and which serves as an energy function to establish recurrence of SMD (Theorem 3.4). Building
on this recurrence  we consider an ordinary differential equation (ODE) approximation of the SMD
scheme and  drawing on various results from the theory of stochastic approximation and variational
analysis  we connect the solution of this ODE to the last iterate of SMD. In so doing  we establish the
algorithm’s convergence with probability 1 from any initial condition (Thereom 4.4) and  to complete
the circle  we also provide a convergence rate estimate for the subclass of strongly variationally
coherent optimization problems.
Importantly  although the ODE approximation of discrete-time Robbins–Monro algorithms has been
widely studied in control and stochastic optimization [10  13]  converting the convergence guarantees
of the ODE solution back to the discrete-time process is a fairly subtle affair that must be done on
an case-by-case basis. Further  even if such conversion goes through  the results typically have the
nature of convergence-in-distribution: almost sure convergence is much harder to obtain [5].

2 Setup and Preliminaries
Let X be a convex compact subset of a d-dimensional real space V with norm (cid:107)·(cid:107). Throughout this
paper  we focus on the stochastic optimization problem

where the objective function g : X → R is of the form

minimize
g(x) 
subject to x ∈ X  

(Opt)

g(x) = E[G(x; ξ)]

(2.1)
for some random function G : X × Ξ → R deﬁned on an underlying (complete) probability space
(Ξ F  P). We make the following assumptions regarding (Opt):
Assumption 1. G(x  ξ) is continuously differentiable in x for almost all ξ ∈ Ξ.
Assumption 2. ∇G(x; ξ) has bounded second moments and is Lipschitz continuous in the mean:
E[(cid:107)∇G(x; ξ)(cid:107)2∗] < ∞ for all x ∈ X and E[∇G(x; ξ)] is Lipschitz on X .3
Assumption 1 is a token regularity assumption which can be relaxed to account for nonsmooth
objectives by using subgradient devices (as opposed to gradients). However  this would make

2For related approaches based on the theory of dynamical systems  see [21] and [12].
3In the above  gradients are treated as elements of the dual space V∗ of V and (cid:107)v(cid:107)∗ = sup{(cid:104)v  x(cid:105) : (cid:107)x(cid:107) ≤ 1}
denotes the dual norm of v ∈ V∗. We also note that ∇G(x; ξ) refers to the gradient of G(x; ξ) with respect to
x; since Ξ need not have a differential structure  there is no danger of confusion.

2

the presentation signiﬁcantly more cumbersome  so we stick with smooth objectives throughout.
Assumption 2 is also standard in the stochastic optimization literature: it holds trivially if ∇G is
uniformly Lipschitz (another commonly used condition) and  by the dominated convergence theorem 
it further implies that g is smooth and ∇g(x) = ∇ E[G(x; ξ)] = E[∇G(x; ξ)] is Lipschitz continuous.
As a result  the solution set
of (Opt) is closed and nonempty (by the compactness of X and the continuity of g).
Remark 2.1. An important special case of (Opt) is when G(x; ξ) = g(x) +(cid:104)ζ  x(cid:105) for some V∗-valued
random vector ζ such that E[ζ] = 0 and E[(cid:107)ζ(cid:107)2∗] < ∞. This gives ∇G(x; ξ) = ∇g(x) + ζ  so (Opt)
can also be seen as a model for deterministic optimization problems with noisy gradient observations.

X ∗ = arg min g

(2.2)

2.1 Variational Coherence

With all this at hand  we now deﬁne the class of variationally coherent optimization problems:
Deﬁnition 2.1. We say that (Opt) is variationally coherent if

(cid:104)∇g(x)  x − x∗(cid:105) ≥ 0

for all x ∈ X   x∗ ∈ X ∗ 

(VC)

with equality if and only if x ∈ X ∗.
Remark 2.2. (VC) can be interpreted in two ways. First  as stated  it is a non-random condition for
g  so it applies equally well to deterministic optimization problems (with or without noisy gradient
observations). Alternatively  by the dominated convergence theorem  (VC) can be written as:

E[(cid:104)∇G(x; ξ)  x − x∗(cid:105)] ≥ 0.

(2.3)
In this form  it can be interpreted as saying that G is variationally coherent “on average”  without any
individual realization thereof satisfying (VC).
Remark 2.3. Importantly  (VC) does not have to be stated in terms of the solution set of (Opt). Indeed 
assume that C is a nonempty subset of X such that

(cid:104)∇g(x)  x − p(cid:105) ≥ 0

for all x ∈ X   p ∈ C 

(2.4)
with equality if and only if x ∈ C. Then  as the next lemma (see appendix) indicates  C = arg min g:
Lemma 2.2. Suppose that (2.4) holds for some nonempty subset C of X . Then C is closed  convex 
and it consists precisely of the global minimizers of g.
Corollary 2.3. If (Opt) is variationally coherent  arg min g is convex and compact.

Remark 2.4. All the results given in this paper also carry through for λ-variationally coherent
optimization problems  a further generalization of variational coherence. More precisely  we say that
(Opt) is λ-variationally coherent if there exists a (component-wise) positive vector λ ∈ Rd such that

d(cid:88)

i=1

λi

∂g
∂xi

(xi − x∗

i ) ≥ 0

for all x ∈ X   x∗ ∈ X ∗ 

(2.5)

with equality if and only if x ∈ X ∗. For simplicity  our analysis will be carried out in the “vanilla"
variational coherence framework  but one should keep in mind that the results to following also hold
for λ-coherent problems.

2.2 Examples of Variational Coherence
Example 2.1 (Convex programs). If g is convex  ∇g is a monotone operator [19]  i.e.

(cid:104)∇g(x) − ∇g(x(cid:48))  x − x(cid:48)(cid:105) ≥ 0

(2.6)
By the ﬁrst-order optimality conditions for g  we have (cid:104)g(x∗)  x − x∗(cid:105) ≥ 0 for all x ∈ X . Hence  by
monotonicity  we get

for all x  x(cid:48) ∈ X .

(cid:104)∇g(x)  x − x∗(cid:105) ≥ (cid:104)∇g(x∗)  x − x∗(cid:105) ≥ 0

(2.7)
By convexity  it follows that (cid:104)∇g(x)  x − x∗(cid:105) < 0 whenever x∗ ∈ X ∗ and x ∈ X \ X ∗  so equality
holds in (2.7) if and only if x ∈ X ∗.

for all x ∈ X   x∗ ∈ X ∗.

3

(cid:104)∇g(x)  x(cid:48) − x(cid:105) ≥ 0 =⇒ g(x(cid:48)) ≥ g(x) 

Example 2.2 (Pseudo/Quasi-convex programs). The previous example shows that variational coher-
ence is a weaker and more general notion than convexity and/or operator monotonicity. In fact  as we
show below  the class of variationally coherent problems also contains all pseudo-convex programs 
i.e. when
for all x  x(cid:48) ∈ X . In this case  we have:
Proposition 2.4. If g is pseudo-convex  (Opt) is variationally coherent.
Proof. Take x∗ ∈ X ∗ and x ∈ X \ X ∗  and assume ad absurdum that (cid:104)∇g(x)  x − x∗(cid:105) ≤ 0. By
(PC)  this implies that g(x∗) ≥ g(x)  contradicting the choice of x and x∗. We thus conclude that
(cid:104)∇g(x)  x− x∗(cid:105) > 0 for all x∗ ∈ X ∗  x ∈ X \X ∗; since (cid:104)∇g(x)  x− x∗(cid:105) ≤ 0 if x ∈ X ∗  our claim
(cid:4)
follows by continuity.

(PC)

We recall that every convex function is pseudo-convex  and every pseudo-convex function is quasi-
convex (i.e. its sublevel sets are convex). Both inclusions are proper  but the latter is fairly thin:
Proposition 2.5. Suppose that g is quasi-convex and non-degenerate  i.e.

(2.8)
where TC(x) is the tangent cone vertexed at x. Then  g is pseudo-convex (and variationally coherent).

(cid:104)g(x)  z(cid:105) (cid:54)= 0 for all nonzero z ∈ TC(x)  x ∈ X \ X ∗ 

Proof. This follows from the following characterization of quasi-convex functions [6]: g is quasi-
convex if and only if g(x(cid:48)) ≤ g(x) implies that (cid:104)∇g(x)  x(cid:48) − x(cid:105) ≤ 0. By contraposition  this yields
the strict part of (PC)  i.e. g(x(cid:48)) > g(x) whenever (cid:104)∇g(x)  x(cid:48) − x(cid:105) > 0. To complete the proof  if
(cid:104)∇g(x)  x(cid:48) − x(cid:105) = 0 and x ∈ X ∗  (PC) is satisﬁed trivially; otherwise  if (cid:104)∇g(x)  x(cid:48) − x(cid:105) = 0 but
x ∈ X \ X ∗  (2.8) implies that x(cid:48) − x = 0  so g(x(cid:48)) = g(x) and (PC) is satisﬁed as an equality. (cid:4)

The non-degeneracy condition (2.8) is satisﬁed by every quasi-convex function after an arbitrarily
small perturbation leaving its minimum set unchanged. By this token  Propositions 2.4 and 2.5 imply
that essentially all quasi-convex programs are also variationally coherent.
Example 2.3 (Star-convex programs). If g is star-convex  then (cid:104)∇g(x)  x − x∗(cid:105) ≥ g(x) − g(x∗)
for all x ∈ X   x∗ ∈ X ∗. This is easily seen to be a special case of variational coherence because
(cid:104)∇g(x)  x − x∗(cid:105) ≥ g(x) − g(x∗) ≥ 0  with the last inequality strict unless x ∈ X ∗. Note that
star-convex functions contain convex functions as a subclass (but not necessarily pseudo/quasi-convex
functions).
Example 2.4 (Beyond quasi-/star-convexity). A simple example of a function that is variationally
coherent without being quasi-convex or star-convex is given by:

g(x) = 2

√

1 + xi 

x ∈ [0  1]d.

(2.9)

d(cid:88)

i=1

When d ≥ 2  it is easy to see g is not quasi-convex: for instance  taking d = 2  x = (0  1)
√
2 = max{g(x)  g(x(cid:48))}  so g is not quasi-
and x(cid:48) = (1  0) yields g(x/2 + x(cid:48)/2) = 2
convex. It is also instantly clear this function is not star-convex even when d = 1 (in which case
it is a concave function). On the other hand  to estabilish (VC)  simply note that X ∗ = {0} and
1 + xi > 0 for all x ∈ [0  1]d\{0}. For a more elaborate example of

(cid:104)∇g(x)  x − 0(cid:105) =(cid:80)d

√
6 > 2

√

i=1 xi/

a variationally coherent problem that is not quasi-convex  see Figure 2.

2.3 Stochastic Mirror Descent

To solve (Opt)  we focus on the widely used family of algorithms known as stochastic mirror descent
(SMD)  formally given in Algorithm 1.4 Heuristically  the main idea of the method is as follows: At
each iteration  the algorithm takes as input an independent and identically distributed (i.i.d.) sample

4Mirror descent dates back to the original work of Nemirovski and Yudin [16]. More recent treatments
include [1  8  15  18  20] and many others; the speciﬁc variant of SMD that we are considering here is most
closely related to Nesterov’s “dual averaging” scheme [18].

4

Y0
Y = V∗

Q

X ⊆ V

−α1∇G(X0; ξ1)

−α2∇G(X1; ξ2)

Y1

Q

X0

Y2

Q

X2

Q

X1

Figure 1: Schematic representation of stochastic mirror descent (Algorithm 1).

of the gradient of G at the algorithm’s current state. Subsequently  the method takes a step along
this stochastic gradient in the dual space Y ≡ V∗ of V (where gradients live)  the result is “mirrored”
back to the problem’s feasible region X to obtain a new solution candidate  and the process repeats.
In pseudocode form  we have:

Algorithm 1 Stochastic mirror descent (SMD)
Require: Initial score variable Y0
1: n ← 0
2: repeat
3: Xn = Q(Yn)
4:
5:
6: until end
7: return solution candidate Xn

Yn+1 = Yn − αn+1∇G(Xn  ξn+1)
n ← n + 1

In the above representation  the key elements of SMD (see also Fig. 1) are:

1. The “mirror map” Q : Y → X that outputs a solution candidate Xn ∈ X as a function of the

auxiliary score variable Yn ∈ Y. In more detail  the algorithm’s mirror map Q is deﬁned as

Q(y) = arg max

x∈X

{(cid:104)y  x(cid:105) − h(x)} 

(2.10)

where h(x) is a strongly convex function that plays the role of a regularizer. Different choices
of the regularizer h yields different speciﬁc algorithm. Due to space limitation  we mention in
passing two well-known examples: When h(x) = 1
2 (i.e. Euclidean regularizer)  mirror
i=1 xi log xi (i.e. entropic regularizer) 

descent becomes gradient descent. When h(x) =(cid:80)d
∞(cid:88)

2. The step-size sequence αn > 0  chosen to satisfy the “(cid:96)2 − (cid:96)1” summability condition:

mirror descent becomes exponential gradient (aka exponential weights).

∞(cid:88)

2(cid:107)x(cid:107)2

n < ∞ 
α2

αn = ∞.

(2.11)

3. A sequence of i.i.d. gradient samples ∇G(x; ξn+1).5

n=1

n=1

3 Recurrence of SMD

In this section  we characterize an interesting recurrence phenomenon that will be useful later for
establishing global convergence. Intuitively speaking  for a variationally coherent program of the

5The speciﬁc indexing convention for ξn has been chosen so that Yn and Xn are both adapted to the natural

ﬁltration Fn of ξn.

5

general form(Opt)  any neighborhood of X ∗ will almost surely be visited by iterates Xn inﬁnitely
often. Note that this already implies that at least a subsequence of iterates converges to global
minima almost surely. To that end  we ﬁrst deﬁne an important divergence measure between a primal
variable x and a dual variable y  called Fenchel coupling  that plays an indispensable role of an energy
function.
Deﬁnition 3.1. Let h : X → R be a regularizer with respect to (cid:107) · (cid:107) that is K-strongly convex.

1. The convex conjugate function h∗ : Rn → R of h is deﬁned as:

h∗(y) = max

x∈X {(cid:104)x  y(cid:105) − h(x)}.

2. The mirror map Q : Rn → X associated with the regularizer h is deﬁned as:

Q(y) = arg max

x∈X {(cid:104)x  y(cid:105) − h(x)}.

3. The Fenchel coupling F : X × Rn → R is deﬁned as:

F (x  y) = h(x) − (cid:104)x  y(cid:105) + h∗(y).

Note that the naming of Fenchel coupling is natural as it consists of all the terms in the well-known
Fenchel’s inequality: h(x) + h∗(y) ≥ (cid:104)x  y(cid:105). The Fenchel’s inequality says that Fenchel coupling is
always non-negative. As indicated by part 1 of the following lemma  a stronger result can be obtained.
We state the two key properties Fenchel coupling next.
Lemma 3.2. Let h : X → R be a K-strongly convex regularizer on X . Then:

1. F (x  y) ≥ 1
2. F (x  ˜y) ≤ F (x  y) + (cid:104)˜y − y  Q(y) − x(cid:105) + 1

2 K(cid:107)Q(y) − x(cid:107)2 ∀x ∈ X  ∀y ∈ Rn.

2K(cid:107)˜y − y(cid:107)2∗ ∀x ∈ X  ∀˜y  y ∈ Rn.

We assume that we are working with mirror maps that are regular in the following weak sense:6
Assumption 3. The mirror map Q is regular: if Q(yn) → x  then F (x  yn) → 0.
Deﬁnition 3.3. Given a point x ∈ X   a set S ⊂ X and a norm (cid:107) · (cid:107).

1. Deﬁne the point-to-set normed distance and Fenchel coupling distance respectively as:

dist(x S) (cid:44) inf s∈S (cid:107)x − s(cid:107) and F (S  y) = inf s∈S F (s  y).
2. Given ε > 0  deﬁne B(S  ε) (cid:44) {x ∈ X | dist(x S) < ε}.
3. Given δ > 0  deﬁne ˜B(S  δ) (cid:44) {Q(y) | F (S  y) < δ}.

We then have the following recurrence result for a variationally coherent optimization problem Opt.
Theorem 3.4. Under Assumptions 1–3  for any ε > 0  δ > 0 and any Xn  the (random) iterates Xn
generated in Algorithm 1 enter both B(X ∗  ε) and ˜B(X ∗  δ) inﬁnitely often almost surely.

4 Global Convergence Results

4.1 Deterministic Convergence
When a perfect gradient ∇g(x) is available (in Line 4 of Algorithm 1)  SMD recovers its deterministic
counterpart: mirror descent (Algorithm 2). We ﬁrst characterize global convergence in this case.

6Mirror maps induced by many common regularizers are regular  including the Euclidean regularizer and the

entropic regularizer.

6

Figure 2: Convergence of stochastic mirror descent for the mean objective g(r  θ) = (2 + cos θ/2 +
cos(4θ))r2(5/3 − r) expressed in polar coordinates over the unit ball (r ≤ 1). In the left subﬁgure  we
have plotted the graph of g; the plot to the right superimposes a typical SMD trajectory over the contours of g.

Algorithm 2 Mirror descent (MD)
Require: Initial score variable y0
1: n ← 0
2: repeat
3:
4:
5:
6: until end
7: return solution candidate xn

xn = Q(yn)
xn+1 = xn − αn+1∇g(xn)
n ← n + 1

Theorem 4.1. Consider an optimization problem Opt that is variationally coherent. Let xn be the
iterates generated by MD. Under Assumption 3  limt→∞ dist(xn X ∗) = 0  for any y0.
Remark 4.1. Here we do not require ∇ g(x) to be Lipschitz continuous. If ∇ g(x) is indeed (locally)
Lipschitz continuous  then Theorem 4.1 follows directly from Theorem 4.4. Otherwise  Theorem 4.1
requires a different argument  brieﬂy outlined as follows. Theorem 3.4 implies that (in the special
case of perfect gradient)  iterates xn generated from MD enter B(X ∗  ε) inﬁnitely often. Now  by
exploiting the properties of Fenchel coupling on a ﬁner-grained level (compared to only using it to
establish recurrence)  we can establish that for any ε-neighborhood B(X ∗  ε)  after a certain number
of iterations  once the iterate xn enters B(X ∗  ε)  it will never exit. Convergence therefore follows.

4.2 Stochastic Almost Sure Convergence

We begin with minimal mathematical preliminaries [4] needed that will be needed.
Deﬁnition 4.2. A semiﬂow Φ on a metric space (M  d) is a continuous map Φ : R+ × M → M:

(t  x) → Φt(x) 

such that the semi-group properties hold: Φ0 = identity  Φt+s = Φt ◦ Φs for all (t  s) ∈ R+ × R+.
Deﬁnition 4.3. Let Φ be a semiﬂow on the metric space (M  d). A continuous function s : R+ → M
is an asymptotic pseudotrajectory (APT) for Φ if for every T > 0  the following holds:

t→∞ sup
lim
0≤h≤T

d(s(t + h)  Φh(s(t))) = 0.

(4.1)

We are now ready to state the convergence result. See Figure 2 for a simulation example.
Theorem 4.4. Consider an optimization problem Opt that is variationally coherent. Let Xn be the
iterates generated by SMD (Algorithm 1). Under Assumptions 1–3  if ∇ g(x) is locally Lipschitz
continuous on X   then dist(xn X ∗) → 0 almost surely as t → ∞  irrespective of Y0.

7

-���-������������-���-������������������������������������������������Remark 4.2. The proof is rather involved and contains several ideas. To enhance the intuition and
understanding  we outline the main steps here  each of which will be proved in detail in the appendix.
To simplify the notation  we assume there is a unique optimal (i.e. X ∗ is a singleton set). The proof is
identical in the multiple minima case  provide we replace x∗ by X ∗ and use the point-to-set distance.

1. We consider the following ODE approximation of SMD:

˙y = v(x) 
x = Q(y) 

where v(x) = −∇ g(x). We verify that the ODE admits a unique solution for y(t) for any
initial condition. Consequently  this solution induces a semiﬂow7  which we denote Φt(y):
it is the state at time t given it starts at y initially. Note that we have used y as the initial
point (as opposed to y0) to indicate that the semiﬂow representing the solution trajectory
should be viewed as a function of the initial point y.

the SMD iterates Y1  Y2  . . .   Yk  . . . at times 0  α1  α1 + α2  . . .  (cid:80)k−1

2. We now relate the iterates generated by SMD to the above ODE’s solution. Connect linearly
i=0 αi  . . . respectively
to form a continuous  piecewise afﬁne (random) curve Y (t). We then show that Y (t) is
almost surely an asymptotic pseudotrajectory of the semi-ﬂow Φ induced by the above ODE.

3. Having characterized the relation between the SMD trajectory (afﬁne interpolation of the
discrete SMD iterates) and the ODE trajectory (the semi-ﬂow)  we now turn to studying
the latter (the semiﬂow given by the ODE trajectory). A desirable property of Φt(y) is that
the distance F (x∗  Φt(y)) between the optimal solution x∗ and the dual variable Φt(y) (as
measured by Fenchel coupling) can never increase as a function of t. We refer to this as the
monotonicity property of Fenchel coupling under the ODE trajectory  to be contrasted to the
discrete-time dynamics  where such monotonicity is absent (even when perfect information
on the gradient is available). More formally  we show that ∀y ∀0 ≤ s ≤ t 

F (x∗  Φs(y)) ≥ F (x∗  Φt(y)).

(4.2)
4. Continuing on the previous point  not only the distance F (x∗  Φt(y)) can never increase as
t increases  but also  provided that Φt(y) is not too close to x∗  F (x∗  Φt(y)) will decrease
no slower than linearly. This suggests that either Φt(y) is already close to x∗ (and hence
x(t) = Q(Φt(y)) is close to x∗)  or their distance will be decreased by a meaningful amount
in (at least) the ensuing short time-frame. We formalize this discussion as follows:

∀ε > 0 ∀y ∃s > 0  F (x∗  Φs(y)) ≤ max{ ε
2

  F (x∗  y) − ε
2

}.

(4.3)

5. Now consider an arbitrary ﬁxed horizon T . If at time t  F (x∗  Φ0(Y (t))) is small  then
by the monotonicity property in Claim 3  F (x∗  Φh(Y (t))) will remain small on the entire
interval h ∈ [0  T ]. Since Y (t) is an asymptotic pseudotrajectory of Φ (Claim 2)  Y (t + h)
and Φh(Y (t)) should be very close for h ∈ [0  T ]  at least for t large enough. This means
that F (x∗  Y (t + h)) should also be small on the entire interval h ∈ [0  T ]. This can be
made precise as follows: ∀ε  T > 0 ∃τ (ε  T ) > 0 such that ∀t ≥ τ ∀h ∈ [0  T ]:

F (x∗  Y (t + h)) < F (x∗  Φh(Y (t))) +

ε
2

  a.s..

(4.4)

6. Finally  we are ready to put the above pieces together. Claim 5 gives us a way to control
the amount by which the two Fenchel coupling functions differ on the interval [0  T ].
Claim 3 and Claim 4 together allow us to extend such control over successive intervals
[T  2T )  [2T  3T )  . . .   thereby establishing that  at least for t large enough  if F (x∗  Y (t))
is small  then F (x∗  Y (t + h)) will remains small ∀h > 0. As it turns out  this means that
after long enough time  if xn ever visits ˜B(x∗  ε)  it will (almost surely) be forever trapped
inside the neighborhood twice that size (i.e. ˜B(x∗  2ε)). Since Theorem 3.4 ensures that
xn visits ˜B(x∗  ε) inﬁnitively often (almost surely)  the hypothesis is guaranteed to be true.
Consequently  this leads to the following claim: ∀ε > 0 ∃τ0 (a positive integer)  such that:
(4.5)

F (x∗  Y (τ0 + h)) < ε ∀h ∈ [0 ∞)  a.s..

7A crucial point to note is that since C may not be invertible  there may not exist a unique solution for x(t).

8

Figure 3: SMD run on the objective function of Fig. 2 with γn ∝ n−1/2 and Gaussian random noise with
standard deviation about 150% the mean value of the gradient. Due to the lack of convexity  the algorithm’s last
iterate converges much faster than its ergodic average.
To conclude  Equation (4.5) implies that F (x∗  Yn) → 0  a.s. as t → ∞  where the SMD iterates Yn
are values at integer time points of the afﬁne trajectory Y (τ ). Per Statement 1 in Lemma 3.2  this
gives (cid:107)Q(Yn) − x∗(cid:107) → 0  a.s. as t → ∞  thereby establishing that Xn = Q(Yn) → x∗  a.s..

4.3 Convergence Rate Analysis

At the level of generality at which (VC) has been stated  it is unlikely that any convergence rate can
be obtained  because unlike in the convex case  one has no handle on measuring the progress of
mirror descent updates (recall that in (VC)  only non-negativity is guaranteed for the inner product).
√
Consequently  we focus here on the class of strongly coherent problems (a generalization of strongly
convex problems) and derive a O(1/
T ) convergence rate in terms of the squared distance to a
solution of (Opt).
Deﬁnition 4.5. We say that g is c-strongly variationally coherent (or c-strongly coherent for short)
if  for some x∗ ∈ X   we have:

(cid:104)∇g(x)  x − x∗(cid:105) ≥ c
2

(cid:107)x − x∗(cid:107)2

for all x ∈ X .

(cid:80)T

(cid:80)T

(4.6)

  where

n

Theorem 4.6. If (Opt) is c-strongly coherent  then (cid:107)¯xT − x∗(cid:107)2 ≤ 2
¯xT =

n=0 γ2
  K is the strong convexity coefﬁcient of h and B = maxx∈X (cid:107)∇ g(x)(cid:107)2∗.

(cid:80)T
(cid:80)T

n=0 γnxn

F (x∗ y0)+ B

2K

c

n=0 γn

n=0 γn

T

n  then (cid:107)¯xT − x∗(cid:107)2 = O( log T√

The proof of Theorem 4.6 is given in the supplement. We mention a few implications of Theorem 4.6.
First  in a strongly coherent optimization problem  if γn = 1√
) (note
that here (cid:96)2 − (cid:96)1 summability is not required for global convergence). By appropriately choosing
√
the step-size sequence  one can further shave off the log T term above and obtain an O(1/
T )
convergence rate. This rate matches existing rates when applying gradient descent to strongly convex
functions  although strongly variational coherence is a strict superset of strong convexity. Finally 
√
note that even though we have characterized the rates in the mirror descent (i.e. perfect gradient case) 
one can easily obtain a mean O(1/
T ) rate in the stochastic case by using a similar argument. This
discussion is omitted due to space limitation.
We end the section (and the paper) with an interesting observation from the simulation shown in
Figure 3. The rate characterized in Theorem 4.6 is with respect to the ergodic average of the mirror
descent iterates  while global convergence results established in Theorem 4.1 and Theorem 4.4 are
both last iterate convergence. Figure 3 then provides a convergence speed comparison on the function
given in Figure 2. It is apparent that the last iterate of SMD (more speciﬁcally  stochastic gradient
descent) converges much faster than the ergodic average in this non-convex objective.

9

△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△△◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇◇△�����������◇�������������������������-���������������������������������������������������������������������������5 Acknowledgments

Zhengyuan Zhou is supported by Stanford Graduate Fellowship and would like to thank Yinyu Ye
and Jose Blanchet for constructive discussions and feedback. Panayotis Mertikopoulos gratefully
acknowledges ﬁnancial support from the Huawei Innovation Research Program ULTRON and the
ANR JCJC project ORACLESS (grant no. ANR–16–CE33–0004–01).

References
[1] A. BECK AND M. TEBOULLE  Mirror descent and nonlinear projected subgradient methods for convex

optimization  Operations Research Letters  31 (2003)  pp. 167–175.

[2] M. BENAÏM  Dynamics of stochastic approximation algorithms  in Séminaire de Probabilités XXXIII 
J. Azéma  M. Émery  M. Ledoux  and M. Yor  eds.  vol. 1709 of Lecture Notes in Mathematics  Springer
Berlin Heidelberg  1999  pp. 1–68.

[3] M. BENAÏM AND M. W. HIRSCH  Asymptotic pseudotrajectories and chain recurrent ﬂows  with applica-

tions  Journal of Dynamics and Differential Equations  8 (1996)  pp. 141–176.

[4] M. BENAÏM AND M. W. HIRSCH  Asymptotic pseudotrajectories and chain recurrent ﬂows  with applica-

tions  Journal of Dynamics and Differential Equations  8 (1996)  pp. 141–176.

[5] V. S. BORKAR  Stochastic Approximation: A Dynamical Systems Viewpoint  Cambridge University Press

and Hindustan Book Agency  2008.

[6] S. BOYD AND L. VANDENBERGHE  Convex Optimization  Berichte über verteilte messysteme  Cambridge

University Press  2004.

[7] N. CESA-BIANCHI  P. GAILLARD  G. LUGOSI  AND G. STOLTZ  Mirror descent meets ﬁxed share (and

feels no regret)  in Advances in Neural Information Processing Systems  989-997  ed.  vol. 25  2012.

[8] J. C. DUCHI  A. AGARWAL  M. JOHANSSON  AND M. I. JORDAN  Ergodic mirror descent  SIAM

Journal on Optimization  22 (2012)  pp. 1549–1578.

[9] F. FACCHINEI AND J.-S. PANG  Finite-Dimensional Variational Inequalities and Complementarity

Problems  Springer Series in Operations Research  Springer  2003.

[10] S. GHADIMI AND G. LAN  Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic program-

ming  SIAM Journal on Optimization  23 (2013)  pp. 2341–2368.

[11] A. JUDITSKY  A. S. NEMIROVSKI  AND C. TAUVEL  Solving variational inequalities with stochastic

mirror-prox algorithm  Stochastic Systems  1 (2011)  pp. 17–58.

[12] W. KRICHENE  A. BAYEN  AND P. BARTLETT  Accelerated mirror descent in continuous and discrete
time  in NIPS ’15: Proceedings of the 29th International Conference on Neural Information Processing
Systems  2015.

[13] H. KUSHNER AND G. YIN  Stochastic Approximation and Recursive Algorithms and Applications 

Stochastic Modelling and Applied Probability  Springer New York  2013.

[14] P. MERTIKOPOULOS  Learning in games with continuous action sets and unknown payoff functions.

https://arxiv.org/abs/1608.07310  2016.

[15] A. S. NEMIROVSKI  A. JUDITSKY  G. G. LAN  AND A. SHAPIRO  Robust stochastic approximation

approach to stochastic programming  SIAM Journal on Optimization  19 (2009)  pp. 1574–1609.

[16] A. S. NEMIROVSKI AND D. B. YUDIN  Problem Complexity and Method Efﬁciency in Optimization 

Wiley  New York  NY  1983.

[17] Y. NESTEROV  Dual extrapolation and its applications to solving variational inequalities and related

problems  Mathematical Programming  109 (2007)  pp. 319–344.

[18]

  Primal-dual subgradient methods for convex problems  Mathematical Programming  120 (2009) 

pp. 221–259.

[19] R. T. ROCKAFELLAR AND R. J. B. WETS  Variational Analysis  vol. 317 of A Series of Comprehensive

Studies in Mathematics  Springer-Verlag  Berlin  1998.

[20] S. SHALEV-SHWARTZ  Online learning and online convex optimization  Foundations and Trends in

Machine Learning  4 (2011)  pp. 107–194.

[21] W. SU  S. BOYD  AND E. J. CANDÈS  A differential equation for modeling Nesterov’s accelerated gradient
method: Theory and insights  in NIPS ’14: Proceedings of the 27th International Conference on Neural
Information Processing Systems  2014  pp. 2510–2518.

[22] A. WIBISONO  A. C. WILSON  AND M. I. JORDAN  A variational perspective on accelerated methods in
optimization  Proceedings of the National Academy of Sciences of the USA  113 (2016)  pp. E7351–E7358.
[23] L. XIAO  Dual averaging methods for regularized stochastic learning and online optimization  Journal of

Machine Learning Research  11 (2010)  pp. 2543–2596.

10

,Zhengyuan Zhou
Panayotis Mertikopoulos
Stephen Boyd
Peter Glynn