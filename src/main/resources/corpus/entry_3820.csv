2009,Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution,We present a sequence of unsupervised  nonparametric Bayesian models for clustering  complex linguistic objects. In this approach  we consider a potentially infinite number of features and categorical outcomes. We evaluate these models for the task  of within- and cross-document event coreference on two corpora.  All the models we investigated show significant improvements when compared against an existing baseline for this task.,Nonparametric Bayesian Models for Unsupervised

Event Coreference Resolution

Cosmin Adrian Bejan1  Matthew Titsworth2  Andrew Hickl2  & Sanda Harabagiu1

1 Human Language Technology Research Institute  University of Texas at Dallas

2 Language Computer Corporation  Richardson  Texas

ady@hlt.utdallas.edu

Abstract

We present a sequence of unsupervised  nonparametric Bayesian models for clus-
tering complex linguistic objects. In this approach  we consider a potentially inﬁ-
nite number of features and categorical outcomes. We evaluated these models for
the task of within- and cross-document event coreference on two corpora. All the
models we investigated show signiﬁcant improvements when compared against an
existing baseline for this task.

1 Introduction
In Natural Language Processing (NLP)  the task of event coreference has numerous applications 
including question answering  multi-document summarization  and information extraction. Two
event mentions are coreferential if they share the same participants and spatio-temporal groundings.
Moreover  two event mentions are identical if they have the same causes and effects. For example 
the three documents listed in Table 1 contains four mentions of identical events but only the arrested 
apprehended  and arrest mentions from the documents 1 and 2 are coreferential. These deﬁnitions
were used in the tasks of Topic Detection and Tracking (TDT)  as reported in [24].

Previous approaches to event coreference resolution [3] used the same lexeme or synonymy of the
verb describing the event to decide coreference. Event coreference was also tried by using the
semantic types of an ontology [17]. However  the features used by these approaches are hard to select
and require the design of domain speciﬁc constraints. To address this problems  we have explored
a sequence of unsupervised  nonparametric Bayesian models that are used to probabilistically infer
coreference clusters of event mentions from a collection of unlabeled documents. Our approach
is motivated by the recent success of unsupervised approaches for entity coreference resolution
[16  22  25] and by the advantages of using a large amount of data at no cost.

One model was inspired by the fully generative Bayesian model proposed by Haghighi and Klein
[16] (henceforth  H&K). However  to employ the H&K’s model for tasks that require clustering
objects with rich linguistic features (such as event coreference resolution)  or to extend this model in
order to enclose additional observable properties is a challenging task [22  25]. In order to counter
this limitation  we make a conditional independence assumption between the observable features
and propose a generalized framework (Section 3) that is able to easily incorporate new features.

During the process of learning the model described in Section 3  it was observed that a large amount
of time was required to incorporate and tune new features. This lead us to the challenge of creating a
framework which considers an unbounded number of features where the most relevant are selected
automatically. To accomplish this new goal  we propose two novel approaches (Section 4). The
ﬁrst incorporates a MarkovIndianBuffetProcess (mIBP) [30] into a HierarchicalDirichletProcess
(HDP) [28]. The second uses an InﬁniteHiddenMarkovModel (iHMM) [5] coupled to an Inﬁnite
FactorialHiddenMarkovModel (iFHMM) [30].

In this paper  we focus on event coreference resolution  though adaptations for event identity resolu-
tion can be easily made. We evaluated the models on the ACE 2005 event corpus [18] and on a new
annotated corpus encoding within- and cross-document event coreference information (Section 5).

1

Document 1: San Diego Chargers receiver Vincent Jackson was arrested on suspicion of drunk driving on
Tuesday morning  ﬁve days before a key NFL playoff game.
. . .
Police apprehended Jackson in San Diego at 2:30 a.m. and booked him for the misdemeanour before his
release.
Document 2: Despite his arrest on suspicion of driving under the inﬂuence yesterday  Chargers receiver
Vincent Jackson will play in Sunday’s AFC divisional playoff game at Pittsburgh.
Document 3: In another anti-piracy operation  Navy warship on Saturday repulsed an attack on a merchant
vessel in the Gulf of Aden and nabbed 23 Somali and Yemeni sea brigands.

Table 1: Examples of coreferential and identical events.

2 Event Coreference Resolution
Models for solving event coreference and event identity can lead to the generation of ad-hoc event
hierarchies from text. A sample of a hierarchy capturing corefering and identical events  including
those from the example presented in Section 1  is illustrated in Figure 1.

generic
events

events

event

mentions

arrest

Event properties:
Suspect:
Authorities:
Time:
Location:

Vincent Jackson
police
Tuesday
San Diego

arrest

... arrested ... apprehended

... arrest ... 

Document 1

Document 2

Event properties:
Suspect:
Authorities:
Time:
Location:

sea brigands
Navy warship
Saturday
Gulf of Aden

arrest

... nabbed ... 

Document 3

Figure 1: A portion of the event hierarchy.

First  we introduce some basic notation.1 Next  to cluster the mentions that share common event
properties (as shown in Figure 1)  we brieﬂy describe the linguistic features of event mentions.
2.1 Notation
As input for our models  we consider a collection of I documents  each document i having Ji event
mentions. Each event mention is characterized by L feature types  FT  and each feature type is
represented by a ﬁnite number of feature values  f v. Therefore  we can represent the observable
properties of an event mention  em  as a vector of pairs h(FT1 : f v1i)  . . .   (FTL : f vLi)i  where each
feature value index i ranges in the feature value space associated with a feature type.
2.2 Linguistic Features
We consider the following set of features associated to an event mention:2
Lexical Features (LF) To capture the lexical context of an event mention  we extract the following
features: the head word of the mention (HW)  the lemma of the HW (HL)  lemmas of left and right
words of the mention (LHL RHL)  and lemmas of left and right mentions (LHE RHE).
Class Features (CF) These features aim to classify mentions into several types of classes:
the
mention HW’s part-of-speech (POS)  the word class of the HW (HWC)  which can take one of the
following values hverb  noun  adjective  otheri  and the event class of the mention (EC). To extract
the event class associated to every event mention  we employed the event identiﬁer described in [6].
WordNet Features (WF) We build three types of clusters over all the words from WordNet [9]
and use them as features for the mention HW. First cluster type associates an unique id to each
(word:HWC) pair (WNW). The second cluster type uses the transitive closure of the synonymous
relations to group words from WordNet (WNS). Finally  the third cluster type considers as grouping
criteria the category from WordNet lexicographer’s ﬁles that is associated to each word (WNL). For
cases when a new word does not belong to any of these WordNet clusters  we create a new cluster
with a new id for each of the three cluster types.
Semantic Features (SF) To extract features that characterize participants and properties of event
mentions  we use s semantic parser [8] trained on PropBank(PB) [23] and FrameNet(FN) [4] cor-
pora. (For instance  for the apprehended mention from our example  Jackson is the feature value

1For consistency  we try to preserve the notation of the original models.
2In this subsection and the following section  the feature term is used in context of a feature type.

2

for A0 PB argument3 and the SUSPECT frame element (FEA0) of the ARREST frame.) Another se-
mantic feature is the semantic frame (FR) that is evoked by an event mention. (For instance  all the
emphasized mentions from our example evoke the ARREST frame from FN.)
Feature Combinations (FC) We also explore various combinations of features presented above.
Examples include HW+POS  HL+FR  FE+A1  etc.
3 Finite Feature Models
In this section  we present a sequence of HDP mixture models for solving event coreference. For this
type of approach  a DirichletProcess (DP) [10] is associated with each document  and each mixture
component  which in our case corresponds to an event  is shared across documents. To describe
these models  we consider Z the set of indicator random variables for indices of events  φz the set
of parameters associated to an event z  φ a notation for all model parameters  and X a notation for
all random variables that represent observable features.

Given a document collection annotated with event mentions  the goal is to ﬁnd the best assignment
of event indices  Z∗  which maximize the posterior probability P (Z | X). In a Bayesian approach 
this probability is computed by integrating out all model parameters:

P (Z|X) = Z P (Z  φ|X)dφ = Z P (Z|X  φ)P (φ|X)dφ

In order to describe our modiﬁcations  we ﬁrst revisit a basic model from the set of models described
in H&K’s paper.
3.1 The One Feature Model
The one feature model  HDP1f   constitutes the simplest representation of an HDP model. In this
model  which is depicted graphically in Figure 2(a)  the observable components are characterized
by only one feature. The distribution over events associated to each document β is generated by a
Dirichlet process with a concentration parameter α > 0. Since this setting enables a clustering of
event mentions at the document level  it is desirable that events are shared across documents and
the number of events K is inferred from data. To ensure this ﬂexibility  a global nonparametric
DP prior with a hyperparameter γ and a global base measure H can be considered for β [28]. The
global distribution drawn from this DP prior  denoted as β0 in Figure 2(a)  encodes the event mixing
weights. Thus  same global events are used for each document  but each event has a document
speciﬁc distribution βi that is drawn from a DP prior centered on β0.
To infer the true posterior probability of P (Z|X)  we follow [28] in using a Gibbs sampling algo-
rithm [12] based on the direct assignment sampling scheme. In this sampling scheme  the β and φ
parameters are integrated out analytically. The formula for sampling an event index for mention j
from document i  Zi j  is given by:4

P (Zi j | Z−i j   HL) ∝ P (Zi j | Z−i j)P (HLi j | Z  HL−i j)

where HLi j is the head lemma of the event mention j from the document i.
First  in the generative process of an event mention  an event index z is sampled by using a mecha-
nism that facilitates sampling from a prior for inﬁnite mixture models called the Chinese Restaurant
Franchise (CRF) representation [28]:

P (Zi j = z | Z−i j   β0) ∝ (cid:26) αβu

0  

if z = znew

nz + αβz

0   otherwise

Here  nz is the number of event mentions with the event index z  znew is a new event index not used
already in Z−i j  βz
0 is the
weight for the unknown mixture component.

0 are the global mixing proportions associated to the K events  and βu

Then  to generate the mention head lemma (in this model  X = hHLi)  the event z is associated with
a multinomial emission distribution over the HL feature values having the parameters φ = hφhl
Z i.
We assume that this emission distribution is drawn from a symmetric Dirichlet distribution with
concentration λHL:

3A0 annotates in PB a speciﬁc type of semantic role which represents the AGENT  the DOER  or the ACTOR
of a speciﬁc event. Another PB argument is A1  which plays the role of the PATIENT  the THEME  or the
EXPERIENCER of an event.

4Z−i j represents a notation for Z − {Zi j}.

3

γ

α

φ

∞

γ

α

H

β0

∞

β

∞

Zi

HLi

Ji

I

(a)

H

β0

∞

φ

∞

β

∞

Zi

HLi FRi

Ji

I

(b)

γ

α

H

β0

∞

φ

∞

β

∞

Zi

Xi

(c)

L

Ji

I

γ

α

θ

φ

∞

H

β0

∞

β

∞

Zi

HLi

POSi

FRi

(d)

Ji

I

Figure 2: Graphical representation of four HDP models. Each node corresponds to a random variable. In
particular  shaded nodes denotes observable variables. Each rectangle captures the replication of the structure
it contains. The number of replications is indicated in the bottom-right corner of the rectangle. The model
depicted in (a) is an HDP model using one feature; the model in (b) employs HL and FR features; (c) illustrates
a ﬂat representation of a limited number of features in a generalized framework (henceforth  HDPf lat); and (d)
captures a simple example of structured network topology of three feature variables (henceforth  HDPstruct).
The dependencies involving parameters φ and θ in models (b)  (c)  and (d) are omitted for clarity.

P (HLi j = hl | Z  HL−i j) ∝ nhl z + λHL

where HLi j is the head lemma of mention j from document i  and nhl z is the number of times
the feature value hl has been associated with the event index z in (Z  HL−i j). We also apply the
Lidstone’s smoothing method to this distribution.
3.2 Adding More Features
A model in which observable components are represented only by one feature has the tendency to
cluster these components based on their feature value. To address this limitation  H&K proposed
a more complex model that is strictly customized for entity coreference resolution. On the other
hand  event coreference involves clustering complex objects characterized by richer features than
entity coreference (or topic detection)  and therefore it is desirable to extend the HDP1f model with
a generalized model where additional features can be easily incorporated.

To facilitate this extension  we assume that feature variables are conditionally independent given Z.
This assumption considerably reduces the complexity of computing P (Z | X). For example  if we
want to incorporate another feature (e.g.  F R) in the previous model  the formula becomes:

P (Zi j | HL  FR) ∝ P (Zi j)P (HLi j  F Ri j | Z) = P (Zi j)P (HLi j | Z)P (F Ri j | Z)

In this formula  we omit the conditioning components of Z  HL  and FR for clarity. The graphical
representation corresponding to this model is illustrated in Figure 2(b). In general  if X consists of
L feature variables  the inference formula for the Gibbs sampler is deﬁned as:

P (Zi j | X) ∝ P (Zi j) YF T ∈X

P (F Ti j | Z)

The graphical model for this general setting is depicted in Figure 2(c). Drawing an analogy  the
graphical representation involving feature variables and Z variables resembles the graphical repre-
sentation of a Naive Bayes classiﬁer.

When dependencies between feature variables exist (e.g.  in our case  frame elements are dependent
of the semantic frames that deﬁne them  and frames are dependent of the words that evoke them) 
various global distributions are involved for computing P (Z | X). For instance  for the model
depicted in Figure 2(d) the posterior probability is given by:

P (Zi j)P (F Ri j | HLi j  θ) YF T ∈X

P (F Ti j | Z)

In this model  P (F Ri j | HLi j  θ) is a global distribution parameterized by θ  and the feature
variables considered are X = hHL  POS  FRi.

4

For all these extended models  we compute the prior and likelihood factors as described in the one
feature model. Also  following H&K  in the inference mechanism we assign soft counts for missing
features (e.g.  unspeciﬁed PB argument).
4 Unbounded Feature Models
First  we present a generative model called the MarkovIndianBuffetProcess (mIBP) that provides a
mechanism in which each object can be represented by a sparse subset of a potentially unbounded set
of latent features [15  14  30].5 Then  to overcome the limitations regarding the number of mixture
components and the number of features associated with objects  we combine this mechanism with
an HDP model to form an mIBP-HDP hybrid. Finally  to account for temporal dependencies  we
employ an mIBP extension  called the InﬁniteFactorialHiddenMarkovModel (iFHMM) [30]  in
combination with an InﬁniteHiddenMarkovModel (iHMM) to form the iFHMM-iHMM model.
4.1 The Markov Indian Buffet Process
As described in [30]  the mIBP deﬁnes a distribution over an unbounded set of binary Markov chains 
where each chain can be associated to a binary latent feature that evolves over time according to
Markov dynamics. Speciﬁcally  if we denote by M the total number of feature chains and by T
the number of observable components (event mentions)  the mIBP deﬁnes a probability distribution
over a binary matrix F with T rows  which correspond to observations  and an unbounded number
of columns (M → ∞)  which correspond to features. An observation yt contains a subset from
the unbounded set of features {f 1  f 2  . . .   f M } that is represented in the matrix by a binary vector
Ft = hF 1
Therefore  F decomposes the observations and represents them as feature factors  which can then
be associated to hidden variables in an iFHMM as depicted in Figure 3(a). The transition matrix of
a binary Markov chain associated to a feature f m is deﬁned as

t = 1 indicates that f i is associated to yt.

t i  where F i

t   . . .   F M

t   F 2

1 − bm bm(cid:19)
W(m) = (cid:18)1 − am am

t−1

t−1

b

1−F m
m

F m
m ).

t ∼ Bernoulli(a

t = i)  the parameters am ∼ Beta(α′/M  1) and bm ∼ Beta(γ ′  δ′) 
t+1 = j | F m
0 = 0. In the generative process  the hidden variable of feature f m for an

where W(m)
ij = P (F m
and the initial state F m
object yt  F m
To compute the probability of the feature matrix F6  in which the parameters a and b are integrated
out analytically  we use the counting variables c00
m to record the 0 → 0  0 → 1 
1 → 0  and 1 → 1 transitions f m has made in the binary chain m. The stochastic process that derives
the probability distribution in terms of these variables is deﬁned as follows. The ﬁrst component
samples a number of Poisson(α′) features. In general  depending on the value that was sampled in
the previous step (t − 1)  a feature f m is sampled for the tth component according to the following
probabilities:

m   and c11

m   c01

m   c10

P (F m

t = 1 | F m

t−1 = 1) =

P (F m

t = 1 | F m

t−1 = 0) =

c11
m + δ′
γ ′ + δ′ + c10

m + c11
m

c00
m

c00
m + c01
m

The tth component then repeats the same mechanism for sampling the next features until it ﬁnishes
the current number of sampled features M . After all features are sampled for the tth component 
a number of Poisson(α′/t) new features are assigned for this component and M gets incremented
accordingly.
4.2 The mIBP-HDP Model
One direct application of the mIBP is to integrate it into the HDP models proposed in Section 3. In
this way  the new nonparametric extension will have the beneﬁts of capturing uncertainty regarding
the number of mixture components that are characterized by a potentially inﬁnite number of features.
Since one observable component is associated with an unbounded countable set of features  we have
to provide a mechanism in which only a ﬁnite set of features will represent the component in the
HDP inference process.

5In this section  a feature is represented by a (feature type:feature value) pair.
6Technical details for computing this probability are described in [30].

5

FM
0

FM
1

FM
2

F2
0

F1
0

F2
1

F1
1

F2
2

F1
2

Y1

Y2

S0

S1

S2

FM
0

FM
1

FM
2

F2
0

F1
0

F2
1

F1
1

F2
2

F1
2

Y1

Y2

FM
T

F2
T

F1
T

YT

ST

FM
T

F2
T

F1
T

YT

(a)

(b)

Figure 3: (a) The Inﬁnite Factorial Hidden Markov Model. (b) The iFHMM-iHMM model. (M→ ∞)

The idea behind this mechanism is to use slice sampling7 [21] in order to derive a ﬁnite set of
features for yt. Letting qm be the number of times feature f m was sampled in the mIBP  and vt an
auxiliary variable for yt such that vt ∼ Uniform(1  max{qm | F m
t = 1})  we deﬁne the ﬁnite feature
set Bt for the observation yt as:

Bt = {f m | F m

t = 1 ∧ qm ≥ vt}

The ﬁniteness of this feature set is based on the observation that  in the generative process of the
mIBP  only a ﬁnite set of features are sampled for a component. Another observation worth men-
tioning regarding the way this set is constructed is that only the most representative features of yt
get selected in Bt.
4.3 The iFHMM-iHMM Model
The iFHMM is a nonparametric Bayesian factor model that extends the Factorial HiddenMarkov
Model (FHMM) [13] by letting the number of parallel Markov chains M be learned from data.
Although the iFHMM allows a more ﬂexible representation of the latent structure  it can not be
used as a framework where the number of clustering components K is inﬁnite. On the other hand 
the iHMM represents a nonparametric extension of the Hidden Markov Model (HMM) [27] that
allows performing inference on an inﬁnite number of states K.
In order to further increase the
representational power for modeling discrete time series data  we propose a nonparametric extension
that combines the best of the two models  and lets the parameters M and K be learned from data.
Each step in the new generative process  whose graphical representation is depicted in Figure 3(b) 
is performed in two phases: (i) the latent feature variables from the iFHMM framework are sampled
using the mIBP mechanism; and (ii) the features sampled so far  which become observable during
this second phase  are used in an adapted beam sampling algorithm [29] to infer the clustering
components (or  in our case  latent events).

To describe the beam sampler for event coreference resolution  we introduce additional notation.
We denote by (s1  . . .   sT ) the sequence of hidden states corresponding to the sequence of event
mentions (y1  . . .   yT )  where each state st belong to one of the K events  st ∈ {1  . . .   K}  and
each mention yt is represented by a sequence of latent features hF 1
t i. One element of
the transition probability π is deﬁned as πij = P (st = j | st−1 = i) and a mention yt is generated
according to a likelihood model F that is parameterized by a state-dependent parameter φst (yt |
st ∼ F(φst )). The observation parameters φ are iid drawn from a prior base distribution H.
The beam sampling algorithm combines the ideas of slice sampling and dynamic programming for
an efﬁcient sampling of state trajectories. Since in time series models the transition probabilities
have independent priors [5]  Van Gael and colleagues [29] also used the HDP mechanism to al-
low couplings across transitions. For sampling the whole hidden state trajectory s  this algorithm
employs a forward ﬁltering-backward sampling technique.

t   . . .   F M

t   F 2

In the forward step of our implementation  we sample the feature variables using the mIBP as de-
scribed in Section 4.1  and the auxiliary variable ut ∼ Uniform(0  πst−1st ) for each mention yt.
As explained in [29]  the auxiliary variables u are used to ﬁlter only those trajectories s for which

7The idea of using this procedure is inspired from [29] where a slice variable was used to sample a ﬁnite

number of state trajectories in the iHMM.

6

πst−1st ≥ ut for all t. Also  in this step  we compute the probabilities P (st | y1:t  u1:t) for all t as
described in [29]:

P (st | y1:t  u1:t) ∝ P (yt | st) Xst−1:ut<πst−1 st

P (st−1 | y1:t−1  u1:t−1)

Here  the dependencies involving parameters π and φ are omitted for clarity.
In the backward step  we ﬁrst sample the event for the last state sT directly from P (sT | y1:T   u1:T )
and then  for all t : T − 1  1  we sample each state st given st+1 by using the formula P (st |
st+1  y1:T   u1:T) ∝ P (st|y1:t  u1:t)P (st+1|st  ut+1).
To sample the emission distribution φ efﬁciently  and to ensure that each mention is characterized
by a ﬁnite set of representative features  we set the base distribution H to be conjugate with the
data distribution F in a Dirichlet-multinomial model with the sufﬁcient statistics of the multinomial
distribution (o1  . . .   oK ) deﬁned as:

ok =

T

Xt=1 Xf m∈Bt

nmk

where nmk counts how many times feature f m was sampled for event k  and Bt stores a ﬁnite set
of features for yt as it is deﬁned in Section 4.2.
5 Evaluation
Event Coreference Data One corpus used for evaluation is ACE 2005 [18]. This corpus annotates
within-document coreference information of speciﬁc types of events (such as Conﬂict  Justice  and
Life). After an initial processing phase  we extracted from ACE 6553 event mentions and 4946
events. To increase the diversity of events and to evaluate the models for both within- and cross-
document event coreference  we created the EventCorefBank corpus (ECB).8 This new corpus con-
tains 43 topics  1744 event mentions  1302 within-document events  and 339 cross-document events.

For a more realistic approach  we trained the models on all the event mentions from the two corpora
and not only on the mentions manually annotated for event coreference (the true event mentions). In
this regard  we ran the event identiﬁer described in [6] on the ACE and ECB corpora  and extracted
45289 and 21175 system mentions respectively.
The Experimental Setup Table 2 lists the recall (R)  precision (P)  and F-score (F) of our exper-
iments averaged over 5 runs of the generative models. Since there is no agreement on the best
coreference resolution metric  we employed four metrics for our evaluation: the link-based MUC
metric [31]  the mention-based B3 metric [2]  the entity-based CEAF metric [19]  and the pairwise
F1 (PW) metric. In the evaluation process  we considered only the true mentions of the ACE test
dataset and of the test sets of a 5-fold cross validation scheme on the ECB dataset. For evaluating
the cross-document coreference annotations  we adopted the same approach as described in [3] by
merging all the documents from the same topic into a meta-document and then scoring this docu-
ment as performed for within-document evaluation. Also  for both corpora  we considered a set of
132 feature types  where each feature type consists on average of 3900 distinct feature values.
The Baseline A simple baseline for event coreference consists in grouping events by their event
classes [1]. To extract event classes  we employed the event identiﬁer described in [6]. Therefore 
this baseline will categorize events into a small number of clusters  since the event identiﬁer is
trained to predict the ﬁve event classes annotated in TimeBank [26]. As it was already observed
[20  11]  considering very few categories for coreference resolution tasks will result in overestimates
of the MUC scorer. For instance  a baseline that groups all entity mentions into the same entity
achieves the highest MUC score than any published system for the task of entity coreference. Similar
behaviour of the MUC metric is observed for event coreference resolution. For example  for cross-
document evaluation on ECB  a baseline that clusters all mentions into one event achieves 73.2%
MUC F-score  while the baseline listed in Table 2 achieves 72.9% MUC F-score.
HDP Extensions Due to memory limitations  we evaluated the HDPf lat and HDPstruct models
only on a restricted subset of manually selected feature types. In general  as shown in Table 2 
the HDPf lat model achieved the best performance results on the ACE test dataset  whereas the

8This resource is available at http://www.hlt.utdallas.edu/∼ady. The annotation process is described in [7].

7

Model

MUC

P

R

94.3
Baseline
62.2
HDP1f (HL)
53.5
HDPf lat
61.9
HDPstruct
mIBP-HDP
48.7
iFHMM-iHMM 48.7

92.2
Baseline
46.9
HDP1f (HL)
37.8
HDPf lat
47.4
HDPstruct
38.2
mIBP-HDP
iFHMM-iHMM 39.5

90.5
Baseline
47.7
HDP1f (HL)
44.4
HDPf lat
51.9
HDPstruct
mIBP-HDP
40.0
iFHMM-iHMM 48.4

33.1
43.1
54.2
49.0
41.9
48.8

39.8
54.8
92.9
82.7
68.8
85.2

61.1
70.5
95.3
89.5
79.8
89.0

B3
P

F

F

P
ACE (within-document event coreference)

R

R

CEAF

49.0
50.9
53.9
54.7
45.1
48.7

97.9
86.0
83.4
86.2
81.7
81.9

25.0
70.6
84.2
76.9
76.4
82.2

39.9
77.5
83.8
81.3
79.0
82.1

14.7
62.3
76.9
69.0
68.8
74.6

64.4
76.4
76.5
77.5
73.8
74.5

ECB (within-document event coreference)

55.6
50.4
53.4
60.1
48.9
53.9

97.7
84.3
82.1
84.3
82.1
82.5

55.8
89.0
99.2
97.1
95.3
98.1

71.0
86.5
89.8
90.2
88.2
89.6

44.5
83.4
93.9
92.7
90.3
93.1

80.1
79.6
78.2
81.1
78.5
78.8

ECB (cross-document event coreference)

72.9
56.8
60.5
65.7
53.2
62.7

93.8
67.0
65.0
69.3
63.1
67.0

49.6
86.2
98.7
95.8
94.1
96.4

64.9
75.3
78.3
80.4
75.5
79.0

36.6
76.2
86.9
86.2
82.7
85.5

72.7
57.1
56.0
60.1
54.6
58.0

F

R

24.0
68.6
76.7
73.0
71.2
74.5

57.2
81.4
85.3
86.5
84.0
85.3

48.7
65.2
68.0
70.8
65.7
69.1

93.5
50.5
43.3
53.2
37.4
37.2

93.7
36.6
27.0
34.4
26.5
29.4

90.7
34.9
29.2
37.5
26.1
33.3

PW
P

8.2
27.7
47.1
38.1
28.9
39.0

25.4
53.4
92.4
83.0
67.9
86.6

28.6
58.9
95.1
85.6
77.0
88.3

F

15.2
35.8
45.1
44.4
32.6
38.1

39.8
42.6
41.3
48.6
37.7
43.7

43.3
43.5
44.4
52.1
38.9
48.2

Table 2: Evaluation results for within- and cross-document event coreference resolution.

HDPstruct model  which also considers dependencies between feature types  proved to be more
effective on the ECB dataset for both within- and cross-document event coreference evaluation. The
set of feature types used to achieve these results consists of combinations of types from all feature
categories described in Section 2.2. For the results of the HDPstruct model listed in Table 2  we also
explored the conditional dependencies between the HL  FR  and FEA types.

As can be observed from Table 2  the results of the HDPf lat and HDPstruct models show an F-score
increase by 4-10% over the HDP1f model  and therefore prove that the HDP extensions provide a
more ﬂexible representation for clustering objects characterized by rich properties.
mIBP-HDP In spite of its advantage of working with a potentially inﬁnite number of features in an
HDP framework  the mIBP-HDP model did not achieve a satisfactory performance in comparison
with the other proposed models. However  the results were obtained by automatically selecting
only 2% of distinct feature values from the entire set of values extracted from both corpora. When
compared with the restricted set of features considered by the HDPf lat and HDPstruct models  the
percentage of values selected by mIBP-HDP is only 6%. A future research area for improving this
model is to consider other distributions for automatic selection of salient feature values.
iFHMM-iHMM In spite of the automatic feature selection employed for the iFHMM-iHMM model 
its results remain competitive against the results of the HDP extensions (where the feature types
were hand tuned). As shown in Table 2  most of the iFHMM-iHMM results fall in between the
HDPf lat and HDPstruct models. Also  these results indicate that the iFHMM-iHMM model is a
better framework than HDP in capturing the event mention dependencies simulated by the mIBP
feature sampling scheme. Similar to the mIBP-HDP model  to achieve these results  the iFHMM-
iHMM model uses only 2% values from the entire set of distinct feature values. For the experiments
of the iFHMM-iHMM results reported in Table 2  we set α′=50  γ ′=0.5  and δ′=0.5.

6 Conclusion

In this paper  we have described how a sequence of unsupervised  nonparametric Bayesian models
can be employed to cluster complex linguistic objects that are characterized by a rich set of features.
The experimental results proved that these models are able to solve real data applications in which
the feature and cluster numbers are treated as free parameters  and the selection of features is per-
formed automatically. While the results of event coreference resolution are promising  we believe
that the classes of models proposed in this paper have a real utility for a wide range of applications.

8

References

[1] David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and

Reasoning about Time and Events  pages 1–8.

[2] Amit Bagga and Breck Baldwin. 1998. Algorithms for Scoring Coreference Chains. In Proc. of LREC.
[3] Amit Bagga and Breck Baldwin. 1999. Cross-Document Event Coreference: Annotations  Experiments 

and Observations. In Proceedings of the ACL-99 Workshop on Coreference and its Applications.

[4] Collin F. Baker  Charles J. Fillmore  and John B. Lowe. 1998. The Berkeley FrameNet project.

Proceedings of COLING-ACL.

In

[5] Matthew J. Beal  Zoubin Ghahramani  and Carl Edward Rasmussen. 2002. The Inﬁnite Hidden Markov

Model. In Proceedings of NIPS.

[6] Cosmin Adrian Bejan. 2007. Deriving Chronological Information from Texts through a Graph-based

Algorithm. In Proceedings of FLAIRS-2007.

[7] Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A Linguistic Resource for Discovering Event Structures

and Resolving Event Coreference. In Proceedings of LREC-2008.

[8] Cosmin Adrian Bejan and Chris Hathaway. 2007. UTD-SRL: A Pipeline Architecture for Extracting Frame

Semantic Structures. In Proceedings of SemEval-2007.

[9] Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.
[10] Thomas S. Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. The Annals of

Statistics  1(2):209–230.

[11] Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing Transitivity in Coreference Resolution.

In Proceedings of ACL/HLT-2008  pages 45–48.

[12] Stuart Geman and Donald Geman. 1984. Stochastic relaxation  Gibbs distributions and the Bayesian

restoration of images. . IEEE Transactions on Pattern Analysis and Machine Intelligence  6:721–741.

[13] Z. Ghahramani and M. Jordan. 1997. Factorial Hidden Markov Models. Machine Learning  29:245–273.
[14] Zoubin Ghahramani  T. L. Grifﬁths  and Peter Sollich  2007. Bayesian Statistics 8  chapter Bayesian

nonparametric latent feature models  pages 201–225. Oxford University Press.

[15] Tom Grifﬁths and Zoubin Ghahramani. 2006.
Process. In Proceedings of NIPS  pages 475–482.

Inﬁnite Latent Feature Models and the Indian Buffet

[16] Aria Haghighi and Dan Klein. 2007. Unsupervised Coreference Resolution in a Nonparametric Bayesian

Model. In Proceedings of the ACL.

[17] Kevin Humphreys  Robert Gaizauskas  Saliha Azzam. 1997. Event Coreference for Information Extrac-
tion. In Proceedings of the Workshop on Operational Factors in Practical  Robust Anaphora Resolution
for Unrestricted Texts  35th Meeting of ACL  pages 75–81.

[18] LDC-ACE05. 2005. ACE (Automatic Content Extraction) English Annotation Guidelines for Events.
[19] X. Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of EMNLP.
[20] X. Luo  A. Ittycheriah  H. Jing  N. Kambhatla  and S. Roukos 2004. A Mention-Synchronous Coreference

Resolution Algorithm Based On the Bell Tree. In Proceedings of ACL-2004.

[21] Radford M. Neal. 2003. Slice Sampling. The Annals of Statistics  31:705–741.
[22] Vincent Ng. 2008. Unsupervised Models for Coreference Resolution. In Proceedings of EMNLP.
[23] Martha Palmer  Daniel Gildea  and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus

of Semantic Roles. Computational Linguistics  31(1):71–105.

[24] Ron Papka. 1999. On-line New Event Detection  Clustering and Tracking. Ph.D. thesis  Department of

Computer Science  University of Massachusetts.

[25] Hoifung Poon and Pedro Domingos. 2008. Joint Unsupervised Coreference Resolution with Markov

Logic. In Proceedings of EMNLP.

[26] J. Pustejovsky  P. Hanks  R. Sauri  A. See  R. Gaizauskas  A. Setzer  D. Radev  B. Sundheim  D. Day  L.

Ferro  and M. Lazo. 2003. The TimeBank Corpus. In Corpus Linguistics  pages 647–656.

[27] Lawrence R. Rabiner. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech

Recognition. In Proceedings of the IEEE  pages 257–286.

[28] Yee Whye Teh  Michael Jordan  Matthew Beal  and David Blei. 2006. Hierarchical Dirichlet Processes.

Journal of the American Statistical Association  101(476):1566–1581.

[29] Jurgen Van Gael  Yunus Saatci  Yee Whye Teh  and Zoubin Ghahramani. 2008. Beam Sampling for the

Inﬁnite Hidden Markov Model. In Proceedings of ICML  pages 1088–1095.

[30] Jurgen Van Gael  Yee Whye Teh  and Zoubin Ghahramani. 2008. The Inﬁnite Factorial Hidden Markov

Model. In Proceedings of NIPS.

[31] Marc Vilain  John Burger  John Aberdeen  Dennis Connolly  and Lynette Hirschman. 1995. A Model-

Theoretic Coreference Scoring Scheme. In Proceedings of MUC-6  pages 45–52.

9

,Dongsung Huh
Terrence Sejnowski