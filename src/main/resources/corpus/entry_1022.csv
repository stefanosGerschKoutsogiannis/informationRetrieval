2019,Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients,The present paper develops a novel aggregated gradient approach for distributed machine learning that adaptively compresses the gradient communication. The key idea is to first quantize the computed gradients  and then skip less informative quantized gradient communications by reusing outdated gradients. Quantizing and skipping result in 'lazy' worker-server communications  which justifies the term Lazily Aggregated Quantized gradient that is henceforth abbreviated as  LAQ. Our LAQ can provably attain the same linear convergence rate as the gradient descent in the strongly convex case  while effecting major savings in the  communication overhead both in transmitted bits as well as in communication rounds. Empirically  experiments with real data corroborate a significant communication reduction compared to existing gradient- and stochastic gradient-based algorithms.,Communication-Efﬁcient Distributed Learning via

Lazily Aggregated Quantized Gradients

Jun Sun†

Zhejiang University

Hangzhou  China 310027
sunjun16sj@gmail.com

Georgios B. Giannakis

University of Minnesota  Twin Cities

Minneapolis  MN 55455

georgios@umn.edu

Tianyi Chen†

Rensselaer Polytechnic Institute

Troy  New York 12180

chent18@rpi.edu

Zaiyue Yang

Southern U. of Science and Technology

Shenzhen  China 518055
yangzy3@sustc.edu.cn

Abstract

The present paper develops a novel aggregated gradient approach for distributed
machine learning that adaptively compresses the gradient communication. The
key idea is to ﬁrst quantize the computed gradients  and then skip less informative
quantized gradient communications by reusing outdated gradients. Quantizing and
skipping result in ‘lazy’ worker-server communications  which justiﬁes the term
Lazily Aggregated Quantized gradient that is henceforth abbreviated as LAQ. Our
LAQ can provably attain the same linear convergence rate as the gradient descent
in the strongly convex case  while effecting major savings in the communication
overhead both in transmitted bits as well as in communication rounds. Empirically 
experiments with real data corroborate a signiﬁcant communication reduction
compared to existing gradient- and stochastic gradient-based algorithms.

1

Introduction

Considering the massive amount of mobile devices  centralized machine learning via cloud computing
incurs considerable communication overhead  and raises serious privacy concerns. Today  the
widespread consensus is that besides in the cloud centers  future machine learning tasks have to be
performed starting from the network edge  namely devices [17  19]. Typically  distributed learning
tasks can be formulated as an optimization problem of the form

min

✓ Xm2M

NmXn=1

fm(✓) with fm(✓) :=

`(xm n; ✓)

(1)

deﬁne f (✓) =Pm2M

where ✓ 2 Rp denotes the parameter to be learned  M with |M| = M denotes the set of servers 
xm n represents the n-th data vector at worker m (e.g.  feature and label)  and Nm is the number of
data samples at worker m. In (1)  `(x; ✓) denotes the loss associated with ✓ and x  and fm(✓) denotes
the aggregated loss corresponding to ✓ and all data at worker m. For the ease in exposition  we also

fm(✓) as the overall loss function.

In the commonly employed worker-server setup  the server collects local gradients from the workers
and updates the parameter using a gradient descent (GD) iteration given by
GD iteration

(2)

✓k+1 = ✓k  ↵ Xm2M

rfm✓k

† Jun Sun and Tianyi Chen contributed equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where ✓k denotes the parameter value at

iteration k  ↵ is the stepsize  and rf (✓k) =
Pm2M rfm(✓k) is the aggregated gradient. When the data samples are distributed across workers 
each worker computes the corresponding local gradient rfm(✓k)  and uploads it to the server. Only
when all the local gradients are collected  the server can obtain the full gradient and update the
parameter. To implement (2) however  the server has to communicate with all workers to obtain
fresh gradients {rfm✓k}M
m=1. In several settings though  communication is much slower than
computation [16]. Thus  as the number of workers grows  worker-server communications become the
bottleneck [10]. This becomes more challenging when incorporating popular deep learning-based
learning models with high-dimensional parameters  and correspondingly large-scale gradients.

1.1 Prior art
Communication-efﬁcient distributed learning methods have gained popularity recently [10  22]. Most
popular methods build on simple gradient updates  and are centered around the key idea of gradient
compression to save communication  including gradient quantization and sparsiﬁcation.
Quantization. Quantization aims to compress gradients by limiting the number of bits that repre-
sent ﬂoating point numbers during communication  and has been successfully applied to several
engineering tasks employing wireless sensor networks [21]. In the context of distributed machine
learning  a 1-bit binary quantization method has been developed in [5  24]. Multi-bit quantization
schemes have been studied in [2  18]  where an adjustable quantization level can endow additional
ﬂexibility to control the tradeoff between the per-iteration communication cost and the convergence
rate. Other variants of quantized gradient schemes include error compensation [32]  variance-reduced
quantization [34]  quantization to a ternary vector [31]  and quantization of gradient difference [20].
Sparsiﬁcation. Sparsiﬁcation amounts to transmitting only gradient coordinates with large enough
magnitudes exceeding a certain threshold [27]. Empirically  the desired accuracy can be attained even
after dropping 99% of the gradients [1]. To avoid losing information  small gradient components
are accumulated and then applied when they are large enough. The accumulated gradient offers
variance reduction of the sparsiﬁed stochastic (S)GD iterates [12  26]. With its impressive empirical
performance granted  except recent efforts [3]  deterministic sparsiﬁcation schemes lack performance
analysis guarantees. However  randomized counterparts that come with the so-termed unbiased
sparsiﬁcation have been developed to offer convergence guarantees [28  30].
Quantization and sparsiﬁcation have been also employed simultaneously [9  13  14]. Nevertheless 
they both introduce noise to (S)GD updates  and thus deteriorate convergence in general. For problems
with strongly convex losses  gradient compression algorithms either converge to the neighborhood of
the optimal solution  or  they converge at sublinear rate. The exception is [18]  where the ﬁrst linear
convergence rate has been established for the quantized gradient-based approaches. However  [18]
only focuses on reducing the required bits per communication  but not the total number of rounds.
Nevertheless  for exchanging messages  e.g.  the p-dimensional ✓ or its gradient  other latencies
(initiating communication links  queueing  and propagating the message) are at least comparable
to the message size-dependent transmission latency [23]. This motivates reducing the number of
communication rounds  sometimes even more so than the bits per round.
Distinct from the aforementioned gradient compression schemes  communication-efﬁcient schemes
that aim to reduce the number of communication rounds have been developed by leveraging higher-
order information [25  36]  periodic aggregation [19  33  35]  and recently by adaptive aggregation
[6  7  11  29]; see also [4] for a lower bound on communication rounds. However  whether we can
save communication bits and rounds simultaneously without sacriﬁcing the desired convergence
properties remains unresolved. This paper aims to address this issue.

1.2 Our contributions
Before introducing our approach  we revisit the canonical form of popular quantized (Q) GD methods
[24]-[20] in the simple setup of (1) with one server and M workers:
QGD iteration

(3)

where Qm✓k is the quantized gradient that coarsely approximates the local gradient rfm(✓k). While
the exact quantization scheme is different across algorithms  transmitting Qm✓k generally requires

✓k+1 = ✓k  ↵ Xm2M

Qm✓k

2

(4)

Qk
m

fewer number of bits than transmitting rfm(✓k). Similar to GD however  only when all the local
quantized gradients {Qm✓k} are collected  the server can update the parameter ✓.
In this context  the present paper puts forth a quantized gradient innovation method (as simple as
QGD) that can skip communication in certain rounds. Speciﬁcally  in contrast to the server-to-worker
downlink communication that can be performed simultaneously (e.g.  by broadcasting ✓k)  the server
has to receive the workers’ gradients sequentially to avoid interference from other workers  which
leads to extra latency. For this reason  our focus here is on reducing the number of worker-to-server
uplink communications  which we will also refer to as uploads. Our algorithm Lazily Aggregated
Quantized gradient descent (LAQ) resembles (3)  and it is given by
LAQ iteration

✓k+1 = ✓k  ↵rk with rk =rk1+Xm2Mk

m := Qm(✓k) Qm(ˆ✓

m := ✓k  8m 2M k  and ˆ✓

where rk is an approximate aggregated gradient that summarizes the parameter change at iteration
k1
m ) is the difference between two quantized gradients of fm at
k  and Qk
k1
m . With a judicious selection criterion that will be
the current iterate ✓k and the old copy ˆ✓
m is uploaded in iteration k 
introduced later  Mk denotes the subset of workers whose local Qk
k1
while parameter iterates are given by ˆ✓
m   8m /2M k. Instead of
m := ˆ✓
requesting fresh quantized gradient from every worker in (3)  the trick is to obtain rk by reﬁning the
previous aggregated gradient rk1; that is  using only the new gradients from the selected workers in
Mk  while reusing the outdated gradients from the rest of workers. If rk1 is stored in the server  this
simple modiﬁcation scales down the per-iteration communication rounds from QGD’s M to LAQ’s
|Mk|. Throughout the paper  one round of communication means one worker’s upload.
Compared to the existing quantization schemes  LAQ ﬁrst quantizes the gradient innovation —
the difference of current gradient and previous quantized gradient  and then skips the gradient
communication — if the gradient innovation of a worker is not large enough  the communication of
this worker is skipped. We will rigorously establish that LAQ achieves the same linear convergence
as GD under the strongly convex assumption of the loss function. Numerical tests will demonstrate
that our approach outperforms existing methods in terms of both communication bits and rounds.
Notation. Bold lowercase letters denote column vectors; kxk2 and kxk1 denote the `2-norm and
`1-norm of x  respectively; and [x]i represents i-th entry of x; while bac denotes downward rounding
of a; and | · | denotes the cardinality of the set or vector.
2 LAQ: Lazily aggregated quantized gradient

k

k

To reduce the communication overhead  two complementary stages are integrated in our algorithm
design: 1) gradient innovation-based quantization; and 2) gradient innovation-based uploading or
aggregation — giving the name Lazily Aggregated Quantized gradient (LAQ). The former reduces
the number of bits per upload  while the latter cuts down the number of uploads  which together
guarantee parsimonious communication. This section explains the principles of our two-stage design.

[

[Qm(k)]i

[fm(k)]i

[Qm( k1m )]i
)

2.1 Gradient innovation-based quantization
Quantization limits the number of bits to represent a
gradient vector during communication. Suppose we
use b bits to quantize each coordinate of the gradient
vector in contrast to 32 bits as in most computers.
With Q denoting the quantization operator  the quan-
k1
m ))  which depends on
tized gradient for worker m at iteration k is Qm(✓k) = Q(rfm(✓k)  Qm(ˆ✓
k1
the gradient rfm(✓k) and the previous quantization Qm(ˆ✓
m ). The gradient is element-wise quan-
tized by projecting to the closest point in a uniformly discretized grid. The grid is a p-dimensional
k1
m )k1. With
hypercube which is centered at Qm(ˆ✓
k1
m ) can
⌧ := 1/(2b  1) deﬁning the quantization granularity  the gradient innovation fm(✓k) Qm(ˆ✓
be quantized by b bits per coordinate at worker m as:
k1
m )]i + Rk
m

2Rk
Figure 1: Quantization example (b = 3)

m = krfm(✓k)  Qm(ˆ✓

k1
m ) with the radius Rk

Rk
m

m

[qm(✓k)]i =$ [rfm(✓k)]i  [Qm(ˆ✓

2⌧R k
m

1

2%  

+

i = 1 ···   p

(5)

3

Qk

m1 :

transmit Rk

k1
m ) = 2⌧R k

mqm(✓k)  Rk

m = Qm(✓k)  Qm(ˆ✓

m in the
which is an integer within [0  2b  1]  and thus can be encoded by b bits. Note that adding Rk
numerator ensures the non-negativity of [qm(✓k)]i  and adding 1/2 in (5) guarantees rounding to the
closest point. Hence  the quantized gradient innovation at worker m is (with 1 := [1 ···   1]>)
m and qm(✓k)

(6)
m and bp bits for qm(✓k)) instead of the original
k1
m ) stored in the memory and ⌧ known a priori  after

m the server can recover the quantized gradient as Qm(✓k) = Qm(ˆ✓

which can be transmitted by 32 + bp bits (32 bits for Rk
32p bits. With the outdated gradients Qm(ˆ✓
receiving Qk
Figure 1 gives an example for quantizing one coordinate of the gradient with b = 3 bits. The
original value is quantized with 3 bits and 23 = 8 values  each of which covers a range of length
m := rfm(✓k)  Qm(✓k) denoting the local quantization error  it is
2⌧R k
clear that the quantization error is less than half of the length of the range that each value covers 
namely  k"k
Qm(✓k)  and the
aggregated quantization error is "k := rf (✓k)  Q(✓k) =PM
m; that is  Q(✓k) = rf (✓k)  "k.

m. The aggregated quantized gradient is Q(✓k) =Pm2M

m centered at itself. With "k

mk1  ⌧R k

k1
m.
m ) + Qk

m=1 "k

2.2 Gradient innovation-based aggregation

k

Server

(cid:76)(cid:12)(cid:18)(cid:1)=(cid:1)k(cid:1)(cid:1)k

The idea of lazy gradient aggregation is that if the difference of two consecutive locally quantized
gradients is small  it is safe to skip the redundant gradient upload  and reuse the previous one
at the server.
In addition  we also ensure the server has a relatively “fresh" gradient for each
worker by enforcing communication if any worker
has not uploaded during the last ¯t rounds. We set a
clock tm  m 2M for worker m counting the number
of iterations since last time it uploaded information.
Equipped with the quantization and selection  our
LAQ update takes the form as (4).
Now it only remains to design the selection criterion
to decide which worker to upload the quantized gra-
dient or its innovation. We propose the following
communication criterion: worker m 2M skips the upload at iteration k  if it satisﬁes
2⌘ ;
2 + kˆ"k1
m k2

Workers
Quantization
Quantization
Selection
Selection
Figure 2: Distributed learning via LAQ

2 + 3⇣k"k
mk2

k
Quantization
Selection

⇠dk✓k+1d  ✓kdk2

k1
m )  Qm(✓k)k2

where D  ¯t and {⇠d}D
m = rfm(ˆ✓
ˆ"k1
we will prove the convergence and communication properties of LAQ under criterion (7).

(7b)
m is the current quantization error  and
k1
m ) is the error of the last uploaded quantized gradient. In next section

d=1 are predetermined constants  "k

k1
m )  Qm(ˆ✓

2 
tm  ¯t

kQm(ˆ✓

DXd=1

↵2M 2

Qk
M

Qk1

(7a)

k

1





2.3 LAQ algorithm development

In summary  as illustrated in Figure 2  LAQ can be implemented as follows. At iteration k  the server
broadcasts the learning parameter to all workers. Each worker calculates the gradient  and then
quantizes it to judge if it needs to upload the quantized gradient innovation Qk
m. Then the server
updates the learning parameter after it receives the gradient innovation from the selected workers.
The algorithm is summarized in Algorithm 2.
To make the difference between LAQ and GD clear  we re-write (4) as:

✓k+1 =✓k  ↵[rQ(✓k) + Xm2Mk

c

=✓k  ↵[rf (✓k)  "k + Xm2Mk

c

(Qm(ˆ✓

k1
m )  Qm(✓k))]

(Qm(ˆ✓

k1
m )  Qm(✓k))]

(8a)

(8b)

c := M\Mk  is the subset of workers which skip communication with server at iteration k.
where Mk
Compared with the GD iteration in (2)  the gradient employed here degrades due to the quantization
error  "k and the missed gradient innovation Pm2Mk
)  Qm(✓k))]. It is clear that if large

(Qm(ˆ✓

k1

c

4

Algorithm 1 QGD
1: Input: stepsize ↵> 0  quantization bit b.
2: Initialize: ✓k.
3: for k = 1  2 ···   K do
4:
5:
6:
7:
8:
9:
10: end for

Server broadcasts ✓k to all workers.
for m = 1  2 ···   M do
Worker m computes rfm(✓k) and Qm(✓k).
Worker m uploads Qk

end for
Server updates ✓ following (4) with Mk = M.

m via (6).

0

d=1 and ¯t.

m)  tm}m2M.

Server broadcasts ✓k to all workers.
for m = 1  2 ···   M do

Algorithm 2 LAQ
1: Input: stepsize ↵> 0  b  D  {⇠d}D
2: Initialize: ✓k  and {Qm(ˆ✓
3: for k = 1  2 ···   K do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for

end if
end for
Server updates ✓ according to (4).

Worker m uploads Qk
Set ˆ✓

k
m = ✓k  and tm = 0.

m = ˆ✓

Worker m computes rfm(✓k) and Qm(✓k).
if (7) holds for worker m then
Worker m uploads nothing.
Set ˆ✓

k1
m and tm tm + 1.

m via (6).

else

k

Table 1: A comparison of QGD and LAQ.

enough number of bits are used to quantize the gradient  and all {⇠d}D
d=1 are set 0 thus Mk := M 
then LAQ reduces to GD. Thus  adjusting b and {⇠d}D
d=1 directly inﬂuences the performance of LAQ.
The rationale behind selection criterion (7) lies in the judicious comparison between the descent
amount of GD and that of LAQ. To compare the descent amount  we ﬁrst establish the one step
descent amount of both algorithms. For all the results in this paper  the following assumption holds.
Assumption 1. The local gradient rfm(·) is Lm-Lipschitz continuous and the global gradient
rf (·) is L-Lipschitz continuous  i.e.  there exist constants Lm and L such that
(9a)
(9b)

krfm(✓1)  rfm(✓2)k2 Lmk✓1  ✓2k2  8✓1  ✓2;
krf (✓1)  rf (✓2)k2 Lk✓1  ✓2k2  8✓1  ✓2.

Building upon Assumption 1  the next lemma describes the descent in objective by GD.
Lemma 1. The gradient descent update yields following descent:

where k

GD := (1  ↵L

f (✓k+1)  f (✓k)  k
2.
2 )↵krf (✓k)k2

GD

(10)

where k

LAQ :=  ↵

The descent of LAQ distinguishes from that of GD due to the quantization and selection  which is
speciﬁed in the following lemma.
Lemma 2. The LAQ update yields following descent:
LAQ + ↵k"kk2
f (✓k+1)  f (✓k)  k
2 + ↵kPm2Mk
k1
(Qm(ˆ✓
m )  Qm(✓k))k2

In lazy aggregation  we consider only k
LAQ with the quantization error in (11) ignored. Rigorous
theorem showing the property of LAQ taking into account the quantization error will be established
in next section.
The following part shows the intuition for criterion (7a)  which is not mathematically strict but
provides the intuition. The lazy aggregation mechanism selects the quantized gradient innovation by
judging its contribution to decreasing the loss function. LAQ is expected to be more communication-
efﬁcient than GD  that is  each upload results in more descent  which translates to:

2.
2↵ )k✓k+1  ✓kk2

2 krf (✓k)k2

2  1

2 + ( L

(11)

2

c

which is tantamount to (see the derivations in the supplementary materials)

k

LAQ

|Mk| 

k
GD
M

.

k(Qm(ˆ✓

k1
m )  Qm(✓k)k2

2  krf (✓k)k2

2/(2M 2)  8m 2M k
c .

5

(12)

(13)

However  for each worker to check (73) locally is impossible because the fully aggregated gradient
rf (✓k) is required  which is exactly what we want to avoid. Moreover  it does not make sense to
reduce uploads if the fully aggregated gradient has been obtained. Therefore  we bypass directly
calculating krf (✓k)k2

2 using its approximation below.

krf (✓k)k2

2 ⇡

2
↵2

DXk=1

⇠dk✓k+1d  ✓kdk2

2

(14)

where {⇠d}D
d=1 are constants. The fundamental reason why (74) holds is that rf (✓k) can be approxi-
mated by weighted previous gradients or parameter differences since f (·) is L-smooth. Combining
(73) and (74) leads to our communication criterion (7a) with quantization error ignored.
We conclude this section by a comparison between LAQ and error-feedback (quantized) schemes.
Comparison with error-feedback schemes. Our LAQ approach is related to the error-feedback
schemes  e.g.  [3  12  24  26  27  32]. Both lines of approaches accumulate either errors or delayed
innovation incurred by communication reduction (e.g.  quantization  sparsiﬁcation  or skipping) 
and upload them in the next communication round. However  the error-feedback schemes skip
communicating certain entries of the gradient  yet communicate with all workers. LAQ skips
communicating with certain workers  but communicates all (quantized) entries. The two methods are
not mutually exclusive  and can be used jointly.

3 Convergence and communication analysis

Our subsequent convergence analysis of LAQ relies on the following assumption on f (✓):
Assumption 2. The function f (·) is µ-strongly convex  e.g.  there exists a constant µ > 0 such that
(15)

f (✓1)  f (✓2)  hrf (✓2)  ✓1  ✓2i +

µ
2 k✓1  ✓2k2

2  8✓1  ✓2.

With ✓⇤ denoting the optimal solution of (1)  we deﬁne Lyapunov function of LAQ as:

V(✓k) = f (✓k)  f (✓⇤) +

⇠j
↵ k✓k+1d  ✓kdk2

2

(16)

DXd=1

DXj=d

The design of Lyapunov function V(✓) is coupled with the communication rule (7a) that contains
parameter difference term. Intuitively  if no communication is being skipped at current iteration  LAQ
behaves like GD that decreases the objective residual in V(✓); if certain uploads are skipped  LAQ’s
rule (7a) guarantees the error of using stale gradients comparable to the parameter difference in V(✓)
to ensure its descending. The following lemma captures the progress of the Lyapunov function.
Lemma 3. Under Assumptions 1 and 2  if the stepsize ↵ and the parameters {⇠d}D
(with any 0 <⇢ 1 < 1 and ⇢2 > 0)

d=1 are selected as

DXd=1

 

1

4(1 + ⇢2)

2(1 + ⇢1

L 1  ⇢1

2 )
⇠d  min⇢ 1  ⇢1
⇠d!  
↵  min( 2
DXd=1
V(✓k+1)  1V(✓k) + Bhk"kk2
2 + Xm2Mk

4(1 + ⇢2) 

2

1

2(1 + ⇢1

L 
c⇣k"k
mk2

2 ) 

⇠d!)
DXd=1
2⌘i
2 + kˆ"k1
m k2

then the Lyapunov function follows

(17a)

(17b)

(18)

where constants 0 < 1 < 1 and B > 0 depend on ↵ and {⇠d}; see details in supplementary materials.
For the tight analysis  (17) appear to be involved  but it admits simple choices. For example  when
we choose ⇢1 = 1/2 and ⇢2 = 1  respectively  then ⇠1 = ⇠2 = ··· ⇠D = 1
8L satisfy (17).
If the quantization error in (18) is null  Lemma 3 readily implies that the Lyapunov function enjoys a
linear convergence rate. In the following  we will demonstrate that under certain conditions  the LAQ
algorithm can still guarantee linear convergence even if we consider the the quantization error.

16D and ↵ = 1

6

(a) Loss v.s. iteration

(b) Loss v.s. communication

(c) Loss v.s. bit

Figure 4: Convergence of the loss function (logistic regression)

(a) Gradient v.s. iteration

(b) Gradient v.s. communication

(c) Gradient v.s. bit

Figure 5: Convergence of gradient norm (neural network)

2 P ;
1  ⌧ 2k

mk2

2 P  8m 2M .

V(✓k)  k
k"k

Theorem 1. Under the same assumptions and the parameters in Lemma 3  Lyapunov function and
the quantization error converge at a linear rate; that is  there exists a constant 2 2 (0  1) such that
(19a)
(19b)
where P is a constant depending on the parameters in (17); see details in supplementary materials.
From the deﬁnition of Lyapunov function  it is clear that f (✓k)  f (✓⇤)  V(✓k)  k
error f (✓k)f (✓⇤) converges linearly. The L-smoothness results in krf (✓k)k2
2Lk
2 also converges linearly.
k✓k  ✓⇤k2
Compared to the previous analysis for LAG [6]  the analysis for LAQ is more involved  since
it needs to deal with not only outdated but also quantized (inexact) gradients. This modi-
ﬁcation deteriorates the monotonic property of the Lyapunov function in (18)  which is the
building block of analysis in [6]. We tackle this issue by i) considering the outdated gradi-
ent in the quantization (6); and  ii) incorporating quantization error in the new selection cri-
terion (7). As a result  Theorem 1 demonstrates that LAQ is able to keep the linear con-
vergence rate even with the presence of the quantization error. This is because the properly
controlled quantization error also converges at a linear rate; see the illustration in Figure 3.

2V0 — the risk
2  2L[f (✓kf (✓⇤)] 
2 converges linearly. Similarly  the µ-strong convexity implies

2V0 — the gradient norm krf (✓k)k2
µ k

2V0 — k✓k  ✓⇤k2

2  2

µ [f (✓k  f (✓⇤)]  2

Proposition 1. Under Assumption 1  if we choose the constants
d=1 satisfying ⇠1  ⇠2 ··· ⇠D and deﬁne dm  m 2M as:
{⇠d}D
(20)
dm := max

m  ⇠d/(3↵2M 2D)  d 2{ 1  2 ···   D} 

then  worker m has at most k/(dm + 1) communications with the
server until the k-th iteration.

d d|L2

f(k)

Bound of 

Q(k)

k

This proposition implies that the smoothness of the local loss func-
tion determines the communication intensity of the local worker.

Iterations

Figure 3: Gradient norm decay

7

0100020003000Number(cid:1)of(cid:1)iterations106105104103102101100101Loss(cid:1)residual(cid:1)(f(cid:1)f)(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)101102103104Number(cid:1)of(cid:1)communications106105104103102101100101Loss(cid:1)residual(cid:1)(f(cid:1)f)(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)1061071081091010Number(cid:1)of(cid:1)bits106105104103102101100101Loss(cid:1)residual(cid:1)(f(cid:1)f)(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)02000400060008000Number(cid:1)of(cid:1)iterations104103102101100101102Gradient(cid:1)norm(cid:1)||rf||(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)020000400006000080000Number(cid:1)of(cid:1)communications104103102101100101102Gradient(cid:1)norm(cid:1)||rf||(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)01342Number(cid:1)of(cid:1)bits1011104103102101100101102Gradient(cid:1)norm(cid:1)||rf||(cid:47)(cid:36)(cid:52)(cid:42)(cid:39)(cid:52)(cid:42)(cid:39)(cid:47)(cid:36)(cid:42)(a) MNIST

(b) ijcnn1

(c) covtype

Figure 6: Test accuracies on three different datasets

4 Numerical tests and conclusions

To validate our performance analysis and verify its communication savings in practical machine
learning problems  we evaluate the performance of the algorithm for the regularized logistic regression
which is strongly convex  and the neural network which is nonconvex. The dataset we use is
MNIST [15]  which are uniformly distributed across M = 10 workers. In the experiments  we set
D = 10 ⇠ 1 = ⇠2 = ···  ⇠ D = 0.8/D  ¯t = 100; see the detailed setup in the supplementary materials.
To benchmark LAQ  we compare it with two classes of algorithms  gradient-based algorithms and
minibatch stochastic gradient-based algorithms — corresponding to the following two tests.

(a) Loss v.s. iteration

(b) Loss v.s. communication

(c) Loss v.s. bit

Figure 7: Convergence of loss function (logistic regression)

Gradient-based tests. We consider GD  QGD [18] and lazily aggregated gradient (LAG) [6].
The number of bits per coordinate is set as b = 3 for logistic regression and 8 for neural network 
respectively. Stepsize is set as ↵ = 0.02 for both algorithms. Figure 4 shows the objective convergence
for the logistic regression task. Clearly  Figure 4(a) veriﬁes Theorem 1  e.g.  the linear convergence
rate under strongly convex loss function. As shown in Figure 4(b)  LAQ requires fewer number
of communication rounds than GD and QGD thanks to our selection rule  but more rounds than
LAG due to the gradient quantization. Nevertheless  the total number of transmitted bits of LAQ is
signiﬁcantly smaller than that of LAG  as demonstrated in Figure 4(c). For neural network model 
Figure 5 reports the convergence of gradient norm  where LAQ also shows competitive performance

Algorithm

Iteration # Communication #

Bit #

LAQ

GD

QGD

LAG

neural network

logistic

logistic

neural network

neural network

logistic

logistic

neural network

2673
8000
2820
8000
2805
8000
2659
8000

620

31845
28200
80000
28050
80000
2382
29916

1.95 ⇥ 107
4.05 ⇥ 1010
7.08 ⇥ 109
4.07 ⇥ 1011
8.81 ⇥ 108
1.02 ⇥ 1011
5.98 ⇥ 108
1.52 ⇥ 1011

Accuracy
0.9082
0.9433
0.9082
0.9433
0.9082
0.9433
0.9082
0.9433

Table 2: Comparison of gradient-based algorithms. For logistic regression  all algorithms terminate
when loss residual reaches 106; for neural network  all algorithms run a ﬁxed number of iterations.

8

1061071081091010Number(cid:1)of(cid:1)bits0.40.60.8Test(cid:1)accuracy(cid:45)(cid:34)QGDQGDLAG104105106107Number(cid:1)of(cid:1)bits0.7500.7750.8000.8250.8500.8750.900Test(cid:1)accuracy(cid:47)(cid:36)(cid:52)GDQGDLAG104105106107108Number(cid:1)of(cid:1)bits0.760.780.800.820.84Test(cid:1)accuracy(cid:47)(cid:36)QGDQGDLAG02008001000400600Number(cid:1)of(cid:1)iterations0.51.01.52.02.5Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)0200040006000800010000Number(cid:1)of(cid:1)communications0.51.01.52.02.5Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)0.00.52.02.51.0(cid:1)1.5(cid:1)Number(cid:1)of(cid:1)bits1090.51.01.52.02.5Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)(a) Loss v.s. iteration

(b) Loss v.s. communication

(c) Loss v.s. bit

Figure 8: Convergence of loss function (neural network)

for nonconvex problem. Similar to the results for logistic model  LAQ requires the fewest number of
bits. Table 2 summarizes the number of iterations  uploads and bits needed to reach a given accuracy.
Figure 6 exhibits the test accuracy of above compared algorithms on three commonly used datasets 
MNIST  ijcnn1 and covtype. Applied to all these datasets  LAQ saves transmitted bits and meanwhile
maintains the same accuracy.
Stochastic gradient-based tests. We test stochastic gradient descent (SGD)  quantized stochastic
gradient descent (QSGD) [2]  sparsiﬁed stochastic gradient descent (SSGD) [30]  and the stochastic
version of LAQ abbreviated as SLAQ. The mini-batch size is 500   ↵ = 0.008  and the number of bits
per coordinate is set as b = 3 for logistic regression and 8 for neural network. As shown in Figures 7
and 8  SLAQ incurs the lowest number of communication rounds and bits. In this stochastic gradient
test  although the communication reduction of SLAQ is not as signiﬁcant as LAQ compared with
gradient based algorithms  SLAQ still outperforms the state-of-the-art algorithms  e.g.  QSGD and
SSGD. The results are summarized in Table 3. More results under different number of bits and the
level of heterogeneity are reported in the supplementary materials.

Algorithm

Iteration # Communication #

Bit #

SLAQ

SGD

QSGD

SSGD

neural network

logistic

logistic

neural network

neural network

logistic

logistic

neural network

1000
1500
1000
1500
1000
1500
1000
1500

8255
11192
10000
15000
10000
15000
10000
15000

1.94 ⇥ 108
1.42 ⇥ 1010
2.51 ⇥ 109
7.63 ⇥ 1010
7.51 ⇥ 108
2.03 ⇥ 1010
1.26 ⇥ 109
3.82 ⇥ 1010

Accuracy
0.9018
0.9107
0.9021
0.9100
0.9021
0.9100
0.9013
0.9104

Table 3: Performance comparison of mini-batch stochastic gradient-based algorithms.

This paper studied the communication-efﬁcient distributed learning problem  and proposed LAQ that
simultaneously quantizes and skips the communication based on gradient innovation. Compared to
the original GD method  linear convergence rate is still maintained for strongly convex loss function.
This is remarkable since LAQ saves both communication bits and rounds signiﬁcantly. Numerical
tests using (strongly convex) regularized logistic regression and (nonconvex) neural network models
demonstrate the advantages of LAQ over existing popular approaches.
Acknowledgments
This work by J. Sun and Z. Yang is supported in part by the Shenzhen Committee on Science and
Innovations under Grant GJHZ20180411143603361  in part by the Department of Science and
Technology of Guangdong Province under Grant 2018A050506003  and in part by the Natural
Science Foundation of China under Grant 61873118. The work by J. Sun is also supported by China
Scholarship Council. The work by G. Giannakis is supported in part by NSF 1500713  and 1711471.

References
[1] Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent.

In Proc. Conf. Empi. Meth. Natural Language Process.  Copenhagen  Denmark  Sep 2017.

9

01500500(cid:1)1000Number(cid:1)of(cid:1)iterations(cid:1)(cid:1)1234Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)0150005000(cid:1)10000(cid:1)Number(cid:1)of(cid:1)communications1234Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)0.00.20.60.80.4Number(cid:1)of(cid:1)bits10111234Loss(cid:1)(f)(cid:54)(cid:47)(cid:36)(cid:52)(cid:54)(cid:42)(cid:39)(cid:52)(cid:54)(cid:42)(cid:39)(cid:54)(cid:54)(cid:42)(cid:39)[2] Dan Alistarh  Demjan Grubic  Jerry Li  Ryota Tomioka  and Milan Vojnovic. QSGD:
Communication-efﬁcient SGD via gradient quantization and encoding. In Proc. Advances
in Neural Info. Process. Syst.  pages 1709–1720  Long Beach  CA  Dec 2017.

[3] Dan Alistarh  Torsten Hoeﬂer  Mikael Johansson  Nikola Konstantinov  Sarit Khirirat  and
Cédric Renggli. The convergence of sparsiﬁed gradient methods. In Proc. Advances in Neural
Info. Process. Syst.  pages 5973–5983  Montreal  Canada  Dec 2018.

[4] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning
and optimization. In Proc. Advances in Neural Info. Process. Syst.  pages 1756–1764  Montreal 
Canada  Dec 2015.

[5] Jeremy Bernstein  Yu-Xiang Wang  Kamyar Azizzadenesheli  and Animashree Anandkumar.
SignSGD: Compressed optimisation for non-convex problems. In Proc. Intl. Conf. Machine
Learn.  pages 559–568  Stockholm  Sweden  Jul 2018.

[6] Tianyi Chen  Georgios Giannakis  Tao Sun  and Wotao Yin. LAG: Lazily aggregated gradient
for communication-efﬁcient distributed learning. In Proc. Advances in Neural Info. Process.
Syst.  pages 5050–5060  Montreal  Canada  Dec 2018.

[7] Tianyi Chen  Kaiqing Zhang  Georgios Giannakis  and Tamer Ba¸sar. Communication-Efﬁcient
Distributed Reinforcement Learning. IEEE Trans. on Automatic Control  submitted April 2019.
arXiv preprint:1812.03239

[8] Mert Gurbuzbalaban  Asuman Ozdaglar  and Pablo A Parrilo. On the convergence rate of
incremental aggregated gradient algorithms. SIAM Journal on Optimization  27(2):1035–1048 
2017.

[9] Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with
sparse and quantized communication. In Proc. Advances in Neural Info. Process. Syst.  pages
2525–2536  Montreal  Canada  Dec 2018.

[10] Michael I Jordan  Jason D Lee  and Yun Yang. Communication-efﬁcient distributed statistical

inference. J. American Statistical Association  to appear  2018.

[11] Michael Kamp  Linara Adilova  Joachim Sicking  Fabian Hüger  Peter Schlicht  Tim Wirtz  and
Stefan Wrobel. Efﬁcient decentralized deep learning by dynamic model averaging. In Euro.
Conf. Machine Learn. Knowledge Disc. Data.  pages 393–409  Dublin  Ireland  2018.

[12] Sai Praneeth Karimireddy  Quentin Rebjock  Sebastian Stich  and Martin Jaggi. Error feedback
ﬁxes signsgd and other gradient compression schemes. In Proc. Intl. Conf. Machine Learn. 
pages 3252–3261  Long Beach  CA  Jun 2019.

[13] Jakub Koneˇcn`y  H Brendan McMahan  Felix X Yu  Peter Richtárik  Ananda Theertha Suresh 
and Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. arXiv
preprint:1610.05492  Oct 2016.

[14] Jakub Koneˇcn`y and Peter Richtárik. Randomized distributed mean estimation: Accuracy vs

communication. Frontiers in Applied Mathematics and Statistics  4:62  Dec 2018.

[15] Yann LeCun  Corinna Cortes  and CJ Burges. Mnist handwritten digit database. AT&T Labs

[Online]. Available: http://yann. lecun. com/exdb/mnist  2:18  2010.

[16] Mu Li  David G Andersen  Alexander J Smola  and Kai Yu. Communication efﬁcient distributed
machine learning with the parameter server. In Proc. Advances in Neural Info. Process. Syst. 
pages 19–27  Montreal  Canada  Dec 2014.

[17] Xiangru Lian  Ce Zhang  Huan Zhang  Cho-Jui Hsieh  Wei Zhang  and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic
gradient descent. In Proc. Advances in Neural Info. Process. Syst.  pages 5330–5340  Long
Beach  CA  Dec 2017.

[18] Sindri Magnússon  Hossein Shokri-Ghadikolaei  and Na Li. On maintaining linear conver-
gence of distributed learning and optimization under limited communication. arXiv preprint
arXiv:1902.11163  2019.

10

[19] Brendan McMahan  Eider Moore  Daniel Ramage  Seth Hampson  and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Proc. Intl. Conf.
Artiﬁcial Intell. and Stat.  pages 1273–1282  Fort Lauderdale  FL  April 2017.

[20] Konstantin Mishchenko  Eduard Gorbunov  Martin Takáˇc  and Peter Richtárik. Distributed

learning with compressed gradient differences. arXiv preprint:1901.09269  Jan 2019.

[21] Eric J Msechu and Georgios B Giannakis. Sensor-centric data reduction for estimation with

WSNs via censoring and quantization. IEEE Trans. Sig. Proc.  60(1):400–414  Jan 2011.

[22] Angelia Nedi´c  Alex Olshevsky  and Michael Rabbat. Network topology and communication-
computation tradeoffs in decentralized optimization. Proceedings of the IEEE  106(5):953–976 
May 2018.

[23] Larry L Peterson and Bruce S Davie. Computer Networks: A Systems Approach. Morgan

Kaufman  Burlington  MA  2007.

[24] Frank Seide  Hao Fu  Jasha Droppo  Gang Li  and Dong Yu. 1-bit stochastic gradient descent
and its application to data-parallel distributed training of speech dnns. In Proc. Conf. Intl.
Speech Comm. Assoc.  Singapore  Sept 2014.

[25] Ohad Shamir  Nati Srebro  and Tong Zhang. Communication-efﬁcient distributed optimization
using an approximate newton-type method. In Proc. Intl. Conf. Machine Learn.  pages 1000–
1008  Beijing  China  Jun 2014.

[26] Sebastian U. Stich  Jean-Baptiste Cordonnier  and Martin Jaggi. Sparsiﬁed SGD with memory.
In Proc. Advances in Neural Info. Process. Syst.  pages 4447–4458  Montreal  Canada  Dec
2018.

[27] Nikko Strom. Scalable distributed DNN training using commodity gpu cloud computing. In

Proc. Conf. Intl. Speech Comm. Assoc.  Dresden  Germany  Sept 2015.

[28] Hongyi Wang  Scott Sievert  Shengchao Liu  Zachary Charles  Dimitris Papailiopoulos  and
Stephen Wright. Atomo: Communication-efﬁcient learning via atomic sparsiﬁcation. In Proc.
Advances in Neural Info. Process. Syst.  pages 9850–9861  Montreal  Canada  Dec 2018.

[29] Jianyu Wang and Gauri Joshi. Cooperative SGD: A uniﬁed framework for the design and
analysis of communication-efﬁcient SGD algorithms. arXiv preprint:1808.07576  August 2018.
[30] Jianqiao Wangni  Jialei Wang  Ji Liu  and Tong Zhang. Gradient sparsiﬁcation for
communication-efﬁcient distributed optimization. In Proc. Advances in Neural Info. Process.
Syst.  pages 1299–1309  Montreal  Canada  Dec 2018.

[31] Wei Wen  Cong Xu  Feng Yan  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Proc. Advances in
Neural Info. Process. Syst.  pages 1509–1519  Long Beach  CA  Dec 2017.

[32] Jiaxiang Wu  Weidong Huang  Junzhou Huang  and Tong Zhang. Error compensated
quantized SGD and its applications to large-scale distributed optimization. arXiv preprint
arXiv:1806.08054  2018.

[33] Hao Yu and Rong Jin. On the computation and communication complexity of parallel SGD
with dynamic batch sizes for stochastic non-convex optimization. In Proc. Intl. Conf. Machine
Learn.  Long Beach  CA  Jun 2019.

[34] Hantian Zhang  Jerry Li  Kaan Kara  Dan Alistarh  Ji Liu  and Ce Zhang. Zipml: Training
linear models with end-to-end low precision  and a little bit of deep learning. In Proc. Intl. Conf.
Machine Learn.  pages 4035–4043  Sydney  Australia  Aug 2017.

[35] Sixin Zhang  Anna E Choromanska  and Yann LeCun. Deep learning with elastic averaging
SGD. In Proc. Advances in Neural Info. Process. Syst.  pages 685–693  Montreal  Canada  Dec
2015.

[36] Yuchen Zhang and Xiao Lin. DiSCO: Distributed optimization for self-concordant empirical

loss. In Proc. Intl. Conf. Machine Learn.  pages 362–370  Lille  France  June 2015.

11

,Hidekazu Oiwa
Ryohei Fujimaki
Youssef Alami Mejjati
Christian Richardt
James Tompkin
Darren Cosker
Kwang In Kim
Jun Sun
Tianyi Chen
Georgios Giannakis
Zaiyue Yang