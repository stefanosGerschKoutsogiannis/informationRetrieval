2009,Maximum likelihood trajectories for continuous-time Markov chains,Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random.  Many computational problems related to such chains have been solved  including determining state distributions as a function of time  parameter estimation  and control.  However  the problem of inferring most likely trajectories  where a trajectory is a sequence of states as well as the amount of time spent in each state  appears unsolved.  We study three versions of this problem: (i) an initial value problem  in which an initial state is given and we seek the most likely trajectory until a given final time  (ii) a boundary value problem  in which initial and final states and times are given  and we seek the most likely trajectory connecting them  and (iii) trajectory inference under partial observability  analogous to finding maximum likelihood trajectories for hidden Markov models.  We show that maximum likelihood trajectories are not always well-defined  and describe a polynomial time test for well-definedness.  When well-definedness holds  we show that each of the three problems can be solved in polynomial time  and we develop efficient dynamic programming algorithms for doing so.,Maximum likelihood trajectories for continuous-time

Markov chains

Theodore J. Perkins

Ottawa Hospital Research Institute

Ottawa  Ontario  Canada
tperkins@ohri.ca

Abstract

Continuous-time Markov chains are used to model systems in which transitions
between states as well as the time the system spends in each state are random.
Many computational problems related to such chains have been solved  including
determining state distributions as a function of time  parameter estimation  and
control. However  the problem of inferring most likely trajectories  where a tra-
jectory is a sequence of states as well as the amount of time spent in each state 
appears unsolved. We study three versions of this problem: (i) an initial value
problem  in which an initial state is given and we seek the most likely trajectory
until a given ﬁnal time  (ii) a boundary value problem  in which initial and ﬁnal
states and times are given  and we seek the most likely trajectory connecting them 
and (iii) trajectory inference under partial observability  analogous to ﬁnding max-
imum likelihood trajectories for hidden Markov models. We show that maximum
likelihood trajectories are not always well-deﬁned  and describe a polynomial time
test for well-deﬁnedness. When well-deﬁnedness holds  we show that each of the
three problems can be solved in polynomial time  and we develop efﬁcient dy-
namic programming algorithms for doing so.

1

Introduction

A continuous-time Markov chain (CTMC) is a model of a dynamical system which  upon entering
some state  remains in that state for a random real-valued amount of time (called the dwell time or
occupancy time) and then transitions randomly to a new state. CTMCs are used in a wide variety of
domains. In stochastic chemical kinetics  states may correspond to the conformation of a molecule
such as a protein  peptide or nucleic acid polymer  and transitions correspond to conformational
changes (e.g.  [1]). Or  the state may correspond to the numbers of different types of molecules in
an interacting system  and transitions are the result of chemical reactions between molecules [2].
In phylogenetics  the states may correspond to the genomes of different organisms  and transitions
to the evolutionary events (mutations) that separate those organisms [3]. Other application domains
include queueing theory  process control and manufacturing  quality control  formal veriﬁcation  and
robot nagivation.
Many computational problems associated with CTMCs have been solved  often by generalizing
methods developed for discrete-time Markov chains (DTMCs). For example  stationary distribu-
tions for CTMCs can be computed in a manner very similar to that for DTMCs [4]. Estimating the
parameters of a CTMC from fully observed data involves estimating state transition probabilities 
just as for DTMCs  but adds estimation of the state dwell time distributions. Estimating parameters
from partially observed data can be done by a generalization of the well-known Baum-Welch algo-
rithm for parameter estimation for hidden Markov models [5] or by Bayesian methods [6  7]. When
the state of a CTMC is observed periodically through time  but some transitions between observa-
tion times may go unseen  the parameter estimation problem can also be solved through embedding

1

techniques [8]. In scenarios such as manufacturing or robot navigation  one may assume that the
state transitions or dwell times are under at least partial control. When control choices are made
once for each state entered  dynamic programming and related methods can be used to develop opti-
mal control strategies [9]. When control choices are made continuously in time  methods for hybrid
system control are more appropriate [10].
Another fundamental and well-studied problem for CTMCs is to compute  given an initial state and
time  the state distribution or most likely state at a later time. These problems are readily solved for
DTMCs by dynamic programming [11]  but for the CTMCs  solutions have a somewhat different
ﬂavor. One approach is based on the forward Chapman-Kolmogorov equations [4]  called the Mas-
ter equation in the stochastic chemical kinetics literature [12]. These specify a system of ordinary
differential equations the describe how the probabilities of being in each state change over time.
Solving the equations  sometimes analytically but more often numerically  yields the entire state
distribution as a function of time. Alternatively  one can uniformize the CTMC  which produces
a DTMC along with a probability distribution for a number of transitions to perform. The process
obtained by choosing the number of transitions  and then producing a trajectory with that many tran-
sitions from the DTMC  has the same state distribution as the original CTMC. This representation
allows particularly efﬁcient computation of the state distribution if that distribution is only required
at one or a smaller number of different times. Finally  especially in the chemical kinetics commu-
nity  stochastic simulation algorithms are popular [13]. These approaches act by simply simulating
trajectories from the CTMC to produce empirical  numerical estimates of state distributions or other
features of the dynamics.
Despite the extensive work on a variety of problems related to to CTMCs  to the best of our knowl-
edge  the problem of ﬁnding most likely trajectories has not been addressed. With this paper  we
attempt to ﬁll that gap. We propose dynamic programming solutions to three variants of the problem:
(i) an initial value problem  where a starting state and ﬁnal time are given  and we seek the most
likely sequence of states and dwell times occurring up until the ﬁnal time  (ii) a boundary value
problem  where initial and ﬁnal states and times are given  and we seek the most likely intervening
trajectory  and (iii) a problem involving partial observability  where we have a sequence of “obser-
vations” that may not give full state information  and we want to infer the most likely trajectory that
the system followed in producing the observations.

2 Deﬁnitions
A CTMC is deﬁned by four things: (i) a ﬁnite state set S  (ii) initial state probabilities  Ps for s ∈ S 
(iii) state transition probabilities Pss(cid:48) for s  s(cid:48) ∈ S  and (iv) state dwell time parameters λs for each
s ∈ S. Let St ∈ S denote the state of the system at time t ∈ [0  +∞). The rules for the evolution
of the system are that it starts in state S0  which is chosen according to the distribution Ps. At any
time t  when the system is in state St = s  the system stays in state s for a random amount of time
that is exponentially distributed with parameter λs. When the system ﬁnally leaves state s  the next
state of the system is s(cid:48) (cid:54)= s with probability Pss(cid:48).
A trajectory of the CTMC is a sequence of states along with the dwell times in all but the
last state U = (s0  t0  s1  t1  . . .   sk−1  tk−1  sk). The meaning of this trajectory is that the
system started in state s0  where it stayed for time t0  then transitioned to state s1  where it
stayed for time t1  and so on. Eventually  the system reaches state sk  where it remains. Let
Ut = (s0  t0  s1  t1  . . .   skt−1  tkt−1  skt) be a random variable describing the trajectory of the
system up until time t. In particular  this means that there are kt state transitions up until time t
(where kt is itself a random variable)  the system enters state skt sometime at or before time t  and
remains in state skt until sometime after time t.
Given the initial state  S0  and a time t  the likelihood of a particular trajectory U is
l(Ut = U|S0) =

if s0 (cid:54)= S0 or(cid:80)k−1

i=0 ti > t

(cid:40) 0
(cid:0)Πk−1

−λsk(t−(cid:80)

i

ti)(cid:17)

i=0 λsie−λsi tiPsisi+1

(cid:1)(cid:16)

e

2

When(cid:80)

(1)
i ti > t  the likelihood is zero  because it means that the speciﬁed transitions have not
completed by time t. Otherwise  the terms inside the ﬁrst parentheses account for the likelihood of
the dwell times and the state transitions in the sequence  and the term inside the second parentheses

otherwise

accounts for the probability that the dwell time in the ﬁnal state does not complete before time t.
With this notation  the initial value problem we study is easily stated as

where s ∈ S and t > 0 are both given. The boundary value problem we study is

arg max
U

l(Ut = U|S0 = s)  

arg max
U

l(Ut = U|S0 = s  St = s(cid:48)).

(2)

(3)

Here  the given s and s(cid:48) are any states in S  possibly the same state  and t > 0 is also given.
A hidden continuous-time Markov chain (HCTMC) adds an observation model to the CTMC.
In particular  we assume a ﬁnite set of possible observations O. When the system is observed
and it is in state s ∈ S  the observer sees observation o ∈ O with probability Pso. Let O =
(o1  τ1  o2  τ2  . . .   om  τm) denote a sequence of observations and the times at which they are made.
We assume that the observation times are ﬁxed  being chosen ahead of time  and depend in no way on
the evolution of the chain itself. Given a trajectory of the system U = (s0  t0  s1  t1  . . .   tk−1  sk) 
let U(t) denote the state of the system at time t implied by that sequence. Then  the probability of
an observation sequence O given the trajectory U can be written as

P (O|Uτm = U) = Πm

i=1PU(τi)oi

(4)

The ﬁnal problem we study in this paper is that of ﬁnding the most likely trajectory given an obser-
vation sequence:

arg max
U

l(Uτm = U|O) ∝ arg max

U

P (O|Uτm = U)l(Uτm = U)

(5)

3 Solving the initial and boundary value problems

In this section we develop solutions to problems (2) and (3). The ﬁrst step in this development is
to show that we can analytically optimize the dwell times if we are given the state sequence. This
is covered in the next subsection. Following that  we develop a dynamic program to ﬁnd optimal
state sequences  assuming that the dwell times are set to their optimal values relative to the state
sequence.

3.1 Maximum likelihood dwell times

Consider a particular trajectory U = (s0  t0  s1  t1  . . .   sk−1  tk−1  sk). Given S0 and a time t  the
likelihood of that particular trajectory  l(Ut = U|S0) is given above by Equation (1). Let us assume
that S0 = s0  as we have no need to consider U starting from the wrong state  and let us maximize
l(Ut = U|S0) with respect to the dwell times. To be concise  let Ttk = {(t0  t1  . . .   tk−1) : ti ≥
i ti ≤ t}. This is the set of all feasible dwell times for the states up until

0 for all 0 ≤ i < k and (cid:80)

state sk. Then we can write the desired optimization as

i=0 λsie−λsi tiPsisi+1
It is more convenient to maximize the logarithm  which gives us

arg max(t0 ... tk−1)∈Ttk

(cid:1)(cid:16)
e−λsk (t−Σiti)(cid:17)
(cid:33)

.

(cid:0)Πk−1
(cid:32)k−1(cid:88)

i=0

arg max(t0 ... tk−1)∈Ttk

log λsi − λsiti + log Psisi+1

− λsk (t − Σjtj)

Dropping the terms that do not depend on any of the ti and rearranging  we ﬁnd the equivalent
problem

arg max(t0 ... tk−1)∈Ttk

k−1(cid:88)
(λsk − λsi)ti

The solution can be obtained by inspection. If λsk ≤ λsi for all 0 ≤ i < k  then we must have all
ti = 0. That is  the system transitions instantaneously through the states s0  s1  . . .   sk−1 and then

i=0

3

(6)

(7)

(8)

dwells in state sk for (at least) time t.1 Otherwise  let j be such that λsj is minimal for 0 ≤ j < k.
Then an optimal solution has tj = t  and all other ti = 0. Intuitively  this says that if state sj has the
largest expected dwell time (corresponding to the smallest λ parameter)  then the most likely setting
of dwell times is obtained by assuming all of the time t is spent in state sj  and all other transitions
happen instantaneously. This is not unintuitive  although it is dissatisfying in the sense that the most
likely set of dwell times are not typical in some sense. For example  none are near their expected
value. Moreoever  the basic character of the solution—that all the time t goes into waiting at the
slowest state—is independent of t. Nevertheless  being able to solve explicitly for the most likely
dwell times for a given state sequence makes it much easier to ﬁnd the most likely Ut. So  let us
press onwards.

3.2 Dynamic programming for the most likely state sequence

Substituting back our solution for the ti into Equation (1)  and continuing our assumption that s0 =
S0  we obtain


(cid:0)Πk−1
(cid:0)Πk−1
= (cid:0)Πk−1

i=0 λsiPsisi+1

(cid:1) e−λsk t
(cid:1) e−(mink−1
(cid:1) e−(mink

i=0 λsi)t

i=0 λsiPsisi+1

i=0 λsiPsisi+1

i=0 λsi )t

if λsk ≤ λsi for
all 0 ≤ i < k
otherwise

(9)

max

(t0 ... tk−1)∈Ttk

l(Ut = U|S0) =

This leads to a dynamic program for ﬁnding the state sequence that maximizes the likelihood. As is
typical  we build maximum likelihood paths of increasing length by ﬁnding the best ways of extend-
ing shorter paths. The main difference with a more typical scenario is that to score an extension we
need to know not just the score and ﬁnal state of the shorter path  but also the smallest dwell time
parameter along that path. Deﬁne a (k  s  λ)-trajectory to be one that includes k ∈ {0  1  2  . . .} state
transitions  ends at state sk = s  and for which the smallest dwell time parameter of any state along
the trajectory is λ. Then deﬁne Fk(s  λ) to be the maximum achievable l(Ut = U|S0)  where we
restrict attention to U that are (k  s  λ)-trajectories. We initialize the dynamic program as:

F0(S0  λS0) = e−tλS0
F0(s  λ) = 0 for all (s  λ) (cid:54)= (S0  λS0)

To compute Fk(s  λ) for larger k  we ﬁrst observe that Fk(s  λ) is undeﬁned if λ > λs. This is
because there are no (k  s  λ)-trajectories if λ > λs. The fact that a trajectory ends at state s implies
that the minimum dwell time parameter along the trajectory can be no greater than λs. So  we only
compute Fk(s  λ) for λ ≤ λs.
To determine Fk+1(s  λ)  we must consider two cases.
If λ < λs  then the best (k + 1  s  λ)-
trajectory must come from some (k  s(cid:48)  λ)-trajectory. That is  the length k trajectory must already
have a dwell time parameter of λ along it. The state s(cid:48) can be any state other than s. If λ = λs  then
the best (k + 1  s  λ)-trajectory may be an extension of any (k  s(cid:48)  λ(cid:48))-trajectory with λ(cid:48) ≥ λ and
s (cid:54)= s(cid:48). To be more concise  deﬁne

G(s  λ) =

{λs(cid:48) : λs(cid:48) ≥ λ}

if λ < λs
if λ = λs

(10)

We then compute F for increasing k as:

Fk+1(s  λ) =

max

s(cid:48)(cid:54)=s λ(cid:48)∈G(s λ)

Fk(s(cid:48)  λ(cid:48))λs(cid:48)Ps(cid:48)se−t(λ−λ(cid:48))

The ﬁrst term on the right hand side accounts for the likelihood of the best (k  s(cid:48)  λ(cid:48))-trajectory. The
next two terms account for the dwell in s(cid:48) and the transition probability to s. The ﬁnal term accounts
for any difference between the smallest dwell time parameters along the k and k + 1 transition
trajectories.

1If the reader is not comfortable with a dwell time exactly equal to zero  one may instead take ti = 0 as a
shorthand for an inﬁnitesimal but positive dwell time. Alternatively  the optimization problem can be modiﬁed
to explicitly require ti > 0. However  this does nothing to change the fundamental nature of the solution  while
resulting in a signiﬁcantly more laborious exposition.

4

(cid:26) {λ}

Figure 1: A continuous-time Markov chain used as a demonstration domain. The ﬁve circles corre-
spond to states  and the arrows to transitions between states. States are also labeled with their dwell
time parameters.

Because the set of possible states  S  is ﬁnite  so is the set of possible dwell time parameters  λs for
s ∈ S. The size of the table Fk for each k is thus at most |S|2. If we limit k to some maximum value
K  then the total size of all the tables is at most K|S|2  and the total computational effort O(K|S|3).
To solve the initial value problem (2)  we scan over all values of k  s and λ to ﬁnd the maximum
value of Fk(s  λ). Such a value implies that the most likely state sequence ends at state s after k
state transitions. We can use a traceback to reconstitute the full sequence of states  and the result of
the previous section to obtain the most likely dwell times. To solve the boundary value problem (3) 
we do the same  except that we only scan over values of k and λ  looking for the maximum value of
Fk(St  λ).

3.3 Examples

3  whereas the direct path (x  z) simply has probability Pxz = 1

In this section  we use the toy chain depicted in Figure 1 to demonstrate the algorithm of the previous
section  and to highlight some properties of maximum likelihood trajectories. First  suppose that we
know the system is in state x at time zero and in state z at time t. There are two different paths 
(x  z) and (x  y  z)  that lead from x to z.
If we ignore the issue of dwell times and consider
only the transition probabilities  then the path (x  y  z) seems more probable.
Its probability is
3 · 1 = 2
PxyPyz = 2
3. However  if
we consider the dwell times as well  the story can change. For example  suppose that t = 1. Note
that λy = 1
10  so that the expected dwell time in state y is 10. If the chain enters state y  the chance
of it leaving y before time t = 1 is quite small. If we run the dynamic programming algorithm of
the previous section to ﬁnd the most likely trajectory  it ﬁnds (s0 = x  t0 = 0  s1 = z) to be most
likely  with a score of 0.1226. Along the way  it computes the likelihood of the most likely path
going through y  which is (s0 = x  t0 = 0  s1 = y  t1 = t  s2 = x). It prefers to place all the dwell
time t in state y  because that state is most likely to have a long dwell time. However  the total score
of this trajectory is still only 0.0603  making the direct path the more likely one. On the other hand 
if t = 2  then the path through y becomes more likely by a score of 0.0546 to 0.0451. If t = 10  then
the path through y still has a likelihood of 0.0245  whereas the direct path has a likelihood below
2 × 10−5  because it is highly unlikely to remain in x and/or z for so long.
Next  suppose that we know S0 = a and that we are interested in knowing the most likely trajectory
out until time t  regardless of the ﬁnal state of that trajectory. For simplicity  suppose also that
λa = λb. There is only one possible state sequence containing k transitions for each k = 0  1  2  . . . 
and the likelihood of any such sequence turns out to be independent of the dwell times (assuming
the dwell times total no more than time t):

(Πk−1

i=0 λe−λti)e−λ(t−Σiti) = e−λtλk

(11)
If λ < 1  this implies the optimal trajectory has the system remaining at state a. However  if λ = 1
then all trajectories of all lengths have the same likelihood. If λ > 1  then there are trajectories of
arbitrarily large likelihood  but no maximum likelihood trajectory. Intuitively  because the likelihood
of a dwell time can be greater than one  the likelihood of a trajectory can be increased by including
short dwells in states with high dwell parameters λ.
In general  if a continuous-time Markov chain has a cycle of states (s0  s1  . . .   sk = s0)  such
that Πk−1
i=0 Psisi+1λsi > 1  then maximum likelihood trajectories do not exist. Rather  a sequence of

5

abxzyλy=1/10λx=1λz=1λaPxz=1/3Pxy=2/3Pyz=Pza=Pab=Pba=1λbFigure 2: Abstract example of a continuous-time trajectory of a chain  along with observations taken
at ﬁxed time intervals.

trajectories with ever-increasing likelihood can be found starting from any state from which the cycle
is reachable. One should  thus  always check the chain for this property before seeking maximum
likelihood trajectories. This can be easily done in polynomial time. For example  one can label the
edges of the transition graph with the weights log Pss(cid:48)λs for the edge from s to s(cid:48)  and then check
the graph for the existence of a positive-weight cycle—a well-known polynomial-time computation.

4 Solving the partially observable problem

l(Uτm = U|O) ∝ P (O|Uτm = U)l(Uτm = U)

We now turn to problem (12)  where we are given an observation sequence O =
(o1  τ1  o2  τ2  . . .   om  τm) and want to ﬁnd the most likely trajectory U. For simplicity  we as-
sume that τ1 = 0. The following can be straightforwardly generalized to allow the ﬁrst observation
to take place sometime after the trajectory begins. Similarly  we restrict attention to trajectories
i tk ≤ τm  so that we do not concern ourselves with
extrapolating the trajectory beyond the ﬁnal observation time. The conditional likelihood of such a
trajectory can be written as

U = (s0  t0  s1  t0  . . .   tk−1  sk) where(cid:80)
(cid:1)(cid:16)

include the probability of starting in state s0  and we have implicitly assumed that(cid:80)

(12)
(13)
The term in the ﬁrst parentheses is P (O|Uτm = U)  and the term in the second parentheses is
l(Uτm = U). The only differences between the second parentheses and Equation (1) is that we now
i tk ≤ τm  as
mentioned above. This form  however  is not convenient for optimizing U. To do this  we need to
rewrite l(Uτm = U) in a way that separates the likelihood into events happening in each interval of
time between observations.

i=0 λsie−λsi tiPsisi+1

(cid:1)(cid:16)

e−λsk (t−Σiti)(cid:17)(cid:17)

(cid:0)Πk−1

= (cid:0)Πm

i=1PU(τi)oi

Ps0

4.1 Decomposing trajectory likelihood by observation intervals

For simplicity  let us further restrict attention to trajectories U that do not include a transition
into a state si precisely at any observation time τj. We do not have space here to show that
this restriction does not affect the value of the optimization problem; this will be addressed in
the full paper. The likelihood of the trajectory can be written in terms of the events in each ob-
servation interval. For example  consider the trajectory and observations depicted in Figure 2.
In the ﬁrst interval  the system starts in state s0 and transitions to s1  where it stays until time
τ2. The likelihood of this happening is Ps0 λs0 e−λs0 t0Ps0s1e−λs1 (τ2−t0). In the second observa-
tion interval  the system never leaves state s1. The probability of this happening is e−λs1 (τ3−τ2).
Finally  in the third interval  the system continues in state s1 before transitioning to state s2
and then s3  where it remains until the ﬁnal observation. The likelihood of this happening is
λs1e−λs1 (t0+t1−τ3)Ps1s2λs2e−λs2 t2 ps2s3e−λs3 (τ4−t0−t1−t2). If we multiply these together  we ob-
tain the full likelihood of the trajectory  Ps0(Π2
In general  let Ui = (si0  ti0  si1  ti1  . . .   siki) denote the sequence of states and dwell times of
trajectory U during the time interval [τi  τi+1). The ﬁrst dwell time ti0  if any  is measured with
respect to the start of the time interval. The component of the likelihood of the whole trajectory U
attributable to the ith time interval is nothing other than l(Uτi+1−τi = Ui|S0 = si0). Thus  the
likelihood of the whole trajectory can be written as
l(Uτm = U) = Ps0Πm−1

i=1 l(Uτi+1−τi = Ui|S0 = si0)

i=0λsie−λsi ti)e−λs3 (τ4−Σj tj ).

(14)

6

s0s1s2s3o1o2o3o4time4.2 Dynamic programming for the optimal trajectory

Combining Equations (12) and (14)  we ﬁnd
l(Uτm = U|O) ∝ PU(0)PU(0)o1Πm−1

i=1 l(Uτi+1−τi = Ui|S0 = U(τi))PU(τi+1)oi+1

(15)
The ﬁrst two terms account for the probability of the initial state and the probability of the ﬁrst
observation given the initial state. The terms inside the product account for the likelihood of the ith
interval of the trajectory  and the probability of the (i + 1)st observation  given the state at the end
of the ith interval of the trajectory.
One immediate implication of this rewriting of the conditional likelihood is the following. At times
τi and τi+1  the system is in states U(τi) and U(τi+1). If U is to maximize the conditional likeli-
hood  it had better be that the fragment of the trajectory between those two times  Ui  is a maximum
likelihood trajectory from state U(τi) to state U(τi+1) in time τi+1 − τi. If it is not  then an alterna-
tive  higher likelihood trajectory fragment could be swapped into U  resulting in a higher conditional
likelihood. Let us deﬁne

(16)
to be the maximum achievable likelihood by any trajectory from state s to state s(cid:48) in time t. Then a
necessary condition for U to maximize the conditional likelihood is

l(Ut = U(cid:48)|S0 = s  St = s(cid:48))

Ht(s  s(cid:48)) = max
U(cid:48)

l(Uτi+1−τi = Ui|S0 = U(τi)) = Hτi+1−τi(U(τi)  U(τi+1)) .

(17)
Moreover  to ﬁnd an optimal U  we can simply assume that the above condition holds  and con-
cern ourselves only with ﬁnding the best endpoints for the each time interval  U(τi) and U(τi+1).
(Of course  the endpoint of one interval must be the same as the initial point of the next interval.)
Speciﬁcally  deﬁne Ji(s) to be the likelihood of the most likely trajectory covering the time interval
[τ1  τi]  accounting for the ﬁrst i observations  and ending at state s. The we can compute J as
follows. To initialize  we set
Then  for i = 1  2  . . .   m − 1 

J1(s) = PsPso1 .

(18)

Ji+1(s) = max

s(cid:48) Ji(s(cid:48))Hτi+1−τi(s(cid:48)  s)Psoi+1 .

(19)

We can then reconstruct the most likely trajectory by ﬁnding s that maximizes Jm(s) and tracing
back to the beginning. This algorithm is identical to the Viterbi algorithm for ﬁnding most likely
state sequences for hidden Markov models  with the exception that the state transition probabilities
in the Viterbi algorithm are replaced by the Hτi+1−τi(s(cid:48)  s) terms above—which can  of course  be
computed based on the results of the previous section.

4.3 Examples

To demonstrate this algorithm  let us return to the CTMC depicted in Figure 1. We assume that
λa = λb = 1  that the system always starts in state x  and that when we observe the system  we
get a real-valued Gaussian observation with standard deviation 1 and means 0  10  3  100 and 100
for states x  y  z  a and b respectively.2 The left side of Figure 3 shows three sample sequences
of 20 observations. The right side of the ﬁgure shows the most likely trajectories inferred under
different assumptions. First  if we assume the time interval between observations is t = 1  and we
consider observations OA  then the most likely trajectory has the system in state x up through the
10th observation  after which it instantly transitions to state z and remains there. This makes sense 
as the lower observations at the start of the series are more likely in state x. If we consider instead
observations OB  which has a high observation at time t = 11  the procedure infers that the system
was in state y at that time. Moreover  it predicts that the system switches into y immediately after the
10th observation  and says there until just before the 12th observation  taking advantage of the fact
that longer dwell times are more likely in state y than in the other states. If we consider observations
OC  which have a spike at t = 5  the transit to state y is moved earlier  and state z is used to explain
observations at t = 6 onward  even though the ﬁrst few are relatively unlikely in that state. If we

2Although our derivations above assume the observation set O is ﬁnite  the same approach goes through if

O is continuous and individual observations have likelihoods instead of probabilities.

7

Figure 3: Left: three length-20 observation sequences  OA  OB  and OC. All three are the same
at most points  but the 11th observation of OB is 10  and the 5th observation of OC is 10. Right:
most likely trajectories inferred by our algorithm  assuming the underlying CTMC is the one given
in Figure 1  with parameters given in the text.

return to observations OA  but we assume that the time interval between observations is t = 2  then
the most likely trajectory is different than it is for t = 1. Although the same states are used to explain
the observations  the most likely trajectory has the system transitioning from x to y immediately
after the 10th observation and dwelling there until just before the 11th observation  where the state
becomes z. This is because  as explained previously  this is the more likely trajectory from x to z
given t = 2. If we assume the time interval between observations is t = 20  then a wider range of
observations during the trajectory are attributed to state y. Intuitively  this is because  although the
observations are somewhat unlikely under state y  it is extremely unlikely for the system to dwell
for so long in state z as to account for all of the observations from the 11th onward.

5 Discussion

We have provided correct  efﬁcient algorithms for inferring most likely trajectories of CTMCs given
either initial or initial and ﬁnal states of the chain  or given noisy/partial observations of the chain.
Given the enormous practical import of the analogous problems for discrete-time chains  we are
hopeful that our methods will prove useful additions to the toolkit of methods available for analyzing
continuous-time chains. An alternative  existing approach to the problems we have addressed here is
to discretize time  producing a DTMC which is then analyzed by standard methods [14]. A problem
with this approach  however  is that if the time step is taken too large  the discretized chain can
collapse a whole set of transition sequences of the CTMC into a single “pseudotransition”  obscuring
the real behavior of the system in continuous time. If the time step is taken to be sufﬁciently small 
then the DTMC should produce substantially the same solutions as our approach. However  the
time complexity of the calculations increases as the time step shrinks  which can be a problem if
we are interested in long time intervals and/or there are states with very short expected dwell times 
necessitating very small time steps.
A related problem on which we are working is to ﬁnd the most probable state sequence of a
continuous-time chain under similar informational assumptions. By this  we mean that the dwell
times  rather than being optimized  are marginalized out  so that we are left with only the sequence
of states and not the particular times they occurred. In many applications  this state sequence may
be of greater interest than the dwell times—especially since  as we have shown  maximum likeli-
hood dwell times are often inﬁnitessimal and hence non-representative of typical system behavior.
Morever  this version of the problem has the advantage of always being well-deﬁned. Because state
sequences have probabilities rather than likelihoods  a most probable state sequence will always
exist.

Acknowledgments

Funding for this work was provided in part by the National Sciences and Engineering Research
Council of Canada and by the Ottawa Hospital Research Institute.

8

xzOA  t=20yxzOA  t=2yxzOC  t=1yxzOB  t=1yxzOA  t=1observation number1510152015101520−20246810observation numberobservation  OAOBOCReferences
[1] FG Ball and JA Rice. Stochastic models for ion channels:

Mathematical biosciences  112(2):189  1992.

introduction and bibliography.

[2] D.J. Wilkinson. Stochastic modelling for systems biology. Chapman & Hall/CRC  2006.
[3] M. Holder and P.O. Lewis. Phylogeny estimation: traditional and Bayesian approaches. Nature

Reviews Genetics  4(4):275–284  2003.

[4] H.M. Taylor and S. Karlin. An introduction to stochastic modeling. Academic Press  1998.
[5] D.R. Fredkin and J.A. Rice. Maximum likelihood estimation and identiﬁcation directly from

single-channel recordings. Proceedings: Biological Sciences  pages 125–132  1992.

[6] R. Rosales  J.A. Stark  W.J. Fitzgerald  and S.B. Hladky. Bayesian restoration of ion channel

records using hidden Markov models. Biophysical Journal  80(3):1088–1103  2001.

[7] M.A. Suchard  R.E. Weiss  and J.S. Sinsheimer. Bayesian selection of continuous-time Markov

chain evolutionary models. Molecular Biology and Evolution  18(6):1001–1013  2001.

[8] DT Crommelin and E. Vanden-Eijnden. Fitting timeseries by continuous-time Markov chains:
Journal of Computational Physics  217(2):782–805 

A quadratic programming approach.
2006.

[9] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley and Sons  New York  1994.

[10] S. Hedlund and A. Rantzer. Optimal control of hybrid systems. In Decision and Control  1999.

Proceedings of the 38th IEEE Conference on  volume 4  1999.

[11] D.P. Bertsekas. Dynamic programming and optimal control. Athena Scientiﬁc Belmont  Mass 

1995.

[12] NG Van Kampen. Stochastic processes in physics and chemistry. North-Holland  2007.
[13] D. T. Gillespie. Exact stochastic simulation of coupled chemical reactions. Journal of Physical

Chemistry  81:2340–2361  1977.

[14] A. Hordijk  D.L. Iglehart  and R. Schassberger. Discrete time methods for simulating continu-

ous time Markov chains. Advances in Applied Probability  pages 772–788  1976.

9

,Khaled Refaat
Arthur Choi
Adnan Darwiche
Marc Bellemare
Sriram Srinivasan
Georg Ostrovski
Tom Schaul
David Saxton
Remi Munos