2012,Collaborative Gaussian Processes for Preference Learning,We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior  but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally  we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.,Collaborative Gaussian Processes for

Preference Learning

Neil Houlsby ∗

Department of Engineering
University of Cambridge

Jose Miguel Hern´andez-Lobato ∗

Department of Engineering
University of Cambridge

Ferenc Husz´ar

Department of Engineering
University of Cambridge

Zoubin Ghahramani

Department of Engineering
University of Cambridge

Abstract

We present a new model based on Gaussian processes (GPs) for learning pair-
wise preferences expressed by multiple users. Inference is simpliﬁed by using
a preference kernel for GPs which allows us to combine supervised GP learn-
ing of user preferences with unsupervised dimensionality reduction for multi-user
systems. The model not only exploits collaborative information from the shared
structure in user behavior  but may also incorporate user features if they are avail-
able. Approximate inference is implemented using a combination of expectation
propagation and variational Bayes. Finally  we present an efﬁcient active learning
strategy for querying preferences. The proposed technique performs favorably on
real-world data against state-of-the-art multi-user preference learning algorithms.

1

Introduction

Preference learning is concerned with making inference from data consisting of pairs of items and
corresponding binary labels indicating user preferences. This data arises in many contexts  including
medical assistive technologies [1]  graphical design [3] and recommendation systems [5]. A popular
modeling approach assumes the existence of a utility function f such that f (x) gives the utility of
an item with feature vector x; f (xi) > f (xj) indicates that item i is preferred to item j. Bayesian
methods can be used to learn f  for example  by modeling f independently for each user as a draw
from a Gaussian process (GP) prior [4]. However  when data from many users is available  such
methods do not leverage similarities in preferences across users. Current multi-user approaches
require that features are available for each user and assume that users with similar features have
similar preferences [2]  or perform single-user learning  ignoring user features  but tie information
across users with a hierachical prior [1]. These methods are not ﬂexible and can only address one of
two possible scenarios: a) user features are available and they are useful for prediction and b) when
this is not the case. Additionally  they involve at least solving U GP problems  where U is the total
number of users. This cost is prohibitive even for modest U. Our approach  by contrast  can address
both a) and b) by combining informative user features with collaborative information. Furthermore 
we perform scalable inference which can handle problems with large U.
Our new multi-user model is based on dimensionality reduction ideas from the ﬁeld of collabora-
tive ﬁltering [19  16]. Unsupervised learning of similarities in users’ behavior is exploited without
requiring access to user-speciﬁc feature vectors. However  if these are available it may be desirable

∗Both authors contributed equally.

1

to incorporate them for predictions; our model can use these user-speciﬁc features as well. The
proposed method is based on a connection between preference learning and GP binary classiﬁca-
tion. We show that both problems are equivalent when a covariance function called the preference
kernel is used. This speciﬁc kernel simpliﬁes the inference process  allowing us to implement more
complex models such as the proposed multi-user approach. Finally  in real scenarios  querying users
for preference may be costly and intrusive  so it is desirable to learn preferences using the least
data possible. With this objective  we present BALD (Bayesian active learning by disagreement)  an
efﬁcient active learning strategy for binary classiﬁcation problems with GP priors.

2 Pairwise preference learning as special case of binary classiﬁcation

The problem of pairwise preference learning can be recast as a special case of binary classiﬁcation.
Let us consider two items i and j with corresponding feature vectors xi  xj ∈ X . In the pairwise
preference learning problem  we are given pairs of feature vectors xi and xj and corresponding class
labels y ∈ {−1  1} such that y = 1 if the user prefers item i to item j and y = −1 otherwise. The
task of interest is then to predict the class label for a new pair of feature vectors not seen before.
This problem can be addressed by introducing a latent preference function f : X (cid:55)→ R such that
f (xi) > f (xj) whenever the user prefers item i to item j and f (xi) < f (xj) otherwise [4]. When
the evaluations of f are contaminated with Gaussian noise with zero mean and (without loss of
generality) variance 1/2  we obtain the following likelihood for f given xi  xj and y

P(y|xi  xj  f ) = Φ[(f [xi] − f [xj])y]  

(1)
where Φ is the standard Gaussian cumulative distribution function. The preference learning problem
can be solved by combining a GP prior on f with the likelihood function in (1) [4]. The posterior
for f can then be used to make predictions on the user preferences for new pairs of items.
Note that the likelihood (1) depends only on the difference between f (xi) and f (xj). Let g : X 2 (cid:55)→
R be the latent function g(xi  xj) = f (xi) − f (xj). We can recast the inference problem in terms
of g and ignore f. When the evaluation of g is contaminated with standard Gaussian noise  the
likelihood for g given xi  xj and y is

P(y|xi  xj  g) = Φ[g(xi  xj)y] .

(2)
Since g is obtained from f through a linear operation  the GP prior on f induces a GP prior on g. The
covariance function kpref of the GP prior on g can be computed from the covariance function k of the
GP on f as kpref((xi  xj)  (xk  xl)) = k(xi  xk)+k(xj  xl)−k(xi  xl)−k(xj  xk). The derivations
can be found in Section 1 of the supplementary material. We call kpref the preference kernel. The
same kernel function can be derived from a large margin classiﬁcation viewpoint [6]. However  to
our knowledge  the preference kernel has not been used previously for GP-based models.
The combination of (2) with a GP prior based on the preference kernel allows us to transform the
pairwise preference learning problem into binary classiﬁcation with GPs. This means that state-of-
the-art methods for GP binary classiﬁcation  such as expectation propagation [14]  can be applied
directly to preference learning. Furthermore  the simpliﬁed likelihood (2) allows us to implement
complex methods such as the multi-user approach which is described in the following section.

3 Multi-user preference learning
Consider I items with feature vectors xi ∈ X for i = 1  . . .   I. The single-user learning approach
assumes an independent latent function for the u-th user  gu : X 2 (cid:55)→ R. Our approach to the multi-
user problem is to assume common structure in the user latent functions. In particular  we assume a
set of D shared latent functions  hd : X 2 (cid:55)→ R for d = 1  . . .   D  such that the user latent functions
are generated by a linear combination of these functions  namely

gu(xj  xk) =

(3)
here wu d ∈ R is the weight given to function hd for user u. We place a GP prior over the shared
latent functions h1  . . .   hD using the preference kernel described in the previous section. This
model allows the preferences of the different users to share some common structure represented by
the latent functions h1  . . .   hD. This approach is similar to dimensionality reduction methods that
are commonly used for addressing collaborative ﬁltering problems [19  16].

wu dhd(xj  xk)  

d=1

D(cid:88)

2

We may extend this model further to the case in which  for each user u  there is a feature vector
uu ∈ U containing information that might be useful for prediction. We denote by U the set of all
the users’ feature vectors  that is  U = {u1  . . .   uU}. The user features are incorporated now by
placing a separate GP prior over the users weights. In particular  we replace the scalars wu d in (3)
d(uu) : U → R. These weight functions describe the contribution of shared latent
with functions w(cid:48)
function hd to the user latent function gu as a function of the user feature vector uu.
In the multi-user setting we are given a list L = {p1  . . .   pP} with all the pairs of items evaluated
by the users  where P ≤ I(I − 1)/2 (the maximum number of pairs). The data consists of L  the
sets of feature vectors for the users U (if available)  the item features X = {x1  . . .   xI}  and U
sets of preference judgements  one for each user  D = {{zu i  yu i}Mu
u=1  where zu i indexes the
i-th pair evaluated by user u  yi u = 1 if this user prefers the ﬁrst item in the pair to the second and
yi u = −1 otherwise. Mu is the number of preference judgements made by the u-th user.

i=1}U

3.1 Probabilistic description

To address the task of predicting preference on unseen item pairs we cast the model into a probabilis-
tic framework. Let G be an U×P ‘user-function’ matrix  where each row corresponds to a particular
user’s latent function  that is  the entry in the u-th column and i-th row is gu i = gu(xα(i)  xβ(i)) and
α(i) and β(i) denote respectively the ﬁrst and second item in the i-th pair from L. Let H be a D× P
‘shared-function’ matrix  where each row represents the shared latent functions  that is  the entry in
the d-th row and i-th column is hd i = hd(xα(i)  xβ(i)). Finally  we introduce the U × D weight
matrix W such that each row contains a user’s weights  that is  the entry in the u-th row and d-th
column of this matrix is w(cid:48)
d(uu). Note that G = WH represents equation (3) in matrix form. Let
T be the U × P target matrix given by T = sign[G + E]  where E is an U × P noise matrix whose
entries are sampled i.i.d. from a standard Gaussian distribution and the function “sign” retains only
the sign of the elements in a matrix. The observations yu i in D = {{zu i  yu i}Mu
u=1 are mapped
to the corresponding entries of T using tu zu i = yu i. Let T(D) and G(D) represent the elements of
T and G corresponding only to the available observations yu i in D. Then  the likelihood for G(D)
given T(D) and conditional distribution for G(D) given H and W are

i=1}U

P(T(D)|G(D)) =

Φ[tu zu i gu zu i ] and P(G(D)|W  H) =

δ[gu zu i − wuh· zu i ]

U(cid:89)

Mu(cid:89)

u=1

i=1

U(cid:89)

Mu(cid:89)

u=1

i=1

respectively  where wu is the u-th row in W  h· i is the i-th column in H and δ represents a point
probability mass at zero. We now select the priors for W and H. We assume that each function
w(cid:48)
1  . . .   w(cid:48)
D is sampled a priori from a GP with zero mean and speciﬁc covariance function. Let
Kusers be the U × U covariance matrix for entries in each column of matrix W. Then

P(W|U) =

N (w· d|0  Kusers)  

(4)

where w· d is the d-th column in W. If user features are unavailable  Kusers becomes the identity
matrix. Finally  we assume that each shared latent function h1  . . .   hD is sampled a priori from a
GP with zero mean and covariance function given by a preference kernel. Let Kitems be the P × P
preference covariance matrix for the item pairs in L. The prior for H is then

D(cid:89)

d=1

D(cid:89)

j=1

P(H|X L) =

N (hj|0  Kitems)  

(5)

.

(6)

where hj is the j-th row in H. The resulting posterior for W  H and G(D) is

P(W  H  G(D)|T(D)  X L) =

P(T(D)|G(D))P(G(D)|W  H)P(W|U)P(H|X L)

P(T(D|X L)

Given a new item pair pP +1  we can compute the predictive distribution for the preference of the
u-th user (1 ≤ u ≤ U) on this pair by integrating out the parameters H  W and G(D) as follows:

P(tu P +1|T(D)  X L  pP +1) =

P(tu P +1|gu P +1)P(gu P +1|wu  h· P +1)

(cid:90)

P(h· P +1|H  X L  pP +1)P(H  W  G(D)|T(D)  X L) dH dW dG(D)  

(7)
where P(tu P +1|gu P +1) = Φ[tu P +1gu P +1]  P(gu P +1|wu  h· P +1) = δ[gu P +1 − wuh· P +1] 
(8)

P(h· P +1|H  X L  pP +1) =

N (hd P +1|kT

itemshd  k(cid:63) − kT
−1

−1
itemsk(cid:63))

D(cid:89)

(cid:63)K

(cid:63)K

d=1

3

Figure 1: Toy example with 1D input. Circles
and crosses denote labelled data. The plot
shows the mean and variance of the GP pre-
dictive distribution. Maximum Entropy Sam-
pling (MES) samples from the region of high-
est marginal uncertainty  ignoring the second
term in (10). BALD samples from the region
of greatest uncertainty in the latent function.

k(cid:63) is the prior variance of hd(xα(P +1)  xβ(P +1)) and k(cid:63) is a P -dimensional vector that contains
the prior covariances between hd(xα(P +1)  xβ(P +1)) and hd(xα(1)  xβ(1))  . . .   hd(xα(P )  xβ(P )).
Computing (6) or (8) is infeasible and approximations must be used. For this  we use a combination
of expectation propagation (EP) [14] and variation Bayes (VB) [7]. Empirical studies show that EP
obtains state-of-the-art performance in the related problem of GP binary classiﬁcation [15].
We want to learn user preferences with the proposed model from the least amount of data possible.
Therefore we desire to query users actively about their preferences on the most informative pairs of
items [3]. Next  we describe a novel method to implement this strategy. This method exploits the
preference kernel and so may be trivially generalized to GP binary classiﬁcation problems also.

4 Bayesian active learning by disagreement

The goal of active learning is to choose item pairs such that we learn the preference functions for the
users using minimal data. Information theoretic approaches to active learning are popular because
they do not require prior knowledge of loss functions or test domains. The central goal is to iden-
tify the new data point that maximizes the expected reduction in posterior entropy. For preference
learning (see Section 2)  this implies choosing the new item features xi and xj that maximize

where D are the user preferences observed so far and H[p(x)] = −(cid:82) p(x) log p(x) dx represents

H[P(g|D)] − EP(y|xi xj  D) [H[P(g|y  xi  xj D)]]  

(9)

the Shannon entropy. This framework  originally proposed in [10]  is difﬁcult to apply directly to
models based on GPs. In these models  entropies can be poorly deﬁned or their computation can
be intractable.
In practice  current approaches make approximations for the computation of the
posterior entropy [12  9]. However  a second difﬁculty arises; if n new data points are available for
selection  with |{−1  1}| = 2 possible values for y. Then O(2n) potentially expensive posterior
updates are required to ﬁnd the maximizer of (9): one for every available feature vector and possible
class value. This is often too expensive in practice.
A solution consists in noting that (9) is equivalent to the conditioned mutual information between y
and g. Using this we can rearrange this equation to compute entropies in y space:

H[P(y|xi  xj D)] − EP(g|D) [H [P(y|xi  xj  g)]] .

(10)
This overcomes the previous challenges. Entropies are now evaluated in output space  which has low
dimension. Furthermore  g is now conditioned only upon D  so only O(1) updates of the posterior
distribution are required. We only need to recompute the posterior once per data point selected  not
for every possible data point under consideration. Expression (10) also provides us with an intuition
about the objective; we seek the xi and xj for which a) the model is marginally uncertain about
y (high H[P(y|xi  xj D)]) and b) conditioned on a particular value of g the model is conﬁdent
about y (low EP(g|D) [H[P(y|xi  xj  g])]). This can be interpreted as seeking the pair xi and xj
for which the latent functions g  under the posterior  ‘disagree’ with each other the most about the
outcome  that is  the preference judgement. Therefore  we refer to this objective as Bayesian Active
Learning by Disagreement (BALD). This method is independent of the approach used for inference 
something which does not hold for the techniques described in [12  8  9]. In the following section
we show how (10) can be applied to binary classiﬁcation with GPs  and hence via the preference
kernel also to any preference learning problem.

4.1 BALD in binary classiﬁcation with GPs

Most approximate inference methods for the problem of binary classiﬁcation with GPs produce
a Gaussian approximation to the posterior distribution of f  the latent function of interest.
In

4

}}x)dfx

(cid:3) = h(cid:2)Φ(cid:0)µx(σ2

h(cid:2)(cid:82) Φ(fx)N (fx|µx  σ2
x + 1)−1/2(cid:1)(cid:3)  where ≈ represents here the Gaussian
x + C 2)−1/2 exp(cid:0)−µ2
x + C 2(cid:1))−1(cid:1)  where
C =(cid:112)π log 2/2. This result is obtained by using the Gaussian approximation to the posterior of fx

the binary GP classiﬁer  the entropy of y given the corresponding value of f can be expressed
in terms of the binary entropy function  h[f ] = −f log f − (1 − f ) log(1 − f ).
In particular 
H[p(y|x  f )] = h [Φ(f (x)]. When a Gaussian is used to approximate the posterior of f  we have
that for each x  fx = f (x) will follow a Gaussian distribution with mean µx and variance σ2
x. The
ﬁrst term in (10)  that is  H[p(y|x D)]  can be handled analytically in this case: H[p(y|x D)] ≈
approximation to the posterior of fx. The second term in (10)  that is  Ep(f|D) [H[p(y|x  f )]]  can
be approximated as Ep(f|D) [H[p(y|x  f )]] ≈ C(σ2
and then approximating h[Φ(fx)] by the squared exponential curve exp(−f 2
be found in Section 3 of the supplementary material).
To summarize  the BALD algorithm for active binary GP classiﬁcation / preference learning ﬁrst
x of f at
applies any approximate inference method to obtain the posterior mean µx and variance σ2
each point of interest x. Then  it selects the feature vector x that maximizes the objective

x(2(cid:0)σ2

x/π log 2) (details can

−1/2 exp(cid:0)−µ2

x(2(cid:0)σ2

x + C 2(cid:1))

−1(cid:1) .

h

Φ

x + 1)

µx(σ2

x + C 2)

(11)
BALD assigns a high value to the feature vector x when the model is both uncertain about the label
x is large). The second term prevents BALD
(µx close to 0) and there is high uncertainty about fx (σ2
from sampling in regions where the model knows that the label is uncertain. Figure 1 illustrates
the differences between BALD and Maximum Entropy Sampling [17] (details in the supplementary
material  Section 5). MES considers only marginal uncertainty (the ﬁrst term in (11))  and hence
seeks data in an uninformative region of the plot. By contrast  BALD samples data from the region
of greatest uncertainty in the latent function.

−1/2(cid:17)(cid:105) − C(σ2

(cid:104)

(cid:16)

5 Expectation propagation and variational Bayes

Approximate inference in our model is implemented using a combination of expectation propagation
(EP) [13] and variational Bayes (VB) [7]. Here  we brieﬂy describe the method  but full details are
in Section 4 of the supplementary. We approximate the posterior (6) by the parametric distribution

(cid:35)

Q(W  H  G(D)) =

N (wud|mw

u d  vw

u d)

N (hd i|mh

d i  vh

d i)

N (gu zu j|mg

u j  vg

u j)

 

(12)

d i  vh

u d  vw

d i  mg

u d  mh

u j  and vg

namely  P(G(D)  W  H  T(D)  X  (cid:96)) = (cid:81)4

where mw
u j are free parameters to be determined by EP
and the superscripts w  h and g indicate the random variables described by these parameters.
The joint distribution P(G(D)  W  H  T(D)  X  (cid:96)) can be factorized into four factors f1  . . .   f4 
a=1 fa(G(D)  W  H)  where f1(G(D)  W  H) =
P(T(D)|G(D))  f2(G(D)  W  H) = P(G(D)|W  H)  f3(G(D)  W  H) = P(W|U) and
f4(G(D)  W  H) = P(H|X  (cid:96)). EP approximates these exact factors by approximate factors
ˆf1(W  H  G(D))  . . .   ˆf4(W  H  G(D)) that have the same functional form as Q
N (hd i| ˆma h

ˆfa(G(D)  W  H) =

N (wud| ˆma w

P(cid:89)

(cid:35)

u d   ˆva w
u d )

d i   ˆva h
d i )

u j   ˆva g
u j )

N (gu zu j| ˆma g
d i   ˆma g
d i   ˆva h

ˆsa  

(13)

u j  ˆva g

u d   ˆva w

u d   ˆma h

where a = 1  . . .   4 and ˆma w
u j and ˆsa are free parameters. Note that
Q is the normalized product of ˆf1  . . .   ˆf4. The ﬁrst step of EP is to initialize ˆf1  . . .   ˆf4 and Q
to be uniform. After that  EP iteratively reﬁnes of ˆf1  . . .   ˆf4 by minimizing the Kullback-Leibler
(KL) divergence between the product of Q\a and fa and the product of Q\a and ˆfa  where Q\a is
the ratio between Q and ˆfa. However  this does not perform well for reﬁning ˆf2; details on this
problem can be found in Section 4 of the supplementary material and in [19]. For this factor we
follow a VB approach. Instead of minimizing KL(Q\2f2(cid:107)Q\2 ˆf2) with respect to the parameters of
ˆf2  we reﬁne this approximate factor so that the reversed version of the KL divergence is minimized 

5

(cid:34) U(cid:89)
(cid:34) N(cid:89)

u=1

D(cid:89)
Mu(cid:89)

d=1

u=1

j=1

(cid:34) U(cid:89)
(cid:34) N(cid:89)

u=1

D(cid:89)
Mu(cid:89)

d=1

u=1

j=1

(cid:35)(cid:34) D(cid:89)
(cid:35)

d=1

P(cid:89)

i=1

(cid:35)(cid:34) D(cid:89)
(cid:35)

d=1

i=1

that is  we minimize KL(Q\2 ˆf2(cid:107)Q\2f2). EP iteratively reﬁnes all the approximate factors until
convergence. This method also approximates the predictive distribution (7). For this  we replace the
exact posterior in (7) with Q. Finally  EP can also approximate the normalization constant in (6)
(the model evidence) as the integral of the product of all the approximate factors ˆf1  . . .   ˆf4.

5.1 A sparse approximation to speed up computation

The cost of GPs is cubic in the number of function evaluations. In our case  reﬁning ˆf3 has cost
O(DU 3)  where U is the number of users  and D the number of shared latent functions. The cost of
reﬁning ˆf4 is O(DP 3)  where P is the number of observed item pairs. These costs can be reduced
by approximating Kusers and Kitems in (4) and (5). We use the FITC approximation [18]. Under this
approximation  an n×n covariance matrix K generated by the evaluation of a covariance function at
n locations is approximated by K(cid:48) = Q+diag(K−Q)  where Q = Knn0K−1
nn0  Kn0n0 is the
n0×n0 matrix generated by the evaluation of the covariance function at all possible combinations of
only n0 < n locations or pseudo-inputs and Knn0 is the n×n0 matrix with the covariances between
all possible combinations of original locations and pseudo-inputs. These approximations allow us
to reﬁne ˆf3 and ˆf4 in O(DU 2
0 P ) operations  where U0 and P0 are the number of
pseudo-inputs for the users and for the item pairs  respectively. A detailed description of the EP
updates based on the FITC approximation is given in Section 4.4 of the supplementary material.

0 U ) and O(DP 2

KT

n0n0

6 Experiments and Discussion

The performance of our collaborative preference model with the BALD active learning strategy is
evaluated in a series of experiments with simulated and real-world data. The analyzed datasets
include a) synthetic data generated from the probabilistic model assumed by the proposed multi-
user method (Synthetic)  b) a collection of user preferences on different movies (MovieLens)  c) the
number of votes obtained by different political parties in the 2010 UK general election (Election) 
d) preferences of users about different types of sushi (Sushi)  and ﬁnally  e) information regarding
the concentration of heavy metals in the Swiss Jura region (Jura). Section 6 in the supplementary
material contains a detailed description of these datasets.

6.1 Comparison with other multi-user methods

Alternative models. Two versions of the proposed collaborative preference (CP) model are used.
The ﬁrst version (CPU) takes into account the available user features  as described in Section 3.
The second version (CP) ignores these features by replacing Kusers in (4) with the identity matrix.
The ﬁrst multi-user method we compare to is the approach of Birlitiu et al. (BI) [1]. This method
does not use user features  and captures similarities between users with a hierarchical GP model.
In particular  a common GP prior is assumed for the preference function of each user; using this
prior the model learns the full GP posterior for each user. The second multi-user method is the
technique of Bonilla et al.
In this model there exists one high-dimensional function
which depends on both the features of the two items to be compared and on the features of the
user who makes the comparison. Relationships between users’ behaviors are captured only via
the user features. We implement BO and BI using the preference kernel and EP for approximate
inference1. The computational costs of BO and BI are rather high; BO has cubic complexity in
u=1 Mu)3)  our model (CPU) has a signiﬁcantly lower
cost of O(D(U 3 + P 3)) (before further speed-up from FITC). BI does not include user features 
but learns U GPs  so has complexity O(U P 3); the equivalent version of our model (CP) has cost
O(N P + DP 3)  which is lower because D << U. More details about BI and BO are given in
sections 7 and 8 of the supplementary material. Finally  we consider a single user approach (SU)
which ﬁts a different GP classiﬁer independently to the data of each user.

the total number of observations i.e. O(((cid:80)U

(BO) [2].

1Although this is not the same as the original implementations (sampling-based for BI  Laplace approxi-
mation for BO)  the preference kernel and EP are likely to augment the performance of these algorithms  and
provides the fairest comparison of the underlying models.

6

Table 1: Average test error with 100 users.

BI

BO

CPU CP
SU
Dataset
0.162 0.180 0.175 0.157 0.226
Synthetic
0.171 0.163 0.160 0.266 0.187
Sushi
MovieLens 0.182 0.166 0.168 0.302 0.217
0.199 0.123 0.077 0.401 0.300
Election
0.159 0.153 0.153 0.254 0.181
Jura

BI

CP

Table 2: Training times (s) with 100 users.
Dataset
SU
CPU
9.498 22.524 311.574 0.927
Synthetic
7.793
Sushi
4.307 20.028 215.136 0.817
5.694
5.313
MovieLens
69.048 0.604
4.013 19.366
13.134 12.408 20.880 120.011 0.888
Election
Jura
3.762
88.502 0.628

2.404 15.234

BO

Table 3: Test error for each method and active learning strategy with at most 1000 users.

Dataset
Synthetic
Sushi
MovieLens
Election
Jura

CPU-B CPU-E CPU-R
0.135
0.139
0.148
0.178
0.170
0.199
0.224
0.202
0.143
0.168

0.135
0.153
0.176
0.158
0.141

CP-B CP-E CP-R
0.153 0.160 0.173
0.144 0.151 0.176
0.163 0.170 0.195
0.097 0.093 0.151
0.138 0.138 0.169

SU-B SU-E SU-R
0.249 0.259 0.268
0.179 0.197 0.212
0.225 0.235 0.248
0.332 0.346 0.338
0.176 0.166 0.197

Experimental procedure. Due to the high computational cost of BI and BO  to compare to these
methods we must subsample the datasets  keeping only 100 users. The available data were split
randomly into training and test sets of item pairs  where the training sets contain 20 pairs per user in
Sushi  MovieLens and Election  15 pairs in Jura and 30 in Synthetic. This was repeated 25 times to
obtain statistically meaningful results. In CPU and CP  we selected the number of latent functions
D to be 20 (see Table 6.1). In general  the proposed models  CPU and CP  are robust to over-ﬁtting
and over-estimation of D does not harm predictive performance. Note that the Synthetic dataset is
generated using D = 5 and CPU and CP still obtain very good results using D = 20. This automatic
pruning of unnecessary degrees of freedom seems to be common in methods based on variational
Bayes [11]. We selected the kernel lengthscales to be equal to the median distance between feature
vectors. This leads to good empirical performance for most methods. An exception is BO  where the
kernel hyperparameters are tuned to some held-out data using automatic relevance determination. In
our model  we can also estimate the kernel lengthscales by maximizing the EP approximation of the
model evidence  as illustrated in Section 9 of the supplementary material. This alternative approach
can be used when it is necessary to ﬁne tune the lengthscale parameters to the data. In CPU we
use U0 = 25 pseudo inputs for approximating Kusers. These pseudo inputs are selected randomly
from the set of available data points. Similarly  in CP and CPU  we use P0 = 25 pseudo inputs for
approximating Kitems  except in the Jura and Election datasets (which contain fewer items) where
we use P0 = 15. The results obtained are not sensitive to the number of pseudo inputs used  as long
as the number is not excessively low.

Results. Average test errors are shown in Table 1. Those highlighted in bold are statistically
different to those not highlighted (calculated using a paired t test). Overall  CP and CPU outperform
SU and BO  and breaks even with BI; the ﬁnal result is notable as BI learns the full mean and
covariance structure across all users  ours uses only a few latent dimensions  which provides the key
to scaling to many more users. CP outperforms CPU in all cases except in the Synthetic dataset.
In the real-world datasets  users with similar features do not seem to have similar preferences and
so correlating behavior of users with similar features is detrimental. In this case  the unsupervised
learning of similarities in user preferences is more useful for prediction than the user features. This
also explains the poor overall results obtained by BO. Finally  running times in seconds are presented
in Table 2. The entries for BO do not include the time spent by this method to tune the kernel hyper-
parameters. CP and CPU are faster than BO and BI. The FITC approximation imposes a large
multiplicative constant in the cost of CP and CPU so for larger datasets the gains are much larger.

6.2 Active learning on large datasets

Here we evaluate the performance of BALD  in particular  we compare CPU  CP  and SU using
BALD (-B)  Maximum Entropy Sampling (-E) and random sampling (-R). We now use all the avail-
able users from each dataset  with a maximum of 1000 users. For each user the available preference
data are split randomly into training  pool and test sets with 5  35 and 5 data points respectively in

7

Synthetic

Sushi

MovieLens

Election

Jura

Figure 2: Average test error for CPU  CP and SU  using the strategies BALD (-B)  entropy (-E) and
random (-R) for active learning. For clarity  the curves for CPU are included only in the Synthetic
and Election datasets. The complete plots can be found in Section 10 of the supplementary material.

Synthetic  Sushi and MovieLens  3  22 and 3 data points in Election and 3  15 and 3 data points
in Jura. Each method is ﬁtted using the training sets and its performance is then evaluated on the
corresponding test sets. After this  the most informative data point is identiﬁed in each of the pool
sets. These data points are moved into the corresponding training sets and the process repeats until
10 of these active additions to the training sets have been completed. The entire process  includ-
ing the dataset splitting is repeated 25 times. Figure 2 shows the learning curve for each method.
For clarity  the curve for CPU is included only for the Synthetic and Election datasets; in the other
datasets CPU is marginally outperformed by CP (see supplementary material  Section 10). Average
errors after 10 queries from the pool set of each user are summarized in Table 3. For each model
(CPU  CP and SU)  the results of the best active learning strategy are highlighted in bold. The re-
sults of the best model/active learning strategy combination are underlined. Highlighted results are
statistically signiﬁcant with respect to non-highlighted results according to a paired t test. BALD
always outperforms random sampling and usually outperforms or obtains equivalent performance
to MES. In particular  BALD signiﬁcantly outperforms MES in 9 cases  while MES is better than
BALD in only 2 cases.

7 Conclusions

We have proposed a multi-user model that combines collaborative ﬁltering methods with GP binary
preference modeling. We have shown that the task of learning user preferences can be recast as a
particular case of binary classiﬁcation with GPs when a covariance function called the preference
kernel is used. We have also presented BALD  a novel active learning strategy for binary classiﬁca-
tion models with GPs. The proposed multi-user model with BALD performs favorably on simulated
and real-world data against single-user methods and existing approaches for multi-user preference
learning  whilst having signiﬁcantly lower computational times than competing multi-user methods.

Acknowledgements

NH is a recipient of the Google Europe Fellowship in Statistical Machine Learning  and this research
is supported in part by this Google Fellowship. JMH is supported by Infosys Labs  Infosys Limited.

8

02468100.10.150.20.250.30.35num sampleserror02468100.10.150.20.250.30.35num sampleserror02468100.20.250.30.35num sampleserror02468100.050.10.150.20.250.30.350.40.45num sampleserror02468100.10.150.20.250.30.350.4num sampleserrorCPU−BCPU−ECPU−RCP−BCP−ECP−RSU−BSU−ESU−RReferences
[1] A. Birlutiu  P. Groot  and T. Heskes. Multi-task preference learning with an application to

hearing aid personalization. Neurocomputing  73(79):1177 – 1185  2010.

[2] Edwin V. Bonilla  Shengbo Guo  and Scott Sanner. Gaussian process preference elicitation. In

Advances in Neural Information Processing Systems 23  pages 262–270  2010.

[3] E. Brochu  N. de Freitas  and A. Ghosh. Active preference learning with discrete choice data.

Advances in Neural Information Processing Systems 20  20:409–416  2007.

[4] W. Chu and Z. Ghahramani. Preference learning with Gaussian processes. In Proceedings of

the 22nd international conference on Machine learning  pages 137–144  2005.

[5] M. De Gemmis  L. Iaquinta  P. Lops  C. Musto  F. Narducci  and G. Semeraro. Preference
In ECML/PKDD-09 Workshop on Preference Learning 

learning in recommender systems.
2009.

[6] J. F¨urnkranz and E. H¨ullermeier. Preference learning. Springer-Verlag New York Inc  2010.
[7] Z. Ghahramani and M. J. Beal. Advanced Mean Field Method—Theory and Practice  chapter

Graphical models and variational methods  pages 161–177. 2001.

[8] B. Krishnapuram  D. Williams  Y. Xue  A. Hartemink  L. Carin  and M. Figueiredo. On semi-
In Advances in neural information processing systems 17  pages

supervised classiﬁcation.
721–728  2004.

[9] N.D. Lawrence  M. Seeger  and R. Herbrich. Fast sparse gaussian process methods: The
informative vector machine. Advances in Neural Information Processing Systems 15  15:609–
616  2002.

[10] D.V. Lindley. On a measure of the information provided by an experiment. The Annals of

Mathematical Statistics  27(4):986–1005  1956.

[11] D. J. C. MacKay. Local minima  symmetry-breaking  and model pruning in variational free
energy minimization. Available at http://www.inference.phy.cam.ac.uk/mackay/minima.pdf 
2001.

[12] D.J.C. MacKay. Information-based objective functions for active data selection. Neural com-

putation  4(4):590–604  1992.

[13] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Pro-
ceedings of the Eighteenth conference on Uncertainty in artiﬁcial intelligence  pages 352–359 
2002.

[14] Tom Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  MIT 

2001.

[15] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process

classiﬁcation. The Journal of Machine Learning Research  9:2035–2078  2008.

[16] T. Raiko  A. Ilin  and K. Juha. Principal component analysis for large scale problems with lots
of missing values. In Joost Kok  Jacek Koronacki  Raomon Mantaras  Stan Matwin  Dunja
Mladenic  and Andrzej Skowron  editors  Machine Learning: ECML 2007  volume 4701 of
Lecture Notes in Computer Science  pages 691–698. Springer Berlin / Heidelberg  2007.

[17] P. Sebastiani and H.P. Wynn. Maximum entropy sampling and optimal Bayesian experimental
design. Journal of the Royal Statistical Society. Series B (Statistical Methodology)  62(1):145–
157  2000.

[18] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances

in Neural Information Processing Systems 18  2005.

[19] D. H. Stern  R. Herbrich  and T. Graepel. Matchbox: large scale online bayesian recommenda-
tions. In Proceedings of the 18th international conference on World wide web  pages 111–120 
2009.

9

,Karl Moritz Hermann
Tomas Kocisky
Edward Grefenstette
Lasse Espeholt
Will Kay
Mustafa Suleyman
Phil Blunsom