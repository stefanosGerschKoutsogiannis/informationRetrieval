2013,Optimal Neural Population Codes for High-dimensional Stimulus Variables,How does neural population process sensory information? Optimal coding theories assume that neural tuning curves are adapted to the prior distribution of the stimulus variable. Most of the previous work has discussed optimal solutions for only one-dimensional stimulus variables. Here  we expand some of these ideas and present new solutions that define optimal tuning curves for high-dimensional stimulus variables. We consider solutions for a minimal case where the number of neurons in the population is equal to the number of stimulus dimensions (diffeomorphic). In the case of two-dimensional stimulus variables  we analytically derive optimal solutions for different optimal criteria such as minimal L2 reconstruction error or maximal mutual information. For higher dimensional case  the learning rule to improve the population code is provided.,Fisher-Optimal Neural Population Codes for
High-Dimensional Diffeomorphic Stimulus

Representations

Zhuo Wang

Department of Mathematics
University of Pennsylvania

Philadelphia  PA 19104

wangzhuo@sas.upenn.edu

Alan A. Stocker

Department of Psychology
University of Pennsylvania

Philadelphia  PA 19104

astocker@sas.upenn.edu

Department of Electrical and Systems Engineering

Daniel D. Lee

University of Pennsylvania

Philadelphia  PA 19104

ddlee@seas.upenn.edu

Abstract

In many neural systems  information about stimulus variables is often represented
in a distributed manner by means of a population code. It is generally assumed that
the responses of the neural population are tuned to the stimulus statistics  and most
prior work has investigated the optimal tuning characteristics of one or a small
number of stimulus variables. In this work  we investigate the optimal tuning for
diffeomorphic representations of high-dimensional stimuli. We analytically derive
the solution that minimizes the L2 reconstruction loss. We compared our solution
with other well-known criteria such as maximal mutual information. Our solution
suggests that the optimal weights do not necessarily decorrelate the inputs  and the
optimal nonlinearity differs from the conventional equalization solution. Results
illustrating these optimal representations are shown for some input distributions
that may be relevant for understanding the coding of perceptual pathways.

1

Introduction

There has been much work investigating how information about stimulus variables is represented by
a population of neurons in the brain [1]. Studies on motion perception [2  3] and sound localization
[4  5] have demonstrated that these representations adapt to the stimulus statistics on various time
scales [6  7  8  9]. This raises the natural question of what encoding scheme is underlying this
adaptive process?
To address this question  several assumptions about the neural representation and its overall objective
need to be made. In the case of a one-dimensional stimulus  a number of theoretical approaches have
previously been investigated. Some work have focused on the scenario with a single neuron [10  11 
12  13  14  15]  while other work focused on the population level [16  17  18  19  20  21  22  23] 
with different model and noise assumptions. However  the question becomes more difﬁcult when
considering adaptation to high dimensional stimuli. An interesting class of solutions to this question
is related to independent component analysis (ICA) [24  25  26]  which considers maximizing the
amount of information in the encoding given a distribution of stimulus inputs. The use of mutual
information as a metric to measure neural coding quality has also been discussed in [27].

1

In this paper  we study Fisher-optimal population codes for the diffeomorphic encoding of stimuli
with multivariate Gaussian distributions. Using Fisher information  we investigate the properties of
representations that would minimize the L2 reconstruction error assuming an optimal decoder. The
optimization problem is derived under a diffeomorphic assumption  i.e.
the number of encoding
neurons matches the dimensionality of the input and the nonlinearity is monotonic. In this case  the
optimal solution can be found analytically and can be given a geometric interpretation. Qualitative
differences between this solution and the previously studied information maximization solutions are
demonstrated and discussed.

2 Model and Methods

2.1 Encoding and Decoding Model

We consider a n dimensional stimulus input s = (s1  . . .   sn) with prior distribution p(s). In general 
a population with m neurons can have m individual activation functions  h1(s)  . . .   hm(s) which
determines the average ﬁring rate of each neuron in response to the stimulus. However  the encoding
process is affected by neural noise. Two commonly used models are Poisson noise model and
constant Gaussian model  for which the observed ﬁring rate vector r = (r1  . . .   rm) follows the
probabilistic distribution p(r|s)  where
rkT ∼ Poisson(hk(s)T )
rkT ∼ Gaussian(hk(s)T  V T )

(1)
(2)
As opposed to encoding  the decoding process involves constructing an estimator ˆs(r)  which de-
terministically maps the response r to an estimate ˆs of the true stimulus s. We choose a maximum
likelihood estimator ˆsMLE(r) = arg maxs p(r|s) because it simpliﬁes the calculation due to its nice
statistical properties as discussed in section 2.3.

(Poisson noise)
(Gaussian noise)

2.2 Fisher Information Matrix

The Fisher information is a key concept widely used in optimal coding theory. For multiple dimen-
sions  the Fisher information matrix is deﬁned element-wise for each s  as in [28] 

(cid:28) ∂

IF (s)i j =

log p(r|s) ·

∂
∂sj

∂si

log p(r|s)

r

(3)

In the supplementary section A we prove that the Fisher information matrix for a population of m
neurons is

(cid:12)(cid:12)(cid:12)(cid:12) s
(cid:29)

m(cid:88)
m(cid:88)

k=1

k=1

IF (s) = T ·

hk(s)

−1∇hk(s) · ∇hk(s)T

IF (s) = T ·

V

−1∇˜hk(s) · ∇˜hk(s)T

(Poisson noise)

(Gaussian noise)

(4)

(5)

where T is length of the encoding time window and V represents the variance of the constant Gaus-
sian noise. The equivalence for two noise models can be established via the variance stabilizing
transformation ˜hk = 2√hk [29]. Without loss of generality  throughout the paper we assume the
Gaussian noise model for mathematical convenience. Also we will simply assume V = 1  T = 1
because they do not change the optimal solution for any Fisher information-related quantities.

2.3 Cramer-Rao Lower Bound

Ideally  a good neural population code should produce estimates ˆs that are close to the true value of
the stimulus s. However multiple measures exist for how well an estimate matches the true value.
One possibility is the L2 loss which is related to the Fisher information matrix via the Cramer-Rao
lower bound [28]. For any unbiased estimator ˆs  including the MLE 

cov[ˆs − s] ≥ IF (s)

−1

2

(6)

in the sense that cov[ˆs − s] − IF (s)−1 is a positive semideﬁnite matrix. Being only a lower bound 
the Cramer-Rao bound can be attained by the MLE ˆs because it is asymptotically efﬁcient. The local
L2 decoding error (cid:104)(cid:107)ˆs − s(cid:107)2|s(cid:105)r = tr(cov(ˆs − s)) ≥ tr(IF (s)−1). In order to minimize the overall
L2 decoding error  one should minimize the attainable lower bound on the right side of Eq.(7)  under
appropriate constraints on hk(·). (cid:10)

(7)

(cid:107)ˆs − s(cid:107)2(cid:11)

s ≥ (cid:104)tr(IF (s)

−1)(cid:105)s

2.4 Mutual Information Limit

Another possible measurement of neural coding quality is the mutual information. This quantity
does not explicitly rely on an estimator ˆs(r) but directly measures the mutual information between
the response and the stimulus.
The link between mutual information and the Fisher information matrix was established in [16]. One
goal (infomax) is to maximize the mutual information I(r  s) = H(r) − H(r|s). Assuming perfect
integration  the ﬁrst term H(r) asymptotically converges to a constant H(s) for long encoding
time because the noise is Gaussian. The second term H(r|s) = (cid:104)H(r|s∗)(cid:105)s∗ because the noise is
independent. For each s∗  the conditional entropy H(r|s = s∗) ∝ 1
2 log det IF (s∗) since r|s∗ is
asymptotically a Gaussian variable with covariance IF (s∗). Therefore the mutual information is

I(r  s) = const +

1
2(cid:104)log det IF (s)(cid:105)s

(8)

2.5 Diffeomorphic Population

Before one can formalize the optimal coding problem  some assumptions about the neural population
need to be made. Under a diffeomorphic assumption  the number of neurons (m) in the population
matches the dimensionality (n) of the input stimulus. Each neuron projects the signal s onto its basis
wk and passes the one-dimensional projection tk = wT
k s through a sigmoidal tuning curve hk(·)
which is bounded 0 ≤ hk(·) ≤ 1. The tuning curve is
rk = hk(wT

(9)
k=1 si-
We would like to optimize for the nonlinear functions h1(·)  . . .   hn(·) and the basis {wk}n
multaneously. We may assume (cid:107)wk(cid:107) = 1 since the scale can be compensated by the nonlinearity.
Such an encoding scheme is called diffeomorphic because the population establishes a smooth and
invertible mapping from the stimulus space s ∈ S to the rate space r ∈ R. An arbitrary observation
−1
k (rk) and then
of the ﬁring rate r can be ﬁrst inverted to calculate the hidden variables tk = h
linearly decoded to obtain ˆsM LE.
Fig.1a shows how the encoding scheme is implemented by a neural network. Fig.1b illustrates
explicitly how a 2D stimulus s is encoded by two neurons with basis w1  w2 and nonlinear mappings
h1  h2.

k s).

(a)

(b)

Figure 1: (a) Illustration of a neural network with diffeomorphic encoding. (b) The Linear-Nonlinear (LN)
encoding process of 2D stimulus for a stimulus s.

3

s1s2s3s4r1r2r3r4inputstimulusWnonlinearmaphk(·)outputs1s2w1w2swT1sr1h1(wT1s)wT2sr2h2(wT2s)3 Review of One Dimensional Solution

·

=1

≥

(cid:48)
h

(s) ds

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:19)3

(cid:19)
(cid:125)

p(s)1/3 ds

(cid:18)(cid:90)
(cid:124)

(cid:18)(cid:90)
(cid:124)

(cid:18)(cid:90)
(cid:82) s

p(s)
h(cid:48)(s)2 ds
overall L2 loss

In the case of encoding an one-dimensional stimulus  the diffeomorphic population is just one neuron
with sigmoidal tuning curve r = h(w · s). The only two options w = ±1 is determined by whether
the sigmoidal tuning curve is increasing or decreasing. Here we simply assume w = 1.
For the L2-minimization problem  we want to minimize (cid:104)tr(IF (s)−1)(cid:105) = (cid:104)h(cid:48)(s)−2(cid:105) because of
Eq.(5) and (7). Now apply Holder’s inequality [30] to non-negative functions p(s)/h(cid:48)(s)2 and h(cid:48)(s) 

(cid:19)2
(cid:125)
The minimum L2 loss is attained by the optimal h∗(s) ∝
−∞ p(t)1/3dt. For one dimensional
Gaussian with variance Var[s]  the right side of Eq.(10) is 6√3πVar[s]. This preliminary result
will be useful for the high dimensional case discussed in Section 4 and 5.
On the other hand  for the infomax problem we want to maximize I(r  s) because of Eq.(5) and (8).
Note that (cid:104)log det IF (s)(cid:105) = 2(cid:104)log h(cid:48)(s)(cid:105). By treating the sigmoidal activation function h(s) as a
cumulative probability distribution [10]  we have
because the KL-divergence DKL(p||h(cid:48)) =(cid:82) p(s) log p(s) ds −
(cid:82) p(s) log h(cid:48)(s) ds is non-negative.
The optimal solution is h∗(s) = (cid:82) s

−∞ p(t)dt and the optimal value is 2H(p)  where H(p) is the
differential entropy of the distribution p(s). This h∗(s) is exactly obtained by equalizing the output
probability to maximize the entropy. For a one dimensional Gaussian with variance Var[s]  the
optimal value is log Var[s] + const.

(cid:48)
p(s) log h

p(s) log p(s) ds

(s) ds ≤

(cid:90)

(cid:90)

(10)

(11)

4 Optimal Diffeomorphic Population

n(cid:88)

k=1

In the case of encoding high-dimensional random stimulus using a diffeomorphic population code 
n neurons encode n stimulus dimensions. The gradient of the k-th neuron’s tuning curve is ∇k =
h(cid:48)
k(wT

k s)wk and the Fisher information matrix is thus

IF (s) =

∇k∇T

k =

k = W H 2W T

(12)

where W = (w1  . . .   wn) and H = diag(h(cid:48)
n s)). Using the fact that
tr(AB) = tr(BA) for any matrices A  B  we know tr(IF (s)−1) = tr((W T )−1H−2W −1) =
tr((W T W )−1H−2). Because H−2 is diagonal  the L2-min problem is simpliﬁed as
p(s)
h(cid:48)
k(wT

L(W  H) = (cid:104)tr(IF (s)

{wk hk(·)} k=1...n

−1)(cid:105) =

k s)2 ds

n(cid:88)

[(W T W )

minimize

−1]kk

n(wT

1(wT

(cid:90)

(13)

k=1

(cid:48)
k(wT
h

k s)2wkwT
1 s)  . . .   h(cid:48)

n(cid:88)

k=1

(cid:90)

n(cid:88)

k=1

If we deﬁne the marginal distribution

pk(t) =

p(s)δ(t − wT

k s) ds

discussed in section 3  the optimal value ((cid:82) pk(t)1/3 dt)3 is attained when h∗

then the optimization over wk and hk can be decoupled in the following way. For any ﬁxed W  
the integral term can be evaluated by marginalizing out all those directions perpendicular to wk. As
(cid:19)3
(t) ∝ pk(t)1/3. The

optimization problem is now

(cid:18)(cid:90)

k

(cid:48)

minimize
{wk} k=1...n

Lh∗ (W ) =

[(W T W )

−1]kk

pk(t)1/3 dt

In general  analytically optimizing such a term for arbitrary prior distribution p(s) is intractable.
However if p(s) is multivariate Gaussian then the optimization can be further simpliﬁed and solved
analytically  as discussed in the following section.

4

(14)

(15)

5 Stimulus with Gaussian Prior

We consider the case when the stimulus prior is Gaussian N (0  Σ). This assumption allows us to
calculate the marginal distribution along any direction wk as an one-dimensional Gaussian with
mean zero and variance wT
k Σwk = (W T ΣW )kk. By plugging in the Gaussian density pk(t) and
using the fact we derived in Section 3  we can further simplify the L2-optimization problem as

minimize
{wk} k=1...n

Lh∗ (W ) = 6√3π ·

5.1 Geometric Interpretation

n(cid:88)

k=1

[(W T W )

−1]kk(W T ΣW )kk

(16)

In the above optimization problem  (W T ΣW )kk has a clear and simple meaning – it is the variance
of the marginal distribution pk(t). For term [(W T W )−1]kk  notice that W T W is the inner product
matrix of the basis {wk}n
i wj. Using the adjoint method we can calculate
the diagonal elements of (W T W )−1 

k=1  i.e. (W T W )ij = wT

[(W T W )

−1]kk =

det(W T
k Wk)
det(W T W )

(17)

k Wk is the inner product matrix of leave-wk-out basis {w1  . . .   wk−1  wk+1  . . .   wn}.
where W T
Let θk be the angle between wk and the hyperplane spanned by all other basis vectors (see Fig.2).
The diagonal element is just [(W T W )−1]kk = (det Wk/ det W )2 = (sin θk)−2 simply because
(cid:124)

(cid:125)
= Volume ({w1  . . .   wk−1  wk+1  . . .   wn})

·|wk| · sin θk

(cid:123)(cid:122)

(cid:125)
Volume ({w1  . . .   wn})

(cid:123)(cid:122)

n dim parallelogram

n−1 dim base parallelogram

(cid:123)(cid:122)

(cid:125)

 

(18)

(cid:124)

(cid:124)

height

Figure 2: Illustration of θk. In this example  w1
and w2 are on the s1-s2 plane. θ3 is just the angle
between w3 and its projection on the s1-s2 plane.

The optimization involves two competing parts. Minimizing (W T ΣW )kk makes all those direc-
tions with small variance favorable. Meanwhile  minimizing [(W T W )−1]kk = (sin θk)−2 strongly
penalizes neurons having similar tuning directions with the rest of population. To qualitatively sum-
marize  the optimal population would tend to encode those directions with small variance while
keeping certain degree of population diversity.

5.2 General Solution

Due to space limitations  we will only present the optimal solution here and the derivation can be
found in Appendix C in the supplementary notes. For any covariance matrix Σ  the optimal solution
for Eq.(16) is

∗

W

−1/4U  where U T U = I and (U T Σ1/2U )kk =

= Σ

1
n

tr(Σ1/2) for all k = 1  . . .   n (19)

Such unitary matrix U is guaranteed to exist yet may not be unique. See Appendix D for a detailed
discussion. In general for dimension n  the solution has a manifold structure with dimension not
y).
less than (n − 1)(n − 2)/2. For n = 2 the solution can be easily derived. Let Σ = diag(σ2
x  σ2
Then optimal solution is given by

U =

1
√2

  W

∗
L2 = Σ

−1/4U =

1
√2

(20)

This 2D solution is special and is unique under reﬂection and permutation unless the prior distribu-
tion is spherically symmetric i.e. Σ = aI.

5

(cid:18)1 −1
(cid:19)

1

1

(cid:33)

(cid:32) 1√
σx − 1√
1√
1√
σy
σy

σx

s1s2s3θ3w3w1w26 Comparison with Infomax Solution

Previous studies have focused on ﬁnding solutions that maximize the mutual information (infomax)
between the stimulus and the neural population response. This is related to independent component
analysis (ICA) [24]. Mutual information can be maximized if and only if each neuron encodes
an independent component of the stimulus and uses the proper nonlinear tuning curve.
Ideally 
the joint distribution p(s) can be decomposed as the product of n one dimensional components

k=1 pk(Wk(s)). For a Gaussian prior with covariance Σ  the infomax solution is

(cid:81)n

W

∗
info = Σ

−1/2U ⇒ cov(W

−1/2 · Σ · Σ
(21)
where Σ−1/2 is the whitening matrix and U is an arbitrary unitary matrix. The derivation can be
found in Appendix E. In the same 2D example where Σ = diag(σ2
y)  the family of optimal
(cid:32) cos φ
solutions is parametrized by an angular variable φ

∗T
infos) = U T Σ

−1/2U = I

(cid:33)

x  σ2

(cid:18)cos φ − sin φ

(cid:19)

cos φ

info(φ) and W ∗

sin φ
In Fig.3 we compare W ∗
L2 for different prior covariances. One observation is that  L2
optimal neurons do not fully decorrelate input signals unless the Gaussian prior is spherical. By
correlating the input signal and encoding redundant information  the channel signal to noise ratio
(SNR) can be balanced to reduce the vulnerability of those independent channels with low SNR.
As a consequence  the overall L2 performance is improved at the cost of transferring a suboptimal
amount of information. Another important observation is that the infomax solution allows a greater
degree of symmetry – Eq.(21) holds for arbitrary unitary matrices while Eq.(19) holds only for a
subset of them.

σy

U (φ) =

1
√2

  W

∗
info(φ) = Σ

−1/2 · U (φ) =

− sin φ

σx
cos φ

σx
sin φ
σy

(22)

(a)

(b)

(c)

(d)

y
σ
1
=
x
σ

y
σ
2
=
x
σ

y
σ
3
=
x
σ

L2-min

infomax

Figure 3: Comparison of L2-min and infomax optimal solution for 2D case. Each row represents the result
for different ratio σx/σy for the prior distribution. (a) The optimal pair of basis vectors w1  w2 for L2-min
with the prior covariance ellipse is unique unless the prior distribution has rotational symmetry. (b) The loss
function with ”+” marking the optimal solution shown in (a). (c) One pair of optimal basis vector w1  w2 for
infomax with the prior covariance ellipse. (d) The loss function with ”+” marking the optimal solution shown
in (c).

6

s1s2slope=1w1w2w01w02090180090180α(degree)β(degree)s1s2slope=1w1w2w01w02090180090180α(degree)β(degree)s1s2slope=√2w1w2090180090180α(degree)β(degree)s1s2slope=2w1w2w01w02090180090180α(degree)β(degree)s1s2slope=√3w1w2090180090180α(degree)β(degree)s1s2slope=3w1w2w01w02090180090180α(degree)β(degree)7 Application – 16-by-16 Gaussian Images

In this section we apply our diffeomorphic coding scheme to an image representation problem. We
assume that the intensity values of all pixels from a set of 16-by-16 images follow a 256-D Gaussian
distribution. Instead of directly deﬁning the pairwise covariance between pixels of s  we calculate
its real Fourier components ˆs

˜s = F T s ⇔ s = F ˆs

(23)

where the real Fourier matrix is F = (f1  . . .   f256) with each ﬁlter fa and its spatial frequency (cid:126)ka.
The covariance of those Fourier components ˜s is typically assumed to be diagonal and the power
decays following some power law

cov(˜s) = D = diag(σ2

1  . . .   σ2

n)  where σ2

β > 0

(24)

−β 

a ∝ |(cid:126)ka|

Therefore the original stimulus s has covariance cov(s) = Σ = F DF T . Such image statistics are
called stationary because the covariance between pair of pixels is fully determined by their relative
position. For the stimulus s with covariance Σ  one naive choice of L2 optimal ﬁlter is simply

W

∗
L2 = Σ

−1/4 · I = F D

−1/4F T

(25)

because Σ1/2 = F D1/2F T has constant diagonal terms (See Appendix F for detailed calculation)
and U = I qualiﬁes for Eq.(19). The covariance matrix and one sample image generated from Σ is
plotted in Fig. 4(a)-(c) below.

(a)

(b)

(c)

Figure 4: For β = 2.5 in the power law: (a) The 256 × 256 covariance matrix Σ. (b) One column of Σ
reshaped to 16 × 16 matrix representing the covariance between any pixels and a ﬁxed pixel in the center. (c)
A random sample from the Gaussian distribution with covariance Σ.

In addition  we have numerically computed the L2 loss using a family of ﬁlters

Wγ = F D

−γF T  

γ ∈ [0  1/2]

(26)

Note that when γ = 0  we have the naive ﬁlter W0 = F F T = I which does nothing to the input
stimulus; when γ = 1/4 or 1/2  we revisit the L2 optimal ﬁlter or the infomax ﬁlter  respectively. As
we can see from Fig. 5(a)-(d)  the L2 optimal ﬁlter half-decorrelates the input stimulus channels to
keep the balance between the simplicity of the ﬁlters and the simplicity of the correlation structure.
In each simulation run  a set of 10 000 16-by-16 images is randomly sampled from the multivariate
(cid:82) y
Gaussian distribution with zero mean and covariance matrix Σ. For each stimulus image s  we
γ s and zk = hk(yk) + ηk to simulate the encoding process. Here hk(y) ∝
calculate y = W T
γ ΣWγ)kk). The additive Gaussian noise ηk is
−∞ pk(t)1/3dt and pk(t) is Gaussian N (0  (W T
−1
γ )−1 ˆy.
independent Gaussian N (0  10−4). To decode  we just calculate ˆyk = h
k (zk) and ˆs = (W T
2. This procedure is repeated 20 times and the result is plotted
Then we measure the L2 loss (cid:107)ˆs − s(cid:107)2
in Fig. 5(e).

8 Discussion and Conclusions

In this paper  we have studied the an optimal diffeomorphic neural population code which minimizes
the L2 reconstruction error. The population of neurons is assumed to have sigmoidal activation
functions encoding linear combinations of a high dimensional stimulus with a multivariate Gaussian

7

(a)

(b)

(c)

(d)

(e)

Figure 5: (a) The 2D ﬁlter Wγ of one speciﬁc neuron for γ = 0  1/4  1/2 from top to bottom. (b) The
cross-section of the ﬁlter Wγ on one speciﬁc row boxed in (a)  plotted as a function. (c) The correlation of the
2D ﬁltered stimulus  between one speciﬁc neuron and all neurons. (d) The cross-section of the 2D correlation
of the ﬁltered stimulus  between the neuron and other neurons on the same row. (e) The simulation result of L2
loss for different ﬁlter Wγ and optimal nonlinearity h and the vertical bar shows the ±3σ interval across trials.

distribution. The optimal solution is provided and compared with solutions which maximize the
mutual information.
In order to derive the optimal solution  we ﬁrst show that the Poisson noise model is equivalent to
the constant Gaussian noise under the variance stabilizing transformation. Then we relate the L2
reconstruction error to the trace of inverse Fisher information matrix via the Cramer-Rao bound.
Minimizing this bound leads to the global optimal solution in the asymptotic limit of long inte-
gration time. The general L2-minimization problem can be simpliﬁed and the optimal solution is
analytically derived when the stimulus distribution is Gaussian.
Compared to the infomax solutions  a careful evaluation and calculation of the Fisher information
matrix is needed for L2 minimization. The manifold of L2 optimal solutions possess a lower di-
mensional structure compared to the infomax solution. Instead of decorrelating the input statistics 
the L2-min solution maintains a certain degree of correlation across the channels. Our result sug-
gests that maximizing mutual information and minimizing the overall decoding loss are not the same
in general – encoding redundant information can be beneﬁcial to improve reconstruction accuracy.
This principle may explain the existence of correlations at many layers in biological perception
systems.
As an example  we have applied our theory to 16-by-16 images with stationary pixel statistics. The
optimal solution exhibits center-surround receptive ﬁelds  but with a decay differing from those
found by decorrelating solutions. We speculate that these solutions may better explain observed
correlations measured in certain neural areas of the brain. Finally  we acknowledge the support of
the Ofﬁce of Naval Research.

References

[1] K Kang  RM Shapley  and H Sompolinsky. Information tuning of populations of neurons in

primary visual cortex. Journal of neuroscience  24(15):3726–3735  2004.

[2] AP Georgopoulos  AB Schwartz  and RE Kettner. Adaptation of the motion-sensitive neuron

h1 is generated locally and governed by contrast frequency. Science  233:1416–1419  1986.

[3] FE Theunissen and JP Miller. Representation of sensory information in the cricket cercal
sensory system. II. information theoretic calculation of system accuracy and optimal tuning-
curve widths of four primary interneurons. J Neurophysiol  66(5):1690–1703  November 1991.
[4] DC Fitzpatrick  R Batra  TR Stanford  and S Kuwada. A neuronal population code for sound

localization. Nature  388:871–874  1997.

8

naive2D filterL2 optimalinfomax081600.51filter cross−section081600.51081600.512D correlation081600.51correlation cross−section081600.51081600.5101/41/200.511.52x 10−8L2 loss (± 3σ)γ  naiveL2 optimalinfomax[5] NS Harper and D McAlpine. Optimal neural population coding of an auditory spatial cue.

Nature  430:682–686  2004.

[6] N Brenner  W Bialek  and R de Ruyter van Steveninck. Adaptive rescaling maximizes infor-

mation transmission. Neuron  26:695–702  2000.

[7] Tvd Twer and DIA MacLeod. Optimal nonlinear codes for the perception of natural colours.

Network: Computation in Neural Systems  12(3):395–407  2001.

[8] I Dean  NS Harper  and D McAlpine. Neural population coding of sound level adapts to

stimulus statistics. Nature neuroscience  8:1684–1689  2005.

[9] Y Ozuysal and SA Baccus. Linking the computational structure of variance adaptation to

biophysical mechanisms. Neuron  73:1002–1015  2012.

[10] SB Laughlin. A simple coding procedure enhances a neurons information capacity. Z. Natur-

forschung  36c(3):910–912  1981.

[11] J-P Nadal and N Parga. Non linear neurons in the low noise limit: A factorial code maximizes

information transfer  1994.

[12] M Bethge  D Rotermund  and K Pawelzik. Optimal short-term population coding: when Fisher

information fails. Neural Computation  14:2317–2351  2002.

[13] M Bethge  D Rotermund  and K Pawelzik. Optimal neural rate coding leads to bimodal ﬁring

rate distributions. Netw. Comput. Neural Syst.  14:303–319  2003.

[14] MD McDonnell and NG Stocks. Maximally informative stimuli and tuning curves for sig-

moidal rate-coding neurons and populations. Phys. Rev. Lett.  101:058103  2008.

[15] Z Wang  A Stocker  and DD Lee. Optimal neural tuning curves for arbitrary stimulus distribu-
tions: Discrimax  infomax and minimum lp loss. Adv. Neural Information Processing Systems 
25:2177–2185  2012.

[16] N Brunel and J-P Nadal. Mutual information  ﬁsher information and population coding. Neural

Computation  10(7):1731–1757  1998.

[17] K Zhang and TJ Sejnowski. Neuronal tuning: To sharpen or broaden? Neural Computation 

11:75–84  1999.

[18] A Pouget  S Deneve  J-C Ducom  and PE Latham. Narrow versus wide tuning curves: Whats

best for a population code? Neural Computation  11:85–90  1999.

[19] H Sompolinsky and H Yoon. The effect of correlations on the ﬁsher information of population

codes. Advances in Neural Information Processing Systems  11  1999.

[20] AP Nikitin  NG Stocks  RP Morse  and MD McDonnell. Neural population coding is optimized

by discrete tuning curves. Phys. Rev. Lett.  103:138101  2009.

[21] D Ganguli and EP Simoncelli. Implicit encoding of prior probabilities in optimal neural pop-

ulations. Adv. Neural Information Processing Systems  23:658–666  2010.

[22] S Yaeli and R Meir. Error-based analysis of optimal tuning functions explains phenomena

observed in sensory neurons. Front Comput Neurosci  4  2010.

[23] E Doi and MS Lewicki. Characterization of minimum error linear coding with sensory and

neural noise. Neural Computation  23  2011.

[24] AJ Bell and TJ Sejnowski. An information-maximization approach to blind separation and

blind deconvolution. Neural Computation  7:1129–1159  1995.

[25] DJ Field BA Olshausen. Emergence of simple-cell receptive ﬁeld properties by learning a

sparse code for natural images. Nature  381:607–609  1996.

[26] A Hyvarinen and E Oja. Independent component analysis: Algorithms and applications. Neu-

ral Networks  13:411–430  2000.

[27] P Berens  A Ecker  S Gerwinn  AS Tolias  and M Bethge. Reassessing optimal neural pop-
ulation codes with neurometric functions. Proceedings of the National Academy of Sciences 
11:4423–4428  2011.

[28] TM Cover and J Thomas. Elements of Information Theory. Wiley  1991.
[29] EL Lehmann and G Casella. Theory of point estimation. New York: Springer-Verlag.  1999.
[30] GH Hardy  JE Littlewood  and G Polya. Inequalities  2nd ed. Cambridge University Press 

1988.

9

,Zhuo Wang
Alan Stocker
Daniel Lee
Qiming ZHANG
Jing Zhang
Wei Liu
Dacheng Tao