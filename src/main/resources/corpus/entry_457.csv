2018,Adversarially Robust Generalization Requires More Data,Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high "standard" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon  we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model  the sample complexity of robust learning can be significantly larger than that of "standard" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems  at least partially  from this inherently larger sample complexity.,Adversarially Robust Generalization

Requires More Data

Ludwig Schmidt

UC Berkeley

ludwig@berkeley.edu

Shibani Santurkar

Dimitris Tsipras

shibani@mit.edu

tsipras@mit.edu

MIT

MIT

Kunal Talwar
Google Brain

kunal@google.com

Abstract

Aleksander M ˛adry

MIT

madry@mit.edu

Machine learning models are often susceptible to adversarial perturbations of their
inputs. Even small perturbations can cause state-of-the-art classiﬁers with high
“standard” accuracy to produce an incorrect prediction with high conﬁdence. To
better understand this phenomenon  we study adversarially robust learning from the
viewpoint of generalization. We show that already in a simple natural data model 
the sample complexity of robust learning can be signiﬁcantly larger than that of
“standard” learning. This gap is information theoretic and holds irrespective of the
training algorithm or the model family. We complement our theoretical results with
experiments on popular image classiﬁcation datasets and show that a similar gap
exists here as well. We postulate that the difﬁculty of training robust classiﬁers
stems  at least partially  from this inherently larger sample complexity.

1

Introduction

Modern machine learning models achieve high accuracy on a broad range of datasets  yet can easily
be misled by small perturbations of their input. While such perturbations are often simple noise to a
human or even imperceptible  they cause state-of-the-art models to misclassify their input with high
conﬁdence. This phenomenon has ﬁrst been studied in the context of secure machine learning for
spam ﬁlters and malware classiﬁcation [7  16  35]. More recently  researchers have demonstrated
the phenomenon under the name of adversarial examples in image classiﬁcation [21  51]  question
answering [28]  voice recognition [12  13  49  62]  and other domains (for instance  see [2  4  14 
22  25  26  32  60]). Overall  the existence of such adversarial examples raises concerns about the
robustness of current classiﬁers. As we increasingly deploy machine learning systems in safety- and
security-critical environments  it is crucial to understand the robustness properties of our models in
more detail.
A growing body of work is exploring this robustness question from the security perspective by
proposing attacks (methods for crafting adversarial examples) and defenses (methods for making
classiﬁers robust to such perturbations). Often  the focus is on deep neural networks  e.g.  see [11  24 
36  37  41  47  53  59]. While there has been success with robust classiﬁers on simple datasets [31 
36  44  48]  more complicated datasets still exhibit a large gap between “standard” and robust
accuracy [3  11]. An implicit assumption underlying most of this work is that the same training
dataset that enables good standard accuracy also sufﬁces to train a robust model. However  it is
unclear if this assumption is valid.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

So far  the generalization aspects of adversarially robust classiﬁcation have not been thoroughly
investigated. Since adversarial robustness is a learning problem  the statistical perspective is of
integral importance. A key observation is that adversarial examples are not at odds with the standard
notion of generalization as long as they occupy only a small total measure under the data distribution.
So to achieve adversarial robustness  a classiﬁer must generalize in a stronger sense. We currently do
not have a good understanding of how such a stronger notion of generalization compares to standard
“benign” generalization  i.e.  without an adversary.
In this work  we address this gap and explore the statistical foundations of adversarially robust
generalization. We focus on sample complexity as a natural starting point since it underlies the core
question of when it is possible to learn an adversarially robust classiﬁer. Concretely  we pose the
following question:

How does the sample complexity of standard generalization compare to that of
adversarially robust generalization?

Put differently  we ask if a dataset that allows for learning a good classiﬁer also sufﬁces for learning a
robust one. To study this question  we analyze robust generalization in two distributional models. By
focusing on speciﬁc distributions  we can establish information-theoretic lower bounds and describe
the exact sample complexity requirements for generalization. We ﬁnd that even for a simple data
distribution such as a mixture of two class-conditional Gaussians  the sample complexity of robust
generalization is signiﬁcantly larger than that of standard generalization. Our lower bound holds for
any model and learning algorithm. Hence no amount of algorithmic ingenuity is able to overcome
this limitation.
In spite of this negative result  simple datasets such as MNIST have recently seen signiﬁcant progress
in terms of adversarial robustness [31  36  44  48]. The most robust models achieve accuracy around
90% against large (cid:96)∞-perturbations. To better understand this discrepancy with our ﬁrst theoretical
result  we also study a second distributional model with binary features. This binary data model
has the same standard generalization behavior as the previous Gaussian model. Moreover  it also
suffers from a signiﬁcantly increased sample complexity whenever one employs linear classiﬁers
to achieve adversarially robust generalization. Nevertheless  a slightly non-linear classiﬁer that
utilizes thresholding turns out to recover the smaller sample complexity of standard generalization.
Since MNIST is a mostly binary dataset  our result provides evidence that (cid:96)∞-robustness on MNIST
is signiﬁcantly easier than on other datasets. Moreover  our results show that distributions with
similar sample complexity for standard generalization can still exhibit considerably different sample
complexity for robust generalization.
To complement our theoretical results  we conduct a range of experiments on MNIST  CIFAR10 
and SVHN. By subsampling the datasets at various rates  we study the impact of sample size
on adversarial robustness. When plotted as a function of training set size  our results show that
the standard accuracy on SVHN indeed plateaus well before the adversarial accuracy reaches its
maximum. On MNIST  explicitly adding thresholding to the model during training signiﬁcantly
reduces the sample complexity  similar to our upper bound in the binary data model. On CIFAR10 
the situation is more nuanced because there are no known approaches that achieve more than 50%
accuracy even against a mild adversary. But as we show below  there is clear evidence for overﬁtting
in the current state-of-the-art methods.
Overall  our results suggest that current approaches may be unable to attain higher adversarial
accuracy on datasets such as CIFAR10 for a fundamental reason: the dataset may not be large
enough to train a standard convolutional network robustly. Moreover  our lower bounds illustrate
that the existence of adversarial examples should not necessarily be seen as a shortcoming of speciﬁc
classiﬁcation methods. Already in a simple data model  adversarial examples provably occur for
any learning approach  even when the classiﬁer already achieves high standard accuracy. So while
vulnerability to adversarial (cid:96)∞-perturbations might seem counter-intuitive at ﬁrst  in some regimes it
is an unavoidable consequence of working in a statistical setting.

1.1 A motivating example: Overﬁtting on CIFAR10

Before we describe our main results  we brieﬂy highlight the importance of generalization for
adversarial robustness via two experiments on MNIST and CIFAR10. In both cases  our goal is to
learn a classiﬁer that achieves good test accuracy even under (cid:96)∞-bounded perturbations. We follow

2

Figure 1: Classiﬁcation accuracies for robust optimization on MNIST and CIFAR10. In both cases 
we trained standard convolutional networks to be robust to (cid:96)∞-perturbations of the input. On MNIST 
the robust test error closely tracks the corresponding training error and the model achieves high robust
accuracy. On CIFAR10  the model still achieves a good natural (non-adversarial) test error  but there
is a signiﬁcant generalization gap for the robust accuracy. This phenomenon motivates our study of
adversarially robust generalization.

the standard robust optimization approach [6  36  54] – also known as adversarial training [21  51] –
and (approximately) solve the saddle point problem

(cid:21)

loss(θ  x(cid:48))

(cid:20)

min

θ

E
x

max

(cid:107)x(cid:48)−x(cid:107)∞≤ε

via stochastic gradient descent over the model parameters θ. We utilize projected gradient descent
for the inner maximization problem over allowed perturbations of magnitude ε (see [36] for details).
Figure 1 displays the training curves for three quantities: (i) adversarial training error  (ii) adversarial
test error  and (iii) standard test error.
The results show that on MNIST  robust optimization is able to learn a model with around 90%
adversarial accuracy and a relatively small gap between training and test error. However  CIFAR10
offers a different picture. Here  the model (a wide residual network [61]) is still able to fully ﬁt the
training set even against an adversary  but the generalization gap is signiﬁcantly larger. The model
only achieves 47% adversarial test accuracy  which is about 50% lower than its training accuracy.1
Moreover  the standard test accuracy is about 87%  so the failure of generalization indeed primarily
occurs in the context of adversarial robustness. This failure may be surprising particularly since
properly tuned convolutional networks rarely overﬁt much on standard vision datasets.

1.2 Outline of the paper

In the next section  we describe our main theoretical results at a high level. Section 3 complements
these results with experiments. We discuss related works in Section 4 and conclude in Section 5. Due
to space constraints  a longer discussion of related work  several open questions  and all proofs are
deferred to the appendix in the supplementary material.

2 Theoretical Results

Our theoretical results concern statistical aspects of adversarially robust classiﬁcation. In order to
understand how properties of data affect the number of samples needed for robust generalization  we
study two concrete distributional models.
While our two data models are clearly much simpler than the image datasets currently being used in
the experimental work on (cid:96)∞-robustness  we believe that the simplicity of our models is a strength in
this context. The fact that we can establish a separation between standard and robust generalization
already in our Gaussian data model is evidence that the existence of adversarial examples for neural

1We remark that this accuracy is still currently the best published robust accuracy on CIFAR10 [3]. For

instance  contemporary approaches to architecture tuning do not yield better robust accuracies [15].

3

0200004000060000Training Steps020406080100AccuracyMNISTAdversarial trainAdversarial testStandard test020000400006000080000Training Steps020406080100CIFAR10Adversarial trainAdversarial testStandard testnetworks should not come as a surprise. The same phenomenon (i.e.  classiﬁers with just enough
samples for high standard accuracy necessarily being vulnerable to (cid:96)∞- attacks) already occurs in
much simpler settings such as a mixture of two Gaussians. Note that more complicated distributional
setups that can “simulate” the Gaussian model directly inherit our lower bounds.
In addition  conclusions from our simple models also transfer to real datasets. As we describe
in the subsection on the Bernoulli model  the beneﬁts of the thresholding layer predicted by our
theoretical analysis do indeed appear in experiments on MNIST as well. Since multiple defenses
against adversarial examples have been primarily evaluated on MNIST [31  44  48]  it is important to
note that (cid:96)∞-robustness on MNIST is a particularly easy case: adding a simple thresholding layer
directly yields nearly state-of-the-art robustness against moderately strong adversaries (ε = 0.1) 
without any further changes to the model architecture or training algorithm.

2.1 The Gaussian model

Our ﬁrst data model is a mixture of two spherical Gaussians with one component per class.
Deﬁnition 1 (Gaussian model). Let θ(cid:63) ∈ Rd be the per-class mean vector and let σ > 0 be the
variance parameter. Then the (θ(cid:63)  σ)-Gaussian model is deﬁned by the following distribution over
(x  y) ∈ Rd × {±1}: First  draw a label y ∈ {±1} uniformly at random. Then sample the data point
x ∈ Rd from N (y · θ(cid:63)  σ2I).

√

While not explicitly speciﬁed in the deﬁnition  we will use the Gaussian model in the regime where
the norm of the vector θ(cid:63) is approximately
d. Hence the main free parameter for controlling the
difﬁculty of the classiﬁcation task is the variance σ2  which controls the amount of overlap between
the two classes.
To contrast the notions of “standard” and “robust” generalization  we brieﬂy recap a standard deﬁnition
of classiﬁcation error.
Deﬁnition 2 (Classiﬁcation error). Let P : Rd×{±1} → R be a distribution. Then the classiﬁcation
error β of a classiﬁer f : Rd → {±1} is deﬁned as β = P(x y)∼P [f (x) (cid:54)= y].
Next  we deﬁne our main quantity of interest  which is an adversarially robust counterpart of the
above classiﬁcation error. Instead of counting misclassiﬁcations under the data distribution  we allow
a bounded worst-case perturbation before passing the perturbed sample to the classiﬁer.
Deﬁnition 3 (Robust classiﬁcation error). Let P : Rd × {±1} → R be a distribution and let
B : Rd → P(Rd) be a perturbation set.2 Then the B-robust classiﬁcation error β of a classiﬁer
f : Rd → {±1} is deﬁned as β = P(x y)∼P [∃ x(cid:48) ∈ B(x) : f (x(cid:48)) (cid:54)= y].
Since (cid:96)∞-perturbations have recently received a signiﬁcant amount of attention  we focus on ro-
bustness to (cid:96)∞-bounded adversaries in our work. For this purpose  we deﬁne the perturbation set
Bε∞(x) = {x(cid:48) ∈ Rd |(cid:107)x(cid:48) − x(cid:107)∞ ≤ ε}. To simplify notation  we refer to robustness with respect to
this set also as (cid:96)ε∞-robustness. As we remark in the discussion section  understanding generalization
for other measures of robustness ((cid:96)2  rotations  etc.) is an important direction for future work.

Standard generalization. The Gaussian model has one parameter for controlling the difﬁculty of
learning a good classiﬁer. In order to simplify the following bounds  we study a regime where it is
possible to achieve good standard classiﬁcation error from a single sample.3 As we will see later 
this also allows us to calibrate our two data models to have comparable standard sample complexity.
Concretely  we prove the following theorem  which is a direct consequence of Gaussian concentration.
Note that in this theorem we use a linear classiﬁer: for a vector w  the linear classiﬁer fw : Rd →
{±1} is deﬁned as fw(x) = sgn((cid:104)w  x(cid:105)).
where c is a universal constant. Let (cid:98)w ∈ Rd be the vector (cid:98)w = y · x. Then with high probability  the
Theorem 4. Let (x  y) be drawn from a (θ(cid:63)  σ)-Gaussian model with (cid:107)θ(cid:63)(cid:107)2 =
linear classiﬁer f(cid:98)w has classiﬁcation error at most 1%.
2We write P(Rd) to denote the power set of Rd  i.e.  the set of subsets of Rd.
3We remark that it is also possible to study a more general setting where standard generalization requires a

d and σ ≤ c · d1/4

√

larger number of samples.

4

To minimize the number of parameters in our bounds  we have set the error probability to 1%.
By tuning the model parameters appropriately  it is possible to achieve a vanishingly small error
probability from a single sample (see Corollary 19 in Appendix D.1).

Robust generalization. As we just demonstrated  we can easily achieve standard generalization
from only a single sample in our Gaussian model. We now show that achieving a low (cid:96)∞-robust
classiﬁcation error requires signiﬁcantly more samples. To this end  we begin with a natural strength-
ening of Theorem 4 and prove that the (class-weighted) sample mean can also be a robust classiﬁer
(given sufﬁcient data).
d and σ ≤ c1d1/4. Let (cid:98)w ∈ Rd be the weighted mean vector (cid:98)w = 1
Theorem 5. Let (x1  y1)  . . .   (xn  yn) be drawn i.i.d. from a (θ(cid:63)  σ)-Gaussian model with (cid:107)θ(cid:63)(cid:107)2 =
√
i=1 yixi. Then with high
probability  the linear classiﬁer f(cid:98)w has (cid:96)ε∞-robust classiﬁcation error at most 1% if

(cid:80)n

n

(cid:26)1

n ≥

√

d

c2 ε2

for ε ≤ 1
for

1

4 d−1/4

4 d−1/4 ≤ ε ≤ 1

4

.

We refer the reader to Corollary 22 in Appendix D.1 for the details. As before  c1 and c2 are two
universal constants. Overall  the theorem shows that it is possible to learn an (cid:96)ε∞-robust classiﬁer
in the Gaussian model as long as ε is bounded by a small constant and we have a large number of
samples.
Next  we show that this signiﬁcantly increased sample complexity is necessary. Our main theorem
establishes a lower bound for all learning algorithms  which we formalize as functions from data
samples to binary classiﬁers. In particular  the lower bound applies not only to learning linear
classiﬁers.
Theorem 6. Let gn be any learning algorithm  i.e.  a function from n samples to a binary classiﬁer
fn. Moreover  let σ = c1 · d1/4  let ε ≥ 0  and let θ ∈ Rd be drawn from N (0  I). We also draw n
samples from the (θ  σ)-Gaussian model. Then the expected (cid:96)ε∞-robust classiﬁcation error of fn is at
least (1 − 1/d) 1

2 if

√

n ≤ c2

ε2
d
log d

.

The proof of the theorem can be found in Corollary 23 (Appendix D.2). It is worth noting that the
classiﬁcation error 1/2 in the lower bound is tight. A classiﬁer that always outputs a ﬁxed prediction
trivially achieves perfect robustness on one of the two classes and hence robust accuracy 1/2.
Comparing Theorems 5 and 6  we see that the sample complexity n required for robust generalization
is bounded as

√

cε2
d
log d

≤ n ≤ c(cid:48)ε2

d .

√

√

Hence the lower bound is nearly tight in our regime of interest. When the perturbation has constant
(cid:96)∞-norm  the sample complexity of robust generalization is larger than that of standard generalization
d  i.e.  polynomial in the problem dimension. This shows that for high-dimensional problems 
by
adversarial robustness can provably require a signiﬁcantly larger number of samples.
Finally  we remark that our lower bound applies also to a more restricted adversary. Our proof uses
only a single adversarial perturbation per class. As a result  the lower bound provides transferable ad-
versarial examples and applies to worst-case distribution shifts without a classiﬁer-adaptive adversary.
We refer the reader to Section 5 for a more detailed discussion.

2.2 The Bernoulli model

As mentioned in the introduction  simpler datasets such as MNIST have recently seen signiﬁcant
progress in terms of (cid:96)∞-robustness. We now investigate a possible mechanism underlying these
advances. To this end  we study a second distributional model that highlights how the data distribution
can signiﬁcantly affect the achievable robustness. The second data model is deﬁned on the hypercube
{±1}d  and the two classes are represented by opposite vertices of that hypercube. When sampling a
datapoint for a given class  we ﬂip each bit of the corresponding class vertex with a certain probability.
This data model is inspired by the MNIST dataset because MNIST images are close to binary (many
pixels are almost fully black or white).

5

Deﬁnition 7 (Bernoulli model). Let θ(cid:63) ∈ {±1}d be the per-class mean vector and let τ > 0 be the
class bias parameter. Then the (θ(cid:63)  τ )-Bernoulli model is deﬁned by the following distribution over
(x  y) ∈ {±1}d × {±1}: First  draw a label y ∈ {±1} uniformly at random from its domain. Then
sample the data point x ∈ {±1}d by sampling each coordinate xi from the distribution

(cid:26) y · θ(cid:63)

−y · θ(cid:63)

xi =

i with probability 1/2 + τ
i with probability 1/2 − τ

.

As in the previous subsection  the model has one parameter for controlling the difﬁculty of learning.
A small value of τ makes the samples less correlated with their respective class vectors and hence
leads to a harder classiﬁcation problem. Note that both the Gaussian and the Bernoulli model are
deﬁned by simple sub-Gaussian distributions. Nevertheless  we will see that they differ signiﬁcantly
in terms of robust sample complexity.

Standard generalization. As in the Gaussian model  we ﬁrst calibrate the distribution so that we
can learn a classiﬁer with good standard accuracy from a single sample.4 The following theorem is a
direct consequence of the fact that bounded random variables exhibit sub-Gaussian concentration.
universal constant. Let (cid:98)w ∈ Rd be the vector (cid:98)w = y · x. Then with high probability  the linear
Theorem 8. Let (x  y) be drawn from a (θ(cid:63)  τ )-Bernoulli model with τ ≥ c · d−1/4 where c is a
classiﬁer f(cid:98)w has classiﬁcation error at most 1%.

To simplify the bound  we have set the error probability to be 1% as in the Gaussian model. We refer
the reader to Corollary 28 in Appendix F.1 for the proof.

Robust generalization. Next  we investigate the sample complexity of robust generalization in
our Bernoulli model. For linear classiﬁers  a small robust classiﬁcation error again requires a large
number of samples:
Theorem 9. Let gn be a linear classiﬁer learning algorithm  i.e.  a function from n samples to a
linear classiﬁer fn. Suppose that we choose θ(cid:63) uniformly at random from {±1}d and draw n samples
from the (θ(cid:63)  τ )-Bernoulli model with τ = c1 · d−1/4. Moreover  let ε < 3τ and 0 < γ < 1/2. Then
the expected (cid:96)ε∞-robust classiﬁcation error of fn is at least 1

2 − γ if

n ≤ c2

ε2γ2d
log d/γ

.

We defer the proof to Appendix F.2. At ﬁrst  the lower bound for linear classiﬁers might suggest that
(cid:96)∞-robustness requires an inherently larger sample complexity here as well. However  in contrast
to the Gaussian model  non-linear classiﬁers can achieve a signiﬁcantly improved robustness. In
particular  consider the following thresholding operation T : Rd → Rd which is deﬁned element-wise
as

(cid:26)+1

−1

T (x)i =

if xi ≥ 0
otherwise .

It is easy to see that for ε < 1  the thresholding operator undoes the action of any (cid:96)∞-bounded adver-
sary  i.e.  we have T (Bε∞(x)) = {x} for any x ∈ {±1}d. Hence we can combine the thresholding
operator with the classiﬁer learned from a single sample to get the following upper bound.
universal constant. Let (cid:98)w ∈ Rd be the vector (cid:98)w = yx. Then with high probability  the classiﬁer
Theorem 10. Let (x  y) be drawn from a (θ(cid:63)  τ )-Bernoulli model with τ ≥ c · d−1/4 where c is a
f(cid:98)w ◦ T has (cid:96)ε∞-robust classiﬁcation error at most 1% for any ε < 1.

This theorem shows a stark contrast to the Gaussian case. Although both models have similar sample
complexity for standard generalization  there is a
d gap between the (cid:96)∞-robust sample complexity
for the Bernoulli and Gaussian models. This discrepancy provides evidence that robust generalization
requires a more nuanced understanding of the data distribution than standard generalization.

√

4To be precise  the two distributions have comparable sample complexity for standard generalization in the

regime where σ ≈ τ−1.

6

Figure 2: Adversarially robust generalization performance as a function of training data size for
(cid:96)∞ adversaries on the MNIST  CIFAR-10 and SVHN datasets. For each choice of training set size
and εtest  we plot the best performance achieved over εtrain and network capacity. This clearly
shows that achieving a certain level of adversarially robust generalization requires signiﬁcantly more
samples than achieving the same level of standard generalization.

In isolation  the thresholding step might seem speciﬁc to the Bernoulli model studied here. However 
our experiments in Section 3 show that an explicit thresholding layer also signiﬁcantly improves the
sample complexity of training a robust neural network on MNIST. We conjecture that the effectiveness
of thresholding is behind many of the successful defenses against adversarial examples on MNIST
(for instance  see Appendix C in [36]).

3 Experiments

We complement our theoretical results by performing experiments on multiple common datasets. We
consider standard convolutional neural networks and train models on datasets of varying complexity.
Speciﬁcally  we study the MNIST [34]  CIFAR-10 [33]  and SVHN [40] datasets. We use a simple
convolutional architecture for MNIST  a standard ResNet model [23] for CIFAR-10  and a wider
ResNet [61] for SVHN. We perform robust optimization to train our classiﬁers on perturbations
generated by projected gradient descent. Appendix G provides additional details for our experiments.

Empirical sample complexity evaluation. We study how the generalization performance of adver-
sarially robust networks varies with the size of the training dataset. To do so  we train networks with
a speciﬁc (cid:96)∞ adversary while reducing the size of the training set. The training subsets are produced
by randomly sub-sampling the complete dataset in a class-balanced fashion. When increasing the
number of samples  we ensure that each dataset is a superset of the previous one.
We evaluate the robustness of each trained network to perturbations of varying magnitude (εtest). For
each choice of training set size N and ﬁxed attack εtest  we select the best performance achieved
across all hyperparameters settings (training perturbations εtrain and model size). On all three
datasets  we observed that the best standard accuracy is usually achieved for the standard trained
network  while the best adversarial accuracy for almost all values of εtest was achieved when training
with the largest εtrain. We maximize over the hyperparameter settings since we are not interested in
the performance of a speciﬁc model  but rather in the inherent generalization properties of the dataset
independently of the classiﬁer used. Figure 2 shows the results of these experiments.
The plots demonstrate the need for more data to achieve adversarially robust generalization. For any
ﬁxed test set accuracy  the number of samples needed is signiﬁcantly higher for robust generalization.
In the SVHN experiments (where we have sufﬁcient training samples to observe plateauing behavior) 
the standard accuracy reaches its maximum with signiﬁcantly fewer samples than the adversarial
accuracy. We report more details of our experiments in Section H of the supplementary material.

Thresholding experiments. Motivated by our theoretical study of the Bernoulli model  we investi-
gate whether thresholding can also improve the sample complexity of robust generalization against
an (cid:96)∞ adversary on MNIST.

7

103104Training Set Size020406080100Test Accuracy (%)MNISTtest=0test=0.1test=0.2test=0.3103104Training Set Size020406080100Test Accuracy (%)CIFAR-10test=0test=2test=4test=8103104105Training Set Size020406080100Test Accuracy (%)SVHNtest=0test=1test=2test=4Figure 3: Adversarial robustness to (cid:96)∞ attacks on the MNIST dataset for a simple convolution
network [36] with and without explicit thresholding ﬁlters. For each training set size choice and
εtest  we report the best test set accuracy achieved over choice of thresholding ﬁlters and εtrain. We
observe that introducing thresholding ﬁlters signiﬁcantly reduces the number of samples needed to
achieve good adversarial generalization.

We repeat the above sample complexity experiments with networks where thresholding ﬁlters are
explicitly encoded in the model. Here  we replace the ﬁrst convolutional layer with a ﬁxed thresholding
layer consisting of two channels  ReLU(x− εf ilter) and ReLU(1− x− εf ilter)  where x is the input
image. Figure 3 shows results for networks trained with such a thresholding layer. For standard
trained networks  we use a value of εf ilter = 0.1 for the thresholding ﬁlters  whereas for adversarially
trained networks we set εf ilter = εtrain. For each data subset size and test perturbation εtest  we plot
the best test accuracy achieved over networks trained with different thresholding ﬁlters  i.e.  different
values of ε. We separately show the effect of explicit thresholding in such networks when they are
trained adversarially using PGD.
As predicted by our theory  the networks achieve good adversarially robust generalization with
signiﬁcantly fewer samples when thresholding ﬁlters are added. Further  note that adding a simple
thresholding layer directly yields nearly state-of-the-art robustness against moderately strong adver-
saries (ε = 0.1)  without any other modiﬁcations to the model architecture or training algorithm. It
is also worth noting that the thresholding ﬁlters could have been learned by the original network
architecture  and that this modiﬁcation only decreases the capacity of the model. Our ﬁndings
emphasize network architecture as a crucial factor for learning adversarially robust networks from a
limited number of samples.
We also experimented with thresholding ﬁlters on the CIFAR10 dataset  but did not observe any
signiﬁcant difference from the standard architecture. This agrees with our theoretical understanding
that thresholding helps primarily in the case of (approximately) binary datasets.

4 Related Work

Due to the large body of work on adversarial robustness  we focus on related papers that also provide
theoretical explanations for adversarial examples. We defer a detailed discussion of related work to
Appedix A and discuss here the works most closely related to ours.
Wang et al. [55] study the adversarial robustness of nearest neighbor classiﬁers. In contrast to our
work  the authors give theoretical guarantees for a speciﬁc classiﬁcation algorithm  and do not
see a separation in sample complexity between robust and regular generalization. Recent work by
Gilmer et al. [20] explores a speciﬁc distribution where robust learning is empirically difﬁcult with
overparametrized neural networks. The main phenomenon is that even a small natural error rate
on their dataset translates to a large adversarial error rate. Our results give a more nuanced picture
that involves the sample complexity required for generalization. In our data models  it is possible to
achieve an error rate that is essentially zero by using a very small number of samples  whereas the
adversarial error rate is still large unless we have seen a lot of samples.
The work of Xu et al. [58] establishes a connection between robust optimization and regularization
for linear classiﬁcation. In particular  they show that robustness to a speciﬁc perturbation set is exactly
equivalent to the standard support vector machine. Subsequent work by Xu and Mannor [57] builds
a deeper connection between robustness and generalization. They prove that for a certain notion
of robustness  robust algorithms generalize. Moreover  they show that robustness is a necessary

8

103104Training Set Size020406080100Test Accuracy (%)Standard Training103104Training Set Size020406080100Test Accuracy (%)Standard Training + Thresholding103104Training Set Size020406080100Test Accuracy (%)Adversarial Training103104Training Set Size020406080100Test Accuracy (%)Adversarial Training + Thresholdingtest=0test=0.1test=0.2test=0.3condition for generalization in an asymptotic sense. Bellet and Habrard [5] gives similar results
for metric learning. However  these results do no imply sample complexity bounds since they are
asymptotic. Our results stand in stark contrast: we show that generalization can  in simple models  be
signiﬁcantly easier than robustness when sample complexity enters the picture.
Fawzi et al. [18] relate the robustness of linear and non-linear classiﬁers to adversarial and
(semi-) random perturbations. Their work studies the setting where the classiﬁer is ﬁxed and does not
encompass the learning task. Fawzi et al. [19] give provable lower bounds for adversarial robustness
in models where robust classiﬁers do not exist. In contrast  we are interested in a setting where robust
classiﬁers exist  but need many samples to learn. Papernot et al. [43] discuss adversarial robustness at
the population level. We defer a more detailed discussion of these works to Appendix A.
There is also a long line of work in machine learning on exploring the connection between various no-
tions of margin and generalization  e.g.  see [46] and references therein. In this setting  the (cid:96)p margin 
i.e.  how robustly classiﬁable the data is for (cid:96)∗
p-bounded classiﬁers  enables dimension-independent
control of the sample complexity. However  the sample complexity in concrete distributional models
can often be signiﬁcantly smaller than what the margin implies.

5 Discussion and Conclusions

The vulnerability of neural networks to adversarial perturbations has recently been a source of much
discussion and is still poorly understood. Different works have argued that this vulnerability stems
from their discontinuous nature [51]  their linear nature [21]  or is a result of high-dimensional
geometry and independent of the model class [20]. Our work gives a more nuanced picture. We show
that for a natural data distribution (the Gaussian model)  the model class we train does not matter and
a standard linear classiﬁer achieves optimal robustness. However  robustness also strongly depends on
properties of the underlying data distribution. For other data models (such as MNIST or the Bernoulli
model)  our results demonstrate that non-linearities are indispensable to learn from few samples. This
dichotomy provides evidence that defenses against adversarial examples need to be tailored to the
speciﬁc dataset (even for the same type of perturbations) and hence may be more complicated than a
single  broad approach. Understanding the interactions between robustness  classiﬁer model  and
data distribution from the perspective of generalization is an important direction for future work. We
refer the reader to Section B in the appendix for concrete questions in this direction.
The focus of our paper is on adversarial perturbations in a setting where the test distribution (before
the adversary’s action) is the same as the training distribution. While this is a natural scenario from a
security point of view  other setups can be more relevant in different robustness contexts. For instance 
we may want a classiﬁer that is robust to small changes between the training and test distribution.
This can be formalized as the classiﬁcation accuracy on unperturbed examples coming from an
adversarially modiﬁed distribution. Here  the power of the adversary is limited by how much the
test distribution can be modiﬁed  and the adversary is not allowed to perturb individual samples
coming from the modiﬁed test distribution. Interestingly  our lower bound for the Gaussian model
also applies to such worst-case distributional shifts. In particular  if the adversary is allowed to shift
the mean θ(cid:63) by a vector in Bε∞  our proof sketched in Section C transfers to the distribution shift
setting. Since the lower bound relies only on a single universal perturbation  this perturbation can
also be applied directly to the mean vector.
What do our results mean for robust classiﬁcation of real images? Our Gaussian lower bound implies
that if an algorithm works for all (or most) settings of the unknown parameter θ(cid:63)  then achieving
strong (cid:96)∞-robustness requires a sample complexity increase that is polynomial in the dimension.
There are a few different ways this lower bound could be bypassed. It is conceivable that the noise
scale σ is signiﬁcantly smaller for real image datasets  making robust classiﬁcation easier. Even if that
was not the case  a good algorithm could work for the parameters θ(cid:63) that correspond to real datasets
while not working for most other parameters. To accomplish this  the algorithm would implicitly
or explicitly have prior information about the correct θ(cid:63). While some prior information is already
incorporated in the model architectures (e.g.  convolutional and pooling layers)  the conventional
wisdom usually is not to bias the neural network with our priors. Our work suggests that there are
trade-offs with robustness here and that adding more prior information could help to learn more
robust classiﬁers.

9

Acknowledgements

During this research project  Ludwig Schmidt was supported by a Google PhD fellowship and
a Microsoft Research fellowship at the Simons Institute for the Theory of Computing. Ludwig
was also an intern in the Google Brain team. Shibani Santurkar is supported by the National
Science Foundation (NSF) under grants IIS-1447786  IIS-1607189  and CCF-1563880  and the Intel
Corporation. Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF
Frontier grant CNS-1413920. Aleksander M ˛adry was supported in part by an Alfred P. Sloan Research
Fellowship  a Google Research Award  and the NSF grants CCF-1553428 and CNS-1815221.

References
[1] Tensor ﬂow models repository. https://www.tensorflow.org/tutorials/layers  2017.

[2] Anurag Arnab  Ondrej Miksik  and Philip H. S. Torr. On the robustness of semantic segmentation
models to adversarial attacks. In Conference on Computer Vision and Pattern Recognition
(CVPR)  2018. URL http://arxiv.org/abs/1711.09856.

[3] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning (ICML)  2018. URL https://arxiv.org/abs/1802.00420.

[4] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy
induction attacks. In International Conference on Machine Learning and Data Mining (MLDM) 
2017. URL https://arxiv.org/abs/1701.04143.

[5] Aurélien Bellet and Amaury Habrard. Robustness and generalization for metric learning.

Neurocomputing  2015. URL https://arxiv.org/abs/1209.1086.

[6] Aharon Ben-Tal  Laurent El Ghaoui  and Arkadi Nemirovski. Robust optimization. Princeton

University Press  2009.

[7] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine

learning. Pattern Recognition  2018. URL https://arxiv.org/abs/1712.03141.

[8] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration Inequalities: A

Nonasymptotic Theory of Independence. Oxford University Press  2013.

[9] Nader H. Bshouty  Nadav Eiron  and Eyal Kushilevitz. PAC learning with nasty noise. In
Algorithmic Learning Theory (ALT)  1999. URL https://link.springer.com/chapter/
10.1007/3-540-46769-6_17.

[10] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples.

arXiv  2016.

[11] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy (SP)  2016. URL http://arxiv.org/abs/1608.04644.
[12] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-
to-text. In Security and Privacy Workshops (SPW)  2018. URL https://arxiv.org/abs/
1801.01944.

[13] Nicholas Carlini  Pratyush Mishra  Tavish Vaidya  Yuankai Zhang  Micah Sherr  Clay
Shields  David Wagner  and Wenchao Zhou. Hidden voice commands. In USENIX Security
Symposium  2016. URL https://www.usenix.org/conference/usenixsecurity16/
technical-sessions/presentation/carlini.

[14] Moustapha M Cisse  Yossi Adi  Natalia Neverova  and Joseph Keshet. Houdini: Fooling
deep structured visual and speech recognition models with adversarial examples. In Neural
Information Processing Systems (NIPS)  2017. URL https://arxiv.org/abs/1707.05373.
Intriguing
properties of adversarial examples. arXiv  2017. URL https://arxiv.org/abs/1711.
02846.

[15] Ekin D. Cubuk Cubuk  Barret Zoph  Samuel S. Schoenholz  and Quoc V. Le.

10

[16] Nilesh Dalvi  Pedro Domingos  Mausam  Sumit Sanghai  and Deepak Verma. Adversarial
classiﬁcation. In International Conference on Knowledge Discovery and Data Mining (KDD) 
2004. URL http://doi.acm.org/10.1145/1014052.1014066.

[17] Logan Engstrom  Brandon Tran  Dimitris Tsipras  Ludwig Schmidt  and Aleksander Madry.
A rotation and a translation sufﬁce: Fooling CNNs with simple transformations. arXiv  2017.
URL https://arxiv.org/abs/1712.02779.

[18] Alhussein Fawzi  Seyed-Mohsen Moosavi-Dezfooli  and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In Neural Information Processing Systems (NIPS) 
2016. URL https://arxiv.org/abs/1608.08967.

[19] Alhussein Fawzi  Hamza Fawzi  and Omar Fawzi. Adversarial vulnerability for any classiﬁer.

arXiv  2018. URL https://arxiv.org/abs/1802.08686.

[20] Justin Gilmer  Luke Metz  Fartash Faghri  Samuel S. Schoenholz  Maithra Raghu  Martin
Wattenberg  and Ian Goodfellow. Adversarial spheres. arXiv  2018. URL https://arxiv.
org/abs/1801.02774.

[21] Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adver-

sarial examples. arXiv  2014. URL http://arxiv.org/abs/1412.6572.

[22] Kathrin Grosse  Nicolas Papernot  Praveen Manoharan  Michael Backes  and Patrick D.
McDaniel. Adversarial perturbations against deep neural networks for malware classiﬁca-
tion. In European Symposium on Research in Computer Security (ESORICS)  2016. URL
http://arxiv.org/abs/1606.04435.

[23] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Conference on Computer Vision and Pattern Recognition (CVPR)  2016. URL
https://arxiv.org/abs/1512.03385.

[24] Warren He  James Wei  Xinyun Chen  Nicholas Carlini  and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive
Technologies  2017. URL https://arxiv.org/abs/1706.04701.

[25] Alex Huang  Abdullah Al-Dujaili  Erik Hemberg  and Una-May O’Reilly. Adversarial deep
learning for robust detection of binary encoded malware. In Security and Privacy Workshops
(SPW)  2018. URL https://arxiv.org/abs/1801.02950.

[26] Sandy H. Huang  Nicolas Papernot  Ian J. Goodfellow  Yan Duan  and Pieter Abbeel. Adversarial
attacks on neural network policies. In International Conference on Learning Representations
(ICLR)  2017. URL https://arxiv.org/abs/1702.02284.

[27] Peter J. Huber. Robust Statistics. Wiley  1981.

[28] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
In Conference on Empirical Methods in Natural Language Processing (EMNLP)  2017. URL
https://arxiv.org/abs/1707.07328.

[29] Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on

Computing  1993. URL http://dx.doi.org/10.1137/0222052.

[30] Michael J. Kearns  Robert E. Schapire  and Linda M. Sellie. Toward efﬁcient agnostic learning.

Machine Learning  1994. URL https://doi.org/10.1023/A:1022615600103.

[31] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In International Conference on Learning Representations (ICLR) 
2018. URL https://arxiv.org/abs/1711.00851.

[32] Jernej Kos  Ian Fischer  and Dawn Song. Adversarial examples for generative models. In
Security and Privacy Workshops (SPW)  2018. URL http://arxiv.org/abs/1702.06832.

[33] Alex Krizhevsky and Geoffrey Hinton.

Learning multiple layers of features from
Technical report  2009. URL https://www.cs.toronto.edu/~kriz/

tiny images.
learning-features-2009-TR.pdf.

11

[34] Yann LeCun  Corinna Cortes  and Christopher J.C. Burges. The mnist database of handwritten

digits. Website  1998. URL http://yann.lecun.com/exdb/mnist/.

[35] Daniel Lowd and Christopher Meek. Adversarial learning. In International Conference on
Knowledge Discovery in Data Mining (KDD)  2005. URL http://doi.acm.org/10.1145/
1081870.1081950.

[36] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR)  2018. URL https://arxiv.org/abs/1706.06083.

[37] Seyed-Mohsen Moosavi-Dezfooli  Alhussein Fawzi  and Pascal Frossard. Deepfool: A simple
and accurate method to fool deep neural networks. In Conference on Computer Vision and
Pattern Recognition (CVPR)  2016. URL https://arxiv.org/abs/1511.04599.

[38] Seyed-Mohsen Moosavi-Dezfooli  Alhussein Fawzi  Omar Fawzi  and Pascal Frossard. Uni-
versal adversarial perturbations. In Conference on Computer Vision and Pattern Recognition
(CVPR)  2017. URL https://arxiv.org/abs/1610.08401.

[39] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations
In Conference on Computer Vision and Pattern Recognition (CVPR)

for deep networks.
Workshops  2017. URL http://arxiv.org/abs/1612.06299.

[40] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y. Ng.
Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on
Deep Learning and Unsupervised Feature Learning  2011. URL http://ufldl.stanford.
edu/housenumbers/.

[41] Nicolas Papernot  Patrick McDaniel  Xi Wu  Somesh Jha  and Ananthram Swami. Distillation as
a defense to adversarial perturbations against deep neural networks. In Symposium on Security
and Privacy (SP)  2016. URL https://arxiv.org/abs/1511.04508.

[42] Nicolas Papernot  Patrick D. McDaniel  Somesh Jha  Matt Fredrikson  Z. Berkay Celik  and
Ananthram Swami. The limitations of deep learning in adversarial settings. In European
Symposium on Security and Privacy (EuroS&P)  2016. URL https://arxiv.org/abs/
1511.07528.

[43] Nicolas Papernot  Patrick McDaniel  Arunesh Sinha  and Michael Wellman. Towards the
science of security and privacy in machine learning. In European Symposium on Security and
Privacy (EuroS&P)  2018. URL https://arxiv.org/abs/1611.03814.

[44] Aditi Raghunathan  Jacob Steinhardt  and Percy Liang. Certiﬁed defenses against adversarial
In International Conference on Learning Representations (ICLR)  2018. URL

examples.
https://arxiv.org/abs/1801.09344.

[45] Phillippe Rigollet and Jan-Christian Hütter. High-dimensional statistics. Lecture notes  2017.

URL http://www-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf.

[46] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press  2014. URL http://www.cs.huji.ac.il/~shais/
UnderstandingMachineLearning/.

[47] Mahmood Sharif  Sruti Bhagavatula  Lujo Bauer  and Michael K. Reiter. Accessorize to a
crime: Real and stealthy attacks on state-of-the-art face recognition. In Conference on Computer
and Communications Security (CCS)  2016. URL http://doi.acm.org/10.1145/2976749.
2978392.

[48] Aman Sinha  Hongseok Namkoong  and John Duchi. Certifying some distributional robustness
with principled adversarial training. In International Conference on Learning Representations
(ICLR)  2018. URL https://arxiv.org/abs/1710.10571.

[49] Liwei Song and Prateek Mittal. Inaudible voice commands. In Conference on Computer and

Communications Security (CCS)  2017. URL http://arxiv.org/abs/1708.07238.

12

[50] Jiawei Su  Danilo Vasconcellos Vargas  and Kouichi Sakurai. One pixel attack for fooling deep

neural networks. arXiv  2017. URL http://arxiv.org/abs/1710.08864.

[51] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian J.
In International
Goodfellow  and Rob Fergus.
Conference on Learning Representations (ICLR)  2014. URL http://arxiv.org/abs/1312.
6199.

Intriguing properties of neural networks.

[52] Florian Tramèr  Nicolas Papernot  Ian J. Goodfellow  Dan Boneh  and Patrick D. McDaniel.
The space of transferable adversarial examples. arXiv  2017. URL http://arxiv.org/abs/
1704.03453.

[53] Florian Tramèr  Alexey Kurakin  Nicolas Papernot  Dan Boneh  and Patrick D. McDaniel.
Ensemble adversarial training: Attacks and defenses. In International Conference on Learning
Representations (ICLR)  2018. URL http://arxiv.org/abs/1705.07204.

[54] Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of

Mathematics  1945.

[55] Yizhen Wang  Somesh Jha  and Kamalika Chaudhuri. Analyzing the robustness of nearest
neighbors to adversarial examples. In International Conference on Machine Learning (ICML) 
2018. URL http://proceedings.mlr.press/v80/wang18c/wang18c.pdf.

[56] Chaowei Xiao  Jun-Yan Zhu  Bo Li  Warren He  Mingyan Liu  and Dawn Song. Spatially
transformed adversarial examples. In International Conference on Learning Representations
(ICLR)  2018. URL https://arxiv.org/abs/1801.02612.

[57] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning  2012. URL

https://arxiv.org/abs/1005.2243.

[58] Huan Xu  Constantine Caramanis  and Shie Mannor. Robustness and regularization of support
vector machines. Journal of Machine Learning Research (JMLR)  2009. URL http://www.
jmlr.org/papers/v10/xu09b.html.

[59] Weilin Xu  David Evans  and Yanjun Qi. Feature squeezing: Detecting adversarial examples in
deep neural networks. In Network and Distributed System Security Symposium (NDSS)  2017.
URL https://arxiv.org/abs/1704.01155.

[60] Xiaojun Xu  Xinyun Chen  Chang Liu  Anna Rohrbach  Trevor Darell  and Dawn Song. Can
you fool AI with adversarial examples on a visual Turing test? arXiv  2017. URL http:
//arxiv.org/abs/1709.08693.

[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision

Conference (BMVC)  2016. URL http://arxiv.org/abs/1605.07146.

[62] Guoming Zhang  Chen Yan  Xiaoyu Ji  Taimin Zhang  Tianchen Zhang  and Wenyuan Xu.
Dolphinatack: Inaudible voice commands. In Conference on Computer and Communications
Security (CCS)  2017. URL http://arxiv.org/abs/1708.09537.

13

,Sang-Woo Lee
Jin-Hwa Kim
Jaehyun Jun
Jung-Woo Ha
Byoung-Tak Zhang
Ludwig Schmidt
Shibani Santurkar
Dimitris Tsipras
Kunal Talwar
Aleksander Madry
Christopher Beckham
Sina Honari
Vikas Verma
Alex Lamb
Farnoosh Ghadiri
R Devon Hjelm
Yoshua Bengio
Chris Pal