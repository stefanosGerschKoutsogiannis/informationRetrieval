2011,Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation,In this paper we describe a maximum likelihood likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefficients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefficients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood  i.e.  the likelihood of the dictionary where the activation coefficients have been integrated out (given a specific prior). We compare the output of both maximum joint likelihood estimation (i.e.  standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection  akin to automatic relevance determination.,Nonnegative dictionary learning in the exponential
noise model for adaptive music signal representation

Onur Dikmen

CNRS LTCI; T´el´ecom ParisTech

75014  Paris  France

C´edric F´evotte

CNRS LTCI; T´el´ecom ParisTech

75014  Paris  France

dikmen@telecom-paristech.fr

fevotte@telecom-paristech.fr

Abstract

In this paper we describe a maximum likelihood approach for dictionary learning
in the multiplicative exponential noise model. This model is prevalent in audio
signal processing where it underlies a generative composite model of the power
spectrogram. Maximum joint likelihood estimation of the dictionary and expan-
sion coefﬁcients leads to a nonnegative matrix factorization problem where the
Itakura-Saito divergence is used. The optimality of this approach is in question be-
cause the number of parameters (which include the expansion coefﬁcients) grows
with the number of observations. In this paper we describe a variational procedure
for optimization of the marginal likelihood  i.e.  the likelihood of the dictionary
where the activation coefﬁcients have been integrated out (given a speciﬁc prior).
We compare the output of both maximum joint likelihood estimation (i.e.  stan-
dard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE)
on real and synthetical datasets. The MMLE approach is shown to embed auto-
matic model order selection  akin to automatic relevance determination.

1 Introduction

In this paper we address the task of nonnegative dictionary learning described by

V ≈ W H 

(1)
where V   W   H are nonnegative matrices of dimensions F × N   F × K and K × N   respectively. V
is the data matrix  where each column vn is a data point  W is the dictionary matrix  with columns
{wk} acting as “patterns” or “explanatory variables” reprensentative of the data  and H is the acti-
vation matrix  with columns {hn}. For example  in this paper we will be interested in music data
such that V is time-frequency spectrogram matrix and W is a collection of spectral signatures of la-
tent elementary audio components. The most common approach to nonnegative dictionary learning
is nonnegative matrix factorization (NMF) [1] which consists in retrieving the factorization (1) by
solving

min
W H

D(V |W H)

d(vf n|[W H]f n)

s.t. W  H ≥ 0  

(2)

where d(x|y) is a measure of ﬁt between nonnegative scalars  vf n are the entries of V   and A ≥ 0
expresses nonnegativity of the entries of matrix A. The cost function D(V |W H) is often a likeli-
hood function − log p(V |W  H) in disguise  e.g.  the Euclidean distance underlies additive Gaussian
noise  the Kullback-Leibler (KL) divergence underlies Poissonian noise  while the Itakura-Saito (IS)
divergence underlies multiplicative exponential noise [2]. The latter noise model will be central to
this work because it underlies a suitable generative model of the power spectrogram  as shown in [3]
and later recalled.

def

= Xf n

1

A criticism about NMF is that little can be said about the asymptotical optimality of the learnt
dictionary W . Indeed  because W is estimated jointly with H  the total number of parameters F K +
KN grows with the number of data points N . As such  this paper instead addresses optimization of
the likelihood in the marginal model described by

p(V |W ) =ZH

p(V |W  H)p(H)dH 

(3)

where H is treated as a random latent variable with prior p(H). The evaluation and optimization of
the marginal likelihood is not trivial in general  and this paper is precisely devoted to these tasks in
the multiplicative exponential noise model.

The maximum marginal likelihood estimation approach we seek here is related to IS-NMF in such
a way that Latent Dirichlet Allocation (LDA) [4] is related to Latent Semantic Indexing (pLSI)
[5]. LDA and pLSI are two estimators in the same model  but LDA seeks estimation of the topic
distributions in the marginal model  from which the topic weights describing each document have
been integrated out. In contrast  pLSI (which is essentially equivalent to KL-NMF as shown in [6])
performs maximum joint likelihood estimation (MJLE) for the topics and weights. Blei et al. [4]
show the better performance of LDA with respect to (w.r.t) pLSI. Welling et al. [7] also report similar
results with a discussion  stating that deterministic latent variable models assign zero probability to
input conﬁgurations that do not appear in the training set. A similar approach is Discrete Component
Analysis (DCA) [8] which considers maximum marginal a posteriori estimation in the Gamma-
Poisson (GaP) model [9]  see also [10] for the maximum marginal likelihood estimation on the same
model. In this paper  we will follow the same objective for the multiplicative exponential noise
model.

We will describe a variational algorithm for the evaluation and optimization of (3); note that the
algorithm exploits speciﬁcities of the model and is not a mere adaptation of LDA or DCA to an
alternative setting. We will consider a nonnegative Generalized inverse-Gaussian (GIG) distribution
as a prior for H  a ﬂexible distribution which takes the Gamma and inverse-Gamma as special
cases. As will be detailed later  this work relates to recent work by Hoffman et al. [11]  which
considers full Bayesian integration of W and H (both assumed random) in the exponential noise
model  in a nonparametric setting allowing for model order selection. We will show that our more
simple maximum likelihood approach inherently performs model selection as well by automatically
pruning “irrelevant” dictionary elements. Applied to a short well structured piano sequence  our
approach is shown to capture the correct number of components  corresponding to the expected note
spectra  and outperforms the nonparametric Bayesian approach of [11].

The paper is organized as follows. Section 2 introduces the multiplicative exponential noise model
with the prior distribution for the expansion coefﬁcients p(H). Sections 3 and 4 describe the MJLE
and MMLE approaches  respectively. Section 5 reports results on synthetical and real audio data.
Section 6 concludes.

2 Model

The generative model assumed in this paper is

vf n = ˆvf n . ǫf n  

(4)

where ˆvf n = Pk wf khkn and ǫf n is a nonnegative multiplicative noise with exponential distri-
bution ǫf n ∼ exp(−ǫf n). In other words  and under independence assumptions  the likelihood
function is

p(V |W  H) = Yf n

(1/ˆvf n) exp(−vf n/ˆvf n) .

(5)

When V is a power spectrogram matrix such that vf n = |xf n|2 and {xf n} are the complex-valued
short-time Fourier transform (STFT) coefﬁcients of some signal data  where f typically acts as a
frequency index and n acts as a time-frame index  it was shown in [3] that an equivalent generative
model of vf n is

xf n = Xk

cf kn 

cf kn ∼ Nc(0  wf khkn)  

(6)

2

where Nc refers to the circular complex Gaussian distribution.1 In other words  the exponential
multiplicative noise model underlies a generative composite model of the STFT. The complex-
valued matrix {cf kn}f n  referred to as kthcomponent  is characterized by a spectral signature wk 
amplitude-modulated in time by the frame-dependent coefﬁcient hkn  which accounts for nonsta-
tionarity. In analogy with LDA or DCA  if our data consisted of word counts  with f indexing words
and n indexing documents  then the columns of W would describe topics and cf kn would denote
the number of occurrences of word f stemming from topic k in document n.
In our setting W is considered a free deterministic parameter to be estimated by maximum likeli-
hood. In contrast  H is treated as a nonnegative random latent variable over which we will integrate.
It is assigned a GIG prior  such that

(7)

(8)

with

hkn ∼ GIG(αk  βk  γk)  

GIG(x|α  β  γ) =

(β/γ)α/2

2Kα(2√βγ)

xα−1 exp−(cid:16)βx +

γ

x(cid:17)  

where K is a modiﬁed Bessel function of the second kind and x  β and γ are nonnegative scalars.
The GIG distribution uniﬁes the Gamma (α > 0  γ = 0) and inverse-Gamma (α < 0  β = 0)
distributions. Its sufﬁcient statistics are x  1/x and log x  and in particular we have

hxi = Kα+1(2√βγ)

Kα(2√βγ) r γ

β

 

x(cid:29) = Kα−1(2√βγ)
Kα(2√βγ) s β
(cid:28) 1

γ

 

(9)

where hxi denotes expectation. Although all derivations and the implementations are done for the
general case  in practice we will only consider the special case of Gamma distribution for simplicity.
In such case  β parameter merely acts as a scale parameter  which we ﬁx so as to solve the scale
ambiguity between the columns of W and the rows of H. We will also assume the shape parameters
{αk} ﬁxed to arbitrary values (typically  αk = 1  which corresponds to the exponential distribution).
Given the generative model speciﬁed by equations (4) and (7) we now describe two estimators for
W .

3 Maximum joint likelihood estimation

3.1 Estimator

The joint (penalized) log-likelihood likelihood of W and H is deﬁned by

CJL(W  H)

def
= log p(V |W  H) + log p(H)
= −DIS(V |W H) − Xkn

(1 − αk) log hkn + βkhkn + γk/hkn + cst  

(10)

(11)

where DIS(V |W H) is deﬁned as in equation (2) with dIS(x|y) = x/y − log(x/y) − 1 (Itakura-
Saito divergence) and “cst” denotes terms constant w.r.t W and H. The subscript JL stands for joint
likelihood  and the estimation of W by maximization of CJL(W  H) will be referred to as maximum
joint likelihood estimation (MJLE).

3.2 MM algorithm for MJLE

We describe an iterative algorithm which sequentially updates W given H and H given W . Each of
the two steps can be achieved in a minorization-maximization (MM) setting [12]  where the original
problem is replaced by the iterative optimization of an easier-to-optimize auxiliary function. We ﬁrst
describe the update of H  from which the update of W will be easily deduced. Given W   our task

consists in maximizing C(H) = −DIS(V |W H) − L(H)  where L(H) =Pkn(1 − αk) log hkn +
βkhkn + γk/hkn. Using Jensen’s inequality to majorize the convex part of DIS(V |W H) (terms in
1A complex random variable has distribution Nc(µ  λ) if and only if its real and imaginary parts are inde-

pendent and distributed as N (ℜ(µ)  λ/2) and N (ℑ(µ)  λ/2)  respectively.

3

vf n/ˆvf n) and ﬁrst order Taylor approximation to majorize its concave part (terms in log ˆvf n)  as in
[13]  the functional

G(H  ˜H) = −(cid:16)Xk
knPf wf kvf n/˜v2

pkn/hkn + qknhkn(cid:17) − L(H) + cst 

(12)

f n  qkn = Pf wf k/˜vf n  ˜vf n = [W ˜H]f n  can be shown to be
where pkn = ˜h2
a tight lower bound of C(H)  i.e.  G(H  ˜H) ≤ C(H) and G( ˜H  ˜H) = C( ˜H). Its iterative max-
imization w.r.t H  where ˜H = H (i) acts as the current iterate at iteration i  produces an ascent
algorithm  such that C(H (i+1)) ≥ C(H (i)). The update is easily shown to amount to solving an
order 2 polynomial with a single positive root given by

hkn =

(αk − 1) +p(αk − 1)2 + 4(pkn + γk)(qkn + βk)

2(qkn + βk)

.

(13)

The update preserves nonnegativity given positive initialization. By exchangeability of W and H
when the data is transposed (V T = H T W T )  and dropping the penalty term (αk = 1  βk = 0 
γk = 0)  the update of W is given by the multiplicative update

which is known from [13].

wf k = ˜wf ksPn hknvf n/˜v2
Pn hkn/˜vf n

f n

 

4 Maximum marginal likelihood estimation

4.1 Estimator

We deﬁne the marginal log-likelihood objective function as

(14)

(15)

CML(W )

def

= logZ p(V |W  H)p(H) dH .

The subscript ML stands for marginal likelihood  and the estimation of W by maximization of
CML(W ) will be referred to as maximum marginal likelihood estimation (MMLE). Note that in
Bayesian estimation the term marginal likelihood is sometimes used as a synonym for the model
evidence  which is the likelihood of data given the model  i.e.  where all random parameters (in-
cluding W ) have been marginalized. This is not the case here where W is treated as a deterministic
parameter and marginal likelihood only refers to the likelihood of W   where H has been integrated
out. The integral in equation (15) is intractable given our model. In the next section we resort to a
variational Bayes procedure for the evaluation and maximization of CML(W ).

4.2 Variational algorithm for MMLE

In the following we propose an iterative lower bound evaluation/maximization procedure for
approximate maximization of CML(W ). We will construct a bound B(W  ˜W ) such that
∀(W  ˜W )  CML(W ) ≥ B(W  ˜W )  where ˜W acts as the current iterate and W acts as the free pa-
rameter over which the bound is maximized. The maximization is approximate in that the bound
will only satisfy B( ˜W   ˜W ) ≈ CML( ˜W )  i.e.  is loosely tight in the current update ˜W   which fails to
ensure ascent of the objective function like in the MM setting of Section 3.2.
We propose to construct the bound from a variational Bayes perspective [14]. The following in-
equality holds for any distribution function q(H)

CML(W ) ≥ hlog p(V |W  H)iq + hlog p(H)iq − hlog q(H)iq

(16)
The inequality becomes an equality when q(H) = p(H|V  W ); when the latter is available in close
form  the EM algorithm consists in using ˜q(H) = p(H|V  ˜W ) and maximize Bvb
˜q (W ) w.r.t W  
and iterate. The true posterior of H being intractable in our case  we take q(H) to be a factorized 

q (W ) .

def
= Bvb

4

parametric distribution qθ(H)  whose parameter θ is updated so as to tighten Bvb
Like in [11]  we choose qθ(H) to be in the same family as the prior  such that

q ( ˜W ) to C( ˜W ).

qθ(H) = Ykn GIG(¯αkn  ¯βkn  ¯γkn) .

(17)

The ﬁrst term of Bvb
q (W ) essentially involves the expectation of −DIS(V |W H) w.r.t to the vari-
ational distribution qθ(H). The product W H introduces some coupling of the coefﬁcients of H
(via the sum Pk wf khkn) which makes the integration difﬁcult. Following [11] and similar to
Section 3.2  we propose to lower bound this term using Jensen’s and Taylor’s type inequalities to
majorize the convex and concave parts of −DIS(V |W H). The contributions of the elements of H
become decoupled w.r.t to k  which allows for evaluation and maximization of the bound. This leads
to

1

φ2
f kn

vf n

wf k(cid:28) 1

hkn(cid:29)q! + log ψf n +

hlog p(V |H  W )iq ≥ −Xf n Xk
where {ψf n} and {φf kn} are nonnegative free parameters such that Pk φf kn = 1. We deﬁne

Bθ φ ψ(W ) as Bvb
q (W ) but where the expectation of the joint log-likelihood is replaced by its lower
bound given right side of equation (18). From there  our algorithm is a two-step procedure consisting
in 1) computing ˜θ  ˜φ  ˜ψ so as to tighten Bθ φ ψ( ˜W ) to CML( ˜W )  and 2) maximizing B ˜θ  ˜φ  ˜ψ(W )
w.r.t W . The corresponding updates are given next. Note that evaluation of the bound only involves
expectations of hkn and 1/hkn w.r.t to the GIG distribution  which is readily given by equation (9).

wf khhkniq − 1!  

ψf n Xk

(18)

Step 1: Tightening the bound Given current dictionary update ˜W   run the following ﬁxed-point
equations.

φf kn =

¯αkn = αk 

 

˜wf k/h1/hkniq
Pj ˜wf j /h1/hjniq
¯βkn = βk +Xf

˜wf k
ψf n

 

ψf n =Xj
˜wf jhhjniq
¯γkn = γk +Xf

vf nφ2
˜wf k

f kn

.

Step 2: Optimizing the bound Given the variational distribution ˜q = q ˜θ from previous step 
update W as

wf k = ˜wf kvuuuut

˜q i−2
Pn vf nhPj ˜wf jh1/hjni−1
PnhPj ˜wf jhhjni˜qi−1

˜q

h1/hkni−1
hhkni˜q

.

(19)

The VB update has a similar form to the MM update of equation (14) but the contributions of H are
replaced by expected values w.r.t the variational distribution.

4.3 Relation to other works

A variational algorithm using the activation matrix H and the latent components C = {cf kn} as
hidden data can easily be devised  as sketched in [2]. Including C in the variational distribution also
allows to decouple the contributions of the activation coefﬁcients w.r.t to k but leads from our expe-
rience to a looser bound  a ﬁnding also reported in [11]. In a fully Bayesian setting  Hoffman et al.

[11] assume Gamma priors for both W and H. The model is such that ˆvf n =Pk λkwf khkn  where
λk acts as a component weight parameter. The number of components is potentially inﬁnite but 
in a nonparametric setting  the prior for λk favors a ﬁnite number of active components. Posterior
inference of the parameters W   H  {λk} is achieved in a variational setting similar to Section 4.2 
by maximizing a lower bound on p(V ). In contrast to this method  our approach does not require to
specify a prior for W   leads to simple updates for W that are directly comparable to IS-NMF and
experiments will reveal that our approach embeds model order selection as well  by automatically
pruning unnecessary columns of W   without resorting to the nonparametric framework.

5

x 105

−1.25

MMLE

−1.3

−1.35

L
M

C

−1.4

−1.45

−1.5

−1.55

5

10

15

K

20

25

(a) CML by MMLE

L
J

C

−3

−4

−5

−6

−7

−8

x 104

MJLE

x 105

−1.4

MJLE

L
M

C

−1.45

−1.5

−1.55

−1.6

−1.65

5

10

15

K

20

25

(c) CML by MJLE

5

10

15

K

20

25

(b) CJL by MJLE

Figure 1: Marginal likelihood CML (a) and joint likelihood CJL (b) versus number of components
K. CML values corresponding to dictionaries estimated by CJL maximization (c).

5 Experiments

In this section  we study the performances of MJLE and MMLE methods on both synthetical and
real-world datasets.2 The prior hyperparameters are ﬁxed to αk = 1  γk = 0 (exponential distri-
bution) and βk = 1  i.e.  hkn ∼ exp(−hkn). We used 5000 algorithm iterations and nonnegative
random initializations in all cases. In order to minimize the odds of getting stuck in local optima  we
adapted the deterministic annealing method proposed in [15] for MMLE. Deterministic annealing
is applied by multiplying the entropy term −hlog q(H)i in the lower bound in (16) by 1/η(i). The
initial η(0) is chosen in (0  1) and increased through iterations. In our experiments  we set η(0) = 0.6
and updated it with the rule η(i+1) = min(1  1.005η(i)).

5.1 Swimmer dataset

First  we consider the synthetical Swimmer dataset [16]  for which the ground truth of the dictionary
is available. The dataset is composed of 256 images of size 32 × 32  representing a swimmer built
of an invariant torso and 4 limbs. Each of the 4 limbs can be in one of 4 positions and the dataset
is formed of all combinations. Hence  the ground truth dictionary corresponds to the collection of
individual limb positions. As explained in [16] the torso is an unidentiﬁable component that can
be paired with any of the limbs  or even split among the limbs. In our experiments  we mapped the
values in the dataset onto the range [1  100] and multiplied with exponential noise  see some samples
in Fig. 2 (a).

We ran the MM and VB algorithms (for MJLE and MMLE  respectively) for K = 1 . . . 20 and the
joint and marginal log-likelihood end values (after the 5000 iterations) are displayed in Fig. 1. The
marginal log-likelihood is here approximated by its lower bound  as described in Section 4.2. In
Fig. 1(a) and (b) the respective objective criteria (CML and CJL) maximized by MMLE and MJLE
are shown. The increase of CML stops after K = 16  whereas CJL continues to increase as K gets
larger. Fig. 1 (c) displays the corresponding marginal likelihood values  CML  of the dictionaries
obtained by MJLE in Fig. 1 (b); this ﬁgure empirically shows that maximizing the joint likelihood
does not necessarily imply maximization of the marginal likelihood. These ﬁgures display the mean
and standard deviation values obtained from 7 experiments.

The likelihood values increase with the number of components  as expected from nested models.
However  the marginal likelihood stagnates after K = 16. Manual inspection reveals that passed
this value of K  the extra columns of W are pruned to zero  leaving the criterion unchanged. Hence 
MMLE appears to embed automatic order selection  similar to automatic relevance determination
[17  18]. The dictionaries learnt from MJLE and MMLE with K = 20 components are shown in
Fig. 2 (b) and (c). As can be seen from Fig. 2 (b)  MJLE produces spurious or duplicated compo-
nents. In contrast  the ground truth is well recovered with MMLE.

2MATLAB code is available at http://perso.telecom-paristech.fr/∼dikmen/nips11/

6

(a) Data

(b) WMJLE

(c) WMMLE

Figure 2: Data samples and dictionaries learnt on the swimmer dataset with K = 20.

5.2 A piano excerpt

In this section  we consider the piano data used in [3]. It is a toy audio sequence recorded in real
conditions  consisting of four notes played all together in the ﬁrst measure and in all possible pairs in
the subsequent measures. A power spectrogram with analysis window of size 46 ms was computed 
leading to F = 513 frequency bins and N = 676 time frames. We ran MMLE with K = 20 on the
spectrogram. We reconstructed STFT component estimates from the factorization ˆW ˆH  where ˆW is
the MMLE dictionary estimate and ˆH = hHiq. We used the minimum mean square error (MMSE)
estimate given by ˆcf kn = gf kn. xf n  where gf kn is the time-frequency Wiener mask deﬁned by
ˆwf kˆhkn/Pj ˆwf j ˆhjn. The estimated dictionary and the reconstructed components in the time do-

main after inverse STFT are shown in Fig. 3 (a). Out of the 20 components  12 were assigned to zero
during inference. The remaining 8 are displayed. 3 of the nonzero dictionary columns have very
small values  leading to inaudible reconstructions. The ﬁve signiﬁcant dictionary vectors correspond
to the frequency templates of the four notes and the transients. For comparison  we applied the non-
parametric approach by Hoffman et al. [11] on the same data with the same hyperparameters for H.
The estimated dictionary and the reconstructed components are presented in Fig. 3 (b). 10 out of
20 components had very small weight values. The most signiﬁcant 8 of the remaining components
are presented in the ﬁgure. These components do not exactly correspond to individual notes and
transients as they did with MMLE. The fourth note is mainly represented in the ﬁfth component  but
partially appears in the ﬁrst three components as well. In general  the performance of the nonpara-
metric approach depends more on initialization  i.e.  requires more repetitions than MMLE. For the
above results  we used 200 repetitions for the nonparametric method and 20 for MMLE (without
annealing  same stopping criterion) and chose the repetition with the highest likelihood.

5.3 Decomposition of a real song

In this last experiment  we decompose the ﬁrst 40 seconds of God Only Knows by the Beach Boys.
This song was produced in mono and we retrieved a downsampled version of it at 22kHz from the
CD release. We computed a power spectrogram with 46 ms analysis window and ran our VB algo-
rithm with K = 50. Fig. 4 displays the original data  and two examples of estimated time-frequency
masks and reconstructed components. The ﬁgure also shows the variance of the reconstructed com-
ponents and the evolution of the variational bound along iterations. In this example  5 components
out of the 50 are completely pruned in the factorization and 7 others are inaudible. Such decompo-
sition can be used in various music editing settings  for example for mono to stereo remixing  see 
e.g.  [3].

7

W

MMLE

c
MMLE

W

hoffman

c
hoffman

(a) MMLE

(b) Hoffman et al.

Figure 3: The estimated dictionary and the reconstructed components by MMLE and the nonpara-
metric approach by Hoffman et al. with K = 20.

Log power data spectrogram

x 10−3

Variance of reconstructed components

200

400

600

800

1000

1200

1400

1600

Temporal data

5

10

15

20

25

30

35

40

Time−frequency Wiener mask of component 13

5

4

3

2

1

0

5

x 105

15

10
40
Variational bound against iterations

35

20

25

30

45

50

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

Time−frequency Wiener mask of component 18

10

5

0

500

400

300

200

100

200

400

600

800

1000

1200

1400

1600

200

400

600

800

1000

1200

1400

1600

Reconstructed component 13

Reconstructed component 18

5

10

15

20

25

30

35

40

1

0

−1

0

5

10

15

20

25

30

35

40

500

400

300

200

100

1

0

−1

0

500

400

300

200

100

1

0

−1

0

Figure 4: Decomposition results of a real song. The Wiener masks take values between 0 (white)
and 1 (black). The ﬁrst example of reconstructed component captures the ﬁrst chord of the song 
repeated 4 times in the intro. The other component captures the cymbal  which starts with the ﬁrst
verse of the song.

Acknowledgments

This work is supported by project ANR-09-JCJC-0073-01 TANGERINE (Theory and applications
of nonnegative matrix factorization).

6 Conclusions

In this paper we have challenged the standard NMF approach to nonnegative dictionary learning 
based on maximum joint likelihood estimation  with a better-posed approach consisting in maximum
marginal likelihood estimation. The proposed algorithm based on variational inference has compa-
rable computational complexity to standard NMF/MJLE. Our experiments on synthetical and real
data have brought up a very attractive feature of MMLE  namely its self-ability to discard irrelevant
columns in the dictionary  without resorting to elaborate schemes such as Bayesian nonparametrics.

8

References

[1] D. D. Lee and H. S. Seung. Learning the parts of objects with nonnegative matrix factorization.

Nature  401:788–791  1999.

[2] C. F´evotte and A. T. Cemgil. Nonnegative matrix factorisations as probabilistic inference in
composite models. In Proc. 17th European Signal Processing Conference (EUSIPCO)  pages
1913–1917  Glasgow  Scotland  Aug. 2009.

[3] C. F´evotte  N. Bertin  and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura-
Saito divergence. With application to music analysis. Neural Computation  21(3):793–830 
Mar. 2009.

[4] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. Journal of

Machine Learning Research  3:993–1022  Jan. 2003.

[5] Thomas Hofman. Probabilistic latent semantic indexing. In Proc. 22nd International Confer-

ence on Research and Development in Information Retrieval (SIGIR)  1999.

[6] E. Gaussier and C. Goutte. Relation between PLSA and NMF and implications. In Proc. 28th
annual international ACM SIGIR conference on Research and development in information
retrieval (SIGIR’05)  pages 601–602  New York  NY  USA  2005. ACM.

[7] M. Welling  C. Chemudugunta  and N. Sutter. Deterministic latent variable models and their

pitfalls. In SIAM Conference on Data Mining (SDM)  pages 196–207  2008.

[8] W. L. Buntine and A. Jakulin. Discrete component analysis. In Lecture Notes in Computer

Science  volume 3940  pages 1–33. Springer  2006.

[9] John F. Canny. GaP: A factor model for discrete data. In Proceedings of the 27th ACM in-
ternational Conference on Research and Development of Information Retrieval (SIGIR)  pages
122–129  2004.

[10] O. Dikmen and C. F´evotte. Maximum marginal likelihood estimation for nonnegative dictio-
nary learning. In Proc. of International Conference on Acoustics  Speech and Signal Process-
ing (ICASSP’11)  Prague  Czech Republic  2011.

[11] M. Hoffman  D. Blei  and P. Cook. Bayesian nonparametric matrix factorization for recorded
music. In Proc. 27th International Conference on Machine Learning (ICML)  Haifa  Israel 
2010.

[12] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician  58:30 –

37  2004.

[13] Y. Cao  P. P. B. Eggermont  and S. Terebey. Cross Burg entropy maximization and its applica-
tion to ringing suppression in image reconstruction. IEEE Transactions on Image Processing 
8(2):286–292  Feb. 1999.

[14] C. M. Bishop. Pattern Recognition And Machine Learning. Springer  2008. ISBN-13: 978-

0387310732.

[15] K. Katahira  K. Watanabe  and M. Okada. Deterministic annealing variant of variational
Bayes method. In International Workshop on Statistical-Mechanical Informatics 2007 (IW-
SMI 2007)  2007.

[16] D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct de-
composition into parts? In Sebastian Thrun  Lawrence Saul  and Bernhard Sch¨olkopf  editors 
Advances in Neural Information Processing Systems 16. MIT Press  Cambridge  MA  2004.

[17] D. J. C. Mackay. Probable networks and plausible predictions – a review of practical Bayesian
models for supervised neural networks. Network: Computation in Neural Systems  6(3):469–
505  1995.

[18] C. M. Bishop. Bayesian PCA. In Advances in Neural Information Processing Systems (NIPS) 

pages 382–388  1999.

9

,Kustaa Kangas
Mikko Koivisto
Teppo Niinimäki