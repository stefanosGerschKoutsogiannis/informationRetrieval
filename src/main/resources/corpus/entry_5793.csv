2013,Unsupervised Spectral Learning of Finite State Transducers,Finite-State Transducers (FST) are a standard tool for modeling paired input-output sequences and are used in numerous applications  ranging from computational biology to natural language processing. Recently Balle et al. presented a spectral algorithm for learning FST from samples of aligned input-output sequences.  In this paper we address the more realistic  yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as finding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation  we provide identifiability results for FST distributions. Then  following previous work on rank minimization  we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.,Unsupervised Spectral Learning of FSTs

Rapha¨el Bailly

Xavier Carreras

Ariadna Quattoni

Universitat Politecnica de Catalunya

Barcelona  08034

rbailly carreras aquattoni@lsi.upc.edu

Abstract

Finite-State Transducers (FST) are a standard tool for modeling paired input-
output sequences and are used in numerous applications  ranging from computa-
tional biology to natural language processing. Recently Balle et al. [4] presented
a spectral algorithm for learning FST from samples of aligned input-output se-
quences. In this paper we address the more realistic  yet challenging setting where
the alignments are unknown to the learning algorithm. We frame FST learning as
ﬁnding a low rank Hankel matrix satisfying constraints derived from observable
statistics. Under this formulation  we provide identiﬁability results for FST dis-
tributions. Then  following previous work on rank minimization  we propose a
regularized convex relaxation of this objective which is based on minimizing a
nuclear norm penalty subject to linear constraints and can be solved efﬁciently.

1

Introduction

This paper addresses the problem of learning probability distributions over pairs of input-output
sequences  also known as transduction problem. A pair of sequences is made of an input sequence 
built from an input alphabet  and an output sequence  built from an output alphabet. Finite State
Transducers (FST) are one of the main probabilistic tools used to model such distributions and
have been used in numerous applications ranging from computational biology to natural language
processing. A variety of algorithms for learning FST have been proposed in the literature  most of
them are based on EM optimizations [9  11] or grammatical inference techniques [8  6].
In essence  an FST can be regarded as an HMM that generates bi-symbols of combined input-output
symbols. The input and output symbols may be generated jointly or independently conditioned on
the previous observations. A particular generation pattern constitutes what we call an alignment.

GAATTCAG-
| | || |
GGA-TC-GA

GAATTCAG-
| || | |
GGAT-C-GA

GAATTC-AG
| | || |
GGA-TCGA-

GAATTC-AG
| || | |
GGAT-CGA-

To be able to handle different alignments  a special empty symbol∗ is added to the input and output
be represented by the pair x∗ (resp. ∗

alphabets. With this enlarged set of bi-symbols  the model is able to generate an input symbol (resp.
an output symbol) without an output symbol (resp. input symbol). These special bi-symbols will
y). As an example  the ﬁrst alignment above will correspond
to the two possible representations G
A. Under this model the
G
probability of observing a pair of un-aligned input-output sequences is obtained by integrating over
all possible alignments.
Recently  following a recent trend of work on spectral learning algorithms for ﬁnite state machines
[14  2  17  18  7  16  10  5]  Balle et al. [4] presented an algorithm for learning FST where the input
to the algorithm are samples of aligned input-output sequences. As with most spectral methods the
core idea of this algorithm is to exploit low-rank decompositions of some Hankel matrix representing

A∗∗

A and G

G

A∗ G

G

A∗ G

G

A∗ A

A

T∗ T

T

T∗ T

T

A
A

G

∗

G

∗

∗

C
C

C
C

1

the distribution of aligned sequences. To estimate this Hankel matrix it is assumed that the algorithm
can sample aligned sequences  i.e. it can directly observe sequences of enlarged bi-symbols.
While the problem of learning FST from fully aligned sequences (what we sometimes refer to as
supervised learning) has been solved  the problem of deriving an unsupervised spectral method that
can be trained from samples of input-output sequences alone (i.e. where the alignment is hidden)
remains open. This setting is signiﬁcantly more difﬁcult due to the fact that we must deal with two
sets of hidden variables: the states and the alignments. In this paper we address this unsupervised
setting and present a spectral algorithm that can approximate the distribution of paired sequences
generated by an FST without having access to aligned sequences. To the best of our knowledge this
is the ﬁrst spectral algorithm for this problem.
The main challenge in the unsupervised setting is that since the alignment information is not avail-
able  the Hankel matrices (as in [4]) can no longer be directly estimated from observable statistics.
However  a key observation is that we can nevertheless compute observable statistics that can con-
straint the coefﬁcients of the Hankel matrix. This is because the probability of observing a pair of
un-aligned input-output sequences (i.e. an observable statistic) is computed by summing over all
possible alignments; i.e. by summing entries of the hidden Hankel matrix. The main idea of our al-
gorithm is to exploit these constraints and ﬁnd a Hankel matrix (from which we can directly recover
the model) which both agrees on the observed statistics and has a low-rank matrix factorization.
In brief  our main contribution is to show that an FST can be approximated by solving an optimiza-
tion which is based on ﬁnding a low-rank matrix satisfying a set of constraints derived from observ-
able statistics and Hankel structure. We provide sample complexity bounds and some identiﬁability
results for this optimization that show that  theoretically  the rank and the parameters of an FST
distribution can be identiﬁed. Following previous work on rank minimization  we propose a regu-
larized convex relaxation of the proposed objective which is based on minimizing a nuclear norm
penalty subject to linear constraints. The proposed relaxation balances a trade-off between model
complexity (measured by the nuclear norm penalty) and ﬁtting the observed statistics. Synthetic
experiments show that the performance of our unsupervised algorithm efﬁciently approximates that
of a supervised method trained from fully aligned sequences.
The paper is organized as follows. Section 2 gives preliminaries on FST and spectral learning
methods  and establishes that an FST can be induced from a Hankel matrix of observable aligned
statistics. Section 3 presents a generalized form of Hankel matrices for FST that allows to express
observation constraints efﬁciently. One can not observe generalized Hankel matrices without as-
summing access to aligned samples. To solve this problem  Section 4 formulates ﬁnding the Hankel
matrix of an FST from unaligned samples as rank minimization problem. This section also presents
the main theoretical results of the method  as well as a convex relaxation of the rank minimization
problem. Section 5 presents results on synthetic data and Section 6 concludes.

2 Preliminaries

2.1 Finite-State Transducers

Deﬁnition 1. A Finite-State Transducer (FST) of rank d is given by:

• alphabets Σ+={x1  . . .   xp} (input)  Σ−={y1  . . .   yq} (output)
• α1∈ Rd  α∞∈ Rd
• ∀x∈ Σ+∪{∗} ∀y∈ Σ−∪{∗}  a matrix Mx
∈ Rd×d  with M∗∗= 0
Deﬁnition 2. Let s be an input sequence  and let t be an output sequence. An alignment of(s  t) is
by removing the empty symbols∗ equals s (resp. t).
Deﬁnition 3. The set of alignments for a pair of sequences(s  t) is denoted[s  t].
Deﬁnition 4. Let Σ={Σ+∪{∗}}×{Σ−∪{∗}}. The set of aligned sequences is Σ
∗. The empty

such that the sequence obtained from x1 . . . xn (resp. y1 . . . yn)

given by a sequence of pairs x1
y1

. . . xn
yn

string is denoted ε.

y

2

for the model T is given by:

Deﬁnition 5. Let T be an FST  and let w= x1
rT(w)= α

Deﬁnition 6. Let(s  t) be an i/o (input/output) sequence. Then the value for(s  t) computed by an
rT((s  t))= Q

FST T is given by the sum of the values for all alignments:

be an aligned sequence. Then the value of w

⋅ α∞
Mxn
∈[s t] rT(x1

1Mx1
y1

. . . xn
yn

. . . xn
yn

)

y1

y1

yn

x1
y1 ...xn
yn

A more complete description of FST can be found in [15].

1

1

tj

tj

t1

tj



M∗

2.2 Computing with an FST

[Id− M]−1α∞

∗−. The forward vector is deﬁned by:
+ Fi−1 j−1Msi

alignments  which is generally exponential in the length of s and t. Standard techniques (e.g. the
edit distance algorithm) can be applied in order to compute such a value in polynomial time.

In order to compute the value of a pair of sequences(s  t)  one needs to sum over all possible
Proposition 1. Let T be an FST  s1∶n∈ Σ
∗+  t1∶m∈ Σ
1Ms1∗ Msi∗   Fi j= Fi−1 jMsi∗ + Fi j−1M∗
  Fi 0= α
F0 j= α


1M∗
It is then possible to compute rT((s  t))= Fn mα∞ in O(d2st). The sum of rT over all possible
∗)=∑s∈Σ+ t∈Σ− rT((s  t)) can be computed with the formula
values rT(Σ
∗)= α
rT(Σ
where M=∑x∈Σ+∪{∗} y∈Σ−∪{∗} Mx
Example 1. Let us consider a particular subclass of FST: Σ−= Σ+={0  1}  where M0
= M1
=
= 0.
M0∗= M∗
1~3 
0 
Let us draw all the paths for the i/o sequence(01  001). The dashed red edges are discarded because
∗
and∗
Let us consider the model A which satisﬁes the constraints above. One has rA(0
) =
)= 1~192 and  as those two aligned sequences are the only possible alignments for
1~96  rA(∗
(01  001)  one has rA((01  001))= 1~64. It is possible to check that rA(Σ
∗)= 1  thus the model

0  M1∗= 1~3
= 0
0  M0

of the constraints. Thus  there are only two different non-zero paths  corresponding to 0
0

1~6  M∗
1~4
0  M1

α1= 1
α∞= 1~4

= 1~6
= 0
1~2

The FST A satisﬁes the constraints.

1 (in green)
1

1 (in blue).
1

∗

y .

0∗
0

0∗

0
0

1
1

1
1

0
0

0

0

0

1

1

0

0

0

0

0

0

0

0

0
0

0
0

0

0

1

1
1
1
1

0

0

1

0

computes a probability distribution.

2.3 Hankel Matrices

Let us recall some basic deﬁnitions and properties.

∗ a set of preﬁxes  V ⊂ Σ

Let Σ be an alphabet  U⊂ Σ
∗ a set of sufﬁxes. U is said to be preﬁx-closed
if uv∈ U⇒ u∈ U. V is said to be sufﬁx-closed if uv∈ V ⇒ v∈ V .
∈ Σ}.
u∈ U  x
Let us denote U Σ the set U∪{ux
∈ Σ}. Let us denote ΣV the set V ∪{x
A Hankel matrix on U and V is a matrix with rows corresponding to elements u∈ U and columns
corresponding to v∈ V   which satisﬁes uv= u
′→ H(u  v)= H(u
′).
′
′
Deﬁnition 7. Let H a Hankel matrix for U Σ and ΣV . One supposes that ε∈ U and ε∈ V . One
then deﬁnes the partial Hankel matrices deﬁned for u∈ U and∈ V :
  H1(v)= H(ε  v)
(u  v)= H(u  x
Hε(u  v)= H(u  v)
  H∞(u)= H(u  ε)
yv)

yvv∈ V  x

  Hx

  v

v

y

y

y

y

3

y



+

+



α
1

H

ε

y

1 H
ε

= H

We will not give a proof of this result  as a more general result is seen further. The rank equality
comes from the fact that the WA deﬁned above has the same rank as Hε  and that the rank of a

The main result that we will be using is the following:
Proposition 2. Let H a Hankel matrix for U Σ and ΣV . One supposes that U is preﬁx-closed  V is

sufﬁx-closed  and that rank(Hε)= rank(H). Then the WA deﬁned by
= Hx
  α∞= H∞   Mx
computes a mapping f such that∀u∈ U ∀v∈ V  f(uv)= Hε(u  v).
mapping f which satisﬁes f(uv)= H(u  v) is at least the rank of H. The following example shows
ΣV with U={ε  a3
b3}  and V ={ε  a3
b3}.
  H=
 ε
Hε=
0 1~4

ïïïïï  H
ïïïïï
1~4
′=
1~4 1~4
1~4 1~4 1~4
One has ε∈ U and ε∈ V   and also rank(Hε)= rank(H)= 1  thus the computed WA is rank 1.
b6)= 1~4. The complete Hankel
Such a WA cannot compute a mapping such that rT()= 0 and rT(a6

that the preﬁx and sufﬁx closeness are necessary conditions.
Example 2. Let us consider the following Hankel over the set of preﬁxes U Σ and the set of sufﬁxes

0 0 1~4 1~4
0 0 1~4 1~4

a
ε
b
0 0
0 0

ïïïïïï

ε
a
b
a3
b3
a4
b4

ïïïï

a2
b2
a3
b3
a4
b4

a4
b4
0
0

a3
b3
0
0

a2
b2
0

a3
b3
0

a3
b3
0

ε
a3
b3

a4
b4

0

0

matrix has at least rank 7.

3

Inducing FST from Generalized Hankel Matrices

y1

y1

y1 ...xn
yn

. . . xn
yn

. . . xn
yn

∈[s t] rT(x1

)  where rT(x1

) is computed from the Hankel.

Proposition 2 tells us that if we had access to certain sub-blocks of the Hankel matrix for aligned
sequences we could recover the FST model. However  we do not have access to the hidden

vations. One natural idea would be to search for a Hankel matrix that agrees with the obser-
vations. To do so  we introduce observable constraints  which are linear constraints of the form

From a matrix satisfying the hypothesis of Proposition 2 and the observation constraints  one can
derive an FST computing a mapping which agrees on the observations.

alignment information: we only have access to the statistics p(s  t)  which we will call obser-
p(s  t)=∑x1
Given an i/o sequence(s  t)  the size of[s  t] (hence the size of the Hankel matrix) is in general ex-
One will denote by P(A) the set of subsets of A.
′w∈ u  w
′={ww
′∈ u
′}.
∗). The set uu
′∈ P(Σ
′ is deﬁned by uu
y1∶n will denote the set{x1∶n
}  which contains a single
Deﬁnition 8. Let u  u
[s  t] will denote the set{x1∶nxn+1∶n+k
 xn+1∶n+k
∈[s  t]}  which extends
We will denote sets of alignments as follows: x1∶n
} with all ways of aligning(s  t). We will also use[s  t]x1∶n
{x1∶n
aligned sequence; then x1∶n
y1∶n
y1∶nyn+1∶n+k
[s  t])  where the aligned part is possibly empty.
[s  t]x1∶n
y1∶n
Deﬁnition 9. A generalized preﬁx (resp. generalized sufﬁx) is the empty set or a set of the form
(resp. x1∶n
y1∶n

ponential in the size of s and t. In order to overcome that problem when considering the observation
constraints  one will consider aggregations of rows and columns  corresponding to sets of preﬁxes
and sufﬁxes. Obviously  the deﬁnition of a Hankel matrix must be extended to this case.

y1∶n analogously.

y1∶n
yn+1∶n+k

y1∶n

3.1 Generalized Hankel

One needs to extend the deﬁnition of a Hankel matrix to the generalized preﬁxes and sufﬁxes.
Deﬁnition 10. Let U be a set of generalized preﬁxes  V be a set of generalized sufﬁxes. Let H be a
matrix indexed by U (in rows) and V (in columns). H is a generalized Hankel matrix if it satisﬁes:

uv= ä
′
u′∈¯u′ v′∈¯v′ u

′⇒ Q
u∈¯u v∈¯v

H(u  v)= Q

u′∈¯u′ v′∈¯v′ H(u
′

′)

v

v

′⊂ U ∀¯v  ¯v

∀¯u  ¯u
′⊂ V  ä
u∈¯u v∈¯v
whereä is the disjoint union.

4

t1

t1

tk

tk

y1∶n

x1∶n
y1∶n

y1∶n−1

∈ U

Deﬁnition 12. Let V be a set of generalized sufﬁxes. V is sufﬁx-closed if

Deﬁnition 13. Let U be a set of generalized preﬁxes  V be a set of generalized sufﬁxes. The right-

In particular  if U and V are sets of regular preﬁxes and sufﬁxes  this deﬁnition encompasses the
regular deﬁnition for a Hankel matrix.
Deﬁnition 11. Let U be a set of generalized preﬁxes. U is preﬁx-closed if

[s  t]x1∶n
∈ U⇒[s  t]x1∶n−1
∈ U
 [s1∶n−1  t1∶k−1]sn
[s1∶n  t1∶k]∈ U⇒[s1∶n−1  t1∶k]sn∗  [s1∶n  t1∶k−1]∗
[s  t]∈ V ⇒ x2∶n
[s  t]∈ V
[s2∶n  t2∶k]∈ V
[s1∶n  t1∶k]∈ V ⇒ s1∗[s2∶n  t1∶k] 
[s1∶n  t2∶k]  s1
y2∶n
∗
operator completion of U is the set U Σ= U∪{ux
u∈ U  x
∈ Σ}. The left-operator completion of V
is the set ΣV = V ∪{x
yvv∈ V  x
a Hankel matrix built from U Σ and ΣV . One supposes that rank(Hε)= rank(H)  U is preﬁx-
= H
= Hx

+
computes a mapping rA such that∀u∈ U ∀v∈ V  rA(uv)= Hε(u  v)
preﬁx (resp. sufﬁx) closure of input (resp. output) strings in S. Let U ={[s  t]}s∈prefS in t∈prefS out
and V ={[s  t]}s∈suffS in t∈suffS out. The sets U and V contain all the observed pairs of unaligned

A key result is the following  which is analogous to Proposition 2 for generalized Hankel matrices:
Proposition 3. Let U and V be two sets of generalized preﬁxes and generalized sufﬁxes. Let H be

Let S be a sample of unaligned sequences. Let prefS in (resp. prefS out  suffS in  suffS out) be the

closed and V is sufﬁx-closed. Then the model A deﬁned by α
1

ε   α∞= H∞  Mx
+

Proof. The proof can be found in the Appendix.

∈ Σ}.

sequences  and one can check that the sizes of U Σ and ΣV are polynomial in the size of S.

1 H



H

y

y

y

ε

y

y

Example 3. Let us continue with the same model A as in Example 1. Let us now consider the
preﬁxes   0

0 and the sufﬁxes   1

0

1~96 
0 

0

0

0

0

0

0

0

1. The Hankel matrices will be:

= 1~24
0  H∞= 1~4
0  Hε= 1~4
H1= 1~4
1~32  H∗
1~32
H1∗= 1~12
=
 H1
= 0
1~192  H0
1~32
′ deﬁned by:
0  M1∗= 1~3
0  α∞= 1~4
1~6 
α1= 1
= 1~6
0 
0  M1
= 0
1~3  M0
= 0
1~8
′ computes the same probability distribution than A.

0

0

0

0

0

1

0

0

0

0

0

1

1

0

and one ﬁnally gets the model A

M∗
One can easily check that A

0

4 FST Learning as Non-convex Optimization

Proposition 3 shows that FST models can be recovered from generalized Hankel matrices. In this
section we show how FST learning can be framed as an optimization problem where one searches
for a low-rank generalized Hankel matrix that agrees with observation constraints derived from a
sample. We assume here that p is a probability distribution over i/o sequences.

We will denote by z =(p([s  t]))s∈Σ∗+ t∈Σ∗− the vector built from observable probabilities  and by
zS=(pS([s  t]))s∈Σ∗+ t∈Σ∗− the set of empirical observable probabilities  where pS is the frequency

deduced from an i.i.d. sample S.

5

minimize

The optimization task is the following:

Let H be the vector describing the coefﬁcients of H. Let K be the matrix such that KH= 0 rep-
resents the Hankel constraints (cf. deﬁnition 10). Let O be the matrix such that OH= z represents
the observation constraints (i.e.∑ H([s  t]  )= p([s  t])).
rank(H)
subject to OH− z2≤ µ
KH= 0
H2≤ 1.

The conditionH2≤ 1 is necessary to bound the set of possible solutions. In particular  if H is
the Hankel matrix of a probability distribution  the condition is satisﬁed: one hasH1≤ 1 as each
column is a probability distribution  as well asHF ≤ 1 and thusH2≤H1HF ≤ 1. Let
Let us denoteHµ the class of Hankel matrices solutions of (1) for a given µ  where z represents the
p(uivj). Let us denoteHS
represents pS(uivj)  the observed frequencies in a sample S i.i.d. with respect to p.
V such that any solution ofH0 leads to an FST
ε   α∞= H∞  Mx
+

Proposition 4. Let p be a distribution over i/o sequences computed by an FST. There exists U and

µ the class of Hankel matrices solutions of (1) for a given µ  where zS

us remark that the set of matrices satisfying the constraints is a compact set.

= Hx

= H

1 H

H
ε

(1)

+





α
1

H

y

y

which computes p.

Proof. The proof can be found in the Appendix.

4.1 Theoretical Properties

µ2

leads to a rank-d FST with probability

ï2
8 log(1~δ)
2 log(1~δ)
S

S>ïï 2+
µ solution of (1) with µ= 1+

We now present the main theoretical results related to the optimization problem (1). The ﬁrst one
concerns the rank identiﬁcation  while the second one concerns the consistency of the method.
Proposition 5. Let p be a rank d distribution computed by an FST. There exists µ2 such that for any

δ> 0 and any i.i.d. sample S
implies that any H∈HS
at least 1− δ.
δ> 0 and any i.i.d. sample S
implies that for any H∈HS
whereA  AS∞ is the maximum distance for model parameters  and σp is a non-zero parameter

S>ïï 1+
ï2
2 log(1~δ)
µ solution of (1) with µ= 1+
2 log(1~δ)
S
)
A  AS∞≤ O( d2

Proof. The proof can be found in the Appendix.
Proposition 6. Let p be a rank d distribution computed by an FST. There exists µ such that for any

leading to a model As there exists

a model A computing p such that

σ3
p

µ

depending on p.

Proof. The proof can be found in the Appendix.

6

0

∗

1
1

0

0

1∗1

1

1~12

) are

∗
1~36

) and rM(0
ïï

Example 4. This continues Examples 1 and 3. Let us ﬁrst remark that  among all the values used to
build the Hankel matrices in Example 3  some of them correspond to observable statistics  as there

not observable. Then the rank minimization objective is not sufﬁcient  as it allows any value for
those variables.
Let us consider now larger sets of preﬁxes and sufﬁxes: ε 

is only one possible alignment for them. Conversely  the exact values of rM(0
0 and ε  1∗  1
ïï H1∗=ïïï 1~12
H=ïïï 1~4
=ïïï
ïï H1
1~24
1~32
1~32
′ subject to the constraints O and K:
ïï=(hij)   O= h11= 1~4
  K= h12= h41 h23= h81
′=ïïï Hε
h12= 1~12
h13= h71 h32= h61
h13= 0
h22= h51 h33= h91
h21= 1~24
)= rA((01  001))= 1~64. One
The relation h63+ h92= 1~64 is due to fact that rA(0
has h22= h51 as they both represent p(∗
′ has a rank greater or equal than 2.
h22= h51= 1~72  h52= 1~216  h63= 1~192  h92= 1~96

h63+ h92= 1~64
)+ rA(∗
1∗). It is clear that H

1. One then has
0
0
0

0
0
?

0
0

The only way to reach rank 2 under the constraints is

We want to minimize the rank of H

H1∗

H1
1

0
0

1
1

0

1
1

0

0

⋮
⋮

∗

?
0

?
0

?
0

0

0
0

0  0

0
0
?

1

H

0

Thus  the process of rank minimization under linear constraints leads to one single model  which is
identical to the original one. Of course  in the general case  the rank minimization objective may
lead to several models.

4.2 Convex Relaxation

The problem as it is stated in (1) is NP-hard to solve in the size of the Hankel matrix  hence impos-
sible to deal with in practical cases. One can solve instead a convex relaxation of the problem (1) 
obtained by replacing the rank objective by the nuclear norm. The relaxed optimization statement is
then the following:

minimize

H∗
subject to OH− z2≤ µ
KH= 0
H2≤ 1.

H

(2)

This type of relaxation has been used extensively in multiple settings [13]. The nuclear norm∗
plays the same role than the1 norm in convex relaxations of the0 norm  used to reach sparsity

under linear constraints.

5 Experiments

We ran synthetic experiments using samples generated from random FST with input-output alpha-
bets of size two. The main goal of our experiments was to compare our algorithm to a supervised
spectral algorithm for FST that has access to alignments. In both methods  the Hankel was deﬁned
for preﬁxes and sufﬁxes up to length 1. Each run consists of generating a target FST  and generating
N aligned samples according to the target distribution. These samples were directly used by the su-
pervised algorithm. Then  we removed the alignment information from each sample (thus obtaining
a pair of unaligned strings)  and we used them to train an FST with our general learning algorithm 
trying different values for a C parameter that trades-off the nuclear norm term and the observation
term. We ran this experiment for 8 target models of 5 states  for different sampling sizes. We mea-
sure the L1 error of the learned models with respect to the target distribution on all unaligned pairs
of strings whose sizes sum up to 6. We report results for geometric averages.
In addition  we ran two additional methods. First a factorized method that assumes that the two
sequences are generated independently  and learns two separate weighted automata using a spectral

7

Figure 1: Learning curves for different methods: SUP  supervised; UNS  unsupervised with dif-
ferent regularizers; EM  expectation maximization. The curves are averages of L1 error for random
target models of 5 states.

method. Its performance is very bad  with L1 error rates at∼0.08  which conﬁrms that our target

models generate highly dependent sequence pairs. This baseline result also implies that the rest
of the methods can learn the dependencies between paired strings. Second  we ran an Expectation
Maximization algorithm (EM).
Figure 1 shows the performance of the learned models with respect to the number of samples.
Clearly  our algorithm is able to estimate the target distribution and gets close to the performance of
the supervised method  while making use of much simpler statistics. EM performed slightly better
than the spectral methods  but nonetheless at the same levels of performance.
One can ﬁnd other experimental results for the unsupervised spectral method in [1]  where it is
shown that  under certain circumstances  unsupervised spectral method can perform better than su-
pervised EM. Though the framework (unsupervised learning of PCFGs) is not the same  the method
is similar and the optimization statement is identical.

6 Conclusion

In this paper we presented a spectral algorithm for learning FST from unaligned sequences. This is
the ﬁrst paper to derive a spectral algorithm for the unsupervised FST learning setting. We prove that
there is theoretical identiﬁability of the rank and the parameters of an FST distribution  using a rank
minimization formulation. However  this problem is NP-hard  and it remains open whether there
exists a polynomial method with identiﬁability results. Classically  rank minimization problems
are solved with convex relaxations such as the nuclear norm minimization we have proposed. In
experiments  we show that our method is comparable to a fully supervised spectral method  and
close to the performance of EM.
Our approach follows a similar idea to that of [3] since both works combine classic ideas from
spectral learning with classic ideas from low rank matrix completion. The basic idea is to frame
learning of distributions over structured objects as a low-rank matrix factorization subject to linear
constraints derived from observable statistics. This method applies to other grammatical inference
domains  such as unsupervised spectral learning of PCFGs ([1]).

Acknowledgments

We are grateful to Borja Balle and the anonymous reviewers for providing us with helpful comments.
This work was supported by a Google Research Award  and by projects XLike (FP7-288342)  ERA-
Net CHISTERA VISEN  TACARDI (TIN2012-38523-C02-02)  BASMATI (TIN2011-27479-C04-
03)  and SGR-LARCA (2009-SGR-1428). Xavier Carreras was supported by the Ram´on y Cajal
program of the Spanish Government (RYC-2008-02223).

8

 0.0001 0.0005 0.001 0.0015 0.002 0.0025 0.00310k20k50k100k200k500k1ML1 errorNumber of Samples (logscale)UNS 0.01UNS 0.1SUPEMReferences
[1] R. Bailly  X. Carreras  F. M. Luque  and A. Quattoni. Unsupervised spectral learning of wcfg

as low-rank matrix completion. EMNLP  2013.

[2] R. Bailly  F. Denis  and L. Ralaivola. Grammatical inference as a principal component analysis

problem. In Proc. ICML  2009.

[3] B. Balle and M. Mohri. Spectral learning of general weighted automata via constrained matrix

completion. In Proc. of NIPS  2012.

[4] B. Balle  A. Quattoni  and X. Carreras. A spectral learning algorithm for ﬁnite state transduc-

ers. ECML–PKDD  2011.

[5] Borja Balle  Ariadna Quattoni  and Xavier Carreras. Local loss optimization in operator mod-
els: A new insight into spectral learning. In John Langford and Joelle Pineau  editors  Pro-
ceedings of the 29th International Conference on Machine Learning (ICML-12)  ICML ’12 
pages 1879–1886  New York  NY  USA  July 2012. Omnipress.

[6] M. Bernard  J-C. Janodet  and M. Sebban. A discriminative model of stochastic edit distance
in the form of a conditional transducer. Grammatical Inference: Algorithms and Applications 
4201  2006.

[7] B. Boots  S. Siddiqi  and G. Gordon. Closing the learning planning loop with predictive state

representations. I. J. Robotic Research  2011.

[8] F. Casacuberta. Inference of ﬁnite-state transducers by using regular grammars and morphisms.

Grammatical Inference: Algorithms and Applications  1891  2000.

[9] A. Clark. Partially supervised learning of morphology with stochastic transducers. In Proc. of

NLPRS  pages 341–348  2001.

[10] Shay B. Cohen  Karl Stratos  Michael Collins  Dean P. Foster  and Lyle Ungar. Spectral learn-
ing of latent-variable pcfgs. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)  pages 223–231  Jeju Island  Korea  July
2012. Association for Computational Linguistics.

[11] J. Eisner. Parameter estimation for probabilistic ﬁnite-state transducers. In Proc. of ACL  pages

1–8  2002.

[12] G. Stewart et J.-G. Sun. Matrix perturbation theory. Academic Press  1990.
[13] Maryam Fazel. Matrix rank minimization with applications. PhD thesis  Stanford University 

Electrical Engineering Dept.  2002.

[14] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden markov models.

In Proc. of COLT  2009.

[15] Mehryar Mohri. Finite-state transducers in language and speech processing. Computational

Linguistics  23(2):269–311  1997.

[16] A.P. Parikh  L. Song  and E.P. Xing. A spectral algorithm for latent tree graphical models.

ICML  2011.

[17] S.M. Siddiqi  B. Boots  and G.J. Gordon. Reduced-rank hidden markov models. AISTATS 

2010.

[18] L. Song  B. Boots  S. Siddiqi  G. Gordon  and A. Smola. Hilbert space embeddings of hidden

markov models. ICML  2010.

[19] L. Zwald and G. Blanchard. On the convergence of eigenspaces in kernel principal component

analysis. NIPS  2005.

9

,Raphael Bailly
Xavier Carreras
Ariadna Quattoni
Christian Kroer
Gabriele Farina
Tuomas Sandholm