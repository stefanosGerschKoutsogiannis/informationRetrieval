2019,Structure Learning with Side Information: Sample Complexity,Graphical models encode the stochastic dependencies among random variables (RVs). The vertices represent the  RVs  and the edges signify the conditional dependencies among the RVs. Structure learning is the process of inferring the edges by observing realizations of the RVs  and it has applications in a wide range of technological  social  and biological networks. Learning the structure of graphs when the vertices are treated in isolation from inferential information known about them is well-investigated. In a wide range of domains  however  often there exist additional inferred knowledge about the structure  which can serve as valuable side information. For instance  the gene networks that represent different subtypes of the same cancer share similar edges across all subtypes and also have exclusive edges corresponding to each subtype  rendering partially similar graphical models for gene expression in different cancer subtypes. Hence  an inferential decision regarding a gene network can serve as side information for inferring other related gene networks.  When such side information is leveraged judiciously  it can translate to significant improvement in structure learning. Leveraging such side information can be abstracted as inferring structures of distinct graphical models that are {\sl partially} similar. This paper focuses on Ising graphical models  and considers the problem of simultaneously learning the structures of two {\sl partially} similar graphs  where any inference about the structure of one graph offers side information for the other graph. The bounded edge subclass of Ising models is considered  and necessary conditions (information-theoretic )  as well as sufficient conditions (algorithmic) for the sample complexity for achieving a bounded probability of error  are established. Furthermore  specific regimes are identified in which the necessary and sufficient conditions coincide  rendering the optimal sample complexity.,Structure Learning with Side Information:

Sample Complexity

Saurabh Sihag

Ali Tajer

Electrical  Computer  and Systems Engineering Department

Rensselaer Polytechnic Institute

Abstract

Graphical models encode the stochastic dependencies among random vari-
ables (RVs). The vertices represent the RVs  and the edges signify the conditional
dependencies among the RVs. Structure learning is the process of inferring the
edges by observing realizations of the RVs  and it has applications in a wide range
of technological  social  and biological networks. Learning the structure of graphs
when the vertices are treated in isolation from inferential information known about
them is well-investigated. In a wide range of domains  however  often there exist
additional inferred knowledge about the structure  which can serve as valuable side
information. For instance  the gene networks that represent different subtypes of
the same cancer share similar edges across all subtypes and also have exclusive
edges corresponding to each subtype  rendering partially similar graphical models
for gene expression in different cancer subtypes. Hence  an inferential decision
regarding a gene network can serve as side information for inferring other related
gene networks. When such side information is leveraged judiciously  it can translate
to signiﬁcant improvement in structure learning. Leveraging such side informa-
tion can be abstracted as inferring structures of distinct graphical models that are
partially similar. This paper focuses on Ising graphical models  and considers the
problem of simultaneously learning the structures of two partially similar graphs 
where any inference about the structure of one graph offers side information for
the other graph. The bounded edge subclass of Ising models is considered  and
necessary conditions (information-theoretic )  as well as sufﬁcient conditions (algo-
rithmic) for the sample complexity for achieving a bounded probability of error 
are established. Furthermore  speciﬁc regimes are identiﬁed in which the necessary
and sufﬁcient conditions coincide  rendering the optimal sample complexity.

1

Introduction

Graphical models are widely used to compactly model the conditional interdependence among
multiple random variables Lauritzen [1996] and Pearl [2009]. The vertices of the graph represent
the random variables (RVs)  while the edges encode the inter-dependence among the RVs. The
complete structure of the graph is analytically captured by the joint probability distribution of the
random variables. Graphical models offer effective and tractable solutions to various inferential
and decision-making solutions in different domains  e.g.  computer vision Won and Derin [1992] 
genetics Chen et al. [2013]  Fang et al. [2016]  Dobra et al. [2004]  social networks Jacob et al.
[2014]  and power systems Dvijotham et al. [2017]. In this paper  we focus on Ising models and
consider the problem of joint model selection of a pair of graphs with partially identical structures
using the samples from their joint distributions.
Graphical models with partially similar structures arise in the domains that consist of multiple layered
networks of information sources. In such an application  each layer shares some of its vertices

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

and the data it generates with other layers that contain the same vertices. For example  the gene
networks that represent the subtypes of the same cancer share similar edges across all subtypes
and also have unique edges exclusive to each subtype of cancer Chen et al. [2013]. In a different
context  a similar problem emerges in analyzing the voting patterns of the members of the US Senate
Guo et al. [2015] for different categories of bills  where the statistical models reveal the common
dependency structure across the members afﬁliated to the same political party and other structures
unique to each class. Similarly  in the context of social networks  the relationships among a group
of individuals on different platforms (e.g.  Twitter and Facebook) form two distinct  but potentially
partially similar graphical models. In such applications  learning one graph provides a signiﬁcant
amount of information that can be used for learning other related graphs.
Due to the costs associated with collecting and processing data samples in large-scale graphical
models  it is of interest to study the sample complexity of learning multiple structures simultaneously 
where inference about each structure serves as side information for other structures.

1.1 Related Work

While the problem of graph structure learning is NP-hard Chickering [1996]  it becomes feasible
under certain restrictions on the structure of the graph. For instance  the studies in Yuan and Lin
[2007]  Rothman et al. [2008]  Ravikumar et al. [2010]  Banerjee et al. [2008] investigate recovering
the structure of the graphical model under sparsity. Such conditions on the structures of the graphical
models can be analyzed theoretically by considering certain restricted sub-classes of graphical models 
for e.g.  graphs with a bounded degree or bounded number of edges.
Information-theoretic analysis of structure learning establishes the algorithm-independent difﬁculty
of recovering the structure of different classes of graphs. The studies in Santhanam and Wainwright
[2012]  Tandon et al. [2014]  Scarlett and Cevher [2016] characterize the necessary conditions
on the sample complexity of selecting the model of a given graph in various sub-classes of Ising
models. In Santhanam and Wainwright [2012]  the necessary and sufﬁcient conditions on the sample
complexity for the exact recovery of the graph are established for the class of Ising models under
restrictions on the maximum degree and the maximum number of edges in the graph. The results
in Santhanam and Wainwright [2012] are extended to a set based graphical model selection in Vats and
Moura [2011]  where the graph estimator outputs a set of potentially true graphs instead of a unique
graph. Similarly  necessary conditions on the sample complexity are established for girth-bounded
graphs and path restricted graphs in Tandon et al. [2014]. In Scarlett and Cevher [2016]  the problem
of graphical model selection is studied for various sub-classes of Ising models under the criterion of
approximate recovery. In Das et al. [2012]  approximate recovery bounds on the sample complexity
are characterized for Ising and Gaussian models without considering the effect of edge weights. The
information-theoretic bounds on the sample complexity for structure recovery in Gaussian models
are established in Wang et al. [2010]  and the information-theoretic bounds for structure learning in
power-law graph class are characterized in Tandon and Ravikumar [2013].
Algorithm-independent bounds on the sample complexity have also been investigated for other
inference tasks besides model selection from the samples. In Gangrade et al. [2017]  the problem of
detecting whether two Markov network structures are identical or different is studied  and sample
complexity is characterized. The problem of property testing for Ising models is investigated
in Neykov and Liu [2017]  and information-theoretic limits for testing graph properties such as
connectivity  cycle presence  and maximum clique size are established. In Devroye et al. [2018] 
the problem of density estimation using the samples from the Ising model  is investigated  and the
minimax rate of estimation is analyzed.
Joint inference of multiple graphical models  even though recognized as an inference problem that
arises in various domains  is primarily studied only algorithmically in Chen et al. [2013]  Fang et al.
[2016]  Guo et al. [2011]  Danaher et al. [2014]  Mohan et al. [2014]  Yang et al. [2015]  Peterson
et al. [2015]  Guo et al. [2015]  Qiu et al. [2016]. In Chen et al. [2013]  an empirical Bayes method 
is deployed to identify interactions that are unique to each class and that are shared across all classes.
In Fang et al. [2016]  Guo et al. [2011]  Danaher et al. [2014]  Mohan et al. [2014]  Yang et al. [2015]
graphical Lasso-based algorithms are designed for joint inference of Gaussian graphical models. An
optimization framework is used in Guo et al. [2015] for joint estimation of the graph structures based
on discrete data. Similarly  Peterson et al. [2015] investigates the problem of joint estimation of

2

Figure 1: Partially similar structures. Yellow nodes in both graphs have identical structures (p = 8  q = 3).

Gaussian graphical models using a Bayesian approach  where the data groups are used to identify
partially similar models  and their similarity is leveraged.

1.2 Contributions

All the studies above on joint graphical model inference propose empirical or algorithmic model-
based frameworks. In this paper  in sharp contrast  we provide an information-theoretic perspective
for jointly learning the structures of a pair of similar Ising models. Such analysis offers algorithm-
independent necessary conditions on the sample complexity for achieving any arbitrary level of
reliability in the inference decision. In our previous work in Sihag and Tajer [2019] we considered
a sparsely connected  path-restricted sub-class in the context of Ising models and established the
algorithm-independent necessary conditions on the sample complexity. In this paper  we consider
a more general sub-class of edge bounded Ising models and provide the necessary conditions for
all feasible values of the number of edges in the graphs. Furthermore  we also analyze a maximum
likelihood (ML)-based graph decoder to establish sufﬁcient conditions on the sample complexity.
Based on these bounds  we also provide the asymptotic scaling behavior of these conditions in
different regimes. These analyses  as a by-product  also recover the existing relevant results on the
recovery of single graphs in Scarlett and Cevher [2016]. Finally  we provide numerical evaluations of
ML-based decoder to study the effect of structural similarity on its performance.
A structurally similar pair of graphs are assumed to have identical connectivities in a subgraph formed
by a known cluster of nodes. Such settings have been analyzed extensively in the context of seeded
graph matching and alignment problems Fishkind et al. [2019]  Lyzinski et al. [2014]  where in
contrast to this paper  the focus is on aligning the vertices of a partially aligned pair of graphs.

2 Graph Model
Consider two1 undirected graphs G1 (cid:44) (V  E1) and G2 (cid:44) (V  E2)  such that the graphs are formed
by the same set of vertices V (cid:44) {1  . . .   p} but have distinct sets of edges  denoted by E1 ⊆ V × V
and E2 ⊆ V × V . When there exists an edge between nodes u  v ∈ V in graph Gi  we denote it
by (u  v) ∈ Ei. Since the graphs are undirected  we have (u  v) = (v  u). We also deﬁne the set
Ni(u) ∈ V as the neighborhood of node u in graph Gi  i.e. 

Ni(u) (cid:44) {w ∈ V : (u  w) ∈ Ei} .

(1)
It is assumed that a pre-speciﬁed cluster of q nodes denoted by Vc ⊆ V have identical internal graph
structures in both G1 and G2. An example of two such graphical models is illustrated in Fig. 1.
We assume Ising models for both graphs G1 and G2  where we deﬁne X u
i ∈ X (cid:44) {−1  1} as the
random variable associated with the node j ∈ V in graph Gi  for i ∈ {1  2}. Accordingly  one sample
from graph Gi is given by the random vector Xi (cid:44) [X 1
i ]. The joint probability density
function (pdf) of Xi associated with the graph Gi is given by

i   . . .   X p

 (cid:88)

u v∈V

  

fi(Xi) =

1
Zi

exp

λuv
i X u

i X v
i

(2)

1The results in this paper can be generalized to settings with more than two graphs. For clarity  we analyze

the setting with two graphs.

3

where

and Zi is the partition function  given by

λuv
i =

(cid:26) λ 
(cid:88)

0 

if (u  v) ∈ Ei
otherwise

 

 (cid:88)

u v∈V

 .

(3)

(4)

Zi =

exp

λuv
i X u

i X v
i

Xi∈{−1 1}p

Throughout the rest of the paper  we refer to Xi as one graph sample. The parameter λ ∈ R+ deﬁned
in (3) captures the interdependency among the random variables associated with the vertices. We
remark that as λ grows or diminishes  i.e.  in the asymptote of large or small values of λ  it becomes
increasingly difﬁcult to distinguish the two distinct Ising models Santhanam and Wainwright [2012].
Finally  corresponding to graph Gi  we also deﬁne the maximum neighborhood weight according to
(5)

(cid:88)

.

λwu
i

ζi (cid:44) max
w∈V

u∈Ni(w)

3 Joint Structure Learning with Side Information

In this section  we formalize the notation of similar graphical models with partially identical structures 
the recovery criterion  and the associated performance measures.

3.1 Graph Similarity Model
Deﬁnition 1. Two graphs G1 and G2 with identical subgraphs with q nodes are said to be η−similar 
where η = q
p .
For given G1 and G2  the edges between a pair of nodes with at least one node not in Vc are assumed
to be structurally independent of each other. We denote the class of Ising models by I  and the class
of η−similar pairs of Ising models by Iη. In this paper  we focus on an edge-bounded sub-class of
Ising models deﬁned next.
Deﬁnition 2. This edge-bounded class of all the η−similar pair of graphs G1 and G2 is speciﬁed by
parameters k ∈ N and γ ∈ (0  1). The maximum number of edges in each graph is k and the number
of edges in the identical subgraphs is γk.

Note that in the deﬁnitions above the choices of γ and η are not independent. Clearly  for any

combination of k and p  γ should satisfy γk ≤(cid:0)q

(cid:1).

2

For convenience in notations  we also deﬁne ¯q (cid:44) p − q and ¯γ (cid:44) 1 − γ. It is also assumed that
the maximum neighborhood weight  deﬁned in (5)  is upper bounded by log ζ  i.e.  ζi ≤ log ζ 
for i ∈ {1  2}. Finally  we remark that all the results provided for the edge-bounded class have
counterparts for the degree-bounded class as well  which due to space limitations are omitted.

3.2 Recovery Criterion and Figure of Merit
The objective is to jointly estimate the structures of graphs G1 and G2 based on a collection of n
independent samples generated by each graph. The collection of n graph samples from the graph Gi
is denoted by Xn

i ∈ X n×p. We deﬁne the graph decoder

ψ : X n×p × X n×p → Iη  

(6)
as a function that maps the collection of samples to the graphs in class Iη. We assume that in
each recovered graph we can tolerate erroneous decisions about at most d number of edges  where
d is pre-speciﬁed. To capture the accuracy of such decisions  we deﬁne P(Iη  d) as the maximal
probability of error over the class Iη  i.e. 
P(Iη  d) (cid:44) max
G1 G2∈Iη

{|Ei∆ ˆEi|} ≥ d

max
i∈{1 2}

(cid:20)

(cid:21)

(7)

P

 

4

where |Ei∆ ˆEi| is the edit distance between Ei and the estimated edge structure ˆEi given by

(8)
Therefore  |Ei∆ ˆEi| represents the number of edges to be inserted or deleted to transform Ei to ˆEi.
Also  d represents the distortion level of the estimated graphs with respect to the true graphs.

|Ei∆ ˆEi| (cid:44) |(Ei\ ˆEi) ∪ ( ˆEi\Ei)| .

4 Sample Complexity: Main Results

In this section  we provide the sufﬁcient and necessary conditions on the sample size n for any graph
decoder to recover a pair of graphs with bounded probabilities of error. The necessary conditions
established are algorithm-independent and characterize the performance benchmarks on the sample
complexity for any designed algorithm. The sufﬁcient conditions determine the feasibility of graph
recovery under the proposed recover algorithm (ML decoding) under given decision reliability
constraints.
A summary of some of the main observations is provided in Table 1.

Table 1: Summary of the main results for recovering Ising models of class Iη.

Parameters

λ = O

k
k = O(p)

λ = O

(cid:17)
(cid:17)
(cid:17)
(cid:17)

k

k

(cid:16) 1√
(cid:16) 1√
(cid:16) 1√
(cid:16) 1

p

k = Ω(p) and k = O(p

4
3 )

λ = O

k = Ω(p

4
3 ) and k = O(p2)

λ = O

k ﬁxed and k ≤ p/4

Approx. recovery (d > 0)

Approx. recovery (d > 0)

(Necessary conditions)

(Sufﬁcient conditions)

Exact recovery (d = 0)
(Necessary conditions)

Ω(k log p)

Ω(k2 log p)

Ω(k log p)

Ω(k)

Ω( p2√

k

)

Ω(k2 log p)

Ω(k log p)

Ω(k2 log p)

Ω(k log p)

Ω(p2 log p)

Ω(p2 log p)

Ω(p2 log p)

4.1 Sufﬁcient Conditions

In order to establish sufﬁcient conditions  we adopt the ML graph decoder deﬁned as

( ˆG1  ˆG2) (cid:44) arg max

(G1 G2)∈Iη

fG1 G2 (Xn

1   Xn

2 ) .

(9)

The ML decoder is optimal under the exact recovery criterion  i.e.  when d = 0 Santhanam and
Wainwright [2012]. Under approximate recovery  however  no error is declared if the estimates
of the two graphs using the ML decoder lie within d distortion level of the true graphs. We use
large deviations analysis of the probability of error of (9) under approximate recovery to analyze its
performance.
Theorem 1 (Class Iη). Consider a pair of η−similar graphs G1 and G2 in class Iη. If the sample
size n satisﬁes

n ≥ r max{A1  2A2}  

where we have deﬁned

(cid:27)

5

 

r (cid:44) 3ζ 2 + 1
sinh2(λ/4)
(2k(cid:48) + γk) + log(2k(cid:48) − d) +2(k(cid:48) + 1) log p + log

4
δ
(2k(cid:48) + γk) + log(2γk − d) +2(γk + 1) log q + log

A1 (cid:44)(cid:104)
A2 (cid:44)(cid:104)

(cid:26)

k(cid:48) (cid:44) min

k 

¯q(¯q − 1)

2

+ q ¯q

 

(cid:21)

2
δ

 

(cid:21)

 

(10)

(11)

(12)

(13)

(14)

then there exists a graph decoder ψ : X n×p × X n×p → Iη that achieves P(Iη  d) ≤ δ.

Note that k(cid:48) deﬁned in (14) counts the maximum number of edges that can exist in the graphs after
excluding those in the shared identical subgraphs.
In order to gain more insight into the sufﬁcient condition in (10)  we evaluate the scaling behavior of
the sufﬁcient conditions for n in terms of parameters λ (parameter of Ising model in (3))  ζ (controls
maximum neighborhood bound)  and k (maximum number of edges in each graph). In all these
regimes  it is assumed that k is increasing with the graph size p. Furthermore  it can be readily
veriﬁed that d  i.e.  the number of errors tolerated by the decoder for the structure of each graph  does
not affect the asymptotic scaling behavior of the sample complexity.

1. λ = Θ(1): When the size of identical subgraphs dominates the sizes of non-identical parts

q (cid:28) 1 and γk (cid:29)(cid:0)¯q

(cid:1) + ¯qq  the sample complexity is dominated by 2rA2  which

such that ¯q
scales according to Ω(ζ 2k log p). Also  when we have k(cid:48) = k  the bound on the sample
complexity scales according to Ω(ζ 2k log p). Therefore  in this regime  under ﬁxed δ  the
bound on sample complexity is always dominated by a term that has a scaling behavior
given by Ω(ζ 2k log p).

2

√

√

2. λ = O(

k−1): By noting that sinh(λ/4) = Ω(λ)  in this regime  Theorem 1 implies that
there exists a constant c > 0 such that when n > c · ζ 2k2 log p  there always exists a graph
√
decoder that achieves P(Iη  d) ≤ δ . If ζ = O(exp(λ
k))  then the bound on sample
complexity scales as Ω(k2 log p) for ﬁxed δ.

3. λ = Θ(

k):

In this regime  when both λ and k are increasing with p  the bound
k) and
k = ω(log(log p))  the bound on sample complexity scales exponentially according to

√
on the sample complexity scales as Ω(ζ 2 log p). When we have ζ ≥ exp(λ
√
√
λ
exp(λ

k).

4.2 Necessary Conditions

For describing the results in this subsection  we denote the binary entropy function by

h(θ) (cid:44) −θ log θ − (1 − θ) log(1 − θ) 

(15)
Theorem 2 (Class Iη with k ≤ p/4). Consider a pair of η−similar graphs G1 and G2 in the class
2k . For any graph decoder ψ : X n×p × X n×p → Iη that achieves
Iη  such that  k ≤ p/4 and γ ≤ q
(16)

for θ ∈ (0  1) .

P(I k

η   d) ≤ δ  

for d = θk  for some θ ∈ (0  1

4 )  the sample size n should satisfy
n ≥ max{B1  B2} (1 − δ − o(1))  

where we have deﬁned

B1 (cid:44) 2(1 − γ) log ¯q + γ log q − 2θ log p
B2 (cid:44)

γ2 exp(−λ((cid:112)γk) − 1)/2) + ¯γ2 exp(−λ(

(1 − γ/2) log 2 − h(θ)

λ tanh λ

(cid:16)

3λk

 

(17)

(18)

(19)

(cid:17) .

√

¯γk − 1)/2)

Next  we discuss the different scaling behavior of the necessary conditions on sample complexity
from Theorem 2. Note that B1 and B2 have different scaling behavior in terms of λ  k and p. In all
the following regimes  we assume that k is increasing with p.

√

√
2. λ = O(

1. λ = Θ(1): In this regime  B1 scales as log p and B2 scales as e
the lower bound on the sample complexity if k = ω(log(log p)).

k. Clearly  B2 dominates

scales as 1/k. Clearly  the sample complexity is dominated by B1.

k−1): By noting that tanh(λ) = O(λ) we ﬁnd that B1 scales as Ω(k log p). B2
√

k) : In this regime  B1 scales as O( log p

/k1.5. If
we have k = ω(log(log p))  B2 dominates the sample complexity and scales exponentially
in k1.5.

λ tanh λ ) and B2 scales as ek1.5

3. λ = Θ(

6

Theorem 3 (Class Iη with k = Ω(p)). Consider a pair of η− similar graphs G1 and G2 in the class
4k }. For
Iη  such that  k = (cid:98)cp1+µ(cid:99) for given constants c > 0 and µ ∈ [0  1)  and γ ≤ min{η  η2p2
any graph decoder ψ : X n×p × X n×p → I k

η that achieves
P(Iη  d) ≤ δ  

for d = θk where θ ∈ (0  1

4 )  the sample size n should satisfy

n ≥ max{B3  B2} (1 − δ − o(1))  

where we have deﬁned B2 in (19)  and

B3 (cid:44) [(1 − γ/2) log 2 − h(θ)] · λ−1 exp(2λ) cosh(4λcpµ) + 1
exp(2λ) cosh(4λcpµ) − 1

.

(20)

(21)

(22)

To analyze the asymptotic scaling behavior of the necessary condition  we note that B2 depends on
λ and k  and B3 depends on λ and p. Therefore  depending on the variations of λ with respect to
p and k  we characterize the scaling behavior of the sufﬁcient condition in terms of k and p. In the
following regimes  we assume that k is increasing with p.

1. λ = Θ(1):

√
In this regime  B2 scales as exp(

√
2. λ = O(

k). On the other hand  we have
exp(2λ) cosh(4λcpµ)−1
exp(2λ) cosh(4λcpµ)+1 = Θ(1) and therefore  B3 = Θ(1) as p → ∞. Clearly  B2 dom-
inates the bound on sample complexity.
k−1):
√
In this regime  B2 scales as 1/k. The analysis of B3 shows that
exp(2λ) cosh(4λcpµ)−1
exp(2λ) cosh(4λcpµ)+1 = O(max{1/
k  k/p2}). Therefore  B3 scales according to
√
Ω(min{k  p2/
√
k). Note that when we have k = Ω(p) and k = O(p4/3)  we have
min{k  p2/
k} = k and therefore  the bound on sample complexity scales as Ω(k). When
k = Ω(p4/3)  the bound on the sample complexity scales as Ω(p2/
√

k): In this regime  B2 scales as ek1.5 and B3 → 0 as k → ∞. Therefore  the

k) asymptotically.

√

3. λ = Θ(

lower bound on sample complexity scales exponentially in k1.5.

The analysis of the results in Theorem 1 and Theorem 2 reveals that the sufﬁcient and the necessary
bounds on the sample complexity scale at the same rate (non-exponential) for the class Iη under a
particular regime  as described in Corollary 1.
Corollary 1 (Optimal Sample Complexity). When the maximum number of edges is ﬁxed and satisﬁes
k ≤ p/4  and we have

(cid:26) q

(cid:27)

γ ≤ min

 

η2p2
4k

2k

and

λ = O(1/p)  

(23)

Theorem 1 indicates that when n > c2p2 log p  for a constant c2  there exists a graph decoder that
recovers both graphs with P(Iη  d) ≤ δ. On the other hand  in this regime  Theorem 2 indicates that
for any graph decoder to achieve P(Iη  d) ≤ δ we should have n > c3p2 log p  for some constant
c3 > 0. Therefore  in this regime  the graph decoder that satisﬁes Theorem 1 achieves the optimal
sample complexity up to constant factors.

Furthermore  we comment that the extreme case of η = 0 corresponds to recovering two independent
graphs the other extreme case of η = 1 corresponds to recovering two identical graphs. In both these
extreme cases  the problem analyzed in this paper simpliﬁes to the problem of structure learning
of one graph studied in Scarlett and Cevher [2016] (for approximate recovery) and in Santhanam
and Wainwright [2012] (for exact recovery  i.e.  d = 0). In general  however  when we depart from
these special cases  the analysis techniques in the context of single graphs in existing literature do not
extend directly to the context of recovering a pair of graphs with structural similarity. Speciﬁcally 
we use novel ensemble constructions for a pair of graphs that accommodate structural similarity in
different regimes of k and analyze the pairwise KL divergences for the graph pairs to recover the
necessary conditions in Theorems 2 and 3. Moreover  our analysis of an ML decoder also recovers
the hitherto uninvestigated sufﬁcient conditions for the sample complexity under the approximate
recovery criterion. Hence  the results provided in this paper are completely different. This observation
is formalized in the following corollary.

7

Corollary 2 (Special Cases). The necessary and sufﬁcient conditions on sample complexity in the
extreme cases of η = 0 and η = 1 subsume the existing results for structure learning in single graphs.

In the context of the asymptotic scaling behaviors summarized in Table 1  we comment that the
necessary condition bounds for approximate recovery are not any looser with respect to that for exact
recovery than those in the context of single-graph recovery. Also  in some regimes the gap between
the necessary conditions and the sufﬁcient conditions on the sample complexity is tighter than others.
For instance  when k = O(p) the mismatch is only a factor k. The mismatch between the necessary
conditions and sufﬁcient conditions is more profound in denser graphs  i.e.  when k = Ω(p).
Furthermore  the analysis of the necessary conditions in Theorems 2 an 3 and sufﬁcient conditions in
Theorem 1 reveals that d does not affect the asymptotic scaling rate of their respective bounds even
when d scales as fast as linearly with k. For instance  in Theorem 1  d appears only in a logarithmic
factor scaling at most at the rate of log k which is dominated by k log p in A1 and A2. Therefore  the
results in Table 1 do not depend on d.

5 Numerical Evaluations
In this section  we evaluate the tradeoffs between decision reliability captured by P(Iη  d) deﬁned
in (7) and the necessary and sufﬁcient conditions on the sample complexities established. We
evaluated these tradeoffs for different approximate recovery levels controlled by d as well as similarity
levels of the two graphs speciﬁed by η. In general  the implementation of an ML decoder may become
infeasible as the size of the graphs grow. Therefore  to gain meaningful insights in the sample
complexity with increasing size of graphs  the evaluations were performed on an ensemble of graphs
that contains graphs with many isolated edges. In this ensemble  we set the size of the graphs to
p  with q = (cid:98)ηp(cid:99) nodes in the shared subgraph. We assumed that each graph contains α isolated
edges  with (cid:98)ηα(cid:99) edges lying in the shared subgraph. Furthermore  the graphs in this ensemble were
constructed in the following manner. We grouped the non-shared cluster with size (p − q) vertices in
(p − q)/2 ﬁxed pairs and randomly connected the vertices in (α − (cid:98)ηα(cid:99)). Similarly  the q vertices
of the shared subgraph were grouped into q/2 ﬁxed pairs and (cid:98)ηα(cid:99) pairs were selected randomly
to be connected. For this ensemble  the implementation of ML decoder can be readily shown to be
equivalent to a counting scheme that counts the number of agreements in the states of different nodes
in the data. This allowed us to visualize the behavior of the sample complexity for ML decoder as the
size of the graphs was increased.

Figure 2: Reliability (P(Iη  d)) versus sample
complexity (n) for different values of η and d.
Solid and dashed curves represent the sufﬁcient
number of samples (based on ML decoder) and
necessary number of samples  respectively.

Figure 3: Reliability (P(Iη  d)) versus graph size
(p) for different values of d  where d represents
the tolerance to distortion in the recovered graphs
with respect to the true graphs.

We ﬁrst considered a graph with p = 100 vertices and α = 20. Figure 2 depicts the variations of the
error probability P(Iη  d) versus n. For each value of P(Iη  d)  the ﬁgure speciﬁes the necessary
(shown by dashed curves) and sufﬁcient conditions (shown by solid curves) on the number of samples
n. The sufﬁcient conditions are obtained by simulations of the ML decoder. The ﬁgure shows

8

050100150200250Number of samples00.20.40.60.81Probability of error = 0.3  d = 3 = 0.3  d = 1 = 0.1  d = 3 = 0.1  d = 1 = 0.3  d = 3 = 0.3  d = 1 = 0.1  d = 3 = 0.1  d = 1100200300400500Size of graphs10-210-1100Probability of errord = 0d = 2d = 4these variations for different levels of graph similarity η = 0.1  0.3 and different values of recovery
approximation d = 1  3. The probability of error was evaluated empirically over 6000 trials.
In Corollary 1  we have provided a regime in which the scaling behaviors of the necessary and
sufﬁcient conditions on the sample complexity coincide  establishing the exact sample complexity. In
this regime  as a result  the ML decoder achieves an optimal structure learning rule. We used the ML
rule to characterize the variations of decision reliability P(Iη  d) as the size of the graph varied in the
range p ∈ [50  500] for ﬁxed number of edges. Figure 3 depicts these variations. For the results in
this ﬁgure we have ﬁxed α = 20 and η = 0.5  and have evaluated the performance based on n = 40
samples from each graph.
In Fig. 3  we observe that the decision reliability measure P(Iη  d) achieves lower error rate with
increase in d for the same number of samples. It is important to note that increase in d signiﬁes a
rise in tolerance to errors in the structure recovery by the graph decoder  and therefore  the decline in
quality of structure recovery decisions with respect to the ground truth. Also  as stated in Corollary 1 
we observe that graph recovery becomes more difﬁcult as the graph size increases while k remains
ﬁxed.

6 Conclusion

In this paper  we have considered the problem of structure learning in the presence of side information
about the structure. This is posed  naturally  as jointly recovering the structures of two graphs with
partial internal structural similarities. Speciﬁcally  it is assumed that both graphs share an identical
subgraph. Any inference about the structure of this subgraph from either of the graphs serves as
the side information for recovering the structure of the other graph. A general recovery criterion
that encompasses both exact and partial recovery of the graphs is considered. We have established
necessary (information-theoretic) and sufﬁcient (algorithmic) bounds on the sample complexity
for achieving a bounded probability of error in structure recovery. The scaling behaviors of these
conditions are analyzed in different regimes. We have also identiﬁed a regime in which the necessary
and sufﬁcient conditions coincide  establishing the optimal sample complexity. We have also provided
numerical evaluations to illustrate the interplay among the various parameters involved.
The setting studied in this paper has been motivated from applications in a broad range of domains
like social networks  genetics  and behavioral analysis. While the existing works have primarily
focused on context speciﬁc algorithmic frameworks for joint inference  our results have established
the information-theoretic benchmarks on the sample complexity in different regimes characterized by
the properties of the graph structures.

References
O. Banerjee  L. E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivariate Gaussian or binary data. Journal of Machine learning research  9:
485–516  Jun. 2008.

X. Chen  F. J. Slack  and H. Zhao. Joint analysis of expression proﬁles from multiple cancers
improves the identiﬁcation of microRNA–gene interactions. Bioinformatics  29(17):2137–2145 
2013.

D. M. Chickering. Learning Bayesian networks is NP-complete. Learning from data  112:121–130 

1996.

P. Danaher  P. Wang  and D. M. Witten. The joint graphical lasso for inverse covariance estimation
across multiple classes. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
76(2):373–397  Mar. 2014.

A. K. Das  P. Netrapalli  S. Sanghavi  and S. Vishwanath. Learning Markov graphs up to edit distance.
In IEEE International Symposium on Information Theory  pages 2731–2735  Cambridge  MA  Jul.
2012.

L. Devroye  A. Mehrabian  and T. Reddad. The minimax learning rate of Normal and Ising undirected

Graphical models. arXiv preprint arXiv:1806.06887  2018.

9

A. Dobra  C. Hans  B. Jones  J. R. Nevins  G. Yao  and M. West. Sparse graphical models for

exploring gene expression data. Journal of Multivariate Analysis  90(1):196–212  2004.

K. Dvijotham  M. Chertkov  P. V. Hentenryck  M. Vuffray  and S. Misra. Graphical models for

optimal power ﬂow. Constraints  22(1):24–49  2017.

J. Fang  L. S. Dongdong  S. Charles  Z. Xu  V. D. Calhoun  and Y.-P. Wang. Joint sparse canonical
correlation analysis for detecting differential imaging genetics modules. Bioinformatics  32(15):
3480–3488  2016.

D. E. Fishkind  S. Adali  H. G. Patsolic  L. Meng  D. Singh  V. Lyzinski  and C. E. Priebe. Seeded

graph matching. Pattern Recognition  87:203 – 215  Mar. 2019.

A. Gangrade  B. Nazer  and V. Saligrama. Lower bounds for two-sample structural change detection
in ising and Gaussian models. In Annual Allerton Conference on Communication  Control  and
Computing  pages 1016–1025  Allerton  IL  Oct. 2017.

J. Guo  E. Levina  G. Michailidis  and J. Zhu.

Biometrika  98(1):1–15  2011.

Joint estimation of multiple graphical models.

J. Guo  J. Cheng  E. Levina  G. Michailidis  and J. Zhu. Estimating heterogeneous graphical models
for discrete data with an application to roll call voting. The Annals of Applied Statistics  9(2):821 –
848  Jun. 2015.

Y. Jacob  L. Denoyer  and P. Gallinari. Learning latent representations of nodes for classifying in
heterogeneous social networks. In Proc. ACM international Conference on Web Search and Data
Mining  pages 373–382  New York  Feb. 2014.

S. L. Lauritzen. Graphical models  volume 17. Clarendon Press  May 1996.

V. Lyzinski  D. E. Fishkind  and C. E. Priebe. Seeded graph matching for correlated Erdös-Rényi

graphs. Journal of Machine Learning Research  15(1):3513–3540  Jan. 2014.

K. Mohan  P. London  M. Fazel  D. Witten  and S.-I. Lee. Node-based learning of multiple Gaussian

graphical models. The Journal of Machine Learning Research  15(1):445–488  2014.

M. Neykov and H. Liu. Property Testing in High Dimensional Ising models. arXiv preprint

arXiv:1709.06688  2017.

J. Pearl. Causality: models  reasoning  and inference. Oxford: Cambridge University Press  2009.

C. B. Peterson  F. C. Stingo  and M. Vannucci. Bayesian inference of multiple Gaussian graphical

models. Journal of the American Statistical Association  110(509):159–174  2015.

H. Qiu  F. Han  H. Liu  and B. Caffo. Joint estimation of multiple graphical models from high-
dimensional time series. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
78(2):487–504  2016.

P. Ravikumar  M. J. Wainwright  and J. D. Lafferty. High-dimensional Ising model selection using

l1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  Jun. 2010.

A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation.

Electronic Journal of Statistics  2:494–515  2008.

N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical

models in high dimensions. IEEE Trans. Information Theory  58(7):4117–4134  May 2012.

J. Scarlett and V. Cevher. On the difﬁculty of selecting Ising models with approximate recovery. IEEE

Transactions on Signal and Information Processing over Networks  2(4):625–638  Dec. 2016.

S. Sihag and A. Tajer. Sample complexity of joint structure learning.

In Proc. International
Conference on Acoustics  Speech and Signal Processing (ICASSP)  pages 5292–5296  Brighton 
UK  May 2019.

10

R. Tandon and P. Ravikumar. On the difﬁculty of learning power law graphical models. In Proc.
IEEE International Symposium on Information Theory  pages 2493–2497  Istanbul  Turkey  Jul.
2013.

R. Tandon  K. Shanmugam  P. K. Ravikumar  and A. G. Dimakis. On the information theoretic limits
of learning Ising models. In Proc. Advances in Neural Information Processing Systems  pages
2303–2311  Montreal  Canada  Dec. 2014.

D. Vats and J. M. Moura. Necessary conditions for consistent set-based graphical model selection. In
Proc. IEEE International Symposium on Information Theory  pages 303–307  Saint-Petersburg 
Russia  Jul. 2011.

W. Wang  M. J. Wainwright  and K. Ramchandran. Information-theoretic bounds on model selection
for Gaussian Markov random ﬁelds. In Proc. IEEE International Symposium on Information
Theory  Austin  Texas  Jun. 2010.

C. S. Won and H. Derin. Unsupervised segmentation of noisy and textured images using Markov

random ﬁelds. CVGIP: Graphical models and image processing  54(4):308–328  1992.

S. Yang  Z. Lu  X. Shen  P. Wonka  and J. Ye. Fused multiple graphical lasso. SIAM Journal on

Optimization  25(2):916–943  2015.

M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika 

94(1):19–35  2007.

11

,Saurabh Sihag
Ali Tajer