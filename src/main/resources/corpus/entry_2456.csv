2018,High Dimensional Linear Regression using Lattice Basis Reduction,We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector \beta^* from n noisy linear observations Y=X \beta^*+W  in R^n  for known X in R^{n \times p} and unknown W in R^n. Unlike most of the literature on this model we make no sparsity assumption on \beta^*. Instead we adopt a regularization based on assuming that the underlying vectors \beta^* have rational entries with the same denominator Q. We call this Q-rationality assumption.  We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm.  We establish that under the Q-rationality assumption  our algorithm recovers exactly the vector \beta^* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise  even when the learner has access to only one observation (n=1). Furthermore  we prove that in the case of the Gaussian white noise for W  n=o(p/\log p) and Q sufficiently large  our algorithm tolerates a nearly optimal information-theoretic level of the noise.,High Dimensional Linear Regression

using Lattice Basis Reduction

David Gamarnik

Sloan School of Management

Massachussetts Institute of Technology

Cambridge  MA 02139
gamarnik@mit.edu

Ilias Zadik

Operations Research Center

Massachussetts Institute of Technology

Cambridge  MA 02139

izadik@mit.edu

Abstract

We consider a high dimensional linear regression problem where the goal is to
efﬁciently recover an unknown vector β∗ from n noisy linear observations Y =
Xβ∗ + W ∈ Rn  for known X ∈ Rn×p and unknown W ∈ Rn. Unlike most
of the literature on this model we make no sparsity assumption on β∗. Instead
we adopt a regularization based on assuming that the underlying vectors β∗ have
rational entries with the same denominator Q ∈ Z>0. We call this Q-rationality
assumption. We propose a new polynomial-time algorithm for this task which
is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction
algorithm. We establish that under the Q-rationality assumption  our algorithm
recovers exactly the vector β∗ for a large class of distributions for the iid entries of
X and non-zero noise W . We prove that it is successful under small noise  even
when the learner has access to only one observation (n = 1). Furthermore  we
prove that in the case of the Gaussian white noise for W   n = o (p/ log p) and Q
sufﬁciently large  our algorithm tolerates a nearly optimal information-theoretic
level of the noise.

1

Introduction

We consider the following high-dimensional linear regression model. Consider n samples of a vector
β∗ ∈ Rp in a vector form Y = Xβ∗ + W for some X ∈ Rn×p and W ∈ Rn. Given the knowledge
of Y and X the goal is to infer β∗ using an efﬁcient algorithm and the minimum number n of samples
possible. Throughout the paper we call p the number of features  X the measurement matrix and W
the noise vector.
We focus on the high-dimensional case where n may be much smaller than p and p grows to inﬁnity 
a setting that has been very popular in the literature during the last years Chen et al. (2001)  Donoho
(2006)  Candes et al. (2006)  Foucart and Rauhut (2013)  Wainwright (2009). In this case  and under
no additional structural assumption  the inference task becomes impossible  even in the noiseless case
W = 0  as the underlying linear system becomes underdetermined. Most papers address this issue by
imposing a sparsity assumption on β∗  which refers to β∗ having only a limited number of non-zero
entries compared to its dimension Donoho (2006)  Candes et al. (2006)  Foucart and Rauhut (2013).
During the past decades  the sparsity assumption led to a fascinating line of research in statistics and
compressed sensing  which established  among other results  that several polynomial-time algorithms 
such as Basis Pursuit Denoising Scheme and LASSO  can efﬁciently recover a sparse β∗ with number
of samples much smaller than the number of features Candes et al. (2006)  Wainwright (2009) 
Foucart and Rauhut (2013). For example  it is established that if β∗ is constrained to have at most
k ≤ p non-zero entries  X has iid N (0  1) entries  W has iid N (0  σ2) entries and n is of the order

(cid:1)  then both of the mentioned algorithms can recover β∗  up to the level of the noise. Different

k log(cid:0) p

k

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

structural assumptions than sparsity have also been considered in the literature. For example  a recent
paper Bora et al. (2017) makes the assumption that β∗ lies near the range of an L-Lipschitz generative
model G : Rk → Rp and it proposes an algorithm which succeeds with n = O(k log L) samples.
A downside of all of the above results is that they provide no guarantee in the case n is much smaller

(cid:1). Consider for example the case where the components of a sparse β∗ are binary-valued 

than k log(cid:0) p

k

k

k

and X  W follow the Gaussian assumptions described above. Supposing that σ is sufﬁciently small 
it is a straightforward argument that even when n = 1  β∗ is recoverable from Y = (cid:104)X  β∗(cid:105) + W by
a brute-force method with probability tending to one as p goes to inﬁnity (whp). On the other hand 
for sparse and binary-valued β∗  the Basis Pursuit method in the noiseless case Donoho and Tanner
(2006) and the Basis Pursuit Denoising Scheme in the noisy case Gamarnik and Zadik (2017b) have
been proven to fail to recover a vector with the same support of β∗  with n = o(k log p) samples
Wainwright (2009). This failure to capture the complexity of the problem accurately enough for

been proven to fail to recover a binary β∗ with n = o(k log(cid:0) p
(cid:1)) samples. Furthermore  LASSO has
small sample sizes also lead to an algorithmic hardness conjecture for the regime n = o(k log(cid:0) p
(cid:1))

Gamarnik and Zadik (2017a)  Gamarnik and Zadik (2017b). While this conjecture still stands in
the general case  as we show in this paper  in the special case where β∗ is rational-valued and the
magnitude of the noise W is sufﬁciently small  the statistical computational gap can be closed and β∗
can be recovered even when n = 1.
The structural assumption we impose on β∗ is that its entries are rational numbers with denominator
equal to some ﬁxed positive integer value Q ∈ Z>0  something we refer to as the Q-rationality
assumption. Note that for any Q  this assumption is trivially satisﬁed by the binary-valued β∗
which was discussed above. The 1-rationality assumption corresponds to β∗ having integer entries 
which is well-motivated in practise. For example  this assumption appears frequently in the study of
global navigation satellite systems (GPS) and communications Hassibi and Boyd (1998)  Hassibi
and Vikalo (2002)  Brunel and Boutros (1999)  Borno (2011). In the ﬁrst reference the authors
propose a mixed linear/integer model of the form Y = Ax + Bz + W where z is an integer valued
vector corresponding to integer multiples of certain wavelength. Several examples corresponding to
regression models with integer valued regression coefﬁcients and zero noise (though not always in the
same model) are also discussed in the book Foucart and Rauhut (2013). In particular one application
is the so-called Single-Pixel camera. In this model a vector β corresponds to color intensities of
an image for different pixels and thus takes discrete values. The model assumes no noise  which
is one of the assumptions we adopt in our model  though the corresponding regression matrix has
i.i.d. +1/ − 1 Bernoulli entries  as opposed to a continuous distribution we assume. Two other
applications involving noiseless regression models found in the same reference are MRI imaging and
Radar detection.
A large body of literature on noiseless regression type models is a series of papers on phase retrieval.
Here the coefﬁcients of the regression vector β∗ and the entries of the regression matrix X are
complex valued  but the observation vector Y = Xβ∗ is only observed through absolute values. This
model has many applications  including crystallography  see Candes et al. (2015). The aforementioned
paper provides many references to phase retrieval model including the cases when the entries of β∗
have a ﬁnite support. We believe that our method can also be extended so that to model the case
where the entries of the regression vector have a ﬁnite support  even if irrationally valued  and the
entries of Y are only observed through their magnitude. In other words  we expect that the method of
the present paper applies to the phase retrieval problem at least in some of the cases and this is one of
the current directions we are exploring.
Noiseless regression model with integer valued regression coefﬁcients were also important in the
theoretical development of compressive sensing methods. Speciﬁcally  Donoho Donoho (2006) and
Donoho and Tanner Donoho and Tanner (2005) Donoho and Tanner (2006) Donoho and Tanner
(2009) consider a noiseless regression model of the form AB where A is a random (say Gaussian)
matrix and B is the unit cube [0  1]p. One of the goals of these papers was to count number of extreme
points of the projected polytope AB in order to explain the effectiveness of the linear programming
based methods. The extreme points of this polytope can only appear as projections of extreme points
of B which are all length-p binary vector  namely one deals with noiseless regression model with
binary coefﬁcients – an important special case of the model we consider in our paper.
In the Bayesian setting  where the ground truth β∗ is sampled according to a discrete distribution
Donoho et al. (2013) proposes a low-complexity algorithm which provably recovers β∗ with n = o(p)

2

y  x1  x2  . . .   xp ∈ Z>0 the goal is to ﬁnd a ∅ (cid:54)= S ⊂ [p] with y =(cid:80)

samples. This algorithm uses the technique of approximate message passing (AMP) and is motivated
by ideas from statistical physics Krzakala et al. (2012). Even though the result from Donoho et al.
(2013) applies to the general discrete case for β∗  it requires the matrix X to be spatially coupled  a
property that in particular does not hold for X with iid standard Gaussian entries. Furthermore the
required sample size for the algorithm to work is only guaranteed to be sublinear in p  a sample size
potentially much bigger than the information-theoretic limit for recovery under sufﬁciently small
noise (n = 1). In the present paper  where β∗ satisﬁes the Q-rationality assumption  we propose
a polynomial-time algorithm which applies for a large class of continuous distributions for the iid
entries of X  including the normal distribution  and provably works even when n = 1.
The algorithm we propose is inspired by the algorithm introduced in Lagarias and Odlyzko (1985)
which solves  in polynomial time  a certain version of the so-called Subset-Sum problem. To
be more speciﬁc  consider the following NP-hard algorithmic problem. Given p ∈ Z>0 and
i∈S xi when at least one such
set S is assumed to exist. Over 30 years ago  this problem received a lot of attention in the ﬁeld of
cryptography  based on the belief that the problem would be hard to solve in many “real" instances.
This would imply that several already built public key cryptosystems  called knapsack public key
cryptosystems  could be considered safe from attacks Lempel (1979)  Merkle and Hellman (1978).
This belief though was proven wrong by several papers in the early 80s  see for example Shamir
(1982). Motivated by this line of research  Lagarias and Odlyzko in Lagarias and Odlyzko (1985)  and
a year later Frieze in Frieze (1986)  using a cleaner and shorter argument  proved the same surprising
2 (1+)p2}
fact: if x1  x2  . . .   xp follow an iid uniform distribution on [2 1
for some  > 0 then there exists a polynomial-in-p time algorithm which solves the subset-sum
problem whp as p → +∞. In other words  even though the problem is NP-hard in the worst-case 
assuming a quadratic-in-p number of bits for the coordinates of x  the algorithmic complexity of the
typical such problem is polynomial in p. The successful efﬁcient algorithm is based on an elegant
application of a seminal algorithm in the computational study of lattices called the Lenstra-Lenstra-
Lovasz (LLL) algorithm  introduced in Lenstra et al. (1982). This algorithm receives as an input
a basis {b1  . . .   bm} ⊂ Zm of a full-dimensional lattice L and returns in time polynomial in m
2 (cid:107)z(cid:107)2  for all
and maxi=1 2 ... m log (cid:107)bi(cid:107)∞ a non-zero vector ˆz in the lattice  such that (cid:107)ˆz(cid:107)2 ≤ 2 m
z ∈ L \ {0}.
Besides its signiﬁcance in cryptography  the result of Lagarias and Odlyzko (1985) and Frieze (1986)
enjoys an interesting linear regression interpretation as well. One can show that under the iid uniform
in [2 1
i∈S xi whp
i = 1(i ∈ S)
as p tends to inﬁnity. Therefore if β∗ is the indicator vector of this unique set S  that is β∗
i = (cid:104)x  β∗(cid:105) where x := (x1  x2  . . .   xp). Furthermore
using only the knowledge of y  x as input to the Lagarias-Odlyzko algorithm we obtain a polynomial
in p time algorithm which recovers exactly β∗ whp as p → +∞. Written in this form  and given our
earlier discussion on high-dimensional linear regression  this statement is equivalent to the statement
that the noiseless high-dimensional linear regression problem with binary β∗ and X generated with
iid elements from Unif[2 1
] is polynomial-time solvable even with one sample (n = 1)  whp
as p grows to inﬁnity. The main focus of this paper is to extend this result to β∗ satisfying the
Q-rationality assumption  continuous distributions on the iid entries of X and non-trivial noise levels.

] assumption for x1  x2  . . .   xp  there exists exactly one set S with y =(cid:80)

] := {1  2  3  . . .   2 1

2 (1+)p2

2 (1+)p2

for i = 1  2  . . .   p  we have that y =(cid:80)

i xiβ∗

2 (1+)p2

Summary of the Results

We propose a polynomial time algorithm for high-dimensional linear regression problem and establish
a general result for its performance. We show that if the entries of X ∈ Rn×p are iid from an arbitrary
continuous distribution with bounded density and ﬁnite expected value  β∗ satisﬁes the Q-rationality
assumption  (cid:107)β∗(cid:107)∞ ≤ R for some R > 0  and W is either an adversarial vector with inﬁnity norm
at most σ or has iid mean-zero entries with variance at most σ2  then under some explicitly stated
assumption on the parameters n  p  σ  R  Q our algorithm recovers exactly the vector β∗ in time
which is polynomial in n  p  log( 1
σ )  log R  log Q  whp as p tends to inﬁnity. As a corollary  we
show that for any Q and R our algorithm can infer correctly β∗  when σ is at most exponential in

−(cid:0)p2/2 + (2 + p) log(QR)(cid:1)  even from one observation (n = 1). We show that for general n our
algorithm can tolerate noise level σ which is exponential in −(cid:0)(2n + p)2/2n + (2 + p/n) log(QR)(cid:1).

We complement our results with the information-theoretic limits of our problem. We show that in
the case of Gaussian white noise W   a noise level which is exponential in − p
n log(QR)  which is

3

σ ).

essentially the second part of our upper bound  cannot be tolerated. This allows us to conclude that
in the regime n = o (p/ log p) and RQ = 2ω(p) our algorithm tolerates the optimal information
theoretic level of noise.
The algorithm we propose receives as input real-valued data Y  X but importantly it truncates in
the ﬁrst step the data by keeping the ﬁrst N bits after zero of every entry. In particular  this allows
the algorithm to perform only ﬁnite-precision artihmetic operations. Here N is a parameter of our
algorithm chosen by the algorithm designer. For our recovery results it is chosen to be polynomial in
p and log( 1
A crucial step towards our main result is the extension of the Lagarias-Odlyzko algorithm Lagarias and
Odlyzko (1985)  Frieze (1986) to not necessarily binary  integer vectors β∗ ∈ Zp  for measurement
matrix X ∈ Zn×p with iid entries not necessarily from the uniform distribution  and ﬁnally  for
non-zero noise vector W . As in Lagarias and Odlyzko (1985) and Frieze (1986)  the algorithm we
construct depends crucially on building an appropriate lattice and applying the LLL algorithm on it.
There is though an important additional step in the algorithm presented in the present paper compared
with the algorithm in Lagarias and Odlyzko (1985) and Frieze (1986). The latter algorithm is proven
to recover a non-zero integer multiple λβ∗ of the underlying binary vector β∗. Then since β∗ is
known to be binary  the exact recovery becomes a matter of renormalizing out the factor λ from every
non-zero coordinate. On the other hand  even if we establish in our case the corresponding result and
recover a non-zero integer multiple of β∗ whp  this last renormalizing step would be impossible as
the ground truth vector is not assumed to be binary. We address this issue as follows. First we notice
that the renormalization step remains valid if the greatest common divisor of the elements of β∗ is
1. Under this assumption from any non-zero integer multiple of β∗  λβ∗ we can obtain the vector
itself by observing that the greatest common divisor of λβ∗ equals to λ  and computing λ by using
for instance the Euclid’s algorithm. We then generalize our recovery guarantee to arbitrary β∗. We
do this by ﬁrst translating implicitly the vector β∗ with a random integer vector Z via translating our
observations Y = Xβ∗ + W by XZ to obtain Y + XZ = X(β∗ + Z) + W . We then prove that the
elements of β∗ + Z have greatest common divisor equal to unity with probability tending to one. This
last step is based on an analytic number theory argument which slightly extends a beautiful result from
probabilistic number theory (see for example  Theorem 332 in Hardy and Wright (1975)) according
π2   where P ⊥⊥ Q refers to P  Q
to which limm→+∞ PP Q∼Unif{1 2 ... m} P⊥⊥Q [gcd (P  Q) = 1] = 6
being independent random variables. This result is not of clear origin in the literature  but possibly
it is attributed to Chebyshev  as mentioned in Erdos and Lorentz (1985). A key implication of this
result for us is the fact that the limit above is strictly positive.

Given two vectors x  y ∈ Rd the Euclidean inner product notation is denoted by (cid:104)x  y(cid:105) :=(cid:80)d
linearly independent b1  . . .   bk ∈ Zk is deﬁned as {(cid:80)k

Notation
Let Z∗ denote Z \ {0}. For k ∈ Z>0 we set [k] := {1  2  . . .   k}. For a vector x ∈ Rd we
deﬁne Diagd×d (x) ∈ Rd×d to be the diagonal matrix with Diagd×d (x)ii = xi  for i ∈ [d]. For
1 ≤ p < ∞ by Lp we refer to the standard p-norm notation for ﬁnite dimensionall real vectors.
i=1 xiyi.
By log : R>0 → R we refer the logarithm with base 2. The lattice L ⊆ Zk generated by a set of
i=1 zibi|z1  z2  . . .   zk ∈ Z}. Throughout
the paper we use the standard asymptotic notation  o  O  Θ  Ω for comparing the growth of two
real-valued sequences an  bn  n ∈ Z>0.Finally  we say that a sequence of events {Ap}p∈N holds with
high probability (whp) as p → +∞ if limp→+∞ P (Ap) = 1.

2 Main Results

2.1 Extended Lagarias-Odlyzko algorithm
Let n  p  R ∈ Z>0. Given X ∈ Zn×p  β∗ ∈ (Z ∩ [−R  R])p and W ∈ Zn  set Y = Xβ∗ + W .
From the knowledge of Y  X the goal is to infer exactly β∗. For this task we propose the following
algorithm which is an extension of the algorithm in Lagarias and Odlyzko (1985) and Frieze (1986).
For realistic purposes the values of R (cid:107)W(cid:107)∞ is not assumed to be known exactly. As a result  the
following algorithm  besides Y  X  receives as an input a number ˆR ∈ Z>0 which is an estimated

4

upper bound in absolute value for the entries of β∗ and a number ˆW ∈ Z>0 which is an estimated
upper bound in absolute value for the entries of W .

Algorithm 1 Extended Lagarias-Odlyzko (ELO) Algorithm
Input: (Y  X  ˆR  ˆW )  Y ∈ Zn  X ∈ Zn×p  ˆR  ˆW ∈ Z>0.
Output: ˆβ∗ an estimate of β∗
1 Generate a random vector Z ∈ { ˆR + 1  ˆR + 2  . . .   2 ˆR + log p}p with iid entries uniform in

(cid:16) ˆR(cid:100)√

{ ˆR + 1  ˆR + 2  . . .   2 ˆR + log p}
2 Set Y1 = Y + XZ.
3 For each i = 1  2  . . .   n  if |(Y1)i| < 3 set (Y2)i = 3 and otherwise set (Y2)i = (Y1)i.
4 Set m = 2n+(cid:100) p
5 Output ˆz ∈ R2n+p from running the LLL basis reduction algorithm on the lattice generated by the
columns of the following (2n + p) × (2n + p) integer-valued matrix 

p(cid:101) + ˆW(cid:100)√
(cid:34) mX −mDiagn×n (Y2) mIn×n

n(cid:101)(cid:17)

.

2 (cid:101)+3p

(cid:35)

Am :=

Ip×p
0n×p

0p×n
0n×n

0p×n
In×n

(1)

6 Compute g = gcd (ˆzn+1  ˆzn+2  . . .   ˆzn+p)   using the Euclid’s algorithm.
7 If g (cid:54)= 0  output ˆβ∗ = 1

g (ˆzn+1  ˆzn+2  . . .   ˆzn+p)t − Z. Otherwise  output ˆβ∗ = 0p×1.

We explain here informally the steps of the (ELO) algorithm and brieﬂy sketch the motivation behind
each one of them. In the ﬁrst and second steps the algorithm translates Y by XZ where Z is a
random vector with iid elements chosen uniformly from { ˆR + 1  ˆR + 2  . . .   2 ˆR + log p}. In that
way β∗ is translated implicitly to β = β∗ + Z because Y1 = Y + XZ = X(β∗ + Z) + W . As we
will establish using a number theoretic argument  gcd (β) = 1 whp as p → +∞ with respect to the
randomness of Z  even though this is not necessarily the case for the original β∗. This is an essential
requirement for our technique to exactly recover β∗ and steps six and seven to be meaningful. In
the third step the algorithm gets rid of the signiﬁcantly small observations. The minor but necessary
modiﬁcation of the noise level affects the observations in a negligible way.
The fourth and ﬁfth steps of the algorithm provide a basis for a speciﬁc lattice in 2n + p dimensions.
The lattice is built with the knowledge of the input and Y2  the modiﬁed Y . The algorithm in step ﬁve
calls the LLL basis reduction algorithm to run for the columns of Am as initial basis for the lattice.
The fact that Y has been modiﬁed to be non-zero on every coordinate is essential here so that Am
is full-rank and the LLL basis reduction algorithm  deﬁned in Lenstra et al. (1982)  can be applied .
This application of the LLL basis reduction algorithm is similar to the one used in Frieze (1986) with
one important modiﬁcation. In order to deal here with multiple equations and non-zero noise  we
use 2n + p dimensions instead of 1 + p in Frieze (1986). Following though a similar strategy as in
Frieze (1986)  it can be established that the n + 1 to n + p coordinates of the output of the algorithm 
ˆz ∈ Z2n+p  correspond to a vector which is a non-zero integer multiple of β  say λβ for λ ∈ Z∗ 
w.h.p. as p → +∞.
The proof of the above result is an important part in the analysis of the algorithm and it is heavily
based on the fact that the matrix Am  which generates the lattice  has its ﬁrst n rows multiplied by the
“large enough" and appropriately chosen integer m which is deﬁned in step four. It can be shown that
this property of Am implies that any vector z in the lattice with “small enough" L2 norm necessarily
satisﬁes (zn+1  zn+2  . . .   zn+p) = λβ for some λ ∈ Z∗ whp as p → +∞. In particular  using
that ˆz is guaranteed to satisfy (cid:107)ˆz(cid:107)2 ≤ 2
2 (cid:107)z(cid:107)2 for all non-zero z in the lattice  it can be derived
that ˆz has a “small enough" L2 norm and therefore indeed satisﬁes the desired property whp as
p → +∞. Assuming now the validity of the gcd (β) = 1 property  step six ﬁnds in polynomial time
this unknown integer λ that corresponds to ˆz  because gcd (ˆzn+1  ˆzn+2  . . .   ˆzn+p) = gcd (λβ) = λ.
Finally step seven scales out λ from every coordinate and then subtracts the known random vector Z 
to output exactly β∗.
Of course the above is based on an informal reasoning. Formally we establish the following result.
Theorem 2.1. Suppose

2n+p

5

(1) X ∈ Zn×p is a matrix with iid entries generated according to a distribution D on Z which
2N probability on each element

for some N ∈ Z>0 and constants C  c > 0  assigns at most c
of Z and satisﬁes E[|V |] ≤ C2N   for V d= D;

(2) β∗ ∈ (Z ∩ [−R  R])p  W ∈ Zn;
(3) Y = Xβ∗ + W .

Suppose furthermore that ˆR ≥ R and

(cid:104)

N ≥ 1
2n

(2n + p)

2n + p + 10 log

(cid:16) ˆR

√

p + ((cid:107)W(cid:107)∞ + 1)

(cid:17)(cid:105)

√

n

+ 6 log ((1 + c) np) .

(cid:16) 1

(2)

(cid:17)

For any ˆW ≥ (cid:107)W(cid:107)∞ the algorithm ELO with input (Y  X  ˆR  ˆW ) outputs exactly β∗ w.p. 1−O
(whp as p → +∞) and terminates in time at most polynomial in n  p  N  log ˆR and log ˆW .
Remark 2.2. In the statement of Theorem 2.1 the only parameters that are assumed to grow to
inﬁnity are p and whichever other parameters among n  R (cid:107)W(cid:107)∞  N are implied to grow to inﬁnity
because of (2). Note in particular that n can remain bounded  including the case n = 1  if N grows
fast enough.
Remark 2.3. It can be easily checked that the assumptions of Theorem 2.1 are satisﬁed for n = 1 
2 } and W = 0. Under these assumptions 
N = (1 + ) p2
the Theorem’s implication is a generalization of the result from Lagarias and Odlyzko (1985) and
Frieze (1986) to the case β∗ ∈ {−1  0  1}p.

2   R = 1  D = Unif{1  2  3  . . .   2(1+) p2

np

2.2 Applications to High-Dimensional Linear Regression

The Model

We ﬁrst deﬁne the Q-rationality assumption.
Deﬁnition 2.4. Let p  Q ∈ Z>0. We say that a vector β ∈ Rp satisﬁes the Q-rationality assumption
if for all i ∈ [p]  β∗
Q   for some Ki ∈ Z.

i = Ki

The high-dimensional linear regression model we are considering is as follows.
Assumptions 1. Let n  p  Q ∈ Z>0 and R  σ  c > 0. Suppose

(1) measurement matrix X ∈ Rn×p with iid entries generated according to a continuous
distribution C which has density f with (cid:107)f(cid:107)∞ ≤ c and satisﬁes E[|V |] < +∞  where
V d= C;

(2) ground truth vector β∗ satisﬁes β∗ ∈ [−R  R]p and the Q-rationality assumption;
(3) Y = Xβ∗ + W for some noise vector W ∈ Rn. It is assumed that either (cid:107)W(cid:107)∞ ≤ σ or

W has iid entries with mean zero and variance at most σ2  depending on the context.

Objective: Based on the knowledge of Y and X the goal is to recover β∗ using an efﬁcient algorithm
and using the smallest number n of samples possible. The recovery should occur with high probability
(w.h.p)  as p diverges to inﬁnity.

The Lattice-Based Regression (LBR) Algorithm

As mentioned in the Introduction  we propose an algorithm to solve the regression problem  which
we call the Lattice-Based Regression (LBR) algorithm. The exact knowledge of Q  R (cid:107)W(cid:107)∞ is
not assumed. Instead the algorithm receives as an input  additional to Y and X  ˆQ ∈ Z>0 which is
an estimated multiple of Q  ˆR ∈ Z>0 which is an estimated upper bound in absolute value for the
entries of β∗ and ˆW ∈ R>0 which is an estimated upper bound in absolute value for the entries of
the noise vector W . Furthermore an integer number N ∈ Z>0 is given to the algorithm as an input 
which  as we will explain  corresponds to a truncation in the data in the ﬁrst step of the algorithm.

6

Algorithm 2 Lattice Based Regression (LBR) Algorithm
Input: (Y  X  N  ˆQ  ˆR  ˆW )  Y ∈ Zn  X ∈ Zn×p and N  ˆQ  ˆR  ˆW ∈ Z>0.
Output: ˆβ∗ an estimate of β∗

8 Set YN = ((Yi)N )i∈[n] and XN = ((Xij)N )i∈[n] j∈[p].
9 Set ( ˆβ1)∗ to be the output of the ELO algorithm with input:

2N ˆQYN   2N XN   ˆQ ˆR  2 ˆQ

2N ˆW + ˆRp

(cid:16)

(cid:17)(cid:17)

.

(cid:16)

10 Output ˆβ∗ = 1
ˆQ

( ˆβ1)∗.

(cid:98)2N|x|(cid:99)

2N   which corresponds to the operation of keeping

Given x ∈ R and N ∈ Z>0 let xN = sign(x)
the ﬁrst N bits after zero of a real number x.
We now explain informally the steps of the algorithm. In the ﬁrst step  the algorithm truncates each
entry of Y and X by keeping only its ﬁrst N bits after zero  for some N ∈ Z>0. This in particular
allows to perform ﬁnite-precision operations and to call the ELO algorithm in the next step which
is designed for integer input. In the second step  the algorithm naturally scales up the truncated
data to integer values  that is it scales YN by 2N ˆQ and XN by 2N . The reason for the additional
multiplication of the observation vector Y by ˆQ is necessary to make sure the ground truth vector β∗
can be treated as integer-valued. To see this notice that Y = Xβ∗ + W and YN   XN being “close" to
Y  X imply

2N ˆQYN = 2N XN ( ˆQβ∗) + “extra noise terms" + 2N ˆQW.

Therefore  assuming the control of the magnitude of the extra noise terms  by using the Q-rationality
assumption and that ˆQ is estimated to be a multiple of Q  the new ground truth vector becomes ˆQβ∗
which is integer-valued. The ﬁnal step of the algorithm consist of rescaling now the output of Step 2 
to an output which is estimated to be the original β∗. In the next subsection  we turn this discussion
into a provable recovery guarantee.

Recovery Guarantees for the LBR algorithm

We state now our main result  explicitly stating the assumptions on the parameters  under which the
LBR algorithm recovers exactly β∗ from bounded but adversarial noise W .
Theorem 2.5.A. Under Assumption 1 and assuming W ∈ [−σ  σ]n for some σ ≥ 0  the following
holds. Suppose ˆQ is a multiple of Q  ˆR ≥ R and

N >

(2n + p)

2n + p + 10 log ˆQ + 10 log

(3)
For any ˆW ≥ σ  the LBR algorithm with input (Y  X  N  ˆQ  ˆR  ˆW ) terminates with ˆβ∗ = β∗ w.p.
1 − O

(whp as p → +∞) and in time polynomial in n  p  N  log ˆR  log ˆW and log ˆQ.

+ 20 log(3 (1 + c) np)

2N σ + ˆRp

(cid:16) 1

(cid:17)

.

1
2

np

(cid:16)

(cid:16)

Applying Theorem 2.5.A we establish the following result handling random noise W .
Theorem 2.5.B. Under Assumption 1 and assuming W ∈ Rn is a vector with iid entries generating
according to an  independent from X  distribution W on R with mean zero and variance at most σ2
for some σ ≥ 0 the following holds. Suppose that ˆQ is a multiple of Q  ˆR ≥ R  and

1
2

(2n + p)

N >

(cid:17)
For any ˆW ≥ √
1 − O

(cid:16) 1

np

2n + p + 10 log ˆQ + 10 log

(4)
npσ the LBR algorithm with input (Y  X  N  ˆQ  ˆR  ˆW ) terminates with ˆβ∗ = β∗ w.p.

+ 20 log(3 (1 + c) np)

npσ + ˆRp

.

(whp as p → +∞) and in time polynomial in n  p  N  log ˆR  log ˆW and log ˆQ.

(cid:16)

2N√

(cid:16)

(cid:17)

(cid:17)

(cid:17)

(cid:17)

Noise tolerance of the LBR algorithm

The assumptions (2) and (4) might make it hard to build an intuition for the truncation level the LBR
algorithm provably works. For this reason  in this subsection we simplify it and state a Proposition

7

explicitly mentioning the optimal truncation level and hence characterizing the optimal level of noise
that the LBR algorithm can tolerate with n samples.
First note that in the statements of Theorem 2.5.A and Theorem 2.5.B the only parameters that
are assumed to grow are p and whichever other parameter is implied to grow because of (2) and
(4). Therefore  importantly  n does not necessarily grow to inﬁnity  if for example N  1
σ grow
appropriately with p. That means that Theorem 2.5.A and Theorem 2.5.B imply non-trivial guarantees
for arbitrary sample size n. The proposition below shows that if σ is at most exponential in −(1 +
for some  > 0  then for appropriately chosen truncation level N
)
the LBR algorithm recovers exactly the vector β∗ with n samples. In particular  with one sample

(cid:104) (p+2n)2
(n = 1) LBR algorithm tolerates noise level up to exponential in −(1 + )(cid:2)p2/2 + (2 + p) log(QR)(cid:3)

2n + (2 + p

n ) log (RQ)

(cid:105)

for some  > 0. On the other hand  if n = Θ(p) and log (RQ) = o(p)  the LBR algorithm tolerates
noise level up to exponential in −O(p).
Proposition 2.6. Under Assumption 1 and assuming W ∈ Rn is a vector with iid entries generating
(cid:21)
according to an  independent from X  distribution W on R with mean zero and variance at most σ2
(cid:17)
for some σ ≥ 0  the following holds. Suppose for some  > 0  σ ≤ 2
n ) log(RQ)
Then the LBR algorithm with input Y  X  ˆQ = Q  ˆR = R  ˆW∞ = 1  features p ≥ 300

(cid:16) 300

2n +(2+ p

−(1+)

(p+2n)2

log

(cid:20)

.

and N satisfying log(cid:0) 1

(cid:1) ≥ N ≥ (1 + )



(1+c)

  terminates with ˆβ∗ = β∗

(cid:104) (p+2n)2

2n + (2 + p

n ) log (RQ)

w.p. 1 − O

(whp as p → +∞) and in time polynomial in n  p  N  log ˆR  log ˆW and log ˆQ.

(cid:17)

(cid:16) 1

np

σ

(cid:105)

Information Theoretic Bounds

In this subsection  we discuss the maximum noise that can be tolerated information-theoretically in
recovering a β∗ ∈ [−R  R]p satisfying the Q-rationality assumption. We establish that under Gaussian
white noise  any successful recovery mechanism can tolerate noise level at most exponentially small
in − [p log (QR) /n].
Proposition 2.7. Suppose that X ∈ Rn×p is a vector with iid entries following a continuous
distribution D with E[|V |] < +∞  where V d= D  β∗ ∈ [−R  R]p satisﬁes the Q-rationality
assumption  W ∈ Rn has iid N (0  σ2) entries and Y = Xβ∗ + W . Suppose furthermore that
2 . Then there is no mechanism which  whp as p → +∞  recovers

σ > R(np)3(cid:16)

(cid:17)− 1

− 1

2p log(2QR+1)

2

n

exactly β∗ with knowledge of Y  X  Q  R  σ. That is  for any ˆβ∗ = ˆβ∗ (Y  X  Q  R  σ) we have

P(cid:16) ˆβ∗ = β∗(cid:17)

< 1.

lim sup
p→+∞

Sharp Optimality of the LBR Algorithm

Using Propositions 2.6 and 2.7 the following sharp result is established.
Proposition 2.8. Under Assumptions 1 where W ∈ Rn is a vector with iid N (0  σ2) entries the
following holds. Suppose that n = o
and
 > 0:

and RQ = 2ω(p). Then for σ0 := 2− p log(RQ)

log p

n

(cid:16) p

(cid:17)

 then the w.h.p. exact recovery of β∗ from the knowledge of Y  X  Q  R  σ is

  then the w.h.p. exact recovery of β∗ from the knowledge of Y  X  Q  R  σ is

possible by the LBR algorithm.

• if σ > σ1−
0
impossible.
• if σ < σ1+

0

3 Synthetic Experiments

In this section we present an experimental analysis of the ELO and LBR algorithms.
ELO algorithm: We focus on p = 30 features sample sizes n = 1  n = 10 and n = 30  R = 100 and
zero-noise W = 0. Each entry of β∗ is iid Unif ({1  2  . . .   R = 100}). For 10 values of α ∈ (0  3) 

8

Unif(cid:0){1  2  3  . . .   2N}(cid:1) for N = p2

speciﬁcally α ∈ {0.25  0.5  0.75  1  1.3  1.6  1.9  2.25  2.5  2.75}  we generate the entries of X iid
2αn. For each combination of n  α we generate 20 independent
instances of inputs. We plot in Figure 1 the fractions of instances where the output of the ELO
algorithm outputs exactly β∗ and the average termination time of the algorithm.

Figure 1: Average performance and runtime of ELO over 20 instances with p = 30 features
and n = 1  10  30 samples.

Comments: First  we observe that importantly the algorithm recovers the vectors correctly on all
α < 1-instances with p = 30 features  even if our theoretical guarantees are only for large enough
p. Second  Theorem 2.1 implies that if N > (2n + p)2 /2n and large p  ELO recovers β∗  with
high probability. In the experiments we observe that indeed ELO algorithm works in that regime 
as then α = p2
2nN < 1. Also the experiments show that ELO works for larger values of α. Finally 
the termination time of the algorithm was on average 1 minute and worst case 5 minutes  granting it
reasonable for many applications.
LBR algorithm: We focus on p = 30 features  n = 10 samples  Q = 1 and R = 100. We generate
each entry of β∗ w.p. 0.5 equal to zero and w.p. 0.5  Unif ({1  2  . . .   R = 100}). We generate
the entries of X iid U (0  1) and of W iid U (−σ  σ) for σ ∈ {0  e−20  e−12  e−4}. We generate 20
independent instances for any combination of σ and truncation level N. We plot the fraction of
instances where the output of LBR algorithm is exactly β∗.

Figure 2: Average performance of LBR algorithm
for various noise and truncation levels.

Comments: The experiments show that  ﬁrst LBR works correctly in many cases for the moderate
value of p = 30 and second that there is indeed an appropriate tuned truncation level (2n + p)2/2n <
N < log (1/σ) for which LBR succeeds. The latter is in exact agreement with Proposition 2.6.

Acknowledgments

The authors would like to gratefully aknowledge the work of Patricio Foncea and Andrew Zheng
on performing the synthetic experiments for the ELO and LBR algorithms  as part of a project for a
graduate-level class at MIT  during Spring 2018.

9

References
Bora  A.  Jalal  A.  Price  E.  and Dimakis  A. G. (2017). Compressed sensing using generative
models. In Proceedings of the 34th International Conference on Machine Learning  ICML 2017 
pages 537–546.

Borno  M. A. (2011). Reduction in solving some integer least squares problems. arXiv Preprint.

Brunel  L. and Boutros  J. (1999). Euclidean space lattice decoding for joint detection in cdma
systems. In Proceedings of the 1999 IEEE Information Theory and Communications Workshop
(Cat. No. 99EX253).

Candes  E. J.  Eldar  Y. C.  Strohmer  T.  and Voroninski  V. (2015). Phase retrieval via matrix

completion. SIAM review  57(2):225–251.

Candes  E. J.  Romberg  J. K.  and Tao  T. (2006). Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics  59(8):1207–1223.

Chen  S. S.  Donoho  D. L.  and Saunders  M. A. (2001). Atomic decomposition by basis pursuit.

SIAM Rev.  43(1):129–159.

Cover  T. M. and Thomas  J. A. (2006). Elements of Information Theory (Wiley Series in Telecommu-

nications and Signal Processing). Wiley-Interscience.

Donoho  D. and Tanner  J. (2009). Observed universality of phase transitions in high-dimensional
geometry  with implications for modern data analysis and signal processing. Philosophical
Transactions of the Royal Society of London A: Mathematical  Physical and Engineering Sciences 
367(1906):4273–4293.

Donoho  D. L. (2006). Compressed sensing. IEEE Transactions on information theory  52(4):1289–

1306.

Donoho  D. L.  Javanmard  A.  and Montanari  A. (2013). Information-theoretically optimal com-
pressed sensing via spatial coupling and approximate message passing. IEEE Transactions on
Information Theory  59(11):7434–7464.

Donoho  D. L. and Tanner  J. (2005). Neighborliness of randomly projected simplices in high
dimensions. Proceedings of the National Academy of Sciences of the United States of America 
102(27):9452–9457.

Donoho  D. L. and Tanner  J. (2006). Counting faces of randomly-projected polytopes when then

projection radically lowers dimension.

Erdos  P. and Lorentz  G. (1985). On the probability that n and g(n) are relatively prime. Acta Arith. 

5:524–531.

Foucart  S. and Rauhut  H. (2013). A mathematical introduction to compressive sensing. Springer.

Frieze  A. M. (1986). On the lagarias-odlyzko algorithm for the subset sum problem. SIAM J.

Comput.  15:536–539.

Gamarnik  D. and Zadik  I. (2017a). High dimensional linear regression with binary coefﬁcients:

Mean squared error and a phase transition. Conference on Learning Theory (COLT).

Gamarnik  D. and Zadik  I. (2017b). Sparse high dimensional linear regression: Algorithmic barrier

and a local search algorithm.

Hardy  G. and Wright  E. (1975). An Introduction to the Theory of Numbers. Oxford Science

Publications  ﬁfth edition edition.

Hassibi  A. and Boyd  S. (1998). Integer parameter estimation in linear models with applications to

gps. IEEE Transactions on Signal Processing.

10

Hassibi  B. and Vikalo  H. (2002). On the expected complexity of integer least-squares problems. In

2002 IEEE International Conference on Acoustics  Speech  and Signal Processing.

Krzakala  F.  Mézard  M.  Sausset  F.  Sun  Y. F.  and Zdeborová  L. (2012). Statistical-physics-based

reconstruction in compressed sensing. Phys. Rev. X  2:021005.

Lagarias  J. C. and Odlyzko  A. M. (1985). Solving low-density subset sum problems. Journal of the

ACM (JACM)  32(1):229–246.

Lempel  A. (1979). Cryptology in transition. ACM Comput. Surv.  11(4):285–303.

Lenstra  A. K.  Lenstra  H. W.  and Lovász  L. (1982). Factoring polynomials with rational coefﬁcients.

Mathematische Annalen  261(4):515–534.

Merkle  R. and Hellman  M. (1978). Hiding information and signatures in trapdoor knapsacks. IEEE

Transactions on Information Theory  24(5):525–530.

Shamir  A. (1982). A polynomial time algorithm for breaking the basic merkle-hellman cryptosystem.

In 23rd Annual Symposium on Foundations of Computer Science (sfcs 1982)  pages 145–152.

Wainwright  M. J. (2009). Sharp thresholds for high-dimensional and noisy sparsity recovery using
constrained quadratic programming (lasso). IEEE transactions on information theory  55(5):2183–
2202.

11

,Ilias Zadik
David Gamarnik