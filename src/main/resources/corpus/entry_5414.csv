2018,Temporal alignment and latent Gaussian process factor inference in population spike trains,We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains  which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly  as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model  achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence  as well as trial-to-trial variation in trajectories. Finally  we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data  and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.,Temporal alignment and latent Gaussian process

factor inference in population spike trains

Lea Duncker & Maneesh Sahani

Gatsby Computational Neuroscience Unit

{duncker maneesh}@gatsby.ucl.ac.uk

University College London

London  W1T 4JG

Abstract

We introduce a novel scalable approach to identifying common latent structure in
neural population spike-trains  which allows for variability both in the trajectory
and in the rate of progression of the underlying computation. Our approach is
based on shared latent Gaussian processes (GPs) which are combined linearly  as
in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to
handle unbinned spike-train data by incorporating a continuous time point-process
likelihood model  achieving scalability with a sparse variational approximation.
Shared variability is separated into terms that express condition dependence  as
well as trial-to-trial variation in trajectories. Finally  we introduce a nested GP
formulation to capture variability in the rate of evolution along the trajectory. We
show that the new method learns to recover latent trajectories in synthetic data  and
can accurately identify the trial-to-trial timing of movement-related parameters
from motor cortical data without any supervision.

1

Introduction

Many computations in the brain are thought to be implemented by the dynamical evolution of activity
distributed across large neural populations. As simultaneous recordings of population activity have
become more common  the need has grown for analytic methods that can identify these dynamical
computational variables from the data. Where the computation is tightly coupled to an externally
measurable covariate — a stimulus or a movement  perhaps — such identiﬁcation is a simple matter
of exploiting linear or non-linear regression to describe a population encoding or decoding model.
However  when aspects of the computation are likely to reﬂect internal mental states  which may vary
substantially even when external covariates remain constant  the relevant variables must be uncovered
from neural data alone  most typically using a latent variable model [1–6].
However  most such methods fail to account properly for at least one key form of dynamical variability
— trial-to-trial differences in the timing of the computation. Such differences may be reﬂected in
behavioural variability  for instance in reaction times or movement durations [7]  or in varying
relationships between external events and neural ﬁring  for instance in sensory onset latencies [8]. In
some cases manual alignment to salient external events or behavioural time-course may be used to
reduce temporal misalignment [8  9]. However  just as with variability in the trajectories themselves 
temporal variations in purely internal states must ultimately be identiﬁed from neural data alone [10].
A particularly challenging problem is to build models that capture the variability both in the latent
trajectories underlying spiking population activity  and in the time-course with which such trajectories
are followed. Temporal misalignment in trajectories might be confused for variability in the trajectory
itself; while genuine variability in that trajectory makes alignment more difﬁcult. Indeed  previous
work has considered these problems separately. Algorithms like dynamic time warping (DTW) [11]

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

treat the time-series as observed and try to estimate an optimal alignment of each series. DTW and
related approaches may suffer in settings where observed data are noisy  though recent work has
begun to explore more robust [12–15]  or probabilistic alternatives for time-series alignment [16–18].
Furthermore  these approaches generally assume a Gaussian noise model and often make assumptions
that restrict applications to non-conjugate observation models. Only recently have studies considered
pairwise alignment of univariate [19–21]  or multivariate point-processes [22]. Overall  simultaneous
inference and temporal alignment of latent trajectories from multivariate point process observations –
such as a set of spike-times of simultaneously recorded neurons – is a relatively unexplored area of
research.
In this work  we develop a novel method to jointly infer shared latent structure directly from the
spiking activity of neural populations  allowing for (and identifying) trial-to-trial variations in the
timing of the latent processes. We make two main contributions.
First  we extend Gaussian Process Factor Analysis (GPFA) [2]  an algorithm that has been successfully
applied in the context of extracting time-varying low-dimensional latent structure from binned neural
population activity on single trials  to directly model the point-process structure of spiking activity.
We do so using a sparse variational approach  which both simpliﬁes the adoption of a non-conjugate
point-process likelihood  and signiﬁcantly improves on the scalability of the GPFA algorithm.
Second  we combine the sparse variational GPFA model with a second Gaussian process in a nested
model architecture. We exploit shared latent structure across trials  or groups of trials  in order to
disentangle variation in timing from variation in trajectories. This extension allows us to infer latent
time-warping functions that align trials in a purely unsupervised manner – that is  using population
spiking activity alone with no behavioural covariates. We apply our method to simulated data and
show that we can more accurately recover ﬁring rate estimates than related methods. Using neural data
from macaque monkeys performing a variable-delay centre-out reaching task  we demonstrate that
the inferred alignment is behaviourally meaningful and predicts reaction times with high accuracy.

2 Gaussian Process Factor Analysis

Gaussian Process Factor Analysis (GPFA) is a method for inferring latent structure and single-
trial trajectories in latent space that inﬂuence the ﬁring of a population of neurons [2]. Temporal
correlations in the high-dimensional neural population are modelled via a lower number of shared
latent processes xk(·)  k = 1  . . .   K  which linearly relate to a high-dimensional signal hhh(·) ∈ RN
in neural space. Thus  inter-trial variability in neural ﬁring that is shared across the population is
modelled via the evolution of the latent processes on a given trial.
In the GPFA model  a Gaussian process (GP) prior is placed over each latent process xk(·)  speciﬁed
via a mean function µk(·) and a covariance function  or kernel  κk(· ·). The extent and nature of the
temporal correlations is speciﬁed by κk(· ·) and governed via hyperparameters.
The classic GPFA model in [2] considers regularly sampled observations of hhh(t) that are corrupted by
axis-aligned Gaussian noise. Recent work has aimed to extend GPFA to a Poisson observation model
[23]. Here  hhh(t) is related to a piecewise-constant ﬁring rate of a Poisson process whose counting
process is observed as spike-counts falling into time bins of a ﬁxed width.
We can summarise the GPFA generative model for observations y(r)
general way by writing

n (ti) of neuron n on trial r in a

k (·) ∼ GP(µk(·)  κk(· ·))
K(cid:88)
x(r)
n (·) =
k (·) + dn
h(r)
n (ti) ∼ p(y(r)
y(r)

cn kx(r)
n (ti)|h(r)

k=1

n (ti))

for k = 1  . . .   K

for n = 1  . . .   N

for

i = 1  . . .   T

(1)

The cn k are weights for each latent and neuron that deﬁne a subspace mapping from low-dimensional
latent space to high-dimensional neural space and dn is a constant offset.
The widespread use of GPFA may be restricted by its poor scaling with time: Since time is evaluated
on an evenly spaced grid with T points  GPFA requires building and inverting a T × T covariance
matrix  leading to O(T 3) complexity. The intractability of performing exact inference in GPFA

2

In the
models with non-conjugate likelihood adds further complexity on top of this [23  24].
next section  we will outline how to improve the scalability of GPFA irrespective of the choice of
observation model via the use of inducing points.

3 Sparse Variational Gaussian Process Factor Analysis (svGPFA)

The framework of sparse variational GP approximations [25] has helped to overcome difﬁculties
associated with the scalability of GP methods to large sample sizes. It has since been applied to
diverse problems in GP inference  contributing to improvements of the scalability of GP methods to
large datasets and complex  potentially non-conjugate  applications [26–30].
A sparse variational extension of the GPFA model can be obtained by extending the work on additive
signal decomposition in [26]. The main idea is to augment the model in (1) by introducing inducing
points uuuk for each latent process k = 1  . . .   K. The inducing points uuuk represent function evaluations
of the kth latent GP at Mk input locations zzzk. A joint prior over the process xk(·) and its inducing
points can hence be written as

p(uuuk|zzzk) = N(cid:16)

uuuk|000  K(k)

zz

(cid:17)

p(xk(·)|uuuk) = GP(˜µk(·)  ˜κk(· ·))

(2)

where the GP prior over xk(·) is now conditioned on the inducing points with conditional mean and
covariance function

−1

˜µk(t) = κκκk(t  zzz)K(k)
zz

uuuk

˜κk(t  t(cid:48)) = κk(t  t(cid:48)) − κκκk(t  zzzk)K(k)

zz

−1

κκκk(zzzk  t(cid:48))

(3)

Here  κκκk(·  zzz) is a vector valued function taking a single input argument and consisting of evaluations
of the covariance function κk(· ·) at the input and inducing point locations zzzk; K(k)
zz is the Gram
matrix of κk(· ·) evaluated at the inducing point locations.
form q(uuu1:K  x1:K) =(cid:81)K
We follow [26] in choosing a factorised variational approximation for posterior inference of the
k=1 p(xk|uuuk)q(uuuk)  with Gaussian q(uuuk) = N (uuuk|mmmk  Sk). This choice
of posterior approximation makes it possible to derive a variational lower bound to the marginal
log-likelihood over the observed data Y = {yyy(r)
N(cid:88)
log p(Y) ≥ R(cid:88)

N }R
K(cid:88)

r=1 of the form

(cid:105) def

log p(yyy(r)

= F

q(uuu(r)

(cid:104)

(cid:104)

KL

(4)

k )(cid:107)p(uuu(r)
k )

r=1

n=1

E
q(h(r)
n )

where q(h(r)

n |h(r)
n )

1   . . .   yyy(r)

(cid:105) − R(cid:88)
neuron obtained from q(x) =(cid:82) p(x|uuu)q(uuu)duuu. q(h(r)
(cid:16)

n (t) and covariance function σ(r)

cn k κκκk( t   zzzk) K(k)
zz

function ν(r)

(cid:88)
(cid:88)

ν(r)
n (t) =

k + dn

n (t  t(cid:48)) are given by
−1

mmm(r)

(cid:16)

−1

r=1

k

k=1

c2
n k

κk(t  t(cid:48)) + κκκk(t  zzzk)

n (t  t(cid:48)) =
σ(r)

n ) is the variational distribution over the afﬁne transformation of the latents for the nth
n ) is a GP with additive structure. Its mean

K(k)
zz

S(r)
k K(k)

zz

−1 − K(k)

zz

κκκk(zzzk  t(cid:48))

−1(cid:17)

(cid:17)

(5)

k

The cost of evaluating this bound on the likelihood now scales linearly in the number of time points
T   with cubic scaling only in the number of inducing points. Maximising the lower bound F in (4)
allows for variational learning of the parameters in q(uuuk)  the kernel hyperparmeters  the inducing
point locations  and the model parameters describing the afﬁne transformation from latents to hns.

3.1 A continuous-time point-process observation model

The form of the variational lower bound in (4) makes it clear that including different observation
models only requires taking a Gaussian expectation of the respective log-likelihood. Importantly  the
inference approach is essentially decoupled from the locations of the observed data. This crucial

3

consequence of the inducing point approach makes it possible to move away from gridded  binned
data and fully exploit the power of GPs in continuous-time.
Previous work has used sparse variational GP approximations to infer the intensity of a univariate
GP modulated Poisson process [27]. Here  we extend this to the multivariate case by combining
the svGPFA model with a point-process likelihood. To do this  we relate the afﬁne transformation
of the latent processes for the nth neuron  hn(·)  to the non-negative intensity function of a point-
process  λn(·)  via a static non-linearity g : R → R+. Thus  the spike times of neuron n on trial r 
n = {t1  . . . tΦ(n r)}  are modelled as
ttt(r)

(cid:90) Tr

(cid:32)

−

(cid:104)

(cid:33) Φ(n r)(cid:89)

(cid:105)

Φ(n r)(cid:88)

i=1

(cid:104)

(cid:105)

(cid:104)

(cid:105)

(cid:90) Tr

0

n |λ(r)

p(ttt(r)

(6)
Where λn(t) = g(hn(t))  Tr is the duration of the rth trial  and Φ(n  r) is the total spike-count of
neuron n on trial r. The expected log-likelihood term in (4) can be evaluated as

n ) = exp

λ(r)
n (ti)

dt λ(r)

n (t)

i=1

0

E
q(h(r)
n )

log p(ttt(r)

n |h(r)
n )

= −

E
q(h(r)
n )

g(h(r)

n (t))

dt+

E
q(h(r)
n )

log g(h(r)

n (ti))

(7)

The resulting expected log-likelihood in (7) still contains an integral over the expected rate function
of the neuron  which cannot be computed analytically. However  the integral is one-dimensional and
can be computed numerically using efﬁcient quadrature rules [31  32].
The svGPFA model with point-process likelihood already fully addresses the two major limitations of
the classic GPFA approach outlined in section 2: Firstly  it improves the scalability of the algorithm
via the use of inducing points  scaling cubically only in the number of inducing points per latent
and linearly in the total number of spiking events. Secondly  our approach appropriately models
neural spike trains as observations of a point-process. The model also provides the basis for further
extensions addressing temporal alignment across trials  which will be the focus of the following
section.

4 Temporal alignment and latent factor inference using Gaussian processes

The svGPFA model we have developed in section 3 aims to extract different latent trajectories on each
trial. It does not explicitly model any structure that is shared across trials  and each trial’s variable
time-course is simply captured via inter-trial differences in latents. In this section  we will extend
our basic svGPFA model in order to disentangle inter-trial variations in time-course from variations
in the latent trajectories themselves. To achieve this  we explicitly model latent structure that is
shared across trials or subsets of trials  as well as structure that is speciﬁc to each trial  and make
use of a nested GP architecture with time-warping functions. In this way  shared  neurally-deﬁned
latent structure provides an anchor for the temporal alignment across trials. We extend the previous
inducing point approach in order to arrive at a sparse variational inference algorithm in this setting.

4.1 A generative model for population spike times with grouped trial structure and variable

time courses

We introduce latent processes that are shared across all trials  across subsets of trials that share the
same experimental condition  or that are speciﬁc to each individual trial. We model each of these as
draws from a GP prior with K latent processes  L experimental conditions  and R trials:
for k = 1  . . .   K
for
for

shared:
condition-speciﬁc:
trial-speciﬁc:

k (· ·))
k (· ·))
k(· ·))

αk(·) ∼ GP(µα
k (·) ∼ GP(µβ
β((cid:96))
k (·) ∼ GP(µγ
γ(r)

k (·)  κα
k (·)  κβ
k(·)  κγ

r = 1  . . .   R

(cid:96) = 1  . . .   L

(8)

Allowing each of these latents to evolve in potentially separate subspaces  we deﬁne the linear
mapping from low-dimensional latent  to high-dimensional neural space to be of the form

n (·) =
h(r)

n kαk(·) + cβ
cα

n kβ(cid:96)(r)

k

(·) + cγ

n kγ(r)

k (·)

+ dn

(9)

K(cid:88)

(cid:16)

k=1

4

(cid:17)

(a)

k = 1  . . .   K

αk

(cid:96) = 1  . . .   L

β((cid:96))
k

t

τ (r)

γ(r)
k

Cα

Cβ

Cγ

hhh(r)

g(·)

λλλ(r)

{ttt(r)
n }N

n=1
r = 1  . . .   R

(b)

t

τ (r)

uuuτ (r)

k = 1  . . .   K

(cid:96) = 1  . . .   L

αk

uuuα
k

β((cid:96))
k

γ(r)
k

uuuβ ((cid:96))
k

uuuγ (r)
k

hhh(r)

λλλ(r)

{ttt(r)
n }N

n=1

r = 1  . . .   R

Figure 1: (a) generative model architecture. (b) the augmented model with inducing points.

where (cid:96)(r) is the condition of trial r. When these latents are evaluated on a canonical time-axis all
trials evolve according to the same time-course. To incorporate inter-trial variability in time-course
into our model  we include a second GP that acts to warp time differently on each trial:

τ (r)(·) ∼ GP(µτ (·)  κτ (· ·))

time-warping:

(10)
As before  we can relate hn(·) to the neuron’s ﬁring rate via a static non-linearity g(·)  and use
a point-process observation model for spike times. Thus  the ﬁring rate of neuron n on trial r is
described as λ(r)

n (τ (r)(t))). This generative model is summarised in Figure 1(a).

n (t) = g(h(r)

r = 1  . . .   R

for

4.2 Sparse variational inference using inducing points

The generative model introduced in section 4.1 is a two-layer GP  where one layer has additive
structure. Previous work in the GP literature has successfully applied inducing point approaches for
variational inference in nested GP architectures [33  34] (albeit without explicit additive decomposi-
tions within layers)  and we can take a similar approach here.
k (·)  and τ (r)(·) 
We introduce sets of inducing points for each of the latent processes αk(·)  β((cid:96))
  and uuuτ (r)  respectively. The augmented model using in-
which we will denote as uuuα
ducing points is summarised in Figure 1(b). For each of the latent processes ζ = {α  β  γ} we choose
a factorised approximating distribution of the form q({ζk  uuuζ
k) with
k). For the time-warping process  we choose q(τ  uuuτ ) = p(τ|uuuτ )q(uuuτ ) with
k  Sζ
q(uuuζ
q(uuuτ ) = N (uuuτ|mmmτ   Sτ ).
(cid:104)
Using this approximation  the variational lower bound to the marginal log-likelihood becomes

k=1|τ ) =(cid:81)K
(cid:105)

k=1 p(ζk|uuuζ

k (·)  γ(r)

k  τ )q(uuuζ

k   uuuβ ((cid:96))

k|mmmζ

  uuuγ (r)

k}K

(cid:104)

k

k

KL

q(uuuτ (r))(cid:107)p(uuuτ (r))

(cid:105) −(cid:88)
(cid:104)

KL

n |h(r)
n )

k )] −(cid:88)

E
q(h(r)
n )

log p(yyy(r)

KL [q(uuuα

k )(cid:107)p(uuuα

k) = N (uuuζ
(cid:88)
−(cid:88)

F =

r n

r
q(uuuβ ((cid:96))

k

)(cid:107)p(uuuβ ((cid:96))

k

)

(cid:105) −(cid:88)

(cid:104)

KL

q(uuuγ (r)

k

)(cid:107)p(uuuγ (r)

k

)

(cid:105)

k

(cid:96) k

r k

The mean and covariance function of the variational GP q(h(r)

n ) are given by

−1(cid:17)

Ψζ (r)

k 2 (t  zzzζ
k)

(11)

(cid:105)(cid:17)

(12)

(cid:88)
(cid:88)

ζ k

2(cid:16)

ν(r)
n (t) =

n k Ψζ (r)
cζ

k 1 (t  zzzζ

k) Kζ (k)

zz

mmmζ (r)

k + dn

σ(r)
n (t  t) =

cζ
n k

Ψζ (r)

k 0 (t) + Tr

Kζ (k)

zz

−1

Sζ (r)
k Kζ (k)

zz

−1 − Kζ (k)

zz

−1

(cid:104)(cid:16)

ζ k

5

The Ψζ (r)

k i (t  zzzζ

k) are the Ψ-statistics [33  35] of the kernel covariance functions

(cid:105)

(cid:105)

(13)

k 0 (t) = E
Ψζ (r)
k) = E
k) = E

k 1 (t  zzzζ
k 2 (t  zzzζ

Ψζ (r)

Ψζ (r)

q(τ (r))

q(τ (r))

q(τ (r))

(cid:105)

κκκζ
k( τ (r)(t)   τ (r)(t))
k( τ (r)(t)   zzzζ
κκκζ
k)
k  τ (r)(t))κκκζ
κκκζ
k(zzzζ

k(τ (r)(t)  zzzζ
k)

(cid:104)
(cid:104)
(cid:104)

The Ψ-statistics can be evaluated analytically for common kernel choices such as the linear  ex-
ponentiated quadratic  or cosine kernel. For other kernel choices they can be computed using
e.g. Gaussian quadrature. Finally  the variational distribution over the time-warping functions

q(τ (r)) =(cid:82) p(τ (r)|uuuτ (r))q(uuu(τ (r))duuu(τ (r) is also a GP with mean and covariance function
(cid:17)

ντ (r)(t) = κκκτ ( t   zzzτ ) Kτ
zz

(cid:16)

(14)

−1(cid:17)

στ (r)(t  t(cid:48)) = κτ (t  t(cid:48)) + κκκτ (t  zzzτ )

−1Sτ (r)Kτ

−1 − Kτ

zz

zz

−1 mmmτ (r)
Kτ
zz

κκκτ (zzzτ   t(cid:48))

.

The variational lower bound in (11) can thus be evaluated tractably and optimised with respect to all
parameters in the model. While the decomposition into shared  condition-speciﬁc and trial-speciﬁc
latents and the addition of the time-warping layer increases the total number of inducing points that
require optimisation  the impact on the time-complexity of the algorithm is minimal: it remains linear
in the total number of spikes across trials and neurons  and only cubic in the number of inducing
points per individual latent process.

5 Results

5.1 Synthetic data

We ﬁrst generate synthetic data for 100 neurons on 15 trials that are each one second in duration.
The neural activity is driven by one shared and one condition-speciﬁc latent process. We omit the
trial-speciﬁc latent process here  such that inter-trial variability is solely due to the variable time-
course of each trial and independent Poisson variability in the spiking of each neuron. The generative
procedure is illustrated in Figure 2 and aims to provide a toy simulation of decision-making  where
trial-to-trial differences in the decision-making process are modelled via the time-warping.

shared

time-warping

population ﬁring rates λ(t)

)
τ
(
α

)
τ
(

(cid:96)
β

condition-speciﬁc

)
t
(
τ

(cid:96) = 1

t

(cid:96) = 2

τ

)
t
(
α

)
t
(

(cid:96)
β

x
d
i

n
o
r
u
e
n

exp (·)

+

t
r
i
a
l
1

t
r
i
a
l

2

t

time

Figure 2: Synthetic data example: the shared latent reﬂects a gating signal; conditions are preferred
and anti-preferred for each neuron. Inter-trial differences in the decision-making process are modelled
via time warping. Warped latents are shown for two example trials per condition. The warped latents
are mixed linearly and passed through an exponential non-linearity to generate ﬁring rates  which
drive a Poisson process. Example spike rasters are superimposed over the population ﬁring rates for
two example trials of different conditions.

We ﬁrst investigate how our sparse variational inference approach compares to the Poisson GPFA
approach proposed in [23] (vLGP)  using published code1 which includes some further numerical
approximations for speed. Figure 3(a) shows that the svGPFA methods achieve an improved approxi-
mation at lower or comparable runtime cost. We next ﬁt the time-warped method (tw-pp-svGPFA)

1https://github.com/catniplab/vlgp

6

01-0.50101-1.501.50101201-0.50101-1.501.5(b)

(c)

(a)

]

l
l
u
f
q
(cid:107)
q
[
L
K
g
o
l

Figure 3: (a) runtime in seconds vs. approximation error of inference approach (under generative
parameters and matched kernel hyperparameters) of pp-svGPFA (solid) with different numbers
of inducing points (nZ) and vLGP (dashed). Approximation error is computed as the mean KL-
divergence between the respective marginal distributions q(hn(t)) and those of a full Gaussian
variational approximation over latent processes. (b) comparison of RMSE of inferred ﬁring rates
across different methods and bin widths. K = 2 for all methods. (c) relative runtime comparisons
across svGPFA methods.

with 5 inducing points for the shared and condition-speciﬁc latents  and 8 inducing points for the
time-warping latent. We use Gauss-Legendre quadrature with 80 abscissas for evaluating the log-
normaliser term in (7). We compare tw-pp-svGPFA with time-warped PCA [10] (twPCA)2  vLGP  a
latent linear dynamical systems model with Poisson observations (PLDS) [5]  and our point-process
(pp-) and Poisson (P-) svGPFA model without time-warping  but with all other settings chosen
equivalently. For the discrete-time methods  we bin the spike trains at different resolutions before
applying either method to the data.
Figure 3(b) shows the RMSE in inferred ﬁring rates of each method. The svGPFA variants achieve
the best posterior estimate of ﬁring rates throughout. The svGPFA models without time-warping
and vLGP have to ﬁt the temporal variability via inter-trial differences in the latent processes. The
PLDS model captures the discrete-time evolution of the latent using a linear dynamical operator
and an additive noise process  often referred to as the innovations. It is this innovations process that
allows for inter-trial variability in this case. Thus  neither of these competing approaches offers a
dissociation of the variations in time-course from other sources of inter-trial variability. In the case of
the PLDS  discrepancies between the inferred latent path  and a canonical path as predicted via the
learnt dynamical operator could in principle be used to estimate an alignment across trials. However 
innovations noise is typically modelled via a stationary parameter which restricts the capacity of such
an approach to capture more complex variations in timing. The twPCA approach does not assume
a noise model for the observations  which limits its ability to recover ﬁring rates and time-warped
structure underlying variable spike trains. This is especially true as spiking is relatively sparse in our
example. Figure 3(c) compares the relative runtime between the different svGPFA models.

5.2 Variable-delay centre-out reaching task

We apply tw-pp-svGPFA to simultaneously recorded spike trains of 105 neurons in dorsal premotor
and primary motor cortex of a macaque monkey (Macaca mulatta) during a variable-delay centre-out
reaching task [2].
In this task  the animal is presented with one of multiple possible target locations  but has to wait for a
variable duration before it receives the instruction to perform an arm-reach to the target. Beyond these
designed sources of inter-trial variability  there are also variations in reaction time and movement
kinematics  as well as extra variability in neural ﬁring.
We ﬁt our model to repeated trials from 5 different target directions (12 trials per condition). We
use 20 inducing points per latent  10 inducing points for the time-warping functions  and 100
Gauss-Legendre quadrature abscissas to evaluate the log-normaliser term in (7).

2https://github.com/ganguli-lab/twpca

7

-20246log(runtime)2610nZ = 5nZ = 10nZ = 20nZ = 50nZ = 100vLGPn.a.151020bin width in ms252025RMSEP-svGPFApp-svGPFAtw-pp-svGPFAvLGPPLDStwPCAn.a.151020bin width in ms158relative runtimeP-svGPFApp-svGPFAtw-pp-svGPFAFigure 4: Leave-one-neuron-out (LONO) cross-validation across
latent dimensionality on 15 held-out trials (3 per condition). Results
are shown for models with an exponential static non-linearity  with
and without the time-warping layer. The time-warped models achieve
a higher predictive log-likelihood than their non-warped analogues.

We perform model comparison across latent dimensionality by leave-one-neuron-out cross-validation
on held-out data [2]. To demonstrate the beneﬁt of time-warping  we compare the time-warped
models to their svGPFA analogues without a time-warping layer. Figure 4 shows that time-warping
results in a substantial increase in predictive log-likelihood on held-out data.
Figure 5(a) shows that the inferred time-warping functions automatically align the latent trajectories
to the onset of movement (MO). The model did not have access to this information and the alignment
is purely based on structure present in the neural population activity. Furthermore  the time passed
in warped time since the go-cue (GC) is highly predictive of reaction times  as is shown in Figure
5(b). We achieve an R2 value of 0.90  which is substantially higher than previously reported values
on the same dataset ([36] report R2 = 0.52 using a switching LDS model). Figure 5(c) shows a
reduction in the standard deviation of the times when the animal’s absolute hand position crosses a
threshold value. This reduction is not simply due to an overall compression of time  since the slope
of the time-warping functions during the movement period was 1.0572 on average across trials. This
demonstrates that the inferred time-warping provides an improved alignment of the behaviourally
measured arm reach compared to MO or GC alignment  which are common ways of aligning data in
reaching experiments [37].

(a)

(b)

(c)

y

x

Figure 5: (a) Inferred time-warping functions and behavioural events. τ (t) automatically aligns trials
to MO. Inset histogram compares the distribution of MO times in measured  and in warped time. (b)
leave-one-out cross-validated predictions of the reaction times  computed by determining a threshold
value for MO in warped time from all but one trials  and taking the time it takes to pass this threshold
from the GC on the held-out trial. (c) within-condition standard deviation (std.) in times when the
absolute hand position crosses a threshold under different methods of alignment. Inset illustrates
target directions and location of the threshold (85% of average reach distance). Grey bars show
average std. across target directions.

6 Conclusion

In this work  we have introduced a scalable sparse variational extension of GPFA  allowing one
to extract latent structure directly from the unbinned spike-time observations of simultaneously
recorded neurons. We further extended GPFA using an explicit separation of shared variability into
condition-dependent and trial-speciﬁc components within a nested GP architecture. This allowed
us to separate trial-to-trial variations in trajectories from variations in time-course. We arrived at a
scalable algorithm for posterior inference using inducing points.

8

151077.3LONO pred-loglik104warpednonwarped1231231.522.500.40.20.30.40.20.30.4 ( t )MOGC10203040Std. in crossing times in msUsing synthetic data  we showed that our svGPFA methods can more accurately recover ﬁring
rate estimates than related methods. Using neural data  we demonstrated that our time-warped
method infers a behaviourally meaningful alignment in a completely unsupervised fashion using only
information present in the neural population spike times. We showed that the inferred time-warping
is behaviourally meaningful  highly predictive of reaction times  and provides an improved temporal
alignment compared to manually aligning to behaviourally deﬁned time-points.
While we have focused on temporal variability in this work  we note that the inference scheme using
inducing points is more generally applicable for a variety of other nested GP models of interest
in neuroscience  such as the GP-LVM [35  38]. Similarly  further extensions of our model with
time-warping to deeper GP hierarchies – for instance using a non-linear mapping from latents to
observations as in the GP-LVM – can be incorporated straightforwardly.
The problem of latent input alignment in the context of the GP-LVM has also recently been considered
in [39]  where maximum a posteriori inference in a Gaussian model is used to recover a single true
underlying input sequence from multiple observations that are generated from different time-warping
functions. The sparse-variational approach we have presented here could hence be used to generalise
this approach to account for further differences across sequences  and extend it to non-Gaussian
settings.
Furthermore  our approach can also be applied to multiple sets of observed data with potentially
different likelihoods in a canonical correlations analysis extension of GPFA. This could be useful
for inferring latent structure from multiple recorded signals. For instance  one could combine the
analysis of LFP or calcium imaging data with that of spike trains. Thus  with slight modiﬁcations to
the model architecture  an analogous inference approach to the one presented here is applicable in a
variety of contexts  including manifold learning  combinations of modalities  and beyond.

Acknowledgements
We would like to thank Vincent Adam for early contributions to this project  Gergö Bohner for helpful
discussions  and the Shenoy laboratory at Stanford University for sharing the centre-out reaching
dataset. This work was funded by the Simons Foundation (SCGB 323228  543039; MS) and the
Gatsby Charitable Foundation.

References
[1] MM Churchland  BM Yu  M Sahani  and KV Shenoy. Techniques for extracting single-trial activity

patterns from large-scale neural recordings. Current Opinion in Neurobiology  17(5):609–618  2007.

[2] BM Yu  JP Cunningham  G Santhanam  SI Ryu  KV Shenoy  and M Sahani. Gaussian-process factor
analysis for low-dimensional single-trial analysis of neural population activity. Journal of Neurophysiology 
102(1):614–635  2009.

[3] Y Gao  EW Archer  L Paninski  and JP Cunningham. Linear dynamical neural population models through

nonlinear embeddings. In Advances in Neural Information Processing Systems  pp. 163–171  2016.

[4] C Pandarinath  DJ O’Shea  J Collins  R Jozefowicz  SD Stavisky  JC Kao  EM Trautmann  MT Kaufman 
SI Ryu  LR Hochberg  JM Henderson  KV Shenoy  LF Abbott  and D Sussillo. Inferring single-trial neural
population dynamics using sequential auto-encoders. Nature Methods  15(10):805–815  2018.

[5] JH Macke  L Buesing  and M Sahani. Estimating state and parameters in state space models of spike trains.

Advanced State Space Methods for Neural and Clinical Data  p. 137  2015.

[6] JP Cunningham and BM Yu. Dimensionality reduction for large-scale neural recordings. Nature Neuro-

science  17(11):1500–1509  2014.

[7] A Afshar  G Santhanam  BM Yu  SI Ryu  M Sahani  and KV Shenoy. Single-trial neural correlates of arm

movement preparation. Neuron  71(3):555–564  2011.

[8] S Kollmorgen and RH Hahnloser. Dynamic alignment models for neural coding. PLoS computational

biology  10(3):e1003508  2014.

[9] PN Lawlor  MG Perich  LE Miller  and KP Kording. Linear-nonlinear-time-warp-poisson models of neural

activity. Journal of Computational Neuroscience  2018.

[10] B Poole  A Williams  N Maheswaranathan  BM Yu  G Santhanam  S Ryu  SA Baccus  K Shenoy  and
S Ganguli. Time-warped PCA: simultaneous alignment and dimensionality reduction of neural data. In
Frontiers in Neuroscience. Computational and Systems Neuroscience (COSYNE)  Salt Lake City  UT  2017.

9

[11] H Sakoe and S Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE

Transactions on Acoustics  Speech  and Signal Processing  26(1):43–49  1978.

[12] EJ Keogh and MJ Pazzani. Derivative dynamic time warping. In Proceedings of the 2001 SIAM Interna-

tional Conference on Data Mining  pp. 1–11. SIAM  2001.

[13] E Hsu  K Pulli  and J Popovi´c. Style translation for human motion. In ACM Transactions on Graphics

(TOG)  vol. 24  pp. 1082–1089. ACM  2005.

[14] M Cuturi and M Blondel. Soft-DTW: a differentiable loss function for time-series. In Proceedings of the

34th International Conference on Machine Learning  pp. 894–903  2017.

[15] F Zhou and F De la Torre. Generalized canonical time warping. IEEE Transactions on Pattern Analysis

and Machine Intelligence  38(2):279–294  2016.

[16] M Lázaro-Gredilla. Bayesian warped Gaussian processes. In F Pereira  CJC Burges  L Bottou  and
KQ Weinberger  eds.  Advances in Neural Information Processing Systems 25  pp. 1619–1627. Curran
Associates  Inc.  2012.

[17] J Snoek  K Swersky  R Zemel  and R Adams. Input warping for Bayesian optimization of non-stationary
functions. In Proceedings of the 31st International Conference on Machine Learning  pp. 1674–1682 
2014.

[18] M Kaiser  C Otte  T Runkler  and CH Ek. Bayesian alignments of warped multi-output Gaussian processes.

arXiv preprint arXiv:1710.02766  2017.

[19] A Arribas-Gil and HG Müller. Pairwise dynamic time warping for event data. Computational Statistics &

Data Analysis  69:255–268  2014.

[20] VM Panaretos  Y Zemel  et al. Amplitude and phase variation of point processes. The Annals of Statistics 

44(2):771–812  2016.

[21] W Wu and A Srivastava. Estimating summary statistics in the spike-train space. Journal of Computational

Neuroscience  34(3):391–410  2013.

[22] Y Zemel and VM Panaretos. Fréchet means and procrustes analysis in wasserstein space. arXiv preprint

arXiv:1701.06876  2017.

[23] Y Zhao and IM Park. Variational latent Gaussian process for recovering single-trial dynamics from

population spike trains. Neural Computation  29(5):1293–1316  2017.

[24] M Opper and C Archambeau. The variational Gaussian approximation revisited. Neural Computation 

21(3):786–792  2009.

[25] MK Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of the

12th International Conference on Artiﬁcial Intelligence and Statistics  pp. 567–574  2009.

[26] V Adam  J Hensman  and M Sahani. Scalable transformed additive signal decomposition by non-conjugate
Gaussian process inference. In Proceedings of the 26th International Workshop on Machine Learning for
Signal Processing (MLSP)  2016.

[27] C Lloyd  T Gunter  MA Osborne  and SJ Roberts. Variational inference for Gaussian process modulated

Poisson processes. In Proceedings of the 32nd International Conference on Machine Learning  2015.

[28] J Hensman  N Fusi  and ND Lawrence. Gaussian processes for big data. In Conference on Uncertainty in

Artiﬁcial Intellegence  pp. 282–290. auai.org  2013.

[29] J Hensman  AGdG Matthews  and Z Ghahramani. Scalable variational Gaussian process classiﬁcation. In

Proceedings of the 18th International Conference on Artiﬁcial Intelligence and Statistics  2015.

[30] AD Saul  J Hensman  A Vehtari  and ND Lawrence. Chained Gaussian processes. In Proceedings of the

19th International Conference on Artiﬁcial Intelligence and Statistics  pp. 1431–1440  2016.

[31] G Mena and L Paninski. On quadrature methods for refractory point process likelihoods. Neural

Computation  26(12):2790–2797  2014.

[32] P Abbott. Tricks of the trade: Legendre-gauss quadrature. Mathematica Journal  9(4):689–691  2005.
[33] A Damianou and N Lawrence. Deep Gaussian processes.

In Proceedings of the 16th International

Conference on Artiﬁcial Intelligence and Statistics  pp. 207–215  2013.

[34] H Salimbeni and M Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In

Advances in Neural Information Processing Systems  pp. 4591–4602  2017.

[35] MK Titsias and ND Lawrence. Bayesian Gaussian process latent variable model. In Proceedings of the

13th International Conference on Artiﬁcial Intelligence and Statistics  pp. 844–851  2010.

[36] B Petreska  BM Yu  JP Cunningham  G Santhanam  SI Ryu  KV Shenoy  and M Sahani. Dynamical
segmentation of single trials from population neural data. In Advances in Neural Information Processing
Systems  pp. 756–764  2011.

10

[37] MM Churchland  JP Cunningham  MT Kaufman  JD Foster  P Nuyujukian  SI Ryu  and KV Shenoy.

Neural population dynamics during reaching. Nature  487(7405):51  2012.

[38] A Wu  NG Roy  S Keeley  and JW Pillow. Gaussian process based nonlinear latent structure discovery
in multivariate spike train data. In Advances in Neural Information Processing Systems  pp. 3499–3508 
2017.

[39] I Kazlauskaite  CH Ek  and ND Campbell. Gaussian process latent variable alignment learning. arXiv

preprint arXiv:1803.02603  2018.

11

,Lea Duncker
Maneesh Sahani