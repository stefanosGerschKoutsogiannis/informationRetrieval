2019,Hyperbolic Graph Convolutional Neural Networks,Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space  which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative  as it enables embeddings with much smaller distortion. However  extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations  such as feature transformation and aggregation  in hyperbolic space. Furthermore  since input features are often Euclidean  it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN)  the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free
graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure  and leads to improved performance when compared to Euclidean analogs  even with very low dimensional embeddings: compared to state-of-the-art GCNs  HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification  also improving state-of-the art on the Pubmed dataset.,Hyperbolic Graph Convolutional Neural Networks

Ines Chami∗‡

Rex Ying∗†

Christopher R´e†

Jure Leskovec†

†Department of Computer Science  Stanford University

‡Institute for Computational and Mathematical Engineering  Stanford University

{chami  rexying  chrismre  jure}@cs.stanford.edu

Abstract

Graph convolutional neural networks (GCNs) embed nodes in a graph into Eu-
clidean space  which has been shown to incur a large distortion when embedding
real-world graphs with scale-free or hierarchical structure. Hyperbolic geome-
try offers an exciting alternative  as it enables embeddings with much smaller
distortion. However  extending GCNs to hyperbolic geometry presents several
unique challenges because it is not clear how to deﬁne neural network operations 
such as feature transformation and aggregation  in hyperbolic space. Furthermore 
since input features are often Euclidean  it is unclear how to transform the features
into hyperbolic embeddings with the right amount of curvature. Here we propose
Hyperbolic Graph Convolutional Neural Network (HGCN)  the ﬁrst inductive
hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic
geometry to learn inductive node representations for hierarchical and scale-free
graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space
and map Euclidean input features to embeddings in hyperbolic spaces with different
trainable curvature at each layer. Experiments demonstrate that HGCN learns
embeddings that preserve hierarchical structure  and leads to improved performance
when compared to Euclidean analogs  even with very low dimensional embeddings:
compared to state-of-the-art GCNs  HGCN achieves an error reduction of up to
63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node
classiﬁcation  also improving state-of-the art on the Pubmed dataset.

1

Introduction

Graph Convolutional Neural Networks (GCNs) are state-of-the-art models for representation learning
in graphs  where nodes of the graph are embedded into points in Euclidean space [15  21  41  45].
However  many real-world graphs  such as protein interaction networks and social networks  often
exhibit scale-free or hierarchical structure [7  50] and Euclidean embeddings  used by existing GCNs 
have a high distortion when embedding such graphs [6  32]. In particular  scale-free graphs have
tree-like structure and in such graphs the graph volume  deﬁned as the number of nodes within some
radius to a center node  grows exponentially as a function of radius. However  the volume of balls in
Euclidean space only grows polynomially with respect to the radius  which leads to high distortion
embeddings [34  35]  while in hyperbolic space  this volume grows exponentially.
Hyperbolic geometry offers an exciting alternative as it enables embeddings with much smaller
distortion when embedding scale-free and hierarchical graphs. However  current hyperbolic embed-
ding techniques only account for the graph structure and do not leverage rich node features. For
instance  Poincar´e embeddings [29] capture the hyperbolic properties of real graphs by learning
shallow embeddings with hyperbolic distance metric and Riemannian optimization. Compared to

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Left: Poincar´e disk geodesics (shortest path) connecting x and y for different curvatures.
As curvature (−1/K) decreases  the distance between x and y increases  and the geodesics lines get
closer to the origin. Center: Hyperbolic distance vs curvature. Right: Poincar´e geodesic lines. x

deep alternatives such as GCNs  shallow embeddings do not take into account features of nodes  lack
scalability  and lack inductive capability. Furthermore  in practice  optimization in hyperbolic space
is challenging.
While extending GCNs to hyperbolic geometry has the potential to lead to more faithful embeddings
and accurate models  it also poses many hard challenges: (1) Input node features are usually Euclidean 
and it is not clear how to optimally use them as inputs to hyperbolic neural networks; (2) It is not
clear how to perform set aggregation  a key step in message passing  in hyperbolic space; And (3)
one needs to choose hyperbolic spaces with the right curvature at every layer of the GCN.
Here we solve the above challenges and propose Hyperbolic Graph Convolutional Networks
(HGCN)2  a class of graph representation learning models that combines the expressiveness of
GCNs and hyperbolic geometry to learn improved representations for real-world hierarchical and
scale-free graphs in inductive settings: (1) We derive the core operations of GCNs in the hyperboloid
model of hyperbolic space to transform input features which lie in Euclidean space into hyperbolic
embeddings; (2) We introduce a hyperbolic attention-based aggregation scheme that captures hierar-
chical structure of networks; (3) At different layers of HGCN we apply feature transformations in
hyperbolic spaces of different trainable curvatures to learn low-distortion hyperbolic embeddings.
The transformation between different hyperbolic spaces at different layers allows HGCN to ﬁnd the
best geometry of hidden layers to achieve low distortion and high separation of class labels. Our
approach jointly trains the weights for hyperbolic graph convolution operators  layer-wise curvatures
and hyperbolic attention to learn inductive embeddings that reﬂect hierarchies in graphs.
Compared to Euclidean GCNs  HGCN offers improved expressiveness for hierarchical graph data.
We demonstrate the efﬁcacy of HGCN in link prediction and node classiﬁcation tasks on a wide
range of open graph datasets which exhibit different extent of hierarchical structure. Experiments
show that HGCN signiﬁcantly outperforms Euclidean-based state-of-the-art graph neural networks
on scale-free graphs and reduces error from 11.5% up to 47.5% on node classiﬁcation tasks and
from 28.2% up to 63.1% on link prediction tasks. Furthermore  HGCN achieves new state-of-the-art
results on the standard PUBMED benchmark. Finally  we analyze the notion of hierarchy learned by
HGCN and show how the embedding geometry transforms from Euclidean features to hyperbolic
embeddings.

2 Related Work

The problem of graph representation learning belongs to the ﬁeld of geometric deep learning. There
exist two major types of approaches: transductive shallow embeddings and inductive GCNs.
Transductive  shallow embeddings. The ﬁrst type of approach attempts to optimize node embed-
dings as parameters by minimizing a reconstruction error. In other words  the mapping from nodes
in a graph to embeddings is an embedding look-up. Examples include matrix factorization [3  24]
and random walk methods [12  31]. Shallow embedding methods have also been developed in
hyperbolic geometry [29  30] for reconstructing trees [35] and graphs [5  13  22]  or embedding text

2Project website with code and data: http://snap.stanford.edu/hgcn

2

√Kxy0−11−10−9−8−7−6−1/K0.51.01.52.02.53.03.5d(x y)[39]. However  shallow (Euclidean and hyperbolic) embedding methods have three major downsides:
(1) They fail to leverage rich node feature information  which can be crucial in tasks such as node
classiﬁcation. (2) These methods are transductive  and therefore cannot be used for inference on
unseen graphs. And  (3) they scale poorly as the number of model parameters grows linearly with the
number of nodes.
(Euclidean) Graph Neural Networks. Instead of learning shallow embeddings  an alternative
approach is to learn a mapping from input graph structure as well as node features to embeddings 
parameterized by neural networks [15  21  25  41  45  47]. While various Graph Neural Network
architectures resolve the disadvantages of shallow embeddings  they generally embed nodes into a
Euclidean space  which leads to a large distortion when embedding real-world graphs with scale-free
or hierarchical structure. Our work builds on GNNs and extends them to hyperbolic geometry.
Hyperbolic Neural Networks. Hyperbolic geometry has been applied to neural networks  to prob-
lems of computer vision or natural language processing [8  14  18  38]. More recently  hyperbolic
neural networks [10] were proposed  where core neural network operations are in hyperbolic space.
In contrast to previous work  we derive the core neural network operations in a more stable model of
hyperbolic space  and propose new operations for set aggregation  which enables HGCN to perform
graph convolutions with attention in hyperbolic space with trainable curvature. After NeurIPS 2019
announced accepted papers  we also became aware of the concurrently developed HGNN model [26]
for learning GNNs in hyperbolic space. The main difference with our work is how our HGCN deﬁnes
the architecture for neighborhood aggregation and uses a learnable curvature. Additionally  while
[26] demonstrates strong performance on graph classiﬁcation tasks and provides an elegant extension
to dynamic graph embeddings  we focus on link prediction and node classiﬁcation.

3 Background

Problem setting. Without loss of generality we describe graph representation learning on a single
graph. Let G = (V E) be a graph with vertex set V and edge set E  and let (x0 E
)i∈V be d-
dimensional input node features where 0 indicates the ﬁrst layer. We use the superscript E to indicate
that node features lie in a Euclidean space and use H to denote hyperbolic features. The goal in graph
representation learning is to learn a mapping f which maps nodes to embedding vectors:

i

f : (V E  (x0 E

i

)i∈V ) → Z ∈ R|V|×d(cid:48)

 

where d(cid:48)
(cid:28) |V|. These embeddings should capture both structural and semantic information and can
then be used as input for downstream tasks such as node classiﬁcation and link prediction.
Graph Convolutional Neural Networks (GCNs). Let N (i) = {j : (i  j) ∈ E} denote a set of
neighbors of i ∈ V  (W (cid:96)  b(cid:96)) be weights and bias parameters for layer (cid:96)  and σ(·) be a non-linear
activation function. General GCN message passing rule at layer (cid:96) for node i then consists of:

i = W (cid:96)x(cid:96)−1 E
h(cid:96) E
i
x(cid:96) E
i = σ(h(cid:96) E
i +

(cid:88)

+ b(cid:96)

j∈N (i)

wijh(cid:96) E

j

)

(feature transform)
(neighborhood aggregation)

(1)
(2)

where aggregation weights wij can be computed using different mechanisms [15  21  41]. Message
passing is then performed for multiple layers to propagate messages over network neighborhoods.
Unlike shallow methods  GCNs leverage node features and can be applied to unseen nodes/graphs in
inductive settings.
The hyperboloid model of hyperbolic space. We review basic concepts of hyperbolic geometry
that serve as building blocks for HGCN. Hyperbolic geometry is a non-Euclidean geometry with a
constant negative curvature  where curvature measures how a geometric object deviates from a ﬂat
plane (cf. [33] for an introduction to differential geometry). Here  we work with the hyperboloid
model for its simplicity and its numerical stability [30]. We review results for any constant negative
curvature  as this allows us to learn curvature as a model parameter  leading to more stable optimization
(cf. Section 4.5 for more details).
Hyperboloid manifold. We ﬁrst introduce our notation for the hyperboloid model of hyperbolic
space. Let (cid:104).  .(cid:105)L : Rd+1 × Rd+1 → R denote the Minkowski inner product  (cid:104)x  y(cid:105)L := −x0y0 +

3

Figure 2: HGCN neighborhood aggregation (Eq. 9) ﬁrst maps messages/embeddings to the tangent
space  performs the aggregation in the tangent space  and then maps back to the hyperbolic space.

TxHd K := {v ∈ Rd+1 : (cid:104)v  x(cid:105)L = 0}.

Hd K := {x ∈ Rd+1 : (cid:104)x  x(cid:105)L = −K  x0 > 0}

in hyperbolic space and we denote ||v||L =(cid:112)

x1y1 + . . . + xdyd. We denote Hd K as the hyperboloid manifold in d dimensions with constant
negative curvature −1/K (K > 0)  and TxHd K the (Euclidean) tangent space centered at point x
(3)
Now for v and w in TxHd K  gK
x (v  w) := (cid:104)v  w(cid:105)L is a Riemannian metric tensor [33] and
(Hd K  gK
x ) is a Riemannian manifold with negative curvature −1/K. TxHd K is a local  ﬁrst-
order approximation of the hyperbolic manifold at x and the restriction of the Minkowski inner
product to TxHd K is positive deﬁnite. TxHd K is useful to perform Euclidean operations undeﬁned
Geodesics and induced distances. Next  we introduce the notion of geodesics and distances in
manifolds  which are generalizations of shortest paths in graphs or straight lines in Euclidean
geometry (Figure 1). Geodesics and distance functions are particularly important in graph embedding
algorithms  as a common optimization objective is to minimize geodesic distances between connected
nodes. Let x ∈ Hd K and u ∈ TxHd K  and assume that u is unit-speed  i.e. (cid:104)u  u(cid:105)L = 1  then we
(cid:17)
have the following result:
Proposition 3.1. Let x ∈ Hd K  u ∈ TxHd K be unit-speed. The unique unit-speed geodesic
γx→u(·) such that γx→u(0) = x  ˙γx→u(0) = u is γK
u 
and the intrinsic distance function between two points x  y in Hd K is then:

(cid:104)v  v(cid:105)L as the norm of v ∈ TxHd K.

x + √Ksinh

x→u(t) = cosh

(cid:16) t√

(cid:16) t√

(cid:17)

K

K

dKL (x  y) = √Karcosh(−(cid:104)x  y(cid:105)L/K).

x : TxHd K → Hd K assigns to v the point expK

(4)
Exponential and logarithmic maps. Mapping between tangent space and hyperbolic space is done
by exponential and logarithmic maps. Given x ∈ Hd K and a tangent vector v ∈ TxHd K  the
exponential map expK
x (v) := γ(1)  where γ is the
unique geodesic satisfying γ(0) = x and ˙γ(0) = v. The logarithmic map is the reverse map that
maps back to the tangent space at x such that logK
x (v)) = v. In general Riemannian manifolds 
these operations are only deﬁned locally but in the hyperbolic space  they form a bijection between
the hyperbolic space and the tangent space at a point. We have the following direct expressions of
the exponential and the logarithmic maps  which allow us to perform operations on points on the
hyperboloid manifold by mapping them to tangent spaces and vice-versa:
Proposition 3.2. For x ∈ Hd K  v ∈ TxHd K and y ∈ Hd K such that v (cid:54)= 0 and y (cid:54)= x  the
exponential and logarithmic maps of the hyperboloid model are given by:

x (expK

(cid:18) ||v||L√

(cid:19)

K

expK

x (v) = cosh

√

x +

Ksinh

(cid:19) v

(cid:18) ||v||L√

K

||v||L

 

logK

x (y) = dKL (x  y)

y + 1
||y + 1

K (cid:104)x  y(cid:105)Lx
K (cid:104)x  y(cid:105)Lx||L

.

4 Hyperbolic Graph Convolutional Networks

Here we introduce HGCN  a generalization of inductive GCNs in hyperbolic geometry that beneﬁts
from the expressiveness of both graph neural networks and hyperbolic embeddings. First  since input

4

features are often Euclidean  we derive a mapping from Euclidean features to hyperbolic space. Next 
we derive two components of graph convolution: The analogs of Euclidean feature transformation
and feature aggregation (Equations 1  2) in the hyperboloid model. Finally  we introduce the HGCN
algorithm with trainable curvature.

4.1 Mapping from Euclidean to hyperbolic spaces
HGCN ﬁrst maps input features to the hyperboloid manifold via the exp map. Let x0 E ∈ Rd denote
input Euclidean features. For instance  these features could be produced by pre-trained Euclidean
neural networks. Let o := {√K  0  . . .   0} ∈ Hd K denote the north pole (origin) in Hd K  which we
(cid:19) x0 E
(cid:19)
use as a reference point to perform tangent space operations. We have (cid:104)(0  x0 E)  o(cid:105) = 0. Therefore 
we interpret (0  x0 E) as a point in ToHd K and use Proposition 3.2 to map it to Hd K with:
||x0 E||2

||x0 E||2√K

||x0 E||2√K

o ((0  x0 E)) =

 √Ksinh

√Kcosh

x0 H = expK

(cid:18)

(cid:18)

(cid:19)

(cid:18)

.

(5)

4.2 Feature transform in hyperbolic space

The feature transform in Equation 1 is used in GCN to map the embedding space of one layer to
the next layer embedding space and capture large neighborhood structures. We now want to learn
transformations of points on the hyperboloid manifold. However  there is no notion of vector space
structure in hyperbolic space. We build upon Hyperbolic Neural Network (HNN) [10] and derive
transformations in the hyperboloid model. The main idea is to leverage the exp and log maps in
Proposition 3.2 so that we can use the tangent space ToHd K to perform Euclidean transformations.
Hyperboloid linear transform. Linear transformation requires multiplication of the embedding
vector by a weight matrix  followed by bias translation. To compute matrix vector multiplication  we
ﬁrst use the logarithmic map to project hyperbolic points xH to ToHd K. Thus the matrix representing
the transform is deﬁned on the tangent space  which is Euclidean and isomorphic to Rd. We then
project the vector in the tangent space back to the manifold using the exponential map. Let W be a
d(cid:48)

× d weight matrix. We deﬁne the hyperboloid matrix multiplication as:

W ⊗K xH := expK

o (·) is on Hd K and expK

(6)
o (·) maps to Hd(cid:48) K. In order to perform bias addition  we use a
where logK
result from the HNN model and deﬁne b as an Euclidean vector located at ToHd K. We then parallel
transport b to the tangent space of the hyperbolic point of interest and map it to the manifold. If
o→xH (·) is the parallel transport from ToHd(cid:48) K to TxH Hd(cid:48) K (c.f. Appendix A for details)  the
P K
hyperboloid bias addition is then deﬁned as:
(7)

o (W logK

o (xH )) 

xH (P K

o→xH (b)).

xH ⊕K b := expK

4.3 Neighborhood aggregation on the hyperboloid manifold

Mean aggregation in Euclidean GCN computes the weighted average(cid:80)

Aggregation (Equation 2) is a crucial step in GCNs as it captures neighborhood structures and features.
Suppose that xi aggregates information from its neighbors (xj)j∈N (i) with weights (wj)j∈N (i).
j∈N (i) wjxj. An analog of
mean aggregation in hyperbolic space is the Fr´echet mean [9]  which  however  has no closed form
solution. Instead  we propose to perform aggregation in tangent spaces using hyperbolic attention.
Attention based aggregation. Attention in GCNs learns a notion of neighbors’ importance and
aggregates neighbors’ messages according to their importance to the center node. However  attention
on Euclidean embeddings does not take into account the hierarchical nature of many real-world
networks. Thus  we further propose hyperbolic attention-based aggregation. Given hyperbolic
embeddings (xH
to the tangent space of the origin to compute
attention weights wij with concatenation and Euclidean Multi-layer Percerptron (MLP). We then
propose a hyperbolic aggregation to average nodes’ representations:
o (xH

wij = SOFTMAXj∈N (i)(MLP(logK

j )  we ﬁrst map xH

i and xH
j

o (xH

i   xH

j )))

(8)

AGGK(xH )i = expK
xH
i

(9)

(cid:19)
i )||logK
.

(xH
j )

(cid:18) (cid:88)

j∈N (i)

wijlogK
xH
i

5

(a) GCN layers.

(b) HGCN layers.

(c) GCN (left)  HGCN (right).

Figure 3: Visualization of embeddings for LP on DISEASE and NC on CORA (visualization on the
Poincar´e disk for HGCN). (a) GCN embeddings in ﬁrst and last layers for DISEASE LP hardly
capture hierarchy (depth indicated by color). (b) In contrast  HGCN preserves node hierarchies. (c)
On CORA NC  HGCN leads to better class separation (indicated by different colors).

Note that our proposed aggregation is directly performed in the tangent space of each center point
i   as this is where the Euclidean approximation is best (cf. Figure 2). We show in our ablation
xH
experiments (cf. Table 2) that this local aggregation outperforms aggregation in tangent space at the
origin (AGGo)  due to the fact that relative distances have lower distortion in our approach.
Non-linear activation with different curvatures. Analogous to Euclidean aggregation (Equation
2)  HGCN uses a non-linear activation function  σ(·) such that σ(0) = 0  to learn non-linear
transformations. Given hyperbolic curvatures −1/K(cid:96)−1 −1/K(cid:96) at layer (cid:96) − 1 and (cid:96) respectively 
we introduce a hyperbolic non-linear activation σ⊗K(cid:96)−1  K(cid:96) with different curvatures. This step is
crucial as it allows us to smoothly vary curvature at each layer. More concretely  HGCN applies the
Euclidean non-linear activation in ToHd K(cid:96)−1 and then maps back to Hd K(cid:96):

(10)
Note that in order to apply the exponential map  points must be located in the tangent space at the
north pole. Fortunately  tangent spaces of the north pole are shared across hyperboloid manifolds of
the same dimension that have different curvatures  making Equation 10 mathematically correct.

σ⊗K(cid:96)−1 K(cid:96) (xH ) = expK(cid:96)

o (σ(logK(cid:96)−1

(xH ))).

o

4.4 HGCN architecture

Having introduced all the building blocks of HGCN  we now summarize the model architecture.
Given a graph G = (V E) and input Euclidean features (x0 E)i∈V  the ﬁrst layer of HGCN maps
from Euclidean to hyperbolic space as detailed in Section 4.1. HGCN then stacks multiple hyperbolic
graph convolution layers. At each layer HGCN transforms and aggregates neighbour’s embeddings
in the tangent space of the center node and projects the result to a hyperbolic space with different
curvature. Hence the message passing in a HGCN layer is:

) ⊕K(cid:96)−1 b(cid:96)

(hyperbolic feature transform)
(attention-based neighborhood aggregation)

(11)
(12)

i = (W (cid:96) ⊗K(cid:96)−1 x(cid:96)−1 H
h(cid:96) H
i = AGGK(cid:96)−1(h(cid:96) H )i
y(cid:96) H
i = σ⊗K(cid:96)−1 K(cid:96) (y(cid:96) H
x(cid:96) H
)

i

i

(13)
where −1/K(cid:96)−1 and −1/K(cid:96) are the hyperbolic curvatures at layer (cid:96)−1 and (cid:96) respectively. Hyperbolic
embeddings (xL H )i∈V at the last layer can then be used to predict node attributes or links.
For link prediction  we use the Fermi-Dirac decoder [23  29]  a generalization of sigmoid  to compute
probability scores for edges:

(non-linear activation with different curvatures)

e(dKLL (xL H

i

j

i

j

) =

  xL H

p((i  j) ∈ E|xL H

(14)
where dKLL (· ·) is the hyperbolic distance and r and t are hyper-parameters. We then train HGCN by
minimizing the cross-entropy loss using negative sampling.
For node classiﬁcation  we map the output of the last HGCN layer to the tangent space of the origin
with the logarithmic map logKL
o (·) and then perform Euclidean multinomial logistic regression. Note
that another possibility is to directly classify points on the hyperboloid manifold using the hyperbolic
multinomial logistic loss [10]. This method performs similarly to Euclidean classiﬁcation (cf. [10]
for an empirical comparison). Finally  we also add a link prediction regularization objective in node
classiﬁcation tasks  to encourage embeddings at the last layer to preserve the graph structure.

 

(cid:105)−1

 xL H

)2−r)/t + 1

(cid:104)

6

Dataset
Hyperbolicity δ
Method

DISEASE

δ = 0

LP

NC

DISEASE-M

δ = 0

HUMAN PPI

δ = 1

AIRPORT

δ = 1

PUBMED
δ = 3.5

CORA
δ = 11

LP

NC

LP

NC

LP

NC

LP
-
-
-
-

NC
-
-
-
-

LP
-
-
-
-

NC
-
-
-
-

HNN[10]
GCN[21]
GAT [41]
SAGE [15]
SGC [44]

w EUC

o
l
l
a
h
S

HYP [29]
EUC-MIXED
HYP-MIXED

N MLP
N

32.5 ± 1.1
45.5 ± 3.3
35.2 ± 3.4
56.9 ± 1.5
28.8 ± 2.5
41.0 ± 1.8
69.7 ± 0.4
70.4 ± 0.4
69.1 ± 0.6
69.5 ± 0.2
74.5 ± 0.9
-13.8%

59.8 ± 2.0
63.5 ± 0.6
49.6 ± 1.1
55.1 ± 1.3
72.6 ± 0.6
75.1 ± 0.3
64.7 ±0.5
69.8 ±0.3
65.9 ± 0.3
65.1 ± 0.2
90.8 ± 0.3
-63.1%

23.8 ± 0.7
22.0 ± 1.5
46.1 ± 0.4
45.9 ± 0.3
51.5 ± 1.0
54.6 ± 0.4
81.3 ± 0.3
83.0 ± 0.7
77.9 ± 2.4
81.0 ± 0.1
79.9 ± 0.2
+18.2%
Table 1: ROC AUC for Link Prediction (LP) and F1 score for Node Classiﬁcation (NC) tasks. For
inductive datasets  we only evaluate inductive methods since shallow methods cannot generalize to
unseen nodes/graphs. We report graph hyperbolicity values δ (lower is more hyperbolic).

92.0 ± 0.0
94.5 ± 0.0
91.5 ± 0.1
93.3 ± 0.0
89.8 ± 0.5
90.8 ± 0.2
89.3 ± 0.4
90.5 ± 0.3
90.4 ± 0.5
89.8 ± 0.3
96.4 ± 0.1
-60.9%

48.2 ± 0.7
68.5 ± 0.3
63.0 ± 0.3
73.9 ± 0.2
72.4 ± 0.2
69.8 ± 0.4
78.1 ± 0.2
79.0 ± 0.3
77.4 ± 2.2
78.9 ± 0.0
80.3 ± 0.3
-6.2%

83.3 ± 0.1
87.5 ± 0.1
86.0 ± 1.3
83.8 ± 0.3
84.1 ± 0.9
94.9 ± 0.1
91.1 ± 0.5
91.2 ± 0.1
86.2 ± 1.0
94.1 ± 0.0
96.3 ± 0.0
-27.5%

60.9 ± 3.4
70.2 ± 0.1
68.3 ± 2.3
69.6 ± 0.1
68.6 ± 0.6
80.5 ± 0.5
81.4 ± 0.6
81.5 ± 0.3
82.1 ± 0.5
80.6 ± 0.1
90.6 ± 0.2
-47.5%

82.5 ± 0.3
87.6 ± 0.2
84.4 ± 0.2
85.6 ± 0.5
83.1 ± 0.5
89.0 ± 0.1
90.4 ± 0.2
93.7 ± 0.1
85.5 ± 0.6
91.5 ± 0.1
92.9 ± 0.1
+12.7%

55.3±0.4
59.3 ± 0.4
69.7 ± 0.3
70.5 ± 0.4
69.1 ± 0.3
71.3 ± 0.1
74.6 ± 0.3
-11.5%

55.3 ± 0.5
60.9 ± 0.4
66.0 ± 0.8
69.5 ± 0.4
67.4 ± 0.5
66.2 ± 0.2
78.1 ± 0.4
-28.2%

55.9 ± 0.3
56.2 ± 0.3
59.4 ± 3.4
62.5 ± 0.7
61.3 ± 0.4
60.5 ± 0.3
72.2 ± 0.5
-25.9%

67.8 ± 0.2
72.9 ± 0.3
77.0 ± 0.5
76.8 ± 0.4
78.1 ± 0.6
76.1 ± 0.2
84.5 ± 0.4
-29.2%

N
N
G

r
u
O

s HGCN

(%) ERR RED

4.5 Trainable curvature

We further analyze the effect of trainable curvatures in HGCN. Theorem 4.1 (proof in Appendix
B) shows that assuming inﬁnite precision  for the link prediction task  we can achieve the same
performance for varying curvatures with an afﬁne invariant decoder by scaling embeddings.
Theorem 4.1. For any hyperbolic curvatures −1/K −1/K(cid:48) < 0  for any node embeddings H =
{hi} ⊂ Hd K of a graph G  we can ﬁnd H(cid:48)
K hi}  such that the
reconstructed graph from H(cid:48) via the Fermi-Dirac decoder is the same as the reconstructed graph
from H  with different decoder parameters (r  t) and (r(cid:48)  t(cid:48)).

  H(cid:48) = {h(cid:48)

⊂ Hd K(cid:48)

(cid:113) K(cid:48)

i|h(cid:48)

i =

However  despite the same expressive power  adjusting curvature at every layer is important for good
performance in practice due to factors of limited machine precision and normalization. First  with
very low or very high curvatures  the scaling factor K(cid:48)
K in Theorem 4.1 becomes close to 0 or very
large  and limited machine precision results in large error due to rounding. This is supported by
Figure 4 and Table 2 where adjusting and training curvature lead to signiﬁcant performance gain.
Second  the norms of hidden layers that achieve the same local minimum in training also vary by a
factor of √K. In practice  however  optimization is much more stable when the values are normalized
[16]. In the context of HGCN  trainable curvature provides a natural way to learn embeddings of the
right scale at each layer  improving optimization. Figure 4 shows the effect of decreasing curvature
(K = +∞ is the Euclidean case) on link prediction performance.
5 Experiments

We comprehensively evaluate our method on a variety of networks  on both node classiﬁcation (NC)
and link prediction (LP) tasks  in transductive and inductive settings. We compare performance of
HGCN against a variety of shallow and GNN-based baselines. We further use visualizations to
investigate the expressiveness of HGCN in link prediction tasks  and also demonstrate its ability to
learn embeddings that capture the hierarchical structure of many real-world networks.

5.1 Experimental setup

Datasets. We use a variety of open transductive and inductive datasets that we detail below (more
details in Appendix). We compute Gromovs δ−hyperbolicity [1  28  17]  a notion from group theory
that measures how tree-like a graph is. The lower δ  the more hyperbolic is the graph dataset  and
δ = 0 for trees. We conjecture that HGCN works better on graphs with small δ-hyperbolicity.
1. Citation networks. CORA [36] and PUBMED [27] are standard benchmarks describing citation
networks where nodes represent scientiﬁc papers  edges are citations between them  and node
labels are academic (sub)areas. CORA contains 2 708 machine learning papers divided into 7
classes while PUBMED has 19 717 publications in the area of medicine grouped in 3 classes.

2. Disease propagation tree. We simulate the SIR disease spreading model [2]  where the label of a
node is whether the node was infected or not. Based on the model  we build tree networks  where

7

Method
HGCN

HGCN-ATTo
HGCN-ATT
HGCN-C

HGCN-ATT-C

DISEASE
78.4 ± 0.3
80.9 ± 0.4
82.0 ± 0.2
89.1 ± 0.2
90.8 ± 0.3

AIRPORT
91.8 ± 0.3
92.3 ± 0.3
92.5 ± 0.2
94.9 ± 0.3
96.4 ± 0.1

Figure 4: Decreasing curvature (−1/K) improves
link prediction performance on DISEASE.

Table 2: ROC AUC for link prediction on AIR-
PORT and DISEASE datasets.

node features indicate the susceptibility to the disease. We build transductive and inductive variants
of this dataset  namely DISEASE and DISEASE-M (which contains multiple tree components).

3. Protein-protein interactions (PPI) networks. PPI is a dataset of human PPI networks [37].
Each human tissue has a PPI network  and the dataset is a union of PPI networks for human tissues.
Each protein has a label indicating the stem cell growth rate after 19 days [40]  which we use
for the node classiﬁcation task. The 16-dimensional feature for each node represents the RNA
expression levels of the corresponding proteins  and we perform log transform on the features.

4. Flight networks. AIRPORT is a transductive dataset where nodes represent airports and edges
represent the airline routes as from OpenFlights.org. Compared to previous compilations [49]  our
dataset has larger size (2 236 nodes). We also augment the graph with geographic information
(longitude  latitude and altitude)  and GDP of the country where the airport belongs to. We use the
population of the country where the airport belongs to as the label for node classiﬁcation.

Baselines. For shallow methods  we consider Euclidean embeddings (EUC) and Poincar´e embeddings
(HYP) [29]. We conjecture that HYP will outperform EUC on hierarchical graphs. For a fair
comparison with HGCN which leverages node features  we also consider EUC-MIXED and HYP-
MIXED baselines  where we concatenate the corresponding shallow embeddings with node features 
followed by a MLP to predict node labels or links. For state-of-the-art Euclidean GNN models 
we consider GCN [21]  GraphSAGE (SAGE) [15]  Graph Attention Networks (GAT) [41] and
Simpliﬁed Graph Convolution (SGC) [44]3. We also consider feature-based approaches: MLP and
its hyperbolic variant (HNN) [10]  which does not utilize the graph structure.
Training. For all methods  we perform a hyper-parameter search on a validation set over initial
learning rate  weight decay  dropout4  number of layers  and activation functions. We measure
performance on the ﬁnal test set over 10 random parameter initializations. For fairness  we also
control the number of dimensions to be the same (16) for all methods. We optimize all models with
Adam [19]  except Poincar´e embeddings which are optimized with RiemannianSGD [4  48]. Further
details can be found in Appendix. We open source our implementation5 of HGCN and baselines.
Evaluation metric. In transductive LP tasks  we randomly split edges into 85/5/10% for training 
validation and test sets. For transductive NC  we use 70/15/15% splits for AIRPORT  30/10/60%
splits for DISEASE  and we use standard splits [21  46] with 20 train examples per class for CORA
and PUBMED. One of the main advantages of HGCN over related hyperbolic graph embedding is its
inductive capability. For inductive tasks  the split is performed across graphs. All nodes/edges in
training graphs are considered the training set  and the model is asked to predict node class or unseen
links for test graphs. Following previous works  we evaluate link prediction by measuring area under
the ROC curve on the test set and evaluate node classiﬁcation by measuring F1 score  except for
CORA and PUBMED  where we report accuracy as is standard in the literature.

5.2 Results

Table 1 reports the performance of HGCN in comparison to baseline methods. HGCN works
best in inductive scenarios where both node features and network topology play an important role.

3The equivalent of GCN in link prediction is GAE [20]. We did not compare link prediction GNNs based on

shallow embeddings such as [49] since they are not inductive.

4HGCN uses DropConnect [42]  as described in Appendix C.
5Code available at http://snap.stanford.edu/hgcn. We provide HGCN implementations for hyperboloid and
Poincar´e models. Empirically  both models give similar performance but hyperboloid model offers more stable
optimization  because Poincar´e distance is numerically unstable [30].

8

−3−2−101−log(K)0.50.60.70.80.9ROCAUCFigure 5: Attention: Euclidean GAT (left)  HGCN (right). Each graph represents a 2-hop neighbor-
hood of the DISEASE-M dataset.

The performance gain of HGCN with respect to Euclidean GNN models is correlated with graph
hyperbolicity. HGCN achieves an average of 45.4% (LP) and 12.3% (NC) error reduction compared
to the best deep baselines for graphs with high hyperbolicity (low δ)  suggesting that GNNs can
signiﬁcantly beneﬁt from hyperbolic geometry  especially in link prediction tasks. Furthermore 
the performance gap between HGCN and HNN suggests that neighborhood aggregation has been
effective in learning node representations in graphs. For example  in disease spread datasets  both
Euclidean attention and hyperbolic geometry lead to signiﬁcant improvement of HGCN over other
baselines. This can be explained by the fact that in disease spread trees  parent nodes contaminate
their children. HGCN can successfully model these asymmetric and hierarchical relationships with
hyperbolic attention and improves performance over all baselines.
On the CORA dataset with low hyperbolicity  HGCN does not outperform Euclidean GNNs  sug-
gesting that Euclidean geometry is better for its underlying graph structure. However  for small
dimensions  HGCN is still signiﬁcantly more effective than GCN even with CORA. Figure 3c shows
2-dimensional HGCN and GCN embeddings trained with LP objective  where colors denote the
label class. HGCN achieves much better label class separation.

5.3 Analysis

Ablations. We further analyze the effect of proposed components in HGCN  namely hyperbolic
attention (ATT) and trainable curvature (C) on AIRPORT and DISEASE datasets in Table 2. We
observe that both attention and trainable curvature lead to performance gains over HGCN with ﬁxed
curvature and no attention. Furthermore  our attention model ATT outperforms ATTo (aggregation
in tangent space at o)  and we conjecture that this is because the local Euclidean average is a better
approximation near the center point rather than near o. Finally  the addition of both ATT and C
improves performance even further  suggesting that both components are important in HGCN.
Visualizations. We ﬁrst visualize the GCN and HGCN embeddings at the ﬁrst and last layers in
Figure 3. We train HGCN with 3-dimensional hyperbolic embeddings and map them to the Poincar´e
disk which is better for visualization. In contrast to GCN  tree structure is preserved in HGCN 
where nodes close to the center are higher in the hierarchy of the tree. This way HGCN smoothly
transforms Euclidean features to Hyperbolic embeddings that preserve node hierarchy.
Figure 5 shows the attention weights in the 2-hop neighborhood of a center node (red) for the
DISEASE dataset. The red node is the node where we compute attention. The darkness of the color
for other nodes denotes their hierarchy. The attention weights for nodes in the neighborhood are
visualized by the intensity of edges. We observe that in HGCN the center node pays more attention
to its (grand)parent. In contrast to Euclidean GAT  our aggregation with attention in hyperbolic
space allows us to pay more attention to nodes with high hierarchy. Such attention is crucial to good
performance in DISEASE  because only sick parents will propagate the disease to their children.

6 Conclusion

We introduced HGCN  a novel architecture that learns hyperbolic embeddings using graph convolu-
tional networks. In HGCN  the Euclidean input features are successively mapped to embeddings in
hyperbolic spaces with trainable curvatures at every layer. HGCN achieves new state-of-the-art in
learning embeddings for real-world hierarchical and scale-free graphs.

9

Acknowledgments

Jure Leskovec is a Chan Zuckerberg Biohub investigator. This research has been supported in part by
DARPA under FA865018C7880 (ASED)  (MSC); NIH under No. U54EB020405 (Mobilize); ARO
under MURI; IARPA under No. 2017-17071900005 (HFC)  NSF under No. OAC-1835598 (CINES);
Stanford Data Science Initiative  Chan Zuckerberg Biohub  JD.com  Amazon  Boeing  Docomo 
Huawei  Hitachi  Observe  Siemens  and UST Global. We gratefully acknowledge the support of
DARPA under Nos. FA87501720095 (D3M)  FA86501827865 (SDH)  and FA86501827882 (ASED);
NIH under No. U54EB020405 (Mobilize)  NSF under Nos. CCF1763315 (Beyond Sparsity) 
CCF1563078 (Volume to Velocity)  and 1937301 (RTML); ONR under No. N000141712266
(Unifying Weak Supervision); the Moore Foundation  NXP  Xilinx  LETI-CEA  Intel  IBM  Microsoft 
NEC  Toshiba  TSMC  ARM  Hitachi  BASF  Accenture  Ericsson  Qualcomm  Analog Devices  the
Okawa Foundation  American Family Insurance  Google Cloud  Swiss Re  TOTAL  and members
of the Stanford DAWN project: Teradata  Facebook  Google  Ant Financial  NEC  VMWare  and
Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. Any opinions  ﬁndings  and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reﬂect the
views  policies  or endorsements  either expressed or implied  of DARPA  NIH  ONR  or the U.S.
Government.

References
[1] Aaron B Adcock  Blair D Sullivan  and Michael W Mahoney. Tree-like structure in large social
and information networks. In 2013 IEEE 13th International Conference on Data Mining  pages
1–10. IEEE  2013.

[2] Roy M Anderson and Robert M May. Infectious diseases of humans: dynamics and control.

Oxford university press  1992.

[3] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding
and clustering. In Advances in neural information processing systems  pages 585–591  2002.
[4] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control  2013.

[5] Benjamin Paul Chamberlain  James Clough  and Marc Peter Deisenroth. Neural embeddings of

graphs in hyperbolic space. arXiv preprint arXiv:1705.10359  2017.

[6] Wei Chen  Wenjie Fang  Guangda Hu  and Michael W Mahoney. On the hyperbolicity of

small-world and treelike random graphs. Internet Mathematics  9(4):434–491  2013.

[7] Aaron Clauset  Cristopher Moore  and Mark EJ Newman. Hierarchical structure and the

prediction of missing links in networks. Nature  453(7191):98  2008.

[8] Bhuwan Dhingra  Christopher J Shallue  Mohammad Norouzi  Andrew M Dai  and George E

Dahl. Embedding text in hyperbolic spaces. NAACL HLT  2018.

[9] Maurice Fr´echet. Les ´el´ements al´eatoires de nature quelconque dans un espace distanci´e. In

Annales de l’institut Henri Poincar´e  1948.

[10] Octavian Ganea  Gary B´ecigneul  and Thomas Hofmann. Hyperbolic neural networks. In

Advances in neural information processing systems  pages 5345–5355  2018.

[11] Octavian-Eugen Ganea  Gary Becigneul  and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In International Conference on Machine Learning  pages
1632–1641  2018.

[12] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.

In
Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and
data mining  pages 855–864. ACM  2016.

[13] Albert Gu  Frederic Sala  Beliz Gunel  and Christopher R´e. Learning mixed-curvature rep-
In International Conference on Learning Representations 

resentations in product spaces.
2019.

10

[14] Caglar Gulcehre  Misha Denil  Mateusz Malinowski  Ali Razavi  Razvan Pascanu  Karl Moritz
Hermann  Peter Battaglia  Victor Bapst  David Raposo  Adam Santoro  et al. Hyperbolic
attention networks. In International Conference on Learning Representations  2019.

[15] Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large

graphs. In Advances in Neural Information Processing Systems  pages 1024–1034  2017.

[16] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[17] Edmond Jonckheere  Poonsuk Lohsoonthorn  and Francis Bonahon. Scaled gromov hyperbolic

graphs. Journal of Graph Theory  2008.

[18] Valentin Khrulkov  Leyla Mirvakhabova  Evgeniya Ustinova  Ivan Oseledets  and Victor Lem-

pitsky. Hyperbolic image embeddings. arXiv preprint arXiv:1904.02239  2019.

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations  2015.

[20] Thomas N Kipf and Max Welling. Variational graph auto-encoders. NeurIPS Workshop on

Bayesian Deep Learning  2016.

[21] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. In International Conference on Learning Representations  2017.

[22] Robert Kleinberg. Geographic routing using hyperbolic space. In IEEE International Conference

on Computer Communications  2007.

[23] Dmitri Krioukov  Fragkiskos Papadopoulos  Maksim Kitsak  Amin Vahdat  and Mari´an Bogun´a.

Hyperbolic geometry of complex networks. Physical Review E  2010.

[24] Joseph B Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric

hypothesis. Psychometrika  1964.

[25] Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel. Gated graph sequence neural

networks. In International Conference on Learning Representations  2016.

[26] Qi Liu  Maximilian Nickel  and Douwe Kiela. Hyperbolic graph neural networks. In Advances

in Neural Information Processing Systems  2019.

[27] Galileo Namata  Ben London  Lise Getoor  Bert Huang  and UMD EDU. Query-driven active

surveying for collective classiﬁcation. 2012.

[28] Onuttom Narayan and Iraj Saniee. Large-scale curvature of networks. Physical Review E 

84(6):066108  2011.

[29] Maximillian Nickel and Douwe Kiela. Poincar´e embeddings for learning hierarchical represen-

tations. In Advances in neural information processing systems  pages 6338–6347  2017.

[30] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model
of hyperbolic geometry. In International Conference on Machine Learning  pages 3776–3785 
2018.

[31] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 701–710. ACM  2014.

[32] Erzs´ebet Ravasz and Albert-L´aszl´o Barab´asi. Hierarchical organization in complex networks.

Physical review E  2003.

[33] Joel W Robbin and Dietmar A Salamon. Introduction to differential geometry. ETH  Lecture

Notes  preliminary version  2011.

11

[34] Frederic Sala  Chris De Sa  Albert Gu  and Christopher R´e. Representation tradeoffs for
hyperbolic embeddings. In International Conference on Machine Learning  pages 4457–4466 
2018.

[35] Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In International

Symposium on Graph Drawing  2011.

[36] Prithviraj Sen  Galileo Namata  Mustafa Bilgic  Lise Getoor  Brian Galligher  and Tina Eliassi-

Rad. Collective classiﬁcation in network data. AI magazine  2008.

[37] Damian Szklarczyk  John H Morris  Helen Cook  Michael Kuhn  Stefan Wyder  Milan Si-
monovic  Alberto Santos  Nadezhda T Doncheva  Alexander Roth  Peer Bork  et al. The
string database in 2017: quality-controlled protein–protein association networks  made broadly
accessible. Nucleic acids research  2016.

[38] Yi Tay  Luu Anh Tuan  and Siu Cheung Hui. Hyperbolic representation learning for fast
and efﬁcient neural question answering. In Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining  pages 583–591. ACM  2018.

[39] Alexandru Tifrea  Gary Becigneul  and Octavian-Eugen Ganea. Poincar´e glove: Hyperbolic

word embeddings. In International Conference on Learning Representations  2019.

[40] Joyce van de Leemput  Nathan C Boles  Thomas R Kiehl  Barbara Corneo  Patty Lederman 
Vilas Menon  Changkyu Lee  Refugio A Martinez  Boaz P Levi  Carol L Thompson  et al.
Cortecon: a temporal transcriptome analysis of in vitro human cerebral cortex development
from human embryonic stem cells. Neuron  2014.

[41] Petar Veliˇckovi´c  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations 
2018.

[42] Li Wan  Matthew Zeiler  Sixin Zhang  Yann Le Cun  and Rob Fergus. Regularization of
neural networks using dropconnect. In International conference on machine learning  pages
1058–1066  2013.

[43] Richard C Wilson  Edwin R Hancock  El˙zbieta Pekalska  and Robert PW Duin. Spherical and
hyperbolic embeddings of data. IEEE transactions on pattern analysis and machine intelligence 
36(11):2255–2269  2014.

[44] Felix Wu  Amauri Souza  Tianyi Zhang  Christopher Fifty  Tao Yu  and Kilian Weinberger.
Simplifying graph convolutional networks. In International Conference on Machine Learning 
pages 6861–6871  2019.

[45] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural

networks? In International Conference on Learning Representations  2019.

[46] Zhilin Yang  William Cohen  and Ruslan Salakhudinov. Revisiting semi-supervised learning
with graph embeddings. In International Conference on Machine Learning  pages 40–48  2016.

[47] Rex Ying  Ruining He  Kaifeng Chen  Pong Eksombatchai  William L Hamilton  and Jure
In
Leskovec. Graph convolutional neural networks for web-scale recommender systems.
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining  pages 974–983. ACM  2018.

[48] Hongyi Zhang  Sashank J Reddi  and Suvrit Sra. Riemannian svrg: Fast stochastic optimization
In Advances in Neural Information Processing Systems  pages

on riemannian manifolds.
4592–4600  2016.

[49] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances

in Neural Information Processing Systems  pages 5165–5175  2018.

[50] Marinka Zitnik  Marcus W Feldman  Jure Leskovec  et al. Evolution of resilience in pro-
tein interactomes across the tree of life. Proceedings of the National Academy of Sciences 
116(10):4426–4433  2019.

12

,Ines Chami
Zhitao Ying
Christopher Ré
Jure Leskovec