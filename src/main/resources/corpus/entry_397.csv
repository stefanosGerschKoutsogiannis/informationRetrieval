2011,Adaptive Hedge,Most methods for decision-theoretic online learning are based on the Hedge algorithm  which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance  leading to suboptimal performance on easy instances  for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate  which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance  but on easy instances it achieves much smaller regret. In particular  our adaptive method achieves constant regret in a probabilistic setting  when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.,Adaptive Hedge

Tim van Erven

Department of Mathematics

VU University

De Boelelaan 1081a

1081 HV Amsterdam  the Netherlands

tim@timvanerven.nl

Wouter M. Koolen

CWI and Department of Computer Science

Royal Holloway  University of London

Egham Hill  Egham  Surrey
TW20 0EX  United Kingdom
wouter@cs.rhul.ac.uk

Peter Gr¨unwald

Centrum Wiskunde & Informatica (CWI)

Science Park 123  P.O. Box 94079

1090 GB Amsterdam  the Netherlands

pdg@cwi.nl

Steven de Rooij

Centrum Wiskunde & Informatica (CWI)

Science Park 123  P.O. Box 94079

1090 GB Amsterdam  the Netherlands

s.de.rooij@cwi.nl

Abstract

Most methods for decision-theoretic online learning are based on the Hedge algo-
rithm  which takes a parameter called the learning rate. In most previous analyses
the learning rate was carefully tuned to obtain optimal worst-case performance 
leading to suboptimal performance on easy instances  for example when there ex-
ists an action that is signiﬁcantly better than all others. We propose a new way
of setting the learning rate  which adapts to the difﬁculty of the learning prob-
lem: in the worst case our procedure still guarantees optimal performance  but on
easy instances it achieves much smaller regret. In particular  our adaptive method
achieves constant regret in a probabilistic setting  when there exists an action that
on average obtains strictly smaller loss than all other actions. We also provide a
simulation study comparing our approach to existing methods.

1

Introduction

Decision-theoretic online learning (DTOL) is a framework to capture learning problems that proceed
in rounds. It was introduced by Freund and Schapire [1] and is closely related to the paradigm of
prediction with expert advice [2  3  4]. In DTOL an agent is given access to a ﬁxed set of K actions 
and at the start of each round must make a decision by assigning a probability to every action. Then
all actions incur a loss from the range [0  1]  and the agent’s loss is the expected loss of the actions
under the probability distribution it produced. Losses add up over rounds and the goal for the agent
is to minimize its regret after T rounds  which is the difference in accumulated loss between the
agent and the action that has accumulated the least amount of loss.
The most commonly studied strategy for the agent is called the Hedge algorithm [1  5]. Its per-
formance crucially depends on a parameter η called the learning rate. Different ways of tuning
the learning rate have been proposed  which all aim to minimize the regret for the worst possi-
ble sequence of losses the actions might incur. If T is known to the agent  then the learning rate

may be tuned to achieve worst-case regret bounded by(cid:112)T ln(K)/2  which is known to be opti-
(cid:112)2L∗

mal as T and K become large [4]. Nevertheless  by slightly relaxing the problem  one can obtain
better guarantees. Suppose for example that the cumulative loss L∗
T of the best action is known
to the agent beforehand. Then  if the learning rate is set appropriately  the regret is bounded by
T ln(K) + ln(K) [4]  which has the same asymptotics as the previous bound in the worst case

1

T

T

T

(because L∗

Kale [6] obtain a bound of 8(cid:112)VARmax

T ≤ T ) but may be much better when L∗

T turns out to be small. Similarly  Hazan and
ln(K) + 10 ln(K) for a modiﬁcation of Hedge if the cumu-
lative empirical variance VARmax
of the best expert is known. In applications it may be unrealistic to
assume that T or (especially) L∗
T or VARmax
is known beforehand  but at the cost of slightly worse
constants such problems may be circumvented using either the doubling trick (setting a budget on
the unknown quantity and restarting the algorithm with a double budget when the budget is depleted)
[4  7  6]  or a variable learning rate that is adjusted each round [4  8].
Bounding the regret in terms of L∗
is based on the idea that worst-case performance is
not the only property of interest: such bounds give essentially the same guarantee in the worst case 
but a much better guarantee in a plausible favourable case (when L∗
is small). In this
paper  we pursue the same goal for a different favourable case. To illustrate our approach  consider
the following simplistic example with two actions: let 0 < a < b < 1 be such that b− a > 2. Then
in odd rounds the ﬁrst action gets loss a +  and the second action gets loss b − ; in even rounds
the actions get losses a −  and b +   respectively. Informally  this seems like a very easy instance
of DTOL  because the cumulative losses of the actions diverge and it is easy to see from the losses
which action is the best one. In fact  the Follow-the-Leader strategy  which puts all probability mass
on the action with smallest cumulative loss  gives a regret of at most 1 in this case — the worst-case
ln(K))  which is of the

bound O((cid:112)L∗
same order(cid:112)T ln(K). On the other hand  for Follow-the-Leader one cannot guarantee sublinear

T ln(K)) is very loose by comparison  and so is O((cid:112)VARmax

T or VARmax

T or VARmax

T

T

T

regret for worst-case instances. (For example  if one out of two actions yields losses 1
2   0  1  0  1  . . .
and the other action yields losses 0  1  0  1  0  . . .  its regret will be at least T /2 − 1.) To get the best
of both worlds  we introduce an adaptive version of Hedge  called AdaHedge  that automatically
adapts to the difﬁculty of the problem by varying the learning rate appropriately. As a result we
obtain constant regret for the simplistic example above and other ‘easy’ instances of DTOL  while

at the same time guaranteeing O((cid:112)L∗

T ln(K)) regret in the worst case.

It remains to characterise what we consider easy problems  which we will do in terms of the prob-
abilities produced by Hedge. As explained below  these may be interpreted as a generalisation of
Bayesian posterior probabilities. We measure the difﬁculty of the problem in terms of the speed at
which the posterior probability of the best action converges to one. In the previous example  this
happens at an exponential rate  whereas for worst-case instances the posterior probability of the best
action does not converge to one at all.

Outline
In the next section we describe a new way of tuning the learning rate  and show that it
yields essentially optimal performance guarantees in the worst case. To construct the AdaHedge
algorithm  we then add the doubling trick to this idea in Section 3  and analyse its worst-case regret.
In Section 4 we show that AdaHedge in fact incurs much smaller regret on easy problems. We
compare AdaHedge to other instances of Hedge by means of a simulation study in Section 5. The
proof of our main technical lemma is postponed to Section 6  and open questions are discussed in
the concluding Section 7. Finally  longer proofs are only available as Additional Material in the full
version at arXiv.org.

2 Tuning the Learning Rate
Setting Let the available actions be indexed by k ∈ {1  . . .   K}. At the start of each round
t = 1  2  . . . the agent A is to assign a probability wk
to each action k by producing a vector
t
t ) with nonnegative components that sum up to 1. Then every action k incurs
wt = (w1
a loss (cid:96)k
t )  and the loss of the agent
t   and the

t   . . .   wK
t ∈ [0  1]  which we collect in the loss vector (cid:96)t = ((cid:96)1

is wt · (cid:96)t = (cid:80)K

t . After T rounds action k has accumulated loss Lk

T = (cid:80)T

t   . . .   (cid:96)K

k=1 wk

t=1 (cid:96)k

t (cid:96)k

agent’s regret is

RA(T ) =

wt · (cid:96)t − L∗
T  

where L∗

T = min1≤k≤K Lk

T is the cumulative loss of the best action.

T(cid:88)

t=1

2

Hedge The Hedge algorithm chooses the weights wk
t   where η > 0 is
the learning rate. As is well-known  these weights may essentially be interpreted as Bayesian pos-
terior probabilities on actions  relative to a uniform prior and pseudo-likelihoods P k
t =

t+1 proportional to e−ηLk

t = e−ηLk

(cid:81)t
s=1 e−η(cid:96)k

s [9  10  4]:

where

Bt =

wk

t+1 =

(cid:88)

k

1

K · P k
Bt

t

 

1

K · e−ηLk

t

(1)

=

t

t

1

(cid:80)
e−ηLk
(cid:88)
k(cid:48) e−ηLk(cid:48)
K · P k
t(cid:89)

t =

k

Bt =

ws · e−η(cid:96)s.

is a generalisation of the Bayesian marginal likelihood. And like the ordinary marginal likelihood 
Bt factorizes into sequential per-round contributions:

(2)

We will sometimes write wt(η) and Bt(η) instead of wt and Bt in order to emphasize the depen-
dence of these quantities on η.

s=1

The Learning Rate and the Mixability Gap A key quantity in our and previous [4] analyses is
the gap between the per-round loss of the Hedge algorithm and the per-round contribution to the
negative logarithm of the “marginal likelihood” BT   which we call the mixability gap:

δt(η) = wt(η) · (cid:96)t −(cid:16) − 1

η ln(wt(η) · e−η(cid:96)t)

(cid:17)

.

In the setting of prediction with expert advice  the subtracted term coincides with the loss incurred
by the Aggregating Pseudo-Algorithm (APA) which  by allowing the losses of the actions to be
mixed with optimal efﬁciency  provides an idealised lower bound for the actual loss of any predic-
tion strategy [9]. The mixability gap measures how closely we approach this ideal. As the same
interpretation still holds in the more general DTOL setting of this paper  we can measure the difﬁ-
culty of the problem  and tune η  in terms of the cumulative mixability gap:

T(cid:88)

T(cid:88)

∆T (η) =

δt(η) =

wt(η) · (cid:96)t + 1

η ln BT (η).

t=1

t=1

We proceed to list some basic properties of the mixability gap. First  it is nonnegative and bounded
above by a constant that depends on η:
Lemma 1. For any t and η > 0 we have 0 ≤ δt(η) ≤ η/8.

Proof. The lower bound follows by applying Jensen’s inequality to the concave function ln  the
upper bound from Hoeffding’s bound on the cumulant generating function [4  Lemma A.1].
Further  the cumulative mixability gap ∆T (η) can be related to L∗
proved in the Additional Material:
Lemma 2. For any T and η ∈ (0  1] we have ∆T (η) ≤ ηL∗

T via the following upper bound 

.

T + ln(K)
e − 1

This relationship will make it possible to provide worst-case guarantees similar to what is possible
when η is tuned in terms of L∗
T . However  for easy instances of DTOL this inequality is very loose 
in which case we can prove substantially better regret bounds. We could now proceed by optimizing
the learning rate η given the rather awkward assumption that ∆T (η) is bounded by a known constant
b for all η  which would be the natural counterpart to an analysis that optimizes η when a bound on
L∗
T is known. However  as ∆T (η) varies with η and is unknown a priori anyway  it makes more
sense to turn the analysis on its head and start by ﬁxing η. We can then simply run the Hedge
algorithm until the smallest T such that ∆T (η) exceeds an appropriate budget b(η)  which we set to

b(η) =

η + 1
e−1

ln(K).

(3)

(cid:16) 1

(cid:17)

3

When at some point the budget is depleted  i.e. ∆T (η) ≥ b(η)  Lemma 2 implies that

(e − 1) ln(K)/L∗
T  

rates proportional to(cid:112)ln(K)/L∗
large  because we can still provide a bound of order O((cid:112)L∗

so that  up to a constant factor  the learning rate used by AdaHedge is at least as large as the learning
T that are used in the literature. On the other hand  it is not too
Theorem 3. Suppose the agent runs Hedge with learning rate η ∈ (0  1]  and after T rounds has
just used up the budget (3)  i.e. b(η) ≤ ∆T (η) < b(η) + η/8. Then its regret is bounded by

T ln(K)) on the worst-case regret:

(4)

η ≥(cid:113)

(cid:113) 4

RHedge(η)(T ) <

e−1 L∗

T ln(K) + 1

e−1 ln(K) + 1
8 .

T(cid:88)

t=1

Proof. The cumulative loss of Hedge is bounded by

wt · (cid:96)t = ∆T (η) − 1

η ln BT < b(η) + η/8 − 1

η ln BT ≤ 1

e−1 ln(K) + 1

8 + 2

η ln(K) + L∗

T   (5)

where we have used the bound BT ≥ 1

K e−ηL∗

T . Plugging in (4) completes the proof.

3 The AdaHedge Algorithm

We now introduce the AdaHedge algorithm by adding the doubling trick to the analysis of the
previous section. The doubling trick divides the rounds in segments i = 1  2  . . .  and on each
segment restarts Hedge with a different learning rate ηi. For AdaHedge we set η1 = 1 initially  and
scale down the learning rate by a factor of φ > 1 for every new segment  such that ηi = φ1−i. We
monitor ∆t(ηi)  measured only on the losses in the i-th segment  and when it exceeds its budget
bi = b(ηi) a new segment is started. The factor φ is a parameter of the algorithm. Theorem 5 below
suggests setting its value to the golden ratio φ = (1 +

5)/2 ≈ 1.62 or simply to φ = 2.

√

Algorithm 1 AdaHedge(φ)

η ← φ
for t = 1  2  . . . do

(cid:46) Requires φ > 1

if t = 1 or ∆ ≥ b then
(cid:46) Start a new segment
η ← η/φ; b ← ( 1
η ) ln(K)
∆ ← 0; w = (w1  . . .   wK) ← ( 1

e−1 + 1

end if
(cid:46) Make a decision
Output probabilities w for round t
Actions receive losses (cid:96)t
(cid:46) Prepare for the next round
∆ ← ∆ + w · (cid:96)t + 1
w ← (w1 · e−η(cid:96)1

η ln(w · e−η(cid:96)t)
t   . . .   wK · e−η(cid:96)K

K   . . .   1
K )

t )/(w · e−η(cid:96)t)

end for

end

The regret of AdaHedge is determined by the number of segments it creates: the fewer segments
there are  the smaller the regret.
Lemma 4. Suppose that after T rounds  the AdaHedge algorithm has started m new segments.
Then its regret is bounded by

RAdaHedge(T ) < 2 ln(K)

+ m

e−1 ln(K) + 1

8

.

(cid:80)m
i=1 1/ηi =(cid:80)m−1

Proof. The regret per segment is bounded as in (5). Summing over all m segments  and plugging in

i=0 φi = (φm − 1)/(φ − 1) gives the required inequality.

(cid:16) φm − 1

(cid:17)

φ − 1

(cid:16) 1

(cid:17)

4

RAdaHedge(T ) ≤ φ(cid:112)φ2 − 1

Using (4)  one can obtain an upper bound on the number of segments that leads to the following
guarantee for AdaHedge:
Theorem 5. Suppose the agent runs AdaHedge for T rounds. Then its regret is bounded by

T ln(K) + O(cid:0)ln(L∗
φ = 2 leads to a very similar factor of φ(cid:112)φ2 − 1/(φ − 1) ≈ 3.46.

(cid:113) 4
5)/2  for which φ(cid:112)φ2 − 1/(φ − 1) ≈ 3.33  but simply taking

For details see the proof in the Additional Material. The value for φ that minimizes the leading
factor is the golden ratio φ = (1 +

T + 2) ln(K)(cid:1) 

φ − 1
√

e−1 L∗

4 Easy Instances

While the previous sections reassure us that AdaHedge performs well for the worst possible se-
quence of losses  we are also interested in its behaviour when the losses are not maximally an-
tagonistic. We will characterise such sequences in terms of convergence of the Hedge posterior
probability of the best action:

w∗
t (η) = max
1≤k≤K
t is proportional to e−ηLk
t−1  so w∗
t corresponds to the posterior probability of the
(Recall that wk
action with smallest cumulative loss.) Technically  this is expressed by the following reﬁnement of
Lemma 1  which is proved in Section 6.

t (η)(cid:1).
Lemma 6. For any t and η ∈ (0  1] we have δt(η) ≤ (e − 2)η(cid:0)1 − w∗

t (η).

wk

This lemma  which may be of independent interest  is a variation on Hoeffding’s bound on the
cumulant generating function. While Lemma 1 leads to a bound on ∆T (η) that grows linearly
in T   Lemma 6 shows that ∆T (η) may grow much slower. In fact  if the posterior probabilities w∗
t
converge to 1 sufﬁciently quickly  then ∆T (η) is bounded  as shown by the following lemma. Recall
that L∗
Lemma 7. Let α and β be positive constants  and let τ ∈ Z+. Suppose that for t = τ  τ + 1  . . .   T
t   and for k (cid:54)= k∗ the
there exists a single action k∗ that achieves minimal cumulative loss Lk∗
cumulative losses diverge as Lk

t ≥ αtβ. Then for all η > 0

T .
T = min1≤k≤K Lk

t = L∗

t − L∗
T(cid:88)

t=τ

(cid:0)1 − w∗

t+1(η)(cid:1) ≤ CK η−1/β 

where CK = (K − 1)α−1/βΓ(1 + 1

β ) is a constant that does not depend on η  τ or T .

The lemma is proved in the Additional Material. Together with Lemmas 1 and 6  it gives an upper
bound on ∆T (η)  which may be used to bound the number of segments started by AdaHedge. This
leads to the following result  whose proof is also delegated to the Additional Material.
Let s(m) denote the round in which AdaHedge starts its m-th segment  and let Lk
s(m)+r−1 − Lk
Lk
Lemma 8. Let α > 0 and β > 1/2 be constants  and let CK be as in Lemma 7. Suppose there
exists a segment m∗ ∈ Z+ started by AdaHedge  such that τ := (cid:98)8 ln(K)φ(m∗−1)(2−1/β) − 8(e −
2)CK + 1(cid:99) ≥ 1 and for some action k∗ the cumulative losses in segment m∗ diverge as

s(m)−1 denote the cumulative loss of action k in that segment.

r (m) =

r (m∗) − Lk∗
Lk

r (m∗) ≥ αrβ

(6)
Then AdaHedge starts at most m∗ segments  and hence by Lemma 4 its regret is bounded by a
constant:

for all r ≥ τ and k (cid:54)= k∗.

RAdaHedge(T ) = O(1).

In the simplistic example from the introduction  we may take α = b − a − 2 and β = 1  such that
(6) is satisﬁed for any τ ≥ 1. Taking m∗ large enough to ensure that τ ≥ 1  we ﬁnd that AdaHedge
never starts more than m∗ = 1 + (cid:100)logφ( e−2
8 ln(2) )(cid:101) segments. Let us also give an example of
a probabilistic setting in which Lemma 8 applies:

α ln(2) + 1

5

Theorem 9. Let α > 0 and δ ∈ (0  1] be constants  and let k∗ be a ﬁxed action. Suppose the loss
vectors (cid:96)t are independent random variables such that the expected differences in loss satisfy

Then  with probability at least 1 − δ  AdaHedge starts at most

(cid:108)

m∗ = 1 +

logφ

E[(cid:96)k

min
k(cid:54)=k∗

t

for all t ∈ Z+.

] ≥ 2α

t − (cid:96)k∗
ln(cid:0)2K/(α2δ)(cid:1)
(cid:16) (K − 1)(e − 2)
RAdaHedge(T ) = O(cid:0)K + log(1/δ)(cid:1).

4α2 ln(K)

α ln(K)

+

(cid:17)(cid:109)

+

1

8 ln(K)

(7)

(8)

segments and consequently its regret is bounded by a constant:

only a bound on the regret of order O((cid:112)T ln(K)) is possible  and that AdaHedge automatically

This shows that the probabilistic setting of the theorem is much easier than the worst case  for which

adapts to this easier setting. The proof of Theorem 9 is in the Additional Material. It veriﬁes that the
conditions of Lemma 8 hold with sufﬁcient probability for β = 1  and α and m∗ as in the theorem.

5 Experiments

We compare AdaHedge to other hedging algorithms in two experiments involving simulated losses.

5.1 Hedging Algorithms

(9)

η =

T   the agent

Follow-the-Leader. This algorithm is included because it is simple and very effective if the losses
are not antagonistic  although as mentioned in the introduction its regret is linear in the worst case.
Hedge with ﬁxed learning rate. We also include Hedge with a ﬁxed learning rate

(cid:113)
which achieves the regret bound(cid:112)2 ln(K)L∗
2 ln(K)/L∗
T  
T + ln(K)1. Since η is a function of L∗

needs to use post-hoc knowledge to use this strategy.
Hedge with doubling trick. The common way to apply the doubling trick to L∗
T is to set a budget on
T and multiply it by some constant φ(cid:48) at the start of each new segment  after which η is optimized
L∗
for the new budget [4  7]. Instead  we proceed the other way around and with each new segment
ﬁrst divide η by φ = 2 and then calculate the new budget such that (9) holds when ∆t(η) reaches
the budget. This way we keep the same invariant (η is never larger than the right-hand side of (9) 
with equality when the budget is depleted)  and the frequency of doubling remains logarithmic in
L∗
T with a constant determined by φ  so both approaches are equally valid. However  controlling the
sequence of values of η allows for easier comparison to AdaHedge.
AdaHedge (Algorithm 1). Like in the previous algorithm  we set φ = 2. Because of how we set up
the doubling  both algorithms now use the same sequence of learning rates 1  1/2  1/4  . . . ; the only
difference is when they decide to start a new segment.
Hedge with variable learning rate. Rather than using the doubling trick  this algorithm  described
in [8]  changes the learning rate each round as a function of L∗
t . This way there is no need to relearn
the weights of the actions in each block  which leads to a better worst-case bound and potentially
better performance in practice. Its behaviour on easy problems  as we are currently interested in  has
not been studied.

5.2 Generating the Losses
In both experiments we choose losses in {0  1}. The experiments are set up as follows.

1Cesa-Bianchi and Lugosi use η = ln(1 +(cid:112)2 ln K/L∗

T ) [4]  but the same bound can be obtained for the

simpliﬁed expression we use.

6

(a) I.I.D. losses

(b) Correlated losses

Figure 1: Simulation results

I.I.D. losses. In the ﬁrst experiment  all T = 10 000 losses for all K = 4 actions are independent 
with distribution depending only on the action: the probabilities of incurring loss 1 are 0.35  0.4 
0.45 and 0.5  respectively. The results are then averaged over 50 repetitions of the experiment.
Correlated losses. In the second experiment  the T = 10 000 loss vectors are still independent 
but no longer identically distributed. In addition there are dependencies within the loss vectors (cid:96)t 
between the losses for the K = 2 available actions: each round is hard with probability 0.3  and
easy otherwise. If round t is hard  then action 1 yields loss 1 with probability 1 − 0.01/t and action
2 yields loss 1 with probability 1− 0.02/t. If the round is easy  then the probabilities are ﬂipped and
the actions yield loss 0 with the same probabilities. The results are averaged over 200 repetitions.

5.3 Discussion and Results

Figure 1 shows the results of the experiments above. We plot the regret (averaged over repetitions
of the experiment) as a function of the number of rounds  for each of the considered algorithms.

I.I.D. Losses.
In the ﬁrst considered regime  the accumulated losses for each action diverge lin-
early with high probability  so that the regret of Follow-the-Leader is bounded. Based on Theorem 9
we expect AdaHedge to incur bounded regret also; this is conﬁrmed in Figure 1(a). Hedge with a
ﬁxed learning rate shows much larger regret. This happens because the learning rate  while it op-
timizes the worst-case bound  is much too small for this easy regime. In fact  if we would include
more rounds  the learning rate would be set to an even smaller value  clearly showing the need to
determine the learning rate adaptively. The doubling trick provides one way to adapt the learning
rate; indeed  we observe that the regret of Hedge with the doubling trick is initially smaller than the
regret of Hedge with ﬁxed learning rate. However  unlike AdaHedge  the algorithm never detects
that its current value of η is working well; instead it keeps exhausting its budget  which leads to a
sequence of clearly visible bumps in its regret. Finally  it appears that the Hedge algorithm with
variable learning rate also achieves bounded regret. This is surprising  as the existing theory for
this algorithm only considers its worst-case behaviour  and the algorithm was not designed to do
speciﬁcally well in easy regimes.

Correlated Losses.
In the second simulation we investigate the case where the mean cumulative
loss of two actions is extremely close — within O(log t) of one another. If the losses of the actions
where independent  such a small difference would be dwarfed by random ﬂuctuations in the cumula-
tive losses  which would be of order O(
t). Thus the two actions can only be distinguished because
we have made their losses dependent. Depending on the application  this may actually be a more nat-
ural scenario than complete independence as in the ﬁrst simulation; for example  we can think of the
losses as mistakes of two binary classiﬁers  say  two naive Bayes classiﬁers with different smooth-
ing parameters. In such a scenario  losses will be dependent  and the difference in cumulative loss
will be much smaller than O(
t). In the previous experiment  the posterior weights of the actions

√

√

7

0100020003000400050006000700080009000100000102030405060708090100Number of RoundsRegret  Hedge (doubling)Hedge (fixed learning rate)Hedge (variable learning rate)AdaHedgeFollow the leader01000200030004000500060007000800090001000002468101214161820Number of RoundsRegret  Hedge (doubling)Hedge (fixed learning rate)Hedge (variable learning rate)AdaHedgeFollow the leaderconverged relatively quickly for a large range of learning rates  so that the exact value of the learning
rate was most important at the start (e.g.  from 3000 rounds onward Hedge with ﬁxed learning rate
does not incur much additional regret any more). In this second setting  using a high learning rate
remains important throughout. This explains why in this case Hedge with variable learning rate can
no longer keep up with Follow-the-Leader. The results for AdaHedge are also interesting: although
Theorem 9 does not apply in this case  we may still hope that ∆t(η) grows slowly enough that the
algorithm does not start too many segments. This turns out to be the case: over the 200 repetitions
of the experiment  AdaHedge started only 2.265 segments on average  which explains its excellent
performance in this simulation.

6 Proof of Lemma 6

Our main technical tool is Lemma 6. Its proof requires the following intermediate result:
Lemma 10. For any η > 0 and any time t  the function f ((cid:96)t) = ln

wt · e−η(cid:96)t

is convex.

(cid:16)

(cid:17)

This may be proved by observing that f is the convex conjugate of the Kullback-Leibler divergence.
An alternative proof based on log-convexity is provided in the Additional Material.
Proof of Lemma 6. We need to bound δt = wt(η) · (cid:96)t + 1
η ln(wt(η) · e−η(cid:96)t)  which is a convex
function of (cid:96)t by Lemma 10. As a consequence  its maximum is achieved when (cid:96)t lies on the
t are either 0 or 1 for all k  and in the remainder of the
boundary of its domain  such that the losses (cid:96)k
proof we will assume (without loss of generality) that this is the case. Now let αt = wt · (cid:96)t be the
posterior probability of the actions with loss 1. Then

ln(cid:0)(1 − αt) + αte−η(cid:1) = αt +

δt = αt +

1
η

ln(cid:0)1 + αt(e−η − 1)(cid:1) .

1
η
2 αtη  which is tight for αt near 0. For αt

Using ln x ≤ x − 1 and e−η ≤ 1 − η + 1
near 1  rewrite

2 η2  we get δt ≤ 1

δt = αt − 1 +

ln(eη(1 − αt) + αt)

1
η

and use ln x ≤ x − 1 and eη ≤ 1 + η + (e − 2)η2 for η ≤ 1 to obtain δt ≤ (e − 2)(1 − αt)η.
Combining the bounds  we ﬁnd

δt ≤ (e − 2)η min{αt  1 − αt}.

Now  let k∗ be an action such that w∗
hand  if (cid:96)k∗
which completes the proof.

t = 1  then αt ≥ w∗

t = wk∗

t

t so 1−αt ≤ 1−w∗

. Then (cid:96)k∗

t = 0 implies αt ≤ 1 − w∗

t . On the other
t . Hence  in both cases min{αt  1−αt} ≤ 1−w∗
t  

7 Conclusion and Future Work

that the regret of AdaHedge is of the optimal order O((cid:112)L∗

We have presented a new algorithm  AdaHedge  that adapts to the difﬁculty of the DTOL learning
problem. This difﬁculty was characterised in terms of convergence of the posterior probability of the
best action. For hard instances of DTOL  for which the posterior does not converge  it was shown
T ln(K)); for easy instances  for which
the posterior converges sufﬁciently fast  the regret was bounded by a constant. This behaviour was
conﬁrmed in a simulation study  where the algorithm outperformed existing versions of Hedge.
A surprising observation in the experiments was the good performance of Hedge with a variable
learning rate on some easy instances. It would be interesting to obtain matching theoretical guar-
antees  like those presented here for AdaHedge. A starting point might be to consider how fast the
posterior probability of the best action converges to one  and plug that into Lemma 6.

Acknowledgments

The authors would like to thank Wojciech Kotłowski for useful discussions. This work was sup-
ported in part by the IST Programme of the European Community  under the PASCAL2 Network
of Excellence  IST-2007-216886  and by NWO Rubicon grant 680-50-1010. This publication only
reﬂects the authors’ views.

8

References
[1] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55:119–139  1997.

[2] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-

putation  108(2):212–261  1994.

[3] V. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences 

56(2):153–173  1998.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[5] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games

and Economic Behavior  29:79–103  1999.

[6] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation
in costs. In Proceedings of the 21st Annual Conference on Learning Theory (COLT)  pages
57–67  2008.

[7] N. Cesa-Bianchi  Y. Freund  D. Haussler  D. P. Helmbold  R. E. Schapire  and M. K. Warmuth.

How to use expert advice. Journal of the ACM  44(3):427–485  1997.

[8] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algo-

rithms. Journal of Computer and System Sciences  64:48–75  2002.

[9] V. Vovk. Competitive on-line statistics. International Statistical Review  69(2):213–248  2001.
[10] D. Haussler  J. Kivinen  and M. K. Warmuth. Sequential prediction of individual sequences
IEEE Transactions on Information Theory  44(5):1906–1925 

under general loss functions.
1998.

[11] A. N. Shiryaev. Probability. Springer-Verlag  1996.

9

,Yinlam Chow
Aviv Tamar
Shie Mannor
Marco Pavone
Shunyu Yao
Tzu Ming Hsu
Jun-Yan Zhu
Jiajun Wu
Antonio Torralba
Bill Freeman
Josh Tenenbaum