2013,Robust Data-Driven Dynamic Programming,In stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (DP)-based solution schemes can be applied. If the conditional expectations in the DP recursions are estimated via kernel regression  however  the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations. If training data is sparse  however  the estimated cost-to-go functions display a high variability and an optimistic bias  while the corresponding control policies perform poorly in out-of-sample tests. To mitigate these small sample effects  we propose a robust data-driven DP scheme  which replaces the expectations in the DP recursions with worst-case expectations over a set of distributions close to the best estimate. We show that the arising min-max problems in the DP recursions reduce to tractable conic programs. We also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains.,Robust Data-Driven Dynamic Programming

Grani A. Hanasusanto
Imperial College London
London SW7 2AZ  UK

g.hanasusanto11@imperial.ac.uk

Daniel Kuhn

École Polytechnique Fédérale de Lausanne

CH-1015 Lausanne  Switzerland

daniel.kuhn@epfl.ch

Abstract

In stochastic optimal control the distribution of the exogenous noise is typically
unknown and must be inferred from limited data before dynamic programming
(DP)-based solution schemes can be applied. If the conditional expectations in the
DP recursions are estimated via kernel regression  however  the historical sample
paths enter the solution procedure directly as they determine the evaluation points
of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically
consistent and admits an efﬁcient computational solution when combined with
parametric value function approximations. If training data is sparse  however  the
estimated cost-to-go functions display a high variability and an optimistic bias 
while the corresponding control policies perform poorly in out-of-sample tests. To
mitigate these small sample effects  we propose a robust data-driven DP scheme 
which replaces the expectations in the DP recursions with worst-case expectations
over a set of distributions close to the best estimate. We show that the arising min-
max problems in the DP recursions reduce to tractable conic programs. We also
demonstrate that the proposed robust DP algorithm dominates various non-robust
schemes in out-of-sample tests across several application domains.

1

Introduction

We consider a stochastic optimal control problem in discrete time with continuous state and action
spaces. At any time t the state of the underlying system has two components. The endogenous state
st ∈ Rd1 captures all decision-dependent information  while the exogenous state ξt ∈ Rd2 captures
the external random disturbances. Conditional on (st  ξt) the decision maker chooses a control
action ut ∈ Ut ⊆ Rm and incurs a cost ct(st  ξt  ut). From time t to t + 1 the system then migrates
to a new state (st+1  ξt+1). Without much loss of generality we assume that the endogenous state
obeys the recursion st+1 = gt(st  ut  ξt+1)  while the evolution of the exogenous state can be
modeled by a Markov process. Note that even if the exogenous state process has ﬁnite memory  it
can be reduced as an equivalent Markov process on a higher-dimensional space. Thus  the Markov
assumption is unrestrictive for most practical purposes. By Bellman’s principle of optimality  a
decision maker aiming to minimize the expected cumulative costs solves the dynamic program

ct(st  ξt  ut) + E[Vt+1(st+1  ξt+1)|ξt]

Vt(st  ξt) = min
ut∈Ut
s. t.

st+1 = gt(st  ut  ξt+1)

(1)
backwards for t = T  . . .   1 with VT +1 ≡ 0; see e.g. [1]. The cost-to-go function Vt(st  ξt) quanti-
ﬁes the minimum expected future cost achievable from state (st  ξt) at time t.
Stochastic optimal control has numerous applications in engineering and science  e.g. in supply
chain management  power systems scheduling  behavioral neuroscience  asset allocation  emergency
service provisioning  etc. [1  2]. There is often a natural distinction between endogenous and exoge-
nous states. For example  in inventory control the inventory level can naturally be interpreted as the
endogenous state  while the uncertain demand represents the exogenous state.

1

In spite of their exceptional modeling power  dynamic programming problems of the above type
suffer from two major shortcomings that limit their practical applicability. First  the backward in-
duction step (1) is computationally burdensome due to the intractability to evaluate the cost-to-go
function Vt for the continuum of all states (st  ξt)  the intractability to evaluate the multivariate
conditional expectations and the intractability to optimize over the continuum of all control actions
ut [2]. Secondly  even if the dynamic programming recursions (1) could be computed efﬁciently 
there is often substantial uncertainty about the conditional distribution of ξt+1 given ξt. Indeed 
the distribution of the exogenous states is typically unknown and must be inferred from historical
observations. If training data is sparse—as is often the case in practice—it is impossible to estimate
this distribution reliably. Thus  we lack essential information to evaluate (1) in the ﬁrst place.
In this paper  we assume that only a set of N sample trajectories of the exogenous state is given 
and we use kernel regression in conjunction with parametric value function approximations to esti-
mate the conditional expectation in (1). Thus  we approximate the conditional distribution of ξt+1
given ξt by a discrete distribution whose discretization points are given by the historical samples 
while the corresponding conditional probabilities are expressed in terms of a normalized Nadaraya-
Watson (NW) kernel function. This data-driven dynamic programming (DDP) approach is concep-
tually appealing and avoids an artiﬁcial separation of estimation and optimization steps. Instead  the
historical samples are used directly in the dynamic programming recursions. It is also asymptoti-
cally consistent in the sense that the true conditional expectation is recovered when N grows [3].
Moreover  DDP computes the value functions only on the N sample trajectories of the exogenous
state  thereby mitigating one of the intractabilities of classical dynamic programming.
Although conceptually and computationally appealing  DDP-based policies exhibit a poor perfor-
mance in out-of-sample tests if the training data is sparse. In this case the estimate of the conditional
expectation in (1) is highly noisy (but largely unbiased). The estimate of the corresponding cost-
to-go value inherits this variability. However  it also displays a downward bias caused by the mini-
mization over ut. This phenomenon is reminiscent of overﬁtting effects in statistics. As estimation
errors in the cost-to-go functions are propagated through the dynamic programming recursions  the
bias grows over time and thus incentivizes poor control decisions in the early time periods.
The detrimental overﬁtting effects observed in DDP originate from ignoring distributional uncer-
tainty: DDP takes the estimated discrete conditional distribution of ξt+1 at face value and ignores
the possibility of estimation errors. In this paper we propose a robust data-driven dynamic pro-
gramming (RDDP) approach that replaces the expectation in (1) by a worst-case expectation over
a set of distributions close to the nominal estimate in view of the χ2-distance. We will demon-
strate that this regularization reduces both the variability and the bias in the approximate cost-to-go
functions and that RDDP dominates ordinary DDP as well as other popular benchmark algorithms
in out-of-sample tests. Leveraging on recent results in robust optimization [4] and value function
approximation [5] we will also show that the nested min-max problems arising in RDDP typically
reduce to conic optimization problems that admit efﬁcient solution with interior point algorithms.
Robust value iteration methods have recently been studied in robust Markov decision process (MDP)
theory [6  7  8  9]. However  these algorithms are not fundamentally data-driven as their primitives
are uncertainty sets for the transition kernels instead of historical observations. Moreover  they
assume ﬁnite state and action spaces. Data-driven approaches to dynamic decision making are rou-
tinely studied in approximate dynamic programming and reinforcement learning [10  11  12]  but
these methods are not robust (in a worst-case sense) with respect to distributional uncertainty and
could therefore be susceptible to overﬁtting effects. The robust value iterations in RDDP are facil-
itated by combining convex parametric function approximation methods (to model the dependence
on the endogenous state) with nonparametric kernel regression techniques (for the dependence on
the exogenous state). This is in contrast to most existing methods  which either rely exclusively
on parametric function approximations [10  11  13] or nonparametric ones [12  14  15  16]. Due
to the convexity in the endogenous state  RDDP further beneﬁts from mathematical programming
techniques to optimize over high-dimensional continuous action spaces without requiring any form
of discretization.
Notation. We use lower-case bold face letters to denote vectors and upper-case bold face letters
to denote matrices. We deﬁne 1 ∈ Rn as the vector with all elements equal to 1  while ∆ = {p ∈
p = 1} denotes the probability simplex in Rn. The dimensions of 1 and ∆ will usually be
Rn
+ : 1
clear from the context. The space of symmetric matrices of dimension n is denoted by Sn. For any
two matrices X  Y ∈ Sn  the relation X (cid:60) Y implies that X − Y is positive semideﬁnite.

(cid:124)

2

2 Data-driven dynamic programming

Assume from now on that the distribution of the exogenous states is unknown and that we are only
given N observation histories {ξi
t}T
t=1 for i = 1  . . .   N. This assumption is typically well justiﬁed
in practice. In this setting  the conditional expectation in (1) cannot be evaluated exactly. However
it can be estimated  for instance  via Nadaraya-Watson (NW) kernel regression [17  18].

t+1  ξi

t+1)

(2)

The conditional probabilities in (2) are set to

E[Vt+1(st+1  ξt+1)|ξt] ≈ N(cid:88)
(cid:80)N
KH(ξt − ξi
t)
k=1 KH(ξt − ξk
t )

i=1

qti(ξt)Vt+1(si

 

(3)

2 K(|H|− 1

qti(ξt) =
where the kernel function KH(ξ) = |H|− 1
2 ξ) is deﬁned in terms of a symmetric multi-
variate density K and a positive deﬁnite bandwidth matrix H. For a large bandwidth  the conditional
probabilities qti(ξt) converge to 1
N   in which case (2) reduces to the (unconditional) sample aver-
age. Conversely  an extremely small bandwidth causes most of the probability mass to be assigned
to the sample point closest to ξt. In the following we set the bandwidth matrix H to its best es-
t}N
timate assuming that the historical observations {ξi
i=1 follow a Gaussian distribution; see [19].
N(cid:88)
Substituting (2) into (1)  results in the data-driven dynamic programming (DDP) formulation
qti(ξt)V d
t+1) ∀i  

i=1
si
t+1 = gt(st  ut  ξi

V d
t (st  ξt) = min
ut∈Ut

T +1 ≡ 0. The idea to use kernel-based approximations to estimate the
with terminal condition V d
expected future costs is appealing due to its simplicity. Such approximations have been studied  for
example  in the context of stochastic optimization with state observation [20]. However  to the best
of our knowledge they have not yet been used in a fully dynamic setting—maybe for the reasons to be
outlined in § 3. On the positive side  DDP with NW kernel regression is asymptotically consistent for
large N under a suitable scaling of the bandwidth matrix and under a mild boundedness assumption
on V d
t+1 [3]. Moreover  DDP evaluates the cost-to-go function of the next period only at the sample
points and thus requires no a-priori discretization of the exogenous state space  thus mitigating one
of the intractabilities of classical dynamic programming.

ct(st  ξt  ut) +

s. t.

t+1(si

t+1  ξi

t+1)

(4)

3 Robust data-driven dynamic programming

u = 1} and Vt+1(st+1  ξt+1) = 1

If the training data is sparse  the NW estimate (2) of the conditional expectation in (4) typically
exhibits a small bias and a high variability. Indeed  the variance of the estimator scales with ∼O( 1
N )
inherits this variability. However  it also displays a signiﬁcant
[21]. The DDP value function V d
t
optimistic bias. The following stylized example illustrates this phenomenon.
(cid:124)
Example 3.1 Assume that d1 = 1  d2 = m = 5  ct(st  ξt  ut) = 0  gt(st  ut  ξt+1) = ξ
t+1ut 
t+1 − st+1. In order to facilitate a
Ut = {u ∈ Rm : 1
(cid:124)
controlled experiment  we also assume that (ξt  ξt+1) follows a multivariate Gaussian distribution 
where each component has unit mean and variance. The correlation between ξt k and ξt+1 k is set
to 30%. All other correlations are zero. Our aim is to solve (1) and to estimate Vt(st  ξt) at ξt = 1.
By permutation symmetry  the optimal decision under full distributional knowledge is u∗
5 1.
An analytical calculation then yields the true cost-to-go value Vt(st  1) = −0.88. In the following
we completely ignore our distributional knowledge. Instead  we assume that only N independent
samples (ξi
t+1) are given  i = 1  . . .   N. To showcase the high variability of NW estimation 
we ﬁx the decision u∗
t and use (2) to estimate its expected cost conditional on ξt = 1. Figure 1
(left) shows that this estimator is unbiased but ﬂuctuates within ±5% around its median even for
N = 500. Next  we use (4) to estimate V d
t (st  1)  that is  the expected cost of the best decision
obtained without distributional information. Figure 1 (middle) shows that this cost estimator is even
more noisy than the one for a ﬁxed decision  exhibits a signiﬁcant downward bias and converges
slowly as N grows.

t = 1

10 s2

t  ξi

3

Figure 1: Estimated costs of true optimal and data-driven decisions. Note the different scales. All
reported values represent averages over 200 independent simulation runs.

The downward bias in V d
overﬁtting effect  which can be explained as follows. Setting Vt+1 ≡ V d

t as an estimator for the true value function Vt is the consequence of an

t+1  we ﬁnd

Vt(st  ξt) = min
ut∈Ut
≈ min
ut∈Ut

≥ E(cid:104)

ct(st  ξt  ut) + E[V d

ct(st  ξt  ut) + E[

t+1(gt(st  ut  ξt+1)  ξt+1)|ξt]
N(cid:88)
N(cid:88)

t+1(gt(st  ut  ξi

qti(ξt)V d

i=1

qti(ξt)V d

t+1(gt(st  ut  ξi

min
ut∈Ut

ct(st  ξt  ut) +

i=1

t+1)  ξi

t+1)|ξt]

t+1)  ξi

t+1)

(cid:105)

.

(cid:12)(cid:12)(cid:12)ξt

The relation in the second line uses our observation that the NW estimator of the expected cost
associated with any ﬁxed decision ut is approximately unbiased. Here  the expectation is with
respect to the (independent and identically distributed) sample trajectories used in the NW estimator.
The last line follows from the conditional Jensen inequality. Note that the expression inside the
t (st  ξt) must
conditional expectation coincides with V d
indeed underestimate Vt(st  ξt) on average. We emphasize that all systematic estimation errors of
this type accumulate as they are propagated through the dynamic programming recursions.
To mitigate the detrimental overﬁtting effects  we propose a regularization that reduces the decision
(cid:124). Thus  we allow the condi-
maker’s overconﬁdence in the weights qt(ξt) = [qt1(ξt) . . . qtN (ξt)]
tional probabilities used in (4) to deviate from their nominal values qt(ξt) up to a certain degree.
This is achieved by considering uncertainty sets ∆ (q) that contain all weight vectors sufﬁciently
close to some nominal weight vector q ∈ ∆ with respect to the χ2-distance for histograms.

t (st  ξt). This argument suggests that V d

∆ (q) = {p ∈ ∆ :

(pi − qi)2/pi ≤ γ}

(5)

N(cid:88)

i=1

4

The χ2-distance belongs to the class of φ-divergences [22]  which also includes the Kullback-Leibler
distances. Our motivation for using uncertainty sets of the type (5) is threefold. First  ∆(q) is
determined by a single size parameter γ  which can easily be calibrated  e.g.  via cross-validation.
Secondly  the χ2-distance guarantees that any distribution p ∈ ∆(q) assigns nonzero probability to
all scenarios that have nonzero probability under the nominal distribution q. Finally  the structure of
∆(q) implied by the χ2-distance has distinct computational beneﬁts that become evident in § 4.
Allowing the conditional probabilities in (4) to range over the uncertainty set ∆(qt(ξt)) results in
the robust data-driven dynamic programming (RDDP) formulation

V r
t (st  ξt) = min
ut∈Ut

ct(st  ξt  ut) + max

piV r

t+1(si

t+1  ξi

t+1)

(6)

si
t+1 = gt(st  ut  ξi

s. t.
T +1 ≡ 0. Thus  each RDDP recursion involves the solution of a robust
with terminal condition V r
optimization problem [4]  which can be viewed as a game against ‘nature’ (or a malicious adversary):
for every action ut chosen by the decision maker  nature selects the corresponding worst-case weight
vector from within p ∈ ∆ (qt(ξt)). By anticipating nature’s moves  the decision maker is forced
to select more conservative decisions that are less susceptible to amplifying estimation errors in the
nominal weights qt(ξt). The level of robustness of the RDDP scheme can be steered by selecting

N(cid:88)
p∈∆(qt(ξt))
t+1) ∀i

i=1

100200300400500−1−0.95−0.9−0.85−0.8NCostEstimatedcostoftrueoptimaldecisionTrueoptimalcost10th&90thpercentilesMedian100200300400500−1.2−1.1−1−0.9−0.8NCostEstimatedcostofDDPdecisionTrueoptimalcost10th&90thpercentilesMedian100200300400500−1−0.95−0.9−0.85−0.8NCostEstimatedcostofRDDPdecisionTrueoptimalcost10th&90thpercentilesMedianthe parameter γ. We suggest to choose γ large enough such that the envelope of all conditional
CDFs of ξt+1 implied by the weight vectors in ∆(qt(ξt)) covers the true conditional CDF with high
conﬁdence (Figure 2). The following example illustrates the potential beneﬁts of the RDDP scheme.
Example 3.2 Consider again Example 3.1. Assuming that only the samples {ξi
i=1 are
known  we can compute a worst-case optimal decision using (6). Fixing this decision  we can then
use (2) to estimate its expected cost conditional on ξt = 1. Note that this cost is generically different
t (st  1). Figure 1 (right) shows that the resulting cost estimator is less noisy and—perhaps
from V r
surprisingly—unbiased. Thus  it clearly dominates V d
t (st  1) as an estimator for the true cost-to-go
value Vt(st  1) (which is not accessible in reality as it relies on full distributional information).

t+1}N

t  ξi

Robust optimization models with uncertainty sets of the type (5) have previously been studied in [23 
24]. However  these static models are fundamentally different in scope from our RDDP formulation.
RDDP seeks the worst-case probabilities of N historical samples of the exogenous state  using the
NW weights as nominal probabilities. In contrast  the static models in [23  24] rely on a partition
of the uncertainty space into N bins. Worst-case probabilities are then assigned to the bins  whose
nominal probabilities are given by the empirical frequencies. This latter approach does not seem to
extend easily to our dynamic setting as it would be unclear where in each bin one should evaluate
the cost-to-go functions.
Instead of immunizing the DDP scheme against estimation
errors in the conditional probabilities (as advocated here) 
one could envisage other regularizations to mitigate the
overﬁtting phenomena. For instance  one could construct
an uncertainty set for (ξi
i=1 and seek control actions
that are optimal in view of the worst-case sample points
within this set. However  this approach would lead to a
harder robust optimization problem  where the search space
of the inner maximization has dimension O(N d2) (as op-
posed to O(N ) for RDDP). Moreover  this approach would
only be tractable if V r
t+1 displayed a very regular (e.g.  lin-
ear or quadratic) dependence on ξt+1. RDDP imposes no
such restrictions on the cost-to-go function; see § 4.

Figure 2: Envelope of all conditional
CDFs implied by weight vectors in
∆(qt(ξt)).

t+1)N

4 Computational solution procedure

In this section we demonstrate that RDDP is computationally tractable under a convexity assumption
and if we approximate the dependence of the cost-to-go functions on the endogenous state through
a piecewise linear or quadratic approximation architecture. This result immediately extends to the
DDP scheme of § 2 as the uncertainty set (5) collapses to a singleton for γ = 0.
Assumption 4.1 For all t = 1  . . .   T   the cost function ct is convex quadratic in (st  ut)  the
transition function gt is afﬁne in (st  ut)  and the feasible set Ut is second-order conic representable.
Under Assumption 4.1  V r

t (st  ξt) can be evaluated by solving a convex optimization problem.

Theorem 4.1 Suppose that Assumption 4.1 holds and that the cost-to-go function V r
the endogenous state. Then  (6) reduces to the following convex minimization problem.
1

V r
t (st  ξt) = min

(cid:124)
ct(st  ξt  ut) + λγ − µ − 2qt(ξt)
z  y ∈ RN

y + 2λqt(ξt)

t+1 is convex in

(cid:124)

s. t. ut ∈ Ut  µ ∈ R 
V r
t+1(gt(st  ut  ξi
zi + µ ≤ λ 

(cid:113)

λ ∈ R+ 
t+1) ≤ zi ∀i

t+1)  ξi

i + (zi + µ)2 ≤ 2λ − zi − µ ∀i

4y2

(7)

t (st  ξt) is convex in st whenever V r

Corollary 4.1 If Assumption 4.1 holds  then RDDP preserves convexity in the exogenous state.
Thus  V r

t+1(st+1  ξt+1) is convex in st+1.
Note that problem (7) becomes a tractable second-order cone program if V r
t+1 is convex piecewise
linear or convex quadratic in st+1. Then  it can be solved efﬁciently with interior point algorithms.

5

01ProbabilityTrueCDFNadaraya−WatsonCDFEnvelopeofimpliedCDFsAlgorithm 1: Robust data-driven dynamic programming

Inputs: Sample trajectories {sk
t }T
t=1 for k = 1  . . .   K;
t}T +1
observation histories {ξi
t=1 for i = 1  . . .   N.
T +1(·  ξi
for all i = 1  . . .   N do

Initialization: Let ˆV r
for all t = T  . . .   1 do

for all k = 1  . . .   K do

T +1) be the zero function for all i = 1  . . .   N.

Let ˆV r

end for
Construct ˆV r

t k i be the optimal value of problem (7) with input ˆV r
t k i)}K

t) from the interpolation points {(sk

t (·  ξi

t   ˆV r

t+1(·  ξj

t+1) ∀j.

k=1 as in (8a) or (8b).

end for

end for
Outputs: Approximate cost-to-go functions ˆV r

t (·  ξi

t) for i = 1  . . .   N and t = 1  . . .   T .

t }T

We now describe an algorithm that computes all cost-to-go functions {V r
t }T
t=1 approximately. Ini-
t}T
tially  we collect historical observation trajectories of the exogenous state {ξi
t=1  i = 1  . . .   N 
and generate sample trajectories of the endogenous state {sk
t=1  k = 1  . . .   K  by simulating the
evolution of st under a prescribed control policy along randomly selected exogenous state trajecto-
ries. Best results are achieved if the sample-generating policy is near-optimal. If no near-optimal
policy is known  an initial naive policy can be improved sequentially in a greedy fashion. The core
of the algorithm computes approximate value functions ˆV r
t   which are piecewise linear or quadratic
in st  by backward induction on t. Iteration t takes ˆV r
t+1 as an input and computes the optimal value
ˆV r
t). For any ﬁxed i we then
t k i of the second-order cone program (7) for each sample state (sk
construct the function ˆV r
t   ˆV r
k=1. If the endogenous
state is univariate (d1 = 1)  the following piecewise linear approximation is used.
t − sk−1

t) from the interpolation points {(sk

t k−1 i + (st − sk−1

t − st)/(sk
(sk

t   ξi
t k i)}K

t − sk−1

ˆV r
t (st  ξi

t (·  ξi

t) = max

)/(sk

(8a)

) ˆV r

) ˆV r

t k i

t

t

t

k

In the multivariate case (d1 > 1)  we aim to ﬁnd the convex quadratic function ˆV r
t) =
(cid:124)
(cid:124)
i st + mi that best explains the given interpolation points in a least-squares sense.
t Mist + 2m
s
This quadratic function can be computed efﬁciently by solving the following semideﬁnite program.

t (st  ξi

min
s. t. Mi ∈ Sd1   Mi (cid:60) 0  mi ∈ Rd1   mi ∈ R

t k i

k=1

(cid:124)
i sk
t + 2m

t + mi − ˆV r

Misk

(sk
t )

(cid:124)

(8b)

(cid:88)K

(cid:104)

(cid:105)2

t (·  ξi

Quadratic approximation architectures of the above type ﬁrst emerged in approximate dynamic pro-
gramming [5]. Once the function ˆV r
t) is computed for all i = 1  . . .   N  the algorithm proceeds
to iteration t − 1. A summary of the overall procedure is provided in Algorithm 1.
Remark 4.1 The RDDP algorithm remains valid if the feasible set Ut depends on the state (st  ξt)
or if the control action ut includes components that are of the ‘here-and-now’-type (i.e.  they are
chosen before ξt+1 is observed) as well as others that are of the ‘wait-and-see’-type (i.e.  they are
chosen after ξt+1 has been revealed). In this setting  problem (7) becomes a two-stage stochastic
program [25] but remains efﬁciently solvable as a second-order cone program.

5 Experimental results

We evaluate the RDDP algorithm of § 4 in the context of an index tracking and a wind energy
commitment application. All semideﬁnite programs are solved with SeDuMi [26] by using the
Yalmip [27] interface  while all linear and second-order cone programs are solved with CPLEX.

5.1

Index tracking

The objective of index tracking is to match the performance of a stock index as closely as possible
with a portfolio of other ﬁnancial instruments. In our experiment  we aim to track the S&P 500

6

Statistic
Mean

Std. dev.
90th prct.
Worst case

LSPI
5.692
11.699
14.597
126.712

DDP
4.697
15.067
9.048
157.201

RDDP
1.285
2.235
2.851
18.832

Table 1: Out-of-sample statistics of sum of

squared tracking errors in(cid:104).

Figure 3: Cumulative distribution function of
sum of squared tracking errors.

index with a combination of the NASDAQ Composite  Russell 2000  S&P MidCap 400  and AMEX
Major Market indices. We set the planning horizon to T = 20 trading days (1 month).
Let st ∈ R+ be the value of the current tracking portfolio relative to the value of S&P 500 on day t 
while ξt ∈ R5
+ denotes the vector of the total index returns (price relatives) from day t − 1 to day t.
The ﬁrst component of ξt represents the return of S&P 500. The objective of index tracking is to
maintain st close to 1 in a least-squares sense throughout the planning horizon  which gives rise to
the following dynamic program with terminal condition VT +1 ≡ 0.

Vt(st  ξt) = min (1 − st)2 + E[Vt(st+1  ξt+1)|ξt]
u1 = 0 

s. t. u ∈ R5
+ 

u = st 

(cid:124)
1

st+1 = ξt+1

(cid:124)

u/ξt+1 1

(9)

Here  ui/st can be interpreted as the portion of the tracking portfolio that is invested in index i on
day t. Our computational experiment is based on historical returns of the indices over 5440 days
from 26-Aug-1991 to 8-Mar-2013 (272 trading months). We solve the index tracking problem using
the DDP and RDDP algorithms (i.e.  the algorithm of § 4 with γ = 0 and γ = 10  respectively)
as well as least-squares policy iteration (LSPI) [10]. As the endogenous state is univariate  DDP
and RDDP employ the piecewise linear approximation architecture (8a). LSPI solves an inﬁnite-
horizon variant of problem (9) with discount factor λ = 0.9  polynomial basis features of degree
3 and a discrete action space comprising 1 000 points sampled uniformly from the true continuous
action space. We train the algorithms on the ﬁrst 80 and test on the remaining 192 trading months.
Table 1 reports several out-of-sample statistics of the sum of squared tracking errors. We ﬁnd that
RDDP outperforms DDP and LSPI by a factor of 4-5 in view of the mean  the standard deviation
and the 90th percentile of the error distribution  and it outperforms the other algorithms by an order
of magnitude in view of the worst-case (maximum) error. Figure 3 further shows that the error
distribution generated by RDDP stochastically dominates those generated by DDP and LSPI.

5.2 Wind energy commitment

Next  we apply RDDP to the wind energy commitment problem proposed in [28  29]. On every
day t  a wind energy producer chooses the energy commitment levels xt ∈ R24
+ for the next 24

Site Statistic
Mean

NC

OH

Std. dev.
10th prct.
Worst case

Mean

Std. dev.
10th prct.
Worst case

Persistence DDP
4.698
6.338
-1.463
-22.666
4.104
5.548
0.118
-21.317

4.039
3.964
0.524
-11.221
2.746
3.428
0.154
-12.065

RDDP
7.549
5.133
1.809
0.481
5.510
4.500
1.395
0.280

Table 2: Out-of-sample statistics of proﬁt (in
$100 000).

Figure 4: Out-of-sample proﬁt distribution for
the North Carolina site.

7

05101520253000.20.40.60.81Sum of squared tracking errors (in ‰)Probability  LSPIDDPRDDP−20−10010203000.20.40.60.81Profit (in $100 000)Probability  PersistenceDDPRDDPhours. The day-ahead prices πt ∈ R24
+ per unit of energy committed are known at the beginning
of the day. However  the hourly amounts of wind energy ωt+1 ∈ R24
+ generated over the day
are uncertain. If the actual production falls short of the commitment levels  there is a penalty of
twice the respective day-ahead price for each unit of unsatisﬁed demand. The wind energy producer
also operates three storage devices indexed by l ∈ {1  2  3}  each of which can have a different
capacity sl  hourly leakage ρl  charging efﬁciency ρl
d. We denote
by sl
+ the hourly ﬁlling levels of storage l over the next 24 hours. The wind producer’s
objective is to maximize the expected proﬁt over a short-term planning horizon of T = 7 days.
The endogenous state is given by the storage levels at the end of day t  st = {sl
the exogenous state comprises the day-ahead prices πt ∈ R24
ωt ∈ R24
The best bidding and storage strategy can be found by solving the dynamic program

+  while
+ and the wind energy production levels
+ of day t − 1  which are revealed to the producer on day t. Thus  we set ξt = (πt  ωt).

c and discharging efﬁciency ρl

t+1 ∈ R24

l=1 ∈ R3

t 24}3

Vt(st  ξt) = max π

s. t. xt  e

(cid:124)
(cid:124)
t+1|ξt] + E[Vt+1(st+1  ξt+1)|ξt]
t xt − 2π
E[eu
t
{c w u}
{+ −} l
∈ R24
∀l
e
+  
t+1
t+1
t+1 h + e+ 1
t+1 h + ew

t+1 ∈ R24
  sl
t+1 h + e+ 2
t+1 h + e+ 3

+

t+1 h ∀h

ωt+1 h = ec
xt h = ec

(10)

e

t+1 h + e

− l
t+1 h 

t+1 h−1 + ρl

sl
t+1 h = ρlsl

− 1
t+1 h + e

− 2
− 3
t+1 h + eu
t+1 h + e
t+1 h − 1
ce+ l
ρl
d

t+1 h ∀h
t+1 h ≤ sl ∀h  l
sl
with terminal condition VT +1 ≡ 0. Here  we adopt the convention that sl
t 24 for all l.
t+1 0 = sl
Besides the usual here-and-now decisions xt  the decision vector ut now also includes wait-and-see
decisions that are chosen after ξt+1 has been revealed (see Remark 4.1): ec represents the amount
of wind energy used to meet the commitment  e+ l represents the amount of wind energy fed into
storage l  e− l represents the amount of energy from storage l used to meet the commitment  ew rep-
resents the amount of wind energy that is wasted  and eu represents the unmet energy commitment.
Our computational experiment is based on day-ahead prices for the PJM market and wind speed data
for North Carolina (33.9375N  77.9375W) and Ohio (41.8125N  81.5625W) from 2002 to 2011 (520
weeks). As ξt is a 48 dimensional vector with high correlations between its components  we perform
principal component analysis to obtain a 6 dimensional subspace that explains more than 90% of
the variability of the historical observations. The conditional probabilities qt(ξt) are subsequently
estimated using the projected data points. The parameters for the storage devices are taken from
[30]. We solve the wind energy commitment problem using the DDP and RDDP algorithms (i.e. 
the algorithm of § 4 with γ = 0 and γ = 1  respectively) as well as a persistence heuristic that naively
pledges the wind generation of the previous day by setting xt = ωt. Persistence was proposed as
a useful baseline in [28]. Note that problem (10) is beyond the scope of traditional reinforcement
learning algorithms due to the high dimensionality of the action spaces and the seasonalities in
the wind and price data. We train DDP and RDDP on the ﬁrst 260 weeks and test the resulting
commitment strategies as well as the persistence heuristic on the last 260 weeks of the data set.
Table 2 reports the test statistics of the different algorithms. We ﬁnd that the persistence heuristic
wins in terms of standard deviation  while RDDP wins in all other categories. However  the higher
standard deviation of RDDP can be explained by a heavier upper tail (which is indeed desirable).
Moreover  the proﬁt distribution generated by RDDP stochastically dominates those generated by
DDP and the persistence heuristic; see Figure 4. Another major beneﬁt of RDDP is that it cuts off
any losses (negative proﬁts)  whereas all other algorithms bear a signiﬁcant risk of incurring a loss.

Concluding remarks The proposed RDDP algorithm combines ideas from robust optimization 
reinforcement learning and approximate dynamic programming. We remark that the N K convex
optimization problems arising in each backward induction step are independent of each other and
thus lend themselves to parallel implementation. We also emphasize that Assumption 4.1 could be
relaxed to allow ct and gt to display a general nonlinear dependence on st. This would invalidate
Corollary 4.1 but not Theorem 4.1. If one is willing to accept a potentially larger mismatch between
the true nonconvex cost-to-go function and its convex approximation architecture  then Algorithm 1
can even be applied to speciﬁc motor control  vehicle control or other nonlinear control problems.
Acknowledgments: This research was supported by EPSRC under grant EP/I014640/1.

8

References
[1] D.P. Bertsekas. Dynamic Programming and Optimal Control  Vol. II. Athena Scientiﬁc  3rd edition  2007.
[2] W. Powell. Approximate Dynamic Programming: Solving the Curses of Dimensionality. Wiley-Blackwell 

2007.

[3] L. Devroye. The uniform convergence of the nadaraya-watson regression function estimate. Canadian

Journal of Statistics  6(2):179–191  1978.

[4] A. Ben-Tal  L. El Ghaoui  and A. Nemirovski. Robust Optimization. Princeton University Press  2009.
[5] A. Keshavarz and S. Boyd. Quadratic approximate dynamic programming for input-afﬁne systems. In-

ternational Journal of Robust and Nonlinear Control  2012. Forthcoming.

[6] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matri-

ces. Operations Research  53(5):780–798  2005.

[7] G. Iyengar. Robust dynamic programming. Mathematics of Operations Research  30(2):257–280  2005.
[8] S. Mannor  O. Mebel  and H. Xu. Lightning does not strike twice: Robust MDPs with coupled uncertainty.

In Proceedings of the 29th International Conference on Machine Learning  pages 385–392  2012.

[9] W. Wiesemann  D. Kuhn  and B. Rustem. Robust Markov decision processes. Mathematics of Operations

Research  38(1):153–183  2013.

[10] M.G. Lagoudakis and R. Parr. Least-squares policy iteration. The Journal of Machine Learning Research 

4:1107–1149  2003.

[11] D.P. Bertsekas. Approximate policy iteration: A survey and some new methods. Journal of Control

Theory and Applications  9(3):310–335  2011.

[12] C.E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning.

Information Processing Systems  pages 751–759  2004.

In Advances in Neural

[13] L. Bu¸soniu  A. Lazaric  M. Ghavamzadeh  R. Munos  R. Babuška  and B. De Schutter. Least-squares

methods for policy iteration. In Reinforcement Learning  pages 75–109. Springer  2012.

[14] X. Xu  T. Xie  D. Hu  and X. Lu. Kernel least-squares temporal difference learning. International Journal

of Information Technology  11(9):54–63  2005.

[15] Y. Engel  S. Mannor  and R. Meir. Reinforcement learning with Gaussian processes. In Proceedings of

the 22nd International Conference on Machine Learning  pages 201–208  2005.

[16] G. Taylor and R. Parr. Kernelized value function approximation for reinforcement learning. In Proceed-

ings of the 26th International Conference on Machine Learning  pages 1017–1024  2009.

[17] E.A. Nadaraya. On estimating regression. Theory of Probability & its Applications  9(1):141–142  1964.
[18] G.S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics  Series A  26(4):359–

372  1964.

[19] B. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall/CRC  1986.
[20] L. Hannah  W. Powell  and D. Blei. Nonparametric density estimation for stochastic optimization with an
observable state variable. In Advances in Neural Information Processing Systems  pages 820–828  2010.

[21] A. Cybakov. Introduction to Nonparametric Estimation. Springer  2009.
[22] L. Pardo. Statistical Inference Based on Divergence Measures  volume 185 of Statistics: A Series of

Textbooks and Monographs. Chapman and Hall/CRC  2005.

[23] Z. Wang  P.W. Glynn  and Y. Ye. Likelihood robust optimization for data-driven newsvendor problems.

Technical report  Stanford University  2009.

[24] A. Ben-Tal  D. den Hertog  A. De Waegenaere  B. Melenberg  and G. Rennen. Robust solutions of

optimization problems affected by uncertain probabilities. Management Science  59(2):341–357  2013.

[25] A. Shapiro  D. Dentcheva  and A. Ruszczy´nski. Lectures on Stochastic Programming: Modeling and

Theory. SIAM  2009.

[26] J.F. Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones. Optimiza-

tion Methods and Software  11-12:625–654  1999.

[27] J. Löfberg. YALMIP : A toolbox for modeling and optimization in MATLAB. In Proceedings of the

CACSD Conference  2004.

[28] L. Hannah and D. Dunson. Approximate dynamic programming for storage problems. In Proceedings of

the 28th International Conference on Machine Learning  pages 337–344  2011.

[29] J.H. Kim and W.B. Powell. Optimal energy commitments with storage and intermittent supply. Opera-

tions Research  59(6):1347–1360  2011.

[30] M. Kraning  Y. Wang  E. Akuiyibo  and S. Boyd. Operation and conﬁguration of a storage portfolio via

convex optimization. In Proceedings of the IFAC World Congress  pages 10487–10492  2011.

9

,Grani Adiwena Hanasusanto
Daniel Kuhn
Aurelien Garivier
Tor Lattimore
Emilie Kaufmann