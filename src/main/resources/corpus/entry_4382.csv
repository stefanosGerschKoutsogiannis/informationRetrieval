2014,Predictive Entropy Search for Efficient Global Optimization of Black-box Functions,We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration  PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore  PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications  including optimization problems in machine learning  finance  biotechnology  and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.,Predictive Entropy Search for Efﬁcient Global

Optimization of Black-box Functions

Jos´e Miguel Hern´andez-Lobato

jmh233@cam.ac.uk
University of Cambridge

Matthew W. Hoffman
mwh30@cam.ac.uk
University of Cambridge

Zoubin Ghahramani

zoubin@eng.cam.ac.uk

University of Cambridge

Abstract

We propose a novel information-theoretic approach for Bayesian optimization
called Predictive Entropy Search (PES). At each iteration  PES selects the next
evaluation point that maximizes the expected information gained with respect to
the global maximum. PES codiﬁes this intractable acquisition function in terms
of the expected reduction in the differential entropy of the predictive distribu-
tion. This reformulation allows PES to obtain approximations that are both more
accurate and efﬁcient than other alternatives such as Entropy Search (ES). Fur-
thermore  PES can easily perform a fully Bayesian treatment of the model hy-
perparameters while ES cannot. We evaluate PES in both synthetic and real-
world applications  including optimization problems in machine learning  ﬁnance 
biotechnology  and robotics. We show that the increased accuracy of PES leads to
signiﬁcant gains in optimization performance.

1

Introduction

Bayesian optimization techniques form a successful approach for optimizing black-box functions
[5]. The goal of these methods is to ﬁnd the global maximizer of a nonlinear and generally non-
convex function f whose derivatives are unavailable. Furthermore  the evaluations of f are usually
corrupted by noise and the process that queries f can be computationally or economically very
expensive. To address these challenges  Bayesian optimization devotes additional effort to model-
ing the unknown function f and its behavior. These additional computations aim to minimize the
number of evaluations that are needed to ﬁnd the global optima.
Optimization problems are widespread in science and engineering and as a result so are Bayesian
approaches to this problem. Bayesian optimization has successfully been used in robotics to adjust
the parameters of a robot’s controller to maximize gait speed and smoothness [16] as well as param-
eter tuning for computer graphics [6]. Another example application in drug discovery is to ﬁnd the
chemical derivative of a particular molecule that best treats a given disease [20]. Finally  Bayesian
optimization can also be used to ﬁnd optimal hyper-parameter values for statistical [29] and machine
learning techniques [24].
As described above  we are interested in ﬁnding the global maximizer x(cid:63) = arg maxx∈X f (x) of
a function f over some bounded domain  typically X ⊂ Rd. We assume that f (x) can only be
evaluated via queries to a black-box that provides noisy outputs of the form yi ∼ N (f (xi)  σ2).
We note  however  that our framework can be extended to other non-Gaussian likelihoods. In this
setting  we describe a sequential search algorithm that  after n iterations  proposes to evaluate f at
some location xn+1. To make this decision the algorithm conditions on all previous observations

Dn = {(x1  y1)  . . .   (xn  yn)}. After N iterations the algorithm makes a ﬁnal recommendation(cid:101)xN
latent function f to guide the search and to select(cid:101)xN . In this work we use a zero-mean Gaussian

for the global maximizer of the latent function f.
We take a Bayesian approach to the problem described above and use a probabilistic model for the

1

Algorithm 1 Generic Bayesian optimization
Input: a black-box with unknown mean f
1: for n = 1  . . .   N do
2:
3:
4:
5: end for

select xn = arg maxx∈X αn−1(x)
query the black-box at xn to obtain yn
augment data Dn = Dn−1 ∪ {(xn  yn)}

6: return (cid:101)xN = arg maxx∈X µN (x)

Algorithm 2 PES acquisition function
Input: a candidate x; data Dn
1: sample M hyperparameter values {ψ(i)}
2: for i = 1  . . .   M do
3:
4:
5:
6:
7: end for
8: return αn(x) as in (10)

sample f (i) ∼ p(f|Dn  φ  ψ(i))
0 and (cid:101)m(i) (cid:101)v(i)
(cid:63) ← arg maxx∈X f (i)(x)
set x(i)
compute m(i)
n (x|x(i)
compute v(i)
(cid:63) )

0   V(i)
n (x) and v(i)

p
r
e
c
o
m
p
u
t
e
d

process (GP) prior for f [22]. This prior is speciﬁed by a positive-deﬁnite kernel function k(x  x(cid:48)).
Given any ﬁnite collection of points {x1  . . .   xn}  the values of f at these points are jointly zero-
mean Gaussian with covariance matrix Kn  where [Kn]ij = k(xi  xj). For the Gaussian likelihood
described above  the vector of concatenated observations yn is also jointly Gaussian with zero-mean.
Therefore  at any location x  the latent function f (x) conditioned on past observations Dn is then
Gaussian with marginal mean µn(x) and variance vn(x) given by

µn(x) = kn(x)T(Kn + σ2I)−1yn  

vn(x) = k(x  x) − kn(x)T(Kn + σ2I)−1kn(x)  

(1)

where kn(x) is a vector of cross-covariance terms between x and {x1  . . .   xn}.
Bayesian optimization techniques use the above predictive distribution p(f (x)|Dn) to guide the
search for the global maximizer x(cid:63). In particular  p(f (x)|Dn) is used during the computation of an
acquisition function αn(x) that is optimized at each iteration to determine the next evaluation loca-
tion xn+1. This process is shown in Algorithm 1. Intuitively  the acquisition function αn(x) should
be high in areas where the maxima is most likely to lie given the current data. However  αn(x)

should also encourage exploration of the search space to guarantee that the recommendation(cid:101)xN is a

global optimum of f  not just a global optimum of the posterior mean. Several acquisition functions
have been proposed in the literature. Some examples are the probability of improvement (PI) [14] 
the expected improvement (EI) [19  13] or upper conﬁdence bounds (UCB) [26]. Alternatively  one
can combine several of these acquisition functions [10].
The acquisition functions described above are based on probabilistic measures of improvement (PI
an EI) or on optimistic estimates of the latent function (UCB) which implicitly trade off between
exploiting the posterior mean and exploring based on the uncertainty. An alternate approach  in-
troduced by [28]  proposes maximizing the expected posterior information gain about the global
maximizer x(cid:63) evaluated over a grid in the input space. A similar strategy was later employed by [9]
which although it requires no such grid  instead relies on a difﬁcult-to-evaluate approximation. In
Section 2 we derive a rearrangement of this information-based acquisition function which leads to a
more straightforward approximation that we call Predictive Entropy Search (PES). In Section 3 we
show empirically that our approximation is more accurate than that of [9]. We evaluate this claim on
both synthetic and real-world problems and further show that this leads to real gains in performance.

2 Predictive entropy search

We propose to follow the information-theoretic method for active data collection described in [17].
We are interested in maximizing information about the location x(cid:63) of the global maximum  whose
posterior distribution is p(x(cid:63)|Dn). Our current information about x(cid:63) can be measured in terms
of the negative differential entropy of p(x(cid:63)|Dn). Therefore  our strategy is to select xn+1 which
maximizes the expected reduction in this quantity. The corresponding acquisition function is

where H[p(x)] = −(cid:82) p(x) log p(x)dx represents the differential entropy of its argument and the

αn(x) = H[p(x(cid:63)|Dn)] − Ep(y|Dn x)[H[p(x(cid:63)|Dn ∪ {(x  y)})]]  

(2)

expectation above is taken with respect to the posterior predictive distribution of y given x. The
exact evaluation of (2) is infeasible in practice. The main difﬁculties are i) p(x(cid:63)|Dn ∪ {(x  y)})
must be computed for many different values of x and y during the optimization of (2) and ii) the
entropy computations themselves are not analytical. In practice  a direct evaluation of (2) is only

2

possible after performing many approximations [9]. To avoid this  we follow the approach described
in [11] by noting that (2) can be equivalently written as the mutual information between x(cid:63) and y
given Dn. Since the mutual information is a symmetric function  αn(x) can be rewritten as

αn(x) = H[p(y|Dn  x)] − Ep(x(cid:63)|Dn)[H[p(y|Dn  x  x(cid:63))]]  

(3)
where p(y|Dn  x  x(cid:63)) is the posterior predictive distribution for y given the observed data Dn and
the location of the global maximizer of f. Intuitively  conditioning on the location x(cid:63) pushes the
posterior predictions up in locations around x(cid:63) and down in regions away from x(cid:63). Note that  unlike
the previous formulation  this objective is based on the entropies of predictive distributions  which
are analytic or can be easily approximated  rather than on the entropies of distributions on x(cid:63) whose
approximation is more challenging.
The ﬁrst term in (3) can be computed analytically using the posterior marginals for f (x) in (1)  that
is  H[p(y|Dn  x)] = 0.5 log[2πe (vn(x) + σ2)]  where we add σ2 to vn(x) because y is obtained
by adding Gaussian noise with variance σ2 to f (x). The second term  on the other hand  must be
approximated. We ﬁrst approximate the expectation in (3) by averaging over samples x(i)
(cid:63) drawn
approximately from p(x(cid:63)|Dn). For each of these samples  we then approximate the corresponding
entropy function H[p(y|Dn  x  x(i)
(cid:63) )] using expectation propagation [18]. The code for all these
operations is publicly available at http://jmhl.org.

2.1 Sampling from the posterior over global maxima

In this section we show how to approximately sample from the conditional distribution of the global
maximizer x(cid:63) given the observed data Dn  that is 

p(x(cid:63)|Dn) = p(cid:0)f (x(cid:63)) = max

x∈X f (x)(cid:12)(cid:12)Dn

(cid:1) .

j≤m

written as(cid:82) p(f|Dn)(cid:81)

(4)
If the domain X is restricted to some ﬁnite set of m points  the latent function f takes the form
of an m-dimensional vector f. The probability that the ith element of f is optimal can then be
I[fi ≥ fj] df. This suggests the following generative process: i) draw
a sample from the posterior distribution p(f|Dn) and ii) return the index of the maximum element
in the sampled vector. This process is known as Thompson sampling or probability matching when
used as an arm-selection strategy in multi-armed bandits [8]. This same approach could be used for
sampling the maximizer over a continuous domain X . At ﬁrst glance this would require constructing
an inﬁnite-dimensional object representing the function f. To avoid this  one could sequentially
construct f while it is being optimized. However  evaluating such an f would ultimately have cost
O(m3) where m is the number of function evaluations necessary to ﬁnd the optimum. Instead 
we propose to sample and optimize an analytic approximation to f. We will brieﬂy derive this
approximation below  but more detail is given in Appendix A.
Given a shift-invariant kernel k  Bochner’s theorem [4] asserts the existence of its Fourier dual s(w) 
which is equal to the spectral density of k. Letting p(w) = s(w)/α be the associated normalized
density  we can write the kernel as the expectation

where b ∼ U[0  2π]. Let φ(x) =(cid:112)2α/m cos(Wx + b) denote an m-dimensional feature mapping

k(x  x(cid:48)) = α Ep(w)[e−iwT(x−x(cid:48))] = 2α Ep(w b)[cos(wTx + b) cos(wTx(cid:48) + b)]  

where W and b consist of m stacked samples from p(w  b). The kernel k can then be approximated
by the inner product of these features  k(x  x(cid:48)) ≈ φ(x)Tφ(x(cid:48)). This approach was used by [21]
as an approximation method in the context of kernel methods. The feature mapping φ(x) allows
us to approximate the Gaussian process prior for f with a linear model f (x) = φ(x)Tθ where
θ ∼ N (0  I) is a standard Gaussian. By conditioning on Dn  the posterior for θ is also multivariate
Gaussian  θ|Dn ∼ N (A−1ΦTyn  σ2A−1) where A = ΦTΦ + σ2I and ΦT = [φ(x1) . . . φ(xn)].
Let φ(i) and θ(i) be a random set of features and the corresponding posterior weights sampled both
according to the generative process given above. They can then be used to construct the function
f (i)(x) = φ(i)(x)Tθ(i)  which is an approximate posterior sample of f—albeit one with a ﬁnite
parameterization. We can then maximize this function to obtain x(i)
(cid:63) = arg maxx∈X f (i)(x)  which
is approximately distributed according to p(x(cid:63)|Dn). Note that for early iterations when n < m  we
can efﬁciently sample θ(i) with cost O(n2m) using the method described in Appendix B.2 of [23].
This allows us to use a large number of features in φ(i)(x).

(5)

3

H in this expression as p(y|Dn  x  x(cid:63)) =(cid:82) p(y|f (x))p(f (x)|Dn  x(cid:63)) df (x). Here p(f (x)|Dn  x(cid:63))

2.2 Approximating the predictive entropy
We now show how to approximate H[p(y|Dn  x  x(cid:63))] in (3). Note that we can write the argument to
is the posterior distribution on f (x) given Dn and the location x(cid:63) of the global maximizer of f.
When the likelihood p(y|f (x)) is Gaussian  we have that p(f (x)|Dn) is analytically tractable since it
is the predictive distribution of a Gaussian process. However  by further conditioning on the location
x(cid:63) of the global maximizer we are introducing additional constraints  namely that f (z) ≤ f (x(cid:63))
for all z ∈ X . These constraints make p(f (x)|Dn  x(cid:63)) intractable. To circumvent this difﬁculty  we
instead use the following simpliﬁed constraints:

C1. x(cid:63) is a local maximum. This is achieved by letting ∇f (x(cid:63)) = 0 and ensuring that
∇2f (x(cid:63)) is negative deﬁnite. We further assume that the non-diagonal elements of
∇2f (x(cid:63))  denoted by upper[∇2f (x(cid:63))]  are known  for example they could all be zero.
This simpliﬁes the negative-deﬁnite constraint. We denote by C1.1 the constraint given by
∇f (x(cid:63)) = 0 and upper[∇2f (x(cid:63))] = 0. We denote by C1.2 the constraint that forces the
elements of diag[∇2f (x(cid:63))] to be negative.
C2. f (x(cid:63)) is larger than past observations. We also assume that f (x(cid:63)) ≥ f (xi) for all
i ≤ n. However  we only observe f (xi) noisily via yi. To avoid making inference on these
latent function values  we approximate the above hard constraints with the soft constraint
f (x(cid:63)) > ymax +   where  ∼ N (0  σ2) and ymax is the largest yi seen so far.
rather than requiring f (x(cid:63)) ≤ f (z) for all z ∈ X .

C3. f (x) is smaller than f (x(cid:63)). This simpliﬁed constraint only conditions on the given x

We incorporate these simpliﬁed constraints into p(f (x)|Dn) to approximate p(f (x)|Dn  x(cid:63)). This
is achieved by multiplying p(f (x)|Dn) with speciﬁc factors that encode the above constraints. In
what follows we brieﬂy show how to construct these factors; more detail is given in Appendix B.
Consider the latent variable z = [f (x(cid:63)); diag[∇2f (x(cid:63))]]. To incorporate constraint C1.1 we
can condition on the data and on the “observations” given by the constraints ∇f (x(cid:63)) = 0 and
upper[∇2f (x(cid:63))] = 0. Since f is distributed according to a GP  the joint distribution between z and
these observations is multivariate Gaussian. The covariance between the noisy observations yn and
the extra noise-free derivative observations can be easily computed [25]. The resulting conditional
distribution is also multivariate Gaussian with mean m0 and covariance V0. These computations
are similar to those performed in (1). Constraints C1.2 and C2 can then be incorporated by writing

(cid:104)(cid:81)d

i=1

I(cid:0)[∇2f (x(cid:63))]ii ≤ 0(cid:1)(cid:105)N (z|m0  V0)  

p(z|Dn  C1  C2) ∝ Φσ2(f (x(cid:63)) − ymax)

(6)

where Φσ2 is the cdf of a zero-mean Gaussian distribution with variance σ2. The ﬁrst new factor in
this expression guarantees that f (x(cid:63)) > ymax +   where we have marginalized  out  and the second
set of factors guarantees that the entries in diag[∇2f (x(cid:63))] are negative.
Later integrals that make use of p(z|Dn  C1  C2)  however  will not admit a closed-form expres-
sion. As a result we compute a Gaussian approximation q(z) to this distribution using Expectation
Propagation (EP) [18]. The resulting algorithm is similar to the implementation of EP for binary
classiﬁcation with Gaussian processes [22]. EP approximates each non-Gaussian factor in (6) with

a Gaussian factor whose mean and variance are (cid:101)mi and(cid:101)vi  respectively. The EP approximation can
then be written as q(z) ∝ [(cid:81)d+1
i=1 N (zi|(cid:101)mi (cid:101)vi)]N (z|m0  V0). Note that these computations have so
far not depended on x  so we can compute {m0  V0 (cid:101)m (cid:101)v} once and store them for later use  where
(cid:101)m = ( ˜m1  . . .   ˜md+1) and(cid:101)v = (˜v1  . . .   ˜vd+1).

We will now describe how to compute the predictive variance of some latent function value f (x)
given these constraints. Let f = [f (x); f (x(cid:63))] be a vector given by the concatenation of the values
of the latent function at x and x(cid:63). The joint distribution between f  z  the evaluations yn collected
so far and the derivative “observations” ∇f (x(cid:63)) = 0 and upper[∇2f (x(cid:63))] = 0 is multivariate
Gaussian. Using q(z)  we then obtain the following approximation:

p(f|Dn  C1  C2) ≈(cid:82) p(f|z Dn  C1.1) q(z) dz = N (f|mf   Vf ) .

(7)
Implicitly we are assuming above that f depends on our observations and constraint C1.1  but is
independent of C1.2 and C2 given z. The computations necessary to obtain mf and Vf are similar

4

to those used above and in (1). The required quantities are similar to the ones used by EP to make
predictions in the Gaussian process binary classiﬁer [22]. We can then incorporate C3 by multiplying
N (f|mf   Vf ) with a factor that guarantees f (x) < f (x(cid:63)). The predictive distribution for f (x) given
Dn and all the constraints can be approximated as

p(f (x)|Dn  C1  C2  C3) ≈ Z−1(cid:82) I(f1 < f2)N (f|mf   Vf ) df2  

(8)

√

where Z is a normalization constant. The variance of the right hand size of (8) is given by

vn(x|x(cid:63)) = [Vf ]1 1 − v−1β(β + α){[Vf ]1 1 − [Vf ]1 2}2  

(9)
where v = [−1  1]TVf [−1  1]  α = m/
v  m = [−1  1]Tmf   β = φ(α)/Φ(α)  and φ(·) and
Φ(·) are the standard Gaussian density function and cdf  respectively. By further approximat-
ing (8) by a Gaussian distribution with the same mean and variance we can write the entropy as
H[p(y|Dn  x  x(cid:63))] ≈ 0.5 log[2πe(vn(x|x(cid:63)) + σ2)].
The computation of (9) can be numerically unstable when s is very close to zero. This occurs when
[Vf ]1 1 is very similar to [Vf ]1 2. To avoid these numerical problems  we multiply [Vf ]1 2 by the
largest 0 ≤ κ ≤ 1 that guarantees that s > 10−10. This can be understood as slightly reducing
the amount of dependence between f (x) and f (x(cid:63)) when x is very close to x(cid:63). Finally  ﬁxing
upper[∇2f (x(cid:63))] to be zero can also produce poor predictions when the actual f does not satisfy
this constraint. To avoid this  we instead ﬁx this quantity to upper[∇2f (i)(x(cid:63))]  where f (i) is the
ith sample function optimized in Section 2.1 to sample x(i)
(cid:63) .

2.3 Hyperparameter learning and the PES acquisition function

We now show how the previous approximations are integrated to compute the acquisition function
used by predictive entropy search (PES). This acquisition function performs a formal treatment of the
hyperparameters. Let ψ denote a vector of hyperparameters which includes any kernel parameters
as well as the noise variance σ2. Let p(ψ|Dn) ∝ p(ψ) p(Dn|ψ) denote the posterior distribution
over these parameters where p(ψ) is a hyperprior and p(Dn|ψ) is the GP marginal likelihood. For a
fully Bayesian treatment of ψ we must marginalize the acquisition function (3) with respect to this
posterior. The corresponding integral has no analytic expression and must be approximated using
Monte Carlo. This approach is also taken in [24].
We draw M samples {ψ(i)} from p(ψ|Dn) using slice sampling [27]. Let x(i)
global maximizer drawn from p(x(cid:63)|Dn  ψ(i)) as described in Section 2.1. Furthermore  let v(i)
and v(i)
model hyperparameters are ﬁxed to ψ(i). We then write the marginalized acquisition function as

(cid:63) denote a sampled
n (x)
(cid:63) ) denote the predictive variances computed as described in Section 2.2 when the

n (x|x(i)

n (x) + σ2] − 0.5 log[v(i)

n (x|x(i)

i=1

0.5 log[v(i)

(cid:63) ) + σ2]

αn(x) = 1
M

(10)
Note that PES is effectively marginalizing the original acquisition function (2) over p(ψ|Dn). This
is a signiﬁcant advantage with respect to other methods that optimize the same information-theoretic
acquisition function but do not marginalize over the hyper-parameters. For example  the approach
of [9] approximates (2) only for ﬁxed ψ. The resulting approximation is computationally very ex-
pensive and recomputing it to average over multiple samples from p(ψ|Dn) is infeasible in practice.
Algorithm 2 shows pseudo-code for computing the PES acquisition function. Note that most of the
computations necessary for evaluating (10) can be done independently of the input x  as noted in the
pseudo-code. This initial cost is dominated by a matrix inversion necessary to pre-compute V for
each hyperparameter sample. The resulting complexity is O[M (n+d+d(d−1)/2)3]. This cost can
be reduced to O[M (n + d)3] by ignoring the derivative observations imposed on upper[∇2f (x(cid:63))]
by constraint C1.1. Nevertheless  in the problems that we consider d is very small (less than 20).
After these precomputations are done  the evaluation of (10) is O[M (n + d + d(d − 1)/2)].

(cid:111)

.

(cid:80)M

(cid:110)

3 Experiments

k(x  x(cid:48)) = γ2 exp{−0.5(cid:80)

In our experiments  we use Gaussian process priors for f with squared-exponential kernels
i}. The corresponding spectral density is zero-mean Gaus-
i)2/(cid:96)2
sian with covariance given by diag([(cid:96)−2
]) and normalizing constant α = γ2. The model hyperpa-
rameters are {γ  (cid:96)1  . . .   (cid:96)d  σ2}. We use broad  uninformative Gamma hyperpriors.

i(xi− x(cid:48)

i

5

Figure 1: Comparison of different estimates of the objective function αn(x) given by (2). Left  ground truth
obtained by the rejection sampling method RS. Middle  approximation produced by the ES method. Right 
approximation produced by the proposed PES method. These plots show that the PES objective is much more
similar to the RS ground truth than the ES objective.

First  we analyze the accuracy of PES in the task of approximating the differential entropy (2).
We compare the PES approximation (10)  with the approximation used by the entropy search (ES)
method [9]. We also compare with the ground truth for (2) obtained using a rejection sampling (RS)
algorithm based on (3). For this experiment we generate the data Dn using an objective function f
sampled from the Gaussian process prior as in [9]. The domain X of f is ﬁxed to be [0  1]2 and data
are generated using γ2 = 1  σ2 = 10−6  and (cid:96)2
i = 0.1. To compute (10) we avoid sampling the
hyperparameters and use the known values directly. We further ﬁx M = 200 and m = 1000.
The ground truth rejection sampling scheme works as follows. First  X is discretized using a uniform
grid. The expectation with respect to p(x(cid:63)|Dn) in (3) is then approximated using sampling. For this 
we sample x(cid:63) by evaluating a random sample from p(f|Dn) on each grid cell and then selecting the
cell with highest value. Given x(cid:63)  we then approximate H[p(y|Dn  x  x(cid:63))] by rejection sampling.
We draw samples from p(f|Dn) and reject those whose corresponding grid cell with highest value is
not x(cid:63). Finally  we approximate H[p(y|Dn  x  x(cid:63))] by ﬁrst  adding zero-mean Gaussian noise with
variance σ2 to the the evaluations at x of the functions not rejected during the previous step and
second  we estimate the differential entropy of the resulting samples using kernels [1].
Figure 1 shows the objective functions produced by RS  ES and PES for a particular Dn with 10
measurements whose locations are selected uniformly at random in [0  1]2. The locations of the
collected measurements are displayed with an “x” in the plots. The particular objective function
used to generate the measurements in Dn is displayed in the left part of Figure 2. The plots in
Figure 1 show that the PES approximation to (2) is more similar to the ground truth given by RS
than the approximation produced by ES. In this ﬁgure we also see a discrepancy between RS and
PES at locations near x = (0.572  0.687). This difference is an artifact of the discretization used in
RS. By zooming in and drawing many more samples we would see the same behavior in both plots.
We now evaluate the performance of PES in the task of ﬁnding the optimum of synthetic black-box
objective functions. For this  we reproduce the within-model comparison experiment described in
[9]. In this experiment we optimize objective functions deﬁned in the 2-dimensional unit domain
X = [0  1]2. Each objective function is generated by ﬁrst sampling 1024 function values from the
GP prior assumed by PES  using the same γ2  (cid:96)i and σ2 as in the previous experiment. The objective
function is then given by the resulting GP posterior mean. We generated a total of 1000 objective
functions by following this procedure. The left plot in Figure 2 shows an example function.
In these experiments we compared the performance of PES with that of ES [9] and expected im-
provement (EI) [13]  a widely used acquisition function in the Bayesian optimization literature. We
again assume that the optimal hyper-parameter values are known to all methods. Predictive perfor-

mance is then measured in terms of the immediate regret (IR) |f ((cid:101)xn) − f (x(cid:63))|  where x(cid:63) is the
known location of the global maximum and(cid:101)xn is the recommendation of each algorithm had we

stopped at step n—for all methods this is given by the maximizer of the posterior mean. The right
plot in Figure 2 shows the decimal logarithm of the median of the IR obtained by each method
across the 1000 different objective functions. Conﬁdence bands equal to one standard deviation are
obtained using the bootstrap method. Note that while averaging these results is also interesting 
corresponding to the expected performance averaged over the prior  here we report the median IR

6

0.200.250.300.35xxxxxxxxxx0.2 0.2 0.2 0.25 0.25 0.25 0.25 0.25 0.25 0.3 0.35 0.000.010.020.030.040.050.060.07xxxxxxxxxx0 0.01 0.01 0.01 0.01 0.02 0.02 0.02 0.02 0.03 0.03 0.03 0.03 0.03 0.03 0.04 0.04 0.04 0.04 0.05 0.05 0.06 0.06 0.06 0.000.050.100.150.200.250.30xxxxxxxxxx0.05 0.05 0.05 0.05 0.05 0.1 0.1 0.1 0.15 0.2 0.2 0.25 0.25 Figure 2: Left  example of objective functions f. Right  median of the immediate regret (IR) for the methods
PES  ES and EI in the experiments with synthetic objective functions.

Figure 3: Median of the immediate regret (IR) for the methods EI  ES  PES and PES-NB in the experiments
with well-known synthetic benchmark functions.

because the empirical distribution of IR values is very heavy-tailed. In this case  the median is more
representative of the exact location of the bulk of the data. These results indicate that the best method
in this setting is PES  which signiﬁcantly outperforms ES and EI. The plot also shows that in this
case ES is signiﬁcantly better than EI.
We perform another series of experiments in which we optimize well-known synthetic benchmark
functions including a mixture of cosines [2] and Branin-Hoo (both functions deﬁned in [0  1]2) as
well as the Hartmann-6 (deﬁned in [0  1]6) [15]. In all instances  we ﬁx the measurement noise
to σ2 = 10−3. For both PES and EI we marginalize the hyperparameters ψ using the approach
described in Section 2.3. ES  by contrast  cannot average its approximation of (2) over the posterior
on ψ. Instead  ES works by ﬁxing ψ to an estimate of its posterior mean (obtained using slice
sampling) [27]. To evaluate the gains produced by the fully Bayesian treatment of ψ in PES  we also
compare with a version of PES (PES-NB) which performs the same non-Bayesian (NB) treatment
of ψ as ES. In PES-NB we use a single ﬁxed hyperparameter as in previous sections with value
given by the posterior mean of ψ. All the methods are initialized with three random measurements
collected using latin hypercube sampling [5].
The plots in Figure 3 show the median IR obtained by each method on each function across 250
random initializations. Overall  PES is better than PES-NB and ES. Furthermore  PES-NB is also
signiﬁcantly better than ES in most of the cases. These results show that the fully Bayesian treatment
of ψ in PES is advantageous and that PES can produce better approximations than ES. Note that
PES performs better than EI in the Branin and cosines functions  while EI is signiﬁcantly better on
the Hartmann problem. This appears to be due to the fact that entropy-based strategies explore more
aggressively which in higher-dimensional spaces takes more iterations. The Hartmann problem 
however  is a relatively simple problem and as a result the comparatively more greedy behavior of
EI does not result in signiﬁcant adverse consequences. Note that the synthetic functions optimized
in the previous experiment were much more multimodal that the ones considered here.

3.1 Experiments with real-world functions

We ﬁnally optimize different real-world cost functions. The ﬁrst one (NNet) returns the predictive
accuracy of a neural network on a random train/test partition of the Boston Housing dataset [3].

7

−2−1012xxxxxxxxxx−2 −1.5 −1.5 −1 −1 −1 −1 −1 −0.5 −0.5 −0.5 −0.5 −0.5 −0.5 −0.5 0 0 0 0 0 0 0 0 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1.5 1.5 1.5 2 ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−5.5−4.5−3.5−2.5−1.5−0.501020304050Number of Function EvaluationsLog10 Median IRMethods●●●EIESPESResults on Synthetic Cost Functions●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−3.9−2.9−1.9−0.90102030Number of Function EvaluationsLog10 Median IRMethods●●●●EIESPESPES−NBResults on Branin Cost Function●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−4.6−3.6−2.6−1.6−0.60102030Number of Function EvaluationsResults on Cosines Cost Function●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−2.7−1.7−0.701020304050Number of Function EvaluationsResults on Hartmann Cost FunctionFigure 4: Median of the immediate regret (IR) for the methods PES  PES-NB  ES and EI in the experiments
with non-analytic real-world cost functions.

The variables to optimize are the weight-decay parameter and the number of training iterations for
the neural network. The second function (Hydrogen) returns the amount of hydrogen production of
a particular bacteria in terms of the PH and Nitrogen levels of the growth medium [7]. The third
one (Portfolio) returns the ratio of the mean and the standard deviation (the Sharpe ratio) of the
1-year ahead returns generated by simulations from a multivariate time-series model that is adjusted
to the daily returns of stocks AXP  BA and HD. The time-series model is formed by univariate
GARCH models connected with a Student’s t copula [12]. These three functions (NNet  Hydrogen
and Portfolio) have as domain [0  1]2. Furthermore  in these examples  the ground truth function
that we want to optimize is unknown and is only available through noisy measurements. To obtain
a ground truth  we approximate each cost function as the predictive distribution of a GP that is
adjusted to data sampled from the original function (1000 uniform samples for NNet and Portfolio
and all the available data for Hydrogen [7]). Finally  we also consider another real-world function
that returns the walking speed of a bipedal robot [30]. This function is deﬁned in [0  1]8 and its
inputs are the parameters of the robot’s controller. In this case the ground truth function is noiseless
and can be exactly evaluated through expensive numerical simulation. We consider two versions of
this problem (Walker A) with zero-mean  additive noise of σ = 0.01 and (Walker B) with σ = 0.1.
Figure 4 shows the median IR values obtained by each method on each function across 250 random
initializations  except in Hydrogen where we used 500 due to its higher level of noise. Overall  PES 
ES and PES-NB perform similarly in NNet  Hydrogen and Portfolio. EI performs rather poorly in
these ﬁrst three functions. This method seems to make excessively greedy decisions and fails to
explore the search space enough. This strategy seems to be advantageous in Walker A  where EI
obtains the best results. By contrast  PES  ES and PES-NB tend to explore more in this latter dataset.
This leads to worse results than those of EI. Nevertheless  PES is signiﬁcantly better than PES-NB
and ES in both Walker datasets and better than EI in the noisier Walker B. In this case  the fully
Bayesian treatment of hyper-parameters performed by PES produces improvements in performance.

4 Conclusions

We have proposed a novel information-theoretic approach for Bayesian optimization. Our method 
predictive entropy search (PES)  greedily maximizes the amount of one-step information on the loca-
tion x(cid:63) of the global maximum using its posterior differential entropy. Since this objective function
is intractable  PES approximates the original objective using a reparameterization that measures
entropy in the posterior predictive distribution of the function evaluations. PES produces more ac-
curate approximations than Entropy Search (ES)  a method based on the original  non-transformed
acquisition function. Furthermore  PES can easily marginalize its approximation with respect to
the posterior distribution of its hyper-parameters  while ES cannot. Experiments with synthetic and
real-world functions show that PES often outperforms ES in terms of immediate regret. In these ex-
periments  we also observe that PES often produces better results than expected improvement (EI) 
a popular heuristic for Bayesian optimization. EI often seems to make excessively greedy deci-
sions  while PES tends to explore more. As a result  EI seems to perform better for simple objective
functions while often getting stuck with noisier objectives or for functions with many modes.

Acknowledgements J.M.H.L acknowledges support from the Rafael del Pino Foundation.

8

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−1.4−0.40.6010203040Function EvaluationsLog10 Median IRMethods●●●●EIESPESPES−NBNNet Cost●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−0.1010203040Function EvaluationsHydrogen●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−1.9−0.9010203040Function EvaluationsPortfolio●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−1.9−0.90102030Function EvaluationsWalker A−0.3●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●0102030Function EvaluationsWalker BReferences
[1] I. Ahmad and P.-E. Lin. A nonparametric estimation of the entropy for absolutely continuous distributions.

IEEE Transactions on Information Theory  22(3):372–375  1976.

[2] B. S. Anderson  A. W. Moore  and D. Cohn. A nonparametric approach to noisy and costly optimization.

In ICML  pages 17–24  2000.

[3] K. Bache and M. Lichman. UCI machine learning repository  2013.
[4] S. Bochner. Lectures on Fourier integrals. Princeton University Press  1959.
[5] E. Brochu  V. M. Cora  and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions 
with application to active user modeling and hierarchical reinforcement learning. Technical Report UBC
TR-2009-23 and arXiv:1012.2599v1  Dept. of Computer Science  University of British Columbia  2009.
[6] E. Brochu  N. de Freitas  and A. Ghosh. Active preference learning with discrete choice data. In NIPS 

pages 409–416  2007.

[7] E. H. Burrows  W.-K. Wong  X. Fern  F. W. R. Chaplen  and R. L. Ely. Optimization of ph and nitrogen
for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning
methods. Biotechnology Progress  25(4):1009–1017  2009.

[8] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In NIPS  pages 2249–2257  2011.
[9] P. Hennig and C. J. Schuler. Entropy search for information-efﬁcient global optimization. Journal of

Machine Learning Research  13  2012.

[10] M. W. Hoffman  E. Brochu  and N. de Freitas. Portfolio allocation for Bayesian optimization. In UAI 

pages 327–336  2011.

[11] N. Houlsby  J. M. Hern´andez-Lobato  F. Huszar  and Z. Ghahramani. Collaborative Gaussian processes

for preference learning. In NIPS  pages 2096–2104  2012.

[12] E. Jondeau and M. Rockinger. The copula-GARCH model of conditional dependencies: An international

stock market application. Journal of international money and ﬁnance  25(5):827–853  2006.

[13] D. R. Jones  M. Schonlau  and W. J. Welch. Efﬁcient global optimization of expensive black-box func-

tions. Journal of Global optimization  13(4):455–492  1998.

[14] H. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the presence of

noise. Journal of Basic Engineering  86  1964.

[15] D. Lizotte. Practical Bayesian Optimization. PhD thesis  University of Alberta  Canada  2008.
[16] D. Lizotte  T. Wang  M. Bowling  and D. Schuurmans. Automatic gait optimization with Gaussian process

regression. In IJCAI  pages 944–949  2007.

[17] D. J. MacKay.

Information-based objective functions for active data selection. Neural Computation 

4(4):590–604  1992.

[18] T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  Massachusetts

Institute of Technology  2001.

[19] J. Moˇckus  V. Tiesis  and A. ˇZilinskas. The application of Bayesian methods for seeking the extremum.

In L. Dixon and G. Szego  editors  Toward Global Optimization  volume 2. Elsevier  1978.

[20] D. M. Negoescu  P. I. Frazier  and W. B. Powell. The knowledge-gradient algorithm for sequencing

experiments in drug discovery. INFORMS Journal on Computing  23(3):346–363  2011.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  pages 1177–1184 

2007.

[22] C. E. Rasmussen and C. K. Williams. Gaussian processes for machine learning. The MIT Press  2006.
[23] M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of Machine

Learning Research  9:759–813  2008.

[24] J. Snoek  H. Larochelle  and R. P. Adams. Practical Bayesian optimization of machine learning algo-

rithms. In NIPS  pages 2960–2968  2012.

[25] E. Solak  R. Murray-Smith  W. E. Leithead  D. J. Leith  and C. E. Rasmussen. Derivative observations in

Gaussian process models of dynamic systems. In NIPS  pages 1057–1064  2003.

[26] N. Srinivas  A. Krause  S. M. Kakade  and M. Seeger. Gaussian process optimization in the bandit setting:

No regret and experimental design. In ICML  pages 1015–1022  2010.

[27] J. Vanhatalo  J. Riihim¨aki  J. Hartikainen  P. Jyl¨anki  V. Tolvanen  and A. Vehtari. Bayesian modeling

with Gaussian processes using the matlab toolbox GPstuff (v3.3). CoRR  abs/1206.5754  2012.

[28] J. Villemonteix  E. Vazquez  and E. Walter. An informational approach to the global optimization of

expensive-to-evaluate functions. Journal of Global Optimization  44(4):509–534  2009.

[29] Z. Wang  S. Mohamed  and N. de Freitas. Adaptive Hamiltonian and Riemann Monte Carlo samplers. In

ICML  2013.

[30] E. Westervelt and J. Grizzle. Feedback Control of Dynamic Bipedal Robot Locomotion. Control and

Automation Series. CRC PressINC  2007.

9

,José Miguel Hernández-Lobato
Matthew Hoffman
Zoubin Ghahramani