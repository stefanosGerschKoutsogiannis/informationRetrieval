2018,Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model,In this paper we consider the problem of computing an $\epsilon$-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in $O(1)$ time. Given such a DMDP with states $\states$  actions $\actions$  discount factor $\gamma\in(0 1)$  and rewards in range $[0  1]$ we provide an algorithm which computes an $\epsilon$-optimal policy with probability $1 - \delta$ where {\it both} the run time spent and number of sample taken is upper bounded by 
\[
O\left[\frac{|\cS||\cA|}{(1-\gamma)^3 \epsilon^2} \log \left(\frac{|\cS||\cA|}{(1-\gamma)\delta \epsilon}
		\right) 
		\log\left(\frac{1}{(1-\gamma)\epsilon}\right)\right] ~.
\]
For fixed values of $\epsilon$  this improves upon the previous best known bounds by a factor of $(1 - \gamma)^{-1}$ and matches the sample complexity lower bounds proved in \cite{azar2013minimax} up to logarithmic factors. 
We also extend our method to computing $\epsilon$-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.,Near-Optimal Time and Sample Complexities for

Solving Markov Decision Processes with a Generative

Model

Aaron Sidford

Stanford University

Mengdi Wang

Princeton University

Xian Wu

Stanford University

sidford@stanford.edu

mengdiw@princeton.edu

xwu20@stanford.edu

Lin F. Yang

Princeton University

lin.yang@princeton.edu

Yinyu Ye

Stanford University
yyye@stanford.edu

Abstract

In this paper we consider the problem of computing an -optimal policy of a dis-
counted Markov Decision Process (DMDP) provided we can only access its tran-
sition function through a generative sampling model that given any state-action
pair samples from the transition function in O(1) time. Given such a DMDP with
states S  actions A  discount factor γ ∈ (0  1)  and rewards in range [0  1] we
provide an algorithm which computes an -optimal policy with probability 1 − δ
where both the time spent and number of sample taken are upper bounded by

O

(1 − γ)32 log

(1 − γ)δ

log

1

(1 − γ)

(cid:20) |S||A|

(cid:18) |S||A|

(cid:19)

(cid:18)

(cid:19)(cid:21)

.

For ﬁxed values of   this improves upon the previous best known bounds by a
factor of (1 − γ)−1 and matches the sample complexity lower bounds proved in
[AMK13] up to logarithmic factors. We also extend our method to computing
-optimal policies for ﬁnite-horizon MDP with a generative model and provide a
nearly matching sample complexity lower bound.

1

Introduction

Markov decision processes (MDPs) are a fundamental mathematical abstraction used to model se-
quential decision making under uncertainty and are a basic model of discrete-time stochastic control
and reinforcement learning (RL). Particularly central to RL is the case of computing or learning an
approximately optimal policy when the MDP itself is not fully known beforehand. One of the sim-
plest such settings is when the states  rewards  and actions are all known but the transition between
states when an action is taken is probabilistic  unknown  and can only be sampled from.
Computing an approximately optimal policy with high probability in this case is known as PAC
RL with a generative model. It is a well studied problem with multiple existing results providing
algorithms with improved the sample complexity (number of sample transitions taken) and running
time (the total time of the algorithm) under various MDP reward structures  e.g. discounted inﬁnite-
horizon  ﬁnite-horizon  etc. (See Section 5 for a detailed review of the literature.)
In this work  we consider this well studied problem of computing approximately optimal policies
of discounted inﬁnite-horizon Markov Decision Processes (DMDP) under the assumption we can
only access the DMDP by sampling state transitions. Formally  we suppose that we have a DMDP

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

with a known set of states  S  a known set of actions that can be taken at each states  A  a known
reward rs a ∈ [0  1] for taking action a ∈ A at state s ∈ S  and a discount factor γ ∈ (0  1). We
assume that taking action a at state s probabilistically transitions an agent to a new state based on
a ﬁxed  but unknown probability vector P s a. The objective is to maximize the cumulative sum
of discounted rewards in expectation. Throughout this paper  we assume that we have a generative
model  a notion introduced by [Kak03]  which allows us to draw random state transitions of the
DMDP. In particular  we assume that we can sample from the distribution deﬁned by P s a for all
(s  a) ∈ S × A in O(1) time. This is a natural assumption and can be achieved in expectation in
certain computational models with linear time preprocessing of the DMDP.1
The main result of this paper is that we provide the ﬁrst algorithm that is sample-optimal and
runtime-optimal (up to polylogarithmic factors) for computing an -optimal policy of a DMDP with

a generative model (in the regime of  ≥ 1/(cid:112)(1 − γ)|S|). In particular  we develop a randomized

Variance-Reduced Q-Value Iteration (vQVI) based algorithm that computes an -optimal policy with
probability 1 − δ with a number of samples  i.e. queries to the generative model  bound by

(cid:20) |S||A|

(cid:18) |S||A|

(cid:19)

(cid:18)

O

(1 − γ)32 log

(1 − γ)δ

log

(cid:19)(cid:21)

.

1

(1 − γ)

This result matches (up to polylogarithmic factors) the following sample complexity lower bound
established in [AMK13] for ﬁnding -optimal policies with probability 1 − δ (see Appendix D):

(cid:20) |S||A|

(cid:18)|S||A|

(cid:19)(cid:21)

Ω

(1 − γ)32 log

δ

.

Furthermore  we show that the algorithm can be implemented using sparse updates such that the
overall run-time complexity is equal to its sample complexity up to constant factors  as long as each
sample transition can be generated in O(1) time. Consequently  up to logarithmic factors our run
time complexity is optimal as well. In addition  the algorithm’s space complexity is Θ(|S||A|).
Our method and analysis builds upon a number of prior works. (See Section 5 for an in-depth com-
parison.) The paper [AMK13] provided the ﬁrst algorithm that achieves the optimal sample com-
plexity for ﬁnding -optimal value functions (rather than -optimal policy)  as well as the matching
lower bound. Unfortunately an -optimal value function does not imply an -optimal policy and if
we directly use the method of [AMK13] to get an -optimal policy for constant   the best known

sample complexity is (cid:101)O(|S||A|(1 − γ)−5−2). 2 This bound is known to be improvable through
(cid:101)O(|S||A|(1 − γ)−4−2) samples and total runtime and the work of [AMK13] which in the regime

related work of [SWWY18] which provides a method for computing an -optimal policy using
of small approximation error  i.e. where  = O((1 − γ)−1/2|S|−1/2)  already provides a method
that achieves the optimal sample complexity. However  when the approximation error takes ﬁxed
values  e.g.  ≥ Ω((1 − γ)−1/2|S|−1/2)  there remains a gap between the best known runtime and
sample complexity for computing an -optimal policy and the theoretical lower bounds. For ﬁxed
values of   which mostly occur in real applications  our algorithm improves upon the previous best
sample and time complexity bounds by a factor of (1 − γ)−1 where γ ∈ (0  1)  the discount factor 
is typically close to 1.
We achieve our results by combining and strengthening techniques from both [AMK13] and
[SWWY18]. On the one hand  in [AMK13] the authors showed that simply constructing a “sparsi-
ﬁed” MDP model by taking samples and then solving this model to high precision yields a sample
optimal algorithm in our setting for computing the approximate value of every state. On the other
hand  [SWWY18] provided faster algorithms for solving explicit DMDPs and improved sample and
time complexities given a sampling oracle. In fact  as we show in Appendix B.1  simply combining
these two results yields the ﬁrst nearly optimal runtime for approximately learning the value function
with a generative model. Unfortunately  it is known that an approximate-optimal value function does
not immediately yield an approximate-optimal policy of comparable quality (see e.g. [Ber13]) and
it is was previously unclear how to combine these methods to improve upon previous known bounds
for computing an approximate policy. To achieve our policy computation algorithm we therefore

1If instead the oracle needed time τ  every running time result in this paper should be multiplied by τ.
2[AMK13] showed that one can obtain -optimal value v (instead of -optimal policy) using sample size
∝ (1− γ)−3−2. By using this -optimal value v  one can get a greedy policy that is [(1− γ)−1]-optimal. By
setting  → (1 − γ)  one can obtain an -optimal policy  using the number of samples ∝ (1 − γ)−5−2.

2

open up both the algorithms and the analysis in [AMK13] and [SWWY18]  combining them in non-
trivial ways. Our proofs leverage techniques ranging from standard probabilistic analysis tools such
as Hoeffding and Bernstein inequalities  to optimization techniques such as variance reduction  to
properties speciﬁc to MDPs such as the Bellman ﬁxed-point recursion for expectation and variance
of the optimal value vector  and monotonicity of value iteration.
Finally  we extend our method to ﬁnite-horizon MDPs  which are also occurred frequently in real

applications. We show that the number of samples needed by this algorithm is (cid:101)O(H 3|S||A|−2)  in

order to obtain an -optimal policy for H-horizon MDP (see Appendix F). We also show that the
preceding sample complexity is optimal up to logarithmic factors by providing a matching lower
bound. We hope this work ultimately opens the door for future practical and theoretical work on
solving MDPs and efﬁcient RL more broadly.

√

v  |v|  and v2 vectors in RN with

2 Preliminaries
We use calligraphy upper case letters for sets or operators  e.g.  S  A and T . We use bold small
case letters for vectors  e.g.  v  r. We denote vs or v(s) as the s-th entry of vector v. We denote
√·  | · |  and
matrix as bold upper case letters  e.g.  P . We denote constants as normal upper case letters  e.g.  M.
For a vector v ∈ RN for index set N   we denote
(·)2 acting coordinate-wise. For two vectors v  u ∈ RN   we denote by v ≤ u as coordinate-wise
comparison  i.e.  ∀i ∈ N : v(i) ≤ u(i). The same deﬁnition are deﬁned to relations ≤  < and >.
We describe a DMDP by the tuple (S A  P   r  γ)  where S is a ﬁnite state space  A is a ﬁnite action
space  P ∈ RS×A×S is the state-action-state transition matrix  r ∈ RS×A is the state-action reward
vector  and γ ∈ (0  1) is a discount factor. We use P s a(s(cid:48)) to denote the probability of going to
state s(cid:48) from state s when taking action a. We also identify each P s a as a vector in RS. We use rs a
to denote the reward obtained from taking action a ∈ A at state s ∈ S and assume r ∈ [0  1]S×A.3
For a vector v ∈ RS  we denote P v ∈ RS×A as (P v)s a = P (cid:62)
s av. A policy π : S → A maps
each state to an action. The objective of MDP is to ﬁnd the optimal policy π∗ that maximizes the
expectation of the cumulative sum of discounted rewards.
In the remainder of this section we give deﬁnitions for several prominent concepts in MDP analysis
that we use throughout the paper.
Deﬁnition 2.1 (Bellman Value Operator). For a given DMDP the value operator T : RS (cid:55)→ RS is
deﬁned for all u ∈ RS and s ∈ S by T (u)s = maxa∈A[ra(s) + γ · P (cid:62)
s av]  and we let v∗ denote
the value of the optimal policy π∗  which is the unique vector such that T (v∗) = v∗.
Deﬁnition 2.2 (Policy). We call any vector π ∈ AS a policy and say that the action prescribed by
policy π to be taken at state s ∈ S is πs. We let Tπ : RS (cid:55)→ RS denote the value operator associated
with π deﬁned for all u ∈ RS and s ∈ S by Tπ(u)s = rs π(s) + γ · P (cid:62)
s π(s)u   and we let vπ denote
the values of policy π  which is the unique vector such that Tπ(vπ) = vπ.
Note that Tπ can be viewed as the value operator for the modiﬁed MDP where the only available
action from each state is given by the policy π. Note that this modiﬁed MDP is essentially just an
uncontrolled Markov Chain  i.e. there are no action choices that can be made.
Deﬁnition 2.3 (-optimal value and policy). We say values u ∈ RS are -optimal if (cid:107)v∗− u(cid:107)∞ ≤ 
and policy π ∈ AS is -optimal if (cid:107)v∗ − vπ(cid:107)∞ ≤   i.e. the values of π are -optimal.
Deﬁnition 2.4 (Q-function). For any policy π  we deﬁne the Q-function of a MDP with respect to π
as a vector Q ∈ RS×A such that Qπ(s  a) = r(s  a) + γP (cid:62)
s avπ. The optimal Q-function is deﬁned
. We call any vector Q ∈ RS×A a Q-function even though it may not relate to a policy
as Q∗ = Qπ∗
or a value vector and deﬁne v(Q) ∈ RS and π(Q) ∈ AS as the value and policy implied by Q  by

∀s ∈ S : v(Q)(s) = max

For a policy π  let P πQ ∈ RS×A be deﬁned as (P πQ)(s  a) =(cid:80)

a∈A Q(s  a)

and π(Q)(s) = arg max

a∈A Q(s  a).

s(cid:48)∈S P s a(s(cid:48))Q(s(cid:48)  π(s(cid:48))).

3A general r ∈ RS×A can always be reduced to this case by shifting and scaling.

3

3 Technique Overview

In this section we provide a more detailed and technical overview of our approach. At a high
level  our algorithm shares a similar framework as the variance reduction algorithm presented in
[SWWY18]. This algorithm used two crucial algorithmic techniques  which are also critical in this
paper. We call these techniques as the monotonicity technique and the variance reduction technique.
Our algorithm and the results of this paper can be viewed as an advanced  non-trivial integration of
these two methods  augmented with a third technique which we refer to as a total-variation technique
which was discovered in several papers [MM99  LH12  AMK13]. In the remainder of this section
we give an overview of these techniques and through this  explain our algorithm.

The Monotonicity Technique Recall that the classic value iteration algorithm for solving a MDP
repeatedly applies the following rule

v(i)(s) ← max

(r(s  a) + γP (cid:62)

s av(i−1)).

a

(3.1)

A greedy policy π(i) can be obtained at each iteration i by

∀s : π(i)(s) ← argmax

(r(s  a) + γP (cid:62)

s av(i)).

a

For any u > 0  it can be shown that if one can approximate v(i)(s) with(cid:98)v(i)(s) such that (cid:107)(cid:98)v(i) −

v(i)(cid:107)∞ ≤ (1 − γ)u and run the above value iteration algorithm using these approximated values 
then after Θ((1− γ)−1 log[u−1(1− γ)−1]) iterations  the ﬁnal iteration gives an value function that
is u-optimal ([Ber13]). However  a u-optimal value function only yields a u/(1− γ)-optimal greedy
policy (in the worst case)  even if (3.2) is precisely computed. To get around this additional loss  a
monotone-VI algorithm was proposed in [SWWY18] as follows. At each iteration  this algorithm
maintains not only an approximated value v(i) but also a policy π(i). The key for improvement is to
keep values as a lower bound of the value of the policy on a set of sample paths with high probability.
In particular  the following monotonicity condition was maintained with high probability

(3.2)

v(i) ≤ Tπ(i)(v(i)) .

By the monotonicity of the Bellman’s operator  the above equation guarantees that v(i) ≤ vπ(i).
value(cid:98)v(R) that is u-optimal then we also obtain a policy π(R) which by the monotonicity condition
If this condition is satisﬁed  then  if after R iterations of approximate value iteration we obtain an
and the monotonicity of the Bellman operator Tπ(R) yields

v(R) ≤ Tπ(R) (v(R)) ≤ T 2

π(R) (v(R)) ≤ . . . ≤ T ∞

π(R) (v(R)) = vπ(R) ≤ v∗.

and therefore this π(R) is an u-optimal policy. Ultimately  this technique avoids the standard loss of
a (1 − γ)−1 factor when converting values to policies.

The Variance Reduction Technique Suppose now that we provide an algorithm that maintains
the monotonicity condition using random samples from P s a to approximately compute (3.1). Fur-
ther  suppose we want to obtain a new value function and policy that is at least (u/2)-optimal. In
Since (cid:107)v(i)(cid:107)∞ ≤ (1 − γ)−1  by Hoeffding bound  (cid:101)O((1 − γ)−4u−2) samples sufﬁces. Note that
s av(i) up to error at most (1−γ)u/2.
order to obtain the desired accuracy  we need to approximate P (cid:62)
(cid:101)O((1 − γ)−4u−2|S||A|) samples/computation time and (cid:101)O((1 − γ)−1) iterations for the value itera-
tion to converge. Overall  this yields a sample/computation complexity of (cid:101)O((1 − γ)−5u−2|S||A|).

the number of samples also determines the computation time and therefore each iteration takes

To reduce the (1− γ)−5 dependence  [SWWY18] uses properties of the input (and the initialization)
vectors: (cid:107)v(0) − v∗(cid:107)∞ ≤ u and rewrites value iteration (3.1) as follows
s a(v(i−1) − v(0)) + P (cid:62)

(3.3)
using only (cid:101)O((1 − γ)−4u−2) samples. For every iteration  we have (cid:107)v(i−1) − v(0)(cid:107)∞ ≤ u (recall
s av(0) is shared over all iterations and we can approximate it up to error (1 − γ)u/4
be approximated up to error (1 − γ)u/4 using only (cid:101)O((1 − γ)−2) samples (note that there is no u-
s a(v(i−1) − v(0)) can
dependence here). By this technique  over (cid:101)O((1−γ)−1) iterations only (cid:101)O((1−γ)−4u−2+(1−γ)−3)

that we demand the monotonicity is satisﬁed at each iteration). Hence P (cid:62)

samples/computation per state action pair are needed  i.e. there is a (1 − γ) improvement.

(cid:2)r(s  a) + P (cid:62)

s av(0)(cid:3) 

v(i)(s) ← max

Notice that P (cid:62)

a

4

The Total-Variance Technique By combining the monotonicity technique and variance reduction

technique  one can obtain a (cid:101)O((1− γ)−4) sample/running time complexity (per state-action pair) on
bound and the best known lower bound of (cid:101)Ω[|S||A|−2(1 − γ)−3] [AMK13]. Here we show how

computing a policy; this was one of the results [SWWY18]. However  there is a gap between this
to remove the last (1 − γ) factor by better exploiting the structure of the MDP. In [SWWY18] the
update error in each iteration was set to be at most (1− γ)u/2 to compensate for error accumulation
through a horizon of length (1 − γ)−1 (i.e.  the accumulated error is sum of the estimation error
at each iteration). To improve we show how to leverage previous work to show that the true error
accumulation is much less. To see this  let us now switch to Bernstein inequality. Suppose we would
like to estimate the value function of some policy π. The estimation error vector of the value function

is upper bounded by (cid:101)O((cid:112)σπ/m)  where σπ(s) = Vars(cid:48)∼P s π(s) (vπ(s(cid:48))) denotes the variance of

accumated error ∝

(cid:112)σπ/m ≤ c1

the value of the next state if starting from state s by playing policy π  and m is the number of
samples collected per state-action pair. The accumulated error due to estimating value functions can
be shown to obey the following inequality (upper to logarithmic factors)

∞(cid:88)
state s  the expected sum of variance of the tail sums of rewards (cid:80) γ2iP i
that(cid:80)

where c1 is a constant and the inequality follows from a Cauchy-Swartz-like inequality. According
to the law of total variance  for any given policy π (in particular  the optimal policy π∗) and initial
πσπ  is exactly the variance
of the total return by playing the policy π. This observation was previously used in the analysis of
[MM99  LH12  AMK13]. Since the upper bound on the total return is (1 − γ)−1  it can be shown
Thus picking m ≈ (1−γ)−3−2 is sufﬁcient to control the accumulated error (instead of (1−γ)−4).
To analyze our algorithm  we will apply the above inequality to the optimal policy π∗ to obtain our
ﬁnal error bound.

πσπ ≤ (1 − γ)−2 · 1 and therefore the total error accumulation is(cid:112)(1 − γ)−3/m.

∞(cid:88)

i=0

γ2iP i

πσπ/m

 

(cid:33)1/2

i γ2iP i

(cid:32)

1
1 − γ

i=0

γiP i
π

Putting it All Together
In the next section we show how to combine these three techniques into
one algorithm and make them work seamlessly. In particular  we provide and analyze how to com-
bine these techniques into an Algorithm 1 which can be used to at least halve the error of a current
policy. Applying this routine a logarithmic number of time then yields our desired bounds. In the
input of the algorithm  we demand the input value v(0) and π(0) satisﬁes the required monotonicity
requirement  i.e.  v(0) ≤ Tπ(0)(v(0)) (in the ﬁrst iteration  the zero vector 0 and an arbitrary pol-
(cid:101)O((1− γ)−3−2) samples per state-action pair. The same set of samples is used to estimate the vari-
icy π satisﬁes the requirement). We then pick a set of samples to estimate P v(0) accurately with

ance vector σv∗. These estimates serve as the initialization of the algorithm. In each iteration i  we
draw fresh new samples to compute estimate of P (v(i) − v(0)). The sum of the estimate of P v(0)
and P (v(i) − v(0)) gives an estimate of P v(i). We then make the above estimates have one-sided
error by shifting them according to their estimation errors (which is estimated from the Bernstein
inequality). These one-side error estimates allow us to preserve monotonicity  i.e.  guarantees the
new value is always improving on the entire sample path with high probability. The estimate of
P v(i) is plugged in to the Bellman’s operator and gives us new value function  v(i+1) and policy
π(i+1)  satisfying the monotonicity and advancing accuracy. Repeating the above procedure for the
desired number of iterations completes the algorithm.
4 Algorithm and Analysis

In this section we provide and analyze our near sample/time optimal -policy computation algo-
rithm. As discussed in Section 3 our algorithm combines three main ideas: variance reduction  the
monotone value/policy iteration  and the reduction of accumulated error via Bernstein inequality.
These ingredients are used in the Algorithm 1 to provide a routine which halves the error of a given
policy. We analyze this procedure in Section 4.1 and use it to obtain our main result in Section 4.2.

4.1 The Analysis of the Variance Reduced Algorithm

In this section we analyze Algorithm 1  showing that each iteration of the algorithm approximately
contracts towards the optimal value and policy and that ultimately the algorithm halves the error

5

(cid:80)m1
(cid:80)m1

m1

m1

s av(0) and σv(0)(s  a)

−1log(8|S||A|δ−1);

s a  s(2)

s a  . . .   s(m1)

s a

from P s a;

\\Compute empirical estimates of P (cid:62)

12: Initialize w = (cid:101)w =(cid:98)σ = Q(0) ← 0S×A  and i ← 0;
Let (cid:101)w(s  a) ← 1
Let(cid:98)σ(s  a) ← 1
s a) − (cid:101)w2(s  a)
w(s  a) ← (cid:101)w(s  a) −(cid:112)2α1(cid:98)σ(s  a) − 4α3/4

Algorithm 1 Variance-Reduced QVI
1: Input: A sampling oracle for DMDP M = (S A  r  P   γ)
2: Input: Upper bound on error u ∈ [0  (1 − γ)−1] and error probability δ ∈ (0  1)
3: Input: Initial values v(0) and policy π(0) such that v(0) ≤ Tπ(0)v(0)  and v∗ − v(0) ≤ u1;
4: Output: v  π such that v ≤ Tπ(v) and v∗ − v ≤ (u/2) · 1.
5:
6: INITIALIZATION:
7: Let β ← (1 − γ)−1  and R ← (cid:100)c1β ln[βu−1](cid:101) for constant c1;
8: Let m1 ← c2β3u−2log(8|S||A|δ−1) for constant c2;
9: Let m2 ← c3β2 log[2R|S||A|δ−1] for constant c3;
10: Let α1 ← m1
11: For each (s  a) ∈ S × A  sample independent samples s(1)
13: for each (s  a) ∈ S × A do
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24: REPEAT:
25: for i = 1 to R do
26:
27:
28:
29:
30:
31:
32:
33:
34: return v(R)  π(R).

\\Compute g(i) the estimate of P(cid:2)v(i) − v(0)(cid:3) with one-sided error
Let v(i) ← v(Q(i−1))  π(i) ← π(Q(i−1)); \\let(cid:101)v(i) ← v(i) (cid:101)π(i) ← π(i) (for analysis);
s a  . . .  (cid:101)s(m2)
For each (s  a) ∈ S × A  draw independent samples(cid:101)s(1)
s a)(cid:3) − (1 − γ)u/8;

\\Shift the empirical estimate to have one-sided error and guarantee monotonicity

Let g(i)(s  a) ← 1
\\Improve Q(i)
Q(i) ← r + γ · [w + g(i)];

\\Compute coarse estimate of the Q-function
Q(0)(s  a) ← r(s  a) + γw(s  a)

1 (cid:107)v(0)(cid:107)∞ − (2/3)α1(cid:107)v(0)(cid:107)∞

s a) − v(0)((cid:101)s(j)

j=1 v(0)(s(j)
s a)
j=1(v(0))2(s(j)

(cid:2)v(i)((cid:101)s(j)

\\successively improve

(cid:80)m2

m2

j=1

s a (cid:101)s(2)

For each s ∈ S  if v(i)(s) ≤ v(i−1)(s)  then v(i)(s) ← v(i−1)(s) and π(i)(s) ← π(i−1)(s);

s a

from P s a;

of the input value and policy with high probability. All proofs in this section are deferred to Ap-
pendix E.1.

We start with bounding the error of (cid:101)w and(cid:98)σ deﬁned in Line 15 and 16 of Algorithm 1. Notice that
Lemma 4.1 (Empirical Estimation Error). Let(cid:101)w and(cid:98)σ be computed in Line 15 and 16 of Algorithm
1. Recall that (cid:101)w and(cid:98)σ are empirical estimates of P v and σv = P v2 − (P v)2 using m1 samples

these are the empirical estimations of P (cid:62)

per (s  a) pair. With probability at least 1 − δ  for L def= log(8|S||A|δ−1)  we have

s av(0) and σv(0)(s  a).

(cid:12)(cid:12)(cid:101)w − P (cid:62)v(0)(cid:12)(cid:12) ≤(cid:113)

and

∀(s  a) ∈ S × A :

2m−1

1 σv(0) · L + 2(3m1)−1(cid:107)v(0)(cid:107)∞L

(cid:12)(cid:12)(cid:98)σ(s  a) − σv(0)(s  a)(cid:12)(cid:12) ≤ 4(cid:107)v(0)(cid:107)2∞ ·(cid:113)

2m−1

1 L.

(4.1)

(4.2)

The proof is a straightforward application of Bernstein’s inequality and Hoeffding’s inequality.
Next we show that the difference between σv(0) and σv∗ is also bounded.

6

√

σv ≤ √

Lemma 4.2. Suppose (cid:107)v − v∗(cid:107)∞ ≤  for some  > 0  then
Next we show that in Line 30  the computed g(i) concentrates to and is an overestimate of P [v(i) −
v(0)] with high probability.

Lemma 4.3. Let g(i) be the estimate of P(cid:2)v(i) − v(0)(cid:3) deﬁned in Line 30 of Algorithm 1. Then

σv∗ +  · 1.

conditioning on the event that (cid:107)v(i) − v(0)(cid:107)∞ ≤ 2u  with probability at least 1 − δ/R 

P(cid:2)v(i) − v(0)(cid:3) − (1 − γ)u

· 1 ≤ g(i) ≤ P(cid:2)v(i) − v(0)(cid:3)

provided appropriately chosen constants c1  c2  and c3 in Algorithm 1.

4

(cid:104)

(cid:105) · 1 

1 L < 1 the error vector ξ satisﬁes
0 ≤ ξ ≤ C

√

Now we present the key contraction lemma  in which we set the constants  c1  c2  c3  in Algorithm 1
to be sufﬁciently large (e.g.  c1 ≥ 4  c2 ≥ 8192  c3 ≥ 128). Note that these constants only need to
be sufﬁciently large so that the concentration inequalities hold.
Lemma 4.4. Let Q(i) be the estimated Q-function of v(i) in Line 33 of Algorithm 1. Let π(i) and
v(i) be estimated in iteration i  as deﬁned in Line 27 and 28. Then  with probability at least 1 − 2δ 
for all 1 ≤ i ≤ R 
v(i−1) ≤ v(i) ≤ Tπ(i) [v(i)]  Q(i) ≤ r + γP v(i) 
where for α1 = m−1

and Q∗− Q(i) ≤ γP π∗(cid:2)Q∗− Q(i−1)(cid:3) + ξ 

(1 − γ)u/C + Cα3/4

1 (cid:107)v(0)(cid:107)∞

α1σv∗ +
for some sufﬁciently large constant C ≥ 8.
Using the previous lemmas we can prove the guarantees of Algorithm 1.
Proposition 4.5. On an input value vector v(0)  policy π(0)  and parameters u ∈ (0  (1− γ)−1]  δ ∈
(0  1) such that v(0) ≤ Tπ(0)[v(0)]  and v∗− v(0) ≤ u1  Algorithm 1 halts in time O((1− γ)−1u−2·
|S||A| · log(|S||Aδ−1(1 − γ)−1u−1)) and outputs values v and policy π such that v ≤ Tπ(v) and
v∗ − v ≤ (u/2)1 with probability at least 1− δ  provided appropriately chosen constants  c1  c2  c3.
We prove this proposition by iteratively applying Lemma 4.4. Suppose v(R) is the output of the
)−1ξ.
Notice that (I − γP π∗
σv∗. We then apply the variance an-
alytical tools presented in Section C to show that (I − γP π∗
)−1ξ ≤ (u/4)1 when setting the
constants properly in Algorithm 1. We refer this technique as the total-variance technique  since
σv∗(cid:107)2∞ ≤ O[(1 − γ)−3] instead of a na¨ıve bound of (1 − γ)−4. We complete the
(cid:107)(I − γP π∗

algorithm  after R iterations. We show v∗ − v(R) ≤ γR−1P π∗(cid:2)Q∗ − Q0(cid:3) + (I − γP π∗

proof by choosing R = (cid:101)Θ((1 − γ)−1 log(u−1)) and showing that γR−1P π∗(cid:2)Q∗ − Q0(cid:3) ≤ (u/4)1.

)−1ξ is related to (I − γP π∗

)−1√

)−1√

4.2 From Halving the Error to Arbitrary Precision

In the previous section  we provided an algorithm that on an input policy  outputs a policy with
value vector that has (cid:96)∞ distance to the optimal value vector only half of that of the input one. In
this section  we give a complete policy computation algorithm by by showing that it is possible to
apply this error “halving” procedure iteratively. We summarize our meta algorithm in Algorithm 2.
Note that in the algorithm  each call of HALFERR draws new samples from the sampling oracle. We
refer in this section to Algorithm 1 as a subroutine HALFERR  which given an input MDP M with
a sampling oracle  an input value function v(i)  and an input policy π(i)  outputs an value function
v(i+1) and a policy π(i+1).
Combining Algorithm 2 and Algorithm 1  we are ready to present main result.
Theorem 4.6. Let M = (S A  P   r  γ) be a DMDP with a generative model. Suppose we can
sample a state from each probability vector P s a within time O(1). Then for any   δ ∈ (0  1)  there
exists an algorithm that halts in time

(cid:20) |S||A|

(cid:18) |S||A|

(cid:19)

(cid:18)

(cid:19)(cid:21)

T := O

(1 − γ)32 log

(1 − γ)δ

log

1

(1 − γ)

7

Algorithm 2 Meta Algorithm
1: Input: A sampling oracle of some M = (S A  r  P   γ)   > 0  δ ∈ (0  1)
2: Initialize: v(0) ← 0  π(0) ← arbitrary policy  R ← Θ[log(−1(1 − γ)−1)]
3: for i = {1  2  . . .   R} do
4:
5:
6: Output: v(R)  π(R).

//HALFERR is initialized with QVI(u = 2−i+1(1 − γ)−1  δ  v(0) = v(i−1)  π(0) = π(i−1))
v(i)  π(i) ← HALFERR ← v(i−1)  π(i−1)

and obtains a policy π such that v∗ − 1 ≤ vπ ≤ v∗  with probability at least 1 − δ where v∗ is the
optimal value of M. The algorithm uses space O(|S||A|) and queries the generative model for at
most O(T ) fresh samples.
Remark 4.7. The full analysis of the halving algorithm is presented in Section E.2. Our algorithm
can be implemented in space O(|S||A|) since in Algorithm 1  the initialization phase can be done
updates can be computed in space O(|S||A|) as well.

for each (s  a) and compute w(s  a) (cid:101)w(s  a) (cid:98)σ(s  a)  Q(0)(s  a) without storing the samples. The

5 Comparison to Previous Work

Algorithm

Phased Q-Learning

Empirical QVI

Empirical QVI

Randomized Primal-Dual

Sublinear Randomized Value

Method

Iteration

Sublinear Randomized QVI

Sample Complexity

(1−γ)32

|S||A|
(1−γ)72 )
|S||A|
(1−γ)52 ) 4

(cid:101)O(C
(cid:101)O(
(cid:101)O(cid:0) |S||A|
(cid:1) if  = (cid:101)O(cid:0)
(cid:101)O(C
(cid:17)
(cid:16) |S||A|
(cid:101)O
(cid:16) |S||A|
(cid:17)
(cid:101)O

|S||A|
(1−γ)42 )

(1−γ)42

(1−γ)32

1√
(1−γ)|S|

(cid:1)

References

[KS99]
[AMK13]

[AMK13]

[Wan17]

[SWWY18]

This Paper

Table 1: Sample Complexity to Compute -Approximate Policies Using the Generative Sampling Model:
Here |S| is the number of states  |A| is the number of actions per state  γ ∈ (0  1) is the discount factor  and C
is an upper bound on the ergodicity. Rewards are bounded between 0 and 1.
There exists a large body of literature on MDPs and RL (see e.g. [Kak03  SLL09  KBJ14  DB15]
and reference therein). The classical MDP problem is to compute an optimal policy exactly or
approximately  when the full MDP model is given as input. For a survey on existing complexity
results when the full MDP model is given  see Appendix A.
Despite the aforementioned results of [Kak03  AMK13  SWWY18]  there exists only a handful of
additional RL methods that achieve a small sample complexity and a small run-time complexity at
the same time for computing an -optimal policy. A classical result is the phased Q-learning method
by [KS99]  which takes samples from the generative model and runs a randomized value itera-
tion. The phased Q-learning method ﬁnds an -optimal policy using O(|S||A|−2/poly(1 − γ))

samples/updates  where each update uses (cid:101)O(1) run time.5 Another work [Wan17] gave a ran-
DMDP. They achieve a total runtime of (cid:101)O(|S|3|A|−2(1 − γ)−6) for the general DMDP and
(cid:101)O(C|S||A|−2(1 − γ)−4) for DMDPs that are ergodic under all possible policies  where C is a
approximate policy in sample size/run time (cid:101)O(|S||A|−2(1−γ)−4)  without requiring any ergodicity

problem-speciﬁc ergodicity measure. A recent closely related work is [SWWY18] which gave a
variance-reduced randomized value iteration that works with the generative model and ﬁnds an -

domized mirror-prox method that applies to a special Bellman saddle point formulation of the

assumption.

4Although not explicitly stated  an immediate derivation shows that obtaining an -optimal policy in
5The dependence on (1 − γ) in [KS99] is not stated explicitly but we believe basic calculations yield

[AMK13] requires O(|S||A|(1 − γ)−5−2) samples.
O(1/(1 − γ)7).

8

(cid:16)

1/(cid:112)(1 − γ)−1|S|(cid:17)

  [AMK13] showed that the solution obtained
Finally  in the case where  = O
by performing exact PI on the empirical MDP model provides not only an -optimal value but also

an -optimal policy. In this case  the number of samples is (cid:101)O(|S||A|(1 − γ)−3−2) and matches the
because of the very small approximation error  = O(1/(cid:112)(1 − γ)|S|). See Table 1 for a list of

sample complexity lower bound. Although this sample complexity is optimal  it requires solving the
empirical MDP exactly (see Appendix B)  and is no longer sublinear in the size of the MDP model

comparable sample complexity results for solving MDP based on the generative model.

time spent and number of sample taken is upper bounded by (cid:101)O((1−γ)−3−2|S||A|). This improves

6 Concluding Remark
In summary  for a discounted Markov Decision Process (DMDP) M = (S A  P   r  γ) provided
we can only access the transition function of the DMDP through a generative sampling model  we
provide an algorithm which computes an -approximate policy with probability 1− δ where both the
upon the previous best known bounds by a factor of 1/(1 − γ) and matches the the lower bounds
proved in [AMK13] up to logarithmic factors.
The appendix is structured as follows. Section A surveys the existing runtime results for solving the
DMDP when a full model is given. Section B provides an runtime optimal algorithm for computing
approximate value functions (by directly combining [AMK13] and [SWWY18]). Section C gives
technical analysis and variance upper bounds for the total-variance technique. Section D discusses
sample complexity lower bounds for obtaining approximate policies with a generative sampling
model. Section E provides proofs to lemmas  propositions and theorems in the main text of the
paper. Section F extends our method and results to the ﬁnite-horizon MDP and provides a nearly
matching sample complexity lower bound.

9

References

[AMK13] Mohammad Gheshlaghi Azar  R´emi Munos  and Hilbert J Kappen. Minimax pac
bounds on the sample complexity of reinforcement learning with a generative model.
Machine learning  91(3):325–349  2013.

[Bel57] Richard Bellman. Dynamic Programming. Princeton University Press  Princeton  NJ 

1957.

[Ber13] Dimitri P Bertsekas. Abstract dynamic programming. Athena Scientiﬁc  Belmont 

MA  2013.

[Dan16] George Dantzig. Linear Programming and Extensions. Princeton University Press 

Princeton  NJ  2016.

[DB15] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon
reinforcement learning. In Advances in Neural Information Processing Systems  pages
2818–2826  2015.

[d’E63] F d’Epenoux. A probabilistic production and inventory problem. Management Science 

10(1):98–108  1963.

[DG60] Guy De Ghellinck. Les problemes de decisions sequentielles. Cahiers du Centre

dEtudes de Recherche Op´erationnelle  2(2):161–179  1960.

[HMZ13] Thomas Dueholm Hansen  Peter Bro Miltersen  and Uri Zwick. Strategy iteration is
strongly polynomial for 2-player turn-based stochastic games with a constant discount
factor. J. ACM  60(1):1:1–1:16  February 2013.

[How60] Ronald A. Howard. Dynamic programming and Markov processes. The MIT press 

Cambridge  MA  1960.

[Kak03] Sham M Kakade. On the sample complexity of reinforcement learning. PhD thesis 

University of London London  England  2003.

[KBJ14] Dileep Kalathil  Vivek S Borkar  and Rahul Jain. Empirical q-value iteration. arXiv

preprint arXiv:1412.0180  2014.

[KS99] Michael J Kearns and Satinder P Singh. Finite-sample convergence rates for q-learning
and indirect algorithms. In Advances in neural information processing systems  pages
996–1002  1999.

[LDK95] Michael L Littman  Thomas L Dean  and Leslie Pack Kaelbling. On the complexity
of solving Markov decision problems. In Proceedings of the Eleventh conference on
Uncertainty in artiﬁcial intelligence  pages 394–402. Morgan Kaufmann Publishers
Inc.  1995.

[LH12] Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International

Conference on Algorithmic Learning Theory  pages 320–334. Springer  2012.

[LS14] Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving
linear programs in o (vrank) iterations and faster algorithms for maximum ﬂow.
In
Foundations of Computer Science (FOCS)  2014 IEEE 55th Annual Symposium on 
pages 424–433. IEEE  2014.

[LS15] Yin Tat Lee and Aaron Sidford. Efﬁcient inverse maintenance and faster algorithms for
linear programming. In Foundations of Computer Science (FOCS)  2015 IEEE 56th
Annual Symposium on  pages 230–249. IEEE  2015.

[MM99] Remi Munos and Andrew W Moore. Variable resolution discretization for high-

accuracy solutions of optimal control problems. Robotics Institute  page 256  1999.

[MS99] Yishay Mansour and Satinder Singh. On the complexity of policy iteration. In Pro-
ceedings of the Fifteenth conference on Uncertainty in artiﬁcial intelligence  pages
401–408. Morgan Kaufmann Publishers Inc.  1999.

10

[Sch13] Bruno Scherrer. Improved and generalized upper bounds on the complexity of policy
In Advances in Neural Information Processing Systems  pages 386–394 

iteration.
2013.

[SLL09] Alexander L Strehl  Lihong Li  and Michael L Littman. Reinforcement learning in
ﬁnite mdps: Pac analysis. Journal of Machine Learning Research  10(Nov):2413–
2444  2009.

[SWWY18] Aaron Sidford  Mengdi Wang  Xian Wu  and Yinyu Ye. Variance reduced value itera-
tion and faster algorithms for solving markov decision processes. In Proceedings of the
Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms  pages 770–787.
SIAM  2018.

[Tse90] Paul Tseng. Solving h-horizon  stationary markov decision problems in time propor-

tional to log (h). Operations Research Letters  9(5):287–297  1990.

[Wan17] Mengdi Wang. Randomized linear programming solves the discounted Markov deci-

sion problem in nearly-linear running time. arXiv preprint arXiv:1704.01869  2017.

[Ye05] Yinyu Ye. A new complexity result on solving the Markov decision problem. Mathe-

matics of Operations Research  30(3):733–749  2005.

[Ye11] Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the
Markov decision problem with a ﬁxed discount rate. Mathematics of Operations Re-
search  36(4):593–603  2011.

11

,Yanyao Shen
Qixing Huang
Nati Srebro
Sujay Sanghavi
Aaron Sidford
Mengdi Wang
Xian Wu
Lin Yang
Yinyu Ye