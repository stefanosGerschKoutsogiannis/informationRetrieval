2011,Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,We consider the minimization of a convex objective function defined on a Hilbert space  which is only available through unbiased estimates of  its gradients.  This problem includes  standard machine learning algorithms such as kernel logistic regression and least-squares regression  and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the  convergence of two well-known algorithms  stochastic gradient descent (a.k.a.~Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations  while leading to the optimal convergence rate in the strongly convex case  is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging  robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.,Non-Asymptotic Analysis of Stochastic

Approximation Algorithms for Machine Learning

Francis Bach

INRIA - Sierra Project-team

Ecole Normale Sup´erieure  Paris  France

francis.bach@ens.fr

Abstract

Eric Moulines

LTCI

Telecom ParisTech  Paris  France
eric.moulines@enst.fr

We consider the minimization of a convex objective function deﬁned on a Hilbert space 
which is only available through unbiased estimates of its gradients. This problem in-
cludes standard machine learning algorithms such as kernel logistic regression and
least-squares regression  and is commonly referred to as a stochastic approximation
problem in the operations research community. We provide a non-asymptotic anal-
ysis of the convergence of two well-known algorithms  stochastic gradient descent
(a.k.a. Robbins-Monro algorithm) as well as a simple modiﬁcation where iterates are
averaged (a.k.a. Polyak-Ruppert averaging). Our analysis suggests that a learning rate
proportional to the inverse of the number of iterations  while leading to the optimal con-
vergence rate in the strongly convex case  is not robust to the lack of strong convexity or
the setting of the proportionality constant. This situation is remedied when using slower
decays together with averaging  robustly leading to the optimal rate of convergence. We
illustrate our theoretical results with simulations on synthetic and standard datasets.

1 Introduction
The minimization of an objective function which is only available through unbiased estimates of
the function values or its gradients is a key methodological problem in many disciplines. Its anal-
ysis has been attacked mainly in three communities: stochastic approximation [1  2  3  4  5  6] 
optimization [7  8]  and machine learning [9  10  11  12  13  14  15]. The main algorithms which
have emerged are stochastic gradient descent (a.k.a. Robbins-Monro algorithm)  as well as a simple
modiﬁcation where iterates are averaged (a.k.a. Polyak-Ruppert averaging).
Traditional results from stochastic approximation rely on strong convexity and asymptotic analysis 
but have made clear that a learning rate proportional to the inverse of the number of iterations  while
leading to the optimal convergence rate in the strongly convex case  is not robust to the wrong setting
of the proportionality constant. On the other hand  using slower decays together with averaging
robustly leads to optimal convergence behavior (both in terms of rates and constants) [4  5].
The analysis from the convex optimization and machine learning literatures however has focused
on differences between strongly convex and non-strongly convex objectives  with learning rates and
roles of averaging being different in these two cases [11  12  13  14  15].
A key desirable behavior of an optimization method is to be adaptive to the hardness of the problem 
and thus one would like a single algorithm to work in all situations  favorable ones such as strongly
convex functions and unfavorable ones such as non-strongly convex functions. In this paper  we
unify the two types of analysis and show that (1) a learning rate proportional to the inverse of the
number of iterations is not suitable because it is not robust to the setting of the proportionality
constant and the lack of strong convexity  (2) the use of averaging with slower decays allows (close
to) optimal rates in all situations.
More precisely  we make the following contributions:

− We provide a direct non-asymptotic analysis of stochastic gradient descent in a machine learn-
ing context (observations of real random functions deﬁned on a Hilbert space) that includes

1

kernel least-squares regression and logistic regression (see Section 2)  with strong convexity
assumptions (Section 3) and without (Section 4).

− We provide a non-asymptotic analysis of Polyak-Ruppert averaging [4  5]  with and without
In particular  we show that slower decays of the

strong convexity (Sections 3.3 and 4.2).
learning rate  together with averaging  are crucial to robustly obtain fast convergence rates.

− We illustrate our theoretical results through experiments on synthetic and non-synthetic exam-

ples in Section 5.

Notation. We consider a Hilbert space H with a scalar product h· ·i. We denote by k · k the
associated norm and use the same notation for the operator norm on bounded linear operators from
H to H  deﬁned as kAk = supkxk61 kAxk (if H is a Euclidean space  then kAk is the largest
singular value of A). We also use the notation “w.p.1” to mean “with probability one”. We denote
by E the expectation or conditional expectation with respect to the underlying probability space.

∀n > 1  θn = θn−1 − γnf ′

2 Problem set-up
We consider a sequence of convex differentiable random functions (fn)n>1 from H to R. We con-
sider the following recursion  starting from θ0 ∈ H:
(1)
where (γn)n>1 is a deterministic sequence of positive scalars  which we refer to as the learning
rate sequence. The function fn is assumed to be differentiable (see  e.g.  [16] for deﬁnitions and
properties of differentiability for functions deﬁned on Hilbert spaces)  and its gradient is an unbiased
estimate of the gradient of a certain function f we wish to minimize:
(H1) Let (Fn)n>0 be an increasing family of σ-ﬁelds. θ0 is F0-measurable  and for each θ ∈ H 
(2)

n(θ) is square-integrable  Fn-measurable and
∀θ ∈ H  ∀n > 1  E(f ′

n(θ)|Fn−1) = f ′(θ)  w.p.1.

the random variable f ′

n(θn−1) 

For an introduction to martingales  σ-ﬁelds  and conditional expectations  see  e.g.  [17]. Note that
depending whether F0 is a trivial σ-ﬁeld or not  θ0 may be random or not. Moreover  we could
restrict Eq. (2) to be satisﬁed only for θn−1 and θ∗ (which is a global minimizer of f ).
n(θn−1)  the goal of stochastic approximation is to minimize the
Given only the noisy gradients f ′
function f with respect to θ. Our assumptions include two usual situations  but also include many
others (e.g.  potentially  active learning):

− Stochastic approximation: in the so-called Robbins-Monro setting  for all θ ∈ H and n > 1 
fn(θ) may be expressed as fn(θ) = f (θ) +hεn  θi  where (εn)n>1 is a square-integrable mar-
tingale difference (i.e.  such that E(εn|Fn−1) = 0)  which corresponds to a noisy observation
f ′(θn−1) + εn of the gradient f ′(θn−1).
− Learning from i.i.d. observations: for all θ ∈ H and n > 1  fn(θ) = ℓ(θ  zn) where zn is an
i.i.d. sequence of observations in a measurable space Z and ℓ : H×Z is a loss function. Then
f (θ) is the generalization error of the predictor deﬁned by θ. Classical examples are least-
squares or logistic regression (linear or non-linear through kernel methods [18  19])  where
fn(θ) = 1
2 (hxn  θi − yn)2  or fn(θ) = log[1 + exp(−yn hxn  θi)]  for xn ∈ H  and yn ∈ R
(or {−1  1} for logistic regression).

Throughout this paper  unless otherwise stated  we assume that each function fn is convex and
smooth  following the traditional deﬁnition of smoothness from the optimization literature  i.e. 
Lipschitz-continuity of the gradients (see  e.g.  [20]). However  we make two slightly different
assumptions: (H2) where the function θ 7→ E(f ′
n(θ)|Fn−1) is Lipschitz-continuous in quadratic
mean and a strengthening of this assumption  (H2’) in which θ 7→ f ′
n(θ) is almost surely Lipschitz-
continuous.
(H2) For each n > 1  the function fn is almost surely convex  differentiable  and:

(3)
(H2’) For each n > 1  the function fn is almost surely convex  differentiable with Lipschitz-

n(θ2)k2|Fn−1) 6 L2kθ1 − θ2k2   w.p.1.

∀n > 1  ∀θ1  θ2 ∈ H  E(kf ′

n(θ1) − f ′

continuous gradient f ′

n  with constant L  that is:
n(θ1) − f ′

∀n > 1  ∀θ1  θ2 ∈ H  kf ′

n(θ2)k 6 Lkθ1 − θ2k   w.p.1.

(4)

2

If fn is twice differentiable  this corresponds to having the operator norm of the Hessian operator
of fn bounded by L. For least-squares or logistic regression  if we assume that (Ekxnk4)1/4 6
R for all n ∈ N  then we may take L = R2 (or even L = R2/4 for logistic regression) for
assumption (H2)  while for assumption (H2’)  we need to have an almost sure bound kxnk 6 R.
3 Strongly convex objectives

In this section  following [21]  we make the additional assumption of strong convexity of f   but not
of all functions fn (see [20] for deﬁnitions and properties of such functions):
(H3) The function f is strongly convex with respect to the norm k·k  with convexity constant µ > 0.

That is  for all θ1  θ2 ∈ H  f (θ1) > f (θ2) + hf ′(θ2)  θ1 − θ2i + µ

2kθ1 − θ2k2.

Note that (H3) simply needs to be satisﬁed for θ2 = θ∗ being the unique global minimizer of f
(such that f ′(θ∗) = 0). In the context of machine learning (least-squares or logistic regression) 
assumption (H3) is satisﬁed as soon as µ
2kθk2 is used as an additional regularizer. For all strongly
convex losses (e.g.  least-squares)  it is also satisﬁed as soon as the expectation E(xn ⊗ xn) is
invertible. Note that this implies that the problem is ﬁnite-dimensional  otherwise  the expectation
is a compact covariance operator  and hence non-invertible (see  e.g.  [22] for an introduction to
covariance operators). For non-strongly convex losses such as the logistic loss  f can never be
strongly convex unless we restrict the domain of θ (which we do in Section 3.2). Alternatively to
restricting the domain  replacing the logistic loss u 7→ log(1 + e−u) by u 7→ log(1 + e−u) + εu2/2 
for some small ε > 0  makes it strongly convex in low-dimensional settings.
By strong convexity of f   if we assume (H3)  then f attains its global minimum at a unique vector
θ∗ ∈ H such that f ′(θ∗) = 0. Moreover  we make the following assumption (in the context of
stochastic approximation  it corresponds to E(kεnk2|Fn−1) 6 σ2):
(H4) There exists σ2 ∈ R+ such that for all n > 1  E(kf ′
3.1 Stochastic gradient descent
Before stating our ﬁrst theorem (see proof in [23])  we introduce the following family of functions
ϕβ : R+ \ {0} → R given by:

n(θ∗)k2|Fn−1) 6 σ2  w.p.1.

ϕβ(t) = (cid:26) tβ −1

log t

β

if β 6= 0 
if β = 0.

The function β 7→ ϕβ(t) is continuous for all t > 0. Moreover  for β > 0  ϕβ(t) < tβ
β < 0  we have ϕβ(t) < 1

−β (both with asymptotic equality when t is large).

β   while for

Theorem 1 (Stochastic gradient descent  strong convexity) Assume (H1 H2 H3 H4). Denote
δn = Ekθn − θ∗k2  where θn ∈ H is the n-th iterate of the recursion in Eq. (1)  with γn = Cn−α.
We have  for α ∈ [0  1]:

2 exp(cid:0)4L2C2ϕ1−2α(n)(cid:1) exp(cid:18)−

exp(2L2C2)

µC
4

n1−α(cid:19)(cid:18)δ0 +
L2(cid:19) + 2σ2C2 ϕµC/2−1(n)

nµC/2

σ2

 

(cid:18)δ0 +

nµC

σ2

L2(cid:19) +

4Cσ2
µnα  

if 0 6 α < 1 

(5)

if α = 1.

δn 6




Sketch of proof. Under our assumptions  it can be shown that (δn) satisﬁes the following recursion:

δn 6 (1 − 2µγn + 2L2γ2

n)δn−1 + 2σ2γ2
n.

(6)

Note that it also appears in [3  Eq. (2)] under different assumptions. Using this deterministic recur-
sion  we then derive bounds using classical techniques from stochastic approximation [2]  but in a
non-asymptotic way  by deriving explicit upper-bounds.
Related work. To the best of our knowledge  this non-asymptotic bound  which depends explicitly
upon the parameters of the problem  is novel (see [1  Theorem 1  Electronic companion paper] for a
simpler bound with no such explicit dependence). It shows in particular that there is convergence in
quadratic mean for any α ∈ (0  1]. Previous results from the stochastic approximation literature have
focused mainly on almost sure convergence of the sequence of iterates. Almost-sure convergence
requires that α > 1/2  with counter-examples for α < 1/2 (see  e.g.  [2] and references therein).

3

Bound on function values. The bounds above imply a corresponding a bound on the functions
values. Indeed  under assumption (H2)  it may be shown that E[f (θn) − f (θ∗)] 6 L
2 δn (see proof
in [23]).
Tightness for quadratic functions. Since the deterministic recursion in Eq. (6) is an equality for
quadratic functions fn  the result in Eq. (5) is optimal (up to constants). Moreover  our results are
consistent with the asymptotic results from [6].

nµC/2

6

1

1

1

1

Forgetting initial conditions. Bounds depend on the initial condition δ0 = E(cid:2)kθ0 − θ∗k2(cid:3) and the
variance σ2 of the noise term. The initial condition is forgotten sub-exponentially fast for α ∈ (0  1) 
but not for α = 1. For α < 1  the asymptotic term in the bound is 4Cσ2
µnα .
Behavior for α = 1. For α = 1  we have ϕµC/2−1(n)
n if Cµ > 2  ϕµC/2−1(n)
nµC/2 = log n
if Cµ = 2 and ϕµC/2−1(n)
nµC/2 if Cµ > 2. Therefore  for α = 1  the choice of C is
critical  as already noticed by [8]: too small C leads to convergence at arbitrarily small rate of the
form n−µC/2  while too large C leads to explosion due to the initial condition. This behavior is
conﬁrmed in simulations in Section 5.
Setting C too large. There is a potentially catastrophic term when C is chosen too large  i.e. 

nµC/2 6

µC/2−1

1−µC/2

exp(cid:0)4L2C2ϕ1−2α(n)(cid:1)  which leads to an increasing bound when n is small. Note that for α < 1 

this catastrophic term is in front of a sub-exponentially decaying factor  so its effect is mitigated
once the term in n1−α takes over ϕ1−2α(n)  and the transient term stops increasing. Moreover  the
asymptotic term is not involved in it (which is also observed in simulations in Section 5).
Minimax rate. Note ﬁnally  that the asymptotic convergence rate in O(n−1) matches optimal
asymptotic minimax rate for stochastic approximation [24  25]. Note that there is no explicit depen-
dence on dimension; this dependence is implicit in the deﬁnition of the constants µ and L.

n

3.2 Bounded gradients

In some cases such as logistic regression  we also have a uniform upper-bound on the gradients  i.e. 
we assume (note that in Theorem 2  this assumption replaces both (H2) and (H4)).

(H5) For each n > 1  almost surely  the function fn if convex  differentiable and has gradients
uniformly bounded by B on the ball of center 0 and radius D  i.e.  for all θ ∈ H and all n > 0 
kθk 6 D ⇒ kf ′

n(θ)k 6 B.

Note that no function may be strongly convex and Lipschitz-continuous (i.e.  with uniformly
bounded gradients) over the entire Hilbert space H. Moreover  if (H2’) is satisﬁed  then we may take
D = kθ∗k and B = LD. The next theorem shows that with a slight modiﬁcation of the recursion
in Eq. (1)  we get simpler bounds than the ones obtained in Theorem 1  obtaining a result which
already appeared in a simpliﬁed form [8] (see proof in [23]):

Theorem 2 (Stochastic gradient descent  strong convexity  bounded gradients) Assume

(H1 H3 H5). Denote δn = E(cid:2)kθn − θ∗k2(cid:3)  where θn ∈ H is the n-th iterate of the follow-

ing recursion:

(7)
where ΠD is the orthogonal projection operator on the ball {θ : kθk 6 D}. Assume kθ∗k 6 D. If
γn = Cn−α  we have  for α ∈ [0  1]:

∀n > 1  θn = ΠD[θn−1 − γnf ′

n(θn−1)] 

δn 6 
(cid:0)δ0 + B2C2ϕ1−2α(n)(cid:1) exp(cid:18)−
µC
2
δ0n−µC + 2B2C2n−µC ϕµC−1(n) 


n1−α(cid:19) +

2B2C2
µnα  

if α ∈ [0  1) ;
if α = 1 .

(8)

The proof follows the same lines than for Theorem 1  but with the deterministic recursion δn 6
n. Note that we obtain the same asymptotic terms than for Theorem 1 (but B
(1− 2µγn)δn−1 + B2γ2
replaces σ). Moreover  the bound is simpler (no explosive multiplicative factors)  but it requires to
know D in advance  while Theorem 1 does not. Note that because we have only assumed Lipschitz-
continuity  we obtain a bound on function values of order O(n−α/2)  which is sub-optimal. For
bounds directly on function values  see [26].

4

n Pn−1

Hessian operator f ′′
kf ′′

n (θ1) − f ′′

k=0 θk and  following [4  5]  we make extra assumptions regarding the

3.3 Polyak-Ruppert averaging
We now consider ¯θn = 1
smoothness of each fn and the fourth-order moment of the driving noise:
(H6) For each n > 1  the function fn is almost surely twice differentiable with Lipschitz-continuous
n   with Lipschitz constant M . That is  for all θ1  θ2 ∈ H and for all n > 1 
Note that (H6) needs only to be satisﬁed for θ2 = θ∗. For least-square regression  we have M = 0 
while for logistic regression  we have M = R3/4.
(H7) There exists τ ∈ R+  such that for each n > 1  E(kf ′

n (θ2)k 6 Mkθ1 − θ2k  where k · k is the operator norm.

Moreover  there exists a nonnegative self-adjoint operator Σ such that for all n  E(f ′
n(θ∗)|Fn−1) 4 Σ almost-surely.
f ′

n(θ∗)k4|Fn−1) 6 τ 4 almost surely.
n(θ∗) ⊗
The operator Σ (which always exists as soon as τ is ﬁnite) is here to characterize precisely the
variance term  which will be independent of the learning rate sequence (γn)  as we now show:
Theorem 3 (Averaging  strong convexity) Assume (H1  H2’  H3  H4  H6  H7). Then  for ¯θn =
n Pn−1
1
(cid:0)Ek¯θn − θ∗k2(cid:1)1/2

6(cid:2) tr f ′′(θ∗)−1Σf ′′(θ∗)−1(cid:3)1/2

M Cτ 2
2µ3/2 (1+(µC)1/2)

6σ

+

n

1

ϕ1−α(n)

k=0 θk and α ∈ (0  1)  we have:
√n
nµ1/2(cid:16) 1
8A
A exp(cid:0)24L4C4(cid:1)(cid:16)δ0 +

ϕ1−α(n)1/2

5M C1/2τ

4LC1/2

2nµ

+

+

+

µ

n

µC1/2
σ2

n1−α/2 +
L2(cid:17)1/2

C

+ L(cid:17)(cid:16)δ0 +
µE(cid:2)kθ0 − θ∗k4(cid:3)

20Cτ 2

+ 2τ 2C3µ + 8τ 2C2(cid:17)1/2

 

(9)

k=1

1
γk

n(θn−1) = 1
γn

k=1 f ′′(θ∗)−1f ′

k(θ∗). Note that we obtain a bound on the root mean square error.

where A is a constant that depends only on µ  C  L and α.
Sketch of proof. Following [4]  we start from Eq. (1)  write it as f ′
(θn−1 − θn)  and
n(θ∗) + f ′′(θ∗)(θn−1 − θ∗)  (b) f ′
notice that (a) f ′
n(θ∗) has zero mean and behaves
n(θn−1) ≈ f ′
n Pn
like an i.i.d. sequence  and (c) 1
(θk−1 − θk) turns out to be negligible owing to a sum-
mation by parts and to the bound obtained in Theorem 1. This implies that ¯θn − θ∗ behaves like
n Pn
− 1
Forgetting initial conditions. There is no sub-exponential forgetting of initial conditions  but
rather a decay at rate O(n−2) (last two lines in Eq. (9)). This is a known problem which may
slow down the convergence  a common practice being to start averaging after a certain number of
iterations [2]. Moreover  the constant A may be large when LC is large  thus the catastrophic terms
are more problematic than for stochastic gradient descent  because they do not appear in front of
sub-exponentially decaying terms (see [23]). This suggests to take CL small.
Asymptotically leading term. When M > 0 and α > 1/2  the asymptotic term for δn is independent
of (γn) and of order O(n−1). Thus  averaging allows to get from the slow rate O(n−α) to the opti-
mal rate O(n−1). The next two leading terms (in the ﬁrst line) have order O(nα−2) and O(n−2α) 
suggesting the setting α = 2/3 to make them equal. When M = 0 (quadratic functions)  the leading
term has rate O(n−1) for all α∈ (0  1) (with then a contribution of the ﬁrst term in the second line).
Case α = 1. We get a simpler bound by directly averaging the bound in Theorem 1  which leads
to an unchanged rate of n−1  i.e.  averaging is not key for α = 1  and does not solve the robustness
problem related to the choice of C or the lack of strong convexity.
Leading term independent of (γn). The term in O(n−1) does not depend on γn. Moreover  as no-
ticed in the stochastic approximation literature [4]  in the context of learning from i.i.d. observations 
this is exactly the Cramer-Rao bound (see  e.g.  [27])  and thus the leading term is asymptotically
optimal. Note that no explicit Hessian inversion has been performed to achieve this bound.
Relationship with prior work on online learning. There is no clear way of adding a bounded
gradient assumption in the general case α ∈ (0  1)  because the proof relies on the recursion without
projections  but for α = 1  the rate of O(n−1) (up to a logarithmic term) can be achieved in the
more general framework of online learning  where averaging is key to deriving bounds for stochastic
approximation from regret bounds. Moreover  bounds are obtained in high probability rather than
simply in quadratic mean (see  e.g.  [11  12  13  14  15]).

5

4 Non-strongly convex objectives
In this section  we do not assume that the function f is strongly convex  but we replace (H3) by:
(H8) The function f attains its global minimum at a certain θ∗ ∈ H (which may not be unique).
In the machine learning scenario  this essentially implies that the best predictor is in the function
class we consider.1 In the following theorem  since θ∗ is not unique  we only derive a bound on
function values. Not assuming strong convexity is essential in practice to make sure that algorithms
are robust and adaptive to the hardness of the learning or optimization problem (much like gradient
descent is).

4.1 Stochastic gradient descent
The following theorem is shown in a similar way to Theorem 1; we ﬁrst derive a deterministic recur-
sion  which we analyze with novel tools compared to the non-stochastic case (see details in [23]) 
obtaining new convergence rates for non-averaged stochastic gradient descent :
Theorem 4 (Stochastic gradient descent  no strong convexity) Assume (H1 H2’ H4 H8). Then 
if γn = Cn−α  for α ∈ [1/2  1]  we have:

E [f (θn) − f (θ∗)] 6

σ2

1

C(cid:16)δ0 +

L2(cid:17) exp(cid:0)4L2C2ϕ1−2α(n)(cid:1)

1 + 4L3/2C3/2

min{ϕ1−α(n)  ϕα/2(n)}

.

(10)

When α = 1/2  the bound goes to zero only when LC < 1/4  at rates which can be arbitrarily
slow. For α ∈ (1/2  2/3)  we get convergence at rate O(n−α/2)  while for α ∈ (2/3  1)  we get a
convergence rate of O(nα−1). For α = 1  the upper bound is of order O((log n)−1)  which may be
very slow (but still convergent). The rate of convergence changes at α = 2/3  where we get our best
rate O(n−1/3)  which does not match the minimax rate of O(n−1/2) for stochastic approximation
in the non-strongly convex case [25]. These rates for stochastic gradient descent without strong
convexity assumptions are new and we conjecture that they are asymptotically minimax optimal (for
stochastic gradient descent  not for stochastic approximation). Nevertheless  the proof of this result
falls out of the scope of this paper.
If we further assume that we have all gradients bounded by B (that is  we assume D = ∞ in (H5)) 
then  we have the following theorem  which allows α ∈ (1/3  1/2) with rate O(n−3α/2+1/2):
Theorem 5 (Stochastic gradient descent  no strong convexity  bounded gradients) Assume
(H1  H2’  H5  H8). Then  if γn = Cn−α  for α ∈ [1/3  1]  we have:
E [f (θn) − f (θ∗)] 6 
(cid:0)δ0 + B2C2ϕ1−2α(n)(cid:1)
C (δ0 + B2C2)1/2


if α ∈ [1/2  1] 
if α ∈ [1/3  1/2].

C min{ϕ1−α(n) ϕα/2(n)}  

(1−2α)1/2ϕ3α/2−1/2(n)  

(1+4L1/2BC 3/2)

1+4L1/2C 1/2

4.2 Polyak-Ruppert averaging
Averaging in the context of non-strongly convex functions has been studied before  in particular in
the optimization and machine learning literature  and the following theorems are similar in spirit to
earlier work [7  8  13  14  15]:
Theorem 6 (averaging  no strong convexity) Assume (H1 H2’ H4 H8). Then  if γn = Cn−α  for
α ∈ [1/2  1]  we have
1
E(cid:2)f (¯θn) − f (θ∗)(cid:3) 6

L2(cid:17) exp(cid:0)2L2C2ϕ1−2α(n)(cid:1)

h1+(2LC)1+ 1

C(cid:16)δ0 +

αi+

σ2C
2n

ϕ1−α(n).

n1−α

(12)

(11)

σ2

2

If α = 1/2  then we only have convergence under LC < 1/4 (as in Theorem 4)  with potentially
slow rate  while for α > 1/2  we have a rate of O(n−α)  with otherwise similar behavior than for
the strongly convex case with no bounded gradients. Here  averaging has allowed the rate to go from
O(max{nα−1  n−α/2}) to O(n−α).

1For least-squares regression with kernels  where fn(θ) = 1

2 (yn − hθ  Φ(xn)i)2  with Φ(xn) being the
feature map associated with a reproducing kernel Hilbert space H with universal kernel [28]  then we need that
x 7→ E(Y |X = x) is a function within the RKHS. Taking care of situations where this is not true is clearly of
importance but out of the scope of this paper.

6

]

∗

f

−
)

n

θ
(
f
[

g
o

l

]

∗

f

−
)

θ
(
f
[

g
o

l

n

power 2

 

sgd − 1/3
ave − 1/3
sgd − 1/2
ave − 1/2
sgd − 2/3
ave − 2/3
sgd − 1
ave − 1

1

0

−1

−2

−3

−4

 
0

1

2

3

4

5

log(n)

power 4

 

2

0

−2

]

∗

f

−
)

θ
(
f
[

n

g
o

l

−4

−6

 
0

1

2

log(n)

3

4

5

sgd − 1/3
ave − 1/3
sgd − 1/2
ave − 1/2
sgd − 2/3
ave − 2/3
sgd − 1
ave − 1

Figure 1: Robustness to lack of strong convexity for different learning rates and stochastic gradient
(sgd) and Polyak-Ruppert averaging (ave). From left to right: f (θ) = |θ|2 and f (θ) = |θ|4  (between
−1 and 1  afﬁne outside of [−1  1]  continuously differentiable). See text for details.

α = 1/2

α = 1

5

0

−5
 
0

 

sgd − C=1/5
ave − C=1/5
sgd − C=1
ave − C=1
sgd − C=5
ave − C=5

]

∗

f

−
)

θ
(
f
[

n

g
o

l

2

log(n)

4

 

sgd − C=1/5
ave − C=1/5
sgd − C=1
ave − C=1
sgd − C=5
ave − C=5

5

0

−5
 
0

2

log(n)

4

Figure 2: Robustness to wrong constants for γn = Cn−α. Left: α = 1/2  right: α = 1. See text for
details. Best seen in color.

Theorem 7 (averaging  no strong convexity  bounded gradients) Assume (H1 H5 H8). If γn =
Cn−α  for α ∈ [0  1]  we have

(δ0 + C2B2ϕ1−2α(n)) +

ϕ1−α(n).

(13)

E(cid:2)f (¯θn) − f (θ∗)(cid:3) 6

nα−1
2C

B2
2n

With the bounded gradient assumption (and in fact without smoothness)  we obtain the minimax
asymptotic rate O(n−1/2) up to logarithmic terms [25] for α = 1/2  and  for α < 1/2  the rate
O(n−α) while for α > 1/2  we get O(nα−1). Here  averaging has also allowed to increase the
range of α which ensures convergence  to α ∈ (0  1).
5 Experiments
Robustness to lack of strong convexity. Deﬁne f : R → R as |θ|q for |θ| 6 1 and extended into
a continuously differentiable function  afﬁne outside of [−1  1]. For all q > 1  we have a convex
function with Lipschitz-continuous gradient with constant L = q(q−1). It is strongly convex around
the origin for q ∈ (1  2]  but its second derivative vanishes for q > 2. In Figure 1  we plot in log-log
scale the average of f (θn) − f (θ∗) over 100 replications of the stochastic approximation problem
(with i.i.d. Gaussian noise of standard deviation 4 added to the gradient). For q = 2 (left plot) 
where we locally have a strongly convex case  all learning rates lead to good estimation with decay
proportional to α (as shown in Theorem 1)  while for the averaging case  all reach the exact same
convergence rate (as shown in Theorem 3). However  for q = 4 where strong convexity does not
hold (right plot)  without averaging  α = 1 is still fastest but becomes the slowest after averaging;
on the contrary  illustrating Section 4  slower decays (such as α = 1/2) leads to faster convergence
when averaging is used. Note also the reduction in variability for the averaged iterations.
Robustness to wrong constants. We consider the function on the real line f   deﬁned as f (θ) =
1
2|θ|2 and consider standard i.i.d. Gaussian noise on the gradients. In Figure 2  we plot the average
performance over 100 replications  for various values of C and α. Note that for α = 1/2 (left plot) 
the 3 curves for stochastic gradient descent end up being aligned and equally spaced  corroborating
a rate proportional to C (see Theorem 1). Moreover  when averaging for α = 1/2  the error ends up

7

Selecting rate after n/10 iterations

 

0

−0.5

]

∗

f

−
)

n

θ
(
f
[

1/3 − sgd
1/3 − ave
1/2 − sgd
1/2 − ave
2/3 − sgd
2/3 − ave
1 − sgd
1 − ave

1/3 − sgd
1/3 − ave
1/2 − sgd
1/2 − ave
2/3 − sgd
2/3 − ave
1 − sgd
1 − ave

g
o

l

−1

−1.5
 
0

1

Selecting rate after n/10 iterations

 

n

]

∗

f

−
)

θ
(
f
[

g
o

l

−0.5

−1

−1.5

−2

−2.5

 
0

1

2

3

4

5

log(n)

2
log(n)

3

4

Figure 3: Comparison on non strongly convex logistic regression problems. Left: synthetic example 
right: “alpha” dataset. See text for details. Best seen in color.

being independent of C and α (see Theorem 3). Finally  when C is too large  there is an explosion
(up to 105)  hinting at the potential instability of having C too large. For α = 1 (right plot)  if C is
too small  convergence is very slow (and not at the rate n−1)  as already observed (see  e.g.  [8  6]).
Medium-scale experiments with linear logistic regression. We consider two situations where
H = Rp:
(a) the “alpha” dataset from the Pascal large scale learning challenge (http://
largescale.ml.tu-berlin.de/)  for which p = 500 and n = 50000  and (b) a synthetic ex-
ample where p = 100  n = 100000; we generate the input data i.i.d. from a multivariate Gaussian
distribution with mean zero and a covariance matrix sampled from a Wishart distribution with p
degrees of freedom (thus with potentially bad condition number)  and the output is obtained through
a classiﬁcation by a random hyperplane. For different values of α  we choose C in an adaptive
way where we consider the lowest test error after n/10 iterations  and report results in Figure 3. In
experiments reported in [23]  we also consider C equal to 1/L suggested by our analysis to avoid
large constants  for which the convergence speed is very slow  suggesting that our global bounds in-
volving the Lipschitz constants may be locally far too pessimistic and that designing a truly adaptive
sequence (γn) instead of a ﬁxed one is a fruitful avenue for future research.

6 Conclusion

In this paper  we have provided a non-asymptotic analysis of stochastic gradient  as well as its
averaged version  for various learning rate sequences of the form γn = Cn−α (see summary of
results in Table 1). Following earlier work from the optimization  machine learning and stochastic
approximation literatures  our analysis highlights that α = 1 is not robust to the choice of C and to
the actual difﬁculty of the problem (strongly convex or not). However  when using averaging with
α ∈ (1/2  1)  we get  both in strongly convex and non-strongly convex situation  close to optimal
rates of convergence. Moreover  we highlight the fact that problems with bounded gradients have
better behaviors  i.e.  logistic regression is easier to optimize than least-squares regression.
Our work can be extended in several ways: ﬁrst  we have focused on results in quadratic mean
and we expect that some of our results can be extended to results in high probability (in the line
of [13  3]). Second  we have focused on differentiable objectives  but the extension to objective
functions with a differentiable stochastic part and a non-differentiable deterministic (in the line
of [14]) would allow an extension to sparse methods.
Acknowledgements.
(SIERRA Project). We thank Mark Schmidt and Nicolas Le Roux for helpful discussions.

Francis Bach was partially supported by the European Research Council

SGD SGD SGD
µ  L

µ  B

α
  1/3)
(0
(1/3   1/2)
(1/2   2/3)
(2/3  
1)

α
α
α
α

α
α
α
α

L
×
×
α/2
1 − α

SGD
L  B
×
α/2
1 − α

(3α − 1)/2

Aver.

Aver. Aver.
µ  L
2α
2α
1
1

L
×
×
1 − α 1 − α
1 − α 1 − α

B
α
α

Table 1: Summary of results: For stochastic gradient descent (SGD) or Polyak-Ruppert averaging
(Aver.)  we provide their rates of convergence of the form n−β corresponding to learning rate se-
quences γn = Cn−α  where β is shown as a function of α. For each method  we list the main
assumptions (µ: strong convexity  L: bounded Hessian  B: bounded gradients).

8

References
[1] M. N. Broadie  D. M. Cicek  and A. Zeevi. General bounds and ﬁnite-time improvement for

stochastic approximation algorithms. Technical report  Columbia University  2009.

[2] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applica-

tions. Springer-Verlag  second edition  2003.

[3] O. Yu. Kul′chitski˘ı and A. `E. Mozgovo˘ı. An estimate for the rate of convergence of recurrent

robust identiﬁcation algorithms. Kibernet. i Vychisl. Tekhn.  89:36–39  1991.

[4] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization  30(4):838–855  1992.

[5] D. Ruppert. Efﬁcient estimations from a slowly convergent Robbins-Monro process. Technical

Report 781  Cornell University Operations Research and Industrial Engineering  1988.

[6] V. Fabian. On asymptotic normality in stochastic approximation. The Annals of Mathematical

Statistics  39(4):1327–1332  1968.

[7] Y. Nesterov and J. P. Vial. Conﬁdence level solutions for stochastic programming. Automatica 

44(6):1559–1568  2008.

[8] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[9] L. Bottou and Y. Le Cun. On-line learning for very large data sets. Applied Stochastic Models

in Business and Industry  21(2):137–151  2005.

[10] L. Bottou and O. Bousquet. The tradeoffs of large scale learning.

Information Processing Systems (NIPS)  20  2008.

In Advances in Neural

[11] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size.

In Proc. ICML  2008.

[12] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for svm. In Proc. ICML  2007.

[13] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Stochastic convex optimization.

In Conference on Learning Theory (COLT)  2009.

[14] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.

Journal of Machine Learning Research  9:2543–2596  2010.

[15] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting.

Journal of Machine Learning Research  10:2899–2934  2009.

[16] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and

Examples. Springer  2006.

[17] R. Durrett. Probability: theory and examples. Duxbury Press  third edition  2004.
[18] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  2001.
[19] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press  2004.

[20] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic

Publishers  2004.

[21] K. Sridharan  N. Srebro  and S. Shalev-Shwartz. Fast rates for regularized objectives. Advances

in Neural Information Processing Systems  22  2008.

[22] N. N. Vakhania  V. I. Tarieladze  and S. A. Chobanyan. Probability distributions on Banach

spaces. Reidel  1987.

[23] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for

machine learning. Technical Report 00608041  HAL  2011.

[24] A.S. Nemirovsky and D.B. Yudin. Problem complexity and method efﬁciency in optimization.

Wiley & Sons  1983.

[25] A. Agarwal  P. L. Bartlett  P. Ravikumar  and M. J. Wainwright. Information-theoretic lower
bounds on the oracle complexity of convex optimization  2010. Tech. report  Arxiv 1009.0571.
[26] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for

stochastic strongly-convex optimization. In Proc. COLT  2001.

[27] G. Casella and R. L. Berger. Statistical Inference. Duxbury Press  2001.
[28] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines.

Journal of Machine Learning Research  2:67–93  2002.

9

,Sivan Sabato
Anand Sarwate
Nati Srebro
Hemant Tyagi
Bernd Gärtner
Andreas Krause
Xiangyu Wang
Fangjian Guo
Katherine Heller
David Dunson
Daniel Ritchie
Anna Thomas
Pat Hanrahan
Noah Goodman