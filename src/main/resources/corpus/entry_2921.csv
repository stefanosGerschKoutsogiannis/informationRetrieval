2012,Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs,Given $\alpha \epsilon$  we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\alpha)\ L^*_\gamma + \epsilon$  where   $L^*_\gamma$ is the optimal $\gamma$-margin error rate. For $\alpha   = 1/\gamma$  polynomial time and sample complexity is achievable   using the hinge-loss. For $\alpha = 0$  \cite{ShalevShSr11} showed   that $\poly(1/\gamma)$ time is impossible  while learning is   possible in time $\exp(\tilde{O}(1/\gamma))$.  An immediate   question  which this paper tackles  is what is achievable if $\alpha   \in (0 1/\gamma)$.  We derive positive results interpolating between   the polynomial time for $\alpha = 1/\gamma$ and the exponential   time for $\alpha=0$. In particular  we show that there are cases in   which $\alpha = o(1/\gamma)$ but the problem is still solvable in   polynomial time. Our results naturally extend to the adversarial   online learning model and to the PAC learning with malicious noise   model.,Learning Halfspaces with the Zero-One Loss:

Time-Accuracy Tradeoffs

Aharon Birnbaum and Shai Shalev-Shwartz
School of Computer Science and Engineering

The Hebrew University

Jerusalem  Israel

Abstract

Given α  ϵ  we study the time complexity required to improperly learn a halfs-
∗
∗
pace with misclassiﬁcation error rate of at most (1 + α) L
γ + ϵ  where L
γ is the
optimal γ-margin error rate. For α = 1/γ  polynomial time and sample com-
plexity is achievable using the hinge-loss. For α = 0  Shalev-Shwartz et al.
[2011] showed that poly(1/γ) time is impossible  while learning is possible in
time exp( ˜O(1/γ)). An immediate question  which this paper tackles  is what is
achievable if α ∈ (0  1/γ). We derive positive results interpolating between the
polynomial time for α = 1/γ and the exponential time for α = 0. In particular 
we show that there are cases in which α = o(1/γ) but the problem is still solvable
in polynomial time. Our results naturally extend to the adversarial online learning
model and to the PAC learning with malicious noise model.

1 Introduction

Some of the most inﬂuential machine learning tools are based on the hypothesis class of halfspaces
with margin. Examples include the Perceptron [Rosenblatt  1958]  Support Vector Machines [Vap-
nik  1998]  and AdaBoost [Freund and Schapire  1997]. In this paper we study the computational
complexity of learning halfspaces with margin.
A halfspace is a mapping h(x) = sign(⟨w  x⟩)  where w  x ∈ X are taken from the unit ball of an
RKHS (e.g. Rn)  and ⟨w  x⟩ is their inner-product. Relying on the kernel trick  our sole assumption
on X is that we are able to calculate efﬁciently the inner-product between any two instances (see
for example Sch¨olkopf and Smola [2002]  Cristianini and Shawe-Taylor [2004]). Given an example
(x  y) ∈ X × {±1} and a vector w  we say that w errs on (x  y) if y⟨w  x⟩ ≤ 0 and we say that w
makes a γ-margin error on (x  y) if y⟨w  x⟩ ≤ γ.
The error rate of a predictor h : X → {±1} is deﬁned as L01(h) = P[h(x) ̸= y]  where the
probability is over some unknown distribution over X ×{±1}. The γ-margin error rate of a predictor
x 7→ ⟨w  x⟩ is deﬁned as Lγ(w) = P[y⟨w  x⟩ ≤ γ]. A learning algorithm A receives an i.i.d.
training set S = (x1  y1)  . . .   (xm  ym) and its goal is to return a predictor  A(S)  whose error rate
is small. We study the runtime required to learn a predictor such that with high probability over the
choice of S  the error rate of the learnt predictor satisﬁes

Lγ(w) .

(1)

L01(A(S)) ≤ (1 + α) L
∗
∗
γ + ϵ where L
γ = min

w:∥w∥=1

There are three parameters of interest: the margin parameter  γ  the multiplicative approximation
factor parameter  α  and the additive error parameter ϵ.
From the statistical perspective (i.e.  if we allow exponential runtime)  Equation (1) is achievable
with α = 0 by letting A be the algorithm which minimizes the number of margin errors over the

1

γ2ϵ2 ). See

training set subject to a norm constraint on w. The sample complexity of A is m = ˜Ω( 1
for example Cristianini and Shawe-Taylor [2004].
∗
If the data is separable with margin (that is  L
γ = 0)  then the aforementioned A can be implemented
in time poly(1/γ  1/ϵ). However  the problem is much harder in the agnostic case  namely  when
∗
γ > 0 and the distribution over examples can be arbitrary.
L
Ben-David and Simon [2000] showed that  no proper learning algorithm can satisfy Equation (1)
with α = 0 while running in time polynomial in both 1/γ and 1/ϵ. By “proper” we mean an algo-
rithm which returns a halfspace predictor. Shalev-Shwartz et al. [2011] extended this results to im-
proper learning—that is  when A(S) should satisfy Equation (1) but is not required to be a halfspace.
They also derived an algorithm that satisﬁes Equation (1) and runs in time exp
 
where C is a constant.
Most algorithms that are being used in practice minimize a convex surrogate loss. That is  in-
stead of minimizing the number of mistakes on the training set  the algorithms minimize ˆL(w) =
i=1 ℓ(yi⟨w  xi⟩)  where ℓ : R → R is a convex function that upper bounds the 0 − 1 loss.
1
m
For example  the Support Vector Machine (SVM) algorithm relies on the hinge loss. The advantage
of surrogate convex losses is that minimizing them can be performed in time poly(1/γ  1/ϵ). It is
easy to verify that minimizing ˆL(w) with respect to the hinge loss yields a predictor that satisﬁes
Equation (1) with α = 1
γ . Furthermore  Long and Servedio [2011]  Ben-David et al. [2012] have
shown that any convex surrogate loss cannot guarantee Equation (1) if α < 1
2

)
− 1

γ log( 1

∑

(

(

1
γ

.

C 1

γ ϵ )

)

m

Despite the centrality of this problem  not much is known on the runtime required to guarantee
Equation (1) with other values of α. In particular  a natural question is how the runtime changes
when enlarging α from 0 to 1

γ . Does it change gradually or perhaps there is a phase transition?

γ   let τ = 1

Our main contribution is an upper bound on the required runtime as a function of α. For any α be-
tween1 5 and 1
γ α. We show that the runtime required to guarantee Equation (1) is at most
exp (C τ min{τ  log(1/γ)})  where C is a universal constant (we ignore additional factors which
are polynomial in 1/γ  1/ϵ—see a precise statement with the exact constants in Theorem 1). That
is  when we enlarge α  the runtime decreases gradually from being exponential to being polynomial.
Furthermore  we show that the algorithm which yields the aforementioned bound is a vanilla SVM
with a speciﬁc kernel. We also show how one can design speciﬁc kernels that will ﬁt well certain
values of α while minimizing our upper bound on the sample and time complexity.
In Section 4 we extend our results to the more challenging learning settings of adversarial online
learning and PAC learning with malicious noise. For both cases  we obtain similar upper bounds
on the runtime as a function of α. The technique we use in the malicious noise case may be of
independent interest.

1

γ

log(1/γ)

. In this case  τ =

An interesting special case is when α =
log(1/γ) and hence the
runtime is still polynomial in 1/γ. This recovers a recent result of Long and Servedio [2011]. Their
technique is based on a smooth boosting algorithm applied on top of a weak learner which constructs
random halfspaces and takes their majority vote. Furthermore  Long and Servedio emphasize that
their algorithm is not based on convex optimization. They show that no convex surrogate can obtain
α = o(1/γ). As mentioned before  our technique is rather different as we do rely on the hinge
loss as a surrogate convex loss. There is no contradiction to Long and Servedio since we apply the
convex loss in the feature space induced by our kernel function. The negative result of Long and
Servedio holds only if the convex surrogate is applied on the original space.

√

√

1We did not analyze the case α < 5 because the runtime is already exponential in 1/γ even when α = 5.
Note  however  that our bound for α = 5 is slightly better than the bound of Shalev-Shwartz et al. [2011]
for α = 0 because our bound does not involve the parameter ϵ in the exponent while their bound depends on
exp(1/γ log(1/(ϵγ))).

2

1.1 Additional related work

The problem of learning kernel-based halfspaces has been extensively studied before in the frame-
work of SVM [Vapnik  1998  Cristianini and Shawe-Taylor  2004  Sch¨olkopf and Smola  2002] and
the Perceptron [Freund and Schapire  1999]. Most algorithms replace the 0-1 error function with a
convex surrogate. As mentioned previously  Ben-David et al. [2012] have shown that this approach
leads to approximation factor of at least 1
2

)
− 1

(

1
γ

.

There has been several works attempting to obtain efﬁcient algorithm for the case α = 0 under
certain distributional assumptions. For example  Kalai et al. [2005]  Blais et al. [2008] have shown
that if the marginal data distribution over X is a product distribution  then it is possible to satisfy
Equation (1) with α = γ = 0  in time poly(n1/ϵ4
). Klivans et al. [2009] derived similar results for
the case of malicious noise. Another distributional assumption is on the conditional probability of
the label given the instance. For example  Kalai and Sastry [2009] solves the problem in polynomial
time if there exists a vector w and a monotonically non-increasing function ϕ such that P(Y =
1|X = x) = ϕ(⟨w  x⟩).
Zhang [2004]  Bartlett et al. [2006] also studied the relationship between surrogate convex loss
functions and the 0-1 loss function. They introduce the notion of well calibrated loss functions 
meaning that the excess risk of a predictor h (over the Bayes optimal) with respect to the 0-1 loss
can be bounded using the excess risk of the predictor with respect to the surrogate loss. It follows that
if the latter is close to zero than the former is also close to zero. However  as Ben-David et al. [2012]
show in detail  without making additional distributional assumptions the fact that a loss function is
well calibrated does not yield ﬁnite-sample or ﬁnite-time bounds.
In terms of techniques  our Theorem 1 can be seen as a generalization of the positive result given
in Shalev-Shwartz et al. [2011]. While Shalev-Shwartz et al. only studied the case α = 0  we are
interested in understanding the whole curve of runtime as a function of α. Similar to the analysis of
Shalev-Shwartz et al.  we approximate the sigmoidal and erf transfer functions using polynomials.
However  we need to break symmetry in the deﬁnition of the exact transfer function to approximate.
The main technical observation is that the Lipschitz constant of the transfer functions we approx-
imate does not depend on α  and is roughly 1/γ no matter what α is. Instead  the change of the
transfer function when α is increasing is in higher order derivatives.
To the best of our knowledge  the only middle point on the curve that has been studied before is the
case α =
  which was analyzed in Long and Servedio [2011]. Our work shows an upper
bound on the entire curve. Besides that  we also provide a recipe for constructing better kernels for
speciﬁc values of α.

log(1/γ)

√

γ

1

2 Main Results

Our main result is an upper bound on the time and sample complexity for all values of α between
5 and 1/γ. The bounds we derive hold for a norm-constraint form of SVM with a speciﬁc kernel 
which we recall now. Given a training set S = (x1  y1)  . . .   (xm  ym)  and a feature mapping
ψ : X → X ′  where X ′ is the unit ball of some Hilbert space  consider the following learning rule:

max{0  1 − yi⟨v  ψ(xi)⟩} .

(2)
∑
)⟩  and
Using the well known kernel-trick  if K(x  x
G is an m × m matrix with Gi j = K(xi  xj)  then we can write a solution of Equation (2) as

) implements the inner product ⟨ψ(x)  ψ(x

argmin
v:∥v∥2≤B

i=1

′

′

v =

i aiψ(xi) where the vector a ∈ Rm is a solution of

max{0  1 − yi (Ga)i} .
(
)
m∑
The above is a convex optimization problem in m variables and can be solved in time poly(m).
Given a solution a ∈ Rm  we deﬁne a classiﬁer ha : X → {±1} to be

argmin
a:aT Ga≤B

(3)

i=1

m∑

m∑

ha(x) = sign

(4)

aiK(xi  x)

.

i=1

3

The upper bounds we derive hold for the above kernel-based SVM with the kernel function

We are now ready to state our main theorem.
Theorem 1 For any γ ∈ (0  1/2) and α ≥ 5  let τ = 1

′

2

) =

K(x  x

1 − 1

1
⟨x  x′⟩ .
)
96τ 2 + e18τ log(8τ α2)+5
= poly(1/γ) · emin{18τ log(8τ α2)   4τ 2}

γ α and let
1
γ2

{

(

4α2

.

 

B = min

Fix ϵ  δ ∈ (0  1/2) and let m be a training set size that satisﬁes

(5)

(

0.06 e4τ 2

)}

+ 3

m ≥ 16

ϵ2 max{2B  (1 + α)2 log(2/δ)} .

Let A be the algorithm which solves Equation (3) with the kernel function given in Equation (5) 
and returns the predictor deﬁned in Equation (4). Then  for any distribution  with probability of at
least 1 − δ  the algorithm A satisﬁes Equation (1).
The proof of the theorem is given in the next section. As a direct corollary we obtain that there is an
efﬁcient algorithm that achieves an approximation factor of α = o(1/γ):
Corollary 2 For any ϵ  δ  γ ∈ (0  1)  let α =
being as deﬁned in Theorem 1  the algorithm A satisﬁes Equation (1).
As another corollary of Theorem 1 we obtain that for any constant c ∈ (0  1)  it is possible to satisfy
Equation (1) with α = c/γ in polynomial time. However  the dependence of the runtime on the
constant c is e4/c2. For example  for c = 1/2 we obtain the multiplicative factor e16 ≈ 8  800  000.
Our next contribution is to show that a more careful design of the kernel function can yield better
bounds.

γ2 . Then  with m  A

and let B = 0.06

γ6 + 3

1/γ√

log(1/γ)

∑
j=1 βjz2j−1 (namely  p is
|p(z)| ≥ 1 .

d

Theorem 3 For any γ  α  let p be a polynomial of the form p(z) =
odd) that satisﬁes

max
z∈[−1 1]

|p(z)| ≤ α and min
z:|z|≥γ

Let m be a training set size that satisﬁes

Let A be the algorithm which solves Equation (3) with the following kernel function

m ≥ 16

ϵ2 max{∥β∥2

1  2 log(4/δ)  (1 + α)2 log(2/δ)}
d∑

|βj|(⟨x  x

′⟩)2j−1  

′

K(x  x

) =

j=1

and returns the predictor deﬁned in Equation (4). Then  for any distribution  with probability of at
least 1 − δ  the algorithm A satisﬁes Equation (1).

∑
The above theorem provides us with a recipe for constructing good kernel functions: Given γ and α 
j=1 βjz2j−1 satisﬁes the
ﬁnd a vector β with minimal ℓ1 norm such that the polynomial p(z) =
conditions given in Theorem 3. For a ﬁxed degree d  this can be written as the following optimization
problem:

d

∥β∥1

s.t. ∀x ∈ [0  1]  p(z) ≤ α ∧ ∀z ∈ [γ  1]  p(z) ≥ 1 .

(6)

min
β∈Rd

Note that for any x  the expression p(x) is a linear function of β. Therefore  the above problem is
a linear program with an inﬁnite number of constraints. Nevertheless  it can be solved efﬁciently
using the Ellipsoid algorithm. Indeed  for any β  we can ﬁnd the extreme points of the polynomial

4

it deﬁnes  and then determine whether β satisﬁes all the constraints or  if it doesn’t  we can ﬁnd a
violated constraint.
To demonstrate how Theorem 3 can yield a better guarantee (in terms of the constants)  we solved
Equation (6) for the simple case of d = 2. For this simple case  we can provide an analytic solution
to Equation (6)  and based on this solution we obtain the following lemma whose proof is provided
in the appendix.
Lemma 4 Given γ < 2/3  consider the polynomial p(z) = β1z + β2z3  where

β1 = 1

γ + γ

1+γ

  β2 = − 1

γ(1+γ) .

Then  p satisﬁes the conditions of Theorem 3 with

√
α = 2
3

3γ

+ 2√

3

≤ 0.385 · 1

γ + 1.155 .

γ + 1.

Furthermore  ∥β∥1 ≤ 2
It is interesting to compare the guarantee given in the above lemma to the guarantee of using the
vanilla hinge-loss. For both cases the sample complexity is order of
γ2ϵ2 . For the vanilla hinge-
γ while for the kernel given in Lemma 4 we obtain the
loss we obtain the approximation factor 1
approximation factor of α ≤ 0.385· 1
γ + 1.155. Recall that Ben-David et al. [2012] have shown that
without utilizing kernels  no convex surrogate loss can guarantee an approximation factor smaller
− 1). The above discussion shows that applying the hinge-loss with a kernel function
than α < 1
can break this barrier without a signiﬁcant increase in runtime2 or sample complexity.

2 ( 1

γ

1

m

∑

3 Proofs
Given a scalar loss function ℓ : R → R  and a vector w  we denote by L(w) = E(x y)∼D[ℓ(y⟨w  x⟩)]
the expected loss value of the predictions of w with respect to a distribution D over X × {±1}.
i=1 ℓ(yi⟨w  xi⟩)
Given a training set S = (x1  y1)  . . .   (xm  ym)  we denote by ˆL(w) = 1
m
the empirical loss of w. We slightly overload our notation and also use L(w) to denote
E(x y)∼D[ℓ(y⟨w  ψ(x)⟩)]  when w is an element of an RKHS corresponding to the mapping ψ. We
deﬁne ˆL(w) analogously.
We will make extensive use of the following loss functions: the zero-one loss  ℓ01(z) = 1[z ≤ 0]  the
γ-zero-one loss  ℓγ(z) = 1[z ≤ γ]  the hinge-loss  ℓh(z) = [1−z]+ = max{0  1−z}  and the ramp-
loss  ℓramp(z) = min{1  ℓh(z)}. We will use L01(w)  Lγ(w)  Lh(w)  and Lramp(w) to denote
the expectations with respect to the different loss functions. Similarly ˆL01(w)  ˆLγ(w)  ˆLh(w)  and
ˆLramp(w) are the empirical losses of w with respect to the different loss functions.
Recall that we output a vector v that solves Equation (3). This vector is in the RKHS corresponding
to the kernel given in Equation (5). Let Bx = maxx∈X K(x  x) ≤ 2. Since the ramp-loss upper
bounds the zero-one loss we have that L01(v) ≤ Lramp(v). The advantage of using the ramp loss is
that it is both a Lipschitz function and it is bounded by 1. Hence  standard Rademacher generaliza-
tion analysis (e.g. Bartlett and Mendelson [2002]  Bousquet [2002]) yields that with probability of
at least 1 − δ/2 over the choice of S we have:

|
Lramp(v) ≤ ˆLramp(v) + 2

BxB

m

+

2 ln(4/δ)

m

}

√

√
{z

.

(7)

=ϵ1

Since the ramp loss is upper bounded by the hinge-loss  we have shown the following inequalities 
(8)

L01(v) ≤ Lramp(v) ≤ ˆLramp(v) + ϵ1 ≤ ˆLh(v) + ϵ1 .

Next  we rely on the following claim adapted from [Shalev-Shwartz et al.  2011  Lemma 2.4]:

2It should be noted that solving SVM with kernels takes more time than solving a linear SVM. Hence  if
the original instance space is a low dimensional Euclidean space we loose polynomially in the time complexity.
However  when the original instance space is also an RKHS  and our kernel is composed on top of the original
kernel  the increase in the time complexity is not signiﬁcant.

5

∑∞

∑∞

j=0 βjzj be any polynomial that satisﬁes

j 2j ≤ B  and let w be
Claim 5 Let p(z) =
any vector in X . Then  there exists vw in the RKHS deﬁned by the kernel given in Equation (5)  such
that ∥vw∥2 ≤ B and for all x ∈ X   ⟨vw  ψ(x)⟩ = p(⟨w  x⟩).
For any polynomial p  let ℓp(z) = ℓh(p(z))  and let ˆLp be deﬁned analogously. If p is an odd
polynomial  we have that ℓp(y⟨w  x⟩) = [1 − yp(⟨w  x⟩)]+. By the deﬁnition of v as minimizing
ˆLh(v) over ∥v∥2 ≤ B  it follows from the above claim that for any odd p that satisﬁes
j 2j ≤
B and for any w

∗ ∈ X  we have that

∑∞

j=0 β2

j=0 β2

ˆLh(v) ≤ ˆLh(vw∗ ) = ˆLp(w

∗

) .

Next  it is straightforward to verify that if p is an odd polynomial that satisﬁes:

max
z∈[−1 1]

|p(z)| ≤ α and min
z∈[γ 1]

p(z) ≥ 1

∗

then  ℓp(z) ≤ (1 + α)ℓγ(z) for all z ∈ [−1  1]. For such polynomials  we have that ˆLp(w
(1 + α) ˆLγ(w
probability of at least 1 − δ/2 over the choice of S we have that
) + ϵ2 .

). Finally  by Hoeffding’s inequality  for any ﬁxed w

∗  if m > log(2/δ)

) ≤ Lγ(w

ˆLγ(w

ϵ2
2

∗

∗

So  overall  we have obtained that with probability of at least 1 − δ 

(9)
) ≤
  then with

∗

L01(v) ≤ (1 + α) Lγ(w

∗

) + (1 + α)ϵ2 + ϵ1 .

Choosing m large enough so that (1 + α)ϵ2 + ϵ1 ≤ ϵ  we obtain:
Corollary 6 Fix γ  ϵ  δ ∈ (0  1) and α > 0. Let p be an odd polynomial such that
and such that Equation (9) holds. Let m be a training set size that satisﬁes:
· max{2B  2 log(4/δ)  (1 + α)2 log(2/δ)} .

m ≥ 16
ϵ2

∑

j 2j ≤ B

j β2

Then  with probability of at least 1−δ  the solution of Equation (3) satisﬁes  L01(v) ≤ (1+α)L
∗
γ +ϵ.
The proof of Theorem 1 follows immediately from the above corollary together with the following
two lemmas  whose proofs are provided in the appendix.

(

)

Lemma 7 For any γ > 0 and α > 2  let τ = 1
(
exists a polynomial that satisﬁes the conditions of Corollary 6 with the parameters α  γ  B.
Lemma 8 For any γ ∈ (0  1/2) and α ∈ [5  1
γ ] 
4α2
ditions of Corollary 6 with the parameters α  γ  B.

αγ and let B =
. Then  there exists a polynomial that satisﬁes the con-

αγ and let B = 1
γ2

. Then  there

96τ 2 + exp

let τ =

0.06 e4τ 2

))

18τ log

8τ α2

(

)

(

+ 3

+ 5

1

3.1 Proof of Theorem 3

d

∑
The proof is similar to the proof of Theorem 1 except that we replace Claim 5 with the following:
j=1 βjz2j−1 be any polynomial  and let w be any vector in X . Then  there
Lemma 9 Let p(z) =
exists vw in the RKHS deﬁned by the kernel given in Theorem 3  such that ∥vw∥2 ≤ ∥β∥1 and for
all x ∈ X   ⟨vw  ψ(x)⟩ = p(⟨w  x⟩).
Proof We start with an explicit deﬁnition of the mapping ψ(x) corresponding to the kernel in the
theorem. The coordinates of ψ(x) are indexed by tuples (k1  . . .   kj) ∈ [n]j for j = 1  3  . . .   2d−1.
Coordinate (k1  . . .   kj) equals to
plicitly the vector vw for which ⟨vw  ψ(x)⟩ = p(⟨w  x⟩). Coordinate (k1  . . . kj) of vw equals to
sign(βj)
⟨vw  ψ(x)⟩ = p(⟨w  x⟩).
Since for any x ∈ X we also have that K(x  x) ≤ ∥β∥1  the proof of Theorem 3 follows using the
same arguments as in the proof of Theorem 1.

√|βj|xk1xk2 . . . xkj . Next  for any w ∈ X   we deﬁne ex-
√|βj|wk1wk2 . . . wkj . It is easy to verify that indeed ∥vw∥2 ≤ ∥β∥1 and for all x ∈ X  

6

4 Extension to other learning models

In this section we brieﬂy describe how our results can be extended to adversarial online learning and
to PAC learning with malicious noise. We start with the online learning model.

4.1 Online learning

m

t=1 ℓh(yt⟨w

Given a sequence (x1  y1)  . . .   (xm  ym)  and a vector w

Online learning is performed in a sequence of consecutive rounds  where at round t the learner is
given an instance  xt ∈ X   and is required to predict its label. After predicting ˆyt  the target label 
yt  is revealed. The goal of the learner is to make as few prediction mistakes as possible. See for
example Cesa-Bianchi and Lugosi [2006].
A classic online classiﬁcation algorithm is the Perceptron [Rosenblatt  1958]. The Perceptron main-
tains a vector wt and predicts according to ˆyt = sign(⟨wt  xt⟩). Initially  w1 = 0  and at round t
the Perceptron updates the vector using the rule wt+1 = wt + 1[ˆyt ̸= yt] ytxt. Freund and Schapire
[1999] observed that the Perceptron can also be implemented efﬁciently in an RKHS using a kernel
function.
∗ such that for all t  yt⟨w
  xt⟩ ≥ 1 and
Agmon [1954] and others have shown that if there exists w
∥xt∥2 ≤ Bx  then the Perceptron will make at most ∥w
∗∥2Bx prediction mistakes. This bound
holds without making any additional distributional assumptions on the sequence of examples.
∑
This mistake bound has been generalized to the noisy case (see for example Gentile [2003])
as follows.
) =
  xt⟩)  where ℓh is the hinge-loss. Then  the average number of prediction mis-
1
m∑
m
takes the Perceptron will make on this sequence is at most
1[ˆyt ̸= yt] ≤ Lh(w
∑
)
t=1 1(yt⟨w
∗
Lγ(w

  xt⟩ ≤ γ). Trivially  Equation (10) can yield a bound whose
Let Lγ(w
) (namely  it corresponds to α = 1
γ ). On the other hand  Ben-David
leading term is
et al. [2009] have derived a mistake bound whose leading term depends on Lγ(w
) (namely  it
corresponds to α = 0)  but the runtime of the algorithm is at least m1/γ2. The main result of this
section is to derive a mistake bound for the Perceptron based on all values of α between 5 and 1/γ.
Theorem 10 For any γ ∈ (0  1/2) and α ≥ 5  let τ = 1
γ α and let Bα γ be the value of B as
deﬁned in Theorem 1. Then  for any sequence (x1  y1)  . . .   (xm  ym)  if the Perceptron is run on
this sequence using the kernel function given in Equation (5)  the average number of prediction
mistakes it will make is at most:

Bx∥w∗∥2 Lh(w∗)

) = 1
m
1 + 1
γ

Bx∥w

∗∥2

let Lh(w

1
m

(

t=1

m

√

∗

) +

m

+

.

m

∗ 

∗

∗

∗

∗

∗

(10)

∗

√

γ∈(0 1/2) α≥5 w∗∈X

min

(1 + α)Lγ(w

∗

) +

2Bα γ (1 + α)Lγ(w∗)

m

+

2Bα γ

m

Proof [sketch] Equation (10) holds if we implement the Perceptron using the kernel function given
in Equation (5)  for which Bx = 2. Furthermore  similarly to the proof of Theorem 1  for any
∗ in the RKHS
polynomial p that satisﬁes the conditions of Corollary 6 we have that there exists v
corresponding to the kernel  with ∥v
) ≤ (1 + α)Lγ(w
∗
). The theorem
follows.

∗∥2 ≤ B and with Lh(v

∗

4.2 PAC learning with malicious noise

In this model  introduced by Valiant [1985] and speciﬁed to the case of halfspaces with margin
by Servedio [2003]  Long and Servedio [2011]  there is an unknown distribution over instances
in X and there is an unknown target vector w
  x⟩| ≥ γ with probability 1.
The learner has an access to an example oracle. At each query to the oracle  with probability of
1 − η it samples a random example x ∈ X according to the unknown distribution over X   and

∗ ∈ X such that |⟨w

∗

7

∗

returns (x  sign(⟨w
  x⟩)). However  with probability η  the oracle returns an arbitrary element of
X × {±1}. The goal of the learner is to output a predictor that has L01(h) ≤ ϵ  with respect to the
“clean” distribution.
Auer and Cesa-Bianchi [1998] described a general conversion from online learning to the malicious
noise setting. Servedio [2003] used this conversion to derive a bound based on the Perceptron’s
mistake bound. In our case  we cannot rely on the conversion of Auer and Cesa-Bianchi [1998]
since it requires a proper learner  while the online learner described in the previous section is not
proper.
Instead  we propose the following simple algorithm. First  sample m examples. Then  solve kernel
SVM on the resulting noisy training set.
Theorem 11 Let γ ∈ (0  1/4)  δ ∈ (0  1/2)  and α > 5. Let B be as deﬁned in Theorem 1.
Let m be a training set size that satisﬁes: m ≥ 64
. Then  with
probability of at least 1− 2δ  the output of kernel-SVM on the noisy training set  denoted h  satisﬁes
L01(h) ≤ (2 + α)η + ϵ/2. It follows that if η ≤ ϵ

ϵ2 · max
2(2+α) then L01(h) ≤ ϵ.

2B   (2 + α)2 log(1/δ)

}

{

Proof Let ¯S be a training set in which we replace the noisy examples with clean iid examples. Let
¯L denotes the empirical loss over ¯S and ˆL denotes the empirical loss over S. As in the proof of
Theorem 1  we have that w.p. of at least 1 − δ  for any v in the RKHS corresponding to the kernel
that satisﬁes ∥v∥2 ≤ B we have that:

(11)
by our assumption on m. Let ˆη be the fraction of noisy examples in S. Note that ¯S and S differ in
at most mˆη elements. Therefore  for any v 

L01(v) ≤ ¯Lramp(v) + 3ϵ/8  

¯Lramp(v) ≤ ˆLramp(v) + ˆη .

(12)
∗ be the target vector in the original space (i.e.  the one which
Now  let v be the minimizer of ˆLh  let w
achieves correct predictions with margin γ on clean examples)  and let vw∗ be its corresponding
element in the RKHS (see Claim 5). We have

ˆLramp(v) ≤ ˆLh(v) ≤ ˆLh(vw∗ ) = ˆLp(w

(13)
In the above  the ﬁrst inequality is since the ramp loss is upper bounded by the hinge loss  the second
inequality is by the deﬁnition of v  the third equality is by Claim 5  the fourth inequality is by the
properties of p  and the last inequality follows from the deﬁnition of ˆη. Combining the above yields 

) ≤ (1 + α) ˆLγ(w

) ≤ (1 + α)ˆη .

∗

∗

L01(v) ≤ (2 + α)ˆη + 3ϵ/8 .

Finally  using Hoefding’s inequality  we know that for the deﬁnition of m  with probability of at
least 1 − δ we have that ˆη ≤ η + ϵ
8(2+α). Applying the union bound and combining the above we
conclude that with probability of at least 1 − 2δ  L01(v) ≤ (2 + α)η + ϵ/2.

5 Summary and Open Problems

We have derived upper bounds on the time and sample complexities as a function of the approxima-
tion factor. We further provided a recipe for designing kernel functions with a small time and sample
complexity for any given value of approximation factor and margin. Our results are applicable to
agnostic PAC Learning  online learning  and PAC learning with malicious noise.
An immediate open question is whether our results can be improved. If not  can computationally
hardness results be formally established. Another open question is whether the upper bounds we
have derived for an improper learner can be also derived for a proper learner.

Acknowledgements: This work is supported by the Israeli Science Foundation grant number 598-
10 and by the German-Israeli Foundation grant number 2254-2010. Shai Shalev-Shwartz is incum-
bent of the John S. Cohen Senior Lectureship in Computer Science.

8

References
S. Agmon. The relaxation method for linear inequalities. Canadian Journal of Mathematics  6(3):382–392 

1954.

P. Auer and N. Cesa-Bianchi. On-line learning with malicious noise and the closure algorithm. Annals of

mathematics and artiﬁcial intelligence  23(1):83–99  1998.

P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results.

Journal of Machine Learning Research  3:463–482  2002.

P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal of the

American Statistical Association  101:138–156  2006.

S. Ben-David and H. Simon. Efﬁcient learning of linear perceptrons. In NIPS  2000.
S. Ben-David  D. Pal    and S. Shalev-Shwartz. Agnostic online learning. In COLT  2009.
S. Ben-David  D. Loker  N. Srebro  and K. Sridharan. Minimizing the misclassiﬁcation error rate using a

surrogate convex loss. In ICML  2012.

E. Blais  R. O’Donnell  and K Wimmer. Polynomial regression under arbitrary product distributions. In COLT 

2008.

O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning

Algorithms. PhD thesis  Ecole Polytechnique  2002.

N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
N. Cristianini and J. Shawe-Taylor. Kernel Methods for Pattern Analysis. Cambridge University Press  2004.
Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning 

37(3):277–296  1999.

Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of Computer and System Sciences  55(1):119–139  August 1997.

C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3):265–299  2003.
A. Kalai  A.R. Klivans  Y. Mansour  and R. Servedio. Agnostically learning halfspaces. In Proceedings of the

46th Foundations of Computer Science (FOCS)  2005.

A.T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In Proceedings of the

22th Annual Conference on Learning Theory  2009.

A.R. Klivans  P.M. Long  and R.A. Servedio. Learning halfspaces with malicious noise. The Journal of Machine

Learning Research  10:2715–2740  2009.

P.M. Long and R.A. Servedio. Learning large-margin halfspaces with more malicious noise. In NIPS  2011.
F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain.

Psychological Review  65:386–407  1958. (Reprinted in Neurocomputing(MIT Press  1988).).

B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Optimization

and Beyond. MIT Press  2002.

R.A. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning Research  4:

633–648  2003.

S. Shalev-Shwartz  O. Shamir  and K. Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM

Journal on Computing  40:1623–1646  2011.

L. G. Valiant. Learning disjunctions of conjunctions. In Proceedings of the 9th International Joint Conference

on Artiﬁcial Intelligence  pages 560–566  August 1985.

V. N. Vapnik. Statistical Learning Theory. Wiley  1998.
T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization.

The Annals of Statistics  32:56–85  2004.

9

,Miguel Carreira-Perpinan
Ramin Raziperchikolaei