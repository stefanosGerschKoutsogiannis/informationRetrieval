2018,Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization,Symmetric nonnegative matrix factorization (NMF)---a special but important class of the general NMF---is demonstrated to be useful for data analysis and in particular for various clustering tasks. Unfortunately  designing fast algorithms for Symmetric NMF is not as easy as for the nonsymmetric counterpart  the latter admitting the splitting property that allows efficient alternating-type algorithms. To overcome this issue  we transfer the symmetric NMF to a nonsymmetric one  then we can adopt the idea from the state-of-the-art algorithms for nonsymmetric NMF to design fast algorithms solving symmetric NMF.  We rigorously establish that solving nonsymmetric reformulation returns a solution for symmetric NMF and then apply fast alternating based algorithms for the corresponding reformulated problem. Furthermore  we show these fast algorithms admit strong convergence guarantee in the sense that the generated sequence is convergent at least at a sublinear rate and it converges globally to a critical point of the symmetric NMF.  We conduct experiments on both synthetic data and image clustering to support our result.,Dropping Symmetry for Fast

Symmetric Nonnegative Matrix Factorization

Zhihui Zhu∗

Mathematical Institute for Data Science

Johns Hopkins University

Baltimore  MD  USA

zzhu29@jhu.edu

Xiao Li∗

Department of Electronic Engineering
The Chinese University of Hong Kong

Shatin  NT  Hong Kong

xli@ee.cuhk.edu.hk

Kai Liu

Qiuwei Li

Department of Computer Science

Colorado School of Mines

Department of Electrical Engineering

Colorado School of Mines

Golden  CO  USA
kaliu@mines.edu

Golden  CO  USA
qiuli@mines.edu

Abstract

Symmetric nonnegative matrix factorization (NMF)—a special but important class
of the general NMF—is demonstrated to be useful for data analysis and in particular
for various clustering tasks. Unfortunately  designing fast algorithms for Symmetric
NMF is not as easy as for the nonsymmetric counterpart  the later admitting the
splitting property that allows efﬁcient alternating-type algorithms. To overcome this
issue  we transfer the symmetric NMF to a nonsymmetric one  then we can adopt
the idea from the state-of-the-art algorithms for nonsymmetric NMF to design
fast algorithms solving symmetric NMF. We rigorously establish that solving
nonsymmetric reformulation returns a solution for symmetric NMF and then apply
fast alternating based algorithms for the corresponding reformulated problem.
Furthermore  we show these fast algorithms admit strong convergence guarantee
in the sense that the generated sequence is convergent at least at a sublinear rate
and it converges globally to a critical point of the symmetric NMF. We conduct
experiments on both synthetic data and image clustering to support our result.

1

Introduction

General nonnegative matrix factorization (NMF) is referred to the following problem: Given a matrix
Y ∈ Rn×m and a factorization rank r  solve

s. t. U ≥ 0  V ≥ 0 

(cid:107)Y − U V T(cid:107)2
F  

1
2

min

U∈Rn×r V ∈Rm×r

(1)
where U ≥ 0 means each element in U is nonnegative. NMF has been successfully used in the
applications of face feature extraction [1  2]  document clustering [3]  source separation [4] and many
others [5]. Because of the ubiquitous applications of NMF  many efﬁcient algorithms have been
proposed for solving (1). Well-known algorithms include MUA [6]  projected gradientd descent [7] 
alternating nonnegative least squares (ANLS) [8]  and hierarchical ALS (HALS) [9]. In particular 
ANLS (which uses the block principal pivoting algorithm to very efﬁciently solve the nonnegative
least squares) and HALS achive the state-of-the-art performance.

∗Equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

One special but important class of NMF  called symmetric NMF  requires the two factors U and V
identical  i.e.  it factorizes a PSD matrix X ∈ Rn×n by solving

min

U∈Rn×r

1
2

(cid:107)X − U U T(cid:107)2
F  

s. t. U ≥ 0.

(2)

As a contrast  (1) is referred to as nonsymmetric NMF. Symmetric NMF (2) has its own applications
in data analysis  machine learning and signal processing [10  11  12]. In particular the symmetric
NMF is equivalent to the classical K-means kernel clustering in [11]and it is inherently suitable for
clustering nonlinearly separable data from a similarity matrix [10].
In the ﬁrst glance  since (2) has only one variable  one may think it
is easier to solve (2) than (1)  or at least (2) can be solved by directly
utilizing efﬁcient algorithms developed for nonsymmetric NMF.
However  the state-of-the-art alternating based algorithms (such as
ANLS and HALS) for nonsymmetric NMF utilize the splitting prop-
erty of (1) and thus can not be used for (2). On the other hand 
ﬁrst-order method such as projected gradient descent (PGD) for solv-
ing (2) suffers from very slow convergence. As a proof of concept 
we show in Figure 1 the convergence of PGD for solving symmetric
NMF and as a comparison  the convergence of gradient descent
(GD) for solving a matrix factorization (MF) (i.e.  (2) without the
nonnegative constraint) which is proved to admit linear convergence
[13  14]. This phenomenon also appears in nonsymmetric NMF and
is the main motivation to have many efﬁcient algorithms such as
ANLS and HALS.

Figure 1: Convergence of MF by
GD and symmetric NMF by PGD
with the same initialization.

Main Contributions This paper addresses the above issue by considering a simple framework that
allows us to design alternating-type algorithms for solving the symmetric NMF  which are similar to
alternating minimization algorithms (such as ANLS and HALS) developed for nonsymmetric NMF.
The main contributions of this paper are summarized as follows.
• Motivated by the splitting property exploited in ANLS and HALS algorithms  we split the bilinear
form of U into two different factors and transfer the symmetric NMF into a nonsymmetric one:

min
U  V

f (U   V ) =

(cid:107)X − U V T(cid:107)2

F +

1
2

(cid:107)U − V (cid:107)2
F  

λ
2

s. t. U ≥ 0  V ≥ 0 

(3)

where the regularizer (cid:107)U − V (cid:107)2
F is introduced to force the two factors identical and λ > 0 is a
balancing factor. The ﬁrst main contribution is to guarantee that any critical point of (3) that has
bounded energy satisﬁes U = V with a sufﬁciently large λ. We further show that any local-search
algorithm with a decreasing property is guaranteed to solve (2) by targeting (3). To the best of our
knowledge  this is the ﬁrst work to rigorously establish that symmetric NMF can be efﬁciently
solved by fast alternating-type algorithms.
• Our second contribution is to provide convergence analysis for our proposed alternating-based
algorithms solving (3). By exploiting the speciﬁc structure in (3)  we show that our proposed
algorithms (without any proximal terms and any additional constraints on U and V except the
nonnegative constraint) is convergent. Moreover  we establish the point-wise global sequence
convergence and show that the proposed alternating-type algorithms achieve at least a global
sublinear convergence rate. Our sequence convergence result provides theoretical guarantees for
the practical utilization of alternating-based algorithms directly solving (3) without any proximal
terms or additional constraint on the factors which are usually needed to guarantee the convergence.

Related Work Due to slow convergence of PGD for solving symmetric NMF  different algorithms
have been proposed to efﬁciently solve (2)  either in a direct way or similar to (3) by splitting the two
factors. Vandaele et al. [15] proposed an alternating algorithm that cyclically optimizes over each
element in U by solving a nonnegative constrained nonconvex univariate fourth order polynomial
minimization. A quasi newton second order method was used in [10] to directly solve the symmetric
NMF optimization problem (2). However  both the element-wise updating approach and the second
order method are observed to be computationally expensive in large scale applications. We will
illustrate this with experiments in Section 4.

2

01000200030004000500010-10100NMF by PGDMF by GDThe idea of solving symmetric NMF by targeting (3) also appears in [10]. However  despite an
algorithm used for solving (3)  no other formal guarantee (such as solving (3) returns a solution of
(2)) was provided in [10]. Lu et al. [16] considered an alternative problem to (3) that also enjoys
the splitting property and utilized alternating direction method of multipliers (ADMM) algorithm
to tackle the corresponding problem with equality constraint (i.e.  U = V ). Unlike the sequence
convergence guarantee of algorithms solving (3)  the ADMM is only guaranteed to have a subsequence
convergence in [16] with an additional proximal term2 and constraint on the boundedness of columns
of U  rendering the problem hard to solve.
A different line of research that may be also related to our work is the classical result on exact penalty
methods [17  18].3 On one hand  our work shares similar spirit to the exact penalty methods as both
approaches attempt to transfer constraints as penalties into the objective function. On the other hand 
our approach differs from the exact penalty methods in the following three folds: 1) as stated in [18] 
most of the results on exact penalty methods require the feasible set compact  which is not satisﬁed
in our problem (2); 2) the exact penalty methods replace all the constraints with certain penalty
functions  while our reformulated problem (3) still has the nonnegative constriant; 3) in general
the exact penalty methods use either nonsmooth penalty functions or differentiable exact penalty
functions which are based on replacing the conventional multipliers with continuously differentiable
multiplier functions  while the penalty in (3) is smooth and the parameter λ is ﬁxed and is independent
of U and V . We comment that our result builds on the speciﬁc structure of the objective function in
(3) while the exact penalty methods focus on general nonlinear programming problems.
Finally  our work is also closely related to recent advances in convergence analysis for alternating
minimizations. The sequence convergence result for general alternating minimization with an
additional proximal term was provided in [19]. When speciﬁed to NMF  as pointed out in [20]  with
the aid of this additional proximal term (and also an additional constraint to bound the factors)  the
convergence of ANLS and HALS can be established from [19  21]. With similar proximal term
and constraint  the subsequence convergence of ADMM for symmetric NMF was obtained in [16].
Although the convergence of these algorithms are observed without the proximal term and constraint
(which are also not used in practice)  these are in general necessary to formally show the convergence
of the algorithms. For alternating minimization methods solving (3)  without any additional constraint 
we show the factors are indeed bounded through the iterations  and without the proximal term  the
algorithms admit sufﬁcient decreasing property. These observations then guarantee the sequence
convergence of the original algorithms that are used in practice. The convergence result for algorithms
solving (3) is not only limited to alternating-type algorithms  though we only consider these as they
achieve state-of-the-art performance.

2 Guarantee when Transfering Symmetric NMF to Nonsymmetric NMF

Compared with (2)  in the ﬁrst glance  (3) is slightly more complicated as it has one more variable.
However  because of this new variable  f (U   V ) is now strongly convex with respect to either U or V  
though it is still nonconvex in terms of the joint variable (U   V ). Moreover  the two decision variables
U and V in (3) are well splitted  like the case in nonsymmetric NMF. This observation suggests an
interesting and useful fact that (3) can be solved by designing tailored alternating minimization type
algorithms developed for tackling nonsymmetric NMF. On the other hand  a theoretical question
raised in the regularized form (3) is that we are not guaranteed U = V and hence solving (3) is not
equivalent to solving (2). In this section  we provide formal guarantee to assure that solving (3) (to a
critical point) indeed gives a critical point solution of (2). Note that (3) is nonconvex and many local
search algorithms are only guaranteed to converge to a critical point rather than the global solution.
Thus  our goal is to guarantee that any critical point that the algorithms may converge to admits the
property that the two factors U and V are identical and further that U is a solution ( critical point) of
the original symmetric NMF (2).
Before stating out the formal result  as an intuitive example  we ﬁrst consider a simple case where
f (u  v) = (1−uv)2/2+λ(u−v)2/2. Its derivative is ∂uf (u  v) = (uv−1)v+λ(u−v)  ∂vf (u  v) =
(uv − 1)u − λ(u − v). Thus  any critical point of f satisﬁes (uv − 1)v + λ(u − v) = 0 and
(uv − 1)u − λ(u − v) = 0  which further indicates that (u − v)(2λ + 1 − uv) = 0. Therefore 

2In k-th iteration  a proximal term (cid:107)U − U k−1(cid:107)2
3We greatly acknolwledge the anonymous area chair for pointing out this related work.

F is added to the objective function when updating U.

3

for any critical point (u  v) such that |uv| < 2λ + 1  it must satisfy u = v. Although (3) is more
complicated than this example as it also has nonnegative constraint  the following result establishes
similar guarantee for (3).4
Theorem 1. Suppose (U (cid:63)  V (cid:63)) be any critical point of (3) satisfying (cid:107)U (cid:63)V (cid:63)T(cid:107) < 2λ + σn(X) 
where σn(·) denotes the n-th largest singular value. Then U (cid:63) = V (cid:63) and U (cid:63) is a critical point of
(2).

Towards interpreting Theorem 1  we note that for any λ > 0  Theorem 1 ensures a certain region
(whose size depends on λ) in which each critical point of (3) has identical factors and also returns a
solution for the original symmetric NMF (2). This further suggests the opportunity of choosing an
appropriate λ such that the corresponding region (i.e.  all (U   V ) such that (cid:107)U V T(cid:107) < 2λ + σn(X))
contains all the possible points that the algorithms will converge to. Towards that end  next result
indicates that for any local search algorithms  if it decreases the objective function  then the iterates
are bounded.
Lemma 1. For any local search algorithm solving (3) with initialization V 0 = U 0  U 0 ≥ 0  suppose
it sequentially decreases the objective value. Then  for any k ≥ 0  the iterate (U k  V k) generated by
this algorithm satisﬁes

(cid:18) 1

(cid:19)
0 (cid:107)2
(cid:107)X − U 0U T
0 (cid:107)F + (cid:107)X(cid:107)F .
k (cid:107)F ≤ (cid:107)X − U 0V T

F ≤

+ 2

√

λ

r

(cid:107)U k(cid:107)2

F + (cid:107)V k(cid:107)2
(cid:107)U kV T

√

F + 2

r(cid:107)X(cid:107)F := B0 

(4)

There are two interesting facts regarding the iterates can be interpreted from (4). The ﬁrst equation
of (4) implies that both U k and V k are bounded and the upper bound decays when the λ increases.
Speciﬁcally  as long as λ is not too close to zero  then the RHS in (4) gives a meaningful bound
which will be used for the convergence analysis of local search algorithms in next section. In terms
of U kV T
k   the second equation of (4) indicates that it is indeed upper bounded by a quantity that
is independent of λ. This suggests a key result that if the iterative algorithm is convergent and the
iterates (U k  V k) converge to a critical point (U (cid:63)  V (cid:63))  then U (cid:63)V (cid:63)T is also bounded  irrespectively
the value of λ. This together with Theorem 1 ensures that many local search algorithms can be
utilized to ﬁnd a critical point of (2) by choosing a sufﬁciently large λ.
for (3)  where (cid:107)X(cid:107)2 means
Theorem 2. Choose λ > 1
2
largest singular value of X. For any local search algorithm solving (3) with initialization V 0 = U 0 
if it sequentially decreases the objective value  is convergent  and converges to a critical point
(U (cid:63)  V (cid:63)) of (3)  then we have U (cid:63) = V (cid:63) and that U (cid:63) is also a critical point of (2).

(cid:13)(cid:13)(cid:13)X − U 0U T

(cid:16)(cid:107)X(cid:107)2 +

− σn(X)

(cid:13)(cid:13)(cid:13)F

(cid:17)

0

Theorem 2 indicates that instead of directly solving the symmetric NMF (2)  one can turn to solve (3)
with a sufﬁciently large regularization parameter λ. The latter is very similar to the nonsymmetric
NMF (1) and obeys similar splitting property  which enables us to utilize efﬁcient alternating-type
algorithms. In the next section  we propose alternating based algorithms for tackling (3) with strong
guarantees on the descend property and convergence issue.

3 Fast Algorithms for Symmetric NMF with Guaranteed Convergence

In the last section  we have shown that the symmetric NMF (2) can be transfered to problem (3)  the
latter admitting splitting property which enables us to design alternating-type algorithms to solve
symmetric NMF. Speciﬁcally  we exploit the splitting property by adopting the main idea in ANLS
and HALS for nonsymmetric NMF to design fast algorithms for (3). Moreover  note that the objective
function f in (3) is strongly convex with respect to U (or V ) with ﬁxed V (or U) because of the
regularized term λ
F . This together with Lemma 1 ensures that strong descend property
and point-wise sequence convergence guarantee of the proposed alternating-type algorithms. With
Theorem 2  we are ﬁnally guaranteed that the algorithms converge to a critical point of symmetric
NMF (2).

2(cid:107)U − V (cid:107)2

3.1 ANLS for symmetric NMF (SymANLS)

4All the proofs can be found at supplementary material.

4

Algorithm 1 SymANLS
Initialization: k = 1 and U 0 = V 0.
1: while stop criterion not meet do
2: U k = arg minV ≥0
V k = arg minU≥0
3:
k = k + 1.
4:
5: end while
Output: factorization (U k  V k).

1

1

2(cid:107)X − U V T
k−1(cid:107)2
2(cid:107)X − U kV T(cid:107)2

F + λ
F + λ

2(cid:107)U − V k−1(cid:107)2
F ;
2(cid:107)U k − V (cid:107)2
F ;

ANLS is an alternating-type algorithm customized for nonsymmetric NMF (1) and its main idea is
that at each time  keep one factor ﬁxed  and update another one via solving a nonnegative constrained
least squares. We use similar idea for solving (3) and refer the corresponding algorithm as SymANLS.
Speciﬁcally  at the k-th iteration  SymANLS ﬁrst updates U k by

U k = arg min

U∈Rn×r U≥0

(cid:107)X − U V T

k−1(cid:107)2

F +

(cid:107)U − V k−1(cid:107)2
F .

λ
2

(5)

V k is then updated in a similar way. We depict the whole procedure of SymANLS in Algorithm 1.
With respect to solving the subproblem (5)  we ﬁrst note that there exists a unique minimizer (i.e. 
U k) for (5) as it involves a strongly objective function as well as a convex feasible region. However 
we note that because of the nonnegative constraint  unlike least squares  in general there is no
closed-from solution for (5) unless r = 1. Fortunately  there exist many feasible methods to solve
the nonnegative constrained least squares  such as projected gradient descend  active set method and
projected Newton’s method. Among these methods  a block principal pivoting method is remarkably
efﬁcient for tackling the subproblem (5) (and also the one for updating V ) [8].
With the speciﬁc structure within (3) (i.e.  its objective function is strongly convex and its feasible
region is convex)  we ﬁrst show that SymANLS monotonically decreases the function value at each
iteration  as required in Theorem 2.
Lemma 2. Let {(U k  V k)} be the iterates sequence generated by Algorithm 1. Then we have

f (U k  V k) − f (U k+1  V k+1) ≥ λ
2

((cid:107)U k+1 − U k(cid:107)2

F + (cid:107)V k+1 − V k(cid:107)2
F ).

We now give the following main convergence guarantee for Algorithm 1.
Theorem 3 (Sequence convergence of Algorithm 1). Let {(U k  V k)} be the sequence generated by
Algorithm 1. Then

k→∞(U k  V k) = (U (cid:63)  V (cid:63)) 
lim

where (U (cid:63)  V (cid:63)) is a critical point of (3). Furthermore the convergence rate is at least sublinear.

Equipped with all the machinery developed above  the global sublinear sequence convergence of
SymANLS to a critical solution of symmetric NMF (2) is formally guaranteed in the following result 
which is a direct consequence of Theorem 2  Lemma 2 and Theorem 3.
Corollary 1 (Convergence of Algorithm 1 to a critical point of (2)). Suppose Algorithm 1 is initialized
with V 0 = U 0. Choose

(cid:16)(cid:107)X(cid:107)2 +

(cid:13)(cid:13)(cid:13)X − U 0U T

0

(cid:13)(cid:13)(cid:13)F

λ >

1
2

(cid:17)

− σn(X)

.

Let {(U k  V k)} be the sequence generated by Algorithm 1. Then {(U k  V k)} is convergent and
converges to (U (cid:63)  V (cid:63)) with U (cid:63) = V (cid:63) and U (cid:63) a critical point of (2). Furthermore  the convergence
rate is at least sublinear.

Remark. We emphasis that the speciﬁc structure within (3) enables Corollary 1 get rid of the
assumption on the boundedness of iterates (U k  V k) and also the requirement of a proximal term 
which is usually required for convergence analysis but not necessarily used in practice. As a contrast
and also as pointed out in [20]  to provide the convergence guarantee for standard ANLS solving
nonsymmetric NMF (1)  one needs to modify it by adding an additional proximal term as well as an
additional constraint to make the factors bounded.

5

3.2 HALS for symmetric NMF (SymHALS)

As we stated before  due to the nonnegative constraint  there is no closed-from solution for (5) 
although one may utilize some efﬁcient algorithms for solving (5). As a contrast  there does exist a
closed-form solution when r = 1. HALS exploits this observation by splitting the pair of variables
(U   V ) into columns (u1 ···   ur  v1 ···   vr) and then optimizing over column by column. We
utilize similar idea for solving (3). Speciﬁcally  rewrite U V T = uivT
j and denote by

i +(cid:80)

j(cid:54)=i ujvT

X i = X −(cid:88)

ujvT
j

j(cid:54)=i
the factorization residual X − U V T excluding uivT
in (3) only with respect to ui  then it is equivalent to

u(cid:92)
i = arg min
ui∈Rn

1
2

(cid:107)X i − uivT
i (cid:107)2

F +

(cid:107)ui − vi(cid:107)2

2 = max

λ
2

i . Now if we minimize the objective function f

(cid:18) (X i + λI)vi

(cid:107)vi(cid:107)2

2 + λ

(cid:19)

  0

.

the presentation easily understood  we directly use X −(cid:80)i−1

Similar closed-form solution also holds when optimizing in terms of vi. With this observation  we uti-
lize alternating-type minimization that at each time minimizes the objective function in (3) only with
respect to one column in U or V and denote the corresponding algorithm as SymHALS. We depict
SymHALS in Algorithm 2  where we use subscript k to denote the k-th iteration. Note that to make
j (vk
)T
1 = X − U k−1(V k−1)T  we can
to update X k
then update X k
i )T by recursively utilizing the previous one. The
detailed information about efﬁcient implementation of SymHALS can be found in the supplementary
material (see the corresponding Algorithm 3 in Section 5).

j=1 uk
i   which is not adopted in practice. Instead  letting X k

i with only the computation of uk

j )T −(cid:80)r

j=i+1 uk−1

(vk−1

i (vk

j

j

Algorithm 2 SymHALS
Initialization: U 0  V 0  iteration k = 1.
1: while stop criterion not meet do
2:
3:

i = X −(cid:80)i−1

for i = 1 : r do

j )T −(cid:80)r

4:

end for
k = k + 1.

5:
6:
7:
8: end while
Output: factorization (U k  V k).

j=1 uk
1

X k
i = arg minui≥0
uk
i = arg minvi≥0
vk

j (vk
2(cid:107)X k
2(cid:107)X k

1

j

)T;

j=i+1 uk−1
j
F + λ
2(cid:107)uk

)T(cid:107)2
F + λ

(vk−1
2(cid:107)ui − vk−1
i − vi(cid:107)2

i

i − ui(vk−1
i
i − uk
i (cid:107)2
i vT

2 = max

(cid:107)2
2 = max

(cid:16) (X k

i

i +λI)vk−1
(cid:107)vk−1
(cid:107)2
2+λ
2+λ   0
;

i +λI)uk
i (cid:107)2

i

i

(cid:107)uk

(cid:17)

(cid:16) (X k

(cid:17)

  0

;

The SymHALS enjoys similar descend property and convergence guarantee to algorithm SymANLS
as both of them are alternating-based algorithms. Thus  we just give out the following results ensuring
Algorithm 2 converge to a critical point of symmetric NMF (2).
Corollary 2 (Convergence of Algorithm 2 to a critical point of (2)). Suppose it is initialized with
V 0 = U 0. Choose

(cid:16)(cid:107)X(cid:107)2 +

(cid:13)(cid:13)(cid:13)X − U 0U T

0

(cid:13)(cid:13)(cid:13)F

λ >

1
2

(cid:17)

− σn(X)

.

Let {(U k  V k)} be the sequence generated by Algorithm 2. Then {(U k  V k)} is convergent and
converges to (U (cid:63)  V (cid:63)) with U (cid:63) = V (cid:63) and U (cid:63) being a critical point of (2). Furthermore  the
convergence rate is at least sublinear.

Remark. Similar to Corollary 1 for SymANLS  Corollary 2 has no assumption on the boundedness
of iterates (U k  V k) and it establishes convergence guarantee for SymHALS without the aid from a
proximal term. As a contrast  to establish the subsequence convergence for classical HALS solving
nonsymmetric NMF [9  22] (i.e.  setting λ = 0 in SymHALS)  one needs the assumption that every
column of (U k  V k) is not zero through all iterations. Though such assumption can be satisﬁed
by using additional constraints  it actually solves a slightly different problem than the original

6

nonsymmetric NMF (1). On the other hand  SymHALS overcomes this issue and admits sequence
convergence because of the additional regularizer in (3).
We end this section by noting that both Theorem 2 and the convergence guarantee in Corollary 1 and
Corollary 2 can be extended to the case with any additional convex constraint and/or regularizer on
U. For example  for promoting sparsity  one can use (cid:96)1 constraint or regularizer which can also be
efﬁciently incorporated into SymHALS or SymGCD. A formal guarantee for this extension is the
subject of ongoing work.

4 Experiments on Synthetic Data and Real Image Data

In this section  we conduct experiments on both synthetic data and real data to illustrate the perfor-
mance of our proposed algorithms and compare it to other state-of-the-art ones  in terms of both
convergence property and image clustering performance.
For comparison convenience  we deﬁne

Ek =

(cid:107)X − U k(U k)T(cid:107)2

F

(cid:107)X(cid:107)2

F

as the normalized ﬁtting error at k-th iteration.
Besides SymANLS and SymHALS  we also apply the greedy coordinate descent (GCD) algorithm in
[23] (which is designed for tackling nonsymmetric NMF) to solve the reformulated problem (3) and
denote the corrosponding algorithm as SymGCD. SymGCD is expected to have similar sequence
convergence guarantee as SymANLS and SymHALS. We list the algorithms to compare: 1) ADMM
in [16]  where there is a regularization item in their augmented Lagrangian and we tune a good one for
comparison; 2) SymNewton [10] which is a Newton-like algorithm by with a the Hessian matrix in
Newton’s method for computation efﬁciency; and 3) PGD in [7]. The algorithm in [15] is inefﬁcient
for large scale data  since they apply an alternating minimization over each coordinate which entails
many loops for large scale U.

4.1 Convergence veriﬁcation
We randomly generate a matrix U ∈ R50×5(n = 50  r = 5) with each entry independently following
a standard Gaussian distribution. To enforce nonnegativity  we then take absolute value on each
entry of U to get U (cid:63). Data matrix X is constructed as U (cid:63)(U (cid:63))T which is nonnegative and PSD.
We initialize all the algorithms with same U 0 and V 0  whose entries are i.i.d. uniformly distributed
between 0 to 1.
To study the effect of the parameter λ in (3)  we show the
value (cid:107)U k−V k(cid:107)2
F versus iteration for different choices of
λ by SymHALS in Figure 2. While for this experimental
setting the lower bound of λ provided in Theorem 2 is
39.9  we observe that (cid:107)U k − V k(cid:107)2
F still converges to 0
with much smaller λ. This suggests that the sufﬁcient
condition on the choice of λ in Theorem 2 is stronger
than necessary  leaving room for future improvements.
Particularly  we suspect that SymHALS converges to a
critical point (U (cid:63)  V (cid:63)) with U (cid:63) = V (cid:63) (i.e. a critical
point of symmetric NMF) for any λ > 0; we leave this
line of theoretical justiﬁcation as our future work. On
the other hand  we note that although SymHALS ﬁnds a
critical point of symmetric NMF for most of the λ  the
convergence speed varies for different λ. For example 
we observe that either a very large or small λ yields a
slow convergence speed. In the sequel  we tune the best
parameter λ for each experiment.

Figure 2: SymHALS with different λ. Here
n = 50  r = 5.

7

02004006008001000Iteration k10-1010-5100 = 0.01 = 0.1 = 1 = 10 = 39.9 = 100We also test on real world dataset CBCL 5  where there are 2429 face image data with dimension
19 × 19. We construct the similarity matrix X following [10  section 7.1  step 1 to step 3]. The
convergence results on synthetic data and real world data are shown in Figure 3 (a1)-(a2) and Figure 3
(b1)-(b2)  respectively. We observe that the SymANLS  SymHALS  and SymGCD 1) converge faster;
2) empirically have a linear convergence rate in terms of Ek.

(a1)

(b1)

(a2)

(b2)

Figure 3: Synthetic data where n = 50  r = 5: (a1)-(a2) ﬁtting error versus iteration and running
time. Real image dataset CBCL where n = 2429  r = 49: (b1)-(b2) ﬁtting error versus iteration and
running time.

4.2

Image clustering

Symmetric NMF can be used for graph clustering [10  11] where each element X ij denotes the
similarity between data i and j. In this subsection  we apply different symmetric NMF algorithms for
graph clustering on image datasets and compare the clustering accuracy [24].
We put all images to be clustered in a data matrix M  where each row is a vectorized image. We
construct similarity matrix following the procedures in [10  section 7.1  step 1 to step 3]  and utilize

self-tuning method to construct the similarity matrix X. Upon deriving (cid:101)U from symmetric NMF
X ≈ (cid:101)U(cid:101)U

T  the label of the i-th image can be obtained by:

l(M i) = arg max

(cid:101)U (ij).

(6)

j
We conduct the experiments on four image datasets:
ORL: 400 facial images from 40 different persons with each one has 10 images from different angles
and emotions 6.
COIL-20: 1440 images from 20 objects 7.
TDT2: 10 212 news articles from 30 categories 8. We extract the ﬁrst 3147 data for experiments
(containing only 2 categories).
MNIST: classical handwritten digits dataset 9  where 60 000 are for training (denoted as
MNISTtrain)  and 10 000 for testing (denoted as MNISTtest). we test on the ﬁrst 3147 data from
MNISTtrain (contains 10 digits) and 3147 from MNISTtest (contains only 3 digits) .

5http://cbcl.mit.edu/software-datasets/FaceData2.html
6http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
7http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php
8https://www.ldc.upenn.edu/collaborations/past-projects
9http://yann.lecun.com/exdb/mnist/

8

0500100015002000Iteration10-1510-1010-5100SymANLSSymHALSSymGCDADMMSymNewtonPGD00.20.40.60.81Time(s)10-1510-1010-5100SymANLSSymHALSSymGCDADMMSymNewtonPGD020406080100Iteration100105SymANLSSymHALSSymGCDADMMSymNewtonPGD00.511.52Time(s)100105SymANLSSymHALSSymGCDADMMSymNewtonPGDIn Figure 4 (a1) and Figure 4(a2)  we display the clustering accuracy on dataset ORL with respect to
iterations and time (only show ﬁrst 10 seconds)  respectively. Similar results for dataset COIL-20
are plotted in Figure 4 (b1)-(b2). We observe that in terms of iteration number  SymNewton has
comparable performance to the three alternating methods for (3) (i.e.  SymANLS  SymHALS  and
SymGCD)  but the latter outperform the former in terms of running time. Such superiority becomes
more apparent when the size of the dataset increases. We show the comparison on larger truncated
datasets MNISTtrain and MNISTtest in supplementary materials. We note that the performance of
ADMM will increase as iterations goes and after almost 3500 iterations on ORL dataset it reaches a
comparable result to other algorithms. Moreover  it requires more iterations for larger dataset. This
observation makes ADMM not practical for image clustering. We run ADMM 5000 iterations on
ORL dataset  and display it in the supplementary material. These results as well as the experimental
results shown in the last subsection demonstrate (i) the power of transfering the symmetric NMF (2)
to a nonsymmetric one (3); and (ii) the efﬁcieny of alternating-type algorithms for sovling (3) by
exploiting the splitting property within the optimization variables in (3).

(a1)

(b1)

(a2)

(b2)

Figure 4: Real dataset: (a1) and (a2) Image clustering quality on ORL dataset  n = 400  r = 40; (b1)
and (b2) Image clustering quality on COIL-20 dataset  n = 1440  r = 20.

Table 1 shows the clustering accuracies of different algorithms on different datasets  where we
run enough iterations for ADMM so that it obtains its best result. We observe from Table 1 that
SymANLS  SymHALS  and SymGCD perform better than or have comparable performance to others
for most of the cases.

Table 1: Summary of image clustering accuracy of different algorithms on ﬁve image datasets

SymANLS
SymHALS
SymGCD
ADMM

SymNewton

PGD

ORL
0.8075
0.7550
0.7900
0.7650
0.7625
0.7700

COIL-20 MNISTtrain
0.7979
0.5854
0.7076
0.6903
0.7472
0.7243

0.6477
0.6657
0.6293
0.5803
0.5990
0.6475

TDT2 MNISTtest
0.9800
0.9806
0.9803
0.9800
0.9793
0.9800

0.8589
0.8608
0.9882
0.8713
0.8589
0.8710

Acknowledgment

The authors thank Dr. Songtao Lu for sharing the code used in [16] and the three anonymous
reviewers as well as the area chair for their constructive comments.

9

0100200300400500Iteration00.20.40.60.8Clustering accuracySymANLSSymHALSSymGCDADMMSymNewtonPGD0246810Time(s)00.20.40.60.8Clustering accuracySymANLSSymHALSSymGCDADMMSymNewtonPGD0100200300400500Iteration00.20.40.60.8Clustering accuracySymANLSSymHALSSymGCDADMMSymNewtonPGD0246810Time(s)00.20.40.60.8Clustering accuracySymANLSSymHALSSymGCDADMMSymNewtonPGDReferences
[1] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization.

Nature  401(6755):788  1999.

[2] David Guillamet and Jordi Vitria. Non-negative matrix factorization for face recognition. In Topics in

artiﬁcial intelligence  pages 336–344. Springer  2002.

[3] Farial Shahnaz  Michael W Berry  V Paul Pauca  and Robert J Plemmons. Document clustering using

nonnegative matrix factorization. Information Processing & Management  42(2):373–386  2006.

[4] Wing-Kin Ma  José M Bioucas-Dias  Tsung-Han Chan  Nicolas Gillis  Paul Gader  Antonio J Plaza 
ArulMurugan Ambikapathi  and Chong-Yung Chi. A signal processing perspective on hyperspectral
unmixing: Insights from remote sensing. IEEE Signal Processing Magazine  31(1):67–81  2014.

[5] Nicolas Gillis. The why and how of nonnegative matrix factorization. Regularization  Optimization 

Kernels  and Support Vector Machines  12(257)  2014.

[6] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in

neural information processing systems  pages 556–562  2001.

[7] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural computation 

19(10):2756–2779  2007.

[8] Jingu Kim and Haesun Park. Toward faster nonnegative matrix factorization: A new algorithm and

comparisons. In International Conference on Data Mining  pages 353–362  2008.

[9] Andrzej Cichocki and Anh-Huy Phan. Fast local algorithms for large scale nonnegative matrix and tensor
factorizations. IEICE transactions on fundamentals of electronics  communications and computer sciences 
92(3):708–721  2009.

[10] Da Kuang  Sangwoon Yun  and Haesun Park. Symnmf: nonnegative low-rank approximation of a similarity

matrix for graph clustering. Journal of Global Optimization  62(3):545–574  2015.

[11] Chris Ding  Xiaofeng He  and Horst D Simon. On the equivalence of nonnegative matrix factorization and

spectral clustering. In Proceedings ofInternational Conference on Data Mining  pages 606–610  2005.

[12] Zhaoshui He  Shengli Xie  Rafal Zdunek  Guoxu Zhou  and Andrzej Cichocki. Symmetric nonnegative
matrix factorization: Algorithms and applications to probabilistic clustering. IEEE Transactions on Neural
Networks  22(12):2117–2131  2011.

[13] Stephen Tu  Ross Boczar  Mahdi Soltanolkotabi  and Benjamin Recht. Low-rank solutions of linear matrix

equations via procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.

[14] Zhihui Zhu  Qiuwei Li  Gongguo Tang  and Michael B Wakin. The global optimization geometry of

low-rank matrix optimization. arXiv preprint arXiv:1703.01256  2017.

[15] Arnaud Vandaele  Nicolas Gillis  Qi Lei  Kai Zhong  and Inderjit Dhillon. Efﬁcient and non-convex coordi-
nate descent for symmetric nonnegative matrix factorization. IEEE Transactions on Signal Processing 
64(21):5571–5584  2016.

[16] Songtao Lu  Mingyi Hong  and Zhengdao Wang. A nonconvex splitting method for symmetric nonnegative
matrix factorization: Convergence analysis and optimality. IEEE Transactions on Signal Processing 
65(12):3120–3135  2017.

[17] G Di Pillo and L Grippo. Exact penalty functions in constrained optimization. SIAM Journal on control

and optimization  27(6):1333–1360  1989.

[18] Gianni Di Pillo. Exact penalty methods. In Algorithms for Continuous Optimization  pages 209–253.

Springer  1994.

[19] Hédy Attouch  Jérôme Bolte  Patrick Redont  and Antoine Soubeyran. Proximal alternating minimization
and projection methods for nonconvex problems: An approach based on the kurdyka-lojasiewicz inequality.
Mathematics of Operations Research  35(2):438–457  2010.

[20] Kejun Huang  Nicholas D Sidiropoulos  and Athanasios P Liavas. A ﬂexible and efﬁcient algorithmic
framework for constrained matrix and tensor factorization. IEEE Transactions on Signal Processing 
64(19):5052–5065  2016.

10

[21] Meisam Razaviyayn  Mingyi Hong  and Zhi-Quan Luo. A uniﬁed convergence analysis of block successive

minimization methods for nonsmooth optimization. SIAM J. Optimization  23(2):1126–1153  2013.

[22] Nicolas Gillis and François Glineur. Accelerated multiplicative updates and hierarchical als algorithms for

nonnegative matrix factorization. Neural computation  24(4):1085–1105  2012.

[23] Cho-Jui Hsieh and Inderjit S Dhillon. Fast coordinate descent methods with variable selection for non-
negative matrix factorization. In Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining  pages 1064–1072. ACM  2011.

[24] Wei Xu  Xin Liu  and Yihong Gong. Document clustering based on non-negative matrix factorization.
In International ACM SIGIR conference on Research and development in informaion retrieval  pages
267–273  2003.

11

,Scott Wisdom
Thomas Powers
John Hershey
Jonathan Le Roux
Les Atlas
Feng Nan
Venkatesh Saligrama
Zhihui Zhu
Xiao Li
Kai Liu
Qiuwei Li