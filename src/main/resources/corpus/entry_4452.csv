2007,Progressive mixture rules are deviation suboptimal,We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g  under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded)  it is known that the progressive mixture rule g_n satisfies E R(g_n) < min_{g in G} R(g) + Cst (log|G|)/n where n denotes the size of the training set  E denotes the expectation wrt the training set distribution. This work shows that  surprisingly  for appropriate reference sets G  the deviation convergence rate of the progressive mixture rule is only no better than Cst / sqrt{n}  and not the expected Cst / n. It also provides an algorithm which does not suffer from this drawback.,Progressive mixture rules are deviation suboptimal

Jean-Yves Audibert

Willow Project - Certis Lab
ParisTech  Ecole des Ponts

77455 Marne-la-Vall´ee  France

audibert@certis.enpc.fr

Abstract

We consider the learning task consisting in predicting as well as the best function
in a ﬁnite reference set G up to the smallest possible additive term. If R(g) denotes
the generalization error of a prediction function g  under reasonable assumptions
on the loss function (typically satisﬁed by the least square loss when the output is
bounded)  it is known that the progressive mixture rule ˆg satisﬁes

ER(ˆg) ≤ ming∈G R(g) + Cst log |G|n  

(1)
where n denotes the size of the training set  and E denotes the expectation w.r.t.
the training set distribution.This work shows that  surprisingly  for appropriate
reference sets G  the deviation convergence rate of the progressive mixture rule is
no better than Cst /√n: it fails to achieve the expected Cst /n. We also provide
an algorithm which does not suffer from this drawback  and which is optimal in
both deviation and expectation convergence rates.

1 Introduction

Why are we concerned by deviations? The efﬁciency of an algorithm can be summarized by its
expected risk  but this does not precise the ﬂuctuations of its risk. In several application ﬁelds of
learning algorithms  these ﬂuctuations play a key role: in ﬁnance for instance  the bigger the losses
can be  the more money the bank needs to freeze in order to alleviate these possible losses. In this
case  a “good” algorithm is an algorithm having not only low expected risk but also small deviations.

Why are we interested in the learning task of doing as well as the best prediction function of a given
ﬁnite set? First  one way of doing model selection among a ﬁnite family of submodels is to cut the
training set into two parts  use the ﬁrst part to learn the best prediction function of each submodel
and use the second part to learn a prediction function which performs as well as the best of the
prediction functions learned on the ﬁrst part of the training set. This scheme is very powerful since
it leads to theoretical results  which  in most situations  would be very hard to prove without it. Our
work here is related to the second step of this scheme.

Secondly  assume we want to predict the value of a continuous variable  and that we have many
candidates for explaining it. An input point can then be seen as the vector containing the prediction
of each candidate. The problem is what to do when the dimensionality d of the input data (equiva-
lently the number of prediction functions) is much higher than the number of training points n. In
this setting  one cannot use linear regression and its variants in order to predict as well as the best
candidate up to a small additive term. Besides  (penalized) empirical risk minimization is doomed
to be suboptimal (see the second part of Theorem 2 and also [1]).

As far as the expected risk is concerned  the only known correct way of predicting as well as the
best prediction function is to use the progressive mixture rule or its variants. These algorithms are
introduced in Section 2 and their main good property is given in Theorem 1. In this work we prove
that they do not work well as far as risk deviations are concerned (see the second part of Theorem

1

3). We also provide a new algorithm for this ’predict as well as the best’ problem (see the end of
Section 4).

2 The progressive mixture rule and its variants

We assume that we observe n pairs of input-output denoted Z1 = (X1  Y1)  . . .   Zn = (Xn  Yn)
and that each pair has been independently drawn from the same unknown distribution denoted P .
The input and output spaces are denoted respectively X and Y  so that P is a probability distribution
on the product space Z   X × Y. The quality of a (prediction) function g : X → Y is measured by
the risk (or generalization error):

R(g) = E(X Y )∼P `[Y  g(X)] 

where `[Y  g(X)] denotes the loss (possibly inﬁnite) incurred by predicting g(X) when the true
output is Y . We work under the following assumptions for the data space and the loss function
` : Y × Y → R ∪ {+∞}.
Main assumptions. The input space is assumed to be inﬁnite: |X| = +∞. The output space is
a non-trivial (i.e. inﬁnite) interval of R symmetrical w.r.t. some a ∈ R: for any y ∈ Y  we have
2a − y ∈ Y. The loss function is

• uniformly exp-concave: there exists λ > 0 such that for any y ∈ Y  the set (cid:8)y0 ∈ R :
7→ e−λ`(y y0) is

`(y  y0) < +∞(cid:9) is an interval containing a on which the function y0
concave.

• symmetrical: for any y1  y2 ∈ Y  `(y1  y2) = `(2a − y1  2a − y2) 
• admissible: for any y  y0 ∈ Y∩]a; +∞[  `(y  2a − y0) > `(y  y0) 
• well behaved at center: for any y ∈ Y∩]a; +∞[  the function `y : y0 7→ `(y  y0) is twice
continuously differentiable on a neighborhood of a and `0y(a) < 0.

These assumptions imply that

for some ζ > 0.

• Y has necessarily one of the following form: ] − ∞; +∞[  [a − ζ; a + ζ] or ]a − ζ; a + ζ[
7→ `(y  y0) is
• for any y ∈ Y  from the exp-concavity assumption  the function `y : y0
convex on the interval on which it is ﬁnite1. As a consequence  the risk R is also a convex
function (on the convex set of prediction functions for which it is ﬁnite).

The assumptions were motivated by the fact that they are satisﬁed in the following settings:

we have a = (ymin + ymax)/2 and may take λ = 1/[2(ymax − ymin)2].

• least square loss with bounded outputs: Y = [ymin; ymax] and `(y1  y2) = (y1−y2)2. Then
• entropy loss: Y = [0; 1] and `(y1  y2) = y1 log(cid:0) y1
1−y2(cid:1). Note that
• exponential (or AdaBoost) loss: Y = [−ymax; ymax] and `(y1  y2) = e−y1y2. Then we
• logit loss: Y = [−ymax; ymax] and `(y1  y2) = log(1 + e−y1y2). Then we have a = 0 and

`(0  1) = `(1  0) = +∞. Then we have a = 1/2 and may take λ = 1.
have a = 0 and may take λ = e−y2

y2(cid:1) + (1 − y1) log (cid:0) 1−y1

max.

may take λ = e−y2

max.

Progressive indirect mixture rule. Let G be a ﬁnite reference set of prediction functions. Under the
previous assumptions  the only known algorithms satisfying (1) are the progressive indirect mixture
rules deﬁned below.
For any i ∈ {0  . . .   n}  the cumulative loss suffered by the prediction function g on the ﬁrst i pairs
of input-output is

1Indeed 

if ξ denotes the function e−λ`y   from Jensen’s inequality  for any probability distribution 

Σi(g)   Pi

j=1 `[Yj  g(Xj)] 

E`y(Y ) = E(cid:0) − 1

λ log ξ(Y )(cid:1) ≥ − 1

λ log Eξ(Y ) ≥ − 1

λ log ξ(EY ) = `y(EY ).

2

where by convention we take Σ0 ≡ 0. Let π denote the uniform distribution on G. We deﬁne the
probability distribution ˆπi on G as

ˆπi ∝ e−λΣi · π
e−λΣi(g0)). This distribution concentrates
equivalently for any g ∈ G  ˆπi(g) = e−λΣi(g)/(Pg0∈G
on functions having low cumulative loss up to time i. For any i ∈ {0  . . .   n}  let ˆhi be a prediction
function such that

The progressive indirect mixture rule produces the prediction function

∀ (x  y) ∈ Z

`[y  ˆhi(x)] ≤ − 1

λ log Eg∼ˆπi e−λ`[y g(x)].

(2)

ˆgpim = 1

n+1 Pn

i=0

ˆhi.

From the uniform exp-concavity assumption and Jensen’s inequality  ˆhi does exist since one may
take ˆhi = Eg∼ˆπi g. This particular choice leads to the progressive mixture rule  for which the
predicted output for any x ∈ X is

ˆgpm(x) = Pg∈G (cid:16) 1

n+1 Pn

i=0

e−λΣi (g)

P g0 ∈G e−λΣi (g0)(cid:17) g(x).

Consequently  any result that holds for any progressive indirect mixture rule in particular holds for
the progressive mixture rule.

The idea of a progressive mean of estimators has been introduced by Barron ([2]) in the context
of density estimation with Kullback-Leibler loss. The form ˆgpm is due to Catoni ([3]). It was also
independently proposed in [4]. The study of this procedure was made in density estimation and least
square regression in [5  6  7  8]. Results for general losses can be found in [9  10]. Finally  the
progressive indirect mixture rule is inspired by the work of Vovk  Haussler  Kivinen and Warmuth
[11  12  13] on sequential prediction and was studied in the “batch” setting in [10]. Finally  in the
upper bounds we state  e.g. Inequality (1)  one should notice that there is no constant larger than 1
in front of ming∈G R(g)  as opposed to some existing upper bounds (e.g. [14]). This work really
studies the behaviour of the excess risk  that is the random variable R(ˆg) − ming∈G R(g).
The largest integer smaller or equal to the logarithm in base 2 of x is denoted by blog2 xc .
3 Expectation convergence rate

The following theorem  whose proof is omitted  shows that the expectation convergence rate of any
progressive indirect mixture rule is (i) at least (log |G|)/n and (ii) cannot be uniformly improved 
even when we consider only probability distributions on Z for which the output has almost surely
two symmetrical values (e.g. {-1;+1} classication with exponential or logit losses).
Theorem 1 Any progressive indirect mixture rule satisﬁes

ER(ˆgpim) ≤ min
g∈G

R(g) + log |G|
λ(n+1) .

Let y1 ∈ Y −{a} and d be a positive integer. There exists a set G of d prediction functions such that:
for any learning algorithm  there exists a probability distribution generating the data for which

• the output marginal is supported by 2a − y1 and y1: P (Y ∈ {2a − y1; y1}) = 1 
• ER(ˆg) ≥ min
g∈G

R(g) + e−1κ(cid:0)1 ∧ blog2 |G|c

[`(y1  a) − `(y1  y)] > 0.

(cid:1)  with κ   sup
y∈Y

n+1

The second part of Theorem 1 has the same (log |G|)/n rate as the lower bounds obtained in sequen-
tial prediction ([12]). From the link between sequential predictions and our “batch” setting with i.i.d.
data (see e.g. [10  Lemma 3])  upper bounds for sequential prediction lead to upper bounds for i.i.d.
data  and lower bounds for i.i.d. data leads to lower bounds for sequential prediction. The converse
of this last assertion is not true  so that the second part of Theorem 1 is not a consequence of the
lower bounds of [12].

3

The following theorem  whose proof is also omitted  shows that for appropriate set G: (i) the em-
pirical risk minimizer has a p(log |G|)/n expectation convergence rate  and (ii) any empirical risk
minimizer and any of its penalized variants are really poor algorithms in our learning task since their
expectation convergence rate cannot be faster than p(log |G|)/n (see [5  p.14] and [1] for results of
the same spirit). This last point explains the interest we have in progressive mixture rules.

Theorem 2 If B   supy y0 y00∈Y [`(y  y0) − `(y  y00)] < +∞  then any empirical risk minimizer 
which produces a prediction function ˆgerm in argming∈G Σn  satisﬁes:
R(g) + Bq 2 log |G|n
.

ER(ˆgerm) ≤ min
g∈G

Let y1  ˜y1 ∈ Y∩]a; +∞[ and d be a positive integer. There exists a set G of d prediction functions
such that: for any learning algorithm producing a prediction function in G (e.g. ˆgerm) there exists a
probability distribution generating the data for which

• the output marginal is supported by 2a − y1 and y1: P (Y ∈ {2a − y1; y1}) = 1 
• ER(ˆg) ≥ min
g∈G

∧ 2(cid:17)  with δ   `(y1  2a − ˜y1) − `(y1  ˜y1) > 0.

8(cid:16)q blog2 |G|c

R(g) + δ

n

The lower bound of Theorem 2 also says that one should not use cross-validation. This holds for the
loss functions considered in this work  and not for  e.g.  the classiﬁcation loss: `(y  y0) = 1y6=y0.

4 Deviation convergence rate

The following theorem shows that the deviation convergence rate of any progressive indirect mix-
ture rule is (i) at least 1/√n and (ii) cannot be uniformly improved  even when we consider only
probability distributions on Z for which the output has almost surely two symmetrical values (e.g.
{-1;+1} classication with exponential or logit losses).
Theorem 3 If B   supy y0 y00∈Y [`(y  y0) − `(y  y00)] < +∞  then any progressive indirect mixture
rule satisﬁes: for any  > 0  with probability at least 1 −  w.r.t. the training set distribution  we
have

R(ˆgpim) ≤ min
g∈G

R(g) + Bq 2 log(2−1)

n+1 + log |G|

λ(n+1)

Let y1 and ˜y1 in Y∩]a; +∞[ such that `y1 is twice continuously differentiable on [a; ˜y1] and
`0y1 ( ˜y1) ≤ 0 and `00y1( ˜y1) > 0. Consider the prediction functions g1 ≡ ˜y1 and g2 ≡ 2a − ˜y1.
For any training set size n large enough  there exist  > 0 and a distribution generating the data
such that

• the output marginal is supported by y1 and 2a − y1
• with probability larger than   we have
R(ˆgpim) − min

g∈{g1 g2}

R(g) ≥ cq log(e−1)

n

where c is a positive constant depending only on the loss function  the symmetry parameter
a and the output values y1 and ˜y1.

Proof 1 See Section 5.

This result is quite surprising since it gives an example of an algorithm which is optimal in terms of
expectation convergence rate and for which the deviation convergence rate is (signiﬁcantly) worse
than the expectation convergence rate.

In fact  despite their popularity based on their unique expectation convergence rate  the progressive
mixture rules are not good algorithms since a long argument essentially based on convexity shows
that the following algorithm has both expectation and deviation convergence rate of order 1/n. Let

4

ˆgerm be the minimizer of the empirical risk among functions in G. Let ˜g be the minimizer of the
empirical risk in the star ˆG = ∪g∈G [g; ˆgerm]. The algorithm producing ˜g satisﬁes for some C > 0 
for any  > 0  with probability at least 1 −  w.r.t. the training set distribution  we have

R(˜g) ≤ min
g∈G

R(g) + C log(−1|G|)

.

n

This algorithm has also the beneﬁt of being parameter-free. On the contrary  in practice  one will
have recourse to cross-validation to tune the parameter λ of the progressive mixture rule.
To summarize  to predict as well as the best prediction function in a given set G  one should not
restrain the algorithm to produce its prediction function among the set G. The progressive mix-
ture rules satisfy this principle since they produce a prediction function in the convex hull of G.
This allows to achieve (log |G|)/n convergence rates in expectation. The proof of the lower bound
of Theorem 3 shows that the progressive mixtures overﬁt the data: the deviations of their excess
risk are not PAC bounded by C log(−1|G|)/n while an appropriate algorithm producing prediction
functions on the edges of the convex hull achieves the log(−1|G|)/n deviation convergence rate.
Future work might look at whether one can transpose this algorithm to the sequential prediction
setting  in which  up to now  the algorithms to predict as well as the best expert were dominated by
algorithms producing a mixture expert inside the convex hull of the set of experts.

5 Proof of Theorem 3

5.1 Proof of the upper bound

Let Zn+1 = (Xn+1  Yn+1) be an input-output pair independent from the training set Z1  . . .   Zn
and with the same distribution P . From the convexity of y0 7→ `(y  y0)  we have

(3)
Now from [15  Theorem 1] (see also [16  Proposition 1])  for any  > 0  with probability at least
1 −   we have

R(ˆgpim) ≤ 1

n+1 Pn

i=0 R(ˆhi).

1

n+1 Pn

i=0 R(ˆhi) ≤ 1

n+1 Pn

i=0 `(cid:0)Yi+1  ˆh(Xi+1)(cid:1) + Bq log(−1)

2(n+1)

Using [12  Theorem 3.8] and the exp-concavity assumption  we have

Let ˜g ∈ argmin

i=0 `(cid:0)Yi+1  ˆh(Xi+1)(cid:1) ≤ min

Pn
G R. By Hoeffding’s inequality  with probability at least 1 −   we have

i=0 `(cid:0)Yi+1  g(Xi+1)(cid:1) + log |G|λ

g∈G Pn

1

n+1 Pn

i=0 `(cid:0)Yi+1  ˜g(Xi+1)(cid:1) ≤ R(˜g) + Bq log(−1)

2(n+1)

Merging (3)  (4)  (5) and (6)  with probability at least 1 − 2  we get
i=0 `(cid:0)Yi+1  ˜g(Xi+1)(cid:1) + log |G|

R(ˆgpim) ≤

n+1 Pn

1

λ(n+1) + Bq log(−1)

2(n+1)

≤ R(˜g) + Bq 2 log(−1)

n+1 + log |G|
λ(n+1) .

(4)

(5)

(6)

5.2 Sketch of the proof of the lower bound

We cannot use standard tools like Assouad’s argument (see e.g. [17  Theorem 14.6]) because if it
were possible  it would mean that the lower bound would hold for any algorithm and in particular
for ˜g  and this is false. To prove that any progressive indirect mixture rule have no fast exponential
deviation inequalities  we will show that on some event with not too small probability  for most of
the i in {0  . . .   n}  π−λΣi concentrates on the wrong function.
The proof is organized as follows. First we deﬁne the probability distribution for which we will
prove that the progressive indirect mixture rules cannot have fast deviation convergence rates. Then
we deﬁne the event on which the progressive indirect mixture rules do not perform well. We lower
bound the probability of this excursion event. Finally we conclude by lower bounding R(ˆgpim) on
the excursion event.

Before starting the proof  note that from the “well behaved at center” and exp-concavity assump-
tions  for any y ∈ Y∩]a; +∞[  on a neighborhood of a  we have: `00y ≥ λ(`0y)2 and since `0y(a) < 0 
y1 and ˜y1 exist. Due to limited space  some technical computations have been removed.

5

5.2.1 Probability distribution generating the data and ﬁrst consequences.
Let γ ∈]0; 1] be a parameter to be tuned later. We consider a distribution generating the data such
that the output distribution satisﬁes for any x ∈ X

P (Y = y1|X = x) = (1 + γ)/2 = 1 − P (Y = y2|X = x) 

where y2 = 2a− y1. Let ˜y2 = 2a− ˜y1. From the symmetry and admissibility assumptions  we have
`(y2  ˜y2) = `(y1  ˜y1) < `(y1  ˜y2) = `(y2  ˜y1). Introduce

We have

δ   `(y1  ˜y2) − `(y1  ˜y1) > 0.

(7)

R(g2) − R(g1) = 1+γ

2 [`(y1  ˜y2) − `(y1  ˜y1)] + 1−γ

(8)
Therefore g1 is the best prediction function in {g1  g2} for the distribution we have chosen. Introduce
Wj   1Yj =y1 − 1Yj =y2 and Si   Pi
Σi(g2) − Σi(g1) = Pi

j=1 Wj. For any i ∈ {1  . . .   n}  we have
j=1[`(Yj  ˜y2) − `(Yj  ˜y1)] = Pi
The weight given by the Gibbs distribution π−λΣi to the function g1 is

2 [`(y2  ˜y2) − `(y2  ˜y1)] = γδ.

j=1 Wjδ = δ Si

π−λΣi(g1) =

e−λΣi(g1)

e−λΣi (g1)+e−λΣi(g2) =

1+eλ[Σi (g1)−Σi (g2)] =

1

1

1+e−λδSi .

(9)

5.2.2 An excursion event on which the progressive indirect mixture rules will not perform

well.

Equality (9) leads us to consider the event:

Eτ = (cid:8)∀i ∈ {τ  . . .   n}  Si ≤ −τ(cid:9) 

with τ the smallest integer larger than (log n)/(λδ) such that n − τ is even (for convenience). We
have
(10)

log n

λδ ≤ τ ≤ log n

λδ + 2.

The event Eτ can be seen as an excursion event of the random walk deﬁned through the random
variables Wj = 1Yj =y1 − 1Yj =y2  j ∈ {1  . . .   n}  which are equal to +1 with probability (1 + γ)/2
and −1 with probability (1 − γ)/2.
From (9)  on the event Eτ   for any i ∈ {τ  . . .   n}  we have
π−λΣi(g1) ≤ 1
n+1 .

(11)
This means that π−λΣi concentrates on the wrong function  i.e. the function g2 having larger risk
(see (8)).

5.2.3 Lower bound of the probability of the excursion event.

This requires to look at the probability that a slightly shifted random walk in the integer space has a
very long excursion above a certain threshold. To lower bound this probability  we will ﬁrst look at
the non-shifted random walk. Then we will see that for small enough shift parameter  probabilities
of shifted random walk events are close to the ones associated to the non-shifted random walk.
Let N be a positive integer. Let σ1  . . .   σN be N independent Rademacher variables: P(σi =
+1) = P(σi = −1) = 1/2. Let si   Pi
j=1 σi be the sum of the ﬁrst i Rademacher variables. We
start with the following lemma for sums of Rademacher variables (proof omitted).

Lemma 1 Let m and t be positive integers. We have

P(cid:0) max
1≤k≤N

sk ≥ t; sN 6= t;(cid:12)(cid:12)sN − t(cid:12)(cid:12) ≤ m(cid:1) = 2P(cid:0)t < sN ≤ t + m(cid:1)

(12)

Let σ01  . . .   σ0N be N independent shifted Rademacher variables to the extent that P(σ0i = +1) =
(1 + γ)/2 = 1 − P(σ0i = −1). These random variables satisfy the following key lemma (proof
omitted)

6

integer  we have

Lemma 2 For any set A ⊂ (cid:8)(1  . . .   N ) ∈ {−1  1}n : (cid:12)(cid:12)PN
(cid:0)1 − γ2(cid:1)N/2

P(cid:8)(σ01  . . .   σ0N ) ∈ A(cid:9) ≥ (cid:16) 1−γ

1+γ(cid:17)M/2

i=1 i(cid:12)(cid:12) ≤ M(cid:9) where M is a positive
P(cid:8)(σ1  . . .   σN ) ∈ A(cid:9)

(13)

We may now lower bound the probability of the excursion event Eτ . Let M be an integer larger than
τ . We still use Wj   1Yj =y1 − 1Yj =y2 for j ∈ {1  . . .   n}. By using Lemma 2 with N = n − 2τ  
we obtain

P(Eτ ) ≥ P(cid:0)W1 = −1  . . .   W2τ = −1; ∀ 2τ < i ≤ n  Pi

j=2τ +1 Wj ≤ τ(cid:1)

= (cid:0) 1−γ
≥ (cid:0) 1−γ

2 (cid:1)2τ
2 (cid:1)2τ(cid:0) 1−γ

P(cid:0)∀ i ∈ {1  . . .   N} Pi
1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

N

j=1 σ0j ≤ τ(cid:1)

2 P(cid:0)|sN| ≤ M ;∀ i ∈ {1  . . .   N}

si ≤ τ(cid:1)

By using Lemma 1  since τ ≤ M  the r.h.s. probability can be lower bounded  and after some
computations  we obtain

P(Eτ ) ≥ τ(cid:0) 1−γ

(14)
where we recall that τ have the order of log n  N = n − 2τ has the order of n and that γ > 0 and
M ≥ τ have to be appropriately chosen.
To control the probabilities of the r.h.s.  we use Stirling’s formula

2 [P(sN = τ ) − P(sN = M )]

1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

2 (cid:1)2τ(cid:0) 1−γ

N

nne−n√2πn e1/(12n+1) < n! < nne−n√2πn e1/(12n) 

(15)

(16)

(17)

and get for any s ∈ [0; N ] such that N − s even 
πN (cid:16)1 − s2

P(sN = s) ≥ q 2

N 2(cid:17)− N

2 (cid:16) 1− s

N
1+ s

N (cid:17)

s
2

e− 1

6(N +s) − 1

6(N−s)

and similarly

P(sN = s) ≤ q 2

πN (cid:16)1 − s2

N 2(cid:17)− N

2 (cid:16) 1− s

N
1+ s

N (cid:17)

s
2

1

12N +1 .

e

These computations and (14) leads us to take M as the smallest integer larger than √n such that
n − M is even. Indeed  from (10)  (16) and (17)  we obtain limn→+∞ √n[P(sN = τ ) − P(sN =
M )] = c  where c = p2/π(cid:0)1 − e−1/2(cid:1) > 0. Therefore for n large enough we have

(18)
The last two terms of the r.h.s. of (18) leads us to take γ of order 1/√n up to possibly a logarithmic
term. We obtain the following lower bound on the excursion probability

P(Eτ ) ≥ cτ

1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

2 (cid:1)2τ(cid:0) 1−γ

2√n(cid:0) 1−γ

N
2

Lemma 3 If γ = pC0(log n)/n with C0 a positive constant  then for any large enough n 

P(Eτ ) ≥ 1
nC0 .

5.2.4 Behavior of the progressive indirect mixture rule on the excursion event.
From now on  we work on the event Eτ . We have ˆgpim = (Pn
ˆhi)/(n + 1). We still use δ  
`(y1  ˜y2)−`(y1  ˜y1) = `(y2  ˜y1)−`(y2  ˜y2). On the event Eτ   for any x ∈ X and any i ∈ {τ  . . .   n} 
by deﬁnition of ˆhi  we have

i=0

`[y2  ˆhi(x)] − `(y2  ˜y2) ≤ − 1
= − 1
≤ − 1

e−λ{`[y2 g(x)]−`(y2  ˜y2)}
λ log Eg∼π−λΣi
λ log (cid:8)e−λδ + (1 − e−λδ)π−λΣi(g2)(cid:9)
λ log (cid:8)1 − (1 − e−λδ) 1

n+1(cid:9)

In particular  for any n large enough  we have `[y2  ˆhi(x)] − `(y2  ˜y2) ≤ Cn−1  with C > 0
independent from γ. From the convexity of the function y 7→ `(y2  y) and by Jensen’s inequality 
we obtain

`[y2  ˆgpim(x)] − `(y2  ˜y2) ≤ 1

n+1 Pn

i=0 `[y2  ˆhi(x)] − `(y2  ˜y2) ≤ τ δ

n+1 + Cn−1 < C1

log n

n

7

for some constant C1 > 0 independent from γ. Let us now prove that for n large enough  we have

˜y2 ≤ ˆgpim(x) ≤ ˜y2 + Cq log n

n ≤ ˜y1 

(19)

with C > 0 independent from γ.
From (19)  we obtain
R(ˆgpim) − R(g1) = 1+γ
= 1+γ
= 1+γ
≥ γδ − (ˆgpim − ˜y2)|`0y1 ( ˜y2)|
≥ γδ − C2q log n
n  

2 (cid:2)`(y1  ˆgpim) − `(y1  ˜y1)(cid:3) + 1−γ
2 (cid:2)`y1(ˆgpim) − `y1 ( ˜y1)(cid:3) + 1−γ
2 (cid:2)δ + `y1 (ˆgpim) − `y1( ˜y2)(cid:3) + 1−γ

2 (cid:2)`(y2  ˆgpim) − `(y2  ˜y1)(cid:3)
2 (cid:2)`y1(2a − ˆgpim) − `y1( ˜y2)(cid:3)
2 (cid:2) − δ + `y1 (2a − ˆgpim) − `y1 ( ˜y1)(cid:3)

(20)
with C2 independent from γ. We may take γ = 2C2
δ p(log n)/n and obtain: for n large enough 
on the event Eτ   we have R(ˆgpim) − R(g1) ≥ Cplog n/n. From Lemma 3  this inequality holds
with probability at least 1/nC4 for some C4 > 0. To conclude  for any n large enough  there exists
 > 0 s.t. with probability at least   R(ˆgpim) − R(g1) ≥ cq log(e−1)
. where c is a positive constant
depending only on the loss function  the symmetry parameter a and the output values y1 and ˜y1.

n

References

[1] G. Lecu´e. Suboptimality of penalized empirical risk minimization in classiﬁcation. In Proceedings of the

20th annual conference on Computational Learning Theory  2007.

[2] A. Barron. Are bayes rules consistent in information? In T.M. Cover and B. Gopinath  editors  Open

Problems in Communication and Computation  pages 85–91. Springer  1987.

[3] O. Catoni. A mixture approach to universal model selection. preprint LMENS 97-30  Available from

http://www.dma.ens.fr/edition/preprints/Index.97.html  1997.

[4] A. Barron and Y. Yang. Information-theoretic determination of minimax rates of convergence. Ann. Stat. 

27(5):1564–1599  1999.

[5] O. Catoni. Universal aggregation rules with exact bias bound. Preprint n.510  http://www.proba.

jussieu.fr/mathdoc/preprints/index.html\#1999  1999.

[6] G. Blanchard. The progressive mixture estimator for regression trees. Ann. Inst. Henri Poincar´e  Probab.

Stat.  35(6):793–820  1999.

[7] Y. Yang. Combining different procedures for adaptive regression. Journal of multivariate analysis 

74:135–161  2000.

[8] F. Bunea and A. Nobel. Sequential procedures for aggregating arbitrary estimators of a conditional mean 

2005. Technical report.

[9] A. Juditsky  P. Rigollet  and A.B. Tsybakov. Learning by mirror averaging. Preprint n.1034  Laboratoire

de Probabilit´es et Mod`eles Al´eatoires  Universit´es Paris 6 and Paris 7  2005.

[10] J.-Y. Audibert. A randomized online learning algorithm for better variance control. In Proceedings of the

19th annual conference on Computational Learning Theory  pages 392–407  2006.

[11] V.G. Vovk. Aggregating strategies. In Proceedings of the 3rd annual workshop on Computational Learn-

ing Theory  pages 371–386  1990.

[12] D. Haussler  J. Kivinen  and M. K. Warmuth. Sequential prediction of individual sequences under general

loss functions. IEEE Trans. on Information Theory  44(5):1906–1925  1998.

[13] V.G. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences  pages

153–173  1998.

[14] M. Wegkamp. Model selection in nonparametric regression. Ann. Stat.  31(1):252–273  2003.
[15] T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In Proceedings of

the 18th annual conference on Computational Learning Theory  pages 173–187  2005.

[16] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[17] L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag 

1996.

8

,Grani Adiwena Hanasusanto
Daniel Kuhn
Aurelien Garivier
Tor Lattimore
Emilie Kaufmann