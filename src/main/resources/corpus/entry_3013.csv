2016,Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods,In this paper  we consider a non-convex loss-minimization problem of learning Supervised PageRank models  which can account for features of nodes and edges. We propose gradient-based and random gradient-free methods to solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the state-of-the-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them. Finally  we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task.,Learning Supervised PageRank with Gradient-Based

and Gradient-Free Optimization Methods

Lev Bogolubsky1 2  Gleb Gusev1 5  Andrei Raigorodskii5 2 1 8  Aleksey Tikhonov1  Maksim Zhukovskii1 5

{bogolubsky  gleb57  raigorodsky  altsoph  zhukmax}@yandex-team.ru

Yandex1  Moscow State University2  Buryat State University8

Pavel Dvurechensky3 4  Alexander Gasnikov4 5

Weierstrass Institute3  Institute for Information Transmission Problems RAS4 

Moscow Institute of Physics and Technology5

pavel.dvurechensky@wias-berlin.de  gasnikov@yandex.ru

Yurii Nesterov6 7

Center for Operations Research and Econometrics6 

Higher School of Economics7

yurii.nesterov@uclouvain.be

Abstract

In this paper  we consider a non-convex loss-minimization problem of learning
Supervised PageRank models  which can account for features of nodes and edges.
We propose gradient-based and random gradient-free methods to solve this problem.
Our algorithms are based on the concept of an inexact oracle and unlike the state-of-
the-art gradient-based method we manage to provide theoretically the convergence
rate guarantees for both of them. Finally  we compare the performance of the
proposed optimization methods with the state of the art applied to a ranking task.

1

INTRODUCTION

The most acknowledged methods of measuring importance of nodes in graphs are based on random
walk models. Particularly  PageRank [18]  HITS [11]  and their variants [8  9  19] are originally
based on a discrete-time Markov random walk on a link graph. Despite undeniable advantages of
PageRank and its mentioned modiﬁcations  these algorithms miss important aspects of the graph
that are not described by its structure. In contrast  a number of approaches allows to account for
different properties of nodes and edges between them by encoding them in restart and transition
probabilities (see [3  4  6  10  12  20  21]). These properties may include  e.g.  the statistics about
users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2])  types of
edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].
In the general ranking framework called Supervised PageRank [21]  weights of nodes and edges in a
graph are linear combinations of their features with coefﬁcients as the model parameters. The existing
optimization method [21] of learning these parameters and the optimizations methods proposed
in the presented paper have two levels. On the lower level  the following problem is solved: to
estimate the value of the loss function (in the case of zero-order oracle) and its derivatives (in the
case of ﬁrst-order oracle) for a given parameter vector. On the upper level  the estimations obtained
on the lower level of the optimization methods (which we also call inexact oracle information) are
used for tuning the parameters by an iterative algorithm. Following [6]  the authors of Supervised
PageRank consider a non-convex loss-minimization problem for learning the parameters and solve

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

it by a two-level gradient-based method. On the lower level of this algorithm  an estimation of the
stationary distribution of the considered Markov random walk is obtained by classical power method
and estimations of derivatives w.r.t. the parameters of the random walk are obtained by power method
introduced in [23  24]. On the upper level  the obtained gradient of the stationary distribution is
exploited by the gradient descent algorithm. As both power methods give imprecise values of the
stationary distribution and its derivatives  there was no proof of the convergence of the state-of-the-art
gradient-based method to a stationary point.
The considered non-convex loss-minimization problem [21] can not be solved by existing optimization
methods such as [16] and [7] due to presence of constraints for parameter vector and the impossibility
to calculate the exact value of the loss function. Moreover  standard global optimization methods can
not be applied  because they require unbiased estimations of the loss function.
In our paper  we propose two two-level methods to solve the problem [21]. On the lower level of
these methods  we use the linearly convergent method [17] to calculate an approximation to the
stationary distribution of Markov random walk. We show that this method allows to approximate
the value of the loss function at any given accuracy and has the lowest proved complexity bound
among methods proposed in [5]. We develop a gradient method for general constrained non-convex
optimization problems with inexact oracle  estimate its convergence rate to the stationary point of the
problem. We exploit this gradient method on the upper level of the two-level algorithm for learning
Supervised PageRank. Our contribution to the gradient-free methods framework consists in adapting
the approach of [16] to the case of constrained optimization problems when the value of the function
is calculated with some known accuracy. We prove a convergence theorem for this method and
exploit it on the upper level of the second two-level algorithm.
Another contribution consists in investigating both for the gradient and gradient-free methods the
trade-off between the accuracy of the lower-level algorithm  which is controlled by the number of
iterations of method in [17] and its generalization (for derivatives estimation)  and the computational
complexity of the two-level algorithm as a whole. Finally  we estimate the complexity of the whole
two-level algorithms for solving the loss-minimization problem with a given accuracy.
In the experiments  we apply our algorithms to learning Supervised PageRank on a real ranking task.
Summing up  both two-level methods  unlike the state-of-the-art [21]  have theoretical guarantees
on convergence rate  and outperform it in the ranking quality in experiments. The main advantages
of the ﬁrst gradient-based algorithm: the guarantees of a convergence do not require the convexity 
this algorithm has less input parameters than gradient-free one. The main advantage of the second
gradient-free algorithm is that it avoids calculating the derivative for each element of a large matrix.

2 MODEL DESCRIPTION

We concider the following random walk on a directed graph Γ = (V  E) introduced in [21]. Assume
that each node i ∈ V and each edge i → j ∈ E is represented by a vector of features Vi ∈ Rm1
+ and
a vector of features Eij ∈ Rm2
+ respectively. A surfer starts from a random page v0 of a seed set
U ⊂ V . The restart probability that v0 = i equals

(2.1)
and [π0]i = 0 for i ∈ V \ U  where ϕ1 ∈ Rm1 is a parameter  which conducts the random walk. We

[π0]i =

(cid:80)
(cid:104)ϕ1  Vi(cid:105)
l∈U(cid:104)ϕ1  Vl(cid:105)  
l∈U(cid:104)ϕ1  Vl(cid:105) should be non-zero.

assume that(cid:80)

At each step  the surfer makes a restart with probability α ∈ (0  1) (originally [18]  α = 0.15) or
traverses an outgoing edge (makes a transition) with probability 1 − α. In the former case  the surfer
chooses a vertex according to the distribution π0. In the latter one  the transition probability of
traversing an edge i → j ∈ E is

i ∈ U

(cid:80)
(cid:104)ϕ2  Eij(cid:105)
l:i→l(cid:104)ϕ2  Eil(cid:105)  

2

(2.2)
where ϕ2 ∈ Rm2 is a parameter and the current position i has non-zero outdegree  and [P (ϕ)]i j =
[π0(ϕ)]j for all j ∈ V if the outdegree of i is zero (thus the surfer always makes a restart in this case).

[P ]i j =

l:i→l(cid:104)ϕ2  Eil(cid:105) is non-zero for all i with non-zero outdegree.

We assume that(cid:80)

By Equations 2.1 and 2.2  the total probability of choosing vertex j ∈ V conditioned by the surfer
being at vertex i equals α[π0(ϕ)]j + (1 − α)[P (ϕ)]i j  where ϕ = (ϕ1  ϕ2)T and we use π0(ϕ) and
P (ϕ) to express the dependence of π0  P on the parameters.
The stationary distribution π(ϕ) ∈ Rp of the described Markov process is a solution of the system
(2.3)

π = απ0(ϕ) + (1 − α)P T (ϕ)π.

In this paper  we learn an algorithm  which ranks nodes i according to scores [π(ϕ)]i.
Let Q be a set of queries and a set of nodes Vq ⊂ V is associated to each query q. For example 
vertices in Vq may represent web pages visited by users after submitting query q. For each q ∈ Q 
some nodes of Vq are manually judged by relevance labels 1  . . .   (cid:96). Our goal is to learn the parameter
vector ϕ of a ranking algorithm πq = πq(ϕ) which minimizes the discrepancy of its ranking scores
[πq]i  i ∈ Vq  from the the assigned labels. We consider the square loss function [12  21  22]

|Q|(cid:88)

q=1

f (ϕ) =

1
|Q|

(cid:107)(Aqπq(ϕ))+(cid:107)2
2.

(2.4)

Each row of matrix Aq ∈ Rrq×pq corresponds to some pair of pages i1  i2 ∈ Vq such that the label of
i1 is strictly greater than the label of i2 (we denote by rq the number of all such pairs from Vq and
pq := |Vq|). The i1-th element of this row is equal to −1  i2-th element is equal to 1  and all other
elements are equal to 0. Vector x+ has components [x+]i = max{xi  0}.
To make ranking scores (2.3) query–dependent  we assume that π is deﬁned on a query–dependent
ij  i → j ∈ Eq. For
graph Γq = (Vq  Eq) with query-dependent feature vectors Vq
example  these features may reﬂect different aspects of query–page relevance. For a given q ∈ Q 
we consider all the objects related to the graph Γq introduced above: Uq := U  π0
q := π0  Pq := P  
πq := π. In this way  the ranking scores πq depend on query via the query–dependent features  but
the parameters of the model α and ϕ are not query–dependent. In what follows  we use the following
notations throughout the paper: nq := |Uq|  m = m1 + m2  r = maxq∈Q rq  p = maxq∈Q pq 
n = maxq∈Q nq  s = maxq∈Q sq  where sq = maxi∈Vq |{j : i → j ∈ Eq}|.
In order to
guarantee that the probabilities in (2.1) and (2.2) are correctly deﬁned  we need to appropriately
choose a set Φ of possible values of parameters ϕ. We choose some ˆϕ and R > 0 such that
Φ = {ϕ ∈ Rm : (cid:107)ϕ − ˆϕ(cid:107)2 ≤ R} lies in the set of vectors with positive components Rm
1. In this
paper  we solve the following loss-minimization problem:

i   i ∈ Vq  Eq

++

f (ϕ)  Φ = {ϕ ∈ Rm : (cid:107)ϕ − ˆϕ(cid:107)2 ≤ R}.

min
ϕ∈Φ

(2.5)

3 NUMERICAL CALCULATION OF f (ϕ) AND ∇f (ϕ)

Our goal is to provide methods for solving Problem 2.5 with guarantees on rate of convergence and
complexity bounds. The calculation of the values of f (ϕ) and its gradient ∇f (ϕ) is problematic 
since it requires to calculate those for |Q| vectors πq(ϕ) deﬁned by Equation 2.3. While the exact
values are impossible to derive in general  existing methods provide estimations of πq(ϕ) and its
derivatives dπq(ϕ)
in an iterative way with a trade-off between time and accuracy. To be able to
dϕT
guarantee convergence of our optimization algorithm in this inexact oracle setting  we consider
numerical methods that calculate approximation for πq(ϕ) and its derivatives with any required
accuracy. We have analysed state-of-the-art methods summarized in the review [5] and power method
used in [18  2  21] and have found that the method [17] is the most suitable.
It constructs a sequence πk and outputs ˜πq(ϕ  N ) by the following rule (integer N > 0 is a parameter):

π0 = π0

q (ϕ) 

πk+1 = P T

q (ϕ)πk 

˜πq(ϕ  N ) =

α

1 − (1 − α)N +1

(1 − α)kπk.

(3.1)

1As probablities [π0

q (ϕ)  Pq(λϕ) =
Pq(ϕ))  in our experiments  we consider the set Φ = {ϕ ∈ Rm : (cid:107)ϕ − em(cid:107)2 ≤ 0.99}   where em ∈ Rm is the
vector of all ones  that has large intersection with the simplex {ϕ ∈ Rm

q (ϕ)]i  i ∈ Vq  [Pq(ϕ)]˜i i  ˜i → i ∈ Eq  are scale-invariant (π0

++ : (cid:107)ϕ(cid:107)1 = 1}

k=0
q (λϕ) = π0

N(cid:88)

3

Lemma 1. Assume that  for some δ1 > 0  Method 3.1 with N =
the vector ˜πq(ϕ  N ) for every q ∈ Q. Then ˜f (ϕ  δ1) = 1|Q|
2 satisﬁes
| ˜f (ϕ  δ1) − f (ϕ)| ≤ δ1. Moreover  the calculation of ˜f (ϕ  δ1) requires not more than |Q|(3mps +
3psN + 6r) a.o.

(cid:109) − 1 is used to calculate

(cid:108) 1
(cid:80)|Q|
α ln 8r
δ1
q=1 (cid:107)(Aq ˜πq(ϕ  N ))+(cid:107)2

The proof of Lemma 1 is in Supplementary Materials.
q (ϕ). Our generalization of the method [17] for
Let pi(ϕ) be the i-th column of the matrix P T
for any q ∈ Q is the following. Choose some non-negative integer N1 and
calculation of dπq(ϕ)
dϕT
calculate ˜πq(ϕ  N1) using (3.1). Choose some N2 ≥ 0  calculate Πk  k = 0  ...  N2 and ˜Πq(ϕ  N2)

Π0 = α

dπ0

q (ϕ)

dϕT + (1 − α)

dpi(ϕ)
dϕT [˜πq(ϕ  N1)]i  Πk+1 = P T

q (ϕ)Πk 

(3.2)

(3.3)

pq(cid:88)

i=1

N2(cid:88)

k=0

1

1 − (1 − α)N2+1

(1 − α)kΠk.

˜Πq(ϕ  N2) =

(cid:80)n1
i=1 |aij|.

(cid:108) 1

In what follows  we use the following norm on the space of matrices A ∈ Rn1×n2: (cid:107)A(cid:107)1 =
maxj=1 ... n2
Lemma 2. Let β1 be some explicitly computable constant (see Supplementary Materials). Assume
α ln 24β1r
that Method 3.1 with N1 =
αδ2
˜πq(ϕ  N1) and Method 3.2  3.3 with N2 =

(cid:109) − 1 is used for every q ∈ Q to calculate the vector
(cid:108) 1
(cid:109) − 1 is used for every q ∈ Q to calculate the
(cid:80)|Q|

matrix ˜Πq(ϕ  N2) (3.3). Then the vector ˜g(ϕ  δ2) = 2|Q|
q (Aq ˜πq(ϕ  N1))+
satisﬁes (cid:107)˜g(ϕ  δ2) − ∇f (ϕ)(cid:107)∞ ≤ δ2. Moreover  the calculation of ˜g(ϕ  δ2) requires not more than
|Q|(10mps + 3psN1 + 3mpsN2 + 7r) a.o.
The proof of Lemma 2 can be found in Supplementary Materials.

(cid:16) ˜Πq(ϕ  N2)
(cid:17)T

α ln 8β1r
αδ2

AT

q=1

4 RANDOM GRADIENT-FREE OPTIMIZATION METHODS

In this section  we ﬁrst describe general framework of random gradient-free methods with inexact
oracle and then apply it for Problem 2.5. Lemma 1 allows to control the accuracy of the inexact
zero-order oracle and hence apply random gradient-free methods with inexact oracle.

4.1 GENERAL FRAMEWORK

Below we extend the framework of random gradient-free methods [1  16  7] for the situation of
presence of uniformly bounded error of unknown nature in the value of an objective function in
general optimization problem. Unlike [16]  we consider a constrained optimization problem and a
randomization on a Euclidean sphere which seems to give better large deviations bounds and doesn’t
need the assumption that the objective function can be calculated at any point of Rm.
Let E be a m-dimensional vector space and E∗ be its dual. In this subsection  we consider a general
function f (·) : E → R and denote its argument by x or y to avoid confusion with other sections. We
denote the value of linear function g ∈ E∗ at x ∈ E by (cid:104)g  x(cid:105). We choose some norm (cid:107) · (cid:107) in E and
∀x  y ∈ E. The problem
say that f ∈ C 1 1
of our interest is to ﬁnd minx∈X f (x)  where f ∈ C 1 1
L ((cid:107) · (cid:107))  X is a closed convex set and there
exists a number D ∈ (0  +∞) such that diamX := maxx y∈X (cid:107)x − y(cid:107) ≤ D. Also we assume that
the inexact zero-order oracle for f (x) returns a value ˜f (x  δ) = f (x) + ˜δ(x)  where ˜δ(x) is the error
satisfying for some δ > 0 (which is known) |˜δ(x)| ≤ δ for all x ∈ X. Let x∗ ∈ arg minx∈X f (x).
Denote f∗ = minx∈X f (x).
τ ( ˜f (x + τ ξ  δ)− ˜f (x  δ))ξ  where
Unlike [16]  we deﬁne the biased gradient-free oracle gτ (x  δ) = m
ξ is a random vector uniformly distributed over the unit sphere S = {t ∈ Rm : (cid:107)t(cid:107)2 = 1}  τ is a
smoothing parameter.

L ((cid:107)·(cid:107)) iff |f (x)− f (y)−(cid:104)∇f (y)  x− y(cid:105)| ≤ L

2 (cid:107)x− y(cid:107)2 

4

Algorithm 1 Gradient-type method

Input: Point x0 ∈ X  stepsize h > 0  number of steps M.
Set k = 0.
repeat

Generate ξk and calculate corresponding gτ (xk  δ).
Calculate xk+1 = ΠX (xk − hgτ (xk  δ)) (ΠX (·) – Euclidean projection onto the set X).
Set k = k + 1.

until k > M
Output: The point yM = arg minx{f (x) : x ∈ {x0  . . .   xM}}.

Theorem 1. Let f ∈ C 1 1
generated by Algorithm 1 with h = 1
M +1 + τ 2L(m+8)
8mLD2
vector ξ.

L ((cid:107) · (cid:107)2) and convex. Assume that x∗ ∈ intX  and the sequence xk is
8mL . Then for any M ≥ 0  we have EΞM−1 f (yM ) − f∗ ≤
Lτ 2 . Here Ξk = (ξ0  . . .   ξk) is the history of realizations of the

8

+ δmD

4τ + δ2m

The full proof of the theorem is in Supplementary Materials.

4.2 SOLVING THE LEARNING PROBLEM

Now  we apply the results of Subsection 4.1 to solve Problem 2.5. Note that presence of constraints
and oracle inexactness do not allow to directly apply the results of [16]. We assume that there is a
local minimum ϕ∗  and Φ is a small vicinity of ϕ∗  in which f (ϕ) (2.4) is convex (generally speaking 
it is nonconvex). We choose the desired accuracy ε for f∗ (the optimal value) approximation in the
sense that EΞM−1 f (yM ) − f∗ ≤ ε. In accordance with Theorem 1  ε gives the number of steps
M of Algorithm 1  the value of τ  the value of the required accuracy δ of the inexact zero-order
oracle. The value δ  by Lemma 1  gives the number of steps N of Method 3.1 required to calculate
a δ-approximation ˜f (ϕ  δ) for f (ϕ). Then the inexact zero-order oracle ˜f (ϕ  δ) is used to make
Algorithm 1 step. Theorem 1 and the choice of the feasible set Φ to be a Euclidean ball make it
natural to choose (cid:107) · (cid:107)2-norm in the space Rm of parameter ϕ. It is easy to see that in this norm
diamΦ ≤ 2R. Algorithm 2 in Supplementary Materials is a formal record of these ideas.
The most computationally hard on each iteration of the main cycle of this method are calculations
of ˜f (ϕk + τ ξk  δ)  ˜f (ϕk  δ). Using Lemma 1  we obtain the complexity of each iteration and the
following result  which gives the complexity of Algorithm 2.
Theorem 2. Assume that the set Φ in (2.5) is chosen in a way such that f (ϕ) is convex on Φ and some
ϕ∗ ∈ arg minϕ∈Φ f (ϕ) belongs also to intΦ. Then the mean total number of arithmetic operations
of the Algorithm 2 for the accuracy ε (i.e. for the inequality EΞM−1f ( ˆϕM ) − f (ϕ∗) ≤ ε to hold) is
not more than

(cid:32)

128mrR(cid:112)L(m + 8)

√

ε3/2

2

(cid:33)

+ 6r

.

768mps|Q| LR2
ε

m +

1
α

ln

5 GRADIENT-BASED OPTIMIZATION METHODS

In this section  we ﬁrst develop a general framework of gradient methods with inexact oracle for
non-convex problems from rather general class and then apply it for the particular Problem 2.5.
Lemma 1 and Lemma 2 allow to control the accuracy of the inexact ﬁrst-order oracle and hence apply
proposed framework.

5.1 GENERAL FRAMEWORK

In this subsection  we generalize the approach in [7] for constrained non-convex optimization
problems. Our main contribution consists in developing this framework for an inexact ﬁrst-order
oracle and unknown "Lipschitz constant" of this oracle.
We consider a composite optimization problem of the form minx∈X{ψ(x) := f (x) + h(x)}  where
X ⊂ E is a closed convex set  h(x) is a simple convex function  e.g. (cid:107)x(cid:107)1. We assume that f (x) is

5

a general function endowed with an inexact ﬁrst-order oracle in the following sense. There exists
a number L ∈ (0  +∞) such that for any δ ≥ 0 and any x ∈ X one can calculate ˜f (x  δ) ∈ R and
˜g(x  δ) ∈ E∗ satisfying

|f (y) − ( ˜f (x  δ) − (cid:104)˜g(x  δ)  y − x(cid:105))| ≤ L
2

(5.1)
for all y ∈ X. The constant L can be considered as "Lipschitz constant" because for the exact ﬁrst-
order oracle for a function f ∈ C 1 1
L ((cid:107) · (cid:107)) Inequality 5.1 holds with δ = 0. This is a generalization
of the concept of (δ  L)-oracle considered in [25] for convex problems.
We choose a prox-function d(x) which is continuously differentiable and 1-strongly convex on X
with respect to (cid:107) · (cid:107). This means that for any x  y ∈ X d(y) − d(x) − (cid:104)∇d(x)  y − x(cid:105) ≥ 1
2(cid:107)y − x(cid:107)2.
We deﬁne also the corresponding Bregman distance V (x  z) = d(x) − d(z) − (cid:104)∇d(z)  x − z(cid:105).

(cid:107)x − y(cid:107)2 + δ.

Algorithm 2 Adaptive projected gradient algorithm

Input: Point x0 ∈ X  number L0 > 0.
Set k = 0  z = +∞.
repeat

Set Mk = Lk  ﬂag = 0.
repeat

16Mk

. Calculate ˜f (xk  δ) and ˜g(xk  δ).

Set δ = ε
Find wk = arg minx∈Q {(cid:104)˜g(xk  δ)  x(cid:105) + MkV (x  xk) + h(x)} and calculate ˜f (wk  δ).
If the inequality ˜f (wk  δ) ≤ ˜f (xk  δ) + (cid:104)˜g(xk  δ)  wk − xk(cid:105) + Mk
set ﬂag = 1. Otherwise set Mk = 2Mk.

2 (cid:107)wk − xk(cid:107)2 + ε

8Mk

holds 

until ﬂag = 1
Set xk+1 = wk  Lk+1 = Mk
2 .
If (cid:107)Mk(xk − xk+1)(cid:107) < z  set z = (cid:107)Mk(xk − xk+1)(cid:107)  K = k.
Set k = k + 1.

until z ≤ ε
Output: The point xK+1.

Theorem 3. Assume that f (x) is endowed with the inexact ﬁrst-order oracle in a sense (5.1) and
that there exists a number ψ∗ > −∞ such that ψ(x) ≥ ψ∗ for all x ∈ X. Then after M iterations of
Algorithm 2 it holds that (cid:107)MK(xK − xK+1)(cid:107)2 ≤ 4L(ψ(x0)−ψ∗)
2 . Moreover  the total number of
inexact oracle calls is not more than 2M + 2 log2

+ ε

M +1

.

2L
L0

The full proof of the theorem is in Supplementary Materials.

5.2 SOLVING THE LEARNING PROBLEM

√

In this subsection  we return to Problem 2.5 and apply the results of the previous subsection. Note
that we can not directly apply the results of [7] due to the inexactness of the oracle. For this problem 
h(·) ≡ 0. It is easy to show that in 1-norm diamΦ ≤ 2R
m. For any δ > 0  Lemma 1 with δ1 = δ
2
allows us to obtain ˜f (ϕ  δ1) such that inequality | ˜f (ϕ  δ1) − f (ϕ)| ≤ δ1 holds and Lemma 2 with
m allows us to obtain ˜g(ϕ  δ2) such that inequality (cid:107)˜g(ϕ  δ2) − ∇f (ϕ)(cid:107)∞ ≤ δ2 holds.
√
δ2 = δ
4R
Similar to [25]  since f ∈ C 1 1
L ((cid:107) · (cid:107)2)  these two inequalities lead to Inequality 5.1 for ˜f (ϕ  δ1) in
the role of ˜f (x  δ)  ˜g(ϕ  δ2) in the role of ˜g(x  δ) and (cid:107) · (cid:107)2 in the role of (cid:107) · (cid:107).
We choose the desired accuracy ε for approximating the stationary point of Problem 2.5. This
accuracy gives the required accuracy δ of the inexact ﬁrst-order oracle for f (ϕ) on each step of the
inner cycle of the Algorithm 2. Knowing the value δ1 = δ
2 and using Lemma 1  we choose the number
of steps N of Method 3.1 and thus approximate f (ϕ) with the required accuracy δ1 by ˜f (ϕ  δ1).
√
Knowing the value δ2 = δ
m and using Lemma 2  we choose the number of steps N1 of Method 3.1
4R
and the number of steps N2 of Method 3.2  3.3 and obtain the approximation ˜g(ϕ  δ2) of ∇f (ϕ) with
the required accuracy δ2. Then we use the inexact ﬁrst-order oracle ( ˜f (ϕ  δ1)  ˜g(ϕ  δ2)) to perform
a step of Algorithm 2. Since Φ is the Euclidean ball  it is natural to set E = Rm and (cid:107) · (cid:107) = (cid:107) · (cid:107)2 

6

2(cid:107)ϕ(cid:107)2

2. Then the Bregman distance is V (ϕ  ω) = 1

choose the prox-function d(ϕ) = 1
Algorithm 4 in Supplementary Materials is a formal record of the above ideas.
The most computationally consuming operations of the inner cycle of Algorithm 4 are calculations of
˜f (ϕk  δ1)  ˜f (ωk  δ1) and ˜g(ϕk  δ2). Using Lemma 1 and Lemma 2  we obtain the complexity of each
iteration. From Theorem 3 we obtain the following result  which gives the complexity of Algorithm
4.
Theorem 4. The total number of arithmetic operations in Algorithm 4 for the accuracy ε (i.e. for the
inequality (cid:107)MK(ϕK − ϕK+1)(cid:107)2

2(cid:107)ϕ − ω(cid:107)2
2.

(cid:19)

.

√

m

(cid:18) 8L(f (ϕ0) − f∗)

ε

(cid:19)

(cid:18)

2 ≤ ε to hold) is not more than
6mps|Q|

·

7r|Q| +

+ log2

2L
L0

ln

1024β1rRL

αε

α

6 EXPERIMENTAL RESULTS

In this section  we compare our gradient-free and gradient-based methods with the state-of-the-art
gradient-based method [21] on the web page ranking problem. In the next section  we describe the
dataset. In Section 6.2  we report the results of the experiments.

6.1 DATA
We consider the user web browsing graph Γq = (Vq  Eq)  q ∈ Q  introduced in [12]. Unlike a link
graph  a user browsing graph is query–dependent. The set of vertices Vq consists of all different
pages visited by users during their sessions started from q. The set of directed edges Eq represents all
the ordered pairs of neighboring elements (˜i  i) from such sessions. We add a page i in the seed set
Uq if and only if there is a session where i is the ﬁrst page visited after submitting query q.
All experiments are performed with data of a popular commercial search engine Yandex2. We chose
a random set of 600 queries Q and collected user sessions started with them. There are ≈ 11.7K
vertices and ≈ 7.5K edges in graphs Γq  q ∈ Q  in total. For each query  a set of pages was labelled
by professional assessors with standard 5 relevance grades (≈ 1.7K labeled query–document pairs
in total). We divide our data into two parts. On the ﬁrst part Q1 (50% of the set of queries Q) we
train the parameters and on the second part Q2 we test the algorithms. For each q ∈ Q and i ∈ Vq 
vector Vq
of m2 = 52
features for an edge ˜i → i ∈ Eq is obtained as the concatenation of Vq
To study a dependency between the efﬁciency of the algorithms and the sizes of the graphs  we sort
the sets Q1  Q2 in ascending order of sizes of the respective graphs. Sets Q1
j contain ﬁrst (in
terms of these order) 100  200  300 elements respectively for j ∈ {1  2}.

i of size m1 = 26 encodes features for query–document pair (q  i). Vector Eq
˜ii

and Vq
i .

˜i

j  Q2

j  Q3

6.2 PERFORMANCES OF THE OPTIMIZATION ALGORITHMS

We optimized the parameters ϕ by three methods: our gradient-free method GFN (Algorithm 2)  the
gradient-based method GBN (Algorithm 4)  and the state-of-the-art gradient-method GBP. The values
of hyperparameters are the following: the Lipschitz constant L = 10−4 in GFN (and L0 = 10−4
in GBN)  the accuracy ε = 10−6 (in both GBN and GFN)  the radius R = 0.99 (in both GBN
and GFN). On all sets of queries  we compare ﬁnal values of the loss function for GBN when
L0 ∈ {10−4  10−3  10−2  10−1  1}. The differences are less than 10−7. We choose L in GFN to be
equal to L0 (we show how the choice of L inﬂuences the output of the gradient-free algorithm  see
supplementary materials  Figure 2). Moreover  we evaluate both our gradient-based and gradient-free
algorithms for different values of the accuracies. The outputs of the algorithms differ insufﬁciently
2  i ∈ {1  2  3}  when ε ≤ 10−6. On the lower level of the state-of-the-art gradient-
on all test sets Qi
based algorithm  the stochastic matrix and its derivative are raised to the power 100. We evaluate
GBP for different values of the step size (50  100  200  500). We stop the GBP algorithms when the
differences between the values of the loss function on the next step and the current step are less than
−10−5 on the test sets.

2yandex.com

7

In Table 1  we present the performances of the optimization algorithms in terms of the loss function
f (2.4). We also compare the algorithms with the untuned Supervised PageRank (ϕ = ϕ0 = em).
On Figure 1  we give the outputs of the optimization algorithms on each iteration of the upper levels
of the learning processes on the test set Q3

2  similar results were obtained for the sets Q1

2.
2  Q2

Q1
2

loss

steps

loss

Q2
2

steps

Q3
2

.00354
.00305
.00297
.00307
.00307
.00308
.00308

0
12
106
31
16
7
2

loss
.0033
.00295
.00292
.00295
.00295
.00295
.00295

steps

0
12
106
40
20
9
3

Meth.
PR
GBN
GFN

GBP 50s.
GBP 100s.
GBP 200s.
GBP 500s.

.00357
.00279
.00274
.00282
.00282
.00283
.00283

0
12
106
16
8
4
2

Table 1: Comparison of the algorithms on the test sets.

Figure 1: Values of the loss function on each iteration of the optimization algorithms on the test set Q3
2.

GFN signiﬁcantly outperforms the state-of-the-art algorithms on all test sets. GBN signiﬁcantly
outperforms the state-of-the-art algorithm on Q1
2 (we obtain the p-values of the paired t-tests for all
the above differences on the test sets of queries  all these values are less than 0.005). However  GBN
requires less iterations of the upper level (until it stops) than GBP for step sizes 50 and 100 on Q2
2.
2  Q3
Finally  we show that Nesterov–Nemirovski method converges to the stationary distribution faster
than the power method (in supplementary materials  on Figure 2  we demonstrate the dependencies of
the value of the loss function on Q1
1 for both methods of computing the untuned Supervised PageRank
ϕ = ϕ0 = em).

7 CONCLUSION

We propose a gradient-free optimization method for general convex problems with inexact zero-order
oracle and an adaptive gradient method for possibly nonconvex general composite optimization
problems with inexact ﬁrst-order oracle. For both methods  we provide convergence rate analysis.
We also apply our new methods for known problem of learning a web-page ranking algorithm.
Our new algorithms not only outperform existing algorithms  but also are guaranteed to solve this
learning problem. In practice  this means that these algorithms can increase the reliability and speed
of a search engine. Also  to the best of our knowledge  this is the ﬁrst time when the ideas of
random gradient-free and gradient optimization methods are combined with some efﬁcient method
for huge-scale optimization using the concept of an inexact oracle.
Acknowledgments The research by P. Dvurechensky and A. Gasnikov presented in Section 4 of this paper was
conducted in IITP RAS and supported by the Russian Science Foundation grant (project 14-50-00150)  the
research presented in Section 5 was supported by RFBR.

8

References
[1] A. Agarwal  O. Dekel and L. Xiao  Optimal algorithms for online convex optimization with multi-point

bandit feedback  2010  23rd Annual Conference on Learning Theory (COLT).

[2] L. Backstrom and J. Leskovec  Supervised random walks: predicting and recommending links in social

networks  2011  WSDM.

[3] Na Dai and Brian D. Davison  Freshness Matters: In Flowers  Food  and Web Authority  2010  SIGIR.
[4] N. Eiron  K. S. McCurley and J. A. Tomlin  Ranking the web frontier  2004  WWW.
[5] A. Gasnikov and D. Dmitriev  Efﬁcient randomized algorithms for PageRank problem  Comp. Math. &

Math. Phys  2015  55(3): 1–18.

[6] B. Gao  T.-Y. Liu  W. W. Huazhong  T. Wang and H. Li  Semi-supervised ranking on very large graphs

with rich metadata  2011  KDD.

[7] S. Ghadimi  G. Lan  Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming 

SIAM Journal on Optimization  2014  23(4)  2341–2368.

[8] T. H. Haveliwala  Efﬁcient computation of PageRank  Stanford University  1999.
[9] T. H. Haveliwala  Topic-Sensitive PageRank  2002  WWW.
[10] G. Jeh and J. Widom  Scaling Personalized Web Search  2003  WWW.
[11] J. M. Kleinberg  Authoritative sources in a hyperlinked environment  1998  SODA.
[12] Y. Liu  B. Gao  T.-Y. Liu  Y. Zhang  Z. Ma  S. He  H. Li  BrowseRank: Letting Web Users Vote for Page

Importance  2008  SIGIR.

[13] J. Matyas  Random optimization  Automation and Remote Control  1965  26: 246–253.
[14] Yu. Nesterov  Introductory Lectures on Convex Optimization  Springer  2004  New York.
[15] Yu. Nesterov  Efﬁciency of coordinate descent methods on huge-scale optimization problems  SIAM

Journal on Optimization  2012  22(2): 341–362.

[16] Yu. Nesterov and V. Spokoiny  Random Gradient-Free Minimization of Convex Functions  Foundations of

Computational Mathematics  2015  1–40.

[17] Yu. Nesterov and A. Nemirovski  Finding the stationary states of Markov chains by iterative methods 

Applied Mathematics and Computation  2015  255: 58–65.

[18] L. Page  S. Brin  R. Motwani  T. Winograd  The PageRank citation ranking: Bringing order to the web 

Stanford InfoLab  1999.

[19] M. Richardson and P. Domingos  The intelligent surfer: Probabilistic combination of link and content

information in PageRank  2002  NIPS.

[20] M. Zhukovskii  G. Gusev  P. Serdyukov  URL Redirection Accounting for Improving Link-Based Ranking

Methods  2013  ECIR.

[21] M. Zhukovskii  G. Gusev  P. Serdyukov  Supervised Nested PageRank  2014  CIKM.
[22] M. Zhukovskii  A. Khropov  G. Gusev  P. Serdyukov  Fresh BrowseRank  2013  SIGIR.
[23] A. L. Andrew  Convergence of an iterative method for derivatives of eigensystems  Journal of Computational

Physics  1978  26: 107–112.

[24] A. Andrew  Iterative computation of derivatives of eigenvalues and eigenvectors  IMA Journal of Applied

Mathematics  1979  24(2): 209–218.

[25] O. Devolder  F. Glineur  Yu. Nesterov  First-order methods of smooth convex optimization with inexact

oracle  Mathematical Programming  2013  146(1): 37–75.

[26] Yu. Nesterov  B.T. Polyak  Cubic regularization of Newton method and its global performance  Mathemati-

cal Programming  2006  108(1) 177–205.

[27] Yu. Nesterov  Gradient methods for minimizing composite functions  Mathematical Programming  2012 

140(1) 125–161.

9

,Lev Bogolubsky
Pavel Dvurechenskii
Alexander Gasnikov
Gleb Gusev
Yurii Nesterov
Andrei Raigorodskii
Aleksey Tikhonov