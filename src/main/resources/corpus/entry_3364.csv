2017,Max-Margin Invariant Features from Transformed Unlabelled Data,The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper  we study kernels that are invariant to a unitary group while having theoretical guarantees in addressing the important practical issue of unavailability of transformed versions of labelled data. A problem we call the Unlabeled Transformation Problem which is a special form of semi-supervised learning and one-shot learning. We present a theoretically motivated alternate approach to the invariant kernel SVM based on which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As an illustration  we design an framework for face recognition and demonstrate the efficacy of our approach on a large scale semi-synthetic dataset with 153 000 images and a new challenging protocol on Labelled Faces in the Wild (LFW) while out-performing strong baselines.,Max-Margin Invariant Features from Transformed

Unlabeled Data

Dipan K. Pal  Ashwin A. Kannan∗  Gautam Arakalgud∗  Marios Savvides

Department of Electrical and Computer Engineering

Carnegie Mellon University

Pittsburgh  PA 15213

{dipanp aalapakk garakalgud marioss}@cmu.edu

Abstract

The study of representations invariant to common transformations of the data
is important to learning. Most techniques have focused on local approximate
invariance implemented within expensive optimization frameworks lacking explicit
theoretical guarantees. In this paper  we study kernels that are invariant to a unitary
group while having theoretical guarantees in addressing the important practical
issue of unavailability of transformed versions of labelled data. A problem we
call the Unlabeled Transformation Problem which is a special form of semi-
supervised learning and one-shot learning. We present a theoretically motivated
alternate approach to the invariant kernel SVM based on which we propose Max-
Margin Invariant Features (MMIF) to solve this problem. As an illustration  we
design an framework for face recognition and demonstrate the efﬁcacy of our
approach on a large scale semi-synthetic dataset with 153 000 images and a new
challenging protocol on Labelled Faces in the Wild (LFW) while out-performing
strong baselines.

1

Introduction

It is becoming increasingly important to learn well generalizing representations that are invariant
Indeed  being invariant to intra-class
to many common nuisance transformations of the data.
transformations while being discriminative to between-class transformations can be said to be one of
the fundamental problems in pattern recognition. The nuisance transformations can give rise to many
‘degrees of freedom’ even in a constrained task such as face recognition (e.g. pose  age-variation 
illumination etc.). Explicitly factoring them out leads to improvements in recognition performance as
found in [10  7  6]. It has also been shown that that features that are explicitly invariant to intra-class
transformations allow the sample complexity of the recognition problem to be reduced [2]. To this
end  the study of invariant representations and machinery built on the concept of explicit invariance is
important.
Invariance through Data Augmentation. Many approaches in the past have enforced invariance by
generating transformed labelled training samples in some form such as [13  17  19  9  15  4]. Perhaps 
one of the most popular method for incorporating invariances in SVMs is the virtual support method
(VSV) in [18]  which used sequential runs of SVMs in order to ﬁnd and augment the support vectors
with transformed versions of themselves.
Indecipherable transformations in data leads to shortage of transformed labelled samples. The
above approaches however  assume that one has explicit knowledge about the transformation. This
is a strong assumption. Indeed  in most general machine learning applications  the transformation

∗Authors contributed equally

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Max-Margin Invariant Features (MMIF) can
solve an important problem we call the Unlabeled Trans-
formation Problem. In the ﬁgure  a traditional classiﬁer
F (x) "learns" invariance to nuisance transformations
directly from the labeled dataset X . On the other hand 
our approach (MMIF) can incorporate additional invari-
ance learned from any unlabeled data that undergoes the
nuisance transformation of interest.

present in the data is not clear and cannot be modelled easily  e.g. transformations between different
views of a general 3D object and between different sentences articulated by the same person. Methods
which work on generating invariance by explicitly transforming or augmenting labelled training data
cannot be applied to these scenarios. Further  in cases where we do know the transformations that
exist and we actually can model them  it is difﬁcult to generate transformed versions of very large
labelled datasets. Hence there arises an important problem: how do we train models to be invariant to
transformations in test data  when we do not have access to transformed labelled training samples ?
Availability of unlabeled transformed data.
Although it is difﬁcult to obtain or generate
transformed labelled data (due to the reasons
mentioned above)  unlabeled transformed data
is more readily available. For instance  if differ-
ent views of speciﬁc objects of interest are not
available  one can simply collect views of gen-
eral objects. Also  if different sentences spoken
by a speciﬁc group of people are not available 
one can simply collect those spoken by mem-
bers of the general population. In both these
scenarios  no explicit knowledge or model of
the transformation is needed  thereby bypassing
the problem of indecipherable transformations.
This situation is common in vision e.g. only
unlabeled transformed images are observed  but
has so far mostly been addressed by the com-
munity by intense efforts in large scale data col-
lection. Note that the transformed data that is
collected is not required to be labelled. We now
are in a position to state the central problem that
this paper addresses.
The Unlabeled Transformation (UT) Problem:
Having access to transformed versions of the training unlabeled data but not of labelled data  how
do we learn a discriminative model of the labelled data  while being invariant to transformations
present in the unlabeled data ?
Overall approach. The approach presented in this paper however (see Fig. 1)  can solve this problem
and learn invariance to transformations observed only through unlabeled samples and does not need
labelled training data augmentation. We explicitly and simultaneously address both problems of
generating invariance to intra-class transformation (through invariant kernels) and being discriminative
to inter or between class transformations (through max-margin classiﬁers). Given a new test sample 
the ﬁnal extracted feature is invariant to the transformations observed in the unlabeled set  and thereby
generalizes using just a single example. This is an example of one-shot learning.
Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to
considerable depth. Nonetheless  the study of invariant kernels and techniques to extract invariant
features has received much less attention. An invariant kernel allows the kernel product to remain
invariant under transformations of the inputs. Most instances of incorporating invariances focused
on local invariances through regularization and optimization such as [18  19  3  21]. Some other
techniques were jittering kernels [17  3] and tangent-distance kernels [5]  both of which sacriﬁced
the positive semi-deﬁnite property of its kernels and were computationally expensive. Though these
methods have had some success  most of them still lack explicit theoretical guarantees towards
invariance. The proposed invariant kernel SVM formulation on the other hand  develops a valid PSD
kernel that is guaranteed to be invariant. [4] used group integration to arrive at invariant kernels
but did not address the Unlabeled Transformation problem which our proposed kernels do address.
Further  our proposed kernels allow for the formulation of the invariant SVM and application to large
scale problems. Recently  [14] presented some work with invariant kernels. However  unlike our
non-parametric formulation  they do not learn the group transformations from the data itself and
assume known parametric transformations (i.e. they assume that transformation is computable).
Key ideas. The key ideas in this paper are twofold.

2

Transformed unlabeled dataNon-transformed labeled dataTrain invariant to Test imagenot invariant to Train1. The ﬁrst is to model transformations using unitary groups (or sub-groups) leading to unitary-
group invariant kernels. Unitary transforms allow the dot product to be preserved and
allow for interesting generalization properties leading to low sample complexity and also
allow learning transformation invariance from unlabeled examples (thereby solving the
Unlabeled Transformation Problem). Classes of learning problems  such as vision  often
have transformations belonging to a unitary-group  that one would like to be invariant
towards (such as translation and rotation). In practice however  [8] found that invariance to
much more general transformations not captured by this model can been achieved.

2. Secondly  we combine max-margin classiﬁers with invariant kernels leading to non-linear
max-margin unitary-group invariant classiﬁers. These theoretically motivated invariant
non-linear SVMs form the foundation upon which Max-Margin Invariant Features (MMIF)
are based. MMIF features can effectively solve the important Unlabeled Transformation
Problem. To the best of our knowledge  this is the ﬁrst theoretically proven formulation of
this nature.

Contributions.
In contrast to many previous studies on invariant kernels  we study non-linear
positive semi-deﬁnite unitary-group invariant kernels guaranteeing invariance that can address the UT
Problem. One of our central theoretical results to applies group integration in the RKHS. It builds on
the observation that  under unitary restrictions on the kernel map  group action in the input space is
reciprocated in the RKHS. Using the proposed invariant kernel  we present a theoretically motivated
approach towards a non-linear invariant SVM that can solve the UT Problem with explicit invariance
guarantees. As our main theoretical contribution  we showcase a result on the generalization of
max-margin classiﬁers in group-invariant subspaces. We propose Max-Margin Invariant Features
(MMIF) to learn highly discriminative non-linear features that also solve the UT problem. On the
practical side  we propose an approach to face recognition to combine MMIFs with a pre-trained
deep learning feature extractor (in our case VGG-Face [12]). MMIF features can be used with deep
learning whenever there is a need to focus on a particular transformation in data (in our application
pose in face recognition) and can further improve performance.

2 Unitary-Group Invariant Kernels
Premise: Consider a dataset of normalized samples along with labels X = {xi} Y = {yi} ∀i ∈
1...N with x ∈ Rd and y ∈ {+1 −1}. We now introduce into the dataset a number of unitary trans-
formations g part of a locally compact unitary-group G. We note again that the set of transformations
under consideration need not be the entire unitary group. They could very well be a subgroup. Our
augmented normalized dataset becomes {gxi  yi} ∀g ∈ G ∀i. For clarity  we denote by gx the action
of group element g ∈ G on x  i.e. gx = g(x). We also deﬁne an orbit of x under G as the set
XG = {gx} ∀g ∈ G. Clearly  X ⊆ XG. An invariant function is deﬁned as follows.
Deﬁnition 2.1 (G-Invariant Function). For any group G  we deﬁne a function f : X → Rn to be
G-invariant if f (x) = f (gx) ∀x ∈ X ∀g ∈ G.
One method of generating an invariant towards a group is through group integration. Group integration
has stemmed from classical invariant theory and can be shown to be a projection onto a G-invariant
subspace for vector spaces. In such a space x = gx ∀g ∈ G and thus the representation x is invariant
under the transformation of any element from the group G. This is ideal for recognition problems
where one would want to be discriminative to between-class transformations (for e.g. between distinct
subjects in face recognition) but be invariant to within-class transformations (for e.g. different images
of the same subject). The set of transformations we model as G are the within-class transformations
that we would like to be invariant towards. An invariant to any group G can be generated through the
following basic (previously) known property (Lemma 2.1) based on group integration.
Lemma 2.1. (Invariance Property) Given a vector ω ∈ Rd  and any afﬁne group G  for any ﬁxed

G gω dg =(cid:82)

G gω dg

g(cid:48) ∈ G and a normalized Haar measure dg  we have g(cid:48)(cid:82)
results in the quantity(cid:82)

The Haar measure (dg) exists for every locally compact group and is unique up to a positive
multiplicative constant (hence normalized). A similar property holds for discrete groups. Lemma 2.1
G gω dg enjoy global invariance (encompassing all elements) to group G.
This property allows one to generate a G-invariant subspace in the inherent space Rd through group
integration. In practice  the integral corresponds to a summation over transformed samples. The

3

following two lemmas (novel results  and part of our contribution) (Lemma 2.2 and 2.3) showcase
G g dg for a unitary-group G 2. These properties would

prove useful in the analysis of unitary-group invariant kernels and features.

elementary properties of the operator Ψ =(cid:82)
Lemma 2.2. If Ψ =(cid:82)
Lemma 2.3. (Unitary Projection) If Ψ = (cid:82)

G g dg for unitary G  then ΨT = Ψ

G g dg for any afﬁne G  then ΨΨ = Ψ  i.e.

it is a

projection operator. Further  if G is unitary  then (cid:104)ω  Ψω(cid:48)(cid:105) = (cid:104)Ψω  ω(cid:48)(cid:105) ∀ω  ω(cid:48) ∈ Rd
Sample Complexity and Generalization. On applying the operator Ψ to the dataset X   all points
in the set {gx | g ∈ G} for any x ∈ X map to the same point Ψx in the G-invariant subspace
thereby reducing the number of distinct points by a factor of |G| (the cardinality of G  if G is ﬁnite).
Theoretically  this would drastically reduce sample complexity while preserving linear feasibility
(separability). It is trivial to observe that a perfect linear separator learned in XΨ = {Ψx | x ∈
X} would also be a perfect separator for XG  thus in theory achieving perfect generalization.
Generalization here refers to the ability to perform correct classiﬁcation even in the presence of the
set of transformations G. We prove a similar result for Reproducing Kernel Hilbert Spaces (RKHS)
in Section 2.2. This property is theoretically powerful since cardinality of G can be large. A classiﬁer
can avoid having to observe transformed versions {gx} of any x and yet generalize perfectly.
The case of Face Recognition. As an illustration  if the group G of transformations considered is
G g dg
represents a pose invariant subspace. In theory  all poses of a subject will converge to the same point
in that subspace leading to near perfect pose invariant recognition.
We have not yet leveraged the power of the unitary structure of the groups which is also critical in
generalization to test cases as we would see later. We now present our central result showcasing that
unitary kernels allow the unitary group action to reciprocate in a Reproducing Kernel Hilbert Space.
This is critical to set the foundation for our core method called Max-Margin Invariant Features.

pose (it is hypothesized that small changes in pose can be modeled as unitary [10])  then Ψ =(cid:82)

2.1 Group Actions Reciprocate in a Reproducing Kernel Hilbert Space

Group integration provides exact invariance as seen in the previous section. However  it requires
the group structure to be preserved  i.e. if the group structure is destroyed  group integration does
not provide an invariant function. In the context of kernels  it is imperative that the group relation
between the samples in XG be preserved in the kernel Hilbert space H corresponding to some kernel
k with a mapping φ. If the kernel k is unitary in the following sense  then this is possible.
Deﬁnition 2.2 (Unitary Kernel). A kernel k(x  y) = (cid:104)φ(x)  φ(y)(cid:105) is a unitary kernel if  for a unitary
group G  the mapping φ(x) : X → H satisﬁes (cid:104)φ(gx)  φ(gy)(cid:105) = (cid:104)φ(x)  φ(y)(cid:105) ∀g ∈ G ∀x  y ∈ X .

The unitary condition is fairly general  a common class of unitary kernels is the RBF kernel. We now
deﬁne a transformation within the RKHS itself as gH : φ(x) → φ(gx) ∀φ(x) ∈ H for any g ∈ G
where G is a unitary group. We then have the following result of signiﬁcance.
Theorem 2.4. (Covariance in the RKHS) If k(x  y) = (cid:104)φ(x)  φ(y)(cid:105) is a unitary kernel in the sense of
Deﬁnition 2.2  then gH is a unitary transformation  and the set GH = {gH | gH : φ(x) → φ(gx) ∀g ∈
G} is a unitary-group in H.

Theorem 2.4 shows that the unitary-group structure is preserved in the RKHS. This paves the way for
new theoretically motivated approaches to achieve invariance to transformations in the RKHS. There
have been a few studies on group invariant kernels [4  10]. However  [4] does not examine whether
the unitary group structure is actually preserved in the RKHS  which is critical. Also  DIKF was
recently proposed as a method utilizing group structure under the unitary kernel [10]. Our result is a
generalization of the theorems they present. Theorem 2.4 shows that since the unitary group structure
is preserved in the RKHS  any method involving group integration would be invariant in the original
space. The preservation of the group structure allows more direct group invariance results to be
applied in the RKHS. It also directly allows one to formulate a non-linear SVM while guaranteeing
invariance theoretically leading to Max-Margin Invariant Features.

2All proofs are presented in the supplementary material

4

2.2

Invariant Non-linear SVM: An Alternate Approach Through Group Integration

We now apply the group integration approach to the kernel SVM. The decision function of SVMs
can be written in the general form as fθ(x) = ωT φ(x) + b for some bias b ∈ R (we agglomerate all
parameters of f in θ) where φ is the kernel feature map  i.e. φ : X → H. Reviewing the SVM  a
maximum margin separator is found by minimizing loss functions such as the hinge loss along with a
regularizer. In order to invoke invariance  we can now utilize group integration in the the kernel space
H using Theorem 2.4. All points in the set {gx ∈ XG} get mapped to φ(gx) = gHφ(x) for a given
g ∈ G in the input space X . Group integration then results in a G-invariant subspace within H through
GH gH dgH using Lemma 2.1. Introducing Lagrange multipliers α = (α1  α2...αN ) ∈ RN  

ΨH =(cid:82)

the dual formulation (utilizing Lemma 2.2 and Lemma 2.3) then becomes

αi +

yiyjαiαj(cid:104)ΨHφ(xi)  ΨHφ(xj)(cid:105)

(1)

−(cid:88)

i

(cid:88)

1
2
0 ≤ αi ≤ 1

i j

min

under the constraints(cid:80)
H = ΨHω∗ =(cid:80)

α

i αiyi = 0 

H = ΨHω∗ = ((cid:82)

N ∀i. The SVM separator is then given by
i yiαiΨHφ(xi) thereby existing in the GH-invariant (or equivalently G-invariant)
ω∗
subspace ΨH within H (since g → gH is a bijection). Effectively  the SVM observes samples from
XΨH = {x | φ(x) = ΨHφ(u)  ∀u ∈ XG} and therefore ω∗
H enjoys exact global invariance to G.
Further  ΨHω∗ is a maximum-margin separator of {φ(XG)} (i.e. the set of all transformed samples).
This can be shown by the following result.
Theorem 2.5. (Generalization) For a unitary group G and unitary kernel k(x  y) = (cid:104)φ(x)  φ(y)(cid:105) 
GH gH dgH) ω∗ is a perfect separator for {ΨHφ(X )} = {ΨHφ(x) | ∀x ∈ X} 
if ω∗
then ΨHω∗ is also a perfect separator for {φ(XG)} = {φ(x) | x ∈ XG} with the same margin.
Further  a max-margin separator of {ΨHφ(X )} is also a max-margin separator of {φ(XG)}.
The invariant non-linear SVM in objective 1  observes samples in the form of ΨHφ(x) and obtains a
max-margin separator ΨHω∗. This allows for the generalization properties of max-margin classiﬁers
to be combined with those of group invariant classiﬁers. While being invariant to nuisance transfor-
mations  max-margin classiﬁers can lead to highly discriminative features (more robust than DIKF
[10] as we ﬁnd in our experiments) that are invariant to within-class transformations.
Theorem 2.5 shows that the margins of φ(XG) and {ΨHφ(XG)} are deeply related and implies that
ΨHφ(x) is a max-margin separator for both datasets. Theoretically  the invariant non-linear SVM is
able to generalize to XG on just observing X and utilizing prior information in the form of G for all
unitary kernels k. This is true in practice for linear kernels. For non-linear kernels in practice  the
invariant SVM still needs to observe and integrate over transformed training inputs.
Leveraging unitary group properties. During test time to achieve invariance  the SVM would
require to observe and integrate over all possible transformations of the test sample. This is a huge
computational and design bottleneck. We would ideally want to achieve invariance and generalize
by observing just a single test sample  in effect perform one shot learning. This would not only
be computationally much cheaper but make the classiﬁer powerful owing to generalization to full
transformed orbits of test samples by observing just that single sample. This is where unitarity of g
helps and we leverage it in the form of the following Lemma.
G g dg for any unitary group G  then for any ﬁxed g(cid:48) ∈ G

Lemma 2.6. (Invariant Projection) If Ψ =(cid:82)

(including the identity element) we have (cid:104)Ψx(cid:48)  Ψω(cid:48)(cid:105) = (cid:104)g(cid:48)x(cid:48)  Ψω(cid:48)(cid:105) ∀ω  ω(cid:48) ∈ Rd
Assuming Ψω(cid:48) is the learned SVM classiﬁer  Lemma 2.6 shows that for any test x(cid:48)  the invariant dot
product (cid:104)Ψx(cid:48)  Ψω(cid:48)(cid:105) which involves observing all transformations of x(cid:48) is equivalent to the quantity
(cid:104)g(cid:48)x(cid:48)  Ψω(cid:48)(cid:105) which involves observing only one transformation of x(cid:48). Hence one can model the entire
orbit of x(cid:48) under G by a single sample g(cid:48)x(cid:48) where g(cid:48) ∈ G can be any particular transformation
including identity. This drastically reduces sample complexity and vastly increases generalization
capabilities of the classiﬁer since one only need to observe one test sample to achieve invariance
Lemma 2.6 also helps us in saving computation  allowing us to apply the computationally expensive
Ψ (group integration) operation only once on he classiﬁer and not the test sample. Thus  the kernel in
the Invariant SVM formulation can be replaced by the form kΨ(x  y) = (cid:104)φ(x)  ΨHφ(y)(cid:105).
For kernels in general  the GH-invariant subspace cannot be explicitly computed since it lies in the
G φ(gxi)dgH. It is important to

RKHS. It is only implicitly projected upon through ΨHφ(xi) =(cid:82)

5

(a) Invariant kernel feature extraction

(b) SVM feature extraction leading to MMIF fea-
tures

Figure 2: MMIF Feature Extraction. (a) l(x) denotes the invariant kernel feature of any x which is invariant
to the transformation G. Invariance is generated by group integration (or pooling). The invariant kernel feature
learns invariance form the unlabeled transformed template set TG. Also  the faces depicted are actual samples
from the large-scale mugshots data (∼ 153  000 images). (b) Once the invariant features have been extracted
for the labelled non-transformed dataset X   then the SVMs learned act as feature extractors. Each binary class
SVM (different color) was trained on the invariant kernel feature of a random subset of l(X ) with random class
assignments. The ﬁnal MMIF feature for x is the concatenation of all SVM inner-products with l(x).

(cid:104)φ(x) (cid:82)

note that during testing however  the SVM formulation will be invariant to transformations of the test
sample regardless of a linear or non-linear kernel.
Positive Semi-Deﬁniteness. The G-invariant kernel map is now of the form kΨ(x  y) =
G φ(gy)dgH(cid:105). This preserves the positive semi-deﬁnite property of the kernel k while
guaranteeing global invariance to unitary transformations.  unlike jittering kernels [17  3] and
tangent-distance kernels [5]. If we wish to include invariance to scaling however (in the sense of
scaling an image)  then we would lose positive-semi-deﬁniteness (it is also not a unitary transform).
Nonetheless  [20] show that conditionally positive deﬁnite kernels still exist for transformations
including scaling  although we focus of unitary transformations in this paper.

3 Max-Margin Invariant Features

G φ(gx)dgH (cid:82)

(cid:104)ΨHφ(x)  ΨHφ(y)(cid:105) = (cid:104)(cid:82)

The previous section utilized a group integration approach to arrive a theoretically invariant non-linear
SVM. It however does not address the Unlabeled Transformation problem i.e. the kernel kΨ(x  y) =
G φ(gy)dgH(cid:105) still requires observing transformed versions
of the labelled input sample namely {gx | gx ∈ XG} (or atleast one of the labelled samples if we
utilize Lemma 2.6). We now present our core approach called Max-Margin Invariant Features (MMIF)
that does not require the observation of any transformed labelled training sample whatsoever.
Assume that we have access to an unlabeled set of M templates T = {ti}i={1 ...M}. We assume that
we can observe all transformations under a unitary-group G  i.e. we have access to TG = {gti | ∀g ∈
G}i={1 ...M}. Also  assume we have access to a set X = {xj}i={1 ...D} of labelled data with N
classes which are not transformed. We can extract an M-dimensional invariant kernel feature for each
xj ∈ X as follows. Let the invariant kernel feature be l(x) ∈ RM to explicitly show the dependence
on x. Then the ith dimension of l for any particular x is computed as

(cid:90)

G

(cid:90)

G

l(x)i = (cid:104)φ(x)  ΨHφ(ti)(cid:105) = (cid:104)φ(x) 

gHφ(ti)dgH(cid:105) = (cid:104)φ(x) 

φ(gti)dgH(cid:105)

(2)

The ﬁrst equality utilizes Lemma 2.6 and the third equality uses Theorem 2.4. This is equivalent
to observing all transformations of x since (cid:104)φ(x)  ΨHφ(ti)(cid:105) = (cid:104)ΨHφ(x)  φ(ti)(cid:105) using Lemma 2.3.
Thereby we have constructed a feature l(x) which is invariant to G without ever needing to observe
transformed versions of the labelled vector x. We now brieﬂy the training of the MMIF feature
extractor. The matching metrics we use for this study is normalized cosine distance.

6

Class 1Class 2Class 3Class 4Test Image Kernel Invariant FeatureIntegration over the group(pooling)Test Imageof ωk =(cid:80)

each dimension k being computed as (cid:104)l(x(cid:48))  ωk(cid:105) for ωk = (cid:80)

Training MMIF SVMs. To learn a K-dimensional MMIF feature (potentially independent of
N)  we learn K independent binary-class linear SVMs. Each SVM trains on the labelled dataset
l(X ) = {l(xj) | j = {1  ...D}} with each sample being label +1 for some subset of the N classes
(potentially just one class) and the rest being labelled −1. This leads us to a classiﬁer in the form
j yjαjl(xj). Here  yj is the label of xj for the kth SVM. It is important to note that the
unlabeled data was only used to extract l(xj). Having multiple classes randomly labelled as positive
allows the SVM to extract some feature that is common between them. This increases generalization
by forcing the extracted feature to be more general (shared between multiple classes) rather than
being highly tuned to a single class. Any K-dimensional MMIF feature can be trained through this
technique leading to a higher dimensional feature vector useful in case where one has limited labelled
samples and classes (N is small). During feature extraction  the K inner products (scores) of the
test sample x(cid:48) with the K distinct binary-class SVMs provides the K-dimensional MMIF feature
vector. This feature vector is highly discriminative due to the max-margin nature of SVMs while
being invariant to G due to the invariant kernels.
MMIF. Given TG and X   the MMIF feature is deﬁned as MMIF(x(cid:48)) ∈ RK for any test x(cid:48) with
j yjαjl(xj) ∀xj ∈ X . Further 
l(x(cid:48)) ∈ RM ∀x with each dimension i being l(x(cid:48))i = (cid:104)φ(x(cid:48))  ΨHφ(ti)(cid:105). The process is illustrated in
Fig. 2.
Inheriting transformation invariance from transformed unlabeled data: A special case of semi-
supervised learning. MMIF features can learn to be invariant to transformations (G) by observing
them only through TG. It can then transfer the invariance knowledge to new unseen samples from
X thereby becoming invariant to XG despite never having observed any samples from XG. This
is a special case of semi-supervised learning where we leverage on the speciﬁc transformations
present in the unlabeled data. This is a very useful property of MMIFs allowing one to learn
transformation invariance from one source and sample points from another source while having
powerful discrimination and generalization properties. The property is can be formally stated as the
following Theorem.
Theorem 3.1. (MMIF is invariant to learnt transformations) MMIF(x(cid:48)) = MMIF(gx(cid:48)) ∀x(cid:48)∀g ∈ G
where G is observed only through TG = {gti | ∀g ∈ G}i={1 ...M}.

Thus we ﬁnd that MMIF can solve the Unlabeled Transformation Problem. MMIFs have an invariant
and a discriminative component. The invariant component of MMIF allows it to generalize to
new transformations of the test sample whereas the discriminative component allows for robust
classiﬁcation due to max-margin classiﬁers. These two properties allow MMIFs to be very useful as
we ﬁnd in our experiments on face recognition.
Max and Mean Pooling in MMIF. Group integration in practice directly results in mean pooling.
Recent work however  showed that group integration can be treated as a subset of I-theory where one
tries to measure moments (or a subset of) of the distribution (cid:104)x  gω(cid:105) g ∈ G since the distribution itself
is also an invariant [1]. Group integration can be seen as measuring the mean or the ﬁrst moment of
the distribution. One can also characterize using the inﬁnite moment or the max of the distribution.
We ﬁnd in our experiments that max pooling outperforms mean pooling in general. All results in this
paper however  still hold under the I-theory framework.
MMIF on external feature extractors (deep networks). MMIF does not make any assumptions
regarding its input and hence one can apply it to features extracted from any feature extractor in
general. The goal of any feature extractor is to (ideally) be invariant to within-class transformation
while maximizing between-class discrimination. However  most feature extractors are not trained
to explicitly factor out speciﬁc transformations. If we have access to even a small dataset with
the transformation we would like to be invariant to  we can transfer the invariance using MMIFs
(e.g. it is unlikely to observe all poses of a person in datasets  but pose is an important nuisance
transformation).
Modelling general non-unitary transformations. General non-linear transformations such as
out-of-plane rotation or pose variation are challenging to model. Nonetheless  a small variation
in these transformations can be approximated by some unitary G assuming piece wise linearity
through transformation-dependent sub-manifold unfolding [11]. Further  it was found that in practice 
integrating over general transformations produced approximate invariance [8].

7

(a) Invariant kernel feature extraction

(b) SVM feature extraction leading to MMIF fea-
tures

Figure 3: (a) Pose-invariant face recognition results on the semi-synthetic large-scale mugshot database (testing
on 114 750 images). Operating on pixels: MMIF (Pixels) outperforms invariance based methods DIKF [10]
and invariant NDP [8]. Operating on deep features: MMIF trained on VGG-Face features [12] (MMIF-VGG)
produces a signiﬁcant improvement in performance. The numbers in the brackets represent VR at 0.1% FAR.
(b) Face recognition results on LFW with raw VGG-Face features and MMIF trained on VGG-Face features.
The values in the bracket show VR at 0.1% FAR.

4 Experiments on Face Recognition

As illustration  we apply MMIFs using two modalities overall 1) on raw pixels and 2) on deep features
from the pre-trained VGG-Face network [12]. We provide more implementation details and results
discussion in the supplementary.
A. MMIF on a large-scale semi-synthetic mugshot database (Raw-pixels and deep features).
We utilize a large-scale semi-synthetic face dataset to generate the sets TG and X for MMIF. In this
dataset  only two major transformations exist  that of pose variation and subject variation. All other
transformations such as illumination  translation  rotation etc are strictly and synthetically controlled.
This provides a very good benchmark for face recognition. where we want to be invariant to pose
variation and be discriminative for subject variation. The experiment follows the exact protocol
and data as described in [10] 3 We test on 750 subjects identities with 153 pose varied real-textured
gray-scale image each (a total of 114 750 images) against each other resulting in about 13 billion
pair-wise comparisons (compared to 6 000 for the standard LFW protocol). Results are reported as
ROC curves along with VR at 0.1% FAR. Fig. 3(a) shows the ROC curves for this experiment. We
ﬁnd that MMIF features out-performs all baselines including VGG-Face features (pre-trained)  DIKF
and NDP approaches thereby demonstrating superior discriminability while being able to effectively
capture pose-invariance from the transformed template set TG. MMIF is able to solve the Unlabeled
Transformation problem by extracting transformation information from unlabeled TG.
B. MMIF on LFW (deep features): Unseen subject protocol. In order to be able to effectively
train under the scenario of general transformations and to challenge our algorithms  we deﬁne a new
much harder protocol on LFW. We choose the top 500 subjects with a total of 6 300 images for
training MMIF on VGG-Face features and test on the remaining subjects with 7 000 images. We
perform all versus all matching  totalling upto 49 million matches (4 orders more than the ofﬁcial
protocol). The evaluation metric is deﬁned to be the standard ROC curve with veriﬁcation rate
reported at 0.1% false accept rate. We split the 500 subjects into two sets of 250 and use as TG and
X . We do not use any alignment for this experiment  and the faces were cropped according to [16].
Fig. 3(b) shows the results of this experiment. We see that MMIF on VGG features signiﬁcantly
outperforms raw VGG on this protocol  boosting the VR at 0.1% FAR from 0.56 to 0.71. This
demonstrates that MMIF is able to generate invariance for highly non-linear transformations that are
not well-deﬁned rendering it useful in real-world scenarios where transformations are unknown but
observable.

3We provide more details in the supplementary. Also note that we do not need utilize identity information  all
that is required is the fact that a set of pose varied images belong to the same subject. Such data can be obtained
through temporal sampling.

8

False Accept Rate0.10.20.30.40.50.60.7Verification Rate0.50.550.60.650.70.750.80.850.90.951(cid:2)∞-DIKF(0.74)(cid:2)1-DIKF(0.61)NDP-(cid:2)∞(0.41)NDP-(cid:2)1(0.32)MMIF(Ours)(0.78)VGGFeatures(0.55)MMIF-VGG(Ours)(0.61)False Accept Rate10-810-710-610-510-410-310-210-1100Verification Rate00.10.20.30.40.50.60.70.80.91MMIFVGG(Ours)(0.71)VGG(0.56)References
[1] F. Anselmi  J. Z. Leibo  L. Rosasco  J. Mutch  A. Tacchetti  and T. Poggio. Magic materials: a theory of

deep hierarchical architectures for learning sensory representations. MIT  CBCL paper  2013.

[2] F. Anselmi  J. Z. Leibo  L. Rosasco  J. Mutch  A. Tacchetti  and T. Poggio. Unsupervised learning of

invariant representations in hierarchical architectures. CoRR  abs/1311.4158  2013.

[3] D. Decoste and B. Schölkopf. Training invariant support vector machines. Mach. Learn.  46(1-3):161–190 

Mar. 2002.

[4] B. Haasdonk and H. Burkhardt. Invariant kernel functions for pattern analysis and machine learning. In

Machine Learning  pages 35–61  2007.

[5] B. Haasdonk and D. Keysers. Tangent distance kernels for support vector machines. In Pattern Recognition 

2002. Proceedings. 16th International Conference on  volume 2  pages 864–868 vol.2  2002.

[6] G. E. Hinton. Learning translation invariant recognition in a massively parallel networks. In PARLE

Parallel Architectures and Languages Europe  pages 1–13. Springer  1987.

[7] J. Z. Leibo  Q. Liao  and T. Poggio. Subtasks of unconstrained face recognition. In International Joint

Conference on Computer Vision  Imaging and Computer Graphics  VISIGRAPP  2014.

[8] Q. Liao  J. Z. Leibo  and T. Poggio. Learning invariant representations and applications to face veriﬁcation.

Advances in Neural Information Processing Systems (NIPS)  2013.

[9] P. Niyogi  F. Girosi  and T. Poggio. Incorporating prior information in machine learning by creating virtual

examples. In Proceedings of the IEEE  pages 2196–2209  1998.

[10] D. K. Pal  F. Juefei-Xu  and M. Savvides. Discriminative invariant kernel features: a bells-and-whistles-free
approach to unsupervised face recognition and pose estimation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 5590–5599  2016.

[11] S. W. Park and M. Savvides. An extension of multifactor analysis for face recognition based on submanifold
learning. In Computer Vision and Pattern Recognition (CVPR)  2010 IEEE Conference on  pages 2645–
2652. IEEE  2010.

[12] O. M. Parkhi  A. Vedaldi  and A. Zisserman. Deep face recognition. 2015.
[13] T. Poggio and T. Vetter. Recognition and structure from one 2d model view: Observations on prototypes 

object classes and symmetries. Laboratory  Massachusetts Institute of Technology  1992.

[14] A. Raj  A. Kumar  Y. Mroueh  T. Fletcher  and B. Schölkopf. Local group invariant representations via orbit
embeddings. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017)  volume 54 of Proceedings of Machine Learning Research  pages 1225–1235  2017.

[15] M. Reisert. Group integration techniques in pattern analysis – a kernel view. PhD Thesis  2008.
[16] C. Sanderson and B. C. Lovell. Multi-region probabilistic histograms for robust and scalable identity

inference. In International Conference on Biometrics  pages 199–208. Springer  2009.

[17] B. Schölkopf and A. J. Smola. Learning with kernels: Support vector machines  regularization  optimiza-

tion  and beyond. MIT press  2002.

[18] B. Schölkopf  C. Burges  and V. Vapnik. Incorporating invariances in support vector learning machines.

pages 47–52. Springer  1996.

[19] B. Schölkopf  P. Simard  A. Smola  and V. Vapnik. Prior knowledge in support vector kernels. Advances in

Neural Information Processing Systems (NIPS)  1998.

[20] C. Walder and O. Chapelle. Learning with transformation invariant kernels. In Advances in Neural

Information Processing Systems  pages 1561–1568  2007.

[21] X. Zhang  W. S. Lee  and Y. W. Teh. Learning with invariance via linear functionals on reproducing kernel

hilbert space. In Advances in Neural Information Processing Systems  pages 2031–2039  2013.

9

,Raman Arora
Andy Cotter
Nati Srebro
Dipan Pal
Ashwin Kannan
Gautam Arakalgud
Marios Savvides
Marylou Gabrié
Andre Manoel
Clément Luneau
jean barbier
Nicolas Macris
Florent Krzakala
Lenka Zdeborová
Boyi Li
Felix Wu
Kilian Weinberger
Serge Belongie