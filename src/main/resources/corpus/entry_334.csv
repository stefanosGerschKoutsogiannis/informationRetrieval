2016,Accelerating Stochastic Composition Optimization,Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method  namely the accelerated stochastic compositional proximal gradient (ASC-PG) method  which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms  and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning  and conduct numerical experiments.,Accelerating Stochastic Composition Optimization

Mengdi Wang⇤  Ji Liu⇤  and Ethan X. Fang

Princeton University  University of Rochester  Pennsylvania State University
mengdiw@princeton.edu  ji.liu.uwisc@gmail.com  xxf13@psu.edu

Abstract

Consider the stochastic composition optimization problem where the objective is a
composition of two expected-value functions. We propose a new stochastic ﬁrst-
order method  namely the accelerated stochastic compositional proximal gradient
(ASC-PG) method  which updates based on queries to the sampling oracle using
two different timescales. The ASC-PG is the ﬁrst proximal gradient method for
the stochastic composition problem that can deal with nonsmooth regularization
penalty. We show that the ASC-PG exhibits faster convergence than the best known
algorithms  and that it achieves the optimal sample-error complexity in several
important special cases. We further demonstrate the application of ASC-PG to
reinforcement learning and conduct numerical experiments.

1

Introduction

The popular stochastic gradient methods are well suited for minimizing expected-value objective
functions or the sum of a large number of loss functions. Stochastic gradient methods ﬁnd wide
applications in estimation  online learning  and training of deep neural networks. Despite their
popularity  they do not apply to the minimization of a nonlinear function involving expected values or
a composition between two expected-value functions.
In this paper  we consider the stochastic composition problem  given by
+R(x)

H(x) := Ev(fv(Ew(gw(x))))

(1)

min
x2<n

where (f  g)(x) = f (g(x)) denotes the function composition  gw(·) : <n 7! <m and
fv(·) : <m 7! < are continuously differentiable functions  v  w are random variables  and
R(x) : <n 7! <[{+1} is an extended real-valued closed convex function. We assume throughout
that there exists at least one optimal solution x⇤ to problem (1). We focus on the case where fv and
gw are smooth  but we allow R to be a nonsmooth penalty such as the `1-norm. We do no require
either the outer function fv or the inner function gw to be convex or monotone. As a result  the
composition problem cannot be reformulated into a saddle point problem in general.
Our algorithmic objective is to develop efﬁcient algorithms for solving problem (1) based on random
evaluations of fv  gw and their gradients. Our theoretical objective is to analyze the rate of conver-
gence for the stochastic algorithm and to improve it when possible. In the online setting  the iteration
complexity of our stochastic methods can be interpreted as a sample-error complexity upper bound
for estimating the optimal solution of problem (1).

|

=:F (x)

{z

}

1.1 Motivating Examples
One motivating example is reinforcement learning [Sutton and Barto  1998]. Consider a controllable
Markov chain with states 1  . . .   S. Estimating the value-per-state of a ﬁxed control policy ⇡ is known

⇤Equal contribution.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

as on-policy learning. It can be casted into an S ⇥ S system of Bellman equations:

P ⇡V ⇡ + r⇡ = V ⇡ 
s˜s is the transition probability from state s to state ˜s  and r⇡

s is
where  2 (0  1) is a discount factor  P ⇡
the expected state transition reward at state s. The solution V ⇡ to the Bellman equation is the value
vector  with V ⇡(s) being the total expected reward starting at state s. In the blackbox simulation
environment  P ⇡  r⇡ are unknown but can be sampled from a simulator. As a result  solving the
Bellman equation becomes a special case of the stochastic composition optimization problem:

min
x2<S

kE[A]x  E[b]k2 

(2)

where A  b are random matrices and random vectors such that E[A] = I  P ⇡ and E[b] = r⇡. It
can be viewed as the composition of the square norm function and the expected linear function. We
will give more details on the reinforcement learning application in Section 4.
Another motivating example is risk-averse learning. For example  consider the mean-variance
minimization problem

min

x

Ea b[h(x; a  b)] + Vara b[h(x; a  b)] 

where h(x; a  b) is some loss function parameterized by random variables a and b  and > 0 is a
regularization parameter. Its batch version takes the form

min

x

1
N

h(x; ai  bi) +


N

NXi=1

NXi=1 h(x; ai  bi) 

1
N

NXi=1

h(x; ai  bi)!2

.

Here the variance term is the composition of the mean square function and an expected loss function.
Although the stochastic composition problem (1) was barely studied  it actually ﬁnds a broad spectrum
of emerging applications in estimation and machine learning (see Wang et al. [2016] for a list of
applications). Fast optimization algorithms with theoretical guarantees will lead to new computation
tools and online learning methods for a broader problem class  no longer limited to the expectation
minimization problem.

1.2 Related Works and Contributions
Contrary to the expectation minimization problem  “unbiased" gradient samples are no longer
available for the stochastic composition problem (1). The objective is nonlinear in the joint probability
distribution of (w  v)  which substantially complicates the problem. In a recent work by Dentcheva
et al. [2015]  a special case of the stochastic composition problem  i.e.  risk-averse optimization 
has been studied. A central limit theorem has been established  showing that the K-sample batch
problem converges to the true problem at the rate of O(1/pK) in a proper sense. For the case
where R(x) = 0  Wang et al. [2016] has proposed and analyzed a class of stochastic compositional
gradient/subgradient methods (SCGD). The SCGD involves two iterations of different time scales 
one for estimating x⇤ by a stochastic quasi-gradient iteration  the other for maintaining a running
estimate of g(x⇤). Wang and Liu [2016] studies the SCGD in the setting where samples are corrupted
with Markov noises (instead of i.i.d. zero-mean noises). Both works establish almost sure convergence
of the algorithm and several convergence rate results  which are the best-known convergence rate
prior to the current paper.
The idea of using two-timescale quasi-gradient traced back to the earlier work Ermoliev [1976]. The
incremental treatment of proximal gradient iteration has been studied extensively for the expectation
minimization problem  see for examples Beck and Teboulle [2009]  Bertsekas [2011]  Ghadimi and
Lan [2015]  Gurbuzbalaban et al. [2015]  Nedi´c [2011]  Nedi´c and Bertsekas [2001]  Nemirovski
et al. [2009]  Rakhlin et al. [2012]  Shamir and Zhang [2013]  Wang and Bertsekas [2016]  Wang et al.
[2015]. However  except for Wang et al. [2016] and Wang and Liu [2016]  all of these works focus
on variants of the expectation minimization problem and do not apply to the stochastic composition
problem (1). Another work partially related to this paper is by Dai et al. [2016]. They consider a
special case of problem (1) arising in kernel estimation  where they assume that all functions fv’s are
convex and their conjugate functions f ?
v ’s can be easily obtained/sampled. Under these additional
assumptions  they essentially rewrite the problem into a saddle point optimization involving functional
variables.

2

In this paper  we propose a new accelerated stochastic compositional proximal gradient (ASC-PG)
method that applies to the penalized problem (1)  which is a more general problem than the one
considered in Wang et al. [2016]. We use a coupled martingale stochastic analysis to show that
ASC-PG achieves signiﬁcantly better sample-error complexity in various cases. We also show that
ASC-PG exhibits optimal sample-error complexity in two important special cases: the case where the
outer function is linear and the case where the inner function is linear.
Our contributions are summarized as follows:
1. We propose the ﬁrst stochastic proximal-gradient method for the stochastic composition problem.
This is the ﬁrst algorithm that is able to address the nonsmooth regularization penalty R(·) without
deteriorating the convergence rate.
2. We obtain a convergence rate O(K4/9) for smooth optimization problems that are not necessarily
convex  where K is the number of queries to the stochastic ﬁrst-order oracle. This improves the best
known convergence rate and provides a new benchmark for the stochastic composition problem.
3. We provide a comprehensive analysis and results that apply to various special cases. In particular 
our results contain as special cases the known optimal rate results for the expectation minimization
problem  i.e.  O(1/pK) for general objectives and O(1/K) for strongly convex objectives.
4. In the special case where the inner function g(·) is a linear mapping  we show that it is sufﬁcient
to use one timescale to guarantee convergence. Our result achieves the non-improvable rate of
convergence O(1/K) for optimal strongly convex optimization and O(1/pK) for nonconvex
smooth optimization. It implies that the inner linearity does not bring fundamental difﬁculty to the
stochastic composition problem.
5. We show that the proposed method leads to a new on-policy reinforcement learning algorithm.
The new learning algorithm achieves the optimal convergence rate O(1/pK) for solving Bellman
equations (or O(1/K) for solving the least square problem) based on K observations of state-to-
state transitions.

In comparison with Wang et al. [2016]  our analysis is more succinct and leads to stronger results.
To the best of our knowledge  Theorems 1 and 2 in this paper provide the best-known rates for the
stochastic composition problem.

Paper Organization. Section 2 states the sampling oracle and the accelerated stochastic composi-
tional proximal gradient algorithm (ASC-PG). Section 3 states the convergence rate results in the case
of general nonconvex objective and in the case of strongly convex objective  respectively. Section 4
describes an application of ASC-PG to reinforcement learning and gives numerical experiments.
Notations and Deﬁnitions. For x 2 <n  we denote by x0 its transpose  and by kxk its Euclidean
norm (i.e.  kxk= px0x). For two sequences {yk} and {zk}  we write yk = O(zk) if there exists
a constant c > 0 such that kykk ckzkk for each k. We denote by Ivalue
condition the indicator function 
which returns “value” if the “condition” is satisﬁed; otherwise 0. We denote by H⇤ the optimal
objective function value of problem (1)  denote by X⇤ the set of optimal solutions  and denote by
PS(x) the Euclidean projection of x onto S for any convex set S. We also denote by short that
f (y) = Ev[fv(y)] and g(x) = Ew[gw(x)].

2 Algorithm

We focus on the black-box sampling environment. Suppose that we have access to a stochastic
ﬁrst-order oracle  which returns random realizations of ﬁrst-order information upon queries. This
is a typical simulation oracle that is available in both online and batch learning. More speciﬁcally 
assume that we are given a Sampling Oracle (SO) such that
• Given some x 2 <n  the SO returns a random vector gw(x) and a noisy subgradient rgw(x).
• Given some y 2 <m  the SO returns a noisy gradient rfv(y).
Now we propose the Accelerated Stochastic Compositional Proximal Gradient (ASC-PG) algorithm 
see Algorithm 1. ASC-PG is a generalization of the SCGD proposed by Wang et al. [2016]  in which
a proximal step is used to replace the projection step.
In Algorithm 1  the extrapolation-smoothing scheme (i.e.  the (y  z)-step) is critical to the acceleration
of convergence. The acceleration is due to the fast running estimation of the unknown quantity

3

Algorithm 1 Accelerated Stochastic Compositional Proximal Gradient (ASC-PG)
Require: x1 2 <n  y0 2 <m  SO  K  stepsize sequences {↵k}K
k=1.
k=1  and {k}K
Ensure: {xk}K
1: Initialize z1 = x1.
2: for k = 1 ···   K do
3:
4:

Query the SO and obtain gradient samples rfvk (yk)  rgwk (zk).
Update the main iterate by

k=1

xk+1 = prox↵kR(·)xk  ↵krg>wk (xk)rfvk (yk) .

5:

Update auxillary iterates by an extrapolation-smoothing scheme:

zk+1 = ✓1 

1

k◆ xk +

1
k

xk+1 

yk+1 = (1  k)yk + kgwk+1(zk+1) 

where the sample gwk+1(zk+1) is obtained via querying the SO.

6: end for

g(xk) := Ew[gw(xk)]. At iteration k  the running estimate yk of g(xk) is obtained using a weighted
smoothing scheme  corresponding to the y-step; while the new query point zk+1 is obtained through
extrapolation  corresponding to the z-step. The updates are constructed in a way such that yk is a
nearly unbiased estimate of g(xk). To see how the extrapolation-smoothing scheme works  we let the
weights be

⇠(k)

t =(tQk

k 

i=t+1(1  i) 

if k > t  0
if k = t  0.

(3)

Then we can verify the following important relations:

xk+1 =

kXt=0

⇠(k)
t zt+1 

yk+1 =

⇠(k)
t gwt+1(zt+1) 

kXt=0

which essentially say that xk+1 is a damped weighted average of {zt+1}k+1
weighted average of {gwt+1(zt+1)}k+1
An Analytical Example of the Extrapolation-Smooth Scheme Now consider the special case
where gw(·) is always a linear mapping gw(z) = Awz + bz and k = 1/(k + 1). We can verify that
⇠(k)
t = 1/(k + 1) for all t. Then we have

and yk+1 is a damped

.

0

0

g(xk+1) =

1

k + 1

kXt=0

In this way  we can see that the scaled error

E[Aw]zt+1 +E[bw] 

yk+1 =

1

k + 1

kXt=0

Awt+1zt+1 +

1

k + 1

kXt=0

bwt+1.

k(yk+1  g(xk+1)) =

(bwt+1  E[bw])

is a zero-mean and zero-drift martingale. Under additional technical assumptions  we have

(Awt+1  E[Aw])zt+1 +

kXt=0
kXt=0
E[kyk+1  g(xk+1)k2]  O (1/k) .

Note that the zero-drift property of the error martingale is the key to the fast convergence rate. The
zero-drift property comes from the near-unbiasedness of yk  which is due to the special construction
of the extrapolation-smoothing scheme. In the more general case where gw is not necessarily linear 
we can use a similar argument to show that yk is a nearly unbiased estimate of g(xk). As a result  the
extrapolation-smoothing (y  z)-step ensures that yk tracks the unknown quantity g(xk) efﬁciently.

4

3 Main Results

We present our main theoretical results in this section. Let us begin by stating our assumptions. Note
that all assumptions involving random realizations of v  w hold with probability 1.
Assumption 1. The samples generated by the SO are unbiased in the following sense:
8x 8y.

1. E{wk vk}(rg>wk (x)rfvk (y)) = rg>(x)rf (y) 8k = 1  2 ···   K 
2. Ewk (gwk (x)) = g(x) 8x.

Note that wk and vk are not necessarily independent.
Assumption 2. The sample gradients and values generated by the SO satisfy

Assumption 3. The sample gradients generated by the SO are uniformly bounded  and the penalty
function R has bounded gradients.

Ew(kgw(x)  g(x)k2)  2 8x.

krgw(x)k ⇥(1) 
Assumption 4. There exist LF   Lf   Lg > 0 such that

krfv(x)k ⇥(1) 

k@R(x)k ⇥(1) 8x 8w 8v

1. F (z)  F (x)  hrF (x)  z  xi + LF
2. krfv(y)  rfv(w)k Lfky  wk 8y 8w 8v.
3. kg(x)  g(z)  rg(z)>(x  z)k Lg

2 kz  xk2 8x 8z.

2 kx  zk2 8x 8z.

Our ﬁrst main result concerns with general optimization problems which are not necessarily convex.
Theorem 1 (Smooth (Nonconvex) Optimization). Let Assumptions 1  2  3  and 4 hold. Denote
by F (x) := (Ev(fv)  Ew(gw))(x) for short and suppose that R(x) = 0 in (1) and E(F (xk))
is bounded from above. Choose ↵k = ka and k = 2kb where a 2 (0  1) and b 2 (0  1) in
Algorithm 1. Then we have

PK
k=1 E(krF (xk)k2)

K

 O(Ka1 + L2

f LgK4b4aIlog K

4a4b=1 + L2

f Kb + Ka).

If Lg 6= 0 and Lf 6= 0  choose a = 5/9 and b = 4/9  yielding

(4)

(5)

(6)

If Lg = 0 or Lf = 0  choose a = b = 1/2  yielding

1
K

1
K

KXk=1
KXk=1

E(krF (xk)k2)  O(K4/9).

E(krF (xk)k2)  O(K1/2).

The result of Theorem 1 strictly improves the best-known results given by Wang et al. [2016]. First
the result of (5) improves the ﬁnite-sample error bound from O(k2/7) to O(k4/9) for general
convex and nonconvex optimization. This improves the best known convergence rate and provides a
new benchmark for the stochastic composition problem. Note that it is possible to relax the condition
“E(F (xk)) is bounded from above" in Theorem 1. However  it would make the analysis more
cumbersome and yield an additional term log K in the error bound.
Our second main result concerns strongly convex objective functions. We say that the objective
function H is optimally strongly convex with parameter > 0 if

H(x)  H(PX⇤(x))  kx P X⇤(x)k2 8x.

(7)
(see Liu and Wright [2015]). Note that any strongly convex function is optimally strongly convex  but
the reverse does not hold. For example  the objective function (2) in on-policy reinforcement learning
is always optimally strongly convex (even if E(A) is a rank deﬁcient matrix)  but not necessarily
strongly convex.

5

Theorem 2. (Strongly Convex Optimization) Suppose that the objective function H(x) in (1) is
optimally strongly convex with parameter > 0 deﬁned in (7). Set ↵k = Caka and k = Cbkb
where Ca > 4  Cb > 2  a 2 (0  1]  and b 2 (0  1] in Algorithm 1. Under Assumptions 1  2  3  and 4 
we have
(8)

E(kxK P X⇤(xK)k2)  OKa + L2
If Lg 6= 0 and Lf 6= 0  choose a = 1 and b = 4/5  yielding

f LgK4a+4b + L2

f Kb .

If Lg = 0 or Lf = 0  choose a = 1 and b = 1  yielding

E(kxK P X⇤(xK)k2)  O(K4/5).

(9)

E(kxK P X⇤(xK)k2)  O(K1).

(10)
Let us discuss the results of Theorem 2. In the general case where Lf 6= 0 and Lg 6= 0  the
convergence rate in (9) is consistent with the result of Wang et al. [2016]. Now consider the special
case where Lg = 0  i.e.  the inner mapping is linear. This result ﬁnds an immediate application to
Bellman error minimization problem (2) which arises from reinforcement learning problem in (and
with `1 norm regularization). The proposed ASC-PG algorithm is able to achieve the optimal rate
O(1/K) without any extra assumption on the outer function fv. To the best of our knowledge  this is
the best (also optimal) sample-error complexity for on-policy reinforcement learning.
Remarks Theorems 1 and 2 give important implications about the special cases where Lf = 0
or Lg = 0. In these cases  we argue that our convergence rate (10) is “optimal" with respect to the
sample size K. To see this  it is worth pointing out the O(1/K) rate of convergence is optimal for
strongly convex expectation minimization problem. Because the expectation minimization problem
is a special case of the problem (1)  the O(1/K) convergence rate must be optimal for the stochastic
composition problem too.
• Consider the case where Lf = 0  which means that the outer function fv(·) is linear with
probability 1. Then the stochastic composition problem (1) reduces to an expectation minimization
problem since (EvfvEwgw)(x) = Ev(fv(Ewgw(x))) = EvEw(fvgw)(x). Therefore  it makes
a perfect sense to obtain the optimal convergence rate.
• Consider the case where Lg = 0  which means that the inner function g(·) is a linear mapping.
The result is quite surprising. Note that even g(·) is a linear mapping  it does not reduce problem
(1) to an expectation minimization problem. However  the ASC-PG still achieves the optimal
convergence rate. This suggests that  when inner linearity holds  the stochastic composition
problem (1) is not fundamentally more difﬁcult than the expectation minimization problem.

The convergence rate results unveiled in Theorems 1 and 2 are the best known results for the
composition problem. We believe that they provide important new result which provides insights into
the complexity of the stochastic composition problem.

4 Application to Reinforcement Learning

In this section  we apply the proposed ASC-PG algorithm to conduct policy value evaluation in
reinforcement learning through attacking Bellman equations. Suppose that there are in total S states.
Let the policy of interest be ⇡. Denote the value function of states by V ⇡ 2 <S  where V ⇡(s) denotes
the value of being at state s under policy ⇡. The Bellman equation of the problem is
V ⇡(s1) = E⇡{rs1 s2 +  · V ⇡(s2)|s1} for all s1  s2 2{ 1  ...  S} 

where rs1 s2 denotes the reward of moving from state s1 to s2  and the expectation is taken over all
possible future state s2 conditioned on current state s1 and the policy ⇡. We have that the solution
V ⇤ 2 <S to the above equation satisﬁes that V ⇤ = V ⇡. Here a moderately large S will make solving
the Bellman equation directly impractical. To resolve the curse of dimensionality  in many practical
applications  we approximate the value of each state by some linear map of its feature s 2 <m 
where d < S to reduce the dimension. In particular  we assume that V ⇡(s) ⇡ T
s w⇤ for some
w⇤ 2 <m.
To compute w⇤  we formulate the problem as a Bellman residual minimization problem that

min
w

SXs=1

(T

s w  q⇡ s0(w))2 

6

8

6

4

2

∥
∗

w
−

k

w
∥

0

0

2

1.5

1

0.5

0

)
∥
∗

w
−

k

w
∥
(
g
o
l

ASC-PG
SCGD
GTD2-MP

12

∥
∗

w
X
−

k

w
Φ
∥

9

6

3

0

0

2.5

2

1.5

1

0.5

)
∥
∗

w
−

k

w
Φ
∥
(
g
o
l

0
7.5

ASC-PG
SCGD
GTD2-MP

3

6

k

9
4

×10

ASC-PG
SCGD
GTD2-MP

8
log(k)

8.5

3

6

k

9
4

×10

ASC-PG
SCGD
GTD2-MP

-0.5

7.5

8
log(k)

8.5

Figure 1: Empirical convergence rate of the ASC-PG algorithm and the GTD2-MP algorithm under
Experiment 1 averaged over 100 runs  where wk denotes the solution at the k-th iteration.

where q⇡ s0(w) = E⇡{rs s0 +  · s0w} =Ps0 P ⇡

ss0({rs s0 +  · s0w); < 1 is a discount factor 
and rs s0 is the random reward of transition from state s to state s0. It is clearly seen that the proposed
ASC-PG algorithm could be directly applied to solve this problem where we take

g(w) = (T

1 w  q⇡ 1(w)  ...  T

f⇣(T

1 w  q⇡ 1(w)  ...  T

S w  q⇡ S (w))⌘ =

S w  q⇡ S (w)) 2 <2S 
SXs=1

(sw  q⇡ s0(w))2 2 <.

We point out that the g(·) function here is a linear map. By our theoretical analysis  we expect to
achieve a faster O(1/k) rate  which is justiﬁed empirically in our later simulation study.
We consider three experiments  where in the ﬁrst two experiments  we compare our proposed
accelerated ASC-PG algorithm with SCGD algorithm [Wang et al.  2016] and the recently proposed
GTD2-MP algorithm [Liu et al.  2015]. Also  in the ﬁrst two experiments  we do not add any
regularization term  i.e. R(·) = 0. In the third experiment  we add an `1-penalization term kwk1.
In all cases  we choose the step sizes via comparison studies as in Dann et al. [2014]:
• Experiment 1: We use the Baird’s example [Baird et al.  1995]  which is a well-known example to
test the off-policy convergent algorithms. This example contains S = 6 states  and two actions at
each state. We refer the readers to Baird et al. [1995] for more detailed information of the example.
• Experiment 2: We generate a Markov decision problem (MDP) using similar setup as in White and
White [2016]. In each instance  we randomly generate an MDP which contains S = 100 states 
and three actions at each state. The dimension of the Given one state and one action  the agent can
move to one of four next possible states. In our simulation  we generate the transition probabilities
for each MDP instance uniformly from [0  1] and normalize the sum of transitions to one  and we
generate the reward for each transition also uniformly in [0  1].
• Experiment 3: We generate the data same as Experiment 2 except that we have a larger d = 100
dimensional feature space  where only the ﬁrst 4 components of w⇤ are non-zeros. We add an
`1-regularization term  kwk1  to the objective function.

Denote by wk the solution at the k-th iteration. For the ﬁrst two experiments  we report the empirical
convergence performance kwk  w⇤k and kwk  w⇤k  where  = (1  ...  S)T 2 <S⇥d and
w⇤ = V   and all wk’s are averaged over 100 runs  in the ﬁrst two subﬁgures of Figures 1 and 2. It is
seen that the ASC-PG algorithm achieves the fastest convergence rate empirically in both experiments.
To further evaluate our theoretical results  we plot log(t) vs. log(kwk  w⇤k) (or log(kwk  ⇤k)
averaged over 100 runs for the ﬁrst two experiments in the second two subﬁgures of Figures 1 and

7

6

4

∥
∗

w
−

k

w
∥

2

0

0

2

1

0

-1

)
∥
∗

w
−

k

w
∥
(
g
o
l

ASC-PG
SCGD
GTD2-MP

50

40

∥
∗

w
Φ
−

k

w
Φ
∥

30

20

10

3

6

k

9
4

×10

0

0

2

ASC-PG
SCGD
GTD2-MP

4

)
∥
∗

w
Φ
−

k

w
Φ
∥
(
g
o
l

3

2

1

ASC-PG
SCGD
GTD2-MP

6

4

k

8

4

×10

ASC-PG
SCGD
GTD2-MP

-2

7.5

8

8.5

9
log(k)

9.5

10

0
7.5

8

8.5

9
log(k)

9.5

10

Figure 2: Empirical convergence rate of the ASC-PG algorithm and the GTD2-MP algorithm under
Experiment 2 averaged over 100 runs  where wk denotes the solution at the k-th iteration.

2. The empirical results further support our theoretical analysis that kwk  w⇤k2= O(1/k) for the
ASC-PG algorithm when g(·) is a linear mapping.
For Experiment 3  as the optimal solution is unknown  we run the ASC-PG algorithm for one million
iterations and take the corresponding solution as the optimal solution ˆw⇤  and we report kwk  ˆw⇤k
and kwk  ˆw⇤k averaged over 100 runs in Figure 3. It is seen the the ASC-PG algorithm achieves
fast empirical convergence rate.

lambda = 1e-3
lambda = 5e-4

5

4

3

2

1

∥
∗

ˆw
−

t

w
∥

0

0

2

6

4

k

8

4

×10

25

20

∥
∗

ˆw
Φ
−

k

w
Φ
∥

15

10

5

0

0

lambda = 1e-3
lambda = 5e-4

2

6

4

t

8

4

×10

Figure 3: Empirical convergence rate of the ASC-PG algorithm with the `1-regularization term kwk1
under Experiment 3 averaged over 100 runs  where wk denotes the solution at the t-th iteration.
5 Conclusion
We develop a proximal gradient method for the penalized stochastic composition problem. The
algorithm updates by interacting with a stochastic ﬁrst-order oracle. Convergence rates are established
under a variety of assumptions  which provide new rate benchmarks. Application of the ASC-
PG to reinforcement learning leads to a new on-policy learning algorithm  which achieves faster
convergence than the best known algorithms. For future research  it remains open whether or under
what circumstances the current O(K4/9) can be further improved. Another direction is to customize
and adapt the algorithm and analysis to more speciﬁc problems arising from reinforcement learning
and risk-averse optimization  in order to fully exploit the potential of the proposed method.
Acknowledgments
This project is in part supported by NSF grants CNS-1548078 and DMS-10009141.

8

References
L. Baird et al. Residual algorithms: Reinforcement learning with function approximation.
Proceedings of the twelfth international conference on machine learning  pages 30–37  1995.

In

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences  2(1):183–202  2009.

D. P. Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical

Programming  Ser. B  129:163–195  2011.

B. Dai  N. He  Y. Pan  B. Boots  and L. Song. Learning from conditional distributions via dual kernel

embeddings. arXiv preprint arXiv:1607.04579  2016.

C. Dann  G. Neumann  and J. Peters. Policy evaluation with temporal differences: A survey and

comparison. The Journal of Machine Learning Research  15(1):809–883  2014.

D. Dentcheva  S. Penev  and A. Ruszczynski. Statistical estimation of composite risk functionals and

risk optimization problems. arXiv preprint arXiv:1504.02658  2015.

Y. M. Ermoliev. Methods of Stochastic Programming. Monographs in Optimization and OR  Nauka 

Moscow  1976.

S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic

programming. Mathematical Programming  pages 1–41  2015.

M. Gurbuzbalaban  A. Ozdaglar  and P. Parrilo. On the convergence rate of incremental aggregated

gradient algorithms. arXiv preprint arXiv:1506.02081  2015.

B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of proximal
gradient td algorithms. In Proc. The 31st Conf. Uncertainty in Artiﬁcial Intelligence  Amsterdam 
Netherlands  2015.

J. Liu and S. J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence

properties. SIAM Journal on Optimization  25(1):351–376  2015.

A. Nedi´c. Random algorithms for convex minimization problems. Mathematical Programming  Ser.

B  129:225–253  2011.

A. Nedi´c and D. P. Bertsekas. Incremental subgradient methods for nondifferentiable optimization.

SIAM Journal on Optimization  12:109–138  2001.

A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19:1574–1609  2009.

A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex
stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning 
pages 449–456  2012.

O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
results and optimal averaging schemes. In Proceedings of The 30th International Conference on
Machine Learning  pages 71–79  2013.

R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press  1998.
M. Wang and D. P. Bertsekas. Stochastic ﬁrst-order methods with random constraint projection.

SIAM Journal on Optimization  26(1):681–717  2016.

M. Wang and J. Liu. A stochastic compositional subgradient method using Markov samples. Pro-

ceedings of Winter Simulation Conference  2016.

M. Wang  Y. Chen  J. Liu  and Y. Gu. Random multi-constraint projection: Stochastic gradient
methods for convex optimization with many constraints. arXiv preprint arXiv:1511.03760  2015.
M. Wang  X. Fang  and H. Liu. Stochastic compositional gradient descent: Algorithms for minimizing

compositions of expected-value functions. Mathematical Programming Series A  2016.

A. White and M. White. Investigating practical  linear temporal difference learning. arXiv preprint

arXiv:1602.08771  2016.

9

,Dominik Rothenhäusler
Christina Heinze
Jonas Peters
Nicolai Meinshausen
Mengdi Wang
Ji Liu
Ethan Fang
Shikun Liu
Andrew Davison
Edward Johns