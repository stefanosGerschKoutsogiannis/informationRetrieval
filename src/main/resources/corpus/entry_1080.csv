2019,Conditional Independence Testing using Generative Adversarial Networks,We consider the hypothesis testing problem of detecting conditional dependence  with a focus on high-dimensional feature spaces. Our contribution is a new test statistic based on samples from a generative adversarial network designed to approximate directly a conditional distribution that encodes the null hypothesis  in a manner that maximizes power (the rate of true negatives). We show that such an approach requires only that density approximation be viable in order to ensure that we control type I error (the rate of false positives); in particular  no assumptions need to be made on the form of the distributions or feature dependencies. Using synthetic simulations with high-dimensional data we demonstrate significant gains in power over competing methods. In addition  we illustrate the use of our test to discover causal markers of disease in genetic data.,Conditional Independence Testing using Generative

Adversarial Networks

1University of Cambridge  2The Alan Turing Institute  3University of California Los Angeles

Alexis Bellot1 2 Mihaela van der Schaar1 2 3

[abellot mschaar]@turing.ac.uk

Abstract

We consider the hypothesis testing problem of detecting conditional dependence 
with a focus on high-dimensional feature spaces. Our contribution is a new test
statistic based on samples from a generative adversarial network designed to
approximate directly a conditional distribution that encodes the null hypothesis  in
a manner that maximizes power (the rate of true negatives). We show that such an
approach requires only that density approximation be viable in order to ensure that
we control type I error (the rate of false positives); in particular  no assumptions
need to be made on the form of the distributions or feature dependencies. Using
synthetic simulations with high-dimensional data we demonstrate signiﬁcant gains
in power over competing methods. In addition  we illustrate the use of our test to
discover causal markers of disease in genetic data.

1

Introduction

Conditional independence tests are concerned with the question of whether two variables X and Y
behave independently of each other  after accounting for the effect of confounders Z. Such questions
can be written as a hypothesis testing problem:

H0 : X|=Y |Z versus H1 : X (cid:54)⊥⊥ Y |Z

Tests for this problem have recently become increasingly popular in the Machine Learning literature
[19  24  18  17  6] and ﬁnd natural applications in causal discovery studies in all areas of science
[12  14]. An area of research where such tests are important is genetics  where one problem is to ﬁnd
genomic mutations directly linked to disease for the design of personalized therapies [26  11]. In this
case  researchers have a limited number of data samples to test relationships even though they expect
complex dependencies between variables and often high-dimensional confounding variables Z. In
settings like this  existing tests may be ineffective because the accumulation of spurious correlations
from a large number of variables makes it difﬁcult to discriminate between the hypotheses. As an
example the work in [16] shows empirically that kernel-based tests have rapidly decreasing power
with increasing data dimensionality.
In this paper  we present a test for conditional independence that relies on a different set of assumptions
that we show to be more robust for testing in high-dimensional samples (X  Y  Z). In particular  we
show that given only a viable approximation to a conditional distribution one can derive conditional
independence tests that are approximately valid in ﬁnite samples and that have non-trivial power. Our
test is based on a modiﬁcation of Generative Adversarial Networks (GANs) [8] that simulates from
a distribution under the assumption of conditional independence  while maintaining good power in
high dimensional data. In our procedure  after training  the ﬁrst step involves simulating from our
network to generate data sets consistent with H0. We then deﬁne a test statistic to capture the X − Y
dependency in each sample and compute an empirical distribution which approximates the behaviour
of the statistic under H0 and can be directly compared to the statistic observed on the real data to
make a decision.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The paper is outlined as follows. In section 2  we provide an overview of conditional hypothesis testing
and related work. In section 3  we provide details of our test and give our main theoretical results.
Sections 4 and 5 provide experiments on synthetic and real data respectively  before concluding in
section 6.

2 Background

We start by introducing our notation and deﬁne central notions of hypothesis testing. Throughout 
we will assume the observed data consists of n i.i.d tuples (Xi  Yi  Zi)  deﬁned in a potentially
high-dimensional space X × Y × Z  typically Rdx × Rdy × Rdz. Conditional independence tests
statistics T : X ×Y×Z → R summarize the evidence in the observational data against the hypothesis
H0 : X|=Y |Z in a real-valued scalar. Its value from observed data  compared to a deﬁned threshold
then determines a decision of whether to reject the null hypothesis H0 or not reject H0. Hypothesis
tests can fail in two ways:

• Type I error: rejecting H0 when it is true.
• Type II error: not rejecting H0 when it is false.

We deﬁne the p-value of a test as the probability of making a type I error  and its power as the
probability of correctly rejecting H0 (that is 1 - Type II error). A good test requires the p-value to
be upper-bounded by a user deﬁned signiﬁcance level α (typically α = 0.05) and seeks maximum
power. Testing for conditional independence is a challenging problem. Shah et al. [20] showed that
no conditional independence test maintains non-trivial power while controlling type I error over any
null distribution. In high dimensional samples (relative to sample size)  the problem of maintaining
good power is exacerbated by spurious correlations which tend to make X and Y appear independent
(conditional on Z) when they are not.

2.1 Related work

A recent favoured line of research has characterized conditional independence in a reproducing
kernel Hilbert space (RKHS) [24  6]. The dependence between variables is assessed considering all
moments of the joint distributions which potentially captures ﬁner differences between them. [24]
uses a measure of partial association in a RKHS to deﬁne the KCIT test with provable control on
type I error asymptotically in the number of samples. Numerous extensions have also been proposed
to remedy high computational costs  such as [21] that approximates the KCIT with random Fourier
features making it signiﬁcantly faster. Computing the limiting distribution of the test becomes harder
to accurately estimate in practice [24]  and different bandwidth parameters give widely divergent
results with dimensionality [16]  which affects power.
To avoid tests that rely on asymptotic null distributions  sampling strategies consider explicitly
estimating the data distribution under the null assumption H0. Permutation-based methods [6  17  3 
19] follow this approach. To induce conditional independence  they select permutations of the data
that preserve the marginal structure between X and Z  and between Y and Z. For a set of continuous
conditioning variables and for sizes of the conditioning set above a few variables  the "similar"
examples (in Z) that they seek to permute are hard to deﬁne as common notions of distance increase
exponentially in magnitude with the number of variables. The approximated permutation will be
inaccurate and its computational complexity will not be manageable for use in practical scenarios. As
an example  [6] constructs a permutation P that enforces invariance in Z (P Z ≈ Z) while [17] uses
nearest neighbors to deﬁne suitable permutation sets.
We propose a different sampling strategy building on the ideas proposed by [4] that introduce the
conditional randomization test (CRT). It assumes that the conditional distribution of X given Z
is known under the null hypothesis (in our experiments we will assume it to be Gaussian for use
in practice). The CRT then compares the known conditional distribution to the distribution of
the observed samples of the original data using summary statistics. Instead we require a weaker
assumption  namely having access to a viable approximation  and give an approximately valid test that
does not depend on the dimensionality of the data or the distribution of the response Y ; resulting in a
non-parametric alternative to the CRT. [3] also expands the CRT by proposing a permutation-based
approach to density estimation. Generative adversarial networks have been used for hypothesis

2

Figure 1: Illustration of conditional independence testing with the GCIT. A generator G is optimized
by adversarial training to estimate the conditional distribution X|Z under H0. We then use G to
generate synthetic samples of ˜X under the estimated conditional distribution. Multiple draws are
taken for each conﬁguration Z and a measure of dependence between generated ˜X and Y   ˆρ  is
computed. The sequence of synthetic ˆρ is subsequently compared to the original sample statistic ρ to
get a p-value.

testing in [18]. In this work  the authors use GANs to model to the data distribution and ﬁt a
classiﬁcation model to discriminate between the true and estimated samples. The difference with
our test is that they provide only a loose characterization of their test statistic’s distribution under
H0 using Hoeffding’s inequality. As an example of how this might impact performance is that
Hoeffding’s inequality does not account for the variance in the data sample which biases the resulting
test. A second contrast with our work is that we avoid estimating the distribution exactly but rather
use the generating mechanism directly to inform our test.

3 Generative Conditional Independence Test

Our test for conditional independence  the GCIT (short for Generative Conditional Independence
Test)  compares an observed sample with a generated sample equal in distribution if and only if the
null hypothesis holds. We use the following representation under H0 
P r(X|Z  Y ) = P r(X|Z) ∼ qH0(X)

(1)
On the right hand side the null model preserves the dependence structure of P r(X  Z) but breaks any
dependency between X and Y . If actually there exists a direct causal link between X and Y then
replacing X with a null sample ˜X ∼ qH0 is likely to break this relationship.
Sampling repeatedly ˜X conditioned on the observed confounders Z results in an exchangeable
sequence of generated triples ( ˜X  Y  Z) and original data (X  Y  Z) under H0. In this context  any
function ρ - such as a statistic ρ : X ×Y ×Z → R - chosen independently of the values of X applied
to the real and generated samples preserves exchangeability. Hence the sequence 

ρ(X  Y  Z)  ρ( ˜X (1)  Y  Z)  ...  ρ( ˜X (M )  Y  Z)

(2)
is exchangeable under the null hypothesis H0  deriving from the fact that the observed data is equally
likely to have arisen from any of the above. Without loss of generality  we assume that larger values
of ρ are more extreme. The p-value of the test can be approximated by comparing the generated
samples with the observed sample 

M(cid:88)

m=1

1{ρ( ˜X (m)  Y  Z) ≥ ρ(X  Y  Z)}/M

(3)
1{ρ( ˜X  Y  Z) ≥ ρ(X  Y  Z)} 
which can be made arbitrarily close to the true probability  E ˜X∼qH0
by sampling additional features ˜X from qH0. 1 is the indicator function. Figure 1 gives a graphical
overview of the GCIT.

3.1 Generating samples from qH0
In this section we describe a sampling algorithm that adapts generative adversarial networks [8] to
generate samples ˜X conditional on high dimensional confounding variables Z. GANs provide a

3

powerful method for general-purpose generative modeling of datasets by designing a discriminator
D explicitly used as an adversary to train a generator G responsible for estimating qH0 := P r(X|Z).
Over successive iterations both functions improve based on the performance of the adversarial player.
Our implementation is based on Energy-based generative neural networks introduced in [25]
which if trained optimally  can be shown to minimize a measure of divergence between probability
measures that directly relates to a theoretical bound shown in this section that underlies our method.
Pseudo-code for the GCIT and full details on the implementation are given in Supplement D.

Discriminator. We deﬁne the discriminator as a function Dη : X × Z → [0  1] parameterized by η
that judges whether a generated sample ˜X from G is likely to be distributed as its real counterpart X
or not  conditional on Z. We train the discriminator by gradient descent to minimize the following
loss function 

LD := Ex∼qH0

(4)
where Gφ(z  v)  v ∼ p(v) is a synthetic sample from the generator (described below) and x ∼ qH0 is
a sample from the data distribution under H0. Note that in contrast to [25] we set the image of D to
lie in (0  1) and include conditional data generation.

Dη(x  z) + E˜v∼p(v) (1 − Dη(Gφ(v  z)  z)

Generator. The generator  G  takes (realizations of) Z and a noise variable  V   as inputs and returns
˜X  a sample from an estimated distribution X|Z. Formally  we deﬁne G : Z × [0  1]d → X
to be a measurable function (speciﬁcally a neural network) parameterized by φ  and V to be d-
dimensional noise variable (independent of all other variables). For the remainder of the paper 
let us denote ˜x ∼ ˆqH0 the generated sample under the model distribution implicitly deﬁned by
ˆx = Gφ(v  z)  v ∼ p(v). In opposition to the discriminator  G is trained to minimize

LG(D) := E˜x∼ˆqH0

Dη(˜x  z) − Ex∼qH0

Dη(x  z)
We estimate the expectations empirically from real and generated samples.

(5)

3.2 Validity of the GCIT

The following result ensures that our sampling mechanism leads to a valid test for the null hypothesis
of conditional independence.
Proposition 1 (Exchangeability) Under the assumption that X|=Y |Z  any sequence of statistics
(ρi)M

i=1 functions of the generated triples ( ˜X (m)  Y  Z)M

m=1 is exchangeable.

Proof. All proofs are given in Supplement C.
Generating conditionally independent samples with a neural network preserves exchangeability of
input samples and thus leads to a valid p-value  deﬁned in eq. (3)  for the hypothesis of conditional
independence. Under the assumption that the conditional distribution qH0 can be estimated exactly 
this implies that we maintain an exact control of the type I error in ﬁnite samples. In practice however 
limited amounts of data and noise will prevent us from learning the conditional distribution exactly.
In such circumstances we show below that the excess type I error - that is the proportion of false
negatives reported above a speciﬁed tolerated level α - is bounded by the loss function LG; which 
moreover  can be made arbitrarily close to 0 for a generator with sufﬁcient capacity. We give this
second result as a corollary of the GAN’s convergence properties in Supplement C.
Theorem 1 An optimal discriminator D∗ minimizing LD exists; and  for any statistic ˆρ =
ρ (X  Y  Z)  the excess type I error over a desired level α is bounded by LG(D∗) 

P r(ˆρ > cα|H0) − α ≤ LG(D∗)

(6)
where cα := inf{c ∈ R : P r(ˆρ > c) ≤ α} is the critical value on the test’s distribution and
P r(ˆρ > cα|H0) is the probability of making a type I error.
Theorem 1 shows that the GCIT has an increase in type I error dependent only on the quality of our
conditional density approximation  given by the loss function with respect to the generator  even in
the worst-case under any statistic ρ. For reasonable choices of ρ  robust to errors in the estimation

4

of the conditional distribution  this bound is expected to be tighter. The key assumption to ensure
control of the type I error  and therefore to ensure the validity of the GCIT  thus rests solely on
our ability to ﬁnd a viable approximation to the conditional distribution of X|Z. The capacity of
deep neural networks and their success in estimating heterogeneous conditional distributions even in
high-dimensional samples make this a reasonable assumption  and the GCIT applicable in a large
number of scenarios previously unexplored.

3.3 Maximizing power
For a ﬁxed sample size  conditional dependence H1 : X (cid:54)⊥⊥ Y |Z  is increasingly difﬁcult to detect
with larger conditioning sets (Z) as spurious correlations due to sample size make X and Y appear
independent. To maximize power it will be desirable that differences between generated samples
˜X (under the model P r(X|Z)) and observed samples X (distributed according to P r(X|Z  Y )) be
as apparent as possible. In order to achieve this we will encourage ˆX and X to have low mutual
information because irrespective of dimensionality  mutual information between distributions in the
null and alternative relates directly to the hardness of hypothesis testing problems  which can be
seen for example via Fano’s inequality (section 2.11 in [23]). To do so  we investigate the use of the
information network proposed in [2] and used in the context of feature selection in [10]. [2] propose
a neural architecture and training procedure for estimating the mutual information between two
random variables. We approximate the mutual information with a neural network Tθ : X × X → R 
parameterized by θ  with the following objective function (to be maximized) 

LInf o := sup

[Tθ] − log E

E
p(n)
x ˜x

θ

x ×p(n)
p(n)

˜x

[exp(Tθ)]

(7)

We estimate Tθ in alternation with the discriminator and generator given samples from the generator
in every iteration. We modify the loss function for the generator to include the mutual information
and perform gradient descent to optimize the generator on the following objective 

LG(D) + λLInf o

(8)
λ > 0 is a hyperparameter controlling the inﬂuence of the information network. This additional term
(λLInf o) encourages the generation of samples ˜X as independent as possible from the observed
variables X such that the resulting differences (between ˜X and X) are truly a consequence of the
direct dependence between X and Y rather than spurious correlations with confounders Z.
To provide some further intuition  one can see why generating data different than the sample observed
in the alternative H1 might be beneﬁcial by considering the following bound (proven in Supplement
C) 

Type I error + Type II error ≥ 1 − δT V (ˆqH0  qH1 )

(9)
where ˆqH0 is the estimated null distribution with the GCIT  qH1 is the distribution under H1 and
where δT V is the total variation distance between probability measures. This result suggests that
when emphasizing the differences between the estimated samples and true samples from H1  which
increases the total variation  can improve the overall performance proﬁle of our test by reducing a
lower bound on type I and type II errors.
Remark. The GCIT aims at generating samples whose conditional distribution matches the distribu-
tion of its real counterparts  but can be independent otherwise. It is that gap that the power maximizing
procedure intends to exploit. In practice  there will be a trade-off between the objectives of the
discriminator and information network but we found that setting λ = 10 in our experiments achieved
good performance. It should be noted also that hyperparameter selection cannot be performed using
cross-validation as we do not have access to ground truth and so the hyperparameters must typically
be ﬁxed a priori. However  we can consider artiﬁcially inducing conditional independence (X|=Y |Z)
(by permuting variables X and Y such as to preserve the marginal dependence in (X  Z) and (Y  Z))
and choose hyperparameters that best control for type I error. We explore this further in Supplement
A and test conﬁgurations of λ with synthetic data in section 4.2.

3.4 Choice of statistic ρ

The bound on the type I error given in Theorem 1 holds for any choice of statistic ρ as it depends
solely on the conditional distribution estimation. For choices of ρ less sensitive to spurious differences

5

between generated and true samples when the null H0 holds  the type I error is expected to be below
this bound. We experimented with various dependence measures (between two samples) as choices
for ρ. We consider the Maximum Mean Discrepancy [9]  Pearson’s correlation coefﬁcient  the
distance correlation (which measures both linear and nonlinear association  in contrast to Pearson’s
correlation)  the Kolmogorov-Smirnov distance between two samples and the randomized dependence
coefﬁcient [13]. In our experiments we use the distance correlation and analyze performance using
all other measures in Supplement A.

4 Synthetic data example

In this section we analyse the performance of the GCIT1 in a controlled fashion with synthetic data
against a wide range of competing algorithms  illustrating the effects of different components of
our method. We consider the CRT [4] with pre-speciﬁed Gaussian sampling distribution  whose
parameters are estimated from data; the kernel-methods KCIT [24] and RCoT [21] with bandwith
parameter estimated with the median of all pairwise distances between X and Y   a common choice
in the literature; and the CCIT [19]  which does not make prior assumptions on data distributions but
was also not speciﬁcally designed for high-dimensional data.
When testing at level α  type I error should be as close as possible to α even though this might not
be the case because of violated assumptions or approximations. An important consideration in our
discussion of power as we increase the dimensionality of Z  is the choice of alternatives H1. For
instance  if the strength of the dependency between X and Y increases  the hypothesis testing problem
will be made artiﬁcially easier and bias our conclusions with regards to data dimensionality  as
observed also in [16]. In every synthetic experiment  we maintain the mutual information between X
and Y approximately constant by ﬁrst generating data and second estimating the mutual information
before deciding to draw a new dataset  if the mutual information disagrees with the previous draw  or
otherwise proceed with testing. We estimate the mutual information with a Gaussian approximation 
M I(X  Y ) = − 1

2 log(1 − ˆρ2)  where ˆρ is the linear correlation between X and Y .

4.1 Setup

We generate synthetic data according to the "post non-linear noise model" similarly to [24  6  21] that
deﬁnes (X  Y  Z) under H0 and H1 as follows 

Y = g(AgZ + g)

Y = h(AhZ + αX + h)

H0 : X = f (Af Z + f ) 
H1 :

(10)
(11)
The matrix dimensions of A(·) are such that X and Y are univariate  matrix entries as well as
parameter α are generated at random in the interval [0  1]  and lastly  the noise variables (·) are 0 on
average with variance 0.025. The distributions of X  Y and   and the complexity of dependencies
via f  g and g will be tuned carefully to make performance comparisons in three settings:
(1) Multivariate Gaussian
We set f  g and h to be the identity functions which induces linear dependencies  Z ∼ N (0  σ2)  and
X ∼ N (0  σ2) under H1 which results in jointly Gaussian data under the null and the alternative.
Such a setting matches the assumptions of all methods and the interest of this study will be to provide
a baseline for more complex scenarios.
(2) Multivariate Laplace
Kernel choice has a large impact on power  as we demonstrate in this setting. In this case  we set
f  g and h as before but use a Laplace distribution to generate Z and X. The RBF kernel in this
case overestimates the "smoothness" of the data. This study highlights the robustness of the GCIT in
comparison to kernel-based methods which is important since hyperparameters cannot be tuned by
cross-validation.
(3) Arbitrary distributions
We set f  g and h to be randomly sampled from {x3  tanh x  exp(−x)}  resulting in more complex
distributions and variable dependencies. Here Z ∼ N (0  σ2)  and X ∼ N (0  σ2) under H1. This
at

implementation

of

tutorial

are

1An

https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/alg/gcit/.

our

test

and

available

6

Figure 2: Power results of the synthetic simulations. (Higher better). Left panel: (1) Multivariate
Gaussian  Middle panel: (2) Multivariate Laplace  Right panel: (3) Arbitrary distributions.

is our most general setting which most faithfully resembles the complexities we can expect in real
applications.
Results: Power as a function of the dimensionality of Z is shown in Figure 2. Each point on the
curves is computed by taking averages over 1000 random experiments with sample size equal to 500
examples. The results from scenario (1) are consistent with our expectations; all methods perform
comparably  the CRT and kernel-based methods achieving high power in lower dimensions while
slightly under-performing in higher dimensions. In scenario (2) and (3)  the failure of the CRT
and kernel-based methods is apparent while the GCIT maintains high power  even with increasing
dimensionality  which demonstrates the robustness of our sampling mechanism to arbitrary complex
data distributions. The CCIT outperforms kernel-based methods in these cases also. An important
contrast of the GCIT with respect to the CCIT is our addition of the information network  which we
argue contributes to the higher power observed across all experiments. We analyze this empirically
below.
Figure 2 in Supplement B shows that type I error is approximately controlled at a level α for all
methods. Observe also that even though the GCIT requires training a new GAN in every iteration 
in Figure 3 Supplement B we show empirically that running times for the GCIT scale much better
with dimensionality and sample size in comparison with the best benchmark  the CCIT: its running
times are prohibitive in practice with more than 1000 samples or 500 dimensions in Z  with each test
taking over 600s versus 60s for the GCIT.

Figure 3: Type I error and power for different values of λ.

4.2 Source of gain: consequences of the information network

The information network aims to encourage maximum power in high-dimensional data. We control
for its inﬂuence by varying λ in the loss function of the GCIT given in eq. (8). Higher values of
λ encourage the generation of independent samples which improves power even though it might
decrease the accuracy of the density approximation in the GAN optimization when the null in fact
holds. We notice this trade-off between power and type I error for higher values of λ in Figure 3.
The underlying data was generated from setting (1)  each curve in the two panels corresponds to

7

0100200300400500Dimension of Z0.30.40.50.60.70.80.91.0PowerKCITRCoTCRTGCITCCIT0100200300400500Dimension of Z0.30.40.50.60.70.80.91.0Power0100200300400500Dimension of Z0.30.40.50.60.70.80.91.0Powera different value of λ. Lastly  we computed the lower-bound from GCIT generated samples and
observed samples (by numerical integration) in eq. (9) to conclude that higher values of λ did
decrease the lower bound  as expected.

5 Genetic data example

There is compelling evidence that the likelihood of a patient’s cancer responding to treatment can
be strongly inﬂuenced by alterations in the cancer genome [7]. We study the response of cancer
cell lines to an anti-cancer drug where the problem is to distinguish between genetic mutations that
inﬂuence directly the cancer cell line response from those that are not directly relevant [1  22]. We
use the subset of the CCLE data [1] relating to the drug PLX4720; it contains 474 cancer cell lines
described by 466 genetic mutations. More details on the data can be found in Supplement E.

Figure 4: Genetic experiment results. Each cell gives the p-value or importance rank (where
appropriate) indicating the dependency between a mutation and drug response.

Evaluating conditional independence relations from real data is difﬁcult as we do not have access
to the ground truth causal links. Instead we give our results in comparison to those of [1]  who
proceeded by reporting discriminative features returned by the parameter values of a ﬁtted elastic
net regression model (EN). This is common practice in genetic studies  see for example also [7].
In addition  we compare with the rank of each feature given by a random forest model importance
scores (RF) and the p-value assigned by the CRT. The results for 10 selected mutations can be found
in Figure 4. The ﬁrst two rows give ranks of heuristic methods and the last two rows give p-values of
conditional independence tests. We distinguish between the mutations where all methods agree (in
the leftmost columns)  and the mutations where not all methods agree (in the rightmost columns).
The mutations on genes PIP5K1A and MAP3K5 are recognized to be discriminative by the random
forest model (high rank) and the GCIT (low p-value)  which highlights the signiﬁcance of the GCIT
for conditional independence testing  suggesting that non-linear dependencies occur which are not
captured by the elastic net or the CRT. For further evaluation  in this case we were able to cross-
reference with a previous study to ﬁnd evidence of the PIP5K1A gene to have a differential response
on cancer cell lines when PLX4720 is applied [22]. The MAP3K5 gene has not previously been
reported in the literature as being directly linked to the PLX4720 drug response  however [15] did
ﬁnd a proliferation of these gene mutations to be of BRAF type in cancer patients. This is interesting
because PLX4720 is precisely designed as a BRAF inhibitor  and thus we would expect it to have
an impact also on MAP3K5 mutations of the BRAF type. FLT3 is an interesting gene  found to be
dependent on cancer response by the EN  RF and CRT  but not by the GCIT. This ﬁnding by the
GCIT was conﬁrmed however by a posterior genetic study [5] that established no link between cancer
response and FLT3 mutations in the presence of PLX4720. Such results encourage us to believe that
the GCIT is able to better detect dependence for these problems.

6 Conclusions and future perspectives

We propose a generative approach to conditional independence testing using generative adversarial
networks. We show this approach results in an approximately valid test for an arbitrary data distri-
bution irrespective of the number of variables observed. We have demonstrated through simulated
data signiﬁcant gains in statistical power  and we illustrated the application of our method to discover
genetic markers for cancer drug response on real high-dimensional data.

8

From a practical perspective  algorithms based on other generative models can be constructed based
on our proposed procedure that may be more adequate for different data modalities. In a general
sense  this work opens the door to principled statistical testing with more heterogeneous data  and
expands our ability to reason and test variable relationships in more challenging scenarios.

7 Acknowledgements

We thank the anonymous reviewers for valuable feedback. This work was supported by the Alan
Turing Institute under the EPSRC grant EP/N510129/1  the ONR and the NSF grants number 1462245
and number 1533983.

References
[1] Jordi Barretina  Giordano Caponigro  Nicolas Stransky  Kavitha Venkatesan  Adam A Margolin 
Sungjoon Kim  Christopher J Wilson  Joseph Lehár  Gregory V Kryukov  Dmitriy Sonkin  et al.
The cancer cell line encyclopedia enables predictive modelling of anticancer drug sensitivity.
Nature  483(7391):603  2012.

[2] Ishmael Belghazi  Sai Rajeswar  Aristide Baratin  R Devon Hjelm  and Aaron Courville. Mine:
mutual information neural estimation. In International Conference on Machine Learning  2018.
[3] Thomas B Berrett  Yi Wang  Rina Foygel Barber  and Richard J Samworth. The conditional

permutation test. arXiv preprint arXiv:1807.05405  2018.

[4] Emmanuel Candes  Yingying Fan  Lucas Janson  and Jinchi Lv. Panning for gold:‘model-
x’knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical
Society: Series B (Statistical Methodology)  80(3):551–577  2018.

[5] Anindya Chatterjee  Joydeep Ghosh  Baskar Ramdas  Raghuveer Singh Mali  Holly Martin 
Michihiro Kobayashi  Sasidhar Vemula  Victor H Canela  Emily R Waskow  Valeria Visconte 
et al. Regulation of stat5 by fak and pak1 in oncogenic ﬂt3-and kit-driven leukemogenesis. Cell
reports  9(4):1333–1348  2014.

[6] Gary Doran  Krikamol Muandet  Kun Zhang  and Bernhard Schölkopf. A permutation-based

kernel conditional independence test. In UAI  pages 132–141  2014.

[7] Mathew J Garnett  Elena J Edelman  Sonja J Heidorn  Chris D Greenman  Anahita Dastur 
King Wai Lau  Patricia Greninger  I Richard Thompson  Xi Luo  Jorge Soares  et al. Systematic
identiﬁcation of genomic markers of drug sensitivity in cancer cells. Nature  483(7391):570 
2012.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[9] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

[10] James Jordon  Jinsung Yoon  and Mihaela van der Schaar. Knockoffgan: Generating knockoffs

for feature selection using generative adversarial networks. In ICLR  2019.

[11] Amit V Khera and Sekar Kathiresan. Genetics of coronary artery disease: discovery  biology

and clinical translation. Nature reviews Genetics  18(6):331  2017.

[12] Steffen L Lauritzen. Graphical models  volume 17. Clarendon Press  1996.
[13] David Lopez-Paz  Philipp Hennig  and Bernhard Schölkopf. The randomized dependence

coefﬁcient. In Advances in neural information processing systems  pages 1–9  2013.

[14] Judea Pearl et al. Causal inference in statistics: An overview. Statistics surveys  3:96–146 

2009.

[15] Todd D Prickett  Brad Zerlanko  Jared J Gartner  Stephen CJ Parker  Ken Dutton-Regester 
Jimmy C Lin  Jamie K Teer  Xiaomu Wei  Jiji Jiang  Guo Chen  et al. Somatic mutations
in map3k5 attenuate its proapoptotic function in melanoma through increased binding to
thioredoxin. Journal of Investigative Dermatology  134(2):452–460  2014.

9

[16] Aaditya Ramdas  Sashank Jakkam Reddi  Barnabás Póczos  Aarti Singh  and Larry A Wasser-
man. On the decreasing power of kernel and distance based nonparametric hypothesis tests in
high dimensions. In AAAI  pages 3571–3577  2015.

[17] Jakob Runge. Conditional independence testing based on a nearest-neighbor estimator of

conditional mutual information. In AISTATS  2018.

[18] Rajat Sen  Karthikeyan Shanmugam  Himanshu Asnani  Arman Rahimzamani  and Sreeram
Kannan. Mimic and classify: A meta-algorithm for conditional independence testing. arXiv
preprint arXiv:1806.09708  2018.

[19] Rajat Sen  Ananda Theertha Suresh  Karthikeyan Shanmugam  Alexandros G Dimakis  and
Sanjay Shakkottai. Model-powered conditional independence test. In Advances in Neural
Information Processing Systems  pages 2951–2961  2017.

[20] Rajen D Shah and Jonas Peters. The hardness of conditional independence testing and the

generalised covariance measure. arXiv preprint arXiv:1804.07203  2018.

[21] Eric V Strobl  Kun Zhang  and Shyam Visweswaran. Approximate kernel-based conditional
independence tests for fast non-parametric causal discovery. arXiv preprint arXiv:1702.03877 
2017.

[22] Wesley Tansey  Victor Veitch  Haoran Zhang  Raul Rabadan  and David M Blei. The hold-
out randomization test: Principled and easy black box feature selection. arXiv preprint
arXiv:1811.00645  2018.

[23] Joy A Thomas and TM Cover. Elements of information theory. John Wiley & Sons  Inc.  New
York. Toni  T.  Welch  D.  Strelkowa  N.  Ipsen  A.  and Stumpf  MPH (2009) “Approximate
Bayesian computation scheme for parameter inference and model selection in dynamical
systems ” Journal of the Royal Society Interface  6:187–202  1991.

[24] Kun Zhang  Jonas Peters  Dominik Janzing  and Bernhard Schölkopf. Kernel-based conditional

independence test and application in causal discovery. In UAI  2012.

[25] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

In ICLR  2017.

[26] Zhihong Zhu  Zhili Zheng  Futao Zhang  Yang Wu  Maciej Trzaskowski  Robert Maier 
Matthew R Robinson  John J McGrath  Peter M Visscher  Naomi R Wray  et al. Causal
associations between risk factors and common diseases inferred from gwas summary data.
Nature communications  9(1):224  2018.

10

,Alexis Bellot
Mihaela van der Schaar