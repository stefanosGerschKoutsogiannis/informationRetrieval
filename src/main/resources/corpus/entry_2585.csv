2019,Improved Regret Bounds for Bandit Combinatorial Optimization,\textit{Bandit combinatorial optimization} is a bandit framework in which a player chooses an action within a given finite set $\mathcal{A} \subseteq \{ 0  1 \}^d$ and incurs a loss that is the inner product of the chosen action and an unobservable loss vector in $\mathbb{R} ^ d$ in each round. In this paper  we aim to reveal the property  which makes the bandit combinatorial optimization hard. Recently  Cohen et al.~\citep{cohen2017tight} obtained a lower bound $\Omega(\sqrt{d k^3 T / \log T})$ of the regret  where $k$ is the maximum $\ell_1$-norm of action vectors  and $T$ is the number of rounds. This lower bound was achieved by considering a continuous strongly-correlated distribution of losses. Our main contribution is that we managed to improve this bound by $\Omega( \sqrt{d k ^3 T} )$ through applying a factor of $\sqrt{\log T}$  which can be done by means of strongly-correlated losses with \textit{binary} values. The bound derives better regret bounds for three specific examples of the bandit combinatorial optimization: the multitask bandit  the bandit ranking and the multiple-play bandit. In particular  the bound obtained for the bandit ranking in the present study addresses an open problem raised in \citep{cohen2017tight}. In addition  we demonstrate that the problem becomes easier without considering correlations among entries of loss vectors. In fact  if each entry of loss vectors is an independent random variable  then  one can achieve a regret of $\tilde{O}(\sqrt{d k^2 T})$  which is $\sqrt{k}$ times smaller than the lower bound shown above. The observed results indicated that correlation among losses is the reason for observing a large regret.,Improved Regret Bounds

for Bandit Combinatorial Optimization∗

Shinji Ito†

NEC Corporation  The University of Tokyo

i-shinji@nec.com

Hanna Sumita

Tokyo Metropolitan University

sumita@tmu.ac.jp

Daisuke Hatano

RIKEN AIP

daisuke.hatano@riken.jp

Kei Takemura
NEC Corporation

kei_takemura@nec.com

Takuro Fukunaga‡

Chuo University  RIKEN AIP  JST PRESTO

fukunaga.07s@g.chuo-u.ac.jp

Naonori Kakimura§

Keio University

kakimura@math.keio.ac.jp

Ken-ichi Kawarabayashi§
National Institute of Informatics

k-keniti@nii.ac.jp

Abstract

Bandit combinatorial optimization is a bandit framework in which a player chooses
an action within a given ﬁnite set A ⊆ {0  1}d and incurs a loss that is the
inner product of the chosen action and an unobservable loss vector in Rd in each
round.
In this paper  we aim to reveal the property  which makes the bandit
combinatorial optimization hard. Recently  Cohen et al. [8] obtained a lower

bound Ω((cid:112)dk3T / log T ) of the regret  where k is the maximum (cid:96)1-norm of

action vectors  and T is the number of rounds. This lower bound was achieved
by considering a continuous strongly-correlated distribution of losses. Our main
contribution is that we managed to improve this bound by Ω(
dk3T ) through
log T   which can be done by means of strongly-correlated
applying a factor of
losses with binary values. The bound derives better regret bounds for three speciﬁc
examples of the bandit combinatorial optimization: the multitask bandit  the bandit
ranking and the multiple-play bandit. In particular  the bound obtained for the
bandit ranking in the present study addresses an open problem raised in [8]. In
addition  we demonstrate that the problem becomes easier without considering
√
correlations among entries of loss vectors. In fact  if each entry of loss vectors is an
independent random variable  then  one can achieve a regret of ˜O(
dk2T )  which
is
k times smaller than the lower bound shown above. The observed results
indicated that correlation among losses is the reason for observing a large regret.

√

√

√

∗This work was supported by JST  ERATO  Grant Number JPMJER1201  Japan.
†This work was supported by JST  ACT-I  Grant Number JPMJPR18U5  Japan.
‡This work was supported by JST  PRESTO  Grant Number JPMJPR1759  Japan.
§This work was supported by JSPS  KAKENHI  Grant Number JP18H05291  Japan.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1

Introduction

This paper is aimed to investigate the bandit combinatorial optimization problem deﬁned as follows:
A player is given a ﬁnite action set A ⊆ {a ∈ {0  1}d | (cid:107)a(cid:107)1 = k} and the number T of rounds for
decision-making. In each round t = 1  2  . . .   T   the player chooses an action at from A. At the
same time  the environment privately chooses a loss vector (cid:96)t = [(cid:96)t1  . . .   (cid:96)td](cid:62) ∈ [0  1]d  and the
player observes the loss (cid:96)(cid:62)
t at incurred by the action at. The goal of the player is to minimize the
t at]  where the expectation is taken with respect to the player’s
internal randomization. The performance of the algorithm is measured in terms of the regret RT
deﬁned by RT = maxa∈A E

expected cumulative loss E[(cid:80)T
(cid:104)(cid:80)T

t at −(cid:80)T

t=1 (cid:96)(cid:62)

t=1 (cid:96)(cid:62)
t a

t=1 (cid:96)(cid:62)

(cid:105)

.

In this study  we focus on the minimax regret  the worst-case regret attained by optimal algo-
rithms  which can be expressed as RT := minalgorithm max{(cid:96)t}T
t=1⊆[0 1]d RT . The minimax re-
gret can be bounded from above by designing algorithms. The current best bound is RT =

O((cid:112)dk3T log(ed/k))  as reported in a number of papers [2; 6; 7; 10; 12]. However  lower bounds
however  Cohen et al. [8] presented the lower bound of RT = Ω((cid:112)dk3T / log T )  which rejected
lower bounds to O((cid:112)log(ed/k) log T ) consisting of logarithmic terms only.

of the minimax regret can be proven by constructing a probabilistic distribution of loss vectors for
which any algorithm incurs a certain degree of regret. To obtain a lower bound  Audibert et al. [2]
constructed a probabilistic distribution of loss vectors for which arbitrary algorithms incurred a regret
of Ω(
dk2T ). Recently 

√
dk2T )  and they conjectured that this bound was tight  i.e.  RT = Θ(

the above-mentioned conjecture  and thereby  they have decreased the gap between the upper and

√

The input distribution constructed by Cohen et al. [8] to derive the lower bound has the unique
characteristics that cannot be found in previous studies  such as lower bounds for a multi-armed
bandit [4]  a combinatorial semi-bandit [6; 20; 23] and a combinatorial bandit [2]. In previous studies
on lower bounds  only binary inputs and an arm-wise independent distribution were considered 
i.e.  (cid:96)t1  . . .   (cid:96)td are mutually independent {0  1}-valued discrete random variables. Such inputs
were proved to result in tight lower bounds for multi-armed bandits [4] and combinatorial semi-
bandits [2; 20]. In contrast to these studies  Cohen et al. [8] introduced loss vectors following a
continuous distribution over [0  1]d and having a strong correlation among d entries. Furthermore 
the lower bound obtained in Cohen et al. [8] includes a 1/
log T term  which does not appear in
the other lower bounds for bandit problems. In addition  they applied the obtained lower bounds to
special cases  such as the multitask bandit and the bandit ranking problem. However  their results
are restricted to the problems under certain parameter constraints  and consequently  the task of
identifying the tight bounds for some important special cases  including the problem referred to as
bandit ranking with full permutations  were left open.
Such characteristics corresponding to the input distribution deﬁned by Cohen et al. [8] lead to the
following research questions:

√

√

Q. 1 Is the 1/
log T factor in the lower bound given by Cohen et al. [8] redundant or inevitable?
Q. 2 Does the continuous distribution of loss vectors make the problem essentially harder than the
discrete (binary) distribution? If we restrict our consideration to the loss vectors in {0  1}d 
then the player can see the number of good arms (i ∈ [d] s.t. lti = 0) in the chosen arms St 
which may  or may not  be more informative than actual values.

Q. 3 Does the correlation of loss among different arms make the problem essentially harder than the

arm-wise independent loss?

Q. 4 Can we obtain tight lower bounds for the special cases such as the bandit ranking problem with

full permutations resolving the open question in [8]?

2 Main Results

Our main results can be interpreted to answer the above four questions. First  we improve the regret
lower bound obtained by [8] to Ω(
log T   as shown in Table 1.
These bounds can be proven by constructing a distribution of strongly-correlated losses using binary
values. We apply the bounds to the three speciﬁc examples of bandit combinatorial optimization

dk3T )  by applying a factor of

√

√

2

Table 1: Regret bounds RT for bandit combinatorial optimization.

Assumption

No assumption

Independent losses

Upper bound by Algorithms

O((cid:112)dk2T log |A|)
= O((cid:112)dk3T log(ed/k))
O((cid:112)dkT log |A| log T )
= O((cid:112)dk2T log(ed/k) log T )

([6] and [7])

(Algorithm 1 and Theorem 3)

Lower bound

Ω((cid:112)dk3T / log T ) by (cid:96)t ∈ [0  1]d

([8]) 
√

√
Ω(
([2])

dk3T ) by (cid:96)t ∈ {0  1}d

Ω(
(Theorems 1 and 2)

dk2T ) by (cid:96)t ∈ {0  1}d

√

that have high practical importance: the multitask bandit problem  the bandit ranking problem
(Theorem 1)  and the multiple-play bandit problem (Theorem 2). This result provides answers to
Q. 1 and Q. 2 outlined in Section 1: The 1/
log T factor in the lower bound is redundant  and the
difference between continuous-valued and discrete-valued losses does not have a large impact on the
hardness of the problem. This observation also addresses Q. 4  an open problem outlined in [8].
The multitask bandit problem [7; 8] is a bandit framework in which the player tries to solve k instances
of the n-armed bandit problem. This is a special case of the bandit combinatorial optimization with
d = kn and

A =

ai = 1

(j ∈ [k])

(1)

 .

jn(cid:88)

i=(j−1)n+1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a ∈ {0  1}d
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

jn(cid:88)

i=(j−1)n+1

k(cid:88)

i=1

a ∈ {0  1}d

A =

In the bandit ranking problem or online ranking problem [13] with bandit feedback problem  the goal
of the player is to ﬁnd a maximum matching in the complete bipartite graph Kk n with d = kn edges 
where k ∈ [n]. The set of all maximum matchings can be expressed as follows:

ai = 1 (j ∈ [k]) 

a(i−1)n+j ≤ 1 (j ∈ [n])

(2)

 .

Considering these problems  we obtain the following regret lower bound.
Theorem 1 (multitask bandit  bandit ranking). Suppose that A is deﬁned by (1) or (2) and n ≥ 2.
There is a probability distribution D over {0  1}d for which the following statement holds: If (cid:96)t
√
is drawn from D for t = 1  . . .   T independently  the regret for any algorithm satisﬁes E[RT ] =
Ω(min{
Considering the bandit ranking problem  the previous work [8] demonstrated the lower bound of

Ω((cid:112)dk3T / log T ) under the assumption of n ≥ 2k  and the full-permutation case (k = n) was left

dk3T   k3/4T})  where the expectation is taken with respect to the randomness of (cid:96)t.

√
as an open problem  as mentioned in the conclusion of this research work. Theorem 1 answers to this
open problem: Even if k = n  the minimax regret is of RT = ˜Θ(
√
k5T )  ignoring
a
log k factor. Theorem 1 can also be extended to the online shortest path problem [5]  by the
standard reduction from multitask bandit to the online shortest path. See e.g.  [8] for details of the
reduction.
The multiple-play bandit problem [7; 16; 18; 23] is another bandit framework in which the player
can choose arbitrary k arms from a set of d arms in each round. This problem corresponds to

√
dk3T ) = ˜Θ(

(cid:1) := {a ∈ {0  1}d | (cid:107)a(cid:107)1 = k}.

A =(cid:0)[d]
Theorem 2 (multiple-play bandit). Suppose that A =(cid:0)[d]
(cid:110)

(cid:16)

k

over {0  1}d for which the following holds: If (cid:96)t is drawn from D for t = 1  . . .   T independently  the
regret for any algorithm will satisfy E[RT ] = Ω
  where the
min
expectation is taken with respect to the randomness of (cid:96)t.
The above lower bound means that RT = Ω(
dk3T ) for T = Ω(dk3/2) and d = Ω(k). It should
be noted that existing works [2; 8; 20] provided weaker lower bounds only for the case of d ≥ 2k 

dk3T   d−k

d k3/4T

( d−k

d )2

√

√

k

(cid:1). There is a probability distribution D

(cid:111)(cid:17)

3

while those provided in the present study are valid for general d and k. The proof of Theorem 2 is
presented in Appendix C.
A basic idea for proving a nearly tight bound is to construct an environment  where all entries of (cid:96)t
are strongly correlated between each other; this concept has been introduced by Cohen et al. [8]. If
losses are strongly correlated  the observed value (cid:96)(cid:62)
t a has a larger variance. For example  the variance
is of order k if all entries are independent  while it can be of order k2 if all entries take the same value.
When the observed values (cid:96)(cid:62)
t a have larger variance  the KL divergence among the values for different
actions a is small  which implies that no algorithm can detect “good” actions properly. Cohen et al.
√
[8] constructed such an environment by means of normal distributions  which improve the lower
k) factor. However  their proposed bound includes a redundant (log T )−1/2 factor
bound by ˜O(
due to the unbounded support of normal distributions.1 We note that their technique has been used
recently for proving a lower bound for bandit PCA [17]  which includes a redundant (log T )−1/2
factor too  for the same reason as the above.
To shave off the (log T )−1/2 factor  in this paper  we introduce a novel class of discrete distributions
over {0  1}d  so that entries of loss vectors are bounded and strongly correlated. To make the losses
correlated  we consider d Bernoulli distributions that share the parameter  by which the observed value
has a large variance of O(k2). However  it is not a straightforward task to set “good” actions in this
approach. The previous work [8] simply decreases the mean parameter in the normal distribution to
set “good” actions  but it does not work as it causes large KL divergences between “good" actions and
the others in our distribution. In the present work  we adjust the parameter of Bernoulli distributions
carefully with the intention of ensuring small KL divergence  which allows improving the regret
lower bound successfully. The idea outlined in the present study can be used to improve the idea of
[8] even considering other problems.
Second  we show that the correlation among losses is the reason of observing a large regret. In fact 
√
if each entry of loss vectors is an independent random variable  then one can achieve a regret of
˜O(
k times smaller than the lower bounds in Theorems 1 and 2. This
√
provides the answer to Q. 3: The correlation among losses makes the problem essentially harder  as
the minimax regret bound becomes larger by a factor of ˜Θ(
Theorem 3 (smaller regret bound for the arm-wise independent loss). There exists an algorithm

that achieves E[RT ] = O((cid:112)dk2T log T log(ed/k)) for T = Ω(d3)  under the assumption that (cid:96)t

dk2T ) as below  which is

follows a distribution of mutually independent d random variables in [0  1]  i.i.d. for t = 1  2  . . .   T .

√

k).

√

This upper bound is nearly tight; Theorem 5 in [2] implies that any algorithm suffers E[RT ] =
dk2T ) in the worst case under the same assumption as in Theorem 3.2 By combining this result
Ω(
and Theorem 3  we obtain the following corollary:
√
Corollary 1. Under the same assumption as in Theorem 3  the minimax regret in the bandit combi-
natorial optimization is of order ˜Θ(

dk2T )  where we ignore logarithmic factors in d and T .

To prove Theorem 3  we analyze regret upper bounds for stochastic linear bandits  which are
generalization of the bandit combinatorial optimization with stochastic environments. In stochastic
linear bandits  a player is given a ﬁnite set A ⊆ Rd of d-dimensional vectors. In each round  the
player chooses at ∈ A and receives loss Lt = (cid:96)∗(cid:62)at + ηt  where ηt is the noise  which is assumed to
be conditionally α-subgaussian. We also assume that supa b∈A (cid:96)∗(cid:62)(a − b) ≤ L. We observe that
bandit combinatorial optimization with the assumption deﬁned in Theorem 3 is a special case of
stochastic linear bandits with α =
For stochastic linear bandits with α = 1 and L = 1  Lattimore and Szepesvári [19] provided an
).3 This upper bound  however  does not directly
algorithm that achieves RT = O(
lead to Theorem 3  because their bound holds only for the case of α = 1 and L = 1; If we directly

k/2 and L = k.

(cid:113)

|A| log T

√

dT log

δ

1 To keep (cid:96)t in the bounded region [0  1]d with high probability  the variances of normal distributions need to

be maintained sufﬁciently small  which makes the KL divergence large.

2 Although the original statement in [2] does not include the independence assumption  we can conﬁrm that

it is satisﬁed in their proof.

3 In their book  the proof is left for the reader as an exercise.

4

(cid:113)

|A| log T

√
) = ˜O(

√

apply their result  we obtain RT = O(
1/k. This is ˜Ω(
To mitigate this issue  we modify their algorithm  so that we can perform a more reﬁned analysis for
the case of arbitrary α and L. The differences between our Algorithm 1 given in Appendix D.1 and
Algorithm 12 in [19] are summarized as follows:

k) times larger than the bound provided in Theorem 3.

dk3T ) by multiplying losses by

dk2T log

δ

• They deal with only the case in which the noise ηt has a bounded variance  i.e.  α = 1. To
deal with the case for a general α  we modify the deﬁnition (31) of Tk in their algorithm.
• They assume that the suboptimality gap maxa b∈A{(cid:96)∗(cid:62)(a − b)} is bounded by 1. To handle
properly the changing suboptimality gaps  we modify the deﬁnition of εt in their algorithm.
• They basically consider maximization problems  while we consider minimization (This doe

not result in essential differences).

We demonstrate that Algorithm 1 achieves the following regret bound:
Theorem 4. For any input parameters δ > 0 and ε1 > 0  with a probability of at least 1 − δ  the
output of Algorithm 1 satisﬁes

(cid:96)∗(cid:62)

(at − a) ≤ 9α

dT log

|A| log T

δ

+ L

2dα2

ε2
1

log

2|A|
δ

+ (L + ε1)d2.

(3)

T(cid:88)

t=1

max
a∈A

(cid:114)

Theorem 4 means that the upper bound L of (cid:96)∗(cid:62)at does not affect the leading term of the regret upper
bound  however  α does affect. By substituting α =
k/2 and L = k with the bound in Theorem 4 
we obtain Theorem 3.

√

3 Related Work

√

Bandit combinatorial optimization was ﬁrst introduced by McMahan and Blum [21] and Awer-
buch and Kleinberg [5]. They proposed the algorithms achieving the regret of ˜O(T 3/4) and
˜O(T 2/3)  respectively  ignoring dependence on d and logarithmic factors in T . Algorithms with
improved regret bounds have been proposed in several papers [2; 6; 7; 10]. These algorithms achieve

achieving the sublinear regret have also been introduced in [7; 9; 12; 22; 14].
With regard to lower bounds in the bandit combinatorial optimization  Audibert et al. [2] showed that
dk2T )  and consequently  they conjectured that this lower bound was tight. However  the
RT = Ω(

RT = O((cid:112)dk3T log(ed/k)) in our problem setting. Recently  computationally efﬁcient algorithms
recent work by Cohen et al. [8]  rejected this conjecture showing that RT = Ω((cid:112)dk3T / log T ).
(cid:1)  Audibert et al. [2] proposed an algorithm achieving the
graph. For general action sets A ⊆(cid:0)[d]
dkT )  and showed that it is minimax optimal  i.e.  there is an action set A ⊆(cid:0)[d]
(cid:1) such
dkT ). With regard to the multiple-play bandit problem  i.e.  the case of A =(cid:0)[d]
(cid:1) 

Combinatorial semi-bandit optimization is a variant of bandit combinatorial optimization  in which
t at  but also the entry (cid:96)ti for each chosen arm i ∈ St.
the player can observe not only the total loss (cid:96)(cid:62)
This problem was introduced by György et al. [11] in the context of the online shortest path problem 
i.e.  they considered the case in which A is a set of all subsets of edges constructing a path in a given

√
regret of O(
√
that RT = Ω(
with semi-bandit feedback  Uchiya et al. [23] showed that RT = Ω(
dT )  but it remained open
√
whether this bound was tight  until the recent work by Lattimore et al. [20] provided the proof that
RT = Ω(
The study on stochastic linear bandits was introduced in the work by Abe and Long [1]. They and
Auer [3] considered the case of ﬁnite action sets that can change every round. Bandit combinatorial
optimization with a stochastic environment can be seen as a special case of stochastic linear bandits

in which the action set is included in(cid:0)[d]
(cid:1) and does not change in every round. Auer [3] introduced a
technique of dividing rounds to achieve RT = O((cid:112)dT (log(|A|T log T ))3) under the assumption

of bounded loss. We remark that a similar technique is used in Algorithm 1. Moreover  a similar
technique was used for spectral bandits considered by Valko et al. [24]  in which they eliminated
inappropriate arms over several phases.

dkT ).

√

k

k

k

k

5

4 Lower Bounds

√

dk2T ) and Ω((cid:112)dk3T / log T ) for multitask bandits  respectively. From the proofs

In this section  we provide proofs for Theorems 1 and 2. First  we revisit the proofs presented in
the previous work: Theorem 5 in [2] and Lemma 4 in [8]  which provide the regret lower bounds
of the order Ω(
provided in the related work  we can observe that regret lower bounds can be derived from upper
bounds on KL divergences determined by distributions of loss vectors. Second  we construct a
distribution of loss vectors  so that the corresponding KL divergence is small enough. Combining
these two results  we obtain Theorem 1  which provides an improved lower bound for multitask
bandits. Finally  we extend the proof for multitask bandit to prove Theorem 2 for multiple-play
bandits.

4.1 Proof idea used in the previous work

This subsection revisits the proofs for regret lower bounds for multitask bandit  given in [2] and [8].
We note that  from Yao’s minimax principle  it sufﬁces to construct a probabilistic distribution of (cid:96)t 
such that in expectations  any deterministic algorithm suffers large regret.
In both proofs  the probabilistic distribution of the loss vectors is deﬁned as follows. First  it is
d](cid:62) ∈ {0  1}d 
necessary to set a parameter ε > 0  which is to be optimized later. For a∗ = [a∗
a probabilistic distribution Da∗ over Rd is deﬁned such that (cid:96) ∼ Da∗ satisﬁes

1  . . .   a∗

E

− εa∗

i

1
2

[(cid:96)i] =

2 − εa∗

(cid:96)∼Da∗
2 − εa∗

(4)
for each i ∈ [d]. More concretely  [2] deﬁne Da∗ such that the i-th entry of the vector follows the
Bernoulli distribution of parameter 1
i   independently. Cohen et al. [8] deﬁne Da∗ such that
i + Z  where Z follows the normal distribution N (0  σ2). We can
the i-th entry is equal to 1
conﬁrm that these two deﬁnitions satisfy (4). The environment picks a∗ ∈ A uniformly at random
before the game begins  and then  in round t = 1  2  . . .   T   generates a loss vector (cid:96)t following Da∗
i.i.d. It should be noted that A is deﬁned by (1) here.
i = 1}  and at be the
We analyze the regret bounds for these loss vectors. Let S∗ = {i ∈ [d] | a∗
action chosen by the player in round t. Let us deﬁne Ni to be the number of rounds in [T ] in which
the player suffers a loss for the i-th entry of loss vectors  i.e.  Ni = |{t ∈ [T ] | ati = 1}|. Then  from
(cid:33)
(4)  the regret RT satisﬁes

(cid:35)

t at − T(cid:88)

(cid:32)
kT − (cid:88)
From (5)  to obtain a lower bound on RT   it sufﬁces to bound(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:20)

we use the following lemma:
Lemma 1. Let D and D(cid:48) be the probability distributions over [0  1]d. Then  we have

(cid:118)(cid:117)(cid:117)(cid:116) T(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ T

(cid:96)∼D (cid:96)(cid:48)∼D(cid:48)(a(cid:62)

(5)
i∈S∗ Ni. To obtain a bound on Ni 

(cid:21)

(6)

E

(cid:96)1 ... (cid:96)T ∼Da∗

[Ni]

.

(cid:34) T(cid:88)

t (cid:96)||a(cid:62)

t (cid:96)(cid:48))

E

(cid:96)1 ... (cid:96)T ∼Da∗

E

(cid:96)1 ... (cid:96)T ∼Da∗

E

(cid:96)1 ... (cid:96)T ∼D(cid:48)

[Ni]

E

(cid:96)1 ... (cid:96)T ∼D

[RT ] ≥

(cid:96)(cid:62)
t a∗

= ε

[Ni] −

t=1

t=1

E

at∼At(D)

t=1

KL

(cid:96)(cid:62)

i∈S∗

for any deterministic algorithm  where At(D) represents the probability distribution of the outputs of
the algorithm in round t for the inputs (cid:96)1  (cid:96)2  . . .   (cid:96)t−1 following D independently.
This lemma follows from Pinsker’s inequality and the chain rule for the KL divergence. For details 
see  e.g.  Lemma A.1. in [4].
Lemma 1 deﬁnes a connection between bounds on Ni and upper bounds on KL divergences of
speciﬁc distributions. To provide a bound on Ni by means of Lemma 1  Audibert et al. [2] and Cohen
et al. [8] used speciﬁc properties of their distributions. We observe that their arguments are focused
on the fact that their distributions of loss vectors satisfy the following condition regarding the KL
divergence:

a ∈ A  a∗  ˆa ∈ {0  1}d  ˆa(cid:62)a − a∗(cid:62)a = 1  (cid:96)∗ ∼ Da∗   ˆ(cid:96) ∼ Dˆa
=⇒ KL((cid:96)∗(cid:62)a||ˆ(cid:96)(cid:62)a) ≤ CDε2 for a constant CD depending on {Da}.

(7)

6

Intuitively  the precondition of (7) means that the discrepancy with respect to the expected loss is at
most ε. In fact  a∗(cid:62)a in (7) corresponds to “goodness of action a" for the loss vector (cid:96) ∼ Da∗  because
the expected loss for action a is equal to k/2 − εa∗(cid:62)a from (4). Consequently  ˆa(cid:62)a − a∗(cid:62)a = 1
means that the expected loss for Da∗ is smaller than one for Dˆa by ε.
We can show that  if (7) is true  then Lemma 1 implies that if a∗ follows a uniform distribution over
A deﬁned by (1)  we obtain
. Therefore  if we

set ε ≤(cid:112)d/(16CDkT )  we obtain
(cid:3) ≤ 3kT /4  and consequently  we
(cid:112)d/(16CDkT ) that satisﬁes (4) and (7). If A is given by (1) with n ≥ 2  then we have a regret

obtain E[RT ] ≥ εkT
from (5). The main observation of this subsection is summarized as follows:
Observation 1. Suppose a family {Da∗ | a∗ ∈ {0  1}d} of distributions with a parameter ε ≤

(cid:3) ≤ k
(cid:2)(cid:80)

(cid:113) kT

a∗ (cid:96)1 ... (cid:96)T ∼Da∗

a∗ (cid:96)1 ... (cid:96)T ∼Da∗

(cid:2)(cid:80)

T
2 + T ε

i∈S∗ Ni

i∈S∗ Ni

d CD

(cid:18)

(cid:19)

E

E

4

lower bound of E[RT ] = Ω(εkT ).

4.2 Construction of the probabilistic distribution
The goal of this subsection is to construct a family {Da∗ | a∗ ∈ {0  1}d} of distributions such that (4)
√
and (7) are satisﬁed with CD = O(1/k2). From Observation 1  such construction leads to a regret
lower bound of E[RT ] = Ω(
dk3T ) for the multitask bandit problem  thereby  proving Theorem 1.
The proposed probabilistic distribution of loss vectors is deﬁned as follows. Let us set a parameter
ε ∈ [0  2−16]  which is to be optimized later. For a∗ = [a∗
d](cid:62) ∈ {0  1}d  let Da∗ be a
distribution of (cid:96) = [(cid:96)1  . . .   (cid:96)d](cid:62) ∈ [0  1]d generated in the following way:

1  . . .   a∗

(i) Draw u0 from a uniform distribution over [0  1].
(ii) Draw bi from a Bernoulli distribution of parameter (1/2 + 2εa∗
i ).
(iii) For i ∈ [d]  draw ui from a uniform distribution over
(iv) Let (cid:96)i = 1 if ui ≥ u0  and otherwise  (cid:96)i = 0.

(cid:26) [0  1/2]

(1/2  1]

(8)

if bi = 1 
if bi = 0.

(cid:18)

i ) + 3

4 ( 1

i ) = 1

2 − εa∗

2 + 2εa∗

2 − 2εa∗

4 Prob[bi = 1] + 3

= − k(cid:88)

i   which means that (4) holds.

KL((cid:96)∗(cid:62)a||ˆ(cid:96)(cid:62)a) = − k(cid:88)
≤ − k(cid:88)

We can conﬁrm that (4) holds for this Da∗. In fact  step (iv) means E[(cid:96)i] = Prob[ui ≥ u0]  and as
u0 follows the uniform distribution over [0  1] and ui ∈ [0  1]  we obtain Prob[ui ≥ u0] = E[ui].
Moreover  from steps (ii) and (iii)  we obtain E[ui] = 1
4 Prob[bi = 0] =
4 ( 1
1
Let us show that (7) is satisﬁed with CD = O(1/k2). As Da∗ is a distribution over {0  1}d 
(cid:96)∗(cid:62)a takes values from {0  1  . . .   k} for any a ∈ A and (cid:96)∗ ∼ Da∗. For i = 0  1  . . .   k  deﬁne
P (i) = Prob[(cid:96)∗(cid:62)a = i] and P (cid:48)(i) = Prob[ˆ(cid:96)(cid:62)a = i]  where (cid:96)∗ ∼ Da∗ and ˆ(cid:96) ∼ Dˆa. Then  from the
deﬁnition  the KL divergence can be expressed as follows:

(cid:19)
k(cid:88)
|P (cid:48)(i) − P (i)|/P (i) ≤ 1/2 holds 4 and the last equality holds  as we have(cid:80)k
where the inequality comes from the fact that log(1 + x) ≥ x − 2x2 for |x| ≤ 1/2 and
(cid:80)k
=
i=0(P (cid:48)(i) − P (i)) = 1 − 1 = 0. Thereby  it sufﬁces to bound (P (cid:48)(i) − P (i))2/P (i) for deriving
an upper bound on the KL divergence. We can then show that P (i) = Ω(1/k) for all i. Indeed  if
ε = 0  then we have P (i) = 1/(k + 1); as Prob[(cid:96)ti = 1] = Prob[ui ≥ u0] from the deﬁnition (8)
 = Prob(cid:2) u0 is the (i + 1)-th smallest among {uj}k
of Da  and as each ui is a uniform random variable over [0  1] under the condition of ε = 0  we have

P (cid:48)(i)
P (i)
P (cid:48)(i) − P (i)

(cid:18) P (cid:48)(i) − P (i)

i=0 P (i) P (cid:48)(i)−P (i)

 k(cid:88)

(P (cid:48)(i) − P (i))2

P (cid:48)(i) − P (i)

(cid:19)2(cid:33)

(cid:3) =

P (i) = Prob

P (i) log

P (i) log

1 +

(cid:96)tj = i

(cid:32)

i=0

− 2

i=0

i=0

= 2

i=0

P (i)

 

P (i)

P (i)

P (i)

P (i)

P (i)

j=0

1

 

k + 1

j=1

4 The statement |P (cid:48)(i) − P (i)|/P (i) ≤ 1/2 comes from ε ≤ 2−16. See Appendix A for details.

7

(cid:16)

i=0 (P (cid:48)(i) − P (i))2(cid:17)
k(cid:80)k

where the last equality comes from the fact that u0  u1  . . .   uk are i.i.d. random variables. Even
if ε > 0  we show in Appendix A that for ε ≤ 2−16  P (i) is sufﬁciently close to 1
k+1 to have an
order of Ω(1/k). Thus  we have P (i) = Ω(1/k) for all i = 1  . . .   k and ε ∈ [0  2−16]  and hence 
KL((cid:96)∗(cid:62)a||ˆ(cid:96)(cid:62)a) = O
. Finally  by proving |P (cid:48)(i) − P (i)| = O(ε/k2) 
we obtain the following lemma:
Lemma 2. Let a∗  ˆa ∈ {0  1}d and (cid:96)∗ ∼ Da∗   ˆ(cid:96) ∼ Dˆa. Then  for ε ∈ [0  2−16] and a ∈ {0  1}d
satisfying (cid:107)a(cid:107)1 = k and ˆa(cid:62)a − ˆa∗(cid:62)a = 1  we have
KL((cid:96)∗(cid:62)a||ˆ(cid:96)(cid:62)a) = O

(cid:18) ε2

(cid:19)

(9)

.

k2 +
The complete proof of this lemma is provided in Appendix A.

ε4
k3/2

Improved lower bound for the multitask bandit problem

4.3
We obtain an improved lower bound for A deﬁned as (1)  by combining Observation 1 and Lemma 2.
From Lemma 2  if ε ≤ 2−16k− 1
4   there is a global constant C for which (7) holds with CD =
T }  we obtain E[RT ] = Ω(εkT ) =
(C/k)2. Consequently  setting ε = min{2−16k− 1
dk3T})  which provides the lower bound in Theorem 1  for A given by (1)  i.e.  the
Ω(min{k 3
√
multitask bandit problem. The key point for shaving off the
log T factor is that our probabilistic
distribution presented in Section 4.2 satisﬁes (7) with CD = O(1/k2)  while the previous work [8]
does not exceed CD = O(log T /k2).

(cid:113) dk

4   1
4C

4 T 

√

4.4

Improved and extended lower bound for the bandit ranking problem

√

For the bandit ranking problem  Cohen et al. [8] have identiﬁed lower bounds by considering
(cid:96)t ∼ Da∗ for a∗ ∈ A  similar to the multitask bandit problem. However  this approach does not
√
work well for the case of full permutations (i.e.  with k = n)  and has left an Ω(
n)-gap between the
lower and the upper bounds  as mentioned in the conclusion of this research work.
n)-gap by improving the lower bound by a surprisingly simple approach.
We can eliminate this Ω(
In contrast to the probability distribution considered by Cohen et al. [8] that has k good arms (i such
i = 1)  we deﬁne the probability distribution with m = (cid:100)k/2(cid:101) good arms  i.e.  we consider
that a∗
a∗ ∈ A(cid:48) ⊆ {0  1}d deﬁned by

a ∈ {0  1}d
(cid:112)d/(32CDkT ) that satisﬁes (4) and (7). Suppose n ≥ 2 and 1 ≤ k ≤ n.

(10)
Lemma 3. Suppose a family {Da∗ | a∗ ∈ {0  1}d} of distributions with a parameter ε ≤
If a∗ is chosen
from A(cid:48) deﬁned by (10)  and (cid:96)t follows Da∗ for t = 1  2  . . .   T   independently  then  for the bandit
ranking problem deﬁned by (2)  any algorithm suffers regret of E[RT ] = Ω(εkT ).

(1 ≤ j ≤ m)
(m < j ≤ k)

a(i−1)n+j ≤ 1

 .

(cid:26) 1

(j ∈ [n])

jn(cid:88)

k(cid:88)

i=(j−1)n+1

A(cid:48) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ai =

0

 

i=1

The proof of this lemma is provided in Appendix B.
The lower bound in Lemma 3 is valid even if k = n  while the approach of the previous work [8]
considering a∗ ∈ A is applicable only to the case of n ≥ 2k. Intuitively  this difference can be
explained as follows: the regret depends on the number of good arms (i ∈ [d] such that a∗
i = 1)
in chosen arms (i ∈ [d] such that ati = 1). If a∗ and at are chosen from A with k = n  and if the
chosen arms (deﬁned by at) include k − 1 good arms  then the chosen arms automatically include
the entire [k] good arms  because a∗ and at express edge sets of perfect matchings of the complete
bipartite graph Kk k. This means that  in this setting  the probability of choosing several good arms
strongly affects that of choosing other good arms  which makes the analysis difﬁcult. However  such
an effect can be reduced if a∗ is chosen from A(cid:48)  i.e.  a∗ has only m = (cid:100)k/2(cid:101) good arms.
The lower bound in Theorem 1 for the bandit ranking problem  i.e.  A given by (2)  can be derived in
the same way as in Section 4.3. This accomplishes the proof of Theorem 1.

8

5 Conclusion

√

In this study  we considered the regret bounds of the bandit combinatorial optimization. As a result 
we managed to improve the regret lower regret bounds comparing with those presented in the existing
study [8] by applying a factor of
log T . The obtained lower bounds apply to three practically
important examples of the bandit combinatorial optimization  and are valid under the parameter
constraints milder than those outlined in the existing studies. In particular  the bound for the bandit
ranking obtained in the present study addresses an open problem outlined in [8]. To shave off
log T
factor  we have introduced a novel class of distributions  which could be potentially used to improve
regret lower bounds considering other problems. Moreover  by obtaining a lower regret bound under
the assumption of independent losses  we demonstrated that correlation among losses is the cause of
observing a large regret.
With respect to the bandit combinatorial optimization  we decreased the gap between the upper and
the lower bounds to O(log(ed/k)). We will consider this issue as an open question for the future
research  in which we will aim to improve the gap to a constant factor only.

√

References
[1] N. Abe and P. M. Long. Associative reinforcement learning using linear probabilistic concepts.

In International Conference on Machine Learning  pages 3–11  1999.

[2] J.-Y. Audibert  S. Bubeck  and G. Lugosi. Regret in online combinatorial optimization. Mathe-

matics of Operations Research  39(1):31–45  2013.

[3] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3(Nov):397–422  2002.

[4] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[5] B. Awerbuch and R. D. Kleinberg. Adaptive routing with end-to-end feedback: Distributed
learning and geometric approaches. In Proceedings of the Thirty-sixth Annual ACM Symposium
on Theory of Computing  pages 45–53. ACM  2004.

[6] S. Bubeck  N. Cesa-Bianchi  and S. Kakade. Towards minimax policies for online linear
optimization with bandit feedback. In Conference on Learning Theory  volume 23  pages
41.1–41.14  2012.

[7] N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences  78(5):1404–1422  2012.

[8] A. Cohen  T. Hazan  and T. Koren. Tight bounds for bandit combinatorial optimization. In

Conference on Learning Theory  pages 629–642  2017.

[9] R. Combes  M. S. T. M. Shahi  A. Proutiere  and M. Lelarge. Combinatorial bandits revisited.

In Advances in Neural Information Processing Systems  pages 2116–2124  2015.

[10] V. Dani  S. M. Kakade  and T. P. Hayes. The price of bandit information for online optimization.

In Advances in Neural Information Processing Systems  pages 345–352  2008.

[11] A. György  T. Linder  G. Lugosi  and G. Ottucsák. The on-line shortest path problem under

partial monitoring. Journal of Machine Learning Research  8(Oct):2369–2403  2007.

[12] E. Hazan and Z. Karnin. Volumetric spanners: an efﬁcient exploration basis for learning. The

Journal of Machine Learning Research  17(1):4062–4095  2016.

[13] D. P. Helmbold and M. K. Warmuth. Learning permutations with exponential weights. Journal

of Machine Learning Research  10(Jul):1705–1736  2009.

[14] S. Ito  D. Hatano  H. Sumita  K. Takemura  T. Fukunaga  N. Kakimura  and K. Kawarabayashi.
Oracle-efﬁcient algorithms for online linear optimization with bandit feedback. In Advances in
Neural Information Processing Systems  2019  to appear.

9

[15] J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian Journal of

Mathematics  12(363-366):234  1960.

[16] J. Komiyama  J. Honda  and H. Nakagawa. Optimal regret analysis of thompson sampling in
stochastic multi-armed bandit problem with multiple plays. In International Conference on
Machine Learning  pages 1152–1161  2015.

[17] W. Kotłowski and G. Neu. Bandit principal component analysis. In Proceedings of the Thirty-

Second Conference on Learning Theory  volume 99  pages 1994–2024  2019.

[18] P. Lagrée  C. Vernade  and O. Cappe. Multiple-play bandits in the position-based model. In

Advances in Neural Information Processing Systems  pages 1597–1605  2016.

[19] T. Lattimore and C. Szepesvári. Bandit Algorithms. preprint  Revision: 1699  2019.

[20] T. Lattimore  B. Kveton  S. Li  and C. Szepesvari. Toprank: A practical algorithm for online

stochastic ranking. In Advances in Neural Information Processing Systems  2018.

[21] H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an
adaptive adversary. In International Conference on Computational Learning Theory  pages
109–123  2004.

[22] S. Sakaue  M. Ishihata  and S.-i. Minato. Efﬁcient bandit combinatorial optimization algorithm
In International Conference on Artiﬁcial

with zero-suppressed binary decision diagrams.
Intelligence and Statistics  pages 585–594  2018.

[23] T. Uchiya  A. Nakamura  and M. Kudo. Algorithms for adversarial bandit problems with
multiple plays. In International Conference on Algorithmic Learning Theory  pages 375–389 
2010.

[24] M. Valko  R. Munos  B. Kveton  and T. Kocák. Spectral bandits for smooth graph functions. In

International Conference on Machine Learning  pages 46–54  2014.

10

,Ali Ahmed
Alireza Aghasi
Paul Hand
Shinji Ito
Daisuke Hatano
Hanna Sumita
Kei Takemura
Takuro Fukunaga
Naonori Kakimura
Ken-Ichi Kawarabayashi