2009,Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing,The replica method is a non-rigorous but widely-used technique from statistical physics used in the asymptotic analysis of many large random nonlinear problems.  This paper applies the replica method to non-Gaussian MAP estimation.  It is shown that with large random linear measurements and Gaussian noise  the asymptotic behavior of the MAP estimate of an n-dimensional vector ``decouples as n scalar MAP estimators.  The result is a counterpart to Guo and Verdus replica analysis on MMSE estimation. The replica MAP analysis can be readily applied to many estimators used in compressed sensing  including basis pursuit  lasso  linear estimation with thresholding and zero-norm estimation.  In the case of lasso estimation  the scalar estimator reduces to a soft-thresholding operator and for zero-norm estimation it reduces to a hard-threshold.  Among other benefits  the replica method provides a computationally tractable method for exactly computing various performance metrics including MSE and sparsity recovery.,Asymptotic Analysis of MAP Estimation via the

Replica Method and Compressed Sensing∗

Sundeep Rangan

Qualcomm Technologies

Bedminster  NJ

Alyson K. Fletcher

University of California  Berkeley

Berkeley  CA

Vivek K Goyal

Mass. Inst. of Tech.

Cambridge  MA

srangan@qualcomm.com

alyson@eecs.berkeley.edu

vgoyal@mit.edu

Abstract

The replica method is a non-rigorous but widely-accepted technique from statis-
tical physics used in the asymptotic analysis of large  random  nonlinear prob-
lems. This paper applies the replica method to non-Gaussian maximum a pos-
teriori (MAP) estimation. It is shown that with random linear measurements and
Gaussian noise  the asymptotic behavior of the MAP estimate of an n-dimensional
vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo
and Verd´u’s replica analysis of minimum mean-squared error estimation.
The replica MAP analysis can be readily applied to many estimators used in
compressed sensing  including basis pursuit  lasso  linear estimation with thresh-
olding  and zero norm-regularized estimation.
In the case of lasso estimation
the scalar estimator reduces to a soft-thresholding operator  and for zero norm-
regularized estimation it reduces to a hard-threshold. Among other beneﬁts  the
replica method provides a computationally-tractable method for exactly comput-
ing various performance metrics including mean-squared error and sparsity pat-
tern recovery probability.

1 Introduction

Estimating a vector x ∈ Rn from measurements of the form
(1)
where Φ ∈ Rm×n represents a known measurement matrix and w ∈ Rm represents measurement
errors or noise  is a generic problem that arises in a range of circumstances. One of the most basic
estimators for x is the maximum a posteriori (MAP) estimate

y = Φx + w 

ˆxmap(y) = arg max

x∈Rn

px|y(x|y) 

(2)

which is deﬁned assuming some prior on x. For most priors  the MAP estimate is nonlinear and its
behavior is not easily characterizable. Even if the priors for x and w are separable  the analysis of
the MAP estimate may be difﬁcult since the matrix Φ couples the n unknown components of x with
the m measurements in the vector y.
The primary contribution of this paper—an abridged version of [1]—is to show that with certain
large random Φ and Gaussian w  there is an asymptotic decoupling of (1) into n scalar MAP estima-
tion problems. Each equivalent scalar problem has an appropriate scalar prior and Gaussian noise
with an effective noise level. The analysis yields the asymptotic joint distribution of each component
xj of x and its corresponding estimate ˆxj in the MAP estimate vector ˆxmap(y). From the joint
distribution  various further computations can be made  such as the mean-squared error (MSE) of
the MAP estimate or the error probability of a hypothesis test computed from the MAP estimate.

∗This work was supported in part by a University of California President’s Postdoctoral Fellowship and

National Science Foundation CAREER Award 0643836.

1

Replica Method. Our analysis is based on a powerful but non-rigorous technique from statistical
physics known as the replica method. The replica method was originally developed by Edwards and
Anderson [2] to study the statistical mechanics of spin glasses. Although not fully rigorous from the
perspective of probability theory  the technique was able to provide explicit solutions for a range of
complex problems where many other methods had previously failed [3].

The replica method was ﬁrst applied to the study of nonlinear MAP estimation problems by
Tanaka [4] and M¨uller [5]. These papers studied the behavior of the MAP estimator of a vector
x with i.i.d. binary components observed through linear measurements of the form (1) with a large
random Φ and Gaussian w. The results were then generalized in a remarkable paper by Guo and
Verd´u [6] to vectors x with arbitrary distributions. Guo and Verd´u’s result was also able to incor-
porate a large class of minimum postulated MSE estimators  where the estimator may assume a
prior that is different from the actual prior. The main result in this paper is the corresponding MAP
statement to Guo and Verd´u’s result. In fact  our result is derived from Guo and Verd´u’s by taking
appropriate limits with large deviations arguments.

The non-rigorous aspect of the replica method involves a set of assumptions that include a self-
averaging property  the validity of a “replica trick ” and the ability to exchange certain limits. Some
progress has been made in formally proving these assumptions; a survey of this work can be found
in [7]. Also  some of the predictions of the replica method have been validated rigorously by other
means [8]. To emphasize our dependence on these unproven assumptions  we will refer to Guo and
Verd´u’s result as the Replica MMSE Claim. Our main result  which depends on Guo and Verd´u’s
analysis  will be called the Replica MAP Claim.

Applications to Compressed Sensing. As an application of our main result  we will develop a few
analyses of estimation problems that arise in compressed sensing [9–11]. In compressed sensing 
one estimates a sparse vector x from random linear measurements. Generically  optimal estimation
of x with a sparse prior is NP-hard [12]. Thus  most attention has focused on greedy heuristics such
as matching pursuit and convex relaxations such as basis pursuit [13] or lasso [14]. While successful
in practice  these algorithms are difﬁcult to analyze precisely.

Recent compressed sensing research has provided scaling laws on numbers of measurements that
guarantee good performance of these methods [15–17]. However  these scaling laws are in general
conservative. There are  of course  notable exceptions including [18] and [19] which provide match-
ing necessary and sufﬁcient conditions for recovery of strictly sparse vectors with basis pursuit and
lasso. However  even these results only consider exact recovery and are limited to measurements
that are noise-free or measurements with a signal-to-noise ratio (SNR) that scales to inﬁnity.

Many common sparse estimators can be seen as MAP estimators with certain postulated priors.
Most importantly  lasso and basis pursuit are MAP estimators assuming a Laplacian prior. Other
commonly-used sparse estimation algorithms  including linear estimation with and without thresh-
olding and zero norm-regularized estimators  can also be seen as MAP-based estimators. For these
algorithms  the replica method provides—under the assumption of the replica hypotheses—not just
bounds  but the exact asymptotic behavior. This in turns permits exact expressions for various per-
formance metrics such as MSE or fraction of support recovery. The expressions apply for arbitrary
ratios k/n  n/m and SNR.

2 Estimation Problem and Assumptions

Consider the estimation of a random vector x ∈ Rn from linear measurements of the form

(3)
where y ∈ Rm is a vector of observations  Φ = AS1/2  A ∈ Rm×n is a measurement matrix  S is
a diagonal matrix of positive scale factors 

y = Φx + w = AS1/2x + w 

S = diag (s1  . . .   sn)   sj > 0 

(4)
and w ∈ Rm is zero-mean  white Gaussian noise. We consider a sequence of such problems indexed
y knowing the measurement matrix A and scale factor matrix S.

by n  with n → ∞. For each n  the problem is to determine an estimatebx of x from the observations

2

The components xj of x are modeled as zero mean and i.i.d. with some prior probability distribution
0. We use the subscript
p0(xj ). The per-component variance of the Gaussian noise is E|wj|2 = σ2
“0” on the prior and noise level to differentiate these quantities from certain “postulated” values to
be deﬁned later.
In (3)  we have factored Φ = AS1/2 so that even with the i.i.d. assumption on xjs above and an
i.i.d. assumption on entries of A  the model can capture variations in powers of the components of
x that are known a priori at the estimator. Variations in the power of x that are not known to the
estimator should be captured in the distribution of x.

We summarize the situation and make additional assumptions to specify the problem precisely as
follows:

(a) The number of measurements m = m(n) is a deterministic quantity that varies with n and

satisﬁes

lim
n→∞

n/m(n) = β

for some β ≥ 0. (The dependence of m on n is usually omitted for brevity.)

(b) The components xj of x are i.i.d. with probability distribution p0(xj ).
(c) The noise w is Gaussian with w ∼ N (0  σ2
(d) The components of the matrix A are i.i.d. zero mean with variance 1/m.
(e) The scale factors sj are i.i.d. and satisfy sj > 0 almost surely.
(f) The scale factor matrix S  measurement matrix A  vector x and noise w are independent.

0Im).

3 Review of the Replica MMSE Claim

We begin by reviewing the Replica MMSE Claim of Guo and Verd´u [6]. Suppose one is given a
post that may be different from
“postulated” prior distribution ppost and a postulated noise level σ2
the true values p0 and σ2

0. We deﬁne the minimum postulated MSE (MPMSE) estimate of x as

where C is a normalization constant.
The Replica MMSE Claim describes the asymptotic behavior of the postulated MMSE estimator via
an equivalent scalar estimator. Let q(x) be a probability distribution deﬁned on some set X ⊆ R.
Given µ > 0  let px|z(x | z ; q  µ) be the conditional distribution
φ(z − x ; µ)q(x) dx(cid:21)−1

px|z(x | z ; q  µ) =(cid:20)Zx∈X

φ(z − x ; µ)q(x)

(6)

where φ(·) is the Gaussian distribution

φ(v ; µ) =

1

√2πµ

e−|v|2/(2µ).

(7)

The distribution px|z(x|z ; q  µ) is the conditional distribution of the scalar random variable x ∼
q(x) from an observation of the form
(8)
where v ∼ N (0  1). Using this distribution  we can deﬁne the scalar conditional MMSE estimate 
(9)

z = x + √µv 

ˆxmmse

scalar(z ; q  µ) =Zx∈X

xpx|z(x|z ; µ) dx.

3

where px|y(x | y ; q  σ2) is the conditional distribution of x given y under the x distribution and
noise variance speciﬁed as parameters after the semicolon:

ˆxmpmse(y) = E(cid:0)x | y ; ppost  σ2

px|y(x | y ; q  σ2) = C−1 exp(cid:18)−

post) dx 

post(cid:1) =Z xpx|y(x | y ; ppost  σ2
2σ2ky − AS1/2xk2(cid:19) q(x) 

1

q(x) =

q(xj ) 

(5)

nYj=1

Also  given two distributions  p0(x) and p1(x)  and two noise levels  µ0 > 0 and µ1 > 0  deﬁne

mse(p1  p0  µ1  µ0  z) =Zx∈X |x − ˆxmmse

scalar(z ; p1  µ1)|2px|z(x | z ; p0  µ0) dx 

(10)

which is the mean-squared error in estimating the scalar x from the variable z in (8) when x has a
true distribution x ∼ p0(x) and the noise level is µ = µ0  but the estimator assumes a distribution
x ∼ p1(x) and noise level µ = µ1.
Replica MMSE Claim [6]. Consider the estimation problem in Section 2. Let ˆxmpmse(y) be the
MPMSE estimator based on a postulated prior ppost and postulated noise level σ2
post. For each
n  let j = j(n) be some deterministic component index with j(n) ∈ {1  . . .   n}. Then there exist
effective noise levels σ2

eﬀ and σ2

p−eﬀ such that:

(a) As n → ∞  the random vectors (xj   sj  ˆxmpmse

) converge in distribution to the random
vector (x  s  ˆx) where x  s  and v are independent with x ∼ p0(x)  s ∼ pS(s)  v ∼ N (0  1) 
and
(11)

ˆx = ˆxmmse

j

scalar(z ; ppost  µp)  z = x + √µv.
p−eﬀ /s.

where µ = σ2

eﬀ /s and µp = σ2

(b) The effective noise levels satisfy the equations

σ2
eﬀ = σ2
σ2
p−eﬀ = σ2

0 + βE [s mse(ppost  p0  µp  µ  z)]
post + βE [s mse(ppost  ppost  µp  µp  z)]  
where the expectations are taken over s ∼ pS(s) and z generated by (11).

(12a)
(12b)

The Replica MMSE Claim asserts that the asymptotic behavior of the joint estimation of the n-
dimensional vector x can be described by n equivalent scalar estimators. In the scalar estimation
problem  a component x ∼ p0(x) is corrupted by additive Gaussian noise yielding a noisy mea-
eﬀ /s  which is the effective noise divided by the
surement z. The additive noise variance is µ = σ2
scale factor s. The estimate of that component is then described by the (generally nonlinear) scalar
estimator ˆx(z ; ppost  µp).
p−eﬀ are described by the solutions to ﬁxed-point equations
The effective noise levels σ2
(12). Note that σ2
p−eﬀ appear implicitly on the left- and right-hand sides of these equations
via the terms µ and µp. When there are multiple solutions to these equations  the true solution is the
minimizer of a certain Gibbs’ function [6].

eﬀ and σ2

eﬀ and σ2

4 Replica MAP Claim

We now turn to MAP estimation. Let X ⊆ R be some (measurable) set and consider an estimator of
the form

ˆxmap(y) = arg min
x∈X n

1
2γky − AS1/2xk2

2 +

f (xj ) 

(13)

nXj=1

where γ > 0 is an algorithm parameter and f : X → R is some scalar-valued  non-negative cost
function. We will assume that the objective function in (13) has a unique essential minimizer for
almost all y.

The estimator (13) can be interpreted as a MAP estimator. Speciﬁcally  for any u > 0  it can be
veriﬁed that ˆxmap(y) is the MAP estimate

ˆxmap(y) = arg max

x∈X n

px|y(x | y ; pu  σ2
u) 

where pu(x) and σ2

u are the prior and noise level

pu(x) =(cid:20)Zx∈X n

exp(−uf (x))dx(cid:21)−1

4

exp(−uf (x))  σ2

u = γ/u 

(14)

estimators

where f (x) = Pj f (xj). To analyze this MAP estimator  we consider a sequence of MMSE

(15)
The proof of the Replica MAP Claim below (see [1]) uses a standard large deviations argument to
show that

u(cid:1) .
bxu(y) = E(cid:0)x | y ; pu  σ2
u→∞bxu(y) = ˆxmap(y)

lim

for all y. Under the assumption that the behaviors of the MMSE estimators are described by the
Replica MMSE Claim  we can then extrapolate the behavior of the MAP estimator.

To state the claim  deﬁne the scalar MAP estimator

ˆxmap
scalar(z ; λ) = arg min

x∈X

F (x  z  λ)  F (x  z  λ) =

1
2λ|z − x|2 + f (x).

(16)

where  again  we assume that (16) has a unique essential minimizer for almost all λ and almost all
z. We also assume that the limit

σ2(z  λ) = lim
x→ˆx

|x − ˆx|2

2(F (x  z  λ) − F (ˆx  z  λ))

 

(17)

exists where ˆx = ˆxmap

scalar(z; λ). We make the following additional assumptions:

Assumption 1 Consider the MAP estimator (13) applied to the estimation problem in Section 2.
Assume:

(a) For all u > 0 sufﬁciently large  assume the postulated prior pu and noise level σ2

u satisfy
the Replica MMSE Claim. Also  assume that for the corresponding effective noise levels 
σ2
eﬀ (u) and σ2

p−eﬀ (u)  the following limits exists:

σ2
eﬀ map = lim
u→∞

σ2
eﬀ (u)  γp = lim
u→∞

uσ2

p−eﬀ(u).

(b) Suppose for each n  ˆxu

j ∈ {1  . . .   n} based on the postulated prior pu and noise level σ2
following limits can be interchanged:

j (n) is the MMSE estimate of the component xj for some index
u. Then  assume that the

lim
n→∞
where the limits are in distribution.

lim
u→∞

ˆxu
j (n) = lim
n→∞

lim
u→∞

ˆxu
j (n) 

(c) Assume that f (x) is non-negative and satisﬁes f (x)/ log |x| → ∞ as |x| → ∞.

Item (a) is stated to reiterate that we are assuming the Replica MMSE Claim is valid. See [1  Sect.
IV] for additional discussion of technical assumptions.
Replica MAP Claim [1]. Consider the estimation problem in Section 2. Let ˆxmap(y) be the MAP
estimator (13) deﬁned for some f (x) and γ > 0 satisfying Assumption 1. For each n  let j = j(n)
be some deterministic component index with j(n) ∈ {1  . . .   n}. Then:

(a) As n → ∞  the random vectors (xj  sj  ˆxmap

) converge in distribution to the random
vector (x  s  ˆx) where x  s  and v are independent with x ∼ p0(x)  s ∼ pS(s)  v ∼ N (0  1) 
and
(18)

scalar(z  λp)  z = x + √µv 

j

where µ = σ2

ˆx = ˆxmap
eﬀ map/s and λp = γp/s.

(b) The limiting effective noise levels σ2

eﬀ map and γp satisfy the equations

σ2
eﬀ map = σ2

(19a)
(19b)
where the expectations are taken over x ∼ p0(x)  s ∼ pS(s)  and v ∼ N (0  1)  with ˆx and
z deﬁned in (18).

0 + βE(cid:2)s|x − ˆx|2(cid:3)
γp = γ + βE(cid:2)sσ2(z  λp)(cid:3)  

5

Analogously to the Replica MMSE Claim  the Replica MAP Claim asserts that asymptotic behavior
of the MAP estimate of any single component of x is described by a simple equivalent scalar esti-
mator. In the equivalent scalar model  the component of the true vector x is corrupted by Gaussian
noise and the estimate of that component is given by a scalar MAP estimate of the component from
the noise-corrupted version.

5 Analysis of Compressed Sensing

Our results thus far hold for any separable distribution for x and under mild conditions on the cost
function f . The role of f is to determine the estimator. In this section  we ﬁrst consider choices of
f that yield MAP estimators relevant to compressed sensing. We then additionally impose a sparse
prior for x for numerical evaluations of asymptotic performance.

Lasso Estimation. We ﬁrst consider the lasso or basis pursuit estimate [13  14] given by

ˆxlasso(y) = arg min

x∈Rn

1
2γky − AS1/2xk2

2 + kxk1 

(20)

where γ > 0 is an algorithm parameter. This estimator is identical to the MAP estimator (13) with
the cost function

With this cost function  the scalar MAP estimator in (16) is given by

f (x) = |x|.

where T soft

λ

(z) is the soft thresholding operator

ˆxmap
scalar(z ; λ) = T soft

λ

(z) 

T soft
λ

(z) =( z − λ 

0 
z + λ 

if z > λ;
if |z| ≤ λ;
if z < −λ.

(21)

(22)

eﬀ map and γp such that
The Replica MAP Claim now states that there exists effective noise levels σ2
for any component index j  the random vector (xj  sj  ˆxj ) converges in distribution to the vector
(x  s  ˆx) where x ∼ p0(x)  s ∼ pS(s)  and ˆx is given by

ˆx = T soft
λp

(z) 

z = x + √µv 

(23)

eﬀ map/s. Hence  the asymptotic behavior of lasso
where v ∼ N (0  1)  λp = γp/s  and µ = σ2
has a remarkably simple description: the asymptotic distribution of the lasso estimate ˆxj of the
component xj is identical to xj being corrupted by Gaussian noise and then soft-thresholded to
yield the estimate ˆxj.
To calculate the effective noise levels  one can perform a simple calculation to show that σ2(z  λ) in
(17) is given by

σ2(z  λ) =(cid:26) λ 

0 

if |z| > λ;
if |z| ≤ λ.

(24)

Hence 

(25)
where we have use the fact that λp = γp/s. Substituting (21) and (25) into (19)  we obtain the
ﬁxed-point equations

E(cid:2)sσ2(z  λp)(cid:3) = E [sλp Pr(|z| > λp)] = γp Pr(|z| > γp/s)

σ2
eﬀ map = σ2

0 + βEhs|x − T soft

λp

(z)|2i

(26a)

(26b)
where the expectations are taken with respect to x ∼ p0(x)  s ∼ pS(s)  and z in (23). Again  while
these ﬁxed-point equations do not have a closed-form solution  they can be relatively easily solved
numerically given distributions of x and s.

γp = γ + βγp Pr(|z| > γp/s) 

6

Zero Norm-Regularized Estimation. Lasso can be regarded as a convex relaxation of zero norm-
regularized estimation

ˆxzero(y) = arg min

x∈Rn

1
2γky − AS1/2xk2

2 + kxk0 

(27)

where kxk0 is the number of nonzero components of x. For certain strictly sparse priors  zero
norm-regularized estimation may provide better performance than lasso. While computing the zero
norm-regularized estimate is generally very difﬁcult  we can use the replica analysis to provide a
simple characterization of its performance. This analysis can provide a bound on the performance
achievable by practical algorithms.

The zero norm-regularized estimator is identical to the MAP estimator (13) with the cost function

f (x) =(cid:26) 0 

1 

if x = 0;
if x 6= 0.

(28)

Technically  this cost function does not satisfy the conditions of the Replica MAP Claim. To avoid
this problem  we can consider an approximation of (28) 

fδ M (x) =(cid:26) 0 

1 

if |x| < δ;
if |x| ∈ [δ  M ] 

which is deﬁned on the set X = {x : |x| ≤ M}. We can then take the limits δ → 0 and M → ∞.
To simplify the presentation  we will just apply the Replica MAP Claim with f (x) in (28) and omit
the details in taking the appropriate limits.

With f (x) given by (28)  the scalar MAP estimator in (16) is given by
t = √2λ 

ˆxmap
scalar(z ; λ) = T hard

(z) 

t

where T hard

t

is the hard thresholding operator 

T hard
t

(z) =(cid:26) z 

0 

if |z| > t;
if |z| ≤ t.

(29)

(30)

Now  similar to the case of lasso estimation  the Replica MAP Claim states there exists effective
eﬀ map and γp such that for any component index j  the random vector (xj   sj  ˆxj )
noise levels σ2
converges in distribution to the vector (x  s  ˆx) where x ∼ p0(x)  s ∼ pS(s)  and ˆx is given by

(31)

(32)

where v ∼ N (0  1)  λp = γp/s  µ = σ2

eﬀ map/s  and

ˆx = T hard

t

(z) 

z = x + √µv 

t =p2λp =q2γp/s.

Thus  the zero norm-regularized estimation of a vector x is equivalent to n scalar components cor-
rupted by some effective noise level σ2
eﬀ map and hard-thresholded based on a effective noise level
γp.
eﬀ map and γp can be computed similarly to
The ﬁxed-point equations for the effective noise levels σ2
the case of lasso. Speciﬁcally  one can verify that (24) and (25) are both satisﬁed for the hard thresh-
olding operator as well. Substituting (25) and (29) into (19)  we obtain the ﬁxed-point equations

σ2
eﬀ map = σ2

(33a)
(33b)
where the expectations are taken with respect to x ∼ p0(x)  s ∼ pS(s)  z in (31)  and t given by
(32). These ﬁxed-point equations can be solved numerically.

γp = γ + βγp Pr(|z| > t) 

0 + βE(cid:2)s|x − T hard

(z)|2(cid:3)  

t

7

0

−2

−4

−6

−8

−10

−12

−14

−16

)

B
d
(
 
r
o
r
r
e
d
e
r
a
u
q
s
 

 

i

n
a
d
e
M

−18

 

0.5

Linear   
(replica)
Linear
(sim.)
Lasso    
(replica)
Lasso 
(sim.)
Zero    
norm−reg
Optimal
MMSE   

 

3

1
2.5
Measurement ratio β = n/m

1.5

2

Figure 1: MSE performance prediction with the Replica MAP Claim. Plotted is the median nor-
malized SE for various sparse recovery algorithms: linear MMSE estimation  lasso  zero norm-
regularized estimation  and optimal MMSE estimation. Solid lines show the asymptotic predicted
MSE from the Replica MAP Claim. For the linear and lasso estimators  the circles and triangles
show the actual median SE over 1000 Monte Carlo simulations.

Numerical Simulation. To validate the predictive power of the Replica MAP Claim for ﬁnite
dimensions  we performed numerical simulations where the components of x are a zero-mean
Bernoulli–Gaussian process. Speciﬁcally 

xj ∼(cid:26) N (0  1)  with prob. 0.1;

0  with prob. 0.9.

We took the vector x to have n = 100 i.i.d. components  and we used ten values of m to vary β =
n/m from 0.5 to 3. For each problem size  we simulated the lasso and linear MMSE estimators over
1000 independent instances with noise levels chosen such that the SNR with perfect side information
is 10 dB. Each set of trials is represented by its median squared error in Fig. 1.

The simulated performance is matched very closely by the asymptotic values predicted by the replica
analysis. (Analysis of the linear MMSE estimator using the Replica MAP Claim is detailed in [1];
the Replica MMSE Claim is also applicable to this estimator.) In addition  the replica analysis can be
applied to zero norm-regularized and optimal MMSE estimators that are computationally infeasible
for large problems. These results are also shown in Fig. 1  illustrating the potential of the replica
method to quantify the precise performance losses of practical algorithms.

Additional numerical simulations in [1] illustrate convergence to the replica MAP limit  applicability
to discrete distributions for x  effects of power variations in the components  and accurate prediction
of the probability of sparsity pattern recovery.

6 Conclusions

We have shown that the behavior of vector MAP estimators with large random measurement matri-
ces and Gaussian noise asymptotically matches that of a set of decoupled scalar estimation problems.
We believe that this equivalence to a simple scalar model will open up numerous doors for analysis 
particularly in problems of interest in compressed sensing. One can use the model to dramatically
improve upon existing performance analyses for sparsity pattern recovery and MSE. Also  the tech-
nique is sufﬁciently general to study effects of dynamic range.

8

References

[1] S. Rangan  A. K. Fletcher  and V. K. Goyal. Asymptotic analysis of MAP estimation via
the replica method and applications to compressed sensing. arXiv:0906.3234v1 [cs.IT].  June
2009.

[2] S. F. Edwards and P. W. Anderson. Theory of spin glasses. J. Phys. F: Metal Physics  5:965–

974  1975.

[3] H. Nishimori. Statistical physics of spin glasses and information processing: An introduction.

International Series of Monographs on Physics. Oxford Univ. Press  Oxford  UK  2001.

[4] T. Tanaka. A statistical-mechanics approach to large-system analysis of CDMA multiuser

detectors. IEEE Trans. Inform. Theory  48(11):2888–2910  November 2002.

[5] R. R. M¨uller. Channel capacity and minimum probability of error in large dual antenna array
systems with binary modulation. IEEE Trans. Signal Process.  51(11):2821–2828  November
2003.

[6] D. Guo and S. Verd´u. Randomly spread CDMA: Asymptotics via statistical physics. IEEE

Trans. Inform. Theory  51(6):1983–2010  June 2005.

[7] M. Talagrand. Spin Glasses: A Challenge for Mathematicians. Springer  New York  2003.
[8] A. Montanari and D. Tse. Analysis of belief propagation for non-linear problems: The example
of CDMA (or: How to prove Tanaka’s formula). arXiv:cs/0602028v1 [cs.IT].  February 2006.
[9] E. J. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. IEEE Trans. Inform. Theory  52(2):489–
509  February 2006.

[10] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory  52(4):1289–1306  April

2006.

[11] E. J. Cand`es and T. Tao. Near-optimal signal recovery from random projections: Universal

encoding strategies? IEEE Trans. Inform. Theory  52(12):5406–5425  December 2006.

[12] B. K. Natarajan. Sparse approximate solutions to linear systems.

24(2):227–234  April 1995.

SIAM J. Computing 

[13] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

J. Sci. Comp.  20(1):33–61  1999.

[14] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc.  Ser. B 

58(1):267–288  1996.

[15] D. L. Donoho  M. Elad  and V. N. Temlyakov. Stable recovery of sparse overcomplete repre-

sentations in the presence of noise. IEEE Trans. Inform. Theory  52(1):6–18  January 2006.

[16] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform.

Theory  50(10):2231–2242  October 2004.

[17] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.

IEEE Trans. Inform. Theory  52(3):1030–1051  March 2006.

[18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
ℓ1-constrained quadratic programming (lasso). IEEE Trans. Inform. Theory  55(5):2183–2202 
May 2009.

[19] D. L. Donoho and J. Tanner. Counting faces of randomly-projected polytopes when the pro-

jection radically lowers dimension. J. Amer. Math. Soc.  22(1):1–53  January 2009.

9

,Alessandro Rudi
Guillermo Canas
Lorenzo Rosasco
Jiezhang Cao
Langyuan Mo
Yifan Zhang
Kui Jia
Chunhua Shen
Mingkui Tan