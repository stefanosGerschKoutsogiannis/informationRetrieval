2019,Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces,In extreme classification settings  embedding-based neural network models are currently not competitive with sparse linear and tree-based methods in terms of accuracy. Most prior works attribute this poor performance to the low-dimensional bottleneck in embedding-based methods. In this paper  we demonstrate that theoretically there is no limitation to using low-dimensional embedding-based methods  and provide experimental evidence that overfitting is the root cause of the poor performance of embedding-based methods. These findings motivate us to investigate novel data augmentation and regularization techniques to mitigate overfitting. To this end  we propose GLaS  a new regularizer for embedding-based neural network approaches. It is a natural generalization from the graph Laplacian and spread-out regularizers  and empirically it addresses the drawback of each regularizer alone when applied to the extreme classification setup. With the proposed techniques  we attain or improve upon the state-of-the-art on most widely tested public extreme classification datasets with hundreds of thousands of labels.,Breaking the Glass Ceiling for Embedding-Based

Classiﬁers for Large Output Spaces

Chuan Guo⇤†

Cornell University

cg563@cornell.edu

Ali Mousavi⇤
Google Research

alimous@google.com

Xiang Wu†
ByteDance

xiang.wu@bytedance.com

Daniel Holtmann-Rice

Google Research
dhr@google.com

Satyen Kale

Google Research

satyenkale@google.com

Sashank Reddi
Google Research

sashank@google.com

Sanjiv Kumar
Google Research

sanjivk@google.com

Abstract

In extreme classiﬁcation settings  embedding-based neural network models are
currently not competitive with sparse linear and tree-based methods in terms of
accuracy. Most prior works attribute this poor performance to the low-dimensional
bottleneck in embedding-based methods. In this paper  we demonstrate that theo-
retically there is no limitation to using low-dimensional embedding-based methods 
and provide experimental evidence that overﬁtting is the root cause of the poor per-
formance of embedding-based methods. These ﬁndings motivate us to investigate
novel data augmentation and regularization techniques to mitigate overﬁtting. To
this end  we propose GLaS  a new regularizer for embedding-based neural network
approaches. It is a natural generalization from the graph Laplacian and spread-out
regularizers  and empirically it addresses the drawback of each regularizer alone
when applied to the extreme classiﬁcation setup. With the proposed techniques  we
attain or improve upon the state-of-the-art on most widely tested public extreme
classiﬁcation datasets with hundreds of thousands of labels.

1

Introduction

We study the problem of multi-label classiﬁcation with large output space  which has garnered
signiﬁcant attention in recent years [36  6  14  3  33  23]. This problem differs from the traditional
classiﬁcation setting insofar that the number of labels is potentially in the millions  presenting
signiﬁcant computational challenges. Many real world applications such as product recommendation
and text retrieval can be formulated under this framework and thus  practical solutions to this problem
can have signiﬁcant and far-reaching impact.
In this unusual yet practical setting  both the number of input feature dimensions D and the number of
labels K could be upwards of hundreds of thousands or even millions. This renders most traditional
machine learning models  such as logistic regression and SVM  infeasible due to excessive number
of model parameters — approximately O(DK). Most recent approaches resort to using sparse linear

⇤Equal Contribution
†Work done at Google

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

models or tree-based methods in order to tackle this challenge [29  23  24  34  33]. An alternate
approach to address this problem is through low-dimensional embeddings. Here  the model consists
of an embedding function  : RD ! Rd  where d is the embedding dimension  and a classiﬁer
f : Rd !{ 0  1}K. Thus  for any input x 2 RD  f ((x)) is the indicator vector or label vector of
the predicted labels. To handle a large number of labels  the embedding dimension d is chosen to be
small in comparison to D; thereby  signiﬁcantly reducing the number of model parameters.
Despite their accomplishments in computer vision and natural language processing domains [17  27] 
embedding-based deep neural networks (DNNs) have not achieved the same level of success in
learning with large output spaces. This point is often attributed to low-dimensional bottleneck layers
in neural networks that cannot represent enough information for the downstream learning task when
the number of potential labels is substantially larger than the embedding dimensionality [24  6  32].
Attempts to circumvent this limitation have been met with limited success [6  31]. As a result  sparse
linear models and tree-based methods are favored in comparison to embedding-based methods for
large-scale multi-label classiﬁcation problems.
In this paper  we investigate embedding-based methods for the problem of our interest. Our main
observation is that  contrary to the widespread belief of limited representation power  overﬁtting is
the cause for the inferior performance of embedding-based methods  which suggests that efforts to
either augment the training set or regularize the model may dramatically boost test set performance.
Inspired by this  we show that a number of regularization techniques can shrink the generalization gap
for embedding-based methods and allow them to achieve  or improve upon  state-of-the-art accuracy
on a variety of widely tested public datasets. The most discernible improvement comes from a novel
regularizer that promotes embeddings for frequently co-occurring labels to be close.
Contributions. In the light of this background  we state the following key contributions of this paper:
1. We demonstrate experimentally that the main reason for the poor performance of neural network
embedding-based models is overﬁtting. Our empirical observation is further supported by theoretical
analysis  where we prove that there exists a low-dimensional embedding-based linear classiﬁer with
perfect accuracy in the limit of inﬁnite expressivity of the embedding map. This shows that  contrary
to speculations in existing literature  low-dimensional embeddings are indeed sufﬁciently expressive
and cannot be a bottleneck.
2. Based on this ﬁnding  we propose a suite of principled data augmentation and regularization
techniques  including a novel regularizer called GLaS  to shrink the gap between training and test
performance.
3. Finally  on several widely tested public datasets  with our proposed techniques  we achieve state-
of-the-art results with very simple network architectures and little tuning. We achieve high precision
and propensity scores  thus demonstrating the effectiveness of our method even on infrequent tail
labels. We also provide an ablation study to highlight the effectiveness of each individual factor. This
provides a strong baseline and several new venues for future research on applying embedding-based
methods to the large output space setting.

1.1 Related Work

There is a vast amount of literature on text classiﬁcation; therefore  we only mention those that are
most relevant to the problem setting of our interest. Existing approaches to our problem setting can
be broadly classiﬁed into three categories: (i) Embedding-based methods  (ii) Tree-based methods
and (iii) Sparse and One-vs-all methods. We discuss these approaches brieﬂy here.
Embedding-based methods learn a model of the form f ((x)) where (x) 2 Rd and d is small.
Embedding methods mainly differ in their choice of the functional form and approaches to learn
the parameters of the function. A variety of approaches such as compressed sensing [12]  bloom
ﬁlter [10]  and SVD [36] are applied to train these models. While most of these approaches assume a
linear functional form [7  9  18  28]  non-linear forms have also been proposed [6]. One criticism of
embedding-based approaches is that label embeddings are compressed to a very small dimensionality
d  which is believed to cause degradation in performance greatly [24  6] and are thus  less favored for
large-scale settings.
Tree-based methods learn a hierarchical structure over the label space and predict the path from the
root to the target label [1  15  29  24  14  22  35]. While this greatly reduces inference time and the

2

number of parameters needed to be learnt  it typically comes at the cost of low prediction accuracy.
Although traditionally done over the label set [29]  more recent methods [24  14] partition the feature
space instead  relying on the assumption that only a small set of features are relevant for any label.
These methods are heavily affected by so-called cascading effect  where the prediction error at the
top cannot be corrected at a lower level.
Sparse and One-vs-all methods restrict the model capacity and improve efﬁciency by applying
sparse linear methods to learn only a small fraction of the non-zero parameters. This allows the
sparse model to be kept in main memory while ensuring that matrix-vector products can be carried
out efﬁciently. Methods such as DiSMEC [3]  ProXML [4]  PD-Sparse [34] and PPD-Sparse [33] are
representative of this strategy and have enjoyed great success recently. DiSMEC and PPD-Sparse
are  in particular  highly parallelizable since they are based on the one-vs-all approach for training
extreme multi-label classiﬁcation models. However  these models are typically simple linear models
and hence  do not capture complex non-linear relationships.

2 Discussion on Embedding-based Methods

In this section  we describe our problem setup more formally and investigate the validity of the
criticism on embedding-based methods. The general learning problem of multi-label classiﬁcation
can be deﬁned as follows. Given an input x 2X⇢ RD  its label y 2Y⇢{
0  1}K is a K-
dimensional vector with multiple non-zero entries  where y(k) = 1 if and only if label k is relevant
for input x. Let Ly denote the set of indices that are non-zero in y. The elements of the set Ly
are  hereafter  referred to as relevant labels in y. The number of distinct labels K is assumed to be
large (on the order of hundreds of thousands or even millions). The goal of all embedding-based
methods is to learn a model of the form f ((x)) : X!{ 0  1}K where (x) 2 Rd and d ⌧ D  K
and f : Rd !{ 0  1}K is a classiﬁer on top of the embedding.
The most common form of f is a linear classiﬁer. A linear classiﬁer is parameterized by a label
embedding matrix V 2 Rd⇥K which is used to predict scores for all labels by computing (x)>V.
V is called a label embedding matrix since its columns can be interpreted as embeddings of the K
labels in the same embedding space  Rd. In the following  for a label y  we will use the notation
vy to denote the embedding of y given by V  i.e the y-th column of V. Depending on the speciﬁc
formulation  the set of labels predicted for the input x can then be obtained by thresholding the scores
at some value ⌧  i.e.  {y : (x)>vy  ⌧} or taking the top m largest scores  i.e.  Top((x)>V  m).
The use of a linear classiﬁer on top of embeddings naturally leads to a low-rank structure for the
score vectors of the labels: the set {(x)>V : x 2X} has rank at most d. This restriction on the
score vectors has frequently been cited as a reason for the poor performance of embedding based
approaches for extreme classiﬁcation problems. However  several studies [31  6] show that the set of
label vectors violates the low-rank structure on large-scale datasets. We should note that the label
vectors are generated by either thresholding the scores or taking the top m highest scores  which is a
highly non-linear transformation. Thus  it is not immediately clear if the low-rank structure of the
score vectors directly translates to a low-rank structure on the label vectors.
There have been efforts to tackle this presumed issue of embedding-based methods  primarily by using
a more complex ﬁnal classiﬁer f than simple linear ones. For instance  Xu et al. [31] decomposed
the label matrix into a low-rank and a sparse part  where the sparse part captures tail labels as outliers.
Bhatia et al. [6] developed an ensemble of local distance preserving embeddings to predict tail labels.
In particular  they cluster data points into sub-regions and use a k-nearest neighbor classiﬁer in the
locally learned embedding space. However  these modern embedding-based approaches have several
drawbacks [3] and cannot outperform other approaches on all large-scale datasets.
While most sparse linear and tree-based methods outperform embedding-based approaches  there has
not been any deﬁnitive proof that the inherent problem with embedding-based methods is their use
of low-dimensional representations for the score vectors. To the contrary  we provide experimental
evidence that a low-dimensional embedding produced by training a simple neural network extractor
can attain near-perfect training accuracy but generalize poorly  suggesting that overﬁtting is the root
cause of the poor performance of embedding-based methods that has been reported in the literature. In
fact  we will show that theoretically there is no limitation to using low-dimensional embedding-based
methods  even with simple linear classiﬁers.

3

2.1 Validity of Low-Dimensional Bottleneck Criticism
We ﬁrst present a different perspective regarding embedding-based models  showing their inferior
performance in large output spaces is due to overﬁtting to training set rather than their inability to
represent the input-label relationship with low-dimensional label embeddings.
Let w be the embedding function parameterized by some vector w that takes as input x 2X and
outputs a feature embedding w(x) 2 Rd. In practice  we may take w to be a linear function
w(x) = w>x or a neural network with multiple linear layers and ReLU activation. We use a linear
classiﬁer on top of the embedding  parameterized by a matrix V 2 Rd⇥K  whose columns give the
label embeddings vy for all labels y. Deﬁne the scoring function h : X! RK as h(x) = w(x)>V.
At training time  we sample an input-label pair (x  y) uniformly and compute the margin loss [20]:

`(h(x)  y) := Xy2Ly Xy0 /2Ly

[h(x)y0  h(x)y + c]+

(1)

Recall that Ly denotes the set of indices that are non-
zero in y. This loss encourages the scores for all relevant
labels to be higher than the scores for irrelevant labels
by a margin of c > 0. However  since the set of labels
is large  computing this sum over the entire set is pro-
hibitively expensive during training. Instead  we use a
stochastic estimate of the loss by sampling a small subset
of labels from Ly and computing the sum over that subset
only. This loss function can be efﬁciently minimized using
batched stochastic gradient descent. An alternative option
is to use the so-called stochastic negative mining loss [25].
Algorithm 1 summarizes the training procedure.
We now illustrate the overﬁtting issue on this embedding-
based model setup. Figure 1 shows the results of train-
ing our model on the AMAZONCAT-13K dataset. The
statistics of this dataset is summarized in Table 5 in the
supplementary material. The blue line shows that training accuracy continues to improve throughout
optimization  culminating in near-perfect accuracy towards the end of training. We emphasize that
this disputes the argument made by previous works that embedding-based models are ill-suited
for this dataset due to the dimensionality constraint. However  we observe in Figure 1 is that our
embedding-based model has severely overﬁtted to the training set. This observation highlights the
need for regularization techniques to improve the performance of embedding-based methods.

Figure 1: Training (blue) and test (red)
accuracy of Alg. 1 on the AMAZONCAT-
13K dataset.
The non-regularized
embedding-based method severely over-
ﬁts to the training data.

Feature embedding model w : X! Rd
Label embedding matrix V 2 Rd⇥K
Loss function ` : RK ⇥ [K] ! R
Learning rates ⌘w ⌘ V

Algorithm 1 Training the basic embedding model
1: Input: Dataset {(x1  y1)  . . .   (xn  yn)}
2:
3:
4:
5:
6: Initialize w  V
7: repeat
8:
9:
10:
11:
12:
13: until convergence

dw   V V  ⌘V

BPB

dL
dV

Sample a batch x1  . . .   xB
Sample indices k1  . . .   kB uniformly from non-zero indices of y1  . . .   yB
Compute loss L 1
Compute gradients dL
Update w w  ⌘w

dV via backpropagation

i=1 `(w(xi)>V  ki)

dw and dL

dL

2.2 Existence of Perfect Accuracy Low-Dimensional Embedding Classiﬁers
We further support our argument theoretically and demonstrate the fact that embedding-based models
can attain near-perfect accuracy is not limited to any speciﬁc dataset  but is feasible in general. We

4

make the following mild assumption on the data: for every x there exists a unique label vector
y = y(x)  and the number of non-zero entries in y(x) is bounded by s ⌧ K  i.e.  the number of true
labels associated with any feature vector is at most some small constant s. Under this assumption  the
following result shows that low-dimensional embedding-based models do not suffer from inability to
represent the input-label relationship. Proof can be found in the supplementary material.
Theorem 2.1. Let S✓X be a sample set. Under the assumption on the data speciﬁed above  there
exists a function  : X! Rd  and a label embedding matrix V 2 Rd⇥K such that:

1. d = O(min{s log(K|S|))  s2 log(K)})
2. For every label y  we have kvyk2 = 1.
3. For all x 2S and y 2 Ly(x)  we have (x)>vy  2
3.
4. For all x 2S and y 62 Ly(x)  we have (x)>vy  1
3.
5. For every pair of labels y  y0 with y 6= y0  we have v>y vy0 q 2 log(4K2)
6. For any x 2S   we have k(x)k2 = O(s( log(K)

) 1
4 ).

d

.

d

This theorem shows that in the limit of inﬁnite model capacity for constructing the embedding map 
there exists a low-dimensional embedding-based linear classiﬁer that thresholds at 1
2 and has perfect
training accuracy. Furthermore  the label embeddings vy are normalized to unit length. Since deep
neural networks have been demonstrated to have excellent function approximation capabilities  this
result naturally motivates a model architecture which uses a deep neural network to mimic the optimal
inﬁnitely expressive embedding map   followed by a linear classiﬁer. Another consequence of the
bound on the dimension in terms of |S| is it shows how overﬁtting is possible with small training sets:
the dependence of the dimension d on s improves to linear from quadratic at the price of a (mild)
logarithmic factor in the size of the sample set. On the other hand  applying the theorem with S = X
shows that d = O(s2 log(K)) sufﬁces to obtain a classiﬁer with perfect test accuracy.

3 Regularizing Embedding-Based Models

Motivated by our ﬁndings  in this section we propose a novel regularization framework and discuss
its effectiveness for the classiﬁcation problem with large output spaces.

3.1 Embedding Normalization

We ﬁrst apply weight normalization proposed in [26]. In each layer  weight vectors of all output
neurons share a single trainable length and each weight vector maintains its own trainable direction.
Weight normalization not only helps stabilize training and accelerate convergence  but also improves
generalization. For the ease of exposition  we assume all label embeddings are `2-normalized to unit
norm  i.e.  vi 2 Sd1  where Sd1 denotes the unit sphere in Rd. In a similar vein  we can assume all
input embeddings are normalized as well: w(x) 2 Sd1. Our regularizer can be easily generalized
to cases where the label embeddings are not unit norm.

3.2 GLaS Regularizer

In large-scale multi-label classiﬁcation  the output space is both large and sparse — most feature
vectors are associated with only very few true labels. Thus it may be desirable for an embedding-based
classiﬁer to have near-orthogonal label embeddings  as suggested by Theorem 2.1. As a result  it is
natural to consider regularizers such as spread-out [37] that explicitly promote such structure.

Spread-out Regularization. Zhang et al. [37] introduced the spread-out regularization technique 
which encourages local feature descriptors of images to be uniformly dispersed over the sphere. We
consider a variant of spread-out regularization that brings the inner product of the embeddings of two
different labels close to zero  i.e.  v>y vy0 ⇡ 0 if y 6= y0. More formally  the spread-out regularizer
corresponds to the following:

`spreadout =

1
K2

KXy=1

KXy0=1

(v>y vy0)2.

5

(2)

Note that due to embedding normalization  diagonal entries v>y vy = 1 and hence these terms will not
play a role in the regularization loss function in (2). Zhang et al. [37] have shown the effectiveness of
this technique in learning good local feature descriptors for images. However  one major drawback of
this regularizer is that it over-penalizes the embeddings of two different labels that occur frequently
together (e.g.  apple and fruit tend to co-occur for many inputs). In other words  label embeddings of
labels that co-occur frequently are also encouraged to be far away  which is clearly undesirable.

Correcting Over-penalization: GLaS Regularization. The spread-out regularizer suffers from
the lack of modeling the co-occurrences of labels. Thus  to correct for this over-penalization  we need
to estimate the degree of occurrence between labels from training data and explicitly model it with
the regularizer.
Let Y 2{ 0  1}n⇥K be the training set label matrix where each row corresponds to a single training
example. Let A = Y >Y so that Ay y0 = number of times labels y and y0 co-occur  and let Z =
diag(A) 2 RK⇥K be the matrix containing only the diagonal component of A. Observe that AZ1
represents the conditional frequency of observing one label given the other. Indeed 

(AZ1)y y0 =

Ay y0
Ay0 y0

=

number of times y and y0 co-occur

number of times y0 occurs

=: F (y|y0).

Similarly  Z1A = (AZ1)> contains the conditional frequencies in reverse  i.e.  (Z1A)y y0 =
F (y0|y). These conditional frequencies encode the degree of co-occurrence between labels y and y0 
and we would like their embeddings vy and vy0 to reﬂect this co-occurrence pattern:

`GLaS =

1

K2V>V 

1
2

(AZ1 + Z1A)

2

F

.

(3)

In the case where all labels are uncorrelated  this loss recovers the spread-out regularizer. While we
choose to deﬁne the degree of label correlation as the average of conditional frequencies between
labels  other measures of similarity such as pointwise mutual information (PMI) and Jaccard distance
can also be used. In Appendix B  we give a theoretical justiﬁcation for using the geometric mean of
the conditional frequencies (see Theorem B.1). In experiments  however  we found empirically that
using arithmetic mean of the conditional frequencies gives a slight but noticeable boost in accuracy
compared to other measures  motivating the deﬁnition (3) of the GLaS regularizer.
One issue that arises when using this regularizer is that calculating `GLaS requires O(K2) operations
and becomes prohibitively expensive when K is large. Instead  we select a batch of rows from V and
compute a stochastic version of the loss on that batch only.

Relationship to Graph Laplacian and Spread-out Regulariza-
tion. While the deﬁnition for the GLaS regularizer is intuitive 
it may seem arbitrary and one can arrive at other regularizers by
following a similar intuition. However  we show that the GLaS regu-
larizer can be recovered as a sum of the well-known graph Laplacian
regularizer and the spread-out regularizer  thus giving our regularizer
its name (Graph Laplacian and Spreadout).
Graph Laplacian as a general technique has been successfully applied
to representation learning problems such as metric learning [5] and
hashing [21]. By adding a graph Laplacian based loss  we can
impose the right structure on the off-diagonal values in the Gram
matrix of label embeddings. More speciﬁcally  to assign similar
embeddings to labels that co-occur frequently  we can explicitly penalize the `2 distance between two
label embeddings with a weight proportional to their co-occurrence statistics. As a result  the graph
Laplacian regularization makes the label embeddings consistent with the connectivity pattern of label
nodes in the item-label graph (Figure 2). We can write the graph Laplacian regularizer as

Figure 2: The item-label bi-
partite graph. The edge be-
tween a label node and an item
node represents an assignment
of the label to the item. La-
bels i and j have co-occurred
in two items.

`Laplacian =

1
K2

KXy=1

KXy0=1

kvy  vy0k2

2uyy0 

(4)

where uyy0 denotes the amount of “adjacency” between graph nodes of labels y and y0 and is only
dependent on the graph structure. However  this loss formulation admits a trivial optimal solution
that assigns all labels the same embedding.

6

`Laplacian + `spreadout =

(a)
=

1
K2

1
K2

KXy=1
KXy=1

2uyy0 + (v>y vy0)2⇤

KXy0=1⇥kvy  vy0k2
KXy0=1⇥(v>y vy0  uyy0)2  (u2

yy0  2uyy0)⇤

Recall that the spread-out regularizer suffers from a completely opposite weakness of encouraging all
label embeddings to be orthogonal regardless of any correlation. Thus  combining the two regularizers
has the effect of compensating their respective weaknesses and promoting their strengths. Summing
the graph Laplacian regularizer (4) and the spread-out regularizer (2) we get

2 = 1. One can see thatPy y0(u2

where (a) holds since kvyk2
yy0  2uyy0) is a constant that only
depends on the graph structure. The non-constant part of the sum can be written as 1
F  
K2kV>V Uk2
2 (AZ1 + Z1A) being the measure
which is exactly the form of GLaS given in (3) with U = 1
of degree of adjacency in the label graph. Note that the graph Laplacian regularizer `Laplacian
encourages frequently co-occurring labels to have similar label embeddings. However  labels that do
not co-occur frequently but have similar embeddings are not penalized by graph Laplacian regularizer.
This is achieved through the spread-out regularizer `spreadout. Thus  our regularizer GLaS captures
the essence of label relation.

Feature embedding model w : X! Rd
Label embedding matrix V 2 Rd⇥K
Loss function ` : RK ⇥Y! R
GLaS loss `GLaS : RB⇥B ⇥ RB⇥B ! R
Regularization weight 
Dropout probability ⇢ 2 [0  1]
Learning rates ⌘w ⌘ V

Algorithm 2 Training with regularization
1: Input: Dataset {(x1  y1)  . . .   (xn  yn)}
2:
3:
4:
5:
6:
7:
8:
9: Initialize w  V
10: repeat
11:
12:
13:
BPB
14:
15:
16:
17: V [vy1|···|vyB ] 2 RB⇥B
18:
dw and dL
19:
20:
21: until convergence
3.3
Input Dropout
Input dropout [13] is a simple regularization and data augmentation technique for text classiﬁcation
models with sparse features. For a selected keep probability ⇢ 2 [0  1] and an input feature x  the
method produces an augmented input x0 = x  Bernoulli(⇢  D)  where  denotes element-wise
multiplication. Thus  non-zero feature coordinates are set to zero with probability 1  ⇢. This can be
interpreted as data augmentation  where features in the input are uniformly removed with probability
1  ⇢. It discourages the model from ﬁtting spurious patterns in input features when training data is
scarce and it also promotes the model to be robust to corruption of the input features. The complete
learning algorithm that integrates all techniques described in this section is presented as Algorithm 2.

Sample a batch x1  . . .   xB
Sample labels y1  . . .   yB uniformly from non-zero indices of y1  . . .   yB
Apply input dropout xi xi  Bernoulli(⇢  D)
Compute loss L 1
i=1 `(w(xi)>V  yi)
Y [y1|···|yB]
U B ⇥ B submatrix of Equation (3) corresponding to indices y1  . . .   yB
Regularize L L + `GLaS(V>V  U )
Compute gradients dL
Update w w  ⌘w
dw   V V  ⌘V

dV via backpropagation

dL
dV

dL

4 Experiments

In this section  we present experimental results of our method on several widely used extreme
multi-label classiﬁcation datasets: AMAZONCAT-13K  AMAZON-670K  WIKILSHTC-325K 

7

DELICIOUS-200K  EURLEX-4K  and WIKIPEDIA-500K. The statistics of these datasets is pre-
sented in Table 5 in the supplementary material.

Batch Size

Linear
91.77

d = 1024

94.21

94.21

GLaS
94.21

Parameters

93.75

⇢ = 0.6

⇢ = 1.0

⇢ = 0.8

Spread-out

93.34

d = 512

93.82

d = 256

93.24

Input Dropout

93.39
1024
94.04

94.21
2048
93.98

Embedding Size

 = 10
94.21

94.08
4096
94.21

Embedding Type

Regularization Weight

Variable
Regularizer

None
92.34
 = 1
93.68

Gravity
93.42
 = 100

Ablation Study. We begin by studying the
performance of Algorithm 2 under different set-
tings of its hyperparameters. In particular  we
investigate variations in the regularization type
and weight  input dropout  batch size  and em-
bedding type and size. Table 1 shows the effects
of different parameters on the performance of
our method on the AMAZONCAT-13K dataset.
We ﬁrst list our base setting that we have de-
rived through cross validation. In the base set-
ting  we use GLaS regularizer (discussed in Sec.
3.2) with regularization weight  = 10  input
dropout with ⇢ = 0.8  batch size B = 4096 
and a non-linear embedding map w with em-
bedding dimension d = 1024. In each row of
Table 1  we alter one parameter from the base
setting to study its impact. For the regulariza-
tion method  we compare our method with the
spread-out regularizer [37] and Gravity regularizer [16] and show that our method signiﬁcantly
outperforms these two. We can observe that the regularization weight and input dropout rate should
not be either excessively small or large as these settings hurt the test accuracy.
As one can expect  embeddings of higher dimensionalities outperform those of lower dimensionalities.
Batch sizes in the range of 1000s do not have a signiﬁcant impact on the performance; however  we
do note that the largest batch size 4096 gives us the highest test accuracy. Finally and as shown in
Table 1  adding the ReLU nonlinearity boosts the performance of w in learning the embedding.

Table 1: Sensitivity of Algorithm 2 to variations in
different parameters for AMAZONCAT-13K. Each
row shows the effect of a single parameter. Our
GLaS regularizer outperforms spread-out and grav-
ity. A moderate regularization weight and input
dropout  a large embedding size  and using non-
linearity lead to a better result.

Nonlinear (ReLU)

Dataset

Test Acc.

Gen. Gap

Train Acc.

AMAZON-670K

Regularization

AMAZONCAT-13K

98.77
99.23
96.10
98.21

GLaS
None
GLaS
None

Generalization Gap. As discussed previ-
ously  one of the main goals of this paper is
to propose regularization techniques that miti-
gate the overﬁtting (Figure 1) of neural network
embedding-based methods for extreme multi-
label classiﬁcation problems. Table 2 studies
the effect of our regularization technique on the
generalization gap  i.e.  the difference between
training and test accuracies. In particular  we
have studied two datasets AMAZONCAT-13K
and AMAZON-670K in two different settings: with and without the regularization technique we
discussed in Section 3. The table shows that regularizing embedding based models with our method
reduces the generalization gap over the unregularized setting while improving test accuracy. As an
example  GLaS regularizer reduces the generalization gap of Algorithm 1 by more than 30% when
applied to the AMAZONCAT-13K dataset.

Table 2: The comparison of generalization gap in
Algorithm 1 and Algorithm 2 when they are ap-
plied to AMAZONCAT-13K and AMAZON-670K
datasets. The GLaS regularizer (Section 3.2) sig-
niﬁcantly improves the generalization gap.

4.56
6.89
49.78
53.68

94.21
92.34
46.32
44.53

Comparison with Previous Work. We compare our method with several other recent works on
the extreme classiﬁcation problem denoted in Table 3. As shown in this Table  on all datasets except
Delicious-200K and EURLex-4K our method matches or outperforms all previous work in terms of
precision@k3. Even on the Delicious-200K dataset  our method’s performance is close to that of the
state-of-the-art  which belongs to another embedding-based method SLEEC [6]. One thing to note
about the Delicious-200k dataset is that its average number of labels per training point is signiﬁcantly
larger than that of other datasets. Due to this  we observed that it took a long time for training to show
steady progress with the ﬁxed margin loss. Hence  we have used the softmax-cross-entropy loss for
the Delicious-200K dataset instead of the loss function in (1). Softmax-cross-entropy loss relaxes the
margin requirement and signiﬁcantly stabilizes training.

3P@k = 1

kPl2rankk(ˆy) yl where ˆy is the predicted score vector and y 2{ 0  1}L is the ground truth labels.

8

Embedding-Based

LEML [36]

RobustXML [31]

XML-CNN [19]

Other Methods

Parabel [23]

DiSMEC [3]

PD-Sparse [34]

PPD-Sparse [33]

Dataset

AMAZONCAT-13K

WIKILSHTC-325K

AMAZON-670K

DELICIOUS-200K

EURLEX-4K

WIKIPEDIA-500K

P@k
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5

Ours
94.21
79.70
64.84
65.46
45.44
34.51
46.38
42.09
38.56
46.4
40.49
38.1
77.5
65.01
54.37
69.91
49.08
38.35

SLEEC [6]

90.53
76.33
61.52
54.83
33.42
23.85
35.05
31.25
28.56
47.85
42.21
39.43
79.26
64.3
52.33
48.2
29.4
21.2

-
-
-

19.82
11.43
8.39
8.13
6.83
6.03
40.73
37.71
35.84
63.4
50.35
41.28
41.3
30.1
19.8

-
-
-
-
-
-

-
-
-

35.39
31.93
29.32

76.38
62.81
51.41
59.85
39.28
29.81

PfastreXML [14]

91.75
77.97
63.68
56.05
36.79
27.09
39.46
35.81
33.05
41.72
37.83
35.58
75.45
62.7
52.51
59.52
40.24
30.72

FastXML [24]

93.11
78.2
63.41
49.75
33.10
24.45
36.99
33.28
30.53
43.07
38.66
36.19
71.36
59.9
50.39
54.1
35.5
26.2

93.03
79.16
64.52
65.04
43.23
32.05
44.89
39.80
36.00
46.97
40.08
36.63
81.73
68.78
57.44
66.73
47.48
36.78

93.40
79.10
64.10
64.40
42.50
31.50
44.70
39.70
36.10
45.50
38.70
35.50
82.4
68.5
57.7
70.2
50.6
39.7

90.60
75.14
60.69
61.26
39.48
28.79

34.37
29.48
27.04
76.43
60.37
49.72

-
-
-

-
-
-

Table 3: Performance comparison (based on precision@k) with several other methods on large-scale
datasets. Our method attains or improves upon the state-of-the-art results. Results of other methods
are derived from the extreme classiﬁcation repository. Italic underlined numbers are the best of the
entire row and bold numbers are the best among embedding-based methods.

-
-
-

-
-
-

64.08
41.26
30.12
45.32
40.37
36.92

83.83
70.72
59.21
70.16
50.57
39.66

27.47
33.00
36.29
26.64
30.65
34.65

-
-
-

-
-
-
-
-
-

88.4
74.6
60.6
53.5
31.8
29.9
31.0
28.0
24.0
45.0
40.0
38.0

-
-
-
-
-
-

-
-
-

3.48
3.79
4.27
2.07
2.26
2.47
6.06
7.24
8.10
24.10
26.37
27.62

Embedding-Based
SLEEC [6]

LEML [36]

Other Methods

Parabel [23]

DiSMEC [3]

PD-Sparse [34]

PPD-Sparse [33]

Dataset

AMAZONCAT-13K

WIKILSHTC-325K

AMAZON-670K

DELICIOUS-200K

EURLEX-4K

PSP@k
PSP@1
PSP@3
PSP@5
PSP@1
PSP@3
PSP@5
PSP@1
PSP@3
PSP@5
PSP@1
PSP@3
PSP@5
PSP@1
PSP@3
PSP@5

Ours
47.53
62.74
71.66
46.22
46.15
47.28
38.94
39.72
41.24
28.68
24.93
23.87
49.77
51.05
53.82

46.75
58.46
65.96
20.27
23.18
25.08
20.62
23.32
25.98
7.17
8.16
8.96
34.25
38.35
40.30

PfastreXML [14]

69.52
73.22
75.48
30.66
31.55
33.12
29.30
30.80
32.43
3.15
3.87
4.43
43.86
45.23
46.03

FastXML [24]

48.31
60.26
69.30
16.35
20.99
23.56
19.37
23.26
26.85
6.48
7.52
8.31
26.62
32.07
35.23

50.93
64.00
72.08
26.76
33.27
37.36
25.43
29.43
32.85
7.25
7.94
8.52
36.36
41.95
44.78

59.10
67.10
71.20
29.1
35.6
39.5
27.8
30.6
34.2
6.5
7.6
8.4
41.20
44.30
46.90

49.58
61.63
68.23
28.34
33.50
36.62

-
-
-

5.29
5.80
6.24
36.28
40.96
42.84

Table 4: Performance comparison (based on propensity scored precision@k  PSP@k) with several
other methods on large-scale datasets. Propensity weights are higher for rarer labels  hence this metric
better reﬂects the model’s ability to generalize to tail labels than precision. Italic underlined numbers
are the best of the entire row and bold numbers are the best among embedding-based methods.

One of the biggest challenges for learning in large output spaces comes from tail labels that are only
assigned to a few inputs  but make up the majority of the whole label set. The propensity scored
precision@K (PSP@K4) metric corrects for this bias by up-weighting rare labels. To demonstrate the
effectiveness of our method at predicting tail labels  we report results using this evaluation metric
in Table 4. While many previous methods that we compare against have to explicitly change their
training objective or algorithm accordingly to account for the re-weighting  in contrast  our simple
embedding based models learn to predict these tail labels remarkably well without any adjustment of
our training loss or procedure. On the dataset with the largest number of labels Amazon-670K  our
method improves the PSP@1 metric by an absolute margin of 9.6%.

hours  the time complexity is O(dPx2S

Training and Inference Speed. We train all models up to 10 epochs and apply early stopping
when evaluation accuracy ceases to improve. Though the overall training process takes minutes to
nnz(x))  where d is the embedding dimensionality  S is the
set of training samples  and nnz(x) is the number of non-zero features of the sparse input x.
At inference time  we apply efﬁcient Maximum Inner Product Search techniques such as [11  30].
The non-exhaustive search achieves low latency due to highly effective clustering based tree indices
[2] and hardware based acceleration [11  8]. For all datasets up to a few million labels  the inference
latency is below 10ms and below 1ms for under 100k labels.

5 Conclusions

In this paper  we showed that from both theoretical and empirical perspectives  neural network
models suffer from overﬁtting instead of low-dimensional embedding bottleneck when applied to
extreme multi-label classiﬁcation problems. To this end  we introduced the GLaS regularization
framework and demonstrated its effectiveness with new state-of-the-art results on several widely
tested large-scale datasets. We hope future work can build on our theoretical and empirical ﬁndings
and more competitive embedding-based methods can be developed along this direction.

4Similar to P@k  PSP@k = 1

kPl2rankk(ˆy)

yl
pl

where pl denotes the propensity weights.

9

References
[1] R. Agrawal  A. Gupta  Y. Prabhu  and M. Varma. Multi-label learning with millions of labels:
Recommending advertiser bid phrases for web pages. In Proceedings of the 22nd international
conference on World Wide Web  pages 13–24. ACM  2013.

[2] A. Auvolat and P. Vincent. Clustering is efﬁcient for approximate maximum inner product

search. CoRR  abs/1507.05910  2015.

[3] R. Babbar and B. Sch¨olkopf. Dismec: Distributed sparse machines for extreme multi-label
classiﬁcation. In Proceedings of the Tenth ACM International Conference on Web Search and
Data Mining  WSDM 2017  Cambridge  United Kingdom  February 6-10  2017  pages 721–729 
2017.

[4] R. Babbar and B. Sch¨olkopf. Data scarcity  robustness and extreme multi-label classiﬁcation.

Machine Learning  pages 1–23  2019.

[5] A. Bellet  A. Habrard  and M. Sebban. A survey on metric learning for feature vectors and

structured data. CoRR  abs/1306.6709  2013.

[6] K. Bhatia  H. Jain  P. Kar  M. Varma  and P. Jain. Sparse local embeddings for extreme
multi-label classiﬁcation. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015  December 7-12  2015  Montreal 
Quebec  Canada  pages 730–738  2015.

[7] W. Bi and J. Kwok. Efﬁcient multi-label classiﬁcation with many labels. In International

Conference on Machine Learning  pages 405–413  2013.

[8] D. W. Blalock and J. V. Guttag. Bolt: Accelerated data mining with fast vector compression. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining  pages 727–735  2017.

[9] Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label
classiﬁcation. In Advances in Neural Information Processing Systems  pages 1529–1537  2012.
[10] M. Ciss´e  N. Usunier  T. Arti`eres  and P. Gallinari. Robust bloom ﬁlters for large multilabel
classiﬁcation tasks. In Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held
December 5-8  2013  Lake Tahoe  Nevada  United States.  pages 1851–1859  2013.

[11] R. Guo  S. Kumar  K. Choromanski  and D. Simcha. Quantization based fast inner product
search. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and
Statistics  AISTATS 2016  Cadiz  Spain  May 9-11  2016  pages 482–490  2016.

[12] D. J. Hsu  S. Kakade  J. Langford  and T. Zhang. Multi-label prediction via compressed sensing.
In Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural
Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009 
Vancouver  British Columbia  Canada.  pages 772–780  2009.

[13] M. Iyyer  V. Manjunatha  J. L. Boyd-Graber  and H. D. III. Deep unordered composition
rivals syntactic methods for text classiﬁcation. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing of the Asian Federation of Natural Language Processing  ACL
2015  July 26-31  2015  Beijing  China  Volume 1: Long Papers  pages 1681–1691  2015.

[14] H. Jain  Y. Prabhu  and M. Varma. Extreme multi-label loss functions for recommendation 
tagging  ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  San Francisco  CA  USA 
August 13-17  2016  pages 935–944  2016.

[15] H. Jain  Y. Prabhu  and M. Varma. Extreme multi-label loss functions for recommendation 
tagging  ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 935–944  2016.

[16] W. Krichene  N. Mayoraz  S. Rendle  X. Lin  X. Yi  L. Hong  E. H. hsin Chi  and J. R. Anderson.
Efﬁcient training on very large corpora via gramian estimation. CoRR  abs/1807.07187  2018.
[17] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems 25  pages 1097–1105.
2012.

[18] Z. Lin  G. Ding  M. Hu  and J. Wang. Multi-label classiﬁcation via feature-aware implicit label

space encoding. In International conference on machine learning  pages 325–333  2014.

[19] J. Liu  W.-C. Chang  Y. Wu  and Y. Yang. Deep learning for extreme multi-label text classi-
ﬁcation. In Proceedings of the 40th International ACM SIGIR Conference on Research and
Development in Information Retrieval  pages 115–124. ACM  2017.
[20] T.-Y. Liu et al. Learning to rank for information retrieval. Foundations and Trends R in

Information Retrieval  3(3):225–331  2009.

10

[21] W. Liu  J. Wang  S. Kumar  and S.-F. Chang. Hashing with graphs. In Proceedings of the 28th

International Conference on International Conference on Machine Learning  2011.

[22] Y. Prabhu  A. Kag  S. Gopinath  K. Dahiya  S. Harsola  R. Agrawal  and M. Varma. Extreme
multi-label learning with label features for warm-start tagging  ranking & recommendation. In
Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining 
pages 441–449. ACM  2018.

[23] Y. Prabhu  A. Kag  S. Harsola  R. Agrawal  and M. Varma. Parabel: Partitioned label trees for
extreme classiﬁcation with application to dynamic search advertising. In Proceedings of the
2018 World Wide Web Conference on World Wide Web  WWW 2018  Lyon  France  April 23-27 
2018  pages 993–1002  2018.

[24] Y. Prabhu and M. Varma. Fastxml: a fast  accurate and stable tree-classiﬁer for extreme multi-
label learning. In The 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  KDD ’14  New York  NY  USA - August 24 - 27  2014  pages 263–272  2014.
[25] S. J. Reddi  S. Kale  F. Yu  D. Holtmann-Rice  J. Chen  and S. Kumar. Stochastic negative

mining for learning with large output spaces. In AISTATS  2019.

[26] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems 29 
pages 901–909. 2016.

[27] I. Sutskever  O. Vinyals  and Q. V. Le. Sequence to sequence learning with neural networks. In

Advances in Neural Information Processing Systems 27  pages 3104–3112. 2014.

[28] F. Tai and H.-T. Lin. Multilabel classiﬁcation with principal label space transformation. Neural

Computation  24(9):2508–2542  2012.

[29] J. Weston  A. Makadia  and H. Yee. Label partitioning for sublinear ranking. In Proceedings of
the 30th International Conference on Machine Learning  ICML 2013  Atlanta  GA  USA  16-21
June 2013  pages 181–189  2013.

[30] X. Wu  R. Guo  A. T. Suresh  S. Kumar  D. N. Holtmann-Rice  D. Simcha  and F. Yu. Multiscale
quantization for fast similarity search. In Advances in Neural Information Processing Systems
30  pages 5745–5755. 2017.

[31] C. Xu  D. Tao  and C. Xu. Robust extreme multi-label learning. In Proceedings of the 22nd ACM
SIGKDD international conference on knowledge discovery and data mining  pages 1275–1284.
ACM  2016.

[32] Z. Yang  Z. Dai  R. Salakhutdinov  and W. W. Cohen. Breaking the softmax bottleneck: A
high-rank RNN language model. In International Conference on Learning Representations 
2018.

[33] I. E. Yen  X. Huang  W. Dai  P. Ravikumar  I. S. Dhillon  and E. P. Xing. Ppdsparse: A parallel
primal-dual sparse method for extreme classiﬁcation. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  Halifax  NS  Canada 
August 13 - 17  2017  pages 545–553  2017.

[34] I. E. Yen  X. Huang  P. Ravikumar  K. Zhong  and I. S. Dhillon. Pd-sparse : A primal and dual
sparse approach to extreme multiclass and multilabel classiﬁcation. In Proceedings of the 33nd
International Conference on Machine Learning  ICML 2016  New York City  NY  USA  June
19-24  2016  pages 3069–3077  2016.

[35] R. You  S. Dai  Z. Zhang  H. Mamitsuka  and S. Zhu. Attentionxml: Extreme multi-label
text classiﬁcation with multi-label attention based recurrent neural networks. arXiv preprint
arXiv:1811.01727  2018.

[36] H. Yu  P. Jain  P. Kar  and I. S. Dhillon. Large-scale multi-label learning with missing labels. In
Proceedings of the 31th International Conference on Machine Learning  ICML 2014  Beijing 
China  21-26 June 2014  pages 593–601  2014.

[37] X. Zhang  F. X. Yu  S. Kumar  and S. Chang. Learning spread-out local feature descriptors. In
IEEE International Conference on Computer Vision  ICCV 2017  Venice  Italy  October 22-29 
2017  pages 4605–4613  2017.

11

,Chuan Guo
Ali Mousavi
Xiang Wu
Daniel Holtmann-Rice
Satyen Kale
Sashank Reddi
Sanjiv Kumar