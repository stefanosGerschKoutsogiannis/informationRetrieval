2010,Efficient and Robust Feature Selection via Joint ℓ2 1-Norms Minimization,Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks  efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper  we propose a new robust feature selection method with emphasizing joint ℓ2 1-norm minimization on both loss function and regularization. The ℓ2 1-norm based loss function is robust to outliers in data points and the ℓ2 1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.,Efﬁcient and Robust Feature Selection via Joint

(cid:96)2 1-Norms Minimization

Feiping Nie

Computer Science and Engineering

University of Texas at Arlington
feipingnie@gmail.com

Xiao Cai

Computer Science and Engineering

University of Texas at Arlington
xiao.cai@mavs.uta.edu

Heng Huang

Computer Science and Engineering

University of Texas at Arlington

heng@uta.edu

Chris Ding

Computer Science and Engineering

University of Texas at Arlington

chqding@uta.edu

Abstract

Feature selection is an important component of many machine learning applica-
tions. Especially in many bioinformatics tasks  efﬁcient and robust feature se-
lection methods are desired to extract meaningful features and eliminate noisy
ones. In this paper  we propose a new robust feature selection method with em-
phasizing joint (cid:96)2 1-norm minimization on both loss function and regularization.
The (cid:96)2 1-norm based loss function is robust to outliers in data points and the (cid:96)2 1-
norm regularization selects features across all data points with joint sparsity. An
efﬁcient algorithm is introduced with proved convergence. Our regression based
objective makes the feature selection process more efﬁcient. Our method has been
applied into both genomic and proteomic biomarkers discovery. Extensive empir-
ical studies are performed on six data sets to demonstrate the performance of our
feature selection method.

1 Introduction

Feature selection  the process of selecting a subset of relevant features  is a key component in build-
ing robust machine learning models for classiﬁcation  clustering  and other tasks. Feature section
has been playing an important role in many applications since it can speed up the learning process 
improve the mode generalization capability  and alleviate the effect of the curse of dimensional-
ity [15]. A large number of developments on feature selection have been made in the literature and
there are many recent reviews and workshops devoted to this topic  e.g.  NIPS Conference [7].
In past ten years  feature selection has seen much activities primarily due to the advances in bioin-
formatics where a large amount of genomic and proteomic data are produced for biological and
biomedical studies. For example  in genomics  DNA microarray data measure the expression levels
of thousands of genes in a single experiment. Gene expression data usually contain a large number
of genes  but a small number of samples. A given disease or a biological function is usually asso-
ciated with a few genes [19]. Out of several thousands of genes to select a few of relevant genes
thus becomes a key problem in bioinformatics research [22]. In proteomics  high-throughput mass
spectrometry (MS) screening measures the molecular weights of individual biomolecules (such as
proteins and nucleic acids) and has potential to discover putative proteomic biomarkers. Each spec-
trum is composed of peak amplitude measurements at approximately 15 500 features represented
by a corresponding mass-to-charge value. The identiﬁcation of meaningful proteomic features from
MS is crucial for disease diagnosis and protein-based biomarker proﬁling [22].

1

In general  there are three models of feature selection methods in the literature: (1) ﬁlter meth-
ods [14] where the selection is independent of classiﬁers  (2) wrapper methods [12] where the pre-
diction method is used as a black box to score subsets of features  and (3) embedded methods where
the procedure of feature selection is embedded directly in the training process. In bioinformatics
applications  many feature selection methods from these categories have been proposed and applied.
Widely used ﬁlter-type feature selection methods include F -statistic [4]  reliefF [11  13]  mRMR
[19]  t-test  and information gain [21] which compute the sensitivity (correlation or relevance) of a
feature with respect to (w.r.t) the class label distribution of the data. These methods can be char-
acterized by using global statistical information. Wrapper-type feature selection methods is tightly
coupled with a speciﬁc classiﬁer  such as correlation-based feature selection (CFS) [9]  support vec-
tor machine recursive feature elimination (SVM-RFE) [8]. They often have good performance  but
their computational cost is very expensive.
Recently sparsity regularization in dimensionality reduction has been widely investigated and also
applied into feature selection studies. (cid:96)1-SVM was proposed to perform feature selection using the
(cid:96)1-norm regularization that tends to give sparse solution [3]. Because the number of selected features
using (cid:96)1-SVM is upper bounded by the sample size  a Hybrid Huberized SVM (HHSVM) was
proposed combining both (cid:96)1-norm and (cid:96)2-norm to form a more structured regularization [26]. But
it was designed only for binary classiﬁcation. In multi-task learning  in parallel works  Obozinsky
et. al. [18] and Argyriou et. al. [1] have developed a similar model for (cid:96)2 1-norm regularization to
couple feature selection across tasks. Such regularization has close connections to group lasso [28].
In this paper  we propose a novel efﬁcient and robust feature selection method to employ joint (cid:96)2 1-
norm minimization on both loss function and regularization. Instead of using (cid:96)2-norm based loss
function that is sensitive to outliers  a (cid:96)2 1-norm based loss function is adopted in our work to remove
outliers. Motivated by previous research [1  18]  a (cid:96)2 1-norm regularization is performed to select
features across all data points with joint sparsity  i.e. each feature (gene expression or mass-to-charge
value in MS) either has small scores for all data points or has large scores over all data points. To
solve this new robust feature selection objective  we propose an efﬁcient algorithm to solve such joint
(cid:96)2 1-norm minimization problem. We also provide the algorithm analysis and prove the convergence
of our algorithm. Extensive experiments have been performed on six bioinformatics data sets and
our method outperforms ﬁve other commonly used feature selection methods in statistical learning
and bioinformatics.

2 Notations and Deﬁnitions

We summarize the notations and the deﬁnition of norms used in this paper. Matrices are written as
boldface uppercase letters. Vectors are written as boldface lowercase letters. For matrix M = (mij) 
its i-th row  j-th column are denoted by mi  mj respectively.
The (cid:96)p-norm of the vector v ∈ Rn is deﬁned as (cid:107)v(cid:107)p =
v ∈ Rn is deﬁned as (cid:107)v(cid:107)0 =

. The (cid:96)0-norm of the vector
|vi|0. The Frobenius norm of the matrix M ∈ Rn×m is deﬁned as

(cid:182) 1

n(cid:80)

|vi|p

p

i=1

(cid:181)

n(cid:80)
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

(cid:107)M(cid:107)F =

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

m(cid:88)

m2

ij =

i=1

j=1

i=1

(cid:107)mi(cid:107)2
2.

(1)

The (cid:96)2 1-norm of a matrix was ﬁrst introduced in [5] as rotational invariant (cid:96)1 norm and also used
for multi-task learning [1  18] and tensor factorization [10]. It is deﬁned as

(cid:107)M(cid:107)2 1 =

n(cid:88)

(cid:176)(cid:176)mi

(cid:176)(cid:176)

2 

n(cid:88)

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

m2

ij =

i=1

j=1

i=1

2

(2)

.

(3)

(cid:195)(cid:88)

(cid:33) 1

p

(cid:107)ai(cid:107)p

r

+

which is rotational invariant for rows: (cid:107)MR(cid:107)2 1 = (cid:107)M(cid:107)2 1 for any rotational matrix R. The
(cid:96)2 1-norm can be generalized to (cid:96)r p-norm

 n(cid:88)

i=1

(cid:33) 1

p

(cid:176)(cid:176)p

r

 p

r

j=1

|mij|r

 m(cid:88)
(cid:80)
i |vi|p) 1
(cid:195)(cid:88)

p

=

(cid:176)(cid:176)mi

(cid:195)
n(cid:88)

 1
(cid:80)
i |ui + vi|p) 1
(cid:33) 1

i=1

(cid:195)(cid:88)

(cid:107)M(cid:107)r p =
(cid:80)
i |ui|p) 1
(cid:195)(cid:88)
(cid:33) 1

(cid:107)bi(cid:107)p

r

Note that (cid:96)r p-norm is a valid norm because it satisﬁes the three norm conditions  including the
triangle inequality (cid:107)A(cid:107)r p + (cid:107)B(cid:107)r p ≥ (cid:107)A + B(cid:107)r p. This can be proved as follows. Starting from
p and setting ui = (cid:107)ai(cid:107)r and
the triangle inequality (
vi = (cid:107)bi(cid:107)r  we obtain

p ≥ (

p + (

(cid:33) 1

p

p ≥

| (cid:107)ai(cid:107)r + (cid:107)bi(cid:107)r|p

p ≥

| (cid:107)ai + bi(cid:107)r|p

  (4)

i

i

i

i

where the second inequality follows the triangle inequality for (cid:96)r norm: (cid:107)ai(cid:107)r+(cid:107)bi(cid:107)r ≥ (cid:107)ai+bi(cid:107)r.
Eq. (4) is just (cid:107)A(cid:107)r p + (cid:107)B(cid:107)r p ≥ (cid:107)A + B(cid:107)r p.
However  the (cid:96)0-norm is not a valid norm because it does not satisfy the positive scalability:
(cid:107)αv(cid:107)0 = |α|(cid:107)v(cid:107)0 for scalar α. The term “norm” here is for convenience.

3 Robust Feature Selection Based on (cid:96)2 1-Norms

Least square regression is one of the popular methods for classiﬁcation. Given training data
{x1  x2 ···   xn} ∈ Rd and the associated class labels {y1  y2 ···   yn} ∈ Rc 
traditional
least square regression solves the following optimization problem to obtain the projection matrix
W ∈ Rd×c and the bias b ∈ Rc:

(5)

(6)

(7)

(8)

(9)

min
W b

min
W

(cid:176)(cid:176)2

2.

n(cid:88)

i=1

(cid:176)(cid:176)WT xi + b − yi
n(cid:88)
(cid:176)(cid:176)2
(cid:176)(cid:176)WT xi − yi
n(cid:88)
(cid:176)(cid:176)
(cid:176)(cid:176)WT xi − yi

2.

2 

min
W

i=1

For simplicity  the bias b can be absorbed into W when the constant value 1 is added as an additional
dimension for each data xi(1 ≤ i ≤ n). Thus the problem becomes:

i=1
In this paper  we use the robust loss function:

where the residual (cid:107)WT xi − yi(cid:107) is not squared and thus outliers have less importance than the
squared residual (cid:107)WT xi − yi(cid:107)2. This loss function has a rotational invariant property while the
pure (cid:96)1-norm loss function does not has such desirable property [5].
We now add a regularization term R(W) with parameter γ. The problem becomes:

(cid:176)(cid:176)WT xi − yi

(cid:176)(cid:176)

2 + γR(W).

Several regularizations are possible:

R1(W) = (cid:107)W(cid:107)2  R2(W) =

(cid:107)wj(cid:107)1  R3(W) =

(cid:176)(cid:176)wi

d(cid:88)

i=1

(cid:176)(cid:176)0

2  R4(W) =

(cid:176)(cid:176)wi

(cid:176)(cid:176)

2.

d(cid:88)

i=1

R1(W) is the ridge regularization. R2(W) is the LASSO regularization. R3(W) and R4(W)
penalizes all c regression coefﬁcients corresponding to a single feature as a whole. This has the

3

min
W

n(cid:88)
c(cid:88)

i=1

j=1

effects of feature selection. Although the (cid:96)0-norm of R3(W) is the most desirable [16]  in this
paper  we use R4(W) instead. The reasons are: (A) the (cid:96)1-norm of R4(W) is convex and can be
easily optimized (the main contribution of this paper); (B) it was shown that results of (cid:96)0-norm is
identical or approximately identical to the (cid:96)1-norm results under practical conditions.
Denote data matrix X = [x1  x2 ···   xn] ∈ Rd×n and label matrix Y = [y1  y2 ···   yn]T ∈
Rn×c. In this paper  we optimize

min
W

J(W) =

2 + γR4(W) =

2 1 + γ (cid:107)W(cid:107)2 1 .

(10)

(cid:176)(cid:176)XT W − Y

(cid:176)(cid:176)

(cid:176)(cid:176)WT xi − yi

(cid:176)(cid:176)

n(cid:88)

i=1

It seems that solving this joint (cid:96)2 1-norm problem is difﬁcult as both of the terms are non-smooth.
Surprisingly  we will show in the next section that the problem can be solved using a simple yet
efﬁcient algorithm.

4 An Efﬁcient Algorithm

4.1 Reformulation as A Constrained Problem

First  the problem in Eq. (10) is equivalent to

(cid:176)(cid:176)XT W − Y

(cid:176)(cid:176)

2 1 + (cid:107)W(cid:107)2 1  

min
W

1
γ

which is further equivalent to

(cid:107)E(cid:107)2 1 + (cid:107)W(cid:107)2 1

s.t.

XT W + γE = Y.

(11)

(12)

min
W E

(cid:176)(cid:176)(cid:176)(cid:176)(cid:183)

(cid:184)(cid:176)(cid:176)(cid:176)(cid:176)

Rewriting the above problem as
W
E

min
W E

W
E
where I ∈ Rn×n is an identity matrix. Denote m = n + d. Let A =
∈ Rm×c  then the problem in Eq. (13) can be written as:
U =

(cid:183)

(cid:184)

XT

γI

s.t.

2 1

W
E

(cid:163)

(cid:164)(cid:183)

(cid:184)
(cid:163)

= Y 

XT

γI

(13)

(cid:164) ∈ Rn×m and

(cid:107)U(cid:107)2 1

min
U

s.t.

AU = Y

(14)

This optimization problem Eq. (14) has been widely used in the Multiple Measurement Vector
(MMV) model in signal processing community. It was generally felt that the (cid:96)2 1-norm minimization
problem is much more difﬁcult to solve than the (cid:96)1-norm minimization problem. Existing algorithms
usually reformulate it as a second-order cone programming (SOCP) or semideﬁnite programming
(SDP) problem  which can be solved by interior point method or the bundle method. However  solv-
ing SOCP or SDP is computationally very expensive  which limits their use in practice. Recently 
an efﬁcient algorithm was proposed to solve the speciﬁc problem Eq. (14) by complicatedly refor-
mulating the problem as a min-max problem and then applying the proximal method to solve it [25].
The reported results show that the algorithm is more efﬁcient than existing algorithms. However  the
algorithm is a gradient descent type method and converges very slow. Moreover  the algorithm is de-
rived to solve the speciﬁc problem  and can not be applied directly to solve other general (cid:96)2 1-norm
minimization problem.
In the next subsection  we will propose a very simple but at the same time much more efﬁcient
method to solve this problem. Theoretical analysis guarantees that the proposed method will con-
verge to the global optimum. More importantly  this method is very easy to implement and can be
readily used to solve other general (cid:96)2 1-norm minimization problem.

4.2 An Efﬁcient Algorithm to Solve the Constrained Problem

The Lagrangian function of the problem in Eq. (14) is

L(U) = (cid:107)U(cid:107)2 1 − T r(ΛT (AU − Y)).

(15)

4

Taking the derivative of L(U) w.r.t U  and setting the derivative to zero  we have:

= 2DU − AT Λ = 0 
where D is a diagonal matrix with the i-th diagonal element as1

∂L(U)
∂U

(16)

(17)
Left multiplying the two sides of Eq. (16) by AD−1  and using the constraint AU = Y  we have:

2(cid:107)ui(cid:107)2

dii =

.

1

2AU − AD−1AT Λ = 0
⇒ 2Y − AD−1AT Λ = 0
⇒ Λ = 2(AD−1AT )−1Y

(18)

(19)

Substitute Eq. (18) into Eq. (16)  we arrive at:

U = D−1AT (AD−1AT )−1Y.

Since the problem in Eq. (14) is a convex problem  U is a global optimum solution to the problem
if and only if the Eq. (19) is satisﬁed. Note that D is dependent to U and thus is also a unknown
variable. We propose an iterative algorithm in this paper to obtain the solution U such that Eq. (19)
is satisﬁed  and prove in the next subsection that the proposed iterative algorithm will converge to
the global optimum.
The algorithm is described in Algorithm 1. In each iteration  U is calculated with the current D 
and then D is updated based on the current calculated U. The iteration procedure is repeated until
the algorithm converges.

Data: A ∈ Rn×m  Y ∈ Rn×c
Result: U ∈ Rm×c
Set t = 0. Initialize Dt ∈ Rm×m as an identity matrix
repeat

Calculate Ut+1 = D−1
Calculate the diagonal matrix Dt+1  where the i-th diagonal element is
t = t + 1.

t AT (AD−1

t AT )−1Y.

2(cid:107)ui

1

t+1(cid:107)2

.

until Converges

Algorithm 1: An efﬁcient iterative algorithm to solve the optimization problem in Eq. (14).

4.3 Algorithm Analysis

The Algorithm 1 monotonically decreases the objective of the problem in Eq. (14) in each iteration.
To prove it  we need the following lemma:
Lemma 1. For any nonzero vectors u  ut ∈ Rc  the following inequality holds:

2

≤ (cid:107)ut(cid:107)2 − (cid:107)ut(cid:107)2
(cid:107)u(cid:107)2 − (cid:107)u(cid:107)2
2(cid:107)ut(cid:107)2
2(cid:107)ut(cid:107)2
√
v − √
Proof. Beginning with an obvious inequality (
vvt + vt ≥ 0 ⇒ √
v − v
√
2
2 and (cid:107)ut(cid:107)2

Substitute the v and vt in Eq. (21) by (cid:107)u(cid:107)2

√
vt)2 ≥ 0 ⇒ v − 2

v − √

vt)2 ≥ 0  we have
≤

⇒ √

√
(

vt
2

√

2

.

vt − vt
√
vt
vt
2
2 respectively  we arrive at the Eq. (20).

v − v
√
2

vt

≤ √

(20)

(21)

(cid:176)(cid:176)

(cid:176)(cid:176)ui

1When ui = 0  then dii = 0 is a subgradient of (cid:107)U(cid:107)2 1 w.r.t. ui. However  we can not set dii = 0 when
ui = 0  otherwise the derived algorithm can not be guaranteed to converge. Two methods can be used to solve
this problem. First  we will see from Eq.(19) that we only need to calculate D−1  so we can let the i-th element
of D−1 as 2
  and the derived algorithm can be

. Second  we can regularize dii as dii =

2
proved to minimize the regularized (cid:96)2 1-norms of U (deﬁned as
of U. It is easy to see that the regularized (cid:96)2 1-norms of U approximates the (cid:96)2 1-norms of U when ς → 0.

(ui)T ui + ς) instead of the (cid:96)2 1-norms

n(cid:80)

(cid:112)

(ui)T ui+ς

√

i=1

2

1

5

The convergence of the Algorithm 1 is summarized in the following theorem:
Theorem 1. The Algorithm 1 will monotonically decrease the objective of the problem in Eq. (14)
in each iteration  and converge to the global optimum of the problem.

Proof. It can easily veriﬁed that Eq. (19) is the solution to the following problem:

Thus in the t iteration 

min
U

T r(UT DU)

s.t.

AU = Y

Ut+1 = arg
U

min
AU=Y

T rUT DtU 

t and ui

t+1 denote the i-th row of matrices Ut and Ut+1  respectively.

t DtUt).

 

t

2

2

2

2

t

2

i=1

i=1

t

2

t+1

which indicates that

T r(UT

That is to say 

t+1DtUt+1) ≤ T r(UT
m(cid:88)

where vectors ui
On the other hand  according to Lemma 1  for each i we have
2 −

(cid:176)(cid:176)2
(cid:176)(cid:176)ui
(cid:176)(cid:176)2
(cid:176)(cid:176)ui
≤ m(cid:88)
(cid:176)(cid:176)ui
(cid:176)(cid:176)
(cid:176)(cid:176)
(cid:176)(cid:176)ui
(cid:176)(cid:176)2
(cid:176)(cid:176)ui
(cid:176)(cid:176)
≤(cid:176)(cid:176)ui
(cid:176)(cid:176)ui
(cid:176)(cid:176)
(cid:195)(cid:176)(cid:176)ui
(cid:33)
(cid:176)(cid:176)2
(cid:176)(cid:176)ui
≤ m(cid:88)
(cid:176)(cid:176)ui
(cid:176)(cid:176)
Thus the following inequality holds:
2 −
m(cid:88)
2 ≤ m(cid:88)
(cid:176)(cid:176)ui
(cid:176)(cid:176)ui
(cid:176)(cid:176)
(cid:176)(cid:176)

2
Combining Eq. (25) and Eq. (27)  we arrive at

(cid:176)(cid:176)ui
(cid:195)(cid:176)(cid:176)ui

m(cid:88)

2 −

(cid:176)(cid:176)

(cid:176)(cid:176)

t+1

t+1

t+1

t+1

t+1

t

2

i=1

i=1

2

t

2

2

t

2

2.

t

i=1

i=1

t

2

(cid:176)(cid:176)ui
(cid:176)(cid:176)2
(cid:176)(cid:176)ui
(cid:176)(cid:176)
(cid:176)(cid:176)2
(cid:176)(cid:176)ui
(cid:176)(cid:176)
(cid:176)(cid:176)ui
(cid:176)(cid:176)

2 −

2

2

2

.

t

t

2

t

t

2

(cid:33)

.

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(cid:176)(cid:176)ui

(cid:176)(cid:176)

That is to say 

(cid:107)Ut+1(cid:107)2 1 ≤ (cid:107)Ut(cid:107)2 1 .

(29)
Thus the Algorithm 1 will monotonically decrease the objective of the problem in Eq. (14) in each
iteration t. In the convergence  Ut and Dt will satisfy the Eq. (19). As the problem in Eq. (14)
is a convex problem  satisfying the Eq. (19) indicates that U is a global optimum solution to the
problem in Eq. (14). Therefore  the Algorithm 1 will converge to the global optimum of the problem
(14).

Note that in each iteration  the Eq. (19) can be solved efﬁciently. First  D is a diagonal matrix and
thus D−1 is also diagonal with the i-th diagonal element as d−1
2. Second  the term
Z = (AD−1AT )−1Y in Eq. (19) can be efﬁciently obtained by solving the linear equation:

ii = 2

(AD−1AT )Z = Y.

(30)
Empirical results show that the convergence is fast and only a few iterations are needed to converge.
Therefore  the proposed method can be applied to large scale problem in practice.
It is worth to point out that the proposed method can be easily extended to solve other (cid:96)2 1-norm
minimization problem. For example  considering a general (cid:96)2 1-norm minimization problem as
follows:

(cid:88)

(cid:107)AkU + Bk(cid:107)2 1

s.t.

U ∈ C

(31)

The problem can be solved by solve the following problem iteratively:

min
U

f(U) +

T r((AkU + Bk)T Dk(AkU + Bk))

s.t.

U ∈ C

(32)

where Dk is a diagonal matrix with the i-th diagonal element as
. Similar theoretical
analysis can be used to prove that the iterative method will converge to a local minimum. If the
problem Eq. (31) is a convex problem  i.e.  f(U) is a convex function and C is a convex set  then the
iterative method will converge to the global minimum.

2(cid:107)(AkU+Bk)i(cid:107)2

1

6

min
U

f(U) +

k

(cid:88)

k

(a) ALLAML

(b) GLIOMA

(c) LUNG

(d) Carcinomas

(e) PROSTATE-GE

(f) PROSTATE-MS

Figure 1: Classiﬁcation accuracy comparisons of six feature selection algorithms on 6 data sets.
SVM with 5-fold cross validation is used for classiﬁcation. RFS is our method.

5 Experimental Results

In order to validate the performance of our feature selection method  we applied our method into two
bioinformatics applications  gene expression and mass spectrometry classiﬁcations. In our experi-
ments  we used ﬁve publicly available microarray data sets and one Mass Spectrometry (MS) data
sets: ALLAML data set [6]  the malignant glioma (GLIOMA) data set [17]  the human lung carcino-
mas (LUNG) data set [2]  Human Carcinomas (Carcinomas) data set [24  27]  Prostate Cancer gene
expression (Prostate-GE) data set [23] for microarray data; and Prostate Cancer (Prostate-MS) [20]
for MS data. The Support Vector Machine (SVM) classiﬁer is employed to these data sets  using
5-fold cross-validation.

5.1 Data Sets Descriptions

We give a brief description on all data sets used in our experiments as follows.
ALLAML data set contains in total 72 samples in two classes  ALL and AML  which contain 47
and 25 samples  respectively. Every sample contains 7 129 gene expression values.
GLIOMA data set contains in total 50 samples in four classes  cancer glioblastomas (CG)  non-
cancer glioblastomas (NG)  cancer oligodendrogliomas (CO) and non-cancer oligodendrogliomas
(NO)  which have 14  14  7 15 samples  respectively. Each sample has 12625 genes. Genes with
minimal variations across the samples were removed. For this data set  intensity thresholds were
set at 20 and 16 000 units. Genes whose expression levels varied < 100 units between samples  or
varied < 3 fold between any two samples  were excluded. After preprocessing  we obtained a data
set with 50 samples and 4433 genes.
LUNG data set contains in total 203 samples in ﬁve classes  which have 139  21  20  6 17 samples 
respectively. Each sample has 12600 genes. The genes with standard deviations smaller than 50
expression units were removed and we obtained a data set with 203 samples and 3312 genes.
Carcinomas data set composed of total 174 samples in eleven classes  prostate  bladder/ureter 
breast  colorectal  gastroesophagus  kidney  liver  ovary  pancreas  lung adenocarcinomas  and lung
squamous cell carcinoma  which have 26  8  26  23  12  11  7  27  6  14  14 samples  respectively.
In the original data [24]  each sample contains 12533 genes. In the preprocessed data set [27]  there
are 174 samples and 9182 genes.

7

01020304050607080707580859095the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFS010203040506070803035404550556065707580the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFS010203040506070807580859095100the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFS01020304050607080102030405060708090100the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFS0102030405060708080828486889092949698the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFS01020304050607080707580859095100the number of features selectedthe classification accuracy ReliefFFscoreRankT−testInformation gainmRMRRFSTable 1: Classiﬁcation Accuracy of SVM using 5-fold cross validation. Six feature selection meth-
ods are compared. RF: ReliefF  F-s: F-score  IG: Information Gain  and RFS: our method.

Average accuracy of top 20 features (%) Average accuracy of top 80 features (%)
RF

F-s T-test

RF
IG mRMR
95.89 96.07 94.29 95.71 94.46

50

F-s T-test

IG mRMR
ALLAML 90.36 89.11 92.86 93.21 93.21
GLIOMA 50
LUNG
91.68 87.7 89.22 93.1 92.61
Carcinom. 79.88 65.48 49.9 85.09 78.22
92.18 95.09 92.18 92.18 93.18
Pro-GE
76.41 98.89 95.56 98.89 95.42
Pro-MS
Average
80.09 81.04 79.29 87.09 85.78

62

56

60

RFS
95.89
74
93.63
91.38
95.09
98.89
91.48

66

66

54

60

58

93.63 91.63 90.66 95.1 94.12
90.24 83.33 68.91 89.65 87.92
91.18 93.18 93.18 89.27 86.36
89.93 98.89 94.44 98.89 93.14
85.81 87.18 83.25 89.10

87

RFS
97.32
70
96.07
93.66
95.09
100
92.02

Prostate-GE data set has in total 102 samples in two classes tumor and normal  which have 52 and
50 samples  respectively. The original data set contains 12600 genes. In our experiment  intensity
thresholds were set at 100 C16000 units. Then we ﬁltered out the genes with max/min ≤ 5 or
(max-min) ≤ 50. After preprocessing  we obtained a data set with 102 samples and 5966 genes.
Prostate-MS data can be obtained from the FDA-NCI Clinical Proteomics Program Databank [20].
This MS data set consists of 190 samples diagnosed as benign prostate hyperplasia  63 samples
considered as no evidence of disease  and 69 samples diagnosed as prostate cancer. The samples
diagnosed as benign prostate hyperplasia as well as samples having no evidence of prostate cancer
were pooled into one set making 253 control samples  whereas the other 69 samples are the cancer
samples.

5.2 Classiﬁcation Accuracy Comparisons

All data sets are standardized to be zero-mean and normalized by standard deviation. SVM classiﬁer
has been individually performed on all data sets using 5-fold cross-validation. We utilize the linear
kernel with the parameter C = 1. We compare our feature selection method (called as RFS) to
several popularly used feature selection methods in bioinformatics  such as F -statistic [4]  reliefF
[11  13]  mRMR [19]  t-test  and information gain [21]. Because the above data sets are for multi-
class classiﬁcation problem  we don’t compare to (cid:96)1-SVM  HHSVM and other methods that were
designed for binary classiﬁcation.
Fig. 1 shows the classiﬁcation accuracy comparisons of all ﬁve feature selection methods on six data
sets. Table 1 shows the detailed experimental results using SVM. We compute the average accuracy
using the top 20 and top 80 features for all feature selection approaches. Obviously our approaches
outperform other methods signiﬁcantly. With top 20 features  our method is around 5%-12% better
than other methods all six data sets.

6 Conclusions

In this paper  we proposed a new efﬁcient and robust feature selection method with emphasizing joint
(cid:96)2 1-norm minimization on both loss function and regularization. The (cid:96)2 1-norm based regression
loss function is robust to outliers in data points and also efﬁcient in calculation. Motivated by
previous work  the (cid:96)2 1-norm regularization is used to select features across all data points with
joint sparsity. We provided an efﬁcient algorithm with proved convergence. Our method has been
applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies have
been performed on two bioinformatics tasks  six data sets  to demonstrate the performance of our
method.

7 Acknowledgements

This research was funded by US NSF-CCF-0830780  0939187  0917274  NSF DMS-0915228  NSF
CNS-0923494  1035913.

8

References
[1] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. NIPS  pages 41–48  2007.
[2] A. Bhattacharjee  W. G. Richards  and et. al. Classiﬁcation of human lung carcinomas by mRNA ex-
pression proﬁling reveals distinct adenocarcinoma subclasses. Proceedings of the National Academy of
Sciences  98(24):13790–13795  2001.

[3] P. Bradley and O. Mangasarian. Feature selection via concave minimization and support vector machines.

ICML  1998.

[4] C. Ding and H. Peng. Minimum redundancy feature selection from microarray gene expression data.

Proceedings of the Computational Systems Bioinformatics  2003.

[5] C. Ding  D. Zhou  X. He  and H. Zha. R1-PCA: Rotational invariant L1-norm principal component

analysis for robust subspace factorization. Proc. Int’l Conf. Machine Learning (ICML)  June 2006.

[6] S. P. Fodor. DNA SEQUENCING: Massively Parallel Genomics. Science  277(5324):393–395  1997.
[7] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. J. Machine Learning Re-

search  2003.

[8] I. Guyon  J.Weston  S. Barnhill  and V. Vapnik. Gene selection for cancer classiﬁcation using support

vector machines. Machine Learning  46(1):389  2002.

[9] M. A. Hall and L. A. Smith. Feature selection for machine learning: Comparing a correlation-based ﬁlter

approach to the wrapper. 1999.

[10] H. Huang and C. Ding. Robust tensor factorization using r1 norm. CVPR 2008  pages 1–8  2008.
[11] K. Kira and L. A. Rendell. A practical approach to feature selection. In A Practical Approach to Feature

Selection  pages 249–256  1992.

[12] R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence  97(1-2):273–324 

1997.

[13] I. Kononenko. Estimating attributes: Analysis and extensions of RELIEF. In European Conference on

Machine Learning  pages 171–182  1994.

[14] P. Langley. Selection of relevant features in machine learning. In AAAI Fall Symposium on Relevance 

pages 140–144  1994.

[15] H. Liu and H. Motoda. Feature Selection for Knowledge Discovery and Data Mining. Springer  1998.
[16] D. Luo  C. Ding  and H. Huang. Towards structural sparsity: An explicit (cid:96)2/(cid:96)0 approach. ICDM  2010.
[17] C. L. Nutt  D. R. Mani  R. A. Betensky  P. Tamayo  J. G. Cairncross  C. Ladd  U. Pohl  C. Hartmann 
and M. E. Mclaughlin. Gene expression-based classiﬁcation of malignant gliomas correlates better with
survival than histological classiﬁcation. Cancer Res.  63:1602–1607  2003.

[18] G. Obozinski  B. Taskar  and M. Jordan. Multi-task feature selection. Technical report  Department of

Statistics  University of California  Berkeley  2006.

[19] H. Peng  F. Long  and C. Ding. Feature selection based on mutual information: Criteria of max-depe
ndency  max-relevance  and min-redundancy. IEEE Trans. Pattern Analysis and Machine Intelligence 
27  2005.

[20] P. C. Petricoin EF  Ornstein DK. Serum proteomic patterns for detection of prostate cancer. J Natl Cancer

Inst.  94(20):1576–8  2002.

[21] L. E. Raileanu and K. Stoffel. Theoretical comparison between the gini index and information gain

criteria. Univeristy of Neuchatel  2000.

[22] Y. Saeys  I. Inza  and P. Larranaga. A review of feature selection techniques in bioinformatics. Bioinfor-

matics  23(19):2507–2517  2007.

[23] D. Singh  P. Febbo  K. Ross  and et al. Gene expression correlates of clinical prostate cancer behavior.

Cancer Cell  pages 203–209  2002.

[24] A. I. Su  J. B. Welsh  L. M. Sapinoso  and et al. Molecular classiﬁcation of human carcinomas by use of

gene expression signatures. Cancer Research  61:7388–7393  2001.

[25] L. Sun  J. Liu  J. Chen  and J. Ye. Efﬁcient recovery of jointly sparse vectors. In Neural Information

Processing Systems  2009.

[26] L. Wang  J. Zhu  and H. Zou. Hybrid huberized support vector machines for microarray classiﬁcation.

ICML  2007.

[27] K. Yang  Z. Cai  J. Li  and G. Lin. A stable gene selection in microarray data analysis. BMC Bioinfor-

matics  7:228  2006.

[28] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B  68:49–67  2005.

9

,Volodymyr Mnih
Nicolas Heess
Alex Graves
koray kavukcuoglu