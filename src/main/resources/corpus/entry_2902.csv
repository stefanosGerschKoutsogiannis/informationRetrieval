2019,Learning Distributions Generated by One-Layer ReLU Networks,We consider the problem of estimating the parameters of a $d$-dimensional rectified Gaussian distribution from i.i.d. samples. A rectified Gaussian distribution is defined by passing a standard Gaussian distribution through a one-layer ReLU neural network. We give a simple algorithm to estimate the parameters (i.e.  the weight matrix and bias vector of the ReLU neural network) up to an error $\eps\norm{W}_F$ using $\widetilde{O}(1/\eps^2)$ samples and $\widetilde{O}(d^2/\eps^2)$ time (log factors are ignored for simplicity). This implies that we can estimate the distribution up to $\eps$ in total variation distance using $\widetilde{O}(\kappa^2d^2/\eps^2)$ samples  where $\kappa$ is the condition number of the covariance matrix. Our only assumption is that the bias vector is non-negative. Without this non-negativity assumption  we show that estimating the bias vector within any error requires the number of samples at least exponential in the infinity norm of the bias vector. Our algorithm is based on the key observation that vector norms and pairwise angles can be estimated separately. We use a recent result on learning from truncated samples. We also prove two sample complexity lower bounds: $\Omega(1/\eps^2)$ samples are required to estimate the parameters up to error $\eps$  while $\Omega(d/\eps^2)$ samples are necessary to estimate the distribution up to $\eps$ in total variation distance. The first lower bound implies that our algorithm is optimal for parameter estimation. Finally  we show an interesting connection between learning a two-layer generative model and non-negative matrix factorization. Experimental results are provided to support our analysis.,Learning Distributions Generated by

One-Layer ReLU Networks

Shanshan Wu  Alexandros G. Dimakis  Sujay Sanghavi

Department of Electrical and Computer Engineering

University of Texas at Austin

shanshan@utexas.edu  dimakis@austin.utexas.edu  sanghavi@mail.utexas.edu

Abstract

We consider the problem of estimating the parameters of a d-dimensional rectiﬁed
Gaussian distribution from i.i.d. samples. A rectiﬁed Gaussian distribution is
deﬁned by passing a standard Gaussian distribution through a one-layer ReLU
neural network. We give a simple algorithm to estimate the parameters (i.e.  the
weight matrix and bias vector of the ReLU neural network) up to an error (cid:107)W(cid:107)F

using (cid:101)O(1/2) samples and (cid:101)O(d2/2) time (log factors are ignored for simplicity).
using (cid:101)O(κ2d2/2) samples  where κ is the condition number of the covariance

This implies that we can estimate the distribution up to  in total variation distance

matrix. Our only assumption is that the bias vector is non-negative. Without
this non-negativity assumption  we show that estimating the bias vector within
any error requires the number of samples at least exponential in the inﬁnity norm
of the bias vector. Our algorithm is based on the key observation that vector
norms and pairwise angles can be estimated separately. We use a recent result
on learning from truncated samples. We also prove two sample complexity lower
bounds: Ω(1/2) samples are required to estimate the parameters up to error  
while Ω(d/2) samples are necessary to estimate the distribution up to  in total
variation distance. The ﬁrst lower bound implies that our algorithm is optimal for
parameter estimation. Finally  we show an interesting connection between learning
a two-layer generative model and non-negative matrix factorization. Experimental
results are provided to support our analysis.

1

Introduction

Estimating a high-dimensional distribution from observed samples is a fundamental problem in
machine learning and statistics. A popular recent generative approach is to model complex distri-
butions by passing a simple distribution (typically a standard Gaussian) through a neural network.
Parameters of the neural network are then learned from data. Generative Adversarial Networks
(GANs) [GPAM+14] and Variational Auto-Encoders (VAEs) [KW13] are built on this method of
modeling high-dimensional distributions.
Current methods for learning such deep generative models do not have provable guarantees or sample
complexity bounds. In this paper we obtain the ﬁrst such results for a single-layer ReLU generative
model. Speciﬁcally  we study the following problem: Assume that the latent variable z is selected
from a standard Gaussian which then drives the generation of samples from a one-layer ReLU
activated neural network with weights W and bias b. We observe the output samples (but not the
latent variable realizations z) and we would like to provably learn the parameters W and b. More
formally:

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Deﬁnition 1. Let W ∈ Rd×k be the weight matrix  and b ∈ Rd be the bias vector. We deﬁne D(W  b)
as the distribution1 of the random variable x ∈ Rd generated as follows:
x = ReLU(W z + b)  where z ∼ N (0  Ik).

(1)

Here z is a standard Gaussian random variable in Rk  and Ik is a k-by-k identity matrix.
Given n samples x1  x2  ...  xn from some D(W  b) with unknown parameters W and b  the goal is
to estimate W and b from the given samples. Since the ReLU operation is not invertible2  estimating
W and b via maximum likelihood is often intractable.
In this paper  we make the following contributions:

• We provide a simple and novel algorithm to estimate the parameters of D(W  b) from i.i.d.
samples  under the assumption that b is non-negative. Our algorithm (Algorithm 1) takes two
steps. In Step 1  we estimate b and the row norms of W using a recent result on estimation
from truncated samples (Algorithm 2). In Step 2  we estimate the angles between any two
row vectors of W using a simple geometric result (Fact 1).

• We prove that the proposed algorithm needs (cid:101)O(1/2) samples and (cid:101)O(d2/2) time  in order to
estimate the parameter W W T (reps. b) within an error (cid:107)W(cid:107)2
F (resp. (cid:107)W(cid:107)F ) (Theorem 1).
learned distribution and the ground truth is within an error  given (cid:101)O(κ2d2/2) samples 
This implies that (for the non-degenerate case) the total variation distance between the
where κ is the condition number of W W T (Corollary 1).
• Without the non-negativity assumption on b  we show that estimating the parameters of
D(W  b) within any error requires Ω(exp((cid:107)b(cid:107)2∞)) samples (Claim 2). Even when the bias
vector b has negative components  our algorithm can still be used to recover part of the
parameters with a small amount of samples (Section H.1).
• We prove two lower bounds on the sample complexity. The ﬁrst lower bound (Theorem 2)
says that Ω(1/2) samples are required in order to estimate b up to error (cid:107)W(cid:107)F   which
implies that our algorithm is optimal in estimating the parameters. The second lower bound
(Theorem 3) says that Ω(d/2) samples are required to estimate the distribution up to total
variation distance .
• We empirically evaluate our algorithm in terms of its dependence over the number of
samples  dimension  and condition number (Figure 1). The empirical results are consistent
with our analysis.
• We provide a new algorithm to estimate the parameters of a two-layer generative model (Al-
gorithm 4 in Appendix H). Our algorithm uses ideas from non-negative matrix factorization
(Claim 3).

The (cid:96)p norm of a vector is deﬁned as (cid:107)x(cid:107)p = ((cid:80)
row and the j-th column. The dot product between two vectors is (cid:104)x  y(cid:105) =(cid:80)

Notation. We use capital letters to denote matrices and lower-case letters to denote vectors. We use
[n] to denote the set {1  2 ···   n}. For a vector x ∈ Rd  we use x(i) to denote its i-th coordinate.
i |x(i)|p)1/p. For a matrix W ∈ Rd×k  we use
W (i  j) to denote its (i  j)-th entry. We use W (i  :) ∈ Rk and W (:  j) ∈ Rd to the denote the i-th
i x(i)y(i). For any
a ∈ R  we use R>a to denote the set R>a := {x ∈ R : x > a}. We use Ik ∈ Rk×k to denote an
identity matrix.

2 Related Work

We brieﬂy review the relevant work  and highlight the differences compared to our paper.
Estimation from truncated samples. Given a d-dimensional distribution D and a subset S ⊆ Rd 
truncation means that we can only observe samples from D if it falls in S. Samples falling outside
S (and their counts in proportion) are not revealed. Estimating the parameters of a multivariate

1It is also called as a rectiﬁed Gaussian distribution  and can be used in non-negative factor analysis [HK07].
2If the activation function σ (e.g.  sigmoid  leaky ReLU  etc.) is invertible  then σ−1(X) ∼ N (b  W W T ).

In that case the problem becomes learning a Gaussian from samples.

2

normal distribution from truncated samples is a fundamental problem in statistics and a breakthrough
was achieved recently [DGTZ18] on this problem. This is different from our problem because our
samples are formed by projecting the samples of a multivariate normal distribution onto the positive
orthant instead of truncating to the positive orthant. Nevertheless  a single coordinate of D(W  b) can
be viewed as a truncated univariate normal distribution (Deﬁnition 2). We use this observation and
leverage on the recent results of [DGTZ18] to estimate b and the row norms of W (Section 4.2).
Learning ReLU neural networks. A recent series of work  e.g.  [GMOV19  GKLW19  GKM18 
LY17  ZSJ+17  Sol17]  considers the problem of estimating the parameters of a ReLU neural network
given samples of the form {(xi  yi)}n
i=1. Here (xi  yi) represents the input features and the output
target  e.g.  yi = ReLU(W xi + b). This is a supervised learning problem  and hence  is different
from our unsupervised density estimation problem.
Learning neural network-based generative models. Many approaches have been proposed to
train a neural network to model complex distributions. Examples include GAN [GPAM+14] and
its variants (e.g.  WGAN [ACB17]  DCGAN [RMC15]  etc.)  VAE [KW13]  autoregressive mod-
els [OKK16]  and reversible generative models [GCB+18]. All of those methods lack theoretical
guarantees and explicit sample complexity bounds. A recent work [NWH18] proves that training an
autoencoder via gradient descent can possibly recover a linear generative model. This is different
from our setting  where we focus on non-linear generative models. Arya and Ankit [MR19] also con-
sider the problem of learning from one-layer ReLU generative models. Their modeling assumption
is different from ours. They assume that the bias vector b is a random variable whose distribution
satisﬁes certain conditions. Besides  there is no distributional assumption on the hidden variable z.
By contrast  in our model  both W and b are deterministic and unknown parameters. The randomness
only comes from z which is assumed to follow a standard Gaussian distribution.

Identiﬁability

3
Our ﬁrst question is whether W is identiﬁable from the distribution D(W  b). Claim 1 below implies
that only W W T can be possibly identiﬁed from D(W  b).
2   and any vector b  D(W1  b) = D(W2  b).
Claim 1. For any matrices satisfying W1W T
1 = W2W T

Proof. Since W1W T
Since z ∼ N (0  Ik)  we have Qz ∼ N (0  Ik). The claim then follows.

1 = W2W T

2   there exists a unitary matrix Q ∈ Rk×k that satisﬁes W2 = W1Q.

Identifying the bias vector b from D(W  b) can be impossible in some cases. For example  if W is a
zero matrix  then any negative coordinate of b cannot be identiﬁed since it will be reset to zero after
the ReLU operation. For the cases when b is identiﬁable  our next claim provides a lower bound on
the sample complexity required to estimate the bias vector to be within an additive error .
Claim 2. For any value δ > 0  there exists one-dimensional distributions D(1  b1) and D(1  b2) such
that: (a) |b1 − b2| = δ; (b) at least Ω(exp(b2

1/2)) samples are required to distinguish them.

Proof. Let b1 < 0 and b2 = b1 − δ. It is easy to check that (a) holds. To show (b)  note that
the probability of observing a positive (i.e.  nonzero) sample from D(1  b1) is upper bounded by
P[ReLU(z − |b1|) > 0] = P[z > |b1|] ≤ exp(−b2
1/2)  where the last step follows from the standard
Gaussian tail bound [Wai19]. The same bound holds for D(1  b2). To distinguish D(1  b1) and
D(1  b2)  we need to observe at least one nonzero sample  which requires Ω(exp(b2

1/2)) samples.

Claim 2 indicates that in order to estimate the parameters within any error  the sample complexity
should scale at least exponentially in (cid:107)b(cid:107)2∞. This is true if b is allowed to take negative values.
Intuitively  if b has large negative values  then most of the samples would be zeros. To avoid this
exponential dependence  we now assume that the bias vector is non-negative. In Section 4  we give an
algorithm to provably learn the parameters of D(W  b) with a sample complexity that is polynomial
in 1/ and does not depend on the values of b. In Section H.1  we show that even when the bias
vector has negative coordinates  our algorithm can still be able to recover part of the parameters with
a small number of samples.

3

4 Algorithm
In this section  we describe a novel algorithm to estimate W W T ∈ Rd×d and b ∈ Rd from i.i.d.
samples of D(W  b). Our goal is to estimate W W T instead of W since W is not identiﬁable (Claim 1).
Our only assumption is that the true b is non-negative. As discussed in Claim 2  this assumption
can potentially avoid the exponential dependence in the values of b. Note that our algorithm does
not require to know the dimension k of the latent variable z. Omitted proofs can be found in the
appendix.

Intuition

4.1
Let W (i  :) ∈ Rk be the i-th row (i ∈ [d]) of W . For any i < j ∈ [d]  the (i  j)-th entry of W W T is
(2)

(cid:104)W (i  :)  W (j  :)(cid:105) = (cid:107)W (i  :)(cid:107)2(cid:107)W (j  :)(cid:107)2 cos(θij) 

where θij is the angle between vectors W (i  :) and W (j  :). Our key idea is to estimate the norms
(cid:107)W (i  :)(cid:107)2  (cid:107)W (j  :)(cid:107)2  and the angles θij separately  as shown in Algorithm 1.
Estimating the row norms3 (cid:107)W (i  :)(cid:107)2 as well as the i-th coordinate of the bias vector b(i) ∈ R can
be done by only looking at the i-th coordinate of the given samples. The idea is to view the problem
as estimating the parameters of a univariate normal distribution from truncated samples4. This part of
the algorithm is described in Section 4.2. To estimate θij ∈ [0  π) for every i < j ∈ [d]  we use a
simple fact that the angle between any two vectors can be estimated from their inner products with a
random Gaussian vector. Details of this part can be found in Section 4.3.

Algorithm 1: Learning a single-layer ReLU generative model
Input: n i.i.d. samples x1 ···   xn ∈ Rd from D(W ∗  b∗)  b∗ is non-negative.
1 for i ← 1 to d do
2

S ← {xm(i)  m ∈ [n] : xm(i) > 0};

Output: (cid:98)Σ ∈ Rd×d (cid:98)b ∈ Rd.
(cid:98)b(i) (cid:98)Σ(i  i) ← NormBiasEst(S);
(cid:17)
(cid:16)
0 (cid:98)b(i)
(cid:98)b(i) ← max
(cid:16)(cid:80)n
(cid:98)θij ← π − 2π
(cid:98)Σ(i  j) ←(cid:113)(cid:98)Σ(i  i)(cid:98)Σ(j  j) cos((cid:98)θij);
(cid:98)Σ(j  i) ←(cid:98)Σ(i  j);

4
5 end
6 for i < j ∈ [d] do
7

m=1

n

3

8

;

9
10 end

1(xm(i) >(cid:98)b(i)) 1(xm(j) >(cid:98)b(j))

(cid:17)

;

4.2 Estimate (cid:107)W (i  :)(cid:107)2 and b(i)
Without loss of generality  we ﬁx i = 1 and describe how to estimate (cid:107)W (1  :)(cid:107)2 ∈ R and b(1) ∈ R
by looking at the ﬁrst coordinate of the given samples.
The starting point of our algorithm is the following observation. Suppose x ∼ D(W  b)  its ﬁrst
coordinate can be written as

x(1) = ReLU(W (1  :)T z + b(1)) = ReLU(y)  where y ∼ N (b(1) (cid:107)W (1  :)(cid:107)2
2).

(3)

Because of the ReLU operation  we can only observe the samples of y when it is positive. Given
samples of x(1) ∈ R  let us keep the samples that have positive values (i.e.  ignore the zero samples).
3Without loss of generality  we can assume that (cid:107)W (i  :)(cid:107)2 (cid:54)= 0 for all i ∈ [d]. If W (i  :) is a zero vector 

one can easily detect that and ﬁgure out the corresponding non-negative bias term.

4Another idea is to use the median of the samples to estimate the i-th coordinate of the bias vector. This

approach will give the same sample complexity bound as that of our proposed algorithm.

4

Now the problem of estimating b(1) and (cid:107)W (1  :)(cid:107)2 is equivalent to estimating the parameters of a
one-dimensional normal distribution using samples falling in the set R>0 := {x ∈ R : x > 0}.
Recently Daskalakis et al. [DGTZ18] gave an efﬁcient algorithm for estimating the mean and
covariance matrix of a multivariate Gaussian distribution from truncated samples. We adapt their
algorithm for the speciﬁc problem described above. Before describing the details  we start with a
formal deﬁnition of the truncated (univariate) normal distribution.
Deﬁnition 2. The univariate normal distribution N (µ  σ2) has probability density function

N (µ  σ2; x) =

1√
2πσ2

exp

− 1
2σ2 (x − µ)2

for x ∈ R.

 

Given a measurable set S ⊆ R  the S-truncated normal distribution N (µ  σ2  S) is deﬁned as

(cid:19)

(cid:18)
(cid:40) N (µ σ2;x)
(cid:82)

0

N (µ  σ2  S; x) =

S N (µ σ2;y)dy

if x ∈ S
if x (cid:54)∈ S

.

(4)

(5)

We are now ready to describe the algorithm in [DGTZ18] applied to our problem. The pseudocode is
given in Algorithm 2. The algorithm is essentially maximum likelihood by projected stochastic gradi-
ent descent (SGD). Given a sample x ∼ N (µ∗  σ∗2  S)  let (cid:96)(µ  σ; x) be the negative log-likelihood
that x is from N (µ  σ2  S)  then (cid:96)(µ  σ; x) is a convex function with respect to a reparameterization
v = [1/σ2  µ/σ2] ∈ R2. We use (cid:96)(v; x) to denote the negative log-likelihood after this reparameter-
ization. Let ¯(cid:96)(v) = Ex[(cid:96)(v; x)] be the expected negative log-likelihood. Although it is intractable
to compute ¯(cid:96)(v)  its gradient ∇¯(cid:96)(v) with respect to v has a simple unbiased estimator. Speciﬁcally 
deﬁne a random vector g ∈ R2 as

  where x ∼ N (µ∗  σ∗2  S)  z ∼ N (µ  σ2  S).

(6)

(cid:20)−x2/2

(cid:21)

(cid:20)−z2/2

(cid:21)

x

+

z

g = −

We have that ∇¯(cid:96)(v) = Ex z[g]  i.e.  g is an unbiased estimator of ∇¯(cid:96)(v).
Eq. (6) indicates that one can maximize the log-likelihood via SGD  however  in order to efﬁciently
perform this optimization  we need three extra steps.
First  the convergence rate of SGD depends on the expected gradient norm E[(cid:107)g(cid:107)2
2] (Theorem 14.11
of [SSBD14]). In order to maintain a small gradient norm  we transform the given samples to a
new space (so that the empirical mean and variance is well-controlled) and perform optimization
in that space. After the optimization is done  the solution is transformed back to the original space.
Speciﬁcally  given samples x1 ···   xn ∼ N (µ∗  σ∗2  R>0)  we transform them as

xi → xi −(cid:98)µ0(cid:98)σ0

n(cid:88)
(xi −(cid:98)µ0)2.
samples truncated to the set R>−(cid:98)µ0/(cid:98)σ0 = {x ∈ R : x > −(cid:98)µ0/(cid:98)σ0}.

  where(cid:98)µ0 =

xi  (cid:98)σ2

0 =

n(cid:88)

i=1

1
n

1
n

i=1

In the transformed space  the problem becomes estimating parameters of a normal distribution with

(7)

Second  we need to control the strong-convexity of the objective function. This is done by pro-
jecting the parameters onto a domain where the strong-convexity is bounded. The domain Dr is
parameterized by r > 0 and is deﬁned as

Dr = {v ∈ R2 : 1/r ≤ v(1) ≤ r |v(2)| ≤ r}.

(8)

According to [DGTZ18  Section 3.4]  r = O(ln(1/α)/α2) is a hyper-parameter that only depends
S N (µ∗  σ∗2; y)dy (i.e.  the probability mass of original truncation set S). In our setting 
we have α ≥ 1/2. This is because the original truncation set is R>0 and µ∗ = b(1) ≥ 0. A large
value of r would lead to a small strong-convexity parameter. In our experiments  we set r = 3.
Third  a single run of the projected SGD algorithm only guarantees a constant probability of success.
To amplify the probability of success to 1 − δ/d  a standard procedure is to repeat the algorithm
O(ln(d/δ)) times. This procedure is illustrated in Step 2-5 in Algorithm 2.

on α =(cid:82)

5

Algorithm 2: NormBiasEst
Input: Samples from N (µ  σ2  R>0).

Output:(cid:98)µ ∈ R (cid:99)σ2 ∈ R.

1 Shift and rescale the samples using (7);
2 Split the samples into B = O(ln(d/δ))
3 For batch i ∈ [B]  run ProjSGD to get
4 S ← {v1 ···   vB};

batches;
vi ∈ R2;

Algorithm 3: ProjSGD

Input: T = (cid:101)O(ln(d/δ)/2)  λ > 0.

Output: v ∈ R2.
1 Initialize v(0) = [1  0] ∈ R2;
2 for t ← 1 to T do
3
4
5

g(t) ← Estimate the gradient using (6);
v(t) ← v(t−1) − g(t)/(λ · t);
v(t) ← Project v(t) to the domain in
(8);

(cid:80)
5 (cid:98)v ← arg minvi∈S
6 Transform(cid:98)v back to the original space;
j∈[B](cid:107)vi − vj(cid:107)2;
7 (cid:98)µ ←(cid:98)v(2)/(cid:98)v(1) (cid:99)σ2 ← 1/(cid:98)v(1);
Lemma 1. For any  ∈ (0  1) and δ ∈ (0  1)  Algorithm 1 takes n = (cid:101)O(cid:0) 1
D(W ∗  b∗) (for some non-negative b∗) and outputs(cid:98)b(i) and(cid:98)Σ(i  i) for all i ∈ [d] that satisfy
|(cid:98)b(i) − b∗(i)| ≤ (cid:107)W ∗(i  :)(cid:107)2

2 ≤(cid:98)Σ(i  i) ≤ (1 + )(cid:107)W ∗(i  :)(cid:107)2

7 v ←(cid:80)T

(1 − )(cid:107)W ∗(i  :)(cid:107)2

t=1 v(t)/T ;

2 ln( d

6 end

2 

δ )(cid:1) samples from

(9)

with probability at least 1 − δ.

4.3 Estimate θij
To estimate the angle between any two vectors W ∗(i  :) and W ∗(j  :) (where i (cid:54)= j ∈ [d])  we will
use the following result.
Fact 1. (Lemma 6.7 in [WS11]). Let z ∼ N (0  Ik) be a standard Gaussian random variable in Rk.
For any two non-zero vectors u  v ∈ Rk  the following holds:

P

z∼N (0 Ik)

[uT z > 0 and vT z > 0] =

π − θ
2π

  where θ = arccos

.

(10)

(cid:19)

(cid:18) (cid:104)u  v(cid:105)

(cid:107)u(cid:107)2(cid:107)v(cid:107)2

Fact 1 says that the angle between any two vectors can be estimated from the sign of their inner
products with a Gaussian random vector. Let x ∼ D(W ∗  b∗)  since b∗ is assumed to be non-negative 
Fact 1 gives an unbiased estimator for the pairwise angles.
Lemma 2. Suppose that x ∼ D(W ∗  b∗) and that b∗ ∈ Rd is non-negative  for all i (cid:54)= j ∈ [d] 

P

x∼D(W ∗ b∗)

[x(i) > b∗(i) and x(j) > b∗(j)] =

π − θ∗
2π

ij

 

(11)

where θ∗

ij is the angle between vectors W ∗(i  :) and W ∗(j  :).

Proof. Since x(i) = ReLU(cid:0)W ∗(i  :)T z + b∗(i)(cid:1) and b∗ is non-negative  we have

LHS =

P

z∼N (0 Ik)

[W ∗(i  :)T z > 0 and W ∗(j  :)T z > 0] =

π − θ∗
2π

ij

= RHS 

(12)

where the second equality follows from Fact 1.
Lemma 2 gives an unbiased estimator of θ∗
ij  however  it requires knowing the true bias vector b∗.
In the previous section  we give an algorithm that can estimate b∗(i) within an additive error of
(cid:107)W ∗(i  :)(cid:107)2 for all i ∈ [d]. Fortunately  this is good enough for estimating θ∗
ij within an additive
error of   as indicated by the following lemma.

Lemma 3. Let x ∼ D(W ∗  b∗)  where b∗ is non-negative. Suppose that(cid:98)b ∈ Rd is non-negative and
satisﬁes |(cid:98)b(i) − b∗(i)| ≤ (cid:107)W ∗(i  :)(cid:107)2 for all i ∈ [d] and some  > 0. Then for all i (cid:54)= j ∈ [d] 

(cid:12)(cid:12)(cid:12)P
[x(i) >(cid:98)b(i) and x(j) >(cid:98)b(j)] − P

x

x

[x(i) > b∗(i) and x(j) > b∗(j)]

(13)

(cid:12)(cid:12)(cid:12) ≤ .

6

m=1 of D(W ∗  b∗) and an estimated bias vector(cid:98)b  Lemma 2 and 3 implies that θ∗

Let 1(·) be the indicator function  e.g.  1(x > 0) = 1 if x > 0 and is 0 otherwise. Given samples
{xm}n
ij can be
estimated as

(cid:98)θij = π − 2π

n(cid:88)

1(xm(i) >(cid:98)b(i) and xm(j) >(cid:98)b(j)).

n

The following lemma shows that the estimated(cid:98)θij is close to the true θ∗
Lemma 4. For a ﬁxed pair of i (cid:54)= j ∈ [d]  for any   δ ∈ (0  1)  suppose(cid:98)b satisﬁes the condition in
Lemma 3  given 80 ln(2/δ)/2 samples  with probability at least 1 − δ  | cos((cid:98)θij) − cos(θ∗
ij)| ≤ .

ij.

m=1

(14)

4.4 Estimate W W T and b
Our overall algorithm is given in Algorithm 1. In the ﬁrst for-loop  we estimate the row norms of W ∗
and b∗. In the second for-loop  we estimate the angles between any two row vectors of W ∗.

Theorem 1. For any  ∈ (0  1) and δ ∈ (0  1)  Algorithm 1 takes n = (cid:101)O(cid:0) 1
D(W ∗  b∗) (for some non-negative b∗) and outputs(cid:98)Σ ∈ Rd×d and(cid:98)b ∈ Rd that satisfy
(cid:107)(cid:98)b − b∗(cid:107)2 ≤ (cid:107)W ∗(cid:107)F
(cid:16) d2
and space (cid:101)O(cid:0) d
with probability at least 1 − δ. Algorithm 1 runs in time (cid:101)O

(cid:107)(cid:98)Σ − W ∗W ∗T(cid:107)F ≤ (cid:107)W ∗(cid:107)2

δ )(cid:1) samples from
δ ) + d2(cid:1).

2 ln( d
δ )

2 ln( d

2 ln( d

(cid:17)

(15)

F  

Theorem 1 characterizes the sample complexity to achieve a small parameter estimation error. We
are also interested in the distance between the estimated distribution and the true distribution. Let
TV(A  B) be the total variation (TV) distance between two distributions A and B. Note that in order
for the TV distance to be meaningful5  we restrict ourselves to the non-degenerate case  i.e.  when W
is a full-rank square matrix. The following corollary characterizes the number of samples used by our
algorithm in order to achieve a small TV distance.
Corollary 1. Suppose that W ∗ ∈ Rd×d is full-rank. Let κ be the condition number of W ∗W ∗T . For
samples from D(W ∗  b∗)

(cid:16) κ2d2
(cid:17)
any  ∈ (0  1/2] and δ ∈ (0  1)  Algorithm 1 takes n = (cid:101)O
(for some non-negative b∗) and outputs a distribution D((cid:98)Σ1/2 (cid:98)b) that satisﬁes
(cid:16)D((cid:98)Σ1/2 (cid:98)b)  D(W ∗  b∗)
(cid:17) ≤  
(cid:16) κ2d4
with probability at least 1 − δ. Algorithm 1 runs in time (cid:101)O

and space (cid:101)O

(cid:16) κ2d3

2

ln( d
δ )

ln( d
δ )

ln( d
δ )

(cid:17)

(cid:17)

(16)

TV

2

2

.

5 Lower Bounds
In the previous section  we gave an algorithm to estimate W ∗W ∗T and b∗ using i.i.d. samples from
D(W ∗  b∗)  and analyzed its sample complexity. In this section  we provide lower bounds for this
density estimation problem. More precisely  we want to know: how many samples are necessary if
we want to learn D(W ∗  b∗) up to some error measure ?
Before stating our lower bounds  we ﬁrst formally deﬁne a framework for distribution learning6.
Let S be a class of distributions. Let d be some distance function between the two distributions (or
between the parameters of the two distributions). We say that a distribution learning algorithm learns
S with sample complexity m() if for any distribution p ∈ S  given m() i.i.d. samples from p  it
constructs a distribution q such that d(p  q) ≤  with success probability at least 2/37.

5The TV distance between two different degenerate distributions can be a constant. As an example  let
N (0  Σ1) and N (0  Σ2) be two Gaussian distributions in Rd. If both Σ1  Σ2 have rank smaller than d  then
TV(N (0  Σ1) N (0  Σ2)) = 1 as long as Σ1 (cid:54)= Σ2.

6This can be viewed as the standard PAC-learning framework [Val84].
7We focus on constant success probability here as standard techniques can be used to boost the success

probability to 1 − δ with an extra multiplicative factor ln(1/δ) in the sample complexity.

7

We have analyzed the performance of Algorithm 1 in terms of two distance metrics: the distance
in the parameter space (Theorem 1)  and the TV distance between two distributions (Corollary 1).
Accordingly  we will provide two sample complexity lower bounds.
Theorem 2. (Lower bound for parameter estimation). Let σ > 0 be a ﬁxed and known scalar. Let
Id be the identity matrix in Rd. Let S := {D(W  b) : W = σId  b ∈ Rd non-negative} be a class

of distributions in Rd. Any algorithm that learns S to satisfy (cid:107)(cid:98)b − b∗(cid:107)2 ≤ (cid:107)W ∗(cid:107)F with success

probability at least 2/3 requires Ω(1/2) samples.
Theorem 3. (Lower bound for distribution estimation). Let S := {D(W  0) : W ∈ Rd×d full rank}
be a set of distributions in Rd. Any algorithm that learns S within total variation distance  and
success probability at least 2/3 requires Ω(d/2) samples.

Comparing the sample complexity achieved by our algorithm (Theorem 1 and Corollary 1) and the
above lower bounds  we can see that 1) our algorithm matches the lower bound (up to log factors) for
parameter estimation; 2) there is a gap between our sample complexity and the lower bound for TV
distance. There are two possible reasons why this gap shows up.

• The lower bound given in Theorem 3 may be loose. In fact  since learning a d-dimensional

Gaussian distribution up to TV distance  requires(cid:101)Θ(d2/2) samples (this is both sufﬁcient

and necessary [ABDH+18])  it is reasonable to guess that learning rectiﬁed Gaussian
distributions also requires at least Ω(d2/2) samples. It is thus interesting to see if one can
show a better lower bound than Ω(d/2).
• Our sample complexity of learning D(W  b) up to TV distance  also depends on the condi-
tion number κ of W W T . Intuitively  this κ dependence shows up because our algorithm
estimates W W T entry-by-entry instead of estimating the matrix as a whole. Besides  our
algorithm is a proper learning algorithm  meaning that the output distribution belongs to
the family D(W  b). By contrast  the lower bound proved in Theorem 3 considers any
non-proper learning algorithm  i.e.  there is no constraint on the output distribution. One
interesting direction for future research is to see if one can remove this κ dependence.

6 Experiments

In this section  we provide empirical results to verify the correctness of our algorithm as well as
the analysis. Code to reproduce our result8 can be found at https://github.com/wushanshan/
densityEstimation.
We evaluate three performance metrics  as shown in Figure 1. The ﬁrst two metrics measure
the error between the estimated parameters and the ground truth. Speciﬁcally  we compute the

the parameter estimation error  we are also interested in the TV distance analyzed in Corollary 1:
TV
. It is difﬁcult to compute the TV distance exactly  so we instead
compute an upper bound of it. Let KL(A||B) denote the KL divergence between two distributions.

estimation errors analyzed in Theorem 1: (cid:107)(cid:98)Σ − W ∗W ∗T(cid:107)F /(cid:107)W(cid:107)2
(cid:17)
(cid:16)D((cid:98)Σ1/2 (cid:98)b)  D(W ∗  b∗)
Let Σ∗ = W ∗W ∗T . Assuming that both Σ∗ and(cid:98)Σ are full-rank  we have
(cid:17) ≤ TV
(cid:16)D((cid:98)Σ1/2 (cid:98)b)  D(W ∗  b∗)

(cid:16)N ((cid:98)b (cid:98)Σ) N (b∗  Σ∗)
(cid:17) ≤

F and (cid:107)(cid:98)b − b(cid:107)2/(cid:107)W(cid:107)F . Besides

TV

(cid:114)

(cid:16)N ((cid:98)b (cid:98)Σ)||N (b∗  Σ∗)
(cid:17)

KL

/2.

The ﬁrst inequality follows from the data-processing inequality given in Lemma 7 of Appendix F
(see also [ABDH+18  Fact A.5]): for any function f and random variables X  Y over the same space 
TV(f (X)  f (Y )) ≤ TV(X  Y ). The second inequality follows from the Pinsker’s inequality [Tsy09 
Lemma 2.5].
Sample Efﬁciency. The left plot of Figure 1 shows that both the parameter estimation errors and
the KL divergence decrease when we have more samples. Our experimental setting is simple: we
set the dimension as d = k = 5 and the condition number as 1; we generate W ∗ as a random
orthonormal matrix; we generate b∗ as a random normal vector  followed by a ReLU operation (to
ensure non-negativity). This plot indicates that our algorithm is able to accurately estimate the true
parameters and obtain a distribution that is close to the true distribution in TV distance.

8The hyper-parameters are B = 1 (in Algorithm 2)  r = 3 and λ = 0.1 (in Algorithm 3).

8

Figure 1: Best viewed in color. Empirical performance of our algorithm with respect to three
parameters: number of samples n  dimension d  and the condition number κ. Left: Fix d = 5 and
κ = 1. Middle: Fix n = 5 × 105 and κ = 1. Right: Fix n = 5 × 105 and d = 5. Every point shows
the mean and standard deviation across 10 runs. Each run corresponds to a different W ∗ and b∗.

Dependence on Dimension. In the middle plot of Figure 1  we use 5 × 105 samples and keep the
condition number to be 1. We then increase the dimension (d = k) from 5 to 25. Both W ∗ and b∗
are generated in the same manner as the previous plot. As shown in the middle plot  the parameter
estimation errors maintain the same value while the KL divergence increases as the dimension
increases. This is consistent with our analysis  because the sample complexity in Theorem 1 is
dimension-free (ignoring the log factor) while the sample complexity in Corollary 1 depends on d2.
Dependence on Condition Number. In the right plot of Figure 1  we keep the dimension d = k = 5
and the number of samples 5 × 105 ﬁxed. We then increase the condition number κ of W ∗W ∗T .
This plot shows the same trend as the middle plot  i.e.  the parameter estimation errors remain the
same while the KL divergence increases as κ increases  which is again consistent with our analysis.
The number of samples required to achieve an additive estimation error (Theorem 1) does not depend
on κ  while the sample complexity to guarantee a small TV distance (Corollary 1) depends on κ2.

7 Conclusion

A popular generative model nowadays is deﬁned by passing a standard Gaussian random variable
through a neural network. In this paper we are interested in the following fundamental question:
Given samples from this distribution  is it possible to recover the parameters of the underlying neural
network? We designed a new algorithm to provably recover the parameters of a single-layer ReLU
generative model from i.i.d. samples  under the assumption that the bias vector is non-negative. We
analyzed the sample complexity of the proposed algorithm in terms of two error metrics: parameter
estimation error and total variation distance. Sample complexity lower bounds and experimental
results are provided to support our analysis.
There are many questions that one could ask here. For example  what happens if the bias vector has
negative values? What if the generative model has two layers? What if the samples are noisy? We
summarized our thoughts on some problems in Appendix H. In particular  we showed an interesting
connection between learning a two-layer generative model and non-negative matrix factorization.
While our focus here is parameter recovery  one interesting direction for future work is to see whether
one can directly estimate the distribution in some distance without ﬁrst estimating the parameters.
Another interesting direction is to develop provable learning algorithms for the agnostic setting
instead of the realizable setting. Besides designing new algorithms  analyzing the existing algorithms 
e.g.  GANs  VAEs  and reversible generative models  is also an important research direction.

8 Acknowledgements

This research has been supported by NSF Grants 1302435  1564000  and 1618689  DMS 1723052 
CCF 1763702  AF 1901292 and research gifts by Google  Western Digital and NVIDIA.

9

012Number of Samples (x105)00.20.40.60.8Error510152025Dimension (d)00.10.20.3Error510152025Condition Number ()00.050.10.15ErrorReferences
[ABDH+18] Hassan Ashtiani  Shai Ben-David  Nicholas Harvey  Christopher Liaw  Abbas Mehra-
bian  and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures
of gaussians via sample compression schemes. In Advances in Neural Information
Processing Systems  pages 3412–3421  2018.

[ACB17] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein gan. arXiv preprint

arXiv:1701.07875  2017.

[AGKM12] Sanjeev Arora  Rong Ge  Ravindran Kannan  and Ankur Moitra. Computing a nonneg-
ative matrix factorization–provably. In Proceedings of the forty-fourth annual ACM
symposium on Theory of computing  pages 145–162. ACM  2012.

[CS09] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances

in neural information processing systems  pages 342–350  2009.

[DGTZ18] Constantinos Daskalakis  Themis Gouleakis  Chistos Tzamos  and Manolis Zampetakis.
Efﬁcient statistics  in high dimensions  from truncated samples. In 2018 IEEE 59th
Annual Symposium on Foundations of Computer Science (FOCS)  pages 639–649.
IEEE  2018.

[DMR18] Luc Devroye  Abbas Mehrabian  and Tommy Reddad. The total variation distance

between high-dimensional gaussians. arXiv preprint arXiv:1810.08693  2018.

[DS04] David Donoho and Victoria Stodden. When does non-negative matrix factorization
give a correct decomposition into parts? In Advances in neural information processing
systems  pages 1141–1148  2004.

[Duc19] John Duchi. Lecture notes for statistics 311/electrical engineering 377. https:

//stanford.edu/class/stats311/lecture-notes.pdf  March 13  2019.

[GCB+18] Will Grathwohl  Ricky TQ Chen  Jesse Betterncourt  Ilya Sutskever  and David Duve-
naud. Ffjord: Free-form continuous dynamics for scalable reversible generative models.
arXiv preprint arXiv:1810.01367  2018.

[GKLW19] Rong Ge  Rohith Kuditipudi  Zhize Li  and Xiang Wang. Learning two-layer neural
networks with symmetric inputs. In International Conference on Learning Representa-
tions  2019.

[GKM18] Surbhi Goel  Adam Klivans  and Raghu Meka. Learning one convolutional layer with

overlapping patches. In International Conference on Machine Learning  2018.

[GMOV19] Weihao Gao  Ashok Makkuva  Sewoong Oh  and Pramod Viswanath. Learning one-
hidden-layer neural networks under general input distributions. In Proceddings of
the 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
pages 1950–1959  2019.

[GPAM+14] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley 
Sherjil Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In
Advances in neural information processing systems  pages 2672–2680  2014.

[HK07] Markus Harva and Ata Kabán. Variational learning for rectiﬁed factor analysis. Signal

Processing  87(3):509–527  2007.

[KW13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[LY17] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with
relu activation. In Advances in Neural Information Processing Systems  pages 597–607 
2017.

[MR19] Arya Mazumdar and Ankit Singh Rawat. Learning and recovery in the relu model.
In Proceedings of 57th Annual Allerton Conference on Communication  Control  and
Computing  2019  2019.

10

[NWH18] Thanh V Nguyen  Raymond KW Wong  and Chinmay Hegde. Autoencoders learn

generative linear models. arXiv preprint arXiv:1806.00572  2018.

[OKK16] Aaron van den Oord  Nal Kalchbrenner  and Koray Kavukcuoglu. Pixel recurrent

neural networks. arXiv preprint arXiv:1601.06759  2016.

[RMC15] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation
learning with deep convolutional generative adversarial networks. arXiv preprint
arXiv:1511.06434  2015.

[Sol17] Mahdi Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural

Information Processing Systems  pages 2007–2017  2017.

[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From

theory to algorithms. Cambridge University Press  2014.

[Tsy09] Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer  2009.

[Val84] Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth annual

ACM symposium on Theory of computing  pages 436–445. ACM  1984.

[Vav09] Stephen A Vavasis. On the complexity of nonnegative matrix factorization. SIAM

Journal on Optimization  20(3):1364–1377  2009.

[Wai19] Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint.

Cambridge University Press  2019.

[WS11] David P Williamson and David B Shmoys. The design of approximation algorithms.

Cambridge University Press  2011.

[ZSJ+17] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery
In International Conference on

guarantees for one-hidden-layer neural networks.
Machine Learning  pages 4140–4149  2017.

11

,Shanshan Wu
Alexandros Dimakis
Sujay Sanghavi