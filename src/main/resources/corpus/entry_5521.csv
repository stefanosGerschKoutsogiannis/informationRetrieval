2019,The Step Decay Schedule: A Near Optimal  Geometrically Decaying Learning Rate Procedure For Least Squares,Minimax optimal convergence rates for numerous classes of stochastic convex optimization problems are well characterized  where the majority of results utilize iterate averaged stochastic gradient descent (SGD) with polynomially decaying step sizes. In contrast  the behavior of SGD’s final iterate has received much less attention despite the widespread use in practice. Motivated by this observation  this work provides a detailed study of the following question: what rate is achievable using the final iterate of SGD for the streaming least squares regression problem with and without strong convexity? 


First  this work shows that even if the time horizon T (i.e. the number of iterations that SGD is run for) is known in advance  the behavior of SGD’s final iterate with any polynomially decaying learning rate scheme is highly sub-optimal compared to the statistical minimax rate (by a condition number factor in the strongly convex case and a factor of $\sqrt{T}$ in the non-strongly convex case). In contrast  this paper shows that Step Decay schedules  which cut the learning rate by a constant factor every constant number of epochs (i.e.  the learning rate decays geometrically) offer significant improvements over any polynomially decaying step size schedule. In particular  the behavior of the final iterate with step decay schedules is off from the statistical minimax rate by only log factors (in the condition number for the strongly convex case  and in T in the non-strongly convex case). Finally  in stark contrast to the known horizon case  this paper shows that the anytime (i.e. the limiting) behavior of SGD’s final iterate is poor (in that it queries iterates with highly sub-optimal function value infinitely often  i.e. in a limsup sense) irrespective of the step size scheme employed. These results demonstrate the subtlety in establishing optimal learning rate schedules (for the final iterate) for stochastic gradient procedures in fixed time horizon settings.,The Step Decay Schedule: A Near Optimal 

Geometrically Decaying Learning Rate Procedure

For Least Squares

Rong Ge 1  Sham M. Kakade 2  Rahul Kidambi3 and Praneeth Netrapalli4

1 Duke University  2 University of Washington  3 Cornell University  4 Microsoft Research  India.

rongge@cs.duke.edu  sham@cs.washington.edu  rkidambi@cornell.edu 

praneeth@microsoft.com

Abstract

Minimax optimal convergence rates for numerous classes of stochastic convex
optimization problems are well characterized  where the majority of results utilize
iterate averaged stochastic gradient descent (SGD) with polynomially decaying
step sizes. In contrast  the behavior of SGD’s ﬁnal iterate has received much less
attention despite the widespread use in practice. Motivated by this observation  this
work provides a detailed study of the following question: what rate is achievable
using the ﬁnal iterate of SGD for the streaming least squares regression problem
with and without strong convexity?
First  this work shows that even if the time horizon T (i.e. the number of iterations
that SGD is run for) is known in advance  the behavior of SGD’s ﬁnal iterate with
any polynomially decaying learning rate scheme is highly sub-optimal compared
to the statistical minimax rate (by a condition number factor in the strongly convex
case and a factor of
T in the non-strongly convex case). In contrast  this paper
shows that Step Decay schedules  which cut the learning rate by a constant factor
every constant number of epochs (i.e.  the learning rate decays geometrically)
offer signiﬁcant improvements over any polynomially decaying step size schedule.
In particular  the behavior of the ﬁnal iterate with step decay schedules is off
from the statistical minimax rate by only log factors (in the condition number
for the strongly convex case  and in T in the non-strongly convex case). Finally 
in stark contrast to the known horizon case  this paper shows that the anytime
(i.e. the limiting) behavior of SGD’s ﬁnal iterate is poor (in that it queries iterates
with highly sub-optimal function value inﬁnitely often  i.e. in a limsup sense)
irrespective of the stepsize scheme employed. These results demonstrate the
subtlety in establishing optimal learning rate schedules (for the ﬁnal iterate) for
stochastic gradient procedures in ﬁxed time horizon settings.

√

1

Introduction

Large scale machine learning relies almost exclusively on stochastic optimization methods [BB07] 
which include stochastic gradient descent (SGD) [RM51] and its variants [DHS11  JZ13]. In this
work  we restrict our attention to the SGD algorithm where we are concerned with the behavior of the
ﬁnal iterate (i.e. the last point when we terminate the algorithm). A majority of (minimax optimal)
theoretical results for SGD focus on polynomially decaying stepsizes [DGBSX12  RSS12  LJSB12 
Bub14] (or constant stepsizes [BM13  DB15a  JKK+16] for the case of least squares regression)
coupled with iterate averaging [Rup88  PJ92] to achieve minimax optimal rates of convergence.
However  practical SGD implementations typically return the ﬁnal iterate of a stochastic gradient
procedure. This line of work in theory (based on iterate averaging) and its discrepancy with regards to

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Algorithm 1: Step Decay scheme
Input: Initial vector w  starting learning rate

η0  number of iterations T

Output: w
for (cid:96) ← 1 to log T do

η(cid:96) ← η0/2(cid:96)
for t ← 1 to T /log T do

w ← w − η(cid:96)(cid:98)∇f (w)

end

end

Figure 1: (Left) The Step Decay scheme for stochastic gradient descent. Note that the algorithm
requires just two parameters - the starting learning rate η0 and number of iterations T .
(Right) Plot of function value error vs. condition number for the ﬁnal iterate of polynomially decaying
stepsizes i.e.  equation(5 6)  step-decay schedule (Algorithm 1) compared against the minimax optimal
sufﬁx averaged iterate with a constant stepsize [JKK+16] for a synthetic two-dimensional least
squares regression problem(1). The condition number κ is varied as {50  100  200  400}. Exhaustive
grid search is performed on starting stepsize and decay parameters. Initial excess risk is dσ2 and the
algorithm is run for T = κ2
max = 4002 steps (for all experiments); results are averaged over 5 random
seeds. Observe that the ﬁnal iterate’s error grows linearly as a function of the condition number κ for
the polynomially decaying stepsize schemes  whereas  the error does not grow as a function of κ for
the geometric “step-decay” stepsize scheme. See section E.1 in supplementary material for details.

practice leads to the question with regards to the behavior of SGD’s ﬁnal iterate. Indeed  this question
has motivated several efforts in stochastic convex optimization literature as elaborated below.
Non-Smooth Stochastic Optimization: The work of [Sha12] raised the question with regards to
the behavior of SGD’s ﬁnal iterate for non-smooth stochastic optimization (with/without strong
convexity). The work of [SZ12] answered this question  indicating that SGD’s ﬁnal iterate with
polynomially decaying stepsizes achieves near minimax rates (up to log factors) in an anytime (i.e. in
a limiting) sense (when number of iterations SGD is run for is not known in advance). Under speciﬁc
choices of step size sequences  [SZ12]’s result on SGD’s ﬁnal iterate is tight owing to the recent work
of [HLPR18]. More recently [JNN19] presented an approach indicating that a more nuanced stepsize
sequence serves to achieve minimax rates (up to constant factors) for the non-smooth stochastic
optimization setting when the end time T is known in advance.
Least Squares Regression (LSR): In contrast to the non-smooth setting  the state of our under-
standing of SGD’s ﬁnal iterate for smooth stochastic convex optimization  or  say  the streaming
least squares regression setting is far less mature − this gap motivates our paper’s contributions. In
particular  this paper studies SGD’s ﬁnal iterate behavior under various stepsize choices for least
squares regression (with and without strong convexity). The use of SGD’s ﬁnal iterate for the least
mean squares objective has featured in several efforts [WH60  Pro74  WS85  RS90]  but these results
do not achieve minimax rates of convergence  which leads to the following question:
“ Can polynomially decaying stepsizes (known to achieve minimax rates when coupled with iterate
averaging [Rup88  PJ92]) offer minimax optimal rates on SGD’s ﬁnal iterate when optimizing the
streaming least squares regression objective? If not  is there any other family of stepsizes that can
guarantee minimax rates on the ﬁnal iterate of stochastic gradient descent? ”
This paper presents progress on answering the above question − refer to contributions below for more
details. Note that the oracle model employed by this work (to quantify SGD’s ﬁnal iterate behavior)
has featured in a string of recent results that present a non-asymptotic understanding of SGD for least
squares regression  with the caveat being that these results crucially rely on iterate averaging with
constant stepsize sequences [BM13  DB15a  JKK+16  JKK+17b  JKK+17a  NR18].

Our contributions: This work establishes upper and lower bounds on the behavior of SGD’s ﬁnal
iterate  as run with standard polynomially decaying stepsizes as well as step decay schedules which

2

factor (in an anytime/limiting sense) [SZ12]. Here (cid:98)∇f ∇f = E(cid:104)(cid:98)∇f

Table 1: Comparison of sub-optimality for ﬁnal iterate of SGD (i.e.  E [f (wT )] − f (w∗)) for
stochastic convex optimization problems. This paper’s focus is on SGD’s ﬁnal iterate for streaming
least squares regression. The minimax rate refers to the best possible worst case rate with access to
stochastic gradients (typically achieved with iterate averaging methods [PJ92  DGBSX12  RSS12]);
the red shows the multiplicative factor increase (over the minimax rate) using the ﬁnal iterate  under
two different learning rate schedules - the polynomial decay and the step decay (refer to Algorithm 1).
Polynomial decay schedules are of the form ηt ∝ 1/tα (for appropriate α ∈ [0.5  1]). For the general
convex cases below  the ﬁnal iterate with a polynomial decay scheme is off minimax rates by a log T
 ∇2f denotes the stochastic
gradient  gradient and the Hessian of the function f. With regards to least squares  we assume
equation (3)  following recent efforts [BM13  DB15a  JKK+16]. While polynomially decaying
stepsizes are nearly minimax optimal for general (strongly) convex functions  this paper indicates
they are highly suboptimal on the ﬁnal iterate for least squares. The geometrically decaying Step
Decay schedule (Algorithm 1) provides marked improvements over any polynomial decay scheme
on the ﬁnal iterate for least squares. For simplicity of presentation  the results for least squares
regression do not show dependence on initial error. See Theorems 1 and 2 for precise statements (and
[NY83  SZ12  HLPR18] for precise statements of the general case).

(cid:105)

General

convex functions

Non-strongly convex
least squares regression

General strongly
convex functions

Strongly convex

least squares regression

Diam (ConstraintSet) ≤ D

Assumptions

(cid:20)(cid:13)(cid:13)(cid:13)(cid:98)∇f
(cid:20)(cid:13)(cid:13)(cid:13)(cid:98)∇f

(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:13)(cid:13)(cid:13)2(cid:21)

Eq. (3)

E

E

∇2f (cid:23) µI
Eq. (3)
∇2f (cid:23) µI

≤ G2

≤ G2

Rate w/ Final iterate
using best poly-decay

Rate w/ Final iterate
using Step Decay

Minimax rate

GD√
T

σ2d
T

G2
µT

σ2d
T

T

Ω

(cid:17)
(cid:17)
(cid:17)

Θ
[SZ12  HLPR18]

(cid:16) GD√
· log T
(cid:16) σ2d
T · √
(cid:16) G2
µT · log T
(cid:16) σ2d
(cid:17)
Θ
[SZ12  HLPR18]
T · κ

T
log T

Ω

O(cid:16) σ2d
O(cid:16) σ2d

–
T · log T

–
T · log T

(cid:17)

(cid:17)

(This work - Theorem 1)

(This work - Theorem 2)

(This work - Theorem 1)

(This work - Theorem 2)

tends to cut the stepsize by a constant factor after every constant number of epochs (see algorithm 1) 
by considering the streaming least squares regression problem (with and without strong convexity).
Our main result indicates that step decay schedules offer signiﬁcant improvements in achieving
near minimax rates over polynomially decaying stepsizes in the known horizon case (when the end
time T is known in advance). Figure 1 illustrates that this difference is evident (empirically) even
when optimizing a two-dimensional synthetic least squares objective. Table 1 provides a summary.
Finally  we present results that indicate the subtle (yet signiﬁcant) differences between the known
time horizon case and the anytime (i.e. the limiting) behavior of SGD’s ﬁnal iterate (see below). Note
that proofs of our main claims can be found in the supplementary material.
Our main contributions are as follows:

• Sub-optimality of polynomially decaying stepsizes: For the strongly convex least squares
case  this work shows that the ﬁnal iterate of a polynomially decaying stepsize scheme
(i.e. with ηt ∝ 1/tα  with α ∈ [0.5  1]) is off the minimax rate dσ2/T by a factor of the
√
condition number of the problem. For the non-strongly convex case of least squares  we
show that any polynomially decaying stepsize can achieve a rate no better than dσ2/
T
(up to log factors)  while the minimax rate is dσ2/T .

• Near-optimality of the step-decay scheme: Given a ﬁxed end time T   the step-decay scheme
(algorithm 1) presents a ﬁnal iterate that is off the statistical minimax rate by just a log(T )
factor when optimizing the strongly convex and non-strongly convex least squares regression
1  thus indicating vast improvements over polynomially decaying stepsize schedules. We
note here that our Theorem 2 for the non-strongly case offers a rate on the initial error
(i.e.  the bias term) that is off the best known rate [BM13] (that employs iterate averaging)
by a dimension factor. That said  Algorithm 1 is rather straightforward and employs the
knowledge of just an initial learning rate and number of iterations for its implementation.

1This dependence can be improved to log of the condition number of the problem (for the strongly convex

case) using a more reﬁned stepsize decay scheme.

3

• SGD has to query bad iterates inﬁnitely often: For the case of optimizing strongly convex
least squares regression  this work shows that any stochastic gradient procedure (in a lim sup
sense) must query sub-optimal iterates (off by nearly a condition number) inﬁnitely often.
• Complementary to our theoretical results for the stochastic linear regression  we evaluate the
empirical performance of different learning rate schemes when training a residual network
on the cifar-10 dataset and observe that the continuous variant of step decay schemes (i.e.
an exponential decay) indeed compares favorably to polynomially decaying stepsizes.

While the upper bounds established in this paper (section 3.2) merit extensions towards broader
smooth convex functions (with/without strong convexity)  the lower bounds established in sections 3.1 
3.3 present implications towards classes of smooth stochastic convex optimization. Even in terms of
upper bounds  note that there are fewer results on non-asymptotic behavior of SGD (beyond least
squares) when working in the oracle model considered in this work (see below). [BM11  BM13 
Bac14  NSW16] are exceptions  yet they do not achieve minimax rates on appropriate problem
classes; [FGKS15] does not work in standard stochastic ﬁrst order oracle model [NY83  ABRW12] 
so their work is not directly comparable to examine extensions towards broader function classes.
As a ﬁnal note  this paper’s result on the sub-optimality of standard polynomially decaying stepsizes
for classes of smooth and strongly convex optimization doesn’t contradict the (minimax) optimality
results in stochastic approximation [PJ92]. Iterate averaging coupled with polynomially decaying
learning rates (or constant learning rates for least squares [BM13  DB15a  JKK+16]) does achieve
minimax rates [Rup88  PJ92]. However  as mentioned previously  this work deals with SGD’s ﬁnal
iterate behavior (i.e. without iterate averaging)  since this bears more relevance towards practice.
Related work: [RM51] introduced the stochastic approximation problem and Stochastic Gradient
Descent (SGD). They present conditions on stepsize schemes satisﬁed by asymptotically convergent
algorithms: these schemes are referred to as “convergent” stepsize sequences. [Rup88  PJ92] proved
the asymptotic optimality of iterate averaged SGD with larger stepsize sequences. In terms of oracle
models and notions of optimality  there exists two lines of thought (see also [JKK+17b]):
Towards statistically optimal estimation procedures: The goal of this line of thought is to match
the excess risk of the statistically optimal estimator [Anb71  KC78  PJ92  LC98] on every problem
instance. Several efforts consider SGD in this oracle [BM11  Bac14  DB15b  FGKS15  NSW16]
presenting non-asymptotic results  often with iterate averaging. With regards to least squares 
[BM13  DB15a  FGKS15  JKK+16  JKK+17b  NR18] use constant step-size SGD with iterate
averaging to achieve minimax rates (on a per-problem basis) in this oracle model. SGD’s ﬁnal
iterate behavior for least squares has featured in several efforts in the signal processing/controls
literature [WH60  NN67  Pro74  WS85  RS90  SSB98]  without achieving minimax rates. This paper
works in this oracle model and analyzes SGD’s ﬁnal iterate behavior with various stepsize choices.
Towards optimality under bounded noise assumptions: The other line of thought presents algorithms
with access to stochastic gradients satisfying bounded noise assumptions  aiming to match lower
bounds provided in [NY83  RR11  ABRW12]. Asymptotic properties of “convergent” stepsize
schemes have been studied in great detail [KC78  BMP90  LPW92  BB99  KY03  Lai03  Bor08].
[DGBSX12  LJSB12  RSS12  GL12  GL13a  HK14  Bub14  DFB16] use iterate averaged SGD to
achieve minimax rates for various problem classes non-asymptotically. [AZ18] present an alternative
approach towards minimizing the gradient norm with access to stochastic gradients. As noted  [SZ12]
achieves anytime optimal rates (upto a log T factor) with the ﬁnal iterate of an SGD procedure  and
this is shown to be tight with the recent work of [HLPR18]. [JNN19] achieve minimax rates on the
ﬁnal iterate using a nuanced stepsize scheme when the number of iterations is ﬁxed in advance.
Geometrically Decaying Stepsize Schedules date to [Gof77]. [DD19] employ the stepdecay schedule
to prove high-probability guarantees for SGD with strongly convex objectives. In stochastic optimiza-
tion  several other works  including [GL13b  HK14  AFGO19  KM19] consider doubling argument
based approaches  where the epoch length is doubled everytime the stepsizes are halved. The step
decay schedule is employed to yield faster rates of convergence under certain growth (and related)
conditions both in convex [XLY16] and non-convex settings [YYJ18  DDC19].
Paper organization: Section 2 describes notation and problem setup. Section 3 presents our results
on the sub-optimality of polynomial decay schemes and the near optimality of the step decay scheme.
Section 3.3 presents results on the anytime behavior of SGD (i.e. the asymptotic/inﬁnite horizon
case). Section 4 presents experimental results and Section 5 presents conclusions.

4

2 Problem Setup

Notation: We present the setup and associated notation in this section. We represent scalars with
normal font a  b  L etc.  vectors with boldface lowercase characters a  b etc. and matrices with
boldface uppercase characters A  B etc. We represent positive semideﬁnite (PSD) ordering between
two matrices using (cid:23). The symbol (cid:38) represents that the inequality holds for some universal constant.
We consider here the minimization of the following expected square loss objective:

f (w) where f (w)

min

w

def
= 1
2

E(x y)∼D[(y − (cid:104)w  x(cid:105))2].

= ∇2f (w) = E(cid:2)xx(cid:62)(cid:3). We are provided access to

(1)

def

Note that the hessian of the objective H
stochastic gradients obtained by sampling a new example (xt  yt) ∼ D. These examples satisfy:

y = (cid:104)w∗  x(cid:105) +  

where   is the noise on the example pair (x  y) ∼ D and w∗ is a minimizer of the objective f (w).
Given an initial iterate w0 and stepsize sequence {ηt}  our stochastic gradient update is:

(cid:98)∇f (wt) = −(yt − (cid:104)wt  xt(cid:105)) · xt.

We assume that the noise  = y − (cid:104)w∗  x(cid:105) ∀ (x  y) ∼ D satisﬁes the following condition:

wt+1 ← wt − ηt(cid:98)∇f (wt−1);
= E(cid:104)(cid:98)∇f (w∗)(cid:98)∇f (w∗)(cid:62)(cid:105)

def

Σ

= E(x y)∼D[(y − (cid:104)w∗  x(cid:105))2xx(cid:62)] (cid:22) σ2H.

(2)

(3)

E(cid:104)(cid:107)x(cid:107)2 xx(cid:62)(cid:105) (cid:22) R2 H

Next  assume that covariates x satisfy the following fourth moment inequality:

(4)
This assumption is satisﬁed  say  when the norm of the covariates sup(cid:107)x(cid:107)2 < R2  but is true more
generally. Finally  note that both 3 and 4 are general and are used in recent works [BM13  JKK+16]
that present a sharp analysis of SGD for streaming least squares problem. Next  we denote by

def
= λmin (H)   L

def
= λmax (H)   and   κ

µ

def
= R2/µ

the smallest eigenvalue  largest eigenvalue and condition number of H respectively. µ > 0 in the
strongly convex case but not necessarily so in the non-strongly convex case (in section 3 and beyond 
the non-strongly case is referred to as the “smooth” case). Let w∗ ∈ arg minw∈Rd f (w). The
equation 2  any algorithm that uses these stochastic gradients and outputs (cid:98)wt has sub-optimality that
excess risk of an estimator w is f (w) − f (w∗). Given t accesses to the stochastic gradient oracle in

is lower bounded by σ2d
t

. More concretely  we have that [VdV00]
≥ 1 .

E [f ((cid:98)wt)] − f (w∗)

lim
t→∞

σ2d/t

The rate of (1 + o(1)) · σ2d/t is achieved using iterate averaged SGD [Rup88  PJ92] with constant
stepsizes [BM13  DB15a  JKK+16]. This rate of σ2d/t is called the statistical minimax rate.

3 Main Results

Sections 3.1  3.2 consider the ﬁxed time horizon setting; the former presents the signiﬁcant sub-
optimality of polynomially decaying stepsizes on SGD’s ﬁnal iterate behavior  the latter section
presenting the near-optimality of SGD’s ﬁnal iterate. Section 3.3 presents negative results on SGD’s
ﬁnal iterate behavior (irrespective of stepsizes employed)  in the anytime (i.e. limiting) sense.

3.1 Suboptimality of polynomial decay schemes

This section begins by showing that there exist problem instances where polynomially decaying
stepsizes considered stochastic approximation theory [RM51  PJ92] i.e.  those of the form a
b+tα   for
any choice of a  b > 0 and α ∈ [0.5  1] are signiﬁcantly suboptimal (by a factor of the condition
number of the problem  or by
T in the smooth case) compared to the statistical minimax rate [KC78].

√

5

Theorem 1. Under assumptions 3  4  there exists a class of problem instances where the following
lower bounds on excess risk hold on SGD’s ﬁnal iterate with polynomially decaying stepsizes when
given access to the oracle as written in equation 2.
Strongly convex case: Suppose µ > 0. For any condition number κ  there exists a least squares
problem instance with initial suboptimality f (w0) − f (w∗) ≤ σ2d such that  for any T ≥ κ 4
3   and
for all a  b ≥ 0 and 0.5 ≤ α ≤ 1  and for the learning rate scheme ηt = a
(f (w0) − f (w∗)) +

E [f (wT )] − f (w∗) ≥ exp

− T

(cid:19)

b+tα   we have
· κ
T

σ2d
64

κ log T

.

Smooth case: For any ﬁxed T > 1  there exists a least squares problem instance such that  for all
a  b ≥ 0 and 0.5 ≤ α ≤ 1  and for the learning rate scheme ηt = a
L · (cid:107)w0 − w∗(cid:107)2 + σ2d

b+tα   we have

(cid:17) ·

.

(cid:18)
E [f (wT )] − f (w∗) ≥(cid:16)

1√
T log T

For both cases (with/without strong convexity)  the minimax rate is σ2d/T . In the strongly convex
case  SGD’s ﬁnal iterate with polynomially decaying stepsizes pays a suboptimality factor of Ω(κ) 
whereas  in the smooth case  SGD’s ﬁnal iterate pays a suboptimality factor of Ω

.

(cid:17)

(cid:16) √

T
log T

3.2 Near optimality of Step Decay schemes

Given the knowledge of an end time T when the algorithm is terminated  this section presents the step
decay schedule (Algorithm 1)  which offers signiﬁcant improvements over standard polynomially
decaying stepsize schemes  and obtains near minimax rates (off by only a log(T ) factor).
Theorem 2. Suppose we are given access to the stochastic gradient oracle 2 satisfying Assumptions 3
and 4. Running Algorithm 1 with an initial stepsize of η1 = 1/(2R2) allows the algorithm to achieve
the following excess risk guarantees.

• Strongly convex case: Suppose µ > 0. We have:

(cid:18)

(cid:19)

E [f (wT )] − f (w∗) ≤ 2 · exp

−

T

2κ log T log κ

(f (w0) − f (w∗)) + 4σ2d · log T
T

.

• Smooth case: We have:

E [f (wT )] − f (w∗) ≤ 2 ·(cid:16)

R2d · (cid:107)w0 − w∗(cid:107)2 + 2σ2d

(cid:17) · log T

T

While theorem 2 presents signiﬁcant improvements over polynomial decay schemes  as mentioned in
the contributions  the above result presents a worse rate on the initial error (by a dimension factor) in
the smooth case (i.e. non-strongly convex case)  compared to the best known result [BM13]  which
relies heavily on iterate averaging to remove this factor. It is an open question with regards to whether
this factor can actually be improved or not. Furthermore  comparing the initial error dependence
between the lower bound for the smooth case (Theorem 1) with the upper bound for the step decay
scheme  we believe that the dependence on the smoothness L should be improved to one on the R2.
In terms of the variance  however  note that the polynomial decay schemes  are plagued by a
polynomial dependence on the condition number κ (for the strongly convex case)  and are off the
minimax rate by a
T factor (for the smooth case). The step decay schedule  on the other hand  is off
the minimax rate [Rup88  PJ92  VdV00] by only a log(T ) factor. It is worth noting that Algorithm 1
admits an efﬁcient implementation in that it requires the knowledge only of R2 (similar to iterate
averaging results [BM13  JKK+16]) and the end time T . Finally  note that this log T factor can be
improved to a log κ factor for the strongly convex case by using an additional polynomial decay
scheme before switching to the Step Decay scheme.
Proposition 3. Suppose we are given access to the stochastic gradient oracle 2 satisfying Assump-
tions 3 and 4. Let µ > 0 and let κ ≥ 2. For any problem and ﬁxed time horizon T / log T > 5κ  there
exists a learning rate scheme that achieves

√

E [f (wT )] − f (w∗) ≤ 2 exp(−T /(6κ log κ)) · (f (w0) − f (w∗)) + 100 log2 κ · σ2d
T

.

6

In order to have improved the dependence on the variance from log(T ) (in theorem 2) to log(κ)
(in proposition 3)  we require access to the strong convexity parameter µ = λmin(H) in addition
to R2 and knowledge of the end time T . This parallels results known for general strongly convex
setting [RSS12  LJSB12  SZ12  Bub14  JNN19].
As a ﬁnal remark  note that this section’s results (on step decay schemes) assumed the knowledge
of a ﬁxed time horizon T . In contrast  most results SGD’s averaged iterate obtain anytime (i.e. 
limiting/inﬁnite horizon) guarantees. Can we hope to achieve such guarantees with the ﬁnal iterate?

3.3 SGD queries bad points inﬁnitely often

This section shows that obtaining near statistical minimax rates with the ﬁnal iterate is not possible
without knowledge of the time horizon T . More concretely  we show that irrespective of the learning
rate sequence employed (be it polynomially decaying or step-decay)  SGD requires to query a point
with sub-optimality at least Ω(κ/ log κ) · σ2d/T for inﬁnitely many time steps T .
Theorem 4. Suppose we are given access to a stochastic gradient oracle 2 satisfying Assumption 3  4.
There exists a universal constant C > 0  and a problem instance  such that for SGD algorithm with
any ηt ≤ 1/2R2 for all t2  we have
lim sup
T→∞

E [f (wT )] − f (w∗)

log(κ + 1)

(σ2d/T )

.

≥ C

κ

once in O(cid:16) κ

log κ

(cid:17)

The bad points guaranteed to exist by Theorem 4 are not rare. We show that such points occur at least

iterations. Refer to Theorem 16 in appendix D in supplementary material.

4 Experimental Results

We present experimental validation on the suitability of the Step-decay schedule (or more precisely 
its continuous counterpart  which is the exponentially decaying schedule)  and compare its with the
polynomially decaying stepsize schedules. In particular  we consider the use of:

ηt =

η0

1 + b · t

(5)

ηt =

√

η0
1 + b

t

(6)

ηt = η0 · exp (−b · t).

(7)

Where  we perform a systematic grid search on the parameters η0 and b. In the section below  we
consider a real world non-convex optimization problem of training a residual network on the cifar-
10 dataset  with an aim to illustrate the practical implications of the results described in the paper.
Complete details of the setup are given in Appendix E in the supplementary material.

4.1 Non-Convex Optimization: Training a Residual Net on cifar-10

training a 44−layer deep residual network [HZRS16a] with pre-activation
We consider
blocks [HZRS16b] (dubbed preresnet-44) on cifar-10 dataset. The code for implementing the network
can be found here 3. For all experiments  we use Nesterov’s momentum [Nes83] implemented in
pytorch 4 with a momentum of 0.9  batchsize 128  100 training epochs  (cid:96)2 regularization of 0.0005.
Our experiments are based on grid searching for the best learning rate decay scheme on the parametric
family of learning rate schemes described above (5) (6) (7); all grid searches are performed on a
separate validation set (obtained by setting aside one-tenth of the training dataset) and with models
trained on the remaining 45000 samples. For presenting the ﬁnal numbers in the plots/tables  we
employ the best hyperparameters from the validation stage and train it on the entire 50  000 samples
and average results run with 10 different random seeds. The parameters for grid searches and
other details are presented in Appendix E. Furthermore  we always extend the grid so that the best
performing grid search parameter lies in the interior of our grid search.
How does the step decay scheme compare with the polynomially decaying stepsizes? Figure 2
and Table 2 present a comparison of the performance of the three schemes (5)-(7). These results
demonstrate that the exponential scheme convicingly outperforms the polynomial step-size schemes.

2Learning rate more than 2/R2 will make the algorithm diverge.
3https://github.com/D-X-Y/ResNeXt-DenseNet
4https://github.com/pytorch

7

Table 2: Comparing Train Cross-Entropy and Test 0/1 Error of various learning rate decay schemes
for the classiﬁcation task on cifar-10 using a 44−layer residual net with pre-activations.

Train Function Value

0.0713 ± 0.015
0.1119 ± 0.036
0.0053 ± 0.0015

Decay Scheme
√

O(1/t) (equation (5))
t) (equation (6))
O(1/
exp(−t) (equation (7))

Test 0/1 error
10.20 ± 0.7%
11.6 ± 0.67%
7.58 ± 0.21%

Figure 2: Plot of the training function value (left) and test 0/1− error (right) comparing the three
decay schemes (two polynomial) 5  6  (and one exponential) 7 scheme.

Does sufﬁx iterate averaging improve over ﬁnal iterate’s behavior for polynomially decaying
stepsizes? Towards answering this question  ﬁrstly  we consider the best performing values of equa-
tion 5 and 6  and then  average iterates of the algorithm starting from 5  10  20  40  80  85  90  95  99
epochs when training the model for a total of 100 epochs. While such iterate averaging
(and their sufﬁx) variants have strong theoretical support for (stochastic) convex optimiza-
tion [Rup88  PJ92  RSS12  Bub14  JKK+16]  their impact on non-convex optimization is largely
debatable. Nevertheless  this experiments’s results (ﬁgure 3) indicates that sufﬁx averaging tends to
hurt the algorithm’s generalization behavior (which is unsurprising given the non-convex nature of
the objective). Note that  ﬁgure 3 serves to indicate that averaging the ﬁnal few (≤ 5) epochs tends to
offer nearly the same result as the ﬁnal iterate’s behavior  indicating that the gains of using sufﬁx
iterate averaging are relatively limited for several such settings.

Figure 3: Performance of the sufﬁx averaged iterate compared to the ﬁnal iterate when varying the
iteration when iterate averaging is begun from {5  10  20  40  80  85  90  95  99} epochs for the 1/T
√
learning rate 5 (left) and the 1/

T learning rate 6 (right).

Does our result on “knowing” the time horizon (for step-decay schedule) present implications
towards hyper-parameter search methods that work based on results extracted from truncated
runs? Towards answering this question  consider the ﬁgure 4 and Tables 3 and 4  which present
a comparison of the performance of three exponential decay schemes each of which is tuned to
achieve the best performance at 33  66 and 100 epochs respectively. The key point to note is that best
performing hyperparameters at 33 and 66 epochs are not the best performing at 100 epochs (which
is made stark from the perspective of the validation error - refer to table 4). This demonstrates that
hyper parameter selection methods that tend to discard hyper-parameters which don’t perform well at

8

20406080100Start of Suffix Averaging102030405060708090Test Accuracyt0t - Final Iteratet0t - Averaged Iterate20406080100Start of Suffix Averaging102030405060708090Test Accuracyt0t + Finalt0t + Avgearlier stages of the optimization (i.e. based on comparing results on truncated runs)  which  for e.g. 
is indeed the case with hyperband [LJD+17]  will beneﬁt from a round of rethinking.

Figure 4: Plot of the training function value (left) and test 0/1− error (right) comparing exponential
decay scheme (equation 7)  with parameters optimized for 33  66 and 100 epochs.

Table 3: Comparing training (softmax) function value by optimizing the exponential decay scheme
with end times of 33/66/100 epochs on cifar-10 dataset using a 44−layer residual net.
exp(−t) [optimized for 33 epochs] (eqn (7))
exp(−t) [optimized for 66 epochs] (eqn (7))
exp(−t) [optimized for 100 epochs] (eqn (7))

Train FVal @66
0.0086 ± 0.002
0.0088 ± 0.0014
0.071 ± 0.017

Train FVal @100
0.0062 ± 0.0015
0.0061 ± 0.0011
0.0053 ± 0.0016

Decay Scheme

Train FVal @33
0.098 ± 0.006
0.107 ± 0.012

0.3 ± 0.06

Table 4: Comparing test 0/1 error by optimizing the exponential decay scheme with end times of
33/66/100 epochs for the classiﬁcation task on cifar-10 dataset using a 44−layer residual net.
exp(−t) [optimized for 33 epochs] (eqn (7))
exp(−t) [optimized for 66 epochs] (eqn (7))
exp(−t) [optimized for 100 epochs] (eqn (7))

Test 0/1 @66
Test 0/1 @100
Test 0/1 @33
10.36 ± 0.235% 8.6 ± 0.26%
8.57 ± 0.25%
10.51 ± 0.45% 8.51 ± 0.13% 8.46 ± 0.19%
9.8 ± 0.66%
14.42 ± 1.47%
7.58 ± 0.21%

Decay Scheme

5 Conclusions and Discussion

The main contribution of this work shows that the behavior of SGD’s ﬁnal iterate for least squares
regression is much more nuanced than what has been indicated by prior efforts that have primarily
considered non-smooth stochastic convex optimization. The results of this paper point out the striking
limitations of polynomially decaying stepsizes on SGD’s ﬁnal iterate  as well as sheds light on the
effectiveness of starkly different schemes based on a Step Decay schedule. Somewhat coincidentally 
practical implementations for certain classes of stochastic optimization do return the ﬁnal iterate of
SGD with step decay schedule − this connection does merit an understanding through future work.

Acknowledgments

Rong Ge acknowledges funding from NSF CCF-1704656  NSF CCF-1845171 (CAREER)  Sloan
Fellowship and Google Faculty Research Award. Sham Kakade acknowledges funding from the
Washington Research Foundation for Innovation in Data-intensive Discovery  NSF Award 1740551 
and ONR award N00014-18-1-2247. Rahul Kidambi acknowledges funding from NSF Award
1740822.

References
[ABRW12] Alekh Agarwal  Peter L. Bartlett  Pradeep Ravikumar  and Martin J. Wainwright.
Information-theoretic lower bounds on the oracle complexity of stochastic convex
optimization. IEEE Transactions on Information Theory  2012.

[AFGO19] Necdet Serhat Aybat  Alireza Fallah  Mert Gürbüzbalaban  and Asuman E. Ozdaglar.
A universally optimal multistage accelerated stochastic gradient method. CoRR 
abs/1901.08022  2019.

9

[Anb71] Dan Anbar. On Optimal Estimation Methods Using Stochastic Approximation Proce-

dures. University of California  1971.

[AZ18] Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR 

abs/1801.02982  2018.

[Bac14] Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong
convexity for logistic regression. Journal of Machine Learning Research (JMLR) 
volume 15  2014.

[BB99] B. Bharath and V. S. Borkar. Stochastic approximation algorithms: overview and recent

trends. S¯adhan¯a  1999.

[BB07] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20 

2007.

[BM11] Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approxima-

tion algorithms for machine learning. In NIPS 24  2011.

[BM13] Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approxi-

mation with convergence rate O(1/n). In NIPS 26  2013.

[BMP90] Albert Benveniste  Michel Metivier  and Pierre Priouret. Adaptive Algorithms and
Stochastic Approximations. Springer texts in Stochastic Modelling and Applied Proba-
bility  1990.

[Bor08] Vivek Borkar. Stochastic approximation. Cambridge Books  2008.
[Bub14] Sébastien Bubeck. Theory of convex optimization for machine learning. CoRR 

abs/1405.4980  2014.

[DB15a] Alexandre Défossez and Francis R. Bach. Averaged least-mean-squares: Bias-variance
trade-offs and optimal sampling distributions. In Artiﬁcal Intelligence and Statistics
(AISTATS)  2015.

[DB15b] Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation

with large step sizes. The Annals of Statistics  2015.

[DD19] Damek Davis and Dmitriy Drusvyatskiy. Robust stochastic optimization with the

proximal point method. CoRR  abs/1907.13307  2019.

[DDC19] Damek Davis  Dmitriy Drusvyatskiy  and Vasileios Charisopoulos. Stochastic al-
gorithms with geometric step decay converge linearly on sharp functions. CoRR 
abs/1907.09547  2019.

[DFB16] Aymeric Dieuleveut  Nicolas Flammarion  and Francis R. Bach. Harder  better  faster 
stronger convergence rates for least-squares regression. CoRR  abs/1602.05419  2016.
[DGBSX12] Ofer Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao. Optimal distributed
online prediction using mini-batches. Journal of Machine Learning Research (JMLR) 
volume 13  2012.

[DHS11] John C. Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for
online learning and stochastic optimization. Journal of Machine Learning Research 
12:2121–2159  2011.

[FGKS15] Roy Frostig  Rong Ge  Sham M. Kakade  and Aaron Sidford. Competing with the

empirical risk minimizer in a single pass. In COLT  2015.

[GL12] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for
strongly convex stochastic composite optimization i: A generic algorithmic framework.
SIAM Journal on Optimization  2012.

[GL13a] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms
for strongly convex stochastic composite optimization  ii: shrinking procedures and
optimal algorithms. SIAM Journal on Optimization  2013.

[GL13b] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms
for strongly convex stochastic composite optimization  ii: Shrinking procedures and
optimal algorithms. SIAM Journal on Optimization  23(4)  2013.

[Gof77] J. L. Gofﬁn. On the convergence rates of subgradient optimization methods. Mathe-

matical Programming  13:329–347  1977.

10

[HK14] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal al-
gorithms for stochastic strongly-convex optimization. Journal of Machine Learning
Research (JMLR)  volume 15  2014.

[HLPR18] Nicholas J. A. Harvey  Christopher Liaw  Yaniv Plan  and Sikander Randhawa. Tight

analyses for non-smooth stochastic gradient descent. CoRR  2018.

[HZRS16a] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for

image recognition. In CVPR  pages 770–778  2016.

[HZRS16b] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep
residual networks. In ECCV (4)  Lecture Notes in Computer Science  pages 630–645.
Springer  2016.

[JKK+16] Prateek Jain  Sham M Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford.
Parallelizing stochastic approximation through mini-batching and tail-averaging. arXiv
preprint arXiv:1610.03774  2016.

[JKK+17a] Prateek Jain  Sham M. Kakade  Rahul Kidambi  Praneeth Netrapalli  Venkata Krishna
Pillutla  and Aaron Sidford. A markov chain theory approach to characterizing the
minimax optimality of stochastic gradient descent (for least squares). CoRR  2017.

[JKK+17b] Prateek Jain  Sham M Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford.

Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227  2017.

[JNN19] Prateek Jain  Dheeraj Nagaraj  and Praneeth Netrapalli. Making the last iterate of sgd

information theoretically optimal. CoRR  2019.

[JZ13] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive

variance reduction. In NIPS 26  2013.

[KC78] Harold J. Kushner and Dean S. Clark. Stochastic Approximation Methods for Con-

strained and Unconstrained Systems. Springer-Verlag  1978.

[KM19] Andrei Kulunchakov and Julien Mairal. A generic acceleration framework for stochastic

composite optimization. CoRR  abs/1906.01164  2019.

[KY03] Harold J. Kushner and George Yin. Stochastic approximation and recursive algorithms

and applications. Springer-Verlag  2003.

[Lai03] Tze Leung Lai. Stochastic approximation: invited paper  2003.
[LC98] Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Texts in

Statistics. Springer  1998.

[LJD+17] Lisha Li  Kevin Jamieson  Giulia DeSalvo  Afshin Rostamizadeh  and Ameet Talwalkar.
Hyperband: A novel bandit-based approach to hyperparameter optimization. The
Journal of Machine Learning Research  18(1):6765–6816  2017.

[LJSB12] Simon Lacoste-Julien  Mark W. Schmidt  and Francis R. Bach. A simpler approach to
obtaining an o(1/t) convergence rate for the projected stochastic subgradient method.
CoRR  2012.

[LPW92] Lennart Ljung  Georg Pﬂug  and Harro Walk. Stochastic Approximation and Opti-
mization of Random Systems. Birkhauser Verlag  Basel  Switzerland  Switzerland 
1992.

[Nes83] Yurii E. Nesterov. A method for unconstrained convex minimization problem with the

rate of convergence O(1/k2). Doklady AN SSSR  269  1983.

[NN67] Jin-Ichi Nagumo and Atsuhiko Noda. A learning method for system identiﬁcation.

IEEE Transactions on Automatic Control  1967.

[NR18] Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic

gradient descent. CoRR  2018.

[NSW16] Deanna Needell  Nathan Srebro  and Rachel Ward. Stochastic gradient descent 
weighted sampling  and the randomized kaczmarz algorithm. Mathematical Pro-
gramming  2016.

[NY83] Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efﬁciency

in Optimization. John Wiley  1983.

11

[PJ92] Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by

averaging. SIAM Journal on Control and Optimization  volume 30  1992.

[Pro74] John G. Proakis. Channel identiﬁcation for high speed digital communications. IEEE

Transactions on Automatic Control  1974.

[RM51] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals

of Mathematical Statistics  vol. 22  1951.

[RR11] Maxim Raginsky and Alexander Rakhlin. Information-based complexity  feedback and
dynamics in convex programming. IEEE Transactions on Information Theory  2011.
IEEE

[RS90] Sumit Roy and John J. Shynk. Analysis of the momentum lms algorithm.

Transactions on Acoustics  Speech and Signal Processing  1990.

[RSS12] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent

optimal for strongly convex stochastic optimization. In ICML  2012.

[Rup88] David Ruppert. Efﬁcient estimations from a slowly convergent robbins-monro process.

Tech. Report  ORIE  Cornell University  1988.

[Sha12] Ohad Shamir. Open problem: Is averaging needed for strongly convex stochastic

gradient descent? In COLT  2012.

[SSB98] Rajesh Sharma  William A. Sethares  and James A. Bucklew. Analysis of momentum

adaptive ﬁltering algorithms. IEEE Transactions on Signal Processing  1998.

[SZ12] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:

Convergence results and optimal averaging schemes. CoRR  abs/1212.1824  2012.

[VdV00] Aad W Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press 

2000.

[WH60] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Defense Technical

Information Center  1960.

[WS85] Bernard Widrow and Samuel D. Stearns. Adaptive Signal Processing. Englewood

Cliffs  NJ: Prentice-Hall  1985.

[XLY16] Yi Xu  Qihang Lin  and Tianbao Yang. Accelerate stochastic subgradient method by

leveraging local error bound. CoRR  abs/1607.01027  2016.

[YYJ18] Tianbao Yang  Yan Yan 0006  Zhuoning Yuan  and Rong Jin. Why does stagewise
training accelerate convergence of testing error over sgd? CoRR  abs/1812.03934 
2018.

12

,Rong Ge
Sham Kakade
Rahul Kidambi
Praneeth Netrapalli