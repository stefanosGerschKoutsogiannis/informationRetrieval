2016,Robust Spectral Detection of Global Structures in the Data by Learning a Regularization,Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy  classic spectral methods usually fail to work  due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work  we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis  we demonstrate that the learned  regularizations suppress down the eigenvalues associated with localized  eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks  clustering from pairwise similarities  rank estimation and matrix completion problems. Using extensive experiments  we illustrate that our method solves the localization problem and works down to the  theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix  non-backtracking matrix  Laplacians and those with rank-one regularizations  which perform poorly in the sparse case with noise.,Robust Spectral Detection of Global Structures in the

Data by Learning a Regularization

Institute of Theoretical Physics  Chinese Academy of Sciences  Beijing 100190  China

Pan Zhang

panzhang@itp.ac.cn

Abstract

Spectral methods are popular in detecting global structures in the given data that
can be represented as a matrix. However when the data matrix is sparse or noisy 
classic spectral methods usually fail to work  due to localization of eigenvectors
(or singular vectors) induced by the sparsity or noise. In this work  we propose
a general method to solve the localization problem by learning a regularization
matrix from the localized eigenvectors. Using matrix perturbation analysis  we
demonstrate that the learned regularizations suppress down the eigenvalues asso-
ciated with localized eigenvectors and enable us to recover the informative eigen-
vectors representing the global structure. We show applications of our method
in several inference problems: community detection in networks  clustering from
pairwise similarities  rank estimation and matrix completion problems. Using ex-
tensive experiments  we illustrate that our method solves the localization problem
and works down to the theoretical detectability limits in different kinds of syn-
thetic data. This is in contrast with existing spectral algorithms based on data
matrix  non-backtracking matrix  Laplacians and those with rank-one regulariza-
tions  which perform poorly in the sparse case with noise.

1

Introduction

In many statistical inference problems  the task is to detect  from given data  a global structure such
as low-rank structure or clustering. The task is usually hard to solve since modern datasets usually
have a large dimensionality. When the dataset can be represented as a matrix  spectral methods are
popular as it gives a natural way to reduce the dimensionality of data using eigenvectors or singular
vectors.
In the point-of-view of inference  data can be seen as measurements to the underlying
structure. Thus more data gives more precise information about the underlying structure.
However in many situations when we do not have enough measurements  i.e.
the data matrix is
sparse  standard spectral methods usually have localization problems thus do not work well. One
example is the community detection in sparse networks  where the task is to partition nodes into
groups such that there are many edges connecting nodes within the same group and comparatively
few edges connecting nodes in different groups. It is well known that when the graph has a large
connectivity c  simply using the ﬁrst few eigenvectors of the adjacency matrix A ∈ {0  1}n×n
(with Aij = 1 denoting an edge between node i and node j and Aij = 0 otherwise) gives a good
result. In this case  like that of a sufﬁciently dense Erd˝os-R´enyi (ER) random graph with average
4c − λ2/2πc  and there
degree c  the spectral density follows Wigner’s semicircle rule  P (λ) =
is a gap between the edge of bulk of eigenvalues and the informative eigenvalue that represents the
underlying community structure. However when the network is large and sparse  the spectral density
of the adjacency matrix deviates from the semicircle  the informative eigenvalue is hidden in the
bulk of eigenvalues  as displayed in Fig. 1 left. Its eigenvectors associated with largest eigenvalues
(which are roughly proportional to log n/ log log n for ER random graphs) are localized on the large-

√

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

A ∈ Rm×n with rank r (cid:28) √

degree nodes  thus reveal only local structures about large degrees rather than the underlying global
structure. Other standard matrices for spectral clustering [19  22]  e.g. Laplacian  random walk
matrix  normalized Laplacian  all have localization problems but on different local structures such
as dangling trees.
Another example is the matrix completion problem which asks to infer missing entries of matrix
mn from only few observed entries. A popular method for this
problem is based on the singular value decomposition (SVD) of the data matrix. However it is
well known that when the matrix is sparse  SVD-based method performs very poorly  because the
singular vectors corresponding to the largest singular values are localized  i.e. highly concentrated
on high-weight column or row indices.
A simple way to ease the pain of localization induced by high degree or weight is trimming [6  13]
which sets to zero columns or rows with a large degree or weight. However trimming throws away
part of the information  thus does not work all the way down to the theoretical limit in the com-
munity detection problem [6  15]. It also performs worse than other methods in matrix completion
problem [25].
In recent years  many methods have been proposed for the sparsity-problem. One kind of methods
use new linear operators related to the belief propagation and Bethe free energy  such as the non-
backtracking matrix [15] and Bethe Hessian [24]. Another kind of methods add to the data matrix or
its variance a rank-one regularization matrix [2  11  16–18  23]. These methods are quite successful
in some inference problems in the sparse regime. However in our understanding none of them works
in a general way to solve the localization problem. For instance  the non-backtracking matrix and
the Bethe Hessian work very well when the graph has a locally-tree-like structure  but they have
again the localization problems when the system has short loops or sub-structures like triangles and
cliques. Moreover its performance is sensitive to the noise in the data [10]. Rank-one regularizations
have been used for a long time in practice  the most famous example is the “teleportation” term
in the Google matrix. However there is no satisfactory way to determine the optimal amount of
regularization in general. Moreover  analogous to the non-backtracking matrix and Bethe Hessian 
the rank-one regularization approach is also sensitive to the noise  as we will show in the paper.
The main contribution of this paper is to illustrate how to solve the localization problem of spec-
tral methods for general inference problems in sparse regime and with noise  by learning a proper
regularization that is speciﬁc for the given data matrix from its localized eigenvectors. In the fol-
lowing text we will ﬁrst discuss in Sec. 2 that all three methods for community detection in sparse
graphs can be put into the framework of regularization. Thus the drawbacks of existing methods
can be seen as improper choices of regularizations. In Sec. 3 we investigate how to choose a good
regularization that is dedicated for the given data  rather than taking a ﬁxed-form regularization as
in the existing approaches. We use matrix perturbation analysis to illustrate how the regulariza-
tion works in penalizing the localized eigenvectors  and making the informative eigenvectors that
correlate with the global structure ﬂoat to the top positions in spectrum. In Sec. 4 we use exten-
sive numerical experiments to validate our approach on several well-studied inference problems 
including the community detection in sparse graphs  clustering from sparse pairwise entries  rank
estimation and matrix completion from few entries.

Figure 1: Spectral density of the adjacency matrix (left) and X-Laplacian (right) of a graph generated
by the stochastic block model with n = 10000 nodes  average degree c = 3  q = 2 groups and
 = 0.125. Red arrows point to eigenvalues out of the bulk.

2

−30300.050.10.150.2−30300.050.10.150.22 Regularization as a uniﬁed framework

We see that the above three methods for the community detection problem in sparse graphs  i.e.
trimming  non-backtracking/Bethe Hessian  and rank-one regularizations  can be understood as do-
ing different ways of regularizations. In this framework  we consider a regularized matrix

L = ˆA + ˆR.

(1)
Here matrix ˆA is the data matrix or its (symmetric) variance  such as ˜A = D−1/2AD−1/2 with
D denoting the diagonal matrix of degrees  and matrix ˆR is a regularization matrix. The rank-one
regularization approaches [2  11  16–18  23] fall naturally into this framework as they set R to be a
rank-one matrix  −ζ11T   with ζ being a tunable parameter controlling strength of regularizations.
It is also easy to see that in the trimming  ˆA is set to be the adjacency matrix and ˆR contains entries
to remove columns or rows with high degrees from A.
For spectral algorithms using the non-backtracking matrix  its relation to form Eq. (1) is not straight-
forward. However we can link them using the theory of graph zeta function [8] which says that an
eigenvalue µ of the non-backtracking operator satisﬁes the following quadratic eigenvalue equation 

det[µ2I − µA + (D − I)] = 0 

where I is the identity matrix. It indicates that a particular vector v that is related to the eigenvector
of the non-backtracking matrix satisﬁes (A − D−I
µ )v = µv. Thus spectral clustering algorithm
using the non-backtracking matrix is equivalent to the spectral clustering algorithm using matrix
with form in Eq. (1)  while ˆA = A  ˆR = D−I
µ   and µ acting as a parameter. We note here that
the parameter does not necessarily be an eigenevalue of the non-backtracking matrix. Actually a
range of parameters work well in practice  like those estimated from the spin-glass transition of the
system [24]. So we have related different approaches of resolving localizations of spectral algorithm
in sparse graphs into the framework of regularization. Although this relation is in the context of
community detection in networks  we think it is a general point-of-view  when the data matrix has a
general form rather than a {0  1} matrix.
As we have argued in the introduction  above three ways of regularization work from case to case
and have different problems  especially when system has noise. It means that in the framework
of regularizations  the effective regularization matrix ˆR added by these methods do not work in a
general way and is not robust. In our understanding  the problem arises from the fact that in all
these methods  the form of regularization is ﬁxed for all kinds of data  regardless of different reasons
for the localization. Thus one way to solve the problem would be looking for the regularizations
that are speciﬁc for the given data  as a feature.
In the following section we will introduce our
method explicitly addressing how to learn such regularizations from localized eigenvectors of the
data matrix.

3 Learning regularizations from localized eigenvectors

The reason that the informative eigenvectors are hidden in the bulk is that some random eigenvectors
have large eigenvalues  due to the localization which represent the local structures of the system. In
the complementary side  if these eigenvectors are not localized  they are supposed to have smaller
eigenvalues than the informative ones which reveal the global structures of the graph. This is the
main assumption that our idea is based on.

In this work we use the Inverse Participation Ratio (IPR)  I(v) =(cid:80)n

i=1 v4

i   to quantify the amount
of localization of a (normalized) eigenvector v. IPR has been used frequently in physics  for exam-
ple for distinguishing the extended state from the localized state when applied on the wave func-
n} to 1 for vector
n   ...  1√
tion [3]. It is easy to check that I(v) ranges from 1
{0  ...  0  1  0  ...  0}. That is  a larger I(v) indicates more localization in vector v.
Our idea is to create a matrix LX with similar structures to A  but with non-localized leading eigen-
vectors. We call the resulting matrix X-Laplacian  and deﬁne it as LX = A + X  where matrix A is
the data matrix (or its variant)  and X is learned using the procedure detailed below:

n for vector { 1√

n   1√

3

Algorithm 1: Regularization Learning
Input: Real symmetric matrix A  number of eigenvectors q  learning rate η = O(1)  threshold ∆.
Output: X-Laplacian  LX  whose leading eigenvectors reveal the global structures in A.

1. Set X to be all-zero matrix.
2. Find set of eigenvectors U = {u1  u2  ...  uq} associated with the ﬁrst q largest

eigenvalues (in algebra) of LX.

3. Identify the eigenvector v that has the largest inverse participation ratio among the q
4. if I(v) < ∆  return LX = A + X; Otherwise  ∀i  Xii ← Xii − ηv2

eigenvectors in U. That is  ﬁnd v = argmaxu∈U I(u).

i   then go to step 2.

We can see that the regularization matrix X is a diagonal matrix  its diagonal entries are learned
gradually from the most localized vector among the ﬁrst several eigenvectors. The effect of X is to
penalize the localized eigenvectors  by suppressing down the eigenvalues associated with the local-
ized eigenvectors. The learning will continue until all q leading eigenvectors are delocalized  thus
are supposed to correlate with the global structure rather than the local structures. As an example 
we show the effect of X to the spectrum in Fig. 1. In the left panel  we plot the spectrum of the
adjacency matrix (i.e. before learning X) and the X-Laplacian (i.e. after learning X) of a sparse
network generated by the stochastic block model with q = 2 groups. For the adjacency matrix in
the left panel  localized eigenvectors have large eigenvalues and contribute a tail to the semicircle 
covering the informative eigenvalue  leaving only one eigenvalue  which corresponds to the eigen-
vector that essentially sorts vertices according to their degree  out of the bulk. The spectral density
of X-Laplacian is shown in the right panel of Fig. 1. We can see that the right corner of the continues
part of the spectral density appearing in the spectrum of the adjacency matrix   is missing here. This
is because due to the effect of X  the eigenvalues that are associated with localized eigenvectors in
the adjacency matrix are pushed into the bulk  maintaining a gap between the edge of bulk and the
informative eigenvalue (being pointed by the left red arrow in the ﬁgure).
The key procedure of the algorithm is the learning part in step 4  which updates diagonal terms of
matrix X using the most localized eigenvector v. Throughout the paper  by default we use learning
rate η = 10 and threshold ∆ = 5/n. As η = O(1) and v2
i = O(1/n)  we can treat the learned entries
in each step  ˆL  as a perturbation to matrix LX. After applying this perturbation  we anticipate that
an eigenvalue of L changes from λi to λi + ˆλi  and an eigenvector changes from ui to ui + ˆui. If
we assume that matrix LX is not ill-conditioned  and the ﬁrst few eigenvectors that we care about
are distinct  then we have ˆλi = uT
ˆLui. Derivation of the above expression is straightforward  but
i
for the completeness we put the derivations in the SI text. In our algorithm  ˆL is a diagonal matrix
with entries ˆLii = −ηv2
i with v denoting the identiﬁed eigenvector who has the largest inverse
ik. For the identiﬁed vector

participation ratio  so last equation can be written as ˆλi = −η(cid:80)

ku2

k v2

v  we further have

ˆλv = −η

i = −ηI(v).
v4

(2)

(cid:88)

i

inverse participation ratio of the new vector ui + ˆui can be written as

It means the eigenvalue of the identiﬁed eigenvector with inverse participation ratio I(v) is decreased
by amount ηI(v). That is  the more localized the eigenvector is  the larger penalty on its eigenvalue.
In addition to the penalty to the localized eigenvalues  We see that the leading eigenvectors are delo-
calizing during learning. We have analyzed the change of eigenvectors after the perturbation given
by the identiﬁed vector v  and obtained (see SI for the derivations) the change of an eigenvector ˆui
uj. Then the

as a function of all the other eigenvalues and eigenvectors  ˆui = (cid:80)
(cid:88)
(cid:88)
As eigenvectors ui and uj are orthogonal to each other  the term 4η(cid:80)n

can be
seen as a signal term and the last term can be seen as a cross-talk noise with zero mean. We see
that the cross-talk noise has a small variance  and empirically its effect can be neglected. For the

I(ui + ˆui) = I(ui) − 4η

kujkuikujl
λi − λj

u2
l u4
jlv2
il
λi − λj

.

(3)

n(cid:88)

(cid:88)

k ujkv2
λi−λj

kuik

l=1

j(cid:54)=i

k(cid:54)=l

n(cid:88)

j(cid:54)=i

jlv2
l u4
u2
λi−λj

il

(cid:80)

l=1

j(cid:54)=i

(cid:80)

j(cid:54)=i

u3
ilv2

l=1

− 4η

4

leading eigenvector corresponding to the largest eigenvalue λi = λ1  it is straightforward to see that
the signal term is strictly positive. Thus if the learning is slow enough  the perturbation will always
decrease the inverse participation ratio of the leading eigenvector. This is essentially an argument
for convergence of the algorithm. For other top eigenvectors  i.e. the second and third eigenvectors
and so on  though λi − λj is not strictly positive  there are much more positive terms than negative
terms in the sum  thus the signal should be positive with a high probability. Thus one can conclude
that the process of learning X makes ﬁrst few eigenvectors de-localizing.
An example illustrating the process of the learning is shown in Fig. 2 where we plot the second
eigenvector vs. the third eigenvector  at several times steps during the learning  for a network gen-
erated by the stochastic block model with q = 3 groups. We see that at t = 0  i.e. without learning 
both eigenvectors are localized  with a large range of distribution in entries. The color of eigen-
vectors encodes the group membership in the planted partition. We see that at t = 0 three colors
are mixed together indicating that two eigenvectors are not correlated with the planted partition. At
t = 4 three colors begin to separate  and range of entry distribution become smaller  indicating that
the localization is lighter. At t = 25  three colors are more separated  the partition obtained by ap-
plying k-means algorithm using these vectors successfully recovers 70% of the group memberships.
Moreover we can see that the range of entries of eigenvectors shrink to [−0.06  0.06]  giving a small
inverse participation ratio.

Figure 2: The second eigenvector V2 compared with the third eigenvector V3 of LX for a network at
three steps with t = 0  4 and 25 during learning. The network has n = 42000 nodes  q = 3 groups 
average degree c = 3   = 0.08  three colors represent group labels in the planted partition.

4 Numerical evaluations

In this section we validate our approach with experiments on several inference problems  i.e. com-
munity detection problems  clustering from sparse pairwise entries  rank estimation and matrix com-
pletion from a few entries. We will compare performance of the X-Laplacian (using mean-removed
data matrix) with recently proposed state-of-the-art spectral methods in the sparse regime.

4.1 Community Detection

First we use synthetic networks generated by the stochastic block model [9]  and its variant with
noise [10]. The standard Stochastic Block Model (SBM)  also called the planted partition model  is
a popular model to generate ensemble of networks with community structure. There are q groups
of nodes and a planted partition {t∗
i } ∈ {1  ...  q}. Edges are generated independently according
to a q × q matrix {pab}. Without loss of generality here we discuss the commonly studied case
where the q groups have equal size and where {pab} has only two distinct entries  pab = cin/n if
a = b and cout/n if a (cid:54)= b. Given the average degree of the graph  there is a so-called detectability
c − 1 + q) [7]   beyond which point it is not possible to
transition ∗ = cout/cin = (
obtain any information about the planted partition. It is also known spectral algorithms based on
the non-backtracking matrix succeed all the way down to the transition [15]. This transition was
recently established rigorously in the case of q = 2 [20  21]. Comparisons of spectral methods using
different matrices are shown in Fig. 3 left. From the ﬁgure we see that the X-Laplacian works as
well as the non-backtracking matrix  down to the detectability transition. While the direct use of the
adjacency matrix  i.e. LX before learning  does not work well when  exceeds about 0.1.
In the right panel of Fig. 3  each network is generated by the stochastic block model with the same
parameter as in the left panel  but with 10 extra cliques  each of which contains 10 randomly selected

√
c − 1)/(

√

5

nodes. Theses cliques do not carry information about the planted partition  hence act as noise to the
system. In addition to the non-backtracking matrix  X-Laplacian  and the adjacency matrix  we put
into comparison the results obtained using other classic and newly proposed matrices  including
Bethe Hessian [24]  Normalized Laplacian (N. Laplacian) Lsym = I − ˜A  and regularized and
normalized Laplacian (R.N. Laplacian) LA = ˜A − ζ11T  with a optimized regularization ζ (we
have scanned the whole range of ζ  and chosen an optimal one that gives the largest overlap  i.e.
fraction of correctly reconstructed labels  in most of cases). From the ﬁgure we see that with the
noise added  only X-Laplacian works down to the original transition (of SBM without cliques). All
other matrices fail in detecting the community structure with  > 0.15.
We have tested other kinds of noisy models  including the noisy stochastic block model  as proposed
in [10]. Our results show that the X-Laplacian works well (see SI text) while all other spectral
methods do not work at all on this dataset [10]. Moreover  in addition to the classic stochastic block
model  we have extensively evaluated our method on networks generated by the degree-corrected
stochastic block model [12]  and the stochastic block model with extensive triangles. We basically
obtained qualitatively results as in Fig. 3 that the X-Laplacian works as well as the state-of-the-art
spectral methods for the dataset. The ﬁgures and detailed results can be found at the SI text.
We have also tested real-world networks with an expert division  and found that although the expert
division is usually easy to detect by directly using the adjacency matrix  the X-Laplacian signiﬁ-
cantly improves the accuracy of detection. For example on the political blogs network [1]  spectral
clustering using the adjacency matrix gives 83 mis-classiﬁed labels among totally 1222 labels  while
the X-Laplacian gives only 50 mis-classiﬁed labels.

Figure 3: Accuracy of community detection  represented by overlap (fraction of correctly recon-
structed labels) between inferred partition and the planted partition  for several methods on networks
generated by the stochastic block model with average degree c = 3 (left) and with extra 10 size-10
cliques (right). All networks has n = 10000 nodes and q = 2 groups   = cout/cin. The black dashed
lines denote the theoretical detectability transition. Each data point is averaged over 20 realizations.

4.2 Clustering from sparse pairwise measurements
Consider the problem of grouping n items into clusters based on the similarity matrix S ∈ Rn×n 
where Sij is the pairwise similarity between items i and j. Here we consider not using all pairwise
similarities  but only O(n) random samples of them. In other words  the similarity graph which
encodes the information of the global clustering structure is sparse  rather than the complete graph.
There are many motivations for choosing such sparse observations  for example in some cases all
measurements are simply not available or even can not be stored.
In this section we use the generative model recently proposed in [26]  since there is a theoretical
limit that can be used to evaluate algorithms. Without loss of generality  we consider the problem
with only q = 2 clusters. The model in [26] ﬁrst assigns items hidden clusters {ti} ∈ {1  2}n  then
generates similarity between a randomly sampled pairs of items according to probability distribution 
pin and pout  associated with membership of two items. There is a theoretical limit ˆc satisfying
pin(s)+(q−1)pout(s)   that with c < ˆc no algorithm could obtain any partial information of
1
ˆc = 1
the planted clusters; while with c > ˆc some algorithms  e.g. spectral clustering using the Bethe
Hessian [26]  achieve partial recovery of the planted clusters.

(cid:82) ds (pin(s)−pout(s))2

q

6

00.10.20.30.50.60.70.80.91εOverlap Detectability transitionAdjacencyNon−backtrackingX−Laplacian00.10.20.30.50.60.70.80.91εOverlap Detectability transitionAdjacencyR. N. AdjacencyN. LaplacianNonbacktrackingBethe HessianX−LaplacianSimilar to the community detection in sparse graphs  spectral algorithms directly using the eigen-
vectors of a similarity matrix S does not work well  due to the localization of eigenvectors induced
by the sparsity. To evaluate whether our method  the X-Laplacian  solves the localization problem 
and how it works compared with the Bethe Hessian  in Fig. 4 we plot the performance (in overlap 
the fraction of correctly reconstructed group labels) of three algorithms on the same set of similarity
matrices. For all the datasets there are two groups with distributions pin and pout being Gaussian
with unit variance and mean 0.75 and −0.75 respectively. In the left panel of Fig. 4 the topology
of pairwise entries is random graph  Bethe Hessian works down to the theoretical limit  while di-
rectly using of the measurement matrix gives a poor performance. We can also see that X-Laplacian
has ﬁxed the localization problem of directly using of the measurement matrix  and works almost
as good as the Bethe-Hessian. We note that the Bethe Hessian needs to know the parameters (i.e.
parameters of distributions pin and pout)  while the X-Laplacian does not use them at all.
In the right panel of Fig. 4  on top of the ER random graph topology  we add some noisy local
structures by randomly selecting 20 nodes and connecting neighbors of each selected node to each
other. The weights for the local pairwise were set to 1  so that the noisy structures do not contain
information about the underlying clustering. We can see that Bethe Hessian is inﬂuenced by noisy
local structures and fails to work  while X-Laplacian solves the localization problems induced by
sparsity  and is robust to the noise. We have also tested other kinds of noise by adding cliques  or
hubs  and obtained similar results (see SI text).

Figure 4: Spectral clustering using sparse pairwise measurements. The X-axis denotes the average
number of pairwise measurements per data point  and the Y-axis is the fraction of correctly recon-
structed labels  maximized over permutations. The model used to generate pairwise measurements
is proposed in [26]  see text for detailed descriptions. In the left panel  the topologies of the pair-
wise measurements are random graphs. In the right panel in addition to the random graph topology
there are 20 randomly selected nodes with all their neighbors connected. Each point in the ﬁgure is
averaged over 20 realizations of size 104.

4.3 Rank estimation and Matrix Completion

nm is the ground-true rank. Only few  say c

The last problem we consider in this paper for evaluating the X-Laplacian is completion of a low rank
matrix from few entries. This problem has many applications including the famous collaborative
ﬁltering. A problem that is closely related to it is the rank estimation from revealed entries. Indeed
estimating rank of the matrix is usually the ﬁrst step before actually doing the matrix completion.
The problem is deﬁned as follows: let Atrue = U V T   where U ∈ Rn×r and V ∈ Rm×r are chosen
uniformly at random and r (cid:28) √
mn  entries of
matrix Atrue are revealed. That is we are given a matrix A ∈ Rn×m who contains only subset of
Atrue  with other elements being zero. Many algorithms have been proposed for matrix completion 
including nuclear norm minimization [5] and methods based on the singular value decomposition [4]
etc. Trimming which sets to zero all rows and columns with a large revealed entries  is usually
introduced to control the localizations of singular vectors and to estimate the rank using the gap of
singular values [14]. Analogous to the community detection problem  trimming is not supposed to
work optimally when matrix A is sparse. Indeed in [25] authors reported that their approach based
on the Bethe Hessian outperforms trimming+SVD when the topology of revealed entries is a sparse
random graph. Moreover  authors in [25] show that the number of negative eigenvalues of the Bethe
Hessian gives a more accurate estimate of the rank of A than that based on trimming+SVD.

√

7

1234560.50.60.70.80.9cOverlap Detectability transitionPairwise measurement matrixBethe HessianX−Laplacian1234560.50.60.70.80.9cOverlap Detectability transitionPairwise measurement matrixBethe HessianX−Laplacian(cid:19)

A 0

(cid:18) 0 A

However  we see that if the topology is not locally-tree-like but with some noise  for example with
some additional cliques  both trimming of the data matrix and Bethe Hessian perform much worse 
reporting a wrong rank  and giving a large reconstruction error  as illustrated in Fig. 5. In the left
panel of the ﬁgure we plot the eigenvalues of the Bethe Hessian  and singular values of trimmed
matrix A with true rank rtrue = 2. We can see that both of them are continuously distributed: there
is no clear gap in singular values of trimmed A  and Bethe Hessian has lots of negative eigenvalues.
In this case since matrix A could be a non-squared matrix  we need to deﬁne the X-Laplacian as
− X. The eigenvalues of LX are also plotted in Fig. 5 where one can see clearly
LX =
that there is a gap between the second largest eigenvalue and the third one. Thus the correct rank
can be estimated using the value minimizing consecutive eigenvalues  as suggested in [14].
After estimating the rank of the matrix  matrix completion is done by using a local optimization
algorithm [27] starting from initial matrices  that obtained using ﬁrst r singular vectors of trim-
ming+SVD  ﬁrst r eigenvectors of Bethe Hessian and X-Laplacian with estimated rank r respec-
tively. The results are shown in Fig. 5 right where we plot the probability that obtained root mean
square error (RMSE) is smaller than 10−7 as a function of average number of revealed entries per
row c  for the ER random-graph topology plus noise represented by several cliques. We can see that
X-Laplacian outperforms Bethe Hessian and Trimming+SVD with c ≥ 13. Moreover  when c ≥ 18 
for all instances  only X-Laplacian gives an accurate completion for all instances.

Figure 5: (Left:) Singular values of sparse data matrix with trimming  eigenvalues of the Bethe
Hessian and X-Laplacian. The data matrix is the outer product of two vectors of size 1000. Their
entries are Gaussian random variables with mean zero and unit variance  so the rank of the original
matrix is 2. The topology of revealed observations are random graphs with average degree c = 8
plus 10 random cliques of size 20. (Right:) Fraction of samples that RMSE is smaller than 10−7 
among 100 samples of rank-3 data matrix U V T of size 1000 × 1000  with the entries of U and V
drawn from a Gaussian distribution of mean 0 and unit variance. The topology of revealed entries is
the random graph with varying average degree c plus 10 size-20 cliques.

5 Conclusion and discussion

We have presented the X-Laplacian  a general approach for detecting latent global structure in a
given data matrix. It is completely a data-driven approach that learns different forms of regulariza-
tion for different data  to solve the problem of localization of eigenvectors or singular vectors. The
mechanics for de-localizing of eigenvectors during learning of regularizations has been illustrated
using the matrix perturbation analysis. We have validated our method using extensive numerical ex-
periments  and shown that it outperforms state-of-the-art algorithms on various inference problems
in the sparse regime and with noise.
In this paper we discuss the X-Laplacian using directly the (mean-removed) data matrix A  but
we note that the data matrix is not the only choice for the X-Laplacian. Actually we have tested
approaches using various variants of A  such as normalized data matrix ˜A  and found they work as
well. We also tried learning regularizations for the Bethe Hessian  and found it succeeds in repairing
Bethe Hessian when Bethe Hessian has localization problem. These indicate that our scheme of
regularization-learning is a general spectral approach for hard inference problems.
A (Matlab) demo of our method can be found at http://panzhang.net.

8

0102030405005101520Eigenvalues TrimmingBethe HessianX−Laplacian5101520253000.20.40.60.81cP(RMSE<10−7) Trimming SVDBethe HessianX−LaplacianReferences
[1] L. A. Adamic and N. Glance. The political blogosphere and the 2004 us election: divided they blog. In

Proceedings of the 3rd international workshop on Link discovery  pages 36–43. ACM  2005.

[2] A. A. Amini  A. Chen  P. J. Bickel  E. Levina  et al. Pseudo-likelihood methods for community detection

in large sparse networks. The Annals of Statistics  41(4):2097–2122  2013.

[3] R. Bell and P. Dean. Atomic vibrations in vitreous silica. Discussions of the Faraday society  50:55–61 

1970.

[4] J.-F. Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[5] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Computa-

tional mathematics  9(6):717–772  2009.

[6] A. COJA-OGHLAN. Graph partitioning via adaptive spectral techniques. Combinatorics  Probability

and Computing  19:227–284  3 2010.

[7] A. Decelle  F. Krzakala  C. Moore  and L. Zdeborov´a. Asymptotic analysis of the stochastic block model

for modular networks and its algorithmic applications. Phys. Rev. E  84:066106  Dec 2011.

[8] K.-i. Hashimoto. Zeta functions of ﬁnite graphs and representations of p-adic groups. Advanced Studies

in Pure Mathematics  15:211–280  1989.

[9] P. W. Holland  K. B. Laskey  and S. Leinhardt. Stochastic blockmodels: First steps. Social networks 

5(2):109–137  1983.

[10] A. Javanmard  A. Montanari  and F. Ricci-Tersenghi. Phase transitions in semideﬁnite relaxations. Pro-

ceedings of the National Academy of Sciences  113(16):E2218  2016.

[11] A. Joseph and B. Yu. Impact of regularization on spectral clustering. arXiv preprint arXiv:1312.1733 

2013.

[12] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Phys.

Rev. E  83:016107  Jan 2011.

[13] R. H. Keshavan  A. Montanari  and S. Oh. Low-rank matrix completion with noisy observations: a
quantitative comparison. In Communication  Control  and Computing  2009. Allerton 2009. 47th Annual
Allerton Conference on  pages 1216–1222. IEEE  2009.

[14] R. H. Keshavan  S. Oh  and A. Montanari. Matrix completion from a few entries. In Information Theory 

2009. ISIT 2009. IEEE International Symposium on  pages 324–328. IEEE  2009.

[15] F. Krzakala  C. Moore  E. Mossel  J. Neeman  A. Sly  L. Zdeborov´a  and P. Zhang. Spectral redemption

in clustering sparse networks. Proc. Natl. Acad. Sci. USA  110(52):20935–20940  2013.

[16] C. M. Le  E. Levina  and R. Vershynin. Sparse random graphs: regularization and concentration of the

laplacian. arXiv preprint arXiv:1502.03049  2015.

[17] C. M. Le and R. Vershynin. Concentration and regularization of random graphs.

arXiv:1506.00669  2015.

arXiv preprint

[18] J. Lei  A. Rinaldo  et al. Consistency of spectral clustering in stochastic block models. The Annals of

Statistics  43(1):215–237  2014.

[19] U. V. Luxburg  M. Belkin  O. Bousquet  and Pertinence. A tutorial on spectral clustering. Stat. Comput 

2007.

[20] L. Massouli´e. Community detection thresholds and the weak ramanujan property. In Proceedings of the

46th Annual ACM Symposium on Theory of Computing  pages 694–703. ACM  2014.

[21] E. Mossel  J. Neeman  and A. Sly.

arXiv:1202.1499  2012.

Stochastic block models and reconstruction.

arXiv preprint

[22] A. Y. Ng  M. I. Jordan  Y. Weiss  et al. On spectral clustering: Analysis and an algorithm. Advances in

neural information processing systems  2:849–856  2002.

[23] T. Qin and K. Rohe. Regularized spectral clustering under the degree-corrected stochastic blockmodel.

In Advances in Neural Information Processing Systems  pages 3120–3128  2013.

[24] A. Saade  F. Krzakala  and L. Zdeborov´a. Spectral clustering of graphs with the bethe hessian.

Advances in Neural Information Processing Systems  pages 406–414  2014.

In

[25] A. Saade  F. Krzakala  and L. Zdeborov´a. Matrix completion from fewer entries: Spectral detectability
and rank estimation. In C. Cortes  N. Lawrence  D. Lee  M. Sugiyama  and R. Garnett  editors  Advances
in Neural Information Processing Systems 28  pages 1261–1269. Curran Associates  Inc.  2015.

[26] A. Saade  M. Lelarge  F. Krzakala  and L. Zdeborov´a. Clustering from sparse pairwise measurements. To
appear in IEEE International Symposium on Information Theory (ISIT). IEEE  arXiv:1601.06683  2016.

[27] S.G.Johnson. The nlopt nonlinear-optimization package  2014.

9

,Pan Zhang
Liqian Ma
Xu Jia
Qianru Sun
Bernt Schiele
Tinne Tuytelaars
Luc Van Gool