2019,Logarithmic Regret for Online Control,We study optimal regret bounds for control in linear dynamical systems under adversarially changing strongly convex cost functions  given the knowledge of transition dynamics. This includes several well studied and influential frameworks such as the Kalman filter and the linear quadratic regulator. State of the art methods achieve regret which scales as T^0.5  where T is the time horizon. 

We show that the optimal regret in this fundamental setting can be significantly smaller  scaling as polylog(T). This regret bound is achieved by two different efficient iterative methods  online gradient descent and online natural gradient.,Logarithmic Regret for Online Control

Naman Agarwal1

Elad Hazan1 2

Karan Singh1 2

1 Google AI Princeton

namanagarwal@google.com  {ehazan karans}@princeton.edu

2 Computer Science  Princeton University

Abstract

We study optimal regret bounds for control in linear dynamical systems under
adversarially changing strongly convex cost functions  given the knowledge of tran-
sition dynamics. This includes several well studied and fundamental frameworks
√
such as the Kalman ﬁlter and the linear quadratic regulator. State of the art methods
achieve regret which scales as O(
We show that the optimal regret in this setting can be signiﬁcantly smaller  scaling
as O(poly(log T )). This regret bound is achieved by two different efﬁcient iterative
methods  online gradient descent and online natural gradient.

T )  where T is the time horizon.

1

Introduction

Algorithms for regret minimization typically attain one of two performance guarantees. For general
convex losses  regret scales as square root of the number of iterations  and this is tight. However  if
the loss function exhibit more curvature  such as quadratic loss functions  there exist algorithms that
attain poly-logarithmic regret. This distinction is also known as “fast rates” in statistical estimation.
Despite their ubiquitous use in online learning and statistical estimation  logarithmic regret algorithms
are almost non-existent in control of dynamical systems. This can be attributed to fundamental
challenges in computing the optimal controller in the presence of noise.
Time-varying cost functions in dynamical systems can be used to model unpredictable dynamic
resource constraints  and the tracking of a desired sequence of exogenous states. At a pinch  if we
have changing (even  strongly) convex loss functions  the optimal controller for a linear dynamical
system is not immediately computable via a convex program. For the special case of quadratic loss 
some previous works [9] remedy the situation by taking a semi-deﬁnite relaxation  and thereby obtain
a controller which has provable guarantees on regret and computational requirements. However  this
semi-deﬁnite relaxation reduces the problem to regret minimization over linear costs  and removes
the curvature which is necessary to obtain logarithmic regret.
In this paper we give the ﬁrst efﬁcient poly-logarithmic regret algorithms for controlling a linear
dynamical system with noise in the dynamics (i.e. the standard model). Our results apply to general
convex loss functions that are strongly convex  and not only to quadratics.

Regret

loss functions

Reference

[1]
[4]
[9]
here

Noise
none

O(log2 T )
adversarial
T )
O(
stochastic
T )
O(
stochastic O(log7 T )

√
√

quadratic (ﬁxed hessian)

convex
quadratic

strongly convex

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Our Results

The setting we consider is a linear dynamical system  a continuous state Markov decision process
with linear transitions  described by the following equation:

xt+1 = Axt + But + wt.

(1.1)

T ) regret bounds are tight (see [13]).

previous noise terms  and take the form ut =(cid:80)H

Here xt is the state of the system  ut is the action (or control) taken by the controller  and wt is the
noise. In each round t  the learner outputs an action ut upon observing the state xt and incurs a cost
of ct(xt  ut)  where ct is convex. The objective here is to choose a sequence of adaptive controls ut
so that a minimum total cost may be incurred.
The approach taken by [9] and other previous works is to use a semi-deﬁnite relaxation for the
controller. However  this removes the properties associated with the curvature of the loss functions 
√
by reducing the problem to an instance of online linear optimization. It is known that without
curvature  O(
Therefore we take a different approach  initiated by [4]. We consider controllers that depend on the
i=1 Miwt−i. While this resulting convex relaxation
does not remove the curvature of the loss functions altogether  it results in an overparametrized
representation of the controller  and it is not a priori clear that the loss functions are strongly convex
with respect to the parameterization. We demonstrate the appropriate conditions on the linear
dynamical system under which the strong convexity is retained.
Henceforth we present two methods that attain poly-logarithmic regret. They differ in terms of the
regret bounds they afford and the computational cost of their execution. The online gradient descent
update (OGD) requires only gradient computation and update  whereas the online natural gradient
(ONG) update  in addition  requires the computation of the preconditioner  which is the expected
Gram matrix of the Jacobian  denoted J  and its inverse. However  the natural gradient update admits
an instance-dependent upper bound on the regret  which while being at least as good as the regret
bound on OGD  offers better guarantees on benign instances (See Corollary 4.5  for example).

Algorithm

OGD
ONG

Update rule (simpliﬁed)
Mt+1 ← Mt − ηt∇ft(Mt)

Mt+1 ← Mt − ηt(E[J(cid:62)J])−1∇ft(Mt)

1.2 Related Work

Applicability

∃K  diag L s.t. A − BK = QLQ−1

(cid:107)L(cid:107) ≤ 1 − δ  (cid:107)Q(cid:107) (cid:107)Q(cid:107)−1 ≤ κ

For a survey of linear dynamical systems (LDS)  as well as learning  prediction and control problems 
see [17]. Recently  there has been a renewed interest in learning dynamical systems in the machine
learning literature. For fully-observable systems  sample complexity and regret bounds for control
(under Gaussian noise) were obtained in [3  10  2]. The technique of spectral ﬁltering for learning and
open-loop control of partially observable systems was introduced and studied in [15  7  14]. Provable
control in the Gaussian noise setting via the policy gradient method was also studied in [11].
The closest work to ours is that of [1] and [9]  aimed at controlling LDS with adversarial loss
functions. The authors in [3] obtain a O(log2 T ) regret algorithm for changing quadratic costs (with
a ﬁxed hessian)  but for dynamical systems that are noise-free. In contrast  our results apply to the
full (noisy) LDS setting  which presents the main challenges as discussed before. Cohen et al. [9]
consider changing quadratic costs with stochastic noise to achieve a O(
We make extensive use of techniques from online learning [8  16  13]. Of particular interest to our
study is the setting of online learning with memory [5]. We also build upon the recent control work
of [4]  who use online learning techniques and convex relaxation to obtain provable bounds for LDS
with adversarial perturbations.

T ) regret bound.

√

2 Problem Setting

We consider a linear dynamical system as deﬁned in (1.1) with costs ct(xt  ut)  where ct is strongly
convex. In this paper we assume that the noise wt is a random variable generated independently at

2

every time step. For any algorithm A  we attribute a cost deﬁned as

JT (A) = E{wt}

ct(xt  ut)

 

(cid:34) T(cid:88)

(cid:35)

where xt+1 = Axt + But + wt  ut = A(x1  . . . xt) and E{wt} represents the expectation over the
entire noise sequence. For the rest of the paper we will drop the subscript {wt} from the expectation
as it will be the only source of randomness. Overloading notation  we shall use JT (K) to denote the
cost of a linear controller K which chooses the action as ut = −Kxt.

t=1

In the paper we assume that x1 = 0 1  as well as the following conditions.

Assumptions.
Assumption 2.1. We assume that (cid:107)B(cid:107) ≤ κB. Furthermore  the perturbation introduced per time
step is bounded  i.i.d  and zero-mean with a lower bounded covariance i.e.

∀t wt ∼ Dw  E[wt] = 0  E[wtw(cid:62)

t ] (cid:23) σ2I and (cid:107)wt(cid:107) ≤ W

This may be adapted to the case of sub-gaussian noise by conditioning on the event that none of the
noise vectors are ever large. Such adaptation introduces a multiplicative log(T ) factor in the regret.
Assumption 2.2. The costs ct(x  u) are α-strongly convex. Wehnever (cid:107)x(cid:107) (cid:107)u(cid:107) ≤ D  it holds that

(cid:107)∇xct(x  u)(cid:107) (cid:107)∇uct(x  u)(cid:107) ≤ GD.

The class of linear controllers we work with are deﬁned as follows; see Section A for a detailed note.
Deﬁnition 2.3 (Diagonal Strong Stability). Given a dynamics (A  B)  a linear controller K is (κ  γ)-
diagonal strongly stable for real numbers κ ≥ 1  γ < 1  if there exists a complex diagonal matrix L
and a non-singular complex matrix Q  such that A − BK = QLQ−1 with the following being true:

1. The spectral norm of L is strictly smaller than one  i.e.  (cid:107)L(cid:107) ≤ 1 − γ.
2. The controller and transforming matrices are bounded  i.e.  (cid:107)K(cid:107) ≤ κ and (cid:107)Q(cid:107) (cid:107)Q−1(cid:107) ≤ κ.
Regret Formulation. Let K = {K : K is (κ  γ)-diagonal strongly stable}. For an algorithm A 
the notion of regret we consider is pseudo-regret  i.e. the sub-optimality of its cost with respect to the
cost for the best linear controller i.e. 

Regret = JT (A) − min

K∈K JT (K).

3 Preliminaries

Notation. We reserve the letters x  y for states and u  v for actions. We denote by dx  du to be the
dimensionality of the state and the control space respectively. Let d = max(dx  du). We reserve
capital letters A  B  K  M for matrices associated with the system and the policy. Other capital letters
are reserved for universal constants in the paper. We use the shorthand Mi:j to denote a subsequence
{Mi  . . .   Mj}. For any matrix U  deﬁne Uvec to be a ﬂattening of the matrix where we stack the
columns upon each other. Further for a collection of matrices M = {M [i]}  let Mvec be the ﬂattening
deﬁned by stacking the ﬂattenings of M [i] upon each other. We use (cid:107)x(cid:107)2
U = x(cid:62)U x to denote the
matrix induced norm. The rest of this section provides a recap of the relevant deﬁnitions and concepts
introduced in [4].

3.1 Reference Policy Class
For the rest of the paper  we ﬁx a (κ  γ)-diagonally strongly stable matrix K (The bold notation is to
stress that we treat this matrix as ﬁxed and not a parameter). Note that this can be any such matrix and
it can be computed via a semi-deﬁnite feasibility program [9] given the knowledge of the dynamics 
before the start of the game. We work with following the class of policies.

1This is only for convenience of presentation. The case with a bounded x1 can be handled similarly.

3

Deﬁnition 3.1 (Disturbance-Action Policy). A disturbance-action policy M = (M [0]  . . .   M [H−1]) 
for horizon H ≥ 1 is deﬁned as the policy which at every time t  chooses the recommended action ut
at a state xt  deﬁned 2 as

H(cid:88)

ut(M ) (cid:44) −Kxt +

M [i−1]wt−i.

For notational convenience  here it may be considered that wi = 0 for all i < 0.

i=1

The policy applies a linear transformation to the disturbances observed in the past H steps. Since
(x  u) is a linear function of the disturbances in the past under a linear controller K  formulating the
policy this way can be seen as a relaxation of the class of linear policies. Note that K is a ﬁxed matrix
and is not part of the parameterization of the policy. As was established in [4] (and we include the
proof for completeness)  with the appropriate choice of parameters  superimposing such a K  to the
policy class allows it to approximate any linear policy in terms of the total cost suffered with a ﬁnite
horizon parameter H.
We refer to the policy played at time t as Mt = {M [i]
t } where the subscript t refers to the time index
and the superscript [i − 1] refers to the action of Mt on wt−i. Note that such a policy can be executed
because wt−1 is perfectly determined on the speciﬁcation of xt as wt−1 = xt − Axt−1 − But−1.

3.2 Evolution of State

This section describes the evolution of the state of the linear dynamical system under a non-stationary
policy composed of a sequence of T policies  where at each time the policy is speciﬁed by Mt =
(M [0]
). We will use M0:T−1 to denote such a non-stationary policy. The following
deﬁnitions ease the burden of notation.

  . . .   M [H−1]

t

t

1. Deﬁne ˜A = A − BK. ˜A shall be helpful in describing the evolution of state starting from a

non-zero state in the absence of disturbances.

2. For any sequence of matrices M0:H  deﬁne Ψi as a linear function that describes the effect

of wt−i on the state xt  formally deﬁned below.

Deﬁnition 3.2. For any sequence of matrices M0:H  deﬁne the disturbance-state transfer matrix Ψi
for i ∈ {0  1  . . . H}  to be a function with h + 1 inputs deﬁned as

H(cid:88)

Ψi(M0:H ) (cid:44) ˜Ai1i≤H +

˜AjBM [i−j−1]

H−j

1i−j∈[1 H].

j=0

It will be important to note that ψi is a linear function of its argument.

3.3 Surrogate State and Surrogate Cost

This section introduces a couple of deﬁnitions required to describe our main algorithm. In essence
they describe a notion of state  its derivative and the expected cost if the system evolved solely under
the past H steps of a non-stationary policy.
Deﬁnition 3.3 (Surrogate State & Surrogate Action). Given a sequence of matrices M0:H+1 and 2H
independent invocations of the random variable w given by {wj ∼ Dw}2H−1
j=0   deﬁne the following
random variables denoting the surrogate state and the surrogate action:

2H(cid:88)

y(M0:H ) =

Ψi(M0:H )w2H−i−i 

i=0

v(M0:H+1) = −Ky(M0:H ) +

M [i−1]

H+1 w2H−i.

H(cid:88)

When M is the same across all arguments we compress the notation to y(M ) and v(M ) respectively.
2xt is completely determined given w0 . . . wt−1. Hence  the use of xt only serves to ease the burden of

presentation.

i=1

4

Algorithm 1 Online Control Algorithm
1: Input: Step size schedule ηt  Parameters κB  κ  γ  T .
2: Deﬁne H = γ−1 log(T κ2)
3: Deﬁne M = {M = {M [0] . . . M [H−1]} : (cid:107)M [i−1](cid:107) ≤ κ3κB(1 − γ)i}.
4: Initialize M0 ∈ M arbitrarily.
5: for t = 0  . . .   T − 1 do
6:

Choose the action:

H(cid:88)

ut = −Kxt +

M [i−1]

t

wt−i.

Observe the new state xt+1 and record wt = xt+1 − Axt − But.

i=1

7:
8: Online Gradient Update:

Mt+1 = ΠM(Mt − ηt∇ft(Mt))

9: Online Natural Gradient Update:

Mvec t+1 = ΠM(Mvec t − ηt(E[J T J])−1∇Mvec tft(Mt))

10: end for

Deﬁnition 3.4 (Surrogate Cost). Deﬁne the surrogate cost function ft to be the cost associated with
the surrogate state-action pair deﬁned above  i.e. ft(M0:H+1) = E [ct(y(M0:H )  v(M0:H+1))] .
When M is the same across all arguments we compress the notation to ft(M ).

Deﬁnition 3.5 (Jacobian). Let z(M ) =

. Since y(M )  v(M ) are random linear functions

(cid:20)y(M )

(cid:21)

v(M )

of M  z(M ) can be reparameterized as z(M ) = JMvec =
which derives its randomness from the random perturbations wi.

Jv

Mvec  where J is a random matrix 

(cid:20)Jy

(cid:21)

3.4 OCO with Memory

We now describe the setting of online convex optimization with memory introduced in [5]. In
this setting  at every step t  an online player chooses some point xt ∈ K ⊂ Rd  a loss function
ft : KH+1 (cid:55)→ R is then revealed  and the learner suffers a loss of ft(xt−H:t). We assume a certain
coordinate-wise Lipschitz regularity on ft of the form such that  for any j ∈ {0  . . .   H}  for any
x0:H   ˜xj ∈ K 

|ft(x0:j−1  xj  xj+1:H ) − ft(x0:j−1  ˜xj  xj+1:H )| ≤ L(cid:107)xj − ˜xj(cid:107).

In addition  we deﬁne ft(x) = ft(x  . . .   x)  and we let

Gf =

sup

t∈{0 ... T} x∈K

(cid:107)∇ft(x)(cid:107)  D = sup
x y∈K

(cid:107)x − y(cid:107).

The resulting goal is to minimize the policy regret [6]  which is deﬁned as

PolicyRegret =

ft(xt−H:t) − min
x∈K

ft(x).

T(cid:88)

t=H

T(cid:88)

t=H

(3.1)

(3.2)

4 Algorithms & Statement of Results

The two variants of our method are spelled out in Algorithm 1. Theorems 4.1 and 4.3 provide the
main guarantees for the two algorithms.

Online Gradient Update
Theorem 4.1 (Online Gradient Update). Suppose Algorithm 1 (Online Gradient Update) is executed

with K being any (κ  γ)-diagonal strongly stable matrix and ηt = Θ(cid:0)ασ2t(cid:1)−1  on an LDS satisfying

Assumption 2.1 with control costs satisfying Assumption 2.2. Then  it holds true that

(cid:18) G2W 4

ασ2

(cid:19)

log7(T )

.

JT (A) − min

K∈K JT (K) ≤ ˜O

5

The above result leverages the following lemma which shows that the function ft(·) is strongly
convex with respect to its argument M. Note that strong convexity of the cost functions ct over
the state-action space does not by itself imply the strong convexity of the surrogate cost ft over the
space of controllers M. This is because  in the surrogate cost ft  ct is applied to y(M )  v(M ) which
themselves are linear functions of M; the linear map M is necessarily column-rank-deﬁcient. To
observe this  note that M maps from a space of dimensionality H × dim(x) × dim(u) to that of
dim(x) + dim(u). The next theorem  which forms the core of our analysis  shows that this is not the
case using the inherent stochastic nature of the dynamical system.
Lemma 4.2. If the cost functions ct(· ·) are α-strongly convex  K is a (κ  γ) diagonal strongly stable
matrix and Assumption 2.1 is met then the idealized functions ft(M ) are λ-strongly convex with
respect to M where

λ =

ασ2γ2
36κ10

We present the proof for simple cases in Section 6  deferring the general proof to Section F.

Online Natural Gradient Update
Theorem 4.3 (Online Natural Gradient Update). Suppose Algorithm 1 (Online Natural Gradient
−1  on an LDS satisfying Assumptions 2.1 and with control
Update) is executed with ηt = Θ (αt)
costs satisfying Assumption 2.2. Then  it holds true that
JT (A)− min

where µ−1 (cid:44) max

(cid:18) GW 2

M∈M(cid:107)(E[J T J])−1∇Mvecft(M )(cid:107).

K∈K JT (K) ≤ ˜O

log7(T )

(cid:19)

αµ

In Theorem 4.3  the regret guarantee depends on an instance-dependent parameter µ  which is a
measure of hardness of the problem. First  we note that the proof of Lemma 4.2 establishes that the
Gram matrix of the Jacobian (Deﬁntion 3.5) is strictly positive deﬁnite and hence we recover the
logarithmic regret guarantee achieved by the Online Gradient Descent Update  with the constants
preserved.
Corollary 4.4. In addition to the assumptions in Theorem 4.3  if K is a (κ  γ)-diagonal strongly
stable matrix  then for the natural gradient update
K∈K JT (K) ≤ ˜O

(cid:18) G2W 4

JT (A) − min

log7(T )

(cid:19)

 

ασ2

Proof. The conclusion follows from Lemma 5.2 and Lemma 6.1 which is the core component in the
proof of Lemma 4.2 showing that E[J T J] ≥ γ2σ2

36κ10 · I .

Secondly  we note that  being instance-dependent  the guarantee the Natural Gradient update offers
can potentially be stronger than that of the Online Gradient method. A case in point is the following
corollary involving spherically symmetric quadratic costs  in which case the Natural Gradient update
yields a regret guarantee under demonstrably more general conditions  in that the bound does not
depend on the minimum eigenvalue of the covariance of the disturbances σ2  unlike OGD.
Corollary 4.5. Under the assumptions on Theorem 4.3  if the cost functions are of the form ct(x  u) =
rt((cid:107)x(cid:107)2 + (cid:107)u(cid:107)2)  where rt ∈ [α  β] is an adversarially chosen sequence of numbers and K is chosen
to be a (κ  γ)-diagonal strongly stable matrix  then the natural gradient update guarantees

JT (A) − min

K∈K JT (K) ≤ ˜O

(cid:18) β2W 2

α

(cid:19)

log7(T )

 

Proof. Note (cid:107)∇Mvec ft(M )(cid:107)(E[J T J])−2 = (cid:107)E[J T (rt · I)JMvec](cid:107)(E[J T J])−2 ≤ β(cid:107)Mvec(cid:107).

5 Regret Analysis

The next section is a condensation of the results from [4] which we present in this form to highlight
the reduction to OCO with memory.

6

5.1 Reduction to Low Regret with Memory

The next lemma shows that achieving low policy regret on the memory based function ft is sufﬁcient
to ensure low regret on the overall dynamical system. Since the proof is essentially provided by [4] 
we provide it in the Appendix for completeness. Deﬁne 

M (cid:44) {M = {M [0] . . . M [H−1]} : (cid:107)M [i−1](cid:107) ≤ κ3κB(1 − γ)i}.

Lemma 5.1. Let the dynamical system satisfy Assumption 2.1 and let K be any (κ  γ)-diagonal
strongly stable matrix. Consider a sequence of loss functions ct(x  u) satisfying Assumption 2.2 and
a sequence of policies M0 . . . MT satisfying

PolicyRegret =

ft(Mt−H−1:t) − min
M∈M

ft(M ) ≤ R(T )

T(cid:88)

t=0

T(cid:88)

t=0

for some function R(T ) and ft as deﬁned in Deﬁnition 3.4. Let A be an online algorithm that plays
the non-stationary controller sequence {M0  . . . MT}. Then as long as H is chosen to be larger than
γ−1 log(T κ2) we have that

J(A) − min

K∗∈K J(K∗) ≤ R(T ) + O(GW 2 log(T )) 

Here O(·)  Θ(·) contain polynomial factors in γ−1  κB  κ  d.
Lemma 5.2. The function ft as deﬁned in Deﬁnition 3.4 is coordinate-wise L-lipschitz and the norm
of the gradient is bounded by Gf   where
2DGW κBκ3

2κBκ3

(cid:18)

(cid:19)

  Gf ≤ GDW Hd

L =

γ

where D (cid:44) W κ2(1 + Hκ2

Bκ3)
γ(1 − κ2(1 − γ)H+1)

+

H +

γ
κBκ3W

.

γ

The proof of this lemma is identical to the analogous lemma in [4] and hence is omitted.

5.2 Analysis for Online Gradient Descent

In the setting of Online Convex Optimization with Memory  as shown by [5]  by running a memory-
based OGD  we can bound the policy regret by the following theorem  proven in the appendix.
Theorem 5.3. Consider the OCO with memory setting deﬁned in Section 3.4. Let {ft}T
t=H be
Lipschitz loss functions with memory such that ft(x) are λ-strongly convex  and let L and Gf be as
deﬁned in (3.1) and (3.2). Then  there exists an algorithm which generates a sequence {xt}T
t=0 such

T(cid:88)

t=H

ft(xt−H:t) − min
x∈K

T(cid:88)

t=H

f + LH 2Gf

(1 + log(T )).

˜ft(x) ≤ G2
(cid:16) G2W 4H 6

λ

(cid:17)

Proof of Theorem 4.1. Setting H = γ−1 log(T κ2)  Theorem 5.3  in conjunction with Lemma 5.2 
implies that policy regret is bounded by ˜O
. An invocation of Lemma 5.1 now
sufﬁces to conclude the proof of the claim.

log T

ασ2

5.3 Analysis for Online Natural Gradient Descent

Consider structured loss functions of the form ft(M0:H+1) = E[ct(z)]  where z =(cid:80)H+1
Deﬁne J =(cid:80)H+1

i=0 Ji[Mi]vec.
Ji is a random matrix  and ct’s are adversarially chosen strongly convex loss functions. In a similar
vein  deﬁne ft(M ) to be the specialization of ft when input the same argument  i.e. M  H + 1 times.

i=0 Ji. The proof of the following theorem may be found in the appendix.

Theorem 5.4. In the setting desribed in this subsection  let ct be α-strongly convex  and fT be such
that it satisﬁes equation (3.1) with constant L  and Gf = maxM∈M (cid:107)(E[J T J])−1∇Mvecft(M )(cid:107).
Then  the online natural gradient update generates a sequence {Mt}T
T(cid:88)
˜ft(M ) ≤ maxM∈M (cid:107)∇Mvecft(M )(cid:107)2

(E[J T J])−1 + LH 2Gf

t=0 such that

T(cid:88)

(1+log(T )).

ft(Mt−H:t)− min
M∈M

α

t=H

t=H

7

Proof of Theorem 4.3. First observe that (cid:107)∇Mvec ft(M )(cid:107)2
(E[J T J])−1 ≤ µ−1(cid:107)∇Mvec ft(M )(cid:107). Setting
H = γ−1 log(T κ2)  Theorem 5.4  in conjunction with Lemma 5.2  imply the stated bound on policy
regret. An invocation of Lemma 5.1 sufﬁces to conclude the proof of the claim.

6 Proof of Strong Convexity in Simpler Cases

We will need some deﬁnitions and preliminaries that are outlined below. By deﬁnition we have that
ft(M ) = E[ct(yt(M )  vt(M ))]. Since we know that ct is strongly convex we have that
y Jy + J(cid:62)
[J(cid:62)

[∇2ct(y(M )  v(M ))] (cid:23) αE{wk}2H−1

∇2ft(M ) = E{wk}2H−1

v Jv].

k=0

k=0

Jy  Jv are random matrices dependent on the noise {wk}2H−1
k=0 . The next lemma implies Lemma 4.2.
Lemma 6.1. If Assumption 2.1 is satisﬁed and K is chosen to be a (κ  γ)-diagonal strongly stable
matrix  then the following holds 

E{wk}2H−1

k=0

[J(cid:62)
y Jy + J(cid:62)

v Jv] (cid:23) γ2σ2

36κ10 · I.

To analyze Jy  Jv  we will need to rearrange the deﬁnition of y(M ) to make the dependence on each
individual M [i] explicit. To this end consider the following deﬁnition for all k ∈ [H + 1].

˜vk(M ) (cid:44) H(cid:88)

i=1

M [i−1]w2H−i−k

Under this deﬁnition it follows that

H(cid:88)

y(M ) =

(A − BK)k−1B˜vk(M ) +

k=1

v(M ) = −Ky(M ) + ˜v0(M )

k=1

H(cid:88)

(A − BK)k−1w2H−k

From the above deﬁnitions  (Jy  Jv) may be characterized in terms of the Jacobian of ˜vk with respect
to M  which we deﬁne for the rest of the section as J˜vk. Deﬁning Mvec as the stacking of rows of
each M [i] vertically  i.e. stacking the columns of (M [i])(cid:62)  it can be observed that for all k 

=(cid:2)Idu ⊗ w(cid:62)

J˜vk =

∂˜vk(M )

∂M

2H−k−1

Idu ⊗ w(cid:62)

2H−k−2 . . . Idu ⊗ w(cid:62)

H−k

(cid:3)

where du is the dimension of the controls. We are now ready to analyze the two simpler cases. Further
on in the section we drop the subscripts {wk}2H−1
6.1 Proof of Lemma 6.1: K = 0
In this section we assume that K = 0 is a (κ  γ)-diagonal strongly stable policy for (A  B). Be
deﬁnition  we have v(M ) = ˜v0(M ). One may conclude the proof with the following observation.

from the expectations for brevity.

k=0

E[J(cid:62)

y Jy + J(cid:62)

v Jv] (cid:23) E[J(cid:62)

v Jv] = E[J(cid:62)

J˜v0 ] = Idu ⊗ Σ (cid:23) σ2I.

˜v0

6.2 Proof of Lemma 6.1: 1-dimensional case

Here  we specialize Lemma 4.2 to one-dimensional state and one-dimensional control. This case
highlights the difﬁculty caused in the proof due to a choosing a non-zero K and presents the main
ideas of the proof in a simpliﬁed notation.
Note that in the one dimensional case  the policy given by M = {M [i]}H−1
i=0 is an H dimensional
vector with M [i] being a scalar. Furthermore y(M )  v(M )  ˜vk(M ) are scalars and hence their
Jacobians Jy  Jv  J˜vk with respect to M are 1 × H vectors. In particular we have that 

J˜vk =

∂˜vk(M )

∂M

= [w2H−k−1 w2H−k−2 . . . wH−k]

8

E[J(cid:62)

y Jy] =

E[J(cid:62)

˜v0

Jy] =

(cid:33)
(cid:125)

H(cid:88)

k1=1

k2=1

(cid:32) H(cid:88)
(cid:124)
(cid:32) H(cid:88)
(cid:124)

k=1

Tk1−k2 · (A − BK)k1−1+k2−1

(cid:123)(cid:122)

(cid:44)G
T−k(A − BK)k−1

(cid:123)(cid:122)

(cid:44)Y

(cid:33)
(cid:125)

·B · σ2

·B2 · σ2

(6.2)

(6.3)

Therefore using the fact that E[wiwj] = 0 for i (cid:54)= j and E[w2
k1  k2  we have that
] = Tk1−k2 · σ2
(6.1)
where Tm is deﬁned as an H × H matrix with [Tm]ij = 1 if and only if i − j = m and 0 otherwise.
This in particular immediately gives us that 

i ] = σ2  it can be observed that for any

E[J(cid:62)
˜vk1

J ˜vk2

First  we prove a few spectral properties of the matrices G and Y deﬁned above. From Gershgorin’s
circle theorem  and the fact that K is (κ  γ)-diagonal strongly stable  we have
(T−k + Tk)(A − BK)k−1(cid:107) ≤ 2γ−1

(cid:107)Y + Y(cid:62)(cid:107) ≤ (cid:107) H(cid:88)

(6.4)

k=1

The spectral properties of G summarized in the lemma below form the core of our analysis.
Lemma 6.2. G is a symmetric positive deﬁnite matrix. In particular G (cid:23) 1
Now consider the statements which follow by the respective deﬁnitions.

4 · I.

E[J(cid:62)

v Jv] = K2 · E[J(cid:62)

= σ2 ·(cid:0)B2K2 · G − BK · (Y + Y(cid:62)) + I(cid:1)
y J˜v0] − K · E[J(cid:62)
(cid:125)
˜v0
.

y Jy] − K · E[J(cid:62)
(cid:123)(cid:122)

(cid:124)

(cid:44)F

Jy] + E[J(cid:62)

˜v0

J˜v0]

Now F (cid:23) 0. We ﬁnish the proof by considering two cases. The ﬁrst case is when 3|B|γ−1κ ≥ 1.
Noting κ ≥ 1  in this case Lemma 6.2 immediately implies that

m(cid:62)(cid:0)F + B2 · G(cid:1) m ≥ m(cid:62)(cid:0)B2 · G(cid:1) m ≥ 1

4(cid:107)m(cid:107)2
9γ−2κ2 ≥ γ2(cid:107)m(cid:107)2
36κ10  

In the second case (when 3|B|γ−1κ ≤ 1)  (6.4) implies that

m(cid:62)(cid:0)F + B2 · G(cid:1) m ≥ m(cid:62)(cid:0)I − BK · (Y + Y(cid:62))(cid:1) m ≥ (1/3)(cid:107)m(cid:107)2 ≥ γ2(cid:107)m(cid:107)2

36κ10 .

6.2.1 Proof of Lemma 6.2
Recall Tm is deﬁned as an H × H matrix with [Tm]ij = 1 if and only if i − j = m and 0 otherwise.
Deﬁne the following matrix for any complex number |ψ| < 1.

H(cid:88)

H(cid:88)

G(ψ) =

Tk1−k2

(cid:0)ψ†(cid:1)k1−1

ψk2−1

k1=1

k2=1

Note that G in Lemma 6.2 is equal to G(A − BK). The following lemma  proven in Section E 
provides a lower bound on the spectrum of the matrix G(ψ). The lemma presents the proof of a more
general case (φ is complex) that aids the multi-dimensional case. A special case when φ = 1 was
proven in [12]  and we follow a similar approach relying on the inverse.
Lemma 6.3. Let ψ be a complex number such that |ψ| ≤ 1. We have that G(ψ) (cid:23) (1/4) · IH.

7 Conclusion

We presented two algorithms for controlling linear dynamical systems with strongly convex costs
√
with regret that scales poly-logarithmically with time. This improves state-of-the-art known regret
bounds that scale as O(
T ). It remains open to extend the poly-log regret guarantees to more general
systems and loss functions  such as exp-concave losses  or alternatively  show that this is impossible.

9

Acknowledgements

The authors thank Sham Kakade and Cyril Zhang for various thoughtful discussions. Elad Hazan
acknowledges funding from NSF grant # CCF-1704860.

References
[1] Yasin Abbasi-Yadkori  Peter Bartlett  and Varun Kanade. Tracking adversarial targets. In

International Conference on Machine Learning  pages 369–377  2014.

[2] Yasin Abbasi-Yadkori  Nevena Lazic  and Csaba Szepesv´ari. Model-free linear quadratic
control via reduction to expert prediction. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics  pages 3108–3117  2019.

[3] Yasin Abbasi-Yadkori and Csaba Szepesv´ari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory  pages
1–26  2011.

[4] Naman Agarwal  Brian Bullins  Elad Hazan  Sham Kakade  and Karan Singh. Online control
with adversarial disturbances. In Proceedings of the 36th International Conference on Machine
Learning  pages 111–119  2019.

[5] Oren Anava  Elad Hazan  and Shie Mannor. Online learning for adversaries with memory: price
of past mistakes. In Advances in Neural Information Processing Systems  pages 784–792  2015.
[6] Raman Arora  Ofer Dekel  and Ambuj Tewari. Online bandit learning against an adaptive
adversary: from regret to policy regret. In Proceedings of the 29th International Conference on
Machine Learning  pages 1503–1510  2012.

[7] Sanjeev Arora  Elad Hazan  Holden Lee  Karan Singh  Cyril Zhang  and Yi Zhang. Towards

provable control for unknown linear dynamical systems. 2018.

[8] Nicolo Cesa-Bianchi and G´abor Lugosi. Prediction  learning  and games. Cambridge university

press  2006.

[9] Alon Cohen  Avinatan Hasidim  Tomer Koren  Nevena Lazic  Yishay Mansour  and Kunal
Talwar. Online linear quadratic control. In International Conference on Machine Learning 
pages 1028–1037  2018.

[10] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. Regret bounds for
robust adaptive control of the linear quadratic regulator. In Advances in Neural Information
Processing Systems  pages 4188–4197  2018.

[11] Maryam Fazel  Rong Ge  Sham M Kakade  and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning  pages 1466–1475  2018.

[12] Surbhi Goel  Adam Klivans  and Raghu Meka. Learning one convolutional layer with overlap-
ping patches. In Proceedings of the 35th International Conference on Machine Learning  pages
1783–1791  2018.

[13] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-

tion  2(3-4):157–325  2016.

[14] Elad Hazan  Holden Lee  Karan Singh  Cyril Zhang  and Yi Zhang. Spectral ﬁltering for
general linear dynamical systems. In Advances in Neural Information Processing Systems 
pages 4634–4643  2018.

[15] Elad Hazan  Karan Singh  and Cyril Zhang. Learning linear dynamical systems via spectral

ﬁltering. In Advances in Neural Information Processing Systems  pages 6702–6712  2017.

[16] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

Trends R(cid:13) in Machine Learning  4(2):107–194  2012.

[17] Robert F Stengel. Optimal control and estimation. Courier Corporation  1994.
[18] Gilbert Strang. Introduction to linear algebra  volume 3.

10

,Naman Agarwal
Elad Hazan
Karan Singh