2016,A Sparse Interactive Model for Matrix Completion with Side Information,Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods  this model does not require the low-rank condition on the model parameter matrix. We prove that when the side features can span the latent feature space of the matrix to be recovered  the number of observed entries needed for an exact recovery is $O(\log N)$ where $N$ is the size of the matrix. When the side features are corrupted latent features of the matrix with a small perturbation  our method can achieve an $\epsilon$-recovery with $O(\log N)$ sample complexity  and maintains a $\O(N^{3/2})$ rate similar to classfic methods with no side information. An efficient linearized Lagrangian algorithm is developed with a strong guarantee of convergence. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.,A Sparse Interactive Model for Matrix Completion

with Side Information

Jin Lu

Guannan Liang

Jiangwen Sun

Jinbo Bi

{jin.lu  guannan.liang  jiangwen.sun  jinbo.bi}@uconn.edu

University of Connecticut

Storrs  CT 06269

Abstract

Matrix completion methods can beneﬁt from side information besides the partial-
ly observed matrix. The use of side features that describe the row and column
entities of a matrix has been shown to reduce the sample complexity for complet-
ing the matrix. We propose a novel sparse formulation that explicitly models the
interaction between the row and column side features to approximate the matrix
entries. Unlike early methods  this model does not require the low rank condition
on the model parameter matrix. We prove that when the side features span the
latent feature space of the matrix to be recovered  the number of observed entries
needed for an exact recovery is O(log N ) where N is the size of the matrix. If the
side features are corrupted latent features of the matrix with a small perturbation 
our method can achieve an -recovery with O(log N ) sample complexity. If side
information is useless  our method maintains a O(N 3/2) sampling rate similar to
classic methods. An efﬁcient linearized Lagrangian algorithm is developed with
a convergence guarantee. Empirical results show that our approach outperforms
three state-of-the-art methods both in simulations and on real world datasets.

Introduction

1
Matrix completion has been a basis of many machine learning approaches for computer vision [6] 
recommender systems [21  24]  signal processing [19  27]  and among many others. Classically 
low-rank matrix completion methods are based on matrix decomposition techniques which require
only the partially observed data in the matrix [15  3  14] by solving the following problem

minE (cid:107)E(cid:107)∗ 

subject to RΩ(E) = RΩ(F) 

(1)
where F ∈ Rm×n is the partially observed low-rank matrix (with a rank of r) that needs to be
recovered  Ω ⊆ {1 ···   m}×{1 ···   n} be the set of indexes where the corresponding components
in F are observed  the mapping RΩ(M): Rm×n → Rm×n gives another matrix whose (i  j)-th
entry is Mi j if (i  j) ∈ Ω (or 0 otherwise)  and (cid:107)E(cid:107)∗ computes the nuclear norm of E. Early
theoretical analysis [4  5  20] proves that O(N r log2 N ) entries are sufﬁcient for an exact recovery
if the observed entries are uniformly sampled at random where N = max{n  m}.
Recent studies start to explore side information for matrix completion and factorization [1  18  7 
17  8]. For example  to infer the missing ratings in a user-movie rating matrix  descriptors of the
users and movies are often known and may help to build a content-based recommender system. For
instance  kids tend to like cartoons  so the age of a user likely interacts with the cartoon feature of a
movie. When few ratings are known  this side information could be the main source for completing
the matrix. Although based on empirical studies  several works found that side features are helpful
[17  18]  those methods are based on non-convex matrix factorization formulations without any
theoretical guarantees. Three recent methods have focussed on convex nuclear-norm regularized
objectives  which leads to theoretical guarantees on matrix recovery [13  28  9  16]. These methods
all construct an inductive model XT GY so that RΩ(XT GY) = RΩ(F) where the side matrices

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

X and Y consist of side features  respectively  for the row entities (e.g.  users) and column entities
(e.g.  movies) of a (rating) matrix. This inductive model has a parameter matrix G which is either
required to be low rank [13] or to have a minimal nuclear norm (cid:107)G(cid:107)∗ [28]. Recovering G of a
(usually) smaller size is argued to be easier than directly recovering the matrix F. With a very
strong assumption on ‘perfect’ side information  i.e.  both X and Y are orthonormal matrices and
respectively in the latent column and row space of the matrix F  the method in [28] is proved to
require much reduced sample complexity O(log N ) for an exact recovery of F. Because most side
features X and Y are not perfect in practice  a very recent work [9] proposes to use a residual matrix
N to handle the noisy side features. This method constructs an inductive model XT GY + N to
approximate F and requires both G and N to be low rank  or have a low nuclear norm. It uses the
nuclear norm of the residual to quantify the usefulness of side information  and proves O(log N )
sampling rate for an -recovery when X and Y span the full latent feature space of F  and o(N )
sample complexity when X and Y contain corrupted latent features of F. An -recovery is deﬁned
as that the expected discrepancy between the predicted matrix and the true matrix is less than an
arbitrarily small  > 0 under a certain probability.
In this paper  we propose a new method for matrix recovery by constructing a sparse interactive
model XT GY to approximate F where G can be sparse but does not need to be low rank. The
(i  j)-th element of G determines the role of the interaction between the i-th feature of users and
the j-th feature of products. The low-rank property of F is commonly assumed to characterize the
observation that similar users tend to rate similar products similarly [4]. When using an inductive
approximation F = XT GY  rank(F) ≤ rank(G)  so a low-rank requirement on G can be a suf-
ﬁcient condition on the low-rank condition of F. Previous relevant methods [13  28  9] all impose
the low-rank condition on G  which is however not a necessary condition for F to be low rank
(only becomes a necessary condition when X and Y are full rank). Given general side matrices
X ∈ Rd1×m and Y ∈ Rd2×n where the numbers of features d1  d2 (cid:28) N  limiting the interactive
model of G ∈ Rd1×d2 to be low rank can be an over-restrictive constraint. In our model  we use
a low-rank matrix E to directly approximate F and then estimate E from the interactive model of
X and Y with a sparse regularizer on G. We show empirically that a low-rank F can be recovered
from a corresponding full (or high) rank G. Our contributions are summarized as follows:
(i) We propose a new formulation that estimates both E and G by imposing a nuclear-norm con-
straint on E but a general regularizer on G  e.g.  the sparse regularizer (cid:107)G(cid:107)1. The proposed model
has recovery guarantees depending on the quality of the side features: (1) when X and Y are full row
rank and span the entire latent feature space of F (but are not required to satisfy the much stronger
condition of being orthonormal as in [28])  O(log N ) observations are still sufﬁcient for our method
to achieve an exact recovery of F. (2) When the side matrices are not full rank and corrupted from
the original latent features of F  i.e.  X and Y do not contain enough basis to exactly recover F 
O(log N ) observed entries can be sufﬁcient for an -recovery.
(ii) A new linearized alternating direction method of multipliers (LADMM) is developed to efﬁ-
ciently solve the proposed formulation. Existing methods that use side information are solved by
standard block-wise coordinate descent algorithms which have convergence guarantee to a glob-
al solution only when each block-wise subproblem has a unique solution [26]. Our LADMM has
stronger convergence property [29] and beneﬁts from the linear convergence rate of ADMM [11  23].
(iii) Prior methods focus on the recovery of F  and little light has been shed to understand whether
the interactive model of G can be retrieved. Because of the explicit use of E and G  our method
aims to directly recover both. The unique G in the case of exact recovery of F can be attained by
our algorithm. When G is not unique in the -recovery case  our algorithm converges to a point in
the optimal solution set.

2 The Proposed Interactive Model
To utilize the side information in X and Y to complete F  we consider to build a predictive model
from the observed components that predicts the missing ones. One can simply build a linear model:
f = xT u + yT v + g  where x and y are the feature vectors respectively for a user and a product 
and u  v and g are model parameters. In real life applications  interactive terms between the features
in X and Y can be very important. For example  male users tend to rate science ﬁction and action
movies higher than female  which can be informative when predicting their ratings. Therefore  a
linear model considering no interactive terms can be oversimple and have low predictive power for
missing entries. We hence add interactive terms by introducing an interaction matrix Hd1×d2 into
the predictive model  which can be written as: f = xT Hy + xT u + yT v + g. By deﬁning

2

(cid:18) H u

(cid:19)

¯x = [xT 1]T   ¯y = [yT 1]T and G(a=d1+1)×(b=d2+1) =
simpliﬁed to: f = ¯xT G¯y. The following optimization problem can be solved to obtain the model
parameter G.

the above model can be

vT

g

g(G) + λE(cid:107)E(cid:107)∗ 

min
G E

subject to

¯XT G ¯Y = E  RΩ(E) = RΩ(F) 

where E is a completed version of F  ¯Xa×m and ¯Yb×n are two matrices that are created by aug-
menting one row of all ones to X and to Y  respectively  and g(G) and (cid:107)E(cid:107)∗ are used to incorporate
the (sparsity) prior of G and low rank prior of E. Because the side information data can be noisy
and not all the features and their interactions are helpful to the prediction of F  a sparse G is often
expected. Our implementation has used g(G) = (cid:107)G(cid:107)1. It is natural to impose low rank requirement
on E because it is a completed version of a low rank matrix F. The tuning parameter λE is used to
balance the two priors in the objective.
Without loss of generality and for convenience of notation  we simply use X and Y to denote the
augmented matrices. Denote the Frobenius norm of a matrix by || · ||F . To account for Gaussian
noise  we relax the equality constraint XT GY = E and replace it by minimizing their squared
residual: (cid:107)XT GY− E(cid:107)2
F and solve the following convex optimization problem to obtain G and E:

1
2

(cid:107)XT GY − E(cid:107)2

F + λGg(G) + λE(cid:107)E(cid:107)∗ 

min
G E

subject to RΩ(E) = RΩ(F).

(2)
where λG is another tuning parameter that together with λE balances the three terms in the objective.
Especially  the regularizer g(·) in our theoretical analysis can take any general matrix norm that
satisﬁes (cid:107)M(cid:107)∗ ≤ Cg(M))  ∀M  for a constant C  so for instance g(·) can be (cid:107)G(cid:107)1  or ||G||F   or
(cid:107)G(cid:107)2. Throughout this paper  the matrices X (and Y) refer to  i.e.  either the original Xd1×m (and
Yd2×n) or the augmented ¯Xa×m (and ¯Yb×n) depending on the user-speciﬁed model.
Our formulation (2) differs from existing methods that make use of side information for matrix
completion in several ways. Existing methods [28  13  9] solve the problem by ﬁnding ˆH that
minimizes (cid:107)H(cid:107)∗ subject to RΩ(XT HY) = RΩ(F)  but we expand it to include the linear term
within the interactive model. The proposed model adds the ﬂexibility to consider both linear and
quadratically interactive terms  and allows the algorithm to determine the terms that should be used
in the model by enforcing the sparsity in H (or G). Because E = XT GY  the rank of G bounds
that of E from above. The existing methods all control the rank of G (e.g. by minimizing (cid:107)G(cid:107)∗) to
incorporate the prior of low rank E (and thus low rank F) in their formulations. However  when the
rank of G is not properly chosen during the tuning of hyperparameters  it may not even be a sufﬁcient
condition to ensure low rank E (if rank(E) (cid:28) the pre-speciﬁed rank(G)). It is easy to see that besides
G a low-rank X or Y can lead to a low-rank E as well. Enforcing a low-rank condition on H or G
may limit the search space of the interactive model and thus impair the prediction performance on
missing matrix entries  which are demonstrated in our empirical results. Moreover  one can observe
that when λG is sufﬁciently large  Eq.(2) is reduced to the standard matrix completion problem (1)
without side information because G may be degenerated into a zero matrix  so our formulation is
applicable when no access to useful side information.
3 Recovery Analysis
Let E0 and G0 be the two matrices such that RΩ(F) = RΩ(E0) and E0 = XT G0Y. In this
section  we give our theoretical results on the sample complexity for achieving an exact recovery of
E0 and G0 when X and Y are both full row rank (i.e.  rank(X) = a and rank(Y) = b)  and an
-recovery of E0 when the two side matrices are corrupted and less informative. The proofs of all
theorems are given in supplementary materials.
3.1 Sample Complexity for Exact Recovery
Before presenting our results  we give a few deﬁnitions. Let F = UΣVT   XT = UXΣXVT
X and
YT = UYΣYVT
Y be the singular value decomposition of F  XT and YT   respectively  where all
Σ matrices are full rank  meaning that singular vectors corresponding to the singular value 0 are not
included in the respective U and V matrices. Let

PU = UUT ∈ Rm×m  PV = VVT ∈ Rn×n 

PX = UXUT

X = XT VXΣ−2

X VT

XX ∈ Rm×m  PY = UYUT

Y = YT VYΣ−2

Y VT

YY ∈ Rn×n 

3

where PU  PV  PX and PY project a vector onto the subspaces spanned  respectively  by the
columns in U  V and rows in X  and Y. For any matrix Mm×n that satisﬁes M = PXMPY 
we deﬁne two linear operators: PT : Rm×n → Rm×n and PT ⊥ : Rm×n → Rm×n as follows:

PT (M) = PUMPY + PXMPV − PUMPV
PT ⊥ (M) = (PX − PU)M(PY − PV) = PX⊥MPY⊥ .

Let µ0 and µ1 be the two coherence measures of F and be deﬁned as follows as discussed in [4  16]:

µ0 = max

max
1≤i≤m

(cid:107)PUei(cid:107)2 

n
r

max
1≤j≤n

(cid:107)PVej(cid:107)2

r

 

µ1 = max
i j

mn
r

([UVT ]i j)2 

where ei is the unit vector with the ith entry equal to 1. Let µXY be the coherence measurement
between X and Y and be deﬁned as:

(cid:32)

µXY = max

max
1≤i≤m

m(cid:107)xi(cid:107)2

2

a

  max
1≤j≤n

n(cid:107)yj(cid:107)2

2

b

(cid:33)

.

(cid:18) m

(cid:19)

X (cid:107)∗ (cid:107)Σ−1

Y (cid:107)∗)  N = max(m  n)  q0 = 1

3 σµ max(µ1  µ)r(a + b) log N and T1 = 8p

3 σµ max(µ1  µ)(1 + log a − log r)r(a + b) log N.

With the above deﬁnitions  we show in the following theorem that when X and Y are both full row
rank  (G0  E0) is the unique solution to Eq.(2) with high probability as long as there are O(r log N )
observed components in F. In other words  with a sampling rate of O(r log N )  our method can
fully recover both E0 and G0 with a high probability when X and Y are full row rank.
Theorem 1 Let µ = max(µ0  µXY)  σ = max((cid:107)Σ−1
2 (1 +
log a − log r)  T0 = 128p
3 σ2µ2(ab + r2) log N  where p
is a constant. Assume T1 ≥ q0T0  X and Y are both full row rank. For any p > 1  with a probability
at least 1 − 4(q0 + 1)N−p+1 − 2q0N−p+2  (G0  E0) is the unique optimizer to Problem (2) with
necessary sampling rate as few as O(r log N ). More precisely  the sampling size |Ω| should satisfy
that |Ω| ≥ 64p
When r (cid:28) N and r = O(1)  the sampling rate for the exact recovery of both E0 and G0 reduces to
O(log N ). A similar sampling rate for a full recovery of E0 has been developed in [28] where both
X and Y  however  need to be orthonormal matrices in their derivation. In Theorem 1  because σ
is mainly determined by the smallest singular values of the side information matrices  and sampling
rate increases when σ increases  it suggests that side information matrices of lower rank would re-
quire more observed F entries for a full recovery of F. An advanced model without the orthonormal
assumption has been given in [9]  but exact recovery is not discussed. In our case  the two matrices
are only required to be full row rank. Moreover  the theoretical or empirical results in our work give
the ﬁrst careful investigation on the recovery of both G0 and E0.
3.2 Sample Complexity for -Recovery
The condition for full-rank side information matrices may not be satisﬁed in some cases to fully
recover E0 (or F). We analyze the error bound of our model and prove a reduced sample complexity
in comparison with standard matrix completion methods for an -recovery when the side information
matrices are not full row rank or their rank is difﬁcult to attain.
Theorem 2 Denote (cid:107)E(cid:107)∗ ≤ α  (cid:107)G(cid:107)1 ≤ γ  (cid:107)XT GY − E(cid:107)F ≤ φ and the perfect side feature
matrices (containing latent features of F) are corrupted with ∆X and ∆Y where (cid:107)∆X(cid:107)F ≤
s1 (cid:107)∆Y(cid:107)F ≤ s2 and S = max(s1  s2). To -recover F that the expected loss E[l(f  F)] < 
for a given arbitrarily small  > 0  O(min((γ2 + φ2) log N  S2α
N )/2) observations are sufﬁ-
cient for our model when corrupted factors of side information are bounded.
Theorem 2 can be inferred from the fact that the trace norm of E and the (cid:96)1-norm of G affect
sample complexity of our model. It meets the intuition that higher rank matrix ought to require
more observations to recover. Besides  for the discovery of G  a sparse interactive matrix can lead
to the decrease of the sample complexity  which implies that the side information  even though when
it is not perfect  could be informative enough such that the original matrix can be compressed by
sparse coding via the estimated interaction between the features of row and column entities of the
matrix. Our empirical evaluations have conﬁrmed the utility of even imperfect side features.
When the rank of the original data matrix r = O(1) (r (cid:28) N)  and correspondingly α = O(1) 
Theorem 2 points out that only O(log N ) sampling rate is required for an -recovery. The clas-
sic matrix completion analysis without side information shows that under certain conditions  one

√

4

can achieve O(N poly log N ) sample complexity for both perfect recovery [4] and -recovery [25] 
which is higher than our complexity. However  the condition for these existing bounds is that the ob-
served entries follow a certain distribution. Recent studies [22] found that if no speciﬁc distribution
is pre-assumed for observed entries  O(N 3/2) sampling rate is sufﬁcient for an -recovery. Com-
pared to those results  our analysis does not require any assumption on the distribution of observed
entries. When X and Y contain insufﬁcient interaction information about F and (cid:107)E(cid:107)∗ = O(N ) 
the sample complexity of our method increases to O(N 3/2) in the worst case  which means that our
model maintains the same complexity as the classic methods.
4 Adaptive LADMM Algorithm
In this section  we develop an adaptive LADMM algorithm [29] to solve problem (2). First  we show
that the ADMM is applicable in our problem and we then derive LADMM steps. A convergence
proof is established to guarantee the performance of our algorithm.
Because it requires separable blocks of variables in order to use ADMM  we ﬁrst deﬁne C = E −
XT GY and use it in Eq.(2). Then the augmented Lagrangian function of (2) is given by

F + λE(cid:107)E(cid:107)∗ + λG(cid:107)G(cid:107)1 + (cid:104)M1  RΩ(E − F)(cid:105) +

(cid:107)E − XT GY − C(cid:107)2

F

(cid:68)

L(E  G  C  M1  M2  β) =
M2  E − XT GY − C

+

(cid:69)

+

(cid:107)C(cid:107)2
(cid:107)RΩ(E − F)(cid:107)2

1
2
β
2

F +

β
2

where M1  M2 ∈ Rm×n are Lagrange multipliers and β > 0 is the penalty parameter. Given Ck 
Gk  Ek  Mk
2 at iteration k  each group of the variables yields their respective subproblems:

1 and Mk

After solving these subproblems  we update the multipliers M1 and M2 as follows;

Ck+1 = arg min
C
Gk+1 = arg min
G
Ek+1 = arg min
E

L(Ek  Gk  Mk
L(Ek  G  Mk
L(E  Gk+1  Mk

2  C  βk) 
2  Ck+1  βk) 

1  Mk

2  Ck+1  βk) 

Mk+1
Mk+1

1 =Mk
2 =Mk

1 + βk(RΩ(Ek+1 − F)) 
2 + βk(Ek+1 − XT Gk+1Y − Ck+1).

(3)

(4)

(5)

We focus on demonstrating the iterative steps of the adaptive LADMM. Given Ck  Gk Ek  Mk
1
and Mk
2  Algorithm 1 describes how to obtain the next iterate (C  E  G  M1  M2). A closed-form
solution has been derived for each subproblem in the supplementary material.

Algorithm 1 The adaptive LADMM algorithm to solve Ck  Gk  Ek  k = 1  ...  K
Input: X  Y and RΩ(F) with parameters λG  λE  τA  τB  ρ and βmax.
Output: C  G  E;
1: Initialize E0  G0  M0

2. Compute A = YT ⊗ XT . k = 0 

1  M0

  0)(cid:12) sgn(gk− f k

1 = AT (Agk +
τAβk
2/βk) and e = vec(E)  g = vec(G)  m = vec(M) 

1 /τA)) where f k

3 )/(2τB)  λE/2(βkτB)) where f k

2 = RΩ(Ek − F + Mk

1/βk);

repeat;

2/βk);
1 /τA|− λG

2: Ck+1 = βk
3: Gk+1 = reshape(max(|gk− f k

βk+1 (Ek − XT GkY + Mk
1) = AT (Agk + ck − ek − mk

4: Ek+1 = SV T (Ek − (f k

ck − bk
c = vec(C).
3 = Ek − XT Gk+1Y − Ck + Mk
f k
1 + βk(RΩ(Ek+1 − F)).
1 = Mk
2 + βk(Ek+1 − XT Gk+1Y − Ck+1).
2 = Mk

5: Mk+1
6: Mk+1
7: βk+1 = min(βmax  ρβk).
8: k = k + 1 until convergence;

2 + f k

2/βk.

Return C  G  E;

The adaptive parameter in Algorithm 1 is ρ > 1  and βmax controls the upper bound of {βk}. The
operator reshape(g) converts a vector g ∈ Rab into a matrix G ∈ Ra×b  which is the inverse

5

1  M0

operator of vec(G). The operator SV T (E  t) is the singular value thresholding process deﬁned in
[3] for soft-thresholding the singular values of an arbitrary matrix E by a threshold t. The matrix
A = YT ⊗ XT where ⊗ indicates the Kronecker product. In the initialization step  M0
2 are
randomly drawn from the standard Gaussian distribution; we initialize E0 and G0 by the iterative
soft-thresholding algorithm [2] and SV T operator respectively.
The adaptive LADMM can effectively solve the proposed optimization problem in several aspect-
s. First  the convergence of the commonly-used block-wise coordinate descent (BCD) method 
sometimes referred to as alternating minimization methods  requires typically that the optimization
problem be strictly convex (or quasiconvex but hemivariate). The strongest result for BCD so far
is established in [26] which requires the alternating subproblems to be optimized in each iteration
to its unique optimal solution. This requirement is often restrictive in practice. Our convex (but
not strictly convex) problem can be solved by the adaptive LADMM with the global convergence
guarantee which is characterized in Theorem 3. Second  two of the subproblems are non-smooth
due to the (cid:96)1-norm or the nuclear norm  so it can be difﬁcult to obtain a closed-form formula to
efﬁciently compute a solution by standard optimization tools; however  adaptive LADMM utilizes
the linearization technique which leads to a closed-form solution for each linearized subproblem 
and signiﬁcantly enhances the efﬁciency of the iterative process. Third  adaptive LADMM can be
practically parallelizable by a similar scheme to that of ADMM. It is also noted that the convergence
rate of LADMM [11] and parallel LADMM is O(1/k) [23] whereas the BCD method still lacks of
clear theoretical results of its convergence rate.
Theorem 3 Deﬁne the operators A and B as A(G) =

  and
. If βk is non-decreasing and upper-bounded  τA > (cid:107)A(cid:107)2  and τB > (cid:107)B(cid:107)2  then
let M =
the sequence {(Ck  Gk  Ek  Mk)} generated by the adaptive LADMM Algorithm 1 converges to a
global minimizer of Eq. (2).

(cid:18)RΩ(E)
(cid:19)

(cid:18)M1

(cid:19)

(cid:18)

M2

(cid:19)

0

−XT GY

  B(E) =

E

2/(cid:107)R(cid:54)Ω(F)(cid:107)2

5 Experimental Results
We validated our method in both simulations and the analysis of two real world datasets: MovieLens
(movie rating) and NCI-DREAM (drug discovery) datasets. Three most recent matrix completion
methods that also utilized side information  MAXIDE[28]  IMC[13] and DirtyIMC[9]  were com-
pared against our method. The design of our experiments focused on demonstrating the effective-
ness of our method in practice. The performance of all methods was measured by the relative mean
squared error (RMSE) calculated on missing entries: (cid:107)R(cid:54)Ω(XT GY − F)(cid:107)2
2. For both
synthetic and real-world datasets  we randomly set q percent of the components in each observed
matrix F to be missing. The hyperparameters λ’s and the rank of G (required by IMC and DirtyIM-
C) were tuned via the same cross validation process: we randomly picked 10% of the given entries to
form a validation set. Then models were obtained by applying each method to the remaining entries
with a speciﬁc choice of λ from 10−3  10−2  ...  104. The average validation RMSE was examined
by repeating the above procedure six times. The hyperparameter values that gave the best average
validation RMSE were chosen for each method. For IMC and DirtyIMC  the best rank of G was
chosen from = 1 to 15 within each data split. For each choice of q  we repeated the above entire
procedure six times and reported the average RMSE on the missing entries.
5.1 Synthetic Datasets
We created two different simulation tests with and without full row rank X and Y. For all the
synthetic datasets  we ﬁrst randomly created X and Y. In order to make our simulations reminiscent
real situations where distributions of side features can be heterogeneous  data for each feature in both
X and Y were generated according to a distribution that was randomly selected from Gaussian 
Poisson and Gamma distributions. We created the sparse G matrices as follows. The location of
the non-zero entries of G were randomly picked but their values were generated by multiplying a
value drawn from N (0  100)  which we repeated several times to chose the matrices that showed
full or high rank. We then generated F with F = XT GY + N where N represents noise and
each component Ni j was drawn from N (0  1). For each simulated F  we ran all methods with
q ∈ [10% − 80%] with an increase step of 10%.
We compared the different methods in three settings  which were labeled as synthetic experiment
I  II and III in our results. In the ﬁrst setting  the dimension of X and Y was set to 15 × 50 and

6

20 × 140 and all features in these two matrices were randomly generated to make them full row
rank. Both the last two settings corresponded to the second test where X and Y were not full
row rank. The dimension of X and Y was set to 16 × 50  21 × 140 and 20 × 50  25 × 140 
respectively  for these two settings where the ﬁrst 15 features in X and 20 features in Y were
randomly created  but the remaining features were generated by arbitrarily linear combinations of
the randomly created features. For all three settings  we used 10 synthetic datasets and reported
mean and standard deviation of RMSE on missing values as shown in Figure 1.

Figure 1: The Comparison of RMSE for Experiments I  II  and III.

Our approach outperformed all other compared methods signiﬁcantly in almost all these settings.
When the missing rate q increased  the RMSE of our method grew much slower than other methods.
We studied the rank of the recovered G and E in the ﬁrst setting. For all methods  the corresponding
G and E that gave the best performance were examined. The ranks of G and E from our method 
MAXIDE  IMC  DirtyIMC were 15  8  1  1 and 15  15  1  2  respectively. These results suggested
that incorporating the strong prior of low rank G might hurt the recovery performance. The retrieved
model matrices G of all compared methods (when using q =10% of missing entries in one of the
10 synthetic datasets) together with the true G are plotted in Figure 2. Only our method was able to
recover the true G and all the other methods merely found approximations.

Figure 2: The heatmap of the true G and recovered G matrices in Synthetic Experiment I.

5.2 Real-world Datasets
We used two relatively large datasets that we could ﬁnd as suitable for our empirical evaluation.
Note that early methods employing side information were often tested on datasets with either X or
Y but not both although some of them might be larger than the two datasets we used.
5.2.1. MovieLens. This dataset was downloaded from [12] and contained 100 000 user ratings
(integers from 1 to 5) from 943 users on 1682 movies. There were 20 movie features such as genre
and release date  as well as 24 user features describing users’ demographic information such as age
and gender. We compared all methods with four different q values: 20-50%. The RMSE values of
each method are shown in Table 1  which shows that our approach signiﬁcantly outperformed other
methods  especially when q was large. Figure 3 shows the constructed G matrix that shows some
interesting observations. For instance  male users tend to rate action  science ﬁction  thriller and war
movies high but low for children’ movies  exhibiting some common intuitions.
5.2.2 NCI-DREAM Challenge. The data on the reactions of 46 breast cancer cell lines to 26 drugs
and the expression data of 18633 genes for all the cell lines were provided by NCI-DREAM Chal-

7

Missing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IIIOur approachMAXIDEIMCDirtyIMCMissing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IIIOur approachMAXIDEIMCDirtyIMCMissing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IIOur approachMAXIDEIMCDirtyIMCMissing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IIOur approachMAXIDEIMCDirtyIMCMissing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IOur approachMAXIDEIMCDirtyIMCMissing percentage0.20.40.60.8RMSE00.20.40.60.81Synthetic Experiment IOur approachMAXIDEIMCDirtyIMCMethods

Our approach

MAXIDE

IMC

DirtyIMC

20%
0.276

(± 0.001)
0.424
(±0.016)
0.935
(±0.001)
0.705
(±0.001)

MovieLens Data
30%
40%
0.284
0.279

(± 0.002)
0.425
(±0.013)
0.943
(±0.001)
0.738
(±0.001)

(± 0.001)
0.419
(±0.008)
0.945
(±0.001)
0.775
(±0.001)

50%
0.292

(± 0.001)
0.421
(±0.013)
0.959
(±0.001)
0.814
(±0.001)

20%
0.181

(± 0.069)
0.268
(±0.036)
0.437
(±0.031)
0.432
(±0.033)

30%
0.139

(± 0.010)
0.240
(±0.007)
0.489
(±0.003)
0.475
(±0.008)

NCI-Dream Challenge

40%
0.145

(± 0.018)
0.255
(±0.016)
0.557
(±0.013)
0.551
(±0.018)

50%
0.190

(± 0.031)
0.288
(±0.022)
0.637
(±0.011)
0.632
(±0.011)

Table 1: The Comparison of RMSE values of different methods on real-world datasets.

lenge [10]. For each drug  we had 14 features that describes their chemical and physical properties
such as molecular weight  XLogP3 and hydrogen bond donor count  and were downloaded from
National Center for Biotechnology Information (http://pubchem.ncbi.nlm.nih.gov/). For the cell
line features  we ran principle component analysis (PCA) and used the top 45 principal components
that accounted for more than 99.99% of the total data variance. We compared the four different
methods with four different q values: 20-50%. The RMSE values of all methods are provided in
Table 1 where our method again shows the best performance. We examined the ranks of both G and
E obtained by all the methods. They were 15  15  1  1 for G and 2  15  1  2 for E  respectively  for
our approach  MAXIDE  IMC and DirtyIMC in sequence. This demonstrates that a low rank E but
a high rank G give the best performance on this dataset. In other words  requiring a low rank G
may hurt the performance of recovering a low rank E.
The constructed G by our method is plotted in Figure 4  where columns represent cell line features
(i.e.  principle components) and rows represent drug features. Please refer to the supplementary ma-
terial for the names of these features. According to this ﬁgure  drug features: XlogP (F2)  hydrogen
bond donor (HBD) (F3)  Hydrogen bond acceptor (HBA) (F4) and Rotatable Bond number (F5) all
played important roles in drug sensitivity. This result aligns well with biological knowledge  as all
these four features are very important descriptors for cellular entry and retention.

Figure 3: HeatMap of G for MovieLens

Figure 4: HeatMap of sign(G) log(|G|) for NCI-
DREAM for a better illustration

6 Conclusion
In this paper  we have proposed a novel sparse inductive model that utilizes side features describing
the row and column entities of a partially observed matrix to predict its missing entries. This method
models the linear predictive power of side features as well as interaction between the features of row
and column entities. Theoretical analysis shows that this model has advantages of reduced sample
complexity over classical matrix completion methods  requiring only O(log N ) observed entries to
achieve a perfect recovery of the original matrix when the side features reﬂect the true latent fea-
ture space of the matrix. When the side features are less informative  our model requires O(log N )
observations for an -recovery of the matrix. Unlike early methods that use a BCD algorithm  we
have developed a LADMM algorithm to optimize the proposed formulation. Given the optimization
problem is convex  this algorithm can converge to a global solution. Computational results demon-
strate the superior performance of this method over three recent methods. Future work includes
the examination of other types and quality of side information and the understanding of whether our
method will beneﬁt a variety of relevant problems  such as multi-label learning  and semi-supervised
clustering etc.
Acknowledgments
Jinbo Bi and her students Jin Lu  Guannan Liang and Jiangwen Sun were supported by NSF grants
IIS-1320586  DBI-1356655  and CCF-1514357 and NIH R01DA037349.

8

References
[1] J. Abernethy  F. Bach  T. Evgeniou  and J.-P. Vert. A new approach to collaborative ﬁltering: Operator
estimation with spectral regularization. The Journal of Machine Learning Research  10:803–826  2009.
[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM journal on imaging sciences  2(1):183–202  2009.

[3] J.-F. Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

[4] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Computa-

J. on Optimization  20(4):1956–1982  Mar. 2010.

tional mathematics  9(6):717–772  2009.

[5] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. Information

Theory  IEEE Transactions on  56(5):2053–2080  2010.

[6] P. Chen and D. Suter. Recovering the missing components in a large noisy low-rank matrix: Application

to sfm. IEEE Trans. Pattern Anal. Mach. Intell.  26(8):1051–1063  Aug. 2004.

[7] T. Chen  W. Zhang  Q. Lu  K. Chen  Z. Zheng  and Y. Yu. Svdfeature: a toolkit for feature-based collab-

orative ﬁltering. The Journal of Machine Learning Research  13(1):3619–3622  2012.

[8] K.-Y. Chiang  C.-J. Hsieh  and E. I. S. Dhillon. Robust principal component analysis with side infor-
mation. In Proceedings of The 33rd International Conference on Machine Learning  pages 2291–2299 
2016.

[9] K.-Y. Chiang  C.-J. Hsieh  and I. S. Dhillon. Matrix completion with noisy side information. Advances in

Neural Information Processing Systems 28  pages 3429–3437  2015.

[10] A. Daemen  O. L. Grifﬁth  L. M. Heiser  N. J. Wang  O. M. Enache  Z. Sanborn  F. Pepin  S. Durinck  J. E.
Korkola  M. Grifﬁth  et al. Modeling precision treatment of breast cancer. Genome Biol  14(10):R110 
2013.

[11] E. X. Fang  B. He  H. Liu  and X. Yuan. Generalized alternating direction method of multipliers: new

theoretical insights and applications. Mathematical Programming Computation  7(2):149–187  2015.

[12] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell.

Syst.  5(4):19:1–19:19  Dec. 2015.

[13] P. Jain and I. S. Dhillon. Provable inductive matrix completion. arXiv preprint arXiv:1306.0626  2013.
[14] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. Information Theory  IEEE

Transactions on  56(6):2980–2998  June 2010.

[15] Z. Lin  M. Chen  and Y. Ma. The Augmented Lagrange Multiplier Method for Exact Recovery of Cor-

rupted Low-Rank Matrices. Mathematical Programming  2010.

[16] G. Liu and P. Li. Low-rank matrix completion in the presence of high coherence. IEEE Transactions on

Signal Processing  64(21):5623–5633  Nov 2016.

[17] A. K. Menon  K.-P. Chitrapura  S. Garg  D. Agarwal  and N. Kota. Response prediction using col-
In Proceedings of the 17th ACM SIGKDD

laborative ﬁltering with hierarchies and side-information.
international conference on Knowledge discovery and data mining  pages 141–149. ACM  2011.

[18] N. Natarajan and I. S. Dhillon.

Inductive matrix completion for predicting gene–disease associations.

Bioinformatics  30(12):i60–i68  2014.

[19] X. Ning and G. Karypis. Sparse linear methods with side information for top-n recommendations. In
Proceedings of the Sixth ACM Conference on Recommender Systems  RecSys ’12  pages 155–162  New
York  NY  USA  2012. ACM.

[20] B. Recht. A simpler approach to matrix completion. The Journal of Machine Learning Research 

12:3413–3430  2011.

[21] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.
In Proceedings of the 22Nd International Conference on Machine Learning  ICML ’05  pages 713–719 
New York  NY  USA  2005. ACM.

[22] O. Shamir and S. Shalev-Shwartz. Matrix completion with the trace norm:

transducing. The Journal of Machine Learning Research  15(1):3401–3423  2014.

learning  bounding  and

[23] W. Shi  Q. Ling  G. Wu  and W. Yin. A proximal gradient algorithm for decentralized composite opti-

mization. IEEE Transactions on Signal Processing  63(22):6013–6023  Nov 2015.

[24] V. Sindhwani  S. Bucak  J. Hu  and A. Mojsilovic. One-class matrix completion with low-density factor-
izations. In Data Mining (ICDM)  2010 IEEE 10th International Conference on  pages 1055–1060  Dec
2010.

[25] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. pages 545–560  2005.
[26] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal

of optimization theory and applications  109(3):475–494  2001.

[27] Z. Weng and X. Wang. Low-rank matrix completion for array signal processing. In Acoustics  Speech and

Signal Processing (ICASSP)  2012 IEEE International Conference on  pages 2697–2700. IEEE  2012.

[28] M. Xu  R. Jin  and Z. hua Zhou. Speedup matrix completion with side information: Application to

multi-label learning. Advances in Neural Information Processing Systems 26  pages 2301–2309  2013.

[29] J. Yang and X.-M. Yuan. Linearized augmented lagrangian and alternating direction methods for nuclear

norm minimization. Math. Comput.  82  2013.

9

,Jin Lu
Guannan Liang
Jiangwen Sun
Jinbo Bi
Patrick McClure
Charles Zheng
Jakub Kaczmarzyk
John Rogers-Lee
Satra Ghosh
Dylan Nielson
Peter Bandettini
Francisco Pereira
Cheng Fu
Huili Chen
Haolan Liu
Xinyun Chen
Yuandong Tian
Farinaz Koushanfar
Jishen Zhao