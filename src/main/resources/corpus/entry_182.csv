2018,A Reduction for Efficient LDA Topic Reconstruction,We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from {\em the same set of topics} but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore  this reduction is approximation preserving  in the sense that approximate distributions-- the only ones we can hope to compute in practice-- are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus  transform it into an approximate distribution for the single-topic world  and run a reconstruction algorithm in the uniform  single topic world-- a much simpler task than direct LDA reconstruction. Indeed  we show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature  $p$-separability and Gibbs sampling for matrix-like topics.,A Reduction for Efﬁcient LDA Topic Reconstruction

Matteo Almanza∗
Sapienza University

Rome  Italy

almanza@di.uniroma1.it

Flavio Chierichetti†
Sapienza University

Rome  Italy

Alessandro Panconesi‡
Sapienza University

Rome  Italy

flavio@di.uniroma1.it

ale@di.uniroma1.it

Andrea Vattani

Spiketrap

San Francisco  CA  USA
avattani@cs.ucsd.edu

Abstract

We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruc-
tion. The main technical idea is to show that the distribution over the documents
generated by LDA can be transformed into a distribution for a much simpler genera-
tive model in which documents are generated from the same set of topics but have a
much simpler structure: documents are single topic and topics are chosen uniformly
at random. Furthermore  this reduction is approximation preserving  in the sense
that approximate distributions — the only ones we can hope to compute in practice
— are mapped into approximate distribution in the simpliﬁed world. This opens
up the possibility of efﬁciently reconstructing LDA topics in a roundabout way.
Compute an approximate document distribution from the given corpus  transform it
into an approximate distribution for the single-topic world  and run a reconstruction
algorithm in the uniform  single-topic world — a much simpler task than direct
LDA reconstruction. We show the viability of the approach by giving very simple
algorithms for a generalization of two notable cases that have been studied in the
literature  p-separability and matrix-like topics.

1

Introduction

Latent Dirichlet Allocation (henceforth LDA) is a well-known paradigm for topic reconstruction (Blei
et al.   2003). The general goal of topic reconstruction is  given a corpus of documents  to reconstruct
the topics. LDA is a generative model according to which documents are generated from a given set
of unknown topics  where each topic is modelled as a probability distribution over the words. One of
the main motivations behind LDA is to allow documents to be able to talk about about multiple topics 
a goal achieved by the following mechanism. To generate a document containing (cid:96) words we ﬁrst
select a probability distribution  the so-called admixture  over the topics. The admixture is randomly
drawn from a Dirichlet distribution  hence the name. Then  the words of the document are selected
one after the other in sequence by ﬁrst selecting a topic at random according to the admixture  and
then by randomly selecting a word according to the selected topic (which  as remarked  is just a
∗Supported in part by the ERC Starting Grant DMAP 680153  and by the “Dipartimenti di Eccellenza
†Supported in part by the ERC Starting Grant DMAP 680153  by a Google Focused Research Award  and by
‡Supported in part by the ERC Starting Grant DMAP 680153  by a Google Focused Research Award  by the
“Dipartimenti di Eccellenza 2018-2022” grant awarded to the Dipartimento di Informatica at Sapienza  and by
BiCi – Bertinoro international Center for informatics.

the “Dipartimenti di Eccellenza 2018-2022” grant awarded to the Dipartimento di Informatica at Sapienza.

2018-2022” grant awarded to the Dipartimento di Informatica at Sapienza.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

probability distribution over the words). In this way all topics contribute to generate a document– to
a degree speciﬁed for each document by a random admixture. To generate another document  another
admixture is selected at random  and the same process is repeated. And so on  so forth.
In this paper we are interested in the problem of LDA topic identiﬁability which  roughly speaking 
can be stated as follows: given a corpus of documents generated by the mechanism just described 
reconstruct as efﬁciently and accurately as possible the K unknown topics (in the paper K will
always denote the number of topics). LDA is actually more general than a mechanism for generating
corpora of text documents  but it helps the intuition to consider it as a generative framework for text
documents and we will stick to this scenario.
This paradigm has attracted a lot of interest  e.g.  (Hong & Davison  2010; Weng et al.   2010; Zhao
et al.   2011; Yan et al.   2013; Sridhar  2015; Alvarez-Melis & Saveski  2016; Li et al.   2016;
Hajjem & Latiri  2017). Several algorithms for LDA topic reconstruction have been proposed (see 
for instance  (Arora et al.   2012  2013; Anandkumar et al.   2013; Bansal et al.   2014)). In this
paper we continue this line of research by presenting a novel approach  the main thrust of which
is  loosely speaking  that of reducing the problem of topic identiﬁability in the LDA framework to
the problem of topic identiﬁability under a much more constrained and simpler generative model.
The simpliﬁed generative mechanism we have in mind is the following. The admixture  instead of
being randomly selected anew for each document from a Dirichlet distribution  will stay put: when
generating a document  a topic is selected uniformly at random with probability 1/K. The second
feature of the simpliﬁed framework is that documents are single topic  i.e. once a topic is selected  all
the words in the document are chosen according to the distribution speciﬁed by that topic. We shall
refer to this mechanism as single topic allocation  denoted as STA. In a nutshell  the contribution of
this paper is to show that if we have an efﬁcient and accurate algorithm for STA topic identiﬁability
— a task seemingly much less daunting than its LDA counterpart — we can use it for efﬁcient and
accurate reconstruction of topics under the LDA paradigm. More precisely  we can do this in the
case of uniform LDA  i.e. when the admixtures come from a symmetric Dirichlet distribution with a
given parameter α  which is a very important and commonly adopted special case (Blei et al.   2003).
Historically  STA-type models have been considered before the advent of LDA (see  e.g.  (Nigam
et al.   2000))  whose main motivation  as mentioned  was precisely that of allowing documents
to be mixtures of topics. In a way  our result vindicates STA in the sense that it shows that LDA
reconstruction is not more general than STA reconstruction.
The main technical tool to achieve this is a reduction between the two paradigms  STA and uniform
LDA. Given a set T of K topics and a Dirichlet parameter α  let D = D(cid:96) be the distribution that they
induce via LDA over the documents of a given length (cid:96). Similarly  let S = S(cid:96) denote the distribution
induced by STA over the documents of the same length (cid:96) when the same set of topics T is used. In a
companion paper (Chierichetti et al.   2018)  we show that there is a reduction such that S can be
computed from D and α  and viceversa. In that paper this fact is used to derive impossibility results
about LDA topic reconstruction whose gist is the following: unless the length of the documents is
greater than or equal to the number of topics  identifying them is impossible. Here  we show how to
exploit this reduction in the opposite direction: if we have an efﬁcient algorithm for identifying the
topics under STA then  thanks to the reduction  we can also use it to identify them under LDA.
Note that the above reduction deals with the exact probability distributions D(cid:96) and S(cid:96) over the
documents  something which is helpful when impossibility results are concerned  but that becomes
an issue if we are seeking reconstruction algorithms that have to be deployed in practice  and which
have a limited number of documents to analyze. A ﬁrst contribution of this paper is to show a robust

version of the above reduction. Fix a set of topics T   and suppose to have an approximation (cid:101)D(cid:96) of the
true distribution D(cid:96) induced by LDA when T is the set of topics. In practice  (cid:101)D(cid:96) can be obtained
of the reduction  on input (cid:101)D(cid:96) and α  produces a distribution (cid:101)S(cid:96) which is a good approximation of S(cid:96) 
LDA from a set of hidden topics T that we wish to reconstruct  compute (cid:101)D  an approximation of the
true document distribution D. Apply the robust version of the reduction to (cid:101)D and α (the Dirichlet

from a large enough corpus of documents in a rather straightforward manner. Suppose also  as it is
customarily assumed in practice  to know the value of the Dirichlet parameter α. The robust version
the true distribution induced by STA when T is the set of topics.
This result suggests an intriguing possibility  namely that LDA topics could be identiﬁed in a rather
roundabout way by means of the following pipeline. Starting from a document corpus generated by

2

of the true distribution S induced by STA from the same set of topics T . Suppose now to have an

parameter which  as remarked  is assumed to be known in practice) to obtain (cid:101)S  an approximation
efﬁcient algorithm that  given (cid:101)S  outputs T (cid:48)  a good approximation of the set T of the unknown
An algorithm capable of producing such a good approximation T (cid:48) from (cid:101)S is called robust in this

topics we are looking for. With such an algorithm we can solve LDA identiﬁability via single-topic
distributions!

paper. As hinted at by the above discussion  the second contribution of this paper is to show that the
pipeline just described can be made to work. We provide a robust algorithm with provable guarantees
with which we can solve in one stroke a natural generalization of two notable cases that have been
studied in the literature. The ﬁrst concerns so-called separable topics (Arora et al.   2012  2013). A
set of topics is p-separable if  for each topic T there is a word w such that T assigns probability at
least p to w and every other topic assigns it probability zero. These special words are called anchor
words. Thus  separability occurs when each topic is essentially identiﬁed uniquely by its anchor
word. This set up has received considerable attention and several algorithms for LDA reconstruction
have been proposed. One of the virtues of the p-separability assumption is that it makes it possible to
derive algorithms with provable guarantees. For instance  the main result of Arora et al. (2012) states
that there is an algorithm such that if a set of LDA topics are p-separable then they are identiﬁable
within additive error δ in the (cid:96)∞-norm  provided that the corpus contains

(cid:18) K 6

Θ

δ2p6γ2(cid:96)

(cid:19)

· log m

(1)

many documents  or more. In the expression  m is the size of the vocabulary  (cid:96) is the length of the
documents and γ is the condition number of the topic-topic covariance matrix. As remarked by the
same authors however  this algorithm is computationally impractical. A follow-up paper shows how
to mitigate the problem by implementing the main steps in a different way (Arora et al.   2013). The
resulting algorithm is much more efﬁcient but  unfortunately  heuristic in nature  thus losing one of
the nice features of its computationally more expensive predecessor.
The second scenario we tackle is that of Grifﬁths & Steyvers (2004) in which Gibbs sampling is
proposed as a heuristic without any performance guarantees for LDA topic reconstruction. In that
paper  Gibbs sampling is applied to a dataset whose underlying set of topics is assumed to have the
following structure. The vocabulary consists of a n × n matrix — each entry is a word (the authors of
Grifﬁths & Steyvers (2004) consider 5 × 5 matrices  i.e. 25 words in total). There are 2n topics  each
corresponding to a row or a column of the matrix. The topic corresponding to a given row has all
zero entries except for that row  whose entries are uniformly 1/n. Topics corresponding to columns
are deﬁned analogously. Note that this set of topics is not p-separable  since every word has positive
probability in at least two topics (its row  and its column).
Both scenarios can be captured at once with the following natural deﬁnition. A set T of topics is
(p  t)-separable if  for every topic T ∈ T   there is a set of words ST of t words such that (i) the
product of the probabilities assigned by T to the words of ST is at least p  and moreover (ii) for every
other topic T (cid:48) ∈ T − {T} there exists a word w ∈ ST such that T (cid:48) assigns probability zero to w. It
can be checked that p-separability is (p  1)-separability and that the matrix scenario is (p  2)-separable
(with p = n−2 for n × n matrices  n ≥ 2). In practice  (p  t)-separability captures the notion that
every topic is uniquely identiﬁed by a set of t words. We shall refer to these sets as anchor sets. With
this terminology  p-separability is just (p  1)-separability with singleton anchor sets.
In this paper we give an algorithm for LDA topic reconstruction (under (p  1)-separability) that 
starting from a random LDA corpus over a vocabulary of m words consisting of

(cid:32)

K 2 · max(cid:0)1  K 2α2(cid:1)

Θ

δ2 · p2

(cid:33)

· log m

many documents of (at least) 2 words each  computes a set of topics T (cid:48) which is an approximation of
let parameter α is typically assumed to be O(1/K)  in which case the term max(cid:0)1  K 2α2(cid:1) resolves
the true set of topics T with error δ (in (cid:96)∞-norm)4. Asymptotically  this compares favourably to the
(cid:16) K2
(cid:17)
bound of Equation (1) but it is also the case that the algorithm is very simple and efﬁcient. The Dirich-
δ2·p2 · log m

to a constant  and the number of documents required for reconstruction becomes Θ

.

4More precisely  there exists a bijection φ : T → T (cid:48) such that  for each T ∈ T   |T − φ(T )|∞ ≤ δ

3

Note that the Dirichlet distribution is such that  as α goes to zero  the admixture becomes more and
more polarized  in the sense that the documents resemble more and more single-topic documents 
which intuitively facilitates topic reconstruction. When α moves in the other direction toward larger
and larger values  the admixture creates documents in which all topics are equally represented  which
makes reconstruction more expensive in the sense that the size of corpora must become bigger and
bigger. These considerations apply to all algorithms  but we note that our dependence on K and p is
much milder than those of the other algorithms we are discussing.
It is interesting to compare the overall structure of our algorithm to that in Arora et al. (2012). The
ﬁrst step of the latter is to project points into a low-dimensional space  where computation is cheaper 
by preserving distances. In a very loose sense  this is equivalent to our reduction  which transforms
the distribution of documents of length 2 from LDA to STA. The second step is a very natural one:
try to identify the anchor words  using a simple combinatorial procedure (or  more generally  the
t-anchor sets  starting from documents of length t + 1). The third step is again very natural: use the
anchors to build the topics. It is here that the full advantage of our approach becomes evident. Our
algorithm attempts the reconstruction in the single topic world — a much less daunting prospect than
reconstruction in the full-ﬂedged LDA world. As a result  our third step is a very simple procedure —
in the LDA world one would have had to pay the price of heavy-duty linear algebra computations.
In order to deal with (p  t)-separable topics the algorithm only needs documents of length t + 1.
Therefore  in order to reconstruct p-separable topics (t = 1) it only needs bigrams  and in the matrix
case (t = 2) only trigrams! Clearly  this has a signiﬁcant positive impact on efﬁciency.
We also present a comparative experimental evaluations which shows that our approach compares
favorably to those of (Arora et al.   2012  2013; Grifﬁths & Steyvers  2004; Anandkumar et al.  
2014).
The paper is organized as follows. We start in § 2 with some quick preliminaries. In § 3 we give the
reduction from LDA to STA  followed by § 4 in which a robust algorithm for STA topic reconstruction
is presented with which we solve the (p  t)-separable case for t = 1  2  which subsumes both p-
separability and matrix-like topics. § 5 presents our experiments. The proofs missing from the main
body of the paper can be found in the Supplementary Material archive.

2 Preliminaries
Throughout the paper  we will use V to denote the underlying vocabulary and assume without loss of
generality that m := |V| ≥ 2  since the case m = 1 is trivial (there can be only one topic).
We will only deal with LDA when the admixtures come from a symmetric Dirichlet distribution
whose parameter will be denoted by α. Since this is the only case we consider and there is no danger
of confusion  we will sometimes omit to specify that we are dealing with symmetric LDA.
We will use the following notation. Given a set of K topics T and a Dirichlet parameter α  DT
(cid:96) will
denote the distribution induced by LDA over the topics of length (cid:96). When there is no danger for
confusion subscripts and superscripts will be dropped. Similarly  ST
(cid:96) will refer to the distribution
induced by STA over the topics of length (cid:96). And  likewise  subscripts and superscripts will be dropped
when no danger for confusion may arise.

3 A Reduction from LDA to STA

In this section we give the approximation preserving reduction from LDA to STA. As usual  in the
background we have a set of unknown topics T that we wish to reconstruct. The reduction takes as

input the Dirichlet parameter α  an approximation (cid:101)D of the document distribution D generated by
LDA with topics T   and gives as output an approximation (cid:101)S of the document distribution S generated

by STA with the same set of topics T . The point of departure is a reduction between the two true
distributions D and S established by (Chierichetti et al.   2018  Section 4).
Deﬁnition 1. Given a permutation π ∈ Sym([(cid:96)])  let Cπ be the partition of [(cid:96)] into the cycles of π:

Cπ = {S | S ⊆ [(cid:96)] and the elements of S form a cycle in π} .

4

Furthermore  for d ∈ V (cid:96) and S = {i1  i2  . . .   i|S|} ⊆ [(cid:96)] with i1 < i2 < . . . < i|S|  let d|S be the
document containing the words d(i1)  . . .   d(i|S|) in this order (that is  let it be the document that is
obtained by removing from d the words whose positions in d are not in S).
For example  if π = (163)(25)(4) then Cπ = {{1  3  6} {2  5} {4}}. And  if d = w1w2w3w4w5w6
and S = {1  3  6} then d|S = w1w3w6.
Theorem 2 (Reduction from LDA to STA (Chierichetti et al.   2018)). Let T be any set of K topics
on a vocabulary V and consider any d ∈ V (cid:96). Then  for any α > 0 
ST
(cid:96) (d) =

· (cid:88)

Γ(K · α + (cid:96))

K · α · ST

(cid:89)

·DT  α

(d)−

|S|(d|S)

. (2)

(cid:16)

(cid:17)

1

(cid:96)

Γ(K · α + 1) · Γ((cid:96))

K · α · Γ((cid:96))

π∈Sym([(cid:96)])

|Cπ|≥2

S∈Cπ

Equation (2) looks rather formidable  but the point is that it can be taken as a blackbox to transform
one probability distribution into the other. Note that the equation is recursive — it speciﬁes how
to compute the STA distribution S(cid:96) over documents of length (cid:96)  from D(cid:96) and the STA distributions
S1  . . .  S(cid:96)−1 over documents of length less than (cid:96). In the base case — documents of length one —
the two distributions D1 and S1 coincide and thus the induction can be kick-started.

The next lemma tells us how to compute a good approximation (cid:101)D of the true document distribution

D induced by LDA starting from a corpus.
Lemma 3 (LDA Probabilities Approximation). Fix (cid:96) ≥ 1  and ξ ∈ (0  1). Let X1  . . .   Xn be n iid
samples from DT  α
. For i ∈ [(cid:96)]  and for a document d ∈ [m]i  let nd be the number of samples having
d as a preﬁx  nd = |{j|j ∈ [n] ∧ d is a preﬁx of Xj}|. For i ∈ [(cid:96)]  and for a document d ∈ [m]i  let

(cid:96)

n be the empirical fraction of the samples whose i-preﬁx is equal to d. Then 

  with probability at least 1− O(m−(cid:96))  for every document d of length

(cid:101)Di(d) = nd
(a) If n ≥(cid:108) 2
(b) For any q > 0  if n ≥(cid:108) 9

(cid:109)
ξ2 · (cid:96) · ln m

i ≤ (cid:96)  it holds that |DT  α

i

(d) − (cid:101)Di(d)| ≤ ξ.
(cid:109)
q·ξ2 · (cid:96) · ln m

document d of length i ≤ (cid:96) such that DT  α

  with probability at least 1 − O(m−(cid:96))  for every
(d).

(d) ≥ q  it holds that (cid:101)Di(d) = (1 ± ξ)DT  α

i

i

i

(cid:101)Di(d) of DT  α
(cid:101)S1(w(cid:48)). Then 

The next theorem establishes our main result of this section  namely that Equation (2) is approximation
preserving.
Theorem 4 (Single-Topic Probabilities Approximation). Fix ξ ∈ (0  1). Given an approximation

(d)  i ∈ {1  2}  deﬁne (cid:101)S1 = (cid:101)D1  and (cid:101)S2(ww(cid:48)) = (Kα + 1)·(cid:101)D2(ww(cid:48))− Kα·(cid:101)S1(w)·
i (d) − (cid:101)Si(d)| ≤ ξ.
(cid:17)DT  α

(b) If  for a given word w  it holds (cid:101)D1(w) =
(cid:16)

(d) − (cid:101)Di(d)| ≤
(cid:17)DT  α

(ww)  then (cid:101)S2(ww) = (1 ± ξ)ST

(a) If for every document d of length i ≤ 2 it holds |DT  α

(w) and (cid:101)D2(ww) =

4(Kα+1)   then

4Kα+1
2 (ww).

1 ± ξ

1 ± ξ

|ST

(cid:16)

1

ξ

i

2

4Kα+1

4 Robust Algorithms for STA Topic Identiﬁability

In this section we give an algorithm for identifying p-separable topics (or  equivalently  (p  1)-
separable topics). As usual  we have a set T of topics in the background that we wish to identify.
The ﬁrst step is to identify anchor words or their proxies. By proxy  or quasi-anchor word  we mean
that the word has “large” probability in one topic and very small probabilities in the remaining ones.
We begin with a technical lemma stating that if a vector has a coordinate that is very large with
respect to the others  then all of its (cid:96)p-norms are close to one another. Loosely speaking  the lemma
says that if a word is an anchor word or a quasi-anchor word then  if we look at the vector consisting
of the probabilities assigned to this word by the topics  the (cid:96)p-norms of the vector are close.

5

2 (ww)/K ST

1 ≤ |v|p

p ≤ (1 − )p−1 · |v|p
1.

Lemma 5. Let v ∈ Rn  and suppose that |v|∞ = (1 − ) · |v|1  for some  ∈ [0  1). Then  for each
p ≥ 1  (1 − )p · |v|p
The next theorem tells us how to spot anchor words. The idea is that if a word w is an anchor
word then there is a signal telling us so. Consider the two documents w and ww. The signal is
the ratio ST
two good approximations (cid:101)S1(w) and (cid:101)S2(ww) of  respectively  ST
1 (w)2. If w is an anchor word this ratio equals 1  and if w is “far” from being
an anchor word then the ratio is bounded below 1. In fact  the theorem tells us more. If we have
ρw = (cid:101)S2(ww)/K (cid:101)S1(w)2 will have (approximately) the same properties. Since we are dealing with an
2 (ww) then the ratio
approximation of the true distribution S  this tells us that we will be able to spot anchors even in this
case.
Now  ﬁx a word w of the dictionary let xw be the (unknown) vector of its probabilities in the K
topics  so that ST

Theorem 6. Let ξ ∈ (0  1) and w ∈ V be any word. Suppose that (cid:101)S1(w) = (1 ± ξ)ST
(cid:101)S2(ww) = (1 ± ξ)ST

2 (ww) = K−1 · |xw|2
2.
2 (ww). Deﬁne ρw = (cid:101)S2(ww)
K ((cid:101)S1(w))2 .

1 (w) = K−1 · |xw|1 and ST

1 (w) and ST

1 (w) and

(1−ξ)2

.

(1+ξ)2

2 (ww).

≤ ρw ≤ (1−w)(1+ξ)

(1+ξ)2 . Moreover  if ρw ≥ 1−ξ

1 (w) and (cid:101)S2(ww) = (1 ± ξ)ST

Then  if w is such that |xw|∞ = (1 − w) · |xw|1  it holds (1−w)2(1−ξ)
Consider the quantity ρw deﬁned by the previous theorem and suppose that ρw ≥ 1−ξ/(1+ξ)2. The
next lemma says that if w is an anchor word  then ρw satisﬁes the inequality. And  viceversa  if ρw
satisﬁes it  then w must be either an anchor word or a quasi-anchor word  which can also be used for
topic reconstruction.

K ((cid:101)S1(w))2   and w be such that |xw|∞ = (1 − w) · |xw|1.
(1+ξ)2 then w ≤ 6ξ.

Lemma 7. Let ξ ∈ (0  1). Suppose that (cid:101)S1(w) = (1 ± ξ)ST
Let ρw = (cid:101)S2(ww)
If w = 0 then ρw ≥ 1−ξ
The previous lemma gives us a simple test to identify anchor words or quasi-anchor words. We
know that each anchor word is uniquely associated with one topic — the one that assigns to it non
zero probability. We will see later that ξ can be chosen in a way that quasi-anchor words too can be
associated with one topic — the one assigning it a much larger probability than the other topics. The
next lemma tells us how to determine whether two different words insist on the same topic.
We say that a topic j is dominant for a word w  if (i) w has a unique largest probability in the topics 
and (ii) its largest probability is in topic j. We say that the words w  w(cid:48) are codominated  if there
exists a topic j such that j is dominant for both w and w(cid:48).
1 (w)  and that |xw|∞ = (1 −

Theorem 8. For w ∈ {w1  w2}  suppose that (cid:101)D1(w) = (1 ± ξ)DT
w) · |xw|1. Suppose further that (cid:101)D2(w1w2) = (1 ± ξ)DT
Deﬁne τ (w1  w2) := (cid:101)D2(w1w2)
(cid:101)D1(w1)·(cid:101)D1(w2)
τ (w1  w2) ≥ (1 − ξ)

. If the words w1 and w2 are co-dominated  then

(1 + ξ)2 · Kα + K(1 − 1)(1 − 2)

2 (w1w2).

Kα + 1

 

otherwise

τ (w1  w2) ≤ (1 + ξ)
(1 − ξ)2

Kα + K(w1 + w2 + w1 w2 )

Kα + 1

.

1−4

α+1   where  = maxw∈A w. Suppose that (cid:101)D1(w) = (1 ± ξ)DT

The next corollary gives a simple way to determine which quasi-anchor words belong to the same
topic.
Corollary 9. Let A  |A| > K  be a set of quasi-anchor words w with |xw|∞ = (1 − w) · |xw|1. Let
1 (w) for w ∈ A  and let E be
ξ < 1
6

the maximal subset of(cid:0)A
(cid:1) such that (cid:101)D2(w1w2) = (1 ± ξ)DT
If E contains all the co-dominated pairs of words  the correct partitioning of A according to the K
topics T can be obtained by iteratively assigning to the same group the pair of words {w1  w2} ∈ E
with largest τ (w1  w2) := (cid:101)D2(w1w2)
(cid:101)D1(w1)·(cid:101)D1(w2)

2 (w1w2) for each {w1  w2} ∈ E.

until reaching K groups.

2

6

We then have the main theorem of this section which gives the full algorithm for topic reconstruction
in the p-separable (equivalent to the (p  1)-separable) case.
Theorem 10 (Main Result). Suppose that T is a set of K = |T | topics  and let δ ≤ 1/48. There
exists an algorithm that  under the p-separability assumption  and under the LDA model DT  α  with
probability 1 − o(1) reconstructs each topic in T to within an (cid:96)∞ additive error upper bounded by δ 
. The algorithm runs in O(n).
by accessing n = Θ

(cid:18) K2·max((Kα)2 1)

· ln m

(cid:19)

δ2p2

iid samples from DT  α

2

Algorithm 1 is a version of the method analyzed in Theorem 10. The most notable feature of our
algorithm is its simplicity.

Algorithm 1 The Algorithm for reconstructing (p  1)-separable topics.
Require: K  p > 0  δ  corpus C of documents  α parameter of the symmetric LDA mixture 
1: Let W be the set of words w whose empirical fraction in C is at least p/2K.
approximations (cid:101)D1 and (cid:101)D2 of D1 and D2 .
2: For each w  w(cid:48) ∈ W   estimate the empirical fraction of the document ww(cid:48) in C — that is  obtain
3: Apply the reduction of Theorem 2 to estimate the uniform single-topic probabilities (cid:101)S1(w) and (cid:101)S2(ww(cid:48)).
4: For each w ∈ W   compute ρw := (cid:101)S2(ww)
K ((cid:101)S1(w))2 and add w to the set A of quasi-anchors if ρw ≥ 1−δ
(1+δ)2 .
6: For each wi  return a topic whose probability on word w ∈ V is (cid:101)S2(wiw)/(cid:101)S1(wi).

5: Use Corollary 9 on A to obtain K pairwise non-codominated quasi-anchor words w1  w2  . . .   wK.

4.1 The general (p  t)-separable case

The algorithm we have developed in the previous section can be generalized to work for (p  t)-
separable topics (this is what we need to deal with the topic structure of Grifﬁths & Steyvers (2004)).
The generalization is quite straightforward and is a natural extension of Algorithm 1 but  for lack of
space  we defer it to the full paper. We will however compare our generalized algorithm to Gibbs
sampling — the method used by Grifﬁths & Steyvers (2004) — in the next section.

5 Experimental Results

We compare our approach5 to three state-of-the-art algorithms: GIBBS sampling6  a popular heuristic
approach  the algorithm from (Arora et al.   2013) for p-separable instances  referred to as RECOVER
from now on  and the implementation of Yau (2018) of the tensor-based algorithm (henceforth
TENSOR) introduced in (Anandkumar et al.   2014). Each of these algorithms was executed on the
same computer: an Intel Xeon CPU E5-2650 v4  2.20GHz  with 64GiB of DDR4 RAM. We used a
single core per algorithm.
The topics. For the experiments we generated a family of k topics in various ways  for k = 10  25  50.
The family NIPS TOPICS was generated by running Gibbs sampling on the NIPS dataset (Newman 
2008). Since these topics are not p-separable in general  a second family was generated by adding
anchor words artiﬁcially. A third family  SYNTHETIC  was generated by sampling from a uniform
Dirichlet distribution with parameter β = 1 and  to enforce p-separability  anchor words were added.
Finally  a fourth family of topics were GRID topics. These are the prototypical grid-like topics of sizes
7 × 7 and 5 × 5 (introduced by Grifﬁths & Steyvers (2004)); notice that these are (p  2)-separable but
not (p  1)-separable.
In each instance except grid topics  the number of words of the vocabulary was set to m = 400.
The corpora. From each one of the set of topics speciﬁed above  we generated a corpus of n
documents of length (cid:96)  for n = 104  105  106 and (cid:96) = 2  3  10  100. Because of space constraints  we
will only show results for n = 106.

5Our implementation can be downloaded from https://github.com/matteojug/lda-sta.
6We use the popular MALLET library McCallum (2002)  http://mallet.cs.umass.edu/  with a 200

iteration burnin period and 1000 iterations.

7

We are interested in two aspects of performance  the wall-clock running time and quality of the
reconstruction  measured as the (cid:96)∞ norm between the true set of topics and the reconstruction. To
assess this  we computed the best possible matching between the two families of topics as follows.
Consider a bipartite graph with the true set of topics on one side of the bipartition and the reconstructed
topics on the other. Between every pair of topics on opposite sides  there is an edge of weight equal
to their (cid:96)∞ distance. The quality of the reconstruction is given by the minimum cost perfect matching
in this graph. All algorithms were run on a single thread.

Figure 1: The top-left plot shows the wall clock time (in seconds  on a log-scale) required by the
algorithms with NIPS TOPICS  with documents of length (cid:96) = 2 and 10 topics (TENSOR is not shown
since it requires (cid:96) > 2). The top-right plot shows (on a linear-scale) the (cid:96)∞ error of the algorithms on
the same instance; observe that STA is faster than the other two algorithms by more than one order of
magnitude  while its error is almost as good as that of RECOVER. The bottom-left plot shows the
wall clock time (in seconds  on a log-scale) required by the algorithms with 10 SYNTHETIC topics 
with documents of length (cid:96) = 3. As before  STA is faster than the other algorithms by more than one
order of magnitude and its error is almost as good as the one of RECOVER.

Conceptually our algorithm implements the following pipeline  C (1)−→ L (2)−→ S (3)−→ T   where the
ﬁrst step  starting from the corpus C  computes the approximation to the distribution induced over the
documents by LDA; the second step implements the reduction from the latter to the STA-induced
distribution  and  lastly  the third step is Algorithm 1. We implemented the steps of this pipeline with
several optimizations. In particular  we did not fully compute the approximate distributions L and S:
rather  we lazily computed their entries that were requested by Algorithm 1.
Algorithm 1 simply picks the ﬁrst two words of a document and throws the rest away  seemingly a
rather wasteful thing to do. A natural alternative is to feed the algorithm we all pairs of words from
the document  hoping that the correlations so introduced can be safely ignored. This variant  which
we call STA in the following  was consistently more accurate than Algorithm 1 at the expense of a
small increase in the running time. Therefore this is the implementation that we discuss.
In the case of grid-like topics  STA is the version of Algorithm 1 for the (p  2)-separable case.
Wall-clock time. The two plots on the left of Figure 1 compare the running times of the algorithms
with corpora of documents of length (cid:96) = 2  3  with 10 topics. As expected  STA for documents of

8

n=106 =2 100101102Time (s)n=106 =2 0.000.010.020.030.040.050.060.070.08L errorSTAGibbsRecovern=106 =3 100101102Time (s)n=106 =3 0.0000.0050.0100.0150.0200.0250.0300.035L errorSTAGibbsRecoverTensorlength 2 is much faster then the other algorithms (while GIBBS is especially cumbersome)  and its
reconstruction quality is close to the best one. This ﬁgure exempliﬁes the general picture: a similar
outcome was observed for all values of n topic families.

Figure 2: On the left: wall clock time (in seconds  on a log-scale) required by the algorithms with 10
SYNTHETIC topics  with documents of length (cid:96) = 100. On the right: the (cid:96)∞ error (on a linear-scale)
of the algorithms on the same instance.

Precision of the reconstruction. Figure 1 exempliﬁes the general picture that emerges from our tests 
for short documents and 1-separable topics. RECOVER and STA have the smallest reconstruction
errors. As expected  GIBBS did not work well with very short documents. Therefore we tested
the algorithms with documents of length (cid:96) = 100. In Figure 2  we show that STA gives the best
reconstruction  and its the fastest one by at least one order of magnitude.

Figure 3: On the left: wall clock time (in seconds  on a log-scale) required by the algorithms on
a 5 × 5 GRID with 10 topics  with documents of length (cid:96) = 10. On the right: the (cid:96)∞ error (on
a linear-scale) of the algorithms on the same instance. Recall that  here  STA is the version of
Algorithm 1 for (p  2)-separability. On this instance  RECOVER is the fastest algorithm; observe 
though  that RECOVER returns topics that are very far from the original ones  since this instance is
not p-separable.
Grid. In a ﬁnal set of experiments  we considered the prototypical GRID instances of sizes 7 × 7 and
5 × 5 (introduced in Grifﬁths & Steyvers (2004)). In Figure 3  we see that STA and GIBBS provide
an (cid:96)∞ error smaller by an order of magnitude than that of RECOVER (and 4 times smaller than that of
Tensor). Moreover  the running time of STA is at least one order of magnitude smaller than that of
GIBBS.
Assessment. A picture emerges from our experiments. STA offers a pretty good reconstruction  while
being extremely competitive in terms of running time. We see this as an encouraging proof of concept
that warrants further investigation of the approach introduced in this paper  that is  reducing LDA-
reconstruction to the much simpler problem of STA-reconstruction. A more careful implementation of
our algorithms could further increase the speed of our approach  while more ideas seem to be needed
to improve the quality of reconstruction. Our experiments show that this could be a worthwhile
endeavor.

9

n=106 =100 102103104Time (s)n=106 =100 0.0000.0020.0040.0060.0080.0100.0120.014L errorSTAGibbsRecoverTensorn=106 =10 102103Time (s)n=106 =10 0.000.050.100.150.20L errorSTAGibbsRecoverTensorReferences
Alvarez-Melis  David  & Saveski  Martin. 2016. Topic Modeling in Twitter: Aggregating Tweets by

Conversations. In: ICWSM.

Anandkumar  Anima  Hsu  Daniel J.  Janzamin  Majid  & Kakade  Sham M. 2013. When are
Overcomplete Topic Models Identiﬁable? Uniqueness of Tensor Tucker Decompositions with
Structured Sparsity. Pages 1986–1994 of: Burges  Christopher J. C.  Bottou  Léon  Ghahramani 
Zoubin  & Weinberger  Kilian Q. (eds)  NIPS.

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel  Kakade  Sham M.  & Telgarsky  Matus. 2014.
Tensor Decompositions for Learning Latent Variable Models. Journal of Machine Learning
Research  15  2773–2832.

Arora  Sanjeev  Ge  Rong  & Moitra  Ankur. 2012. Learning Topic Models – Going Beyond SVD.
Pages 1–10 of: Proceedings of the 2012 IEEE 53rd Annual Symposium on Foundations of Computer
Science. FOCS ’12. Washington  DC  USA: IEEE Computer Society.

Arora  Sanjeev  Ge  Rong  Halpern  Yonatan  Mimno  David M.  Moitra  Ankur  Sontag  David 
Wu  Yichen  & Zhu  Michael. 2013. A Practical Algorithm for Topic Modeling with Provable
Guarantees. Pages 280–288 of: ICML (2). JMLR Workshop and Conference Proceedings  vol. 28.
JMLR.org.

Bansal  Trapit  Bhattacharyya  Chiranjib  & Kannan  Ravindran. 2014. A provable SVD-based
algorithm for learning topics in dominant admixture corpus. Pages 1997–2005 of: Advances in
Neural Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014  December 8-13 2014  Montreal  Quebec  Canada.

Blei  David M.  Ng  Andrew Y.  & Jordan  Michael I. 2003. Latent Dirichlet Allocation. J. Mach.

Learn. Res.  3(Mar.)  993–1022.

Chierichetti  Flavio  Panconesi  Alessandro  & Vattani  Andrea. 2018. The equivalence of Single-

Topic and LDA topic reconstruction. Zenodo  10.5281/zenodo.1470295.

Grifﬁths  T. L.  & Steyvers  M. 2004. Finding scientiﬁc topics. Proceedings of the National Academy

of Sciences  101(Suppl. 1)  5228–5235.

Hajjem  Malek  & Latiri  Chiraz. 2017. Combining IR and LDA Topic Modeling for Filtering
Microblogs. Pages 761–770 of: Zanni-Merk  Cecilia  Frydman  Claudia S.  Toro  Carlos  Hicks 
Yulia  Howlett  Robert J.  & Jain  Lakhmi C. (eds)  KES. Procedia Computer Science  vol. 112.
Elsevier.

Hong  Liangjie  & Davison  Brian D. 2010. Empirical Study of Topic Modeling in Twitter. Pages
80–88 of: Proceedings of the First Workshop on Social Media Analytics. SOMA ’10. New York 
NY  USA: ACM.

Li  Chenliang  Wang  Haoran  Zhang  Zhiqian  Sun  Aixin  & Ma  Zongyang. 2016. Topic Modeling
for Short Texts with Auxiliary Word Embeddings. Pages 165–174 of: Proceedings of the 39th
International ACM SIGIR Conference on Research and Development in Information Retrieval.
SIGIR ’16. New York  NY  USA: ACM.

McCallum  A.K. 2002. A machine learning for language toolkit.

Newman  David. 2008. NIPS Dataset.

Nigam  Kamal  McCallum  Andrew  Thrun  Sebastian  & Mitchell  Tom M. 2000. Text Classiﬁcation

from Labeled and Unlabeled Documents using EM. Machine Learning  39  103–134.

Sridhar  Vivek Kumar Rangarajan. 2015. Unsupervised Topic Modeling for Short Texts Using
Distributed Representations of Words. Pages 192–200 of: Blunsom  Phil  Cohen  Shay B.  Dhillon 
Paramveer S.  & Liang  Percy (eds)  VS@HLT-NAACL. The Association for Computational
Linguistics.

10

Weng  Jianshu  Lim  Ee-Peng  Jiang  Jing  & He  Qi. 2010. TwitterRank: Finding Topic-sensitive
Inﬂuential Twitterers. Pages 261–270 of: Proceedings of the Third ACM International Conference
on Web Search and Data Mining. WSDM ’10. New York  NY  USA: ACM.

Yan  Xiaohui  Guo  Jiafeng  Lan  Yanyan  & Cheng  Xueqi. 2013. A biterm topic model for short

texts. In: WWW.

Yau  Chyi-Kwei. 2018. tensor-lda. https://github.com/chyikwei/tensor-lda.

Zhao  Wayne Xin  Jiang  Jing  Weng  Jianshu  He  Jing  Lim  Ee-Peng  Yan  Hongfei  & Li  Xiaom-
ing. 2011. Comparing Twitter and Traditional Media Using Topic Models. Pages 338–349 of:
Proceedings of the 33rd European Conference on Advances in Information Retrieval. ECIR’11.
Berlin  Heidelberg: Springer-Verlag.

11

,Matteo Almanza
Flavio Chierichetti
Alessandro Panconesi
Andrea Vattani