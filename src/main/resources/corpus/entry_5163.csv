2019,Spectral Modification of Graphs for Improved Spectral Clustering,Spectral clustering algorithms provide approximate solutions to  hard optimization problems that formulate graph partitioning  in terms of the graph conductance. It is well understood that  the quality of these approximate solutions is negatively affected  by a possibly significant gap between the conductance and the second eigenvalue  of the graph.  In this paper we show that for \textbf{any} graph $G$   there exists a `spectral maximizer' graph $H$ which is cut-similar to $G$     but has eigenvalues that are near the theoretical limit  implied by the cut structure of $G$. Applying then spectral clustering  on $H$ has the potential to produce improved  cuts that also exist in $G$ due to  the cut similarity.   This leads to the second contribution of this  work: we describe a practical spectral modification algorithm that   raises the eigenvalues of the input graph  while preserving its  cuts. Combined with spectral clustering on the modified  graph  this yields demonstrably improved cuts.,Spectral Modiﬁcation of Graphs
for Improved Spectral Clustering

Ioannis Koutis

Department of Computer Science
New Jersey Institute of Technology

Newark  NJ 07102
ikoutis@njit.edu

Huong Le

Department of Computer Science
New Jersey Institute of Technology

Newark  NJ 07102
hyl4@njit.edu

Abstract

Spectral clustering algorithms provide approximate solutions to hard optimization
problems that formulate graph partitioning in terms of the graph conductance. It
is well understood that the quality of these approximate solutions is negatively
affected by a possibly signiﬁcant gap between the conductance and the second
eigenvalue of the graph. In this paper we show that for any graph G  there exists
a ‘spectral maximizer’ graph H which is cut-similar to G  but has eigenvalues
that are near the theoretical limit implied by the cut structure of G. Applying then
spectral clustering on H has the potential to produce improved cuts that also exist
in G due to the cut similarity. This leads to the second contribution of this work:
we describe a practical spectral modiﬁcation algorithm that raises the eigenvalues
of the input graph  while preserving its cuts. Combined with spectral clustering on
the modiﬁed graph  this yields demonstrably improved cuts.

1

Introduction

Spectral Clustering is a widely known family of algorithms that use eigenvectors to partition the
vertices of a graph into meaningful clusters. The introduction of spectral partitioning methods goes
back to the work of Donath and Hoffman [8] who used eigenvectors for partitioning logic circuits 
but owes its popularity to the work of Shi and Malik [25] who brought it in the realm of computer
vision and machine learning  subsequently leading to a vast amount of related works. Several other
clustering methods have since emerged  including of course methods based on neural networks. But
spectral clustering remains a frequently used baseline  and a serious contender to state-of-the-art
graph embedding methods  e.g. [20  11  28  22].
The remarkable performance of spectral clustering is possibly due to the fact that it produces outputs
with theoretically understood approximation properties. Roughly speaking  spectral clustering
computes the second eigenvalue λ of the normalized graph Laplacian as an approximation to the
graph conductance  i.e. the value of the optimal cut. Cheeger inequality shows that while λ is never
greater than φ  it can be as small as φ2 [6]. That implies that the approximation can be a factor of
(φ/λ) away from the optimal value  which can be up to O(n) even for unweighted graphs. While
this may be often a pessimistic estimate  there are known families of graphs where the estimate is
realized; in such graphs  spectral clustering computes cuts that are far from optimal [12]. It is thus
understood that the ratio (φ/λ) affects directly the quality of spectral clustering  a fact that is viewed
as an inherent limitation.
This paper shows that this limitation can be greatly alleviated via spectral modiﬁcation: a set of
operations that approximately preserve the cut structure of the input while ‘raising’ its spectrum  in
effect suppressing the ratio (φ/λ) and improving the output.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2 Spectral Modiﬁcation: High-level Overview and Context

This section collects a number of required notions from spectral graph theory and puts spectral
modiﬁcation in perspective with important recent discoveries that inspire it. It also describes the
motivation for our work and gives a high-level overview that may be useful for the reader before we
delve into more technical details.

2.1 Cut and Spectral Similarity

LG(u  v) = −wuv and (ii) LG(u  u) = −(cid:80)

u(cid:54)=v LG(u  v).

Let G = (V  E  w) be a weighted graph. The Laplacian matrix LG of graph G is deﬁned by: (i)
The quadratic form of a semi-positive deﬁnite matrix A is deﬁned by R(A  x) = xT Ax. For a
subset of vertices S ⊆ V   we denote by cutG(S) the total weight of the edges leaving the set S.
Let G and H be two weighted graphs. We say that the two graphs are ρ-cut similar  if there exist
numbers α  β with ρ = α/β  such that for all S ⊂ V   we have α·cutH (S) ≤ cutG(S) ≤ β·cutH (S).
We say that the two graphs are ρ-spectral similar  if there exist numbers α  β with ρ = α/β such that
for all real vectors x  we have α · R(LH   x) ≤ R(LG  x) ≤ β · R(LH   x).
It is well understood that ρ-spectral similarity implies ρ-cut similarity  but not vice-versa [26].

2.2 Low-diameter Cut Approximators and Spectral Maximizers

Let G = (V  E) be the path graph on n vertices  and for the sake of simplicity assume that n is
a power of 2. Let T = (V ∪ I  E) be the full binary tree  where V is the set of leaves being in
one-to-one correspondence with the path vertices as illustrated in Figure 1a  and I is the set of internal
vertices. An interesting feature of T is that it provides a cut-approximator for G  i.e. it contains
information that allows estimating all cuts in G  within a factor of 2. In section 3  we describe how
the cut approximator T gives rise to a weighted complete graph H = (V  E  w) on the original set
of vertices V   via a canonical process of eliminating the internal vertices of T ; ﬁgure 1b provides a
glimpse to the edge weights of H. Graph H is O(1)-cut similar with G  but with a very different
eigenvalue distribution  as illustrated in Figure 1c. More speciﬁcally  the second eigenvalue λ of the
normalized Laplacian of G is Θ(1/n2)  while that of H is Ω(1/(n log n))  essentially closing the
gap with the conductance φ = Θ(1/n). An alternative way of viewing this is that H has a second
eigenvalue which –up to an O(log n) factor– is the maximum possible  since the eigenvalue is always
smaller than φ. In some sense  the same is true for all eigenvalues of H  which leads us to call H a
spectral maximizer of G. These properties of H can be proved using only the logarithmic diameter
of T and the fact that T is a cut-approximator.
These observations set the backdrop for the idea of spectral modiﬁcation  which aims to modify the
input graph G in order to bring it spectrally closer to its maximizer H. It is worth noting that  in some
sense  spectral modiﬁcation is an objective countering that of spectral graph sparsiﬁcation  which
aims to spectrally preserve a graph [2].

(a)

(b)

(c)

Figure 1: (a) The path graph G and the cut-approximating binary tree R. The binary tree is depicted
with weights that are discussed in section 3. (b) Heatmap of the log-entries of the adjacency matrix
for n = 8196 of the spectral maximizer H. It can be seen that H is a dense graph that inherits the
tri-diagonal path structure  but also has other long-range edges. (c) Ratios of the ﬁrst 30 normalized
eigenvalues of H and G  for n = 8196. H has signiﬁcantly larger eigenvalues.

2

2.3 Contributions and Perspective

A key contribution of this paper is the observation that every graph G has a spectral maximizer H.
The path-tree example of the previous section is merely an instantiation of our central claim  but a
vast generalization is possible (with a small loss)  using the fact that all graphs have low-diameter cut
approximators  as shown by Räcke [23]. Technically  this is captured by a Cheeger-like inequality that
we present in Section 3.3. We show that the inequality applies not only for the standard normalized
cuts problem  but also for generalized cut problems that capture semi-supervised clustering problems.
The original result of [23] has undergone several subsequent algorithmic improvements and re-
ﬁnements [3  14  24  19]. It is currently possible to compute a cut approximator in nearly-linear 1
time [19]; this implies a similar time for the construction of a maximizer. As discussed in the previous
section  the approximator is a compact representation of all cuts in a graph  and thus it is likely that its
computation is a waste  when we only want to compute a k-clustering. Indeed  all existing algorithms
are complicated and far from practical.
On the other hand a signiﬁcant strength of spectral clustering is its speed  due to the existence of
provably fast linear system solvers for graph Laplacians [16  17]. A theoretical upper bound for the
computation of k eigenvectors is O(km log2 m)  where m is the number of edges in the graph; in
practice  for a graph with millions of edges  one eigenvector can be computed in mere seconds on
standard hardware  without even exploiting the ample potential for parallelism.
This motivates the second contribution of the paper: a fast algorithm that modiﬁes the input graph G
into a graph M which is spectrally closer to the maximizer H  and thus more amenable to spectral
clustering. The emphasis here is in the running time of the modiﬁcation algorithm and the size of
its output. These are kept low in order to not severely impact the speed of spectral clustering. We
present the algorithm  and discuss its properties in Section 4.
Finally  applying spectral clustering on graph M and mapping the output back to G has the potential
to ‘discover’ dramatically different and improved cuts. One such case is illustrated in Figure 2  on a
known bad case of spectral clustering taken from [12].

(a) 2-way partitioning on G

(b) improved partitioning based on M

Figure 2: Input G is a direct graph product of: (i) the path graph  (ii) a graph consisting of two binary
trees with their roots connected [12]. The modiﬁcation of G sways the lowest eigenvector away from
the cut computed in G. The asymptotic improvement in the value of the cut is O(n1/4).

We note that different graph modiﬁcation ideas have been explored in previous works (e.g. [27  1  4]).
In particular  in the context of ‘regularized spectral clustering’  it has been observed that adding a
small copy of the identity matrix or the complete graph onto the input graph G  improves the quality
of spectral clustering [1  21]. The improved performance has been partially explained for block-
stochastic models and stochastic social network graphs [13  29] . In the latter case  the improvement
is attributed to the ‘masking’ of unbalanced sparse cuts in the graph caused by altering their cut
ratio [29]. It is conceivable that the theoretical results of this paper will help shed additional light on
regularized spectral clustering. It is clear though  that regularized spectral clustering does not yield
improvement such as that in Figure 2.

3 Cut Approximators  Spectral Maximizers and Cheeger Inequalities

In this section we prove our main claim that for every graph G there exists another graph H which is
cut-similar to G but satisﬁes a tight Cheeger inequality. We will ﬁrst state our claims  and give the
proofs in subsection 3.4.

1O(m logc m) time  where m is the number of edges in G and c is a fairly large constant.

3

3.1 Deﬁnitions of Graph Objects

Deﬁnition 3.1 (Hierarchical Cut Decomposition). A hierarchical cut decomposition for a graph
G = (V  E  w) is represented as a rooted tree T = (V ∪ I  E(cid:48)  w(cid:48))  with the following properties:
(i) Every vertex u of T identiﬁes a set Su ⊆ V .
(ii) If r is the root of T then Sr = V .
(iii) If u has children v1  . . .   vt in T . then Svi ∩ Svj = ∅ for all (i  j).
(iv) If u is the parent of v in T then w(cid:48)(u  v) = cutG(v).
Deﬁnition 3.2 (α-Cut Approximator). We say that a hierarchical decomposition T = (V ∪ I  E(cid:48)  w(cid:48))
for G is an α-cut approximator for G if for all S ⊆ V there exists a set IS ⊆ I such that

cutG(S) ≤ cutT (S ∪ IS) ≤ α · cutG(S).

Given a graph G and an associated cut approximator T we now deﬁne the spectral maximizer for
G – the choice of terminology will be justiﬁed subsequently.
Deﬁnition 3.3 (Spectral Maximizer). Let T = (V ∪ I  E(cid:48)) be a cut approximator for a graph
G = (V  E  w) and let

(cid:18) LI

(cid:19)

LT =

V
V T D

I V .

ordered so that its ﬁrst |I| rows are indexed by I in an arbitrary order  and its last |V | rows are
indexed by V in the given order. We deﬁne the graph maximizer H to be the graph with Laplacian
matrix LH = D − V T L−1
The matrix D − V T L−1
I V in the above deﬁnition is known as the Schur complement with respect
to the elimination of the vertices/variables in I  and given the fact that LT is Laplacian  it is well
known to be a Laplacian matrix (e.g see [9]). Graph theoretically  the elimination of a vertex v from
a graph introduces a weighted clique on the neighbors of v. The elimination of a set of vertices I can
be performed as a sequence of vertex eliminations (in an arbitrary order).
Important Remark: We use the term ‘spectral maximizer’ for brevity and simplicity. It should be
made clear that the spectral maximizer is not a unique graph  as it depends on T .

3.2 Properties of Spectral Maximizers
In order to state our claims we ﬁx a triple (G T (α)  H)  where G is a graph  T is an associated
α-cut approximator and H is the spectral maximizer corresponding to T . We will also denote by
diam(T ) the diameter of the tree  i.e. the number of edges on the longest path in T .
We ﬁrst introduce some required notation additional to that from Section 2.1. Let G and H be two
graphs on the same vertex set  with the requirement that G is connected. In particular  H may be
not connected  or not even using a susbet of V . Then we say that G spectrally dominates H  if for
all vectors x we have R(LG  x) ≥ R(LH   x). We denote spectral domination by G (cid:23) H. We also
write α · G to denote the graph G with its weights multiplied by α.
Theorem 3.1 (Spectral Domination of Cut Structure). Given a triple (G T (α)  H)  let ˜G be an
arbitrary graph which is ρ-cut similar to G. Then  we have diam(T ) · ρ · H (cid:23) ˜G.
Theorem 3.2 (Cut Similarity of Spectral Maximizer). Given a triple (G T (α)  H)  the maxi-
mizer H is α · diam(T )-cut similar with G. In particular  we have cutH (S)/α ≤ cutG(S) ≤
diam(T )cutH (S).
We are now ready to discuss the justiﬁcation for the term ‘spectral maximizer’. The reader should
think of the parameters diam(T ) and α as small  i.e. of size ˜O(1)2. Then  Theorem 3.1 shows that
–up to a ˜O(1) factor– H spectrally dominates every graph that is ˜O(1)-cut similar with G. This
directly implies that –up to the same factor– the ith eigenvalue of LH is greater than that of L ˜G  for
every graph ˜G which is cut-similar to G. Combined with Theorem 3.2  we get that LH has nearly the
maximum possible eigenvalues that any graph with similar cuts can have. In the particular case of λ2

2We use the ˜O(·) notation to hide factors logarithmic in n  that we do not attempt to optimize.

4

we show that it is is actually within ˜O(1) from the graph conductance. This extends to a generalized
notion of conductance with algorithmic implications for supervised clustering; we discuss this in
the supplementary material.

3.3 Cheeger Inequalities for Spectral Maximizers

Deﬁnition 3.4 (Generalized Conductance). Let A and B be two graphs on the same set of vertices
V . We deﬁne the generalized conductance φ(A  B) of the pair as: φ(A  B) = minS⊆V
Deﬁnition 3.5 (Second Generalized Eigenvalue). The smallest generalized eigenvalue of a pair of
graphs (A  B) is given by λ2(A  B) = minx

cutA(S)
cutB (S) .

xT LAx
xT LB x .

The generalized deﬁnition encompasses the standard conductance of a graph. Concretely  let K be the
complete weighted graph  where the weight of edge (u  v) is set to be wK(u  v) = volA(u)volA(v) 
i.e. the product of the degrees of u and v in A. Also  let λ2 denote the second eigenvalue of the
normalized Laplacian of A  i.e. ˆL = D−1/2LAD−1/2  where D is the diagonal matrix of the vertex
degrees in A. Then  it is easy to show that:

φ(A  K) = φ(A) = minS⊆V

cutA(S)

volK (S)volK (V −S) and λ2(A  K) = λ2.

The Cheeger inequality [6] states that λ2 ≥ φ2/2. A Cheeger inequality is also known for the
generalized conductance [7]: λ2(A  B) ≥ φ(A  B)φ(A)/8.
We prove the following Theorem.
Theorem 3.3 (Extended Cheeger Inequality for Cut Structure).
For any graph G  there exists a graph H such that (i) H is ˜O(1)-cut similar with G  and (ii) H
satisﬁes the following inequality for all graphs B:

λ2(H  B) ≤ φ(H  B) ≤ ˜O(1)λ2(H  B).

A consequence of Theorem 3.3 is that the actual performance of spectral clustering on a given graph G
ultimately depends on its ‘spectral distance’ from its maximizer H. This is captured in the following
Corollary.
Corollary 3.1 (Actual Cheeger Inequality).
Let G be a graph and H be the graph whose existence is guaranteed by Theorem 3.3. Further 
suppose that G and H are δ-spectral similar. Then  for all graphs B  G satisﬁes the following
inequality: λ2(G  B) ≤ φ(G  B) ≤ ˜O(δ)λ2(G  B).

3.4 Proofs

In this section we simplify the notation and sometimes use G to mean both a graph and its corre-
sponding Laplacian LG.
Lemma 3.1. (Edge-Path Support [5]) Let P be an unweighted path graph on k vertices  with
endpoints u1  uk. Also let Eu1uk be the graph consisting only of the edge (u1  uk). Then we have
kP (cid:23) Eu1uk.
Lemma 3.2 (Quadratic form of Schur complement). Let H and T be the graphs matrices appearing
in Deﬁnition 3.3. We have

R(H  x) = min
y∈R|I|

R(T  

(cid:18) y

x

(cid:19)

).

We ﬁnally need the following (adjusted) Lemma from [23  3]:
Lemma 3.3. Every graph G has an ˜O(1) cut-approximator R. The diameter of T is O(log n)  where
n is the number of vertices in G.

We are now ready to proceed with the proofs.
Proof. (of Theorem 3.1) We ﬁrst show the intermediate claim diam(T ) · T (cid:23) G. The technique
uses elements from support theory [5]. Let Euv be an arbitrary edge of G of weight wuv. Let Puv be

5

We observe that  by construction of R  we have T =(cid:80)

the unique path between u and v in R; notice that by deﬁnition the path has length at most diam(T ).
(u v)∈G wuvPuv. Let y  x be arbitrary vectors

of appropriate dimensions  and z = [y  x]T . We have

(cid:80)
(cid:80)
(u v)∈G wuvR(Puv  z)
(u v)∈G wuvR(Euv  z)

R(T   z)
R(G  z)

=

≥ min
(u v)∈G

R(Puv  z)
R(Euv  z)

≥ 1/diam(T ).

repeated if we replace T with T (cid:48) =(cid:80)

The ﬁrst inequality is standard for a ratio of sums of positive numbers  and the second inequality is
an application of lemma 3.1. This proves the intermediate claim. Notice now that since the claim
holds for all vectors z = [y  x]T for arbitrary y  it also holds for vectors where y is deﬁned as in
Lemma 3.2. That implies T (H  x) ≥ T (G  x)/diam(T )  i.e. diam(T ) · H (cid:23) G.
To prove the claim for a G(cid:48) which is ρ-cut similar to G  we observe that the above proof can be
uvPuv. Thus we get diam(T )·T (cid:48) (cid:23) G(cid:48) (A). Notice
that T (cid:48) keeps the same edges of T but with different weights. Observe now that if v is a vertex
in T (cid:48) then the edge to its parent has weight equal to cutG(cid:48)(Sv)  where Sv is the set identiﬁed by
v according to the deﬁnition of the cut approximator. However by the cut similarity of G and G(cid:48)
we know that cutG(cid:48)(Sv) ≥ cutGSv/ρ. It follows that the edges of T (cid:48) have weight at most ρ times
smaller than their weights in T   which directly implies that T (cid:22) ρT (cid:48). Substituting into inequality
(A) above  we get that ρ · diam(T ) · T (cid:23) G(cid:48). Then applying lemma 3.2 one more time gives the
claim.

(u v)∈G w(cid:48)

Proof. (of Theorem 3.2) The proof is a relatively easy consequence of lemma 3.2 and deﬁnition 3.2.
We include it in the supplementary material.

Proof. (of Theorem 3.3) Let (G T (α)  H) be the given triple. Also  let B = (V  E  w) be an
arbitrary graph. The ﬁrst part of the inequality is trivial. Let x be the eigenvector corresponding
to the smallest non-zero eigenvalue of the generalized problem LH x = λLBx. Using the standard
Courant-Fischer characterization of eigenvalues  we have

λ2(H  B) =

R(LH   x)
R(LB  x)

=

R(LT   z)
R(LB  x)

 

(1)

where z is the extension of x described in lemma 3.2. For an edge Euv  let Puv denote the (unique)
path connecting u and v in T . Using lemma 3.1  we get:

R(LB  x) =

wuvR(LPuv   z) =

R(wuvLPuv   z)

(u v)∈B

(u v)∈B

Note that we now get the quadratic form of the graph T (cid:48) =(cid:80)

(u v)∈B wuvPuv. Because T (cid:48) is a sum
of paths on T   it has the same edges with T . Denote by wT (q  q(cid:48)) the weight of the edge (q  q(cid:48)) on
(cid:80)
T   where q(cid:48) is the parent of q. Continuing then on inequality 1  we get
(cid:80)
(q q(cid:48))∈T wT (q  q(cid:48))(zq − zq(cid:48))2
(q q(cid:48))∈T wT (cid:48)(q  q(cid:48))(zq − zq(cid:48))2 ≥ min

λ2(H  B) ≥ R(LT   z)
R(LT (cid:48)  z)

wT (q  q(cid:48))
wT (cid:48)(q  q(cid:48))

(u v)∈B

q∈T

(2)

=

If Sq ⊆ V is the set identiﬁed by q  we have

wT (q  q) = cutG(Sq) ≥ cutH (Sq)/α 

where the inequality comes from Theorem 3.2. Observe now that (q  q(cid:48)) appears on T (cid:48) exactly on the
paths Puv such u ∈ Sq and v ∈ S(cid:48)
q. It follows that the edge (q  q(cid:48)) receives in T (cid:48) a total weight equal
to the total weight of the edges leaving Sq on B  i.e. wT (cid:48)(q  q(cid:48)) = cutB(Sq). Further continuing on
inequality 2  we get that
λ2(H  B) ≥ min
q∈T

= φ(H  B)/α.

cutH (Sq)
α · cutB(Sq)

wT (q  q(cid:48))
wT (cid:48)(q  q(cid:48))

cutH (S)
α · cutB(S)

≥ min

≥ min

S

q

The Theorem then follows by invoking lemma 3.3 and Theorem 3.2.

6

(cid:88)

wuv(xu − xv)2 ≤ (cid:88)

(cid:88)

4 A Spectral Modiﬁcation Algorithm

The goal of spectral modiﬁcation is to construct a modiﬁer M of the input graph G = (V  E  w) 
which is spectrally similar to the maximizer described in Section 3. Then Corollary 3.1 shows that
improved Cheeger inequalities also hold for M  up to the spectral similarity factor. Echoing the
construction of the maximizer in Section 3  we will construct a graph M on a set of vertices V ∪ Vadd 
where Vadd is a set of additional vertices. The modiﬁer M is then deﬁned as the Schur complement of
M with respect to the elimination of the nodes in Vadd. We solve the generalized eigenvalue problem
LM x = λDx  where D is the diagonal of LG. The modiﬁer M is a dense graph  but we effectively
use only M. We accomplish that using standard techniques that we discuss in the supplementary ﬁle.
Cut Approximators for Trees. Towards designing a modiﬁcation algorithm  we observe that
computing a low-diameter cut approximator of a tree T is can be carried out with a recursive top-
down analysis of the cut structure of T   in O(n log n) time  essentially following the algorithm
in [23]; key to the algorithm is a linear time algorithm for computing the sparsest cut on a tree. A
low-diameter cut approximator for a tree can also be constructed in a bottom-up fashion in O(n)
time  using the decompositions from [15]. Our code implements the linear time algorithm.
We consider the following general framework for spectral modiﬁcation. Given a graph G = (V  E  w):
(a) Compute a set of weighted trees T1  . . .   Tk on vertex set V . [tree decomposition step]
(b) Compute a cut approximator Mj for each tree Tj.
(c) Form the graph M = αG + M1 + . . .Mk.
The cut approximators Tj in step (b) share the same set of leaves V   but each Tj has its own set of
additional internal vertices Vaddj . Thus  the weighted graphs in the sum of step (c) have mutually
disjoint edge sets  and the sum simply denotes the union of all these edges. The vertex set of M is

V ∪ Vadd  where Vadd =(cid:83)

j Vaddj .

Tree Decomposition Step. In this step we aim to process the input graph G in order to compute a
set of trees  such that the sum of their maximizers is spectrally close to the maximizer of G. There
exist several potential ways to perform that. We now give an algebra-based heuristic algorithm that
we have implemented and used in our experiments.
1: procedure ENERGY_TD(G  k)
z ← approximate second eigenvector of LGx = λDx
2:
G(cid:48) ← (V  E  w(cid:48))  where w(cid:48)
3:
for j = 1 : k do
4:
5:
6:
7:
8:
9:

Rj = (V  Ej  w(cid:48)) ← maximum weight spanning tree of G(cid:48)
Tj ← (V  Ej  w)
For each e ∈ Ej  let w(cid:48)

(cid:46) Tree with edge set Ej with weights from G
(cid:46) Update weights in G(cid:48)

end for
return {T1  . . .   Tk}

(cid:46) D is the diagonal of LG

uv = wuv(zu − zv)2

e = w(cid:48)

e/df

ENERGY_TD is based on the following reasoning. Assuming that the graph G is spectrally away
from its maximizer H  we expect the second eigenvector z to be “bad” in the sense that the associated
Rayleigh quotient R(G  z)/zT Dz is signiﬁcantly lower than it would have been for the maximizer
H. Steps 4-7 ﬁnd k trees in G that yield most of the ‘energy’ R(G  z). Adding the maximizers
of these trees attempts to directly ‘push’ the Rayleigh quotient for z higher in the spectrum of the
modiﬁed graph M. At the same time  because the trees Tj are subtrees of G  and their maximizers
have similar cuts  the modiﬁer M has cuts similar to those in G. We further discuss some properties
of ENERGY_TD and its running time  in the supplementary ﬁle.

4.1 Implementation and Experiments

We provide a MATLAB implentation. We plan to provide a Python implementation in the near future.
The submitted code and all future updates can be found in: https://github.com/ikoutis/spectral-modiﬁcation
Remark on Baseline Spectral Clustering: We use the baseline spectral clustering implementation
from [7]. We solve the eigenvalue problem LGx = λDx  which yields the standard embedding.
A differentiation is that we further process the embedding by projecting the points onto the unit
hypersphere  as analyzed in [18]. This actually yields a signiﬁcant improvement of the baseline.

7

Parameter Settings: For all our experiments we set k = 3  df = 1/2  and α = 1 in ENERGY_TD.
Synthetic Datasets. The synthetic example described in Figure 2 highlights the potential of spectral
modiﬁcation to induce the computation of asymptotically better cuts in graphs with ‘elongated’
features  or high diameter. The output has been computationally veriﬁed for a range of values for n
(up to millions). In the supplementary ﬁle we also describe a synthetic example of a weighted where
spectral modiﬁcation yields a cut smaller by a Θ(1/n) factor. In Figure 3  we also give a synthetic
example taken from [7]  where spectral modiﬁcation clearly outperforms even a supervised method.

(a) Standard spectral
(c) Modiﬁed spectral
Figure 3: (a) The ‘4-moons’ example from [7]. (A)RI is the Adjusted Rand Index.

(b) Supervised spectral [7]

Social Networks. We performed experiments with four graphs (BlogCatalog  PPI  Wikipedia 
Flickr) used as a benchmark in the recent literature [20  22]. We compare against NetMF [22] as it
has previously reported an improvement over DeepWalk [20] and other competing methods. The
evaluation methodology is identical to that in [22]. The second normalized eigenvalue λ of these
graphs are quite high (0.43 0.49  0.20  0.06 respectively) and so there is little room for improvement.
Nevertheless we observe improvements in the standard Micro-F1 scores. We cannot however attribute
them directly to our theory  as it is not sensitive to ˜O(1) factors. The dimension of the embedding
is equal to the number of clusters  except for the Flickr data set which is set to 128 because NetMF
method is too expensive to be run on dimension 195 (# clusters). We also wish to highlight the fact
that the implemented version of baseline spectral clustering performs much better than standard
version. A more detailed discussion can be found in the supplementary ﬁle.

4

Figure 4: Micro-F1 scores in 10x cross-validation using LIBLINEAR [10].

Conclusion. The performance of spectral clustering depends crucially on spectral properties of
its input graph  which most often force it to output clusters of poor approximation quality. This has
been viewed as an inherent limitation of spectral clustering. We show however that for any input
graph  there exists a ‘maximizer’ graph with similar cuts  but with an eigenvalue distribution which
is favorable for spectral clustering. We propose a spectral modiﬁcation algorithm that attempts to
exploit this fact via fast operations that improve the eigenvalue distribution of the input without
changing its cut structure. The implemented spectral modiﬁcation algorithm is heuristic and subject
to various improvements. Nevertheless  it yields demonstrable asymptotic improvements in a number
of adversarial instances. In future work we will explore the performance of spectral modiﬁcation on
larger and more diverse sets of instances  and the implementation of modiﬁcation algorithms with
theoretical guarantees
Acknowledgements. This work has been partially supported by grants CCF-1149048  CCF-1813374.

8

References
[1] Arash A. Amini  Aiyou Chen  Peter J. Bickel  and Elizaveta Levina. Pseudo-likelihood methods for
community detection in large sparse networks. Ann. Statist.  41(4):2097–2122  08 2013. doi: 10.1214/
13-AOS1138. URL https://doi.org/10.1214/13-AOS1138.

[2] Joshua Batson  Daniel A. Spielman  Nikhil Srivastava  and Shang-Hua Teng. Spectral sparsiﬁcation of
graphs: Theory and algorithms. Commun. ACM  56(8):87–94  August 2013. ISSN 0001-0782. doi:
10.1145/2492007.2492029. URL http://doi.acm.org/10.1145/2492007.2492029.

[3] Marcin Bienkowski  Miroslaw Korzeniowski  and Harald Räcke. A practical algorithm for constructing
oblivious routing schemes. In Proceedings of the Fifteenth Annual ACM Symposium on Parallel Algorithms
and Architectures  SPAA ’03  pages 24–33  New York  NY  USA  2003. ACM. ISBN 1-58113-661-7. doi:
10.1145/777412.777418. URL http://doi.acm.org/10.1145/777412.777418.

[4] Aleksandar Bojchevski  Yves Matkovic  and Stephan Günnemann. Robust spectral clustering for noisy
data: Modeling sparse corruptions improves latent embeddings. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  KDD ’17  page 737–746  New York 
NY  USA  2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.
3098156. URL https://doi.org/10.1145/3097983.3098156.

[5] Erik G. Boman and Bruce Hendrickson. Support theory for preconditioning. SIAM J. Matrix Anal. Appl. 

25(3):694–717  2003. ISSN 0895-4798.

[6] F.R.K. Chung. Spectral Graph Theory  volume 92 of Regional Conference Series in Mathematics. American

Mathematical Society  1997.

[7] Mihai Cucuringu  Ioannis Koutis  Sanjay Chawla  Gary Miller  and Richard Peng. Simple and scalable
constrained clustering: a generalized spectral method. In Arthur Gretton and Christian C. Robert  editors 
Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics  volume 51 of
Proceedings of Machine Learning Research  pages 445–454  Cadiz  Spain  09–11 May 2016. PMLR. URL
http://proceedings.mlr.press/v51/cucuringu16.html.

[8] W.E. Donath and A.J. Hoffman. Algorithms for partitioning graphs and computer logic based on eigenvec-

tors of connection matrices. IBM Technical Disclosure Bulletin  15(3):938–944  1972.

[9] David Durfee  Rasmus Kyng  John Peebles  Anup B. Rao  and Sushant Sachdeva. Sampling random
In Proceedings of the 49th Annual ACM SIGACT
spanning trees faster than matrix multiplication.
Symposium on Theory of Computing  STOC 2017  pages 730–742  New York  NY  USA  2017. ACM.
ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055499. URL http://doi.acm.org/10.1145/
3055399.3055499.

[10] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. LIBLINEAR: A
Library for Large Linear Classiﬁcation. Technical report  2008. URL https://www.csie.ntu.edu.tw/
~cjlin/papers/liblinear.pdf.

[11] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the
22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  KDD ’16  pages
855–864  New York  NY  USA  2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939754.
URL http://doi.acm.org/10.1145/2939672.2939754.

[12] Stephen Guattery and Gary L. Miller. On the quality of spectral separators. SIAM J. Matrix Anal.
ISSN 0895-4798. doi: 10.1137/S0895479896312262. URL http:

Appl.  19(3):701–719  jul 1998.
//dx.doi.org/10.1137/S0895479896312262.

[13] Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. Ann. Statist.  44(4):1765–1791 

08 2016. doi: 10.1214/16-AOS1447. URL https://doi.org/10.1214/16-AOS1447.

[14] Jonathan A. Kelner  Yin Tat Lee  Lorenzo Orecchia  and Aaron Sidford. An almost-linear-time algorithm
for approximate max ﬂow in undirected graphs  and its multicommodity generalizations. In Proceedings
of the Twenty-ﬁfth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA ’14  pages 217–226 
Philadelphia  PA  USA  2014. Society for Industrial and Applied Mathematics. ISBN 978-1-611973-38-9.
URL http://dl.acm.org/citation.cfm?id=2634074.2634090.

[15] Ioannis Koutis and Gary L. Miller. Graph partitioning into isolated  high conductance clusters: Theory 
computation and applications to preconditioning. In Symposiun on Parallel Algorithms and Architectures
(SPAA)  2008.

9

[16] Ioannis Koutis  Gary L. Miller  and Richard Peng. A nearly-m log n time solver for sdd linear systems. In
Proceedings of the 2011 IEEE 52Nd Annual Symposium on Foundations of Computer Science  FOCS ’11 
pages 590–598  Washington  DC  USA  2011. IEEE Computer Society. ISBN 978-0-7695-4571-4. doi:
10.1109/FOCS.2011.85. URL http://dx.doi.org/10.1109/FOCS.2011.85.

[17] Ioannis Koutis  Gary L. Miller  and Richard Peng. A fast solver for a class of linear systems. Commun.
ACM  55(10):99–107  October 2012. ISSN 0001-0782. doi: 10.1145/2347736.2347759. URL http:
//doi.acm.org/10.1145/2347736.2347759.

[18] James R. Lee  Shayan Oveis Gharan  and Luca Trevisan. Multiway spectral partitioning and higher-order
cheeger inequalities. J. ACM  61(6):37:1–37:30  December 2014. ISSN 0004-5411. doi: 10.1145/2665063.
URL http://doi.acm.org/10.1145/2665063.

[19] Richard Peng. Approximate undirected maximum ﬂows in o(mpolylog(n)) time. In Proceedings of the
Twenty-seventh Annual ACM-SIAM Symposium on Discrete Algorithms  SODA ’16  pages 1862–1867 
Philadelphia  PA  USA  2016. Society for Industrial and Applied Mathematics. ISBN 978-1-611974-33-1.
URL http://dl.acm.org/citation.cfm?id=2884435.2884565.

[20] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social representations.
In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  KDD ’14  pages 701–710  New York  NY  USA  2014. ACM. ISBN 978-1-4503-2956-9. doi:
10.1145/2623330.2623732. URL http://doi.acm.org/10.1145/2623330.2623732.

[21] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic blockmodel.
In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 
NIPS’13  pages 3120–3128  USA  2013. Curran Associates Inc. URL http://dl.acm.org/citation.
cfm?id=2999792.2999960.

[22] Jiezhong Qiu  Yuxiao Dong  Hao Ma  Jian Li  Kuansan Wang  and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk  line  pte  and node2vec. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining  WSDM 2018  Marina Del Rey  CA  USA 
February 5-9  2018  pages 459–467  2018. doi: 10.1145/3159652.3159706. URL https://doi.org/10.
1145/3159652.3159706.

[23] Harald Räcke. Minimizing congestion in general networks. In Proceedings of the 43rd Symposium on

Foundations of Computer Science  pages 43–52. IEEE  2002.

[24] Harald Räcke  Chintan Shah  and Hanjo Täubig. Computing cut-based hierarchical decompositions
in almost linear time. In Proceedings of the Twenty-ﬁfth Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA ’14  pages 227–238  Philadelphia  PA  USA  2014. Society for Industrial and Applied
Mathematics.
ISBN 978-1-611973-38-9. URL http://dl.acm.org/citation.cfm?id=2634074.
2634091.

[25] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal.
Mach. Intell.  22(8):888–905  August 2000. ISSN 0162-8828. doi: 10.1109/34.868688. URL https:
//doi.org/10.1109/34.868688.

[26] D. Spielman and S. Teng. Spectral sparsiﬁcation of graphs. SIAM Journal on Computing  40(4):981–1025 

2011. doi: 10.1137/08074489X. URL https://doi.org/10.1137/08074489X.

[27] David A. Tolliver and Gary L. Miller. Graph partitioning by spectral rounding: Applications in image
segmentation and clustering. In Proceedings of the 2006 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition - Volume 1  CVPR ’06  page 1053–1060  USA  2006. IEEE Computer
Society. ISBN 0769525970. doi: 10.1109/CVPR.2006.129. URL https://doi.org/10.1109/CVPR.
2006.129.

[28] Junyuan Xie  Ross Girshick  and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In
Proceedings of the 33rd International Conference on International Conference on Machine Learning -
Volume 48  ICML’16  pages 478–487. JMLR.org  2016. URL http://dl.acm.org/citation.cfm?
id=3045390.3045442.

[29] Yilin Zhang and Karl Rohe.

conductance.
and R. Garnett 
10631–10640. Curran Associates 
8262-understanding-regularized-spectral-clustering-via-graph-conductance.pdf.

Understanding regularized spectral clustering via graph
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi 
Information Processing Systems 31  pages
URL http://papers.nips.cc/paper/

editors  Advances

in Neural
2018.

Inc. 

10

,Laming Chen
Guoxin Zhang
Eric Zhou
Ioannis Koutis
Huong Le